<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 39]
- [cs.LG](#cs.LG) [Total: 77]
- [cs.AI](#cs.AI) [Total: 13]
- [q-bio.CB](#q-bio.CB) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [cs.HC](#cs.HC) [Total: 2]
- [stat.ML](#stat.ML) [Total: 5]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.SD](#cs.SD) [Total: 4]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.SI](#cs.SI) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.SE](#cs.SE) [Total: 6]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 21]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Towards Probabilistic Question Answering Over Tabular Data](https://arxiv.org/abs/2506.20747)
*Chen Shen,Sajjadur Rahman,Estevam Hruschka*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Current approaches for question answering (QA) over tabular data, such as
NL2SQL systems, perform well for factual questions where answers are directly
retrieved from tables. However, they fall short on probabilistic questions
requiring reasoning under uncertainty. In this paper, we introduce a new
benchmark LUCARIO and a framework for probabilistic QA over large tabular data.
Our method induces Bayesian Networks from tables, translates natural language
queries into probabilistic queries, and uses large language models (LLMs) to
generate final answers. Empirical results demonstrate significant improvements
over baselines, highlighting the benefits of hybrid symbolic-neural reasoning.

</details>


### [2] [Multi-lingual Functional Evaluation for Large Language Models](https://arxiv.org/abs/2506.20793)
*Victor Ojewale,Inioluwa Deborah Raji,Suresh Venkatasubramanian*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Multi-lingual competence in large language models is often evaluated via
static data benchmarks such as Belebele, M-MMLU and M-GSM. However, these
evaluations often fail to provide an adequate understanding of the practical
performance and robustness of models across multi-lingual settings. In
response, we create multi-lingual functional benchmarks -- Cross-Lingual Grade
School Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following
Eval (CL-IFEval)-- by translating existing functional benchmark templates from
English to five additional languages that span the range of resources available
for NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that
some static multi-lingual benchmarks capture functional performance much more
closely than others (i.e. across models, there is a 24%, 17% and 18% decrease
in performance between M-GSM and CL-GSM Symbolic in English, French and Spanish
respectively; similarly there's a 15 - 24% performance drop across languages
between Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between
M-MMLU and CL-IFEval). Similarly, we find that model robustness across
languages varies significantly, with certain languages (eg. Arabic, English)
being the most consistently well performing across evaluation iterations.

</details>


### [3] [The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas](https://arxiv.org/abs/2506.20803)
*Chenglei Si,Tatsunori Hashimoto,Diyi Yang*

Key words: LLM, 研究创意, 执行差距, 评估指标

TL;DR: LLM生成的研究创意在初始阶段可能更具新颖性，但执行后的研究表明，其实际效果不如人类专家的创意。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨LLM生成的研究创意是否能在实际执行中产生更好的研究成果。

Method: 招募43位专家执行随机分配的LLM或人类生成的创意，并完成实验和写作。

Result: 执行后，LLM创意的评分显著下降，人类创意在许多指标上表现更好。

Conclusion: 当前LLM在生成有效研究创意方面存在局限性，且创意评估需考虑执行结果。

Abstract: Large Language Models (LLMs) have shown promise in accelerating the
scientific research pipeline. A key capability for this process is the ability
to generate novel research ideas, and prior studies have found settings in
which LLM-generated research ideas were judged as more novel than human-expert
ideas. However, a good idea should not simply appear to be novel, it should
also result in better research after being executed. To test whether
AI-generated ideas lead to better research outcomes, we conduct an execution
study by recruiting 43 expert researchers to execute randomly-assigned ideas,
either written by experts or generated by an LLM. Each expert spent over 100
hours implementing the idea and wrote a 4-page short paper to document the
experiments. All the executed projects are then reviewed blindly by expert NLP
researchers. Comparing the review scores of the same ideas before and after
execution, the scores of the LLM-generated ideas decrease significantly more
than expert-written ideas on all evaluation metrics (novelty, excitement,
effectiveness, and overall; p < 0.05), closing the gap between LLM and human
ideas observed at the ideation stage. When comparing the aggregated review
scores from the execution study, we even observe that for many metrics there is
a flip in rankings where human ideas score higher than LLM ideas. This
ideation-execution gap highlights the limitations of current LLMs in generating
truly effective research ideas and the challenge of evaluating research ideas
in the absence of execution outcomes.

</details>


### [4] [MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering](https://arxiv.org/abs/2506.20821)
*Chinmay Gondhalekar,Urjitkumar Patel,Fang-Chun Yeh*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span
hundreds of pages and combine diverse modalities, including dense narrative
text, structured tables, and complex figures. Answering questions over such
content often requires joint reasoning across modalities, which strains
traditional large language models (LLMs) and retrieval-augmented generation
(RAG) pipelines due to token limitations, layout loss, and fragmented
cross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation
framework purpose-built for financial QA. MultiFinRAG first performs multimodal
extraction by grouping table and figure images into batches and sending them to
a lightweight, quantized open-source multimodal LLM, which produces both
structured JSON outputs and concise textual summaries. These outputs, along
with narrative text, are embedded and indexed with modality-aware similarity
thresholds for precise retrieval. A tiered fallback strategy then dynamically
escalates from text-only to text+table+image contexts when necessary, enabling
cross-modal reasoning while reducing irrelevant context. Despite running on
commodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy
than ChatGPT-4o (free-tier) on complex financial QA tasks involving text,
tables, images, and combined multimodal reasoning.

</details>


### [5] [Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes](https://arxiv.org/abs/2506.20822)
*Quintin Myers,Yanjun Gao*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) are increasingly proposed for detecting and
responding to violent content online, yet their ability to reason about morally
ambiguous, real-world scenarios remains underexamined. We present the first
study to evaluate LLMs using a validated social science instrument designed to
measure human response to everyday conflict, namely the Violent Behavior
Vignette Questionnaire (VBVQ). To assess potential bias, we introduce
persona-based prompting that varies race, age, and geographic identity within
the United States. Six LLMs developed across different geopolitical and
organizational contexts are evaluated under a unified zero-shot setting. Our
study reveals two key findings: (1) LLMs surface-level text generation often
diverges from their internal preference for violent responses; (2) their
violent tendencies vary across demographics, frequently contradicting
established findings in criminology, social science, and psychology.

</details>


### [6] [Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine](https://arxiv.org/abs/2506.20876)
*Sebastian Joseph,Lily Chen,Barry Wei,Michael Mackert,Iain J. Marshall,Paul Pu Liang,Ramez Kouzy,Byron C. Wallace,Junyi Jessy Li*

Key words: 自动事实查证、医学、交互式沟通、临床专家、证据合成

TL;DR: 本文探讨了自动事实查证系统在医学领域的应用及其挑战，指出当前系统未被广泛使用的原因，并提出应将事实查证视为交互式沟通问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 由于医学决策的高风险和医学文献的复杂性，公众对医疗健康领域的事实查证系统需求增加，但现有系统未被充分利用。

Method: 通过研究临床专家如何验证社交媒体上的真实医疗声明，分析医学事实查证的挑战。

Result: 发现医学事实查证中存在的核心问题：难以将声明与临床试验联系起来、模糊性和主观性标签。

Conclusion: 建议将事实查证视为交互式沟通问题，而非端到端过程进行评价。

Abstract: Technological progress has led to concrete advancements in tasks that were
regarded as challenging, such as automatic fact-checking. Interest in adopting
these systems for public health and medicine has grown due to the high-stakes
nature of medical decisions and challenges in critically appraising a vast and
diverse medical literature. Evidence-based medicine connects to every
individual, and yet the nature of it is highly technical, rendering the medical
literacy of majority users inadequate to sufficiently navigate the domain. Such
problems with medical communication ripens the ground for end-to-end
fact-checking agents: check a claim against current medical literature and
return with an evidence-backed verdict. And yet, such systems remain largely
unused. To understand this, we present the first study examining how clinical
experts verify real claims from social media by synthesizing medical evidence.
In searching for this upper-bound, we reveal fundamental challenges in
end-to-end fact-checking when applied to medicine: Difficulties connecting
claims in the wild to scientific evidence in the form of clinical trials;
ambiguities in underspecified claims mixed with mismatched intentions; and
inherently subjective veracity labels. We argue that fact-checking should be
approached and evaluated as an interactive communication problem, rather than
an end-to-end process.

</details>


### [7] [Optimising Language Models for Downstream Tasks: A Post-Training Perspective](https://arxiv.org/abs/2506.20917)
*Zhengyan Shi*

Key words: 语言模型, 无标签数据, 参数高效微调, 监督微调, 评估基准

TL;DR: 该论文提出了一系列方法以提高语言模型（LM）在下游任务中的适应能力，包括从无标签数据中提取任务相关知识的策略、参数高效微调方法以及改进的监督微调方法，同时开发了新的评估基准。实验表明这些方法显著提升了LM的鲁棒性、效率和泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前语言模型（LM）在适应特定任务时存在效率低、过拟合等问题，限制了其在实际语言任务中的应用。

Method: 1. 引入新颖的继续预训练技术从无标签数据中提取任务相关知识；2. 提出参数高效微调方法，降低计算成本；3. 改进监督微调方法，提升指令遵循能力；4. 开发新的评估基准，如多跳空间推理任务。

Result: 这些方法显著提升了语言模型的鲁棒性、效率和泛化能力，适用于广泛的NLP任务。

Conclusion: 论文提出的方法为实现更鲁棒、高效的语言模型迈出了重要一步，推动了人工通用智能的发展。

Abstract: Language models (LMs) have demonstrated remarkable capabilities in NLP, yet
adapting them efficiently and robustly to specific tasks remains challenging.
As their scale and complexity grow, fine-tuning LMs on labelled data often
underutilizes available unlabelled data, leads to overfitting on small
task-specific sets, and imposes significant computational costs. These
limitations hamper their application to the open-ended landscape of real-world
language tasks.
  This thesis proposes a series of methods to better adapt LMs to downstream
applications. First, we explore strategies for extracting task-relevant
knowledge from unlabelled data, introducing a novel continued pre-training
technique that outperforms state-of-the-art semi-supervised approaches. Next,
we present a parameter-efficient fine-tuning method that substantially reduces
memory and compute costs while maintaining competitive performance. We also
introduce improved supervised fine-tuning methods that enable LMs to better
follow instructions, especially when labelled data is scarce, enhancing their
performance across a range of NLP tasks, including open-ended generation.
Finally, we develop new evaluation methods and benchmarks, such as multi-hop
spatial reasoning tasks, to assess LM capabilities and adaptation more
comprehensively.
  Through extensive empirical studies across diverse NLP tasks, our results
demonstrate that these approaches substantially improve LM robustness,
efficiency, and generalization, making them more adaptable to a broad range of
applications. These advances mark a significant step towards more robust and
efficient LMs, bringing us closer to the goal of artificial general
intelligence.

</details>


### [8] [FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language](https://arxiv.org/abs/2506.20920)
*Guilherme Penedo,Hynek Kydlíček,Vinko Sabolčec,Bettina Messmer,Negar Foroutan,Amir Hossein Kargaran,Colin Raffel,Martin Jaggi,Leandro Von Werra,Thomas Wolf*

Key words: 多语言预训练数据集、FineWeb2、大语言模型、数据去重、数据集重平衡

TL;DR: 该论文提出了一种自动适配多种语言的预训练数据集整理流程，通过优化筛选和去重方法，创建了高质量的非英语语料库，并提出了基于复制计数和质量的数据集重平衡方法，最终发布了包含20TB数据的FineWeb2多语言数据集。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为了解决多语言大语言模型预训练中因语言多样性导致的筛选和去重难题，研究团队致力于开发一种能自动适应任何语言的数据集整理流程。

Method: 基于FineWeb的数据集整理流程，通过九种语言的实验验证设计选择，采用基于可测量标准的任务选择方法，优化数据集的重平衡策略。

Result: 新流程创建的非英语语料库优于现有数据集，同时提出的重平衡方法进一步提升了模型性能。最终发布了包含20TB数据和相关代码的FineWeb2数据集。

Conclusion: 该研究提出的多语言预训练数据集整理流程有效提升了模型性能，为多语言LLM的发展提供了重要资源。

Abstract: Pre-training state-of-the-art large language models (LLMs) requires vast
amounts of clean and diverse text data. While the open development of large
high-quality English pre-training datasets has seen substantial recent
progress, training performant multilingual LLMs remains a challenge, in large
part due to the inherent difficulty of tailoring filtering and deduplication
pipelines to a large number of languages. In this work, we introduce a new
pre-training dataset curation pipeline based on FineWeb that can be
automatically adapted to support any language. We extensively ablate our
pipeline design choices on a set of nine diverse languages, guided by a set of
meaningful and informative evaluation tasks that were chosen through a novel
selection process based on measurable criteria. Ultimately, we show that our
pipeline can be used to create non-English corpora that produce more performant
models than prior datasets. We additionally introduce a straightforward and
principled approach to rebalance datasets that takes into consideration both
duplication count and quality, providing an additional performance uplift.
Finally, we scale our pipeline to over 1000 languages using almost 100 Common
Crawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document)
multilingual dataset which we release along with our pipeline, training, and
evaluation codebases.

</details>


### [9] [KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model](https://arxiv.org/abs/2506.20923)
*Xinping Zhao,Xinshuo Hu,Zifei Shan,Shouzheng Huang,Yao Zhou,Zetian Sun,Zhenyu Liu,Dongfang Li,Xinyuan Wei,Qian Chen,Youcheng Pan,Yang Xiang,Meishan Zhang,Haofen Wang,Jun Yu,Baotian Hu,Min Zhang*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we propose KaLM-Embedding-V2, a versatile and compact
embedding model, which achieves impressive performance in general-purpose text
embedding tasks by leveraging superior training techniques and data. Our key
innovations include: (1) To better align the architecture with representation
learning, we remove the causal attention mask and adopt a fully bidirectional
transformer with simple yet effective mean-pooling to produce fixed-length
embeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on
large-scale weakly supervised open-source corpora; (ii) fine-tuning on
high-quality retrieval and non-retrieval datasets; and (iii) model-soup
parameter averaging for robust generalization. Besides, we introduce a
focal-style reweighting mechanism that concentrates learning on difficult
samples and an online hard-negative mixing strategy to continuously enrich hard
negatives without expensive offline mining; (3) We collect over 20 categories
of data for pre-training and 100 categories of data for fine-tuning, to boost
both the performance and generalization of the embedding model. Extensive
evaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English
show that our model significantly outperforms others of comparable size, and
competes with 3x, 14x, 18x, and 26x larger embedding models, setting a new
standard for a versatile and compact embedding model with less than 1B
parameters.

</details>


### [10] [Can Gradient Descent Simulate Prompting?](https://arxiv.org/abs/2506.20989)
*Eric Zhang,Leshem Choshen,Jacob Andreas*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: There are two primary ways of incorporating new information into a language
model (LM): changing its prompt or changing its parameters, e.g. via
fine-tuning. Parameter updates incur no long-term storage cost for model
changes. However, for many model updates, prompting is significantly more
effective: prompted models can generalize robustly from single examples and
draw logical inferences that do not occur under standard fine-tuning. Can
models be modified so that fine-tuning does emulate prompting? This paper
describes a method for meta-training LMs such that gradient updates emulate the
effects of conditioning on new information. Our approach uses tools from
gradient-based meta-learning but uses an LM's own prompted predictions as
targets, eliminating the need for ground-truth labels. Subsequent gradient
descent training recovers some (and occasionally all) of prompted model
performance -- showing improvement on the ``reversal curse'' tasks, and
answering questions about text passages after a single gradient update. These
results suggest that, with appropriate initialization, gradient descent can be
surprisingly expressive. Our results suggest new avenues for long-context
modeling and offer insight into the generalization capabilities of
gradient-based learning.

</details>


### [11] [SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control](https://arxiv.org/abs/2506.20993)
*Adithya Chittem,Aishna Shrivastava,Sai Tarun Pendela,Jagat Sesh Challa,Dhruv Kumar*

Key words: 大语言模型（LLM）, 性格模型, 16PF, 特定属性控制（SAC）

TL;DR: 本文通过扩展机器性格量表（MPI），引入16种性格因子模型（16PF）和特定属性控制（SAC）框架，实现了对LLM性格的精细控制和动态调节。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有LLM性格模型依赖大五模型（OCEAN），缺乏对特质强度的控制，本文旨在填补这一空白。

Method: 扩展MPI以支持16PF模型，开发SAC框架，通过形容词锚定和行为问题动态调节特质强度。

Result: 实验表明，连续谱建模比二元特质切换更一致可控，且特质强度的变化会影响相关特质。

Conclusion: LLM内化了多维性格结构，SAC为医疗、教育等领域提供了更可控的人机交互途径。

Abstract: Large language models (LLMs) have gained significant traction across a wide
range of fields in recent years. There is also a growing expectation for them
to display human-like personalities during interactions. To meet this
expectation, numerous studies have proposed methods for modelling LLM
personalities through psychometric evaluations. However, most existing models
face two major limitations: they rely on the Big Five (OCEAN) framework, which
only provides coarse personality dimensions, and they lack mechanisms for
controlling trait intensity. In this paper, we address this gap by extending
the Machine Personality Inventory (MPI), which originally used the Big Five
model, to incorporate the 16 Personality Factor (16PF) model, allowing
expressive control over sixteen distinct traits. We also developed a structured
framework known as Specific Attribute Control (SAC) for evaluating and
dynamically inducing trait intensity in LLMs. Our method introduces
adjective-based semantic anchoring to guide trait intensity expression and
leverages behavioural questions across five intensity factors:
\textit{Frequency}, \textit{Depth}, \textit{Threshold}, \textit{Effort}, and
\textit{Willingness}. Through experimentation, we find that modelling intensity
as a continuous spectrum yields substantially more consistent and controllable
personality expression compared to binary trait toggling. Moreover, we observe
that changes in target trait intensity systematically influence closely related
traits in psychologically coherent directions, suggesting that LLMs internalize
multi-dimensional personality structures rather than treating traits in
isolation. Our work opens new pathways for controlled and nuanced human-machine
interactions in domains such as healthcare, education, and interviewing
processes, bringing us one step closer to truly human-like social machines.

</details>


### [12] [Large Language Models Acing Chartered Accountancy](https://arxiv.org/abs/2506.21031)
*Jatin Gupta,Akhil Sharma,Saransh Singhania,Mohammad Adnan,Sakshi Deo,Ali Imam Abidi,Keshav Gupta*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Advanced intelligent systems, particularly Large Language Models (LLMs), are
significantly reshaping financial practices through advancements in Natural
Language Processing (NLP). However, the extent to which these models
effectively capture and apply domain-specific financial knowledge remains
uncertain. Addressing a critical gap in the expansive Indian financial context,
this paper introduces CA-Ben, a Chartered Accountancy benchmark specifically
designed to evaluate the financial, legal, and quantitative reasoning
capabilities of LLMs. CA-Ben comprises structured question-answer datasets
derived from the rigorous examinations conducted by the Institute of Chartered
Accountants of India (ICAI), spanning foundational, intermediate, and advanced
CA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1
405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated
using standardized protocols. Results indicate variations in performance, with
Claude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and
legal reasoning. Notable challenges emerged in numerical computations and legal
interpretations. The findings emphasize the strengths and limitations of
current LLMs, suggesting future improvements through hybrid reasoning and
retrieval-augmented generation methods, particularly for quantitative analysis
and accurate legal interpretation.

</details>


### [13] [A Semi-supervised Scalable Unified Framework for E-commerce Query Classification](https://arxiv.org/abs/2506.21049)
*Chunyuan Yuan,Chong Zhang,Zheng Fang,Ming Pang,Xue Jiang,Changping Peng,Zhangang Lin,Ching Law*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Query classification, including multiple subtasks such as intent and category
prediction, is vital to e-commerce applications. E-commerce queries are usually
short and lack context, and the information between labels cannot be used,
resulting in insufficient prior information for modeling. Most existing
industrial query classification methods rely on users' posterior click behavior
to construct training samples, resulting in a Matthew vicious cycle.
Furthermore, the subtasks of query classification lack a unified framework,
leading to low efficiency for algorithm optimization.
  In this paper, we propose a novel Semi-supervised Scalable Unified Framework
(SSUF), containing multiple enhanced modules to unify the query classification
tasks. The knowledge-enhanced module uses world knowledge to enhance query
representations and solve the problem of insufficient query information. The
label-enhanced module uses label semantics and semi-supervised signals to
reduce the dependence on posterior labels. The structure-enhanced module
enhances the label representation based on the complex label relations. Each
module is highly pluggable, and input features can be added or removed as
needed according to each subtask. We conduct extensive offline and online A/B
experiments, and the results show that SSUF significantly outperforms the
state-of-the-art models.

</details>


### [14] [MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for Conversational Stance Detection](https://arxiv.org/abs/2506.21053)
*Fuqiang Niu,Genan Dai,Yisha Lu,Jiayu Liao,Xiang Li,Hu Huang,Bowen Zhang*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In the realm of contemporary social media, automatic stance detection is
pivotal for opinion mining, as it synthesizes and examines user perspectives on
contentious topics to uncover prevailing trends and sentiments. Traditional
stance detection research often targets individual instances, thereby limiting
its capacity to model multi-party discussions typical in real social media
scenarios. This shortcoming largely stems from the scarcity of datasets that
authentically capture the dynamics of social media interactions, hindering
advancements in conversational stance detection. In this paper, we introduce
MT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational
stance detection. To the best of our knowledge, MT2-CSD is the largest dataset
available for this purpose, comprising 24,457 annotated instances and
exhibiting the greatest conversational depth, thereby presenting new challenges
for stance detection. To address these challenges, we propose the Large
Language model enhanced Conversational Relational Attention Network (LLM-CRAN),
which exploits the reasoning capabilities of LLMs to improve conversational
understanding. We conduct extensive experiments to evaluate the efficacy of
LLM-CRAN on the MT2-CSD dataset. The experimental results indicate that
LLM-CRAN significantly outperforms strong baseline models in the task of
conversational stance detection.

</details>


### [15] [DALR: Dual-level Alignment Learning for Multimodal Sentence Representation Learning](https://arxiv.org/abs/2506.21096)
*Kang He,Yuzhe Ding. Haining Wang,Fei Li,Chong Teng,Donghong Ji*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Previous multimodal sentence representation learning methods have achieved
impressive performance. However, most approaches focus on aligning images and
text at a coarse level, facing two critical challenges:cross-modal misalignment
bias and intra-modal semantic divergence, which significantly degrade sentence
representation quality. To address these challenges, we propose DALR
(Dual-level Alignment Learning for Multimodal Sentence Representation). For
cross-modal alignment, we propose a consistency learning module that softens
negative samples and utilizes semantic similarity from an auxiliary task to
achieve fine-grained cross-modal alignment. Additionally, we contend that
sentence relationships go beyond binary positive-negative labels, exhibiting a
more intricate ranking structure. To better capture these relationships and
enhance representation quality, we integrate ranking distillation with global
intra-modal alignment learning. Comprehensive experiments on semantic textual
similarity (STS) and transfer (TR) tasks validate the effectiveness of our
approach, consistently demonstrating its superiority over state-of-the-art
baselines.

</details>


### [16] [ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry](https://arxiv.org/abs/2506.21098)
*Qinwen Chen,Wenbiao Tao,Zhiwei Zhu,Mingfan Xi,Liangzhong Guo,Yuan Wang,Wei Wang,Yunshi Lan*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Community Question Answering (CQA) platforms can be deemed as important
knowledge bases in community, but effectively leveraging historical
interactions and domain knowledge in real-time remains a challenge. Existing
methods often underutilize external knowledge, fail to incorporate dynamic
historical QA context, or lack memory mechanisms suited for industrial
deployment. We propose ComRAG, a retrieval-augmented generation framework for
real-time industrial CQA that integrates static knowledge with dynamic
historical QA pairs via a centroid-based memory mechanism designed for
retrieval, generation, and efficient storage. Evaluated on three industrial CQA
datasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%
improvement in vector similarity, reducing latency by 8.7% to 23.3%, and
lowering chunk growth from 20.23% to 2.06% over iterations.

</details>


### [17] [Progtuning: Progressive Fine-tuning Framework for Transformer-based Language Models](https://arxiv.org/abs/2506.21119)
*Xiaoshuang Ji,Zhendong Zhao,Xiaojun Chen,Xin Zhao,Zeyao Liu*

Key words: Transformer, 微调, 参数高效, 渐进学习, 资源分配

TL;DR: 论文提出了一种结合渐进学习的微调框架Progtuning，通过逐步减少更新的Transformer块数量，优化资源分配并减少约25%的更新参数，同时保持性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着模型规模增大，全参数微调成本高；现有参数高效微调方法仍存在资源分配不均的问题。

Method: 提出Progtuning框架，根据贡献度逐步减少更新的Transformer块数量。

Result: 减少约25%的更新参数，同时保持竞争力性能，且能适应多种参数高效微调方法。

Conclusion: Progtuning显著提高了资源利用效率，并在多种场景下表现优异。

Abstract: Fine-tuning is a promising technique for leveraging Transformer-based
language models in downstream tasks. As model sizes continue to grow, updating
all model parameters becomes increasingly costly. Parameter-efficient
fine-tuning methods effectively address this issue by selectively updating a
small subset of parameters. However, fine-tuning and most existing
parameter-efficient fine-tuning methods require updating the same number of
parameters as the initial size, ignoring the unequal contribution across
Transformer blocks and leading to extremely inefficient allocation of computing
resources. In this paper, we propose Progtuning, the novel fine-tuning
framework combined with progressive learning for Transformer-based language
models. Specifically, Progtuning progressively reduces the number of updated
transformer blocks based on the contribution. Remarkably, Progtuning optimizes
resource allocation and reduces the number of updated parameters by
approximately 25\%, while still maintaining competitive performance. And it
also exhibits high adaptability with parameter-efficient fine-tuning methods,
demonstrating excellent performance across various adaptation scenarios.

</details>


### [18] [Compressed and Smooth Latent Space for Text Diffusion Modeling](https://arxiv.org/abs/2506.21170)
*Viacheslav Meshchaninov,Egor Chimbulatov,Alexander Shabalin,Aleksandr Abramov,Dmitry Vetrov*

Key words: 

TL;DR: Cosmos提出了一种在压缩的潜在空间中进行文本生成的新方法，解决了自回归模型速度慢和扩散模型在高维空间中应用困难的问题，实现了更快的推理速度和可比或更优的生成质量。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 自回归语言模型在文本生成中存在速度慢和全局连贯性差的问题，而扩散模型在高维令牌空间中应用困难。Cosmos旨在通过压缩潜在空间解决这些问题。

Method: Cosmos使用一种自动编码器在压缩潜在空间中操作，同时训练令牌级重建和与预训练语言编码器的对齐，支持有效的扰动增强。

Result: 实验表明，Cosmos能将文本表示压缩8倍，同时生成质量与令牌级扩散模型相当；增加潜在序列长度后，其性能甚至超过扩散和自回归基线。

Conclusion: Cosmos在故事生成、问题生成、摘要和去毒任务中表现出色，推理速度提高了2倍以上，为文本生成提供了高效且高质量的解决方案。

Abstract: Autoregressive language models dominate modern text generation, yet their
sequential nature introduces fundamental limitations: decoding is slow, and
maintaining global coherence remains challenging. Diffusion models offer a
promising alternative by enabling parallel generation and flexible control;
however, their application to text generation is hindered by the high
dimensionality of token-level representations. We introduce Cosmos, a novel
approach to text generation that operates entirely in a compressed, smooth
latent space tailored specifically for diffusion. This space is learned using
an autoencoder trained simultaneously for token-level reconstruction and
alignment with frozen activations from a pretrained language encoder, providing
robust semantic grounding and enabling effective perturbation-based
augmentations. Empirically, we demonstrate that text representations can be
compressed by $8\times$ while maintaining generation quality comparable to
token-level diffusion models. Furthermore, increasing the latent sequence
length allows Cosmos to surpass both diffusion-based and autoregressive
baselines. We evaluate Cosmos on four diverse generative tasks including story
generation, question generation, summarization, and detoxification and compare
it with various generative paradigms. Cosmos achieves comparable or superior
generation quality while offering more than $2\times$ faster inference.

</details>


### [19] [CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment](https://arxiv.org/abs/2506.20243)
*Papa Séga Wade,Mihai Andries,Ioannis Kanellos,Thierry Moudenc*

Key words: 自动流畅度评估,自监督学习,分块策略,CNN-BiLSTM,语音分析

TL;DR: 论文提出了一种基于分块的自动流畅度评估方法，通过融合多种自监督学习模型和层级CNN-BiLSTM框架，显著提升了评估性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 非母语者的语音节奏、停顿和不流畅性是自动流畅度评估的难点，需要更精细的建模方法。

Method: 结合Wav2Vec2、HuBERT和WavLM的自监督学习模型，采用分块策略（Silero-VAD分割）和权重融合机制，提取语音速率、停顿时长等流畅度标记，并用CNN-BiLSTM捕捉局部与长期依赖。

Result: 在Speechocean762和Avalinguo数据集上，F1分数和Pearson相关系数分别提升2.8-4.2和6.2-4.0分，优于基线模型。

Conclusion: 分块多模型融合方法对流畅度评估有效，但需进一步研究其在非规则韵律方言中的泛化能力。

Abstract: Automatic fluency assessment (AFA) remains challenging, particularly in
capturing speech rhythm, pauses, and disfluencies in non-native speakers. We
introduce a chunk-based approach integrating self-supervised learning (SSL)
models (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths
in phonetic, prosodic, and noisy speech modeling, with a hierarchical
CNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero
voice activity detection (Silero-VAD), enabling fine-grained temporal analysis
while mitigating over-segmentation artifacts. SSL embeddings are fused via a
learnable weighted mechanism, balancing acoustic and linguistic features, and
enriched with chunk-level fluency markers (e.g., speech rate, pause durations,
n-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies
across chunks. Evaluated on Avalinguo and Speechocean762, our approach improves
F1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines
on Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on
Avalinguo, surpassing Pyannote.audio-based segmentation baselines. These
findings highlight chunk-based multi-SSL fusion for robust fluency evaluation,
though future work should explore generalization to dialects with irregular
prosody.

</details>


### [20] [Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks](https://arxiv.org/abs/2506.21182)
*Isaac Chung,Imene Kerboua,Marton Kardos,Roman Solomatin,Kenneth Enevoldsen*

Key words: MTEB, 文本嵌入, 基准测试, 可复现性, 持续集成

TL;DR: 本文关注MTEB基准测试的工程实践，确保其可复现性和扩展性，包括持续集成、数据处理和社区贡献管理。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 提升MTEB基准测试的可复现性和扩展性，以支持文本嵌入模型的标准化评估。

Method: 采用持续集成流程验证数据集完整性，自动化测试执行，并增强基准结果的可推广性。

Result: 通过工程设计优化，MTEB扩展了任务和数据集，同时保持了质量和相关性。

Conclusion: 工程实践对机器学习评估框架的可复现性和可用性至关重要，MTEB的经验为类似挑战提供了借鉴。

Abstract: The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation
platform for text embedding models. While previous work has established the
core benchmark methodology, this paper focuses on the engineering aspects that
ensure MTEB's continued reproducibility and extensibility. We present our
approach to maintaining robust continuous integration pipelines that validate
dataset integrity, automate test execution, and assess benchmark results'
generalizability. We detail the design choices that collectively enhance
reproducibility and usability. Furthermore, we discuss our strategies for
handling community contributions and extending the benchmark with new tasks and
datasets. These engineering practices have been instrumental in scaling MTEB to
become more comprehensive while maintaining quality and, ultimately, relevance
to the field. Our experiences offer valuable insights for benchmark maintainers
facing similar challenges in ensuring reproducibility and usability in machine
learning evaluation frameworks. The MTEB repository is available at:
https://github.com/embeddings-benchmark/mteb

</details>


### [21] [Prompt-Guided Turn-Taking Prediction](https://arxiv.org/abs/2506.21191)
*Koji Inoue,Mikey Elmers,Yahui Fu,Zi Haur Pang,Divesh Lala,Keiko Ochi,Tatsuya Kawahara*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Turn-taking prediction models are essential components in spoken dialogue
systems and conversational robots. Recent approaches leverage transformer-based
architectures to predict speech activity continuously and in real-time. In this
study, we propose a novel model that enables turn-taking prediction to be
dynamically controlled via textual prompts. This approach allows intuitive and
explicit control through instructions such as "faster" or "calmer" adapting
dynamically to conversational partners and contexts. The proposed model builds
upon a transformer-based voice activity projection (VAP) model, incorporating
textual prompt embeddings into both channel-wise transformers and a
cross-channel transformer. We evaluated the feasibility of our approach using
over 950 hours of human-human spoken dialogue data. Since textual prompt data
for the proposed approach was not available in existing datasets, we utilized a
large language model (LLM) to generate synthetic prompt sentences. Experimental
results demonstrated that the proposed model improved prediction accuracy and
effectively varied turn-taking timing behaviors according to the textual
prompts.

</details>


### [22] [Enhancing Automatic Term Extraction with Large Language Models via Syntactic Retrieval](https://arxiv.org/abs/2506.21222)
*Yongchan Chun,Minhyuk Kim,Dongjun Kim,Chanjun Park,Heuiseok Lim*

Key words: 自动术语提取（ATE）、大型语言模型（LLM）、句法相似性、检索提示、少样本学习

TL;DR: 本文提出了一种基于检索的提示策略，用于自动术语提取（ATE），通过句法相似性而非语义相似性选择示例，从而提升领域无关性和术语边界捕捉能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管大型语言模型（LLM）在多种NLP任务中表现出色，但其在自动术语提取（ATE）中的应用尚未充分探索。本研究旨在探索如何利用LLM进行ATE，并通过句法相似性改进检索效率。

Method: 提出了一种基于检索的提示策略，在少样本设置下，根据句法相似性选择示例。这种方法不依赖领域知识，能够更可靠地捕捉术语边界。

Result: 在三个专门的ATE基准测试中，句法检索方法显著提高了F1分数，尤其是在查询句与检索示例之间的词汇重叠分析中表现出色。

Conclusion: 句法线索在适应LLM进行术语提取任务时具有重要作用，句法检索策略是一种有效的领域无关方法。

Abstract: Automatic Term Extraction (ATE) identifies domain-specific expressions that
are crucial for downstream tasks such as machine translation and information
retrieval. Although large language models (LLMs) have significantly advanced
various NLP tasks, their potential for ATE has scarcely been examined. We
propose a retrieval-based prompting strategy that, in the few-shot setting,
selects demonstrations according to \emph{syntactic} rather than semantic
similarity. This syntactic retrieval method is domain-agnostic and provides
more reliable guidance for capturing term boundaries. We evaluate the approach
in both in-domain and cross-domain settings, analyzing how lexical overlap
between the query sentence and its retrieved examples affects performance.
Experiments on three specialized ATE benchmarks show that syntactic retrieval
improves F1-score. These findings highlight the importance of syntactic cues
when adapting LLMs to terminology-extraction tasks.

</details>


### [23] [Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents](https://arxiv.org/abs/2506.21252)
*Tianyi Men,Zhuoran Jin,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Key words: MLLMs, 奖励建模, 代理任务, 基准测试, 多模态智能

TL;DR: 论文提出Agent-RewardBench，一个评估MLLMs奖励建模能力的基准，覆盖多维度场景和任务步骤，实验显示现有模型表现有限。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前MLLMs在多模态任务中因缺乏外部反馈，难以实现自我纠正和泛化，急需针对代理任务的奖励建模基准。

Method: 构建Agent-RewardBench，包含多维度场景、步骤级奖励评估、难度控制和高质量控制，从10个模型中精心采样并进行人工验证。

Result: 实验表明，即使最先进的多模态模型在奖励建模方面表现有限，需针对代理任务进行专门训练。

Conclusion: Agent-RewardBench为MLLMs在代理任务中的奖励建模提供了有效评估工具，并揭示了现有模型的局限性。

Abstract: As Multimodal Large Language Models (MLLMs) advance, multimodal agents show
promise in real-world tasks like web navigation and embodied intelligence.
However, due to limitations in a lack of external feedback, these agents
struggle with self-correction and generalization. A promising approach is to
use reward models as external feedback, but there is no clear on how to select
reward models for agents. Thus, there is an urgent need to build a reward bench
targeted at agents. To address these challenges, we propose Agent-RewardBench,
a benchmark designed to evaluate reward modeling ability in MLLMs. The
benchmark is characterized by three key features: (1) Multiple dimensions and
real-world agent scenarios evaluation. It covers perception, planning, and
safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the
assessment of agent capabilities at the individual steps of a task, providing a
more granular view of performance during the planning process; and (3)
Appropriately difficulty and high-quality. We carefully sample from 10 diverse
models, difficulty control to maintain task challenges, and manual verification
to ensure the integrity of the data. Experiments demonstrate that even
state-of-the-art multimodal models show limited performance, highlighting the
need for specialized training in agent reward modeling. Code is available at
github.

</details>


### [24] [Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?](https://arxiv.org/abs/2506.21274)
*Andrea McGlinchey,Peter J Barclay*

Key words: 大型语言模型,假文本检测,统计分类器,模型欺骗性

TL;DR: 大型语言模型能生成逼真的‘假文本’，但简单分类器仍能有效检测。研究表明，随着模型规模增大，检测的可行性可能持续存在。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨大型语言模型生成的假文本与检测器之间的‘军备竞赛’是否会导致检测能力达到瓶颈。

Method: 使用统计分类器检测古典侦探小说风格的假文本，比较不同模型版本的生成与检测能力。

Result: Gemini在0.5版本更新后生成欺骗性文本的能力增强，而GPT无显著变化。

Conclusion: 即使模型规模持续增大，可靠检测假文本仍可能可行，但新模型架构可能提升欺骗性。

Abstract: Large language models can produce convincing "fake text" in domains such as
academic writing, product reviews, and political news. Many approaches have
been investigated for the detection of artificially generated text. While this
may seem to presage an endless "arms race", we note that newer LLMs use ever
more parameters, training data, and energy, while relatively simple classifiers
demonstrate a good level of detection accuracy with modest resources. To
approach the question of whether the models' ability to beat the detectors may
therefore reach a plateau, we examine the ability of statistical classifiers to
identify "fake text" in the style of classical detective fiction. Over a 0.5
version increase, we found that Gemini showed an increased ability to generate
deceptive text, while GPT did not. This suggests that reliable detection of
fake text may remain feasible even for ever-larger models, though new model
architectures may improve their deceptiveness

</details>


### [25] [Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning](https://arxiv.org/abs/2506.21285)
*Xin Xu,Tianhao Chen,Fan Zhang,Wanlong Liu,Pengxiang Li,Ajay Kumar Jaiswal,Yuchen Yan,Jishan Hu,Yang Wang,Hao Chen,Shiwei Liu,Shizhe Diao,Can Yang,Lu Yin*

Key words: LLM，自我批判，迭代优化，推理能力，Double-Checker

TL;DR: 论文提出Double-Checker框架，通过自我批判和迭代优化提升慢思考LLM的推理能力，在AIME基准上表现显著提升。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 慢思考LLM在生成批判性反馈和优化解决方案方面能力有限，需要一种方法来增强其自我批判和迭代优化的能力。

Method: 提出Double-Checker框架，基于1730个自我批判实例微调，使LLM能在推理中迭代批判和优化输出。

Result: 在多个推理基准上验证，Double-Checker显著提升性能，AIME基准pass@1从4.4%提升至18.2%。

Conclusion: Double-Checker为LLM的自我批判和优化提供了有效方向，增强了其可信度和推理能力。

Abstract: While slow-thinking large language models (LLMs) exhibit reflection-like
reasoning, commonly referred to as the "aha moment:, their ability to generate
informative critiques and refine prior solutions remains limited. In this
paper, we introduce Double-Checker, a principled framework designed to enhance
the reasoning capabilities of slow-thinking LLMs by fostering explicit
self-critique and iterative refinement of their previous solutions. By
fine-tuning on our curated 1,730 self-critical instances, Double-Checker
empowers long-CoT LLMs to iteratively critique and refine their outputs during
inference until they evaluate their solutions as correct under self-generated
critiques. We validate the efficacy of Double-Checker across a comprehensive
suite of reasoning benchmarks, demonstrating that iterative self-critique
significantly enhances the reasoning capabilities of long-CoT LLMs. Notably,
our Double-Checker increases the pass@1 performance on challenging AIME
benchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These
results highlight a promising direction for developing more trustworthy and
effective LLMs capable of structured self-critique.

</details>


### [26] [Small Encoders Can Rival Large Decoders in Detecting Groundedness](https://arxiv.org/abs/2506.21288)
*Istabrak Abbes,Gabriele Prato,Quentin Fournier,Fernando Rodriguez,Alaa Boukhary,Adam Elwood,Sarath Chandar*

Key words: 大语言模型, 上下文增强, 可靠性检测, 低延迟

TL;DR: 该研究提出了一种轻量级模型，用于检测查询是否基于上下文文档，从而在LLMs生成答案前减少资源消耗和延迟。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: LLMs在上下文信息不足时会产生不可靠的回答，影响事实一致性和可信度，因此需要一种有效的检测机制。

Method: 使用轻量级编码器模型（如RoBERTa和NomicBERT）在精选数据集上进行微调，检测查询是否基于上下文。

Result: 这些模型的准确率与最先进的LLMs相当，同时大幅降低了推理延迟。

Conclusion: 该方法能高效解决LLMs的groundedness问题，提升可信度和资源效率。

Abstract: Augmenting large language models (LLMs) with external context significantly
improves their performance in natural language processing (NLP) tasks. However,
LLMs struggle to answer queries reliably when the provided context lacks
information, often resorting to ungrounded speculation or internal knowledge.
Groundedness - generating responses strictly supported by the context - is
essential for ensuring factual consistency and trustworthiness. This study
focuses on detecting whether a given query is grounded in a document provided
in context before the costly answer generation by LLMs. Such a detection
mechanism can significantly reduce both inference time and resource
consumption. We show that lightweight, task specific encoder models such as
RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy
comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in
groundedness detection while reducing inference latency by orders of magnitude.
The code is available at : https://github.com/chandarlab/Hallucinate-less

</details>


### [27] [Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models](https://arxiv.org/abs/2506.21294)
*Bram Willemsen,Gabriel Skantze*

Key words: 指代表达式, 自回归语言模型, 视觉对话, 多模态

TL;DR: 本文研究了一种仅基于文本的自回归语言模型方法，用于从视觉对话中提取指代表达式，发现即使使用中等规模的模型和小数据集，纯文本方法也有效，但问题本质上是多模态的。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究仅通过语言上下文是否能有效检测视觉对话中具有可感知指代的提及。

Method: 使用预训练的大型语言模型（LLM），通过下一个标记预测来标注对话中的提及范围。

Result: 结果表明，即使使用中等规模的LLM和小数据集，纯文本方法也能有效完成任务。

Conclusion: 任务本质上是多模态的，单模态方法存在局限性。

Abstract: In this paper, we explore the use of a text-only, autoregressive language
modeling approach for the extraction of referring expressions from visually
grounded dialogue. More specifically, the aim is to investigate the extent to
which the linguistic context alone can inform the detection of mentions that
have a (visually perceivable) referent in the visual context of the
conversation. To this end, we adapt a pretrained large language model (LLM) to
perform a relatively course-grained annotation of mention spans in unfolding
conversations by demarcating mention span boundaries in text via next-token
prediction. Our findings indicate that even when using a moderately sized LLM,
relatively small datasets, and parameter-efficient fine-tuning, a text-only
approach can be effective, highlighting the relative importance of the
linguistic context for this task. Nevertheless, we argue that the task
represents an inherently multimodal problem and discuss limitations fundamental
to unimodal approaches.

</details>


### [28] [Structuralist Approach to AI Literary Criticism: Leveraging Greimas Semiotic Square for Large Language Models](https://arxiv.org/abs/2506.21360)
*Fangzhou Dong,Yifan Zeng,Yingpeng Sang,Hong Shen*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) excel in understanding and generating text but
struggle with providing professional literary criticism for works with profound
thoughts and complex narratives. This paper proposes GLASS (Greimas Literary
Analysis via Semiotic Square), a structured analytical framework based on
Greimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth
literary analysis. GLASS facilitates the rapid dissection of narrative
structures and deep meanings in narrative works. We propose the first dataset
for GSS-based literary criticism, featuring detailed analyses of 48 works. Then
we propose quantitative metrics for GSS-based literary criticism using the
LLM-as-a-judge paradigm. Our framework's results, compared with expert
criticism across multiple works and LLMs, show high performance. Finally, we
applied GLASS to 39 classic works, producing original and high-quality analyses
that address existing research gaps. This research provides an AI-based tool
for literary research and education, offering insights into the cognitive
mechanisms underlying literary engagement.

</details>


### [29] [Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation](https://arxiv.org/abs/2506.21384)
*Guanting Dong,Xiaoxi Li,Yuyao Zhang,Mengjie Deng*

Key words: RAG, 检索增强生成, 开放域, 查询分解, 意图感知

TL;DR: Omni-RAG 是一个新颖的框架，旨在增强实时开放域环境下的检索增强生成（RAG）系统的鲁棒性和有效性，通过深度查询理解与分解、意图感知知识检索和重排与生成三个关键模块处理复杂和嘈杂的查询。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前 RAG 系统在处理嘈杂、模糊或包含多意图的用户查询时表现不佳，这限制了其在真实场景中的应用。Omni-RAG 旨在解决这一问题，提升 RAG 系统在复杂输入下的表现。

Method: Omni-RAG 包括三个模块：1) 深度查询理解与分解，使用 LLM 去噪和分解查询；2) 意图感知知识检索，为子查询检索并汇总结果；3) 重排与生成，通过重排优化文档选择，并由 LLM 生成最终响应。

Result: Omni-RAG 能够有效处理复杂和嘈杂的查询，填补了当前 RAG 能力与真实应用需求之间的差距。

Conclusion: Omni-RAG 通过模块化设计提高了 RAG 系统在真实场景中的鲁棒性和生成质量，为未来的研究提供了新方向。

Abstract: Real-world live retrieval-augmented generation (RAG) systems face significant
challenges when processing user queries that are often noisy, ambiguous, and
contain multiple intents. While RAG enhances large language models (LLMs) with
external knowledge, current systems typically struggle with such complex
inputs, as they are often trained or evaluated on cleaner data. This paper
introduces Omni-RAG, a novel framework designed to improve the robustness and
effectiveness of RAG systems in live, open-domain settings. Omni-RAG employs
LLM-assisted query understanding to preprocess user inputs through three key
modules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs
with tailored prompts to denoise queries (e.g., correcting spelling errors) and
decompose multi-intent queries into structured sub-queries; (2) Intent-Aware
Knowledge Retrieval, which performs retrieval for each sub-query from a corpus
(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking
and Generation, where a reranker (i.e., BGE) refines document selection before
a final response is generated by an LLM (i.e., Falcon-10B) using a
chain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG
capabilities and the demands of real-world applications, such as those
highlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex
and noisy queries.

</details>


### [30] [Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection](https://arxiv.org/abs/2506.21443)
*Ali Şenol,Garima Agrawal,Huan Liu*

Key words: 概念漂移,LLM,欺诈检测,领域知识,动态平台

TL;DR: 提出了一种融合领域知识的LLM框架，用于检测动态平台上的欺诈对话和概念漂移，显著提升了准确性和鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 动态平台上语言模式变化快，概念漂移使恶意意图难以检测，需结合领域知识提升LLM的检测能力。

Method: 基于LLM的框架包含三个组件：欺诈对话检测模块、语义漂移检测单元和漂移分类模块，结合领域知识优化性能。

Result: 在假评论数据集和SEConvo数据集上，系统实现了98%的分类准确率，显著优于零样本基线。

Conclusion: 领域知识和漂移感知的引入显著提升了高风险NLP应用的性能、可解释性和鲁棒性。

Abstract: Detecting deceptive conversations on dynamic platforms is increasingly
difficult due to evolving language patterns and Concept Drift (CD)-i.e.,
semantic or topical shifts that alter the context or intent of interactions
over time. These shifts can obscure malicious intent or mimic normal dialogue,
making accurate classification challenging. While Large Language Models (LLMs)
show strong performance in natural language tasks, they often struggle with
contextual ambiguity and hallucinations in risk-sensitive scenarios. To address
these challenges, we present a Domain Knowledge (DK)-Enhanced LLM framework
that integrates pretrained LLMs with structured, task-specific insights to
perform fraud and concept drift detection. The proposed architecture consists
of three main components: (1) a DK-LLM module to detect fake or deceptive
conversations; (2) a drift detection unit (OCDD) to determine whether a
semantic shift has occurred; and (3) a second DK-LLM module to classify the
drift as either benign or fraudulent. We first validate the value of domain
knowledge using a fake review dataset and then apply our full framework to
SEConvo, a multiturn dialogue dataset that includes various types of fraud and
spam attacks. Results show that our system detects fake conversations with high
accuracy and effectively classifies the nature of drift. Guided by structured
prompts, the LLaMA-based implementation achieves 98% classification accuracy.
Comparative studies against zero-shot baselines demonstrate that incorporating
domain knowledge and drift awareness significantly improves performance,
interpretability, and robustness in high-stakes NLP applications.

</details>


### [31] [Text2Cypher Across Languages: Evaluating Foundational Models Beyond English](https://arxiv.org/abs/2506.21445)
*Makbule Gulcin Ozsoy,William Tai*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advances in large language models have enabled natural language
interfaces that translate user questions into database queries, such as
Text2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database
accessibility, most research today focuses solely on English, with limited
evaluation in other languages. This paper investigates the performance of
foundational LLMs on the Text2Cypher task across multiple languages. We create
and release a multilingual test set by translating English questions into
Spanish and Turkish while preserving the original Cypher queries, enabling fair
cross-lingual comparison. We evaluate multiple foundational models using
standardized prompts and metrics. Our results show a consistent performance
pattern: highest on English, then Spanish, and lowest on Turkish. We attribute
this to differences in training data availability and linguistic
characteristics. Additionally, we explore the impact of translating task
prompts into Spanish and Turkish. Results show little to no change in
evaluation metrics, suggesting prompt translation has minor impact. Our
findings highlight the need for more inclusive evaluation and development in
multilingual query generation. Future work includes schema localization and
fine-tuning across diverse languages.

</details>


### [32] [Aligning Spoken Dialogue Models from User Interactions](https://arxiv.org/abs/2506.21463)
*Anne Wu,Laurent Mazaré,Neil Zeghidour,Alexandre Défossez*

Key words: 偏好对齐, 语音对话模型, 多轮对话, 实时交互, 离线微调

TL;DR: 提出了一种新颖的偏好对齐框架，用于通过用户交互改进实时对话模型的性能，特别关注语音交互的复杂性和多轮对话的动态性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的偏好学习方法主要针对文本语言模型，无法直接应用于实时语音交互的复杂场景，如打断、插话等动态行为以及无明确说话人切换的情况。

Method: 构建了一个大规模的偏好对数据集（超过15万对），标注了AI反馈，涵盖语言内容及时序上下文变化；利用离线对齐方法微调全双工自回归语音到语音模型。

Result: 实验表明，基于通用对话的反馈能显著提升语音对话模型的事实性、安全性和上下文对齐性；人类评估进一步验证了模型在多轮对话中的有效性。

Conclusion: 研究揭示了多种动态因素间的平衡校准对自然实时语音对话系统的重要性。

Abstract: We propose a novel preference alignment framework for improving spoken
dialogue models on real-time conversations from user interactions. Current
preference learning methods primarily focus on text-based language models, and
are not directly suited to the complexities of real-time speech interactions,
with richer dynamics (e.g. interruption, interjection) and no explicit
segmentation between speaker turns.We create a large-scale dataset of more than
150,000 preference pairs from raw multi-turn speech conversations, annotated
with AI feedback, to cover preferences over both linguistic content and
temporal context variations. We leverage offline alignment methods to finetune
a full-duplex autoregressive speech-to-speech model. Extensive experiments
demonstrate that feedback on generic conversations can be consistently
effective in improving spoken dialogue models to produce more factual, safer
and more contextually aligned interactions. We deploy the finetuned model and
conduct holistic human evaluations to assess the impact beyond single-turn
conversations. Our findings shed light on the importance of a well-calibrated
balance among various dynamics, crucial for natural real-time speech dialogue
systems.

</details>


### [33] [TopK Language Models](https://arxiv.org/abs/2506.21468)
*Ryosuke Takahashi,Tatsuro Inaba,Kentaro Inui,Benjamin Heinzerling*

Key words: 稀疏自编码器、变压器语言模型、TopK激活函数、模型可解释性

TL;DR: 摘要介绍了稀疏自编码器（SAE）在分析变压器语言模型中的局限性，并提出了一种改进架构TopK LM，结合了TopK激活函数，消除了后训练需求并提升了可解释性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决现有稀疏自编码器在分析语言模型激活空间时的局限性，如后训练问题、特征稳定性不足以及跨检查点比较困难。

Method: 通过在变压器架构中引入TopK激活函数，将隐藏状态直接与TopK SAE的潜在特征对应，无需后训练。

Result: TopK LM在模型大小、计算效率和可解释性之间取得平衡，保持了原始能力的同时提供了稳定的稀疏表示，支持神经元干预和详细分析。

Conclusion: TopK LM为语言模型的学习和概念表示提供了稳定且可靠的工具，有望推动模型可解释性和可控性的研究。

Abstract: Sparse autoencoders (SAEs) have become an important tool for analyzing and
interpreting the activation space of transformer-based language models (LMs).
However, SAEs suffer several shortcomings that diminish their utility and
internal validity. Since SAEs are trained post-hoc, it is unclear if the
failure to discover a particular concept is a failure on the SAE's side or due
to the underlying LM not representing this concept. This problem is exacerbated
by training conditions and architecture choices affecting which features an SAE
learns. When tracing how LMs learn concepts during training, the lack of
feature stability also makes it difficult to compare SAEs features across
different checkpoints. To address these limitations, we introduce a
modification to the transformer architecture that incorporates a TopK
activation function at chosen layers, making the model's hidden states
equivalent to the latent features of a TopK SAE. This approach eliminates the
need for post-hoc training while providing interpretability comparable to SAEs.
The resulting TopK LMs offer a favorable trade-off between model size,
computational efficiency, and interpretability. Despite this simple
architectural change, TopK LMs maintain their original capabilities while
providing robust interpretability benefits. Our experiments demonstrate that
the sparse representations learned by TopK LMs enable successful steering
through targeted neuron interventions and facilitate detailed analysis of
neuron formation processes across checkpoints and layers. These features make
TopK LMs stable and reliable tools for understanding how language models learn
and represent concepts, which we believe will significantly advance future
research on model interpretability and controllability.

</details>


### [34] [Bridging Offline and Online Reinforcement Learning for LLMs](https://arxiv.org/abs/2506.21495)
*Jack Lanchantin,Angelica Chen,Janice Lan,Xian Li,Swarnadeep Saha,Tianlu Wang,Jing Xu,Ping Yu,Weizhe Yuan,Jason E Weston,Sainbayar Sukhbaatar,Ilia Kulikov*

Key words: 强化学习, 大语言模型, 微调, 在线学习, 多任务学习

TL;DR: 研究了强化学习方法在微调大语言模型时从离线到半在线再到完全在线模式的效果，比较了不同优化目标的表现，发现在线与半在线方法表现相似且优于离线方法，并展示了多任务学习的优势。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索强化学习在不同在线程度下微调大语言模型的有效性，尤其是对可验证和不可验证任务的适应性。

Method: 比较了Direct Preference Optimization和Group Reward Policy Optimization在三种模式下的表现，分析了训练动态和超参数选择策略。

Result: 在线与半在线方法表现相似且优于离线方法，多任务学习能提升两种任务的性能。

Conclusion: 在线强化学习方法在微调大语言模型中表现优越，多任务学习是一种有效策略。

Abstract: We investigate the effectiveness of reinforcement learning methods for
finetuning large language models when transitioning from offline to semi-online
to fully online regimes for both verifiable and non-verifiable tasks. Our
experiments cover training on verifiable math as well as non-verifiable
instruction following with a set of benchmark evaluations for both. Across
these settings, we extensively compare online and semi-online Direct Preference
Optimization and Group Reward Policy Optimization objectives, and surprisingly
find similar performance and convergence between these variants, which all
strongly outperform offline methods. We provide a detailed analysis of the
training dynamics and hyperparameter selection strategies to achieve optimal
results. Finally, we show that multi-tasking with verifiable and non-verifiable
rewards jointly yields improved performance across both task types.

</details>


### [35] [Enhancing User Engagement in Socially-Driven Dialogue through Interactive LLM Alignments](https://arxiv.org/abs/2506.21497)
*Jiashuo Wang,Kaitao Song,Chunpu Xu,Changhe Song,Yang Xiao,Dongsheng Li,Lili Qiu,Wenjie Li*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Enhancing user engagement through interactions plays an essential role in
socially-driven dialogues. While prior works have optimized models to reason
over relevant knowledge or plan a dialogue act flow, the relationship between
user engagement and knowledge or dialogue acts is subtle and does not guarantee
user engagement in socially-driven dialogues. To this end, we enable
interactive LLMs to learn user engagement by leveraging signals from the future
development of conversations. Specifically, we adopt a more direct and relevant
indicator of user engagement, i.e., the user's reaction related to dialogue
intention after the interaction, as a reward to align interactive LLMs. To
achieve this, we develop a user simulator to interact with target interactive
LLMs and explore interactions between the user and the interactive LLM system
via \textit{i$\times$MCTS} (\textit{M}onte \textit{C}arlo \textit{T}ree
\textit{S}earch for \textit{i}nteraction). In this way, we collect a dataset
containing pairs of higher and lower-quality experiences using
\textit{i$\times$MCTS}, and align interactive LLMs for high-level user
engagement by direct preference optimization (DPO) accordingly. Experiments
conducted on two socially-driven dialogue scenarios (emotional support
conversations and persuasion for good) demonstrate that our method effectively
enhances user engagement in interactive LLMs.

</details>


### [36] [skLEP: A Slovak General Language Understanding Benchmark](https://arxiv.org/abs/2506.21508)
*Marek Šuppa,Andrej Ridzik,Daniel Hládek,Tomáš Javůrek,Viktória Ondrejová,Kristína Sásiková,Martin Tamajka,Marián Šimko*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this work, we introduce skLEP, the first comprehensive benchmark
specifically designed for evaluating Slovak natural language understanding
(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span
token-level, sentence-pair, and document-level challenges, thereby offering a
thorough assessment of model capabilities. To create this benchmark, we curated
new, original datasets tailored for Slovak and meticulously translated
established English NLU resources. Within this paper, we also present the first
systematic and extensive evaluation of a wide array of Slovak-specific,
multilingual, and English pre-trained language models using the skLEP tasks.
Finally, we also release the complete benchmark data, an open-source toolkit
facilitating both fine-tuning and evaluation of models, and a public
leaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering
reproducibility and drive future research in Slovak NLU.

</details>


### [37] [Potemkin Understanding in Large Language Models](https://arxiv.org/abs/2506.21521)
*Marina Mancoridis,Bec Weeks,Keyon Vafa,Sendhil Mullainathan*

Key words: 大语言模型、基准测试、波将金理解、概念表征、评估

TL;DR: 本文探讨了基于基准测试评估大语言模型（LLM）能力的合理性，提出了“波将金理解”（Potemkin understanding）的概念，即模型在测试中表现出的虚假理解。作者设计了两种量化方法，证明了这种现象在模型、任务和领域中普遍存在，并指出其反映了概念表征的内部矛盾。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 评估大语言模型能力的基准测试通常与人类测试相同（如AP考试），但模型的“理解”是否与人类一致尚不明确。本文旨在验证模型是否通过非人类方式“理解”概念，从而质疑基准测试的有效性。

Method: 1. 设计了一种专门针对三个领域的基准测试；2. 提出了一种通用方法，为波将金理解的普遍性提供下界估计。

Result: 研究发现波将金理解在模型、任务和领域中普遍存在，且这种失败不仅是理解错误，还反映了概念表征的内在矛盾。

Conclusion: 基准测试可能无法真实反映模型的“理解”能力，模型的虚假理解现象普遍存在，需重新思考评估方法。

Abstract: Large language models (LLMs) are regularly evaluated using benchmark
datasets. But what justifies making inferences about an LLM's capabilities
based on its answers to a curated set of questions? This paper first introduces
a formal framework to address this question. The key is to note that the
benchmarks used to test LLMs -- such as AP exams -- are also those used to test
people. However, this raises an implication: these benchmarks are only valid
tests if LLMs misunderstand concepts in ways that mirror human
misunderstandings. Otherwise, success on benchmarks only demonstrates potemkin
understanding: the illusion of understanding driven by answers irreconcilable
with how any human would interpret a concept. We present two procedures for
quantifying the existence of potemkins: one using a specially designed
benchmark in three domains, the other using a general procedure that provides a
lower-bound on their prevalence. We find that potemkins are ubiquitous across
models, tasks, and domains. We also find that these failures reflect not just
incorrect understanding, but deeper internal incoherence in concept
representations.

</details>


### [38] ["What's Up, Doc?": Analyzing How Users Seek Health Information in Large-Scale Conversational AI Datasets](https://arxiv.org/abs/2506.21532)
*Akshay Paruchuri,Maryam Aziz,Rohit Vartak,Ayman Ali,Best Uchehara,Xin Liu,Ishan Chatterjee,Monica Agrawal*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: People are increasingly seeking healthcare information from large language
models (LLMs) via interactive chatbots, yet the nature and inherent risks of
these conversations remain largely unexplored. In this paper, we filter
large-scale conversational AI datasets to achieve HealthChat-11K, a curated
dataset of 11K real-world conversations composed of 25K user messages. We use
HealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs
when seeking healthcare information in order to systematically study user
interactions across 21 distinct health specialties. Our analysis reveals
insights into the nature of how and why users seek health information, such as
common interactions, instances of incomplete context, affective behaviors, and
interactions (e.g., leading questions) that can induce sycophancy, underscoring
the need for improvements in the healthcare support capabilities of LLMs
deployed as conversational AI. Code and artifacts to retrieve our analyses and
combine them into a curated dataset can be found here:
https://github.com/yahskapar/HealthChat

</details>


### [39] [Data Efficacy for Language Model Training](https://arxiv.org/abs/2506.21545)
*Yalun Dai,Yangyu Huang,Xin Zhang,Wenshan Wu,Chong Li,Wenhui Lu,Shijie Cao,Li Dong,Scarlett Li*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Data is fundamental to the training of language models (LM). Recent research
has been dedicated to data efficiency, which aims to maximize performance by
selecting a minimal or optimal subset of training data. Techniques such as data
filtering, sampling, and selection play a crucial role in this area. To
complement it, we define Data Efficacy, which focuses on maximizing performance
by optimizing the organization of training data and remains relatively
underexplored. This work introduces a general paradigm, DELT, for considering
data efficacy in LM training, which highlights the significance of training
data organization. DELT comprises three components: Data Scoring, Data
Selection, and Data Ordering. Among these components, we design
Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which
considers both the learnability and quality of each data sample from the
gradient consistency perspective. We also devise Folding Ordering (FO), as a
novel instance of Data Ordering, which addresses issues such as model
forgetting and data distribution bias. Comprehensive experiments validate the
data efficacy in LM training, which demonstrates the following: Firstly,
various instances of the proposed DELT enhance LM performance to varying
degrees without increasing the data scale and model size. Secondly, among these
instances, the combination of our proposed LQS for data scoring and Folding for
data ordering achieves the most significant improvement. Lastly, data efficacy
can be achieved together with data efficiency by applying data selection.
Therefore, we believe that data efficacy is a promising foundational area in LM
training.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [40] [Progressive Size-Adaptive Federated Learning: A Comprehensive Framework for Heterogeneous Multi-Modal Data Systems](https://arxiv.org/abs/2506.20685)
*Sajid Hussain,Muhammad Sohail,Nauman Ali Khan,Naima Iltaf,Ihtesham ul Islam*

Key words: 联邦学习, 数据集大小, 多模态数据, 通信效率, 自适应框架

TL;DR: 该论文提出了一种基于数据集大小特性的自适应联邦学习框架SAFL，通过实验揭示了数据集大小和模态对联邦学习效果的影响，并展示了SAFL的高效性和准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有联邦学习方法多关注模型异质性和聚合技术，但忽视了数据集大小特性对训练动态的影响。

Method: 提出了Size-Based Adaptive Federated Learning (SAFL)框架，通过系统组织联邦学习，利用数据集大小特性和多模态数据。

Result: 实验表明：1) 最佳数据集大小范围为1000-1500样本；2) 结构化数据表现优于非结构化数据；3) 数据集过大时性能下降。SAFL平均准确率达87.68%，通信效率高。

Conclusion: SAFL填补了数据特性如何驱动联邦学习策略的研究空白，为实际应用提供了理论和实践指导。

Abstract: Federated Learning (FL) has emerged as a transformative paradigm for
distributed machine learning while preserving data privacy. However, existing
approaches predominantly focus on model heterogeneity and aggregation
techniques, largely overlooking the fundamental impact of dataset size
characteristics on federated training dynamics. This paper introduces
Size-Based Adaptive Federated Learning (SAFL), a novel progressive training
framework that systematically organizes federated learning based on dataset
size characteristics across heterogeneous multi-modal data. Our comprehensive
experimental evaluation across 13 diverse datasets spanning 7 modalities
(vision, text, time series, audio, sensor, medical vision, and multimodal)
reveals critical insights: 1) an optimal dataset size range of 1000-1500
samples for federated learning effectiveness; 2) a clear modality performance
hierarchy with structured data (time series, sensor) significantly
outperforming unstructured data (text, multimodal); and 3) systematic
performance degradation for large datasets exceeding 2000 samples. SAFL
achieves an average accuracy of 87.68% across all datasets, with structured
data modalities reaching 99%+ accuracy. The framework demonstrates superior
communication efficiency, reducing total data transfer to 7.38 GB across 558
communications while maintaining high performance. Our real-time monitoring
framework provides unprecedented insights into system resource utilization,
network efficiency, and training dynamics. This work fills critical gaps in
understanding how data characteristics should drive federated learning
strategies, providing both theoretical insights and practical guidance for
real-world FL deployments in neural network and learning systems.

</details>


### [41] [E-ABIN: an Explainable module for Anomaly detection in BIological Networks](https://arxiv.org/abs/2506.20693)
*Ugo Lomoio,Tommaso Mazza,Pierangelo Veltri,Pietro Hiram Guzzi*

Key words: 异常检测, 生物网络, 可解释性, 机器学习, 图深度学习

TL;DR: E-ABIN是一个用于生物网络中异常检测的可解释性框架，结合传统机器学习与图深度学习技术，提供用户友好平台，应用于基因表达或甲基化网络。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大规模组学数据的增加需要能够处理复杂基因表达数据并提供可解释结果的框架。现有方法多限于单一数据集且缺乏图形界面。

Method: E-ABIN结合支持向量机、随机森林、图自动编码器（GAEs）和图对抗属性网络（GAANs），提供高预测准确性和可解释性。

Result: 在膀胱癌和乳糜泻案例中，E-ABIN成功发现生物学相关异常并为疾病机制提供见解。

Conclusion: E-ABIN是一个通用、可解释的异常检测框架，适合复杂生物网络分析。

Abstract: The increasing availability of large-scale omics data calls for robust
analytical frameworks capable of handling complex gene expression datasets
while offering interpretable results. Recent advances in artificial
intelligence have enabled the identification of aberrant molecular patterns
distinguishing disease states from healthy controls. Coupled with improvements
in model interpretability, these tools now support the identification of genes
potentially driving disease phenotypes. However, current approaches to gene
anomaly detection often remain limited to single datasets and lack accessible
graphical interfaces. Here, we introduce E-ABIN, a general-purpose, explainable
framework for Anomaly detection in Biological Networks. E-ABIN combines
classical machine learning and graph-based deep learning techniques within a
unified, user-friendly platform, enabling the detection and interpretation of
anomalies from gene expression or methylation-derived networks. By integrating
algorithms such as Support Vector Machines, Random Forests, Graph Autoencoders
(GAEs), and Graph Adversarial Attributed Networks (GAANs), E-ABIN ensures a
high predictive accuracy while maintaining interpretability. We demonstrate the
utility of E-ABIN through case studies of bladder cancer and coeliac disease,
where it effectively uncovers biologically relevant anomalies and offers
insights into disease mechanisms.

</details>


### [42] [On Context-Content Uncertainty Principle](https://arxiv.org/abs/2506.20699)
*Xin Li*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Context-Content Uncertainty Principle (CCUP) proposes that inference
under uncertainty is governed by an entropy asymmetry between context and
content: high-entropy contexts must be interpreted through alignment with
low-entropy, structured content. In this paper, we develop a layered
computational framework that derives operational principles from this
foundational asymmetry. At the base level, CCUP formalizes inference as
directional entropy minimization, establishing a variational gradient that
favors content-first structuring. Building upon this, we identify four
hierarchical layers of operational principles: (\textbf{L1}) \emph{Core
Inference Constraints}, including structure-before-specificity, asymmetric
inference flow, cycle-consistent bootstrapping, and conditional compression,
all shown to be mutually reducible; (\textbf{L2}) \emph{Resource Allocation
Principles}, such as precision-weighted attention, asymmetric learning rates,
and attractor-based memory encoding; (\textbf{L3}) \emph{Temporal Bootstrapping
Dynamics}, which organize learning over time via structure-guided curricula;
and (\textbf{L4}) \emph{Spatial Hierarchical Composition}, which integrates
these mechanisms into self-organizing cycles of memory, inference, and
planning. We present formal equivalence theorems, a dependency lattice among
principles, and computational simulations demonstrating the efficiency gains of
CCUP-aligned inference. This work provides a unified theoretical foundation for
understanding how brains and machines minimize uncertainty through recursive
structure-specificity alignment. The brain is not just an inference machine. It
is a cycle-consistent entropy gradient resolver, aligning structure and
specificity via path-dependent, content-seeded simulation.

</details>


### [43] [Diffusion Tree Sampling: Scalable inference-time alignment of diffusion models](https://arxiv.org/abs/2506.20701)
*Vineet Jain,Kusha Sareen,Mohammad Pedramfar,Siamak Ravanbakhsh*

Key words: 扩散模型,推理时对齐,树采样,计算效率

TL;DR: 本文提出了一种基于树的方法（DTS和DTS*），通过在扩散过程中重用过去的计算来改进推理时对齐问题，显著降低了计算成本。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 预训练扩散模型在推理时适应新目标是一个尚未解决的问题，现有方法在高噪声水平下估计不准确且未重用过去信息。

Method: 引入树采样方法（DTS和DTS*），通过传播终端奖励并迭代优化估计值，实现目标分布的高效采样。

Result: 在MNIST和CIFAR-10上，DTS以10倍更少的计算量达到最佳基线的FID；在文本生成和语言任务中，DTS*以5倍更少的计算量匹配最优结果。

Conclusion: 通过重用过去生成的信息，该方法提供了一种可扩展的推理时对齐方案，为扩散模型的优化提供了新思路。

Abstract: Adapting a pretrained diffusion model to new objectives at inference time
remains an open problem in generative modeling. Existing steering methods
suffer from inaccurate value estimation, especially at high noise levels, which
biases guidance. Moreover, information from past runs is not reused to improve
sample quality, resulting in inefficient use of compute. Inspired by the
success of Monte Carlo Tree Search, we address these limitations by casting
inference-time alignment as a search problem that reuses past computations. We
introduce a tree-based approach that samples from the reward-aligned target
density by propagating terminal rewards back through the diffusion chain and
iteratively refining value estimates with each additional generation. Our
proposed method, Diffusion Tree Sampling (DTS), produces asymptotically exact
samples from the target distribution in the limit of infinite rollouts, and its
greedy variant, Diffusion Tree Search (DTS$^\star$), performs a global search
for high reward samples. On MNIST and CIFAR-10 class-conditional generation,
DTS matches the FID of the best-performing baseline with up to $10\times$ less
compute. In text-to-image generation and language completion tasks, DTS$^\star$
effectively searches for high reward samples that match best-of-N with up to
$5\times$ less compute. By reusing information from previous generations, we
get an anytime algorithm that turns additional compute into steadily better
samples, providing a scalable approach for inference-time alignment of
diffusion models.

</details>


### [44] [On Convolutions, Intrinsic Dimension, and Diffusion Models](https://arxiv.org/abs/2506.20705)
*Kin Kwan Leung,Rasa Hosseinzadeh,Gabriel Loaiza-Ganem*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The manifold hypothesis asserts that data of interest in high-dimensional
ambient spaces, such as image data, lies on unknown low-dimensional
submanifolds. Diffusion models (DMs) -- which operate by convolving data with
progressively larger amounts of Gaussian noise and then learning to revert this
process -- have risen to prominence as the most performant generative models,
and are known to be able to learn distributions with low-dimensional support.
For a given datum in one of these submanifolds, we should thus intuitively
expect DMs to have implicitly learned its corresponding local intrinsic
dimension (LID), i.e. the dimension of the submanifold it belongs to. Kamkari
et al. (2024b) recently showed that this is indeed the case by linking this LID
to the rate of change of the log marginal densities of the DM with respect to
the amount of added noise, resulting in an LID estimator known as FLIPD. LID
estimators such as FLIPD have a plethora of uses, among others they quantify
the complexity of a given datum, and can be used to detect outliers,
adversarial examples and AI-generated text. FLIPD achieves state-of-the-art
performance at LID estimation, yet its theoretical underpinnings are incomplete
since Kamkari et al. (2024b) only proved its correctness under the highly
unrealistic assumption of affine submanifolds. In this work we bridge this gap
by formally proving the correctness of FLIPD under realistic assumptions.
Additionally, we show that an analogous result holds when Gaussian convolutions
are replaced with uniform ones, and discuss the relevance of this result.

</details>


### [45] [Test-time Scaling Techniques in Theoretical Physics -- A Comparison of Methods on the TPBench Dataset](https://arxiv.org/abs/2506.20729)
*Zhiqi Gao,Tianyi Li,Yurii Kvasiuk,Sai Chaitanya Tadepalli,Maja Rudolph,Daniel J. H. Chung,Frederic Sala,Moritz Münchmeyer*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) have shown strong capabilities in complex
reasoning, and test-time scaling techniques can enhance their performance with
comparably low cost. Many of these methods have been developed and evaluated on
mathematical reasoning benchmarks such as AIME. This paper investigates whether
the lessons learned from these benchmarks generalize to the domain of advanced
theoretical physics. We evaluate a range of common test-time scaling methods on
the TPBench physics dataset and compare their effectiveness with results on
AIME. To better leverage the structure of physics problems, we develop a novel,
symbolic weak-verifier framework to improve parallel scaling results. Our
empirical results demonstrate that this method significantly outperforms
existing test-time scaling approaches on TPBench. We also evaluate our method
on AIME, confirming its effectiveness in solving advanced mathematical
problems. Our findings highlight the power of step-wise symbolic verification
for tackling complex scientific problems.

</details>


### [46] [A Survey of AI for Materials Science: Foundation Models, LLM Agents, Datasets, and Tools](https://arxiv.org/abs/2506.20743)
*Minh-Hao Van,Prateek Verma,Chen Zhao,Xintao Wu*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Foundation models (FMs) are catalyzing a transformative shift in materials
science (MatSci) by enabling scalable, general-purpose, and multimodal AI
systems for scientific discovery. Unlike traditional machine learning models,
which are typically narrow in scope and require task-specific engineering, FMs
offer cross-domain generalization and exhibit emergent capabilities. Their
versatility is especially well-suited to materials science, where research
challenges span diverse data types and scales. This survey provides a
comprehensive overview of foundation models, agentic systems, datasets, and
computational tools supporting this growing field. We introduce a task-driven
taxonomy encompassing six broad application areas: data extraction,
interpretation and Q\&A; atomistic simulation; property prediction; materials
structure, design and discovery; process planning, discovery, and optimization;
and multiscale modeling. We discuss recent advances in both unimodal and
multimodal FMs, as well as emerging large language model (LLM) agents.
Furthermore, we review standardized datasets, open-source tools, and autonomous
experimental platforms that collectively fuel the development and integration
of FMs into research workflows. We assess the early successes of foundation
models and identify persistent limitations, including challenges in
generalizability, interpretability, data imbalance, safety concerns, and
limited multimodal fusion. Finally, we articulate future research directions
centered on scalable pretraining, continual learning, data governance, and
trustworthiness.

</details>


### [47] [Multiple Streams of Relation Extraction: Enriching and Recalling in Transformers](https://arxiv.org/abs/2506.20746)
*Todd Nief,David Reber,Sean Richardson,Ari Holtzman*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: When an LLM learns a relation during finetuning (e.g., new movie releases,
corporate mergers, etc.), where does this information go? Is it extracted when
the model processes an entity, recalled just-in-time before a prediction, or
are there multiple separate heuristics? Existing localization approaches (e.g.
activation patching) are ill-suited for this analysis because they tend to
replace parts of the residual stream, potentially deleting information. To fill
this gap, we propose dynamic weight-grafting between fine-tuned and pre-trained
language models to show that fine-tuned language models both (1) extract
relation information learned during finetuning while processing entities and
(2) ``recall" this information in later layers while generating predictions. In
some cases, models need both of these pathways to correctly generate finetuned
information while, in other cases, a single ``enrichment" or ``recall" pathway
alone is sufficient. We examine the necessity and sufficiency of these
information pathways, examining what layers they occur at, how much redundancy
they exhibit, and which model components are involved -- finding that the
``recall" pathway occurs via both task-specific attention mechanisms and a
relation extraction step in the output of the attention and the feedforward
networks at the final layers before next token prediction.

</details>


### [48] [Characterization and Mitigation of Training Instabilities in Microscaling Formats](https://arxiv.org/abs/2506.20752)
*Huangyuan Su,Mujin Kwun,Stephanie Gil,Sham Kakade,Nikhil Anand*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Training large language models is an expensive, compute-bound process that
must be repeated as models scale, algorithms improve, and new data is
collected. To address this, next-generation hardware accelerators increasingly
support lower-precision arithmetic formats, such as the Microscaling (MX)
formats introduced in NVIDIA's Blackwell architecture. These formats use a
shared scale within blocks of parameters to extend representable range and
perform forward/backward GEMM operations in reduced precision for efficiency
gains. In this work, we investigate the challenges and viability of
block-scaled precision formats during model training. Across nearly one
thousand language models trained from scratch -- spanning compute budgets from
$2 \times 10^{17}$ to $4.8 \times 10^{19}$ FLOPs and sweeping over a broad
range of weight-activation precision combinations -- we consistently observe
that training in MX formats exhibits sharp, stochastic instabilities in the
loss, particularly at larger compute scales. To explain this phenomenon, we
conduct controlled experiments and ablations on a smaller proxy model that
exhibits similar behavior as the language model, sweeping across architectural
settings, hyperparameters, and precision formats. These experiments motivate a
simple model in which multiplicative gradient bias introduced by the
quantization of layer-norm affine parameters and a small fraction of
activations can trigger runaway divergence. Through \emph{in situ} intervention
experiments on our proxy model, we demonstrate that instabilities can be
averted or delayed by modifying precision schemes mid-training. Guided by these
findings, we evaluate stabilization strategies in the LLM setting and show that
certain hybrid configurations recover performance competitive with
full-precision training. We release our code at
https://github.com/Hither1/systems-scaling.

</details>


### [49] [Stochastic and Non-local Closure Modeling for Nonlinear Dynamical Systems via Latent Score-based Generative Models](https://arxiv.org/abs/2506.20771)
*Xinghao Dong,Huchen Yang,Jin-Long Wu*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose a latent score-based generative AI framework for learning
stochastic, non-local closure models and constitutive laws in nonlinear
dynamical systems of computational mechanics. This work addresses a key
challenge of modeling complex multiscale dynamical systems without a clear
scale separation, for which numerically resolving all scales is prohibitively
expensive, e.g., for engineering turbulent flows. While classical closure
modeling methods leverage domain knowledge to approximate subgrid-scale
phenomena, their deterministic and local assumptions can be too restrictive in
regimes lacking a clear scale separation. Recent developments of
diffusion-based stochastic models have shown promise in the context of closure
modeling, but their prohibitive computational inference cost limits practical
applications for many real-world applications. This work addresses this
limitation by jointly training convolutional autoencoders with conditional
diffusion models in the latent spaces, significantly reducing the
dimensionality of the sampling process while preserving essential physical
characteristics. Numerical results demonstrate that the joint training approach
helps discover a proper latent space that not only guarantees small
reconstruction errors but also ensures good performance of the diffusion model
in the latent space. When integrated into numerical simulations, the proposed
stochastic modeling framework via latent conditional diffusion models achieves
significant computational acceleration while maintaining comparable predictive
accuracy to standard diffusion models in physical spaces.

</details>


### [50] [Stochastic Parameter Decomposition](https://arxiv.org/abs/2506.20790)
*Lucius Bushnaq,Dan Braun,Lee Sharkey*

Key words: 随机参数分解, 线性参数分解, 机制可解释性, 神经网络分析

TL;DR: 本文提出了一种新的随机参数分解（SPD）方法，解决了现有方法（APD）计算成本高和对超参数敏感的问题，并在更大、更复杂的模型中展示了其可扩展性和稳健性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前线性参数分解方法（如APD）存在计算成本高和对超参数敏感的缺点，限制了其在更大模型中的应用。本文旨在提出一种更高效、更稳健的分解方法。

Method: 引入随机参数分解（SPD）方法，通过改进计算效率和超参数鲁棒性，解决了APD的局限性。

Result: SPD在较大和较复杂的模型中表现优于APD，避免了参数收缩问题，并能更好地识别玩具模型中的真实机制。

Conclusion: SPD通过结合因果中介分析和网络分解方法，为机制可解释性研究开辟了新方向，并推动了线性参数分解方法在大模型中的应用。

Abstract: A key step in reverse engineering neural networks is to decompose them into
simpler parts that can be studied in relative isolation. Linear parameter
decomposition -- a framework that has been proposed to resolve several issues
with current decomposition methods -- decomposes neural network parameters into
a sum of sparsely used vectors in parameter space. However, the current main
method in this framework, Attribution-based Parameter Decomposition (APD), is
impractical on account of its computational cost and sensitivity to
hyperparameters. In this work, we introduce \textit{Stochastic Parameter
Decomposition} (SPD), a method that is more scalable and robust to
hyperparameters than APD, which we demonstrate by decomposing models that are
slightly larger and more complex than was possible to decompose with APD. We
also show that SPD avoids other issues, such as shrinkage of the learned
parameters, and better identifies ground truth mechanisms in toy models. By
bridging causal mediation analysis and network decomposition methods, this
demonstration opens up new research possibilities in mechanistic
interpretability by removing barriers to scaling linear parameter decomposition
methods to larger models. We release a library for running SPD and reproducing
our experiments at https://github.com/goodfire-ai/spd.

</details>


### [51] [GPU Kernel Scientist: An LLM-Driven Framework for Iterative Kernel Optimization](https://arxiv.org/abs/2506.20807)
*Martin Andrews,Sam Witteveen*

Key words: GPU优化, LLM, AMD MI300, 自动化, 多阶段进化

TL;DR: 自动化方法利用LLM多阶段进化流程优化GPU内核，针对AMD MI300架构，克服传统开发辅助不足的问题，展示LLM驱动代理在资源受限或快速变化硬件环境中的潜力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对GPU内核优化的复杂性，尤其是在新架构或文档不足的情况下，传统方法效率低下，需要一种自动化工具来加速和简化流程。

Method: 采用LLM驱动的多阶段进化过程，包括选择基础代码、生成优化假设并自动实验，结合时间数据反馈迭代优化。

Result: 虽然定量结果未公布，但方法在AMD MI300架构上展示了潜力，能够高效优化内核。

Conclusion: LLM驱动的“GPU Kernel Scientist”为资源受限或快速变化的硬件环境提供了一种高效的优化工具。

Abstract: Optimizing GPU kernels for high performance is a complex task, often
demanding deep architectural knowledge, extensive profiling, and iterative
experimentation. This challenge is amplified when targeting newer or
less-documented GPU architectures where traditional development aids are
scarce. This paper introduces an LLM-powered "GPU Kernel Scientist," an
automated methodology for iteratively refining accelerator kernels.
  Our methodology employs LLMs in a multi-stage, evolutionary process: (a)
strategically selecting promising prior code versions as a basis for new
iterations; (b) generating hypotheses for optimization experiments, based on
existing code and assimilated knowledge from general GPU literature; and (c)
autonomously implementing these experiments through code modification and
subsequent submission to an external evaluation system, using only observed
timing data as performance feedback. We detail how this approach navigates the
challenges of the AMD MI300 target architecture and leverages LLMs to
compensate for limited domain-specific human expertise.
  Since quantitative results from an ongoing performance competition were
embargoed on paper submission date, we present the architectural design,
operational workflow, and qualitative insights, highlighting the potential of
LLM-driven agents to democratise and accelerate GPU kernel optimization,
especially in resource-constrained or rapidly evolving hardware environments.

</details>


### [52] [FINN-GL: Generalized Mixed-Precision Extensions for FPGA-Accelerated LSTMs](https://arxiv.org/abs/2506.20810)
*Shashwat Khandelwal,Jakoba Petri-Koenig,Thomas B. Preußer,Michaela Blott,Shreejith Shanker*

Key words: LSTM, FPGA, FINN, ONNX, 量化, 加速器

TL;DR: 该论文提出了一个通用的LSTM在FPGA上部署的工具流程，利用FINN框架和ONNX的Scan操作符，支持混合量化和功能验证，并在中价位股票预测任务中验证了其性能与资源消耗的平衡。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的LSTM加速工具主要针对前馈网络，且LSTM加速通常需要完全自定义实现，难以在资源受限环境中实时部署。因此，需要一种通用的方法来高效部署RNN加速器。

Method: 利用FINN框架和ONNX的Scan操作符模型化LSTM的循环计算，引入自定义变换将量化ONNX计算图映射到FINN编译器和Vitis HLS的硬件块。

Result: 生成的量化ConvLSTM加速器在性能和资源消耗之间取得平衡，同时保持了高推理精度，甚至优于现有最先进模型。

Conclusion: 提出的工具流程具有通用性，为FPGA上资源高效的RNN加速器设计铺平了道路。

Abstract: Recurrent neural networks (RNNs), particularly LSTMs, are effective for
time-series tasks like sentiment analysis and short-term stock prediction.
However, their computational complexity poses challenges for real-time
deployment in resource constrained environments. While FPGAs offer a promising
platform for energy-efficient AI acceleration, existing tools mainly target
feed-forward networks, and LSTM acceleration typically requires full custom
implementation. In this paper, we address this gap by leveraging the
open-source and extensible FINN framework to enable the generalized deployment
of LSTMs on FPGAs. Specifically, we leverage the Scan operator from the Open
Neural Network Exchange (ONNX) specification to model the recurrent nature of
LSTM computations, enabling support for mixed quantisation within them and
functional verification of LSTM-based models. Furthermore, we introduce custom
transformations within the FINN compiler to map the quantised ONNX computation
graph to hardware blocks from the HLS kernel library of the FINN compiler and
Vitis HLS. We validate the proposed tool-flow by training a quantised ConvLSTM
model for a mid-price stock prediction task using the widely used dataset and
generating a corresponding hardware IP of the model using our flow, targeting
the XCZU7EV device. We show that the generated quantised ConvLSTM accelerator
through our flow achieves a balance between performance (latency) and resource
consumption, while matching (or bettering) inference accuracy of
state-of-the-art models with reduced precision. We believe that the
generalisable nature of the proposed flow will pave the way for
resource-efficient RNN accelerator designs on FPGAs.

</details>


### [53] [Divide, Specialize, and Route: A New Approach to Efficient Ensemble Learning](https://arxiv.org/abs/2506.20814)
*Jakub Piwko,Jędrzej Ruciński,Dawid Płudowski,Antoni Zajko,Patryzja Żak,Mateusz Zacharecki,Anna Kozak,Katarzyna Woźnica*

Key words: 集成学习,二进制分类,数据集复杂性,计算效率,可解释性

TL;DR: Hellsemble是一种新颖且可解释的集成学习框架，通过动态分区数据集的复杂性来提高分类性能，同时保持计算效率和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统集成学习方法（如bagging、boosting和DES）在高计算成本和异构数据分布适应性方面存在局限性，亟需一种更高效且适应性强的方法。

Method: Hellsemble通过逐步将数据集按难度分区，利用路由器模型将新实例分配给最适合的基学习器，每个基学习器专注于处理递增难度的子集。

Result: 实验表明，Hellsemble在OpenML-CC18和Tabzilla基准测试中优于传统集成方法。

Conclusion: 利用实例级难度构建的集成系统是高效且鲁棒的方向。

Abstract: Ensemble learning has proven effective in boosting predictive performance,
but traditional methods such as bagging, boosting, and dynamic ensemble
selection (DES) suffer from high computational cost and limited adaptability to
heterogeneous data distributions. To address these limitations, we propose
Hellsemble, a novel and interpretable ensemble framework for binary
classification that leverages dataset complexity during both training and
inference. Hellsemble incrementally partitions the dataset into circles of
difficulty by iteratively passing misclassified instances from simpler models
to subsequent ones, forming a committee of specialised base learners. Each
model is trained on increasingly challenging subsets, while a separate router
model learns to assign new instances to the most suitable base model based on
inferred difficulty. Hellsemble achieves strong classification accuracy while
maintaining computational efficiency and interpretability. Experimental results
on OpenML-CC18 and Tabzilla benchmarks demonstrate that Hellsemble often
outperforms classical ensemble methods. Our findings suggest that embracing
instance-level difficulty offers a promising direction for constructing
efficient and robust ensemble systems.

</details>


### [54] [Universal and Efficient Detection of Adversarial Data through Nonuniform Impact on Network Layers](https://arxiv.org/abs/2506.20816)
*Furkan Mumcu,Yasin Yilmaz*

Key words: 深度学习, 对抗攻击, 检测方法, DNN, 实时处理

TL;DR: 该论文提出了一种新型的通用且高效的方法，用于检测对抗样本，通过分析攻击对不同DNN层的影响程度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有对抗攻击防御方法要么效果不佳，要么计算效率低，难以实时处理，因此需要一种更实用的检测方法。

Method: 训练了一个轻量级回归模型，通过预测深层特征与早期层特征之间的误差来检测对抗样本。

Result: 该方法高效、适用于实时处理，且兼容任何DNN架构，适用于图像、视频和音频等多个领域。

Conclusion: 提出的检测方法在理论和实验中均表现出高度有效性，适用于多种应用场景。

Abstract: Deep Neural Networks (DNNs) are notoriously vulnerable to adversarial input
designs with limited noise budgets. While numerous successful attacks with
subtle modifications to original input have been proposed, defense techniques
against these attacks are relatively understudied. Existing defense approaches
either focus on improving DNN robustness by negating the effects of
perturbations or use a secondary model to detect adversarial data. Although
equally important, the attack detection approach, which is studied in this
work, provides a more practical defense compared to the robustness approach. We
show that the existing detection methods are either ineffective against the
state-of-the-art attack techniques or computationally inefficient for real-time
processing. We propose a novel universal and efficient method to detect
adversarial examples by analyzing the varying degrees of impact of attacks on
different DNN layers. {Our method trains a lightweight regression model that
predicts deeper-layer features from early-layer features, and uses the
prediction error to detect adversarial samples.} Through theoretical arguments
and extensive experiments, we demonstrate that our detection method is highly
effective, computationally efficient for real-time processing, compatible with
any DNN architecture, and applicable across different domains, such as image,
video, and audio.

</details>


### [55] [Demystifying Distributed Training of Graph Neural Networks for Link Prediction](https://arxiv.org/abs/2506.20818)
*Xin Huang,Chul-Ho Lee*

Key words: 图神经网络,链接预测,分布式训练,图稀疏化,SpLPG

TL;DR: 本文探讨了分布式GNN框架在链接预测任务中的性能下降问题，并提出SpLPG方法，通过图稀疏化减少通信成本，同时保持预测准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有分布式GNN框架多针对节点分类优化，链接预测性能未充分探索，尤其是因图分区导致的信息损失和负采样方式。

Method: 提出SpLPG方法，利用图稀疏化解决性能下降问题，降低通信成本。

Result: 在多个真实数据集上，SpLPG减少约80%通信开销，同时基本保持链接预测准确性。

Conclusion: SpLPG有效解决了分布式GNN链接预测的性能和通信效率问题。

Abstract: Graph neural networks (GNNs) are powerful tools for solving graph-related
problems. Distributed GNN frameworks and systems enhance the scalability of
GNNs and accelerate model training, yet most are optimized for node
classification. Their performance on link prediction remains underexplored.
This paper demystifies distributed training of GNNs for link prediction by
investigating the issue of performance degradation when each worker trains a
GNN on its assigned partitioned subgraph without having access to the entire
graph. We discover that the main sources of the issue come from not only the
information loss caused by graph partitioning but also the ways of drawing
negative samples during model training. While sharing the complete graph
information with each worker resolves the issue and preserves link prediction
accuracy, it incurs a high communication cost. We propose SpLPG, which
effectively leverages graph sparsification to mitigate the issue of performance
degradation at a reduced communication cost. Experiment results on several
public real-world datasets demonstrate the effectiveness of SpLPG, which
reduces the communication overhead by up to about 80% while mostly preserving
link prediction accuracy.

</details>


### [56] [Learning-Based Resource Management in Integrated Sensing and Communication Systems](https://arxiv.org/abs/2506.20849)
*Ziyang Lu,M. Cenk Gursoy,Chilukuri K. Mohan,Pramod K. Varshney*

Key words: 集成传感与通信；资源分配；约束深度强化学习；动态优化

TL;DR: 提出了一种基于约束深度强化学习（CDRL）的方法，用于集成传感与通信系统中时间资源分配，以优化目标跟踪和通信质量。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决集成传感与通信系统中时间资源分配的动态优化问题，提升目标通信质量。

Method: 采用约束深度强化学习（CDRL）方法，在时间预算约束下优化雷达跟踪与通信的资源分配。

Result: 数值结果表明，CDRL框架在动态环境中能够有效提升通信质量，同时满足时间约束。

Conclusion: CDRL方法为集成传感与通信系统提供了高效的时间资源分配解决方案。

Abstract: In this paper, we tackle the task of adaptive time allocation in integrated
sensing and communication systems equipped with radar and communication units.
The dual-functional radar-communication system's task involves allocating dwell
times for tracking multiple targets and utilizing the remaining time for data
transmission towards estimated target locations. We introduce a novel
constrained deep reinforcement learning (CDRL) approach, designed to optimize
resource allocation between tracking and communication under time budget
constraints, thereby enhancing target communication quality. Our numerical
results demonstrate the efficiency of our proposed CDRL framework, confirming
its ability to maximize communication quality in highly dynamic environments
while adhering to time constraints.

</details>


### [57] [Multi-Objective Reinforcement Learning for Cognitive Radar Resource Management](https://arxiv.org/abs/2506.20853)
*Ziyang Lu,Subodh Kalia,M. Cenk Gursoy,Chilukuri K. Mohan,Pramod K. Varshney*

Key words: 认知雷达，时间分配，多目标优化，深度强化学习，DDPG，SAC，NSGA-II

TL;DR: 该论文研究了多功能认知雷达系统中的时间分配问题，通过多目标优化和深度强化学习方法（DDPG和SAC）寻找帕累托最优解，并比较两种算法的性能。结果表明SAC在稳定性和样本效率上更优，同时使用NSGA-II算法估计帕累托前沿上界。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决多功能认知雷达系统中新目标扫描与旧目标跟踪之间的时间分配问题，提升系统在动态环境中的效率与适应性。

Method: 采用深度强化学习（DDPG和SAC）进行多目标优化，并使用NSGA-II算法估计帕累托前沿上界。

Result: DDPG和SAC均能有效适应不同场景，SAC在稳定性和样本效率上优于DDPG；NSGA-II为帕累托前沿提供了上界估计。

Conclusion: 该研究为动态环境中多功能认知雷达系统的多目标平衡提供了高效且自适应的解决方案。

Abstract: The time allocation problem in multi-function cognitive radar systems focuses
on the trade-off between scanning for newly emerging targets and tracking the
previously detected targets. We formulate this as a multi-objective
optimization problem and employ deep reinforcement learning to find
Pareto-optimal solutions and compare deep deterministic policy gradient (DDPG)
and soft actor-critic (SAC) algorithms. Our results demonstrate the
effectiveness of both algorithms in adapting to various scenarios, with SAC
showing improved stability and sample efficiency compared to DDPG. We further
employ the NSGA-II algorithm to estimate an upper bound on the Pareto front of
the considered problem. This work contributes to the development of more
efficient and adaptive cognitive radar systems capable of balancing multiple
competing objectives in dynamic environments.

</details>


### [58] [Leaner Training, Lower Leakage: Revisiting Memorization in LLM Fine-Tuning with LoRA](https://arxiv.org/abs/2506.20856)
*Fei Wang,Baochun Li*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Memorization in large language models (LLMs) makes them vulnerable to data
extraction attacks. While pre-training memorization has been extensively
studied, fewer works have explored its impact in fine-tuning, particularly for
LoRA fine-tuning, a widely adopted parameter-efficient method.
  In this work, we re-examine memorization in fine-tuning and uncover a
surprising divergence from prior findings across different fine-tuning
strategies. Factors such as model scale and data duplication, which strongly
influence memorization in pre-training and full fine-tuning, do not follow the
same trend in LoRA fine-tuning. Using a more relaxed similarity-based
memorization metric, we demonstrate that LoRA significantly reduces
memorization risks compared to full fine-tuning, while still maintaining strong
task performance.

</details>


### [59] [Omniwise: Predicting GPU Kernels Performance with LLMs](https://arxiv.org/abs/2506.20886)
*Zixian Wang,Cole Ramos,Muhammad A. Awad,Keith Lowery*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In recent years, the rapid advancement of deep neural networks (DNNs) has
revolutionized artificial intelligence, enabling models with unprecedented
capabilities in understanding, generating, and processing complex data. These
powerful architectures have transformed a wide range of downstream
applications, tackling tasks beyond human reach. In this paper, we introduce
Omniwise, the first end-to-end, self-supervised fine-tuning pipeline that
applies large language models (LLMs) to GPU kernel performance prediction--a
novel use case in performance profiling. Omniwise is model-agnostic and
lightweight, achieving strong results even with a small 3B-parameter model. It
can predict key performance metrics, including memory bandwidth, cache hit
rates, GFLOPs, and arithmetic intensity, directly from kernel code without the
need for code execution or profiling tools. Our approach achieves over 90% of
predictions within 10% relative error on GPU kernels executed on AMD MI250 and
MI300X architectures. In addition to the pipeline, we develop an online
inference server and a Visual Studio Code plugin that seamlessly integrate
LLM-based performance prediction into developers' workflows.

</details>


### [60] [On the Necessity of Output Distribution Reweighting for Effective Class Unlearning](https://arxiv.org/abs/2506.20893)
*Yian Wang,Ali Ebrahimpour-Boroojeny,Hari Sundaram*

Key words: machine unlearning, output-reweighting, RWFT, membership inference attacks, total variation distance

TL;DR: 提出了RWFT，一种轻量级的输出重加权遗忘方法，可以无需完全重新训练就能从训练好的分类器中删除整个类别，解决了现有方法在预测被删除类别样本时的不足。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 强制执行用户删除权利和减少有害或有偏见的预测需要从训练好的模型中遗忘特定类别，而完全再训练成本高昂，现有遗忘方法无法有效模仿重新训练模型的行为。

Method: 提出RWFT，通过对被遗忘类别样本的预测概率质量进行简单重新分配，设计了一种新的度量标准TV距离来量化残差泄漏。

Result: RWFT在现有评估指标和新提出的TV距离度量上均优于最先进的方法，分别提升了2.79%和111.45%。

Conclusion: RWFT是一种高效的遗忘方法，能够在不完全重新训练的情况下，接近重新训练的模型性能，并且在安全性上有显著提升。

Abstract: In this work, we introduce an output-reweighting unlearning method, RWFT, a
lightweight technique that erases an entire class from a trained classifier
without full retraining. Forgetting specific classes from trained models is
essential for enforcing user deletion rights and mitigating harmful or biased
predictions. The full retraining is costly and existing unlearning methods fail
to replicate the behavior of the retrained models when predicting samples from
the unlearned class. We prove this failure by designing a variant of membership
inference attacks, MIA-NN that successfully reveals the unlearned class for any
of these methods. We propose a simple redistribution of the probability mass
for the prediction on the samples in the forgotten class which is robust to
MIA-NN. We also introduce a new metric based on the total variation (TV)
distance of the prediction probabilities to quantify residual leakage to
prevent future methods from susceptibility to the new attack. Through extensive
experiments with state of the art baselines in machine unlearning, we show that
our approach matches the results of full retraining in both metrics used for
evaluation by prior work and the new metric we propose in this work. Compare to
state-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%
in our new TV-based metric over the best existing method.

</details>


### [61] [Graph-Structured Feedback Multimodel Ensemble Online Conformal Prediction](https://arxiv.org/abs/2506.20898)
*Erfan Hajihashemi,Yanning Shen*

Key words: 在线共形预测,多模型,二部图反馈,动态选择

TL;DR: 提出了一种新的多模型在线共形预测算法，通过二部图反馈动态选择有效模型子集，以降低计算复杂性和减小预测集大小。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 面对分布变化时，现有多模型在线共形预测方法因预设模型集选择问题导致计算复杂、预测集过大。

Method: 利用二部图反馈动态优化模型子集，并结合预测集大小和模型损失选择模型。

Result: 算法确保了覆盖有效性，实现了次线性遗憾，实验显示预测集更小且性能优于现有方法。

Conclusion: 所提方法通过动态模型选择和反馈优化，显著提升了多模型在线共形预测的效率。

Abstract: Online conformal prediction has demonstrated its capability to construct a
prediction set for each incoming data point that covers the true label with a
predetermined probability. To cope with potential distribution shift,
multi-model online conformal prediction has been introduced to select and
leverage different models from a preselected candidate set. Along with the
improved flexibility, the choice of the preselected set also brings challenges.
A candidate set that includes a large number of models may increase the
computational complexity. In addition, the inclusion of irrelevant models with
poor performance may negatively impact the performance and lead to
unnecessarily large prediction sets. To address these challenges, we propose a
novel multi-model online conformal prediction algorithm that identifies a
subset of effective models at each time step by collecting feedback from a
bipartite graph, which is refined upon receiving new data. A model is then
selected from this subset to construct the prediction set, resulting in reduced
computational complexity and smaller prediction sets. Additionally, we
demonstrate that using prediction set size as feedback, alongside model loss,
can significantly improve efficiency by constructing smaller prediction sets
while still satisfying the required coverage guarantee. The proposed algorithms
are proven to ensure valid coverage and achieve sublinear regret. Experiments
on real and synthetic datasets validate that the proposed methods construct
smaller prediction sets and outperform existing multi-model online conformal
prediction approaches.

</details>


### [62] [Optimal Single-Policy Sample Complexity and Transient Coverage for Average-Reward Offline RL](https://arxiv.org/abs/2506.20904)
*Matthew Zurek,Guy Zamir,Yudong Chen*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study offline reinforcement learning in average-reward MDPs, which
presents increased challenges from the perspectives of distribution shift and
non-uniform coverage, and has been relatively underexamined from a theoretical
perspective. While previous work obtains performance guarantees under
single-policy data coverage assumptions, such guarantees utilize additional
complexity measures which are uniform over all policies, such as the uniform
mixing time. We develop sharp guarantees depending only on the target policy,
specifically the bias span and a novel policy hitting radius, yielding the
first fully single-policy sample complexity bound for average-reward offline
RL. We are also the first to handle general weakly communicating MDPs,
contrasting restrictive structural assumptions made in prior work. To achieve
this, we introduce an algorithm based on pessimistic discounted value iteration
enhanced by a novel quantile clipping technique, which enables the use of a
sharper empirical-span-based penalty function. Our algorithm also does not
require any prior parameter knowledge for its implementation. Remarkably, we
show via hard examples that learning under our conditions requires coverage
assumptions beyond the stationary distribution of the target policy,
distinguishing single-policy complexity measures from previously examined
cases. We also develop lower bounds nearly matching our main result.

</details>


### [63] [Explainable AI for Radar Resource Management: Modified LIME in Deep Reinforcement Learning](https://arxiv.org/abs/2506.20916)
*Ziyang Lu,M. Cenk Gursoy,Chilukuri K. Mohan,Pramod K. Varshney*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Deep reinforcement learning has been extensively studied in decision-making
processes and has demonstrated superior performance over conventional
approaches in various fields, including radar resource management (RRM).
However, a notable limitation of neural networks is their ``black box" nature
and recent research work has increasingly focused on explainable AI (XAI)
techniques to describe the rationale behind neural network decisions. One
promising XAI method is local interpretable model-agnostic explanations (LIME).
However, the sampling process in LIME ignores the correlations between
features. In this paper, we propose a modified LIME approach that integrates
deep learning (DL) into the sampling process, which we refer to as DL-LIME. We
employ DL-LIME within deep reinforcement learning for radar resource
management. Numerical results show that DL-LIME outperforms conventional LIME
in terms of both fidelity and task performance, demonstrating superior
performance with both metrics. DL-LIME also provides insights on which factors
are more important in decision making for radar resource management.

</details>


### [64] [LLM-guided Chemical Process Optimization with a Multi-Agent Approach](https://arxiv.org/abs/2506.20921)
*Tong Zeng,Srivathsan Badrinarayanan,Janghoon Ock,Cheng-Kai Lai,Amir Barati Farimani*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Chemical process optimization is crucial to maximize production efficiency
and economic performance. Traditional methods, including gradient-based
solvers, evolutionary algorithms, and parameter grid searches, become
impractical when operating constraints are ill-defined or unavailable,
requiring engineers to rely on subjective heuristics to estimate feasible
parameter ranges. To address this constraint definition bottleneck, we present
a multi-agent framework of large language model (LLM) agents that autonomously
infer operating constraints from minimal process descriptions, then
collaboratively guide optimization using the inferred constraints. Our
AutoGen-based agentic framework employs OpenAI's o3 model, with specialized
agents for constraint generation, parameter validation, simulation execution,
and optimization guidance. Through two phases - autonomous constraint
generation using embedded domain knowledge, followed by iterative multi-agent
optimization - the framework eliminates the need for predefined operational
bounds. Validated on the hydrodealkylation process across cost, yield, and
yield-to-cost ratio metrics, the framework demonstrated competitive performance
with conventional optimization methods while achieving better computational
efficiency, requiring fewer iterations to converge. Our approach converged in
under 20 minutes, achieving a 31-fold speedup over grid search. Beyond
computational efficiency, the framework's reasoning-guided search demonstrates
sophisticated process understanding, correctly identifying utility trade-offs,
and applying domain-informed heuristics. This approach shows significant
potential for optimization scenarios where operational constraints are poorly
characterized or unavailable, particularly for emerging processes and retrofit
applications.

</details>


### [65] [Interpretable Representation Learning for Additive Rule Ensembles](https://arxiv.org/abs/2506.20927)
*Shahrzad Behzadimanesh,Pierre Le Bodic,Geoffrey I. Webb,Mario Boley*

Key words: 符号规则集合, 稀疏线性变换, 解释性模型, 贪婪优化, 逻辑回归

TL;DR: 研究了如何通过引入可学习的稀疏线性变换增强符号规则集合的表达能力，同时保持模型的解释性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统符号规则集合依赖于简单的轴平行决策区域，当输入特征不够独立或表达力不足时，需增加规则复杂性，影响解释性。本文旨在提升规则表达能力而不牺牲解释性。

Method: 引入可学习的稀疏线性变换（形式为x^T w ≥ t），通过顺序贪婪优化和迭代加权逻辑回归训练模型，使其决策区域为普通多面体。

Result: 实验表明，新方法在保持与现有方法相同测试风险的前提下，显著降低了模型复杂性。

Conclusion: 扩展的符号规则集合通过稀疏线性变换提升了表达能力，同时保持了模型的解释性和高效性。

Abstract: Small additive ensembles of symbolic rules offer interpretable prediction
models. Traditionally, these ensembles use rule conditions based on
conjunctions of simple threshold propositions $x \geq t$ on a single input
variable $x$ and threshold $t$, resulting geometrically in axis-parallel
polytopes as decision regions. While this form ensures a high degree of
interpretability for individual rules and can be learned efficiently using the
gradient boosting approach, it relies on having access to a curated set of
expressive and ideally independent input features so that a small ensemble of
axis-parallel regions can describe the target variable well. Absent such
features, reaching sufficient accuracy requires increasing the number and
complexity of individual rules, which diminishes the interpretability of the
model. Here, we extend classical rule ensembles by introducing logical
propositions with learnable sparse linear transformations of input variables,
i.e., propositions of the form $\mathbf{x}^\mathrm{T}\mathbf{w} \geq t$, where
$\mathbf{w}$ is a learnable sparse weight vector, enabling decision regions as
general polytopes with oblique faces. We propose a learning method using
sequential greedy optimization based on an iteratively reweighted formulation
of logistic regression. Experimental results demonstrate that the proposed
method efficiently constructs rule ensembles with the same test risk as
state-of-the-art methods while significantly reducing model complexity across
ten benchmark datasets.

</details>


### [66] [Model State Arithmetic for Machine Unlearning](https://arxiv.org/abs/2506.20941)
*Keivan Rezaei,Mehrdad Saberi,Abhilasha Ravichander,Soheil Feizi*

Key words: 大型语言模型,数据遗忘,模型检查点,MSA算法

TL;DR: 提出了一种名为MSA的新算法，通过利用模型检查点估计和消除数据点的影响，以实现高效的数据遗忘。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大型语言模型训练数据可能包含隐私、版权或不准确内容，完全重新训练计算成本高，因此需要高效的数据遗忘算法。

Method: 利用模型检查点（即预训练过程中不同阶段的模型状态）估计和消除特定数据点的影响。

Result: 实验表明，MSA在多个基准测试、模型和评估指标上均优于现有算法，证明了其有效性。

Conclusion: MSA为大型语言模型提供了一种灵活的数据遗忘方法，有望成为未来研究的重要方向。

Abstract: Large language models are trained on massive corpora of web data, which may
include private data, copyrighted material, factually inaccurate data, or data
that degrades model performance. Eliminating the influence of such problematic
datapoints through complete retraining -- by repeatedly pretraining the model
on datasets that exclude these specific instances -- is computationally
prohibitive. For this reason, unlearning algorithms have emerged that aim to
eliminate the influence of particular datapoints, while otherwise preserving
the model -- at a low computational cost. However, precisely estimating and
undoing the influence of individual datapoints has proved to be challenging. In
this work, we propose a new algorithm, MSA, for estimating and undoing the
influence of datapoints -- by leveraging model checkpoints i.e. artifacts
capturing model states at different stages of pretraining. Our experimental
results demonstrate that MSA consistently outperforms existing machine
unlearning algorithms across multiple benchmarks, models, and evaluation
metrics, suggesting that MSA could be an effective approach towards more
flexible large language models that are capable of data erasure.

</details>


### [67] [Antibody Design and Optimization with Multi-scale Equivariant Graph Diffusion Models for Accurate Complex Antigen Binding](https://arxiv.org/abs/2506.20957)
*Jiameng Chen,Xiantao Cai,Jia Wu,Wenbin Hu*

Key words: 抗体设计,几何深度学习,等变扩散,序列-结构协同设计

TL;DR: 提出了一种名为AbMEGD的端到端框架，用于抗体序列和结构协同设计，解决了现有方法在几何特征捕捉和抗原接口泛化上的局限性，并展示了显著的性能提升。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决抗体设计中几何特征捕捉和新型抗原接口泛化的挑战。

Method: 采用多尺度等变图扩散（AbMEGD）方法，结合几何深度学习和E(3)等变扩散，实现原子级几何特征和序列-结构交互的捕捉。

Result: 在SAbDab数据库上，AbMEGD在氨基酸恢复率和结构偏差等方面显著优于现有方法（如DiffAb）。

Conclusion: AbMEGD在保持结构完整性的同时提升了功能性，为抗体设计树立了新基准。

Abstract: Antibody design remains a critical challenge in therapeutic and diagnostic
development, particularly for complex antigens with diverse binding interfaces.
Current computational methods face two main limitations: (1) capturing
geometric features while preserving symmetries, and (2) generalizing novel
antigen interfaces. Despite recent advancements, these methods often fail to
accurately capture molecular interactions and maintain structural integrity. To
address these challenges, we propose \textbf{AbMEGD}, an end-to-end framework
integrating \textbf{M}ulti-scale \textbf{E}quivariant \textbf{G}raph
\textbf{D}iffusion for antibody sequence and structure co-design. Leveraging
advanced geometric deep learning, AbMEGD combines atomic-level geometric
features with residue-level embeddings, capturing local atomic details and
global sequence-structure interactions. Its E(3)-equivariant diffusion method
ensures geometric precision, computational efficiency, and robust
generalizability for complex antigens. Furthermore, experiments using the
SAbDab database demonstrate a 10.13\% increase in amino acid recovery, 3.32\%
rise in improvement percentage, and a 0.062~\AA\ reduction in root mean square
deviation within the critical CDR-H3 region compared to DiffAb, a leading
antibody design model. These results highlight AbMEGD's ability to balance
structural integrity with improved functionality, establishing a new benchmark
for sequence-structure co-design and affinity optimization. The code is
available at: https://github.com/Patrick221215/AbMEGD.

</details>


### [68] [SharpZO: Hybrid Sharpness-Aware Vision Language Model Prompt Tuning via Forward-Only Passes](https://arxiv.org/abs/2506.20990)
*Yifan Yang,Zhen Zhang,Rupak Vignesh Swaminathan,Jing Liu,Nathan Susanj,Zheng Zhang*

Key words: 视觉语言模型、零阶优化、进化策略、边缘设备、微调

TL;DR: 提出了一种名为SharpZO的混合优化方法，通过两阶段优化提升零阶视觉语言模型微调性能，仅需前向传播即可实现更好的精度和收敛速度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的零阶优化方法在视觉语言模型微调中性能不佳，且依赖高方差的进化策略。SharpZO旨在解决这一问题，尤其适应边缘设备的约束条件。

Method: 采用两阶段优化：先以Sharpness-aware的进化策略全局探索和平滑损失表面，再用稀疏零阶优化进行局部精细搜索，整个过程仅需前向传播。

Result: SharpZO在CLIP模型上显著提升了精度和收敛速度，平均性能比现有方法高出7%。

Conclusion: SharpZO为资源受限设备上的视觉语言模型微调提供了一种高效且无需反向传播的解决方案。

Abstract: Fine-tuning vision language models (VLMs) has achieved remarkable performance
across various downstream tasks; yet, it requires access to model gradients
through backpropagation (BP), making them unsuitable for memory-constrained,
inference-only edge devices. To address this limitation, previous work has
explored various BP-free fine-tuning methods. However, these approaches often
rely on high-variance evolutionary strategies (ES) or zeroth-order (ZO)
optimization, and often fail to achieve satisfactory performance. In this
paper, we propose a hybrid Sharpness-aware Zeroth-order optimization (SharpZO)
approach, specifically designed to enhance the performance of ZO VLM
fine-tuning via a sharpness-aware warm-up training. SharpZO features a
two-stage optimization process: a sharpness-aware ES stage that globally
explores and smooths the loss landscape to construct a strong initialization,
followed by a fine-grained local search via sparse ZO optimization. The entire
optimization relies solely on forward passes. Detailed theoretical analysis and
extensive experiments on CLIP models demonstrate that SharpZO significantly
improves accuracy and convergence speed, achieving up to 7% average gain over
state-of-the-art forward-only methods.

</details>


### [69] [Distilling Normalizing Flows](https://arxiv.org/abs/2506.21003)
*Steven Walton,Valeriy Klyukin,Maksim Artemev,Denis Derkach,Nikita Orlov,Humphrey Shi*

Key words: 显式密度学习器,归一化流,知识蒸馏,生成模型

TL;DR: 本文探讨了显式密度学习器在生成模型中的应用,重点研究了归一化流中的知识蒸馏技术,以提升小模型的采样质量和密度估计能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 显式密度学习器在概率分布建模上优于生成对抗网络,但训练难度大且采样质量低,因此研究如何通过知识蒸馏提升小归一化流的性能。

Method: 利用知识蒸馏技术,将知识从大模型传递到小归一化流中,探索非传统知识传递方式在中间层的应用。

Result: 实验表明,通过蒸馏可以显著减小模型规模,同时性能超过未经蒸馏的小模型,并因参数减少而提高吞吐量。

Conclusion: 知识蒸馏在归一化流中具有独特优势,能实现高效的小模型性能提升。

Abstract: Explicit density learners are becoming an increasingly popular technique for
generative models because of their ability to better model probability
distributions. They have advantages over Generative Adversarial Networks due to
their ability to perform density estimation and having exact latent-variable
inference. This has many advantages, including: being able to simply
interpolate, calculate sample likelihood, and analyze the probability
distribution. The downside of these models is that they are often more
difficult to train and have lower sampling quality.
  Normalizing flows are explicit density models, that use composable bijective
functions to turn an intractable probability function into a tractable one. In
this work, we present novel knowledge distillation techniques to increase
sampling quality and density estimation of smaller student normalizing flows.
We seek to study the capacity of knowledge distillation in Compositional
Normalizing Flows to understand the benefits and weaknesses provided by these
architectures. Normalizing flows have unique properties that allow for a
non-traditional forms of knowledge transfer, where we can transfer that
knowledge within intermediate layers. We find that through this distillation,
we can make students significantly smaller while making substantial performance
gains over a non-distilled student. With smaller models there is a
proportionally increased throughput as this is dependent upon the number of
bijectors, and thus parameters, in the network.

</details>


### [70] [TRIDENT: Tri-Modal Molecular Representation Learning with Taxonomic Annotations and Local Correspondence](https://arxiv.org/abs/2506.21028)
*Feng Jiang,Mangal Prakash,Hehuan Ma,Jianyuan Deng,Yuzhi Guo,Amina Mollaysa,Tommaso Mansi,Rui Liao,Junzhou Huang*

Key words: 分子表示学习, 多模态学习, SMILES, 分类注释, 分子性质预测

TL;DR: TRIDENT 是一种新颖的多模态框架，整合了分子 SMILES、文本描述和分类功能注释，通过全局和局部对齐目标学习丰富的分子表示，在多个下游任务中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有工作多忽视了分子的文本和分类信息，TRIDENT 旨在利用这些信息提升分子表示学习的效果。

Method: TRIDENT 结合了 SMILES、文本和分类注释，使用体积对齐目标和局部对齐目标进行多模态特征对齐，并通过动量机制动态平衡全局与局部对齐。

Result: 在 11 个下游任务中达到最佳性能，验证了多模态信息的价值。

Conclusion: 整合文本和分类信息能够显著提升分子性质预测的表示学习效果。

Abstract: Molecular property prediction aims to learn representations that map chemical
structures to functional properties. While multimodal learning has emerged as a
powerful paradigm to learn molecular representations, prior works have largely
overlooked textual and taxonomic information of molecules for representation
learning. We introduce TRIDENT, a novel framework that integrates molecular
SMILES, textual descriptions, and taxonomic functional annotations to learn
rich molecular representations. To achieve this, we curate a comprehensive
dataset of molecule-text pairs with structured, multi-level functional
annotations. Instead of relying on conventional contrastive loss, TRIDENT
employs a volume-based alignment objective to jointly align tri-modal features
at the global level, enabling soft, geometry-aware alignment across modalities.
Additionally, TRIDENT introduces a novel local alignment objective that
captures detailed relationships between molecular substructures and their
corresponding sub-textual descriptions. A momentum-based mechanism dynamically
balances global and local alignment, enabling the model to learn both broad
functional semantics and fine-grained structure-function mappings. TRIDENT
achieves state-of-the-art performance on 11 downstream tasks, demonstrating the
value of combining SMILES, textual, and taxonomic functional annotations for
molecular property prediction.

</details>


### [71] [Little By Little: Continual Learning via Self-Activated Sparse Mixture-of-Rank Adaptive Learning](https://arxiv.org/abs/2506.21035)
*Haodong Lu,Chongyang Zhao,Jason Xue,Lina Yao,Kristen Moore,Dong Gong*

Key words: Continual Learning, Mixture-of-Experts, LoRA, Rank Adaptation, Sparse Activation

TL;DR: 提出MoRA方法解决持续学习中灾难性遗忘和任务干扰问题，通过细粒度rank-1专家混合和稀疏激活提升性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有LoRA-based Mixture-of-Experts方法在持续学习中面临干扰、冗余和路由模糊问题，需要更精细的专家利用机制。

Method: MoRA将rank-r更新分解为rank-1专家，稀疏激活相关rank以避免干扰和冗余，并提出自适应路由和剪枝策略。

Result: MoRA在CLIP和大语言模型中验证了其在持续学习和泛化任务中的有效性，显著减少遗忘并提升性能。

Conclusion: MoRA通过细粒度专家混合和稀疏激活机制，在持续学习中实现了更优的效果和更强的泛化能力。

Abstract: Continual learning (CL) with large pre-trained models is challenged by
catastrophic forgetting and task interference. Existing LoRA-based
Mixture-of-Experts (MoE) approaches mitigate forgetting by assigning and
freezing task-specific adapters, but suffer from interference, redundancy, and
ambiguous routing due to coarse adapter-level selection. However, this design
introduces three key challenges: 1) Interference: Activating full LoRA experts
per input leads to subspace interference and prevents selective reuse of useful
components across tasks. 2) Redundancy: Newly added experts often duplicate or
contradict existing knowledge due to unnecessary activation of unrelated ranks
and insufficient reuse of relevant ones. 3) Ambiguity: Overlapping features
across tasks confuse the router, resulting in unstable expert assignments. As
more experts accumulate, earlier task routing degrades, accelerating
forgetting. We propose MoRA, a Mixture-of-Rank Adaptive learning approach with
self-activated and sparse rank activation for CL. Unlike mixing multiple
low-rank matrices, MoRA decomposes each rank-r update into r rank-1 components,
each treated as an independent expert, enabling fine-grained mixture of rank-1
expert utilization while mitigating interference and redundancy. To avoid
ambiguous routing, we propose that each rank-1 expert can infer its own
relevance via intermediate activations. Coupled with our proposed rank pruning
and activation budgets, MoRA adaptively selects a sparse mixture of ranks per
input. We validate MoRA on continual learning tasks with CLIP and large
language models (LLMs), analyzing both in-domain learning and out-of-domain
forgetting/generalization during fine-tuning. MoRA shows significant
effectiveness on enhancing CL with PTMs, and improving generalization while
mitigating forgetting.

</details>


### [72] [An Information-Theoretic Analysis for Federated Learning under Concept Drift](https://arxiv.org/abs/2506.21036)
*Fu Peng,Meng Zhang,Ming Tang*

Key words: 联邦学习, 概念漂移, 信息理论, KL散度, 互信息

TL;DR: 该论文研究了联邦学习中概念漂移的问题，提出了一种基于信息理论的算法来缓解性能下降，并通过实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现实世界中的数据通常以流的形式出现，分布会随时间变化（概念漂移），导致联邦学习性能下降，需要新的解决方法。

Method: 使用信息理论和马尔可夫链建模概念漂移，提出通过KL散度和互信息正则化的算法来优化长期性能。

Result: 实验结果表明，提出的算法在三种漂移模式（周期性、渐进性、随机性）下均优于现有方法。

Conclusion: 该算法能有效适应联邦学习中的概念漂移，提升模型对未来数据的泛化能力。

Abstract: Recent studies in federated learning (FL) commonly train models on static
datasets. However, real-world data often arrives as streams with shifting
distributions, causing performance degradation known as concept drift. This
paper analyzes FL performance under concept drift using information theory and
proposes an algorithm to mitigate the performance degradation. We model concept
drift as a Markov chain and introduce the \emph{Stationary Generalization
Error} to assess a model's capability to capture characteristics of future
unseen data. Its upper bound is derived using KL divergence and mutual
information. We study three drift patterns (periodic, gradual, and random) and
their impact on FL performance. Inspired by this, we propose an algorithm that
regularizes the empirical risk minimization approach with KL divergence and
mutual information, thereby enhancing long-term performance. We also explore
the performance-cost tradeoff by identifying a Pareto front. To validate our
approach, we build an FL testbed using Raspberry Pi4 devices. Experimental
results corroborate with theoretical findings, confirming that drift patterns
significantly affect performance. Our method consistently outperforms existing
approaches for these three patterns, demonstrating its effectiveness in
adapting concept drift in FL.

</details>


### [73] [RL-Selector: Reinforcement Learning-Guided Data Selection via Redundancy Assessment](https://arxiv.org/abs/2506.21037)
*Suorong Yang,Peijia Li,Furao Shen,Jian Zhao*

Key words: 数据选择, 强化学习, 样本冗余, 训练效率, 泛化性能

TL;DR: 论文提出了一种基于强化学习的数据选择方法RL-Selector，利用epsilon-sample cover量化样本冗余，显著提升训练效率和模型性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大规模数据集训练带来高计算和存储成本，现有数据选择方法忽视样本间的动态关系和冗余问题。

Method: 引入epsilon-sample cover概念量化样本冗余，通过强化学习优化数据选择策略。

Result: 在多个基准数据集和架构上，RL-Selector表现优于现有方法，提升训练效率和泛化能力。

Conclusion: RL-Selector为高效数据选择提供了新思路，适用于资源受限的场景。

Abstract: Modern deep architectures often rely on large-scale datasets, but training on
these datasets incurs high computational and storage overhead. Real-world
datasets often contain substantial redundancies, prompting the need for more
data-efficient training paradigms. Data selection has shown promise to mitigate
redundancy by identifying the most representative samples, thereby reducing
training costs without compromising performance. Existing methods typically
rely on static scoring metrics or pretrained models, overlooking the combined
effect of selected samples and their evolving dynamics during training. We
introduce the concept of epsilon-sample cover, which quantifies sample
redundancy based on inter-sample relationships, capturing the intrinsic
structure of the dataset. Based on this, we reformulate data selection as a
reinforcement learning (RL) process and propose RL-Selector, where a
lightweight RL agent optimizes the selection policy by leveraging
epsilon-sample cover derived from evolving dataset distribution as a reward
signal. Extensive experiments across benchmark datasets and diverse
architectures demonstrate that our method consistently outperforms existing
state-of-the-art baselines. Models trained with our selected datasets show
enhanced generalization performance with improved training efficiency.

</details>


### [74] [Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement Learning](https://arxiv.org/abs/2506.21039)
*Jaebak Hwang,Sanghyeon Lee,Jeongmo Kim,Seungyul Han*

Key words: 强化学习,长时程任务,分层强化学习,子目标可达性,失败感知

TL;DR: SSE是一个基于图的分层强化学习框架，通过结构化约束高层决策，确保单步子目标可达性，并采用解耦的探索策略和失败感知路径细化，显著提升长时程任务的效率和成功率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 长时程目标导向任务在奖励稀疏和目标遥远时对强化学习提出了挑战，现有分层和图基方法存在子目标不可行和规划效率低的问题。

Method: SSE通过结构化约束高层决策确保子目标可达性，使用解耦探索策略系统遍历未探索目标区域，并通过失败感知路径细化动态调整边成本以提高子目标可靠性。

Result: SSE在多样化的长时程基准测试中，效率和成功率均优于现有目标导向和分层强化学习方法。

Conclusion: SSE通过严格子目标执行和动态路径优化，有效解决了长时程任务中的规划效率和子目标可行性问题。

Abstract: Long-horizon goal-conditioned tasks pose fundamental challenges for
reinforcement learning (RL), particularly when goals are distant and rewards
are sparse. While hierarchical and graph-based methods offer partial solutions,
they often suffer from subgoal infeasibility and inefficient planning. We
introduce Strict Subgoal Execution (SSE), a graph-based hierarchical RL
framework that enforces single-step subgoal reachability by structurally
constraining high-level decision-making. To enhance exploration, SSE employs a
decoupled exploration policy that systematically traverses underexplored
regions of the goal space. Furthermore, a failure-aware path refinement, which
refines graph-based planning by dynamically adjusting edge costs according to
observed low-level success rates, thereby improving subgoal reliability.
Experimental results across diverse long-horizon benchmarks demonstrate that
SSE consistently outperforms existing goal-conditioned RL and hierarchical RL
approaches in both efficiency and success rate.

</details>


### [75] [Efficient Skill Discovery via Regret-Aware Optimization](https://arxiv.org/abs/2506.21044)
*He Zhang,Ming Zhou,Shaopeng Zhai,Ying Sun,Hui Xiong*

Key words: 无监督技能发现, 极小极大博弈, 遗憾感知, 高维环境, 多样性

TL;DR: 该论文提出了一种基于遗憾感知的无监督技能发现方法，通过将技能发现框架为技能生成与策略学习的极小极大博弈，提升了高维环境下的效率和多样性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法在探索多样性方面表现良好，但在高维环境下效率较低，因此需要一种更高效的方法来发现多样且可区分的技能。

Method: 将技能发现视为技能生成与策略学习的极小极大博弈，利用遗憾量化策略强度的收敛程度，并通过可学习的技能生成器指导技能发现。

Result: 实验表明，该方法在效率和多样性上优于基线，并在高维环境中实现了15%的零样本改进。

Conclusion: 提出的遗憾感知方法在高维环境中显著提升了技能发现的效率和多样性。

Abstract: Unsupervised skill discovery aims to learn diverse and distinguishable
behaviors in open-ended reinforcement learning. For existing methods, they
focus on improving diversity through pure exploration, mutual information
optimization, and learning temporal representation. Despite that they perform
well on exploration, they remain limited in terms of efficiency, especially for
the high-dimensional situations. In this work, we frame skill discovery as a
min-max game of skill generation and policy learning, proposing a regret-aware
method on top of temporal representation learning that expands the discovered
skill space along the direction of upgradable policy strength. The key insight
behind the proposed method is that the skill discovery is adversarial to the
policy learning, i.e., skills with weak strength should be further explored
while less exploration for the skills with converged strength. As an
implementation, we score the degree of strength convergence with regret, and
guide the skill discovery with a learnable skill generator. To avoid
degeneration, skill generation comes from an up-gradable population of skill
generators. We conduct experiments on environments with varying complexities
and dimension sizes. Empirical results show that our method outperforms
baselines in both efficiency and diversity. Moreover, our method achieves a 15%
zero shot improvement in high-dimensional environments, compared to existing
methods.

</details>


### [76] [FedDAA: Dynamic Client Clustering for Concept Drift Adaptation in Federated Learning](https://arxiv.org/abs/2506.21054)
*Fu Peng,Ming Tang*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In federated learning (FL), the data distribution of each client may change
over time, introducing both temporal and spatial data heterogeneity, known as
concept drift. Data heterogeneity arises from three drift sources: real drift
(a shift in the conditional distribution P(y|x)), virtual drift (a shift in the
input distribution P(x)), and label drift (a shift in the label distribution
P(y)). However, most existing FL methods addressing concept drift primarily
focus on real drift. When clients experience virtual or label drift, these
methods often fail to selectively retain useful historical knowledge, leading
to catastrophic forgetting. A key challenge lies in distinguishing different
sources of drift, as they require distinct adaptation strategies: real drift
calls for discarding outdated data, while virtual or label drift benefits from
retaining historical data. Without explicitly identifying the drift sources, a
general adaptation strategy is suboptimal and may harm generalization. To
address this challenge, we propose FedDAA, a dynamic clustered FL framework
designed to adapt to multi-source concept drift while preserving valuable
historical knowledge. Specifically, FedDAA integrates three modules: a cluster
number determination module to find the optimal number of clusters; a real
drift detection module to distinguish real drift from virtual/label drift; and
a concept drift adaptation module to adapt to new data while retaining useful
historical information. We provide theoretical convergence guarantees, and
experiments show that FedDAA achieves 7.84% to 8.52% accuracy improvements over
state-of-the-art methods on Fashion-MNIST, CIFAR-10, and CIFAR-100.

</details>


### [77] [Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge Graph](https://arxiv.org/abs/2506.21071)
*Jingwei Wang,Zai Zhang,Hao Qian,Chunjing Gan,Binbin Hu,Ziqi Liu,Zhiqiang Zhang,Jun Zhou,Bin Shi,Bo Dong*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Teaching large language models (LLMs) to use tools is crucial for improving
their problem-solving abilities and expanding their applications. However,
effectively using tools is challenging because it requires a deep understanding
of tool functionalities and user intentions. Previous methods relied mainly on
LLMs to generate instruction data, but the quality of these data was often
insufficient. In this paper, we propose a new method that uses knowledge graphs
to generate high-quality instruction data for LLMs. Knowledge graphs are
manually curated datasets rich in semantic information. We begin by extracting
various query pathways from a given knowledge graph, which are transformed into
a broad spectrum of user queries. We then translate the relationships between
entities into actionable tools and parse the pathways of each query into
detailed solution steps, thereby creating high-quality instruction data. Our
experiments show that fine-tuning on just a small sample of this synthetic data
can significantly improve the tool utilization and overall capabilities of
LLMs.

</details>


### [78] [Chain-of-Thought Enhanced Shallow Transformers for Wireless Symbol Detection](https://arxiv.org/abs/2506.21093)
*Li Fan,Peng Wang,Jing Yang,Cong Shen*

Key words: Transformer, in-context learning, wireless symbol detection, shallow model, computational efficiency

TL;DR: 提出了一种基于CoT增强的浅层Transformer框架CHOOSE，用于无线符号检测，在保持存储和计算效率的同时，性能接近深层模型。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决传统ICL-based Transformer模型因深度架构导致的高存储和计算成本问题。

Method: 通过引入自回归潜在推理步骤，提升浅层模型（1-2层）的推理能力而不增加深度。

Result: 实验表明，CHOOSE优于传统浅层Transformer，性能接近深层模型，且高效计算。

Conclusion: CHOOSE为资源受限的移动设备部署Transformer算法提供了可行方案。

Abstract: Transformers have shown potential in solving wireless communication problems,
particularly via in-context learning (ICL), where models adapt to new tasks
through prompts without requiring model updates. However, prior ICL-based
Transformer models rely on deep architectures with many layers to achieve
satisfactory performance, resulting in substantial storage and computational
costs. In this work, we propose CHain Of thOught Symbol dEtection (CHOOSE), a
CoT-enhanced shallow Transformer framework for wireless symbol detection. By
introducing autoregressive latent reasoning steps within the hidden space,
CHOOSE significantly improves the reasoning capacity of shallow models (1-2
layers) without increasing model depth. This design enables lightweight
Transformers to achieve detection performance comparable to much deeper models,
making them well-suited for deployment on resource-constrained mobile devices.
Experimental results demonstrate that our approach outperforms conventional
shallow Transformers and achieves performance comparable to that of deep
Transformers, while maintaining storage and computational efficiency. This
represents a promising direction for implementing Transformer-based algorithms
in wireless receivers with limited computational resources.

</details>


### [79] [FeDa4Fair: Client-Level Federated Datasets for Fairness Evaluation](https://arxiv.org/abs/2506.21095)
*Xenia Heilmann,Luca Corbucci,Mattia Cerrato,Anna Monreale*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Federated Learning (FL) enables collaborative model training across multiple
clients without sharing clients' private data. However, fairness remains a key
concern, as biases in local clients' datasets can impact the entire federated
system. Heterogeneous data distributions across clients may lead to models that
are fairer for some clients than others. Although several fairness-enhancing
solutions are present in the literature, most focus on mitigating bias for a
single sensitive attribute, typically binary, overlooking the diverse and
sometimes conflicting fairness needs of different clients. This limited
perspective can limit the effectiveness of fairness interventions for the
different clients. To support more robust and reproducible fairness research in
FL, we aim to enable a consistent benchmarking of fairness-aware FL methods at
both the global and client levels. In this paper, we contribute in three ways:
(1) We introduce FeDa4Fair, a library to generate tabular datasets tailored to
evaluating fair FL methods under heterogeneous client bias; (2) we release four
bias-heterogeneous datasets and corresponding benchmarks to compare fairness
mitigation methods in a controlled environment; (3) we provide ready-to-use
functions for evaluating fairness outcomes for these datasets.

</details>


### [80] [Learning to Skip the Middle Layers of Transformers](https://arxiv.org/abs/2506.21103)
*Tim Lawson,Laurence Aitchison*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Conditional computation is a popular strategy to make Transformers more
efficient. Existing methods often target individual modules (e.g.,
mixture-of-experts layers) or skip layers independently of one another.
However, interpretability research has demonstrated that the middle layers of
Transformers exhibit greater redundancy, and that early layers aggregate
information into token positions. Guided by these insights, we propose a novel
architecture that dynamically skips a variable number of layers from the middle
outward. In particular, a learned gating mechanism determines whether to bypass
a symmetric span of central blocks based on the input, and a gated attention
mechanism prevents subsequent tokens from attending to skipped token positions.
Residual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and
gate sparsity with an adaptive regularization loss. We had aimed to reduce
compute requirements for 'simpler' tokens and potentially foster an emergent
multi-level representational hierarchy but, at the scales investigated, our
approach does not achieve improvements in the trade-off between validation
cross-entropy and estimated FLOPs compared to dense baselines with fewer
layers. We release our code at https://github.com/tim-lawson/skip-middle.

</details>


### [81] [Interpretable Hierarchical Concept Reasoning through Attention-Guided Graph Learning](https://arxiv.org/abs/2506.21102)
*David Debot,Pietro Barbiero,Gabriele Dominici,Giuseppe Marra*

Key words: 概念基础模型、可解释性、层次化概念推理、注意力机制

TL;DR: H-CMR是一种新型概念基础模型，通过层次化概念记忆推理器提供对概念和任务预测的双重可解释性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有概念基础模型（CBMs）仅对最终任务预测提供可解释性，而概念预测本身仍使用黑盒神经网络。H-CMR旨在同时解决概念和任务预测的可解释性问题。

Method: H-CMR利用学习的有向无环图建模概念间关系，并通过注意力机制选择逻辑规则进行层次化预测。

Result: H-CMR在保持性能的同时，支持用户干预以提高推理准确性和训练数据效率。

Conclusion: H-CMR为概念和任务预测提供了更强的可解释性和交互性，同时保持了高性能。

Abstract: Concept-Based Models (CBMs) are a class of deep learning models that provide
interpretability by explaining predictions through high-level concepts. These
models first predict concepts and then use them to perform a downstream task.
However, current CBMs offer interpretability only for the final task
prediction, while the concept predictions themselves are typically made via
black-box neural networks. To address this limitation, we propose Hierarchical
Concept Memory Reasoner (H-CMR), a new CBM that provides interpretability for
both concept and task predictions. H-CMR models relationships between concepts
using a learned directed acyclic graph, where edges represent logic rules that
define concepts in terms of other concepts. During inference, H-CMR employs a
neural attention mechanism to select a subset of these rules, which are then
applied hierarchically to predict all concepts and the final task. Experimental
results demonstrate that H-CMR matches state-of-the-art performance while
enabling strong human interaction through concept and model interventions. The
former can significantly improve accuracy at inference time, while the latter
can enhance data efficiency during training when background knowledge is
available.

</details>


### [82] [Complexity-aware fine-tuning](https://arxiv.org/abs/2506.21220)
*Andrey Goncharov,Daniil Vyazhev,Petr Sychev,Edvard Khalafyan,Alexey Zaytsev*

Key words: 语言模型, 微调, 熵, 数据分类, 推理

TL;DR: 提出了一种通过熵识别复杂数据、结合监督微调和蒸馏的高效微调方法，显著提升了小规模语言模型的性能，同时减少了数据需求。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对通用大型语言模型（LLMs）在特定领域微调时数据需求大、成本高的问题，提出一种更高效的微调方法。

Method: 通过单标记答案熵对训练数据分类，结合监督微调（SFT）和蒸馏技术，仅对复杂数据使用推理。

Result: 方法在平均准确率上显著优于标准SFT（0.55 vs 0.43），并与蒸馏性能相当，同时减少62%的数据需求。

Conclusion: 该方法为高效微调提供了一种可行方案，推动了进一步研究。

Abstract: General-purpose Large Language Models (LLMs) are frequently fine-tuned
through supervised fine-tuning (SFT) to enhance performance in specific
domains. Better results can be achieved by distilling the chain-of-thought of a
larger model at the cost of numerous expensive calls and a much greater amount
of data. We propose a novel blueprint for efficient fine-tuning that uses
reasoning only for complex data identified by entropy. Specifically, across two
small open models ($\approx 3B$) we split the training data into complexity
categories by a single token answer entropy (ROC AUC $0.73$), fine-tune large
language models (LLMs) via SFT and distillation, and show that our pipeline
significantly outperforms the standard SFT approach ($0.55$ vs $0.43$ average
accuracy) and provides comparable with distillation performance while using
$62\%$ less data ($0.55$ average accuracy for both). We publish our code and
data to facilitate further research in this direction.

</details>


### [83] [Unlasting: Unpaired Single-Cell Multi-Perturbation Estimation by Dual Conditional Diffusion Implicit Bridges](https://arxiv.org/abs/2506.21107)
*Changxi Chi,Jun Xia,Yufei Huang,Jingbo Zhou,Siyuan Li,Yunfan Liu,Chang Yu,Stan Z. Li*

Key words: 单细胞扰动, 非配对数据, 双扩散隐式桥, 基因调控网络, 掩码模型

TL;DR: 提出基于双扩散隐式桥（DDIB）的框架解决单细胞扰动数据的非配对问题，结合基因调控网络（GRN）和掩码机制提升生成质量，并引入新评估指标捕捉细胞异质性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 单细胞测序是破坏性过程，无法捕捉同一细胞在扰动前后的表型，导致数据非配对。现有方法或强行配对，或忽略未扰动与扰动细胞的固有关系。

Method: 提出Unlasting框架，利用双条件扩散模型学习数据分布映射，整合GRN传播扰动信号，设计掩码模型预测静默基因，并引入新评估指标。

Result: 框架有效解决非配对数据问题，提升生成质量，新评估指标更好地反映细胞响应异质性。

Conclusion: Unlasting框架通过DDIB和GRN指导，解决了单细胞扰动数据的非配对问题，同时提升了模型对扰动和异质性的理解。

Abstract: Estimating single-cell responses across various perturbations facilitates the
identification of key genes and enhances drug screening, significantly boosting
experimental efficiency. However, single-cell sequencing is a destructive
process, making it impossible to capture the same cell's phenotype before and
after perturbation. Consequently, data collected under perturbed and
unperturbed conditions are inherently unpaired. Existing methods either attempt
to forcibly pair unpaired data using random sampling, or neglect the inherent
relationship between unperturbed and perturbed cells during the modeling. In
this work, we propose a framework based on Dual Diffusion Implicit Bridges
(DDIB) to learn the mapping between different data distributions, effectively
addressing the challenge of unpaired data. We further interpret this framework
as a form of data augmentation. We integrate gene regulatory network (GRN)
information to propagate perturbation signals in a biologically meaningful way,
and further incorporate a masking mechanism to predict silent genes, improving
the quality of generated profiles. Moreover, gene expression under the same
perturbation often varies significantly across cells, frequently exhibiting a
bimodal distribution that reflects intrinsic heterogeneity. To capture this, we
introduce a more suitable evaluation metric. We propose Unlasting, dual
conditional diffusion models that overcome the problem of unpaired single-cell
perturbation data and strengthen the model's insight into perturbations under
the guidance of the GRN, with a dedicated mask model designed to improve
generation quality by predicting silent genes. In addition, we introduce a
biologically grounded evaluation metric that better reflects the inherent
heterogeneity in single-cell responses.

</details>


### [84] [DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster](https://arxiv.org/abs/2506.21263)
*Ji Qi,WenPeng Zhu,Li Li,Ming Wu,YingJun Wu,Wu He,Xun Gao,Jason Zeng,Michael Heinrich*

Key words: 分布式训练,大型语言模型,低通信,管道并行,梯度压缩

TL;DR: DiLoCoX是一个低通信的大规模分布式训练框架，通过管道并行和自适应梯度压缩显著提升训练速度和参数规模。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决在慢速网络上训练大型模型（如100B+参数）的挑战，释放分散集群的潜力。

Method: 结合管道并行、双优化器策略、一步延迟通信与本地训练重叠、自适应梯度压缩。

Result: 在1Gbps网络上成功预训练107B模型，速度提升357倍，收敛几乎无损。

Conclusion: DiLoCoX是首个成功应用于100B+参数模型的分散训练框架。

Abstract: The distributed training of foundation models, particularly large language
models (LLMs), demands a high level of communication. Consequently, it is
highly dependent on a centralized cluster with fast and reliable interconnects.
Can we conduct training on slow networks and thereby unleash the power of
decentralized clusters when dealing with models exceeding 100 billion
parameters? In this paper, we propose DiLoCoX, a low-communication large-scale
decentralized cluster training framework. It combines Pipeline Parallelism with
Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local
Training, and an Adaptive Gradient Compression Scheme. This combination
significantly improves the scale of parameters and the speed of model
pre-training. We justify the benefits of one-step-delay overlap of
communication and local training, as well as the adaptive gradient compression
scheme, through a theoretical analysis of convergence. Empirically, we
demonstrate that DiLoCoX is capable of pre-training a 107B foundation model
over a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x
speedup in distributed training while maintaining negligible degradation in
model convergence. To the best of our knowledge, this is the first
decentralized training framework successfully applied to models with over 100
billion parameters.

</details>


### [85] [Robust Policy Switching for Antifragile Reinforcement Learning for UAV Deconfliction in Adversarial Environments](https://arxiv.org/abs/2506.21127)
*Deepak Kumar Panda,Weisi Guo*

Key words: 无人机导航, 强化学习, 抗脆弱性, 对抗攻击, 折扣汤普森采样

TL;DR: 本文提出了一种抗脆弱的强化学习框架，通过动态选择多种鲁棒策略，提升无人机导航对对抗攻击的适应性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有鲁棒RL方法对分布外扰动的泛化能力有限，无法有效应对动态的对抗攻击。

Method: 采用折扣汤普森采样（DTS）动态选择策略，并结合多臂老虎机（MAB）模型优化策略选择。

Result: 在复杂导航环境中，该方法相较于传统鲁棒RL，表现出更短的导航路径和更高的冲突规避率。

Conclusion: 抗脆弱RL框架有效提升了无人机导航对对抗攻击的适应性，优于现有技术。

Abstract: The increasing automation of navigation for unmanned aerial vehicles (UAVs)
has exposed them to adversarial attacks that exploit vulnerabilities in
reinforcement learning (RL) through sensor manipulation. Although existing
robust RL methods aim to mitigate such threats, their effectiveness has limited
generalization to out-of-distribution shifts from the optimal value
distribution, as they are primarily designed to handle fixed perturbation. To
address this limitation, this paper introduces an antifragile RL framework that
enhances adaptability to broader distributional shifts by incorporating a
switching mechanism based on discounted Thompson sampling (DTS). This mechanism
dynamically selects among multiple robust policies to minimize adversarially
induced state-action-value distribution shifts. The proposed approach first
derives a diverse ensemble of action robust policies by accounting for a range
of perturbations in the policy space. These policies are then modeled as a
multiarmed bandit (MAB) problem, where DTS optimally selects policies in
response to nonstationary Bernoulli rewards, effectively adapting to evolving
adversarial strategies. Theoretical framework has also been provided where by
optimizing the DTS to minimize the overall regrets due to distributional shift,
results in effective adaptation against unseen adversarial attacks thus
inducing antifragility. Extensive numerical simulations validate the
effectiveness of the proposed framework in complex navigation environments with
multiple dynamic three-dimensional obstacles and with stronger projected
gradient descent (PGD) and spoofing attacks. Compared to conventional robust,
non-adaptive RL methods, the antifragile approach achieves superior
performance, demonstrating shorter navigation path lengths and a higher rate of
conflict-free navigation trajectories compared to existing robust RL techniques

</details>


### [86] [Curriculum-Guided Antifragile Reinforcement Learning for Secure UAV Deconfliction under Observation-Space Attacks](https://arxiv.org/abs/2506.21129)
*Deepak Kumar Panda,Adolfo Perrusquia,Weisi Guo*

Key words: 强化学习, 抗脆弱性, 对抗攻击, 无人机导航, Wasserstein距离

TL;DR: 论文提出了一种抗脆弱强化学习框架，用于对抗观察空间中的分布外对抗攻击，通过增量对抗扰动训练模型，使其在无人机避障场景中表现优于标准方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在安全关键系统中部署的强化学习策略容易受到观察空间的对抗攻击，导致决策不安全或次优，需要一种能够适应并抵抗这种攻击的方法。

Method: 提出抗脆弱RL框架，通过模拟攻击者逐步增加观察空间的扰动强度，使用Wasserstein距离最小化进行专家指导的批评对齐，稳定价值函数分布。

Result: 在无人机避障场景中，抗脆弱策略在对抗攻击下表现优于基线，累积奖励提高15%，冲突事件减少30%。

Conclusion: 抗脆弱强化学习在动态威胁环境中具有理论和实际的可行性，能够提供安全可靠的决策支持。

Abstract: Reinforcement learning (RL) policies deployed in safety-critical systems,
such as unmanned aerial vehicle (UAV) navigation in dynamic airspace, are
vulnerable to out-ofdistribution (OOD) adversarial attacks in the observation
space. These attacks induce distributional shifts that significantly degrade
value estimation, leading to unsafe or suboptimal decision making rendering the
existing policy fragile. To address this vulnerability, we propose an
antifragile RL framework designed to adapt against curriculum of incremental
adversarial perturbations. The framework introduces a simulated attacker which
incrementally increases the strength of observation-space perturbations which
enables the RL agent to adapt and generalize across a wider range of OOD
observations and anticipate previously unseen attacks. We begin with a
theoretical characterization of fragility, formally defining catastrophic
forgetting as a monotonic divergence in value function distributions with
increasing perturbation strength. Building on this, we define antifragility as
the boundedness of such value shifts and derive adaptation conditions under
which forgetting is stabilized. Our method enforces these bounds through
iterative expert-guided critic alignment using Wasserstein distance
minimization across incrementally perturbed observations. We empirically
evaluate the approach in a UAV deconfliction scenario involving dynamic 3D
obstacles. Results show that the antifragile policy consistently outperforms
standard and robust RL baselines when subjected to both projected gradient
descent (PGD) and GPS spoofing attacks, achieving up to 15% higher cumulative
reward and over 30% fewer conflict events. These findings demonstrate the
practical and theoretical viability of antifragile reinforcement learning for
secure and resilient decision-making in environments with evolving threat
scenarios.

</details>


### [87] [NaLaFormer: Norm-Aware Linear Attention for Transformer Models](https://arxiv.org/abs/2506.21137)
*Weikang Meng,Yadan Luo,Liangyu Huo,Yaowei Wang,Xin Li,Zheng Zhang*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Linear attention has emerged as a viable alternative to softmax attention by
reducing complexity from quadratic to linear in sequence length. To preserve
two fundamental properties of softmax, non-negativity and entropy reduction,
current works employ various linearly separatable kernel functions with $L1$
normalization instead of softmax operator. However, query norms are neglected
by the normalization operation in linear attention, such degradation heavily
leads to an entropy gap. Meanwhile, existing works inhibit negative values of
query and key vectors resulting in a missing inner-product interactions after
being mapped. To address these dual challenges, we propose a novel Norm-Aware
Linear Attention mechanism serving to restore norm-guided dynamic spikiness and
recover kernel-perturbed norm distributions. Specifically, we first decouple
query and key matrices into two components: norm and direction, to achieve
norm-aware spikiness control and norm consistency, respectively. We
mathematically reveal that the extent of entropy reduction varies with the
query norm in softmax normalization, motivating a query-norm aware kernel
function for dynamic control over entropy reduction. Furthermore, to ensure
norm consistency and enforce non-negativity constraints, we employ a
norm-preserving mapping to project all elements of the angular matrix into
positive values, leveraging cosine similarity to inhibit dimensions with
opposite directions. We conduct extensive experiments demonstrating that the
NaLaFormer improves performance on vision and language tasks, enhancing both
expressiveness and efficiency by up to 4.2\%.

</details>


### [88] [Latent Prototype Routing: Achieving Near-Perfect Load Balancing in Mixture-of-Experts](https://arxiv.org/abs/2506.21328)
*Jiajie Yang*

Key words: Mixture-of-Experts, Routing, Load Balancing, Gini Coefficient, DeepSeek-V3

TL;DR: 论文提出了一种新的路由框架Latent Prototype Routing (LPR)，通过聚类视角改进MoE架构中的专家路由，显著提升了专家负载均衡性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前MoE系统存在严重的负载不均衡问题，仅少数专家被激活，导致模型容量和计算资源利用不足，需要改进路由机制。

Method: 提出了LPR框架，通过聚类视角重新设计专家路由，提升了专家利用的均衡性，同时不影响下游性能。

Result: 实验显示LPR将Gini系数从0.70降至0.035，min-max专家负载比从1e-6提升到0.70，实现了近乎完美的负载均衡。

Conclusion: LPR是一种高效的路由框架，解决了MoE架构的负载不均衡问题，且不影响模型性能。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a key strategy for
scaling large language models (LLMs) efficiently. However, current MoE systems
suffer from severe load imbalance, where only a small subset of experts is
consistently activated during training and inference, leading to significant
underutilization of model capacity and computational resources. In this work,
we revisit expert routing through a clustering perspective and propose Latent
Prototype Routing (LPR), a novel routing framework that generalizes existing
approaches while promoting balanced expert utilization without compromising
downstream performance. Extensive experiments across multiple open-source MoE
models -- including DeepSeek-V3, Qwen3-MoE, and Mixtral -- demonstrate that LPR
reduces the Gini coefficient of expert load from 0.70 to 0.035 on average,
improves the min-max expert load ratio from 1e-6 to 0.70, achieving
near-perfect load balancing.

</details>


### [89] [DBConformer: Dual-Branch Convolutional Transformer for EEG Decoding](https://arxiv.org/abs/2506.21140)
*Ziwei Wang,Hongbin Wang,Tianwang Jia,Xingyi He,Siyang Li,Dongrui Wu*

Key words: EEG, BCI, CNN-Transformer, DBConformer, 运动想象, 癫痫检测

TL;DR: 论文提出了一种名为DBConformer的双分支卷积Transformer网络，用于改善EEG解码中的长程依赖性和通道间关系建模，性能显著优于现有基线模型。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前基于CNN的EEG解码模型难以捕捉长程时间依赖和全局通道间关系，现有CNN-Transformer混合模型在局部和全局特征整合上表现不佳。

Method: 提出DBConformer，包含时间分支和空间分支的Conformer，分别建模长程时间依赖和通道间交互，并引入轻量级通道注意力模块优化空间表征。

Result: 在五个运动想象数据集和两个癫痫检测数据集上的实验表明，DBConformer性能优于10个基线模型，参数量仅为EEG Conformer的八分之一。

Conclusion: DBConformer在性能和解释性上表现优异，为稳健且可解释的EEG解码提供了可靠方案。

Abstract: Electroencephalography (EEG)-based brain-computer interfaces (BCIs) transform
spontaneous/evoked neural activity into control commands for external
communication. While convolutional neural networks (CNNs) remain the mainstream
backbone for EEG decoding, their inherently short receptive field makes it
difficult to capture long-range temporal dependencies and global inter-channel
relationships. Recent CNN-Transformer (Conformers) hybrids partially address
this issue, but most adopt a serial design, resulting in suboptimal integration
of local and global features, and often overlook explicit channel-wise
modeling. To address these limitations, we propose DBConformer, a dual-branch
convolutional Transformer network tailored for EEG decoding. It integrates a
temporal Conformer to model long-range temporal dependencies and a spatial
Conformer to extract inter-channel interactions, capturing both temporal
dynamics and spatial patterns in EEG signals. A lightweight channel attention
module further refines spatial representations by assigning data-driven
importance to EEG channels. Extensive experiments on five motor imagery (MI)
datasets and two seizure detection datasets under three evaluation settings
demonstrate that DBConformer consistently outperforms 10 competitive baseline
models, with over eight times fewer parameters than the high-capacity EEG
Conformer baseline. Further, the visualization results confirm that the
features extracted by DBConformer are physiologically interpretable and aligned
with sensorimotor priors in MI. The superior performance and interpretability
of DBConformer make it reliable for robust and explainable EEG decoding. Code
is publicized at https://github.com/wzwvv/DBConformer.

</details>


### [90] [Generative Adversarial Evasion and Out-of-Distribution Detection for UAV Cyber-Attacks](https://arxiv.org/abs/2506.21142)
*Deepak Kumar Panda,Weisi Guo*

Key words: 无人机（UAV）、入侵检测系统（IDS）、条件生成对抗网络（cGAN）、条件变分自编码器（CVAE）、对抗攻击

TL;DR: 摘要提出了一种基于条件生成对抗网络（cGAN）的框架，用于生成隐蔽的对抗攻击以规避无人机入侵检测系统（IDS），并提出条件变分自编码器（CVAE）来检测此类攻击。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着无人机越来越多地融入民用空域，传统的异常检测方法难以识别新型威胁，需要更智能和鲁棒的入侵检测系统来应对隐蔽的对抗攻击。

Method: 研究者设计了基于cGAN的框架，通过扰动已知攻击生成对抗样本，这些样本被误分类为良性数据，同时保持与异常分布的统计相似性；随后采用CVAE通过负对数似然来检测对抗样本。

Result: 实验表明，基于CVAE的遗憾评分在检测隐蔽对抗攻击时显著优于传统的马氏距离检测器。

Conclusion: 研究强调了高级概率建模对于增强入侵检测系统对抗基于生成模型的网络入侵能力的重要性。

Abstract: The growing integration of UAVs into civilian airspace underscores the need
for resilient and intelligent intrusion detection systems (IDS), as traditional
anomaly detection methods often fail to identify novel threats. A common
approach treats unfamiliar attacks as out-of-distribution (OOD) samples;
however, this leaves systems vulnerable when mitigation is inadequate.
Moreover, conventional OOD detectors struggle to distinguish stealthy
adversarial attacks from genuine OOD events. This paper introduces a
conditional generative adversarial network (cGAN)-based framework for crafting
stealthy adversarial attacks that evade IDS mechanisms. We first design a
robust multi-class IDS classifier trained on benign UAV telemetry and known
cyber-attacks, including Denial of Service (DoS), false data injection (FDI),
man-in-the-middle (MiTM), and replay attacks. Using this classifier, our cGAN
perturbs known attacks to generate adversarial samples that misclassify as
benign while retaining statistical resemblance to OOD distributions. These
adversarial samples are iteratively refined to achieve high stealth and success
rates. To detect such perturbations, we implement a conditional variational
autoencoder (CVAE), leveraging negative log-likelihood to separate adversarial
inputs from authentic OOD samples. Comparative evaluation shows that CVAE-based
regret scores significantly outperform traditional Mahalanobis distance-based
detectors in identifying stealthy adversarial threats. Our findings emphasize
the importance of advanced probabilistic modeling to strengthen IDS
capabilities against adaptive, generative-model-based cyber intrusions.

</details>


### [91] [Scalable Bayesian Low-Rank Adaptation of Large Language Models via Stochastic Variational Subspace Inference](https://arxiv.org/abs/2506.21408)
*Colin Samplawski,Adam D. Cobb,Manoj Acharya,Ramneet Kaur,Susmit Jha*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Despite their widespread use, large language models (LLMs) are known to
hallucinate incorrect information and be poorly calibrated. This makes the
uncertainty quantification of these models of critical importance, especially
in high-stakes domains, such as autonomy and healthcare. Prior work has made
Bayesian deep learning-based approaches to this problem more tractable by
performing inference over the low-rank adaptation (LoRA) parameters of a
fine-tuned model. While effective, these approaches struggle to scale to larger
LLMs due to requiring further additional parameters compared to LoRA. In this
work we present $\textbf{Scala}$ble $\textbf{B}$ayesian $\textbf{L}$ow-Rank
Adaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform
Bayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By
repurposing the LoRA parameters as projection matrices, we are able to map
samples from this subspace into the full weight space of the LLM. This allows
us to learn all the parameters of our approach using stochastic variational
inference. Despite the low dimensionality of our subspace, we are able to
achieve competitive performance with state-of-the-art approaches while only
requiring ${\sim}1000$ additional parameters. Furthermore, it allows us to
scale up to the largest Bayesian LLM to date, with four times as a many base
parameters as prior work.

</details>


### [92] [Personalized Federated Learning via Dual-Prompt Optimization and Cross Fusion](https://arxiv.org/abs/2506.21144)
*Yuguang Zhang,Kuangpu Guo,Zhihe Lu,Yunbo Wang,Jian Liang*

Key words: 联邦学习, 视觉语言模型, 双提示学习, 交叉融合, 个性化

TL;DR: 提出了一种基于双提示学习和交叉融合的个性化联邦学习框架pFedDC，通过全局和局部提示以及跨模态融合解决联邦学习中的数据异质性问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 联邦学习在数据、计算和通信方面的异质性带来挑战，预训练的视觉语言模型虽具潜力，但现有方法忽略了联合标签域分布变化。

Method: 设计了双提示学习和交叉融合模块，全局提示捕捉共享知识，局部提示编码客户端特有语义，跨融合模块集成不同层次的提示以生成个性化表示。

Result: 在九种异质性数据集上的实验表明，pFedDC优于现有方法。

Conclusion: pFedDC通过双提示学习和交叉融合有效解决联邦学习中的异质性问题，展现了优越性能。

Abstract: Federated learning (FL) enables collaborative model training across
decentralized clients without sharing local data, but is challenged by
heterogeneity in data, computation, and communication. Pretrained
vision-language models (VLMs), with their strong generalization and lightweight
tuning via prompts, offer a promising solution. However, existing federated
prompt-learning methods rely only on text prompts and overlook joint
label-domain distribution shifts. In this paper, we propose a personalized FL
framework based on dual-prompt learning and cross fusion, termed pFedDC.
Specifically, each client maintains both global and local prompts across vision
and language modalities: global prompts capture common knowledge shared across
the federation, while local prompts encode client-specific semantics and domain
characteristics. Meanwhile, a cross-fusion module is designed to adaptively
integrate prompts from different levels, enabling the model to generate
personalized representations aligned with each client's unique data
distribution. Extensive experiments across nine datasets with various types of
heterogeneity show that pFedDC consistently outperforms state-of-the-art
methods.

</details>


### [93] [Linearity-based neural network compression](https://arxiv.org/abs/2506.21146)
*Silas Dobler,Florian Lemmerich*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In neural network compression, most current methods reduce unnecessary
parameters by measuring importance and redundancy. To augment already highly
optimized existing solutions, we propose linearity-based compression as a novel
way to reduce weights in a neural network. It is based on the intuition that
with ReLU-like activation functions, neurons that are almost always activated
behave linearly, allowing for merging of subsequent layers. We introduce the
theory underlying this compression and evaluate our approach experimentally.
Our novel method achieves a lossless compression down to 1/4 of the original
model size in over the majority of tested models. Applying our method on
already importance-based pruned models shows very little interference between
different types of compression, demonstrating the option of successful
combination of techniques. Overall, our work lays the foundation for a new type
of compression method that enables smaller and ultimately more efficient neural
network models.

</details>


### [94] [Diverse Mini-Batch Selection in Reinforcement Learning for Efficient Chemical Exploration in de novo Drug Design](https://arxiv.org/abs/2506.21158)
*Hampus Gummesson Svensson,Ola Engkvist,Jon Paul Janet,Christian Tyrchan,Morteza Haghir Chehreghani*

Key words: 增强学习, 药物发现, 行列式点过程, 多样化探索, 小批量选择

TL;DR: 该论文提出了一种用于增强学习的多样化小批量选择方法，通过使用行列式点过程来提高探索效率和解决模式崩溃问题，特别是在药物发现领域展示了其效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在增强学习等应用中，评估实例的质量（如人类反馈或物理模拟）通常成本高昂，而提出新实例则相对容易。尤其是在药物发现中，需要多样化的解决方案来满足未解决的医疗需求。

Method: 论文提出使用行列式点过程（DPPs）进行多样化小批量选择，并将其应用于增强学习中，重点研究了其在药物发现中的应用。

Result: 实验结果表明，该方法能显著提高解决方案的多样性，同时保持高质量。

Conclusion: 多样化小批量选择框架在药物发现中具有潜力，可以更快地满足未解决的医疗需求。

Abstract: In many real-world applications, evaluating the goodness of instances is
often costly and time-consuming, e.g., human feedback and physics simulations,
in contrast to proposing new instances. In particular, this is even more
critical in reinforcement learning, as new interactions with the environment
(i.e., new instances) need to be evaluated to provide a reward signal to learn
from. As sufficient exploration is crucial, learning from a diverse mini-batch
can have a large impact and help mitigate mode collapse. In this paper, we
introduce diverse mini-batch selection for reinforcement learning and propose
to use determinantal point processes for this task. We study this framework in
the context of a real-world problem, namely drug discovery. We experimentally
study how our proposed framework can improve the effectiveness of chemical
exploration in de novo drug design, where finding diverse and high-quality
solutions is essential. We conduct a comprehensive evaluation with three
well-established molecular generation oracles over numerous generative steps.
Our experiments conclude that our diverse mini-batch selection framework can
substantially improve the diversity of the solutions, while still obtaining
solutions of high quality. In drug discovery, such outcome can potentially lead
to fulfilling unmet medication needs faster.

</details>


### [95] [Artificial Delegates Resolve Fairness Issues in Perpetual Voting with Partial Turnout](https://arxiv.org/abs/2506.21186)
*Apurva Shah,Axel Abels,Ann Nowé,Tom Lenaerts*

Key words: 持续性投票, 公平性, AI代理人, 缺席问题, 集体决策

TL;DR: 本文研究了通过引入AI代理人来弥补投票中缺席问题对公平性的影响，并验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决现实中投票系统因部分参与者缺席而导致的公平性和代表性不足的问题。

Method: 引入AI代理人（Artificial Delegates）学习缺席选民偏好，并将其整合到持续性投票系统中进行评估。

Result: 缺席显著影响公平性，但AI代理人能有效弥补这些影响并在多种场景中增强系统稳健性。

Conclusion: AI代理人是提升投票系统公平性和代表性的有效工具。

Abstract: Perpetual voting addresses fairness in sequential collective decision-making
by evaluating representational equity over time. However, existing perpetual
voting rules rely on full participation and complete approval information,
assumptions that rarely hold in practice, where partial turnout is the norm. In
this work, we study the integration of Artificial Delegates,
preference-learning agents trained to represent absent voters, into perpetual
voting systems. We examine how absenteeism affects fairness and
representativeness under various voting methods and evaluate the extent to
which Artificial Delegates can compensate for missing participation. Our
findings indicate that while absenteeism significantly affects fairness,
Artificial Delegates reliably mitigate these effects and enhance robustness
across diverse scenarios.

</details>


### [96] [Zero-Shot Learning for Obsolescence Risk Forecasting](https://arxiv.org/abs/2506.21240)
*Elie Saad,Aya Mrabah,Mariem Besbes,Marc Zolghadri,Victor Czmil,Claude Baron,Vincent Bourgeois*

Key words: 零样本学习, 大语言模型, 过时风险预测, 电子元件

TL;DR: 提出一种基于零样本学习和大语言模型的新方法，用于预测电子元件过时风险，解决数据不足问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 电子元件过时对依赖这些元件的行业带来高成本和系统安全性、可用性中断问题，而现有可靠数据不足限制了风险预测的准确性。

Method: 采用零样本学习（ZSL）结合大语言模型（LLMs），从表格数据中提取领域特定知识，用于预测过时风险。

Result: 在两个真实数据集上的应用表明该方法能有效预测风险，并通过比较四种LLMs强调了选择合适模型的重要性。

Conclusion: 提出的ZSL结合LLMs的方法能有效解决数据不足问题，为过时风险预测提供新思路。

Abstract: Component obsolescence poses significant challenges in industries reliant on
electronic components, causing increased costs and disruptions in the security
and availability of systems. Accurate obsolescence risk prediction is essential
but hindered by a lack of reliable data. This paper proposes a novel approach
to forecasting obsolescence risk using zero-shot learning (ZSL) with large
language models (LLMs) to address data limitations by leveraging
domain-specific knowledge from tabular datasets. Applied to two real-world
datasets, the method demonstrates effective risk prediction. A comparative
evaluation of four LLMs underscores the importance of selecting the right model
for specific forecasting tasks.

</details>


### [97] [Improved seeding strategies for k-means and k-GMM](https://arxiv.org/abs/2506.21291)
*Guillaume Carrière,Frédéric Cazals*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We revisit the randomized seeding techniques for k-means clustering and k-GMM
(Gaussian Mixture model fitting with Expectation-Maximization), formalizing
their three key ingredients: the metric used for seed sampling, the number of
candidate seeds, and the metric used for seed selection. This analysis yields
novel families of initialization methods exploiting a lookahead
principle--conditioning the seed selection to an enhanced coherence with the
final metric used to assess the algorithm, and a multipass strategy to tame
down the effect of randomization.
  Experiments show a consistent constant factor improvement over classical
contenders in terms of the final metric (SSE for k-means, log-likelihood for
k-GMM), at a modest overhead. In particular, for k-means, our methods improve
on the recently designed multi-swap strategy, which was the first one to
outperform the greedy k-means++ seeding.
  Our experimental analysis also shed light on subtle properties of k-means
often overlooked, including the (lack of) correlations between the SSE upon
seeding and the final SSE, the variance reduction phenomena observed in
iterative seeding methods, and the sensitivity of the final SSE to the pool
size for greedy methods.
  Practically, our most effective seeding methods are strong candidates to
become one of the--if not the--standard techniques. From a theoretical
perspective, our formalization of seeding opens the door to a new line of
analytical approaches.

</details>


### [98] [AGTCNet: A Graph-Temporal Approach for Principled Motor Imagery EEG Classification](https://arxiv.org/abs/2506.21338)
*Galvin Brice S. Lim,Brian Godwin S. Lim,Argel A. Bandala,John Anthony C. Jose,Timothy Scott C. Chu,Edwin Sybingco*

Key words: 脑机接口, EEG, 运动想象分类, 图卷积网络

TL;DR: 提出了一种名为AGTCNet的新型图-时序卷积网络，用于运动想象脑电信号分类，显著优于现有方法，并提升了解码效率和实用性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 脑机接口(BCI)在运动障碍患者中应用潜力巨大，但跨受试者和跨会话的复杂性和变异性导致现有系统效果不佳。

Method: 采用AGTCNet模型，结合图卷积注意力网络(GCAT)和脑电图电极拓扑结构学习时空EEG表征。

Result: AGTCNet在BCI Competition IV和EEG Motor Movement/Imagery数据集上表现优异，模型尺寸减少49.87%，推理时间加快64.65%。

Conclusion: AGTCNet通过高效学习时空特征，显著提升了脑电信号分类性能，显示出其在实际BCI部署中的潜力。

Abstract: Brain-computer interface (BCI) technology utilizing electroencephalography
(EEG) marks a transformative innovation, empowering motor-impaired individuals
to engage with their environment on equal footing. Despite its promising
potential, developing subject-invariant and session-invariant BCI systems
remains a significant challenge due to the inherent complexity and variability
of neural activity across individuals and over time, compounded by EEG hardware
constraints. While prior studies have sought to develop robust BCI systems,
existing approaches remain ineffective in capturing the intricate
spatiotemporal dependencies within multichannel EEG signals. This study
addresses this gap by introducing the attentive graph-temporal convolutional
network (AGTCNet), a novel graph-temporal model for motor imagery EEG (MI-EEG)
classification. Specifically, AGTCNet leverages the topographic configuration
of EEG electrodes as an inductive bias and integrates graph convolutional
attention network (GCAT) to jointly learn expressive spatiotemporal EEG
representations. The proposed model significantly outperformed existing MI-EEG
classifiers, achieving state-of-the-art performance while utilizing a compact
architecture, underscoring its effectiveness and practicality for BCI
deployment. With a 49.87% reduction in model size, 64.65% faster inference
time, and shorter input EEG signal, AGTCNet achieved a moving average accuracy
of 66.82% for subject-independent classification on the BCI Competition IV
Dataset 2a, which further improved to 82.88% when fine-tuned for
subject-specific classification. On the EEG Motor Movement/Imagery Dataset,
AGTCNet achieved moving average accuracies of 64.14% and 85.22% for 4-class and
2-class subject-independent classifications, respectively, with further
improvements to 72.13% and 90.54% for subject-specific classifications.

</details>


### [99] [DynamicBench: Evaluating Real-Time Report Generation in Large Language Models](https://arxiv.org/abs/2506.21343)
*Jingyao Li,Hao Sun,Zile Qiao,Yong Jiang,Pengjun Xie,Fei Huang,Hong Xu,Jiaya Jia*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Traditional benchmarks for large language models (LLMs) typically rely on
static evaluations through storytelling or opinion expression, which fail to
capture the dynamic requirements of real-time information processing in
contemporary applications. To address this limitation, we present DynamicBench,
a benchmark designed to evaluate the proficiency of LLMs in storing and
processing up-to-the-minute data. DynamicBench utilizes a dual-path retrieval
pipeline, integrating web searches with local report databases. It necessitates
domain-specific knowledge, ensuring accurate responses report generation within
specialized fields. By evaluating models in scenarios that either provide or
withhold external documents, DynamicBench effectively measures their capability
to independently process recent information or leverage contextual
enhancements. Additionally, we introduce an advanced report generation system
adept at managing dynamic information synthesis. Our experimental results
confirm the efficacy of our approach, with our method achieving
state-of-the-art performance, surpassing GPT4o in document-free and
document-assisted scenarios by 7.0% and 5.8%, respectively. The code and data
will be made publicly available.

</details>


### [100] [Lipschitz Bounds for Persistent Laplacian Eigenvalues under One-Simplex Insertions](https://arxiv.org/abs/2506.21352)
*Le Vu Anh,Mehmet Dik,Nguyen Viet Anh*

Key words: 持久拉普拉斯算子, 特征值稳定性, 拓扑数据分析, Lipschitz界限, 动态数据

TL;DR: 该论文研究了持久拉普拉斯算子的特征值稳定性，证明了单个单纯形（如顶点、边或三角形）的加入对特征值的影响上限，为拓扑数据分析提供了首个特征值级别的鲁棒性保证。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 持久拉普拉斯算子的特征值是描述数据几何和拓扑特性的重要工具，但其在单个单纯形加入时的变化尚未明确。填补这一空白对于依赖特征值的后续应用（如热核签名和谱神经网络）至关重要。

Method: 通过理论证明，论文确立了插入一个单纯形后，每个上升持久拉普拉斯算子特征值的变化上限不超过该单纯形边界欧几里得范数的两倍。

Result: 研究证明了一个统一的Lipschitz界限，即特征值在局部更新中具有稳定性，适用于动态数据场景。

Conclusion: 该结果为谱拓扑数据分析提供了首个特征值级别的鲁棒性保证，支持了局部更新下的特征值稳定性和动态数据的可靠误差控制。

Abstract: Persistent Laplacians are matrix operators that track how the shape and
structure of data transform across scales and are popularly adopted in biology,
physics, and machine learning. Their eigenvalues are concise descriptors of
geometric and topological features in a filtration. Although earlier work
established global algebraic stability for these operators, the precise change
in a single eigenvalue when one simplex, such as a vertex, edge, or triangle,
is added has remained unknown. This is important because downstream tools,
including heat-kernel signatures and spectral neural networks, depend directly
on these eigenvalues. We close this gap by proving a uniform Lipschitz bound:
after inserting one simplex, every up-persistent Laplacian eigenvalue can vary
by at most twice the Euclidean norm of that simplex's boundary, independent of
filtration scale and complex size. This result delivers the first
eigenvalue-level robustness guarantee for spectral topological data analysis.
It guarantees that spectral features remain stable under local updates and
enables reliable error control in dynamic data settings.

</details>


### [101] [SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning](https://arxiv.org/abs/2506.21355)
*Melanie Rieff,Maya Varma,Ossian Rabow,Subathra Adithan,Julie Kim,Ken Chang,Hannah Lee,Nidhi Rohatgi,Christian Bluethgen,Mohamed S. Muneer,Jean-Benoit Delbrouck,Michael Moor*

Key words: 多模态上下文学习、医学任务、SMMILE基准、多模态大型语言模型

TL;DR: 研究者提出了首个专家驱动的多模态上下文学习基准SMMILE，用于医学任务。尽管多模态大型语言模型在医学视觉问答中表现优异，但在上下文学习能力上表现中等至较差。研究发现，即使少量无关样本也会显著降低性能，且样本顺序存在“近期偏见”。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索多模态上下文学习（ICL）在医学领域的潜力，因为临床医生常需从有限示例中快速适应多样化任务。

Method: 引入SMMILE基准，包含111个由医学专家设计的问题（涉及6个医学专业和13种成像模态），并评估15种多模态大型语言模型的ICL能力。

Result: 多数模型在医学任务中表现中等至较差；ICL对性能的提升有限（8%-9.4%），且易受无关样本和样本顺序影响。

Conclusion: 当前多模态大型语言模型在医学任务上下文学习中存在显著局限性和偏见。

Abstract: Multimodal in-context learning (ICL) remains underexplored despite
significant potential for domains such as medicine. Clinicians routinely
encounter diverse, specialized tasks requiring adaptation from limited
examples, such as drawing insights from a few relevant prior cases or
considering a constrained set of differential diagnoses. While multimodal large
language models (MLLMs) have shown advances in medical visual question
answering (VQA), their ability to learn multimodal tasks from context is
largely unknown. We introduce SMMILE, the first expert-driven multimodal ICL
benchmark for medical tasks. Eleven medical experts curated problems, each
including a multimodal query and multimodal in-context examples as task
demonstrations. SMMILE encompasses 111 problems (517 question-image-answer
triplets) covering 6 medical specialties and 13 imaging modalities. We further
introduce SMMILE++, an augmented variant with 1038 permuted problems. A
comprehensive evaluation of 15 MLLMs demonstrates that most models exhibit
moderate to poor multimodal ICL ability in medical tasks. In open-ended
evaluations, ICL contributes only 8% average improvement over zero-shot on
SMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant
in-context examples: even a single noisy or irrelevant example can degrade
performance by up to 9.5%. Moreover, example ordering exhibits a recency bias,
i.e., placing the most relevant example last can lead to substantial
performance improvements by up to 71%. Our findings highlight critical
limitations and biases in current MLLMs when learning multimodal medical tasks
from context.

</details>


### [102] [rQdia: Regularizing Q-Value Distributions With Image Augmentation](https://arxiv.org/abs/2506.21367)
*Sam Lerman,Jing Bi*

Key words: 强化学习, Q值分布, 正则化, 像素学习, 样本效率

TL;DR: rQdia通过增强图像正则化Q值分布，提升了多个强化学习任务的性能，尤其在样本效率和长期训练方面表现突出。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决像素基强化学习中Q值分布的不稳定性问题，提升模型性能。

Method: 引入辅助损失函数（MSE）以均衡增强图像的Q值分布。

Result: 在MuJoCo和Atari任务中显著提升了性能，超越了状态编码基线。

Conclusion: rQdia是一种简单有效的正则化方法，适用于像素基强化学习。

Abstract: rQdia regularizes Q-value distributions with augmented images in pixel-based
deep reinforcement learning. With a simple auxiliary loss, that equalizes these
distributions via MSE, rQdia boosts DrQ and SAC on 9/12 and 10/12 tasks
respectively in the MuJoCo Continuous Control Suite from pixels, and
Data-Efficient Rainbow on 18/26 Atari Arcade environments. Gains are measured
in both sample efficiency and longer-term training. Moreover, the addition of
rQdia finally propels model-free continuous control from pixels over the state
encoding baseline.

</details>


### [103] [MAx-DNN: Multi-Level Arithmetic Approximation for Energy-Efficient DNN Hardware Accelerators](https://arxiv.org/abs/2506.21371)
*Vasileios Leon,Georgios Makris,Sotirios Xydis,Kiamal Pekmestzi,Dimitrios Soudris*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Nowadays, the rapid growth of Deep Neural Network (DNN) architectures has
established them as the defacto approach for providing advanced Machine
Learning tasks with excellent accuracy. Targeting low-power DNN computing, this
paper examines the interplay of fine-grained error resilience of DNN workloads
in collaboration with hardware approximation techniques, to achieve higher
levels of energy efficiency. Utilizing the state-of-the-art ROUP approximate
multipliers, we systematically explore their fine-grained distribution across
the network according to our layer-, filter-, and kernel-level approaches, and
examine their impact on accuracy and energy. We use the ResNet-8 model on the
CIFAR-10 dataset to evaluate our approximations. The proposed solution delivers
up to 54% energy gains in exchange for up to 4% accuracy loss, compared to the
baseline quantized model, while it provides 2x energy gains with better
accuracy versus the state-of-the-art DNN approximations.

</details>


### [104] [Pay Attention to Small Weights](https://arxiv.org/abs/2506.21374)
*Chao Zhou,Tom Jacobs,Advait Gadhikar,Rebekka Burkholz*

Key words: finetuning, gradient-free, NANOADAM, generalization

TL;DR: 提出了一种动态更新小权重的方法NANOADAM，以减少finetuning时的资源消耗，同时避免灾难性遗忘并提升泛化性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: finetuning大型预训练神经网络资源消耗高，且大梯度常与小权重相关。

Method: 通过动态更新小权重（不依赖梯度计算），保留大权重以保护预训练特征。

Result: 在NLP和视觉任务中表现出更好的泛化性能，支持更大的学习率。

Conclusion: NANOADAM是一种高效且实用的finetuning方法。

Abstract: Finetuning large pretrained neural networks is known to be
resource-intensive, both in terms of memory and computational cost. To mitigate
this, a common approach is to restrict training to a subset of the model
parameters. By analyzing the relationship between gradients and weights during
finetuning, we observe a notable pattern: large gradients are often associated
with small-magnitude weights. This correlation is more pronounced in finetuning
settings than in training from scratch. Motivated by this observation, we
propose NANOADAM, which dynamically updates only the small-magnitude weights
during finetuning and offers several practical advantages: first, this
criterion is gradient-free -- the parameter subset can be determined without
gradient computation; second, it preserves large-magnitude weights, which are
likely to encode critical features learned during pretraining, thereby reducing
the risk of catastrophic forgetting; thirdly, it permits the use of larger
learning rates and consistently leads to better generalization performance in
experiments. We demonstrate this for both NLP and vision tasks.

</details>


### [105] [Temporal-Aware Graph Attention Network for Cryptocurrency Transaction Fraud Detection](https://arxiv.org/abs/2506.21382)
*Zhi Zheng,Bochuan Zhou,Yuping Song*

Key words: 加密货币欺诈检测,图注意力网络,时序嵌入,类别不平衡,三重注意力机制

TL;DR: 提出了一种增强型时序感知图注意力网络（ATGAT），通过多模块设计提升加密货币交易欺诈检测性能，显著优于传统方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 加密货币交易欺诈检测面临复杂交易模式与严重类别不平衡的双重挑战，传统方法难以捕捉交易网络中的时序与结构依赖。

Method: 1. 多尺度时间差特征与周期位置编码融合的时序嵌入模块；2. 结构、时序与全局上下文注意力联合优化的三重注意力机制；3. 加权BCE损失解决类别不平衡。

Result: 在Elliptic++数据集上AUC达0.9130，较XGBoost提升9.2%，GCN提升12.0%，GAT提升10.0%。

Conclusion: ATGAT验证了时序感知与三重注意力机制对图神经网络的增强效果，为金融机构提供了更可靠的欺诈检测工具，其设计原则可推广至其他时序图异常检测任务。

Abstract: Cryptocurrency transaction fraud detection faces the dual challenges of
increasingly complex transaction patterns and severe class imbalance.
Traditional methods rely on manual feature engineering and struggle to capture
temporal and structural dependencies in transaction networks. This paper
proposes an Augmented Temporal-aware Graph Attention Network (ATGAT) that
enhances detection performance through three modules: (1) designing an advanced
temporal embedding module that fuses multi-scale time difference features with
periodic position encoding; (2) constructing a temporal-aware triple attention
mechanism that jointly optimizes structural, temporal, and global context
attention; (3) employing weighted BCE loss to address class imbalance.
Experiments on the Elliptic++ cryptocurrency dataset demonstrate that ATGAT
achieves an AUC of 0.9130, representing a 9.2% improvement over the best
traditional method XGBoost, 12.0% over GCN, and 10.0% over standard GAT. This
method not only validates the enhancement effect of temporal awareness and
triple attention mechanisms on graph neural networks, but also provides
financial institutions with more reliable fraud detection tools, with its
design principles generalizable to other temporal graph anomaly detection
tasks.

</details>


### [106] [Early Stopping Tabular In-Context Learning](https://arxiv.org/abs/2506.21387)
*Jaris Küken,Lennart Purucker,Frank Hutter*

Key words: 表格基础模型, 上下文学习, 早期停止, 推理加速, Transformer

TL;DR: 这篇论文提出了一种通过动态早期停止上下文学习来加速表格基础模型推理的方法，在小规模任务上实现了1.3倍的加速，大规模任务上达到2.2倍，且性能几乎无损失。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 表格基础模型在上下文学习中表现出色，但推理成本较高，尤其是大规模数据集。为了解决这一问题，论文提出了动态早期停止上下文学习的方法。

Method: 通过在每个Transformer编码层后动态评估是否停止上下文学习，一旦停止，使用预训练的逐层解码器解码嵌入。

Result: 在34个小规模分类任务上，实现了1.3倍的推理加速；在5个大规模任务上，加速达到2.2倍，且预测性能几乎未下降。

Conclusion: 早期退出是一种高效且实用的策略，可以显著提升表格上下文学习的效率。

Abstract: Tabular foundation models have shown strong performance across various
tabular learning tasks via in-context learning, offering robust generalization
without any downstream finetuning. However, their inference-time costs remain
high, particularly for larger datasets. To address this, we propose
early-stopping the in-context learning process. We achieve this by dynamically
evaluating whether to stop in-context learning after each Transformer encoder
layer. Once stopped, we decode the embedding using a pre-trained layer-wise
decoder. Experiments across 34 small classification tasks size show that early
stopping in-context learning accelerates inference by up to x1.3 with
negligible degradation in predictive performance. To assess scalability, we
further evaluate our method on five larger classification tasks, achieving
speedups of up to x2.2. Our results demonstrate the potential of early exiting
as an effective and practical strategy for improving the efficiency of tabular
in-context learning.

</details>


### [107] [Distributed Cross-Channel Hierarchical Aggregation for Foundation Models](https://arxiv.org/abs/2506.21411)
*Aristeidis Tsaris,Isaac Lyngaas,John Lagregren,Mohamed Wahib,Larry York,Prasanna Balaprakash,Dan Lu,Feiyi Wang,Xiao Wang*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Vision-based scientific foundation models hold significant promise for
advancing scientific discovery and innovation. This potential stems from their
ability to aggregate images from diverse sources such as varying physical
groundings or data acquisition systems and to learn spatio-temporal
correlations using transformer architectures. However, tokenizing and
aggregating images can be compute-intensive, a challenge not fully addressed by
current distributed methods. In this work, we introduce the Distributed
Cross-Channel Hierarchical Aggregation (D-CHAG) approach designed for datasets
with a large number of channels across image modalities. Our method is
compatible with any model-parallel strategy and any type of vision transformer
architecture, significantly improving computational efficiency. We evaluated
D-CHAG on hyperspectral imaging and weather forecasting tasks. When integrated
with tensor parallelism and model sharding, our approach achieved up to a 75%
reduction in memory usage and more than doubled sustained throughput on up to
1,024 AMD GPUs on the Frontier Supercomputer.

</details>


### [108] [Flow-Based Single-Step Completion for Efficient and Expressive Policy Learning](https://arxiv.org/abs/2506.21427)
*Prajwal Koirala,Cody Fleming*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Generative models such as diffusion and flow-matching offer expressive
policies for offline reinforcement learning (RL) by capturing rich, multimodal
action distributions, but their iterative sampling introduces high inference
costs and training instability due to gradient propagation across sampling
steps. We propose the \textit{Single-Step Completion Policy} (SSCP), a
generative policy trained with an augmented flow-matching objective to predict
direct completion vectors from intermediate flow samples, enabling accurate,
one-shot action generation. In an off-policy actor-critic framework, SSCP
combines the expressiveness of generative models with the training and
inference efficiency of unimodal policies, without requiring long
backpropagation chains. Our method scales effectively to offline,
offline-to-online, and online RL settings, offering substantial gains in speed
and adaptability over diffusion-based baselines. We further extend SSCP to
goal-conditioned RL, enabling flat policies to exploit subgoal structures
without explicit hierarchical inference. SSCP achieves strong results across
standard offline RL and behavior cloning benchmarks, positioning it as a
versatile, expressive, and efficient framework for deep RL and sequential
decision-making.

</details>


### [109] [Deception Detection in Dyadic Exchanges Using Multimodal Machine Learning: A Study on a Swedish Cohort](https://arxiv.org/abs/2506.21429)
*Franco Rugolon,Thomas Jack Samuels,Stephan Hau,Lennart Högman*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This study investigates the efficacy of using multimodal machine learning
techniques to detect deception in dyadic interactions, focusing on the
integration of data from both the deceiver and the deceived. We compare early
and late fusion approaches, utilizing audio and video data - specifically,
Action Units and gaze information - across all possible combinations of
modalities and participants. Our dataset, newly collected from Swedish native
speakers engaged in truth or lie scenarios on emotionally relevant topics,
serves as the basis for our analysis. The results demonstrate that
incorporating both speech and facial information yields superior performance
compared to single-modality approaches. Moreover, including data from both
participants significantly enhances deception detection accuracy, with the best
performance (71%) achieved using a late fusion strategy applied to both
modalities and participants. These findings align with psychological theories
suggesting differential control of facial and vocal expressions during initial
interactions. As the first study of its kind on a Scandinavian cohort, this
research lays the groundwork for future investigations into dyadic
interactions, particularly within psychotherapy settings.

</details>


### [110] [Towards an Optimal Control Perspective of ResNet Training](https://arxiv.org/abs/2506.21453)
*Jens Püttschneider,Simon Heilig,Asja Fischer,Timm Faulwasser*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose a training formulation for ResNets reflecting an optimal control
problem that is applicable for standard architectures and general loss
functions. We suggest bridging both worlds via penalizing intermediate outputs
of hidden states corresponding to stage cost terms in optimal control. For
standard ResNets, we obtain intermediate outputs by propagating the state
through the subsequent skip connections and the output layer. We demonstrate
that our training dynamic biases the weights of the unnecessary deeper residual
layers to vanish. This indicates the potential for a theory-grounded layer
pruning strategy.

</details>


### [111] [A Keyword-Based Technique to Evaluate Broad Question Answer Script](https://arxiv.org/abs/2506.21461)
*Tamim Al Mahmud,Md Gulzar Hussain,Sumaiya Kabir,Hasnain Ahmad,Mahmudus Sobhan*

Key words: 评估系统,主观答题卷,关键词提取,语法检查

TL;DR: 提出了一种高效评估主观答题卷的电子系统，通过关键词提取和语法检查实现自动化评分。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决主观答题卷的电子化评估问题，提高评分效率和准确性。

Method: 集成系统提取答题卷关键词并与开放/闭域关键词对比，同时检查语法和拼写错误。

Result: 在100名学生答题卷上测试，系统精确度为0.91。

Conclusion: 该系统能有效评估主观答题卷，具有高精确度。

Abstract: Evaluation is the method of assessing and determining the educational system
through various techniques such as verbal or viva-voice test, subjective or
objective written test. This paper presents an efficient solution to evaluate
the subjective answer script electronically. In this paper, we proposed and
implemented an integrated system that examines and evaluates the written answer
script. This article focuses on finding the keywords from the answer script and
then compares them with the keywords that have been parsed from both open and
closed domain. The system also checks the grammatical and spelling errors in
the answer script. Our proposed system tested with answer scripts of 100
students and gives precision score 0.91.

</details>


### [112] [Optimising 4th-Order Runge-Kutta Methods: A Dynamic Heuristic Approach for Efficiency and Low Storage](https://arxiv.org/abs/2506.21465)
*Gavin Lee Goodship,Luis Miralles-Pechuan,Stephen O'Sullivan*

Key words: 扩展稳定性Runge-Kutta（ESRK）、遗传算法（GA）、强化学习（RL）、低存储方法、计算效率

TL;DR: 论文提出了一种结合遗传算法（GA）和强化学习（RL）的自动启发式方法，用于优化低存储扩展稳定性Runge-Kutta（ESRK）方法，显著提高计算效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在科学和工程中的大规模计算问题（如天气预报、空气动力学分析和复杂生物建模）中，平衡精度、稳定性和计算效率仍然是一个挑战，特别是在高阶低存储方案中。

Method: 采用GA驱动的突变进行搜索空间探索，并结合RL启发的状态转移机制动态优化启发式选择，从而系统减少参数并保持四阶精度。

Result: 在基准测试中，最佳启发式方法在保持数值稳定性和精度的同时，比传统ESRK优化过程减少了25%的IPOPT运行时间。

Conclusion: 该框架展示了自适应启发式发现在高保真模拟中提高资源效率的潜力，并为低存储Runge-Kutta方法在计算流体动力学等领域的应用开辟了新途径。

Abstract: Extended Stability Runge-Kutta (ESRK) methods are crucial for solving
large-scale computational problems in science and engineering, including
weather forecasting, aerodynamic analysis, and complex biological modelling.
However, balancing accuracy, stability, and computational efficiency remains
challenging, particularly for high-order, low-storage schemes. This study
introduces a hybrid Genetic Algorithm (GA) and Reinforcement Learning (RL)
approach for automated heuristic discovery, optimising low-storage ESRK
methods. Unlike traditional approaches that rely on manually designed
heuristics or exhaustive numerical searches, our method leverages GA-driven
mutations for search-space exploration and an RL-inspired state transition
mechanism to refine heuristic selection dynamically. This enables systematic
parameter reduction, preserving fourth-order accuracy while significantly
improving computational efficiency.The proposed GA-RL heuristic optimisation
framework is validated through rigorous testing on benchmark problems,
including the 1D and 2D Brusselator systems and the steady-state Navier-Stokes
equations. The best-performing heuristic achieves a 25\% reduction in IPOPT
runtime compared to traditional ESRK optimisation processes while maintaining
numerical stability and accuracy. These findings demonstrate the potential of
adaptive heuristic discovery to improve resource efficiency in high-fidelity
simulations and broaden the applicability of low-storage Runge-Kutta methods in
real-world computational fluid dynamics, physics simulations, and other
demanding fields. This work establishes a new paradigm in heuristic
optimisation for numerical methods, opening pathways for further exploration
using Deep RL and AutoML-based heuristic search

</details>


### [113] [Devising a solution to the problems of Cancer awareness in Telangana](https://arxiv.org/abs/2506.21500)
*Priyanka Avhad,Vedanti Kshirsagar,Urvi Ranjan,Mahek Nakhua*

Key words: 机器学习, 乳腺癌, 宫颈癌, 筛查, 分类模型

TL;DR: 开发了一个基于机器学习的分类模型，用于预测乳腺癌和宫颈癌的易感性，并提供就近医院建议，以提高癌症筛查意识和降低死亡率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于Telangana地区女性对乳腺癌和宫颈癌的筛查率和意识较低，开发了一种解决方案以提升癌症素养和早期检测率。

Method: 使用决策树分类和支持向量分类算法，基于人口统计学因素预测癌症易感性，并结合地理位置提供医疗服务建议。

Result: 模型成功预测了乳腺癌和宫颈癌的易感性，并设计了医疗服务推荐系统。

Conclusion: 该解决方案有助于提高癌症意识，减少死亡率，并提升Telangana地区的癌症素养。

Abstract: According to the data, the percent of women who underwent screening for
cervical cancer, breast and oral cancer in Telangana in the year 2020 was 3.3
percent, 0.3 percent and 2.3 percent respectively. Although early detection is
the only way to reduce morbidity and mortality, people have very low awareness
about cervical and breast cancer signs and symptoms and screening practices. We
developed an ML classification model to predict if a person is susceptible to
breast or cervical cancer based on demographic factors. We devised a system to
provide suggestions for the nearest hospital or Cancer treatment centres based
on the users location or address. In addition to this, we can integrate the
health card to maintain medical records of all individuals and conduct
awareness drives and campaigns. For ML classification models, we used decision
tree classification and support vector classification algorithms for cervical
cancer susceptibility and breast cancer susceptibility respectively. Thus, by
devising this solution we come one step closer to our goal which is spreading
cancer awareness, thereby, decreasing the cancer mortality and increasing
cancer literacy among the people of Telangana.

</details>


### [114] [Process mining-driven modeling and simulation to enhance fault diagnosis in cyber-physical systems](https://arxiv.org/abs/2506.21502)
*Francesco Vitale,Nicola Dall'Ora,Sebastiano Gaiardelli,Enrico Fraccaroli,Nicola Mazzocca,Franco Fummi*

Key words: 故障诊断, CPS, 无监督学习, 流程挖掘, 随机模拟

TL;DR: 提出了一种新的无监督故障诊断方法，结合多变量时间序列、流程挖掘和随机模拟，用于CPS中的故障建模与分类。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决CPS中故障诊断需要大量领域知识且模型复杂、难以解释的问题。

Method: 通过多变量时间序列分析检测异常，转化为事件日志后利用流程挖掘生成可解释模型，再通过随机模拟增强根因分析。

Result: 在Robotic Arm Dataset上验证了方法的有效性，支持故障行为的建模、模拟和分类。

Conclusion: 该方法能够生成全面的故障词典，支持预测性维护和工业环境中的数字孪生开发。

Abstract: Fault diagnosis in Cyber-Physical Systems (CPSs) is essential for ensuring
system dependability and operational efficiency by accurately detecting
anomalies and identifying their root causes. However, the manual modeling of
faulty behaviors often demands extensive domain expertise and produces models
that are complex, error-prone, and difficult to interpret. To address this
challenge, we present a novel unsupervised fault diagnosis methodology that
integrates collective anomaly detection in multivariate time series, process
mining, and stochastic simulation. Initially, collective anomalies are detected
from low-level sensor data using multivariate time-series analysis. These
anomalies are then transformed into structured event logs, enabling the
discovery of interpretable process models through process mining. By
incorporating timing distributions into the extracted Petri nets, the approach
supports stochastic simulation of faulty behaviors, thereby enhancing root
cause analysis and behavioral understanding. The methodology is validated using
the Robotic Arm Dataset (RoAD), a widely recognized benchmark in smart
manufacturing. Experimental results demonstrate its effectiveness in modeling,
simulating, and classifying faulty behaviors in CPSs. This enables the creation
of comprehensive fault dictionaries that support predictive maintenance and the
development of digital twins for industrial environments.

</details>


### [115] [mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and Model Selection at Scale](https://arxiv.org/abs/2506.21550)
*Xiaona Zhou,Constantin Brif,Ismini Lourentzou*

Key words: 多变量时间序列,异常检测,无监督模型选择,基准测试,大语言模型

TL;DR: mTSBench是目前最大的多变量时间序列异常检测（MTS-AD）和无监督模型选择基准，评估了24种方法，发现现有选择方法仍有不足，为未来研究提供了统一评估工具。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 多变量时间序列异常检测在多个领域至关重要，但面临复杂依赖关系、时间动态和稀疏标签的挑战，需要更好的评估和模型选择方法。

Method: 引入mTSBench基准，覆盖19个数据集的344条时间序列，评估24种异常检测方法（包括基于大语言模型的方法），并系统比较无监督模型选择技术。

Result: 结果证实无单一检测器在所有数据集表现最佳，且当前选择方法与最优解差距显著。

Conclusion: mTSBench为未来自适应异常检测和鲁棒模型选择提供了严谨、可重复的评估框架。

Abstract: Multivariate time series anomaly detection (MTS-AD) is critical in domains
like healthcare, cybersecurity, and industrial monitoring, yet remains
challenging due to complex inter-variable dependencies, temporal dynamics, and
sparse anomaly labels. We introduce mTSBench, the largest benchmark to date for
MTS-AD and unsupervised model selection, spanning 344 labeled time series
across 19 datasets and 12 diverse application domains. mTSBench evaluates 24
anomaly detection methods, including large language model (LLM)-based detectors
for multivariate time series, and systematically benchmarks unsupervised model
selection techniques under standardized conditions. Consistent with prior
findings, our results confirm that no single detector excels across datasets,
underscoring the importance of model selection. However, even state-of-the-art
selection methods remain far from optimal, revealing critical gaps. mTSBench
provides a unified evaluation suite to enable rigorous, reproducible
comparisons and catalyze future advances in adaptive anomaly detection and
robust model selection.

</details>


### [116] [Where to find Grokking in LLM Pretraining? Monitor Memorization-to-Generalization without Test](https://arxiv.org/abs/2506.21551)
*Ziyue Li,Chenrui Fan,Tianyi Zhou*

Key words: 顿悟、大型语言模型、预训练、泛化能力、记忆转泛化

TL;DR: 论文研究了在大型语言模型（OLMoE）的单次预训练中发生的“顿悟”现象，揭示了泛化能力延迟出现的机制。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索神经网络训练中泛化能力延迟出现的“顿悟”现象，尤其是在大规模基础模型（如7B参数的LLM）预训练中的表现。

Method: 通过分析训练损失和多样性基准任务（如数学推理、代码生成和知识检索任务）的泛化表现，研究“顿悟”现象。量化样本路径的距离和复杂性，预测泛化改进。

Result: 研究发现“顿悟”现象在大规模预训练中依然存在，揭示了从记忆到泛化的转换机制。开发的新指标能有效预测泛化能力。

Conclusion: 研究为理解“顿悟”现象提供了机制性解释，新指标可用于监控预训练中的泛化表现，无需微调和测试。

Abstract: Grokking, i.e., test performance keeps improving long after training loss
converged, has been recently witnessed in neural network training, making the
mechanism of generalization and other emerging capabilities such as reasoning
mysterious. While prior studies usually train small models on a few toy or
highly-specific tasks for thousands of epochs, we conduct the first study of
grokking on checkpoints during one-pass pretraining of a 7B large language
model (LLM), i.e., OLMoE. We compute the training loss and evaluate
generalization on diverse benchmark tasks, including math reasoning, code
generation, and commonsense/domain-specific knowledge retrieval tasks.
  Our study, for the first time, verifies that grokking still happens in the
pretraining of large-scale foundation models, though different data may enter
grokking stages asynchronously. We further demystify grokking's "emergence of
generalization" by investigating LLM internal dynamics. Specifically, we find
that training samples' pathways (i.e., expert choices across layers) evolve
from random, instance-specific to more structured and shareable between samples
during grokking. Also, the complexity of a sample's pathway reduces despite the
converged loss. These indicate a memorization-to-generalization conversion,
providing a mechanistic explanation of delayed generalization. In the study, we
develop two novel metrics to quantify pathway distance and the complexity of a
single pathway. We show their ability to predict the generalization improvement
on diverse downstream tasks. They are efficient, simple to compute and solely
dependent on training data. Hence, they have practical value for pretraining,
enabling us to monitor the generalization performance without finetuning and
test. Theoretically, we show that more structured pathways reduce model
complexity and improve the generalization bound.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [117] [The Singapore Consensus on Global AI Safety Research Priorities](https://arxiv.org/abs/2506.20702)
*Yoshua Bengio,Tegan Maharaj,Luke Ong,Stuart Russell,Dawn Song,Max Tegmark,Lan Xue,Ya-Qin Zhang,Stephen Casper,Wan Sie Lee,Sören Mindermann,Vanessa Wilfred,Vidhisha Balachandran,Fazl Barez,Michael Belinsky,Imane Bello,Malo Bourgon,Mark Brakel,Siméon Campos,Duncan Cass-Beggs,Jiahao Chen,Rumman Chowdhury,Kuan Chua Seah,Jeff Clune,Juntao Dai,Agnes Delaborde,Nouha Dziri,Francisco Eiras,Joshua Engels,Jinyu Fan,Adam Gleave,Noah Goodman,Fynn Heide,Dan Hendrycks,Cyrus Hodes,Bryan Low Kian Hsiang,Minlie Huang,Sami Jawhar,Wang Jingyu,Adam Tauman Kalai,Meindert Kamphuis,Mohan Kankanhalli,Subhash Kantamneni,Mathias Bonde Kirk,Thomas Kwa,Jeffrey Ladish,Kwok-Yan Lam,Wan Lee Sie,Taewhi Lee,Xiaojian Li,Jiajun Liu,Chaochao Lu,Yifan Mai,Richard Mallah,Julian Michael,Nick Moës,Simon Möller,Kihyuk Nam,Kwan Yee Ng,Mark Nitzberg,Besmira Nushi,Seán O hÉigeartaigh,Alejandro Ortega,Pierre Peigné,James Petrie,Benjamin Prud'Homme,Reihaneh Rabbany,Nayat Sanchez-Pi,Sarah Schwettmann,Buck Shlegeris,Saad Siddiqui,Aradhana Sinha,Martín Soto,Cheston Tan,Dong Ting,Robert Trager,Brian Tse,Anthony Tung K. H.,Vanessa Wilfred,John Willes,Denise Wong,Wei Xu,Rongwu Xu,Yi Zeng,HongJiang Zhang,Djordje Žikelić*

Key words: AI安全、可信生态系统、分层防御、风险评估、部署控制

TL;DR: 论文讨论了AI安全的重要性，提出了从开发、评估到控制的解决方案，并基于国际会议和报告整合了研究重点。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着AI能力的快速提升，如何确保其安全、可信赖和可靠成为关键问题，需要构建一个可信的生态系统。

Method: 采用分层防御模型，将AI安全研究分为开发（可信AI系统）、评估（风险）和控制（部署后监控）三个领域。

Result: 会议整合了国际AI安全报告的研究重点，提出了三大挑战领域。

Conclusion: 通过多领域协作和分层模型，可以推动AI安全研究，实现安全与创新的平衡。

Abstract: Rapidly improving AI capabilities and autonomy hold significant promise of
transformation, but are also driving vigorous debate on how to ensure that AI
is safe, i.e., trustworthy, reliable, and secure. Building a trusted ecosystem
is therefore essential -- it helps people embrace AI with confidence and gives
maximal space for innovation while avoiding backlash.
  The "2025 Singapore Conference on AI (SCAI): International Scientific
Exchange on AI Safety" aimed to support research in this space by bringing
together AI scientists across geographies to identify and synthesise research
priorities in AI safety. This resulting report builds on the International AI
Safety Report chaired by Yoshua Bengio and backed by 33 governments. By
adopting a defence-in-depth model, this report organises AI safety research
domains into three types: challenges with creating trustworthy AI systems
(Development), challenges with evaluating their risks (Assessment), and
challenges with monitoring and intervening after deployment (Control).

</details>


### [118] [MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation](https://arxiv.org/abs/2506.20737)
*Gurusha Juneja,Alon Albalak,Wenyue Hua,William Yang Wang*

Key words: LLM-based agents, contextual privacy, multi-turn conversation, MAGPIE benchmark, privacy preservation

TL;DR: 该论文研究了基于LLM的代理在理解上下文隐私方面的表现，并评估了其在多轮对话中保护用户隐私的能力。通过新基准MAGPIE发现，当前模型在隐私识别和协作任务完成方面存在显著不足。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着LLM代理的广泛应用，隐私保护在多代理协作中变得至关重要。论文旨在评估现有模型是否理解上下文隐私，并在非对抗性多轮对话中保护隐私。

Method: 论文提出了MAGPIE基准，包含158个高复杂度场景，并评估了GPT-4o和Claude-2.7-Sonnet等先进模型在隐私识别和协作任务中的表现。

Result: 当前模型的隐私识别能力不足，错误分类私人数据的比例较高（25.2%和43.6%），且在多轮对话中隐私泄露率高达59.9%和50.5%。多代理系统在71%的场景中未能完成任务。

Conclusion: 现有模型在上下文隐私保护和协作任务完成方面尚未达到理想水平，需进一步改进。

Abstract: The proliferation of LLM-based agents has led to increasing deployment of
inter-agent collaboration for tasks like scheduling, negotiation, resource
allocation etc. In such systems, privacy is critical, as agents often access
proprietary tools and domain-specific databases requiring strict
confidentiality. This paper examines whether LLM-based agents demonstrate an
understanding of contextual privacy. And, if instructed, do these systems
preserve inference time user privacy in non-adversarial multi-turn
conversation. Existing benchmarks to evaluate contextual privacy in LLM-agents
primarily assess single-turn, low-complexity tasks where private information
can be easily excluded. We first present a benchmark - MAGPIE comprising 158
real-life high-stakes scenarios across 15 domains. These scenarios are designed
such that complete exclusion of private data impedes task completion yet
unrestricted information sharing could lead to substantial losses. We then
evaluate the current state-of-the-art LLMs on (a) their understanding of
contextually private data and (b) their ability to collaborate without
violating user privacy. Empirical experiments demonstrate that current models,
including GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual
privacy, misclassifying private data as shareable 25.2\% and 43.6\% of the
time. In multi-turn conversations, these models disclose private information in
59.9\% and 50.5\% of cases even under explicit privacy instructions.
Furthermore, multi-agent systems fail to complete tasks in 71\% of scenarios.
These results underscore that current models are not aligned towards both
contextual privacy preservation and collaborative task-solving.

</details>


### [119] [Dynamic Context-Aware Prompt Recommendation for Domain-Specific AI Applications](https://arxiv.org/abs/2506.20815)
*Xinye Tang,Haijun Zhai,Chaitanya Belwal,Vineeth Thayanithi,Philip Baumann,Yogesh K Roy*

Key words: LLM, 提示推荐, 上下文感知, 知识检索, 自适应排名

TL;DR: 该论文提出了一种动态上下文感知的提示推荐系统，用于特定领域的AI应用，结合查询分析、知识检索、技能组织和自适应排名，生成相关且可操作的提示建议。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 由于LLM应用高度依赖用户提示的质量，而领域特定应用的提示设计往往具有挑战性，因此需要一种动态且上下文感知的解决方案来改善提示质量。

Method: 系统结合上下文查询分析、检索增强的知识基础、分层技能组织和自适应技能排名，利用行为遥测和两阶段分层推理动态选择并合成提示。

Result: 实验表明，该方法在真实数据集上表现出高实用性和相关性，并通过自动评估和专家验证。

Conclusion: 该系统显著提升了提示推荐的质量和效率，适用于领域特定的AI应用。

Abstract: LLM-powered applications are highly susceptible to the quality of user
prompts, and crafting high-quality prompts can often be challenging especially
for domain-specific applications. This paper presents a novel dynamic
context-aware prompt recommendation system for domain-specific AI applications.
Our solution combines contextual query analysis, retrieval-augmented knowledge
grounding, hierarchical skill organization, and adaptive skill ranking to
generate relevant and actionable prompt suggestions.
  The system leverages behavioral telemetry and a two-stage hierarchical
reasoning process to dynamically select and rank relevant skills, and
synthesizes prompts using both predefined and adaptive templates enhanced with
few-shot learning. Experiments on real-world datasets demonstrate that our
approach achieves high usefulness and relevance, as validated by both automated
and expert evaluations.

</details>


### [120] [Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation](https://arxiv.org/abs/2506.20949)
*Chenkai Sun,Denghui Zhang,ChengXiang Zhai,Heng Ji*

Key words: 语言模型, 社会影响, 安全性评估, 间接危害

TL;DR: 该论文提出了一种框架，用于评估语言模型在社会系统中的长期影响，并引入了一个测试模型安全性的数据集。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着语言模型在高风险社会决策中的影响增加，需要理解其建议的深远影响，以确保其有益性。

Method: 论文提出了一个概念验证框架，用于模拟模型生成建议在社会系统中的传播，并引入了一个包含100种间接危害场景的数据集，测试模型预测潜在不良后果的能力。

Result: 该方法在新数据集上实现了20%以上的改进，并在现有安全基准测试中平均胜率超过70%。

Conclusion: 研究表明，这种方法为实现更安全的语言模型代理提供了有前景的方向。

Abstract: Given the growing influence of language model-based agents on high-stakes
societal decisions, from public policy to healthcare, ensuring their beneficial
impact requires understanding the far-reaching implications of their
suggestions. We propose a proof-of-concept framework that projects how
model-generated advice could propagate through societal systems on a
macroscopic scale over time, enabling more robust alignment. To assess the
long-term safety awareness of language models, we also introduce a dataset of
100 indirect harm scenarios, testing models' ability to foresee adverse,
non-obvious outcomes from seemingly harmless user prompts. Our approach
achieves not only over 20% improvement on the new dataset but also an average
win rate exceeding 70% against strong baselines on existing safety benchmarks
(AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer
agents.

</details>


### [121] [Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?](https://arxiv.org/abs/2506.21215)
*Haoang Chi,He Li,Wenjing Yang,Feng Liu,Long Lan,Xiaoguang Ren,Tongliang Liu,Bo Han*

Key words: 因果推理、大型语言模型、G^2-Reasoner、自回归机制

TL;DR: 论文探讨了大型语言模型（LLMs）的因果推理能力，指出目前LLMs仅能进行浅层（level-1）因果推理，缺乏类似人类的深层（level-2）推理能力。作者提出了G^2-Reasoner方法，通过结合通用知识和目标导向提示来提升LLMs的因果推理能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 因果推理能力是推进LLMs向强人工智能发展的关键。尽管LLMs在因果推理方面表现出一定能力，但实际能力与人类仍存在差距，作者旨在揭示这一问题并提出改进方法。

Method: 通过分析LLMs的自回归机制，作者提出G^2-Reasoner方法，将通用知识和目标导向提示融入LLMs的因果推理过程。

Result: 实验表明，G^2-Reasoner显著提升了LLMs在新颖和反事实情境中的因果推理能力。

Conclusion: 该研究为LLMs实现真正的因果推理（level-2）提供了新方向。

Abstract: Causal reasoning capability is critical in advancing large language models
(LLMs) toward strong artificial intelligence. While versatile LLMs appear to
have demonstrated capabilities in understanding contextual causality and
providing responses that obey the laws of causality, it remains unclear whether
they perform genuine causal reasoning akin to humans. However, current evidence
indicates the contrary. Specifically, LLMs are only capable of performing
shallow (level-1) causal reasoning, primarily attributed to the causal
knowledge embedded in their parameters, but they lack the capacity for genuine
human-like (level-2) causal reasoning. To support this hypothesis,
methodologically, we delve into the autoregression mechanism of
transformer-based LLMs, revealing that it is not inherently causal.
Empirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,
whose corpora are fresh and nearly unseen for the studied LLMs. The LLMs
exhibit a significant performance drop on CausalProbe-2024 compared to earlier
benchmarks, indicating the fact that they primarily engage in level-1 causal
reasoning. To bridge the gap towards level-2 causal reasoning, we draw
inspiration from the fact that human reasoning is usually facilitated by
general knowledge and intended goals. We propose G^2-Reasoner, a method that
incorporates general knowledge and goal-oriented prompts into LLMs' causal
reasoning processes. Experiments demonstrate that G^2-Reasoner significantly
enhances LLMs' causal reasoning capability, particularly in fresh and
counterfactual contexts. This work sheds light on a new path for LLMs to
advance towards genuine causal reasoning, going beyond level-1 and making
strides towards level-2.

</details>


### [122] [World-aware Planning Narratives Enhance Large Vision-Language Model Planner](https://arxiv.org/abs/2506.21230)
*Junhao Shi,Zhaoye Fei,Siyin Wang,Qipeng Guo,Jingjing Gong,Xipeng QIu*

Key words: LVLM, WAP, 环境理解, 常识推理, 长期规划

TL;DR: WAP框架通过增强LVLM的环境理解能力，显著提升了在复杂场景中的任务成功率，尤其是常识推理和长期规划。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有大型视觉语言模型在陌生环境和多步目标场景中表现不佳，依赖环境无关的模仿学习，无法处理上下文敏感的指令。

Method: 提出WAP框架，通过四种认知能力（视觉外观建模、空间推理、功能抽象和语法基础）结合课程学习，仅使用原始视觉观察进行模型训练。

Result: 在EB-ALFRED基准测试中，任务成功率显著提升（Qwen2.5-VL提升60.7），尤其是在常识推理（+60.0）和长期规划（+70.0）方面。

Conclusion: WAP框架有效增强了模型的环境理解能力，开源模型性能优于GPT-4o和Claude-3.5-Sonnet等专有系统。

Abstract: Large Vision-Language Models (LVLMs) show promise for embodied planning tasks
but struggle with complex scenarios involving unfamiliar environments and
multi-step goals. Current approaches rely on environment-agnostic imitation
learning that disconnects instructions from environmental contexts, causing
models to struggle with context-sensitive instructions and rely on
supplementary cues rather than visual reasoning during long-horizon
interactions. In this work, we propose World-Aware Planning Narrative
Enhancement (WAP), a framework that infuses LVLMs with comprehensive
environmental understanding through four cognitive capabilities (visual
appearance modeling, spatial reasoning, functional abstraction, and syntactic
grounding) while developing and evaluating models using only raw visual
observations through curriculum learning. Evaluations on the EB-ALFRED
benchmark demonstrate substantial improvements, with Qwen2.5-VL achieving a
60.7 absolute improvement in task success rates, particularly in commonsense
reasoning (+60.0) and long-horizon planning (+70.0). Notably, our enhanced
open-source models outperform proprietary systems like GPT-4o and
Claude-3.5-Sonnet by a large margin.

</details>


### [123] [IXAII: An Interactive Explainable Artificial Intelligence Interface for Decision Support Systems](https://arxiv.org/abs/2506.21310)
*Pauline Speckmann,Mario Nadj,Christian Janiesch*

Key words: 可解释AI, 交互性, 用户视角, LIME, SHAP

TL;DR: 本文介绍了交互式解释性智能系统IXAII，结合多种解释方法（LIME、SHAP、Anchors、DiCE），为用户提供定制化解释视图，并通过专家和普通用户评估验证其有效性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有的可解释AI方法多为静态且忽视用户视角，限制了其实际效果。

Method: 开发了交互式系统IXAII，整合多种解释方法，支持用户自主选择内容和展示格式。

Result: 评估表明IXAII通过多视角解释和可视化选项提升了透明度，用户反馈积极。

Conclusion: IXAII弥合了可解释AI、交互性与实际应用之间的鸿沟，为人机交互提供了新视角。

Abstract: Although several post-hoc methods for explainable AI have been developed,
most are static and neglect the user perspective, limiting their effectiveness
for the target audience. In response, we developed the interactive explainable
intelligent system called IXAII that offers explanations from four explainable
AI methods: LIME, SHAP, Anchors, and DiCE. Our prototype provides tailored
views for five user groups and gives users agency over the explanations'
content and their format. We evaluated IXAII through interviews with experts
and lay users. Our results indicate that IXAII, which provides different
explanations with multiple visualization options, is perceived as helpful to
increase transparency. By bridging the gaps between explainable AI methods,
interactivity, and practical implementation, we provide a novel perspective on
AI explanation practices and human-AI interaction.

</details>


### [124] [Active Inference AI Systems for Scientific Discovery](https://arxiv.org/abs/2506.21329)
*Karthik Duraisamy*

Key words: AI驱动科学发现, 主动推理, 因果模型, 知识图谱, 人类判断

TL;DR: AI-driven scientific discovery需要解决抽象、推理与现实三方面的鸿沟，提出基于主动推理的AI系统架构，强调内部模型与外部验证的结合以及人类判断的不可或缺性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前AI系统在科学发现中存在局限性，需通过填补抽象、推理与现实三个缺口来推动AI驱动的科学进步。

Method: 提出主动推理AI系统，结合因果自监督基础模型、贝叶斯保护的符号/神经符号规划器、知识图谱的持续增长，以及高保真模拟与自动化实验室的闭环互动。

Result: 架构通过内部模型与外部验证的结合实现科学发现，强调人类判断的长期必要性。

Conclusion: AI驱动的科学发现需构建支持因果推理与实证校准的系统，并将人类判断永久纳入其架构。

Abstract: The rapid evolution of artificial intelligence has led to expectations of
transformative scientific discovery, yet current systems remain fundamentally
limited by their operational architectures, brittle reasoning mechanisms, and
their separation from experimental reality. Building on earlier work, we
contend that progress in AI-driven science now depends on closing three
fundamental gaps -- the abstraction gap, the reasoning gap, and the reality gap
-- rather than on model size/data/test time compute. Scientific reasoning
demands internal representations that support simulation of actions and
response, causal structures that distinguish correlation from mechanism, and
continuous calibration. We define active inference AI systems for scientific
discovery as those that (i) maintain long-lived research memories grounded in
causal self-supervised foundation models, (ii) symbolic or neuro-symbolic
planners equipped with Bayesian guardrails, (iii) grow persistent knowledge
graphs where thinking generates novel conceptual nodes, reasoning establishes
causal edges, and real-world interaction prunes false connections while
strengthening verified pathways, and (iv) refine their internal representations
through closed-loop interaction with both high-fidelity simulators and
automated laboratories - an operational loop where mental simulation guides
action and empirical surprise reshapes understanding. In essence, we outline an
architecture where discovery arises from the interplay between internal models
that enable counterfactual reasoning and external validation that grounds
hypotheses in reality. It is also argued that the inherent ambiguity in
feedback from simulations and experiments, and underlying uncertainties makes
human judgment indispensable, not as a temporary scaffold but as a permanent
architectural component.

</details>


### [125] [TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in Multimodal Table Understanding](https://arxiv.org/abs/2506.21393)
*Junwen Zhang,Pu Chen,Yin Zhang*

Key words: 多模态理解,表格分析,神经符号推理,混合专家架构,真实世界数据

TL;DR: 论文提出了TableMoE模型，用于解决真实环境下多模态表格理解的挑战，通过神经符号混合专家架构显著提升性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现实中的表格结构复杂、符号密集且视觉受损，现有多模态大语言模型（MLLMs）表现不佳，需要更鲁棒的方法。

Method: 采用神经符号混合专家架构（MoCE），通过神经符号路由机制动态分配表格元素至专用专家模块。

Result: TableMoE在四个基准测试中显著优于现有模型，并通过消融实验验证了核心组件的有效性。

Conclusion: 结合神经符号推理的多模态表格理解方法有效提升了鲁棒性和可解释性。

Abstract: Multimodal understanding of tables in real-world contexts is challenging due
to the complexity of structure, symbolic density, and visual degradation (blur,
skew, watermarking, incomplete structures or fonts, multi-span or
hierarchically nested layouts). Existing multimodal large language models
(MLLMs) struggle with such WildStruct conditions, resulting in limited
performance and poor generalization. To address these challenges, we propose
TableMoE, a neuro-symbolic Mixture-of-Connector-Experts (MoCE) architecture
specifically designed for robust, structured reasoning over multimodal table
data. TableMoE features an innovative Neuro-Symbolic Routing mechanism, which
predicts latent semantic token roles (e.g., header, data cell, axis, formula)
and dynamically routes table elements to specialized experts (Table-to-HTML,
Table-to-JSON, Table-to-Code) using a confidence-aware gating strategy informed
by symbolic reasoning graphs. To facilitate effective alignment-driven
pretraining, we introduce the large-scale TableMoE-Align dataset, consisting of
1.2M table-HTML-JSON-code quadruples across finance, science, biomedicine and
industry, utilized exclusively for model pretraining. For evaluation, we curate
and release four challenging WildStruct benchmarks: WMMFinQA, WMMTatQA,
WMMTabDialog, and WMMFinanceMath, designed specifically to stress-test models
under real-world multimodal degradation and structural complexity. Experimental
results demonstrate that TableMoE significantly surpasses existing
state-of-the-art models. Extensive ablation studies validate each core
component, emphasizing the critical role of Neuro-Symbolic Routing and
structured expert alignment. Through qualitative analyses, we further showcase
TableMoE's interpretability and enhanced robustness, underscoring the
effectiveness of integrating neuro-symbolic reasoning for multimodal table
understanding.

</details>


### [126] [Spatial Mental Modeling from Limited Views](https://arxiv.org/abs/2506.21458)
*Baiqiao Yin,Qineng Wang,Pingyue Zhang,Jianshu Zhang,Kangrui Wang,Zihan Wang,Jieyu Zhang,Keshigeyan Chandrasegaran,Han Liu,Ranjay Krishna,Saining Xie,Manling Li,Jiajun Wu,Li Fei-Fei*

Key words: 视觉语言模型、空间心理模型、MindCube、认知地图、推理

TL;DR: MindCube基准测试揭示了视觉语言模型（VLMs）在构建空间心理模型上的不足，提出‘先地图后推理’方法提升性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 评估VLMs是否能够像人类一样从少量视角构建完整场景的空间心理模型。

Method: 提出了MindCube基准测试，探索了三种方法（中间视图、自然语言推理链、认知地图），并通过‘先地图后推理’方法与强化学习提升性能。

Result: 方法将准确率从37.8%提升至60.8%，强化学习进一步提升至70.7%。

Conclusion: 构建和利用结构化空间表示能显著提升VLMs对不可见空间的理解能力。

Abstract: Can Vision Language Models (VLMs) imagine the full scene from just a few
views, like humans do? Humans form spatial mental models, internal
representations of unseen space, to reason about layout, perspective, and
motion. Our new MindCube benchmark with 21,154 questions across 3,268 images
exposes this critical gap, where existing VLMs exhibit near-random performance.
Using MindCube, we systematically evaluate how well VLMs build robust spatial
mental models through representing positions (cognitive mapping), orientations
(perspective-taking), and dynamics (mental simulation for "what-if" movements).
We then explore three approaches to help VLMs approximate spatial mental
models, including unseen intermediate views, natural language reasoning chains,
and cognitive maps. The significant improvement comes from a synergistic
approach, "map-then-reason", that jointly trains the model to first generate a
cognitive map and then reason upon it. By training models to reason over these
internal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding
reinforcement learning pushed performance even further to 70.7% (+32.9%). Our
key insight is that such scaffolding of spatial mental models, actively
constructing and utilizing internal structured spatial representations with
flexible reasoning processes, significantly improves understanding of
unobservable space.

</details>


### [127] [Ad-Hoc Human-AI Coordination Challenge](https://arxiv.org/abs/2506.21490)
*Tin Dizdarević,Ravi Hammond,Tobias Gessler,Anisoara Calinescu,Jonathan Cook,Matteo Gallici,Andrei Lupu,Jakob Nicolaus Foerster*

Key words: human-AI coordination, Hanabi game, proxy agents, evaluation challenge

TL;DR: 论文提出Ad-Hoc Human-AI Coordination Challenge（AH2AC2），通过开发人类代理代理解决人与AI协调的评估问题，并使用有限的人类游戏数据进行测试。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 实现人与AI的无缝协调在现实应用中至关重要，但目前仍具挑战性。为了解决人类评估成本高和难以复现的问题，本研究提出了AH2AC2。

Method: 开发人类代理代理，作为廉价、可复现的人类评估伙伴，并使用开源数据集（3,079局游戏）进行测试。

Result: 在双人和三人Hanabi场景中提供了基线结果，并通过受控评估系统确保公平性。

Conclusion: AH2AC2为人类与AI协调提供了新的评估方法，解决了现有评估的局限性。

Abstract: Achieving seamless coordination between AI agents and humans is crucial for
real-world applications, yet it remains a significant open challenge. Hanabi is
a cooperative card game featuring imperfect information, constrained
communication, theory of mind requirements, and coordinated action -- making it
an ideal testbed for human-AI coordination. However, its use for human-AI
interaction has been limited by the challenges of human evaluation. In this
work, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to
overcome the constraints of costly and difficult-to-reproduce human
evaluations. We develop \textit{human proxy agents} on a large-scale human
dataset that serve as robust, cheap, and reproducible human-like evaluation
partners in AH2AC2. To encourage the development of data-efficient methods, we
open-source a dataset of 3,079 games, deliberately limiting the amount of
available human gameplay data. We present baseline results for both two- and
three- player Hanabi scenarios. To ensure fair evaluation, we host the proxy
agents through a controlled evaluation system rather than releasing them
publicly. The code is available at
\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}.

</details>


### [128] [Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge](https://arxiv.org/abs/2506.21506)
*Boyu Gou,Zanming Huang,Yuting Ning,Yu Gu,Michael Lin,Weijian Qi,Andrei Kopanev,Botao Yu,Bernal Jiménez Gutiérrez,Yiheng Shu,Chan Hee Song,Jiaman Wu,Shijie Chen,Hanane Nour Moussa,Tianshu Zhang,Jian Xie,Yifei Li,Tianci Xue,Zeyi Liao,Kai Zhang,Boyuan Zheng,Zhaowei Cai,Viktor Rozgic,Morteza Ziyadi,Huan Sun,Yu Su*

Key words: 代理搜索、Mind2Web 2、Agent-as-a-Judge、评估基准、信息合成

TL;DR: Mind2Web 2 introduces一个包含130个任务的基准，用于评估开放性和复杂性更高的代理搜索系统，并提出基于树形结构的Agent-as-a-Judge框架进行自动评估。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有评估方法无法满足代理搜索系统日益增长的复杂性和开放性问题，需新的基准和评估方法。

Method: 构建Mind2Web 2基准，包含130个任务及1,000小时人工标注；提出Agent-as-a-Judge框架，基于任务特定评分标准自动化评估答案正确性和来源标注。

Result: OpenAI Deep Research系统表现最佳，可达人类性能的50-70%，同时耗时减半。

Conclusion: Mind2Web 2为下一代代理搜索系统的开发和评估提供了严格基础。

Abstract: Agentic search such as Deep Research systems, where large language models
autonomously browse the web, synthesize information, and return comprehensive
citation-backed answers, represents a major shift in how users interact with
web-scale information. While promising greater efficiency and cognitive
offloading, the growing complexity and open-endedness of agentic search have
outpaced existing evaluation benchmarks and methodologies, which largely assume
short search horizons and static answers. In this paper, we introduce Mind2Web
2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that
require real-time web browsing and extensive information synthesis, constructed
with over 1,000 hours of human labor. To address the challenge of evaluating
time-varying and complex answers, we propose a novel Agent-as-a-Judge
framework. Our method constructs task-specific judge agents based on a
tree-structured rubric design to automatically assess both answer correctness
and source attribution. We conduct a comprehensive evaluation of nine frontier
agentic search systems and human performance, along with a detailed error
analysis to draw insights for future development. The best-performing system,
OpenAI Deep Research, can already achieve 50-70% of human performance while
spending half the time, showing a great potential. Altogether, Mind2Web 2
provides a rigorous foundation for developing and benchmarking the next
generation of agentic search systems.

</details>


### [129] [PsyLite Technical Report](https://arxiv.org/abs/2506.21536)
*Fangjun Ding,Renyu Zhang,Xinyu Feng,Chengye Xie,Zheng Zhang,Yanting Zhang*

Key words: 心理咨询、大语言模型、PsyLite、ORPO偏好优化、量化技术

TL;DR: PsyLite是一种轻量级的心理咨询大语言模型，通过混合蒸馏数据微调和ORPO偏好优化提升深度推理、咨询能力和对话安全性，在多个评估中表现优异，同时实现了低硬件部署。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着数字技术的发展，AI驱动的心理咨询成为重要研究方向，但现有模型在对话安全、场景处理和轻量部署方面仍有不足。

Method: 基于InternLM2.5-7B-chat开发PsyLite，采用两阶段训练策略（混合蒸馏数据微调和ORPO偏好优化），并设计了创新的条件RAG引入幽默元素和增强对话安全。

Result: PsyLite在CEval、CPsyCounE和SafeDialBench评估中表现优于基线模型，心理咨询专业性提升47.6%，对话安全性提升2.4%，并通过量化技术实现低硬件需求（5GB内存即可运行）。

Conclusion: PsyLite为资源受限环境下的心理咨询应用提供了可行解决方案，同时提升了专业性和安全性。

Abstract: With the rapid development of digital technology, AI-driven psychological
counseling has gradually become an important research direction in the field of
mental health. However, existing models still have deficiencies in dialogue
safety, detailed scenario handling, and lightweight deployment. To address
these issues, this study proposes PsyLite, a lightweight psychological
counseling large language model agent developed based on the base model
InternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation
data fine-tuning and ORPO preference optimization), PsyLite enhances the
model's deep-reasoning ability, psychological counseling ability, and safe
dialogue ability. After deployment using Ollama and Open WebUI, a custom
workflow is created with Pipelines. An innovative conditional RAG is designed
to introduce crosstalk humor elements at appropriate times during psychological
counseling to enhance user experience and decline dangerous requests to
strengthen dialogue safety. Evaluations show that PsyLite outperforms the
baseline models in the Chinese general evaluation (CEval), psychological
counseling professional evaluation (CPsyCounE), and dialogue safety evaluation
(SafeDialBench), particularly in psychological counseling professionalism
(CPsyCounE score improvement of 47.6\%) and dialogue safety (\safe{} score
improvement of 2.4\%). Additionally, the model uses quantization technology
(GGUF q4\_k\_m) to achieve low hardware deployment (5GB memory is sufficient
for operation), providing a feasible solution for psychological counseling
applications in resource-constrained environments.

</details>


<div id='q-bio.CB'></div>

# q-bio.CB [[Back]](#toc)

### [130] [scMamba: A Scalable Foundation Model for Single-Cell Multi-Omics Integration Beyond Highly Variable Feature Selection](https://arxiv.org/abs/2506.20697)
*Zhen Yuan,Shaoqing Jiao,Yihang Xiao,Jiajie Peng*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: q-bio.CB

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The advent of single-cell multi-omics technologies has enabled the
simultaneous profiling of diverse omics layers within individual cells.
Integrating such multimodal data provides unprecedented insights into cellular
identity, regulatory processes, and disease mechanisms. However, it remains
challenging, as current methods often rely on selecting highly variable genes
or peaks during preprocessing, which may inadvertently discard crucial
biological information. Here, we present scMamba, a foundation model designed
to integrate single-cell multi-omics data without the need for prior feature
selection while preserving genomic positional information. scMamba introduces a
patch-based cell tokenization strategy that treats genomics regions as words
(tokens) and cells as sentences. Building upon the concept of state space
duality, scMamba distills rich biological insights from high-dimensional,
sparse single-cell multi-omics data. Additionally, our novel contrastive
learning approach, enhanced with cosine similarity regularization, enables
superior alignment across omics layers compared to traditional methods.
Systematic benchmarking across multiple datasets demonstrates that scMamba
significantly outperforms state-of-the-art methods in preserving biological
variation, aligning omics layers, and enhancing key downstream tasks such as
clustering, cell type annotation, and trajectory inference. Our findings
position scMamba as a powerful tool for large-scale single-cell multi-omics
integration, capable of handling large-scale atlases and advancing biological
discovery.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [131] [Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends](https://arxiv.org/abs/2506.20966)
*Tian-Yu Xiang,Ao-Qun Jin,Xiao-Hu Zhou,Mei-Jiang Gui,Xiao-Liang Xie,Shi-Qi Liu,Shuang-Yi Wang,Sheng-Bin Duan,Fu-Chao Xie,Wen-Kai Wang,Si-Cheng Wang,Ling-Yun Li,Tian Tu,Zeng-Guang Hou*

Key words: VLA模型, 后训练, 人类运动学习, 机器人操纵

TL;DR: 该论文综述了从人类运动学习视角出发的视觉-语言-动作（VLA）模型后训练策略，提出了一种分类法，并总结了关键挑战与未来研究方向。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: VLA模型在机器人操纵任务中展现出泛化能力，但在高精度需求下表现不足，亟需通过后训练提升其与环境的交互能力。

Method: 论文通过人类运动学习的三维度（环境、体现和任务）提出后训练分类法，包括环境感知增强、体现意识提升、任务理解深化和多组件整合。

Result: 提出了一种结构化分类法，并从人类运动学习角度总结了VLA模型后训练的挑战与趋势。

Conclusion: 论文为VLA模型的后训练提供了理论框架和实践启示，推动了该领域的未来发展。

Abstract: Vision-language-action (VLA) models extend vision-language models (VLM) by
integrating action generation modules for robotic manipulation. Leveraging
strengths of VLM in vision perception and instruction understanding, VLA models
exhibit promising generalization across diverse manipulation tasks. However,
applications demanding high precision and accuracy reveal performance gaps
without further adaptation. Evidence from multiple domains highlights the
critical role of post-training to align foundational models with downstream
applications, spurring extensive research on post-training VLA models. VLA
model post-training aims to address the challenge of improving an embodiment's
ability to interact with the environment for the given tasks, analogous to the
process of humans motor skills acquisition. Accordingly, this paper reviews
post-training strategies for VLA models through the lens of human motor
learning, focusing on three dimensions: environments, embodiments, and tasks. A
structured taxonomy is introduced aligned with human learning mechanisms: (1)
enhancing environmental perception, (2) improving embodiment awareness, (3)
deepening task comprehension, and (4) multi-component integration. Finally, key
challenges and trends in post-training VLA models are identified, establishing
a conceptual framework to guide future research. This work delivers both a
comprehensive overview of current VLA model post-training methods from a human
motor learning perspective and practical insights for VLA model development.
(Project website: https://github.com/AoqunJin/Awesome-VLA-Post-Training)

</details>


### [132] [V2X-REALM: Vision-Language Model-Based Robust End-to-End Cooperative Autonomous Driving with Adaptive Long-Tail Modeling](https://arxiv.org/abs/2506.21041)
*Junwei You,Pei Li,Zhuoyu Jiang,Zilin Huang,Rui Gan,Haotian Shi,Bin Ran*

Key words: 自动驾驶,长尾场景,视觉语言模型,协作感知,多模态学习

TL;DR: V2X-REALM是一个基于视觉语言模型的框架，通过自适应多模态学习解决城市环境中长尾场景下自动驾驶的规划与决策问题。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决城市环境中罕见、多样且视觉退化长尾场景下自动驾驶的鲁棒性问题，尤其在协作场景中更为关键。

Method: 提出V2X-REALM框架，包含长尾场景生成与评估管道、门控多场景自适应注意力模块和多任务场景对比学习目标。

Result: 实验表明，V2X-REALM在复杂驾驶条件下显著优于现有基线，提升了鲁棒性、语义推理和规划准确性。

Conclusion: V2X-REALM推动了端到端协作自动驾驶的可扩展性。

Abstract: Ensuring robust planning and decision-making under rare, diverse, and
visually degraded long-tail scenarios remains a fundamental challenge for
autonomous driving in urban environments. This issue becomes more critical in
cooperative settings, where vehicles and infrastructure jointly perceive and
reason across complex environments. To address this challenge, we propose
V2X-REALM, a vision-language model (VLM)-based framework with adaptive
multimodal learning for robust cooperative autonomous driving under long-tail
scenarios. V2X-REALM introduces three core innovations: (i) a prompt-driven
long-tail scenario generation and evaluation pipeline that leverages foundation
models to synthesize realistic long-tail conditions such as snow and fog across
vehicle- and infrastructure-side views, enriching training diversity
efficiently; (ii) a gated multi-scenario adaptive attention module that
modulates the visual stream using scenario priors to recalibrate ambiguous or
corrupted features; and (iii) a multi-task scenario-aware contrastive learning
objective that improves multimodal alignment and promotes cross-scenario
feature separability. Extensive experiments demonstrate that V2X-REALM
significantly outperforms existing baselines in robustness, semantic reasoning,
safety, and planning accuracy under complex, challenging driving conditions,
advancing the scalability of end-to-end cooperative autonomous driving.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [133] [Uncertainty-Aware Machine-Learning Framework for Predicting Dislocation Plasticity and Stress-Strain Response in FCC Alloys](https://arxiv.org/abs/2506.20839)
*Jing Luo,Yejun Gu,Yanfei Wang,Xiaolong Ma,Jaafar. A El-Awady*

Key words: 机器学习、混合密度网络、位错密度、应力分布、不确定性量化、合金设计

TL;DR: 本文提出了一种基于混合密度网络（MDN）的全面方法，通过结合文献中的实验数据，预测位错密度分布及其导致的应力分布，并结合统计参数量化不确定性，优化合金设计。

<details>
  <summary>Details</summary>

Main category: cond-mat.mtrl-sci

Motivation: 机器学习的应用促进了结构材料研究的进展，但如何整合现有数据并量化预测模型中的不确定性仍是一个关键问题。

Method: 研究采用混合密度网络（MDN）模型，利用文献中的实验数据预测位错密度分布和应力分布，并将这些统计参数融入位错介导的塑性模型。

Result: 该方法不仅提高了机械性能预测的准确性和可靠性，还通过显式不确定性量化为合金设计提供了指导。

Conclusion: 提出的策略不仅改善了材料性能预测，还在快速发展的行业中促进了新材料的开发。

Abstract: Machine learning has significantly advanced the understanding and application
of structural materials, with an increasing emphasis on integrating existing
data and quantifying uncertainties in predictive modeling. This study presents
a comprehensive methodology utilizing a mixed density network (MDN) model,
trained on extensive experimental data from literature. This approach uniquely
predicts the distribution of dislocation density, inferred as a latent
variable, and the resulting stress distribution at the grain level. The
incorporation of statistical parameters of those predicted distributions into a
dislocation-mediated plasticity model allows for accurate stress-strain
predictions with explicit uncertainty quantification. This strategy not only
improves the accuracy and reliability of mechanical property predictions but
also plays a vital role in optimizing alloy design, thereby facilitating the
development of new materials in a rapidly evolving industry.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [134] [Control and optimization for Neural Partial Differential Equations in Supervised Learning](https://arxiv.org/abs/2506.20764)
*Alain Bensoussan,Minh-Binh Tran,Bangjie Wang*

Key words: 控制理论，偏微分方程，神经网络，监督学习，优化

TL;DR: 探讨抛物型和双曲型系统中算子系数的控制与优化问题，提出神经网络可视为偏微分方程的新视角，并证明相关优化问题的解存在性。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: 现有文献对抛物型和双曲型系统的控制与优化问题研究较多，但对其中算子系数的控制与优化尚未深入探索，尤其在神经网络和监督学习中具有重要意义。

Method: 将神经网络解释为偏微分方程，提出一种双系统框架，用于抛物型偏微分方程的控制与优化问题，并理论证明其解的存在性。

Result: 证明了抛物型偏微分方程的优化问题存在最小化解，并对双曲型偏微分方程的近似控制问题证明了解的存在性。

Conclusion: 研究为偏微分方程中算子系数的控制与优化问题奠定了基础，未来可开发高效数值算法。

Abstract: Although there is a substantial body of literature on control and
optimization problems for parabolic and hyperbolic systems, the specific
problem of controlling and optimizing the coefficients of the associated
operators within such systems has not yet been thoroughly explored. In this
work, we aim to initiate a line of research in control theory focused on
optimizing and controlling the coefficients of these operators-a problem that
naturally arises in the context of neural networks and supervised learning.
  In supervised learning, the primary objective is to transport initial data
toward target data through the layers of a neural network. We propose a novel
perspective: neural networks can be interpreted as partial differential
equations (PDEs). From this viewpoint, the control problem traditionally
studied in the context of ordinary differential equations (ODEs) is
reformulated as a control problem for PDEs, specifically targeting the
optimization and control of coefficients in parabolic and hyperbolic operators.
To the best of our knowledge, this specific problem has not yet been
systematically addressed in the control theory of PDEs.
  To this end, we propose a dual system formulation for the control and
optimization problem associated with parabolic PDEs, laying the groundwork for
the development of efficient numerical schemes in future research. We also
provide a theoretical proof showing that the control and optimization problem
for parabolic PDEs admits minimizers. Finally, we investigate the control
problem associated with hyperbolic PDEs and prove the existence of solutions
for a corresponding approximated control problem.

</details>


### [135] [Faster Fixed-Point Methods for Multichain MDPs](https://arxiv.org/abs/2506.20910)
*Matthew Zurek,Yudong Chen*

Key words: 值迭代、多链MDPs、平均奖励、收敛率、复杂度分析

TL;DR: 研究针对多链马尔可夫决策过程（MDPs）的平均奖励问题的值迭代算法，提出改进的收敛率和复杂度分析方法。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: 解决多链MDPs中平均奖励问题的理论挑战，包括缺乏收缩性和导航子问题的优化需求。

Method: 开发新算法，结合平均奖励与折扣问题的联系，改进值迭代的收敛性。

Result: 获得更快的收敛率、更精确的复杂度分析，并扩展了值迭代的理论基础。

Conclusion: 算法在多链MDPs中表现优异，为平均奖励和折扣问题提供了更高效的解决方案。

Abstract: We study value-iteration (VI) algorithms for solving general (a.k.a.
multichain) Markov decision processes (MDPs) under the average-reward
criterion, a fundamental but theoretically challenging setting. Beyond the
difficulties inherent to all average-reward problems posed by the lack of
contractivity and non-uniqueness of solutions to the Bellman operator, in the
multichain setting an optimal policy must solve the navigation subproblem of
steering towards the best connected component, in addition to optimizing
long-run performance within each component. We develop algorithms which better
solve this navigational subproblem in order to achieve faster convergence for
multichain MDPs, obtaining improved rates of convergence and sharper measures
of complexity relative to prior work. Many key components of our results are of
potential independent interest, including novel connections between
average-reward and discounted problems, optimal fixed-point methods for
discounted VI which extend to general Banach spaces, new sublinear convergence
rates for the discounted value error, and refined suboptimality decompositions
for multichain MDPs. Overall our results yield faster convergence rates for
discounted and average-reward problems and expand the theoretical foundations
of VI approaches.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [136] [Exploring the Effects of Chatbot Anthropomorphism and Human Empathy on Human Prosocial Behavior Toward Chatbots](https://arxiv.org/abs/2506.20748)
*Jingshu Li,Zicheng Zhu,Renwen Zhang,Yi-Chieh Lee*

Key words: 聊天机器人，拟人化，亲社会行为，CASA框架，同理心

TL;DR: 研究通过CASA框架探讨聊天机器人拟人化如何影响人类同理心及亲社会行为，发现人类身份和情感表达提升亲社会行为。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 填补人们对帮助聊天机器人动机的研究空白，探索拟人化对亲社会行为的影响。

Method: 在线实验（N=244），聊天机器人在协作任务中犯错并解释原因，测量参与者的亲社会行为与意图。

Result: 人类身份和情感表达增加亲社会行为，同理心起中介作用。质化分析发现两种动机：同理心和将机器人拟人化。

Conclusion: 拟人化促进人类对聊天机器人的亲社会行为，为理解与推广此类行为提供启示。

Abstract: Chatbots are increasingly integrated into people's lives and are widely used
to help people. Recently, there has also been growing interest in the reverse
direction-humans help chatbots-due to a wide range of benefits including better
chatbot performance, human well-being, and collaborative outcomes. However,
little research has explored the factors that motivate people to help chatbots.
To address this gap, we draw on the Computers Are Social Actors (CASA)
framework to examine how chatbot anthropomorphism-including human-like
identity, emotional expression, and non-verbal expression-influences human
empathy toward chatbots and their subsequent prosocial behaviors and
intentions. We also explore people's own interpretations of their prosocial
behaviors toward chatbots. We conducted an online experiment (N = 244) in which
chatbots made mistakes in a collaborative image labeling task and explained the
reasons to participants. We then measured participants' prosocial behaviors and
intentions toward the chatbots. Our findings revealed that human identity and
emotional expression of chatbots increased participants' prosocial behavior and
intention toward chatbots, with empathy mediating these effects. Qualitative
analysis further identified two motivations for participants' prosocial
behaviors: empathy for the chatbot and perceiving the chatbot as human-like. We
discuss the implications of these results for understanding and promoting human
prosocial behaviors toward chatbots.

</details>


### [137] [A Systematic Review of Human-AI Co-Creativity](https://arxiv.org/abs/2506.21333)
*Saloni Singh,Koen Hndriks,Drik Heylen,Kim Baraka*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The co creativity community is making significant progress in developing more
sophisticated and tailored systems to support and enhance human creativity.
Design considerations from prior work can serve as a valuable and efficient
foundation for future systems. To support this effort, we conducted a
systematic literature review of 62 papers on co-creative systems. These papers
cover a diverse range of applications, including visual arts, design, and
writing, where the AI acts not just as a tool but as an active collaborator in
the creative process. From this review, we identified several key dimensions
relevant to system design: phase of the creative process, creative task,
proactive behavior of the system, user control, system embodiment, and AI model
type. Our findings suggest that systems offering high user control lead to
greater satisfaction, trust, and a stronger sense of ownership over creative
outcomes. Furthermore, proactive systems, when adaptive and context sensitive,
can enhance collaboration. We also extracted 24 design considerations,
highlighting the value of encouraging users to externalize their thoughts and
of increasing the system's social presence and transparency to foster trust.
Despite recent advancements, important gaps remain, such as limited support for
early creative phases like problem clarification, and challenges related to
user adaptation to AI systems.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [138] [The final solution of the Hitchhiker's problem #5](https://arxiv.org/abs/2506.20672)
*Matjaž Omladič,Martin Vuk,Aljaž Zalar*

Key words: 拟联结函数；极值问题；解析方法；依赖建模

TL;DR: 本文通过解析方法解决了《Hitchhiker's Guide》中关于多元拟联结函数质量分布的极值问题，并否定了相关猜想。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 拟联结函数在依赖建模中备受关注，但缺乏统计解释。前人通过线性规划部分解决了问题，但未完全解答。

Method: 采用解析方法（非线性规划）研究多元拟联结函数的质量分布极值。

Result: 完全解决了《Hitchhiker's Guide》中的开放问题，并否定了相关猜想。

Conclusion: 解析方法有效解决了拟联结函数的极值问题，为依赖建模提供了新视角。

Abstract: A recent survey, nicknamed "Hitchhiker's Guide", J.J. Arias-Garc{\i}a, R.
Mesiar, and B. De Baets, A hitchhiker's guide to quasi-copulas, Fuzzy Sets and
Systems 393 (2020) 1-28, has raised the rating of quasi-copula problems in the
dependence modeling community in spite of the lack of statistical
interpretation of quasi-copulas. In our previous work (arXiv:2410.19339,
accepted in Fuzzy Sets and Systems), we addressed the question of extreme
values of the mass distribution associated with multivariate quasi-copulas.
Using a linear programming approach, we were able to solve Open Problem 5 of
the "Guide" up to dimension d = 17 and disprove a recent conjecture on the
solution to that problem. In this paper, we use an analytical approach to
provide a complete answer to the original question.

</details>


### [139] [Stable Minima of ReLU Neural Networks Suffer from the Curse of Dimensionality: The Neural Shattering Phenomenon](https://arxiv.org/abs/2506.20779)
*Tongtong Liang,Dan Qiao,Yu-Xiang Wang,Rahul Parhi*

Key words: 平坦性, 泛化能力, ReLU网络, 高维度, 极小值稳定性

TL;DR: 研究了平坦性/低曲率隐式偏差对两层过参数ReLU网络在多变量输入下泛化能力的影响，揭示了平坦解在高维度中泛化性能指数级下降的现象。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 研究梯度下降训练中极小值稳定性和边缘稳定现象，探讨平坦性是否在多变量输入下仍能保证泛化。

Method: 通过理论分析，包括上界和下界证明，以及数值模拟，研究平坦解在多变量输入下的泛化性能。

Result: 平坦解虽能泛化，但其收敛速率随输入维度增长指数级下降，且与低范数解相比在高维度中表现更差。

Conclusion: 平坦极小值在高维度中可能无法泛化，揭示了其在高维度中的局限性。

Abstract: We study the implicit bias of flatness / low (loss) curvature and its effects
on generalization in two-layer overparameterized ReLU networks with
multivariate inputs -- a problem well motivated by the minima stability and
edge-of-stability phenomena in gradient-descent training. Existing work either
requires interpolation or focuses only on univariate inputs. This paper
presents new and somewhat surprising theoretical results for multivariate
inputs. On two natural settings (1) generalization gap for flat solutions, and
(2) mean-squared error (MSE) in nonparametric function estimation by stable
minima, we prove upper and lower bounds, which establish that while flatness
does imply generalization, the resulting rates of convergence necessarily
deteriorate exponentially as the input dimension grows. This gives an
exponential separation between the flat solutions vis-\`a-vis low-norm
solutions (i.e., weight decay), which knowingly do not suffer from the curse of
dimensionality. In particular, our minimax lower bound construction, based on a
novel packing argument with boundary-localized ReLU neurons, reveals how flat
solutions can exploit a kind of ''neural shattering'' where neurons rarely
activate, but with high weight magnitudes. This leads to poor performance in
high dimensions. We corroborate these theoretical findings with extensive
numerical simulations. To the best of our knowledge, our analysis provides the
first systematic explanation for why flat minima may fail to generalize in high
dimensions.

</details>


### [140] [Hyperspherical Variational Autoencoders Using Efficient Spherical Cauchy Distribution](https://arxiv.org/abs/2506.21278)
*Lukas Sablica,Kurt Hornik*

Key words: 变分自编码器, 球面柯西分布, 方向性数据, Möbius变换, KL散度

TL;DR: 提出了一种新型的变分自编码器（VAE）架构，采用球面柯西（spCauchy）潜在分布，优于传统高斯或von Mises-Fisher分布，具有更自然的超球面表示、高效潜空间利用和稳定性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 针对传统高斯和vMF分布在方向性数据表示中的不足，提出spCauchy分布以更好地捕捉方向性数据，同时避免数值不稳定性和过正则化问题。

Method: 使用spCauchy潜在分布，通过Möbius变换实现高效且可微分的重参数化，并利用快速收敛的幂级数计算KL散度。

Result: spCauchy在VAE中提供了理论优势，如更灵活的潜空间表示和更稳定的训练，适用于高维生成模型。

Conclusion: spCauchy是一种高效且稳定的潜在分布选择，适合VAE在高维数据中的应用。

Abstract: We propose a novel variational autoencoder (VAE) architecture that employs a
spherical Cauchy (spCauchy) latent distribution. Unlike traditional Gaussian
latent spaces or the widely used von Mises-Fisher (vMF) distribution, spCauchy
provides a more natural hyperspherical representation of latent variables,
better capturing directional data while maintaining flexibility. Its
heavy-tailed nature prevents over-regularization, ensuring efficient latent
space utilization while offering a more expressive representation.
Additionally, spCauchy circumvents the numerical instabilities inherent to vMF,
which arise from computing normalization constants involving Bessel functions.
Instead, it enables a fully differentiable and efficient reparameterization
trick via M\"obius transformations, allowing for stable and scalable training.
The KL divergence can be computed through a rapidly converging power series,
eliminating concerns of underflow or overflow associated with evaluation of
ratios of hypergeometric functions. These properties make spCauchy a compelling
alternative for VAEs, offering both theoretical advantages and practical
efficiency in high-dimensional generative modeling.

</details>


### [141] [Active Learning for Manifold Gaussian Process Regression](https://arxiv.org/abs/2506.20928)
*Yuanxing Cheng,Lulu Kang,Yiwei Wang,Chun Liu*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper introduces an active learning framework for manifold Gaussian
Process (GP) regression, combining manifold learning with strategic data
selection to improve accuracy in high-dimensional spaces. Our method jointly
optimizes a neural network for dimensionality reduction and a Gaussian process
regressor in the latent space, supervised by an active learning criterion that
minimizes global prediction error. Experiments on synthetic data demonstrate
superior performance over randomly sequential learning. The framework
efficiently handles complex, discontinuous functions while preserving
computational tractability, offering practical value for scientific and
engineering applications. Future work will focus on scalability and
uncertainty-aware manifold learning.

</details>


### [142] [Lower Bounds on the Size of Markov Equivalence Classes](https://arxiv.org/abs/2506.20933)
*Erik Jahn,Frederick Eberhardt,Leonard J. Schulman*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Causal discovery algorithms typically recover causal graphs only up to their
Markov equivalence classes unless additional parametric assumptions are made.
The sizes of these equivalence classes reflect the limits of what can be
learned about the underlying causal graph from purely observational data. Under
the assumptions of acyclicity, causal sufficiency, and a uniform model prior,
Markov equivalence classes are known to be small on average. In this paper, we
show that this is no longer the case when any of these assumptions is relaxed.
Specifically, we prove exponentially large lower bounds for the expected size
of Markov equivalence classes in three settings: sparse random directed acyclic
graphs, uniformly random acyclic directed mixed graphs, and uniformly random
directed cyclic graphs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [143] [ClusterRCA: Network Failure Diagnosis in HPC Systems Using Multimodal Data](https://arxiv.org/abs/2506.20673)
*Yongqian Sun,Xijie Pan,Xiao Xiong,Lei Tao,Jiaju Wang,Shenglin Zhang,Yuan Yuan,Yuqi Li,Kunlin Jian*

Key words: HPC, 网络故障诊断, 多模态数据, 分类器, 图方法

TL;DR: ClusterRCA是一种新型框架，通过多模态数据诊断高性能计算（HPC）系统中的网络故障，结合分类器和图方法，准确定位故障节点和类型。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 现有方法因数据异构性和精度不足，难以直接应用于HPC场景，需提出新解决方案。

Method: ClusterRCA从拓扑连接的NIC对提取特征，结合分类器和图方法，构建故障图并进行定制随机游走以定位根因。

Result: 实验表明，ClusterRCA在HPC系统中诊断网络故障具备高精度，并在不同应用场景中表现稳健。

Conclusion: ClusterRCA为HPC系统提供了一种高效、准确的网络故障诊断框架。

Abstract: Network failure diagnosis is challenging yet critical for high-performance
computing (HPC) systems. Existing methods cannot be directly applied to HPC
scenarios due to data heterogeneity and lack of accuracy. This paper proposes a
novel framework, called ClusterRCA, to localize culprit nodes and determine
failure types by leveraging multimodal data. ClusterRCA extracts features from
topologically connected network interface controller (NIC) pairs to analyze the
diverse, multimodal data in HPC systems. To accurately localize culprit nodes
and determine failure types, ClusterRCA combines classifier-based and
graph-based approaches. A failure graph is constructed based on the output of
the state classifier, and then it performs a customized random walk on the
graph to localize the root cause. Experiments on datasets collected by a
top-tier global HPC device vendor show ClusterRCA achieves high accuracy in
diagnosing network failure for HPC systems. ClusterRCA also maintains robust
performance across different application scenarios.

</details>


### [144] [Utility-Driven Speculative Decoding for Mixture-of-Experts](https://arxiv.org/abs/2506.20675)
*Anish Saxena,Po-An Tsai,Hritvik Taneja,Aamer Jaleel,Moinuddin Qureshi*

Key words: GPU内存带宽，LLM推理，MoE模型，推测解码，Cascade框架

TL;DR: GPU内存带宽是低延迟大型语言模型(LLM)推理的主要瓶颈。MoE模型中传统推测解码效果不佳，导致数据移动和验证时间增加。Cascade框架通过动态调整K值和选择性启用推测解码，优化了MoE模型的推理效率。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 解决MoE模型中推测解码效率低下问题，避免因数据移动和验证时间增加导致的性能下降。

Method: 提出Cascade框架，通过轻量级指标“推测效用”动态调整K值，并选择性启用推测解码。

Result: Cascade将减速限制在5%以内，吞吐量提升7-14%，使推测解码在MoE模型中实用化。

Conclusion: Cascade框架为MoE模型提供了一种高效的推测解码解决方案，显著提升了推理性能。

Abstract: GPU memory bandwidth is the main bottleneck for low-latency Large Language
Model (LLM) inference. Speculative decoding leverages idle GPU compute by using
a lightweight drafter to propose K tokens, which the LLM verifies in parallel,
boosting token throughput. In conventional dense LLMs, all model weights are
fetched each iteration, so speculation adds no latency overhead. Emerging
Mixture of Experts (MoE) models activate only a subset of weights per token,
greatly reducing data movement. However, we show that speculation is
ineffective for MoEs: draft tokens collectively activate more weights,
increasing data movement and verification time by 2-3x. When token throughput
gains fail to offset this overhead, speculation causes slowdowns up to 1.5x,
making it infeasible. Even when useful, the optimal K varies by task, model,
and even between requests and iterations. Thus, despite widespread use in dense
LLMs, speculation remains impractical in leading MoEs.
  We present Cascade, a utility-driven framework that selectively enables
speculation to avoid slowdowns and dynamically tunes K to accelerate MoE
serving. Cascade uses a lightweight metric, speculation utility, the ratio of
token gains to verification cost, which shows iteration-level locality,
enabling periodic decisions via short test and longer set phases. For each
request, Cascade disables speculation if utility drops below one during
testing, and when utility exceeds one, tests multiple K-values to choose the
utility-maximizing K for the set phase. We implement Cascade in vLLM and
evaluate it on five popular MoEs with workloads spanning code, math,
extraction, and mixed tasks. Cascade limits slowdown to 5% (vs. 1.5x) and
improves throughput by 7-14% over static K, making speculative decoding
practical for MoEs.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [145] [DRAGON: Distributional Rewards Optimize Diffusion Generative Models](https://arxiv.org/abs/2504.15217)
*Yatong Bai,Jonah Casebeer,Somayeh Sojoudi,Nicholas J. Bryan*

Key words: DRAGON, 奖励函数, 媒体生成, 强化学习

TL;DR: DRAGON是一个灵活的框架，用于优化媒体生成模型，适用于多种奖励函数类型，其性能优于传统方法，并在实验中表现出色。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 为了解决传统强化学习方法（如RLHF或DPO）在灵活性和适用范围上的限制，提出DRAGON框架以支持更广泛的奖励函数类型。

Method: DRAGON通过选择编码器和参考示例构建奖励函数，利用在线和策略生成的正负示例对比来最大化奖励。

Result: DRAGON在20种不同奖励函数上的实验中平均胜率为81.45%，并且在未使用人类偏好标注的情况下，音乐质量胜率达到60.95%。

Conclusion: DRAGON为设计和优化奖励函数提供了新方法，能够显著提升人类感知的质量。

Abstract: We present Distributional RewArds for Generative OptimizatioN (DRAGON), a
versatile framework for fine-tuning media generation models towards a desired
outcome. Compared with traditional reinforcement learning with human feedback
(RLHF) or pairwise preference approaches such as direct preference optimization
(DPO), DRAGON is more flexible. It can optimize reward functions that evaluate
either individual examples or distributions of them, making it compatible with
a broad spectrum of instance-wise, instance-to-distribution, and
distribution-to-distribution rewards. Leveraging this versatility, we construct
novel reward functions by selecting an encoder and a set of reference examples
to create an exemplar distribution. When cross-modality encoders such as CLAP
are used, the reference examples may be of a different modality (e.g., text
versus audio). Then, DRAGON gathers online and on-policy generations, scores
them to construct a positive demonstration set and a negative set, and
leverages the contrast between the two sets to maximize the reward. For
evaluation, we fine-tune an audio-domain text-to-music diffusion model with 20
different reward functions, including a custom music aesthetics model, CLAP
score, Vendi diversity, and Frechet audio distance (FAD). We further compare
instance-wise (per-song) and full-dataset FAD settings while ablating multiple
FAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an
81.45% average win rate. Moreover, reward functions based on exemplar sets
indeed enhance generations and are comparable to model-based rewards. With an
appropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality
win rate without training on human preference annotations. As such, DRAGON
exhibits a new approach to designing and optimizing reward functions for
improving human-perceived quality. Sound examples at
https://ml-dragon.github.io/web.

</details>


### [146] [Exploring Adapter Design Tradeoffs for Low Resource Music Generation](https://arxiv.org/abs/2506.21298)
*Atharva Mehta,Shivam Chauhan,Monojit Choudhury*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Fine-tuning large-scale music generation models, such as MusicGen and
Mustango, is a computationally expensive process, often requiring updates to
billions of parameters and, therefore, significant hardware resources.
Parameter-Efficient Fine-Tuning (PEFT) techniques, particularly adapter-based
methods, have emerged as a promising alternative, enabling adaptation with
minimal trainable parameters while preserving model performance. However, the
design choices for adapters, including their architecture, placement, and size,
are numerous, and it is unclear which of these combinations would produce
optimal adapters and why, for a given case of low-resource music genre. In this
paper, we attempt to answer this question by studying various adapter
configurations for two AI music models, MusicGen and Mustango, on two genres:
Hindustani Classical and Turkish Makam music.
  Our findings reveal distinct trade-offs: convolution-based adapters excel in
capturing fine-grained local musical details such as ornamentations and short
melodic phrases, while transformer-based adapters better preserve long-range
dependencies crucial for structured improvisation. Additionally, we analyze
computational resource requirements across different adapter scales,
demonstrating how mid-sized adapters (40M parameters) achieve an optimal
balance between expressivity and quality. Furthermore, we find that Mustango, a
diffusion-based model, generates more diverse outputs with better adherence to
the description in the input prompt while lacking in providing stability in
notes, rhythm alignment, and aesthetics. Also, it is computationally intensive
and requires significantly more time to train. In contrast, autoregressive
models like MusicGen offer faster training and are more efficient, and can
produce better quality output in comparison, but have slightly higher
redundancy in their generations.

</details>


### [147] [A Hierarchical Deep Learning Approach for Minority Instrument Detection](https://arxiv.org/abs/2506.21167)
*Dylan Sechet,Francesca Bugiotti,Matthieu Kowalski,Edouard d'Hérouville,Filip Langiewicz*

Key words: 乐器识别, 音乐信息检索, 层次分类, Hornbostel-Sachs分类, MedleyDB数据集

TL;DR: 论文探讨了在音乐信息检索中识别音频片段中乐器活动的重要性，提出了一种基于Hornbostel-Sachs分类的层次分类方法，并在MedleyDB数据集上验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 针对音乐信息检索中乐器识别的需求，尤其是解决精细乐器标注数据不足的问题，通过层次分类方法提升乐器活动的检测能力。

Method: 采用了基于Hornbostel-Sachs分类的层次分类系统，结合MedleyDB数据集，测试了新的层次音乐预测模型。

Result: 研究表明，层次分类方法在粗粒度乐器检测上表现更可靠，为乐器识别与组别识别之间的差距提供了解决方案。

Conclusion: 层次分类方法为音乐信息检索中的乐器识别提供了新的思路，有助于未来在该领域的进一步研究。

Abstract: Identifying instrument activities within audio excerpts is vital in music
information retrieval, with significant implications for music cataloging and
discovery. Prior deep learning endeavors in musical instrument recognition have
predominantly emphasized instrument classes with ample data availability.
Recent studies have demonstrated the applicability of hierarchical
classification in detecting instrument activities in orchestral music, even
with limited fine-grained annotations at the instrument level. Based on the
Hornbostel-Sachs classification, such a hierarchical classification system is
evaluated using the MedleyDB dataset, renowned for its diversity and richness
concerning various instruments and music genres. This work presents various
strategies to integrate hierarchical structures into models and tests a new
class of models for hierarchical music prediction. This study showcases more
reliable coarse-level instrument detection by bridging the gap between detailed
instrument identification and group-level recognition, paving the way for
further advancements in this domain.

</details>


### [148] [Integrating Vehicle Acoustic Data for Enhanced Urban Traffic Management: A Study on Speed Classification in Suzhou](https://arxiv.org/abs/2506.21269)
*Pengfei Fan,Yuli Zhang,Xinheng Wang,Ruiyuan Jiang,Hankang Gu,Dongyao Jia,Shangbo Wang*

Key words: 声学数据集, 双模态特征融合, 深度学习, 智能交通, 噪声监测

TL;DR: 该研究公开了苏州城市道路声学数据集（SZUR-Acoustic Dataset），并提出了一种双模态特征融合深度卷积神经网络（BMCNN），用于建模车辆噪声与驾驶速度的耦合关系。实验结果表明，该方法在两个数据集上表现优越，并验证了各模块对性能提升和过拟合抑制的贡献。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 通过声学数据建模车辆噪声与速度的关系，为智能城市交通管理系统提供实时噪声监测和速度估计工具，优化交通流量控制、减少路边噪声污染，并支持可持续城市规划。

Method: 采用自适应去噪和归一化策略预处理数据，构建双模态特征融合网络（BMCNN），通过并行分支提取MFCC和小波包能量特征，并利用跨模态注意力机制融合特征。

Result: BMCNN在SZUR-Acoustic Dataset和IDMT-Traffic数据集上的分类准确率分别为87.56%和96.28%。消融实验和鲁棒性测试验证了各模块的有效性。

Conclusion: 该方法为智能交通管理提供了有效工具，能够实时监测噪声和估计速度，从而优化交通控制和减少噪声污染。

Abstract: This study presents and publicly releases the Suzhou Urban Road Acoustic
Dataset (SZUR-Acoustic Dataset), which is accompanied by comprehensive
data-acquisition protocols and annotation guidelines to ensure transparency and
reproducibility of the experimental workflow. To model the coupling between
vehicular noise and driving speed, we propose a bimodal-feature-fusion deep
convolutional neural network (BMCNN). During preprocessing, an adaptive
denoising and normalization strategy is applied to suppress environmental
background interference; in the network architecture, parallel branches extract
Mel-frequency cepstral coefficients (MFCCs) and wavelet-packet energy features,
which are subsequently fused via a cross-modal attention mechanism in the
intermediate feature space to fully exploit time-frequency information.
Experimental results demonstrate that BMCNN achieves a classification accuracy
of 87.56% on the SZUR-Acoustic Dataset and 96.28% on the public IDMT-Traffic
dataset. Ablation studies and robustness tests on the Suzhou dataset further
validate the contributions of each module to performance improvement and
overfitting mitigation. The proposed acoustics-based speed classification
method can be integrated into smart-city traffic management systems for
real-time noise monitoring and speed estimation, thereby optimizing traffic
flow control, reducing roadside noise pollution, and supporting sustainable
urban planning.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [149] [From On-chain to Macro: Assessing the Importance of Data Source Diversity in Cryptocurrency Market Forecasting](https://arxiv.org/abs/2506.21246)
*Giorgos Demosthenous,Chryssis Georgiou,Eliada Polydorou*

Key words: 加密货币预测, 数据源多样性, 特征降维, 预测模型, 链上指标

TL;DR: 通过整合多种数据源（如技术指标、链上指标、情绪与兴趣指标、传统市场指数和宏观经济指标），研究数据源多样性对加密货币预测模型性能的影响。

<details>
  <summary>Details</summary>

Main category: q-fin.PM

Motivation: 旨在明确数据源多样性对加密货币预测模型的重要性，并揭示短期和长期市场驱动因素。

Method: 提出Crypto100指数和一种新型特征降维算法，用以识别最具影响力的数据特征。

Result: 实验表明数据源多样性显著提升预测模型性能，链上指标对短期和长期预测尤为关键，传统市场指数和宏观经济指标在长期预测中作用增强。

Conclusion: 多样化数据源能显著提高预测模型的准确性和鲁棒性，为开发更优模型奠定基础。

Abstract: This study investigates the impact of data source diversity on the
performance of cryptocurrency forecasting models by integrating various data
categories, including technical indicators, on-chain metrics, sentiment and
interest metrics, traditional market indices, and macroeconomic indicators. We
introduce the Crypto100 index, representing the top 100 cryptocurrencies by
market capitalization, and propose a novel feature reduction algorithm to
identify the most impactful and resilient features from diverse data sources.
Our comprehensive experiments demonstrate that data source diversity
significantly enhances the predictive performance of forecasting models across
different time horizons. Key findings include the paramount importance of
on-chain metrics for both short-term and long-term predictions, the growing
relevance of traditional market indices and macroeconomic indicators for
longer-term forecasts, and substantial improvements in model accuracy when
diverse data sources are utilized. These insights help demystify the short-term
and long-term driving factors of the cryptocurrency market and lay the
groundwork for developing more accurate and resilient forecasting models.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [150] [Structural System Identification via Validation and Adaptation](https://arxiv.org/abs/2506.20799)
*Cristian López,Keegan J. Moore*

Key words: 结构系统识别,参数估计,生成模型,神经网络

TL;DR: 提出了一种基于生成模型的结构系统识别方法，通过神经网络从数据中估计参数并验证模型。

<details>
  <summary>Details</summary>

Main category: math.DS

Motivation: 估计动态系统的参数值对于结合实验数据和科学理论进行理解和预测至关重要。

Method: 使用生成模型框架，通过神经网络将随机噪声映射到物理参数，并与已知运动方程生成加速度，通过误差损失和判别网络验证参数。

Result: 分析和实验验证了参数估计的准确性和模型的有效性。

Conclusion: 该方法成功实现了非线性结构系统的参数估计和验证。

Abstract: Estimating the governing equation parameter values is essential for
integrating experimental data with scientific theory to understand, validate,
and predict the dynamics of complex systems. In this work, we propose a new
method for structural system identification (SI), uncertainty quantification,
and validation directly from data. Inspired by generative modeling frameworks,
a neural network maps random noise to physically meaningful parameters. These
parameters are then used in the known equation of motion to obtain fake
accelerations, which are compared to real training data via a mean square error
loss. To simultaneously validate the learned parameters, we use independent
validation datasets. The generated accelerations from these datasets are
evaluated by a discriminator network, which determines whether the output is
real or fake, and guides the parameter-generator network. Analytical and real
experiments show the parameter estimation accuracy and model validation for
different nonlinear structural systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [151] [Hybrid Deep Learning and Signal Processing for Arabic Dialect Recognition in Low-Resource Settings](https://arxiv.org/abs/2506.21386)
*Ghazal Al-Shwayyat,Omer Nezih Gerek*

Key words: 阿拉伯语方言识别、混合模型、MFCC、CNN、低资源场景

TL;DR: 论文研究了在低资源场景下，结合传统信号处理技术与深度学习架构的混合模型用于阿拉伯语方言识别，发现MFCC+CNN模型表现最佳，准确率达91.2%。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 阿拉伯语方言识别因语言多样性和标注数据稀缺而具有挑战性，尤其在资源有限的情况下。

Method: 开发了两种混合模型：MFCC+CNN和DWT+RNN，并基于Common Voice Arabic数据集的方言标注子集进行训练和评估。

Result: MFCC+CNN模型准确率91.2%，显著优于DWT+RNN模型的66.5%。

Conclusion: 结合频谱特征与卷积模型在低资源场景下效果显著，未来可扩展数据集或探索自监督学习。

Abstract: Arabic dialect recognition presents a significant challenge in speech
technology due to the linguistic diversity of Arabic and the scarcity of large
annotated datasets, particularly for underrepresented dialects. This research
investigates hybrid modeling strategies that integrate classical signal
processing techniques with deep learning architectures to address this problem
in low-resource scenarios. Two hybrid models were developed and evaluated: (1)
Mel-Frequency Cepstral Coefficients (MFCC) combined with a Convolutional Neural
Network (CNN), and (2) Discrete Wavelet Transform (DWT) features combined with
a Recurrent Neural Network (RNN). The models were trained on a dialect-filtered
subset of the Common Voice Arabic dataset, with dialect labels assigned based
on speaker metadata. Experimental results demonstrate that the MFCC + CNN
architecture achieved superior performance, with an accuracy of 91.2% and
strong precision, recall, and F1-scores, significantly outperforming the
Wavelet + RNN configuration, which achieved an accuracy of 66.5%. These
findings highlight the effectiveness of leveraging spectral features with
convolutional models for Arabic dialect recognition, especially when working
with limited labeled data. The study also identifies limitations related to
dataset size, potential regional overlaps in labeling, and model optimization,
providing a roadmap for future research. Recommendations for further
improvement include the adoption of larger annotated corpora, integration of
self-supervised learning techniques, and exploration of advanced neural
architectures such as Transformers. Overall, this research establishes a strong
baseline for future developments in Arabic dialect recognition within
resource-constrained environments.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [152] [Global and Local Contrastive Learning for Joint Representations from Cardiac MRI and ECG](https://arxiv.org/abs/2506.20683)
*Alexander Selivanov,Philip Müller,Özgün Turgut,Nil Stolt-Ansó,Daniel Rückert*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: An electrocardiogram (ECG) is a widely used, cost-effective tool for
detecting electrical abnormalities in the heart. However, it cannot directly
measure functional parameters, such as ventricular volumes and ejection
fraction, which are crucial for assessing cardiac function. Cardiac magnetic
resonance (CMR) is the gold standard for these measurements, providing detailed
structural and functional insights, but is expensive and less accessible. To
bridge this gap, we propose PTACL (Patient and Temporal Alignment Contrastive
Learning), a multimodal contrastive learning framework that enhances ECG
representations by integrating spatio-temporal information from CMR. PTACL uses
global patient-level contrastive loss and local temporal-level contrastive
loss. The global loss aligns patient-level representations by pulling ECG and
CMR embeddings from the same patient closer together, while pushing apart
embeddings from different patients. Local loss enforces fine-grained temporal
alignment within each patient by contrasting encoded ECG segments with
corresponding encoded CMR frames. This approach enriches ECG representations
with diagnostic information beyond electrical activity and transfers more
insights between modalities than global alignment alone, all without
introducing new learnable weights. We evaluate PTACL on paired ECG-CMR data
from 27,951 subjects in the UK Biobank. Compared to baseline approaches, PTACL
achieves better performance in two clinically relevant tasks: (1) retrieving
patients with similar cardiac phenotypes and (2) predicting CMR-derived cardiac
function parameters, such as ventricular volumes and ejection fraction. Our
results highlight the potential of PTACL to enhance non-invasive cardiac
diagnostics using ECG. The code is available at:
https://github.com/alsalivan/ecgcmr

</details>


### [153] [U-R-VEDA: Integrating UNET, Residual Links, Edge and Dual Attention, and Vision Transformer for Accurate Semantic Segmentation of CMRs](https://arxiv.org/abs/2506.20689)
*Racheal Mukisa,Arvind K. Bansal*

Key words: 心脏MRI,语义分割,深度学习,U-R-Veda,注意力机制

TL;DR: 提出了一种结合卷积和视觉Transformer的U-R-Veda模型，用于心脏MRI图像的语义分割，显著提高了准确性。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 自动化准确的心脏图像分割是心脏病诊断和管理的关键步骤，为医学图像分析提供基础。

Method: U-R-Veda模型整合了卷积变换、视觉Transformer、残差连接、通道和空间注意力机制，以及基于边缘检测的跳跃连接。

Result: 模型在DSC指标上达到95.2%的平均准确率，显著优于其他模型。

Conclusion: U-R-Veda模型在心脏MRI图像分割中表现出色，为医学图像分析提供了更高精度的方法。

Abstract: Artificial intelligence, including deep learning models, will play a
transformative role in automated medical image analysis for the diagnosis of
cardiac disorders and their management. Automated accurate delineation of
cardiac images is the first necessary initial step for the quantification and
automated diagnosis of cardiac disorders. In this paper, we propose a deep
learning based enhanced UNet model, U-R-Veda, which integrates convolution
transformations, vision transformer, residual links, channel-attention, and
spatial attention, together with edge-detection based skip-connections for an
accurate fully-automated semantic segmentation of cardiac magnetic resonance
(CMR) images. The model extracts local-features and their interrelationships
using a stack of combination convolution blocks, with embedded channel and
spatial attention in the convolution block, and vision transformers. Deep
embedding of channel and spatial attention in the convolution block identifies
important features and their spatial localization. The combined edge
information with channel and spatial attention as skip connection reduces
information-loss during convolution transformations. The overall model
significantly improves the semantic segmentation of CMR images necessary for
improved medical image analysis. An algorithm for the dual attention module
(channel and spatial attention) has been presented. Performance results show
that U-R-Veda achieves an average accuracy of 95.2%, based on DSC metrics. The
model outperforms the accuracy attained by other models, based on DSC and HD
metrics, especially for the delineation of right-ventricle and
left-ventricle-myocardium.

</details>


### [154] [A Novel Framework for Integrating 3D Ultrasound into Percutaneous Liver Tumour Ablation](https://arxiv.org/abs/2506.21162)
*Shuwei Xing,Derek W. Cool,David Tessier,Elvis C. S. Chen,Terry M. Peters,Aaron Fenster*

Key words: 3D超声, 肿瘤消融, 图像配准, 多模态可视化

TL;DR: 提出了一种将3D超声整合到标准消融工作流程的新框架，通过2D超声-CT/MRI配准降低复杂性，并展示了其临床效果。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 解决3D超声在经皮肝肿瘤消融中肿瘤识别的挑战，推动其临床治疗应用。

Method: 提出2D超声-CT/MRI配准方法，利用3D超声作为中介，并开发多模态图像可视化技术。

Result: 2D配准误差2-4mm，运行时间0.22s/对；非刚性配准比刚性配准误差降低40%。

Conclusion: 框架提升了3D超声在肿瘤消融中的作用，扩展了其在临床治疗中的应用潜力。

Abstract: 3D ultrasound (US) imaging has shown significant benefits in enhancing the
outcomes of percutaneous liver tumour ablation. Its clinical integration is
crucial for transitioning 3D US into the therapeutic domain. However,
challenges of tumour identification in US images continue to hinder its broader
adoption. In this work, we propose a novel framework for integrating 3D US into
the standard ablation workflow. We present a key component, a clinically viable
2D US-CT/MRI registration approach, leveraging 3D US as an intermediary to
reduce registration complexity. To facilitate efficient verification of the
registration workflow, we also propose an intuitive multimodal image
visualization technique. In our study, 2D US-CT/MRI registration achieved a
landmark distance error of approximately 2-4 mm with a runtime of 0.22s per
image pair. Additionally, non-rigid registration reduced the mean alignment
error by approximately 40% compared to rigid registration. Results demonstrated
the efficacy of the proposed 2D US-CT/MRI registration workflow. Our
integration framework advanced the capabilities of 3D US imaging in improving
percutaneous tumour ablation, demonstrating the potential to expand the
therapeutic role of 3D US in clinical interventions.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [155] [Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware Diffusion and Temporal Video Models](https://arxiv.org/abs/2506.20946)
*Donggoo Kang,Jangyeong Kim,Dasol Jeong,Junyoung Choi,Jeonga Wi,Hyunmin Lee,Joonho Gwon,Joonki Paik*

Key words: 纹理合成, 视频生成, 3D纹理, 几何感知, UV扩散

TL;DR: 本文介绍了VideoTex，一种利用视频生成模型解决3D纹理中空间和时间不一致性的新框架，通过几何感知条件和结构化的UV扩散策略，实现了更平滑且一致的纹理合成。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 当前纹理合成方法因缺乏全局上下文和几何理解而导致不一致，而视频生成模型的成功为解决这一问题提供了灵感。

Method: VideoTex结合几何感知条件和结构化的UV扩散策略，利用3D网格结构提升纹理合成质量。

Result: VideoTex在纹理保真度、接缝融合和稳定性上优于现有方法，适合动态实时应用。

Conclusion: VideoTex通过视频生成模型和几何条件，显著提升了纹理合成的时空一致性，为高质量动态纹理应用奠定了基础。

Abstract: Current texture synthesis methods, which generate textures from fixed
viewpoints, suffer from inconsistencies due to the lack of global context and
geometric understanding. Meanwhile, recent advancements in video generation
models have demonstrated remarkable success in achieving temporally consistent
videos. In this paper, we introduce VideoTex, a novel framework for seamless
texture synthesis that leverages video generation models to address both
spatial and temporal inconsistencies in 3D textures. Our approach incorporates
geometry-aware conditions, enabling precise utilization of 3D mesh structures.
Additionally, we propose a structure-wise UV diffusion strategy, which enhances
the generation of occluded areas by preserving semantic information, resulting
in smoother and more coherent textures. VideoTex not only achieves smoother
transitions across UV boundaries but also ensures high-quality, temporally
stable textures across video frames. Extensive experiments demonstrate that
VideoTex outperforms existing methods in texture fidelity, seam blending, and
stability, paving the way for dynamic real-time applications that demand both
visual quality and temporal coherence.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [156] [Poster: Enhancing GNN Robustness for Network Intrusion Detection via Agent-based Analysis](https://arxiv.org/abs/2506.20806)
*Zhonghao Zhan,Huichi Zhou,Hamed Haddadi*

Key words: 图神经网络（GNN），网络入侵检测系统（NIDS），大型语言模型（LLM），对抗攻击，物联网（IoT）

TL;DR: 该论文提出了一种结合大型语言模型（LLM）与图神经网络（GNN）的新方法，以提升网络入侵检测系统（NIDS）在对抗攻击和分布漂移情况下的鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 当前GNN在物联网（IoT）环境中的入侵检测性能因对抗攻击和分布漂移而下降，且现有评估方法不够现实。

Method: 通过将LLM作为模拟网络安全专家代理，分析网络流数据生成的图结构，识别并减轻可疑或对抗性扰动元素。

Result: 实验表明，LLM的集成显著提升了GNN在各类对抗攻击下的鲁棒性。

Conclusion: LLM可作为NIDS架构中的补充层，提高其在现实环境中的适用性。

Abstract: Graph Neural Networks (GNNs) show great promise for Network Intrusion
Detection Systems (NIDS), particularly in IoT environments, but suffer
performance degradation due to distribution drift and lack robustness against
realistic adversarial attacks. Current robustness evaluations often rely on
unrealistic synthetic perturbations and lack demonstrations on systematic
analysis of different kinds of adversarial attack, which encompass both
black-box and white-box scenarios. This work proposes a novel approach to
enhance GNN robustness and generalization by employing Large Language Models
(LLMs) in an agentic pipeline as simulated cybersecurity expert agents. These
agents scrutinize graph structures derived from network flow data, identifying
and potentially mitigating suspicious or adversarially perturbed elements
before GNN processing. Our experiments, using a framework designed for
realistic evaluation and testing with a variety of adversarial attacks
including a dataset collected from physical testbed experiments, demonstrate
that integrating LLM analysis can significantly improve the resilience of
GNN-based NIDS against challenges, showcasing the potential of LLM agent as a
complementary layer in intrusion detection architectures.

</details>


### [157] [ZKPROV: A Zero-Knowledge Approach to Dataset Provenance for Large Language Models](https://arxiv.org/abs/2506.20915)
*Mina Namazi,Alexander Nemecek,Erman Ayday*

Key words: 大语言模型, 零知识证明, 数据来源验证, 隐私保护, 密码学框架

TL;DR: ZKPROV是一个新颖的密码学框架，通过零知识证明验证大语言模型（LLM）的数据来源，确保训练数据可靠且不泄露敏感信息。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 在大语言模型部署于敏感领域（如医疗）时，确保其计算来源的完整性至关重要。现有方法要么计算成本高，要么依赖可信执行环境，ZKPROV旨在提供一种平衡的解决方案。

Method: ZKPROV利用零知识证明将训练模型与授权数据集绑定，避免验证每个训练步骤，同时使用数据集签名元数据和紧凑模型参数承诺来保证隐私。

Result: 实验结果显示ZKPROV能高效生成和验证证明，适用于实际部署，并提供了形式化的安全保障。

Conclusion: ZKPROV在保护数据集机密性的同时，确保了可信的数据来源验证，为大语言模型在敏感领域的应用提供了实用解决方案。

Abstract: As the deployment of large language models (LLMs) grows in sensitive domains,
ensuring the integrity of their computational provenance becomes a critical
challenge, particularly in regulated sectors such as healthcare, where strict
requirements are applied in dataset usage. We introduce ZKPROV, a novel
cryptographic framework that enables zero-knowledge proofs of LLM provenance.
It allows users to verify that a model is trained on a reliable dataset without
revealing sensitive information about it or its parameters. Unlike prior
approaches that focus on complete verification of the training process
(incurring significant computational cost) or depend on trusted execution
environments, ZKPROV offers a distinct balance. Our method cryptographically
binds a trained model to its authorized training dataset(s) through
zero-knowledge proofs while avoiding proof of every training step. By
leveraging dataset-signed metadata and compact model parameter commitments,
ZKPROV provides sound and privacy-preserving assurances that the result of the
LLM is derived from a model trained on the claimed authorized and relevant
dataset. Experimental results demonstrate the efficiency and scalability of the
ZKPROV in generating this proof and verifying it, achieving a practical
solution for real-world deployments. We also provide formal security
guarantees, proving that our approach preserves dataset confidentiality while
ensuring trustworthy dataset provenance.

</details>


### [158] [PhishKey: A Novel Centroid-Based Approach for Enhanced Phishing Detection Using Adaptive HTML Component Extraction](https://arxiv.org/abs/2506.21106)
*Felipe Castaño,Eduardo Fidalgo,Enrique Alegre,Rocio Alaiz-Rodríguez,Raul Orduna,Francesco Zola*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Phishing attacks pose a significant cybersecurity threat, evolving rapidly to
bypass detection mechanisms and exploit human vulnerabilities. This paper
introduces PhishKey to address the challenges of adaptability, robustness, and
efficiency. PhishKey is a novel phishing detection method using automatic
feature extraction from hybrid sources. PhishKey combines character-level
processing with Convolutional Neural Networks (CNN) for URL classification, and
a Centroid-Based Key Component Phishing Extractor (CAPE) for HTML content at
the word level. CAPE reduces noise and ensures complete sample processing
avoiding crop operations on the input data. The predictions from both modules
are integrated using a soft-voting ensemble to achieve more accurate and
reliable classifications. Experimental evaluations on four state-of-the-art
datasets demonstrate the effectiveness of PhishKey. It achieves up to 98.70% F1
Score and shows strong resistance to adversarial manipulations such as
injection attacks with minimal performance degradation.

</details>


### [159] [Empowering Digital Agriculture: A Privacy-Preserving Framework for Data Sharing and Collaborative Research](https://arxiv.org/abs/2506.20872)
*Osama Zafar,Rosemarie Santa González,Mina Namazi,Alfonso Morales,Erman Ayday*

Key words: 数据驱动农业, 隐私保护, 差分隐私, 联邦学习

TL;DR: 提出了一种隐私保护框架，结合降维和差分隐私技术，支持农民安全共享数据并促成协作研究，同时保护敏感信息。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 数据驱动农业的潜力受到隐私问题的限制，农民担心数据共享会被滥用。

Method: 结合主成分分析（PCA）和拉普拉斯噪声的差分隐私技术，支持联邦学习和数据聚合。

Result: 实证验证表明框架能抵御对抗性攻击，性能接近集中式系统。

Conclusion: 该框架可促进农民协作和研究目标实现，推动农业数据的安全整合和创新。

Abstract: Data-driven agriculture, which integrates technology and data into
agricultural practices, has the potential to improve crop yield, disease
resilience, and long-term soil health. However, privacy concerns, such as
adverse pricing, discrimination, and resource manipulation, deter farmers from
sharing data, as it can be used against them. To address this barrier, we
propose a privacy-preserving framework that enables secure data sharing and
collaboration for research and development while mitigating privacy risks. The
framework combines dimensionality reduction techniques (like Principal
Component Analysis (PCA)) and differential privacy by introducing Laplacian
noise to protect sensitive information. The proposed framework allows
researchers to identify potential collaborators for a target farmer and train
personalized machine learning models either on the data of identified
collaborators via federated learning or directly on the aggregated
privacy-protected data. It also allows farmers to identify potential
collaborators based on similarities. We have validated this on real-life
datasets, demonstrating robust privacy protection against adversarial attacks
and utility performance comparable to a centralized system. We demonstrate how
this framework can facilitate collaboration among farmers and help researchers
pursue broader research objectives. The adoption of the framework can empower
researchers and policymakers to leverage agricultural data responsibly, paving
the way for transformative advances in data-driven agriculture. By addressing
critical privacy challenges, this work supports secure data integration,
fostering innovation and sustainability in agricultural systems.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [160] [Enhancing Homophily-Heterophily Separation: Relation-Aware Learning in Heterogeneous Graphs](https://arxiv.org/abs/2506.20980)
*Ziyu Zheng,Yaming Yang,Ziyu Guan,Wei Zhao,Weigang Lu*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.SI

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Real-world networks usually have a property of node heterophily, that is, the
connected nodes usually have different features or different labels. This
heterophily issue has been extensively studied in homogeneous graphs but
remains under-explored in heterogeneous graphs, where there are multiple types
of nodes and edges. Capturing node heterophily in heterogeneous graphs is very
challenging since both node/edge heterogeneity and node heterophily should be
carefully taken into consideration. Existing methods typically convert
heterogeneous graphs into homogeneous ones to learn node heterophily, which
will inevitably lose the potential heterophily conveyed by heterogeneous
relations. To bridge this gap, we propose Relation-Aware Separation of
Homophily and Heterophily (RASH), a novel contrastive learning framework that
explicitly models high-order semantics of heterogeneous interactions and
adaptively separates homophilic and heterophilic patterns. Particularly, RASH
introduces dual heterogeneous hypergraphs to encode multi-relational bipartite
subgraphs and dynamically constructs homophilic graphs and heterophilic graphs
based on relation importance. A multi-relation contrastive loss is designed to
align heterogeneous and homophilic/heterophilic views by maximizing mutual
information. In this way, RASH simultaneously resolves the challenges of
heterogeneity and heterophily in heterogeneous graphs. Extensive experiments on
benchmark datasets demonstrate the effectiveness of RASH across various
downstream tasks. The code is available at:
https://github.com/zhengziyu77/RASH.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [161] [Transformer-Based Spatial-Temporal Counterfactual Outcomes Estimation](https://arxiv.org/abs/2506.21154)
*He Li,Haoang Chi,Mingyu Liu,Wanrong Huang,Liyang Xu,Wenjing Yang*

Key words: 反事实结果, 时空数据, Transformer, 因果效应

TL;DR: 提出了一种基于Transformer的新框架，用于估计具有时空属性的反事实结果，表现出更强的估计能力。

<details>
  <summary>Details</summary>

Main category: stat.ME

Motivation: 现实世界具有时间和空间维度，而现有方法在性能和泛化能力上存在局限，需要在时空属性下改进反事实结果估计。

Method: 使用Transformer框架估计时空反事实结果，并提出了一种一致且渐近正态的估计器。

Result: 模拟实验表明其优于基线方法，真实数据实验分析了哥伦比亚冲突对森林损失的因果效应。

Conclusion: 提出的方法在时空反事实估计中表现出更强的能力，为实际问题提供了有价值的洞见。

Abstract: The real world naturally has dimensions of time and space. Therefore,
estimating the counterfactual outcomes with spatial-temporal attributes is a
crucial problem. However, previous methods are based on classical statistical
models, which still have limitations in performance and generalization. This
paper proposes a novel framework for estimating counterfactual outcomes with
spatial-temporal attributes using the Transformer, exhibiting stronger
estimation ability. Under mild assumptions, the proposed estimator within this
framework is consistent and asymptotically normal. To validate the
effectiveness of our approach, we conduct simulation experiments and real data
experiments. Simulation experiments show that our estimator has a stronger
estimation capability than baseline methods. Real data experiments provide a
valuable conclusion to the causal effect of conflicts on forest loss in
Colombia. The source code is available at
https://github.com/lihe-maxsize/DeppSTCI_Release_Version-master.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [162] [On Uniform Weighted Deep Polynomial approximation](https://arxiv.org/abs/2506.21306)
*Kingsley Yeon,Steven B. Damelin*

Key words: 有理逼近, 深度多项式, 非光滑函数, 加权近似, 参数化

TL;DR: 本文提出了一种加权深度多项式近似方法，针对具有不对称行为的函数（如一边无限增长而另一边衰减的函数），通过结合可学习的深度多项式和单侧权重，有效捕捉局部非光滑性和全局增长特性。数值实验表明，该方法优于泰勒、切比雪夫和标准深度多项式近似方法。

<details>
  <summary>Details</summary>

Main category: math.NA

Motivation: 传统有理逼近方法对非光滑或奇异函数（如|x|和x^{1/p}）具有根指数收敛性，而多项式逼近仅能实现代数收敛。虽然已有研究表明复合多项式架构可以实现指数逼近速率，但仍需针对特定函数行为（如不对称性）设计更高效的逼近方法。

Method: 提出了一种加权深度多项式近似框架，将可学习的深度多项式与单侧权重相乘，以同时捕捉局部非光滑性和全局增长特性。此外，还提出了一种基于图的稳定参数化策略，用于优化这些近似函数。

Result: 数值实验显示，所提出的加权深度多项式近似方法在参数数量相同的情况下，性能优于泰勒、切比雪夫和标准深度多项式近似方法。

Conclusion: 加权深度多项式近似方法能有效处理具有不对称行为的函数，为高精度逼近提供了新途径。

Abstract: It is a classical result in rational approximation theory that certain
non-smooth or singular functions, such as $|x|$ and $x^{1/p}$, can be
efficiently approximated using rational functions with root-exponential
convergence in terms of degrees of freedom \cite{Sta, GN}. In contrast,
polynomial approximations admit only algebraic convergence by Jackson's theorem
\cite{Lub2}. Recent work shows that composite polynomial architectures can
recover exponential approximation rates even without smoothness \cite{KY}. In
this work, we introduce and analyze a class of weighted deep polynomial
approximants tailored for functions with asymmetric behavior-growing unbounded
on one side and decaying on the other. By multiplying a learnable deep
polynomial with a one-sided weight, we capture both local non-smoothness and
global growth. We show numerically that this framework outperforms Taylor,
Chebyshev, and standard deep polynomial approximants, even when all use the
same number of parameters. To optimize these approximants in practice, we
propose a stable graph-based parameterization strategy building on \cite{Jar}.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [163] [Spiking Neural Networks for SAR Interferometric Phase Unwrapping: A Theoretical Framework for Energy-Efficient Processing](https://arxiv.org/abs/2506.20782)
*Marc Bara*

Key words: 脉冲神经网络, SAR干涉, 相位解缠, 能效, 神经形态计算

TL;DR: 首次提出将脉冲神经网络（SNN）应用于合成孔径雷达（SAR）干涉相位解缠的理论框架，填补了研究空白，并展示了SNN在能效方面的潜力。

<details>
  <summary>Details</summary>

Main category: cs.NE

Motivation: 随着地球观测数据量激增（如NISAR任务将在两年内生成100PB数据），高效能处理对可持续数据中心运营至关重要。SNN的事件驱动计算模型可节约30-100倍能源，同时保持精度。

Method: 开发针对相位数据的脉冲编码方案，提出利用相位解缠空间传播特性的SNN架构，并分析计算复杂度和收敛性。

Result: 研究表明SNN的时间动力学特性可自然建模相位解缠的空间连续性约束。

Conclusion: 该工作开启了神经形态计算与SAR干涉测量的新研究方向，为大规模InSAR处理提供了可持续的补充方案。

Abstract: We present the first theoretical framework for applying spiking neural
networks (SNNs) to synthetic aperture radar (SAR) interferometric phase
unwrapping. Despite extensive research in both domains, our comprehensive
literature review confirms that SNNs have never been applied to phase
unwrapping, representing a significant gap in current methodologies. As Earth
observation data volumes continue to grow exponentially (with missions like
NISAR expected to generate 100PB in two years) energy-efficient processing
becomes critical for sustainable data center operations. SNNs, with their
event-driven computation model, offer potential energy savings of 30-100x
compared to conventional approaches while maintaining comparable accuracy. We
develop spike encoding schemes specifically designed for wrapped phase data,
propose SNN architectures that leverage the spatial propagation nature of phase
unwrapping, and provide theoretical analysis of computational complexity and
convergence properties. Our framework demonstrates how the temporal dynamics
inherent in SNNs can naturally model the spatial continuity constraints
fundamental to phase unwrapping. This work opens a new research direction at
the intersection of neuromorphic computing and SAR interferometry, offering a
complementary approach to existing algorithms that could enable more
sustainable large-scale InSAR processing.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [164] [Agile Management for Machine Learning: A Systematic Mapping Study](https://arxiv.org/abs/2506.20759)
*Lucas Romao,Hugo Villamizar,Romeu Oliveira,Silvio Alonso,Marcos Kalinowski*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: [Context] Machine learning (ML)-enabled systems are present in our society,
driving significant digital transformations. The dynamic nature of ML
development, characterized by experimental cycles and rapid changes in data,
poses challenges to traditional project management. Agile methods, with their
flexibility and incremental delivery, seem well-suited to address this
dynamism. However, it is unclear how to effectively apply these methods in the
context of ML-enabled systems, where challenges require tailored approaches.
[Goal] Our goal is to outline the state of the art in agile management for
ML-enabled systems. [Method] We conducted a systematic mapping study using a
hybrid search strategy that combines database searches with backward and
forward snowballing iterations. [Results] Our study identified 27 papers
published between 2008 and 2024. From these, we identified eight frameworks and
categorized recommendations and practices into eight key themes, such as
Iteration Flexibility, Innovative ML-specific Artifacts, and the Minimal Viable
Model. The main challenge identified across studies was accurate effort
estimation for ML-related tasks. [Conclusion] This study contributes by mapping
the state of the art and identifying open gaps in the field. While relevant
work exists, more robust empirical evaluation is still needed to validate these
contributions.

</details>


### [165] [Generating Reliable Adverse event Profiles for Health through Automated Integrated Data (GRAPH-AID): A Semi-Automated Ontology Building Approach](https://arxiv.org/abs/2506.20851)
*Srikar Reddy Gadusu,Larry Callahan,Samir Lababidi,Arunasri Nishtala,Sophia Healey,Hande McGinty*

Key words: ontology generation, Neo4j, OWL, Python, rdflib, FAERS, drug safety

TL;DR: 提出了一种利用Python和rdflib库的用户友好方法，用于从Neo4j数据库生成OWL本体，并通过FDA FAERS数据库案例展示其可行性，以支持药物安全监测。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 随着数据和知识的快速膨胀，需要一种更易用的方法来将Neo4j数据库与OWL本体无缝集成，以简化知识图谱的创建过程。

Method: 使用Python及其rdflib库开发脚本，自动从Neo4j数据库中生成OWL本体的类和公理，以FDA FAERS数据库为例实现这一方法。

Result: 开发了一种无需理解描述逻辑（DL）语法的用户友好方法，能够自动生成OWL本体，支持药物安全监测和公共健康决策。

Conclusion: 该方法为快速增长的不良药物事件数据集的 ontology 生成提供了实用解决方案，简化了集成过程并提高了可访问性。

Abstract: As data and knowledge expand rapidly, adopting systematic methodologies for
ontology generation has become crucial. With the daily increases in data
volumes and frequent content changes, the demand for databases to store and
retrieve information for the creation of knowledge graphs has become
increasingly urgent. The previously established Knowledge Acquisition and
Representation Methodology (KNARM) outlines a systematic approach to address
these challenges and create knowledge graphs. However, following this
methodology highlights the existing challenge of seamlessly integrating Neo4j
databases with the Web Ontology Language (OWL). Previous attempts to integrate
data from Neo4j into an ontology have been discussed, but these approaches
often require an understanding of description logics (DL) syntax, which may not
be familiar to many users. Thus, a more accessible method is necessary to
bridge this gap. This paper presents a user-friendly approach that utilizes
Python and its rdflib library to support ontology development. We showcase our
novel approach through a Neo4j database we created by integrating data from the
Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS)
database. Using this dataset, we developed a Python script that automatically
generates the required classes and their axioms, facilitating a smoother
integration process. This approach offers a practical solution to the
challenges of ontology generation in the context of rapidly growing adverse
drug event datasets, supporting improved drug safety monitoring and public
health decision-making.

</details>


### [166] [Engineering RAG Systems for Real-World Applications: Design, Development, and Evaluation](https://arxiv.org/abs/2506.20869)
*Md Toufique Hasan,Muhammad Waseem,Kai-Kristian Kemell,Ayman Asad Khan,Mika Saari,Pekka Abrahamsson*

Key words: RAG系统, 大型语言模型, 实证研究, 用户体验, 多领域应用

TL;DR: 本文介绍了五个基于真实场景的RAG应用，并总结了12个关键经验教训。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 解决RAG系统在真实世界应用中的实证研究和用户体验不足的问题。

Method: 开发五个领域的RAG应用，结合多语言OCR、语义检索和领域适配LLM，通过100名参与者的网络评估。

Result: 用户反馈显示出系统在易用性、相关性等方面的表现，并总结了影响可靠性和可用性的挑战。

Conclusion: 本文强调了RAG系统的技术、操作和伦理挑战，为未来开发提供了实用指导。

Abstract: Retrieval-Augmented Generation (RAG) systems are emerging as a key approach
for grounding Large Language Models (LLMs) in external knowledge, addressing
limitations in factual accuracy and contextual relevance. However, there is a
lack of empirical studies that report on the development of RAG-based
implementations grounded in real-world use cases, evaluated through general
user involvement, and accompanied by systematic documentation of lessons
learned. This paper presents five domain-specific RAG applications developed
for real-world scenarios across governance, cybersecurity, agriculture,
industrial research, and medical diagnostics. Each system incorporates
multilingual OCR, semantic retrieval via vector embeddings, and domain-adapted
LLMs, deployed through local servers or cloud APIs to meet distinct user needs.
A web-based evaluation involving a total of 100 participants assessed the
systems across six dimensions: (i) Ease of Use, (ii) Relevance, (iii)
Transparency, (iv) Responsiveness, (v) Accuracy, and (vi) Likelihood of
Recommendation. Based on user feedback and our development experience, we
documented twelve key lessons learned, highlighting technical, operational, and
ethical challenges affecting the reliability and usability of RAG systems in
practice.

</details>


### [167] [Complex Model Transformations by Reinforcement Learning with Uncertain Human Guidance](https://arxiv.org/abs/2506.20883)
*Kyanna Dagenais,Istvan David*

Key words: 强化学习, 模型转换, 人类指导, 复杂问题, 人在循环

TL;DR: 该论文提出了一种利用强化学习（RL）开发复杂模型转换（MT）序列的方法，并通过人类指导提升RL性能。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 复杂模型转换（如模型同步、自动化修复）手动开发困难且易错，RL能通过探索找到最优MT序列，但在复杂问题中性能不足。结合人类指导可以显著提升RL效果。

Method: 提出一个框架，将用户定义的MT映射到RL原语，并作为RL程序执行以寻找最优MT序列，同时整合不确定的人类建议。

Result: 评估表明，即使人类建议不确定，也能显著提升RL性能，并更高效地开发复杂MT序列。

Conclusion: 该方法通过权衡人类建议的确定性和及时性，推动了RL驱动的“人在循环”工程方法。

Abstract: Model-driven engineering problems often require complex model transformations
(MTs), i.e., MTs that are chained in extensive sequences. Pertinent examples of
such problems include model synchronization, automated model repair, and design
space exploration. Manually developing complex MTs is an error-prone and often
infeasible process. Reinforcement learning (RL) is an apt way to alleviate
these issues. In RL, an autonomous agent explores the state space through trial
and error to identify beneficial sequences of actions, such as MTs. However, RL
methods exhibit performance issues in complex problems. In these situations,
human guidance can be of high utility. In this paper, we present an approach
and technical framework for developing complex MT sequences through RL, guided
by potentially uncertain human advice. Our framework allows user-defined MTs to
be mapped onto RL primitives, and executes them as RL programs to find optimal
MT sequences. Our evaluation shows that human guidance, even if uncertain,
substantially improves RL performance, and results in more efficient
development of complex MTs. Through a trade-off between the certainty and
timeliness of human advice, our method takes a step towards RL-driven
human-in-the-loop engineering methods.

</details>


### [168] [How Good Are Synthetic Requirements ? Evaluating LLM-Generated Datasets for AI4RE](https://arxiv.org/abs/2506.21138)
*Abdelkarim El-Hajjami,Camille Salinesi*

Key words: AI4RE, 合成数据, 需求工程, 数据质量, 提示优化

TL;DR: Synthline v1提出了一种改进的产品线方法，用于生成高质量合成需求数据，通过优化提示策略和后处理技术提升任务性能。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 公开标注的需求数据集稀缺是AI4RE发展的主要障碍，需要系统方法来优化合成数据质量。

Method: 采用多样本提示、PACE自动提示优化和后生成筛选技术，评估对四项分类任务的影响。

Result: 多样本提示显著提升数据效用和多样性；PACE对功能分类效果显著但其他任务表现不一；合成数据在特定任务中优于人工数据。

Conclusion: 合成需求数据能有效缓解数据集稀缺问题，部分任务表现优于人工数据。

Abstract: The shortage of publicly available, labeled requirements datasets remains a
major barrier to advancing Artificial Intelligence for Requirements Engineering
(AI4RE). While Large Language Models offer promising capabilities for synthetic
data generation, systematic approaches to control and optimize the quality of
generated requirements remain underexplored. This paper presents Synthline v1,
an enhanced Product Line approach for generating synthetic requirements data
that extends our earlier v0 version with advanced generation strategies and
curation techniques. We investigate four research questions assessing how
prompting strategies, automated prompt optimization, and post-generation
curation affect data quality across four classification tasks: defect
detection, functional vs. non-functional, quality vs. non-quality, and security
vs. non-security. Our evaluation shows that multi-sample prompting
significantly boosts both utility and diversity over single-sample generation,
with F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic
Editing) for automated prompt optimization yields task-dependent results,
greatly improving functional classification (+32.5 points) but reducing
performance on others. Interestingly, similarity-based curation improves
diversity but often harms classification performance, indicating that some
redundancy may help ML models. Most importantly, our results show that
synthetic requirements can match or outperform human-authored ones for specific
tasks, with synthetic data surpassing human data for security (+7.8 points) and
defect classification (+15.4 points). These findings offer practical insights
for AI4RE and chart a viable path to mitigating dataset scarcity through
systematic synthetic generation.

</details>


### [169] [$T^3$: Multi-level Tree-based Automatic Program Repair with Large Language Models](https://arxiv.org/abs/2506.21211)
*Quanming Liu,Xupeng Bu,Zhichao Yan,Ru Li*

Key words: 自动程序修复，APR，大语言模型，LLM，思维链，CoT，树搜索

TL;DR: 这篇论文系统评估了常见CoT技术在APR任务中的表现，并提出了一种创新框架$T^3$，结合LLM的强大推理能力和树搜索，显著提升了自动程序修复的效率。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 由于自动程序修复(APR)需要复杂的逻辑和多步推理能力，目前CoT技术在该领域的应用仍不足，故研究如何结合LLM和CoT技术以提升APR的推理能力。

Method: 提出了一种名为$T^3$的创新框架，通过将LLM的推理能力与树搜索技术结合，优化修复候选方案的生成。同时，该框架还提供了样本选择和修复策略优化的指导。

Result: $T^3$框架显著提高了生成候选修复方案的精确性，并为APR任务提供了高效的自动化调试框架。

Conclusion: $T^3$框架成功整合了LLM和CoT技术的优势，为提升APR任务的效率和精度提供了一个可行的解决方案。

Abstract: Automatic Program Repair (APR) is a core technology in software development
and maintenance, with aims to enable automated defect repair with minimal human
intervention. In recent years, the substantial advancements in Large Language
Models (LLMs) and the Chain-of-Thought (CoT) techniques have significantly
enhanced the reasoning capabilities of these models. However, due to the
complex logic and multi-step reasoning ability needed, the application of CoT
techniques in the APR domain remains insufficient. This study systematically
evaluates the performance of several common CoT techniques in APR tasks and
proposes an innovative framework $T^3$, which integrates the powerful reasoning
capabilities of LLMs with tree search, effectively improving the precision of
generating candidate repair solutions. Furthermore, $T^3$ provides valuable
guidance for optimizing sample selection and repair strategies in APR tasks,
establishing a robust framework for achieving efficient automated debugging.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [170] [Quantum Reinforcement Learning Trading Agent for Sector Rotation in the Taiwan Stock Market](https://arxiv.org/abs/2506.20930)
*Chi-Sheng Chen,Xinyu Zhang,Ya-Chuan Chen*

Key words: 量子强化学习、板块轮动、PPO、量子计算、金融应用

TL;DR: 该论文提出了一种混合量子-经典强化学习框架，用于台湾股市的板块轮动投资。研究发现，量子增强模型在训练奖励上表现更优，但在实际投资指标（如累计收益和夏普比率）上表现不如经典模型，揭示了奖励设计与实际投资目标之间的不匹配问题。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 研究旨在探索量子增强模型在金融领域的应用潜力，尤其是在台湾股市的板块轮动策略中。通过比较量子与经典模型的性能，揭示当前强化学习在金融领域的核心挑战。

Method: 采用近端策略优化（PPO）作为骨干算法，结合经典架构（LSTM、Transformer）和量子增强模型（QNN、QRWKV、QASA）构建策略和价值网络，并通过自动化特征工程管道提取金融指标。

Result: 量子增强模型在训练奖励上优于经典模型，但在实际投资指标（如累计收益和夏普比率）中表现不佳，存在奖励设计与实际目标的偏差问题。

Conclusion: 当前奖励设计可能导致模型过度拟合短期波动而非优化风险调整收益。未来改进方向包括奖励重塑、模型正则化和基于验证的早停策略。

Abstract: We propose a hybrid quantum-classical reinforcement learning framework for
sector rotation in the Taiwan stock market. Our system employs Proximal Policy
Optimization (PPO) as the backbone algorithm and integrates both classical
architectures (LSTM, Transformer) and quantum-enhanced models (QNN, QRWKV,
QASA) as policy and value networks. An automated feature engineering pipeline
extracts financial indicators from capital share data to ensure consistent
model input across all configurations. Empirical backtesting reveals a key
finding: although quantum-enhanced models consistently achieve higher training
rewards, they underperform classical models in real-world investment metrics
such as cumulative return and Sharpe ratio. This discrepancy highlights a core
challenge in applying reinforcement learning to financial domains -- namely,
the mismatch between proxy reward signals and true investment objectives. Our
analysis suggests that current reward designs may incentivize overfitting to
short-term volatility rather than optimizing risk-adjusted returns. This issue
is compounded by the inherent expressiveness and optimization instability of
quantum circuits under Noisy Intermediate-Scale Quantum (NISQ) constraints. We
discuss the implications of this reward-performance gap and propose directions
for future improvement, including reward shaping, model regularization, and
validation-based early stopping. Our work offers a reproducible benchmark and
critical insights into the practical challenges of deploying quantum
reinforcement learning in real-world finance.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [171] [Leveraging Vision-Language Models to Select Trustworthy Super-Resolution Samples Generated by Diffusion Models](https://arxiv.org/abs/2506.20832)
*Cansu Korkmaz,Ahmet Murat Tekalp,Zafer Dogan*

Key words: 超分辨率, 信任度评分, 视觉语言模型, 扩散模型

TL;DR: 本文提出了一种基于视觉语言模型（VLM）的自动化框架，用于从扩散模型生成的高分辨率图像中选择最可信的样本，并通过新颖的信任度评分（TWS）量化其可靠性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决超分辨率（SR）问题中单一样本选择的不确定性问题，特别是在信息关键应用中避免模糊和伪影。

Method: 利用BLIP-2、GPT-4o等VLM进行结构化查询评估语义正确性、视觉质量和伪影存在情况，并通过TWS（结合CLIP嵌入、SSIM和小波分解）量化可靠性。

Result: TWS与人类偏好强相关，VLM指导的选择能持续获得高TWS值，且优于传统指标如PSNR和LPIPS。

Conclusion: 通过语义对齐和量化评分，本文为生成式SR的可信度设定了新标准。

Abstract: Super-resolution (SR) is an ill-posed inverse problem with many feasible
solutions consistent with a given low-resolution image. On one hand, regressive
SR models aim to balance fidelity and perceptual quality to yield a single
solution, but this trade-off often introduces artifacts that create ambiguity
in information-critical applications such as recognizing digits or letters. On
the other hand, diffusion models generate a diverse set of SR images, but
selecting the most trustworthy solution from this set remains a challenge. This
paper introduces a robust, automated framework for identifying the most
trustworthy SR sample from a diffusion-generated set by leveraging the semantic
reasoning capabilities of vision-language models (VLMs). Specifically, VLMs
such as BLIP-2, GPT-4o, and their variants are prompted with structured queries
to assess semantic correctness, visual quality, and artifact presence. The
top-ranked SR candidates are then ensembled to yield a single trustworthy
output in a cost-effective manner. To rigorously assess the validity of
VLM-selected samples, we propose a novel Trustworthiness Score (TWS) a hybrid
metric that quantifies SR reliability based on three complementary components:
semantic similarity via CLIP embeddings, structural integrity using SSIM on
edge maps, and artifact sensitivity through multi-level wavelet decomposition.
We empirically show that TWS correlates strongly with human preference in both
ambiguous and natural images, and that VLM-guided selections consistently yield
high TWS values. Compared to conventional metrics like PSNR, LPIPS, which fail
to reflect information fidelity, our approach offers a principled, scalable,
and generalizable solution for navigating the uncertainty of the diffusion SR
space. By aligning outputs with human expectations and semantic correctness,
this work sets a new benchmark for trustworthiness in generative SR.

</details>


### [172] [FixCLR: Negative-Class Contrastive Learning for Semi-Supervised Domain Generalization](https://arxiv.org/abs/2506.20841)
*Ha Min Son,Shahbaz Rezaei,Xin Liu*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Semi-supervised domain generalization (SSDG) aims to solve the problem of
generalizing to out-of-distribution data when only a few labels are available.
Due to label scarcity, applying domain generalization methods often
underperform. Consequently, existing SSDG methods combine semi-supervised
learning methods with various regularization terms. However, these methods do
not explicitly regularize to learn domains invariant representations across all
domains, which is a key goal for domain generalization. To address this, we
introduce FixCLR. Inspired by success in self-supervised learning, we change
two crucial components to adapt contrastive learning for explicit domain
invariance regularization: utilization of class information from pseudo-labels
and using only a repelling term. FixCLR can also be added on top of most
existing SSDG and semi-supervised methods for complementary performance
improvements. Our research includes extensive experiments that have not been
previously explored in SSDG studies. These experiments include benchmarking
different improvements to semi-supervised methods, evaluating the performance
of pretrained versus non-pretrained models, and testing on datasets with many
domains. Overall, FixCLR proves to be an effective SSDG method, especially when
combined with other semi-supervised methods.

</details>


### [173] [THIRDEYE: Cue-Aware Monocular Depth Estimation via Brain-Inspired Multi-Stage Fusion](https://arxiv.org/abs/2506.20877)
*Calin Teodor Ioan*

Key words: 单目深度估计、视觉线索、皮质层次结构、预训练网络、记忆模块

TL;DR: ThirdEye通过专门的预训练网络显式提供单目视觉线索，结合皮质层次结构和记忆模块加权融合，实现高分辨率深度估计，无需大量微调。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统单目深度估计方法忽视了人类视觉系统依赖的显式视觉线索，如遮挡边界、阴影和透视。ThirdEye旨在通过预训练网络显式提供这些线索，提升模型性能。

Method: ThirdEye使用专门预训练的冻结网络提供视觉线索，通过三阶段皮质层次结构（V1->V2->V3）和记忆模块加权融合线索，最终通过自适应分箱变换器生成高分辨率视差图。

Result: ThirdEye通过冻结的线索专家网络继承大量外部监督，仅需少量微调，实验细节和定量结果将在后续版本中公布。

Conclusion: ThirdEye通过显式视觉线索和皮质层次结构设计，显著提升了单目深度估计的性能和效率。

Abstract: Monocular depth estimation methods traditionally train deep models to infer
depth directly from RGB pixels. This implicit learning often overlooks explicit
monocular cues that the human visual system relies on, such as occlusion
boundaries, shading, and perspective. Rather than expecting a network to
discover these cues unaided, we present ThirdEye, a cue-aware pipeline that
deliberately supplies each cue through specialised, pre-trained, and frozen
networks. These cues are fused in a three-stage cortical hierarchy (V1->V2->V3)
equipped with a key-value working-memory module that weights them by
reliability. An adaptive-bins transformer head then produces a high-resolution
disparity map. Because the cue experts are frozen, ThirdEye inherits large
amounts of external supervision while requiring only modest fine-tuning. This
extended version provides additional architectural detail, neuroscientific
motivation, and an expanded experimental protocol; quantitative results will
appear in a future revision.

</details>


### [174] [HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context](https://arxiv.org/abs/2506.21277)
*Qize Yang,Shimin Yao,Weixuan Chen,Shenghao Fu,Detao Bai,Jiaxing Zhao,Boyuan Sun,Bowen Yin,Xihan Wei,Jingren Zhou*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: With the rapid evolution of multimodal large language models, the capacity to
deeply understand and interpret human intentions has emerged as a critical
capability, which demands detailed and thoughtful reasoning. In recent studies,
Reinforcement Learning (RL) has demonstrated potential in enhancing the
reasoning capabilities of Large Language Models (LLMs). Nonetheless, the
challenges associated with adapting RL to multimodal data and formats remain
largely unaddressed. In this paper, we identify two issues in existing
multimodal reasoning models: insufficient global context understanding and
shortcut problems. Insufficient context understanding can happen when a model
misinterprets multimodal context, resulting in incorrect answers. The shortcut
problem occurs when the model overlooks crucial clues in multimodal inputs,
directly addressing the query without considering the multimodal information.
To tackle these issues, we emphasize the necessity for the model to reason with
a clear understanding of the global context within multimodal inputs. This
global context understanding can effectively prevent the model from overlooking
key multimodal cues and ensure a thorough reasoning process. To ensure the
accurate interpretation of multimodal context information, we implement a
context reward judged by a large language model, alongside format and accuracy
rewards. Additionally, to improve complex reasoning capability, we employ the
LLM to assess the logical reward, determining whether the reasoning process
successfully integrates multimodal information with logical methods. We also
introduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating
models in understanding complex human intentions and emotions. Our proposed
method demonstrates advanced performance across multiple omni-modal benchmarks
compared to other open-source omni-modal models.

</details>


### [175] [OmniEval: A Benchmark for Evaluating Omni-modal Models with Visual, Auditory, and Textual Inputs](https://arxiv.org/abs/2506.20960)
*Yiman Zhang,Ziheng Luo,Qiangyu Yan,Wei He,Borui Jiang,Xinghao Chen,Kai Han*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we introduce OmniEval, a benchmark for evaluating
omni-modality models like MiniCPM-O 2.6, which encompasses visual, auditory,
and textual inputs. Compared with existing benchmarks, our OmniEval has several
distinctive features: (i) Full-modal collaboration: We design evaluation tasks
that highlight the strong coupling between audio and video, requiring models to
effectively leverage the collaborative perception of all modalities; (ii)
Diversity of videos: OmniEval includes 810 audio-visual synchronized videos,
285 Chinese videos and 525 English videos; (iii) Diversity and granularity of
tasks: OmniEval contains 2617 question-answer pairs, comprising 1412 open-ended
questions and 1205 multiple-choice questions. These questions are divided into
3 major task types and 12 sub-task types to achieve comprehensive evaluation.
Among them, we introduce a more granular video localization task named
Grounding. Then we conduct experiments on OmniEval with several omni-modality
models. We hope that our OmniEval can provide a platform for evaluating the
ability to construct and understand coherence from the context of all
modalities. Codes and data could be found at https://omnieval.github.io/.

</details>


### [176] [Evidence-based diagnostic reasoning with multi-agent copilot for human pathology](https://arxiv.org/abs/2506.20964)
*Chengkuan Chen,Luca L. Weishaupt,Drew F. K. Williamson,Richard J. Chen,Tong Ding,Bowen Chen,Anurag Vaidya,Long Phi Le,Guillaume Jaume,Ming Y. Lu,Faisal Mahmood*

Key words: 计算病理学,多模态大语言模型,病理学诊断,全切片图像,AI推理

TL;DR: PathChat+是一种专为病理学设计的MLLM，通过整合多模态数据和自主推理能力，显著提升了病理学诊断的准确性和效率，并支持多图像理解和视觉报告生成。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统病理学AI模型缺乏文本支持和多图像理解能力，PathChat+旨在弥补这些不足，提供更全面的病理诊断工具。

Method: 通过训练100多万病理学指令样本和550万问答对，开发了PathChat+，并结合SlideSeek系统实现多图像自主分析和推理。

Result: PathChat+在多项病理学基准测试中表现优于现有模型，并在DDxBench上达到高准确率。

Conclusion: PathChat+和SlideSeek系统为病理学提供了更强大的AI支持，推动数字化病理学的发展。

Abstract: Pathology is experiencing rapid digital transformation driven by whole-slide
imaging and artificial intelligence (AI). While deep learning-based
computational pathology has achieved notable success, traditional models
primarily focus on image analysis without integrating natural language
instruction or rich, text-based context. Current multimodal large language
models (MLLMs) in computational pathology face limitations, including
insufficient training data, inadequate support and evaluation for multi-image
understanding, and a lack of autonomous, diagnostic reasoning capabilities. To
address these limitations, we introduce PathChat+, a new MLLM specifically
designed for human pathology, trained on over 1 million diverse,
pathology-specific instruction samples and nearly 5.5 million question answer
turns. Extensive evaluations across diverse pathology benchmarks demonstrated
that PathChat+ substantially outperforms the prior PathChat copilot, as well as
both state-of-the-art (SOTA) general-purpose and other pathology-specific
models. Furthermore, we present SlideSeek, a reasoning-enabled multi-agent AI
system leveraging PathChat+ to autonomously evaluate gigapixel whole-slide
images (WSIs) through iterative, hierarchical diagnostic reasoning, reaching
high accuracy on DDxBench, a challenging open-ended differential diagnosis
benchmark, while also capable of generating visually grounded,
humanly-interpretable summary reports.

</details>


### [177] [Logios : An open source Greek Polytonic Optical Character Recognition system](https://arxiv.org/abs/2506.21474)
*Perifanos Konstantinos,Goutsos Dionisis*

Key words: OCR, 希腊多调文本, 卷积层, 循环层, 开源

TL;DR: 本文介绍了一种专为希腊多调文本设计的OCR系统，结合卷积和循环层提高识别准确性和效率。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决传统OCR方法在希腊多调脚本识别中的局限性。

Method: 结合卷积层进行特征提取，循环层进行序列学习。

Result: 显著提高了希腊多调文本识别的准确性和效率。

Conclusion: 该系统已开源并供学术使用，为希腊多调文本的数字化提供了高效解决方案。

Abstract: In this paper, we present an Optical Character Recognition (OCR) system
specifically designed for the accurate recognition and digitization of Greek
polytonic texts. By leveraging the combined strengths of convolutional layers
for feature extraction and recurrent layers for sequence learning, our system
addresses the unique challenges posed by Greek polytonic scripts. This approach
aims to overcome the limitations of traditional OCR methods, offering
significant improvements in accuracy and efficiency. We release the underlying
model as an open-source library and make our OCR platform available for
academic use.

</details>


### [178] [DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing](https://arxiv.org/abs/2506.20967)
*Lingling Cai,Kang Zhao,Hangjie Yuan,Xiang Wang,Yingya Zhang,Kejie Huang*

Key words: 视频编辑、视频扩散变换器、零样本、高效计算

TL;DR: 论文提出了DFVEdit，一种高效零样本视频编辑方法，针对视频扩散变换器（Video DiTs），通过流变换直接在潜在空间操作，避免了注意力修改和微调的高计算成本。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有视频编辑方法应用于Video DiTs时计算开销大，需优化以降低资源消耗。

Method: 提出DFVEdit方法，基于连续流视角统一编辑与采样，引入条件增量流向量（CDFV），结合隐含交叉注意力（ICA）和嵌入强化（ER）提升编辑质量。

Result: DFVEdit在Video DiTs中实现了至少20倍推理加速和85%内存减少，且在结构保真度、时空一致性和编辑质量上达到最优性能。

Conclusion: DFVEdit为Video DiTs提供了一种高效且高质量的编辑解决方案。

Abstract: The advent of Video Diffusion Transformers (Video DiTs) marks a milestone in
video generation. However, directly applying existing video editing methods to
Video DiTs often incurs substantial computational overhead, due to
resource-intensive attention modification or finetuning. To alleviate this
problem, we present DFVEdit, an efficient zero-shot video editing method
tailored for Video DiTs. DFVEdit eliminates the need for both attention
modification and fine-tuning by directly operating on clean latents via flow
transformation. To be more specific, we observe that editing and sampling can
be unified under the continuous flow perspective. Building upon this
foundation, we propose the Conditional Delta Flow Vector (CDFV) -- a
theoretically unbiased estimation of DFV -- and integrate Implicit Cross
Attention (ICA) guidance as well as Embedding Reinforcement (ER) to further
enhance editing quality. DFVEdit excels in practical efficiency, offering at
least 20x inference speed-up and 85\% memory reduction on Video DiTs compared
to attention-engineering-based editing methods. Extensive quantitative and
qualitative experiments demonstrate that DFVEdit can be seamlessly applied to
popular Video DiTs (e.g., CogVideoX and Wan2.1), attaining state-of-the-art
performance on structural fidelity, spatial-temporal consistency, and editing
quality.

</details>


### [179] [From Cradle to Cane: A Two-Pass Framework for High-Fidelity Lifespan Face Aging](https://arxiv.org/abs/2506.20977)
*Tao Liu,Dafeng Zhang,Gengchen Li,Shizhuo Liu,Yongqi Song,Senmao Li,Shiqi Yang,Boqian Li,Kai Wang,Yaxing Wang*

Key words: 人脸老化, 扩散模型, 身份一致性, Age-ID权衡, Cradle2Cane

TL;DR: 提出了一种基于扩散模型的两阶段人脸老化框架Cradle2Cane，通过自适应噪声注入和身份感知嵌入，解决了老化准确性和身份一致性之间的平衡问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 以往的人脸老化方法在老化准确性和身份一致性之间难以平衡，导致结果不理想。本文旨在解决这一Age-ID权衡问题。

Method: 采用两阶段扩散模型框架：第一阶段通过自适应噪声注入（AdaNI）实现老化准确性；第二阶段通过身份感知嵌入（IDEmb）增强身份一致性。

Result: 在CelebA-HQ测试集上，通过Face++和Qwen-VL评估，Cradle2Cane在老化准确性和身份一致性上优于现有方法。

Conclusion: Cradle2Cane成功解决了Age-ID权衡问题，为实际应用提供了更优的老化效果。

Abstract: Face aging has become a crucial task in computer vision, with applications
ranging from entertainment to healthcare. However, existing methods struggle
with achieving a realistic and seamless transformation across the entire
lifespan, especially when handling large age gaps or extreme head poses. The
core challenge lies in balancing age accuracy and identity preservation--what
we refer to as the Age-ID trade-off. Most prior methods either prioritize age
transformation at the expense of identity consistency or vice versa. In this
work, we address this issue by proposing a two-pass face aging framework, named
Cradle2Cane, based on few-step text-to-image (T2I) diffusion models. The first
pass focuses on solving age accuracy by introducing an adaptive noise injection
(AdaNI) mechanism. This mechanism is guided by including prompt descriptions of
age and gender for the given person as the textual condition. Also, by
adjusting the noise level, we can control the strength of aging while allowing
more flexibility in transforming the face. However, identity preservation is
weakly ensured here to facilitate stronger age transformations. In the second
pass, we enhance identity preservation while maintaining age-specific features
by conditioning the model on two identity-aware embeddings (IDEmb): SVR-ArcFace
and Rotate-CLIP. This pass allows for denoising the transformed image from the
first pass, ensuring stronger identity preservation without compromising the
aging accuracy. Both passes are jointly trained in an end-to-end way. Extensive
experiments on the CelebA-HQ test dataset, evaluated through Face++ and Qwen-VL
protocols, show that our Cradle2Cane outperforms existing face aging methods in
age accuracy and identity consistency.

</details>


### [180] [HalluSegBench: Counterfactual Visual Reasoning for Segmentation Hallucination Evaluation](https://arxiv.org/abs/2506.21546)
*Xinzhuo Li,Adheesh Juvekar,Xingyou Liu,Muntasir Wahed,Kiet A. Nguyen,Ismini Lourentzou*

Key words: 视觉语言分割, 幻觉评估, 反事实推理, HalluSegBench

TL;DR: 论文摘要介绍了HalluSegBench，首个通过反事实视觉推理评估视觉接地中幻觉的基准测试，包含1340个反事实实例对和新的量化指标，揭示了视觉驱动幻觉比标签驱动更普遍。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有分割幻觉评估协议主要关注标签或文本幻觉，缺乏对视觉上下文的操控，无法诊断关键失败。

Method: 引入HalluSegBench，包含1340个反事实实例对和新的量化指标，评估幻觉敏感性。

Result: 实验显示视觉驱动幻觉比标签驱动更普遍，模型常持续错误分割。

Conclusion: 反事实推理是诊断接地保真度的必要工具。

Abstract: Recent progress in vision-language segmentation has significantly advanced
grounded visual understanding. However, these models often exhibit
hallucinations by producing segmentation masks for objects not grounded in the
image content or by incorrectly labeling irrelevant regions. Existing
evaluation protocols for segmentation hallucination primarily focus on label or
textual hallucinations without manipulating the visual context, limiting their
capacity to diagnose critical failures. In response, we introduce
HalluSegBench, the first benchmark specifically designed to evaluate
hallucinations in visual grounding through the lens of counterfactual visual
reasoning. Our benchmark consists of a novel dataset of 1340 counterfactual
instance pairs spanning 281 unique object classes, and a set of newly
introduced metrics that quantify hallucination sensitivity under visually
coherent scene edits. Experiments on HalluSegBench with state-of-the-art
vision-language segmentation models reveal that vision-driven hallucinations
are significantly more prevalent than label-driven ones, with models often
persisting in false segmentation, highlighting the need for counterfactual
reasoning to diagnose grounding fidelity.

</details>


### [181] [Segment Anything in Pathology Images with Natural Language](https://arxiv.org/abs/2506.20988)
*Zhixuan Chen,Junlin Hou,Liqi Lin,Yihui Wang,Yequan Bie,Xi Wang,Yanning Zhou,Ronald Cheong Kin Chan,Hao Chen*

Key words: 病理图像分割、文本提示、可解释AI、精准肿瘤学、PathSeg

TL;DR: 论文提出PathSegmentor，首个基于文本提示的病理图像分割基础模型，并引入大规模数据集PathSeg，显著提升分割精度和适用性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决病理图像分割中标注数据有限和类别定义受限的挑战，推动精准肿瘤学中可解释AI的发展。

Method: 提出PathSegmentor模型，支持自然语言提示分割，无需空间输入；构建PathSeg数据集，包含275k样本。

Result: 模型在总体Dice分数上分别超过现有空间和文本提示模型0.145和0.429，且在外部数据集上表现鲁棒。

Conclusion: PathSegmentor提高了诊断模型的可解释性，为临床决策提供支持，推动了病理分割技术的发展。

Abstract: Pathology image segmentation is crucial in computational pathology for
analyzing histological features relevant to cancer diagnosis and prognosis.
However, current methods face major challenges in clinical applications due to
limited annotated data and restricted category definitions. To address these
limitations, we propose PathSegmentor, the first text-prompted segmentation
foundation model designed specifically for pathology images. We also introduce
PathSeg , the largest and most comprehensive dataset for pathology
segmentation, built from 17 public sources and containing 275k image-mask-label
triples across 160 diverse categories. With PathSegmentor, users can perform
semantic segmentation using natural language prompts, eliminating the need for
laborious spatial inputs such as points or boxes. Extensive experiments
demonstrate that PathSegmentor outperforms specialized models with higher
accuracy and broader applicability, while maintaining a compact architecture.
It significantly surpasses existing spatial- and text-prompted models by 0.145
and 0.429 in overall Dice scores, respectively, showing strong robustness in
segmenting complex structures and generalizing to external datasets. Moreover,
PathSegmentor's outputs enhance the interpretability of diagnostic models
through feature importance estimation and imaging biomarker discovery, offering
pathologists evidence-based support for clinical decision-making. This work
advances the development of explainable AI in precision oncology.

</details>


### [182] [Multimodal Prompt Alignment for Facial Expression Recognition](https://arxiv.org/abs/2506.21017)
*Fuyan Ma,Yiran He,Bin Sun,Shutao Li*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Prompt learning has been widely adopted to efficiently adapt vision-language
models (VLMs) like CLIP for various downstream tasks. Despite their success,
current VLM-based facial expression recognition (FER) methods struggle to
capture fine-grained textual-visual relationships, which are essential for
distinguishing subtle differences between facial expressions. To address this
challenge, we propose a multimodal prompt alignment framework for FER, called
MPA-FER, that provides fine-grained semantic guidance to the learning process
of prompted visual features, resulting in more precise and interpretable
representations. Specifically, we introduce a multi-granularity hard prompt
generation strategy that utilizes a large language model (LLM) like ChatGPT to
generate detailed descriptions for each facial expression. The LLM-based
external knowledge is injected into the soft prompts by minimizing the feature
discrepancy between the soft prompts and the hard prompts. To preserve the
generalization abilities of the pretrained CLIP model, our approach
incorporates prototype-guided visual feature alignment, ensuring that the
prompted visual features from the frozen image encoder align closely with
class-specific prototypes. Additionally, we propose a cross-modal global-local
alignment module that focuses on expression-relevant facial features, further
improving the alignment between textual and visual features. Extensive
experiments demonstrate our framework outperforms state-of-the-art methods on
three FER benchmark datasets, while retaining the benefits of the pretrained
model and minimizing computational costs.

</details>


### [183] [Improving Diffusion-Based Image Editing Faithfulness via Guidance and Scheduling](https://arxiv.org/abs/2506.21045)
*Hansam Cho,Seoung Bum Kim*

Key words: 图像编辑, 扩散模型, 忠实性, 可编辑性, FGS

TL;DR: 提出了一种名为FGS的方法，通过引入忠实性指导和调度策略，解决图像编辑中可编辑性与忠实性之间的矛盾，实验证明其能够在不明显降低可编辑性的情况下提高忠实性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 图像编辑中的忠实性与可编辑性之间存在权衡关系，当前方法难以在两者之间取得最优平衡，因此需要一种能够有效提升忠实性同时保持可编辑性的解决方案。

Method: 提出了FGS（Faithfulness Guidance and Scheduling）方法，包含忠实性指导和调度策略，旨在增强输入图像信息的保留能力，并解决可编辑性与忠实性之间的不对齐问题。

Result: 实验结果显示，FGS在保持可编辑性的同时显著提升了忠实性，且与多种编辑方法兼容，适用于多样化的编辑任务。

Conclusion: FGS通过忠实性指导和调度策略有效缓解了编辑过程中的忠实性与可编辑性矛盾，为高质量图像编辑提供了新思路。

Abstract: Text-guided diffusion models have become essential for high-quality image
synthesis, enabling dynamic image editing. In image editing, two crucial
aspects are editability, which determines the extent of modification, and
faithfulness, which reflects how well unaltered elements are preserved.
However, achieving optimal results is challenging because of the inherent
trade-off between editability and faithfulness. To address this, we propose
Faithfulness Guidance and Scheduling (FGS), which enhances faithfulness with
minimal impact on editability. FGS incorporates faithfulness guidance to
strengthen the preservation of input image information and introduces a
scheduling strategy to resolve misalignment between editability and
faithfulness. Experimental results demonstrate that FGS achieves superior
faithfulness while maintaining editability. Moreover, its compatibility with
various editing methods enables precise, high-quality image edits across
diverse tasks.

</details>


### [184] [EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for Efficient Egocentric Perception](https://arxiv.org/abs/2506.21080)
*Sanjoy Chowdhury,Subrata Biswas,Sayan Nag,Tushar Nagarajan,Calvin Murdock,Ishwarya Ananthabhotla,Yijun Qian,Vamsi Krishna Ithapu,Dinesh Manocha,Ruohan Gao*

Key words: EgoAdapt, 跨模态蒸馏, 策略学习, 自我中心任务, 高效推理

TL;DR: EgoAdapt框架通过跨模态蒸馏和策略学习，显著提升了多感官自我中心任务的效率，降低了计算成本。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决现代感知模型在资源受限环境中的高计算成本问题。

Method: 提出EgoAdapt框架，结合跨模态蒸馏和任务特定策略学习。

Result: 在三个数据集上，计算量减少89.09%，参数减少82.02%，能耗降低9.6倍，性能仍优于或持平当前最优模型。

Conclusion: EgoAdapt在保持高性能的同时显著提升了效率，适用于多种自我中心任务。

Abstract: Modern perception models, particularly those designed for multisensory
egocentric tasks, have achieved remarkable performance but often come with
substantial computational costs. These high demands pose challenges for
real-world deployment, especially in resource-constrained environments. In this
paper, we introduce EgoAdapt, a framework that adaptively performs cross-modal
distillation and policy learning to enable efficient inference across different
egocentric perception tasks, including egocentric action recognition, active
speaker localization, and behavior anticipation. Our proposed policy module is
adaptable to task-specific action spaces, making it broadly applicable.
Experimental results on three challenging egocentric datasets EPIC-Kitchens,
EasyCom, and Aria Everyday Activities demonstrate that our method significantly
enhances efficiency, reducing GMACs by up to 89.09%, parameters up to 82.02%,
and energy up to 9.6x, while still on-par and in many cases outperforming, the
performance of corresponding state-of-the-art models.

</details>


### [185] [IPFormer-VideoLLM: Enhancing Multi-modal Video Understanding for Multi-shot Scenes](https://arxiv.org/abs/2506.21116)
*Yujia Liang,Jile Jiao,Zhicheng Wang,Xuetao Feng,Zixuan Ye,Yuan Wang,Hao Lu*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Video Large Language Models (VideoLLMs) have demonstrated remarkable
understanding capabilities, but are found struggling to tackle multi-shot
scenarios,e.g., video clips with varying camera angles or scene changes. This
challenge can render failures such as instance identity forgetting and key
frame negligence. In this work, we first attribute the challenge to the lack of
multi-shot annotations among existing datasets and therefore we introduce a new
dataset termed MultiClip-Bench, featuring dense descriptions and
instruction-based question-answering pairs tailored for multi-shot scenarios.
We empirically find that the training set significantly boosts the multi-shot
performance, while the testing benchmark provides a reliable measure of the
model capability in multi-shot scenarios. By further analyzing and discovering
that current models only encode instance features in a discrete or lossy
manner, at the risk of missing identity information, we then contribute a new
model IPFormer-VideoLLM. Its key idea is the injection of instance-level
features as instance prompts through an efficient attention-based connector.
This allows for the aggregation of instance-specific information across scenes.
Experiments demonstrate that our proposed dataset and model not only enhance
the multi-scene video understanding significantly, but also offer distinct
advantages across various video benchmarks.

</details>


### [186] [Robust Deep Learning for Myocardial Scar Segmentation in Cardiac MRI with Noisy Labels](https://arxiv.org/abs/2506.21151)
*Aida Moafi,Danial Moafi,Evgeny M. Mirkes,Gerry P. McCann,Abbas S. Alatrany,Jayanth R. Arnold,Mostafa Mehdipour Ghazi*

Key words: 心肌疤痕分割、深度学习、心脏MRI、Kullback-Leibler损失、数据增强

TL;DR: 提出了一种基于深度学习的自动化心肌疤痕分割方法，通过微调先进模型解决标签噪声、数据异质性和类别不平衡问题，并在急性与慢性病例中验证了其优越性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 心肌疤痕的精确分割对临床评估和治疗计划至关重要，但现有方法面临标签噪声和数据异质性等挑战。

Method: 通过Kullback-Leibler损失函数和大量数据增强技术微调先进模型，解决标签噪声、数据异质性和类别不平衡问题。

Result: 方法在急性和慢性病例中表现出色，优于nnU-Net等先进模型，并在分布外测试数据中展示强泛化能力。

Conclusion: 该方法为心肌疤痕的自动化量化提供了可靠基础，支持深度学习在心脏影像中的广泛应用。

Abstract: The accurate segmentation of myocardial scars from cardiac MRI is essential
for clinical assessment and treatment planning. In this study, we propose a
robust deep-learning pipeline for fully automated myocardial scar detection and
segmentation by fine-tuning state-of-the-art models. The method explicitly
addresses challenges of label noise from semi-automatic annotations, data
heterogeneity, and class imbalance through the use of Kullback-Leibler loss and
extensive data augmentation. We evaluate the model's performance on both acute
and chronic cases and demonstrate its ability to produce accurate and smooth
segmentations despite noisy labels. In particular, our approach outperforms
state-of-the-art models like nnU-Net and shows strong generalizability in an
out-of-distribution test set, highlighting its robustness across various
imaging conditions and clinical tasks. These results establish a reliable
foundation for automated myocardial scar quantification and support the broader
clinical adoption of deep learning in cardiac imaging.

</details>


### [187] [Transferring disentangled representations: bridging the gap between synthetic and real images](https://arxiv.org/abs/2409.18017)
*Jacopo Dapueto,Nicoletta Noceti,Francesca Odone*

Key words: 解耦表示学习,合成数据,干预式度量,迁移学习

TL;DR: 研究探讨了如何利用合成数据学习可泛化到真实数据的解耦表示，分析了微调效果及其保留特性，并提出新的度量方法。结果显示解耦表示的部分特性可以成功迁移。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 研究旨在解决真实图像中因生成因素相关、分辨率限制和缺乏真实标签导致解耦表示学习效果不佳的问题。

Method: 利用合成数据学习解耦表示，探讨微调效果及其特性保留，并提出新的干预式度量方法。

Result: 研究表明，解耦表示的部分特性可以成功从合成数据迁移到真实数据。

Conclusion: 解耦表示学习在合成数据与真实数据间的迁移是可行且有效的。

Abstract: Developing meaningful and efficient representations that separate the
fundamental structure of the data generation mechanism is crucial in
representation learning. However, Disentangled Representation Learning has not
fully shown its potential on real images, because of correlated generative
factors, their resolution and limited access to ground truth labels.
Specifically on the latter, we investigate the possibility of leveraging
synthetic data to learn general-purpose disentangled representations applicable
to real data, discussing the effect of fine-tuning and what properties of
disentanglement are preserved after the transfer. We provide an extensive
empirical study to address these issues. In addition, we propose a new
interpretable intervention-based metric, to measure the quality of factors
encoding in the representation. Our results indicate that some level of
disentanglement, transferring a representation from synthetic to real data, is
possible and effective.

</details>


### [188] [Task-Aware KV Compression For Cost-Effective Long Video Understanding](https://arxiv.org/abs/2506.21184)
*Minghao Qin,Yan Shu,Peitian Zhang,Kun Lun,Huaying Yuan,Juenjie Zhou,Shitao Xiao,Bo Zhao,Zheng Liu*

Key words: 长视频理解、多模态大语言模型、KV压缩、计算效率、选择性加载

TL;DR: Video-X^2L通过双层KV压缩和选择性KV重加载技术，灵活保留长视频理解任务中的关键信息，显著提升计算效率并减少信息损失。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 长视频理解（LVU）对现有多模态大语言模型（MLLMs）提出了巨大挑战，主要由于高昂的计算成本。现有KV压缩方法在高压缩比下信息损失严重。

Method: Video-X^2L采用双层KV压缩（生成低压缩和高压缩KV）和选择性KV重加载（解码阶段动态选择KV），无需额外训练且兼容现有MLLMs。

Result: 在多个LVU基准测试（如VideoMME、MLVU等）中，Video-X^2L显著优于现有KV压缩方法，同时大幅节省计算成本。

Conclusion: Video-X^2L是一种简单高效的解决方案，能够在不增加训练负担的情况下，显著提升长视频理解的性能和效率。

Abstract: Long-video understanding (LVU) remains a severe challenge for existing
multimodal large language models (MLLMs), primarily due to the prohibitive
computational cost. Recent approaches have explored KV compression to mitigate
this issue, but they often suffer from significant information loss at high
compression ratios. In this paper, we introduce Video-X^2L, which flexibly
preserves critical video information for each LVU task. Video-X^2L involves two
key operations. The first one is called bi-level KV compression. During the
MLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:
low-compression KVs (L-KVs) to capture fine-grained video details and
high-compression KVs (H-KVs) to offer compact video representations. The second
one is called selective KV re-loading. During the MLLM's decoding stage,
Video-X^2L selectively re-loads L-KVs for the most critical video chunks while
using H-KVs for other less important ones. This allows the MLLM to fully
utilize task-specific information while maintaining the overall compactness.
Video-X^2L is simple yet effective: it is free from additional training and
directly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L
with a variety of popular LVU benchmarks, including VideoMME, MLVU,
LongVideoBench, and VNBench. Our experiment result shows that Video-X^2L
outperforms existing KV-compression methods by a huge advantage while
substantially saving the computation cost.

</details>


### [189] [BitMark for Infinity: Watermarking Bitwise Autoregressive Image Generative Models](https://arxiv.org/abs/2506.21209)
*Louis Kerner,Michel Meintz,Bihe Zhao,Franziska Boenisch,Adam Dziedzic*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: State-of-the-art text-to-image models like Infinity generate photorealistic
images at an unprecedented speed. These models operate in a bitwise
autoregressive manner over a discrete set of tokens that is practically
infinite in size. However, their impressive generative power comes with a
growing risk: as their outputs increasingly populate the Internet, they are
likely to be scraped and reused as training data-potentially by the very same
models. This phenomenon has been shown to lead to model collapse, where
repeated training on generated content, especially from the models' own
previous versions, causes a gradual degradation in performance. A promising
mitigation strategy is watermarking, which embeds human-imperceptible yet
detectable signals into generated images-enabling the identification of
generated content. In this work, we introduce BitMark, a robust bitwise
watermarking framework for Infinity. Our method embeds a watermark directly at
the bit level of the token stream across multiple scales (also referred to as
resolutions) during Infinity's image generation process. Our bitwise watermark
subtly influences the bits to preserve visual fidelity and generation speed
while remaining robust against a spectrum of removal techniques. Furthermore,
it exhibits high radioactivity, i.e., when watermarked generated images are
used to train another image generative model, this second model's outputs will
also carry the watermark. The radioactive traces remain detectable even when
only fine-tuning diffusion or image autoregressive models on images watermarked
with our BitMark. Overall, our approach provides a principled step toward
preventing model collapse in image generative models by enabling reliable
detection of generated outputs.

</details>


### [190] [Holistic Surgical Phase Recognition with Hierarchical Input Dependent State Space Models](https://arxiv.org/abs/2506.21330)
*Haoyang Wu,Tsun-Hsuan Wang,Mathias Lechner,Ramin Hasani,Jennifer A. Eckhoff,Paul Pak,Ozanan R. Meireles,Guy Rosman,Yutong Ban,Daniela Rus*

Key words: 机器人辅助手术、状态空间模型、视频分析、监督策略

TL;DR: 本文提出了一种新颖的分层输入相关状态空间模型，用于分析机器人辅助手术中的长时间视频，解决了传统Transformer模型因二次注意力机制难以高效处理长视频的问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 机器人辅助手术的视频分析因时长较长，现有方法难以高效处理，尤其是Transformer模型的二次注意力机制限制了其对长视频的处理能力。

Method: 通过结合局部聚合和全局关系状态空间模型模块，以及一种混合离散-连续监督策略，实现了对全长度视频的高效分析。

Result: 实验表明，该方法在多个数据集（Cholec80、MICCAI2016和Heichole）上显著优于现有方法，性能提升幅度较大。

Conclusion: 提出的状态空间模型方法在机器人辅助手术视频分析中表现出色，能够同时捕捉局部和全局动态信息。

Abstract: Surgical workflow analysis is essential in robot-assisted surgeries, yet the
long duration of such procedures poses significant challenges for comprehensive
video analysis. Recent approaches have predominantly relied on transformer
models; however, their quadratic attention mechanism restricts efficient
processing of lengthy surgical videos. In this paper, we propose a novel
hierarchical input-dependent state space model that leverages the linear
scaling property of state space models to enable decision making on full-length
videos while capturing both local and global dynamics. Our framework
incorporates a temporally consistent visual feature extractor, which appends a
state space model head to a visual feature extractor to propagate temporal
information. The proposed model consists of two key modules: a
local-aggregation state space model block that effectively captures intricate
local dynamics, and a global-relation state space model block that models
temporal dependencies across the entire video. The model is trained using a
hybrid discrete-continuous supervision strategy, where both signals of discrete
phase labels and continuous phase progresses are propagated through the
network. Experiments have shown that our method outperforms the current
state-of-the-art methods by a large margin (+2.8% on Cholec80, +4.3% on
MICCAI2016, and +12.9% on Heichole datasets). Code will be publicly available
after paper acceptance.

</details>


### [191] [CA-I2P: Channel-Adaptive Registration Network with Global Optimal Selection](https://arxiv.org/abs/2506.21364)
*Zhixin Cheng,Jiacheng Deng,Xinjun Li,Xiaotian Yin,Bohao Liao,Baoqun Yin,Wenfei Yang,Tianzhu Zhang*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Detection-free methods typically follow a coarse-to-fine pipeline, extracting
image and point cloud features for patch-level matching and refining dense
pixel-to-point correspondences. However, differences in feature channel
attention between images and point clouds may lead to degraded matching
results, ultimately impairing registration accuracy. Furthermore, similar
structures in the scene could lead to redundant correspondences in cross-modal
matching. To address these issues, we propose Channel Adaptive Adjustment
Module (CAA) and Global Optimal Selection Module (GOS). CAA enhances
intra-modal features and suppresses cross-modal sensitivity, while GOS replaces
local selection with global optimization. Experiments on RGB-D Scenes V2 and
7-Scenes demonstrate the superiority of our method, achieving state-of-the-art
performance in image-to-point cloud registration.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [192] [Evaluating PDE discovery methods for multiscale modeling of biological signals](https://arxiv.org/abs/2506.20694)
*Andréa Ducos,Audrey Denizot,Thomas Guyet,Hugues Berry*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: q-bio.QM

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Biological systems are non-linear, include unobserved variables and the
physical principles that govern their dynamics are partly unknown. This makes
the characterization of their behavior very challenging. Notably, their
activity occurs on multiple interdependent spatial and temporal scales that
require linking mechanisms across scales. To address the challenge of bridging
gaps between scales, we leverage partial differential equations (PDE)
discovery. PDE discovery suggests meso-scale dynamics characteristics from
micro-scale data. In this article, we present our framework combining
particle-based simulations and PDE discovery and conduct preliminary
experiments to assess equation discovery in controlled settings. We evaluate
five state-of-the-art PDE discovery methods on particle-based simulations of
calcium diffusion in astrocytes. The performances of the methods are evaluated
on both the form of the discovered equation and the forecasted temporal
variations of calcium concentration. Our results show that several methods
accurately recover the diffusion term, highlighting the potential of PDE
discovery for capturing macroscopic dynamics in biological systems from
microscopic data.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [193] [IMC-PINN-FE: A Physics-Informed Neural Network for Patient-Specific Left Ventricular Finite Element Modeling with Image Motion Consistency and Biomechanical Parameter Estimation](https://arxiv.org/abs/2506.20696)
*Siyu Mu,Wei Xuan Chan,Choon Hwai Yap*

Key words: 心脏生物力学, 物理信息神经网络, 有限元建模, 图像一致性, 个性化建模

TL;DR: IMC-PINN-FE是一种结合物理信息神经网络和有限元建模的新方法，用于快速、个性化且图像一致的心脏生物力学建模。

<details>
  <summary>Details</summary>

Main category: physics.med-ph

Motivation: 传统有限元方法计算成本高且难以重现心脏运动，需一种高效精确的替代方案。

Method: 提出IMC-PINN-FE框架，结合图像运动一致性和有限元方法，利用MRI或超声心动图数据估计心肌刚度和主动张力。

Result: 计算速度提升75倍，运动匹配更精确（Dice从0.849提高到0.927），同时保持真实的压力-容积行为。

Conclusion: IMC-PINN-FE为心脏生物力学建模提供了一种快速、个性化且稳健的方法。

Abstract: Elucidating the biomechanical behavior of the myocardium is crucial for
understanding cardiac physiology, but cannot be directly inferred from clinical
imaging and typically requires finite element (FE) simulations. However,
conventional FE methods are computationally expensive and often fail to
reproduce observed cardiac motions. We propose IMC-PINN-FE, a physics-informed
neural network (PINN) framework that integrates imaged motion consistency (IMC)
with FE modeling for patient-specific left ventricular (LV) biomechanics.
Cardiac motion is first estimated from MRI or echocardiography using either a
pre-trained attention-based network or an unsupervised cyclic-regularized
network, followed by extraction of motion modes. IMC-PINN-FE then rapidly
estimates myocardial stiffness and active tension by fitting clinical pressure
measurements, accelerating computation from hours to seconds compared to
traditional inverse FE. Based on these parameters, it performs FE modeling
across the cardiac cycle at 75x speedup. Through motion constraints, it matches
imaged displacements more accurately, improving average Dice from 0.849 to
0.927, while preserving realistic pressure-volume behavior. IMC-PINN-FE
advances previous PINN-FE models by introducing back-computation of material
properties and better motion fidelity. Using motion from a single subject to
reconstruct shape modes also avoids the need for large datasets and improves
patient specificity. IMC-PINN-FE offers a robust and efficient approach for
rapid, personalized, and image-consistent cardiac biomechanical modeling.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [194] [Efficacy of Temporal Fusion Transformers for Runoff Simulation](https://arxiv.org/abs/2506.20831)
*Sinan Rasiya Koya,Tirthankar Roy*

Key words: 降雨径流建模, Temporal Fusion Transformers, LSTM, 水文过程线, 可解释AI

TL;DR: 论文对比了Temporal Fusion Transformers (TFTs)和LSTM在降雨径流建模中的表现，显示TFT略优于LSTM，尤其在模拟水文过程线的中段和峰值时表现更佳，并展示了TFT在处理长序列和大型流域时的优势。

<details>
  <summary>Details</summary>

Main category: physics.geo-ph

Motivation: 探讨TFT和LSTM在降雨径流建模中的性能差异，并为水文建模提供更优的模型选择。

Method: 在531个美国CAMELS流域和五个不同国家的Caravan数据集上训练并比较了十次随机初始化的TFT和LSTM模型，评估了其性能、变异性及数据差异。

Result: TFT在模拟水文过程线的中段和峰值时表现优于LSTM，且能更好地处理长序列和大型流域，同时TFT具有可解释性，能识别关键变量。但两种模型在Caravan数据集上性能显著下降。

Conclusion: TFT在水文建模中具有潜力，尤其是在处理复杂流域时，但数据质量对模型性能有重要影响。

Abstract: Combining attention with recurrence has shown to be valuable in sequence
modeling, including hydrological predictions. Here, we explore the strength of
Temporal Fusion Transformers (TFTs) over Long Short-Term Memory (LSTM) networks
in rainfall-runoff modeling. We train ten randomly initialized models, TFT and
LSTM, for 531 CAMELS catchments in the US. We repeat the experiment with five
subsets of the Caravan dataset, each representing catchments in the US,
Australia, Brazil, Great Britain, and Chile. Then, the performance of the
models, their variability regarding the catchment attributes, and the
difference according to the datasets are assessed. Our findings show that TFT
slightly outperforms LSTM, especially in simulating the midsection and peak of
hydrographs. Furthermore, we show the ability of TFT to handle longer sequences
and why it can be a better candidate for higher or larger catchments. Being an
explainable AI technique, TFT identifies the key dynamic and static variables,
providing valuable scientific insights. However, both TFT and LSTM exhibit a
considerable drop in performance with the Caravan dataset, indicating possible
data quality issues. Overall, the study highlights the potential of TFT in
improving hydrological modeling and understanding.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [195] [CovDocker: Benchmarking Covalent Drug Design with Tasks, Datasets, and Solutions](https://arxiv.org/abs/2506.21085)
*Yangzhe Peng,Kaiyuan Gao,Liang He,Yuheng Cong,Haiguang Liu,Kun He,Lijun Wu*

Key words: 共价对接、药物设计、深度学习、基准、共价键

TL;DR: 论文提出了一种用于共价对接的综合基准CovDocker，解决了现有方法难以处理共价键形成及其结构变化的局限性，并通过分解任务和优化模型展示了其有效性。

<details>
  <summary>Details</summary>

Main category: q-bio.BM

Motivation: 共价相互作用因其强且持久的结合特性在药物设计中至关重要，但现有对接方法和深度学习模型很少考虑共价键的形成，因此需要开发新的基准。

Method: 将共价对接分解为三个任务：反应位点预测、共价反应预测和共价对接，并采用Uni-Mol和Chemformer等先进模型建立基线性能。

Result: 基准验证了其在准确预测相互作用位点和模拟共价结合中分子转化方面的有效性。

Conclusion: CovDocker为共价药物设计研究提供了严谨的框架，展现了数据驱动方法在加速选择性共价抑制剂发现中的潜力。

Abstract: Molecular docking plays a crucial role in predicting the binding mode of
ligands to target proteins, and covalent interactions, which involve the
formation of a covalent bond between the ligand and the target, are
particularly valuable due to their strong, enduring binding nature. However,
most existing docking methods and deep learning approaches hardly account for
the formation of covalent bonds and the associated structural changes. To
address this gap, we introduce a comprehensive benchmark for covalent docking,
CovDocker, which is designed to better capture the complexities of covalent
binding. We decompose the covalent docking process into three main tasks:
reactive location prediction, covalent reaction prediction, and covalent
docking. By adapting state-of-the-art models, such as Uni-Mol and Chemformer,
we establish baseline performances and demonstrate the effectiveness of the
benchmark in accurately predicting interaction sites and modeling the molecular
transformations involved in covalent binding. These results confirm the role of
the benchmark as a rigorous framework for advancing research in covalent drug
design. It underscores the potential of data-driven approaches to accelerate
the discovery of selective covalent inhibitors and addresses critical
challenges in therapeutic development.

</details>


### [196] [MegaFold: System-Level Optimizations for Accelerating Protein Structure Prediction Models](https://arxiv.org/abs/2506.20686)
*Hoa La,Ahan Gupta,Alex Morehead,Jianlin Cheng,Minjia Zhang*

Key words: 蛋白质结构预测, AlphaFold3, MegaFold, 加速训练, 内存优化

TL;DR: MegaFold是一个跨平台系统，旨在加速AlphaFold3（AF3）训练，通过优化数据管道、内存效率高的EvoAttention和操作符融合，显著降低了内存使用和训练时间。

<details>
  <summary>Details</summary>

Main category: q-bio.BM

Motivation: 虽然AlphaFold3在蛋白质结构预测上取得了突破，但其高计算和内存需求限制了训练的扩展性。MegaFold旨在解决这些问题。

Method: MegaFold采用了提前缓存、Triton内核优化的EvoAttention和深度融合等技术，以优化GPU利用率和内存使用。

Result: 在NVIDIA H200和AMD MI250 GPU上，MegaFold减少了峰值内存使用（1.23×）并提升了训练速度（1.73×和1.62×），同时支持更长序列训练。

Conclusion: MegaFold显著提升了现代蛋白质折叠模型的扩展性，为高效训练提供了技术支持。

Abstract: Protein structure prediction models such as AlphaFold3 (AF3) push the
frontier of biomolecular modeling by incorporating science-informed
architectural changes to the transformer architecture. However, these advances
come at a steep system cost, introducing: compute- and memory-intensive
operators, 2D attention mechanisms, and retrieval-augmented data pipelines,
which collectively hinder the scalability of AF3 training. In this work, we
present MegaFold, a cross-platform system to accelerate AF3 training. MegaFold
tackles key bottlenecks through ahead-of-time caching to eliminate GPU idle
time from the retrieval-augmented data pipeline, Triton-based kernels for
memory-efficient EvoAttention on heterogeneous devices, and deep fusion for
common and critical small operators in AF3. Evaluation on both NVIDIA H200 and
AMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by
up to 1.23$\times$ and improves per-iteration training time by up-to
1.73$\times$ and 1.62$\times$ respectively. More importantly, MegaFold enables
training on 1.35$\times$ longer sequence lengths compared to PyTorch baselines
without running out-of-memory, significantly improving the scalability of
modern protein folding models. We open source our code at
https://github.com/Supercomputing-System-AI-Lab/MegaFold/.

</details>
