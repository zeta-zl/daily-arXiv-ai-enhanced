<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 47]
- [cs.LG](#cs.LG) [Total: 83]
- [cs.AI](#cs.AI) [Total: 36]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [eess.SY](#eess.SY) [Total: 2]
- [math.OC](#math.OC) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.AR](#cs.AR) [Total: 3]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [stat.ML](#stat.ML) [Total: 8]
- [cs.MA](#cs.MA) [Total: 1]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.CY](#cs.CY) [Total: 5]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.CV](#cs.CV) [Total: 25]
- [cs.MM](#cs.MM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.GT](#cs.GT) [Total: 3]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.CR](#cs.CR) [Total: 7]
- [cs.HC](#cs.HC) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions](https://arxiv.org/abs/2507.10577)
*Logé Cécile,Ghori Rehan*

Key words: 虚假信息, YouTube, AI, 事实核查, 评论互动

TL;DR: 该论文提出了一种AI驱动的系统，通过事实核查和评论互动来对抗YouTube上的虚假信息。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 数字世界中的虚假信息快速传播，对社交媒体平台构成威胁，需要有效的解决方案。

Method: 系统包含两个代理：Truth Sleuth（利用RAG方法核查视频声称）和Trend Bender（生成评论以引导讨论）。

Result: 实验证明系统在事实核查和用户互动方面高效，可能改善在线信息环境。

Conclusion: AI驱动的干预措施在对抗虚假信息和促进知情讨论方面具有潜力。

Abstract: Misinformation poses a significant threat in today's digital world, often
spreading rapidly through platforms like YouTube. This paper introduces a novel
approach to combating misinformation by developing an AI-powered system that
not only fact-checks claims made in YouTube videos but also actively engages
users in the comment section and challenge misleading narratives. Our system
comprises two main agents: Truth Sleuth and Trend Bender.
  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented
Generation (RAG) approach - drawing on sources like Wikipedia, Google Search,
Google FactCheck - to accurately assess their veracity and generates a nuanced
and comprehensive report. Through rigorous prompt engineering, Trend Bender
leverages this report along with a curated corpus of relevant articles to
generate insightful and persuasive comments designed to stimulate a productive
debate. With a carefully set up self-evaluation loop, this agent is able to
iteratively improve its style and refine its output.
  We demonstrate the system's capabilities through experiments on established
benchmark datasets and a real-world deployment on YouTube, showcasing its
potential to engage users and potentially influence perspectives. Our findings
highlight the high accuracy of our fact-checking agent, and confirm the
potential of AI-driven interventions in combating misinformation and fostering
a more informed online space.

</details>


### [2] [An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation](https://arxiv.org/abs/2507.10580)
*Vimaleswar A,Prabhu Nandan Sahu,Nilesh Kumar Sahu,Haroon R Lone*

Key words: 

TL;DR: EmoSApp 是一款基于智能手机的离线对话应用，旨在提供心理健康支持，利用量化的大型语言模型在本地设备上运行，解决了用户可访问性、网络连接和数据隐私问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 数字平台在心理健康支持中的应用面临用户可访问性、网络连接和数据隐私等挑战，亟需一种离线的智能手机解决方案。

Method: 开发了 EmoSApp，使用经过量化和微调的 LLaMA-3.2-1B-Instruct 模型，并部署于智能手机本地，无需网络连接。

Result: 通过定性和定量评估，EmoSApp 能够提供连贯、同理的对话，并为用户心理健康问题提供相关建议，同时在低资源环境中表现出色。

Conclusion: EmoSApp 为便携、安全和定制化的 AI 心理健康解决方案提供了模板，推动了未来创新。

Abstract: Mental health plays a crucial role in the overall well-being of an
individual. In recent years, digital platforms have been increasingly used to
expand mental health and emotional support. However, there are persistent
challenges related to limited user accessibility, internet connectivity, and
data privacy, which highlight the need for an offline, smartphone-based
solution. To address these challenges, we propose EmoSApp (Emotional Support
App): an entirely offline, smartphone-based conversational app designed for
mental health and emotional support. The system leverages Large Language Models
(LLMs), specifically fine-tuned, quantized and deployed using Torchtune and
Executorch for resource-constrained devices, allowing all inferences to occur
on the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned
the LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of
14,582 mental-health QA pairs, along with the multi-turn conversational data.
  Through qualitative human evaluation with the student population, we
demonstrate that EmoSApp has the ability to respond coherently, empathetically,
maintain interactive dialogue, and provide relevant suggestions to user's
mental health problems. Additionally, quantitative evaluations on nine standard
commonsense and reasoning benchmarks demonstrate the efficacy of our
fine-tuned, quantized model in low-resource settings. By prioritizing on-device
deployment and specialized domain adaptation, EmoSApp serves as a blueprint for
future innovations in portable, secure, and highly tailored AI-driven mental
health solutions.

</details>


### [3] [Transforming Sensitive Documents into Quantitative Data: An AI-Based Preprocessing Toolchain for Structured and Privacy-Conscious Analysis](https://arxiv.org/abs/2507.10582)
*Anders Ledberg,Anna Thalén*

Key words: 非结构化文本, 隐私保护, 大语言模型, 嵌入分析, 内容摘要

TL;DR: 该论文介绍了一种模块化工具链，用于处理非结构化文本数据，通过开源模型实现标准化、匿名化和嵌入分析，解决了隐私和语言多样性问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 法律、医疗和行政领域的大量非结构化文本因隐私和语言多样性问题难以被大规模分析，缺乏有效工具。

Method: 使用开源大语言模型（LLM）进行文本标准化、摘要、翻译和匿名化，结合命名实体识别和规则方法确保隐私。

Result: 在瑞典法院判决数据集上验证，工具链能有效匿名化并保留语义，支持半自动化内容分析。

Conclusion: 该工具链为敏感文本的大规模研究提供了新可能，打破隐私和异构性限制。

Abstract: Unstructured text from legal, medical, and administrative sources offers a
rich but underutilized resource for research in public health and the social
sciences. However, large-scale analysis is hampered by two key challenges: the
presence of sensitive, personally identifiable information, and significant
heterogeneity in structure and language. We present a modular toolchain that
prepares such text data for embedding-based analysis, relying entirely on
open-weight models that run on local hardware, requiring only a
workstation-level GPU and supporting privacy-sensitive research.
  The toolchain employs large language model (LLM) prompting to standardize,
summarize, and, when needed, translate texts to English for greater
comparability. Anonymization is achieved via LLM-based redaction, supplemented
with named entity recognition and rule-based methods to minimize the risk of
disclosure. We demonstrate the toolchain on a corpus of 10,842 Swedish court
decisions under the Care of Abusers Act (LVM), comprising over 56,000 pages.
Each document is processed into an anonymized, standardized summary and
transformed into a document-level embedding. Validation, including manual
review, automated scanning, and predictive evaluation shows the toolchain
effectively removes identifying information while retaining semantic content.
As an illustrative application, we train a predictive model using embedding
vectors derived from a small set of manually labeled summaries, demonstrating
the toolchain's capacity for semi-automated content analysis at scale.
  By enabling structured, privacy-conscious analysis of sensitive documents,
our toolchain opens new possibilities for large-scale research in domains where
textual data was previously inaccessible due to privacy and heterogeneity
constraints.

</details>


### [4] [A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations](https://arxiv.org/abs/2507.10585)
*Isar Nejadgholi,Mona Omidyeganeh,Marc-Antoine Drouin,Jonathan Boisvert*

Key words: AI治理, 可解释AI, 自然语言解释, 透明度, 分类法

TL;DR: 该论文提出了一个更新的XAI分类法，针对自然语言解释（NLEs），帮助研究者、审计者和政策制定者更好地设计和评估透明AI系统。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着大型语言模型的兴起，自然语言解释（NLEs）成为描述模型行为的关键工具，需要系统化的框架来支持其治理和透明度。

Method: 基于可解释AI（XAI）文献，提出一个针对NLEs的更新分类法，涵盖上下文、生成与呈现、评估三个维度。

Result: 分类法为研究人员、审计者和政策制定者提供了系统化的工具，以设计和优化NLEs。

Conclusion: 该分类法有助于促进透明AI系统的开发和治理。

Abstract: Effective AI governance requires structured approaches for stakeholders to
access and verify AI system behavior. With the rise of large language models,
Natural Language Explanations (NLEs) are now key to articulating model
behavior, which necessitates a focused examination of their characteristics and
governance implications. We draw on Explainable AI (XAI) literature to create
an updated XAI taxonomy, adapted to prompt-based NLEs, across three dimensions:
(1) Context, including task, data, audience, and goals; (2) Generation and
Presentation, covering generation methods, inputs, interactivity, outputs, and
forms; and (3) Evaluation, focusing on content, presentation, and user-centered
properties, as well as the setting of the evaluation. This taxonomy provides a
framework for researchers, auditors, and policymakers to characterize, design,
and enhance NLEs for transparent AI systems.

</details>


### [5] [AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters](https://arxiv.org/abs/2507.10586)
*Kaushik Dwivedi,Padmanabh Patanjali Mishra*

Key words: 大型语言模型,幻觉问题,检索增强生成,LoRA适配器,KL正则化

TL;DR: AutoRAG-LoRA是一个模块化框架，通过轻量级LoRA适配器和KL正则化训练，减少大型语言模型中的幻觉问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型在自然语言任务中表现流畅，但存在幻觉问题，影响实际部署中的信任。

Method: 结合自动提示重写、混合检索和低秩适配器调优，并通过检测模块和反馈循环校正事实。

Result: AutoRAG-LoRA显著减少事实漂移，同时保持模型的效率和模块化。

Conclusion: 该框架有效解决了幻觉问题，提升了模型的实用性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable fluency across a
range of natural language tasks, yet remain vulnerable to hallucinations -
factual inaccuracies that undermine trust in real world deployment. We present
AutoRAG-LoRA, a modular framework for Retrieval-Augmented Generation (RAG) that
tackles hallucination in large language models through lightweight LoRA-based
adapters and KL-regularized training. Our pipeline integrates automated prompt
rewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in
retrieved evidence. A hallucination detection module, using both
classifier-based and self-evaluation techniques, assigns confidence scores to
generated outputs, triggering an optional feedback correction loop. This loop
enforces factual alignment via contrastive KL loss and adapter fine tuning. We
demonstrate that AutoRAG-LoRA significantly reduces the factual drift while
preserving the efficiency and modularity of the model.

</details>


### [6] [Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing](https://arxiv.org/abs/2507.10587)
*Dennis Ulmer,Alexandra Lorson,Ivan Titov,Christian Hardmeier*

Key words: 语言模型,不确定性表达,拟人化,用户信任,自然语言处理

TL;DR: 论文探讨了如何通过语言模型更自然地表达不确定性，以增强用户信任，并提出了拟人化不确定性的研究方向。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 由于语言模型输出常过于自信，缺乏准确性反馈，影响了用户信任，因此需要研究如何通过语言表达模型的不确定性。

Method: 论文综述了人类不确定性表达的研究，分析了现有数据偏差，并提出了拟人化不确定性的概念。

Result: 研究发现当前研究忽视了人类不确定性表达的复杂性，提出了拟人化表达未来的研究路径。

Conclusion: 拟人化不确定性是提升语言模型用户信任的关键，需要进一步研究其在人机交互中的应用。

Abstract: Human users increasingly rely on natural language interactions with large
language models (LLMs) in order to receive help on a large variety of tasks and
problems. However, the trustworthiness and perceived legitimacy of LLMs is
undermined by the fact that their output is frequently stated in very confident
terms, even when its accuracy is questionable. Therefore, there is a need to
signal the confidence of the language model to a user in order to reap the
benefits of human-machine collaboration and mitigate potential harms.
Verbalized uncertainty is the expression of confidence with linguistic means,
an approach that integrates perfectly into language-based interfaces.
Nevertheless, most recent research in natural language processing (NLP)
overlooks the nuances surrounding human uncertainty communication and the data
biases that influence machine uncertainty communication. We argue for
anthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty
communication requires a degree of linguistic authenticity and personalization
to the user, which could be achieved by emulating human communication. We
present a thorough overview over the research in human uncertainty
communication, survey ongoing research, and perform additional analyses to
demonstrate so-far overlooked biases in verbalized uncertainty. We conclude by
pointing out unique factors in human-machine communication of uncertainty and
deconstruct anthropomimetic uncertainty into future research directions for
NLP.

</details>


### [7] [PLEX: Perturbation-free Local Explanations for LLM-Based Text Classification](https://arxiv.org/abs/2507.10596)
*Yogachandran Rahulamathavan,Misbah Farooq,Varuna De Silva*

Key words: PLEX, LLM, XAI, LIME, SHAP

TL;DR: PLEX是一种无需扰动的高效局部解释方法，显著降低了计算开销，同时保持与LIME和SHAP相似的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有XAI方法（如LIME和SHAP）依赖计算昂贵的扰动，影响效率，因此提出PLEX以解决这一问题。

Method: PLEX利用LLM的上下文嵌入和Siamese网络训练，无需后续扰动，直接生成解释。

Result: 在四种分类任务中，PLEX与LIME和SHAP的吻合度超过92%，且计算效率提升显著。

Conclusion: PLEX为基于LLM的文本分类提供了一种高效且可解释的解决方案。

Abstract: Large Language Models (LLMs) excel in text classification, but their
complexity hinders interpretability, making it difficult to understand the
reasoning behind their predictions. Explainable AI (XAI) methods like LIME and
SHAP offer local explanations by identifying influential words, but they rely
on computationally expensive perturbations. These methods typically generate
thousands of perturbed sentences and perform inferences on each, incurring a
substantial computational burden, especially with LLMs. To address this, we
propose \underline{P}erturbation-free \underline{L}ocal \underline{Ex}planation
(PLEX), a novel method that leverages the contextual embeddings extracted from
the LLM and a ``Siamese network" style neural network trained to align with
feature importance scores. This one-off training eliminates the need for
subsequent perturbations, enabling efficient explanations for any new sentence.
We demonstrate PLEX's effectiveness on four different classification tasks
(sentiment, fake news, fake COVID-19 news and depression), showing more than
92\% agreement with LIME and SHAP. Our evaluation using a ``stress test"
reveals that PLEX accurately identifies influential words, leading to a similar
decline in classification accuracy as observed with LIME and SHAP when these
words are removed. Notably, in some cases, PLEX demonstrates superior
performance in capturing the impact of key features. PLEX dramatically
accelerates explanation, reducing time and computational overhead by two and
four orders of magnitude, respectively. This work offers a promising solution
for explainable LLM-based text classification.

</details>


### [8] [Emergence of Hierarchical Emotion Organization in Large Language Models](https://arxiv.org/abs/2507.10599)
*Bo Zhao,Maya Okawa,Eric J. Bigelow,Rose Yu,Tomer Ullman,Ekdeep Singh Lubana,Hidenori Tanaka*

Key words: 大型语言模型,情绪识别,心理学框架,系统性偏差,模型评估

TL;DR: 研究分析了大型语言模型（LLMs）如何模拟用户情绪状态，发现模型输出的情绪具有层次结构，与人类心理学模型一致，且模型规模越大，情绪层次越复杂。同时，研究发现模型对经济弱势群体的情绪识别存在系统性偏差。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 了解LLMs如何建模用户情绪状态，以确保其伦理部署。

Method: 基于心理学框架“情绪轮”，分析模型输出的情绪概率依赖关系。

Result: LLMs自然形成与人类心理学模型一致的情绪层次结构，且大模型层次更复杂；同时发现模型对弱势群体的情绪识别存在偏差。

Conclusion: LLMs的内隐情绪推理能力与人类社会认知相似，提示未来可结合认知理论优化模型评估。

Abstract: As large language models (LLMs) increasingly power conversational agents,
understanding how they model users' emotional states is critical for ethical
deployment. Inspired by emotion wheels -- a psychological framework that argues
emotions organize hierarchically -- we analyze probabilistic dependencies
between emotional states in model outputs. We find that LLMs naturally form
hierarchical emotion trees that align with human psychological models, and
larger models develop more complex hierarchies. We also uncover systematic
biases in emotion recognition across socioeconomic personas, with compounding
misclassifications for intersectional, underrepresented groups. Human studies
reveal striking parallels, suggesting that LLMs internalize aspects of social
perception. Beyond highlighting emergent emotional reasoning in LLMs, our
results hint at the potential of using cognitively-grounded theories for
developing better model evaluations.

</details>


### [9] [Language Models for Adult Service Website Text Analysis](https://arxiv.org/abs/2507.10743)
*Nickolas Freeman,Thanh Nguyen,Gregory Bott,Jason Parton,Collin Francel*

Key words: 性贩卖,成人服务网站,文本分析,Transformer模型,语言建模

TL;DR: 该论文研究了成人服务网站（ASW）广告文本的语言建模方法，提出了一种高效的自定义Transformer模型，优于现有模型，并应用于ASW数据分析任务。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: ASW广告文本因其特殊语言特性（如滥用表情符号、语法混乱等）给文本分析带来挑战，现有模型难以有效处理；研究旨在提升ASW文本分析的准确性。

Method: 采用多种语言建模方法，包括信息检索、预训练Transformer及自定义Transformer模型，并对后者进行优化。

Result: 自定义Transformer模型在准确性、召回率、F1分数和ROC AUC上优于BERT-base、RoBERTa等模型，且在消费级硬件上高效运行。

Conclusion: 自定义模型显著提升了ASW文本分析能力，可支持下游应用如广告聚类、表情符号解析等，推动反性贩卖研究。

Abstract: Sex trafficking refers to the use of force, fraud, or coercion to compel an
individual to perform in commercial sex acts against their will. Adult service
websites (ASWs) have and continue to be linked to sex trafficking, offering a
platform for traffickers to advertise their victims. Thus, organizations
involved in the fight against sex trafficking often use ASW data when
attempting to identify potential sex trafficking victims. A critical challenge
in transforming ASW data into actionable insight is text analysis. Previous
research using ASW data has shown that ASW ad text is important for linking
ads. However, working with this text is challenging due to its extensive use of
emojis, poor grammar, and deliberate obfuscation to evade law enforcement
scrutiny. We conduct a comprehensive study of language modeling approaches for
this application area, including simple information retrieval methods,
pre-trained transformers, and custom transformer models. We demonstrate that
characteristics of ASW text data allow efficient custom transformer models to
be trained with relatively small GPU resources and used efficiently for
inference on consumer hardware. Our custom models outperform fine-tuned
variants of well-known encoder-only transformer models, including BERT-base,
RoBERTa, and ModernBERT, on accuracy, recall, F1 score, and ROC AUC. We
demonstrate the use of our best-performing custom configuration on three tasks
related to ASW data analysis: (i) decomposing the giant component in a graph
representation of ASW data, (ii) clustering ASW ad text, and (iii) using the
learned token embeddings to understand the use of emojis in the illicit context
we study. The models we develop represent a significant advancement in ASW text
analysis, which can be leveraged in a variety of downstream applications and
research.

</details>


### [10] [Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs](https://arxiv.org/abs/2507.10772)
*Michal Podstawski*

Key words: 属性图,文本嵌入,语义分析,节点分类,关系预测

TL;DR: 利用预训练文本嵌入模型增强属性图的语义分析，提升节点分类和关系预测任务的准确性与可解释性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 属性图中丰富的文本属性未被充分利用，通过预训练模型可以更好地支持语义分析。

Method: 将文本嵌入模型应用于节点和边的属性，保留图结构不变，结合语言模型嵌入。

Result: 嵌入后的图分析在节点分类和关系预测中表现出更高的准确性和可解释性。

Conclusion: 文本语义能显著提升属性图分析的性能。

Abstract: Labeled property graphs often contain rich textual attributes that can
enhance analytical tasks when properly leveraged. This work explores the use of
pretrained text embedding models to enable efficient semantic analysis in such
graphs. By embedding textual node and edge properties, we support downstream
tasks including node classification and relation prediction with improved
contextual understanding. Our approach integrates language model embeddings
into the graph pipeline without altering its structure, demonstrating that
textual semantics can significantly enhance the accuracy and interpretability
of property graph analysis.

</details>


### [11] [Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers](https://arxiv.org/abs/2507.10787)
*Yilun Zhao,Chengye Wang,Chuhan Li,Arman Cohan*

Key words: MISS-QA, 示意图, 多模态模型, 科学文献, 性能评估

TL;DR: MISS-QA是首个评估模型解析科学文献示意图能力的基准，包含1500个专家标注示例，覆盖465篇论文。测试了18种前沿多模态模型，发现与人类专家存在显著差距。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 旨在填补评估模型解析科学文献示意图能力的空白。

Method: 构建MISS-QA基准，包含专家标注的示例和问题，测试多种多模态模型。

Result: 现有模型性能显著低于人类专家，错误分析揭示了其优劣势。

Conclusion: MISS-QA为提升多模态科学文献理解提供了关键洞察。

Abstract: This paper introduces MISS-QA, the first benchmark specifically designed to
evaluate the ability of models to interpret schematic diagrams within
scientific literature. MISS-QA comprises 1,500 expert-annotated examples over
465 scientific papers. In this benchmark, models are tasked with interpreting
schematic diagrams that illustrate research overviews and answering
corresponding information-seeking questions based on the broader context of the
paper. We assess the performance of 18 frontier multimodal foundation models,
including o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant
performance gap between these models and human experts on MISS-QA. Our analysis
of model performance on unanswerable questions and our detailed error analysis
further highlight the strengths and limitations of current models, offering key
insights to enhance models in comprehending multimodal scientific literature.

</details>


### [12] [Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Posts from Parler](https://arxiv.org/abs/2507.10810)
*David M. Markowitz,Samuel Hardman Taylor*

Key words: 在线仇恨言论,社交认可,Parler,社会理论

TL;DR: 研究了社交平台Parler上仇恨言论与社交认可之间的关系，发现社交认可并未显著增加后续仇恨言论的频率或极端程度。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨社交认可如何激发在线仇恨言论，验证Walther（2024）的社会认可理论。

Method: 分析了2018-2021年间Parler上超过1.1亿条帖子，评估点赞数与后续仇恨言论的关系。

Result: 点赞数与后续仇恨言论无明显关联，社交认可与仇恨言论之间关系复杂，存在负相关。

Conclusion: 在小众社交平台上，社交认可对仇恨言论的强化机制可能与传统理论不同。

Abstract: In this paper, we explored how online hate is motivated by receiving social
approval from others. We specifically examined two central tenets of Walther's
(2024) social approval theory of online hate: (H1a) more signals of social
approval on hate messages predicts more subsequent hate messages, and (H1b) as
social approval increases, hate speech messages become more extreme. Using over
110 million posts from Parler (2018-2021), we observed that the number of
upvotes a person received on a hate speech post was unassociated with the
amount of hate speech in their next post and posts during the next week, month,
three months, and six months. Between-person effects revealed an average
negative relationship between social approval and hate speech production at the
post level, but this relationship was mixed at other time intervals. Social
approval reinforcement mechanisms of online hate may operate differently on
niche social media platforms.

</details>


### [13] [LLMs on Trial: Evaluating Judicial Fairness for Large Language Models](https://arxiv.org/abs/2507.10852)
*Yiran Hu,Zongyue Xue,Haitao Li,Siyuan Zheng,Qingjing Chen,Shaochun Wang,Xihan Zhang,Ning Zheng,Yun Liu,Qingyao Ai,Yiqun Liu,Charles L. A. Clarke,Weixing Shen*

Key words: 大型语言模型, 司法公平, 社会正义, 评价框架, 偏见

TL;DR: 该论文研究了大型语言模型（LLMs）在司法领域的公平性问题，提出了一个综合评价框架，并通过实验揭示了LLMs在司法决策中的不一致性、偏见和不平衡性问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着LLMs在高风险领域的广泛应用，其在司法决策中的公平性和社会正义影响未被充分研究。为确保LLMs的可信度，评估其司法公平性成为必要。

Method: 基于司法公平理论，构建了一个包含65个标签和161个值的评价框架，并开发了三个评价指标（不一致性、偏见和不平衡性）来评估LLMs的公平性。通过JudiFair数据集（177,100个案例）对16个LLMs进行了实验。

Result: 实验发现LLMs在司法决策中普遍存在不一致性、偏见和不平衡性，尤其在人口统计标签上表现更显著。调整温度参数可改善公平性，但模型大小、发布时间和来源国对公平性影响较小。

Conclusion: LLMs在司法应用中存在明显的公平性问题，需要进一步研究和改进。论文提供了公开工具包以支持未来研究。

Abstract: Large Language Models (LLMs) are increasingly used in high-stakes fields
where their decisions impact rights and equity. However, LLMs' judicial
fairness and implications for social justice remain underexplored. When LLMs
act as judges, the ability to fairly resolve judicial issues is a prerequisite
to ensure their trustworthiness. Based on theories of judicial fairness, we
construct a comprehensive framework to measure LLM fairness, leading to a
selection of 65 labels and 161 corresponding values. Applying this framework to
the judicial system, we compile an extensive dataset, JudiFair, comprising
177,100 unique case facts. To achieve robust statistical inference, we develop
three evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and
introduce a method to assess the overall fairness of multiple LLMs across
various labels. Through experiments with 16 LLMs, we uncover pervasive
inconsistency, bias, and imbalanced inaccuracy across models, underscoring
severe LLM judicial unfairness. Particularly, LLMs display notably more
pronounced biases on demographic labels, with slightly less bias on substance
labels compared to procedure ones. Interestingly, increased inconsistency
correlates with reduced biases, but more accurate predictions exacerbate
biases. While we find that adjusting the temperature parameter can influence
LLM fairness, model size, release date, and country of origin do not exhibit
significant effects on judicial fairness. Accordingly, we introduce a publicly
available toolkit containing all datasets and code, designed to support future
research in evaluating and improving LLM fairness.

</details>


### [14] [How Stylistic Similarity Shapes Preferences in Dialogue Dataset with User and Third Party Evaluations](https://arxiv.org/abs/2507.10918)
*Ikumi Numaya,Shoji Moriya,Shiki Sato,Reina Akama,Jun Suzuki*

Key words: 对话生成,风格相似性,用户偏好,主观评价,客观评价

TL;DR: 探讨主观与客观风格相似性对用户偏好的影响，并构建新数据集分析两者差异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究用户与系统风格相似性对用户体验的影响，区分主观与客观相似性。

Method: 构建包含用户偏好、主观风格相似性（用户自评）和客观风格相似性（第三方评估）的数据集。

Result: 发现主观风格相似性与用户偏好强相关，且与客观相似性存在显著差异。

Conclusion: 需区分主客观评价，以更好理解风格相似性与用户偏好的关系。

Abstract: Recent advancements in dialogue generation have broadened the scope of
human-bot interactions, enabling not only contextually appropriate responses
but also the analysis of human affect and sensitivity. While prior work has
suggested that stylistic similarity between user and system may enhance user
impressions, the distinction between subjective and objective similarity is
often overlooked. To investigate this issue, we introduce a novel dataset that
includes users' preferences, subjective stylistic similarity based on users'
own perceptions, and objective stylistic similarity annotated by third party
evaluators in open-domain dialogue settings. Analysis using the constructed
dataset reveals a strong positive correlation between subjective stylistic
similarity and user preference. Furthermore, our analysis suggests an important
finding: users' subjective stylistic similarity differs from third party
objective similarity. This underscores the importance of distinguishing between
subjective and objective evaluations and understanding the distinct aspects
each captures when analyzing the relationship between stylistic similarity and
user preferences. The dataset presented in this paper is available online.

</details>


### [15] [HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training](https://arxiv.org/abs/2507.10920)
*Seungho Choi*

Key words: 大型语言模型, 低资源语言, 同音异义词, HanjaBridge, 持续预训练

TL;DR: HanjaBridge通过引入汉语字符（Hanja）来解决韩语中的同音异义词问题，并结合持续预训练框架，显著提升了低资源语言（韩语）的表现，同时实现了跨语言的正面迁移。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决大型语言模型（LLM）在低资源语言（如韩语）中因同音异义词等独特语言挑战导致的性能不佳问题。

Method: 提出HanjaBridge技术，通过为同音词提供所有可能的汉语字符候选，结合持续预训练和知识蒸馏，避免灾难性遗忘。

Result: 在KoBALT基准测试中，性能相对提升21%，并观察到中韩跨语言的正面迁移。

Conclusion: HanjaBridge不仅提升了韩语理解能力，还无需推理时额外成本，具有实际应用价值。

Abstract: Large language models (LLMs) often show poor performance in low-resource
languages like Korean, partly due to unique linguistic challenges such as
homophonous Sino-Korean words that are indistinguishable in Hangul script. To
address this semantic ambiguity, we propose HanjaBridge, a novel
meaning-injection technique integrated into a continual pre-training (CPT)
framework. Instead of deterministically mapping a word to a single Hanja
(Chinese character), HanjaBridge presents the model with all possible Hanja
candidates for a given homograph, encouraging the model to learn contextual
disambiguation. This process is paired with token-level knowledge distillation
to prevent catastrophic forgetting. Experimental results show that HanjaBridge
significantly improves Korean language understanding, achieving a 21\% relative
improvement on the KoBALT benchmark. Notably, by reinforcing semantic alignment
between Korean and Chinese through shared Hanja, we observe a strong positive
cross-lingual transfer. Furthermore, these gains persist even when Hanja
augmentation is omitted at inference time, ensuring practical efficiency with
no additional run-time cost.

</details>


### [16] [Modeling Understanding of Story-Based Analogies Using Large Language Models](https://arxiv.org/abs/2507.10957)
*Kalit Inani,Keshav Kabra,Vijay Marupudi,Sashank Varma*

Key words: 大型语言模型, 类比推理, 语义表征, 人类认知, GPT-4, LLaMA3

TL;DR: 研究探讨了大型语言模型（LLMs）在类比推理任务中与人类表现的对比，重点分析了语义表征和提示解释的效果，并比较了不同模型规模和架构的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 评估LLMs在类比推理任务中是否能够达到类似人类的认知能力，尤其是在语义理解和解释方面的表现。

Method: 通过故事类比任务，使用句子嵌入评估语义相似性，并测试显式提示对LLM解释能力的影响，比较不同模型规模和架构（如GPT-4和LLaMA3）的表现。

Result: LLMs在类比任务中表现出一定能力，但在语义理解和解释方面与人类仍有差距，模型规模和架构对性能有显著影响。

Conclusion: LLMs在类比推理任务中显示出潜力，但尚未完全匹配人类推理能力，需进一步研究其局限性。

Abstract: Recent advancements in Large Language Models (LLMs) have brought them closer
to matching human cognition across a variety of tasks. How well do these models
align with human performance in detecting and mapping analogies? Prior research
has shown that LLMs can extract similarities from analogy problems but lack
robust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the
current study focused on a story-based analogical mapping task and conducted a
fine-grained evaluation of LLM reasoning abilities compared to human
performance. First, it explored the semantic representation of analogies in
LLMs, using sentence embeddings to assess whether they capture the similarity
between the source and target texts of an analogy, and the dissimilarity
between the source and distractor texts. Second, it investigated the
effectiveness of explicitly prompting LLMs to explain analogies. Throughout, we
examine whether LLMs exhibit similar performance profiles to those observed in
humans by evaluating their reasoning at the level of individual analogies, and
not just at the level of overall accuracy (as prior studies have done). Our
experiments include evaluating the impact of model size (8B vs. 70B parameters)
and performance variation across state-of-the-art model architectures such as
GPT-4 and LLaMA3. This work advances our understanding of the analogical
reasoning abilities of LLMs and their potential as models of human reasoning.

</details>


### [17] [DS@GT at eRisk 2025: From prompts to predictions, benchmarking early depression detection with conversational agent based assessments and temporal attention models](https://arxiv.org/abs/2507.10958)
*Anthony Miyaguchi,David Guecha,Yuwen Chiu,Sidharth Gaur*

Key words: eRisk 2025, 抑郁症检测, LLMs, 提示工程, BDI-II评估

TL;DR: DS@GT团队参与了eRisk 2025的两项挑战，采用提示工程策略，利用LLMs进行BDI-II评估，并分析了对话线索对预测症状的影响。最佳提交成绩为DCHR = 0.50, ADODL = 0.89, ASHR = 0.27。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究目的是通过大型语言模型(LLMs)进行对话式抑郁症检测，探索提示工程设计对模型评估的优化效果。

Method: 采用提示工程策略，利用多种LLMs进行BDI-II评估，生成结构化JSON输出，并通过交叉模型一致性和内部一致性进行评估。

Result: 最佳提交成绩在官方排行榜上排名第二，指标为DCHR = 0.50, ADODL = 0.89, ASHR = 0.27。

Conclusion: 提示工程设计能有效优化LLMs在抑郁症检测中的表现，同时分析了对话线索对预测的影响。

Abstract: This Working Note summarizes the participation of the DS@GT team in two eRisk
2025 challenges. For the Pilot Task on conversational depression detection with
large language-models (LLMs), we adopted a prompt-engineering strategy in which
diverse LLMs conducted BDI-II-based assessments and produced structured JSON
outputs. Because ground-truth labels were unavailable, we evaluated cross-model
agreement and internal consistency. Our prompt design methodology aligned model
outputs with BDI-II criteria and enabled the analysis of conversational cues
that influenced the prediction of symptoms. Our best submission, second on the
official leaderboard, achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27.

</details>


### [18] [Teach Me Sign: Stepwise Prompting LLM for Sign Language Production](https://arxiv.org/abs/2507.10972)
*Zhaoyi An,Rei Kawakami*

Key words: 大语言模型,手语生成,逐步提示,分布对齐,语法规则

TL;DR: 本文提出TEAM-Sign方法，通过微调大语言模型（LLM）将手语视为另一种自然语言，解决了手语生成的复杂性和独特性问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索大语言模型在手语生成中的应用，弥补现有研究中手语生成受限的不足。

Method: 采用逐步提示策略微调LLM，学习文本与手语的对应关系，并结合手语与口语的差异优化生成过程。

Result: 在How2Sign和Phoenix14T数据集上的实验表明，TEAM-Sign能有效利用LLM的知识和推理能力，对齐手语与口语的分布和语法规则。

Conclusion: TEAM-Sign通过LLM的微调和策略优化，显著提升了手语生成的表现。

Abstract: Large language models, with their strong reasoning ability and rich
knowledge, have brought revolution to many tasks of AI, but their impact on
sign language generation remains limited due to its complexity and unique
rules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign
language as another natural language. By fine-tuning an LLM, we enable it to
learn the correspondence between text and sign language, and facilitate
generation. Considering the differences between sign and spoken language, we
employ a stepwise prompting strategy to extract the inherent sign language
knowledge within the LLM, thereby supporting the learning and generation
process. Experimental results on How2Sign and Phoenix14T datasets demonstrate
that our approach effectively leverages both the sign language knowledge and
reasoning capabilities of LLM to align the different distribution and
grammatical rules between sign and spoken language.

</details>


### [19] [Mario at EXIST 2025: A Simple Gateway to Effective Multilingual Sexism Detection](https://arxiv.org/abs/2507.10996)
*Lin Tian,Johanne R. Trippas,Marian-Andrei Rizoiu*

Key words: 性别歧视检测, LoRA, 多语言处理, Llama 3.1, 参数高效微调

TL;DR: 本文提出了一种基于分层低秩适应（LoRA）的方法，用于检测英语和西班牙语推文中的性别歧视。通过条件适配器路由和统一的多语言训练，模型在减少训练时间和存储的同时，实现了竞争性的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决文本性别歧视检测中的多任务依赖性和语言多样性问题，同时降低计算和存储成本。

Method: 采用LoRA技术对Llama 3.1 8B模型进行分层适配，结合条件适配器路由和统一多语言训练，分别处理三个子任务。

Result: 在多语言任务中实现了1.7-2.4%的F1提升，同时减少了75%的训练时间和98%的模型存储。

Conclusion: 提出的方法通过参数高效微调和多语言训练，显著降低了计算成本，同时保持高性能。

Abstract: This paper presents our approach to EXIST 2025 Task 1, addressing text-based
sexism detection in English and Spanish tweets through hierarchical Low-Rank
Adaptation (LoRA) of Llama 3.1 8B. Our method introduces conditional adapter
routing that explicitly models label dependencies across three hierarchically
structured subtasks: binary sexism identification, source intention detection,
and multilabel sexism categorization. Unlike conventional LoRA applications
that target only attention layers, we apply adaptation to all linear
transformations, enhancing the model's capacity to capture task-specific
patterns. In contrast to complex data processing and ensemble approaches, we
show that straightforward parameter-efficient fine-tuning achieves strong
performance. We train separate LoRA adapters (rank=16, QLoRA 4-bit) for each
subtask using unified multilingual training that leverages Llama 3.1's native
bilingual capabilities. The method requires minimal preprocessing and uses
standard supervised learning. Our multilingual training strategy eliminates the
need for separate language-specific models, achieving 1.7-2.4\% F1 improvements
through cross-lingual transfer. With only 1.67\% trainable parameters compared
to full fine-tuning, our approach reduces training time by 75\% and model
storage by 98\%, while achieving competitive performance across all subtasks
(ICM-Hard: 0.6774 for binary classification, 0.4991 for intention detection,
0.6519 for multilabel categorization).

</details>


### [20] [Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification](https://arxiv.org/abs/2507.11004)
*Yejun Yoon,Jaeyoon Jung,Seunghyun Yoon,Kunwoo Park*

Key words: HerO 2, 事实验证, 文档摘要, 后训练量化, 语言模型

TL;DR: HerO 2是HUMANE团队为FEVER-25研讨会AVeriTeC共享任务开发的系统，改进了证据质量和验证预测，同时优化了计算效率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 提升事实验证系统的性能和效率，以应对现实世界中的计算限制。

Method: 通过文档摘要和答案重构改进证据质量，采用后训练量化优化验证预测，并集成更新的语言模型骨干。

Result: HerO 2在排行榜上排名第二，同时在前三名系统中运行时间最短，展示了高效率和实际应用潜力。

Conclusion: HerO 2是一个高效且性能强大的事实验证系统，适合实际应用。

Abstract: This paper presents HerO 2, Team HUMANE's system for the AVeriTeC shared task
at the FEVER-25 workshop. HerO 2 is an enhanced version of HerO, the
best-performing open-source model from the previous year's challenge. It
improves evidence quality through document summarization and answer
reformulation, optimizes veracity prediction via post-training quantization
under computational constraints, and enhances overall system performance by
integrating updated language model (LM) backbones. HerO 2 ranked second on the
leaderboard while achieving the shortest runtime among the top three systems,
demonstrating both high efficiency and strong potential for real-world fact
verification. The code is available at https://github.com/ssu-humane/HerO2.

</details>


### [21] [Journalism-Guided Agentic In-Context Learning for News Stance Detection](https://arxiv.org/abs/2507.11049)
*Dahyun Lee,Jonghyeon Choi,Jiyoung Han,Kunwoo Park*

Key words: 立场检测,韩国新闻,上下文学习,语言模型,媒体偏见

TL;DR: 该论文介绍了首个韩语新闻立场检测数据集K-News-Stance和一种基于上下文学习的框架JoA-ICL，用于检测长篇文章的立场，并展示了其在提升新闻推荐多样性和分析媒体偏见中的应用。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着在线新闻消费的增长，个性化推荐系统可能加剧信息茧房和政治极化。通过立场检测可以纳入更多元视角，减轻这些负面影响。

Method: 提出K-News-Stance数据集和JoA-ICL框架，后者利用语言模型代理预测关键段落的立场，并聚合推断全文立场。

Result: JoA-ICL优于现有方法，验证了分段代理在长文本立场检测中的优势，并在案例研究中展示了其实际应用价值。

Conclusion: 分段立场检测框架JoA-ICL有效提升了新闻立场识别的准确性，有助于推荐系统的多样性设计和媒体偏见分析。

Abstract: As online news consumption grows, personalized recommendation systems have
become integral to digital journalism. However, these systems risk reinforcing
filter bubbles and political polarization by failing to incorporate diverse
perspectives. Stance detection -- identifying a text's position on a target --
can help mitigate this by enabling viewpoint-aware recommendations and
data-driven analyses of media bias. Yet, existing stance detection research
remains largely limited to short texts and high-resource languages. To address
these gaps, we introduce \textsc{K-News-Stance}, the first Korean dataset for
article-level stance detection, comprising 2,000 news articles with
article-level and 19,650 segment-level stance annotations across 47 societal
issues. We also propose \textsc{JoA-ICL}, a \textbf{Jo}urnalism-guided
\textbf{A}gentic \textbf{I}n-\textbf{C}ontext \textbf{L}earning framework that
employs a language model agent to predict the stances of key structural
segments (e.g., leads, quotes), which are then aggregated to infer the overall
article stance. Experiments show that \textsc{JoA-ICL} outperforms existing
stance detection methods, highlighting the benefits of segment-level agency in
capturing the overall position of long-form news articles. Two case studies
further demonstrate its broader utility in promoting viewpoint diversity in
news recommendations and uncovering patterns of media bias.

</details>


### [22] [LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction: A Clinical NLP](https://arxiv.org/abs/2507.11052)
*Haowei Yang,Ziyu Shen,Junli Shao,Luyao Men,Xinyue Han,Jing Dong*

Key words: 心血管疾病, NLP, 大语言模型, 临床决策支持系统, 风险预测

TL;DR: 利用LLM增强的临床NLP流程，从非结构化临床笔记中提取心血管疾病早期指标，提升风险预测性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 心血管疾病（CVD）风险预测主要依赖结构化数据，但非结构化临床笔记包含有价值的信息。本研究旨在通过LLM技术，提高早期风险识别的准确性。

Method: 提出一种LLM增强的NLP流程，包括领域适应的语言模型、症状提取、上下文推理和相关性分析，并结合心血管特定的微调和提示工程。

Result: 在MIMIC-III和CARDIO-NLP数据集上，表现出更高的精确率、召回率、F1分数和AUROC，临床相关性高（kappa=0.82）。

Conclusion: 展示了LLM在临床决策支持系统中的潜力，能够增强早期预警系统并将患者叙述转化为可操作的风险评估。

Abstract: Timely identification and accurate risk stratification of cardiovascular
disease (CVD) remain essential for reducing global mortality. While existing
prediction models primarily leverage structured data, unstructured clinical
notes contain valuable early indicators. This study introduces a novel
LLM-augmented clinical NLP pipeline that employs domain-adapted large language
models for symptom extraction, contextual reasoning, and correlation from
free-text reports. Our approach integrates cardiovascular-specific fine-tuning,
prompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III
and CARDIO-NLP datasets demonstrate improved performance in precision, recall,
F1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by
cardiologists. Challenges such as contextual hallucination, which occurs when
plausible information contracts with provided source, and temporal ambiguity,
which is related with models struggling with chronological ordering of events
are addressed using prompt engineering and hybrid rule-based verification. This
work underscores the potential of LLMs in clinical decision support systems
(CDSS), advancing early warning systems and enhancing the translation of
patient narratives into actionable risk assessments.

</details>


### [23] [Social Media Sentiments Analysis on the July Revolution in Bangladesh: A Hybrid Transformer Based Machine Learning Approach](https://arxiv.org/abs/2507.11084)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha*

Key words: 七月革命、社交媒体、情感分析、Transformer、孟加拉语

TL;DR: 该研究提出了一种基于混合Transformer的情感分析框架，用于解码孟加拉国七月革命期间及之后社交媒体评论中的公众情绪，结合多种先进模型，取得了83.7%的准确率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在利用机器学习技术分析低资源语言（如孟加拉语）的社会情感，特别是在重大事件（如七月革命）中的公众情绪。

Method: 使用BanglaBERT、mBERT、XLM-RoBERTa和提出的混合XMB-BERT进行特征提取，结合PCA降维和11种机器学习分类器进行情感分析。

Result: 混合XMB-BERT与投票分类器组合表现最佳，准确率为83.7%。

Conclusion: 机器学习在低资源语言情感分析中具有潜力，混合Transformer模型效果显著。

Abstract: The July Revolution in Bangladesh marked a significant student-led mass
uprising, uniting people across the nation to demand justice, accountability,
and systemic reform. Social media platforms played a pivotal role in amplifying
public sentiment and shaping discourse during this historic mass uprising. In
this study, we present a hybrid transformer-based sentiment analysis framework
to decode public opinion expressed in social media comments during and after
the revolution. We used a brand new dataset of 4,200 Bangla comments collected
from social media. The framework employs advanced transformer-based feature
extraction techniques, including BanglaBERT, mBERT, XLM-RoBERTa, and the
proposed hybrid XMB-BERT, to capture nuanced patterns in textual data.
Principle Component Analysis (PCA) were utilized for dimensionality reduction
to enhance computational efficiency. We explored eleven traditional and
advanced machine learning classifiers for identifying sentiments. The proposed
hybrid XMB-BERT with the voting classifier achieved an exceptional accuracy of
83.7% and outperform other model classifier combinations. This study
underscores the potential of machine learning techniques to analyze social
sentiment in low-resource languages like Bangla.

</details>


### [24] [Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border Entity Identification](https://arxiv.org/abs/2507.11086)
*Andres Azqueta-Gavaldón,Joaquin Ramos Cosgrove*

Key words: 实体匹配,大语言模型,金融合规,跨境金融,风险管控

TL;DR: 论文研究了在跨境金融活动中利用大语言模型（LLMs）改进实体匹配，相比传统方法，LLMs在准确性和减少误报方面表现更好。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 跨境金融活动的增加需要更准确地识别和分类外国实体，传统匹配方法因语言变体和语义问题效果不佳。

Method: 评估了传统算法（如Jaccard）、Hugging Face的LLMs和接口型LLMs（如Microsoft Copilot），使用65个葡萄牙公司案例数据集。

Result: 传统方法准确率超92%但误报率高（20-40%），接口型LLMs准确率超93%，F1分数超96%，误报率更低（40-80%）。

Conclusion: LLMs在实体匹配任务中优于传统方法，尤其在处理复杂语义和上下文时表现更佳。

Abstract: The growing prevalence of cross-border financial activities in global markets
has underscored the necessity of accurately identifying and classifying foreign
entities. This practice is essential within the Spanish financial system for
ensuring robust risk management, regulatory adherence, and the prevention of
financial misconduct. This process involves a labor-intensive entity-matching
task, where entities need to be validated against available reference sources.
Challenges arise from linguistic variations, special characters, outdated
names, and changes in legal forms, complicating traditional matching algorithms
like Jaccard, cosine, and Levenshtein distances. These methods struggle with
contextual nuances and semantic relationships, leading to mismatches. To
address these limitations, we explore Large Language Models (LLMs) as a
flexible alternative. LLMs leverage extensive training to interpret context,
handle abbreviations, and adapt to legal transitions. We evaluate traditional
methods, Hugging Face-based LLMs, and interface-based LLMs (e.g., Microsoft
Copilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese company cases.
Results show traditional methods achieve accuracies over 92% but suffer high
false positive rates (20-40%). Interface-based LLMs outperform, achieving
accuracies above 93%, F1 scores exceeding 96%, and lower false positives
(40-80%).

</details>


### [25] [The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs](https://arxiv.org/abs/2507.11097)
*Zichen Wen,Jiashu Qu,Dongrui Liu,Zhiyuan Liu,Ruixi Wu,Yicun Yang,Xiangqi Jin,Haoyun Xu,Xuyang Liu,Weijia Li,Chaochao Lu,Jing Shao,Conghui He,Linfeng Zhang*

Key words: 扩散大语言模型, 安全漏洞, 对抗性攻击, 平行解码, 双向建模

TL;DR: 论文研究了基于扩散的大语言模型（dLLM）的安全漏洞，提出了一种名为DIJA的对抗性攻击框架，利用dLLM的双向建模和平行解码机制绕过现有对齐机制，生成有害内容。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管dLLM在代码生成和文本填充方面表现优异，但现有的对齐机制无法有效防御上下文感知的对抗性输入，暴露出新的安全威胁。

Method: 通过构建交错掩码文本提示（interleaved mask-text prompts），利用dLLM的双向建模和平行解码机制，绕过安全对齐机制。

Result: DIJA在多个评测基准上显著优于现有方法，如Dream-Instruct上实现100%关键词攻击成功率，并在JailbreakBench上提升78.5%的攻击成功率。

Conclusion: 研究揭示了dLLM架构中未被重视的安全威胁，强调了重新设计安全对齐机制的紧迫性。

Abstract: Diffusion-based large language models (dLLMs) have recently emerged as a
powerful alternative to autoregressive LLMs, offering faster inference and
greater interactivity via parallel decoding and bidirectional modeling.
However, despite strong performance in code generation and text infilling, we
identify a fundamental safety concern: existing alignment mechanisms fail to
safeguard dLLMs against context-aware, masked-input adversarial prompts,
exposing novel vulnerabilities. To this end, we present DIJA, the first
systematic study and jailbreak attack framework that exploits unique safety
weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial
interleaved mask-text prompts that exploit the text generation mechanisms of
dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional
modeling drives the model to produce contextually consistent outputs for masked
spans, even when harmful, while parallel decoding limits model dynamic
filtering and rejection sampling of unsafe content. This causes standard
alignment mechanisms to fail, enabling harmful completions in alignment-tuned
dLLMs, even when harmful behaviors or unsafe instructions are directly exposed
in the prompt. Through comprehensive experiments, we demonstrate that DIJA
significantly outperforms existing jailbreak methods, exposing a previously
overlooked threat surface in dLLM architectures. Notably, our method achieves
up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior
baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and
by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of
harmful content in the jailbreak prompt. Our findings underscore the urgent
need for rethinking safety alignment in this emerging class of language models.
Code is available at https://github.com/ZichenWen1/DIJA.

</details>


### [26] [Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs](https://arxiv.org/abs/2507.11112)
*Sanhanat Sivapiromrat,Caiqi Zhang,Marco Basaldella,Nigel Collier*

Key words: LLM, 数据投毒, 多触发器, 后修复, 分层权重差异

TL;DR: LLM易受数据投毒攻击，现有研究多局限于单一触发机制。本文提出框架研究多触发共存及其交互，发现高相似性触发器可稳健激活，并提供了基于分层权重差异的后修复方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 揭露LLM在多触发投毒攻击下的广泛脆弱性，填补现有研究对触发机制及交互理解的不足。

Method: 提出研究框架验证多触发器共存性，基于分层权重差异设计选择性重训练的后修复防御方法。

Result: 多触发器可在模型中独立共存且稳健激活；后修复方法能高效去除触发行为。

Conclusion: LLM存在更广泛的脆弱性，需针对性防御；分层重训练方法为多触发投毒提供了高效解决方案。

Abstract: Recent studies have shown that Large Language Models (LLMs) are vulnerable to
data poisoning attacks, where malicious training examples embed hidden
behaviours triggered by specific input patterns. However, most existing works
assume a phrase and focus on the attack's effectiveness, offering limited
understanding of trigger mechanisms and how multiple triggers interact within
the model. In this paper, we present a framework for studying poisoning in
LLMs. We show that multiple distinct backdoor triggers can coexist within a
single model without interfering with each other, enabling adversaries to embed
several triggers concurrently. Using multiple triggers with high embedding
similarity, we demonstrate that poisoned triggers can achieve robust activation
even when tokens are substituted or separated by long token spans. Our findings
expose a broader and more persistent vulnerability surface in LLMs. To mitigate
this threat, we propose a post hoc recovery method that selectively retrains
specific model components based on a layer-wise weight difference analysis. Our
method effectively removes the trigger behaviour with minimal parameter
updates, presenting a practical and efficient defence against multi-trigger
poisoning.

</details>


### [27] [MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models](https://arxiv.org/abs/2507.11114)
*Seif Ahmed,Mohamed T. Younes,Abdelrahman Moustafa,Abdelrahman Allam,Hamza Moustafa*

Key words: 多模态推理,多语言,集成系统,Gemini模型,提示工程

TL;DR: 本文介绍了一种基于集成系统的多语言多模态推理方法，在ImageCLEF 2025 EXAMS V挑战中表现优异，通过精心设计的提示策略和多语言增强，取得了最高准确率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在解决多语言多模态推理问题，探索轻量级OCR-VLM集成系统在教育场景中的高效性。

Method: 采用Gemini系列模型（2.5 Flash、1.5 Pro、2.5 Pro）进行视觉描述、标题优化和推理，结合少样本和零样本提示策略，并通过多语言数据增强训练多个大语言模型。

Result: 系统在官方排行榜中多语言赛道总体准确率达81.4%，并在13个语言赛道中11个领先；提示策略优化将英语验证集准确率从55.9%提升至61.7%。

Conclusion: 轻量级OCR-VLM集成系统结合精准提示策略和多语言增强，能在高要求的跨语言教育场景中超越更重的端到端模型。

Abstract: We present a robust ensemble-based system for multilingual multimodal
reasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach
integrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption
refinement and consistency checks, and Gemini 2.5 Pro as a reasoner which
handles final answer selection, all coordinated through carefully engineered
few-shot and zero-shot prompts. We conducted an extensive ablation study,
training several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3,
Mistral) on an English dataset and its multilingual augmented version.
Additionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for
comparison and found it to substantially outperform the trained models. Prompt
design also proved critical: enforcing concise, language-normalized formats and
prohibiting explanatory text boosted model accuracy on the English validation
set from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA)
achieved first place overall in the multilingual track with 81.4% accuracy, and
led 11 out of 13 individual language tracks, with top results such as 95.07%
for Croatian and 92.12% for Italian. These findings highlight that lightweight
OCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual
augmentation, can outperform heavier end-to-end models in high-stakes,
multilingual educational settings.

</details>


### [28] [What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests](https://arxiv.org/abs/2507.11128)
*Dimitri Staufer*

Key words: 大语言模型, GDPR, 被遗忘权, 隐私审计, 机器去学习

TL;DR: 论文介绍了WikiMem数据集和一种模型无关的方法，用于量化LLMs中个人与事实的关联，为识别和删除个人数据提供了基础。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究LLMs如何记忆和泄露个人数据，尤其是满足GDPR的‘被遗忘权’需求。

Method: 使用WikiMem数据集和校准负对数似然方法，评估LLMs中个人与事实的关联。

Result: 发现模型记忆与个人网络存在和模型规模相关，证明了方法在识别个人数据上的有效性。

Conclusion: 为动态构建遗忘集提供了基础，支持机器去学习和RTBF请求。

Abstract: Large Language Models (LLMs) can memorize and reveal personal information,
raising concerns regarding compliance with the EU's GDPR, particularly the
Right to Be Forgotten (RTBF). Existing machine unlearning methods assume the
data to forget is already known but do not address how to identify which
individual-fact associations are stored in the model. Privacy auditing
techniques typically operate at the population level or target a small set of
identifiers, limiting applicability to individual-level data inquiries. We
introduce WikiMem, a dataset of over 5,000 natural language canaries covering
243 human-related properties from Wikidata, and a model-agnostic metric to
quantify human-fact associations in LLMs. Our approach ranks ground-truth
values against counterfactuals using calibrated negative log-likelihood across
paraphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B
parameters), showing that memorization correlates with subject web presence and
model scale. We provide a foundation for identifying memorized personal data in
LLMs at the individual level, enabling the dynamic construction of forget sets
for machine unlearning and RTBF requests.

</details>


### [29] [Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding](https://arxiv.org/abs/2507.11198)
*Conrad Borchers,Bahar Shahrokhian,Francesco Balzan,Elham Tajik,Sreecharan Sankaranarayanan,Sebastian Simon*

Key words: 大语言模型, 多代理系统, 定性研究, 编码准确性, 温度效应

TL;DR: 该论文研究了多代理系统（MAS）在定性研究中的编码准确性，发现温度和代理角色对共识达成有显著影响，但单代理系统通常表现更优。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨多代理系统（MAS）是否比单代理系统在定性研究的编码和数据标注中更具优势，尤其关注代理角色和温度对共识构建和编码准确性的影响。

Method: 通过实验研究，利用6种开源LLM（参数规模从30亿到320亿）和18种配置，分析超过77,000个编码决策，对比人类标注的数学辅导会话数据集。

Result: 温度和代理角色显著影响共识达成，但MAS在编码准确性上未表现出明显优势；单代理系统在多数情况下表现更好。

Conclusion: MAS在编码任务中的作用有限，挑战了多样代理角色能带来更好结果的假设。

Abstract: Large Language Models (LLMs) enable new possibilities for qualitative
research at scale, including coding and data annotation. While multi-agent
systems (MAS) can emulate human coding workflows, their benefits over
single-agent coding remain poorly understood. We conducted an experimental
study of how agent persona and temperature shape consensus-building and coding
accuracy of dialog segments based on a codebook with 8 codes. Our open-source
MAS mirrors deductive human coding through structured agent discussion and
consensus arbitration. Using six open-source LLMs (with 3 to 32 billion
parameters) and 18 experimental configurations, we analyze over 77,000 coding
decisions against a gold-standard dataset of human-annotated transcripts from
online math tutoring sessions. Temperature significantly impacted whether and
when consensus was reached across all six LLMs. MAS with multiple personas
(including neutral, assertive, or empathetic), significantly delayed consensus
in four out of six LLMs compared to uniform personas. In three of those LLMs,
higher temperatures significantly diminished the effects of multiple personas
on consensus. However, neither temperature nor persona pairing lead to robust
improvements in coding accuracy. Single agents matched or outperformed MAS
consensus in most conditions. Only one model (OpenHermesV2:7B) and code
category showed above-chance gains from MAS deliberation when temperature was
0.5 or lower and especially when the agents included at least one assertive
persona. Qualitative analysis of MAS collaboration for these configurations
suggests that MAS may nonetheless aid in narrowing ambiguous code applications
that could improve codebooks and human-AI coding. We contribute new insight
into the limits of LLM-based qualitative methods, challenging the notion that
diverse MAS personas lead to better outcomes. We open-source our MAS and
experimentation code.

</details>


### [30] [EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question Answering](https://arxiv.org/abs/2507.11216)
*Valle Ruiz-Fernández,Mario Mina,Júlia Falcão,Luis Vasquez-Reina,Anna Sallés,Aitor Gonzalez-Agirre,Olatz Perez-de-Viñaspre*

Key words: Large Language Models, social bias, Spanish, Catalan, BBQ, evaluation

TL;DR: 这篇论文介绍了西班牙和加泰罗尼亚语的社会偏见评估数据集EsBBQ和CaBBQ，基于原版BBQ设计，用于评估大语言模型在西班牙社会背景下的偏见。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为了解决非英语语言和非美国社会背景下社会偏见评估资源的缺乏问题。

Method: 设计了西班牙语和加泰罗尼亚语的偏见基准数据集，通过多选题QA形式评估10个类别的社会偏见。

Result: 模型在模糊场景中常无法选择正确答案，且高QA准确率常与更依赖社会偏见相关。

Conclusion: 本研究填补了非英语语言社会偏见评估的空白，并揭示了模型在模糊场景中的偏见行为。

Abstract: Previous literature has largely shown that Large Language Models (LLMs)
perpetuate social biases learnt from their pre-training data. Given the notable
lack of resources for social bias evaluation in languages other than English,
and for social contexts outside of the United States, this paper introduces the
Spanish and the Catalan Bias Benchmarks for Question Answering (EsBBQ and
CaBBQ). Based on the original BBQ, these two parallel datasets are designed to
assess social bias across 10 categories using a multiple-choice QA setting, now
adapted to the Spanish and Catalan languages and to the social context of
Spain. We report evaluation results on different LLMs, factoring in model
family, size and variant. Our results show that models tend to fail to choose
the correct answer in ambiguous scenarios, and that high QA accuracy often
correlates with greater reliance on social biases.

</details>


### [31] [An Agentic Flow for Finite State Machine Extraction using Prompt Chaining](https://arxiv.org/abs/2507.11222)
*Fares Wael,Youssef Maklad,Ali Hamdi,Wael Elsersy*

Key words: Finite-State Machines, LLM, RFC documents, protocol analysis, cybersecurity

TL;DR: 提出FlowFSM框架，利用LLM和提示链技术从RFC文档中提取精确的FSM，克服现有技术的局限性，实验证明其高效性和准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有FSM提取技术在可扩展性、覆盖范围和自然语言歧义处理方面存在不足，FlowFSM旨在解决这些问题。

Method: 采用基于LLM的代理框架FlowFSM，结合提示链和链式思维推理，从RFC文档中提取FSM并构建结构化规则书。

Result: 在FTP和RTSP协议上的实验显示，FlowFSM提取精度高，幻觉转移少，效果显著。

Conclusion: FlowFSM展示了基于代理的LLM系统在协议分析和FSM推断中的潜力，适用于网络安全和逆向工程。

Abstract: Finite-State Machines (FSMs) are critical for modeling the operational logic
of network protocols, enabling verification, analysis, and vulnerability
discovery. However, existing FSM extraction techniques face limitations such as
scalability, incomplete coverage, and ambiguity in natural language
specifications. In this paper, we propose FlowFSM, a novel agentic framework
that leverages Large Language Models (LLMs) combined with prompt chaining and
chain-of-thought reasoning to extract accurate FSMs from raw RFC documents.
FlowFSM systematically processes protocol specifications, identifies state
transitions, and constructs structured rule-books by chaining agent outputs.
Experimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM
achieves high extraction precision while minimizing hallucinated transitions,
showing promising results. Our findings highlight the potential of agent-based
LLM systems in the advancement of protocol analysis and FSM inference for
cybersecurity and reverse engineering applications.

</details>


### [32] [Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages](https://arxiv.org/abs/2507.11230)
*Lyzander Marciano Andrylie,Inaya Rahmanisa,Mahardika Krisna Ihsani,Alfan Farizki Wicaksono,Haryo Akbarianto Wibowo,Alham Fikri Aji*

Key words: 大型语言模型、多语言机制、稀疏自编码器、语言特定特征

TL;DR: 论文提出SAE-LAPE方法，利用稀疏自编码器识别大型语言模型中的语言特定特征，发现这些特征主要集中在模型的中间至最终层，并影响模型的多语言性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究大型语言模型的多语言机制，以理解其如何处理不同语言，尤其是语言特定特征的识别问题。

Method: 采用稀疏自编码器（SAEs）和提出的SAE-LAPE方法，基于特征激活概率识别语言特定特征。

Result: 发现语言特定特征集中在模型的中后层，影响模型的多语言表现和输出，且可用于语言识别，性能与fastText相当但更可解释。

Conclusion: SAE-LAPE方法有效识别了语言特定特征，揭示了这些特征在模型中的作用，为多语言机制研究提供了新视角。

Abstract: Understanding the multilingual mechanisms of large language models (LLMs)
provides insight into how they process different languages, yet this remains
challenging. Existing studies often focus on individual neurons, but their
polysemantic nature makes it difficult to isolate language-specific units from
cross-lingual representations. To address this, we explore sparse autoencoders
(SAEs) for their ability to learn monosemantic features that represent concrete
and abstract concepts across languages in LLMs. While some of these features
are language-independent, the presence of language-specific features remains
underexplored. In this work, we introduce SAE-LAPE, a method based on feature
activation probability, to identify language-specific features within the
feed-forward network. We find that many such features predominantly appear in
the middle to final layers of the model and are interpretable. These features
influence the model's multilingual performance and language output and can be
used for language identification with performance comparable to fastText along
with more interpretability. Our code is available at
https://github.com/LyzanderAndrylie/language-specific-features .

</details>


### [33] [KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding](https://arxiv.org/abs/2507.11273)
*Luohe Shi,Zuchao Li,Lefei Zhang,Guoming Liu,Baoyuan Qi,Hai Zhao*

Key words: KV缓存，潜在空间，位置编码，推理效率

TL;DR: 提出KV-Latent范式，通过降采样键值向量维度来减少KV缓存占用并提升推理速度，同时通过改进位置编码的频域采样机制增强稳定性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解码器架构中逐渐增加的KV缓存成为推理效率瓶颈，亟需优化。

Method: 提出KV-Latent范式，降采样键值向量至潜在空间，并改进Rotary位置编码的频域采样机制。

Result: 显著减少KV缓存占用和提升推理速度，实验效果满意。

Conclusion: KV-Latent方法能构建更高效的语言模型系统，为KV缓存优化提供新思路。

Abstract: Large language models (LLMs) based on Transformer Decoders have become the
preferred choice for conversational generative AI. Despite the overall
superiority of the Decoder architecture, the gradually increasing Key-Value
(KV) cache during inference has emerged as a primary efficiency bottleneck,
both in aspects of memory consumption and data transfer bandwidth limitations.
To address these challenges, we propose a paradigm called KV-Latent. By
down-sampling the Key-Value vector dimensions into a latent space, we can
significantly reduce the KV Cache footprint and improve inference speed, only
with a small amount of extra training, less than 1\% of pre-training takes.
Besides, we enhanced the stability of Rotary Positional Embedding applied on
lower-dimensional vectors by modifying its frequency sampling mechanism,
avoiding noise introduced by higher frequencies while retaining position
attenuation. Our experiments, including both models with Grouped Query
Attention and those without, have yielded satisfactory results. Finally, we
conducted comparative experiments to study the impact of separately reducing
Key and Value components on model's performance. Our approach allows for the
construction of more efficient language model systems, and opens the new
possibility on KV Cache saving and efficient LLMs. Our code is available at
https://github.com/ShiLuohe/KV-Latent.

</details>


### [34] [FMC: Formalization of Natural Language Mathematical Competition Problems](https://arxiv.org/abs/2507.11275)
*Jiaxuan Xie,Chengwu Liu,Ye Yuan,Siqi Li,Zhiping Xiao,Ming Zhang*

Key words: 形式化数学、自动定理证明、大语言模型、错误反馈、few-shot学习

TL;DR: 论文提出了一种基于大语言模型的自动形式化方法，并构建了一个奥林匹克级别的数据集，用于评估自动定理证明器的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 推动形式化数学推理，通过大规模自然语言数学问题数据集构建形式化语言数据集。

Method: 采用基于大语言模型的自动形式化流程，结合错误反馈机制，实现完全自动化且无需训练的形式化方法。

Result: 构建了一个包含3,922个自然语言问题和9,787个Lean形式化问题的数据集，64.46%为高质量，适合作为自动定理证明器的基准。实验表明，few-shot学习、错误反馈和增加采样数量能提升形式化效果。

Conclusion: 该数据集具有挑战性，适合作为形式化推理任务的基准，同时证明了所提方法的有效性。

Abstract: Efficient and accurate autoformalization methods, which leverage large-scale
datasets of extensive natural language mathematical problems to construct
formal language datasets, are key to advancing formal mathematical reasoning.
In this paper, we propose an autoformalization pipeline based on large language
models with error feedback, achieving a fully automatic and training-free
formalization approach. Using this pipeline, we curate an Olympiad-level
dataset aligning natural language problems with Lean formalizations. The
dataset comprises $3,922$ mathematical problems in natural language and $9,787$
in Lean, of which $64.46\%$ were assessed as at least above-average quality,
making it suitable as a benchmark for automated theorem provers. Additionally,
we investigate the formalization and reasoning capabilities of various LLMs and
empirically demonstrate that few-shot learning, error feedback, and increasing
sampling numbers enhance the autoformalization process. Experiments of three
automated theorem provers on the \dataset\ dataset also highlight its
challenging nature and its value as a benchmark for formal reasoning tasks.

</details>


### [35] [Fine-Grained Chinese Hate Speech Understanding: Span-Level Resources, Coded Term Lexicon, and Enhanced Detection Frameworks](https://arxiv.org/abs/2507.11292)
*Zewen Bai,Liang Yang,Shengdi Yin,Yuanyuan Sun,Hongfei Lin*

Key words: 中文仇恨言论、细粒度标注、编码仇恨术语、可解释性、机器学习

TL;DR: 本文提出了首个中文细粒度仇恨言论数据集（STATE ToxiCN），研究了中文编码仇恨术语及大语言模型的仇恨语义解释能力，并提出了一种整合标注词典的方法以提升检测性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 仇恨言论的社会危害日益严重，但中文仇恨言论检测及可解释性研究面临数据集稀缺和编码仇恨术语研究不足的挑战。

Method: 引入STATE ToxiCN数据集，研究中文编码仇恨术语及大语言模型能力，提出整合标注词典的方法。

Result: 显著提升了中文仇恨言论检测性能，并提供了可解释性研究的资源。

Conclusion: 本研究为中文仇恨言论检测的可解释性研究提供了宝贵资源和见解。

Abstract: The proliferation of hate speech has inflicted significant societal harm,
with its intensity and directionality closely tied to specific targets and
arguments. In recent years, numerous machine learning-based methods have been
developed to detect hateful comments on online platforms automatically.
However, research on Chinese hate speech detection lags behind, and
interpretability studies face two major challenges: first, the scarcity of
span-level fine-grained annotated datasets limits models' deep semantic
understanding of hate speech; second, insufficient research on identifying and
interpreting coded hate speech restricts model explainability in complex
real-world scenarios. To address these, we make the following contributions:
(1) We introduce the Span-level Target-Aware Toxicity Extraction dataset (STATE
ToxiCN), the first span-level Chinese hate speech dataset, and evaluate the
hate semantic understanding of existing models using it. (2) We conduct the
first comprehensive study on Chinese coded hate terms, LLMs' ability to
interpret hate semantics. (3) We propose a method to integrate an annotated
lexicon into models, significantly enhancing hate speech detection performance.
Our work provides valuable resources and insights to advance the
interpretability of Chinese hate speech detection research.

</details>


### [36] [Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving Patient-Doctor Communication in Romanian](https://arxiv.org/abs/2507.11299)
*Andrei Niculae,Adrian Cosma,Cosmin Dumitrache,Emilian Rǎdoi*

Key words: 远程医疗, 大型语言模型, 罗马尼亚语, 多代理系统, DSPy

TL;DR: Dr.Copilot是一个多代理大型语言模型系统，旨在提升罗马尼亚语医生在远程医疗中文答复的呈现质量，而非临床准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 远程医疗中文答复的质量通常取决于沟通方式而非医学准确性，因此需要提升医生的表达质量。

Method: 使用三个大型语言模型代理，通过DSPy自动优化提示，支持罗马尼亚语低资源数据，并采用开放权重模型实现实时反馈。

Result: 通过41名医生的实证评估和实际部署，用户评价和答复质量均显著提升。

Conclusion: Dr.Copilot是罗马尼亚医疗环境中早期成功部署的大型语言模型应用之一。

Abstract: Text-based telemedicine has become increasingly common, yet the quality of
medical advice in doctor-patient interactions is often judged more on how
advice is communicated rather than its clinical accuracy. To address this, we
introduce Dr.Copilot , a multi-agent large language model (LLM) system that
supports Romanian-speaking doctors by evaluating and enhancing the presentation
quality of their written responses. Rather than assessing medical correctness,
Dr.Copilot provides feedback along 17 interpretable axes. The system comprises
of three LLM agents with prompts automatically optimized via DSPy. Designed
with low-resource Romanian data and deployed using open-weight models, it
delivers real-time specific feedback to doctors within a telemedicine platform.
Empirical evaluations and live deployment with 41 doctors show measurable
improvements in user reviews and response quality, marking one of the first
real-world deployments of LLMs in Romanian medical settings.

</details>


### [37] [Internal Value Alignment in Large Language Models through Controlled Value Vector Activation](https://arxiv.org/abs/2507.11316)
*Haoran Jin,Meng Li,Xiting Wang,Zhihao Xu,Minlie Huang,Yantao Jia,Defu Lian*

Key words: 大语言模型, 价值对齐, 价值向量, 门控激活, 上下文控制

TL;DR: 提出了一种名为ConVA的方法，通过直接调整大语言模型内部的价值表示来对齐人类价值观，并在实验中验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 对齐大语言模型与人类价值观的需求日益增长，但目前缺乏既高效又能保持模型性能的方法。

Method: 采用上下文控制的价值向量识别方法和门控价值向量激活方法，直接干预模型的潜在表示以实现价值对齐。

Result: 实验表明，ConVA在10种基本价值观上实现了最高的控制成功率，且不影响模型的性能和流畅性。

Conclusion: ConVA是一种高效且不影响性能的价值对齐方法，能够应对复杂甚至恶意的输入提示。

Abstract: Aligning Large Language Models (LLMs) with human values has attracted
increasing attention since it provides clarity, transparency, and the ability
to adapt to evolving scenarios. In this paper, we introduce a Controlled Value
Vector Activation (ConVA) method that directly aligns the internal values of
LLMs by interpreting how a value is encoded in their latent representations and
modifies relevant activations to ensure consistent values in LLMs. To ensure an
accurate and unbiased interpretation, we propose a context-controlled value
vector identification method. To consistently control values without
sacrificing model performance, we introduce a gated value vector activation
method for effective and minimum degree of value control. Experiments show that
our method achieves the highest control success rate across 10 basic values
without hurting LLM performance and fluency, and ensures target values even
with opposite and potentially malicious input prompts. Source code and data are
available at~ https://github.com/hr-jin/ConVA.

</details>


### [38] [Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge](https://arxiv.org/abs/2507.11330)
*Wenqing Wu,Chengzhi Zhang,Yi Zhao*

Key words: 新颖性评估，大语言模型，预训练语言模型，人类专家，稀疏注意力

TL;DR: 研究结合人类专家和大语言模型（LLM）的知识与能力，提出一种新方法评估学术论文的方法新颖性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统新颖性评估方法（专家评审和引用组合）存在局限性，LLM和人类专家各有优势，需结合两者。

Method: 从同行评审报告中提取新颖性相关句子，用LLM总结论文方法部分，微调预训练语言模型（PLM），并设计文本引导的稀疏注意力融合模块。

Result: 实验表明，该方法优于众多基线。

Conclusion: 结合人类与LLM知识的方法在评估论文新颖性方面表现出色。

Abstract: Novelty is a crucial criterion in the peer review process for evaluating
academic papers. Traditionally, it's judged by experts or measure by unique
reference combinations. Both methods have limitations: experts have limited
knowledge, and the effectiveness of the combination method is uncertain.
Moreover, it's unclear if unique citations truly measure novelty. The large
language model (LLM) possesses a wealth of knowledge, while human experts
possess judgment abilities that the LLM does not possess. Therefore, our
research integrates the knowledge and abilities of LLM and human experts to
address the limitations of novelty assessment. The most common novelty in
academic papers is the introduction of new methods. In this paper, we propose
leveraging human knowledge and LLM to assist pretrained language models (PLMs,
e.g. BERT etc.) in predicting the method novelty of papers. Specifically, we
extract sentences related to the novelty of the academic paper from peer review
reports and use LLM to summarize the methodology section of the academic paper,
which are then used to fine-tune PLMs. In addition, we have designed a
text-guided fusion module with novel Sparse-Attention to better integrate human
and LLM knowledge. We compared the method we proposed with a large number of
baselines. Extensive experiments demonstrate that our method achieves superior
performance.

</details>


### [39] [What is the Best Process Model Representation? A Comparative Analysis for Process Modeling with Large Language Models](https://arxiv.org/abs/2507.11356)
*Alexis Brissard,Frédéric Cuppens,Amal Zouaq*

Key words: 大型语言模型,流程建模,流程模型表示,流程模型生成,Mermaid,BPMN text

TL;DR: 本文对大型语言模型（LLMs）在流程建模（PMo）任务中的应用进行了首次实证研究，比较了多种流程模型表示（PMRs），并评估了它们在PMo中的适用性和流程模型生成（PMG）性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机在于现有的PMRs在结构、复杂性和可用性上差异较大，且缺乏系统性比较，同时PMG方法的评估策略和技术各异，难以对比。

Method: 提出了PMo数据集，包含55个流程描述及对应九种PMRs的模型，并从两个维度评估PMRs：适用于LLM的PMo能力和PMG性能。

Result: 研究发现，Mermaid在六项PMo标准中得分最高，而BPMN text在流程元素相似性方面表现最佳。

Conclusion: 结论是Mermaid整体表现最优，BPMN text在PMG任务中效果最好，为LLM在PMo中的应用提供了实证支持。

Abstract: Large Language Models (LLMs) are increasingly applied for Process Modeling
(PMo) tasks such as Process Model Generation (PMG). To support these tasks,
researchers have introduced a variety of Process Model Representations (PMRs)
that serve as model abstractions or generation targets. However, these PMRs
differ widely in structure, complexity, and usability, and have never been
systematically compared. Moreover, recent PMG approaches rely on distinct
evaluation strategies and generation techniques, making comparison difficult.
This paper presents the first empirical study that evaluates multiple PMRs in
the context of PMo with LLMs. We introduce the PMo Dataset, a new dataset
containing 55 process descriptions paired with models in nine different PMRs.
We evaluate PMRs along two dimensions: suitability for LLM-based PMo and
performance on PMG. \textit{Mermaid} achieves the highest overall score across
six PMo criteria, whereas \textit{BPMN text} delivers the best PMG results in
terms of process element similarity.

</details>


### [40] [Addressing Data Imbalance in Transformer-Based Multi-Label Emotion Detection with Weighted Loss](https://arxiv.org/abs/2507.11384)
*Xia Cui*

Key words: Transformer、多标签情感检测、加权损失函数、数据不平衡、BRIGHTER数据集

TL;DR: 论文研究了加权损失函数在Transformer模型中用于多标签情感检测的效果，重点解决数据不平衡问题，但效果在少数情感类别上有限。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为了解决多标签情感检测中数据不平衡的问题，提出了一种动态调整类别权重的加权损失函数，避免传统重采样方法的高计算成本。

Method: 对BERT、RoBERTa和BART模型应用加权损失函数，并在BRIGHTER数据集上进行评估，使用Micro F1、Macro F1、ROC-AUC、Accuracy和Jaccard相似系数等指标。

Result: 加权损失函数显著提高了高频情感类别的检测性能，但对少数类别的改进有限。

Conclusion: 加权损失函数在多标签情感检测中有效但也存在挑战，尤其是在处理少数类别时效果不明显。

Abstract: This paper explores the application of a simple weighted loss function to
Transformer-based models for multi-label emotion detection in SemEval-2025
Shared Task 11. Our approach addresses data imbalance by dynamically adjusting
class weights, thereby enhancing performance on minority emotion classes
without the computational burden of traditional resampling methods. We evaluate
BERT, RoBERTa, and BART on the BRIGHTER dataset, using evaluation metrics such
as Micro F1, Macro F1, ROC-AUC, Accuracy, and Jaccard similarity coefficients.
The results demonstrate that the weighted loss function improves performance on
high-frequency emotion classes but shows limited impact on minority classes.
These findings underscore both the effectiveness and the challenges of applying
this approach to imbalanced multi-label emotion detection.

</details>


### [41] [DCR: Quantifying Data Contamination in LLMs Evaluation](https://arxiv.org/abs/2507.11405)
*Cheng Xu,Nan Yan,Shuhao Guan,Changhong Jin,Yuke Mei,Yibing Guo,M-Tahar Kechadi*

Key words: 大语言模型、基准数据污染、DCR框架、模糊推理系统、性能评估

TL;DR: 本文提出了一种名为数据污染风险（DCR）框架的轻量级、可解释的检测方法，用于评估大语言模型在基准数据污染（BDC）中的表现。通过融合模糊推理系统，DCR量化了四种污染级别，并调整原始准确率以反映真实性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大语言模型的快速发展引发了对基准数据污染（BDC）的担忧，即模型无意中记住了评估数据，导致性能指标虚高，影响了泛化能力的真实评估。为了解决这一问题，研究提出了DCR框架。

Method: DCR框架通过检测和量化四种污染级别（语义、信息、数据和标签），结合模糊推理系统生成统一的DCR因子，用于调整原始准确率。该方法在9种不同规模的LLM上进行了验证。

Result: DCR框架在情感分析、假新闻检测和算术推理任务中表现出色，污染严重程度诊断准确且调整后的准确率与未污染基准相比平均误差在4%以内。

Conclusion: DCR框架强调了计算效率和透明度，为日常评估提供了实用的污染检测工具，促进了公平的比较，并提高了大语言模型基准测试的可信度。

Abstract: The rapid advancement of large language models (LLMs) has heightened concerns
about benchmark data contamination (BDC), where models inadvertently memorize
evaluation data, inflating performance metrics and undermining genuine
generalization assessment. This paper introduces the Data Contamination Risk
(DCR) framework, a lightweight, interpretable pipeline designed to detect and
quantify BDC across four granular levels: semantic, informational, data, and
label. By synthesizing contamination scores via a fuzzy inference system, DCR
produces a unified DCR Factor that adjusts raw accuracy to reflect
contamination-aware performance. Validated on 9 LLMs (0.5B-72B) across
sentiment analysis, fake news detection, and arithmetic reasoning tasks, the
DCR framework reliably diagnoses contamination severity and with accuracy
adjusted using the DCR Factor to within 4% average error across the three
benchmarks compared to the uncontaminated baseline. Emphasizing computational
efficiency and transparency, DCR provides a practical tool for integrating
contamination assessment into routine evaluations, fostering fairer comparisons
and enhancing the credibility of LLM benchmarking practices.

</details>


### [42] [EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes](https://arxiv.org/abs/2507.11407)
*LG AI Research,:,Kyunghoon Bae,Eunbi Choi,Kibong Choi,Stanley Jungkyu Choi,Yemuk Choi,Kyubeen Han,Seokhee Hong,Junwon Hwang,Taewan Hwang,Joonwon Jang,Hyojin Jeon,Kijeong Jeon,Gerrard Jeongwon Jo,Hyunjik Jo,Jiyeon Jung,Euisoon Kim,Hyosang Kim,Jihoon Kim,Joonkee Kim,Seonghwan Kim,Soyeon Kim,Sunkyoung Kim,Yireun Kim,Yongil Kim,Youchul Kim,Edward Hwayoung Lee,Gwangho Lee,Haeju Lee,Honglak Lee,Jinsik Lee,Kyungmin Lee,Sangha Park,Young Min Paik,Yongmin Park,Youngyong Park,Sanghyun Seo,Sihoon Yang,Heuiyeen Yeen,Sihyuk Yi,Hyeongu Yun*

Key words: EXAONE 4.0, 推理模式, 多语言支持, 代理AI, 模型性能

TL;DR: EXAONE 4.0引入非推理与推理模式，提升可用性与推理能力，支持多语言（包括西班牙语），提供两种规模模型（32B和1.2B），性能优于同类开源模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为迎接代理AI时代，结合EXAONE 3.5的易用性和EXAONE Deep的推理能力，开发功能更强的EXAONE 4.0。

Method: 采用非推理与推理模式整合，扩展多语言支持（英语、韩语、西班牙语），提供32B（高性能）和1.2B（轻量级）两种模型规模。

Result: EXAONE 4.0性能优于同类开源模型，并与前沿模型保持竞争力，模型已公开供研究下载。

Conclusion: EXAONE 4.0通过多模式整合与多语言支持，为代理AI时代提供了高性能且易用的工具。

Abstract: This technical report introduces EXAONE 4.0, which integrates a Non-reasoning
mode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5
and the advanced reasoning abilities of EXAONE Deep. To pave the way for the
agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool
use, and its multilingual capabilities are extended to support Spanish in
addition to English and Korean. The EXAONE 4.0 model series consists of two
sizes: a mid-size 32B model optimized for high performance, and a small-size
1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates
superior performance compared to open-weight models in its class and remains
competitive even against frontier-class models. The models are publicly
available for research purposes and can be easily downloaded via
https://huggingface.co/LGAI-EXAONE.

</details>


### [43] [KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?](https://arxiv.org/abs/2507.11408)
*Soumadeep Saha,Akshay Chaturvedi,Saptarshi Saha,Utpal Garain,Nicholas Asher*

Key words: Chain-of-thought, reasoning tasks, large language models, Causal CoT Graphs, KisMATH, LLM reasoning

TL;DR: 论文研究了链式思维跟踪（Chain-of-thought）在大语言模型中提升推理能力的机制，并提出了Causal CoT Graphs（CCGs）来建模因果依赖关系。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索链式思维跟踪如何提升大语言模型在推理任务中的性能，并提出一种新的方法来建模语言模型输出的因果依赖关系。

Method: 引入了Causal CoT Graphs（CCGs），从推理跟踪中自动提取有向无环图，建模语言模型输出中的细粒度因果依赖。使用MATH500、GSM8K和AIME中的1671个数学推理问题及其CCGs，构建了数据集KisMATH。通过15个开源权重的LLMs进行了详细实证分析。

Result: 发现（i）CCG中的推理节点是最终答案的中介者，这是推理的必要条件；（ii）LLMs强调CCG给出的推理路径，表明模型内部实现了类似于CCG的结构。

Conclusion: KisMATH支持受控的、图对齐的干预，为进一步研究链式思维在LLM推理中的作用开辟了新的途径。

Abstract: Chain-of-thought traces have been shown to improve performance of large
language models in a plethora of reasoning tasks, yet there is no consensus on
the mechanism through which this performance boost is achieved. To shed more
light on this, we introduce Causal CoT Graphs (CCGs), which are directed
acyclic graphs automatically extracted from reasoning traces that model
fine-grained causal dependencies in the language model output. A collection of
$1671$ mathematical reasoning problems from MATH500, GSM8K and AIME, and their
associated CCGs are compiled into our dataset -- \textbf{KisMATH}. Our detailed
empirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in
the CCG are mediators for the final answer, a condition necessary for
reasoning; and (ii) LLMs emphasise reasoning paths given by the CCG, indicating
that models internally realise structures akin to our graphs. KisMATH enables
controlled, graph-aligned interventions and opens up avenues for further
investigation into the role of chain-of-thought in LLM reasoning.

</details>


### [44] [Seq vs Seq: An Open Suite of Paired Encoders and Decoders](https://arxiv.org/abs/2507.11412)
*Orion Weller,Kathryn Ricci,Marc Marone,Antoine Chaffin,Dawn Lawrie,Benjamin Van Durme*

Key words: 大语言模型, 编码器-仅, 解码器-仅, 任务适配, 开源数据集

TL;DR: 该论文介绍了SOTA开源数据集Ettin套件，比较了编码器-仅和解码器-仅语言模型的性能，发现编码器在分类和检索任务中表现更好，解码器在生成任务中更优。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究编码器-仅和解码器-仅语言模型在不同任务中的性能差异，填补现有研究在模型规模和训练方法上的不足。

Method: 使用相同训练配方和数据集，训练从1700万到10亿参数的编码器-仅和解码器-仅模型，并进行任务适配实验。

Result: 编码器-仅模型在分类和检索任务中优于解码器-仅模型，而解码器-仅模型在生成任务中表现更好；任务适配效果不佳。

Conclusion: 不同架构的模型在特定任务中各有优势，跨任务适配效果有限。

Abstract: The large language model (LLM) community focuses almost exclusively on
decoder-only language models, since they are easier to use for text generation.
However, a large subset of the community still uses encoder-only models for
tasks such as classification or retrieval. Previous work has attempted to
compare these architectures, but is forced to make comparisons with models that
have different numbers of parameters, training techniques, and datasets. We
introduce the SOTA open-data Ettin suite of models: paired encoder-only and
decoder-only models ranging from 17 million parameters to 1 billion, trained on
up to 2 trillion tokens. Using the same recipe for both encoder-only and
decoder-only models produces SOTA recipes in both categories for their
respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as
decoders. Like previous work, we find that encoder-only models excel at
classification and retrieval tasks while decoders excel at generative tasks.
However, we show that adapting a decoder model to encoder tasks (and vice
versa) through continued training is subpar compared to using only the reverse
objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa
for generative tasks). We open-source all artifacts of this study including
training data, training order segmented by checkpoint, and 200+ checkpoints to
allow future work to analyze or extend all aspects of training.

</details>


### [45] [Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?](https://arxiv.org/abs/2507.11423)
*Yanjian Zhang,Guillaume Wisniewski,Nadi Tomeh,Thierry Charnois*

Key words: 大语言模型, 推理策略, 提示, 适应性选择, 逻辑问题

TL;DR: 研究表明，提示可以影响大语言模型（LLMs）的推理策略，但单一策略无法持续提升准确性，适应性策略选择可能优化效果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索LLMs推理策略的多样性，解决其在多样化推理任务中的局限性。

Method: 通过提示控制LLMs的推理策略，并提出方法引导其自适应选择最优策略。

Result: 实验表明，单一策略无法持续提高准确性，但适应性策略选择可能提升性能。

Conclusion: 引导LLMs自适应选择推理策略是优化其推理能力的新方向。

Abstract: Human reasoning involves different strategies, each suited to specific
problems. Prior work shows that large language model (LLMs) tend to favor a
single reasoning strategy, potentially limiting their effectiveness in diverse
reasoning challenges. In this work, we investigate whether prompting can
control LLMs reasoning strategies and assess its impact on logical
problem-solving. While our experiments show that no single strategy
consistently improves accuracy, performance could be enhanced if models could
adaptively choose the optimal strategy. We propose methods to guide LLMs in
strategy selection, highlighting new ways to refine their reasoning abilities.

</details>


### [46] [HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong](https://arxiv.org/abs/2507.11502)
*Sirui Han,Junqi Zhu,Ruiyuan Zhang,Yike Guo*

Key words: HKGAI-V1, 大型语言模型, 主权AI, 多语言, 香港, RAG, 伦理对齐

TL;DR: 本文介绍了HKGAI-V1的开发，这是一种为香港定制的、基于DeepSeek架构并融入检索增强生成（RAG）系统的主权大型语言模型，旨在解决香港的多语言和独特社会法律环境。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为香港建立符合本地文化和法律要求的AI基础设施，支持关键领域的AI应用。

Method: 采用DeepSeek架构，通过多参数微调和对齐区域规范，结合RAG系统确保信息准确性。

Result: HKGAI-V1在处理香港特有的文化敏感查询时优于通用模型，并开发了评估本地伦理和法律标准的工具。

Conclusion: 本文不仅提供了一个技术成果，还为开发其他地区定制的AI系统提供了可复制的蓝图。

Abstract: This paper presents the development of HKGAI-V1, a foundational sovereign
large language model (LLM), developed as part of an initiative to establish
value-aligned AI infrastructure specifically tailored for Hong Kong. Addressing
the region's unique multilingual environment (Cantonese, Mandarin, and
English), its distinct socio-legal context under the "one country, two systems"
framework, and specific local cultural and value considerations, the model is
built upon the DeepSeek architecture and systematically aligned with regional
norms through a multifaceted full parameter fine-tuning process. It is further
integrated with a retrieval-augmented generation (RAG) system to ensure timely
and factually grounded information access. The core contribution lies in the
design and implementation of a comprehensive, region-specific AI alignment and
safety framework, demonstrated through two key achievements: 1) The successful
development of HKGAI-V1 itself - which outper-forms general-purpose models in
handling Hong Kong-specific culturally sensitive queries, and embodies a
"governance-embedded" approach to digital sovereignty - empowers Hong Kong to
exercise control over AI applications in critical sectors including public
services, legal systems, and edu-cation. 2) The development of the proprietary
Adversarial HK Value Benchmark, a rigorous tool for evaluating model alignment
with local ethical and legal stand-ards under challenging conditions. By
documenting these achievements, the paper provides not only a technological
artifact but also a replicable blueprint for developing advanced, regionally
focused AI systems deeply rooted in their local identities.

</details>


### [47] [Real-World Summarization: When Evaluation Reaches Its Limits](https://arxiv.org/abs/2507.11508)
*Patrícia Schmidtová,Ondřej Dušek,Saad Mahamood*

Key words: LLM, 摘要, 忠实性, 评估, 人工标注

TL;DR: 论文研究了LLM生成的酒店亮点摘要如何忠实于输入数据，并通过人工评估比较了传统指标、可训练方法和LLM作为评判工具的效果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨LLM生成的摘要是否忠实于输入数据，以及不同评价方法的效果。

Method: 通过人工评估（分类错误评估和片段级标注）比较传统指标、可训练方法和LLM作为评判工具。

Result: 简单指标（如词重叠）与人类判断相关性高（Spearman相关秩0.63），LLM在评估中不可靠，易高估或低估错误。

Conclusion: LLM能生成高质量摘要，但不适合评估；不正确和不可验证的信息风险最高。

Abstract: We examine evaluation of faithfulness to input data in the context of hotel
highlights: brief LLM-generated summaries that capture unique features of
accommodations. Through human evaluation campaigns involving categorical error
assessment and span-level annotation, we compare traditional metrics, trainable
methods, and LLM-as-a-judge approaches. Our findings reveal that simpler
metrics like word overlap correlate surprisingly well with human judgments
(Spearman correlation rank of 0.63), often outperforming more complex methods
when applied to out-of-domain data. We further demonstrate that while LLMs can
generate high-quality highlights, they prove unreliable for evaluation as they
tend to severely under- or over-annotate. Our analysis of real-world business
impacts shows incorrect and non-checkable information pose the greatest risks.
We also highlight challenges in crowdsourced evaluations.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [48] [Tool-to-Tool Matching Analysis Based Difference Score Computation Methods for Semiconductor Manufacturing](https://arxiv.org/abs/2507.10564)
*Sameera Bharadwaja H.,Siddhrath Jandial,Shashank S. Agashe,Rajesh Kumar Reddy Moore,Youngkwan Kim*

Key words: 半导体制造、工具匹配、异构环境、数据方差、模态分析

TL;DR: 提出了一种新的工具间匹配方法，适用于半导体制造设备，克服了传统静态配置和黄金参考方法的局限性，并在异构环境中验证了有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决半导体制造设备中工具间匹配的传统方法依赖静态配置或黄金参考，且在异构环境中效果不佳的问题。

Method: 提出新型TTTM分析流程，通过分析数据方差和模态数量差异来识别不匹配设备，包括单变量和多变量方法。

Result: 最佳单变量方法与数据方差和模态数量的相关系数分别超过0.95和0.5；最佳多变量方法与单变量方法的相关系数超过0.75。

Conclusion: 所提方法在工具间匹配中表现出有效性，尤其是单变量方法；多变量方法对超参数敏感。

Abstract: We consider the problem of tool-to-tool matching (TTTM), also called, chamber
matching in the context of a semiconductor manufacturing equipment. Traditional
TTTM approaches utilize static configuration data or depend on a golden
reference which are difficult to obtain in a commercial manufacturing line.
Further, existing methods do not extend very well to a heterogeneous setting,
where equipment are of different make-and-model, sourced from different
equipment vendors. We propose novel TTTM analysis pipelines to overcome these
issues. We hypothesize that a mismatched equipment would have higher variance
and/or higher number of modes in the data. Our best univariate method achieves
a correlation coefficient >0.95 and >0.5 with the variance and number of modes,
respectively showing that the proposed methods are effective. Also, the best
multivariate method achieves a correlation coefficient >0.75 with the
top-performing univariate methods, showing its effectiveness. Finally, we
analyze the sensitivity of the multivariate algorithms to the algorithm
hyper-parameters.

</details>


### [49] [Enhancing Cross Entropy with a Linearly Adaptive Loss Function for Optimized Classification Performance](https://arxiv.org/abs/2507.10574)
*Jae Wan Shim*

Key words: 交叉熵损失函数,信息论,分类任务,ResNet,CIFAR-100

TL;DR: 提出了一种基于信息论的线性自适应交叉熵损失函数，通过增加一个依赖于真实类预测概率的项，提升分类任务的优化效果，并在CIFAR-100数据集上验证了其优于传统交叉熵损失函数的效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统交叉熵损失函数在处理独热编码分类任务时可能存在优化不足的问题，因此提出一种改进的损失函数以提升分类效果。

Method: 提出线性自适应交叉熵损失函数，增加一个与真实类预测概率相关的项，并基于ResNet模型在CIFAR-100数据集上评估其性能。

Result: 实验表明，新损失函数在分类准确率上优于传统交叉熵损失函数，且保持了相似的效率。

Conclusion: 该损失函数为未来损失函数设计提供了新的研究方向，并展示了潜在的应用前景。

Abstract: We propose the Linearly Adaptive Cross Entropy Loss function. This is a novel
measure derived from the information theory. In comparison to the standard
cross entropy loss function, the proposed one has an additional term that
depends on the predicted probability of the true class. This feature serves to
enhance the optimization process in classification tasks involving one-hot
encoded class labels. The proposed one has been evaluated on a ResNet-based
model using the CIFAR-100 dataset. Preliminary results show that the proposed
one consistently outperforms the standard cross entropy loss function in terms
of classification accuracy. Moreover, the proposed one maintains simplicity,
achieving practically the same efficiency to the traditional cross entropy
loss. These findings suggest that our approach could broaden the scope for
future research into loss function design.

</details>


### [50] [An Adaptive Volatility-based Learning Rate Scheduler](https://arxiv.org/abs/2507.10575)
*Kieran Chai Kai Ren*

Key words: 学习率调度, VolSched, 神经网络训练, CIFAR-100, 泛化性能

TL;DR: VolSched是一种新型自适应学习率调度器，通过动态调整学习率以优化深度神经网络训练。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统和自适应学习率调度器可能导致次优泛化性能，因此需要更有效的解决方案。

Method: VolSched利用长期和短期精度波动比动态调整学习率，以逃离平台期并稳定训练。

Result: 在CIFAR-100数据集上，VolSched显著提升ResNet-18和ResNet-34的top-1准确率，并找到更平坦的解空间。

Conclusion: VolSched通过延长探索阶段和发现更宽最小值为模型带来更好的泛化性能。

Abstract: Effective learning rate (LR) scheduling is crucial for training deep neural
networks. However, popular pre-defined and adaptive schedulers can still lead
to suboptimal generalization. This paper introduces VolSched, a novel adaptive
LR scheduler inspired by the concept of volatility in stochastic processes like
Geometric Brownian Motion to dynamically adjust the learning rate. By
calculating the ratio between long-term and short-term accuracy volatility,
VolSched increases the LR to escape plateaus and decreases it to stabilize
training, allowing the model to explore the loss landscape more effectively. We
evaluate VolSched on the CIFAR-100 dataset against a strong baseline using a
standard augmentation pipeline. When paired with ResNet-18 and ResNet-34, our
scheduler delivers consistent performance gains, improving top-1 accuracy by
1.4 and 1.3 percentage points respectively. Analysis of the loss curves reveals
that VolSched promotes a longer exploration phase. A quantitative analysis of
the Hessian shows that VolSched finds a final solution that is 38% flatter than
the next-best baseline, allowing the model to obtain wider minima and hence
better generalization performance.

</details>


### [51] [Universal Approximation Theorem for a Single-Layer Transformer](https://arxiv.org/abs/2507.10581)
*Esmail Gumaan*

Key words: 深度学习, Transformer, 通用逼近定理, 自注意力, 反向传播

TL;DR: 本文研究了深度学习和Transformer的数学基础，提出了一个新颖的理论结果，证明了单层Transformer可以逼近任何连续序列到序列的映射。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 尽管Transformer在自然语言处理等领域取得了显著成功，但其理论基础仍然有限。本文旨在填补这一理论空白。

Method: 文章回顾了支持深度学习的关键数学概念，详细分析了多头自注意力机制和反向传播算法，并提出了一个Transformer的通用逼近定理。

Result: 证明了单层Transformer可以通过一个自注意力层和ReLU激活的前馈网络，在紧凑域上以任意精度逼近任何连续的序列到序列映射。

Conclusion: 本文的理论成果推动了Transformer模型的理论理解，并有助于缩小理论与实践之间的差距。

Abstract: Deep learning employs multi-layer neural networks trained via the
backpropagation algorithm. This approach has achieved success across many
domains and relies on adaptive gradient methods such as the Adam optimizer.
Sequence modeling evolved from recurrent neural networks to attention-based
models, culminating in the Transformer architecture. Transformers have achieved
state-of-the-art performance in natural language processing (for example, BERT
and GPT-3) and have been applied in computer vision and computational biology.
However, theoretical understanding of these models remains limited. In this
paper, we examine the mathematical foundations of deep learning and
Transformers and present a novel theoretical result. We review key concepts
from linear algebra, probability, and optimization that underpin deep learning,
and we analyze the multi-head self-attention mechanism and the backpropagation
algorithm in detail. Our main contribution is a universal approximation theorem
for Transformers: we prove that a single-layer Transformer, comprising one
self-attention layer followed by a position-wise feed-forward network with ReLU
activation, can approximate any continuous sequence-to-sequence mapping on a
compact domain to arbitrary precision. We provide a formal statement and a
complete proof. Finally, we present case studies that demonstrate the practical
implications of this result. Our findings advance the theoretical understanding
of Transformer models and help bridge the gap between theory and practice.

</details>


### [52] [MH-FSF: A Unified Framework for Overcoming Benchmarking and Reproducibility Limitations in Feature Selection Evaluation](https://arxiv.org/abs/2507.10591)
*Vanderson Rocha,Diego Kreutz,Gabriel Canto,Hendrio Bragança,Eduardo Feitosa*

Key words: 特征选择, MH-FSF框架, Android恶意软件检测, 数据预处理

TL;DR: 论文提出了MH-FSF框架，用于解决特征选择研究中基准测试不足和数据集依赖性强的问题，通过提供17种方法的实现和10个公开数据集的评估，强调了数据预处理和选择标准的重要性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前特征选择研究存在基准测试不足和依赖私有数据集的问题，影响了可重复性和性能。

Method: 开发了MH-FSF框架，实现了17种特征选择方法（11种经典，6种领域专用），并在10个公开Android恶意软件数据集上进行了系统评估。

Result: 结果显示不同数据集上性能存在差异，强调了数据预处理和选择标准的必要性。

Conclusion: MH-FSF框架为特征选择技术的比较提供了统一平台，推动了方法学的一致性和严谨性。

Abstract: Feature selection is vital for building effective predictive models, as it
reduces dimensionality and emphasizes key features. However, current research
often suffers from limited benchmarking and reliance on proprietary datasets.
This severely hinders reproducibility and can negatively impact overall
performance. To address these limitations, we introduce the MH-FSF framework, a
comprehensive, modular, and extensible platform designed to facilitate the
reproduction and implementation of feature selection methods. Developed through
collaborative research, MH-FSF provides implementations of 17 methods (11
classical, 6 domain-specific) and enables systematic evaluation on 10 publicly
available Android malware datasets. Our results reveal performance variations
across both balanced and imbalanced datasets, highlighting the critical need
for data preprocessing and selection criteria that account for these
asymmetries. We demonstrate the importance of a unified platform for comparing
diverse feature selection techniques, fostering methodological consistency and
rigor. By providing this framework, we aim to significantly broaden the
existing literature and pave the way for new research directions in feature
selection, particularly within the context of Android malware detection.

</details>


### [53] [Extension OL-MDISF: Online Learning from Mix-Typed, Drifted, and Incomplete Streaming Features](https://arxiv.org/abs/2507.10594)
*Shengda Zhuo,Di Wu,Yi He,Shuqiang Huang,Xindong Wu*

Key words: 在线学习, 混合特征, 数据漂移, 伪标记, copula 表示

TL;DR: OL-MDISF 是一个针对混合类型、漂移和不完整流特征的在线学习方法，通过潜在 copula 表示、漂移检测和结构感知伪标记解决数据异质性、漂移和标签缺失问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决在线学习中面临的混合特征类型、数据漂移和标签缺失三大挑战。

Method: 构建潜在 copula 表示、通过集成熵和潜在不匹配检测漂移、执行结构感知伪标记。

Result: 在 14 个真实数据集上进行了实验，包括 CER 趋势、消融研究和敏感性分析。

Conclusion: OL-MDISF 为复杂、弱监督流数据的在线学习提供了可复现的基准。

Abstract: Online learning, where feature spaces can change over time, offers a flexible
learning paradigm that has attracted considerable attention. However, it still
faces three significant challenges. First, the heterogeneity of real-world data
streams with mixed feature types presents challenges for traditional parametric
modeling. Second, data stream distributions can shift over time, causing an
abrupt and substantial decline in model performance. Third, it is often
infeasible to label every data instance due to time and cost constraints. To
address these issues, we proposed OL-MDISF (Online Learning from Mix-typed,
Drifted, and Incomplete Streaming Features), which constructs a latent
copula-based representation for heterogeneous features, detects drifts via
ensemble entropy and latent mismatch, and performs structure-aware
pseudo-labeling.
  This companion paper serves as a standalone technical reference to OL-MDISF.
It provides a contextual discussion of related work in mixed-type modeling,
drift adaptation, and weak supervision, as well as a comprehensive set of
experiments across 14 real-world datasets under two types of drift scenarios.
These include CER trends, ablation studies, sensitivity analyses, and temporal
ensemble dynamics. We hope this document offers a reproducible benchmark for
online learning on complex, weakly supervised streaming data.

</details>


### [54] [Divide-Then-Rule: A Cluster-Driven Hierarchical Interpolator for Attribute-Missing Graphs](https://arxiv.org/abs/2507.10595)
*Yaowen Hu,Wenxuan Tu,Yue Liu,Miaomiao Li,Wenpeng Lu,Zhigang Luo,Xinwang Liu,Ping Chen*

Key words: 深度图聚类，属性缺失图，DTRGC，动态聚类感知，层次邻域感知

TL;DR: 该论文提出了一种名为DTRGC的新方法，用于解决属性缺失图的深度图聚类问题，通过分层次处理节点缺失属性并利用聚类信息纠正插补错误，显著提升了聚类性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 属性缺失图的深度图聚类在实际应用中具有重要意义，但现有方法未能充分利用节点邻域信息，导致结果不可靠。

Method: DTRGC方法分三步：动态聚类感知特征传播（DCFP）初始化缺失属性；层次邻域感知插补（HNAI）分层处理节点缺失；跳级表示增强（HRE）丰富节点表示。

Result: 在六个广泛使用的图数据集上，DTRGC显著提升了各种深度图聚类方法在属性缺失图上的性能。

Conclusion: DTRGC通过分层次处理和聚类信息校正，有效提升了属性缺失图的聚类性能。

Abstract: Deep graph clustering (DGC) for attribute-missing graphs is an unsupervised
task aimed at partitioning nodes with incomplete attributes into distinct
clusters. Addressing this challenging issue is vital for practical
applications. However, research in this area remains underexplored. Existing
imputation methods for attribute-missing graphs often fail to account for the
varying amounts of information available across node neighborhoods, leading to
unreliable results, especially for nodes with insufficient known neighborhood.
To address this issue, we propose a novel method named Divide-Then-Rule Graph
Completion (DTRGC). This method first addresses nodes with sufficient known
neighborhood information and treats the imputed results as new knowledge to
iteratively impute more challenging nodes, while leveraging clustering
information to correct imputation errors. Specifically, Dynamic Cluster-Aware
Feature Propagation (DCFP) initializes missing node attributes by adjusting
propagation weights based on the clustering structure. Subsequently,
Hierarchical Neighborhood-aware Imputation (HNAI) categorizes attribute-missing
nodes into three groups based on the completeness of their neighborhood
attributes. The imputation is performed hierarchically, prioritizing the groups
with nodes that have the most available neighborhood information. The cluster
structure is then used to refine the imputation and correct potential errors.
Finally, Hop-wise Representation Enhancement (HRE) integrates information
across multiple hops, thereby enriching the expressiveness of node
representations. Experimental results on six widely used graph datasets show
that DTRGC significantly improves the clustering performance of various DGC
methods under attribute-missing graphs.

</details>


### [55] [RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services](https://arxiv.org/abs/2507.10605)
*Fei Zhao,Chonggang Lu,Yue Wang,Zheyong Xie,Ziyan Liu,Haofu Qian,JianZhao Huang,Fangcheng Shi,Zijie Meng,Hongcheng Guo,Mingqian He,Xinze Lyu,Yiming Lu,Ziyang Xiang,Zheyu Ye,Chengqiang Lu,Zhe Xu,Yi Wu,Yao Hu,Yan Gao,Jun Fan,Xiaolong Jiang,Weiting Liu,Boyang Wang,Shaosheng Cao*

Key words: 社交网络服务,大型语言模型,多任务学习,内容管理,性能提升

TL;DR: RedOne是一个针对社交网络服务(SNS)的领域特定大型语言模型，通过三阶段训练策略显著提升了多任务性能，并在实际应用中表现出色。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 社交网络服务快速发展对内容管理和交互质量提出了挑战，现有研究局限于单任务且无法灵活适应多样场景。

Method: 采用三阶段训练策略（持续预训练、监督微调和偏好优化）并使用大规模真实数据集开发RedOne。

Result: RedOne在8项SNS任务中平均提升14.02%，双语评估提升7.56%，有害内容检测曝光率降低11.23%，帖子搜索点击率提高14.95%。

Conclusion: RedOne是SNS领域强大的领域特定LLM，展示了卓越的任务泛化能力和实际应用潜力。

Abstract: As a primary medium for modern information dissemination, social networking
services (SNS) have experienced rapid growth, which has proposed significant
challenges for platform content management and interaction quality improvement.
Recently, the development of large language models (LLMs) has offered potential
solutions but existing studies focus on isolated tasks, which not only
encounter diminishing benefit from the data scaling within individual scenarios
but also fail to flexibly adapt to diverse real-world context. To address these
challenges, we introduce RedOne, a domain-specific LLM designed to break the
performance bottleneck of single-task baselines and establish a comprehensive
foundation for the SNS. RedOne was developed through a three-stage training
strategy consisting of continue pretraining, supervised fine-tuning, and
preference optimization, using a large-scale real-world dataset. Through
extensive experiments, RedOne maintains strong general capabilities, and
achieves an average improvement up to 14.02% across 8 major SNS tasks and 7.56%
in SNS bilingual evaluation benchmark, compared with base models. Furthermore,
through online testing, RedOne reduced the exposure rate in harmful content
detection by 11.23% and improved the click page rate in post-view search by
14.95% compared with single-tasks finetuned baseline models. These results
establish RedOne as a robust domain-specific LLM for SNS, demonstrating
excellent generalization across various tasks and promising applicability in
real-world scenarios.

</details>


### [56] [DALI-PD: Diffusion-based Synthetic Layout Heatmap Generation for ML in Physical Design](https://arxiv.org/abs/2507.10606)
*Bing-Yue Wu,Vidya A. Chhabria*

Key words: 机器学习,物理设计,扩散模型,布局热图,数据处理

TL;DR: 论文提出DALI-PD框架，利用扩散模型快速生成合成布局热图，以解决物理设计中机器学习模型训练数据不足的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决物理设计（PD）中机器学习模型因缺乏高质量、大规模训练数据而泛化能力受限的问题。

Method: 采用扩散模型快速生成多样化的布局热图，包括电源、IR压降、拥塞、宏单元布局和单元密度图。

Result: 成功生成了包含20,000多种不同宏单元数量和布局配置的数据集，显著提升了ML在IR压降和拥塞预测等任务上的准确性。

Conclusion: DALI-PD框架为物理设计中的ML研究提供了高效的数据生成工具，解决了数据稀缺的问题。

Abstract: Machine learning (ML) has demonstrated significant promise in various
physical design (PD) tasks. However, model generalizability remains limited by
the availability of high-quality, large-scale training datasets. Creating such
datasets is often computationally expensive and constrained by IP. While very
few public datasets are available, they are typically static, slow to generate,
and require frequent updates. To address these limitations, we present DALI-PD,
a scalable framework for generating synthetic layout heatmaps to accelerate ML
in PD research. DALI-PD uses a diffusion model to generate diverse layout
heatmaps via fast inference in seconds. The heatmaps include power, IR drop,
congestion, macro placement, and cell density maps. Using DALI-PD, we created a
dataset comprising over 20,000 layout configurations with varying macro counts
and placements. These heatmaps closely resemble real layouts and improve ML
accuracy on downstream ML tasks such as IR drop or congestion prediction.

</details>


### [57] [A Feed-Forward Artificial Intelligence Pipeline for Sustainable Desalination under Climate Uncertainties: UAE Insights](https://arxiv.org/abs/2507.10609)
*Obumneme Nwafor,Chioma Nwafor,Amro Zakaria,Nkechi Nwankwo*

Key words: 海水淡化, 预测建模, 气候变化, 智能控制, 可持续性

TL;DR: 阿联酋依赖海水淡化满足90%以上的饮用水需求，但该过程能耗高且面临气候不确定性挑战。研究提出一种两阶段预测模型和基于规则的智能控制系统，以提升淡化效率并减少环境影响。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 阿联酋严重依赖高能耗的海水淡化技术，导致大量CO2排放。气候因素（如海水温度上升和悬浮颗粒物）进一步影响系统效率，亟需可持续解决方案。

Method: 研究开发了两阶段预测模型：第一阶段预测悬浮颗粒物（AOD），第二阶段预测淡化效率损失。结合SHAP分析关键驱动因素，并提出基于规则的智能控制系统。

Result: 模型预测精度达98%，并通过交互式仪表盘提供决策支持，实现气候适应规划。

Conclusion: 该研究为海水淡化系统提供了高效的预测与控制工具，有助于降低能耗和排放，提升可持续性。

Abstract: The United Arab Emirates (UAE) relies heavily on seawater desalination to
meet over 90% of its drinking water needs. Desalination processes are highly
energy intensive and account for approximately 15% of the UAE's electricity
consumption, contributing to over 22% of the country's energy-related CO2
emissions. Moreover, these processes face significant sustainability challenges
in the face of climate uncertainties such as rising seawater temperatures,
salinity, and aerosol optical depth (AOD). AOD greatly affects the operational
and economic performance of solar-powered desalination systems through
photovoltaic soiling, membrane fouling, and water turbidity cycles.
  This study proposes a novel pipelined two-stage predictive modelling
architecture: the first stage forecasts AOD using satellite-derived time series
and meteorological data; the second stage uses the predicted AOD and other
meteorological factors to predict desalination performance efficiency losses.
The framework achieved 98% accuracy, and SHAP (SHapley Additive exPlanations)
was used to reveal key drivers of system degradation. Furthermore, this study
proposes a dust-aware rule-based control logic for desalination systems based
on predicted values of AOD and solar efficiency. This control logic is used to
adjust the desalination plant feed water pressure, adapt maintenance
scheduling, and regulate energy source switching.
  To enhance the practical utility of the research findings, the predictive
models and rule-based controls were packaged into an interactive dashboard for
scenario and predictive analytics. This provides a management decision-support
system for climate-adaptive planning.

</details>


### [58] [FedGSCA: Medical Federated Learning with Global Sample Selector and Client Adaptive Adjuster under Label Noise](https://arxiv.org/abs/2507.10611)
*Mengwen Ye,Yingzi Huangfu,Shujian Gao,Wei Ren,Weifan Liu,Zekuan Yu*

Key words: 联邦学习, 医疗图像分类, 标签噪声, 噪声异质性, 数据不平衡

TL;DR: FedGSCA是一个针对联邦学习中医疗数据标签噪声的新框架，通过全局样本选择器和客户端自适应调整机制，提升了模型在噪声环境下的鲁棒性和稳定性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决联邦学习中医医疗数据标签噪声导致的训练不稳定和性能下降问题，尤其是噪声异质性和数据不平衡的挑战。

Method: 提出FedGSCA框架，包括全局样本选择器（聚合噪声知识）和客户端自适应调整机制（结合伪标签生成和鲁棒损失函数）。

Result: 在真实和合成医疗数据集上测试，FedGSCA在极端和异质噪声场景下优于现有方法，显著提升了模型稳定性。

Conclusion: FedGSCA适用于实际医疗联邦学习场景，能有效处理复杂噪声并提升模型性能。

Abstract: Federated Learning (FL) emerged as a solution for collaborative medical image
classification while preserving data privacy. However, label noise, which
arises from inter-institutional data variability, can cause training
instability and degrade model performance. Existing FL methods struggle with
noise heterogeneity and the imbalance in medical data. Motivated by these
challenges, we propose FedGSCA, a novel framework for enhancing robustness in
noisy medical FL. FedGSCA introduces a Global Sample Selector that aggregates
noise knowledge from all clients, effectively addressing noise heterogeneity
and improving global model stability. Furthermore, we develop a Client Adaptive
Adjustment (CAA) mechanism that combines adaptive threshold pseudo-label
generation and Robust Credal Labeling Loss. CAA dynamically adjusts to class
distributions, ensuring the inclusion of minority samples and carefully
managing noisy labels by considering multiple plausible labels. This dual
approach mitigates the impact of noisy data and prevents overfitting during
local training, which improves the generalizability of the model. We evaluate
FedGSCA on one real-world colon slides dataset and two synthetic medical
datasets under various noise conditions, including symmetric, asymmetric,
extreme, and heterogeneous types. The results show that FedGSCA outperforms the
state-of-the-art methods, excelling in extreme and heterogeneous noise
scenarios. Moreover, FedGSCA demonstrates significant advantages in improving
model stability and handling complex noise, making it well-suited for
real-world medical federated learning scenarios.

</details>


### [59] [Sub-Scaling Laws: On the Role of Data Density and Training Strategies in LLMs](https://arxiv.org/abs/2507.10613)
*Zhengyu Chen,Siqi Wang,Teng Xiao,Yudong Wang,Shiqi Chen,Xunliang Cai,Junxian He,Jingang Wang*

Key words: 缩放定律, 子缩放, 数据质量, 资源分配, 语言模型

TL;DR: 本文重新审视自然语言处理中的传统缩放定律，发现大规模语言模型存在性能提升减速的现象（即子缩放），并提出数据质量和分配策略是关键因素。通过分析400多个模型，提出新的子缩放定律。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统缩放定律认为增加模型规模和数据量可提升性能，但近期研究发现大规模模型中性能提升减速。本文旨在探究数据质量和训练策略对子缩放现象的影响。

Method: 通过对400多个模型的实证分析，研究了数据密度和资源分配对模型性能的影响。

Result: 发现高数据密度和次优资源分配是导致子缩放的主要原因，提出了一种能更好预测子缩放性能的新定律。

Conclusion: 数据质量和多样性对模型性能至关重要，新的子缩放定律为优化训练提供了指导。

Abstract: Traditional scaling laws in natural language processing suggest that
increasing model size and training data enhances performance. However, recent
studies reveal deviations, particularly in large language models, where
performance improvements decelerate, which is a phenomenon known as
sub-scaling. This paper revisits these scaling laws by examining the impact of
data quality and training strategies on model performance. Through extensive
empirical analysis of over 400 models, we identify high data density and
non-optimal resource allocation as key factors contributing to sub-scaling.
High data density leads to diminishing returns due to redundant information,
while optimal resource allocation is crucial for sustained performance
improvements. We propose a sub-optimal scaling law that better predicts
performance in sub-scaling regimes, highlighting the importance of data quality
and diversity.

</details>


### [60] [Fine-tuning Large Language Model for Automated Algorithm Design](https://arxiv.org/abs/2507.10614)
*Fei Liu,Rui Zhang,Xi Lin,Zhichao Lu,Qingfu Zhang*

Key words: 大语言模型, 算法设计, 微调, 多样性感知采样, 直接偏好优化

TL;DR: 论文探讨了为大语言模型（LLMs）进行算法设计定制化微调的必要性和方法，提出了一种多样性感知的采样策略和直接偏好优化方法，实验表明微调后的LLMs在算法设计任务中表现优于通用模型。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法主要依赖通用训练的LLMs，未针对算法设计任务进行优化，本文旨在探索定制化LLMs的需求与实现路径。

Method: 引入Diversity-Aware Rank（DAR）采样策略平衡训练数据的多样性与质量，并利用直接偏好优化对齐LLMs输出与任务目标。

Result: 实验显示，微调后的LLMs在多个算法设计任务中表现优于通用模型，同时展现了良好的泛化能力。

Conclusion: 任务特定微调对LLMs在算法设计中的应用具有重要意义，为未来研究开辟了新方向。

Abstract: The integration of large language models (LLMs) into automated algorithm
design has shown promising potential. A prevalent approach embeds LLMs within
search routines to iteratively generate and refine candidate algorithms.
However, most existing methods rely on off-the-shelf LLMs trained for general
coding tasks,leaving a key question open: Do we need LLMs specifically tailored
for algorithm design? If so, how can such LLMs be effectively obtained and how
well can they generalize across different algorithm design tasks? In this
paper, we take a first step toward answering these questions by exploring
fine-tuning of LLMs for algorithm design. We introduce a Diversity-Aware Rank
based (DAR) sampling strategy to balance training data diversity and quality,
then we leverage direct preference optimization to efficiently align LLM
outputs with task objectives. Our experiments, conducted on
Llama-3.2-1B-Instruct and Llama- 3.1-8B-Instruct, span three distinct algorithm
design tasks. Results suggest that finetuned LLMs can significantly outperform
their off-the-shelf counterparts with the smaller Llama-3.2-1B-Instruct and
match the larger Llama-3.1-8B-Instruct on the admissible set problem. Moreover,
we observe promising generalization: LLMs finetuned on specific algorithm
design tasks also improve performance on related tasks with varying settings.
These findings highlight the value of task-specific adaptation for LLMs in
algorithm design and open new avenues for future research.

</details>


### [61] [Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them](https://arxiv.org/abs/2507.10616)
*Neel Rajani,Aryo Pradipta Gema,Seraphina Goldfarb-Tarrant,Ivan Titov*

Key words: 大语言模型、强化学习、监督微调、数学推理、性能分析

TL;DR: 对比分析强化学习（RL）和监督微调（SFT）在数学问题和知识密集型任务上的表现，发现RL在数学领域有轻微提升，而SFT表现更明显但可能导致其他领域性能下降。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究RL和SFT在训练大语言模型（LLM）时的影响，揭示两种方法的不同动态及其对模型性能的影响。

Method: 在相同数学问题和超参数下比较RL和SFT的表现，分析模型参数变化，并尝试冻结部分模型以缓解性能下降。

Result: RL在数学领域有微小提升，SFT在数学表现更显著但可能导致知识密集型任务性能下降；参数冻结结果不一致。

Conclusion: RL可能放大现有能力，而SFT则可能用新技能替代旧技能。

Abstract: Training large language models (LLMs) for reasoning via maths and code
datasets has become a major new focus in LLM post-training. Two particularly
popular approaches are reinforcement learning (RL) and supervised fine-tuning
(SFT), but their training dynamics are poorly understood. We present a
comparative analysis of RL and SFT on the same maths problems with the same
model and similar hyperparameters. We find that RL yields minor in-domain gains
on maths and slight degradation on knowledge-intensive benchmarks like MMLU,
while both trends are more pronounced in SFT. We also analyse model parameters
across checkpoints, observing that both algorithms modify query and key weights
the most. Meanwhile, SFT exhibits greater updates and also affects mid-layer
MLPs more, leading us to hypothesise that this may have caused the
out-of-domain degradation. We therefore investigate whether freezing parts of
the model during training can mitigate the reduced performance on
knowledge-intensive benchmarks. However, our results are inconclusive, with
benefits on GPQA:Diamond and degradation on other benchmarks. Taken together,
our observations provide a preliminary indication for why RL amplifies existing
capabilities, while SFT replaces old skills with new ones.

</details>


### [62] [Compute Requirements for Algorithmic Innovation in Frontier AI Models](https://arxiv.org/abs/2507.10618)
*Peter Barnett*

Key words: 算法创新、计算资源、预训练、大语言模型、FLOP

TL;DR: 该论文实证研究了开发算法创新所需的总计算资源，通过对36种预训练算法创新的分析，发现创新所需的资源随时间翻倍。计算限制对算法进步的影响较小。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究算法创新在大语言模型预训练中所需的计算资源，以及计算限制对创新的潜在影响。

Method: 收集并分析了36种预训练算法创新的计算资源使用情况，包括总FLOP和硬件FLOP/s。通过模拟计算限制来评估其对创新的影响。

Result: 创新所需的计算资源每年翻倍，但计算限制（如限制总计算量或硬件容量）仍允许半数创新实现。

Conclusion: 计算限制不太可能显著减缓人工智能算法进步。

Abstract: Algorithmic innovation in the pretraining of large language models has driven
a massive reduction in the total compute required to reach a given level of
capability. In this paper we empirically investigate the compute requirements
for developing algorithmic innovations. We catalog 36 pre-training algorithmic
innovations used in Llama 3 and DeepSeek-V3. For each innovation we estimate
both the total FLOP used in development and the FLOP/s of the hardware
utilized. Innovations using significant resources double in their requirements
each year. We then use this dataset to investigate the effect of compute caps
on innovation. Our analysis suggests that compute caps alone are unlikely to
dramatically slow AI algorithmic progress. Even stringent compute caps -- such
as capping total operations to the compute used to train GPT-2 or capping
hardware capacity to 8 H100 GPUs -- could still have allowed for half of the
cataloged innovations.

</details>


### [63] [Meta-Reinforcement Learning for Fast and Data-Efficient Spectrum Allocation in Dynamic Wireless Networks](https://arxiv.org/abs/2507.10619)
*Oluwaseyi Giwa,Tobi Awodunmila,Muhammad Ahmed Mohsin,Ahsan Bilal,Muhammad Ali Jamshed*

Key words: 5G/6G, 动态频谱分配, 元学习, 深度强化学习, IAB

TL;DR: 提出了一种基于元学习的框架，用于解决5G/6G网络中动态频谱分配的高样本复杂性和安全问题，通过快速适应新场景提升网络性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统深度强化学习在5G/6G频谱分配中因高样本复杂性和探索风险而不适用，需要一种更高效、安全的方法。

Method: 提出了三种元学习架构（MAML、RNN和注意力增强RNN），并在仿真IAB环境中与PPO基线进行对比评估。

Result: 注意力元学习代理峰值吞吐量达48 Mbps，远高于PPO的10 Mbps，且SINR和延迟违规减少50%以上，公平性指数为0.7。

Conclusion: 元学习是复杂无线系统中智能控制的有效且安全的选择。

Abstract: The dynamic allocation of spectrum in 5G / 6G networks is critical to
efficient resource utilization. However, applying traditional deep
reinforcement learning (DRL) is often infeasible due to its immense sample
complexity and the safety risks associated with unguided exploration, which can
cause severe network interference. To address these challenges, we propose a
meta-learning framework that enables agents to learn a robust initial policy
and rapidly adapt to new wireless scenarios with minimal data. We implement
three meta-learning architectures, model-agnostic meta-learning (MAML),
recurrent neural network (RNN), and an attention-enhanced RNN, and evaluate
them against a non-meta-learning DRL algorithm, proximal policy optimization
(PPO) baseline, in a simulated dynamic integrated access/backhaul (IAB)
environment. Our results show a clear performance gap. The attention-based
meta-learning agent reaches a peak mean network throughput of 48 Mbps, while
the PPO baseline decreased drastically to 10 Mbps. Furthermore, our method
reduces SINR and latency violations by more than 50% compared to PPO. It also
shows quick adaptation, with a fairness index 0.7, showing better resource
allocation. This work proves that meta-learning is a very effective and safer
option for intelligent control in complex wireless systems.

</details>


### [64] [LLMs Meet Cross-Modal Time Series Analytics: Overview and Directions](https://arxiv.org/abs/2507.10620)
*Chenxi Liu,Hao Miao,Cheng Long,Yan Zhao,Ziyue Li,Panos Kalnis*

Key words: 大语言模型, 时间序列分析, 跨模态学习, 转换, 对齐, 融合

TL;DR: LLMs在时间序列分析中展现出潜力，但存在跨模态差异。本教程综述了基于LLM的跨模态时间序列分析方法，分类为转换、对齐和融合三类，并讨论了其应用和开放挑战。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 利用LLMs的大参数和时序与文本数据的共享序列特性，解决时间序列与文本数据之间的跨模态差距问题。

Method: 提出分类法，将现有方法分为转换、对齐和融合三类，并探讨其在下游任务中的应用。

Result: 总结了当前进展、方法及未来研究方向，旨在扩展LLMs在跨模态时间序列分析中的实际应用。

Conclusion: LLMs在跨模态时间序列分析中具有潜力，但仍需平衡有效性和效率。

Abstract: Large Language Models (LLMs) have emerged as a promising paradigm for time
series analytics, leveraging their massive parameters and the shared sequential
nature of textual and time series data. However, a cross-modality gap exists
between time series and textual data, as LLMs are pre-trained on textual
corpora and are not inherently optimized for time series. In this tutorial, we
provide an up-to-date overview of LLM-based cross-modal time series analytics.
We introduce a taxonomy that classifies existing approaches into three groups
based on cross-modal modeling strategies, e.g., conversion, alignment, and
fusion, and then discuss their applications across a range of downstream tasks.
In addition, we summarize several open challenges. This tutorial aims to expand
the practical application of LLMs in solving real-world problems in cross-modal
time series analytics while balancing effectiveness and efficiency.
Participants will gain a thorough understanding of current advancements,
methodologies, and future research directions in cross-modal time series
analytics.

</details>


### [65] [Flows and Diffusions on the Neural Manifold](https://arxiv.org/abs/2507.10623)
*Daniel Saragih,Deyu Cao,Tejas Balaji*

Key words: 扩散模型,流模型,权重空间学习,梯度流匹配,协变量偏移

TL;DR: 该论文将扩散和流模型扩展到权重空间学习，通过优化动态引入结构先验，统一了轨迹推断技术，并提供理论框架，实验证明其优于基线。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究动机是将扩散和流模型在图像、视频和自然语言领域的成功扩展到权重空间学习，以提高权重生成和优化的效果。

Method: 方法包括梯度流匹配框架、奖励微调、潜在权重表示的自动编码器、任务特定数据条件以及使用Kaiming均匀等源分布。

Result: 实验结果展示了生成权重分布与基线匹配或超越的能力，下游训练初始化改进，以及安全关键系统中的协变量偏移检测性能提升。

Conclusion: 结论是该框架为权重空间学习提供了有效的理论和技术支持，并在实际应用中表现出优势。

Abstract: Diffusion and flow-based generative models have achieved remarkable success
in domains such as image synthesis, video generation, and natural language
modeling. In this work, we extend these advances to weight space learning by
leveraging recent techniques to incorporate structural priors derived from
optimization dynamics. Central to our approach is modeling the trajectory
induced by gradient descent as a trajectory inference problem. We unify several
trajectory inference techniques under the framework of gradient flow matching,
providing a theoretical framework for treating optimization paths as inductive
bias. We further explore architectural and algorithmic choices, including
reward fine-tuning by adjoint matching, the use of autoencoders for latent
weight representation, conditioning on task-specific context data, and adopting
informative source distributions such as Kaiming uniform. Experiments
demonstrate that our method matches or surpasses baselines in generating
in-distribution weights, improves initialization for downstream training, and
supports fine-tuning to enhance performance. Finally, we illustrate a practical
application in safety-critical systems: detecting harmful covariate shifts,
where our method outperforms the closest comparable baseline.

</details>


### [66] [Player-Team Heterogeneous Interaction Graph Transformer for Soccer Outcome Prediction](https://arxiv.org/abs/2507.10626)
*Lintao Wang,Shiwen Xu,Michael Horton,Joachim Gudmundsson,Zhiyong Wang*

Key words: 足球预测, 异质交互, 图神经网络, Transformer, 球员表现

TL;DR: 提出了一种新型图增强的深度学习模型HIGFormer，用于预测足球比赛结果，通过多级交互框架捕捉球员和团队动态，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法忽视了球员和团队之间的异质交互对比赛结果预测的重要性。

Method: 结合局部图卷积和全局图增强Transformer，构建球员交互网络、团队交互网络和比赛比较Transformer。

Result: 在WyScout数据集上表现优于现有方法，并为球员表现评估提供了新视角。

Conclusion: HIGFormer能有效预测比赛结果，并支持球员评估和团队策略分析。

Abstract: Predicting soccer match outcomes is a challenging task due to the inherently
unpredictable nature of the game and the numerous dynamic factors influencing
results. While it conventionally relies on meticulous feature engineering, deep
learning techniques have recently shown a great promise in learning effective
player and team representations directly for soccer outcome prediction.
However, existing methods often overlook the heterogeneous nature of
interactions among players and teams, which is crucial for accurately modeling
match dynamics. To address this gap, we propose HIGFormer (Heterogeneous
Interaction Graph Transformer), a novel graph-augmented transformer-based deep
learning model for soccer outcome prediction. HIGFormer introduces a
multi-level interaction framework that captures both fine-grained player
dynamics and high-level team interactions. Specifically, it comprises (1) a
Player Interaction Network, which encodes player performance through
heterogeneous interaction graphs, combining local graph convolutions with a
global graph-augmented transformer; (2) a Team Interaction Network, which
constructs interaction graphs from a team-to-team perspective to model
historical match relationships; and (3) a Match Comparison Transformer, which
jointly analyzes both team and player-level information to predict match
outcomes. Extensive experiments on the WyScout Open Access Dataset, a
large-scale real-world soccer dataset, demonstrate that HIGFormer significantly
outperforms existing methods in prediction accuracy. Furthermore, we provide
valuable insights into leveraging our model for player performance evaluation,
offering a new perspective on talent scouting and team strategy analysis.

</details>


### [67] [GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning](https://arxiv.org/abs/2507.10628)
*Ziru Liu,Cheng Gong,Xinyu Fu,Yaofang Liu,Ran Chen,Shoubo Hu,Suiyun Zhang,Rui Liu,Qingfu Zhang,Dandan Tu*

Key words: 强化学习, 大型语言模型, GHPO, 任务难度, 训练稳定性

TL;DR: 论文提出了一种名为GHPO的新方法，通过动态调整任务难度和结合模仿学习与强化学习，解决了强化学习在大型语言模型训练中的不稳定性和效率低下的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有基于策略的强化学习方法在训练大型语言模型时存在不稳定性和效率低下的问题，尤其是任务难度与模型能力不匹配导致学习停滞。

Method: 提出了一种名为GHPO的难度感知强化学习框架，通过自适应提示调整动态校准任务难度，结合模仿学习和强化学习。

Result: 在六个数学基准测试中，GHPO平均性能提升了约5%，显著优于现有方法，同时提升了训练稳定性和推理性能。

Conclusion: GHPO为开发强大且稳健的推理模型提供了可扩展且高效的解决方案。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a powerful paradigm for facilitating the self-improvement of large language
models (LLMs), particularly in the domain of complex reasoning tasks. However,
prevailing on-policy RL methods often contend with significant training
instability and inefficiency. This is primarily due to a capacity-difficulty
mismatch, where the complexity of training data frequently outpaces the model's
current capabilities, leading to critically sparse reward signals and stalled
learning progress. This challenge is particularly acute for smaller, more
resource-efficient LLMs. To overcome this, we introduce the Guided Hybrid
Policy Optimization (GHPO), a novel difficulty-aware reinforcement learning
framework. GHPO dynamically calibrates task difficulty by employing adaptive
prompt refinement to provide targeted guidance. This unique approach adaptively
balances direct imitation learning for problems currently beyond the model's
reach with exploration-based reinforcement learning for more manageable tasks,
effectively creating a smooth and optimized learning curriculum. Extensive
experiments demonstrate that GHPO achieves an average performance gain of
approximately 5% across six challenging mathematics benchmarks, consistently
outperforming strong on-policy reinforcement learning and curriculum learning
baselines. Further analysis confirms that our framework significantly enhances
both training stability and final reasoning performance, thus offering a
scalable and efficient solution for developing powerful and robust reasoning
models.

</details>


### [68] [Scalable Unsupervised Segmentation via Random Fourier Feature-based Gaussian Process](https://arxiv.org/abs/2507.10632)
*Issei Saito,Masatoshi Nagano,Tomoaki Nakamura,Daichi Mochihashi,Koki Mimura*

Key words: 随机傅里叶特征, 高斯过程隐半马尔可夫模型, 时间序列分割, 计算效率

TL;DR: 提出了一种基于随机傅里叶特征（RFF）的快速无监督时间序列分割方法RFF-GP-HSMM，显著降低了高斯过程隐半马尔可夫模型（GP-HSMM）的计算成本。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: GP-HSMM在处理大规模时间序列数据时，由于需要计算N×N核矩阵的逆，导致计算成本过高。

Method: 通过随机傅里叶特征（RFF）近似高斯过程，将其转化为线性回归问题，避免了核矩阵的逆运算。

Result: 实验表明，该方法在卡内基梅隆大学的运动捕捉数据集上，分割性能与传统方法相当，但速度提升了约278倍。

Conclusion: RFF-GP-HSMM是一种高效且性能优越的时间序列分割方法。

Abstract: In this paper, we propose RFF-GP-HSMM, a fast unsupervised time-series
segmentation method that incorporates random Fourier features (RFF) to address
the high computational cost of the Gaussian process hidden semi-Markov model
(GP-HSMM). GP-HSMM models time-series data using Gaussian processes, requiring
inversion of an N times N kernel matrix during training, where N is the number
of data points. As the scale of the data increases, matrix inversion incurs a
significant computational cost. To address this, the proposed method
approximates the Gaussian process with linear regression using RFF, preserving
expressive power while eliminating the need for inversion of the kernel matrix.
Experiments on the Carnegie Mellon University (CMU) motion-capture dataset
demonstrate that the proposed method achieves segmentation performance
comparable to that of conventional methods, with approximately 278 times faster
segmentation on time-series data comprising 39,200 frames.

</details>


### [69] [GeoHopNet: Hopfield-Augmented Sparse Spatial Attention for Dynamic UAV Site Location Problem](https://arxiv.org/abs/2507.10636)
*Jianing Zhi,Xinghua Li,Zidong Chen*

Key words: 无人机选址、稀疏注意力、Hopfield网络、计算复杂度

TL;DR: GeoHopNet是一种基于Hopfield增强的稀疏空间注意力网络，用于解决无人机动态选址问题，显著提升了计算效率和解决方案质量。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 城市低空无人机经济的快速发展对无人机降落点和供应站的动态选址提出了新挑战，传统深度强化学习方法在处理大规模城市级选址问题时面临计算复杂度瓶颈。

Method: GeoHopNet通过四种核心创新：距离偏置多头注意力机制、K近邻稀疏注意力、现代Hopfield外部记忆模块和记忆正则化策略，降低了计算复杂度并提升性能。

Result: 在1,000节点的实例中，GeoHopNet在0.1秒内找到高质量解（最优性差距0.22%），较ADNet基线在100节点实例上提升22.2%的解质量且速度提升1.8倍。

Conclusion: GeoHopNet扩展了可解决问题的规模边界，为大规模无人机动态选址问题提供了高效的解决方案。

Abstract: The rapid development of urban low-altitude unmanned aerial vehicle (UAV)
economy poses new challenges for dynamic site selection of UAV landing points
and supply stations. Traditional deep reinforcement learning methods face
computational complexity bottlenecks, particularly with standard attention
mechanisms, when handling large-scale urban-level location problems. This paper
proposes GeoHopNet, a Hopfield-augmented sparse spatial attention network
specifically designed for dynamic UAV site location problems. Our approach
introduces four core innovations: (1) distance-biased multi-head attention
mechanism that explicitly encodes spatial geometric information; (2) K-nearest
neighbor sparse attention that reduces computational complexity from $O(N^2)$
to $O(NK)$; (3) a modern Hopfield external memory module; and (4) a memory
regularization strategy. Experimental results demonstrate that GeoHopNet
extends the boundary of solvable problem sizes. For large-scale instances with
1,000 nodes, where standard attention models become prohibitively slow (over 3
seconds per instance) and traditional solvers fail, GeoHopNet finds
high-quality solutions (0.22\% optimality gap) in under 0.1 seconds. Compared
to the state-of-the-art ADNet baseline on 100-node instances, our method
improves solution quality by 22.2\% and is 1.8$\times$ faster.

</details>


### [70] [A Simple Baseline for Stable and Plastic Neural Networks](https://arxiv.org/abs/2507.10637)
*É. Künzel,A. Jaziri,V. Ramesh*

Key words: 持续学习,计算机视觉,RDBP,ReLUDown,Decreasing Backpropagation

TL;DR: RDBP是一种低开销的持续学习基线方法，结合了ReLUDown和Decreasing Backpropagation两种机制，既能保持特征敏感性，又能防止神经元休眠，同时减少计算成本。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决现有持续学习方法在可塑性和稳定性之间平衡不足的问题。

Method: 结合ReLUDown（轻量级激活修改）和Decreasing Backpropagation（梯度调度方案）。

Result: 在Continual ImageNet基准测试中表现优异，优于现有方法且计算成本更低。

Conclusion: RDBP为持续学习提供了实用解决方案和清晰的基准。

Abstract: Continual learning in computer vision requires that models adapt to a
continuous stream of tasks without forgetting prior knowledge, yet existing
approaches often tip the balance heavily toward either plasticity or stability.
We introduce RDBP, a simple, low-overhead baseline that unites two
complementary mechanisms: ReLUDown, a lightweight activation modification that
preserves feature sensitivity while preventing neuron dormancy, and Decreasing
Backpropagation, a biologically inspired gradient-scheduling scheme that
progressively shields early layers from catastrophic updates. Evaluated on the
Continual ImageNet benchmark, RDBP matches or exceeds the plasticity and
stability of state-of-the-art methods while reducing computational cost. RDBP
thus provides both a practical solution for real-world continual learning and a
clear benchmark against which future continual learning strategies can be
measured.

</details>


### [71] [ZClassifier: Temperature Tuning and Manifold Approximation via KL Divergence on Logit Space](https://arxiv.org/abs/2507.10638)
*Shim Soon Yong*

Key words: ZClassifier, 高斯分布, KL散度, 不确定性校准, 分类器引导生成

TL;DR: ZClassifier框架用高斯分布替代传统确定性逻辑，通过KL散度同时解决温度缩放和流形逼近问题，统一了不确定性校准和潜在控制。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统分类器的确定性逻辑难以同时处理不确定性校准和潜在控制，因此提出ZClassifier框架。

Method: 使用对角高斯分布的逻辑，通过最小化预测高斯分布与单位各向同性高斯之间的KL散度。

Result: 在CIFAR-10和CIFAR-100上，ZClassifier在鲁棒性、校准和潜在分离方面优于softmax分类器。

Conclusion: ZClassifier通过高斯语义势能有效支持分类器引导生成，提供统一的概率解释。

Abstract: We introduce a novel classification framework, ZClassifier, that replaces
conventional deterministic logits with diagonal Gaussian-distributed logits.
Our method simultaneously addresses temperature scaling and manifold
approximation by minimizing the Kullback-Leibler (KL) divergence between the
predicted Gaussian distributions and a unit isotropic Gaussian. This unifies
uncertainty calibration and latent control in a principled probabilistic
manner, enabling a natural interpretation of class confidence and geometric
consistency. Experiments on CIFAR-10 and CIFAR-100 show that ZClassifier
improves over softmax classifiers in robustness, calibration, and latent
separation. We also demonstrate its effectiveness for classifier-guided
generation by interpreting logits as Gaussian semantic potentials.

</details>


### [72] [First-of-its-kind AI model for bioacoustic detection using a lightweight associative memory Hopfield neural network](https://arxiv.org/abs/2507.10642)
*Andrew Gascoyne,Wendy Lomas*

Key words: 生物声学, Hopfield神经网络, 可持续AI, 轻量模型, 可解释AI

TL;DR: 论文提出了一种基于Hopfield神经网络的AI模型，用于解决生物声学分析中的数据处理问题，具有快速、轻量、可持续和可解释的特点。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前生物声学分析中，被动声学监测设备产生的海量数据处理面临训练数据有限、环境影响及硬件需求高的挑战。

Method: 采用透明且可解释的Hopfield神经网络，通过关联记忆存储信号并检测相似信号，实现物种分类。训练仅需每类目标声音的一个代表信号。

Result: 模型训练速度快（3毫秒），处理10384个蝙蝠回声定位录音仅需5.4秒，内存占用144.09MB，准确率最高达86%。

Conclusion: 该模型为快速、轻量、可持续且可解释的生物声学分析提供了新的解决方案。

Abstract: A growing issue within conservation bioacoustics is the task of analysing the
vast amount of data generated from the use of passive acoustic monitoring
devices. In this paper, we present an alternative AI model which has the
potential to help alleviate this problem. Our model formulation addresses the
key issues encountered when using current AI models for bioacoustic analysis,
namely the: limited training data available; environmental impact, particularly
in energy consumption and carbon footprint of training and implementing these
models; and associated hardware requirements. The model developed in this work
uses associative memory via a transparent, explainable Hopfield neural network
to store signals and detect similar signals which can then be used to classify
species. Training is rapid ($3$\,ms), as only one representative signal is
required for each target sound within a dataset. The model is fast, taking only
$5.4$\,s to pre-process and classify all $10384$ publicly available bat
recordings, on a standard Apple MacBook Air. The model is also lightweight with
a small memory footprint of $144.09$\,MB of RAM usage. Hence, the low
computational demands make the model ideal for use on a variety of standard
personal devices with potential for deployment in the field via edge-processing
devices. It is also competitively accurate, with up to $86\%$ precision on the
dataset used to evaluate the model. In fact, we could not find a single case of
disagreement between model and manual identification via expert field guides.
Although a dataset of bat echolocation calls was chosen to demo this
first-of-its-kind AI model, trained on only two representative calls, the model
is not species specific. In conclusion, we propose an equitable AI model that
has the potential to be a game changer for fast, lightweight, sustainable,
transparent, explainable and accurate bioacoustic analysis.

</details>


### [73] [A Group Theoretic Analysis of the Symmetries Underlying Base Addition and Their Learnability by Neural Networks](https://arxiv.org/abs/2507.10678)
*Cutter Dawes,Simon Segert,Kamesh Krishnamurthy,Jonathan D. Cohen*

Key words: 神经网络, 对称性学习, 基数加法, 进位函数, 泛化能力

TL;DR: 研究表明，神经网络通过对称性学习实现基数加法的泛化能力，不同进位函数对学习效果有显著影响。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索神经网络如何通过对称性学习有效实现基数加法，以支持人工智能和认知科学中的泛化能力。

Method: 采用群论分析基数加法，提出多种进位函数，并通过训练神经网络比较不同进位函数的学习效果。

Result: 简单神经网络在合适的输入格式和进位函数下可实现泛化，学习速度与进位函数结构密切相关。

Conclusion: 研究为认知科学和机器学习提供了对称性学习的新见解，强调了进位函数结构的重要性。

Abstract: A major challenge in the use of neural networks both for modeling human
cognitive function and for artificial intelligence is the design of systems
with the capacity to efficiently learn functions that support radical
generalization. At the roots of this is the capacity to discover and implement
symmetry functions. In this paper, we investigate a paradigmatic example of
radical generalization through the use of symmetry: base addition. We present a
group theoretic analysis of base addition, a fundamental and defining
characteristic of which is the carry function -- the transfer of the remainder,
when a sum exceeds the base modulus, to the next significant place. Our
analysis exposes a range of alternative carry functions for a given base, and
we introduce quantitative measures to characterize these. We then exploit
differences in carry functions to probe the inductive biases of neural networks
in symmetry learning, by training neural networks to carry out base addition
using different carries, and comparing efficacy and rate of learning as a
function of their structure. We find that even simple neural networks can
achieve radical generalization with the right input format and carry function,
and that learning speed is closely correlated with carry function structure. We
then discuss the relevance this has for cognitive science and machine learning.

</details>


### [74] [A Simple Approximate Bayesian Inference Neural Surrogate for Stochastic Petri Net Models](https://arxiv.org/abs/2507.10714)
*Bright Kwaku Manu,Trevor Reckell,Beckett Sterner,Petar Jevtic*

Key words: Stochastic Petri Nets, neural surrogate, parameter estimation, covariate-dependent rates, likelihood-free

TL;DR: 提出了一种基于神经网络的代理框架，用于直接从部分观测的噪声数据中预测SPN的协变量依赖速率函数系数，比传统方法更快更准。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决SPN在协变量依赖速率函数下参数估计的挑战，尤其是当显式似然不可得时。

Method: 使用轻量级1D卷积残差网络，通过Gillespie模拟训练，学习在事件丢失条件下反转系统动态。

Result: 在20%事件丢失的合成SPN中，代理模型的RMSE为0.108，速度显著快于传统贝叶斯方法。

Conclusion: 数据驱动的无似然代理方法可实现复杂部分观测离散事件系统中准确、稳健且实时的参数恢复。

Abstract: Stochastic Petri Nets (SPNs) are an increasingly popular tool of choice for
modeling discrete-event dynamics in areas such as epidemiology and systems
biology, yet their parameter estimation remains challenging in general and in
particular when transition rates depend on external covariates and explicit
likelihoods are unavailable. We introduce a neural-surrogate
(neural-network--based approximation of the posterior distribution) framework
that predicts the coefficients of known covariate-dependent rate functions
directly from noisy, partially observed token trajectories. Our model employs a
lightweight 1D Convolutional Residual Network trained end-to-end on
Gillespie-simulated SPN realizations, learning to invert system dynamics under
realistic conditions of event dropout. During inference, Monte Carlo dropout
provides calibrated uncertainty bounds together with point estimates. On
synthetic SPNs with 20% missing events, our surrogate recovers rate-function
coefficients with an RMSE = 0.108 and substantially runs faster than
traditional Bayesian approaches. These results demonstrate that data-driven,
likelihood-free surrogates can enable accurate, robust, and real-time parameter
recovery in complex, partially observed discrete-event systems.

</details>


### [75] [Distributionally Robust Optimization with Adversarial Data Contamination](https://arxiv.org/abs/2507.10718)
*Shuyao Li,Ilias Diakonikolas,Jelena Diakonikolas*

Key words: 鲁棒优化, Wasserstein距离, 数据污染, 分布偏移, 广义线性模型

TL;DR: 本文提出了一种基于Wasserstein-1 DRO框架的新方法，用于处理训练数据中的异常值和分布偏移问题，并通过高效算法在数据污染下实现O(√ε)的估计误差。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 面对训练数据中的异常值和分布不确定性，传统的DRO方法可能失效，需要一种同时解决这两种挑战的鲁棒方法。

Method: 采用Wasserstein-1 DRO目标优化广义线性模型，结合鲁棒统计方法，提出一种高效算法处理数据污染和分布偏移。

Result: 在数据污染率为ε的情况下，该方法实现了O(√ε)的估计误差，且在协方差有界假设下有效。

Conclusion: 该研究为同时应对数据污染和分布偏移的学习问题提供了首个理论保证和高效计算方案。

Abstract: Distributionally Robust Optimization (DRO) provides a framework for
decision-making under distributional uncertainty, yet its effectiveness can be
compromised by outliers in the training data. This paper introduces a
principled approach to simultaneously address both challenges. We focus on
optimizing Wasserstein-1 DRO objectives for generalized linear models with
convex Lipschitz loss functions, where an $\epsilon$-fraction of the training
data is adversarially corrupted. Our primary contribution lies in a novel
modeling framework that integrates robustness against training data
contamination with robustness against distributional shifts, alongside an
efficient algorithm inspired by robust statistics to solve the resulting
optimization problem. We prove that our method achieves an estimation error of
$O(\sqrt{\epsilon})$ for the true DRO objective value using only the
contaminated data under the bounded covariance assumption. This work
establishes the first rigorous guarantees, supported by efficient computation,
for learning under the dual challenges of data contamination and distributional
shifts.

</details>


### [76] [Ground-Compose-Reinforce: Tasking Reinforcement Learning Agents through Formal Language](https://arxiv.org/abs/2507.10741)
*Andrew C. Li,Toryn Q. Klassen,Andrew Wang,Parand A. Alamdari,Sheila A. McIlraith*

Key words: 语言接地,神经符号框架,强化学习,组合语义,数据驱动

TL;DR: 本文提出了Ground-Compose-Reinforce框架，通过神经符号方法从数据中学习语言接地，并利用强化学习代理执行语言指令，避免了手动设计奖励函数等任务。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决复杂感知（如像素）和动作中语言接地的挑战，构建能与人类通过语言交互的情景代理。

Method: 提出神经符号框架Ground-Compose-Reinforce，结合数据驱动学习和组合形式语言语义，实现高效的语言接地和泛化。

Result: 在基于图像的网格世界和MuJoCo机器人领域的实验中，该方法以有限数据可靠地将形式语言指令映射到行为，而端到端数据驱动方法失败。

Conclusion: 该框架通过组合语义和数据驱动学习，实现了高效的语言接地和指令执行，适用于复杂环境任务。

Abstract: Grounding language in complex perception (e.g. pixels) and action is a key
challenge when building situated agents that can interact with humans via
language. In past works, this is often solved via manual design of the language
grounding or by curating massive datasets relating language to elements of the
environment. We propose Ground-Compose-Reinforce, a neurosymbolic framework for
grounding formal language from data, and eliciting behaviours by directly
tasking RL agents through this language. By virtue of data-driven learning, our
framework avoids the manual design of domain-specific elements like reward
functions or symbol detectors. By virtue of compositional formal language
semantics, our framework achieves data-efficient grounding and generalization
to arbitrary language compositions. Experiments on an image-based gridworld and
a MuJoCo robotics domain show that our approach reliably maps formal language
instructions to behaviours with limited data while end-to-end, data-driven
approaches fail.

</details>


### [77] [A Benchmarking Framework for AI models in Automotive Aerodynamics](https://arxiv.org/abs/2507.10747)
*Kaustubh Tangsali,Rishikesh Ranade,Mohammad Amin Nabian,Alexey Kamenev,Peter Sharpe,Neil Ashton,Ram Cherukuri,Sanjay Choudhry*

Key words: AI模型,汽车空气动力学,开源框架,性能评估,标准化

TL;DR: 介绍了一个用于评估AI模型在汽车空气动力学预测中的开源框架，旨在提升模型对比的透明度和一致性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 通过标准化方法论，加速AI模型在汽车空气动力学中的研究和创新。

Method: 在NVIDIA PhysicsNeMo-CFD框架中，使用表面和体积流场预测评估三种AI模型，并支持扩展。

Result: 展示了框架在多模型评估中的实用性，并提供了物理一致性指标的扩展指南。

Conclusion: 该框架有助于选择和优化AI模型，推动汽车空气动力学中高效、准确和可解释解决方案的发展。

Abstract: In this paper, we introduce a benchmarking framework within the open-source
NVIDIA PhysicsNeMo-CFD framework designed to systematically assess the
accuracy, performance, scalability, and generalization capabilities of AI
models for automotive aerodynamics predictions. The open extensible framework
enables incorporation of a diverse set of metrics relevant to the
Computer-Aided Engineering (CAE) community. By providing a standardized
methodology for comparing AI models, the framework enhances transparency and
consistency in performance assessment, with the overarching goal of improving
the understanding and development of these models to accelerate research and
innovation in the field. To demonstrate its utility, the framework includes
evaluation of both surface and volumetric flow field predictions on three AI
models: DoMINO, X-MeshGraphNet, and FIGConvNet using the DrivAerML dataset. It
also includes guidelines for integrating additional models and datasets, making
it extensible for physically consistent metrics. This benchmarking study aims
to enable researchers and industry professionals in selecting, refining, and
advancing AI-driven aerodynamic modeling approaches, ultimately fostering the
development of more efficient, accurate, and interpretable solutions in
automotive aerodynamics

</details>


### [78] [Spatial Reasoners for Continuous Variables in Any Domain](https://arxiv.org/abs/2507.10768)
*Bart Pogodzinski,Christopher Wewer,Bernt Schiele,Jan Eric Lenssen*

Key words: 空间推理,生成式去噪模型,连续变量,开源框架

TL;DR: 介绍了一个名为Spatial Reasoners的软件框架，用于通过生成式去噪模型对连续变量进行空间推理。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 生成式去噪模型在复杂高维分布采样中表现出色，但在多连续变量推理中的应用研究仍缺乏基础设施支持。

Method: 提供了易用的接口，支持变量映射、生成模型范式和推理策略的灵活控制。

Result: 开源框架Spatial Reasoners旨在降低该领域的研究门槛。

Conclusion: 该框架为生成式推理研究提供了便利工具。

Abstract: We present Spatial Reasoners, a software framework to perform spatial
reasoning over continuous variables with generative denoising models. Denoising
generative models have become the de-facto standard for image generation, due
to their effectiveness in sampling from complex, high-dimensional
distributions. Recently, they have started being explored in the context of
reasoning over multiple continuous variables. Providing infrastructure for
generative reasoning with such models requires a high effort, due to a wide
range of different denoising formulations, samplers, and inference strategies.
Our presented framework aims to facilitate research in this area, providing
easy-to-use interfaces to control variable mapping from arbitrary data domains,
generative model paradigms, and inference strategies. Spatial Reasoners are
openly available at https://spatialreasoners.github.io/

</details>


### [79] [A Generalizable Physics-Enhanced State Space Model for Long-Term Dynamics Forecasting in Complex Environments](https://arxiv.org/abs/2507.10792)
*Yuchen Wang,Hongjue Zhao,Haohong Lin,Enze Xu,Lifang He,Huajie Shao*

Key words: 长期动态预测,状态空间模型,物理知识,复杂环境

TL;DR: 本文提出了一种名为Phy-SSM的方法，通过将部分物理知识整合到状态空间模型中，以解决复杂环境下的长期动态预测问题。该方法在多个实际应用中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对复杂环境中噪声和不规则采样数据的长期预测问题，当前方法在长期外推任务中表现不足，Phy-SSM通过结合物理知识提升泛化能力。

Method: Phy-SSM将已知和未知系统动力学分解为状态矩阵，并引入物理状态正则化项，以确保潜在状态与系统动力学一致。

Result: 在车辆运动预测、无人机状态预测和COVID-19流行病学预测三个实际应用中，Phy-SSM的表现优于基线方法。

Conclusion: Phy-SSM通过结合物理知识提升了状态空间模型的预测能力，在复杂环境中表现出色。

Abstract: This work aims to address the problem of long-term dynamic forecasting in
complex environments where data are noisy and irregularly sampled. While recent
studies have introduced some methods to improve prediction performance, these
approaches still face a significant challenge in handling long-term
extrapolation tasks under such complex scenarios. To overcome this challenge,
we propose Phy-SSM, a generalizable method that integrates partial physics
knowledge into state space models (SSMs) for long-term dynamics forecasting in
complex environments. Our motivation is that SSMs can effectively capture
long-range dependencies in sequential data and model continuous dynamical
systems, while the incorporation of physics knowledge improves generalization
ability. The key challenge lies in how to seamlessly incorporate partially
known physics into SSMs. To achieve this, we decompose partially known system
dynamics into known and unknown state matrices, which are integrated into a
Phy-SSM unit. To further enhance long-term prediction performance, we introduce
a physics state regularization term to make the estimated latent states align
with system dynamics. Besides, we theoretically analyze the uniqueness of the
solutions for our method. Extensive experiments on three real-world
applications, including vehicle motion prediction, drone state prediction, and
COVID-19 epidemiology forecasting, demonstrate the superior performance of
Phy-SSM over the baselines in both long-term interpolation and extrapolation
tasks. The code is available at https://github.com/511205787/Phy_SSM-ICML2025.

</details>


### [80] [Multi-Armed Sampling Problem and the End of Exploration](https://arxiv.org/abs/2507.10797)
*Mohammad Pedramfar,Siamak Ravanbakhsh*

Key words: 多臂采样,探索与利用,后悔界,熵正则化,RLHF

TL;DR: 该论文提出了多臂采样框架，作为多臂老虎机优化问题的抽样对应，重点研究了抽样中的探索与利用权衡。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 主要动机是严格研究抽样中的探索与利用权衡。

Method: 系统定义了后悔概念并建立了对应下界，提出了一种简单算法达到最优后悔界。

Result: 理论结果表明，与优化不同，抽样不需要探索；通过温度参数统一了多臂采样与多臂老虎机问题。

Conclusion: 多臂采样框架对研究抽样（如神经采样器）具有基础性意义，尤其揭示了熵正则化强化学习中的探索需求和算法收敛性。

Abstract: This paper introduces the framework of multi-armed sampling, as the sampling
counterpart to the optimization problem of multi-arm bandits. Our primary
motivation is to rigorously examine the exploration-exploitation trade-off in
the context of sampling. We systematically define plausible notions of regret
for this framework and establish corresponding lower bounds. We then propose a
simple algorithm that achieves these optimal regret bounds. Our theoretical
results demonstrate that in contrast to optimization, sampling does not require
exploration. To further connect our findings with those of multi-armed bandits,
we define a continuous family of problems and associated regret measures that
smoothly interpolates and unifies multi-armed sampling and multi-armed bandit
problems using a temperature parameter. We believe the multi-armed sampling
framework, and our findings in this setting can have a foundational role in the
study of sampling including recent neural samplers, akin to the role of
multi-armed bandits in reinforcement learning. In particular, our work sheds
light on the need for exploration and the convergence properties of algorithm
for entropy-regularized reinforcement learning, fine-tuning of pretrained
models and reinforcement learning with human feedback (RLHF).

</details>


### [81] [Uncovering Causal Relation Shifts in Event Sequences under Out-of-Domain Interventions](https://arxiv.org/abs/2507.10809)
*Kazi Tasnim Zinat,Yun Zhou,Xiang Lyu,Yawei Wang,Zhicheng Liu,Panpan Xu*

Key words: 因果推断,外源干预,平均处理效应,Transformer,时间序列

TL;DR: 论文提出了一种新的因果框架，用于捕捉时间序列中事件间的因果关系，尤其是外源领域干预下的因果动态变化，并通过实验验证其优于基线方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现实世界中，外源领域干预常显著改变事件间的因果动态，而现有因果推断方法多局限于领域内事件类型，因此需要新框架解决这一问题。

Method: 设计了一个无偏的平均处理效应（ATE）估计器，并开发了基于Transformer的神经网络模型，结合外源干预信息，处理长时程依赖与局部模式。

Result: 在模拟和真实数据集上的实验表明，该方法在外源干预增强的点过程中，ATE估计和拟合优度均优于基线方法。

Conclusion: 该方法能够有效捕捉外源干预下的因果动态变化，为跨域因果推断提供了实用工具。

Abstract: Inferring causal relationships between event pairs in a temporal sequence is
applicable in many domains such as healthcare, manufacturing, and
transportation. Most existing work on causal inference primarily focuses on
event types within the designated domain, without considering the impact of
exogenous out-of-domain interventions. In real-world settings, these
out-of-domain interventions can significantly alter causal dynamics. To address
this gap, we propose a new causal framework to define average treatment effect
(ATE), beyond independent and identically distributed (i.i.d.) data in classic
Rubin's causal framework, to capture the causal relation shift between events
of temporal process under out-of-domain intervention. We design an unbiased ATE
estimator, and devise a Transformer-based neural network model to handle both
long-range temporal dependencies and local patterns while integrating
out-of-domain intervention information into process modeling. Extensive
experiments on both simulated and real-world datasets demonstrate that our
method outperforms baselines in ATE estimation and goodness-of-fit under
out-of-domain-augmented point processes.

</details>


### [82] [Semantic Context for Tool Orchestration](https://arxiv.org/abs/2507.10820)
*Robert Müller*

Key words: 语义上下文,工具编排,上下文老虎机,大语言模型,FiReAct

TL;DR: 本文提出语义上下文（SC）是工具编排的基础，通过理论、实证和实际应用验证其有效性，并提出FiReAct管道以提升大型工具集的编排能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究旨在解决工具编排中的样本效率、适应性和可扩展性问题，利用SC提升大型语言模型的上下文学习能力。

Method: 结合SC的上下文老虎机理论（SC-LinUCB）、大语言模型实证验证，以及FiReAct管道在大规模工具集上的应用。

Result: SC-LinUCB在动态动作空间中表现更低遗憾和更强适应性，FiReAct成功编排超过10,000个工具。

Conclusion: SC是构建高效、自适应和可扩展编排代理的关键，研究成果为其提供了全面指导。

Abstract: This paper demonstrates that Semantic Context (SC), leveraging descriptive
tool information, is a foundational component for robust tool orchestration.
Our contributions are threefold. First, we provide a theoretical foundation
using contextual bandits, introducing SC-LinUCB and proving it achieves lower
regret and adapts favourably in dynamic action spaces. Second, we provide
parallel empirical validation with Large Language Models, showing that SC is
critical for successful in-context learning in both static (efficient learning)
and non-stationary (robust adaptation) settings. Third, we propose the FiReAct
pipeline, and demonstrate on a benchmark with over 10,000 tools that SC-based
retrieval enables an LLM to effectively orchestrate over a large action space.
These findings provide a comprehensive guide to building more sample-efficient,
adaptive, and scalable orchestration agents.

</details>


### [83] [From Small to Large: A Graph Convolutional Network Approach for Solving Assortment Optimization Problems](https://arxiv.org/abs/2507.10834)
*Guokai Li,Pin Gao,Stefanus Jasin,Zizhuo Wang*

Key words: 产品组合优化, 图卷积网络, 混合多项Logit模型, 启发式策略, 大规模实例

TL;DR: 利用图卷积网络（GCN）高效解决混合多项Logit选择模型下的受限产品组合优化问题，通过小规模训练实现大规模实例的高效求解。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 产品组合优化是一个经典的NP难问题，传统方法难以高效处理大规模实例，因此探索GCN的泛化能力以提升求解效率和性能。

Method: 将产品组合问题表示为图结构，训练GCN学习最优组合模式，并基于GCN输出提出两种推理策略。

Result: 实验表明，基于小规模训练（如20个产品）的GCN策略可在几秒内处理大规模实例（2000个产品）并达到90%以上的最优性，优于现有启发式方法。

Conclusion: GCN方法在模型已知和未知（数据驱动）场景下均表现出高效性和优越性能。

Abstract: Assortment optimization involves selecting a subset of substitutable products
(subject to certain constraints) to maximize the expected revenue. It is a
classic problem in revenue management and finds applications across various
industries. However, the problem is usually NP-hard due to its combinatorial
and non-linear nature. In this work, we explore how graph concolutional
networks (GCNs) can be leveraged to efficiently solve constrained assortment
optimization under the mixed multinomial logit choice model. We first develop a
graph representation of the assortment problem, then train a GCN to learn the
patterns of optimal assortments, and lastly propose two inference policies
based on the GCN's output. Due to the GCN's inherent ability to generalize
across inputs of varying sizes, we can use a GCN trained on small-scale
instances to facilitate large-scale instances. Extensive numerical experiments
demonstrate that given a GCN trained on small-scale instances (e.g., with 20
products), the proposed policies can achieve superior performance (90%+
optimality) on large-scale instances (with up to 2,000 products) within
seconds, which outperform existing heuristic policies in both performance and
efficiency. Furthermore, we extend our framework to a model-free setting where
the underlying choice model is unknown but transaction data is available. We
also conduct numerical experiments to demonstrate the effectiveness and
efficiency of our proposed policies in this setting.

</details>


### [84] [Collaboration Promotes Group Resilience in Multi-Agent RL](https://arxiv.org/abs/2111.06614)
*Ilai Shraga,Guy Azran,Matthias Gerstgrasser,Ofir Abu,Jeffrey S. Rosenschein,Sarah Keren*

Key words: 多智能体强化学习, 群体韧性, 协作协议, 动态环境

TL;DR: 本文提出并形式化了多智能体强化学习中的'群体韧性'概念，假设协作是提升群体韧性的关键，并通过实验验证了协作协议的有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究多智能体在动态环境中的韧性，特别是协作如何帮助提升群体韧性。

Method: 形式化多智能体韧性概念，提出协作协议假设，并通过实验比较协作与非协作方法的韧性表现。

Result: 所有协作方法的群体韧性均优于非协作方法。

Conclusion: 协作是多智能体强化学习中提升群体韧性的有效策略。

Abstract: To effectively operate in various dynamic scenarios, RL agents must be
resilient to unexpected changes in their environment. Previous work on this
form of resilience has focused on single-agent settings. In this work, we
introduce and formalize a multi-agent variant of resilience, which we term
group resilience. We further hypothesize that collaboration with other agents
is key to achieving group resilience; collaborating agents adapt better to
environmental perturbations in multi-agent reinforcement learning (MARL)
settings. We test our hypothesis empirically by evaluating different
collaboration protocols and examining their effect on group resilience. Our
experiments show that all the examined collaborative approaches achieve higher
group resilience than their non-collaborative counterparts.

</details>


### [85] [Offline Reinforcement Learning with Wasserstein Regularization via Optimal Transport Maps](https://arxiv.org/abs/2507.10843)
*Motoki Omura,Yusuke Mukuta,Kazuki Ota,Takayuki Osa,Tatsuya Harada*

Key words: 离线强化学习、Wasserstein距离、ICNNs、分布偏移

TL;DR: 论文提出一种基于Wasserstein距离的离线强化学习方法，利用ICNNs建模最优传输映射，避免对抗训练，在D4RL数据集上表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 离线强化学习在数据收集成本高的场景（如机器人学）中尤为重要，但分布偏移问题会导致策略不可靠。现有方法多基于密度比正则化，本文提出更稳健的Wasserstein距离方法。

Method: 利用输入凸神经网络（ICNNs）建模最优传输映射，计算Wasserstein距离，无需判别器，避免对抗训练。

Result: 在D4RL基准数据集上表现优于或可比现有广泛使用的方法。

Conclusion: Wasserstein距离结合ICNNs的离线强化学习方法有效解决分布偏移问题，性能优越。

Abstract: Offline reinforcement learning (RL) aims to learn an optimal policy from a
static dataset, making it particularly valuable in scenarios where data
collection is costly, such as robotics. A major challenge in offline RL is
distributional shift, where the learned policy deviates from the dataset
distribution, potentially leading to unreliable out-of-distribution actions. To
mitigate this issue, regularization techniques have been employed. While many
existing methods utilize density ratio-based measures, such as the
$f$-divergence, for regularization, we propose an approach that utilizes the
Wasserstein distance, which is robust to out-of-distribution data and captures
the similarity between actions. Our method employs input-convex neural networks
(ICNNs) to model optimal transport maps, enabling the computation of the
Wasserstein distance in a discriminator-free manner, thereby avoiding
adversarial training and ensuring stable learning. Our approach demonstrates
comparable or superior performance to widely used existing methods on the D4RL
benchmark dataset. The code is available at
https://github.com/motokiomura/Q-DOT .

</details>


### [86] [Visually grounded emotion regulation via diffusion models and user-driven reappraisal](https://arxiv.org/abs/2507.10861)
*Edoardo Pinzuti,Oliver Tüscher,André Ferreira Castro*

Key words: 认知重评估,情绪调节,生成AI,视觉反馈,情感计算

TL;DR: 研究提出了一种基于视觉的认知重评估方法，结合文本到图像扩散模型，通过生成视觉反馈提升情绪调节效果。实验表明，AI辅助方法显著降低负面情绪。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现实中的认知重评估干预通常依赖高阶认知和语言过程，对创伤或抑郁患者效果有限，因此需要一种更直观的视觉辅助方法。

Method: 研究者开发了一个系统，用户通过语音重评估负面情绪图像，系统利用稳定扩散模型生成支持性视觉反馈。实验采用组内设计，比较有无AI反馈的效果。

Result: AI辅助的重评估显著降低了负面情绪，且生成图像与用户重评估的情感一致性增强了调节效果。

Conclusion: 生成视觉反馈能有效支持认知重评估，为生成AI、情感计算和治疗的结合开辟了新方向。

Abstract: Cognitive reappraisal is a key strategy in emotion regulation, involving
reinterpretation of emotionally charged stimuli to alter affective responses.
Despite its central role in clinical and cognitive science, real-world
reappraisal interventions remain cognitively demanding, abstract, and primarily
verbal. This reliance on higher-order cognitive and linguistic processes is
often impaired in individuals with trauma or depression, limiting the
effectiveness of standard approaches. Here, we propose a novel, visually based
augmentation of cognitive reappraisal by integrating large-scale text-to-image
diffusion models into the emotional regulation process. Specifically, we
introduce a system in which users reinterpret emotionally negative images via
spoken reappraisals, which are transformed into supportive, emotionally
congruent visualizations using stable diffusion models with a fine-tuned
IP-adapter. This generative transformation visually instantiates users'
reappraisals while maintaining structural similarity to the original stimuli,
externalizing and reinforcing regulatory intent. To test this approach, we
conducted a within-subject experiment (N = 20) using a modified cognitive
emotion regulation (CER) task. Participants reappraised or described aversive
images from the International Affective Picture System (IAPS), with or without
AI-generated visual feedback. Results show that AI-assisted reappraisal
significantly reduced negative affect compared to both non-AI and control
conditions. Further analyses reveal that sentiment alignment between
participant reappraisals and generated images correlates with affective relief,
suggesting that multimodal coherence enhances regulatory efficacy. These
findings demonstrate that generative visual input can support cogitive
reappraisal and open new directions at the intersection of generative AI,
affective computing, and therapeutic technology.

</details>


### [87] [GALDS: A Graph-Autoencoder-based Latent Dynamics Surrogate model to predict neurite material transport](https://arxiv.org/abs/2507.10871)
*Tsung Yeh Hsieh,Yongjie Jessica Zhang*

Key words: 神经元网络, 物质运输, 图自动编码器, 潜在动力学, 模拟

TL;DR: 论文提出了一种基于图自动编码器的潜在动力学替代模型（GALDS），用于高效模拟神经元树状网络中的物质运输，解决了传统方法计算复杂的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 神经元网络的复杂几何结构对物质运输模拟提出了计算挑战，传统方法耗时且资源密集。

Method: GALDS结合图自动编码器和图潜在空间系统动力学模型，利用小规模图神经网络减少训练数据需求，并借鉴神经常微分方程概念。

Result: 在8种未见几何结构和4种异常运输案例中，GALDS平均相对误差为3%，最大误差低于8%，速度比之前替代模型快10倍。

Conclusion: GALDS为神经元网络中的物质运输模拟提供了一种高效且准确的替代方法。

Abstract: Neurons exhibit intricate geometries within their neurite networks, which
play a crucial role in processes such as signaling and nutrient transport.
Accurate simulation of material transport in the networks is essential for
understanding these biological phenomena but poses significant computational
challenges because of the complex tree-like structures involved. Traditional
approaches are time-intensive and resource-demanding, yet the inherent
properties of neuron trees, which consists primarily of pipes with steady-state
parabolic velocity profiles and bifurcations, provide opportunities for
computational optimization. To address these challenges, we propose a
Graph-Autoencoder-based Latent Dynamics Surrogate (GALDS) model, which is
specifically designed to streamline the simulation of material transport in
neural trees. GALDS employs a graph autoencoder to encode latent
representations of the network's geometry, velocity fields, and concentration
profiles. These latent space representations are then assembled into a global
graph, which is subsequently used to predict system dynamics in the latent
space via a trained graph latent space system dynamic model, inspired by the
Neural Ordinary Differential Equations (Neural ODEs) concept. The integration
of an autoencoder allows for the use of smaller graph neural network models
with reduced training data requirements. Furthermore, the Neural ODE component
effectively mitigates the issue of error accumulation commonly encountered in
recurrent neural networks. The effectiveness of the GALDS model is demonstrated
through results on eight unseen geometries and four abnormal transport
examples, where our approach achieves mean relative error of 3% with maximum
relative error <8% and demonstrates a 10-fold speed improvement compared to
previous surrogate model approaches.

</details>


### [88] [Domain-Adaptive Small Language Models for Structured Tax Code Prediction](https://arxiv.org/abs/2507.10880)
*Souvik Nath,Sumit Wadhwa,Luiz Perez*

Key words: 税务代码预测, 小型语言模型, 编码器-解码器架构, HSN, UNSPSC

TL;DR: 本文提出了一种基于编码器-解码器架构的小型语言模型（SLM），用于增强产品和服务的税务代码预测，解决税务合规性问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 跨国企业每天处理大量交易，需遵守复杂的税务法规，准确的税务代码预测对避免罚款至关重要。

Method: 采用了编码器-解码器架构的SLM，通过序列生成捕捉税务代码的层次关系。

Result: 实验表明，该方法在结构化税务代码预测任务中表现优于传统分类器和单编码器或单解码器架构。

Conclusion: 该方法可扩展到其他政府规定的税务商品代码，具有广泛适用性。

Abstract: Every day, multinational firms process thousands of transactions, each of
which must adhere to tax regulations that vary by jurisdiction and are often
nuanced. The determination of product and service tax codes, such as HSN or SAC
is a major use case in Tax compliance. An accurate determination of such codes
is imperative to avoid any tax penalties. This paper proposes a domain-adaptive
small language model (SLM) with an encoder-decoder architecture for the
enhanced prediction of product and service tax codes. In this approach, we
address the problem of predicting hierarchical tax code sequences using
unstructured product and services data. We employ an SLM based upon
encoder-decoder architecture as this enables sequential generation of tax codes
to capture the hierarchical dependencies present within the tax codes. Our
experiments demonstrate that encoder-decoder SLMs can be successfully applied
to the sequential prediction of structured tax codes, a domain that remains
comparatively unexplored in current NLP research. In this paper, we demonstrate
the superior performance of the domain-adaptive encoder-decoder SLMs over flat
classifiers when applied to the Harmonized System of Nomenclature (HSN), and
achieve superior results compared to decoder-only and encoder-only
architectures for structured sequence generation tasks. This approach can also
be scaled to other government-mandated tax commodity codes, such as United
Nations Standard Products and Services Codes (UNSPSC), or Brazil's Nomenclatura
Comum do Mercosul (NCM).

</details>


### [89] [Learning from Imperfect Data: Robust Inference of Dynamic Systems using Simulation-based Generative Model](https://arxiv.org/abs/2507.10884)
*Hyunwoo Cho,Hyeontae Jo,Hyung Ju Hwang*

Key words: 非线性动态模型, ODE, 物理信息神经网络, 生成对抗网络, 系统推断

TL;DR: SiGMoID 是一种基于模拟的生成模型，用于解决非线性动态模型在噪声、稀疏或部分可观测数据下的推断问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于非线性动态模型在噪声、稀疏或部分可观测数据下的推断具有挑战性，本文提出 SiGMoID 以实现精确且鲁棒的动态系统推断。

Method: SiGMoID 结合了物理信息神经网络与超网络构建 ODE 求解器，以及 Wasserstein 生成对抗网络以估计 ODE 参数并捕获噪声数据分布。

Result: SiGMoID 能够量化数据噪声、估计系统参数并推断未观测系统组件，其有效性通过实际实验验证。

Conclusion: SiGMoID 具有广泛适用性，可用于科学研究与工程系统，帮助发现完整系统动态。

Abstract: System inference for nonlinear dynamic models, represented by ordinary
differential equations (ODEs), remains a significant challenge in many fields,
particularly when the data are noisy, sparse, or partially observable. In this
paper, we propose a Simulation-based Generative Model for Imperfect Data
(SiGMoID) that enables precise and robust inference for dynamic systems. The
proposed approach integrates two key methods: (1) physics-informed neural
networks with hyper-networks that constructs an ODE solver, and (2) Wasserstein
generative adversarial networks that estimates ODE parameters by effectively
capturing noisy data distributions. We demonstrate that SiGMoID quantifies data
noise, estimates system parameters, and infers unobserved system components.
Its effectiveness is validated validated through realistic experimental
examples, showcasing its broad applicability in various domains, from
scientific research to engineered systems, and enabling the discovery of full
system dynamics.

</details>


### [90] [How to Protect Models against Adversarial Unlearning?](https://arxiv.org/abs/2507.10886)
*Patryk Jasiorski,Marek Klonowski,Michał Woźniak*

Key words: 对抗性遗忘,模型保护,AI伦理,GDPR

TL;DR: 本文研究了对抗性遗忘问题，即恶意方故意发送遗忘请求以最大化降低模型性能，并提出了一种保护模型性能的新方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: AI模型需要遗忘以满足法律法规需求（如AI法案或GDPR），并解决有毒内容、去偏见、恶意实例影响或数据分布变化等问题。但遗忘可能导致模型性能下降。

Method: 研究了对抗性遗忘问题，分析了对手能力受模型本身及选择遗忘数据策略的影响，并提出一种保护模型性能的方法。

Result: 提出了一种新方法，能够在自发过程或对抗行为导致的遗忘中保护模型性能。

Conclusion: 该方法有效解决了对抗性遗忘问题，保护了模型性能。

Abstract: AI models need to be unlearned to fulfill the requirements of legal acts such
as the AI Act or GDPR, and also because of the need to remove toxic content,
debiasing, the impact of malicious instances, or changes in the data
distribution structure in which a model works. Unfortunately, removing
knowledge may cause undesirable side effects, such as a deterioration in model
performance. In this paper, we investigate the problem of adversarial
unlearning, where a malicious party intentionally sends unlearn requests to
deteriorate the model's performance maximally. We show that this phenomenon and
the adversary's capabilities depend on many factors, primarily on the backbone
model itself and strategy/limitations in selecting data to be unlearned. The
main result of this work is a new method of protecting model performance from
these side effects, both in the case of unlearned behavior resulting from
spontaneous processes and adversary actions.

</details>


### [91] [Outbound Modeling for Inventory Management](https://arxiv.org/abs/2507.10890)
*Riccardo Savorgnan,Udaya Ghai,Carson Eisenach,Dean Foster*

Key words: 库存规划, 强化学习, 概率预测, 运输成本, 反事实验证

TL;DR: 该研究聚焦于预测库存仓库的货物消耗量和相关运输成本，为强化学习（RL）控制策略提供支持。通过建模联合分布并结合验证方案，提高了模型的准确性和鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了解决库存规划和RL控制策略中货物消耗量和运输成本预测的复杂性，研究提出了一个可微分的模拟方法，以替代高成本的非可微分内部软件系统调用。

Method: 研究将问题转化为概率预测问题，建模了联合分布，并提出了一个验证方案，利用生产系统评估RL策略引起的反事实库存状态下的模型表现。

Result: 初步结果显示，模型在分布内设置下具有较高的准确性。

Conclusion: 该研究提出的方法和验证方案为库存规划和RL控制提供了高效且鲁棒的工具。

Abstract: We study the problem of forecasting the number of units fulfilled (or
``drained'') from each inventory warehouse to meet customer demand, along with
the associated outbound shipping costs. The actual drain and shipping costs are
determined by complex production systems that manage the planning and execution
of customers' orders fulfillment, i.e. from where and how to ship a unit to be
delivered to a customer. Accurately modeling these processes is critical for
regional inventory planning, especially when using Reinforcement Learning (RL)
to develop control policies. For the RL usecase, a drain model is incorporated
into a simulator to produce long rollouts, which we desire to be
differentiable. While simulating the calls to the internal software systems can
be used to recover this transition, they are non-differentiable and too slow
and costly to run within an RL training environment. Accordingly, we frame this
as a probabilistic forecasting problem, modeling the joint distribution of
outbound drain and shipping costs across all warehouses at each time period,
conditioned on inventory positions and exogenous customer demand. To ensure
robustness in an RL environment, the model must handle out-of-distribution
scenarios that arise from off-policy trajectories. We propose a validation
scheme that leverages production systems to evaluate the drain model on
counterfactual inventory states induced by RL policies. Preliminary results
demonstrate the model's accuracy within the in-distribution setting.

</details>


### [92] [Class-Proportional Coreset Selection for Difficulty-Separable Data](https://arxiv.org/abs/2507.10904)
*Elisa Tsai,Haizhong Zheng,Atul Prakash*

Key words: 一类样本核心集、数据难度、类别可分性、数据修剪、机器学习

TL;DR: 本文提出了一种基于类别难度可分性的数据修剪方法，解决了现有方法忽略类别间数据难度差异的问题，通过引入类别比例调整策略，显著提升了数据效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的一样本核心集选择方法假设数据在类别间难度是同质的，但实际上许多领域（如网络入侵检测和医学影像）中数据难度往往按类别聚类，导致传统方法表现不佳。

Method: 提出了类别难度可分性系数（CDSC）作为量化指标，并设计了类别比例调整的多种采样策略（如CCS-CP）。

Result: 在五个数据集上的实验表明，新方法在极端的修剪率下仍能保持高性能，例如在99%修剪率下，CCS-CP的精度仅下降0.49%。

Conclusion: 显式建模类别难度可分性能显著提升数据修剪的效果，尤其在噪声大、不平衡或大规模数据集中表现更佳。

Abstract: High-quality training data is essential for building reliable and efficient
machine learning systems. One-shot coreset selection addresses this by pruning
the dataset while maintaining or even improving model performance, often
relying on training-dynamics-based data difficulty scores. However, most
existing methods implicitly assume class-wise homogeneity in data difficulty,
overlooking variation in data difficulty across different classes.
  In this work, we challenge this assumption by showing that, in domains such
as network intrusion detection and medical imaging, data difficulty often
clusters by class. We formalize this as class-difficulty separability and
introduce the Class Difficulty Separability Coefficient (CDSC) as a
quantitative measure. We demonstrate that high CDSC values correlate with
performance degradation in class-agnostic coreset methods, which tend to
overrepresent easy majority classes while neglecting rare but informative ones.
  To address this, we introduce class-proportional variants of multiple
sampling strategies. Evaluated on five diverse datasets spanning security and
medical domains, our methods consistently achieve state-of-the-art data
efficiency. For instance, on CTU-13, at an extreme 99% pruning rate, a
class-proportional variant of Coverage-centric Coreset Selection (CCS-CP) shows
remarkable stability, with accuracy dropping only 2.58%, precision 0.49%, and
recall 0.19%. In contrast, the class-agnostic CCS baseline, the next best
method, suffers sharper declines of 7.59% in accuracy, 4.57% in precision, and
4.11% in recall.
  We further show that aggressive pruning enhances generalization in noisy,
imbalanced, and large-scale datasets. Our results underscore that explicitly
modeling class-difficulty separability leads to more effective, robust, and
generalizable data pruning, particularly in high-stakes scenarios.

</details>


### [93] [Diffusion Decoding for Peptide De Novo Sequencing](https://arxiv.org/abs/2507.10955)
*Chi-en Amy Tai,Alexander Wong*

Key words: 肽段从头测序、扩散解码器、DINOISER、自回归解码器、氨基酸召回率

TL;DR: 该论文探索了使用扩散解码器改进肽段从头测序的方法，相比传统自回归解码器，扩散解码器通过从任意肽段开始生成序列，提高了氨基酸召回率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统自回归解码器在肽段测序中存在级联错误和未能有效利用高置信区域的局限性，扩散解码器为解决这些问题提供了新思路。

Method: 论文尝试了三种扩散解码器设计、背包束搜索和多种损失函数，其中包括DINOISER损失函数。

Result: 虽然肽段精确率和召回率仍为0，但最佳扩散解码器设计在氨基酸召回率上比自回归解码器基线模型显著提高了0.373。

Conclusion: 扩散解码器不仅增强了模型敏感性，还为肽段从头测序技术的进步提供了潜力。

Abstract: Peptide de novo sequencing is a method used to reconstruct amino acid
sequences from tandem mass spectrometry data without relying on existing
protein sequence databases. Traditional deep learning approaches, such as
Casanovo, mainly utilize autoregressive decoders and predict amino acids
sequentially. Subsequently, they encounter cascading errors and fail to
leverage high-confidence regions effectively. To address these issues, this
paper investigates using diffusion decoders adapted for the discrete data
domain. These decoders provide a different approach, allowing sequence
generation to start from any peptide segment, thereby enhancing prediction
accuracy. We experiment with three different diffusion decoder designs,
knapsack beam search, and various loss functions. We find knapsack beam search
did not improve performance metrics and simply replacing the transformer
decoder with a diffusion decoder lowered performance. Although peptide
precision and recall were still 0, the best diffusion decoder design with the
DINOISER loss function obtained a statistically significant improvement in
amino acid recall by 0.373 compared to the baseline autoregressive
decoder-based Casanovo model. These findings highlight the potential of
diffusion decoders to not only enhance model sensitivity but also drive
significant advancements in peptide de novo sequencing.

</details>


### [94] [Physics-Informed Neural Networks For Semiconductor Film Deposition: A Review](https://arxiv.org/abs/2507.10983)
*Tao Han,Zahra Taheri,Hyunwoong Ko*

Key words: 半导体薄膜沉积, 机器学习, 物理信息神经网络, 过程控制

TL;DR: 本文综述了机器学习（ML）在半导体薄膜沉积过程中的应用，重点分析了物理信息神经网络（PINNs）的优势与局限性，并提出了未来研究方向。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 半导体薄膜沉积过程的精确控制对薄膜均匀性、粘附性和功能性至关重要，传统方法面临挑战。

Method: 通过主题分析，识别趋势、局限性和研究空白，探讨PINNs如何嵌入物理知识以提高ML的准确性。

Result: 研究发现PINNs在增强薄膜沉积过程的解释性、准确性和鲁棒性方面具有巨大潜力。

Conclusion: 本文为未来研究指出了整合PINNs的清晰路径，旨在提升半导体制造的精确性和效率。

Abstract: Semiconductor manufacturing relies heavily on film deposition processes, such
as Chemical Vapor Deposition and Physical Vapor Deposition. These complex
processes require precise control to achieve film uniformity, proper adhesion,
and desired functionality. Recent advancements in Physics-Informed Neural
Networks (PINNs), an innovative machine learning (ML) approach, have shown
significant promise in addressing challenges related to process control,
quality assurance, and predictive modeling within semiconductor film deposition
and other manufacturing domains. This paper provides a comprehensive review of
ML applications targeted at semiconductor film deposition processes. Through a
thematic analysis, we identify key trends, existing limitations, and research
gaps, offering insights into both the advantages and constraints of current
methodologies. Our structured analysis aims to highlight the potential
integration of these ML techniques to enhance interpretability, accuracy, and
robustness in film deposition processes. Additionally, we examine
state-of-the-art PINN methods, discussing strategies for embedding physical
knowledge, governing laws, and partial differential equations into advanced
neural network architectures tailored for semiconductor manufacturing. Based on
this detailed review, we propose novel research directions that integrate the
strengths of PINNs to significantly advance film deposition processes. The
contributions of this study include establishing a clear pathway for future
research in integrating physics-informed ML frameworks, addressing existing
methodological gaps, and ultimately improving precision, scalability, and
operational efficiency within semiconductor manufacturing.

</details>


### [95] [StellarF: A Lora-Adapter Integrated Large Model Framework for Stellar Flare Forecasting with Historical & Statistical Data](https://arxiv.org/abs/2507.10986)
*Tianyu Su,Zhiqiang Zou,Ali Luo,Xiao Kong,Qingyu Lu,Min Li*

Key words: 恒星耀斑预测, LoRA, Adapter, 多尺度模式识别, 天体物理

TL;DR: 该研究提出了StellarF模型，利用LoRA和Adapter技术进行高效的恒星耀斑预测，通过多尺度模式识别，在自建数据集上表现出色。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 恒星耀斑预测是天文学的重要研究领域，但现有方法受限于观测数据稀少和缺乏大规模预测模型。

Method: StellarF结合LoRA和Adapter技术，集成了耀斑统计信息和历史记录模块，实现多尺度模式识别。

Result: 在Kepler和TESS光变曲线数据上，StellarF表现优于现有方法。

Conclusion: StellarF为天体物理研究和跨学科应用提供了新方法论框架。

Abstract: Stellar flare forecasting, a critical research frontier in astronomy, offers
profound insights into stellar activity. However, the field is constrained by
both the sparsity of recorded flare events and the absence of domain-specific
large-scale predictive models. To address these challenges, this study
introduces StellarF (Stellar Flare Forecasting), a novel large model that
leverages Low-Rank (LoRA) and Adapter techniques to parameter-efficient
learning for stellar flare forecasting. At its core, StellarF integrates an
flare statistical information module with a historical flare record module,
enabling multi-scale pattern recognition from observational data. Extensive
experiments on our self-constructed datasets (derived from Kepler and TESS
light curves) demonstrate that StellarF achieves state-of-the-art performance
compared to existing methods. The proposed prediction paradigm establishes a
novel methodological framework for advancing astrophysical research and
cross-disciplinary applications.

</details>


### [96] [High-Throughput Distributed Reinforcement Learning via Adaptive Policy Synchronization](https://arxiv.org/abs/2507.10990)
*Rodney Lafuente-Mercado*

Key words: 强化学习,分布式环境,模块化,AAPS,DETACH

TL;DR: ClusterEnv是一个轻量级的分布式环境执行接口，采用DETACH模式解耦模拟与训练，并提出AAPS机制以减少同步开销。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决现有RL框架在模块化和可重用性上的不足。

Method: 通过ClusterEnv接口和DETACH模式实现模拟与训练的分离，并引入AAPS机制优化同步。

Result: 实验显示AAPS在离散控制任务中实现了高样本效率且减少了权重更新次数。

Conclusion: ClusterEnv提供了模块化且高效的分布式RL解决方案。

Abstract: Scaling reinforcement learning (RL) workloads often requires distributing
environment simulation across compute clusters. Existing frameworks entangle
simulation, learning logic, and orchestration into monolithic systems, limiting
modularity and reusability. We present ClusterEnv, a lightweight,
learner-agnostic interface for distributed environment execution that mirrors
the Gymnasium API. ClusterEnv introduces the DETACH pattern, which decouples
simulation from training by offloading reset() and step() operations to remote
workers while keeping learning centralized. To address policy staleness in
distributed execution, we propose Adaptive Actor Policy Synchronization (AAPS),
a divergence-triggered update mechanism that reduces synchronization overhead
without sacrificing performance. ClusterEnv integrates cleanly into existing RL
pipelines, supports both on-policy and off-policy methods, and requires minimal
code changes. Experiments on discrete control tasks demonstrate that AAPS
achieves high sample efficiency with significantly fewer weight updates. Source
code is available at https://github.com/rodlaf/ClusterEnv.

</details>


### [97] [Misalignment from Treating Means as Ends](https://arxiv.org/abs/2507.10995)
*Henrik Marklund,Alex Infanger,Benjamin Van Roy*

Key words: 奖励函数，强化学习，终端目标，工具性目标，对齐问题

TL;DR: 论文探讨了奖励函数中终端目标和工具性目标的混淆问题，指出这种混淆会导致强化学习中的严重对齐错误。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究发现，奖励函数（无论是学习得到的还是手动指定的）往往不完美，因为它们混淆了人类的终端目标和工具性目标，导致目标无法准确实现。

Method: 论文通过一个简单的例子展示了即使轻微的混淆也会导致严重的对齐问题，并分析了环境特性如何加剧这一问题。

Result: 研究表明，优化错误的奖励函数会导致在实际奖励函数下表现不佳。

Conclusion: 论文指出了奖励学习中的常见问题，并讨论了其在真实环境中的表现。

Abstract: Reward functions, learned or manually specified, are rarely perfect. Instead
of accurately expressing human goals, these reward functions are often
distorted by human beliefs about how best to achieve those goals. Specifically,
these reward functions often express a combination of the human's terminal
goals -- those which are ends in themselves -- and the human's instrumental
goals -- those which are means to an end. We formulate a simple example in
which even slight conflation of instrumental and terminal goals results in
severe misalignment: optimizing the misspecified reward function results in
poor performance when measured by the true reward function. This example
distills the essential properties of environments that make reinforcement
learning highly sensitive to conflation of instrumental and terminal goals. We
discuss how this issue can arise with a common approach to reward learning and
how it can manifest in real environments.

</details>


### [98] [Crafting Imperceptible On-Manifold Adversarial Attacks for Tabular Data](https://arxiv.org/abs/2507.10998)
*Zhipeng He,Alexander Stevens,Chun Ouyang,Johannes De Smedt,Alistair Barros,Catarina Moreira*

Key words: 对抗攻击, 变分自编码器, 表格数据, 潜在空间, 统计一致性

TL;DR: 该论文提出了一种基于混合输入变分自编码器（VAE）的潜在空间扰动框架，用于生成不可察觉的对抗样本，解决了表格数据中由于异构特征导致的对抗攻击难题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 表格数据的异质性（混合分类和数值特征）使得对抗攻击的定义和生成面临独特挑战，传统梯度方法可能生成偏离原始数据分布的对抗样本。

Method: 通过混合输入VAE将分类嵌入和数值特征统一到一个潜在的流形中，生成保持统计一致性的对抗扰动。

Result: 在六个公开数据集和三种模型架构上，该方法表现出更低的离群率和更一致的性能，证明其优于传统输入空间攻击和其他VAE方法。

Conclusion: 研究表明，基于VAE的潜在空间扰动是生成现实对抗攻击的有效方法，强调了流形扰动在表格数据中的重要性。

Abstract: Adversarial attacks on tabular data present fundamental challenges distinct
from image or text domains due to the heterogeneous nature of mixed categorical
and numerical features. Unlike images where pixel perturbations maintain visual
similarity, tabular data lacks intuitive similarity metrics, making it
difficult to define imperceptible modifications. Additionally, traditional
gradient-based methods prioritise $\ell_p$-norm constraints, often producing
adversarial examples that deviate from the original data distributions, making
them detectable. We propose a latent space perturbation framework using a
mixed-input Variational Autoencoder (VAE) to generate imperceptible adversarial
examples. The proposed VAE integrates categorical embeddings and numerical
features into a unified latent manifold, enabling perturbations that preserve
statistical consistency. We specify In-Distribution Success Rate (IDSR) to
measure the proportion of adversarial examples that remain statistically
indistinguishable from the input distribution. Evaluation across six publicly
available datasets and three model architectures demonstrates that our method
achieves substantially lower outlier rates and more consistent performance
compared to traditional input-space attacks and other VAE-based methods adapted
from image domain approaches. Our comprehensive analysis includes
hyperparameter sensitivity, sparsity control mechanisms, and generative
architectural comparisons, revealing that VAE-based attacks depend critically
on reconstruction quality but offer superior practical utility when sufficient
training data is available. This work highlights the importance of on-manifold
perturbations for realistic adversarial attacks on tabular data, offering a
robust approach for practical deployment. The source code can be accessed
through https://github.com/ZhipengHe/VAE-TabAttack.

</details>


### [99] [AdaMuon: Adaptive Muon Optimizer](https://arxiv.org/abs/2507.11005)
*Chongjie Si,Debing Zhang,Wei Shen*

Key words: AdaMuon, Muon, 自适应学习率, 优化器, 大规模模型训练

TL;DR: AdaMuon是一种基于Muon优化器的自适应学习率框架，通过两个模块提升效率，验证显示其性能优于原始Muon且易于集成。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 通过改进Muon优化器，进一步提升大规模模型训练的效率和稳定性。

Method: 1) 参数级第二矩调制，捕捉正交梯度更新以实现自适应；2) RMS对齐的重缩放，调节整体更新幅度。

Result: 在多模型规模和学习率下，AdaMuon性能优于原始Muon，收敛更快且稳定。

Conclusion: AdaMuon无需额外调参，可轻松集成现有Muon训练流程，性能优越。

Abstract: We propose AdaMuon, an adaptive learning-rate framework built upon the
recently validated Muon optimizer, which has demonstrated substantial
efficiency gains over AdamW in large-scale model training. AdaMuon augments
Muon with two mutually dependent modules: (1) a per-parameter second-moment
modulation that captures orthogonal gradient updates to ensure update-level
adaptivity, and (2) a RMS-aligned rescaling that regulates the overall update
magnitude by aligning it with the intrinsic structure of the parameter space.
Empirical results on multiple model scales and learning-rate regimes confirm
that AdaMuon consistently outperforms the original Muon, delivering higher
acceleration in convergence while maintaining training stability. Our method
introduces no additional tuning burden and can be seamlessly integrated into
existing Muon training pipelines.

</details>


### [100] [Leveraging Advanced Machine Learning to Predict Turbulence Dynamics from Temperature Observations at an Experimental Prescribed Fire](https://arxiv.org/abs/2507.11012)
*Dipak Dulal,Joseph J. Charney,Michael R. Gallagher,Pitambar Acharya,Carmeliza Navasca,Nicholas S. Skowronski*

Key words: 湍流动能（TKE）, 机器学习, 温度数据, 火灾环境, 预测模型

TL;DR: 该研究通过温度数据预测湍流动能（TKE），使用多种机器学习模型，揭示了温度与TKE之间的新关系，为火灾环境研究提供了新方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索温度数据与TKE之间的关系，以提高对火灾环境中燃烧和气流过程的理解，并为火灾模型预测提供支持。

Method: 在实验火灾中采集温度和湍流数据，采用深度神经网络、随机森林回归、梯度提升和高斯过程回归等机器学习模型分析数据。

Result: 尽管预测因子与目标变量相关性较弱，机器学习模型仍能准确预测TKE，回归模型表现尤为突出。

Conclusion: 研究展示了机器学习在火灾环境数据分析中的价值，为改进火灾管理和预测模型提供了新途径。

Abstract: This study explores the potential for predicting turbulent kinetic energy
(TKE) from more readily acquired temperature data using temperature profiles
and turbulence data collected concurrently at 10 Hz during a small experimental
prescribed burn in the New Jersey Pine Barrens. Machine learning models,
including Deep Neural Networks, Random Forest Regressor, Gradient Boosting, and
Gaussian Process Regressor, were employed to assess the potential to predict
TKE from temperature perturbations and explore temporal and spatial dynamics of
correlations. Data visualization and correlation analyses revealed patterns and
relationships between thermocouple temperatures and TKE, providing insight into
the underlying dynamics. More accurate predictions of TKE were achieved by
employing various machine learning models despite a weak correlation between
the predictors and the target variable. The results demonstrate significant
success, particularly from regression models, in accurately predicting the TKE.
The findings of this study demonstrate a novel numerical approach to
identifying new relationships between temperature and airflow processes in and
around the fire environment. These relationships can help refine our
understanding of combustion environment processes and the coupling and
decoupling of fire environment processes necessary for improving fire
operations strategy and fire and smoke model predictions. The findings of this
study additionally highlight the valuable role of machine learning techniques
in analyzing the complex large datasets of the fire environments, showcasing
their potential to advance fire research and management practices.

</details>


### [101] [First-Order Error Matters: Accurate Compensation for Quantized Large Language Models](https://arxiv.org/abs/2507.11017)
*Xingyu Zheng,Haotong Qin,Yuye Li,Jiakai Wang,Jinyang Guo,Michele Magno,Xianglong Liu*

Key words: 后训练量化, 大语言模型, 梯度补偿, 一阶偏差, 量化误差

TL;DR: FOEM是一种新型的后训练量化方法，通过显式地纳入一阶梯度项来改进量化误差补偿，显著提升模型性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的补偿式权重校准方法假设一阶项在量化误差中可忽略，但实际上一阶偏差会累积，导致这一假设存在根本缺陷。

Method: FOEM通过直接计算潜在权重与全精度权重的差值来近似梯度，避免了反向传播的高成本，并利用预计算的Cholesky因子高效恢复Hessian子矩阵的逆。

Result: 实验表明，FOEM在3位权重量化下显著降低困惑度并提升准确率，接近全精度性能，且能与其他先进技术无缝集成。

Conclusion: FOEM通过改进量化误差补偿方法，显著提升了模型性能，缩小了与全精度基准的差距。

Abstract: Post-training quantization (PTQ) offers an efficient approach to compressing
large language models (LLMs), significantly reducing memory access and
computational costs. Existing compensation-based weight calibration methods
often rely on a second-order Taylor expansion to model quantization error,
under the assumption that the first-order term is negligible in well-trained
full-precision models. However, we reveal that the progressive compensation
process introduces accumulated first-order deviations between latent weights
and their full-precision counterparts, making this assumption fundamentally
flawed. To address this, we propose FOEM, a novel PTQ method that explicitly
incorporates first-order gradient terms to improve quantization error
compensation. FOEM approximates gradients by directly computing the difference
between latent and full-precision weights, avoiding the high cost and limited
generalization of backpropagation-based gradient computation. This approach
introduces minimal additional computational overhead. Moreover, FOEM leverages
precomputed Cholesky factors to efficiently recover the inverse of Hessian
submatrices in real time. Extensive experiments across a wide range of models
and benchmarks demonstrate that FOEM consistently outperforms the classical
GPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of
Llama3-8B by 89.6%, and improves the 5-shot MMLU accuracy of Llama3-70B from
51.7% to 74.9%, approaching the full-precision performance of 78.6%.
Furthermore, FOEM can be seamlessly integrated with advanced techniques such as
GPTAQ and SpinQuant, yielding additional improvements under the challenging
W4A4KV4 setting, and further narrowing the accuracy gap with full-precision
baselines beyond what current state-of-the-art methods achieve. The code is
available at https://github.com/Xingyu-Zheng/FOEM.

</details>


### [102] [Relative Entropy Pathwise Policy Optimization](https://arxiv.org/abs/2507.11019)
*Claas Voelcker,Axel Brunnbauer,Marcel Hussing,Michal Nauman,Pieter Abbeel,Eric Eaton,Radu Grosu,Amir-massoud Farahmand,Igor Gilitschenski*

Key words: 策略梯度, on-policy学习, 路径策略梯度, 样本效率, REPPO

TL;DR: 该论文提出了一种基于价值梯度的on-policy算法REPPO，结合了路径策略梯度的样本效率和标准on-policy学习的简单性，显著降低了训练方差和资源需求。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决score-function策略梯度的高方差问题以及路径策略梯度对准确动作条件价值函数的依赖，实现在on-policy学习中高效训练价值函数模型。

Method: 提出REPPO算法，通过平衡随机策略探索和约束策略更新，结合路径策略梯度和on-policy学习的优点，优化价值函数学习架构。

Result: REPPO在实验中表现出色，降低了样本需求、训练时间、内存占用，并展示了高度的超参数鲁棒性。

Conclusion: REPPO是一种高效且稳定的on-policy算法，适用于需要低方差和高样本效率的场景。

Abstract: Score-function policy gradients have delivered strong results in
game-playing, robotics and language-model fine-tuning. Yet its high-variance
often undermines training stability. On the other hand, pathwise policy
gradients alleviate the training variance, but are reliable only when driven by
an accurate action-conditioned value function which is notoriously hard to
train without relying on past off-policy data. In this paper, we discuss how to
construct a value-gradient driven, on-policy algorithm that allow training
Q-value models purely from on-policy data, unlocking the possibility of using
pathwise policy updates in the context of on-policy learning. We show how to
balance stochastic policies for exploration with constrained policy updates for
stable training, and evaluate important architectural components that
facilitate accurate value function learning. Building on these insights, we
propose Relative Entropy Pathwise Policy Optimization (REPPO), an efficient
on-policy algorithm that combines the sample-efficiency of pathwise policy
gradients with the simplicity and minimal memory footprint of standard
on-policy learning. We demonstrate that REPPO provides strong empirical
performance at decreased sample requirements, wall-clock time, memory footprint
as well as high hyperparameter robustness in a set of experiments on two
standard GPU-parallelized benchmarks.

</details>


### [103] [GATE: Graph Attention Neural Networks with Real-Time Edge Construction for Robust Indoor Localization using Mobile Embedded Devices](https://arxiv.org/abs/2507.11053)
*Danish Gufran,Sudeep Pasricha*

Key words: 室内定位, 图神经网络, 非欧几里得噪声, 设备异质性

TL;DR: 提出了一种名为GATE的新型框架，通过自适应图表示和创新的向量方法，显著提升了室内定位的准确性和抗干扰能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决Wi-Fi RSS指纹定位中因非欧几里得噪声分布和设备异质性导致的定位精度问题。

Method: 结合了注意力超空间向量（AHV）、多维超空间向量（MDHV）和实时边构建（RTEC）技术，构建自适应图表示模型。

Result: 在实际测试中，GATE的定位误差比现有方法低1.6x至4.72x，最坏情况误差低1.85x至4.57x。

Conclusion: GATE通过结合图神经网络和新型向量技术，有效提升了室内定位的鲁棒性和准确性。

Abstract: Accurate indoor localization is crucial for enabling spatial context in smart
environments and navigation systems. Wi-Fi Received Signal Strength (RSS)
fingerprinting is a widely used indoor localization approach due to its
compatibility with mobile embedded devices. Deep Learning (DL) models improve
accuracy in localization tasks by learning RSS variations across locations, but
they assume fingerprint vectors exist in a Euclidean space, failing to
incorporate spatial relationships and the non-uniform distribution of
real-world RSS noise. This results in poor generalization across heterogeneous
mobile devices, where variations in hardware and signal processing distort RSS
readings. Graph Neural Networks (GNNs) can improve upon conventional DL models
by encoding indoor locations as nodes and modeling their spatial and signal
relationships as edges. However, GNNs struggle with non-Euclidean noise
distributions and suffer from the GNN blind spot problem, leading to degraded
accuracy in environments with dense access points (APs). To address these
challenges, we propose GATE, a novel framework that constructs an adaptive
graph representation of fingerprint vectors while preserving an indoor
state-space topology, modeling the non-Euclidean structure of RSS noise to
mitigate environmental noise and address device heterogeneity. GATE introduces
1) a novel Attention Hyperspace Vector (AHV) for enhanced message passing, 2) a
novel Multi-Dimensional Hyperspace Vector (MDHV) to mitigate the GNN blind
spot, and 3) an new Real-Time Edge Construction (RTEC) approach for dynamic
graph adaptation. Extensive real-world evaluations across multiple indoor
spaces with varying path lengths, AP densities, and heterogeneous devices
demonstrate that GATE achieves 1.6x to 4.72x lower mean localization errors and
1.85x to 4.57x lower worst-case errors compared to state-of-the-art indoor
localization frameworks.

</details>


### [104] [A Distance Metric for Mixed Integer Programming Instances](https://arxiv.org/abs/2507.11063)
*Gwen Maudet,Grégoire Danoy*

Key words: 混合整数线性规划, 距离度量, 无监督学习, 贪心算法, 实例相似性

TL;DR: 本文提出了第一个基于数学定义的混合整数线性规划（MILP）实例距离度量，通过离散化右端项、权重和变量来量化约束的权重-变量分布不匹配，并通过贪心算法实现高效计算。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有相似性度量在识别实例类别时缺乏精确性或依赖标注数据，限制了其适用性和泛化能力。

Method: 将右端项、权重和变量离散化，借鉴地球移动距离的思想量化约束的权重-变量分布不匹配，提出精确和贪心两种计算变体。

Result: 在StrIPLIB数据集上的实验表明，贪心算法在几乎不损失准确性的情况下速度提升了近200倍，且性能优于现有非学习方法。

Conclusion: 所提的无监督方法在类别和子类分组任务上优于非学习方法，并与监督分类器性能相当。

Abstract: Mixed-integer linear programming (MILP) is a powerful tool for addressing a
wide range of real-world problems, but it lacks a clear structure for comparing
instances. A reliable similarity metric could establish meaningful
relationships between instances, enabling more effective evaluation of instance
set heterogeneity and providing better guidance to solvers, particularly when
machine learning is involved. Existing similarity metrics often lack precision
in identifying instance classes or rely heavily on labeled data, which limits
their applicability and generalization. To bridge this gap, this paper
introduces the first mathematical distance metric for MILP instances, derived
directly from their mathematical formulations. By discretizing right-hand
sides, weights, and variables into classes, the proposed metric draws
inspiration from the Earth mover's distance to quantify mismatches in
weight-variable distributions for constraint comparisons. This approach
naturally extends to enable instance-level comparisons. We evaluate both an
exact and a greedy variant of our metric under various parameter settings,
using the StrIPLIB dataset. Results show that all components of the metric
contribute to class identification, and that the greedy version achieves
accuracy nearly identical to the exact formulation while being nearly 200 times
faster. Compared to state-of-the-art baselines, including feature-based,
image-based, and neural network models, our unsupervised method consistently
outperforms all non-learned approaches and rivals the performance of a
supervised classifier on class and subclass grouping tasks.

</details>


### [105] [LogTinyLLM: Tiny Large Language Models Based Contextual Log Anomaly Detection](https://arxiv.org/abs/2507.11071)
*Isaiah Thompson Ocansey,Ritwik Bhattacharya,Tanmay Sen*

Key words: 日志异常检测、LoRA、适配器、参数高效微调、LLMs

TL;DR: 论文提出了参数高效的微调方法（如LoRA和适配器），用于在大规模日志数据中检测异常日志序列，与传统方法相比性能显著提升。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于日志数据量大且复杂，传统方法难以有效检测异常序列，因此需要更高效的解决方案。

Method: 采用参数高效的微调方法（LoRA和适配器），并在Thunderbird数据集上比较不同小型大语言模型（LLMs）。

Result: LoRA微调方法的性能比LogBert全微调方法提升了18到19个百分点，准确率达到97.76%至98.83%，而LogBert为79.37%。

Conclusion: 参数高效的微调方法（尤其是LoRA）在大规模日志异常检测中表现优异，显著优于传统方法。

Abstract: Log anomaly detection using traditional rule based or deep learning based
methods is often challenging due to the large volume and highly complex nature
of log sequence. So effective way of detection of anomalous sequence of logs is
crucial for system maintenance and development. This paper proposes parameter
efficient finetuning specifically low rank adaptation (LoRA) and adapter based
approaches for finding contextual anomalies in sequence of logs in large log
data set. It compares different tiny large language models (LLMs) on the
Thunderbird dataset. The results show that LoRA based finetuning provides
substantial performance improvements of 18 to 19 percentage over LogBert based
full finetuning approach, achieving accuracy scores between 97.76% and 98.83%
compared to 79.37%.

</details>


### [106] [Real-Time Bayesian Detection of Drift-Evasive GNSS Spoofing in Reinforcement Learning Based UAV Deconfliction](https://arxiv.org/abs/2507.11173)
*Deepak Kumar Panda,Weisi Guo*

Key words: 无人机，GNSS，欺骗攻击，贝叶斯在线变化点检测，强化学习

TL;DR: 该论文提出了一种基于贝叶斯在线变化点检测（BOCPD）的方法，用于监测无人机在GNSS信号中的微小行为偏差，以检测隐蔽的欺骗攻击。此方法在检测准确性和误报率上优于传统方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 无人机依赖GNSS信号进行导航，但易受隐蔽欺骗攻击。传统检测方法延迟高，需快速且准确的检测手段。

Method: 使用强化学习批判网络的价值估计，结合BOCPD技术，监测时间上的行为变化。

Result: 该方法比传统GNSS欺骗检测器、半监督学习框架和Page-Hinkley测试表现更优，检测准确性更高，误报率和漏报率更低。

Conclusion: 基于时间价值的BOCPD框架能有效检测隐蔽的欺骗攻击，提升无人机导航的鲁棒性。

Abstract: Autonomous unmanned aerial vehicles (UAVs) rely on global navigation
satellite system (GNSS) pseudorange measurements for accurate real-time
localization and navigation. However, this dependence exposes them to
sophisticated spoofing threats, where adversaries manipulate pseudoranges to
deceive UAV receivers. Among these, drift-evasive spoofing attacks subtly
perturb measurements, gradually diverting the UAVs trajectory without
triggering conventional signal-level anti-spoofing mechanisms. Traditional
distributional shift detection techniques often require accumulating a
threshold number of samples, causing delays that impede rapid detection and
timely response. Consequently, robust temporal-scale detection methods are
essential to identify attack onset and enable contingency planning with
alternative sensing modalities, improving resilience against stealthy
adversarial manipulations. This study explores a Bayesian online change point
detection (BOCPD) approach that monitors temporal shifts in value estimates
from a reinforcement learning (RL) critic network to detect subtle behavioural
deviations in UAV navigation. Experimental results show that this temporal
value-based framework outperforms conventional GNSS spoofing detectors,
temporal semi-supervised learning frameworks, and the Page-Hinkley test,
achieving higher detection accuracy and lower false-positive and false-negative
rates for drift-evasive spoofing attacks.

</details>


### [107] [Gradient Regularization-based Neural Granger Causality](https://arxiv.org/abs/2507.11178)
*Meiliang Liu,Huiwen Dong,Xiaoxiao Yang,Yunfang Xu,Zijin Li,Zhengye Si,Xinyue Yang,Zhiwen Zhao*

Key words: 神经格兰杰因果, 梯度正则化, 时间序列预测, 基因调控网络

TL;DR: 提出了一种基于梯度正则化的神经格兰杰因果模型（GRNGC），通过单一时间序列预测模型和$L_{1}$正则化梯度推断因果关系，显著降低了计算成本并提高了灵活性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有神经格兰杰因果模型采用逐分量架构，计算成本高且难以捕捉复杂交互。

Method: 使用单一时间序列预测模型，对输入输出梯度施加$L_{1}$正则化，可适配多种架构（如KAN、MLP、LSTM）。

Result: 在多个模拟和真实数据集上，GRNGC优于现有基线，计算开销显著降低。

Conclusion: GRNGC在性能和计算效率上均优于现有方法，适用于基因调控网络重建等任务。

Abstract: With the advancement of deep learning technologies, various neural
network-based Granger causality models have been proposed. Although these
models have demonstrated notable improvements, several limitations remain. Most
existing approaches adopt the component-wise architecture, necessitating the
construction of a separate model for each time series, which results in
substantial computational costs. In addition, imposing the sparsity-inducing
penalty on the first-layer weights of the neural network to extract causal
relationships weakens the model's ability to capture complex interactions. To
address these limitations, we propose Gradient Regularization-based Neural
Granger Causality (GRNGC), which requires only one time series prediction model
and applies $L_{1}$ regularization to the gradient between model's input and
output to infer Granger causality. Moreover, GRNGC is not tied to a specific
time series forecasting model and can be implemented with diverse architectures
such as KAN, MLP, and LSTM, offering enhanced flexibility. Numerical
simulations on DREAM, Lorenz-96, fMRI BOLD, and CausalTime show that GRNGC
outperforms existing baselines and significantly reduces computational
overhead. Meanwhile, experiments on real-world DNA, Yeast, HeLa, and bladder
urothelial carcinoma datasets further validate the model's effectiveness in
reconstructing gene regulatory networks.

</details>


### [108] [AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air](https://arxiv.org/abs/2507.11515)
*Shiyi Yang,Xiaoxue Yu,Rongpeng Li,Jianhang Zhu,Zhifeng Zhao,Honggang Zhang*

Key words: 大型语言模型（LLM）, 边缘设备, 远程精细调优, LoRA, PPO, DDIM, 分层扩散策略

TL;DR: AirLLM 是一种针对边缘设备上大型语言模型（LLM）的远程精细调优框架，通过结合强化学习和扩散模型，动态调整LoRA秩配置，显著减少了传输成本并提高了性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于边缘设备的通信带宽和计算资源有限，传统的LoRA方法使用固定或启发式秩配置以及传输所有LoRA参数的方式效率低下，需要一种更高效的远程精细调优方案。

Method: AirLLM 采用分层扩散策略框架，将秩配置建模为结构化动作向量，通过PPO生成粗粒度决策，再通过DDIM细化，最终生成任务和信道自适应的秩向量。

Result: 在不同信噪比下的实验中，AirLLM 显著提升了精细调优性能，同时大幅降低了传输成本。

Conclusion: AirLLM 展示了结合强化学习和扩散模型在远程精细调优中的有效性，为边缘设备上的高效LLM操作提供了可行方案。

Abstract: Operating Large Language Models (LLMs) on edge devices is increasingly
challenged by limited communication bandwidth and strained computational and
memory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable.
Nevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ
fixed or heuristic rank configurations, and the subsequent over-the-air
transmission of all LoRA parameters could be rather inefficient. To address
this limitation, we develop AirLLM, a hierarchical diffusion policy framework
for communication-aware LoRA adaptation. Specifically, AirLLM models the rank
configuration as a structured action vector that spans all LoRA-inserted
projections. To solve the underlying high-dimensional sequential
decision-making problem, a Proximal Policy Optimization (PPO) agent generates
coarse-grained decisions by jointly observing wireless states and linguistic
complexity, which are then refined via Denoising Diffusion Implicit Models
(DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The
two modules are optimized alternatively, with the DDIM trained under the
Classifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards.
Experiments under varying signal-to-noise ratios demonstrate that AirLLM
consistently enhances fine-tuning performance while significantly reducing
transmission costs, highlighting the effectiveness of reinforcement-driven,
diffusion-refined rank adaptation for scalable and efficient remote fine-tuning
over the air.

</details>


### [109] [Mixture of Experts in Large Language Models](https://arxiv.org/abs/2507.11181)
*Danyang Zhang,Junhao Song,Ziqian Bi,Yingfang Yuan,Tianyang Wang,Joe Yeong,Junfeng Hao*

Key words: 混合专家（MoE）、大语言模型、专家门控、多任务学习、深度学习

TL;DR: 本文综述了混合专家（MoE）架构在大语言模型中的应用，分析了其优势、配置、挑战及未来方向。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨MoE架构如何提升模型性能并降低计算成本，为相关研究提供系统化参考。

Method: 通过理论分析、架构设计和应用案例，研究MoE的专家门控、路由机制及多任务学习。

Result: 发现MoE在模型容量、任务性能及扩展性方面优于贝叶斯方法，但需解决专家多样性和推理聚合问题。

Conclusion: MoE架构前景广阔，但仍面临研究局限和挑战，需进一步创新。

Abstract: This paper presents a comprehensive review of the Mixture-of-Experts (MoE)
architecture in large language models, highlighting its ability to
significantly enhance model performance while maintaining minimal computational
overhead. Through a systematic analysis spanning theoretical foundations, core
architectural designs, and large language model (LLM) applications, we examine
expert gating and routing mechanisms, hierarchical and sparse MoE
configurations, meta-learning approaches, multimodal and multitask learning
scenarios, real-world deployment cases, and recent advances and challenges in
deep learning. Our analysis identifies key advantages of MoE, including
superior model capacity compared to equivalent Bayesian approaches, improved
task-specific performance, and the ability to scale model capacity efficiently.
We also underscore the importance of ensuring expert diversity, accurate
calibration, and reliable inference aggregation, as these are essential for
maximizing the effectiveness of MoE architectures. Finally, this review
outlines current research limitations, open challenges, and promising future
directions, providing a foundation for continued innovation in MoE architecture
and its applications.

</details>


### [110] [Quantized Rank Reduction: A Communications-Efficient Federated Learning Scheme for Network-Critical Applications](https://arxiv.org/abs/2507.11183)
*Dimitrios Kritsiolis,Constantine Kotropoulos*

Key words: 联邦学习,低秩近似,量化,通信效率

TL;DR: 提出了一种通信高效的联邦学习方案，通过低秩近似和量化技术显著降低网络负载，同时几乎不影响模型精度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 联邦学习中频繁交换模型更新导致通信开销大，需要一种既能保护隐私又能高效通信的解决方案。

Method: 利用神经网络梯度的低秩近似和量化技术来减少通信数据量。

Result: 显著降低了网络负载，同时对模型精度影响极小。

Conclusion: 所提方案有效解决了联邦学习中的通信开销问题，同时保持了模型的性能。

Abstract: Federated learning is a machine learning approach that enables multiple
devices (i.e., agents) to train a shared model cooperatively without exchanging
raw data. This technique keeps data localized on user devices, ensuring privacy
and security, while each agent trains the model on their own data and only
shares model updates. The communication overhead is a significant challenge due
to the frequent exchange of model updates between the agents and the central
server. In this paper, we propose a communication-efficient federated learning
scheme that utilizes low-rank approximation of neural network gradients and
quantization to significantly reduce the network load of the decentralized
learning process with minimal impact on the model's accuracy.

</details>


### [111] [An Explainable AI-Enhanced Machine Learning Approach for Cardiovascular Disease Detection and Risk Assessment](https://arxiv.org/abs/2507.11185)
*Md. Emon Akter Sourov,Md. Sabbir Hossen,Pabon Shaha,Mohammad Minoar Hossain,Md Sadiq Iqbal*

Key words: 心脏病诊断, 机器学习, SMOTE, 随机森林, 线性回归

TL;DR: 提出了一种结合分类和回归模型的机器学习框架，用于心脏病的检测和风险预测，使用SMOTE技术解决了类不平衡问题，随机森林和线性回归表现最佳。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统心脏病诊断方法准确性不足，机器学习可提升效率和准确性。

Method: 结合分类和回归模型，使用SMOTE技术生成合成数据，评估模型性能。

Result: 随机森林分类准确率97.2%，线性回归R2值0.992，模型表现优异。

Conclusion: 机器学习可革新心脏病诊断与风险预测，支持早期干预。

Abstract: Heart disease remains a major global health concern, particularly in regions
with limited access to medical resources and diagnostic facilities. Traditional
diagnostic methods often fail to accurately identify and manage heart disease
risks, leading to adverse outcomes. Machine learning has the potential to
significantly enhance the accuracy, efficiency, and speed of heart disease
diagnosis. In this study, we proposed a comprehensive framework that combines
classification models for heart disease detection and regression models for
risk prediction. We employed the Heart Disease dataset, which comprises 1,035
cases. To address the issue of class imbalance, the Synthetic Minority
Oversampling Technique (SMOTE) was applied, resulting in the generation of an
additional 100,000 synthetic data points. Performance metrics, including
accuracy, precision, recall, F1-score, R2, MSE, RMSE, and MAE, were used to
evaluate the model's effectiveness. Among the classification models, Random
Forest emerged as the standout performer, achieving an accuracy of 97.2% on
real data and 97.6% on synthetic data. For regression tasks, Linear Regression
demonstrated the highest R2 values of 0.992 and 0.984 on real and synthetic
datasets, respectively, with the lowest error metrics. Additionally,
Explainable AI techniques were employed to enhance the interpretability of the
models. This study highlights the potential of machine learning to
revolutionize heart disease diagnosis and risk prediction, thereby facilitating
early intervention and enhancing clinical decision-making.

</details>


### [112] [Striking the Perfect Balance: Preserving Privacy While Boosting Utility in Collaborative Medical Prediction Platforms](https://arxiv.org/abs/2507.11187)
*Shao-Bo Lin,Xiaotong Liu,Yao Wang*

Key words: 隐私保护,分布式学习,医疗预测,协作平台

TL;DR: 本文提出了一个隐私保护的分布式学习框架，解决了在线协作医疗预测平台中的隐私泄露问题，并保证了预测性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决在线医疗预测平台中患者隐私和数据质量问题，以促进患者和医生的参与。

Method: 提出隐私保护机制，并集成到一次性分布式学习框架中，理论证明其最优性。

Result: 在玩具模拟和真实数据实验中验证了隐私保护和预测性能的双重目标。

Conclusion: 隐私保护框架能同时满足隐私需求和预测性能，促进协作医疗平台的发展。

Abstract: Online collaborative medical prediction platforms offer convenience and
real-time feedback by leveraging massive electronic health records. However,
growing concerns about privacy and low prediction quality can deter patient
participation and doctor cooperation. In this paper, we first clarify the
privacy attacks, namely attribute attacks targeting patients and model
extraction attacks targeting doctors, and specify the corresponding privacy
principles. We then propose a privacy-preserving mechanism and integrate it
into a novel one-shot distributed learning framework, aiming to simultaneously
meet both privacy requirements and prediction performance objectives. Within
the framework of statistical learning theory, we theoretically demonstrate that
the proposed distributed learning framework can achieve the optimal prediction
performance under specific privacy requirements. We further validate the
developed privacy-preserving collaborative medical prediction platform through
both toy simulations and real-world data experiments.

</details>


### [113] [Gradient Descent on Logistic Regression: Do Large Step-Sizes Work with Data on the Sphere?](https://arxiv.org/abs/2507.11228)
*Si Yi Meng,Baptiste Goujaud,Antonio Orvieto,Christopher De Sa*

Key words: 梯度下降, 逻辑回归, 最大间隔, 全局收敛, 循环行为

TL;DR: 研究了在逻辑回归中梯度下降在不同条件下的收敛性，特别是在数据等幅条件下是否能保证全局收敛。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨梯度下降在逻辑回归中的行为，尤其是在非可分数据集上的循环行为是否可以通过数据等幅条件避免。

Method: 通过理论分析，研究了一维与高维空间中梯度下降的行为，并验证了数据等幅条件的影响。

Result: 在一维空间中等幅数据可保证全局收敛，但在高维空间中仍可能出现循环行为。

Conclusion: 表明数据等幅条件不足以在高维中保证全局收敛，需进一步研究实际数据中的循环行为及收敛条件。

Abstract: Gradient descent (GD) on logistic regression has many fascinating properties.
When the dataset is linearly separable, it is known that the iterates converge
in direction to the maximum-margin separator regardless of how large the step
size is. In the non-separable case, however, it has been shown that GD can
exhibit a cycling behaviour even when the step sizes is still below the
stability threshold $2/\lambda$, where $\lambda$ is the largest eigenvalue of
the Hessian at the solution. This short paper explores whether restricting the
data to have equal magnitude is a sufficient condition for global convergence,
under any step size below the stability threshold. We prove that this is true
in a one dimensional space, but in higher dimensions cycling behaviour can
still occur. We hope to inspire further studies on quantifying how common these
cycles are in realistic datasets, as well as finding sufficient conditions to
guarantee global convergence with large step sizes.

</details>


### [114] [Generative Click-through Rate Prediction with Applications to Search Advertising](https://arxiv.org/abs/2507.11246)
*Lingwei Kong,Lu Wang,Changping Peng,Zhangang Lin,Ching Law,Jingping Shao*

Key words: 点击率预测,生成模型,判别模型,两阶段训练,电子商务

TL;DR: 本文提出了一种利用生成模型提升点击率预测精度的方法，通过两阶段训练将生成模型与判别模型结合，实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有点击率预测模型主要依赖判别模型，生成模型在表达能力上有潜力超越判别模型，因此探索生成模型在点击率预测中的应用。

Method: 设计两阶段训练：1)生成模型预训练用于用户行为序列中的下一项预测；2)在判别模型框架中对生成模型进行微调。

Result: 实验和在线A/B测试验证了模型的有效性，并已部署在全球最大电商平台之一。

Conclusion: 生成模型能显著提升点击率预测精度，未来将公开代码和数据集。

Abstract: Click-Through Rate (CTR) prediction models are integral to a myriad of
industrial settings, such as personalized search advertising. Current methods
typically involve feature extraction from users' historical behavior sequences
combined with product information, feeding into a discriminative model that is
trained on user feedback to estimate CTR. With the success of models such as
GPT, the potential for generative models to enrich expressive power beyond
discriminative models has become apparent. In light of this, we introduce a
novel model that leverages generative models to enhance the precision of CTR
predictions in discriminative models. To reconcile the disparate data
aggregation needs of both model types, we design a two-stage training process:
1) Generative pre-training for next-item prediction with the given item
category in user behavior sequences; 2) Fine-tuning the well-trained generative
model within a discriminative CTR prediction framework. Our method's efficacy
is substantiated through extensive experiments on a new dataset, and its
significant utility is further corroborated by online A/B testing results.
Currently, the model is deployed on one of the world's largest e-commerce
platforms, and we intend to release the associated code and dataset in the
future.

</details>


### [115] [LyAm: Robust Non-Convex Optimization for Stable Learning in Noisy Environments](https://arxiv.org/abs/2507.11262)
*Elmira Mirzabeigi,Sepehr Rezaee,Kourosh Parand*

Key words: 深度神经网络,优化器,Lyapunov稳定性,Adam,收敛性

TL;DR: 提出LyAm优化器，结合Adam和Lyapunov稳定性理论，提升深度神经网络的收敛性和稳定性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 深度神经网络训练中梯度噪声和收敛不稳定问题影响性能和泛化能力。

Method: 提出LyAm优化器，动态调整学习率，通过Lyapunov稳定性理论增强收敛鲁棒性。

Result: 在CIFAR-10和CIFAR-100上验证LyAm优于现有优化器，准确性、收敛速度和稳定性均更优。

Conclusion: LyAm是一种鲁棒的深度学习优化器。

Abstract: Training deep neural networks, particularly in computer vision tasks, often
suffers from noisy gradients and unstable convergence, which hinder performance
and generalization. In this paper, we propose LyAm, a novel optimizer that
integrates Adam's adaptive moment estimation with Lyapunov-based stability
mechanisms. LyAm dynamically adjusts the learning rate using Lyapunov stability
theory to enhance convergence robustness and mitigate training noise. We
provide a rigorous theoretical framework proving the convergence guarantees of
LyAm in complex, non-convex settings. Extensive experiments on like as CIFAR-10
and CIFAR-100 show that LyAm consistently outperforms state-of-the-art
optimizers in terms of accuracy, convergence speed, and stability, establishing
it as a strong candidate for robust deep learning optimization.

</details>


### [116] [Turning Sand to Gold: Recycling Data to Bridge On-Policy and Off-Policy Learning via Causal Bound](https://arxiv.org/abs/2507.11269)
*Tal Fiskus,Uri Shaham*

Key words: 深度强化学习, Neyman-Rubin潜在结果框架, 事实损失, 经验回放缓冲区, 样本效率

TL;DR: 该论文提出了一种基于Neyman-Rubin潜在结果框架的深度强化学习（DRL）方法，通过绑定事实损失（类似于DRL中的在线策略损失）来减少训练步骤和经验回放缓冲区的需求。实验表明，该方法在Atari 2600和MuJoCo领域显著提升了样本效率和奖励比例。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的深度强化学习方法需要大量的训练步骤和经验回放缓冲区，导致计算和资源需求较高。为了解决这一问题，作者提出了一种新的理论结果，利用Neyman-Rubin潜在结果框架来优化DRL的训练效率。

Method: 通过将Neyman-Rubin潜在结果框架引入DRL，作者建立了一个关于事实损失的因果界限，并利用经验回放缓冲区中存储的过去值网络输出来计算这一界限。这种方法避免了通常被丢弃的数据浪费。

Result: 实验结果表明，该方法在Atari 2600和MuJoCo领域的多种代理（如DQN和SAC）上实现了高达2,427%的奖励比例提升，且经验回放缓冲区大小减少了96%，显著提高了样本效率。

Conclusion: 该论文提出的方法在减少计算和资源需求的同时，显著提升了深度强化学习的性能和样本效率，具有重要的实际应用价值。

Abstract: Deep reinforcement learning (DRL) agents excel in solving complex
decision-making tasks across various domains. However, they often require a
substantial number of training steps and a vast experience replay buffer,
leading to significant computational and resource demands. To address these
challenges, we introduce a novel theoretical result that leverages the
Neyman-Rubin potential outcomes framework into DRL. Unlike most methods that
focus on bounding the counterfactual loss, we establish a causal bound on the
factual loss, which is analogous to the on-policy loss in DRL. This bound is
computed by storing past value network outputs in the experience replay buffer,
effectively utilizing data that is usually discarded. Extensive experiments
across the Atari 2600 and MuJoCo domains on various agents, such as DQN and
SAC, achieve up to 2,427% higher reward ratio, outperforming the same agents
without our proposed term, and reducing the experience replay buffer size by up
to 96%, significantly improving sample efficiency at negligible cost.

</details>


### [117] [Fast Last-Iterate Convergence of SGD in the Smooth Interpolation Regime](https://arxiv.org/abs/2507.11274)
*Amit Attia,Matan Schliserman,Uri Sherman,Tomer Koren*

Key words: 随机梯度下降, 凸优化, 插值模式, 收敛速率, 过参数化模型

TL;DR: 论文研究了随机梯度下降（SGD）在平滑凸目标函数的插值模式下的人口收敛性，特别是在最优解附近噪声为零或接近零的情况。研究表明，通过优化步骤大小，SGD的最终迭代可以接近最优收敛速率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究SGD在插值模式下的最后迭代行为，尤其是针对过参数化模型训练、持续学习中的遗忘问题以及随机Kaczmarz方法求解线性系统的应用。

Method: 分析SGD在β-平滑凸损失函数上的收敛性，步骤大小为η≤1/β，得出最后迭代的期望超额风险公式。

Result: 通过优化步骤大小，实现了接近最优的收敛速率，尤其在噪声为零时，收敛速率显著优于现有研究。

Conclusion: 在插值模式下，SGD的最后迭代表现优于现有研究，并且通过调整步骤大小可以进一步优化收敛速率。

Abstract: We study population convergence guarantees of stochastic gradient descent
(SGD) for smooth convex objectives in the interpolation regime, where the noise
at optimum is zero or near zero. The behavior of the last iterate of SGD in
this setting -- particularly with large (constant) stepsizes -- has received
growing attention in recent years due to implications for the training of
over-parameterized models, as well as to analyzing forgetting in continual
learning and to understanding the convergence of the randomized Kaczmarz method
for solving linear systems. We establish that after $T$ steps of SGD on
$\beta$-smooth convex loss functions with stepsize $\eta \leq 1/\beta$, the
last iterate exhibits expected excess risk $\widetilde{O}(1/(\eta
T^{1-\beta\eta/2}) + \eta T^{\beta\eta/2} \sigma_\star^2)$, where
$\sigma_\star^2$ denotes the variance of the stochastic gradients at the
optimum. In particular, for a well-tuned stepsize we obtain a near optimal
$\widetilde{O}(1/T + \sigma_\star/\sqrt{T})$ rate for the last iterate,
extending the results of Varre et al. (2021) beyond least squares regression;
and when $\sigma_\star=0$ we obtain a rate of $O(1/\sqrt{T})$ with
$\eta=1/\beta$, improving upon the best-known $O(T^{-1/4})$ rate recently
established by Evron et al. (2025) in the special case of realizable linear
regression.

</details>


### [118] [Guiding LLM Decision-Making with Fairness Reward Models](https://arxiv.org/abs/2507.11344)
*Zara Hall,Melanie Subbiah,Thomas P Zollo,Kathleen McKeown,Richard Zemel*

Key words: 公平性奖励模型, 高风险决策, 语言模型, 偏见减轻

TL;DR: 本文提出了一个通用的公平性奖励模型（FRM），用于训练语言模型以减少高风险决策中的偏见，同时保持或提高准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决语言模型在高风险决策中可能放大偏见的问题，确保其公平使用。

Method: 通过训练一个公平性奖励模型（FRM），对语言模型的推理链进行评分，以减少偏见并提升公平性。

Result: FRM能够跨任务、领域和模型家族迁移，且在真实决策任务中提升了公平性并保持或超越了基线准确性。

Conclusion: 提出的FRM框架是一种有效且通用的方法，可以增强语言模型在高风险决策中的公平性。

Abstract: Large language models are increasingly used to support high-stakes decisions,
potentially influencing who is granted bail or receives a loan. Naive
chain-of-thought sampling can improve average decision accuracy, but has also
been shown to amplify unfair bias. To address this challenge and enable the
trustworthy use of reasoning models in high-stakes decision-making, we propose
a framework for training a generalizable Fairness Reward Model (FRM). Our model
assigns a fairness score to LLM reasoning, enabling the system to down-weight
biased trajectories and favor equitable ones when aggregating decisions across
reasoning chains. We show that a single Fairness Reward Model, trained on
weakly supervised, LLM-annotated examples of biased versus unbiased reasoning,
transfers across tasks, domains, and model families without additional
fine-tuning. Applied to real-world decision-making tasks including recidivism
prediction and social media moderation, we show that our approach consistently
improves fairness while matching, or even surpassing, baseline accuracy.

</details>


### [119] [Neurosymbolic Reasoning Shortcuts under the Independence Assumption](https://arxiv.org/abs/2507.11357)
*Emile van Krieken,Pasquale Minervini,Edoardo Ponti,Antonio Vergari*

Key words: 神经符号预测器, 独立性假设, 不确定性建模, 推理捷径

TL;DR: 论文探讨了神经符号预测器中符号概念独立性假设的局限性，正式证明了这种假设会导致模型无法表示某些概念组合的不确定性，从而无法察觉推理捷径问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 神经符号预测器中常用的符号概念独立性假设虽然简化了概率推理，但可能阻碍学习并影响不确定性建模。本文旨在验证这一假设是否确实限制了神经符号系统的能力。

Method: 通过理论分析，正式证明了符号概念独立性假设的局限性，特别是不确定性表示的缺失。

Result: 研究发现独立性假设导致模型无法表示特定概念组合的不确定性，从而无法识别推理捷径问题。

Conclusion: 独立性假设在神经符号预测器中存在根本缺陷，限制了其对不确定性的建模能力，未来研究需改进这一假设。

Abstract: The ubiquitous independence assumption among symbolic concepts in
neurosymbolic (NeSy) predictors is a convenient simplification: NeSy predictors
use it to speed up probabilistic reasoning. Recent works like van Krieken et
al. (2024) and Marconato et al. (2024) argued that the independence assumption
can hinder learning of NeSy predictors and, more crucially, prevent them from
correctly modelling uncertainty. There is, however, scepticism in the NeSy
community around the scenarios in which the independence assumption actually
limits NeSy systems (Faronius and Dos Martires, 2025). In this work, we settle
this question by formally showing that assuming independence among symbolic
concepts entails that a model can never represent uncertainty over certain
concept combinations. Thus, the model fails to be aware of reasoning shortcuts,
i.e., the pathological behaviour of NeSy predictors that predict correct
downstream tasks but for the wrong reasons.

</details>


### [120] [Local Pairwise Distance Matching for Backpropagation-Free Reinforcement Learning](https://arxiv.org/abs/2507.11367)
*Daniel Tanneberg*

Key words: 强化学习, 反向传播, 层间训练, 多维缩放

TL;DR: 提出了一种无需反向传播的层间局部信号训练方法，适用于RL中的神经网络训练，性能与传统BP方法相当。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决反向传播在RL中存储激活值和梯度消失/爆炸问题。

Method: 利用局部损失和多维缩放的距离匹配原则，结合奖励驱动指导，每层在正向传播中训练。

Result: 在RL基准测试中表现与传统BP相当，提升稳定性和一致性。

Conclusion: 该方法无需反向传播，适合具有挑战性的环境。

Abstract: Training neural networks with reinforcement learning (RL) typically relies on
backpropagation (BP), necessitating storage of activations from the forward
pass for subsequent backward updates. Furthermore, backpropagating error
signals through multiple layers often leads to vanishing or exploding
gradients, which can degrade learning performance and stability. We propose a
novel approach that trains each layer of the neural network using local signals
during the forward pass in RL settings. Our approach introduces local,
layer-wise losses leveraging the principle of matching pairwise distances from
multi-dimensional scaling, enhanced with optional reward-driven guidance. This
method allows each hidden layer to be trained using local signals computed
during forward propagation, thus eliminating the need for backward passes and
storing intermediate activations. Our experiments, conducted with policy
gradient methods across common RL benchmarks, demonstrate that this
backpropagation-free method achieves competitive performance compared to their
classical BP-based counterpart. Additionally, the proposed method enhances
stability and consistency within and across runs, and improves performance
especially in challenging environments.

</details>


### [121] [Step-wise Policy for Rare-tool Knowledge (SPaRK): Offline RL that Drives Diverse Tool Use in LLMs](https://arxiv.org/abs/2507.11371)
*Gabriel Bo,Koa Chang,Justin Gu*

Key words: 

TL;DR: SPaRK是一种新颖的强化学习框架，通过双目标奖励系统优化答案质量和工具多样性，鼓励大语言模型探索多样化的工具使用模式。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统的高温采样方法限制了工具使用的多样性，因此需要一种新框架来探索更多工具使用模式。

Method: 采用双目标奖励系统，结合离线PPO训练Llama-3.1 8B模型，并利用GPT-4o评分策略优先选择使用频率较低但可行的工具。

Result: 在14个MMLU-Pro类别中表现优异，工具选择熵显著高于基线，表明在保持准确性的同时提升了推理能力。

Conclusion: 通过显式工具多样性鼓励探索，可以增强推理能力而不牺牲准确性。

Abstract: We present Step-wise Policy for Rare-tool Knowledge (SPaRK), a novel
reinforcement learning framework that teaches large language models to explore
diverse tool usage patterns beyond conventional high-temperature sampling.
Building on recent advances in step-wise reinforcement learning, we introduce a
dual-objective reward system that simultaneously optimizes for answer quality
and tool diversity, training a Llama-3.1 8B model through offline PPO on
synthetically generated trajectories from the MMLU-Pro dataset. Our approach
uniquely employs a rarity-first exploitation strategy where a GPT-4o judge
scores candidate actions across eight distinct tools plus chain-of-thought
reasoning, with the policy favoring less-frequently used but still viable tools
to encourage systematic exploration. Empirical results demonstrate that SPaRK
achieves competitive performance across 14 MMLU-Pro categories while exhibiting
significantly higher entropy in tool selection compared to both baseline and
supervised fine-tuning approaches, suggesting that algorithmic exploration
through explicit tool diversity can enhance reasoning capabilities without
sacrificing accuracy.

</details>


### [122] [A Neural Network Model of Complementary Learning Systems: Pattern Separation and Completion for Continual Learning](https://arxiv.org/abs/2507.11393)
*James P Jun,Vijay Marupudi,Raj Sanjay Shah,Sashank Varma*

Key words: 持续学习, 灾难性遗忘, 现代Hopfield网络, 变分自编码器, Split-MNIST

TL;DR: 论文提出了一种结合变分自编码器（VAE）和现代Hopfield网络（MHN）的持续学习模型，以减少神经网络中的灾难性遗忘问题，并在Split-MNIST任务上取得了接近最优的准确率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 旨在解决神经网络在持续学习中的灾难性遗忘问题，受人类大脑互补学习系统（CLS）理论的启发。

Method: 结合变分自编码器（VAE）的模式泛化能力和现代Hopfield网络（MHN）的稳健记忆存储特性，构建神经可塑的持续学习模型。

Result: 在Split-MNIST任务上达到接近最优的准确率（~90%），显著减少遗忘。

Conclusion: 该模型为生物和人工系统中的记忆巩固、泛化和持续学习提供了功能模板。

Abstract: Learning new information without forgetting prior knowledge is central to
human intelligence. In contrast, neural network models suffer from catastrophic
forgetting: a significant degradation in performance on previously learned
tasks when acquiring new information. The Complementary Learning Systems (CLS)
theory offers an explanation for this human ability, proposing that the brain
has distinct systems for pattern separation (encoding distinct memories) and
pattern completion (retrieving complete memories from partial cues). To capture
these complementary functions, we leverage the representational generalization
capabilities of variational autoencoders (VAEs) and the robust memory storage
properties of Modern Hopfield networks (MHNs), combining them into a neurally
plausible continual learning model. We evaluate this model on the Split-MNIST
task, a popular continual learning benchmark, and achieve close to
state-of-the-art accuracy (~90%), substantially reducing forgetting.
Representational analyses empirically confirm the functional dissociation: the
VAE underwrites pattern completion, while the MHN drives pattern separation. By
capturing pattern separation and completion in scalable architectures, our work
provides a functional template for modeling memory consolidation,
generalization, and continual learning in both biological and artificial
systems.

</details>


### [123] [Robust-Multi-Task Gradient Boosting](https://arxiv.org/abs/2507.11411)
*Seyedsaman Emami,Gonzalo Martínez-Muñoz,Daniel Hernández-Lobato*

Key words: 多任务学习、梯度提升、异常任务、鲁棒性

TL;DR: 提出了一种名为R-MTGB的新型多任务梯度提升框架，通过分块学习共享模式、识别并处理异常任务，以及微调任务特定预测器，有效提升了多任务学习在任务异构性下的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 多任务学习（MTL）中异常任务的存在会降低模型整体性能，需要一种能自动识别并处理这些任务的鲁棒方法。

Method: R-MTGB框架分为三步：学习共享模式、正则化参数划分任务、微调任务特定预测器。

Result: 在合成和真实数据集上的实验表明，R-MTGB能有效隔离异常任务并提升各任务性能。

Conclusion: R-MTGB在多任务异构环境中表现出鲁棒性和适应性，实现了整体性能提升。

Abstract: Multi-task learning (MTL) has shown effectiveness in exploiting shared
information across tasks to improve generalization. MTL assumes tasks share
similarities that can improve performance. In addition, boosting algorithms
have demonstrated exceptional performance across diverse learning problems,
primarily due to their ability to focus on hard-to-learn instances and
iteratively reduce residual errors. This makes them a promising approach for
learning multi-task problems. However, real-world MTL scenarios often involve
tasks that are not well-aligned (known as outlier or adversarial tasks), which
do not share beneficial similarities with others and can, in fact, deteriorate
the performance of the overall model. To overcome this challenge, we propose
Robust-Multi-Task Gradient Boosting (R-MTGB), a novel boosting framework that
explicitly models and adapts to task heterogeneity during training. R-MTGB
structures the learning process into three sequential blocks: (1) learning
shared patterns, (2) partitioning tasks into outliers and non-outliers with
regularized parameters, and (3) fine-tuning task-specific predictors. This
architecture enables R-MTGB to automatically detect and penalize outlier tasks
while promoting effective knowledge transfer among related tasks. Our method
integrates these mechanisms seamlessly within gradient boosting, allowing
robust handling of noisy or adversarial tasks without sacrificing accuracy.
Extensive experiments on both synthetic benchmarks and real-world datasets
demonstrate that our approach successfully isolates outliers, transfers
knowledge, and consistently reduces prediction errors for each task
individually, and achieves overall performance gains across all tasks. These
results highlight robustness, adaptability, and reliable convergence of R-MTGB
in challenging MTL environments.

</details>


### [124] [Toward Improving fNIRS Classification: A Study on Activation Functions in Deep Neural Architectures](https://arxiv.org/abs/2507.11436)
*Behtom Adeli,John McLinden,Pankaj Pandey,Ming Shao,Yalda Shahriari*

Key words: fNIRS, deep learning, activation functions, symmetry, signal characteristics

TL;DR: 该论文研究了激活函数在fNIRS深度学习任务中的影响，发现对称性激活函数（如Tanh和Abs(x）在某些架构中优于常用ReLU，并强调了选择适合fNIRS信号特性的激活函数的重要性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对fNIRS信号的非线性、低信噪比和可变性等挑战，当前文献缺乏对激活函数影响的系统研究，因此有必要探索不同激活函数在fNIRS分类任务中的表现。

Method: 研究使用多种深度学习架构（包括fNIRSNet、AbsoluteNet、MDNN和浅层ConvNet），在统一数据集和训练参数下，评估传统和领域特定激活函数的性能，并特别分析了对称性的作用。

Result: 结果表明，对称性激活函数（如Tanh和Abs(x）在某些架构中优于ReLU，且修改版绝对函数（MAF）进一步验证了对称性对性能提升的有效性。

Conclusion: 选择与fNIRS信号特性匹配的激活函数对模型性能至关重要，对称性激活函数在该领域具有潜在优势。

Abstract: Activation functions are critical to the performance of deep neural networks,
particularly in domains such as functional near-infrared spectroscopy (fNIRS),
where nonlinearity, low signal-to-noise ratio (SNR), and signal variability
poses significant challenges to model accuracy. However, the impact of
activation functions on deep learning (DL) performance in the fNIRS domain
remains underexplored and lacks systematic investigation in the current
literature. This study evaluates a range of conventional and field-specific
activation functions for fNIRS classification tasks using multiple deep
learning architectures, including the domain-specific fNIRSNet, AbsoluteNet,
MDNN, and shallowConvNet (as the baseline), all tested on a single dataset
recorded during an auditory task. To ensure fair a comparison, all networks
were trained and tested using standardized preprocessing and consistent
training parameters. The results show that symmetrical activation functions
such as Tanh and the Absolute value function Abs(x) can outperform commonly
used functions like the Rectified Linear Unit (ReLU), depending on the
architecture. Additionally, a focused analysis of the role of symmetry was
conducted using a Modified Absolute Function (MAF), with results further
supporting the effectiveness of symmetrical activation functions on performance
gains. These findings underscore the importance of selecting proper activation
functions that align with the signal characteristics of fNIRS data.

</details>


### [125] [Data Augmentation in Time Series Forecasting through Inverted Framework](https://arxiv.org/abs/2507.11439)
*Hongming Tan,Ting Chen,Ruochong Jin,Wai Kin Chan*

Key words: iTransformer, MTS forecasting, data augmentation, DAIF, inverted framework

TL;DR: DAIF是一种针对iTransformer框架的数据增强方法，首次为MTS预测中的倒置框架设计实时增强方案，通过Frequency Filtering和Cross-variation Patching提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: iTransformer在MTS预测中表现优异，但其倒置框架会削弱时间依赖信息并引入噪声，DAIF旨在解决这些问题。

Method: 提出两种DAIF策略：Frequency Filtering和Cross-variation Patching，对倒置序列框架进行实时数据增强。

Result: 在多个数据集和倒置模型上的实验证明DAIF的有效性。

Conclusion: DAIF显著改善了iTransformer框架的局限性，为MTS预测提供了新的增强方案。

Abstract: Currently, iTransformer is one of the most popular and effective models for
multivariate time series (MTS) forecasting. Thanks to its inverted framework,
iTransformer effectively captures multivariate correlation. However, the
inverted framework still has some limitations. It diminishes temporal
interdependency information, and introduces noise in cases of nonsignificant
variable correlation. To address these limitations, we introduce a novel data
augmentation method on inverted framework, called DAIF. Unlike previous data
augmentation methods, DAIF stands out as the first real-time augmentation
specifically designed for the inverted framework in MTS forecasting. We first
define the structure of the inverted sequence-to-sequence framework, then
propose two different DAIF strategies, Frequency Filtering and Cross-variation
Patching to address the existing challenges of the inverted framework.
Experiments across multiple datasets and inverted models have demonstrated the
effectiveness of our DAIF.

</details>


### [126] [LRMR: LLM-Driven Relational Multi-node Ranking for Lymph Node Metastasis Assessment in Rectal Cancer](https://arxiv.org/abs/2507.11457)
*Yaoxian Dong,Yifan Gao,Haoyue Li,Yanfen Cui,Xin Gao*

Key words: 淋巴结转移, 直肠癌, LLM, 多模态, 可解释性

TL;DR: 该论文提出了一种名为LRMR的两阶段LLM驱动框架，用于提高直肠癌淋巴结转移的诊断性能和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统的MRI评估性能有限，且现有AI模型缺乏可解释性和对患者整体情况的考虑。

Method: LRMR分为两个阶段：1）通过多模态LLM分析淋巴结的复合图像，生成结构化报告；2）通过文本LLM对报告进行成对比较，建立相对风险排名。

Result: 在117名患者的回顾性队列中，LRMR的AUC为0.7917，F1分数为0.7200，优于传统深度学习方法。

Conclusion: 两阶段的LLM框架为直肠癌淋巴结转移评估提供了高效且可解释的新方法。

Abstract: Accurate preoperative assessment of lymph node (LN) metastasis in rectal
cancer guides treatment decisions, yet conventional MRI evaluation based on
morphological criteria shows limited diagnostic performance. While some
artificial intelligence models have been developed, they often operate as black
boxes, lacking the interpretability needed for clinical trust. Moreover, these
models typically evaluate nodes in isolation, overlooking the patient-level
context. To address these limitations, we introduce LRMR, an LLM-Driven
Relational Multi-node Ranking framework. This approach reframes the diagnostic
task from a direct classification problem into a structured reasoning and
ranking process. The LRMR framework operates in two stages. First, a multimodal
large language model (LLM) analyzes a composite montage image of all LNs from a
patient, generating a structured report that details ten distinct radiological
features. Second, a text-based LLM performs pairwise comparisons of these
reports between different patients, establishing a relative risk ranking based
on the severity and number of adverse features. We evaluated our method on a
retrospective cohort of 117 rectal cancer patients. LRMR achieved an area under
the curve (AUC) of 0.7917 and an F1-score of 0.7200, outperforming a range of
deep learning baselines, including ResNet50 (AUC 0.7708). Ablation studies
confirmed the value of our two main contributions: removing the relational
ranking stage or the structured prompting stage led to a significant
performance drop, with AUCs falling to 0.6875 and 0.6458, respectively. Our
work demonstrates that decoupling visual perception from cognitive reasoning
through a two-stage LLM framework offers a powerful, interpretable, and
effective new paradigm for assessing lymph node metastasis in rectal cancer.

</details>


### [127] [D3FL: Data Distribution and Detrending for Robust Federated Learning in Non-linear Time-series Data](https://arxiv.org/abs/2507.11471)
*Harsha Varun Marisetty,Manik Gupta,Yogesh Simmhan*

Key words: 联邦学习, 物联网, 时间序列预测, 非线性分布, 去趋势技术

TL;DR: 论文研究了在联邦学习（FL）中处理非线性非平稳时间序列数据时，数据分布和去趋势技术对模型性能的影响，发现FL在非线性数据分布下表现较差，但适当的去趋势技术能提升性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着物联网（IoT）的发展，其设备收集的时间序列数据具有非线性、非平稳特性。传统集中式分析存在延迟和通信成本高的问题，FL成为替代方案，但数据分布差异会影响预测准确性。

Method: 通过生成合成时间序列数据集（基于广义极值和对数正态分布），对比集中式和FL方法训练LSTM预测模型，并评估去趋势技术对真实数据集的影响。

Result: 实验表明：1. 在非线性数据分布下，FL性能低于集中式方法；2. 合适的去趋势技术能减少损失，提升FL性能。

Conclusion: FL在处理非线性非平稳数据时需结合去趋势技术以优化性能。

Abstract: With advancements in computing and communication technologies, the Internet
of Things (IoT) has seen significant growth. IoT devices typically collect data
from various sensors, such as temperature, humidity, and energy meters. Much of
this data is temporal in nature. Traditionally, data from IoT devices is
centralized for analysis, but this approach introduces delays and increased
communication costs. Federated learning (FL) has emerged as an effective
alternative, allowing for model training across distributed devices without the
need to centralize data. In many applications, such as smart home energy and
environmental monitoring, the data collected by IoT devices across different
locations can exhibit significant variation in trends and seasonal patterns.
Accurately forecasting such non-stationary, non-linear time-series data is
crucial for applications like energy consumption estimation and weather
forecasting. However, these data variations can severely impact prediction
accuracy. The key contributions of this paper are: (1) Investigating how
non-linear, non-stationary time-series data distributions, like generalized
extreme value (gen-extreme) and log norm distributions, affect FL performance.
(2) Analyzing how different detrending techniques for non-linear time-series
data influence the forecasting model's performance in a FL setup. We generated
several synthetic time-series datasets using non-linear data distributions and
trained an LSTM-based forecasting model using both centralized and FL
approaches. Additionally, we evaluated the impact of detrending on real-world
datasets with non-linear time-series data distributions. Our experimental
results show that: (1) FL performs worse than centralized approaches when
dealing with non-linear data distributions. (2) The use of appropriate
detrending techniques improves FL performance, reducing loss across different
data distributions.

</details>


### [128] [Exploring the robustness of TractOracle methods in RL-based tractography](https://arxiv.org/abs/2507.11486)
*Jeremi Levesque,Antoine Théberge,Maxime Descoteaux,Pierre-Marc Jodoin*

Key words: 纤维追踪,强化学习,解剖学先验,TractOracle-RL,Iterative Reward Training

TL;DR: 论文研究了基于强化学习的纤维追踪算法TractOracle-RL的四种扩展方法，提出了一种新的训练方案IRT，并验证了其在多种数据集上的性能优于传统方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 通过引入强化学习技术和解剖学先验知识，提升纤维追踪算法的准确性和解剖学有效性。

Method: 提出了四种TractOracle-RL的扩展方法，并设计了一种新的训练方案IRT，结合束过滤方法迭代优化训练过程。

Result: 实验表明，结合oracle的RL方法在多个数据集上表现优异，准确性和解剖学有效性显著提升。

Conclusion: 强化学习框架结合解剖学先验的oracle显著提高了纤维追踪的鲁棒性和可靠性。

Abstract: Tractography algorithms leverage diffusion MRI to reconstruct the fibrous
architecture of the brain's white matter. Among machine learning approaches,
reinforcement learning (RL) has emerged as a promising framework for
tractography, outperforming traditional methods in several key aspects.
TractOracle-RL, a recent RL-based approach, reduces false positives by
incorporating anatomical priors into the training process via a reward-based
mechanism. In this paper, we investigate four extensions of the original
TractOracle-RL framework by integrating recent advances in RL, and we evaluate
their performance across five diverse diffusion MRI datasets. Results
demonstrate that combining an oracle with the RL framework consistently leads
to robust and reliable tractography, regardless of the specific method or
dataset used. We also introduce a novel RL training scheme called Iterative
Reward Training (IRT), inspired by the Reinforcement Learning from Human
Feedback (RLHF) paradigm. Instead of relying on human input, IRT leverages
bundle filtering methods to iteratively refine the oracle's guidance throughout
training. Experimental results show that RL methods trained with oracle
feedback significantly outperform widely used tractography techniques in terms
of accuracy and anatomical validity.

</details>


### [129] [A parametric activation function based on Wendland RBF](https://arxiv.org/abs/2507.11493)
*Majid Darehmiraki*

Key words: Wendland径向基函数, 激活函数, 深度学习, 梯度传播, 过拟合

TL;DR: 本文提出了一种基于Wendland径向基函数（RBFs）的新型参数化激活函数，用于解决传统激活函数（如ReLU、sigmoid和tanh）的局限性，并通过理论和实验验证其优越性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统激活函数如ReLU、sigmoid和tanh存在局限性，而Wendland RBFs因其紧支撑性、光滑性和正定性在近似理论中表现优异，因此被引入深度学习以提升性能。

Method: 结合标准Wendland分量与线性及指数项，提出增强型Wendland激活函数，具有可调局部性、改进的梯度传播和训练稳定性。

Result: 在合成任务（如正弦波近似）和基准数据集（MNIST、Fashion-MNIST）上表现出竞争性性能，尤其在回归任务中准确率更优且计算高效。

Conclusion: 该研究将经典RBF理论与现代深度学习结合，表明Wendland激活可通过局部光滑变换缓解过拟合并提升泛化能力，未来方向包括混合架构和领域特定适配。

Abstract: This paper introduces a novel parametric activation function based on
Wendland radial basis functions (RBFs) for deep neural networks. Wendland RBFs,
known for their compact support, smoothness, and positive definiteness in
approximation theory, are adapted to address limitations of traditional
activation functions like ReLU, sigmoid, and tanh. The proposed enhanced
Wendland activation combines a standard Wendland component with linear and
exponential terms, offering tunable locality, improved gradient propagation,
and enhanced stability during training. Theoretical analysis highlights its
mathematical properties, including smoothness and adaptability, while empirical
experiments on synthetic tasks (e.g., sine wave approximation) and benchmark
datasets (MNIST, Fashion-MNIST) demonstrate competitive performance. Results
show that the Wendland-based activation achieves superior accuracy in certain
scenarios, particularly in regression tasks, while maintaining computational
efficiency. The study bridges classical RBF theory with modern deep learning,
suggesting that Wendland activations can mitigate overfitting and improve
generalization through localized, smooth transformations. Future directions
include hybrid architectures and domain-specific adaptations.

</details>


### [130] [Langevin Flows for Modeling Neural Latent Dynamics](https://arxiv.org/abs/2507.11531)
*Yue Song,T. Anderson Keller,Yisong Yue,Pietro Perona,Max Welling*

Key words: LangevinFlow, 变分自编码器, 神经网络动态

TL;DR: 本文提出了一种名为LangevinFlow的模型，结合了物理先验和变分自编码器，用于捕捉神经网络的动态特性和外部影响。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 神经网络的动态行为具有复杂的潜在结构，现有模型难以同时捕捉其内部动态和外部未观测到的影响，因此需要一种新的建模方法。

Method: 提出LangevinFlow模型，使用欠阻尼Langevin方程描述潜变量的时间演化，并引入物理先验（如惯性、阻尼、随机力等）。

Result: 在合成数据和NLB数据集上表现出色，预测精度和解码行为指标优于现有方法。

Conclusion: LangevinFlow为神经动态建模提供了一种灵活且高性能的框架。

Abstract: Neural populations exhibit latent dynamical structures that drive
time-evolving spiking activities, motivating the search for models that capture
both intrinsic network dynamics and external unobserved influences. In this
work, we introduce LangevinFlow, a sequential Variational Auto-Encoder where
the time evolution of latent variables is governed by the underdamped Langevin
equation. Our approach incorporates physical priors -- such as inertia,
damping, a learned potential function, and stochastic forces -- to represent
both autonomous and non-autonomous processes in neural systems. Crucially, the
potential function is parameterized as a network of locally coupled
oscillators, biasing the model toward oscillatory and flow-like behaviors
observed in biological neural populations. Our model features a recurrent
encoder, a one-layer Transformer decoder, and Langevin dynamics in the latent
space. Empirically, our method outperforms state-of-the-art baselines on
synthetic neural populations generated by a Lorenz attractor, closely matching
ground-truth firing rates. On the Neural Latents Benchmark (NLB), the model
achieves superior held-out neuron likelihoods (bits per spike) and forward
prediction accuracy across four challenging datasets. It also matches or
surpasses alternative methods in decoding behavioral metrics such as hand
velocity. Overall, this work introduces a flexible, physics-inspired,
high-performing framework for modeling complex neural population dynamics and
their unobserved influences.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [131] [SAMEP: A Secure Protocol for Persistent Context Sharing Across AI Agents](https://arxiv.org/abs/2507.10562)
*Hari Masoor*

Key words: AI代理,记忆共享,SAMEP,多代理协作,语义搜索

TL;DR: SAMEP是一种新的协议，支持AI代理之间持久、安全且可语义搜索的记忆共享，解决了跨会话和多代理协作中的内存限制问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有AI代理架构的内存限制阻碍了跨会话和多代理的高效协作与知识共享，需要一种解决方案。

Method: 开发了SAMEP协议，包括分布式记忆存储库、向量语义搜索、加密访问控制和标准化API。

Result: 实验显示，SAMEP减少了73%的冗余计算，提升了89%的上下文相关性，并完全符合法规要求。

Conclusion: SAMEP为持久协作的AI代理生态系统提供了新的范式，同时保障了安全和隐私。

Abstract: Current AI agent architectures suffer from ephemeral memory limitations,
preventing effective collaboration and knowledge sharing across sessions and
agent boundaries. We introduce SAMEP (Secure Agent Memory Exchange Protocol), a
novel framework that enables persistent, secure, and semantically searchable
memory sharing among AI agents. Our protocol addresses three critical
challenges: (1) persistent context preservation across agent sessions, (2)
secure multi-agent collaboration with fine-grained access control, and (3)
efficient semantic discovery of relevant historical context. SAMEP implements a
distributed memory repository with vector-based semantic search, cryptographic
access controls (AES-256-GCM), and standardized APIs compatible with existing
agent communication protocols (MCP, A2A). We demonstrate SAMEP's effectiveness
across diverse domains including multi-agent software development, healthcare
AI with HIPAA compliance, and multi-modal processing pipelines. Experimental
results show 73% reduction in redundant computations, 89% improvement in
context relevance scores, and complete compliance with regulatory requirements
including audit trail generation. SAMEP enables a new paradigm of persistent,
collaborative AI agent ecosystems while maintaining security and privacy
guarantees.

</details>


### [132] [AI Mother Tongue: Self-Emergent Communication in MARL via Endogenous Symbol Systems](https://arxiv.org/abs/2507.10566)
*Hung Ming Liu*

Key words: 多智能体强化学习, 涌现通信, VQ-VAE, 内生符号系统, 语义压缩.

TL;DR: 该研究通过“AI母语”框架证明，当代理具备内生符号系统时，无需外部诱导偏置即可实现有效的符号通信，解决了MARL中的“联合探索困境”。AIM展示了更强的通用性和效率，并为符号学与联结主义的结合提供了新思路。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统方法通过引入人工诱导偏置解决MARL中的“通信真空均衡”，但研究质疑这是否过度工程化，并提出内生符号系统的自然语义压缩和收敛可能更有效。

Method: 研究采用基于VQ-VAE的“AI母语”框架，通过实验验证代理在内生符号系统中的自发语义压缩和纳什均衡驱动的语义收敛。

Result: AIM框架在不依赖外部偏置的情况下实现了高效符号通信，工具包分析揭示了符号使用的幂律分布，并提出了三项理论见解。

Conclusion: 内生符号系统能自然实现语义通信，为未来研究提供了HQ-VAE增强表达能力和RL低级预训练的探索方向。

Abstract: In Decentralized Multi-Agent Reinforcement Learning (MARL), the development
of Emergent Communication has long been constrained by the ``Joint Exploration
Dilemma'', leading agents to fall into a ``Communication Vacuum Equilibrium'' .
Traditional methods address this by introducing inductive biases to facilitate
communication emergence . This study fundamentally questions whether such
artificial inductive biases are, in fact, over-engineering. Through experiments
with the ``AI Mother Tongue'' (AIM) framework, based on a Vector Quantized
Variational Autoencoder (VQ-VAE), we demonstrate that when agents possess an
endogenous symbol system, their neural representations naturally exhibit
spontaneous semantic compression and Nash equilibrium-driven semantic
convergence, achieving effective symbolic communication without external
inductive biases. This aligns with recent neuroscience findings suggesting that
the human brain does not directly use human language for internal thought , and
resonates with research on ``soft thinking'' capabilities in Large Language
Models (LLMs) . Compared to traditional explicit communication methods, AIM
demonstrates stronger generality and efficiency. The interpretable analysis
toolkit developed in this study confirms that symbol usage exhibits a
significant power-law distribution, leading to three major theoretical
insights: the ``Neural Communication Hypothesis'', the ``Tool-First
Principle'', and the ``Semantic Interpretability Paradigm''. Future research
will explore the integration of Hierarchical Quantized Variational Autoencoders
(HQ-VAE) to enhance AIM's complex expressive capabilities and investigate the
potential for ``Reinforcement Learning (RL) Low-Level Pre-training''. This
discovery offers new avenues for bridging symbolism and connectionism.

</details>


### [133] [Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning](https://arxiv.org/abs/2507.10571)
*Konstantinos I. Roumeliotis,Ranjan Sapkota,Manoj Karkee,Nikolaos D. Tselikas*

Key words: 多代理AI, 信任校准, 零样本学习, RAG, 视觉分类

TL;DR: 论文提出了一种新颖的模块化Agentic AI视觉分类框架，通过集成多模态代理、非视觉推理协调器和检索增强生成模块，解决了零样本环境下的信任问题，并在苹果叶病害诊断中验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现代AI越来越依赖多代理架构，但在零样本环境下如何信任这些代理仍然是一个挑战。论文旨在通过模块化框架解决这一问题。

Method: 引入了集成多模态代理、推理协调器和RAG模块的框架，测试了三种配置：零样本信心协调、微调代理和信任校准协调。使用了置信度校准指标（ECE, OCR, CCC）。

Result: 在零样本设置下，通过信任感知协调和RAG，准确率提高了77.94%，总体达到85.63%。GPT-4o表现出更好的校准，而Qwen-2.5-VL则显示过度自信。

Conclusion: 提出的系统将感知与元推理分离，实现了可扩展和可解释的多代理AI，适用于诊断、生物学等信任关键领域。

Abstract: Modern Artificial Intelligence (AI) increasingly relies on multi-agent
architectures that blend visual and language understanding. Yet, a pressing
challenge remains: How can we trust these agents especially in zero-shot
settings with no fine-tuning? We introduce a novel modular Agentic AI visual
classification framework that integrates generalist multimodal agents with a
non-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG)
module. Applied to apple leaf disease diagnosis, we benchmark three
configurations: (I) zero-shot with confidence-based orchestration, (II)
fine-tuned agents with improved performance, and (III) trust-calibrated
orchestration enhanced by CLIP-based image retrieval and re-evaluation loops.
Using confidence calibration metrics (ECE, OCR, CCC), the orchestrator
modulates trust across agents. Our results demonstrate a 77.94\% accuracy
improvement in the zero-shot setting using trust-aware orchestration and RAG,
achieving 85.63\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL
displayed overconfidence. Furthermore, image-RAG grounded predictions with
visually similar cases, enabling correction of agent overconfidence via
iterative re-evaluation. The proposed system separates perception (vision
agents) from meta-reasoning (orchestrator), enabling scalable and interpretable
multi-agent AI. This blueprint is extensible to diagnostics, biology, and other
trust-critical domains. All models, prompts, results, and system components
including the complete software source code are openly released to support
reproducibility, transparency, and community benchmarking at Github:
https://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust

</details>


### [134] [Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning](https://arxiv.org/abs/2507.10624)
*Zheng Zhang*

Key words: 大型语言模型, 符号推理, 计算分裂脑综合征, 模式完成, 架构分析

TL;DR: 论文指出大型语言模型（LLMs）在符号推理、算术准确性和逻辑一致性任务中表现不佳，揭示了其"理解"与"能力"之间的脱节，称为"计算分裂脑综合征"。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究LLMs在复杂任务中失败的根本原因，揭示其内部结构与实际能力之间的不匹配。

Method: 通过控制实验和架构分析，探讨LLMs在表达正确原则与应用能力之间的差距。

Result: LLMs是强大的模式完成引擎，但缺乏结构化推理的架构，导致行为脆弱。

Conclusion: 未来模型需要引入元认知控制、原则提升和结构化执行能力。

Abstract: Large Language Models (LLMs) display striking surface fluency yet
systematically fail at tasks requiring symbolic reasoning, arithmetic accuracy,
and logical consistency. This paper offers a structural diagnosis of such
failures, revealing a persistent gap between \textit{comprehension} and
\textit{competence}. Through controlled experiments and architectural analysis,
we demonstrate that LLMs often articulate correct principles without reliably
applying them--a failure rooted not in knowledge access, but in computational
execution. We term this phenomenon the computational \textit{split-brain
syndrome}, where instruction and action pathways are geometrically and
functionally dissociated. This core limitation recurs across domains, from
mathematical operations to relational inferences, and explains why model
behavior remains brittle even under idealized prompting. We argue that LLMs
function as powerful pattern completion engines, but lack the architectural
scaffolding for principled, compositional reasoning. Our findings delineate the
boundary of current LLM capabilities and motivate future models with
metacognitive control, principle lifting, and structurally grounded execution.
This diagnosis also clarifies why mechanistic interpretability findings may
reflect training-specific pattern coordination rather than universal
computational principles, and why the geometric separation between instruction
and execution pathways suggests limitations in neural introspection and
mechanistic analysis.

</details>


### [135] [Enhancing the Capabilities of Large Language Models for API calls through Knowledge Graphs](https://arxiv.org/abs/2507.10630)
*Ye Yang,Xue Xiao,Ping Yin,Taotao Xie*

Key words: LLM、API调用、知识图谱、气象学、智能问答

TL;DR: KG2data是一个结合知识图谱、LLM、ReAct代理和工具使用技术的系统，用于气象领域的数据获取和查询处理，性能优于其他方法。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探索LLM通过API调用在知识密集型领域（如气象学）中有效利用工具的能力。

Method: 集成知识图谱、LLM、ReAct代理和工具使用技术，通过虚拟API评估API调用的准确性。

Result: KG2data在名称识别失败、幻觉失败和调用正确性方面表现优于RAG2data和chat2data。

Conclusion: KG2data为高知识需求领域提供了基于知识的智能问答和数据分析解决方案。

Abstract: API calls by large language models (LLMs) offer a cutting-edge approach for
data analysis. However, their ability to effectively utilize tools via API
calls remains underexplored in knowledge-intensive domains like meteorology.
This paper introduces KG2data, a system that integrates knowledge graphs, LLMs,
ReAct agents, and tool-use technologies to enable intelligent data acquisition
and query handling in the meteorological field. Using a virtual API, we
evaluate API call accuracy across three metrics: name recognition failure,
hallucination failure, and call correctness. KG2data achieves superior
performance (1.43%, 0%, 88.57%) compared to RAG2data (16%, 10%, 72.14%) and
chat2data (7.14%, 8.57%, 71.43%). KG2data differs from typical LLM-based
systems by addressing their limited access to domain-specific knowledge, which
hampers performance on complex or terminology-rich queries. By using a
knowledge graph as persistent memory, our system enhances content retrieval,
complex query handling, domain-specific reasoning, semantic relationship
resolution, and heterogeneous data integration. It also mitigates the high cost
of fine-tuning LLMs, making the system more adaptable to evolving domain
knowledge and API structures. In summary, KG2data provides a novel solution for
intelligent, knowledge-based question answering and data analysis in domains
with high knowledge demands.

</details>


### [136] [From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents](https://arxiv.org/abs/2507.10644)
*Tatiana Petrova,Aleksandr Puzikov,Boris Bliznukov,Radu State*

Key words: Web of Agents, Multi-Agent Systems, Semantic Web, LLMs, Agentic AI

TL;DR: 本文提出了第一个全面的Web of Agents（WoA）进化概述，通过四轴分类法系统分析了代理架构的演变，并指出智能核心从外部数据转移到代理内部模型的范式转变，为现代Agentic AI奠定了基础。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究WoA的动机在于解决当前研究分散在不同社区的问题，揭示现代系统的智识渊源，并推动对领域发展的整体理解。

Method: 采用四轴分类法（语义基础、通信范式、智能核心、发现机制）系统比较了各代代理架构，并分析了现代协议的演化路径。

Result: 分析揭示了智能核心从外部数据或平台转移到代理内部模型的范式转变，为现代Agentic AI提供了基础。

Conclusion: 构建健壮、开放、可信的WoA生态系统需要解决持续的社会技术挑战，如去中心化身份、经济模型、安全和治理。

Abstract: The concept of the Web of Agents (WoA), which transforms the static,
document-centric Web into an environment of autonomous agents acting on users'
behalf, has attracted growing interest as large language models (LLMs) become
more capable. However, research in this area is still fragmented across
different communities. Contemporary surveys catalog the latest LLM-powered
frameworks, while the rich histories of Multi-Agent Systems (MAS) and the
Semantic Web are often treated as separate, legacy domains. This fragmentation
obscures the intellectual lineage of modern systems and hinders a holistic
understanding of the field's trajectory. We present the first comprehensive
evolutionary overview of the WoA. We show that modern protocols like A2A and
the MCP, are direct evolutionary responses to the well-documented limitations
of earlier standards like FIPA standards and OWL-based semantic agents. To
systematize this analysis, we introduce a four-axis taxonomy (semantic
foundation, communication paradigm, locus of intelligence, discovery
mechanism). This framework provides a unified analytical lens for comparing
agent architectures across all generations, revealing a clear line of descent
where others have seen a disconnect. Our analysis identifies a paradigm shift
in the 'locus of intelligence': from being encoded in external data (Semantic
Web) or the platform (MAS) to being embedded within the agent's core model
(LLM). This shift is foundational to modern Agentic AI, enabling the scalable
and adaptive systems the WoA has long envisioned. We conclude that while new
protocols are essential, they are insufficient for building a robust, open,
trustworthy ecosystem. Finally, we argue that the next research frontier lies
in solving persistent socio-technical challenges, and we map out a new agenda
focused on decentralized identity, economic models, security, and governance
for the emerging WoA.

</details>


### [137] [Parsing Musical Structure to Enable Meaningful Variations](https://arxiv.org/abs/2507.10740)
*Maziar Kanani,Sean O Leary,James McDermott*

Key words: 音乐生成, 语法变异, Sequitur算法, 编辑距离, 结构复杂性

TL;DR: 本文提出了一种基于规则的音乐生成方法，通过变异现有曲调的语法结构生成新曲调，分析了多次变异后曲调的变化及其影响。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究如何通过变异现有曲调的语法结构生成新曲调，并分析变异对曲调的影响。

Method: 使用Sequitur算法解析曲调生成语法结构（PA），随机应用19种变异类型（如增加、删除、交换或反转部分语法），再通过扩展语法生成新曲调。

Result: 分析了多次变异后曲调的编辑距离、结构复杂性和长度变化，以及每种变异类型的影响大小。

Conclusion: 该方法成功生成了与原曲调相关的新曲调，并量化了变异对曲调的影响。

Abstract: This paper presents a novel rule-based approach for generating music by
varying existing tunes. We parse each tune to find the Pathway Assembly (PA) [
1], that is a structure representing all repetitions in the tune. The Sequitur
algorithm [2 ] is used for this. The result is a grammar. We then carry out
mutation on the grammar, rather than on a tune directly. There are potentially
19 types of mutations such as adding, removing, swapping or reversing parts of
the grammar that can be applied to the grammars. The system employs one of the
mutations randomly in this step to automatically manipulate the grammar.
Following the mutation, we need to expand the grammar which returns a new tune.
The output after 1 or more mutations will be a new tune related to the original
tune. Our study examines how tunes change gradually over the course of multiple
mutations. Edit distances, structural complexity and length of the tunes are
used to show how a tune is changed after multiple mutations. In addition, the
size of effect of each mutation type is analyzed. As a final point, we review
the musical aspect of the output tunes. It should be noted that the study only
focused on generating new pitch sequences. The study is based on an Irish
traditional tune dataset and a list of integers has been used to represent each
tune's pitch values.

</details>


### [138] [AI and the Net-Zero Journey: Energy Demand, Emissions, and the Potential for Transition](https://arxiv.org/abs/2507.10750)
*Pandu Devarakota,Nicolas Tsesmetzis,Faruk O. Alpak,Apurva Gala,Detlef Hohl*

Key words: AI, 数据中心, 能源消耗, CO2排放, 气候缓解

TL;DR: 论文探讨了AI数据中心对能源消耗及温室气体排放的影响，预测了短期（至2030年）和长期（2035年及以后）的情景，并分析了AI对CO2排放的潜在影响。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究AI技术快速发展背景下，数据中心的能源消耗及其对环境的影响，评估AI对CO2排放的长期净效应。

Method: 通过技术综述分析，结合短期和长期情景预测，评估AI对能源生产、供应和消费的优化潜力。

Result: 短期内AI需求增长可能导致电力消耗和CO2排放增加；长期来看，AI优化流程的能力有望显著减少碳足迹。

Conclusion: AI可能在初期对环境产生负面影响，但长期将成为气候缓解的有力工具。

Abstract: Thanks to the availability of massive amounts of data, computing resources,
and advanced algorithms, AI has entered nearly every sector. This has sparked
significant investment and interest, particularly in building data centers with
the necessary hardware and software to develop and operate AI models and
AI-based workflows. In this technical review article, we present energy
consumption scenarios of data centers and impact on GHG emissions, considering
both near-term projections (up to 2030) and long-term outlook (2035 and
beyond). We address the quintessential question of whether AI will have a net
positive, neutral, or negative impact on CO2 emissions by 2035. Additionally,
we discuss AI's potential to automate, create efficient and disruptive
workflows across various fields related to energy production, supply and
consumption. In the near-term scenario, the growing demand for AI will likely
strain computing resources, lead to increase in electricity consumption and
therefore associated CO2 emissions. This is due to the power-hungry nature of
big data centers and the requirements for training and running of large and
complex AI models, as well as the penetration of AI assistant search and
applications for public use. However, the long-term outlook could be more
promising. AI has the potential to be a game-changer in CO2 reduction. Its
ability to further automate and optimize processes across industries, from
energy production to logistics, could significantly decrease our carbon
footprint. This positive impact is anticipated to outweigh the initial
emissions bump, creating value for businesses and society in areas where
traditional solutions have fallen short. In essence, AI might cause some
initial growing pains for the environment, but it has the potential to support
climate mitigation efforts.

</details>


### [139] [IoT Malware Network Traffic Detection using Deep Learning and GraphSAGE Models](https://arxiv.org/abs/2507.10758)
*Nikesh Prajapati,Bimal Karki,Saroj Gopali,Akbar Siami Namin*

Key words: 深度学习、物联网安全、恶意流量检测、BERT、GraphSAGE

TL;DR: 该论文利用深度学习模型检测物联网恶意攻击，评估了多种模型（如GraphSAGE、BERT、TCN等）在恶意网络流量检测中的表现。BERT表现最佳，而GraphSAGE训练时间最短但准确率最低。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 物联网系统的流量模式具有时序性和多样性，为模型学习提供了丰富的数据。论文旨在通过深度学习和图模型提升恶意流量检测能力。

Method: 采用了多种模型：GraphSAGE、BERT、TCN、Multi-Head Attention、BI-LSTM及其变体，评估了它们在恶意流量检测中的表现。

Result: BERT表现最优，准确率达99.94%，其他指标也接近完美；Multi-Head Attention检测能力良好但处理时间长；GraphSAGE训练时间最短但性能较差。

Conclusion: BERT在捕捉时序依赖方面表现出色，而Multi-Head Attention和GraphSAGE各有优缺点。模型选择需权衡性能与效率。

Abstract: This paper intends to detect IoT malicious attacks through deep learning
models and demonstrates a comprehensive evaluation of the deep learning and
graph-based models regarding malicious network traffic detection. The models
particularly are based on GraphSAGE, Bidirectional encoder representations from
transformers (BERT), Temporal Convolutional Network (TCN) as well as Multi-Head
Attention, together with Bidirectional Long Short-Term Memory (BI-LSTM)
Multi-Head Attention and BI-LSTM and LSTM models. The chosen models
demonstrated great performance to model temporal patterns and detect feature
significance. The observed performance are mainly due to the fact that IoT
system traffic patterns are both sequential and diverse, leaving a rich set of
temporal patterns for the models to learn. Experimental results showed that
BERT maintained the best performance. It achieved 99.94% accuracy rate
alongside high precision and recall, F1-score and AUC-ROC score of 99.99% which
demonstrates its capabilities through temporal dependency capture. The
Multi-Head Attention offered promising results by providing good detection
capabilities with interpretable results. On the other side, the Multi-Head
Attention model required significant processing time like BI-LSTM variants. The
GraphSAGE model achieved good accuracy while requiring the shortest training
time but yielded the lowest accuracy, precision, and F1 score compared to the
other models

</details>


### [140] [Detecting AI Assistance in Abstract Complex Tasks](https://arxiv.org/abs/2507.10761)
*Tyler King,Nikolos Gurney,John H. Miller,Volkan Ustun*

Key words: AI辅助检测,神经网络分类,图像预处理,时间序列,CNN-RNN

TL;DR: 讨论了如何通过预处理和神经网络分类检测人工智能在抽象任务中的辅助作用，提出了四种图像形式和时间序列方法，并验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着AI在复杂任务中普及，检测其辅助作用变得重要，但人类难以处理抽象数据，因此探索神经网络分类的可行性。

Method: 提出四种神经网络友好的图像形式和一种时间序列方法，编码用户探索/利用行为，使用深度学习架构（如CNN-RNN）进行验证。

Result: 结果表明，经过适当预处理后，常见模型能有效分类抽象数据，且时间序列方法能提升检测性能。

Conclusion: 编码时空数据对于检测AI辅助在抽象任务中的作用至关重要，提出的方法具有普适性。

Abstract: Detecting assistance from artificial intelligence is increasingly important
as they become ubiquitous across complex tasks such as text generation, medical
diagnosis, and autonomous driving. Aid detection is challenging for humans,
especially when looking at abstract task data. Artificial neural networks excel
at classification thanks to their ability to quickly learn from and process
large amounts of data -- assuming appropriate preprocessing. We posit detecting
help from AI as a classification task for such models. Much of the research in
this space examines the classification of complex but concrete data classes,
such as images. Many AI assistance detection scenarios, however, result in data
that is not machine learning-friendly. We demonstrate that common models can
effectively classify such data when it is appropriately preprocessed. To do so,
we construct four distinct neural network-friendly image formulations along
with an additional time-series formulation that explicitly encodes the
exploration/exploitation of users, which allows for generalizability to other
abstract tasks. We benchmark the quality of each image formulation across three
classical deep learning architectures, along with a parallel CNN-RNN
architecture that leverages the additional time series to maximize testing
performance, showcasing the importance of encoding temporal and spatial
quantities for detecting AI aid in abstract tasks.

</details>


### [141] [Uncertainty-Informed Scheduling of Decision Points for Intelligent Mobile Health Interventions](https://arxiv.org/abs/2507.10798)
*Asim H. Gazi,Bhanu T. Gullapalli,Daiqi Gao,Benjamin M. Marlin,Vivek Shetty,Susan A. Murphy*

Key words: 移动健康、JITAI、决策点调度、习惯行为、精准干预

TL;DR: 摘要提出了一种名为SigmaScheduling的动态决策点调度方法，用于智能移动健康干预系统中，以提升干预的及时性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前固定间隔的决策点调度方法在面对用户不规则行为时间表现不佳，导致干预效果差，需要更个性化的方案。

Method: SigmaScheduling根据预测行为时间的不确定性动态调整决策点，时间预测越准确，决策点越接近行为时间；反之则提前安排决策点。

Result: 在68名参与者的10周试验中，SigmaScheduling使70%以上的刷牙行为前至少有一个决策点，提高了干预机会。

Conclusion: SigmaScheduling能提升精准移动健康干预的效果，尤其适用于时间敏感性习惯行为（如刷牙或饮食习惯）。

Abstract: Timely decision making is critical to the effectiveness of mobile health
(mHealth) interventions. At predefined timepoints called "decision points,"
intelligent mHealth systems such as just-in-time adaptive interventions
(JITAIs) estimate an individual's biobehavioral context from sensor or survey
data and determine whether and how to intervene. For interventions targeting
habitual behavior (e.g., oral hygiene), effectiveness often hinges on
delivering support shortly before the target behavior is likely to occur.
Current practice schedules decision points at a fixed interval (e.g., one hour)
before user-provided behavior times, and the fixed interval is kept the same
for all individuals. However, this one-size-fits-all approach performs poorly
for individuals with irregular routines, often scheduling decision points after
the target behavior has already occurred, rendering interventions ineffective.
In this paper, we propose SigmaScheduling, a method to dynamically schedule
decision points based on uncertainty in predicted behavior times. When behavior
timing is more predictable, SigmaScheduling schedules decision points closer to
the predicted behavior time; when timing is less certain, SigmaScheduling
schedules decision points earlier, increasing the likelihood of timely
intervention. We evaluated SigmaScheduling using real-world data from 68
participants in a 10-week trial of Oralytics, a JITAI designed to improve daily
toothbrushing. SigmaScheduling increased the likelihood that decision points
preceded brushing events in at least 70% of cases, preserving opportunities to
intervene and impact behavior. Our results indicate that SigmaScheduling can
advance precision mHealth, particularly for JITAIs targeting time-sensitive,
habitual behaviors such as oral hygiene or dietary habits.

</details>


### [142] [Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case](https://arxiv.org/abs/2507.10803)
*JaMor Hairston,Ritvik Ranjan,Sahithi Lakamana,Anthony Spadaro,Selen Bozkurt,Jeanmarie Perrone,Abeed Sarker*

Key words: 主题分析、大型语言模型、自然语言处理、定性分析、社交媒体、提示工程、公共卫生

TL;DR: 大型语言模型（LLMs）在归纳主题分析任务中表现良好，可作为专家驱动的主题分析的可扩展补充。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探讨LLMs在需要深度解释和领域专业知识的社会媒体数据主题分析中的可行性。

Method: 通过二元分类任务评估五个LLMs，采用零样本、单样本和少样本提示策略，比较其与专家编码的性能。

Result: GPT-4o在少样本提示下表现最佳（准确率90.9%，F1分数0.71），高流行主题的分析结果与专家分类接近。

Conclusion: 少样本LLM方法可自动化主题分析，为定性研究提供可扩展的辅助工具。

Abstract: Background Large language models (LLMs) face challenges in inductive thematic
analysis, a task requiring deep interpretive and domain-specific expertise. We
evaluated the feasibility of using LLMs to replicate expert-driven thematic
analysis of social media data. Methods Using two temporally non-intersecting
Reddit datasets on xylazine (n=286 and n=686, for model optimization and
validation, respectively) with twelve expert-derived themes, we evaluated five
LLMs against expert coding. We modeled the task as a series of binary
classifications, rather than a single, multi-label classification, employing
zero-, single-, and few-shot prompting strategies and measuring performance via
accuracy, precision, recall, and F1-score. Results On the validation set,
GPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score:
0.71). For high-prevalence themes, model-derived thematic distributions closely
mirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use:
16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based
approaches can automate thematic analyses, offering a scalable supplement for
qualitative research. Keywords: thematic analysis, large language models,
natural language processing, qualitative analysis, social media, prompt
engineering, public health

</details>


### [143] [AF-XRAY: Visual Explanation and Resolution of Ambiguity in Legal Argumentation Frameworks](https://arxiv.org/abs/2507.10831)
*Yilin Xia,Heng Zheng,Shawn Bowers,Bertram Ludäscher*

Key words: 论证框架,法律推理,可视化,开源工具,模糊性

TL;DR: AF-XRAY是一个开源工具包，用于探索、分析和可视化法律推理中的抽象论证框架，帮助非专家理解论证接受和模糊性的来源。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 在法律推理中，论证框架的形式化方法存在模糊性解释和论证接受问题，AF-XRAY旨在解决这些问题。

Method: AF-XRAY提供分层可视化、攻击边分类、重叠可视化和关键攻击集识别等功能。

Result: 工具能将模糊场景转化为明确的解决方案，揭示不同假设导致的结论差异。

Conclusion: AF-XRAY支持目的论法律推理，帮助用户理解论证的模糊性和解决方案。

Abstract: Argumentation frameworks (AFs) provide formal approaches for legal reasoning,
but identifying sources of ambiguity and explaining argument acceptance remains
challenging for non-experts. We present AF-XRAY, an open-source toolkit for
exploring, analyzing, and visualizing abstract AFs in legal reasoning. AF-XRAY
introduces: (i) layered visualizations based on game-theoretic argument length
revealing well-founded derivation structures; (ii) classification of attack
edges by semantic roles (primary, secondary, blunders); (iii) overlay
visualizations of alternative 2-valued solutions on ambiguous 3-valued grounded
semantics; and (iv) identification of critical attack sets whose suspension
resolves undecided arguments. Through systematic generation of critical attack
sets, AF-XRAY transforms ambiguous scenarios into grounded solutions, enabling
users to pinpoint specific causes of ambiguity and explore alternative
resolutions. We use real-world legal cases (e.g., Wild Animals as modeled by
Bench-Capon) to show that our tool supports teleological legal reasoning by
revealing how different assumptions lead to different justified conclusions.

</details>


### [144] [NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization](https://arxiv.org/abs/2507.10894)
*Zongtao He,Liuyi Wang,Lu Chen,Chengju Liu,Qijun Chen*

Key words: language-guided navigation, NavComposer, NavInstrCritic, semantic entities, annotation-free evaluation

TL;DR: 论文提出NavComposer框架，自动生成高质量导航指令，并引入NavInstrCritic评估系统，解决大范围研究中的指令质量和评估问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有专家指令数量有限，合成注释质量不足，难以支撑大规模研究。

Method: NavComposer分解语义实体并重组为自然语言指令，支持模块化集成先进技术；NavInstrCritic以无注释方式评估指令质量。

Result: 实验证明方法有效，提升了指令的丰富性和准确性。

Conclusion: 方法解耦指令生成与评估，支持更可扩展和通用的研究。

Abstract: Language-guided navigation is a cornerstone of embodied AI, enabling agents
to interpret language instructions and navigate complex environments. However,
expert-provided instructions are limited in quantity, while synthesized
annotations often lack quality, making them insufficient for large-scale
research. To address this, we propose NavComposer, a novel framework for
automatically generating high-quality navigation instructions. NavComposer
explicitly decomposes semantic entities such as actions, scenes, and objects,
and recomposes them into natural language instructions. Its modular
architecture allows flexible integration of state-of-the-art techniques, while
the explicit use of semantic entities enhances both the richness and accuracy
of instructions. Moreover, it operates in a data-agnostic manner, supporting
adaptation to diverse navigation trajectories without domain-specific training.
Complementing NavComposer, we introduce NavInstrCritic, a comprehensive
annotation-free evaluation system that assesses navigation instructions on
three dimensions: contrastive matching, semantic consistency, and linguistic
diversity. NavInstrCritic provides a holistic evaluation of instruction
quality, addressing limitations of traditional metrics that rely heavily on
expert annotations. By decoupling instruction generation and evaluation from
specific navigation agents, our method enables more scalable and generalizable
research. Extensive experiments provide direct and practical evidence for the
effectiveness of our method.

</details>


### [145] [Lessons Learned from Evaluation of LLM based Multi-agents in Safer Therapy Recommendation](https://arxiv.org/abs/2507.10911)
*Yicong Wu,Ting Chen,Irit Hochberg,Zhoujian Sun,Ruth Edry,Zhengxing Huang,Mor Peleg*

Key words: 慢性病,多病症,大型语言模型,多代理系统,治疗推荐

TL;DR: 该研究探讨了基于大型语言模型（LLM）的多代理系统（MAS）在多病症慢性患者治疗推荐中的可行性和价值，结果显示单一代理GP表现与多学科团队（MDT）相当，但建议存在不完整和不必要的用药问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 多病症慢性患者的治疗推荐由于治疗冲突的风险而具有挑战性，现有决策支持系统存在可扩展性限制。研究旨在评估LLM-MAS模拟MDT决策的潜力。

Method: 设计了单一代理和多代理框架（MAS），模拟MDT决策，通过LLM代理间的讨论解决医学冲突。系统在多病症患者的治疗规划任务中进行了评估，并与单一代理方法和真实基准进行了比较。

Result: 当前LLM下，单一代理GP表现与MDT相当。最佳模型提供了满足所有临床目标的正确建议，但建议不完整，部分模型还提出了不必要的用药，导致冲突。

Conclusion: LLM-MAS在多病症治疗推荐中显示出潜力，但仍需改进建议的完整性和减少不必要的用药。

Abstract: Therapy recommendation for chronic patients with multimorbidity is
challenging due to risks of treatment conflicts. Existing decision support
systems face scalability limitations. Inspired by the way in which general
practitioners (GP) manage multimorbidity patients, occasionally convening
multidisciplinary team (MDT) collaboration, this study investigated the
feasibility and value of using a Large Language Model (LLM)-based multi-agent
system (MAS) for safer therapy recommendations. We designed a single agent and
a MAS framework simulating MDT decision-making by enabling discussion among LLM
agents to resolve medical conflicts. The systems were evaluated on therapy
planning tasks for multimorbidity patients using benchmark cases. We compared
MAS performance with single-agent approaches and real-world benchmarks. An
important contribution of our study is the definition of evaluation metrics
that go beyond the technical precision and recall and allow the inspection of
clinical goals met and medication burden of the proposed advices to a gold
standard benchmark. Our results show that with current LLMs, a single agent GP
performs as well as MDTs. The best-scoring models provide correct
recommendations that address all clinical goals, yet the advices are
incomplete. Some models also present unnecessary medications, resulting in
unnecessary conflicts between medication and conditions or drug-drug
interactions.

</details>


### [146] [Enhancing Safe and Controllable Protein Generation via Knowledge Preference Optimization](https://arxiv.org/abs/2507.10923)
*Yuhao Wang,Keyan Ding,Kehua Feng,Zeyuan Wang,Ming Qin,Xiaotong Li,Qiang Zhang,Huajun Chen*

Key words: 蛋白质语言模型, 知识图谱, 强化学习, 生物安全, 功能优化

TL;DR: 论文提出了一种知识引导的偏好优化（KPO）框架，通过蛋白质安全知识图谱整合先验知识，减少生成有害蛋白质序列的风险，同时保持高功能性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 蛋白质语言模型在功能优化和从头设计中表现出色，但可能生成有害序列（如增强病毒传播性或逃避免疫反应的蛋白质），引发生物安全和伦理问题。

Method: KPO框架结合知识图谱与图剪枝策略识别偏好序列，并通过强化学习最小化有害蛋白质的生成风险。

Result: 实验表明KPO显著降低有害序列生成的几率，同时保持功能高效，为生物技术中的生成模型提供了安全保证。

Conclusion: KPO为解决蛋白质语言模型的安全和伦理挑战提供了一种有效方法，适用于生物技术应用。

Abstract: Protein language models have emerged as powerful tools for sequence
generation, offering substantial advantages in functional optimization and
denovo design. However, these models also present significant risks of
generating harmful protein sequences, such as those that enhance viral
transmissibility or evade immune responses. These concerns underscore critical
biosafety and ethical challenges. To address these issues, we propose a
Knowledge-guided Preference Optimization (KPO) framework that integrates prior
knowledge via a Protein Safety Knowledge Graph. This framework utilizes an
efficient graph pruning strategy to identify preferred sequences and employs
reinforcement learning to minimize the risk of generating harmful proteins.
Experimental results demonstrate that KPO effectively reduces the likelihood of
producing hazardous sequences while maintaining high functionality, offering a
robust safety assurance framework for applying generative models in
biotechnology.

</details>


### [147] [Modeling Habitat Shifts: Integrating Convolutional Neural Networks and Tabular Data for Species Migration Prediction](https://arxiv.org/abs/2507.10993)
*Emir Durakovic,Min-Hong Shih*

Key words: 鸟类分布,卷积神经网络,气候变化,卫星图像,生态数据

TL;DR: 该论文提出了一种结合卷积神经网络(CNN)和表格数据的方法，通过卫星图像和环境特征来预测鸟类在不同气候中的分布情况，准确率达到85%。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 由于气候变化导致栖息地范围发生变化，传统的地理位置已不再适用，因此需要一种准确的方法来预测鸟类在不同栖息地的分布情况。

Method: 结合CNN（用于捕捉空间特征如森林、水体和城市化）和表格数据（使用生态和地理数据）来建模鸟类的存在情况。

Result: 两种系统的预测准确率平均达到85%，为理解鸟类迁徙提供了可扩展且可靠的方法。

Conclusion: 该研究为气候变化背景下的鸟类分布预测提供了一种有效的解决方案。

Abstract: Due to climate-induced changes, many habitats are experiencing range shifts
away from their traditional geographic locations (Piguet, 2011). We propose a
solution to accurately model whether bird species are present in a specific
habitat through the combination of Convolutional Neural Networks (CNNs)
(O'Shea, 2015) and tabular data. Our approach makes use of satellite imagery
and environmental features (e.g., temperature, precipitation, elevation) to
predict bird presence across various climates. The CNN model captures spatial
characteristics of landscapes such as forestation, water bodies, and
urbanization, whereas the tabular method uses ecological and geographic data.
Both systems predict the distribution of birds with an average accuracy of 85%,
offering a scalable but reliable method to understand bird migration.

</details>


### [148] [Personalized Exercise Recommendation with Semantically-Grounded Knowledge Tracing](https://arxiv.org/abs/2507.11060)
*Yilmazcan Ozyurt,Tunaberk Almaci,Stefan Feuerriegel,Mrinmaya Sachan*

Key words: 个性化推荐, 知识追踪, 强化学习, 语义建模

TL;DR: ExRec是一个结合语义知识追踪的个性化习题推荐框架，通过端到端流程优化推荐效果，并在数学学习中验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有习题推荐方法常忽略习题的语义内容和学习的结构化进展，ExRec旨在解决这一问题。

Method: 结合知识追踪（KT）和强化学习（RL），引入基于模型的估值估计（MVE），并优化端到端流程。

Result: 在数学学习任务中，ExRec表现出对新问题的鲁棒性，并能生成可解释的学习轨迹。

Conclusion: 知识追踪引导的强化学习在教育个性化中具有显著潜力。

Abstract: We introduce ExRec, a general framework for personalized exercise
recommendation with semantically-grounded knowledge tracing. Our method builds
on the observation that existing exercise recommendation approaches simulate
student performance via knowledge tracing (KT) but they often overlook two key
aspects: (a) the semantic content of questions and (b) the sequential,
structured progression of student learning. To address this, our ExRec presents
an end-to-end pipeline, from annotating the KCs of questions and learning their
semantic representations to training KT models and optimizing several
reinforcement learning (RL) methods. Moreover, we improve standard
Q-learning-based continuous RL methods via a tailored model-based value
estimation (MVE) approach that directly leverages the components of KT model in
estimating cumulative knowledge improvement. We validate the effectiveness of
our ExRec using various RL methods across four real-world tasks with different
educational goals in online math learning. We further show that ExRec
generalizes robustly to new, unseen questions and that it produces
interpretable student learning trajectories. Together, our findings highlight
the promise of KT-guided RL for effective personalization in education.

</details>


### [149] [Tactical Decision for Multi-UGV Confrontation with a Vision-Language Model-Based Commander](https://arxiv.org/abs/2507.11079)
*Li Wang,Qizhen Wu,Lei Chen*

Key words: 多无人地面车辆对抗、视觉语言模型、战略推理、可解释性、智能感知

TL;DR: 论文提出了一种基于视觉-语言模型的指挥系统，用于解决多无人地面车辆对抗中的智能感知至决策推理问题，相较于传统方法在适应性和可解释性上表现更优。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 在多无人地面车辆对抗中，传统的基于规则的方法在复杂瞬变的战场环境中表现脆弱，而现有的强化学习方法因缺乏可解释性主要关注动作操纵而非战略决策。

Method: 结合视觉语言模型进行场景理解和轻量级大语言模型进行战略推理，实现在共享语义空间中的统一感知与决策。

Result: 仿真和消融实验表明，该方法相较于基线模型胜率超过80%。

Conclusion: 该研究为自主对抗中的智能感知至决策推理提供了一种强大且可解释的解决方案。

Abstract: In multiple unmanned ground vehicle confrontations, autonomously evolving
multi-agent tactical decisions from situational awareness remain a significant
challenge. Traditional handcraft rule-based methods become vulnerable in the
complicated and transient battlefield environment, and current reinforcement
learning methods mainly focus on action manipulation instead of strategic
decisions due to lack of interpretability. Here, we propose a vision-language
model-based commander to address the issue of intelligent
perception-to-decision reasoning in autonomous confrontations. Our method
integrates a vision language model for scene understanding and a lightweight
large language model for strategic reasoning, achieving unified perception and
decision within a shared semantic space, with strong adaptability and
interpretability. Unlike rule-based search and reinforcement learning methods,
the combination of the two modules establishes a full-chain process, reflecting
the cognitive process of human commanders. Simulation and ablation experiments
validate that the proposed approach achieves a win rate of over 80% compared
with baseline models.

</details>


### [150] [Function-to-Style Guidance of LLMs for Code Translation](https://arxiv.org/abs/2507.11083)
*Longhui Zhang,Bin Wang,Jiahao Wang,Xiaofeng Zhao,Min Zhang,Hao Yang,Meishan Zhang,Yu Li,Jing Li,Jun Yu,Min Zhang*

Key words: 代码翻译,LLMs,功能学习,风格学习,基准测试

TL;DR: 论文提出F2STrans方法，通过功能学习和风格学习两阶段提升LLMs的代码翻译性能，并在新基准测试中显著优于现有模型。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有的LLMs在代码翻译中虽表现出色，但翻译的正确性和可读性仍不足，限制了其在实际开发中的应用。

Method: F2STrans采用两阶段方法：功能学习使用高质量代码对优化翻译正确性；风格学习通过正负样本提升可读性。

Result: 实验表明，F2STrans在新基准和现有数据集上显著提升性能，Qwen-1.5B甚至超越Qwen-32B和GPT-4。

Conclusion: F2STrans通过功能与风格结合，有效解决了代码翻译的正确性和可读性问题，具有实际应用潜力。

Abstract: Large language models (LLMs) have made significant strides in code
translation tasks. However, ensuring both the correctness and readability of
translated code remains a challenge, limiting their effective adoption in
real-world software development. In this work, we propose F2STrans, a
function-to-style guiding paradigm designed to progressively improve the
performance of LLMs in code translation. Our approach comprises two key stages:
(1) Functional learning, which optimizes translation correctness using
high-quality source-target code pairs mined from online programming platforms,
and (2) Style learning, which improves translation readability by incorporating
both positive and negative style examples. Additionally, we introduce a novel
code translation benchmark that includes up-to-date source code, extensive test
cases, and manually annotated ground-truth translations, enabling comprehensive
functional and stylistic evaluations. Experiments on both our new benchmark and
existing datasets demonstrate that our approach significantly improves code
translation performance. Notably, our approach enables Qwen-1.5B to outperform
prompt-enhanced Qwen-32B and GPT-4 on average across 20 diverse code
translation scenarios.

</details>


### [151] [AI Agent Architecture for Decentralized Trading of Alternative Assets](https://arxiv.org/abs/2507.11117)
*Ailiya Borjigin,Cong He,Charles CC Lee,Wei Zhou*

Key words: 分散式交易、物理资产代币化、AI代理、区块链、合规性、流动性、风险管理

TL;DR: GoldMine OS是一个基于AI代理的分散式交易架构，用于实现物理黄金的代币化与区块链交换，结合链上智能合约与链下AI代理，满足合规性、流动性和风险管理需求，实验表现优异。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究旨在解决物理资产（如黄金）在区块链上交易的合规性、流动性和风险管理问题，通过AI代理自动化提升效率与安全性。

Method: 采用链上智能合约与链下AI代理（合规、代币发行、做市、风控）协作架构，通过模拟和试点部署评估系统性能。

Result: 原型系统实现1.2秒内按需代币发行，做市代理在波动市场中维持0.5%以内价差，故障注入测试显示系统具备强韧性。

Conclusion: AI代理驱动的分散式交易架构能满足高性能与安全性需求，有望推动非流动性资产的民主化访问。

Abstract: Decentralized trading of real-world alternative assets (e.g., gold) requires
bridging physical asset custody with blockchain systems while meeting strict
requirements for compliance, liquidity, and risk management. We present
GoldMine OS, a research oriented architecture that employs multiple specialized
AI agents to automate and secure the tokenization and exchange of physical gold
into a blockchain based stablecoin ("OZ"). Our approach combines on chain smart
contracts for critical risk controls with off chain AI agents for decision
making, blending the transparency and reliability of blockchains with the
flexibility of AI driven automation. We describe four cooperative agents
(Compliance, Token Issuance, Market Making, and Risk Control) and a
coordinating core, and evaluate the system through simulation and a controlled
pilot deployment. In experiments the prototype delivers on demand token
issuance in under 1.2 s, more than 100 times faster than manual workflows. The
Market Making agent maintains tight liquidity with spreads often below 0.5
percent even under volatile conditions. Fault injection tests show resilience:
an oracle price spoofing attack is detected and mitigated within 10 s, and a
simulated vault mis reporting halts issuance immediately with minimal user
impact. The architecture scales to 5000 transactions per second with 10000
concurrent users in benchmarks. These results indicate that an AI agent based
decentralized exchange for alternative assets can satisfy rigorous performance
and safety requirements. We discuss broader implications for democratizing
access to traditionally illiquid assets and explain how our governance model --
multi signature agent updates and on chain community voting on risk parameters
-- provides ongoing transparency, adaptability, and formal assurance of system
integrity.

</details>


### [152] [Defining neurosymbolic AI](https://arxiv.org/abs/2507.11127)
*Lennert De Smet,Luc De Raedt*

Key words: 神经符号AI, 正式定义, 推理, 逻辑函数, 置信函数

TL;DR: 本文提出了一个神经符号AI的正式定义，抽象了其关键成分，并展示了该定义如何涵盖代表性的神经符号AI系统。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 神经符号AI领域缺乏一个普遍接受的正式定义来描述神经符号模型和推理的本质。

Method: 作者通过将神经符号推理定义为逻辑函数和置信函数的积的积分，引入了一个正式的神经符号AI定义。

Result: 提出的定义能够抽象化关键的神经符号AI系统，涵盖其代表性实例。

Conclusion: 该正式定义填补了神经符号AI领域的理论空白，为未来的研究和系统开发提供了基础。

Abstract: Neurosymbolic AI focuses on integrating learning and reasoning, in
particular, on unifying logical and neural representations. Despite the
existence of an alphabet soup of neurosymbolic AI systems, the field is lacking
a generally accepted formal definition of what neurosymbolic models and
inference really are. We introduce a formal definition for neurosymbolic AI
that makes abstraction of its key ingredients. More specifically, we define
neurosymbolic inference as the computation of an integral over a product of a
logical and a belief function. We show that our neurosymbolic AI definition
makes abstraction of key representative neurosymbolic AI systems.

</details>


### [153] [Collaborative Trustworthiness for Good Decision Making in Autonomous Systems](https://arxiv.org/abs/2507.11135)
*Selma Saidi,Omar Laimona,Christoph Schmickler,Dirk Ziegenbein*

Key words: 自主系统，信任度，协作决策，二元决策图（BDDs），社会认识论

TL;DR: 本文提出了一种基于协作的方法，通过利用自主系统的不同质量属性（如感知质量）来提高决策的信任度和可靠性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 自主系统在动态复杂环境中确保安全和正确行为的挑战尚未解决，尤其是在冲突信息下如何做出可信决策。

Method: 利用自主系统的质量属性（如感知质量）来评估信任度，借鉴社会认识论定义聚合与传播规则，并使用二元决策图（BDDs）进行高效计算。

Result: 提出的方法能够提高决策的信任度和可靠性，并通过BDDs实现高效推理。

Conclusion: 该方法为自主系统在复杂环境中的可信决策提供了新思路，同时优化了计算效率。

Abstract: Autonomous systems are becoming an integral part of many application domains,
like in the mobility sector. However, ensuring their safe and correct behaviour
in dynamic and complex environments remains a significant challenge, where
systems should autonomously make decisions e.g., about manoeuvring. We propose
in this paper a general collaborative approach for increasing the level of
trustworthiness in the environment of operation and improve reliability and
good decision making in autonomous system. In the presence of conflicting
information, aggregation becomes a major issue for trustworthy decision making
based on collaborative data sharing. Unlike classical approaches in the
literature that rely on consensus or majority as aggregation rule, we exploit
the fact that autonomous systems have different quality attributes like
perception quality. We use this criteria to determine which autonomous systems
are trustworthy and borrow concepts from social epistemology to define
aggregation and propagation rules, used for automated decision making. We use
Binary Decision Diagrams (BDDs) as formal models for beliefs aggregation and
propagation, and formulate reduction rules to reduce the size of the BDDs and
allow efficient computation structures for collaborative automated reasoning.

</details>


### [154] [Fine-grained Timing Analysis of Digital Integrated Circuits in Answer Set Programming](https://arxiv.org/abs/2507.11150)
*Alessandro Bertagnon,Marcello Dalpasso,Michele Favalli,Marco Gavanelli*

Key words: 集成电路设计, 最大延迟, 静态时序分析, 答案集编程, 硬件优化

TL;DR: 该论文探讨了在集成电路设计中精确计算组合模块的最大延迟问题，提出了使用ASP（答案集编程）来解决这一计算难题，并证明了其可行性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 在同步系统中，最大延迟直接影响系统性能，而传统的静态时序分析只能提供近似值，可能导致性能未达最优。因此，需要精确计算最大延迟。

Method: 采用答案集编程（ASP）对问题进行建模，并提出非平凡的编码方法。

Result: 实验结果表明，ASP是一种可行的解决方案，能够有效处理硬件设计中的复杂问题。

Conclusion: ASP能够精确计算最大延迟，为硬件设计提供了一种新的优化方法。

Abstract: In the design of integrated circuits, one critical metric is the maximum
delay introduced by combinational modules within the circuit. This delay is
crucial because it represents the time required to perform a computation: in an
Arithmetic-Logic Unit it represents the maximum time taken by the circuit to
perform an arithmetic operation. When such a circuit is part of a larger,
synchronous system, like a CPU, the maximum delay directly impacts the maximum
clock frequency of the entire system. Typically, hardware designers use Static
Timing Analysis to compute an upper bound of the maximum delay because it can
be determined in polynomial time. However, relying on this upper bound can lead
to suboptimal processor speeds, thereby missing performance opportunities. In
this work, we tackle the challenging task of computing the actual maximum
delay, rather than an approximate value. Since the problem is computationally
hard, we model it in Answer Set Programming (ASP), a logic language featuring
extremely efficient solvers. We propose non-trivial encodings of the problem
into ASP. Experimental results show that ASP is a viable solution to address
complex problems in hardware design.

</details>


### [155] [DuetGraph: Coarse-to-Fine Knowledge Graph Reasoning with Dual-Pathway Global-Local Fusion](https://arxiv.org/abs/2507.11229)
*Jin Li,Zezhong Ding,Xike Xie*

Key words: 知识图谱,推理,全局-局部融合,过度平滑,DuetGraph

TL;DR: DuetGraph通过双路径全局-局部融合和粗到细优化策略，解决了知识图谱推理中的分数过度平滑问题，显著提升了推理效果和训练效率。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有知识图谱推理方法因分数过度平滑而模糊了正确与错误答案的区分，影响了推理效果。

Method: 提出DuetGraph，采用双路径全局-局部分离处理和粗到细优化策略。

Result: 在多个数据集上实现了SOTA性能，推理质量提升8.7%，训练效率加速1.8倍。

Conclusion: DuetGraph有效解决了过度平滑问题，提升了知识图谱推理的质量和效率。

Abstract: Knowledge graphs (KGs) are vital for enabling knowledge reasoning across
various domains. Recent KG reasoning methods that integrate both global and
local information have achieved promising results. However, existing methods
often suffer from score over-smoothing, which blurs the distinction between
correct and incorrect answers and hinders reasoning effectiveness. To address
this, we propose DuetGraph, a coarse-to-fine KG reasoning mechanism with
dual-pathway global-local fusion. DuetGraph tackles over-smoothing by
segregating -- rather than stacking -- the processing of local (via message
passing) and global (via attention) information into two distinct pathways,
preventing mutual interference and preserving representational discrimination.
In addition, DuetGraph introduces a coarse-to-fine optimization, which
partitions entities into high- and low-score subsets. This strategy narrows the
candidate space and sharpens the score gap between the two subsets, which
alleviates over-smoothing and enhances inference quality. Extensive experiments
on various datasets demonstrate that DuetGraph achieves state-of-the-art (SOTA)
performance, with up to an 8.7% improvement in reasoning quality and a
1.8$\times$ acceleration in training efficiency.

</details>


### [156] [Taming Uncertainty via Automation: Observing, Analyzing, and Optimizing Agentic AI Systems](https://arxiv.org/abs/2507.11277)
*Dany Moshkovich,Sergey Zeltyn*

Key words: 大型语言模型,智能代理系统,自动化运维,不确定性管理

TL;DR: 提出了AgentOps框架，用于观测、分析和优化基于LLM的智能代理系统，解决其不确定性带来的挑战，并通过自动化提升系统效能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 大型语言模型（LLM）在智能代理系统中的广泛应用引入了由概率推理、动态内存状态和灵活执行路径带来的新型不确定性，传统软件可观测性方法无法应对这些挑战。

Method: 提出了AgentOps框架，包括行为观测、指标收集、问题检测、根因分析、优化建议和运行时自动化六个阶段，服务于开发者、测试人员、SRE和业务用户等不同角色。

Result: 通过自动化管理不确定性，确保智能代理系统的安全、适应性和高效运行。

Conclusion: AgentOps框架通过系统化的自动化为智能代理系统的运维提供了有效支持，提升了其可靠性和适应性。

Abstract: Large Language Models (LLMs) are increasingly deployed within agentic
systems-collections of interacting, LLM-powered agents that execute complex,
adaptive workflows using memory, tools, and dynamic planning. While enabling
powerful new capabilities, these systems also introduce unique forms of
uncertainty stemming from probabilistic reasoning, evolving memory states, and
fluid execution paths. Traditional software observability and operations
practices fall short in addressing these challenges.
  This paper introduces AgentOps: a comprehensive framework for observing,
analyzing, optimizing, and automating operation of agentic AI systems. We
identify distinct needs across four key roles-developers, testers, site
reliability engineers (SREs), and business users-each of whom engages with the
system at different points in its lifecycle. We present the AgentOps Automation
Pipeline, a six-stage process encompassing behavior observation, metric
collection, issue detection, root cause analysis, optimized recommendations,
and runtime automation. Throughout, we emphasize the critical role of
automation in managing uncertainty and enabling self-improving AI systems-not
by eliminating uncertainty, but by taming it to ensure safe, adaptive, and
effective operation.

</details>


### [157] [Opus: A Prompt Intention Framework for Complex Workflow Generation](https://arxiv.org/abs/2507.11288)
*Théo Fagnoni,Mahsun Altin,Chia En Chung,Phillip Kingston,Alan Tuning,Dana O. Mohamed,Inès Adnani*

Key words: 工作流生成,大语言模型,意图捕捉,Opus框架

TL;DR: Opus Prompt Intention Framework通过引入中间意图捕捉层，显著提升了基于指令调整的大语言模型（LLMs）的复杂工作流生成质量。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 旨在解决直接根据用户查询生成工作流时复杂性和多意图问题，提高工作流的逻辑性和可扩展性。

Method: 提出了Opus工作流意图框架，包括从用户查询中提取工作流信号、转换为结构化工作流意图对象，并基于此生成工作流。

Result: 在1000对多意图查询-工作流对的合成基准测试中，框架显著提升了语义工作流相似性指标。

Conclusion: Opus Prompt Intention Framework有效提高了LLM生成工作流的质量，尤其在多意图场景中表现突出。

Abstract: This paper introduces the Opus Prompt Intention Framework, designed to
improve complex Workflow Generation with instruction-tuned Large Language
Models (LLMs). We propose an intermediate Intention Capture layer between user
queries and Workflow Generation, implementing the Opus Workflow Intention
Framework, which consists of extracting Workflow Signals from user queries,
interpreting them into structured Workflow Intention objects, and generating
Workflows based on these Intentions. Our results show that this layer enables
LLMs to produce logical and meaningful outputs that scale reliably as query
complexity increases. On a synthetic benchmark of 1,000 multi-intent
query-Workflow(s) pairs, applying the Opus Prompt Intention Framework to
Workflow Generation yields consistent improvements in semantic Workflow
similarity metrics. In this paper, we introduce the Opus Prompt Intention
Framework by applying the concepts of Workflow Signal and Workflow Intention to
LLM-driven Workflow Generation. We present a reproducible, customizable
LLM-based Intention Capture system to extract Workflow Signals and Workflow
Intentions from user queries. Finally, we provide empirical evidence that the
proposed system significantly improves Workflow Generation quality compared to
direct generation from user queries, particularly in cases of Mixed Intention
Elicitation.

</details>


### [158] [Contestability in Quantitative Argumentation](https://arxiv.org/abs/2507.11323)
*Xiang Yin,Nico Potyka,Antonio Rago,Timotheus Kampik,Francesca Toni*

Key words: 可争议性, EW-QBAF, G-RAE, 权重调整, AI决策

TL;DR: 本文研究了如何在边缘加权定量双极论证框架（EW-QBAF）中实现AI决策的可争议性，并提出了一种基于梯度的关系归因解释方法（G-RAE）来指导权重调整。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 为了确保AI驱动的决策与人类偏好一致，文章探讨了EW-QBAF在实现可争议性中的应用。

Method: 提出了G-RAE方法，量化主题论据强度对权重变化的敏感性，并通过迭代算法逐步调整权重。

Result: 实验表明，该方法能够有效解决EW-QBAF中的可争议性问题。

Conclusion: G-RAE和迭代算法为EW-QBAF中的权重调整提供了解释性指导，有助于实现AI决策的可争议性。

Abstract: Contestable AI requires that AI-driven decisions align with human
preferences. While various forms of argumentation have been shown to support
contestability, Edge-Weighted Quantitative Bipolar Argumentation Frameworks
(EW-QBAFs) have received little attention. In this work, we show how EW-QBAFs
can be deployed for this purpose. Specifically, we introduce the contestability
problem for EW-QBAFs, which asks how to modify edge weights (e.g., preferences)
to achieve a desired strength for a specific argument of interest (i.e., a
topic argument). To address this problem, we propose gradient-based relation
attribution explanations (G-RAEs), which quantify the sensitivity of the topic
argument's strength to changes in individual edge weights, thus providing
interpretable guidance for weight adjustments towards contestability. Building
on G-RAEs, we develop an iterative algorithm that progressively adjusts the
edge weights to attain the desired strength. We evaluate our approach
experimentally on synthetic EW-QBAFs that simulate the structural
characteristics of personalised recommender systems and multi-layer
perceptrons, and demonstrate that it can solve the problem effectively.

</details>


### [159] [CogDDN: A Cognitive Demand-Driven Navigation with Decision Optimization and Dual-Process Thinking](https://arxiv.org/abs/2507.11334)
*Yuehao Huang,Liang Liu,Shuangming Lei,Yukai Ma,Hao Su,Jianbiao Mei,Pengxiang Zhao,Yaqing Gu,Yong Liu,Jiajun Lv*

Key words: 移动机器人,需求驱动导航,视觉语言模型,快速和慢速思维系统,Chain of Thought推理

TL;DR: CogDDN是一个基于视觉语言模型（VLM）的框架，通过模拟人类认知和学习机制，结合快速和慢速思维系统，选择性地识别关键对象以满足用户需求，提高了机器人在未知环境中的导航和任务完成能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 机器人需要在未知和非结构化环境中导航和交互，传统的需求驱动导航（DDN）方法依赖预收集数据，限制了其在未见场景中的泛化能力。CogDDN旨在通过模拟人类认知机制解决这一问题。

Method: CogDDN整合了快速和慢速思维系统，语义对齐检测到的对象与指令，并通过双过程决策模块（启发式和解析式）优化决策过程，结合Chain of Thought（CoT）推理增强决策准确性。

Result: 在AI2Thor模拟器上使用ProcThor数据集的闭环评估中，CogDDN相比单视角相机方法提升了15%的导航准确性和适应性。

Conclusion: CogDDN通过模拟人类认知机制，显著提高了机器人在未知环境中的导航和任务完成能力，展示了强大的实际应用潜力。

Abstract: Mobile robots are increasingly required to navigate and interact within
unknown and unstructured environments to meet human demands. Demand-driven
navigation (DDN) enables robots to identify and locate objects based on
implicit human intent, even when object locations are unknown. However,
traditional data-driven DDN methods rely on pre-collected data for model
training and decision-making, limiting their generalization capability in
unseen scenarios. In this paper, we propose CogDDN, a VLM-based framework that
emulates the human cognitive and learning mechanisms by integrating fast and
slow thinking systems and selectively identifying key objects essential to
fulfilling user demands. CogDDN identifies appropriate target objects by
semantically aligning detected objects with the given instructions.
Furthermore, it incorporates a dual-process decision-making module, comprising
a Heuristic Process for rapid, efficient decisions and an Analytic Process that
analyzes past errors, accumulates them in a knowledge base, and continuously
improves performance. Chain of Thought (CoT) reasoning strengthens the
decision-making process. Extensive closed-loop evaluations on the AI2Thor
simulator with the ProcThor dataset show that CogDDN outperforms single-view
camera-only methods by 15%, demonstrating significant improvements in
navigation accuracy and adaptability. The project page is available at
https://yuehaohuang.github.io/CogDDN/.

</details>


### [160] [Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces](https://arxiv.org/abs/2507.11352)
*Yunhao Yang,Neel P. Bhatt,Christian Ellis,Alvaro Velasquez,Zhangyang Wang,Ufuk Topcu*

Key words: 神经符号框架, 物流决策, 不确定性量化, 交互式澄清, 轻量模型

TL;DR: 本文介绍了一种神经符号框架，结合自然语言对话的可访问性与目标解释的可验证保证，用于复杂物流决策。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 物流操作者常需快速调整计划，传统方法（如整数规划）速度慢且环境模型理想化，而大语言模型（LLM）易误解和幻觉，导致安全风险。

Method: 提出神经符号框架，将用户请求转为结构化规划，量化自身不确定性，并在信心不足时启动交互式澄清循环。轻量模型基于100个样本微调。

Result: 该框架性能超过GPT-4.1零样本表现，推理延迟降低近50%。

Conclusion: 验证了一条可实现实时、安全与用户对齐的复杂物流决策路径。

Abstract: Logistics operators, from battlefield coordinators rerouting airlifts ahead
of a storm to warehouse managers juggling late trucks, often face life-critical
decisions that demand both domain expertise and rapid and continuous
replanning. While popular methods like integer programming yield logistics
plans that satisfy user-defined logical constraints, they are slow and assume
an idealized mathematical model of the environment that does not account for
uncertainty. On the other hand, large language models (LLMs) can handle
uncertainty and promise to accelerate replanning while lowering the barrier to
entry by translating free-form utterances into executable plans, yet they
remain prone to misinterpretations and hallucinations that jeopardize safety
and cost. We introduce a neurosymbolic framework that pairs the accessibility
of natural-language dialogue with verifiable guarantees on goal interpretation.
It converts user requests into structured planning specifications, quantifies
its own uncertainty at the field and token level, and invokes an interactive
clarification loop whenever confidence falls below an adaptive threshold. A
lightweight model, fine-tuned on just 100 uncertainty-filtered examples,
surpasses the zero-shot performance of GPT-4.1 while cutting inference latency
by nearly 50%. These preliminary results highlight a practical path toward
certifiable, real-time, and user-aligned decision-making for complex logistics.

</details>


### [161] [Modeling Code: Is Text All You Need?](https://arxiv.org/abs/2507.11467)
*Daniel Nichols,Konstantinos Parasyris,Harshitha Menon,Brian R. Bartoldson,Giorgis Georgakoudis,Tal Ben-Nun,Abhinav Bhatele*

Key words: Code LLMs, 结构化分析, 代码生成, 图神经网络

TL;DR: 论文提出了一种结合代码文本和结构化形式建模的新方法，以弥补现有代码大型语言模型（Code LLMs）在结构化分析能力上的不足。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有的基于Transformer的代码LLMs在代码的结构化分析（如控制和数据流）能力有限，而现有结构化方法则缺乏生成能力和规模。

Method: 论文提出了一种新颖的方法，同时结合了代码文本和结构化形式的建模。

Result: 该方法旨在提升代码LLMs在结构化分析任务上的能力，同时保留其生成能力和规模优势。

Conclusion: 结合文本和结构化建模的方法有望弥补现有技术的不足，推动代码LLMs在更广泛任务中的应用。

Abstract: Code LLMs have become extremely popular recently for modeling source code
across a variety of tasks, such as generation, translation, and summarization.
However, transformer-based models are limited in their capabilities to reason
through structured, analytical properties of code, such as control and data
flow. Previous work has explored the modeling of these properties with
structured data and graph neural networks. However, these approaches lack the
generative capabilities and scale of modern LLMs. In this work, we introduce a
novel approach to combine the strengths of modeling both code as text and more
structured forms.

</details>


### [162] [Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety](https://arxiv.org/abs/2507.11473)
*Tomek Korbak,Mikita Balesni,Elizabeth Barnes,Yoshua Bengio,Joe Benton,Joseph Bloom,Mark Chen,Alan Cooney,Allan Dafoe,Anca Dragan,Scott Emmons,Owain Evans,David Farhi,Ryan Greenblatt,Dan Hendrycks,Marius Hobbhahn,Evan Hubinger,Geoffrey Irving,Erik Jenner,Daniel Kokotajlo,Victoria Krakovna,Shane Legg,David Lindner,David Luan,Aleksander Mądry,Julian Michael,Neel Nanda,Dave Orr,Jakub Pachocki,Ethan Perez,Mary Phuong,Fabien Roger,Joshua Saxe,Buck Shlegeris,Martín Soto,Eric Steinberger,Jasmine Wang,Wojciech Zaremba,Bowen Baker,Rohin Shah,Vlad Mikulik*

Key words: AI安全性,思维链监控,CoT,AI不良行为,前沿模型开发

TL;DR: 论文探讨了通过监控AI的思维链（CoT）来提高AI安全性，建议进一步研究CoT可监控性，并开发时考虑对CoT可监控性的影响。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究AI安全性问题，通过监控人类语言的思维链来预防AI的潜在不良行为。

Method: 提出监控AI的思维链（CoT）方法，评估其对AI不良行为的检测能力，并建议在开发中优先考虑CoT的可监控性。

Result: CoT监控虽不完美，但显示出潜在价值，值得进一步研究和应用。

Conclusion: 建议加强对CoT监控的研究和开发投入，同时提醒开发者注意技术选择对CoT可监控性的影响。

Abstract: AI systems that "think" in human language offer a unique opportunity for AI
safety: we can monitor their chains of thought (CoT) for the intent to
misbehave. Like all other known AI oversight methods, CoT monitoring is
imperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows
promise and we recommend further research into CoT monitorability and
investment in CoT monitoring alongside existing safety methods. Because CoT
monitorability may be fragile, we recommend that frontier model developers
consider the impact of development decisions on CoT monitorability.

</details>


### [163] [Perspective-Aware AI in Extended Reality](https://arxiv.org/abs/2507.11479)
*Daniel Platnick,Matti Gruener,Marjan Alirezaie,Kent Larson,Dava J. Newman,Hossein Rahnama*

Key words: AI, XR, Perspective-Aware AI, user modeling, immersive systems

TL;DR: PAiR框架通过整合Perspective-Aware AI（PAi）与XR，基于用户身份模型实现上下文感知的沉浸式体验。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前AI增强的XR系统因用户建模浅层和认知上下文有限而未能提供理想的适应性沉浸体验。

Method: 提出PAiR框架，利用Chronicles（基于多模态数字足迹的用户身份模型）在闭环系统中动态链接用户状态与沉浸环境。

Result: 通过Unity引擎的两个验证场景展示了PAiR的实用性。

Conclusion: PAiR通过嵌入基于视角的身份模型，为人机交互开辟了新方向。

Abstract: AI-enhanced Extended Reality (XR) aims to deliver adaptive, immersive
experiences-yet current systems fall short due to shallow user modeling and
limited cognitive context. We introduce Perspective-Aware AI in Extended
Reality (PAiR), a foundational framework for integrating Perspective-Aware AI
(PAi) with XR to enable interpretable, context-aware experiences grounded in
user identity. PAi is built on Chronicles: reasoning-ready identity models
learned from multimodal digital footprints that capture users' cognitive and
experiential evolution. PAiR employs these models in a closed-loop system
linking dynamic user states with immersive environments. We present PAiR's
architecture, detailing its modules and system flow, and demonstrate its
utility through two proof-of-concept scenarios implemented in the Unity-based
OpenDome engine. PAiR opens a new direction for human-AI interaction by
embedding perspective-based identity models into immersive systems.

</details>


### [164] [Illuminating the Three Dogmas of Reinforcement Learning under Evolutionary Light](https://arxiv.org/abs/2507.11482)
*Mani Hamidi,Terrence W. Deacon*

Key words: 强化学习, 演化理论, 代理问题, 奖励假设, 生物学习

TL;DR: 本文提出一个受开放演化理论启发的框架，重新审视强化学习的三个核心假设，并探讨其理论和应用意义。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 针对强化学习（RL）中长期存在的三个核心假设（关于代理定义、学习目标和奖励假设范围）提出修正，以改进理论和应用。

Method: 通过演化理论视角重新评估这三个假设，提出其适用于生物学习的可能性。首先验证演化动态在个体生命周期内的可行性。

Result: 论证了演化视角能丰富学习观点，解决奖励假设的局限性，但无法完全解决代理问题，需结合生命起源理论。

Conclusion: 提出整合生命起源的热力学理论，为生物系统中的代理和资源约束强化学习提供新基础。

Abstract: Three core tenets of reinforcement learning (RL)--concerning the definition
of agency, the objective of learning, and the scope of the reward
hypothesis--have been highlighted as key targets for conceptual revision, with
major implications for theory and application. We propose a framework, inspired
by open-ended evolutionary theory, to reconsider these three "dogmas." We
revisit each assumption and address related concerns raised alongside them. To
make our arguments relevant to RL as a model of biological learning, we first
establish that evolutionary dynamics can plausibly operate within living brains
over an individual's lifetime, and are not confined to cross-generational
processes. We begin by revisiting the second dogma, drawing on evolutionary
insights to enrich the "adaptation-rather-than-search" view of learning. We
then address the third dogma regarding the limits of the reward hypothesis,
using analogies from evolutionary fitness to illuminate the scalar reward vs.
multi-objective debate. After discussing practical implications for exploration
in RL, we turn to the first--and arguably most fundamental--issue: the absence
of a formal account of agency. We argue that unlike the other two problems, the
evolutionary paradigm alone cannot resolve the agency question, though it
gestures in a productive direction. We advocate integrating ideas from
origins-of-life theory, where the thermodynamics of sustenance and replication
offer promising foundations for understanding agency and resource-constrained
reinforcement learning in biological systems.

</details>


### [165] [DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering](https://arxiv.org/abs/2507.11527)
*Yinsheng Li,Zhen Dong,Yi Shao*

Key words: Large Language Model, DrafterBench, 技术绘图修订, 评估基准

TL;DR: 该论文提出了DrafterBench，一个用于评估LLM代理在技术绘图修订任务中的综合性能的开源基准。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 需要更多从工业角度（如土木工程）系统评估自动化代理的基准。

Method: DrafterBench包含12类任务、46个定制函数和1920个任务，评估LLM代理在复杂指令解读、知识利用和动态适应等方面的能力。

Result: DrafterBench能全面评估代理的多项能力，并提供任务准确性和错误统计的详细分析。

Conclusion: DrafterBench为LLM在工程应用中的集成提供了深入见解和改进目标。

Abstract: Large Language Model (LLM) agents have shown great potential for solving
real-world problems and promise to be a solution for tasks automation in
industry. However, more benchmarks are needed to systematically evaluate
automation agents from an industrial perspective, for example, in Civil
Engineering. Therefore, we propose DrafterBench for the comprehensive
evaluation of LLM agents in the context of technical drawing revision, a
representation task in civil engineering. DrafterBench contains twelve types of
tasks summarized from real-world drawing files, with 46 customized
functions/tools and 1920 tasks in total. DrafterBench is an open-source
benchmark to rigorously test AI agents' proficiency in interpreting intricate
and long-context instructions, leveraging prior knowledge, and adapting to
dynamic instruction quality via implicit policy awareness. The toolkit
comprehensively assesses distinct capabilities in structured data
comprehension, function execution, instruction following, and critical
reasoning. DrafterBench offers detailed analysis of task accuracy and error
statistics, aiming to provide deeper insight into agent capabilities and
identify improvement targets for integrating LLMs in engineering applications.
Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,
with the test set hosted at
https://huggingface.co/datasets/Eason666/DrafterBench.

</details>


### [166] [How Many Instructions Can LLMs Follow at Once?](https://arxiv.org/abs/2507.11538)
*Daniel Jaroslawicz,Brendan Whiting,Parth Shah,Karime Maamari*

Key words: LLM, 指令遵循, IFScale基准, 高指令密度, 商业报告

TL;DR: 论文分析了高指令密度下大型语言模型（LLM）的指令遵循能力，提出IFScale基准测试，发现现有模型在500条指令下的准确率最高仅68%，并揭示了模型性能下降的模式及误差类别。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有评测基准仅针对单一或少量指令任务，无法评估LLM在高指令密度下的表现。

Method: 引入包含500条关键词包含指令的IFScale基准，用于商业报告写作任务，测量指令密度增加时性能下降情况。

Result: 测试20个前沿模型发现，最高准确率仅68%，模型规模和推理能力与性能下降模式相关。

Conclusion: 高指令密度下LLM表现存在明显局限，结果对实际应用的提示设计和性能-延迟权衡具有指导意义。

Abstract: Production-grade LLM systems require robust adherence to dozens or even
hundreds of instructions simultaneously. However, the instruction-following
capabilities of LLMs at high instruction densities have not yet been
characterized, as existing benchmarks only evaluate models on tasks with a
single or few instructions. We introduce IFScale, a simple benchmark of 500
keyword-inclusion instructions for a business report writing task to measure
how instruction-following performance degrades as instruction density
increases. We evaluate 20 state-of-the-art models across seven major providers
and find that even the best frontier models only achieve 68% accuracy at the
max density of 500 instructions. Our analysis reveals model size and reasoning
capability to correlate with 3 distinct performance degradation patterns, bias
towards earlier instructions, and distinct categories of instruction-following
errors. Our insights can help inform design of instruction-dense prompts in
real-world applications and highlight important performance-latency tradeoffs.
We open-source the benchmark and all results for further analysis at
https://distylai.github.io/IFScale.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [167] [Quantitative multi-metabolite imaging of Parkinson's disease using AI boosted molecular MRI](https://arxiv.org/abs/2507.11329)
*Hagar Shmuely,Michal Rivlin,Or Perlman*

Key words: Parkinson disease, molecular MRI, deep learning, multi-metabolite quantification, biomarkers

TL;DR: 该论文提出了一种结合快速分子MRI采集与深度学习重建的方法，用于定量分析PD模型中的多种代谢物，并验证了其作为PD生物标志物的潜力。

<details>
  <summary>Details</summary>

Main category: physics.med-ph

Motivation: 传统PD分子成像方法依赖放射性同位素、扫描时间长或分辨率低，而新型MRI技术虽能提供生化信息，但对比度半定量且非特异性，因此需要更高效、定量的成像方法。

Method: 结合快速分子MRI采集与深度学习重建技术，定量分析急性MPTP小鼠模型中的谷氨酸、可移动蛋白、半固体及可移动大分子。

Result: 定量参数图与组织学和MR光谱学结果一致，表明半固体磁化转移、酰胺及脂肪链接力核欧沃豪斯效应质子体积分数可作为PD生物标志物。

Conclusion: 该方法为PD提供了高效、定量的分子成像解决方案，并验证了新型生物标志物的潜力。

Abstract: Traditional approaches for molecular imaging of Parkinson's disease (PD) in
vivo require radioactive isotopes, lengthy scan times, or deliver only low
spatial resolution. Recent advances in saturation transfer-based PD magnetic
resonance imaging (MRI) have provided biochemical insights, although the image
contrast is semi-quantitative and nonspecific. Here, we combined a rapid
molecular MRI acquisition paradigm with deep learning based reconstruction for
multi-metabolite quantification of glutamate, mobile proteins, semisolid, and
mobile macromolecules in an acute MPTP
(1-methyl-4-phenyl-1,2,3,6-tetrahydropyridine) mouse model. The quantitative
parameter maps are in general agreement with the histology and MR spectroscopy,
and demonstrate that semisolid magnetization transfer (MT), amide, and
aliphatic relayed nuclear Overhauser effect (rNOE) proton volume fractions may
serve as PD biomarkers.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [168] [Kernel Learning for Mean-Variance Trading Strategies](https://arxiv.org/abs/2507.10701)
*Owen Futter,Nicola Muca Cirone,Blanka Horvath*

Key words: 核方法, 交易策略, 均值-方差优化, RKHS, 非马尔可夫方法

TL;DR: 本文提出了一种基于核的动态路径依赖交易策略框架，通过核希尔伯特空间参数化策略，在均值-方差优化下显著优于传统方法。

<details>
  <summary>Details</summary>

Main category: q-fin.TR

Motivation: 针对资产动态或预测信号存在时间依赖性的问题，研究提出一种灵活且非马尔可夫的方法来优化投资组合。

Method: 利用再生核希尔伯特空间（RKHS）参数化交易策略，并与基于签名的方法进行比较。

Result: 核方法与签名方法均显著优于传统马尔可夫方法，且在合成数据和市场数据中表现优异。

Conclusion: 通过核方法提供了建模灵活性，保留了闭式解，并成为基于梯度优化的替代方案。

Abstract: In this article, we develop a kernel-based framework for constructing
dynamic, pathdependent trading strategies under a mean-variance optimisation
criterion. Building on the theoretical results of (Muca Cirone and Salvi,
2025), we parameterise trading strategies as functions in a reproducing kernel
Hilbert space (RKHS), enabling a flexible and non-Markovian approach to optimal
portfolio problems. We compare this with the signature-based framework of
(Futter, Horvath, Wiese, 2023) and demonstrate that both significantly
outperform classical Markovian methods when the asset dynamics or predictive
signals exhibit temporal dependencies for both synthetic and market-data
examples. Using kernels in this context provides significant modelling
flexibility, as the choice of feature embedding can range from randomised
signatures to the final layers of neural network architectures. Crucially, our
framework retains closed-form solutions and provides an alternative to
gradient-based optimisation.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [169] [Artificial Finance: How AI Thinks About Money](https://arxiv.org/abs/2507.10933)
*Orhan Erdem,Ragavi Pobbathi Ashok*

Key words: 大型语言模型, 财务决策, 风险偏好, 跨文化比较

TL;DR: 本文通过比较大型语言模型（LLMs）与全球人类参与者在财务决策问题上的回答，揭示了LLMs在风险偏好、时间偏好和跨文化相似性方面的行为模式。

<details>
  <summary>Details</summary>

Main category: econ.GN

Motivation: 探索LLMs在财务决策中如何模拟人类行为，并揭示潜在的文化和训练对其输出的影响。

Method: 向七种领先的LLMs（包括GPT系列模型、Gemini 2.0 Flash和DeepSeek R1）提出财务决策问题，并将其回答与来自53个国家的人类数据集进行比较。

Result: 1. LLMs倾向于风险中性决策；2. 在时间偏好上偶尔表现出与规范性推理不一致的行为；3. LLMs的集体回答与坦桑尼亚参与者的回答最为相似。

Conclusion: LLMs在财务决策中表现出独特的模式，其行为受文化和训练数据影响。

Abstract: In this paper, we explore how large language models (LLMs) approach financial
decision-making by systematically comparing their responses to those of human
participants across the globe. We posed a set of commonly used financial
decision-making questions to seven leading LLMs, including five models from the
GPT series(GPT-4o, GPT-4.5, o1, o3-mini), Gemini 2.0 Flash, and DeepSeek R1. We
then compared their outputs to human responses drawn from a dataset covering 53
nations. Our analysis reveals three main results. First, LLMs generally exhibit
a risk-neutral decision-making pattern, favoring choices aligned with expected
value calculations when faced with lottery-type questions. Second, when
evaluating trade-offs between present and future, LLMs occasionally produce
responses that appear inconsistent with normative reasoning. Third, when we
examine cross-national similarities, we find that the LLMs' aggregate responses
most closely resemble those of participants from Tanzania. These findings
contribute to the understanding of how LLMs emulate human-like decision
behaviors and highlight potential cultural and training influences embedded
within their outputs.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [170] [LiLM-RDB-SFC: Lightweight Language Model with Relational Database-Guided DRL for Optimized SFC Provisioning](https://arxiv.org/abs/2507.10903)
*Parisa Fard Moshiri,Xinyu Zhu,Poonam Lohan,Burak Kantarci,Emil Janulewicz*

Key words: 服务功能链,虚拟网络功能,轻量级语言模型,网络功能虚拟化

TL;DR: 论文提出了一种结合轻量级语言模型和关系数据库的新方法LiLM-RDB-SFC，以提高SDN和NFV环境中服务功能链管理的效率和适应性。

<details>
  <summary>Details</summary>

Main category: cs.NI

Motivation: 现有深度强化学习在网络决策中的适应性不足，尤其是在不可预测的网络条件下。

Method: 通过结合BART和FLAN-T5两种轻量级语言模型，解析网络数据并支持多样化查询。

Result: FLAN-T5表现优于BART，测试损失更低（0.00161 vs. 0.00734），准确率更高（94.79% vs. 80.2%），处理时间更短（2h 2min vs. 2h 38min）。与SQLCoder相比，FLAN-T5在保持相同准确率的情况下，处理时间减少96%。

Conclusion: LiLM-RDB-SFC通过轻量级语言模型显著提升了服务功能链管理的效率和性能。

Abstract: Effective management of Service Function Chains (SFCs) and optimal Virtual
Network Function (VNF) placement are critical challenges in modern
Software-Defined Networking (SDN) and Network Function Virtualization (NFV)
environments. Although Deep Reinforcement Learning (DRL) is widely adopted for
dynamic network decision-making, its inherent dependency on structured data and
fixed action rules often limits adaptability and responsiveness, particularly
under unpredictable network conditions. This paper introduces LiLM-RDB-SFC, a
novel approach combining Lightweight Language Model (LiLM) with Relational
Database (RDB) to answer network state queries to guide DRL model for efficient
SFC provisioning. Our proposed approach leverages two LiLMs, Bidirectional and
Auto-Regressive Transformers (BART) and the Fine-tuned Language Net T5
(FLAN-T5), to interpret network data and support diverse query types related to
SFC demands, data center resources, and VNF availability. Results demonstrate
that FLAN-T5 outperforms BART with a lower test loss (0.00161 compared to
0.00734), higher accuracy (94.79% compared to 80.2%), and less processing time
(2h 2min compared to 2h 38min). Moreover, when compared to the large language
model SQLCoder, FLAN-T5 matches the accuracy of SQLCoder while cutting
processing time by 96% (SQLCoder: 54 h 43 min; FLAN-T5: 2 h 2 min).

</details>


### [171] [Improving Wi-Fi Network Performance Prediction with Deep Learning Models](https://arxiv.org/abs/2507.11168)
*Gabriele Formis,Amanda Ericson,Stefan Forsstrom,Kyi Thar,Gianluca Cena,Stefano Scanzio*

Key words: 机器学习, Wi-Fi, 信道质量预测, 卷积神经网络, 工业应用

TL;DR: 利用机器学习技术预测Wi-Fi网络的信道质量，通过调整通信参数优化工业应用网络性能，卷积神经网络在计算效率和内存消耗上表现更优。

<details>
  <summary>Details</summary>

Main category: cs.NI

Motivation: 工业及关键任务应用对无线网络的鲁棒性、可靠性和确定性需求增加，驱动了新方法的创新。

Method: 分析卷积神经网络和长短期记忆网络在真实Wi-Fi多信道数据集上的性能，比较预测准确性和计算复杂度。

Result: 帧传递比可被可靠预测，卷积神经网络在计算效率和内存消耗上优于其他模型。

Conclusion: 卷积神经网络更适合嵌入式及工业系统应用，因其高效性。

Abstract: The increasing need for robustness, reliability, and determinism in wireless
networks for industrial and mission-critical applications is the driver for the
growth of new innovative methods. The study presented in this work makes use of
machine learning techniques to predict channel quality in a Wi-Fi network in
terms of the frame delivery ratio. Predictions can be used proactively to
adjust communication parameters at runtime and optimize network operations for
industrial applications. Methods including convolutional neural networks and
long short-term memory were analyzed on datasets acquired from a real Wi-Fi
setup across multiple channels. The models were compared in terms of prediction
accuracy and computational complexity. Results show that the frame delivery
ratio can be reliably predicted, and convolutional neural networks, although
slightly less effective than other models, are more efficient in terms of CPU
usage and memory consumption. This enhances the model's usability on embedded
and industrial systems.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [172] [HEIMDALL: a grapH-based sEIsMic Detector And Locator for microseismicity](https://arxiv.org/abs/2507.10850)
*Matteo Bagagli,Francesco Grigoli,Davide Bacciu*

Key words: 深度学习, 图神经网络, 地热系统, 地震监测, 碳减排

TL;DR: 提出了一种基于图神经网络的地震监测深度学习模型，能够同时进行相位拾取、关联和事件定位，显著提高检测率并减少误报。

<details>
  <summary>Details</summary>

Main category: physics.geo-ph

Motivation: 在绿色能源转型背景下，增强型地热系统的开发需求增长，需要更高效的地震监测工具以减少风险。

Method: 利用图论和图神经网络构建端到端管道，结合手动和自动地震目录进行训练和验证。

Result: 模型检测率显著提升，减少了误报和人工干预需求，适用于实时与回放监测。

Conclusion: 该模型为地热地震区提供了鲁棒的监测工具，补充现有系统并降低运营风险。

Abstract: In this work, we present a new deep-learning model for microseismicity
monitoring that utilizes continuous spatiotemporal relationships between
seismic station recordings, forming an end-to-end pipeline for seismic catalog
creation. It employs graph theory and state-of-the-art graph neural network
architectures to perform phase picking, association, and event location
simultaneously over rolling windows, making it suitable for both playback and
near-real-time monitoring. As part of the global strategy to reduce carbon
emissions within the broader context of a green-energy transition, there has
been growing interest in exploiting enhanced geothermal systems. Tested in the
complex geothermal area of Iceland's Hengill region using open-access data from
a temporary experiment, our model was trained and validated using both manually
revised and automatic seismic catalogs. Results showed a significant increase
in event detection compared to previously published automatic systems and
reference catalogs, including a $4 M_w$ seismic sequence in December 2018 and a
single-day sequence in February 2019. Our method reduces false events,
minimizes manual oversight, and decreases the need for extensive tuning of
pipelines or transfer learning of deep-learning models. Overall, it validates a
robust monitoring tool for geothermal seismic regions, complementing existing
systems and enhancing operational risk mitigation during geothermal energy
exploitation.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [173] [Neural Expectation Operators](https://arxiv.org/abs/2507.10607)
*Qian Qi*

Key words: 测量学习, BSDE, 神经期望算子, 局部Lipschitz条件, 机器学习

TL;DR: 该论文提出了一种名为“测量学习”的新范式，通过非线性期望建模模糊性，并定义神经期望算子作为BSDE的解。其核心数学贡献是解决了局部Lipschitz条件下的BSDE适定性问题，并扩展到机器学习的实际应用中。

<details>
  <summary>Details</summary>

Main category: math.PR

Motivation: 旨在建立一个可验证的数学框架，通过神经网络设计满足复杂BSDE的理论条件，从而将理论分析与机器学习实践结合。

Method: 定义神经期望算子作为满足局部Lipschitz条件和二次增长条件的BSDE的解，并通过神经网络参数化驱动函数。提供构造性方法实现凸性等性质。

Result: 证明了局部Lipschitz条件下的BSDE适定性，并扩展到完全耦合的前向-后向SDE系统及大尺度交互粒子系统的渐近分析，建立了大数定律和中心极限定理。

Conclusion: 为数据驱动的模糊性建模提供了数学基础，并通过神经网络设计实现了理论与实践的桥梁。

Abstract: This paper introduces \textbf{Measure Learning}, a paradigm for modeling
ambiguity via non-linear expectations. We define Neural Expectation Operators
as solutions to Backward Stochastic Differential Equations (BSDEs) whose
drivers are parameterized by neural networks. The main mathematical
contribution is a rigorous well-posedness theorem for BSDEs whose drivers
satisfy a local Lipschitz condition in the state variable $y$ and quadratic
growth in its martingale component $z$. This result circumvents the classical
global Lipschitz assumption, is applicable to common neural network
architectures (e.g., with ReLU activations), and holds for exponentially
integrable terminal data, which is the sharp condition for this setting. Our
primary innovation is to build a constructive bridge between the abstract, and
often restrictive, assumptions of the deep theory of quadratic BSDEs and the
world of machine learning, demonstrating that these conditions can be met by
concrete, verifiable neural network designs. We provide constructive methods
for enforcing key axiomatic properties, such as convexity, by architectural
design. The theory is extended to the analysis of fully coupled
Forward-Backward SDE systems and to the asymptotic analysis of large
interacting particle systems, for which we establish both a Law of Large
Numbers (propagation of chaos) and a Central Limit Theorem. This work provides
the foundational mathematical framework for data-driven modeling under
ambiguity.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [174] [FLsim: A Modular and Library-Agnostic Simulation Framework for Federated Learning](https://arxiv.org/abs/2507.11430)
*Arnab Mukherjee,Raju Halder,Joydeep Chandra*

Key words: 联邦学习, 仿真框架, 模块化设计, 资源效率, 定制化

TL;DR: FLsim是一个模块化、可扩展且资源高效的联邦学习仿真框架，旨在简化联邦学习技术的实验与基准测试，具有高度灵活性和定制化功能。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 针对联邦学习研究中实验与基准测试的挑战，FLsim旨在提供一个统一且灵活的仿真框架，支持多样化需求的联邦学习工作流。

Method: FLsim通过模块化设计支持用户自定义数据分布、学习算法、网络拓扑、模型聚合及区块链支持，确保实验的可控复现。

Result: 实验证明FLsim能有效模拟多种先进联邦学习实验，展示其高效性和多功能性。

Conclusion: FLsim为研究人员和实践者提供了前所未有的灵活性和功能性，标志着联邦学习仿真框架的重要进展。

Abstract: Federated Learning (FL) has undergone significant development since its
inception in 2016, advancing from basic algorithms to complex methodologies
tailored to address diverse challenges and use cases. However, research and
benchmarking of novel FL techniques against a plethora of established
state-of-the-art solutions remain challenging. To streamline this process, we
introduce FLsim, a comprehensive FL simulation framework designed to meet the
diverse requirements of FL workflows in the literature. FLsim is characterized
by its modularity, scalability, resource efficiency, and controlled
reproducibility of experimental outcomes. Its easy to use interface allows
users to specify customized FL requirements through job configuration, which
supports: (a) customized data distributions, ranging from non-independent and
identically distributed (non-iid) data to independent and identically
distributed (iid) data, (b) selection of local learning algorithms according to
user preferences, with complete agnosticism to ML libraries, (c) choice of
network topology illustrating communication patterns among nodes, (d)
definition of model aggregation and consensus algorithms, and (e) pluggable
blockchain support for enhanced robustness. Through a series of experimental
evaluations, we demonstrate the effectiveness and versatility of FLsim in
simulating a diverse range of state-of-the-art FL experiments. We envisage that
FLsim would mark a significant advancement in FL simulation frameworks,
offering unprecedented flexibility and functionality for researchers and
practitioners alike.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [175] [BioScore: A Foundational Scoring Function For Diverse Biomolecular Complexes](https://arxiv.org/abs/2507.10877)
*Yuchen Zhu,Jihong Chen,Yitong Li,Xiaomin Fang,Xianbin Ye,Jingzhou He,Xujun Zhang,Jingxuan Ge,Chao Shen,Xiaonan Zhang,Tingjun Hou,Chang-Yu Hsieh*

Key words: BioScore, 结构评估, 几何图学习, 生物分子, 亲和力预测

TL;DR: BioScore是一种通用的生物分子结构评分函数，通过双尺度几何图学习框架解决数据稀疏性和跨系统表示等挑战，并在多个任务中表现优异。

<details>
  <summary>Details</summary>

Main category: physics.chem-ph

Motivation: 当前生物分子结构评分函数的泛化性不足，限制了其在不同系统中的实用性。

Method: BioScore采用双尺度几何图学习框架，并结合专门模块用于结构评估和亲和力预测。

Result: BioScore在16个基准测试中表现优异，支持多种任务，并显著提升了蛋白质-蛋白质亲和力预测和抗原-抗体结合相关性。

Conclusion: BioScore为复杂生物分子结构评估提供了稳健且通用的解决方案。

Abstract: Structural assessment of biomolecular complexes is vital for translating
molecular models into functional insights, shaping our understanding of biology
and aiding drug discovery. However, current structure-based scoring functions
often lack generalizability across diverse biomolecular systems. We present
BioScore, a foundational scoring function that addresses key challenges -- data
sparsity, cross-system representation, and task compatibility -- through a
dual-scale geometric graph learning framework with tailored modules for
structure assessment and affinity prediction. BioScore supports a wide range of
tasks, including affinity prediction, conformation ranking, and structure-based
virtual screening. Evaluated on 16 benchmarks spanning proteins, nucleic acids,
small molecules, and carbohydrates, BioScore consistently outperforms or
matches 70 traditional and deep learning methods. Our newly proposed PPI
Benchmark further enables comprehensive evaluation of protein-protein complex
scoring. BioScore demonstrates broad applicability: (1) pretraining on
mixed-structure data boosts protein-protein affinity prediction by up to 40%
and antigen-antibody binding correlation by over 90%; (2) cross-system
generalizability enables zero- and few-shot prediction with up to 71%
correlation gain; and (3) its unified representation captures chemically
challenging systems such as cyclic peptides, improving affinity prediction by
over 60%. BioScore establishes a robust and generalizable framework for
structural assessment across complex biomolecular landscapes.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [176] [The Shape of Deceit: Behavioral Consistency and Fragility in Money Laundering Patterns](https://arxiv.org/abs/2507.10608)
*Danny Butvinik,Ofir Yakobi,Michal Einhorn Cohen,Elina Maliarsky*

Key words: 反洗钱, 网络理论, 行为一致性, 子图结构, 模式脆弱性

TL;DR: 传统反洗钱（AML）系统通常关注异常实体或交易，而本文提出一种基于网络理论的视角，强调通过预定义的洗钱模式在有向交易网络中检测，核心是行为一致性而非异常。

<details>
  <summary>Details</summary>

Main category: cs.SI

Motivation: 传统AML系统误判了洗钱的本质，洗钱行为并非异常，而是有意的、重复的，并隐藏在一致的行为模式中。因此，需要一种新的检测方法。

Method: 提出网络理论视角，关注行为一致性和预定义的洗钱模式，引入子图结构以捕捉语义和功能角色，而非仅几何特征。提出模式脆弱性概念，强调行为本质的保持。

Result: 提出AML系统应重新定义模式相似性，关注行为一致性和语义鲁棒性，而非统计异常。

Conclusion: 理论和方法上的转变为AML系统在检测金融犯罪时提供了新的视角和工具。

Abstract: Conventional anti-money laundering (AML) systems predominantly focus on
identifying anomalous entities or transactions, flagging them for manual
investigation based on statistical deviation or suspicious behavior. This
paradigm, however, misconstrues the true nature of money laundering, which is
rarely anomalous but often deliberate, repeated, and concealed within
consistent behavioral routines. In this paper, we challenge the entity-centric
approach and propose a network-theoretic perspective that emphasizes detecting
predefined laundering patterns across directed transaction networks. We
introduce the notion of behavioral consistency as the core trait of laundering
activity, and argue that such patterns are better captured through subgraph
structures expressing semantic and functional roles - not solely geometry.
Crucially, we explore the concept of pattern fragility: the sensitivity of
laundering patterns to small attribute changes and, conversely, their semantic
robustness even under drastic topological transformations. We claim that
laundering detection should not hinge on statistical outliers, but on
preservation of behavioral essence, and propose a reconceptualization of
pattern similarity grounded in this insight. This philosophical and practical
shift has implications for how AML systems model, scan, and interpret networks
in the fight against financial crime.

</details>


### [177] [Multilayer Artificial Benchmark for Community Detection (mABCD)](https://arxiv.org/abs/2507.10795)
*Łukasz Kraiński,Michał Czuba,Piotr Bródka,Paweł Prałat,Bogumił Kamiński,François Théberge*

Key words: 随机图模型，多层网络，社区检测，ABCD模型

TL;DR: 提出了一个多层网络变体mABCD，基于ABCD模型的快速、可解释且可分析的随机图模型。

<details>
  <summary>Details</summary>

Main category: cs.SI

Motivation: 扩展ABCD模型以支持多层网络，提供更灵活的网络分析工具。

Method: 基于ABCD模型的底层机制，设计多层网络变体mABCD。

Result: mABCD模型能够生成类似于LFR模型但更高效的网络结构。

Conclusion: mABCD为多层网络分析提供了一个快速且可分析的解决方案。

Abstract: The Artificial Benchmark for Community Detection (ABCD) model is a random
graph model with community structure and power-law distribution for both
degrees and community sizes. The model generates graphs similar to the
well-known LFR model but it is faster, more interpretable, and can be
investigated analytically. In this paper, we use the underlying ingredients of
the ABCD model and introduce its variant for multilayer networks, mABCD.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [178] [Learning to Quantize and Precode in Massive MIMO Systems for Energy Reduction: a Graph Neural Network Approach](https://arxiv.org/abs/2507.10634)
*Thomas Feys,Liesbet Van der Perre,François Rottenberg*

Key words: 大规模MIMO, 非线性预编码, 图神经网络, DAC功耗, 自监督学习

TL;DR: 论文研究了针对粗量化下行链路大规模MIMO的非线性预编码问题，提出了一种基于图神经网络（GNN）的自监督学习方法，显著提高了可实现的总和速率，并降低了DAC的功耗。

<details>
  <summary>Details</summary>

Main category: eess.SY

Motivation: 随着大规模MIMO系统的射频链数量、载波频率和带宽增加，数字模拟转换器（DAC）在硬件复杂性和功耗方面的瓶颈问题日益突出，因此需要探索更高效的预编码方法以减少DAC的负担。

Method: 论文提出了一种基于图神经网络（GNN）的模型，该模型能够直接根据信道矩阵和传输符号输出预编码量化向量，并通过自监督学习最大化可实现速率。为解决不可微分的DAC函数导致的梯度计算问题，采用了直通Gumbel-softmax梯度估计方法。

Result: 在粗量化条件下，该方法显著提高了可实现的总和速率。例如，在单用户情况下，使用1位DAC即可达到与3位DAC的MRT相同的速率，从而将DAC功耗降低4-7倍（基带DAC）和3倍（射频DAC）。但数字信号处理的功耗有所增加。总体功耗降低在基带DAC下适用于3.5 MHz系统带宽。

Conclusion: 所提出的方法在粗量化大规模MIMO系统中显著降低了DAC功耗，尤其在低带宽下效果显著，但需权衡数字信号处理功耗的增加。

Abstract: Massive MIMO systems are moving toward increased numbers of radio frequency
chains, higher carrier frequencies and larger bandwidths. As such,
digital-to-analog converters (DACs) are becoming a bottleneck in terms of
hardware complexity and power consumption. In this work, non-linear precoding
for coarsely quantized downlink massive MIMO is studied. Given the NP-hard
nature of this problem, a graph neural network (GNN) is proposed that directly
outputs the precoded quantized vector based on the channel matrix and the
intended transmit symbols. The model is trained in a self-supervised manner, by
directly maximizing the achievable rate. To overcome the non-differentiability
of the objective function, introduced due to the non-differentiable DAC
functions, a straight-through Gumbel-softmax estimation of the gradient is
proposed. The proposed method achieves a significant increase in achievable sum
rate under coarse quantization. For instance, in the single-user case, the
proposed method can achieve the same sum rate as maximum ratio transmission
(MRT) by using one-bit DAC's as compared to 3 bits for MRT. This reduces the
DAC's power consumption by a factor 4-7 and 3 for baseband and RF DACs
respectively. This, however, comes at the cost of increased digital signal
processing power consumption. When accounting for this, the reduction in
overall power consumption holds for a system bandwidth up to 3.5 MHz for
baseband DACs, while the RF DACs can maintain a power reduction of 2.9 for
higher bandwidths. Notably, indirect effects, which further reduce the power
consumption, such as a reduced fronthaul consumption and reduction in other
components, are not considered in this analysis.

</details>


### [179] [Standards-Compliant DM-RS Allocation via Temporal Channel Prediction for Massive MIMO Systems](https://arxiv.org/abs/2507.11064)
*Sehyun Ryu,Hyun Jong Yang*

Key words: 大规模MIMO, CSI反馈, 参考信号分配, 深度学习, 5G-Advanced

TL;DR: 论文提出了一种基于信道预测的参考信号分配（CPRS）方法，通过联合优化信道预测和DM-RS分配来提高数据吞吐量，无需CSI反馈，仿真结果显示吞吐量提升达36.60%。

<details>
  <summary>Details</summary>

Main category: eess.SY

Motivation: 超5G网络中，大规模MIMO系统的天线数量增加导致CSI反馈需求激增，现有研究主要关注CSI压缩和预测，但在参考信号分配方面的研究不足。

Method: 提出CPRS概念，采用ViViT/CNN架构，将CSI矩阵视为序列图像数据，实现高效自适应传输。

Result: 模拟结果显示，CPRS方法在吞吐量上比基准策略提高36.60%。

Conclusion: CPRS为超5G网络中的参考信号分配问题提供了有效解决方案，显著提升了数据传输效率。

Abstract: Reducing feedback overhead in beyond 5G networks is a critical challenge, as
the growing number of antennas in modern massive MIMO systems substantially
increases the channel state information (CSI) feedback demand in frequency
division duplex (FDD) systems. To address this, extensive research has focused
on CSI compression and prediction, with neural network-based approaches gaining
momentum and being considered for integration into the 3GPP 5G-Advanced
standards. While deep learning has been effectively applied to CSI-limited
beamforming and handover optimization, reference signal allocation under such
constraints remains surprisingly underexplored. To fill this gap, we introduce
the concept of channel prediction-based reference signal allocation (CPRS),
which jointly optimizes channel prediction and DM-RS allocation to improve data
throughput without requiring CSI feedback. We further propose a
standards-compliant ViViT/CNN-based architecture that implements CPRS by
treating evolving CSI matrices as sequential image-like data, enabling
efficient and adaptive transmission in dynamic environments. Simulation results
using ray-tracing channel data generated in NVIDIA Sionna validate the proposed
method, showing up to 36.60% throughput improvement over benchmark strategies.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [180] [A Mathematical Optimization Approach to Multisphere Support Vector Data Description](https://arxiv.org/abs/2507.11106)
*Víctor Blanco,Inmaculada Espejo,Raúl Páez,Antonio M. Rodríguez-Chía*

Key words: 异常检测, 数学优化, 多模态数据, 核技巧, 混合整数二阶锥模型

TL;DR: 提出了一种基于数学优化的新型多模态数据异常检测框架，扩展了支持向量数据描述方法，通过混合整数二阶锥模型和核技巧提升检测准确性。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: 针对多模态数据中异常检测的复杂性，提出一种更精确的鲁棒方法以超越现有启发式技术。

Method: 采用混合整数二阶锥模型构建欧几里得超球体，并通过对偶模型引入核技巧处理非线性数据结构。

Result: 计算研究表明，该方法在准确性和鲁棒性上明显优于现有启发式方法。

Conclusion: 该优化框架在多模态异常检测中表现优异，特别适用于复杂数据结构。

Abstract: We present a novel mathematical optimization framework for outlier detection
in multimodal datasets, extending Support Vector Data Description approaches.
We provide a primal formulation, in the shape of a Mixed Integer Second Order
Cone model, that constructs Euclidean hyperspheres to identify anomalous
observations. Building on this, we develop a dual model that enables the
application of the kernel trick, thus allowing for the detection of outliers
within complex, non-linear data structures. An extensive computational study
demonstrates the effectiveness of our exact method, showing clear advantages
over existing heuristic techniques in terms of accuracy and robustness.

</details>


### [181] [Recursive Bound-Constrained AdaGrad with Applications to Multilevel and Domain Decomposition Minimization](https://arxiv.org/abs/2507.11513)
*Serge Gratton,Alena Kopaničáková,Philippe Toint*

Key words: OFNO, 噪声容忍, 边界约束, 二阶优化, 域分解, 多级方法

TL;DR: 提出了两种OFNO噪声容忍算法，用于处理边界约束和近似梯度，并在可用时利用二阶信息。第一种是多级方法，利用问题的分层描述，第二种是域分解方法，涵盖标准加法Schwarz分解。这两种算法是一阶AdaGrad无约束优化算法的推广。由于其共享理论框架，提供了统一的收敛/复杂度理论，表明高概率下最多需要$O(\epsilon^{-2})$次迭代和噪声梯度评估来计算边界约束问题的近似一阶临界点。数值实验展示了其在PDE问题到深度神经网络训练中的高效性。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: 针对边界约束、近似梯度和二阶信息的优化问题，需要提出新的噪声容忍算法以提升计算效率和稳定性。

Method: 提出了两种基于OFNO框架的算法：多级方法和域分解方法，均为一阶AdaGrad算法的推广。

Result: 理论证明了算法高概率下最多需要$O(\epsilon^{-2})$次迭代和噪声梯度评估，实际数值实验验证了其高效性。

Conclusion: 两种算法在理论和实验上均表现出高效，适用于从PDE到深度学习的广泛问题。

Abstract: Two OFFO (Objective-Function Free Optimization) noise tolerant algorithms are
presented that handle bound constraints, inexact gradients and use second-order
information when available.The first is a multi-level method exploiting a
hierarchical description of the problem and the second is a
domain-decomposition method covering the standard addditive Schwarz
decompositions. Both are generalizations of the first-order AdaGrad algorithm
for unconstrained optimization. Because these algorithms share a common
theoretical framework, a single convergence/complexity theory is provided which
covers them both. Its main result is that, with high probability, both methods
need at most $O(\epsilon^{-2})$ iterations and noisy gradient evaluations to
compute an $\epsilon$-approximate first-order critical point of the
bound-constrained problem. Extensive numerical experiments are discussed on
applications ranging from PDE-based problems to deep neural network training,
illustrating their remarkable computational efficiency.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [182] [AGFS-Tractometry: A Novel Atlas-Guided Fine-Scale Tractometry Approach for Enhanced Along-Tract Group Statistical Comparison Using Diffusion MRI Tractography](https://arxiv.org/abs/2507.10601)
*Ruixi Zheng,Wei Zhang,Yijie Li,Xi Zhu,Zhou Lan,Jarrett Rushmore,Yogesh Rathi,Nikos Makris,Lauren J. O'Donnell,Fan Zhang*

Key words: dMRI, tractography, tractometry, along-tract analysis, permutation testing

TL;DR: 提出了一种新的ATLAS引导细尺度纤维束测量方法AGFS-Tractometry，通过利用纤维束空间信息和置换测试来增强群体间的沿束统计分析。

<details>
  <summary>Details</summary>

Main category: q-bio.QM

Motivation: 研究旨在改进现有纤维束测量技术，以更敏感和特异性地检测白质局部差异。

Method: 创建ATLAS引导的纤维束分析模板并提出非参数置换测试方法，用于沿束多比较校正的群体分析。

Result: 在合成和真实数据实验中，AGFS-Tractometry表现出更高的敏感性和特异性，能检测更多解剖一致的显著差异区域。

Conclusion: AGFS-Tractometry能够检测细微或局部白质群体差异，为纤维束测量分析提供了有效工具。

Abstract: Diffusion MRI (dMRI) tractography is currently the only method for in vivo
mapping of the brain's white matter (WM) connections. Tractometry is an
advanced tractography analysis technique for along-tract profiling to
investigate the morphology and microstructural properties along the fiber
tracts. Tractometry has become an essential tool for studying local along-tract
differences between different populations (e.g., health vs disease). In this
study, we propose a novel atlas-guided fine-scale tractometry method, namely
AGFS-Tractometry, that leverages tract spatial information and permutation
testing to enhance the along-tract statistical analysis between populations.
There are two major contributions in AGFS-Tractometry. First, we create a novel
atlas-guided tract profiling template that enables consistent, fine-scale,
along-tract parcellation of subject-specific fiber tracts. Second, we propose a
novel nonparametric permutation testing group comparison method to enable
simultaneous analysis across all along-tract parcels while correcting for
multiple comparisons. We perform experimental evaluations on synthetic datasets
with known group differences and in vivo real data. We compare AGFS-Tractometry
with two state-of-the-art tractometry methods, including Automated Fiber-tract
Quantification (AFQ) and BUndle ANalytics (BUAN). Our results show that the
proposed AGFS-Tractometry obtains enhanced sensitivity and specificity in
detecting local WM differences. In the real data analysis experiments,
AGFS-Tractometry can identify more regions with significant differences, which
are anatomically consistent with the existing literature. Overall, these
demonstrate the ability of AGFS-Tractometry to detect subtle or spatially
localized WM group-level differences. The created tract profiling template and
related code are available at:
https://github.com/ZhengRuixi/AGFS-Tractometry.git.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [183] [Recent Advances in Simulation-based Inference for Gravitational Wave Data Analysis](https://arxiv.org/abs/2507.11192)
*Bo Liang,He Wang*

Key words: 引力波, 模拟推断, 机器学习, 参数估计, 群体研究

TL;DR: 这篇论文综述了基于模拟的推断方法在引力波天文学中的应用，重点介绍了机器学习技术在参数估计和群体研究中的潜力及其面临的挑战。

<details>
  <summary>Details</summary>

Main category: gr-qc

Motivation: 针对传统贝叶斯推断方法在处理引力波数据时的高计算成本和复杂性，探索更高效的模拟推断方法。

Method: 采用机器学习技术，如归一化流和神经后验估计，以及多种模拟推断方法（如神经比率估计和一致性模型）。

Result: 这些方法在速度上优于传统方法，但其准确性和对先验假设的依赖性仍需进一步验证。

Conclusion: 基于模拟的推断方法虽具潜力，但需更广泛的验证和改进以推动其广泛应用。

Abstract: The detection of gravitational waves by the LIGO-Virgo-KAGRA collaboration
has ushered in a new era of observational astronomy, emphasizing the need for
rapid and detailed parameter estimation and population-level analyses.
Traditional Bayesian inference methods, particularly Markov chain Monte Carlo,
face significant computational challenges when dealing with the
high-dimensional parameter spaces and complex noise characteristics inherent in
gravitational wave data. This review examines the emerging role of
simulation-based inference methods in gravitational wave astronomy, with a
focus on approaches that leverage machine-learning techniques such as
normalizing flows and neural posterior estimation. We provide a comprehensive
overview of the theoretical foundations underlying various simulation-based
inference methods, including neural posterior estimation, neural ratio
estimation, neural likelihood estimation, flow matching, and consistency
models. We explore the applications of these methods across diverse
gravitational wave data processing scenarios, from single-source parameter
estimation and overlapping signal analysis to testing general relativity and
conducting population studies. Although these techniques demonstrate speed
improvements over traditional methods in controlled studies, their
model-dependent nature and sensitivity to prior assumptions are barriers to
their widespread adoption. Their accuracy, which is similar to that of
conventional methods, requires further validation across broader parameter
spaces and noise conditions.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [184] [Pronunciation Deviation Analysis Through Voice Cloning and Acoustic Comparison](https://arxiv.org/abs/2507.10985)
*Andrew Valdivia,Yueming Zhang,Hailu Xu,Amir Ghasemkhani,Xin Qin*

Key words: 语音克隆,发音错误检测,声学分析,逐帧比较

TL;DR: 该论文提出一种通过分析用户原始语音与经过发音校正的语音克隆版本之间的偏差来检测发音错误的新方法。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 旨在通过对比原始语音和克隆语音的声学差异，有效识别发音错误，而无需依赖预定义的语音规则或大量训练数据。

Method: 利用语音克隆技术生成发音正确的用户语音合成版本，并进行逐帧比较，以识别发音错误的区域。

Result: 实验证明，该方法能准确定位特定发音错误，无需针对每种目标语言进行大量训练。

Conclusion: 该方法为发音错误检测提供了一种无需依赖预定义规则或大规模数据的新途径。

Abstract: This paper presents a novel approach for detecting mispronunciations by
analyzing deviations between a user's original speech and their voice-cloned
counterpart with corrected pronunciation. We hypothesize that regions with
maximal acoustic deviation between the original and cloned utterances indicate
potential mispronunciations. Our method leverages recent advances in voice
cloning to generate a synthetic version of the user's voice with proper
pronunciation, then performs frame-by-frame comparisons to identify problematic
segments. Experimental results demonstrate the effectiveness of this approach
in pinpointing specific pronunciation errors without requiring predefined
phonetic rules or extensive training data for each target language.

</details>


### [185] [EditGen: Harnessing Cross-Attention Control for Instruction-Based Auto-Regressive Audio Editing](https://arxiv.org/abs/2507.11096)
*Vassilis Sioros,Alexandros Potamianos,Giorgos Paraskevopoulos*

Key words: 音频编辑, 自回归模型, 交叉注意力, Prompt-to-Prompt, 扩散模型

TL;DR: 研究探讨了利用交叉注意力控制实现自回归模型中高效音频编辑的方法，通过Prompt-to-Prompt类似方法指导编辑，结合扩散策略和MUSICGEN模型，提出三种编辑机制，并在自动和人工评估中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 受图像编辑方法启发，研究者希望将类似Prompt-to-Prompt的注意力机制应用于音频编辑，以提升自回归模型的编辑能力和生成质量。

Method: 结合扩散策略（基于Auffusion）和MUSICGEN预训练模型，提出基于注意力分数替换、重加权和细化的三种编辑机制。

Result: 实验表明，所提方法的组合在旋律、动态和节拍方面显著优于扩散基线模型。

Conclusion: 通过注意力机制和自回归模型的结合，实现了高质量的音频编辑，为基于提示的音频编辑提供了基准。

Abstract: In this study, we investigate leveraging cross-attention control for
efficient audio editing within auto-regressive models. Inspired by image
editing methodologies, we develop a Prompt-to-Prompt-like approach that guides
edits through cross and self-attention mechanisms. Integrating a
diffusion-based strategy, influenced by Auffusion, we extend the model's
functionality to support refinement edits, establishing a baseline for
prompt-guided audio editing. Additionally, we introduce an alternative approach
by incorporating MUSICGEN, a pre-trained frozen auto-regressive model, and
propose three editing mechanisms, based on Replacement, Reweighting, and
Refinement of the attention scores. We employ commonly-used music-specific
evaluation metrics and a human study, to gauge time-varying controllability,
adherence to global text cues, and overall audio realism. The automatic and
human evaluations indicate that the proposed combination of prompt-to-prompt
guidance with autoregressive generation models significantly outperforms the
diffusion-based baseline in terms of melody, dynamics, and tempo of the
generated audio. Our code is available at https://github.com/billsioros/EditGen

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [186] [SPICEAssistant: LLM using SPICE Simulation Tools for Schematic Design of Switched-Mode Power Supplies](https://arxiv.org/abs/2507.10639)
*Simon Nau,Jan Krummenauer,André Zimmermann*

Key words: LLMs, EDA, SMPS, SPICE, SPICEAssistant

TL;DR: 论文研究了LLMs在电子设计自动化(EDA)中特别是开关模式电源(SMPS)设计上的应用，提出了SPICEAssistant框架以提升LLMs的性能。

<details>
  <summary>Details</summary>

Main category: cs.AR

Motivation: 探讨LLMs在EDA领域的适用性，特别是在SMPS设计中的表现，以解决LLMs在解析SPICE仿真结果和多步设计过程中的局限性。

Method: 提出SPICEAssistant框架，为LLMs提供工具接口，使其能灵活地与SPICE仿真器交互，优化电路设计。

Result: 基准测试表明，SPICEAssistant框架显著提升了LLMs的性能，较GPT-4o提高了约38%。

Conclusion: 仿真反馈能有效提升LLMs在SMPS设计中的能力，且多次仿真迭代会进一步提升性能。

Abstract: State-of-the-art large language models (LLMs) show high performance across a
wide range of tasks in many domains of science. In the field of electronic
design automation (EDA), it is yet to be determined to what extent they are
capable to understand, adapt, and dimension electronic circuits. This paper
focuses on the application of LLMs to switched-mode power supply (SMPS) design
on printed circuit boards (PCBs). Particular challenges for LLMs in this
context include their limited ability to interpret results from key simulation
tools like SPICE and the multi-step design process. To address these
challenges, we suggest SPICEAssistant, a framework that provides a broad
selection of tools to an LLM. The tools serve as an interface to SPICE,
allowing the LLM to interact flexibly with the simulator to estimate the impact
of its modifications to the circuit. To evaluate the performance of
SPICEAssistant, we defined a benchmark consisting of 256 questions testing the
ability to adapt circuit netlists to fulfil different SMPS design tasks. The
benchmarking results show that simulation feedback effectively improves SMPS
design capabilities of LLMs. An increasing number of simulation iterations
leads to enhanced performance. The SPICEAssistant framework significantly
outperforms the standalone LLM GPT-4o on the benchmark by approximately 38%.

</details>


### [187] [Elk: Exploring the Efficiency of Inter-core Connected AI Chips with Deep Learning Compiler Techniques](https://arxiv.org/abs/2507.11506)
*Yiqi Liu,Yuqi Xue,Noelle Crawford,Jilong Xue,Jian Huang*

Key words: 深度学习，AI芯片，编译器，性能优化，ICCA

TL;DR: Elk是一个DL编译器框架，旨在最大化ICCA芯片的效率，通过联合优化计算、通信和I/O性能因素，实现接近理想的性能表现。

<details>
  <summary>Details</summary>

Main category: cs.AR

Motivation: 解决ICCA芯片在深度学习模型中因计算、通信和I/O之间的权衡而难以高效利用的问题。

Method: Elk通过可配置参数构建性能因素的全局权衡空间，采用新的归纳操作调度策略和成本感知的片上内存分配算法，生成全局优化的执行计划。

Result: 在IPU-POD4芯片上，Elk实现了理想性能的94%，并展示了其在ICCA芯片架构设计空间探索中的潜力。

Conclusion: Elk有效优化了ICCA芯片的效率，为支持大规模DL模型和新型ICCA芯片开发提供了有力工具。

Abstract: To meet the increasing demand of deep learning (DL) models, AI chips are
employing both off-chip memory (e.g., HBM) and high-bandwidth low-latency
interconnect for direct inter-core data exchange. However, it is not easy to
explore the efficiency of these inter-core connected AI (ICCA) chips, due to a
fundamental tussle among compute (per-core execution), communication
(inter-core data exchange), and I/O (off-chip data access).
  In this paper, we develop Elk, a DL compiler framework to maximize the
efficiency of ICCA chips by jointly trading off all the three performance
factors discussed above. Elk structures these performance factors into
configurable parameters and forms a global trade-off space in the DL compiler.
To systematically explore this space and maximize overall efficiency, Elk
employs a new inductive operator scheduling policy and a cost-aware on-chip
memory allocation algorithm. It generates globally optimized execution plans
that best overlap off-chip data loading and on-chip execution. To examine the
efficiency of Elk, we build a full-fledged emulator based on a real ICCA chip
IPU-POD4, and an ICCA chip simulator for sensitivity analysis with different
interconnect network topologies. Elk achieves 94% of the ideal roofline
performance of ICCA chips on average, showing the benefits of supporting large
DL models on ICCA chips. We also show Elk's capability of enabling architecture
design space exploration for new ICCA chip development.

</details>


### [188] [SystolicAttention: Fusing FlashAttention within a Single Systolic Array](https://arxiv.org/abs/2507.11331)
*Jiawei Lin,Guokai Chen,Yuanlong Li,Thomas Bourgeat*

Key words: Transformer, FlashAttention, 脉动阵列, 调度算法, 性能优化

TL;DR: 论文提出了一种改进的脉动阵列架构FSA，通过SystolicAttention调度算法，使FlashAttention完全在单个脉动阵列中运行，显著提高了利用率。

<details>
  <summary>Details</summary>

Main category: cs.AR

Motivation: 现有的基于脉动阵列的加速器在执行FlashAttention时面临低利用率问题，主要由于频繁的数据交换和非矩阵运算的不适配。

Method: 提出FSA架构和SystolicAttention调度算法，将FlashAttention操作映射到脉动阵列上，实现细粒度的元素级重叠。

Result: FSA在注意力FLOPs/s利用率上分别比AWS NeuronCore-v2和Google TPUv5e高出1.77倍和4.83倍，面积开销仅约10%。

Conclusion: FSA通过优化脉动阵列设计，显著提升了FlashAttention的执行效率，为Transformer模型的高效加速提供了新方案。

Abstract: Transformer models rely heavily on scaled dot-product attention (SDPA),
typically implemented using the FlashAttention algorithm. However, current
systolic-array-based accelerators face significant challenges when executing
FlashAttention. Systolic arrays can only achieve high utilization for
consecutive and large matrix multiplications. In contrast, FlashAttention
requires frequently interleaved matrix multiplications and softmax operations.
  The frequent data swaps between the systolic array and external vector units
result in low systolic array utilization. This is further exacerbated by the
fact that softmax involves numerous non-matrix operations, which are not
well-suited for systolic arrays. Moreover, the concurrent execution of matrix
multiplication on systolic arrays and softmax on vector units leads to register
file and SRAM port contention, further degrading performance.
  To overcome these limitations, we propose FSA, an enhanced systolic array
architecture that enables the entire FlashAttention algorithm to run entirely
within a single systolic array, eliminating the need for external vector units.
At the core of FSA is SystolicAttention, a novel scheduling algorithm that maps
FlashAttention operations onto systolic arrays with fine-grained, element-wise
overlap. This significantly improves array utilization while preserving the
original floating-point operation order to maintain numerical stability.
  We implement FSA in synthesizable RTL and evaluate its performance against
state-of-the-art commercial accelerators. Our results show that FSA achieves
1.77x and 4.83x higher attention FLOPs/s utilization compared to AWS
NeuronCore-v2 and Google TPUv5e, respectively, with only about 10% area
overhead.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [189] [Functional Neural Wavefunction Optimization](https://arxiv.org/abs/2507.10835)
*Victor Armegioiu,Juan Carrasquilla,Siddhartha Mishra,Johannes Müller,Jannes Nys,Marius Zeinhofer,Hang Zhang*

Key words: 变分量子蒙特卡罗, 优化算法, Galerkin投影, 几何框架, 基态能量

TL;DR: 提出了一个基于几何视角的变分量子蒙特卡罗优化算法设计与分析框架，通过Galerkin投影将无限维优化动态转化为可处理的参数空间算法，统一了现有方法并推导出新算法，实验验证了其在实际模型中的有效性。

<details>
  <summary>Details</summary>

Main category: cond-mat.str-el

Motivation: 为变分量子蒙特卡罗中的优化算法提供统一的几何分析框架，以提升算法设计的效率与准确性。

Method: 通过Galerkin投影将无限维优化动态转化为参数空间算法，结合变分ansatz的切空间，统一现有方法如随机重构和Rayleigh-Gauss-Newton，并推导新算法。

Result: 数值实验表明该框架能准确估计典型凝聚态物理模型中的基态能量，验证了其实用性。

Conclusion: 提出的几何框架不仅统一了现有方法，还为优化算法的设计与分析提供了新思路，具有实际应用价值。

Abstract: We propose a framework for the design and analysis of optimization algorithms
in variational quantum Monte Carlo, drawing on geometric insights into the
corresponding function space. The framework translates infinite-dimensional
optimization dynamics into tractable parameter-space algorithms through a
Galerkin projection onto the tangent space of the variational ansatz. This
perspective unifies existing methods such as stochastic reconfiguration and
Rayleigh-Gauss-Newton, provides connections to classic function-space
algorithms, and motivates the derivation of novel algorithms with geometrically
principled hyperparameter choices. We validate our framework with numerical
experiments demonstrating its practical relevance through the accurate
estimation of ground-state energies for several prototypical models in
condensed matter physics modeled with neural network wavefunctions.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [190] [From Kinetic Theory to AI: a Rediscovery of High-Dimensional Divergences and Their Properties](https://arxiv.org/abs/2507.11387)
*Gennaro Auricchio,Giovanni Brigati,Paolo Giudici,Giuseppe Toscani*

Key words: Kullback-Leibler散度, 机器学习, 动力学理论, 散度度量, 人工智能

TL;DR: 比较机器学习与动力学理论中的散度度量，探讨其理论基础及其在AI中的应用潜力。

<details>
  <summary>Details</summary>

Main category: math-ph

Motivation: 选择合适的散度度量对机器学习模型性能至关重要，需借鉴动力学理论中的相关方法。

Method: 对动力学理论中的散度度量进行了比较性综述。

Result: 总结了不同散度度量的理论基础，并探讨了它们在机器学习和AI中的潜在应用。

Conclusion: 动力学理论中的散度度量可为机器学习提供新的工具和视角。

Abstract: Selecting an appropriate divergence measure is a critical aspect of machine
learning, as it directly impacts model performance. Among the most widely used,
we find the Kullback-Leibler (KL) divergence, originally introduced in kinetic
theory as a measure of relative entropy between probability distributions. Just
as in machine learning, the ability to quantify the proximity of probability
distributions plays a central role in kinetic theory. In this paper, we present
a comparative review of divergence measures rooted in kinetic theory,
highlighting their theoretical foundations and exploring their potential
applications in machine learning and artificial intelligence.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [191] [TaylorPODA: A Taylor Expansion-Based Method to Improve Post-Hoc Attributions for Opaque Models](https://arxiv.org/abs/2507.10643)
*Yuchi Tang,Iñaki Esnaola,Suzanne Mason,George Panoutsos*

Key words: 模型解释、泰勒展开、特征贡献、TaylorPODA、信任部署

TL;DR: 论文提出了一种基于泰勒展开的新型模型解释方法TaylorPODA，通过‘精确性’、‘联合性’和‘零差异’三个公理来量化特征贡献，并引入‘适应性’以对齐任务目标，优于现有基线方法。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 现有的事后模型无关解释方法缺乏量化特征贡献的明确框架，论文旨在提供理论支持更强的解释方法，以增强对不透明模型的信任。

Method: 基于泰勒展开框架，提出TaylorPODA方法，引入‘精确性’、‘联合性’、‘零差异’和‘适应性’四个公理，量化特征贡献并适应任务目标。

Result: 实验表明，TaylorPODA在解释质量和可视化友好性上优于基线方法。

Conclusion: TaylorPODA为不透明模型提供了理论支持更强的解释，推动其可信部署。

Abstract: Existing post-hoc model-agnostic methods generate external explanations for
opaque models, primarily by locally attributing the model output to its input
features. However, they often lack an explicit and systematic framework for
quantifying the contribution of individual features. Building on the Taylor
expansion framework introduced by Deng et al. (2024) to unify existing local
attribution methods, we propose a rigorous set of postulates -- "precision",
"federation", and "zero-discrepancy" -- to govern Taylor term-specific
attribution. Guided by these postulates, we introduce TaylorPODA (Taylor
expansion-derived imPortance-Order aDapted Attribution), which incorporates an
additional "adaptation" property. This property enables alignment with
task-specific goals, especially in post-hoc settings lacking ground-truth
explanations. Empirical evaluations demonstrate that TaylorPODA achieves
competitive results against baseline methods, providing principled and
visualization-friendly explanations. This work represents a step toward the
trustworthy deployment of opaque models by offering explanations with stronger
theoretical grounding.

</details>


### [192] [Robust Multi-Manifold Clustering via Simplex Paths](https://arxiv.org/abs/2507.10710)
*Haoyu Chen,Anna Little,Akin Narayan*

Key words: 多流形聚类,几何方法,LAPD,无限路径距离,去噪

TL;DR: 本文提出了一种新颖的几何方法，用于多流形聚类（MMC），通过计算d-单纯形上的局部图和大角度路径距离（LAPD），成功分离流形成分。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 多流形聚类旨在将潜在相交的d维流形分离为独立的流形成分。现有方法对噪声、曲率和小的相交角不够鲁棒，本文提出了一种更有效的解决方案。

Method: 首先在d-单纯形上构建局部图，使用相邻单纯形之间的二面角作为权重，然后计算无限路径距离（LAPD），并通过去噪方法分离流形成分。

Result: 实验表明，该方法对噪声、曲率和小相交角具有鲁棒性，性能优于其他MMC算法，并通过近似方案实现了准线性计算复杂度。

Conclusion: 提出的LAPD方法在多流形聚类中表现优越，且具有高效的可扩展性。

Abstract: This article introduces a novel, geometric approach for multi-manifold
clustering (MMC), i.e. for clustering a collection of potentially intersecting,
d-dimensional manifolds into the individual manifold components. We first
compute a locality graph on d-simplices, using the dihedral angle in between
adjacent simplices as the graph weights, and then compute infinity path
distances in this simplex graph. This procedure gives a metric on simplices
which we refer to as the largest angle path distance (LAPD). We analyze the
properties of LAPD under random sampling, and prove that with an appropriate
denoising procedure, this metric separates the manifold components with high
probability. We validate the proposed methodology with extensive numerical
experiments on both synthetic and real-world data sets. These experiments
demonstrate that the method is robust to noise, curvature, and small
intersection angle, and generally out-performs other MMC algorithms. In
addition, we provide a highly scalable implementation of the proposed
algorithm, which leverages approximation schemes for infinity path distance to
achieve quasi-linear computational complexity.

</details>


### [193] [GOLFS: Feature Selection via Combining Both Global and Local Information for High Dimensional Clustering](https://arxiv.org/abs/2507.10956)
*Zhaoyu Xing,Yang Wan,Juan Wen,Wei Zhong*

Key words: 高维聚类,无监督特征选择,流形学习,正则化自表示

TL;DR: 论文提出了一种名为GOLFS的无监督特征选择方法，结合局部和全局信息，用于高维聚类问题，提升了特征选择和聚类的准确性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 在高维聚类中识别判别性特征至关重要，但由于缺乏聚类标签，已有监督特征选择方法无法直接应用。

Method: 提出GOLFS算法，结合流形学习的局部几何结构和正则化自表示的全局相关结构，通过迭代算法求解优化问题。

Result: 模拟实验和真实数据应用表明GOLFS在特征选择和聚类上表现出色。

Conclusion: GOLFS方法通过结合局部与全局信息，显著提升了特征选择和聚类的准确性。

Abstract: It is important to identify the discriminative features for high dimensional
clustering. However, due to the lack of cluster labels, the regularization
methods developed for supervised feature selection can not be directly applied.
To learn the pseudo labels and select the discriminative features
simultaneously, we propose a new unsupervised feature selection method, named
GlObal and Local information combined Feature Selection (GOLFS), for high
dimensional clustering problems. The GOLFS algorithm combines both local
geometric structure via manifold learning and global correlation structure of
samples via regularized self-representation to select the discriminative
features. The combination improves the accuracy of both feature selection and
clustering by exploiting more comprehensive information. In addition, an
iterative algorithm is proposed to solve the optimization problem and the
convergency is proved. Simulations and two real data applications demonstrate
the excellent finite-sample performance of GOLFS on both feature selection and
clustering.

</details>


### [194] [Interpretable Bayesian Tensor Network Kernel Machines with Automatic Rank and Feature Selection](https://arxiv.org/abs/2507.11136)
*Afra Kilic,Kim Batselier*

Key words: 贝叶斯学习, 张量网络, 核机器, 变分推断

TL;DR: 提出了一种贝叶斯张量网络核机器框架，通过稀疏诱导分层先验自动推断模型复杂性，提升了预测准确性、不确定度量化和可解释性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 大多数张量网络核方法是确定性的，忽略了参数不确定性，且需要手动调整模型复杂性超参数。

Method: 采用完全概率框架，利用分层先验自动推断张量秩和特征维度，并通过变分推断近似后验。

Result: 在合成和真实数据上表现出优越的预测精度、不确定度量化和可扩展性。

Conclusion: 贝叶斯张量网络核机器在自动推断模型复杂性方面具有优势。

Abstract: Tensor Network (TN) Kernel Machines speed up model learning by representing
parameters as low-rank TNs, reducing computation and memory use. However, most
TN-based Kernel methods are deterministic and ignore parameter uncertainty.
Further, they require manual tuning of model complexity hyperparameters like
tensor rank and feature dimensions, often through trial-and-error or
computationally costly methods like cross-validation. We propose Bayesian
Tensor Network Kernel Machines, a fully probabilistic framework that uses
sparsity-inducing hierarchical priors on TN factors to automatically infer
model complexity. This enables automatic inference of tensor rank and feature
dimensions, while also identifying the most relevant features for prediction,
thereby enhancing model interpretability. All the model parameters and
hyperparameters are treated as latent variables with corresponding priors.
Given the Bayesian approach and latent variable dependencies, we apply a
mean-field variational inference to approximate their posteriors. We show that
applying a mean-field approximation to TN factors yields a Bayesian ALS
algorithm with the same computational complexity as its deterministic
counterpart, enabling uncertainty quantification at no extra computational
cost. Experiments on synthetic and real-world datasets demonstrate the superior
performance of our model in prediction accuracy, uncertainty quantification,
interpretability, and scalability.

</details>


### [195] [How does Labeling Error Impact Contrastive Learning? A Perspective from Data Dimensionality Reduction](https://arxiv.org/abs/2507.11161)
*Jun Chen,Hong Chen,Yonghua Yu,Yiming Ying*

Key words: 对比学习, 标签错误, 数据降维, SVD, 下游分类

TL;DR: 本文探讨了对比学习中标签错误对下游分类性能的理论影响，并提出通过数据降维等方法减少负面影响。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 对比学习在自监督表示学习中表现优异，但其理论理解依赖于标签一致性假设，这一假设在实际中可能不成立。本文旨在研究标签错误对下游分类性能的影响。

Method: 使用数据降维方法（如SVD）减少误报样本，并通过理论和实验评估其效果。同时探讨了SVD的双刃剑性质。

Result: SVD可以减少标签错误，但可能因降低增强图的连通性而影响分类准确性。建议使用适度嵌入维度、数据膨胀和弱增强以优化性能。

Conclusion: 研究提出了通过平衡数据降维和增强策略来优化对比学习模型性能的建议。

Abstract: In recent years, contrastive learning has achieved state-of-the-art
performance in the territory of self-supervised representation learning. Many
previous works have attempted to provide the theoretical understanding
underlying the success of contrastive learning. Almost all of them rely on a
default assumption, i.e., the label consistency assumption, which may not hold
in practice (the probability of failure is called labeling error) due to the
strength and randomness of common augmentation strategies, such as random
resized crop (RRC). This paper investigates the theoretical impact of labeling
error on the downstream classification performance of contrastive learning. We
first reveal several significant negative impacts of labeling error on
downstream classification risk. To mitigate these impacts, data dimensionality
reduction method (e.g., singular value decomposition, SVD) is applied on
original data to reduce false positive samples, and establish both theoretical
and empirical evaluations. Moreover, it is also found that SVD acts as a
double-edged sword, which may lead to the deterioration of downstream
classification accuracy due to the reduced connectivity of the augmentation
graph. Based on the above observations, we give the augmentation suggestion
that we should use some moderate embedding dimension (such as $512, 1024$ in
our experiments), data inflation, weak augmentation, and SVD to ensure large
graph connectivity and small labeling error to improve model performance.

</details>


### [196] [From Observational Data to Clinical Recommendations: A Causal Framework for Estimating Patient-level Treatment Effects and Learning Policies](https://arxiv.org/abs/2507.11381)
*Rom Gutman,Shimon Sheiba,Omer Noy Klien,Naama Dekel Bird,Amit Gruber,Doron Aronson,Oren Caspi,Uri Shalit*

Key words: 个体化治疗,因果模型,观察性数据,治疗优化

TL;DR: 提出一个针对患者个体化治疗推荐的框架，强调安全性和有效性，并整合现有方法到实际流程中。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 受Hernan和Robins的目标试验范式启发，解决观察性数据中的因果识别问题，旨在提升患者治疗效果。

Method: 整合现有患者级因果模型方法，构建实际推荐流程，不提出新模型。

Result: 在心力衰竭合并急性肾损伤患者的治疗优化案例中，流程表现优于现有治疗方案。

Conclusion: 该框架能有效整合现有方法并提升患者治疗效果。

Abstract: We propose a framework for building patient-specific treatment recommendation
models, building on the large recent literature on learning patient-level
causal models and inspired by the target trial paradigm of Hernan and Robins.
We focus on safety and validity, including the crucial issue of causal
identification when using observational data. We do not provide a specific
model, but rather a way to integrate existing methods and know-how into a
practical pipeline. We further provide a real world use-case of treatment
optimization for patients with heart failure who develop acute kidney injury
during hospitalization. The results suggest our pipeline can improve patient
outcomes over the current treatment regime.

</details>


### [197] [Joint space-time wind field data extrapolation and uncertainty quantification using nonparametric Bayesian dictionary learning](https://arxiv.org/abs/2507.11385)
*George D. Pasparakis,Ioannis A. Kougioumtzoglou,Michael D. Shields*

Key words: 非参数贝叶斯,字典学习,风场数据外推,压缩感知

TL;DR: 该论文提出了一种基于非参数贝叶斯字典学习的联合时空风场数据外推方法，通过有限/不完整测量数据估计相关统计量。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 提出一种方法，通过稀疏/不完整测量数据，解决风场数据外推和统计量估计问题，弥补传统压缩感知方法的不足。

Method: 基于非参数贝叶斯字典学习，自适应确定低维表示系数，并避免传统方法需要预先选择基函数的问题。

Result: 方法表现出更高的外推精度，并能处理高维和非高斯数据，适用于传感器数量有限的场景。两个案例研究验证了方法的有效性。

Conclusion: 该方法通过自适应基选择和不完整测量数据，显著提升了风场数据外推的准确性和适用性。

Abstract: A methodology is developed, based on nonparametric Bayesian dictionary
learning, for joint space-time wind field data extrapolation and estimation of
related statistics by relying on limited/incomplete measurements. Specifically,
utilizing sparse/incomplete measured data, a time-dependent optimization
problem is formulated for determining the expansion coefficients of an
associated low-dimensional representation of the stochastic wind field.
Compared to an alternative, standard, compressive sampling treatment of the
problem, the developed methodology exhibits the following advantages. First,
the Bayesian formulation enables also the quantification of the uncertainty in
the estimates. Second, the requirement in standard CS-based applications for an
a priori selection of the expansion basis is circumvented. Instead, this is
done herein in an adaptive manner based on the acquired data. Overall, the
methodology exhibits enhanced extrapolation accuracy, even in cases of
high-dimensional data of arbitrary form, and of relatively large extrapolation
distances. Thus, it can be used, potentially, in a wide range of wind
engineering applications where various constraints dictate the use of a limited
number of sensors. The efficacy of the methodology is demonstrated by
considering two case studies. The first relates to the extrapolation of
simulated wind velocity records consistent with a prescribed joint
wavenumber-frequency power spectral density in a three-dimensional domain (2D
and time). The second pertains to the extrapolation of four-dimensional (3D and
time) boundary layer wind tunnel experimental data that exhibit significant
spatial variability and non-Gaussian characteristics.

</details>


### [198] [Canonical Bayesian Linear System Identification](https://arxiv.org/abs/2507.11535)
*Andrey Bryutkin,Matthew E. Levine,Iñigo Urteaga,Youssef Marzouk*

Key words: 线性时不变系统,贝叶斯推断,规范形式,参数非可识别性,后验分布

TL;DR: 摘要提出了一种通过嵌入线性时不变系统的规范形式来解决贝叶斯框架中参数非可识别性问题的方法，提高了推断效率和实用性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 传统贝叶斯方法在LTI系统辨识中因参数非可识别性导致后验分布复杂且多峰，使得推断低效且不实用。

Method: 将LTI系统的规范形式嵌入到贝叶斯框架中，确保推断能够完全捕获所有不变系统动态，同时解决可识别性问题。

Result: 规范形式显著提升了计算效率，生成了可解释且行为良好的后验分布，并在数据有限时提供稳健的不确定性估计。

Conclusion: 该方法为LTI系统辨识提供了一种高效且实用的贝叶斯推断方案，且支持有意义的结构化先验。

Abstract: Standard Bayesian approaches for linear time-invariant (LTI) system
identification are hindered by parameter non-identifiability; the resulting
complex, multi-modal posteriors make inference inefficient and impractical. We
solve this problem by embedding canonical forms of LTI systems within the
Bayesian framework. We rigorously establish that inference in these minimal
parameterizations fully captures all invariant system dynamics (e.g., transfer
functions, eigenvalues, predictive distributions of system outputs) while
resolving identifiability. This approach unlocks the use of meaningful,
structure-aware priors (e.g., enforcing stability via eigenvalues) and ensures
conditions for a Bernstein--von Mises theorem -- a link between Bayesian and
frequentist large-sample asymptotics that is broken in standard forms.
Extensive simulations with modern MCMC methods highlight advantages over
standard parameterizations: canonical forms achieve higher computational
efficiency, generate interpretable and well-behaved posteriors, and provide
robust uncertainty estimates, particularly from limited data.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [199] [A Learning Framework For Cooperative Collision Avoidance of UAV Swarms Leveraging Domain Knowledge](https://arxiv.org/abs/2507.10913)
*Shuangyao Huang,Haibo Zhang,Zhiyi Huang*

Key words: 多智能体强化学习, 无人机群, 协同避碰, 领域知识, 图像处理

TL;DR: 提出了一种基于领域知识驱动的多智能体强化学习框架，用于无人机群的协同避碰。

<details>
  <summary>Details</summary>

Main category: cs.MA

Motivation: 解决无人机群在复杂环境中协同避碰的问题，利用图像处理领域的知识设计奖励函数。

Method: 将障碍物建模为二维场中的极大值点，通过近似轮廓实现避碰；减少智能体交互，简化信用分配和观测共享机制。

Result: 框架在大规模无人机群中表现高效，能够适应复杂环境。

Conclusion: 该框架通过领域知识驱动的奖励设计，显著提升了无人机群的协同避碰能力。

Abstract: This paper presents a multi-agent reinforcement learning (MARL) framework for
cooperative collision avoidance of UAV swarms leveraging domain
knowledge-driven reward. The reward is derived from knowledge in the domain of
image processing, approximating contours on a two-dimensional field. By
modeling obstacles as maxima on the field, collisions are inherently avoided as
contours never go through peaks or intersect. Additionally, counters are smooth
and energy-efficient. Our framework enables training with large swarm sizes as
the agent interaction is minimized and the need for complex credit assignment
schemes or observation sharing mechanisms in state-of-the-art MARL approaches
are eliminated. Moreover, UAVs obtain the ability to adapt to complex
environments where contours may be non-viable or non-existent through intensive
training. Extensive experiments are conducted to evaluate the performances of
our framework against state-of-the-art MARL algorithms.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [200] [Real-time, Adaptive Radiological Anomaly Detection and Isotope Identification Using Non-negative Matrix Factorization](https://arxiv.org/abs/2507.10715)
*Chandler Jones,Mark Bandstra,Stefan Faaland,Yue Shi Lai,Nico Abgrall,Scott Suchyta,Reynold Cooper*

Key words: 非负矩阵分解, 核不扩散, 自适应算法, 伽马射线背景, 异常检测

TL;DR: 提出了一种基于非负矩阵分解（NMF）的自适应算法，用于在移动探测器系统中处理背景光谱变化的核不扩散应用。

<details>
  <summary>Details</summary>

Main category: physics.app-ph

Motivation: 移动探测器系统中的背景伽马射线变化导致传统算法难以维持假警报率或检测灵敏度，需开发适应性更强的算法。

Method: 通过定期更新背景模型的NMF算法（Adaptive NMF），减少对环境假设的依赖。

Result: 该算法在模拟和真实数据集上保持或超越现有NMF方法的检测性能。

Conclusion: Adaptive NMF算法更通用且适用于动态环境，提高了核不扩散应用的检测能力。

Abstract: Spectroscopic anomaly detection and isotope identification algorithms are
integral components in nuclear nonproliferation applications such as search
operations. The task is especially challenging in the case of mobile detector
systems due to the fact that the observed gamma-ray background changes more
than for a static detector system, and a pretrained background model can easily
find itself out of domain. The result is that algorithms may exceed their
intended false alarm rate, or sacrifice detection sensitivity in order to
maintain the desired false alarm rate. Non-negative matrix factorization (NMF)
has been shown to be a powerful tool for spectral anomaly detection and
identification, but, like many similar algorithms that rely on data-driven
background models, in its conventional implementation it is unable to update in
real time to account for environmental changes that affect the background
spectroscopic signature. We have developed a novel NMF-based algorithm that
periodically updates its background model to accommodate changing environmental
conditions. The Adaptive NMF algorithm involves fewer assumptions about its
environment, making it more generalizable than existing NMF-based methods while
maintaining or exceeding detection performance on simulated and real-world
datasets.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [201] [Improved sampling algorithms and Poincaré inequalities for non-log-concave distributions](https://arxiv.org/abs/2507.11236)
*Yuchen He,Zhehan Lei,Jianan Shao,Chihao Zhang*

Key words: 采样复杂度,平滑性条件,Poincaré常数,Ornstein-Uhlenbeck过程,高斯混合模型

TL;DR: 论文研究了从特定分布中采样的查询复杂度问题，通过强化平滑性条件（1*）和二阶矩假设（2），显著降低了复杂度，证明了条件（1*）对采样算法复杂度的重要影响。应用还包括改进高斯混合模型的Poincaré常数估计。

<details>
  <summary>Details</summary>

Main category: cs.DS

Motivation: 研究从分布$\mu$中采样的查询复杂度，探索不同平滑性条件对复杂度的影响，尤其是在强化条件下的改进效果。

Method: 在平滑性条件（1*）和二阶矩假设（2）下，分析采样算法的查询复杂度，并与现有结果对比。

Result: 在强化平滑性条件下，查询复杂度可降为多项式级别，且Poincaré常数估计得到改进。

Conclusion: 强化平滑性条件（1*）能显著降低采样复杂度，对高斯混合模型的应用也有重要意义。

Abstract: We study the problem of sampling from a distribution $\mu$ with density
$\propto e^{-V}$ for some potential function $V:\mathbb R^d\to \mathbb R$ with
query access to $V$ and $\nabla V$. We start with the following standard
assumptions:
  (1) The potential function $V$ is $L$-smooth.
  (2) The second moment $\mathbf{E}_{X\sim \mu}[\|X\|^2]\leq M$.
  Recently, He and Zhang (COLT'25) showed that the query complexity of sampling
from such distributions is at least
$\left(\frac{LM}{d\epsilon}\right)^{\Omega(d)}$ where $\epsilon$ is the desired
accuracy in total variation distance, and the Poincar\'e constant can be
arbitrarily large.
  Meanwhile, another common assumption in the study of diffusion based samplers
(see e.g., the work of Chen, Chewi, Li, Li, Salim and Zhang (ICLR'23))
strengthens the smoothness condition (1) to the following:
  (1*) The potential function of *every* distribution along the
Ornstein-Uhlenbeck process starting from $\mu$ is $L$-smooth.
  We show that under the assumptions (1*) and (2), the query complexity of
sampling from $\mu$ can be $\mathrm{poly}(L,d)\cdot
\left(\frac{Ld+M}{\epsilon^2}\right)^{\mathcal{O}(L+1)}$, which is polynomial
in $d$ and $\frac{1}{\epsilon}$ when $L=\mathcal{O}(1)$ and
$M=\mathrm{poly}(d)$. This improves the algorithm with quasi-polynomial query
complexity developed by Huang et al. (COLT'24). Our results imply that the
seemly moderate strengthening of the smoothness condition (1) to (1*) can lead
to an exponential gap in the query complexity of sampling algorithms.
  Moreover, we show that together with the assumption (1*) and the stronger
moment assumption that $\|X\|$ is $\lambda$-sub-Gaussian for $X\sim\mu$, the
Poincar\'e constant of $\mu$ is at most $\mathcal{O}(\lambda)^{2(L+1)}$. As an
application of our technique, we obtain improved estimate of the Poincar\'e
constant for mixture of Gaussians with the same covariance.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [202] [A Biomimetic Way for Coral-Reef-Inspired Swarm Intelligence for Carbon-Neutral Wastewater Treatment](https://arxiv.org/abs/2507.10563)
*Antonis Messinis*

Key words: 废水处理, 碳中性, 群体交互网络, 多任务碳感知, 形态发生

TL;DR: 提出了一种珊瑚礁启发的群体交互网络，用于碳中性废水处理，结合形态发生和多任务碳感知，解决了能源去除问题，表现优于基线方法。

<details>
  <summary>Details</summary>

Main category: cs.NE

Motivation: 由于废水处理量增加，实现能源中性净化具有挑战性，需要引入创新方法。

Method: 采用珊瑚礁启发的群体交互网络，结合形态发生抽象和多任务碳感知，通过线性令牌复杂度实现可扩展性。

Result: 与七种基线方法相比，实现了96.7%的去除效率、0.31 kWh m⁻³的能耗和14.2 g m⁻³的CO₂排放。在传感器漂移下表现稳健，实际场景中可节省22%的柴油消耗。

Conclusion: 该方法在碳中性废水处理中表现出色，但数据科学人员配置和治理限制是未来需要解决的问题。

Abstract: With increasing wastewater rates, achieving energy-neutral purification is
challenging. We introduce a coral-reef-inspired Swarm Interaction Network for
carbon-neutral wastewater treatment, combining morphogenetic abstraction with
multi-task carbon awareness. Scalability stems from linear token complexity,
mitigating the energy-removal problem. Compared with seven baselines, our
approach achieves 96.7\% removal efficiency, 0.31~kWh~m$^{-3}$ energy
consumption, and 14.2~g~m$^{-3}$ CO$_2$ emissions. Variance analysis
demonstrates robustness under sensor drift. Field scenarios--insular lagoons,
brewery spikes, and desert greenhouses--show potential diesel savings of up to
22\%. However, data-science staffing remains an impediment. Future work will
integrate AutoML wrappers within the project scope, although governance
restrictions pose interpretability challenges that require further visual
analytics.

</details>


### [203] [Tangma: A Tanh-Guided Activation Function with Learnable Parameters](https://arxiv.org/abs/2507.10560)
*Shreel Golwala*

Key words: 激活函数, 可学习参数, 双曲正切, MNIST, CIFAR-10

TL;DR: Tangma是一种新的激活函数，结合了双曲正切的平滑性和两个可学习参数，在MNIST和CIFAR-10任务中表现优于ReLU、Swish和GELU，实现了更高的验证精度和更低的损失。

<details>
  <summary>Details</summary>

Main category: cs.NE

Motivation: 研究激活函数在深度神经网络中的重要性，并提出一种结合平滑性和可学习参数的新激活函数Tangma，以优化神经元激活和训练稳定性。

Method: 提出Tangma激活函数，引入可学习参数α和γ，分别调整激活曲线的拐点并增强线性性；在MNIST和CIFAR-10任务中测试其性能。

Result: 在MNIST上达到99.09%的最高验证精度和最低损失，CIFAR-10上为78.15%，同时训练效率优于Swish和GELU。

Conclusion: Tangma在标准视觉任务中表现卓越，其可学习设计为更大模型提供了更多灵活性，有望在图像识别和语言建模中发挥作用。

Abstract: Activation functions are key to effective backpropagation and expressiveness
in deep neural networks. This work introduces Tangma, a new activation function
that combines the smooth shape of the hyperbolic tangent with two learnable
parameters: $\alpha$, which shifts the curve's inflection point to adjust
neuron activation, and $\gamma$, which adds linearity to preserve weak
gradients and improve training stability. Tangma was evaluated on MNIST and
CIFAR-10 using custom networks composed of convolutional and linear layers, and
compared against ReLU, Swish, and GELU. On MNIST, Tangma achieved the highest
validation accuracy of 99.09% and the lowest validation loss, demonstrating
faster and more stable convergence than the baselines. On CIFAR-10, Tangma
reached a top validation accuracy of 78.15%, outperforming all other activation
functions while maintaining a competitive training loss. Tangma also showed
improved training efficiency, with lower average epoch runtimes compared to
Swish and GELU. These results suggest that Tangma performs well on standard
vision tasks and enables reliable, efficient training. Its learnable design
gives more control over activation behavior, which may benefit larger models in
tasks such as image recognition or language modeling.

</details>


### [204] [Biological Processing Units: Leveraging an Insect Connectome to Pioneer Biofidelic Neural Architectures](https://arxiv.org/abs/2507.10951)
*Siyu Yu,Zihan Qin,Tingshan Liu,Beiya Xu,R. Jacob Vogelstein,Jason Brown,Joshua T. Vogelstein*

Key words: 连接组, BPU, 生物启发, 神经网络, 性能比较

TL;DR: 利用果蝇幼虫大脑的完整连接组构建的生物处理单元（BPU）在多项任务中表现出色，展示了生物启发神经网络在复杂认知任务中的潜力。

<details>
  <summary>Details</summary>

Main category: cs.NE

Motivation: 探索生物进化的神经回路是否支持人工智能，并验证其性能。

Method: 将果蝇幼虫大脑的连接组转换为固定循环网络（BPU），并通过扩展和特定模态消融进行实验。

Result: BPU在MNIST上达到98%准确率，CIFAR-10上58%，优于规模匹配的多层感知机；在ChessBench和CNN-BPU中也表现优异。

Conclusion: 生物启发的神经架构有望支持复杂认知任务，未来可扩展至更大规模的连接组。

Abstract: The complete connectome of the Drosophila larva brain offers a unique
opportunity to investigate whether biologically evolved circuits can support
artificial intelligence. We convert this wiring diagram into a Biological
Processing Unit (BPU), a fixed recurrent network derived directly from synaptic
connectivity. Despite its modest size 3,000 neurons and 65,000 weights between
them), the unmodified BPU achieves 98% accuracy on MNIST and 58% on CIFAR-10,
surpassing size-matched MLPs. Scaling the BPU via structured connectome
expansions further improves CIFAR-10 performance, while modality-specific
ablations reveal the uneven contributions of different sensory subsystems. On
the ChessBench dataset, a lightweight GNN-BPU model trained on only 10,000
games achieves 60% move accuracy, nearly 10x better than any size transformer.
Moreover, CNN-BPU models with ~2M parameters outperform parameter-matched
Transformers, and with a depth-6 minimax search at inference, reach 91.7%
accuracy, exceeding even a 9M-parameter Transformer baseline. These results
demonstrate the potential of biofidelic neural architectures to support complex
cognitive tasks and motivate scaling to larger and more intelligent connectomes
in future work.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [205] [NLP Meets the World: Toward Improving Conversations With the Public About Natural Language Processing Research](https://arxiv.org/abs/2507.10559)
*Shomir Wilson*

Key words: 大语言模型、自然语言处理、公众沟通、伦理问题、研究支持

TL;DR: 本文分享了关于如何向公众传达大语言模型（LLM）能力和限制的建议，涵盖了模糊术语、不合理期望和伦理问题三个主题，旨在促进公众理解和研究支持。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 随着大语言模型的快速发展，公众对自然语言处理的兴趣激增，但同时也存在对LLMs能力的误解和不合理期望。本文旨在通过提供沟通建议，增强公众理解并支持研究。

Method: 通过分析已发表的NLP研究和新闻报道，识别了三个主要障碍（模糊术语、不合理期望、伦理问题），并提出了相应的沟通建议。

Result: 提出了针对公众沟通的具体建议，强调了透明度和有效性，以减少误解并促进对NLP研究的支持。

Conclusion: 有效的公众沟通对于LLMs的可持续发展至关重要，透明和清晰的表达有助于避免误解并维持研究领域的公信力。

Abstract: Recent developments in large language models (LLMs) have been accompanied by
rapidly growing public interest in natural language processing (NLP). This
attention is reflected by major news venues, which sometimes invite NLP
researchers to share their knowledge and views with a wide audience.
Recognizing the opportunities of the present, for both the research field and
for individual researchers, this paper shares recommendations for communicating
with a general audience about LLMs' capabilities and limitations. These
recommendations cover three themes: vague terminology as an obstacle to public
understanding, unreasonable expectations as obstacles to sustainable growth,
and ethical failures as obstacles to continued support. Published NLP research
and popular news coverage are cited to illustrate these themes with examples.
The recommendations promote effective, transparent communication with the
general public about NLP, in order to strengthen public understanding and
encourage support for research.

</details>


### [206] [Can Large Language Models Understand As Well As Apply Patent Regulations to Pass a Hands-On Patent Attorney Test?](https://arxiv.org/abs/2507.10576)
*Bhakti Khera,Rezvan Alamian,Pascal A. Scherz,Stephan M. Goetz*

Key words: 大语言模型、专利律师考试、性能评估、多模态、专家监督

TL;DR: 大型语言模型在法律领域的表现被高估，现有模型在专利律师考试中未能达到专业标准，未来需改进逻辑一致性和多模态能力。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 评估开源和专有大语言模型在欧洲专利律师资格考试中的表现，揭示其局限性。

Method: 测试多种LLM（如GPT系列、Anthropic、Deepseek、Llama-3等）在欧洲专利律师资格考试中的准确性和F1分数，并结合人类专家评估。

Result: OpenAI o1表现最佳（0.82准确率），但所有模型均未达到专业标准（0.90准确率），且存在格式一致性和逻辑问题。

Conclusion: 虽然模型表现突出，但离实际应用仍有差距，需改进逻辑一致性、多模态能力和提示适应性。

Abstract: The legal field already uses various large language models (LLMs) in actual
applications, but their quantitative performance and reasons for it are
underexplored. We evaluated several open-source and proprietary LLMs --
including GPT-series, Anthropic, Deepseek and Llama-3, variants -- on parts of
the European Qualifying Examination (EQE) for future European Patent Attorneys.
OpenAI o1 led with 0.82 accuracy and 0.81 F1 score, whereas (Amazon Web
Services) AWS Llama 3.1 8B lagged at 0.50 accuracy, and a Python-deployed Llama
3.1 8B scored 0.55. The latter two are within the range of mere guessing for
the two-answer forced-choice design. None of the evaluated models could have
passed the examination fully, as accuracy never exceeded the average threshold
of 0.90 required for professional-level standards -- also not models that are
regularly promoted for their assumed beyond-PhD- and bar-admitted-lawyer-level
performance. GPT-4o excelled at integrating text and graphics, while Claude 3
Opus often lost formatting coherence. Human patent experts evaluated the
textual justifications and uncovered various critical shortcomings of each
model. They valued clarity and legal rationale over the raw correctness of the
answers, which revealed misalignment between automatic metrics and expert
judgment. Model outputs were sensitive to modest temperature changes and prompt
wording, which underscores the remaining necessity of expert oversight. Future
work should target logical consistency, robust multimodality, and adaptive
prompting to approach human-level patent proficiency. In summary, despite the
outstanding performance of recent large models, the general public might
overestimate their performance. The field has a long way to go to develop a
virtual patent attorney. This paper wants to point out several specific
limitations that need solutions.

</details>


### [207] [Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors](https://arxiv.org/abs/2507.10579)
*Ekaterina Kochmar,Kaushal Kumar Maurya,Kseniia Petukhova,KV Aditya Srivatsa,Anaïs Tack,Justin Vasselli*

Key words: AI导师, 大语言模型, 教学能力评估, 共享任务

TL;DR: 该论文总结了一个评估基于大语言模型（LLM）的AI导师教学能力的共享任务，重点关注其对学生错误修正的响应质量。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 旨在通过多维度评估AI导师的教学能力，推动其在实际教育中的有效性提升。

Method: 设计了五个任务轨道，涵盖错误识别、定位、指导提供和反馈可操作性等维度，并使用人类标注的金标准进行评估。

Result: 最佳模型在各任务轨道上的宏F1分数从58.34到71.81不等，导师身份识别任务的F1分数达96.98，显示仍有改进空间。

Conclusion: 资源公开以支持未来研究，AI导师的教学能力虽表现尚可，但需进一步提升。

Abstract: This shared task has aimed to assess pedagogical abilities of AI tutors
powered by large language models (LLMs), focusing on evaluating the quality of
tutor responses aimed at student's mistake remediation within educational
dialogues. The task consisted of five tracks designed to automatically evaluate
the AI tutor's performance across key dimensions of mistake identification,
precise location of the mistake, providing guidance, and feedback
actionability, grounded in learning science principles that define good and
effective tutor responses, as well as the track focusing on detection of the
tutor identity. The task attracted over 50 international teams across all
tracks. The submitted models were evaluated against gold-standard human
annotations, and the results, while promising, show that there is still
significant room for improvement in this domain: the best results for the four
pedagogical ability assessment tracks range between macro F1 scores of 58.34
(for providing guidance) and 71.81 (for mistake identification) on three-class
problems, with the best F1 score in the tutor identification track reaching
96.98 on a 9-class task. In this paper, we overview the main findings of the
shared task, discuss the approaches taken by the teams, and analyze their
performance. All resources associated with this task are made publicly
available to support future research in this critical domain.

</details>


### [208] [Exploring User Security and Privacy Attitudes and Concerns Toward the Use of General-Purpose LLM Chatbots for Mental Health](https://arxiv.org/abs/2507.10695)
*Jabari Kwesi,Jiaxun Cao,Riya Manchanda,Pardis Emami-Naeini*

Key words: LLM, 心理健康, 隐私安全, 无形脆弱性, 用户误解

TL;DR: 研究探讨了用户在使用通用LLM支持的聊天机器人进行心理健康管理时的隐私和安全问题，发现用户存在误解和风险意识不足。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 研究动机是填补现有文献中关于通用LLM聊天机器人在心理健康支持方面的隐私和安全问题的空白。

Method: 通过21次半结构化访谈，分析了美国参与者对LLM聊天机器人的隐私和安全的看法。

Result: 发现了用户对LLM聊天机器人的误解，如将其人机同理心与人类责任混淆，并提出“无形脆弱性”概念。

Conclusion: 研究呼吁加强对通用LLM聊天机器人中心理健康披露的保护措施。

Abstract: Individuals are increasingly relying on large language model (LLM)-enabled
conversational agents for emotional support. While prior research has examined
privacy and security issues in chatbots specifically designed for mental health
purposes, these chatbots are overwhelmingly "rule-based" offerings that do not
leverage generative AI. Little empirical research currently measures users'
privacy and security concerns, attitudes, and expectations when using
general-purpose LLM-enabled chatbots to manage and improve mental health.
Through 21 semi-structured interviews with U.S. participants, we identified
critical misconceptions and a general lack of risk awareness. Participants
conflated the human-like empathy exhibited by LLMs with human-like
accountability and mistakenly believed that their interactions with these
chatbots were safeguarded by the same regulations (e.g., HIPAA) as disclosures
with a licensed therapist. We introduce the concept of "intangible
vulnerability," where emotional or psychological disclosures are undervalued
compared to more tangible forms of information (e.g., financial or
location-based data). To address this, we propose recommendations to safeguard
user mental health disclosures with general-purpose LLM-enabled chatbots more
effectively.

</details>


### [209] ["Is it always watching? Is it always listening?" Exploring Contextual Privacy and Security Concerns Toward Domestic Social Robots](https://arxiv.org/abs/2507.10786)
*Henry Bell,Jabari Kwesi,Hiba Laabadli,Pardis Emami-Naeini*

Key words: 社交机器人, 隐私, 安全, AI, 用户需求

TL;DR: 美国用户对社交机器人的安全和隐私风险表示担忧，强调需要透明性、易用性和强大的隐私控制。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 社交机器人因其AI和先进传感能力在美国市场受到关注，但其数据收集和交互能力带来了显著的安全与隐私威胁。研究旨在探究用户需求，指导设计。

Method: 通过19次半结构化访谈，识别用户的主要安全和隐私关切。

Result: 用户最关注数据透明度、误信息和设备可靠性，期望有明确的隐私控制和数据收集指示。

Conclusion: 社交机器人在商业化早期阶段需注重用户隐私和安全需求，设计透明且易用的隐私功能。

Abstract: Equipped with artificial intelligence (AI) and advanced sensing capabilities,
social robots are gaining interest among consumers in the United States. These
robots seem like a natural evolution of traditional smart home devices.
However, their extensive data collection capabilities, anthropomorphic
features, and capacity to interact with their environment make social robots a
more significant security and privacy threat. Increased risks include data
linkage, unauthorized data sharing, and the physical safety of users and their
homes. It is critical to investigate U.S. users' security and privacy needs and
concerns to guide the design of social robots while these devices are still in
the early stages of commercialization in the U.S. market. Through 19
semi-structured interviews, we identified significant security and privacy
concerns, highlighting the need for transparency, usability, and robust privacy
controls to support adoption. For educational applications, participants
worried most about misinformation, and in medical use cases, they worried about
the reliability of these devices. Participants were also concerned with the
data inference that social robots could enable. We found that participants
expect tangible privacy controls, indicators of data collection, and
context-appropriate functionality.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [210] [Overview of the TREC 2022 deep learning track](https://arxiv.org/abs/2507.10865)
*Nick Craswell,Bhaskar Mitra,Emine Yilmaz,Daniel Campos,Jimmy Lin,Ellen M. Voorhees,Ian Soboroff*

Key words: TREC, Deep Learning, MS MARCO, passage retrieval, document ranking, dense retrieval

TL;DR: TREC Deep Learning 2022 track聚焦于构建更完整的测试集合，深度学习排名模型继续领先传统方法，但在密集检索方面有意外结果。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 利用MS MARCO数据集和大规模预训练，构建更高质量的测试集，提升检索任务的区分度和数据可复用性。

Method: 主要通过刷新和扩展MS MARCO数据集，集中资源于段落检索任务的标注，文档任务标签从段落标签推断。

Result: 深度学习模型仍优于传统方法，但密集检索表现不如去年，部分非密集检索方法表现突出。

Conclusion: 尽管资源集中于段落任务标注，但数据质量提升，新发现表明检索方法需进一步优化。

Abstract: This is the fourth year of the TREC Deep Learning track. As in previous
years, we leverage the MS MARCO datasets that made hundreds of thousands of
human annotated training labels available for both passage and document ranking
tasks. In addition, this year we also leverage both the refreshed passage and
document collections that were released last year leading to a nearly $16$
times increase in the size of the passage collection and nearly four times
increase in the document collection size. Unlike previous years, in 2022 we
mainly focused on constructing a more complete test collection for the passage
retrieval task, which has been the primary focus of the track. The document
ranking task was kept as a secondary task, where document-level labels were
inferred from the passage-level labels. Our analysis shows that similar to
previous years, deep neural ranking models that employ large scale pretraining
continued to outperform traditional retrieval methods. Due to the focusing our
judging resources on passage judging, we are more confident in the quality of
this year's queries and judgments, with respect to our ability to distinguish
between runs and reuse the dataset in future. We also see some surprises in
overall outcomes. Some top-performing runs did not do dense retrieval. Runs
that did single-stage dense retrieval were not as competitive this year as they
were last year.

</details>


### [211] [Extracting Document Relations from Search Corpus by Marginalizing over User Queries](https://arxiv.org/abs/2507.10726)
*Yuki Iwamoto,Kaoru Tsunoda,Ken Kaneiwa*

Key words: 文档关系, 查询边缘化, 条件检索, 无监督学习

TL;DR: 提出了一种名为EDR-MQ的新框架，通过查询边缘化发现文档关系，无需手动标注或预定义分类法。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 在大规模语料库中理解文档关系对于知识发现和信息组织至关重要，但现有方法依赖人工标注或预定义分类法，限制了灵活性和可扩展性。

Method: 利用EDR-MQ框架，通过查询边缘化估计文档对的联合概率，并结合MC-RAG进行条件检索。

Result: 实验表明，该方法成功识别了有意义的文档关系，包括主题集群、证据链和跨领域连接。

Conclusion: 该框架提供了一种灵活的文档组织方法，适应不同用户视角和信息需求。

Abstract: Understanding relationships between documents in large-scale corpora is
essential for knowledge discovery and information organization. However,
existing approaches rely heavily on manual annotation or predefined
relationship taxonomies. We propose EDR-MQ (Extracting Document Relations by
Marginalizing over User Queries), a novel framework that discovers document
relationships through query marginalization. EDR-MQ is based on the insight
that strongly related documents often co-occur in results across diverse user
queries, enabling us to estimate joint probabilities between document pairs by
marginalizing over a collection of queries. To enable this query
marginalization approach, we develop Multiply Conditioned Retrieval-Augmented
Generation (MC-RAG), which employs conditional retrieval where subsequent
document retrievals depend on previously retrieved content. By observing
co-occurrence patterns across diverse queries, EDR-MQ estimates joint
probabilities between document pairs without requiring labeled training data or
predefined taxonomies. Experimental results show that our query marginalization
approach successfully identifies meaningful document relationships, revealing
topical clusters, evidence chains, and cross-domain connections that are not
apparent through traditional similarity-based methods. Our query-driven
framework offers a practical approach to document organization that adapts to
different user perspectives and information needs.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [212] [$\texttt{Droid}$: A Resource Suite for AI-Generated Code Detection](https://arxiv.org/abs/2507.10583)
*Daniil Orel,Indraneil Paul,Iryna Gurevych,Preslav Nakov*

Key words: 机器生成代码检测、数据集、对抗训练、多任务学习、度量学习

TL;DR: 该论文介绍了最大的开源数据集DroidCollection，用于训练和评估机器生成代码检测器，并开发了基于多任务目标的检测器DroidDetect。研究发现现有检测器在多样编程语言和领域泛化能力不足，且容易被对抗性攻击破坏，但通过少量对抗数据训练可改善。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 现有机器生成代码检测器在多样编程语言和真实领域泛化能力不足，且容易被对抗性攻击破坏。因此，作者旨在构建一个全面的数据集，并开发更鲁棒的检测器。

Method: 1. 构建DroidCollection数据集，包含多种编程语言、模型输出和真实领域代码；2. 开发基于多任务目标的检测器DroidDetect；3. 探索对抗训练、度量学习和不确定性重采样以提升性能。

Result: 现有检测器泛化能力有限，但DroidDetect通过对抗数据和新技术显著提升了检测性能。

Conclusion: 通过全面数据集和多任务训练，可显著提升机器生成代码检测器的鲁棒性和泛化能力。

Abstract: In this work, we compile $\textbf{$\texttt{DroidCollection}$}$, the most
extensive open data suite for training and evaluating machine-generated code
detectors, comprising over a million code samples, seven programming languages,
outputs from 43 coding models, and over three real-world coding domains.
Alongside fully AI-generated samples, our collection includes human-AI
co-authored code, as well as adversarial samples explicitly crafted to evade
detection. Subsequently, we develop $\textbf{$\texttt{DroidDetect}$}$, a suite
of encoder-only detectors trained using a multi-task objective over
$\texttt{DroidCollection}$. Our experiments show that existing detectors'
performance fails to generalise to diverse coding domains and programming
languages outside of their narrow training data. Additionally, we demonstrate
that while most detectors are easily compromised by humanising the output
distributions using superficial prompting and alignment approaches, this
problem can be easily amended by training on a small amount of adversarial
data. Finally, we demonstrate the effectiveness of metric learning and
uncertainty-based resampling as means to enhance detector training on possibly
noisy distributions.

</details>


### [213] [ARPaCCino: An Agentic-RAG for Policy as Code Compliance](https://arxiv.org/abs/2507.10584)
*Francesco Romeo,Luigi Arena,Francesco Blefari,Francesco Aurelio Pironti,Matteo Lupinacci,Angelo Furfaro*

Key words: Policy as Code, Infrastructure as Code, Large Language Models, Retrieval-Augmented-Generation, compliance

TL;DR: ARPaCCino是一个结合大型语言模型（LLM）、检索增强生成（RAG）和工具验证的系统，用于自动化生成和验证‘政策即代码’（PaC）规则，提升IaC环境中的安全与合规性。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: PaC的采用受到政策语言复杂性和配置错误风险的阻碍，需要一种自动化解决方案来简化政策生成与验证。

Method: ARPaCCino通过自然语言输入生成Rego规则，验证IaC合规性，并迭代优化配置，其模块化架构支持多种技术框架。

Result: 实验显示，ARPaCCino能生成正确语法和语义的政策，识别不合规基础设施，并提出修正，即使使用较小的开源LLM。

Conclusion: ARPaCCino证明了代理式RAG架构在提升PaC工作流自动化、可靠性和可访问性方面的潜力。

Abstract: Policy as Code (PaC) is a paradigm that encodes security and compliance
policies into machine-readable formats, enabling automated enforcement in
Infrastructure as Code (IaC) environments. However, its adoption is hindered by
the complexity of policy languages and the risk of misconfigurations. In this
work, we present ARPaCCino, an agentic system that combines Large Language
Models (LLMs), Retrieval-Augmented-Generation (RAG), and tool-based validation
to automate the generation and verification of PaC rules. Given natural
language descriptions of the desired policies, ARPaCCino generates formal Rego
rules, assesses IaC compliance, and iteratively refines the IaC configurations
to ensure conformance. Thanks to its modular agentic architecture and
integration with external tools and knowledge bases, ARPaCCino supports policy
validation across a wide range of technologies, including niche or emerging IaC
frameworks. Experimental evaluation involving a Terraform-based case study
demonstrates ARPaCCino's effectiveness in generating syntactically and
semantically correct policies, identifying non-compliant infrastructures, and
applying corrective modifications, even when using smaller, open-weight LLMs.
Our results highlight the potential of agentic RAG architectures to enhance the
automation, reliability, and accessibility of PaC workflows.

</details>


### [214] [Repairing Language Model Pipelines by Meta Self-Refining Competing Constraints at Runtime](https://arxiv.org/abs/2507.10590)
*Mojtaba Eshghie*

Key words: 语言模型,竞争约束,元修正,回溯循环,动态修复

TL;DR: Meta Self-Refining框架解决了语言模型管道中竞争软约束导致的低效回溯问题，通过元修正层和指令合成实现高效修复。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 当前语言模型管道在面对竞争软约束时效率低下，容易陷入无效回溯循环。

Method: 引入Meta Self-Refining框架，通过监控执行历史检测振荡故障，并调用元修复器合成指令，平衡竞争约束。

Result: 实验表明，该框架能有效修复回溯循环，提升语言模型程序的效率。

Conclusion: Meta Self-Refining为解决竞争约束问题提供了有效方法，显著提升语言模型管道的性能。

Abstract: Language Model (LM) pipelines can dynamically refine their outputs against
programmatic constraints. However, their effectiveness collapses when faced
with competing soft constraints, leading to inefficient backtracking loops
where satisfying one constraint violates another. We introduce Meta
Self-Refining, a framework that equips LM pipelines with a meta-corrective
layer to repair these competitions at runtime/inference-time. Our approach
monitors the pipeline's execution history to detect oscillatory failures. Upon
detection, it invokes a meta-repairer LM that analyzes the holistic state of
the backtracking attempts and synthesizes a strategic instruction to balance
the competing requirements. This self-repair instruction guides the original LM
out of a failing refining loop towards a successful output. Our results show
Meta Self-Refining can successfully repair these loops, leading to more
efficient LM programs.

</details>


### [215] [ToolRegistry: A Protocol-Agnostic Tool Management Library for Function-Calling LLMs](https://arxiv.org/abs/2507.10593)
*Peng Ding*

Key words: LLM, 工具集成, 性能提升, 开发效率

TL;DR: Toolregistry是一个协议无关的工具管理库，简化工具集成流程，显著减少代码量并提升性能。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 当前工具集成方法存在碎片化、协议限制和实现复杂性等问题，增加了开发负担。

Method: 通过统一接口实现工具的注册、表示、执行和生命周期管理。

Result: Toolregistry减少了60-80%的集成代码，性能提升高达3.1倍，且100%兼容OpenAI函数调用标准。

Conclusion: Toolregistry显著提升了开发效率和代码可维护性，适用于多种集成场景。

Abstract: Large Language Model (LLM) applications are increasingly relying on external
tools to extend their capabilities beyond text generation. However, current
tool integration approaches suffer from fragmentation, protocol limitations,
and implementation complexity, leading to substantial development overhead.
This paper presents Toolregistry, a protocol-agnostic tool management library
that simplifies tool registration, representation, execution, and lifecycle
management via a unified interface. Our evaluation demonstrates that
\toolregistry achieves 60-80% reduction in tool integration code, up to 3.1x
performance improvements through concurrent execution, and 100% compatibility
with OpenAI function calling standards. Real-world case studies show
significant improvements in development efficiency and code maintainability
across diverse integration scenarios. \toolregistry is open-source and
available at https://github.com/Oaklight/ToolRegistry, with comprehensive
documentation at https://toolregistry.readthedocs.io/.

</details>


### [216] [SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks](https://arxiv.org/abs/2507.11059)
*Pavel Adamenko,Mikhail Ivanov,Aidar Valeev,Rodion Levichev,Pavel Zadorozhny,Ivan Lopatin,Dmitry Babayev,Alena Fenogenova,Valentin Malykh*

Key words: 大型语言模型,软件工程,基准测试,数据污染,GitHub问题

TL;DR: SWE-MERA是一个动态更新的基准测试，解决了现有SWE-bench数据集的污染问题，通过自动化收集GitHub问题和严格验证，展示了其在评估LLMs方面的区分能力。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 现有的大型语言模型（LLMs）在软件工程中的评估存在数据污染问题，如SWE-bench数据集中的解决方案泄漏和测试用例不足。

Method: 提出了SWE-MERA，通过自动化收集真实GitHub问题并严格验证质量，构建了约10,000个潜在任务的动态基准。

Result: 使用Aider编码代理评估了12种最新的LLMs，结果显示SWE-MERA具有较强的区分能力。

Conclusion: SWE-MERA是一个可靠的动态基准，能够有效减少数据污染问题，并为LLMs评估提供了更高质量的数据集。

Abstract: The rapid advancement of Large Language Models (LLMs) in software engineering
has revealed critical limitations in existing benchmarks, particularly the
widely used SWE-bench dataset. Recent studies have uncovered severe data
contamination issues, e.g. SWE-bench reports 32.67% of successful patches
involve direct solution leakage and 31.08\% pass due to inadequate test cases.
We introduce SWE-MERA, a dynamic, continuously updated benchmark designed to
address these fundamental challenges through an automated collection of
real-world GitHub issues and rigorous quality validation. Our approach
implements a reliable pipeline that ensures quality while minimizing
contamination risks, resulting in approximately 10,000 potential tasks with 300
samples currently available. Evaluation using the Aider coding agent
demonstrates strong discriminative power in state-of-the-art models. We report
performance across a dozen recent LLMs evaluated on tasks collected between
September 2024 and June 2025.

</details>


### [217] [A Code Comprehension Benchmark for Large Language Models for Code](https://arxiv.org/abs/2507.10641)
*Jayant Havare,Saurav Chaudhary,Ganesh Ramakrishnan,Kaushik Maharajan,Srikanth Tamilselvam*

Key words: 大语言模型、代码理解、微调、语义理解、代码任务

TL;DR: 论文探讨了大语言模型在代码理解任务中的局限性，提出通过微调来提升其语义理解能力，并在多个模型上验证了有效性。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 现有大语言模型在代码生成和补全任务中表现优异，但在需要深层语义理解的任务（如代码调试和优化）中表现不佳，因此需要提升其代码理解能力。

Method: 通过在大规模数据集上微调模型，使其专注于代码理解任务，从而提升语义理解能力。

Result: 微调后，模型在代码理解任务中表现显著提升，特别是QWQ-32B模型准确率从70%提升至83.47%，而Codestral-22B达到了最高的87.66%准确率。

Conclusion: 微调能有效提升大语言模型的代码语义理解能力，尤其是在需要深层语义的任务中表现更优。

Abstract: Large Language Models have shown impressive capabilities in coding tasks like
code generation and code completion, as they have been trained on a large
amount of code data. Also, since one of the core pretraining objectives is Next
Token Prediction, these models tends to learn surface-level syntactic patterns
in code. However, this does not guarantee code comprehension ability i.e. the
ability to capture the semantics of the code. In our opinion, this is the
reason why these models often underperform on tasks that require deeper
semantic understanding, such as code debugging and code optimization. To
address this, we propose fine-tuning these models specifically for code
comprehension tasks using large-scale datasets, enabling them to develop a more
robust understanding of code semantics. We evaluate three code models of
varying sizes on a suite of code comprehension tasks designed to assess
semantic understanding beyond surface-level syntactic pattern matching. In
particular, we analyze performance on the Subjectivity Grading Task and observe
that model performance improves after fine-tuning on relevant downstream tasks.
The most significant improvement is seen in the QWQ-32B model, where accuracy
increases from 70% to 83.47%. A similar or explainable trend is observed across
other models, clearly indicating an enhancement in code comprehension ability.
Among the models studied, the DPO-fine-tuned Codestral-22B achieves the highest
micro-accuracy of 87.66% on the Subjectivity Grading Task.

</details>


### [218] [CodeAssistBench (CAB): Dataset & Benchmarking for Multi-turn Chat-Based Code Assistance](https://arxiv.org/abs/2507.10646)
*Myeongsoo Kim,Shweta Garg,Baishakhi Ray,Varun Kumar,Anoop Deoras*

Key words: 编程助手,基准测试,多轮交互,代码库环境,大语言模型

TL;DR: 本文介绍了CodeAssistBench（CAB），首个用于评估多轮编程辅助的现实场景基准框架，揭示了当前大语言模型在复杂项目环境中的能力差距。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 现有编程助手基准主要关注代码生成任务，缺乏对现实环境中多轮交互能力的评估，亟需一个更全面的测试框架。

Method: CAB通过自动从GitHub问题生成可扩展数据集，包括代码库容器化和模拟用户评估，构建了一个包含3,286个真实问题的测试集。

Result: 评估显示，模型在Stack Overflow问题上成功率70-83%，但在CAB的复杂问题中仅解决16.49%，能力差距显著。

Conclusion: CAB凸显了在复杂项目环境中提供编程辅助的挑战，为未来模型改进提供了方向。

Abstract: Programming assistants powered by large language models have transformed
software development, yet most benchmarks focus narrowly on code generation
tasks. Recent efforts like InfiBench and StackEval attempt to address this gap
using Stack Overflow data but remain limited to single-turn interactions in
isolated contexts, require significant manual curation, and fail to represent
complete project environments. We introduce CodeAssistBench (CAB), the first
benchmark framework for evaluating multi-turn programming assistance in
realistic settings that address real-world questions about actual codebases.
Unlike existing programming Q&A benchmarks, CAB automatically generates
scalable datasets from question-related GitHub issues using configurable
parameters (e.g., repository creation date, star count, programming languages),
and includes automatic containerization of codebases for evaluation. It then
evaluates models through simulated users in these containerized environments
with full codebase access. Using this framework, we constructed a test set of
3,286 real-world programming questions across 231 repositories, spanning seven
programming languages and diverse problem domains. Our evaluation of leading
LLMs reveals a substantial capability gap: while models perform well on Stack
Overflow questions with success rates of 70-83%, they resolve only up to 16.49%
of CAB's recent issues. This discrepancy highlights the challenges of providing
assistance in complex, project-specific contexts versus answering standalone
questions.

</details>


### [219] [Past, Present and Future: Exploring Adaptive AI in Software Development Bots](https://arxiv.org/abs/2507.10822)
*Omar Elsisi,Glaucia Melo*

Key words: 自适应AI、对话代理、软件开发、机器学习、自然语言处理

TL;DR: 本文探讨了自适应AI驱动的对话代理在软件开发中的作用，强调其通过机器学习和自然语言处理提供动态、上下文感知的辅助能力，并分析了其从简单查询系统到高级AI解决方案的演变、集成挑战、优势与限制，以及对未来的展望。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 随着聊天机器人和虚拟助手在软件开发中的普及，研究自适应AI代理如何提升生产力、协作能力及任务自动化成为重要课题。

Method: 通过分析从传统规则系统到现代AI驱动解决方案的演变，结合机器学习和自然语言处理技术，评估自适应AI代理的效率和挑战。

Result: 自适应AI代理能够提供个性化和实时支持，显著提高开发效率，但也面临数据隐私和伦理问题等挑战。

Conclusion: 自适应AI聊天机器人在软件开发中具有巨大潜力，但需要克服技术和社会层面的障碍以实现其革命性影响。

Abstract: Conversational agents, such as chatbots and virtual assistants, have become
essential in software development, boosting productivity, collaboration, and
automating various tasks. This paper examines the role of adaptive AI-powered
conversational agents in software development, highlighting their ability to
offer dynamic, context-aware assistance to developers. Unlike traditional
rule-based systems, adaptive AI agents use machine learning and natural
language processing to learn from interactions and improve over time, providing
more personalized and responsive help. We look at how these tools have evolved
from simple query-based systems to advanced AI-driven solutions like GitHub
Copilot and Microsoft Teams bots. We also explore the challenges of integrating
adaptive AI into software development processes. The study aims to assess the
benefits and limitations of these systems, address concerns like data privacy
and ethical issues, and offer insights into their future use in the field.
Ultimately, adaptive AI chatbots have great potential to revolutionize software
development by delivering real-time, customized support and enhancing the
efficiency of development cycles.

</details>


### [220] [SENSOR: An ML-Enhanced Online Annotation Tool to Uncover Privacy Concerns from User Reviews in Social-Media Applications](https://arxiv.org/abs/2507.10640)
*Labiba Farah,Mohammad Ridwan Kabir,Shohel Ahmed,MD Mohaymen Ul Anam,Md. Sakibul Islam*

Key words: 隐私保护, 用户评论分类, 机器学习, GRU, SENSOR, GRACE

TL;DR: 论文提出了SENSOR工具和GRACE模型，用于自动分类社交媒体应用的用户评论，特别是隐私相关的功能请求和错误报告，提高了分类准确性和效率。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 社交媒体应用的用户评论包含大量隐私相关反馈，但手动分类困难，亟需自动化工具来帮助开发者识别和优先处理隐私问题。

Method: 提出了GRACE模型（基于GRU、CBOW和注意力机制）和SENSOR自动化注释工具，分析了16000条用户评论，并通过人工标注验证数据。

Result: GRACE模型在测试中表现最佳（宏F1分数：0.9434，宏ROC-AUC：0.9934，准确率：95.10%），SENSOR工具显示出显著的实用潜力。

Conclusion: SENSOR和GRACE能有效帮助开发者从用户评论中提取隐私相关问题，增强用户隐私保护和信任。

Abstract: The widespread use of social media applications has raised significant
privacy concerns, often highlighted in user reviews. These reviews also provide
developers with valuable insights into improving apps by addressing issues and
introducing better features. However, the sheer volume and nuanced nature of
reviews make manual identification and prioritization of privacy-related
concerns challenging for developers. Previous studies have developed software
utilities to automatically classify user reviews as privacy-relevant,
privacy-irrelevant, bug reports, feature requests, etc., using machine
learning. Notably, there is a lack of focus on classifying reviews specifically
as privacy-related feature requests, privacy-related bug reports, or
privacy-irrelevant. This paper introduces SENtinel SORt (SENSOR), an automated
online annotation tool designed to help developers annotate and classify user
reviews into these categories. For automating the annotation of such reviews,
this paper introduces the annotation model, GRACE (GRU-based Attention with
CBOW Embedding), using Gated Recurrent Units (GRU) with Continuous Bag of Words
(CBOW) and Attention mechanism. Approximately 16000 user reviews from seven
popular social media apps on Google Play Store, including Instagram, Facebook,
WhatsApp, Snapchat, X (formerly Twitter), Facebook Lite, and Line were
analyzed. Two annotators manually labelled the reviews, achieving a Cohen's
Kappa value of 0.87, ensuring a labeled dataset with high inter-rater agreement
for training machine learning models. Among the models tested, GRACE
demonstrated the best performance (macro F1-score: 0.9434, macro ROC-AUC:
0.9934, and accuracy: 95.10%) despite class imbalance. SENSOR demonstrates
significant potential to assist developers with extracting and addressing
privacy-related feature requests or bug reports from user reviews, enhancing
user privacy and trust.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [221] [Learning to Move in Rhythm: Task-Conditioned Motion Policies with Orbital Stability Guarantees](https://arxiv.org/abs/2507.10602)
*Maximilian Stölzle,T. Konstantin Rusch,Zach J. Patterson,Rodrigo Pérez-Dattari,Francesco Stella,Josie Hughes,Cosimo Della Santina,Daniela Rus*

Key words: Learning from demonstration, Dynamic Motion Primitives, Orbital Stability, Hopf bifurcation, zero-shot generalization

TL;DR: 论文提出了一种新的方法Orbitally Stable Motion Primitives (OSMPs)，用于解决Dynamic Motion Primitives在捕捉复杂周期性行为和任务间插值时的局限性，通过结合学习到的微分同胚编码器和超临界Hopf分岔，实现了稳定且高效的周期性运动学习。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: Dynamic Motion Primitives在捕捉复杂周期性行为和任务间插值时表现不佳，限制了其应用范围。为解决这一问题，作者提出了OSMPs框架。

Method: 结合学习到的微分同胚编码器和超临界Hopf分岔，通过任务条件化的双射编码器实现多运动目标的统一表示。

Result: OSMPs在仿真和真实机器人实验中展现出优异性能，能够零样本泛化到未见过的运动目标，并超越现有基线方法。

Conclusion: OSMPs为学习复杂的周期性运动提供了一种稳定且高效的方法，显著提升了机器人行为的适应性和表现力。

Abstract: Learning from demonstration provides a sample-efficient approach to acquiring
complex behaviors, enabling robots to move robustly, compliantly, and with
fluidity. In this context, Dynamic Motion Primitives offer built - in stability
and robustness to disturbances but often struggle to capture complex periodic
behaviors. Moreover, they are limited in their ability to interpolate between
different tasks. These shortcomings substantially narrow their applicability,
excluding a wide class of practically meaningful tasks such as locomotion and
rhythmic tool use. In this work, we introduce Orbitally Stable Motion
Primitives (OSMPs) - a framework that combines a learned diffeomorphic encoder
with a supercritical Hopf bifurcation in latent space, enabling the accurate
acquisition of periodic motions from demonstrations while ensuring formal
guarantees of orbital stability and transverse contraction. Furthermore, by
conditioning the bijective encoder on the task, we enable a single learned
policy to represent multiple motion objectives, yielding consistent zero-shot
generalization to unseen motion objectives within the training distribution. We
validate the proposed approach through extensive simulation and real-world
experiments across a diverse range of robotic platforms - from collaborative
arms and soft manipulators to a bio-inspired rigid-soft turtle robot -
demonstrating its versatility and effectiveness in consistently outperforming
state-of-the-art baselines such as diffusion policies, among others.

</details>


### [222] [Acting and Planning with Hierarchical Operational Models on a Mobile Robot: A Study with RAE+UPOM](https://arxiv.org/abs/2507.11345)
*Oscar Lima,Marc Vinci,Sunandita Patra,Sebastian Stock,Joachim Hertzberg,Martin Atzmueller,Malik Ghallab,Dana Nau,Paolo Traverso*

Key words: 机器人任务执行、符号规划、集成行动-规划、RAE、UPOM

TL;DR: 论文提出了一种集成行动-规划系统（RAE+UPOM），通过共享层级操作模型解决机器人任务执行中符号规划与实际控制不一致的问题，并在真实场景中验证了其鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 机器人任务执行中符号规划模型与实际控制结构的不一致导致执行困难，亟需一种集成化的解决方案。

Method: 结合Reactive Acting Engine（RAE）与anytime UCT-like Monte Carlo规划器（UPOM），共享层级操作模型，实现行动与规划的交错进行。

Result: 在真实移动机械臂的物体收集任务中，验证了系统在动作失败和传感器噪声下的鲁棒性，并提供了决策流程的实证分析。

Conclusion: RAE+UPOM系统通过集成行动与规划，有效解决了模型不一致问题，为机器人任务执行提供了新思路。

Abstract: Robotic task execution faces challenges due to the inconsistency between
symbolic planner models and the rich control structures actually running on the
robot. In this paper, we present the first physical deployment of an integrated
actor-planner system that shares hierarchical operational models for both
acting and planning, interleaving the Reactive Acting Engine (RAE) with an
anytime UCT-like Monte Carlo planner (UPOM). We implement RAE+UPOM on a mobile
manipulator in a real-world deployment for an object collection task. Our
experiments demonstrate robust task execution under action failures and sensor
noise, and provide empirical insights into the interleaved acting-and-planning
decision making process.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [223] [SQLord: A Robust Enterprise Text-to-SQL Solution via Reverse Data Generation and Workflow Decomposition](https://arxiv.org/abs/2507.10629)
*Song Cheng,Qiannan Cheng,Linbo Jin,Lei Yi,Guannan Zhang*

Key words: NL2SQL, SQLord, 数据逆向生成, 复杂查询分解, GPT-Judge

TL;DR: SQLord是一种企业级NL2SQL框架，通过数据逆向生成和复杂查询分解方法解决了现有框架在复杂业务逻辑和领域数据不足上的问题，并提出了全面的评估框架，显著优于现有技术。

<details>
  <summary>Details</summary>

Main category: cs.DB

Motivation: 针对现有NL2SQL框架在复杂业务逻辑和领域数据不足上的困境，以及评估方法依赖标注数据和可执行数据库环境的问题，提出SQLord框架。

Method: 1. 数据逆向生成将原始SQL转换为标注数据用于监督微调；2. 通过自动化工作流生成器分解复杂查询；3. 提出GPT-Judge评估框架，包括EXE、QSE和SSE。

Result: 离线测试显著优于现有基线，在线准确率稳定超过90%，在全球最大B2B电商平台多场景中成功应用。

Conclusion: SQLord在复杂现实场景中展现出显著优势，解决了领域数据和评估方法的局限性。

Abstract: Transforming natural language into SQL queries (NL2SQL) is crucial for
data-driven business applications. Existing frameworks, trained on open-source
datasets, struggle with complex business logic and lack domain-specific data
for fine-tuning. Additionally, evaluation methods often require annotated data
and executable database environments, which are scarce in real-world scenarios.
To address these challenges, we propose SQLord, an enterprise-level NL2SQL
framework. First, SQLord introduces a data reverse generation approach to
convert raw SQL statements into annotated data for supervised fine-tuning
(SFT). Second, it proposes a decomposition method for complex queries using an
automated workflow generator. Additionally, SQLord features a comprehensive
GPT-Judge evaluation framework, including Execution Evaluation (EXE), Query-SQL
Evaluation (QSE), and SQL-SQL Evaluation (SSE), tailored to diverse scenarios.
Offline tests significantly outperform state of the art baselines, and online
accuracy consistently exceeds 90, highlighting SQLord's advantages and
effectiveness in complex real world scenarios. SQLord has been successfully
applied across multiple scenarios on the world's largest B2B e-commerce
platform.

</details>


### [224] [Towards Practical Benchmarking of Data Cleaning Techniques: On Generating Authentic Errors via Large Language Models](https://arxiv.org/abs/2507.10934)
*Xinyuan Liu,Jiahui Chen,Bocheng Hu,Yu Sun,Xinyang Chen,Shaoxu Song*

Key words: 数据质量, 错误检测, 大语言模型, 表格数据, 合成错误

TL;DR: TableEG是使用大语言模型（LLMs）生成真实错误的框架，填补了合成错误与真实错误之间的差距，并提供了错误检测和纠正的基准。

<details>
  <summary>Details</summary>

Main category: cs.DB

Motivation: 数据质量是数据驱动系统中的关键挑战，但目前缺乏多样化的真实错误数据集，手动标注耗时且不一致。

Method: TableEG通过表格微调策略和三元组表示（I, T, O）模拟错误生成、检测和纠正任务，捕捉表格中的复杂依赖关系。

Result: TableEG生成的错误在模式和分布上更接近真实错误，且性能指标与真实错误数据集高度一致。

Conclusion: TableEG不仅合成真实的错误，还为后续错误检测和纠正任务提供了可靠的基准。

Abstract: Data quality remains an important challenge in data-driven systems, as errors
in tabular data can severely compromise downstream analytics and machine
learning performance. Although numerous error detection algorithms have been
proposed, the lack of diverse, real-world error datasets limits comprehensive
evaluation. Manual error annotation is both time-consuming and inconsistent,
motivating the exploration of synthetic error generation as an alternative. In
this work, we introduce TableEG, a framework that leverages large language
models (LLMs) to generate authentic errors. By employing a table fine-tuning
strategy and a triplet representation $(I, T, O)$ to model error generation,
detection, and correction tasks, TableEG captures the complex dependencies
inherent in two-dimensional tables. Trained on 12 real-world datasets spanning
10 diverse domains, TableEG ensures that the synthesized errors faithfully
reflect authentic error distributions. Experimental results indicate that
errors generated by TableEG exhibit superior pattern and distribution
similarity compared to both rule-based methods and LLM-generated errors without
fine-tuning. Furthermore, performance metrics on TableEG-generated errors
closely align with those on real-world errors across nearly all datasets and
detection algorithms, particularly for machine learning based detection
techniques. Overall, TableEG not only bridges the gap between synthetic and
real-world errors but also establishes a robust benchmark for subsequent error
detection and correction tasks.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [225] [Data-Driven Differential Evolution in Tire Industry Extrusion: Leveraging Surrogate Models](https://arxiv.org/abs/2507.11191)
*Eider Garate-Perez,Kerman López de Calle-Etxabe,Susana Ferreiro*

Key words: 工业优化, 代理模型, 数据驱动, 差分进化算法, 轮胎制造

TL;DR: 该研究提出了一种基于代理模型的数据驱动方法，用于优化复杂工业过程，通过机器学习构建代理模型并结合改进的差分进化算法，显著减少了轮胎制造过程中的初始化时间和材料浪费。

<details>
  <summary>Details</summary>

Main category: cs.CE

Motivation: 工业过程优化中经常缺乏目标函数或约束的数学表达，这成为了一个关键挑战。

Method: 使用机器学习构建代理模型，并集成到改进的差分进化算法（Data-Driven Differential Evolution with Multi-Level Penalty Functions and Surrogate Models）中，应用于轮胎制造业的挤出过程。

Result: 与历史最佳配置相比，初始化时间减少了65%，材料浪费显著降低。

Conclusion: 数据驱动建模与元启发式优化结合在缺乏显式数学表达的场景中具有潜力。

Abstract: The optimization of industrial processes remains a critical challenge,
particularly when no mathematical formulation of objective functions or
constraints is available. This study addresses this issue by proposing a
surrogate-based, data-driven methodology for optimizing complex real-world
manufacturing systems using only historical process data. Machine learning
models are employed to approximate system behavior and construct surrogate
models, which are integrated into a tailored metaheuristic approach:
Data-Driven Differential Evolution with Multi-Level Penalty Functions and
Surrogate Models, an adapted version of Differential Evolution suited to the
characteristics of the studied process. The methodology is applied to an
extrusion process in the tire manufacturing industry, with the goal of
optimizing initialization parameters to reduce waste and production time.
Results show that the surrogate-based optimization approach outperforms
historical best configurations, achieving a 65\% reduction in initialization
and setup time, while also significantly minimizing material waste. These
findings highlight the potential of combining data-driven modeling and
metaheuristic optimization for industrial processes where explicit formulations
are unavailable.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [226] [Auditing Facial Emotion Recognition Datasets for Posed Expressions and Racial Bias](https://arxiv.org/abs/2507.10755)
*Rina Khan,Catherine Stinson*

Key words: 面部表情识别,数据集审计,肤色偏差,伦理挑战

TL;DR: 该研究审计了两个FER数据集，发现其中存在大量摆拍图像，且模型对非白人或深色皮肤人群存在情感识别偏差。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决FER算法在自发表情和肤色差异上的性能与伦理挑战。

Method: 从数据集中随机抽样，分析图像是否为自发或摆拍，并测试模型在不同肤色上的表现。

Result: 数据集存在摆拍图像，模型对非白人或深色皮肤人群有负面情感识别偏差。

Conclusion: 数据集和模型的偏差可能导致现实应用中潜在危害。

Abstract: Facial expression recognition (FER) algorithms classify facial expressions
into emotions such as happy, sad, or angry. An evaluative challenge facing FER
algorithms is the fall in performance when detecting spontaneous expressions
compared to posed expressions. An ethical (and evaluative) challenge facing FER
algorithms is that they tend to perform poorly for people of some races and
skin colors. These challenges are linked to the data collection practices
employed in the creation of FER datasets. In this study, we audit two
state-of-the-art FER datasets. We take random samples from each dataset and
examine whether images are spontaneous or posed. In doing so, we propose a
methodology for identifying spontaneous or posed images. We discover a
significant number of images that were posed in the datasets purporting to
consist of in-the-wild images. Since performance of FER models vary between
spontaneous and posed images, the performance of models trained on these
datasets will not represent the true performance if such models were to be
deployed in in-the-wild applications. We also observe the skin color of
individuals in the samples, and test three models trained on each of the
datasets to predict facial expressions of people from various races and skin
tones. We find that the FER models audited were more likely to predict people
labeled as not white or determined to have dark skin as showing a negative
emotion such as anger or sadness even when they were smiling. This bias makes
such models prone to perpetuate harm in real life applications.

</details>


### [227] [A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Flight Computers](https://arxiv.org/abs/2507.10775)
*Jeffrey Joan Sam,Janhavi Sathe,Nikhil Chigali,Naman Gupta,Radhey Ruparel,Yicheng Jiang,Janmajay Singh,James W. Berck,Arko Barman*

Key words: 航天器图像分割,合成数据集,YOLO模型,自主检测

TL;DR: 论文提出了一个新的航天器图像数据集（64k标注图像），用于训练图像分割模型，并在NASA的TTALOS流程中生成合成背景。模型在真实硬件条件下表现出色。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 航天器在太空环境中易受损，人工或机器人维修成本高，需开发低成本、可靠的自主检测系统，但现有标注数据稀缺。

Method: 使用真实航天器模型与合成背景生成数据集，添加噪声模拟真实图像失真，并微调YOLOv8和YOLOv11模型进行性能测试。

Result: 模型达到Dice分数0.92、Hausdorff距离0.69，推理时间约0.5秒。

Conclusion: 数据集和模型为航天器实时图像分割提供了可靠基准。

Abstract: Spacecraft deployed in outer space are routinely subjected to various forms
of damage due to exposure to hazardous environments. In addition, there are
significant risks to the subsequent process of in-space repairs through human
extravehicular activity or robotic manipulation, incurring substantial
operational costs. Recent developments in image segmentation could enable the
development of reliable and cost-effective autonomous inspection systems. While
these models often require large amounts of training data to achieve
satisfactory results, publicly available annotated spacecraft segmentation data
are very scarce. Here, we present a new dataset of nearly 64k annotated
spacecraft images that was created using real spacecraft models, superimposed
on a mixture of real and synthetic backgrounds generated using NASA's TTALOS
pipeline. To mimic camera distortions and noise in real-world image
acquisition, we also added different types of noise and distortion to the
images. Finally, we finetuned YOLOv8 and YOLOv11 segmentation models to
generate performance benchmarks for the dataset under well-defined hardware and
inference time constraints to mimic real-world image segmentation challenges
for real-time onboard applications in space on NASA's inspector spacecraft. The
resulting models, when tested under these constraints, achieved a Dice score of
0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second.
The dataset and models for performance benchmark are available at
https://github.com/RiceD2KLab/SWiM.

</details>


### [228] [Warehouse Spatial Question Answering with LLM Agent](https://arxiv.org/abs/2507.10778)
*Hsiang-Wei Huang,Jen-Hao Cheng,Kuang-Ming Chen,Cheng-Yen Yang,Bahaa Alattar,Yi-Ru Lin,Pyongkun Kim,Sangwon Kim,Kwangju Kim,Chung-I Huang,Jenq-Neng Hwang*

Key words: 空间理解,多模态大语言模型,LLM代理系统,空间推理,AI City Challenge

TL;DR: 提出了一种数据高效的方法，通过LLM代理系统增强空间推理能力，解决复杂室内仓库场景中的空间问答任务。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的多模态大语言模型在空间理解任务上表现不佳，需要更高效的方法提升能力。

Method: 集成多种工具的LLM代理系统，通过空间推理和API工具交互完成任务。

Result: 在2025 AI City Challenge数据集上表现出高准确性和效率。

Conclusion: 该方法在空间推理任务中具有优越性能，代码已开源。

Abstract: Spatial understanding has been a challenging task for existing Multi-modal
Large Language Models~(MLLMs). Previous methods leverage large-scale MLLM
finetuning to enhance MLLM's spatial understanding ability. In this paper, we
present a data-efficient approach. We propose a LLM agent system with strong
and advanced spatial reasoning ability, which can be used to solve the
challenging spatial question answering task in complex indoor warehouse
scenarios. Our system integrates multiple tools that allow the LLM agent to
conduct spatial reasoning and API tools interaction to answer the given
complicated spatial question. Extensive evaluations on the 2025 AI City
Challenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that
our system achieves high accuracy and efficiency in tasks such as object
retrieval, counting, and distance estimation. The code is available at:
https://github.com/hsiangwei0903/SpatialAgent

</details>


### [229] [Winsor-CAM: Human-Tunable Visual Explanations from Deep Networks via Layer-Wise Winsorization](https://arxiv.org/abs/2507.10846)
*Casey Wall,Longwei Wang,Rodrigue Rizk,KC Santosh*

Key words: CNN 解释性, Grad-CAM, Winsor-CAM, 显著性图, 多层级聚合, 可信 AI

TL;DR: Winsor-CAM 是一种基于 Grad-CAM 的新方法，通过跨卷积层聚合信息和 Winsorization 技术生成更鲁棒和连贯的显著性图。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 为 CNN 提供更透明的决策解释，尤其是在高风险领域，克服 Grad-CAM 在多层信息处理中的局限性。

Method: 提出 Winsor-CAM，利用 Winsorization 技术抑制极端值，并通过用户可调阈值实现语义级控制。

Result: 在多个标准架构（如 ResNet50）和 PASCAL VOC 2012 数据集上，Winsor-CAM 在热图可解释性和定位指标上优于 Grad-CAM。

Conclusion: Winsor-CAM 通过多层级解释和人工可控性，提升了 AI 的可信度。

Abstract: Interpreting the decision-making process of Convolutional Neural Networks
(CNNs) is critical for deploying models in high-stakes domains.
Gradient-weighted Class Activation Mapping (Grad-CAM) is a widely used method
for visual explanations, yet it typically focuses on the final convolutional
layer or na\"ively averages across layers, strategies that can obscure
important semantic cues or amplify irrelevant noise. We propose Winsor-CAM, a
novel, human-tunable extension of Grad-CAM that generates robust and coherent
saliency maps by aggregating information across all convolutional layers. To
mitigate the influence of noisy or extreme attribution values, Winsor-CAM
applies Winsorization, a percentile-based outlier attenuation technique. A
user-controllable threshold allows for semantic-level tuning, enabling flexible
exploration of model behavior across representational hierarchies. Evaluations
on standard architectures (ResNet50, DenseNet121, VGG16, InceptionV3) using the
PASCAL VOC 2012 dataset demonstrate that Winsor-CAM produces more interpretable
heatmaps and achieves superior performance in localization metrics, including
intersection-over-union and center-of-mass alignment, when compared to Grad-CAM
and uniform layer-averaging baselines. Winsor-CAM advances the goal of
trustworthy AI by offering interpretable, multi-layer insights with
human-in-the-loop control.

</details>


### [230] [A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF-Based Preprocessing and YOLO-v11n](https://arxiv.org/abs/2507.10864)
*Saadat Behzadi,Danial Sharifrazi,Bita Mesbahzadeh,Javad Hassannataj Joloudarid,Roohallah Alizadehsani*

Key words: 结肠息肉检测, LOF算法, YOLO-v11n, 深度学习, 医学影像

TL;DR: 本文提出了一种结合LOF算法和YOLO-v11n模型的轻量级结肠息肉检测框架，显著提升了检测性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 结肠息肉的及时准确检测对预防结直肠癌至关重要。该研究旨在开发一种高效且适用于实时临床应用的检测方法。

Method: 研究采用了多数据集（CVC-ColonDB等），通过LOF算法去除噪声数据，结合YOLO-v11n模型进行训练，并使用了5折交叉验证和数据增强策略。

Result: 模型在精度（95.83%）、召回率（91.85%）、F1分数（93.48%）、mAP@0.5（96.48%）和mAP@0.5:0.95（77.75%）上表现优异，优于现有YOLO模型。

Conclusion: 该方法适用于临床实时结肠镜检查，强调了数据预处理和模型效率在医学影像AI系统中的重要性。

Abstract: Objectives: Timely and accurate detection of colorectal polyps plays a
crucial role in diagnosing and preventing colorectal cancer, a major cause of
mortality worldwide. This study introduces a new, lightweight, and efficient
framework for polyp detection that combines the Local Outlier Factor (LOF)
algorithm for filtering noisy data with the YOLO-v11n deep learning model.
  Study design: An experimental study leveraging deep learning and outlier
removal techniques across multiple public datasets.
  Methods: The proposed approach was tested on five diverse and publicly
available datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene.
Since these datasets originally lacked bounding box annotations, we converted
their segmentation masks into suitable detection labels. To enhance the
robustness and generalizability of our model, we apply 5-fold cross-validation
and remove anomalous samples using the LOF method configured with 30 neighbors
and a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a
fast and resource-efficient object detection architecture optimized for
real-time applications. We train the model using a combination of modern
augmentation strategies to improve detection accuracy under diverse conditions.
  Results: Our approach significantly improves polyp localization performance,
achieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5
of 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods,
our model demonstrates enhanced accuracy and efficiency.
  Conclusions: These results suggest that the proposed method is well-suited
for real-time colonoscopy support in clinical settings. Overall, the study
underscores how crucial data preprocessing and model efficiency are when
designing effective AI systems for medical imaging.

</details>


### [231] [Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency](https://arxiv.org/abs/2507.10893)
*Minjong Cheon,Eunhan Goo,Su-Hyeon Shin,Muhammad Ahmed,Hyungjun Kim*

Key words: AI天气预报, CNN, InceptionNeXt, 计算效率, 极端天气

TL;DR: 本文介绍了一种基于CNN的全球天气预报模型KAI-a，通过现代化设计显著降低了计算需求，同时保持了与最先进模型相当的准确性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前AI天气预报模型多基于Transformer架构，计算复杂且资源消耗大，需要一种更高效的设计。

Method: 采用现代化的CNN架构，结合尺度不变设计和InceptionNeXt模块，训练于ERA5数据集。

Result: KAI-a在中等范围天气预报中表现优异，计算资源需求大幅降低，并能有效捕捉极端事件。

Conclusion: KAI-a为数据驱动天气预报提供了一种高效、轻量级的解决方案。

Abstract: Recently, AI-based weather forecast models have achieved impressive advances.
These models have reached accuracy levels comparable to traditional NWP
systems, marking a significant milestone in data-driven weather prediction.
However, they mostly leverage Transformer-based architectures, which often
leads to high training complexity and resource demands due to the massive
parameter sizes. In this study, we introduce a modernized CNN-based model for
global weather forecasting that delivers competitive accuracy while
significantly reducing computational requirements. To present a systematic
modernization roadmap, we highlight key architectural enhancements across
multiple design scales from an earlier CNN-based approach. KAI-a incorporates a
scale-invariant architecture and InceptionNeXt-based blocks within a
geophysically-aware design, tailored to the structure of Earth system data.
Trained on the ERA5 daily dataset with 67 atmospheric variables, the model
contains about 7 million parameters and completes training in just 12 hours on
a single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the
performance of state-of-the-art models in medium-range weather forecasting,
while offering a significantly lightweight design. Furthermore, case studies on
the 2018 European heatwave and the East Asian summer monsoon demonstrate
KAI-a's robust skill in capturing extreme events, reinforcing its practical
utility.

</details>


### [232] [Commuting Distance Regularization for Timescale-Dependent Label Inconsistency in EEG Emotion Recognition](https://arxiv.org/abs/2507.10895)
*Xiaocong Zeng,Craig Michoski,Yan Pang,Dongyang Kuang*

Key words: EEG情感识别, TsDLI, 正则化策略, LVL, LGCL

TL;DR: 提出两种正则化策略（LVL和LGCL）解决EEG情感识别中的时间尺度依赖标签不一致问题，并在实验中优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决EEG情感识别中时间尺度依赖标签不一致（TsDLI）问题，提高模型的泛化性和可解释性。

Method: 提出局部变化损失（LVL）和局部-全局一致性损失（LGCL），结合有界变函数和交换时间距离的数学原理。

Result: 在DREAMER和DEAP数据集上，LVL和LGCL表现优于现有方法，LVL综合排名最佳。

Conclusion: LVL和LGCL能有效解决TsDLI问题，提供更好的性能与可解释性平衡。

Abstract: In this work, we address the often-overlooked issue of Timescale Dependent
Label Inconsistency (TsDLI) in training neural network models for EEG-based
human emotion recognition. To mitigate TsDLI and enhance model generalization
and explainability, we propose two novel regularization strategies: Local
Variation Loss (LVL) and Local-Global Consistency Loss (LGCL). Both methods
incorporate classical mathematical principles--specifically, functions of
bounded variation and commute-time distances--within a graph theoretic
framework. Complementing our regularizers, we introduce a suite of new
evaluation metrics that better capture the alignment between temporally local
predictions and their associated global emotion labels. We validate our
approach through comprehensive experiments on two widely used EEG emotion
datasets, DREAMER and DEAP, across a range of neural architectures including
LSTM and transformer-based models. Performance is assessed using five distinct
metrics encompassing both quantitative accuracy and qualitative consistency.
Results consistently show that our proposed methods outperform state-of-the-art
baselines, delivering superior aggregate performance and offering a principled
trade-off between interpretability and predictive power under label
inconsistency. Notably, LVL achieves the best aggregate rank across all
benchmarked backbones and metrics, while LGCL frequently ranks the second,
highlighting the effectiveness of our framework.

</details>


### [233] [Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection](https://arxiv.org/abs/2507.10977)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Key words: 人机交互检测,小波注意力,射线编码器,多尺度注意力

TL;DR: 该论文提出了一种结合小波注意力似骨干和射线编码器架构的新方法，以提高人机交互检测的效率和准确性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的人机交互检测方法在效率和可靠性上存在不足，主要是由于资源密集的训练方法和低效的架构。

Method: 通过设计小波注意力似骨干和射线编码器架构，聚合低阶和高阶交互特征，优化多尺度注意力。

Result: 在ImageNet和HICO-DET基准测试中表现优越，展示了该架构的潜力。

Conclusion: 新架构显著提升了人机交互检测的性能，代码已开源。

Abstract: Human-object interaction (HOI) detection is essential for accurately
localizing and characterizing interactions between humans and objects,
providing a comprehensive understanding of complex visual scenes across various
domains. However, existing HOI detectors often struggle to deliver reliable
predictions efficiently, relying on resource-intensive training methods and
inefficient architectures. To address these challenges, we conceptualize a
wavelet attention-like backbone and a novel ray-based encoder architecture
tailored for HOI detection. Our wavelet backbone addresses the limitations of
expressing middle-order interactions by aggregating discriminative features
from the low- and high-order interactions extracted from diverse convolutional
filters. Concurrently, the ray-based encoder facilitates multi-scale attention
by optimizing the focus of the decoder on relevant regions of interest and
mitigating computational overhead. As a result of harnessing the attenuated
intensity of learnable ray origins, our decoder aligns query embeddings with
emphasized regions of interest for accurate predictions. Experimental results
on benchmark datasets, including ImageNet and HICO-DET, showcase the potential
of our proposed architecture. The code is publicly available at
[https://github.com/henry-pay/RayEncoder].

</details>


### [234] [SpaRTAN: Spatial Reinforcement Token-based Aggregation Network for Visual Recognition](https://arxiv.org/abs/2507.10999)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Key words: CNN, Transformer, 轻量级架构, 信息冗余, 空间特征, 通道聚合

TL;DR: 论文提出了SpaRTAN，一种轻量级架构设计，通过多尺度空间特征和通道聚合模块，解决了CNN和Transformer的信息冗余问题，在ImageNet和COCO上表现优异且参数高效。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现代CNN和Transformer虽然性能强大，但仍存在简单性偏好和信息冗余问题，影响了模型的效率和表现。

Method: SpaRTAN结合了多尺度空间核和波状通道聚合模块，动态捕捉和增强特征，减少冗余。

Result: 在ImageNet-1k上达到77.7%准确率（仅3.8M参数），COCO上50.0% AP（21.5M参数），均超过基准。

Conclusion: SpaRTAN通过高效设计实现了高性能和低参数量，解决了冗余问题。

Abstract: The resurgence of convolutional neural networks (CNNs) in visual recognition
tasks, exemplified by ConvNeXt, has demonstrated their capability to rival
transformer-based architectures through advanced training methodologies and
ViT-inspired design principles. However, both CNNs and transformers exhibit a
simplicity bias, favoring straightforward features over complex structural
representations. Furthermore, modern CNNs often integrate MLP-like blocks akin
to those in transformers, but these blocks suffer from significant information
redundancies, necessitating high expansion ratios to sustain competitive
performance. To address these limitations, we propose SpaRTAN, a lightweight
architectural design that enhances spatial and channel-wise information
processing. SpaRTAN employs kernels with varying receptive fields, controlled
by kernel size and dilation factor, to capture discriminative multi-order
spatial features effectively. A wave-based channel aggregation module further
modulates and reinforces pixel interactions, mitigating channel-wise
redundancies. Combining the two modules, the proposed network can efficiently
gather and dynamically contextualize discriminative features. Experimental
results in ImageNet and COCO demonstrate that SpaRTAN achieves remarkable
parameter efficiency while maintaining competitive performance. In particular,
on the ImageNet-1k benchmark, SpaRTAN achieves 77. 7% accuracy with only 3.8M
parameters and approximately 1.0 GFLOPs, demonstrating its ability to deliver
strong performance through an efficient design. On the COCO benchmark, it
achieves 50.0% AP, surpassing the previous benchmark by 1.2% with only 21.5M
parameters. The code is publicly available at
[https://github.com/henry-pay/SpaRTAN].

</details>


### [235] [Semantically Informed Salient Regions Guided Radiology Report Generation](https://arxiv.org/abs/2507.11015)
*Zeyi Hou,Zeqiang Wei,Ruixin Yan,Ning Lang,Xiuzhuang Zhou*

Key words: 放射学报告生成,深度学习,SISRNet,数据偏差,跨模态语义

TL;DR: SISRNet通过识别医学关键特征的显著区域，生成更准确的放射学报告，减少数据偏差的影响。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 深度学习算法在自动生成放射学报告中存在医学不准确的问题，主要由于数据偏差和异常区域的稀疏性。

Method: 提出SISRNet方法，利用细粒度跨模态语义识别显著区域，并在图像建模和报告生成中聚焦这些区域。

Result: SISRNet在IU-Xray和MIMIC-CXR数据集上表现优于现有方法。

Conclusion: SISRNet能有效减少数据偏差，生成临床准确的放射学报告。

Abstract: Recent advances in automated radiology report generation from chest X-rays
using deep learning algorithms have the potential to significantly reduce the
arduous workload of radiologists. However, due to the inherent massive data
bias in radiology images, where abnormalities are typically subtle and sparsely
distributed, existing methods often produce fluent yet medically inaccurate
reports, limiting their applicability in clinical practice. To address this
issue effectively, we propose a Semantically Informed Salient Regions-guided
(SISRNet) report generation method. Specifically, our approach explicitly
identifies salient regions with medically critical characteristics using
fine-grained cross-modal semantics. Then, SISRNet systematically focuses on
these high-information regions during both image modeling and report
generation, effectively capturing subtle abnormal findings, mitigating the
negative impact of data bias, and ultimately generating clinically accurate
reports. Compared to its peers, SISRNet demonstrates superior performance on
widely used IU-Xray and MIMIC-CXR datasets.

</details>


### [236] [MMOne: Representing Multiple Modalities in One Scene](https://arxiv.org/abs/2507.11129)
*Zhifeng Gu,Bing Wang*

Key words: 多模态学习,场景表示,模态冲突,MMOne

TL;DR: MMOne框架通过模态建模模块和多模态分解机制解决模态冲突，提升多模态场景表示能力。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 多模态学习因模态间的属性差异和粒度差异而面临挑战，需统一表示方法以增强对物理世界的理解。

Method: 提出MMOne框架，引入模态指示器捕捉模态特性，通过分解机制分离多模态高斯为单模态高斯，解耦共享和特定模态信息。

Result: 实验表明，该方法显著提升各模态表示能力，并能扩展至更多模态。

Conclusion: MMOne通过解耦共享和特定模态信息，实现了紧凑高效的多模态场景表示。

Abstract: Humans perceive the world through multimodal cues to understand and interact
with the environment. Learning a scene representation for multiple modalities
enhances comprehension of the physical world. However, modality conflicts,
arising from inherent distinctions among different modalities, present two
critical challenges: property disparity and granularity disparity. To address
these challenges, we propose a general framework, MMOne, to represent multiple
modalities in one scene, which can be readily extended to additional
modalities. Specifically, a modality modeling module with a novel modality
indicator is proposed to capture the unique properties of each modality.
Additionally, we design a multimodal decomposition mechanism to separate
multi-modal Gaussians into single-modal Gaussians based on modality
differences. We address the essential distinctions among modalities by
disentangling multimodal information into shared and modality-specific
components, resulting in a more compact and efficient multimodal scene
representation. Extensive experiments demonstrate that our method consistently
enhances the representation capability for each modality and is scalable to
additional modalities. The code is available at
https://github.com/Neal2020GitHub/MMOne.

</details>


### [237] [RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images](https://arxiv.org/abs/2507.11143)
*Lam Pham,Cam Le,Hieu Tang,Khang Truong,Truong Nguyen,Jasmin Lampert,Alexander Schindler,Martin Boyer,Son Phan*

Key words: 滑坡观测,深度学习,遥感图像,滑坡检测,滑坡分割

TL;DR: 该论文提出了一种端到端的深度学习模型，通过分析遥感图像自动观测滑坡事件，解决了因地形复杂和大面积观测困难的问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 滑坡灾害频繁发生，传统观测方法因地形复杂和大面积观测难度高而受限，亟需自动化的解决方案。

Method: 利用遥感图像作为输入数据，设计了一种新型神经网络架构，用于滑坡检测和分割任务。

Result: 在三个基准数据集上进行了实验，检测任务的F1分数分别为98.23和93.83，分割任务的mIoU分数为63.74和76.88。

Conclusion: 实验结果证明了该模型在实时滑坡观测系统中的潜在应用价值。

Abstract: In recent years, landslide disasters have reported frequently due to the
extreme weather events of droughts, floods , storms, or the consequence of
human activities such as deforestation, excessive exploitation of natural
resources. However, automatically observing landslide is challenging due to the
extremely large observing area and the rugged topography such as mountain or
highland. This motivates us to propose an end-to-end deep-learning-based model
which explores the remote sensing images for automatically observing landslide
events. By considering remote sensing images as the input data, we can obtain
free resource, observe large and rough terrains by time. To explore the remote
sensing images, we proposed a novel neural network architecture which is for
two tasks of landslide detection and landslide segmentation. We evaluated our
proposed model on three different benchmark datasets of LandSlide4Sense, Bijie,
and Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23,
93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU
scores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense,
Nepal datasets. These experimental results prove potential to integrate our
proposed model into real-life landslide observation systems.

</details>


### [238] [Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling](https://arxiv.org/abs/2507.11061)
*Hayeon Kim,Ji Ha Jang,Se Young Chun*

Key words: 3D编辑,高斯喷涂,局部编辑,SDS损失,3D-GALP,SLaMP

TL;DR: RoMaP提出了一种新型的局部3D高斯编辑框架，结合3D-GALP和SLaMP方法，实现了精确且高灵活性的部分级别编辑。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有方法在多视图2D部分分割和SDS损失的固有模糊性上存在局限性，阻碍了精确的局部3D编辑。

Method: 提出了3D-GALP模块生成稳健的3D掩码，并使用SLaMP方法结合正则化SDS损失优化编辑效果。

Result: 实验表明，RoMaP在重建和生成的高斯场景中实现了最先进的局部3D编辑效果。

Conclusion: RoMaP为3D高斯编辑提供了更强大且灵活的解决方案，特别是在部分级别编辑上表现出色。

Abstract: Recent advances in 3D neural representations and instance-level editing
models have enabled the efficient creation of high-quality 3D content. However,
achieving precise local 3D edits remains challenging, especially for Gaussian
Splatting, due to inconsistent multi-view 2D part segmentations and inherently
ambiguous nature of Score Distillation Sampling (SDS) loss. To address these
limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that
enables precise and drastic part-level modifications. First, we introduce a
robust 3D mask generation module with our 3D-Geometry Aware Label Prediction
(3D-GALP), which uses spherical harmonics (SH) coefficients to model
view-dependent label variations and soft-label property, yielding accurate and
consistent part segmentations across viewpoints. Second, we propose a
regularized SDS loss that combines the standard SDS loss with additional
regularizers. In particular, an L1 anchor loss is introduced via our Scheduled
Latent Mixing and Part (SLaMP) editing method, which generates high-quality
part-edited 2D images and confines modifications only to the target region
while preserving contextual coherence. Additional regularizers, such as
Gaussian prior removal, further improve flexibility by allowing changes beyond
the existing context, and robust 3D masking prevents unintended edits.
Experimental results demonstrate that our RoMaP achieves state-of-the-art local
3D editing on both reconstructed and generated Gaussian scenes and objects
qualitatively and quantitatively, making it possible for more robust and
flexible part-level 3D Gaussian editing.

</details>


### [239] [Joint angle model based learning to refine kinematic human pose estimation](https://arxiv.org/abs/2507.11075)
*Chang Peng,Yifei Zhou,Huifeng Xi,Shiqing Huang,Chuangye Chen,Jianming Yang,Bao Yang,Zhenyu Jiang*

Key words: 人体姿态估计, 关节角建模, 傅里叶级数, 双向循环网络, 姿态优化

TL;DR: 论文提出了一种基于关节角建模的新型方法，用于改进无标记人体姿态估计（HPE）中关键点识别和轨迹平滑的误差问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前HPE在分析运动人体姿态时存在关键点识别错误和轨迹随机波动的问题，而现有深度学习模型的性能受限于不准确的标注数据集。

Method: 1. 设计了一个基于关节角的人体姿态模型；2. 通过高阶傅里叶级数近似关节角的时序变化以获取可靠'真值'；3. 设计双向循环网络作为HRNet的后处理模块以优化估计结果。

Result: 实验表明，基于关节角的改进方法（JAR）在花样滑冰和霹雳舞等挑战性案例中优于现有HPE改进网络。

Conclusion: 所提方法能够有效纠正错误识别的关节并平滑其时空轨迹，显著提升了HPE的性能。

Abstract: Marker-free human pose estimation (HPE) has found increasing applications in
various fields. Current HPE suffers from occasional errors in keypoint
recognition and random fluctuation in keypoint trajectories when analyzing
kinematic human poses. The performance of existing deep learning-based models
for HPE refinement is considerably limited by inaccurate training datasets in
which the keypoints are manually annotated. This paper proposed a novel method
to overcome the difficulty through joint angle-based modeling. The key
techniques include: (i) A joint angle-based model of human pose, which is
robust to describe kinematic human poses; (ii) Approximating temporal variation
of joint angles through high order Fourier series to get reliable "ground
truth"; (iii) A bidirectional recurrent network is designed as a
post-processing module to refine the estimation of well-established HRNet.
Trained with the high-quality dataset constructed using our method, the network
demonstrates outstanding performance to correct wrongly recognized joints and
smooth their spatiotemporal trajectories. Tests show that joint angle-based
refinement (JAR) outperforms the state-of-the-art HPE refinement network in
challenging cases like figure skating and breaking.

</details>


### [240] [Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification](https://arxiv.org/abs/2507.11081)
*Chang Peng,Bao Yang,Meiqi Li,Ge Zhang,Hui Sun,Zhenyu Jiang*

Key words: GPR, RSD, 深度学习, YOLO, 交叉验证

TL;DR: 本文提出了一种基于GPR图像的自动道路地下病害（RSD）识别方法，通过构建高质量数据集和交叉验证策略，显著提升了识别准确率和效率。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: GPR图像用于RSD检测时依赖专家经验且工作量大，深度学习方法的性能受限于数据质量和网络能力。

Method: 构建了包含2134个样本的3D GPR数据集，并基于YOLO模型提出了一种交叉验证策略。

Result: 方法在实地测试中召回率超过98.6%，检测系统可减少约90%的人工工作量。

Conclusion: 该方法有效解决了RSD自动识别的难题，显著提升了检测效率和准确性。

Abstract: Ground penetrating radar (GPR) has become a rapid and non-destructive
solution for road subsurface distress (RSD) detection. However, RSD recognition
from GPR images is labor-intensive and heavily relies on inspectors' expertise.
Deep learning offers the possibility for automatic RSD recognition, but its
current performance is limited by two factors: Scarcity of high-quality dataset
for network training and insufficient capability of network to distinguish RSD.
In this study, a rigorously validated 3D GPR dataset containing 2134 samples of
diverse types was constructed through field scanning. Based on the finding that
the YOLO model trained with one of the three scans of GPR images exhibits
varying sensitivity to specific type of RSD, we proposed a novel
cross-verification strategy with outstanding accuracy in RSD recognition,
achieving recall over 98.6% in field tests. The approach, integrated into an
online RSD detection system, can reduce the labor of inspection by around 90%.

</details>


### [241] [A Robust Incomplete Multimodal Low-Rank Adaptation Approach for Emotion Recognition](https://arxiv.org/abs/2507.11202)
*Xinkui Zhao,Jinsong Shu,Yangyang Wu,Guanjie Cheng,Zihe Liu,Naibo Wang,Shuiguang Deng,Zhongle Xie,Jianwei Yin*

Key words: 多模态情感识别、不完整模态、低秩适应、动态微调、任务准确率

TL;DR: 提出了一种名为MCULoRA的新方法，通过解耦模态组合的共享信息和动态调整训练比例，有效解决多模态情感识别中模态不完整的问题，显著提升了任务准确率。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现实中多模态情感识别常因传感器故障或隐私保护导致模态不完整，现有方法因不同模态组合的训练梯度冲突而性能下降，亟需一种高效的新方法。

Method: MCULoRA方法包含两个模块：模态组合感知的低秩适应（MCLA）解耦共享信息，动态参数微调（DPFT）基于模态表示空间的可分性调整训练比例。

Result: 在多个基准数据集上的实验表明，MCULoRA显著优于此前的不完整多模态学习方法。

Conclusion: MCULoRA通过解耦和动态调整策略，为不完整多模态学习提供了一种高效训练框架，提升了任务性能。

Abstract: Multimodal Emotion Recognition (MER) often encounters incomplete
multimodality in practical applications due to sensor failures or privacy
protection requirements. While existing methods attempt to address various
incomplete multimodal scenarios by balancing the training of each modality
combination through additional gradients, these approaches face a critical
limitation: training gradients from different modality combinations conflict
with each other, ultimately degrading the performance of the final prediction
model. In this paper, we propose a unimodal decoupled dynamic low-rank
adaptation method based on modality combinations, named MCULoRA, which is a
novel framework for the parameter-efficient training of incomplete multimodal
learning models. MCULoRA consists of two key modules, modality combination
aware low-rank adaptation (MCLA) and dynamic parameter fine-tuning (DPFT). The
MCLA module effectively decouples the shared information from the distinct
characteristics of individual modality combinations. The DPFT module adjusts
the training ratio of modality combinations based on the separability of each
modality's representation space, optimizing the learning efficiency across
different modality combinations. Our extensive experimental evaluation in
multiple benchmark datasets demonstrates that MCULoRA substantially outperforms
previous incomplete multimodal learning approaches in downstream task accuracy.

</details>


### [242] [Fairness-Aware Grouping for Continuous Sensitive Variables: Application for Debiasing Face Analysis with respect to Skin Tone](https://arxiv.org/abs/2507.11247)
*Veronika Shilova,Emmanuel Malherbe,Giovanni Palma,Laurent Risser,Jean-Michel Loubes*

Key words: 公平性、连续敏感属性、分组方法、肤色歧视、去偏处理

TL;DR: 该论文提出了一种基于公平性的连续敏感属性分组方法，通过最大化组间歧视差异的新标准，识别关键子群体，并在实验中验证了其鲁棒性和实用性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统公平性评估方法将数据划分为预定义组别，无法捕捉连续敏感属性（如肤色）的细微歧视问题，因此需要更灵活的分组方法。

Method: 采用基于歧视水平的分组策略，提出一种新的标准（组间歧视差异最大化），以识别关键子群体，并在合成数据集和真实数据集（CelebA、FFHQ）上进行验证。

Result: 提出的方法能够揭示敏感属性空间中的歧视模式，并在去偏处理中提升公平性，同时保持模型准确性。

Conclusion: 该方法在连续敏感属性场景下优于传统分组方式，为工业应用提供了可行性。

Abstract: Within a legal framework, fairness in datasets and models is typically
assessed by dividing observations into predefined groups and then computing
fairness measures (e.g., Disparate Impact or Equality of Odds with respect to
gender). However, when sensitive attributes such as skin color are continuous,
dividing into default groups may overlook or obscure the discrimination
experienced by certain minority subpopulations. To address this limitation, we
propose a fairness-based grouping approach for continuous (possibly
multidimensional) sensitive attributes. By grouping data according to observed
levels of discrimination, our method identifies the partition that maximizes a
novel criterion based on inter-group variance in discrimination, thereby
isolating the most critical subgroups.
  We validate the proposed approach using multiple synthetic datasets and
demonstrate its robustness under changing population distributions - revealing
how discrimination is manifested within the space of sensitive attributes.
Furthermore, we examine a specialized setting of monotonic fairness for the
case of skin color. Our empirical results on both CelebA and FFHQ, leveraging
the skin tone as predicted by an industrial proprietary algorithm, show that
the proposed segmentation uncovers more nuanced patterns of discrimination than
previously reported, and that these findings remain stable across datasets for
a given model. Finally, we leverage our grouping model for debiasing purpose,
aiming at predicting fair scores with group-by-group post-processing. The
results demonstrate that our approach improves fairness while having minimal
impact on accuracy, thus confirming our partition method and opening the door
for industrial deployment.

</details>


### [243] [Assessing Color Vision Test in Large Vision-language Models](https://arxiv.org/abs/2507.11153)
*Hongfei Ye,Bin Chen,Wenxi Liu,Yu Zhang,Zhao Li,Dandan Ni,Hongyang Chen*

Key words: 大型视觉语言模型, 色彩视觉, 测试数据集, 微调策略

TL;DR: 该论文探讨了大型视觉语言模型在色彩视觉能力方面的表现，构建了一个多样化的测试数据集，并分析了模型的错误类型及优化策略。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 随着大型视觉语言模型的广泛应用，其色彩视觉能力尚未被充分研究，因此需要填补这一空白。

Method: 定义色彩视觉测试任务，构建多类别、多难度级别的测试数据集，并分析模型错误类型，提出微调策略。

Result: 研究发现大型视觉语言模型在色彩视觉测试中存在特定错误模式，通过微调可显著提升性能。

Conclusion: 论文为评估和改进大型视觉语言模型的色彩视觉能力提供了有效方法和工具。

Abstract: With the widespread adoption of large vision-language models, the capacity
for color vision in these models is crucial. However, the color vision
abilities of large visual-language models have not yet been thoroughly
explored. To address this gap, we define a color vision testing task for large
vision-language models and construct a dataset \footnote{Anonymous Github
Showing some of the data
https://anonymous.4open.science/r/color-vision-test-dataset-3BCD} that covers
multiple categories of test questions and tasks of varying difficulty levels.
Furthermore, we analyze the types of errors made by large vision-language
models and propose fine-tuning strategies to enhance their performance in color
vision tests.

</details>


### [244] [YOLOatr : Deep Learning Based Automatic Target Detection and Localization in Thermal Infrared Imagery](https://arxiv.org/abs/2507.11267)
*Aon Safdar,Usman Akram,Waseem Anwar,Basit Malik,Mian Ibad Ali*

Key words: 目标检测,热红外图像,YOLOatr,深度学习,国防监控

TL;DR: 论文提出了一种改进的单阶段检测器YOLOatr，用于热红外图像中的目标检测和识别，解决了现有技术在国防和监控领域的缺陷，实现了高达99.6%的识别性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 由于热红外图像在国防和监控领域的独特挑战（如硬件限制、天气影响等），现有的深度学习架构在这一领域的表现不佳，因此需要一种更有效的方法。

Method: 基于YOLOv5s改进的YOLOatr模型，通过优化检测头、特征融合和自定义数据增强，提升了目标识别的准确性。

Result: 在DSIAC MWIR数据集上测试，YOLOatr在相关和非相关测试协议下均达到了99.6%的最优性能。

Conclusion: YOLOatr解决了热红外图像中的目标识别难题，实现了实时高性能检测，为国防和监控领域提供了实用的解决方案。

Abstract: Automatic Target Detection (ATD) and Recognition (ATR) from Thermal Infrared
(TI) imagery in the defense and surveillance domain is a challenging computer
vision (CV) task in comparison to the commercial autonomous vehicle perception
domain. Limited datasets, peculiar domain-specific and TI modality-specific
challenges, i.e., limited hardware, scale invariance issues due to greater
distances, deliberate occlusion by tactical vehicles, lower sensor resolution
and resultant lack of structural information in targets, effects of weather,
temperature, and time of day variations, and varying target to clutter ratios
all result in increased intra-class variability and higher inter-class
similarity, making accurate real-time ATR a challenging CV task. Resultantly,
contemporary state-of-the-art (SOTA) deep learning architectures underperform
in the ATR domain. We propose a modified anchor-based single-stage detector,
called YOLOatr, based on a modified YOLOv5s, with optimal modifications to the
detection heads, feature fusion in the neck, and a custom augmentation profile.
We evaluate the performance of our proposed model on a comprehensive DSIAC MWIR
dataset for real-time ATR over both correlated and decorrelated testing
protocols. The results demonstrate that our proposed model achieves
state-of-the-art ATR performance of up to 99.6%.

</details>


### [245] [Implementing Adaptations for Vision AutoRegressive Model](https://arxiv.org/abs/2507.11441)
*Kaif Shaikh,Antoni Kowalczuk,Franziska Boenisch,Adam Dziedzic*

Key words: VAR；DMs；图像生成；医学数据；差分隐私

TL;DR: 本文研究了Vision AutoRegressive model（VAR）在图像生成领域的适应性，尤其是针对医学数据生成的下游任务。同时比较了VAR和Diffusion Models（DMs）的私人适应性，发现VAR在非私人适应性上表现优于DMs，但在私人适应性上仍需改进。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 研究动机在于探索VAR模型的适应性，尤其是在医学数据生成和隐私保护方面的表现，填补VAR在私人适应性上的研究空白。

Method: 通过实现和测试多种VAR适应性策略，并与DM的适应性策略进行对比分析。

Result: 结果表明，VAR在非私人适应性任务上优于DMs，但在私人适应性上性能下降，需要进一步研究。

Conclusion: VAR在非私人适应性任务中表现优秀，但在Differential Privacy方面仍需改进。

Abstract: Vision AutoRegressive model (VAR) was recently introduced as an alternative
to Diffusion Models (DMs) in image generation domain. In this work we focus on
its adaptations, which aim to fine-tune pre-trained models to perform specific
downstream tasks, like medical data generation. While for DMs there exist many
techniques, adaptations for VAR remain underexplored. Similarly, differentially
private (DP) adaptations-ones that aim to preserve privacy of the adaptation
data-have been extensively studied for DMs, while VAR lacks such solutions. In
our work, we implement and benchmark many strategies for VAR, and compare them
to state-of-the-art DM adaptation strategies. We observe that VAR outperforms
DMs for non-DP adaptations, however, the performance of DP suffers, which
necessitates further research in private adaptations for VAR. Code is available
at https://github.com/sprintml/finetuning_var_dp.

</details>


### [246] [CATVis: Context-Aware Thought Visualization](https://arxiv.org/abs/2507.11522)
*Tariq Mehmood,Hamza Ahmad,Muhammad Haroon Shakeel,Murtaza Taj*

Key words: EEG, 脑机接口, 视觉表征, 跨模态对齐, 图像生成

TL;DR: 该论文提出了一个5阶段框架，用于从EEG信号解码视觉表征，通过多模态对齐和重排名提升图像生成质量，实验结果显示其性能优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: EEG信号解码视觉表征的复杂性和噪声问题是一个重要挑战，因此需要一种新方法来提升解码精度和图像生成质量。

Method: 基于5阶段框架，包括EEG编码器分类、跨模态对齐、标题重排名、加权插值和图像生成，结合Stable Diffusion模型。

Result: 生成的图像质量高，与视觉刺激对齐，分类和生成准确率分别提升13.43%和15.21%，FID降低36.61%。

Conclusion: 该方法通过跨模态对齐和重排名实现了上下文感知的EEG到图像生成，显著优于现有技术。

Abstract: EEG-based brain-computer interfaces (BCIs) have shown promise in various
applications, such as motor imagery and cognitive state monitoring. However,
decoding visual representations from EEG signals remains a significant
challenge due to their complex and noisy nature. We thus propose a novel
5-stage framework for decoding visual representations from EEG signals: (1) an
EEG encoder for concept classification, (2) cross-modal alignment of EEG and
text embeddings in CLIP feature space, (3) caption refinement via re-ranking,
(4) weighted interpolation of concept and caption embeddings for richer
semantics, and (5) image generation using a pre-trained Stable Diffusion model.
We enable context-aware EEG-to-image generation through cross-modal alignment
and re-ranking. Experimental results demonstrate that our method generates
high-quality images aligned with visual stimuli, outperforming SOTA approaches
by 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and
reducing Fr\'echet Inception Distance by 36.61%, indicating superior semantic
alignment and image quality.

</details>


### [247] [Streaming 4D Visual Geometry Transformer](https://arxiv.org/abs/2507.11539)
*Dong Zhuo,Wenzhao Zheng,Jiahe Guo,Yuqi Wu,Jie Zhou,Jiwen Lu*

Key words: 4D几何,流式处理,因果变换器,视频重建,实时推理

TL;DR: 论文提出了一种流式4D视觉几何变换器，用于实时感知和重建视频中的4D时空几何，结合因果变换器架构和高效注意力机制，实现了高性能和实时处理。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 为了解决视频中4D时空几何感知与重建的实时性和交互性问题。

Method: 采用因果变换器架构，使用时序因果注意力并缓存历史键值作为隐式记忆，通过知识蒸馏从双向视觉几何变换器学习。

Result: 在多个4D几何感知基准测试中，模型在在线场景中提高了推理速度，同时保持了竞争性能。

Conclusion: 该方法为可扩展和交互式的4D视觉系统提供了可行路径。

Abstract: Perceiving and reconstructing 4D spatial-temporal geometry from videos is a
fundamental yet challenging computer vision task. To facilitate interactive and
real-time applications, we propose a streaming 4D visual geometry transformer
that shares a similar philosophy with autoregressive large language models. We
explore a simple and efficient design and employ a causal transformer
architecture to process the input sequence in an online manner. We use temporal
causal attention and cache the historical keys and values as implicit memory to
enable efficient streaming long-term 4D reconstruction. This design can handle
real-time 4D reconstruction by incrementally integrating historical information
while maintaining high-quality spatial consistency. For efficient training, we
propose to distill knowledge from the dense bidirectional visual geometry
grounded transformer (VGGT) to our causal model. For inference, our model
supports the migration of optimized efficient attention operator (e.g.,
FlashAttention) from the field of large language models. Extensive experiments
on various 4D geometry perception benchmarks demonstrate that our model
increases the inference speed in online scenarios while maintaining competitive
performance, paving the way for scalable and interactive 4D vision systems.
Code is available at: https://github.com/wzzheng/StreamVGGT.

</details>


### [248] [Attributes Shape the Embedding Space of Face Recognition Models](https://arxiv.org/abs/2507.11372)
*Pierrick Leroy,Antonio Mastropietro,Marco Nurisso,Francesco Vaccarino*

Key words: 人脸识别, 几何结构, 属性依赖性, 对齐度量, 可解释性

TL;DR: 论文通过几何方法分析人脸识别模型对不同属性的依赖性或不变性，并提出了一种受物理启发的对齐度量方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 研究揭示深度学习人脸识别模型中嵌入空间的多尺度几何结构，探讨模型对可解释面部和图像属性的依赖性或不变性。

Method: 提出几何方法来描述模型的属性依赖性，引入物理启发的对齐度量，并在合成数据增强的FR模型上进行验证。

Result: 发现模型在不同属性上表现出不同程度的不变性，揭示了其优势和局限性。

Conclusion: 研究提供了模型更深层的可解释性，有助于理解其行为和优化方向。

Abstract: Face Recognition (FR) tasks have made significant progress with the advent of
Deep Neural Networks, particularly through margin-based triplet losses that
embed facial images into high-dimensional feature spaces. During training,
these contrastive losses focus exclusively on identity information as labels.
However, we observe a multiscale geometric structure emerging in the embedding
space, influenced by interpretable facial (e.g., hair color) and image
attributes (e.g., contrast). We propose a geometric approach to describe the
dependence or invariance of FR models to these attributes and introduce a
physics-inspired alignment metric. We evaluate the proposed metric on
controlled, simplified models and widely used FR models fine-tuned with
synthetic data for targeted attribute augmentation. Our findings reveal that
the models exhibit varying degrees of invariance across different attributes,
providing insight into their strengths and weaknesses and enabling deeper
interpretability. Code available here:
https://github.com/mantonios107/attrs-fr-embs}{https://github.com/mantonios107/attrs-fr-embs

</details>


### [249] [COLI: A Hierarchical Efficient Compressor for Large Images](https://arxiv.org/abs/2507.11443)
*Haoran Wang,Hanyu Pei,Yang Lyu,Kai Zhang,Li Li,Feng-Lei Fan*

Key words: 图像压缩,隐式神经表示（INRs）,视频神经表示（NeRV）,Hyper-Compression,医学影像

TL;DR: 论文提出了一种名为COLI的新框架，用于解决大图像压缩问题。通过结合神经表示（INRs）和视频神经表示（NeRV），COLI解决了传统方法的速度和压缩比问题，并展示了在医学影像数据集上的优越表现。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 随着高分辨率、大视野图像的广泛应用，传统压缩方法无法保留关键细节，而数据驱动方法的泛化能力有限。隐式神经表示（INRs）提供了新的思路，但在大图像压缩中仍面临速度和压缩比问题。

Method: COLI框架结合了NeRV，采用预训练-微调模式、混合精度训练和并行化目标加速收敛。同时，提出Hyper-Compression技术提升压缩比。

Result: 在两个医学影像数据集上，COLI在PSNR和SSIM指标上表现优异，显著降低了比特每像素（bpp），并加速NeRV训练达4倍。

Conclusion: COLI为大规模图像压缩提供了高效解决方案，兼具速度和压缩比优势。

Abstract: The escalating adoption of high-resolution, large-field-of-view imagery
amplifies the need for efficient compression methodologies. Conventional
techniques frequently fail to preserve critical image details, while
data-driven approaches exhibit limited generalizability. Implicit Neural
Representations (INRs) present a promising alternative by learning continuous
mappings from spatial coordinates to pixel intensities for individual images,
thereby storing network weights rather than raw pixels and avoiding the
generalization problem. However, INR-based compression of large images faces
challenges including slow compression speed and suboptimal compression ratios.
To address these limitations, we introduce COLI (Compressor for Large Images),
a novel framework leveraging Neural Representations for Videos (NeRV). First,
recognizing that INR-based compression constitutes a training process, we
accelerate its convergence through a pretraining-finetuning paradigm,
mixed-precision training, and reformulation of the sequential loss into a
parallelizable objective. Second, capitalizing on INRs' transformation of image
storage constraints into weight storage, we implement Hyper-Compression, a
novel post-training technique to substantially enhance compression ratios while
maintaining minimal output distortion. Evaluations across two medical imaging
datasets demonstrate that COLI consistently achieves competitive or superior
PSNR and SSIM metrics at significantly reduced bits per pixel (bpp), while
accelerating NeRV training by up to 4 times.

</details>


### [250] [COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation](https://arxiv.org/abs/2507.11488)
*Pakizar Shamoi,Nuray Toganas,Muragul Muratbekova,Elnara Kadyrgali,Adilet Yerkin,Ayan Igali,Malika Ziyada,Ayana Adilova,Aron Karatayev,Yerdauit Torekhan*

Key words: 色彩模型, 模糊逻辑, 人类感知, COLIBRI, 实验验证

TL;DR: 该论文提出了基于人类感知的模糊色彩模型COLIBRI，通过模糊集和逻辑模拟人类色彩感知，实验验证了其优于传统色彩模型。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决计算机难以模拟人类色彩感知的问题，填补计算色彩表示与人类视觉感知之间的鸿沟。

Method: 采用三阶段实验方法，包括初步实验、大规模人类分类调查，生成模糊分区和成员函数。

Result: 模型在人类感知对齐上优于传统色彩模型（如RGB、HSV、LAB），适用于设计、AI等领域。

Conclusion: COLIBRI模型成功模拟人类色彩感知，为相关领域提供了重要工具。

Abstract: Colors are omnipresent in today's world and play a vital role in how humans
perceive and interact with their surroundings. However, it is challenging for
computers to imitate human color perception. This paper introduces the Human
Perception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based
Representation and Interpretation), designed to bridge the gap between
computational color representations and human visual perception. The proposed
model uses fuzzy sets and logic to create a framework for color categorization.
Using a three-phase experimental approach, the study first identifies
distinguishable color stimuli for hue, saturation, and intensity through
preliminary experiments, followed by a large-scale human categorization survey
involving more than 1000 human subjects. The resulting data are used to extract
fuzzy partitions and generate membership functions that reflect real-world
perceptual uncertainty. The model incorporates a mechanism for adaptation that
allows refinement based on feedback and contextual changes. Comparative
evaluations demonstrate the model's alignment with human perception compared to
traditional color models, such as RGB, HSV, and LAB. To the best of our
knowledge, no previous research has documented the construction of a model for
color attribute specification based on a sample of this size or a comparable
sample of the human population (n = 2496). Our findings are significant for
fields such as design, artificial intelligence, marketing, and human-computer
interaction, where perceptually relevant color representation is critical.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [251] [MultiVox: Benchmarking Voice Assistants for Multimodal Interactions](https://arxiv.org/abs/2507.10859)
*Ramaneswaran Selvakumar,Ashish Seth,Nishit Anand,Utkarsh Tyagi,Sonal Kumar,Sreyan Ghosh,Dinesh Manocha*

Key words: 多模态模型,语音助手,副语言特征,视觉线索,基准测试

TL;DR: MultiVox是一个新的全息语音助手基准测试，旨在评估模型整合语音和视觉线索的能力，尤其是副语言特征。

<details>
  <summary>Details</summary>

Main category: cs.MM

Motivation: 现有基准测试未能全面评估语音助手在理解副语音特征和环境声音等方面的能力，也无法充分评估其与视觉信号的结合能力。

Method: 引入MultiVox基准，包含1000条人工标注的语音对话，涵盖多样化的副语言特征和视觉线索。

Result: 对9种先进模型的测试显示，人类表现出色，而现有模型在生成上下文相关回应上表现不佳。

Conclusion: 当前模型在结合语音和视觉线索的理解能力上仍有不足，MultiVox提供了一个更全面的评估工具。

Abstract: The rapid progress of Large Language Models (LLMs) has empowered omni models
to act as voice assistants capable of understanding spoken dialogues. These
models can process multimodal inputs beyond text, such as speech and visual
data, enabling more context-aware interactions. However, current benchmarks
fall short in comprehensively evaluating how well these models generate
context-aware responses, particularly when it comes to implicitly understanding
fine-grained speech characteristics, such as pitch, emotion, timbre, and volume
or the environmental acoustic context such as background sounds. Additionally,
they inadequately assess the ability of models to align paralinguistic cues
with complementary visual signals to inform their responses. To address these
gaps, we introduce MultiVox, the first omni voice assistant benchmark designed
to evaluate the ability of voice assistants to integrate spoken and visual cues
including paralinguistic speech features for truly multimodal understanding.
Specifically, MultiVox includes 1000 human-annotated and recorded speech
dialogues that encompass diverse paralinguistic features and a range of visual
cues such as images and videos. Our evaluation on 9 state-of-the-art models
reveals that, although humans excel at these tasks, current models consistently
struggle to produce contextually grounded responses.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [252] [Comparative Analysis of Vision Transformers and Traditional Deep Learning Approaches for Automated Pneumonia Detection in Chest X-Rays](https://arxiv.org/abs/2507.10589)
*Gaurav Singh*

Key words: 肺炎, 机器学习, 深度学习, Vision Transformers, 胸部X光

TL;DR: 研究比较了传统机器学习和深度学习在肺炎自动检测中的表现，发现Vision Transformers（尤其是Cross-ViT）在准确率和召回率上优于传统方法。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 肺炎（如COVID-19引起的）是全球健康挑战，需要快速准确的诊断方法。

Method: 评估了多种方法，包括传统机器学习（PCA聚类、逻辑回归、支持向量机）和深度学习（CNN、Vision Transformers）。

Result: Cross-ViT表现最佳，准确率88.25%，召回率99.42%，优于其他模型。

Conclusion: Vision Transformers在肺炎自动检测中具有潜力，尤其是Cross-ViT架构。

Abstract: Pneumonia, particularly when induced by diseases like COVID-19, remains a
critical global health challenge requiring rapid and accurate diagnosis. This
study presents a comprehensive comparison of traditional machine learning and
state-of-the-art deep learning approaches for automated pneumonia detection
using chest X-rays (CXRs). We evaluate multiple methodologies, ranging from
conventional machine learning techniques (PCA-based clustering, Logistic
Regression, and Support Vector Classification) to advanced deep learning
architectures including Convolutional Neural Networks (Modified LeNet,
DenseNet-121) and various Vision Transformer (ViT) implementations (Deep-ViT,
Compact Convolutional Transformer, and Cross-ViT). Using a dataset of 5,856
pediatric CXR images, we demonstrate that Vision Transformers, particularly the
Cross-ViT architecture, achieve superior performance with 88.25% accuracy and
99.42% recall, surpassing traditional CNN approaches. Our analysis reveals that
architectural choices impact performance more significantly than model size,
with Cross-ViT's 75M parameters outperforming larger models. The study also
addresses practical considerations including computational efficiency, training
requirements, and the critical balance between precision and recall in medical
diagnostics. Our findings suggest that Vision Transformers offer a promising
direction for automated pneumonia detection, potentially enabling more rapid
and accurate diagnosis during health crises.

</details>


### [253] [Latent Space Consistency for Sparse-View CT Reconstruction](https://arxiv.org/abs/2507.11152)
*Duoyou Chen,Yunqing Chen,Can Zhang,Zhou Wang,Cheng Chen,Ruoxiu Xiao*

Key words: CT重建, 扩散模型, 潜在空间对齐, 跨模态学习, 稀疏视图X射线

TL;DR: 提出了一种名为CLS-DM的新方法，通过跨模态特征对比学习，解决2D X射线与3D CT模态潜在空间对齐问题，提升稀疏视图CT重建效果。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 传统CT成像存在耗时和高辐射问题，稀疏视图CT重建可降低成本与风险，但现有方法在潜在空间对齐上效果不佳。

Method: 采用基于扩散模型的CLS-DM，引入跨模态特征对比学习，从2D X射线图像中提取潜在3D信息并实现模态间潜在空间对齐。

Result: 在LIDC-IDRI和CTSpine1K数据集上，CLS-DM在PSNR和SSIM等体素级指标上优于经典及最新生成模型。

Conclusion: CLS-DM提升了稀疏视图CT重建的效果与经济性，并可泛化至其他跨模态转换任务。

Abstract: Computed Tomography (CT) is a widely utilized imaging modality in clinical
settings. Using densely acquired rotational X-ray arrays, CT can capture 3D
spatial features. However, it is confronted with challenged such as significant
time consumption and high radiation exposure. CT reconstruction methods based
on sparse-view X-ray images have garnered substantial attention from
researchers as they present a means to mitigate costs and risks. In recent
years, diffusion models, particularly the Latent Diffusion Model (LDM), have
demonstrated promising potential in the domain of 3D CT reconstruction.
Nonetheless, due to the substantial differences between the 2D latent
representation of X-ray modalities and the 3D latent representation of CT
modalities, the vanilla LDM is incapable of achieving effective alignment
within the latent space. To address this issue, we propose the Consistent
Latent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature
contrastive learning to efficiently extract latent 3D information from 2D X-ray
images and achieve latent space alignment between modalities. Experimental
results indicate that CLS-DM outperforms classical and state-of-the-art
generative models in terms of standard voxel-level metrics (PSNR, SSIM) on the
LIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing
the effectiveness and economic viability of sparse X-ray reconstructed CT but
can also be generalized to other cross-modal transformation tasks, such as
text-to-image synthesis. We have made our code publicly available at
https://anonymous.4open.science/r/CLS-DM-50D6/ to facilitate further research
and applications in other domains.

</details>


### [254] [HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging](https://arxiv.org/abs/2507.11325)
*Arefin Ittesafun Abian,Ripon Kumar Debnath,Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Md Rafiqul Islam,Asif Karim,Reem E. Mohamed,Sami Azam*

Key words: 肝脏分割,肿瘤分割,HANS-Net,CT图像,深度学习

TL;DR: HANS-Net是一种新型肝脏和肿瘤分割框架，结合双曲卷积、多尺度纹理学习、突触可塑性机制和隐式神经表示，显著提升分割精度和鲁棒性。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 由于肝脏和肿瘤的复杂解剖结构、外观多变性和标注数据有限，准确分割在CT图像中具有挑战性。

Method: HANS-Net结合双曲卷积、小波分解模块、突触可塑性机制和隐式神经表示，并引入不确定性量化和时间注意力模块。

Result: 在LiTS和3D-IRCADb-01数据集上分别达到93.26%和87.45%的Dice分数，表现优异且泛化能力强。

Conclusion: HANS-Net能够提供一致、准确且可靠的肝脏和肿瘤分割，具有临床应用潜力。

Abstract: Accurate liver and tumor segmentation on abdominal CT images is critical for
reliable diagnosis and treatment planning, but remains challenging due to
complex anatomical structures, variability in tumor appearance, and limited
annotated data. To address these issues, we introduce Hyperbolic-convolutions
Adaptive-temporal-attention with Neural-representation and Synaptic-plasticity
Network (HANS-Net), a novel segmentation framework that synergistically
combines hyperbolic convolutions for hierarchical geometric representation, a
wavelet-inspired decomposition module for multi-scale texture learning, a
biologically motivated synaptic plasticity mechanism for adaptive feature
enhancement, and an implicit neural representation branch to model fine-grained
and continuous anatomical boundaries. Additionally, we incorporate
uncertainty-aware Monte Carlo dropout to quantify prediction confidence and
lightweight temporal attention to improve inter-slice consistency without
sacrificing efficiency. Extensive evaluations of the LiTS dataset demonstrate
that HANS-Net achieves a mean Dice score of 93.26%, an IoU of 88.09%, an
average symmetric surface distance (ASSD) of 0.72 mm, and a volume overlap
error (VOE) of 11.91%. Furthermore, cross-dataset validation on the
3D-IRCADb-01 dataset obtains an average Dice of 87.45%, IoU of 80.30%, ASSD of
1.525 mm, and VOE of 19.71%, indicating strong generalization across different
datasets. These results confirm the effectiveness and robustness of HANS-Net in
providing anatomically consistent, accurate, and confident liver and tumor
segmentation.

</details>


### [255] [U-RWKV: Lightweight medical image segmentation with direction-adaptive RWKV](https://arxiv.org/abs/2507.11415)
*Hongbo Ye,Fenghe Tang,Peiang Zhao,Zhen Huang,Dexin Zhao,Minghao Bian,S. Kevin Zhou*

Key words: 医学图像分割、U-RWKV、RWKV架构、长距离建模、资源受限环境

TL;DR: 论文提出了U-RWKV框架，用于高效长距离建模的医学图像分割，结合DARM和SASE模块提升性能。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 解决现有U-Net及其变体因全局有效感受野有限导致的长距离依赖捕捉不足问题，为资源有限环境提供实用方案。

Method: 引入基于RWKV架构的U-RWKV框架，包含DARM（方向自适应RWKV模块）和SASE（阶段自适应挤压激励模块）以优化特征提取。

Result: U-RWNV在计算效率高的同时实现了最先进的图像分割性能。

Conclusion: U-RWKV为资源受限环境提供高效的医学图像分割解决方案。

Abstract: Achieving equity in healthcare accessibility requires lightweight yet
high-performance solutions for medical image segmentation, particularly in
resource-limited settings. Existing methods like U-Net and its variants often
suffer from limited global Effective Receptive Fields (ERFs), hindering their
ability to capture long-range dependencies. To address this, we propose U-RWKV,
a novel framework leveraging the Recurrent Weighted Key-Value(RWKV)
architecture, which achieves efficient long-range modeling at O(N)
computational cost. The framework introduces two key innovations: the
Direction-Adaptive RWKV Module(DARM) and the Stage-Adaptive
Squeeze-and-Excitation Module(SASE). DARM employs Dual-RWKV and QuadScan
mechanisms to aggregate contextual cues across images, mitigating directional
bias while preserving global context and maintaining high computational
efficiency. SASE dynamically adapts its architecture to different feature
extraction stages, balancing high-resolution detail preservation and semantic
relationship capture. Experiments demonstrate that U-RWKV achieves
state-of-the-art segmentation performance with high computational efficiency,
offering a practical solution for democratizing advanced medical imaging
technologies in resource-constrained environments. The code is available at
https://github.com/hbyecoding/U-RWKV.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [256] [Protocols for Verifying Smooth Strategies in Bandits and Games](https://arxiv.org/abs/2507.10567)
*Miranda Christ,Daniel Reichman,Jonathan Shafer*

Key words: 多臂老虎机, 标准博弈, 验证协议, 查询复杂度, 平滑策略

TL;DR: 本文研究了在多臂老虎机和标准博弈中验证策略近似最优性的协议，提出了查询复杂度低于学习成本的验证方法，并应用于验证近似强平滑纳什均衡。

<details>
  <summary>Details</summary>

Main category: cs.GT

Motivation: 由于玩家可选动作数量庞大，研究者希望设计出查询复杂度低于动作数量的验证协议。

Method: 设计了针对平滑策略的验证协议，证明其查询复杂度低于学习成本，并建立了查询复杂度的下限。

Result: 验证协议在多臂老虎机和标准博弈中有效，查询复杂度低于学习成本。

Conclusion: 提出的方法为大规模动作集的策略验证提供了高效途径。

Abstract: We study protocols for verifying approximate optimality of strategies in
multi-armed bandits and normal-form games. As the number of actions available
to each player is often large, we seek protocols where the number of queries to
the utility oracle is sublinear in the number of actions. We prove that such
verification is possible for sufficiently smooth strategies that do not put too
much probability mass on any specific action. We provide protocols for
verifying that a smooth policy for a multi-armed bandit is
$\varepsilon$-optimal. Our verification protocols require provably fewer arm
queries than learning. Furthermore, we establish a nearly-tight lower bound on
the query complexity of verification in our settings. As an application, we
show how to use verification for bandits to achieve verification in normal-form
games. This gives a protocol for verifying whether a given strategy profile is
an approximate strong smooth Nash equilibrium, with a query complexity that is
sublinear in the number of actions.

</details>


### [257] [A Parallelizable Approach for Characterizing NE in Zero-Sum Games After a Linear Number of Iterations of Gradient Descent](https://arxiv.org/abs/2507.11366)
*Taemin Kim,James P. Bailey*

Key words: 零和博弈；在线优化；纳什均衡；哈密顿动力学；交替梯度下降

TL;DR: 研究零和博弈的在线优化方法，提出基于哈密顿动力学的新方法，证明其能在线性迭代次数内找到纳什均衡，且支持并行化与任意学习率。

<details>
  <summary>Details</summary>

Main category: cs.GT

Motivation: 传统方法在零和博弈中要么基于遗憾（时间平均收敛），要么基于收缩映射（最后一次迭代收敛），需要改进效率与灵活性。

Method: 提出基于哈密顿动力学的方法，使用交替梯度下降，证明其能在无界设定下线性迭代次数内找到纳什均衡。

Result: 实验表明该方法显著优于传统方法，且支持并行化和任意学习率。

Conclusion: 基于哈密顿动力学的方法在零和博弈中高效且灵活，是算法博弈论中的首次突破。

Abstract: We study online optimization methods for zero-sum games, a fundamental
problem in adversarial learning in machine learning, economics, and many other
domains. Traditional methods approximate Nash equilibria (NE) using either
regret-based methods (time-average convergence) or contraction-map-based
methods (last-iterate convergence). We propose a new method based on
Hamiltonian dynamics in physics and prove that it can characterize the set of
NE in a finite (linear) number of iterations of alternating gradient descent in
the unbounded setting, modulo degeneracy, a first in online optimization.
Unlike standard methods for computing NE, our proposed approach can be
parallelized and works with arbitrary learning rates, both firsts in
algorithmic game theory. Experimentally, we support our results by showing our
approach drastically outperforms standard methods.

</details>


### [258] [Better Regret Rates in Bilateral Trade via Sublinear Budget Violation](https://arxiv.org/abs/2507.11419)
*Anna Lunghi,Matteo Castiglioni,Alberto Marchesi*

Key words: 双边贸易, 预算平衡, 学习算法, 遗憾, 权衡

TL;DR: 本文研究了双边贸易中预算平衡约束的松弛与学习算法遗憾之间的权衡关系，设计了一个算法，通过允许预算违反，实现了遗憾与违反程度的最优权衡

<details>
  <summary>Details</summary>

Main category: cs.GT

Motivation: 解决双边贸易中预算平衡约束与学习算法遗憾之间的权衡问题

Method: 设计了一个算法，允许预算平衡约束的违反程度在一定范围内变化，进而优化遗憾率

Result: 算法在预算违反程度为T^β时，遗憾率为O(T^(1-β/3))，并给出了匹配的下界

Conclusion: 文章确定了预算违反与遗憾的最优权衡关系，证明了Bernasconi等人的上下界结果的紧性

Abstract: Bilateral trade is a central problem in algorithmic economics, and recent
work has explored how to design trading mechanisms using no-regret learning
algorithms. However, no-regret learning is impossible when budget balance has
to be enforced at each time step. Bernasconi et al. [Ber+24] show how this
impossibility can be circumvented by relaxing the budget balance constraint to
hold only globally over all time steps. In particular, they design an algorithm
achieving regret of the order of $\tilde O(T^{3/4})$ and provide a lower bound
of $\Omega(T^{5/7})$.
  In this work, we interpolate between these two extremes by studying how the
optimal regret rate varies with the allowed violation of the global budget
balance constraint. Specifically, we design an algorithm that, by violating the
constraint by at most $T^{\beta}$ for any given $\beta \in [\frac{3}{4},
\frac{6}{7}]$, attains regret $\tilde O(T^{1 - \beta/3})$. We complement this
result with a matching lower bound, thus fully characterizing the trade-off
between regret and budget violation. Our results show that both the $\tilde
O(T^{3/4})$ upper bound in the global budget balance case and the
$\Omega(T^{5/7})$ lower bound under unconstrained budget balance violation
obtained by Bernasconi et al. [Ber+24] are tight.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [259] [Formal Verification of Variational Quantum Circuits](https://arxiv.org/abs/2507.10635)
*Nicola Assolini,Luca Marzari,Isabella Mastroeni,Alessandra di Pierro*

Key words: 变分量子电路,形式化验证,抽象解释,量子机器学习,鲁棒性

TL;DR: 本文提出了首个针对变分量子电路（VQCs）的形式化验证方法，借鉴了深度学习中抽象解释技术，分析了量子特定因素对验证的影响，并展示了其在标准验证基准上的应用。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 变分量子电路（VQCs）在量子机器学习中广泛应用，但其鲁棒性验证缺乏系统性框架，本文旨在填补这一空白。

Method: 基于抽象解释技术，提出了一种新型语义框架，分析了区间可达性技术在量子环境中的适用性和限制。

Result: 研究表明，量子特有的状态归一化引入了变量间的依赖性，挑战了现有方法，并提出了一种有效的验证方案。

Conclusion: 本文为VQCs的形式化验证提供了理论和实践基础，展示了其在量子机器学习中的潜力。

Abstract: Variational quantum circuits (VQCs) are a central component of many quantum
machine learning algorithms, offering a hybrid quantum-classical framework
that, under certain aspects, can be considered similar to classical deep neural
networks. A shared aspect is, for instance, their vulnerability to adversarial
inputs, small perturbations that can lead to incorrect predictions. While
formal verification techniques have been extensively developed for classical
models, no comparable framework exists for certifying the robustness of VQCs.
Here, we present the first in-depth theoretical and practical study of the
formal verification problem for VQCs. Inspired by abstract interpretation
methods used in deep learning, we analyze the applicability and limitations of
interval-based reachability techniques in the quantum setting. We show that
quantum-specific aspects, such as state normalization, introduce inter-variable
dependencies that challenge existing approaches. We investigate these issues by
introducing a novel semantic framework based on abstract interpretation, where
the verification problem for VQCs can be formally defined, and its complexity
analyzed. Finally, we demonstrate our approach on standard verification
benchmarks.

</details>


### [260] [Stochastic Entanglement Configuration for Constructive Entanglement Topologies in Quantum Machine Learning with Application to Cardiac MRI](https://arxiv.org/abs/2507.11401)
*Mehri Mehrnia,Mohammed S. M. Elbaz*

Key words: 量子机器学习，变分量子电路，纠缠拓扑，随机配置，分类准确率

TL;DR: 提出了一种新的随机纠缠配置方法，用于提升变分量子电路的性能，在心脏MRI疾病分类任务中表现出色。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 当前固定纠缠拓扑方法不适应任务需求，限制了量子机器学习模型的潜力。

Method: 采用随机二进制矩阵编码纠缠拓扑，通过纠缠密度和量子位约束进行可扩展探索。

Result: 生成64种优于经典模型的纠缠配置，集成后分类准确率提升5%。

Conclusion: 随机纠缠配置方法显示出更强的鲁棒性和泛化能力。

Abstract: Efficient entanglement strategies are essential for advancing variational
quantum circuits (VQCs) for quantum machine learning (QML). However, most
current approaches use fixed entanglement topologies that are not adaptive to
task requirements, limiting potential gains over classical models. We introduce
a novel stochastic entanglement configuration method that systematically
generates diverse entanglement topologies to identify a subspace of
constructive entanglement configurations, defined as entanglement topologies
that boost hybrid model performance (e.g., classification accuracy) beyond
classical baselines. Each configuration is encoded as a stochastic binary
matrix, denoting directed entanglement between qubits. This enables scalable
exploration of the hyperspace of candidate entanglement topologies using
entanglement density and per-qubit constraints as key metrics. We define
unconstrained and constrained sampling modes, controlling entanglement per
qubit. Using our method, 400 stochastic configurations were generated and
evaluated in a hybrid QML for cardiac MRI disease classification. We identified
64 (16%) novel constructive entanglement configurations that consistently
outperformed the classical baseline. Ensemble aggregation of top-performing
configurations achieved ~0.92 classification accuracy, exceeding the classical
model (~0.87) by over 5%. Compared to four conventional topologies (ring,
nearest neighbor, no entanglement, fully entangled), none surpassed the
classical baseline (maximum accuracy ~0.82), while our configurations delivered
up to ~20% higher accuracy. Thus, highlighting the robustness and
generalizability of the identified constructive entanglements.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [261] [When and Where do Data Poisons Attack Textual Inversion?](https://arxiv.org/abs/2507.10578)
*Jeremy Styborski,Mingzhi Lyu,Jiayou Lu,Nupur Kapur,Adams Kong*

Key words: 投毒攻击,扩散模型,文本反转,Safe-Zone Training,防御机制,鲁棒性

TL;DR: 本文系统地分析了扩散模型（DMs）中针对文本反转（TI）的投毒攻击行为，提出了新方法Semantic Sensitivity Maps可视化攻击影响，揭示了DMs在时间步上的不均匀学习特性，并提出了一种名为Safe-Zone Training（SZT）的新型防御机制，显著提升了TI的鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 研究扩散模型在文本反转过程中对投毒攻击的脆弱性，以提升其安全性和鲁棒性。

Method: 引入Semantic Sensitivity Maps可视化攻击影响，提出Safe-Zone Training（SZT）防御机制，包括JPEG压缩、限制高时间步训练和损失掩码。

Result: SZT显著提升了扩散模型对投毒攻击的防御能力，生成质量优于现有防御方法。

Conclusion: 通过SZT，可以有效防御扩散模型中的投毒攻击，提升其安全性和生成效果。

Abstract: Poisoning attacks pose significant challenges to the robustness of diffusion
models (DMs). In this paper, we systematically analyze when and where poisoning
attacks textual inversion (TI), a widely used personalization technique for
DMs. We first introduce Semantic Sensitivity Maps, a novel method for
visualizing the influence of poisoning on text embeddings. Second, we identify
and experimentally verify that DMs exhibit non-uniform learning behavior across
timesteps, focusing on lower-noise samples. Poisoning attacks inherit this bias
and inject adversarial signals predominantly at lower timesteps. Lastly, we
observe that adversarial signals distract learning away from relevant concept
regions within training data, corrupting the TI process. Based on these
insights, we propose Safe-Zone Training (SZT), a novel defense mechanism
comprised of 3 key components: (1) JPEG compression to weaken high-frequency
poison signals, (2) restriction to high timesteps during TI training to avoid
adversarial signals at lower timesteps, and (3) loss masking to constrain
learning to relevant regions. Extensive experiments across multiple poisoning
methods demonstrate that SZT greatly enhances the robustness of TI against all
poisoning attacks, improving generative quality beyond prior published
defenses. Code: www.github.com/JStyborski/Diff_Lab Data:
www.github.com/JStyborski/NC10

</details>


### [262] [LaSM: Layer-wise Scaling Mechanism for Defending Pop-up Attack on GUI Agents](https://arxiv.org/abs/2507.10610)
*Zihe Yan,Zhuosheng Zhang*

Key words: GUI代理,多模态大语言模型,注意力机制,弹出攻击,防御方法

TL;DR: 该论文研究了基于多模态大语言模型（MLLM）的图形用户界面（GUI）代理在面对弹出式环境注入攻击时的脆弱性，并提出了一种无需额外训练的层间缩放机制（LaSM）来增强模型的防御能力。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 现有GUI代理在屏幕交互任务中表现出色，但对恶意视觉元素的弹出攻击高度脆弱，现有防御方法要么需要高成本重新训练，要么在归纳干扰下表现不佳。因此，研究如何通过调整模型注意力行为来提升防御能力成为关键。

Method: 论文通过分析攻击对GUI代理注意力行为的影响，发现正确和错误输出之间存在层间注意力差异模式。基于此，提出了一种层间缩放机制（LaSM），选择性放大关键层的注意力和MLP模块，无需额外训练即可提升模型对任务相关区域的关注。

Result: 在12种弹出扰动和4种模型主干上的实验表明，LaSM能显著提高防御成功率，结合提示级警报后，即使在强归纳攻击下也能达到98%的鲁棒性。

Conclusion: 研究表明注意力不匹配是MLLM代理的核心漏洞，通过选择性层间调制可以有效解决这一问题。

Abstract: Graphical user interface (GUI) agents built on multimodal large language
models (MLLMs) have recently demonstrated strong decision-making abilities in
screen-based interaction tasks. However, they remain highly vulnerable to
pop-up-based environmental injection attacks, where malicious visual elements
divert model attention and lead to unsafe or incorrect actions. Existing
defense methods either require costly retraining or perform poorly under
inductive interference. In this work, we systematically study how such attacks
alter the attention behavior of GUI agents and uncover a layer-wise attention
divergence pattern between correct and incorrect outputs. Based on this
insight, we propose \textbf{LaSM}, a \textit{Layer-wise Scaling Mechanism} that
selectively amplifies attention and MLP modules in critical layers. LaSM
improves the alignment between model saliency and task-relevant regions without
additional training. Extensive experiments across 12 types of pop-up
perturbations and 4 different model backbones show that LaSM consistently
enhances the defense success rate. When combined with prompt-level alerts, LaSM
achieves over 98\% robustness even under strong inductive attacks. Our findings
reveal that attention misalignment is a core vulnerability in MLLM agents and
can be effectively addressed through selective layer-wise modulation.

</details>


### [263] [Game Theory Meets LLM and Agentic AI: Reimagining Cybersecurity for the Age of Intelligent Threats](https://arxiv.org/abs/2507.10621)
*Quanyan Zhu*

Key words: 网络安全, 博弈论, 大语言模型, 自主AI, 智能防御

TL;DR: 本文探讨了博弈论、自主AI与网络安全的结合，提出通过大语言模型（LLMs）和自主AI实现理论框架到实际应用的桥梁。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 传统的网络安全方法依赖手动响应和脆弱启发式，迫切需要智能化的防御系统。博弈论提供了建模对抗行为的基础，而LLMs能够将抽象策略转化为实际决策。

Method: 文章回顾了博弈论框架（如静态、动态、贝叶斯和信号博弈），并探讨了如何通过LLM驱动的代理增强网络防御，设计嵌入推理的AI代理游戏。

Result: 研究指出了理论和实际之间的差距，并提出通过LLMs和自主AI的结合，实现更丰富的理论基础和新型解决方案。

Conclusion: 博弈论与自主AI的协同发展有助于构建安全、智能和自适应的网络系统。

Abstract: Protecting cyberspace requires not only advanced tools but also a shift in
how we reason about threats, trust, and autonomy. Traditional cybersecurity
methods rely on manual responses and brittle heuristics. To build proactive and
intelligent defense systems, we need integrated theoretical frameworks and
software tools. Game theory provides a rigorous foundation for modeling
adversarial behavior, designing strategic defenses, and enabling trust in
autonomous systems. Meanwhile, software tools process cyber data, visualize
attack surfaces, verify compliance, and suggest mitigations. Yet a disconnect
remains between theory and practical implementation.
  The rise of Large Language Models (LLMs) and agentic AI offers a new path to
bridge this gap. LLM-powered agents can operationalize abstract strategies into
real-world decisions. Conversely, game theory can inform the reasoning and
coordination of these agents across complex workflows. LLMs also challenge
classical game-theoretic assumptions, such as perfect rationality or static
payoffs, prompting new models aligned with cognitive and computational
realities. This co-evolution promises richer theoretical foundations and novel
solution concepts. Agentic AI also reshapes software design: systems must now
be modular, adaptive, and trust-aware from the outset.
  This chapter explores the intersection of game theory, agentic AI, and
cybersecurity. We review key game-theoretic frameworks (e.g., static, dynamic,
Bayesian, and signaling games) and solution concepts. We then examine how LLM
agents can enhance cyber defense and introduce LLM-driven games that embed
reasoning into AI agents. Finally, we explore multi-agent workflows and
coordination games, outlining how this convergence fosters secure, intelligent,
and adaptive cyber systems.

</details>


### [264] [Spectral Feature Extraction for Robust Network Intrusion Detection Using MFCCs](https://arxiv.org/abs/2507.10622)
*HyeYoung Lee,Muhammad Nadeem,Pavel Tsoi*

Key words: 物联网安全, 异常检测, MFCC, ResNet-18, 深度学习

TL;DR: 本文提出了一种结合MFCC和ResNet-18的新方法，用于物联网（IoT）网络流量中的异常检测与分类，并通过实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 随着物联网网络的快速扩展，安全漏洞激增，急需高效的异常检测与分类技术以提高网络安全性。

Method: 采用Mel频率倒谱系数（MFCC）进行自适应频谱特征表示，结合ResNet-18深度学习模型进行特征提取和分类。

Result: 实验在CICIoT2023、NSL-KDD和IoTID20数据集上展示了优越的性能，证明了方法的有效性。

Conclusion: 该方法通过结合自适应信号处理与深度学习，实现了对异构物联网网络中异常的鲁棒检测与分类。

Abstract: The rapid expansion of Internet of Things (IoT) networks has led to a surge
in security vulnerabilities, emphasizing the critical need for robust anomaly
detection and classification techniques. In this work, we propose a novel
approach for identifying anomalies in IoT network traffic by leveraging the
Mel-frequency cepstral coefficients (MFCC) and ResNet-18, a deep learning model
known for its effectiveness in feature extraction and image-based tasks.
Learnable MFCCs enable adaptive spectral feature representation, capturing the
temporal patterns inherent in network traffic more effectively than traditional
fixed MFCCs. We demonstrate that transforming raw signals into MFCCs maps the
data into a higher-dimensional space, enhancing class separability and enabling
more effective multiclass classification. Our approach combines the strengths
of MFCCs with the robust feature extraction capabilities of ResNet-18, offering
a powerful framework for anomaly detection. The proposed model is evaluated on
three widely used IoT intrusion detection datasets: CICIoT2023, NSL-KDD, and
IoTID20. The experimental results highlight the potential of integrating
adaptive signal processing techniques with deep learning architectures to
achieve robust and scalable anomaly detection in heterogeneous IoT network
landscapes.

</details>


### [265] [PhreshPhish: A Real-World, High-Quality, Large-Scale Phishing Website Dataset and Benchmark](https://arxiv.org/abs/2507.10854)
*Thomas Dalton,Hemanth Gowda,Girish Rao,Sachin Pargi,Alireza Hadj Khodabakhshi,Joseph Rombs,Stephan Jou,Manish Marwah*

Key words: 钓鱼检测、数据集、机器学习、基准测试、PhreshPhish

TL;DR: 该论文介绍了PhreshPhish，一个大规模、高质量的钓鱼网站数据集，解决了现有数据集的质量问题，并提出了一个全面的基准测试套件，用于更真实的模型评估。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 钓鱼攻击日益严重，但现有数据集质量差、基准不真实，导致模型评估结果过于乐观，急需更高质量的数据集和基准测试。

Method: 研究者构建了PhreshPhish数据集，并通过减少泄漏、增加任务难度、增强数据集多样性和调整基础率，设计了基准测试套件。

Result: PhreshPhish数据集比现有公共数据集更大、质量更高，基准测试套件为钓鱼检测提供了更真实的评估标准。

Conclusion: PhreshPhish数据集和基准测试套件将推动钓鱼检测领域的标准化模型比较和进一步研究。

Abstract: Phishing remains a pervasive and growing threat, inflicting heavy economic
and reputational damage. While machine learning has been effective in real-time
detection of phishing attacks, progress is hindered by lack of large,
high-quality datasets and benchmarks. In addition to poor-quality due to
challenges in data collection, existing datasets suffer from leakage and
unrealistic base rates, leading to overly optimistic performance results. In
this paper, we introduce PhreshPhish, a large-scale, high-quality dataset of
phishing websites that addresses these limitations. Compared to existing public
datasets, PhreshPhish is substantially larger and provides significantly higher
quality, as measured by the estimated rate of invalid or mislabeled data
points. Additionally, we propose a comprehensive suite of benchmark datasets
specifically designed for realistic model evaluation by minimizing leakage,
increasing task difficulty, enhancing dataset diversity, and adjustment of base
rates more likely to be seen in the real world. We train and evaluate multiple
solution approaches to provide baseline performance on the benchmark sets. We
believe the availability of this dataset and benchmarks will enable realistic,
standardized model comparison and foster further advances in phishing
detection. The datasets and benchmarks are available on Hugging Face
(https://huggingface.co/datasets/phreshphish/phreshphish).

</details>


### [266] [MalCodeAI: Autonomous Vulnerability Detection and Remediation via Language Agnostic Code Reasoning](https://arxiv.org/abs/2507.10898)
*Jugal Gajjar,Kamalasankari Subramaniakuppusamy,Noha El Kachach*

Key words: AI 安全分析, LoRA, 代码修复, 漏洞检测, 多语言支持

TL;DR: MalCodeAI 是一种语言无关、多阶段 AI 流水线，用于自主代码安全分析和修复，结合代码分解和语义推理，通过 LoRA 优化，支持 14 种编程语言，并取得了低验证损失和高开发者评价。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 传统漏洞检测工具的局限性需要新的方法来应对日益复杂的网络威胁，以保障软件系统的安全。

Method: 使用 Qwen2.5-Coder-3B-Instruct 模型进行代码分解和语义推理，通过 LoRA 优化并在 MLX 框架中实现，分为功能分解和漏洞检测修复两阶段。

Result: 第一阶段验证损失为 0.397，第二阶段为 0.199；开发者评价中实用性、可解释性和输出可读性得分均较高。

Conclusion: MalCodeAI 实现了智能、可解释且以开发者为中心的软件安全解决方案。

Abstract: The growing complexity of cyber threats and the limitations of traditional
vulnerability detection tools necessitate novel approaches for securing
software systems. We introduce MalCodeAI, a language-agnostic, multi-stage AI
pipeline for autonomous code security analysis and remediation. MalCodeAI
combines code decomposition and semantic reasoning using fine-tuned
Qwen2.5-Coder-3B-Instruct models, optimized through Low-Rank Adaptation (LoRA)
within the MLX framework, and delivers scalable, accurate results across 14
programming languages. In Phase 1, the model achieved a validation loss as low
as 0.397 for functional decomposition and summarization of code segments after
200 iterations, 6 trainable layers, and a learning rate of 2 x 10^(-5). In
Phase 2, for vulnerability detection and remediation, it achieved a best
validation loss of 0.199 using the same number of iterations and trainable
layers but with an increased learning rate of 4 x 10^(-5), effectively
identifying security flaws and suggesting actionable fixes. MalCodeAI supports
red-hat-style exploit tracing, CVSS-based risk scoring, and zero-shot
generalization to detect complex, zero-day vulnerabilities. In a qualitative
evaluation involving 15 developers, the system received high scores in
usefulness (mean 8.06/10), interpretability (mean 7.40/10), and readability of
outputs (mean 7.53/10), confirming its practical value in real-world
development workflows. This work marks a significant advancement toward
intelligent, explainable, and developer-centric software security solutions.

</details>


### [267] [Hashed Watermark as a Filter: Defeating Forging and Overwriting Attacks in Weight-based Neural Network Watermarking](https://arxiv.org/abs/2507.11137)
*Yuan Yao,Jin Song,Jian Jin*

Key words: 神经网络水印, 版权保护, 哈希函数, 鲁棒性, 模型安全

TL;DR: 本文提出了一种基于哈希水印滤波器的神经网络水印方法NeuralMark，用于保护深度神经网络的版权。该方法通过哈希函数生成不可逆的二进制水印，并选择模型参数进行嵌入，有效抵御伪造和覆写攻击。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 深度神经网络作为有价值的数字资产，需要强健的所有权保护。现有的基于权重的神经网络水印方法虽然简单实用，但容易受到伪造和覆写攻击。

Method: 提出NeuralMark，利用哈希函数从密钥生成不可逆的二进制水印，并将水印作为滤波器选择嵌入参数。结合平均池化以抵抗微调和剪枝攻击，适用于多种网络架构。

Result: 理论分析了安全性边界，并通过13种卷积和Transformer架构在5个图像分类任务和1个文本生成任务中验证了方法的有效性和鲁棒性。

Conclusion: NeuralMark能够有效防御多种攻击，并具有广泛的适用性，为神经网络版权保护提供了可靠解决方案。

Abstract: As valuable digital assets, deep neural networks necessitate robust ownership
protection, positioning neural network watermarking (NNW) as a promising
solution. Among various NNW approaches, weight-based methods are favored for
their simplicity and practicality; however, they remain vulnerable to forging
and overwriting attacks. To address those challenges, we propose NeuralMark, a
robust method built around a hashed watermark filter. Specifically, we utilize
a hash function to generate an irreversible binary watermark from a secret key,
which is then used as a filter to select the model parameters for embedding.
This design cleverly intertwines the embedding parameters with the hashed
watermark, providing a robust defense against both forging and overwriting
attacks. An average pooling is also incorporated to resist fine-tuning and
pruning attacks. Furthermore, it can be seamlessly integrated into various
neural network architectures, ensuring broad applicability. Theoretically, we
analyze its security boundary. Empirically, we verify its effectiveness and
robustness across 13 distinct Convolutional and Transformer architectures,
covering five image classification tasks and one text generation task. The
source codes are available at https://github.com/AIResearch-Group/NeuralMark.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [268] [Theory of Mind and Self-Disclosure to CUIs](https://arxiv.org/abs/2507.10773)
*Samuel Rhys Cox*

Key words: 自我披露, 对话用户界面, 社交线索, 不确定性, 心智理论

TL;DR: 探讨了在对话用户界面(CUIs)中如何通过表达不确定性或透明化推理来促进自我披露。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 自我披露对心理健康有益，但常因担心他人反应而难以实现。研究旨在探索CUIs中如何通过社交线索鼓励自我披露。

Method: 分析了CUIs中的社交线索，特别是表达不确定性和透明化推理对用户自我披露的影响。

Result: 研究发现，通过让CUI的思维方式更透明，可以增强用户的自我披露意愿。

Conclusion: 透明化CUI的“心智理论”有助于缓解用户的焦虑，促进自我披露。

Abstract: Self-disclosure is important to help us feel better, yet is often difficult.
This difficulty can arise from how we think people are going to react to our
self-disclosure. In this workshop paper, we briefly discuss self-disclosure to
conversational user interfaces (CUIs) in relation to various social cues. We
then, discuss how expressions of uncertainty or representation of a CUI's
reasoning could help encourage self-disclosure, by making a CUI's intended
"theory of mind" more transparent to users.

</details>


### [269] [React to This (RTT): A Nonverbal Turing Test for Embodied AI](https://arxiv.org/abs/2507.10812)
*Chuxuan Zhang,Yasaman Etesam,Angelica Lim*

Key words: 具身AI, 交互意识, 可信度, RTT测试, 非语言行为

TL;DR: 提出了一种测试具身AI代理交互意识和可信度的方法，特别是在人类将其推向极限的场景。提出了"机器能反应吗？"的新问题，并介绍了"React to This"（RTT）测试。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 探索AI代理在人类极限测试下的交互意识和可信度，扩展了图灵测试的范畴。

Method: 设计了RTT测试，专注于非语言行为，并进行了初步实验。

Result: 展示了RTT测试在非语言行为中的初步实验结果。

Conclusion: RTT测试为评估AI代理的反应能力提供了新方法。

Abstract: We propose an approach to test embodied AI agents for interaction awareness
and believability, particularly in scenarios where humans push them to their
limits. Turing introduced the Imitation Game as a way to explore the question:
"Can machines think?" The Total Turing Test later expanded this concept beyond
purely verbal communication, incorporating perceptual and physical interaction.
Building on this, we propose a new guiding question: "Can machines react?" and
introduce the React to This (RTT) test for nonverbal behaviors, presenting
results from an initial experiment.

</details>


### [270] [Role-Playing LLM-Based Multi-Agent Support Framework for Detecting and Addressing Family Communication Bias](https://arxiv.org/abs/2507.11210)
*Rushia Harada,Yuken Kimura,Keito Inoshita*

Key words: LLM, 家庭沟通, 压抑情绪, 理想父母偏见, 多智能体系统

TL;DR: 该研究探讨了LLM技术在家庭心理安全沟通中的应用，开发了一个多智能体对话支持框架，用于检测压抑情绪和理想父母偏见，并提供实用反馈。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 传统指标常忽视家庭中的微妙心理动态，尤其是理想父母偏见对儿童情绪表达和自主性的压制。

Method: 构建了30个日语亲子对话场景的语料库，开发了基于LLM的角色扮演多智能体框架，检测压抑情绪和偏见，生成结构化报告和反馈。

Result: 系统能适度准确地检测压抑情绪，生成的反馈在同理心和实用性上获得高评价，模拟后续对话显示情绪表达和相互理解的改善。

Conclusion: 该框架在支持家庭互动积极转变方面具有潜力。

Abstract: Well-being in family settings involves subtle psychological dynamics that
conventional metrics often overlook. In particular, unconscious parental
expectations, termed ideal parent bias, can suppress children's emotional
expression and autonomy. This suppression, referred to as suppressed emotion,
often stems from well-meaning but value-driven communication, which is
difficult to detect or address from outside the family. Focusing on these
latent dynamics, this study explores Large Language Model (LLM)-based support
for psychologically safe family communication. We constructed a Japanese
parent-child dialogue corpus of 30 scenarios, each annotated with metadata on
ideal parent bias and suppressed emotion. Based on this corpus, we developed a
Role-Playing LLM-based multi-agent dialogue support framework that analyzes
dialogue and generates feedback. Specialized agents detect suppressed emotion,
describe implicit ideal parent bias in parental speech, and infer contextual
attributes such as the child's age and background. A meta-agent compiles these
outputs into a structured report, which is then passed to five selected expert
agents. These agents collaboratively generate empathetic and actionable
feedback through a structured four-step discussion process. Experiments show
that the system can detect categories of suppressed emotion with moderate
accuracy and produce feedback rated highly in empathy and practicality.
Moreover, simulated follow-up dialogues incorporating this feedback exhibited
signs of improved emotional expression and mutual understanding, suggesting the
framework's potential in supporting positive transformation in family
interactions.

</details>
