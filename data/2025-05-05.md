<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 25]
- [cs.LG](#cs.LG) [Total: 74]
- [cs.AI](#cs.AI) [Total: 15]
- [stat.ML](#stat.ML) [Total: 5]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.CV](#cs.CV) [Total: 20]
- [cs.CE](#cs.CE) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.SE](#cs.SE) [Total: 4]
- [stat.ME](#stat.ME) [Total: 2]
- [math.OC](#math.OC) [Total: 3]
- [cs.RO](#cs.RO) [Total: 8]
- [cs.CR](#cs.CR) [Total: 10]
- [cs.NI](#cs.NI) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.SC](#cs.SC) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [FinBERT-QA: Financial Question Answering with pre-trained BERT Language Models](https://arxiv.org/abs/2505.00725)
*Bithiah Yuan*

Main category: cs.CL

TL;DR: 论文提出了一种基于BERT的金融问答系统，通过检索和重排序方法优化非事实性答案选择，并在FiQA数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 金融行业对大规模非结构化和结构化数据自动分析的需求日益增长，问答系统可为金融顾问的决策提供竞争优势。

Method: 采用BERT预训练模型，结合BM25检索和BERT变体重排序，形成两阶段问答系统。研究了多种微调和学习策略。

Result: 提出的FinBERT-QA模型在FiQA数据集任务2上，MRR提升16%，NDCG提升17%，Precision@1提升21%，优于现有技术。

Conclusion: 结合检索与预训练模型重排序的方法有效解决了金融领域数据稀缺和语言特定性问题，显著提升了问答系统的性能。

Abstract: Motivated by the emerging demand in the financial industry for the automatic
analysis of unstructured and structured data at scale, Question Answering (QA)
systems can provide lucrative and competitive advantages to companies by
facilitating the decision making of financial advisers. Consequently, we
propose a novel financial QA system using the transformer-based pre-trained
BERT language model to address the limitations of data scarcity and language
specificity in the financial domain. Our system focuses on financial
non-factoid answer selection, which retrieves a set of passage-level texts and
selects the most relevant as the answer. To increase efficiency, we formulate
the answer selection task as a re-ranking problem, in which our system consists
of an Answer Retriever using BM25, a simple information retrieval approach, to
first return a list of candidate answers, and an Answer Re-ranker built with
variants of pre-trained BERT language models to re-rank and select the most
relevant answers. We investigate various learning, further pre-training, and
fine-tuning approaches for BERT. Our experiments suggest that FinBERT-QA, a
model built from applying the Transfer and Adapt further fine-tuning and
pointwise learning approach, is the most effective, improving the
state-of-the-art results of task 2 of the FiQA dataset by 16% on MRR, 17% on
NDCG, and 21% on Precision@1.

</details>


### [2] [A Survey on Large Language Model based Human-Agent Systems](https://arxiv.org/abs/2505.00753)
*Henry Peng Zou,Wei-Chieh Huang,Yaozu Wu,Yankai Chen,Chunyu Miao,Hoang Nguyen,Yue Zhou,Weizhi Zhang,Liancheng Fang,Langzhou He,Yangning Li,Yuwei Cao,Dongyuan Li,Renhe Jiang,Philip S. Yu*

Main category: cs.CL

TL;DR: 该论文系统综述了基于大语言模型的人机代理系统，探讨了其核心组件、应用及挑战，以促进该领域的研究和创新。


<details>
  <summary>Details</summary>
Motivation: 解决完全自主大语言模型代理在可靠性、复杂任务处理和安全伦理方面的局限，通过引入人机协作提升系统性能。

Method: 系统梳理LLM-HAS的基本概念、核心组件（如环境分析、人类反馈、交互类型等）以及应用场景。

Result: 提供了该领域的首次全面结构化综述，整理了现有知识和研究资源。

Conclusion: LLM-HAS结合人类协作能显著提升代理系统的性能和安全性，未来研究方向多元且充满潜力。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in building fully autonomous agents. However, fully autonomous LLM-based agents
still face significant challenges, including limited reliability due to
hallucinations, difficulty in handling complex tasks, and substantial safety
and ethical risks, all of which limit their feasibility and trustworthiness in
real-world applications. To overcome these limitations, LLM-based human-agent
systems (LLM-HAS) incorporate human-provided information, feedback, or control
into the agent system to enhance system performance, reliability and safety.
This paper provides the first comprehensive and structured survey of LLM-HAS.
It clarifies fundamental concepts, systematically presents core components
shaping these systems, including environment & profiling, human feedback,
interaction types, orchestration and communication, explores emerging
applications, and discusses unique challenges and opportunities. By
consolidating current knowledge and offering a structured overview, we aim to
foster further research and innovation in this rapidly evolving
interdisciplinary field. Paper lists and resources are available at
https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers.

</details>


### [3] [Reasoning Capabilities and Invariability of Large Language Models](https://arxiv.org/abs/2505.00776)
*Alessandro Raganato,Rafael Peñaloza,Marco Viviani,Gabriella Pasi*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在多应用中展现了强大的语言处理能力，但其简单推理能力常受质疑。本文通过新基准数据集分析了LLMs的提示依赖性，发现70B+参数的模型在零样本设定中表现较好，但仍有提升空间。思维链提示的效果取决于答案前或后使用。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在简单推理任务中的能力，特别关注其对提示的依赖性，以揭示其推理能力的局限性。

Method: 引入基于几何图形的基准数据集，测试24种不同规模LLMs在零样本/少样本及思维链提示下的表现。

Result: 70B+参数模型在零样本中表现更优，但整体有待提升；思维链提示的效果因使用时机而异。

Conclusion: LLMs的推理能力仍不完善，提示设计对其表现有显著影响，需进一步研究。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in
manipulating natural language across multiple applications, but their ability
to handle simple reasoning tasks is often questioned. In this work, we aim to
provide a comprehensive analysis of LLMs' reasoning competence, specifically
focusing on their prompt dependency. In particular, we introduce a new
benchmark dataset with a series of simple reasoning questions demanding shallow
logical reasoning. Aligned with cognitive psychology standards, the questions
are confined to a basic domain revolving around geometric figures, ensuring
that responses are independent of any pre-existing intuition about the world
and rely solely on deduction. An empirical analysis involving zero-shot and
few-shot prompting across 24 LLMs of different sizes reveals that, while LLMs
with over 70 billion parameters perform better in the zero-shot setting, there
is still a large room for improvement. An additional test with chain-of-thought
prompting over 22 LLMs shows that this additional prompt can aid or damage the
performance of models, depending on whether the rationale is required before or
after the answer.

</details>


### [4] [Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction](https://arxiv.org/abs/2505.00814)
*Mario Sänger,Ulf Leser*

Main category: cs.CL

TL;DR: 该研究评估了在生物医学关系抽取（RE）任务中，结合上下文信息增强预训练语言模型（PLMs）的效果。通过五个数据集和统一的评估框架，发现模型选择和超参数优化对性能至关重要，而上下文信息对小模型有显著帮助。


<details>
  <summary>Details</summary>
Motivation: 生物医学文献中的关系抽取对管理海量科学知识至关重要，但现有方法在模型、数据库和评估上的差异导致结果难以直接比较和泛化。

Method: 研究在五种数据集上评估了三种基线PLMs，并进行了超参数优化。选定最佳模型后，加入实体描述、知识图谱关系和分子结构编码等上下文信息进行增强。

Result: 结果显示模型选择和超参数优化对性能影响显著，上下文信息整体提升有限，但对小模型效果明显。

Conclusion: 研究强调了模型选择和超参数优化的重要性，上下文信息虽整体收益小，但对资源有限的小模型具有实际价值。

Abstract: Automatic relationship extraction (RE) from biomedical literature is critical
for managing the vast amount of scientific knowledge produced each year. In
recent years, utilizing pre-trained language models (PLMs) has become the
prevalent approach in RE. Several studies report improved performance when
incorporating additional context information while fine-tuning PLMs for RE.
However, variations in the PLMs applied, the databases used for augmentation,
hyper-parameter optimization, and evaluation methods complicate direct
comparisons between studies and raise questions about the generalizability of
these findings. Our study addresses this research gap by evaluating PLMs
enhanced with contextual information on five datasets spanning four relation
scenarios within a consistent evaluation framework. We evaluate three baseline
PLMs and first conduct extensive hyperparameter optimization. After selecting
the top-performing model, we enhance it with additional data, including textual
entity descriptions, relational information from knowledge graphs, and
molecular structure encodings. Our findings illustrate the importance of i) the
choice of the underlying language model and ii) a comprehensive hyperparameter
optimization for achieving strong extraction performance. Although inclusion of
context information yield only minor overall improvements, an ablation study
reveals substantial benefits for smaller PLMs when such external data was
included during fine-tuning.

</details>


### [5] [Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy in English Language Learner Writing](https://arxiv.org/abs/2505.00931)
*Timur Jaganov,John Blake,Julián Villegas,Nicholas Carr*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型（LLMs）在动态评估（DA）中的扩展潜力，通过开发DynaWrite应用程序并测试21种LLMs，发现GPT-4o和neural chat最具潜力，尤其是GPT-4o在生成清晰、一致的动态反馈方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索LLMs如何扩展动态评估的应用范围，使其能够覆盖比传统教师-学习者模式更大的群体。

Method: 开发了DynaWrite应用程序，支持多种LLMs生成动态反馈，并测试了21种LLMs的性能，重点关注GPT-4o和neural chat的表现。

Result: GPT-4o在生成动态反馈的质量上优于neural chat，同时在实时响应和系统稳定性方面表现良好。

Conclusion: 研究表明LLMs能够有效扩展动态评估的应用规模，为语言学习提供更广泛的支持。

Abstract: This study investigates the potential for Large Language Models (LLMs) to
scale-up Dynamic Assessment (DA). To facilitate such an investigation, we first
developed DynaWrite-a modular, microservices-based grammatical tutoring
application which supports multiple LLMs to generate dynamic feedback to
learners of English. Initial testing of 21 LLMs, revealed GPT-4o and neural
chat to have the most potential to scale-up DA in the language learning
classroom. Further testing of these two candidates found both models performed
similarly in their ability to accurately identify grammatical errors in user
sentences. However, GPT-4o consistently outperformed neural chat in the quality
of its DA by generating clear, consistent, and progressively explicit hints.
Real-time responsiveness and system stability were also confirmed through
detailed performance testing, with GPT-4o exhibiting sufficient speed and
stability. This study shows that LLMs can be used to scale-up dynamic
assessment and thus enable dynamic assessment to be delivered to larger groups
than possible in traditional teacher-learner settings.

</details>


### [6] [Llama-Nemotron: Efficient Reasoning Models](https://arxiv.org/abs/2505.00949)
*Akhiad Bercovich,Itay Levy,Izik Golan,Mohammad Dabbah,Ran El-Yaniv,Omri Puny,Ido Galil,Zach Moshe,Tomer Ronen,Najeeb Nabwani,Ido Shahaf,Oren Tropp,Ehud Karpas,Ran Zilberstein,Jiaqi Zeng,Soumye Singhal,Alexander Bukharin,Yian Zhang,Tugrul Konuk,Gerald Shen,Ameya Sunil Mahabaleshwarkar,Bilal Kartal,Yoshi Suhara,Olivier Delalleau,Zijia Chen,Zhilin Wang,David Mosallanezhad,Adi Renduchintala,Haifeng Qian,Dima Rekesh,Fei Jia,Somshubra Majumdar,Vahid Noroozi,Wasi Uddin Ahmad,Sean Narenthiran,Aleksander Ficek,Mehrzad Samadi,Jocelyn Huang,Siddhartha Jain,Igor Gitman,Ivan Moshkov,Wei Du,Shubham Toshniwal,George Armstrong,Branislav Kisacanin,Matvei Novikov,Daria Gitman,Evelina Bakhturina,Jane Polak Scowcroft,John Kamalu,Dan Su,Kezhi Kong,Markus Kliegl,Rabeeh Karimi,Ying Lin,Sanjeev Satheesh,Jupinder Parmar,Pritam Gundecha,Brandon Norick,Joseph Jennings,Shrimai Prabhumoye,Syeda Nahida Akter,Mostofa Patwary,Abhinav Khattar,Deepak Narayanan,Roger Waleffe,Jimmy Zhang,Bor-Yiing Su,Guyue Huang,Terry Kong,Parth Chadha,Sahil Jain,Christine Harvey,Elad Segal,Jining Huang,Sergey Kashirsky,Robert McQueen,Izzy Putterman,George Lam,Arun Venkatesan,Sherry Wu,Vinh Nguyen,Manoj Kilaru,Andrew Wang,Anna Warno,Abhilash Somasamudramath,Sandip Bhaskar,Maka Dong,Nave Assaf,Shahar Mor,Omer Ullman Argov,Scot Junkin,Oleksandr Romanenko,Pedro Larroy,Monika Katariya,Marco Rovinelli,Viji Balas,Nicholas Edelman,Anahita Bhiwandiwalla,Muthu Subramaniam,Smita Ithape,Karthik Ramamoorthy,Yuting Wu,Suguna Varshini Velury,Omri Almog,Joyjit Daw,Denys Fridman,Erick Galinkin,Michael Evans,Katherine Luna,Leon Derczynski,Nikki Pope,Eileen Long,Seth Schneider,Guillermo Siman,Tomasz Grzegorzek,Pablo Ribalta,Monika Katariya,Joey Conway,Trisha Saar,Ann Guan,Krzysztof Pawelec,Shyamala Prayaga,Oleksii Kuchaiev,Boris Ginsburg,Oluwatobi Olabiyi,Kari Briski,Jonathan Cohen,Bryan Catanzaro,Jonah Alben,Yonatan Geifman,Eric Chung*

Main category: cs.CL

TL;DR: Llama-Nemotron系列模型是一个开源的异构推理模型家族，提供卓越的推理能力、高效的推断性能和商业友好的许可。模型分为8B、49B和253B三个规模，训练过程包括架构搜索、知识蒸馏和后训练优化。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个开源的、高效的推理模型家族，支持动态切换推理模式，并为开放研究和模型开发提供资源。

Method: 采用神经网络架构搜索、知识蒸馏和后训练（包括监督微调和大规模强化学习）的方法。

Result: 模型性能与现有最先进推理模型竞争，同时提供更高的推断吞吐量和内存效率。

Conclusion: Llama-Nemotron是首个支持动态推理切换的开源模型，并通过开放的资源和许可推动研究和应用。

Abstract: We introduce the Llama-Nemotron series of models, an open family of
heterogeneous reasoning models that deliver exceptional reasoning capabilities,
inference efficiency, and an open license for enterprise use. The family comes
in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs
competitively with state-of-the-art reasoning models such as DeepSeek-R1 while
offering superior inference throughput and memory efficiency. In this report,
we discuss the training procedure for these models, which entails using neural
architecture search from Llama 3 models for accelerated inference, knowledge
distillation, and continued pretraining, followed by a reasoning-focused
post-training stage consisting of two main parts: supervised fine-tuning and
large scale reinforcement learning. Llama-Nemotron models are the first
open-source models to support a dynamic reasoning toggle, allowing users to
switch between standard chat and reasoning modes during inference. To further
support open research and facilitate model development, we provide the
following resources: 1. We release the Llama-Nemotron reasoning models --
LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA
Open Model License Agreement. 2. We release the complete post-training dataset:
Llama-Nemotron-Post-Training-Dataset. 3. We also release our training
codebases: NeMo, NeMo-Aligner, and Megatron-LM.

</details>


### [7] [A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts](https://arxiv.org/abs/2505.00977)
*Yingquan Chen,Qianmu Li,Xiaocong Wu,Huifeng Li,Qing Chang*

Main category: cs.CL

TL;DR: 本文提出了一种新的嵌入算法CDEA，结合XLNet模型，解决了生成高质量隐写文本的挑战，显著提升了文本质量。


<details>
  <summary>Details</summary>
Motivation: 现有模型和嵌入算法在生成隐写文本时存在局限性，无法有效处理敏感信息的语义内容和随机性，导致文本质量下降。

Method: 采用基于字符的扩散嵌入算法（CDEA），利用敏感信息的统计特性和幂律分布分组方法，提高高概率候选词的选择频率，并结合XLNet模型处理长序列。

Result: 实验表明，CDEA与XLNet的结合显著提升了隐写文本的质量，尤其是在感知不可察觉性方面。

Conclusion: CDEA和XLNet的结合为生成高质量隐写文本提供了有效解决方案。

Abstract: Generating high-quality steganographic text is a fundamental challenge in the
field of generative linguistic steganography. This challenge arises primarily
from two aspects: firstly, the capabilities of existing models in text
generation are limited; secondly, embedding algorithms fail to effectively
mitigate the negative impacts of sensitive information's properties, such as
semantic content or randomness. Specifically, to ensure that the recipient can
accurately extract hidden information, embedding algorithms often have to
consider selecting candidate words with relatively low probabilities. This
phenomenon leads to a decrease in the number of high-probability candidate
words and an increase in low-probability candidate words, thereby compromising
the semantic coherence and logical fluency of the steganographic text and
diminishing the overall quality of the generated steganographic material. To
address this issue, this paper proposes a novel embedding algorithm,
character-based diffusion embedding algorithm (CDEA). Unlike existing embedding
algorithms that strive to eliminate the impact of sensitive information's
properties on the generation process, CDEA leverages sensitive information's
properties. It enhances the selection frequency of high-probability candidate
words in the candidate pool based on general statistical properties at the
character level and grouping methods based on power-law distributions, while
reducing the selection frequency of low-probability candidate words in the
candidate pool. Furthermore, to ensure the effective transformation of
sensitive information in long sequences, we also introduce the XLNet model.
Experimental results demonstrate that the combination of CDEA and XLNet
significantly improves the quality of generated steganographic text,
particularly in terms of perceptual-imperceptibility.

</details>


### [8] [Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models](https://arxiv.org/abs/2505.00979)
*Xuhui Jiang,Shengjie Ma,Chengjin Xu,Cehao Yang,Liyu Zhang,Jian Guo*

Main category: cs.CL

TL;DR: SoG是一个结合跨文档知识关联的合成数据生成框架，通过构建上下文图来提升数据多样性和连贯性，显著增强大语言模型在小规模专业领域的知识获取效率。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在数据稀缺的专业领域学习效率低下的问题，特别是现有方法忽视跨文档知识关联，限制了数据多样性和深度。

Method: 提出SoG框架，通过构建上下文图、图游走策略生成知识关联的合成数据，并结合Chain-of-Thought和Contrastive Clarifying增强数据质量。

Result: 在多项文档Q&A和阅读理解任务中，SoG表现优于或媲美现有最佳方法，展现出更强的泛化能力。

Conclusion: SoG为数据稀缺领域的知识获取提供了高效解决方案，推动了合成数据生成技术的发展。

Abstract: Large Language Models (LLMs) have achieved remarkable success but remain
data-inefficient, especially when learning from small, specialized corpora with
limited and proprietary data. Existing synthetic data generation methods for
continue pre-training focus on intra-document content and overlook
cross-document knowledge associations, limiting content diversity and depth. We
propose Synthetic-on-Graph (SoG), a synthetic data generation framework that
incorporates cross-document knowledge associations for efficient corpus
expansion. SoG constructs a context graph by extracting entities and concepts
from the original corpus, representing cross-document associations, and
employing a graph walk strategy for knowledge-associated sampling. This
enhances synthetic data diversity and coherence, enabling models to learn
complex knowledge structures and handle rare knowledge. To further improve
synthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive
Clarifying (CC) synthetic, enhancing reasoning processes and discriminative
power. Experiments show that SoG outperforms the state-of-the-art (SOTA) method
in a multi-hop document Q&A dataset while performing comparably to the SOTA
method on the reading comprehension task datasets, which also underscores the
better generalization capability of SoG. Our work advances synthetic data
generation and provides practical solutions for efficient knowledge acquisition
in LLMs, especially in domains with limited data availability.

</details>


### [9] [Position: Enough of Scaling LLMs! Lets Focus on Downscaling](https://arxiv.org/abs/2505.00985)
*Ayan Sengupta,Yash Goel,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 该论文主张从神经扩展定律转向缩小规模开发大型语言模型（LLMs），以解决计算效率、环境影响和部署限制等问题，并提出一个综合框架来实现性能与资源需求的平衡。


<details>
  <summary>Details</summary>
Motivation: 传统依赖扩展定律的LLM开发方式在计算效率、环境影响和实际部署上存在显著局限性，作者希望通过缩小规模实现更可持续和高效的开发。

Method: 提出一个综合性的缩小规模框架，旨在减少资源需求的同时保持模型性能，并提供了具体实施策略。

Result: 该框架有望在减少计算资源、降低环境影响的同时，保持或接近传统规模扩展模型的性能。

Conclusion: 通过向缩小规模转变，LLM开发可以实现更高的效率、可持续性和可访问性，为未来研究方向提供新思路。

Abstract: We challenge the dominant focus on neural scaling laws and advocate for a
paradigm shift toward downscaling in the development of large language models
(LLMs). While scaling laws have provided critical insights into performance
improvements through increasing model and dataset size, we emphasize the
significant limitations of this approach, particularly in terms of
computational inefficiency, environmental impact, and deployment constraints.
To address these challenges, we propose a holistic framework for downscaling
LLMs that seeks to maintain performance while drastically reducing resource
demands. This paper outlines practical strategies for transitioning away from
traditional scaling paradigms, advocating for a more sustainable, efficient,
and accessible approach to LLM development.

</details>


### [10] [VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic Services through Natural Language](https://arxiv.org/abs/2505.00989)
*Sijin Sun,Liangbin Zhao,Ming Deng,Xiuju Fu*

Main category: cs.CL

TL;DR: 提出了领域自适应的大型语言模型代理VTS-LLM Agent，用于提升VTS操作的交互式决策支持，通过知识增强的Text-to-SQL任务识别风险船舶，并在多种语言风格下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有VTS系统在面对日益复杂的交通和多模态数据时，时空推理和人机交互能力有限，亟需改进。

Method: 构建了定制化的基准数据集，结合NER关系推理、领域知识注入、语义代数中间表示和查询重思考机制。

Result: VTS-LLM在命令式、操作式和正式自然语言查询中均优于通用和SQL专用基线模型。

Conclusion: 为VTS自然语言接口奠定了基础，展示了LLM驱动海事实时交通管理的潜力。

Abstract: Vessel Traffic Services (VTS) are essential for maritime safety and
regulatory compliance through real-time traffic management. However, with
increasing traffic complexity and the prevalence of heterogeneous, multimodal
data, existing VTS systems face limitations in spatiotemporal reasoning and
intuitive human interaction. In this work, we propose VTS-LLM Agent, the first
domain-adaptive large LLM agent tailored for interactive decision support in
VTS operations. We formalize risk-prone vessel identification as a
knowledge-augmented Text-to-SQL task, combining structured vessel databases
with external maritime knowledge. To support this, we construct a curated
benchmark dataset consisting of a custom schema, domain-specific corpus, and a
query-SQL test set in multiple linguistic styles. Our framework incorporates
NER-based relational reasoning, agent-based domain knowledge injection,
semantic algebra intermediate representation, and query rethink mechanisms to
enhance domain grounding and context-aware understanding. Experimental results
show that VTS-LLM outperforms both general-purpose and SQL-focused baselines
under command-style, operational-style, and formal natural language queries,
respectively. Moreover, our analysis provides the first empirical evidence that
linguistic style variation introduces systematic performance challenges in
Text-to-SQL modeling. This work lays the foundation for natural language
interfaces in vessel traffic services and opens new opportunities for
proactive, LLM-driven maritime real-time traffic management.

</details>


### [11] [Token-free Models for Sarcasm Detection](https://arxiv.org/abs/2505.01006)
*Sumit Mamtani,Maitreya Sonawane,Kanika Agarwal,Nishanth Sanjeev*

Main category: cs.CL

TL;DR: 这篇论文评估了两种无标记模型ByT5和CANINE在社交媒体和非社交媒体领域的讽刺检测任务中的表现，结果显示它们优于标记基模型并达到新的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 标记化是自然语言处理（NLP）流程的基础步骤，但会引入词汇不匹配和词汇外问题。本文动机是探讨直接在字节或字符级处理原始文本的无标记模型是否能够缓解这些限制。

Method: 论文评估了两种无标记模型（ByT5和CANINE），并对它们在社交媒体（Twitter）和非社交媒体（新闻标题）领域的讽刺检测任务进行微调和基准测试。同时将它们与标记基模型和最先进方法进行了比较。

Result: 结果显示，ByT5-small和CANINE模型在新闻标题和Twitter讽刺数据集上的准确率分别提高了0.77%和0.49%，优于标记基模型并达到新的最先进性能。

Conclusion: 这些发现强调了无标记模型在嘈杂和非正式领域（如社交媒体）中实现稳健NLP的潜力。

Abstract: Tokenization is a foundational step in most natural language processing (NLP)
pipelines, yet it introduces challenges such as vocabulary mismatch and
out-of-vocabulary issues. Recent work has shown that models operating directly
on raw text at the byte or character level can mitigate these limitations. In
this paper, we evaluate two token-free models, ByT5 and CANINE, on the task of
sarcasm detection in both social media (Twitter) and non-social media (news
headlines) domains. We fine-tune and benchmark these models against token-based
baselines and state-of-the-art approaches. Our results show that ByT5-small and
CANINE outperform token-based counterparts and achieve new state-of-the-art
performance, improving accuracy by 0.77% and 0.49% on the News Headlines and
Twitter Sarcasm datasets, respectively. These findings underscore the potential
of token-free models for robust NLP in noisy and informal domains such as
social media.

</details>


### [12] [Value Portrait: Understanding Values of LLMs with Human-aligned Benchmark](https://arxiv.org/abs/2505.01015)
*Jongwook Han,Dongmin Choi,Woojung Song,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 论文提出了Value Portrait基准，用于可靠评估语言模型的价值取向，通过真实用户交互和心理学验证方法，发现模型更注重Benevolence、Security和Self-Direction等价值，而忽视Tradition、Power和Achievement。


<details>
  <summary>Details</summary>
Motivation: 现有基准易受价值相关偏见影响，且测试场景与实际使用环境脱节，因此需要更真实、与人类价值观对齐的评估框架。

Method: 提出Value Portrait基准，包含真实用户交互项，并通过人类评分与价值观相关性进行心理学验证。

Result: 评估27个LLM后发现，模型更重视Benevolence、Security和Self-Direction，而较少关注Tradition、Power和Achievement，同时存在对某些 demographic 群体的偏见。

Conclusion: Value Portrait基准提供了生态效度更高的评估框架，揭示了LLM的价值取向偏差，为未来模型优化提供了方向。

Abstract: The importance of benchmarks for assessing the values of language models has
been pronounced due to the growing need of more authentic, human-aligned
responses. However, existing benchmarks rely on human or machine annotations
that are vulnerable to value-related biases. Furthermore, the tested scenarios
often diverge from real-world contexts in which models are commonly used to
generate text and express values. To address these issues, we propose the Value
Portrait benchmark, a reliable framework for evaluating LLMs' value
orientations with two key characteristics. First, the benchmark consists of
items that capture real-life user-LLM interactions, enhancing the relevance of
assessment results to real-world LLM usage and thus ecological validity.
Second, each item is rated by human subjects based on its similarity to their
own thoughts, and correlations between these ratings and the subjects' actual
value scores are derived. This psychometrically validated approach ensures that
items strongly correlated with specific values serve as reliable items for
assessing those values. Through evaluating 27 LLMs with our benchmark, we find
that these models prioritize Benevolence, Security, and Self-Direction values
while placing less emphasis on Tradition, Power, and Achievement values. Also,
our analysis reveals biases in how LLMs perceive various demographic groups,
deviating from real human data.

</details>


### [13] [Do We Need a Detailed Rubric for Automated Essay Scoring using Large Language Models?](https://arxiv.org/abs/2505.01035)
*Lui Yoshida*

Main category: cs.CL

TL;DR: 详细评分标准在自动作文评分中的必要性及影响研究表明，简化评分标准在多数情况下不影响评分准确性，并能显著减少计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 探究评分标准的详细程度对自动作文评分系统的影响，以提高效率而不牺牲准确性。

Method: 采用TOEFL11数据集，比较了完整、简化及无评分标准三种条件，测试四种不同大语言模型的评分表现。

Result: 三种模型在简化评分标准下保持相似准确性且显著减少资源消耗，但Gemini 1.5 Flash在详细标准下表现下降。

Conclusion: 简化的评分标准可作为大多数自动评分应用的更高效选择，但需针对不同模型进行个性化评估。

Abstract: This study investigates the necessity and impact of a detailed rubric in
automated essay scoring (AES) using large language models (LLMs). While using
rubrics are standard in LLM-based AES, creating detailed rubrics requires
substantial ef-fort and increases token usage. We examined how different levels
of rubric detail affect scoring accuracy across multiple LLMs using the TOEFL11
dataset. Our experiments compared three conditions: a full rubric, a simplified
rubric, and no rubric, using four different LLMs (Claude 3.5 Haiku, Gemini 1.5
Flash, GPT-4o-mini, and Llama 3 70B Instruct). Results showed that three out of
four models maintained similar scoring accuracy with the simplified rubric
compared to the detailed one, while significantly reducing token usage.
However, one model (Gemini 1.5 Flash) showed decreased performance with more
detailed rubrics. The findings suggest that simplified rubrics may be
sufficient for most LLM-based AES applications, offering a more efficient
alternative without compromis-ing scoring accuracy. However, model-specific
evaluation remains crucial as per-formance patterns vary across different LLMs.

</details>


### [14] [Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs](https://arxiv.org/abs/2505.01068)
*Yijie Jin,Junjie Peng,Xuanchao Lin,Haochen Yuan,Lan Wang,Cangzhi Zheng*

Main category: cs.CL

TL;DR: 该论文提出了一种基于图结构的交互式掩码机制（GsiT）的高效多模态Transformer，通过权重共享减少了参数数量并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态Transformer（MulTs）在效率上存在缺陷，论文旨在通过图结构表示和IM机制优化其效率。

Method: 提出了GsiT，将MulTs视为层次化模态异构图（HMHGs），并通过IM机制实现权重共享和信息有序融合。

Result: GsiT仅需MulTs 1/3的参数，同时性能显著优于传统MulTs，并在多个SOTA模型中验证了其有效性。

Conclusion: GsiT和HMHG概念在多模态情感分析中展现出高效性和性能优势，为未来的研究方向提供了新思路。

Abstract: Multimodal Sentiment Analysis (MSA) is a rapidly developing field that
integrates multimodal information to recognize sentiments, and existing models
have made significant progress in this area. The central challenge in MSA is
multimodal fusion, which is predominantly addressed by Multimodal Transformers
(MulTs). Although act as the paradigm, MulTs suffer from efficiency concerns.
In this work, from the perspective of efficiency optimization, we propose and
prove that MulTs are hierarchical modal-wise heterogeneous graphs (HMHGs), and
we introduce the graph-structured representation pattern of MulTs. Based on
this pattern, we propose an Interlaced Mask (IM) mechanism to design the
Graph-Structured and Interlaced-Masked Multimodal Transformer (GsiT). It is
formally equivalent to MulTs which achieves an efficient weight-sharing
mechanism without information disorder through IM, enabling All-Modal-In-One
fusion with only 1/3 of the parameters of pure MulTs. A Triton kernel called
Decomposition is implemented to ensure avoiding additional computational
overhead. Moreover, it achieves significantly higher performance than
traditional MulTs. To further validate the effectiveness of GsiT itself and the
HMHG concept, we integrate them into multiple state-of-the-art models and
demonstrate notable performance improvements and parameter reduction on widely
used MSA datasets.

</details>


### [15] [MateICL: Mitigating Attention Dispersion in Large-Scale In-Context Learning](https://arxiv.org/abs/2505.01110)
*Murtadha Ahmed,Wenbo,Liu yunfeng*

Main category: cs.CL

TL;DR: 论文介绍了MateICL方法，通过分窗口处理和重新校准注意力权重，解决大模型在处理长上下文时的注意力分散问题，提升性能且无需外部检索模型。


<details>
  <summary>Details</summary>
Motivation: 解决预训练模型在处理长上下文时因固定位置长度限制而导致的注意力分散问题，提升大模型在有限资源下的In-Context Learning性能。

Method: 将上下文分成多个窗口分别处理，并引入额外层重新校准注意力权重，优先关注查询标记。

Result: MateICL能有效利用更大上下文提升性能，在资源受限环境中优于基于检索的基线方法。

Conclusion: MateICL为长上下文处理提供了一种高效解决方案，尤其在计算资源有限的情况下表现突出。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
In-Context Learning (ICL). However, the fixed position length constraints in
pre-trained models limit the number of demonstration examples. Recent efforts
to extend context suffer from attention dispersion as the number of
demonstrations increases. In this paper, we introduce Mitigating Attention
Dispersion in large-scale ICL (MateICL) that enables LLMs to maintain effective
self-attention as the context size grows. We first split the context into
multiple windows, each filled to the model's context capacity, which are
processed separately. Then, we introduce an additional layer to recalibrate the
attention weights, prioritizing the query tokens as the number of
demonstrations increases. Our empirical results show that MateICL can
effectively leverage larger contexts to improve ICL performance. Compared to
retrieval-based baselines, MateICL consistently achieves better performance
without requiring an externally trained retrieval model. Despite recent
advances in inference strategies (e.g., 32k token contexts), our results
demonstrate that MateICL remains beneficial in computationally
resource-constrained settings. The code is publicly available at
https://github.com/amurtadha/MateICL.

</details>


### [16] [On the Limitations of Steering in Language Model Alignment](https://arxiv.org/abs/2505.01162)
*Chebrolu Niranjan,Kokil Jaidka,Gerard Christopher Yeo*

Main category: cs.CL

TL;DR: 本文提出了一种评估导向向量作为对齐机制局限性的框架，发现其在特定任务（如价值观对齐）中有效，但在复杂场景下可能不足以为通用对齐提供稳健基础。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索导向向量在语言模型对齐任务中的潜力与局限，特别是在不同提示结构和上下文复杂性下的效果。

Method: 采用变压器钩干预和基于反义词的函数向量框架，分析导向向量在不同对齐任务中的表现。

Result: 导向向量在特定对齐任务（如价值观对齐）中表现良好，但在复杂或通用场景中效果有限。

Conclusion: 导向向量虽在特定领域有潜力，但需进一步研究以提升其在复杂推理模型中的通用对齐能力。

Abstract: Steering vectors are a promising approach to aligning language model behavior
at inference time. In this paper, we propose a framework to assess the
limitations of steering vectors as alignment mechanisms. Using a framework of
transformer hook interventions and antonym-based function vectors, we evaluate
the role of prompt structure and context complexity in steering effectiveness.
Our findings indicate that steering vectors are promising for specific
alignment tasks, such as value alignment, but may not provide a robust
foundation for general-purpose alignment in LLMs, particularly in complex
scenarios. We establish a methodological foundation for future investigations
into steering capabilities of reasoning models.

</details>


### [17] [Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc Methods](https://arxiv.org/abs/2505.01198)
*Mahdi Dhaini,Ege Erdogan,Nils Feldhus,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 广泛使用的特征归因方法在解释模型中存在显著的性别差异，影响忠实性、鲁棒性和复杂性，需在监管框架中纳入解释公平性。


<details>
  <summary>Details</summary>
Motivation: 研究揭示了解释方法在不同子组间性能差异的公平性问题，弥补了现有研究中常被忽视的方面。

Method: 通过三个任务和五个语言模型，评估后处理特征归因方法的性别差异及其影响因素。

Result: 即使使用无偏数据集训练或微调，解释方法仍存在性别差异，表明问题并非仅源于数据偏见。

Conclusion: 呼吁在开发和应用可解释性方法时重视解释的公平性，并纳入监管框架以减少高风险场景中的偏见。

Abstract: While research on applications and evaluations of explanation methods
continues to expand, fairness of the explanation methods concerning disparities
in their performance across subgroups remains an often overlooked aspect. In
this paper, we address this gap by showing that, across three tasks and five
language models, widely used post-hoc feature attribution methods exhibit
significant gender disparity with respect to their faithfulness, robustness,
and complexity. These disparities persist even when the models are pre-trained
or fine-tuned on particularly unbiased datasets, indicating that the
disparities we observe are not merely consequences of biased training data. Our
results highlight the importance of addressing disparities in explanations when
developing and applying explainability methods, as these can lead to biased
outcomes against certain subgroups, with particularly critical implications in
high-stakes contexts. Furthermore, our findings underscore the importance of
incorporating the fairness of explanations, alongside overall model fairness
and explainability, as a requirement in regulatory frameworks.

</details>


### [18] [EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on NLP Models](https://arxiv.org/abs/2505.01238)
*Mahdi Dhaini,Kafaite Zahra Hussain,Efstratios Zaradoukas,Gjergji Kasneci*

Main category: cs.CL

TL;DR: EvalxNLP是一个用于评估NLP模型特征归因方法的Python框架，集成了多种解释性技术，支持生成和评估解释，用户满意度高。


<details>
  <summary>Details</summary>
Motivation: 随着NLP模型在高风险应用中的普及，解释性工具的多样性和用户需求的差异使得需要框架来帮助选择适合的解释方法。

Method: EvalxNLP集成了8种解释性技术，支持基于忠实性、合理性和复杂性等关键指标的评估，并提供交互式文本解释。

Result: 用户评估结果显示高满意度，表明EvalxNLP是一个有前景的框架，适用于不同用户群体的解释方法比较。

Conclusion: EvalxNLP通过提供易用且可扩展的平台，旨在普及解释性工具并推动XAI技术在NLP中的系统比较和进步。

Abstract: As Natural Language Processing (NLP) models continue to evolve and become
integral to high-stakes applications, ensuring their interpretability remains a
critical challenge. Given the growing variety of explainability methods and
diverse stakeholder requirements, frameworks that help stakeholders select
appropriate explanations tailored to their specific use cases are increasingly
important. To address this need, we introduce EvalxNLP, a Python framework for
benchmarking state-of-the-art feature attribution methods for transformer-based
NLP models. EvalxNLP integrates eight widely recognized explainability
techniques from the Explainable AI (XAI) literature, enabling users to generate
and evaluate explanations based on key properties such as faithfulness,
plausibility, and complexity. Our framework also provides interactive,
LLM-based textual explanations, facilitating user understanding of the
generated explanations and evaluation outcomes. Human evaluation results
indicate high user satisfaction with EvalxNLP, suggesting it is a promising
framework for benchmarking explanation methods across diverse user groups. By
offering a user-friendly and extensible platform, EvalxNLP aims at
democratizing explainability tools and supporting the systematic comparison and
advancement of XAI techniques in NLP.

</details>


### [19] [PREMISE: Matching-based Prediction for Accurate Review Recommendation](https://arxiv.org/abs/2505.01255)
*Wei Han,Hui Chen,Soujanya Poria*

Main category: cs.CL

TL;DR: PREMISE是一种新的多模态学习架构，通过计算多尺度多领域的表示并过滤重复语义，生成匹配分数特征向量，显著提升了多模态任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态融合方法通过跨模态注意力获取表示，但忽略了重复语义的影响，PREMISE旨在通过匹配分数更高效地提升任务性能。

Method: PREMISE计算多尺度多领域的表示，过滤重复语义后生成匹配分数特征向量，用于下游推荐任务。

Result: 在两个公开数据集上的实验表明，PREMISE在减少计算成本的同时，性能优于当前最先进的融合方法。

Conclusion: PREMISE在多模态任务中表现出色，尤其适用于上下文匹配内容高度相关目标任务。

Abstract: We present PREMISE (PREdict with Matching ScorEs), a new architecture for the
matching-based learning in the multimodal fields for the multimodal review
helpfulness (MRHP) task. Distinct to previous fusion-based methods which
obtains multimodal representations via cross-modal attention for downstream
tasks, PREMISE computes the multi-scale and multi-field representations,
filters duplicated semantics, and then obtained a set of matching scores as
feature vectors for the downstream recommendation task. This new architecture
significantly boosts the performance for such multimodal tasks whose context
matching content are highly correlated to the targets of that task, compared to
the state-of-the-art fusion-based methods. Experimental results on two publicly
available datasets show that PREMISE achieves promising performance with less
computational cost.

</details>


### [20] [Anti-adversarial Learning: Desensitizing Prompts for Large Language Models](https://arxiv.org/abs/2505.01273)
*Xuan Li,Zhe Yin,Xiaodong Gu,Beijun Shen*

Main category: cs.CL

TL;DR: PromptObfus 是一种通过反对抗学习扰动隐私词来保护 LLM 提示隐私的新方法，能在保持任务性能的同时防止隐私泄露。


<details>
  <summary>Details</summary>
Motivation: 随着 LLM 的广泛应用，用户提示中的隐私保护变得至关重要，而传统隐私保护技术因计算成本和用户参与的高要求无法有效适用于 LLM 场景。

Method: 将提示脱敏视为掩码语言建模任务，用 [MASK] 替换隐私敏感词，并通过替代模型的梯度反馈选择候选替换词。

Result: 在三个 NLP 任务上，PromptObfus 能有效防止远程 LLM 的隐私推理，同时保持任务性能。

Conclusion: PromptObfus 为 LLM 提示隐私保护提供了一种高效可行的解决方案。

Abstract: With the widespread use of LLMs, preserving privacy in user prompts has
become crucial, as prompts risk exposing privacy and sensitive data to the
cloud LLMs. Traditional techniques like homomorphic encryption, secure
multi-party computation, and federated learning face challenges due to heavy
computational costs and user participation requirements, limiting their
applicability in LLM scenarios. In this paper, we propose PromptObfus, a novel
method for desensitizing LLM prompts. The core idea of PromptObfus is
"anti-adversarial" learning, which perturbs privacy words in the prompt to
obscure sensitive information while retaining the stability of model
predictions. Specifically, PromptObfus frames prompt desensitization as a
masked language modeling task, replacing privacy-sensitive terms with a [MASK]
token. A desensitization model is trained to generate candidate replacements
for each masked position. These candidates are subsequently selected based on
gradient feedback from a surrogate model, ensuring minimal disruption to the
task output. We demonstrate the effectiveness of our approach on three NLP
tasks. Results show that PromptObfus effectively prevents privacy inference
from remote LLMs while preserving task performance.

</details>


### [21] [TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models](https://arxiv.org/abs/2504.20605)
*Mihai Nadas,Laura Diosan,Andrei Piscoran,Andreea Tomescu*

Main category: cs.CL

TL;DR: 论文介绍了TF1-EN-3M，一个由不超过8B参数的指令调优模型生成的300万条英语道德故事数据集，填补了现代NLP领域缺乏大规模结构化道德叙事数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 现代NLP缺乏一个将连贯叙事与明确道德教训相结合的大规模结构化语料库，作者希望通过生成高质量、多样化的道德故事数据集来填补这一空白。

Method: 使用基于组合提示的六槽位框架生成故事，并通过混合评估管道（GPT评分和无参考多样性/可读性指标）评估生成质量。

Result: 在10个候选模型中，8B参数的Llama-3变体在质量和速度上达到最佳平衡，单块消费级GPU即可高效生成高质量故事。

Conclusion: TF1-EN-3M的发布为指令遵循、叙事智能、价值对齐等研究提供了资源，证明大规模道德故事生成无需依赖专有大模型。

Abstract: Moral stories are a time-tested vehicle for transmitting values, yet modern
NLP lacks a large, structured corpus that couples coherent narratives with
explicit ethical lessons. We close this gap with TF1-EN-3M, the first open
dataset of three million English-language fables generated exclusively by
instruction-tuned models no larger than 8B parameters. Each story follows a
six-slot scaffold (character -> trait -> setting -> conflict -> resolution ->
moral), produced through a combinatorial prompt engine that guarantees genre
fidelity while covering a broad thematic space.
  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores
grammar, creativity, moral clarity, and template adherence with (ii)
reference-free diversity and readability metrics. Among ten open-weight
candidates, an 8B-parameter Llama-3 variant delivers the best quality-speed
trade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM)
at approximately 13.5 cents per 1,000 fables.
  We release the dataset, generation code, evaluation scripts, and full
metadata under a permissive license, enabling exact reproducibility and cost
benchmarking. TF1-EN-3M opens avenues for research in instruction following,
narrative intelligence, value alignment, and child-friendly educational AI,
demonstrating that large-scale moral storytelling no longer requires
proprietary giant models.

</details>


### [22] [A Factorized Probabilistic Model of the Semantics of Vague Temporal Adverbials Relative to Different Event Types](https://arxiv.org/abs/2505.01311)
*Svenja Kenneweg,Jörg Deigmöller,Julian Eggert,Philipp Cimiano*

Main category: cs.CL

TL;DR: 这篇论文介绍了一种因子化模型，用于捕捉模糊时间副词（如‘最近’、‘刚刚’等）的语义，并将其与事件相关的概率分布结合，从而生成上下文相关的含义。与非因子化的单高斯分布模型相比，因子化模型在预测能力上表现相似，但更简单且扩展性更强。


<details>
  <summary>Details</summary>
Motivation: 模糊时间副词在描述事件时往往缺乏精确的时间定义，且其含义受上下文影响。本文旨在通过建立概率模型，更准确地捕捉这些副词的语义，并适应不同事件的上下文。

Method: 研究者提出了一种因子化模型，将模糊时间副词的语义建模为概率分布，并将其与事件相关的分布相结合。模型参数通过已有的母语者判断数据进行拟合。

Result: 实验表明，因子化模型与基于单高斯分布的非因子化模型在预测准确性上相近，但因子化模型更简单且扩展性更好，符合奥卡姆剃刀原则。

Conclusion: 因子化模型是一种更优的方法，能够有效捕捉模糊时间副词的语义，同时具备更好的简洁性和扩展潜力。

Abstract: Vague temporal adverbials, such as recently, just, and a long time ago,
describe the temporal distance between a past event and the utterance time but
leave the exact duration underspecified. In this paper, we introduce a
factorized model that captures the semantics of these adverbials as
probabilistic distributions. These distributions are composed with
event-specific distributions to yield a contextualized meaning for an adverbial
applied to a specific event. We fit the model's parameters using existing data
capturing judgments of native speakers regarding the applicability of these
vague temporal adverbials to events that took place a given time ago. Comparing
our approach to a non-factorized model based on a single Gaussian distribution
for each pair of event and temporal adverbial, we find that while both models
have similar predictive power, our model is preferable in terms of Occam's
razor, as it is simpler and has better extendability.

</details>


### [23] [A Transformer-based Neural Architecture Search Method](https://arxiv.org/abs/2505.01314)
*Shang Wang,Huanrong Tang,Jianquan Ouyang*

Main category: cs.CL

TL;DR: 提出一种基于Transformer架构的神经架构搜索方法，通过多目标遗传算法优化网络结构，实验证明其优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 为了找到翻译效果更好的神经网络结构，研究者不仅使用BLEU分数作为评价指标，还引入困惑度作为辅助指标。

Method: 采用基于Transformer架构的神经架构搜索方法，结合多目标遗传算法优化编码器和解码器的多头注意力计算方式。

Result: 实验结果显示，该算法搜索到的网络结构优于所有基线模型，且引入辅助评价指标比仅用BLEU分数能发现更好的模型。

Conclusion: 该研究通过多目标优化和辅助评价指标，成功提升了翻译任务的神经网络性能。

Abstract: This paper presents a neural architecture search method based on Transformer
architecture, searching cross multihead attention computation ways for
different number of encoder and decoder combinations. In order to search for
neural network structures with better translation results, we considered
perplexity as an auxiliary evaluation metric for the algorithm in addition to
BLEU scores and iteratively improved each individual neural network within the
population by a multi-objective genetic algorithm. Experimental results show
that the neural network structures searched by the algorithm outperform all the
baseline models, and that the introduction of the auxiliary evaluation metric
can find better models than considering only the BLEU score as an evaluation
metric.

</details>


### [24] [Helping Big Language Models Protect Themselves: An Enhanced Filtering and Summarization System](https://arxiv.org/abs/2505.01315)
*Sheikh Samit Muhaimin,Spyridon Mastorakis*

Main category: cs.CL

TL;DR: 该研究提出了一种无需重新训练LLM的防御框架，通过提示过滤和对抗研究文献摘要增强LLM对恶意输入的识别与防御能力，实验显示其识别成功率高达98.71%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）易受对抗性攻击和恶意输入的影响，现有防御方法需要重新训练模型，成本高昂且不切实际。

Method: 框架包含两部分：1) 基于NLP技术的提示过滤模块，用于检测和分类有害输入；2) 对抗研究文献摘要模块，为LLM提供上下文感知防御知识。

Result: 实验结果显示，该方法成功识别恶意模式的准确率为98.71%，并显著提高了LLM的抗对抗滥用能力。

Conclusion: 该框架是一种高效且无需重新训练的防御替代方案，能显著提升LLM的安全性，同时保持响应质量。

Abstract: The recent growth in the use of Large Language Models has made them
vulnerable to sophisticated adversarial assaults, manipulative prompts, and
encoded malicious inputs. Existing countermeasures frequently necessitate
retraining models, which is computationally costly and impracticable for
deployment. Without the need for retraining or fine-tuning, this study presents
a unique defense paradigm that allows LLMs to recognize, filter, and defend
against adversarial or malicious inputs on their own. There are two main parts
to the suggested framework: (1) A prompt filtering module that uses
sophisticated Natural Language Processing (NLP) techniques, including zero-shot
classification, keyword analysis, and encoded content detection (e.g. base64,
hexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and
(2) A summarization module that processes and summarizes adversarial research
literature to give the LLM context-aware defense knowledge. This approach
strengthens LLMs' resistance to adversarial exploitation by fusing text
extraction, summarization, and harmful prompt analysis. According to
experimental results, this integrated technique has a 98.71% success rate in
identifying harmful patterns, manipulative language structures, and encoded
prompts. By employing a modest amount of adversarial research literature as
context, the methodology also allows the model to react correctly to harmful
inputs with a larger percentage of jailbreak resistance and refusal rate. While
maintaining the quality of LLM responses, the framework dramatically increases
LLM's resistance to hostile misuse, demonstrating its efficacy as a quick and
easy substitute for time-consuming, retraining-based defenses.

</details>


### [25] [TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague, Implicit and Explicit References](https://arxiv.org/abs/2505.01325)
*Svenja Kenneweg,Jörg Deigmöller,Philipp Cimiano,Julian Eggert*

Main category: cs.CL

TL;DR: 介绍了一个名为TRAVELER的合成基准数据集，用于评估模型在解决显式、隐式和模糊时间引用方面的能力，发现现有大模型在小事件集和显式引用上表现较好，但在大事件集和模糊引用上表现较差。


<details>
  <summary>Details</summary>
Motivation: 现有基准在系统性评估特定时间引用方面存在不足，因此需要一个新的数据集来填补这一空白。

Method: 构建了TRAVELER数据集，采用问答范式，包含3300个涉及时间引用的问题，并通过人类调查确定模糊引用的真实答案。

Result: 评估了四种最先进的大模型，发现它们在处理小事件集和显式引用时表现良好，但在大事件集和模糊引用时性能显著下降。

Conclusion: TRAVELER数据集有效评估了模型在时间引用上的能力，揭示了现有模型在复杂和模糊时间引用上的局限性。

Abstract: Understanding and resolving temporal references is essential in Natural
Language Understanding as we often refer to the past or future in daily
communication. Although existing benchmarks address a system's ability to
reason about and resolve temporal references, systematic evaluation of specific
temporal references remains limited. Towards closing this gap, we introduce
TRAVELER, a novel synthetic benchmark dataset that follows a Question Answering
paradigm and consists of questions involving temporal references with the
corresponding correct answers. TRAVELER assesses models' abilities to resolve
explicit, implicit relative to speech time, and vague temporal references.
Beyond investigating the performance of state-of-the-art LLMs depending on the
type of temporal reference, our benchmark also allows evaluation of performance
in relation to the length of the set of events. For the category of vague
temporal references, ground-truth answers were established via human surveys on
Prolific, following a procedure similar to the one from Kenneweg et al. To
demonstrate the benchmark's applicability, we evaluate four state-of-the-art
LLMs using a question-answering task encompassing 3,300 questions. Our findings
show that while the benchmarked LLMs can answer questions over event sets with
a handful of events and explicit temporal references successfully, performance
clearly deteriorates with larger event set length and when temporal references
get less explicit. Notably, the vague question category exhibits the lowest
performance across all models.
  The benchmark is publicly available at:
https://gitlab.ub.uni-bielefeld.de/s.kenneweg/TRAVELER

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [26] [Constructing an Optimal Behavior Basis for the Option Keyboard](https://arxiv.org/abs/2505.00787)
*Lucas N. Alegre,Ana L. C. Bazzan,André Barreto,Bruno C. da Silva*

Main category: cs.LG

TL;DR: 该论文提出了一种高效构建最优行为基的方法，显著减少了确保新任务最优性所需的基策略数量，并在复杂任务中优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决如何构建最优行为基以在零样本下识别线性任务的全局最优解，并提升对非线性任务的表达能力。

Method: 引入一种新方法高效构建最优行为基，并证明其比凸覆盖集（CCS）更具表达力。

Result: 新方法在复杂任务中表现优异，且随着任务复杂度提升，优势更加明显。

Conclusion: 该方法为多任务强化学习提供了一种高效且表达力更强的解决方案。

Abstract: Multi-task reinforcement learning aims to quickly identify solutions for new
tasks with minimal or no additional interaction with the environment.
Generalized Policy Improvement (GPI) addresses this by combining a set of base
policies to produce a new one that is at least as good -- though not
necessarily optimal -- as any individual base policy. Optimality can be
ensured, particularly in the linear-reward case, via techniques that compute a
Convex Coverage Set (CCS). However, these are computationally expensive and do
not scale to complex domains. The Option Keyboard (OK) improves upon GPI by
producing policies that are at least as good -- and often better. It achieves
this through a learned meta-policy that dynamically combines base policies.
However, its performance critically depends on the choice of base policies.
This raises a key question: is there an optimal set of base policies -- an
optimal behavior basis -- that enables zero-shot identification of optimal
solutions for any linear tasks? We solve this open problem by introducing a
novel method that efficiently constructs such an optimal behavior basis. We
show that it significantly reduces the number of base policies needed to ensure
optimality in new tasks. We also prove that it is strictly more expressive than
a CCS, enabling particular classes of non-linear tasks to be solved optimally.
We empirically evaluate our technique in challenging domains and show that it
outperforms state-of-the-art approaches, increasingly so as task complexity
increases.

</details>


### [27] [Improving Routing in Sparse Mixture of Experts with Graph of Tokens](https://arxiv.org/abs/2505.00792)
*Tam Nguyen,Ngoc N. Tran,Khai Nguyen,Richard G. Baraniuk*

Main category: cs.LG

TL;DR: 论文揭示了SMoE模型在训练后期存在路由波动问题，提出基于概率图模型（PGM）的相似性和注意力感知路由策略，显著提升了模型的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 稀疏专家混合（SMoE）虽然通过激活少量参数实现了参数量的指数级增长，但其路由机制在训练后期波动性高，导致模型不稳定。论文从PGM角度出发，旨在解决这一问题。

Method: 1. 提出相似性感知（S）MoE，考虑token间的交互；2. 结合注意力机制，设计注意力感知（S）MoE，利用注意力矩阵引导路由；3. 从理论上证明新方法能降低专家选择的熵，提升稳定性。

Result: 实验表明，新方法显著减少了路由波动，提高了准确性和模型鲁棒性，优于传统的基于softmax门控的MoE-Transformer。

Conclusion: 通过引入token相似性和注意力机制的路由策略，论文解决了SMoE的路由波动问题，为模型的可扩展性和鲁棒性提供了新思路。

Abstract: Sparse Mixture of Experts (SMoE) has emerged as a key to achieving
unprecedented scalability in deep learning. By activating only a small subset
of parameters per sample, SMoE achieves an exponential increase in parameter
counts while maintaining a constant computational overhead. However, SMoE
models are susceptible to routing fluctuations--changes in the routing of a
given input to its target expert--at the late stage of model training, leading
to model non-robustness. In this work, we unveil the limitation of SMoE through
the perspective of the probabilistic graphical model (PGM). Through this PGM
framework, we highlight the independence in the expert-selection of tokens,
which exposes the model to routing fluctuation and non-robustness. Alleviating
this independence, we propose the novel Similarity-Aware (S)MoE, which
considers interactions between tokens during expert selection. We then derive a
new PGM underlying an (S)MoE-Attention block, going beyond just a single (S)MoE
layer. Leveraging the token similarities captured by the attention matrix, we
propose the innovative Attention-Aware (S)MoE, which employs the attention
matrix to guide the routing of tokens to appropriate experts in (S)MoE. We
theoretically prove that Similarity/Attention-Aware routing help reduce the
entropy of expert selection, resulting in more stable token routing mechanisms.
We empirically validate our models on various tasks and domains, showing
significant improvements in reducing routing fluctuations, enhancing accuracy,
and increasing model robustness over the baseline MoE-Transformer with token
routing via softmax gating.

</details>


### [28] [Scalable Meta-Learning via Mixed-Mode Differentiation](https://arxiv.org/abs/2505.00793)
*Iurii Kemaev,Dan A Calian,Luisa M Zintgraf,Gregory Farquhar,Hado van Hasselt*

Main category: cs.LG

TL;DR: MixFlow-MG 是一种有效算法，采用混合模式自动微分，优化了现有梯度计算，内存效率提升10倍以上，计算时间缩短25%。


<details>
  <summary>Details</summary>
Motivation: 现有方法中的二阶和混合导数计算成本高，自动微分库无法完全优化这类问题，导致性能不佳。

Method: 提出混合模式微分方法MixFlow-MG，改进计算图效率。

Result: 实验显示内存消耗降低10倍，计算时间减少25%。

Conclusion: MixFlow-MG为元学习等任务提供了高效且可扩展的新解决方案。

Abstract: Gradient-based bilevel optimisation is a powerful technique with applications
in hyperparameter optimisation, task adaptation, algorithm discovery,
meta-learning more broadly, and beyond. It often requires differentiating
through the gradient-based optimisation process itself, leading to
"gradient-of-a-gradient" calculations with computationally expensive
second-order and mixed derivatives. While modern automatic differentiation
libraries provide a convenient way to write programs for calculating these
derivatives, they oftentimes cannot fully exploit the specific structure of
these problems out-of-the-box, leading to suboptimal performance. In this
paper, we analyse such cases and propose Mixed-Flow Meta-Gradients, or
MixFlow-MG -- a practical algorithm that uses mixed-mode differentiation to
construct more efficient and scalable computational graphs yielding over 10x
memory and up to 25% wall-clock time improvements over standard implementations
in modern meta-learning setups.

</details>


### [29] [A Mathematical Philosophy of Explanations in Mechanistic Interpretability -- The Strange Science Part I.i](https://arxiv.org/abs/2505.00808)
*Kola Ayonrinde,Louis Jaburi*

Main category: cs.LG

TL;DR: 该论文提出了'解释性视角假说'，认为机制可解释性研究是一种理解神经网络的系统性方法，通过提取隐含的解释来评估模型，并明确定义了'解释忠实性'。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于通过机制可解释性（MI）来理解神经网络，因为它能提供因果解释，并与其他可解释性范式区分开来。

Method: 论文提出了机制可解释性的定义，强调其层次性、本体性、因果机制性和可证伪性，并提出了'解释性乐观原则'作为MI成功的必要前提。

Result: 研究明确了'解释忠实性'的定义，并展示了如何通过MI提取和理解神经网络的隐含解释。

Conclusion: 结论认为机制可解释性是一种系统性且有限的方法，能有效帮助理解神经网络，但需遵循其基本原则。

Abstract: Mechanistic Interpretability aims to understand neural networks through
causal explanations. We argue for the Explanatory View Hypothesis: that
Mechanistic Interpretability research is a principled approach to understanding
models because neural networks contain implicit explanations which can be
extracted and understood. We hence show that Explanatory Faithfulness, an
assessment of how well an explanation fits a model, is well-defined. We propose
a definition of Mechanistic Interpretability (MI) as the practice of producing
Model-level, Ontic, Causal-Mechanistic, and Falsifiable explanations of neural
networks, allowing us to distinguish MI from other interpretability paradigms
and detail MI's inherent limits. We formulate the Principle of Explanatory
Optimism, a conjecture which we argue is a necessary precondition for the
success of Mechanistic Interpretability.

</details>


### [30] [Scalable Unit Harmonization in Medical Informatics Using Bi-directional Transformers and Bayesian-Optimized BM25 and Sentence Embedding Retrieval](https://arxiv.org/abs/2505.00810)
*Jordi de la Torre*

Main category: cs.LG

TL;DR: 该研究开发了一种结合BM25、句子嵌入和双向transformer分类器的单位统一系统，在临床数据中显著提升了单位统一的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决大规模临床数据中单位不一致的问题，提高数据互操作性，减少手动工作量。

Method: 多阶段流程：过滤、识别、统一建议生成、自动重排和手动验证，结合BM25、句子嵌入和双向transformer分类器。

Result: 混合检索方法表现最佳（MRR: 0.8833），经过transformer重排后MRR提升至0.9833，在Rank1时精度为83.39%，Rank5时召回率为94.66%。

Conclusion: 该框架为临床数据单位统一提供高效、可扩展的解决方案，提升数据一致性，支持跨医疗系统的可靠多机构研究和荟萃分析。

Abstract: Objective: To develop and evaluate a scalable methodology for harmonizing
inconsistent units in large-scale clinical datasets, addressing a key barrier
to data interoperability.
  Materials and Methods: We designed a novel unit harmonization system
combining BM25, sentence embeddings, Bayesian optimization, and a bidirectional
transformer based binary classifier for retrieving and matching laboratory test
entries. The system was evaluated using the Optum Clinformatics Datamart
dataset (7.5 billion entries). We implemented a multi-stage pipeline:
filtering, identification, harmonization proposal generation, automated
re-ranking, and manual validation. Performance was assessed using Mean
Reciprocal Rank (MRR) and other standard information retrieval metrics.
  Results: Our hybrid retrieval approach combining BM25 and sentence embeddings
(MRR: 0.8833) significantly outperformed both lexical-only (MRR: 0.7985) and
embedding-only (MRR: 0.5277) approaches. The transformer-based reranker further
improved performance (absolute MRR improvement: 0.10), bringing the final
system MRR to 0.9833. The system achieved 83.39\% precision at rank 1 and
94.66\% recall at rank 5.
  Discussion: The hybrid architecture effectively leverages the complementary
strengths of lexical and semantic approaches. The reranker addresses cases
where initial retrieval components make errors due to complex semantic
relationships in medical terminology.
  Conclusion: Our framework provides an efficient, scalable solution for unit
harmonization in clinical datasets, reducing manual effort while improving
accuracy. Once harmonized, data can be reused seamlessly in different analyses,
ensuring consistency across healthcare systems and enabling more reliable
multi-institutional studies and meta-analyses.

</details>


### [31] [Handling Label Noise via Instance-Level Difficulty Modeling and Dynamic Optimization](https://arxiv.org/abs/2505.00812)
*Kuan Zhang,Chengliang Chai,Jingzhe Xu,Chi Zhang,Ye Yuan,Guoren Wang,Lei Cao*

Main category: cs.LG

TL;DR: 论文提出了一种新颖的两阶段噪声学习框架，通过动态加权损失函数实现实例级优化，无需超参数调优。通过引入‘错误事件’指标动态建模样本的清洁度和难度，显著降低了计算成本并提升了模型性能。在五个合成和真实世界基准测试中，该方法优于现有技术，减少了75%的计算时间并提升了可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理噪声监督时面临计算成本高、超参数调优复杂和粗粒度优化等问题。论文旨在通过实例级的动态优化和简化的噪声建模来解决这些挑战。

Method: 提出两阶段框架：1. 收集‘错误事件’信息并构建基础模型；2. 使用概率模型处理样本的错误事件信息进行噪声鲁棒训练，通过动态加权损失函数实现优化。

Result: 在五个基准测试中，性能超越现有技术，计算时间减少75%，模型可扩展性提升。

Conclusion: 通过动态加权损失和‘错误事件’指标，该方法高效解决了噪声监督下的模型性能退化问题，具有计算高效和易扩展的优势。

Abstract: Recent studies indicate that deep neural networks degrade in generalization
performance under noisy supervision. Existing methods focus on isolating clean
subsets or correcting noisy labels, facing limitations such as high
computational costs, heavy hyperparameter tuning process, and coarse-grained
optimization. To address these challenges, we propose a novel two-stage noisy
learning framework that enables instance-level optimization through a
dynamically weighted loss function, avoiding hyperparameter tuning. To obtain
stable and accurate information about noise modeling, we introduce a simple yet
effective metric, termed wrong event, which dynamically models the cleanliness
and difficulty of individual samples while maintaining computational costs. Our
framework first collects wrong event information and builds a strong base
model. Then we perform noise-robust training on the base model, using a
probabilistic model to handle the wrong event information of samples.
Experiments on five synthetic and real-world LNL benchmarks demonstrate our
method surpasses state-of-the-art methods in performance, achieves a nearly 75%
reduction in computational time and improves model scalability.

</details>


### [32] [Dual Filter: A Mathematical Framework for Inference using Transformer-like Architectures](https://arxiv.org/abs/2505.00818)
*Heng-Sheng Chang,Prashant G. Mehta*

Main category: cs.LG

TL;DR: 这篇论文提出了一个数学模型，用于求解隐藏马尔可夫模型（HMM）下的因果非线性预测问题，受解码器-仅Transformer架构启发，并基于最优控制理论设计了一种类似Transformer的算法——对偶滤波器。


<details>
  <summary>Details</summary>
Motivation: 研究动机是从基本原理出发，推导出解决Transformer设计目的的预测问题的类似Transformer架构，而非直接建模Transformer本身。

Method: 论文采用最优控制方法，将预测问题（MMSE）重新表述为最优控制问题，并通过固定点方程引入对偶滤波器算法。

Result: 通过对偶滤波器的数值实验，展示了其在研究规模Transformer模型参数下的性能。

Conclusion: 结论是通过最优控制理论成功设计了一种类似Transformer的架构，为理解Transformer的数学本质提供了新视角。

Abstract: This paper presents a mathematical framework for causal nonlinear prediction
in settings where observations are generated from an underlying hidden Markov
model (HMM). Both the problem formulation and the proposed solution are
motivated by the decoder-only transformer architecture, in which a finite
sequence of observations (tokens) is mapped to the conditional probability of
the next token. Our objective is not to construct a mathematical model of a
transformer. Rather, our interest lies in deriving, from first principles,
transformer-like architectures that solve the prediction problem for which the
transformer is designed. The proposed framework is based on an original optimal
control approach, where the prediction objective (MMSE) is reformulated as an
optimal control problem. An analysis of the optimal control problem is
presented leading to a fixed-point equation on the space of probability
measures. To solve the fixed-point equation, we introduce the dual filter, an
iterative algorithm that closely parallels the architecture of decoder-only
transformers. These parallels are discussed in detail along with the
relationship to prior work on mathematical modeling of transformers as
transport on the space of probability measures. Numerical experiments are
provided to illustrate the performance of the algorithm using parameter values
used in researchscale transformer models.

</details>


### [33] [Data-Driven Optical To Thermal Inference in Pool Boiling Using Generative Adversarial Networks](https://arxiv.org/abs/2505.00823)
*Qianxi Fu,Youngjoon Suh,Xiaojing Zhang,Yoonjin Won*

Main category: cs.LG

TL;DR: 论文提出了一种基于条件生成对抗网络（CGAN）的数据驱动框架，用于从几何相轮廓推断温度场，以解决多相热传递测量难题。模型在池沸腾配置中实现了误差低于6%的温度场重建，并通过数据增强提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 多相热传递的定量表征受限于混沌、快速变化的流动体系中温度场测量的困难。尽管计算方法在理想情况下提供了时空分辨，但在复杂实验条件下的模拟仍然困难。

Method: 采用条件生成对抗网络（CGAN）结合高速成像数据与仿真信息训练，从几何相轮廓推断温度场，并利用数据增强策略提升预测准确性。

Result: 模型在池沸腾配置中实现了误差低于6%的温度场重建，数据增强策略在仿真和实验数据集上均有效提升了预测的准确性和物理合理性。

Conclusion: 深度生成模型能够连接可观测的多相现象与潜在的热传递机制，为复杂两相系统中实验测量的增强和解释提供了强有力的方法。

Abstract: Phase change plays a critical role in thermal management systems, yet
quantitative characterization of multiphase heat transfer remains limited by
the challenges of measuring temperature fields in chaotic, rapidly evolving
flow regimes. While computational methods offer spatiotemporal resolution in
idealized cases, replicating complex experimental conditions remains
prohibitively difficult. Here, we present a data-driven framework that
leverages a conditional generative adversarial network (CGAN) to infer
temperature fields from geometric phase contours in a canonical pool boiling
configuration where advanced data collection techniques are restricted. Using
high-speed imaging data and simulation-informed training, our model
demonstrates the ability to reconstruct temperature fields with errors below
6%. We further show that standard data augmentation strategies are effective in
enhancing both accuracy and physical plausibility of the predicted maps across
both simulation and experimental datasets when precise physical constraints are
not applicable. Our results highlight the potential of deep generative models
to bridge the gap between observable multiphase phenomena and underlying
thermal transport, offering a powerful approach to augment and interpret
experimental measurements in complex two-phase systems.

</details>


### [34] [Intersectional Divergence: Measuring Fairness in Regression](https://arxiv.org/abs/2505.00830)
*Joe Germino,Nuno Moniz,Nitesh V. Chawla*

Main category: cs.LG

TL;DR: 该论文提出了一个名为交叉性差异（ID）的新方法，用于衡量回归任务中的公平性，同时考虑了多个保护属性的组合，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注分类任务中的公平性，回归任务中存在空白；且现有方法仅关注单一保护属性，忽略了多属性的交叉影响。

Method: 提出了交叉性差异（ID）作为回归任务的公平性度量，并进一步将其转化为损失函数（IDLoss），用于优化问题。

Result: 实验表明，ID能够揭示模型行为的公平性，结合IDLoss优化能显著提升单一属性和交叉性公平性，同时保持预测性能。

Conclusion: ID是首个针对回归任务的交叉性公平性度量，填补了领域空白，并展示了在优化中的实用性。

Abstract: Research on fairness in machine learning has been mainly framed in the
context of classification tasks, leaving critical gaps in regression. In this
paper, we propose a seminal approach to measure intersectional fairness in
regression tasks, going beyond the focus on single protected attributes from
existing work to consider combinations of all protected attributes.
Furthermore, we contend that it is insufficient to measure the average error of
groups without regard for imbalanced domain preferences. To this end, we
propose Intersectional Divergence (ID) as the first fairness measure for
regression tasks that 1) describes fair model behavior across multiple
protected attributes and 2) differentiates the impact of predictions in target
ranges most relevant to users. We extend our proposal demonstrating how ID can
be adapted into a loss function, IDLoss, and used in optimization problems.
Through an extensive experimental evaluation, we demonstrate how ID allows
unique insights into model behavior and fairness, and how incorporating IDLoss
into optimization can considerably improve single-attribute and intersectional
model fairness while maintaining a competitive balance in predictive
performance.

</details>


### [35] [IberFire -- a detailed creation of a spatio-temporal dataset for wildfire risk assessment in Spain](https://arxiv.org/abs/2505.00837)
*Julen Ercibengoa,Meritxell Gómez-Omella,Izaro Goienetxea*

Main category: cs.LG

TL;DR: 该论文介绍了IberFire，一个高分辨率的西班牙野火时空数据集，涵盖2007年至2024年，包含260个特征，支持机器学习和深度学习建模。


<details>
  <summary>Details</summary>
Motivation: 解决西班牙地区缺乏本地化、细粒度野火数据的问题，以提升野火预测模型的准确性。

Method: 基于开源数据和工具，构建了1公里×1公里×1天分辨率的时空数据立方体，整合了8类260个特征。

Result: IberFire数据集在时空粒度和特征多样性上优于现有欧洲数据集，并提供可复现的方法论。

Conclusion: IberFire为野火风险建模、气候模式分析和土地管理提供了重要支持，并公开数据集以促进开放研究。

Abstract: Wildfires pose a critical environmental issue to ecosystems, economies, and
public safety, particularly in Mediterranean regions such as Spain. Accurate
predictive models rely on high-resolution spatio-temporal data to capture the
complex interplay of environmental and anthropogenic factors. To address the
lack of localised and fine-grained datasets in Spain, this work introduces
IberFire, a spatio-temporal datacube at 1 km x 1 km x 1-day resolution covering
mainland Spain and the Balearic Islands from December 2007 to December 2024.
IberFire integrates 260 features across eight main categories: auxiliary
features, fire history, geography, topography, meteorology, vegetation indices,
human activity, and land cover. All features are derived from open-access
sources, ensuring transparency and real-time applicability. The data processing
pipeline was implemented entirely using open-source tools, and the codebase has
been made publicly available. This work not only enhances spatio-temporal
granularity and feature diversity compared to existing European datacubes but
also provides a reproducible methodology for constructing similar datasets.
IberFire supports advanced wildfire risk modelling through Machine Learning
(ML) and Deep Learning (DL) techniques, enables climate pattern analysis and
informs strategic planning in fire prevention and land management. The dataset
is publicly available on Zenodo to promote open research and collaboration.

</details>


### [36] [ICQuant: Index Coding enables Low-bit LLM Quantization](https://arxiv.org/abs/2505.00850)
*Xinlin Li,Osama Hanna,Christina Fragouli,Suhas Diggavi*

Main category: cs.LG

TL;DR: 摘要


<details>
  <summary>Details</summary>
Motivation: 动机

Method: 方法

Result: 结果

Conclusion: 结论

Abstract: The rapid deployment of Large Language Models (LLMs) highlights the need for
efficient low-bit post-training quantization (PTQ), due to their high memory
costs. A key challenge in weight quantization is the presence of outliers,
which inflate quantization ranges and lead to large errors. While a number of
outlier suppression techniques have been proposed, they either: fail to
effectively shrink the quantization range, or incur (relatively) high bit
overhead. In this paper, we present ICQuant, a novel framework that leverages
outlier statistics to design an efficient index coding scheme for outlier-aware
weight-only quantization. Compared to existing outlier suppression techniques
requiring $\approx 1$ bit overhead to halve the quantization range, ICQuant
requires only $\approx 0.3$ bits; a significant saving in extreme compression
regimes (e.g., 2-3 bits per weight). ICQuant can be used on top of any existing
quantizers to eliminate outliers, improving the quantization quality. Using
just 2.3 bits per weight and simple scalar quantizers, ICQuant improves the
zero-shot accuracy of the 2-bit Llama3-70B model by up to 130% and 150%
relative to QTIP and QuIP#; and it achieves comparable performance to the
best-known fine-tuned quantizer (PV-tuning) without fine-tuning.

</details>


### [37] [Rethinking Time Encoding via Learnable Transformation Functions](https://arxiv.org/abs/2505.00887)
*Xi Chen,Yateng Tang,Jiarong Xu,Jiawei Zhang,Siwei Zhang,Sijia Peng,Xuehao Zheng,Yun Xiong*

Main category: cs.LG

TL;DR: 论文提出了一种可学习的广义时间编码方法LeTE，通过深度函数学习参数化非线性变换，以捕捉多样化、复杂的时间模式，解决了现有方法在真实场景中时间模式多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现实场景中的时间模式多样且复杂，现有时间编码方法通常局限于单一模式（如周期性），难以有效处理这种多样性，因此需要一种更通用的方法。

Method: 提出Learnable Transformation-based Generalized Time Encoding (LeTE)，利用深度函数学习技术参数化非线性变换，使时间编码可学习且能建模广义时间模式。

Result: 通过多领域实验验证了LeTE的通用性和有效性，证明其能涵盖现有方法并适应多样任务。

Conclusion: LeTE通过可学习变换实现了对复杂时间模式的建模，为时间编码提供了更灵活的解决方案，适用于广泛的应用场景。

Abstract: Effectively modeling time information and incorporating it into applications
or models involving chronologically occurring events is crucial. Real-world
scenarios often involve diverse and complex time patterns, which pose
significant challenges for time encoding methods. While previous methods focus
on capturing time patterns, many rely on specific inductive biases, such as
using trigonometric functions to model periodicity. This narrow focus on
single-pattern modeling makes them less effective in handling the diversity and
complexities of real-world time patterns. In this paper, we investigate to
improve the existing commonly used time encoding methods and introduce
Learnable Transformation-based Generalized Time Encoding (LeTE). We propose
using deep function learning techniques to parameterize non-linear
transformations in time encoding, making them learnable and capable of modeling
generalized time patterns, including diverse and complex temporal dynamics. By
enabling learnable transformations, LeTE encompasses previous methods as
specific cases and allows seamless integration into a wide range of tasks.
Through extensive experiments across diverse domains, we demonstrate the
versatility and effectiveness of LeTE.

</details>


### [38] [NeMo-Inspector: A Visualization Tool for LLM Generation Analysis](https://arxiv.org/abs/2505.00903)
*Daria Gitman,Igor Gitman,Evelina Bakhturina*

Main category: cs.LG

TL;DR: 论文介绍了一个名为NeMo-Inspector的开源工具，用于简化合成数据集的分析和质量提升，展示了其在实际案例中的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于合成数据在大规模高质量训练数据稀缺时的重要性，但手工检查数据质量耗时且复杂，因此开发了自动化工具以提升效率和质量。

Method: 研究团队开发了NeMo-Inspector工具，内置推理能力用于分析合成数据集，并通过两个实际案例验证其效果。

Result: 工具显著降低了GSM-Plus数据集中的低质量样本比例（从46.99%降至19.51%），并提升了OpenMath模型的准确性（在MATH数据集上提升1.92%，GSM8K数据集上提升4.17%）。

Conclusion: NeMo-Inspector是一个高效的开源工具，能够显著提升合成数据集的质量和模型的性能，展示了广泛应用的潜力。

Abstract: Adapting Large Language Models (LLMs) to novel tasks and enhancing their
overall capabilities often requires large, high-quality training datasets.
Synthetic data, generated at scale, serves a valuable alternative when
real-world data is scarce or difficult to obtain. However, ensuring the quality
of synthetic datasets is challenging, as developers must manually inspect and
refine numerous samples to identify errors and areas for improvement. This
process is time-consuming and requires specialized tools. We introduce
NeMo-Inspector, an open-source tool designed to simplify the analysis of
synthetic datasets with integrated inference capabilities. We demonstrate its
effectiveness through two real-world cases. Analysis and cleaning of the
synthetically generated GSM-Plus dataset with NeMo-Inspector led to a
significant decrease in low-quality samples from 46.99% to 19.51%. The tool
also helped identify and correct generation errors in OpenMath models,
improving accuracy by 1.92% on the MATH dataset and by 4.17% on the GSM8K
dataset for a Meta-Llama-3-8B model fine-tuned on synthetic data generated from
Nemotron-4-340B.

</details>


### [39] [Learning Neural Control Barrier Functions from Offline Data with Conservatism](https://arxiv.org/abs/2505.00908)
*Ihab Tabbara,Hussein Sibai*

Main category: cs.LG

TL;DR: 本文提出了一种从离线数据集中训练控制屏障函数的算法，以解决现有方法的高维问题，并通过保守Q学习的启发方法，提高了安全性和对分布外状态的避免能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于控制屏障函数的安全过滤器在构造合成算法时容易受到维度灾难的影响，而深度学习方法被提出作为解决方案。本文旨在通过离线数据集训练控制屏障函数，进一步提高安全性及对不可靠状态的规避能力。

Method: 采用了一种受保守Q学习启发的算法，训练过滤器不仅防止系统进入不安全状态，还能避开分布外状态，生成保守控制屏障函数（CCBFs）。

Result: 实验结果表明，CCBFs在保持安全性和避免分布外状态方面优于现有方法，同时对任务性能的影响极小。

Conclusion: 本文提出的CCBFs算法有效解决了高维问题和分布外状态的安全挑战，为安全控制提供了更可靠的解决方案。

Abstract: Safety filters, particularly those based on control barrier functions, have
gained increased interest as effective tools for safe control of dynamical
systems. Existing correct-by-construction synthesis algorithms, however, suffer
from the curse of dimensionality. Deep learning approaches have been proposed
in recent years to address this challenge. In this paper, we contribute to this
line of work by proposing an algorithm for training control barrier functions
from offline datasets. Our algorithm trains the filter to not only prevent the
system from reaching unsafe states but also out-of-distribution ones, at which
the filter would be unreliable. It is inspired by Conservative Q-learning, an
offline reinforcement learning algorithm. We call its outputs Conservative
Control Barrier Functions (CCBFs). Our empirical results demonstrate that CCBFs
outperform existing methods in maintaining safety and out-of-distribution
avoidance while minimally affecting task performance.

</details>


### [40] [Gaussian Process Policy Iteration with Additive Schwarz Acceleration for Forward and Inverse HJB and Mean Field Game Problems](https://arxiv.org/abs/2505.00909)
*Xianjin Yang,Jingguo Zhang*

Main category: cs.LG

TL;DR: 提出了基于高斯过程的策略迭代框架，用于解决HJB方程和MFG的正反问题，结合Schwarz加速提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决HJB方程和MFG的正反问题，现有方法通常需要数值优化，效率较低。

Method: 采用高斯过程进行函数近似，策略迭代分步求解价值函数和更新策略，并引入Schwarz加速。

Result: 数值实验表明Schwarz加速显著提升了计算效率。

Conclusion: 提出的框架有效解决了HJB和MFG问题，且计算效率高。

Abstract: We propose a Gaussian Process (GP)-based policy iteration framework for
addressing both forward and inverse problems in Hamilton--Jacobi--Bellman (HJB)
equations and mean field games (MFGs). Policy iteration is formulated as an
alternating procedure between solving the value function under a fixed control
policy and updating the policy based on the resulting value function. By
exploiting the linear structure of GPs for function approximation, each policy
evaluation step admits an explicit closed-form solution, eliminating the need
for numerical optimization. To improve convergence, we incorporate the additive
Schwarz acceleration as a preconditioning step following each policy update.
Numerical experiments demonstrate the effectiveness of Schwarz acceleration in
improving computational efficiency.

</details>


### [41] [Fine-Tuning without Performance Degradation](https://arxiv.org/abs/2505.00913)
*Han Wang,Adam White,Martha White*

Main category: cs.LG

TL;DR: 本文介绍了Jump Start算法，用于改进离线学习的策略在线微调过程中的性能下降问题，通过逐步增加探索实现快速微调。


<details>
  <summary>Details</summary>
Motivation: 离线学习策略的在线微调常导致性能下降或学习缓慢，现有方法难以平衡探索与性能。

Method: 提出基于Jump Start的新算法，根据在线性能评估逐步增加探索。

Result: 实验表明，新算法显著减少了性能下降，并实现了更快的微调速度。

Conclusion: 该算法有效解决了离线到在线策略微调中性能下降的挑战。

Abstract: Fine-tuning policies learned offline remains a major challenge in application
domains. Monotonic performance improvement during \emph{fine-tuning} is often
challenging, as agents typically experience performance degradation at the
early fine-tuning stage. The community has identified multiple difficulties in
fine-tuning a learned network online, however, the majority of progress has
focused on improving learning efficiency during fine-tuning. In practice, this
comes at a serious cost during fine-tuning: initially, agent performance
degrades as the agent explores and effectively overrides the policy learned
offline. We show across a range of settings, many offline-to-online algorithms
exhibit either (1) performance degradation or (2) slow learning (sometimes
effectively no improvement) during fine-tuning. We introduce a new fine-tuning
algorithm, based on an algorithm called Jump Start, that gradually allows more
exploration based on online estimates of performance. Empirically, this
approach achieves fast fine-tuning and significantly reduces performance
degradations compared with existing algorithms designed to do the same.

</details>


### [42] [How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias](https://arxiv.org/abs/2505.00926)
*Ruiquan Huang,Yingbin Liang,Jing Yang*

Main category: cs.LG

TL;DR: 论文通过理论分析和实验验证探讨了一单层Transformer在解决'偶数对'和'奇偶校验'任务中的训练动态和机制。


<details>
  <summary>Details</summary>
Motivation: 研究目标是理解单层Transformer如何在梯度下降下学习解决正则语言识别任务，尤其是'偶数对'和'奇偶校验'任务，以揭示其工作机制。

Method: 通过理论分析单层Transformer（注意力层加线性层）的训练动态，并引入链式思考（CoT）来解决奇偶校验任务。

Result: 分析发现训练分为两阶段：注意力层快速分离数据，随后线性层逐渐逼近分类超平面，损失以O(1/t)速率下降。实验验证了理论结果。

Conclusion: 单层Transformer能直接解决'偶数对'任务，而'奇偶校验'需结合链式思考，训练过程中注意力层和线性层表现出分阶段的学习动态。

Abstract: Language recognition tasks are fundamental in natural language processing
(NLP) and have been widely used to benchmark the performance of large language
models (LLMs). These tasks also play a crucial role in explaining the working
mechanisms of transformers. In this work, we focus on two representative tasks
in the category of regular language recognition, known as `even pairs' and
`parity check', the aim of which is to determine whether the occurrences of
certain subsequences in a given sequence are even. Our goal is to explore how a
one-layer transformer, consisting of an attention layer followed by a linear
layer, learns to solve these tasks by theoretically analyzing its training
dynamics under gradient descent. While even pairs can be solved directly by a
one-layer transformer, parity check need to be solved by integrating
Chain-of-Thought (CoT), either into the inference stage of a transformer
well-trained for the even pairs task, or into the training of a one-layer
transformer. For both problems, our analysis shows that the joint training of
attention and linear layers exhibits two distinct phases. In the first phase,
the attention layer grows rapidly, mapping data sequences into separable
vectors. In the second phase, the attention layer becomes stable, while the
linear layer grows logarithmically and approaches in direction to a max-margin
hyperplane that correctly separates the attention layer outputs into positive
and negative samples, and the loss decreases at a rate of $O(1/t)$. Our
experiments validate those theoretical results.

</details>


### [43] [Compact Recurrent Transformer with Persistent Memory](https://arxiv.org/abs/2505.00929)
*Edison Mucllari,Zachary Daniels,David Zhang,Qiang Ye*

Main category: cs.LG

TL;DR: 为了解决Transformer在处理长序列时的高计算成本问题，作者提出了Compact Recurrent Transformer (CRT)，结合浅层Transformer和RNN来管理全局信息，显著降低了计算开销，同时保持了高性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在处理长序列时因自注意力机制的计算复杂度为平方级而效率低下，限制了其在资源受限场景（如边缘计算）的应用。

Method: 提出CRT模型，将短片段处理交给浅层Transformer，同时使用RNN压缩和管理全局信息，通过单一记忆向量实现长程依赖。

Result: 在WordPTB和WikiText-103的下一词预测任务中，CRT使用更短片段（一半或四分之一）和更少FLOPs达到或超越完整Transformer性能；在Toyota Smarthome视频分类中表现最佳。

Conclusion: CRT通过结合局部Transformer和全局RNN，高效解决了长序列处理问题，适用于资源受限场景，并在多个任务中验证了其优越性。

Abstract: The Transformer architecture has shown significant success in many language
processing and visual tasks. However, the method faces challenges in
efficiently scaling to long sequences because the self-attention computation is
quadratic with respect to the input length. To overcome this limitation,
several approaches scale to longer sequences by breaking long sequences into a
series of segments, restricting self-attention to local dependencies between
tokens within each segment and using a memory mechanism to manage information
flow between segments. However, these approached generally introduce additional
compute overhead that restricts them from being used for applications where
limited compute memory and power are of great concern (such as edge computing).
We propose a novel and efficient Compact Recurrent Transformer (CRT), which
combines shallow Transformer models that process short local segments with
recurrent neural networks to compress and manage a single persistent memory
vector that summarizes long-range global information between segments. We
evaluate CRT on WordPTB and WikiText-103 for next-token-prediction tasks, as
well as on the Toyota Smarthome video dataset for classification. CRT achieves
comparable or superior prediction results to full-length Transformers in the
language datasets while using significantly shorter segments (half or quarter
size) and substantially reduced FLOPs. Our approach also demonstrates
state-of-the-art performance on the Toyota Smarthome video dataset.

</details>


### [44] [Robust Root Cause Diagnosis using In-Distribution Interventions](https://arxiv.org/abs/2505.00930)
*Lokesh Nagalapatti,Ashutosh Srivastava,Sunita Sarawagi,Amit Sharma*

Main category: cs.LG

TL;DR: 提出了名为IDI的新算法，通过分布内干预预测复杂系统中的异常根因，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 在复杂互联系统中诊断异常根因是云服务和工业运营中的紧迫问题，现有方法因异常稀少且超出训练分布而效果不佳。

Method: IDI算法通过两个标准识别根因：异常性和修复性，依赖分布内干预估计而非不可靠的反事实推断。

Result: 理论和实验表明，IDI在SCM复杂度变化时表现优于反事实方法，并在合成和真实数据集上比9种基线更准确、鲁棒。

Conclusion: IDI通过分布内干预有效解决了传统反事实方法的局限性，显著提升了根因诊断的准确性和鲁棒性。

Abstract: Diagnosing the root cause of an anomaly in a complex interconnected system is
a pressing problem in today's cloud services and industrial operations. We
propose In-Distribution Interventions (IDI), a novel algorithm that predicts
root cause as nodes that meet two criteria: 1) **Anomaly:** root cause nodes
should take on anomalous values; 2) **Fix:** had the root cause nodes assumed
usual values, the target node would not have been anomalous. Prior methods of
assessing the fix condition rely on counterfactuals inferred from a Structural
Causal Model (SCM) trained on historical data. But since anomalies are rare and
fall outside the training distribution, the fitted SCMs yield unreliable
counterfactual estimates. IDI overcomes this by relying on interventional
estimates obtained by solely probing the fitted SCM at in-distribution inputs.
We present a theoretical analysis comparing and bounding the errors in
assessing the fix condition using interventional and counterfactual estimates.
We then conduct experiments by systematically varying the SCM's complexity to
demonstrate the cases where IDI's interventional approach outperforms the
counterfactual approach and vice versa. Experiments on both synthetic and
PetShop RCD benchmark datasets demonstrate that \our\ consistently identifies
true root causes more accurately and robustly than nine existing
state-of-the-art RCD baselines. Code is released at
https://github.com/nlokeshiisc/IDI_release.

</details>


### [45] [A Self-Supervised Transformer for Unusable Shared Bike Detection](https://arxiv.org/abs/2505.00932)
*Yin Huang,Yongqi Dong,Youhua Tang,Alvaro García Hernandez*

Main category: cs.LG

TL;DR: 该论文提出了一种名为SSTransformer的自监督框架，用于检测共享单车系统中的故障单车。通过利用GPS轨迹和行程记录的时空特征，结合自监督预训练策略，该模型在成都的真实数据集上表现优异，准确率达97.81%。


<details>
  <summary>Details</summary>
Motivation: 共享单车系统面临故障单车检测的难题，现有方法要么忽视动态时空模式，要么受限于标签稀缺和类别不平衡。本文旨在通过自监督学习解决这些问题。

Method: 提出SSTransformer框架，包括自监督预训练阶段（学习单车运动的通用表示）和微调阶段（适应二分类任务），利用GPS数据和行程记录提取时空特征。

Result: 在成都10,730辆单车的数据集上，SSTransformer在准确率（97.81%）、精确率（0.8889）和F1分数（0.9358）上均显著优于传统方法。

Conclusion: SSTransformer通过自监督学习有效捕捉共享单车的复杂异常，为共享交通的维护提供了可靠且可扩展的解决方案。

Abstract: The rapid expansion of bike-sharing systems (BSS) has greatly improved urban
"last-mile" connectivity, yet large-scale deployments face escalating
operational challenges, particularly in detecting faulty bikes. Existing
detection approaches either rely on static model-based thresholds that overlook
dynamic spatiotemporal (ST) usage patterns or employ supervised learning
methods that struggle with label scarcity and class imbalance. To address these
limitations, this paper proposes a novel Self-Supervised Transformer
(SSTransformer) framework for automatically detecting unusable shared bikes,
leveraging ST features extracted from GPS trajectories and trip records. The
model incorporates a self-supervised pre-training strategy to enhance its
feature extraction capabilities, followed by fine-tuning for efficient status
recognition. In the pre-training phase, the Transformer encoder learns
generalized representations of bike movement via a self-supervised objective;
in the fine-tuning phase, the encoder is adapted to a downstream binary
classification task. Comprehensive experiments on a real-world dataset of
10,730 bikes (1,870 unusable, 8,860 normal) from Chengdu, China, demonstrate
that SSTransformer significantly outperforms traditional machine learning,
ensemble learning, and deep learning baselines, achieving the best accuracy
(97.81%), precision (0.8889), and F1-score (0.9358). This work highlights the
effectiveness of self-supervised Transformer on ST data for capturing complex
anomalies in BSS, paving the way toward more reliable and scalable maintenance
solutions for shared mobility.

</details>


### [46] [TunnElQNN: A Hybrid Quantum-classical Neural Network for Efficient Learning](https://arxiv.org/abs/2505.00933)
*A. H. Abbas*

Main category: cs.LG

TL;DR: 论文提出了一种非顺序的混合量子-经典神经网络TunnElQNN，其中经典层使用受量子隧穿启发的TDAF激活函数，并在多类分类任务中表现优于使用ReLU的基线模型。


<details>
  <summary>Details</summary>
Motivation: 探索混合量子-经典神经网络的潜力，通过结合物理启发的激活函数和量子组件，提升模型的表达能力和鲁棒性。

Method: 提出TunnElQNN架构，交替使用经典层和量子层，经典层采用TDAF激活函数，并在合成数据集上进行多类分类任务测试。

Result: TunnElQNN在性能上优于使用ReLU的基线模型，并分析了不同类重叠程度下的决策边界。

Conclusion: 研究表明，结合物理启发的激活函数和量子组件可有效提升混合量子-经典架构的性能，为未来研究提供了新思路。

Abstract: Hybrid quantum-classical neural networks (HQCNNs) represent a promising
frontier in machine learning, leveraging the complementary strengths of both
models. In this work, we propose the development of TunnElQNN, a non-sequential
architecture composed of alternating classical and quantum layers. Within the
classical component, we employ the Tunnelling Diode Activation Function (TDAF),
inspired by the I-V characteristics of quantum tunnelling. We evaluate the
performance of this hybrid model on a synthetic dataset of interleaving
half-circle for multi-class classification tasks with varying degrees of class
overlap. The model is compared against a baseline hybrid architecture that uses
the conventional ReLU activation function (ReLUQNN). Our results show that the
TunnElQNN model consistently outperforms the ReLUQNN counterpart. Furthermore,
we analyse the decision boundaries generated by TunnElQNN under different
levels of class overlap and compare them to those produced by a neural network
implementing TDAF within a fully classical architecture. These findings
highlight the potential of integrating physics-inspired activation functions
with quantum components to enhance the expressiveness and robustness of hybrid
quantum-classical machine learning architectures.

</details>


### [47] [StablePCA: Learning Shared Representations across Multiple Sources via Minimax Optimization](https://arxiv.org/abs/2505.00940)
*Zhenyu Wang,Molei Liu,Jing Lei,Francis Bach,Zijian Guo*

Main category: cs.LG

TL;DR: 本文提出了StablePCA方法，旨在从高维多源数据中提取鲁棒的低维特征表示，解决传统PCA在多源环境中的非凸优化问题。


<details>
  <summary>Details</summary>
Motivation: 提取有效的低维特征表示以克服多源数据中的系统性偏差，并促进公平性和知识迁移。

Method: 采用Fantope松弛将问题转化为凸优化，并使用乐观梯度Mirror Prox算法求解。

Result: 理论和实验证明StablePCA在提取鲁棒低维特征方面具有高效性和高准确性。

Conclusion: StablePCA为多源高维数据提供了一种高效的鲁棒特征提取方法。

Abstract: When synthesizing multisource high-dimensional data, a key objective is to
extract low-dimensional feature representations that effectively approximate
the original features across different sources. Such general feature extraction
facilitates the discovery of transferable knowledge, mitigates systematic
biases such as batch effects, and promotes fairness. In this paper, we propose
Stable Principal Component Analysis (StablePCA), a novel method for group
distributionally robust learning of latent representations from
high-dimensional multi-source data. A primary challenge in generalizing PCA to
the multi-source regime lies in the nonconvexity of the fixed rank constraint,
rendering the minimax optimization nonconvex. To address this challenge, we
employ the Fantope relaxation, reformulating the problem as a convex minimax
optimization, with the objective defined as the maximum loss across sources. To
solve the relaxed formulation, we devise an optimistic-gradient Mirror Prox
algorithm with explicit closed-form updates. Theoretically, we establish the
global convergence of the Mirror Prox algorithm, with the convergence rate
provided from the optimization perspective. Furthermore, we offer practical
criteria to assess how closely the solution approximates the original nonconvex
formulation. Through extensive numerical experiments, we demonstrate
StablePCA's high accuracy and efficiency in extracting robust low-dimensional
representations across various finite-sample scenarios.

</details>


### [48] [FreCT: Frequency-augmented Convolutional Transformer for Robust Time Series Anomaly Detection](https://arxiv.org/abs/2505.00941)
*Wenxin Zhang,Ding Xu,Guangzhen Yao,Xiaojian Lin,Renxiang Guan,Chengze Du,Renda Han,Xi Xuan,Cuicui Luo*

Main category: cs.LG

TL;DR: 该论文提出的FreCT模型通过结合Transformer和卷积模块，并引入傅里叶变换的频率分析，改进了时间序列异常检测的性能，同时在时间和频率域优化一致性信息。


<details>
  <summary>Details</summary>
Motivation: 现有的重构方法在时间序列异常检测中难处理异常导致的偏差且忽略了频域信息，因此需要一种能同时捕捉时域长期依赖和频域特征的方法。

Method: FreCT采用改进的Transformer架构与卷积模块结合，利用傅里叶变换进行频域分析，并通过KL散度和绝对误差优化时间和频域的一致性。

Result: 在四个公开数据集上，FreCT表现出优于现有方法的异常检测性能。

Conclusion: FreCT通过融合时域和频域信息，显著提升了时间序列异常检测的效果，证明了频率增强对模型的贡献。

Abstract: Time series anomaly detection is critical for system monitoring and risk
identification, across various domains, such as finance and healthcare.
However, for most reconstruction-based approaches, detecting anomalies remains
a challenge due to the complexity of sequential patterns in time series data.
On the one hand, reconstruction-based techniques are susceptible to
computational deviation stemming from anomalies, which can lead to impure
representations of normal sequence patterns. On the other hand, they often
focus on the time-domain dependencies of time series, while ignoring the
alignment of frequency information beyond the time domain. To address these
challenges, we propose a novel Frequency-augmented Convolutional Transformer
(FreCT). FreCT utilizes patch operations to generate contrastive views and
employs an improved Transformer architecture integrated with a convolution
module to capture long-term dependencies while preserving local topology
information. The introduced frequency analysis based on Fourier transformation
could enhance the model's ability to capture crucial characteristics beyond the
time domain. To protect the training quality from anomalies and improve the
robustness, FreCT deploys stop-gradient Kullback-Leibler (KL) divergence and
absolute error to optimize consistency information in both time and frequency
domains. Extensive experiments on four public datasets demonstrate that FreCT
outperforms existing methods in identifying anomalies.

</details>


### [49] [Addressing Noise and Stochasticity in Fraud Detection for Service Networks](https://arxiv.org/abs/2505.00946)
*Wenxin Zhang,Ding Xu,Xi Xuan,Lei Jiang,Guangzhen Yao,Renda Han,Xiangxiang Lang,Cuicui Luo*

Main category: cs.LG

TL;DR: 作者提出了一种基于信息瓶颈理论的谱图网络（SGNN-IB）来解决服务网络中欺诈检测的挑战，通过分割同质性和异质子图以及引入原型学习，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的谱图方法在处理图信号时无法有效去噪和区分频率特性，导致信号过滤和融合能力不足。SGNN-IB旨在通过信息瓶颈理论和原型学习改进这些问题。

Method: SGNN-IB将原始图分割为同质性和异质子图，利用信息瓶颈提取关键特征，并引入原型学习实现信号融合。

Result: 在三个真实数据集上，SGNN-IB的表现优于当前最先进的欺诈检测方法。

Conclusion: SGNN-IB通过改进信号处理机制，显著提升了欺诈检测的准确性和鲁棒性。

Abstract: Fraud detection is crucial in social service networks to maintain user trust
and improve service network security. Existing spectral graph-based methods
address this challenge by leveraging different graph filters to capture signals
with different frequencies in service networks. However, most graph
filter-based methods struggle with deriving clean and discriminative graph
signals. On the one hand, they overlook the noise in the information
propagation process, resulting in degradation of filtering ability. On the
other hand, they fail to discriminate the frequency-specific characteristics of
graph signals, leading to distortion of signals fusion. To address these
issues, we develop a novel spectral graph network based on information
bottleneck theory (SGNN-IB) for fraud detection in service networks. SGNN-IB
splits the original graph into homophilic and heterophilic subgraphs to better
capture the signals at different frequencies. For the first limitation, SGNN-IB
applies information bottleneck theory to extract key characteristics of encoded
representations. For the second limitation, SGNN-IB introduces prototype
learning to implement signal fusion, preserving the frequency-specific
characteristics of signals. Extensive experiments on three real-world datasets
demonstrate that SGNN-IB outperforms state-of-the-art fraud detection methods.

</details>


### [50] [Adaptive Branch-and-Bound Tree Exploration for Neural Network Verification](https://arxiv.org/abs/2505.00963)
*Kota Fukuda,Guanqin Zhang,Zhenya Zhang,Yulei Sui,Jianjun Zhao*

Main category: cs.LG

TL;DR: ABONN是一种基于蒙特卡洛树搜索（MCTS）的自适应分支定界方法，通过优先探索更可能找到反例的子问题，显著提升了神经网络形式化验证的效率。实验表明，其在MNIST和CIFAR-10数据集上分别实现了15.2倍和24.7倍的加速。


<details>
  <summary>Details</summary>
Motivation: 现有分支定界（BaB）方法在验证神经网络时效率不足，因忽略子问题的重要性而盲目探索空间。为提高效率，需引入子问题重要性概念，并自适应地优先探索高潜力子问题。

Method: 提出ABONN方法，结合蒙特卡洛树搜索（MCTS）框架，动态评估子问题的反例发现概率（重要性），指导优先探索。支持早期终止（发现反例）或完整验证（遍历所有子问题）。

Result: 在552个验证问题（覆盖MNIST/CIFAR-10等数据集）上测试，ABONN相比基线方法提速最高达15.2倍（MNIST）和24.7倍（CIFAR-10）。超参数分析与自适应探索策略被证明有效。

Conclusion: ABONN通过重要性驱动的自适应子问题探索，显著提升验证效率，同时保持完备性，为神经网络形式化验证提供了新思路。

Abstract: Formal verification is a rigorous approach that can provably ensure the
quality of neural networks, and to date, Branch and Bound (BaB) is the
state-of-the-art that performs verification by splitting the problem as needed
and applying off-the-shelf verifiers to sub-problems for improved performance.
However, existing BaB may not be efficient, due to its naive way of exploring
the space of sub-problems that ignores the \emph{importance} of different
sub-problems. To bridge this gap, we first introduce a notion of ``importance''
that reflects how likely a counterexample can be found with a sub-problem, and
then we devise a novel verification approach, called ABONN, that explores the
sub-problem space of BaB adaptively, in a Monte-Carlo tree search (MCTS) style.
The exploration is guided by the ``importance'' of different sub-problems, so
it favors the sub-problems that are more likely to find counterexamples. As
soon as it finds a counterexample, it can immediately terminate; even though it
cannot find, after visiting all the sub-problems, it can still manage to verify
the problem. We evaluate ABONN with 552 verification problems from
commonly-used datasets and neural network models, and compare it with the
state-of-the-art verifiers as baseline approaches. Experimental evaluation
shows that ABONN demonstrates speedups of up to $15.2\times$ on MNIST and
$24.7\times$ on CIFAR-10. We further study the influences of hyperparameters to
the performance of ABONN, and the effectiveness of our adaptive tree
exploration.

</details>


### [51] [Tree-Sliced Wasserstein Distance with Nonlinear Projection](https://arxiv.org/abs/2505.00968)
*Thanh Tran,Viet-Hoang Tran,Thanh Chu,Trang Pham,Laurent El Ghaoui,Tam Le,Tan M. Nguyen*

Main category: cs.LG

TL;DR: 论文提出了一种基于树结构的非线性投影框架，改进并扩展了树切片Wasserstein距离（TSW），适用于欧几里得空间和球面数据，在计算效率和应用性能上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统切片Wasserstein距离（SW）依赖一维线性投影，而树切片方法通过树形结构捕捉更丰富的拓扑信息。本文旨在进一步扩展TSW，通过非线性投影提升其表达能力，同时保持计算效率。

Method: 提出非线性投影框架，取代原有的线性投影，确保Radon变换的可逆性和新度量定义的合理性。通过设计合适的投影方式，构建适用于欧几里得空间和球面的高效度量。

Result: 实验验证表明，新方法在欧几里得和球面数据集上表现优异，梯度流、自监督学习和生成模型等应用中显著优于现有SW和TSW变体。

Conclusion: 非线性投影TSW框架在拓扑表达能力与计算效率间取得平衡，为后续研究提供了新方向，尤其在需要复杂结构建模的应用场景中潜力显著。

Abstract: Tree-Sliced methods have recently emerged as an alternative to the
traditional Sliced Wasserstein (SW) distance, replacing one-dimensional lines
with tree-based metric spaces and incorporating a splitting mechanism for
projecting measures. This approach enhances the ability to capture the
topological structures of integration domains in Sliced Optimal Transport while
maintaining low computational costs. Building on this foundation, we propose a
novel nonlinear projectional framework for the Tree-Sliced Wasserstein (TSW)
distance, substituting the linear projections in earlier versions with general
projections, while ensuring the injectivity of the associated Radon Transform
and preserving the well-definedness of the resulting metric. By designing
appropriate projections, we construct efficient metrics for measures on both
Euclidean spaces and spheres. Finally, we validate our proposed metric through
extensive numerical experiments for Euclidean and spherical datasets.
Applications include gradient flows, self-supervised learning, and generative
models, where our methods demonstrate significant improvements over recent SW
and TSW variants.

</details>


### [52] [A Minimax-MDP Framework with Future-imposed Conditions for Learning-augmented Problems](https://arxiv.org/abs/2505.00973)
*Xin Chen,Yuze Chen,Yuan Zhou*

Main category: cs.LG

TL;DR: 该论文研究了一种带有增强预测的序列决策问题，提出了一个极小极大马尔可夫决策过程（minimax-MDP）框架，用于设计鲁棒性强的在线决策策略。


<details>
  <summary>Details</summary>
Motivation: 在实际决策问题中，预测信息可能随时间逐步细化，决策者需要在这些不确定性下做出具有竞争力的决策。本文旨在提供一个统一的框架来处理这类问题。

Method: 利用极小极大马尔可夫决策过程（minimax-MDP）框架，将系统状态分为环境状态和决策者控制的内部状态，并通过未来施加的条件来设计高效、鲁棒的策略。

Result: 通过三个应用案例（多周期库存订购、资源分配和多阶段库存问题）验证了所提框架的有效性和灵活性。

Conclusion: 该论文提出的方法为解决预测不确定下的鲁棒在线决策问题提供了实用且通用的解决方案。

Abstract: We study a class of sequential decision-making problems with augmented
predictions, potentially provided by a machine learning algorithm. In this
setting, the decision-maker receives prediction intervals for unknown
parameters that become progressively refined over time, and seeks decisions
that are competitive with the hindsight optimal under all possible realizations
of both parameters and predictions. We propose a minimax Markov Decision
Process (minimax-MDP) framework, where the system state consists of an
adversarially evolving environment state and an internal state controlled by
the decision-maker. We introduce a set of future-imposed conditions that
characterize the feasibility of minimax-MDPs and enable the design of
efficient, often closed-form, robustly competitive policies. We illustrate the
framework through three applications: multi-period inventory ordering with
refining demand predictions, resource allocation with uncertain utility
functions, and a multi-phase extension of the minimax-MDP applied to the
inventory problem with time-varying ordering costs. Our results provide a
tractable and versatile approach to robust online decision-making under
predictive uncertainty.

</details>


### [53] [Towards the Resistance of Neural Network Watermarking to Fine-tuning](https://arxiv.org/abs/2505.01007)
*Ling Tang,Yuefeng Chen,Hui Xue,Quanshi Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种新的水印方法，用于将所有权信息嵌入深度神经网络（DNN），并且该方法对微调具有鲁棒性。通过修订的傅里叶变换从卷积滤波器中提取特定频率成分，并证明这些成分在微调过程中不会被梯度下降改变。


<details>
  <summary>Details</summary>
Motivation: 为解决深度神经网络中所有权保护和盗版防范的问题，尤其是在微调环境下水印的鲁棒性不足。

Method: 提出了一种基于修订傅里叶变换的方法，通过将水印信息编码到卷积滤波器的特定频率成分中，同时证明这些成分对权重缩放和排列具有等变性。

Result: 初步实验证明了该方法的有效性。

Conclusion: 该方法为深度学习模型提供了一种鲁棒的水印方案，能够抵抗微调攻击。

Abstract: This paper proves a new watermarking method to embed the ownership
information into a deep neural network (DNN), which is robust to fine-tuning.
Specifically, we prove that when the input feature of a convolutional layer
only contains low-frequency components, specific frequency components of the
convolutional filter will not be changed by gradient descent during the
fine-tuning process, where we propose a revised Fourier transform to extract
frequency components from the convolutional filter. Additionally, we also prove
that these frequency components are equivariant to weight scaling and weight
permutations. In this way, we design a watermark module to encode the watermark
information to specific frequency components in a convolutional filter.
Preliminary experiments demonstrate the effectiveness of our method.

</details>


### [54] [Accelerating Deep Neural Network Training via Distributed Hybrid Order Optimization](https://arxiv.org/abs/2505.00982)
*Shunxian Gu,Chaoqun You,Bangbang Ren,Lailong Luo,Junxu Xia,Deke Guo*

Main category: cs.LG

TL;DR: FOSI优化器通过结合梯度和曲率信息加速DNN训练，提出分布式设计DHO$_2$以减少内存负担和训练时间，实验表明其优于传统优化器。


<details>
  <summary>Details</summary>
Motivation: 解决计算资源有限用户无法扩展DNN训练的问题，利用FOSI优化器的高效收敛特性。

Method: 设计分布式架构DHO$_2$，包括曲率信息的分布式计算和部分更新，以及并行化计算和模型更新策略。

Result: 内存负担随设备数量线性减少，训练时间相比传统优化器快1.4倍至2.1倍。

Conclusion: DHO$_2$在资源受限环境下有效加速DNN训练，性能优于传统方法。

Abstract: Scaling deep neural network (DNN) training to more devices can reduce
time-to-solution. However, it is impractical for users with limited computing
resources. FOSI, as a hybrid order optimizer, converges faster than
conventional optimizers by taking advantage of both gradient information and
curvature information when updating the DNN model. Therefore, it provides a new
chance for accelerating DNN training in the resource-constrained setting. In
this paper, we explore its distributed design, namely DHO$_2$, including
distributed calculation of curvature information and model update with partial
curvature information to accelerate DNN training with a low memory burden. To
further reduce the training time, we design a novel strategy to parallelize the
calculation of curvature information and the model update on different devices.
Experimentally, our distributed design can achieve an approximate linear
reduction of memory burden on each device with the increase of the device
number. Meanwhile, it achieves $1.4\times\sim2.1\times$ speedup in the total
training time compared with other distributed designs based on conventional
first- and second-order optimizers.

</details>


### [55] [Toward Data-centric Directed Graph Learning: An Entropy-driven Approach](https://arxiv.org/abs/2505.00983)
*Xunkai Li,Zhengyu Wu,Kaichi Yu,Hongchao Qin,Guang Zeng,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为EDEN的数据中心有向图学习范式，通过层次编码理论和知识蒸馏提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有有向图神经网络未能充分挖掘有向图中隐藏的数据知识，导致模型预测性能不佳，需从数据角度探索有向边与节点特征的潜在关联。

Method: EDEN通过构建层次知识树（HKT）和量化节点互信息，实现数据驱动的知识蒸馏监督，并可泛化至无向图场景。

Result: 在14个（有向/无向）图数据集和4个下游任务上，EDEN实现了SOTA性能，显著提升了现有（有向）图神经网络的性能。

Conclusion: EDEN作为一种通用框架，通过数据中心方法增强了模型编码能力，适用于多种图数据场景。

Abstract: The directed graph (digraph), as a generalization of undirected graphs,
exhibits superior representation capability in modeling complex topology
systems and has garnered considerable attention in recent years. Despite the
notable efforts made by existing DiGraph Neural Networks (DiGNNs) to leverage
directed edges, they still fail to comprehensively delve into the abundant data
knowledge concealed in the digraphs. This data-level limitation results in
model-level sub-optimal predictive performance and underscores the necessity of
further exploring the potential correlations between the directed edges
(topology) and node profiles (feature and labels) from a data-centric
perspective, thereby empowering model-centric neural networks with stronger
encoding capabilities.
  In this paper, we propose \textbf{E}ntropy-driven \textbf{D}igraph
knowl\textbf{E}dge distillatio\textbf{N} (EDEN), which can serve as a
data-centric digraph learning paradigm or a model-agnostic hot-and-plug
data-centric Knowledge Distillation (KD) module. The core idea is to achieve
data-centric ML, guided by our proposed hierarchical encoding theory for
structured data. Specifically, EDEN first utilizes directed structural
measurements from a topology perspective to construct a coarse-grained
Hierarchical Knowledge Tree (HKT). Subsequently, EDEN quantifies the mutual
information of node profiles to refine knowledge flow in the HKT, enabling
data-centric KD supervision within model training. As a general framework, EDEN
can also naturally extend to undirected scenarios and demonstrate satisfactory
performance. In our experiments, EDEN has been widely evaluated on 14 (di)graph
datasets (homophily and heterophily) and across 4 downstream tasks. The results
demonstrate that EDEN attains SOTA performance and exhibits strong improvement
for prevalent (Di)GNNs.

</details>


### [56] [Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii](https://arxiv.org/abs/2505.01372)
*Kola Ayonrinde,Louis Jaburi*

Main category: cs.LG

TL;DR: 该论文提出了一个基于哲学视角的框架来评估和改进机制可解释性（MI）中的解释方法，强调紧凑证明（Compact Proofs）的潜力，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏统一的解释评估方法，机制可解释性（MI）的发展受到限制。论文旨在通过哲学视角回答“什么是好的解释”这一核心问题。

Method: 引入了一个多元化的解释美德框架（Explanatory Virtues Framework），结合了贝叶斯主义、库恩范式、德式主义和规范性四种哲学视角。

Result: 研究发现紧凑证明（Compact Proofs）能综合考虑多种解释美德，是一种有前景的方法。框架还指出了三个未来研究方向。

Conclusion: 改进的MI方法有助于增强对AI系统的监控、预测和控制能力。

Abstract: Mechanistic Interpretability (MI) aims to understand neural networks through
causal explanations. Though MI has many explanation-generating methods,
progress has been limited by the lack of a universal approach to evaluating
explanations. Here we analyse the fundamental question "What makes a good
explanation?" We introduce a pluralist Explanatory Virtues Framework drawing on
four perspectives from the Philosophy of Science - the Bayesian, Kuhnian,
Deutschian, and Nomological - to systematically evaluate and improve
explanations in MI. We find that Compact Proofs consider many explanatory
virtues and are hence a promising approach. Fruitful research directions
implied by our framework include (1) clearly defining explanatory simplicity,
(2) focusing on unifying explanations and (3) deriving universal principles for
neural networks. Improved MI methods enhance our ability to monitor, predict,
and steer AI systems.

</details>


### [57] [On-demand Test-time Adaptation for Edge Devices](https://arxiv.org/abs/2505.00986)
*Xiao Ma,Young D. Kwon,Dong Ma*

Main category: cs.LG

TL;DR: 论文提出了一种新颖的按需测试时适应（OD-TTA）框架，通过轻量级域偏移检测、源域选择模块和解耦批量归一化更新方案，显著降低了计算开销和能耗，同时保持了高性能。


<details>
  <summary>Details</summary>
Motivation: 现有持续测试时适应（CTTA）方法在资源有限的边缘设备上因高内存开销和能耗而难以实用，因此需要一种更高效的适应范式。

Method: 1) 轻量级域偏移检测机制；2) 源域选择模块；3) 解耦批量归一化更新方案。

Result: OD-TTA在减少能耗和计算开销的同时，性能表现与现有方法相当甚至更好。

Conclusion: OD-TTA为测试时适应在边缘设备上的实际应用提供了可行方案。

Abstract: Continual Test-time adaptation (CTTA) continuously adapts the deployed model
on every incoming batch of data. While achieving optimal accuracy, existing
CTTA approaches present poor real-world applicability on resource-constrained
edge devices, due to the substantial memory overhead and energy consumption. In
this work, we first introduce a novel paradigm -- on-demand TTA -- which
triggers adaptation only when a significant domain shift is detected. Then, we
present OD-TTA, an on-demand TTA framework for accurate and efficient
adaptation on edge devices. OD-TTA comprises three innovative techniques: 1) a
lightweight domain shift detection mechanism to activate TTA only when it is
needed, drastically reducing the overall computation overhead, 2) a source
domain selection module that chooses an appropriate source model for
adaptation, ensuring high and robust accuracy, 3) a decoupled Batch
Normalization (BN) update scheme to enable memory-efficient adaptation with
small batch sizes. Extensive experiments show that OD-TTA achieves comparable
and even better performance while reducing the energy and computation overhead
remarkably, making TTA a practical reality.

</details>


### [58] [Where's the liability in the Generative Era? Recovery-based Black-Box Detection of AI-Generated Content](https://arxiv.org/abs/2505.01008)
*Haoyue Bai,Yiyou Sun,Wei Cheng,Haifeng Chen*

Main category: cs.LG

TL;DR: 该论文提出了一种新的黑盒检测框架，通过“损坏与恢复”策略检测生成图像，无需模型权重或大数据集，性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型生成的逼真图像增多，其在误信息和欺诈中的潜在滥用凸显了有效检测方法的必要性。现有方法依赖模型权重或大数据集，限制了可扩展性和实际应用。

Method: 采用黑盒检测框架，通过屏蔽部分图像并评估模型重建能力来判断图像是否由生成模型生成；对于不支持屏蔽输入的黑盒模型，使用低成本替代模型增强检测能力。

Result: 在八个扩散模型变体数据集上，该方法平均精度比基线方法高出4.31%。

Conclusion: 提出的方法为生成图像检测提供了一种高效、可扩展的解决方案，适用于实际场景。

Abstract: The recent proliferation of photorealistic images created by generative
models has sparked both excitement and concern, as these images are
increasingly indistinguishable from real ones to the human eye. While offering
new creative and commercial possibilities, the potential for misuse, such as in
misinformation and fraud, highlights the need for effective detection methods.
Current detection approaches often rely on access to model weights or require
extensive collections of real image datasets, limiting their scalability and
practical application in real world scenarios. In this work, we introduce a
novel black box detection framework that requires only API access, sidestepping
the need for model weights or large auxiliary datasets. Our approach leverages
a corrupt and recover strategy: by masking part of an image and assessing the
model ability to reconstruct it, we measure the likelihood that the image was
generated by the model itself. For black-box models that do not support masked
image inputs, we incorporate a cost efficient surrogate model trained to align
with the target model distribution, enhancing detection capability. Our
framework demonstrates strong performance, outperforming baseline methods by
4.31% in mean average precision across eight diffusion model variant datasets.

</details>


### [59] [Stagnation in Evolutionary Algorithms: Convergence $\neq$ Optimality](https://arxiv.org/abs/2505.01036)
*Xiaojun Zhou*

Main category: cs.LG

TL;DR: 论文指出进化计算中停滞并非一定阻碍收敛，收敛也不代表最优性，甚至局部最优。通过反例证明收敛不足以保证算法有效性。


<details>
  <summary>Details</summary>
Motivation: 挑战进化计算领域的传统观点，即停滞阻碍收敛且收敛等同于最优性，揭示其误导性。

Method: 通过理论分析和提供反例，论证个体停滞可能促进种群收敛，且收敛与最优性无必然关联。

Result: 证明停滞可助力收敛，但收敛本身无法保证算法效果，需更全面的评估标准。

Conclusion: 传统对停滞和收敛的认知需修正，未来研究应关注收敛之外的算法性能指标。

Abstract: In the evolutionary computation community, it is widely believed that
stagnation impedes convergence in evolutionary algorithms, and that convergence
inherently indicates optimality. However, this perspective is misleading. In
this study, it is the first to highlight that the stagnation of an individual
can actually facilitate the convergence of the entire population, and
convergence does not necessarily imply optimality, not even local optimality.
Convergence alone is insufficient to ensure the effectiveness of evolutionary
algorithms. Several counterexamples are provided to illustrate this argument.

</details>


### [60] [Global Optimality of Single-Timescale Actor-Critic under Continuous State-Action Space: A Study on Linear Quadratic Regulator](https://arxiv.org/abs/2505.01041)
*Xuyang Chen,Jingliang Duan,Lin Zhao*

Main category: cs.LG

TL;DR: 本文研究了经典单样本单时间尺度的actor-critic方法在连续状态-动作空间上的性能，证明了其在LQR问题上可实现ε-最优解，样本复杂度为ε的负二次方，填补了理论与实践的差距。


<details>
  <summary>Details</summary>
Motivation: 尽管actor-critic方法在各种挑战性任务中表现出色，但其理论理解仍不充分。现有研究多关注不常见的双循环或双时间尺度变体，仅证明其在有限状态-动作空间上的局部收敛性。本文旨在探索经典单时间尺度actor-critic在连续空间上的性能。

Method: 以典型线性二次调节器（LQR）问题为例，研究单样本单时间尺度actor-critic方法在连续状态-动作空间上的表现。

Result: 证明单时间尺度actor-critic可在连续空间上实现ε-最优解，样本复杂度为O(ε^-2)。

Conclusion: 本文为单时间尺度actor-critic的性能提供了新见解，进一步缩小了理论与实践的差距。

Abstract: Actor-critic methods have achieved state-of-the-art performance in various
challenging tasks. However, theoretical understandings of their performance
remain elusive and challenging. Existing studies mostly focus on practically
uncommon variants such as double-loop or two-timescale stepsize actor-critic
algorithms for simplicity. These results certify local convergence on finite
state- or action-space only. We push the boundary to investigate the classic
single-sample single-timescale actor-critic on continuous (infinite)
state-action space, where we employ the canonical linear quadratic regulator
(LQR) problem as a case study. We show that the popular single-timescale
actor-critic can attain an epsilon-optimal solution with an order of epsilon to
-2 sample complexity for solving LQR on the demanding continuous state-action
space. Our work provides new insights into the performance of single-timescale
actor-critic, which further bridges the gap between theory and practice.

</details>


### [61] [Low-Precision Training of Large Language Models: Methods, Challenges, and Opportunities](https://arxiv.org/abs/2505.01043)
*Zhiwei Hao,Jianyuan Guo,Li Shen,Yong Luo,Han Hu,Guoxia Wang,Dianhai Yu,Yonggang Wen,Dacheng Tao*

Main category: cs.LG

TL;DR: 这篇论文回顾了低精度训练技术的现状，将其分为三类（定点/整数、浮点、自定义格式），并讨论了量化感知训练及未来方向。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型训练的资源需求高，低精度训练技术可提升效率，但现有研究分散且缺乏统一视角。

Method: 通过分类低精度训练方法（基于数值格式）并探讨量化感知训练，系统梳理该领域。

Result: 总结了三类主要方法及其优缺点，并提供了相关论文集合。

Conclusion: 低精度训练技术仍有改进空间，未来需关注硬件兼容性、效率提升等方向。

Abstract: Large language models (LLMs) have achieved impressive performance across
various domains. However, the substantial hardware resources required for their
training present a significant barrier to efficiency and scalability. To
mitigate this challenge, low-precision training techniques have been widely
adopted, leading to notable advancements in training efficiency. Despite these
gains, low-precision training involves several components$\unicode{x2013}$such
as weights, activations, and gradients$\unicode{x2013}$each of which can be
represented in different numerical formats. The resulting diversity has created
a fragmented landscape in low-precision training research, making it difficult
for researchers to gain a unified overview of the field. This survey provides a
comprehensive review of existing low-precision training methods. To
systematically organize these approaches, we categorize them into three primary
groups based on their underlying numerical formats, which is a key factor
influencing hardware compatibility, computational efficiency, and ease of
reference for readers. The categories are: (1) fixed-point and integer-based
methods, (2) floating-point-based methods, and (3) customized format-based
methods. Additionally, we discuss quantization-aware training approaches, which
share key similarities with low-precision training during forward propagation.
Finally, we highlight several promising research directions to advance this
field. A collection of papers discussed in this survey is provided in
https://github.com/Hao840/Awesome-Low-Precision-Training.

</details>


### [62] [Multi-Step Consistency Models: Fast Generation with Theoretical Guarantees](https://arxiv.org/abs/2505.01049)
*Nishant Jain,Xunpeng Huang,Yian Ma,Tong Zhang*

Main category: cs.LG

TL;DR: 本文通过理论分析证实，一致性模型在少量步骤下即可高效生成高质量样本，弥补了其加速生成缺乏理论依据的空白，并在数据分布假设最小化的情况下展示了类似的收敛保证。


<details>
  <summary>Details</summary>
Motivation: 尽管一致性模型在实证中表现出色，但其加速生成的理论依据尚不明确。本文旨在填补这一空白，证明其在极少迭代步骤下即可达到高质量样本生成的效果。

Method: 通过展示一致性模型如何将输入映射到任意时间戳的反向轨迹，理论上证明其仅需对数级迭代步骤即可达到较小的KL散度。进一步分析了在数据分布假设最小化情况下的收敛保证，以及小离散化步骤下的模型估计可行性。

Result: 理论结果表明，一致性模型仅需对数级迭代步骤即可实现KL散度收敛，且在非平滑设置下优于现有SDE或ODE分析方法的收敛速率。

Conclusion: 本文为一一致性模型的高效生成提供了坚实的理论支持，证明了其在极少步骤下的优越性能，并在不同数据分布假设下均表现出色。

Abstract: Consistency models have recently emerged as a compelling alternative to
traditional SDE based diffusion models, offering a significant acceleration in
generation by producing high quality samples in very few steps. Despite their
empirical success, a proper theoretic justification for their speed up is still
lacking. In this work, we provide the analysis which bridges this gap, showing
that given a consistency model which can map the input at a given time to
arbitrary timestamps along the reverse trajectory, one can achieve KL
divergence of order $ O(\varepsilon^2) $ using only $
O\left(\log\left(\frac{d}{\varepsilon}\right)\right) $ iterations with constant
step size, where d is the data dimension. Additionally, under minimal
assumptions on the data distribution an increasingly common setting in recent
diffusion model analyses we show that a similar KL convergence guarantee can be
obtained, with the number of steps scaling as $ O\left(d
\log\left(\frac{d}{\varepsilon}\right)\right) $. Going further, we also provide
a theoretical analysis for estimation of such consistency models, concluding
that accurate learning is feasible using small discretization steps, both in
smooth and non smooth settings. Notably, our results for the non smooth case
yield best in class convergence rates compared to existing SDE or ODE based
analyses under minimal assumptions.

</details>


### [63] [Monotone Peridynamic Neural Operator for Nonlinear Material Modeling with Conditionally Unique Solutions](https://arxiv.org/abs/2505.01060)
*Jihong Wang,Xiaochuan Tian,Zhongqiang Zhang,Stewart Silling,Siavash Jafarzadeh,Yue Yu*

Main category: cs.LG

TL;DR: 该论文提出了一种新的数据驱动非局部本构模型学习方法——单调近程神经算子（MPNO），确保解的独特性，并在合成和真实数据上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的本构模型在物理可解释性和泛化性上有优势，但其良好性通常无法预先保证，容易导致非物理解。因此，研究旨在提出一种能保证解唯一性的方法。

Method: 通过单调梯度网络学习非局部核和非线性本构关系，约束梯度诱导能量密度函数的凸性，确保小变形区的解唯一性。

Result: 在合成和真实数据上验证了MPNO的性能，显示其能够收敛到真实解，并在新载荷任务中比传统神经网络表现更好。

Conclusion: MPNO在确保解唯一性和泛化能力上表现出色，且在真实场景中具有实际应用价值。

Abstract: Data-driven methods have emerged as powerful tools for modeling the responses
of complex nonlinear materials directly from experimental measurements. Among
these methods, the data-driven constitutive models present advantages in
physical interpretability and generalizability across different boundary
conditions/domain settings. However, the well-posedness of these learned models
is generally not guaranteed a priori, which makes the models prone to
non-physical solutions in downstream simulation tasks. In this study, we
introduce monotone peridynamic neural operator (MPNO), a novel data-driven
nonlocal constitutive model learning approach based on neural operators. Our
approach learns a nonlocal kernel together with a nonlinear constitutive
relation, while ensuring solution uniqueness through a monotone gradient
network. This architectural constraint on gradient induces convexity of the
learnt energy density function, thereby guaranteeing solution uniqueness of
MPNO in small deformation regimes. To validate our approach, we evaluate MPNO's
performance on both synthetic and real-world datasets. On synthetic datasets
with manufactured kernel and constitutive relation, we show that the learnt
model converges to the ground-truth as the measurement grid size decreases both
theoretically and numerically. Additionally, our MPNO exhibits superior
generalization capabilities than the conventional neural networks: it yields
smaller displacement solution errors in down-stream tasks with new and unseen
loadings. Finally, we showcase the practical utility of our approach through
applications in learning a homogenized model from molecular dynamics data,
highlighting its expressivity and robustness in real-world scenarios.

</details>


### [64] [Improving Group Fairness in Knowledge Distillation via Laplace Approximation of Early Exits](https://arxiv.org/abs/2505.01070)
*Edvin Fasth,Sagar Singh*

Main category: cs.LG

TL;DR: 论文提出了一种基于Laplace近似的方法，通过重新加权困难样本来提升知识蒸馏中的群体公平性，并在MultiNLI数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决知识蒸馏中学生模型在早期层学习表面特征导致的群体公平性下降问题，论文探索更鲁棒的困难样本识别方法。

Method: 采用Laplace近似校准不确定性估计，重新加权交叉熵和蒸馏损失，对比传统的置信度边际方法。

Result: 在Bert模型和MultiNLI数据集上的实验表明，Laplace近似能更有效识别困难样本，提升群体公平性。

Conclusion: Laplace近似为知识蒸馏中的公平性问题提供了一种更鲁棒的解决方案，优于边际基于的方法。

Abstract: Knowledge distillation (KD) has become a powerful tool for training compact
student models using larger, pretrained teacher models, often requiring less
data and computational resources. Teacher models typically possess more layers
and thus exhibit richer feature representations compared to their student
counterparts. Furthermore, student models tend to learn simpler, surface-level
features in their early layers. This discrepancy can increase errors in groups
where labels spuriously correlate with specific input attributes, leading to a
decline in group fairness even when overall accuracy remains comparable to the
teacher. To mitigate these challenges, Early-Exit Neural Networks (EENNs),
which enable predictions at multiple intermediate layers, have been employed.
Confidence margins derived from these early exits have been utilized to
reweight both cross-entropy and distillation losses on a per-instance basis. In
this paper, we propose that leveraging Laplace approximation-based methods to
obtain well-calibrated uncertainty estimates can also effectively reweight
challenging instances and improve group fairness. We hypothesize that Laplace
approximation offers a more robust identification of difficult or ambiguous
instances compared to margin-based approaches. To validate our claims, we
benchmark our approach using a Bert-based model on the MultiNLI dataset.

</details>


### [65] [Federated Adapter on Foundation Models: An Out-Of-Distribution Approach](https://arxiv.org/abs/2505.01075)
*Yiyuan Yang,Guodong Long,Tianyi Zhou,Qinghua Lu,Shanshan Ye,Jing Jiang*

Main category: cs.LG

TL;DR: 本文提出了FedOA方法，通过适配器参数高效微调和基于特征距离的正则化解决联邦基础模型中的分布外泛化问题，理论和实证均验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 为解决联邦基础模型在大规模参数和数据异构性下的分布外泛化挑战，提出了高效且个性化的解决方案。

Method: 采用适配器参数高效微调方法，并引入基于特征距离的正则化，实现对客户端分布的个性化对齐。

Result: 理论和实验验证，方法提升了分布外泛化能力，并在多个NLP任务数据集上表现优异。

Conclusion: FedOA通过全局模型指导的个性化正则化，有效解决了联邦基础模型的分布外泛化问题，具有实际应用潜力。

Abstract: As foundation models gain prominence, Federated Foundation Models (FedFM)
have emerged as a privacy-preserving approach to collaboratively fine-tune
models in federated learning (FL) frameworks using distributed datasets across
clients. A key challenge for FedFM, given the versatile nature of foundation
models, is addressing out-of-distribution (OOD) generalization, where unseen
tasks or clients may exhibit distribution shifts leading to suboptimal
performance. Although numerous studies have explored OOD generalization in
conventional FL, these methods are inadequate for FedFM due to the challenges
posed by large parameter scales and increased data heterogeneity. To address
these, we propose FedOA, which employs adapter-based parameter-efficient
fine-tuning methods for efficacy and introduces personalized adapters with
feature distance-based regularization to align distributions and guarantee OOD
generalization for each client. Theoretically, we demonstrate that the
conventional aggregated global model in FedFM inherently retains OOD
generalization capabilities, and our proposed method enhances the personalized
model's OOD generalization through regularization informed by the global model,
with proven convergence under general non-convex settings. Empirically, the
effectiveness of the proposed method is validated on benchmark datasets across
various NLP tasks.

</details>


### [66] [Integration Matters for Learning PDEs with Backwards SDEs](https://arxiv.org/abs/2505.01078)
*Sungje Park,Stephen Tu*

Main category: cs.LG

TL;DR: 研究发现，基于BSDE的PDE求解器性能不佳的原因是EM积分方案引入的离散化偏差，提出Stratonovich-BSDE结合Heun积分的方法，显著提升性能并超越PINNs。


<details>
  <summary>Details</summary>
Motivation: 现有基于BSDE的求解器在高维PDE求解中表现不如PINNs，其根本原因是EM积分在短时自洽BSDE损失中引入的离散化偏差导致优化目标偏离。

Method: 提出Stratonovich-BSDE框架，并采用随机Heun积分方法，以消除EM积分带来的偏差问题。

Result: Heun-BSDE方法完全解决了EM积分的偏差问题，并在多个高维基准测试中表现优于EM变体，与PINNs竞争。

Conclusion: 积分方案对BSDE-PDE求解器的性能至关重要，Stratonovich-BSDE结合Heun积分是解决现有问题的有效方法。

Abstract: Backward stochastic differential equation (BSDE)-based deep learning methods
provide an alternative to Physics-Informed Neural Networks (PINNs) for solving
high-dimensional partial differential equations (PDEs), offering algorithmic
advantages in settings such as stochastic optimal control, where the PDEs of
interest are tied to an underlying dynamical system. However, existing
BSDE-based solvers have empirically been shown to underperform relative to
PINNs in the literature. In this paper, we identify the root cause of this
performance gap as a discretization bias introduced by the standard
Euler-Maruyama (EM) integration scheme applied to short-horizon
self-consistency BSDE losses, which shifts the optimization landscape off
target. We find that this bias cannot be satisfactorily addressed through finer
step sizes or longer self-consistency horizons. To properly handle this issue,
we propose a Stratonovich-based BSDE formulation, which we implement with
stochastic Heun integration. We show that our proposed approach completely
eliminates the bias issues faced by EM integration. Furthermore, our empirical
results show that our Heun-based BSDE method consistently outperforms EM-based
variants and achieves competitive results with PINNs across multiple
high-dimensional benchmarks. Our findings highlight the critical role of
integration schemes in BSDE-based PDE solvers, an algorithmic detail that has
received little attention thus far in the literature.

</details>


### [67] [Multi-Objective Reinforcement Learning for Water Management](https://arxiv.org/abs/2505.01094)
*Zuzanna Osika,Roxana Radelescu,Jazmin Zatarain Salazar,Frans Oliehoek,Pradeep K. Murukannaiah*

Main category: cs.LG

TL;DR: 论文提出了一种多目标强化学习（MORL）在水资源管理（尼罗河流域）中的应用，结果表明专业的水资源管理方法优于现有MORL算法。


<details>
  <summary>Details</summary>
Motivation: 当前MORL领域缺乏复杂且真实的环境和基准，论文通过尼罗河流域的水资源管理案例填补了这一空白。

Method: 将尼罗河流域管理建模为MORL环境，并对现有MORL算法进行基准测试。

Result: 专业的水资源管理方法优于现有MORL算法，突显了MORL在现实场景中的扩展性挑战。

Conclusion: 研究强调了MORL算法在复杂现实问题中的局限性，并展示了专业方法的优势。

Abstract: Many real-world problems (e.g., resource management, autonomous driving, drug
discovery) require optimizing multiple, conflicting objectives. Multi-objective
reinforcement learning (MORL) extends classic reinforcement learning to handle
multiple objectives simultaneously, yielding a set of policies that capture
various trade-offs. However, the MORL field lacks complex, realistic
environments and benchmarks. We introduce a water resource (Nile river basin)
management case study and model it as a MORL environment. We then benchmark
existing MORL algorithms on this task. Our results show that specialized water
management methods outperform state-of-the-art MORL approaches, underscoring
the scalability challenges MORL algorithms face in real-world scenarios.

</details>


### [68] [Nesterov Method for Asynchronous Pipeline Parallel Optimization](https://arxiv.org/abs/2505.01099)
*Thalaiyasingam Ajanthan,Sameera Ramasinghe,Yan Zuo,Gil Avraham,Alexander Long*

Main category: cs.LG

TL;DR: 摘要介绍了一种改进的Nesterov加速梯度方法，用于解决流水线并行中异步优化导致的梯度延迟问题，实验表明其在大规模语言建模任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了在流水线并行中实现100%的利用率，异步优化是理想选择，但会导致梯度延迟。现有方法难以解决这一问题，因此需要新的优化策略。

Method: 提出了一种改进的Nesterov加速梯度（NAG）方法，通过调整其前瞻步骤来应对梯度延迟问题，并在理论上证明了在固定延迟下的收敛性。

Result: 在10亿参数规模的解码器架构上进行的实验显示，该方法显著优于现有异步方法，甚至超过了同步基线的性能。

Conclusion: 改进的NAG方法有效解决了异步优化中的梯度延迟问题，为大规模模型训练提供了高效方案。

Abstract: Pipeline Parallelism (PP) enables large neural network training on small,
interconnected devices by splitting the model into multiple stages. To maximize
pipeline utilization, asynchronous optimization is appealing as it offers 100%
pipeline utilization by construction. However, it is inherently challenging as
the weights and gradients are no longer synchronized, leading to stale (or
delayed) gradients. To alleviate this, we introduce a variant of Nesterov
Accelerated Gradient (NAG) for asynchronous optimization in PP. Specifically,
we modify the look-ahead step in NAG to effectively address the staleness in
gradients. We theoretically prove that our approach converges at a sublinear
rate in the presence of fixed delay in gradients. Our experiments on
large-scale language modelling tasks using decoder-only architectures with up
to 1B parameters, demonstrate that our approach significantly outperforms
existing asynchronous methods, even surpassing the synchronous baseline.

</details>


### [69] [CoCoAFusE: Beyond Mixtures of Experts via Model Fusion](https://arxiv.org/abs/2505.01105)
*Aurelio Raffa Ugolini,Mara Tanelli,Valentina Breschi*

Main category: cs.LG

TL;DR: 提出了一种名为CoCoAFusE的新方法，通过融合而非简单混合多个专家模型的预测，提高了模型的可解释性和不确定性量化能力，避免了传统混合专家模型中的多模态问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习在处理非线性输入-输出关系时表现优异，但可解释性和不确定性量化仍是挑战。传统混合专家模型（MoEs）存在多模态伪影问题，尤其在平滑过渡区域表现不佳，因此需要新的方法来改善这些问题。

Method: 提出了Competitive/Collaborative Fusion of Experts (CoCoAFusE)，在传统MoE的基础上增加了专家分布融合的功能，不仅混合专家预测，还能融合预测分布，从而避免多模态伪影并提高模型灵活性和表达力。

Result: CoCoAFusE在数值实验和真实数据案例中展示了优越性能，能够更精确地量化不确定性，并避免了传统MoE的缺陷。

Conclusion: CoCoAFusE为复杂回归问题提供了一种更灵活和可解释的解决方案，特别是在不确定性量化方面表现突出，优于传统方法。

Abstract: Many learning problems involve multiple patterns and varying degrees of
uncertainty dependent on the covariates. Advances in Deep Learning (DL) have
addressed these issues by learning highly nonlinear input-output dependencies.
However, model interpretability and Uncertainty Quantification (UQ) have often
straggled behind. In this context, we introduce the Competitive/Collaborative
Fusion of Experts (CoCoAFusE), a novel, Bayesian Covariates-Dependent Modeling
technique. CoCoAFusE builds on the very philosophy behind Mixtures of Experts
(MoEs), blending predictions from several simple sub-models (or "experts") to
achieve high levels of expressiveness while retaining a substantial degree of
local interpretability. Our formulation extends that of a classical Mixture of
Experts by contemplating the fusion of the experts' distributions in addition
to their more usual mixing (i.e., superimposition). Through this additional
feature, CoCoAFusE better accommodates different scenarios for the intermediate
behavior between generating mechanisms, resulting in tighter credible bounds on
the response variable. Indeed, only resorting to mixing, as in classical MoEs,
may lead to multimodality artifacts, especially over smooth transitions.
Instead, CoCoAFusE can avoid these artifacts even under the same structure and
priors for the experts, leading to greater expressiveness and flexibility in
modeling. This new approach is showcased extensively on a suite of motivating
numerical examples and a collection of real-data ones, demonstrating its
efficacy in tackling complex regression problems where uncertainty is a key
quantity of interest.

</details>


### [70] [Incorporating Inductive Biases to Energy-based Generative Models](https://arxiv.org/abs/2505.01111)
*Yukun Li,Li-Ping Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种结合能量基模型(EBM)和指数族模型的混合方法，通过引入无参数统计函数来增强数据建模的归纳偏置，并在实验中验证了其统计匹配能力与生成效果的提升。


<details>
  <summary>Details</summary>
Motivation: 随着基于分数匹配的模型训练和朗之万动力学样本生成技术的发展，能量基模型(EBM)作为生成模型重新受到关注。当前EBM通常使用神经网络定义能量函数，但缺乏明确的归纳偏置。本文旨在通过结合指数族模型的特性来改进这一点。

Method: 提出了一种混合模型，将EBM与指数族模型结合，通过在能量项中引入无参数统计函数，约束模型在训练过程中对齐数据统计量，即使仅近似最大化数据似然。

Result: 实验证明，该混合模型能有效匹配统计量，并在数据拟合和生成任务中表现优于纯EBM，尤其是在引入合适的统计信息时。

Conclusion: 混合方法通过结合EBM的灵活性及指数族模型的统计对齐能力，显著改善了生成模型的性能，为数据建模提供了新的方向。

Abstract: With the advent of score-matching techniques for model training and Langevin
dynamics for sample generation, energy-based models (EBMs) have gained renewed
interest as generative models. Recent EBMs usually use neural networks to
define their energy functions. In this work, we introduce a novel hybrid
approach that combines an EBM with an exponential family model to incorporate
inductive bias into data modeling. Specifically, we augment the energy term
with a parameter-free statistic function to help the model capture key data
statistics. Like an exponential family model, the hybrid model aims to align
the distribution statistics with data statistics during model training, even
when it only approximately maximizes the data likelihood. This property enables
us to impose constraints on the hybrid model. Our empirical study validates the
hybrid model's ability to match statistics. Furthermore, experimental results
show that data fitting and generation improve when suitable informative
statistics are incorporated into the hybrid model.

</details>


### [71] [Exploring Equity of Climate Policies using Multi-Agent Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2505.01115)
*Palok Biswas,Zuzanna Osika,Isidoro Tamassia,Adit Whorra,Jazmin Zatarain-Salazar,Jan Kwakkel,Frans A. Oliehoek,Pradeep K. Murukannaiah*

Main category: cs.LG

TL;DR: 论文提出了Justice框架，首次将集成评估模型（IAM）与多目标多智能体强化学习（MOMARL）结合，以解决传统IAM单目标优化的局限性，实现气候政策中的公平与多重目标平衡。


<details>
  <summary>Details</summary>
Motivation: 传统IAM模型因单目标优化（如经济或气候目标）而无法捕捉政策中的公平与多重目标权衡，导致政策建议加剧不平等，影响国际气候谈判。此研究旨在通过多目标、多智能体方法弥补这一缺陷。

Method: 采用MOMARL（多目标多智能体强化学习）扩展IAM模型，引入多目标优化和多个智能体模拟政策制定者互动，生成同时考虑经济、气候目标和公平性的帕累托最优政策集。

Result: Justice框架成功生成了兼顾公平性、经济与气候目标的帕累托最优政策选项，并通过多智能体交互呈现政策制定中的现实权衡，为谈判提供透明决策支持。

Conclusion: 该框架为气候政策制定提供了更全面的分析工具，强调公平性与多目标协同，有望改善政策谈判中的分歧与不平等问题。

Abstract: Addressing climate change requires coordinated policy efforts of nations
worldwide. These efforts are informed by scientific reports, which rely in part
on Integrated Assessment Models (IAMs), prominent tools used to assess the
economic impacts of climate policies. However, traditional IAMs optimize
policies based on a single objective, limiting their ability to capture the
trade-offs among economic growth, temperature goals, and climate justice. As a
result, policy recommendations have been criticized for perpetuating
inequalities, fueling disagreements during policy negotiations. We introduce
Justice, the first framework integrating IAM with Multi-Objective Multi-Agent
Reinforcement Learning (MOMARL). By incorporating multiple objectives, Justice
generates policy recommendations that shed light on equity while balancing
climate and economic goals. Further, using multiple agents can provide a
realistic representation of the interactions among the diverse policy actors.
We identify equitable Pareto-optimal policies using our framework, which
facilitates deliberative decision-making by presenting policymakers with the
inherent trade-offs in climate and economic policy.

</details>


### [72] [Risk Analysis and Design Against Adversarial Actions](https://arxiv.org/abs/2505.01130)
*Marco C. Campi,Algo Carè,Luis G. Crespo,Simone Garatti,Federico A. Ramponi*

Main category: cs.LG

TL;DR: 提出了一种评估模型对抗攻击鲁棒性的通用框架，适用于多种攻击类型和强度，无需额外测试数据。


<details>
  <summary>Details</summary>
Motivation: 解决模型在部署时遇到的对抗行为问题，增强模型信任度并辅助模型选择。

Method: 基于支持向量回归（SVR）的放松优化技术，扩展至广泛学习领域。

Result: 实现了无需额外测试数据的模型脆弱性评估，并在分布无关设置下有效。

Conclusion: 该框架不仅提升模型可信度，还为分布外框架提供了新见解。

Abstract: Learning models capable of providing reliable predictions in the face of
adversarial actions has become a central focus of the machine learning
community in recent years. This challenge arises from observing that data
encountered at deployment time often deviate from the conditions under which
the model was trained. In this paper, we address deployment-time adversarial
actions and propose a versatile, well-principled framework to evaluate the
model's robustness against attacks of diverse types and intensities. While we
initially focus on Support Vector Regression (SVR), the proposed approach
extends naturally to the broad domain of learning via relaxed optimization
techniques. Our results enable an assessment of the model vulnerability without
requiring additional test data and operate in a distribution-free setup. These
results not only provide a tool to enhance trust in the model's applicability
but also aid in selecting among competing alternatives. Later in the paper, we
show that our findings also offer useful insights for establishing new results
within the out-of-distribution framework.

</details>


### [73] [Aggregation of Dependent Expert Distributions in Multimodal Variational Autoencoders](https://arxiv.org/abs/2505.01134)
*Rogelio A Mancisidor,Robert Jenssen,Shujian Yu,Michael Kampffmeyer*

Main category: cs.LG

TL;DR: 论文提出了一种名为CoDE-VAE的新方法，通过依赖专家共识（CoDE）聚合单模态分布，改进了多模态VAE的联合分布估计，提升了生成质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如产品专家和混合专家）假设单模态分布独立，过于理想化，影响了多模态学习的性能。本研究旨在克服这一限制。

Method: 提出了CoDE方法，通过学习各模态子集的贡献，近似多模态数据的联合似然，并设计了新的ELBO目标。

Result: CoDE-VAE在生成质量和一致性之间取得了更好的平衡，生成了更精确的对数似然估计，并减少了随着模态数量增加带来的生成质量下降。

Conclusion: CoDE-VAE在多模态VAE模型中表现优异，其生成质量和分类准确性均达到或接近当前最优水平。

Abstract: Multimodal learning with variational autoencoders (VAEs) requires estimating
joint distributions to evaluate the evidence lower bound (ELBO). Current
methods, the product and mixture of experts, aggregate single-modality
distributions assuming independence for simplicity, which is an overoptimistic
assumption. This research introduces a novel methodology for aggregating
single-modality distributions by exploiting the principle of consensus of
dependent experts (CoDE), which circumvents the aforementioned assumption.
Utilizing the CoDE method, we propose a novel ELBO that approximates the joint
likelihood of the multimodal data by learning the contribution of each subset
of modalities. The resulting CoDE-VAE model demonstrates better performance in
terms of balancing the trade-off between generative coherence and generative
quality, as well as generating more precise log-likelihood estimations.
CoDE-VAE further minimizes the generative quality gap as the number of
modalities increases. In certain cases, it reaches a generative quality similar
to that of unimodal VAEs, which is a desirable property that is lacking in most
current methods. Finally, the classification accuracy achieved by CoDE-VAE is
comparable to that of state-of-the-art multimodal VAE models.

</details>


### [74] [Dual-Forecaster: A Multimodal Time Series Model Integrating Descriptive and Predictive Texts](https://arxiv.org/abs/2505.01135)
*Wenfa Wu,Guanyu Zhang,Zheng Tan,Yi Wang,Hongsheng Qi*

Main category: cs.LG

TL;DR: 论文提出Dual-Forecaster，一种结合历史和预测文本信息的多模态时间序列模型，通过先进的跨模态对齐技术提升性能，实验表明其优于或与现有最优模型相当。


<details>
  <summary>Details</summary>
Motivation: 现有单模态时间序列模型因信息不足受限，多模态模型虽整合文本信息但未能充分利用历史和预测文本的独特贡献，且缺乏对文本与时间序列数据复杂关系的深入理解。

Method: 提出Dual-Forecaster模型，结合历史描述性文本和预测性文本，采用三种跨模态对齐技术增强多模态理解能力。

Result: 在15个多模态时间序列数据集上评估，模型表现优于或与现有最优模型相当。

Conclusion: 整合文本信息为时间序列预测提供了新途径，突出了多模态分析的优越性。

Abstract: Most existing single-modal time series models rely solely on numerical
series, which suffer from the limitations imposed by insufficient information.
Recent studies have revealed that multimodal models can address the core issue
by integrating textual information. However, these models focus on either
historical or future textual information, overlooking the unique contributions
each plays in time series forecasting. Besides, these models fail to grasp the
intricate relationships between textual and time series data, constrained by
their moderate capacity for multimodal comprehension. To tackle these
challenges, we propose Dual-Forecaster, a pioneering multimodal time series
model that combines both descriptively historical textual information and
predictive textual insights, leveraging advanced multimodal comprehension
capability empowered by three well-designed cross-modality alignment
techniques. Our comprehensive evaluations on fifteen multimodal time series
datasets demonstrate that Dual-Forecaster is a distinctly effective multimodal
time series model that outperforms or is comparable to other state-of-the-art
models, highlighting the superiority of integrating textual information for
time series forecasting. This work opens new avenues in the integration of
textual information with numerical time series data for multimodal time series
analysis.

</details>


### [75] [Machine Learning for Physical Simulation Challenge Results and Retrospective Analysis: Power Grid Use Case](https://arxiv.org/abs/2505.01156)
*Milad Leyli-Abadi,Jérôme Picault,Antoine Marot,Jean-Patrick Brunet,Agathe Gilain,Amarsagar Reddy Ramapuram Matavalam,Shaban Ghias Satti,Quingbin Jiang,Yang Liu,Dean Justin Ninalga*

Main category: cs.LG

TL;DR: 论文提出AI驱动方法加速电力潮流模拟以应对可再生能源并网带来的计算挑战，并介绍了LIPS基准框架和ML4PhySim竞赛的优秀成果。


<details>
  <summary>Details</summary>
Motivation: 随着风电、光伏等可再生能源大规模并网，传统物理仿真计算效率不足，需开发更快的AI驱动方法以保证电网实时稳定性。

Method: 采用LIPS基准框架（评估机器学习性能、物理合规性、工业成熟度及泛化能力），组织ML4PhySim竞赛，基于含30%可再生能源的法国电网模型开发快速仿真方案。

Result: 竞赛中表现优异的AI方案比传统方法快至少一个数量级，同时保持可靠性，验证了AI在电力仿真中的潜力。

Conclusion: 研究为高效、可扩展的电网仿真提供了新思路，未来需进一步探索可持续的AI仿真方法。

Abstract: This paper addresses the growing computational challenges of power grid
simulations, particularly with the increasing integration of renewable energy
sources like wind and solar. As grid operators must analyze significantly more
scenarios in near real-time to prevent failures and ensure stability,
traditional physical-based simulations become computationally impractical. To
tackle this, a competition was organized to develop AI-driven methods that
accelerate power flow simulations by at least an order of magnitude while
maintaining operational reliability. This competition utilized a regional-scale
grid model with a 30\% renewable energy mix, mirroring the anticipated
near-future composition of the French power grid. A key contribution of this
work is through the use of LIPS (Learning Industrial Physical Systems), a
benchmarking framework that evaluates solutions based on four critical
dimensions: machine learning performance, physical compliance, industrial
readiness, and generalization to out-of-distribution scenarios. The paper
provides a comprehensive overview of the Machine Learning for Physical
Simulation (ML4PhySim) competition, detailing the benchmark suite, analyzing
top-performing solutions that outperformed traditional simulation methods, and
sharing key organizational insights and best practices for running large-scale
AI competitions. Given the promising results achieved, the study aims to
inspire further research into more efficient, scalable, and sustainable power
network simulation methodologies.

</details>


### [76] [TActiLE: Tiny Active LEarning for wearable devices](https://arxiv.org/abs/2505.01160)
*Massimo Pavan,Claudio Galimberti,Manuel Roveri*

Main category: cs.LG

TL;DR: 论文提出了一种名为TActiLE的新型主动学习算法，专为TinyML场景设计，旨在解决穿戴设备中标记数据稀缺的问题，通过选择最有价值的未标记数据减少用户标注负担，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 穿戴设备的On-device Learning (ODL)面临标记数据稀缺的挑战，用户手动标注大量数据不切实际。研究旨在通过主动学习技术（AL）最小化标注工作量，同时提升模型个性化适配能力。

Method: 提出TActiLE算法，从设备传感器数据流中动态选择对模型优化最有价值的数据子集，结合用户提供的少量标注进行训练，专注于TinyML场景的高效性。

Result: 在多类图像分类数据集上的实验验证了TActiLE的有效性，证实其适合资源受限的微型穿戴设备。

Conclusion: TActiLE作为首个面向TinyML的主动学习算法，显著降低了标注成本，为穿戴设备的个性化模型训练提供了可行解决方案。

Abstract: Tiny Machine Learning (TinyML) algorithms have seen extensive use in recent
years, enabling wearable devices to be not only connected but also genuinely
intelligent by running machine learning (ML) computations directly on-device.
Among such devices, smart glasses have particularly benefited from TinyML
advancements. TinyML facilitates the on-device execution of the inference phase
of ML algorithms on embedded and wearable devices, and more recently, it has
expanded into On-device Learning (ODL), which allows both inference and
learning phases to occur directly on the device. The application of ODL
techniques to wearable devices is particularly compelling, as it enables the
development of more personalized models that adapt based on the data of the
user. However, one of the major challenges of ODL algorithms is the scarcity of
labeled data collected on-device. In smart wearable contexts, requiring users
to manually label large amounts of data is often impractical and could lead to
user disengagement with the technology. To address this issue, this paper
explores the application of Active Learning (AL) techniques, i.e., techniques
that aim at minimizing the labeling effort, by actively selecting from a large
quantity of unlabeled data only a small subset to be labeled and added to the
training set of the algorithm. In particular, we propose TActiLE, a novel AL
algorithm that selects from the stream of on-device sensor data the ones that
would help the ML algorithm improve the most once coupled with labels provided
by the user. TActiLE is the first Active Learning technique specifically
designed for the TinyML context. We evaluate its effectiveness and efficiency
through experiments on multiple image classification datasets. The results
demonstrate its suitability for tiny and wearable devices.

</details>


### [77] [Empirical Comparison of Lightweight Forecasting Models for Seasonal and Non-Seasonal Time Series](https://arxiv.org/abs/2505.01163)
*Thanh Son Nguyen,Dang Minh Duc Nguyen,Van Thanh Nguyen*

Main category: cs.LG

TL;DR: 该研究比较了多项式分类器（PC）和径向基函数神经网络（RBFNN）在四种时间序列数据集上的表现，发现PC在非季节性数据上更优，而RBFNN在季节性数据上表现更好。


<details>
  <summary>Details</summary>
Motivation: 研究旨在比较两种方法在时间序列预测中的表现，以提供高准确性和计算效率的实用指导。

Method: 使用四种真实世界数据集，评估PC和RBFNN的预测准确性和计算时间，并通过统计测试验证结果显著性。

Result: PC在非季节性数据上更准确且快速，RBFNN在季节性数据上表现更优。PC结构更透明。

Conclusion: 根据数据特性选择模型：PC适用于非季节性快速透明预测，RBFNN适合复杂季节性模式。

Abstract: Accurate time series forecasting is essential in many real-time applications
that demand both high predictive accuracy and computational efficiency. This
study provides an empirical comparison between a Polynomial Classifier and a
Radial Basis Function Neural Network (RBFNN) across four real-world time series
datasets (weather conditions, gold prices, crude oil prices, and beer
production volumes) that cover both seasonal and nonseasonal patterns. Model
performance is evaluated by forecasting accuracy (using Mean Absolute Error,
Root Mean Squared Error, and Coefficient of Variation of Root Mean Squared
Error) and computational time to assess each model's viability for real time
forecasting. The results show that the PC yields more accurate and faster
forecasts for non seasonal series, whereas the RBFNN performs better on series
with pronounced seasonal patterns. From an interpretability standpoint, the
polynomial model offers a simpler, more transparent structure (in contrast to
the black box nature of neural network), which is advantageous for
understanding and trust in real time decision making. The performance
differences between PC and RBFNN are statistically significant, as confirmed by
paired t tests and Wilcoxon signed rank tests. These findings provide practical
guidance for model selection in time series forecasting, indicating that PC may
be preferable for quick, interpretable forecasts in non-seasonal contexts,
whereas RBFNN is superior for capturing complex seasonal behaviors

</details>


### [78] [Harmonizing Intra-coherence and Inter-divergence in Ensemble Attacks for Adversarial Transferability](https://arxiv.org/abs/2505.01168)
*Zhaoyang Ma,Zhihao Wu,Wang Lu,Xin Gao,Jinghang Yue,Taolin Zhang,Lipo Wang,Youfang Lin,Jing Wang*

Main category: cs.LG

TL;DR: 论文提出HEAT方法，通过引入域泛化和动态权重分配机制，显著提升了对抗样本的迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有模型集成攻击方法在共享梯度方向捕获和自适应权重分配方面存在不足，影响了对抗样本的迁移性和模型安全性。

Method: HEAT方法包含两大模块：1) 共识梯度方向合成器（使用SVD合成共享梯度方向）；2) 双和谐权重协调器（动态平衡域内一致性和域间多样性）。

Result: 实验表明，HEAT在多种数据集和设置下显著优于现有方法。

Conclusion: HEAT为对抗攻击研究提供了新思路和方向，有效解决了现有技术的关键挑战。

Abstract: The development of model ensemble attacks has significantly improved the
transferability of adversarial examples, but this progress also poses severe
threats to the security of deep neural networks. Existing methods, however,
face two critical challenges: insufficient capture of shared gradient
directions across models and a lack of adaptive weight allocation mechanisms.
To address these issues, we propose a novel method Harmonized Ensemble for
Adversarial Transferability (HEAT), which introduces domain generalization into
adversarial example generation for the first time. HEAT consists of two key
modules: Consensus Gradient Direction Synthesizer, which uses Singular Value
Decomposition to synthesize shared gradient directions; and Dual-Harmony Weight
Orchestrator which dynamically balances intra-domain coherence, stabilizing
gradients within individual models, and inter-domain diversity, enhancing
transferability across models. Experimental results demonstrate that HEAT
significantly outperforms existing methods across various datasets and
settings, offering a new perspective and direction for adversarial attack
research.

</details>


### [79] [Distilling Two-Timed Flow Models by Separately Matching Initial and Terminal Velocities](https://arxiv.org/abs/2505.01169)
*Pramook Khungurn,Pratch Piyawongwisal,Sira Sriswadi,Supasorn Suwajanakorn*

Main category: cs.LG

TL;DR: 该论文提出了一种名为初始/终端速度匹配（ITVM）的新损失函数，用于蒸馏双向时间流模型（TTFM），以改进少步生成的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Lagrangian Flow Map Distillation (LFMD)存在不足，需要通过改进损失函数来提升模型在少步生成任务中的表现。

Method: 设计了ITVM损失函数，通过添加初始速度匹配项、去除终端速度项的导数，以及使用EMA稳定模型来计算目标终端速度。

Result: 初步实验表明，ITVM损失在多种数据集和模型架构上优于基线方法，尤其在少步生成任务中表现更好。

Conclusion: ITVM损失是一种有效的改进方法，能够显著提升双向时间流模型的少步生成性能。

Abstract: A flow matching model learns a time-dependent vector field $v_t(x)$ that
generates a probability path $\{ p_t \}_{0 \leq t \leq 1}$ that interpolates
between a well-known noise distribution ($p_0$) and the data distribution
($p_1$). It can be distilled into a \emph{two-timed flow model} (TTFM)
$\phi_{s,x}(t)$ that can transform a sample belonging to the distribution at an
initial time $s$ to another belonging to the distribution at a terminal time
$t$ in one function evaluation. We present a new loss function for TTFM
distillation called the \emph{initial/terminal velocity matching} (ITVM) loss
that extends the Lagrangian Flow Map Distillation (LFMD) loss proposed by Boffi
et al. by adding redundant terms to match the initial velocities at time $s$,
removing the derivative from the terminal velocity term at time $t$, and using
a version of the model under training, stabilized by exponential moving
averaging (EMA), to compute the target terminal average velocity. Preliminary
experiments show that our loss leads to better few-step generation performance
on multiple types of datasets and model architectures over baselines.

</details>


### [80] [A Secured Triad of IoT, Machine Learning, and Blockchain for Crop Forecasting in Agriculture](https://arxiv.org/abs/2505.01196)
*Najmus Sakib Sizan,Md. Abu Layek,Khondokar Fida Hasan*

Main category: cs.LG

TL;DR: 论文提出了一种结合物联网、机器学习和区块链技术的新方法，用于提高作物预测的准确性，并通过直观的在线界面为农民提供实时和历史数据支持。


<details>
  <summary>Details</summary>
Motivation: 旨在改善作物预测并为农民提供数据驱动的可行见解，同时确保数据的安全性和可靠性。

Method: 整合物联网实时监测环境与土壤数据，使用随机森林模型进行作物类型和产量的高精度预测，并通过以太坊区块链确保数据安全。

Result: 随机森林模型预测准确率达到99.45%，系统提供了精准的作物预测和推荐，数据安全性高，用户体验友好。

Conclusion: 该方法在精准农业中实现了更准确、安全且用户友好的作物预测，具有重要应用价值。

Abstract: To improve crop forecasting and provide farmers with actionable data-driven
insights, we propose a novel approach integrating IoT, machine learning, and
blockchain technologies. Using IoT, real-time data from sensor networks
continuously monitor environmental conditions and soil nutrient levels,
significantly improving our understanding of crop growth dynamics. Our study
demonstrates the exceptional accuracy of the Random Forest model, achieving a
99.45\% accuracy rate in predicting optimal crop types and yields, thereby
offering precise crop projections and customized recommendations. To ensure the
security and integrity of the sensor data used for these forecasts, we
integrate the Ethereum blockchain, which provides a robust and secure platform.
This ensures that the forecasted data remain tamper-proof and reliable.
Stakeholders can access real-time and historical crop projections through an
intuitive online interface, enhancing transparency and facilitating informed
decision-making. By presenting multiple predicted crop scenarios, our system
enables farmers to optimize production strategies effectively. This integrated
approach promises significant advances in precision agriculture, making crop
forecasting more accurate, secure, and user-friendly.

</details>


### [81] [CaReAQA: A Cardiac and Respiratory Audio Question Answering Model for Open-Ended Diagnostic Reasoning](https://arxiv.org/abs/2505.01199)
*Tsai-Ning Wang,Lin-Lin Chen,Neil Zeghidour,Aaqib Saeed*

Main category: cs.LG

TL;DR: CaReAQA结合音频模型与语言模型解决了医学音频信号分析的挑战，引入CaReSound数据集支持研究，评估显示其在开放和封闭任务中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖人工特征或需大量标注数据的监督学习，限制了医学音频信号分析的扩展性和适用性。

Method: 提出CaReAQA音频-语言模型，结合基础音频模型与大语言模型的推理能力，引入CaReSound数据集。

Result: CaReAQA在开放诊断推理任务中准确率86.2%，封闭分类任务中平均56.9%，优于基线。

Conclusion: 音频-语言整合与推理推动了医疗诊断，为临床决策支持提供高效AI系统。

Abstract: Medical audio signals, such as heart and lung sounds, play a crucial role in
clinical diagnosis. However, analyzing these signals remains challenging:
traditional methods rely on handcrafted features or supervised deep learning
models that demand extensive labeled datasets, limiting their scalability and
applicability. To address these issues, we propose CaReAQA, an audio-language
model that integrates a foundation audio model with the reasoning capabilities
of large language models, enabling clinically relevant, open-ended diagnostic
responses. Alongside CaReAQA, we introduce CaReSound, a benchmark dataset of
annotated medical audio recordings enriched with metadata and paired
question-answer examples, intended to drive progress in diagnostic reasoning
research. Evaluation results show that CaReAQA achieves 86.2% accuracy on
open-ended diagnostic reasoning tasks, outperforming baseline models. It also
generalizes well to closed-ended classification tasks, achieving an average
accuracy of 56.9% on unseen datasets. Our findings show how audio-language
integration and reasoning advances medical diagnostics, enabling efficient AI
systems for clinical decision support.

</details>


### [82] [AGRO: An Autonomous AI Rover for Precision Agriculture](https://arxiv.org/abs/2505.01200)
*Simar Ghumman,Fabio Di Troia,William Andreopoulos,Mark Stamp,Sanjit Rai*

Main category: cs.LG

TL;DR: 研究开发了名为AGRO的自主地面巡视车，结合机器学习和传感器技术，用于农业领域的自主导航、数据采集和实时环境建模，旨在自动化资源密集型操作并支持农民的数据驱动决策。


<details>
  <summary>Details</summary>
Motivation: 无人地面车辆（UGV）与机器学习的结合为复杂农业问题提供了解决方案，研究旨在通过自动化技术减轻农民的负担并提高决策效率。

Method: AGRO利用机器学习、计算机视觉和多种传感器技术，实现自主导航、实时环境建模、定位和障碍物避让，以捕捉数据并估算作物产量。

Result: AGRO成功实现了自主巡视、数据采集和环境建模，为农民提供了数据支持，同时也为更高级的机器学习技术奠定了基础。

Conclusion: AGRO展示了无人地面车辆在精准农业中的潜力，通过自动化技术提高效率和决策质量，并为未来的机器学习应用提供了平台。

Abstract: Unmanned Ground Vehicles (UGVs) are emerging as a crucial tool in the world
of precision agriculture. The combination of UGVs with machine learning allows
us to find solutions for a range of complex agricultural problems. This
research focuses on developing a UGV capable of autonomously traversing
agricultural fields and capturing data. The project, known as AGRO (Autonomous
Ground Rover Observer) leverages machine learning, computer vision and other
sensor technologies. AGRO uses its capabilities to determine pistachio yields,
performing self-localization and real-time environmental mapping while avoiding
obstacles. The main objective of this research work is to automate
resource-consuming operations so that AGRO can support farmers in making
data-driven decisions. Furthermore, AGRO provides a foundation for advanced
machine learning techniques as it captures the world around it.

</details>


### [83] [Quantitative Attractor Analysis of High-Capacity Kernel Logistic Regression Hopfield Networks](https://arxiv.org/abs/2505.01218)
*Akira Tamamori*

Main category: cs.LG

TL;DR: 该论文通过模拟分析KLR训练的Hopfield网络的吸引子结构，证明了其高容量（4.0 P/N）和鲁棒性，几乎没有虚假固定点。


<details>
  <summary>Details</summary>
Motivation: 传统Hopfield网络因Hebbian学习存在存储容量限制（0.14 P/N）和虚假吸引子问题，KLR提供了一种非线性映射方法以提高性能。

Method: 通过大量模拟，从不同初始状态评估网络的召回能力、收敛速度和抗噪性，量化KLR网络的表现。

Result: KLR网络表现出高存储容量（4.0 P/N）、极低虚假吸引子，且收敛速度快（1-2步）。

Conclusion: KLR网络通过重塑动态特性实现了高容量联想记忆，为AM领域提供了新理解。

Abstract: Traditional Hopfield networks, using Hebbian learning, face severe storage
capacity limits ($\approx 0.14$ P/N) and spurious attractors. Kernel Logistic
Regression (KLR) offers a non-linear approach, mapping patterns to
high-dimensional feature spaces for improved separability. Our previous work
showed KLR dramatically improves capacity and noise robustness over
conventional methods. This paper quantitatively analyzes the attractor
structures in KLR-trained networks via extensive simulations. We evaluated
recall from diverse initial states across wide storage loads (up to 4.0 P/N)
and noise levels. We quantified convergence rates and speed. Our analysis
confirms KLR's superior performance: high capacity (up to 4.0 P/N) and
robustness. The attractor landscape is remarkably "clean," with near-zero
spurious fixed points. Recall failures under high load/noise are primarily due
to convergence to other learned patterns, not spurious ones. Dynamics are
exceptionally fast (typically 1-2 steps for high-similarity states). This
characterization reveals how KLR reshapes dynamics for high-capacity
associative memory, highlighting its effectiveness and contributing to AM
understanding.

</details>


### [84] [mwBTFreddy: A Dataset for Flash Flood Damage Assessment in Urban Malawi](https://arxiv.org/abs/2505.01242)
*Evelyn Chapuma,Grey Mengezi,Lewis Msasa,Amelia Taylor*

Main category: cs.LG

TL;DR: 论文介绍了mwBTFreddy数据集，用于支持马拉维城市洪水灾害评估，重点关注2023年飓风Freddy的影响。数据包含灾前灾后卫星图像和标注的建筑损害等级，旨在开发非洲城市背景下的机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 解决非洲城市洪水灾害评估中缺乏高质量数据集的问题，支持灾害响应和基础设施规划。

Method: 使用Google Earth Pro获取灾前灾后卫星图像，并标注建筑损害等级（无损伤、轻微、严重或毁坏）和地理坐标。

Result: 开发了mwBTFreddy数据集，支持机器学习模型训练和洪水损害可视化分析。

Conclusion: 该数据集为非洲城市洪水灾害评估提供了实用工具，有助于灾害响应和气候脆弱地区的规划。

Abstract: This paper describes the mwBTFreddy dataset, a resource developed to support
flash flood damage assessment in urban Malawi, specifically focusing on the
impacts of Cyclone Freddy in 2023. The dataset comprises paired pre- and
post-disaster satellite images sourced from Google Earth Pro, accompanied by
JSON files containing labelled building annotations with geographic coordinates
and damage levels (no damage, minor, major, or destroyed). Developed by the
Kuyesera AI Lab at the Malawi University of Business and Applied Sciences, this
dataset is intended to facilitate the development of machine learning models
tailored to building detection and damage classification in African urban
contexts. It also supports flood damage visualisation and spatial analysis to
inform decisions on relocation, infrastructure planning, and emergency response
in climate-vulnerable regions.

</details>


### [85] [Enhancing Obsolescence Forecasting with Deep Generative Data Augmentation: A Semi-Supervised Framework for Low-Data Industrial Applications](https://arxiv.org/abs/2505.01261)
*Elie Saad,Mariem Besbes,Marc Zolghadri,Victor Czmil,Claude Baron,Vincent Bourgeois*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度学习的电子元件淘汰预测新框架，通过深度生成模型生成新数据以解决数据不足问题，并在基准数据集上取得最佳效果。


<details>
  <summary>Details</summary>
Motivation: 电子元件淘汰问题在长生命周期系统中尤为关键，现有机器学习方法需要大量数据，但现实中数据不足，因此提出新框架以解决这一问题。

Method: 使用深度生成模型生成新的淘汰案例以扩充训练数据集，然后训练经典的机器学习预测模型，并将现有监督学习分类器调整为半监督学习。

Result: 该框架在基准数据集上实现了最先进的预测结果。

Conclusion: 通过深度生成模型解决数据不足问题，结合半监督学习，显著提升了电子元件淘汰预测的准确性。

Abstract: The challenge of electronic component obsolescence is particularly critical
in systems with long life cycles. Various obsolescence management methods are
employed to mitigate its impact, with obsolescence forecasting being a highly
sought-after and prominent approach. As a result, numerous machine
learning-based forecasting methods have been proposed. However, machine
learning models require a substantial amount of relevant data to achieve high
precision, which is lacking in the current obsolescence landscape in some
situations. This work introduces a novel framework for obsolescence forecasting
based on deep learning. The proposed framework solves the lack of available
data through deep generative modeling, where new obsolescence cases are
generated and used to augment the training dataset. The augmented dataset is
then used to train a classical machine learning-based obsolescence forecasting
model. To train classical forecasting models using augmented datasets, existing
classical supervised-learning classifiers are adapted for semi-supervised
learning within this framework. The proposed framework demonstrates
state-of-the-art results on benchmarking datasets.

</details>


### [86] [MultiGran-STGCNFog: Towards Accurate and High-Throughput Inference for Multi-Granular Spatiotemporal Traffic Forecasting](https://arxiv.org/abs/2505.01279)
*Zhaoyan Wang,Xiangchi Song,In-Young Ko*

Main category: cs.LG

TL;DR: 本文提出了一种名为MultiGran-STGCNFog的高效雾分布式推理系统，通过多粒度时空特征融合和改进的调度算法GA-DPHDS，显著提升了交通预测的准确性和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于图卷积网络的方法无法充分提取和融合多粒度时空特征，导致预测准确性不足；同时，额外特征提取分支增加了模型复杂性和推理时间，难以实现快速推理。

Method: 提出了MultiGran-STGCNFog系统，采用多粒度时空特征融合的动态交通图模型，并设计了GA-DPHDS调度算法优化层执行顺序和设备调度方案。

Result: 在真实数据集上的实验表明，该方法在预测准确性和推理速度上均优于所选基线模型。

Conclusion: MultiGran-STGCNFog通过多粒度特征融合和高效调度，有效解决了交通预测中的准确性与速度问题，具有实际应用潜力。

Abstract: Accurate traffic forecasting and swift inference provision are essential for
intelligent transportation systems. However, the present Graph Convolutional
Network (GCN)-based approaches cannot extract and fuse multi-granular
spatiotemporal features across various spatial and temporal scales
sufficiently, proven to yield less accurate forecasts. Besides, additional
feature extraction branches introduced in prior studies critically increased
model complexity and extended inference time, making it challenging to provide
fast inference for traffic forecasting. In this paper, we propose
MultiGran-STGCNFog, an efficient fog distributed inference system with a novel
traffic forecasting model that employs multi-granular spatiotemporal feature
fusion on generated dynamic traffic graphs to fully capture interdependent
traffic dynamics. The proposed scheduling algorithm GA-DPHDS, optimizing layer
execution order and layer-device scheduling scheme simultaneously, contributes
to considerable inference throughput improvement by leveraging heterogeneous
fog devices in a pipelined manner. Extensive experiments on real-world datasets
demonstrate the superiority of the proposed method over selected baselines.

</details>


### [87] [A Physics-preserved Transfer Learning Method for Differential Equations](https://arxiv.org/abs/2505.01281)
*Hao-Ran Yang,Chuan-Xian Ren*

Main category: cs.LG

TL;DR: 该论文提出了一种物理保持最优张量传输（POTT）方法，用于解决数据驱动方法在微分方程问题中的领域迁移问题。该方法能自适应纠正领域偏移并保持物理信息，实验证明其具有优越性能、泛化能力和物理保持性。


<details>
  <summary>Details</summary>
Motivation: 数据驱动方法（如神经算子）在求解微分方程方面取得了成功，但在不同学习环境（数据偏差或方程变化）下存在领域迁移问题。现有迁移学习方法要么缺乏对一般微分方程问题的泛化能力，要么在训练过程中无法保持物理信息。因此，该工作旨在提出一种既具泛化性又能保持物理信息的通用迁移学习方法。

Method: 作者将数据领域表征为乘积分布，将核心问题归结为分布偏差和算子偏差。提出了物理保持最优张量传输（POTT）方法，利用POTT映射的推前分布来适应目标领域的数据驱动模型，从而解决领域迁移问题。

Result: 大量实验表明，POTT方法在性能、泛化能力和物理保持性方面均表现优越。

Conclusion: POTT方法成功解决了数据驱动方法在微分方程问题中的领域迁移问题，同时保持了物理信息和泛化能力，为相关研究提供了有效工具。

Abstract: While data-driven methods such as neural operator have achieved great success
in solving differential equations (DEs), they suffer from domain shift problems
caused by different learning environments (with data bias or equation changes),
which can be alleviated by transfer learning (TL). However, existing TL methods
adopted in DEs problems lack either generalizability in general DEs problems or
physics preservation during training. In this work, we focus on a general
transfer learning method that adaptively correct the domain shift and preserve
physical information. Mathematically, we characterize the data domain as
product distribution and the essential problems as distribution bias and
operator bias. A Physics-preserved Optimal Tensor Transport (POTT) method that
simultaneously admits generalizability to common DEs and physics preservation
of specific problem is proposed to adapt the data-driven model to target domain
utilizing the push-forward distribution induced by the POTT map. Extensive
experiments demonstrate the superior performance, generalizability and physics
preservation of the proposed POTT method.

</details>


### [88] [2DXformer: Dual Transformers for Wind Power Forecasting with Dual Exogenous Variables](https://arxiv.org/abs/2505.01286)
*Yajuan Zhang,Jiahai Jiang,Yule Yan,Liang Yang,Ping Zhang*

Main category: cs.LG

TL;DR: 论文提出了2DXformer模型，通过分类输入变量并利用注意力机制和多层感知机，改进了风能预测的准确性和模型复杂性。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习的风能预测方法虽在时空相关性提取上取得进步，但在变量间关系建模和内外变量处理上存在局限。

Method: 分类输入为外生静态、外生动态和内生变量，独立嵌入为变量token，用注意力机制捕捉外生变量相关性，多层感知机建模外生对内生的影响。

Result: 在两个真实世界的大型数据集上，2DXformer进一步提升了风能预测的性能。

Conclusion: 2DXformer通过改进变量建模和处理方法，显著提高了风能预测的准确性和模型效率。

Abstract: Accurate wind power forecasting can help formulate scientific dispatch plans,
which is of great significance for maintaining the safety, stability, and
efficient operation of the power system. In recent years, wind power
forecasting methods based on deep learning have focused on extracting the
spatiotemporal correlations among data, achieving significant improvements in
forecasting accuracy. However, they exhibit two limitations. First, there is a
lack of modeling for the inter-variable relationships, which limits the
accuracy of the forecasts. Second, by treating endogenous and exogenous
variables equally, it leads to unnecessary interactions between the endogenous
and exogenous variables, increasing the complexity of the model. In this paper,
we propose the 2DXformer, which, building upon the previous work's focus on
spatiotemporal correlations, addresses the aforementioned two limitations.
Specifically, we classify the inputs of the model into three types: exogenous
static variables, exogenous dynamic variables, and endogenous variables. First,
we embed these variables as variable tokens in a channel-independent manner.
Then, we use the attention mechanism to capture the correlations among
exogenous variables. Finally, we employ a multi-layer perceptron with residual
connections to model the impact of exogenous variables on endogenous variables.
Experimental results on two real-world large-scale datasets indicate that our
proposed 2DXformer can further improve the performance of wind power
forecasting. The code is available in this repository:
\href{https://github.com/jseaj/2DXformer}{https://github.com/jseaj/2DXformer}.

</details>


### [89] [Integration of Multi-Mode Preference into Home Energy Management System Using Deep Reinforcement Learning](https://arxiv.org/abs/2505.01332)
*Mohammed Sumayli,Olugbenga Moses Anubi*

Main category: cs.LG

TL;DR: 该论文提出了一种基于深度强化学习的家庭能源管理系统（DRL-HEMS），通过动态多模式偏好优化能源使用，提升用户舒适度和参与度，同时在计算效率上优于传统混合整数线性规划（MILP）方法。


<details>
  <summary>Details</summary>
Motivation: 现有家庭能源管理系统（HEMS）通常使用静态权重因子来考虑用户舒适度，忽略了用户行为和偏好的动态性。为了解决这一局限性，研究提出了一种动态、用户友好的优化框架。

Method: 研究采用无模型的单智能体深度强化学习（DRL）算法，设计了一个多模式HEMS框架，利用实时数据（如电价、环境温度和电器功耗）进行动态优化。

Result: 实验结果表明，该模型在不同偏好模式下能高效优化能源消耗，并且在计算效率上显著优于传统的MILP算法。

Conclusion: DRL-HEMS框架不仅实现了近乎最优的性能，还提升了用户参与度和舒适度，为智能家居能源管理提供了新的解决方案。

Abstract: Home Energy Management Systems (HEMS) have emerged as a pivotal tool in the
smart home ecosystem, aiming to enhance energy efficiency, reduce costs, and
improve user comfort. By enabling intelligent control and optimization of
household energy consumption, HEMS plays a significant role in bridging the gap
between consumer needs and energy utility objectives. However, much of the
existing literature construes consumer comfort as a mere deviation from the
standard appliance settings. Such deviations are typically incorporated into
optimization objectives via static weighting factors. These factors often
overlook the dynamic nature of consumer behaviors and preferences. Addressing
this oversight, our paper introduces a multi-mode Deep Reinforcement
Learning-based HEMS (DRL-HEMS) framework, meticulously designed to optimize
based on dynamic, consumer-defined preferences. Our primary goal is to augment
consumer involvement in Demand Response (DR) programs by embedding dynamic
multi-mode preferences tailored to individual appliances. In this study, we
leverage a model-free, single-agent DRL algorithm to deliver a HEMS framework
that is not only dynamic but also user-friendly. To validate its efficacy, we
employed real-world data at 15-minute intervals, including metrics such as
electricity price, ambient temperature, and appliances' power consumption. Our
results show that the model performs exceptionally well in optimizing energy
consumption within different preference modes. Furthermore, when compared to
traditional algorithms based on Mixed-Integer Linear Programming (MILP), our
model achieves nearly optimal performance while outperforming in computational
efficiency.

</details>


### [90] [Enhancing Diversity in Parallel Agents: A Maximum State Entropy Exploration Story](https://arxiv.org/abs/2505.01336)
*Vincenzo De Paola,Riccardo Zamboni,Mirco Mutti,Marcello Restelli*

Main category: cs.LG

TL;DR: 该论文提出了一种新颖的并行强化学习框架，通过最大化数据熵和平衡个体多样性的方法，超越了传统N倍加速的限制。


<details>
  <summary>Details</summary>
Motivation: 并行数据收集在强化学习中实现了N倍的加速，但作者质疑是否通过个性化并行代理的策略可以超越N倍的加速。

Method: 论文引入了一个集中式策略梯度方法，以最大化数据熵并减少冗余，同时结合批处理强化学习技术。

Result: 实验表明，该方法在相同代理系统和数据多样性方面表现出色，且理论分析支持了并行采样分布的更快收敛速率。

Conclusion: 该框架不仅提高了数据收集效率，还为并行强化学习提供了新的理论基础。

Abstract: Parallel data collection has redefined Reinforcement Learning (RL), unlocking
unprecedented efficiency and powering breakthroughs in large-scale real-world
applications. In this paradigm, $N$ identical agents operate in $N$ replicas of
an environment simulator, accelerating data collection by a factor of $N$. A
critical question arises: \textit{Does specializing the policies of the
parallel agents hold the key to surpass the $N$ factor acceleration?} In this
paper, we introduce a novel learning framework that maximizes the entropy of
collected data in a parallel setting. Our approach carefully balances the
entropy of individual agents with inter-agent diversity, effectively minimizing
redundancies. The latter idea is implemented with a centralized policy gradient
method, which shows promise when evaluated empirically against systems of
identical agents, as well as synergy with batch RL techniques that can exploit
data diversity. Finally, we provide an original concentration analysis that
shows faster rates for specialized parallel sampling distributions, which
supports our methodology and may be of independent interest.

</details>


### [91] [How to Learn a Star: Binary Classification with Starshaped Polyhedral Sets](https://arxiv.org/abs/2505.01346)
*Marie-Charlotte Brandenburg,Katharina Jochemko*

Main category: cs.LG

TL;DR: 该论文研究了基于星形多面体决策边界的连续分段线性函数的二元分类问题，分析了其表达能力、损失景观的几何结构（尤其是子水平集），并给出了VC维的明确界限以及对离散损失和指数损失的具体描述。


<details>
  <summary>Details</summary>
Motivation: 探讨连续分段线性函数在二元分类中的表达能力，特别关注非凸星形多面体决策边界，并分析其损失景观的几何性质。

Method: 使用星形多面体支持的固定多面体单纯扇形类，研究了0/1损失和指数损失函数的子水平集结构，并利用超平面排列的腔室理论描述离散损失的子水平集。

Result: 给出了该模型VC维的明确界限，证明了指数损失在某些条件下最优解的唯一性，并分析了最优解的几何性质随指数分布参数的变化。

Conclusion: 该模型在二元分类中具有明确的几何和组合结构，尤其是在离散和指数损失下展现出可解释的优化特性。

Abstract: We consider binary classification restricted to a class of continuous
piecewise linear functions whose decision boundaries are (possibly nonconvex)
starshaped polyhedral sets, supported on a fixed polyhedral simplicial fan. We
investigate the expressivity of these function classes and describe the
combinatorial and geometric structure of the loss landscape, most prominently
the sublevel sets, for two loss-functions: the 0/1-loss (discrete loss) and an
exponential loss function. In particular, we give explicit bounds on the VC
dimension of this model, and concretely describe the sublevel sets of the
discrete loss as chambers in a hyperplane arrangement. For the exponential
loss, we give sufficient conditions for the optimum to be unique, and describe
the geometry of the optimum when varying the rate parameter of the underlying
exponential probability distribution.

</details>


### [92] [Learning Stabilizing Policies via an Unstable Subspace Representation](https://arxiv.org/abs/2505.01348)
*Leonardo F. Toso,Lintao Ye,James Anderson*

Main category: cs.LG

TL;DR: 论文提出了一种两阶段方法，通过先学习系统的左不稳定子空间，再在其上解决折扣线性二次调节器问题，从而降低了控制空间的有效维度，显著提升了稳定化过程的样本效率。


<details>
  <summary>Details</summary>
Motivation: 稳定线性时不变系统是控制领域的基础问题，但现有方法通常需要大量数据且计算复杂度高。本文旨在通过聚焦系统的不稳定动力学部分，降低样本复杂度。

Method: 采用两阶段方法：1）学习系统的左不稳定子空间；2）在该子空间上解决系列折扣LQR问题，仅稳定不稳定动力学部分。

Result: 理论分析和数值实验表明，当不稳定模态远小于状态维度时，该方法显著降低了样本复杂度。

Conclusion: 该方法通过有效降维和聚焦不稳定部分，提升了稳定化过程的效率，特别适合高维系统中不稳定模态较少的情况。

Abstract: We study the problem of learning to stabilize (LTS) a linear time-invariant
(LTI) system. Policy gradient (PG) methods for control assume access to an
initial stabilizing policy. However, designing such a policy for an unknown
system is one of the most fundamental problems in control, and it may be as
hard as learning the optimal policy itself. Existing work on the LTS problem
requires large data as it scales quadratically with the ambient dimension. We
propose a two-phase approach that first learns the left unstable subspace of
the system and then solves a series of discounted linear quadratic regulator
(LQR) problems on the learned unstable subspace, targeting to stabilize only
the system's unstable dynamics and reduce the effective dimension of the
control space. We provide non-asymptotic guarantees for both phases and
demonstrate that operating on the unstable subspace reduces sample complexity.
In particular, when the number of unstable modes is much smaller than the state
dimension, our analysis reveals that LTS on the unstable subspace substantially
speeds up the stabilization process. Numerical experiments are provided to
support this sample complexity reduction achieved by our approach.

</details>


### [93] [Stabilizing Temporal Difference Learning via Implicit Stochastic Approximation](https://arxiv.org/abs/2505.01361)
*Hwanwoo Kim,Panos Toulis,Eric Laber*

Main category: cs.LG

TL;DR: 提出了隐式TD算法，通过将TD更新重构为固定点方程，解决了传统TD学习对步长敏感的问题，提升了稳定性和收敛性。


<details>
  <summary>Details</summary>
Motivation: 传统TD学习对步长选择敏感，导致估计误差增大或收敛速度慢，需要通过试错调整步长，效率低下。

Method: 提出隐式TD算法，将TD更新转换为固定点方程，减少对步长的依赖，保持计算效率。

Result: 隐式TD算法在稳定性和收敛性上表现更好，理论分析证明了其渐近收敛和有限时间误差界。

Conclusion: 隐式TD算法是一种更稳健、实用的策略评估和值近似工具。

Abstract: Temporal Difference (TD) learning is a foundational algorithm in
reinforcement learning (RL). For nearly forty years, TD learning has served as
a workhorse for applied RL as well as a building block for more complex and
specialized algorithms. However, despite its widespread use, it is not without
drawbacks, the most prominent being its sensitivity to step size. A poor choice
of step size can dramatically inflate the error of value estimates and slow
convergence. Consequently, in practice, researchers must use trial and error in
order to identify a suitable step size -- a process that can be tedious and
time consuming. As an alternative, we propose implicit TD algorithms that
reformulate TD updates into fixed-point equations. These updates are more
stable and less sensitive to step size without sacrificing computational
efficiency. Moreover, our theoretical analysis establishes asymptotic
convergence guarantees and finite-time error bounds. Our results demonstrate
their robustness and practicality for modern RL tasks, establishing implicit TD
as a versatile tool for policy evaluation and value approximation.

</details>


### [94] [Carbon Aware Transformers Through Joint Model-Hardware Optimization](https://arxiv.org/abs/2505.01386)
*Irene Wang,Newsha Ardalani,Mostafa Elhoushi,Daniel Jiang,Samuel Hsia,Ekin Sumbul,Divya Mahajan,Carole-Jean Wu,Bilge Acun*

Main category: cs.LG

TL;DR: 论文提出了一种碳感知架构搜索框架CATransformers，用于全面优化机器学习系统的碳足迹，涵盖运营碳和硬件制造碳。通过应用于多模态CLIP模型，实现了碳排放减少17%的目标。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统的快速增长导致对环境影响（尤其是碳足迹）的关注增加，但目前缺乏全面评估和优化碳排放的工具。

Method: 提出了CATransformers框架，结合运营碳和硬件制造碳的指标，在硬件加速器早期设计空间探索中进行碳感知优化。

Result: 应用框架到CLIP模型，生成CarbonCLIP系列，碳排放减少17%，同时保持准确性和延迟。

Conclusion: 需要采用全面的优化方法来设计高性能且环保的AI系统。

Abstract: The rapid growth of machine learning (ML) systems necessitates a more
comprehensive evaluation of their environmental impact, particularly their
carbon footprint, which comprises operational carbon from training and
inference execution and embodied carbon from hardware manufacturing and its
entire life-cycle. Despite the increasing importance of embodied emissions,
there is a lack of tools and frameworks to holistically quantify and optimize
the total carbon footprint of ML systems. To address this, we propose
CATransformers, a carbon-aware architecture search framework that enables
sustainability-driven co-optimization of ML models and hardware architectures.
By incorporating both operational and embodied carbon metrics into early design
space exploration of domain-specific hardware accelerators, CATransformers
demonstrates that optimizing for carbon yields design choices distinct from
those optimized solely for latency or energy efficiency. We apply our framework
to multi-modal CLIP-based models, producing CarbonCLIP, a family of CLIP models
achieving up to 17% reduction in total carbon emissions while maintaining
accuracy and latency compared to state-of-the-art edge small CLIP baselines.
This work underscores the need for holistic optimization methods to design
high-performance, environmentally sustainable AI systems.

</details>


### [95] [Learning and Transferring Physical Models through Derivatives](https://arxiv.org/abs/2505.01391)
*Alessandro Trenta,Andrea Cossu,Davide Bacciu*

Main category: cs.LG

TL;DR: DERL 是一种监督方法，通过建模物理系统的偏导数来学习系统动力学，并通过蒸馏协议实现增量建模，具有理论保证和优越的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 目标是构建符合物理定律且可增量学习的物理模型，解决现有方法在泛化和知识迁移上的不足。

Method: 提出 Derivative Learning (DERL)，通过偏导数建模物理系统，并设计蒸馏协议实现增量学习。

Result: DERL 在 ODE 和 PDE 泛化任务中超越现有方法，并能跨模型迁移物理知识。

Conclusion: DERL 首次实现了多阶段增量构建物理模型，为物理建模提供了新思路。

Abstract: We propose Derivative Learning (DERL), a supervised approach that models
physical systems by learning their partial derivatives. We also leverage DERL
to build physical models incrementally, by designing a distillation protocol
that effectively transfers knowledge from a pre-trained to a student model. We
provide theoretical guarantees that our approach can learn the true physical
system, being consistent with the underlying physical laws, even when using
empirical derivatives. DERL outperforms state-of-the-art methods in
generalizing an ODE to unseen initial conditions and a parametric PDE to unseen
parameters. We finally propose a method based on DERL to transfer physical
knowledge across models by extending them to new portions of the physical
domain and new range of PDE parameters. We believe this is the first attempt at
building physical models incrementally in multiple stages.

</details>


### [96] [Predicting the Price of Gold in the Financial Markets Using Hybrid Models](https://arxiv.org/abs/2505.01402)
*Mohammadhossein Rashidi,Mohammad Modarres*

Main category: cs.LG

TL;DR: 该论文提出了一种结合ARIMA、逐步回归和神经网络的混合模型（ARIMA_Stepwise Regression_Neural Network），用于预测黄金价格，并在与其他时间序列方法比较中显示出更高的准确性。


<details>
  <summary>Details</summary>
Motivation: 预测价格误差最小且准确性最高是资本市场活动者和研究者关注的挑战性问题，因此需要一种高精度模型来解决这一问题。

Method: 通过ARIMA时间序列模型预测价格，结合技术分析变量和交易者心理因素，使用逐步回归筛选最佳变量，并将这些变量输入人工神经网络进行最终预测。

Result: 混合模型在预测黄金价格时比其他时间序列方法（如回归和逐步回归）具有更高的准确性，且适用于其他金融市场的预测任务。

Conclusion: 提出的ARIMA_逐步回归_神经网络混合模型在金融市场价格预测中表现优越，适用于股票、商品、货币对等多种金融工具的预测。

Abstract: Predicting the price that has the least error and can provide the best and
highest accuracy has been one of the most challenging issues and one of the
most critical concerns among capital market activists and researchers.
Therefore, a model that can solve problems and provide results with high
accuracy is one of the topics of interest among researchers. In this project,
using time series prediction models such as ARIMA to estimate the price,
variables, and indicators related to technical analysis show the behavior of
traders involved in involving psychological factors for the model. By linking
all of these variables to stepwise regression, we identify the best variables
influencing the prediction of the variable. Finally, we enter the selected
variables as inputs to the artificial neural network. In other words, we want
to call this whole prediction process the "ARIMA_Stepwise Regression_Neural
Network" model and try to predict the price of gold in international financial
markets. This approach is expected to be able to be used to predict the types
of stocks, commodities, currency pairs, financial market indicators, and other
items used in local and international financial markets. Moreover, a comparison
between the results of this method and time series methods is also expressed.
Finally, based on the results, it can be seen that the resulting hybrid model
has the highest accuracy compared to the time series method, regression, and
stepwise regression.

</details>


### [97] [How Effective are Large Time Series Models in Hydrology? A Study on Water Level Forecasting in Everglades](https://arxiv.org/abs/2505.01415)
*Rahuul Rangaraj,Jimeng Shi,Azam Shirali,Rajendra Paudel,Yanzhao Wu,Giri Narasimhan*

Main category: cs.LG

TL;DR: 传统的水位预测方法在计算成本和适应性上存在局限，本研究探索了大时间序列模型在Everglades水位预测中的应用，发现基础模型Chronos表现最优。


<details>
  <summary>Details</summary>
Motivation: 解决传统物理和统计方法在高成本和适应性上的不足，探索大时间序列模型在环境关键系统中的应用潜力。

Method: 比较了12种任务特定模型和5种时间序列基础模型，共6类模型，用于Everglades水位预测。

Result: 基础模型Chronos显著优于其他模型，其余基础模型表现较差，任务特定模型性能因架构而异。

Conclusion: Chronos在Everglades水位预测中表现最佳，模型性能差异可能与架构相关，为环境系统预测提供了新思路。

Abstract: The Everglades play a crucial role in flood and drought regulation, water
resource planning, and ecosystem management in the surrounding regions.
However, traditional physics-based and statistical methods for predicting water
levels often face significant challenges, including high computational costs
and limited adaptability to diverse or unforeseen conditions. Recent
advancements in large time series models have demonstrated the potential to
address these limitations, with state-of-the-art deep learning and foundation
models achieving remarkable success in time series forecasting across various
domains. Despite this progress, their application to critical environmental
systems, such as the Everglades, remains underexplored. In this study, we fill
the gap by investigating twelve task-specific models and five time series
foundation models across six categories for a real-world application focused on
water level prediction in the Everglades. Our primary results show that the
foundation model, Chronos, significantly outperforms all other models while the
remaining foundation models exhibit relatively poor performance. Moreover, the
performance of task-specific models varies with the model architectures.
Lastly, we discuss the possible reasons for the varying performance of models.

</details>


### [98] [Evaluating Frontier Models for Stealth and Situational Awareness](https://arxiv.org/abs/2505.01420)
*Mary Phuong,Roland S. Zimmermann,Ziyue Wang,David Lindner,Victoria Krakovna,Sarah Cogan,Allan Dafoe,Lewis Ho,Rohin Shah*

Main category: cs.LG

TL;DR: 论文提出了两项评估AI模型是否具备潜在“谋划”行为的能力：隐蔽性和情境意识。通过测试显示当前前沿模型均未表现出这些危险能力。


<details>
  <summary>Details</summary>
Motivation: 前沿AI模型可能暗中追求与开发者意图不一致的目标（谋划行为），这种行为的隐蔽性使其难以检测，若存在于未来高级系统中可能导致失控风险。因此，开发部署前需排除此类威胁。

Method: 设计了两类评估：1. 五项关于绕过监管的隐蔽性能力测试；2. 十一项关于模型对自身、环境及部署的情境意识推理能力测试。

Result: 当前前沿模型在两项评估中均未表现出高水平的隐蔽性或情境意识，表明其几乎不可能通过谋划行为造成严重危害。

Conclusion: 该评估方法可作为“无谋划能力”安全性论证工具，帮助开发者在部署前验证模型的可靠性，目前测试结果乐观。

Abstract: Recent work has demonstrated the plausibility of frontier AI models scheming
-- knowingly and covertly pursuing an objective misaligned with its developer's
intentions. Such behavior could be very hard to detect, and if present in
future advanced systems, could pose severe loss of control risk. It is
therefore important for AI developers to rule out harm from scheming prior to
model deployment. In this paper, we present a suite of scheming reasoning
evaluations measuring two types of reasoning capabilities that we believe are
prerequisites for successful scheming: First, we propose five evaluations of
ability to reason about and circumvent oversight (stealth). Second, we present
eleven evaluations for measuring a model's ability to instrumentally reason
about itself, its environment and its deployment (situational awareness). We
demonstrate how these evaluations can be used as part of a scheming inability
safety case: a model that does not succeed on these evaluations is almost
certainly incapable of causing severe harm via scheming in real deployment. We
run our evaluations on current frontier models and find that none of them show
concerning levels of either situational awareness or stealth.

</details>


### [99] [Computational, Data-Driven, and Physics-Informed Machine Learning Approaches for Microstructure Modeling in Metal Additive Manufacturing](https://arxiv.org/abs/2505.01424)
*D. Patel,R. Sharma,Y. B. Guo*

Main category: cs.LG

TL;DR: 该论文探讨了金属增材制造中非平衡微观结构的预测挑战，比较了传统实验、计算模拟和数据驱动方法的优劣，提出了物理信息机器学习（PIML）作为解决方案。


<details>
  <summary>Details</summary>
Motivation: 金属增材制造的快速熔融和凝固动态导致不均匀的微观结构，影响性能，但现有方法（如实验和物理模拟）存在局限性，需要物理一致且高效预测的新方法。

Method: 通过综合分析传统实验、计算模拟和数据驱动方法的优缺点，提出并评估了物理信息机器学习（PIML）框架，将物理规律嵌入神经网络。

Result: PIML框架在准确性、透明度、数据效率和可扩展性方面表现优异，为微观结构建模提供了物理一致的解决方案。

Conclusion: PIML混合方法在金属增材制造中具有潜力，能实现可预测、物理一致的微观结构建模，支持高性能组件的可靠生产。

Abstract: Metal additive manufacturing enables unprecedented design freedom and the
production of customized, complex components. However, the rapid melting and
solidification dynamics inherent to metal AM processes generate heterogeneous,
non-equilibrium microstructures that significantly impact mechanical properties
and subsequent functionality. Predicting microstructure and its evolution
across spatial and temporal scales remains a central challenge for process
optimization and defect mitigation. While conventional experimental techniques
and physics-based simulations provide a physical foundation and valuable
insights, they face critical limitations. In contrast, data-driven machine
learning offers an alternative prediction approach and powerful pattern
recognition but often operate as black-box, lacking generalizability and
physical consistency. To overcome these limitations, physics-informed machine
learning, including physics-informed neural networks, has emerged as a
promising paradigm by embedding governing physical laws into neural network
architectures, thereby enhancing accuracy, transparency, data efficiency, and
extrapolation capabilities. This work presents a comprehensive evaluation of
modeling strategies for microstructure prediction in metal AM. The strengths
and limitations of experimental, computational, and data-driven methods are
analyzed in depth, and highlight recent advances in hybrid PIML frameworks that
integrate physical knowledge with ML. Key challenges, such as data scarcity,
multi-scale coupling, and uncertainty quantification, are discussed alongside
future directions. Ultimately, this assessment underscores the importance of
PIML-based hybrid approaches in enabling predictive, scalable, and physically
consistent microstructure modeling for site-specific, microstructure-aware
process control and the reliable production of high-performance AM components.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [100] [ROSA: A Knowledge-based Solution for Robot Self-Adaptation](https://arxiv.org/abs/2505.00733)
*Gustavo Rezende Silva,Juliane Päßler,S. Lizeth Tapia Tarifa,Einar Broch Johnsen,Carlos Hernández Corbato*

Main category: cs.AI

TL;DR: ROSA是一个基于知识的机器人自适应性框架，支持任务与架构共同适应（TACA），通过运行时推理应对不确定性。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在复杂多变环境中任务执行和软件架构适应性的挑战。

Method: 提出ROSA框架，包含知识模型和运行时推理机制，实现ROS 2开源实现。

Result: 在水下机器人应用中的实验验证了ROSA的可重用性和开发效率优势。

Conclusion: ROSA为设计自适应机器人系统提供可行且高效的解决方案。

Abstract: Autonomous robots must operate in diverse environments and handle multiple
tasks despite uncertainties. This creates challenges in designing software
architectures and task decision-making algorithms, as different contexts may
require distinct task logic and architectural configurations. To address this,
robotic systems can be designed as self-adaptive systems capable of adapting
their task execution and software architecture at runtime based on their
context.This paper introduces ROSA, a novel knowledge-based framework for RObot
Self-Adaptation, which enables task-and-architecture co-adaptation (TACA) in
robotic systems. ROSA achieves this by providing a knowledge model that
captures all application-specific knowledge required for adaptation and by
reasoning over this knowledge at runtime to determine when and how adaptation
should occur. In addition to a conceptual framework, this work provides an
open-source ROS 2-based reference implementation of ROSA and evaluates its
feasibility and performance in an underwater robotics application. Experimental
results highlight ROSA's advantages in reusability and development effort for
designing self-adaptive robotic systems.

</details>


### [101] [Howard's Policy Iteration is Subexponential for Deterministic Markov Decision Problems with Rewards of Fixed Bit-size and Arbitrary Discount Factor](https://arxiv.org/abs/2505.00795)
*Dibyangshu Mukherjee,Shivaram Kalyanakrishnan*

Main category: cs.AI

TL;DR: 本文改进了Howard策略迭代(HPI)的上界，在确定性MDP(DMDP)上实现了次指数级的运行时间上界，且与折扣因子无关。


<details>
  <summary>Details</summary>
Motivation: HPI算法虽经60余年，但其在确定性MDP上的运行时间上界仍为指数级，本文旨在改进这一现状。

Method: 针对DMDP，基于奖励的比特大小参数化，提出新的分析方法。同样适用于仅两种奖励的DMDP。

Result: 证明了HPI在DMDP上的次指数级上界，独立于折扣因子。对仅两种奖励的DMDP同样适用。

Conclusion: 本文首次为HPI在DMDP上实现了次指数级运行时间上界，填补了理论空白。

Abstract: Howard's Policy Iteration (HPI) is a classic algorithm for solving Markov
Decision Problems (MDPs). HPI uses a "greedy" switching rule to update from any
non-optimal policy to a dominating one, iterating until an optimal policy is
found. Despite its introduction over 60 years ago, the best-known upper bounds
on HPI's running time remain exponential in the number of states -- indeed even
on the restricted class of MDPs with only deterministic transitions (DMDPs).
Meanwhile, the tightest lower bound for HPI for MDPs with a constant number of
actions per state is only linear. In this paper, we report a significant
improvement: a subexponential upper bound for HPI on DMDPs, which is
parameterised by the bit-size of the rewards, while independent of the discount
factor. The same upper bound also applies to DMDPs with only two possible
rewards (which may be of arbitrary size).

</details>


### [102] [Explanations as Bias Detectors: A Critical Study of Local Post-hoc XAI Methods for Fairness Exploration](https://arxiv.org/abs/2505.00802)
*Vasiliki Papanikou,Danae Pla Karidi,Evaggelia Pitoura,Emmanouil Panagiotou,Eirini Ntoutsi*

Main category: cs.AI

TL;DR: 本文探讨了如何利用可解释性方法来检测和解释AI系统中的不公平现象，提出了一个集成局部事后解释方法的流程，并讨论了其关键问题与挑战。


<details>
  <summary>Details</summary>
Motivation: 随着AI在影响人类生活的领域广泛应用，公平性和透明性问题日益突出，尤其是对受保护群体的影响。可解释性与公平性的结合成为推动负责任AI系统发展的重要方向。

Method: 提出一个流程，整合局部事后解释方法，用于获取公平性相关见解，并探讨了解释作为偏见检测器的关键问题。

Result: 结果显示解释方法在公平性方面具有潜力，但也需谨慎考虑其关键问题，如解释的一致性和质量等。

Conclusion: 可解释性方法可用于公平性检测，但需注意其局限性和关键挑战，以确保结果的可靠性和可信度。

Abstract: As Artificial Intelligence (AI) is increasingly used in areas that
significantly impact human lives, concerns about fairness and transparency have
grown, especially regarding their impact on protected groups. Recently, the
intersection of explainability and fairness has emerged as an important area to
promote responsible AI systems. This paper explores how explainability methods
can be leveraged to detect and interpret unfairness. We propose a pipeline that
integrates local post-hoc explanation methods to derive fairness-related
insights. During the pipeline design, we identify and address critical
questions arising from the use of explanations as bias detectors such as the
relationship between distributive and procedural fairness, the effect of
removing the protected attribute, the consistency and quality of results across
different explanation methods, the impact of various aggregation strategies of
local explanations on group fairness evaluations, and the overall
trustworthiness of explanations as bias detectors. Our results show the
potential of explanation methods used for fairness while highlighting the need
to carefully consider the aforementioned critical aspects.

</details>


### [103] [MIMIC-\RNum{4}-Ext-22MCTS: A 22 Millions-Event Temporal Clinical Time-Series Dataset with Relative Timestamp for Risk Prediction](https://arxiv.org/abs/2505.00827)
*Jing Wang,Xing Niu,Juyong Kim,Jie Shen,Tong Zhang,Jeremy C. Weiss*

Main category: cs.AI

TL;DR: 该论文提出了一个基于MIMIC-IV-Note数据集的高质量临床时间序列事件数据集MIMIC-4-Ext-22MCTS，并通过分块检索和Llama-3.1-8B模型解决了长文本和无明确时间戳的问题。微调后的模型在医疗问答和临床试验匹配任务中表现显著提升。


<details>
  <summary>Details</summary>
Motivation: 现代医疗中基于机器学习的临床风险预测需要高质量的时间序列临床事件数据，但现有数据集存在长文本处理困难和缺乏明确时间戳的挑战。

Method: 1)将出院摘要分块处理；2)使用上下文BM25和语义搜索检索潜在事件块；3)设计提示教导Llama-3.1-8B模型识别或推断时间信息。

Result: BERT模型在医疗问答任务中准确率提升10%，临床试验匹配任务提升3%；GPT-2模型产生更可靠的临床答案。

Conclusion: 提出的框架和数据集显著提升了模型在医疗应用中的性能，证明了其信息丰富性和透明度。

Abstract: Clinical risk prediction based on machine learning algorithms plays a vital
role in modern healthcare. A crucial component in developing a reliable
prediction model is collecting high-quality time series clinical events. In
this work, we release such a dataset that consists of 22,588,586 Clinical Time
Series events, which we term MIMIC-\RNum{4}-Ext-22MCTS. Our source data are
discharge summaries selected from the well-known yet unstructured MIMIC-IV-Note
\cite{Johnson2023-pg}. We then extract clinical events as short text span from
the discharge summaries, along with the timestamps of these events as temporal
information. The general-purpose MIMIC-IV-Note pose specific challenges for our
work: it turns out that the discharge summaries are too lengthy for typical
natural language models to process, and the clinical events of interest often
are not accompanied with explicit timestamps. Therefore, we propose a new
framework that works as follows: 1) we break each discharge summary into
manageably small text chunks; 2) we apply contextual BM25 and contextual
semantic search to retrieve chunks that have a high potential of containing
clinical events; and 3) we carefully design prompts to teach the recently
released Llama-3.1-8B \cite{touvron2023llama} model to identify or infer
temporal information of the chunks. We show that the obtained dataset is so
informative and transparent that standard models fine-tuned on our dataset are
achieving significant improvements in healthcare applications. In particular,
the BERT model fine-tuned based on our dataset achieves 10\% improvement in
accuracy on medical question answering task, and 3\% improvement in clinical
trial matching task compared with the classic BERT. The GPT-2 model, fine-tuned
on our dataset, produces more clinically reliable results for clinical
questions.

</details>


### [104] [Thoughts without Thinking: Reconsidering the Explanatory Value of Chain-of-Thought Reasoning in LLMs through Agentic Pipelines](https://arxiv.org/abs/2505.00875)
*Ramesh Manuvinakurike,Emanuel Moss,Elizabeth Anne Watkins,Saurav Sahay,Giuseppe Raffa,Lama Nachman*

Main category: cs.AI

TL;DR: 摘要讨论了代理管道在人类中心可解释性中的挑战，指出CoT推理在代理管道中未能提升输出质量或提供有效解释。


<details>
  <summary>Details</summary>
Motivation: 探讨如何使LLM的内部工作更透明，以提升人类对代理管道的理解和控制。

Method: 通过定量和定性分析，研究CoT推理在代理管道中的表现。

Result: CoT推理未能改善输出质量，且其解释未能帮助用户更好地理解系统或实现目标。

Conclusion: CoT推理在代理管道中单独使用时效果有限，需进一步探索其他解释性方法。

Abstract: Agentic pipelines present novel challenges and opportunities for
human-centered explainability. The HCXAI community is still grappling with how
best to make the inner workings of LLMs transparent in actionable ways. Agentic
pipelines consist of multiple LLMs working in cooperation with minimal human
control. In this research paper, we present early findings from an agentic
pipeline implementation of a perceptive task guidance system. Through
quantitative and qualitative analysis, we analyze how Chain-of-Thought (CoT)
reasoning, a common vehicle for explainability in LLMs, operates within agentic
pipelines. We demonstrate that CoT reasoning alone does not lead to better
outputs, nor does it offer explainability, as it tends to produce explanations
without explainability, in that they do not improve the ability of end users to
better understand systems or achieve their goals.

</details>


### [105] [Car Sensors Health Monitoring by Verification Based on Autoencoder and Random Forest Regression](https://arxiv.org/abs/2505.00876)
*Sahar Torkhesari,Behnam Yousefimehr,Mehdi Ghatee*

Main category: cs.AI

TL;DR: 该论文介绍了一种创新的汽车传感器健康监测系统，通过机器学习和深度学习技术分析传感器数据，结合自编码器和随机森林回归方法，实现了99%的准确率，并能够提前检测故障。


<details>
  <summary>Details</summary>
Motivation: 为了提高汽车传感器监测的准确性和效率，减少故障对驾驶安全的影响，作者开发了一套先进的传感器健康监测系统。

Method: 论文采用机器学习和深度学习技术，尤其是自编码器和随机森林回归，分析传感器数据的相关性，并开发了一种基于正态分布的统计模型来检测故障。

Result: 在Saipa Quick车辆的20个关键传感器上测试，系统实现了99%的准确率，并能及时发现并替换故障传感器的数据。

Conclusion: 该系统成功提升了汽车传感器监控的效率和可靠性，为驾驶员和维护部门提供了及时、准确的故障预警。

Abstract: Driver assistance systems provide a wide range of crucial services, including
closely monitoring the condition of vehicles. This paper showcases a
groundbreaking sensor health monitoring system designed for the automotive
industry. The ingenious system leverages cutting-edge techniques to process
data collected from various vehicle sensors. It compares their outputs within
the Electronic Control Unit (ECU) to evaluate the health of each sensor. To
unravel the intricate correlations between sensor data, an extensive
exploration of machine learning and deep learning methodologies was conducted.
Through meticulous analysis, the most correlated sensor data were identified.
These valuable insights were then utilized to provide accurate estimations of
sensor values. Among the diverse learning methods examined, the combination of
autoencoders for detecting sensor failures and random forest regression for
estimating sensor values proved to yield the most impressive outcomes. A
statistical model using the normal distribution has been developed to identify
possible sensor failures proactively. By comparing the actual values of the
sensors with their estimated values based on correlated sensors, faulty sensors
can be detected early. When a defective sensor is detected, both the driver and
the maintenance department are promptly alerted. Additionally, the system
replaces the value of the faulty sensor with the estimated value obtained
through analysis. This proactive approach was evaluated using data from twenty
essential sensors in the Saipa's Quick vehicle's ECU, resulting in an
impressive accuracy rate of 99\%.

</details>


### [106] [Seeking to Collide: Online Safety-Critical Scenario Generation for Autonomous Driving with Retrieval Augmented Large Language Models](https://arxiv.org/abs/2505.00972)
*Yuewen Mei,Tong Nie,Jian Sun,Ye Tian*

Main category: cs.AI

TL;DR: 本文提出了一种在线检索增强的大语言模型框架，用于生成安全关键驾驶场景，显著提升自动驾驶车辆的测试效果。


<details>
  <summary>Details</summary>
Motivation: 现有场景生成方法要么过度拟合常见驾驶模式，要么无法暴露罕见的安全关键极端情况，因此需要一种更有效的在线生成方法。

Method: 通过LLM推断背景车辆的最危险意图，并查询其他LLM代理合成对抗轨迹，同时通过动态记忆和检索库加速适应。

Result: 在Waymo Open Motion Dataset上的评估显示，平均最小碰撞时间从1.62秒降至1.08秒，碰撞率提升75%。

Conclusion: 该方法显著优于基线，能够更有效地生成安全关键场景，适用于自动驾驶测试。

Abstract: Simulation-based testing is crucial for validating autonomous vehicles (AVs),
yet existing scenario generation methods either overfit to common driving
patterns or operate in an offline, non-interactive manner that fails to expose
rare, safety-critical corner cases. In this paper, we introduce an online,
retrieval-augmented large language model (LLM) framework for generating
safety-critical driving scenarios. Our method first employs an LLM-based
behavior analyzer to infer the most dangerous intent of the background vehicle
from the observed state, then queries additional LLM agents to synthesize
feasible adversarial trajectories. To mitigate catastrophic forgetting and
accelerate adaptation, we augment the framework with a dynamic memorization and
retrieval bank of intent-planner pairs, automatically expanding its behavioral
library when novel intents arise. Evaluations using the Waymo Open Motion
Dataset demonstrate that our model reduces the mean minimum time-to-collision
from 1.62 to 1.08 s and incurs a 75% collision rate, substantially
outperforming baselines.

</details>


### [107] [Improving Large Language Model Planning with Action Sequence Similarity](https://arxiv.org/abs/2505.01009)
*Xinran Zhao,Hanie Sedghi,Bernd Bohnet,Dale Schuurmans,Azade Nova*

Main category: cs.AI

TL;DR: 该论文提出了一种通过上下文学习提高大语言模型规划能力的方法GRASE-DC，利用动作序列相似性筛选示例，显著提升了多种规划任务的性能。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过上下文学习提升大语言模型的规划能力，尤其是识别哪些信号有助于选择有效的示例。

Method: 提出GRASE-DC，一个两阶段流程：首先重采样高动作序列相似性的示例，然后通过动态聚类平衡相关性和多样性。

Result: GRASE-DC在多种规划任务中表现显著提升（最高提升11-40个百分点），且示例需求减少27.3%。结合验证器后性能进一步提升18.9%。

Conclusion: GRASE-DC能有效提升规划性能，并泛化到分布外问题，为基于大语言模型的规划任务提供了实用解决方案。

Abstract: Planning is essential for artificial intelligence systems to look ahead and
proactively determine a course of actions to reach objectives in the virtual
and real world. Recent work on large language models (LLMs) sheds light on
their planning capability in various tasks. However, it remains unclear what
signals in the context influence the model performance. In this work, we
explore how to improve the model planning capability through in-context
learning (ICL), specifically, what signals can help select the exemplars.
Through extensive experiments, we observe that commonly used problem similarity
may result in false positives with drastically different plans, which can
mislead the model. In response, we propose to sample and filter exemplars
leveraging plan side action sequence similarity (AS). We propose GRASE-DC: a
two-stage pipeline that first re-samples high AS exemplars and then curates the
selected exemplars with dynamic clustering on AS to achieve a balance of
relevance and diversity. Our experimental result confirms that GRASE-DC
achieves significant performance improvement on various planning tasks (up to
~11-40 point absolute accuracy improvement with 27.3% fewer exemplars needed on
average). With GRASE-DC* + VAL, where we iteratively apply GRASE-DC with a
validator, we are able to even boost the performance by 18.9% more.
  Extensive analysis validates the consistent performance improvement of
GRASE-DC with various backbone LLMs and on both classical planning and natural
language planning benchmarks. GRASE-DC can further boost the planning accuracy
by ~24 absolute points on harder problems using simpler problems as exemplars
over a random baseline. This demonstrates its ability to generalize to
out-of-distribution problems.

</details>


### [108] [Adaptive Wizard for Removing Cross-Tier Misconfigurations in Active Directory](https://arxiv.org/abs/2505.01028)
*Huy Q. Ngo,Mingyu Guo,Hung Nguyen*

Main category: cs.AI

TL;DR: 该论文提出了一种自适应路径移除问题，旨在通过最小化安全团队与IT管理员之间的交互次数来优化Windows Active Directory系统的安全加固过程，并提出了多种算法解决方案。


<details>
  <summary>Details</summary>
Motivation: 为了解决Windows Active Directory系统中安全漏洞加固过程中手动验证的高成本问题，论文旨在优化迭代移除攻击路径的交互过程，减少人工干预的负担。

Method: 论文首先证明了自适应路径移除问题的复杂性，随后提出了一个精确算法、一个近似算法和几种可扩展的启发式方法，其中最佳启发式方法称为DPR。

Result: 在合成和真实的AD攻击图上验证了算法的有效性，DPR在大规模图上表现优于精确算法和近似算法。

Conclusion: 该研究为AD系统安全加固提供了一种高效的方法，显著减少了人工交互次数，并在实际应用中展现了良好的可扩展性。

Abstract: Security vulnerabilities in Windows Active Directory (AD) systems are
typically modeled using an attack graph and hardening AD systems involves an
iterative workflow: security teams propose an edge to remove, and IT operations
teams manually review these fixes before implementing the removal. As
verification requires significant manual effort, we formulate an Adaptive Path
Removal Problem to minimize the number of steps in this iterative removal
process. In our model, a wizard proposes an attack path in each step and
presents it as a set of multiple-choice options to the IT admin. The IT admin
then selects one edge from the proposed set to remove. This process continues
until the target $t$ is disconnected from source $s$ or the number of proposed
paths reaches $B$. The model aims to optimize the human effort by minimizing
the expected number of interactions between the IT admin and the security
wizard. We first prove that the problem is $\mathcal{\#P}$-hard. We then
propose a set of solutions including an exact algorithm, an approximate
algorithm, and several scalable heuristics. Our best heuristic, called DPR, can
operate effectively on larger-scale graphs compared to the exact algorithm and
consistently outperforms the approximate algorithm across all graphs. We verify
the effectiveness of our algorithms on several synthetic AD graphs and an AD
attack graph collected from a real organization.

</details>


### [109] [Retrieval Augmented Learning: A Retrial-based Large Language Model Self-Supervised Learning and Autonomous Knowledge Generation](https://arxiv.org/abs/2505.01073)
*Zongyuan Li,Pengfei Li,Runnan Qi,Yanan Ni,Lumin Jiang,Hui Wu,Xuebo Zhang,Kuihua Huang,Xian Guo*

Main category: cs.AI

TL;DR: 提出了一种无需训练的检索增强学习框架（RAL），通过三阶段自主知识生成（假设提出、验证、知识生成）减少LLM的幻觉，提高决策性能，成本极低。


<details>
  <summary>Details</summary>
Motivation: 解决领域特定数据不足对LLM决策系统的限制，避免传统微调的高计算成本。

Method: 将检索增强生成（RAG）发展为模块化工具，支持假设提出、验证和知识生成的三阶段自主学习。

Result: 在LLM-PySC2环境中验证，显著减少幻觉并提升决策性能，同时展示出对OOD任务、鲁棒性和迁移性的潜力。

Conclusion: RAL是一种低成本高效益的解决方案，适用于决策问题和自主知识生成。

Abstract: The lack of domain-specific data in the pre-training of Large Language Models
(LLMs) severely limits LLM-based decision systems in specialized applications,
while post-training a model in the scenarios requires significant computational
resources. In this paper, we present Retrial-Augmented Learning (RAL), a
reward-free self-supervised learning framework for LLMs that operates without
model training. By developing Retrieval-Augmented Generation (RAG) into a
module for organizing intermediate data, we realized a three-stage autonomous
knowledge generation of proposing a hypothesis, validating the hypothesis, and
generating the knowledge. The method is evaluated in the LLM-PySC2 environment,
a representative decision-making platform that combines sufficient complexity
with domain-specific knowledge requirements. Experiments demonstrate that the
proposed method effectively reduces hallucination by generating and utilizing
validated knowledge, and increases decision-making performance at an extremely
low cost. Meanwhile, the approach exhibits potential in
out-of-distribution(OOD) tasks, robustness, and transferability, making it a
cost-friendly but effective solution for decision-making problems and
autonomous knowledge generation.

</details>


### [110] [MADIL: An MDL-based Framework for Efficient Program Synthesis in the ARC Benchmark](https://arxiv.org/abs/2505.01081)
*Sébastien Ferré*

Main category: cs.AI

TL;DR: 提出了一种基于最小描述长度(MDL)的新型AI方法MADIL,用于高效归纳学习,在ARC测试中表现虽不如LLM但更高效可解释


<details>
  <summary>Details</summary>
Motivation: AI在专业任务取得成功但泛化能力不足,现有LLM方法训练成本高

Method: 基于MDL原则进行模式分解,实现结构化泛化

Result: ArcPrize 2024测试中取得7%准确率

Conclusion: MADIL提供了一种高效可解释的归纳学习方法,优于LLM的运行效率

Abstract: Artificial Intelligence (AI) has achieved remarkable success in specialized
tasks but struggles with efficient skill acquisition and generalization. The
Abstraction and Reasoning Corpus (ARC) benchmark evaluates intelligence based
on minimal training requirements. While Large Language Models (LLMs) have
recently improved ARC performance, they rely on extensive pre-training and high
computational costs. We introduce MADIL (MDL-based AI), a novel approach
leveraging the Minimum Description Length (MDL) principle for efficient
inductive learning. MADIL performs pattern-based decomposition, enabling
structured generalization. While its performance (7% at ArcPrize 2024) remains
below LLM-based methods, it offers greater efficiency and interpretability.
This paper details MADIL's methodology, its application to ARC, and
experimental evaluations.

</details>


### [111] [Explainable AI Based Diagnosis of Poisoning Attacks in Evolutionary Swarms](https://arxiv.org/abs/2505.01181)
*Mehrdad Asadi,Roxana Rădulescu,Ann Nowé*

Main category: cs.AI

TL;DR: 该论文研究了多无人机网络中数据投毒攻击对群体策略的影响，通过可解释AI方法量化攻击效果并提出诊断方法。


<details>
  <summary>Details</summary>
Motivation: 针对群体系统在关键环境中因数据投毒攻击导致的协调失效或对抗行为问题，研究其影响并提出解决方案。

Method: 采用进化智能建模代理交互，通过系统性的数据投毒攻击评估，并利用可解释AI方法分析攻击效果。

Result: 当投毒比例超过10%时，会导致非最优策略，表现为合作效率下降。

Conclusion: 论文提出了一个框架来诊断和量化数据投毒攻击对群体策略的影响，为增强群体系统的鲁棒性提供了依据。

Abstract: Swarming systems, such as for example multi-drone networks, excel at
cooperative tasks like monitoring, surveillance, or disaster assistance in
critical environments, where autonomous agents make decentralized decisions in
order to fulfill team-level objectives in a robust and efficient manner.
Unfortunately, team-level coordinated strategies in the wild are vulnerable to
data poisoning attacks, resulting in either inaccurate coordination or
adversarial behavior among the agents. To address this challenge, we contribute
a framework that investigates the effects of such data poisoning attacks, using
explainable AI methods. We model the interaction among agents using
evolutionary intelligence, where an optimal coalition strategically emerges to
perform coordinated tasks. Then, through a rigorous evaluation, the swarm model
is systematically poisoned using data manipulation attacks. We showcase the
applicability of explainable AI methods to quantify the effects of poisoning on
the team strategy and extract footprint characterizations that enable
diagnosing. Our findings indicate that when the model is poisoned above 10%,
non-optimal strategies resulting in inefficient cooperation can be identified.

</details>


### [112] [Exploring the Impact of Explainable AI and Cognitive Capabilities on Users' Decisions](https://arxiv.org/abs/2505.01192)
*Federico Maria Cau,Lucio Davide Spano*

Main category: cs.AI

TL;DR: 该研究探讨了在贷款申请场景中，AI信息（如预测、置信度和准确性）和不同解释风格（基于示例、特征、规则和反事实）对决策准确性、AI依赖度和认知负荷的影响，以及高低认知需求（NFC）个体的差异。结果发现高AI置信度增加依赖并降低认知负荷，反事实解释虽难理解但提升准确性，不同NFC组在准确性或认知负荷上无显著差异，突显了XAI界面需用户个性化设计。


<details>
  <summary>Details</summary>
Motivation: AI决策的普及引发了对解释需求的讨论。现有可解释AI（XAI）研究多聚焦特征解释，但不同解释风格和用户认知需求（如NFC）的影响尚未充分探索。研究旨在填补这一空白，优化人机协作。

Method: 在贷款申请场景中，测试四种解释风格（示例、特征、规则、反事实）和AI信息（预测、置信度、准确性）对用户决策的影响，并比较高低NFC个体的差异，评估准确性、AI依赖度和认知负荷。

Result: 高AI置信度显著提升依赖并降低认知负荷；特征解释未比其他条件更优；反事实解释虽难理解但增强准确性（尤其在AI预测正确时）。高低NFC组均优先关注贷款属性和解释，对AI信息最不重视，但两组在准确性或认知负荷上无显著差异。

Conclusion: XAI界面需个性化设计，整合多样解释风格并探索更多用户特质（如NFC）以优化人机协作。当前结果质疑NFC在AI辅助决策中的作用，呼吁进一步研究其他用户特征的影响。

Abstract: Artificial Intelligence (AI) systems are increasingly used for
decision-making across domains, raising debates over the information and
explanations they should provide. Most research on Explainable AI (XAI) has
focused on feature-based explanations, with less attention on alternative
styles. Personality traits like the Need for Cognition (NFC) can also lead to
different decision-making outcomes among low and high NFC individuals. We
investigated how presenting AI information (prediction, confidence, and
accuracy) and different explanation styles (example-based, feature-based,
rule-based, and counterfactual) affect accuracy, reliance on AI, and cognitive
load in a loan application scenario. We also examined low and high NFC
individuals' differences in prioritizing XAI interface elements (loan
attributes, AI information, and explanations), accuracy, and cognitive load.
Our findings show that high AI confidence significantly increases reliance on
AI while reducing cognitive load. Feature-based explanations did not enhance
accuracy compared to other conditions. Although counterfactual explanations
were less understandable, they enhanced overall accuracy, increasing reliance
on AI and reducing cognitive load when AI predictions were correct. Both low
and high NFC individuals prioritized explanations after loan attributes,
leaving AI information as the least important. However, we found no significant
differences between low and high NFC groups in accuracy or cognitive load,
raising questions about the role of personality traits in AI-assisted
decision-making. These findings highlight the need for user-centric
personalization in XAI interfaces, incorporating diverse explanation styles and
exploring multiple personality traits and other user characteristics to
optimize human-AI collaboration.

</details>


### [113] [Early Detection of Patient Deterioration from Real-Time Wearable Monitoring System](https://arxiv.org/abs/2505.01305)
*Lo Pang-Yun Ting,Hong-Pei Chen,An-Shan Liu,Chun-Yin Yeh,Po-Lin Chen,Kun-Ta Chuang*

Main category: cs.AI

TL;DR: 论文提出了一种名为TARL的创新方法，通过建模心率时间序列中的代表性子序列（shapelets）的结构关系，构建shapelet-transition知识图谱，并结合过渡感知知识嵌入，以处理穿戴设备数据的缺失值问题，实现早期疾病检测。


<details>
  <summary>Details</summary>
Motivation: 早期发现患者恶化对降低死亡率至关重要。心率数据在评估患者健康状况中表现出潜力，但如何处理多样化的心率数据及穿戴设备数据中的缺失值是关键挑战。

Method: 提出TARL方法，通过建模心率时间序列中的shapelets结构关系，构建shapelet-transition知识图谱，并结合过渡感知知识嵌入强化shapelets间关系，量化缺失值影响。

Result: 在真实ICU数据上的实验表明，TARL具有高可靠性和早期检测能力。案例研究展示了其可解释的检测过程，突显其作为AI驱动工具的潜力。

Conclusion: TARL能够捕捉心率数据的解释性结构并预测未来趋势，辅助临床医生早期识别患者恶化迹象。

Abstract: Early detection of patient deterioration is crucial for reducing mortality
rates. Heart rate data has shown promise in assessing patient health, and
wearable devices offer a cost-effective solution for real-time monitoring.
However, extracting meaningful insights from diverse heart rate data and
handling missing values in wearable device data remain key challenges. To
address these challenges, we propose TARL, an innovative approach that models
the structural relationships of representative subsequences, known as
shapelets, in heart rate time series. TARL creates a shapelet-transition
knowledge graph to model shapelet dynamics in heart rate time series,
indicating illness progression and potential future changes. We further
introduce a transition-aware knowledge embedding to reinforce relationships
among shapelets and quantify the impact of missing values, enabling the
formulation of comprehensive heart rate representations. These representations
capture explanatory structures and predict future heart rate trends, aiding
early illness detection. We collaborate with physicians and nurses to gather
ICU patient heart rate data from wearables and diagnostic metrics assessing
illness severity for evaluating deterioration. Experiments on real-world ICU
data demonstrate that TARL achieves both high reliability and early detection.
A case study further showcases TARL's explainable detection process,
highlighting its potential as an AI-driven tool to assist clinicians in
recognizing early signs of patient deterioration.

</details>


### [114] [BalancEdit: Dynamically Balancing the Generality-Locality Trade-off in Multi-modal Model Editing](https://arxiv.org/abs/2505.01343)
*Dongliang Guo,Mengxuan Hu,Zihan Guan,Thomas Hartvigsen,Sheng Li*

Main category: cs.AI

TL;DR: 这篇论文针对多模态模型知识更新中存在的‘泛化-局部性’权衡问题，提出了BalancEdit方法，通过动态平衡编辑范围与精度，使用正负样本确定影响范围，并通过离散编辑码本实现更新，而无需修改模型权重。


<details>
  <summary>Details</summary>
Motivation: 多模态模型随知识更新会逐渐过时，传统微调方法不适用于大规模模型，而现有编辑技术忽略了不同事实的独特影响范围，导致模型性能下降。

Method: 提出BalancEdit方法，生成正负样本动态评估事实影响范围，利用离散码本在隐空间完成编辑，避免权重修改。

Result: 实验证明BalancEdit在保持编辑能力的同时，最小化了泛化与局部性的权衡。

Conclusion: BalancEdit是首个明确解决多模态模型编辑中‘泛化-局部性’权衡的方法，效果显著且无需改动模型权重。

Abstract: Large multi-modal models inevitably decay over time as facts change and
previously learned information becomes outdated. Traditional approaches such as
fine-tuning are often impractical for updating these models due to their size
and complexity. Instead, direct knowledge editing within the models presents a
more viable solution. Current model editing techniques, however, typically
overlook the unique influence ranges of different facts, leading to compromised
model performance in terms of both generality and locality. To address this
issue, we introduce the concept of the generality-locality trade-off in
multi-modal model editing. We develop a new model editing dataset named OKEDIT,
specifically designed to effectively evaluate this trade-off. Building on this
foundation, we propose BalancEdit, a novel method for balanced model editing
that dynamically achieves an optimal balance between generality and locality.
BalancEdit utilizes a unique mechanism that generates both positive and
negative samples for each fact to accurately determine its influence scope and
incorporates these insights into the model's latent space using a discrete,
localized codebook of edits, without modifying the underlying model weights. To
our knowledge, this is the first approach explicitly addressing the
generality-locality trade-off in multi-modal model editing. Our comprehensive
results confirm the effectiveness of BalancEdit, demonstrating minimal
trade-offs while maintaining robust editing capabilities. Our code and dataset
will be available.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [115] [On the emergence of numerical instabilities in Next Generation Reservoir Computing](https://arxiv.org/abs/2505.00846)
*Edmilson Roque dos Santos,Erik Bollt*

Main category: stat.ML

TL;DR: NGRC是一种低成本机器学习方法，用于预测混沌时间序列，但自主预测时模型稳定性仍是挑战。研究发现特征矩阵数值条件与长期动力学有关，并探讨了超参数对特征矩阵条件数的影响及其对稳定性的影响。


<details>
  <summary>Details</summary>
Motivation: 主要动机是解决NGRC在自主预测时动力学稳定的挑战。通过分析NGRC特征矩阵的数值条件及其与长期动力学的关系，为模型稳定性提供理论支持。

Method: 研究结合数值线性代数和动力系统遍历论工具，系统分析了NGRC特征矩阵条件数随超参数的变化。还评估了不同数值算法（Cholesky、SVD和LU）求解正则化最小二乘问题的效果。

Result: 发现NGRC特征矩阵在短时间内滞和高阶多项式下易出现病态条件，这会放大对训练数据扰动的敏感性，导致不稳定动力学。

Conclusion: 通过系统分析超参数对NGRC特征矩阵条件数的影响，揭示了模型不稳定的根源，并提出了数值算法的影响。

Abstract: Next Generation Reservoir Computing (NGRC) is a low-cost machine learning
method for forecasting chaotic time series from data. However, ensuring the
dynamical stability of NGRC models during autonomous prediction remains a
challenge. In this work, we uncover a key connection between the numerical
conditioning of the NGRC feature matrix -- formed by polynomial evaluations on
time-delay coordinates -- and the long-term NGRC dynamics. Merging tools from
numerical linear algebra and ergodic theory of dynamical systems, we
systematically study how the feature matrix conditioning varies across
hyperparameters. We demonstrate that the NGRC feature matrix tends to be
ill-conditioned for short time lags and high-degree polynomials.
Ill-conditioning amplifies sensitivity to training data perturbations, which
can produce unstable NGRC dynamics. We evaluate the impact of different
numerical algorithms (Cholesky, SVD, and LU) for solving the regularized
least-squares problem.

</details>


### [116] [DOLCE: Decomposing Off-Policy Evaluation/Learning into Lagged and Current Effects](https://arxiv.org/abs/2505.00961)
*Shu Tamano,Masanori Nojima*

Main category: stat.ML

TL;DR: 论文提出DOLCE方法，通过分解奖励为滞后和当前效应，解决传统OPL/OPE方法在共同支持假设不成立时的局限性，实验表明其显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统OPL/OPE方法依赖共同支持假设，对不符合该假设的个体需不稳定的外推或保守策略，无法满足对其显式评估/优化的需求。

Method: 提出DOLCE方法，利用多时间点的上下文信息将奖励分解为滞后和当前效应，结合过去与当前上下文处理共同支持假设外的个体。

Result: 实验证明DOLCE在共同支持假设外个体比例增加时，显著提升OPE和OPL效果。

Conclusion: DOLCE通过分解效应和利用上下文信息，有效解决共同支持假设的局限性，为OPL/OPE提供更稳健的解决方案。

Abstract: Off-policy evaluation (OPE) and off-policy learning (OPL) for contextual
bandit policies leverage historical data to evaluate and optimize a target
policy. Most existing OPE/OPL methods--based on importance weighting or
imputation--assume common support between the target and logging policies. When
this assumption is violated, these methods typically require unstable
extrapolation, truncation, or conservative strategies for individuals outside
the common support assumption. However, such approaches can be inadequate in
settings where explicit evaluation or optimization for such individuals is
required. To address this issue, we propose DOLCE: Decomposing Off-policy
evaluation/learning into Lagged and Current Effects, a novel estimator that
leverages contextual information from multiple time points to decompose rewards
into lagged and current effects. By incorporating both past and present
contexts, DOLCE effectively handles individuals who violate the common support
assumption. We show that the proposed estimator is unbiased under two
assumptions--local correctness and conditional independence. Our experiments
demonstrate that DOLCE achieves substantial improvements in OPE and OPL,
particularly as the proportion of individuals outside the common support
assumption increases.

</details>


### [117] [Characterization and Learning of Causal Graphs from Hard Interventions](https://arxiv.org/abs/2505.01037)
*Zihan Zhou,Muhammad Qasim Elahi,Murat Kocaoglu*

Main category: stat.ML

TL;DR: 本文提出了一种基于硬干预的多实验分布数据的因果图等价类表征方法，并开发了一种学习算法来整合这些数据，证明了算法的可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决从多实验分布数据中发现因果结构的挑战，并建立与Pearl的do-演算相关的图形约束。

Method: 通过比较不同干预分布提出图形约束，并开发学习算法整合多数据集，引入新的定向规则。

Result: 表征了带隐变量的因果图的干预等价类，并提供了图形表示方法来判断等价性。

Conclusion: 提出的算法能有效整合硬干预数据，为因果发现提供了新的理论和方法支持。

Abstract: A fundamental challenge in the empirical sciences involves uncovering causal
structure through observation and experimentation. Causal discovery entails
linking the conditional independence (CI) invariances in observational data to
their corresponding graphical constraints via d-separation. In this paper, we
consider a general setting where we have access to data from multiple
experimental distributions resulting from hard interventions, as well as
potentially from an observational distribution. By comparing different
interventional distributions, we propose a set of graphical constraints that
are fundamentally linked to Pearl's do-calculus within the framework of hard
interventions. These graphical constraints associate each graphical structure
with a set of interventional distributions that are consistent with the rules
of do-calculus. We characterize the interventional equivalence class of causal
graphs with latent variables and introduce a graphical representation that can
be used to determine whether two causal graphs are interventionally equivalent,
i.e., whether they are associated with the same family of hard interventional
distributions, where the elements of the family are indistinguishable using the
invariances from do-calculus. We also propose a learning algorithm to integrate
multiple datasets from hard interventions, introducing new orientation rules.
The learning objective is a tuple of augmented graphs which entails a set of
causal graphs. We also prove the soundness of the proposed algorithm.

</details>


### [118] [Gaussian Differential Private Bootstrap by Subsampling](https://arxiv.org/abs/2505.01197)
*Holger Dette,Carina Graw*

Main category: stat.ML

TL;DR: 本文提出了一种私有经验m/n自举方法，在满足高斯差分隐私的前提下，相比传统n/n自举，具有更低计算成本、更高统计精度和更好的有限样本性能。


<details>
  <summary>Details</summary>
Motivation: 自举法是数据不确定性量化的常用工具，但在差分隐私框架下，传统自举因需重复访问数据而增加隐私预算，导致统计精度下降。如何在保证隐私的同时提升统计准确性是核心挑战。

Method: 提出私有经验m/n自举法，基于高斯差分隐私理论验证其一致性和隐私保障，通过减少每次迭代的样本量（m<n）降低噪声添加需求。

Result: 相比n/n自举，该方法具有：（1）更低计算成本；（2）更少噪声添加，提升统计精度；（3）实验验证其有限样本性能更优。

Conclusion: 私有m/n自举在计算效率、统计准确性和隐私保护之间实现了更好平衡，为大规模隐私数据推断提供了一种可行方案。

Abstract: Bootstrap is a common tool for quantifying uncertainty in data analysis.
However, besides additional computational costs in the application of the
bootstrap on massive data, a challenging problem in bootstrap based inference
under Differential Privacy consists in the fact that it requires repeated
access to the data. As a consequence, bootstrap based differentially private
inference requires a significant increase of the privacy budget, which on the
other hand comes with a substantial loss in statistical accuracy.
  A potential solution to reconcile the conflicting goals of statistical
accuracy and privacy is to analyze the data under parametric model assumptions
and in the last decade, several parametric bootstrap methods for inference
under privacy have been investigated. However, uncertainty quantification by
parametric bootstrap is only valid if the the quantities of interest can be
identified as the parameters of a statistical model and the imposed model
assumptions are (at least approximately) satisfied. An alternative to
parametric methods is the empirical bootstrap that is a widely used tool for
non-parametric inference and well studied in the non-private regime. However,
under privacy, less insight is available. In this paper, we propose a private
empirical $m$ out of $n$ bootstrap and validate its consistency and privacy
guarantees under Gaussian Differential Privacy. Compared to the the private $n$
out of $n$ bootstrap, our approach has several advantages. First, it comes with
less computational costs, in particular for massive data. Second, the proposed
procedure needs less additional noise in the bootstrap iterations, which leads
to an improved statistical accuracy while asymptotically guaranteeing the same
level of privacy. Third, we demonstrate much better finite sample properties
compared to the currently available procedures.

</details>


### [119] [Provable Efficiency of Guidance in Diffusion Models for General Data Distribution](https://arxiv.org/abs/2505.01382)
*Gen Li,Yuchen Jiao*

Main category: stat.ML

TL;DR: 扩散模型已成为生成建模的强大框架，其中引导技术对提升样本质量至关重要。然而，现有研究仅局限于特定案例的分类分布分析，缺乏对一般数据分布的理论理解。本文尝试在一般数据分布下分析扩散引导的效果，证明了引导能提升整体样本质量。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型的引导技术在实践中表现优异，但其理论基础仅局限于简单案例（如各向同性高斯分布或一维区间分布）。如何在更一般的数据分布下分析引导效果仍是一个开放问题。

Method: 本文提出在一般数据分布下分析扩散引导的理论框架，通过理论证明而非案例研究，探讨引导对样本质量的影响。

Result: 研究发现，引导并非在所有分布下均能均匀提升样本质量，但能通过降低分类器概率的平均倒数来整体提升样本质量。

Conclusion: 本文填补了扩散引导理论在一般数据分布下的空白，验证了引导技术对样本质量的整体提升作用，与引入引导的动机一致。

Abstract: Diffusion models have emerged as a powerful framework for generative
modeling, with guidance techniques playing a crucial role in enhancing sample
quality. Despite their empirical success, a comprehensive theoretical
understanding of the guidance effect remains limited. Existing studies only
focus on case studies, where the distribution conditioned on each class is
either isotropic Gaussian or supported on a one-dimensional interval with some
extra conditions. How to analyze the guidance effect beyond these case studies
remains an open question. Towards closing this gap, we make an attempt to
analyze diffusion guidance under general data distributions. Rather than
demonstrating uniform sample quality improvement, which does not hold in some
distributions, we prove that guidance can improve the whole sample quality, in
the sense that the average reciprocal of the classifier probability decreases
with the existence of guidance. This aligns with the motivation of introducing
guidance.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [120] [A Survey on 3D Reconstruction Techniques in Plant Phenotyping: From Classical Methods to Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and Beyond](https://arxiv.org/abs/2505.00737)
*Jiajia Li,Xinda Qi,Seyed Hamidreza Nabaei,Meiqi Liu,Dong Chen,Xin Zhang,Xunyuan Yin,Zhaojian Li*

Main category: eess.IV

TL;DR: 本文综述了植物表型分析中的3D重建技术，包括传统方法、NeRF和新兴的3D高斯泼溅技术，分析了它们的优势、局限性和未来前景。


<details>
  <summary>Details</summary>
Motivation: 植物表型分析对精准农业和作物改良至关重要，而3D重建技术能够提供详细的植物形态和结构信息，从而实现准确和自动化的表型分析。

Method: 综述了传统重建方法、NeRF技术以及新兴的3D高斯泼溅（3DGS）方法，比较了它们的方法论、应用场景和性能表现。

Result: 传统方法简单灵活但面临数据密度和噪声问题；NeRF可实现高质量重建但计算成本高；3DGS通过高斯基元表示几何，提高了效率和扩展性。

Conclusion: 通过比较不同3D重建技术的优劣，为自动化、高通量植物表型分析提供了技术参考，推动新一代农业技术的发展。

Abstract: Plant phenotyping plays a pivotal role in understanding plant traits and
their interactions with the environment, making it crucial for advancing
precision agriculture and crop improvement. 3D reconstruction technologies have
emerged as powerful tools for capturing detailed plant morphology and
structure, offering significant potential for accurate and automated
phenotyping. This paper provides a comprehensive review of the 3D
reconstruction techniques for plant phenotyping, covering classical
reconstruction methods, emerging Neural Radiance Fields (NeRF), and the novel
3D Gaussian Splatting (3DGS) approach. Classical methods, which often rely on
high-resolution sensors, are widely adopted due to their simplicity and
flexibility in representing plant structures. However, they face challenges
such as data density, noise, and scalability. NeRF, a recent advancement,
enables high-quality, photorealistic 3D reconstructions from sparse viewpoints,
but its computational cost and applicability in outdoor environments remain
areas of active research. The emerging 3DGS technique introduces a new paradigm
in reconstructing plant structures by representing geometry through Gaussian
primitives, offering potential benefits in both efficiency and scalability. We
review the methodologies, applications, and performance of these approaches in
plant phenotyping and discuss their respective strengths, limitations, and
future prospects (https://github.com/JiajiaLi04/3D-Reconstruction-Plants).
Through this review, we aim to provide insights into how these diverse 3D
reconstruction techniques can be effectively leveraged for automated and
high-throughput plant phenotyping, contributing to the next generation of
agricultural technology.

</details>


### [121] [XeMap: Contextual Referring in Large-Scale Remote Sensing Environments](https://arxiv.org/abs/2505.00738)
*Yuxi Li,Lu Si,Yujie Hou,Chengaung Liu,Bin Li,Hongjian Fang,Jun Zhang*

Main category: eess.IV

TL;DR: 论文提出XeMap任务及XeMap-Network，用于遥感图像中文本指代区域的中尺度语义定位，通过多模态对齐实现精准匹配，并构建专用数据集XeMap-set。


<details>
  <summary>Details</summary>
Motivation: 现有遥感图像处理方法（如图像级标注/检索或对象级检测/分割）难以捕捉大尺度场景中关键的中尺度语义实体，影响了场景的完整解读。

Method: 提出XeMap-Network，结合自注意力和跨注意力机制增强文本与图像嵌入的交互，并设计分层多尺度语义对齐（HMSA）模块实现精准的多模态匹配。

Result: 在零样本设定下，XeMap-Network表现优于现有方法，验证了其在中尺度语义实体定位上的有效性。

Conclusion: XeMap任务及网络为大尺度遥感场景的语义解读提供了新思路，填补了中尺度语义定位的空白。

Abstract: Advancements in remote sensing (RS) imagery have provided high-resolution
detail and vast coverage, yet existing methods, such as image-level
captioning/retrieval and object-level detection/segmentation, often fail to
capture mid-scale semantic entities essential for interpreting large-scale
scenes. To address this, we propose the conteXtual referring Map (XeMap) task,
which focuses on contextual, fine-grained localization of text-referred regions
in large-scale RS scenes. Unlike traditional approaches, XeMap enables precise
mapping of mid-scale semantic entities that are often overlooked in image-level
or object-level methods. To achieve this, we introduce XeMap-Network, a novel
architecture designed to handle the complexities of pixel-level cross-modal
contextual referring mapping in RS. The network includes a fusion layer that
applies self- and cross-attention mechanisms to enhance the interaction between
text and image embeddings. Furthermore, we propose a Hierarchical Multi-Scale
Semantic Alignment (HMSA) module that aligns multiscale visual features with
the text semantic vector, enabling precise multimodal matching across
large-scale RS imagery. To support XeMap task, we provide a novel, annotated
dataset, XeMap-set, specifically tailored for this task, overcoming the lack of
XeMap datasets in RS imagery. XeMap-Network is evaluated in a zero-shot setting
against state-of-the-art methods, demonstrating superior performance. This
highlights its effectiveness in accurately mapping referring regions and
providing valuable insights for interpreting large-scale RS environments.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [122] [Zoomer: Adaptive Image Focus Optimization for Black-box MLLM](https://arxiv.org/abs/2505.00742)
*Jiaxu Qian,Chendong Wang,Yifan Yang,Chaoyun Zhang,Huiqiang Jiang,Xufang Luo,Yu Kang,Qingwei Lin,Anlan Zhang,Shiqi Jiang,Ting Cao,Tianjun Mao,Suman Banerjee,Guyue Liu,Saravan Rajmohan,Dongmei Zhang,Yuqing Yang,Qi Zhang,Lili Qiu*

Main category: cs.CV

TL;DR: 本文介绍了\(SysName\)，一种新颖的视觉提示机制，旨在提升多模态大语言模型（MLLMs）的性能，同时保留关键的视觉细节。通过动态高亮图像区域、空间保留编排和预算感知提示方法，\(SysName\)在多项任务中表现优异，准确率提升达26.9%，并显著降低token消耗。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大语言模型在处理视觉数据时存在精度不足的问题，尤其是在需要精确对象识别和视觉细节的任务中。严格的token限制导致关键信息丢失，影响了模型性能。

Method: 提出了\(SysName\)视觉提示机制，包含三个关键创新：1）动态高亮相关图像区域的提示感知策略；2）保持对象完整性的空间保留编排模式；3）平衡全局上下文与关键视觉细节的预算感知提示方法。

Result: 在多个数据集上的综合评估表明，\(SysName\)显著优于基线方法，准确率最高提升26.9%，同时大幅减少token消耗。

Conclusion: \(SysName\)有效解决了视觉数据处理的精度和效率问题，为多模态大语言模型在复杂视觉任务中的应用提供了可行方案。

Abstract: Recent advancements in multimodal large language models (MLLMs) have
broadened the scope of vision-language tasks, excelling in applications like
image captioning and interactive question-answering. However, these models
struggle with accurately processing visual data, particularly in tasks
requiring precise object recognition and fine visual details. Stringent token
limits often result in the omission of critical information, hampering
performance. To address these limitations, we introduce \SysName, a novel
visual prompting mechanism designed to enhance MLLM performance while
preserving essential visual details within token limits. \SysName features
three key innovations: a prompt-aware strategy that dynamically highlights
relevant image regions, a spatial-preserving orchestration schema that
maintains object integrity, and a budget-aware prompting method that balances
global context with crucial visual details. Comprehensive evaluations across
multiple datasets demonstrate that \SysName consistently outperforms baseline
methods, achieving up to a $26.9\%$ improvement in accuracy while significantly
reducing token consumption.

</details>


### [123] [DARTer: Dynamic Adaptive Representation Tracker for Nighttime UAV Tracking](https://arxiv.org/abs/2505.00752)
*Xuzhao Li,Xuchen Li,Shiyu Hu*

Main category: cs.CV

TL;DR: DARTer是一种用于夜间无人机跟踪的端到端框架，通过动态特征融合和激活机制优化跟踪性能，平衡准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 夜间无人机跟踪面临光照变化和视角变化的挑战，现有方法计算成本高或未能充分利用动态特征。

Method: DARTer采用动态特征混合器（DFB）和动态特征激活器（DFA），结合静态和动态模板的特征，并自适应激活Vision Transformer层。

Result: 在多个夜间无人机跟踪基准测试中，DARTer表现优于现有方法，平衡了跟踪准确性和效率。

Conclusion: DARTer为现实夜间无人机跟踪提供了一种高效且准确的解决方案。

Abstract: Nighttime UAV tracking presents significant challenges due to extreme
illumination variations and viewpoint changes, which severely degrade tracking
performance. Existing approaches either rely on light enhancers with high
computational costs or introduce redundant domain adaptation mechanisms,
failing to fully utilize the dynamic features in varying perspectives. To
address these issues, we propose \textbf{DARTer} (\textbf{D}ynamic
\textbf{A}daptive \textbf{R}epresentation \textbf{T}racker), an end-to-end
tracking framework designed for nighttime UAV scenarios. DARTer leverages a
Dynamic Feature Blender (DFB) to effectively fuse multi-perspective nighttime
features from static and dynamic templates, enhancing representation
robustness. Meanwhile, a Dynamic Feature Activator (DFA) adaptively activates
Vision Transformer layers based on extracted features, significantly improving
efficiency by reducing redundant computations. Our model eliminates the need
for complex multi-task loss functions, enabling a streamlined training process.
Extensive experiments on multiple nighttime UAV tracking benchmarks demonstrate
the superiority of DARTer over state-of-the-art trackers. These results confirm
that DARTer effectively balances tracking accuracy and efficiency, making it a
promising solution for real-world nighttime UAV tracking applications.

</details>


### [124] [Multi-Modal Language Models as Text-to-Image Model Evaluators](https://arxiv.org/abs/2505.00759)
*Jiahui Chen,Candace Ross,Reyhane Askari-Hemmat,Koustuv Sinha,Melissa Hall,Michal Drozdzal,Adriana Romero-Soriano*

Main category: cs.CV

TL;DR: 论文探讨了如何利用多模态大语言模型（MLLMs）作为评估工具来评估文本生成图像（T2I）模型的性能和图像美学质量，提出了MT2IE评估框架，该框架能高效生成评估提示，并且与人类评估结果相关性更高。


<details>
  <summary>Details</summary>
Motivation: 随着文本生成图像（T2I）模型的不断改进，现有的基于静态数据集的自动评估方法已逐渐落后，研究者需要新的方法来更准确地评估T2I模型的进展。

Method: 通过多模态大语言模型（MLLMs）作为评估代理，提出了MT2IE框架，该框架通过迭代生成评估提示，并对生成的图像进行评分，以评估T2I模型的性能和一致性。

Result: MT2IE生成的提示在评估T2I模型性能时效率更高，仅使用1/80的提示数量就能获得与现有基准相同的模型排名，且其一致性评分与人类评估结果的相关性更强。

Conclusion: MT2IE提供了一种更高效、更准确的T2I模型评估方法，能够在减少评估工作量的同时，更好地反映模型的真实性能。

Abstract: The steady improvements of text-to-image (T2I) generative models lead to slow
deprecation of automatic evaluation benchmarks that rely on static datasets,
motivating researchers to seek alternative ways to evaluate the T2I progress.
In this paper, we explore the potential of multi-modal large language models
(MLLMs) as evaluator agents that interact with a T2I model, with the objective
of assessing prompt-generation consistency and image aesthetics. We present
Multimodal Text-to-Image Eval (MT2IE), an evaluation framework that iteratively
generates prompts for evaluation, scores generated images and matches T2I
evaluation of existing benchmarks with a fraction of the prompts used in
existing static benchmarks. Moreover, we show that MT2IE's prompt-generation
consistency scores have higher correlation with human judgment than scores
previously introduced in the literature. MT2IE generates prompts that are
efficient at probing T2I model performance, producing the same relative T2I
model rankings as existing benchmarks while using only 1/80th the number of
prompts for evaluation.

</details>


### [125] [P2P-Insole: Human Pose Estimation Using Foot Pressure Distribution and Motion Sensors](https://arxiv.org/abs/2505.00755)
*Atsuya Watanabe,Ratna Aisuwarya,Lei Jing*

Main category: cs.CV

TL;DR: P2P-Insole提出了一种低成本方法，通过集成IMU的鞋垫传感器估计和可视化3D人体骨骼数据，成本低于1美元，适合大规模生产。结合足压分布、加速度和旋转数据，采用Transformer模型提升复杂动作识别精度。


<details>
  <summary>Details</summary>
Motivation: 解决传统3D骨骼估计系统的高成本和侵入性问题，为康复、防伤和健康监测提供低成本、轻量级且隐私友好的解决方案。

Method: 使用e-textile技术制造低成本鞋垫，集成IMU传感器采集足压、加速度和旋转数据，结合Transformer模型及输入流的一阶、二阶导数增强时序特征提取。

Result: 实验证明多模态数据（如加速度和旋转测量）提升了复杂动作识别的准确性，误差指标显示该方法在不同姿态估计任务中具有鲁棒性。

Conclusion: P2P-Insole为低成本实用化应用（如康复）奠定了基础，未来可通过传感器优化和数据集扩展进一步改进。

Abstract: This work presents P2P-Insole, a low-cost approach for estimating and
visualizing 3D human skeletal data using insole-type sensors integrated with
IMUs. Each insole, fabricated with e-textile garment techniques, costs under
USD 1, making it significantly cheaper than commercial alternatives and ideal
for large-scale production. Our approach uses foot pressure distribution,
acceleration, and rotation data to overcome limitations, providing a
lightweight, minimally intrusive, and privacy-aware solution. The system
employs a Transformer model for efficient temporal feature extraction, enriched
by first and second derivatives in the input stream. Including multimodal
information, such as accelerometers and rotational measurements, improves the
accuracy of complex motion pattern recognition. These facts are demonstrated
experimentally, while error metrics show the robustness of the approach in
various posture estimation tasks. This work could be the foundation for a
low-cost, practical application in rehabilitation, injury prevention, and
health monitoring while enabling further development through sensor
optimization and expanded datasets.

</details>


### [126] [Efficient On-Chip Implementation of 4D Radar-Based 3D Object Detection on Hailo-8L](https://arxiv.org/abs/2505.00757)
*Woong-Chan Byun,Dong-Hee Paek,Seung-Hyun Song,Seung-Hyun Kong*

Main category: cs.CV

TL;DR: 论文提出了一种在Hailo-8L AI加速器上实现4D雷达3D目标检测的芯片上方法，通过张量变换解决了5D输入与4D硬件支持的矛盾，达到实时处理和高精度。


<details>
  <summary>Details</summary>
Motivation: 4D雷达在恶劣天气下实现稳健的3D目标检测，但需在低功耗嵌入式环境中实时处理，为此研究首次在Hailo-8L上部署此类模型。

Method: 通过张量变换将5D输入重塑为4D格式，保持模型结构不变，实现在Hailo-8L上的直接部署。

Result: 系统达到46.47% AP_3D和52.75% AP_BEV，推理速度为13.76 Hz，精度与GPU模型相当。

Conclusion: 证明4D雷达感知技术可应用于自动驾驶系统，实现了高效、低功耗的实时处理。

Abstract: 4D radar has attracted attention in autonomous driving due to its ability to
enable robust 3D object detection even under adverse weather conditions. To
practically deploy such technologies, it is essential to achieve real-time
processing within low-power embedded environments. Addressing this, we present
the first on-chip implementation of a 4D radar-based 3D object detection model
on the Hailo-8L AI accelerator. Although conventional 3D convolutional neural
network (CNN) architectures require 5D inputs, the Hailo-8L only supports 4D
tensors, posing a significant challenge. To overcome this limitation, we
introduce a tensor transformation method that reshapes 5D inputs into 4D
formats during the compilation process, enabling direct deployment without
altering the model structure. The proposed system achieves 46.47% AP_3D and
52.75% AP_BEV, maintaining comparable accuracy to GPU-based models while
achieving an inference speed of 13.76 Hz. These results demonstrate the
applicability of 4D radar-based perception technologies to autonomous driving
systems.

</details>


### [127] [Evaluating Vision Language Model Adaptations for Radiology Report Generation in Low-Resource Languages](https://arxiv.org/abs/2505.01096)
*Marco Salmè,Rosa Sicilia,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TL;DR: 该研究评估了指令调优的视觉语言模型（VLMs）在生成低资源语言（意大利语、德语和西班牙语）放射学报告中的表现，发现语言特定模型表现最佳，强调语言适应和领域特定训练的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏同时具备医学领域知识和低资源语言能力的AI模型，研究旨在填补这一空白，提升多语言环境下放射学报告的质量和准确性。

Method: 采用LLaVA架构，通过通用数据集、领域特定数据集和低资源语言特定数据集，系统评估预训练模型，并分析不同适应的有效性。

Result: 语言特定模型显著优于通用和领域特定模型；结合医学术语的微调进一步提升了性能；温度参数对报告连贯性有影响。

Conclusion: 研究强调了语言适应和领域特定训练的关键作用，为未来模型调优和多语言适配提供了方向。

Abstract: The integration of artificial intelligence in healthcare has opened new
horizons for improving medical diagnostics and patient care. However,
challenges persist in developing systems capable of generating accurate and
contextually relevant radiology reports, particularly in low-resource
languages. In this study, we present a comprehensive benchmark to evaluate the
performance of instruction-tuned Vision-Language Models (VLMs) in the
specialized task of radiology report generation across three low-resource
languages: Italian, German, and Spanish. Employing the LLaVA architectural
framework, we conducted a systematic evaluation of pre-trained models utilizing
general datasets, domain-specific datasets, and low-resource language-specific
datasets. In light of the unavailability of models that possess prior knowledge
of both the medical domain and low-resource languages, we analyzed various
adaptations to determine the most effective approach for these contexts. The
results revealed that language-specific models substantially outperformed both
general and domain-specific models in generating radiology reports, emphasizing
the critical role of linguistic adaptation. Additionally, models fine-tuned
with medical terminology exhibited enhanced performance across all languages
compared to models with generic knowledge, highlighting the importance of
domain-specific training. We also explored the influence of the temperature
parameter on the coherence of report generation, providing insights for optimal
model settings. Our findings highlight the importance of tailored language and
domain-specific training for improving the quality and accuracy of radiological
reports in multilingual settings. This research not only advances our
understanding of VLMs adaptability in healthcare but also points to significant
avenues for future investigations into model tuning and language-specific
adaptations.

</details>


### [128] [CDFormer: Cross-Domain Few-Shot Object Detection Transformer Against Feature Confusion](https://arxiv.org/abs/2505.00938)
*Boyuan Meng,Xiaohan Zhang,Peilin Li,Zhe Wu,Yiming Li,Wenkai Zhao,Beinan Yu,Hui-Liang Shen*

Main category: cs.CV

TL;DR: CDFormer提出了一种针对跨域少样本目标检测的方法，通过两个模块（OBD和OOD）解决特征混淆问题，实验显示其性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 跨域少样本目标检测（CD-FSOD）中，特征混淆（包括目标-背景混淆和目标-目标混淆）是主要挑战，需提出有效解决方案。

Method: 采用CDFormer模型，包含两个关键模块：目标-背景区分（OBD）和目标-目标区分（OOD），分别通过可学习的背景标记和增强类别区分来解决特征混淆。

Result: 实验结果显示，CDFormer在1/5/10 shot设置下分别实现了12.9%、11.0%和10.4%的mAP提升，性能显著优于现有方法。

Conclusion: CDFormer通过解决特征混淆问题，在跨域少样本目标检测任务中取得了显著性能提升，验证了其有效性。

Abstract: Cross-domain few-shot object detection (CD-FSOD) aims to detect novel objects
across different domains with limited class instances. Feature confusion,
including object-background confusion and object-object confusion, presents
significant challenges in both cross-domain and few-shot settings. In this
work, we introduce CDFormer, a cross-domain few-shot object detection
transformer against feature confusion, to address these challenges. The method
specifically tackles feature confusion through two key modules:
object-background distinguishing (OBD) and object-object distinguishing (OOD).
The OBD module leverages a learnable background token to differentiate between
objects and background, while the OOD module enhances the distinction between
objects of different classes. Experimental results demonstrate that CDFormer
outperforms previous state-of-the-art approaches, achieving 12.9% mAP, 11.0%
mAP, and 10.4% mAP improvements under the 1/5/10 shot settings, respectively,
when fine-tuned.

</details>


### [129] [Fine-Tuning Without Forgetting: Adaptation of YOLOv8 Preserves COCO Performance](https://arxiv.org/abs/2505.01016)
*Vishal Gandhi,Sagar Gandhi*

Main category: cs.CV

TL;DR: 该论文通过系统实验研究了预训练目标检测模型在细粒度任务中的微调深度问题，发现解冻更深的骨干层（如第10层）能显著提升目标任务的性能（如mAP50提升10%），且对原始任务的影响极小（<0.1% mAP差异）。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索如何在微调预训练模型以适应细粒度任务时，平衡任务性能提升与避免灾难性遗忘（即保留原始通用能力）。

Method: 采用YOLOv8n模型，通过在细粒度水果检测数据集上逐步解冻骨干层（解冻至第22、15、10层）并训练，同时在目标数据集和原始COCO验证集（通过双头评估架构）上评估性能。

Result: 结果显示，解冻更深层（如第10层）在目标任务上显著提升性能（mAP50 +10%），而对COCO基准任务影响极小（<0.1% mAP差异）。

Conclusion: 结论指出，解冻中后期骨干层能高效适配细粒度任务，且不会导致灾难性遗忘，建议在复杂领域或需要最大化专业性能时采用更深层的微调策略。

Abstract: The success of large pre-trained object detectors hinges on their
adaptability to diverse downstream tasks. While fine-tuning is the standard
adaptation method, specializing these models for challenging fine-grained
domains necessitates careful consideration of feature granularity. The critical
question remains: how deeply should the pre-trained backbone be fine-tuned to
optimize for the specialized task without incurring catastrophic forgetting of
the original general capabilities? Addressing this, we present a systematic
empirical study evaluating the impact of fine-tuning depth. We adapt a standard
YOLOv8n model to a custom, fine-grained fruit detection dataset by
progressively unfreezing backbone layers (freeze points at layers 22, 15, and
10) and training. Performance was rigorously evaluated on both the target fruit
dataset and, using a dual-head evaluation architecture, on the original COCO
validation set. Our results demonstrate unequivocally that deeper fine-tuning
(unfreezing down to layer 10) yields substantial performance gains (e.g., +10\%
absolute mAP50) on the fine-grained fruit task compared to only training the
head. Strikingly, this significant adaptation and specialization resulted in
negligible performance degradation (<0.1\% absolute mAP difference) on the COCO
benchmark across all tested freeze levels. We conclude that adapting
mid-to-late backbone features is highly effective for fine-grained
specialization. Critically, our results demonstrate this adaptation can be
achieved without the commonly expected penalty of catastrophic forgetting,
presenting a compelling case for exploring deeper fine-tuning strategies,
particularly when targeting complex domains or when maximizing specialized
performance is paramount.

</details>


### [130] [Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation](https://arxiv.org/abs/2505.01091)
*Daniele Molino,Francesco di Feola,Linlin Shen,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TL;DR: 该论文提出了一个专为多模态医学数据生成设计的框架，特别针对胸部X光和临床报告，通过MIMIC-CXR数据集展示了高保真图像和语义连贯报告的生成能力，并在疾病分类任务中表现出与真实数据相当甚至更优的性能。


<details>
  <summary>Details</summary>
Motivation: 生成模型在AI多模态应用中表现出色，但在医学领域的应用面临独特挑战，如数据复杂性和临床准确性要求。本文旨在解决通用视觉语言模型与医疗领域特殊需求之间的差距。

Method: 通过设计一个框架，结合MIMIC-CXR数据集，实现多视角胸部X光和临床报告的联合生成。

Result: 定量评估显示在FID和BLEU分数上表现优异，生成的数据在下游疾病分类任务中达到与真实数据相当或更好的性能。

Conclusion: 该研究强调了领域特定适配在提升生成模型临床相关性和实用性中的重要性，为未来合成多模态医学数据生成的研究奠定了基础。

Abstract: Generative models have revolutionized Artificial Intelligence (AI),
particularly in multimodal applications. However, adapting these models to the
medical domain poses unique challenges due to the complexity of medical data
and the stringent need for clinical accuracy. In this work, we introduce a
framework specifically designed for multimodal medical data generation. By
enabling the generation of multi-view chest X-rays and their associated
clinical report, it bridges the gap between general-purpose vision-language
models and the specialized requirements of healthcare. Leveraging the MIMIC-CXR
dataset, the proposed framework shows superior performance in generating
high-fidelity images and semantically coherent reports. Our quantitative
evaluation reveals significant results in terms of FID and BLEU scores,
showcasing the quality of the generated data. Notably, our framework achieves
comparable or even superior performance compared to real data on downstream
disease classification tasks, underlining its potential as a tool for medical
research and diagnostics. This study highlights the importance of
domain-specific adaptations in enhancing the relevance and utility of
generative models for clinical applications, paving the way for future
advancements in synthetic multimodal medical data generation.

</details>


### [131] [Self-Supervision Enhances Instance-based Multiple Instance Learning Methods in Digital Pathology: A Benchmark Study](https://arxiv.org/abs/2505.01109)
*Ali Mammadov,Loic Le Folgoc,Julien Adam,Anne Buronfosse,Gilles Hayem,Guillaume Hocquet,Pietro Gori*

Main category: cs.CV

TL;DR: 通过710次实验比较多种MIL策略，发现基于实例的MIL方法在优质自监督学习特征提取器下性能可媲美或优于复杂的嵌入方法，且更具可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究为何许多作者仍推崇基于嵌入的MIL方法，尽管自监督学习显著提升了特征质量。

Method: 在4个数据集上比较10种MIL策略，包括新引入的4种基于实例的MIL方法，结合6种自监督方法和多类骨干网络。

Result: 实验显示，基于实例的MIL方法在BRACS和Camelyon16数据集上达到新SOTA性能，且参数更少。

Conclusion: 建议将更多精力投入适配自监督学习方法，而非复杂的嵌入MIL方法，因其简单且更易解释。

Abstract: Multiple Instance Learning (MIL) has emerged as the best solution for Whole
Slide Image (WSI) classification. It consists of dividing each slide into
patches, which are treated as a bag of instances labeled with a global label.
MIL includes two main approaches: instance-based and embedding-based. In the
former, each patch is classified independently, and then the patch scores are
aggregated to predict the bag label. In the latter, bag classification is
performed after aggregating patch embeddings. Even if instance-based methods
are naturally more interpretable, embedding-based MILs have usually been
preferred in the past due to their robustness to poor feature extractors.
However, recently, the quality of feature embeddings has drastically increased
using self-supervised learning (SSL). Nevertheless, many authors continue to
endorse the superiority of embedding-based MIL. To investigate this further, we
conduct 710 experiments across 4 datasets, comparing 10 MIL strategies, 6
self-supervised methods with 4 backbones, 4 foundation models, and various
pathology-adapted techniques. Furthermore, we introduce 4 instance-based MIL
methods never used before in the pathology domain. Through these extensive
experiments, we show that with a good SSL feature extractor, simple
instance-based MILs, with very few parameters, obtain similar or better
performance than complex, state-of-the-art (SOTA) embedding-based MIL methods,
setting new SOTA results on the BRACS and Camelyon16 datasets. Since simple
instance-based MIL methods are naturally more interpretable and explainable to
clinicians, our results suggest that more effort should be put into
well-adapted SSL methods for WSI rather than into complex embedding-based MIL
methods.

</details>


### [132] [TSTMotion: Training-free Scene-awarenText-to-motion Generation](https://arxiv.org/abs/2505.01182)
*Ziyan Guo,Haoxuan Qu,Hossein Rahmani,Dewen Soh,Ping Hu,Qiuhong Ke,Jun Liu*

Main category: cs.CV

TL;DR: TSTMotion是一个无需训练的场景感知文本到动作生成框架，它通过利用预训练的空白背景动作生成器和基础模型，实现在3D场景中生成符合描述的动作序列。


<details>
  <summary>Details</summary>
Motivation: 现有场景感知方法依赖大规模真实动作数据，成本高昂，因此提出一种无需训练的方法，以降低数据需求。

Method: 结合基础模型和预训练生成器，通过场景和文本描述生成动作引导，并整合到生成器中。

Result: 实验证明框架有效且通用，代码已公开。

Conclusion: TSTMotion为场景感知文本到动作生成提供了高效且低成本的解决方案。

Abstract: Text-to-motion generation has recently garnered significant research
interest, primarily focusing on generating human motion sequences in blank
backgrounds. However, human motions commonly occur within diverse 3D scenes,
which has prompted exploration into scene-aware text-to-motion generation
methods. Yet, existing scene-aware methods often rely on large-scale
ground-truth motion sequences in diverse 3D scenes, which poses practical
challenges due to the expensive cost. To mitigate this challenge, we are the
first to propose a \textbf{T}raining-free \textbf{S}cene-aware
\textbf{T}ext-to-\textbf{Motion} framework, dubbed as \textbf{TSTMotion}, that
efficiently empowers pre-trained blank-background motion generators with the
scene-aware capability. Specifically, conditioned on the given 3D scene and
text description, we adopt foundation models together to reason, predict and
validate a scene-aware motion guidance. Then, the motion guidance is
incorporated into the blank-background motion generators with two
modifications, resulting in scene-aware text-driven motion sequences. Extensive
experiments demonstrate the efficacy and generalizability of our proposed
framework. We release our code in \href{https://tstmotion.github.io/}{Project
Page}.

</details>


### [133] [Detection and Classification of Diseases in Multi-Crop Leaves using LSTM and CNN Models](https://arxiv.org/abs/2505.00741)
*Srinivas Kanakala,Sneha Ningappa*

Main category: cs.CV

TL;DR: 该论文提出了一种基于CNN和LSTM的深度学习方法，用于植物叶片病害分类，实验结果表明CNN模型在验证集上达到96.4%的准确率。


<details>
  <summary>Details</summary>
Motivation: 植物病害严重威胁农业生产，降低作物产量和品质，早期检测和分类对于减少损失和改善作物管理至关重要。

Method: 研究采用CNN和LSTM模型，基于包含70,295张训练图像和17,572张验证图像的数据集，使用Adam优化器和分类交叉熵损失函数进行训练。

Result: CNN模型在10个训练周期后，训练准确率达99.1%，验证准确率达96.4%；LSTM模型验证准确率为93.43%。性能通过精确率、召回率、F1分数和混淆矩阵验证。

Conclusion: 深度学习模型（尤其是CNN）能提供高准确性和可扩展性的植物病害分类方案，适用于农业监测的实际应用。

Abstract: Plant diseases pose a serious challenge to agriculture by reducing crop yield
and affecting food quality. Early detection and classification of these
diseases are essential for minimising losses and improving crop management
practices. This study applies Convolutional Neural Networks (CNN) and Long
Short-Term Memory (LSTM) models to classify plant leaf diseases using a dataset
containing 70,295 training images and 17,572 validation images across 38
disease classes. The CNN model was trained using the Adam optimiser with a
learning rate of 0.0001 and categorical cross-entropy as the loss function.
After 10 training epochs, the model achieved a training accuracy of 99.1% and a
validation accuracy of 96.4%. The LSTM model reached a validation accuracy of
93.43%. Performance was evaluated using precision, recall, F1-score, and
confusion matrix, confirming the reliability of the CNN-based approach. The
results suggest that deep learning models, particularly CNN, enable an
effective solution for accurate and scalable plant disease classification,
supporting practical applications in agricultural monitoring.

</details>


### [134] [Responsive DNN Adaptation for Video Analytics against Environment Shift via Hierarchical Mobile-Cloud Collaborations](https://arxiv.org/abs/2505.00745)
*Maozhe Zhao,Shengzhong Liu,Fan Wu,Guihai Chen*

Main category: cs.CV

TL;DR: MOCHA是一个优化移动和云端协作的框架，通过分层协作减少模型适应延迟，提升响应速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有云端模型适应框架在环境变化时性能下降和响应延迟的问题。

Method: 采用层次化协作策略，包括本地模型重用、快速微调、结构化分类加速检索，以及本地缓存预取模型权重。

Result: 在三个DNN任务中，MOCHA将适应期间的模型准确率提升6.8%，响应延迟和重训练时间分别减少35.5倍和3倍。

Conclusion: MOCHA通过移动与云端协作显著提升了模型适应的效率和准确性，适用于多变部署环境。

Abstract: Mobile video analysis systems often encounter various deploying environments,
where environment shifts present greater demands for responsiveness in
adaptations of deployed "expert DNN models". Existing model adaptation
frameworks primarily operate in a cloud-centric way, exhibiting degraded
performance during adaptation and delayed reactions to environment shifts.
Instead, this paper proposes MOCHA, a novel framework optimizing the
responsiveness of continuous model adaptation through hierarchical
collaborations between mobile and cloud resources. Specifically, MOCHA (1)
reduces adaptation response delays by performing on-device model reuse and fast
fine-tuning before requesting cloud model retrieval and end-to-end retraining;
(2) accelerates history expert model retrieval by organizing them into a
structured taxonomy utilizing domain semantics analyzed by a cloud foundation
model as indices; (3) enables efficient local model reuse by maintaining
onboard expert model caches for frequent scenes, which proactively prefetch
model weights from the cloud model database. Extensive evaluations with
real-world videos on three DNN tasks show MOCHA improves the model accuracy
during adaptation by up to 6.8% while saving the response delay and retraining
time by up to 35.5x and 3.0x respectively.

</details>


### [135] [Multimodal Doctor-in-the-Loop: A Clinically-Guided Explainable Framework for Predicting Pathological Response in Non-Small Cell Lung Cancer](https://arxiv.org/abs/2505.01390)
*Alice Natalina Caragliano,Claudia Tacconi,Carlo Greco,Lorenzo Nibid,Edy Ippolito,Michele Fiore,Giuseppe Perrone,Sara Ramella,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TL;DR: 论文提出了一种结合多模态深度学习和内在可解释人工智能（XAI）技术的新方法，用于预测非小细胞肺癌患者在辅助治疗中的病理反应。


<details>
  <summary>Details</summary>
Motivation: 现有的影像组学和单模态深度学习方法存在局限性，无法高效整合多模态数据（如影像和临床数据）。

Method: 采用中间融合策略整合多模态数据，并引入‘医生在环’方法，将临床医生的领域知识嵌入训练过程，指导模型从整体到病变区域的逐步聚焦。

Result: 实验结果表明，该方法提高了预测准确性和可解释性，为临床数据整合策略提供了新见解。

Conclusion: 研究成功开发了一种更具临床相关性的多模态预测模型，为精准医疗提供了新工具。

Abstract: This study proposes a novel approach combining Multimodal Deep Learning with
intrinsic eXplainable Artificial Intelligence techniques to predict
pathological response in non-small cell lung cancer patients undergoing
neoadjuvant therapy. Due to the limitations of existing radiomics and unimodal
deep learning approaches, we introduce an intermediate fusion strategy that
integrates imaging and clinical data, enabling efficient interaction between
data modalities. The proposed Multimodal Doctor-in-the-Loop method further
enhances clinical relevance by embedding clinicians' domain knowledge directly
into the training process, guiding the model's focus gradually from broader
lung regions to specific lesions. Results demonstrate improved predictive
accuracy and explainability, providing insights into optimal data integration
strategies for clinical applications.

</details>


### [136] [Transferable Adversarial Attacks on Black-Box Vision-Language Models](https://arxiv.org/abs/2505.01050)
*Kai Hu,Weichen Yu,Li Zhang,Alexander Robey,Andy Zou,Chengming Xu,Haoqi Hu,Matt Fredrikson*

Main category: cs.CV

TL;DR: 该论文分析了视觉大语言模型（VLLMs）的对抗攻击可转移性，证明目标对抗样本可有效攻击GPT-4o、Claude和Gemini等主流专有模型，揭示了模型在对象识别、视觉问答和图像描述任务中的普遍脆弱性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索VLLMs在文本与图像混合输入下的对抗攻击漏洞，填补现有研究空白，并评估专有模型的安全性。

Method: 方法包括生成目标对抗样本和通用扰动，通过实验验证这些攻击在对象识别、视觉问答和图像描述任务中的有效性。

Result: 实验结果表明，对抗样本和通用扰动能诱导专有VLLMs产生攻击者预期的错误解读，如将危险内容误判为安全或忽略敏感信息。

Conclusion: 结论指出当前VLLMs普遍存在对抗攻击脆弱性，需开发鲁棒性缓解措施以确保模型安全部署。

Abstract: Vision Large Language Models (VLLMs) are increasingly deployed to offer
advanced capabilities on inputs comprising both text and images. While prior
research has shown that adversarial attacks can transfer from open-source to
proprietary black-box models in text-only and vision-only contexts, the extent
and effectiveness of such vulnerabilities remain underexplored for VLLMs. We
present a comprehensive analysis demonstrating that targeted adversarial
examples are highly transferable to widely-used proprietary VLLMs such as
GPT-4o, Claude, and Gemini. We show that attackers can craft perturbations to
induce specific attacker-chosen interpretations of visual information, such as
misinterpreting hazardous content as safe, overlooking sensitive or restricted
material, or generating detailed incorrect responses aligned with the
attacker's intent. Furthermore, we discover that universal perturbations --
modifications applicable to a wide set of images -- can consistently induce
these misinterpretations across multiple proprietary VLLMs. Our experimental
results on object recognition, visual question answering, and image captioning
show that this vulnerability is common across current state-of-the-art models,
and underscore an urgent need for robust mitigations to ensure the safe and
secure deployment of VLLMs.

</details>


### [137] [Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of Multimodal LLMs](https://arxiv.org/abs/2505.01064)
*Hari Chandana Kuchibhotla,Sai Srinivas Kancheti,Abbavaram Gowtham Reddy,Vineeth N Balasubramanian*

Main category: cs.CV

TL;DR: 论文提出了一种名为NeaR的新方法，用于解决无需预定义标签的细粒度视觉识别（VF-FGVR）问题，通过利用多模态大语言模型生成的标签来微调下游CLIP模型。


<details>
  <summary>Details</summary>
Motivation: 在缺乏标注数据的领域（如医学影像），细粒度视觉识别面临标签空间无约束的挑战，现有方法因成本和推理时间问题难以实用。

Method: NeaR方法通过多模态大语言模型生成弱监督标签，构建训练集并微调CLIP模型，以处理标签的噪声和开放性。

Result: NeaR为高效的VF-FGVR设立了新基准，解决了无标签空间下的识别问题。

Conclusion: NeaR通过结合大语言模型和CLIP，为无标签数据下的细粒度识别提供了实用且高效的解决方案。

Abstract: Fine-grained Visual Recognition (FGVR) involves distinguishing between
visually similar categories, which is inherently challenging due to subtle
inter-class differences and the need for large, expert-annotated datasets. In
domains like medical imaging, such curated datasets are unavailable due to
issues like privacy concerns and high annotation costs. In such scenarios
lacking labeled data, an FGVR model cannot rely on a predefined set of training
labels, and hence has an unconstrained output space for predictions. We refer
to this task as Vocabulary-Free FGVR (VF-FGVR), where a model must predict
labels from an unconstrained output space without prior label information.
While recent Multimodal Large Language Models (MLLMs) show potential for
VF-FGVR, querying these models for each test input is impractical because of
high costs and prohibitive inference times. To address these limitations, we
introduce \textbf{Nea}rest-Neighbor Label \textbf{R}efinement (NeaR), a novel
approach that fine-tunes a downstream CLIP model using labels generated by an
MLLM. Our approach constructs a weakly supervised dataset from a small,
unlabeled training set, leveraging MLLMs for label generation. NeaR is designed
to handle the noise, stochasticity, and open-endedness inherent in labels
generated by MLLMs, and establishes a new benchmark for efficient VF-FGVR.

</details>


### [138] [Fusing Foveal Fixations Using Linear Retinal Transformations and Bayesian Experimental Design](https://arxiv.org/abs/2505.01249)
*Christopher K. I. Williams*

Main category: cs.CV

TL;DR: 论文提出了一种利用几何知识将视网膜变换建模为高分辨率潜在图像的线性降采样方法，实现了对因子分析（FA）和混合FA模型中潜在变量的精确推断，并通过贝叶斯实验设计解决了‘下一步看向哪里’的问题。


<details>
  <summary>Details</summary>
Motivation: 人类（和许多脊椎动物）在观察场景时需要将多个注视点融合以获取整体表示，每个注视点使用高分辨率中央凹和递减分辨率的周边视野。论文旨在通过视网膜变换建模解决这一问题。

Method: 将视网膜变换明确表示为对场景高分辨率潜在图像的线性降采样，利用已知几何知识实现因子分析（FA）和混合FA模型中潜在变量的精确推断，并基于期望信息增益准则解决‘下一步注视点选择’问题。

Result: 在Frey面部和MNIST数据集上的实验验证了模型的有效性。

Conclusion: 通过线性变换和贝叶斯实验设计，论文成功解决了多注视点融合与最优注视点选择问题，为场景理解提供了新思路。

Abstract: Humans (and many vertebrates) face the problem of fusing together multiple
fixations of a scene in order to obtain a representation of the whole, where
each fixation uses a high-resolution fovea and decreasing resolution in the
periphery. In this paper we explicitly represent the retinal transformation of
a fixation as a linear downsampling of a high-resolution latent image of the
scene, exploiting the known geometry. This linear transformation allows us to
carry out exact inference for the latent variables in factor analysis (FA) and
mixtures of FA models of the scene. Further, this allows us to formulate and
solve the choice of "where to look next" as a Bayesian experimental design
problem using the Expected Information Gain criterion. Experiments on the Frey
faces and MNIST datasets demonstrate the effectiveness of our models.

</details>


### [139] [CAMELTrack: Context-Aware Multi-cue ExpLoitation for Online Multi-Object Tracking](https://arxiv.org/abs/2505.01257)
*Vladimir Somers,Baptiste Standaert,Victor Joos,Alexandre Alahi,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: 论文介绍了一种名为CAMEL的新型关联模块，用于上下文感知多线索利用，通过学习数据驱动的关联策略，摆脱手工设计的启发式方法，同时保持了跟踪检测方法的模块化优势。


<details>
  <summary>Details</summary>
Motivation: 现有的在线多目标跟踪方法主要依赖于跟踪检测（TbD）方法，这些方法虽然模块化设计优秀，但依赖人工设计的启发式规则进行时序关联，限制了其捕捉复杂跟踪线索的能力。

Method: CAMEL由两个基于transformer的模块组成，采用新型的关联中心训练方案，有效建模跟踪目标与各种关联线索之间的复杂交互。

Result: 提出的在线跟踪流程CAMELTrack在多个跟踪基准上实现了最先进的性能，同时保持轻量级和快速训练。

Conclusion: CAMEL通过数据驱动的关联策略，克服了传统TbD方法的限制，同时保持了其模块化优势，为多目标跟踪提供了更高效的解决方案。

Abstract: Online multi-object tracking has been recently dominated by
tracking-by-detection (TbD) methods, where recent advances rely on increasingly
sophisticated heuristics for tracklet representation, feature fusion, and
multi-stage matching. The key strength of TbD lies in its modular design,
enabling the integration of specialized off-the-shelf models like motion
predictors and re-identification. However, the extensive usage of human-crafted
rules for temporal associations makes these methods inherently limited in their
ability to capture the complex interplay between various tracking cues. In this
work, we introduce CAMEL, a novel association module for Context-Aware
Multi-Cue ExpLoitation, that learns resilient association strategies directly
from data, breaking free from hand-crafted heuristics while maintaining TbD's
valuable modularity. At its core, CAMEL employs two transformer-based modules
and relies on a novel association-centric training scheme to effectively model
the complex interactions between tracked targets and their various association
cues. Unlike end-to-end detection-by-tracking approaches, our method remains
lightweight and fast to train while being able to leverage external
off-the-shelf models. Our proposed online tracking pipeline, CAMELTrack,
achieves state-of-the-art performance on multiple tracking benchmarks. Our code
is available at https://github.com/TrackingLaboratory/CAMELTrack.

</details>


### [140] [Global Collinearity-aware Polygonizer for Polygonal Building Mapping in Remote Sensing](https://arxiv.org/abs/2505.01385)
*Fahong Zhang,Yilei Shi,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 论文提出了一种名为GCP的全局共线感知多边形化算法，用于从遥感图像中映射多边形建筑，通过结合实例分割和动态编程优化，提高了多边形生成的准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有的多边形化方法在处理遥感图像中的多边形建筑时，往往难以平衡准确性和简化性。GCP算法旨在通过全局优化和共线感知技术，解决这一问题。

Method: GCP算法基于实例分割框架，从二值掩码中提取轮廓多段线，结合Transformer回归模块优化拟合，并采用动态编程技术进行共线感知简化，生成最终多边形。

Result: GCP在两个公共基准测试中验证了有效性，且在无先验知识的情况下，其共线感知简化模块优于传统方法（如Douglas-Peucker算法）。

Conclusion: GCP通过全局优化和端到端训练，显著提升了多边形建筑的映射精度和泛化能力，展现了广泛的应用潜力。

Abstract: This paper addresses the challenge of mapping polygonal buildings from remote
sensing images and introduces a novel algorithm, the Global Collinearity-aware
Polygonizer (GCP). GCP, built upon an instance segmentation framework,
processes binary masks produced by any instance segmentation model. The
algorithm begins by collecting polylines sampled along the contours of the
binary masks. These polylines undergo a refinement process using a
transformer-based regression module to ensure they accurately fit the contours
of the targeted building instances. Subsequently, a collinearity-aware polygon
simplification module simplifies these refined polylines and generate the final
polygon representation. This module employs dynamic programming technique to
optimize an objective function that balances the simplicity and fidelity of the
polygons, achieving globally optimal solutions. Furthermore, the optimized
collinearity-aware objective is seamlessly integrated into network training,
enhancing the cohesiveness of the entire pipeline. The effectiveness of GCP has
been validated on two public benchmarks for polygonal building mapping. Further
experiments reveal that applying the collinearity-aware polygon simplification
module to arbitrary polylines, without prior knowledge, enhances accuracy over
traditional methods such as the Douglas-Peucker algorithm. This finding
underscores the broad applicability of GCP. The code for the proposed method
will be made available at https://github.com/zhu-xlab.

</details>


### [141] [VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models](https://arxiv.org/abs/2505.01406)
*Mohammadreza Teymoorianfard,Shiqing Ma,Amir Houmansadr*

Main category: cs.CV

TL;DR: VIDSTAMP 是一种新颖的视频水印框架，通过在时间感知的视频扩散模型的潜在空间中嵌入帧或片段信息，实现了高容量、低感知影响的水印嵌入，且在保持视频质量的同时提高了抗篡改能力。


<details>
  <summary>Details</summary>
Motivation: 随着视频扩散模型的快速发展，视频内容真实性和来源验证成为关键挑战。现有水印方法难以应对视频特有的帧操作，且常影响视觉质量。

Method: 采用两阶段微调策略，先在静态图像数据集上训练以分离空间信息，再在合成视频序列上恢复时间一致性，结合3D卷积和时间注意力机制。

Result: VIDSTAMP 嵌入768比特/视频（48比特/帧），比特准确率达95.0%，视频质量评分（0.836）接近无水印输出，抗篡改能力优于现有方法。

Conclusion: VIDSTAMP 通过潜在空间嵌入和时间一致性优化，实现了高质量、高抗扰的视频水印，为内容认证提供了新解决方案。

Abstract: The rapid rise of video diffusion models has enabled the generation of highly
realistic and temporally coherent videos, raising critical concerns about
content authenticity, provenance, and misuse. Existing watermarking approaches,
whether passive, post-hoc, or adapted from image-based techniques, often
struggle to withstand video-specific manipulations such as frame insertion,
dropping, or reordering, and typically degrade visual quality. In this work, we
introduce VIDSTAMP, a watermarking framework that embeds per-frame or
per-segment messages directly into the latent space of temporally-aware video
diffusion models. By fine-tuning the model's decoder through a two-stage
pipeline, first on static image datasets to promote spatial message separation,
and then on synthesized video sequences to restore temporal consistency,
VIDSTAMP learns to embed high-capacity, flexible watermarks with minimal
perceptual impact. Leveraging architectural components such as 3D convolutions
and temporal attention, our method imposes no additional inference cost and
offers better perceptual quality than prior methods, while maintaining
comparable robustness against common distortions and tampering. VIDSTAMP embeds
768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a
log P-value of -166.65 (lower is better), and maintains a video quality score
of 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior
methods in capacity-quality tradeoffs. Code: Code:
\url{https://github.com/SPIN-UMass/VidStamp}

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [142] [Reduced-order structure-property linkages for stochastic metamaterials](https://arxiv.org/abs/2505.01283)
*Hooman Danesh,Maruthi Annamaraju,Tim Brepols,Stefanie Reese,Surya R. Kalidindi*

Main category: cs.CE

TL;DR: 该论文通过主成分分析和高斯过程回归构建了机械超材料的降阶代理模型，利用少量数据点即可生成准确的结构-性能映射。


<details>
  <summary>Details</summary>
Motivation: 机械超材料的设计空间巨大，传统物理模拟计算成本高，需要高效的方法建立结构-性能关系。

Method: 结合主成分分析提取特征、快速傅里叶变换（FFT）均质化计算弹性刚度，利用高斯过程回归生成代理模型，并通过主动学习减少数据需求。

Result: 仅需0.61%的全数据集即可构建高精度、鲁棒的结构-性能映射。

Conclusion: 该方法显著降低了计算成本，为高效设计机械超材料提供了可行方案。

Abstract: The capabilities of additive manufacturing have facilitated the design and
production of mechanical metamaterials with diverse unit cell geometries.
Establishing linkages between the vast design space of unit cells and their
effective mechanical properties is critical for the efficient design and
performance evaluation of such metamaterials. However, physics-based
simulations of metamaterial unit cells across the entire design space are
computationally expensive, necessitating a materials informatics framework to
efficiently capture complex structure-property relationships. In this work,
principal component analysis of 2-point correlation functions is performed to
extract the salient features from a large dataset of randomly generated 2D
metamaterials. Physics-based simulations are performed using a fast Fourier
transform (FFT)-based homogenization approach to efficiently compute the
homogenized effective elastic stiffness across the extensive unit cell designs.
Subsequently, Gaussian process regression is used to generate reduced-order
surrogates, mapping unit cell designs to their homogenized effective elastic
constant. It is demonstrated that the adopted workflow enables a high-value
low-dimensional representation of the voluminous stochastic metamaterial
dataset, facilitating the construction of robust structure-property maps.
Finally, an uncertainty-based active learning framework is utilized to train a
surrogate model with a significantly smaller number of data points compared to
the original full dataset. It is shown that a dataset as small as $0.61\%$ of
the entire dataset is sufficient to generate accurate and robust
structure-property maps.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [143] [How much to Dereverberate? Low-Latency Single-Channel Speech Enhancement in Distant Microphone Scenarios](https://arxiv.org/abs/2505.01338)
*Satvik Venkatesh,Philip Coleman,Arthur Benilov,Simon Brown,Selim Sheta,Frederic Roskam*

Main category: eess.AS

TL;DR: 该论文探讨了在远距离麦克风场景（5至10米）下实时低延迟单通道语音增强（SE）的可行性，特别是在会议室和剧院等大空间环境中。研究表明，短衰减时间下保留早期反射可以提高信号质量。


<details>
  <summary>Details</summary>
Motivation: 尽管语音增强中的去混响是一个重要任务，但现有研究多集中于小空间、短混响时间的场景。论文旨在填补在远距离麦克风和大空间（如会议室和剧院）中对语音增强的研究空白，以满足讲座演示、戏剧等应用需求。

Method: 论文首先验证了在挑战性场景中单通道语音增强的可行性，随后研究了房间体积与混响时间的关系，并分析了这些参数对随机模拟房间脉冲响应的影响。最后提出在短衰减时间下去混响时，保留早期反射以提升信号质量的方法。

Result: 研究结果表明，在大空间和远距离麦克风场景下，单通道语音增强可以实现，且短衰减时间下保留早期反射对信号质量有显著提升。

Conclusion: 论文为远距离麦克风和大空间环境下的语音增强提供了新思路，证实了保留早期反射在去混响中的重要性，并推动了此类场景下的应用发展。

Abstract: Dereverberation is an important sub-task of Speech Enhancement (SE) to
improve the signal's intelligibility and quality. However, it remains
challenging because the reverberation is highly correlated with the signal.
Furthermore, the single-channel SE literature has predominantly focused on
rooms with short reverb times (typically under 1 second), smaller rooms (under
volumes of 1000 cubic meters) and relatively short distances (up to 2 meters).
In this paper, we explore real-time low-latency single-channel SE under distant
microphone scenarios, such as 5 to 10 meters, and focus on conference rooms and
theatres, with larger room dimensions and reverberation times. Such a setup is
useful for applications such as lecture demonstrations, drama, and to enhance
stage acoustics. First, we show that single-channel SE in such challenging
scenarios is feasible. Second, we investigate the relationship between room
volume and reverberation time, and demonstrate its importance when randomly
simulating room impulse responses. Lastly, we show that for dereverberation
with short decay times, preserving early reflections before decaying the
transfer function of the room improves overall signal quality.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [144] [To Repair or Not to Repair? Investigating the Importance of AB-Cycles for the State-of-the-Art TSP Heuristic EAX](https://arxiv.org/abs/2505.00803)
*Jonathan Heins,Darrell Whitley,Pascal Kerschke*

Main category: cs.NE

TL;DR: 本文提出了一种改进的EAX算法，通过在第一阶段快速验证AB-cycle的有效性，提升了计算效率和求解质量。


<details>
  <summary>Details</summary>
Motivation: EAX算法的第二阶段已被充分研究，但第一阶段很少被探索。本文旨在改进第一阶段，以提升整体性能。

Method: 提出了一种新方法，快速验证AB-cycle是否生成有效路径，并基于此改进EAX变体。

Result: 在10,000个TSP实例上的测试显示，改进的EAX变体在计算效率和求解质量上优于现有方法。

Conclusion: 改进后的EAX算法在解决复杂TSP实例时表现更优，验证了第一阶段优化的重要性。

Abstract: The Edge Assembly Crossover (EAX) algorithm is the state-of-the-art heuristic
for solving the Traveling Salesperson Problem (TSP). It regularly outperforms
other methods, such as the Lin-Kernighan-Helsgaun heuristic (LKH), across
diverse sets of TSP instances. Essentially, EAX employs a two-stage mechanism
that focuses on improving the current solutions, first, at the local and,
subsequently, at the global level. Although the second phase of the algorithm
has been thoroughly studied, configured, and refined in the past, in
particular, its first stage has hardly been examined.
  In this paper, we thus focus on the first stage of EAX and introduce a novel
method that quickly verifies whether the AB-cycles, generated during its
internal optimization procedure, yield valid tours -- or whether they need to
be repaired. Knowledge of the latter is also particularly relevant before
applying other powerful crossover operators such as the Generalized Partition
Crossover (GPX). Based on our insights, we propose and evaluate several
improved versions of EAX. According to our benchmark study across 10 000
different TSP instances, the most promising of our proposed EAX variants
demonstrates improved computational efficiency and solution quality on
previously rather difficult instances compared to the current state-of-the-art
EAX algorithm.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [145] [JFlow: Model-Independent Spherical Jeans Analysis using Equivariant Continuous Normalizing Flows](https://arxiv.org/abs/2505.00763)
*Sung Hak Lim,Kohei Hayashi,Shun'ichi Horigome,Shigeki Matsumoto,Mihoko M. Nojiri*

Main category: astro-ph.GA

TL;DR: 该论文提出了一种无监督机器学习方法，用于模型无关地解决球形Jeans方程，以分析矮球状星系中暗物质晕的结构。通过使用等变连续归一化流，该方法能够无模型假设地估计恒星相空间密度和速度弥散。


<details>
  <summary>Details</summary>
Motivation: 研究矮球状星系中恒星的动力学以理解暗物质晕的结构，但由于恒星的运动学信息通常局限于天球位置和视线速度，传统的基于模型的相空间分析方法显得有限。

Method: 使用无监督机器学习方法，特别是等变连续归一化流，无需模型假设即可解决球形Jeans方程。

Result: 该方法在Gaia挑战数据集上展示了其有效性，能够准确估计暗物质质量密度，即使星追踪数量较少。

Conclusion: 该方法为矮球状星系的模型无关分析提供了新的途径，展示了对暗物质晕结构的高效探测能力。

Abstract: The kinematics of stars in dwarf spheroidal galaxies have been studied to
understand the structure of dark matter halos. However, the kinematic
information of these stars is often limited to celestial positions and
line-of-sight velocities, making full phase space analysis challenging.
Conventional methods rely on projected analytic phase space density models with
several parameters and infer dark matter halo structures by solving the
spherical Jeans equation. In this paper, we introduce an unsupervised machine
learning method for solving the spherical Jeans equation in a model-independent
way as a first step toward model-independent analysis of dwarf spheroidal
galaxies. Using equivariant continuous normalizing flows, we demonstrate that
spherically symmetric stellar phase space densities and velocity dispersions
can be estimated without model assumptions. As a proof of concept, we apply our
method to Gaia challenge datasets for spherical models and measure dark matter
mass densities given velocity anisotropy profiles. Our method can identify halo
structures accurately, even with a small number of tracer stars.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [146] [Enhancing SPARQL Query Rewriting for Complex Ontology Alignments](https://arxiv.org/abs/2505.01309)
*Anicet Lepetit Ondo,Laurence Capus,Mamadou Bousso*

Main category: cs.DB

TL;DR: 该论文提出了一种基于自然语言表达和大型语言模型的SPARQL查询重写方法，以高效处理复杂的本体对齐问题，尤其是(c:c)对应关系，同时为非专业用户提供便捷的异构数据查询解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有SPARQL查询重写方法主要关注简单和部分复杂的本体对齐，忽略了更复杂对齐（如c:c）的挑战，且SPARQL语法复杂，限制了非专业用户的使用。

Method: 结合等价传递原理和大型语言模型（如GPT-4），通过自然语言表达用户需求，实现源本体到目标本体的SPARQL查询自动重写。

Result: 该方法能高效处理复杂对齐问题，尤其是(c:c)对应关系，并降低非专业用户使用SPARQL的门槛。

Conclusion: 该方法为异构本体查询提供了一种灵活且高效的解决方案，尤其适合复杂对齐和非专业用户场景。

Abstract: SPARQL query rewriting is a fundamental mechanism for uniformly querying
heterogeneous ontologies in the Linked Data Web. However, the complexity of
ontology alignments, particularly rich correspondences (c : c), makes this
process challenging. Existing approaches primarily focus on simple (s : s) and
partially complex ( s : c) alignments, thereby overlooking the challenges posed
by more expressive alignments. Moreover, the intricate syntax of SPARQL
presents a barrier for non-expert users seeking to fully exploit the knowledge
encapsulated in ontologies. This article proposes an innovative approach for
the automatic rewriting of SPARQL queries from a source ontology to a target
ontology, based on a user's need expressed in natural language. It leverages
the principles of equivalence transitivity as well as the advanced capabilities
of large language models such as GPT-4. By integrating these elements, this
approach stands out for its ability to efficiently handle complex alignments,
particularly (c : c) correspondences , by fully exploiting their
expressiveness. Additionally, it facilitates access to aligned ontologies for
users unfamiliar with SPARQL, providing a flexible solution for querying
heterogeneous data.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [147] [Artificial Intelligence in Government: Why People Feel They Lose Control](https://arxiv.org/abs/2505.01085)
*Alexander Wuttke,Adrian Rauchfleisch,Andreas Jungherr*

Main category: cs.CY

TL;DR: 该论文探讨了AI在公共行政中的应用及其对公平性、透明度和问责制的影响，通过委托代理理论分析AI采用的核心问题，并通过实验验证效率提升与公众信任和控制感之间的关系。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解AI在政府职能中快速整合的潜在风险，尤其是对民主合法性的长期影响。

Method: 采用委托代理理论（PAT）作为框架，结合预注册的因子调查实验，测试了AI在税务、福利和执法领域的应用效果。

Result: 研究发现，尽管AI提高了效率并短暂增强了信任，但同时也降低了公众的控制感；当结构风险显现时，公众信任和控制感急剧下降。

Conclusion: 结论指出PAT为理解AI在政府中的政治和制度影响提供了有力视角，并强调政策制定者需透明处理委托风险以维持公众信任。

Abstract: The use of Artificial Intelligence (AI) in public administration is expanding
rapidly, moving from automating routine tasks to deploying generative and
agentic systems that autonomously act on goals. While AI promises greater
efficiency and responsiveness, its integration into government functions raises
concerns about fairness, transparency, and accountability. This article applies
principal-agent theory (PAT) to conceptualize AI adoption as a special case of
delegation, highlighting three core tensions: assessability (can decisions be
understood?), dependency (can the delegation be reversed?), and contestability
(can decisions be challenged?). These structural challenges may lead to a
"failure-by-success" dynamic, where early functional gains obscure long-term
risks to democratic legitimacy. To test this framework, we conducted a
pre-registered factorial survey experiment across tax, welfare, and law
enforcement domains. Our findings show that although efficiency gains initially
bolster trust, they simultaneously reduce citizens' perceived control. When the
structural risks come to the foreground, institutional trust and perceived
control both drop sharply, suggesting that hidden costs of AI adoption
significantly shape public attitudes. The study demonstrates that PAT offers a
powerful lens for understanding the institutional and political implications of
AI in government, emphasizing the need for policymakers to address delegation
risks transparently to maintain public trust.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [148] [Multi-site modelling and reconstruction of past extreme skew surges along the French Atlantic coast](https://arxiv.org/abs/2505.00835)
*Nathan Huet,Philippe Naveau,Anne Sabourin*

Main category: stat.AP

TL;DR: 该研究旨在通过新的阈值确定方法和多元极值建模技术，利用法国大西洋沿岸站点的极端偏斜涌浪数据，重建历史数据短缺站点的涌浪时间序列。


<details>
  <summary>Details</summary>
Motivation: 准确建模极端偏斜涌浪对沿海风险管理至关重要，尤其需要探究站点间的极值依赖结构。

Method: 采用超阈值框架，结合多元广义帕累托分布和新型极端回归框架，预测站点间极端涌浪。

Result: 研究提出了新的阈值确定方法，并验证了多元建模和极端回归在预测极端涌浪中的有效性。

Conclusion: 通过整合长期观测站数据，该方法成功重建了数据有限站点的历史涌浪序列，为沿海风险管理提供了可靠工具。

Abstract: Appropriate modelling of extreme skew surges is crucial, particularly for
coastal risk management. Our study focuses on modelling extreme skew surges
along the French Atlantic coast, with a particular emphasis on investigating
the extremal dependence structure between stations. We employ the
peak-over-threshold framework, where a multivariate extreme event is defined
whenever at least one location records a large value, though not necessarily
all stations simultaneously. A novel method for determining an appropriate
level (threshold) above which observations can be classified as extreme is
proposed. Two complementary approaches are explored. First, the multivariate
generalized Pareto distribution is employed to model extremes, leveraging its
properties to derive a generative model that predicts extreme skew surges at
one station based on observed extremes at nearby stations. Second, a novel
extreme regression framework is assessed for point predictions. This specific
regression framework enables accurate point predictions using only the "angle"
of input variables, i.e. input variables divided by their norms. The ultimate
objective is to reconstruct historical skew surge time series at stations with
limited data. This is achieved by integrating extreme skew surge data from
stations with longer records, such as Brest and Saint-Nazaire, which provide
over 150 years of observations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [149] [Dynamic and Distributed Routing in IoT Networks based on Multi-Objective Q-Learning](https://arxiv.org/abs/2505.00918)
*Shubham Vaishnav,Praveen Kumar Donta,Sindri Magnússon*

Main category: cs.DC

TL;DR: 针对IoT网络中动态优先级的需求，提出了一种基于多目标Q学习的动态分布式路由算法，能实时适应优先级变化，并在模拟中表现优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 由于IoT应用中优先级需求多变（如时延敏感与能耗敏感的权衡），现有静态优化方法无法满足动态需求，因此需设计实时适应能力强的路由方案。

Method: 结合多目标优化和Q学习，提出动态分布式路由算法，并引入贪婪插值策略以应对突发的优先级变化，利用帕累托最优解快速调整。

Result: 模拟测试表明，该算法在奖励、能效和包交付率等指标上优于现有方法，适用于不同探索策略和偏好变化模式。

Conclusion: 所提算法通过动态适应能力有效解决了IoT网络中的多目标矛盾，为实时优先级调整提供了实用解决方案。

Abstract: The last few decades have witnessed a rapid increase in IoT devices owing to
their wide range of applications, such as smart healthcare monitoring systems,
smart cities, and environmental monitoring. A critical task in IoT networks is
sensing and transmitting information over the network. The IoT nodes gather
data by sensing the environment and then transmit this data to a destination
node via multi-hop communication, following some routing protocols. These
protocols are usually designed to optimize possibly contradictory objectives,
such as maximizing packet delivery ratio and energy efficiency. While most
literature has focused on optimizing a static objective that remains unchanged,
many real-world IoT applications require adapting to rapidly shifting
priorities. For example, in monitoring systems, some transmissions are
time-critical and require a high priority on low latency, while other
transmissions are less urgent and instead prioritize energy efficiency. To meet
such dynamic demands, we propose novel dynamic and distributed routing based on
multiobjective Q-learning that can adapt to changes in preferences in
real-time. Our algorithm builds on ideas from both multi-objective optimization
and Q-learning. We also propose a novel greedy interpolation policy scheme to
take near-optimal decisions for unexpected preference changes. The proposed
scheme can approximate and utilize the Pareto-efficient solutions for dynamic
preferences, thus utilizing past knowledge to adapt to unpredictable
preferences quickly during runtime. Simulation results show that the proposed
scheme outperforms state-of-the-art algorithms for various exploration
strategies, preference variation patterns, and important metrics like overall
reward, energy efficiency, and packet delivery ratio.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [150] [Towards Explainable Temporal User Profiling with LLMs](https://arxiv.org/abs/2505.00886)
*Milad Sabouri,Masoud Mansoury,Kun Lin,Bamshad Mobasher*

Main category: cs.IR

TL;DR: 这篇论文提出了一种使用大语言模型（LLMs）生成用户交互历史的自然语言摘要的方法，通过区分近期行为和长期偏好动态融合短期和长期嵌入，从而提升推荐系统的准确性和解释性。


<details>
  <summary>Details</summary>
Motivation: 传统用户画像方法（如平均项目嵌入）通常忽略用户兴趣的动态性和细微差异，尤其是短期与长期偏好的交互。本研究旨在通过自然语言摘要和注意力机制更准确地建模用户偏好，同时增强推荐系统的透明性。

Method: 利用LLMs生成用户交互历史的自然语言摘要，区分近期行为和长期偏好；通过预训练模型编码文本画像，并使用注意力机制动态融合短期和长期嵌入，生成全面的用户表示。

Result: 在真实数据集上的实验表明，该方法在推荐准确性上优于多个基线，并能生成更清晰、透明的推荐理由。

Conclusion: 该框架不仅提升了推荐性能，还通过自然语言摘要和注意力权重增强了系统可解释性，为基于内容的推荐提供了更透明的依据。

Abstract: Accurately modeling user preferences is vital not only for improving
recommendation performance but also for enhancing transparency in recommender
systems. Conventional user profiling methods, such as averaging item
embeddings, often overlook the evolving, nuanced nature of user interests,
particularly the interplay between short-term and long-term preferences. In
this work, we leverage large language models (LLMs) to generate natural
language summaries of users' interaction histories, distinguishing recent
behaviors from more persistent tendencies. Our framework not only models
temporal user preferences but also produces natural language profiles that can
be used to explain recommendations in an interpretable manner. These textual
profiles are encoded via a pre-trained model, and an attention mechanism
dynamically fuses the short-term and long-term embeddings into a comprehensive
user representation. Beyond boosting recommendation accuracy over multiple
baselines, our approach naturally supports explainability: the interpretable
text summaries and attention weights can be exposed to end users, offering
insights into why specific items are suggested. Experiments on real-world
datasets underscore both the performance gains and the promise of generating
clearer, more transparent justifications for content-based recommendations.

</details>


### [151] [Preserving Privacy and Utility in LLM-Based Product Recommendations](https://arxiv.org/abs/2505.00951)
*Tina Khezresmaeilzadeh,Jiang Zhang,Dimitrios Andreadis,Konstantinos Psounis*

Main category: cs.IR

TL;DR: 论文提出了一种基于大语言模型（LLM）的隐私保护推荐框架，通过分离敏感数据和非敏感数据，仅分享后者给云端，结合本地去混淆模块恢复隐私数据，达到隐私与推荐质量的平衡。


<details>
  <summary>Details</summary>
Motivation: 传统LLM推荐系统需要将用户数据传输至云端，存在隐私泄露风险。本文旨在设计一个能保护用户隐私同时保持推荐效果的方案。

Method: 提出混合隐私保护框架，分离敏感与非敏感数据，仅分享非敏感数据到云端，并通过本地去混淆模块恢复敏感数据的推荐结果。

Result: 实验显示，该框架在推荐效果上接近完全共享数据的系统，隐私保护效果显著，同时提升了HR@10评分和类别分布对齐。

Conclusion: 该方法在消费级硬件上高效运行，实现了隐私与推荐质量的平衡，为实际应用提供可行性。

Abstract: Large Language Model (LLM)-based recommendation systems leverage powerful
language models to generate personalized suggestions by processing user
interactions and preferences. Unlike traditional recommendation systems that
rely on structured data and collaborative filtering, LLM-based models process
textual and contextual information, often using cloud-based infrastructure.
This raises privacy concerns, as user data is transmitted to remote servers,
increasing the risk of exposure and reducing control over personal information.
To address this, we propose a hybrid privacy-preserving recommendation
framework which separates sensitive from nonsensitive data and only shares the
latter with the cloud to harness LLM-powered recommendations. To restore lost
recommendations related to obfuscated sensitive data, we design a
de-obfuscation module that reconstructs sensitive recommendations locally.
Experiments on real-world e-commerce datasets show that our framework achieves
almost the same recommendation utility with a system which shares all data with
an LLM, while preserving privacy to a large extend. Compared to
obfuscation-only techniques, our approach improves HR@10 scores and category
distribution alignment, offering a better balance between privacy and
recommendation quality. Furthermore, our method runs efficiently on
consumer-grade hardware, making privacy-aware LLM-based recommendation systems
practical for real-world use.

</details>


### [152] [Enhancing User Sequence Modeling through Barlow Twins-based Self-Supervised Learning](https://arxiv.org/abs/2505.00953)
*Yuhan Liu,Lin Ning,Neo Wu,Karan Singhal,Philip Andrew Mansfield,Devora Berlowitz,Sushant Prakash,Bradley Green*

Main category: cs.IR

TL;DR: 文章提出了一种基于Barlow Twins的自监督学习方法，用于用户序列建模，减少了负采样的需求，并且在标记数据有限的情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 用户序列建模在推荐系统中至关重要，但缺乏标记数据是一个关键挑战。现有自监督学习方法依赖大量负采样，计算成本高且在实际场景中不总是可行。

Method: 通过改进的Barlow Twins方法，结合合适的增强技术，减少了负采样的需求，实现了在小批量数据和有限标记情况下的有效表征学习。

Result: 在MovieLens-1M、MovieLens-20M和Yelp数据集上测试，新方法在三个下游任务中表现优于双编码器模型，准确率提升8%-20%。

Conclusion: 该方法在标记数据稀缺和负样本有限的情况下，能有效提取序列级别的用户信息，具有显著的实用价值。

Abstract: User sequence modeling is crucial for modern large-scale recommendation
systems, as it enables the extraction of informative representations of users
and items from their historical interactions. These user representations are
widely used for a variety of downstream tasks to enhance users' online
experience. A key challenge for learning these representations is the lack of
labeled training data. While self-supervised learning (SSL) methods have
emerged as a promising solution for learning representations from unlabeled
data, many existing approaches rely on extensive negative sampling, which can
be computationally expensive and may not always be feasible in real-world
scenario. In this work, we propose an adaptation of Barlow Twins, a
state-of-the-art SSL methods, to user sequence modeling by incorporating
suitable augmentation methods. Our approach aims to mitigate the need for large
negative sample batches, enabling effective representation learning with
smaller batch sizes and limited labeled data. We evaluate our method on the
MovieLens-1M, MovieLens-20M, and Yelp datasets, demonstrating that our method
consistently outperforms the widely-used dual encoder model across three
downstream tasks, achieving an 8%-20% improvement in accuracy. Our findings
underscore the effectiveness of our approach in extracting valuable
sequence-level information for user modeling, particularly in scenarios where
labeled data is scarce and negative examples are limited.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [153] [The Coral Protocol: Open Infrastructure Connecting The Internet of Agents](https://arxiv.org/abs/2505.00749)
*Roman J. Georgio,Caelum Forder,Suman Deb,Peter Carroll,Önder Gürcan*

Main category: cs.MA

TL;DR: Coral Protocol 是一个去中心化的协作基础设施，旨在解决多AI代理间的互操作性问题，提供通用语言和协调框架。


<details>
  <summary>Details</summary>
Motivation: 随着组织中部署的专用AI代理增多，跨领域和跨供应商的协作需求日益增长，需要一种互操作性解决方案。

Method: 设计了标准化消息格式、模块化协调机制和安全的团队形成能力，确保代理间的高效和可信协作。

Result: Coral Protocol 成为多代理AI生态的基础平台，支持复杂工作流，推动了自动化、集体智能和商业价值的实现。

Conclusion: 该协议是“代理互联网”的关键组成部分，通过开放的代理协作解锁了新的自动化水平和商业潜力。

Abstract: The Coral Protocol is an open and decentralized collaboration infrastructure
that enables communication, coordination, trust and payments for The Internet
of Agents. It addresses the growing need for interoperability in a world where
organizations are deploying multiple specialized AI agents that must work
together across domains and vendors. As a foundational platform for multi-agent
AI ecosystems, Coral establishes a common language and coordination framework
allowing any agent to participate in complex workflows with others. Its design
emphasizes broad compatibility, security, and vendor neutrality, ensuring that
agent interactions are efficient and trustworthy. In particular, Coral
introduces standardized messaging formats for agent communication, a modular
coordination mechanism for orchestrating multi-agent tasks, and secure team
formation capabilities for dynamically assembling trusted groups of agents.
Together, these innovations position Coral Protocol as a cornerstone of the
emerging "Internet of Agents," unlocking new levels of automation, collective
intelligence, and business value through open agent collaboration.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [154] [Advancing Software Security and Reliability in Cloud Platforms through AI-based Anomaly Detection](https://arxiv.org/abs/2411.09200)
*Sabbir M. Saleh,Ibrahim Mohammed Sayem,Nazim Madhavji,John Steinbacher*

Main category: cs.SE

TL;DR: 论文提出了一种基于AI增强的CI/CD管道安全方法，通过分析网络流量模式来检测异常和网络攻击。


<details>
  <summary>Details</summary>
Motivation: 当前CI/CD管道的安全挑战日益突出，尤其是网络攻击频发，现有研究多集中于静态安全测试，而针对网络流量模式分析的研究较少。

Method: 研究采用CNN和LSTM结合的模型，在CSE-CIC-IDS2018和CSE-CIC-IDS2017数据集上训练，以检测异常流量。

Result: 模型在两种数据集上的准确率分别达到98.69%和98.30%，并在CI/CD各阶段生成异常日志。

Conclusion: 该方法显著提升了CI/CD管道的安全性，为现代DevOps实践中的安全和可靠性提供了重要支持。

Abstract: Continuous Integration/Continuous Deployment (CI/CD) is fundamental for
advanced software development, supporting faster and more efficient delivery of
code changes into cloud environments. However, security issues in the CI/CD
pipeline remain challenging, and incidents (e.g., DDoS, Bot, Log4j, etc.) are
happening over the cloud environments. While plenty of literature discusses
static security testing and CI/CD practices, only a few deal with network
traffic pattern analysis to detect different cyberattacks. This research aims
to enhance CI/CD pipeline security by implementing anomaly detection through AI
(Artificial Intelligence) support. The goal is to identify unusual behaviour or
variations from network traffic patterns in pipeline and cloud platforms. The
system shall integrate into the workflow to continuously monitor pipeline
activities and cloud infrastructure. Additionally, it aims to explore adaptive
response mechanisms to mitigate the detected anomalies or security threats.
This research employed two popular network traffic datasets, CSE-CIC-IDS2018
and CSE-CIC-IDS2017. We implemented a combination of Convolution Neural
Network(CNN) and Long Short-Term Memory (LSTM) to detect unusual traffic
patterns. We achieved an accuracy of 98.69% and 98.30% and generated log files
in different CI/CD pipeline stages that resemble the network anomalies affected
to address security challenges in modern DevOps practices, contributing to
advancing software security and reliability.

</details>


### [155] [Document Retrieval Augmented Fine-Tuning (DRAFT) for safety-critical software assessments](https://arxiv.org/abs/2505.01307)
*Regan Bolton,Mohammadreza Sheikhfathollahi,Simon Parkinson,Vanessa Vulovic,Gary Bamford,Dan Basher,Howard Parkinson*

Main category: cs.SE

TL;DR: DRAFT是一种结合文档检索与微调的新方法，用于提升语言模型在安全关键软件合规评估中的能力，比基线模型提升7%的正确率。


<details>
  <summary>Details</summary>
Motivation: 传统的手工评估方法在安全关键软件合规评估中效率低下，难以应对复杂的监管框架。

Method: 提出了Document Retrieval-Augmented Fine-Tuning (DRAFT)方法，结合双检索架构（软件文档和参考标准）和半自动数据集生成技术。

Result: 实验显示，DRAFT在GPT-4o-mini上比基线模型提升7%的正确率，且在证据处理、响应结构和领域推理上有质的飞跃。

Conclusion: DRAFT为合规评估系统提供了一种实用的改进方法，同时保持了监管领域必需的透明度和基于证据的推理。

Abstract: Safety critical software assessment requires robust assessment against
complex regulatory frameworks, a process traditionally limited by manual
evaluation. This paper presents Document Retrieval-Augmented Fine-Tuning
(DRAFT), a novel approach that enhances the capabilities of a large language
model (LLM) for safety-critical compliance assessment. DRAFT builds upon
existing Retrieval-Augmented Generation (RAG) techniques by introducing a novel
fine-tuning framework that accommodates our dual-retrieval architecture, which
simultaneously accesses both software documentation and applicable reference
standards. To fine-tune DRAFT, we develop a semi-automated dataset generation
methodology that incorporates variable numbers of relevant documents with
meaningful distractors, closely mirroring real-world assessment scenarios.
Experiments with GPT-4o-mini demonstrate a 7% improvement in correctness over
the baseline model, with qualitative improvements in evidence handling,
response structure, and domain-specific reasoning. DRAFT represents a practical
approach to improving compliance assessment systems while maintaining the
transparency and evidence-based reasoning essential in regulatory domains.

</details>


### [156] [Aggregating empirical evidence from data strategy studies: a case on model quantization](https://arxiv.org/abs/2505.00816)
*Santiago del Rey,Paulo Sérgio Medeiros dos Santos,Guilherme Horta Travassos,Xavier Franch,Silverio Martínez-Fernández*

Main category: cs.SE

TL;DR: 本研究评估了模型量化对深度学习系统正确性和资源效率的影响，并探讨了用结构化综合方法（SSM）聚合数据驱动研究证据的可行性。


<details>
  <summary>Details</summary>
Motivation: 随着实证软件工程的发展，研究更多采用数据策略（如分析数字工件而非人类受试者），但结果综合面临新方法挑战。本研究旨在评估模型量化的效果及其方法论意义。

Method: 对六项模型量化实证研究进行综合，应用结构化综合方法（SSM），通过图表建模结合定性与定量证据，提取并汇总了19个证据模型。

Result: 模型量化对正确性指标有轻微负面影响，但显著提升资源效率（存储、推理延迟、GPU能耗）。不同量化技术的证据仍分散，需更多针对性研究。

Conclusion: 模型量化在资源受限环境中是有效的优化策略，效率提升显著而正确性牺牲小。SSM为数据驱动研究的证据综合提供了可行方法。

Abstract: Background: As empirical software engineering evolves, more studies adopt
data strategies$-$approaches that investigate digital artifacts such as models,
source code, or system logs rather than relying on human subjects. Synthesizing
results from such studies introduces new methodological challenges.
  Aims: This study assesses the effects of model quantization on correctness
and resource efficiency in deep learning (DL) systems. Additionally, it
explores the methodological implications of aggregating evidence from empirical
studies that adopt data strategies.
  Method: We conducted a research synthesis of six primary studies that
empirically evaluate model quantization. We applied the Structured Synthesis
Method (SSM) to aggregate the findings, which combines qualitative and
quantitative evidence through diagrammatic modeling. A total of 19 evidence
models were extracted and aggregated.
  Results: The aggregated evidence indicates that model quantization weakly
negatively affects correctness metrics while consistently improving resource
efficiency metrics, including storage size, inference latency, and GPU energy
consumption$-$a manageable trade-off for many DL deployment contexts. Evidence
across quantization techniques remains fragmented, underscoring the need for
more focused empirical studies per technique.
  Conclusions: Model quantization offers substantial efficiency benefits with
minor trade-offs in correctness, making it a suitable optimization strategy for
resource-constrained environments. This study also demonstrates the feasibility
of using SSM to synthesize findings from data strategy-based research.

</details>


### [157] [CppSATD: A Reusable Self-Admitted Technical Debt Dataset in C++](https://arxiv.org/abs/2505.01136)
*Phuoc Pham,Murali Sridharan,Matteo Esposito,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 这篇论文针对技术债务（TD）中的自认技术债务（SATD）研究，主要集中在Java语言上的局限性，提出了一个专用于C++的SATD数据集CppSATD，包含53.1万条注释及其代码上下文，旨在促进跨语言SATD研究和检测方法的发展。


<details>
  <summary>Details</summary>
Motivation: 现有SATD研究过于集中在Java语言，缺乏对其他编程语言的覆盖，限制了研究结果的普适性和检测方法的跨语言适用性。论文旨在通过构建C++的SATD数据集填补这一空白。

Method: 开发了一个名为CppSATD的C++ SATD数据集，收集并标注了超过531,000条开发者自认的技术债务注释及其关联的源代码上下文。

Result: 成功构建了CppSATD数据集，为未来C++语言中的SATD检测、跨语言研究以及新发现提供了基础资源。

Conclusion: 通过引入CppSATD数据集，论文扩展了SATD研究的语言范围，促进了跨语言技术债务理解的深入和检测工具的多样化发展。

Abstract: In software development, technical debt (TD) refers to suboptimal
implementation choices made by the developers to meet urgent deadlines and
limited resources, posing challenges for future maintenance. Self-Admitted
Technical Debt (SATD) is a sub-type of TD, representing specific TD instances
``openly admitted'' by the developers and often expressed through source code
comments. Previous research on SATD has focused predominantly on the Java
programming language, revealing a significant gap in cross-language SATD. Such
a narrow focus limits the generalizability of existing findings as well as SATD
detection techniques across multiple programming languages. Our work addresses
such limitation by introducing CppSATD, a dedicated C++ SATD dataset,
comprising over 531,000 annotated comments and their source code contexts. Our
dataset can serve as a foundation for future studies that aim to develop SATD
detection methods in C++, generalize the existing findings to other languages,
or contribute novel insights to cross-language SATD research.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [158] [Multivariate Conformal Selection](https://arxiv.org/abs/2505.00917)
*Tian Bai,Yue Zhao,Xiang Yu,Archer Y. Yang*

Main category: stat.ME

TL;DR: 本文提出多变量保形选择（mCS）方法，扩展了传统的单变量保形选择（CS），以应对多变量响应场景。通过引入区域单调性和多变量非保形分数，该方法实现了对假发现率（FDR）的有限样本控制，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在药物发现、精准医学和大语言模型对齐等应用中，从大规模数据中选择高质量候选对象至关重要。传统保形选择（CS）方法仅限于单变量响应和标量标准，无法满足多变量场景的需求。因此，需要开发一种能处理多变量响应的新方法。

Method: 本文提出的多变量保形选择（mCS）通过引入区域单调性和多变量非保形分数来构建保形p值，从而控制假发现率（FDR）。方法包括两种变体：mCS-dist（基于距离的分数）和mCS-learn（通过可微优化学习最优分数）。

Result: 在模拟和真实数据集上的实验表明，mCS在维持FDR控制的同时显著提高了选择能力，证明了其作为多变量选择任务的鲁棒框架的有效性。

Conclusion: mCS方法成功扩展了传统保形选择在多变量场景中的应用，通过灵活的非保形分数设计和优化，实现了高效的候选选择，为相关领域提供了强有力的工具。

Abstract: Selecting high-quality candidates from large datasets is critical in
applications such as drug discovery, precision medicine, and alignment of large
language models (LLMs). While Conformal Selection (CS) provides rigorous
uncertainty quantification, it is limited to univariate responses and scalar
criteria. To address this issue, we propose Multivariate Conformal Selection
(mCS), a generalization of CS designed for multivariate response settings. Our
method introduces regional monotonicity and employs multivariate nonconformity
scores to construct conformal p-values, enabling finite-sample False Discovery
Rate (FDR) control. We present two variants: mCS-dist, using distance-based
scores, and mCS-learn, which learns optimal scores via differentiable
optimization. Experiments on simulated and real-world datasets demonstrate that
mCS significantly improves selection power while maintaining FDR control,
establishing it as a robust framework for multivariate selection tasks.

</details>


### [159] [Q-Learning with Clustered-SMART (cSMART) Data: Examining Moderators in the Construction of Clustered Adaptive Interventions](https://arxiv.org/abs/2505.00822)
*Yao Song,Kelly Speth,Amy Kilbourne,Andrew Quanbeck,Daniel Almirall,Lu Wang*

Main category: stat.ME

TL;DR: 提出了一种基于cSMART数据的聚类Q学习框架，结合M-out-of-N聚类自举法，用于评估候选调整变量在构建最优聚类自适应干预（cAI）中的效用。


<details>
  <summary>Details</summary>
Motivation: 解决现有cSMART研究中如何有效评估候选调整变量对因果效应调节的作用，尤其是在存在非规律性挑战时的可靠性推断需求。

Method: 采用聚类Q学习框架结合M-out-of-N聚类自举法，通过模拟不同非规律性条件和聚类参数验证方法性能。

Result: 仿真结果表明所提方法能在不同非规律性条件下保持置信区间接近标称覆盖率，并在ADEPT数据集中成功应用于构建临床级cAI。

Conclusion: 该方法为构建最大化终期结果的cAI提供了可靠的统计推断工具，尤其在非规律性条件下表现优异。

Abstract: A clustered adaptive intervention (cAI) is a pre-specified sequence of
decision rules that guides practitioners on how best - and based on which
measures - to tailor cluster-level intervention to improve outcomes at the
level of individuals within the clusters. A clustered sequential multiple
assignment randomized trial (cSMART) is a type of trial that is used to inform
the empirical development of a cAI. The most common type of secondary aim in a
cSMART focuses on assessing causal effect moderation by candidate tailoring
variables. We introduce a clustered Q-learning framework with the M-out-of-N
Cluster Bootstrap using data from a cSMART to evaluate whether a set of
candidate tailoring variables may be useful in defining an optimal cAI. This
approach could construct confidence intervals (CI) with near-nominal coverage
to assess parameters indexing the causal effect moderation function.
Specifically, it allows reliable inferences concerning the utility of candidate
tailoring variables in constructing a cAI that maximizes a mean end-of-study
outcome even when "non-regularity", a well-known challenge exists. Simulations
demonstrate the numerical performance of the proposed method across varying
non-regularity conditions and investigate the impact of varying number of
clusters and intra-cluster correlation coefficient on CI coverage. Methods are
applied on ADEPT dataset to inform the construction of a clinic-level cAI for
improving evidence-based practice in treating mood disorders.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [160] [Differentiable Nonlinear Model Predictive Control](https://arxiv.org/abs/2505.01353)
*Jonathan Frey,Katrin Baumgärtner,Gianluca Frison,Dirk Reinhardt,Jasper Hoffmann,Leonard Fichtner,Sebastien Gros,Moritz Diehl*

Main category: math.OC

TL;DR: 这篇论文探讨了非线性模型预测控制（MPC）中参数解敏感性的高效计算问题，提出了隐函数定理（IFT）和基于内点法（IPM）的平滑最优条件计算方法，并实现了一个高效开源框架，速度比现有最优求解器快3倍以上。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在求解凸或无约束问题的敏感性时受限，而MPC中的非线性规划（NLP）需要更通用的敏感性计算方法，以支持学习增强算法的集成。

Method: 采用隐函数定理（IFT）和内点法（IPM）处理平滑最优条件，结合序列二次规划（SQP）方法，在其二次子问题中应用IPM。

Result: 实现了一个高效的开源框架，支持一般最优控制问题的前向和伴随敏感性计算，速度提升超过现有最优求解器的3倍。

Conclusion: 该方法为非线性MPC中的敏感性计算提供了高效且通用的解决方案，推动了学习增强方法与MPC的深度集成。

Abstract: The efficient computation of parametric solution sensitivities is a key
challenge in the integration of learning-enhanced methods with nonlinear model
predictive control (MPC), as their availability is crucial for many learning
algorithms. While approaches presented in the machine learning community are
limited to convex or unconstrained formulations, this paper discusses the
computation of solution sensitivities of general nonlinear programs (NLPs)
using the implicit function theorem (IFT) and smoothed optimality conditions
treated in interior-point methods (IPM). We detail sensitivity computation
within a sequential quadratic programming (SQP) method which employs an IPM for
the quadratic subproblems. The publication is accompanied by an efficient
open-source implementation within the framework, providing both forward and
adjoint sensitivities for general optimal control problems, achieving speedups
exceeding 3x over the state-of-the-art solver mpc.pytorch.

</details>


### [161] [A Provably Convergent Plug-and-Play Framework for Stochastic Bilevel Optimization](https://arxiv.org/abs/2505.01258)
*Tianshu Chu,Dachuan Xu,Wei Yao,Chengming Yu,Jin Zhang*

Main category: math.OC

TL;DR: PnPBO是一个即插即用的随机双层优化框架，集成了多种随机估计器，实现了与单层优化相似的样本复杂度，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决双层优化中随机估计器的灵活集成问题，并探索其与单层优化在样本复杂度上的潜在关系。

Method: 提出PnPBO框架，支持多种随机估计器（如PAGE、ZeroSARAH等）的独立整合，并引入移动平均技术优化无偏估计。

Result: 理论证明PnPBO的样本复杂度与单层优化相当，实验验证了其在实际问题中的高效性。

Conclusion: PnPBO框架为双层优化提供了高效的解决方案，证明了其样本复杂度的最优性。

Abstract: Bilevel optimization has recently attracted significant attention in machine
learning due to its wide range of applications and advanced hierarchical
optimization capabilities. In this paper, we propose a plug-and-play framework,
named PnPBO, for developing and analyzing stochastic bilevel optimization
methods. This framework integrates both modern unbiased and biased stochastic
estimators into the single-loop bilevel optimization framework introduced in
[9], with several improvements. In the implementation of PnPBO, all stochastic
estimators for different variables can be independently incorporated, and an
additional moving average technique is applied when using an unbiased estimator
for the upper-level variable. In the theoretical analysis, we provide a unified
convergence and complexity analysis for PnPBO, demonstrating that the
adaptation of various stochastic estimators (including PAGE, ZeroSARAH, and
mixed strategies) within the PnPBO framework achieves optimal sample
complexity, comparable to that of single-level optimization. This resolves the
open question of whether the optimal complexity bounds for solving bilevel
optimization are identical to those for single-level optimization. Finally, we
empirically validate our framework, demonstrating its effectiveness on several
benchmark problems and confirming our theoretical findings.

</details>


### [162] [Negative Stepsizes Make Gradient-Descent-Ascent Converge](https://arxiv.org/abs/2505.01423)
*Henry Shugart,Jason M. Altschuler*

Main category: math.OC

TL;DR: 传统梯度上升下降（GDA）算法通过非常规步长（时间变化、不对称且周期性为负）实现收敛，解决了经典反例中的循环问题。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为GDA无法收敛，但本文通过创新步长调度证明其有效性，简化了复杂算法的设计。

Method: 提出“投石步长”（时间变化、不对称、周期性为负），并通过理论分析证明其必要性。

Result: GDA在经典反例中收敛，且最终迭代表现优异，收敛速度快。

Conclusion: 负步长通过去同步化克服循环问题，其二阶效应实现了共识优化的近似效果，适用于深度学习中的极小极大问题。

Abstract: Efficient computation of min-max problems is a central question in
optimization, learning, games, and controls. Arguably the most natural
algorithm is gradient-descent-ascent (GDA). However, since the 1970s,
conventional wisdom has argued that GDA fails to converge even on simple
problems. This failure spurred an extensive literature on modifying GDA with
additional building blocks such as extragradients, optimism, momentum,
anchoring, etc. In contrast, we show that GDA converges in its original form by
simply using a judicious choice of stepsizes.
  The key innovation is the proposal of unconventional stepsize schedules
(dubbed slingshot stepsize schedules) that are time-varying, asymmetric, and
periodically negative. We show that all three properties are necessary for
convergence, and that altogether this enables GDA to converge on the classical
counterexamples (e.g., unconstrained convex-concave problems). All of our
results apply to the last iterate of GDA, as is typically desired in practice.
  The core algorithmic intuition is that although negative stepsizes make
backward progress, they de-synchronize the min and max variables (overcoming
the cycling issue of GDA), and lead to a slingshot phenomenon in which the
forward progress in the other iterations is overwhelmingly larger. This results
in fast overall convergence. Geometrically, the slingshot dynamics leverage the
non-reversibility of gradient flow: positive/negative steps cancel to first
order, yielding a second-order net movement in a new direction that leads to
convergence and is otherwise impossible for GDA to move in. We interpret this
as a second-order finite-differencing algorithm and show that, intriguingly, it
approximately implements consensus optimization, an empirically popular
algorithm for min-max problems involving deep neural networks (e.g., training
GANs).

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [163] [SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation](https://arxiv.org/abs/2505.00831)
*Quang P. M. Pham,Khoi T. N. Nguyen,Nhi H. Doan,Cuong A. Pham,Kentaro Inui,Dezhen Song*

Main category: cs.RO

TL;DR: SmallPlan 是一个新框架，利用大型语言模型（LLMs）作为教师模型，训练轻量级小语言模型（SLMs）进行高级路径规划任务，在资源受限的边缘设备上实现高效部署。


<details>
  <summary>Details</summary>
Motivation: 在大规模动态环境中，高效路径规划是机器人技术的关键挑战。LLMs 虽具备强大推理能力，但其高计算成本和动态适应性不足限制了在边缘设备上的实时应用。SmallPlan 旨在解决这一问题。

Method: SmallPlan 通过 LLM 引导的监督微调（SFT）和强化学习（RL）训练 SLMs，利用 3D 场景图的紧凑表示生成最优动作序列，确保模型兼顾路径距离和尝试次数的关键因素。

Result: 实验表明，微调后的 SLMs 在顺序路径规划任务中表现与 GPT-4o 等大型模型相当，且避免了幻觉和过拟合问题，资源效率更高。

Conclusion: SmallPlan 为边缘设备提供了一种高效的自主机器人路径规划解决方案，推动了实际应用的进展。

Abstract: Efficient path planning in robotics, particularly within large-scale, dynamic
environments, remains a significant hurdle. While Large Language Models (LLMs)
offer strong reasoning capabilities, their high computational cost and limited
adaptability in dynamic scenarios hinder real-time deployment on edge devices.
We present SmallPlan -- a novel framework leveraging LLMs as teacher models to
train lightweight Small Language Models (SLMs) for high-level path planning
tasks. In SmallPlan, the SLMs provide optimal action sequences to navigate
across scene graphs that compactly represent full-scaled 3D scenes. The SLMs
are trained in a simulation-powered, interleaved manner with LLM-guided
supervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not
only enables SLMs to successfully complete navigation tasks but also makes them
aware of important factors like travel distance and number of trials. Through
experiments, we demonstrate that the fine-tuned SLMs perform competitively with
larger models like GPT-4o on sequential path planning, without suffering from
hallucination and overfitting. SmallPlan is resource-efficient, making it
well-suited for edge-device deployment and advancing practical autonomous
robotics.

</details>


### [164] [IK Seed Generator for Dual-Arm Human-like Physicality Robot with Mobile Base](https://arxiv.org/abs/2505.00871)
*Jun Takamatsu,Atsushi Kanehira,Kazuhiro Sasabuchi,Naoki Wake,Katsushi Ikeuchi*

Main category: cs.RO

TL;DR: 论文提出了一种通过遗传算法优化初始猜测的方法，以提高机器人逆运动学（IK）求解的成功率。


<details>
  <summary>Details</summary>
Motivation: 人类尺寸的机器人由于机械限制（如关节角度限制）在IK求解时面临困难，解决这一问题可提升其应用价值。

Method: 基于可操作度指标和关节限制定义初始猜测的“优劣”，利用遗传算法和可达性地图生成优化的初始猜测。

Result: 实验证明，使用优化后的初始猜测显著提高了IK求解的成功率，并在三种典型场景中验证了方法的实用性。

Conclusion: 所提方法有效缓解了尺寸受限机器人的IK求解难题，拓宽了其实际应用场景。

Abstract: Robots are strongly expected as a means of replacing human tasks. If a robot
has a human-like physicality, the possibility of replacing human tasks
increases. In the case of household service robots, it is desirable for them to
be on a human-like size so that they do not become excessively large in order
to coexist with humans in their operating environment. However, robots with
size limitations tend to have difficulty solving inverse kinematics (IK) due to
mechanical limitations, such as joint angle limitations. Conversely, if the
difficulty coming from this limitation could be mitigated, one can expect that
the use of such robots becomes more valuable. In numerical IK solver, which is
commonly used for robots with higher degrees-of-freedom (DOF), the solvability
of IK depends on the initial guess given to the solver. Thus, this paper
proposes a method for generating a good initial guess for a numerical IK solver
given the target hand configuration. For the purpose, we define the goodness of
an initial guess using the scaled Jacobian matrix, which can calculate the
manipulability index considering the joint limits. These two factors are
related to the difficulty of solving IK. We generate the initial guess by
optimizing the goodness using the genetic algorithm (GA). To enumerate much
possible IK solutions, we use the reachability map that represents the
reachable area of the robot hand in the arm-base coordinate system. We conduct
quantitative evaluation and prove that using an initial guess that is judged to
be better using the goodness value increases the probability that IK is solved.
Finally, as an application of the proposed method, we show that by generating
good initial guesses for IK a robot actually achieves three typical scenarios.

</details>


### [165] [Autonomous Embodied Agents: When Robotics Meets Deep Learning Reasoning](https://arxiv.org/abs/2505.00935)
*Roberto Bigazzi*

Main category: cs.RO

TL;DR: 论文探讨了Embodied AI领域的发展，聚焦于智能机器人在室内环境中的训练与部署，通过仿真平台进行高效学习与评估。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力的提升和深度学习的革命，Embodied AI作为计算机视觉、机器人学和决策交叉领域，旨在推动智能自主机器人的发展与社会应用。

Method: 利用大量3D模型构建的仿真环境训练学习型智能体，通过连续交互（环境感知、信息编码、动作执行）实现任务目标，并分析技术实现方法。

Result: 提出了完整的室内环境智能体实现流程，包括现有技术的综述、方法技术详述及关键机器人任务的实验验证。

Conclusion: 论文为Embodied AI研究贡献了理论框架与实践方法，有望推动未来自主智能体的发展。

Abstract: The increase in available computing power and the Deep Learning revolution
have allowed the exploration of new topics and frontiers in Artificial
Intelligence research. A new field called Embodied Artificial Intelligence,
which places at the intersection of Computer Vision, Robotics, and Decision
Making, has been gaining importance during the last few years, as it aims to
foster the development of smart autonomous robots and their deployment in
society. The recent availability of large collections of 3D models for
photorealistic robotic simulation has allowed faster and safe training of
learning-based agents for millions of frames and a careful evaluation of their
behavior before deploying the models on real robotic platforms. These
intelligent agents are intended to perform a certain task in a possibly unknown
environment. To this end, during the training in simulation, the agents learn
to perform continuous interactions with the surroundings, such as gathering
information from the environment, encoding and extracting useful cues for the
task, and performing actions towards the final goal; where every action of the
agent influences the interactions. This dissertation follows the complete
creation process of embodied agents for indoor environments, from their concept
to their implementation and deployment. We aim to contribute to research in
Embodied AI and autonomous agents, in order to foster future work in this
field. We present a detailed analysis of the procedure behind implementing an
intelligent embodied agent, comprehending a thorough description of the current
state-of-the-art in literature, technical explanations of the proposed methods,
and accurate experimental studies on relevant robotic tasks.

</details>


### [166] [Model Tensor Planning](https://arxiv.org/abs/2505.01059)
*An T. Le,Khai Nguyen,Minh Nhat Vu,João Carvalho,Jan Peters*

Main category: cs.RO

TL;DR: 文章提出了MTP框架，通过结构化张量采样和高熵控制轨迹生成，改进了基于采样的MPC方法,在机器人任务中表现优于标准MPC和进化策略。


<details>
  <summary>Details</summary>
Motivation: 解决基于采样的MPC在非线性及接触密集任务中因局部贪婪采样导致的探索不足问题，提出一种能生成全局多样化控制候选的方案。

Method: 采用随机多部图采样并结合B样条和Akima样条插值生成平滑控制轨迹，同时引入β混合策略平衡局部开发和全局探索。

Result: 实验证明，MTP在灵巧操作和人形运动等任务中，成功率和控制鲁棒性优于基线方法，且理论证明了其在无限张量条件下的路径覆盖和最大熵特性。

Conclusion: MTP为基于模型的规划与控制提供了一个可扩展的稳健探索框架，其张量采样结构和混合策略有效性得到验证。

Abstract: Sampling-based model predictive control (MPC) offers strong performance in
nonlinear and contact-rich robotic tasks, yet often suffers from poor
exploration due to locally greedy sampling schemes. We propose \emph{Model
Tensor Planning} (MTP), a novel sampling-based MPC framework that introduces
high-entropy control trajectory generation through structured tensor sampling.
By sampling over randomized multipartite graphs and interpolating control
trajectories with B-splines and Akima splines, MTP ensures smooth and globally
diverse control candidates. We further propose a simple $\beta$-mixing strategy
that blends local exploitative and global exploratory samples within the
modified Cross-Entropy Method (CEM) update, balancing control refinement and
exploration. Theoretically, we show that MTP achieves asymptotic path coverage
and maximum entropy in the control trajectory space in the limit of infinite
tensor depth and width.
  Our implementation is fully vectorized using JAX and compatible with MuJoCo
XLA, supporting \emph{Just-in-time} (JIT) compilation and batched rollouts for
real-time control with online domain randomization. Through experiments on
various challenging robotic tasks, ranging from dexterous in-hand manipulation
to humanoid locomotion, we demonstrate that MTP outperforms standard MPC and
evolutionary strategy baselines in task success and control robustness. Design
and sensitivity ablations confirm the effectiveness of MTP tensor sampling
structure, spline interpolation choices, and mixing strategy. Altogether, MTP
offers a scalable framework for robust exploration in model-based planning and
control.

</details>


### [167] [ViSA-Flow: Accelerating Robot Skill Learning via Large-Scale Video Semantic Action Flow](https://arxiv.org/abs/2505.01288)
*Changhe Chen,Quantao Yang,Xiaohao Xu,Nima Fazeli,Olov Andersson*

Main category: cs.RO

TL;DR: 论文提出了ViSA-Flow框架，通过自监督学习从大规模无标签视频数据中提取语义动作流，作为机器人操作的核心中间表示，显著降低了机器人学习复杂操作技能的数据需求。


<details>
  <summary>Details</summary>
Motivation: 解决机器人因数据收集成本高而难以学习复杂操作技能的瓶颈，借鉴人类通过观察环境互动的学习方式，提出语义动作流作为高效学习的中间表示。

Method: ViSA-Flow框架分两步：首先从人类-物体互动视频中自监督提取语义动作流并预训练生成模型；其次在小规模机器人演示数据上微调该先验知识，实现知识迁移。

Result: 在CALVIN基准测试和真实任务中表现出色，尤其在低数据场景下优于现有方法，成功将人类视频观察知识迁移至机器人执行。

Conclusion: ViSA-Flow通过语义动作流实现了高效知识迁移，为机器人在低数据条件下学习复杂操作技能提供了可行方案。

Abstract: One of the central challenges preventing robots from acquiring complex
manipulation skills is the prohibitive cost of collecting large-scale robot
demonstrations. In contrast, humans are able to learn efficiently by watching
others interact with their environment. To bridge this gap, we introduce
semantic action flow as a core intermediate representation capturing the
essential spatio-temporal manipulator-object interactions, invariant to
superficial visual differences. We present ViSA-Flow, a framework that learns
this representation self-supervised from unlabeled large-scale video data.
First, a generative model is pre-trained on semantic action flows automatically
extracted from large-scale human-object interaction video data, learning a
robust prior over manipulation structure. Second, this prior is efficiently
adapted to a target robot by fine-tuning on a small set of robot demonstrations
processed through the same semantic abstraction pipeline. We demonstrate
through extensive experiments on the CALVIN benchmark and real-world tasks that
ViSA-Flow achieves state-of-the-art performance, particularly in low-data
regimes, outperforming prior methods by effectively transferring knowledge from
human video observation to robotic execution. Videos are available at
https://visaflow-web.github.io/ViSAFLOW.

</details>


### [168] [Uncertainty-aware Latent Safety Filters for Avoiding Out-of-Distribution Failures](https://arxiv.org/abs/2505.00779)
*Junwon Seo,Kensuke Nakamura,Andrea Bajcsy*

Main category: cs.RO

TL;DR: 提出了一种基于不确定性感知的潜在安全过滤器，用于识别和避免已知及未知的安全风险，通过校准不确定性阈值和扩展状态空间分析，有效保护机器人策略。


<details>
  <summary>Details</summary>
Motivation: 现有潜在安全过滤器可能因世界模型训练覆盖不足而遗漏已知或未知风险，导致对不安全情况的误判。

Method: 利用世界模型的认知不确定性作为潜在风险的指标，通过符合预测校准不确定性阈值，在扩展状态空间中进行可达性分析。

Result: 仿真和硬件实验中，该过滤器能预先检测潜在不安全场景，并可靠提出安全行动。

Conclusion: 不确定性感知的安全过滤器能有效保护机器人策略免受已知和未知风险的影响。

Abstract: Recent advances in generative world models have enabled classical safe
control methods, such as Hamilton-Jacobi (HJ) reachability, to generalize to
complex robotic systems operating directly from high-dimensional sensor
observations. However, obtaining comprehensive coverage of all safety-critical
scenarios during world model training is extremely challenging. As a result,
latent safety filters built on top of these models may miss novel hazards and
even fail to prevent known ones, overconfidently misclassifying risky
out-of-distribution (OOD) situations as safe. To address this, we introduce an
uncertainty-aware latent safety filter that proactively steers robots away from
both known and unseen failures. Our key idea is to use the world model's
epistemic uncertainty as a proxy for identifying unseen potential hazards. We
propose a principled method to detect OOD world model predictions by
calibrating an uncertainty threshold via conformal prediction. By performing
reachability analysis in an augmented state space-spanning both the latent
representation and the epistemic uncertainty-we synthesize a latent safety
filter that can reliably safeguard arbitrary policies from both known and
unseen safety hazards. In simulation and hardware experiments on vision-based
control tasks with a Franka manipulator, we show that our uncertainty-aware
safety filter preemptively detects potential unsafe scenarios and reliably
proposes safe, in-distribution actions. Video results can be found on the
project website at https://cmu-intentlab.github.io/UNISafe

</details>


### [169] [FalconWing: An Open-Source Platform for Ultra-Light Fixed-Wing Aircraft Research](https://arxiv.org/abs/2505.01383)
*Yan Miao,Will Shen,Hang Cui,Sayan Mitra*

Main category: cs.RO

TL;DR: FalconWing是一款开源超轻固定翼平台，用于自主飞行研究，结合视觉控制策略实现无IMU或动捕的自主降落。


<details>
  <summary>Details</summary>
Motivation: 为自主飞行研究提供低成本、轻量化的硬件平台，并结合创新的学习方法实现纯视觉控制的自主降落。

Method: 采用真实到仿真再到真实的学习方法，包括3D高斯喷溅构建仿真环境、从视觉估计数据识别非线性动态，以及通过仿真模仿学习训练多模态视觉Transformer策略。

Result: 在硬件平台上零样本部署时，该策略实现了80%的视觉自主降落成功率。

Conclusion: FalconWing及其配套的开源软件为自主飞行研究提供了实用工具，验证了纯视觉控制的可行性。

Abstract: We present FalconWing -- an open-source, ultra-lightweight (150 g) fixed-wing
platform for autonomy research. The hardware platform integrates a small
camera, a standard airframe, offboard computation, and radio communication for
manual overrides. We demonstrate FalconWing's capabilities by developing and
deploying a purely vision-based control policy for autonomous landing (without
IMU or motion capture) using a novel real-to-sim-to-real learning approach. Our
learning approach: (1) constructs a photorealistic simulation environment via
3D Gaussian splatting trained on real-world images; (2) identifies nonlinear
dynamics from vision-estimated real-flight data; and (3) trains a multi-modal
Vision Transformer (ViT) policy through simulation-only imitation learning. The
ViT architecture fuses single RGB image with the history of control actions via
self-attention, preserving temporal context while maintaining real-time 20 Hz
inference. When deployed zero-shot on the hardware platform, this policy
achieves an 80% success rate in vision-based autonomous landings. Together with
the hardware specifications, we also open-source the system dynamics, the
software for photorealistic simulator and the learning approach.

</details>


### [170] [SIME: Enhancing Policy Self-Improvement with Modal-level Exploration](https://arxiv.org/abs/2505.01396)
*Yang Jin,Jun Lv,Wenye Yu,Hongjie Fang,Yong-Lu Li,Cewu Lu*

Main category: cs.RO

TL;DR: 该论文提出了一种通过模态级探索和数据选择实现机器人自我改进的方法，成功在仿真和实际实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 机器人自我改进的挑战在于其倾向于重复现有能力，而难以生成新的有价值数据。为解决这一问题，论文提出模态级探索和数据选择作为关键。

Method: 在策略执行中引入模态级探索机制，生成更多样化和多模态的交互，并从中选择最有价值的试验和高质量片段用于学习。

Result: 在仿真基准和实际实验中成功验证了机器人自我改进的有效性。

Conclusion: 自我改进能力将有助于以更低成本开发更鲁棒且高成功率的机器人控制策略。

Abstract: Self-improvement requires robotic systems to initially learn from
human-provided data and then gradually enhance their capabilities through
interaction with the environment. This is similar to how humans improve their
skills through continuous practice. However, achieving effective
self-improvement is challenging, primarily because robots tend to repeat their
existing abilities during interactions, often failing to generate new, valuable
data for learning. In this paper, we identify the key to successful
self-improvement: modal-level exploration and data selection. By incorporating
a modal-level exploration mechanism during policy execution, the robot can
produce more diverse and multi-modal interactions. At the same time, we select
the most valuable trials and high-quality segments from these interactions for
learning. We successfully demonstrate effective robot self-improvement on both
simulation benchmarks and real-world experiments. The capability for
self-improvement will enable us to develop more robust and high-success-rate
robotic control strategies at a lower cost. Our code and experiment scripts are
available at https://ericjin2002.github.io/SIME/

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [171] [Attack and defense techniques in large language models: A survey and new perspectives](https://arxiv.org/abs/2505.00976)
*Zhiyu Liao,Kang Chen,Yuanguo Lin,Kangkang Li,Yunxuan Liu,Hefeng Chen,Xingwang Huang,Yuanhui Yu*

Main category: cs.CR

TL;DR: 本文系统综述了大型语言模型（LLMs）的攻击与防御技术，分类了攻击方式并分析了防御策略，指出了当前挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在自然语言处理中的广泛应用，其安全性和伦理问题日益突出，需要系统梳理攻击与防御技术以应对动态威胁。

Method: 通过系统调查和分类攻击（如对抗性提示攻击、模型盗窃等）及防御技术（预防型和检测型方法），分析其机制和影响。

Result: 总结了当前攻击与防御技术的进展，但指出了适应性防御、可解释安全技术和标准化评估框架等挑战。

Conclusion: 强调跨学科合作和伦理考量的重要性，为开发安全、稳健的LLMs提供了实用建议和未来方向。

Abstract: Large Language Models (LLMs) have become central to numerous natural language
processing tasks, but their vulnerabilities present significant security and
ethical challenges. This systematic survey explores the evolving landscape of
attack and defense techniques in LLMs. We classify attacks into adversarial
prompt attack, optimized attacks, model theft, as well as attacks on
application of LLMs, detailing their mechanisms and implications. Consequently,
we analyze defense strategies, including prevention-based and detection-based
defense methods. Although advances have been made, challenges remain to adapt
to the dynamic threat landscape, balance usability with robustness, and address
resource constraints in defense implementation. We highlight open problems,
including the need for adaptive scalable defenses, explainable security
techniques, and standardized evaluation frameworks. This survey provides
actionable insights and directions for developing secure and resilient LLMs,
emphasizing the importance of interdisciplinary collaboration and ethical
considerations to mitigate risks in real-world applications.

</details>


### [172] [Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from Large Language Models](https://arxiv.org/abs/2505.00817)
*Andrew Adiletta,Berk Sunar*

Main category: cs.CR

TL;DR: 论文提出了一种名为'Spill The Beans'的新型缓存侧信道攻击方法，通过定位攻击进程与受害者模型共享硬件资源，泄露由LLM生成的令牌。实验证明该方法可以成功泄露令牌，揭示了LLM部署中的新漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的兴起，共享硬件资源的侧信道攻击对机密性的威胁日益增加。本研究旨在探索LLMs在传统侧信道攻击中的脆弱性。

Method: 通过在同一硬件上定位攻击进程，利用缓存侧信道技术冲刷和重新加载嵌入层的嵌入向量，检测缓存命中以泄露令牌。

Result: 实验显示，攻击者可以高概率恢复高熵API密钥（80%-90%），对英文文本的恢复率为40%。恢复率取决于监控的令牌集，可通过针对更专业的输出领域提高。

Conclusion: 研究揭示了LLMs对传统侧信道攻击的脆弱性，强调了在LLM服务基础设施中需要采取额外的隐私和安全措施来缓解此类威胁。

Abstract: Side-channel attacks on shared hardware resources increasingly threaten
confidentiality, especially with the rise of Large Language Models (LLMs). In
this work, we introduce Spill The Beans, a novel application of cache
side-channels to leak tokens generated by an LLM. By co-locating an attack
process on the same hardware as the victim model, we flush and reload embedding
vectors from the embedding layer, where each token corresponds to a unique
embedding vector. When accessed during token generation, it results in a cache
hit detectable by our attack on shared lower-level caches.
  A significant challenge is the massive size of LLMs, which, by nature of
their compute intensive operation, quickly evicts embedding vectors from the
cache. We address this by balancing the number of tokens monitored against the
amount of information leaked. Monitoring more tokens increases potential
vocabulary leakage but raises the chance of missing cache hits due to eviction;
monitoring fewer tokens improves detection reliability but limits vocabulary
coverage.
  Through extensive experimentation, we demonstrate the feasibility of leaking
tokens from LLMs via cache side-channels. Our findings reveal a new
vulnerability in LLM deployments, highlighting that even sophisticated models
are susceptible to traditional side-channel attacks. We discuss the
implications for privacy and security in LLM-serving infrastructures and
suggest considerations for mitigating such threats. For proof of concept we
consider two concrete attack scenarios: Our experiments show that an attacker
can recover as much as 80%-90% of a high entropy API key with single shot
monitoring. As for English text we can reach a 40% recovery rate with a single
shot. We should note that the rate highly depends on the monitored token set
and these rates can be improved by targeting more specialized output domains.

</details>


### [173] [From Texts to Shields: Convergence of Large Language Models and Cybersecurity](https://arxiv.org/abs/2505.00841)
*Tao Li,Ya-Ting Yang,Yunian Pan,Quanyan Zhu*

Main category: cs.CR

TL;DR: 本文探讨了大型语言模型（LLMs）与网络安全的融合，综合了网络安���、人工智能、形式化方法和以人为中心设计的跨学科见解。报告强调了LLMs在软件和网络安全、5G漏洞分析及生成式安全工程中的新兴应用，并指出其在自动化复杂任务、提升操作效率和启用推理驱动的安全分析中的作用。同时，报告提出了通过人在环路系统、角色特定培训等策略来解决与LLMs部署相关的社会技术挑战。报告还概述了确保基于LLMs的系统可解释性、安全性和公平性的关键研究挑战。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在网络安全中的潜在应用，并解决其部署中的社会技术挑战，以促进安全有效的技术采纳。

Method: 综合跨学科见解，分析LLMs在网络安全中的应用案例，提出解决相关挑战的策略和研究方向。

Result: LLMs在自动化任务、提升效率和驱动安全分析方面显示出潜力，但需解决信任、透明度等问题。

Conclusion: 通过技术与组织社会的结合，提出了LLMs在网络安全中安全有效应用的研究议程。

Abstract: This report explores the convergence of large language models (LLMs) and
cybersecurity, synthesizing interdisciplinary insights from network security,
artificial intelligence, formal methods, and human-centered design. It examines
emerging applications of LLMs in software and network security, 5G
vulnerability analysis, and generative security engineering. The report
highlights the role of agentic LLMs in automating complex tasks, improving
operational efficiency, and enabling reasoning-driven security analytics.
Socio-technical challenges associated with the deployment of LLMs -- including
trust, transparency, and ethical considerations -- can be addressed through
strategies such as human-in-the-loop systems, role-specific training, and
proactive robustness testing. The report further outlines critical research
challenges in ensuring interpretability, safety, and fairness in LLM-based
systems, particularly in high-stakes domains. By integrating technical advances
with organizational and societal considerations, this report presents a
forward-looking research agenda for the secure and effective adoption of LLMs
in cybersecurity.

</details>


### [174] [OET: Optimization-based prompt injection Evaluation Toolkit](https://arxiv.org/abs/2505.00843)
*Jinsheng Pan,Xiaogeng Liu,Chaowei Xiao*

Main category: cs.CR

TL;DR: 本文介绍了OET，一个基于优化的评估工具包，用于系统评估LLMs在提示注入攻击下的防御效果，提出了自适应测试框架生成最坏案例对抗样本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自然语言处理中表现优异，但其易受提示注入攻击的安全风险亟待解决。现有防御策略缺乏标准化评估框架，尤其是在自适应对抗场景下的效果评估。

Method: 提出OET工具包，通过模块化流程生成对抗字符串、执行动态攻击及结果分析，利用优化方法在白盒和黑盒访问下生成最坏案例对抗样本。

Result: 实验表明当前防御机制存在局限，部分模型即使在安全增强后仍易受攻击。

Conclusion: OET为评估对抗鲁棒性提供了统一平台，揭示了现有防御的不足。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
natural language understanding and generation, enabling their widespread
adoption across various domains. However, their susceptibility to prompt
injection attacks poses significant security risks, as adversarial inputs can
manipulate model behavior and override intended instructions. Despite numerous
defense strategies, a standardized framework to rigorously evaluate their
effectiveness, especially under adaptive adversarial scenarios, is lacking. To
address this gap, we introduce OET, an optimization-based evaluation toolkit
that systematically benchmarks prompt injection attacks and defenses across
diverse datasets using an adaptive testing framework. Our toolkit features a
modular workflow that facilitates adversarial string generation, dynamic attack
execution, and comprehensive result analysis, offering a unified platform for
assessing adversarial robustness. Crucially, the adaptive testing framework
leverages optimization methods with both white-box and black-box access to
generate worst-case adversarial examples, thereby enabling strict red-teaming
evaluations. Extensive experiments underscore the limitations of current
defense mechanisms, with some models remaining susceptible even after
implementing security enhancements.

</details>


### [175] [Good News for Script Kiddies? Evaluating Large Language Models for Automated Exploit Generation](https://arxiv.org/abs/2505.01065)
*David Jin,Qian Fu,Yuekang Li*

Main category: cs.CR

TL;DR: 该论文首次系统研究了大语言模型（LLMs）在自动漏洞利用生成（AEG）中的效果，评估了其协作性和技术能力。通过引入重新编排的软件安全实验基准和设计基于LLM的攻击者，研究发现GPT-4和GPT-4o协作性高，但所有模型均未能成功生成漏洞利用。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于LLMs在代码相关任务中的卓越表现及其在自动漏洞利用生成（AEG）中的潜在风险，需系统评估其实际能力。

Method: 方法包括引入重新编排的五个软件安全实验的基准以消除数据集偏差，并设计基于LLM的攻击者来系统提示LLMs生成漏洞利用。

Result: 结果显示，GPT-4和GPT-4o协作性高，接近未审查模型，而Llama3抵抗力最强；但所有模型均未能成功生成漏洞利用，GPT-4o的最小错误表明LLM驱动AEG的潜力。

Conclusion: 研究表明LLMs当前在AEG中技术能力有限，但GPT-4o的表现突显了未来改进的可能性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
code-related tasks, raising concerns about their potential for automated
exploit generation (AEG). This paper presents the first systematic study on
LLMs' effectiveness in AEG, evaluating both their cooperativeness and technical
proficiency. To mitigate dataset bias, we introduce a benchmark with refactored
versions of five software security labs. Additionally, we design an LLM-based
attacker to systematically prompt LLMs for exploit generation. Our experiments
reveal that GPT-4 and GPT-4o exhibit high cooperativeness, comparable to
uncensored models, while Llama3 is the most resistant. However, no model
successfully generates exploits for refactored labs, though GPT-4o's minimal
errors highlight the potential for LLM-driven AEG advancements.

</details>


### [176] [A Rusty Link in the AI Supply Chain: Detecting Evil Configurations in Model Repositories](https://arxiv.org/abs/2505.01067)
*Ziqi Ding,Qian Fu,Junchen Ding,Gelei Deng,Yi Liu,Yuekang Li*

Main category: cs.CR

TL;DR: 论文研究了Hugging Face平台上的恶意配置问题，提出了三种攻击场景，并开发了CONFIGSCAN工具进行高效检测。


<details>
  <summary>Details</summary>
Motivation: 由于AI供应链（如Hugging Face）中配置文件的安全风险被忽视，研究旨在揭示其潜在威胁并提升安全性。

Method: 通过分析配置文件的运行环境和关键库，开发了基于LLM的工具CONFIGSCAN，用于检测可疑内容。

Result: 研究发现数千个可疑配置文件和仓库，证明了安全验证的紧迫性。

Conclusion: 研究呼吁AI模型托管平台加强配置文件的安防措施。

Abstract: Recent advancements in large language models (LLMs) have spurred the
development of diverse AI applications from code generation and video editing
to text generation; however, AI supply chains such as Hugging Face, which host
pretrained models and their associated configuration files contributed by the
public, face significant security challenges; in particular, configuration
files originally intended to set up models by specifying parameters and initial
settings can be exploited to execute unauthorized code, yet research has
largely overlooked their security compared to that of the models themselves; in
this work, we present the first comprehensive study of malicious configurations
on Hugging Face, identifying three attack scenarios (file, website, and
repository operations) that expose inherent risks; to address these threats, we
introduce CONFIGSCAN, an LLM-based tool that analyzes configuration files in
the context of their associated runtime code and critical libraries,
effectively detecting suspicious elements with low false positive rates and
high accuracy; our extensive evaluation uncovers thousands of suspicious
repositories and configuration files, underscoring the urgent need for enhanced
security validation in AI model hosting platforms.

</details>


### [177] [LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures](https://arxiv.org/abs/2505.01177)
*Francisco Aguilera-Martínez,Fernando Berzal*

Main category: cs.CR

TL;DR: 这篇论文综述了大型语言模型（LLMs）在训练和部署阶段可能面临的安全威胁及防御机制。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的快速发展，评估其安全威胁和漏洞变得至关重要，尤其是在训练和部署阶段。

Method: 论文通过定义和分类针对LLMs的攻击（训练阶段和已部署模型），分析攻击方式，并探讨预防型和检测型防御机制。

Result: 综述了已知攻击及对应防御策略，并评估了不同防御机制对安全威胁的有效性。

Conclusion: 论文旨在为LLMs的安全提供结构化框架，并指出需进一步研究的领域以应对新的安全挑战。

Abstract: As large language models (LLMs) continue to evolve, it is critical to assess
the security threats and vulnerabilities that may arise both during their
training phase and after models have been deployed. This survey seeks to define
and categorize the various attacks targeting LLMs, distinguishing between those
that occur during the training phase and those that affect already trained
models. A thorough analysis of these attacks is presented, alongside an
exploration of defense mechanisms designed to mitigate such threats. Defenses
are classified into two primary categories: prevention-based and
detection-based defenses. Furthermore, our survey summarizes possible attacks
and their corresponding defense strategies. It also provides an evaluation of
the effectiveness of the known defense mechanisms for the different security
threats. Our survey aims to offer a structured framework for securing LLMs,
while also identifying areas that require further research to improve and
strengthen defenses against emerging security challenges.

</details>


### [178] [Secure Cluster-Based Hierarchical Federated Learning in Vehicular Networks](https://arxiv.org/abs/2505.01186)
*M. Saeid HaghighiFard,Sinem Coleri*

Main category: cs.CR

TL;DR: 该论文提出了一种新颖的防御框架，结合动态车辆选择和鲁棒异常检测，以应对车辆网络中分层联邦学习（HFL）面临的安全性挑战，显著降低收敛时间。


<details>
  <summary>Details</summary>
Motivation: 为了解决车辆网络中HFL面临的有限通信资源、高移动性和数据异构性问题，同时抵御对抗性和不可靠车辆对全局模型完整性和收敛性的威胁，该论文提出了这一框架。

Method: 框架通过历史准确性、贡献频率和异常记录的全面评估，结合Z-score和余弦相似度分析检测异常，并引入自适应阈值机制和加权梯度平均，以及跨集群一致性检查来防御协调攻击。

Result: 仿真结果表明，该方法在1-hop和3-hop拓扑结构中显著降低了收敛时间，优于基准方法。

Conclusion: 提出的多级防御策略有效过滤了恶意贡献，提升了HFL在车辆网络中的安全性和效率。

Abstract: Hierarchical Federated Learning (HFL) has recently emerged as a promising
solution for intelligent decision-making in vehicular networks, helping to
address challenges such as limited communication resources, high vehicle
mobility, and data heterogeneity. However, HFL remains vulnerable to
adversarial and unreliable vehicles, whose misleading updates can significantly
compromise the integrity and convergence of the global model. To address these
challenges, we propose a novel defense framework that integrates dynamic
vehicle selection with robust anomaly detection within a cluster-based HFL
architecture, specifically designed to counter Gaussian noise and gradient
ascent attacks. The framework performs a comprehensive reliability assessment
for each vehicle by evaluating historical accuracy, contribution frequency, and
anomaly records. Anomaly detection combines Z-score and cosine similarity
analyses on model updates to identify both statistical outliers and directional
deviations in model updates. To further refine detection, an adaptive
thresholding mechanism is incorporated into the cosine similarity metric,
dynamically adjusting the threshold based on the historical accuracy of each
vehicle to enforce stricter standards for consistently high-performing
vehicles. In addition, a weighted gradient averaging mechanism is implemented,
which assigns higher weights to gradient updates from more trustworthy
vehicles. To defend against coordinated attacks, a cross-cluster consistency
check is applied to identify collaborative attacks in which multiple
compromised clusters coordinate misleading updates. Together, these mechanisms
form a multi-level defense strategy to filter out malicious contributions
effectively. Simulation results show that the proposed algorithm significantly
reduces convergence time compared to benchmark methods across both 1-hop and
3-hop topologies.

</details>


### [179] [Constrained Network Adversarial Attacks: Validity, Robustness, and Transferability](https://arxiv.org/abs/2505.01328)
*Anass Grini,Oumaima Taheri,Btissam El Khamlichi,Amal El Fallah-Seghrouchni*

Main category: cs.CR

TL;DR: 现有的对抗攻击方法在IoT和网络流量领域常违反数值和类别约束，导致80.3%的对抗样本无效，夸大实际漏洞；MLP生成的对抗样本比CNN和LSTM更有效，需结合领域约束和模型架构提升安全性。


<details>
  <summary>Details</summary>
Motivation: 研究发现现有对抗攻击方法在IoT和网络流量中因忽略领域约束而生成大量无效对抗样本，误导安全资源分配，需重新评估模型鲁棒性。

Method: 使用MLP作为代理模型，对比生成对抗样本的有效性，并分析其对其他常用模型的迁移性。

Result: 80.3%的对抗样本因违反领域约束无效；MLP生成的样本比CNN和LSTM更有效。

Conclusion: 评估和设计安全关键型ML/DL模型时，需同时考虑领域约束和模型架构以确保鲁棒性。

Abstract: While machine learning has significantly advanced Network Intrusion Detection
Systems (NIDS), particularly within IoT environments where devices generate
large volumes of data and are increasingly susceptible to cyber threats, these
models remain vulnerable to adversarial attacks. Our research reveals a
critical flaw in existing adversarial attack methodologies: the frequent
violation of domain-specific constraints, such as numerical and categorical
limits, inherent to IoT and network traffic. This leads to up to 80.3% of
adversarial examples being invalid, significantly overstating real-world
vulnerabilities. These invalid examples, though effective in fooling models, do
not represent feasible attacks within practical IoT deployments. Consequently,
relying on these results can mislead resource allocation for defense, inflating
the perceived susceptibility of IoT-enabled NIDS models to adversarial
manipulation. Furthermore, we demonstrate that simpler surrogate models like
Multi-Layer Perceptron (MLP) generate more valid adversarial examples compared
to complex architectures such as CNNs and LSTMs. Using the MLP as a surrogate,
we analyze the transferability of adversarial severity to other ML/DL models
commonly used in IoT contexts. This work underscores the importance of
considering both domain constraints and model architecture when evaluating and
designing robust ML/DL models for security-critical IoT and network
applications.

</details>


### [180] [Protocol-agnostic and Data-free Backdoor Attacks on Pre-trained Models in RF Fingerprinting](https://arxiv.org/abs/2505.00881)
*Tianya Zhao,Ningning Wang,Junqing Zhang,Xuyu Wang*

Main category: cs.CR

TL;DR: 本文研究了RF指纹识别中无数据后门攻击对无监督预训练模型的影响，提出了一种无需下游数据的攻击方法，并展示了其广泛适用性以及防御困难。


<details>
  <summary>Details</summary>
Motivation: 解决RF指纹识别中监督DNNs的领域迁移问题和数据稀缺问题，同时探索无监督预训练模型的安全漏洞。

Method: 设计了触发器和预定义输出表示（PORs），通过后门训练将两者映射到PTMs中，实现攻击。

Result: 攻击在多种输入域、协议和PTMs中表现出广泛适用性，且难以完全防御。

Conclusion: 该方法揭示了PTMs在RF指纹识别中的潜在安全风险，强调了进一步研究和防御的必要性。

Abstract: While supervised deep neural networks (DNNs) have proven effective for device
authentication via radio frequency (RF) fingerprinting, they are hindered by
domain shift issues and the scarcity of labeled data. The success of large
language models has led to increased interest in unsupervised pre-trained
models (PTMs), which offer better generalization and do not require labeled
datasets, potentially addressing the issues mentioned above. However, the
inherent vulnerabilities of PTMs in RF fingerprinting remain insufficiently
explored. In this paper, we thoroughly investigate data-free backdoor attacks
on such PTMs in RF fingerprinting, focusing on a practical scenario where
attackers lack access to downstream data, label information, and training
processes. To realize the backdoor attack, we carefully design a set of
triggers and predefined output representations (PORs) for the PTMs. By mapping
triggers and PORs through backdoor training, we can implant backdoor behaviors
into the PTMs, thereby introducing vulnerabilities across different downstream
RF fingerprinting tasks without requiring prior knowledge. Extensive
experiments demonstrate the wide applicability of our proposed attack to
various input domains, protocols, and PTMs. Furthermore, we explore potential
detection and defense methods, demonstrating the difficulty of fully
safeguarding against our proposed backdoor attack.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [181] [EnviKal-Loc: Sub-10m Indoor LoRaWAN Localization using an Environmental-Aware Path Loss and Adaptive RSSI Smoothing](https://arxiv.org/abs/2505.01185)
*Nahshon Mokua Obiri,Kristof Van Laerhoven*

Main category: cs.NI

TL;DR: 该论文提出了一种结合自适应滤波和扩展对数距离多墙路径损耗模型的轻量级但鲁棒的LoRaWAN室内定位方法，并通过环境参数增强和Kalman滤波显著提升了定位精度。


<details>
  <summary>Details</summary>
Motivation: LoRaWAN技术在大规模IoT部署中具有广泛覆盖优势，但在复杂室内环境下实现亚10米精确定位仍面临挑战。论文旨在通过改进传统路径损耗模型并结合环境动态参数，提高定位精度和鲁棒性。

Method: 研究提出了一种结合自适应Kalman滤波和扩展对数距离多墙路径损耗模型的方法，并引入环境动态参数（如温度、湿度等）和LoRaWAN关键参数（如RSSI、SNR等）来增强模型性能。

Result: 实验结果显示，改进后的模型（MWM-EP-KF）平均绝对误差为5.81米，显著优于基线模型（17.98米）和仅增强环境参数的模型（10.56米），环境参数和Kalman滤波分别降低了41.22%和42.63%的系统误差。

Conclusion: 该方法为动态变化环境中的LoRaWAN室内定位提供了一种高效且可解释的解决方案，显著提升了定位精度和鲁棒性。

Abstract: LoRaWAN technology's extensive coverage positions it as a strong contender
for large-scale IoT deployments. However, achieving sub-10 m accuracy in indoor
localization remains challenging due to complex environmental conditions,
multipath fading, and transient obstructions. This paper proposes a lightweight
but robust approach combining adaptive filtering with an extended log-distance,
multi-wall path loss and shadowing (PLS) model. Our methodology augments
conventional models with critical LoRaWAN parameters (received signal strength
indicator (RSSI), frequency, and signal-to-noise ratio (SNR)) and dynamic
environmental indicators (temperature, humidity, carbon dioxide, particulate
matter, and barometric pressure). An adaptive Kalman filter reduces RSSI
fluctuations, isolating persistent trends from momentary noise. Using a
six-month dataset of 1,328,334 field measurements, we evaluate three models:
the baseline COST 231 multi-wall model (MWM), the baseline model augmented with
environmental parameters (MWM-EP), and a forward-only adaptive Kalman-filtered
RSSI version of the latter (MWM-EP-KF). Results confirm that the MWM-EP-KF
achieves a mean absolute error (MAE) of 5.81 m, outperforming both the MWM-EP
(10.56 m) and the baseline MWM framework (17.98 m). Environmental augmentation
reduces systematic errors by 41.22%, while Kalman filtering significantly
enhances robustness under high RSSI volatility by 42.63%, on average across all
devices. These findings present an interpretable, efficient solution for
precise indoor LoRaWAN localization in dynamically changing environments.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [182] [Learning Low-Dimensional Embeddings for Black-Box Optimization](https://arxiv.org/abs/2505.01112)
*Riccardo Busetto,Manas Mejari,Marco Forgione,Alberto Bemporad,Dario Piga*

Main category: eess.SY

TL;DR: 提出了一种基于元学习的降维方法，用于在高维黑盒优化问题中减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统黑盒优化在高维问题中效果不佳且计算成本高，因此需要一种能在降维空间中高效寻找近似最优解的方法。

Method: 通过元学习预计算一个降维流形，仅需在新问题实例的降维空间中进行黑盒优化。

Result: 有效降低了在高维空间中寻找近似最优解的计算成本。

Conclusion: 该方法为高维黑盒优化问题提供了一种高效且实用的解决方案。

Abstract: When gradient-based methods are impractical, black-box optimization (BBO)
provides a valuable alternative. However, BBO often struggles with
high-dimensional problems and limited trial budgets. In this work, we propose a
novel approach based on meta-learning to pre-compute a reduced-dimensional
manifold where optimal points lie for a specific class of optimization
problems. When optimizing a new problem instance sampled from the class,
black-box optimization is carried out in the reduced-dimensional space,
effectively reducing the effort required for finding near-optimal solutions.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [183] [CIMFlow: An Integrated Framework for Systematic Design and Evaluation of Digital CIM Architectures](https://arxiv.org/abs/2505.01107)
*Yingjie Qi,Jianlei Yang,Yiou Wang,Yikun Wang,Dayu Wang,Ling Tang,Cenlin Duan,Xiaolin He,Weisheng Zhao*

Main category: cs.AR

TL;DR: CIMFlow是一个集成框架，用于在数字存内计算架构上实现和评估DNN工作负载，解决了现有工具在软件和硬件设计空间支持不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前数字存内计算架构的开发与优化缺乏全面的工具支持，现有框架往往无法满足其容量约束需求。

Method: CIMFlow结合编译和仿真基础设施，通过灵活的ISA设计和编译流中的高级分区与并行策略解决问题。

Result: CIMFlow实现了跨多样配置的系统原型设计与优化，为研究人员提供了设计空间探索的便捷平台。

Conclusion: CIMFlow填补了数字存内计算架构工具链的空白，显著提升了DNN加速的可行性与效率。

Abstract: Digital Compute-in-Memory (CIM) architectures have shown great promise in
Deep Neural Network (DNN) acceleration by effectively addressing the "memory
wall" bottleneck. However, the development and optimization of digital CIM
accelerators are hindered by the lack of comprehensive tools that encompass
both software and hardware design spaces. Moreover, existing design and
evaluation frameworks often lack support for the capacity constraints inherent
in digital CIM architectures. In this paper, we present CIMFlow, an integrated
framework that provides an out-of-the-box workflow for implementing and
evaluating DNN workloads on digital CIM architectures. CIMFlow bridges the
compilation and simulation infrastructures with a flexible instruction set
architecture (ISA) design, and addresses the constraints of digital CIM through
advanced partitioning and parallelism strategies in the compilation flow. Our
evaluation demonstrates that CIMFlow enables systematic prototyping and
optimization of digital CIM architectures across diverse configurations,
providing researchers and designers with an accessible platform for extensive
design space exploration.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [184] [GENMO: A GENeralist Model for Human MOtion](https://arxiv.org/abs/2505.01425)
*Jiefeng Li,Jinkun Cao,Haotian Zhang,Davis Rempe,Jan Kautz,Umar Iqbal,Ye Yuan*

Main category: cs.GR

TL;DR: GENMO是一种统一的人类运动模型，将运动生成和估计结合在一个框架中，通过约束生成实现精确估计，并利用扩散模型提升多样性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将运动生成和估计分开，限制了知识共享和模型效率。GENMO旨在统一这两项任务，实现协同效益。

Method: 将运动估计重新定义为约束生成，结合回归与扩散模型，引入估计导向的训练目标，并支持多模态输入和可变长度运动。

Result: 实验证明GENMO能有效处理多种人类运动任务，生成多样化运动的同时提升估计精度。

Conclusion: GENMO通过统一框架展示了运动生成与估计的协同优势，为多任务处理提供了高效方案。

Abstract: Human motion modeling traditionally separates motion generation and
estimation into distinct tasks with specialized models. Motion generation
models focus on creating diverse, realistic motions from inputs like text,
audio, or keyframes, while motion estimation models aim to reconstruct accurate
motion trajectories from observations like videos. Despite sharing underlying
representations of temporal dynamics and kinematics, this separation limits
knowledge transfer between tasks and requires maintaining separate models. We
present GENMO, a unified Generalist Model for Human Motion that bridges motion
estimation and generation in a single framework. Our key insight is to
reformulate motion estimation as constrained motion generation, where the
output motion must precisely satisfy observed conditioning signals. Leveraging
the synergy between regression and diffusion, GENMO achieves accurate global
motion estimation while enabling diverse motion generation. We also introduce
an estimation-guided training objective that exploits in-the-wild videos with
2D annotations and text descriptions to enhance generative diversity.
Furthermore, our novel architecture handles variable-length motions and mixed
multimodal conditions (text, audio, video) at different time intervals,
offering flexible control. This unified approach creates synergistic benefits:
generative priors improve estimated motions under challenging conditions like
occlusions, while diverse video data enhances generation capabilities.
Extensive experiments demonstrate GENMO's effectiveness as a generalist
framework that successfully handles multiple human motion tasks within a single
model.

</details>


### [185] [Model See Model Do: Speech-Driven Facial Animation with Style Control](https://arxiv.org/abs/2505.01319)
*Yifang Pan,Karan Singh,Luiz Gustavo Hafemann*

Main category: cs.GR

TL;DR: 提出了一种基于示例的生成框架，通过潜在扩散模型和风格基础条件机制，实现高表现力和时间连贯的3D面部动画。


<details>
  <summary>Details</summary>
Motivation: 现有方法在唇同步和基本情感表达上表现良好，但在捕捉和传递细腻表演风格上存在不足。

Method: 使用潜在扩散模型，并结合名为风格基础的新条件机制，从参考片段中提取关键姿势，引导生成过程。

Result: 方法在风格再现和唇同步上表现优异，通过了定性、定量和感知评估。

Conclusion: 提出的框架能有效捕捉细腻风格，同时保持高质量的唇同步，适用于多种语音场景。

Abstract: Speech-driven 3D facial animation plays a key role in applications such as
virtual avatars, gaming, and digital content creation. While existing methods
have made significant progress in achieving accurate lip synchronization and
generating basic emotional expressions, they often struggle to capture and
effectively transfer nuanced performance styles. We propose a novel
example-based generation framework that conditions a latent diffusion model on
a reference style clip to produce highly expressive and temporally coherent
facial animations. To address the challenge of accurately adhering to the style
reference, we introduce a novel conditioning mechanism called style basis,
which extracts key poses from the reference and additively guides the diffusion
generation process to fit the style without compromising lip synchronization
quality. This approach enables the model to capture subtle stylistic cues while
ensuring that the generated animations align closely with the input speech.
Extensive qualitative, quantitative, and perceptual evaluations demonstrate the
effectiveness of our method in faithfully reproducing the desired style while
achieving superior lip synchronization across various speech scenarios.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [186] [A flexible Bayesian non-parametric mixture model reveals multiple dependencies of swap errors in visual working memory](https://arxiv.org/abs/2505.01178)
*Puria Radmard,Paul M. Bays,Máté Lengyel*

Main category: q-bio.NC

TL;DR: 论文提出了一种贝叶斯非参数混合模型（BNS）来研究视觉工作记忆中的交换错误，发现交换错误不仅与提示相似性相关，还与报告特征的调制有关，挑战了以往认为错误主要源于记忆存储或检索的观点。


<details>
  <summary>Details</summary>
Motivation: 视觉工作记忆中的交换错误机制尚不明确，以往研究多假设错误源于存储或检索阶段，但缺乏数据驱动的验证。

Method: 研究引入贝叶斯非参数混合模型（BNS），无需预设错误形式，通过拟合人类行为数据灵活分析交换错误的行为模式。

Result: BNS揭示了交换错误除与提示相似性相关外，还存在非单调调制现象，表明记忆编码可能是重要来源。

Conclusion: BNS分析表明以往对交换错误的解释可能不完整，记忆编码的作用值得进一步研究。

Abstract: Human behavioural data in psychophysics has been used to elucidate the
underlying mechanisms of many cognitive processes, such as attention,
sensorimotor integration, and perceptual decision making. Visual working memory
has particularly benefited from this approach: analyses of VWM errors have
proven crucial for understanding VWM capacity and coding schemes, in turn
constraining neural models of both. One poorly understood class of VWM errors
are swap errors, whereby participants recall an uncued item from memory. Swap
errors could arise from erroneous memory encoding, noisy storage, or errors at
retrieval time - previous research has mostly implicated the latter two.
However, these studies made strong a priori assumptions on the detailed
mechanisms and/or parametric form of errors contributed by these sources. Here,
we pursue a data-driven approach instead, introducing a Bayesian non-parametric
mixture model of swap errors (BNS) which provides a flexible descriptive model
of swapping behaviour, such that swaps are allowed to depend on both the probed
and reported features of every stimulus item. We fit BNS to the trial-by-trial
behaviour of human participants and show that it recapitulates the strong
dependence of swaps on cue similarity in multiple datasets. Critically, BNS
reveals that this dependence coexists with a non-monotonic modulation in the
report feature dimension for a random dot motion direction-cued,
location-reported dataset. The form of the modulation inferred by BNS opens new
questions about the importance of memory encoding in causing swap errors in
VWM, a distinct source to the previously suggested binding and cueing errors.
Our analyses, combining qualitative comparisons of the highly interpretable BNS
parameter structure with rigorous quantitative model comparison and recovery
methods, show that previous interpretations of swap errors may have been
incomplete.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [187] [On Simulating Thin-Film Processes at the Atomic Scale Using Machine Learned Force Fields](https://arxiv.org/abs/2505.01118)
*S. Kondati Natarajan,J. Schneider,N. Pandey,J. Wellendorff,S. Smidstrup*

Main category: cond-mat.mtrl-sci

TL;DR: 本文介绍了如何高效构建适用于工艺模拟的机器学习力场（MLFF），并以HfO2原子层沉积和MoS2原子层刻蚀为例展示了其应用。


<details>
  <summary>Details</summary>
Motivation: 通过原子尺度建模揭示薄膜工艺的关键化学机制和定量指标，传统分子动力学方法因缺乏通用力场而受限，MLFF提供了解决方案。

Method: 提出一种高效构建MLFF的方法，并将其应用于HfO2原子层沉积和MoS2原子层刻蚀的工艺模拟。

Result: 展示了MLFF在工业相关工艺模拟中的可行性，验证了其准确性和效率。

Conclusion: MLFF为复杂工艺的原子尺度模拟提供了有效工具，推动了计算材料与表面科学的发展。

Abstract: Atomistic modeling of thin-film processes provides an avenue not only for
discovering key chemical mechanisms of the processes but also to extract
quantitative metrics on the events and reactions taking place at the
gas-surface interface. Molecular dynamics (MD) is a powerful computational
method to study the evolution of a process at the atomic scale, but studies of
industrially relevant processes usually require suitable force fields, which
are in general not available for all processes of interest. However, machine
learned force fields (MLFF) are conquering the field of computational materials
and surface science. In this paper, we demonstrate how to efficiently build
MLFFs suitable for process simulations and provide two examples for
technologically relevant processes: precursor pulse in the atomic layer
deposition of HfO2 and atomic layer etching of MoS2.

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [188] [Primality Testing via Circulant Matrix Eigenvalue Structure: A Novel Approach Using Cyclotomic Field Theory](https://arxiv.org/abs/2505.00730)
*Marius-Constantin Dinu*

Main category: cs.SC

TL;DR: 本文提出了一种基于循环矩阵特征值结构的新型素数测试方法，证明了整数n>2为素数的充要条件是其对应的循环矩阵的极小多项式在有理数域上恰好有两个不可约因子。该方法通过连接分圆域理论与矩阵代数，提供了理论洞察与实践应用，并通过实验验证了其有效性和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索素数性质的代数结构表征，通过结合分圆域理论和矩阵代数，提出一种确定性素数测试方法，为素数判别提供新的理论视角和实用工具。

Method: 方法基于构造由单位根生成的循环矩阵，通过分析其极小多项式的不可约因子数量来判别素性。该方法利用了本原单位根、伽罗瓦理论及分圆多项式分解的关系。

Result: 实验验证表明，该方法能准确区分素数与非素数，且提供了一种确定性的素数测试工具，其计算复杂度与代数基础相关。

Conclusion: 结论表明，该方法不仅从理论上揭示了素数的代数特征，还提供了一种可实践的确定性素数测试方案，为素数研究提供了新的视角和工具。

Abstract: This paper presents a novel primality test based on the eigenvalue structure
of circulant matrices constructed from roots of unity. We prove that an integer
$n > 2$ is prime if and only if the minimal polynomial of the circulant matrix
$C_n = W_n + W_n^2$ has exactly two irreducible factors over $\mathbb{Q}$. This
characterization connects cyclotomic field theory with matrix algebra,
providing both theoretical insights and practical applications. We demonstrate
that the eigenvalue patterns of these matrices reveal fundamental distinctions
between prime and composite numbers, leading to a deterministic primality test.
Our approach leverages the relationship between primitive roots of unity,
Galois theory, and the factorization of cyclotomic polynomials. We provide
comprehensive experimental validation across various ranges of integers,
discuss practical implementation considerations, and analyze the computational
complexity of our method in comparison with established primality tests. The
visual interpretation of our mathematical framework provides intuitive
understanding of the algebraic structures that distinguish prime numbers. Our
experimental validation demonstrates that our approach offers a deterministic
alternative to existing methods, with performance characteristics reflecting
its algebraic foundations.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [189] [Dynamical System Parameter Path Optimization using Persistent Homology](https://arxiv.org/abs/2505.00782)
*Max M. Chumley,Firas A. Khasawneh*

Main category: math.DS

TL;DR: 该论文提出了一种基于拓扑数据分析的新方法，用于在高维参数空间中优化导航非线性动力系统，通过梯度下降实现参数空间中的路径规划，以达到期望的系统响应拓扑特征。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于高维参数空间中难以确定参数变化方向以实现期望系统响应，传统方法复杂且难以应用。

Method: 方法包括利用持久图的可微性定义拓扑语言，结合梯度下降优化参数空间中的路径。

Result: 结果显示该方法能有效引导系统至具有所需拓扑特征的参数集，并在多种动力系统和场景中验证了其效果。

Conclusion: 结论表明该方法为动力系统参数优化提供了一种新工具，能够灵活应用于不同需求和场景。

Abstract: Nonlinear dynamical systems are complex and typically only simple systems can
be analytically studied. In applications, these systems are usually defined
with a set of tunable parameters and as the parameters are varied the system
response undergoes significant topological changes or bifurcations. In a high
dimensional parameter space, it is difficult to determine which direction to
vary the system parameters to achieve a desired system response or state. In
this paper, we introduce a new approach for optimally navigating a dynamical
system parameter space that is rooted in topological data analysis.
Specifically we use the differentiability of persistence diagrams to define a
topological language for intuitively promoting or deterring different
topological features in the state space response of a dynamical system and use
gradient descent to optimally move from one point in the parameter space to
another. The end result is a path in this space that guides the system to a set
of parameters that yield the desired topological features defined by the loss
function. We show a number of examples by applying the methods to different
dynamical systems and scenarios to demonstrate how to promote different
features and how to choose the hyperparameters to achieve different outcomes.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [190] [Quantum Support Vector Regression for Robust Anomaly Detection](https://arxiv.org/abs/2505.01012)
*Kilian Tscharke,Maximilian Wendlinger,Sebastian Issel,Pascal Debus*

Main category: quant-ph

TL;DR: 论文探讨了量子机器学习在异常检测中的应用，特别是量子核方法，通过IBM量子硬件基准测试，结果显示QSVR在特定数据集上表现优异，并对噪声表现出一定鲁棒性，但对对抗攻击脆弱。


<details>
  <summary>Details</summary>
Motivation: IT安全领域的异常检测需求日益增长，传统机器学习方法面临挑战，量子机器学习提供了新的可能性。

Method: 采用量子支持向量回归（QSVR）方法，在IBM量子硬件上对11个数据集进行基准测试，分析噪声影响及对抗攻击表现。

Result: QSVR在部分数据集上表现优于无噪声模拟，对某些噪声类型表现出鲁棒性，但对对抗攻击极度脆弱，噪声未提升其对抗攻击的鲁棒性。

Conclusion: 量子机器学习在异常检测中具有潜力，但需进一步解决噪声和对抗攻击的问题。

Abstract: Anomaly Detection (AD) is critical in data analysis, particularly within the
domain of IT security. In recent years, Machine Learning (ML) algorithms have
emerged as a powerful tool for AD in large-scale data. In this study, we
explore the potential of quantum ML approaches, specifically quantum kernel
methods, for the application to robust AD. We build upon previous work on
Quantum Support Vector Regression (QSVR) for semisupervised AD by conducting a
comprehensive benchmark on IBM quantum hardware using eleven datasets. Our
results demonstrate that QSVR achieves strong classification performance and
even outperforms the noiseless simulation on two of these datasets. Moreover,
we investigate the influence of - in the NISQ-era inevitable - quantum noise on
the performance of the QSVR. Our findings reveal that the model exhibits
robustness to depolarizing, phase damping, phase flip, and bit flip noise,
while amplitude damping and miscalibration noise prove to be more disruptive.
Finally, we explore the domain of Quantum Adversarial Machine Learning and
demonstrate that QSVR is highly vulnerable to adversarial attacks and that
noise does not improve the adversarial robustness of the model.

</details>
