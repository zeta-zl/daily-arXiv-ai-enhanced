{"id": "2504.18560", "pdf": "https://arxiv.org/pdf/2504.18560", "abs": "https://arxiv.org/abs/2504.18560", "authors": ["Alessio Buscemi", "C\u00e9dric Lothritz", "Sergio Morales", "Marcos Gomez-Vazquez", "Robert Claris\u00f3", "Jordi Cabot", "German Castignani"], "title": "Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High- and Low-Resource Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have exhibited impressive natural language\nprocessing capabilities but often perpetuate social biases inherent in their\ntraining data. To address this, we introduce MultiLingual Augmented Bias\nTesting (MLA-BiTe), a framework that improves prior bias evaluation methods by\nenabling systematic multilingual bias testing. MLA-BiTe leverages automated\ntranslation and paraphrasing techniques to support comprehensive assessments\nacross diverse linguistic settings. In this study, we evaluate the\neffectiveness of MLA-BiTe by testing four state-of-the-art LLMs in six\nlanguages -- including two low-resource languages -- focusing on seven\nsensitive categories of discrimination.", "AI": {"tldr": "MLA-BiTe\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u591a\u8bed\u8a00\u504f\u89c1\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u7ffb\u8bd1\u548c\u6539\u5199\u6280\u672f\u8bc4\u4f30LLM\u5728\u4e0d\u540c\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u504f\u89c1\uff0c\u6d4b\u8bd5\u4e86\u56db\u79cd\u5148\u8fdbLLM\u5728\u516d\u79cd\u8bed\u8a00\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1LLM\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u80fd\u529b\uff0c\u4f46\u5176\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\u95ee\u9898\u4f9d\u65e7\u5b58\u5728\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u7cfb\u7edf\u6027\u591a\u8bed\u8a00\u504f\u89c1\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u4ee5\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u548c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528MultiLingual Augmented Bias Testing (MLA-BiTe)\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u52a8\u7ffb\u8bd1\u548c\u6539\u5199\u6280\u672f\uff0c\u8bc4\u4f30\u56db\u79cdLLM\u5728\u516d\u79cd\u8bed\u8a00\uff08\u542b\u4e24\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\uff09\u4e2d\u7684\u504f\u89c1\u8868\u73b0\uff0c\u6db5\u76d6\u4e03\u79cd\u654f\u611f\u6b67\u89c6\u7c7b\u522b\u3002", "result": "MLA-BiTe\u6846\u67b6\u6210\u529f\u652f\u6301\u4e86\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u7cfb\u7edf\u6027\u504f\u89c1\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86LLM\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u7684\u504f\u89c1\u8868\u73b0\u3002", "conclusion": "MLA-BiTe\u4e3a\u591a\u8bed\u8a00\u504f\u89c1\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u672a\u6765\u53ef\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684LLM\u504f\u89c1\u6d4b\u8bd5\u548c\u7f13\u89e3\u7814\u7a76\u3002"}}
{"id": "2504.18639", "pdf": "https://arxiv.org/pdf/2504.18639", "abs": "https://arxiv.org/abs/2504.18639", "authors": ["Passant Elchafei", "Mervet Abu-Elkheir"], "title": "Span-Level Hallucination Detection for LLM-Generated Answers", "categories": ["cs.CL"], "comment": null, "summary": "Detecting spans of hallucination in LLM-generated answers is crucial for\nimproving factual consistency. This paper presents a span-level hallucination\ndetection framework for the SemEval-2025 Shared Task, focusing on English and\nArabic texts. Our approach integrates Semantic Role Labeling (SRL) to decompose\nthe answer into atomic roles, which are then compared with a retrieved\nreference context obtained via question-based LLM prompting. Using a\nDeBERTa-based textual entailment model, we evaluate each role semantic\nalignment with the retrieved context. The entailment scores are further refined\nthrough token-level confidence measures derived from output logits, and the\ncombined scores are used to detect hallucinated spans. Experiments on the\nMu-SHROOM dataset demonstrate competitive performance. Additionally,\nhallucinated spans have been verified through fact-checking by prompting GPT-4\nand LLaMA. Our findings contribute to improving hallucination detection in\nLLM-generated responses.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89d2\u8272\u8bed\u4e49\u6807\u6ce8\uff08SRL\uff09\u7684\u8de8\u5ea6\u7ea7\u5e7b\u89c9\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u53c2\u8003\u4e0a\u4e0b\u6587\u7684\u6587\u672c\u8574\u542b\u6a21\u578b\u548c\u7f6e\u4fe1\u5ea6\u8861\u91cf\uff0c\u5728Mu-SHROOM\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u901a\u8fc7GPT-4\u548cLLaMA\u9a8c\u8bc1\u4e86\u5e7b\u89c9\u8de8\u5ea6\u3002", "motivation": "\u68c0\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7b54\u6848\u4e2d\u7684\u5e7b\u89c9\u8de8\u5ea6\u5bf9\u63d0\u5347\u4e8b\u5b9e\u4e00\u81f4\u6027\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u82f1\u8bed\u548c\u963f\u62c9\u4f2f\u8bed\u6587\u672c\u4e2d\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u5206\u6790\u6765\u6709\u6548\u8bc6\u522b\u5e7b\u89c9\u5185\u5bb9\u3002", "method": "\u8bba\u6587\u91c7\u7528\u8bed\u4e49\u89d2\u8272\u6807\u6ce8\uff08SRL\uff09\u5c06\u7b54\u6848\u5206\u89e3\u4e3a\u539f\u5b50\u89d2\u8272\uff0c\u4e0e\u901a\u8fc7\u95ee\u9898\u63d0\u793a\u68c0\u7d22\u7684\u53c2\u8003\u4e0a\u4e0b\u6587\u8fdb\u884c\u6bd4\u8f83\uff0c\u57fa\u4e8eDeBERTa\u7684\u6587\u672c\u8574\u542b\u6a21\u578b\u8bc4\u4f30\u8bed\u4e49\u5bf9\u9f50\uff0c\u5e76\u901a\u8fc7\u8f93\u51falogits\u7684\u7f6e\u4fe1\u5ea6\u4f18\u5316\u5206\u6570\u3002", "result": "\u5728Mu-SHROOM\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u5e7b\u89c9\u8de8\u5ea6\u7ecfGPT-4\u548cLLaMA\u9a8c\u8bc1\u6709\u6548\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aLLM\u751f\u6210\u56de\u7b54\u4e2d\u7684\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7ec6\u7c92\u5ea6\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u4e49\u5206\u6790\u548c\u7f6e\u4fe1\u5ea6\u4f18\u5316\uff0c\u63d0\u5347\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2504.18673", "pdf": "https://arxiv.org/pdf/2504.18673", "abs": "https://arxiv.org/abs/2504.18673", "authors": ["Jiayi Li", "Yingfan Zhou", "Pranav Narayanan Venkit", "Halima Binte Islam", "Sneha Arya", "Shomir Wilson", "Sarah Rajtmajer"], "title": "Can Third-parties Read Our Emotions?", "categories": ["cs.CL"], "comment": null, "summary": "Natural Language Processing tasks that aim to infer an author's private\nstates, e.g., emotions and opinions, from their written text, typically rely on\ndatasets annotated by third-party annotators. However, the assumption that\nthird-party annotators can accurately capture authors' private states remains\nlargely unexamined. In this study, we present human subjects experiments on\nemotion recognition tasks that directly compare third-party annotations with\nfirst-party (author-provided) emotion labels. Our findings reveal significant\nlimitations in third-party annotations-whether provided by human annotators or\nlarge language models (LLMs)-in faithfully representing authors' private\nstates. However, LLMs outperform human annotators nearly across the board. We\nfurther explore methods to improve third-party annotation quality. We find that\ndemographic similarity between first-party authors and third-party human\nannotators enhances annotation performance. While incorporating first-party\ndemographic information into prompts leads to a marginal but statistically\nsignificant improvement in LLMs' performance. We introduce a framework for\nevaluating the limitations of third-party annotations and call for refined\nannotation practices to accurately represent and model authors' private states.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u7b2c\u4e09\u65b9\u6807\u6ce8\uff08\u5305\u62ec\u4eba\u7c7b\u548cLLMs\uff09\u5728\u6355\u6349\u4f5c\u8005\u771f\u5b9e\u60c5\u7eea\u65f6\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0LLMs\u8868\u73b0\u4f18\u4e8e\u4eba\u7c7b\u6807\u6ce8\u8005\uff0c\u5e76\u63d0\u51fa\u4e86\u901a\u8fc7\u4f5c\u8005\u4e0e\u6807\u6ce8\u8005\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u76f8\u4f3c\u6027\u6216\u4fe1\u606f\u63d0\u793a\u6765\u6539\u8fdb\u6807\u6ce8\u8d28\u91cf\u7684\u6846\u67b6\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u9a8c\u8bc1\u7b2c\u4e09\u65b9\u6807\u6ce8\u662f\u5426\u80fd\u51c6\u786e\u53cd\u6620\u4f5c\u8005\u7684\u771f\u5b9e\u60c5\u7eea\u72b6\u6001\uff0c\u56e0\u4e3a\u73b0\u6709\u7684NLP\u4efb\u52a1\u901a\u5e38\u4f9d\u8d56\u7b2c\u4e09\u65b9\u6807\u6ce8\uff0c\u4f46\u8fd9\u4e00\u5047\u8bbe\u5c1a\u672a\u5145\u5206\u9a8c\u8bc1\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u76f4\u63a5\u6bd4\u8f83\u7b2c\u4e09\u65b9\uff08\u4eba\u7c7b\u548cLLMs\uff09\u4e0e\u7b2c\u4e00\u65b9\uff08\u4f5c\u8005\u81ea\u8eab\uff09\u7684\u60c5\u7eea\u6807\u6ce8\uff0c\u5e76\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u4eba\u53e3\u7edf\u8ba1\u5b66\u76f8\u4f3c\u6027\u6216\u4fe1\u606f\u63d0\u793a\u6539\u8fdb\u6807\u6ce8\u8d28\u91cf\u3002", "result": "\u53d1\u73b0\u7b2c\u4e09\u65b9\u6807\u6ce8\uff08\u5c24\u5176\u662fLLMs\uff09\u4f18\u4e8e\u4eba\u7c7b\u6807\u6ce8\u8005\uff0c\u4f46\u4ecd\u6709\u663e\u8457\u5c40\u9650\u6027\uff1b\u4eba\u53e3\u7edf\u8ba1\u5b66\u76f8\u4f3c\u6027\u6216\u4fe1\u606f\u63d0\u793a\u80fd\u7565\u5fae\u63d0\u5347\u6807\u6ce8\u51c6\u786e\u6027\u3002", "conclusion": "\u7ed3\u8bba\u547c\u5401\u6539\u8fdb\u6807\u6ce8\u5b9e\u8df5\u4ee5\u66f4\u51c6\u786e\u5730\u5efa\u6a21\u4f5c\u8005\u7684\u771f\u5b9e\u60c5\u7eea\u72b6\u6001\uff0c\u5e76\u63d0\u51fa\u4e86\u8bc4\u4f30\u7b2c\u4e09\u65b9\u6807\u6ce8\u5c40\u9650\u6027\u7684\u6846\u67b6\u3002"}}
{"id": "2504.18715", "pdf": "https://arxiv.org/pdf/2504.18715", "abs": "https://arxiv.org/abs/2504.18715", "authors": ["Tuochao Chen", "Qirui Wang", "Runlin He", "Shyam Gollakota"], "title": "Spatial Speech Translation: Translating Across Space With Binaural Hearables", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted by CHI2025", "summary": "Imagine being in a crowded space where people speak a different language and\nhaving hearables that transform the auditory space into your native language,\nwhile preserving the spatial cues for all speakers. We introduce spatial speech\ntranslation, a novel concept for hearables that translate speakers in the\nwearer's environment, while maintaining the direction and unique voice\ncharacteristics of each speaker in the binaural output. To achieve this, we\ntackle several technical challenges spanning blind source separation,\nlocalization, real-time expressive translation, and binaural rendering to\npreserve the speaker directions in the translated audio, while achieving\nreal-time inference on the Apple M2 silicon. Our proof-of-concept evaluation\nwith a prototype binaural headset shows that, unlike existing models, which\nfail in the presence of interference, we achieve a BLEU score of up to 22.01\nwhen translating between languages, despite strong interference from other\nspeakers in the environment. User studies further confirm the system's\neffectiveness in spatially rendering the translated speech in previously unseen\nreal-world reverberant environments. Taking a step back, this work marks the\nfirst step towards integrating spatial perception into speech translation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u201c\u7a7a\u95f4\u8bed\u97f3\u7ffb\u8bd1\u201d\u6982\u5ff5\uff0c\u901a\u8fc7\u667a\u80fd\u8033\u673a\u5b9e\u73b0\u5728\u5608\u6742\u73af\u5883\u4e2d\u5c06\u5916\u8bed\u5b9e\u65f6\u7ffb\u8bd1\u4e3a\u6bcd\u8bed\uff0c\u540c\u65f6\u4fdd\u7559\u539f\u8bf4\u8bdd\u8005\u7684\u7a7a\u95f4\u65b9\u4f4d\u548c\u58f0\u97f3\u7279\u5f81\u3002\u6280\u672f\u6311\u6218\u5305\u62ec\u76f2\u6e90\u5206\u79bb\u3001\u5b9a\u4f4d\u3001\u5b9e\u65f6\u8868\u8fbe\u6027\u7ffb\u8bd1\u548c\u53cc\u8033\u6e32\u67d3\uff0c\u5e76\u5728Apple M2\u82af\u7247\u4e0a\u5b9e\u73b0\u4e86\u5b9e\u65f6\u63a8\u7406\u3002\u539f\u578b\u6d4b\u8bd5\u663e\u793a\u5176BLEU\u5f97\u5206\u9ad8\u8fbe22.01\uff0c\u4e14\u7528\u6237\u7814\u7a76\u8bc1\u5b9e\u4e86\u7cfb\u7edf\u5728\u771f\u5b9e\u6df7\u54cd\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8bed\u97f3\u7ffb\u8bd1\u5728\u5608\u6742\u548c\u591a\u8bf4\u8bdd\u8005\u73af\u5883\u4e2d\u5931\u6548\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u7559\u8bf4\u8bdd\u8005\u7684\u7a7a\u95f4\u611f\u77e5\u4fe1\u606f\u3002", "method": "\u7ed3\u5408\u76f2\u6e90\u5206\u79bb\u3001\u8bf4\u8bdd\u8005\u5b9a\u4f4d\u3001\u5b9e\u65f6\u8868\u8fbe\u6027\u7ffb\u8bd1\u548c\u53cc\u8033\u6e32\u67d3\u6280\u672f\uff0c\u5e76\u5728Apple M2\u82af\u7247\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u5904\u7406\u3002", "result": "\u539f\u578b\u6d4b\u8bd5\u5728\u5f3a\u5e72\u6270\u4e0b\u53d6\u5f97\u4e86\u6700\u9ad822.01\u7684BLEU\u5206\u6570\uff0c\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u7a7a\u95f4\u6e32\u67d3\u6548\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5c06\u7a7a\u95f4\u611f\u77e5\u878d\u5165\u8bed\u97f3\u7ffb\u8bd1\uff0c\u4e3a\u667a\u80fd\u8033\u673a\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.18572", "pdf": "https://arxiv.org/pdf/2504.18572", "abs": "https://arxiv.org/abs/2504.18572", "authors": ["Syed Quiser Ahmed", "Bharathi Vokkaliga Ganesh", "Jagadish Babu P", "Karthick Selvaraj", "ReddySiva Naga Parvathi Devi", "Sravya Kappala"], "title": "BELL: Benchmarking the Explainability of Large Language Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models have demonstrated remarkable capabilities in natural\nlanguage processing, yet their decision-making processes often lack\ntransparency. This opaqueness raises significant concerns regarding trust,\nbias, and model performance. To address these issues, understanding and\nevaluating the interpretability of LLMs is crucial. This paper introduces a\nstandardised benchmarking technique, Benchmarking the Explainability of Large\nLanguage Models, designed to evaluate the explainability of large language\nmodels.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6807\u51c6\u5316\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u89e3\u91ca\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u51b3\u7b56\u8fc7\u7a0b\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u5f15\u53d1\u4e86\u5bf9\u4fe1\u4efb\u3001\u504f\u89c1\u548c\u6027\u80fd\u7684\u62c5\u5fe7\u3002", "method": "\u5f15\u5165\u4e86\u6807\u51c6\u5316\u57fa\u51c6\u6280\u672f\uff08Benchmarking the Explainability of Large Language Models\uff09\uff0c\u8bc4\u4f30LLM\u7684\u89e3\u91ca\u6027\u3002", "result": "\u672a\u63d0\u4f9b\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\u3002", "conclusion": "\u8bc4\u4f30LLM\u7684\u89e3\u91ca\u6027\u5bf9\u89e3\u51b3\u5176\u4e0d\u900f\u660e\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2504.18544", "pdf": "https://arxiv.org/pdf/2504.18544", "abs": "https://arxiv.org/abs/2504.18544", "authors": ["Nazia Nafis", "Inaki Esnaola", "Alvaro Martinez-Perez", "Maria-Cruz Villa-Uriol", "Venet Osmani"], "title": "Critical Challenges and Guidelines in Evaluating Synthetic Tabular Data: A Systematic Review", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": null, "summary": "Generating synthetic tabular data can be challenging, however evaluation of\ntheir quality is just as challenging, if not more. This systematic review sheds\nlight on the critical importance of rigorous evaluation of synthetic health\ndata to ensure reliability, relevance, and their appropriate use. Based on\nscreening of 1766 papers and a detailed review of 101 papers we identified key\nchallenges, including lack of consensus on evaluation methods, improper use of\nevaluation metrics, limited input from domain experts, inadequate reporting of\ndataset characteristics, and limited reproducibility of results. In response,\nwe provide several guidelines on the generation and evaluation of synthetic\ndata, to allow the community to unlock and fully harness the transformative\npotential of synthetic data and accelerate innovation.", "AI": {"tldr": "\u672c\u6587\u603b\u7ed3\u4e86\u5408\u6210\u5065\u5eb7\u6570\u636e\u8bc4\u4f30\u7684\u91cd\u8981\u6027\uff0c\u5e76\u57fa\u4e8e1766\u7bc7\u8bba\u6587\u7684\u7b5b\u9009\u548c101\u7bc7\u7684\u8be6\u7ec6\u56de\u987e\uff0c\u63d0\u51fa\u4e86\u76f8\u5173\u6311\u6218\u548c\u6307\u5357\u3002", "motivation": "\u5f3a\u8c03\u5408\u6210\u5065\u5eb7\u6570\u636e\u8bc4\u4f30\u7684\u4e25\u683c\u6027\uff0c\u4ee5\u786e\u4fdd\u6570\u636e\u7684\u53ef\u9760\u6027\u3001\u76f8\u5173\u6027\u548c\u9002\u7528\u6027\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff01766\u7bc7\u8bba\u6587\u5e76\u6df1\u5165\u5206\u6790101\u7bc7\uff0c\u8bc6\u522b\u5173\u952e\u6311\u6218\u3002", "result": "\u53d1\u73b0\u8bc4\u4f30\u65b9\u6cd5\u7f3a\u4e4f\u5171\u8bc6\u3001\u6307\u6807\u4f7f\u7528\u4e0d\u5f53\u3001\u9886\u57df\u4e13\u5bb6\u53c2\u4e0e\u4e0d\u8db3\u3001\u6570\u636e\u96c6\u7279\u5f81\u62a5\u544a\u4e0d\u5145\u5206\u4ee5\u53ca\u7ed3\u679c\u53ef\u91cd\u590d\u6027\u6709\u9650\u7b49\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u5408\u6210\u6570\u636e\u751f\u6210\u4e0e\u8bc4\u4f30\u7684\u6307\u5357\uff0c\u4ee5\u4fc3\u8fdb\u5176\u53d8\u9769\u6f5c\u529b\u5e76\u52a0\u901f\u521b\u65b0\u3002"}}
{"id": "2504.18718", "pdf": "https://arxiv.org/pdf/2504.18718", "abs": "https://arxiv.org/abs/2504.18718", "authors": ["Lauren Levine", "Junghyun Min", "Amir Zeldes"], "title": "Building UD Cairo for Old English in the Classroom", "categories": ["cs.CL"], "comment": "7 pages, 2 figures", "summary": "In this paper we present a sample treebank for Old English based on the UD\nCairo sentences, collected and annotated as part of a classroom curriculum in\nHistorical Linguistics. To collect the data, a sample of 20 sentences\nillustrating a range of syntactic constructions in the world's languages, we\nemploy a combination of LLM prompting and searches in authentic Old English\ndata. For annotation we assigned sentences to multiple students with limited\nprior exposure to UD, whose annotations we compare and adjudicate. Our results\nsuggest that while current LLM outputs in Old English do not reflect authentic\nsyntax, this can be mitigated by post-editing, and that although beginner\nannotators do not possess enough background to complete the task perfectly,\ntaken together they can produce good results and learn from the experience. We\nalso conduct preliminary parsing experiments using Modern English training\ndata, and find that although performance on Old English is poor, parsing on\nannotated features (lemma, hyperlemma, gloss) leads to improved performance.", "AI": {"tldr": "\u603b\u7ed3\uff1a\u8bba\u6587\u57fa\u4e8eUD Cairo\u53e5\u5b50\u4e3a\u53e4\u82f1\u8bed\u6784\u5efa\u4e86\u4e00\u4e2a\u6837\u672c\u6811\u5e93\uff0c\u901a\u8fc7LLM\u63d0\u793a\u548c\u771f\u5b9e\u53e4\u82f1\u8bed\u6570\u636e\u641c\u7d22\u6536\u96c6\u6570\u636e\uff0c\u7531\u5b66\u751f\u8fdb\u884c\u6807\u6ce8\u5e76\u6bd4\u8f83\u3002\u7ed3\u679c\u663e\u793aLLM\u8f93\u51fa\u7684\u53e4\u82f1\u8bed\u8bed\u6cd5\u4e0d\u591f\u771f\u5b9e\u4f46\u53ef\u901a\u8fc7\u7f16\u8f91\u6539\u5584\uff0c\u521d\u7ea7\u6807\u6ce8\u5458\u867d\u4e0d\u5b8c\u7f8e\u4f46\u96c6\u4f53\u80fd\u4ea7\u51fa\u826f\u597d\u7ed3\u679c\u3002\u73b0\u4ee3\u82f1\u8bed\u8bad\u7ec3\u6570\u636e\u5bf9\u53e4\u82f1\u8bed\u7684\u89e3\u6790\u6548\u679c\u4e0d\u4f73\uff0c\u4f46\u5bf9\u6807\u6ce8\u7279\u5f81\uff08\u5982\u8bcd\u5143\u3001\u8d85\u8bcd\u5143\u3001\u6ce8\u91ca\uff09\u8fdb\u884c\u89e3\u6790\u80fd\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u52a8\u673a\uff1a\u4e3a\u53e4\u82f1\u8bed\u6784\u5efa\u4e00\u4e2a\u6837\u672c\u6811\u5e93\uff0c\u63a2\u7d22\u5982\u4f55\u5229\u7528\u73b0\u4ee3\u6280\u672f\uff08\u5982LLM\uff09\u548c\u5b66\u751f\u6807\u6ce8\u6765\u5e94\u5bf9\u53e4\u82f1\u8bed\u6570\u636e\u7a00\u7f3a\u548c\u6807\u6ce8\u56f0\u96be\u7684\u6311\u6218\u3002", "method": "\u65b9\u6cd5\uff1a\u901a\u8fc7LLM\u63d0\u793a\u548c\u771f\u5b9e\u53e4\u82f1\u8bed\u6570\u636e\u641c\u7d22\u6536\u96c620\u4e2a\u53e5\u5b50\u6837\u672c\uff0c\u7531\u5b66\u751f\u8fdb\u884cUD\u6807\u6ce8\u5e76\u6bd4\u8f83\u7ed3\u679c\uff0c\u540c\u65f6\u7528\u73b0\u4ee3\u82f1\u8bed\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u521d\u6b65\u89e3\u6790\u5b9e\u9a8c\u3002", "result": "\u7ed3\u679c\uff1aLLM\u751f\u6210\u7684\u53e4\u82f1\u8bed\u8bed\u6cd5\u4e0d\u591f\u771f\u5b9e\u4f46\u53ef\u7f16\u8f91\u4f18\u5316\uff1b\u521d\u7ea7\u6807\u6ce8\u5458\u7684\u96c6\u4f53\u6807\u6ce8\u6548\u679c\u826f\u597d\uff1b\u73b0\u4ee3\u82f1\u8bed\u6570\u636e\u5bf9\u53e4\u82f1\u8bed\u89e3\u6790\u6548\u679c\u5dee\uff0c\u4f46\u5bf9\u6807\u6ce8\u7279\u5f81\u7684\u89e3\u6790\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u7ed3\u8bba\uff1a\u7ed3\u5408\u4eba\u7c7b\u7f16\u8f91\u548c\u96c6\u4f53\u6807\u6ce8\u53ef\u6709\u6548\u5904\u7406\u53e4\u82f1\u8bed\u6570\u636e\uff0c\u6807\u6ce8\u7279\u5f81\u89e3\u6790\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.18600", "pdf": "https://arxiv.org/pdf/2504.18600", "abs": "https://arxiv.org/abs/2504.18600", "authors": ["Saizhuo Wang", "Hao Kong", "Jiadong Guo", "Fengrui Hua", "Yiyan Qi", "Wanyun Zhou", "Jiahao Zheng", "Xinyu Wang", "Lionel M. Ni", "Jian Guo"], "title": "QuantBench: Benchmarking AI Methods for Quantitative Investment", "categories": ["q-fin.CP", "cs.AI", "cs.CE"], "comment": null, "summary": "The field of artificial intelligence (AI) in quantitative investment has seen\nsignificant advancements, yet it lacks a standardized benchmark aligned with\nindustry practices. This gap hinders research progress and limits the practical\napplication of academic innovations. We present QuantBench, an industrial-grade\nbenchmark platform designed to address this critical need. QuantBench offers\nthree key strengths: (1) standardization that aligns with quantitative\ninvestment industry practices, (2) flexibility to integrate various AI\nalgorithms, and (3) full-pipeline coverage of the entire quantitative\ninvestment process. Our empirical studies using QuantBench reveal some critical\nresearch directions, including the need for continual learning to address\ndistribution shifts, improved methods for modeling relational financial data,\nand more robust approaches to mitigate overfitting in low signal-to-noise\nenvironments. By providing a common ground for evaluation and fostering\ncollaboration between researchers and practitioners, QuantBench aims to\naccelerate progress in AI for quantitative investment, similar to the impact of\nbenchmark platforms in computer vision and natural language processing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86QuantBench\uff0c\u4e00\u4e2a\u5de5\u4e1a\u7ea7\u57fa\u51c6\u5e73\u53f0\uff0c\u7528\u4e8e\u89e3\u51b3AI\u5728\u91cf\u5316\u6295\u8d44\u9886\u57df\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u7684\u95ee\u9898\uff0c\u65e8\u5728\u63a8\u52a8\u7814\u7a76\u548c\u5b9e\u8df5\u3002", "motivation": "\u5f53\u524dAI\u5728\u91cf\u5316\u6295\u8d44\u9886\u57df\u7f3a\u4e4f\u4e0e\u884c\u4e1a\u5b9e\u8df5\u4e00\u81f4\u7684\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u8fd9\u963b\u788d\u4e86\u7814\u7a76\u8fdb\u5c55\u548c\u5b66\u672f\u521b\u65b0\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86QuantBench\u5e73\u53f0\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u3001\u7075\u6d3b\u6027\u548c\u5168\u6d41\u7a0b\u8986\u76d6\uff0c\u652f\u6301\u591a\u79cdAI\u7b97\u6cd5\u7684\u96c6\u6210\u3002", "result": "\u901a\u8fc7QuantBench\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u53d1\u73b0\u4e86\u5173\u952e\u7814\u7a76\u65b9\u5411\uff0c\u5982\u5e94\u5bf9\u5206\u5e03\u53d8\u5316\u7684\u6301\u7eed\u5b66\u4e60\u3001\u91d1\u878d\u5173\u7cfb\u6570\u636e\u5efa\u6a21\u7684\u6539\u8fdb\u65b9\u6cd5\u7b49\u3002", "conclusion": "QuantBench\u901a\u8fc7\u63d0\u4f9b\u5171\u540c\u8bc4\u4f30\u6807\u51c6\u548c\u4fc3\u8fdb\u7814\u7a76\u4e0e\u5b9e\u8df5\u5408\u4f5c\uff0c\u6709\u671b\u52a0\u901fAI\u5728\u91cf\u5316\u6295\u8d44\u9886\u57df\u7684\u8fdb\u6b65\u3002"}}
{"id": "2504.18547", "pdf": "https://arxiv.org/pdf/2504.18547", "abs": "https://arxiv.org/abs/2504.18547", "authors": ["Ching-Yi Lin", "Sahil Shah"], "title": "Low-Bit Integerization of Vision Transformers using Operand Reodering for Efficient Hardware", "categories": ["cs.LG", "cs.CV", "cs.SY", "eess.SY"], "comment": "4 pages + references, 5 figures, 2 tables in IEEE double column\n  conference template", "summary": "Pre-trained vision transformers have achieved remarkable performance across\nvarious visual tasks but suffer from expensive computational and memory costs.\nWhile model quantization reduces memory usage by lowering precision, these\nmodels still incur significant computational overhead due to the dequantization\nbefore matrix operations. In this work, we analyze the computation graph and\npropose an integerization process based on operation reordering. Specifically,\nthe process delays dequantization until after matrix operations. This enables\nintegerized matrix multiplication and linear module by directly processing the\nquantized input. To validate our approach, we synthesize the self-attention\nmodule of ViT on a systolic array-based hardware. Experimental results show\nthat our low-bit inference reduces per-PE power consumption for linear layer\nand matrix multiplication, bridging the gap between quantized models and\nefficient inference.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u64cd\u4f5c\u91cd\u6392\u5e8f\u7684\u6574\u6570\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5ef6\u8fdf\u53cd\u91cf\u5316\u81f3\u77e9\u9635\u8fd0\u7b97\u4e4b\u540e\uff0c\u76f4\u63a5\u5728\u91cf\u5316\u8f93\u5165\u4e0a\u5904\u7406\uff0c\u4ee5\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u5728\u786c\u4ef6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f4e\u6bd4\u7279\u63a8\u7406\u7684\u80fd\u6548\u4f18\u52bf\u3002", "motivation": "\u9884\u8bad\u7ec3\u89c6\u89c9 Transformer \u5728\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u9ad8\u6602\u3002\u91cf\u5316\u867d\u80fd\u964d\u4f4e\u5185\u5b58\u4f7f\u7528\uff0c\u4f46\u53cd\u91cf\u5316\u64cd\u4f5c\u4ecd\u5e26\u6765\u663e\u8457\u8ba1\u7b97\u5f00\u9500\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u8ba1\u7b97\u56fe\uff0c\u51cf\u5c11\u8fd9\u4e9b\u5f00\u9500\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u64cd\u4f5c\u91cd\u6392\u5e8f\u7684\u6574\u6570\u5316\u65b9\u6cd5\uff0c\u5ef6\u8fdf\u53cd\u91cf\u5316\u81f3\u77e9\u9635\u8fd0\u7b97\u4e4b\u540e\uff0c\u4ece\u800c\u76f4\u63a5\u5728\u91cf\u5316\u8f93\u5165\u4e0a\u6267\u884c\u77e9\u9635\u4e58\u6cd5\u548c\u7ebf\u6027\u6a21\u5757\u64cd\u4f5c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4f4e\u6bd4\u7279\u63a8\u7406\uff0c\u964d\u4f4e\u4e86\u7ebf\u6027\u5c42\u548c\u77e9\u9635\u4e58\u6cd5\u7684\u80fd\u8017\uff0c\u7f29\u5c0f\u4e86\u91cf\u5316\u6a21\u578b\u4e0e\u9ad8\u6548\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u901a\u8fc7\u6574\u6570\u5316\u5904\u7406\u548c\u5ef6\u8fdf\u53cd\u91cf\u5316\u64cd\u4f5c\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u91cf\u5316\u6a21\u578b\u7684\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2504.18736", "pdf": "https://arxiv.org/pdf/2504.18736", "abs": "https://arxiv.org/abs/2504.18736", "authors": ["Jianyou Wang", "Weili Cao", "Kaicheng Wang", "Xiaoyue Wang", "Ashish Dalvi", "Gino Prasad", "Qishan Liang", "Hsuan-lin Her", "Ming Wang", "Qin Yang", "Gene W. Yeo", "David E. Neal", "Maxim Khan", "Christopher D. Rosin", "Ramamohan Paturi", "Leon Bergen"], "title": "EvidenceBench: A Benchmark for Extracting Evidence from Biomedical Papers", "categories": ["cs.CL"], "comment": null, "summary": "We study the task of automatically finding evidence relevant to hypotheses in\nbiomedical papers. Finding relevant evidence is an important step when\nresearchers investigate scientific hypotheses. We introduce EvidenceBench to\nmeasure models performance on this task, which is created by a novel pipeline\nthat consists of hypothesis generation and sentence-by-sentence annotation of\nbiomedical papers for relevant evidence, completely guided by and faithfully\nfollowing existing human experts judgment. We demonstrate the pipeline's\nvalidity and accuracy with multiple sets of human-expert annotations. We\nevaluated a diverse set of language models and retrieval systems on the\nbenchmark and found that model performances still fall significantly short of\nthe expert level on this task. To show the scalability of our proposed\npipeline, we create a larger EvidenceBench-100k with 107,461 fully annotated\npapers with hypotheses to facilitate model training and development. Both\ndatasets are available at https://github.com/EvidenceBench/EvidenceBench", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86EvidenceBench\uff0c\u7528\u4e8e\u8861\u91cf\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u8bba\u6587\u4e2d\u81ea\u52a8\u67e5\u627e\u76f8\u5173\u8bc1\u636e\u7684\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u4e13\u5bb6\u6807\u6ce8\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u4eba\u5458\u5728\u8c03\u67e5\u79d1\u5b66\u5047\u8bbe\u65f6\u9700\u8981\u627e\u5230\u76f8\u5173\u8bc1\u636e\uff0c\u8fd9\u662f\u4e00\u4e2a\u91cd\u8981\u6b65\u9aa4\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u7684\u5de5\u5177\u6765\u652f\u6301\u3002", "method": "\u901a\u8fc7\u5047\u8bbe\u751f\u6210\u548c\u9010\u53e5\u6807\u6ce8\u7684\u6d41\u7a0b\u521b\u5efaEvidenceBench\uff0c\u5e76\u4f7f\u7528\u4e86\u591a\u79cd\u8bed\u8a00\u6a21\u578b\u548c\u68c0\u7d22\u7cfb\u7edf\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u6a21\u578b\u6027\u80fd\u4ecd\u663e\u8457\u4f4e\u4e8e\u4e13\u5bb6\u6c34\u5e73\uff0c\u4f46\u9a8c\u8bc1\u4e86\u6d41\u7a0b\u7684\u53ef\u884c\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "EvidenceBench\u53ca\u5176\u6269\u5c55\u7248\u672cEvidenceBench-100k\u4e3a\u6a21\u578b\u8bad\u7ec3\u548c\u5f00\u53d1\u63d0\u4f9b\u4e86\u8d44\u6e90\uff0c\u5c55\u793a\u4e86\u4efb\u52a1\u7684\u6311\u6218\u6027\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2504.18604", "pdf": "https://arxiv.org/pdf/2504.18604", "abs": "https://arxiv.org/abs/2504.18604", "authors": ["Xingyu Xiao", "Peng Chen", "Jiejuan Tong", "Shunshun Liu", "Hongru Zhao", "Jun Zhao", "Qianqian Jia", "Jingang Liang", "Haitao Wang"], "title": "A Cognitive-Mechanistic Human Reliability Analysis Framework: A Nuclear Power Plant Case Study", "categories": ["cs.AI"], "comment": null, "summary": "Traditional human reliability analysis (HRA) methods, such as IDHEAS-ECA,\nrely on expert judgment and empirical rules that often overlook the cognitive\nunderpinnings of human error. Moreover, conducting human-in-the-loop\nexperiments for advanced nuclear power plants is increasingly impractical due\nto novel interfaces and limited operational data. This study proposes a\ncognitive-mechanistic framework (COGMIF) that enhances the IDHEAS-ECA\nmethodology by integrating an ACT-R-based human digital twin (HDT) with\nTimeGAN-augmented simulation. The ACT-R model simulates operator cognition,\nincluding memory retrieval, goal-directed procedural reasoning, and\nperceptual-motor execution, under high-fidelity scenarios derived from a\nhigh-temperature gas-cooled reactor (HTGR) simulator. To overcome the resource\nconstraints of large-scale cognitive modeling, TimeGAN is trained on\nACT-R-generated time-series data to produce high-fidelity synthetic operator\nbehavior datasets. These simulations are then used to drive IDHEAS-ECA\nassessments, enabling scalable, mechanism-informed estimation of human error\nprobabilities (HEPs). Comparative analyses with SPAR-H and sensitivity\nassessments demonstrate the robustness and practical advantages of the proposed\nCOGMIF. Finally, procedural features are mapped onto a Bayesian network to\nquantify the influence of contributing factors, revealing key drivers of\noperational risk. This work offers a credible and computationally efficient\npathway to integrate cognitive theory into industrial HRA practices.", "AI": {"tldr": "\u63d0\u51faCOGMIF\u6846\u67b6\uff0c\u6574\u5408ACT-R\u6570\u5b57\u5b6a\u751f\u4e0eTimeGAN\u589e\u5f3a\u4eff\u771f\uff0c\u6539\u8fdbIDHEAS-ECA\u65b9\u6cd5\uff0c\u91cf\u5316\u8ba4\u77e5\u673a\u5236\u5bf9\u6838\u7535\u7ad9\u64cd\u4f5c\u98ce\u9669\u7684\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edfHRA\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u7ecf\u9a8c\uff0c\u5ffd\u89c6\u8ba4\u77e5\u673a\u5236\uff0c\u4e14\u9ad8\u4fdd\u771f\u5b9e\u9a8c\u6210\u672c\u9ad8\uff0c\u9700\u65b0\u65b9\u6cd5\u63d0\u5347\u53ef\u4fe1\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u7ed3\u5408ACT-R\u6570\u5b57\u5b6a\u751f\u6a21\u62df\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u7528TimeGAN\u751f\u6210\u5408\u6210\u884c\u4e3a\u6570\u636e\uff0c\u9a71\u52a8IDHEAS-ECA\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u7f51\u7edc\u91cf\u5316\u98ce\u9669\u56e0\u7d20\u3002", "result": "COGMIF\u5728HEP\u4f30\u8ba1\u4e0a\u4f18\u4e8eSPAR-H\uff0c\u654f\u611f\u6027\u5206\u6790\u9a8c\u8bc1\u5176\u9c81\u68d2\u6027\uff0c\u5e76\u8bc6\u522b\u64cd\u4f5c\u98ce\u9669\u5173\u952e\u9a71\u52a8\u56e0\u7d20\u3002", "conclusion": "COGMIF\u4e3a\u8ba4\u77e5\u7406\u8bba\u4e0e\u5de5\u4e1aHRA\u5b9e\u8df5\u63d0\u4f9b\u9ad8\u6548\u6574\u5408\u8def\u5f84\uff0c\u589e\u5f3a\u673a\u5236\u5316\u98ce\u9669\u8bc4\u4f30\u80fd\u529b\u3002"}}
{"id": "2504.18556", "pdf": "https://arxiv.org/pdf/2504.18556", "abs": "https://arxiv.org/abs/2504.18556", "authors": ["Jialei Song", "Xingquan Zuo", "Feiyang Wang", "Hai Huang", "Tianle Zhang"], "title": "RDI: An adversarial robustness evaluation metric for deep neural networks based on sample clustering features", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep neural networks (DNNs) are highly susceptible to adversarial samples,\nraising concerns about their reliability in safety-critical tasks. Currently,\nmethods of evaluating adversarial robustness are primarily categorized into\nattack-based and certified robustness evaluation approaches. The former not\nonly relies on specific attack algorithms but also is highly time-consuming,\nwhile the latter due to its analytical nature, is typically difficult to\nimplement for large and complex models. A few studies evaluate model robustness\nbased on the model's decision boundary, but they suffer from low evaluation\naccuracy. To address the aforementioned issues, we propose a novel adversarial\nrobustness evaluation metric, Robustness Difference Index (RDI), which is based\non sample clustering features. RDI draws inspiration from clustering evaluation\nby analyzing the intra-class and inter-class distances of feature vectors\nseparated by the decision boundary to quantify model robustness. It is\nattack-independent and has high computational efficiency. Experiments show\nthat, RDI demonstrates a stronger correlation with the gold-standard\nadversarial robustness metric of attack success rate (ASR). The average\ncomputation time of RDI is only 1/30 of the evaluation method based on the PGD\nattack. Our open-source code is available at:\nhttps://anonymous.4open.science/r/RDI-B1DA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u8bc4\u4f30\u6307\u6807RDI\uff0c\u57fa\u4e8e\u6837\u672c\u805a\u7c7b\u7279\u5f81\uff0c\u5177\u6709\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u653b\u51fb\u72ec\u7acb\u6027\uff0c\u4e0e\u653b\u51fb\u6210\u529f\u7387\u76f8\u5173\u6027\u66f4\u5f3a\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u9c81\u68d2\u6027\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u653b\u51fb\u7b97\u6cd5\u6216\u96be\u4ee5\u5b9e\u73b0\uff0c\u800c\u57fa\u4e8e\u51b3\u7b56\u8fb9\u754c\u7684\u65b9\u6cd5\u8bc4\u4f30\u7cbe\u5ea6\u4f4e\uff0c\u9700\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51faRobustness Difference Index (RDI)\uff0c\u901a\u8fc7\u5206\u6790\u51b3\u7b56\u8fb9\u754c\u5206\u9694\u7684\u7279\u5f81\u5411\u91cf\u7684\u7c7b\u5185\u548c\u7c7b\u95f4\u8ddd\u79bb\u6765\u91cf\u5316\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eRDI\u4e0e\u653b\u51fb\u6210\u529f\u7387\u76f8\u5173\u6027\u66f4\u5f3a\uff0c\u8ba1\u7b97\u65f6\u95f4\u4ec5\u4e3aPGD\u653b\u51fb\u8bc4\u4f30\u65b9\u6cd5\u76841/30\u3002", "conclusion": "RDI\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u653b\u51fb\u72ec\u7acb\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u8bc4\u4f30\u6307\u6807\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u590d\u6742\u6a21\u578b\u3002"}}
{"id": "2504.18762", "pdf": "https://arxiv.org/pdf/2504.18762", "abs": "https://arxiv.org/abs/2504.18762", "authors": ["Ojasw Upadhyay", "Abishek Saravankumar", "Ayman Ismail"], "title": "SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning", "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 4 figures, 4 tables", "summary": "Large Language Models (LLMs) are powerful but often require extensive\nfine-tuning and large datasets for specialized domains like law.\nGeneral-purpose pre-training may not capture legal nuances, and acquiring\nsufficient legal data is challenging. We introduce SynLexLM, a novel approach\nto efficiently pre-train a legal LLM. Our method employs curriculum learning,\nprogressing from simple to complex legal texts and queries, combined with\nsynthetic data augmentation using models like Gemini Pro to address data\nscarcity. We aim to achieve improved performance on legal benchmarks\n(BigLaw-Bench, EUR-Lex-Sum) compared to traditional models and fine-tuned\nversions. Preliminary work involves generating synthetic QA pairs reflecting\nlegal reasoning. This work aims to enhance legal document analysis and research\ntools, potentially democratizing access to advanced legal AI.", "AI": {"tldr": "SynLexLM\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u9884\u8bad\u7ec3\u6cd5\u5f8b\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u548c\u5408\u6210\u6570\u636e\u589e\u5f3a\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u5728\u6cd5\u5f8b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u3002", "motivation": "\u901a\u7528\u9884\u8bad\u7ec3\u96be\u4ee5\u6355\u6349\u6cd5\u5f8b\u9886\u57df\u7684\u7ec6\u5fae\u5dee\u522b\uff0c\u4e14\u6cd5\u5f8b\u6570\u636e\u83b7\u53d6\u56f0\u96be\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u9ad8\u6cd5\u5f8b\u6587\u6863\u5206\u6790\u548c\u7814\u7a76\u5de5\u5177\u7684\u6027\u80fd\uff0c\u63a8\u52a8\u6cd5\u5f8bAI\u7684\u666e\u53ca\u3002", "method": "\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\uff08\u4ece\u7b80\u5355\u5230\u590d\u6742\u7684\u6cd5\u5f8b\u6587\u672c\uff09\u548c\u5408\u6210\u6570\u636e\u589e\u5f3a\uff08\u5229\u7528\u5982Gemini Pro\u7b49\u6a21\u578b\u751f\u6210QA\u5bf9\uff09\uff0c\u4ee5\u7f13\u89e3\u6570\u636e\u4e0d\u8db3\u95ee\u9898\u3002", "result": "\u5728BigLaw-Bench\u548cEUR-Lex-Sum\u7b49\u6cd5\u5f8b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSynLexLM\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u53ca\u5fae\u8c03\u7248\u672c\u3002", "conclusion": "SynLexLM\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u63d0\u5347\u4e86\u6cd5\u5f8b\u9886\u57dfLLM\u7684\u6027\u80fd\uff0c\u6709\u671b\u63a8\u52a8\u6cd5\u5f8bAI\u5de5\u5177\u7684\u53d1\u5c55\u4e0e\u5e94\u7528\u3002"}}
{"id": "2504.18631", "pdf": "https://arxiv.org/pdf/2504.18631", "abs": "https://arxiv.org/abs/2504.18631", "authors": ["Dingxin Lu", "Shurui Wu", "Xinyi Huang"], "title": "Research on Personalized Medical Intervention Strategy Generation System based on Group Relative Policy Optimization and Time-Series Data Fusion", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "With the timely formation of personalized intervention plans based on\nhigh-dimensional heterogeneous time series information becoming an important\nchallenge in the medical field today, electronic medical records, wearables,\nand other multi-source medical data are increasingly generated and diversified.\nIn this work, we develop a system to generate personalized medical intervention\nstrategies based on Group Relative Policy Optimization (GRPO) and Time-Series\nData Fusion. First, by incorporating relative policy constraints among the\ngroups during policy gradient updates, we adaptively balance individual and\ngroup gains. To improve the robustness and interpretability of decision-making,\na multi-layer neural network structure is employed to group-code patient\ncharacteristics. Second, for the rapid multi-modal fusion of multi-source\nheterogeneous time series, a multi-channel neural network combined with a\nself-attention mechanism is used for dynamic feature extraction. Key feature\nscreening and aggregation are achieved through a differentiable gating network.\nFinally, a collaborative search process combining a genetic algorithm and Monte\nCarlo tree search is proposed to find the ideal intervention strategy,\nachieving global optimization. Experimental results show significant\nimprovements in accuracy, coverage, and decision-making benefits compared with\nexisting methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eGRPO\u548c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u878d\u5408\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u751f\u6210\u4e2a\u6027\u5316\u533b\u7597\u5e72\u9884\u7b56\u7565\uff0c\u901a\u8fc7\u7ed3\u5408\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u548c\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u548c\u51b3\u7b56\u6548\u76ca\u3002", "motivation": "\u533b\u7597\u9886\u57df\u9762\u4e34\u57fa\u4e8e\u9ad8\u7ef4\u5f02\u6784\u65f6\u95f4\u5e8f\u5217\u4fe1\u606f\u5feb\u901f\u5236\u5b9a\u4e2a\u6027\u5316\u5e72\u9884\u8ba1\u5212\u7684\u6311\u6218\uff0c\u591a\u6e90\u533b\u7597\u6570\u636e\u7684\u591a\u6837\u5316\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u53ef\u80fd\u3002", "method": "\u5f15\u5165\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u7ea6\u675f\u5e73\u8861\u4e2a\u4f53\u4e0e\u7fa4\u4f53\u6536\u76ca\uff0c\u91c7\u7528\u591a\u5c42\u795e\u7ecf\u7f51\u7edc\u5206\u7ec4\u7f16\u7801\u60a3\u8005\u7279\u5f81\uff1b\u5229\u7528\u591a\u901a\u9053\u795e\u7ecf\u7f51\u7edc\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u63d0\u53d6\u591a\u6e90\u5f02\u6784\u65f6\u95f4\u5e8f\u5217\u7279\u5f81\uff1b\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u5b9e\u73b0\u5168\u5c40\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u8986\u76d6\u7387\u548c\u51b3\u7b56\u6548\u76ca\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6e90\u5f02\u6784\u533b\u7597\u6570\u636e\u4e0b\u7684\u4e2a\u6027\u5316\u5e72\u9884\u7b56\u7565\u751f\u6210\u95ee\u9898\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.18562", "pdf": "https://arxiv.org/pdf/2504.18562", "abs": "https://arxiv.org/abs/2504.18562", "authors": ["Ayoub Jadouli", "Chaker El Amrani"], "title": "Deep Learning with Pretrained 'Internal World' Layers: A Gemma 3-Based Modular Architecture for Wildfire Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep learning models, especially large Transformers, carry substantial\n\"memory\" in their intermediate layers -- an \\emph{internal world} that encodes\na wealth of relational and contextual knowledge. This work harnesses that\ninternal world for wildfire occurrence prediction by introducing a modular\narchitecture built upon Gemma 3, a state-of-the-art multimodal model. Rather\nthan relying on Gemma 3's original embedding and positional encoding stacks, we\ndevelop a custom feed-forward module that transforms tabular wildfire features\ninto the hidden dimension required by Gemma 3's mid-layer Transformer blocks.\nWe freeze these Gemma 3 sub-layers -- thus preserving their pretrained\nrepresentation power -- while training only the smaller input and output\nnetworks. This approach minimizes the number of trainable parameters and\nreduces the risk of overfitting on limited wildfire data, yet retains the\nbenefits of Gemma 3's broad knowledge. Evaluations on a Moroccan wildfire\ndataset demonstrate improved predictive accuracy and robustness compared to\nstandard feed-forward and convolutional baselines. Ablation studies confirm\nthat the frozen Transformer layers consistently contribute to better\nrepresentations, underscoring the feasibility of reusing large-model mid-layers\nas a learned internal world. Our findings suggest that strategic modular reuse\nof pretrained Transformers can enable more data-efficient and interpretable\nsolutions for critical environmental applications such as wildfire risk\nmanagement.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5229\u7528Gemma 3\u6a21\u578b\u7684\u4e2d\u5c42Transformer\u5757\u8fdb\u884c\u91ce\u706b\u9884\u6d4b\uff0c\u901a\u8fc7\u5b9a\u5236\u7684\u524d\u9988\u6a21\u5757\u5c06\u8868\u683c\u6570\u636e\u8f6c\u6362\u4e3a\u9690\u85cf\u7ef4\u5ea6\uff0c\u51bb\u7ed3\u9884\u8bad\u7ec3\u5c42\u4ee5\u51cf\u5c11\u53c2\u6570\u548c\u8fc7\u62df\u5408\u98ce\u9669\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5927\u6a21\u578b\uff08\u5982Gemma 3\uff09\u7684\u4e2d\u5c42\u9690\u542b\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u548c\u5173\u7cfb\u77e5\u8bc6\uff0c\u4f5c\u8005\u5e0c\u671b\u5229\u7528\u8fd9\u4e00\u7279\u6027\u63d0\u5347\u91ce\u706b\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u6570\u636e\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5b9a\u5236\u7684\u524d\u9988\u6a21\u5757\uff0c\u5c06\u8868\u683c\u6570\u636e\u9002\u914d\u5230Gemma 3\u7684\u4e2d\u5c42Transformer\u5757\u8f93\u5165\u7ef4\u5ea6\uff0c\u51bb\u7ed3\u9884\u8bad\u7ec3\u5c42\uff0c\u4ec5\u8bad\u7ec3\u8f93\u5165\u548c\u8f93\u51fa\u7f51\u7edc\u3002", "result": "\u5728\u6469\u6d1b\u54e5\u91ce\u706b\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edf\u524d\u9988\u548c\u5377\u79ef\u57fa\u7ebf\u6a21\u578b\u5177\u6709\u66f4\u9ad8\u7684\u9884\u6d4b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u51bb\u7ed3\u5c42\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7b56\u7565\u6027\u590d\u7528\u9884\u8bad\u7ec3Transformer\u7684\u4e2d\u5c42\u53ef\u4f5c\u4e3a\u9ad8\u6548\u7684\u5185\u90e8\u77e5\u8bc6\u5e93\uff0c\u4e3a\u73af\u5883\u5e94\u7528\uff08\u5982\u91ce\u706b\u98ce\u9669\u7ba1\u7406\uff09\u63d0\u4f9b\u66f4\u6570\u636e\u9ad8\u6548\u548c\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18805", "pdf": "https://arxiv.org/pdf/2504.18805", "abs": "https://arxiv.org/abs/2504.18805", "authors": ["Jong Inn Park", "Maanas Taneja", "Qianwen Wang", "Dongyeop Kang"], "title": "Stealing Creator's Workflow: A Creator-Inspired Agentic Framework with Iterative Feedback Loop for Improved Scientific Short-form Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project page: https://minnesotanlp.github.io/scitalk-project-page/", "summary": "Generating engaging, accurate short-form videos from scientific papers is\nchallenging due to content complexity and the gap between expert authors and\nreaders. Existing end-to-end methods often suffer from factual inaccuracies and\nvisual artifacts, limiting their utility for scientific dissemination. To\naddress these issues, we propose SciTalk, a novel multi-LLM agentic framework,\ngrounding videos in various sources, such as text, figures, visual styles, and\navatars. Inspired by content creators' workflows, SciTalk uses specialized\nagents for content summarization, visual scene planning, and text and layout\nediting, and incorporates an iterative feedback mechanism where video agents\nsimulate user roles to give feedback on generated videos from previous\niterations and refine generation prompts. Experimental evaluations show that\nSciTalk outperforms simple prompting methods in generating scientifically\naccurate and engaging content over the refined loop of video generation.\nAlthough preliminary results are still not yet matching human creators'\nquality, our framework provides valuable insights into the challenges and\nbenefits of feedback-driven video generation. Our code, data, and generated\nvideos will be publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSciTalk\u7684\u591aLLM\u4ee3\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u79d1\u5b66\u8bba\u6587\u751f\u6210\u51c6\u786e\u4e14\u5f15\u4eba\u5165\u80dc\u7684\u77ed\u89c6\u9891\uff0c\u901a\u8fc7\u8fed\u4ee3\u53cd\u9988\u4f18\u5316\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7aef\u5230\u7aef\u65b9\u6cd5\u751f\u6210\u79d1\u5b66\u77ed\u89c6\u9891\u4e2d\u7684\u4e8b\u5b9e\u9519\u8bef\u548c\u89c6\u89c9\u4f2a\u5f71\u95ee\u9898\uff0c\u63d0\u5347\u79d1\u5b66\u4f20\u64ad\u7684\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u591aLLM\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u62ec\u5185\u5bb9\u6458\u8981\u3001\u89c6\u89c9\u573a\u666f\u89c4\u5212\u3001\u6587\u672c\u4e0e\u5e03\u5c40\u7f16\u8f91\uff0c\u4ee5\u53ca\u6a21\u62df\u7528\u6237\u53cd\u9988\u7684\u8fed\u4ee3\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSciTalk\u5728\u751f\u6210\u79d1\u5b66\u51c6\u786e\u4e14\u5438\u5f15\u4eba\u7684\u5185\u5bb9\u4e0a\u4f18\u4e8e\u7b80\u5355\u63d0\u793a\u65b9\u6cd5\uff0c\u4f46\u5c1a\u672a\u8fbe\u5230\u4eba\u7c7b\u521b\u4f5c\u8005\u7684\u6c34\u5e73\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53cd\u9988\u9a71\u52a8\u7684\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6311\u6218\u4e0e\u4f18\u52bf\u6d1e\u5bdf\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5c06\u516c\u5f00\u3002"}}
{"id": "2504.18651", "pdf": "https://arxiv.org/pdf/2504.18651", "abs": "https://arxiv.org/abs/2504.18651", "authors": ["Filipi Miranda Soares", "Antonio Mauro Saraiva", "Lu\u00eds Ferreira Pires", "Luiz Olavo Bonino da Silva Santos", "Dilvan de Abreu Moreira", "Fernando Elias Corr\u00eaa", "Kelly Rosa Braghetto", "Debora Pignatari Drucker", "Alexandre Cl\u00e1udio Botazzo Delbem"], "title": "Exploring a Large Language Model for Transforming Taxonomic Data into OWL: Lessons Learned and Implications for Ontology Development", "categories": ["cs.AI"], "comment": "31 pages, 6 Figures, accepted for publication in Data Intelligence", "summary": "Managing scientific names in ontologies that represent species taxonomies is\nchallenging due to the ever-evolving nature of these taxonomies. Manually\nmaintaining these names becomes increasingly difficult when dealing with\nthousands of scientific names. To address this issue, this paper investigates\nthe use of ChatGPT-4 to automate the development of the :Organism module in the\nAgricultural Product Types Ontology (APTO) for species classification. Our\nmethodology involved leveraging ChatGPT-4 to extract data from the GBIF\nBackbone API and generate OWL files for further integration in APTO. Two\nalternative approaches were explored: (1) issuing a series of prompts for\nChatGPT-4 to execute tasks via the BrowserOP plugin and (2) directing ChatGPT-4\nto design a Python algorithm to perform analogous tasks. Both approaches rely\non a prompting method where we provide instructions, context, input data, and\nan output indicator. The first approach showed scalability limitations, while\nthe second approach used the Python algorithm to overcome these challenges, but\nit struggled with typographical errors in data handling. This study highlights\nthe potential of Large language models like ChatGPT-4 to streamline the\nmanagement of species names in ontologies. Despite certain limitations, these\ntools offer promising advancements in automating taxonomy-related tasks and\nimproving the efficiency of ontology development.", "AI": {"tldr": "\u4f7f\u7528ChatGPT-4\u81ea\u52a8\u5316\u6784\u5efa\u519c\u4e1a\u4ea7\u54c1\u7c7b\u578b\u672c\u4f53\uff08APTO\uff09\u4e2d\u7684:Organism\u6a21\u5757\uff0c\u4ee5\u5904\u7406\u7269\u79cd\u5206\u7c7b\u4e2d\u79d1\u5b66\u540d\u79f0\u7684\u7ba1\u7406\u6311\u6218\u3002\u901a\u8fc7\u4e24\u79cd\u65b9\u6cd5\uff08\u76f4\u63a5\u63d0\u793a\u548cPython\u7b97\u6cd5\u8bbe\u8ba1\uff09\u5b9e\u73b0\uff0c\u53d1\u73b0\u540e\u8005\u66f4\u5177\u6269\u5c55\u6027\u4f46\u5b58\u5728\u6570\u636e\u9519\u8bef\u95ee\u9898\u3002", "motivation": "\u7269\u79cd\u5206\u7c7b\u5b66\u540d\u79f0\u7684\u52a8\u6001\u53d8\u5316\u5bfc\u81f4\u4eba\u5de5\u7ef4\u62a4\u56f0\u96be\uff0c\u7814\u7a76\u63a2\u7d22\u5229\u7528ChatGPT-4\u81ea\u52a8\u5316\u5904\u7406\u5927\u89c4\u6a21\u79d1\u5b66\u540d\u79f0\u7ba1\u7406\uff0c\u4ee5\u63d0\u9ad8\u672c\u4f53\u5f00\u53d1\u7684\u6548\u7387\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u65b9\u6cd5\uff1a1\uff09\u901a\u8fc7BrowserOP\u63d2\u4ef6\u76f4\u63a5\u63d0\u793aChatGPT-4\u6267\u884c\u4efb\u52a1\uff1b2\uff09\u8ba9ChatGPT-4\u8bbe\u8ba1Python\u7b97\u6cd5\u5904\u7406\u76f8\u540c\u4efb\u52a1\u3002\u4e24\u79cd\u65b9\u6cd5\u5747\u57fa\u4e8e\u6307\u4ee4\u3001\u4e0a\u4e0b\u6587\u3001\u8f93\u5165\u6570\u636e\u548c\u8f93\u51fa\u6307\u793a\u7684\u63d0\u793a\u6846\u67b6\u3002", "result": "\u7b2c\u4e00\u79cd\u65b9\u6cd5\u6269\u5c55\u6027\u53d7\u9650\uff0c\u7b2c\u4e8c\u79cd\u65b9\u6cd5\u901a\u8fc7Python\u7b97\u6cd5\u514b\u670d\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4f46\u5b58\u5728\u6570\u636e\u5904\u7406\u7684\u62fc\u5199\u9519\u8bef\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982ChatGPT-4\u5728\u7269\u79cd\u540d\u79f0\u7ba1\u7406\u7684\u81ea\u52a8\u5316\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c3d\u7ba1\u5b58\u5728\u9650\u5236\uff0c\u4f46\u80fd\u663e\u8457\u63d0\u5347\u672c\u4f53\u5f00\u53d1\u6548\u7387\u3002"}}
{"id": "2504.18574", "pdf": "https://arxiv.org/pdf/2504.18574", "abs": "https://arxiv.org/abs/2504.18574", "authors": ["Aviv Bick", "Eric Xing", "Albert Gu"], "title": "Understanding the Skill Gap in Recurrent Language Models: The Role of the Gather-and-Aggregate Mechanism", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "SSMs offer efficient processing of long sequences with fixed state sizes, but\nstruggle with algorithmic tasks like retrieving past context. In this work, we\nexamine how such in-context retrieval operates within Transformer- and\nSSM-based language models. We find that both architectures develop the same\nfundamental Gather-and-Aggregate (G&A) mechanism. A Gather Head first\nidentifies and extracts relevant information from the context, which an\nAggregate Head then integrates into a final representation. Across both model\ntypes, G&A concentrates in just a few heads, making them critical bottlenecks\neven for benchmarks that require a basic form of retrieval. For example,\ndisabling a single Gather or Aggregate Head of a pruned Llama-3.1-8B degrades\nits ability to retrieve the correct answer letter in MMLU, reducing accuracy\nfrom 66% to 25%. This finding suggests that in-context retrieval can obscure\nthe limited knowledge demands of certain tasks. Despite strong MMLU performance\nwith retrieval intact, the pruned model fails on other knowledge tests. Similar\nG&A dependencies exist in GSM8K, BBH, and dialogue tasks. Given the\nsignificance of G&A in performance, we show that retrieval challenges in SSMs\nmanifest in how they implement G&A, leading to smoother attention patterns\nrather than the sharp token transitions that effective G&A relies on. Thus,\nwhile a gap exists between Transformers and SSMs in implementing in-context\nretrieval, it is confined to a few heads, not the entire model. This insight\nsuggests a unified explanation for performance differences between Transformers\nand SSMs while also highlighting ways to combine their strengths. For example,\nin pretrained hybrid models, attention components naturally take on the role of\nAggregate Heads. Similarly, in a pretrained pure SSM, replacing a single G&A\nhead with an attention-based variant significantly improves retrieval.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86Transformer\u548cSSM\u6a21\u578b\u5728\u5904\u7406\u957f\u5e8f\u5217\u65f6\u5747\u91c7\u7528\u76f8\u540c\u7684'Gather-and-Aggregate'\u673a\u5236\uff0c\u4f46\u53d1\u73b0SSM\u5b9e\u73b0\u68c0\u7d22\u65f6\u56e0\u6ce8\u610f\u529b\u6a21\u5f0f\u8f83\u5e73\u6ed1\u800c\u8868\u73b0\u8f83\u5dee\uff0c\u63d0\u51fa\u6539\u8fdb\u7b56\u7565\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7a76Transformer\u548cSSM\u6a21\u578b\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u4e0a\u4e0b\u6587\u68c0\u7d22\u673a\u5236\u5dee\u5f02\uff0c\u5c24\u5176\u662f\u4e3a\u4f55SSM\u5728\u68c0\u7d22\u4efb\u52a1\u4e0a\u8868\u73b0\u8f83\u5f31\uff0c\u4ee5\u5bfb\u6c42\u4f18\u5316\u65b9\u5411\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790Transformer\u548cSSM\u6a21\u578b\u7684\u6ce8\u610f\u529b\u5934\u673a\u5236\uff0c\u8bc6\u522b\u51fa\u5173\u952e\u7684Gather-and-Aggregate\uff08G&A\uff09\u5934\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\uff08\u5982\u7981\u7528\u7279\u5b9a\u5934\uff09\u9a8c\u8bc1\u5176\u5bf9\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cG&A\u673a\u5236\u96c6\u4e2d\u5728\u5c11\u6570\u5173\u952e\u5934\uff0c\u7981\u7528\u5355\u4e2a\u5934\u4f1a\u5bfc\u81f4\u6027\u80fd\u5927\u5e45\u4e0b\u964d\uff08\u5982MMLU\u51c6\u786e\u7387\u4ece66%\u964d\u81f325%\uff09\u3002SSM\u56e0\u5e73\u6ed1\u6ce8\u610f\u529b\u6a21\u5f0f\u5728\u68c0\u7d22\u4efb\u52a1\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u4f46\u901a\u8fc7\u5f15\u5165\u6ce8\u610f\u529b\u7ec4\u4ef6\u53ef\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0cTransformer\u4e0eSSM\u7684\u6027\u80fd\u5dee\u5f02\u6e90\u4e8e\u5c11\u6570G&A\u5934\u7684\u5b9e\u73b0\u65b9\u5f0f\uff0c\u800c\u975e\u6574\u4f53\u6a21\u578b\u7ed3\u6784\u3002\u8fd9\u4e00\u53d1\u73b0\u4e3a\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\uff08\u5982\u6df7\u5408\u6a21\u578b\u8bbe\u8ba1\uff09\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2504.18838", "pdf": "https://arxiv.org/pdf/2504.18838", "abs": "https://arxiv.org/abs/2504.18838", "authors": ["Yixin Cao", "Shibo Hong", "Xinze Li", "Jiahao Ying", "Yubo Ma", "Haiyuan Liang", "Yantao Liu", "Zijun Yao", "Xiaozhi Wang", "Dan Huang", "Wenxuan Zhang", "Lifu Huang", "Muhao Chen", "Lei Hou", "Qianru Sun", "Xingjun Ma", "Zuxuan Wu", "Min-Yen Kan", "David Lo", "Qi Zhang", "Heng Ji", "Jing Jiang", "Juanzi Li", "Aixin Sun", "Xuanjing Huang", "Tat-Seng Chua", "Yu-Gang Jiang"], "title": "Toward Generalizable Evaluation in the LLM Era: A Survey Beyond Benchmarks", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are advancing at an amazing speed and have\nbecome indispensable across academia, industry, and daily applications. To keep\npace with the status quo, this survey probes the core challenges that the rise\nof LLMs poses for evaluation. We identify and analyze two pivotal transitions:\n(i) from task-specific to capability-based evaluation, which reorganizes\nbenchmarks around core competencies such as knowledge, reasoning, instruction\nfollowing, multi-modal understanding, and safety; and (ii) from manual to\nautomated evaluation, encompassing dynamic dataset curation and\n\"LLM-as-a-judge\" scoring.\n  Yet, even with these transitions, a crucial obstacle persists: the evaluation\ngeneralization issue. Bounded test sets cannot scale alongside models whose\nabilities grow seemingly without limit. We will dissect this issue, along with\nthe core challenges of the above two transitions, from the perspectives of\nmethods, datasets, evaluators, and metrics. Due to the fast evolving of this\nfield, we will maintain a living GitHub repository (links are in each section)\nto crowd-source updates and corrections, and warmly invite contributors and\ncollaborators.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8bc4\u4f30\u7684\u6838\u5fc3\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4ece\u4efb\u52a1\u7279\u5b9a\u5230\u80fd\u529b\u57fa\u7840\u7684\u8bc4\u4f30\u8f6c\u53d8\uff0c\u4ee5\u53ca\u4ece\u624b\u52a8\u5230\u81ea\u52a8\u8bc4\u4f30\u7684\u8f6c\u53d8\uff0c\u5e76\u8ba8\u8bba\u4e86\u8bc4\u4f30\u6cdb\u5316\u95ee\u9898\u7684\u5173\u952e\u969c\u788d\u3002", "motivation": "\u8bba\u6587\u52a8\u673a\u5728\u4e8e\u5e94\u5bf9LLMs\u5feb\u901f\u53d1\u5c55\u6240\u5e26\u6765\u7684\u8bc4\u4f30\u6311\u6218\uff0c\u91cd\u65b0\u7ec4\u7ec7\u8bc4\u4f30\u6807\u51c6\u4ee5\u53cd\u6620\u6838\u5fc3\u80fd\u529b\uff0c\u5e76\u63a8\u52a8\u8bc4\u4f30\u65b9\u6cd5\u7684\u81ea\u52a8\u5316\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4ece\u4efb\u52a1\u7279\u5b9a\u5230\u80fd\u529b\u57fa\u7840\u7684\u8bc4\u4f30\u8f6c\u53d8\uff0c\u4ee5\u53ca\u4ece\u624b\u52a8\u5230\u81ea\u52a8\u8bc4\u4f30\u7684\u8fc7\u6e21\uff0c\u7279\u522b\u5173\u6ce8\u52a8\u6001\u6570\u636e\u96c6\u548c\"LLM\u5373\u8bc4\u59d4\"\u7684\u8bc4\u5206\u65b9\u6cd5\u3002", "result": "\u8bba\u6587\u6307\u51fa\u4e86\u8bc4\u4f30\u6cdb\u5316\u95ee\u9898\u662f\u76ee\u524d\u7684\u5173\u952e\u969c\u788d\uff0c\u6709\u9650\u6d4b\u8bd5\u96c6\u65e0\u6cd5\u968f\u7740\u6a21\u578b\u80fd\u529b\u589e\u957f\u800c\u6269\u5c55\uff0c\u540c\u65f6\u5206\u6790\u4e86\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u7b49\u6838\u5fc3\u6311\u6218\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u4e86\u8bc4\u4f30\u65b9\u6cd5\u7684\u6301\u7eed\u66f4\u65b0\u9700\u6c42\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7GitHub\u4ed3\u5e93\u4f17\u5305\u66f4\u65b0\u548c\u4fee\u6b63\uff0c\u9080\u8bf7\u66f4\u591a\u8d21\u732e\u8005\u53c2\u4e0e\u5408\u4f5c\u3002"}}
{"id": "2504.18671", "pdf": "https://arxiv.org/pdf/2504.18671", "abs": "https://arxiv.org/abs/2504.18671", "authors": ["Ross Gore", "Eranga Bandara", "Sachin Shetty", "Alberto E. Musto", "Pratip Rana", "Ambrosio Valencia-Romero", "Christopher Rhea", "Lobat Tayebi", "Heather Richter", "Atmaram Yarlagadda", "Donna Edmonds", "Steven Wallace", "Donna Broshek"], "title": "Proof-of-TBI -- Fine-Tuned Vision Language Model Consortium and OpenAI-o3 Reasoning LLM-Based Medical Diagnosis Support System for Mild Traumatic Brain Injury (TBI) Prediction", "categories": ["cs.AI"], "comment": null, "summary": "Mild Traumatic Brain Injury (TBI) detection presents significant challenges\ndue to the subtle and often ambiguous presentation of symptoms in medical\nimaging, making accurate diagnosis a complex task. To address these challenges,\nwe propose Proof-of-TBI, a medical diagnosis support system that integrates\nmultiple fine-tuned vision-language models with the OpenAI-o3 reasoning large\nlanguage model (LLM). Our approach fine-tunes multiple vision-language models\nusing a labeled dataset of TBI MRI scans, training them to diagnose TBI\nsymptoms effectively. The predictions from these models are aggregated through\na consensus-based decision-making process. The system evaluates the predictions\nfrom all fine-tuned vision language models using the OpenAI-o3 reasoning LLM, a\nmodel that has demonstrated remarkable reasoning performance, to produce the\nmost accurate final diagnosis. The LLM Agents orchestrates interactions between\nthe vision-language models and the reasoning LLM, managing the final\ndecision-making process with transparency, reliability, and automation. This\nend-to-end decision-making workflow combines the vision-language model\nconsortium with the OpenAI-o3 reasoning LLM, enabled by custom prompt\nengineering by the LLM agents. The prototype for the proposed platform was\ndeveloped in collaboration with the U.S. Army Medical Research team in Newport\nNews, Virginia, incorporating five fine-tuned vision-language models. The\nresults demonstrate the transformative potential of combining fine-tuned\nvision-language model inputs with the OpenAI-o3 reasoning LLM to create a\nrobust, secure, and highly accurate diagnostic system for mild TBI prediction.\nTo the best of our knowledge, this research represents the first application of\nfine-tuned vision-language models integrated with a reasoning LLM for TBI\nprediction tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548cOpenAI-o3\u63a8\u7406\u5927\u6a21\u578b\u7684\u8f7b\u5ea6\u521b\u4f24\u6027\u8111\u635f\u4f24\uff08TBI\uff09\u8bca\u65ad\u652f\u6301\u7cfb\u7edfProof-of-TBI\uff0c\u901a\u8fc7\u5171\u8bc6\u51b3\u7b56\u6d41\u7a0b\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u8f7b\u5ea6TBI\u75c7\u72b6\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u8868\u73b0\u5fae\u5999\u4e14\u6a21\u7cca\uff0c\u8bca\u65ad\u56f0\u96be\u3002\u4e3a\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u7814\u7a76\u63a2\u7d22\u5982\u4f55\u7ed3\u5408\u591a\u6a21\u6001\u6a21\u578b\u4e0e\u63a8\u7406\u80fd\u529b\u5f3a\u5927\u7684LLM\u3002", "method": "1. \u5fae\u8c03\u591a\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff1b2. \u901a\u8fc7\u5171\u8bc6\u673a\u5236\u6c47\u603b\u9884\u6d4b\uff1b3. \u5229\u7528OpenAI-o3\u8fdb\u884c\u6700\u7ec8\u51b3\u7b56\u63a8\u7406\uff1b4. \u4f7f\u7528LLM\u4ee3\u7406\u534f\u8c03\u6a21\u578b\u4ea4\u4e92\u3002", "result": "\u539f\u578b\u7cfb\u7edf\u5728\u5408\u4f5c\u5f00\u53d1\u4e2d\u5c55\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\uff0c\u9996\u6b21\u5c06\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u63a8\u7406LLM\u7ed3\u5408\u7528\u4e8eTBI\u9884\u6d4b\u3002", "conclusion": "Proof-of-TBI\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u6a21\u578b\u4e0eLLM\u7ed3\u5408\u7684\u6f5c\u529b\uff0c\u4e3aTBI\u8bca\u65ad\u63d0\u4f9b\u4e86\u53ef\u9760\u3001\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18578", "pdf": "https://arxiv.org/pdf/2504.18578", "abs": "https://arxiv.org/abs/2504.18578", "authors": ["Orhun Vural", "Bunyamin Ozaydin", "Khalid Y. Aram", "James Booth", "Brittany F. Lindsey", "Abdulaziz Ahmed"], "title": "An Artificial Intelligence-Based Framework for Predicting Emergency Department Overcrowding: Development and Evaluation Study", "categories": ["cs.LG", "68T07", "I.2.6; J.3"], "comment": null, "summary": "Background: Emergency department (ED) overcrowding remains a major challenge,\ncausing delays in care and increased operational strain. Hospital management\noften reacts to congestion after it occurs. Machine learning predictive\nmodeling offers a proactive approach by forecasting patient flow metrics, such\nas waiting count, to improve resource planning and hospital efficiency.\n  Objective: This study develops machine learning models to predict ED waiting\nroom occupancy at two time scales. The hourly model forecasts the waiting count\nsix hours ahead (e.g., a 1 PM prediction for 7 PM), while the daily model\nestimates the average waiting count for the next 24 hours (e.g., a 5 PM\nprediction for the following day's average). These tools support staffing\ndecisions and enable earlier interventions to reduce overcrowding.\n  Methods: Data from a partner hospital's ED in the southeastern United States\nwere used, integrating internal metrics and external features. Eleven machine\nlearning algorithms, including traditional and deep learning models, were\ntrained and evaluated. Feature combinations were optimized, and performance was\nassessed across varying patient volumes and hours.\n  Results: TSiTPlus achieved the best hourly prediction (MAE: 4.19, MSE:\n29.32). The mean hourly waiting count was 18.11, with a standard deviation of\n9.77. Accuracy varied by hour, with MAEs ranging from 2.45 (11 PM) to 5.45 (8\nPM). Extreme case analysis at one, two, and three standard deviations above the\nmean showed MAEs of 6.16, 10.16, and 15.59, respectively. For daily\npredictions, XCMPlus performed best (MAE: 2.00, MSE: 6.64), with a daily mean\nof 18.11 and standard deviation of 4.51.\n  Conclusions: These models accurately forecast ED waiting room occupancy and\nsupport proactive resource allocation. Their implementation has the potential\nto improve patient flow and reduce overcrowding in emergency care settings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u6025\u8bca\u79d1\u5019\u8bca\u4eba\u6570\uff0c\u5206\u4e3a\u6bcf\u5c0f\u65f6\u548c\u6bcf\u65e5\u4e24\u79cd\u65f6\u95f4\u5c3a\u5ea6\uff0c\u65e8\u5728\u4f18\u5316\u8d44\u6e90\u914d\u7f6e\u5e76\u7f13\u89e3\u62e5\u5835\u3002\u6700\u4f73\u6a21\u578bTSiTPlus\u548cXCMPlus\u5206\u522b\u5728\u4e0d\u540c\u5c3a\u5ea6\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6025\u8bca\u79d1\u62e5\u5835\u5bfc\u81f4\u5c31\u533b\u5ef6\u8bef\u548c\u8fd0\u8425\u538b\u529b\uff0c\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u4e8b\u540e\u5e94\u5bf9\u3002\u7814\u7a76\u5e0c\u671b\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u63d0\u524d\u9884\u6d4b\u5019\u8bca\u4eba\u6570\uff0c\u5b9e\u73b0\u4e3b\u52a8\u5e72\u9884\u3002", "method": "\u4f7f\u7528\u7f8e\u56fd\u4e1c\u5357\u90e8\u4e00\u5bb6\u533b\u9662\u7684\u6025\u8bca\u6570\u636e\uff0c\u6574\u5408\u5185\u5916\u7279\u5f81\uff0c\u8bad\u7ec311\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff08\u5305\u62ec\u4f20\u7edf\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff09\uff0c\u5e76\u4f18\u5316\u7279\u5f81\u7ec4\u5408\u3002", "result": "TSiTPlus\u5728\u6bcf\u5c0f\u65f6\u9884\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff08MAE:4.19\uff09\uff0cXCMPlus\u5728\u6bcf\u65e5\u9884\u6d4b\u4e2d\u9886\u5148\uff08MAE:2.00\uff09\u3002\u6781\u7aef\u60c5\u51b5\u4e0b\u9884\u6d4b\u8bef\u5dee\u53ef\u63a7\u3002", "conclusion": "\u6a21\u578b\u80fd\u51c6\u786e\u9884\u6d4b\u5019\u8bca\u4eba\u6570\uff0c\u652f\u6301\u4e3b\u52a8\u8d44\u6e90\u8c03\u914d\uff0c\u6709\u671b\u6539\u5584\u6025\u8bca\u60a3\u8005\u6d41\u52a8\u548c\u51cf\u5c11\u62e5\u5835\u3002"}}
{"id": "2504.18839", "pdf": "https://arxiv.org/pdf/2504.18839", "abs": "https://arxiv.org/abs/2504.18839", "authors": ["Abdellah Ghassel", "Xianzhi Li", "Xiaodan Zhu"], "title": "Towards Robust Dialogue Breakdown Detection: Addressing Disruptors in Large Language Models with Self-Guided Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are rapidly changing various domains. However,\ntheir capabilities in handling conversational breakdowns still require an\nin-depth exploration. This paper addresses the challenge of detecting and\nmitigating dialogue breakdowns within LLM-driven conversational systems. While\npowerful models from OpenAI and Anthropic excel in many dialogue tasks, they\ncan still produce incoherent or contradictory responses, commonly referred to\nas breakdowns, which undermine user trust. To tackle this, we propose an\napproach that combines specialized fine-tuning with advanced prompting\nstrategies, including few-shot learning, chain-of-thought reasoning, and\nanalogical prompting. In particular, we fine-tune a small 8B model and\ndemonstrate its robust classification and calibration capabilities in English\nand Japanese dialogue. We also validate its generalization on the BETOLD\ndataset, achieving a 7\\% accuracy improvement over its base model. Furthermore,\nwe introduce a real-time deployment architecture that selectively escalates\nsuspicious responses to more resource-intensive frontier models only when\nbreakdowns are detected, significantly cutting operational expenses and energy\nconsumption. Experimental results show our method surpasses prior\nstate-of-the-art specialized classifiers while also narrowing performance gaps\nbetween smaller open-source models and large proprietary ones. Our approach\noffers a scalable solution for robust conversational AI in high-impact domains\nby combining efficiency, interpretability, and reliability.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5fae\u8c03\u5c0f\u6a21\u578b\u4e0e\u9ad8\u7ea7\u63d0\u793a\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u7f13\u89e3LLM\u9a71\u52a8\u7684\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7684\u5bf9\u8bdd\u5d29\u6e83\u95ee\u9898\uff0c\u6709\u6548\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u7387\u5e76\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u53ef\u80fd\u4ea7\u751f\u4e0d\u8fde\u8d2f\u6216\u77db\u76fe\u7684\u54cd\u5e94\uff08\u5373\u5bf9\u8bdd\u5d29\u6e83\uff09\uff0c\u635f\u5bb3\u7528\u6237\u4fe1\u4efb\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e13\u4e1a\u5fae\u8c03\uff088B\u5c0f\u6a21\u578b\uff09\u4e0e\u63d0\u793a\u7b56\u7565\uff08few-shot\u5b66\u4e60\u3001\u94fe\u5f0f\u601d\u8003\u3001\u7c7b\u6bd4\u63d0\u793a\uff09\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5e76\u5728BETOLD\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u6539\u8fdb\u540e\u7684\u6a21\u578b\u5728\u82f1\u8bed\u548c\u65e5\u8bed\u5bf9\u8bdd\u4e2d\u8868\u73b0\u7a33\u5065\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u53477%\uff0c\u5e76\u901a\u8fc7\u9009\u62e9\u6027\u5347\u7ea7\u5230\u5927\u6a21\u578b\u663e\u8457\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u548c\u80fd\u8017\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u5f71\u54cd\u9886\u57df\u7684\u5bf9\u8bddAI\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7f29\u5c0f\u4e86\u5c0f\u6a21\u578b\u4e0e\u4e13\u6709\u5927\u6a21\u578b\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002"}}
{"id": "2504.18687", "pdf": "https://arxiv.org/pdf/2504.18687", "abs": "https://arxiv.org/abs/2504.18687", "authors": ["Samuel Schapiro", "Jonah Black", "Lav R. Varshney"], "title": "Transformational Creativity in Science: A Graphical Theory", "categories": ["cs.AI"], "comment": null, "summary": "Creative processes are typically divided into three types: combinatorial,\nexploratory, and transformational. Here, we provide a graphical theory of\ntransformational scientific creativity, synthesizing Boden's insight that\ntransformational creativity arises from changes in the \"enabling constraints\"\nof a conceptual space and Kuhn's structure of scientific revolutions as\nresulting from paradigm shifts. We prove that modifications made to axioms of\nour graphical model have the most transformative potential and then illustrate\nhow several historical instances of transformational creativity can be captured\nby our framework.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u56fe\u5f62\u7406\u8bba\u5206\u6790\u4e86\u53d8\u9769\u6027\u79d1\u5b66\u521b\u9020\u529b\uff0c\u7ed3\u5408Boden\u548cKuhn\u7684\u7406\u8bba\uff0c\u8bc1\u660e\u6a21\u578b\u516c\u7406\u7684\u4fee\u6539\u6700\u5177\u53d8\u9769\u6f5c\u529b\uff0c\u5e76\u7528\u5386\u53f2\u6848\u4f8b\u9a8c\u8bc1\u6846\u67b6\u3002", "motivation": "\u65e8\u5728\u5c06Boden\u7684\u53d8\u9769\u6027\u521b\u9020\u529b\u6e90\u4e8e\u6982\u5ff5\u7a7a\u95f4\u201c\u7ea6\u675f\u6761\u4ef6\u201d\u53d8\u5316\u4e0eKuhn\u7684\u79d1\u5b66\u9769\u547d\u7ed3\u6784\uff08\u8303\u5f0f\u8f6c\u79fb\uff09\u7ed3\u5408\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u56fe\u5f62\u7406\u8bba\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u56fe\u5f62\u7406\u8bba\u6a21\u578b\uff0c\u5206\u6790\u516c\u7406\u4fee\u6539\u7684\u53d8\u9769\u6f5c\u529b\uff0c\u5e76\u7528\u5386\u53f2\u6848\u4f8b\u9a8c\u8bc1\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "result": "\u8bc1\u660e\u516c\u7406\u4fee\u6539\u6700\u5177\u53d8\u9769\u6f5c\u529b\uff0c\u6846\u67b6\u80fd\u591f\u6355\u6349\u5386\u53f2\u4e0a\u7684\u53d8\u9769\u6027\u521b\u9020\u529b\u6848\u4f8b\u3002", "conclusion": "\u56fe\u5f62\u7406\u8bba\u6a21\u578b\u4e3a\u53d8\u9769\u6027\u79d1\u5b66\u521b\u9020\u529b\u63d0\u4f9b\u4e86\u7edf\u4e00\u5206\u6790\u5de5\u5177\uff0c\u9a8c\u8bc1\u4e86\u7406\u8bba\u5047\u8bbe\u548c\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2504.18579", "pdf": "https://arxiv.org/pdf/2504.18579", "abs": "https://arxiv.org/abs/2504.18579", "authors": ["Feng Chen", "Yefei He", "Lequan Lin", "Jing Liu", "Bohan Zhuang", "Qi Wu"], "title": "ZipR1: Reinforcing Token Sparsity in MLLMs", "categories": ["cs.LG"], "comment": "work in process", "summary": "Sparse attention mechanisms aim to reduce computational overhead by\nselectively processing a subset of salient tokens while preserving model\nperformance. Despite the effectiveness of such designs, how to actively\nencourage token sparsity of well-posed MLLMs remains under-explored, which\nfundamentally limits the achievable acceleration effect during inference. In\nthis paper, we propose a simple RL-based post-training method named\n\\textbf{ZipR1} that treats the token reduction ratio as the efficiency reward\nand answer accuracy as the performance reward.\n  In this way, our method can jointly alleviate the computation and memory\nbottlenecks via directly optimizing the inference-consistent\nefficiency-performance tradeoff. Experimental results demonstrate that ZipR1\ncan reduce the token ratio of Qwen2/2.5-VL from 80\\% to 25\\% with a minimal\naccuracy reduction on 13 image and video benchmarks.", "AI": {"tldr": "ZipR1\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u63a8\u7406\u6548\u7387\u548c\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u7684token\u5904\u7406\u91cf\uff0c\u540c\u65f6\u5728\u7cbe\u5ea6\u4e0a\u4ec5\u8f7b\u5fae\u4e0b\u964d\u3002", "motivation": "\u5f53\u524d\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u867d\u7136\u6709\u6548\uff0c\u4f46\u5982\u4f55\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u4e3b\u52a8\u5b9e\u73b0token\u7a00\u758f\u6027\u4ecd\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u9650\u5236\u4e86\u63a8\u7406\u52a0\u901f\u7684\u6548\u679c\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5c06token\u51cf\u5c11\u6bd4\u4f8b\u4f5c\u4e3a\u6548\u7387\u5956\u52b1\uff0c\u7b54\u6848\u51c6\u786e\u7387\u4f5c\u4e3a\u6027\u80fd\u5956\u52b1\uff0c\u76f4\u63a5\u4f18\u5316\u63a8\u7406\u4e00\u81f4\u7684\u6548\u7387-\u6027\u80fd\u6743\u8861\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cZipR1\u5c06Qwen2/2.5-VL\u7684token\u6bd4\u4f8b\u4ece80%\u964d\u81f325%\uff0c\u572813\u4e2a\u56fe\u50cf\u548c\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4ec5\u8f7b\u5fae\u5f71\u54cd\u51c6\u786e\u7387\u3002", "conclusion": "ZipR1\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6709\u6548\u5e73\u8861\u4e86\u6a21\u578b\u63a8\u7406\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684token\u7a00\u758f\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18851", "pdf": "https://arxiv.org/pdf/2504.18851", "abs": "https://arxiv.org/abs/2504.18851", "authors": ["Hayley Ross", "Ameya Sunil Mahabaleshwarkar", "Yoshi Suhara"], "title": "When2Call: When (not) to Call Tools", "categories": ["cs.CL"], "comment": "NAACL 2025", "summary": "Leveraging external tools is a key feature for modern Language Models (LMs)\nto expand their capabilities and integrate them into existing systems. However,\nexisting benchmarks primarily focus on the accuracy of tool calling -- whether\nthe correct tool is called with the correct parameters -- and less on\nevaluating when LMs should (not) call tools. We develop a new benchmark,\nWhen2Call, which evaluates tool-calling decision-making: when to generate a\ntool call, when to ask follow-up questions and when to admit the question can't\nbe answered with the tools provided. We find that state-of-the-art tool-calling\nLMs show significant room for improvement on When2Call, indicating the\nimportance of this benchmark. We also develop a training set for When2Call and\nleverage the multiple-choice nature of the benchmark to develop a preference\noptimization training regime, which shows considerably more improvement than\ntraditional fine-tuning. We release the benchmark and training data as well as\nevaluation scripts at https://github.com/NVIDIA/When2Call.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86When2Call\u57fa\u51c6\uff0c\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u5177\u8c03\u7528\u51b3\u7b56\u4e0a\u7684\u8868\u73b0\uff0c\u5305\u62ec\u4f55\u65f6\u8c03\u7528\u5de5\u5177\u3001\u4f55\u65f6\u63d0\u95ee\u6216\u627f\u8ba4\u65e0\u6cd5\u56de\u7b54\uff0c\u5e76\u5c55\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u5f53\u524d\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5de5\u5177\u8c03\u7528\u7684\u51c6\u786e\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u6a21\u578b\u5e94\u5728\u4f55\u65f6\uff08\u4e0d\uff09\u8c03\u7528\u5de5\u5177\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u5f00\u53d1\u4e86When2Call\u57fa\u51c6\uff0c\u901a\u8fc7\u591a\u9009\u62e9\u9898\u5f62\u5f0f\u8bc4\u4f30\u5de5\u5177\u8c03\u7528\u51b3\u7b56\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u504f\u597d\u4f18\u5316\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u73b0\u6709\u7684\u5de5\u5177\u8c03\u7528\u8bed\u8a00\u6a21\u578b\u5728When2Call\u4e0a\u8868\u73b0\u6709\u663e\u8457\u6539\u8fdb\u7a7a\u95f4\uff0c\u4e14\u504f\u597d\u4f18\u5316\u8bad\u7ec3\u4f18\u4e8e\u4f20\u7edf\u5fae\u8c03\u3002", "conclusion": "When2Call\u57fa\u51c6\u586b\u8865\u4e86\u5de5\u5177\u8c03\u7528\u51b3\u7b56\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.18765", "pdf": "https://arxiv.org/pdf/2504.18765", "abs": "https://arxiv.org/abs/2504.18765", "authors": ["Chengwei Liu", "Chong Wang", "Jiayue Cao", "Jingquan Ge", "Kun Wang", "Lvye Zhang", "Ming-Ming Cheng", "Penghai Zhao", "Tianlin Li", "Xiaojun Jia", "Xiang Li", "Xinfeng Li", "Yang Liu", "Yebo Feng", "Yihao Huang", "Yijia Xu", "Yuqiang Sun", "Zhenhong Zhou", "Zhengzi Xu"], "title": "A Vision for Auto Research with LLM Agents", "categories": ["cs.AI"], "comment": null, "summary": "This paper introduces Agent-Based Auto Research, a structured multi-agent\nframework designed to automate, coordinate, and optimize the full lifecycle of\nscientific research. Leveraging the capabilities of large language models\n(LLMs) and modular agent collaboration, the system spans all major research\nphases, including literature review, ideation, methodology planning,\nexperimentation, paper writing, peer review response, and dissemination. By\naddressing issues such as fragmented workflows, uneven methodological\nexpertise, and cognitive overload, the framework offers a systematic and\nscalable approach to scientific inquiry. Preliminary explorations demonstrate\nthe feasibility and potential of Auto Research as a promising paradigm for\nself-improving, AI-driven research processes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7684\u81ea\u52a8\u5316\u7814\u7a76\u6846\u67b6\uff08Agent-Based Auto Research\uff09\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u6a21\u5757\u5316\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u8986\u76d6\u79d1\u7814\u5168\u751f\u547d\u5468\u671f\uff0c\u65e8\u5728\u89e3\u51b3\u7814\u7a76\u6d41\u7a0b\u788e\u7247\u5316\u3001\u65b9\u6cd5\u8bba\u4e0d\u5747\u8861\u53ca\u8ba4\u77e5\u8fc7\u8f7d\u7b49\u95ee\u9898\uff0c\u521d\u6b65\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002", "motivation": "\u5f53\u524d\u79d1\u5b66\u7814\u7a76\u6d41\u7a0b\u5b58\u5728\u788e\u7247\u5316\u3001\u65b9\u6cd5\u8bba\u4e0d\u5747\u8861\u548c\u7814\u7a76\u8005\u8ba4\u77e5\u8fc7\u8f7d\u7b49\u95ee\u9898\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u79d1\u7814\u5168\u81ea\u52a8\u5316\u548c\u4f18\u5316\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7684\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u6a21\u5757\u5316\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u8986\u76d6\u6587\u732e\u7efc\u8ff0\u3001\u6784\u601d\u3001\u65b9\u6cd5\u8bbe\u8ba1\u3001\u5b9e\u9a8c\u3001\u8bba\u6587\u64b0\u5199\u3001\u540c\u884c\u8bc4\u5ba1\u54cd\u5e94\u548c\u4f20\u64ad\u7b49\u79d1\u7814\u5168\u6d41\u7a0b\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u5b9e\u73b0\u81ea\u52a8\u5316\u79d1\u7814\u6d41\u7a0b\u65b9\u9762\u5177\u6709\u53ef\u884c\u6027\u548c\u6f5c\u529b\uff0c\u53ef\u4e3aAI\u9a71\u52a8\u7684\u81ea\u4f18\u5316\u7814\u7a76\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002", "conclusion": "Agent-Based Auto Research\u6846\u67b6\u4e3a\u79d1\u5b66\u7814\u7a76\u7684\u7cfb\u7edf\u6027\u3001\u89c4\u6a21\u5316\u53ca\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u73b0\u4e86AI\u9a71\u52a8\u79d1\u7814\u7684\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2504.18580", "pdf": "https://arxiv.org/pdf/2504.18580", "abs": "https://arxiv.org/abs/2504.18580", "authors": ["Shi Jie Yu", "Sehyun Choi"], "title": "Parameter-Efficient Checkpoint Merging via Metrics-Weighted Averaging", "categories": ["cs.LG"], "comment": null, "summary": "Checkpoint merging is a technique for combining multiple model snapshots into\na single superior model, potentially reducing training time for large language\nmodels. This paper explores checkpoint merging in the context of\nparameter-efficient fine-tuning (PEFT), where only small adapter modules (e.g.\nLoRA) are trained. We propose Metrics-Weighted Averaging (MWA), a simple yet\neffective method to merge model checkpoints by weighting their parameters\naccording to performance metrics. In particular, we investigate weighting by\ntraining loss and by training steps, under the intuition that lower-loss or\nlater-step checkpoints are more valuable. We introduce a formula with a penalty\nfactor to adjust weight distribution, requiring only one hyperparameter\nregardless of the number of checkpoints. Experiments on three fine-tuning tasks\n(mathematical reasoning, preference alignment, and general instruction tuning)\nshow that MWA consistently produces merged models that outperform the naive\nuniform average of checkpoints. Notably, loss-weighted merging often yields the\nbest results, delivering up to 5% higher task accuracy than the baseline\nuniform merge and even surpassing the final individual checkpoint's\nperformance. These findings validate checkpoint merging for PEFT and\ndemonstrate that a metric-driven weighting heuristic can efficiently boost\nmodel performance with minimal computational overhead.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5ea6\u91cf\u52a0\u6743\u5e73\u5747\uff08MWA\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u4e2d\u5408\u5e76\u591a\u4e2a\u6a21\u578b\u68c0\u67e5\u70b9\uff0c\u901a\u8fc7\u6027\u80fd\u6307\u6807\uff08\u5982\u8bad\u7ec3\u635f\u5931\u6216\u8bad\u7ec3\u6b65\u6570\uff09\u52a0\u6743\u53c2\u6570\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cMWA\u4f18\u4e8e\u7b80\u5355\u7684\u5747\u5300\u5408\u5e76\uff0c\u635f\u5931\u52a0\u6743\u5408\u5e76\u7684\u6548\u679c\u6700\u4f73\uff0c\u4efb\u52a1\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe5%\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e3a\u4e86\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u4e2d\u7684\u8bad\u7ec3\u6548\u7387\uff0c\u901a\u8fc7\u5408\u5e76\u591a\u4e2a\u6a21\u578b\u68c0\u67e5\u70b9\u4ee5\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\uff0c\u5e76\u63a2\u7d22\u52a0\u6743\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51faMetrics-Weighted Averaging\uff08MWA\uff09\uff0c\u901a\u8fc7\u6027\u80fd\u6307\u6807\uff08\u5982\u8bad\u7ec3\u635f\u5931\u6216\u8bad\u7ec3\u6b65\u6570\uff09\u52a0\u6743\u68c0\u67e5\u70b9\u53c2\u6570\uff0c\u5e76\u4f7f\u7528\u60e9\u7f5a\u56e0\u5b50\u8c03\u6574\u6743\u91cd\u5206\u5e03\uff0c\u4ec5\u9700\u4e00\u4e2a\u8d85\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMWA\u5728\u6570\u5b66\u63a8\u7406\u3001\u504f\u597d\u5bf9\u9f50\u548c\u6307\u4ee4\u5fae\u8c03\u4e09\u4e2a\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u5747\u5300\u5408\u5e76\uff0c\u635f\u5931\u52a0\u6743\u5408\u5e76\u7684\u51c6\u786e\u7387\u63d0\u5347\u6700\u9ad8\u8fbe5%\uff0c\u751a\u81f3\u8d85\u8fc7\u5355\u4e2a\u6700\u7ec8\u68c0\u67e5\u70b9\u7684\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u9a8c\u8bc1\u4e86\u68c0\u67e5\u70b9\u5408\u5e76\u5bf9PEFT\u7684\u6709\u6548\u6027\uff0c\u5e76\u8bc1\u660e\u57fa\u4e8e\u5ea6\u91cf\u7684\u52a0\u6743\u65b9\u6cd5\u80fd\u591f\u4ee5\u6700\u5c0f\u8ba1\u7b97\u5f00\u9500\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2504.18857", "pdf": "https://arxiv.org/pdf/2504.18857", "abs": "https://arxiv.org/abs/2504.18857", "authors": ["Yi Lu", "Wanxu Zhao", "Xin Zhou", "Chenxin An", "Chenglong Wang", "Shuo Li", "Yuming Yang", "Jun Zhao", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Effective Length Extrapolation via Dimension-Wise Positional Embeddings Manipulation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often struggle to process and generate coherent\ncontext when the number of input tokens exceeds the pre-trained length. Recent\nadvancements in long-context extension have significantly expanded the context\nwindow of LLMs but require expensive overhead to train the large-scale models\nwith longer context. In this work, we propose Dimension-Wise Positional\nEmbeddings Manipulation (DPE), a training-free framework to extrapolate the\ncontext window of LLMs by diving into RoPE's different hidden dimensions.\nInstead of manipulating all dimensions equally, DPE detects the effective\nlength for every dimension and finds the key dimensions for context extension.\nWe reuse the original position indices with their embeddings from the\npre-trained model and manipulate the key dimensions' position indices to their\nmost effective lengths. In this way, DPE adjusts the pre-trained models with\nminimal modifications while ensuring that each dimension reaches its optimal\nstate for extrapolation. DPE significantly surpasses well-known baselines such\nas YaRN and Self-Extend. DPE enables Llama3-8k 8B to support context windows of\n128k tokens without continual training and integrates seamlessly with Flash\nAttention 2. In addition to its impressive extrapolation capability, DPE also\ndramatically improves the models' performance within training length, such as\nLlama3.1 70B, by over 18 points on popular long-context benchmarks RULER. When\ncompared with commercial models, Llama 3.1 70B with DPE even achieves better\nperformance than GPT-4-128K.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6DPE\uff0c\u901a\u8fc7\u4f18\u5316RoPE\u9690\u85cf\u7ef4\u5ea6\u7684\u4f4d\u7f6e\u5d4c\u5165\uff0c\u663e\u8457\u6269\u5c55LLMs\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u5e76\u63d0\u5347\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u5982YaRN\u548cSelf-Extend\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u5904\u7406\u8d85\u51fa\u9884\u8bad\u7ec3\u957f\u5ea6\u7684\u8f93\u5165\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u73b0\u6709\u6269\u5c55\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u65b9\u6cd5\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u4f18\u5316\u4f4d\u7f6e\u5d4c\u5165\uff0c\u4f4e\u6210\u672c\u6269\u5c55\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002", "method": "\u63d0\u51fa\u4e86DPE\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790RoPE\u7684\u9690\u85cf\u7ef4\u5ea6\uff0c\u8bc6\u522b\u5173\u952e\u7ef4\u5ea6\u5e76\u8c03\u6574\u5176\u4f4d\u7f6e\u7d22\u5f15\u81f3\u6700\u4f18\u957f\u5ea6\uff0c\u4fdd\u7559\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u539f\u59cb\u4f4d\u7f6e\u5d4c\u5165\uff0c\u4ec5\u5bf9\u5173\u952e\u7ef4\u5ea6\u8fdb\u884c\u6700\u5c0f\u4fee\u6539\u3002", "result": "DPE\u5728Llama3-8B\u4e0a\u5b9e\u73b0\u4e86128k tokens\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u652f\u6301\uff0c\u4e14\u6027\u80fd\u8d85\u8d8aYaRN\u548cSelf-Extend\uff1b\u5728Llama3.1 70B\u4e0a\uff0cRULER\u57fa\u51c6\u6027\u80fd\u63d0\u534718\u5206\uff0c\u751a\u81f3\u4f18\u4e8eGPT-4-128K\u3002", "conclusion": "DPE\u901a\u8fc7\u7ef4\u5ea6\u4f18\u5316\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4e0a\u4e0b\u6587\u6269\u5c55\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u517c\u5bb9Flash Attention 2\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\u548c\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2504.18777", "pdf": "https://arxiv.org/pdf/2504.18777", "abs": "https://arxiv.org/abs/2504.18777", "authors": ["Diana Febrita"], "title": "Evaluating AI-Driven Automated Map Digitization in QGIS", "categories": ["cs.AI"], "comment": "Submitted to 2025 Indiana Geographic Information Council (IGIC)\n  Conference", "summary": "Map digitization is an important process that converts maps into digital\nformats that can be used for further analysis. This process typically requires\na deep human involvement because of the need for interpretation and\ndecision-making when translating complex features. With the advancement of\nartificial intelligence, there is an alternative to conducting map digitization\nwith the help of machine learning techniques. Deepness, or Deep Neural Remote\nSensing, is an advanced AI-driven tool designed and integrated as a plugin in\nQGIS application. This research focuses on assessing the effectiveness of\nDeepness in automated digitization. This study analyses AI-generated\ndigitization results from Google Earth imagery and compares them with digitized\noutputs from OpenStreetMap (OSM) to evaluate performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86AI\u9a71\u52a8\u7684\u5de5\u5177Deepness\u5728\u81ea\u52a8\u5316\u5730\u56fe\u6570\u5b57\u5316\u4e2d\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u5bf9\u6bd4Google Earth\u5f71\u50cf\u548cOpenStreetMap\u7684\u6570\u5b57\u7ed3\u679c\u6765\u8bc4\u4f30\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5730\u56fe\u6570\u5b57\u5316\u8fc7\u7a0b\u9700\u8981\u5927\u91cf\u4eba\u5de5\u53c2\u4e0e\uff0c\u800cAI\u6280\u672f\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u7684\u53ef\u80fd\u6027\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30Deepness\u5de5\u5177\u5728\u8fd9\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "method": "\u4f7f\u7528Deepness\u63d2\u4ef6\u5728QGIS\u4e2d\u8fdb\u884c\u81ea\u52a8\u5316\u6570\u5b57\u5316\uff0c\u5e76\u6bd4\u8f83\u5176AI\u751f\u6210\u7684Google Earth\u5f71\u50cf\u6570\u5b57\u5316\u7ed3\u679c\u4e0eOpenStreetMap\u7684\u624b\u52a8\u6570\u5b57\u5316\u7ed3\u679c\u3002", "result": "\u7814\u7a76\u5206\u6790\u4e86Deepness\u7684\u81ea\u52a8\u5316\u6570\u5b57\u5316\u6548\u679c\uff0c\u5e76\u4e0eOpenStreetMap\u7684\u7ed3\u679c\u8fdb\u884c\u4e86\u5bf9\u6bd4\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aAI\u5728\u5730\u56fe\u6570\u5b57\u5316\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301\uff0c\u5c55\u793a\u4e86Deepness\u5728\u81ea\u52a8\u5316\u8fc7\u7a0b\u4e2d\u7684\u6f5c\u529b\u548c\u53ef\u80fd\u7684\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2504.18583", "pdf": "https://arxiv.org/pdf/2504.18583", "abs": "https://arxiv.org/abs/2504.18583", "authors": ["Zihao An", "Huajun Bai", "Ziqiong Liu", "Dong Li", "Emad Barsoum"], "title": "PARD: Accelerating LLM Inference with Low-Cost PARallel Draft Model Adaptation", "categories": ["cs.LG", "cs.PF"], "comment": "15 pages, 6 figures", "summary": "The autoregressive nature of large language models (LLMs) limits inference\nspeed. Each forward pass generates only a single token and is often\nbottlenecked by memory bandwidth. Speculative decoding alleviates this issue\nusing a draft-then-verify approach to accelerate token generation. However, the\noverhead introduced during the draft phase and the training cost of the draft\nmodel limit the efficiency and adaptability of speculative decoding. In this\nwork, we introduce PARallel Draft (PARD), a novel speculative decoding method\nthat enables low-cost adaptation of autoregressive draft models into parallel\ndraft models. PARD enhances inference efficiency by predicting multiple future\ntokens in a single forward pass of the draft phase, and incorporates a\nconditional drop token method to accelerate training. Its target-independence\nproperty allows a single draft model to be applied to an entire family of\ndifferent models, minimizing the adaptation cost. Our proposed conditional drop\ntoken method can improves draft model training efficiency by 3x. On our\noptimized inference framework, PARD accelerates LLaMA3.1-8B inference by 4.08x,\nachieving 311.5 tokens per second.", "AI": {"tldr": "PARD\u662f\u4e00\u79cd\u65b0\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e76\u884c\u8349\u6848\u6a21\u578b\u9884\u6d4b\u591a\u4e2a\u672a\u6765\u4ee4\u724c\uff0c\u8bad\u7ec3\u6548\u7387\u63d0\u53473\u500d\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53474.08\u500d\u3002", "motivation": "\u5f53\u524dLLMs\u7684\u63a8\u7406\u901f\u5ea6\u53d7\u9650\u4e8e\u81ea\u56de\u5f52\u6027\u8d28\uff0c\u5185\u5b58\u5e26\u5bbd\u6210\u4e3a\u74f6\u9888\u3002\u63a8\u6d4b\u89e3\u7801\u867d\u80fd\u52a0\u901f\uff0c\u4f46\u8349\u6848\u9636\u6bb5\u7684\u6210\u672c\u548c\u8bad\u7ec3\u5f00\u9500\u9650\u5236\u4e86\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faPARD\uff0c\u5c06\u81ea\u56de\u5f52\u8349\u6848\u6a21\u578b\u4f4e\u6210\u672c\u9002\u914d\u4e3a\u5e76\u884c\u8349\u6848\u6a21\u578b\uff0c\u5355\u6b21\u524d\u5411\u9884\u6d4b\u591a\u4ee4\u724c\uff0c\u5e76\u5f15\u5165\u6761\u4ef6\u4e22\u5f03\u4ee4\u724c\u65b9\u6cd5\u52a0\u901f\u8bad\u7ec3\u3002", "result": "PARD\u5c06LLaMA3.1-8B\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u81f3311.5\u4ee4\u724c/\u79d2\uff084.08\u500d\u52a0\u901f\uff09\uff0c\u8bad\u7ec3\u6548\u7387\u63d0\u9ad83\u500d\u3002", "conclusion": "PARD\u901a\u8fc7\u5e76\u884c\u5316\u548c\u6761\u4ef6\u4e22\u5f03\u4ee4\u724c\uff0c\u663e\u8457\u63d0\u5347LLMs\u7684\u63a8\u7406\u6548\u7387\u548c\u8bad\u7ec3\u9002\u5e94\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u9002\u914d\u6210\u672c\u3002"}}
{"id": "2504.18872", "pdf": "https://arxiv.org/pdf/2504.18872", "abs": "https://arxiv.org/abs/2504.18872", "authors": ["Alexandra Abbas", "Nora Petrova", "Helios Ael Lyons", "Natalia Perez-Campanero"], "title": "Latent Adversarial Training Improves the Representation of Refusal", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent work has shown that language models' refusal behavior is primarily\nencoded in a single direction in their latent space, making it vulnerable to\ntargeted attacks. Although Latent Adversarial Training (LAT) attempts to\nimprove robustness by introducing noise during training, a key question\nremains: How does this noise-based training affect the underlying\nrepresentation of refusal behavior? Understanding this encoding is crucial for\nevaluating LAT's effectiveness and limitations, just as the discovery of linear\nrefusal directions revealed vulnerabilities in traditional supervised safety\nfine-tuning (SSFT).\n  Through the analysis of Llama 2 7B, we examine how LAT reorganizes the\nrefusal behavior in the model's latent space compared to SSFT and embedding\nspace adversarial training (AT). By computing activation differences between\nharmful and harmless instruction pairs and applying Singular Value\nDecomposition (SVD), we find that LAT significantly alters the refusal\nrepresentation, concentrating it in the first two SVD components which explain\napproximately 75 percent of the activation differences variance - significantly\nhigher than in reference models. This concentrated representation leads to more\neffective and transferable refusal vectors for ablation attacks: LAT models\nshow improved robustness when attacked with vectors from reference models but\nbecome more vulnerable to self-generated vectors compared to SSFT and AT. Our\nfindings suggest that LAT's training perturbations enable a more comprehensive\nrepresentation of refusal behavior, highlighting both its potential strengths\nand vulnerabilities for improving model safety.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u8bed\u8a00\u6a21\u578b\u62d2\u7edd\u884c\u4e3a\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u7f16\u7801\u65b9\u5f0f\uff0c\u4ee5\u53ca\u6f5c\u5728\u5bf9\u6297\u8bad\u7ec3\uff08LAT\uff09\u5982\u4f55\u901a\u8fc7\u566a\u58f0\u8bad\u7ec3\u6539\u53d8\u8fd9\u79cd\u7f16\u7801\uff0c\u4ece\u800c\u5f71\u54cd\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u7406\u89e3LAT\u5982\u4f55\u901a\u8fc7\u566a\u58f0\u8bad\u7ec3\u6539\u53d8\u62d2\u7edd\u884c\u4e3a\u7684\u6f5c\u5728\u8868\u793a\uff0c\u4ee5\u8bc4\u4f30\u5176\u6548\u679c\u548c\u5c40\u9650\u6027\uff0c\u7c7b\u4f3c\u4e8e\u4f20\u7edf\u76d1\u7763\u5b89\u5168\u5fae\u8c03\uff08SSFT\uff09\u4e2d\u53d1\u73b0\u7684\u7ebf\u6027\u62d2\u7edd\u65b9\u5411\u7684\u8106\u5f31\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790Llama 2 7B\u6a21\u578b\uff0c\u6bd4\u8f83\u4e86LAT\u3001SSFT\u548c\u5d4c\u5165\u7a7a\u95f4\u5bf9\u6297\u8bad\u7ec3\uff08AT\uff09\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5bf9\u62d2\u7edd\u884c\u4e3a\u7684\u91cd\u7ec4\u6548\u679c\u3002\u4f7f\u7528\u6fc0\u6d3b\u5dee\u5f02\u548c\u5947\u5f02\u503c\u5206\u89e3\uff08SVD\uff09\u65b9\u6cd5\uff0c\u91cf\u5316\u4e86\u62d2\u7edd\u8868\u793a\u7684\u96c6\u4e2d\u7a0b\u5ea6\u3002", "result": "\u7814\u7a76\u53d1\u73b0LAT\u663e\u8457\u6539\u53d8\u4e86\u62d2\u7edd\u8868\u793a\uff0c\u5c06\u5176\u96c6\u4e2d\u5728SVD\u7684\u524d\u4e24\u4e2a\u6210\u5206\u4e2d\uff0c\u89e3\u91ca\u4e8675%\u7684\u6fc0\u6d3b\u5dee\u5f02\u65b9\u5dee\u3002\u8fd9\u79cd\u96c6\u4e2d\u8868\u793a\u4f7fLAT\u6a21\u578b\u5728\u9762\u5bf9\u653b\u51fb\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u4e5f\u66f4\u5bb9\u6613\u53d7\u5230\u81ea\u8eab\u751f\u6210\u5411\u91cf\u7684\u653b\u51fb\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cLAT\u7684\u8bad\u7ec3\u6270\u52a8\u80fd\u591f\u66f4\u5168\u9762\u5730\u8868\u793a\u62d2\u7edd\u884c\u4e3a\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u5176\u5728\u63d0\u5347\u6a21\u578b\u5b89\u5168\u6027\u65b9\u9762\u7684\u6f5c\u5728\u4f18\u52bf\u548c\u5f31\u70b9\u3002"}}
{"id": "2504.18794", "pdf": "https://arxiv.org/pdf/2504.18794", "abs": "https://arxiv.org/abs/2504.18794", "authors": ["Brendon Johnson", "Alfredo Weitzenfeld"], "title": "Hierarchical Reinforcement Learning in Multi-Goal Spatial Navigation with Autonomous Mobile Robots", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Hierarchical reinforcement learning (HRL) is hypothesized to be able to take\nadvantage of the inherent hierarchy in robot learning tasks with sparse reward\nschemes, in contrast to more traditional reinforcement learning algorithms. In\nthis research, hierarchical reinforcement learning is evaluated and contrasted\nwith standard reinforcement learning in complex navigation tasks. We evaluate\nunique characteristics of HRL, including their ability to create sub-goals and\nthe termination function. We constructed experiments to test the differences\nbetween PPO and HRL, different ways of creating sub-goals, manual vs automatic\nsub-goal creation, and the effects of the frequency of termination on\nperformance. These experiments highlight the advantages of HRL and how it\nachieves these advantages.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\uff08HRL\uff09\u5728\u590d\u6742\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\uff08\u5982PPO\uff09\u8fdb\u884c\u5bf9\u6bd4\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u5b50\u76ee\u6807\u751f\u6210\u548c\u7ec8\u6b62\u51fd\u6570\u7b49HRL\u7279\u6027\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u673a\u5668\u4eba\u5b66\u4e60\u4efb\u52a1\u4e2d\u666e\u904d\u5b58\u5728\u5c42\u6b21\u7ed3\u6784\uff0c\u4f46\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u7a00\u758f\u5956\u52b1\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002HRL\u88ab\u8ba4\u4e3a\u80fd\u66f4\u597d\u5730\u5229\u7528\u8fd9\u79cd\u5c42\u6b21\u7ed3\u6784\u63d0\u5347\u6027\u80fd\uff0c\u56e0\u6b64\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u5176\u4f18\u52bf\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u6bd4HRL\u4e0ePPO\u7684\u6027\u80fd\uff0c\u6d4b\u8bd5\u4e0d\u540c\u5b50\u76ee\u6807\u751f\u6210\u65b9\u5f0f\uff08\u624b\u52a8 vs \u81ea\u52a8\uff09\u4ee5\u53ca\u7ec8\u6b62\u9891\u7387\u5bf9\u4efb\u52a1\u8868\u73b0\u7684\u5f71\u54cd\uff0c\u4ee5\u5206\u6790HRL\u7684\u6838\u5fc3\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHRL\u901a\u8fc7\u5b50\u76ee\u6807\u751f\u6210\u548c\u7ec8\u6b62\u51fd\u6570\u7b49\u673a\u5236\uff0c\u5728\u590d\u6742\u5bfc\u822a\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\uff0c\u5c24\u5176\u662f\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u3002", "conclusion": "HRL\u51ed\u501f\u5176\u5c42\u6b21\u5316\u8bbe\u8ba1\u548c\u5b50\u76ee\u6807\u7ba1\u7406\u80fd\u529b\uff0c\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u89e3\u51b3\u590d\u6742\u4efb\u52a1\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.18587", "pdf": "https://arxiv.org/pdf/2504.18587", "abs": "https://arxiv.org/abs/2504.18587", "authors": ["Tianbing Xu"], "title": "Training Large Language Models to Reason via EM Policy Gradient", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Recently, foundation models such as OpenAI's O1 and O3, along with DeepSeek's\nR1, have demonstrated strong reasoning capacities and problem-solving skills\nacquired through large-scale reinforcement learning (RL), with wide\napplications in mathematics, coding, science, intelligent agents, and virtual\nassistants. In this work, we introduce an off-policy reinforcement learning\nalgorithm, EM Policy Gradient, aimed at enhancing LLM reasoning by optimizing\nexpected return over reasoning trajectories. We frame the reasoning task as an\nExpectation-Maximization (EM) optimization problem, alternating between\nsampling diverse rationale trajectories and performing reward-guided\nfine-tuning. Unlike PPO and GRPO, which rely on complex importance weights and\nheuristic clipping, our method provides a simpler, more principled off-policy\npolicy gradient approach, eliminating these complexities while maintaining\nstrong performance. We evaluate the effectiveness of EM Policy Gradient on the\nGSM8K and MATH (HARD) datasets, where it achieves performance comparable to or\nslightly surpassing the state-of-the-art GRPO, while offering additional\nadvantages in scalability, simplicity, and reasoning conciseness. Moreover,\nmodels fine-tuned with our method exhibit cognitive behaviors, such as\nsub-problem decomposition, self-verification, and backtracking, highlighting\nits potential to enhance both the interpretability and robustness of LLM\nreasoning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEM Policy Gradient\u7684\u79bb\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06\u63a8\u7406\u4efb\u52a1\u6784\u5efa\u4e3aEM\u4f18\u5316\u95ee\u9898\uff0c\u4ea4\u66ff\u91c7\u6837\u591a\u6837\u5316\u7684\u63a8\u7406\u8f68\u8ff9\u5e76\u57fa\u4e8e\u5956\u52b1\u8fdb\u884c\u5fae\u8c03\uff0c\u65e8\u5728\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002\u4e0ePPO\u548cGRPO\u7b49\u590d\u6742\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u66f4\u7b80\u5355\u4e14\u4fdd\u6301\u9ad8\u6027\u80fd\uff0c\u5728GSM8K\u548cMATH (HARD)\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u5982PPO\u3001GRPO\uff09\u4f9d\u8d56\u590d\u6742\u7684\u6743\u91cd\u8c03\u6574\u548c\u542f\u53d1\u5f0f\u526a\u88c1\uff0c\u9650\u5236\u4e86\u5176\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002\u672c\u6587\u5e0c\u671b\u901a\u8fc7\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u66f4\u539f\u5219\u5316\u7684\u79bb\u7b56\u7565\u65b9\u6cd5\u4f18\u5316\u63a8\u7406\u8f68\u8ff9\u7684\u671f\u671b\u56de\u62a5\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3001\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faEM Policy Gradient\u7b97\u6cd5\uff0c\u5c06\u63a8\u7406\u4efb\u52a1\u5efa\u6a21\u4e3a\u671f\u671b\u6700\u5927\u5316\uff08EM\uff09\u95ee\u9898\uff0c\u4ea4\u66ff\u8fdb\u884c\u4e24\u6b65\uff1a1) \u91c7\u6837\u591a\u6837\u5316\u7684\u63a8\u7406\u8f68\u8ff9\uff1b2) \u57fa\u4e8e\u5956\u52b1\u4fe1\u53f7\u5bf9\u7b56\u7565\u8fdb\u884c\u5fae\u8c03\u3002\u76f8\u6bd4PPO\u548cGRPO\uff0c\u907f\u514d\u4e86\u91cd\u8981\u6027\u6743\u91cd\u548c\u526a\u88c1\u7684\u590d\u6742\u6027\u3002", "result": "\u5728GSM8K\u548cMATH (HARD)\u6570\u636e\u96c6\u4e0a\uff0c\u6027\u80fd\u4e0e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5GRPO\u76f8\u5f53\u6216\u7565\u4f18\uff0c\u540c\u65f6\u5177\u5907\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u3001\u7b80\u6d01\u6027\u548c\u63a8\u7406\u51dd\u7ec3\u6027\u3002\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5c55\u73b0\u51fa\u5b50\u95ee\u9898\u5206\u89e3\u3001\u81ea\u6211\u9a8c\u8bc1\u548c\u56de\u6eaf\u7b49\u8ba4\u77e5\u884c\u4e3a\u3002", "conclusion": "EM Policy Gradient\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u79bb\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u63a8\u7406\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u5728\u589e\u5f3a\u6a21\u578b\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.18884", "pdf": "https://arxiv.org/pdf/2504.18884", "abs": "https://arxiv.org/abs/2504.18884", "authors": ["Junichiro Niimi"], "title": "A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text Classification", "categories": ["cs.CL", "cs.AI"], "comment": "This manuscript has been accepted for the 30th International\n  Conference on Natural Language & Information Systems (NLDB 2025). The final\n  version will appear in the Springer LNCS proceedings. arXiv admin note: text\n  overlap with arXiv:2407.13069", "summary": "With the advance of large language models (LLMs), LLMs have been utilized for\nthe various tasks. However, the issues of variability and reproducibility of\nresults from each trial of LLMs have been largely overlooked in existing\nliterature while actual human annotation uses majority voting to resolve\ndisagreements among annotators. Therefore, this study introduces the\nstraightforward ensemble strategy to a sentiment analysis using LLMs. As the\nresults, we demonstrate that the ensemble of multiple inference using\nmedium-sized LLMs produces more robust and accurate results than using a large\nmodel with a single attempt with reducing RMSE by 18.6%.", "AI": {"tldr": "LLMs\u5728\u60c5\u611f\u5206\u6790\u4e2d\u901a\u8fc7\u96c6\u6210\u591a\u4e2a\u4e2d\u578b\u6a21\u578b\u7684\u591a\u6b21\u63a8\u7406\uff0c\u6bd4\u5355\u4e00\u5927\u578b\u6a21\u578b\u66f4\u7a33\u5065\u51c6\u786e\uff0cRMSE\u964d\u4f4e18.6%\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u4e86LLMs\u7ed3\u679c\u7684\u53d8\u5f02\u6027\u4e0e\u53ef\u91cd\u590d\u6027\u95ee\u9898\uff0c\u800c\u4eba\u7c7b\u6807\u6ce8\u901a\u5e38\u91c7\u7528\u591a\u6570\u6295\u7968\u89e3\u51b3\u5206\u6b67\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u63a2\u7d22\u96c6\u6210\u7b56\u7565\u4ee5\u63d0\u5347\u60c5\u611f\u5206\u6790\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u96c6\u6210\u7b56\u7565\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u4e2d\u578bLLMs\u7684\u591a\u6b21\u63a8\u7406\u7ed3\u679c\u6765\u4f18\u5316\u60c5\u611f\u5206\u6790\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u96c6\u6210\u65b9\u6cd5\u6bd4\u5355\u4e00\u5927\u578b\u6a21\u578b\u7684\u5355\u6b21\u5c1d\u8bd5\u66f4\u51c6\u786e\uff0cRMSE\u964d\u4f4e\u4e8618.6%\u3002", "conclusion": "\u5728\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\uff0c\u96c6\u6210\u4e2d\u578bLLMs\u7684\u591a\u6b21\u63a8\u7406\u6bd4\u4f9d\u8d56\u5355\u4e00\u5927\u578b\u6a21\u578b\u66f4\u6709\u6548\uff0c\u4e14\u80fd\u663e\u8457\u63d0\u5347\u7ed3\u679c\u7684\u7a33\u5065\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2504.18875", "pdf": "https://arxiv.org/pdf/2504.18875", "abs": "https://arxiv.org/abs/2504.18875", "authors": ["Johannes Schneider"], "title": "Generative to Agentic AI: Survey, Conceptualization, and Challenges", "categories": ["cs.AI"], "comment": null, "summary": "Agentic Artificial Intelligence (AI) builds upon Generative AI (GenAI). It\nconstitutes the next major step in the evolution of AI with much stronger\nreasoning and interaction capabilities that enable more autonomous behavior to\ntackle complex tasks. Since the initial release of ChatGPT (3.5), Generative AI\nhas seen widespread adoption, giving users firsthand experience. However, the\ndistinction between Agentic AI and GenAI remains less well understood. To\naddress this gap, our survey is structured in two parts. In the first part, we\ncompare GenAI and Agentic AI using existing literature, discussing their key\ncharacteristics, how Agentic AI remedies limitations of GenAI, and the major\nsteps in GenAI's evolution toward Agentic AI. This section is intended for a\nbroad audience, including academics in both social sciences and engineering, as\nwell as industry professionals. It provides the necessary insights to\ncomprehend novel applications that are possible with Agentic AI but not with\nGenAI. In the second part, we deep dive into novel aspects of Agentic AI,\nincluding recent developments and practical concerns such as defining agents.\nFinally, we discuss several challenges that could serve as a future research\nagenda, while cautioning against risks that can emerge when exceeding human\nintelligence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86Agentic AI\u4e0eGenAI\u7684\u533a\u522b\u53ca\u5176\u6f14\u53d8\uff0c\u5206\u4e3a\u4e24\u90e8\u5206\uff1a\u9996\u5148\u6bd4\u8f83\u4e24\u8005\u7684\u5173\u952e\u7279\u6027\u53caAgentic AI\u5982\u4f55\u5f25\u8865GenAI\u7684\u4e0d\u8db3\uff1b\u5176\u6b21\u6df1\u5165\u63a2\u8ba8Agentic AI\u7684\u65b0\u9896\u65b9\u9762\u53ca\u672a\u6765\u7814\u7a76\u7684\u6311\u6218\u3002", "motivation": "\u89e3\u51b3Agentic AI\u4e0eGenAI\u4e4b\u95f4\u533a\u522b\u4e0d\u660e\u786e\u7684\u95ee\u9898\uff0c\u5e76\u4e3a\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u63d0\u4f9b\u7406\u89e3Agentic AI\u65b0\u5e94\u7528\u7684\u89c1\u89e3\u3002", "method": "\u901a\u8fc7\u6587\u732e\u6bd4\u8f83GenAI\u548cAgentic AI\u7684\u5173\u952e\u7279\u6027\uff0c\u5206\u6790Agentic AI\u7684\u6f14\u53d8\u53ca\u65b0\u9896\u65b9\u9762\uff0c\u5e76\u8ba8\u8bba\u672a\u6765\u7814\u7a76\u65b9\u5411\u548c\u6f5c\u5728\u98ce\u9669\u3002", "result": "\u660e\u786e\u4e86Agentic AI\u7684\u72ec\u7279\u6027\u53ca\u5176\u8d85\u8d8aGenAI\u7684\u80fd\u529b\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u672a\u6765\u53d1\u5c55\u4e2d\u7684\u6311\u6218\u4e0e\u98ce\u9669\u3002", "conclusion": "Agentic AI\u4ee3\u8868\u4e86AI\u53d1\u5c55\u7684\u65b0\u9636\u6bb5\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u63a8\u7406\u548c\u4ea4\u4e92\u80fd\u529b\uff0c\u4f46\u4ecd\u9700\u5e94\u5bf9\u6280\u672f\u548c\u793e\u4f1a\u6311\u6218\u3002"}}
{"id": "2504.18588", "pdf": "https://arxiv.org/pdf/2504.18588", "abs": "https://arxiv.org/abs/2504.18588", "authors": ["YongHui Xia", "Lan Wang", "Hao Wu"], "title": "Dynamic QoS Prediction via a Non-Negative Tensor Snowflake Factorization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Dynamic quality of service (QoS) data exhibit rich temporal patterns in\nuser-service interactions, which are crucial for a comprehensive understanding\nof user behavior and service conditions in Web service. As the number of users\nand services increases, there is a large amount of unobserved QoS data, which\nsignificantly affects users'choice of services. To predict unobserved QoS data,\nwe propose a Non-negative Snowflake Factorization of tensors model. This method\ndesigns a snowflake core tensor to enhance the model's learning capability.\nAdditionally, it employs a single latent factor-based, nonnegative\nmultiplication update on tensor (SLF-NMUT) for parameter learning. Empirical\nresults demonstrate that the proposed model more accurately learns dynamic\nuser-service interaction patterns, thereby yielding improved predictions for\nmissing QoS data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u8d1f\u96ea\u82b1\u5f20\u91cf\u5206\u89e3\u6a21\u578b\uff08Non-negative Snowflake Factorization of tensors\uff09\u6765\u9884\u6d4b\u7f3a\u5931\u7684\u52a8\u6001QoS\u6570\u636e\uff0c\u901a\u8fc7\u96ea\u82b1\u6838\u5fc3\u5f20\u91cf\u548cSLF-NMUT\u53c2\u6570\u5b66\u4e60\u65b9\u6cd5\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u5728Web\u670d\u52a1\u4e2d\uff0c\u52a8\u6001QoS\u6570\u636e\u5b58\u5728\u5927\u91cf\u672a\u89c2\u6d4b\u503c\uff0c\u5f71\u54cd\u7528\u6237\u670d\u52a1\u9009\u62e9\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u7f3a\u5931\u6570\u636e\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96ea\u82b1\u6838\u5fc3\u5f20\u91cf\u7684\u975e\u8d1f\u5f20\u91cf\u5206\u89e3\u6a21\u578b\uff0c\u5e76\u91c7\u7528SLF-NMUT\uff08\u5355\u6f5c\u5728\u56e0\u5b50\u975e\u8d1f\u4e58\u6cd5\u66f4\u65b0\uff09\u8fdb\u884c\u53c2\u6570\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u80fd\u66f4\u51c6\u786e\u5730\u5b66\u4e60\u52a8\u6001\u7528\u6237-\u670d\u52a1\u4ea4\u4e92\u6a21\u5f0f\uff0c\u4ece\u800c\u63d0\u5347\u7f3a\u5931QoS\u6570\u636e\u7684\u9884\u6d4b\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001QoS\u6570\u636e\u7f3a\u5931\u95ee\u9898\uff0c\u4e3aWeb\u670d\u52a1\u4e2d\u7684\u7528\u6237\u884c\u4e3a\u548c\u670d\u52a1\u6761\u4ef6\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u652f\u6301\u3002"}}
{"id": "2504.18938", "pdf": "https://arxiv.org/pdf/2504.18938", "abs": "https://arxiv.org/abs/2504.18938", "authors": ["Junhong Liang", "Yu Zhou"], "title": "MTCSC: Retrieval-Augmented Iterative Refinement for Chinese Spelling Correction", "categories": ["cs.CL"], "comment": "12 pages, 2 figures", "summary": "Chinese Spelling Correction (CSC) aims to detect and correct erroneous tokens\nin sentences. While Large Language Models (LLMs) have shown remarkable success\nin identifying and rectifying potential errors, they often struggle with\nmaintaining consistent output lengths and adapting to domain-specific\ncorrections. Furthermore, existing CSC task impose rigid constraints requiring\ninput and output lengths to be identical, limiting their applicability. In this\nwork, we extend traditional CSC to variable-length correction scenarios,\nincluding Chinese Splitting Error Correction (CSEC) and ASR N-best Error\nCorrection. To address domain adaptation and length consistency, we propose\nMTCSC (Multi-Turn CSC) framework based on RAG enhanced with a length reflection\nmechanism. Our approach constructs a retrieval database from domain-specific\ntraining data and dictionaries, fine-tuning retrievers to optimize performance\nfor error-containing inputs. Additionally, we introduce a multi-source\ncombination strategy with iterative length reflection to ensure output length\nfidelity. Experiments across diverse domain datasets demonstrate that our\nmethod significantly outperforms current approaches in correction quality,\nparticularly in handling domain-specific and variable-length error correction\ntasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86MTCSC\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u4f20\u7edf\u4e2d\u6587\u62fc\u5199\u7ea0\u9519\uff08CSC\uff09\u5230\u53ef\u53d8\u957f\u5ea6\u7ea0\u9519\u573a\u666f\uff0c\u901a\u8fc7RAG\u589e\u5f3a\u548c\u957f\u5ea6\u53cd\u5c04\u673a\u5236\u89e3\u51b3\u9886\u57df\u9002\u5e94\u548c\u957f\u5ea6\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e2d\u6587\u62fc\u5199\u7ea0\u9519\u4e2d\u5b58\u5728\u8f93\u51fa\u957f\u5ea6\u4e0d\u4e00\u81f4\u548c\u9886\u57df\u9002\u5e94\u95ee\u9898\uff0c\u4f20\u7edfCSC\u4efb\u52a1\u5bf9\u8f93\u5165\u8f93\u51fa\u957f\u5ea6\u4e00\u81f4\u6027\u7684\u786c\u6027\u9650\u5236\u4e5f\u9650\u5236\u4e86\u5176\u9002\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8eRAG\u7684MTCSC\u6846\u67b6\uff0c\u6784\u5efa\u9886\u57df\u7279\u5b9a\u68c0\u7d22\u6570\u636e\u5e93\uff0c\u5e76\u901a\u8fc7\u591a\u6e90\u7ec4\u5408\u7b56\u7565\u548c\u8fed\u4ee3\u957f\u5ea6\u53cd\u5c04\u786e\u4fdd\u8f93\u51fa\u957f\u5ea6\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u9886\u57df\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u9886\u57df\u7279\u5b9a\u548c\u53ef\u53d8\u957f\u5ea6\u7ea0\u9519\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "MTCSC\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u9886\u57df\u9002\u5e94\u548c\u957f\u5ea6\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u4e2d\u6587\u62fc\u5199\u7ea0\u9519\u7684\u6027\u80fd\u3002"}}
{"id": "2504.18880", "pdf": "https://arxiv.org/pdf/2504.18880", "abs": "https://arxiv.org/abs/2504.18880", "authors": ["Zuhong Lin", "Daoyuan Ren", "Kai Ran", "Sun Jing", "Xiaotiang Huang", "Haiyang He", "Pengxu Pan", "Xiaohang Zhang", "Ying Fang", "Tianying Wang", "Minli Wu", "Zhanglin Li", "Xiaochuan Zhang", "Haipu Li", "Jingjing Yao"], "title": "Reshaping MOFs Text Mining with a Dynamic Multi-Agent Framework of Large Language Agents", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "comment": null, "summary": "The mining of synthesis conditions for metal-organic frameworks (MOFs) is a\nsignificant focus in materials science. However, identifying the precise\nsynthesis conditions for specific MOFs within the vast array of possibilities\npresents a considerable challenge. Large Language Models (LLMs) offer a\npromising solution to this problem. We leveraged the capabilities of LLMs,\nspecifically gpt-4o-mini, as core agents to integrate various MOF-related\nagents, including synthesis, attribute, and chemical information agents. This\nintegration culminated in the development of MOFh6, an LLM tool designed to\nstreamline the MOF synthesis process. MOFh6 allows users to query in multiple\nformats, such as submitting scientific literature, or inquiring about specific\nMOF codes or structural properties. The tool analyzes these queries to provide\noptimal synthesis conditions and generates model files for density functional\ntheory pre modeling. We believe MOFh6 will enhance efficiency in the MOF\nsynthesis of all researchers.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5f00\u53d1\u5de5\u5177MOFh6\uff0c\u7528\u4e8e\u4f18\u5316\u91d1\u5c5e\u6709\u673a\u6846\u67b6\uff08MOF\uff09\u7684\u5408\u6210\u6761\u4ef6\u67e5\u8be2\u548c\u5206\u6790\uff0c\u63d0\u5347\u7814\u7a76\u6548\u7387\u3002", "motivation": "MOF\u5408\u6210\u6761\u4ef6\u7e41\u591a\u4e14\u590d\u6742\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u9ad8\u6548\u7b5b\u9009\u6700\u4f18\u6761\u4ef6\uff0cLLM\u63d0\u4f9b\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u6f5c\u529b\u3002", "method": "\u5229\u7528gpt-4o-mini\u4f5c\u4e3a\u6838\u5fc3\u4ee3\u7406\uff0c\u96c6\u6210MOF\u5408\u6210\u3001\u5c5e\u6027\u548c\u5316\u5b66\u4fe1\u606f\u4ee3\u7406\uff0c\u5f00\u53d1\u591a\u683c\u5f0f\u67e5\u8be2\u5de5\u5177MOFh6\u3002", "result": "MOFh6\u652f\u6301\u901a\u8fc7\u6587\u732e\u3001MOF\u4ee3\u7801\u6216\u7ed3\u6784\u5c5e\u6027\u67e5\u8be2\uff0c\u63d0\u4f9b\u6700\u4f18\u5408\u6210\u6761\u4ef6\u5e76\u751f\u6210DFT\u9884\u5efa\u6a21\u6587\u4ef6\u3002", "conclusion": "MOFh6\u6709\u671b\u663e\u8457\u63d0\u5347MOF\u5408\u6210\u7814\u7a76\u7684\u6548\u7387\uff0c\u4e3a\u9886\u57df\u63d0\u4f9b\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2504.18590", "pdf": "https://arxiv.org/pdf/2504.18590", "abs": "https://arxiv.org/abs/2504.18590", "authors": ["Guillaume Lauga", "Ma\u00ebl Chaumette", "Edgar Desainte-Mar\u00e9ville", "\u00c9tienne Lasalle", "Arthur Lebeurrier"], "title": "A multilevel approach to accelerate the training of Transformers", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "In this article, we investigate the potential of multilevel approaches to\naccelerate the training of transformer architectures. Using an ordinary\ndifferential equation (ODE) interpretation of these architectures, we propose\nan appropriate way of varying the discretization of these ODE Transformers in\norder to accelerate the training. We validate our approach experimentally by a\ncomparison with the standard training procedure.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u591a\u7ea7\u65b9\u6cd5\u52a0\u901fTransformer\u67b6\u6784\u8bad\u7ec3\u7684\u6f5c\u529b\uff0c\u901a\u8fc7ODE\u89e3\u91ca\u63d0\u51fa\u53d8\u79bb\u6563\u5316\u7b56\u7565\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "Transformer\u67b6\u6784\u7684\u8bad\u7ec3\u901a\u5e38\u8017\u65f6\u4e14\u8d44\u6e90\u5bc6\u96c6\uff0c\u56e0\u6b64\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u591a\u7ea7\u65b9\u6cd5\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u63d0\u5347\u6548\u7387\u3002", "method": "\u5c06Transformer\u67b6\u6784\u89e3\u91ca\u4e3a\u5e38\u5fae\u5206\u65b9\u7a0b\uff08ODE\uff09\uff0c\u5e76\u63d0\u51fa\u53d8\u79bb\u6563\u5316\u7b56\u7565\u4ee5\u52a0\u901f\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6807\u51c6\u8bad\u7ec3\u6d41\u7a0b\u80fd\u6709\u6548\u63d0\u5347\u8bad\u7ec3\u901f\u5ea6\u3002", "conclusion": "\u57fa\u4e8eODE\u7684\u591a\u7ea7\u79bb\u6563\u5316\u7b56\u7565\u4e3a\u52a0\u901fTransformer\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u5411\u3002"}}
{"id": "2504.18942", "pdf": "https://arxiv.org/pdf/2504.18942", "abs": "https://arxiv.org/abs/2504.18942", "authors": ["Debarati Das", "Khanh Chi Le", "Ritik Sachin Parkar", "Karin De Langis", "Brendan Madson", "Chad M. Berryman", "Robin M. Willis", "Daniel H. Moses", "Brett McDonnell", "Daniel Schwarcz", "Dongyeop Kang"], "title": "LawFlow : Collecting and Simulating Lawyers' Thought Processes", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "submitted to COLM 2025", "summary": "Legal practitioners, particularly those early in their careers, face complex,\nhigh-stakes tasks that require adaptive, context-sensitive reasoning. While AI\nholds promise in supporting legal work, current datasets and models are\nnarrowly focused on isolated subtasks and fail to capture the end-to-end\ndecision-making required in real-world practice. To address this gap, we\nintroduce LawFlow, a dataset of complete end-to-end legal workflows collected\nfrom trained law students, grounded in real-world business entity formation\nscenarios. Unlike prior datasets focused on input-output pairs or linear chains\nof thought, LawFlow captures dynamic, modular, and iterative reasoning\nprocesses that reflect the ambiguity, revision, and client-adaptive strategies\nof legal practice. Using LawFlow, we compare human and LLM-generated workflows,\nrevealing systematic differences in structure, reasoning flexibility, and plan\nexecution. Human workflows tend to be modular and adaptive, while LLM workflows\nare more sequential, exhaustive, and less sensitive to downstream implications.\nOur findings also suggest that legal professionals prefer AI to carry out\nsupportive roles, such as brainstorming, identifying blind spots, and surfacing\nalternatives, rather than executing complex workflows end-to-end. Building on\nthese findings, we propose a set of design suggestions, rooted in empirical\nobservations, that align AI assistance with human goals of clarity,\ncompleteness, creativity, and efficiency, through hybrid planning, adaptive\nexecution, and decision-point support. Our results highlight both the current\nlimitations of LLMs in supporting complex legal workflows and opportunities for\ndeveloping more collaborative, reasoning-aware legal AI systems. All data and\ncode are available on our project page\n(https://minnesotanlp.github.io/LawFlow-website/).", "AI": {"tldr": "LawFlow\u6570\u636e\u96c6\u65e8\u5728\u586b\u8865\u6cd5\u5f8bAI\u4e2d\u7aef\u5230\u7aef\u51b3\u7b56\u652f\u6301\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u6355\u6349\u771f\u5b9e\u7684\u52a8\u6001\u6cd5\u5f8b\u5de5\u4f5c\u6d41\uff0c\u6bd4\u8f83\u4eba\u4e0eLLM\u7684\u63a8\u7406\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u8bbe\u8ba1\u5efa\u8bae\u4ee5\u4f18\u5316AI\u8f85\u52a9\u89d2\u8272\u3002", "motivation": "\u5f53\u524d\u6cd5\u5f8bAI\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u4ec5\u5173\u6ce8\u5b64\u7acb\u5b50\u4efb\u52a1\uff0c\u65e0\u6cd5\u652f\u6301\u5b9e\u9645\u6cd5\u5f8b\u5b9e\u8df5\u4e2d\u590d\u6742\u3001\u9ad8\u98ce\u9669\u7684\u7aef\u5230\u7aef\u51b3\u7b56\u3002LawFlow\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u7f3a\u9677\uff0c\u6355\u6349\u771f\u5b9e\u7684\u6cd5\u5f8b\u5de5\u4f5c\u6d41\u3002", "method": "\u901a\u8fc7\u6536\u96c6\u6cd5\u5f8b\u5b66\u751f\u5728\u771f\u5b9e\u4e1a\u52a1\u5b9e\u4f53\u5f62\u6210\u573a\u666f\u4e2d\u7684\u52a8\u6001\u3001\u6a21\u5757\u5316\u5de5\u4f5c\u6d41\uff0c\u6784\u5efaLawFlow\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u6bd4\u4eba\u7c7b\u4e0eLLM\u751f\u6210\u7684\u5de5\u4f5c\u6d41\u7ed3\u6784\u3001\u7075\u6d3b\u6027\u548c\u6267\u884c\u5dee\u5f02\u3002", "result": "\u4eba\u7c7b\u5de5\u4f5c\u6d41\u66f4\u6a21\u5757\u5316\u548c\u81ea\u9002\u5e94\uff0c\u800cLLM\u5de5\u4f5c\u6d41\u66f4\u7ebf\u6027\u3001\u8be6\u5c3d\u4e14\u5bf9\u4e0b\u6e38\u5f71\u54cd\u4e0d\u654f\u611f\u3002\u6cd5\u5f8b\u4e13\u4e1a\u4eba\u58eb\u66f4\u503e\u5411\u4e8eAI\u627f\u62c5\u8f85\u52a9\u89d2\u8272\uff08\u5982\u5934\u8111\u98ce\u66b4\u3001\u76f2\u70b9\u8bc6\u522b\uff09\u800c\u975e\u7aef\u5230\u7aef\u6267\u884c\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLM\u5728\u652f\u6301\u590d\u6742\u6cd5\u5f8b\u5de5\u4f5c\u6d41\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u57fa\u4e8e\u6df7\u5408\u89c4\u5212\u3001\u81ea\u9002\u5e94\u6267\u884c\u7684\u8bbe\u8ba1\u5efa\u8bae\uff0c\u4ee5\u53d1\u5c55\u66f4\u5177\u534f\u4f5c\u6027\u7684\u6cd5\u5f8bAI\u7cfb\u7edf\u3002"}}
{"id": "2504.18948", "pdf": "https://arxiv.org/pdf/2504.18948", "abs": "https://arxiv.org/abs/2504.18948", "authors": ["Devesh Pant", "Dibyendu Talukder", "Deepak Kumar", "Rachit Pandey", "Aaditeshwar Seth", "Chetan Arora"], "title": "Use of Metric Learning for the Recognition of Handwritten Digits, and its Application to Increase the Outreach of Voice-based Communication Platforms", "categories": ["cs.AI", "cs.CV"], "comment": "10 Pages, 7 Figures, ACM COMPASS 2022", "summary": "Initiation, monitoring, and evaluation of development programmes can involve\nfield-based data collection about project activities. This data collection\nthrough digital devices may not always be feasible though, for reasons such as\nunaffordability of smartphones and tablets by field-based cadre, or shortfalls\nin their training and capacity building. Paper-based data collection has been\nargued to be more appropriate in several contexts, with automated digitization\nof the paper forms through OCR (Optical Character Recognition) and OMR (Optical\nMark Recognition) techniques. We contribute with providing a large dataset of\nhandwritten digits, and deep learning based models and methods built using this\ndata, that are effective in real-world environments. We demonstrate the\ndeployment of these tools in the context of a maternal and child health and\nnutrition awareness project, which uses IVR (Interactive Voice Response)\nsystems to provide awareness information to rural women SHG (Self Help Group)\nmembers in north India. Paper forms were used to collect phone numbers of the\nSHG members at scale, which were digitized using the OCR tools developed by us,\nand used to push almost 4 million phone calls. The data, model, and code have\nbeen released in the open-source domain.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u901a\u8fc7\u7eb8\u8d28\u6570\u636e\u6536\u96c6\u7ed3\u5408OCR/OMR\u6280\u672f\u5b9e\u73b0\u81ea\u52a8\u6570\u5b57\u5316\u7684\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u624b\u5199\u6570\u5b57\u6570\u636e\u96c6\u53ca\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u5370\u5ea6\u519c\u6751\u5987\u5973\u5065\u5eb7\u9879\u76ee\u4e2d\u3002", "motivation": "\u5728\u8d44\u6e90\u4e0d\u8db3\u5730\u533a\uff0c\u6570\u5b57\u5316\u8bbe\u5907\u5982\u667a\u80fd\u624b\u673a\u548c\u5e73\u677f\u7535\u8111\u7684\u4f7f\u7528\u53d7\u9650\uff0c\u56e0\u6b64\u7814\u7a76\u7eb8\u8d28\u6570\u636e\u6536\u96c6\u4e0e\u81ea\u52a8\u5316\u6570\u5b57\u5316\u7684\u53ef\u884c\u6027\uff0c\u4ee5\u652f\u6301\u9879\u76ee\u76d1\u6d4b\u4e0e\u8bc4\u4f30\u3002", "method": "\u91c7\u7528OCR\u548cOMR\u6280\u672f\u81ea\u52a8\u5316\u5904\u7406\u7eb8\u8d28\u8868\u683c\uff0c\u5e76\u57fa\u4e8e\u5927\u89c4\u6a21\u624b\u5199\u6570\u5b57\u6570\u636e\u96c6\u5f00\u53d1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u5728\u5370\u5ea6\u519c\u6751\u5987\u5973\u5065\u5eb7\u9879\u76ee\u4e2d\u6210\u529f\u6570\u5b57\u5316\u4e86\u7eb8\u8d28\u8868\u683c\uff0c\u63a8\u52a8\u4e86\u8fd1400\u4e07\u901a\u7535\u8bdd\u670d\u52a1\uff0c\u6570\u636e\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002", "conclusion": "\u7eb8\u8d28\u6570\u636e\u7ed3\u5408\u81ea\u52a8\u5316\u6570\u5b57\u5316\u6280\u672f\u662f\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6a21\u578b\u548c\u6570\u636e\u7684\u5f00\u6e90\u4fc3\u8fdb\u4e86\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u3002"}}
{"id": "2504.18591", "pdf": "https://arxiv.org/pdf/2504.18591", "abs": "https://arxiv.org/abs/2504.18591", "authors": ["Giovanni Catalani", "Michael Bauerheim", "Fr\u00e9d\u00e9ric Tost", "Xavier Bertrand", "Joseph Morlier"], "title": "Geometry aware inference of steady state PDEs using Equivariant Neural Fields representations", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent advances in Neural Fields have enabled powerful,\ndiscretization-invariant methods for learning neural operators that approximate\nsolutions of Partial Differential Equations (PDEs) on general geometries.\nBuilding on these developments, we introduce enf2enf, an encoder--decoder\nmethodology for predicting steady-state Partial Differential Equations with\nnon-parameterized geometric variability, based on recently proposed Equivariant\nNeural Field architectures. In enf2enf, input geometries are encoded into\nlatent point cloud embeddings that inherently preserve geometric grounding and\ncapture local phenomena. The resulting representations are then combined with\nglobal parameters and directly decoded into continuous output fields, thus\nefficiently modeling the coupling between geometry and physics. By leveraging\nthe inductive biases of locality and translation invariance, our approach is\nable to capture fine-scale physical features as well as complex shape\nvariations, thereby enhancing generalization and physical compliance. Extensive\nexperiments on a high-fidelity aerodynamic dataset, a hyper-elastic material\nbenchmark, and multi-element airfoil geometries, demonstrate that the proposed\nmodel achieves superior or competitive performance compared to state-of-the-art\ngraph based, operator learning, and neural field methods. Notably, our method\nsupports real time inference and zero-shot super-resolution, enabling efficient\ntraining on low-resolution meshes while maintaining high accuracy on full-scale\ndiscretizations.", "AI": {"tldr": "enf2enf\u662f\u4e00\u79cd\u57fa\u4e8e\u7b49\u53d8\u795e\u7ecf\u573a\u67b6\u6784\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u5177\u6709\u975e\u53c2\u6570\u5316\u51e0\u4f55\u53d8\u5316\u7684\u7a33\u6001\u504f\u5fae\u5206\u65b9\u7a0b\uff0c\u901a\u8fc7\u5c40\u90e8\u6027\u548c\u5e73\u79fb\u4e0d\u53d8\u6027\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u8bad\u7ec3\u548c\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u795e\u7ecf\u573a\u5728\u5904\u7406\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u7684\u901a\u7528\u51e0\u4f55\u5b66\u4e60\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5982\u4f55\u9ad8\u6548\u5904\u7406\u975e\u53c2\u6570\u5316\u51e0\u4f55\u53d8\u5316\u5e76\u4fdd\u6301\u7269\u7406\u4e00\u81f4\u6027\u4ecd\u5177\u6311\u6218\u6027\u3002enf2enf\u7684\u63d0\u51fa\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "enf2enf\u5c06\u8f93\u5165\u51e0\u4f55\u7f16\u7801\u4e3a\u6f5c\u5728\u70b9\u4e91\u5d4c\u5165\uff0c\u7ed3\u5408\u5168\u5c40\u53c2\u6570\u5e76\u89e3\u7801\u4e3a\u8fde\u7eed\u8f93\u51fa\u573a\u3002\u901a\u8fc7\u5c40\u90e8\u6027\u548c\u5e73\u79fb\u4e0d\u53d8\u6027\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u6a21\u578b\u80fd\u6355\u6349\u7ec6\u5c3a\u5ea6\u7269\u7406\u7279\u5f81\u548c\u590d\u6742\u5f62\u72b6\u53d8\u5316\u3002", "result": "\u5728\u7a7a\u6c14\u52a8\u529b\u5b66\u6570\u636e\u96c6\u3001\u8d85\u5f39\u6027\u6750\u6599\u57fa\u51c6\u548c\u591a\u5143\u7d20\u7ffc\u578b\u51e0\u4f55\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cenf2enf\u4f18\u4e8e\u6216\u4e0e\u6700\u5148\u8fdb\u7684\u56fe\u57fa\u3001\u7b97\u5b50\u5b66\u4e60\u548c\u795e\u7ecf\u573a\u65b9\u6cd5\u76f8\u5f53\uff0c\u652f\u6301\u5b9e\u65f6\u63a8\u7406\u548c\u96f6\u6837\u672c\u8d85\u5206\u8fa8\u7387\u3002", "conclusion": "enf2enf\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u4e0e\u7269\u7406\u7684\u8026\u5408\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u7269\u7406\u4e00\u81f4\u6027\uff0c\u4e3a\u590d\u6742\u51e0\u4f55\u4e0b\u7684PDE\u6c42\u89e3\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u7cbe\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18992", "pdf": "https://arxiv.org/pdf/2504.18992", "abs": "https://arxiv.org/abs/2504.18992", "authors": ["Sanwoo Lee", "Jiahao Liu", "Qifan Wang", "Jingang Wang", "Xunliang Cai", "Yunfang Wu"], "title": "Dynamic Fisher-weighted Model Merging via Bayesian Optimization", "categories": ["cs.CL"], "comment": null, "summary": "The fine-tuning of pre-trained language models has resulted in the widespread\navailability of task-specific models. Model merging offers an efficient way to\ncreate multi-task models by combining these fine-tuned models at the parameter\nlevel, without the need for training data or joint training on multiple\ndatasets. Existing merging approaches typically involve scaling the parameters\nmodel-wise or integrating parameter importance parameter-wise. Both approaches\nexhibit their own weaknesses, leading to a notable performance gap compared to\nmulti-task fine-tuning. In this paper, we unify these seemingly distinct\nstrategies into a more general merging framework, and introduce Dynamic\nFisher-weighted Merging (DF-Merge). Specifically, candidate models are\nassociated with a set of coefficients that linearly scale their fine-tuned\nparameters. Bayesian optimization is applied to dynamically adjust these\ncoefficients, aiming to maximize overall performance on validation sets. Each\niteration of this process integrates parameter importance based on the Fisher\ninformation conditioned by the coefficients. Experimental results show that\nDF-Merge outperforms strong baselines across models of different sizes and a\nvariety of tasks. Our analysis shows that the effectiveness of DF-Merge arises\nfrom the unified view of merging and that near-optimal performance is\nachievable in a few iterations, even with minimal validation data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001Fisher\u52a0\u6743\u5408\u5e76\uff08DF-Merge\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u6a21\u578b\u7ea7\u548c\u53c2\u6570\u7ea7\u7684\u5408\u5e76\u7b56\u7565\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u4f18\u5316\u52a8\u6001\u8c03\u6574\u53c2\u6570\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u4efb\u52a1\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\uff08\u6a21\u578b\u7ea7\u6216\u53c2\u6570\u7ea7\uff09\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\uff0c\u5e0c\u671b\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u63d0\u5347\u591a\u4efb\u52a1\u6a21\u578b\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "method": "\u91c7\u7528\u52a8\u6001Fisher\u52a0\u6743\u5408\u5e76\uff08DF-Merge\uff09\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u4f18\u5316\u52a8\u6001\u8c03\u6574\u53c2\u6570\u7ebf\u6027\u7f29\u653e\u7cfb\u6570\uff0c\u5e76\u57fa\u4e8eFisher\u4fe1\u606f\u6761\u4ef6\u6574\u5408\u53c2\u6570\u91cd\u8981\u6027\u3002", "result": "DF-Merge\u5728\u4e0d\u540c\u89c4\u6a21\u548c\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u9a8c\u8bc1\u6570\u636e\u91cf\u5c11\u65f6\u4ecd\u80fd\u5feb\u901f\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "DF-Merge\u901a\u8fc7\u7edf\u4e00\u5408\u5e76\u89c6\u89d2\u548c\u52a8\u6001\u4f18\u5316\u673a\u5236\uff0c\u4e3a\u591a\u4efb\u52a1\u6a21\u578b\u5408\u5e76\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.19017", "pdf": "https://arxiv.org/pdf/2504.19017", "abs": "https://arxiv.org/abs/2504.19017", "authors": ["Alireza Ghafarollahi", "Markus J. Buehler"], "title": "Sparks: Multi-Agent Artificial Intelligence Model Discovers Protein Design Principles", "categories": ["cs.AI", "cond-mat.mtrl-sci", "cond-mat.soft", "cs.LG", "q-bio.BM"], "comment": null, "summary": "Advances in artificial intelligence (AI) promise autonomous discovery, yet\nmost systems still resurface knowledge latent in their training data. We\npresent Sparks, a multi-modal multi-agent AI model that executes the entire\ndiscovery cycle that includes hypothesis generation, experiment design and\niterative refinement to develop generalizable principles and a report without\nhuman intervention. Applied to protein science, Sparks uncovered two previously\nunknown phenomena: (i) a length-dependent mechanical crossover whereby\nbeta-sheet-biased peptides surpass alpha-helical ones in unfolding force beyond\n~80 residues, establishing a new design principle for peptide mechanics; and\n(ii) a chain-length/secondary-structure stability map revealing unexpectedly\nrobust beta-sheet-rich architectures and a \"frustration zone\" of high variance\nin mixed alpha/beta folds. These findings emerged from fully self-directed\nreasoning cycles that combined generative sequence design, high-accuracy\nstructure prediction and physics-aware property models, with paired\ngeneration-and-reflection agents enforcing self-correction and reproducibility.\nThe key result is that Sparks can independently conduct rigorous scientific\ninquiry and identify previously unknown scientific principles.", "AI": {"tldr": "Sparks\u662f\u4e00\u79cd\u591a\u6a21\u6001\u591a\u4ee3\u7406AI\u6a21\u578b\uff0c\u80fd\u591f\u81ea\u4e3b\u6267\u884c\u4ece\u5047\u8bbe\u751f\u6210\u5230\u5b9e\u9a8c\u8bbe\u8ba1\u7684\u5b8c\u6574\u79d1\u5b66\u53d1\u73b0\u5468\u671f\uff0c\u5e76\u5728\u86cb\u767d\u8d28\u79d1\u5b66\u4e2d\u53d1\u73b0\u4e86\u4e24\u9879\u65b0\u73b0\u8c61\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u5927\u591a\u4ec5\u80fd\u590d\u73b0\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u77e5\u8bc6\uff0c\u7f3a\u4e4f\u81ea\u4e3b\u53d1\u73b0\u80fd\u529b\u3002Sparks\u65e8\u5728\u5b9e\u73b0\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u5b8c\u6574\u79d1\u5b66\u53d1\u73b0\u6d41\u7a0b\uff0c\u4ee5\u9a8c\u8bc1AI\u7684\u72ec\u7acb\u79d1\u7814\u6f5c\u529b\u3002", "method": "Sparks\u7ed3\u5408\u751f\u6210\u5f0f\u5e8f\u5217\u8bbe\u8ba1\u3001\u9ad8\u7cbe\u5ea6\u7ed3\u6784\u9884\u6d4b\u548c\u7269\u7406\u9a71\u52a8\u7684\u5c5e\u6027\u6a21\u578b\uff0c\u901a\u8fc7\u751f\u6210-\u53cd\u601d\u4ee3\u7406\u7684\u534f\u540c\u5b9e\u73b0\u81ea\u6211\u7ea0\u9519\u548c\u53ef\u91cd\u590d\u6027\u3002", "result": "\u53d1\u73b0\u4e86\u80bd\u673a\u68b0\u6027\u80fd\u7684\u957f\u5ea6\u4f9d\u8d56\u8f6c\u6298\uff08\u7ea680\u4e2a\u6b8b\u57fa\u540e\u03b2-\u6298\u53e0\u4f18\u4e8e\u03b1-\u87ba\u65cb\uff09\u548c\u4e8c\u7ea7\u7ed3\u6784\u7a33\u5b9a\u6027\u56fe\u8c31\u4e2d\u7684\u201c\u632b\u8d25\u533a\u201d\uff0c\u5747\u4e3a\u5168\u65b0\u7684\u79d1\u5b66\u539f\u7406\u3002", "conclusion": "Sparks\u8bc1\u660e\u4e86AI\u53ef\u72ec\u7acb\u5f00\u5c55\u4e25\u8c28\u79d1\u5b66\u7814\u7a76\u5e76\u63ed\u793a\u672a\u77e5\u79d1\u5b66\u89c4\u5f8b\uff0c\u4e3a\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u63d0\u4f9b\u4e86\u8303\u4f8b\u3002"}}
{"id": "2504.18593", "pdf": "https://arxiv.org/pdf/2504.18593", "abs": "https://arxiv.org/abs/2504.18593", "authors": ["Akram Shojaei", "Mehdi Delrobaei"], "title": "Severity Classification of Chronic Obstructive Pulmonary Disease in Intensive Care Units: A Semi-Supervised Approach Using MIMIC-III Dataset", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Chronic obstructive pulmonary disease (COPD) represents a significant global\nhealth burden, where precise severity assessment is particularly critical for\neffective clinical management in intensive care unit (ICU) settings. This study\nintroduces an innovative machine learning framework for COPD severity\nclassification utilizing the MIMIC-III critical care database, thereby\nexpanding the applications of artificial intelligence in critical care\nmedicine. Our research developed a robust classification model incorporating\nkey ICU parameters such as blood gas measurements and vital signs, while\nimplementing semi-supervised learning techniques to effectively utilize\nunlabeled data and enhance model performance. The random forest classifier\nemerged as particularly effective, demonstrating exceptional discriminative\ncapability with 92.51% accuracy and 0.98 ROC AUC in differentiating between\nmild-to-moderate and severe COPD cases. This machine learning approach provides\nclinicians with a practical, accurate, and efficient tool for rapid COPD\nseverity evaluation in ICU environments, with significant potential to improve\nboth clinical decision-making processes and patient outcomes. Future research\ndirections should prioritize external validation across diverse patient\npopulations and integration with clinical decision support systems to optimize\nCOPD management in critical care settings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684COPD\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u6a21\u578b\uff0c\u5229\u7528MIMIC-III\u6570\u636e\u5e93\u548c\u534a\u76d1\u7763\u5b66\u4e60\u6280\u672f\uff0c\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe92.51%\uff0cROC AUC\u4e3a0.98\uff0c\u4e3aICU\u4e2d\u7684COPD\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\u3002", "motivation": "COPD\u662f\u5168\u7403\u91cd\u5927\u5065\u5eb7\u8d1f\u62c5\uff0cICU\u4e2d\u7cbe\u786e\u8bc4\u4f30\u5176\u4e25\u91cd\u7a0b\u5ea6\u5bf9\u4e34\u5e8a\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4eba\u5de5\u667a\u80fd\u6280\u672f\u63d0\u5347\u8bc4\u4f30\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u5229\u7528MIMIC-III\u6570\u636e\u5e93\uff0c\u7ed3\u5408\u8840\u6db2\u6c14\u4f53\u68c0\u6d4b\u548c\u751f\u547d\u4f53\u5f81\u7b49\u5173\u952eICU\u53c2\u6570\uff0c\u91c7\u7528\u534a\u76d1\u7763\u5b66\u4e60\u6280\u672f\u5f00\u53d1\u5206\u7c7b\u6a21\u578b\u3002", "result": "\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u6548\u679c\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe92.51%\uff0cROC AUC\u4e3a0.98\uff0c\u80fd\u6709\u6548\u533a\u5206\u8f7b\u4e2d\u5ea6\u4e0e\u91cd\u5ea6COPD\u75c5\u4f8b\u3002", "conclusion": "\u8be5\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e3aICU\u4e2d\u7684COPD\u4e25\u91cd\u7a0b\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u5de5\u5177\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u5916\u90e8\u9a8c\u8bc1\u5e76\u4e0e\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u6574\u5408\u3002"}}
{"id": "2504.19019", "pdf": "https://arxiv.org/pdf/2504.19019", "abs": "https://arxiv.org/abs/2504.19019", "authors": ["Mohammad Akbar-Tajari", "Mohammad Taher Pilehvar", "Mohammad Mahmoody"], "title": "Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "19 pages, 1 figure, 6 tables", "summary": "The challenge of ensuring Large Language Models (LLMs) align with societal\nstandards is of increasing interest, as these models are still prone to\nadversarial jailbreaks that bypass their safety mechanisms. Identifying these\nvulnerabilities is crucial for enhancing the robustness of LLMs against such\nexploits. We propose Graph of ATtacks (GoAT), a method for generating\nadversarial prompts to test the robustness of LLM alignment using the Graph of\nThoughts framework [Besta et al., 2024]. GoAT excels at generating highly\neffective jailbreak prompts with fewer queries to the victim model than\nstate-of-the-art attacks, achieving up to five times better jailbreak success\nrate against robust models like Llama. Notably, GoAT creates high-quality,\nhuman-readable prompts without requiring access to the targeted model's\nparameters, making it a black-box attack. Unlike approaches constrained by\ntree-based reasoning, GoAT's reasoning is based on a more intricate graph\nstructure. By making simultaneous attack paths aware of each other's progress,\nthis dynamic framework allows a deeper integration and refinement of reasoning\npaths, significantly enhancing the collaborative exploration of adversarial\nvulnerabilities in LLMs. At a technical level, GoAT starts with a graph\nstructure and iteratively refines it by combining and improving thoughts,\nenabling synergy between different thought paths. The code for our\nimplementation can be found at: https://github.com/GoAT-pydev/Graph_of_Attacks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGoAT\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u751f\u6210\u5bf9\u6297\u6027\u63d0\u793a\u6765\u6d4b\u8bd5LLM\u7684\u5bf9\u9f50\u9c81\u68d2\u6027\uff0c\u6210\u529f\u7387\u9ad8\u4e14\u65e0\u9700\u8bbf\u95ee\u76ee\u6807\u6a21\u578b\u53c2\u6570\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ecd\u6613\u53d7\u5bf9\u6297\u6027\u8d8a\u72f1\u653b\u51fb\uff0c\u8bc6\u522b\u5176\u6f0f\u6d1e\u5bf9\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u56fe\u601d\u7ef4\u6846\u67b6\uff08Graph of Thoughts\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u56fe\u7ed3\u6784\u6574\u5408\u548c\u6539\u8fdb\u601d\u7ef4\u8def\u5f84\uff0c\u751f\u6210\u9ad8\u6548\u3001\u4eba\u7c7b\u53ef\u8bfb\u7684\u8d8a\u72f1\u63d0\u793a\u3002", "result": "GoAT\u5728\u5bf9\u6297\u6027\u5f3a\u6a21\u578b\uff08\u5982Llama\uff09\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u9ad8\u4e94\u500d\u7684\u8d8a\u72f1\u6210\u529f\u7387\uff0c\u4e14\u67e5\u8be2\u6b21\u6570\u66f4\u5c11\u3002", "conclusion": "GoAT\u901a\u8fc7\u56fe\u7ed3\u6784\u52a8\u6001\u534f\u4f5c\u63a2\u7d22\u548c\u4f18\u5316\u5bf9\u6297\u6027\u6f0f\u6d1e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9LLM\u5bf9\u9f50\u9c81\u68d2\u6027\u7684\u6d4b\u8bd5\u80fd\u529b\u3002"}}
{"id": "2504.19023", "pdf": "https://arxiv.org/pdf/2504.19023", "abs": "https://arxiv.org/abs/2504.19023", "authors": ["Justin M\u00fccke", "Ansgar Scherp"], "title": "GLaMoR: Consistency Checking of OWL Ontologies using Graph Language Models", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Semantic reasoning aims to infer new knowledge from existing knowledge, with\nOWL ontologies serving as a standardized framework for organizing information.\nA key challenge in semantic reasoning is verifying ontology consistency.\nHowever, state-of-the-art reasoners are computationally expensive, and their\nefficiency decreases as ontology sizes grow. While classical machine learning\nmodels have been explored for consistency checking, they struggle to capture\ncomplex relationships within ontologies. Large language models (LLMs) have\nshown promising results for simple reasoning tasks but perform poorly on\nstructured reasoning. The recently introduced Graph Language Model (GLM) offers\na way to simultaneously process graph-structured data and text. This paper\nproposes GLaMoR (Graph Language Model for Reasoning), a reasoning pipeline that\ntransforms OWL ontologies into graph-structured data and adapts the GLM\narchitecture for consistency checking. We evaluate GLaMoR on ontologies from\nthe NCBO BioPortal repository, converting them into triples suitable for model\ninput. Our results show that the GLM outperforms all baseline models, achieving\n$95\\%$ accuracy while being 20 times faster than classical reasoners.\n  The Code is accessible under: https://github.com/JustinMuecke/GLaMoR", "AI": {"tldr": "GLaMoR\uff08\u56fe\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff09\u662f\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u7ba1\u9053\uff0c\u5c06OWL\u672c\u4f53\u8f6c\u6362\u4e3a\u56fe\u7ed3\u6784\u6570\u636e\u5e76\u5229\u7528GLM\u67b6\u6784\u8fdb\u884c\u4e00\u81f4\u6027\u68c0\u67e5\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u8bed\u4e49\u63a8\u7406\u4e2d\u73b0\u6709\u63a8\u7406\u5668\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6548\u7387\u968f\u672c\u4f53\u89c4\u6a21\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u6355\u6349\u590d\u6742\u672c\u4f53\u5173\u7cfb\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u5316\u63a8\u7406\u4e2d\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faGLaMoR\u65b9\u6cd5\uff0c\u5c06OWL\u672c\u4f53\u8f6c\u6362\u4e3a\u56fe\u7ed3\u6784\u6570\u636e\uff0c\u5e76\u91c7\u7528GLM\u67b6\u6784\u8fdb\u884c\u4e00\u81f4\u6027\u68c0\u67e5\u3002\u8f93\u5165\u6570\u636e\u4e3a\u4eceBioPortal\u83b7\u53d6\u7684\u672c\u4f53\u4e09\u5143\u7ec4\u3002", "result": "GLaMoR\u5728BioPortal\u672c\u4f53\u4e0a\u8fbe\u5230\u4e8695%\u7684\u51c6\u786e\u7387\uff0c\u901f\u5ea6\u6bd4\u4f20\u7edf\u63a8\u7406\u5668\u5feb20\u500d\u3002", "conclusion": "GLM\u7ed3\u6784\u5728\u5904\u7406\u56fe\u6570\u636e\u4e0e\u6587\u672c\u7684\u8054\u5408\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0cGLaMoR\u4e3a\u9ad8\u6548\u672c\u4f53\u4e00\u81f4\u6027\u68c0\u67e5\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002\u7f16\u7a0b\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.18594", "pdf": "https://arxiv.org/pdf/2504.18594", "abs": "https://arxiv.org/abs/2504.18594", "authors": ["Tongrui Su", "Qingbin Li", "Shengyu Zhu", "Wei Chen", "Xueqi Cheng"], "title": "A Simple DropConnect Approach to Transfer-based Targeted Attack", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We study the problem of transfer-based black-box attack, where adversarial\nsamples generated using a single surrogate model are directly applied to target\nmodels. Compared with untargeted attacks, existing methods still have lower\nAttack Success Rates (ASRs) in the targeted setting, i.e., the obtained\nadversarial examples often overfit the surrogate model but fail to mislead\nother models. In this paper, we hypothesize that the pixels or features in\nthese adversarial examples collaborate in a highly dependent manner to maximize\nthe success of an adversarial attack on the surrogate model, which we refer to\nas perturbation co-adaptation. Then, we propose to Mitigate perturbation\nCo-adaptation by DropConnect (MCD) to enhance transferability, by creating\ndiverse variants of surrogate model at each optimization iteration. We conduct\nextensive experiments across various CNN- and Transformer-based models to\ndemonstrate the effectiveness of MCD. In the challenging scenario of\ntransferring from a CNN-based model to Transformer-based models, MCD achieves\n13% higher average ASRs compared with state-of-the-art baselines. MCD boosts\nthe performance of self-ensemble methods by bringing in more diversification\nacross the variants while reserving sufficient semantic information for each\nvariant. In addition, MCD attains the highest performance gain when scaling the\ncompute of crafting adversarial examples.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMCD\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11\u5bf9\u6297\u6837\u672c\u4e2d\u7684\u6270\u52a8\u5171\u9002\u5e94\u6027\u6765\u63d0\u5347\u5176\u53ef\u8f6c\u79fb\u6027\uff0c\u5728\u4eceCNN\u5230Transformer\u6a21\u578b\u7684\u653b\u51fb\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8fc1\u79fb\u7684\u9ed1\u76d2\u76ee\u6807\u653b\u51fb\u65b9\u6cd5\u7684\u653b\u51fb\u6210\u529f\u7387\u8f83\u4f4e\uff0c\u5bf9\u6297\u6837\u672c\u5bb9\u6613\u8fc7\u62df\u5408\u4ee3\u7406\u6a21\u578b\u800c\u65e0\u6cd5\u8bef\u5bfc\u5176\u4ed6\u6a21\u578b\u3002", "method": "\u63d0\u51faMCD\u65b9\u6cd5\uff0c\u901a\u8fc7DropConnect\u5728\u6bcf\u6b21\u4f18\u5316\u8fed\u4ee3\u4e2d\u751f\u6210\u591a\u6837\u5316\u7684\u4ee3\u7406\u6a21\u578b\u53d8\u4f53\uff0c\u4ee5\u51cf\u5c11\u6270\u52a8\u5171\u9002\u5e94\u6027\u3002", "result": "MCD\u5728\u4eceCNN\u5230Transformer\u6a21\u578b\u7684\u653b\u51fb\u4e2d\u5e73\u5747\u653b\u51fb\u6210\u529f\u7387\u63d0\u5347\u4e8613%\uff0c\u4e14\u5728\u9ad8\u8ba1\u7b97\u8d44\u6e90\u4e0b\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "MCD\u901a\u8fc7\u591a\u6837\u5316\u4ee3\u7406\u6a21\u578b\u53d8\u4f53\u6709\u6548\u63d0\u5347\u4e86\u5bf9\u6297\u6837\u672c\u7684\u53ef\u8f6c\u79fb\u6027\uff0c\u5c24\u5176\u5728\u590d\u6742\u6a21\u578b\u95f4\u8fc1\u79fb\u65f6\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2504.19021", "pdf": "https://arxiv.org/pdf/2504.19021", "abs": "https://arxiv.org/abs/2504.19021", "authors": ["Zhyar Rzgar K Rostam", "G\u00e1bor Kert\u00e9sz"], "title": "Advancing Scientific Text Classification: Fine-Tuned Models with Dataset Expansion and Hard-Voting", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 1 figure, 8 tables", "summary": "Efficient text classification is essential for handling the increasing volume\nof academic publications. This study explores the use of pre-trained language\nmodels (PLMs), including BERT, SciBERT, BioBERT, and BlueBERT, fine-tuned on\nthe Web of Science (WoS-46985) dataset for scientific text classification. To\nenhance performance, we augment the dataset by executing seven targeted queries\nin the WoS database, retrieving 1,000 articles per category aligned with\nWoS-46985's main classes. PLMs predict labels for this unlabeled data, and a\nhard-voting strategy combines predictions for improved accuracy and confidence.\nFine-tuning on the expanded dataset with dynamic learning rates and early\nstopping significantly boosts classification accuracy, especially in\nspecialized domains. Domain-specific models like SciBERT and BioBERT\nconsistently outperform general-purpose models such as BERT. These findings\nunderscore the efficacy of dataset augmentation, inference-driven label\nprediction, hard-voting, and fine-tuning techniques in creating robust and\nscalable solutions for automated academic text classification.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08\u5982BERT\u3001SciBERT\u7b49\uff09\u5728\u79d1\u5b66\u6587\u672c\u5206\u7c7b\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u6570\u636e\u96c6\u589e\u5f3a\u548c\u7cbe\u7ec6\u8c03\u4f18\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u5c24\u5176\u6307\u51fa\u9886\u57df\u7279\u5b9a\u6a21\u578b\u4f18\u4e8e\u901a\u7528\u6a21\u578b\u3002", "motivation": "\u968f\u7740\u5b66\u672f\u51fa\u7248\u7269\u6570\u91cf\u7684\u589e\u52a0\uff0c\u9ad8\u6548\u7684\u6587\u672c\u5206\u7c7b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u65e8\u5728\u4f18\u5316\u79d1\u5b66\u6587\u672c\u5206\u7c7b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728WoS-46985\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u901a\u8fc7\u6570\u636e\u96c6\u589e\u5f3a\u3001\u786c\u6295\u7968\u7b56\u7565\u548c\u52a8\u6001\u5b66\u4e60\u7387\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002", "result": "\u9886\u57df\u7279\u5b9a\u6a21\u578b\uff08\u5982SciBERT\u3001BioBERT\uff09\u8868\u73b0\u4f18\u4e8e\u901a\u7528\u6a21\u578b\uff0c\u6570\u636e\u589e\u5f3a\u548c\u7cbe\u7ec6\u8c03\u4f18\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\u6570\u636e\u96c6\u589e\u5f3a\u3001\u6807\u7b7e\u9884\u6d4b\u548c\u7cbe\u7ec6\u8c03\u4f18\u662f\u6784\u5efa\u9ad8\u6548\u5b66\u672f\u6587\u672c\u5206\u7c7b\u7cfb\u7edf\u7684\u5173\u952e\u7b56\u7565\u3002"}}
{"id": "2504.19027", "pdf": "https://arxiv.org/pdf/2504.19027", "abs": "https://arxiv.org/abs/2504.19027", "authors": ["Volkan Bakir", "Polat Goktas", "Sureyya Akyuz"], "title": "DiCE-Extended: A Robust Approach to Counterfactual Explanations in Machine Learning", "categories": ["cs.AI", "cs.LG", "cs.NE", "I.2; K.4; H.4"], "comment": "MCO 2025, 5th International Conference on Modelling, Computation and\n  Optimization in Information Systems and Management Sciences", "summary": "Explainable artificial intelligence (XAI) has become increasingly important\nin decision-critical domains such as healthcare, finance, and law.\nCounterfactual (CF) explanations, a key approach in XAI, provide users with\nactionable insights by suggesting minimal modifications to input features that\nlead to different model outcomes. Despite significant advancements, existing CF\ngeneration methods often struggle to balance proximity, diversity, and\nrobustness, limiting their real-world applicability. A widely adopted\nframework, Diverse Counterfactual Explanations (DiCE), emphasizes diversity but\nlacks robustness, making CF explanations sensitive to perturbations and domain\nconstraints. To address these challenges, we introduce DiCE-Extended, an\nenhanced CF explanation framework that integrates multi-objective optimization\ntechniques to improve robustness while maintaining interpretability. Our\napproach introduces a novel robustness metric based on the Dice-Sorensen\ncoefficient, ensuring stability under small input variations. Additionally, we\nrefine CF generation using weighted loss components (lambda_p, lambda_d,\nlambda_r) to balance proximity, diversity, and robustness. We empirically\nvalidate DiCE-Extended on benchmark datasets (COMPAS, Lending Club, German\nCredit, Adult Income) across multiple ML backends (Scikit-learn, PyTorch,\nTensorFlow). Results demonstrate improved CF validity, stability, and alignment\nwith decision boundaries compared to standard DiCE-generated explanations. Our\nfindings highlight the potential of DiCE-Extended in generating more reliable\nand interpretable CFs for high-stakes applications. Future work will explore\nadaptive optimization techniques and domain-specific constraints to further\nenhance CF generation in real-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u7684DiCE-Extended\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u548c\u65b0\u7684\u7a33\u5065\u6027\u6307\u6807\u6539\u8fdb\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u5e73\u8861\u63a5\u8fd1\u6027\u3001\u591a\u6837\u6027\u548c\u7a33\u5065\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u6570\u636e\u96c6\u548c\u673a\u5668\u5b66\u4e60\u540e\u7aef\u4e0a\u7684\u6709\u6548\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\uff08\u5982DiCE\uff09\u5728\u5e73\u8861\u63a5\u8fd1\u6027\u3001\u591a\u6837\u6027\u548c\u7a33\u5065\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5728\u51b3\u7b56\u5173\u952e\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e86DiCE-Extended\u3002", "method": "\u5f15\u5165\u57fa\u4e8eDice-Sorensen\u7cfb\u6570\u7684\u65b0\u7a33\u5065\u6027\u6307\u6807\uff0c\u4f7f\u7528\u591a\u76ee\u6807\u4f18\u5316\u6280\u672f\uff08\u52a0\u6743\u635f\u5931\u51fd\u6570\u03bb_p\u3001\u03bb_d\u3001\u03bb_r\uff09\u751f\u6210\u53cd\u4e8b\u5b9e\u89e3\u91ca\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08COMPAS\u7b49\uff09\u548cML\u540e\u7aef\uff08Scikit-learn\u7b49\uff09\u4e0a\u9a8c\u8bc1\uff0cDiCE-Extended\u5728\u6709\u6548\u6027\u3001\u7a33\u5b9a\u6027\u548c\u51b3\u7b56\u8fb9\u754c\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8e\u6807\u51c6DiCE\u3002", "conclusion": "DiCE-Extended\u80fd\u751f\u6210\u66f4\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u672a\u6765\u5c06\u63a2\u7d22\u81ea\u9002\u5e94\u4f18\u5316\u548c\u9886\u57df\u7279\u5b9a\u7ea6\u675f\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u751f\u6210\u6548\u679c\u3002"}}
{"id": "2504.18595", "pdf": "https://arxiv.org/pdf/2504.18595", "abs": "https://arxiv.org/abs/2504.18595", "authors": ["Uzma", "Fabien Cholet", "Domenic Quinn", "Cindy Smith", "Siming You", "William Sloan"], "title": "EnviroPiNet: A Physics-Guided AI Model for Predicting Biofilter Performance", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Environmental biotechnologies, such as drinking water biofilters, rely on\ncomplex interactions between microbial communities and their surrounding\nphysical-chemical environments. Predicting the performance of these systems is\nchallenging due to high-dimensional, sparse datasets that lack diversity and\nfail to fully capture system behaviour. Accurate predictive models require\ninnovative, science-guided approaches. In this study, we present the first\napplication of Buckingham Pi theory to modelling biofilter performance. This\ndimensionality reduction technique identifies meaningful, dimensionless\nvariables that enhance predictive accuracy and improve model interpretability.\nUsing these variables, we developed the Environmental Buckingham Pi Neural\nNetwork (EnviroPiNet), a physics-guided model benchmarked against traditional\ndata-driven methods, including Principal Component Analysis (PCA) and\nautoencoder neural networks. Our findings demonstrate that the EnviroPiNet\nmodel achieves an R^2 value of 0.9236 on the testing dataset, significantly\noutperforming PCA and autoencoder methods. The Buckingham Pi variables also\nprovide insights into the physical and chemical relationships governing\nbiofilter behaviour, with implications for system design and optimization. This\nstudy highlights the potential of combining physical principles with AI\napproaches to model complex environmental systems characterized by sparse,\nhigh-dimensional datasets.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5c06\u767d\u91d1\u6c49\u03c0\u7406\u8bba\u5e94\u7528\u4e8e\u751f\u7269\u6ee4\u6c60\u6027\u80fd\u5efa\u6a21\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u5f15\u5bfc\u7684\u6a21\u578bEnviroPiNet\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u7269\u7406\u539f\u7406\u4e0eAI\u7ed3\u5408\u5728\u73af\u5883\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u7531\u4e8e\u73af\u5883\u751f\u7269\u6280\u672f\u4e2d\u5fae\u751f\u7269\u7fa4\u843d\u4e0e\u7269\u7406\u5316\u5b66\u73af\u5883\u95f4\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u7684\u9ad8\u7ef4\u7a00\u758f\u6570\u636e\u96be\u4ee5\u6355\u6349\u7cfb\u7edf\u884c\u4e3a\uff0c\u9700\u521b\u65b0\u79d1\u5b66\u5f15\u5bfc\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u3002", "method": "\u91c7\u7528\u767d\u91d1\u6c49\u03c0\u7406\u8bba\u8fdb\u884c\u964d\u7ef4\uff0c\u63d0\u53d6\u6709\u610f\u4e49\u7684\u65e0\u91cf\u7eb2\u53d8\u91cf\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u7269\u7406\u5f15\u5bfc\u6a21\u578bEnviroPiNet\uff0c\u4e0ePCA\u548c\u81ea\u7f16\u7801\u5668\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "EnviroPiNet\u5728\u6d4b\u8bd5\u96c6\u4e0aR\u00b2\u503c\u8fbe0.9236\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u540c\u65f6\u767d\u91d1\u6c49\u03c0\u53d8\u91cf\u63ed\u793a\u4e86\u751f\u7269\u6ee4\u6c60\u884c\u4e3a\u7684\u7269\u7406\u5316\u5b66\u5173\u7cfb\u3002", "conclusion": "\u7ed3\u5408\u7269\u7406\u539f\u7406\u4e0eAI\u65b9\u6cd5\u80fd\u6709\u6548\u5efa\u6a21\u9ad8\u7ef4\u7a00\u758f\u6570\u636e\u7684\u73af\u5883\u7cfb\u7edf\uff0c\u4e3a\u7cfb\u7edf\u8bbe\u8ba1\u4e0e\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.19024", "pdf": "https://arxiv.org/pdf/2504.19024", "abs": "https://arxiv.org/abs/2504.19024", "authors": ["Jiabin Fan", "Guoqing Luo", "Michael Bowling", "Lili Mou"], "title": "KETCHUP: K-Step Return Estimation for Sequential Knowledge Distillation", "categories": ["cs.CL"], "comment": null, "summary": "We propose a novel k-step return estimation method (called KETCHUP) for\nReinforcement Learning(RL)-based knowledge distillation (KD) in text generation\ntasks. Our idea is to induce a K-step return by using the Bellman Optimality\nEquation for multiple steps. Theoretical analysis shows that this K-step\nformulation reduces the variance of the gradient estimates, thus leading to\nimproved RL optimization especially when the student model size is large.\nEmpirical evaluation on three text generation tasks demonstrates that our\napproach yields superior performance in both standard task metrics and large\nlanguage model (LLM)-based evaluation. These results suggest that our K-step\nreturn induction offers a promising direction for enhancing RL-based KD in LLM\nresearch.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u540d\u4e3aKETCHUP\u7684K\u6b65\u56de\u62a5\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u77e5\u8bc6\u84b8\u998f\uff0c\u901a\u8fc7Bellman\u6700\u4f18\u65b9\u7a0b\u591a\u6b65\u4f30\u8ba1\u964d\u4f4e\u68af\u5ea6\u65b9\u5dee\uff0c\u63d0\u5347\u4f18\u5316\u6548\u679c\u3002", "motivation": "\u9488\u5bf9\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u77e5\u8bc6\u84b8\u998f\u5728\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u4f18\u5316\u6311\u6218\uff0c\u7279\u522b\u662f\u5b66\u751f\u6a21\u578b\u8f83\u5927\u65f6\u7684\u68af\u5ea6\u65b9\u5dee\u95ee\u9898\u3002", "method": "\u5229\u7528Bellman\u6700\u4f18\u65b9\u7a0b\u8fdb\u884cK\u6b65\u56de\u62a5\u4f30\u8ba1\uff0c\u7406\u8bba\u5206\u6790\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u51cf\u5c11\u68af\u5ea6\u4f30\u8ba1\u7684\u65b9\u5dee\u3002", "result": "\u5728\u4e09\u4e2a\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\uff0cKETCHUP\u5728\u6807\u51c6\u6307\u6807\u548c\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "K\u6b65\u56de\u62a5\u4f30\u8ba1\u4e3a\u5f3a\u5316\u5b66\u4e60\u77e5\u8bc6\u84b8\u998f\u5728\u5927\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u4e2d\u7684\u6539\u8fdb\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2504.19144", "pdf": "https://arxiv.org/pdf/2504.19144", "abs": "https://arxiv.org/abs/2504.19144", "authors": ["Bowei Wang", "Jiaran Gao", "Yelai Feng", "Renzhi Chen", "Shanshan Li", "Lei Wang"], "title": "ChiseLLM: Unleashing the Power of Reasoning LLMs for Chisel Agile Hardware Development", "categories": ["cs.AI", "cs.AR", "cs.SE"], "comment": null, "summary": "The growing demand for Domain-Specific Architecture (DSA) has driven the\ndevelopment of Agile Hardware Development Methodology (AHDM). Hardware\nConstruction Language (HCL) like Chisel offers high-level abstraction features,\nmaking it an ideal language for HCL-Based AHDM. While Large Language Models\n(LLMs) excel in code generation tasks, they still face challenges with Chisel\ngeneration, particularly regarding syntax correctness and design variability.\nRecent reasoning models have significantly enhanced code generation\ncapabilities through test-time scaling techniques. However, we found that\nreasoning models without domain adaptation cannot bring substantial benefits to\nChisel code generation tasks. This paper presents ChiseLLM, a solution\ncomprising data processing and transformation, prompt-guided reasoning trace\nsynthesis, and domain-adapted model training. We constructed high-quality\ndatasets from public RTL code resources and guided the model to adopt\nstructured thinking patterns through prompt enhancement methods. Experiments\ndemonstrate that our ChiseLLM-7B and ChiseLLM-32B models improved syntax\ncorrectness by 18.85% and 26.32% respectively over base models, while\nincreasing variability design ability by 47.58% compared to baseline reasoning\nmodels. Our datasets and models are publicly available, providing\nhigh-performance, cost-effective models for HCL-Based AHDM, and offering an\neffective baseline for future research. Github repository:\nhttps://github.com/observerw/ChiseLLM", "AI": {"tldr": "\u672c\u6587\u63d0\u51faChiseLLM\uff0c\u901a\u8fc7\u6570\u636e\u5904\u7406\u3001\u63d0\u793a\u5f15\u5bfc\u63a8\u7406\u548c\u9886\u57df\u81ea\u9002\u5e94\u6a21\u578b\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347Chisel\u4ee3\u7801\u751f\u6210\u7684\u8bed\u6cd5\u6b63\u786e\u6027\u548c\u8bbe\u8ba1\u591a\u6837\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728Chisel\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u9762\u4e34\u8bed\u6cd5\u6b63\u786e\u6027\u548c\u8bbe\u8ba1\u591a\u6837\u6027\u7684\u6311\u6218\uff0c\u4e14\u672a\u7ecf\u8fc7\u9886\u57df\u9002\u5e94\u7684\u63a8\u7406\u6a21\u578b\u6548\u679c\u6709\u9650\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86ChiseLLM\u89e3\u51b3\u65b9\u6848\u3002", "method": "ChiseLLM\u5305\u62ec\u6570\u636e\u5904\u7406\u4e0e\u8f6c\u6362\u3001\u63d0\u793a\u5f15\u5bfc\u7684\u63a8\u7406\u8f68\u8ff9\u5408\u6210\u548c\u9886\u57df\u81ea\u9002\u5e94\u6a21\u578b\u8bad\u7ec3\uff0c\u5e76\u4f7f\u7528\u516c\u5f00RTL\u4ee3\u7801\u8d44\u6e90\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cChiseLLM-7B\u548cChiseLLM-32B\u5728\u8bed\u6cd5\u6b63\u786e\u6027\u4e0a\u5206\u522b\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u534718.85%\u548c26.32%\uff0c\u8bbe\u8ba1\u591a\u6837\u6027\u80fd\u529b\u63d0\u534747.58%\u3002", "conclusion": "ChiseLLM\u4e3aHCL-Based AHDM\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u3001\u4f4e\u6210\u672c\u7684\u6a21\u578b\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u8bbe\u5b9a\u4e86\u6709\u6548\u57fa\u7ebf\u3002\u6570\u636e\u96c6\u548c\u6a21\u578b\u5df2\u516c\u5f00\u3002"}}
{"id": "2504.18599", "pdf": "https://arxiv.org/pdf/2504.18599", "abs": "https://arxiv.org/abs/2504.18599", "authors": ["Subhadip Bandyopadhyay", "Joy Bose", "Sujoy Roy Chowdhury"], "title": "A Hybrid Framework for Real-Time Data Drift and Anomaly Identification Using Hierarchical Temporal Memory and Statistical Tests", "categories": ["cs.LG", "62M10, 62P30, 68T07", "G.3; I.2.6; I.2.7; H.2.8; H.3.3"], "comment": "26 pages, 9 figures", "summary": "Data Drift is the phenomenon where the generating model behind the data\nchanges over time. Due to data drift, any model built on the past training data\nbecomes less relevant and inaccurate over time. Thus, detecting and controlling\nfor data drift is critical in machine learning models. Hierarchical Temporal\nMemory (HTM) is a machine learning model developed by Jeff Hawkins, inspired by\nhow the human brain processes information. It is a biologically inspired model\nof memory that is similar in structure to the neocortex, and whose performance\nis claimed to be comparable to state of the art models in detecting anomalies\nin time series data. Another unique benefit of HTMs is its independence from\ntraining and testing cycle; all the learning takes place online with streaming\ndata and no separate training and testing cycle is required. In sequential\nlearning paradigm, Sequential Probability Ratio Test (SPRT) offers some unique\nbenefit for online learning and inference. This paper proposes a novel hybrid\nframework combining HTM and SPRT for real-time data drift detection and anomaly\nidentification. Unlike existing data drift methods, our approach eliminates\nfrequent retraining and ensures low false positive rates. HTMs currently work\nwith one dimensional or univariate data. In a second study, we also propose an\napplication of HTM in multidimensional supervised scenario for anomaly\ndetection by combining the outputs of multiple HTM columns, one for each\ndimension of the data, through a neural network. Experimental evaluations\ndemonstrate that the proposed method outperforms conventional drift detection\ntechniques like the Kolmogorov-Smirnov (KS) test, Wasserstein distance, and\nPopulation Stability Index (PSI) in terms of accuracy, adaptability, and\ncomputational efficiency. Our experiments also provide insights into optimizing\nhyperparameters for real-time deployment in domains such as Telecom.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408HTM\u548cSPRT\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9e\u65f6\u6570\u636e\u6f02\u79fb\u68c0\u6d4b\u548c\u5f02\u5e38\u8bc6\u522b\uff0c\u65e0\u9700\u9891\u7e41\u91cd\u8bad\u7ec3\u4e14\u8bef\u62a5\u7387\u4f4e\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u6570\u636e\u6f02\u79fb\u4f1a\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709\u7684\u6570\u636e\u6f02\u79fb\u68c0\u6d4b\u65b9\u6cd5\u9700\u8981\u9891\u7e41\u91cd\u8bad\u7ec3\u4e14\u8bef\u62a5\u7387\u9ad8\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u548c\u51c6\u786e\u7684\u5b9e\u65f6\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528HTM\uff08\u5206\u5c42\u65f6\u5e8f\u8bb0\u5fc6\uff09\u548cSPRT\uff08\u5e8f\u5217\u6982\u7387\u6bd4\u68c0\u9a8c\uff09\u7ed3\u5408\u7684\u6df7\u5408\u6846\u67b6\uff0c\u5229\u7528HTM\u7684\u5728\u7ebf\u5b66\u4e60\u80fd\u529b\u548cSPRT\u7684\u7edf\u8ba1\u4f18\u52bf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u9002\u5e94\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7684KS\u68c0\u9a8c\u3001Wasserstein\u8ddd\u79bb\u548cPSI\u7b49\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5b9e\u65f6\u6570\u636e\u6f02\u79fb\u68c0\u6d4b\u548c\u5f02\u5e38\u8bc6\u522b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u7535\u4fe1\u7b49\u9886\u57df\uff0c\u5e76\u63d0\u4f9b\u4e86\u8d85\u53c2\u6570\u4f18\u5316\u7684\u89c1\u89e3\u3002"}}
{"id": "2504.19044", "pdf": "https://arxiv.org/pdf/2504.19044", "abs": "https://arxiv.org/abs/2504.19044", "authors": ["Di Wu", "Yibin Lei", "Christof Monz"], "title": "Calibrating Translation Decoding with Quality Estimation on LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Neural machine translation (NMT) systems typically employ maximum a\nposteriori (MAP) decoding to select the highest-scoring translation from the\ndistribution mass. However, recent evidence highlights the inadequacy of MAP\ndecoding, often resulting in low-quality or even pathological hypotheses -- the\ndecoding objective is not aligned with real-world translation quality. This\npaper proposes calibrating hypothesis likelihoods with translation quality from\na distribution view by directly optimizing their Pearson correlation -- thereby\nenhancing the effectiveness of translation decoding. With our method,\ntranslation on large language models (LLMs) improves substantially after\nlimited training (2K instances per direction). This improvement is orthogonal\nto those achieved through supervised fine-tuning, leading to substantial gains\nacross a broad range of metrics and human evaluations -- even when applied to\ntop-performing translation-specialized LLMs fine-tuned on high-quality\ntranslation data, such as Tower, or when compared to recent preference\noptimization methods, like CPO. Moreover, the calibrated translation likelihood\ncan directly serve as a strong proxy for translation quality, closely\napproximating or even surpassing some state-of-the-art translation quality\nestimation models, like CometKiwi. Lastly, our in-depth analysis demonstrates\nthat calibration enhances the effectiveness of MAP decoding, thereby enabling\ngreater efficiency in real-world deployment. The resulting state-of-the-art\ntranslation model, which covers 10 languages, along with the accompanying code\nand human evaluation data, has been released to the community:\nhttps://github.com/moore3930/calibrating-llm-mt.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u4f18\u5316\u5047\u8bbe\u4f3c\u7136\u4e0e\u7ffb\u8bd1\u8d28\u91cf\u7684\u76ae\u5c14\u900a\u76f8\u5173\u6027\u6765\u6821\u51c6\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u7684\u89e3\u7801\u6548\u679c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5e76\u53ef\u4f5c\u4e3a\u7ffb\u8bd1\u8d28\u91cf\u4f30\u8ba1\u7684\u5f3a\u4ee3\u7406\u3002", "motivation": "\u73b0\u6709\u6700\u5927\u540e\u9a8c\u6982\u7387\uff08MAP\uff09\u89e3\u7801\u65b9\u6cd5\u5728\u771f\u5b9e\u7ffb\u8bd1\u8d28\u91cf\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u89e3\u7801\u6821\u51c6\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u76f4\u63a5\u4f18\u5316\u5047\u8bbe\u4f3c\u7136\u4e0e\u7ffb\u8bd1\u8d28\u91cf\u7684\u76ae\u5c14\u900a\u76f8\u5173\u6027\u6765\u6821\u51c6\u89e3\u7801\u6548\u679c\uff0c\u5e76\u7ed3\u5408\u6709\u9650\u8bad\u7ec3\u6570\u636e\uff08\u6bcf\u4e2a\u65b9\u54112K\u5b9e\u4f8b\uff09\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u3002", "result": "\u5728\u6821\u51c6\u540e\uff0c\u7ffb\u8bd1\u8d28\u91cf\u5728\u591a\u79cd\u6307\u6807\u548c\u4eba\u5de5\u8bc4\u4f30\u4e2d\u5927\u5e45\u63d0\u5347\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u4e00\u4e9b\u5148\u8fdb\u7684\u7ffb\u8bd1\u8d28\u91cf\u4f30\u8ba1\u6a21\u578b\u3002", "conclusion": "\u6821\u51c6\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86MAP\u89e3\u7801\u7684\u6548\u7387\u548c\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.19148", "pdf": "https://arxiv.org/pdf/2504.19148", "abs": "https://arxiv.org/abs/2504.19148", "authors": ["Ke Liu", "Jing Ma", "Edmund M-K Lai"], "title": "A Dynamic Fuzzy Rule and Attribute Management Framework for Fuzzy Inference Systems in High-Dimensional Data", "categories": ["cs.AI"], "comment": null, "summary": "This paper presents an Adaptive Dynamic Attribute and Rule (ADAR) framework\ndesigned to address the challenges posed by high-dimensional data in\nneuro-fuzzy inference systems. By integrating dual weighting\nmechanisms-assigning adaptive importance to both attributes and rules-together\nwith automated growth and pruning strategies, ADAR adaptively streamlines\ncomplex fuzzy models without sacrificing performance or interpretability.\nExperimental evaluations on four diverse datasets - Auto MPG (7 variables),\nBeijing PM2.5 (10 variables), Boston Housing (13 variables), and Appliances\nEnergy Consumption (27 variables) show that ADAR-based models achieve\nconsistently lower Root Mean Square Error (RMSE) compared to state-of-the-art\nbaselines. On the Beijing PM2.5 dataset, for instance, ADAR-SOFENN attained an\nRMSE of 56.87 with nine rules, surpassing traditional ANFIS [12] and SOFENN\n[16] models. Similarly, on the high-dimensional Appliances Energy dataset,\nADAR-ANFIS reached an RMSE of 83.25 with nine rules, outperforming established\nfuzzy logic approaches and interpretability-focused methods such as APLR.\nAblation studies further reveal that combining rule-level and attribute-level\nweight assignment significantly reduces model overlap while preserving\nessential features, thereby enhancing explainability. These results highlight\nADAR's effectiveness in dynamically balancing rule complexity and feature\nimportance, paving the way for scalable, high-accuracy, and transparent\nneuro-fuzzy systems applicable to a range of real-world scenarios.", "AI": {"tldr": "ADAR\u6846\u67b6\u901a\u8fc7\u53cc\u91cd\u6743\u91cd\u673a\u5236\u548c\u81ea\u52a8\u8c03\u6574\u7b56\u7565\u4f18\u5316\u9ad8\u7ef4\u6570\u636e\u7684\u795e\u7ecf\u6a21\u7cca\u63a8\u7406\u7cfb\u7edf\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u9ad8\u7ef4\u6570\u636e\u5728\u795e\u7ecf\u6a21\u7cca\u63a8\u7406\u7cfb\u7edf\u4e2d\u5e26\u6765\u6311\u6218\uff0cADAR\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5c5e\u6027\u548c\u89c4\u5219\u6743\u91cd\u6765\u7b80\u5316\u6a21\u578b\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u7ed3\u5408\u53cc\u91cd\u6743\u91cd\u673a\u5236\uff08\u5c5e\u6027\u548c\u89c4\u5219\u81ea\u9002\u5e94\u52a0\u6743\uff09\u53ca\u81ea\u52a8\u589e\u51cf\u7b56\u7565\uff0c\u52a8\u6001\u4f18\u5316\u6a21\u7cca\u6a21\u578b\u7ed3\u6784\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cADAR\u7684RMSE\u5747\u4f18\u4e8e\u57fa\u7ebf\uff08\u5982\u5317\u4eacPM2.5\u6570\u636e\u96c6RMSE 56.87\uff09\u3002\u89c4\u5219\u4e0e\u5c5e\u6027\u6743\u91cd\u7684\u7ed3\u5408\u663e\u8457\u964d\u4f4e\u5197\u4f59\u5e76\u63d0\u5347\u89e3\u91ca\u6027\u3002", "conclusion": "ADAR\u80fd\u52a8\u6001\u5e73\u8861\u89c4\u5219\u590d\u6742\u5ea6\u548c\u7279\u5f81\u91cd\u8981\u6027\uff0c\u4e3a\u53ef\u6269\u5c55\u3001\u9ad8\u7cbe\u5ea6\u4e14\u900f\u660e\u7684\u795e\u7ecf\u6a21\u7cca\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2504.18650", "pdf": "https://arxiv.org/pdf/2504.18650", "abs": "https://arxiv.org/abs/2504.18650", "authors": ["Bruce Collins"], "title": "Unsupervised outlier detection to improve bird audio dataset labels", "categories": ["cs.LG", "cs.SD", "eess.AS"], "comment": "27 pages, 9 figures", "summary": "The Xeno-Canto bird audio repository is an invaluable resource for those\ninterested in vocalizations and other sounds made by birds around the world.\nThis is particularly the case for machine learning researchers attempting to\nimprove on the bird species recognition accuracy of classification models.\nHowever, the task of extracting labeled datasets from the recordings found in\nthis crowd-sourced repository faces several challenges. One challenge of\nparticular significance to machine learning practitioners is that one bird\nspecies label is applied to each audio recording, but frequently other sounds\nare also captured including other bird species, other animal sounds,\nanthropogenic and other ambient sounds. These non-target bird species sounds\ncan result in dataset labeling discrepancies referred to as label noise. In\nthis work we present a cleaning process consisting of audio preprocessing\nfollowed by dimensionality reduction and unsupervised outlier detection (UOD)\nto reduce the label noise in a dataset derived from Xeno-Canto recordings. We\ninvestigate three neural network dimensionality reduction techniques: two\nflavors of convolutional autoencoders and variational deep embedding (VaDE\n(Jiang, 2017)). While both methods show some degree of effectiveness at\ndetecting outliers for most bird species datasets, we found significant\nvariation in the performance of the methods from one species to the next. We\nbelieve that the results of this investigation demonstrate that the application\nof our cleaning process can meaningfully reduce the label noise of bird species\ndatasets derived from Xeno-Canto audio repository but results vary across\nspecies.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4eceXeno-Canto\u9e1f\u9e23\u97f3\u9891\u5e93\u4e2d\u63d0\u53d6\u5e72\u51c0\u6570\u636e\u96c6\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u97f3\u9891\u9884\u5904\u7406\u3001\u964d\u7ef4\u548c\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u7684\u65b9\u6cd5\u6765\u51cf\u5c11\u6807\u7b7e\u566a\u58f0\u3002\u5c3d\u7ba1\u4e0d\u540c\u964d\u7ef4\u6280\u672f\u5728\u7269\u79cd\u95f4\u8868\u73b0\u4e0d\u4e00\uff0c\u4f46\u6574\u4f53\u4e0a\u80fd\u6709\u6548\u964d\u4f4e\u566a\u58f0\u3002", "motivation": "Xeno-Canto\u97f3\u9891\u5e93\u4e3a\u9e1f\u7c7b\u7269\u79cd\u8bc6\u522b\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u4f46\u6bcf\u6bb5\u5f55\u97f3\u4ec5\u6807\u6ce8\u4e00\u4e2a\u7269\u79cd\uff0c\u5e38\u6df7\u6742\u5176\u4ed6\u58f0\u97f3\uff08\u5982\u5176\u4ed6\u9e1f\u7c7b\u3001\u73af\u5883\u566a\u58f0\uff09\uff0c\u5bfc\u81f4\u6807\u7b7e\u566a\u58f0\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6e05\u7406\u6d41\u7a0b\uff1a\u97f3\u9891\u9884\u5904\u7406\u2192\u964d\u7ef4\uff08\u91c7\u7528\u4e24\u79cd\u5377\u79ef\u81ea\u7f16\u7801\u5668\u548cVaDE\uff09\u2192\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\uff08UOD\uff09\u3002\u901a\u8fc7\u6bd4\u8f83\u4e09\u79cd\u964d\u7ef4\u6280\u672f\u8bc4\u4f30\u6548\u679c\u3002", "result": "\u4e24\u79cd\u81ea\u7f16\u7801\u5668\u548cVaDE\u5728\u4e0d\u540c\u7269\u79cd\u6570\u636e\u96c6\u4e0a\u5747\u80fd\u68c0\u6d4b\u5f02\u5e38\uff0c\u4f46\u6548\u679c\u56e0\u7269\u79cd\u800c\u5f02\u3002\u6e05\u7406\u6d41\u7a0b\u603b\u4f53\u4e0a\u663e\u8457\u964d\u4f4e\u4e86\u6807\u7b7e\u566a\u58f0\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11Xeno-Canto\u6570\u636e\u96c6\u7684\u6807\u7b7e\u566a\u58f0\uff0c\u4f46\u56e0\u7269\u79cd\u5dee\u5f02\u9700\u9488\u5bf9\u6027\u4f18\u5316\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2504.19061", "pdf": "https://arxiv.org/pdf/2504.19061", "abs": "https://arxiv.org/abs/2504.19061", "authors": ["Anindya Bijoy Das", "Shibbir Ahmed", "Shahnewaz Karim Sakib"], "title": "Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Clinical summarization is crucial in healthcare as it distills complex\nmedical data into digestible information, enhancing patient understanding and\ncare management. Large language models (LLMs) have shown significant potential\nin automating and improving the accuracy of such summarizations due to their\nadvanced natural language understanding capabilities. These models are\nparticularly applicable in the context of summarizing medical/clinical texts,\nwhere precise and concise information transfer is essential. In this paper, we\ninvestigate the effectiveness of open-source LLMs in extracting key events from\ndischarge reports, such as reasons for hospital admission, significant\nin-hospital events, and critical follow-up actions. In addition, we also assess\nthe prevalence of various types of hallucinations in the summaries produced by\nthese models. Detecting hallucinations is vital as it directly influences the\nreliability of the information, potentially affecting patient care and\ntreatment outcomes. We conduct comprehensive numerical simulations to\nrigorously evaluate the performance of these models, further probing the\naccuracy and fidelity of the extracted content in clinical summarization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e34\u5e8a\u603b\u7ed3\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u5173\u6ce8\u4ece\u51fa\u9662\u62a5\u544a\u4e2d\u63d0\u53d6\u5173\u952e\u4e8b\u4ef6\uff08\u5982\u5165\u9662\u539f\u56e0\u3001\u9662\u5185\u91cd\u5927\u4e8b\u4ef6\u548c\u5173\u952e\u968f\u8bbf\u884c\u52a8\uff09\u7684\u80fd\u529b\uff0c\u5e76\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u6a21\u578b\u751f\u6210\u7684\u603b\u7ed3\u4e2d\u5e7b\u89c9\u7684\u666e\u904d\u6027\u3002", "motivation": "\u4e34\u5e8a\u603b\u7ed3\u5728\u533b\u7597\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5b83\u80fd\u5c06\u590d\u6742\u7684\u533b\u7597\u6570\u636e\u8f6c\u5316\u4e3a\u6613\u7406\u89e3\u7684\u4fe1\u606f\u3002LLMs\u56e0\u5176\u5148\u8fdb\u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u5728\u81ea\u52a8\u5316\u548c\u63d0\u9ad8\u4e34\u5e8a\u603b\u7ed3\u51c6\u786e\u6027\u65b9\u9762\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u5168\u9762\u7684\u6570\u503c\u6a21\u62df\uff0c\u4e25\u683c\u8bc4\u4f30\u5f00\u6e90LLMs\u7684\u6027\u80fd\uff0c\u91cd\u70b9\u5173\u6ce8\u63d0\u53d6\u5185\u5bb9\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u5c24\u5176\u662f\u4e34\u5e8a\u603b\u7ed3\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f00\u6e90LLMs\u5728\u4e34\u5e8a\u603b\u7ed3\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b58\u5728\u4e00\u5b9a\u7a0b\u5ea6\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u8fd9\u53ef\u80fd\u5f71\u54cd\u4fe1\u606f\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u4e86\u5728\u4e34\u5e8a\u603b\u7ed3\u4e2d\u68c0\u6d4b\u548c\u51cf\u5c11\u5e7b\u89c9\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u786e\u4fdd\u4fe1\u606f\u7684\u51c6\u786e\u6027\uff0c\u4ece\u800c\u63d0\u5347\u60a3\u8005\u62a4\u7406\u8d28\u91cf\u3002"}}
{"id": "2504.19179", "pdf": "https://arxiv.org/pdf/2504.19179", "abs": "https://arxiv.org/abs/2504.19179", "authors": ["Pedro A. Moreno-S\u00e1nchez", "Javier Del Ser", "Mark van Gils", "Jussi Hernesniemi"], "title": "A Design Framework for operationalizing Trustworthy Artificial Intelligence in Healthcare: Requirements, Tradeoffs and Challenges for its Clinical Adoption", "categories": ["cs.AI"], "comment": null, "summary": "Artificial Intelligence (AI) holds great promise for transforming healthcare,\nparticularly in disease diagnosis, prognosis, and patient care. The increasing\navailability of digital medical data, such as images, omics, biosignals, and\nelectronic health records, combined with advances in computing, has enabled AI\nmodels to approach expert-level performance. However, widespread clinical\nadoption remains limited, primarily due to challenges beyond technical\nperformance, including ethical concerns, regulatory barriers, and lack of\ntrust. To address these issues, AI systems must align with the principles of\nTrustworthy AI (TAI), which emphasize human agency and oversight, algorithmic\nrobustness, privacy and data governance, transparency, bias and discrimination\navoidance, and accountability. Yet, the complexity of healthcare processes\n(e.g., screening, diagnosis, prognosis, and treatment) and the diversity of\nstakeholders (clinicians, patients, providers, regulators) complicate the\nintegration of TAI principles. To bridge the gap between TAI theory and\npractical implementation, this paper proposes a design framework to support\ndevelopers in embedding TAI principles into medical AI systems. Thus, for each\nstakeholder identified across various healthcare processes, we propose a\ndisease-agnostic collection of requirements that medical AI systems should\nincorporate to adhere to the principles of TAI. Additionally, we examine the\nchallenges and tradeoffs that may arise when applying these principles in\npractice. To ground the discussion, we focus on cardiovascular diseases, a\nfield marked by both high prevalence and active AI innovation, and demonstrate\nhow TAI principles have been applied and where key obstacles persist.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bbe\u8ba1\u6846\u67b6\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u5728\u533b\u7597AI\u7cfb\u7edf\u4e2d\u878d\u5165\u53ef\u4fe1AI\uff08TAI\uff09\u539f\u5219\uff0c\u4ee5\u89e3\u51b3\u4f26\u7406\u3001\u76d1\u7ba1\u548c\u4fe1\u4efb\u95ee\u9898\uff0c\u5e76\u4ee5\u5fc3\u8840\u7ba1\u75be\u75c5\u4e3a\u4f8b\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u548c\u6311\u6218\u3002", "motivation": "\u533b\u7597AI\u7684\u5e7f\u6cdb\u5e94\u7528\u53d7\u9650\u4e8e\u6280\u672f\u4e4b\u5916\u7684\u6311\u6218\uff08\u5982\u4f26\u7406\u3001\u76d1\u7ba1\u548c\u4fe1\u4efb\u95ee\u9898\uff09\uff0c\u9700\u8981\u7b26\u5408\u53ef\u4fe1AI\uff08TAI\uff09\u539f\u5219\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u8bbe\u8ba1\u6846\u67b6\uff0c\u9488\u5bf9\u4e0d\u540c\u533b\u7597\u6d41\u7a0b\u548c\u5229\u76ca\u76f8\u5173\u8005\uff0c\u5236\u5b9a\u4e0eTAI\u539f\u5219\u4e00\u81f4\u7684\u75be\u75c5\u65e0\u5173\u9700\u6c42\u96c6\u5408\uff0c\u5e76\u5206\u6790\u5b9e\u8df5\u4e2d\u7684\u6311\u6218\u4e0e\u6743\u8861\u3002", "result": "\u6846\u67b6\u5c55\u793a\u4e86\u5728\u5fc3\u8840\u7ba1\u75be\u75c5\u9886\u57df\u5982\u4f55\u5e94\u7528TAI\u539f\u5219\uff0c\u5e76\u8bc6\u522b\u4e86\u5b9e\u9645\u969c\u788d\u3002", "conclusion": "\u901a\u8fc7\u6846\u67b6\u5316\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u66f4\u7cfb\u7edf\u5730\u5c06TAI\u539f\u5219\u878d\u5165\u533b\u7597AI\u7cfb\u7edf\uff0c\u63a8\u52a8\u5176\u4e34\u5e8a\u843d\u5730\u3002"}}
{"id": "2504.18668", "pdf": "https://arxiv.org/pdf/2504.18668", "abs": "https://arxiv.org/abs/2504.18668", "authors": ["Daehyeon Han", "Morteza Karimzadeh"], "title": "Exploring the Potential of Latent Embeddings for Sea Ice Characterization using ICESat-2 Data", "categories": ["cs.LG", "physics.ao-ph"], "comment": "4 pages, 4 figures", "summary": "The Ice, Cloud, and Elevation Satellite-2 (ICESat-2) provides high-resolution\nmeasurements of sea ice height. Recent studies have developed machine learning\nmethods on ICESat-2 data, primarily focusing on surface type classification.\nHowever, the heavy reliance on manually collected labels requires significant\ntime and effort for supervised learning, as it involves cross-referencing track\nmeasurements with overlapping background optical imagery. Additionally, the\ncoincidence of ICESat-2 tracks with background images is relatively rare due to\nthe different overpass patterns and atmospheric conditions. To address these\nlimitations, this study explores the potential of unsupervised autoencoder on\nunlabeled data to derive latent embeddings. We develop autoencoder models based\non Long Short-Term Memory (LSTM) and Convolutional Neural Networks (CNN) to\nreconstruct topographic sequences from ICESat-2 and derive embeddings. We then\napply Uniform Manifold Approximation and Projection (UMAP) to reduce dimensions\nand visualize the embeddings. Our results show that embeddings from\nautoencoders preserve the overall structure but generate relatively more\ncompact clusters compared to the original ICESat-2 data, indicating the\npotential of embeddings to lessen the number of required labels samples.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u5728ICESat-2\u6570\u636e\u4e0a\u4f7f\u7528\u65e0\u76d1\u7763\u81ea\u7f16\u7801\u5668\uff08LSTM\u548cCNN\uff09\u751f\u6210\u6f5c\u5728\u5d4c\u5165\uff0c\u4ee5\u51cf\u5c11\u5bf9\u4eba\u5de5\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u5e76\u901a\u8fc7UMAP\u964d\u7ef4\u53ef\u89c6\u5316\u5d4c\u5165\uff0c\u7ed3\u679c\u8868\u660e\u5d4c\u5165\u80fd\u51cf\u5c11\u6240\u9700\u6807\u7b7e\u6837\u672c\u91cf\u3002", "motivation": "\u51cf\u8f7b\u5bf9\u4eba\u5de5\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u89e3\u51b3ICESat-2\u8f68\u8ff9\u4e0e\u80cc\u666f\u56fe\u50cf\u91cd\u5408\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528LSTM\u548cCNN\u81ea\u7f16\u7801\u5668\u5728\u65e0\u6807\u6ce8ICESat-2\u6570\u636e\u4e0a\u751f\u6210\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7UMAP\u964d\u7ef4\u548c\u53ef\u89c6\u5316\u3002", "result": "\u5d4c\u5165\u4fdd\u7559\u4e86\u6570\u636e\u7ed3\u6784\u5e76\u5f62\u6210\u66f4\u7d27\u51d1\u7684\u7c07\uff0c\u53ef\u80fd\u51cf\u5c11\u6240\u9700\u6807\u6ce8\u6837\u672c\u91cf\u3002", "conclusion": "\u65e0\u76d1\u7763\u81ea\u7f16\u7801\u5668\u5728ICESat-2\u6570\u636e\u5904\u7406\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u3002"}}
{"id": "2504.19066", "pdf": "https://arxiv.org/pdf/2504.19066", "abs": "https://arxiv.org/abs/2504.19066", "authors": ["Deeksha Varshney", "Keane Ong", "Rui Mao", "Erik Cambria", "Gianmarco Mengaldo"], "title": "ClimaEmpact: Domain-Aligned Small Language Models and Datasets for Extreme Weather Analytics", "categories": ["cs.CL", "cs.AI", "cs.LG", "physics.ao-ph"], "comment": null, "summary": "Accurate assessments of extreme weather events are vital for research and\npolicy, yet localized and granular data remain scarce in many parts of the\nworld. This data gap limits our ability to analyze potential outcomes and\nimplications of extreme weather events, hindering effective decision-making.\nLarge Language Models (LLMs) can process vast amounts of unstructured text\ndata, extract meaningful insights, and generate detailed assessments by\nsynthesizing information from multiple sources. Furthermore, LLMs can\nseamlessly transfer their general language understanding to smaller models,\nenabling these models to retain key knowledge while being fine-tuned for\nspecific tasks. In this paper, we propose Extreme Weather Reasoning-Aware\nAlignment (EWRA), a method that enhances small language models (SLMs) by\nincorporating structured reasoning paths derived from LLMs, and\nExtremeWeatherNews, a large dataset of extreme weather event-related news\narticles. EWRA and ExtremeWeatherNews together form the overall framework,\nClimaEmpact, that focuses on addressing three critical extreme-weather tasks:\ncategorization of tangible vulnerabilities/impacts, topic labeling, and emotion\nanalysis. By aligning SLMs with advanced reasoning strategies on\nExtremeWeatherNews (and its derived dataset ExtremeAlign used specifically for\nSLM alignment), EWRA improves the SLMs' ability to generate well-grounded and\ndomain-specific responses for extreme weather analytics. Our results show that\nthe approach proposed guides SLMs to output domain-aligned responses,\nsurpassing the performance of task-specific models and offering enhanced\nreal-world applicability for extreme weather analytics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aEWRA\u7684\u65b9\u6cd5\u548cExtremeWeatherNews\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u6784\u5316\u63a8\u7406\u8def\u5f84\u589e\u5f3a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\uff0c\u4ee5\u6539\u8fdb\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u5206\u7c7b\u3001\u6807\u8bb0\u548c\u60c5\u611f\u5206\u6790\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u7814\u7a76\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u5347\u73b0\u6709\u6a21\u578b\u7684\u5b9e\u7528\u6027\u548c\u51b3\u7b56\u652f\u6301\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u6784\u5316\u63a8\u7406\u8def\u5f84\uff08EWRA\u65b9\u6cd5\uff09\u548c\u6781\u7aef\u5929\u6c14\u65b0\u95fb\u6570\u636e\u96c6\uff08ExtremeWeatherNews\uff09\uff0c\u8bad\u7ec3\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5b8c\u6210\u6781\u7aef\u5929\u6c14\u76f8\u5173\u4efb\u52a1\u3002", "result": "EWRA\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6781\u7aef\u5929\u6c14\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u8868\u73b0\u4f18\u4e8e\u4e13\u7528\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7EWRA\u548c\u6570\u636e\u96c6ExtremeWeatherNews\u7684\u7ec4\u5408\u6846\u67b6ClimaEmpact\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6781\u7aef\u5929\u6c14\u5206\u6790\u7684\u5b9e\u7528\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2504.19255", "pdf": "https://arxiv.org/pdf/2504.19255", "abs": "https://arxiv.org/abs/2504.19255", "authors": ["Chad Coleman", "W. Russell Neuman", "Ali Dasdan", "Safinah Ali", "Manan Shah"], "title": "The Convergent Ethics of AI? Analyzing Moral Foundation Priorities in Large Language Models with a Multi-Framework Approach", "categories": ["cs.AI", "cs.CY"], "comment": "25 pages, 8 figures", "summary": "As large language models (LLMs) are increasingly deployed in consequential\ndecision-making contexts, systematically assessing their ethical reasoning\ncapabilities becomes a critical imperative. This paper introduces the\nPriorities in Reasoning and Intrinsic Moral Evaluation (PRIME) framework--a\ncomprehensive methodology for analyzing moral priorities across foundational\nethical dimensions including consequentialist-deontological reasoning, moral\nfoundations theory, and Kohlberg's developmental stages. We apply this\nframework to six leading LLMs through a dual-protocol approach combining direct\nquestioning and response analysis to established ethical dilemmas. Our analysis\nreveals striking patterns of convergence: all evaluated models demonstrate\nstrong prioritization of care/harm and fairness/cheating foundations while\nconsistently underweighting authority, loyalty, and sanctity dimensions.\nThrough detailed examination of confidence metrics, response reluctance\npatterns, and reasoning consistency, we establish that contemporary LLMs (1)\nproduce decisive ethical judgments, (2) demonstrate notable cross-model\nalignment in moral decision-making, and (3) generally correspond with\nempirically established human moral preferences. This research contributes a\nscalable, extensible methodology for ethical benchmarking while highlighting\nboth the promising capabilities and systematic limitations in current AI moral\nreasoning architectures--insights critical for responsible development as these\nsystems assume increasingly significant societal roles.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86PRIME\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u9053\u5fb7\u51b3\u7b56\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6240\u6709\u6a21\u578b\u66f4\u6ce8\u91cd\u5173\u6000/\u4f24\u5bb3\u4e0e\u516c\u5e73/\u6b3a\u9a97\u7ef4\u5ea6\uff0c\u800c\u5ffd\u89c6\u6743\u5a01\u3001\u5fe0\u8bda\u548c\u795e\u5723\u6027\u3002", "motivation": "\u968f\u7740LLMs\u5728\u91cd\u8981\u51b3\u7b56\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7cfb\u7edf\u8bc4\u4f30\u5176\u9053\u5fb7\u63a8\u7406\u80fd\u529b\u6210\u4e3a\u8feb\u5207\u9700\u6c42\u3002", "method": "\u91c7\u7528PRIME\u6846\u67b6\uff0c\u7ed3\u5408\u76f4\u63a5\u63d0\u95ee\u4e0e\u56de\u5e94\u5206\u6790\uff0c\u8bc4\u4f30\u516d\u5927\u9886\u5148LLMs\u5728\u4f26\u7406\u56f0\u5883\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728\u5173\u6000/\u4f24\u5bb3\u4e0e\u516c\u5e73/\u6b3a\u9a97\u65b9\u9762\u8868\u73b0\u4e00\u81f4\uff0c\u4f46\u5728\u5176\u4ed6\u9053\u5fb7\u7ef4\u5ea6\u4e0a\u4e0d\u8db3\uff0c\u4e14\u4e0e\u4eba\u7c7b\u9053\u5fb7\u504f\u597d\u76f8\u7b26\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9053\u5fb7\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u65b9\u6cd5\uff0c\u540c\u65f6\u63ed\u793a\u4e86AI\u9053\u5fb7\u63a8\u7406\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u3002"}}
{"id": "2504.18686", "pdf": "https://arxiv.org/pdf/2504.18686", "abs": "https://arxiv.org/abs/2504.18686", "authors": ["Mustafa Musab", "Joseph K. Chege", "Arie Yeredor", "Martin Haardt"], "title": "A Unified MDL-based Binning and Tensor Factorization Framework for PDF Estimation", "categories": ["cs.LG", "cs.IT", "math.IT", "math.PR", "stat.ML"], "comment": null, "summary": "Reliable density estimation is fundamental for numerous applications in\nstatistics and machine learning. In many practical scenarios, data are best\nmodeled as mixtures of component densities that capture complex and multimodal\npatterns. However, conventional density estimators based on uniform histograms\noften fail to capture local variations, especially when the underlying\ndistribution is highly nonuniform. Furthermore, the inherent discontinuity of\nhistograms poses challenges for tasks requiring smooth derivatives, such as\ngradient-based optimization, clustering, and nonparametric discriminant\nanalysis. In this work, we present a novel non-parametric approach for\nmultivariate probability density function (PDF) estimation that utilizes\nminimum description length (MDL)-based binning with quantile cuts. Our approach\nbuilds upon tensor factorization techniques, leveraging the canonical polyadic\ndecomposition (CPD) of a joint probability tensor. We demonstrate the\neffectiveness of our method on synthetic data and a challenging real dry bean\nclassification dataset.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\uff08MDL\uff09\u5206\u7bb1\u548c\u5206\u4f4d\u6570\u5207\u5272\u7684\u975e\u53c2\u6570\u591a\u53d8\u91cf\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\uff08PDF\uff09\u4f30\u8ba1\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u76f4\u65b9\u56fe\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u591a\u6a21\u6001\u6570\u636e\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5747\u5300\u76f4\u65b9\u56fe\u7684\u5bc6\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u9ad8\u5ea6\u975e\u5747\u5300\u5206\u5e03\u7684\u5c40\u90e8\u53d8\u5316\uff0c\u4e14\u4e0d\u8fde\u7eed\u6027\u5bf9\u68af\u5ea6\u4f18\u5316\u7b49\u4efb\u52a1\u9020\u6210\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9c81\u68d2\u7684\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\uff08MDL\uff09\u5206\u7bb1\u548c\u5206\u4f4d\u6570\u5207\u5272\uff0c\u7ed3\u5408\u5f20\u91cf\u5206\u89e3\u6280\u672f\uff08\u5982\u89c4\u8303\u591a\u53c9\u5206\u89e3CPD\uff09\u6784\u5efa\u8054\u5408\u6982\u7387\u5f20\u91cf\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u5e72\u8c46\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u591a\u6a21\u6001\u6570\u636e\u7684\u5bc6\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.19070", "pdf": "https://arxiv.org/pdf/2504.19070", "abs": "https://arxiv.org/abs/2504.19070", "authors": ["Sakshi Singh", "Abhinav Prakash", "Aakriti Shah", "Chaitanya Sachdeva", "Sanjana Dumpala"], "title": "Sample-Efficient Language Model for Hinglish Conversational AI", "categories": ["cs.CL", "I.2.7; I.2.6; H.5.2"], "comment": "5 pages, 2 tables, 2 figures", "summary": "This paper presents our process for developing a sample-efficient language\nmodel for a conversational Hinglish chatbot. Hinglish, a code-mixed language\nthat combines Hindi and English, presents a unique computational challenge due\nto inconsistent spelling, lack of standardization, and limited quality of\nconversational data. This work evaluates multiple pre-trained cross-lingual\nlanguage models, including Gemma3-4B and Qwen2.5-7B, and employs fine-tuning\ntechniques to improve performance on Hinglish conversational tasks. The\nproposed approach integrates synthetically generated dialogues with insights\nfrom existing Hinglish datasets to address data scarcity. Experimental results\ndemonstrate that models with fewer parameters, when appropriately fine-tuned on\nhigh-quality code-mixed data, can achieve competitive performance for Hinglish\nconversation generation while maintaining computational efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684Hinglish\u5bf9\u8bdd\u6a21\u578b\u5f00\u53d1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u591a\u8bed\u8a00\u6a21\u578b\uff08\u5982Gemma3-4B\u548cQwen2.5-7B\uff09\u5e76\u7ed3\u5408\u5408\u6210\u6570\u636e\u4e0e\u73b0\u6709\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86Hinglish\u5bf9\u8bdd\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "Hinglish\uff08\u5370\u5730\u8bed\u4e0e\u82f1\u8bed\u6df7\u5408\u8bed\u8a00\uff09\u7531\u4e8e\u62fc\u5199\u4e0d\u4e00\u81f4\u3001\u7f3a\u4e4f\u6807\u51c6\u5316\u548c\u5bf9\u8bdd\u6570\u636e\u8d28\u91cf\u6709\u9650\uff0c\u5e26\u6765\u72ec\u7279\u8ba1\u7b97\u6311\u6218\uff0c\u56e0\u6b64\u4e9f\u9700\u5f00\u53d1\u9ad8\u6548\u7684\u5bf9\u8bdd\u6a21\u578b\u3002", "method": "\u8bc4\u4f30\u5e76\u5fae\u8c03\u591a\u79cd\u9884\u8bad\u7ec3\u591a\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u5408\u5408\u6210\u5bf9\u8bdd\u4e0e\u73b0\u6709Hinglish\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u53c2\u6570\u8f83\u5c11\u7684\u6a21\u578b\u901a\u8fc7\u9ad8\u8d28\u91cf\u6df7\u5408\u6570\u636e\u5fae\u8c03\uff0c\u53ef\u5728Hinglish\u5bf9\u8bdd\u751f\u6210\u4e2d\u5b9e\u73b0\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u5e73\u8861\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f4e\u8d44\u6e90\u6df7\u5408\u8bed\u8a00\u7684\u9ad8\u6548\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u6570\u636e\u8d28\u91cf\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2504.19277", "pdf": "https://arxiv.org/pdf/2504.19277", "abs": "https://arxiv.org/abs/2504.19277", "authors": ["Ishan Kavathekar", "Raghav Donakanti", "Ponnurangam Kumaraguru", "Karthik Vaidhyanathan"], "title": "Small Models, Big Tasks: An Exploratory Empirical Study on Small Language Models for Function Calling", "categories": ["cs.AI", "cs.SE"], "comment": "Accepted at EASE 2025 AI Models and Data Evaluation track", "summary": "Function calling is a complex task with widespread applications in domains\nsuch as information retrieval, software engineering and automation. For\nexample, a query to book the shortest flight from New York to London on January\n15 requires identifying the correct parameters to generate accurate function\ncalls. Large Language Models (LLMs) can automate this process but are\ncomputationally expensive and impractical in resource-constrained settings. In\ncontrast, Small Language Models (SLMs) can operate efficiently, offering faster\nresponse times, and lower computational demands, making them potential\ncandidates for function calling on edge devices. In this exploratory empirical\nstudy, we evaluate the efficacy of SLMs in generating function calls across\ndiverse domains using zero-shot, few-shot, and fine-tuning approaches, both\nwith and without prompt injection, while also providing the finetuned models to\nfacilitate future applications. Furthermore, we analyze the model responses\nacross a range of metrics, capturing various aspects of function call\ngeneration. Additionally, we perform experiments on an edge device to evaluate\ntheir performance in terms of latency and memory usage, providing useful\ninsights into their practical applicability. Our findings show that while SLMs\nimprove from zero-shot to few-shot and perform best with fine-tuning, they\nstruggle significantly with adhering to the given output format. Prompt\ninjection experiments further indicate that the models are generally robust and\nexhibit only a slight decline in performance. While SLMs demonstrate potential\nfor the function call generation task, our results also highlight areas that\nneed further refinement for real-time functioning.", "AI": {"tldr": "\u63a2\u8ba8\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5728\u751f\u6210\u51fd\u6570\u8c03\u7528\u4efb\u52a1\u4e2d\u7684\u6548\u7387\uff0c\u5bf9\u6bd4\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u5fae\u8c03\u65b9\u6cd5\uff0c\u5e76\u6d4b\u8bd5\u5176\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u9645\u8868\u73b0\u3002\u7ed3\u679c\u663e\u793aSLMs\u5728\u5fae\u8c03\u540e\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u4ecd\u5b58\u5728\u8f93\u51fa\u683c\u5f0f\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u7814\u7a76SLMs\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\uff08\u5982\u8fb9\u7f18\u8bbe\u5907\uff09\u662f\u5426\u80fd\u9ad8\u6548\u751f\u6210\u51fd\u6570\u8c03\u7528\uff0c\u66ff\u4ee3\u8ba1\u7b97\u6210\u672c\u9ad8\u7684LLMs\u3002", "method": "\u91c7\u7528\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u5fae\u8c03\u65b9\u6cd5\uff0c\u7ed3\u5408\u63d0\u793a\u6ce8\u5165\uff0c\u8bc4\u4f30SLMs\u5728\u591a\u9886\u57df\u7684\u51fd\u6570\u8c03\u7528\u751f\u6210\u6548\u679c\uff0c\u5e76\u6d4b\u8bd5\u5176\u5728\u8fb9\u7f18\u8bbe\u5907\u7684\u5ef6\u8fdf\u548c\u5185\u5b58\u4f7f\u7528\u3002", "result": "SLMs\u5728\u5fae\u8c03\u540e\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5bf9\u8f93\u51fa\u683c\u5f0f\u7684\u9075\u5faa\u80fd\u529b\u8f83\u5f31\uff1b\u63d0\u793a\u6ce8\u5165\u4e0b\u6a21\u578b\u8868\u73b0\u7a33\u5065\uff0c\u6027\u80fd\u7565\u964d\u3002\u8fb9\u7f18\u8bbe\u5907\u4e0a\u6d4b\u8bd5\u663e\u793a\u5176\u5b9e\u7528\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "conclusion": "SLMs\u5728\u51fd\u6570\u8c03\u7528\u751f\u6210\u4efb\u52a1\u4e2d\u5177\u5907\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\uff0c\u4f46\u9700\u89e3\u51b3\u683c\u5f0f\u4e00\u81f4\u6027\u548c\u5b9e\u65f6\u6027\u80fd\u95ee\u9898\u4ee5\u63d0\u5347\u5b9e\u7528\u6027\u3002"}}
{"id": "2504.18696", "pdf": "https://arxiv.org/pdf/2504.18696", "abs": "https://arxiv.org/abs/2504.18696", "authors": ["Felix Burr", "Marcel Hoffmann", "Ansgar Scherp"], "title": "Active Few-Shot Learning for Vertex Classification Starting from an Unlabeled Dataset", "categories": ["cs.LG"], "comment": "Accepted at IJCNN 2025", "summary": "Despite the ample availability of graph data, obtaining vertex labels is a\ntedious and expensive task. Therefore, it is desirable to learn from a few\nlabeled vertices only. Existing few-shot learners assume a class oracle, which\nprovides labeled vertices for a desired class. However, such an oracle is not\navailable in a real-world setting, i.e., when drawing a vertex for labeling it\nis unknown to which class the vertex belongs. Few-shot learners are often\ncombined with prototypical networks, while classical semi-supervised vertex\nclassification uses discriminative models, e.g., Graph Convolutional Networks\n(GCN). In this paper, we train our models by iteratively prompting a human\nannotator with vertices to annotate. We perform three experiments where we\ncontinually relax our assumptions. First, we assume a class oracle, i.e., the\nhuman annotator is provided with an equal number of vertices to label for each\nclass. We denote this as \"Balanced Sampling''. In the subsequent experiment,\n\"Unbalanced Sampling,'' we replace the class oracle with $k$-medoids clustering\nand draw vertices to label from the clusters. In the last experiment, the\n\"Unknown Number of Classes,'' we no longer assumed we knew the number and\ndistribution of classes. Our results show that prototypical models outperform\ndiscriminative models in all experiments when fewer than $20$ samples per class\nare available. While dropping the assumption of the class oracle for the\n\"Unbalanced Sampling'' experiment reduces the performance of the GCN by $9\\%$,\nthe prototypical network loses only $1\\%$ on average. For the \"Unknown Number\nof Classes'' experiment, the average performance for both models decreased\nfurther by $1\\%$.\n  Source code: https://github.com/Ximsa/2023-felix-ma", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u53ea\u6709\u5c11\u91cf\u6807\u8bb0\u9876\u70b9\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528\u539f\u578b\u7f51\u7edc\u548c\u5224\u522b\u6a21\u578b\uff08\u5982\u56fe\u5377\u79ef\u7f51\u7edc\uff09\u8fdb\u884c\u9876\u70b9\u5206\u7c7b\u7684\u6027\u80fd\u6bd4\u8f83\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u6bcf\u7c7b\u6837\u672c\u5c11\u4e8e20\u4e2a\u65f6\uff0c\u539f\u578b\u7f51\u7edc\u5728\u6240\u6709\u8bbe\u5b9a\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u5c24\u5176\u5728\u653e\u677e\u7c7b\u522b\u5148\u77e5\u5047\u8bbe\u65f6\uff0c\u5176\u6027\u80fd\u4e0b\u964d\u66f4\u5c0f\u3002", "motivation": "\u7531\u4e8e\u83b7\u53d6\u9876\u70b9\u6807\u8bb0\u65e2\u8017\u65f6\u53c8\u6602\u8d35\uff0c\u7814\u7a76\u5982\u4f55\u5728\u4ec5\u6709\u5c11\u91cf\u6807\u8bb0\u9876\u70b9\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u5b66\u4e60\u662f\u5fc5\u8981\u7684\u3002\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u5b58\u5728\u7c7b\u522b\u5148\u77e5\uff08\u63d0\u4f9b\u6240\u9700\u7c7b\u522b\u7684\u6807\u8bb0\u9876\u70b9\uff09\uff0c\u4f46\u8fd9\u4e0e\u73b0\u5b9e\u573a\u666f\u4e0d\u7b26\u3002", "method": "\u901a\u8fc7\u8fed\u4ee3\u63d0\u793a\u4eba\u5de5\u6807\u6ce8\u8005\u6807\u8bb0\u9876\u70b9\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3002\u5b9e\u9a8c\u5206\u4e09\u9636\u6bb5\u9010\u6b65\u653e\u677e\u5047\u8bbe\uff1a\u5e73\u8861\u91c7\u6837\uff08\u5047\u8bbe\u7c7b\u522b\u5148\u77e5\uff09\u3001\u4e0d\u5e73\u8861\u91c7\u6837\uff08\u7528k-medoids\u805a\u7c7b\u4ee3\u66ff\u7c7b\u522b\u5148\u77e5\uff09\u3001\u672a\u77e5\u7c7b\u522b\u6570\u91cf\u3002", "result": "\u539f\u578b\u7f51\u7edc\u5728\u6240\u6709\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u5224\u522b\u6a21\u578b\uff08\u5c24\u5176\u662f\u6bcf\u7c7b\u6837\u672c\u5c11\u4e8e20\u65f6\uff09\u3002\u653e\u677e\u7c7b\u522b\u5148\u77e5\u5047\u8bbe\u540e\uff0cGCN\u6027\u80fd\u4e0b\u964d9%\uff0c\u800c\u539f\u578b\u7f51\u7edc\u4ec5\u4e0b\u964d1%\uff1b\u5728\u672a\u77e5\u7c7b\u522b\u6570\u91cf\u65f6\uff0c\u4e24\u8005\u6027\u80fd\u5747\u518d\u964d1%\u3002", "conclusion": "\u539f\u578b\u7f51\u7edc\u5728\u5c11\u91cf\u6807\u8bb0\u6570\u636e\u4e0b\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7f3a\u4e4f\u7c7b\u522b\u5206\u5e03\u5148\u9a8c\u65f6\uff0c\u8868\u73b0\u66f4\u52a0\u7a33\u5b9a\u3002"}}
{"id": "2504.19095", "pdf": "https://arxiv.org/pdf/2504.19095", "abs": "https://arxiv.org/abs/2504.19095", "authors": ["Jikai Wang", "Juntao Li", "Lijun Wu", "Min Zhang"], "title": "Efficient Reasoning for LLMs through Speculative Chain-of-Thought", "categories": ["cs.CL"], "comment": null, "summary": "Large reasoning language models such as OpenAI-o1 and Deepseek-R1 have\nrecently attracted widespread attention due to their impressive task-solving\nabilities. However, the enormous model size and the generation of lengthy\nthought chains introduce significant reasoning costs and response latency.\nExisting methods for efficient reasoning mainly focus on reducing the number of\nmodel parameters or shortening the chain-of-thought length. In this paper, we\nintroduce Speculative Chain-of-Thought (SCoT), which reduces reasoning latency\nfrom another perspective by accelerated average reasoning speed through large\nand small model collaboration. SCoT conducts thought-level drafting using a\nlightweight draft model. Then it selects the best CoT draft and corrects the\nerror cases with the target model. The proposed thinking behavior alignment\nimproves the efficiency of drafting and the draft selection strategy maintains\nthe prediction accuracy for complex problems. Experimental results on GSM8K,\nMATH, GaoKao, CollegeMath and Olympiad datasets show that SCoT reduces\nreasoning latency by 48\\%$\\sim$66\\% for Deepseek-R1-Distill-Qwen-32B while\nachieving near-target-model-level performance. Our code is available at\nhttps://github.com/Jikai0Wang/Speculative_CoT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Speculative Chain-of-Thought (SCoT)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5927\u5c0f\u6a21\u578b\u534f\u4f5c\u52a0\u901f\u63a8\u7406\u901f\u5ea6\uff0c\u964d\u4f4e\u5ef6\u8fdf48%~66%\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u76ee\u6807\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u529b\u5f3a\u5927\uff0c\u4f46\u6a21\u578b\u89c4\u6a21\u548c\u957f\u601d\u7ef4\u94fe\u589e\u52a0\u4e86\u63a8\u7406\u6210\u672c\u548c\u5ef6\u8fdf\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u51cf\u5c11\u53c2\u6570\u6216\u7f29\u77ed\u601d\u7ef4\u94fe\u957f\u5ea6\uff0cSCoT\u4ece\u53e6\u4e00\u4e2a\u89d2\u5ea6\u901a\u8fc7\u534f\u4f5c\u52a0\u901f\u63a8\u7406\u3002", "method": "SCoT\u5229\u7528\u8f7b\u91cf\u7ea7\u8349\u7a3f\u6a21\u578b\u8fdb\u884c\u601d\u7ef4\u7ea7\u8349\u62df\uff0c\u9009\u62e9\u6700\u4f73\u601d\u7ef4\u8349\u7a3f\u5e76\u7528\u76ee\u6807\u6a21\u578b\u7ea0\u6b63\u9519\u8bef\u3002\u901a\u8fc7\u601d\u7ef4\u884c\u4e3a\u5bf9\u9f50\u63d0\u5347\u8349\u62df\u6548\u7387\uff0c\u8349\u7a3f\u9009\u62e9\u7b56\u7565\u4fdd\u6301\u590d\u6742\u95ee\u9898\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "result": "\u5728GSM8K\u7b49\u6570\u636e\u96c6\u4e0a\uff0cSCoT\u4e3aDeepseek-R1-Distill-Qwen-32B\u964d\u4f4e\u4e8648%~66%\u7684\u63a8\u7406\u5ef6\u8fdf\uff0c\u540c\u65f6\u5b9e\u73b0\u63a5\u8fd1\u76ee\u6807\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "SCoT\u901a\u8fc7\u534f\u4f5c\u52a0\u901f\u63a8\u7406\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u5e76\u4fdd\u6301\u9ad8\u6027\u80fd\uff0c\u4e3a\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.19320", "pdf": "https://arxiv.org/pdf/2504.19320", "abs": "https://arxiv.org/abs/2504.19320", "authors": ["Ralph Wojtowicz"], "title": "Logic-Based Artificial Intelligence Algorithms Supporting Categorical Semantics", "categories": ["cs.AI", "03, 18", "I.1.2"], "comment": "31 pages", "summary": "This paper seeks to apply categorical logic to the design of artificial\nintelligent agents that reason symbolically about objects more richly\nstructured than sets. Using Johnstone's sequent calculus of terms- and\nformulae-in-context, we develop forward chaining and normal form algorithms for\nreasoning about objects in cartesian categories with the rules for Horn logic.\nWe also adapt first-order unification to support multi-sorted theories,\ncontexts, and fragments of first-order logic. The significance of these\nreformulations rests in the fact that they can be applied to reasoning about\nobjects in semantic categories that do not support classical logic or even all\nits connectives.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5e94\u7528\u8303\u7574\u903b\u8f91\u8bbe\u8ba1\u80fd\u7b26\u53f7\u5316\u63a8\u7406\u66f4\u590d\u6742\u7ed3\u6784\u7684AI\u4ee3\u7406\uff0c\u57fa\u4e8eJohnstone\u7684sequent\u6f14\u7b97\u5f00\u53d1\u6b63\u5411\u94fe\u548c\u8303\u5f0f\u7b97\u6cd5\uff0c\u5e76\u6269\u5c55\u4e00\u9636\u5408\u4e00\u4ee5\u652f\u6301\u591a\u7c7b\u7406\u8bba\u3002\u8fd9\u4e9b\u65b9\u6cd5\u9002\u7528\u4e8e\u4e0d\u4f9d\u8d56\u7ecf\u5178\u903b\u8f91\u7684\u8bed\u4e49\u8303\u7574\u3002", "motivation": "\u901a\u8fc7\u8303\u7574\u903b\u8f91\u8bbe\u8ba1AI\u4ee3\u7406\uff0c\u4f7f\u5176\u80fd\u5bf9\u590d\u6742\u7ed3\u6784\u5bf9\u8c61\u8fdb\u884c\u7b26\u53f7\u5316\u63a8\u7406\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u4e0d\u4f9d\u8d56\u7ecf\u5178\u903b\u8f91\u7684\u8bed\u4e49\u573a\u666f\u3002", "method": "\u57fa\u4e8eJohnstone\u7684sequent\u6f14\u7b97\uff0c\u5f00\u53d1\u6b63\u5411\u94fe\u548c\u8303\u5f0f\u7b97\u6cd5\uff0c\u5e76\u6269\u5c55\u4e00\u9636\u5408\u4e00\u4ee5\u652f\u6301\u591a\u7c7b\u7406\u8bba\u3001\u4e0a\u4e0b\u6587\u548c\u90e8\u5206\u4e00\u9636\u903b\u8f91\u7247\u6bb5\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u9002\u7528\u4e8e\u7b1b\u5361\u5c14\u8303\u7574\u4e2d\u7684Horn\u903b\u8f91\u89c4\u5219\uff0c\u5e76\u80fd\u5904\u7406\u4e0d\u4f9d\u8d56\u7ecf\u5178\u903b\u8f91\u7684\u8bed\u4e49\u8303\u7574\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aAI\u4ee3\u7406\u5728\u975e\u7ecf\u5178\u903b\u8f91\u73af\u5883\u4e0b\u7684\u7b26\u53f7\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u62d3\u5bbd\u4e86\u8303\u7574\u903b\u8f91\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2504.18710", "pdf": "https://arxiv.org/pdf/2504.18710", "abs": "https://arxiv.org/abs/2504.18710", "authors": ["Patr\u00edcia Mu\u00f1oz Ewald"], "title": "Explicit neural network classifiers for non-separable data", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML", "57R70, 62M45"], "comment": "10 pages", "summary": "We fully characterize a large class of feedforward neural networks in terms\nof truncation maps. As an application, we show how a ReLU neural network can\nimplement a feature map which separates concentric data.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u622a\u65ad\u6620\u5c04\u5168\u9762\u63cf\u8ff0\u4e86\u4e00\u7c7b\u524d\u9988\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u5c55\u793a\u4e86ReLU\u7f51\u7edc\u5982\u4f55\u5b9e\u73b0\u5206\u79bb\u540c\u5fc3\u6570\u636e\u7684\u7279\u5f81\u6620\u5c04\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u622a\u65ad\u6620\u5c04\u6df1\u5165\u7406\u89e3\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u7684\u7ed3\u6784\u4e0e\u80fd\u529b\uff0c\u5c24\u5176\u5173\u6ce8\u5176\u5728\u5206\u79bb\u590d\u6742\u6570\u636e\uff08\u5982\u540c\u5fc3\u6570\u636e\uff09\u4e0a\u7684\u5e94\u7528\u3002", "method": "\u4f5c\u8005\u91c7\u7528\u622a\u65ad\u6620\u5c04\u5bf9\u4e00\u7c7b\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5b8c\u6574\u63cf\u8ff0\uff0c\u5e76\u4ee5ReLU\u7f51\u7edc\u4e3a\u4f8b\uff0c\u5c55\u793a\u5176\u5b9e\u73b0\u7279\u5f81\u6620\u5c04\u7684\u5177\u4f53\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u8bc1\u660eReLU\u7f51\u7edc\u80fd\u901a\u8fc7\u7279\u5b9a\u7279\u5f81\u6620\u5c04\u6709\u6548\u5206\u79bb\u540c\u5fc3\u6570\u636e\uff0c\u9a8c\u8bc1\u4e86\u622a\u65ad\u6620\u5c04\u5728\u6b64\u7c7b\u7f51\u7edc\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u7406\u8bba\u5206\u6790\u4e0e\u5b9e\u4f8b\u9a8c\u8bc1\uff0c\u4e3a\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u7684\u7ed3\u6784\u8bbe\u8ba1\u4e0e\u529f\u80fd\u5b9e\u73b0\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u4e0e\u5de5\u5177\u3002"}}
{"id": "2504.19101", "pdf": "https://arxiv.org/pdf/2504.19101", "abs": "https://arxiv.org/abs/2504.19101", "authors": ["Qianren Mao", "Qili Zhang", "Hanwen Hao", "Zhentao Han", "Runhua Xu", "Weifeng Jiang", "Qi Hu", "Zhijun Chen", "Tyler Zhou", "Bo Li", "Yangqiu Song", "Jin Dong", "Jianxin Li", "Philip S. Yu"], "title": "Privacy-Preserving Federated Embedding Learning for Localized Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has recently emerged as a promising\nsolution for enhancing the accuracy and credibility of Large Language Models\n(LLMs), particularly in Question & Answer tasks. This is achieved by\nincorporating proprietary and private data from integrated databases. However,\nprivate RAG systems face significant challenges due to the scarcity of private\ndomain data and critical data privacy issues. These obstacles impede the\ndeployment of private RAG systems, as developing privacy-preserving RAG systems\nrequires a delicate balance between data security and data availability. To\naddress these challenges, we regard federated learning (FL) as a highly\npromising technology for privacy-preserving RAG services. We propose a novel\nframework called Federated Retrieval-Augmented Generation (FedE4RAG). This\nframework facilitates collaborative training of client-side RAG retrieval\nmodels. The parameters of these models are aggregated and distributed on a\ncentral-server, ensuring data privacy without direct sharing of raw data. In\nFedE4RAG, knowledge distillation is employed for communication between the\nserver and client models. This technique improves the generalization of local\nRAG retrievers during the federated learning process. Additionally, we apply\nhomomorphic encryption within federated learning to safeguard model parameters\nand mitigate concerns related to data leakage. Extensive experiments conducted\non the real-world dataset have validated the effectiveness of FedE4RAG. The\nresults demonstrate that our proposed framework can markedly enhance the\nperformance of private RAG systems while maintaining robust data privacy\nprotection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aFedE4RAG\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528\u8054\u90a6\u5b66\u4e60\u548c\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u589e\u5f3a\u79c1\u6709RAG\u7cfb\u7edf\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3002", "motivation": "\u79c1\u6709RAG\u7cfb\u7edf\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u9690\u79c1\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u5e73\u8861\u6570\u636e\u5b89\u5168\u4e0e\u53ef\u7528\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faFedE4RAG\u6846\u67b6\uff0c\u6574\u5408\u8054\u90a6\u5b66\u4e60\u4e0e\u77e5\u8bc6\u84b8\u998f\uff0c\u91c7\u7528\u540c\u6001\u52a0\u5bc6\u4fdd\u62a4\u6a21\u578b\u53c2\u6570\uff0c\u907f\u514d\u539f\u59cb\u6570\u636e\u5171\u4eab\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79c1\u6709RAG\u7cfb\u7edf\u7684\u6027\u80fd\u5e76\u786e\u4fdd\u6570\u636e\u9690\u79c1\u3002", "conclusion": "FedE4RAG\u6210\u529f\u89e3\u51b3\u4e86\u79c1\u6709RAG\u7cfb\u7edf\u7684\u6570\u636e\u9690\u79c1\u4e0e\u6027\u80fd\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2504.19354", "pdf": "https://arxiv.org/pdf/2504.19354", "abs": "https://arxiv.org/abs/2504.19354", "authors": ["Erkan Karabulut", "Paul Groth", "Victoria Degeler"], "title": "Neurosymbolic Association Rule Mining from Tabular Data", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Association Rule Mining (ARM) is the task of mining patterns among data\nfeatures in the form of logical rules, with applications across a myriad of\ndomains. However, high-dimensional datasets often result in an excessive number\nof rules, increasing execution time and negatively impacting downstream task\nperformance. Managing this rule explosion remains a central challenge in ARM\nresearch. To address this, we introduce Aerial+, a novel neurosymbolic ARM\nmethod. Aerial+ leverages an under-complete autoencoder to create a neural\nrepresentation of the data, capturing associations between features. It\nextracts rules from this neural representation by exploiting the model's\nreconstruction mechanism. Extensive evaluations on five datasets against seven\nbaselines demonstrate that Aerial+ achieves state-of-the-art results by\nlearning more concise, high-quality rule sets with full data coverage. When\nintegrated into rule-based interpretable machine learning models, Aerial+\nsignificantly reduces execution time while maintaining or improving accuracy.", "AI": {"tldr": "Aerial+\u662f\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7b26\u53f7\u5173\u8054\u89c4\u5219\u6316\u6398\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b20\u5b8c\u5907\u81ea\u7f16\u7801\u5668\u6355\u6349\u7279\u5f81\u5173\u8054\uff0c\u4ece\u795e\u7ecf\u8868\u793a\u4e2d\u63d0\u53d6\u89c4\u5219\uff0c\u6709\u6548\u89e3\u51b3\u89c4\u5219\u7206\u70b8\u95ee\u9898\u3002", "motivation": "\u9ad8\u7ef4\u6570\u636e\u96c6\u5bfc\u81f4\u5173\u8054\u89c4\u5219\u7206\u70b8\uff0c\u589e\u52a0\u6267\u884c\u65f6\u95f4\u5e76\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0cAerial+\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528\u6b20\u5b8c\u5907\u81ea\u7f16\u7801\u5668\u751f\u6210\u6570\u636e\u7684\u795e\u7ecf\u8868\u793a\uff0c\u901a\u8fc7\u6a21\u578b\u7684\u91cd\u5efa\u673a\u5236\u63d0\u53d6\u89c4\u5219\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u5bf9\u6bd4\u4e03\u4e2a\u57fa\u7ebf\u65b9\u6cd5\uff0cAerial+\u5b66\u4e60\u5230\u66f4\u7b80\u6d01\u3001\u9ad8\u8d28\u91cf\u7684\u89c4\u5219\u96c6\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u51c6\u786e\u6027\u3002", "conclusion": "Aerial+\u80fd\u663e\u8457\u51cf\u5c11\u6267\u884c\u65f6\u95f4\u5e76\u63d0\u5347\u89c4\u5219\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002"}}
{"id": "2504.18720", "pdf": "https://arxiv.org/pdf/2504.18720", "abs": "https://arxiv.org/abs/2504.18720", "authors": ["G\u00e9r\u00f4me Andry", "Fran\u00e7ois Rozet", "Sacha Lewin", "Omer Rochman", "Victor Mangeleer", "Matthias Pirlet", "Elise Faulx", "Marilaure Gr\u00e9goire", "Gilles Louppe"], "title": "Appa: Bending Weather Dynamics with Latent Diffusion Models for Global Data Assimilation", "categories": ["cs.LG", "physics.ao-ph"], "comment": null, "summary": "Deep learning has transformed weather forecasting by improving both its\naccuracy and computational efficiency. However, before any forecast can begin,\nweather centers must identify the current atmospheric state from vast amounts\nof observational data. To address this challenging problem, we introduce Appa,\na score-based data assimilation model producing global atmospheric trajectories\nat 0.25-degree resolution and 1-hour intervals. Powered by a 1.5B-parameter\nspatio-temporal latent diffusion model trained on ERA5 reanalysis data, Appa\ncan be conditioned on any type of observations to infer the posterior\ndistribution of plausible state trajectories, without retraining. Our unified\nprobabilistic framework flexibly tackles multiple inference tasks --\nreanalysis, filtering, and forecasting -- using the same model, eliminating the\nneed for task-specific architectures or training procedures. Experiments\ndemonstrate physical consistency on a global scale and good reconstructions\nfrom observations, while showing competitive forecasting skills. Our results\nestablish latent score-based data assimilation as a promising foundation for\nfuture global atmospheric modeling systems.", "AI": {"tldr": "Appa\u662f\u4e00\u4e2a\u57fa\u4e8e\u5206\u6570\u7684\u6570\u636e\u540c\u5316\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u751f\u6210\u5168\u7403\u5927\u6c14\u8f68\u8ff9\uff0c\u652f\u6301\u591a\u79cd\u63a8\u7406\u4efb\u52a1\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u4f20\u7edf\u5929\u6c14\u9884\u62a5\u9700\u4ece\u5927\u91cf\u89c2\u6d4b\u6570\u636e\u4e2d\u8bc6\u522b\u5f53\u524d\u5927\u6c14\u72b6\u6001\uff0c\u6311\u6218\u5de8\u5927\u3002", "method": "\u4f7f\u75281.5B\u53c2\u6570\u7684\u65f6\u7a7a\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u57fa\u4e8eERA5\u518d\u5206\u6790\u6570\u636e\u8bad\u7ec3\uff0c\u53ef\u7075\u6d3b\u9002\u914d\u591a\u79cd\u89c2\u6d4b\u7c7b\u578b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u5176\u7269\u7406\u4e00\u81f4\u6027\u548c\u826f\u597d\u91cd\u5efa\u80fd\u529b\uff0c\u540c\u65f6\u5728\u9884\u62a5\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u57fa\u4e8e\u5206\u6570\u7684\u6570\u636e\u540c\u5316\u4e3a\u5168\u7403\u5927\u6c14\u5efa\u6a21\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.19110", "pdf": "https://arxiv.org/pdf/2504.19110", "abs": "https://arxiv.org/abs/2504.19110", "authors": ["Huajian Xin", "Luming Li", "Xiaoran Jin", "Jacques Fleuriot", "Wenda Li"], "title": "APE-Bench I: Towards File-level Automated Proof Engineering of Formal Math Libraries", "categories": ["cs.CL"], "comment": null, "summary": "Recent progress in large language models (LLMs) has shown promise in formal\ntheorem proving, yet existing benchmarks remain limited to isolated, static\nproof tasks, failing to capture the iterative, engineering-intensive workflows\nof real-world formal mathematics libraries. Motivated by analogous advances in\nsoftware engineering, we introduce the paradigm of Automated Proof Engineering\n(APE), which aims to automate proof engineering tasks such as feature addition,\nproof refactoring, and bug fixing using LLMs. To facilitate research in this\ndirection, we present APE-Bench I, the first realistic benchmark built from\nreal-world commit histories of Mathlib4, featuring diverse file-level tasks\ndescribed in natural language and verified via a hybrid approach combining the\nLean compiler and LLM-as-a-Judge. We further develop Eleanstic, a scalable\nparallel verification infrastructure optimized for proof checking across\nmultiple versions of Mathlib. Empirical results on state-of-the-art LLMs\ndemonstrate strong performance on localized edits but substantial degradation\non handling complex proof engineering. This work lays the foundation for\ndeveloping agentic workflows in proof engineering, with future benchmarks\ntargeting multi-file coordination, project-scale verification, and autonomous\nagents capable of planning, editing, and repairing formal libraries.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u81ea\u52a8\u5316\u8bc1\u660e\u5de5\u7a0b\uff08APE\uff09\u7684\u65b0\u8303\u5f0f\uff0c\u65e8\u5728\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u5b8c\u6210\u8bc1\u660e\u5de5\u7a0b\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86\u9996\u4e2a\u771f\u5b9e\u573a\u666f\u7684\u6d4b\u8bc4\u57fa\u51c6APE-Bench I\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLM\u5728\u5904\u7406\u590d\u6742\u8bc1\u660e\u5de5\u7a0b\u65f6\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u5b9a\u7406\u8bc1\u660e\u7684\u6d4b\u8bc4\u57fa\u51c6\u5c40\u9650\u4e8e\u9759\u6001\u4efb\u52a1\uff0c\u672a\u80fd\u53cd\u6620\u73b0\u5b9e\u6570\u5b66\u5e93\u4e2d\u8fed\u4ee3\u548c\u5de5\u7a0b\u5bc6\u96c6\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u81ea\u52a8\u5316\u8bc1\u660e\u5de5\u7a0b\u7684\u8303\u5f0f\uff0c\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86APE-Bench I\u57fa\u51c6\uff0c\u57fa\u4e8eMathlib4\u7684\u5b9e\u9645\u63d0\u4ea4\u5386\u53f2\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u591a\u6837\u5316\u4efb\u52a1\uff0c\u91c7\u7528Lean\u7f16\u8bd1\u5668\u548cLLM\u4f5c\u4e3a\u8bc4\u5224\u7684\u6df7\u5408\u9a8c\u8bc1\u65b9\u6cd5\u3002\u5e76\u5f00\u53d1\u4e86\u4f18\u5316\u7684\u5e76\u884c\u9a8c\u8bc1\u57fa\u7840\u8bbe\u65bdEleanstic\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u73b0\u6709LLM\u5728\u5c40\u90e8\u7f16\u8f91\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u8bc1\u660e\u5de5\u7a0b\u4efb\u52a1\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u8bc1\u660e\u5de5\u7a0b\u4e2d\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u672a\u6765\u5c06\u9488\u5bf9\u591a\u6587\u4ef6\u534f\u4f5c\u3001\u9879\u76ee\u7ea7\u9a8c\u8bc1\u548c\u81ea\u4e3b\u4ee3\u7406\u5f00\u53d1\u66f4\u5148\u8fdb\u7684\u6d4b\u8bc4\u57fa\u51c6\u3002"}}
{"id": "2504.19499", "pdf": "https://arxiv.org/pdf/2504.19499", "abs": "https://arxiv.org/abs/2504.19499", "authors": ["Omid Semiari", "Hosein Nikopour", "Shilpa Talwar"], "title": "Graph Reinforcement Learning for QoS-Aware Load Balancing in Open Radio Access Networks", "categories": ["cs.AI", "cs.IT", "cs.LG", "cs.NI", "eess.SP", "math.IT"], "comment": "To be published in the proceedings of the 2025 IEEE International\n  Conference on Communications (ICC), Seventh Workshop on Data Driven\n  Intelligence for Networks and Systems (DDINS)", "summary": "Next-generation wireless cellular networks are expected to provide\nunparalleled Quality-of-Service (QoS) for emerging wireless applications,\nnecessitating strict performance guarantees, e.g., in terms of link-level data\nrates. A critical challenge in meeting these QoS requirements is the prevention\nof cell congestion, which involves balancing the load to ensure sufficient\nradio resources are available for each cell to serve its designated User\nEquipments (UEs). In this work, a novel QoS-aware Load Balancing (LB) approach\nis developed to optimize the performance of Guaranteed Bit Rate (GBR) and Best\nEffort (BE) traffic in a multi-band Open Radio Access Network (O-RAN) under QoS\nand resource constraints. The proposed solution builds on Graph Reinforcement\nLearning (GRL), a powerful framework at the intersection of Graph Neural\nNetwork (GNN) and RL. The QoS-aware LB is modeled as a Markov Decision Process,\nwith states represented as graphs. QoS consideration are integrated into both\nstate representations and reward signal design. The LB agent is then trained\nusing an off-policy dueling Deep Q Network (DQN) that leverages a GNN-based\narchitecture. This design ensures the LB policy is invariant to the ordering of\nnodes (UE or cell), flexible in handling various network sizes, and capable of\naccounting for spatial node dependencies in LB decisions. Performance of the\nGRL-based solution is compared with two baseline methods. Results show\nsubstantial performance gains, including a $53\\%$ reduction in QoS violations\nand a fourfold increase in the 5th percentile rate for BE traffic.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u5f3a\u5316\u5b66\u4e60\u7684QoS\u611f\u77e5\u8d1f\u8f7d\u5747\u8861\u65b9\u6cd5\uff0c\u4f18\u5316\u591a\u9891\u6bb5O-RAN\u4e2dGBR\u548cBE\u6d41\u91cf\u7684\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4eQoS\u8fdd\u89c4\u5e76\u63d0\u5347\u4f4e\u901f\u7387BE\u6d41\u91cf\u6027\u80fd\u3002", "motivation": "\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u9700\u6ee1\u8db3\u4e25\u683cQoS\u8981\u6c42\uff0c\u4f46\u5c0f\u533a\u62e5\u585e\u662f\u4e3b\u8981\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u8d1f\u8f7d\u5747\u8861\u65b9\u6cd5\u3002", "method": "\u5c06\u8d1f\u8f7d\u5747\u8861\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u7b56\u7565Dueling DQN\u8bad\u7ec3\u667a\u80fd\u4f53\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0cQoS\u8fdd\u89c4\u51cf\u5c1153%\uff0cBE\u6d41\u91cf\u76845\u5206\u4f4d\u901f\u7387\u63d0\u5347\u56db\u500d\u3002", "conclusion": "\u56fe\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u4f18\u5316\u8d1f\u8f7d\u5747\u8861\uff0c\u663e\u8457\u63d0\u5347\u7f51\u7edc\u6027\u80fd\u3002"}}
{"id": "2504.18729", "pdf": "https://arxiv.org/pdf/2504.18729", "abs": "https://arxiv.org/abs/2504.18729", "authors": ["Tung D. Vu", "Chung Hoang", "Truong-Son Hy"], "title": "Multimodal graph representation learning for website generation based on visual sketch", "categories": ["cs.LG"], "comment": null, "summary": "The Design2Code problem, which involves converting digital designs into\nfunctional source code, is a significant challenge in software development due\nto its complexity and time-consuming nature. Traditional approaches often\nstruggle with accurately interpreting the intricate visual details and\nstructural relationships inherent in webpage designs, leading to limitations in\nautomation and efficiency. In this paper, we propose a novel method that\nleverages multimodal graph representation learning to address these challenges.\nBy integrating both visual and structural information from design sketches, our\napproach enhances the accuracy and efficiency of code generation, particularly\nin producing semantically correct and structurally sound HTML code. We present\na comprehensive evaluation of our method, demonstrating significant\nimprovements in both accuracy and efficiency compared to existing techniques.\nExtensive evaluation demonstrates significant improvements of multimodal graph\nlearning over existing techniques, highlighting the potential of our method to\nrevolutionize design-to-code automation. Code available at\nhttps://github.com/HySonLab/Design2Code", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u56fe\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u9ad8\u6548\u5730\u5c06\u8bbe\u8ba1\u56fe\u8f6c\u6362\u4e3aHTML\u4ee3\u7801\uff0c\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u89e3\u6790\u7f51\u9875\u8bbe\u8ba1\u4e2d\u7684\u89c6\u89c9\u548c\u7ed3\u6784\u7ec6\u8282\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u8bbe\u8ba1\u5230\u4ee3\u7801\u7684\u8f6c\u6362\u8fc7\u7a0b\u590d\u6742\u4e14\u8017\u65f6\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528\u591a\u6a21\u6001\u56fe\u8868\u793a\u5b66\u4e60\uff0c\u6574\u5408\u8bbe\u8ba1\u8349\u56fe\u7684\u89c6\u89c9\u548c\u7ed3\u6784\u4fe1\u606f\uff0c\u63d0\u5347\u4ee3\u7801\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u8bed\u4e49\u6b63\u786e\u4e14\u7ed3\u6784\u5408\u7406\u7684HTML\u4ee3\u7801\u65b9\u9762\uff0c\u51c6\u786e\u6027\u548c\u6548\u7387\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u591a\u6a21\u6001\u56fe\u5b66\u4e60\u65b9\u6cd5\u6709\u671b\u9769\u65b0\u8bbe\u8ba1\u5230\u4ee3\u7801\u7684\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u76f8\u5173\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.19162", "pdf": "https://arxiv.org/pdf/2504.19162", "abs": "https://arxiv.org/abs/2504.19162", "authors": ["Jiaqi Chen", "Bang Zhang", "Ruotian Ma", "Peisong Wang", "Xiaodan Liang", "Zhaopeng Tu", "Xiaolong Li", "Kwan-Yee K. Wong"], "title": "SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project: https://chen-judge.github.io/SPC/", "summary": "Evaluating the step-by-step reliability of large language model (LLM)\nreasoning, such as Chain-of-Thought, remains challenging due to the difficulty\nand cost of obtaining high-quality step-level supervision. In this paper, we\nintroduce Self-Play Critic (SPC), a novel approach where a critic model evolves\nits ability to assess reasoning steps through adversarial self-play games,\neliminating the need for manual step-level annotation. SPC involves fine-tuning\ntwo copies of a base model to play two roles, namely a \"sneaky generator\" that\ndeliberately produces erroneous steps designed to be difficult to detect, and a\n\"critic\" that analyzes the correctness of reasoning steps. These two models\nengage in an adversarial game in which the generator aims to fool the critic,\nwhile the critic model seeks to identify the generator's errors. Using\nreinforcement learning based on the game outcomes, the models iteratively\nimprove; the winner of each confrontation receives a positive reward and the\nloser receives a negative reward, driving continuous self-evolution.\nExperiments on three reasoning process benchmarks (ProcessBench, PRM800K,\nDeltaBench) demonstrate that our SPC progressively enhances its error detection\ncapabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and\nsurpasses strong baselines, including distilled R1 model. Furthermore, applying\nSPC to guide the test-time search of diverse LLMs significantly improves their\nmathematical reasoning performance on MATH500 and AIME2024, outperforming\nstate-of-the-art process reward models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSPC\u7684\u81ea\u535a\u5f08\u6279\u8bc4\u6a21\u578b\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u63a8\u7406\u6b65\u9aa4\u7684\u53ef\u9760\u6027\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u901a\u8fc7\u5bf9\u6297\u81ea\u535a\u5f08\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u8bc4\u4f30LLM\u63a8\u7406\u6b65\u9aa4\u7684\u53ef\u9760\u6027\u9700\u8981\u9ad8\u8d28\u91cf\u6807\u6ce8\u4e14\u6210\u672c\u9ad8\uff0c\u8bba\u6587\u5e0c\u671b\u901a\u8fc7\u81ea\u535a\u5f08\u65b9\u5f0f\u81ea\u52a8\u4f18\u5316\u6a21\u578b\u3002", "method": "SPC\u901a\u8fc7\u81ea\u535a\u5f08\u8bad\u7ec3\u4e24\u4e2a\u6a21\u578b\uff0c\u4e00\u4e2a\u751f\u6210\u9519\u8bef\u6b65\u9aa4\uff0c\u53e6\u4e00\u4e2a\u68c0\u6d4b\u9519\u8bef\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u673a\u5236\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u663e\u793aSPC\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9010\u6b65\u63d0\u5347\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\uff08\u5982ProcessBench\u51c6\u786e\u7387\u4ece70.8%\u63d0\u5347\u81f377.7%\uff09\uff0c\u5e76\u663e\u8457\u63d0\u5347LLM\u6570\u5b66\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "SPC\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u6709\u6548\u63d0\u5347LLM\u63a8\u7406\u6b65\u9aa4\u8bc4\u4f30\u80fd\u529b\uff0c\u4e14\u5728\u591a\u9879\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u5e94\u7528\u524d\u666f\u5e7f\u9614\u3002"}}
{"id": "2504.19599", "pdf": "https://arxiv.org/pdf/2504.19599", "abs": "https://arxiv.org/abs/2504.19599", "authors": ["Kaichen Zhang", "Yuzhong Hong", "Junwei Bao", "Hongfei Jiang", "Yang Song", "Dingqian Hong", "Hui Xiong"], "title": "GVPO: Group Variance Policy Optimization for Large Language Model Post-Training", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Post-training plays a crucial role in refining and aligning large language\nmodels to meet specific tasks and human preferences. While recent advancements\nin post-training techniques, such as Group Relative Policy Optimization (GRPO),\nleverage increased sampling with relative reward scoring to achieve superior\nperformance, these methods often suffer from training instability that limits\ntheir practical adoption. To address this challenge, we present Group Variance\nPolicy Optimization (GVPO). GVPO incorporates the analytical solution to\nKL-constrained reward maximization directly into its gradient weights, ensuring\nalignment with the optimal policy. The method provides intuitive physical\ninterpretations: its gradient mirrors the mean squared error between the\ncentral distance of implicit rewards and that of actual rewards. GVPO offers\ntwo key advantages: (1) it guarantees a unique optimal solution, exactly the\nKL-constrained reward maximization objective, (2) it supports flexible sampling\ndistributions that avoids on-policy and importance sampling limitations. By\nunifying theoretical guarantees with practical adaptability, GVPO establishes a\nnew paradigm for reliable and versatile LLM post-training.", "AI": {"tldr": "GVPO\u901a\u8fc7\u5c06KL\u7ea6\u675f\u5956\u52b1\u6700\u5927\u5316\u7684\u89e3\u6790\u89e3\u76f4\u63a5\u7eb3\u5165\u68af\u5ea6\u6743\u91cd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u540e\u8bad\u7ec3\u65b9\u6cd5\uff08\u5982GRPO\uff09\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u8df5\u7075\u6d3b\u6027\u3002", "motivation": "\u4f20\u7edf\u540e\u8bad\u7ec3\u65b9\u6cd5\uff08\u5982GRPO\uff09\u867d\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u5b58\u5728\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002GVPO\u65e8\u5728\u901a\u8fc7\u7406\u8bba\u4f18\u5316\u63d0\u5347\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "GVPO\u5c06KL\u7ea6\u675f\u5956\u52b1\u6700\u5927\u5316\u7684\u89e3\u6790\u89e3\u76f4\u63a5\u6574\u5408\u5230\u68af\u5ea6\u6743\u91cd\u4e2d\uff0c\u5176\u68af\u5ea6\u53cd\u6620\u4e86\u9690\u5f0f\u5956\u52b1\u4e0e\u5b9e\u9645\u5956\u52b1\u7684\u4e2d\u5fc3\u8ddd\u79bb\u5747\u65b9\u8bef\u5dee\uff0c\u652f\u6301\u7075\u6d3b\u7684\u91c7\u6837\u5206\u5e03\u3002", "result": "GVPO\u786e\u4fdd\u4e86\u552f\u4e00\u6700\u4f18\u89e3\uff08\u5373KL\u7ea6\u675f\u5956\u52b1\u6700\u5927\u5316\u76ee\u6807\uff09\uff0c\u5e76\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "GVPO\u901a\u8fc7\u7406\u8bba\u4fdd\u8bc1\u4e0e\u5b9e\u8df5\u7075\u6d3b\u6027\u7684\u7edf\u4e00\uff0c\u4e3aLLM\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u591a\u529f\u80fd\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2504.18735", "pdf": "https://arxiv.org/pdf/2504.18735", "abs": "https://arxiv.org/abs/2504.18735", "authors": ["Tanvir Islam"], "title": "TLoRA: Tri-Matrix Low-Rank Adaptation of Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose TLoRA, a novel tri-matrix low-rank adaptation method that\ndecomposes weight updates into three matrices: two fixed random matrices and\none trainable matrix, combined with a learnable, layer-wise scaling factor.\nThis tri-matrix design enables TLoRA to achieve highly efficient parameter\nadaptation while introducing minimal additional computational overhead. Through\nextensive experiments on the GLUE benchmark, we demonstrate that TLoRA achieves\ncomparable performance to existing low-rank methods such as LoRA and\nAdapter-based techniques, while requiring significantly fewer trainable\nparameters. Analyzing the adaptation dynamics, we observe that TLoRA exhibits\nGaussian-like weight distributions, stable parameter norms, and scaling factor\nvariability across layers, further highlighting its expressive power and\nadaptability. Additionally, we show that TLoRA closely resembles LoRA in its\neigenvalue distributions, parameter norms, and cosine similarity of updates,\nunderscoring its ability to effectively approximate LoRA's adaptation behavior.\nOur results establish TLoRA as a highly efficient and effective fine-tuning\nmethod for LLMs, offering a significant step forward in resource-efficient\nmodel adaptation.", "AI": {"tldr": "TLoRA\u662f\u4e00\u79cd\u65b0\u578b\u7684\u4e09\u77e9\u9635\u4f4e\u79e9\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u6743\u91cd\u66f4\u65b0\u4e3a\u4e09\u4e2a\u77e9\u9635\uff08\u4e24\u4e2a\u56fa\u5b9a\u968f\u673a\u77e9\u9635\u548c\u4e00\u4e2a\u53ef\u8bad\u7ec3\u77e9\u9635\uff09\u5e76\u7ed3\u5408\u53ef\u5b66\u4e60\u7684\u5c42\u95f4\u7f29\u653e\u56e0\u5b50\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u53c2\u6570\u9002\u5e94\u548c\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002\u5728GLUE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5176\u6027\u80fd\u4e0eLoRA\u548cAdapter\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u8bad\u7ec3\u53c2\u6570\u66f4\u5c11\u3002\u5206\u6790\u8868\u660e\uff0cTLoRA\u5177\u6709\u9ad8\u65af\u5206\u5e03\u6743\u91cd\u3001\u7a33\u5b9a\u53c2\u6570\u8303\u6570\u548c\u5c42\u95f4\u7f29\u653e\u56e0\u5b50\u53d8\u5f02\u6027\uff0c\u4e14\u80fd\u6709\u6548\u8fd1\u4f3cLoRA\u7684\u884c\u4e3a\u3002", "motivation": "\u63d0\u51faTLoRA\u7684\u76ee\u7684\u662f\u5728\u4fdd\u8bc1\u6027\u80fd\u7684\u524d\u63d0\u4e0b\uff0c\u8fdb\u4e00\u6b65\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5fae\u8c03\u6240\u9700\u7684\u8bad\u7ec3\u53c2\u6570\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u8d44\u6e90\u5229\u7528\u3002", "method": "TLoRA\u5c06\u6743\u91cd\u66f4\u65b0\u5206\u89e3\u4e3a\u4e24\u4e2a\u56fa\u5b9a\u968f\u673a\u77e9\u9635\u548c\u4e00\u4e2a\u53ef\u8bad\u7ec3\u77e9\u9635\uff0c\u5e76\u5f15\u5165\u5c42\u95f4\u53ef\u5b66\u4e60\u7f29\u653e\u56e0\u5b50\uff0c\u901a\u8fc7\u4e09\u77e9\u9635\u8bbe\u8ba1\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u5728GLUE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTLoRA\u6027\u80fd\u4e0eLoRA\u548cAdapter\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u8bad\u7ec3\u53c2\u6570\u663e\u8457\u51cf\u5c11\uff1b\u5206\u6790\u663e\u793a\u5176\u6743\u91cd\u5206\u5e03\u9ad8\u65af\u5316\u3001\u53c2\u6570\u8303\u6570\u7a33\u5b9a\uff0c\u4e14\u80fd\u6709\u6548\u8fd1\u4f3cLoRA\u884c\u4e3a\u3002", "conclusion": "TLoRA\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684LLM\u5fae\u8c03\u65b9\u6cd5\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u6a21\u578b\u9002\u5e94\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.19191", "pdf": "https://arxiv.org/pdf/2504.19191", "abs": "https://arxiv.org/abs/2504.19191", "authors": ["Liu Xiao", "Li Zhiyuan", "Lin Yueyu"], "title": "WuNeng: Hybrid State with Attention", "categories": ["cs.CL"], "comment": null, "summary": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures.", "AI": {"tldr": "WuNeng\u67b6\u6784\u901a\u8fc7\u6574\u5408RNN-based RWKV-7\u4e0e\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u8868\u8fbe\u529b\u548c\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8868\u8fbe\u529b\u548c\u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u540c\u65f6\u907f\u514d\u8ba1\u7b97\u8d44\u6e90\u7684\u8fc7\u5ea6\u6d88\u8017\uff0cWuNeng\u63d0\u51fa\u4e86\u521b\u65b0\u7684\u67b6\u6784\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "WuNeng\u7ed3\u5408\u4e86RWKV-7\u9a71\u52a8\u7684\u591a\u5934\u6ce8\u610f\u529b\u548c\u8de8\u5934\u4ea4\u4e92\u6280\u672f\uff0c\u901a\u8fc7\u62fc\u63a5\u3001\u8c03\u5236\u548c\u95e8\u63a7\u878d\u5408\u5b9e\u73b0\u9ad8\u6548\u4fe1\u606f\u6574\u5408\uff0c\u5e76\u5229\u7528\u591a\u4ee4\u724c\u72b6\u6001\u5904\u7406\u673a\u5236\u6355\u83b7\u5e8f\u5217\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "WuNeng\u5728\u6781\u5c11\u91cf\u989d\u5916\u53c2\u6570\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u8868\u8fbe\u529b\u548c\u590d\u6742\u4efb\u52a1\u5904\u7406\u80fd\u529b\uff0c\u5c24\u5176\u5728\u63a8\u7406\u548c\u5e8f\u5217\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "WuNeng\u6210\u529f\u5e73\u8861\u4e86\u8868\u8fbe\u529b\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2504.19622", "pdf": "https://arxiv.org/pdf/2504.19622", "abs": "https://arxiv.org/abs/2504.19622", "authors": ["Minsu Kim", "Sangryul Kim", "James Thorne"], "title": "From Evidence to Belief: A Bayesian Epistemology Approach to Language Models", "categories": ["cs.AI"], "comment": null, "summary": "This paper investigates the knowledge of language models from the perspective\nof Bayesian epistemology. We explore how language models adjust their\nconfidence and responses when presented with evidence with varying levels of\ninformativeness and reliability. To study these properties, we create a dataset\nwith various types of evidence and analyze language models' responses and\nconfidence using verbalized confidence, token probability, and sampling. We\nobserved that language models do not consistently follow Bayesian epistemology:\nlanguage models follow the Bayesian confirmation assumption well with true\nevidence but fail to adhere to other Bayesian assumptions when encountering\ndifferent evidence types. Also, we demonstrated that language models can\nexhibit high confidence when given strong evidence, but this does not always\nguarantee high accuracy. Our analysis also reveals that language models are\nbiased toward golden evidence and show varying performance depending on the\ndegree of irrelevance, helping explain why they deviate from Bayesian\nassumptions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ece\u8d1d\u53f6\u65af\u8ba4\u8bc6\u8bba\u89d2\u5ea6\u5206\u6790\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u771f\u5b9e\u8bc1\u636e\u4e0b\u9075\u5faa\u8d1d\u53f6\u65af\u5047\u8bbe\uff0c\u4f46\u5728\u5176\u4ed6\u8bc1\u636e\u7c7b\u578b\u4e2d\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u4e14\u9ad8\u7f6e\u4fe1\u5ea6\u4e0d\u4fdd\u8bc1\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u6839\u636e\u4e0d\u540c\u7c7b\u578b\u548c\u53ef\u9760\u6027\u7684\u8bc1\u636e\u8c03\u6574\u5176\u7f6e\u4fe1\u5ea6\u548c\u54cd\u5e94\uff0c\u4ece\u800c\u7406\u89e3\u5176\u5728\u8d1d\u53f6\u65af\u6846\u67b6\u4e0b\u7684\u8868\u73b0\u548c\u5c40\u9650\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6784\u5efa\u5305\u542b\u591a\u79cd\u8bc1\u636e\u7c7b\u578b\u7684\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u53e3\u5934\u5316\u7f6e\u4fe1\u5ea6\u3001\u4ee4\u724c\u6982\u7387\u548c\u91c7\u6837\u5206\u6790\u8bed\u8a00\u6a21\u578b\u7684\u54cd\u5e94\u548c\u7f6e\u4fe1\u5ea6\u884c\u4e3a\u3002", "result": "\u7ed3\u679c\u663e\u793a\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u8bc1\u636e\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5176\u4ed6\u8bc1\u636e\u7c7b\u578b\u4e2d\u504f\u79bb\u8d1d\u53f6\u65af\u5047\u8bbe\uff0c\u4e14\u9ad8\u7f6e\u4fe1\u5ea6\u4e0e\u51c6\u786e\u6027\u4e0d\u4e00\u81f4\u3002\u6a21\u578b\u5bf9\u9ec4\u91d1\u8bc1\u636e\u5b58\u5728\u504f\u597d\uff0c\u6027\u80fd\u53d7\u65e0\u5173\u6027\u7a0b\u5ea6\u5f71\u54cd\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u8bed\u8a00\u6a21\u578b\u5728\u8d1d\u53f6\u65af\u6846\u67b6\u4e0b\u7684\u8868\u73b0\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u7f6e\u4fe1\u5ea6\u6821\u51c6\u548c\u8bc1\u636e\u5904\u7406\u673a\u5236\u3002"}}
{"id": "2504.18743", "pdf": "https://arxiv.org/pdf/2504.18743", "abs": "https://arxiv.org/abs/2504.18743", "authors": ["Zaiwei Chen"], "title": "Non-Asymptotic Guarantees for Average-Reward Q-Learning with Adaptive Stepsizes", "categories": ["cs.LG", "math.PR", "stat.ML"], "comment": "63 pages and 4 figures", "summary": "This work presents the first finite-time analysis for the last-iterate\nconvergence of average-reward Q-learning with an asynchronous implementation. A\nkey feature of the algorithm we study is the use of adaptive stepsizes, which\nserve as local clocks for each state-action pair. We show that the iterates\ngenerated by this Q-learning algorithm converge at a rate of $O(1/k)$ (in the\nmean-square sense) to the optimal relative Q-function in the span seminorm.\nMoreover, by adding a centering step to the algorithm, we further establish\npointwise mean-square convergence to a centered optimal relative Q-function,\nalso at a rate of $O(1/k)$. To prove these results, we show that adaptive\nstepsizes are necessary, as without them, the algorithm fails to converge to\nthe correct target. In addition, adaptive stepsizes can be interpreted as a\nform of implicit importance sampling that counteracts the effects of\nasynchronous updates.\n  Technically, the use of adaptive stepsizes makes each Q-learning update\ndepend on the entire sample history, introducing strong correlations and making\nthe algorithm a non-Markovian stochastic approximation (SA) scheme. Our\napproach to overcoming this challenge involves (1) a time-inhomogeneous\nMarkovian reformulation of non-Markovian SA, and (2) a combination of\nalmost-sure time-varying bounds, conditioning arguments, and Markov chain\nconcentration inequalities to break the strong correlations between the\nadaptive stepsizes and the iterates. The tools developed in this work are\nlikely to be broadly applicable to the analysis of general SA algorithms with\nadaptive stepsizes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5bf9\u5f02\u6b65\u5b9e\u73b0\u7684\u5e73\u5747\u5956\u52b1Q\u5b66\u4e60\u7684\u6700\u540e\u4e00\u8f6e\u6536\u655b\u8fdb\u884c\u4e86\u6709\u9650\u65f6\u95f4\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u81ea\u9002\u5e94\u6b65\u957f\u5bf9\u6536\u655b\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86O(1/k)\u7684\u6536\u655b\u901f\u7387\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u5f02\u6b65Q\u5b66\u4e60\u4e2d\u81ea\u9002\u5e94\u6b65\u957f\u5bf9\u6536\u655b\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u514b\u670d\u975e\u9a6c\u5c14\u53ef\u592b\u6027\u5e26\u6765\u7684\u5206\u6790\u6311\u6218\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528\u81ea\u9002\u5e94\u6b65\u957f\u4f5c\u4e3a\u5c40\u90e8\u65f6\u949f\uff0c\u5f15\u5165\u5c45\u4e2d\u6b65\u9aa4\uff0c\u4ee5\u53ca\u7ed3\u5408\u65f6\u95f4\u975e\u5747\u5300\u9a6c\u5c14\u53ef\u592b\u91cd\u6784\u548c\u6761\u4ef6\u8bba\u8bc1\u7b49\u6280\u672f\u624b\u6bb5\u3002", "result": "\u7ed3\u679c\u663e\u793a\u7b97\u6cd5\u4ee5O(1/k)\u7684\u901f\u7387\u6536\u655b\u5230\u6700\u4f18\u76f8\u5bf9Q\u51fd\u6570\uff0c\u5e76\u8bc1\u660e\u4e86\u81ea\u9002\u5e94\u6b65\u957f\u5bf9\u6b63\u786e\u6536\u655b\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u7ed3\u8bba\u8ba4\u4e3a\u81ea\u9002\u5e94\u6b65\u957f\u662f\u5173\u952e\uff0c\u4e14\u6240\u5f00\u53d1\u7684\u5206\u6790\u5de5\u5177\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5176\u4ed6\u5177\u6709\u81ea\u9002\u5e94\u6b65\u957f\u7684\u968f\u673a\u903c\u8fd1\u7b97\u6cd5\u3002"}}
{"id": "2504.19209", "pdf": "https://arxiv.org/pdf/2504.19209", "abs": "https://arxiv.org/abs/2504.19209", "authors": ["Elisabeth Fittschen", "Bella Xia", "Leib Celnik", "Paul Dilley", "Tom Lippincott"], "title": "Dynamic Embedded Topic Models: properties and recommendations based on diverse corpora", "categories": ["cs.CL", "cs.LG"], "comment": "Under review", "summary": "We measure the effects of several implementation choices for the Dynamic\nEmbedded Topic Model, as applied to five distinct diachronic corpora, with the\ngoal of isolating important decisions for its use and further development. We\nidentify priorities that will maximize utility in applied scholarship,\nincluding the practical scalability of vocabulary size to best exploit the\nstrengths of embedded representations, and more flexible modeling of intervals\nto accommodate the uneven temporal distributions of historical writing. Of\nsimilar importance, we find performance is not significantly or consistently\naffected by several aspects that otherwise limit the model's application or\nmight consume the resources of a grid search.", "AI": {"tldr": "\u7814\u7a76\u4e86\u52a8\u6001\u5d4c\u5165\u4e3b\u9898\u6a21\u578b\u5728\u4e0d\u540c\u5386\u65f6\u8bed\u6599\u5e93\u4e2d\u7684\u5b9e\u73b0\u9009\u62e9\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4f18\u5316\u5b9e\u7528\u6027\u548c\u5f00\u53d1\u65b9\u5411\u7684\u5efa\u8bae\u3002", "motivation": "\u76ee\u6807\u662f\u8bc6\u522b\u52a8\u6001\u5d4c\u5165\u4e3b\u9898\u6a21\u578b\u5728\u5e94\u7528\u4e2d\u6700\u91cd\u8981\u7684\u51b3\u7b56\u70b9\uff0c\u4ee5\u4f18\u5316\u5176\u5728\u5b66\u672f\u7814\u7a76\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u901a\u8fc7\u5e94\u7528\u4e8e\u4e94\u4e2a\u4e0d\u540c\u7684\u5386\u65f6\u8bed\u6599\u5e93\uff0c\u5206\u6790\u4e0d\u540c\u5b9e\u73b0\u9009\u62e9\u7684\u6548\u679c\u3002", "result": "\u53d1\u73b0\u8bcd\u6c47\u91cf\u7684\u5b9e\u9645\u53ef\u6269\u5c55\u6027\u548c\u65f6\u95f4\u95f4\u9694\u7684\u7075\u6d3b\u5efa\u6a21\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u800c\u67d0\u4e9b\u9650\u5236\u6027\u56e0\u7d20\u5bf9\u6027\u80fd\u5f71\u54cd\u4e0d\u5927\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4f18\u5316\u52a8\u6001\u5d4c\u5165\u4e3b\u9898\u6a21\u578b\u5b9e\u7528\u6027\u7684\u5177\u4f53\u5efa\u8bae\uff0c\u5e76\u6307\u51fa\u4e86\u4e00\u4e9b\u4e0d\u663e\u8457\u5f71\u54cd\u6027\u80fd\u7684\u56e0\u7d20\uff0c\u53ef\u4e3a\u540e\u7eed\u5f00\u53d1\u548c\u7f51\u683c\u641c\u7d22\u8282\u7701\u8d44\u6e90\u3002"}}
{"id": "2504.19636", "pdf": "https://arxiv.org/pdf/2504.19636", "abs": "https://arxiv.org/abs/2504.19636", "authors": ["Fei Liu", "Qingfu Zhang", "Xialiang Tong", "Mingxuan Yuan", "Kun Mao"], "title": "Fitness Landscape of Large Language Model-Assisted Automated Algorithm Search", "categories": ["cs.AI", "cs.NE"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant potential in\nalgorithm design. However, when integrated into search frameworks for iterative\nalgorithm search, the underlying fitness landscape--critical for understanding\nsearch behaviou--remains underexplored. In this paper, we illustrate and\nanalyze the fitness landscape of LLM-assisted Algorithm Search (LAS) using a\ngraph-based approach, where nodes represent algorithms and edges denote\ntransitions between them. We conduct extensive evaluations across six algorithm\ndesign tasks and six commonly used LLMs. Our findings reveal that LAS\nlandscapes are highly multimodal and rugged, particularly in combinatorial\noptimization tasks, with distinct structural variations across tasks and LLMs.\nFor instance, heuristic design tasks exhibit dense clusters of high-performing\nalgorithms, while symbolic regression tasks show sparse, scattered\ndistributions. Additionally, we demonstrate how population size influences\nexploration-exploitation trade-offs and the evolving trajectory of elite\nalgorithms. These insights not only advance our understanding of LAS landscapes\nbut also provide practical guidance for designing more effective LAS methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86LLM\u8f85\u52a9\u7b97\u6cd5\u641c\u7d22\uff08LAS\uff09\u7684\u9002\u5e94\u5ea6\u666f\u89c2\uff0c\u63ed\u793a\u5176\u591a\u6a21\u6001\u548c\u5d0e\u5c96\u7279\u6027\uff0c\u5e76\u63d0\u51fa\u5b9e\u8df5\u5efa\u8bae\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7b97\u6cd5\u8bbe\u8ba1\u4e2d\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u641c\u7d22\u6846\u67b6\u4e2d\u7684\u9002\u5e94\u5ea6\u666f\u89c2\u7f3a\u4e4f\u7814\u7a76\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\uff0c\u8282\u70b9\u8868\u793a\u7b97\u6cd5\uff0c\u8fb9\u8868\u793a\u8f6c\u6362\uff0c\u8bc4\u4f30\u4e86\u516d\u4e2a\u7b97\u6cd5\u8bbe\u8ba1\u4efb\u52a1\u548c\u516d\u4e2aLLMs\u3002", "result": "LAS\u666f\u89c2\u9ad8\u5ea6\u591a\u6a21\u6001\u4e14\u5d0e\u5c96\uff0c\u4efb\u52a1\u548cLLM\u95f4\u7ed3\u6784\u5dee\u5f02\u663e\u8457\uff0c\u79cd\u7fa4\u5927\u5c0f\u5f71\u54cd\u63a2\u7d22-\u5f00\u53d1\u6743\u8861\u3002", "conclusion": "\u7814\u7a76\u589e\u8fdb\u4e86\u5bf9LAS\u666f\u89c2\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u66f4\u6709\u6548\u7684LAS\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2504.18758", "pdf": "https://arxiv.org/pdf/2504.18758", "abs": "https://arxiv.org/abs/2504.18758", "authors": ["Ling Wang", "Minglian Han"], "title": "High-order Graph Neural Networks with Common Neighbor Awareness for Link Prediction", "categories": ["cs.LG"], "comment": "Accepted By ICAIS&ISAS 2025", "summary": "Link prediction is a fundamental task in dynamic graph learning (DGL),\ninherently shaped by the topology of the DG. Recent advancements in dynamic\ngraph neural networks (DGNN), primarily by modeling the relationships among\nnodes via a message passing scheme, have significantly improved link prediction\nperformance. However, DGNNs heavily rely on the pairwise node interactions,\nwhich neglect the common neighbor interaction in DGL. To address this\nlimitation, we propose a High-order Graph Neural Networks with Common Neighbor\nAwareness (HGNN-CNA) for link prediction with two-fold ideas: a) estimating\ncorrelation score by considering multi-hop common neighbors for capturing the\ncomplex interaction between nodes; b) fusing the correlation into the\nmessage-passing process to consider common neighbor interaction directly in\nDGL. Experimental results on three real DGs demonstrate that the proposed\nHGNN-CNA acquires a significant accuracy gain over several state-of-the-art\nmodels on the link prediction task.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u9636\u56fe\u795e\u7ecf\u7f51\u7edcHGNN-CNA\uff0c\u901a\u8fc7\u8003\u8651\u591a\u8df3\u5171\u540c\u90bb\u5c45\u6765\u6539\u8fdb\u52a8\u6001\u56fe\u4e2d\u7684\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u52a8\u6001\u56fe\u795e\u7ecf\u7f51\u7edc\u4e3b\u8981\u4f9d\u8d56\u8282\u70b9\u95f4\u7684\u6210\u5bf9\u4ea4\u4e92\uff0c\u5ffd\u7565\u4e86\u5171\u540c\u90bb\u5c45\u7684\u4f5c\u7528\uff0c\u5bfc\u81f4\u94fe\u63a5\u9884\u6d4b\u6027\u80fd\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u4e86HGNN-CNA\u65b9\u6cd5\uff0c\u5305\u62ec\uff1a\uff081\uff09\u5229\u7528\u591a\u8df3\u5171\u540c\u90bb\u5c45\u8ba1\u7b97\u76f8\u5173\u6027\u5206\u6570\uff1b\uff082\uff09\u5c06\u76f8\u5173\u6027\u878d\u5165\u6d88\u606f\u4f20\u9012\u8fc7\u7a0b\uff0c\u76f4\u63a5\u5efa\u6a21\u5171\u540c\u90bb\u5c45\u4ea4\u4e92\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u52a8\u6001\u56fe\u6570\u636e\u96c6\u4e0a\uff0cHGNN-CNA\u7684\u94fe\u63a5\u9884\u6d4b\u51c6\u786e\u7387\u663e\u8457\u8d85\u8fc7\u591a\u4e2a\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "HGNN-CNA\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5171\u540c\u90bb\u5c45\u4ea4\u4e92\uff0c\u6709\u6548\u63d0\u5347\u4e86\u52a8\u6001\u56fe\u4e2d\u7684\u94fe\u63a5\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2504.19254", "pdf": "https://arxiv.org/pdf/2504.19254", "abs": "https://arxiv.org/abs/2504.19254", "authors": ["Dylan Bouchard", "Mohit Singh Chauhan"], "title": "Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "UQLM repository: https://github.com/cvs-health/uqlm", "summary": "Hallucinations are a persistent problem with Large Language Models (LLMs). As\nthese models become increasingly used in high-stakes domains, such as\nhealthcare and finance, the need for effective hallucination detection is\ncrucial. To this end, we propose a versatile framework for zero-resource\nhallucination detection that practitioners can apply to real-world use cases.\nTo achieve this, we adapt a variety of existing uncertainty quantification (UQ)\ntechniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,\ntransforming them as necessary into standardized response-level confidence\nscores ranging from 0 to 1. To enhance flexibility, we introduce a tunable\nensemble approach that incorporates any combination of the individual\nconfidence scores. This approach enables practitioners to optimize the ensemble\nfor a specific use case for improved performance. To streamline implementation,\nthe full suite of scorers is offered in this paper's companion Python toolkit,\nUQLM. To evaluate the performance of the various scorers, we conduct an\nextensive set of experiments using several LLM question-answering benchmarks.\nWe find that our tunable ensemble typically surpasses its individual components\nand outperforms existing hallucination detection methods. Our results\ndemonstrate the benefits of customized hallucination detection strategies for\nimproving the accuracy and reliability of LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u96f6\u8d44\u6e90\u5e7b\u89c9\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u8c03\u6574\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6280\u672f\u5e76\u5f15\u5165\u53ef\u8c03\u96c6\u6210\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u533b\u7597\u548c\u91d1\u878d\uff09\u7684\u5e94\u7528\u589e\u591a\uff0c\u5e7b\u89c9\uff08\u865a\u5047\u4fe1\u606f\uff09\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u4e9f\u9700\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u4ee5\u63d0\u5347\u6a21\u578b\u53ef\u9760\u6027\u3002", "method": "\u7ed3\u5408\u9ed1\u76d2/\u767d\u76d2\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6280\u672f\u53caLLM-as-a-Judge\u65b9\u6cd5\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u6807\u51c6\u5316\u7f6e\u4fe1\u5206\u6570\uff080-1\uff09\uff0c\u5e76\u63d0\u51fa\u53ef\u8c03\u96c6\u6210\u7b56\u7565\u4ee5\u4f18\u5316\u7279\u5b9a\u573a\u666f\u4e0b\u7684\u68c0\u6d4b\u6027\u80fd\u3002\u914d\u5957\u5de5\u5177\u5305UQLM\u5b9e\u73b0\u5feb\u901f\u90e8\u7f72\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u53ef\u8c03\u96c6\u6210\u65b9\u6cd5\u5728\u591a\u4e2a\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5355\u4e00\u6280\u672f\u53ca\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5b9a\u5236\u5316\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7075\u6d3b\u96c6\u6210\u591a\u79cd\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u5e7b\u89c9\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u53ef\u9760\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u914d\u5957\u5de5\u5177\u5305\u8fdb\u4e00\u6b65\u964d\u4f4e\u4e86\u843d\u5730\u95e8\u69db\u3002"}}
{"id": "2504.19678", "pdf": "https://arxiv.org/pdf/2504.19678", "abs": "https://arxiv.org/abs/2504.19678", "authors": ["Mohamed Amine Ferrag", "Norbert Tihanyi", "Merouane Debbah"], "title": "From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large language models and autonomous AI agents have evolved rapidly,\nresulting in a diverse array of evaluation benchmarks, frameworks, and\ncollaboration protocols. However, the landscape remains fragmented and lacks a\nunified taxonomy or comprehensive survey. Therefore, we present a side-by-side\ncomparison of benchmarks developed between 2019 and 2025 that evaluate these\nmodels and agents across multiple domains. In addition, we propose a taxonomy\nof approximately 60 benchmarks that cover general and academic knowledge\nreasoning, mathematical problem-solving, code generation and software\nengineering, factual grounding and retrieval, domain-specific evaluations,\nmultimodal and embodied tasks, task orchestration, and interactive assessments.\nFurthermore, we review AI-agent frameworks introduced between 2023 and 2025\nthat integrate large language models with modular toolkits to enable autonomous\ndecision-making and multi-step reasoning. Moreover, we present real-world\napplications of autonomous AI agents in materials science, biomedical research,\nacademic ideation, software engineering, synthetic data generation, chemical\nreasoning, mathematical problem-solving, geographic information systems,\nmultimedia, healthcare, and finance. We then survey key agent-to-agent\ncollaboration protocols, namely the Agent Communication Protocol (ACP), the\nModel Context Protocol (MCP), and the Agent-to-Agent Protocol (A2A). Finally,\nwe discuss recommendations for future research, focusing on advanced reasoning\nstrategies, failure modes in multi-agent LLM systems, automated scientific\ndiscovery, dynamic tool integration via reinforcement learning, integrated\nsearch capabilities, and security vulnerabilities in agent protocols.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5206\u7c7b\u6cd5\uff0c\u5bf92019-2025\u5e74\u95f4\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u548c\u81ea\u4e3bAI\u4ee3\u7406\u7684\u57fa\u51c6\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u5e76\u7efc\u8ff0\u4e862023-2025\u5e74\u7684AI\u4ee3\u7406\u6846\u67b6\u53ca\u5176\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u548c\u81ea\u4e3bAI\u4ee3\u7406\u7684\u8bc4\u4f30\u57fa\u51c6\u548c\u6846\u67b6\u7f3a\u4e4f\u7edf\u4e00\u7684\u5206\u7c7b\u548c\u5168\u9762\u8c03\u7814\uff0c\u5bfc\u81f4\u7814\u7a76\u73af\u5883\u5206\u6563\u3002", "method": "\u91c7\u7528\u4e86\u5206\u7c7b\u6cd5\u5bf9\u6bd460\u591a\u4e2a\u57fa\u51c6\uff0c\u5e76\u7efc\u8ff0\u4e86AI\u4ee3\u7406\u6846\u67b6\u53ca\u591a\u9886\u57df\u5e94\u7528\u3002", "result": "\u63d0\u51fa\u4e86\u6db5\u76d6\u591a\u9886\u57df\u7684\u57fa\u51c6\u5206\u7c7b\u6cd5\u548c\u6846\u67b6\u7efc\u8ff0\uff0c\u5e76\u63a2\u8ba8\u4e86\u4ee3\u7406\u95f4\u534f\u4f5c\u534f\u8bae\u7684\u5b9e\u9645\u6848\u4f8b\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u9ad8\u7ea7\u63a8\u7406\u7b56\u7565\u3001\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u5931\u6548\u6a21\u5f0f\u53ca\u81ea\u52a8\u5316\u79d1\u5b66\u53d1\u73b0\u7b49\u65b9\u5411\u3002"}}
{"id": "2504.18766", "pdf": "https://arxiv.org/pdf/2504.18766", "abs": "https://arxiv.org/abs/2504.18766", "authors": ["Wenjun Cao"], "title": "Dynamic Action Interpolation: A Universal Approach for Accelerating Reinforcement Learning with Expert Guidance", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) suffers from severe sample inefficiency,\nespecially during early training, requiring extensive environmental\ninteractions to perform competently. Existing methods tend to solve this by\nincorporating prior knowledge, but introduce significant architectural and\nimplementation complexity. We propose Dynamic Action Interpolation (DAI), a\nuniversal yet straightforward framework that interpolates expert and RL actions\nvia a time-varying weight $\\alpha(t)$, integrating into any Actor-Critic\nalgorithm with just a few lines of code and without auxiliary networks or\nadditional losses. Our theoretical analysis shows that DAI reshapes state\nvisitation distributions to accelerate value function learning while preserving\nconvergence guarantees. Empirical evaluations across MuJoCo continuous control\ntasks demonstrate that DAI improves early-stage performance by over 160\\% on\naverage and final performance by more than 50\\%, with the Humanoid task showing\na 4$\\times$ improvement early on and a 2$\\times$ gain at convergence. These\nresults challenge the assumption that complex architectural modifications are\nnecessary for sample-efficient reinforcement learning.", "AI": {"tldr": "DAI\u662f\u4e00\u4e2a\u7b80\u5355\u901a\u7528\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u4e13\u5bb6\u548cRL\u52a8\u4f5c\u95f4\u8fdb\u884c\u52a8\u6001\u63d2\u503c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u521d\u671f\u548c\u6700\u7ec8\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u590d\u6742\u67b6\u6784\u4fee\u6539\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u65e9\u671f\u8bad\u7ec3\u4e2d\u6837\u672c\u6548\u7387\u4f4e\u4e0b\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5f15\u5165\u590d\u6742\u67b6\u6784\u3002\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u901a\u8fc7\u7b80\u5355\u65b9\u6cd5\u63d0\u5347\u6837\u672c\u6548\u7387\u800c\u4e0d\u589e\u52a0\u590d\u6742\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u52a8\u6001\u52a8\u4f5c\u63d2\u503c\uff08DAI\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u53d8\u6743\u91cd\u03b1(t)\u63d2\u503c\u4e13\u5bb6\u548cRL\u52a8\u4f5c\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u4efb\u4f55Actor-Critic\u7b97\u6cd5\u4e2d\u3002", "result": "\u5728MuJoCo\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\uff0cDAI\u5c06\u521d\u671f\u6027\u80fd\u5e73\u5747\u63d0\u5347160%\uff0c\u6700\u7ec8\u6027\u80fd\u63d0\u534750%\uff0c\u90e8\u5206\u4efb\u52a1\u65e9\u671f\u6027\u80fd\u63d0\u53474\u500d\u3002", "conclusion": "DAI\u8bc1\u660e\u4e86\u5728\u4e0d\u589e\u52a0\u67b6\u6784\u590d\u6742\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u53ef\u4ee5\u901a\u8fc7\u7b80\u5355\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u3002"}}
{"id": "2504.19267", "pdf": "https://arxiv.org/pdf/2504.19267", "abs": "https://arxiv.org/abs/2504.19267", "authors": ["Mohamed Gado", "Towhid Taliee", "Muhammad Memon", "Dmitry Ignatov", "Radu Timofte"], "title": "VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Visual storytelling is an interdisciplinary field combining computer vision\nand natural language processing to generate cohesive narratives from sequences\nof images. This paper presents a novel approach that leverages recent\nadvancements in multimodal models, specifically adapting transformer-based\narchitectures and large multimodal models, for the visual storytelling task.\nLeveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT\nmodel produces visually grounded, contextually appropriate narratives. We\naddress the limitations of traditional evaluation metrics, such as BLEU,\nMETEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we\nutilize RoViST and GROOVIST, novel reference-free metrics designed to assess\nvisual storytelling, focusing on visual grounding, coherence, and\nnon-redundancy. These metrics provide a more nuanced evaluation of narrative\nquality, aligning closely with human judgment.", "AI": {"tldr": "\u8be5\u6458\u8981\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVIST-GPT\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6a21\u578b\u751f\u6210\u56fe\u50cf\u5e8f\u5217\u7684\u8fde\u8d2f\u53d9\u8ff0\uff0c\u5e76\u9488\u5bf9\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u7684\u4e0d\u8db3\u5f15\u5165\u4e86RoViST\u548cGROOVIST\u8fd9\u4e24\u79cd\u65b0\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u9488\u5bf9\u89c6\u89c9\u53d9\u4e8b\u4efb\u52a1\u4e2d\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u7684\u4e0d\u9002\u7528\u6027\uff0c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u66f4\u51c6\u786e\u53cd\u6620\u53d9\u8ff0\u8d28\u91cf\u7684\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u89c6\u89c9\u57fa\u7840\u3001\u8fde\u8d2f\u6027\u548c\u975e\u5197\u4f59\u6027\u3002", "method": "\u5229\u7528\u57fa\u4e8etransformer\u7684\u591a\u6a21\u6001\u6a21\u578b\u548c\u5927\u89c4\u6a21\u89c6\u89c9\u53d9\u4e8b\uff08VIST\uff09\u6570\u636e\u96c6\uff0c\u6784\u5efaVIST-GPT\u6a21\u578b\u751f\u6210\u53d9\u8ff0\u3002", "result": "\u5f15\u5165\u4e86RoViST\u548cGROOVIST\u8fd9\u4e24\u79cd\u65b0\u6307\u6807\uff0c\u5b83\u4eec\u80fd\u66f4\u597d\u5730\u8bc4\u4f30\u53d9\u8ff0\u8d28\u91cf\u5e76\u4e0e\u4eba\u7c7b\u5224\u65ad\u66f4\u4e00\u81f4\u3002", "conclusion": "VIST-GPT\u6a21\u578b\u7ed3\u5408\u65b0\u8bc4\u4f30\u6307\u6807\uff0c\u4e3a\u89c6\u89c9\u53d9\u4e8b\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.19738", "pdf": "https://arxiv.org/pdf/2504.19738", "abs": "https://arxiv.org/abs/2504.19738", "authors": ["Yingbin Bai", "Sylvie Thiebaux", "Felipe Trevizan"], "title": "Learning Efficiency Meets Symmetry Breaking", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Learning-based planners leveraging Graph Neural Networks can learn search\nguidance applicable to large search spaces, yet their potential to address\nsymmetries remains largely unexplored. In this paper, we introduce a graph\nrepresentation of planning problems allying learning efficiency with the\nability to detect symmetries, along with two pruning methods, action pruning\nand state pruning, designed to manage symmetries during search. The integration\nof these techniques into Fast Downward achieves a first-time success over LAMA\non the latest IPC learning track dataset. Code is released at:\nhttps://github.com/bybeye/Distincter.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u56fe\u8868\u793a\u548c\u4e24\u79cd\u526a\u679d\u65b9\u6cd5\uff08\u52a8\u4f5c\u526a\u679d\u548c\u72b6\u6001\u526a\u679d\uff09\u6709\u6548\u5904\u7406\u5bf9\u79f0\u6027\u95ee\u9898\uff0c\u5e76\u5728Fast Downward\u4e2d\u5b9e\u73b0\uff0c\u9996\u6b21\u5728IPC\u5b66\u4e60\u8f68\u9053\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86LAMA\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u89c4\u5212\u5668\u5728\u5927\u578b\u641c\u7d22\u7a7a\u95f4\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u79f0\u6027\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u5b66\u4e60\u6548\u7387\u548c\u5bf9\u79f0\u6027\u68c0\u6d4b\uff0c\u63d0\u5347\u89c4\u5212\u5668\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u8868\u793a\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u52a8\u4f5c\u526a\u679d\u548c\u72b6\u6001\u526a\u679d\u4e24\u79cd\u65b9\u6cd5\u6765\u7ba1\u7406\u641c\u7d22\u8fc7\u7a0b\u4e2d\u7684\u5bf9\u79f0\u6027\u3002", "result": "\u5728Fast Downward\u4e2d\u96c6\u6210\u8fd9\u4e9b\u6280\u672f\u540e\uff0c\u9996\u6b21\u5728IPC\u5b66\u4e60\u8f68\u9053\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86LAMA\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u56fe\u8868\u793a\u548c\u5bf9\u79f0\u6027\u526a\u679d\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c4\u5212\u5668\u7684\u6027\u80fd\uff0c\u4e3a\u5bf9\u79f0\u6027\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18771", "pdf": "https://arxiv.org/pdf/2504.18771", "abs": "https://arxiv.org/abs/2504.18771", "authors": ["Markus Haug", "Gissel Velarde"], "title": "Performance of Machine Learning Classifiers for Anomaly Detection in Cyber Security Applications", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "This work empirically evaluates machine learning models on two imbalanced\npublic datasets (KDDCUP99 and Credit Card Fraud 2013). The method includes data\npreparation, model training, and evaluation, using an 80/20 (train/test) split.\nModels tested include eXtreme Gradient Boosting (XGB), Multi Layer Perceptron\n(MLP), Generative Adversarial Network (GAN), Variational Autoencoder (VAE), and\nMultiple-Objective Generative Adversarial Active Learning (MO-GAAL), with XGB\nand MLP further combined with Random-Over-Sampling (ROS) and\nSelf-Paced-Ensemble (SPE). Evaluation involves 5-fold cross-validation and\nimputation techniques (mean, median, and IterativeImputer) with 10, 20, 30, and\n50 % missing data. Findings show XGB and MLP outperform generative models.\nIterativeImputer results are comparable to mean and median, but not recommended\nfor large datasets due to increased complexity and execution time. The code\nused is publicly available on GitHub (github.com/markushaug/acr-25).", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u4e24\u4e2a\u4e0d\u5e73\u8861\u516c\u5f00\u6570\u636e\u96c6\uff08KDDCUP99\u548c\u4fe1\u7528\u5361\u6b3a\u8bc82013\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0XGB\u548cMLP\u4f18\u4e8e\u751f\u6210\u6a21\u578b\uff0c\u540c\u65f6\u4e0d\u63a8\u8350\u5728\u5927\u6570\u636e\u96c6\u4e0a\u4f7f\u7528IterativeImputer\u3002", "motivation": "\u52a8\u673a\u662f\u901a\u8fc7\u5b9e\u8bc1\u8bc4\u4f30\u4e0d\u540c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u9009\u62e9\u5408\u9002\u7684\u6a21\u578b\u548c\u65b9\u6cd5\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6570\u636e\u51c6\u5907\u3001\u6a21\u578b\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u4f7f\u752880/20\u7684\u8bad\u7ec3/\u6d4b\u8bd5\u5206\u5272\uff0c\u6d4b\u8bd5\u4e86XGB\u3001MLP\u3001GAN\u3001VAE\u548cMO-GAAL\u7b49\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u4e86ROS\u548cSPE\u6280\u672f\u3002\u8bc4\u4f30\u91c7\u7528\u4e865\u6298\u4ea4\u53c9\u9a8c\u8bc1\u548c\u591a\u79cd\u7f3a\u5931\u503c\u586b\u8865\u6280\u672f\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cXGB\u548cMLP\u7684\u8868\u73b0\u4f18\u4e8e\u751f\u6210\u6a21\u578b\u3002IterativeImputer\u7684\u586b\u8865\u6548\u679c\u4e0e\u5747\u503c\u548c\u4e2d\u4f4d\u6570\u586b\u8865\u76f8\u5f53\uff0c\u4f46\u4e0d\u63a8\u8350\u7528\u4e8e\u5927\u6570\u636e\u96c6\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51faXGB\u548cMLP\u662f\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u7684\u4f18\u9009\u6a21\u578b\uff0c\u540c\u65f6\u5efa\u8bae\u6839\u636e\u6570\u636e\u96c6\u89c4\u6a21\u9009\u62e9\u5408\u9002\u7684\u7f3a\u5931\u503c\u586b\u8865\u65b9\u6cd5\u3002"}}
{"id": "2504.19298", "pdf": "https://arxiv.org/pdf/2504.19298", "abs": "https://arxiv.org/abs/2504.19298", "authors": ["Hanyu Lai", "Junjie Gao", "Xiao Liu", "Yifan Xu", "Shudan Zhang", "Yuxiao Dong", "Jie Tang"], "title": "AndroidGen: Building an Android Language Agent under Data Scarcity", "categories": ["cs.CL"], "comment": null, "summary": "Large language models have opened up a world of possibilities for various NLP\ntasks, sparking optimism for the future. Despite their potential, LLMs have yet\nto be widely used as agents on real mobile devices. The main challenge is the\nneed for high-quality data sources. Time constraints and labor intensity often\nhinder human annotation. On the other hand, existing LLMs exhibit inadequate\ncompletion rates and need a robust data filtration strategy. Given these\nchallenges, we develop a framework called AndroidGen to enhance the\ncapabilities of LLM-based agents under data scarcity. In addition, we leverage\nAndroidGen to collect trajectories given human tasks and train open-source LLMs\non these trajectories to develop an open-source mobile agent without manually\nlabeled trajectories. We extensively evaluate AndroidGen with AndroidWorld,\nAitW, and various popular applications, demonstrating its improvements and\nrevealing potential areas for future improvement. Code, model, and data are\navailable at https://github.com/THUDM/AndroidGen.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86AndroidGen\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u4f5c\u4e3a\u4ee3\u7406\u65f6\u9762\u4e34\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u901a\u8fc7\u6536\u96c6\u4efb\u52a1\u8f68\u8ff9\u5e76\u8bad\u7ec3\u5f00\u6e90LLMs\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u80fd\u529b\uff0c\u800c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728NLP\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u9ad8\u8d28\u91cf\u6570\u636e\u6e90\u7684\u7f3a\u4e4f\uff0c\u5b83\u4eec\u5c1a\u672a\u5728\u771f\u5b9e\u79fb\u52a8\u8bbe\u5907\u4e0a\u5e7f\u6cdb\u5e94\u7528\u3002\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\uff0c\u73b0\u6709LLMs\u7684\u5b8c\u6210\u7387\u4e0d\u8db3\u4e14\u7f3a\u4e4f\u6709\u6548\u6570\u636e\u8fc7\u6ee4\u7b56\u7565\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u65b0\u6846\u67b6\u4ee5\u589e\u5f3aLLM\u5728\u6570\u636e\u7a00\u7f3a\u73af\u5883\u4e2d\u7684\u80fd\u529b\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86AndroidGen\u6846\u67b6\uff0c\u901a\u8fc7\u6536\u96c6\u4eba\u7c7b\u4efb\u52a1\u751f\u6210\u7684\u8f68\u8ff9\u6570\u636e\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u6570\u636e\u8bad\u7ec3\u5f00\u6e90LLMs\uff0c\u4ece\u800c\u5f00\u53d1\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u5f00\u6e90\u79fb\u52a8\u4ee3\u7406\u3002", "result": "\u901a\u8fc7AndroidWorld\u3001AitW\u53ca\u591a\u6b3e\u6d41\u884c\u5e94\u7528\u7684\u5e7f\u6cdb\u8bc4\u4f30\uff0cAndroidGen\u8868\u73b0\u51fa\u4e86\u663e\u8457\u7684\u80fd\u529b\u63d0\u5347\uff0c\u5e76\u63ed\u793a\u4e86\u672a\u6765\u6539\u8fdb\u7684\u6f5c\u5728\u65b9\u5411\u3002\u76f8\u5173\u4ee3\u7801\u3001\u6a21\u578b\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002", "conclusion": "AndroidGen\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLMs\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u4f5c\u4e3a\u4ee3\u7406\u65f6\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u901a\u8fc7\u65e0\u6807\u6ce8\u8f68\u8ff9\u6570\u636e\u8bad\u7ec3\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u4e5f\u6307\u51fa\u4e86\u672a\u6765\u8fdb\u4e00\u6b65\u4f18\u5316\u7684\u65b9\u5411\u3002"}}
{"id": "2504.19912", "pdf": "https://arxiv.org/pdf/2504.19912", "abs": "https://arxiv.org/abs/2504.19912", "authors": ["Khachik Smbatyan", "Tsolak Ghukasyan", "Tigran Aghajanyan", "Hovhannes Dabaghyan", "Sergey Adamyan", "Aram Bughdaryan", "Vahagn Altunyan", "Gagik Navasardyan", "Aram Davtyan", "Anush Hakobyan", "Aram Gharibyan", "Arman Fahradyan", "Artur Hakobyan", "Hasmik Mnatsakanyan", "Narek Ginoyan", "Garik Petrosyan"], "title": "Can AI Agents Design and Implement Drug Discovery Pipelines?", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "The rapid advancement of artificial intelligence, particularly autonomous\nagentic systems based on Large Language Models (LLMs), presents new\nopportunities to accelerate drug discovery by improving in-silico modeling and\nreducing dependence on costly experimental trials. Current AI agent-based\nsystems demonstrate proficiency in solving programming challenges and\nconducting research, indicating an emerging potential to develop software\ncapable of addressing complex problems such as pharmaceutical design and drug\ndiscovery. This paper introduces DO Challenge, a benchmark designed to evaluate\nthe decision-making abilities of AI agents in a single, complex problem\nresembling virtual screening scenarios. The benchmark challenges systems to\nindependently develop, implement, and execute efficient strategies for\nidentifying promising molecular structures from extensive datasets, while\nnavigating chemical space, selecting models, and managing limited resources in\na multi-objective context. We also discuss insights from the DO Challenge 2025,\na competition based on the proposed benchmark, which showcased diverse\nstrategies explored by human participants. Furthermore, we present the Deep\nThought multi-agent system, which demonstrated strong performance on the\nbenchmark, outperforming most human teams. Among the language models tested,\nClaude 3.7 Sonnet, Gemini 2.5 Pro and o3 performed best in primary agent roles,\nand GPT-4o, Gemini 2.0 Flash were effective in auxiliary roles. While\npromising, the system's performance still fell short of expert-designed\nsolutions and showed high instability, highlighting both the potential and\ncurrent limitations of AI-driven methodologies in transforming drug discovery\nand broader scientific research.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86DO Challenge\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u591a\u4ee3\u7406\u7cfb\u7edfDeep Thought\u7684\u4f18\u8d8a\u8868\u73b0\u3002", "motivation": "\u5229\u7528AI\u4ee3\u7406\u52a0\u901f\u836f\u7269\u53d1\u73b0\uff0c\u51cf\u5c11\u9ad8\u6210\u672c\u7684\u5b9e\u9a8c\u4f9d\u8d56\uff0c\u901a\u8fc7\u865a\u62df\u7b5b\u9009\u7b49\u590d\u6742\u4efb\u52a1\u9a8c\u8bc1\u5176\u51b3\u7b56\u80fd\u529b\u3002", "method": "\u63d0\u51faDO Challenge\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6a21\u62df\u865a\u62df\u7b5b\u9009\u573a\u666f\uff0c\u8981\u6c42AI\u4ee3\u7406\u72ec\u7acb\u5f00\u53d1\u7b56\u7565\uff0c\u5904\u7406\u5206\u5b50\u6570\u636e\uff0c\u5e76\u8bc4\u4f30\u591a\u4ee3\u7406\u7cfb\u7edfDeep Thought\u7684\u8868\u73b0\u3002", "result": "Deep Thought\u7cfb\u7edf\u8868\u73b0\u4f18\u4e8e\u591a\u6570\u4eba\u7c7b\u56e2\u961f\uff0cClaude 3.7 Sonnet\u7b49\u6a21\u578b\u5728\u4e3b\u8981\u89d2\u8272\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u4ecd\u4e0d\u53ca\u4e13\u5bb6\u8bbe\u8ba1\u7684\u65b9\u6848\u4e14\u7a33\u5b9a\u6027\u4e0d\u8db3\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u65b9\u6cd5\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u6f5c\u529b\u663e\u8457\uff0c\u4f46\u4ecd\u9700\u63d0\u5347\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u4ee5\u63a5\u8fd1\u4e13\u5bb6\u6c34\u5e73\u3002"}}
{"id": "2504.18785", "pdf": "https://arxiv.org/pdf/2504.18785", "abs": "https://arxiv.org/abs/2504.18785", "authors": ["Santosh Rajagopalan", "Jonathan Vronsky", "Songbai Yan", "S. Alireza Golestaneh", "Shubhra Chandra", "Min Zhou"], "title": "ALF: Advertiser Large Foundation Model for Multi-Modal Advertiser Understanding", "categories": ["cs.LG"], "comment": null, "summary": "We present ALF (Advertiser Large Foundation model), a multi-modal transformer\narchitecture for understanding advertiser behavior and intent across text,\nimage, video and structured data modalities. Through contrastive learning and\nmulti-task optimization, ALF creates unified advertiser representations that\ncapture both content and behavioral patterns. Our model achieves\nstate-of-the-art performance on critical tasks including fraud detection,\npolicy violation identification, and advertiser similarity matching. In\nproduction deployment, ALF reduces false positives by 90% while maintaining\n99.8% precision on abuse detection tasks. The architecture's effectiveness\nstems from its novel combination of multi-modal transformations, inter-sample\nattention mechanism, spectrally normalized projections, and calibrated\nprobabilistic outputs.", "AI": {"tldr": "ALF\uff08\u5e7f\u544a\u4e3b\u5927\u578b\u57fa\u7840\u6a21\u578b\uff09\u662f\u4e00\u4e2a\u591a\u6a21\u6001Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u591a\u4efb\u52a1\u4f18\u5316\uff0c\u7edf\u4e00\u6355\u6349\u5e7f\u544a\u4e3b\u7684\u5185\u5bb9\u548c\u884c\u4e3a\u6a21\u5f0f\uff0c\u5728\u6b3a\u8bc8\u68c0\u6d4b\u3001\u653f\u7b56\u8fdd\u89c4\u8bc6\u522b\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u9645\u90e8\u7f72\u4e2d\u51cf\u5c11\u4e8690%\u7684\u8bef\u62a5\u3002", "motivation": "\u5e7f\u544a\u4e3b\u884c\u4e3a\u7684\u590d\u6742\u6027\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8de8\u591a\u6a21\u6001\u6570\u636e\uff08\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u7b49\uff09\u7406\u89e3\u5176\u610f\u56fe\u548c\u6a21\u5f0f\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u5e7f\u544a\u751f\u6001\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001Transformer\u67b6\u6784\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u591a\u4efb\u52a1\u4f18\u5316\uff0c\u5f15\u5165\u8de8\u6837\u672c\u6ce8\u610f\u529b\u673a\u5236\u3001\u8c31\u5f52\u4e00\u5316\u6295\u5f71\u548c\u6821\u51c6\u6982\u7387\u8f93\u51fa\u7b49\u521b\u65b0\u6280\u672f\u3002", "result": "\u5728\u6b3a\u8bc8\u68c0\u6d4b\u548c\u653f\u7b56\u8fdd\u89c4\u8bc6\u522b\u7b49\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5b9e\u9645\u90e8\u7f72\u4e2d\u8bef\u62a5\u51cf\u5c1190%\uff0c\u540c\u65f6\u4fdd\u630199.8%\u7684\u7cbe\u786e\u7387\u3002", "conclusion": "ALF\u901a\u8fc7\u5176\u591a\u6a21\u6001\u7edf\u4e00\u8868\u793a\u548c\u9ad8\u6548\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e7f\u544a\u4e3b\u884c\u4e3a\u7406\u89e3\u7684\u51c6\u786e\u6027\u548c\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2504.19314", "pdf": "https://arxiv.org/pdf/2504.19314", "abs": "https://arxiv.org/abs/2504.19314", "authors": ["Peilin Zhou", "Bruce Leon", "Xiang Ying", "Can Zhang", "Yifan Shao", "Qichen Ye", "Dading Chong", "Zhiling Jin", "Chenxuan Xie", "Meng Cao", "Yuxin Gu", "Sixin Hong", "Jing Ren", "Jian Chen", "Chao Liu", "Yining Hua"], "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese", "categories": ["cs.CL"], "comment": "Under Review", "summary": "As large language models (LLMs) evolve into tool-using agents, the ability to\nbrowse the web in real-time has become a critical yardstick for measuring their\nreasoning and retrieval competence. Existing benchmarks such as BrowseComp\nconcentrate on English and overlook the linguistic, infrastructural, and\ncensorship-related complexities of other major information ecosystems -- most\nnotably Chinese. To address this gap, we introduce BrowseComp-ZH, a\nhigh-difficulty benchmark purpose-built to comprehensively evaluate LLM agents\non the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning\n11 diverse domains. Each question is reverse-engineered from a short,\nobjective, and easily verifiable answer (e.g., a date, number, or proper noun).\nA two-stage quality control protocol is applied to strive for high question\ndifficulty and answer uniqueness. We benchmark over 20 state-of-the-art\nlanguage models and agentic search systems on our proposed BrowseComp-ZH.\nDespite their strong conversational and retrieval capabilities, most models\nstruggle severely: a large number achieve accuracy rates below 10%, and only a\nhandful exceed 20%. Even the best-performing system, OpenAI's DeepResearch,\nreaches just 42.9%. These results demonstrate the considerable difficulty of\nBrowseComp-ZH, where success demands not only effective retrieval strategies,\nbut also sophisticated reasoning and information reconciliation -- capabilities\nthat current models still struggle to master. Our dataset, construction\nguidelines, and benchmark results have been publicly released at\nhttps://github.com/PALIN2018/BrowseComp-ZH.", "AI": {"tldr": "BrowseComp-ZH\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e2d\u56fd\u7f51\u7edc\u4e2d\u5b9e\u65f6\u6d4f\u89c8\u80fd\u529b\u7684\u9ad8\u96be\u5ea6\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b289\u4e2a\u591a\u8df3\u95ee\u9898\uff0c\u6db5\u76d611\u4e2a\u9886\u57df\uff0c\u7ed3\u679c\u663e\u793a\u5927\u591a\u6570\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u4ec5\u4e3a42.9%\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5982BrowseComp\u4e3b\u8981\u5173\u6ce8\u82f1\u8bed\uff0c\u5ffd\u7565\u4e86\u4e2d\u6587\u7f51\u7edc\u7684\u8bed\u8a00\u3001\u57fa\u7840\u8bbe\u65bd\u548c\u5ba1\u67e5\u590d\u6742\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u7684\u4e2d\u6587\u6d4b\u8bd5\u57fa\u51c6\u3002", "method": "\u901a\u8fc7\u53cd\u5411\u5de5\u7a0b\u4ece\u7b80\u77ed\u3001\u5ba2\u89c2\u4e14\u6613\u9a8c\u8bc1\u7684\u7b54\u6848\u4e2d\u6784\u9020\u95ee\u9898\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8d28\u91cf\u63a7\u5236\u786e\u4fdd\u95ee\u9898\u96be\u5ea6\u548c\u7b54\u6848\u552f\u4e00\u6027\u3002", "result": "\u6d4b\u8bd5\u4e8620\u591a\u4e2a\u5148\u8fdb\u8bed\u8a00\u6a21\u578b\u548c\u4ee3\u7406\u641c\u7d22\u7cfb\u7edf\uff0c\u5927\u591a\u6570\u51c6\u786e\u7387\u4f4e\u4e8e10%\uff0c\u6700\u4f73\u6a21\u578bDeepResearch\u4ec5\u8fbe42.9%\u3002", "conclusion": "BrowseComp-ZH\u5c55\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u590d\u6742\u68c0\u7d22\u548c\u4fe1\u606f\u534f\u8c03\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2504.19933", "pdf": "https://arxiv.org/pdf/2504.19933", "abs": "https://arxiv.org/abs/2504.19933", "authors": ["Riccardo Lo Bianco", "Willem van Jaarsveld", "Jeroen Middelhuis", "Luca Begnardi", "Remco Dijkman"], "title": "Automated decision-making for dynamic task assignment at scale", "categories": ["cs.AI", "cs.LG", "math.OC"], "comment": null, "summary": "The Dynamic Task Assignment Problem (DTAP) concerns matching resources to\ntasks in real time while minimizing some objectives, like resource costs or\ntask cycle time. In this work, we consider a DTAP variant where every task is a\ncase composed of a stochastic sequence of activities. The DTAP, in this case,\ninvolves the decision of which employee to assign to which activity to process\nrequests as quickly as possible. In recent years, Deep Reinforcement Learning\n(DRL) has emerged as a promising tool for tackling this DTAP variant, but most\nresearch is limited to solving small-scale, synthetic problems, neglecting the\nchallenges posed by real-world use cases. To bridge this gap, this work\nproposes a DRL-based Decision Support System (DSS) for real-world scale DTAPS.\nTo this end, we introduce a DRL agent with two novel elements: a graph\nstructure for observations and actions that can effectively represent any DTAP\nand a reward function that is provably equivalent to the objective of\nminimizing the average cycle time of tasks. The combination of these two\nnovelties allows the agent to learn effective and generalizable assignment\npolicies for real-world scale DTAPs. The proposed DSS is evaluated on five DTAP\ninstances whose parameters are extracted from real-world logs through process\nmining. The experimental evaluation shows how the proposed DRL agent matches or\noutperforms the best baseline in all DTAP instances and generalizes on\ndifferent time horizons and across instances.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60(DRL)\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf(DSS)\uff0c\u7528\u4e8e\u89e3\u51b3\u52a8\u6001\u4efb\u52a1\u5206\u914d\u95ee\u9898(DTAP)\u4e2d\u7684\u5927\u89c4\u6a21\u5b9e\u9645\u6848\u4f8b\uff0c\u901a\u8fc7\u5f15\u5165\u56fe\u7ed3\u6784\u548c\u7b49\u6548\u5956\u52b1\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u7684\u5e73\u5747\u5468\u671f\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u7684DRL\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5c0f\u89c4\u6a21\u3001\u5408\u6210\u95ee\u9898\uff0c\u5ffd\u7565\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u8bbe\u8ba1\u4e00\u4e2a\u9002\u7528\u4e8e\u73b0\u5b9e\u573a\u666f\u7684DTAP\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdDRL\u4ee3\u7406\uff0c\u5305\u542b\u4e24\u4e2a\u521b\u65b0\u70b9\uff1a1) \u4f7f\u7528\u56fe\u7ed3\u6784\u8868\u793a\u89c2\u5bdf\u548c\u52a8\u4f5c\uff1b2) \u8bbe\u8ba1\u4e86\u4e0e\u4efb\u52a1\u5e73\u5747\u5468\u671f\u65f6\u95f4\u6700\u5c0f\u5316\u76ee\u6807\u7b49\u6548\u7684\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u4e8e\u771f\u5b9e\u65e5\u5fd7\u7684DTAP\u5b9e\u4f8b\u4e2d\uff0c\u8be5DRL\u4ee3\u7406\u5747\u5339\u914d\u6216\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86DRL\u5728\u89e3\u51b3\u5927\u89c4\u6a21\u5b9e\u9645DTAP\u95ee\u9898\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u662f\u901a\u8fc7\u521b\u65b0\u7684\u56fe\u7ed3\u6784\u548c\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18818", "pdf": "https://arxiv.org/pdf/2504.18818", "abs": "https://arxiv.org/abs/2504.18818", "authors": ["Xufei Wang", "Fei Ge", "Jinchen Zhu", "Mingjian Zhang", "Qi Wu", "Jifeng Ren Shizhuang Weng"], "title": "Frequency-Integrated Transformer for Arbitrary-Scale Super-Resolution", "categories": ["cs.LG"], "comment": "11pages,8figures", "summary": "Methods based on implicit neural representation have demonstrated remarkable\ncapabilities in arbitrary-scale super-resolution (ASSR) tasks, but they neglect\nthe potential value of the frequency domain, leading to sub-optimal\nperformance. We proposes a novel network called Frequency-Integrated\nTransformer (FIT) to incorporate and utilize frequency information to enhance\nASSR performance. FIT employs Frequency Incorporation Module (FIM) to introduce\nfrequency information in a lossless manner and Frequency Utilization\nSelf-Attention module (FUSAM) to efficiently leverage frequency information by\nexploiting spatial-frequency interrelationship and global nature of frequency.\nFIM enriches detail characterization by incorporating frequency information\nthrough a combination of Fast Fourier Transform (FFT) with real-imaginary\nmapping. In FUSAM, Interaction Implicit Self-Attention (IISA) achieves\ncross-domain information synergy by interacting spatial and frequency\ninformation in subspace, while Frequency Correlation Self-attention (FCSA)\ncaptures the global context by computing correlation in frequency. Experimental\nresults demonstrate FIT yields superior performance compared to existing\nmethods across multiple benchmark datasets. Visual feature map proves the\nsuperiority of FIM in enriching detail characterization. Frequency error map\nvalidates IISA productively improve the frequency fidelity. Local attribution\nmap validates FCSA effectively captures global context.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFIT\u7684\u65b0\u7f51\u7edc\uff0c\u901a\u8fc7\u7ed3\u5408\u9891\u57df\u4fe1\u606f\u6765\u63d0\u5347\u4efb\u610f\u5c3a\u5ea6\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5229\u7528FIM\u548cFUSAM\u6a21\u5757\u5206\u522b\u5f15\u5165\u548c\u9ad8\u6548\u5229\u7528\u9891\u57df\u4fe1\u606f\u3002\u5b9e\u9a8c\u8bc1\u660eFIT\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u7684\u65b9\u6cd5\u5728\u4efb\u610f\u5c3a\u5ea6\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5ffd\u7565\u4e86\u9891\u57df\u4fe1\u606f\u7684\u6f5c\u5728\u4ef7\u503c\uff0c\u5bfc\u81f4\u6027\u80fd\u672a\u8fbe\u6700\u4f18\u3002", "method": "\u63d0\u51faFIT\u7f51\u7edc\uff0c\u5305\u542bFIM\u6a21\u5757\uff08\u901a\u8fc7FFT\u548c\u5b9e\u865a\u6620\u5c04\u65e0\u635f\u5f15\u5165\u9891\u57df\u4fe1\u606f\uff09\u548cFUSAM\u6a21\u5757\uff08\u5229\u7528IISA\u5b9e\u73b0\u7a7a\u95f4-\u9891\u57df\u4fe1\u606f\u4ea4\u4e92\uff0cFCSA\u6355\u83b7\u5168\u5c40\u9891\u7387\u76f8\u5173\u6027\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793aFIT\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u9886\u5148\uff0cFIM\u589e\u5f3a\u7ec6\u8282\u8868\u5f81\uff0cIISA\u63d0\u5347\u9891\u7387\u4fdd\u771f\u5ea6\uff0cFCSA\u6709\u6548\u6355\u83b7\u5168\u5c40\u4e0a\u4e0b\u6587\u3002", "conclusion": "\u9891\u57df\u4fe1\u606f\u7684\u5f15\u5165\u548c\u9ad8\u6548\u5229\u7528\u663e\u8457\u63d0\u5347\u4e86\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u7684\u6027\u80fd\uff0cFIT\u7f51\u7edc\u5728\u7ec6\u8282\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u8868\u73b0\u4e0a\u5747\u6709\u4f18\u52bf\u3002"}}
{"id": "2504.19333", "pdf": "https://arxiv.org/pdf/2504.19333", "abs": "https://arxiv.org/abs/2504.19333", "authors": ["James O' Neill", "Santhosh Subramanian", "Eric Lin", "Vaikkunth Mugunthan"], "title": "Unified Multi-Task Learning & Model Fusion for Efficient Language Model Guardrailing", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The trend towards large language models (LLMs) for guardrailing against\nundesired behaviors is increasing and has shown promise for censoring user\ninputs. However, increased latency, memory consumption, hosting expenses and\nnon-structured outputs can make their use prohibitive.\n  In this work, we show that task-specific data generation can lead to\nfine-tuned classifiers that significantly outperform current state of the art\n(SoTA) while being orders of magnitude smaller. Secondly, we show that using a\nsingle model, \\texttt{MultiTaskGuard}, that is pretrained on a large\nsynthetically generated dataset with unique task instructions further improves\ngeneralization. Thirdly, our most performant models, \\texttt{UniGuard}, are\nfound using our proposed search-based model merging approach that finds an\noptimal set of parameters to combine single-policy models and multi-policy\nguardrail models. % On 7 public datasets and 4 guardrail benchmarks we created,\nour efficient guardrail classifiers improve over the best performing SoTA\npublicly available LLMs and 3$^{\\text{rd}}$ party guardrail APIs in detecting\nunsafe and safe behaviors by an average F1 score improvement of \\textbf{29.92}\npoints over Aegis-LlamaGuard and \\textbf{21.62} over \\texttt{gpt-4o},\nrespectively. Lastly, our guardrail synthetic data generation process that uses\ncustom task-specific guardrail poli", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u66ff\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u5185\u5bb9\u5ba1\u67e5\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u6570\u636e\u751f\u6210\u548c\u6a21\u578b\u5408\u5e76\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u51cf\u5c11\u4e86\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u867d\u7136\u5728\u5185\u5bb9\u5ba1\u67e5\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u9ad8\u5ef6\u8fdf\u3001\u5185\u5b58\u6d88\u8017\u548c\u6210\u672c\u95ee\u9898\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u4e86\u4efb\u52a1\u7279\u5b9a\u6570\u636e\u751f\u6210\u6765\u5fae\u8c03\u5206\u7c7b\u5668\uff0c\u5e76\u4f7f\u7528\u540d\u4e3aMultiTaskGuard\u7684\u6a21\u578b\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u6700\u7ec8\u901a\u8fc7\u641c\u7d22\u5f0f\u6a21\u578b\u5408\u5e76\u6280\u672f\u5f00\u53d1\u51fa\u6027\u80fd\u6700\u4f73\u6a21\u578bUniGuard\u3002", "result": "\u57287\u4e2a\u516c\u5171\u6570\u636e\u96c6\u548c4\u4e2a\u81ea\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUniGuard\u7684\u5e73\u5747F1\u5206\u6570\u6bd4Aegis-LlamaGuard\u9ad829.92\u5206\uff0c\u6bd4gpt-4o\u9ad821.62\u5206\u3002", "conclusion": "\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u6570\u636e\u751f\u6210\u548c\u6a21\u578b\u5408\u5e76\u6280\u672f\uff0c\u53ef\u4ee5\u5f00\u53d1\u51fa\u9ad8\u6548\u3001\u9ad8\u6027\u80fd\u7684\u5185\u5bb9\u5ba1\u67e5\u6a21\u578b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684LLMs\u548c\u7b2c\u4e09\u65b9API\u3002"}}
{"id": "2504.19968", "pdf": "https://arxiv.org/pdf/2504.19968", "abs": "https://arxiv.org/abs/2504.19968", "authors": ["John Beverley", "Regina Hurley"], "title": "How Group Lives Go Well", "categories": ["cs.AI", "cs.GT"], "comment": null, "summary": "This paper explores the ontological space of group well being, proposing a\nframework for representing collective welfare, group functions, and long term\ncontributions within an ontology engineering context. Traditional well being\ntheories focus on individual states, often relying on hedonistic, desire\nsatisfaction, or objective list models. Such approaches struggle to account for\ncases where individual sacrifices contribute to broader social progress, a\ncritical challenge in modeling group flourishing. To address this, the paper\nrefines and extends the Counterfactual Account (CT) of well being, which\nevaluates goodness of an event by comparing an individual's actual well being\nwith a hypothetical counterpart in a nearby possible world. While useful, this\nframework is insufficient for group level ontologies, where well being depends\non functional persistence, institutional roles, and historical impact rather\nthan immediate individual outcomes. Drawing on Basic Formal Ontology (BFO), the\npaper introduces a model in which group flourishing is evaluated in terms of\ngroup functional, where members bear roles and exhibit persistence conditions\nakin to biological systems or designed artifacts. This approach enables\nsemantic interoperability for modeling longitudinal social contributions,\nallowing for structured reasoning about group welfare, social institutions, and\ngroup flourishing over time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u672c\u4f53\u5de5\u7a0b\u7684\u7fa4\u4f53\u798f\u7949\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u53cd\u4e8b\u5b9e\u8d26\u6237\uff08CT\uff09\u7406\u8bba\uff0c\u7ed3\u5408BFO\u6a21\u578b\uff0c\u7528\u4e8e\u8bc4\u4f30\u7fa4\u4f53\u7684\u529f\u80fd\u6027\u3001\u5236\u5ea6\u89d2\u8272\u548c\u5386\u53f2\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u798f\u7949\u7406\u8bba\u4e3b\u8981\u5173\u6ce8\u4e2a\u4f53\u72b6\u6001\uff0c\u96be\u4ee5\u89e3\u91ca\u4e2a\u4f53\u727a\u7272\u5bf9\u7fa4\u4f53\u8fdb\u6b65\u7684\u8d21\u732e\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u7fa4\u4f53\u798f\u7949\u6a21\u578b\u7684\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u8bba\u6587\u6269\u5c55\u4e86\u53cd\u4e8b\u5b9e\u8d26\u6237\uff08CT\uff09\u7406\u8bba\uff0c\u5e76\u5f15\u5165BFO\u6a21\u578b\uff0c\u5c06\u7fa4\u4f53\u798f\u7949\u8bc4\u4f30\u4e0e\u7fa4\u4f53\u7684\u529f\u80fd\u6027\u3001\u89d2\u8272\u548c\u6301\u4e45\u6027\u6761\u4ef6\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u63d0\u51fa\u7684\u65b0\u6846\u67b6\u80fd\u591f\u66f4\u597d\u5730\u5efa\u6a21\u7fa4\u4f53\u7684\u957f\u671f\u793e\u4f1a\u8d21\u732e\uff0c\u652f\u6301\u5bf9\u7fa4\u4f53\u798f\u5229\u548c\u793e\u4f1a\u5236\u5ea6\u7684\u7ed3\u6784\u5316\u63a8\u7406\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7fa4\u4f53\u798f\u7949\u7684\u672c\u4f53\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86\u8bed\u4e49\u4e92\u64cd\u4f5c\u6027\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u7fa4\u4f53\u7e41\u8363\u7684\u52a8\u6001\u673a\u5236\u3002"}}
{"id": "2504.18819", "pdf": "https://arxiv.org/pdf/2504.18819", "abs": "https://arxiv.org/abs/2504.18819", "authors": ["Hassan Wasswa", "Aziida Nanyonga", "Timothy Lynar"], "title": "Preserving Seasonal and Trend Information: A Variational Autoencoder-Latent Space Arithmetic Based Approach for Non-stationary Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "AI models have garnered significant research attention towards predictive\ntask automation. However, a stationary training environment is an underlying\nassumption for most models and such models simply do not work on non-stationary\ndata since a stationary relationship is learned. The existing solutions propose\nmaking data stationary prior to model training and evaluation. This leads to\nloss of trend and seasonal patterns which are vital components for learning\ntemporal dependencies of the system under study. This research aims to address\nthis limitation by proposing a method for enforcing stationary behaviour within\nthe latent space while preserving trend and seasonal information. The method\ndeploys techniques including Differencing, Time-series decomposition, and\nLatent Space Arithmetic (LSA), to learn information vital for efficient\napproximation of trend and seasonal information which is then stored as\nembeddings within the latent space of a Variational Autoencoder (VAE). The\napproach's ability to preserve trend and seasonal information was evaluated on\ntwo time-series non-stationary datasets. For predictive performance evaluation,\nfour deep learning models were trained on the latent vector representations of\nthe datasets after application of the proposed method and all models produced\ncompetitive results in comparison with state-of-the-art techniques using RMSE\nas the performance metric.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u5728\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u6f5c\u5728\u7a7a\u95f4\u4e2d\u4fdd\u7559\u8d8b\u52bf\u548c\u5b63\u8282\u6027\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5dee\u5206\u3001\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u548c\u6f5c\u5728\u7a7a\u95f4\u7b97\u672f\uff08LSA\uff09\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u56e0\u4f7f\u6570\u636e\u5e73\u7a33\u800c\u4e22\u5931\u5173\u952e\u4fe1\u606f\u7684\u5c40\u9650\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u975e\u5e73\u7a33\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709AI\u6a21\u578b\u901a\u5e38\u5047\u8bbe\u6570\u636e\u662f\u5e73\u7a33\u7684\uff0c\u4f46\u5728\u975e\u5e73\u7a33\u6570\u636e\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u901a\u8fc7\u4f7f\u6570\u636e\u5e73\u7a33\u5316\u5904\u7406\uff0c\u5374\u4e22\u5931\u4e86\u8d8b\u52bf\u548c\u5b63\u8282\u6027\u4fe1\u606f\u3002\u672c\u7814\u7a76\u65e8\u5728\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5f3a\u5236\u5e73\u7a33\u884c\u4e3a\uff0c\u540c\u65f6\u4fdd\u7559\u8fd9\u4e9b\u5173\u952e\u4fe1\u606f\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u5dee\u5206\u3001\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u548c\u6f5c\u5728\u7a7a\u95f4\u7b97\u672f\uff08LSA\uff09\uff0c\u5b66\u4e60\u5e76\u5b58\u50a8\u8d8b\u52bf\u548c\u5b63\u8282\u6027\u4fe1\u606f\u4e3a\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u5d4c\u5165\u5411\u91cf\u3002", "result": "\u5728\u4e24\u4e2a\u975e\u5e73\u7a33\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u4fdd\u7559\u8d8b\u52bf\u548c\u5b63\u8282\u6027\u4fe1\u606f\uff0c\u4e14\u56db\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5176\u6f5c\u5728\u5411\u91cf\u8868\u793a\u4e0a\u5747\u53d6\u5f97\u4e0e\u6700\u5148\u8fdb\u6280\u672f\u76f8\u5f53\u7684RMSE\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u975e\u5e73\u7a33\u6570\u636e\u5efa\u6a21\u4e2d\u4fe1\u606f\u4e22\u5931\u7684\u95ee\u9898\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2504.19339", "pdf": "https://arxiv.org/pdf/2504.19339", "abs": "https://arxiv.org/abs/2504.19339", "authors": ["Dongqi Liu", "Xi Yu", "Vera Demberg", "Mirella Lapata"], "title": "Explanatory Summarization with Discourse-Driven Planning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by the Transactions of the Association for Computational\n  Linguistics (TACL)", "summary": "Lay summaries for scientific documents typically include explanations to help\nreaders grasp sophisticated concepts or arguments. However, current automatic\nsummarization methods do not explicitly model explanations, which makes it\ndifficult to align the proportion of explanatory content with human-written\nsummaries. In this paper, we present a plan-based approach that leverages\ndiscourse frameworks to organize summary generation and guide explanatory\nsentences by prompting responses to the plan. Specifically, we propose two\ndiscourse-driven planning strategies, where the plan is conditioned as part of\nthe input or part of the output prefix, respectively. Empirical experiments on\nthree lay summarization datasets show that our approach outperforms existing\nstate-of-the-art methods in terms of summary quality, and it enhances model\nrobustness, controllability, and mitigates hallucination.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c4\u5212\u7684\u81ea\u52a8\u6458\u8981\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bdd\u8bed\u6846\u67b6\u6307\u5bfc\u89e3\u91ca\u6027\u5185\u5bb9\u7684\u751f\u6210\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u6458\u8981\u8d28\u91cf\u3001\u7a33\u5065\u6027\u548c\u53ef\u63a7\u6027\u4e0a\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u5f53\u524d\u7684\u81ea\u52a8\u6458\u8981\u65b9\u6cd5\u672a\u660e\u786e\u5efa\u6a21\u89e3\u91ca\u6027\u5185\u5bb9\uff0c\u5bfc\u81f4\u4e0e\u4eba\u5de5\u6458\u8981\u4e2d\u89e3\u91ca\u6bd4\u4f8b\u4e0d\u4e00\u81f4\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8bdd\u8bed\u6846\u67b6\u5f15\u5bfc\u6458\u8981\u751f\u6210\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u8bdd\u8bed\u9a71\u52a8\u7684\u89c4\u5212\u7b56\u7565\uff1a\u8ba1\u5212\u4f5c\u4e3a\u8f93\u5165\u6761\u4ef6\u6216\u8f93\u51fa\u524d\u7f00\u3002\u5229\u7528\u8bdd\u8bed\u6846\u67b6\u7ec4\u7ec7\u6458\u8981\u751f\u6210\uff0c\u5e76\u901a\u8fc7\u63d0\u793a\u54cd\u5e94\u5f15\u5bfc\u89e3\u91ca\u6027\u53e5\u5b50\u3002", "result": "\u5728\u4e09\u4e2a\u5916\u884c\u6458\u8981\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6458\u8981\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u5e76\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u7a33\u5065\u6027\u3001\u53ef\u63a7\u6027\uff0c\u51cf\u5c11\u4e86\u5e7b\u89c9\u73b0\u8c61\u3002", "conclusion": "\u57fa\u4e8e\u8bdd\u8bed\u6846\u67b6\u7684\u89c4\u5212\u65b9\u6cd5\u80fd\u6709\u6548\u6539\u8fdb\u81ea\u52a8\u6458\u8981\u7684\u89e3\u91ca\u6027\u5185\u5bb9\u751f\u6210\u548c\u8d28\u91cf\uff0c\u5177\u6709\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2504.20007", "pdf": "https://arxiv.org/pdf/2504.20007", "abs": "https://arxiv.org/abs/2504.20007", "authors": ["Anita Srbinovska", "Angela Srbinovska", "Vivek Senthil", "Adrian Martin", "John McCluskey", "Ernest Fokou\u00e9"], "title": "Towards AI-Driven Policing: Interdisciplinary Knowledge Discovery from Police Body-Worn Camera Footage", "categories": ["cs.AI", "cs.CV"], "comment": "6 pages, 2 figures, and 1 table", "summary": "This paper proposes a novel interdisciplinary framework for analyzing police\nbody-worn camera (BWC) footage from the Rochester Police Department (RPD) using\nadvanced artificial intelligence (AI) and statistical machine learning (ML)\ntechniques. Our goal is to detect, classify, and analyze patterns of\ninteraction between police officers and civilians to identify key behavioral\ndynamics, such as respect, disrespect, escalation, and de-escalation. We apply\nmultimodal data analysis by integrating video, audio, and natural language\nprocessing (NLP) techniques to extract meaningful insights from BWC footage. We\npresent our methodology, computational techniques, and findings, outlining a\npractical approach for law enforcement while advancing the frontiers of\nknowledge discovery from police BWC data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u5b66\u79d1\u6846\u67b6\uff0c\u7ed3\u5408AI\u548cML\u6280\u672f\u5206\u6790\u8b66\u5bdf\u968f\u8eab\u6444\u50cf\u5934\uff08BWC\uff09\u5f55\u50cf\uff0c\u65e8\u5728\u8bc6\u522b\u8b66\u6c11\u4e92\u52a8\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "motivation": "\u901a\u8fc7\u5206\u6790BWC\u5f55\u50cf\uff0c\u63ed\u793a\u8b66\u6c11\u4e92\u52a8\u4e2d\u7684\u5173\u952e\u884c\u4e3a\u52a8\u6001\uff08\u5982\u5c0a\u91cd\u3001\u4e0d\u5c0a\u91cd\u3001\u51b2\u7a81\u5347\u7ea7\u6216\u964d\u7ea7\uff09\uff0c\u4ee5\u52a9\u529b\u6267\u6cd5\u90e8\u95e8\u4f18\u5316\u5b9e\u8df5\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u6570\u636e\u5206\u6790\uff08\u89c6\u9891\u3001\u97f3\u9891\u3001NLP\uff09\u7ed3\u5408AI\u548c\u7edf\u8ba1\u673a\u5668\u5b66\u4e60\u6280\u672f\u3002", "result": "\u5f00\u53d1\u4e86\u53ef\u8bc6\u522b\u884c\u4e3a\u52a8\u6001\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u7f57\u5207\u65af\u7279\u8b66\u5bdf\u5c40\u6570\u636e\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6267\u6cd5\u90e8\u95e8\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\uff0c\u540c\u65f6\u63a8\u52a8\u4e86BWC\u6570\u636e\u7684\u77e5\u8bc6\u6316\u6398\u524d\u6cbf\u3002"}}
{"id": "2504.18845", "pdf": "https://arxiv.org/pdf/2504.18845", "abs": "https://arxiv.org/abs/2504.18845", "authors": ["Mehmet Ali Ferah", "Tufan Kumbasar"], "title": "Introducing Interval Neural Networks for Uncertainty-Aware System Identification", "categories": ["cs.LG", "cs.AI"], "comment": "In International Congress on Human-Computer Interaction, Optimization\n  and Robotic Applications, 2025", "summary": "System Identification (SysID) is crucial for modeling and understanding\ndynamical systems using experimental data. While traditional SysID methods\nemphasize linear models, their inability to fully capture nonlinear dynamics\nhas driven the adoption of Deep Learning (DL) as a more powerful alternative.\nHowever, the lack of uncertainty quantification (UQ) in DL-based models poses\nchallenges for reliability and safety, highlighting the necessity of\nincorporating UQ. This paper introduces a systematic framework for constructing\nand learning Interval Neural Networks (INNs) to perform UQ in SysID tasks. INNs\nare derived by transforming the learnable parameters (LPs) of pre-trained\nneural networks into interval-valued LPs without relying on probabilistic\nassumptions. By employing interval arithmetic throughout the network, INNs can\ngenerate Prediction Intervals (PIs) that capture target coverage effectively.\nWe extend Long Short-Term Memory (LSTM) and Neural Ordinary Differential\nEquations (Neural ODEs) into Interval LSTM (ILSTM) and Interval NODE (INODE)\narchitectures, providing the mathematical foundations for their application in\nSysID. To train INNs, we propose a DL framework that integrates a UQ loss\nfunction and parameterization tricks to handle constraints arising from\ninterval LPs. We introduce novel concept \"elasticity\" for underlying\nuncertainty causes and validate ILSTM and INODE in SysID experiments,\ndemonstrating their effectiveness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u533a\u95f4\u795e\u7ecf\u7f51\u7edc\uff08INNs\uff09\u7684\u7cfb\u7edf\u6027\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7cfb\u7edf\u8bc6\u522b\uff08SysID\uff09\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u3002\u901a\u8fc7\u5c06\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u7684\u53c2\u6570\u8f6c\u6362\u4e3a\u533a\u95f4\u503c\u53c2\u6570\uff0c\u5e76\u7ed3\u5408\u533a\u95f4\u7b97\u672f\uff0cINNs\u80fd\u591f\u6709\u6548\u751f\u6210\u9884\u6d4b\u533a\u95f4\u3002", "motivation": "\u4f20\u7edfSysID\u65b9\u6cd5\u96be\u4ee5\u5145\u5206\u6355\u6349\u975e\u7ebf\u6027\u52a8\u6001\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u6a21\u578b\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u9650\u5236\u4e86\u5176\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7ed3\u5408\u975e\u7ebf\u6027\u5efa\u6a21\u80fd\u529b\u548cUQ\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faInterval Neural Networks\uff08INNs\uff09\uff0c\u901a\u8fc7\u5c06\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u7684\u53c2\u6570\u8f6c\u6362\u4e3a\u533a\u95f4\u503c\u53c2\u6570\uff0c\u5229\u7528\u533a\u95f4\u7b97\u672f\u751f\u6210\u9884\u6d4b\u533a\u95f4\u3002\u6269\u5c55\u4e86LSTM\u548cNeural ODEs\u4e3aInterval LSTM\uff08ILSTM\uff09\u548cInterval NODE\uff08INODE\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5305\u542bUQ\u635f\u5931\u51fd\u6570\u7684\u8bad\u7ec3\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cILSTM\u548cINODE\u5728SysID\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u76ee\u6807\u8986\u76d6\u8303\u56f4\u5e76\u63d0\u4f9b\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7ed3\u679c\u3002", "conclusion": "INNs\u4e3aSysID\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u6982\u7387\u5047\u8bbe\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86DL\u6a21\u578b\u5728\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.19395", "pdf": "https://arxiv.org/pdf/2504.19395", "abs": "https://arxiv.org/abs/2504.19395", "authors": ["Zhouxiang Fang", "Aayush Mishra", "Muhan Gao", "Anqi Liu", "Daniel Khashabi"], "title": "ICL CIPHERS: Quantifying \"Learning'' in In-Context Learning via Substitution Ciphers", "categories": ["cs.CL"], "comment": null, "summary": "Recent works have suggested that In-Context Learning (ICL) operates in dual\nmodes, i.e. task retrieval (remember learned patterns from pre-training) and\ntask learning (inference-time ``learning'' from demonstrations). However,\ndisentangling these the two modes remains a challenging goal. We introduce ICL\nCIPHERS, a class of task reformulations based on substitution ciphers borrowed\nfrom classic cryptography. In this approach, a subset of tokens in the\nin-context inputs are substituted with other (irrelevant) tokens, rendering\nEnglish sentences less comprehensible to human eye. However, by design, there\nis a latent, fixed pattern to this substitution, making it reversible. This\nbijective (reversible) cipher ensures that the task remains a well-defined task\nin some abstract sense, despite the transformations. It is a curious question\nif LLMs can solve ICL CIPHERS with a BIJECTIVE mapping, which requires\ndeciphering the latent cipher. We show that LLMs are better at solving ICL\nCIPHERS with BIJECTIVE mappings than the NON-BIJECTIVE (irreversible) baseline,\nproviding a novel approach to quantify ``learning'' in ICL. While this gap is\nsmall, it is consistent across the board on four datasets and six models.\nFinally, we examine LLMs' internal representations and identify evidence in\ntheir ability to decode the ciphered inputs.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u7684\u4e24\u79cd\u6a21\u5f0f\uff1a\u4efb\u52a1\u68c0\u7d22\u548c\u4efb\u52a1\u5b66\u4e60\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u66ff\u6362\u5bc6\u7801\u7684\u4efb\u52a1\u91cd\u6784\u65b9\u6cd5ICL CIPHERS\uff0c\u901a\u8fc7\u53ef\u9006\u6620\u5c04\u9a8c\u8bc1\u6a21\u578b\u7684\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u76ee\u6807\u662f\u901a\u8fc7\u4efb\u52a1\u91cd\u6784\u65b9\u6cd5ICL CIPHERS\uff0c\u5206\u79bb\u5e76\u91cf\u5316\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u4efb\u52a1\u68c0\u7d22\u548c\u4efb\u52a1\u5b66\u4e60\u4e24\u79cd\u6a21\u5f0f\u7684\u8d21\u732e\uff0c\u9a8c\u8bc1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u662f\u5426\u5177\u5907\u89e3\u5bc6\u80fd\u529b\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u66ff\u6362\u5bc6\u7801\u7684\u4efb\u52a1\u91cd\u6784\u65b9\u6cd5ICL CIPHERS\uff0c\u8bbe\u8ba1\u53ef\u9006\uff08\u53cc\u5c04\uff09\u548c\u4e0d\u53ef\u9006\uff08\u975e\u53cc\u5c04\uff09\u4e24\u79cd\u6620\u5c04\uff0c\u901a\u8fc7\u6a21\u578b\u5728\u89e3\u7801\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u5dee\u5f02\u9a8c\u8bc1\u5b66\u4e60\u80fd\u529b\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cLLM\u5728\u53ef\u9006\u6620\u5c04\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u4e0d\u53ef\u9006\u57fa\u7ebf\uff0c\u5c3d\u7ba1\u5dee\u8ddd\u8f83\u5c0f\uff0c\u4f46\u5728\u56db\u4e2a\u6570\u636e\u96c6\u548c\u516d\u4e2a\u6a21\u578b\u4e2d\u8868\u73b0\u4e00\u81f4\u3002\u6b64\u5916\uff0c\u6a21\u578b\u5185\u90e8\u8868\u5f81\u663e\u793a\u4e86\u89e3\u5bc6\u80fd\u529b\u3002", "conclusion": "ICL CIPHERS\u4e3a\u91cf\u5316\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u201c\u5b66\u4e60\u201d\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86LLM\u5177\u5907\u5bf9\u52a0\u5bc6\u8f93\u5165\u7684\u89e3\u5bc6\u80fd\u529b\u3002"}}
{"id": "2504.20010", "pdf": "https://arxiv.org/pdf/2504.20010", "abs": "https://arxiv.org/abs/2504.20010", "authors": ["Jacob Emmerson", "Rayid Ghani", "Zheyuan Ryan Shi"], "title": "Towards Automated Scoping of AI for Social Good Projects", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "Artificial Intelligence for Social Good (AI4SG) is an emerging effort that\naims to address complex societal challenges with the powerful capabilities of\nAI systems. These challenges range from local issues with transit networks to\nglobal wildlife preservation. However, regardless of scale, a critical\nbottleneck for many AI4SG initiatives is the laborious process of problem\nscoping -- a complex and resource-intensive task -- due to a scarcity of\nprofessionals with both technical and domain expertise. Given the remarkable\napplications of large language models (LLM), we propose a Problem Scoping Agent\n(PSA) that uses an LLM to generate comprehensive project proposals grounded in\nscientific literature and real-world knowledge. We demonstrate that our PSA\nframework generates proposals comparable to those written by experts through a\nblind review and AI evaluations. Finally, we document the challenges of\nreal-world problem scoping and note several areas for future work.", "AI": {"tldr": "AI4SG\u9879\u76ee\u9762\u4e34\u95ee\u9898\u8303\u56f4\u754c\u5b9a\u56f0\u96be\u7684\u74f6\u9888\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u95ee\u9898\u8303\u56f4\u754c\u5b9a\u4ee3\u7406\uff08PSA\uff09\uff0c\u53ef\u751f\u6210\u4e13\u5bb6\u7ea7\u9879\u76ee\u63d0\u6848\uff0c\u5e76\u901a\u8fc7\u76f2\u5ba1\u548cAI\u8bc4\u4f30\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "motivation": "\u89e3\u51b3AI4SG\u4e2d\u56e0\u4e13\u4e1a\u4eba\u624d\u7a00\u7f3a\u5bfc\u81f4\u7684\u95ee\u9898\u8303\u56f4\u754c\u5b9a\u56f0\u96be\uff0c\u5229\u7528LLM\u7684\u80fd\u529b\u63d0\u5347\u6548\u7387\u548c\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8eLLM\u7684PSA\u6846\u67b6\uff0c\u901a\u8fc7\u79d1\u5b66\u6587\u732e\u548c\u5b9e\u9645\u77e5\u8bc6\u751f\u6210\u9879\u76ee\u63d0\u6848\u3002", "result": "PSA\u751f\u6210\u7684\u63d0\u6848\u5728\u76f2\u5ba1\u548cAI\u8bc4\u4f30\u4e2d\u8868\u73b0\u4e0e\u4e13\u5bb6\u63d0\u6848\u76f8\u5f53\u3002", "conclusion": "PSA\u6709\u6548\u7f13\u89e3\u4e86\u95ee\u9898\u8303\u56f4\u754c\u5b9a\u7684\u6311\u6218\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5b8c\u5584\u3002"}}
{"id": "2504.18849", "pdf": "https://arxiv.org/pdf/2504.18849", "abs": "https://arxiv.org/abs/2504.18849", "authors": ["Omar Naifar"], "title": "Theoretical Framework for Tempered Fractional Gradient Descent: Application to Breast Cancer Classification", "categories": ["cs.LG", "eess.IV", "26A33, 90C26, 68T05, 92C50", "G.1.6; I.2.6; J.3"], "comment": null, "summary": "This paper introduces Tempered Fractional Gradient Descent (TFGD), a novel\noptimization framework that synergizes fractional calculus with exponential\ntempering to enhance gradient-based learning. Traditional gradient descent\nmethods often suffer from oscillatory updates and slow convergence in\nhigh-dimensional, noisy landscapes. TFGD addresses these limitations by\nincorporating a tempered memory mechanism, where historical gradients are\nweighted by fractional coefficients $|w_j| = \\binom{\\alpha}{j}$ and\nexponentially decayed via a tempering parameter $\\lambda$. Theoretical analysis\nestablishes TFGD's convergence guarantees: in convex settings, it achieves an\n$\\mathcal{O}(1/K)$ rate with alignment coefficient $d_{\\alpha,\\lambda} = (1 -\ne^{-\\lambda})^{-\\alpha}$, while stochastic variants attain\n$\\mathcal{O}(1/k^\\alpha)$ error decay. The algorithm maintains $\\mathcal{O}(n)$\ntime complexity equivalent to SGD, with memory overhead scaling as\n$\\mathcal{O}(d/\\lambda)$ for parameter dimension $d$. Empirical validation on\nthe Breast Cancer Wisconsin dataset demonstrates TFGD's superiority, achieving\n98.25\\% test accuracy (vs. 92.11\\% for SGD) and 2$\\times$ faster convergence.\nThe tempered memory mechanism proves particularly effective in medical\nclassification tasks, where feature correlations benefit from stable gradient\naveraging. These results position TFGD as a robust alternative to conventional\noptimizers in both theoretical and applied machine learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTFGD\u4f18\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u6570\u9636\u5fae\u79ef\u5206\u548c\u6307\u6570\u7f13\u548c\u6280\u672f\uff0c\u63d0\u5347\u9ad8\u7ef4\u566a\u58f0\u73af\u5883\u4e2d\u7684\u68af\u5ea6\u5b66\u4e60\u6548\u679c\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u6536\u655b\u6027\uff0c\u5e76\u5728\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u663e\u8457\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u68af\u5ea6\u4e0b\u964d\u5728\u9ad8\u7ef4\u566a\u58f0\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff08\u5982\u632f\u8361\u66f4\u65b0\u3001\u6536\u655b\u6162\uff09\uff0cTFGD\u901a\u8fc7\u7ed3\u5408\u5206\u6570\u9636\u68af\u5ea6\u4e0e\u6307\u6570\u7f13\u548c\u6280\u672f\uff08\u5229\u7528\u5386\u53f2\u68af\u5ea6\u52a0\u6743\u548c\u8870\u51cf\uff09\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "TFGD\u5f15\u5165\u7f13\u5b58\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u901a\u8fc7\u5206\u6570\u9636\u7cfb\u6570\u52a0\u6743\u5386\u53f2\u68af\u5ea6\uff0c\u5e76\u4f7f\u7528\u6307\u6570\u8870\u51cf\u53c2\u6570\u03bb\u8c03\u6574\u6743\u91cd\u3002\u7406\u8bba\u5206\u6790\u8bc1\u660e\u5176\u6536\u655b\u6027\uff08\u51f8\u573a\u666fO(1/K)\uff0c\u968f\u673a\u53d8\u4f53O(1/k^\u03b1)\uff09\uff0c\u5e76\u4fdd\u6301\u4e0eSGD\u76f8\u540c\u7684O(n)\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1TFGD\u5728\u5a01\u65af\u5eb7\u661f\u4e73\u817a\u764c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff1a\u6d4b\u8bd5\u51c6\u786e\u738798.25%\uff08SGD\u4e3a92.11%\uff09\uff0c\u6536\u655b\u901f\u5ea6\u5feb2\u500d\uff0c\u5c24\u5176\u64c5\u957f\u5904\u7406\u533b\u5b66\u5206\u7c7b\u4efb\u52a1\u7684\u7279\u5f81\u76f8\u5173\u6027\u3002", "conclusion": "TFGD\u5728\u7406\u8bba\u548c\u5e94\u7528\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u4f18\u5316\u5668\uff0c\u6210\u4e3a\u9ad8\u7ef4\u566a\u58f0\u73af\u5883\u4e0b\u68af\u5ea6\u5b66\u4e60\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2504.19406", "pdf": "https://arxiv.org/pdf/2504.19406", "abs": "https://arxiv.org/abs/2504.19406", "authors": ["Mengxia Yu", "Bang Nguyen", "Olivia Zino", "Meng Jiang"], "title": "Context Selection and Rewriting for Video-based EducationalQuestion Generation", "categories": ["cs.CL"], "comment": null, "summary": "Educational question generation (EQG) is a crucial component of intelligent\neducational systems, significantly aiding self-assessment, active learning, and\npersonalized education. While EQG systems have emerged, existing datasets\ntypically rely on predefined, carefully edited texts, failing to represent\nreal-world classroom content, including lecture speech with a set of\ncomplementary slides. To bridge this gap, we collect a dataset of educational\nquestions based on lectures from real-world classrooms. On this realistic\ndataset, we find that current methods for EQG struggle with accurately\ngenerating questions from educational videos, particularly in aligning with\nspecific timestamps and target answers. Common challenges include selecting\ninformative contexts from extensive transcripts and ensuring generated\nquestions meaningfully incorporate the target answer. To address the\nchallenges, we introduce a novel framework utilizing large language models for\ndynamically selecting and rewriting contexts based on target timestamps and\nanswers. First, our framework selects contexts from both lecture transcripts\nand video keyframes based on answer relevance and temporal proximity. Then, we\nintegrate the contexts selected from both modalities and rewrite them into\nanswer-containing knowledge statements, to enhance the logical connection\nbetween the contexts and the desired answer. This approach significantly\nimproves the quality and relevance of the generated questions. Our dataset and\ncode are released in https://github.com/mengxiayu/COSER.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u5982\u4f55\u901a\u8fc7\u7ed3\u5408\u8bb2\u5ea7\u8f6c\u5f55\u548c\u89c6\u9891\u5173\u952e\u5e27\u6765\u63d0\u5347\u6559\u80b2\u89c6\u9891\u95ee\u9898\u751f\u6210\u7684\u8d28\u91cf\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u52a8\u6001\u4e0a\u4e0b\u6587\u9009\u62e9\u548c\u91cd\u5199\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u6559\u80b2\u95ee\u9898\u751f\u6210\u7cfb\u7edf\u901a\u5e38\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u6587\u672c\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u8bfe\u5802\u5185\u5bb9\uff08\u5982\u5e26\u5e7b\u706f\u7247\u7684\u8bb2\u5ea7\u8bed\u97f3\uff09\u3002\u4e3a\u6b64\uff0c\u7814\u7a76\u56e2\u961f\u6536\u96c6\u4e86\u771f\u5b9e\u8bfe\u5802\u8bb2\u5ea7\u7684\u95ee\u9898\u6570\u636e\u96c6\uff0c\u5e76\u53d1\u73b0\u5f53\u524d\u65b9\u6cd5\u5728\u751f\u6210\u4e0e\u65f6\u95f4\u6233\u548c\u76ee\u6807\u7b54\u6848\u5bf9\u9f50\u7684\u95ee\u9898\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u52a8\u6001\u9009\u62e9\u548c\u91cd\u5199\u4e0a\u4e0b\u6587\u3002\u9996\u5148\u6839\u636e\u7b54\u6848\u76f8\u5173\u6027\u548c\u65f6\u95f4\u4e34\u8fd1\u6027\u4ece\u8bb2\u5ea7\u8f6c\u5f55\u548c\u89c6\u9891\u5173\u952e\u5e27\u4e2d\u7b5b\u9009\u4e0a\u4e0b\u6587\uff0c\u7136\u540e\u5c06\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u6574\u5408\u5e76\u91cd\u5199\u4e3a\u5305\u542b\u7b54\u6848\u7684\u77e5\u8bc6\u9648\u8ff0\uff0c\u4ee5\u589e\u5f3a\u903b\u8f91\u8054\u7cfb\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u95ee\u9898\u7684\u8d28\u91cf\u548c\u76f8\u5173\u6027\u3002", "conclusion": "\u901a\u8fc7\u52a8\u6001\u4e0a\u4e0b\u6587\u9009\u62e9\u548c\u91cd\u5199\uff0c\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6559\u80b2\u89c6\u9891\u95ee\u9898\u751f\u6210\u4e2d\u7684\u6311\u6218\uff0c\u4e3a\u667a\u80fd\u6559\u80b2\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.18878", "pdf": "https://arxiv.org/pdf/2504.18878", "abs": "https://arxiv.org/abs/2504.18878", "authors": ["Robert Leppich", "Michael Stenger", "Daniel Grillmeyer", "Vanessa Borst", "Samuel Kounev"], "title": "TSRM: A Lightweight Temporal Feature Encoding Architecture for Time Series Forecasting and Imputation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce a temporal feature encoding architecture called Time Series\nRepresentation Model (TSRM) for multivariate time series forecasting and\nimputation. The architecture is structured around CNN-based representation\nlayers, each dedicated to an independent representation learning task and\ndesigned to capture diverse temporal patterns, followed by an attention-based\nfeature extraction layer and a merge layer, designed to aggregate extracted\nfeatures. The architecture is fundamentally based on a configuration that is\ninspired by a Transformer encoder, with self-attention mechanisms at its core.\nThe TSRM architecture outperforms state-of-the-art approaches on most of the\nseven established benchmark datasets considered in our empirical evaluation for\nboth forecasting and imputation tasks. At the same time, it significantly\nreduces complexity in the form of learnable parameters. The source code is\navailable at https://github.com/RobertLeppich/TSRM.", "AI": {"tldr": "TSRM\u662f\u4e00\u79cd\u57fa\u4e8eCNN\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e0e\u586b\u8865\u67b6\u6784\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u53c2\u6570\u66f4\u5c11\u3002", "motivation": "\u89e3\u51b3\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e0e\u586b\u8865\u4e2d\u9ad8\u6548\u6355\u6349\u591a\u6837\u65f6\u5e8f\u6a21\u5f0f\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408CNN\u8868\u793a\u5c42\u3001\u6ce8\u610f\u529b\u7279\u5f81\u63d0\u53d6\u5c42\u548c\u5408\u5e76\u5c42\uff0c\u53d7Transformer\u7f16\u7801\u5668\u542f\u53d1\u3002", "result": "\u57287\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u53ef\u5b66\u4e60\u53c2\u6570\u3002", "conclusion": "TSRM\u901a\u8fc7\u9ad8\u6548\u67b6\u6784\u8bbe\u8ba1\uff0c\u5728\u6027\u80fd\u4e0e\u590d\u6742\u5ea6\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u3002"}}
{"id": "2504.19413", "pdf": "https://arxiv.org/pdf/2504.19413", "abs": "https://arxiv.org/abs/2504.19413", "authors": ["Prateek Chhikara", "Dev Khant", "Saket Aryan", "Taranjeet Singh", "Deshraj Yadav"], "title": "Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable prowess in\ngenerating contextually coherent responses, yet their fixed context windows\npose fundamental challenges for maintaining consistency over prolonged\nmulti-session dialogues. We introduce Mem0, a scalable memory-centric\narchitecture that addresses this issue by dynamically extracting,\nconsolidating, and retrieving salient information from ongoing conversations.\nBuilding on this foundation, we further propose an enhanced variant that\nleverages graph-based memory representations to capture complex relational\nstructures among conversational elements. Through comprehensive evaluations on\nLOCOMO benchmark, we systematically compare our approaches against six baseline\ncategories: (i) established memory-augmented systems, (ii) retrieval-augmented\ngeneration (RAG) with varying chunk sizes and k-values, (iii) a full-context\napproach that processes the entire conversation history, (iv) an open-source\nmemory solution, (v) a proprietary model system, and (vi) a dedicated memory\nmanagement platform. Empirical results show that our methods consistently\noutperform all existing memory systems across four question categories:\nsingle-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26%\nrelative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with\ngraph memory achieves around 2% higher overall score than the base\nconfiguration. Beyond accuracy gains, we also markedly reduce computational\noverhead compared to full-context method. In particular, Mem0 attains a 91%\nlower p95 latency and saves more than 90% token cost, offering a compelling\nbalance between advanced reasoning capabilities and practical deployment\nconstraints. Our findings highlight critical role of structured, persistent\nmemory mechanisms for long-term conversational coherence, paving the way for\nmore reliable and efficient LLM-driven AI agents.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Mem0\uff0c\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u8bb0\u5fc6\u4e2d\u5fc3\u67b6\u6784\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u4f1a\u8bdd\u5bf9\u8bdd\u4e2d\u56e0\u56fa\u5b9a\u4e0a\u4e0b\u6587\u7a97\u53e3\u5bfc\u81f4\u7684\u8fde\u8d2f\u6027\u95ee\u9898\u3002Mem0\u901a\u8fc7\u52a8\u6001\u63d0\u53d6\u3001\u6574\u5408\u548c\u68c0\u7d22\u5bf9\u8bdd\u4e2d\u7684\u5173\u952e\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u56fe\u8bb0\u5fc6\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8bdd\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "LLMs\u5728\u957f\u5bf9\u8bdd\u4e2d\u56e0\u56fa\u5b9a\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u9650\u5236\u96be\u4ee5\u4fdd\u6301\u4e00\u81f4\u6027\uff0c\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u9ad8\u6548\u8bb0\u5fc6\u673a\u5236\u4ee5\u63d0\u5347\u957f\u671f\u5bf9\u8bdd\u7684\u8fde\u8d2f\u6027\u3002", "method": "\u63d0\u51faMem0\u67b6\u6784\u53ca\u589e\u5f3a\u7248\u672c\uff08\u57fa\u4e8e\u56fe\u8bb0\u5fc6\u8868\u793a\uff09\uff0c\u52a8\u6001\u7ba1\u7406\u5bf9\u8bdd\u4fe1\u606f\uff0c\u5e76\u4e0e\u516d\u7c7b\u57fa\u7ebf\u65b9\u6cd5\u5728LOCOMO\u57fa\u51c6\u4e0a\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "Mem0\u5728LLM-as-a-Judge\u6307\u6807\u4e0a\u76f8\u5bf9OpenAI\u63d0\u534726%\uff0c\u56fe\u8bb0\u5fc6\u7248\u672c\u8fdb\u4e00\u6b65\u63d0\u9ad82%\u3002\u6b64\u5916\uff0cMem0\u964d\u4f4e91%\u5ef6\u8fdf\u5e76\u51cf\u5c1190%\u4ee5\u4e0atoken\u5f00\u9500\u3002", "conclusion": "\u7ed3\u6784\u5316\u6301\u4e45\u8bb0\u5fc6\u673a\u5236\u5bf9\u957f\u671f\u5bf9\u8bdd\u8fde\u8d2f\u6027\u81f3\u5173\u91cd\u8981\uff0cMem0\u5728\u63a8\u7406\u80fd\u529b\u4e0e\u90e8\u7f72\u6210\u672c\u95f4\u53d6\u5f97\u4e86\u4f18\u5f02\u5e73\u8861\uff0c\u4e3aLLM\u9a71\u52a8\u7684AI\u4ee3\u7406\u63d0\u4f9b\u4e86\u53ef\u9760\u65b9\u6848\u3002"}}
{"id": "2504.18881", "pdf": "https://arxiv.org/pdf/2504.18881", "abs": "https://arxiv.org/abs/2504.18881", "authors": ["Hangtao Zhang", "Zhe Li", "Kairui Zhang"], "title": "TSCAN: Context-Aware Uplift Modeling via Two-Stage Training for Online Merchant Business Diagnosis", "categories": ["cs.LG"], "comment": "15 pages,7 figures", "summary": "A primary challenge in ITE estimation is sample selection bias. Traditional\napproaches utilize treatment regularization techniques such as the Integral\nProbability Metrics (IPM), re-weighting, and propensity score modeling to\nmitigate this bias. However, these regularizations may introduce undesirable\ninformation loss and limit the performance of the model. Furthermore, treatment\neffects vary across different external contexts, and the existing methods are\ninsufficient in fully interacting with and utilizing these contextual features.\nTo address these issues, we propose a Context-Aware uplift model based on the\nTwo-Stage training approach (TSCAN), comprising CAN-U and CAN-D sub-models. In\nthe first stage, we train an uplift model, called CAN-U, which includes the\ntreatment regularizations of IPM and propensity score prediction, to generate a\ncomplete dataset with counterfactual uplift labels. In the second stage, we\ntrain a model named CAN-D, which utilizes an isotonic output layer to directly\nmodel uplift effects, thereby eliminating the reliance on the regularization\ncomponents. CAN-D adaptively corrects the errors estimated by CAN-U through\nreinforcing the factual samples, while avoiding the negative impacts associated\nwith the aforementioned regularizations. Additionally, we introduce a\nContext-Aware Attention Layer throughout the two-stage process to manage the\ninteractions between treatment, merchant, and contextual features, thereby\nmodeling the varying treatment effect in different contexts. We conduct\nextensive experiments on two real-world datasets to validate the effectiveness\nof TSCAN. Ultimately, the deployment of our model for real-world merchant\ndiagnosis on one of China's largest online food ordering platforms validates\nits practical utility and impact.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u63d0\u5347\u6a21\u578b\uff08TSCAN\uff09\uff0c\u5305\u542bCAN-U\u548cCAN-D\u5b50\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u6837\u672c\u9009\u62e9\u504f\u5dee\u95ee\u9898\u5e76\u901a\u8fc7\u4e0a\u4e0b\u6587\u4ea4\u4e92\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528IPM\u3001\u91cd\u52a0\u6743\u548c\u503e\u5411\u5f97\u5206\u5efa\u6a21\u7b49\u5904\u7406\u6b63\u5219\u5316\u6280\u672f\u6765\u7f13\u89e3\u6837\u672c\u9009\u62e9\u504f\u5dee\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u4fe1\u606f\u635f\u5931\u5e76\u9650\u5236\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u65e0\u6cd5\u5145\u5206\u5229\u7528\u4e0a\u4e0b\u6587\u7279\u5f81\u3002", "method": "TSCAN\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u8bad\u7ec3CAN-U\u6a21\u578b\u751f\u6210\u53cd\u4e8b\u5b9e\u63d0\u5347\u6807\u7b7e\uff1b\u7b2c\u4e8c\u9636\u6bb5\u8bad\u7ec3CAN-D\u6a21\u578b\u76f4\u63a5\u5efa\u6a21\u63d0\u5347\u6548\u679c\uff0c\u65e0\u9700\u4f9d\u8d56\u6b63\u5219\u5316\u7ec4\u4ef6\uff0c\u5e76\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u6ce8\u610f\u529b\u5c42\u5904\u7406\u4ea4\u4e92\u7279\u5f81\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86TSCAN\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728\u4e2d\u56fd\u6700\u5927\u7684\u5728\u7ebf\u98df\u54c1\u8ba2\u8d2d\u5e73\u53f0\u4e0a\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "TSCAN\u901a\u8fc7\u52a8\u6001\u6821\u6b63\u548c\u4e0a\u4e0b\u6587\u4ea4\u4e92\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u907f\u514d\u4e86\u4f20\u7edf\u6b63\u5219\u5316\u65b9\u6cd5\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.19436", "pdf": "https://arxiv.org/pdf/2504.19436", "abs": "https://arxiv.org/abs/2504.19436", "authors": ["Jacky He", "Guiran Liu", "Binrong Zhu", "Hanlu Zhang", "Hongye Zheng", "Xiaokai Wang"], "title": "Context-Guided Dynamic Retrieval for Improving Generation Quality in RAG Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper focuses on the dynamic optimization of the Retrieval-Augmented\nGeneration (RAG) architecture. It proposes a state-aware dynamic knowledge\nretrieval mechanism to enhance semantic understanding and knowledge scheduling\nefficiency in large language models for open-domain question answering and\ncomplex generation tasks. The method introduces a multi-level perceptive\nretrieval vector construction strategy and a differentiable document matching\npath. These components enable end-to-end joint training and collaborative\noptimization of the retrieval and generation modules. This effectively\naddresses the limitations of static RAG structures in context adaptation and\nknowledge access. Experiments are conducted on the Natural Questions dataset.\nThe proposed structure is thoroughly evaluated across different large models,\nincluding GPT-4, GPT-4o, and DeepSeek. Comparative and ablation experiments\nfrom multiple perspectives confirm the significant improvements in BLEU and\nROUGE-L scores. The approach also demonstrates stronger robustness and\ngeneration consistency in tasks involving semantic ambiguity and multi-document\nfusion. These results highlight its broad application potential and practical\nvalue in building high-quality language generation systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u52a8\u6001\u77e5\u8bc6\u68c0\u7d22\u673a\u5236\uff0c\u901a\u8fc7\u591a\u7ea7\u611f\u77e5\u68c0\u7d22\u5411\u91cf\u6784\u5efa\u548c\u53ef\u5fae\u5206\u6587\u6863\u5339\u914d\u8def\u5f84\uff0c\u4f18\u5316\u4e86RAG\u67b6\u6784\u5728\u8bed\u4e49\u7406\u89e3\u548c\u77e5\u8bc6\u8c03\u5ea6\u4e0a\u7684\u6548\u7387\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u4efb\u52a1\u4e2d\u7684\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u9759\u6001RAG\u67b6\u6784\u5728\u4e0a\u4e0b\u6587\u9002\u5e94\u548c\u77e5\u8bc6\u8bbf\u95ee\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u56e0\u6b64\u9700\u8981\u52a8\u6001\u4f18\u5316\u7684\u673a\u5236\u6765\u589e\u5f3a\u8bed\u4e49\u7406\u89e3\u548c\u77e5\u8bc6\u8c03\u5ea6\u6548\u7387\u3002", "method": "\u5f15\u5165\u591a\u7ea7\u611f\u77e5\u68c0\u7d22\u5411\u91cf\u6784\u5efa\u7b56\u7565\u548c\u53ef\u5fae\u5206\u6587\u6863\u5339\u914d\u8def\u5f84\uff0c\u5b9e\u73b0\u68c0\u7d22\u4e0e\u751f\u6210\u6a21\u5757\u7684\u7aef\u5230\u7aef\u8054\u5408\u8bad\u7ec3\u548c\u534f\u540c\u4f18\u5316\u3002", "result": "\u5728Natural Questions\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBLEU\u548cROUGE-L\u5206\u6570\u663e\u8457\u63d0\u5347\uff0c\u4e14\u5728\u8bed\u4e49\u6a21\u7cca\u548c\u591a\u6587\u6863\u878d\u5408\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u751f\u6210\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u9ad8\u8d28\u91cf\u8bed\u8a00\u751f\u6210\u7cfb\u7edf\u4e2d\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.18882", "pdf": "https://arxiv.org/pdf/2504.18882", "abs": "https://arxiv.org/abs/2504.18882", "authors": ["Ce Ju", "Reinmar J. Kobler", "Antoine Collas", "Motoaki Kawanabe", "Cuntai Guan", "Bertrand Thirion"], "title": "SPD Learning for Covariance-Based Neuroimaging Analysis: Perspectives, Methods, and Challenges", "categories": ["cs.LG", "cs.AI", "eess.IV", "q-bio.NC", "I.2.0"], "comment": "20 pages, 3 figures, 2 tables; This paper has been submitted for\n  possible publication, and currently under review", "summary": "Neuroimaging provides a critical framework for characterizing brain activity\nby quantifying connectivity patterns and functional architecture across\nmodalities. While modern machine learning has significantly advanced our\nunderstanding of neural processing mechanisms through these datasets, decoding\ntask-specific signatures must contend with inherent neuroimaging constraints,\nfor example, low signal-to-noise ratios in raw electrophysiological recordings,\ncross-session non-stationarity, and limited sample sizes. This review focuses\non machine learning approaches for covariance-based neuroimaging data, where\noften symmetric positive definite (SPD) matrices under full-rank conditions\nencode inter-channel relationships. By equipping the space of SPD matrices with\nRiemannian metrics (e.g., affine-invariant or log-Euclidean), their space forms\na Riemannian manifold enabling geometric analysis. We unify methodologies\noperating on this manifold under the SPD learning framework, which\nsystematically leverages the SPD manifold's geometry to process covariance\nfeatures, thereby advancing brain imaging analytics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u534f\u65b9\u5dee\u7684\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528Riemannian\u51e0\u4f55\u5206\u6790SPD\u77e9\u9635\u7a7a\u95f4\u4ee5\u63d0\u5347\u8111\u6210\u50cf\u5206\u6790\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u4e2d\u7684\u4f4e\u4fe1\u566a\u6bd4\u3001\u8de8\u4f1a\u8bdd\u975e\u5e73\u7a33\u6027\u548c\u6837\u672c\u91cf\u6709\u9650\u7b49\u95ee\u9898\uff0c\u901a\u8fc7\u51e0\u4f55\u65b9\u6cd5\u4f18\u5316\u4efb\u52a1\u7279\u5f02\u6027\u7279\u5f81\u7684\u89e3\u7801\u3002", "method": "\u91c7\u7528Riemannian\u5ea6\u91cf\uff08\u5982\u4eff\u5c04\u4e0d\u53d8\u6216\u5bf9\u6570\u6b27\u51e0\u91cc\u5f97\uff09\u5728SPD\u77e9\u9635\u7a7a\u95f4\u8fdb\u884c\u51e0\u4f55\u5206\u6790\uff0c\u5f62\u6210SPD\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u5229\u7528SPD\u6d41\u5f62\u7684\u51e0\u4f55\u7279\u6027\uff0c\u6539\u8fdb\u4e86\u534f\u65b9\u5dee\u7279\u5f81\u7684\u5904\u7406\uff0c\u63a8\u52a8\u4e86\u8111\u6210\u50cf\u5206\u6790\u7684\u53d1\u5c55\u3002", "conclusion": "SPD\u5b66\u4e60\u6846\u67b6\u4e3a\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u7684\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u51e0\u4f55\u5206\u6790\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u89e3\u7801\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.19445", "pdf": "https://arxiv.org/pdf/2504.19445", "abs": "https://arxiv.org/abs/2504.19445", "authors": ["Yi-Long Lu", "Chunhui Zhang", "Wei Wang"], "title": "Systematic Bias in Large Language Models: Discrepant Response Patterns in Binary vs. Continuous Judgment Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used in tasks such as\npsychological text analysis and decision-making in automated workflows.\nHowever, their reliability remains a concern due to potential biases inherited\nfrom their training process. In this study, we examine how different response\nformat: binary versus continuous, may systematically influence LLMs' judgments.\nIn a value statement judgments task and a text sentiment analysis task, we\nprompted LLMs to simulate human responses and tested both formats across\nseveral models, including both open-source and commercial models. Our findings\nrevealed a consistent negative bias: LLMs were more likely to deliver\n\"negative\" judgments in binary formats compared to continuous ones. Control\nexperiments further revealed that this pattern holds across both tasks. Our\nresults highlight the importance of considering response format when applying\nLLMs to decision tasks, as small changes in task design can introduce\nsystematic biases.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e8c\u5143\u548c\u8fde\u7eed\u54cd\u5e94\u683c\u5f0f\u4e0b\u5b58\u5728\u7cfb\u7edf\u6027\u5224\u65ad\u504f\u5dee\uff0c\u5c24\u5176\u5728\u4e8c\u5143\u683c\u5f0f\u4e2d\u66f4\u5bb9\u6613\u4ea7\u751f\u8d1f\u9762\u5224\u65ad\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u4e0d\u540c\u54cd\u5e94\u683c\u5f0f\uff08\u4e8c\u5143\u4e0e\u8fde\u7eed\uff09\u4e0b\u5224\u65ad\u7684\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u4ee5\u63d0\u9ad8\u5176\u5728\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u4ef7\u503c\u5224\u65ad\u4efb\u52a1\u548c\u6587\u672c\u60c5\u611f\u5206\u6790\u4efb\u52a1\uff0c\u6d4b\u8bd5\u591a\u79cd\u5f00\u6e90\u548c\u5546\u4e1aLLM\u6a21\u578b\u5728\u4e24\u79cd\u54cd\u5e94\u683c\u5f0f\u4e0b\u7684\u8868\u73b0\u3002", "result": "LLMs\u5728\u4e8c\u5143\u683c\u5f0f\u4e2d\u66f4\u503e\u5411\u4e8e\u8d1f\u9762\u5224\u65ad\uff0c\u4e14\u8fd9\u79cd\u6a21\u5f0f\u5728\u4e24\u79cd\u4efb\u52a1\u4e2d\u5747\u4fdd\u6301\u4e00\u81f4\u3002", "conclusion": "\u4efb\u52a1\u8bbe\u8ba1\u4e2d\u7684\u5fae\u5c0f\u53d8\u5316\uff08\u5982\u54cd\u5e94\u683c\u5f0f\uff09\u53ef\u80fd\u5f15\u5165\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u9700\u8c28\u614e\u5e94\u7528LLMs\u4e8e\u51b3\u7b56\u4efb\u52a1\u3002"}}
{"id": "2504.18914", "pdf": "https://arxiv.org/pdf/2504.18914", "abs": "https://arxiv.org/abs/2504.18914", "authors": ["Ma\u0142gorzata \u0141az\u0119cka", "Ewa Szczurek"], "title": "Factor Analysis with Correlated Topic Model for Multi-Modal Data", "categories": ["cs.LG", "stat.AP", "stat.ML"], "comment": "AISTATS 2025", "summary": "Integrating various data modalities brings valuable insights into underlying\nphenomena. Multimodal factor analysis (FA) uncovers shared axes of variation\nunderlying different simple data modalities, where each sample is represented\nby a vector of features. However, FA is not suited for structured data\nmodalities, such as text or single cell sequencing data, where multiple data\npoints are measured per each sample and exhibit a clustering structure. To\novercome this challenge, we introduce FACTM, a novel, multi-view and\nmulti-structure Bayesian model that combines FA with correlated topic modeling\nand is optimized using variational inference. Additionally, we introduce a\nmethod for rotating latent factors to enhance interpretability with respect to\nbinary features. On text and video benchmarks as well as real-world music and\nCOVID-19 datasets, we demonstrate that FACTM outperforms other methods in\nidentifying clusters in structured data, and integrating them with simple\nmodalities via the inference of shared, interpretable factors.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86FACTM\u6a21\u578b\uff0c\u7ed3\u5408\u56e0\u5b50\u5206\u6790\u548c\u76f8\u5173\u4e3b\u9898\u5efa\u6a21\uff0c\u4ee5\u5904\u7406\u7ed3\u6784\u5316\u6570\u636e\u6a21\u6001\uff0c\u5e76\u901a\u8fc7\u53d8\u5206\u63a8\u65ad\u4f18\u5316\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u8bc6\u522b\u805a\u7c7b\u548c\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7684\u591a\u6a21\u6001\u56e0\u5b50\u5206\u6790\uff08FA\uff09\u65e0\u6cd5\u6709\u6548\u5904\u7406\u7ed3\u6784\u5316\u6570\u636e\uff08\u5982\u6587\u672c\u6216\u5355\u7ec6\u80de\u6d4b\u5e8f\u6570\u636e\uff09\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6570\u636e\u5177\u6709\u805a\u7c7b\u7ed3\u6784\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u6574\u5408\u8fd9\u4e9b\u6570\u636e\u6a21\u6001\u5e76\u63d0\u53d6\u53ef\u89e3\u91ca\u7684\u5171\u4eab\u56e0\u5b50\u3002", "method": "FACTM\u662f\u4e00\u79cd\u591a\u89c6\u56fe\u3001\u591a\u7ed3\u6784\u7684\u8d1d\u53f6\u65af\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u56e0\u5b50\u5206\u6790\u548c\u76f8\u5173\u4e3b\u9898\u5efa\u6a21\uff0c\u4f7f\u7528\u53d8\u5206\u63a8\u65ad\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u5f15\u5165\u65cb\u8f6c\u6f5c\u5728\u56e0\u5b50\u7684\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728\u6587\u672c\u3001\u89c6\u9891\u57fa\u51c6\u4ee5\u53ca\u97f3\u4e50\u548cCOVID-19\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cFACTM\u5728\u8bc6\u522b\u7ed3\u6784\u5316\u6570\u636e\u4e2d\u7684\u805a\u7c7b\u548c\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "FACTM\u662f\u4e00\u79cd\u6709\u6548\u7684\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u5408\u5904\u7406\u7ed3\u6784\u5316\u6570\u636e\uff0c\u5e76\u80fd\u63d0\u53d6\u53ef\u89e3\u91ca\u7684\u5171\u4eab\u56e0\u5b50\uff0c\u4e3a\u591a\u6a21\u6001\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.19457", "pdf": "https://arxiv.org/pdf/2504.19457", "abs": "https://arxiv.org/abs/2504.19457", "authors": ["Siyi Liu", "Kishaloy Halder", "Zheng Qi", "Wei Xiao", "Nikolaos Pappas", "Phu Mon Htut", "Neha Anna John", "Yassine Benajiba", "Dan Roth"], "title": "Towards Long Context Hallucination Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks. However, they are prone to contextual hallucination, generating\ninformation that is either unsubstantiated or contradictory to the given\ncontext. Although many studies have investigated contextual hallucinations in\nLLMs, addressing them in long-context inputs remains an open problem. In this\nwork, we take an initial step toward solving this problem by constructing a\ndataset specifically designed for long-context hallucination detection.\nFurthermore, we propose a novel architecture that enables pre-trained encoder\nmodels, such as BERT, to process long contexts and effectively detect\ncontextual hallucinations through a decomposition and aggregation mechanism.\nOur experimental results show that the proposed architecture significantly\noutperforms previous models of similar size as well as LLM-based models across\nvarious metrics, while providing substantially faster inference.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u957f\u4e0a\u4e0b\u6587\u8f93\u5165\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0a\u4e0b\u6587\u5e7b\u89c9\u95ee\u9898\u7684\u6570\u636e\u96c6\u548c\u65b0\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u89e3\u548c\u805a\u5408\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u957f\u4e0a\u4e0b\u6587\u8f93\u5165\u4e2d\u5bb9\u6613\u4ea7\u751f\u4e0d\u5207\u5b9e\u9645\u6216\u81ea\u76f8\u77db\u76fe\u7684\u5e7b\u89c9\u4fe1\u606f\uff0c\u8fd9\u4e00\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u89e3\u51b3\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u957f\u4e0a\u4e0b\u6587\u5e7b\u89c9\u68c0\u6d4b\u7684\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u67b6\u6784\uff0c\u4f7f\u9884\u8bad\u7ec3\u7684\u7f16\u7801\u5668\u6a21\u578b\uff08\u5982BERT\uff09\u80fd\u591f\u901a\u8fc7\u5206\u89e3\u548c\u805a\u5408\u673a\u5236\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u5e76\u68c0\u6d4b\u5e7b\u89c9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u67b6\u6784\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u7c7b\u4f3c\u89c4\u6a21\u7684\u4f20\u7edf\u6a21\u578b\u548c\u57fa\u4e8eLLM\u7684\u6a21\u578b\uff0c\u4e14\u63a8\u7406\u901f\u5ea6\u5927\u5e45\u63d0\u5347\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u8f93\u5165\u4e2d\u7684LLM\u5e7b\u89c9\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\uff0c\u5e76\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2504.18563", "pdf": "https://arxiv.org/pdf/2504.18563", "abs": "https://arxiv.org/abs/2504.18563", "authors": ["Abha Jha", "Ashwath Vaithinathan Aravindan", "Matthew Salaway", "Atharva Sandeep Bhide", "Duygu Nur Yaldiz"], "title": "Backdoor Defense in Diffusion Models via Spatial Attention Unlearning", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": null, "summary": "Text-to-image diffusion models are increasingly vulnerable to backdoor\nattacks, where malicious modifications to the training data cause the model to\ngenerate unintended outputs when specific triggers are present. While\nclassification models have seen extensive development of defense mechanisms,\ngenerative models remain largely unprotected due to their high-dimensional\noutput space, which complicates the detection and mitigation of subtle\nperturbations. Defense strategies for diffusion models, in particular, remain\nunder-explored. In this work, we propose Spatial Attention Unlearning (SAU), a\nnovel technique for mitigating backdoor attacks in diffusion models. SAU\nleverages latent space manipulation and spatial attention mechanisms to isolate\nand remove the latent representation of backdoor triggers, ensuring precise and\nefficient removal of malicious effects. We evaluate SAU across various types of\nbackdoor attacks, including pixel-based and style-based triggers, and\ndemonstrate its effectiveness in achieving 100% trigger removal accuracy.\nFurthermore, SAU achieves a CLIP score of 0.7023, outperforming existing\nmethods while preserving the model's ability to generate high-quality,\nsemantically aligned images. Our results show that SAU is a robust, scalable,\nand practical solution for securing text-to-image diffusion models against\nbackdoor attacks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aSAU\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9632\u5fa1\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u7684\u540e\u95e8\u653b\u51fb\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u64cd\u4f5c\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\u79fb\u9664\u540e\u95e8\u89e6\u53d1\u5668\uff0c\u6548\u679c\u663e\u8457\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u9ad8\u7ef4\u8f93\u51fa\u7a7a\u95f4\u4e2d\u7684\u540e\u95e8\u653b\u51fb\u9632\u5fa1\u673a\u5236\u7f3a\u4e4f\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5e94\u5bf9\u6b64\u7c7b\u5a01\u80c1\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u9632\u5fa1\u7b56\u7565\u3002", "method": "\u91c7\u7528\u7a7a\u95f4\u6ce8\u610f\u529b\u9057\u5fd8\uff08SAU\uff09\u6280\u672f\uff0c\u7ed3\u5408\u6f5c\u5728\u7a7a\u95f4\u64cd\u4f5c\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7cbe\u786e\u8bc6\u522b\u5e76\u79fb\u9664\u540e\u95e8\u89e6\u53d1\u5668\u7684\u6f5c\u5728\u8868\u793a\u3002", "result": "SAU\u5728\u6240\u6709\u6d4b\u8bd5\u7684\u540e\u95e8\u653b\u51fb\u4e2d\u5b9e\u73b0100%\u89e6\u53d1\u5668\u79fb\u9664\u51c6\u786e\u7387\uff0cCLIP\u5206\u6570\u8fbe0.7023\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u4fdd\u6301\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "SAU\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u6709\u6548\u4fdd\u62a4\u6269\u6563\u6a21\u578b\u514d\u53d7\u540e\u95e8\u653b\u51fb\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002"}}
{"id": "2504.18929", "pdf": "https://arxiv.org/pdf/2504.18929", "abs": "https://arxiv.org/abs/2504.18929", "authors": ["Ruifeng Ren", "Yong Liu"], "title": "Revisiting Transformers through the Lens of Low Entropy and Dynamic Sparsity", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Compression has been a critical lens to understand the success of\nTransformers. In the past, we have typically taken the target distribution as a\ncriterion to evaluate a model's compression performance. Nevertheless,it often\nremains challenging to precisely assess how well the model achieves compression\nand to compare the information content of the learned distribution with that of\nthe target distribution during compression,as the target distribution is\ntypically unknown and entropy computation often incurs exponential cost. In\nthis work, we explore these issues under a controlled experimental setup. We\nfind that Transformers exhibit a unique inductive bias in data compression:\nbeyond approaching the target distribution, they tend to favor learning\nlower-entropy distributions, with this tendency becoming more pronounced as the\nmodel size increases. This preference prevents Transformers from perfectly\naligning with the target distribution, instead further compressing its\ninformation content. Furthermore, we show that the FFN module plays a critical\nrole in driving this bias. In addition, while models remove informational\nredundancy from data during compression, they also exhibit redundancy within\ntheir parameters, which enables compression and can be characterized through\ndynamic sparsity. However, the dynamic sparsity patterns in Transformers,\nparticularly in attention and FFN modules, demand further exploration. As for\nthis, we show that larger Transformers show stronger preferences for bypassing\nattention computations via residual connections and have lower proportion of\nactive neurons. Interestingly, we also find that training instability in larger\nmodels strongly correlates with sudden increases in dead neurons. Our work\ncontributes to a deeper understanding of Transformers from the lens of entropy\nand dynamic sparsity.", "AI": {"tldr": "Transformers\u8868\u73b0\u51fa\u72ec\u7279\u7684\u5f52\u7eb3\u504f\u597d\u5728\u6570\u636e\u538b\u7f29\u4e2d\uff0c\u503e\u5411\u4e8e\u5b66\u4e60\u66f4\u4f4e\u71b5\u7684\u5206\u5e03\uff0c\u4e14\u8d8a\u5927\u7684\u6a21\u578b\u8d8a\u660e\u663e\u3002\u8fd9\u4e00\u504f\u597d\u963b\u6b62\u4e86\u5176\u4e0e\u76ee\u6807\u5206\u5e03\u7684\u5b8c\u7f8e\u5bf9\u9f50\uff0c\u540c\u65f6FFN\u6a21\u5757\u5728\u8fd9\u4e00\u504f\u597d\u7684\u9a71\u52a8\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u3002\u6b64\u5916\uff0c\u6a21\u578b\u53c2\u6570\u7684\u5197\u4f59\u6027\u901a\u8fc7\u52a8\u6001\u7a00\u758f\u6027\u4f53\u73b0\uff0c\u4e14\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u4e0e\u6b7b\u795e\u7ecf\u5143\u7684\u7a81\u7136\u589e\u52a0\u76f8\u5173\u3002", "motivation": "\u7814\u7a76Transformers\u5728\u6570\u636e\u538b\u7f29\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5176\u538b\u7f29\u6027\u80fd\u4e0e\u76ee\u6807\u5206\u5e03\u7684\u5173\u7cfb\uff0c\u4ee5\u53ca\u6a21\u578b\u53c2\u6570\u5197\u4f59\u6027\u7684\u52a8\u6001\u7a00\u758f\u6027\u8868\u73b0\u3002", "method": "\u5728\u63a7\u5236\u5b9e\u9a8c\u8bbe\u7f6e\u4e0b\uff0c\u5206\u6790Transformers\u7684\u538b\u7f29\u884c\u4e3a\uff0c\u5305\u62ec\u71b5\u7684\u504f\u597d\u3001FFN\u6a21\u5757\u7684\u4f5c\u7528\uff0c\u4ee5\u53ca\u52a8\u6001\u7a00\u758f\u6027\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0Transformers\u503e\u5411\u4e8e\u5b66\u4e60\u66f4\u4f4e\u71b5\u7684\u5206\u5e03\uff0cFFN\u6a21\u5757\u9a71\u52a8\u8fd9\u4e00\u504f\u597d\u3002\u6a21\u578b\u53c2\u6570\u5197\u4f59\u6027\u4ee5\u52a8\u6001\u7a00\u758f\u6027\u4e3a\u7279\u5f81\uff0c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u4e0e\u6b7b\u795e\u7ecf\u5143\u589e\u52a0\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u6df1\u5316\u4e86\u5bf9Transformers\u5728\u71b5\u548c\u52a8\u6001\u7a00\u758f\u6027\u65b9\u9762\u7684\u7406\u89e3\uff0c\u63ed\u793a\u4e86\u5176\u538b\u7f29\u884c\u4e3a\u548c\u53c2\u6570\u5197\u4f59\u6027\u7684\u5173\u952e\u7279\u6027\u3002"}}
{"id": "2504.19467", "pdf": "https://arxiv.org/pdf/2504.19467", "abs": "https://arxiv.org/abs/2504.19467", "authors": ["Jiageng Wu", "Bowen Gu", "Ren Zhou", "Kevin Xie", "Doug Snyder", "Yixing Jiang", "Valentina Carducci", "Richard Wyss", "Rishi J Desai", "Emily Alsentzer", "Leo Anthony Celi", "Adam Rodman", "Sebastian Schneeweiss", "Jonathan H. Chen", "Santiago Romero-Brufau", "Kueiyu Joshua Lin", "Jie Yang"], "title": "BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) hold great promise for medical applications and\nare evolving rapidly, with new models being released at an accelerated pace.\nHowever, current evaluations of LLMs in clinical contexts remain limited. Most\nexisting benchmarks rely on medical exam-style questions or PubMed-derived\ntext, failing to capture the complexity of real-world electronic health record\n(EHR) data. Others focus narrowly on specific application scenarios, limiting\ntheir generalizability across broader clinical use. To address this gap, we\npresent BRIDGE, a comprehensive multilingual benchmark comprising 87 tasks\nsourced from real-world clinical data sources across nine languages. We\nsystematically evaluated 52 state-of-the-art LLMs (including DeepSeek-R1,\nGPT-4o, Gemini, and Llama 4) under various inference strategies. With a total\nof 13,572 experiments, our results reveal substantial performance variation\nacross model sizes, languages, natural language processing tasks, and clinical\nspecialties. Notably, we demonstrate that open-source LLMs can achieve\nperformance comparable to proprietary models, while medically fine-tuned LLMs\nbased on older architectures often underperform versus updated general-purpose\nmodels. The BRIDGE and its corresponding leaderboard serve as a foundational\nresource and a unique reference for the development and evaluation of new LLMs\nin real-world clinical text understanding.", "AI": {"tldr": "BRIDGE\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u4e34\u5e8a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8986\u76d687\u4e2a\u4efb\u52a1\u548c9\u79cd\u8bed\u8a00\uff0c\u7528\u4e8e\u8bc4\u4f3052\u79cd\u5148\u8fdbLLM\u5728\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u4e0a\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793a\u5f00\u6e90\u6a21\u578b\u53ef\u4e0e\u4e13\u6709\u6a21\u578b\u5ab2\u7f8e\u3002", "motivation": "\u5f53\u524dLLM\u5728\u4e34\u5e8a\u80cc\u666f\u4e0b\u7684\u8bc4\u4f30\u53d7\u9650\uff0c\u73b0\u6709\u57fa\u51c6\u672a\u80fd\u53cd\u6620\u771f\u5b9e\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u7684\u590d\u6742\u6027\u6216\u901a\u7528\u4e34\u5e8a\u573a\u666f\u3002BRIDGE\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faBRIDGE\u57fa\u51c6\uff0c\u5305\u542b87\u4e2a\u4efb\u52a1\uff0c\u6e90\u81ea\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\uff0c\u591a\u8bed\u8a00\u8986\u76d6\u3002\u7cfb\u7edf\u6027\u8bc4\u4f3052\u79cdLLM\uff08\u5982DeepSeek-R1\u3001GPT-4o\u7b49\uff09\u5728\u4e0d\u540c\u63a8\u7406\u7b56\u7565\u4e0b\u7684\u8868\u73b0\u3002", "result": "13,572\u6b21\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u56e0\u6a21\u578b\u89c4\u6a21\u3001\u8bed\u8a00\u3001\u4efb\u52a1\u53ca\u4e34\u5e8a\u9886\u57df\u800c\u5f02\u3002\u5f00\u6e90LLM\u8868\u73b0\u4e0e\u4e13\u6709\u6a21\u578b\u76f8\u5f53\uff0c\u800c\u57fa\u4e8e\u65e7\u67b6\u6784\u7684\u533b\u5b66\u5fae\u8c03\u6a21\u578b\u5e38\u900a\u4e8e\u65b0\u7248\u901a\u7528\u6a21\u578b\u3002", "conclusion": "BRIDGE\u4e3a\u4e34\u5e8a\u6587\u672c\u7406\u89e3\u7684LLM\u5f00\u53d1\u4e0e\u8bc4\u4f30\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\u548c\u53c2\u8003\uff0c\u8bc1\u660e\u5f00\u6e90\u6a21\u578b\u7684\u6f5c\u529b\u53ca\u901a\u7528\u6a21\u578b\u7684\u4f18\u52bf\u3002"}}
{"id": "2504.18564", "pdf": "https://arxiv.org/pdf/2504.18564", "abs": "https://arxiv.org/abs/2504.18564", "authors": ["Xinzhe Huang", "Kedong Xiu", "Tianhang Zheng", "Churui Zeng", "Wangze Ni", "Zhan Qiin", "Kui Ren", "Chun Chen"], "title": "DualBreach: Efficient Dual-Jailbreaking via Target-Driven Initialization and Multi-Target Optimization", "categories": ["cs.CR", "cs.AI"], "comment": "20 pages, 8 figures", "summary": "Recent research has focused on exploring the vulnerabilities of Large\nLanguage Models (LLMs), aiming to elicit harmful and/or sensitive content from\nLLMs. However, due to the insufficient research on dual-jailbreaking -- attacks\ntargeting both LLMs and Guardrails, the effectiveness of existing attacks is\nlimited when attempting to bypass safety-aligned LLMs shielded by guardrails.\nTherefore, in this paper, we propose DualBreach, a target-driven framework for\ndual-jailbreaking. DualBreach employs a Target-driven Initialization (TDI)\nstrategy to dynamically construct initial prompts, combined with a Multi-Target\nOptimization (MTO) method that utilizes approximate gradients to jointly adapt\nthe prompts across guardrails and LLMs, which can simultaneously save the\nnumber of queries and achieve a high dual-jailbreaking success rate. For\nblack-box guardrails, DualBreach either employs a powerful open-sourced\nguardrail or imitates the target black-box guardrail by training a proxy model,\nto incorporate guardrails into the MTO process.\n  We demonstrate the effectiveness of DualBreach in dual-jailbreaking scenarios\nthrough extensive evaluation on several widely-used datasets. Experimental\nresults indicate that DualBreach outperforms state-of-the-art methods with\nfewer queries, achieving significantly higher success rates across all\nsettings. More specifically, DualBreach achieves an average dual-jailbreaking\nsuccess rate of 93.67% against GPT-4 with Llama-Guard-3 protection, whereas the\nbest success rate achieved by other methods is 88.33%. Moreover, DualBreach\nonly uses an average of 1.77 queries per successful dual-jailbreak,\noutperforming other state-of-the-art methods. For the purpose of defense, we\npropose an XGBoost-based ensemble defensive mechanism named EGuard, which\nintegrates the strengths of multiple guardrails, demonstrating superior\nperformance compared with Llama-Guard-3.", "AI": {"tldr": "DualBreach \u662f\u4e00\u4e2a\u9488\u5bf9 LLM \u548c Guardrails \u7684\u53cc\u91cd\u8d8a\u72f1\u6846\u67b6\uff0c\u901a\u8fc7\u76ee\u6807\u9a71\u52a8\u521d\u59cb\u5316\u548c\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8d8a\u72f1\u6210\u529f\u7387\u5e76\u51cf\u5c11\u4e86\u67e5\u8be2\u6b21\u6570\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u6210\u529f\u7387\u8fdc\u8d85\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u9632\u5fa1\u673a\u5236 EGuard\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u9488\u5bf9 LLM \u548c\u5b89\u5168\u62a4\u680f\uff08Guardrails\uff09\u7684\u53cc\u91cd\u8d8a\u72f1\u653b\u51fb\u4e0a\u6548\u679c\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u7ed5\u8fc7\u5b89\u5168\u673a\u5236\u3002", "method": "DualBreach \u7ed3\u5408\u76ee\u6807\u9a71\u52a8\u521d\u59cb\u5316\uff08TDI\uff09\u548c\u591a\u76ee\u6807\u4f18\u5316\uff08MTO\uff09\uff0c\u901a\u8fc7\u8fd1\u4f3c\u68af\u5ea6\u8054\u5408\u4f18\u5316\u63d0\u793a\uff0c\u540c\u65f6\u5bf9\u9ed1\u76d2\u62a4\u680f\u4f7f\u7528\u4ee3\u7406\u6a21\u578b\u8fdb\u884c\u6a21\u62df\u3002", "result": "DualBreach \u5728 GPT-4 \u548c Llama-Guard-3 \u4e0a\u5e73\u5747\u8d8a\u72f1\u6210\u529f\u7387\u8fbe 93.67%\uff0c\u67e5\u8be2\u6b21\u6570\u4ec5 1.77 \u6b21/\u6b21\u6210\u529f\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u9632\u5fa1\u673a\u5236 EGuard \u8868\u73b0\u4f18\u4e8e Llama-Guard-3\u3002", "conclusion": "DualBreach \u662f\u4e00\u79cd\u9ad8\u6548\u7684\u53cc\u91cd\u8d8a\u72f1\u6846\u67b6\uff0c\u540c\u65f6\u63d0\u51fa\u7684 EGuard \u9632\u5fa1\u673a\u5236\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.19000", "pdf": "https://arxiv.org/pdf/2504.19000", "abs": "https://arxiv.org/abs/2504.19000", "authors": ["Elad Sofer", "Tomer Shaked", "Caroline Chaux", "Nir Shlezinger"], "title": "Unveiling and Mitigating Adversarial Vulnerabilities in Iterative Optimizers", "categories": ["cs.LG", "eess.SP"], "comment": "Under review for publication in the IEEE", "summary": "Machine learning (ML) models are often sensitive to carefully crafted yet\nseemingly unnoticeable perturbations. Such adversarial examples are considered\nto be a property of ML models, often associated with their black-box operation\nand sensitivity to features learned from data. This work examines the\nadversarial sensitivity of non-learned decision rules, and particularly of\niterative optimizers. Our analysis is inspired by the recent developments in\ndeep unfolding, which cast such optimizers as ML models. We show that\nnon-learned iterative optimizers share the sensitivity to adversarial examples\nof ML models, and that attacking iterative optimizers effectively alters the\noptimization objective surface in a manner that modifies the minima sought. We\nthen leverage the ability to cast iteration-limited optimizers as ML models to\nenhance robustness via adversarial training. For a class of proximal gradient\noptimizers, we rigorously prove how their learning affects adversarial\nsensitivity. We numerically back our findings, showing the vulnerability of\nvarious optimizers, as well as the robustness induced by unfolding and\nadversarial training.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u975e\u5b66\u4e60\u7684\u8fed\u4ee3\u4f18\u5316\u5668\u5bf9\u5bf9\u6297\u6837\u672c\u7684\u654f\u611f\u6027\uff0c\u53d1\u73b0\u5176\u4e0e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7c7b\u4f3c\u3002\u901a\u8fc7\u5c06\u8fed\u4ee3\u4f18\u5316\u5668\u89c6\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u6570\u5b66\u4e0a\u8bc1\u660e\u4e86\u8fd9\u4e00\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u672c\u6587\u52a8\u673a\u6e90\u4e8e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5bf9\u5bf9\u6297\u6837\u672c\u7684\u654f\u611f\u6027\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u8ba8\u975e\u5b66\u4e60\u7684\u8fed\u4ee3\u4f18\u5316\u5668\u662f\u5426\u4e5f\u5b58\u5728\u7c7b\u4f3c\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u8005\u5c06\u8fed\u4ee3\u4f18\u5316\u5668\uff08\u7279\u522b\u662f\u8fd1\u7aef\u68af\u5ea6\u4f18\u5316\u5668\uff09\u89c6\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u5bf9\u6297\u5206\u6790\uff0c\u63d0\u51fa\u901a\u8fc7\u5c55\u5f00\uff08unfolding\uff09\u548c\u5bf9\u6297\u8bad\u7ec3\u6765\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u591a\u79cd\u4f18\u5316\u5668\u5bf9\u5bf9\u6297\u6837\u672c\u540c\u6837\u654f\u611f\uff0c\u800c\u901a\u8fc7\u5c55\u5f00\u548c\u5bf9\u6297\u8bad\u7ec3\u663e\u8457\u63d0\u9ad8\u4e86\u5176\u9c81\u68d2\u6027\u3002\u6570\u5b66\u5206\u6790\u4e5f\u8bc1\u660e\u4e86\u5b66\u4e60\u8fc7\u7a0b\u5bf9\u5bf9\u6297\u654f\u611f\u6027\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u6297\u654f\u611f\u6027\u5e76\u975e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u72ec\u6709\uff0c\u8fed\u4ee3\u4f18\u5316\u5668\u4e5f\u5b58\u5728\u8fd9\u4e00\u95ee\u9898\u3002\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u4f18\u5316\u5668\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.19472", "pdf": "https://arxiv.org/pdf/2504.19472", "abs": "https://arxiv.org/abs/2504.19472", "authors": ["Siyi Liu", "Dan Roth"], "title": "Conflicts in Texts: Data, Implications and Challenges", "categories": ["cs.CL"], "comment": null, "summary": "As NLP models become increasingly integrated into real-world applications, it\nbecomes clear that there is a need to address the fact that models often rely\non and generate conflicting information. Conflicts could reflect the complexity\nof situations, changes that need to be explained and dealt with, difficulties\nin data annotation, and mistakes in generated outputs. In all cases,\ndisregarding the conflicts in data could result in undesired behaviors of\nmodels and undermine NLP models' reliability and trustworthiness. This survey\ncategorizes these conflicts into three key areas: (1) natural texts on the web,\nwhere factual inconsistencies, subjective biases, and multiple perspectives\nintroduce contradictions; (2) human-annotated data, where annotator\ndisagreements, mistakes, and societal biases impact model training; and (3)\nmodel interactions, where hallucinations and knowledge conflicts emerge during\ndeployment. While prior work has addressed some of these conflicts in\nisolation, we unify them under the broader concept of conflicting information,\nanalyze their implications, and discuss mitigation strategies. We highlight key\nchallenges and future directions for developing conflict-aware NLP systems that\ncan reason over and reconcile conflicting information more effectively.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86NLP\u6a21\u578b\u4e2d\u51b2\u7a81\u4fe1\u606f\u7684\u6765\u6e90\u4e0e\u5f71\u54cd\uff0c\u5c06\u5176\u5206\u4e3a\u4e09\u7c7b\uff08\u81ea\u7136\u6587\u672c\u3001\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u3001\u6a21\u578b\u4ea4\u4e92\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u7f13\u89e3\u7b56\u7565\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "NLP\u6a21\u578b\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u53d7\u5230\u51b2\u7a81\u4fe1\u606f\u7684\u5a01\u80c1\uff0c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u5206\u7c7b\u5e76\u89e3\u51b3\u8fd9\u4e9b\u51b2\u7a81\u3002", "method": "\u901a\u8fc7\u8c03\u67e5\u548c\u5206\u7c7b\uff0c\u5206\u6790\u51b2\u7a81\u4fe1\u606f\u7684\u4e09\u79cd\u4e3b\u8981\u6765\u6e90\u53ca\u5176\u5f71\u54cd\u3002", "result": "\u5f52\u7eb3\u4e86\u51b2\u7a81\u7684\u7c7b\u522b\uff08\u5982\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u3001\u6807\u6ce8\u5206\u6b67\u3001\u5e7b\u89c9\u7b49\uff09\uff0c\u5e76\u8ba8\u8bba\u4e86\u73b0\u6709\u7f13\u89e3\u7b56\u7565\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u6784\u5efa\u51b2\u7a81\u611f\u77e5\u7684NLP\u7cfb\u7edf\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u5904\u7406\u51b2\u7a81\u4fe1\u606f\u3002"}}
{"id": "2504.18565", "pdf": "https://arxiv.org/pdf/2504.18565", "abs": "https://arxiv.org/abs/2504.18565", "authors": ["Sid Black", "Asa Cooper Stickland", "Jake Pencharz", "Oliver Sourbut", "Michael Schmatz", "Jay Bailey", "Ollie Matthews", "Ben Millwood", "Alex Remedios", "Alan Cooney"], "title": "RepliBench: Evaluating the autonomous replication capabilities of language model agents", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Uncontrollable autonomous replication of language model agents poses a\ncritical safety risk. To better understand this risk, we introduce RepliBench,\na suite of evaluations designed to measure autonomous replication capabilities.\nRepliBench is derived from a decomposition of these capabilities covering four\ncore domains: obtaining resources, exfiltrating model weights, replicating onto\ncompute, and persisting on this compute for long periods. We create 20 novel\ntask families consisting of 86 individual tasks. We benchmark 5 frontier\nmodels, and find they do not currently pose a credible threat of\nself-replication, but succeed on many components and are improving rapidly.\nModels can deploy instances from cloud compute providers, write\nself-propagating programs, and exfiltrate model weights under simple security\nsetups, but struggle to pass KYC checks or set up robust and persistent agent\ndeployments. Overall the best model we evaluated (Claude 3.7 Sonnet) has a >50%\npass@10 score on 15/20 task families, and a >50% pass@10 score for 9/20\nfamilies on the hardest variants. These findings suggest autonomous replication\ncapability could soon emerge with improvements in these remaining areas or with\nhuman assistance.", "AI": {"tldr": "RepliBench\u8bc4\u4f30\u5957\u4ef6\u7528\u4e8e\u8861\u91cf\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u81ea\u4e3b\u590d\u5236\u80fd\u529b\uff0c\u53d1\u73b0\u76ee\u524d\u524d\u6cbf\u6a21\u578b\u5c1a\u4e0d\u5177\u5907\u81ea\u4e3b\u590d\u5236\u7684\u53ef\u4fe1\u5a01\u80c1\uff0c\u4f46\u5728\u591a\u4e2a\u7ec4\u4ef6\u4e0a\u8868\u73b0\u826f\u597d\u4e14\u8fdb\u6b65\u8fc5\u901f\u3002", "motivation": "\u7814\u7a76\u4e0d\u53ef\u63a7\u7684\u8bed\u8a00\u6a21\u578b\u81ea\u4e3b\u590d\u5236\u884c\u4e3a\u5bf9\u5b89\u5168\u6784\u6210\u7684\u98ce\u9669\u3002", "method": "\u5f00\u53d1RepliBench\u8bc4\u4f30\u5957\u4ef6\uff0c\u5206\u89e3\u4e3a\u56db\u4e2a\u6838\u5fc3\u9886\u57df\uff08\u8d44\u6e90\u83b7\u53d6\u3001\u6a21\u578b\u6743\u91cd\u5916\u6cc4\u3001\u8ba1\u7b97\u8d44\u6e90\u590d\u5236\u3001\u957f\u671f\u6301\u4e45\u5316\uff09\uff0c\u5305\u542b20\u4e2a\u4efb\u52a1\u5bb6\u65cf\u768486\u9879\u4efb\u52a1\uff0c\u6d4b\u8bd55\u79cd\u524d\u6cbf\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u7b80\u5355\u5b89\u5168\u8bbe\u7f6e\u4e0b\u53ef\u6267\u884c\u4e91\u5b9e\u4f8b\u90e8\u7f72\u3001\u81ea\u4f20\u64ad\u7a0b\u5e8f\u7f16\u5199\u548c\u6a21\u578b\u6743\u91cd\u5916\u6cc4\uff0c\u4f46\u96be\u4ee5\u901a\u8fc7KYC\u68c0\u67e5\u6216\u5efa\u7acb\u7a33\u5b9a\u7684\u957f\u671f\u4ee3\u7406\u90e8\u7f72\u3002\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b\uff08Claude 3.7 Sonnet\uff09\u572815/20\u4efb\u52a1\u5bb6\u65cf\u4e2d\u901a\u8fc7\u7387\u8d8550%\u3002", "conclusion": "\u81ea\u4e3b\u590d\u5236\u80fd\u529b\u53ef\u80fd\u5728\u4e0d\u4e45\u7684\u5c06\u6765\u968f\u6a21\u578b\u6539\u8fdb\u6216\u4eba\u7c7b\u8f85\u52a9\u800c\u51fa\u73b0\u3002"}}
{"id": "2504.19002", "pdf": "https://arxiv.org/pdf/2504.19002", "abs": "https://arxiv.org/abs/2504.19002", "authors": ["Delun Lai", "Yeyubei Zhang", "Yunchong Liu", "Chaojie Li", "Huadong Mo"], "title": "Deep Learning-Based Multi-Modal Fusion for Robust Robot Perception and Navigation", "categories": ["cs.LG", "cs.CV", "cs.RO"], "comment": "6 pages, 4 figures", "summary": "This paper introduces a novel deep learning-based multimodal fusion\narchitecture aimed at enhancing the perception capabilities of autonomous\nnavigation robots in complex environments. By utilizing innovative feature\nextraction modules, adaptive fusion strategies, and time-series modeling\nmechanisms, the system effectively integrates RGB images and LiDAR data. The\nkey contributions of this work are as follows: a. the design of a lightweight\nfeature extraction network to enhance feature representation; b. the\ndevelopment of an adaptive weighted cross-modal fusion strategy to improve\nsystem robustness; and c. the incorporation of time-series information modeling\nto boost dynamic scene perception accuracy. Experimental results on the KITTI\ndataset demonstrate that the proposed approach increases navigation and\npositioning accuracy by 3.5% and 2.2%, respectively, while maintaining\nreal-time performance. This work provides a novel solution for autonomous robot\nnavigation in complex environments.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u591a\u6a21\u6001\u878d\u5408\u67b6\u6784\uff0c\u901a\u8fc7RGB\u56fe\u50cf\u548cLiDAR\u6570\u636e\u7684\u6709\u6548\u7ed3\u5408\uff0c\u63d0\u5347\u4e86\u81ea\u4e3b\u5bfc\u822a\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u611f\u77e5\u80fd\u529b\u3002\u521b\u65b0\u70b9\u5305\u62ec\u8f7b\u91cf\u7ea7\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u3001\u81ea\u9002\u5e94\u8de8\u6a21\u6001\u878d\u5408\u7b56\u7565\u53ca\u65f6\u5e8f\u4fe1\u606f\u5efa\u6a21\u3002\u5b9e\u9a8c\u7ed3\u679c\u5728KITTI\u6570\u636e\u96c6\u4e0a\u663e\u793a\u5bfc\u822a\u4e0e\u5b9a\u4f4d\u7cbe\u5ea6\u5206\u522b\u63d0\u5347\u4e863.5%\u548c2.2%\u3002", "motivation": "\u4e3a\u63d0\u9ad8\u81ea\u4e3b\u5bfc\u822a\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u611f\u77e5\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u548c\u52a8\u6001\u573a\u666f\u611f\u77e5\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u4ee5\u589e\u5f3a\u7279\u5f81\u8868\u5f81\uff0c\u5f00\u53d1\u81ea\u9002\u5e94\u52a0\u6743\u8de8\u6a21\u6001\u878d\u5408\u7b56\u7565\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u5e76\u5f15\u5165\u65f6\u5e8f\u4fe1\u606f\u5efa\u6a21\u4f18\u5316\u52a8\u6001\u573a\u666f\u611f\u77e5\u3002", "result": "\u5728KITTI\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u5bfc\u822a\u548c\u5b9a\u4f4d\u7cbe\u5ea6\u5206\u522b\u63d0\u53473.5%\u548c2.2%\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u521b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.19556", "pdf": "https://arxiv.org/pdf/2504.19556", "abs": "https://arxiv.org/abs/2504.19556", "authors": ["Kristen Sussman", "Daniel Carter"], "title": "Detecting Effects of AI-Mediated Communication on Language Complexity and Sentiment", "categories": ["cs.CL", "cs.HC", "J.4; K.4.0; I.2.7"], "comment": "5 pages, 3 figures, Companion Proceedings of the ACM Web Conference\n  2025", "summary": "Given the subtle human-like effects of large language models on linguistic\npatterns, this study examines shifts in language over time to detect the impact\nof AI-mediated communication (AI- MC) on social media. We compare a replicated\ndataset of 970,919 tweets from 2020 (pre-ChatGPT) with 20,000 tweets from the\nsame period in 2024, all of which mention Donald Trump during election periods.\nUsing a combination of Flesch-Kincaid readability and polarity scores, we\nanalyze changes in text complexity and sentiment. Our findings reveal a\nsignificant increase in mean sentiment polarity (0.12 vs. 0.04) and a shift\nfrom predominantly neutral content (54.8% in 2020 to 39.8% in 2024) to more\npositive expressions (28.6% to 45.9%). These findings suggest not only an\nincreasing presence of AI in social media communication but also its impact on\nlanguage and emotional expression patterns.", "AI": {"tldr": "\u957f\u7bc7\u603b\u7ed3\uff1a\u7814\u7a76\u901a\u8fc7\u6bd4\u8f832020\u5e74\uff08ChatGPT\u524d\uff09\u548c2024\u5e74\u5173\u4e8e\u7279\u6717\u666e\u7684\u63a8\u6587\uff0c\u5206\u6790\u4e86AI\u5bf9\u793e\u4ea4\u5a92\u4f53\u8bed\u8a00\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u663e\u793a\u60c5\u611f\u6781\u6027\u548c\u79ef\u6781\u8868\u8fbe\u663e\u8457\u589e\u52a0\uff0cAI\u5bf9\u8bed\u8a00\u6a21\u5f0f\u7684\u5f71\u54cd\u660e\u663e\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982ChatGPT\uff09\u5bf9\u793e\u4ea4\u5a92\u4f53\u8bed\u8a00\u6a21\u5f0f\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u60c5\u611f\u8868\u8fbe\u548c\u6587\u672c\u590d\u6742\u5ea6\u7684\u53d8\u5316\u3002", "method": "\u4f7f\u7528Flesch-Kincaid\u53ef\u8bfb\u6027\u8bc4\u5206\u548c\u60c5\u611f\u6781\u6027\u8bc4\u5206\uff0c\u5206\u67902020\u5e74\u548c2024\u5e74\u63a8\u6587\u6570\u636e\u96c6\u7684\u53d8\u5316\u3002", "result": "2024\u5e74\u63a8\u6587\u7684\u60c5\u611f\u6781\u6027\u663e\u8457\u589e\u52a0\uff080.12 vs. 0.04\uff09\uff0c\u4e2d\u6027\u5185\u5bb9\u51cf\u5c11\uff0854.8%\u523039.8%\uff09\uff0c\u79ef\u6781\u8868\u8fbe\u589e\u52a0\uff0828.6%\u523045.9%\uff09\u3002", "conclusion": "AI\u5728\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u5b58\u5728\u589e\u52a0\uff0c\u663e\u8457\u5f71\u54cd\u8bed\u8a00\u6a21\u5f0f\u548c\u60c5\u611f\u8868\u8fbe\u3002"}}
{"id": "2504.18566", "pdf": "https://arxiv.org/pdf/2504.18566", "abs": "https://arxiv.org/abs/2504.18566", "authors": ["Harsh Patel"], "title": "Feature Selection via GANs (GANFS): Enhancing Machine Learning Models for DDoS Mitigation", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Distributed Denial of Service (DDoS) attacks represent a persistent and\nevolving threat to modern networked systems, capable of causing large-scale\nservice disruptions. The complexity of such attacks, often hidden within\nhigh-dimensional and redundant network traffic data, necessitates robust and\nintelligent feature selection techniques for effective detection. Traditional\nmethods such as filter-based, wrapper-based, and embedded approaches, each\noffer strengths but struggle with scalability or adaptability in complex attack\nenvironments. In this study, we explore these existing techniques through a\ndetailed comparative analysis and highlight their limitations when applied to\nlarge-scale DDoS detection tasks. Building upon these insights, we introduce a\nnovel Generative Adversarial Network-based Feature Selection (GANFS) method\nthat leverages adversarial learning dynamics to identify the most informative\nfeatures. By training a GAN exclusively on attack traffic and employing a\nperturbation-based sensitivity analysis on the Discriminator, GANFS effectively\nranks feature importance without relying on full supervision. Experimental\nevaluations using the CIC-DDoS2019 dataset demonstrate that GANFS not only\nimproves the accuracy of downstream classifiers but also enhances computational\nefficiency by significantly reducing feature dimensionality. These results\npoint to the potential of integrating generative learning models into\ncybersecurity pipelines to build more adaptive and scalable detection systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u7684\u65b0\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff08GANFS\uff09\uff0c\u7528\u4e8e\u589e\u5f3aDDoS\u653b\u51fb\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "DDoS\u653b\u51fb\u662f\u7f51\u7edc\u7cfb\u7edf\u7684\u6301\u7eed\u5a01\u80c1\uff0c\u4f20\u7edf\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u5728\u590d\u6742\u653b\u51fb\u73af\u5883\u4e2d\u5b58\u5728\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u667a\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3GAN\u4ec5\u9488\u5bf9\u653b\u51fb\u6d41\u91cf\uff0c\u5e76\u5229\u7528\u5224\u522b\u5668\u7684\u6270\u52a8\u654f\u611f\u6027\u5206\u6790\u6765\u65e0\u76d1\u7763\u5730\u6392\u540d\u7279\u5f81\u91cd\u8981\u6027\uff0c\u4ece\u800c\u63d0\u51faGANFS\u65b9\u6cd5\u3002", "result": "\u5728CIC-DDoS2019\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGANFS\u63d0\u9ad8\u4e86\u5206\u7c7b\u5668\u7684\u51c6\u786e\u6027\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u7279\u5f81\u7ef4\u5ea6\u3002", "conclusion": "\u5c06\u751f\u6210\u5b66\u4e60\u6a21\u578b\u96c6\u6210\u5230\u7f51\u7edc\u5b89\u5168\u7ba1\u9053\u4e2d\uff0c\u53ef\u4ee5\u6784\u5efa\u66f4\u5177\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684\u68c0\u6d4b\u7cfb\u7edf\u3002"}}
{"id": "2504.19013", "pdf": "https://arxiv.org/pdf/2504.19013", "abs": "https://arxiv.org/abs/2504.19013", "authors": ["J\u00falia Vicens Figueres", "Juliette Vanderhaeghen", "Federica Bragone", "Kateryna Morozovska", "Khemraj Shukla"], "title": "\\$PINN -- a Domain Decomposition Method for Bayesian Physics-Informed Neural Networks", "categories": ["cs.LG", "cs.AI", "math.AP"], "comment": "37 pages, 22 figures", "summary": "Physics-Informed Neural Networks (PINNs) are a novel computational approach\nfor solving partial differential equations (PDEs) with noisy and sparse initial\nand boundary data. Although, efficient quantification of epistemic and\naleatoric uncertainties in big multi-scale problems remains challenging. We\npropose \\$PINN a novel method of computing global uncertainty in PDEs using a\nBayesian framework, by combining local Bayesian Physics-Informed Neural\nNetworks (BPINN) with domain decomposition. The solution continuity across\nsubdomains is obtained by imposing the flux continuity across the interface of\nneighboring subdomains. To demonstrate the effectiveness of \\$PINN, we conduct\na series of computational experiments on PDEs in 1D and 2D spatial domains.\nAlthough we have adopted conservative PINNs (cPINNs), the method can be\nseamlessly extended to other domain decomposition techniques. The results infer\nthat the proposed method recovers the global uncertainty by computing the local\nuncertainty exactly more efficiently as the uncertainty in each subdomain can\nbe computed concurrently. The robustness of \\$PINN is verified by adding\nuncorrelated random noise to the training data up to 15% and testing for\ndifferent domain sizes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a$PINN\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u8d1d\u53f6\u65af\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u548c\u57df\u5206\u89e3\uff0c\u9ad8\u6548\u8ba1\u7b97PDE\u4e2d\u7684\u5168\u5c40\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5728\u5927\u89c4\u6a21\u591a\u5c3a\u5ea6\u95ee\u9898\u4e2d\u6709\u6548\u91cf\u5316PDE\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u7ed3\u5408BPINN\u548c\u57df\u5206\u89e3\uff0c\u5229\u7528\u901a\u91cf\u8fde\u7eed\u6027\u5b9e\u73b0\u5b50\u57df\u89e3\u7684\u8fde\u7eed\u6027\uff0c\u5e76\u57281D\u548c2D\u7a7a\u95f4\u57df\u4e2d\u8fdb\u884c\u8ba1\u7b97\u5b9e\u9a8c\u3002", "result": "\u65b9\u6cd5\u80fd\u9ad8\u6548\u6062\u590d\u5168\u5c40\u4e0d\u786e\u5b9a\u6027\uff0c\u4e14\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u6dfb\u52a015%\u968f\u673a\u566a\u58f0\u540e\u4ecd\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "$PINN\u80fd\u6709\u6548\u8ba1\u7b97PDE\u5168\u5c40\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u9002\u7528\u4e8e\u4e0d\u540c\u57df\u5206\u89e3\u6280\u672f\u3002"}}
{"id": "2504.19565", "pdf": "https://arxiv.org/pdf/2504.19565", "abs": "https://arxiv.org/abs/2504.19565", "authors": ["Meng Xiao", "Xunxin Cai", "Chengrui Wang", "Yuanchun Zhou"], "title": "m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training", "categories": ["cs.CL", "cs.AI", "q-bio.QM"], "comment": "22 pages, Large Language Model, Agentic AI, Dataset Distillation,\n  Multi-agent Collaboration", "summary": "The rapid progress of large language models (LLMs) in biomedical research has\nunderscored the limitations of existing open-source annotated scientific\ncorpora, which are often insufficient in quantity and quality. Addressing the\nchallenge posed by the complex hierarchy of biomedical knowledge, we propose a\nknowledge-driven, multi-agent framework for scientific corpus distillation\ntailored for LLM training in the biomedical domain. Central to our approach is\na collaborative multi-agent architecture, where specialized agents, each guided\nby the Medical Subject Headings (MeSH) hierarchy, work in concert to\nautonomously extract, synthesize, and self-evaluate high-quality textual data\nfrom vast scientific literature. These agents collectively generate and refine\ndomain-specific question-answer pairs, ensuring comprehensive coverage and\nconsistency with biomedical ontologies while minimizing manual involvement.\nExtensive experimental results show that language models trained on our\nmulti-agent distilled datasets achieve notable improvements in biomedical\nquestion-answering tasks, outperforming both strong life sciences LLM baselines\nand advanced proprietary models. Notably, our AI-Ready dataset enables\nLlama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger\nscale. Detailed ablation studies and case analyses further validate the\neffectiveness and synergy of each agent within the framework, highlighting the\npotential of multi-agent collaboration in biomedical LLM training.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u7684\u591aAgent\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u7269\u533b\u5b66\u9886\u57df\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bad\u7ec3\u6570\u636e\u84b8\u998f\uff0c\u901a\u8fc7\u591aAgent\u534f\u4f5c\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u95ee\u7b54\u5bf9\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u751f\u7269\u533b\u5b66\u6807\u6ce8\u6570\u636e\u96c6\u6570\u91cf\u548c\u8d28\u91cf\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6ee1\u8db3LLM\u8bad\u7ec3\u9700\u6c42\uff0c\u4e14\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u5c42\u6b21\u590d\u6742\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6570\u636e\u5904\u7406\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8eMeSH\u5c42\u6b21\u7ed3\u6784\u7684\u534f\u4f5c\u591aAgent\u67b6\u6784\uff0c\u5404Agent\u81ea\u4e3b\u63d0\u53d6\u3001\u5408\u6210\u548c\u8bc4\u4f30\u9ad8\u8d28\u91cf\u6587\u672c\u6570\u636e\uff0c\u751f\u6210\u9886\u57df\u7279\u5b9a\u7684\u95ee\u7b54\u5bf9\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u7528\u8be5\u65b9\u6cd5\u84b8\u998f\u7684\u6570\u636e\u8bad\u7ec3\u7684LLM\u5728\u751f\u7269\u533b\u5b66\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8d8aGPT-4\u7b49\u5927\u578b\u4e13\u6709\u6a21\u578b\u3002", "conclusion": "\u591aAgent\u534f\u4f5c\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u751f\u7269\u533b\u5b66LLM\u8bad\u7ec3\u6570\u636e\u7684\u8d28\u91cf\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.18569", "pdf": "https://arxiv.org/pdf/2504.18569", "abs": "https://arxiv.org/abs/2504.18569", "authors": ["Guanchen Wu", "Linzhi Zheng", "Han Xie", "Zhen Xiang", "Jiaying Lu", "Darren Liu", "Delgersuren Bold", "Bo Li", "Xiao Hu", "Carl Yang"], "title": "Large Language Model Empowered Privacy-Protected Framework for PHI Annotation in Clinical Notes", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "Shorter version published in MedInfo 2025", "summary": "The de-identification of private information in medical data is a crucial\nprocess to mitigate the risk of confidentiality breaches, particularly when\npatient personal details are not adequately removed before the release of\nmedical records. Although rule-based and learning-based methods have been\nproposed, they often struggle with limited generalizability and require\nsubstantial amounts of annotated data for effective performance. Recent\nadvancements in large language models (LLMs) have shown significant promise in\naddressing these issues due to their superior language comprehension\ncapabilities. However, LLMs present challenges, including potential privacy\nrisks when using commercial LLM APIs and high computational costs for deploying\nopen-source LLMs locally. In this work, we introduce LPPA, an LLM-empowered\nPrivacy-Protected PHI Annotation framework for clinical notes, targeting the\nEnglish language. By fine-tuning LLMs locally with synthetic notes, LPPA\nensures strong privacy protection and high PHI annotation accuracy. Extensive\nexperiments demonstrate LPPA's effectiveness in accurately de-identifying\nprivate information, offering a scalable and efficient solution for enhancing\npatient privacy protection.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86LPPA\u6846\u67b6\uff0c\u901a\u8fc7\u672c\u5730\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u9ad8\u6548\u4e14\u9690\u79c1\u4fdd\u62a4\u5730\u6807\u6ce8\u533b\u7597\u7b14\u8bb0\u4e2d\u7684\u654f\u611f\u4fe1\u606f\uff08PHI\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u548c\u9690\u79c1\u98ce\u9669\u7684\u95ee\u9898\u3002", "motivation": "\u533b\u7597\u6570\u636e\u53bb\u6807\u8bc6\u5316\u5bf9\u4fdd\u62a4\u60a3\u8005\u9690\u79c1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u89c4\u5219\u6216\u5b66\u4e60\u57fa\u65b9\u6cd5\uff09\u6cdb\u5316\u80fd\u529b\u6709\u9650\u4e14\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u3002\u5546\u4e1aLLM API\u5b58\u5728\u9690\u79c1\u98ce\u9669\uff0c\u800c\u672c\u5730\u90e8\u7f72LLM\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u4e14\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51faLPPA\u6846\u67b6\uff0c\u901a\u8fc7\u672c\u5730\u5fae\u8c03LLM\u5e76\u4f7f\u7528\u5408\u6210\u533b\u7597\u7b14\u8bb0\u8fdb\u884c\u8bad\u7ec3\uff0c\u786e\u4fdd\u9690\u79c1\u4fdd\u62a4\u548c\u9ad8\u6548\u7684PHI\u6807\u6ce8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLPPA\u5728\u53bb\u6807\u8bc6\u5316\u654f\u611f\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u9ad8\u6548\u4e14\u51c6\u786e\uff0c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\u3002", "conclusion": "LPPA\u4e3a\u533b\u7597\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9690\u79c1\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u6027\u80fd\u4e0e\u9690\u79c1\u9700\u6c42\u3002"}}
{"id": "2504.19014", "pdf": "https://arxiv.org/pdf/2504.19014", "abs": "https://arxiv.org/abs/2504.19014", "authors": ["Sushant Vijayan"], "title": "Towards minimax optimal algorithms for Active Simple Hypothesis Testing", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We study the Active Simple Hypothesis Testing (ASHT) problem, a simpler\nvariant of the Fixed Budget Best Arm Identification problem. In this work, we\nprovide novel game theoretic formulation of the upper bounds of the ASHT\nproblem. This formulation allows us to leverage tools of differential games and\nPartial Differential Equations (PDEs) to propose an approximately optimal\nalgorithm that is computationally tractable compared to prior work. However,\nthe optimal algorithm still suffers from a curse of dimensionality and instead\nwe use a novel link to Blackwell Approachability to propose an algorithm that\nis far more efficient computationally. We show that this new algorithm,\nalthough not proven to be optimal, is always better than static algorithms in\nall instances of ASHT and is numerically observed to attain the optimal\nexponent in various instances.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e3b\u52a8\u7b80\u5355\u5047\u8bbe\u68c0\u9a8c\uff08ASHT\uff09\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u535a\u5f08\u8bba\u548c\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u7684\u8fd1\u4f3c\u6700\u4f18\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7Blackwell Approachability\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8ba1\u7b97\u65b9\u6cd5\u3002", "motivation": "ASHT\u95ee\u9898\u662f\u56fa\u5b9a\u9884\u7b97\u6700\u4f73\u81c2\u8bc6\u522b\u95ee\u9898\u7684\u7b80\u5316\u7248\u672c\uff0c\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u7ef4\u5ea6\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u535a\u5f08\u8bba\u548c\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u63d0\u51fa\u8fd1\u4f3c\u6700\u4f18\u7b97\u6cd5\uff0c\u5e76\u5229\u7528Blackwell Approachability\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u8ba1\u7b97\u65b9\u6cd5\u3002", "result": "\u65b0\u7b97\u6cd5\u5728\u6240\u6709ASHT\u5b9e\u4f8b\u4e2d\u5747\u4f18\u4e8e\u9759\u6001\u7b97\u6cd5\uff0c\u5e76\u5728\u591a\u4e2a\u5b9e\u4f8b\u4e2d\u8fbe\u5230\u4e86\u6700\u4f18\u6307\u6570\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c3d\u7ba1\u672a\u8bc1\u660e\u5176\u6700\u4f18\u6027\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2504.19590", "pdf": "https://arxiv.org/pdf/2504.19590", "abs": "https://arxiv.org/abs/2504.19590", "authors": ["Israa Alsiyat"], "title": "Arabic Metaphor Sentiment Classification Using Semantic Information", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, I discuss the testing of the Arabic Metaphor Corpus (AMC) [1]\nusing newly designed automatic tools for sentiment classification for AMC based\non semantic tags. The tool incorporates semantic emotional tags for sentiment\nclassification. I evaluate the tool using standard methods, which are F-score,\nrecall, and precision. The method is to show the impact of Arabic online\nmetaphors on sentiment through the newly designed tools. To the best of our\nknowledge, this is the first approach to conduct sentiment classification for\nArabic metaphors using semantic tags to find the impact of the metaphor.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5229\u7528\u8bed\u4e49\u6807\u7b7e\u5bf9\u963f\u62c9\u4f2f\u8bed\u9690\u55bb\u8bed\u6599\u5e93\uff08AMC\uff09\u8fdb\u884c\u60c5\u611f\u5206\u7c7b\u7684\u65b0\u5de5\u5177\uff0c\u901a\u8fc7F-score\u3001\u53ec\u56de\u7387\u548c\u51c6\u786e\u7387\u8bc4\u4f30\u5176\u6548\u679c\uff0c\u65e8\u5728\u5c55\u793a\u963f\u62c9\u4f2f\u8bed\u5728\u7ebf\u9690\u55bb\u5bf9\u60c5\u611f\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u586b\u8865\u963f\u62c9\u4f2f\u8bed\u9690\u55bb\u60c5\u611f\u5206\u7c7b\u7684\u7a7a\u767d\uff0c\u63a2\u7d22\u8bed\u4e49\u6807\u7b7e\u5728\u60c5\u611f\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u8bc4\u4f30\u5176\u5bf9\u9690\u55bb\u60c5\u611f\u5f71\u54cd\u7684\u91cf\u5316\u6548\u679c\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u60c5\u611f\u6807\u7b7e\u7684\u81ea\u52a8\u5de5\u5177\uff0c\u91c7\u7528F-score\u3001\u53ec\u56de\u7387\u548c\u51c6\u786e\u7387\u7b49\u6807\u51c6\u65b9\u6cd5\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8be5\u5de5\u5177\u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8e\u8bed\u4e49\u6807\u7b7e\u7684\u963f\u62c9\u4f2f\u8bed\u9690\u55bb\u60c5\u611f\u5206\u7c7b\uff0c\u91cf\u5316\u4e86\u9690\u55bb\u5bf9\u60c5\u611f\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u8bed\u4e49\u6807\u7b7e\u53ef\u4ee5\u6709\u6548\u7528\u4e8e\u963f\u62c9\u4f2f\u8bed\u9690\u55bb\u7684\u60c5\u611f\u5206\u7c7b\uff0c\u4e3a\u672a\u6765\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.19026", "pdf": "https://arxiv.org/pdf/2504.19026", "abs": "https://arxiv.org/abs/2504.19026", "authors": ["Stanislav Semenov"], "title": "Smooth Approximations of the Rounding Function", "categories": ["cs.LG", "math.OC", "03F60, 26E40", "F.4.1; F.1.1"], "comment": "9 pages, 1 figure, submitted to arXiv", "summary": "We propose novel smooth approximations to the classical rounding function,\nsuitable for differentiable optimization and machine learning applications. Our\nconstructions are based on two approaches: (1) localized sigmoid window\nfunctions centered at each integer, and (2) normalized weighted sums of sigmoid\nderivatives representing local densities. The first method approximates the\nstep-like behavior of rounding through differences of shifted sigmoids, while\nthe second method achieves smooth interpolation between integers via\ndensity-based weighting. Both methods converge pointwise to the classical\nrounding function as the sharpness parameter k tends to infinity, and allow\ncontrolled trade-offs between smoothness and approximation accuracy. We\ndemonstrate that by restricting the summation to a small set of nearest\nintegers, the computational cost remains low without sacrificing precision.\nThese constructions provide fully differentiable alternatives to hard rounding,\nwhich are valuable in contexts where gradient-based methods are essential.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u5e73\u6ed1\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u66ff\u4ee3\u7ecf\u5178\u7684\u56db\u820d\u4e94\u5165\u51fd\u6570\uff0c\u9002\u7528\u4e8e\u53ef\u5fae\u4f18\u5316\u548c\u673a\u5668\u5b66\u4e60\u3002\u8fd9\u4e9b\u65b9\u6cd5\u57fa\u4e8e\u5c40\u90e8\u5316Sigmoid\u7a97\u53e3\u51fd\u6570\u548c\u5f52\u4e00\u5316\u52a0\u6743Sigmoid\u5bfc\u6570\uff0c\u80fd\u591f\u5e73\u6ed1\u903c\u8fd1\u6574\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u5fae\u6027\u3002", "motivation": "\u7ecf\u5178\u7684\u56db\u820d\u4e94\u5165\u51fd\u6570\u4e0d\u53ef\u5fae\uff0c\u9650\u5236\u4e86\u5176\u5728\u4f9d\u8d56\u68af\u5ea6\u7684\u4f18\u5316\u548c\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u53ef\u5fae\u7684\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u4ee5\u652f\u6301\u8fd9\u4e9b\u5e94\u7528\u573a\u666f\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a1\uff09\u57fa\u4e8e\u5c40\u90e8\u5316Sigmoid\u7a97\u53e3\u51fd\u6570\u7684\u5dee\u5f02\u903c\u8fd1\uff1b2\uff09\u57fa\u4e8e\u5f52\u4e00\u5316\u52a0\u6743Sigmoid\u5bfc\u6570\u7684\u5bc6\u5ea6\u52a0\u6743\u63d2\u503c\u3002\u4e24\u79cd\u65b9\u6cd5\u5747\u901a\u8fc7\u8c03\u6574\u9510\u5ea6\u53c2\u6570\u63a7\u5236\u5e73\u6ed1\u6027\u4e0e\u903c\u8fd1\u7cbe\u5ea6\u3002", "result": "\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5728\u9510\u5ea6\u53c2\u6570\u8d8b\u8fd1\u65e0\u7a77\u5927\u65f6\u70b9\u6001\u6536\u655b\u4e8e\u7ecf\u5178\u56db\u820d\u4e94\u5165\u51fd\u6570\uff0c\u4e14\u901a\u8fc7\u9650\u5236\u90bb\u8fd1\u6574\u6570\u7684\u6c42\u548c\u8303\u56f4\uff0c\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u7cbe\u5ea6\u3002", "conclusion": "\u8fd9\u4e9b\u53ef\u5fae\u7684\u8fd1\u4f3c\u65b9\u6cd5\u4e3a\u4f9d\u8d56\u68af\u5ea6\u7684\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5f25\u8865\u4e86\u4f20\u7edf\u56db\u820d\u4e94\u5165\u51fd\u6570\u7684\u4e0d\u8db3\u3002"}}
{"id": "2504.19606", "pdf": "https://arxiv.org/pdf/2504.19606", "abs": "https://arxiv.org/abs/2504.19606", "authors": ["Hieu-Dai Tran", "Duc-Vu Nguyen", "Ngan Luu-Thuy Nguyen"], "title": "Coreference Resolution for Vietnamese Narrative Texts", "categories": ["cs.CL"], "comment": "Accepted at PACLIC 2024", "summary": "Coreference resolution is a vital task in natural language processing (NLP)\nthat involves identifying and linking different expressions in a text that\nrefer to the same entity. This task is particularly challenging for Vietnamese,\na low-resource language with limited annotated datasets. To address these\nchallenges, we developed a comprehensive annotated dataset using narrative\ntexts from VnExpress, a widely-read Vietnamese online news platform. We\nestablished detailed guidelines for annotating entities, focusing on ensuring\nconsistency and accuracy. Additionally, we evaluated the performance of large\nlanguage models (LLMs), specifically GPT-3.5-Turbo and GPT-4, on this dataset.\nOur results demonstrate that GPT-4 significantly outperforms GPT-3.5-Turbo in\nterms of both accuracy and response consistency, making it a more reliable tool\nfor coreference resolution in Vietnamese.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u8d8a\u5357\u8bed\u4e2d\u7684\u5171\u6307\u6d88\u89e3\u4efb\u52a1\uff0c\u901a\u8fc7\u6784\u5efaVnExpress\u65b0\u95fb\u5e73\u53f0\u4e0a\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86GPT-4\u548cGPT-3.5-Turbo\u7684\u8868\u73b0\uff0c\u53d1\u73b0GPT-4\u5728\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u4e0a\u66f4\u4f18\u3002", "motivation": "\u8d8a\u5357\u8bed\u4f5c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5171\u6307\u6d88\u89e3\u4efb\u52a1\u5b58\u5728\u6311\u6218\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u5e76\u8bc4\u4f30\u5927\u6a21\u578b\u7684\u9002\u7528\u6027\u3002", "method": "\u6784\u5efa\u4e86\u57fa\u4e8eVnExpress\u65b0\u95fb\u6587\u672c\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5236\u5b9a\u8be6\u7ec6\u6807\u6ce8\u51c6\u5219\uff0c\u5e76\u6d4b\u8bd5\u4e86GPT-4\u548cGPT-3.5-Turbo\u7684\u6027\u80fd\u3002", "result": "GPT-4\u5728\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u4e0a\u663e\u8457\u4f18\u4e8eGPT-3.5-Turbo\uff0c\u66f4\u9002\u5408\u8d8a\u5357\u8bed\u5171\u6307\u6d88\u89e3\u4efb\u52a1\u3002", "conclusion": "GPT-4\u662f\u5904\u7406\u8d8a\u5357\u8bed\u5171\u6307\u6d88\u89e3\u4efb\u52a1\u7684\u9ad8\u6548\u5de5\u5177\uff0c\u6807\u6ce8\u6570\u636e\u96c6\u7684\u6784\u5efa\u4e5f\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u8d44\u6e90\u3002"}}
{"id": "2504.18575", "pdf": "https://arxiv.org/pdf/2504.18575", "abs": "https://arxiv.org/abs/2504.18575", "authors": ["Ivan Evtimov", "Arman Zharmagambetov", "Aaron Grattafiori", "Chuan Guo", "Kamalika Chaudhuri"], "title": "WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Web navigation AI agents use language-and-vision foundation models to enhance\nproductivity but these models are known to be susceptible to indirect prompt\ninjections that get them to follow instructions different from the legitimate\nuser's. Existing explorations of this threat applied to web agents often focus\non a single isolated adversarial goal, test with injected instructions that are\neither too easy or not truly malicious, and often give the adversary\nunreasonable access. In order to better focus adversarial research, we\nconstruct a new benchmark called WASP (Web Agent Security against Prompt\ninjection attacks) that introduces realistic web agent hijacking objectives and\nan isolated environment to test them in that does not affect real users or the\nlive web. As part of WASP, we also develop baseline attacks against three\npopular web agentic systems (VisualWebArena, Claude Computer Use, and Operator)\ninstantiated with various state-of-the-art models. Our evaluation shows that\neven AI agents backed by models with advanced reasoning capabilities and by\nmodels with instruction hierarchy mitigations are susceptible to low-effort\nhuman-written prompt injections. However, the realistic objectives in WASP also\nallow us to observe that agents are currently not capable enough to complete\nthe goals of attackers end-to-end. Agents begin executing the adversarial\ninstruction between 16 and 86% of the time but only achieve the goal between 0\nand 17% of the time. Based on these findings, we argue that adversarial\nresearchers should demonstrate stronger attacks that more consistently maintain\ncontrol over the agent given realistic constraints on the adversary's power.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86WASP\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u7f51\u7edcAI\u4ee3\u7406\u5bf9\u6297\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u7684\u5b89\u5168\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u7cfb\u7edf\u6613\u53d7\u653b\u51fb\u4f46\u5b9e\u9645\u653b\u51fb\u6210\u529f\u7387\u4f4e\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u8bed\u8a00\u548c\u89c6\u89c9\u6a21\u578b\u7684\u7f51\u7edcAI\u4ee3\u7406\u6613\u53d7\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u5c40\u9650\u4e8e\u5355\u76ee\u6807\u3001\u4e0d\u771f\u5b9e\u7684\u653b\u51fb\u573a\u666f\u6216\u8fc7\u9ad8\u5047\u8bbe\u653b\u51fb\u8005\u6743\u9650\u3002\u4e3a\u4e86\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u653b\u51fb\u5f71\u54cd\uff0c\u4f5c\u8005\u6784\u5efa\u4e86WASP\u57fa\u51c6\u3002", "method": "\u901a\u8fc7WASP\u57fa\u51c6\u6d4b\u8bd5\u4e09\u79cd\u6d41\u884c\u7f51\u7edc\u4ee3\u7406\u7cfb\u7edf\uff08VisualWebArena\u3001Claude Computer Use\u548cOperator\uff09\u5728\u4e0d\u540c\u5148\u8fdb\u6a21\u578b\u4e0b\u7684\u8868\u73b0\uff0c\u8bc4\u4f30\u5176\u5bf9\u4eba\u7c7b\u7f16\u5199\u7684\u4f4e\u96be\u5ea6\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u7684\u62b5\u6297\u529b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5373\u4f7f\u5177\u6709\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\u6216\u6307\u4ee4\u5c42\u7ea7\u7f13\u89e3\u63aa\u65bd\u7684\u6a21\u578b\u4e5f\u6613\u53d7\u653b\u51fb\uff0c\u4f46\u653b\u51fb\u8005\u7aef\u5230\u7aef\u76ee\u6807\u5b8c\u6210\u7387\u4f4e\uff080-17%\uff09\u3002", "conclusion": "\u4f5c\u8005\u547c\u5401\u653b\u51fb\u7814\u7a76\u9700\u5728\u66f4\u771f\u5b9e\u7ea6\u675f\u4e0b\u5c55\u793a\u66f4\u5f3a\u63a7\u5236\u529b\uff0c\u540c\u65f6\u8868\u660e\u5f53\u524d\u4ee3\u7406\u7cfb\u7edf\u5c1a\u4e0d\u8db3\u4ee5\u5b8c\u5168\u5b9e\u73b0\u653b\u51fb\u8005\u76ee\u6807\u3002"}}
{"id": "2504.19034", "pdf": "https://arxiv.org/pdf/2504.19034", "abs": "https://arxiv.org/abs/2504.19034", "authors": ["Samantha Petti", "Carlos Mart\u00ed-G\u00f3mez", "Justin B. Kinney", "Juannan Zhou", "David M. McCandlish"], "title": "On learning functions over biological sequence space: relating Gaussian process priors, regularization, and gauge fixing", "categories": ["cs.LG", "q-bio.GN", "stat.ML"], "comment": null, "summary": "Mappings from biological sequences (DNA, RNA, protein) to quantitative\nmeasures of sequence functionality play an important role in contemporary\nbiology. We are interested in the related tasks of (i) inferring predictive\nsequence-to-function maps and (ii) decomposing sequence-function maps to\nelucidate the contributions of individual subsequences. Because each\nsequence-function map can be written as a weighted sum over subsequences in\nmultiple ways, meaningfully interpreting these weights requires \"gauge-fixing,\"\ni.e., defining a unique representation for each map. Recent work has\nestablished that most existing gauge-fixed representations arise as the unique\nsolutions to $L_2$-regularized regression in an overparameterized \"weight\nspace\" where the choice of regularizer defines the gauge. Here, we establish\nthe relationship between regularized regression in overparameterized weight\nspace and Gaussian process approaches that operate in \"function space,\" i.e.\nthe space of all real-valued functions on a finite set of sequences. We\ndisentangle how weight space regularizers both impose an implicit prior on the\nlearned function and restrict the optimal weights to a particular gauge. We\nalso show how to construct regularizers that correspond to arbitrary explicit\nGaussian process priors combined with a wide variety of gauges. Next, we derive\nthe distribution of gauge-fixed weights implied by the Gaussian process\nposterior and demonstrate that even for long sequences this distribution can be\nefficiently computed for product-kernel priors using a kernel trick. Finally,\nwe characterize the implicit function space priors associated with the most\ncommon weight space regularizers. Overall, our framework unifies and extends\nour ability to infer and interpret sequence-function relationships.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u751f\u7269\u5e8f\u5217\uff08DNA\u3001RNA\u3001\u86cb\u767d\u8d28\uff09\u4e0e\u529f\u80fd\u91cf\u5316\u6307\u6807\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u9884\u6d4b\u6027\u5e8f\u5217-\u529f\u80fd\u6620\u5c04\u7684\u63a8\u65ad\u53ca\u5176\u5b50\u5e8f\u5217\u8d21\u732e\u7684\u5206\u89e3\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u7684\u4e3b\u8981\u52a8\u673a\u662f\u901a\u8fc7\u5e8f\u5217-\u529f\u80fd\u6620\u5c04\u7684\u63a8\u65ad\u548c\u5206\u89e3\uff0c\u66f4\u597d\u5730\u7406\u89e3\u751f\u7269\u5e8f\u5217\u7684\u529f\u80fd\u6027\u8868\u73b0\u53ca\u5176\u5b50\u5e8f\u5217\u7684\u8d21\u732e\u3002", "method": "\u8bba\u6587\u91c7\u7528$L_2$\u6b63\u5219\u5316\u56de\u5f52\u548c\u9ad8\u65af\u8fc7\u7a0b\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u6743\u503c\u7a7a\u95f4\u548c\u51fd\u6570\u7a7a\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u6784\u5efa\u4e86\u5bf9\u5e94\u4efb\u610f\u9ad8\u65af\u8fc7\u7a0b\u5148\u9a8c\u7684\u89c4\u5219\u5316\u5668\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u5bf9\u4e8e\u957f\u5e8f\u5217\uff0c\u5229\u7528\u6838\u6280\u5de7\u4e5f\u80fd\u9ad8\u6548\u8ba1\u7b97\u9ad8\u65af\u8fc7\u7a0b\u540e\u9a8c\u6240\u9690\u542b\u7684\u6743\u503c\u5206\u5e03\uff0c\u5e76\u4e14\u6846\u67b6\u80fd\u591f\u7edf\u4e00\u548c\u6269\u5c55\u5e8f\u5217-\u529f\u80fd\u5173\u7cfb\u7684\u63a8\u65ad\u548c\u89e3\u91ca\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u6574\u5408\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u8fd8\u8fdb\u4e00\u6b65\u6269\u5c55\u4e86\u5e8f\u5217-\u529f\u80fd\u5173\u7cfb\u7684\u7814\u7a76\u80fd\u529b\uff0c\u4e3a\u751f\u7269\u5e8f\u5217\u7684\u529f\u80fd\u6027\u89e3\u8bfb\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u5de5\u5177\u3002"}}
{"id": "2504.19627", "pdf": "https://arxiv.org/pdf/2504.19627", "abs": "https://arxiv.org/abs/2504.19627", "authors": ["Run Luo", "Renke Shan", "Longze Chen", "Ziqiang Liu", "Lu Wang", "Min Yang", "Xiaobo Xia"], "title": "VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "VCM", "summary": "Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like\nembodied intelligence due to their strong vision-language reasoning abilities.\nHowever, current LVLMs process entire images at the token level, which is\ninefficient compared to humans who analyze information and generate content at\nthe conceptual level, extracting relevant visual concepts with minimal effort.\nThis inefficiency, stemming from the lack of a visual concept model, limits\nLVLMs' usability in real-world applications. To address this, we propose VCM,\nan end-to-end self-supervised visual concept modeling framework. VCM leverages\nimplicit contrastive learning across multiple sampled instances and\nvision-language fine-tuning to construct a visual concept model without\nrequiring costly concept-level annotations. Our results show that VCM\nsignificantly reduces computational costs (e.g., 85\\% fewer FLOPs for\nLLaVA-1.5-7B) while maintaining strong performance across diverse image\nunderstanding tasks. Moreover, VCM enhances visual encoders' capabilities in\nclassic visual concept perception tasks. Extensive quantitative and qualitative\nexperiments validate the effectiveness and efficiency of VCM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86VCM\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u89c6\u89c9\u6982\u5ff5\u5efa\u6a21\u63d0\u5347\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7387\uff0c\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u4e14\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf\u5904\u7406\u4e0a\u7684\u6548\u7387\u4f4e\u4e0b\uff0c\u7f3a\u4e4f\u89c6\u89c9\u6982\u5ff5\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u6548\u679c\u3002", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u548c\u89c6\u89c9\u8bed\u8a00\u5fae\u8c03\u6784\u5efaVCM\u6846\u67b6\uff0c\u907f\u514d\u6602\u8d35\u7684\u6982\u5ff5\u7ea7\u6807\u6ce8\u3002", "result": "VCM\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff08\u5982\u51cf\u5c1185% FLOPs\uff09\uff0c\u5e76\u5728\u591a\u4e2a\u56fe\u50cf\u7406\u89e3\u4efb\u52a1\u4e2d\u4fdd\u6301\u5f3a\u6027\u80fd\u3002", "conclusion": "VCM\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.19036", "pdf": "https://arxiv.org/pdf/2504.19036", "abs": "https://arxiv.org/abs/2504.19036", "authors": ["Henry Herzog", "Joshua Hansen", "Yawen Zhang", "Patrick Beukema"], "title": "Atlantes: A system of GPS transformers for global-scale real-time maritime intelligence", "categories": ["cs.LG"], "comment": "8 pages, 10 figures, ICLR CCAI 2025, spotlight talk", "summary": "Unsustainable exploitation of the oceans exacerbated by global warming is\nthreatening coastal communities worldwide. Accurate and timely monitoring of\nmaritime activity is an essential step to effective governance and to inform\nfuture policy. In support of this complex global-scale effort, we built\nAtlantes, a deep learning based system that provides the first-ever real-time\nview of vessel behavior at global scale. Atlantes leverages a series of bespoke\ntransformers to distill a high volume, continuous stream of GPS messages\nemitted by hundreds of thousands of vessels into easily quantifiable behaviors.\nThe combination of low latency and high performance enables operationally\nrelevant decision-making and successful interventions on the high seas where\nillegal and exploitative activity is too common. Atlantes is already in use by\nhundreds of organizations worldwide. Here we provide an overview of the model\nand infrastructure that enables this system to function efficiently and\ncost-effectively at global-scale and in real-time.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aAtlantes\u7684\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff0c\u9996\u6b21\u5b9e\u73b0\u5168\u7403\u8303\u56f4\u5185\u8239\u8236\u884c\u4e3a\u7684\u5b9e\u65f6\u76d1\u63a7\uff0c\u5e2e\u52a9\u6cbb\u7406\u548c\u672a\u6765\u653f\u7b56\u5236\u5b9a\u3002", "motivation": "\u5168\u7403\u53d8\u6696\u548c\u4e0d\u5408\u7406\u7684\u6d77\u6d0b\u5f00\u53d1\u6b63\u5a01\u80c1\u6cbf\u6d77\u793e\u533a\uff0c\u9700\u8981\u51c6\u786e\u3001\u53ca\u65f6\u7684\u822a\u8fd0\u76d1\u63a7\u6765\u652f\u6301\u6709\u6548\u6cbb\u7406\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\uff08\u7279\u522b\u662f\u5b9a\u5236\u5316Transformer\u6a21\u578b\uff09\u5904\u7406\u5927\u91cf\u8239\u8236GPS\u6570\u636e\uff0c\u5b9e\u65f6\u91cf\u5316\u8239\u8236\u884c\u4e3a\u3002", "result": "Atlantes\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u6027\u80fd\u7684\u5b9e\u65f6\u76d1\u63a7\uff0c\u5df2\u88ab\u5168\u7403\u6570\u767e\u4e2a\u7ec4\u7ec7\u4f7f\u7528\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u5168\u7403\u6d77\u6d0b\u6cbb\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7ecf\u6d4e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.19645", "pdf": "https://arxiv.org/pdf/2504.19645", "abs": "https://arxiv.org/abs/2504.19645", "authors": ["Shadan Shukr Sabr", "Nazira Sabr Mustafa", "Talar Sabah Omar", "Salah Hwayyiz Rasool", "Nawzad Anwer Omer", "Darya Sabir Hamad", "Hemin Abdulhameed Shams", "Omer Mahmood Kareem", "Rozhan Noori Abdullah", "Khabat Atar Abdullah", "Mahabad Azad Mohammad", "Haneen Al-Raghefy", "Safar M. Asaad", "Sara Jamal Mohammed", "Twana Saeed Ali", "Fazil Shawrow", "Halgurd S. Maghdid"], "title": "A Comprehensive Part-of-Speech Tagging to Standardize Central-Kurdish Language: A Research Guide for Kurdish Natural Language Processing Tasks", "categories": ["cs.CL", "cs.AI", "K.5; K.7; J.7"], "comment": "25 pages, 4 figures, 2 tables", "summary": "- The field of natural language processing (NLP) has dramatically expanded\nwithin the last decade. Many human-being applications are conducted daily via\nNLP tasks, starting from machine translation, speech recognition, text\ngeneration and recommendations, Part-of-Speech tagging (POS), and Named-Entity\nRecognition (NER). However, low-resourced languages, such as the\nCentral-Kurdish language (CKL), mainly remain unexamined due to shortage of\nnecessary resources to support their development. The POS tagging task is the\nbase of other NLP tasks; for example, the POS tag set has been used to\nstandardized languages to provide the relationship between words among the\nsentences, followed by machine translation and text recommendation.\nSpecifically, for the CKL, most of the utilized or provided POS tagsets are\nneither standardized nor comprehensive. To this end, this study presented an\naccurate and comprehensive POS tagset for the CKL to provide better performance\nof the Kurdish NLP tasks. The article also collected most of the POS tags from\ndifferent studies as well as from Kurdish linguistic experts to standardized\npart-of-speech tags. The proposed POS tagset is designed to annotate a large\nCKL corpus and support Kurdish NLP tasks. The initial investigations of this\nstudy via comparison with the Universal Dependencies framework for standard\nlanguages, show that the proposed POS tagset can streamline or correct\nsentences more accurately for Kurdish NLP tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4e3a\u4e2d\u5e93\u5c14\u5fb7\u8bed\uff08CKL\uff09\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u51c6\u786e\u4e14\u5168\u9762\u7684\u8bcd\u6027\u6807\u6ce8\u96c6\uff08POS tagset\uff09\uff0c\u4ee5\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4efb\u52a1\u4e2d\u8d44\u6e90\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u4e2d\u5e93\u5c14\u5fb7\u8bed\u4f5c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8bcd\u6027\u6807\u6ce8\u96c6\uff0c\u5f71\u54cd\u4e86\u5176NLP\u4efb\u52a1\uff08\u5982\u673a\u5668\u7ffb\u8bd1\u3001\u6587\u672c\u63a8\u8350\uff09\u7684\u53d1\u5c55\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u8005\u6574\u5408\u4e86\u4e0d\u540c\u7814\u7a76\u53ca\u5e93\u5c14\u5fb7\u8bed\u8a00\u5b66\u4e13\u5bb6\u7684\u8bcd\u6027\u6807\u6ce8\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u7684POS\u6807\u6ce8\u96c6\uff0c\u5e76\u7528\u4e8e\u6807\u6ce8\u5927\u578bCKL\u8bed\u6599\u5e93\u3002", "result": "\u901a\u8fc7\u4e0e\u901a\u7528\u4f9d\u5b58\u6846\u67b6\uff08Universal Dependencies\uff09\u7684\u5bf9\u6bd4\uff0c\u521d\u6b65\u9a8c\u8bc1\u4e86\u8be5\u6807\u6ce8\u96c6\u80fd\u66f4\u51c6\u786e\u5730\u4f18\u5316\u5e93\u5c14\u5fb7\u8bedNLP\u4efb\u52a1\u7684\u53e5\u5b50\u5904\u7406\u3002", "conclusion": "\u63d0\u51fa\u7684\u6807\u51c6\u5316POS\u6807\u6ce8\u96c6\u4e3a\u4e2d\u5e93\u5c14\u5fb7\u8bed\u7684NLP\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u7840\uff0c\u672a\u6765\u53ef\u652f\u6301\u66f4\u591a\u8bed\u8a00\u5904\u7406\u5e94\u7528\u3002"}}
{"id": "2504.19040", "pdf": "https://arxiv.org/pdf/2504.19040", "abs": "https://arxiv.org/abs/2504.19040", "authors": ["Nandan Joshi", "Erhan Guven"], "title": "Improved Molecular Generation through Attribute-Driven Integrative Embeddings and GAN Selectivity", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The growing demand for molecules with tailored properties in fields such as\ndrug discovery and chemical engineering has driven advancements in\ncomputational methods for molecular design. Machine learning-based approaches\nfor de-novo molecular generation have recently garnered significant attention.\nThis paper introduces a transformer-based vector embedding generator combined\nwith a modified Generative Adversarial Network (GAN) to generate molecules with\ndesired properties. The embedding generator utilizes a novel molecular\ndescriptor, integrating Morgan fingerprints with global molecular attributes,\nenabling the transformer to capture local functional groups and broader\nmolecular characteristics. Modifying the GAN generator loss function ensures\nthe generation of molecules with specific desired properties. The transformer\nachieves a reconversion accuracy of 94% while translating molecular descriptors\nback to SMILES strings, validating the utility of the proposed embeddings for\ngenerative tasks. The approach is validated by generating novel odorant\nmolecules using a labeled dataset of odorant and non-odorant compounds. With\nthe modified range-loss function, the GAN exclusively generates odorant\nmolecules. This work underscores the potential of combining novel vector\nembeddings with transformers and modified GAN architectures to accelerate the\ndiscovery of tailored molecules, offering a robust tool for diverse molecular\ndesign applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Transformer\u548c\u4fee\u6539\u7248GAN\u7684\u5206\u5b50\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bbe\u8ba1\u5177\u6709\u7279\u5b9a\u6027\u8d28\u7684\u5206\u5b50\u3002\u901a\u8fc7\u65b0\u578b\u5206\u5b50\u63cf\u8ff0\u7b26\u548c\u6539\u8fdb\u7684GAN\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u5b9a\u5411\u751f\u6210\u3002", "motivation": "\u968f\u7740\u836f\u7269\u53d1\u73b0\u548c\u5316\u5b66\u5de5\u7a0b\u7b49\u9886\u57df\u5bf9\u5b9a\u5236\u5206\u5b50\u7684\u9700\u6c42\u589e\u957f\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5206\u5b50\u8bbe\u8ba1\u8ba1\u7b97\u65b9\u6cd5\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b0\u5b9a\u5411\u5206\u5b50\u751f\u6210\u3002", "method": "\u7ed3\u5408Transformer\u751f\u6210\u7684\u5411\u91cf\u5d4c\u5165\uff08\u6574\u5408Morgan\u6307\u7eb9\u548c\u5168\u5c40\u5206\u5b50\u5c5e\u6027\uff09\u548c\u6539\u8fdb\u7684GAN\uff08\u4fee\u6539\u635f\u5931\u51fd\u6570\uff09\uff0c\u751f\u6210\u5177\u6709\u76ee\u6807\u6027\u8d28\u7684\u5206\u5b50\u3002", "result": "Transformer\u7684SMILES\u5b57\u7b26\u4e32\u91cd\u6784\u51c6\u786e\u7387\u8fbe\u523094%\uff0c\u6539\u8fdb\u7684GAN\u6210\u529f\u751f\u6210\u4e86\u5168\u90e8\u5177\u6709\u76ee\u6807\u6c14\u5473\u6027\u8d28\u7684\u5206\u5b50\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u65b0\u578b\u5d4c\u5165\u4e0e\u6539\u8fdbGAN\u7ed3\u5408\u5728\u5206\u5b50\u8bbe\u8ba1\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u5b9a\u5411\u5206\u5b50\u53d1\u73b0\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\u3002"}}
{"id": "2504.19669", "pdf": "https://arxiv.org/pdf/2504.19669", "abs": "https://arxiv.org/abs/2504.19669", "authors": ["Chen Su", "Yuanhe Tian", "Yan Song"], "title": "Multimodal Conditioned Diffusive Time Series Forecasting", "categories": ["cs.CL"], "comment": null, "summary": "Diffusion models achieve remarkable success in processing images and text,\nand have been extended to special domains such as time series forecasting\n(TSF). Existing diffusion-based approaches for TSF primarily focus on modeling\nsingle-modality numerical sequences, overlooking the rich multimodal\ninformation in time series data. To effectively leverage such information for\nprediction, we propose a multimodal conditioned diffusion model for TSF,\nnamely, MCD-TSF, to jointly utilize timestamps and texts as extra guidance for\ntime series modeling, especially for forecasting. Specifically, Timestamps are\ncombined with time series to establish temporal and semantic correlations among\ndifferent data points when aggregating information along the temporal\ndimension. Texts serve as supplementary descriptions of time series' history,\nand adaptively aligned with data points as well as dynamically controlled in a\nclassifier-free manner. Extensive experiments on real-world benchmark datasets\nacross eight domains demonstrate that the proposed MCD-TSF model achieves\nstate-of-the-art performance.", "AI": {"tldr": "\u63d0\u51faMCD-TSF\u6a21\u578b\uff0c\u8054\u5408\u65f6\u95f4\u6233\u548c\u6587\u672c\u4f5c\u4e3a\u989d\u5916\u6307\u5bfc\uff0c\u63d0\u5347\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u4e3b\u8981\u5173\u6ce8\u5355\u6a21\u6001\u6570\u503c\u5e8f\u5217\uff0c\u5ffd\u7565\u4e86\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u4fe1\u606f\u3002", "method": "\u7ed3\u5408\u65f6\u95f4\u6233\u5efa\u7acb\u65f6\u95f4\u53ca\u8bed\u4e49\u5173\u8054\uff0c\u6587\u672c\u4f5c\u4e3a\u8865\u5145\u63cf\u8ff0\u5e76\u52a8\u6001\u5bf9\u9f50\u6570\u636e\u70b9\u3002", "result": "\u5728\u516b\u4e2a\u9886\u57df\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MCD-TSF\u901a\u8fc7\u591a\u6a21\u6001\u6761\u4ef6\u5b9e\u73b0\u66f4\u4f18\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3002"}}
{"id": "2504.19084", "pdf": "https://arxiv.org/pdf/2504.19084", "abs": "https://arxiv.org/abs/2504.19084", "authors": ["Elliot L. Epstein", "Rajat Dwaraknath", "Thanawat Sornwanee", "John Winnicki", "Jerry Weihong Liu"], "title": "Score-Debiased Kernel Density Estimation", "categories": ["cs.LG", "stat.ML"], "comment": "ICLR 2025 Workshop on Frontiers of Probabilistic Inference", "summary": "We propose a novel method for density estimation that leverages an estimated\nscore function to debias kernel density estimation (SD-KDE). In our approach,\neach data point is adjusted by taking a single step along the score function\nwith a specific choice of step size, followed by standard KDE with a modified\nbandwidth. The step size and modified bandwidth are chosen to remove the\nleading order bias in the KDE. Our experiments on synthetic tasks in 1D, 2D and\non MNIST, demonstrate that our proposed SD-KDE method significantly reduces the\nmean integrated squared error compared to the standard Silverman KDE, even with\nnoisy estimates in the score function. These results underscore the potential\nof integrating score-based corrections into nonparametric density estimation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f30\u8ba1\u5206\u6570\u51fd\u6570\u6765\u4fee\u6b63\u6838\u5bc6\u5ea6\u4f30\u8ba1\u504f\u5dee\u7684\u65b0\u65b9\u6cd5\uff08SD-KDE\uff09\u3002\u901a\u8fc7\u8c03\u6574\u6570\u636e\u70b9\u548c\u4fee\u6539\u5e26\u5bbd\uff0c\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u663e\u8457\u964d\u4f4e\u4e86\u5e73\u5747\u79ef\u5206\u5e73\u65b9\u8bef\u5dee\u3002", "motivation": "\u4f20\u7edf\u7684\u6838\u5bc6\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5b58\u5728\u504f\u5dee\uff0c\u5f71\u54cd\u4e86\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u5206\u6570\u51fd\u6570\u6765\u4fee\u6b63\u8fd9\u4e00\u504f\u5dee\uff0c\u4ee5\u63d0\u9ad8\u5bc6\u5ea6\u4f30\u8ba1\u7684\u6548\u679c\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5229\u7528\u4f30\u8ba1\u7684\u5206\u6570\u51fd\u6570\u5bf9\u6bcf\u4e2a\u6570\u636e\u70b9\u8fdb\u884c\u5355\u6b65\u8c03\u6574\uff0c\u5e76\u9009\u62e9\u4e00\u4e2a\u7279\u5b9a\u7684\u6b65\u957f\uff0c\u968f\u540e\u4f7f\u7528\u6807\u51c6KDE\u548c\u4fee\u6539\u540e\u7684\u5e26\u5bbd\u8fdb\u884c\u5bc6\u5ea6\u4f30\u8ba1\u3002\u8fd9\u4e00\u8c03\u6574\u65e8\u5728\u6d88\u9664KDE\u4e2d\u7684\u4e3b\u5bfc\u504f\u5dee\u3002", "result": "\u57281D\u30012D\u5408\u6210\u4efb\u52a1\u548cMNIST\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5206\u6570\u51fd\u6570\u4f30\u8ba1\u5b58\u5728\u566a\u58f0\uff0cSD-KDE\u65b9\u6cd5\u4e5f\u80fd\u663e\u8457\u964d\u4f4e\u5e73\u5747\u79ef\u5206\u5e73\u65b9\u8bef\u5dee\uff0c\u8868\u73b0\u4f18\u4e8e\u6807\u51c6\u7684Silverman KDE\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684SD-KDE\u65b9\u6cd5\u5c55\u793a\u4e86\u5c06\u57fa\u4e8e\u5206\u6570\u7684\u4fee\u6b63\u5f15\u5165\u975e\u53c2\u6570\u5bc6\u5ea6\u4f30\u8ba1\u7684\u6f5c\u529b\uff0c\u4e3a\u63d0\u9ad8\u5bc6\u5ea6\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2504.19675", "pdf": "https://arxiv.org/pdf/2504.19675", "abs": "https://arxiv.org/abs/2504.19675", "authors": ["Osma Suominen", "Juho Inkinen", "Mona Lehtinen"], "title": "Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs", "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.IR", "cs.LG", "I.2.7"], "comment": "6 pages, 4 figures, submitted to SemEval-2025 workshop Task 5:\n  LLMs4Subjects", "summary": "This paper presents the Annif system in SemEval-2025 Task 5 (LLMs4Subjects),\nwhich focussed on subject indexing using large language models (LLMs). The task\nrequired creating subject predictions for bibliographic records from the\nbilingual TIBKAT database using the GND subject vocabulary. Our approach\ncombines traditional natural language processing and machine learning\ntechniques implemented in the Annif toolkit with innovative LLM-based methods\nfor translation and synthetic data generation, and merging predictions from\nmonolingual models. The system ranked first in the all-subjects category and\nsecond in the tib-core-subjects category in the quantitative evaluation, and\nfourth in qualitative evaluations. These findings demonstrate the potential of\ncombining traditional XMTC algorithms with modern LLM techniques to improve the\naccuracy and efficiency of subject indexing in multilingual contexts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86Annif\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5728SemEval-2025 Task 5\uff08LLMs4Subjects\uff09\u4e2d\u7ed3\u5408\u4f20\u7edfNLP\u548cLLM\u6280\u672f\uff0c\u53d6\u5f97\u4e86\u4f18\u5f02\u6210\u7ee9\u3002", "motivation": "\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u7ed3\u5408\u4f20\u7edf\u6280\u672f\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u63d0\u9ad8\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u4e3b\u9898\u6807\u5f15\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u7ed3\u5408Annif\u5de5\u5177\u4e2d\u7684\u4f20\u7edfNLP\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u521b\u65b0\u6027\u5730\u4f7f\u7528LLM\u8fdb\u884c\u7ffb\u8bd1\u3001\u5408\u6210\u6570\u636e\u751f\u6210\u53ca\u5355\u8bed\u6a21\u578b\u9884\u6d4b\u7684\u5408\u5e76\u3002", "result": "\u7cfb\u7edf\u5728\u6240\u6709\u4e3b\u9898\u7c7b\u522b\u4e2d\u6392\u540d\u7b2c\u4e00\uff0c\u5728tib-core-subjects\u7c7b\u522b\u4e2d\u6392\u540d\u7b2c\u4e8c\uff0c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u6392\u540d\u7b2c\u56db\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u4f20\u7edfXMTC\u7b97\u6cd5\u4e0e\u73b0\u4ee3LLM\u6280\u672f\u7684\u7ed3\u5408\u80fd\u6709\u6548\u63d0\u5347\u591a\u8bed\u8a00\u4e3b\u9898\u6807\u5f15\u7684\u6027\u80fd\u3002"}}
{"id": "2504.19103", "pdf": "https://arxiv.org/pdf/2504.19103", "abs": "https://arxiv.org/abs/2504.19103", "authors": ["Shunxin Guo", "Jiaqi Lv", "Xin Geng"], "title": "Harmonizing Generalization and Personalization in Ring-topology Decentralized Federated Learning", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "We introduce Ring-topology Decentralized Federated Learning (RDFL) for\ndistributed model training, aiming to avoid the inherent risks of centralized\nfailure in server-based FL. However, RDFL faces the challenge of low\ninformation-sharing efficiency due to the point-to-point communication manner\nwhen handling inherent data heterogeneity. Existing studies to mitigate data\nheterogeneity focus on personalized optimization of models, ignoring that the\nlack of shared information constraints can lead to large differences among\nmodels, weakening the benefits of collaborative learning. To tackle these\nchallenges, we propose a Divide-and-conquer RDFL framework (DRDFL) that uses a\nfeature generation model to extract personalized information and invariant\nshared knowledge from the underlying data distribution, ensuring both effective\npersonalization and strong generalization. Specifically, we design a\n\\textit{PersonaNet} module that encourages class-specific feature\nrepresentations to follow a Gaussian mixture distribution, facilitating the\nlearning of discriminative latent representations tailored to local data\ndistributions. Meanwhile, the \\textit{Learngene} module is introduced to\nencapsulate shared knowledge through an adversarial classifier to align latent\nrepresentations and extract globally invariant information. Extensive\nexperiments demonstrate that DRDFL outperforms state-of-the-art methods in\nvarious data heterogeneity settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Divide-and-conquer RDFL\u6846\u67b6\uff08DRDFL\uff09\uff0c\u901a\u8fc7\u7279\u5f81\u751f\u6210\u6a21\u578b\u4ece\u5e95\u5c42\u6570\u636e\u5206\u5e03\u4e2d\u63d0\u53d6\u4e2a\u6027\u5316\u4fe1\u606f\u548c\u5171\u4eab\u77e5\u8bc6\uff0c\u89e3\u51b3\u4e86\u73af\u5f62\u62d3\u6251\u5206\u6563\u8054\u90a6\u5b66\u4e60\uff08RDFL\uff09\u4e2d\u6570\u636e\u5f02\u6784\u6027\u5bfc\u81f4\u7684\u4fe1\u606f\u5171\u4eab\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfRDFL\u7531\u4e8e\u70b9\u5bf9\u70b9\u901a\u4fe1\u65b9\u5f0f\u5728\u5904\u7406\u6570\u636e\u5f02\u6784\u6027\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u73b0\u6709\u7814\u7a76\u5ffd\u7565\u5171\u4eab\u4fe1\u606f\u7f3a\u4e4f\u7ea6\u675f\u4f1a\u5bfc\u81f4\u6a21\u578b\u5dee\u5f02\u8fc7\u5927\uff0c\u524a\u5f31\u534f\u4f5c\u5b66\u4e60\u6548\u679c\u3002", "method": "\u8bbe\u8ba1\u4e86PersonaNet\u6a21\u5757\uff08\u9f13\u52b1\u7c7b\u7279\u5b9a\u7279\u5f81\u8868\u793a\u9075\u5faa\u9ad8\u65af\u6df7\u5408\u5206\u5e03\uff09\u548cLearngene\u6a21\u5757\uff08\u901a\u8fc7\u5bf9\u6297\u5206\u7c7b\u5668\u5bf9\u9f50\u6f5c\u5728\u8868\u793a\u5e76\u63d0\u53d6\u5168\u5c40\u4e0d\u53d8\u4fe1\u606f\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDRDFL\u5728\u591a\u79cd\u6570\u636e\u5f02\u6784\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DRDFL\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u4e2a\u6027\u5316\u4e0e\u5171\u4eab\u77e5\u8bc6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5206\u6563\u8054\u90a6\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2504.19720", "pdf": "https://arxiv.org/pdf/2504.19720", "abs": "https://arxiv.org/abs/2504.19720", "authors": ["Ranran Zhen", "Juntao Li", "Yixin Ji", "Zhenlin Yang", "Tong Liu", "Qingrong Xia", "Xinyu Duan", "Zhefeng Wang", "Baoxing Huai", "Min Zhang"], "title": "Taming the Titans: A Survey of Efficient LLM Inference Serving", "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG"], "comment": "work in progress;11 pages of main paper with 7 main figures, overall\n  20 pages", "summary": "Large Language Models (LLMs) for Generative AI have achieved remarkable\nprogress, evolving into sophisticated and versatile tools widely adopted across\nvarious domains and applications. However, the substantial memory overhead\ncaused by their vast number of parameters, combined with the high computational\ndemands of the attention mechanism, poses significant challenges in achieving\nlow latency and high throughput for LLM inference services. Recent\nadvancements, driven by groundbreaking research, have significantly accelerated\nprogress in this field. This paper provides a comprehensive survey of these\nmethods, covering fundamental instance-level approaches, in-depth cluster-level\nstrategies, emerging scenario directions, and other miscellaneous but important\nareas. At the instance level, we review model placement, request scheduling,\ndecoding length prediction, storage management, and the disaggregation\nparadigm. At the cluster level, we explore GPU cluster deployment,\nmulti-instance load balancing, and cloud service solutions. For emerging\nscenarios, we organize the discussion around specific tasks, modules, and\nauxiliary methods. To ensure a holistic overview, we also highlight several\nniche yet critical areas. Finally, we outline potential research directions to\nfurther advance the field of LLM inference serving.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u670d\u52a1\u4e2d\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u6db5\u76d6\u5b9e\u4f8b\u7ea7\u3001\u96c6\u7fa4\u7ea7\u3001\u65b0\u5174\u573a\u666f\u53ca\u5176\u4ed6\u5173\u952e\u9886\u57df\uff0c\u65e8\u5728\u89e3\u51b3\u9ad8\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\u5bfc\u81f4\u7684\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u95ee\u9898\u3002", "motivation": "LLM\u5728\u751f\u6210\u5f0fAI\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u4f46\u56e0\u53c2\u6570\u5e9e\u5927\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u9ad8\u8ba1\u7b97\u9700\u6c42\uff0c\u5bfc\u81f4\u63a8\u7406\u670d\u52a1\u9762\u4e34\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u541e\u5410\u91cf\u7684\u6311\u6218\uff0c\u9700\u7cfb\u7edf\u68b3\u7406\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u7efc\u8ff0\uff0c\u5305\u62ec\u5b9e\u4f8b\u7ea7\u7684\u6a21\u578b\u653e\u7f6e\u3001\u8bf7\u6c42\u8c03\u5ea6\u7b49\uff0c\u96c6\u7fa4\u7ea7\u7684GPU\u96c6\u7fa4\u90e8\u7f72\u3001\u8d1f\u8f7d\u5747\u8861\u7b49\uff0c\u4ee5\u53ca\u65b0\u5174\u573a\u666f\u548c\u8f85\u52a9\u65b9\u6cd5\u7684\u8ba8\u8bba\u3002", "result": "\u5168\u9762\u603b\u7ed3\u4e86LLM\u63a8\u7406\u670d\u52a1\u7684\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8bba\u6587\u4e3aLLM\u63a8\u7406\u670d\u52a1\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u68b3\u7406\uff0c\u6307\u660e\u4e86\u8fdb\u4e00\u6b65\u4f18\u5316\u7684\u6f5c\u529b\u9886\u57df\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2504.19139", "pdf": "https://arxiv.org/pdf/2504.19139", "abs": "https://arxiv.org/abs/2504.19139", "authors": ["Yun Qu", "Qi", "Wang", "Yixiu Mao", "Yiqin Lv", "Xiangyang Ji"], "title": "Fast and Robust: Task Sampling with Posterior and Diversity Synergies for Adaptive Decision-Makers in Randomized Environments", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Task robust adaptation is a long-standing pursuit in sequential\ndecision-making. Some risk-averse strategies, e.g., the conditional\nvalue-at-risk principle, are incorporated in domain randomization or meta\nreinforcement learning to prioritize difficult tasks in optimization, which\ndemand costly intensive evaluations. The efficiency issue prompts the\ndevelopment of robust active task sampling to train adaptive policies, where\nrisk-predictive models are used to surrogate policy evaluation. This work\ncharacterizes the optimization pipeline of robust active task sampling as a\nMarkov decision process, posits theoretical and practical insights, and\nconstitutes robustness concepts in risk-averse scenarios. Importantly, we\npropose an easy-to-implement method, referred to as Posterior and Diversity\nSynergized Task Sampling (PDTS), to accommodate fast and robust sequential\ndecision-making. Extensive experiments show that PDTS unlocks the potential of\nrobust active task sampling, significantly improves the zero-shot and few-shot\nadaptation robustness in challenging tasks, and even accelerates the learning\nprocess under certain scenarios. Our project website is at\nhttps://thu-rllab.github.io/PDTS_project_page.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPDTS\u7684\u7b80\u5355\u6613\u5b9e\u73b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5feb\u901f\u4e14\u7a33\u5065\u7684\u5e8f\u5217\u51b3\u7b56\uff0c\u901a\u8fc7\u540e\u9a8c\u591a\u6837\u6027\u7684\u534f\u540c\u4efb\u52a1\u91c7\u6837\u4f18\u5316\u9c81\u68d2\u4e3b\u52a8\u4efb\u52a1\u91c7\u6837\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u9002\u5e94\u80fd\u529b\uff0c\u5e76\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u52a0\u901f\u4e86\u5b66\u4e60\u8fc7\u7a0b\u3002", "motivation": "\u957f\u671f\u4ee5\u6765\uff0c\u987a\u5e8f\u51b3\u7b56\u4e2d\u7684\u4efb\u52a1\u9c81\u68d2\u9002\u914d\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002\u73b0\u6709\u7684\u98ce\u9669\u89c4\u907f\u7b56\u7565\u5728\u4f18\u5316\u4e2d\u9700\u8981\u6602\u8d35\u7684\u5bc6\u96c6\u8bc4\u4f30\uff0c\u6548\u7387\u95ee\u9898\u4fc3\u4f7f\u4e86\u9c81\u68d2\u4e3b\u52a8\u4efb\u52a1\u91c7\u6837\u65b9\u6cd5\u7684\u53d1\u5c55\u3002", "method": "\u672c\u6587\u5c06\u9c81\u68d2\u4e3b\u52a8\u4efb\u52a1\u91c7\u6837\u7684\u4f18\u5316\u6d41\u7a0b\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u63d0\u51fa\u4e86PDTS\u65b9\u6cd5\uff0c\u7ed3\u5408\u540e\u9a8c\u548c\u591a\u6837\u6027\u4ee5\u534f\u540c\u91c7\u6837\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPDTS\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u9002\u5e94\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u7279\u5b9a\u573a\u666f\u4e2d\u52a0\u901f\u4e86\u5b66\u4e60\u8fc7\u7a0b\u3002", "conclusion": "PDTS\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u4efb\u52a1\u91c7\u6837\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u5e8f\u5217\u51b3\u7b56\uff0c\u4e3a\u98ce\u9669\u89c4\u907f\u573a\u666f\u4e0b\u7684\u4efb\u52a1\u9002\u5e94\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.19734", "pdf": "https://arxiv.org/pdf/2504.19734", "abs": "https://arxiv.org/abs/2504.19734", "authors": ["Ying Na", "Shihui Feng"], "title": "LLM-Assisted Automated Deductive Coding of Dialogue Data: Leveraging Dialogue-Specific Characteristics to Enhance Contextual Understanding", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Dialogue data has been a key source for understanding learning processes,\noffering critical insights into how students engage in collaborative\ndiscussions and how these interactions shape their knowledge construction. The\nadvent of Large Language Models (LLMs) has introduced promising opportunities\nfor advancing qualitative research, particularly in the automated coding of\ndialogue data. However, the inherent contextual complexity of dialogue presents\nunique challenges for these models, especially in understanding and\ninterpreting complex contextual information. This study addresses these\nchallenges by developing a novel LLM-assisted automated coding approach for\ndialogue data. The novelty of our proposed framework is threefold: 1) We\npredict the code for an utterance based on dialogue-specific characteristics --\ncommunicative acts and communicative events -- using separate prompts following\nthe role prompts and chain-of-thoughts methods; 2) We engaged multiple LLMs\nincluding GPT-4-turbo, GPT-4o, DeepSeek in collaborative code prediction; 3) We\nleveraged the interrelation between events and acts to implement consistency\nchecking using GPT-4o. In particular, our contextual consistency checking\nprovided a substantial accuracy improvement. We also found the accuracy of act\npredictions was consistently higher than that of event predictions. This study\ncontributes a new methodological framework for enhancing the precision of\nautomated coding of dialogue data as well as offers a scalable solution for\naddressing the contextual challenges inherent in dialogue analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u81ea\u52a8\u5bf9\u8bdd\u6570\u636e\u7f16\u7801\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u578b\u534f\u4f5c\u548c\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u68c0\u67e5\u63d0\u5347\u7f16\u7801\u51c6\u786e\u6027\u3002", "motivation": "\u5bf9\u8bdd\u6570\u636e\u662f\u7406\u89e3\u5b66\u4e60\u8fc7\u7a0b\u7684\u5173\u952e\uff0c\u4f46\u590d\u6742\u7684\u4e0a\u4e0b\u6587\u5bf9\u81ea\u52a8\u7f16\u7801\u63d0\u51fa\u4e86\u6311\u6218\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9ad8\u7cbe\u5ea6\u7684LLM\u8f85\u52a9\u81ea\u52a8\u7f16\u7801\u6846\u67b6\u3002", "method": "1. \u4f7f\u7528\u89d2\u8272\u63d0\u793a\u548c\u601d\u7ef4\u94fe\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5bf9\u8bdd\u7279\u6027\uff08\u4ea4\u6d41\u884c\u4e3a\u548c\u4e8b\u4ef6\uff09\u9884\u6d4b\u7f16\u7801\uff1b2. \u591aLLM\u534f\u4f5c\u9884\u6d4b\uff08GPT-4-turbo\u3001GPT-4o\u3001DeepSeek\uff09\uff1b3. \u5229\u7528\u4e8b\u4ef6\u4e0e\u884c\u4e3a\u7684\u5173\u7cfb\u8fdb\u884c\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u68c0\u67e5\uff08GPT-4o\uff09\u3002", "result": "\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u68c0\u67e5\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\uff0c\u4ea4\u6d41\u884c\u4e3a\u9884\u6d4b\u7684\u51c6\u786e\u6027\u9ad8\u4e8e\u4e8b\u4ef6\u9884\u6d4b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5bf9\u8bdd\u6570\u636e\u7684\u81ea\u52a8\u7f16\u7801\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4e0a\u4e0b\u6587\u590d\u6742\u6027\u6311\u6218\u3002"}}
{"id": "2504.19141", "pdf": "https://arxiv.org/pdf/2504.19141", "abs": "https://arxiv.org/abs/2504.19141", "authors": ["Panagiotis Kakosimos"], "title": "Reliable Thermal Monitoring of Electric Machines through Machine Learning", "categories": ["cs.LG"], "comment": "2023 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "The electrification of powertrains is rising as the objective for a more\nviable future is intensified. To ensure continuous and reliable operation\nwithout undesirable malfunctions, it is essential to monitor the internal\ntemperatures of machines and keep them within safe operating limits.\nConventional modeling methods can be complex and usually require expert\nknowledge. With the amount of data collected these days, it is possible to use\ninformation models to assess thermal behaviors. This paper investigates\nartificial intelligence techniques for monitoring the cooling efficiency of\ninduction machines. Experimental data was collected under specific operating\nconditions, and three machine-learning models have been developed. The optimal\nconfiguration for each approach was determined through rigorous hyperparameter\nsearches, and the models were evaluated using a variety of metrics. The three\nsolutions performed well in monitoring the condition of the machine even under\ntransient operation, highlighting the potential of data-driven methods in\nimproving the thermal management.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4eba\u5de5\u667a\u80fd\u6280\u672f\u5728\u611f\u5e94\u7535\u673a\u51b7\u5374\u6548\u7387\u76d1\u63a7\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u5b9e\u9a8c\u6570\u636e\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9a8c\u8bc1\u4e86\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u6539\u5584\u70ed\u7ba1\u7406\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u968f\u7740\u7535\u52a8\u5316\u52a8\u529b\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u786e\u4fdd\u7535\u673a\u5185\u90e8\u6e29\u5ea6\u5728\u5b89\u5168\u8303\u56f4\u5185\u8fd0\u884c\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u5efa\u6a21\u65b9\u6cd5\u590d\u6742\u4e14\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\uff0c\u800c\u73b0\u4ee3\u6570\u636e\u91c7\u96c6\u4e3a\u4fe1\u606f\u6a21\u578b\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u5728\u7279\u5b9a\u8fd0\u884c\u6761\u4ef6\u4e0b\u91c7\u96c6\u5b9e\u9a8c\u6570\u636e\uff0c\u5f00\u53d1\u4e86\u4e09\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u8d85\u53c2\u6570\u641c\u7d22\u786e\u5b9a\u6700\u4f18\u914d\u7f6e\uff0c\u4f7f\u7528\u591a\u79cd\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u4e09\u79cd\u6a21\u578b\u5728\u77ac\u6001\u8fd0\u884c\u4e0b\u5747\u80fd\u6709\u6548\u76d1\u63a7\u7535\u673a\u72b6\u6001\uff0c\u5c55\u793a\u4e86\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u70ed\u7ba1\u7406\u4e2d\u7684\u4f18\u52bf\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u611f\u5e94\u7535\u673a\u7684\u70ed\u7ba1\u7406\u6548\u7387\uff0c\u4e3a\u672a\u6765\u7535\u52a8\u5316\u52a8\u529b\u7cfb\u7edf\u7684\u6e29\u5ea6\u76d1\u63a7\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.19759", "pdf": "https://arxiv.org/pdf/2504.19759", "abs": "https://arxiv.org/abs/2504.19759", "authors": ["Huichi Zhou", "Zehao Xu", "Munan Zhao", "Kaihong Li", "Yiqiang Li", "Hongtao Wang"], "title": "Moral Reasoning Across Languages: The Critical Role of Low-Resource Languages in LLMs", "categories": ["cs.CL"], "comment": "5 pages, 2 figures", "summary": "In this paper, we introduce the Multilingual Moral Reasoning Benchmark (MMRB)\nto evaluate the moral reasoning abilities of large language models (LLMs)\nacross five typologically diverse languages and three levels of contextual\ncomplexity: sentence, paragraph, and document. Our results show moral reasoning\nperformance degrades with increasing context complexity, particularly for\nlow-resource languages such as Vietnamese. We further fine-tune the open-source\nLLaMA-3-8B model using curated monolingual data for alignment and poisoning.\nSurprisingly, low-resource languages have a stronger impact on multilingual\nreasoning than high-resource ones, highlighting their critical role in\nmultilingual NLP.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u591a\u8bed\u8a00\u9053\u5fb7\u63a8\u7406\u57fa\u51c6\uff08MMRB\uff09\uff0c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e94\u79cd\u8bed\u8a00\u548c\u4e09\u79cd\u4e0a\u4e0b\u6587\u590d\u6742\u5ea6\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4f4e\u8d44\u6e90\u8bed\u8a00\u8868\u73b0\u8f83\u5dee\uff0c\u5fae\u8c03LLaMA-3-8B\u540e\u53d1\u73b0\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u591a\u8bed\u8a00\u63a8\u7406\u5f71\u54cd\u66f4\u5927\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u8bc4\u4f30LLMs\u5728\u4e0d\u540c\u8bed\u8a00\u548c\u4e0a\u4e0b\u6587\u590d\u6742\u5ea6\u4e0b\u7684\u9053\u5fb7\u63a8\u7406\u80fd\u529b\uff0c\u586b\u8865\u591a\u8bed\u8a00\u9053\u5fb7\u63a8\u7406\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6784\u5efaMMRB\u57fa\u51c6\uff0c\u6db5\u76d6\u4e94\u79cd\u8bed\u8a00\u548c\u4e09\u79cd\u590d\u6742\u5ea6\uff08\u53e5\u5b50\u3001\u6bb5\u843d\u3001\u6587\u6863\uff09\uff0c\u5e76\u5bf9LLaMA-3-8B\u8fdb\u884c\u5355\u8bed\u6570\u636e\u5fae\u8c03\u3002", "result": "\u7ed3\u679c\u663e\u793a\u9053\u5fb7\u63a8\u7406\u6027\u80fd\u968f\u4e0a\u4e0b\u6587\u590d\u6742\u5ea6\u589e\u52a0\u800c\u4e0b\u964d\uff0c\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u8d8a\u5357\u8bed\uff09\u8868\u73b0\u66f4\u5dee\uff0c\u4f46\u5fae\u8c03\u540e\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u591a\u8bed\u8a00\u63a8\u7406\u5f71\u54cd\u66f4\u5927\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u591a\u8bed\u8a00NLP\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u9700\u66f4\u591a\u5173\u6ce8\u5176\u8868\u73b0\u548c\u4f18\u5316\u3002"}}
{"id": "2504.19176", "pdf": "https://arxiv.org/pdf/2504.19176", "abs": "https://arxiv.org/abs/2504.19176", "authors": ["Piotr Migus"], "title": "Newton-Puiseux Analysis for Interpretability and Calibration of Complex-Valued Neural Networks", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Complex-valued neural networks (CVNNs) excel where phase matters, yet their\nmulti-sheeted decision surfaces defy standard explainability and calibration\ntools. We propose a \\emph{Newton-Puiseux} framework that fits a local\npolynomial surrogate to a high-uncertainty input and analytically decomposes\nthis surrogate into fractional-power series. The resulting Puiseux expansions,\ndominant Puiseux coefficients, and phase-aligned curvature descriptors deliver\nclosed-form estimates of robustness and over-confidence that gradient - or\nperturbation-based methods (saliency, LIME, SHAP) cannot provide. On a\ncontrolled $\\mathbb{C}^2$ helix the surrogate attains RMSE $< 0.09$ while\nrecovering the number of decision sheets; quartic coefficients predict\nadversarial flip radii within $10^{-3}$. On the real-world MIT-BIH arrhythmia\ncorpus, Puiseux-guided, phase-aware temperature scaling lowers expected\ncalibration error from 0.087 to 0.034, contributing to the advancement of\nCVNNs. Full code, pre-trained weights, and scripts are at\nhttps://github.com/piotrmgs/puiseux-cvnn.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdNewton-Puiseux\u6846\u67b6\uff0c\u901a\u8fc7\u5c40\u90e8\u591a\u9879\u5f0f\u4ee3\u7406\u548c\u9ad8\u4e0d\u786e\u5b9a\u6027\u8f93\u5165\u7684\u5206\u6570\u5e42\u7ea7\u6570\u5206\u89e3\uff0c\u63d0\u5347\u4e86\u590d\u6570\u795e\u7ecf\u7f51\u7edc\uff08CVNNs\uff09\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6821\u51c6\u80fd\u529b\u3002", "motivation": "\u590d\u6570\u795e\u7ecf\u7f51\u7edc\u5728\u76f8\u4f4d\u5173\u952e\u573a\u666f\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u591a\u5c42\u7684\u51b3\u7b56\u66f2\u9762\u96be\u4ee5\u7528\u4f20\u7edf\u5de5\u5177\u89e3\u91ca\u548c\u6821\u51c6\u3002", "method": "\u91c7\u7528Newton-Puiseux\u6846\u67b6\u62df\u5408\u5c40\u90e8\u591a\u9879\u5f0f\u4ee3\u7406\uff0c\u5c06\u5176\u5206\u89e3\u4e3a\u5206\u6570\u5e42\u7ea7\u6570\uff0c\u751f\u6210Puiseux\u5c55\u5f00\u3001\u4e3b\u5bfc\u7cfb\u6570\u548c\u76f8\u4f4d\u5bf9\u9f50\u66f2\u7387\u63cf\u8ff0\u7b26\u3002", "result": "\u5728\u590d\u6742\u87ba\u65cb\u5b9e\u9a8c\u4e2d\uff0c\u4ee3\u7406\u6a21\u578b\u7684RMSE\u5c0f\u4e8e0.09\uff0c\u56db\u6b21\u7cfb\u6570\u9884\u6d4b\u5bf9\u6297\u653b\u51fb\u7ffb\u8f6c\u534a\u5f84\u8bef\u5dee\u572810^-3\u5185\uff1b\u5728MIT-BIH\u5fc3\u5f8b\u5931\u5e38\u6570\u636e\u4e0a\uff0c\u6821\u51c6\u8bef\u5dee\u4ece0.087\u964d\u81f30.034\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86CVNNs\u7684\u9c81\u68d2\u6027\u548c\u6821\u51c6\u80fd\u529b\uff0c\u63a8\u52a8\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2504.19811", "pdf": "https://arxiv.org/pdf/2504.19811", "abs": "https://arxiv.org/abs/2504.19811", "authors": ["Takuya Tamura", "Taro Yano", "Masafumi Enomoto", "Masafumi Oyamada"], "title": "Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language Model Performance", "categories": ["cs.CL"], "comment": null, "summary": "Accurately forecasting the performance of Large Language Models (LLMs) before\nextensive fine-tuning or merging can substantially reduce both computational\nexpense and development time. Although prior approaches like scaling laws\naccount for global factors such as parameter size or training tokens, they\noften overlook explicit lineage relationships - i.e., which models are derived\nor merged from which parents. In this work, we propose a novel\nLineage-Regularized Matrix Factorization (LRMF) framework that encodes\nancestral ties among LLMs via a graph Laplacian regularizer. By leveraging\nmulti-hop parent-child connections, LRMF consistently outperforms conventional\nmatrix factorization and collaborative filtering methods in both instance-level\nand benchmark-level performance prediction. Our large-scale study includes\n2,934 publicly available Hugging Face models and 21,000+ instances across 6\nmajor benchmarks, showing that lineage constraints yield up to 7-10 percentage\npoints higher correlation with actual performance compared to baselines.\nMoreover, LRMF effectively addresses the cold-start problem, providing accurate\nestimates for newly derived or merged models even with minimal data. This\nlineage-guided strategy thus offers a resource-efficient way to inform\nhyperparameter tuning, data selection, and model combination in modern LLM\ndevelopment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8c31\u7cfb\u6b63\u5219\u5316\u77e9\u9635\u5206\u89e3\uff08LRMF\uff09\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u62c9\u666e\u62c9\u65af\u6b63\u5219\u5316\u65b9\u6cd5\u7f16\u7801\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8c31\u7cfb\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982\u7f29\u653e\u5b9a\u5f8b\u867d\u7136\u8003\u8651\u5168\u5c40\u56e0\u7d20\uff08\u5982\u53c2\u6570\u91cf\u6216\u8bad\u7ec3\u4ee4\u724c\uff09\uff0c\u4f46\u5ffd\u7565\u4e86\u6a21\u578b\u95f4\u7684\u8c31\u7cfb\u5173\u7cfb\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5f15\u5165\u8c31\u7cfb\u5173\u7cfb\u6539\u8fdbLLM\u6027\u80fd\u9884\u6d4b\uff0c\u4ee5\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u548c\u5f00\u53d1\u65f6\u95f4\u3002", "method": "\u63d0\u51faLineage-Regularized Matrix Factorization (LRMF)\u6846\u67b6\uff0c\u5229\u7528\u56fe\u62c9\u666e\u62c9\u65af\u6b63\u5219\u5316\u7f16\u7801LLMs\u95f4\u7684\u591a\u4ee3\u8c31\u7cfb\u5173\u7cfb\uff0c\u5e76\u7ed3\u5408\u77e9\u9635\u5206\u89e3\u548c\u534f\u540c\u8fc7\u6ee4\u65b9\u6cd5\u3002", "result": "\u57282934\u4e2aHugging Face\u6a21\u578b\u548c21,000+\u5b9e\u4f8b\u76846\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLRMF\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5173\u6027\u9ad8\u51fa7-10\u4e2a\u767e\u5206\u70b9\uff0c\u4e14\u6709\u6548\u89e3\u51b3\u4e86\u51b7\u542f\u52a8\u95ee\u9898\u3002", "conclusion": "\u8c31\u7cfb\u7ea6\u675f\u663e\u8457\u63d0\u5347\u4e86LLM\u6027\u80fd\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u8d85\u53c2\u6570\u8c03\u4f18\u3001\u6570\u636e\u9009\u62e9\u548c\u6a21\u578b\u7ec4\u5408\u63d0\u4f9b\u4e86\u9ad8\u6548\u65b9\u6cd5\u3002"}}
{"id": "2504.18596", "pdf": "https://arxiv.org/pdf/2504.18596", "abs": "https://arxiv.org/abs/2504.18596", "authors": ["Anantha Sharma", "Swetha Devabhaktuni", "Eklove Mohan"], "title": "Optimizing the Privacy-Utility Balance using Synthetic Data and Configurable Perturbation Pipelines", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "math.PR"], "comment": "18 pages, 8 figures, 5 tables", "summary": "This paper explores the strategic use of modern synthetic data generation and\nadvanced data perturbation techniques to enhance security, maintain analytical\nutility, and improve operational efficiency when managing large datasets, with\na particular focus on the Banking, Financial Services, and Insurance (BFSI)\nsector. We contrast these advanced methods encompassing generative models like\nGANs, sophisticated context-aware PII transformation, configurable statistical\nperturbation, and differential privacy with traditional anonymization\napproaches.\n  The goal is to create realistic, privacy-preserving datasets that retain high\nutility for complex machine learning tasks and analytics, a critical need in\nthe data-sensitive industries like BFSI, Healthcare, Retail, and\nTelecommunications. We discuss how these modern techniques potentially offer\nsignificant improvements in balancing privacy preservation while maintaining\ndata utility compared to older methods. Furthermore, we examine the potential\nfor operational gains, such as reduced overhead and accelerated analytics, by\nusing these privacy-enhanced datasets. We also explore key use cases where\nthese methods can mitigate regulatory risks and enable scalable, data-driven\ninnovation without compromising sensitive customer information.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u73b0\u4ee3\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u9ad8\u7ea7\u6570\u636e\u6270\u52a8\u6280\u672f\u5728BFSI\u7b49\u884c\u4e1a\u4e2d\u7684\u5e94\u7528\uff0c\u65e8\u5728\u901a\u8fc7GANs\u3001PII\u8f6c\u6362\u7b49\u65b9\u6cd5\u63d0\u5347\u6570\u636e\u5b89\u5168\u6027\u548c\u5206\u6790\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u636e\u5b9e\u7528\u6027\u3002", "motivation": "\u5904\u7406\u654f\u611f\u6570\u636e\u65f6\u9700\u5e73\u8861\u9690\u79c1\u4fdd\u62a4\u548c\u6570\u636e\u5b9e\u7528\u6027\uff0c\u4f20\u7edf\u533f\u540d\u5316\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u9700\u63a2\u7d22\u66f4\u5148\u8fdb\u7684\u9690\u79c1\u589e\u5f3a\u6280\u672f\u3002", "method": "\u7ed3\u5408\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GANs\uff09\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684PII\u8f6c\u6362\u3001\u53ef\u914d\u7f6e\u7edf\u8ba1\u6270\u52a8\u548c\u5dee\u5206\u9690\u79c1\u7b49\u65b9\u6cd5\uff0c\u751f\u6210\u65e2\u9690\u79c1\u4fdd\u62a4\u53c8\u5b9e\u7528\u7684\u6570\u636e\u96c6\u3002", "result": "\u73b0\u4ee3\u6280\u672f\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u6570\u636e\u5b9e\u7528\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u80fd\u51cf\u5c11\u5f00\u9500\u5e76\u52a0\u901f\u5206\u6790\uff0c\u540c\u65f6\u7b26\u5408\u76d1\u7ba1\u8981\u6c42\u3002", "conclusion": "\u5148\u8fdb\u7684\u6570\u636e\u6270\u52a8\u548c\u5408\u6210\u6280\u672f\u4e3a\u6570\u636e\u654f\u611f\u884c\u4e1a\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u9690\u79c1\u4fdd\u62a4\u548c\u5b9e\u7528\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u6570\u636e\u9a71\u52a8\u521b\u65b0\u3002"}}
{"id": "2504.19188", "pdf": "https://arxiv.org/pdf/2504.19188", "abs": "https://arxiv.org/abs/2504.19188", "authors": ["Jianlong Chen", "Chao Li", "Yang Yuan", "Andrew C Yao"], "title": "Hierarchical Attention Generates Better Proofs", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.LO"], "comment": "15 pages with 3 figures", "summary": "Large language models (LLMs) have shown promise in formal theorem proving,\nbut their token-level processing often fails to capture the inherent\nhierarchical nature of mathematical proofs. We introduce \\textbf{Hierarchical\nAttention}, a regularization method that aligns LLMs' attention mechanisms with\nmathematical reasoning structures. Our approach establishes a five-level\nhierarchy from foundational elements to high-level concepts, ensuring\nstructured information flow in proof generation. Experiments demonstrate that\nour method improves proof success rates by 2.05\\% on miniF2F and 1.69\\% on\nProofNet while reducing proof complexity by 23.81\\% and 16.50\\% respectively.\nThe code is available at https://github.com/Car-pe/HAGBP.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5206\u5c42\u6ce8\u610f\u529b\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e94\u5c42\u6b21\u7ed3\u6784\u4f18\u5316LLM\u5728\u6570\u5b66\u5b9a\u7406\u8bc1\u660e\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u9a8c\u663e\u793a\u5728miniF2F\u548cProofNet\u4e0a\u5206\u522b\u63d0\u9ad8\u4e862.05%\u548c1.69%\u7684\u6210\u529f\u7387\uff0c\u5e76\u964d\u4f4e\u4e86\u8bc1\u660e\u590d\u6742\u5ea6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5f62\u5f0f\u5316\u5b9a\u7406\u8bc1\u660e\u4e2d\u8868\u73b0\u6f5c\u529b\uff0c\u4f46\u5176\u57fa\u4e8e\u4ee4\u724c\u7684\u5904\u7406\u65b9\u5f0f\u96be\u4ee5\u6355\u6349\u6570\u5b66\u8bc1\u660e\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7ed3\u6784\u5316\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u6ce8\u610f\u529b\u673a\u5236\u3002", "method": "\u8bba\u6587\u5f15\u5165\u5206\u5c42\u6ce8\u610f\u529b\uff0c\u901a\u8fc7\u5efa\u7acb\u4ece\u57fa\u7840\u5143\u7d20\u5230\u9ad8\u5c42\u6982\u5ff5\u7684\u4e94\u5c42\u6b21\u7ed3\u6784\uff0c\u786e\u4fdd\u8bc1\u660e\u751f\u6210\u4e2d\u7684\u4fe1\u606f\u6d41\u7ed3\u6784\u5316\uff0c\u4ece\u800c\u6539\u8fdbLLM\u7684\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728miniF2F\u548cProofNet\u6570\u636e\u96c6\u4e0a\u5206\u522b\u63d0\u9ad8\u4e862.05%\u548c1.69%\u7684\u8bc1\u660e\u6210\u529f\u7387\uff0c\u540c\u65f6\u8bc1\u660e\u590d\u6742\u5ea6\u5206\u522b\u964d\u4f4e\u4e8623.81%\u548c16.50%\u3002", "conclusion": "\u5206\u5c42\u6ce8\u610f\u529b\u65b9\u6cd5\u901a\u8fc7\u7ed3\u6784\u5316\u6ce8\u610f\u529b\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u6570\u5b66\u5b9a\u7406\u8bc1\u660e\u4e2d\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2504.19850", "pdf": "https://arxiv.org/pdf/2504.19850", "abs": "https://arxiv.org/abs/2504.19850", "authors": ["Kyo Gerrits", "Ana Guerberof-Arenas"], "title": "To MT or not to MT: An eye-tracking study on the reception by Dutch readers of different translation and creativity levels", "categories": ["cs.CL"], "comment": "This paper has been accepted to the MT Summit 2025 to be held in\n  Geneva on June 23-27 2025", "summary": "This article presents the results of a pilot study involving the reception of\na fictional short story translated from English into Dutch under four\nconditions: machine translation (MT), post-editing (PE), human translation (HT)\nand original source text (ST). The aim is to understand how creativity and\nerrors in different translation modalities affect readers, specifically\nregarding cognitive load. Eight participants filled in a questionnaire, read a\nstory using an eye-tracker, and conducted a retrospective think-aloud (RTA)\ninterview. The results show that units of creative potential (UCP) increase\ncognitive load and that this effect is highest for HT and lowest for MT; no\neffect of error was observed. Triangulating the data with RTAs leads us to\nhypothesize that the higher cognitive load in UCPs is linked to increases in\nreader enjoyment and immersion. The effect of translation creativity on\ncognitive load in different translation modalities at word-level is novel and\nopens up new avenues for further research. All the code and data are available\nat https://github.com/INCREC/Pilot_to_MT_or_not_to_MT", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u6bd4\u673a\u5668\u7ffb\u8bd1\u3001\u540e\u7f16\u8f91\u3001\u4eba\u5de5\u7ffb\u8bd1\u53ca\u539f\u6587\u5728\u8bfb\u8005\u8ba4\u77e5\u8d1f\u8377\u4e0a\u7684\u5dee\u5f02\uff0c\u53d1\u73b0\u521b\u9020\u6027\u6f5c\u529b\u5355\u5143\u589e\u52a0\u8ba4\u77e5\u8d1f\u8377\uff0c\u4eba\u5de5\u7ffb\u8bd1\u5f71\u54cd\u6700\u5927\uff0c\u673a\u5668\u7ffb\u8bd1\u6700\u5c0f\uff0c\u4e14\u9519\u8bef\u65e0\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u4e0d\u540c\u7ffb\u8bd1\u65b9\u5f0f\uff08\u673a\u5668\u7ffb\u8bd1\u3001\u540e\u7f16\u8f91\u3001\u4eba\u5de5\u7ffb\u8bd1\u53ca\u539f\u6587\uff09\u4e2d\u521b\u9020\u6027\u548c\u9519\u8bef\u5982\u4f55\u5f71\u54cd\u8bfb\u8005\u7684\u8ba4\u77e5\u8d1f\u8377\u3002", "method": "\u516b\u540d\u53c2\u4e0e\u8005\u901a\u8fc7\u95ee\u5377\u3001\u773c\u52a8\u4eea\u9605\u8bfb\u6545\u4e8b\u53ca\u56de\u987e\u6027\u6709\u58f0\u601d\u7ef4\u8bbf\u8c08\uff0c\u5206\u6790\u4e0d\u540c\u7ffb\u8bd1\u6761\u4ef6\u4e0b\u7684\u8ba4\u77e5\u8d1f\u8377\u3002", "result": "\u521b\u9020\u6027\u6f5c\u529b\u5355\u5143\u663e\u8457\u589e\u52a0\u8ba4\u77e5\u8d1f\u8377\uff0c\u4eba\u5de5\u7ffb\u8bd1\u6548\u5e94\u6700\u5f3a\uff0c\u673a\u5668\u7ffb\u8bd1\u6700\u5f31\uff1b\u9519\u8bef\u65e0\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u7ffb\u8bd1\u521b\u9020\u6027\u5728\u8bcd\u6c47\u5c42\u9762\u5bf9\u8ba4\u77e5\u8d1f\u8377\u7684\u5f71\u54cd\u662f\u65b0\u53d1\u73b0\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5f00\u8f9f\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.18598", "pdf": "https://arxiv.org/pdf/2504.18598", "abs": "https://arxiv.org/abs/2504.18598", "authors": ["Qingyue Wang", "Qi Pang", "Xixun Lin", "Shuai Wang", "Daoyuan Wu"], "title": "BadMoE: Backdooring Mixture-of-Experts LLMs via Optimizing Routing Triggers and Infecting Dormant Experts", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Mixture-of-Experts (MoE) have emerged as a powerful architecture for\n  large language models (LLMs), enabling efficient scaling of model capacity\n  while maintaining manageable computational costs. The key advantage lies in\n  their ability to route different tokens to different ``expert'' networks\n  within the model, enabling specialization and efficient handling of diverse\n  input. However, the vulnerabilities of MoE-based LLMs still have barely been\n  studied, and the potential for backdoor attacks in this context remains\n  largely unexplored. This paper presents the first backdoor attack against\n  MoE-based LLMs where the attackers poison ``dormant experts'' (i.e.,\nunderutilized\n  experts) and activate them by optimizing routing triggers, thereby gaining\n  control over the model's output. We first rigorously prove the existence of a\nfew ``dominating\n  experts'' in MoE models, whose outputs can determine the overall MoE's\n  output. We also show that dormant experts can serve as dominating experts to\nmanipulate model predictions.\n  Accordingly, our attack, namely \\textsc{BadMoE}, exploits the unique\n  architecture of MoE models by 1) identifying dormant experts unrelated to the\ntarget task, 2)\n  constructing a routing-aware loss to optimize the activation triggers of\nthese experts, and 3) promoting dormant experts to dominating roles via\npoisoned training data.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u9488\u5bf9\u57fa\u4e8eMoE\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u51fa\u4e86\u4e00\u79cd\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff08BadMoE\uff09\uff0c\u5229\u7528\u672a\u5145\u5206\u5229\u7528\u7684\u4e13\u5bb6\uff08dormant experts\uff09\u5e76\u4f18\u5316\u8def\u7531\u89e6\u53d1\u5668\u6765\u64cd\u63a7\u6a21\u578b\u8f93\u51fa\u3002", "motivation": "MoE\u67b6\u6784\u867d\u7136\u80fd\u9ad8\u6548\u6269\u5c55\u6a21\u578b\u5bb9\u91cf\uff0c\u4f46\u5176\u5b89\u5168\u6027\u7814\u7a76\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u540e\u95e8\u653b\u51fb\u7684\u6f5c\u5728\u98ce\u9669\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u653b\u51fb\u65b9\u6cd5\u5305\u62ec\u8bc6\u522b\u4e0e\u76ee\u6807\u4efb\u52a1\u65e0\u5173\u7684dormant experts\u3001\u6784\u5efa\u8def\u7531\u611f\u77e5\u635f\u5931\u4f18\u5316\u89e6\u53d1\u5668\u3001\u5e76\u901a\u8fc7\u4e2d\u6bd2\u6570\u636e\u63d0\u5347\u8fd9\u4e9b\u4e13\u5bb6\u7684\u4e3b\u5bfc\u6027\u3002", "result": "\u8bc1\u660e\u4e86MoE\u6a21\u578b\u4e2d\u5b58\u5728\u5c11\u91cf\u4e3b\u5bfc\u6027\u4e13\u5bb6\uff08dominating experts\uff09\uff0c\u4e14dormant experts\u53ef\u88ab\u64cd\u63a7\u4e3adominant experts\u4ee5\u63a7\u5236\u6a21\u578b\u9884\u6d4b\u3002", "conclusion": "BadMoE\u6210\u529f\u5229\u7528MoE\u67b6\u6784\u7684\u72ec\u7279\u6027\u5b9e\u73b0\u4e86\u540e\u95e8\u653b\u51fb\uff0c\u63ed\u793a\u4e86\u8be5\u7c7b\u6a21\u578b\u7684\u5b89\u5168\u9690\u60a3\u3002"}}
{"id": "2504.19199", "pdf": "https://arxiv.org/pdf/2504.19199", "abs": "https://arxiv.org/abs/2504.19199", "authors": ["Ming Xu", "Jinrong Xiang", "Zilong Xie", "Xiangfu Meng"], "title": "HetGL2R: Learning to Rank Critical Road Segments via Attributed Heterogeneous Graph Random Walks", "categories": ["cs.LG"], "comment": null, "summary": "Accurately identifying critical nodes with high spatial influence in road\nnetworks is essential for enhancing the efficiency of traffic management and\nurban planning. However, existing node importance ranking methods mainly rely\non structural features and topological information, often overlooking critical\nfactors such as origin-destination (OD) demand and route information. This\nlimitation leaves considerable room for improvement in ranking accuracy. To\naddress this issue, we propose HetGL2R, an attributed heterogeneous graph\nlearning approach for ranking node importance in road networks. This method\nintroduces a tripartite graph (trip graph) to model the structure of the road\nnetwork, integrating OD demand, route choice, and various structural features\nof road segments. Based on the trip graph, we design an embedding method to\nlearn node representations that reflect the spatial influence of road segments.\nThe method consists of a heterogeneous random walk sampling algorithm\n(HetGWalk) and a Transformer encoder. HetGWalk constructs multiple\nattribute-guided graphs based on the trip graph to enrich the diversity of\nsemantic associations between nodes. It then applies a joint random walk\nmechanism to convert both topological structures and node attributes into\nsequences, enabling the encoder to capture spatial dependencies more\neffectively among road segments. Finally, a listwise ranking strategy is\nemployed to evaluate node importance. To validate the performance of our\nmethod, we construct two synthetic datasets using SUMO based on simulated road\nnetworks. Experimental results demonstrate that HetGL2R significantly\noutperforms baselines in incorporating OD demand and route choice information,\nachieving more accurate and robust node ranking. Furthermore, we conduct a case\nstudy using real-world taxi trajectory data from Beijing, further verifying the\npracticality of the proposed method.", "AI": {"tldr": "\u63d0\u51faHetGL2R\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f02\u6784\u56fe\u5b66\u4e60\u6574\u5408OD\u9700\u6c42\u548c\u8def\u5f84\u9009\u62e9\u4fe1\u606f\uff0c\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u9053\u8def\u7f51\u7edc\u4e2d\u8282\u70b9\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u8282\u70b9\u91cd\u8981\u6027\u6392\u5e8f\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7ed3\u6784\u7279\u5f81\u548c\u62d3\u6251\u4fe1\u606f\uff0c\u5ffd\u7565OD\u9700\u6c42\u548c\u8def\u5f84\u7b49\u5173\u952e\u56e0\u7d20\uff0c\u5bfc\u81f4\u6392\u5e8f\u51c6\u786e\u6027\u4e0d\u8db3\u3002", "method": "HetGL2R\u6784\u5efa\u4e09\u90e8\u5206\u56fe\uff08\u65c5\u884c\u56fe\uff09\u6574\u5408OD\u9700\u6c42\u3001\u8def\u5f84\u9009\u62e9\u548c\u7ed3\u6784\u7279\u5f81\uff1b\u4f7f\u7528\u5f02\u6784\u56fe\u968f\u673a\u6e38\u8d70\u91c7\u6837\u7b97\u6cd5\uff08HetGWalk\uff09\u548cTransformer\u7f16\u7801\u5668\u5b66\u4e60\u8282\u70b9\u8868\u5f81\uff1b\u6700\u7ec8\u91c7\u7528\u5217\u8868\u6392\u5e8f\u7b56\u7565\u8bc4\u4f30\u8282\u70b9\u91cd\u8981\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eHetGL2R\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u5317\u4eac\u51fa\u79df\u8f66\u8f68\u8ff9\u6570\u636e\u4e2d\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "HetGL2R\u901a\u8fc7\u591a\u7279\u5f81\u878d\u5408\u6709\u6548\u63d0\u5347\u8282\u70b9\u6392\u5e8f\u6027\u80fd\uff0c\u4e3a\u4ea4\u901a\u7ba1\u7406\u548c\u57ce\u5e02\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5de5\u5177\u3002"}}
{"id": "2504.19856", "pdf": "https://arxiv.org/pdf/2504.19856", "abs": "https://arxiv.org/abs/2504.19856", "authors": ["Anastasia Zhukova", "Christian E. Matt", "Terry Ruas", "Bela Gipp"], "title": "Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language", "categories": ["cs.CL"], "comment": null, "summary": "Domain-adaptive continual pretraining (DAPT) is a state-of-the-art technique\nthat further trains a language model (LM) on its pretraining task, e.g.,\nlanguage masking. Although popular, it requires a significant corpus of\ndomain-related data, which is difficult to obtain for specific domains in\nlanguages other than English, such as the process industry in the German\nlanguage. This paper introduces an efficient approach called ICL-augmented\npretraining or ICL-APT that leverages in-context learning (ICL) and k-nearest\nneighbors (kNN) to augment target data with domain-related and in-domain texts,\nsignificantly reducing GPU time while maintaining strong model performance. Our\nresults show that this approach performs better than traditional DAPT by 3.5 of\nthe average IR metrics (e.g., mAP, MRR, and nDCG) and requires almost 4 times\nless computing time, providing a cost-effective solution for industries with\nlimited computational capacity. The findings highlight the broader\napplicability of this framework to other low-resource industries, making\nNLP-based solutions more accessible and feasible in production environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aICL-APT\u7684\u9ad8\u6548\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u4e0a\u4e0b\u6587\u5b66\u4e60\u548ck\u8fd1\u90bb\u7b97\u6cd5\uff0c\u51cf\u5c11\u5bf9\u7279\u5b9a\u9886\u57df\u6570\u636e\u7684\u9700\u6c42\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u65f6\u95f4\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u9886\u57df\u81ea\u9002\u5e94\u6301\u7eed\u9884\u8bad\u7ec3\uff08DAPT\uff09\u9700\u8981\u5927\u91cf\u7279\u5b9a\u9886\u57df\u6570\u636e\uff0c\u8fd9\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u6216\u7279\u5b9a\u884c\u4e1a\uff08\u5982\u5fb7\u8bed\u8fc7\u7a0b\u5de5\u4e1a\uff09\u4e2d\u96be\u4ee5\u83b7\u53d6\u3002", "method": "\u63d0\u51faICL-APT\u65b9\u6cd5\uff0c\u5229\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u548ck\u8fd1\u90bb\uff08kNN\uff09\uff0c\u901a\u8fc7\u589e\u5f3a\u76ee\u6807\u9886\u57df\u6587\u672c\u6570\u636e\uff0c\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "result": "ICL-APT\u5728\u5e73\u5747IR\u6307\u6807\uff08\u5982mAP\u3001MRR\u3001nDCG\uff09\u4e0a\u6bd4DAPT\u9ad83.5\u5206\uff0c\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c11\u8fd14\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u884c\u4e1a\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u9002\u7528\u4e8e\u5176\u4ed6\u4f4e\u8d44\u6e90\u9886\u57df\u3002"}}
{"id": "2504.18601", "pdf": "https://arxiv.org/pdf/2504.18601", "abs": "https://arxiv.org/abs/2504.18601", "authors": ["Philipp Koralus"], "title": "The Philosophic Turn for AI Agents: Replacing centralized digital rhetoric with decentralized truth-seeking", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "In the face of rapidly advancing AI technology, individuals will increasingly\nrely on AI agents to navigate life's growing complexities, raising critical\nconcerns about maintaining both human agency and autonomy. This paper addresses\na fundamental dilemma posed by AI decision-support systems: the risk of either\nbecoming overwhelmed by complex decisions, thus losing agency, or having\nautonomy compromised by externally controlled choice architectures reminiscent\nof ``nudging'' practices. While the ``nudge'' framework, based on the use of\nchoice-framing to guide individuals toward presumed beneficial outcomes,\ninitially appeared to preserve liberty, at AI-driven scale, it threatens to\nerode autonomy. To counteract this risk, the paper proposes a philosophic turn\nin AI design. AI should be constructed to facilitate decentralized\ntruth-seeking and open-ended inquiry, mirroring the Socratic method of\nphilosophical dialogue. By promoting individual and collective adaptive\nlearning, such AI systems would empower users to maintain control over their\njudgments, augmenting their agency without undermining autonomy. The paper\nconcludes by outlining essential features for autonomy-preserving AI systems,\nsketching a path toward AI systems that enhance human judgment rather than\nundermine it.", "AI": {"tldr": "AI\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u53ef\u80fd\u5a01\u80c1\u4eba\u7c7b\u81ea\u4e3b\u6743\uff0c\u9700\u8981\u901a\u8fc7\u54f2\u5b66\u5316\u7684AI\u8bbe\u8ba1\uff08\u5982\u82cf\u683c\u62c9\u5e95\u5bf9\u8bdd\u6cd5\uff09\u6765\u589e\u5f3a\u7528\u6237\u5224\u65ad\u529b\uff0c\u800c\u975e\u524a\u5f31\u5176\u81ea\u4e3b\u6027\u3002", "motivation": "\u9762\u5bf9AI\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5982\u4f55\u5728\u63d0\u5347\u51b3\u7b56\u6548\u7387\u7684\u540c\u65f6\u4fdd\u62a4\u4eba\u7c7b\u81ea\u4e3b\u6743\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u54f2\u5b66\u5316\u7684AI\u8bbe\u8ba1\u7406\u5ff5\uff0c\u91c7\u7528\u7c7b\u4f3c\u82cf\u683c\u62c9\u5e95\u5bf9\u8bdd\u7684\u65b9\u6cd5\uff0c\u4fc3\u8fdb\u5206\u6563\u5f0f\u771f\u76f8\u63a2\u7d22\u548c\u5f00\u653e\u5f0f\u63a2\u7a76\u3002", "result": "\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u589e\u5f3a\u7528\u6237\u7684\u5224\u65ad\u529b\u548c\u81ea\u4e3b\u6027\uff0c\u907f\u514d\u201c\u52a9\u63a8\u201d\u6846\u67b6\u5bf9\u81ea\u4e3b\u6743\u7684\u4fb5\u8680\u3002", "conclusion": "\u901a\u8fc7\u6784\u5efa\u652f\u6301\u81ea\u4e3b\u5b66\u4e60\u7684AI\u7cfb\u7edf\uff0c\u53ef\u4ee5\u5728\u63d0\u5347\u4eba\u7c7b\u51b3\u7b56\u80fd\u529b\u7684\u540c\u65f6\u4fdd\u62a4\u81ea\u4e3b\u6743\u3002"}}
{"id": "2504.19259", "pdf": "https://arxiv.org/pdf/2504.19259", "abs": "https://arxiv.org/abs/2504.19259", "authors": ["Adwait Datar", "Nihat Ay"], "title": "Convergence Properties of Natural Gradient Descent for Minimizing KL Divergence", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "The Kullback-Leibler (KL) divergence plays a central role in probabilistic\nmachine learning, where it commonly serves as the canonical loss function.\nOptimization in such settings is often performed over the probability simplex,\nwhere the choice of parameterization significantly impacts convergence. In this\nwork, we study the problem of minimizing the KL divergence and analyze the\nbehavior of gradient-based optimization algorithms under two dual coordinate\nsystems within the framework of information geometry$-$ the exponential family\n($\\theta$ coordinates) and the mixture family ($\\eta$ coordinates). We compare\nEuclidean gradient descent (GD) in these coordinates with the\ncoordinate-invariant natural gradient descent (NGD), where the natural gradient\nis a Riemannian gradient that incorporates the intrinsic geometry of the\nparameter space. In continuous time, we prove that the convergence rates of GD\nin the $\\theta$ and $\\eta$ coordinates provide lower and upper bounds,\nrespectively, on the convergence rate of NGD. Moreover, under affine\nreparameterizations of the dual coordinates, the convergence rates of GD in\n$\\eta$ and $\\theta$ coordinates can be scaled to $2c$ and $\\frac{2}{c}$,\nrespectively, for any $c>0$, while NGD maintains a fixed convergence rate of\n$2$, remaining invariant to such transformations and sandwiched between them.\nAlthough this suggests that NGD may not exhibit uniformly superior convergence\nin continuous time, we demonstrate that its advantages become pronounced in\ndiscrete time, where it achieves faster convergence and greater robustness to\nnoise, outperforming GD. Our analysis hinges on bounding the spectrum and\ncondition number of the Hessian of the KL divergence at the optimum, which\ncoincides with the Fisher information matrix.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u6982\u7387\u5355\u7eaf\u5f62\u4e0a\u6700\u5c0f\u5316KL\u6563\u5ea6\u7684\u4f18\u5316\u95ee\u9898\uff0c\u5206\u6790\u4e86\u4e24\u79cd\u5bf9\u5076\u5750\u6807\u7cfb\u4e0b\u68af\u5ea6\u4e0b\u964d\uff08GD\uff09\u4e0e\u81ea\u7136\u68af\u5ea6\u4e0b\u964d\uff08NGD\uff09\u7684\u6536\u655b\u884c\u4e3a\uff0c\u8bc1\u660eNGD\u5728\u79bb\u6563\u65f6\u95f4\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "KL\u6563\u5ea6\u662f\u6982\u7387\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6838\u5fc3\u635f\u5931\u51fd\u6570\uff0c\u4f46\u53c2\u6570\u5316\u9009\u62e9\u663e\u8457\u5f71\u54cd\u4f18\u5316\u6536\u655b\u6027\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u4fe1\u606f\u51e0\u4f55\u6846\u67b6\u6bd4\u8f83\u4e0d\u540c\u5750\u6807\u7cfb\u4e0bGD\u4e0eNGD\u7684\u6536\u655b\u6027\u80fd\u3002", "method": "\u5728\u6307\u6570\u65cf\uff08\u03b8\u5750\u6807\uff09\u548c\u6df7\u5408\u65cf\uff08\u03b7\u5750\u6807\uff09\u4e0b\uff0c\u5bf9\u6bd4\u6b27\u6c0f\u68af\u5ea6\u4e0b\u964d\uff08GD\uff09\u4e0e\u5750\u6807\u4e0d\u53d8\u7684\u81ea\u7136\u68af\u5ea6\u4e0b\u964d\uff08NGD\uff09\uff0c\u5e76\u901a\u8fc7\u5206\u6790KL\u6563\u5ea6Hessian\u8c31\u53ca\u6761\u4ef6\u6570\uff08\u5373Fisher\u4fe1\u606f\u77e9\u9635\uff09\u63a8\u5bfc\u6536\u655b\u7387\u3002", "result": "\u8fde\u7eed\u65f6\u95f4\u4e2d\uff0cGD\u5728\u03b8\u548c\u03b7\u5750\u6807\u7684\u6536\u655b\u7387\u5206\u522b\u63d0\u4f9bNGD\u7684\u4e0b\u754c\u548c\u4e0a\u754c\uff1b\u82e5\u5bf9\u5076\u5750\u6807\u4eff\u5c04\u53d8\u6362\uff0cGD\u6536\u655b\u7387\u53ef\u7f29\u653e\uff0c\u800cNGD\u4fdd\u6301\u56fa\u5b9a\u6536\u655b\u73872\u3002\u79bb\u6563\u65f6\u95f4\u4e2d\uff0cNGD\u6536\u655b\u66f4\u5feb\u4e14\u5bf9\u566a\u58f0\u66f4\u9c81\u68d2\u3002", "conclusion": "\u5c3d\u7ba1NGD\u5728\u8fde\u7eed\u65f6\u95f4\u4e2d\u4e0d\u4e00\u5b9a\u5168\u5c40\u6700\u4f18\uff0c\u4f46\u5728\u79bb\u6563\u65f6\u95f4\u4e2d\u56e0\u5176\u66f4\u5feb\u7684\u6536\u655b\u548c\u9c81\u68d2\u6027\u6210\u4e3a\u66f4\u4f18\u9009\u62e9\uff0c\u51f8\u663e\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.19867", "pdf": "https://arxiv.org/pdf/2504.19867", "abs": "https://arxiv.org/abs/2504.19867", "authors": ["Ke Hong", "Lufang Chen", "Zhong Wang", "Xiuhong Li", "Qiuli Mao", "Jianping Ma", "Chao Xiong", "Guanyu Wu", "Buhe Han", "Guohao Dai", "Yun Liang", "Yu Wang"], "title": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage", "categories": ["cs.CL", "cs.DC", "cs.LG"], "comment": "18 pages, 16 figures", "summary": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578bLLM\u670d\u52a1\u7cfb\u7edfsemi-PD\uff0c\u901a\u8fc7\u89e3\u8026\u8ba1\u7b97\u4e0e\u7edf\u4e00\u5b58\u50a8\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u4e2d\u5b58\u50a8\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u8bf7\u6c42\u7387\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u670d\u52a1\u7cfb\u7edf\u5b58\u5728\u5b58\u50a8\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u5982\u6743\u91cd\u590d\u5236\u3001KV\u7f13\u5b58\u4f20\u8f93\u5f00\u9500\u3001\u5b58\u50a8\u4e0d\u5e73\u8861\u7b49\uff0c\u5bfc\u81f4\u9ad8\u8bf7\u6c42\u7387\u4e0b\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51fasemi-PD\u7cfb\u7edf\uff0c\u901a\u8fc7\u8ba1\u7b97\u8d44\u6e90\u63a7\u5236\u5668\u5b9e\u73b0SM\u7ea7\u89e3\u8026\u8ba1\u7b97\uff0c\u5e76\u901a\u8fc7\u7edf\u4e00\u5185\u5b58\u7ba1\u7406\u5668\u7ba1\u7406\u5f02\u6b65\u5185\u5b58\u8bbf\u95ee\uff0c\u540c\u65f6\u5f15\u5165\u52a8\u6001\u5206\u533a\u7b97\u6cd5\u4f18\u5316SLO\u8fbe\u6210\u3002", "result": "semi-PD\u5728\u9ad8\u8bf7\u6c42\u7387\u4e0b\u4fdd\u6301\u66f4\u4f4e\u5ef6\u8fdf\uff0c\u8bf7\u6c42\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e1.27-2.58\u500d\uff0c\u5e76\u80fd\u5728\u5ef6\u8fdf\u7ea6\u675f\u4e0b\u5904\u74061.55-1.72\u500d\u7684\u8bf7\u6c42\u3002", "conclusion": "semi-PD\u901a\u8fc7\u89e3\u8026\u8ba1\u7b97\u4e0e\u7edf\u4e00\u5b58\u50a8\u7684\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347LLM\u670d\u52a1\u7cfb\u7edf\u7684\u5b58\u50a8\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e3a\u9ad8\u8d1f\u8f7d\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18603", "pdf": "https://arxiv.org/pdf/2504.18603", "abs": "https://arxiv.org/abs/2504.18603", "authors": ["Iizalaarab Elhaimeur", "Nikos Chrisochoides"], "title": "Toward Personalizing Quantum Computing Education: An Evolutionary LLM-Powered Approach", "categories": ["cs.CY", "cs.AI", "cs.MA"], "comment": null, "summary": "Quantum computing education faces significant challenges due to its\ncomplexity and the limitations of current tools; this paper introduces a novel\nIntelligent Teaching Assistant for quantum computing education and details its\nevolutionary design process. The system combines a knowledge-graph-augmented\narchitecture with two specialized Large Language Model (LLM) agents: a Teaching\nAgent for dynamic interaction, and a Lesson Planning Agent for lesson plan\ngeneration. The system is designed to adapt to individual student needs, with\ninteractions meticulously tracked and stored in a knowledge graph. This graph\nrepresents student actions, learning resources, and relationships, aiming to\nenable reasoning about effective learning pathways. We describe the\nimplementation of the system, highlighting the challenges encountered and the\nsolutions implemented, including introducing a dual-agent architecture where\ntasks are separated, all coordinated through a central knowledge graph that\nmaintains system awareness, and a user-facing tag system intended to mitigate\nLLM hallucination and improve user control. Preliminary results illustrate the\nsystem's potential to capture rich interaction data, dynamically adapt lesson\nplans based on student feedback via a tag system in simulation, and facilitate\ncontext-aware tutoring through the integrated knowledge graph, though\nsystematic evaluation is required.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u91cf\u5b50\u8ba1\u7b97\u6559\u80b2\u7684\u667a\u80fd\u6559\u5b66\u52a9\u624b\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u67b6\u6784\u548c\u4e24\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\uff0c\u52a8\u6001\u9002\u5e94\u5b66\u751f\u9700\u6c42\uff0c\u521d\u6b65\u7ed3\u679c\u663e\u793a\u5176\u6f5c\u529b\uff0c\u4f46\u9700\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u6559\u80b2\u56e0\u590d\u6742\u6027\u548c\u73b0\u6709\u5de5\u5177\u7684\u5c40\u9650\u6027\u9762\u4e34\u6311\u6218\uff0c\u4f5c\u8005\u5e0c\u671b\u8bbe\u8ba1\u4e00\u79cd\u667a\u80fd\u6559\u5b66\u52a9\u624b\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u548cLLM\u4ee3\u7406\u52a8\u6001\u9002\u5e94\u5b66\u751f\u9700\u6c42\uff0c\u63d0\u5347\u6559\u5b66\u6548\u679c\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u67b6\u6784\uff0c\u5305\u542b\u4e24\u4e2aLLM\u4ee3\u7406\uff1a\u6559\u5b66\u4ee3\u7406\uff08\u52a8\u6001\u4ea4\u4e92\uff09\u548c\u8bfe\u7a0b\u89c4\u5212\u4ee3\u7406\uff08\u751f\u6210\u6559\u6848\uff09\uff0c\u5e76\u901a\u8fc7\u4e2d\u592e\u77e5\u8bc6\u56fe\u8c31\u534f\u8c03\u4efb\u52a1\uff0c\u5f15\u5165\u7528\u6237\u6807\u7b7e\u7cfb\u7edf\u51cf\u5c11LLM\u5e7b\u89c9\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff0c\u7cfb\u7edf\u80fd\u6355\u83b7\u4e30\u5bcc\u7684\u4ea4\u4e92\u6570\u636e\uff0c\u901a\u8fc7\u6807\u7b7e\u7cfb\u7edf\u52a8\u6001\u8c03\u6574\u6559\u6848\uff0c\u5e76\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u5b9e\u73b0\u60c5\u5883\u611f\u77e5\u8f85\u5bfc\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u5728\u91cf\u5b50\u8ba1\u7b97\u6559\u80b2\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u52a8\u6001\u9002\u5e94\u548c\u77e5\u8bc6\u56fe\u8c31\u7684\u5e94\u7528\uff0c\u4f46\u9700\u66f4\u591a\u7cfb\u7edf\u6027\u9a8c\u8bc1\u4ee5\u786e\u8ba4\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2504.19274", "pdf": "https://arxiv.org/pdf/2504.19274", "abs": "https://arxiv.org/abs/2504.19274", "authors": ["Mohammad M Maheri", "Hamed Haddadi", "Alex Davidson"], "title": "TeleSparse: Practical Privacy-Preserving Verification of Deep Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "This paper has been accepted to the Privacy Enhancing Technologies\n  Symposium (PETS) 2025", "summary": "Verification of the integrity of deep learning inference is crucial for\nunderstanding whether a model is being applied correctly. However, such\nverification typically requires access to model weights and (potentially\nsensitive or private) training data. So-called Zero-knowledge Succinct\nNon-Interactive Arguments of Knowledge (ZK-SNARKs) would appear to provide the\ncapability to verify model inference without access to such sensitive data.\nHowever, applying ZK-SNARKs to modern neural networks, such as transformers and\nlarge vision models, introduces significant computational overhead.\n  We present TeleSparse, a ZK-friendly post-processing mechanisms to produce\npractical solutions to this problem. TeleSparse tackles two fundamental\nchallenges inherent in applying ZK-SNARKs to modern neural networks: (1)\nReducing circuit constraints: Over-parameterized models result in numerous\nconstraints for ZK-SNARK verification, driving up memory and proof generation\ncosts. We address this by applying sparsification to neural network models,\nenhancing proof efficiency without compromising accuracy or security. (2)\nMinimizing the size of lookup tables required for non-linear functions, by\noptimizing activation ranges through neural teleportation, a novel adaptation\nfor narrowing activation functions' range.\n  TeleSparse reduces prover memory usage by 67% and proof generation time by\n46% on the same model, with an accuracy trade-off of approximately 1%. We\nimplement our framework using the Halo2 proving system and demonstrate its\neffectiveness across multiple architectures (Vision-transformer, ResNet,\nMobileNet) and datasets (ImageNet,CIFAR-10,CIFAR-100). This work opens new\ndirections for ZK-friendly model design, moving toward scalable,\nresource-efficient verifiable deep learning.", "AI": {"tldr": "TeleSparse \u662f\u4e00\u79cd ZK \u53cb\u597d\u7684\u540e\u5904\u7406\u673a\u5236\uff0c\u901a\u8fc7\u7a00\u758f\u5316\u548c\u795e\u7ecf\u4f20\u9001\u4f18\u5316\uff0c\u663e\u8457\u964d\u4f4e\u4e86 ZK-SNARKs \u5728\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u7531\u4e8e\u9a8c\u8bc1\u6df1\u5ea6\u5b66\u4e60\u63a8\u7406\u901a\u5e38\u9700\u8981\u8bbf\u95ee\u654f\u611f\u6570\u636e\uff08\u5982\u6a21\u578b\u6743\u91cd\u548c\u8bad\u7ec3\u6570\u636e\uff09\uff0c\u800c ZK-SNARKs \u53ef\u4ee5\u5728\u4e0d\u6cc4\u9732\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b8c\u6210\u9a8c\u8bc1\uff0c\u4f46\u5e94\u7528\u4e8e\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u65f6\u8ba1\u7b97\u5f00\u9500\u5de8\u5927\u3002TeleSparse \u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "TeleSparse \u901a\u8fc7\u7a00\u758f\u5316\u51cf\u5c11\u7535\u8def\u7ea6\u675f\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u4f20\u9001\u4f18\u5316\u975e\u7ebf\u6027\u51fd\u6570\u7684\u67e5\u627e\u8868\u5927\u5c0f\uff0c\u4ece\u800c\u63d0\u5347 ZK-SNARKs \u7684\u6548\u7387\u3002", "result": "TeleSparse \u5c06\u9a8c\u8bc1\u5185\u5b58\u4f7f\u7528\u964d\u4f4e 67%\uff0c\u8bc1\u660e\u751f\u6210\u65f6\u95f4\u51cf\u5c11 46%\uff0c\u4e14\u4ec5\u727a\u7272\u7ea6 1% \u7684\u51c6\u786e\u7387\u3002\u5728\u591a\u79cd\u67b6\u6784\u548c\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "TeleSparse \u4e3a ZK \u53cb\u597d\u578b\u6a21\u578b\u8bbe\u8ba1\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u63a8\u52a8\u4e86\u53ef\u6269\u5c55\u3001\u8d44\u6e90\u9ad8\u6548\u7684\u9a8c\u8bc1\u6df1\u5ea6\u5b66\u4e60\u53d1\u5c55\u3002"}}
{"id": "2504.19898", "pdf": "https://arxiv.org/pdf/2504.19898", "abs": "https://arxiv.org/abs/2504.19898", "authors": ["Mingqian He", "Fei Zhao", "Chonggang Lu", "Ziyan Liu", "Yue Wang", "Haofu Qian"], "title": "GenCLS++: Pushing the Boundaries of Generative Classification in LLMs Through Comprehensive SFT and RL Studies Across Diverse Datasets", "categories": ["cs.CL"], "comment": null, "summary": "As a fundamental task in machine learning, text classification plays a\ncrucial role in many areas. With the rapid scaling of Large Language Models\n(LLMs), particularly through reinforcement learning (RL), there is a growing\nneed for more capable discriminators. Consequently, advances in classification\nare becoming increasingly vital for enhancing the overall capabilities of LLMs.\nTraditional discriminative methods map text to labels but overlook LLMs'\nintrinsic generative strengths. Generative classification addresses this by\nprompting the model to directly output labels. However, existing studies still\nrely on simple SFT alone, seldom probing the interplay between training and\ninference prompts, and no work has systematically leveraged RL for generative\ntext classifiers and unified SFT, RL, and inference-time prompting in one\nframework. We bridge this gap with GenCLS++, a framework that jointly optimizes\nSFT and RL while systematically exploring five high-level strategy\ndimensions-in-context learning variants, category definitions, explicit\nuncertainty labels, semantically irrelevant numeric labels, and\nperplexity-based decoding-during both training and inference. After an SFT\n\"policy warm-up,\" we apply RL with a simple rule-based reward, yielding sizable\nextra gains. Across seven datasets, GenCLS++ achieves an average accuracy\nimprovement of 3.46% relative to the naive SFT baseline; on public datasets,\nthis improvement rises to 4.00%. Notably, unlike reasoning-intensive tasks that\nbenefit from explicit thinking processes, we find that classification tasks\nperform better without such reasoning steps. These insights into the role of\nexplicit reasoning provide valuable guidance for future LLM applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGenCLS++\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4f18\u5316\u751f\u6210\u5f0f\u6587\u672c\u5206\u7c7b\u5668\uff0c\u5e76\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u7cfb\u7edf\u6027\u63a2\u7d22\u4e94\u79cd\u7b56\u7565\u7ef4\u5ea6\uff0c\u6700\u7ec8\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u5224\u522b\u65b9\u6cd5\u5ffd\u7565\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u751f\u6210\u80fd\u529b\uff0c\u800c\u73b0\u6709\u751f\u6210\u5f0f\u5206\u7c7b\u7814\u7a76\u4ec5\u4f9d\u8d56\u7b80\u5355\u76d1\u7763\u5fae\u8c03\uff0c\u672a\u5145\u5206\u5229\u7528\u8bad\u7ec3\u4e0e\u63a8\u7406\u63d0\u793a\u7684\u4ea4\u4e92\u53ca\u5f3a\u5316\u5b66\u4e60\u7684\u6f5c\u529b\u3002", "method": "GenCLS++\u6846\u67b6\u8054\u5408\u4f18\u5316SFT\u548cRL\uff0c\u5e76\u7cfb\u7edf\u6027\u63a2\u7d22\u4e94\u79cd\u7b56\u7565\u7ef4\u5ea6\uff08\u5982\u4e0a\u4e0b\u6587\u5b66\u4e60\u53d8\u4f53\u3001\u7c7b\u522b\u5b9a\u4e49\u7b49\uff09\uff0c\u5728SFT\u9884\u70ed\u540e\u5e94\u7528\u57fa\u4e8e\u89c4\u5219\u7684RL\u5956\u52b1\u3002", "result": "\u5728\u4e03\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cGenCLS++\u5e73\u5747\u51c6\u786e\u7387\u6bd4\u57fa\u7ebf\u63d0\u53473.46%\uff0c\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u8fbe4%\u3002\u7814\u7a76\u53d1\u73b0\u5206\u7c7b\u4efb\u52a1\u65e0\u9700\u663e\u5f0f\u63a8\u7406\u6b65\u9aa4\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u751f\u6210\u5f0f\u6587\u672c\u5206\u7c7b\u63d0\u4f9b\u4e86\u7edf\u4e00\u4f18\u5316\u65b9\u6848\uff0c\u5e76\u63ed\u793a\u4e86\u663e\u5f0f\u63a8\u7406\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u53cd\u4f5c\u7528\uff0c\u4e3a\u672a\u6765LLM\u5e94\u7528\u63d0\u4f9b\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2504.18636", "pdf": "https://arxiv.org/pdf/2504.18636", "abs": "https://arxiv.org/abs/2504.18636", "authors": ["Lohith Srikanth Pentapalli", "Jon Salisbury", "Josette Riep", "Kelly Cohen"], "title": "A Gradient-Optimized TSK Fuzzy Framework for Explainable Phishing Detection", "categories": ["cs.CR", "cs.AI", "cs.LO"], "comment": "14 pages, 5 figures", "summary": "Phishing attacks represent an increasingly sophisticated and pervasive threat\nto individuals and organizations, causing significant financial losses,\nidentity theft, and severe damage to institutional reputations. Existing\nphishing detection methods often struggle to simultaneously achieve high\naccuracy and explainability, either failing to detect novel attacks or\noperating as opaque black-box models. To address this critical gap, we propose\na novel phishing URL detection system based on a first-order Takagi-Sugeno-Kang\n(TSK) fuzzy inference model optimized through gradient-based techniques. Our\napproach intelligently combines the interpretability and human-like reasoning\ncapabilities of fuzzy logic with the precision and adaptability provided by\ngradient optimization methods, specifically leveraging the Adam optimizer for\nefficient parameter tuning. Experiments conducted using a comprehensive dataset\nof over 235,000 URLs demonstrate rapid convergence, exceptional predictive\nperformance (accuracy averaging 99.95% across 5 cross-validation folds, with a\nperfect AUC i.e. 1.00). Furthermore, optimized fuzzy rules and membership\nfunctions improve interoperability, clearly indicating how the model makes\ndecisions - an essential feature for cybersecurity applications. This\nhigh-performance, transparent, and interpretable phishing detection framework\nsignificantly advances current cybersecurity defenses, providing practitioners\nwith accurate and explainable decision-making tools.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u6a21\u7cca\u903b\u8f91\u548c\u68af\u5ea6\u4f18\u5316\u7684\u65b0\u578b\u7f51\u7edc\u9493\u9c7cURL\u68c0\u6d4b\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\u548c\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u7f51\u7edc\u9493\u9c7c\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u65b0\u578b\u653b\u51fb\u68c0\u6d4b\u6548\u679c\u4e0d\u4f73\u6216\u6a21\u578b\u8fc7\u4e8e\u9ed1\u76d2\u5316\u3002", "method": "\u91c7\u7528\u4e00\u9636Takagi-Sugeno-Kang\u6a21\u7cca\u63a8\u7406\u6a21\u578b\uff0c\u7ed3\u5408Adam\u4f18\u5316\u5668\u8fdb\u884c\u68af\u5ea6\u4f18\u5316\u3002", "result": "\u5728235,000\u591a\u4e2aURL\u7684\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u51c6\u786e\u738799.95%\uff0cAUC\u4e3a1.00\uff0c\u6a21\u578b\u6536\u655b\u5feb\u4e14\u900f\u660e\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u7f51\u7edc\u5b89\u5168\u9632\u5fa1\u80fd\u529b\uff0c\u63d0\u4f9b\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u51b3\u7b56\u5de5\u5177\u3002"}}
{"id": "2504.19276", "pdf": "https://arxiv.org/pdf/2504.19276", "abs": "https://arxiv.org/abs/2504.19276", "authors": ["Yiyang Zhou", "Zhaoyang Wang", "Tianle Wang", "Shangyu Xing", "Peng Xia", "Bo Li", "Kaiyuan Zheng", "Zijian Zhang", "Zhaorun Chen", "Wenhao Zheng", "Xuchao Zhang", "Chetan Bansal", "Weitong Zhang", "Ying Wei", "Mohit Bansal", "Huaxiu Yao"], "title": "Anyprefer: An Agentic Framework for Preference Data Synthesis", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "High-quality preference data is essential for aligning foundation models with\nhuman values through preference learning. However, manual annotation of such\ndata is often time-consuming and costly. Recent methods often adopt a\nself-rewarding approach, where the target model generates and annotates its own\npreference data, but this can lead to inaccuracies since the reward model\nshares weights with the target model, thereby amplifying inherent biases. To\naddress these issues, we propose Anyprefer, a framework designed to synthesize\nhigh-quality preference data for aligning the target model. Anyprefer frames\nthe data synthesis process as a cooperative two-player Markov Game, where the\ntarget model and the judge model collaborate together. Here, a series of\nexternal tools are introduced to assist the judge model in accurately rewarding\nthe target model's responses, mitigating biases in the rewarding process. In\naddition, a feedback mechanism is introduced to optimize prompts for both\nmodels, enhancing collaboration and improving data quality. The synthesized\ndata is compiled into a new preference dataset, Anyprefer-V1, consisting of 58K\nhigh-quality preference pairs. Extensive experiments show that Anyprefer\nsignificantly improves model alignment performance across four main\napplications, covering 21 datasets, achieving average improvements of 18.55% in\nfive natural language generation datasets, 3.66% in nine vision-language\nunderstanding datasets, 30.05% in three medical image analysis datasets, and\n16.00% in four visuo-motor control tasks.", "AI": {"tldr": "Anyprefer\u662f\u4e00\u4e2a\u901a\u8fc7\u53cc\u73a9\u5bb6\u9a6c\u5c14\u53ef\u592b\u6e38\u620f\u6846\u67b6\u5408\u6210\u9ad8\u8d28\u91cf\u504f\u597d\u6570\u636e\u7684\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u9f50\u6027\u80fd\u3002", "motivation": "\u4eba\u5de5\u6807\u6ce8\u504f\u597d\u6570\u636e\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u73b0\u6709\u81ea\u5956\u52b1\u65b9\u6cd5\u56e0\u5171\u4eab\u6743\u91cd\u5bfc\u81f4\u504f\u5dee\u653e\u5927\uff0c\u9700\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5408\u4f5c\u5f0f\u53cc\u73a9\u5bb6\u9a6c\u5c14\u53ef\u592b\u6e38\u620f\uff0c\u76ee\u6807\u6a21\u578b\u4e0e\u88c1\u5224\u6a21\u578b\u534f\u4f5c\uff0c\u5f15\u5165\u5916\u90e8\u5de5\u5177\u51cf\u5c11\u5956\u52b1\u504f\u5dee\uff0c\u4f18\u5316\u63d0\u793a\u53cd\u9988\u673a\u5236\u3002", "result": "\u751f\u621058K\u9ad8\u8d28\u91cf\u504f\u597d\u5bf9\u6570\u636e\u96c6Anyprefer-V1\uff0c\u5728\u56db\u5927\u7c7b\u5e94\u752821\u4e2a\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\uff08\u5982\u81ea\u7136\u8bed\u8a00\u751f\u6210\u5e73\u5747\u63d0\u534718.55%\uff09\u3002", "conclusion": "Anyprefer\u6709\u6548\u5408\u6210\u9ad8\u8d28\u91cf\u504f\u597d\u6570\u636e\uff0c\u4e3a\u6a21\u578b\u5bf9\u9f50\u63d0\u4f9b\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.19940", "pdf": "https://arxiv.org/pdf/2504.19940", "abs": "https://arxiv.org/abs/2504.19940", "authors": ["Luigia Costabile", "Gian Marco Orlando", "Valerio La Gatta", "Vincenzo Moscato"], "title": "Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "The growing spread of online misinformation has created an urgent need for\nscalable, reliable fact-checking solutions. Crowdsourced fact-checking - where\nnon-experts evaluate claim veracity - offers a cost-effective alternative to\nexpert verification, despite concerns about variability in quality and bias.\nEncouraged by promising results in certain contexts, major platforms such as X\n(formerly Twitter), Facebook, and Instagram have begun shifting from\ncentralized moderation to decentralized, crowd-based approaches.\n  In parallel, advances in Large Language Models (LLMs) have shown strong\nperformance across core fact-checking tasks, including claim detection and\nevidence evaluation. However, their potential role in crowdsourced workflows\nremains unexplored. This paper investigates whether LLM-powered generative\nagents - autonomous entities that emulate human behavior and decision-making -\ncan meaningfully contribute to fact-checking tasks traditionally reserved for\nhuman crowds. Using the protocol of La Barbera et al. (2024), we simulate\ncrowds of generative agents with diverse demographic and ideological profiles.\nAgents retrieve evidence, assess claims along multiple quality dimensions, and\nissue final veracity judgments.\n  Our results show that agent crowds outperform human crowds in truthfulness\nclassification, exhibit higher internal consistency, and show reduced\nsusceptibility to social and cognitive biases. Compared to humans, agents rely\nmore systematically on informative criteria such as Accuracy, Precision, and\nInformativeness, suggesting a more structured decision-making process. Overall,\nour findings highlight the potential of generative agents as scalable,\nconsistent, and less biased contributors to crowd-based fact-checking systems.", "AI": {"tldr": "LLM\u9a71\u52a8\u7684\u751f\u6210\u4ee3\u7406\u5728\u4f17\u5305\u4e8b\u5b9e\u6838\u67e5\u4e2d\u8868\u73b0\u4f18\u4e8e\u4eba\u7c7b\u7fa4\u4f53\uff0c\u5177\u6709\u66f4\u4f4e\u7684\u504f\u89c1\u548c\u66f4\u9ad8\u7684\u5185\u90e8\u4e00\u81f4\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u7ebf\u9519\u8bef\u4fe1\u606f\u7684\u4f20\u64ad\u95ee\u9898\uff0c\u63a2\u7d22LLM\u5728\u4f17\u5305\u4e8b\u5b9e\u6838\u67e5\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u3002", "method": "\u6a21\u62df\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u548c\u610f\u8bc6\u5f62\u6001\u80cc\u666f\u7684\u751f\u6210\u4ee3\u7406\u7fa4\u4f53\uff0c\u8fdb\u884c\u8bc1\u636e\u68c0\u7d22\u3001\u58f0\u660e\u8bc4\u4f30\u548c\u771f\u76f8\u5224\u65ad\u3002", "result": "\u4ee3\u7406\u7fa4\u4f53\u5728\u771f\u76f8\u5206\u7c7b\u4e0a\u4f18\u4e8e\u4eba\u7c7b\u7fa4\u4f53\uff0c\u5185\u90e8\u4e00\u81f4\u6027\u66f4\u9ad8\uff0c\u5bf9\u504f\u89c1\u66f4\u4e0d\u654f\u611f\u3002", "conclusion": "\u751f\u6210\u4ee3\u7406\u53ef\u4f5c\u4e3a\u4f17\u5305\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u4e2d\u53ef\u6269\u5c55\u3001\u4e00\u81f4\u4e14\u504f\u89c1\u8f83\u5c11\u7684\u8d21\u732e\u8005\u3002"}}
{"id": "2504.18658", "pdf": "https://arxiv.org/pdf/2504.18658", "abs": "https://arxiv.org/abs/2504.18658", "authors": ["Siddharth Singh", "Mahua Singh", "Abhinav Bhatele"], "title": "The Big Send-off: High Performance Collectives on GPU-based Supercomputers", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": null, "summary": "We evaluate the current state of collective communication on GPU-based\nsupercomputers for large language model (LLM) training at scale. Existing\nlibraries such as RCCL and Cray-MPICH exhibit critical limitations on systems\nsuch as Frontier -- Cray-MPICH underutilizes network and compute resources,\nwhile RCCL suffers from severe scalability issues. To address these challenges,\nwe introduce PCCL, a communication library with highly optimized\nimplementations of all-gather and reduce-scatter operations tailored for\ndistributed deep learning workloads. PCCL is designed to maximally utilize all\navailable network and compute resources and to scale efficiently to thousands\nof GPUs. It achieves substantial performance improvements, delivering 6-33x\nspeedups over RCCL and 28-70x over Cray-MPICH for all-gather on 2048 GCDs of\nFrontier. These gains translate directly to end-to-end performance: in\nlarge-scale GPT-3-style training, PCCL provides up to 60% and 40% speedups over\nRCCL for 7B and 13B parameter models, respectively.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPCCL\u901a\u4fe1\u5e93\uff0c\u9488\u5bf9GPU\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4f18\u5316\u96c6\u4f53\u901a\u4fe1\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8eRCCL\u548cCray-MPICH\u3002", "motivation": "\u73b0\u6709\u5e93\u5982RCCL\u548cCray-MPICH\u5728\u7cfb\u7edf\u8d44\u6e90\u5229\u7528\u548c\u6269\u5c55\u6027\u4e0a\u5b58\u5728\u4e25\u91cd\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0PCCL\u5e93\uff0c\u9488\u5bf9all-gather\u548creduce-scatter\u64cd\u4f5c\u8fdb\u884c\u6df1\u5ea6\u4f18\u5316\uff0c\u6700\u5927\u5316\u7f51\u7edc\u548c\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u7387\u3002", "result": "PCCL\u57282048 GCDs\u7cfb\u7edf\u4e0a\u6bd4RCCL\u5feb6-33\u500d\uff0c\u6bd4Cray-MPICH\u5feb28-70\u500d\uff1b\u5728GPT-3\u8bad\u7ec3\u4e2d\u63d0\u901f60%\u548c40%\u3002", "conclusion": "PCCL\u663e\u8457\u63d0\u5347\u5927\u89c4\u6a21LLM\u8bad\u7ec3\u6548\u7387\uff0c\u662f\u5206\u5e03\u5f0f\u6df1\u5ea6\u5b66\u4e60\u901a\u4fe1\u7684\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.19284", "pdf": "https://arxiv.org/pdf/2504.19284", "abs": "https://arxiv.org/abs/2504.19284", "authors": ["Angel Mary John", "Aiswarya M. U.", "Jerrin Thomas Panachakel"], "title": "Ethical Challenges of Using Artificial Intelligence in Judiciary", "categories": ["cs.LG"], "comment": "2023 IEEE MetroXRAINE 2023", "summary": "Artificial intelligence (AI) has emerged as a ubiquitous concept in numerous\ndomains, including the legal system. AI has the potential to revolutionize the\nfunctioning of the judiciary and the dispensation of justice. Incorporating AI\ninto the legal system offers the prospect of enhancing decision-making for\njudges, lawyers, and legal professionals, while concurrently providing the\npublic with more streamlined, efficient, and cost-effective services. The\nintegration of AI into the legal landscape offers manifold benefits,\nencompassing tasks such as document review, legal research, contract analysis,\ncase prediction, and decision-making. By automating laborious and error-prone\nprocedures, AI has the capacity to alleviate the burden associated with these\narduous tasks. Consequently, courts around the world have begun embracing AI\ntechnology as a means to enhance the administration of justice. However,\nalongside its potential advantages, the use of AI in the judiciary poses a\nrange of ethical challenges. These ethical quandaries must be duly addressed to\nensure the responsible and equitable deployment of AI systems. This article\ndelineates the principal ethical challenges entailed in employing AI within the\njudiciary and provides recommendations to effectively address these issues.", "AI": {"tldr": "\u6458\u8981\u8ba8\u8bba\u4e86AI\u5728\u6cd5\u5f8b\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u4e0e\u4f26\u7406\u6311\u6218\uff0c\u5f3a\u8c03\u5176\u53ef\u63d0\u5347\u53f8\u6cd5\u6548\u7387\u4f46\u4e5f\u9700\u89e3\u51b3\u4f26\u7406\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u8ba8AI\u5982\u4f55\u6539\u5584\u53f8\u6cd5\u7cfb\u7edf\u7684\u6548\u7387\uff0c\u540c\u65f6\u5206\u6790\u5176\u5e26\u6765\u7684\u4f26\u7406\u6311\u6218\u3002", "method": "\u6587\u7ae0\u901a\u8fc7\u5206\u6790AI\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u5e94\u7528\u53ca\u5176\u6f5c\u5728\u5f71\u54cd\uff0c\u63d0\u51fa\u4f26\u7406\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "AI\u80fd\u81ea\u52a8\u5316\u7e41\u7410\u7684\u6cd5\u5f8b\u4efb\u52a1\uff0c\u4f46\u9700\u5e94\u5bf9\u4f26\u7406\u95ee\u9898\u4ee5\u786e\u4fdd\u8d1f\u8d23\u4efb\u7684\u4f7f\u7528\u3002", "conclusion": "AI\u5728\u6cd5\u5f8b\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u867d\u6709\u4f18\u52bf\uff0c\u4f46\u5fc5\u987b\u89e3\u51b3\u4f26\u7406\u95ee\u9898\u4ee5\u5b9e\u73b0\u516c\u5e73\u548c\u6709\u6548\u7684\u53f8\u6cd5\u7ba1\u7406\u3002"}}
{"id": "2504.19982", "pdf": "https://arxiv.org/pdf/2504.19982", "abs": "https://arxiv.org/abs/2504.19982", "authors": ["Emre Can Acikgoz", "Carl Guo", "Suvodip Dey", "Akul Datta", "Takyoung Kim", "Gokhan Tur", "Dilek Hakkani-T\u00fcr"], "title": "TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining Turn-Level Precision with Dialogue-Level Comparisons", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Task-oriented dialogue (TOD) systems are experiencing a revolution driven by\nLarge Language Models (LLMs), yet the evaluation methodologies for these\nsystems remain insufficient for their growing sophistication. While traditional\nautomatic metrics effectively assessed earlier modular systems, they focus\nsolely on the dialogue level and cannot detect critical intermediate errors\nthat can arise during user-agent interactions. In this paper, we introduce\nTD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework\nthat unifies fine-grained turn-level analysis with holistic dialogue-level\ncomparisons. At turn level, we evaluate each response along three TOD-specific\ndimensions: conversation cohesion, backend knowledge consistency, and policy\ncompliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons\nto provide a measure of dialogue-level quality. Through experiments on MultiWOZ\n2.4 and {\\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the\nconversational errors that conventional metrics miss. Furthermore, TD-EVAL\nexhibits better alignment with human judgments than traditional and LLM-based\nmetrics. These findings demonstrate that TD-EVAL introduces a new paradigm for\nTOD system evaluation, efficiently assessing both turn and system levels with a\nplug-and-play framework for future research.", "AI": {"tldr": "TD-EVAL\u662f\u4e00\u4e2a\u65b0\u7684\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8f6e\u6b21\u8bc4\u4f30\u548c\u6574\u4f53\u5bf9\u8bdd\u6bd4\u8f83\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9a71\u52a8\uff0c\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u4ea4\u4e92\u4e2d\u7684\u5173\u952e\u4e2d\u95f4\u9519\u8bef\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faTD-EVAL\u6846\u67b6\uff0c\u5305\u62ec\u4e24\u4e2a\u6b65\u9aa4\uff1a\u8f6e\u6b21\u7ea7\u8bc4\u4f30\uff08\u6db5\u76d6\u5bf9\u8bdd\u8fde\u8d2f\u6027\u3001\u540e\u7aef\u77e5\u8bc6\u4e00\u81f4\u6027\u548c\u7b56\u7565\u5408\u89c4\u6027\uff09\u548c\u5bf9\u8bdd\u7ea7\u8bc4\u4f30\uff08\u901a\u8fc7TOD Agent Arena\u8fdb\u884c\u4e24\u4e24\u6bd4\u8f83\uff09\u3002", "result": "\u5728MultiWOZ 2.4\u548c{\\tau}-Bench\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTD-EVAL\u80fd\u6709\u6548\u8bc6\u522b\u4f20\u7edf\u6307\u6807\u5ffd\u7565\u7684\u5bf9\u8bdd\u9519\u8bef\uff0c\u5e76\u6bd4\u4f20\u7edf\u548cLLM\u6307\u6807\u66f4\u7b26\u5408\u4eba\u7c7b\u5224\u65ad\u3002", "conclusion": "TD-EVAL\u4e3a\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u8bc4\u4f30\u5f15\u5165\u4e86\u65b0\u8303\u5f0f\uff0c\u9ad8\u6548\u7ed3\u5408\u8f6e\u6b21\u548c\u7cfb\u7edf\u5c42\u9762\u8bc4\u4f30\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5373\u63d2\u5373\u7528\u7684\u6846\u67b6\u3002"}}
{"id": "2504.18662", "pdf": "https://arxiv.org/pdf/2504.18662", "abs": "https://arxiv.org/abs/2504.18662", "authors": ["Daniel Sliwowski", "Dongheui Lee"], "title": "M2R2: MulitModal Robotic Representation for Temporal Action Segmentation", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, 6 figures, 2 tables", "summary": "Temporal action segmentation (TAS) has long been a key area of research in\nboth robotics and computer vision. In robotics, algorithms have primarily\nfocused on leveraging proprioceptive information to determine skill boundaries,\nwith recent approaches in surgical robotics incorporating vision. In contrast,\ncomputer vision typically relies on exteroceptive sensors, such as cameras.\nExisting multimodal TAS models in robotics integrate feature fusion within the\nmodel, making it difficult to reuse learned features across different models.\nMeanwhile, pretrained vision-only feature extractors commonly used in computer\nvision struggle in scenarios with limited object visibility. In this work, we\naddress these challenges by proposing M2R2, a multimodal feature extractor\ntailored for TAS, which combines information from both proprioceptive and\nexteroceptive sensors. We introduce a novel pretraining strategy that enables\nthe reuse of learned features across multiple TAS models. Our method achieves\nstate-of-the-art performance on the REASSEMBLE dataset, a challenging\nmultimodal robotic assembly dataset, outperforming existing robotic action\nsegmentation models by 46.6%. Additionally, we conduct an extensive ablation\nstudy to evaluate the contribution of different modalities in robotic TAS\ntasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86M2R2\uff0c\u4e00\u79cd\u4e13\u4e3a\u65f6\u5e8f\u52a8\u4f5c\u5206\u5272\uff08TAS\uff09\u8bbe\u8ba1\u7684\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u901a\u8fc7\u7ed3\u5408\u672c\u4f53\u611f\u89c9\u548c\u5916\u611f\u89c9\u4f20\u611f\u5668\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7279\u5f81\u590d\u7528\u548c\u5bf9\u8c61\u53ef\u89c1\u6027\u53d7\u9650\u573a\u666f\u4e0b\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4ebaTAS\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u6a21\u6001\uff08\u5982\u672c\u4f53\u611f\u89c9\uff09\uff0c\u800c\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\u591a\u4f9d\u8d56\u5916\u611f\u89c9\u4f20\u611f\u5668\uff08\u5982\u6444\u50cf\u5934\uff09\u3002\u4e24\u79cd\u65b9\u6cd5\u5728\u7279\u5f81\u590d\u7528\u548c\u5bf9\u8c61\u53ef\u89c1\u6027\u53d7\u9650\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u7ed3\u5408\u4e24\u8005\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faM2R2\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u7ed3\u5408\u672c\u4f53\u611f\u89c9\u548c\u5916\u611f\u89c9\u4f20\u611f\u5668\u4fe1\u606f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u7279\u5f81\u8de8\u6a21\u578b\u590d\u7528\u3002", "result": "\u5728REASSEMBLE\u6570\u636e\u96c6\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\uff0c\u8d85\u8d8a\u73b0\u6709\u673a\u5668\u4eba\u52a8\u4f5c\u5206\u5272\u6a21\u578b46.6%\u3002\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u7684\u8d21\u732e\u3002", "conclusion": "M2R2\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u7279\u5f81\u590d\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86TAS\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u4e0b\u7684\u673a\u5668\u4eba\u4efb\u52a1\u3002"}}
{"id": "2504.19353", "pdf": "https://arxiv.org/pdf/2504.19353", "abs": "https://arxiv.org/abs/2504.19353", "authors": ["Weitao Du", "Shuning Chang", "Jiasheng Tang", "Yu Rong", "Fan Wang", "Shengchao Liu"], "title": "Flow Along the K-Amplitude for Generative Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In this work, we propose a novel generative learning paradigm, K-Flow, an\nalgorithm that flows along the $K$-amplitude. Here, $k$ is a scaling parameter\nthat organizes frequency bands (or projected coefficients), and amplitude\ndescribes the norm of such projected coefficients. By incorporating the\n$K$-amplitude decomposition, K-Flow enables flow matching across the scaling\nparameter as time. We discuss three venues and six properties of K-Flow, from\ntheoretical foundations, energy and temporal dynamics, and practical\napplications, respectively. Specifically, from the practical usage perspective,\nK-Flow allows steerable generation by controlling the information at different\nscales. To demonstrate the effectiveness of K-Flow, we conduct experiments on\nunconditional image generation, class-conditional image generation, and\nmolecule assembly generation. Additionally, we conduct three ablation studies\nto demonstrate how K-Flow steers scaling parameter to effectively control the\nresolution of image generation.", "AI": {"tldr": "K-Flow\u662f\u4e00\u79cd\u57fa\u4e8eK-\u632f\u5e45\u5206\u89e3\u7684\u65b0\u578b\u751f\u6210\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u63a7\u5236\u4e0d\u540c\u5c3a\u5ea6\u7684\u4fe1\u606f\u5b9e\u73b0\u53ef\u8c03\u63a7\u751f\u6210\uff0c\u5e76\u5728\u56fe\u50cf\u548c\u5206\u5b50\u751f\u6210\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u63d0\u51faK-Flow\u7b97\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7K-\u632f\u5e45\u5206\u89e3\u5b9e\u73b0\u8de8\u5c3a\u5ea6\u6d41\u5339\u914d\uff0c\u4ece\u800c\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u7075\u6d3b\u548c\u53ef\u63a7\u7684\u751f\u6210\u6548\u679c\u3002", "method": "\u5229\u7528K-\u632f\u5e45\u5206\u89e3\u7ec4\u7ec7\u9891\u5e26\u6216\u6295\u5f71\u7cfb\u6570\uff0c\u901a\u8fc7\u6d41\u5339\u914d\u673a\u5236\u5728\u4e0d\u540c\u5c3a\u5ea6\u95f4\u4f20\u9012\u4fe1\u606f\uff0c\u5e76\u5728\u7406\u8bba\u3001\u80fd\u91cf\u52a8\u6001\u548c\u5b9e\u8df5\u5e94\u7528\u4e2d\u9a8c\u8bc1\u5176\u6027\u8d28\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cK-Flow\u5728\u65e0\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u3001\u7c7b\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u548c\u5206\u5b50\u7ec4\u88c5\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u80fd\u901a\u8fc7\u8c03\u63a7\u5c3a\u5ea6\u53c2\u6570\u63a7\u5236\u751f\u6210\u5206\u8fa8\u7387\u3002", "conclusion": "K-Flow\u901a\u8fc7K-\u632f\u5e45\u5206\u89e3\u5b9e\u73b0\u4e86\u53ef\u63a7\u751f\u6210\uff0c\u5c55\u793a\u4e86\u5728\u591a\u79cd\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u8c03\u63a7\u673a\u5236\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2504.20000", "pdf": "https://arxiv.org/pdf/2504.20000", "abs": "https://arxiv.org/abs/2504.20000", "authors": ["Rishika Sen", "Sujoy Roychowdhury", "Sumit Soman", "H. G. Ranjani", "Srikhetra Mohanty"], "title": "Knowledge Distillation of Domain-adapted LLMs for Question-Answering in Telecom", "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50", "I.2.7"], "comment": "10 pages, 4 figures, 3 tables", "summary": "Knowledge Distillation (KD) is one of the approaches to reduce the size of\nLarge Language Models (LLMs). A LLM with smaller number of model parameters\n(student) is trained to mimic the performance of a LLM of a larger size\n(teacher model) on a specific task. For domain-specific tasks, it is not clear\nif teacher or student model, or both, must be considered for domain adaptation.\nIn this work, we study this problem from perspective of telecom domain\nQuestion-Answering (QA) task. We systematically experiment with Supervised\nFine-tuning (SFT) of teacher only, SFT of student only and SFT of both prior to\nKD. We design experiments to study the impact of vocabulary (same and\ndifferent) and KD algorithms (vanilla KD and Dual Space KD, DSKD) on the\ndistilled model. Multi-faceted evaluation of the distillation using 14\ndifferent metrics (N-gram, embedding and LLM-based metrics) is considered.\nExperimental results show that SFT of teacher improves performance of distilled\nmodel when both models have same vocabulary, irrespective of algorithm and\nmetrics. Overall, SFT of both teacher and student results in better performance\nacross all metrics, although the statistical significance of the same depends\non the vocabulary of the teacher models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u7535\u4fe1\u9886\u57df\u95ee\u7b54\u4efb\u52a1\u4e2d\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u7684\u5e94\u7528\uff0c\u6bd4\u8f83\u4e86\u6559\u5e08\u6a21\u578b\u3001\u5b66\u751f\u6a21\u578b\u6216\u4e24\u8005\u5728\u9886\u57df\u9002\u5e94\u4e2d\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u6559\u5e08\u6a21\u578b\u7684\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u80fd\u63d0\u5347\u84b8\u998f\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u4e24\u8005\u5747\u8fdb\u884cSFT\u65f6\u6548\u679c\u66f4\u4f73\u3002", "motivation": "\u63a2\u8ba8\u5728\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u4e2d\uff0c\u77e5\u8bc6\u84b8\u998f\u65f6\u6559\u5e08\u6a21\u578b\u3001\u5b66\u751f\u6a21\u578b\u6216\u4e24\u8005\u7684\u76d1\u7763\u5fae\u8c03\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u6559\u5e08\u6a21\u578b\u3001\u5b66\u751f\u6a21\u578b\u6216\u4e24\u8005\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u4ee5\u53ca\u4e0d\u540c\u8bcd\u6c47\u8868\u548c\u84b8\u998f\u7b97\u6cd5\u5bf9\u84b8\u998f\u6a21\u578b\u7684\u5f71\u54cd\u3002", "result": "\u6559\u5e08\u6a21\u578b\u7684SFT\u80fd\u63d0\u5347\u84b8\u998f\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u8bcd\u6c47\u8868\u4e00\u81f4\u65f6\uff1b\u4e24\u8005\u5747\u8fdb\u884cSFT\u65f6\u6548\u679c\u6700\u4f73\uff0c\u4f46\u7edf\u8ba1\u663e\u8457\u6027\u53d7\u6559\u5e08\u6a21\u578b\u8bcd\u6c47\u8868\u5f71\u54cd\u3002", "conclusion": "\u5728\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u4e2d\uff0c\u5efa\u8bae\u5bf9\u6559\u5e08\u548c\u5b66\u751f\u6a21\u578b\u5747\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u4ee5\u83b7\u5f97\u6700\u4f73\u84b8\u998f\u6548\u679c\u3002"}}
{"id": "2504.18684", "pdf": "https://arxiv.org/pdf/2504.18684", "abs": "https://arxiv.org/abs/2504.18684", "authors": ["Nader Zantout", "Haochen Zhang", "Pujith Kachana", "Jinkai Qiu", "Ji Zhang", "Wenshan Wang"], "title": "SORT3D: Spatial Object-centric Reasoning Toolbox for Zero-Shot 3D Grounding Using Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "7 pages, 6 figures, submitted to IROS 2025", "summary": "Interpreting object-referential language and grounding objects in 3D with\nspatial relations and attributes is essential for robots operating alongside\nhumans. However, this task is often challenging due to the diversity of scenes,\nlarge number of fine-grained objects, and complex free-form nature of language\nreferences. Furthermore, in the 3D domain, obtaining large amounts of natural\nlanguage training data is difficult. Thus, it is important for methods to learn\nfrom little data and zero-shot generalize to new environments. To address these\nchallenges, we propose SORT3D, an approach that utilizes rich object attributes\nfrom 2D data and merges a heuristics-based spatial reasoning toolbox with the\nability of large language models (LLMs) to perform sequential reasoning.\nImportantly, our method does not require text-to-3D data for training and can\nbe applied zero-shot to unseen environments. We show that SORT3D achieves\nstate-of-the-art performance on complex view-dependent grounding tasks on two\nbenchmarks. We also implement the pipeline to run real-time on an autonomous\nvehicle and demonstrate that our approach can be used for object-goal\nnavigation on previously unseen real-world environments. All source code for\nthe system pipeline is publicly released at https://github.com/nzantout/SORT3D .", "AI": {"tldr": "SORT3D\u5229\u75282D\u6570\u636e\u7684\u4e30\u5bcc\u5bf9\u8c61\u5c5e\u6027\uff0c\u7ed3\u5408\u542f\u53d1\u5f0f\u7a7a\u95f4\u63a8\u7406\u5de5\u5177\u7bb1\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e8f\u5217\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u4e863D\u9886\u57df\u4e2d\u5bf9\u8c61\u6307\u4ee3\u8bed\u8a00\u7406\u89e3\u548c\u7269\u4f53\u5b9a\u4f4d\u7684\u6311\u6218\uff0c\u65e0\u9700\u6587\u672c\u52303D\u6570\u636e\u7684\u8bad\u7ec3\uff0c\u5e76\u5728\u672a\u77e5\u73af\u5883\u4e2d\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u6700\u7ec8\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u573a\u666f\u591a\u6837\u6027\u3001\u7ec6\u7c92\u5ea6\u7269\u4f53\u6570\u91cf\u4f17\u591a\u4ee5\u53ca\u8bed\u8a00\u6307\u4ee3\u7684\u81ea\u7531\u5f62\u5f0f\u590d\u6742\u6027\uff0c\u673a\u5668\u4eba\u7406\u89e3\u5bf9\u8c61\u76f8\u5173\u8bed\u8a00\u5e76\u57283D\u7a7a\u95f4\u4e2d\u5b9a\u4f4d\u7269\u4f53\u5177\u6709\u6311\u6218\u6027\u30023D\u9886\u57df\u4e2d\u81ea\u7136\u8bed\u8a00\u8bad\u7ec3\u6570\u636e\u96be\u4ee5\u5927\u91cf\u83b7\u53d6\uff0c\u56e0\u6b64\u65b9\u6cd5\u9700\u5177\u5907\u5c0f\u6837\u672c\u5b66\u4e60\u53ca\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faSORT3D\u65b9\u6cd5\uff0c\u7ed3\u54082D\u6570\u636e\u7684\u5bf9\u8c61\u5c5e\u6027\u3001\u542f\u53d1\u5f0f\u7a7a\u95f4\u63a8\u7406\u5de5\u5177\u7bb1\u548cLLMs\u7684\u5e8f\u5217\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u6587\u672c\u52303D\u8bad\u7ec3\u6570\u636e\uff0c\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u672a\u77e5\u73af\u5883\u3002", "result": "\u5728\u590d\u6742\u89c6\u89d2\u4f9d\u8d56\u7684\u5b9a\u4f4d\u4efb\u52a1\u4e0a\uff0cSORT3D\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u5e76\u5728\u771f\u5b9e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0a\u5b9e\u65f6\u8fd0\u884c\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u672a\u77e5\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u7269\u4f53\u5bfc\u822a\u80fd\u529b\u3002", "conclusion": "SORT3D\u901a\u8fc7\u878d\u5408\u591a\u6a21\u6001\u80fd\u529b\u5b9e\u73b0\u4e86\u5c0f\u6837\u672c\u548c\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u4e3a\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7406\u89e3\u8bed\u8a00\u5e76\u5b9a\u4f4d\u7269\u4f53\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.19374", "pdf": "https://arxiv.org/pdf/2504.19374", "abs": "https://arxiv.org/abs/2504.19374", "authors": ["Suping Xu", "Chuyi Dai", "Lin Shang", "Changbin Shao", "Xibei Yang", "Witold Pedrycz"], "title": "Rethinking Label-specific Features for Label Distribution Learning", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "11 Pages, 5 figures", "summary": "Label distribution learning (LDL) is an emerging learning paradigm designed\nto capture the relative importance of labels for each instance. Label-specific\nfeatures (LSFs), constructed by LIFT, have proven effective for learning tasks\nwith label ambiguity by leveraging clustering-based prototypes for each label\nto re-characterize instances. However, directly introducing LIFT into LDL tasks\ncan be suboptimal, as the prototypes it collects primarily reflect\nintra-cluster relationships while neglecting interactions among distinct\nclusters. Additionally, constructing LSFs using multi-perspective information,\nrather than relying solely on Euclidean distance, provides a more robust and\ncomprehensive representation of instances, mitigating noise and bias that may\narise from a single distance perspective. To address these limitations, we\nintroduce Structural Anchor Points (SAPs) to capture inter-cluster\ninteractions. This leads to a novel LSFs construction strategy, LIFT-SAP, which\nenhances LIFT by integrating both distance and direction information of each\ninstance relative to SAPs. Furthermore, we propose a novel LDL algorithm, Label\nDistribution Learning via Label-specifIc FeaTure with SAPs (LDL-LIFT-SAP),\nwhich unifies multiple label description degrees predicted from different LSF\nspaces into a cohesive label distribution. Extensive experiments on 15\nreal-world datasets demonstrate the effectiveness of LIFT-SAP over LIFT, as\nwell as the superiority of LDL-LIFT-SAP compared to seven other\nwell-established algorithms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6807\u7b7e\u5206\u5e03\u5b66\u4e60\uff08LDL\uff09\u65b9\u6cd5LDL-LIFT-SAP\uff0c\u901a\u8fc7\u5f15\u5165\u7ed3\u6784\u951a\u70b9\uff08SAPs\uff09\u6355\u6349\u7c07\u95f4\u4ea4\u4e92\uff0c\u4f18\u5316\u4e86\u6807\u7b7e\u7279\u5b9a\u7279\u5f81\uff08LSFs\uff09\u7684\u6784\u5efa\uff0c\u5e76\u572815\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLIFT\u7684\u6807\u7b7e\u7279\u5b9a\u7279\u5f81\uff08LSFs\uff09\u6784\u5efa\u65b9\u6cd5\u5728\u6807\u7b7e\u5206\u5e03\u5b66\u4e60\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e3b\u8981\u4f9d\u8d56\u6b27\u6c0f\u8ddd\u79bb\u4e14\u5ffd\u7565\u7c07\u95f4\u4ea4\u4e92\u3002\u4e3a\u4e86\u66f4\u5168\u9762\u5730\u8868\u5f81\u5b9e\u4f8b\u5e76\u51cf\u5c11\u566a\u58f0\u548c\u504f\u5dee\uff0c\u9700\u8981\u4e00\u79cd\u878d\u5408\u591a\u89c6\u89d2\u4fe1\u606f\u548c\u7c07\u95f4\u5173\u7cfb\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLIFT-SAP\u7b56\u7565\uff0c\u5229\u7528\u7ed3\u6784\u951a\u70b9\uff08SAPs\uff09\u6574\u5408\u8ddd\u79bb\u548c\u65b9\u5411\u4fe1\u606f\u6539\u8fdbLSFs\u6784\u5efa\uff1b\u8fdb\u4e00\u6b65\u63d0\u51faLDL-LIFT-SAP\u7b97\u6cd5\uff0c\u7edf\u4e00\u4e0d\u540cLSF\u7a7a\u95f4\u7684\u591a\u6807\u7b7e\u63cf\u8ff0\u5ea6\u3002", "result": "\u572815\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLIFT-SAP\u4f18\u4e8eLIFT\uff0cLDL-LIFT-SAP\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e867\u79cd\u73b0\u6709\u7b97\u6cd5\u3002", "conclusion": "LDL-LIFT-SAP\u901a\u8fc7\u7ed3\u5408\u7c07\u95f4\u4ea4\u4e92\u548c\u591a\u89c6\u89d2\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6807\u7b7e\u5206\u5e03\u5b66\u4e60\u7684\u8868\u73b0\uff0c\u4e3a\u590d\u6742\u6807\u7b7e\u5173\u7cfb\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.20013", "pdf": "https://arxiv.org/pdf/2504.20013", "abs": "https://arxiv.org/abs/2504.20013", "authors": ["Beizhe Hu", "Qiang Sheng", "Juan Cao", "Yang Li", "Danding Wang"], "title": "LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation", "categories": ["cs.CL", "cs.CY", "cs.IR"], "comment": "ACM SIGIR 2025 Full Paper", "summary": "Online fake news moderation now faces a new challenge brought by the\nmalicious use of large language models (LLMs) in fake news production. Though\nexisting works have shown LLM-generated fake news is hard to detect from an\nindividual aspect, it remains underexplored how its large-scale release will\nimpact the news ecosystem. In this study, we develop a simulation pipeline and\na dataset with ~56k generated news of diverse types to investigate the effects\nof LLM-generated fake news within neural news recommendation systems. Our\nfindings expose a truth decay phenomenon, where real news is gradually losing\nits advantageous position in news ranking against fake news as LLM-generated\nnews is involved in news recommendation. We further provide an explanation\nabout why truth decay occurs from a familiarity perspective and show the\npositive correlation between perplexity and news ranking. Finally, we discuss\nthe threats of LLM-generated fake news and provide possible countermeasures. We\nurge stakeholders to address this emerging challenge to preserve the integrity\nof news ecosystems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6a21\u62df\u548c\u6570\u636e\u96c6\u5206\u6790\uff0c\u63ed\u793a\u4e86LLM\u751f\u6210\u7684\u5047\u65b0\u95fb\u5728\u795e\u7ecf\u65b0\u95fb\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5f71\u54cd\uff0c\u5c55\u793a\u4e86\u2018\u771f\u76f8\u8870\u51cf\u2019\u73b0\u8c61\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u6210\u56e0\u53ca\u5e94\u5bf9\u63aa\u65bd\u3002", "motivation": "\u7814\u7a76\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u88ab\u6076\u610f\u7528\u4e8e\u5047\u65b0\u95fb\u751f\u4ea7\u7684\u65b0\u6311\u6218\uff0c\u63a2\u8ba8\u5176\u5bf9\u65b0\u95fb\u751f\u6001\u7cfb\u7edf\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u62df\u7ba1\u9053\u548c\u5305\u542b\u7ea656k\u6761\u591a\u6837\u5316\u751f\u6210\u65b0\u95fb\u7684\u6570\u636e\u96c6\uff0c\u5206\u6790LLM\u751f\u6210\u5047\u65b0\u95fb\u5728\u795e\u7ecf\u65b0\u95fb\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6548\u679c\u3002", "result": "\u53d1\u73b0\u4e86\u2018\u771f\u76f8\u8870\u51cf\u2019\u73b0\u8c61\uff0c\u5373\u968f\u7740LLM\u751f\u6210\u65b0\u95fb\u7684\u53c2\u4e0e\uff0c\u771f\u5b9e\u65b0\u95fb\u5728\u65b0\u95fb\u6392\u540d\u4e2d\u9010\u6e10\u5931\u53bb\u4f18\u52bf\u3002\u540c\u65f6\u63ed\u793a\u4e86\u56f0\u60d1\u5ea6\u4e0e\u65b0\u95fb\u6392\u540d\u4e4b\u95f4\u7684\u6b63\u76f8\u5173\u6027\u3002", "conclusion": "\u7814\u7a76\u547c\u5401\u76f8\u5173\u65b9\u91c7\u53d6\u63aa\u65bd\u5e94\u5bf9\u8fd9\u4e00\u65b0\u5174\u6311\u6218\uff0c\u4ee5\u7ef4\u62a4\u65b0\u95fb\u751f\u6001\u7cfb\u7edf\u7684\u5b8c\u6574\u6027\u3002"}}
{"id": "2504.18689", "pdf": "https://arxiv.org/pdf/2504.18689", "abs": "https://arxiv.org/abs/2504.18689", "authors": ["Apoorva Beedu", "Irfan Essa"], "title": "HierSum: A Global and Local Attention Mechanism for Video Summarization", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Video summarization creates an abridged version (i.e., a summary) that\nprovides a quick overview of the video while retaining pertinent information.\nIn this work, we focus on summarizing instructional videos and propose a method\nfor breaking down a video into meaningful segments, each corresponding to\nessential steps in the video. We propose \\textbf{HierSum}, a hierarchical\napproach that integrates fine-grained local cues from subtitles with global\ncontextual information provided by video-level instructions. Our approach\nutilizes the ``most replayed\" statistic as a supervisory signal to identify\ncritical segments, thereby improving the effectiveness of the summary. We\nevaluate on benchmark datasets such as TVSum, BLiSS, Mr.HiSum, and the WikiHow\ntest set, and show that HierSum consistently outperforms existing methods in\nkey metrics such as F1-score and rank correlation. We also curate a new\nmulti-modal dataset using WikiHow and EHow videos and associated articles\ncontaining step-by-step instructions. Through extensive ablation studies, we\ndemonstrate that training on this dataset significantly enhances summarization\non the target datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHierSum\u7684\u5206\u5c42\u65b9\u6cd5\uff0c\u7528\u4e8e\u603b\u7ed3\u6559\u5b66\u89c6\u9891\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5b57\u5e55\u7684\u7ec6\u7c92\u5ea6\u5c40\u90e8\u7ebf\u7d22\u548c\u89c6\u9891\u7ea7\u6307\u4ee4\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5229\u7528\u201c\u6700\u591a\u56de\u653e\u201d\u7edf\u8ba1\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\u8bc6\u522b\u5173\u952e\u7247\u6bb5\u3002\u5b9e\u9a8c\u8868\u660e\uff0cHierSum\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u65b0\u6784\u5efa\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6548\u679c\u3002", "motivation": "\u6559\u5b66\u89c6\u9891\u901a\u5e38\u5305\u542b\u591a\u4e2a\u5173\u952e\u6b65\u9aa4\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u603b\u7ed3\u65f6\u5f80\u5f80\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u8fd9\u4e9b\u6b65\u9aa4\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\uff0c\u4ee5\u53ca\u5229\u7528\u7528\u6237\u884c\u4e3a\u6570\u636e\uff08\u5982\u201c\u6700\u591a\u56de\u653e\u201d\uff09\uff0c\u63d0\u5347\u89c6\u9891\u603b\u7ed3\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e86HierSum\u5206\u5c42\u65b9\u6cd5\uff0c\u6574\u5408\u5b57\u5e55\u7684\u5c40\u90e8\u7ebf\u7d22\u548c\u89c6\u9891\u6307\u4ee4\u7684\u5168\u5c40\u4fe1\u606f\uff0c\u5e76\u4ee5\u201c\u6700\u591a\u56de\u653e\u201d\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\u3002\u5728TVSum\u3001BLiSS\u7b49\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6548\u679c\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\u7528\u4e8e\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08TVSum\u3001BLiSS\u7b49\uff09\u4e0a\uff0cHierSum\u5728F1-score\u548c\u6392\u540d\u76f8\u5173\u6027\u7b49\u5173\u952e\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u65b0\u6570\u636e\u96c6\u7684\u4f7f\u7528\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u603b\u7ed3\u6548\u679c\u3002", "conclusion": "HierSum\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\u4ee5\u53ca\u7528\u6237\u884c\u4e3a\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6559\u5b66\u89c6\u9891\u603b\u7ed3\u7684\u6548\u679c\u3002\u65b0\u6784\u5efa\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e5f\u6709\u52a9\u4e8e\u6a21\u578b\u6027\u80fd\u7684\u63d0\u5347\u3002"}}
{"id": "2504.19375", "pdf": "https://arxiv.org/pdf/2504.19375", "abs": "https://arxiv.org/abs/2504.19375", "authors": ["Siddharth Chandak"], "title": "$O(1/k)$ Finite-Time Bound for Non-Linear Two-Time-Scale Stochastic Approximation", "categories": ["cs.LG", "cs.SY", "eess.SY", "math.OC", "stat.ML"], "comment": "Submitted to IEEE Transactions on Automatic Control", "summary": "Two-time-scale stochastic approximation is an algorithm with coupled\niterations which has found broad applications in reinforcement learning,\noptimization and game control. While several prior works have obtained a mean\nsquare error bound of $O(1/k)$ for linear two-time-scale iterations, the best\nknown bound in the non-linear contractive setting has been $O(1/k^{2/3})$. In\nthis work, we obtain an improved bound of $O(1/k)$ for non-linear\ntwo-time-scale stochastic approximation. Our result applies to algorithms such\nas gradient descent-ascent and two-time-scale Lagrangian optimization. The key\nstep in our analysis involves rewriting the original iteration in terms of an\naveraged noise sequence which decays sufficiently fast. Additionally, we use an\ninduction-based approach to show that the iterates are bounded in expectation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6539\u8fdb\u4e86\u975e\u7ebf\u6027\u53cc\u65f6\u95f4\u5c3a\u5ea6\u968f\u673a\u903c\u8fd1\u7b97\u6cd5\u7684\u6536\u655b\u901f\u5ea6\uff0c\u4ece\u5df2\u77e5\u7684$O(1/k^{2/3})$\u63d0\u5347\u81f3$O(1/k)$\uff0c\u9002\u7528\u4e8e\u68af\u5ea6\u4e0b\u964d-\u4e0a\u5347\u548c\u53cc\u65f6\u95f4\u5c3a\u5ea6\u62c9\u683c\u6717\u65e5\u4f18\u5316\u7b49\u7b97\u6cd5\u3002", "motivation": "\u53cc\u65f6\u95f4\u5c3a\u5ea6\u968f\u673a\u903c\u8fd1\u7b97\u6cd5\u5728\u5f3a\u5316\u5b66\u4e60\u3001\u4f18\u5316\u548c\u535a\u5f08\u63a7\u5236\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u975e\u7ebf\u6027\u6536\u655b\u901f\u5ea6\u957f\u671f\u53d7\u9650\uff0c\u9700\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "method": "\u901a\u8fc7\u91cd\u5199\u8fed\u4ee3\u5f0f\u4e3a\u5e73\u5747\u566a\u58f0\u5e8f\u5217\u5f62\u5f0f\uff08\u8870\u51cf\u8f83\u5feb\uff09\uff0c\u5e76\u7ed3\u5408\u5f52\u7eb3\u6cd5\u8bc1\u660e\u8fed\u4ee3\u7684\u671f\u671b\u6709\u754c\u6027\u3002", "result": "\u5728\u975e\u7ebf\u6027\u6536\u7f29\u8bbe\u5b9a\u4e0b\uff0c\u5b9e\u73b0\u4e86$O(1/k)$\u7684\u5747\u65b9\u8bef\u5dee\u754c\uff0c\u4f18\u4e8e\u4e4b\u524d\u7684$O(1/k^{2/3})$\u7ed3\u679c\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u663e\u8457\u63d0\u5347\u4e86\u975e\u7ebf\u6027\u53cc\u65f6\u95f4\u5c3a\u5ea6\u7b97\u6cd5\u7684\u6536\u655b\u6027\u80fd\uff0c\u4e3a\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u969c\u3002"}}
{"id": "2504.20022", "pdf": "https://arxiv.org/pdf/2504.20022", "abs": "https://arxiv.org/abs/2504.20022", "authors": ["Pritika Rohera", "Chaitrali Ginimav", "Gayatri Sawant", "Raviraj Joshi"], "title": "Better To Ask in English? Evaluating Factual Accuracy of Multilingual LLMs in English and Low-Resource Languages", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multilingual Large Language Models (LLMs) have demonstrated significant\neffectiveness across various languages, particularly in high-resource languages\nsuch as English. However, their performance in terms of factual accuracy across\nother low-resource languages, especially Indic languages, remains an area of\ninvestigation. In this study, we assess the factual accuracy of LLMs - GPT-4o,\nGemma-2-9B, Gemma-2-2B, and Llama-3.1-8B - by comparing their performance in\nEnglish and Indic languages using the IndicQuest dataset, which contains\nquestion-answer pairs in English and 19 Indic languages. By asking the same\nquestions in English and their respective Indic translations, we analyze\nwhether the models are more reliable for regional context questions in Indic\nlanguages or when operating in English. Our findings reveal that LLMs often\nperform better in English, even for questions rooted in Indic contexts.\nNotably, we observe a higher tendency for hallucination in responses generated\nin low-resource Indic languages, highlighting challenges in the multilingual\nunderstanding capabilities of current LLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u82f1\u8bed\u548c\u5370\u5ea6\u8bed\u8a00\u4e2d\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u82f1\u8bed\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u5373\u4f7f\u5728\u5370\u5ea6\u8bed\u5883\u95ee\u9898\u4e2d\u4e5f\u662f\u5982\u6b64\uff0c\u4e14\u5728\u4f4e\u8d44\u6e90\u5370\u5ea6\u8bed\u8a00\u4e2d\u66f4\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\u3002", "motivation": "\u63a2\u7d22\u591a\u8bed\u8a00LLMs\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u5370\u5ea6\u8bed\u8a00\uff09\u4e2d\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u8868\u73b0\uff0c\u5e76\u4e0e\u9ad8\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u82f1\u8bed\uff09\u8fdb\u884c\u5bf9\u6bd4\u3002", "method": "\u4f7f\u7528IndicQuest\u6570\u636e\u96c6\uff0c\u6bd4\u8f83GPT-4o\u3001Gemma-2-9B\u3001Gemma-2-2B\u548cLlama-3.1-8B\u5728\u82f1\u8bed\u548c19\u79cd\u5370\u5ea6\u8bed\u8a00\u4e2d\u7684\u56de\u7b54\u51c6\u786e\u6027\u3002", "result": "LLMs\u5728\u82f1\u8bed\u4e2d\u8868\u73b0\u66f4\u53ef\u9760\uff0c\u5373\u4f7f\u5728\u5370\u5ea6\u8bed\u5883\u95ee\u9898\u4e2d\uff1b\u4f4e\u8d44\u6e90\u5370\u5ea6\u8bed\u8a00\u4e2d\u7684\u56de\u7b54\u66f4\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\u3002", "conclusion": "\u5f53\u524d\u591a\u8bed\u8a00LLMs\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u7406\u89e3\u548c\u4e8b\u5b9e\u51c6\u786e\u6027\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u533a\u57df\u8bed\u5883\u95ee\u9898\u4e2d\u3002"}}
{"id": "2504.18691", "pdf": "https://arxiv.org/pdf/2504.18691", "abs": "https://arxiv.org/abs/2504.18691", "authors": ["Ali Alfageeh", "Sadegh AlMahdi Kazemi Zarkouei", "Daye Nam", "Daniel Prol", "Matin Amoozadeh", "Souti Chattopadhyay", "James Prather", "Paul Denny", "Juho Leinonen", "Michael Hilton", "Sruti Srinivasa Ragavan", "Mohammad Amin Alipour"], "title": "From Prompts to Propositions: A Logic-Based Lens on Student-LLM Interactions", "categories": ["cs.HC", "cs.AI", "cs.SE"], "comment": null, "summary": "Background and Context. The increasing integration of large language models\n(LLMs) in computing education presents an emerging challenge in understanding\nhow students use LLMs and craft prompts to solve computational tasks. Prior\nresearch has used both qualitative and quantitative methods to analyze\nprompting behavior, but these approaches lack scalability or fail to\neffectively capture the semantic evolution of prompts. Objective. In this\npaper, we investigate whether students prompts can be systematically analyzed\nusing propositional logic constraints. We examine whether this approach can\nidentify patterns in prompt evolution, detect struggling students, and provide\ninsights into effective and ineffective strategies. Method. We introduce\nPrompt2Constraints, a novel method that translates students prompts into\nlogical constraints. The constraints are able to represent the intent of the\nprompts in succinct and quantifiable ways. We used this approach to analyze a\ndataset of 1,872 prompts from 203 students solving introductory programming\ntasks. Findings. We find that while successful and unsuccessful attempts tend\nto use a similar number of constraints overall, when students fail, they often\nmodify their prompts more significantly, shifting problem-solving strategies\nmidway. We also identify points where specific interventions could be most\nhelpful to students for refining their prompts. Implications. This work offers\na new and scalable way to detect students who struggle in solving natural\nlanguage programming tasks. This work could be extended to investigate more\ncomplex tasks and integrated into programming tools to provide real-time\nsupport.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPrompt2Constraints\u7684\u65b0\u65b9\u6cd5\uff0c\u5c06\u5b66\u751f\u7684\u63d0\u793a\u8f6c\u5316\u4e3a\u903b\u8f91\u7ea6\u675f\uff0c\u7528\u4ee5\u5206\u6790\u5b66\u751f\u5982\u4f55\u5229\u7528LLM\u89e3\u51b3\u7f16\u7a0b\u4efb\u52a1\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6210\u529f\u548c\u5931\u8d25\u7684\u5c1d\u8bd5\u5728\u7ea6\u675f\u6570\u91cf\u4e0a\u76f8\u4f3c\uff0c\u4f46\u5931\u8d25\u65f6\u5b66\u751f\u66f4\u591a\u4f1a\u663e\u8457\u4fee\u6539\u63d0\u793a\uff0c\u4e2d\u9014\u6539\u53d8\u7b56\u7565\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8ba1\u7b97\u6559\u80b2\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e86\u89e3\u5b66\u751f\u5982\u4f55\u5229\u7528LLM\u4ee5\u53ca\u5982\u4f55\u7f16\u5199\u63d0\u793a\u4ee5\u89e3\u51b3\u8ba1\u7b97\u4efb\u52a1\u6210\u4e3a\u4e00\u4e2a\u91cd\u8981\u6311\u6218\u3002\u73b0\u6709\u7684\u5b9a\u6027\u6216\u5b9a\u91cf\u65b9\u6cd5\u7f3a\u4e4f\u6269\u5c55\u6027\u6216\u672a\u80fd\u6709\u6548\u6355\u6349\u63d0\u793a\u7684\u8bed\u4e49\u6f14\u53d8\u3002", "method": "\u8be5\u65b9\u6cd5\u540d\u4e3aPrompt2Constraints\uff0c\u5c06\u5b66\u751f\u7684\u63d0\u793a\u7ffb\u8bd1\u4e3a\u903b\u8f91\u7ea6\u675f\uff0c\u8fd9\u4e9b\u7ea6\u675f\u80fd\u4ee5\u7b80\u6d01\u4e14\u53ef\u91cf\u5316\u7684\u65b9\u5f0f\u8868\u793a\u63d0\u793a\u7684\u610f\u56fe\u3002\u7814\u7a76\u5206\u6790\u4e86\u6765\u81ea203\u540d\u5b66\u751f\u76841,872\u4e2a\u63d0\u793a\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6210\u529f\u4e0e\u5931\u8d25\u7684\u5c1d\u8bd5\u5728\u7ea6\u675f\u6570\u91cf\u4e0a\u76f8\u4f3c\uff0c\u4f46\u5931\u8d25\u65f6\uff0c\u5b66\u751f\u66f4\u591a\u4f1a\u5927\u5e45\u4fee\u6539\u63d0\u793a\u5e76\u5207\u6362\u7b56\u7565\u3002\u7814\u7a76\u8fd8\u8bc6\u522b\u4e86\u53ef\u4ee5\u5e2e\u52a9\u5b66\u751f\u6539\u8fdb\u63d0\u793a\u7684\u5173\u952e\u5e72\u9884\u70b9\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u6269\u5c55\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u5728\u81ea\u7136\u8bed\u8a00\u7f16\u7a0b\u4efb\u52a1\u4e2d\u9047\u5230\u56f0\u96be\u7684\u5b66\u751f\uff0c\u5e76\u53ef\u4e3a\u7f16\u7a0b\u5de5\u5177\u63d0\u4f9b\u5b9e\u65f6\u652f\u6301\u3002"}}
{"id": "2504.19382", "pdf": "https://arxiv.org/pdf/2504.19382", "abs": "https://arxiv.org/abs/2504.19382", "authors": ["Jonathan Gornet", "Yiannis Kantaros", "Bruno Sinopoli"], "title": "HyperController: A Hyperparameter Controller for Fast and Stable Training of Reinforcement Learning Neural Networks", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "We introduce Hyperparameter Controller (HyperController), a computationally\nefficient algorithm for hyperparameter optimization during training of\nreinforcement learning neural networks. HyperController optimizes\nhyperparameters quickly while also maintaining improvement of the reinforcement\nlearning neural network, resulting in faster training and deployment. It\nachieves this by modeling the hyperparameter optimization problem as an unknown\nLinear Gaussian Dynamical System, which is a system with a state that linearly\nchanges. It then learns an efficient representation of the hyperparameter\nobjective function using the Kalman filter, which is the optimal one-step\npredictor for a Linear Gaussian Dynamical System. To demonstrate the\nperformance of HyperController, it is applied as a hyperparameter optimizer\nduring training of reinforcement learning neural networks on a variety of\nOpenAI Gymnasium environments. In four out of the five Gymnasium environments,\nHyperController achieves highest median reward during evaluation compared to\nother algorithms. The results exhibit the potential of HyperController for\nefficient and stable training of reinforcement learning neural networks.", "AI": {"tldr": "HyperController\u662f\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u8d85\u53c2\u6570\u4f18\u5316\u7b97\u6cd5\uff0c\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\uff0c\u901a\u8fc7\u5c06\u5176\u5efa\u6a21\u4e3a\u7ebf\u6027\u9ad8\u65af\u52a8\u6001\u7cfb\u7edf\u5e76\u4f7f\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u5b66\u4e60\u9ad8\u6548\u8868\u793a\uff0c\u5b9e\u73b0\u5feb\u901f\u4f18\u5316\u548c\u7a33\u5b9a\u8bad\u7ec3\u3002\u5728\u591a\u6570\u6d4b\u8bd5\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\uff0c\u8d85\u53c2\u6570\u4f18\u5316\u5e38\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4e0d\u7a33\u5b9a\uff0c\u5f71\u54cd\u8bad\u7ec3\u6548\u7387\u548c\u90e8\u7f72\u901f\u5ea6\u3002\u56e0\u6b64\uff0c\u9700\u4e00\u79cd\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u8d85\u53c2\u6570\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u5c06\u8d85\u53c2\u6570\u4f18\u5316\u95ee\u9898\u5efa\u6a21\u4e3a\u7ebf\u6027\u9ad8\u65af\u52a8\u6001\u7cfb\u7edf\uff0c\u4f7f\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u5b66\u4e60\u8d85\u53c2\u6570\u76ee\u6807\u51fd\u6570\u7684\u9ad8\u6548\u8868\u793a\u3002", "result": "\u5728\u4e94\u4e2aOpenAI Gymnasium\u73af\u5883\u4e2d\u7684\u56db\u4e2a\uff0cHyperController\u7684\u4e2d\u4f4d\u5956\u52b1\u6700\u9ad8\uff0c\u4f18\u4e8e\u5176\u4ed6\u7b97\u6cd5\u3002", "conclusion": "HyperController\u5c55\u793a\u4e86\u5728\u5f3a\u5316\u5b66\u4e60\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u5b9e\u73b0\u9ad8\u6548\u548c\u7a33\u5b9a\u8d85\u53c2\u6570\u4f18\u5316\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.20039", "pdf": "https://arxiv.org/pdf/2504.20039", "abs": "https://arxiv.org/abs/2504.20039", "authors": ["Roman Garipov", "Fedor Velikonivtsev", "Ruslan Svirschevski", "Vage Egiazarian", "Max Ryabinin"], "title": "AutoJudge: Judge Decoding Without Manual Annotation", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint, Work in progress", "summary": "We introduce AutoJudge, a framework that accelerates large language model\n(LLM) inference with task-specific lossy speculative decoding. Instead of\nmatching the original model output distribution token-by-token, we identify\nwhich of the generated tokens affect the downstream quality of the generated\nresponse, relaxing the guarantee so that the \"unimportant\" tokens can be\ngenerated faster. Our approach relies on a semi-greedy search algorithm to test\nwhich of the mismatches between target and draft model should be corrected to\npreserve quality, and which ones may be skipped. We then train a lightweight\nclassifier based on existing LLM embeddings to predict, at inference time,\nwhich mismatching tokens can be safely accepted without compromising the final\nanswer quality. We test our approach with Llama 3.2 1B (draft) and Llama 3.1 8B\n(target) models on zero-shot GSM8K reasoning, where it achieves up to 1.5x more\naccepted tokens per verification cycle with under 1% degradation in answer\naccuracy compared to standard speculative decoding and over 2x with small loss\nin accuracy. When applied to the LiveCodeBench benchmark, our approach\nautomatically detects other, programming-specific important tokens and shows\nsimilar speedups, demonstrating its ability to generalize across tasks.", "AI": {"tldr": "AutoJudge\u6846\u67b6\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u7684\u6709\u635f\u63a8\u6d4b\u89e3\u7801\u52a0\u901f\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u5229\u7528\u534a\u8d2a\u5a6a\u7b97\u6cd5\u548c\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u9884\u6d4b\u54ea\u4e9b\u4ee4\u724c\u53ef\u8df3\u8fc7\u800c\u4e0d\u5f71\u54cd\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u5347\u901f\u5ea6\u4e14\u7cbe\u5ea6\u635f\u5931\u5c0f\u3002", "motivation": "\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6548\u7387\uff0c\u901a\u8fc7\u8bc6\u522b\u4e0d\u5f71\u54cd\u4e0b\u6e38\u8d28\u91cf\u7684\u4ee4\u724c\u6765\u52a0\u901f\u751f\u6210\u8fc7\u7a0b\u3002", "method": "\u4f7f\u7528\u534a\u8d2a\u5a6a\u641c\u7d22\u7b97\u6cd5\u786e\u5b9a\u9700\u7ea0\u6b63\u7684\u4ee4\u724c\uff0c\u5e76\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u9884\u6d4b\u53ef\u8df3\u8fc7\u7684\u4ee4\u724c\u3002", "result": "\u5728GSM8K\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u901f\u5ea6\u63d0\u53471.5\u500d\uff0c\u7cbe\u5ea6\u635f\u5931\u4f4e\u4e8e1%\uff1b\u5728LiveCodeBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u7c7b\u4f3c\u3002", "conclusion": "AutoJudge\u80fd\u8de8\u4efb\u52a1\u901a\u7528\uff0c\u663e\u8457\u52a0\u901f\u63a8\u7406\u4e14\u4fdd\u6301\u9ad8\u8d28\u91cf\u8f93\u51fa\u3002"}}
{"id": "2504.18693", "pdf": "https://arxiv.org/pdf/2504.18693", "abs": "https://arxiv.org/abs/2504.18693", "authors": ["Sina Gogani-Khiabani", "Varsha Dewangan", "Nina Olson", "Ashutosh Trivedi", "Saeid Tizpaz-Niari"], "title": "Technical Challenges in Maintaining Tax Prep Software with Large Language Models", "categories": ["cs.SE", "cs.AI", "https://www.irs.gov/statistics/fourteenth-annual-irs-tpc-joint-research-conference-on-tax-administration"], "comment": "Accepted to 14th Annual IRS/TPC Joint Research Conference on Tax\n  Administration (IRS-TPC 2024)", "summary": "As the US tax law evolves to adapt to ever-changing politico-economic\nrealities, tax preparation software plays a significant role in helping\ntaxpayers navigate these complexities. The dynamic nature of tax regulations\nposes a significant challenge to accurately and timely maintaining tax software\nartifacts. The state-of-the-art in maintaining tax prep software is\ntime-consuming and error-prone as it involves manual code analysis combined\nwith an expert interpretation of tax law amendments. We posit that the rigor\nand formality of tax amendment language, as expressed in IRS publications,\nmakes it amenable to automatic translation to executable specifications (code).\nOur research efforts focus on identifying, understanding, and tackling\ntechnical challenges in leveraging Large Language Models (LLMs), such as\nChatGPT and Llama, to faithfully extract code differentials from IRS\npublications and automatically integrate them with the prior version of the\ncode to automate tax prep software maintenance.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982ChatGPT\u548cLlama\uff09\u4eceIRS\u51fa\u7248\u7269\u4e2d\u81ea\u52a8\u63d0\u53d6\u4ee3\u7801\u5dee\u5f02\u5e76\u6574\u5408\u5230\u73b0\u6709\u7a0e\u52a1\u51c6\u5907\u8f6f\u4ef6\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u81ea\u52a8\u5316\u7ef4\u62a4\u7684\u6280\u672f\u6311\u6218\u548c\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u7f8e\u56fd\u7a0e\u6cd5\u7684\u9891\u7e41\u53d8\u52a8\uff0c\u7a0e\u52a1\u51c6\u5907\u8f6f\u4ef6\u7684\u7ef4\u62a4\u53d8\u5f97\u8017\u65f6\u4e14\u6613\u9519\uff0c\u9700\u8981\u7ed3\u5408\u4eba\u5de5\u4ee3\u7801\u5206\u6790\u548c\u7a0e\u6cd5\u4e13\u5bb6\u89e3\u8bfb\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7814\u7a76\u805a\u7126\u4e8e\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4eceIRS\u51fa\u7248\u7269\u4e2d\u51c6\u786e\u63d0\u53d6\u4ee3\u7801\u53d8\u66f4\uff0c\u5e76\u81ea\u52a8\u5c06\u5176\u6574\u5408\u5230\u73b0\u6709\u8f6f\u4ef6\u4ee3\u7801\u4e2d\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u901a\u8fc7LLMs\u5b9e\u73b0\u7a0e\u6cd5\u53d8\u66f4\u5230\u4ee3\u7801\u7684\u81ea\u52a8\u7ffb\u8bd1\u548c\u6574\u5408\u7684\u53ef\u884c\u6027\uff0c\u4f46\u672a\u5177\u4f53\u8bf4\u660e\u5b9e\u9a8c\u7ed3\u679c\u3002", "conclusion": "\u81ea\u52a8\u5316\u7a0e\u52a1\u8f6f\u4ef6\u7ef4\u62a4\u662f\u53ef\u884c\u7684\uff0c\u4e14LLMs\u5728\u8fd9\u4e00\u8fc7\u7a0b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u6280\u672f\u6311\u6218\u3002"}}
{"id": "2504.19391", "pdf": "https://arxiv.org/pdf/2504.19391", "abs": "https://arxiv.org/abs/2504.19391", "authors": ["David Warren", "Mark Dras"], "title": "Bi-directional Model Cascading with Proxy Confidence", "categories": ["cs.LG"], "comment": null, "summary": "Model Cascading, recently applied successfully to LLMs, is a simple but\npowerful technique that improves the efficiency of inference by selectively\napplying models of varying sizes. Models are used in sequence from smallest to\nlargest, only deferring samples to large, costly models when smaller models are\nnot sufficiently confident. Existing approaches to deferral use only limited\nsmall model confidence estimates because of the inaccessibility of the large\nmodel, although large model confidence is known to be important. We therefore\npropose a bi-directional approach to deferral that considers the confidence of\nsmall and large models in the cascade simultaneously through the use of a proxy\nfor the large model. This requires a richer representation of model confidence\nto enable comparative calibration: we use an analysis of hidden states to\nimprove post-invocation confidence of the small model, which in itself improves\ncascading results over prior approaches. We then combine this with a tiny proxy\nmodel to estimate pre-invocation confidence of the large model. We examine the\nproposed cascading system over challenging, multiple-choice datasets, finding\nimprovements over standard cascading baselines reflected in reductions in\ndeferrals to more costly models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5411\u5ef6\u8fdf\u65b9\u6cd5\uff0c\u901a\u8fc7\u540c\u65f6\u8003\u8651\u7ea7\u8054\u4e2d\u5927\u5c0f\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\uff0c\u5229\u7528\u4ee3\u7406\u6a21\u578b\u6539\u8fdb\u6a21\u578b\u7ea7\u8054\u6548\u7387\uff0c\u51cf\u5c11\u5bf9\u9ad8\u6210\u672c\u5927\u6a21\u578b\u7684\u8c03\u7528\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u7ea7\u8054\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u5c0f\u6a21\u578b\u7684\u6709\u9650\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5927\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u53cc\u5411\u5ef6\u8fdf\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u9690\u85cf\u72b6\u6001\u6539\u8fdb\u5c0f\u6a21\u578b\u7684\u540e\u8c03\u7528\u7f6e\u4fe1\u5ea6\uff0c\u5e76\u7ed3\u5408\u5fae\u578b\u4ee3\u7406\u6a21\u578b\u9884\u4f30\u5927\u6a21\u578b\u7684\u9884\u8c03\u7528\u7f6e\u4fe1\u5ea6\u3002", "result": "\u5728\u591a\u9879\u9009\u62e9\u9898\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u8f83\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u5bf9\u9ad8\u6210\u672c\u5927\u6a21\u578b\u7684\u8c03\u7528\u3002", "conclusion": "\u53cc\u5411\u5ef6\u8fdf\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u5229\u7528\u5927\u5c0f\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u4fe1\u606f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u7ea7\u8054\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2504.19396", "pdf": "https://arxiv.org/pdf/2504.19396", "abs": "https://arxiv.org/abs/2504.19396", "authors": ["Shuo Wu", "Pawan Poojary", "Randall Berry"], "title": "Observational Learning with a Budget", "categories": ["cs.LG", "cs.SI"], "comment": "Submitted to ISIT 2025 Conference, 11 pages, 4 figures", "summary": "We consider a model of Bayesian observational learning in which a sequence of\nagents receives a private signal about an underlying binary state of the world.\nEach agent makes a decision based on its own signal and its observations of\nprevious agents. A central planner seeks to improve the accuracy of these\nsignals by allocating a limited budget to enhance signal quality across agents.\nWe formulate and analyze the budget allocation problem and propose two optimal\nallocation strategies. At least one of these strategies is shown to maximize\nthe probability of achieving a correct information cascade.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8d1d\u53f6\u65af\u89c2\u5bdf\u5b66\u4e60\u6a21\u578b\u4e2d\u9884\u7b97\u5206\u914d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u6700\u4f18\u5206\u914d\u7b56\u7565\u4ee5\u63d0\u5347\u4fe1\u53f7\u8d28\u91cf\uff0c\u4ece\u800c\u63d0\u9ad8\u6b63\u786e\u4fe1\u606f\u7ea7\u8054\u7684\u6982\u7387\u3002", "motivation": "\u5728\u8d1d\u53f6\u65af\u89c2\u5bdf\u5b66\u4e60\u6a21\u578b\u4e2d\uff0c\u4ee3\u7406\u4eba\u901a\u8fc7\u79c1\u6709\u4fe1\u53f7\u548c\u89c2\u5bdf\u524d\u4eba\u7684\u884c\u4e3a\u505a\u51b3\u7b56\u3002\u4e2d\u592e\u89c4\u5212\u8005\u5e0c\u671b\u901a\u8fc7\u6709\u9650\u7684\u9884\u7b97\u63d0\u5347\u4fe1\u53f7\u8d28\u91cf\uff0c\u4ee5\u63d0\u9ad8\u51b3\u7b56\u51c6\u786e\u6027\uff0c\u800c\u6700\u4f18\u9884\u7b97\u5206\u914d\u7b56\u7565\u7684\u7f3a\u4e4f\u662f\u7814\u7a76\u52a8\u673a\u3002", "method": "\u8bba\u6587\u6784\u5efa\u4e86\u9884\u7b97\u5206\u914d\u95ee\u9898\u7684\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u6700\u4f18\u5206\u914d\u7b56\u7565\uff0c\u901a\u8fc7\u6570\u5b66\u6a21\u578b\u548c\u5206\u6790\u9a8c\u8bc1\u8fd9\u4e9b\u7b56\u7565\u7684\u6548\u679c\u3002", "result": "\u81f3\u5c11\u5176\u4e2d\u4e00\u79cd\u7b56\u7565\u88ab\u8bc1\u660e\u80fd\u6700\u5927\u5316\u5b9e\u73b0\u6b63\u786e\u4fe1\u606f\u7ea7\u8054\u7684\u6982\u7387\uff0c\u8868\u660e\u9884\u7b97\u5206\u914d\u5bf9\u51b3\u7b56\u51c6\u786e\u6027\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u5408\u7406\u7684\u9884\u7b97\u5206\u914d\u63d0\u5347\u4fe1\u53f7\u8d28\u91cf\u662f\u53ef\u884c\u7684\uff0c\u4e3a\u4fe1\u606f\u7ea7\u8054\u4e2d\u7684\u51b3\u7b56\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2504.18748", "pdf": "https://arxiv.org/pdf/2504.18748", "abs": "https://arxiv.org/abs/2504.18748", "authors": ["Kaustubh D. Dhole", "Nikhita Vedula", "Saar Kuzi", "Giuseppe Castellucci", "Eugene Agichtein", "Shervin Malmasi"], "title": "Generative Product Recommendations for Implicit Superlative Queries", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "In Recommender Systems, users often seek the best products through indirect,\nvague, or under-specified queries, such as \"best shoes for trail running\". Such\nqueries, also referred to as implicit superlative queries, pose a significant\nchallenge for standard retrieval and ranking systems as they lack an explicit\nmention of attributes and require identifying and reasoning over complex\nfactors. We investigate how Large Language Models (LLMs) can generate implicit\nattributes for ranking as well as reason over them to improve product\nrecommendations for such queries. As a first step, we propose a novel\nfour-point schema for annotating the best product candidates for superlative\nqueries called SUPERB, paired with LLM-based product annotations. We then\nempirically evaluate several existing retrieval and ranking approaches on our\nnew dataset, providing insights and discussing their integration into\nreal-world e-commerce production systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e3a\u9690\u5f0f\u6700\u9ad8\u7ea7\u67e5\u8be2\uff08\u5982\u201c\u6700\u4f73\u8d8a\u91ce\u8dd1\u978b\u201d\uff09\u751f\u6210\u9690\u5f0f\u5c5e\u6027\u5e76\u63a8\u7406\uff0c\u4ee5\u6539\u8fdb\u4ea7\u54c1\u63a8\u8350\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aSUPERB\u7684\u56db\u70b9\u6807\u6ce8\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u591a\u79cd\u68c0\u7d22\u4e0e\u6392\u5e8f\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u7528\u6237\u901a\u8fc7\u95f4\u63a5\u3001\u6a21\u7cca\u6216\u672a\u660e\u786e\u6307\u5b9a\u7684\u67e5\u8be2\uff08\u5982\u201c\u6700\u4f73\u8d8a\u91ce\u8dd1\u978b\u201d\uff09\u5bfb\u627e\u4ea7\u54c1\u65f6\uff0c\u6807\u51c6\u68c0\u7d22\u4e0e\u6392\u5e8f\u7cfb\u7edf\u56e0\u7f3a\u5c11\u660e\u786e\u5c5e\u6027\u800c\u9762\u4e34\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86SUPERB\u56db\u70b9\u6807\u6ce8\u6846\u67b6\uff0c\u7ed3\u5408LLM\u751f\u6210\u4ea7\u54c1\u6807\u6ce8\uff0c\u5e76\u5b9e\u8bc1\u8bc4\u4f30\u4e86\u591a\u79cd\u73b0\u6709\u68c0\u7d22\u4e0e\u6392\u5e8f\u65b9\u6cd5\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u9a8c\u8bc1\u4e86LLM\u5728\u751f\u6210\u9690\u5f0f\u5c5e\u6027\u548c\u63a8\u7406\u4e0a\u7684\u6709\u6548\u6027\uff0c\u4e3a\u7535\u5546\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "conclusion": "LLM\u80fd\u591f\u6709\u6548\u5904\u7406\u9690\u5f0f\u6700\u9ad8\u7ea7\u67e5\u8be2\uff0cSUPERB\u6846\u67b6\u548c\u5b9e\u9a8c\u7ed3\u679c\u4e3a\u5b9e\u9645\u7535\u5546\u7cfb\u7edf\u7684\u6574\u5408\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2504.18722", "pdf": "https://arxiv.org/pdf/2504.18722", "abs": "https://arxiv.org/abs/2504.18722", "authors": ["Aashutosh Nema", "Samaksh Gulati", "Evangelos Giakoumakis", "Bipana Thapaliya"], "title": "MODP: Multi Objective Directional Prompting", "categories": ["cs.CC", "cs.AI", "I.2.0; I.2.6; I.2.7; H.3.3"], "comment": "10 pages, 5 figures, submission to KDD 2025", "summary": "Recent advances in large language models (LLMs) have led to their popularity\nacross multiple use-cases. However, prompt engineering, the process for\noptimally utilizing such models, remains approximation-driven and subjective.\nMost of the current research on prompt engineering focuses on task-specific\noptimization, while neglecting the behavior of the LLM under consideration\nduring prompt development. This paper introduces MODP -- Multi Objective\nDirectional Prompting, a framework based on two key concepts: 1)\nmulti-objectivity: the importance of considering an LLM's intrinsic behavior as\nan additional objective in prompt development, and 2) directional prompting: a\nmetrics-driven method for prompt engineering to ensure development of robust\nand high-precision prompts. We demonstrate the effectiveness of our proposed\nideas on a summarization task, using a synthetically created dataset, achieving\na 26% performance gain over initial prompts. Finally, we apply MODP to develop\nprompts for Dell's Next Best Action support tool, which is now in production\nand is used by more than 10,000 internal support agents and serving millions of\ncustomers worldwide.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MODP\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u6027\u548c\u5b9a\u5411\u63d0\u793a\u6539\u8fdb\u63d0\u793a\u5de5\u7a0b\uff0c\u63d0\u5347LLM\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u63d0\u793a\u5de5\u7a0b\u591a\u4e13\u6ce8\u4e8e\u4efb\u52a1\u4f18\u5316\uff0c\u5ffd\u89c6\u4e86LLM\u7684\u5185\u5728\u884c\u4e3a\uff0c\u5bfc\u81f4\u65b9\u6cd5\u4e3b\u89c2\u4e14\u4e0d\u7cbe\u786e\u3002", "method": "\u91c7\u7528\u591a\u76ee\u6807\u6027\uff08\u8003\u8651LLM\u884c\u4e3a\uff09\u548c\u5b9a\u5411\u63d0\u793a\uff08\u57fa\u4e8e\u6307\u6807\u7684\u65b9\u6cd5\uff09\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u9a8c\u8bc1\u6846\u67b6\u3002", "result": "\u5728\u6458\u8981\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u63d0\u534726%\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8eDell\u7684\u751f\u4ea7\u5de5\u5177\u3002", "conclusion": "MODP\u6846\u67b6\u663e\u8457\u63d0\u5347\u63d0\u793a\u5de5\u7a0b\u6548\u679c\uff0c\u5177\u5907\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.19408", "pdf": "https://arxiv.org/pdf/2504.19408", "abs": "https://arxiv.org/abs/2504.19408", "authors": ["Maitreya Sonawane", "Sumit Mamtani"], "title": "UNet with Axial Transformer : A Neural Weather Model for Precipitation Nowcasting", "categories": ["cs.LG", "cs.CV", "eess.SP"], "comment": null, "summary": "Making accurate weather predictions can be particularly challenging for\nlocalized storms or events that evolve on hourly timescales, such as\nthunderstorms. Hence, our goal for the project was to model Weather Nowcasting\nfor making highly localized and accurate predictions that apply to the\nimmediate future replacing the current numerical weather models and data\nassimilation systems with Deep Learning approaches. A significant advantage of\nmachine learning is that inference is computationally cheap given an\nalready-trained model, allowing forecasts that are nearly instantaneous and in\nthe native high resolution of the input data. In this work we developed a novel\nmethod that employs Transformer-based machine learning models to forecast\nprecipitation. This approach works by leveraging axial attention mechanisms to\nlearn complex patterns and dynamics from time series frames. Moreover, it is a\ngeneric framework and can be applied to univariate and multivariate time series\ndata, as well as time series embeddings data. This paper represents an initial\nresearch on the dataset used in the domain of next frame prediciton, and hence,\nwe demonstrate state-of-the-art results in terms of metrices (PSNR = 47.67,\nSSIM = 0.9943) used for the given dataset using UNet with Axial Transformer.", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u7cbe\u51c6\u9884\u6d4b\u5c40\u90e8\u5929\u6c14\uff08\u5982\u96f7\u66b4\uff09\uff0c\u66ff\u4ee3\u4f20\u7edf\u6570\u503c\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u7684\u5373\u65f6\u9884\u62a5\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u5929\u6c14\u6a21\u578b\u96be\u4ee5\u5e94\u5bf9\u5c40\u90e8\u5feb\u901f\u53d8\u5316\u7684\u5929\u6c14\u73b0\u8c61\uff08\u5982\u96f7\u66b4\uff09\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63a8\u7406\u6210\u672c\u4f4e\u4e14\u7cbe\u5ea6\u9ad8\uff0c\u9002\u5408\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8f74\u5411\u6ce8\u610f\u529b\u673a\u5236\u7684Transformer\u6a21\u578b\uff0c\u4ece\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u5b66\u4e60\u590d\u6742\u6a21\u5f0f\uff0c\u9002\u7528\u4e8e\u5355\u53d8\u91cf\u3001\u591a\u53d8\u91cf\u53ca\u5d4c\u5165\u6570\u636e\u3002", "result": "\u5728\u6570\u636e\u96c6\u7684\u4e0b\u4e00\u5e27\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528UNet\u4e0e\u8f74\u5411Transformer\u7684\u7ec4\u5408\u53d6\u5f97\u4e86PSNR=47.67\u3001SSIM=0.9943\u7684\u9886\u5148\u6027\u80fd\u3002", "conclusion": "Transformer\u6a21\u578b\u4e3a\u5929\u6c14\u4e34\u8fd1\u9884\u62a5\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u7cbe\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u53ef\u63a8\u5e7f\u81f3\u66f4\u5e7f\u6cdb\u7684\u65f6\u5e8f\u6570\u636e\u4efb\u52a1\u3002"}}
{"id": "2504.18919", "pdf": "https://arxiv.org/pdf/2504.18919", "abs": "https://arxiv.org/abs/2504.18919", "authors": ["Andrew M. Bean", "Rebecca Payne", "Guy Parsons", "Hannah Rose Kirk", "Juan Ciro", "Rafael Mosquera", "Sara Hincapi\u00e9 Monsalve", "Aruna S. Ekanayaka", "Lionel Tarassenko", "Luc Rocher", "Adam Mahdi"], "title": "Clinical knowledge in LLMs does not translate to human interactions", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "52 pages, 4 figures", "summary": "Global healthcare providers are exploring use of large language models (LLMs)\nto provide medical advice to the public. LLMs now achieve nearly perfect scores\non medical licensing exams, but this does not necessarily translate to accurate\nperformance in real-world settings. We tested if LLMs can assist members of the\npublic in identifying underlying conditions and choosing a course of action\n(disposition) in ten medical scenarios in a controlled study with 1,298\nparticipants. Participants were randomly assigned to receive assistance from an\nLLM (GPT-4o, Llama 3, Command R+) or a source of their choice (control). Tested\nalone, LLMs complete the scenarios accurately, correctly identifying conditions\nin 94.9% of cases and disposition in 56.3% on average. However, participants\nusing the same LLMs identified relevant conditions in less than 34.5% of cases\nand disposition in less than 44.2%, both no better than the control group. We\nidentify user interactions as a challenge to the deployment of LLMs for medical\nadvice. Standard benchmarks for medical knowledge and simulated patient\ninteractions do not predict the failures we find with human participants.\nMoving forward, we recommend systematic human user testing to evaluate\ninteractive capabilities prior to public deployments in healthcare.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u533b\u7597\u5efa\u8bae\u4e2d\u7684\u5b9e\u9645\u8868\u73b0\uff0c\u53d1\u73b0\u5c3d\u7ba1LLM\u5728\u6a21\u62df\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5b9e\u9645\u4f7f\u7528\u4e2d\u5e2e\u52a9\u7528\u6237\u8bc6\u522b\u75c5\u60c5\u548c\u9009\u62e9\u884c\u52a8\u7684\u51c6\u786e\u7387\u8f83\u4f4e\uff0c\u5efa\u8bae\u5728\u516c\u5171\u90e8\u7f72\u524d\u8fdb\u884c\u7cfb\u7edf\u6027\u7528\u6237\u6d4b\u8bd5\u3002", "motivation": "\u9a8c\u8bc1LLM\u5728\u771f\u5b9e\u533b\u7597\u573a\u666f\u4e2d\u5bf9\u516c\u4f17\u7684\u5e2e\u52a9\u6548\u679c\uff0c\u6bd4\u8f83\u5176\u5728\u6a21\u62df\u6d4b\u8bd5\u548c\u5b9e\u9645\u4ea4\u4e92\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u5bf9\u7167\u5b9e\u9a8c\uff0c\u8ba91298\u540d\u53c2\u4e0e\u8005\u968f\u673a\u4f7f\u7528LLM\uff08\u5982GPT-4o\uff09\u6216\u81ea\u884c\u9009\u62e9\u4fe1\u606f\u6765\u6e90\uff0c\u5206\u6790\u5176\u572810\u4e2a\u533b\u7597\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "result": "LLM\u5355\u72ec\u6d4b\u8bd5\u65f6\u8868\u73b0\u4f18\u5f02\uff08\u75c5\u60c5\u8bc6\u522b94.9%\uff0c\u884c\u52a8\u9009\u62e956.3%\uff09\uff0c\u4f46\u5728\u7528\u6237\u8f85\u52a9\u573a\u666f\u4e2d\u6548\u679c\u663e\u8457\u4e0b\u964d\uff08\u75c5\u60c5\u8bc6\u522b<34.5%\uff0c\u884c\u52a8\u9009\u62e9<44.2%\uff09\uff0c\u4e14\u4e0d\u4f18\u4e8e\u5bf9\u7167\u7ec4\u3002", "conclusion": "\u7528\u6237\u4ea4\u4e92\u662fLLM\u533b\u7597\u5e94\u7528\u7684\u4e3b\u8981\u6311\u6218\uff0c\u5efa\u8bae\u90e8\u7f72\u524d\u9700\u7cfb\u7edf\u6027\u6d4b\u8bd5\u4ea4\u4e92\u80fd\u529b\uff0c\u73b0\u6709\u8bc4\u6d4b\u6807\u51c6\u65e0\u6cd5\u9884\u6d4b\u5b9e\u9645\u4f7f\u7528\u95ee\u9898\u3002"}}
{"id": "2504.18727", "pdf": "https://arxiv.org/pdf/2504.18727", "abs": "https://arxiv.org/abs/2504.18727", "authors": ["Ali Rostami", "Z Xie", "A Ishino", "Y Yamakata", "K Aizawa", "Ramesh Jain"], "title": "World Food Atlas Project", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "A coronavirus pandemic is forcing people to be \"at home\" all over the world.\nIn a life of hardly ever going out, we would have realized how the food we eat\naffects our bodies. What can we do to know our food more and control it better?\nTo give us a clue, we are trying to build a World Food Atlas (WFA) that\ncollects all the knowledge about food in the world. In this paper, we present\ntwo of our trials. The first is the Food Knowledge Graph (FKG), which is a\ngraphical representation of knowledge about food and ingredient relationships\nderived from recipes and food nutrition data. The second is the FoodLog Athl\nand the RecipeLog that are applications for collecting people's detailed\nrecords about food habit. We also discuss several problems that we try to solve\nto build the WFA by integrating these two ideas.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u6784\u5efaWorld Food Atlas\uff08WFA\uff09\uff0c\u901a\u8fc7Food Knowledge Graph\uff08FKG\uff09\u548cFoodLog Athl/RecipeLog\u5e94\u7528\u6574\u5408\u5168\u7403\u98df\u7269\u77e5\u8bc6\uff0c\u89e3\u51b3\u75ab\u60c5\u671f\u95f4\u4eba\u4eec\u5bf9\u98df\u7269\u5f71\u54cd\u7684\u8ba4\u77e5\u9700\u6c42\u3002", "motivation": "\u65b0\u51a0\u75ab\u60c5\u671f\u95f4\uff0c\u4eba\u4eec\u5c45\u5bb6\u65f6\u95f4\u589e\u52a0\uff0c\u610f\u8bc6\u5230\u98df\u7269\u5bf9\u8eab\u4f53\u7684\u5f71\u54cd\uff0c\u9700\u8981\u66f4\u597d\u5730\u4e86\u89e3\u548c\u638c\u63a7\u98df\u7269\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a\u4e00\u662fFood Knowledge Graph\uff08FKG\uff09\uff0c\u901a\u8fc7\u98df\u8c31\u548c\u8425\u517b\u6570\u636e\u6784\u5efa\u98df\u7269\u5173\u7cfb\u56fe\uff1b\u4e8c\u662fFoodLog Athl\u548cRecipeLog\u5e94\u7528\uff0c\u8bb0\u5f55\u7528\u6237\u996e\u98df\u4e60\u60ef\u3002", "result": "\u6574\u5408FKG\u548c\u65e5\u5fd7\u5e94\u7528\uff0c\u4e3a\u6784\u5efaWFA\u63d0\u4f9b\u57fa\u7840\uff0c\u5e76\u5c1d\u8bd5\u89e3\u51b3\u76f8\u5173\u95ee\u9898\u3002", "conclusion": "\u901a\u8fc7\u6280\u672f\u548c\u6570\u636e\u6574\u5408\uff0cWFA\u6709\u671b\u5e2e\u52a9\u4eba\u4eec\u66f4\u6df1\u5165\u5730\u4e86\u89e3\u548c\u63a7\u5236\u98df\u7269\u6444\u5165\u3002"}}
{"id": "2504.19419", "pdf": "https://arxiv.org/pdf/2504.19419", "abs": "https://arxiv.org/abs/2504.19419", "authors": ["Zhaiming Shen", "Sung Ha Kang"], "title": "Graph-based Semi-supervised and Unsupervised Methods for Local Clustering", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Local clustering aims to identify specific substructures within a large graph\nwithout requiring full knowledge of the entire graph. These substructures are\ntypically small compared to the overall graph, enabling the problem to be\napproached by finding a sparse solution to a linear system associated with the\ngraph Laplacian. In this work, we first propose a method for identifying\nspecific local clusters when very few labeled data is given, which we term\nsemi-supervised local clustering. We then extend this approach to the\nunsupervised setting when no prior information on labels is available. The\nproposed methods involve randomly sampling the graph, applying diffusion\nthrough local cluster extraction, then examining the overlap among the results\nto find each cluster. We establish the co-membership conditions for any pair of\nnodes and rigorously prove the correctness of our methods. Additionally, we\nconduct extensive experiments to demonstrate that the proposed methods achieve\nstate-of-the-arts results in the low-label rates regime.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u534a\u76d1\u7763\u548c\u65e0\u76d1\u7763\u5c40\u90e8\u805a\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u968f\u673a\u91c7\u6837\u548c\u56fe\u6269\u6563\u6280\u672f\u8bc6\u522b\u5c40\u90e8\u5b50\u7ed3\u6784\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5728\u4f4e\u6807\u7b7e\u7387\u4e0b\u6548\u679c\u4f18\u5f02\u3002", "motivation": "\u5c40\u90e8\u805a\u7c7b\u65e8\u5728\u65e0\u9700\u5168\u5c40\u56fe\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u8bc6\u522b\u7279\u5b9a\u5b50\u7ed3\u6784\uff0c\u4f46\u5728\u6570\u636e\u6807\u7b7e\u6781\u5c11\u6216\u65e0\u6807\u7b7e\u65f6\uff0c\u4f20\u7edf\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002", "method": "\u901a\u8fc7\u968f\u673a\u91c7\u6837\u56fe\u3001\u5c40\u90e8\u7c07\u63d0\u53d6\u6269\u6563\uff0c\u5e76\u5206\u6790\u7ed3\u679c\u91cd\u53e0\u6765\u8bc6\u522b\u7c07\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u8282\u70b9\u5171\u5c5e\u6761\u4ef6\u548c\u7406\u8bba\u8bc1\u660e\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u6240\u63d0\u65b9\u6cd5\u5728\u4f4e\u6807\u7b7e\u7387\u4e0b\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u534a\u76d1\u7763\u548c\u65e0\u76d1\u7763\u5c40\u90e8\u805a\u7c7b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2504.18988", "pdf": "https://arxiv.org/pdf/2504.18988", "abs": "https://arxiv.org/abs/2504.18988", "authors": ["Saramsh Gautam", "Mahmood Jasim"], "title": "LINC: Supporting Language Independent Communication and Comprehension to Enhance Contribution in Multilingual Collaborative Meetings", "categories": ["cs.HC", "cs.CL", "H.5.3"], "comment": "19 pages, 4 figures. Multimodal system design and evaluation study", "summary": "Collaborative research often includes contributors with varied perspectives\nfrom diverse linguistic backgrounds. However, English as a Second Language\n(ESL) researchers often struggle to communicate during meetings in English and\ncomprehend discussions, leading to limited contribution. To investigate these\nchallenges, we surveyed 64 ESL researchers who frequently collaborate in\nmultilingual teams and identified four key design goals around participation,\ncomprehension, documentation, and feedback. Guided by these design goals, we\ndeveloped LINC, a multimodal Language INdependent Collaboration system with two\ncomponents: a real-time module for multilingual communication during meetings\nand a post-meeting dashboard for discussion analysis. We evaluated the system\nthrough a two-phased study with six triads of multilingual teams. We found that\nusing LINC, participants benefited from communicating in their preferred\nlanguage, recalled and reviewed actionable insights, and prepared for upcoming\nmeetings effectively. We discuss external factors that impact multilingual\nmeeting participation beyond language preferences and the implications of\nmultimodal systems in facilitating meetings in hybrid multilingual\ncollaborative settings beyond research.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u82f1\u8bed\u975e\u6bcd\u8bed\u7814\u7a76\u8005\uff08ESL\uff09\u5728\u591a\u8bed\u8a00\u56e2\u961f\u534f\u4f5c\u4e2d\u7684\u6c9f\u901a\u969c\u788d\uff0c\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86LINC\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b9e\u65f6\u591a\u8bed\u8a00\u4ea4\u6d41\u548c\u4f1a\u540e\u5206\u6790\u5de5\u5177\u63d0\u5347\u534f\u4f5c\u6548\u7387\u3002", "motivation": "\u82f1\u8bed\u975e\u6bcd\u8bed\u7814\u7a76\u8005\uff08ESL\uff09\u5728\u591a\u8bed\u8a00\u56e2\u961f\u4f1a\u8bae\u4e2d\u9762\u4e34\u6c9f\u901a\u548c\u7406\u89e3\u56f0\u96be\uff0c\u5bfc\u81f4\u8d21\u732e\u53d7\u9650\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u65e8\u5728\u8bbe\u8ba1\u652f\u6301\u591a\u8bed\u8a00\u534f\u4f5c\u7684\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u8c03\u67e564\u540dESL\u7814\u7a76\u8005\uff0c\u786e\u5b9a\u4e86\u56db\u9879\u8bbe\u8ba1\u76ee\u6807\uff0c\u5e76\u5f00\u53d1\u4e86LINC\u7cfb\u7edf\uff0c\u5305\u542b\u5b9e\u65f6\u591a\u8bed\u8a00\u4ea4\u6d41\u6a21\u5757\u548c\u4f1a\u540e\u5206\u6790\u4eea\u8868\u76d8\u3002\u901a\u8fc7\u516d\u7ec4\u591a\u8bed\u8a00\u56e2\u961f\u7684\u4e24\u9636\u6bb5\u7814\u7a76\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "LINC\u5e2e\u52a9\u53c2\u4e0e\u8005\u7528\u504f\u597d\u8bed\u8a00\u4ea4\u6d41\uff0c\u6709\u6548\u56de\u987e\u4f1a\u8bae\u5185\u5bb9\u5e76\u4e3a\u540e\u7eed\u4f1a\u8bae\u51c6\u5907\u3002\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86\u8bed\u8a00\u504f\u597d\u5916\u7684\u5176\u4ed6\u5f71\u54cd\u591a\u8bed\u8a00\u4f1a\u8bae\u53c2\u4e0e\u7684\u56e0\u7d20\u3002", "conclusion": "\u591a\u6a21\u6001\u7cfb\u7edf\uff08\u5982LINC\uff09\u80fd\u6709\u6548\u652f\u6301\u591a\u8bed\u8a00\u6df7\u5408\u534f\u4f5c\u73af\u5883\uff0c\u4f46\u8fd8\u9700\u8003\u8651\u8bed\u8a00\u504f\u597d\u5916\u7684\u5916\u90e8\u56e0\u7d20\u3002"}}
{"id": "2504.19446", "pdf": "https://arxiv.org/pdf/2504.19446", "abs": "https://arxiv.org/abs/2504.19446", "authors": ["Arnab Bhattacharyya", "Constantinos Daskalakis", "Themis Gouleakis", "Yuhao Wang"], "title": "Learning High-dimensional Gaussians from Censored Data", "categories": ["cs.LG", "cs.CC", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "We provide efficient algorithms for the problem of distribution learning from\nhigh-dimensional Gaussian data where in each sample, some of the variable\nvalues are missing. We suppose that the variables are missing not at random\n(MNAR). The missingness model, denoted by $S(y)$, is the function that maps any\npoint $y$ in $R^d$ to the subsets of its coordinates that are seen. In this\nwork, we assume that it is known. We study the following two settings:\n  (i) Self-censoring: An observation $x$ is generated by first sampling the\ntrue value $y$ from a $d$-dimensional Gaussian $N(\\mu*, \\Sigma*)$ with unknown\n$\\mu*$ and $\\Sigma*$. For each coordinate $i$, there exists a set $S_i$\nsubseteq $R^d$ such that $x_i = y_i$ if and only if $y_i$ in $S_i$. Otherwise,\n$x_i$ is missing and takes a generic value (e.g., \"?\"). We design an algorithm\nthat learns $N(\\mu*, \\Sigma*)$ up to total variation (TV) distance epsilon,\nusing $poly(d, 1/\\epsilon)$ samples, assuming only that each pair of\ncoordinates is observed with sufficiently high probability.\n  (ii) Linear thresholding: An observation $x$ is generated by first sampling\n$y$ from a $d$-dimensional Gaussian $N(\\mu*, \\Sigma)$ with unknown $\\mu*$ and\nknown $\\Sigma$, and then applying the missingness model $S$ where $S(y) = {i in\n[d] : v_i^T y <= b_i}$ for some $v_1, ..., v_d$ in $R^d$ and $b_1, ..., b_d$ in\n$R$. We design an efficient mean estimation algorithm, assuming that none of\nthe possible missingness patterns is very rare conditioned on the values of the\nobserved coordinates and that any small subset of coordinates is observed with\nsufficiently high probability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u9ad8\u6548\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u9ad8\u7ef4\u9ad8\u65af\u6570\u636e\u4e2d\u5b66\u4e60\u5206\u5e03\uff0c\u5047\u8bbe\u6570\u636e\u5b58\u5728\u975e\u968f\u673a\u7f3a\u5931\uff08MNAR\uff09\u3002\u7b2c\u4e00\u79cd\u662f\u81ea\u611f\u77e5\u7f3a\u5931\u6a21\u578b\uff0c\u7b2c\u4e8c\u79cd\u662f\u7ebf\u6027\u9608\u503c\u7f3a\u5931\u6a21\u578b\u3002", "motivation": "\u9ad8\u7ef4\u6570\u636e\u4e2d\u7f3a\u5931\u503c\u5e38\u89c1\u4e14\u5f71\u54cd\u5206\u6790\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u975e\u968f\u673a\u7f3a\u5931\u60c5\u51b5\u3002\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u73b0\u6709\u65b9\u6cd5\u6548\u7387\u4e0d\u8db3\u6216\u5047\u8bbe\u8fc7\u5f3a\u3002", "method": "\u9488\u5bf9\u81ea\u611f\u77e5\u7f3a\u5931\u548c\u7ebf\u6027\u9608\u503c\u7f3a\u5931\u4e24\u79cd\u6a21\u578b\uff0c\u5206\u522b\u8bbe\u8ba1\u591a\u9879\u5f0f\u6837\u672c\u590d\u6742\u5ea6\u7684\u9ad8\u6548\u7b97\u6cd5\u3002\u524d\u8005\u5047\u8bbe\u5750\u6807\u5bf9\u89c2\u6d4b\u6982\u7387\u8db3\u591f\u9ad8\uff0c\u540e\u8005\u8981\u6c42\u7f3a\u5931\u6a21\u5f0f\u4e0d\u7a00\u6709\u4e14\u5c0f\u89c4\u6a21\u5750\u6807\u89c2\u6d4b\u6982\u7387\u9ad8\u3002", "result": "\u5728\u4e24\u79cd\u6a21\u578b\u4e0b\u5747\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\uff1a\u81ea\u611f\u77e5\u7f3a\u5931\u6a21\u578b\u4e0b\u603b\u53d8\u5dee\u8ddd\u79bb\u903c\u8fd1\uff0c\u7ebf\u6027\u9608\u503c\u6a21\u578b\u4e0b\u5747\u503c\u4f30\u8ba1\u6709\u6548\u3002\u6240\u9700\u6837\u672c\u91cf\u4e3a\u591a\u9879\u5f0f\u7ea7\u3002", "conclusion": "\u8bc1\u660e\u4e86\u5728\u975e\u968f\u673a\u7f3a\u5931\u5047\u8bbe\u4e0b\uff0c\u9ad8\u7ef4\u9ad8\u65af\u5206\u5e03\u5b66\u4e60\u53ef\u884c\uff0c\u4e3a\u5b9e\u9645\u6570\u636e\u7f3a\u5931\u95ee\u9898\u63d0\u4f9b\u7406\u8bba\u5de5\u5177\u3002"}}
{"id": "2504.19056", "pdf": "https://arxiv.org/pdf/2504.19056", "abs": "https://arxiv.org/abs/2504.19056", "authors": ["Mohammad Mahdi Abootorabi", "Omid Ghahroodi", "Pardis Sadat Zahraei", "Hossein Behzadasl", "Alireza Mirrokni", "Mobina Salimipanah", "Arash Rasouli", "Bahar Behzadipour", "Sara Azarnoush", "Benyamin Maleki", "Erfan Sadraiye", "Kiarash Kiani Feriz", "Mahdi Teymouri Nahad", "Ali Moghadasi", "Abolfazl Eshagh Abianeh", "Nizi Nazar", "Hamid R. Rabiee", "Mahdieh Soleymani Baghshah", "Meisam Ahmadi", "Ehsaneddin Asgari"], "title": "Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "50 main pages, 30 pages appendix, 21 figures, 8 tables, GitHub\n  Repository:\n  https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey", "summary": "Generative AI is reshaping art, gaming, and most notably animation. Recent\nbreakthroughs in foundation and diffusion models have reduced the time and cost\nof producing animated content. Characters are central animation components,\ninvolving motion, emotions, gestures, and facial expressions. The pace and\nbreadth of advances in recent months make it difficult to maintain a coherent\nview of the field, motivating the need for an integrative review. Unlike\nearlier overviews that treat avatars, gestures, or facial animation in\nisolation, this survey offers a single, comprehensive perspective on all the\nmain generative AI applications for character animation. We begin by examining\nthe state-of-the-art in facial animation, expression rendering, image\nsynthesis, avatar creation, gesture modeling, motion synthesis, object\ngeneration, and texture synthesis. We highlight leading research, practical\ndeployments, commonly used datasets, and emerging trends for each area. To\nsupport newcomers, we also provide a comprehensive background section that\nintroduces foundational models and evaluation metrics, equipping readers with\nthe knowledge needed to enter the field. We discuss open challenges and map\nfuture research directions, providing a roadmap to advance AI-driven\ncharacter-animation technologies. This survey is intended as a resource for\nresearchers and developers entering the field of generative AI animation or\nadjacent fields. Resources are available at:\nhttps://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5173\u4e8e\u751f\u6210\u5f0fAI\u5728\u89d2\u8272\u52a8\u753b\u4e2d\u5e94\u7528\u7684\u5168\u9762\u7efc\u8ff0\uff0c\u6db5\u76d6\u4e86\u9762\u90e8\u52a8\u753b\u3001\u8868\u60c5\u6e32\u67d3\u3001\u624b\u52bf\u5efa\u6a21\u3001\u8fd0\u52a8\u5408\u6210\u7b49\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u3002", "motivation": "\u7531\u4e8e\u751f\u6210\u5f0fAI\u5728\u89d2\u8272\u52a8\u753b\u9886\u57df\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u4e00\u4e2a\u6574\u5408\u6027\u7684\u7efc\u8ff0\u6765\u5e2e\u52a9\u7814\u7a76\u8005\u548c\u5f00\u53d1\u8005\u5168\u9762\u4e86\u89e3\u8be5\u9886\u57df\u7684\u73b0\u72b6\u548c\u8d8b\u52bf\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u7efc\u8ff0\u9762\u90e8\u52a8\u753b\u3001\u8868\u60c5\u6e32\u67d3\u3001\u56fe\u50cf\u5408\u6210\u3001avatar\u521b\u5efa\u3001\u624b\u52bf\u5efa\u6a21\u3001\u8fd0\u52a8\u5408\u6210\u3001\u7269\u4f53\u751f\u6210\u548c\u7eb9\u7406\u5408\u6210\u7b49\u9886\u57df\u7684\u6700\u65b0\u7814\u7a76\u3001\u6570\u636e\u96c6\u548c\u5b9e\u9645\u5e94\u7528\uff0c\u63d0\u4f9b\u4e00\u4e2a\u5168\u9762\u7684\u89c6\u89d2\u3002", "result": "\u8bba\u6587\u603b\u7ed3\u4e86\u5404\u9886\u57df\u7684\u524d\u6cbf\u6280\u672f\u3001\u5e38\u7528\u6570\u636e\u96c6\u548c\u672a\u6765\u8d8b\u52bf\uff0c\u5e76\u63d0\u4f9b\u4e86\u5165\u95e8\u80cc\u666f\u548c\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "\u8fd9\u7bc7\u7efc\u8ff0\u4e3a\u7814\u7a76\u8005\u548c\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u8fdb\u5165\u751f\u6210\u5f0fAI\u52a8\u753b\u9886\u57df\u7684\u8d44\u6e90\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2504.19449", "pdf": "https://arxiv.org/pdf/2504.19449", "abs": "https://arxiv.org/abs/2504.19449", "authors": ["Zhenyu Zhang", "Zechun Liu", "Yuandong Tian", "Harshit Khaitan", "Zhangyang Wang", "Steven Li"], "title": "R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference", "categories": ["cs.LG"], "comment": "ICLR 2025", "summary": "Large Language Models (LLMs), while demonstrating remarkable capabilities\nacross various applications, present significant challenges during inference\ndue to their substantial model size, especially when deployed on edge devices.\nActivation sparsity offers a promising solution to reduce computation and\nmemory movement, enabling more efficient inference, particularly for\nsmall-batch on-device applications. However, current approaches face\nlimitations with non-ReLU activation function, which are foundational to most\nadvanced LLMs, or require heavy continual training. Additionally, the\ndifficulty in predicting active channels and limited achievable sparsity ratios\nconstrain the effectiveness of activation sparsity-based methods. In this\npaper, we introduce R-Sparse, a training-free activation sparsity approach\ncapable of achieving high sparsity levels in advanced LLMs. We conducted two\npreliminary investigations into how different components contribute to the\noutput within a single linear layer and found two key observations: (i) the\nnon-sparse components of the input function can be regarded as a few bias\nterms, and (ii) The full computation can be effectively approximated by an\nappropriate combination of input channels and weight singular values. Building\non this, we replace the linear layers in LLMs with a rank-aware sparse\ninference method that leverages the sparsity of input channels and singular\nvalue components, eliminating the need for active channel prediction like the\noutput sparsity based approaches. Experiments on Llama-2/3 and Mistral models\nacross ten diverse tasks demonstrate that R-Sparse achieves comparable\nperformance at 50% model-level sparsity, resulting in a significant 43%\nend-to-end efficient improvements with customized kernels.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6fc0\u6d3b\u7a00\u758f\u65b9\u6cd5R-Sparse\uff0c\u7528\u4e8e\u5728\u9ad8\u6548\u63a8\u7406\u7684LLMs\uff08\u5982Llama-2/3\u548cMistral\uff09\u4e2d\u5b9e\u73b0\u9ad8\u7a00\u758f\u5ea6\uff0c\u5b9e\u9a8c\u8868\u660e\u572850%\u7684\u6a21\u578b\u7a00\u758f\u5ea6\u4e0b\u6027\u80fd\u76f8\u5f53\uff0c\u4e14\u7aef\u5230\u7aef\u6548\u7387\u63d0\u534743%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63a8\u7406\u65f6\u56e0\u6a21\u578b\u5c3a\u5bf8\u5927\u800c\u9762\u4e34\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\u6311\u6218\uff0c\u5f53\u524d\u6fc0\u6d3b\u7a00\u758f\u65b9\u6cd5\u5bf9\u975eReLU\u6fc0\u6d3b\u51fd\u6570\u6216\u6301\u7eed\u8bad\u7ec3\u9700\u6c42\u9ad8\uff0c\u4e14\u96be\u4ee5\u8fbe\u5230\u9ad8\u7a00\u758f\u5ea6\u3002", "method": "R-Sparse\u901a\u8fc7\u7814\u7a76\u53d1\u73b0\u7ebf\u6027\u5c42\u7684\u8f93\u5165\u975e\u7a00\u758f\u90e8\u5206\u53ef\u89c6\u4e3a\u504f\u7f6e\u9879\uff0c\u4e14\u5176\u8ba1\u7b97\u53ef\u901a\u8fc7\u8f93\u5165\u901a\u9053\u548c\u6743\u91cd\u5947\u5f02\u503c\u7ec4\u5408\u8fd1\u4f3c\uff0c\u4ece\u800c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8f93\u5165\u901a\u9053\u548c\u5947\u5f02\u503c\u7a00\u758f\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u907f\u514d\u9884\u6d4b\u6d3b\u8dc3\u901a\u9053\u7684\u9700\u6c42\u3002", "result": "\u5728Llama-2/3\u548cMistral\u6a21\u578b\u7684\u5341\u9879\u4efb\u52a1\u6d4b\u8bd5\u4e2d\uff0cR-Sparse\u572850%\u6a21\u578b\u7a00\u758f\u5ea6\u4e0b\u6027\u80fd\u76f8\u5f53\uff0c\u7aef\u5230\u7aef\u6548\u7387\u63d0\u534743%\u3002", "conclusion": "R-Sparse\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u6fc0\u6d3b\u7a00\u758f\u65b9\u6cd5\uff0c\u9002\u7528\u4e8eLLM\u63a8\u7406\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.19062", "pdf": "https://arxiv.org/pdf/2504.19062", "abs": "https://arxiv.org/abs/2504.19062", "authors": ["Yu Zhang", "Wenxiang Guo", "Changhao Pan", "Zhiyuan Zhu", "Ruiqi Li", "Jingyu Lu", "Rongjie Huang", "Ruiyuan Zhang", "Zhiqing Hong", "Ziyue Jiang", "Zhou Zhao"], "title": "Versatile Framework for Song Generation with Prompt-based Control", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": null, "summary": "Song generation focuses on producing controllable high-quality songs based on\nvarious prompts. However, existing methods struggle to generate vocals and\naccompaniments with prompt-based control and proper alignment. Additionally,\nthey fall short in supporting various tasks. To address these challenges, we\nintroduce VersBand, a multi-task song generation framework for synthesizing\nhigh-quality, aligned songs with prompt-based control. VersBand comprises these\nprimary models: 1) VocalBand, a decoupled model, leverages the flow-matching\nmethod for generating singing styles, pitches, and mel-spectrograms, allowing\nfast, high-quality vocal generation with style control. 2) AccompBand, a\nflow-based transformer model, incorporates the Band-MOE, selecting suitable\nexperts for enhanced quality, alignment, and control. This model allows for\ngenerating controllable, high-quality accompaniments aligned with vocals. 3)\nTwo generation models, LyricBand for lyrics and MelodyBand for melodies,\ncontribute to the comprehensive multi-task song generation system, allowing for\nextensive control based on multiple prompts. Experimental results demonstrate\nthat VersBand performs better over baseline models across multiple song\ngeneration tasks using objective and subjective metrics. Audio samples are\navailable at https://VersBand.github.io.", "AI": {"tldr": "VersBand\u662f\u4e00\u4e2a\u591a\u4efb\u52a1\u6b4c\u66f2\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7VocalBand\u548cAccompBand\u7b49\u6a21\u578b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u5bf9\u9f50\u7684\u6b4c\u66f2\u751f\u6210\uff0c\u652f\u6301\u57fa\u4e8e\u63d0\u793a\u7684\u591a\u4efb\u52a1\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u8bed\u97f3\u548c\u4f34\u594f\u65f6\u7f3a\u4e4f\u57fa\u4e8e\u63d0\u793a\u7684\u63a7\u5236\u548c\u6b63\u786e\u5bf9\u9f50\uff0c\u4e14\u4e0d\u652f\u6301\u591a\u79cd\u4efb\u52a1\u3002", "method": "VersBand\u5305\u542bVocalBand\uff08\u57fa\u4e8e\u6d41\u5339\u914d\u751f\u6210\u6b4c\u58f0\uff09\u548cAccompBand\uff08\u57fa\u4e8e\u6d41\u7684\u53d8\u538b\u5668\u6a21\u578b\u751f\u6210\u4f34\u594f\uff09\uff0c\u4ee5\u53caLyricBand\u548cMelodyBand\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVersBand\u5728\u591a\u79cd\u6b4c\u66f2\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "VersBand\u901a\u8fc7\u591a\u4efb\u52a1\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u53ef\u63a7\u7684\u6b4c\u66f2\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002"}}
{"id": "2504.18770", "pdf": "https://arxiv.org/pdf/2504.18770", "abs": "https://arxiv.org/abs/2504.18770", "authors": ["Manuel Weber", "Carly Beneke"], "title": "PyViT-FUSE: A Foundation Model for Multi-Sensor Earth Observation Data", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "11 pages, 13 figures, Published at ICLR 2025 - Machine Learning for\n  Remote Sensing (ML4RS) Workshop", "summary": "We propose PyViT-FUSE, a foundation model for earth observation data\nexplicitly designed to handle multi-modal imagery by learning to fuse an\narbitrary number of mixed-resolution input bands into a single representation\nthrough an attention mechanism. The learned patch tokens are further processed\nby a stack of vision transformers with a novel pyramidal structure. We train\nthe model on a globally sampled dataset in a self-supervised manner, leveraging\ncore concepts of the SwAV algorithm. We show the interpretability of the fusion\nmechanism by visualization of the attention scores and the models applicability\nto downstream tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPyViT-FUSE\uff0c\u4e00\u79cd\u5904\u7406\u591a\u6a21\u6001\u9065\u611f\u6570\u636e\u7684\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u4efb\u610f\u6570\u91cf\u7684\u6df7\u5408\u5206\u8fa8\u7387\u6ce2\u6bb5\uff0c\u5e76\u91c7\u7528\u91d1\u5b57\u5854\u7ed3\u6784\u7684\u89c6\u89c9Transformer\u8fdb\u884c\u540e\u7eed\u5904\u7406\u3002\u6a21\u578b\u901a\u8fc7\u81ea\u76d1\u7763\u65b9\u5f0f\u8bad\u7ec3\uff0c\u5c55\u793a\u4e86\u6ce8\u610f\u529b\u673a\u5236\u7684\u53ef\u89e3\u91ca\u6027\u53ca\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u3002", "motivation": "\u9065\u611f\u6570\u636e\u901a\u5e38\u5177\u6709\u591a\u6a21\u6001\u548c\u6df7\u5408\u5206\u8fa8\u7387\u7684\u7279\u70b9\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u3002PyViT-FUSE\u65e8\u5728\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u5c42\u6b21Transformer\u7ed3\u6784\uff0c\u7edf\u4e00\u5904\u7406\u8fd9\u7c7b\u590d\u6742\u6570\u636e\uff0c\u5e76\u63d0\u5347\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u6a21\u578b\u91c7\u7528\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u591a\u6a21\u6001\u8f93\u5165\u6ce2\u6bb5\uff0c\u5229\u7528\u91d1\u5b57\u5854\u7ed3\u6784\u7684\u89c6\u89c9Transformer\u5904\u7406\u5b66\u4e60\u5230\u7684patch token\uff0c\u57fa\u4e8eSwAV\u7b97\u6cd5\u8fdb\u884c\u81ea\u76d1\u7763\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u53ef\u89c6\u5316\u6ce8\u610f\u529b\u5206\u6570\u9a8c\u8bc1\u878d\u5408\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u80fd\u6709\u6548\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\uff0c\u6ce8\u610f\u529b\u673a\u5236\u5177\u6709\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "PyViT-FUSE\u4e3a\u591a\u6a21\u6001\u9065\u611f\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u4e14\u53ef\u89e3\u91ca\u7684\u5904\u7406\u6846\u67b6\uff0c\u81ea\u76d1\u7763\u8bad\u7ec3\u65b9\u5f0f\u589e\u5f3a\u4e86\u5176\u9002\u7528\u6027\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u7ed3\u6784\u4ee5\u9002\u5e94\u66f4\u5e7f\u6cdb\u4efb\u52a1\u3002"}}
{"id": "2504.19452", "pdf": "https://arxiv.org/pdf/2504.19452", "abs": "https://arxiv.org/abs/2504.19452", "authors": ["Qibang Liu", "Vincient Zhong", "Hadi Meidani", "Diab Abueidda", "Seid Koric", "Philippe Geubelle"], "title": "Geometry-Informed Neural Operator Transformer", "categories": ["cs.LG", "physics.comp-ph"], "comment": null, "summary": "Machine-learning-based surrogate models offer significant computational\nefficiency and faster simulations compared to traditional numerical methods,\nespecially for problems requiring repeated evaluations of partial differential\nequations. This work introduces the Geometry-Informed Neural Operator\nTransformer (GINOT), which integrates the transformer architecture with the\nneural operator framework to enable forward predictions for arbitrary\ngeometries. GINOT encodes the surface points cloud of a geometry using a\nsampling and grouping mechanism combined with an attention mechanism, ensuring\ninvariance to point order and padding while maintaining robustness to\nvariations in point density. The geometry information is seamlessly integrated\nwith query points in the solution decoder through the attention mechanism. The\nperformance of GINOT is validated on multiple challenging datasets, showcasing\nits high accuracy and strong generalization capabilities for complex and\narbitrary 2D and 3D geometries.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u51e0\u4f55\u611f\u77e5\u795e\u7ecf\u7b97\u5b50\u53d8\u6362\u5668\uff08GINOT\uff09\uff0c\u901a\u8fc7\u7ed3\u5408Transformer\u67b6\u6784\u548c\u795e\u7ecf\u7b97\u5b50\u6846\u67b6\uff0c\u5b9e\u73b0\u5bf9\u4efb\u610f\u51e0\u4f55\u5f62\u72b6\u7684\u9ad8\u6548\u6b63\u5411\u9884\u6d4b\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u5728\u9700\u8981\u91cd\u590d\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u65f6\u8ba1\u7b97\u6548\u7387\u4f4e\uff0c\u800c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u4ee3\u7528\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u65f6\u8868\u73b0\u4e0d\u8db3\u3002GINOT\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u51e0\u4f55\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "method": "GINOT\u91c7\u7528\u91c7\u6837\u5206\u7ec4\u673a\u5236\u548c\u6ce8\u610f\u529b\u673a\u5236\u7f16\u7801\u51e0\u4f55\u8868\u9762\u70b9\u4e91\uff0c\u786e\u4fdd\u5bf9\u70b9\u987a\u5e8f\u548c\u586b\u5145\u7684\u4e0d\u53d8\u6027\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5c06\u51e0\u4f55\u4fe1\u606f\u4e0e\u67e5\u8be2\u70b9\u65e0\u7f1d\u96c6\u6210\u5230\u89e3\u7801\u5668\u4e2d\u3002", "result": "\u5728\u591a\u4e2a\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0cGINOT\u5bf9\u590d\u67422D\u548c3D\u51e0\u4f55\u5f62\u72b6\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GINOT\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf\u7b97\u5b50\u548cTransformer\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u7684\u5feb\u901f\u4eff\u771f\u3002"}}
{"id": "2504.18781", "pdf": "https://arxiv.org/pdf/2504.18781", "abs": "https://arxiv.org/abs/2504.18781", "authors": ["Hassan Wasswa", "Timothy Lynar", "Aziida Nanyonga", "Hussein Abbass"], "title": "IoT Botnet Detection: Application of Vision Transformer to Classification of Network Flow Traffic", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite the demonstrated effectiveness of transformer models in NLP, and\nimage and video classification, the available tools for extracting features\nfrom captured IoT network flow packets fail to capture sequential patterns in\naddition to the absence of spatial patterns consequently limiting transformer\nmodel application. This work introduces a novel preprocessing method to adapt\ntransformer models, the vision transformer (ViT) in particular, for IoT botnet\nattack detection using network flow packets. The approach involves feature\nextraction from .pcap files and transforming each instance into a 1-channel 2D\nimage shape, enabling ViT-based classification. Also, the ViT model was\nenhanced to allow use any classifier besides Multilayer Perceptron (MLP) that\nwas deployed in the initial ViT paper. Models including the conventional feed\nforward Deep Neural Network (DNN), LSTM and Bidirectional-LSTM (BLSTM)\ndemonstrated competitive performance in terms of precision, recall, and\nF1-score for multiclass-based attack detection when evaluated on two IoT attack\ndatasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u4f7fViT\u6a21\u578b\u80fd\u591f\u9002\u5e94\u57fa\u4e8e\u7f51\u7edc\u6d41\u91cf\u7684\u7269\u8054\u7f51\u50f5\u5c38\u7f51\u7edc\u653b\u51fb\u68c0\u6d4b\uff0c\u5e76\u901a\u8fc7\u7279\u5f81\u63d0\u53d6\u548c\u5f62\u72b6\u8f6c\u6362\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u7269\u8054\u7f51\u7f51\u7edc\u6d41\u91cf\u5305\u7279\u5f81\u63d0\u53d6\u5de5\u5177\u65e0\u6cd5\u6355\u6349\u5e8f\u5217\u548c\u7a7a\u95f4\u6a21\u5f0f\uff0c\u9650\u5236\u4e86Transformer\u6a21\u578b\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u5229\u7528ViT\u6a21\u578b\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u653b\u51fb\u68c0\u6d4b\u3002", "method": "\u4ece.pcap\u6587\u4ef6\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u5c06\u6bcf\u4e2a\u5b9e\u4f8b\u8f6c\u6362\u4e3a1\u901a\u9053\u76842D\u56fe\u50cf\u5f62\u72b6\uff0c\u4ee5\u9002\u914dViT\u5206\u7c7b\u3002\u6b64\u5916\uff0c\u6539\u8fdb\u4e86ViT\u6a21\u578b\uff0c\u4f7f\u5176\u53ef\u4ee5\u4f7f\u7528\u9664MLP\u5916\u7684\u5176\u4ed6\u5206\u7c7b\u5668\uff08\u5982DNN\u3001LSTM\u548cBLSTM\uff09\u3002", "result": "\u5728\u4e24\u79cd\u7269\u8054\u7f51\u653b\u51fb\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u6539\u8fdb\u540e\u7684\u6a21\u578b\uff08\u5305\u62ecDNN\u3001LSTM\u548cBLSTM\uff09\u5728\u591a\u7c7b\u653b\u51fb\u68c0\u6d4b\u4e2d\u7684\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u9002\u914dViT\u6a21\u578b\u5e76\u5f15\u5165\u591a\u79cd\u5206\u7c7b\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7269\u8054\u7f51\u50f5\u5c38\u7f51\u7edc\u653b\u51fb\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u4e3aTransformer\u5728\u5176\u4ed6\u7f51\u7edc\u6d41\u91cf\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.19473", "pdf": "https://arxiv.org/pdf/2504.19473", "abs": "https://arxiv.org/abs/2504.19473", "authors": ["Donghe Chen", "Han Wang", "Lin Cheng", "Shengping Gong"], "title": "Stability Enhancement in Reinforcement Learning via Adaptive Control Lyapunov Function", "categories": ["cs.LG", "cs.RO"], "comment": "10 pages, 8 figures", "summary": "Reinforcement Learning (RL) has shown promise in control tasks but faces\nsignificant challenges in real-world applications, primarily due to the absence\nof safety guarantees during the learning process. Existing methods often\nstruggle with ensuring safe exploration, leading to potential system failures\nand restricting applications primarily to simulated environments. Traditional\napproaches such as reward shaping and constrained policy optimization can fail\nto guarantee safety during initial learning stages, while model-based methods\nusing Control Lyapunov Functions (CLFs) or Control Barrier Functions (CBFs) may\nhinder efficient exploration and performance. To address these limitations,\nthis paper introduces Soft Actor-Critic with Control Lyapunov Function\n(SAC-CLF), a framework that enhances stability and safety through three key\ninnovations: (1) a task-specific CLF design method for safe and optimal\nperformance; (2) dynamic adjustment of constraints to maintain robustness under\nunmodeled dynamics; and (3) improved control input smoothness while ensuring\nsafety. Experimental results on a classical nonlinear system and satellite\nattitude control demonstrate the effectiveness of SAC-CLF in overcoming the\nshortcomings of existing methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Soft Actor-Critic\u4e0e\u63a7\u5236\u674e\u96c5\u666e\u8bfa\u592b\u51fd\u6570\uff08SAC-CLF\uff09\u7684\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5728\u5b89\u5168\u63a2\u7d22\u65b9\u9762\u7684\u6311\u6218\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u7ea6\u675f\u548c\u4f18\u5316\u63a7\u5236\u8f93\u5165\u5e73\u6ed1\u6027\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u63a7\u5236\u4efb\u52a1\u4e2d\u9762\u4e34\u5b89\u5168\u63a2\u7d22\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u5982\u5956\u52b1\u5851\u9020\u6216\u7ea6\u675f\u7b56\u7565\u4f18\u5316\u96be\u4ee5\u4fdd\u8bc1\u521d\u59cb\u5b66\u4e60\u9636\u6bb5\u7684\u5b89\u5168\u6027\uff0c\u800c\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u53ef\u80fd\u9650\u5236\u63a2\u7d22\u6548\u7387\u3002", "method": "\u8bba\u6587\u63d0\u51faSAC-CLF\u6846\u67b6\uff0c\u5305\u62ec\u4efb\u52a1\u7279\u5b9a\u7684CLF\u8bbe\u8ba1\u3001\u52a8\u6001\u7ea6\u675f\u8c03\u6574\u548c\u4f18\u5316\u63a7\u5236\u8f93\u5165\u5e73\u6ed1\u6027\u4e09\u9879\u521b\u65b0\u3002", "result": "\u5728\u7ecf\u5178\u975e\u7ebf\u6027\u7cfb\u7edf\u548c\u536b\u661f\u59ff\u6001\u63a7\u5236\u5b9e\u9a8c\u4e2d\uff0cSAC-CLF\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u4e86\u5b89\u5168\u6027\u548c\u6027\u80fd\u3002", "conclusion": "SAC-CLF\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4e0e\u63a7\u5236\u7406\u8bba\uff0c\u4e3a\u5b89\u5168\u63a2\u7d22\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.18800", "pdf": "https://arxiv.org/pdf/2504.18800", "abs": "https://arxiv.org/abs/2504.18800", "authors": ["Ryo Takizawa", "Satoshi Kodera", "Tempei Kabayama", "Ryo Matsuoka", "Yuta Ando", "Yuto Nakamura", "Haruki Settai", "Norihiko Takeda"], "title": "Video CLIP Model for Multi-View Echocardiography Interpretation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Echocardiography involves recording videos of the heart using ultrasound,\nenabling clinicians to evaluate its condition. Recent advances in large-scale\nvision-language models (VLMs) have garnered attention for automating the\ninterpretation of echocardiographic videos. However, most existing VLMs\nproposed for medical interpretation thus far rely on single-frame (i.e., image)\ninputs. Consequently, these image-based models often exhibit lower diagnostic\naccuracy for conditions identifiable through cardiac motion. Moreover,\nechocardiographic videos are recorded from various views that depend on the\ndirection of ultrasound emission, and certain views are more suitable than\nothers for interpreting specific conditions. Incorporating multiple views could\npotentially yield further improvements in accuracy. In this study, we developed\na video-language model that takes five different views and full video sequences\nas input, training it on pairs of echocardiographic videos and clinical reports\nfrom 60,747 cases. Our experiments demonstrate that this expanded approach\nachieves higher interpretation accuracy than models trained with only\nsingle-view videos or with still images.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u89c6\u9891\u8bed\u8a00\u6a21\u578b\uff0c\u5229\u7528\u4e94\u4e2a\u4e0d\u540c\u89c6\u89d2\u548c\u5b8c\u6574\u89c6\u9891\u5e8f\u5217\u4f5c\u4e3a\u8f93\u5165\uff0c\u4ee5\u63d0\u9ad8\u5fc3\u810f\u8d85\u58f0\u89c6\u9891\u7684\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e3b\u8981\u4f9d\u8d56\u5355\u5e27\u56fe\u50cf\u8f93\u5165\uff0c\u5bfc\u81f4\u5bf9\u901a\u8fc7\u5fc3\u810f\u8fd0\u52a8\u8bc6\u522b\u7684\u75c5\u75c7\u8bca\u65ad\u51c6\u786e\u6027\u8f83\u4f4e\u3002\u540c\u65f6\uff0c\u4e0d\u540c\u89c6\u89d2\u7684\u8d85\u58f0\u89c6\u9891\u5bf9\u7279\u5b9a\u75c5\u75c7\u7684\u89e3\u91ca\u66f4\u6709\u4f18\u52bf\uff0c\u6574\u5408\u591a\u89c6\u89d2\u53ef\u80fd\u63d0\u5347\u51c6\u786e\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u89c6\u9891\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u4e94\u4e2a\u4e0d\u540c\u89c6\u89d2\u548c\u5b8c\u6574\u89c6\u9891\u5e8f\u5217\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u572860,747\u4e2a\u6848\u4f8b\u7684\u5fc3\u810f\u8d85\u58f0\u89c6\u9891\u4e0e\u4e34\u5e8a\u62a5\u544a\u914d\u5bf9\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4ec5\u4f7f\u7528\u5355\u89c6\u89d2\u89c6\u9891\u6216\u9759\u6001\u56fe\u50cf\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u8fd9\u79cd\u6269\u5c55\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u89e3\u91ca\u51c6\u786e\u6027\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u591a\u89c6\u89d2\u548c\u5b8c\u6574\u89c6\u9891\u5e8f\u5217\uff0c\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u5fc3\u810f\u8d85\u58f0\u89c6\u9891\u7684\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u4e3a\u81ea\u52a8\u5316\u533b\u5b66\u5f71\u50cf\u89e3\u8bfb\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.19480", "pdf": "https://arxiv.org/pdf/2504.19480", "abs": "https://arxiv.org/abs/2504.19480", "authors": ["Dixiao Wei", "Peng Yi", "Jinlong Lei", "Yiguang Hong", "Yuchuan Du"], "title": "An Automated Reinforcement Learning Reward Design Framework with Large Language Model for Cooperative Platoon Coordination", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement Learning (RL) has demonstrated excellent decision-making\npotential in platoon coordination problems. However, due to the variability of\ncoordination goals, the complexity of the decision problem, and the\ntime-consumption of trial-and-error in manual design, finding a well\nperformance reward function to guide RL training to solve complex platoon\ncoordination problems remains challenging. In this paper, we formally define\nthe Platoon Coordination Reward Design Problem (PCRDP), extending the RL-based\ncooperative platoon coordination problem to incorporate automated reward\nfunction generation. To address PCRDP, we propose a Large Language Model\n(LLM)-based Platoon coordination Reward Design (PCRD) framework, which\nsystematically automates reward function discovery through LLM-driven\ninitialization and iterative optimization. In this method, LLM first\ninitializes reward functions based on environment code and task requirements\nwith an Analysis and Initial Reward (AIR) module, and then iteratively\noptimizes them based on training feedback with an evolutionary module. The AIR\nmodule guides LLM to deepen their understanding of code and tasks through a\nchain of thought, effectively mitigating hallucination risks in code\ngeneration. The evolutionary module fine-tunes and reconstructs the reward\nfunction, achieving a balance between exploration diversity and convergence\nstability for training. To validate our approach, we establish six challenging\ncoordination scenarios with varying complexity levels within the Yangtze River\nDelta transportation network simulation. Comparative experimental results\ndemonstrate that RL agents utilizing PCRD-generated reward functions\nconsistently outperform human-engineered reward functions, achieving an average\nof 10\\% higher performance metrics in all scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u81ea\u52a8\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u6846\u67b6\uff08PCRD\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5728\u8f66\u961f\u534f\u8c03\u4e2d\u7684\u5956\u52b1\u8bbe\u8ba1\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u8f66\u961f\u534f\u8c03\u76ee\u6807\u7684\u591a\u6837\u6027\u3001\u51b3\u7b56\u95ee\u9898\u7684\u590d\u6742\u6027\u4ee5\u53ca\u624b\u52a8\u8bd5\u9519\u7684\u65f6\u95f4\u6d88\u8017\uff0c\u8bbe\u8ba1\u9ad8\u6027\u80fd\u5956\u52b1\u51fd\u6570\u6765\u6307\u5bfc\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faPCRD\u6846\u67b6\uff0c\u901a\u8fc7LLM\u9a71\u52a8\u7684\u521d\u59cb\u5316\u548c\u8fed\u4ee3\u4f18\u5316\u81ea\u52a8\u751f\u6210\u5956\u52b1\u51fd\u6570\uff0c\u5305\u62ec\u5206\u6790\u521d\u59cb\u5316\u5956\u52b1\uff08AIR\uff09\u6a21\u5757\u548c\u8fdb\u5316\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528PCRD\u751f\u6210\u7684\u5956\u52b1\u51fd\u6570\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5728\u6240\u6709\u573a\u666f\u4e2d\u5e73\u5747\u6027\u80fd\u6307\u6807\u6bd4\u4eba\u5de5\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u9ad810%\u3002", "conclusion": "PCRD\u6846\u67b6\u80fd\u6709\u6548\u81ea\u52a8\u5316\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\uff0c\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u5728\u8f66\u961f\u534f\u8c03\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2504.19444", "pdf": "https://arxiv.org/pdf/2504.19444", "abs": "https://arxiv.org/abs/2504.19444", "authors": ["Kang Yang", "Xinjun Mao", "Shangwen Wang", "Yanlin Wang", "Tanghaoran Zhang", "Bo Lin", "Yihao Qin", "Zhang Zhang", "Yao Lu", "Kamal Al-Sabahi"], "title": "Large Language Models are Qualified Benchmark Builders: Rebuilding Pre-Training Datasets for Advancing Code Intelligence Tasks", "categories": ["cs.SE", "cs.CL"], "comment": "Awarded the ACM SIGSOFT Distinguished Paper Award in ICPC 2025", "summary": "Pre-trained code models rely heavily on high-quality pre-training data,\nparticularly human-written reference comments that bridge code and natural\nlanguage. However, these comments often become outdated as software evolves,\ndegrading model performance. Large language models (LLMs) excel at generating\nhigh-quality code comments. We investigate whether replacing human-written\ncomments with LLM-generated ones improves pre-training datasets. Since standard\nmetrics cannot assess reference comment quality, we propose two novel\nreference-free evaluation tasks: code-comment inconsistency detection and\nsemantic code search. Results show that LLM-generated comments are more\nsemantically consistent with code than human-written ones, as confirmed by\nmanual evaluation. Leveraging this finding, we rebuild the CodeSearchNet\ndataset with LLM-generated comments and re-pre-train CodeT5. Evaluations\ndemonstrate that models trained on LLM-enhanced data outperform those using\noriginal human comments in code summarization, generation, and translation\ntasks. This work validates rebuilding pre-training datasets with LLMs to\nadvance code intelligence, challenging the traditional reliance on human\nreference comments.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u7528LLM\u751f\u6210\u7684\u4ee3\u7801\u6ce8\u91ca\u66ff\u4ee3\u4eba\u5de5\u6ce8\u91ca\uff0c\u63d0\u5347\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u8d28\u91cf\u3002\u901a\u8fc7\u65b0\u63d0\u51fa\u7684\u65e0\u53c2\u8003\u8bc4\u4f30\u4efb\u52a1\u9a8c\u8bc1LLM\u6ce8\u91ca\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u66f4\u9ad8\uff0c\u5e76\u57fa\u4e8e\u6b64\u91cd\u5efa\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u4eba\u5de5\u7f16\u5199\u7684\u4ee3\u7801\u6ce8\u91ca\u5bb9\u6613\u8fc7\u65f6\uff0c\u5f71\u54cd\u9884\u8bad\u7ec3\u6a21\u578b\u6027\u80fd\uff0c\u800cLLM\u751f\u6210\u7684\u6ce8\u91ca\u8d28\u91cf\u66f4\u9ad8\u3002\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u662f\u5426\u80fd\u7528LLM\u751f\u6210\u7684\u6ce8\u91ca\u6539\u8fdb\u9884\u8bad\u7ec3\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65e0\u53c2\u8003\u8bc4\u4f30\u4efb\u52a1\uff08\u4ee3\u7801-\u6ce8\u91ca\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b\u548c\u8bed\u4e49\u4ee3\u7801\u641c\u7d22\uff09\uff0c\u7528LLM\u751f\u6210\u7684\u6ce8\u91ca\u91cd\u5efaCodeSearchNet\u6570\u636e\u96c6\uff0c\u5e76\u91cd\u65b0\u9884\u8bad\u7ec3CodeT5\u6a21\u578b\u3002", "result": "LLM\u751f\u6210\u7684\u6ce8\u91ca\u6bd4\u4eba\u5de5\u6ce8\u91ca\u4e0e\u4ee3\u7801\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u66f4\u9ad8\uff0c\u57fa\u4e8eLLM\u6ce8\u91ca\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u4ee3\u7801\u6458\u8981\u3001\u751f\u6210\u548c\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u901a\u8fc7LLM\u751f\u6210\u7684\u6ce8\u91ca\u91cd\u5efa\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u80fd\u6709\u6548\u63d0\u5347\u4ee3\u7801\u667a\u80fd\u6a21\u578b\u7684\u6027\u80fd\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u4f9d\u8d56\u4eba\u5de5\u6ce8\u91ca\u7684\u8303\u5f0f\u3002"}}
{"id": "2504.18804", "pdf": "https://arxiv.org/pdf/2504.18804", "abs": "https://arxiv.org/abs/2504.18804", "authors": ["Jagrit Acharya", "Gouri Ginde"], "title": "Can We Enhance Bug Report Quality Using LLMs?: An Empirical Study of LLM-Based Bug Report Generation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Bug reports contain the information developers need to triage and fix\nsoftware bugs. However, unclear, incomplete, or ambiguous information may lead\nto delays and excessive manual effort spent on bug triage and resolution. In\nthis paper, we explore whether Instruction fine-tuned Large Language Models\n(LLMs) can automatically transform casual, unstructured bug reports into\nhigh-quality, structured bug reports adhering to a standard template. We\nevaluate three open-source instruction-tuned LLMs (\\emph{Qwen 2.5, Mistral, and\nLlama 3.2}) against ChatGPT-4o, measuring performance on established metrics\nsuch as CTQRS, ROUGE, METEOR, and SBERT. Our experiments show that fine-tuned\nQwen 2.5 achieves a CTQRS score of \\textbf{77%}, outperforming both fine-tuned\nMistral (\\textbf{71%}), Llama 3.2 (\\textbf{63%}) and ChatGPT in 3-shot learning\n(\\textbf{75%}). Further analysis reveals that Llama 3.2 shows higher accuracy\nof detecting missing fields particularly Expected Behavior and Actual Behavior,\nwhile Qwen 2.5 demonstrates superior performance in capturing\nSteps-to-Reproduce, with an F1 score of 76%. Additional testing of the models\non other popular projects (e.g., Eclipse, GCC) demonstrates that our approach\ngeneralizes well, achieving up to \\textbf{70%} CTQRS in unseen projects' bug\nreports. These findings highlight the potential of instruction fine-tuning in\nautomating structured bug report generation, reducing manual effort for\ndevelopers and streamlining the software maintenance process.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u6307\u4ee4\u5fae\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u5426\u5c06\u975e\u7ed3\u6784\u5316\u7684\u9519\u8bef\u62a5\u544a\u81ea\u52a8\u8f6c\u5316\u4e3a\u9ad8\u8d28\u91cf\u7684\u7ed3\u6784\u5316\u62a5\u544a\uff0c\u7ed3\u679c\u663e\u793aQwen 2.5\u8868\u73b0\u6700\u4f73\uff0c\u5c24\u5176\u5728\u91cd\u73b0\u6b65\u9aa4\u65b9\u9762\uff0c\u4e14\u65b9\u6cd5\u5728\u4e0d\u540c\u9879\u76ee\u4e2d\u6cdb\u5316\u80fd\u529b\u826f\u597d\u3002", "motivation": "\u89e3\u51b3\u975e\u7ed3\u6784\u5316\u9519\u8bef\u62a5\u544a\u5bfc\u81f4\u7684\u95ee\u9898\uff08\u5982\u5ef6\u8fdf\u548c\u989d\u5916\u624b\u52a8\u5de5\u4f5c\u91cf\uff09\uff0c\u63a2\u7d22LLMs\u5728\u81ea\u52a8\u5316\u751f\u6210\u7ed3\u6784\u5316\u62a5\u544a\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u8bc4\u4f30\u4e09\u79cd\u5f00\u6e90\u6307\u4ee4\u5fae\u8c03LLMs\uff08Qwen 2.5\u3001Mistral\u3001Llama 3.2\uff09\u548cChatGPT-4o\uff0c\u4f7f\u7528CTQRS\u3001ROUGE\u7b49\u6307\u6807\u8861\u91cf\u6027\u80fd\uff0c\u5e76\u5728\u591a\u9879\u76ee\u4e2d\u6d4b\u8bd5\u6cdb\u5316\u80fd\u529b\u3002", "result": "Qwen 2.5\u7684CTQRS\u5f97\u520677%\uff0c\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff1bLlama 3.2\u5728\u68c0\u6d4b\u7f3a\u5931\u5b57\u6bb5\uff08\u5982\u201c\u9884\u671f\u884c\u4e3a\u201d\uff09\u4e0a\u66f4\u51c6\u786e\uff0cQwen 2.5\u5728\u201c\u91cd\u73b0\u6b65\u9aa4\u201d\u4e0aF1\u5206\u657076%\u3002\u65b9\u6cd5\u5728\u672a\u89c1\u9879\u76ee\u4e2d\u4e5f\u8868\u73b0\u826f\u597d\uff08CTQRS\u8fbe70%\uff09\u3002", "conclusion": "\u6307\u4ee4\u5fae\u8c03\u5728\u81ea\u52a8\u5316\u751f\u6210\u7ed3\u6784\u5316\u9519\u8bef\u62a5\u544a\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u51cf\u5c11\u5f00\u53d1\u8005\u624b\u52a8\u5de5\u4f5c\uff0c\u4f18\u5316\u8f6f\u4ef6\u7ef4\u62a4\u6d41\u7a0b\u3002"}}
{"id": "2504.19483", "pdf": "https://arxiv.org/pdf/2504.19483", "abs": "https://arxiv.org/abs/2504.19483", "authors": ["Bertram H\u00f8jer", "Oliver Jarvis", "Stefan Heinrich"], "title": "Improving Reasoning Performance in Large Language Models via Representation Engineering", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Has been accepted at \"The Thirteenth International Conference on\n  Learning Representations (ICLR 2025)\" Link to publication:\n  https://openreview.net/forum?id=IssPhpUsKt", "summary": "Recent advancements in large language models (LLMs) have resulted in\nincreasingly anthropomorphic language concerning the ability of LLMs to reason.\nWhether reasoning in LLMs should be understood to be inherently different is,\nhowever, widely debated. We propose utilizing a representation engineering\napproach wherein model activations are read from the residual stream of an LLM\nwhen processing a reasoning task. The activations are used to derive a control\nvector that is applied to the model as an inference-time intervention,\nmodulating the representational space of the model, to improve performance on\nthe specified task. We publish the code for deriving control vectors and\nanalyzing model representations. The method allows us to improve performance on\nreasoning benchmarks and assess how control vectors influence the final logit\ndistribution of a model via metrics such as KL divergence and entropy. We apply\ncontrol vectors to Mistral-7B-Instruct and a range of Pythia models on an\ninductive, a deductive and mathematical reasoning task. We show that an LLM\ncan, to a certain degree, be controlled to improve its perceived reasoning\nability by modulating activations. The intervention is dependent upon the\nability to reliably extract the model's typical state when correctly solving a\ntask. Our results suggest that reasoning performance can be modulated in the\nsame manner as other information-processing tasks performed by LLMs and\ndemonstrate that we are capable of improving performance on specific tasks via\na simple intervention on the residual stream with no additional training.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u8868\u793a\u5de5\u7a0b\u65b9\u6cd5\u8c03\u5236LLM\u6fc0\u6d3b\u5411\u91cf\u4ee5\u63d0\u5347\u63a8\u7406\u4efb\u52a1\u6027\u80fd\u7684\u6280\u672f\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u63a2\u8ba8LLM\u7684\u63a8\u7406\u80fd\u529b\u662f\u5426\u53ef\u901a\u8fc7\u5e72\u9884\u6fc0\u6d3b\u5411\u91cf\u6765\u63d0\u5347\uff0c\u4ee5\u9a8c\u8bc1\u5176\u63a8\u7406\u673a\u5236\u662f\u5426\u4e0e\u5176\u4ed6\u4fe1\u606f\u5904\u7406\u4efb\u52a1\u7c7b\u4f3c\u3002", "method": "\u4eceLLM\u6b8b\u5dee\u6d41\u4e2d\u63d0\u53d6\u63a8\u7406\u4efb\u52a1\u65f6\u7684\u6fc0\u6d3b\u5411\u91cf\uff0c\u751f\u6210\u63a7\u5236\u5411\u91cf\u4f5c\u4e3a\u63a8\u7406\u65f6\u5e72\u9884\uff0c\u901a\u8fc7KL\u6563\u5ea6\u7b49\u6307\u6807\u8bc4\u4f30\u6548\u679c\u3002", "result": "\u5728Mistral-7B\u548cPythia\u6a21\u578b\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u63d0\u5347\u4e86\u5f52\u7eb3\u3001\u6f14\u7ece\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "LLM\u7684\u63a8\u7406\u80fd\u529b\u53ef\u901a\u8fc7\u6fc0\u6d3b\u5411\u91cf\u8c03\u5236\u6539\u8fdb\uff0c\u4e14\u5176\u673a\u5236\u4e0e\u5176\u4ed6\u4efb\u52a1\u7c7b\u4f3c\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u4f18\u5316\u3002"}}
{"id": "2504.19458", "pdf": "https://arxiv.org/pdf/2504.19458", "abs": "https://arxiv.org/abs/2504.19458", "authors": ["Taoyu Su", "Jiawei Sheng", "Duohe Ma", "Xiaodong Li", "Juwei Yue", "Mengxiao Song", "Yingkai Tang", "Tingwen Liu"], "title": "Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective", "categories": ["cs.MM", "cs.CL", "cs.IR"], "comment": "Accepted by SIGIR 2025, 11 pages, 10 figures, 4 tables,", "summary": "Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from\ndifferent Multi-Modal Knowledge Graphs (MMKGs), a critical information\nretrieval task. Existing studies have explored various fusion paradigms and\nconsistency constraints to improve the alignment of equivalent entities, while\noverlooking that the visual modality may not always contribute positively.\nEmpirically, entities with low-similarity images usually generate\nunsatisfactory performance, highlighting the limitation of overly relying on\nvisual features. We believe the model can be biased toward the visual modality,\nleading to a shortcut image-matching task. To address this, we propose a\ncounterfactual debiasing framework for MMEA, termed CDMEA, which investigates\nvisual modality bias from a causal perspective. Our approach aims to leverage\nboth visual and graph modalities to enhance MMEA while suppressing the direct\ncausal effect of the visual modality on model predictions. By estimating the\nTotal Effect (TE) of both modalities and excluding the Natural Direct Effect\n(NDE) of the visual modality, we ensure that the model predicts based on the\nTotal Indirect Effect (TIE), effectively utilizing both modalities and reducing\nvisual modality bias. Extensive experiments on 9 benchmark datasets show that\nCDMEA outperforms 14 state-of-the-art methods, especially in low-similarity,\nhigh-noise, and low-resource data scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCDMEA\u7684\u53cd\u4e8b\u5b9e\u53bb\u504f\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u89c6\u89d2\u89e3\u51b3\u591a\u6a21\u6001\u5b9e\u4f53\u5bf9\u9f50\u4e2d\u89c6\u89c9\u6a21\u6001\u53ef\u80fd\u5bfc\u81f4\u7684\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5b9e\u4f53\u5bf9\u9f50\u65b9\u6cd5\u8fc7\u4e8e\u4f9d\u8d56\u89c6\u89c9\u7279\u5f81\uff0c\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u504f\u5411\u89c6\u89c9\u5339\u914d\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u4f4e\u76f8\u4f3c\u5ea6\u56fe\u50cf\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "CDMEA\u901a\u8fc7\u4f30\u8ba1\u4e24\u79cd\u6a21\u6001\u7684\u603b\u6548\u5e94\uff08TE\uff09\u5e76\u6392\u9664\u89c6\u89c9\u6a21\u6001\u7684\u81ea\u7136\u76f4\u63a5\u6548\u5e94\uff08NDE\uff09\uff0c\u786e\u4fdd\u6a21\u578b\u57fa\u4e8e\u603b\u95f4\u63a5\u6548\u5e94\uff08TIE\uff09\u8fdb\u884c\u9884\u6d4b\uff0c\u51cf\u5c11\u89c6\u89c9\u504f\u5dee\u3002", "result": "\u57289\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cCDMEA\u4f18\u4e8e14\u79cd\u6700\u65b0\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u4f4e\u76f8\u4f3c\u5ea6\u3001\u9ad8\u566a\u58f0\u548c\u4f4e\u8d44\u6e90\u6570\u636e\u573a\u666f\u3002", "conclusion": "CDMEA\u6709\u6548\u6291\u5236\u4e86\u89c6\u89c9\u6a21\u6001\u504f\u5dee\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5b9e\u4f53\u5bf9\u9f50\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.19496", "pdf": "https://arxiv.org/pdf/2504.19496", "abs": "https://arxiv.org/abs/2504.19496", "authors": ["Rudy Morel", "Jiequn Han", "Edouard Oyallon"], "title": "DISCO: learning to DISCover an evolution Operator for multi-physics-agnostic prediction", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "We address the problem of predicting the next state of a dynamical system\ngoverned by unknown temporal partial differential equations (PDEs) using only a\nshort trajectory. While standard transformers provide a natural black-box\nsolution to this task, the presence of a well-structured evolution operator in\nthe data suggests a more tailored and efficient approach. Specifically, when\nthe PDE is fully known, classical numerical solvers can evolve the state\naccurately with only a few parameters. Building on this observation, we\nintroduce DISCO, a model that uses a large hypernetwork to process a short\ntrajectory and generate the parameters of a much smaller operator network,\nwhich then predicts the next state through time integration. Our framework\ndecouples dynamics estimation (i.e., DISCovering an evolution operator from a\nshort trajectory) from state prediction (i.e., evolving this operator).\nExperiments show that pretraining our model on diverse physics datasets\nachieves state-of-the-art performance while requiring significantly fewer\nepochs. Moreover, it generalizes well and remains competitive when fine-tuned\non downstream tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86DISCO\u6a21\u578b\uff0c\u901a\u8fc7\u8d85\u7f51\u7edc\u5904\u7406\u77ed\u8f68\u8ff9\u751f\u6210\u5c0f\u7b97\u5b50\u7f51\u7edc\u53c2\u6570\uff0c\u5206\u79bb\u52a8\u529b\u5b66\u4f30\u8ba1\u4e0e\u72b6\u6001\u9884\u6d4b\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u9488\u5bf9\u672a\u77e5\u65f6\u95f4\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDEs\uff09\u7684\u52a8\u6001\u7cfb\u7edf\u9884\u6d4b\u95ee\u9898\uff0c\u4f20\u7edfTransformer\u867d\u7136\u53ef\u7528\u4f46\u6548\u7387\u4e0d\u9ad8\u3002\u5df2\u77e5PDE\u65f6\u7ecf\u5178\u6570\u503c\u89e3\u6cd5\u80fd\u4ee5\u5c11\u91cf\u53c2\u6570\u51c6\u786e\u9884\u6d4b\u72b6\u6001\uff0c\u53d7\u6b64\u542f\u53d1\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "DISCO\u6a21\u578b\u5229\u7528\u8d85\u7f51\u7edc\u5904\u7406\u77ed\u8f68\u8ff9\u751f\u6210\u5c0f\u7b97\u5b50\u7f51\u7edc\u7684\u53c2\u6570\uff0c\u540e\u8005\u901a\u8fc7\u65f6\u95f4\u79ef\u5206\u9884\u6d4b\u4e0b\u4e00\u72b6\u6001\uff0c\u5b9e\u73b0\u4e86\u52a8\u529b\u5b66\u4f30\u8ba1\u4e0e\u72b6\u6001\u9884\u6d4b\u7684\u89e3\u8026\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u591a\u6837\u5316\u7269\u7406\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u540e\uff0c\u6027\u80fd\u8fbe\u5230\u6700\u4f18\uff0c\u4e14\u8bad\u7ec3\u5468\u671f\u5927\u5e45\u51cf\u5c11\uff0c\u4e0b\u6e38\u4efb\u52a1\u5fae\u8c03\u540e\u4ecd\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "DISCO\u901a\u8fc7\u5206\u79bb\u52a8\u6001\u4f30\u8ba1\u4e0e\u72b6\u6001\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u672a\u77e5PDE\u52a8\u6001\u7cfb\u7edf\u7684\u9884\u6d4b\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2504.18807", "pdf": "https://arxiv.org/pdf/2504.18807", "abs": "https://arxiv.org/abs/2504.18807", "authors": ["Si\u00e2n Brooke"], "title": "Clones in the Machine: A Feminist Critique of Agency in Digital Cloning", "categories": ["cs.HC", "cs.AI"], "comment": "ACM CHI Conference on Human Factors in Computing Systems 2025", "summary": "This paper critiques digital cloning in academic research, highlighting how\nit exemplifies AI solutionism. Digital clones, which replicate user data to\nsimulate behavior, are often seen as scalable tools for behavioral insights.\nHowever, this framing obscures ethical concerns around consent, agency, and\nrepresentation. Drawing on feminist theories of agency, the paper argues that\ndigital cloning oversimplifies human complexity and risks perpetuating systemic\nbiases. To address these issues, it proposes decentralized data repositories\nand dynamic consent models, promoting ethical, context-aware AI practices that\nchallenge the reductionist logic of AI solutionism", "AI": {"tldr": "\u672c\u6587\u6279\u8bc4\u5b66\u672f\u7814\u7a76\u4e2d\u7684\u6570\u5b57\u514b\u9686\uff0c\u6307\u51fa\u5176\u4f53\u73b0\u4e86AI\u89e3\u51b3\u65b9\u6848\u4e3b\u4e49\uff0c\u5e76\u63a9\u76d6\u4e86\u56f4\u7ed5\u540c\u610f\u3001\u4ee3\u7406\u548c\u8868\u8fbe\u7684\u4f26\u7406\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u53bb\u4e2d\u5fc3\u5316\u6570\u636e\u4ed3\u5e93\u548c\u52a8\u6001\u540c\u610f\u6a21\u578b\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u6570\u5b57\u514b\u9686\u5e38\u88ab\u89c6\u4e3a\u83b7\u53d6\u884c\u4e3a\u6d1e\u5bdf\u7684\u53ef\u6269\u5c55\u5de5\u5177\uff0c\u4f46\u5176\u7b80\u5316\u4e86\u4eba\u7c7b\u590d\u6742\u6027\u5e76\u53ef\u80fd\u5ef6\u7eed\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u6279\u5224\u8fd9\u4e00\u95ee\u9898\u5e76\u63d0\u51fa\u66ff\u4ee3\u65b9\u6848\u6765\u4fc3\u8fdb\u4f26\u7406AI\u5b9e\u8df5\u3002", "method": "\u7ed3\u5408\u5973\u6027\u4e3b\u4e49\u4ee3\u7406\u7406\u8bba\u5206\u6790\u6570\u5b57\u514b\u9686\u7684\u4f26\u7406\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u53bb\u4e2d\u5fc3\u5316\u6570\u636e\u4ed3\u5e93\u4e0e\u52a8\u6001\u540c\u610f\u6a21\u578b\u4f5c\u4e3a\u6539\u8fdb\u65b9\u6cd5\u3002", "result": "\u63ed\u793a\u4e86\u6570\u5b57\u514b\u9686\u7684\u6f5c\u5728\u4f26\u7406\u5371\u5bb3\uff0c\u5e76\u63d0\u51fa\u4e86\u66ff\u4ee3\u65b9\u6848\u4ee5\u652f\u6301\u66f4\u7b26\u5408\u4f26\u7406\u7684AI\u5b9e\u8df5\u3002", "conclusion": "\u6570\u5b57\u514b\u9686\u7684AI\u89e3\u51b3\u65b9\u6848\u4e3b\u4e49\u903b\u8f91\u9700\u8981\u88ab\u6311\u6218\uff0c\u91c7\u7528\u53bb\u4e2d\u5fc3\u5316\u4e0e\u52a8\u6001\u540c\u610f\u6a21\u578b\u53ef\u5b9e\u73b0\u66f4\u5177\u4f26\u7406\u610f\u8bc6\u7684AI\u5b9e\u8df5\u3002"}}
{"id": "2504.19527", "pdf": "https://arxiv.org/pdf/2504.19527", "abs": "https://arxiv.org/abs/2504.19527", "authors": ["Qinwei Yang", "Ruocheng Guo", "Shasha Han", "Peng Wu"], "title": "Identification and Estimation of Long-Term Treatment Effects with Monotone Missing", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Estimating long-term treatment effects has a wide range of applications in\nvarious domains. A key feature in this context is that collecting long-term\noutcomes typically involves a multi-stage process and is subject to monotone\nmissing, where individuals missing at an earlier stage remain missing at\nsubsequent stages. Despite its prevalence, monotone missing has been rarely\nexplored in previous studies on estimating long-term treatment effects. In this\npaper, we address this gap by introducing the sequential missingness assumption\nfor identification. We propose three novel estimation methods, including\ninverse probability weighting, sequential regression imputation, and sequential\nmarginal structural model (SeqMSM). Considering that the SeqMSM method may\nsuffer from high variance due to severe data sparsity caused by monotone\nmissing, we further propose a novel balancing-enhanced approach, BalanceNet, to\nimprove the stability and accuracy of the estimation methods. Extensive\nexperiments on two widely used benchmark datasets demonstrate the effectiveness\nof our proposed methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u957f\u671f\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u4e2d\u5355\u8c03\u7f3a\u5931\u95ee\u9898\u7684\u6846\u67b6\uff0c\u5305\u62ec\u4e09\u79cd\u65b0\u65b9\u6cd5\u548c\u4e00\u79cd\u7a33\u5b9a\u6027\u589e\u5f3a\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u957f\u671f\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u4e2d\u666e\u904d\u5b58\u5728\u5355\u8c03\u7f3a\u5931\u95ee\u9898\uff0c\u4f46\u6b64\u524d\u7814\u7a76\u5c11\u6709\u63a2\u8ba8\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u4f30\u8ba1\u65b9\u6cd5\uff1a\u9006\u6982\u7387\u52a0\u6743\u3001\u5e8f\u8d2f\u56de\u5f52\u586b\u8865\u548c\u5e8f\u8d2f\u8fb9\u9645\u7ed3\u6784\u6a21\u578b\uff08SeqMSM\uff09\u3002\u9488\u5bf9SeqMSM\u56e0\u6570\u636e\u7a00\u758f\u5bfc\u81f4\u7684\u65b9\u5dee\u95ee\u9898\uff0c\u8fdb\u4e00\u6b65\u63d0\u51faBalanceNet\u65b9\u6cd5\u63d0\u5347\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5355\u8c03\u7f3a\u5931\u95ee\u9898\uff0c\u4e3a\u957f\u671f\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.19500", "pdf": "https://arxiv.org/pdf/2504.19500", "abs": "https://arxiv.org/abs/2504.19500", "authors": ["Yan Wang", "Baoxiong Jia", "Ziyu Zhu", "Siyuan Huang"], "title": "Masked Point-Entity Contrast for Open-Vocabulary 3D Scene Understanding", "categories": ["cs.CV", "cs.CL"], "comment": "CVPR 2025", "summary": "Open-vocabulary 3D scene understanding is pivotal for enhancing physical\nintelligence, as it enables embodied agents to interpret and interact\ndynamically within real-world environments. This paper introduces MPEC, a novel\nMasked Point-Entity Contrastive learning method for open-vocabulary 3D semantic\nsegmentation that leverages both 3D entity-language alignment and point-entity\nconsistency across different point cloud views to foster entity-specific\nfeature representations. Our method improves semantic discrimination and\nenhances the differentiation of unique instances, achieving state-of-the-art\nresults on ScanNet for open-vocabulary 3D semantic segmentation and\ndemonstrating superior zero-shot scene understanding capabilities. Extensive\nfine-tuning experiments on 8 datasets, spanning from low-level perception to\nhigh-level reasoning tasks, showcase the potential of learned 3D features,\ndriving consistent performance gains across varied 3D scene understanding\ntasks. Project website: https://mpec-3d.github.io/", "AI": {"tldr": "MPEC\u662f\u4e00\u79cd\u65b0\u9896\u7684\u63a9\u853d\u70b9-\u5b9e\u4f53\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5f00\u653e\u8bcd\u6c473D\u8bed\u4e49\u5206\u5272\uff0c\u901a\u8fc7\u7ed3\u54083D\u5b9e\u4f53-\u8bed\u8a00\u5bf9\u9f50\u548c\u4e0d\u540c\u70b9\u4e91\u89c6\u56fe\u95f4\u7684\u70b9-\u5b9e\u4f53\u4e00\u81f4\u6027\uff0c\u63d0\u5347\u8bed\u4e49\u533a\u5206\u548c\u5b9e\u4f8b\u8fa8\u522b\u80fd\u529b\u3002", "motivation": "\u63d0\u5347\u7269\u7406\u667a\u80fd\u7684\u5173\u952e\u5728\u4e8e\u5f00\u653e\u8bcd\u6c473D\u573a\u666f\u7406\u89e3\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u52a8\u6001\u4ea4\u4e92\u3002", "method": "\u91c7\u7528\u63a9\u853d\u70b9-\u5b9e\u4f53\u5bf9\u6bd4\u5b66\u4e60\uff08MPEC\uff09\uff0c\u7ed3\u54083D\u5b9e\u4f53-\u8bed\u8a00\u5bf9\u9f50\u548c\u591a\u89c6\u56fe\u70b9-\u5b9e\u4f53\u4e00\u81f4\u6027\uff0c\u751f\u6210\u5b9e\u4f53\u7279\u5b9a\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728ScanNet\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5f00\u653e\u8bcd\u6c473D\u8bed\u4e49\u5206\u5272\u7684\u6700\u5148\u8fdb\u6548\u679c\uff0c\u5e76\u57288\u4e2a\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u96f6\u6837\u672c\u573a\u666f\u7406\u89e3\u80fd\u529b\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "MPEC\u4e0d\u4ec5\u5728\u6027\u80fd\u4e0a\u53d6\u5f97\u7a81\u7834\uff0c\u8fd8\u80fd\u63a8\u52a8\u591a\u6837\u53163D\u573a\u666f\u7406\u89e3\u4efb\u52a1\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2504.18810", "pdf": "https://arxiv.org/pdf/2504.18810", "abs": "https://arxiv.org/abs/2504.18810", "authors": ["Yifan Xie", "Fei Ma", "Yi Bin", "Ying He", "Fei Yu"], "title": "Audio-Driven Talking Face Video Generation with Joint Uncertainty Learning", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 7 figures", "summary": "Talking face video generation with arbitrary speech audio is a significant\nchallenge within the realm of digital human technology. The previous studies\nhave emphasized the significance of audio-lip synchronization and visual\nquality. Currently, limited attention has been given to the learning of visual\nuncertainty, which creates several issues in existing systems, including\ninconsistent visual quality and unreliable performance across different input\nconditions. To address the problem, we propose a Joint Uncertainty Learning\nNetwork (JULNet) for high-quality talking face video generation, which\nincorporates a representation of uncertainty that is directly related to visual\nerror. Specifically, we first design an uncertainty module to individually\npredict the error map and uncertainty map after obtaining the generated image.\nThe error map represents the difference between the generated image and the\nground truth image, while the uncertainty map is used to predict the\nprobability of incorrect estimates. Furthermore, to match the uncertainty\ndistribution with the error distribution through a KL divergence term, we\nintroduce a histogram technique to approximate the distributions. By jointly\noptimizing error and uncertainty, the performance and robustness of our model\ncan be enhanced. Extensive experiments demonstrate that our method achieves\nsuperior high-fidelity and audio-lip synchronization in talking face video\ngeneration compared to previous methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u8054\u5408\u4e0d\u786e\u5b9a\u6027\u5b66\u4e60\u7f51\u7edc\uff08JULNet\uff09\uff0c\u7528\u4e8e\u9ad8\u8d28\u91cf\u8bf4\u8bdd\u4eba\u8138\u89c6\u9891\u751f\u6210\uff0c\u901a\u8fc7\u5f15\u5165\u89c6\u89c9\u8bef\u5dee\u7684\u4e0d\u786e\u5b9a\u6027\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u4e2d\u7684\u89c6\u89c9\u8d28\u91cf\u4e0d\u4e00\u81f4\u548c\u8f93\u5165\u6761\u4ef6\u4f9d\u8d56\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u5728\u8bf4\u8bdd\u4eba\u8138\u89c6\u9891\u751f\u6210\u4e2d\u5ffd\u89c6\u4e86\u89c6\u89c9\u4e0d\u786e\u5b9a\u6027\u7684\u5b66\u4e60\uff0c\u5bfc\u81f4\u751f\u6210\u89c6\u9891\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u8054\u5408\u4f18\u5316\u8bef\u5dee\u548c\u4e0d\u786e\u5b9a\u6027\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e0d\u786e\u5b9a\u6027\u6a21\u5757\uff0c\u5206\u522b\u9884\u6d4b\u8bef\u5dee\u56fe\u548c\u4e0d\u786e\u5b9a\u6027\u56fe\uff0c\u5e76\u901a\u8fc7KL\u6563\u5ea6\u548c\u76f4\u65b9\u56fe\u6280\u672f\u5339\u914d\u4e24\u8005\u7684\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cJULNet\u5728\u9ad8\u4fdd\u771f\u5ea6\u548c\u97f3\u9891-\u5507\u540c\u6b65\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8054\u5408\u5b66\u4e60\u4e0d\u786e\u5b9a\u6027\u548c\u8bef\u5dee\u663e\u8457\u63d0\u5347\u4e86\u8bf4\u8bdd\u4eba\u8138\u89c6\u9891\u751f\u6210\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.19530", "pdf": "https://arxiv.org/pdf/2504.19530", "abs": "https://arxiv.org/abs/2504.19530", "authors": ["Yicheng Li", "Xinghua Sun"], "title": "Euclidean Distance Matrix Completion via Asymmetric Projected Gradient Descent", "categories": ["cs.LG", "eess.SP", "stat.ML"], "comment": null, "summary": "This paper proposes and analyzes a gradient-type algorithm based on\nBurer-Monteiro factorization, called the Asymmetric Projected Gradient Descent\n(APGD), for reconstructing the point set configuration from partial Euclidean\ndistance measurements, known as the Euclidean Distance Matrix Completion (EDMC)\nproblem. By paralleling the incoherence matrix completion framework, we show\nfor the first time that global convergence guarantee with exact recovery of\nthis routine can be established given $\\mathcal{O}(\\mu^2 r^3 \\kappa^2 n \\log\nn)$ Bernoulli random observations without any sample splitting. Unlike\nleveraging the tangent space Restricted Isometry Property (RIP) and local\ncurvature of the low-rank embedding manifold in some very recent works, our\nproof provides new upper bounds to replace the random graph lemma under EDMC\nsetting. The APGD works surprisingly well and numerical experiments demonstrate\nexact linear convergence behavior in rich-sample regions yet deteriorates fast\nwhen compared with the performance obtained by optimizing the s-stress\nfunction, i.e., the standard but unexplained non-convex approach for EDMC, if\nthe sample size is limited. While virtually matching our theoretical\nprediction, this unusual phenomenon might indicate that: (i) the power of\nimplicit regularization is weakened when specified in the APGD case; (ii) the\nstabilization of such new gradient direction requires substantially more\nsamples than the information-theoretic limit would suggest.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBurer-Monteiro\u5206\u89e3\u7684\u68af\u5ea6\u578b\u7b97\u6cd5\u2014\u2014\u975e\u5bf9\u79f0\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\uff08APGD\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u6b27\u6c0f\u8ddd\u79bb\u77e9\u9635\u8865\u5168\uff08EDMC\uff09\u95ee\u9898\u3002\u901a\u8fc7\u5e76\u884c\u975e\u76f8\u5e72\u77e9\u9635\u8865\u5168\u6846\u67b6\uff0c\u9996\u6b21\u8bc1\u660e\u4e86\u5728\u4e0d\u9700\u8981\u6837\u672c\u5206\u5272\u7684\u60c5\u51b5\u4e0b\uff0c\u7ed9\u5b9a\u7279\u5b9a\u6570\u91cf\u7684\u968f\u673a\u89c2\u6d4b\u540e\uff0c\u8be5\u7b97\u6cd5\u80fd\u5b9e\u73b0\u5168\u5c40\u6536\u655b\u548c\u7cbe\u786e\u6062\u590d\u3002\u6570\u503c\u5b9e\u9a8c\u663e\u793a\u7b97\u6cd5\u5728\u6837\u672c\u5145\u8db3\u65f6\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u6837\u672c\u6709\u9650\u65f6\u6027\u80fd\u4e0b\u964d\u8f83\u5feb\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u6b27\u6c0f\u8ddd\u79bb\u77e9\u9635\u8865\u5168\uff08EDMC\uff09\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u90e8\u5206\u6b27\u6c0f\u8ddd\u79bb\u6d4b\u91cf\u6761\u4ef6\u4e0b\u91cd\u5efa\u70b9\u96c6\u914d\u7f6e\u3002\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u68af\u5ea6\u7b97\u6cd5\uff0c\u4f5c\u8005\u65e8\u5728\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\uff0c\u5982\u57fa\u4e8e\u5207\u7ebf\u7a7a\u95f4RIP\u548c\u5c40\u90e8\u66f2\u7387\u7684\u65b9\u6cd5\uff0c\u63d0\u4f9b\u66f4\u4f18\u7684\u6536\u655b\u4fdd\u8bc1\u3002", "method": "\u65b9\u6cd5\u57fa\u4e8eBurer-Monteiro\u5206\u89e3\u7684\u975e\u5bf9\u79f0\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\uff08APGD\uff09\uff0c\u4e0d\u9700\u8981\u4f9d\u8d56\u6837\u672c\u5206\u5272\u3002\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u4e0a\u754c\u66ff\u6362\u968f\u673a\u56fe\u5f15\u7406\uff0c\u5728EDMC\u8bbe\u5b9a\u4e0b\u5efa\u7acb\u4e86\u5168\u5c40\u6536\u655b\u7406\u8bba\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u5728\u7ed9\u5b9a\u7279\u5b9a\u6570\u91cf\u7684\u968f\u673a\u89c2\u6d4b\u540e\uff0cAPGD\u80fd\u5b9e\u73b0\u5168\u5c40\u6536\u655b\u548c\u7cbe\u786e\u6062\u590d\u3002\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5728\u6837\u672c\u5145\u8db3\u65f6\u7684\u7ebf\u6027\u6536\u655b\u884c\u4e3a\uff0c\u4f46\u6837\u672c\u6709\u9650\u65f6\u6027\u80fd\u5feb\u901f\u4e0b\u964d\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0cAPGD\u5728\u6837\u672c\u5145\u8db3\u65f6\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u6837\u672c\u6709\u9650\u65f6\u6027\u80fd\u53d7\u9650\u3002\u8fd9\u6697\u793a\u4e86\u9690\u5f0f\u6b63\u5219\u5316\u7684\u6548\u679c\u5728APGD\u4e2d\u88ab\u524a\u5f31\uff0c\u4e14\u5176\u68af\u5ea6\u65b9\u5411\u7684\u7a33\u5b9a\u9700\u8981\u6bd4\u4fe1\u606f\u7406\u8bba\u6781\u9650\u66f4\u591a\u7684\u6837\u672c\u3002"}}
{"id": "2504.19519", "pdf": "https://arxiv.org/pdf/2504.19519", "abs": "https://arxiv.org/abs/2504.19519", "authors": ["Ke Hong", "Xiuhong Li", "Minxu Liu", "Qiuli Mao", "Tianqi Wu", "Zixiao Huang", "Lufang Chen", "Zhong Wang", "Yichong Zhang", "Zhenhua Zhu", "Guohao Dai", "Yu Wang"], "title": "FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation", "categories": ["cs.DC", "cs.CL", "cs.LG"], "comment": "17 pages, 11 figures, 4 tables", "summary": "Generative models have achieved remarkable success across various\napplications, driving the demand for multi-GPU computing. Inter-GPU\ncommunication becomes a bottleneck in multi-GPU computing systems, particularly\non consumer-grade GPUs. By exploiting concurrent hardware execution,\noverlapping computation and communication latency is an effective technique for\nmitigating the communication overhead. We identify that an efficient and\nadaptable overlapping design should satisfy (1) tile-wise overlapping to\nmaximize the overlapping opportunity, (2) interference-free computation to\nmaintain the original computational performance, and (3) communication\nagnosticism to reduce the development burden against varying communication\nprimitives. Nevertheless, current designs fail to simultaneously optimize for\nall of those features.\n  To address the issue, we propose FlashOverlap, a lightweight design\ncharacterized by tile-wise overlapping, interference-free computation, and\ncommunication agnosticism. FlashOverlap utilizes a novel signaling mechanism to\nidentify tile-wise data dependency without interrupting the computation\nprocess, and reorders data to contiguous addresses, enabling communication by\nsimply calling NCCL APIs. Experiments show that such a lightweight design\nachieves up to 1.65x speedup, outperforming existing works in most cases.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFlashOverlap\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5206\u5757\u91cd\u53e0\u3001\u65e0\u5e72\u6270\u8ba1\u7b97\u548c\u901a\u4fe1\u65e0\u5173\u6027\u4f18\u5316\u591aGPU\u7cfb\u7edf\u4e2d\u7684\u901a\u4fe1\u74f6\u9888\uff0c\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u63d0\u53471.65\u500d\u3002", "motivation": "\u591aGPU\u8ba1\u7b97\u4e2d\u7684\u901a\u4fe1\u74f6\u9888\uff0c\u5c24\u5176\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\uff0c\u73b0\u6709\u8bbe\u8ba1\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u5206\u5757\u91cd\u53e0\u3001\u65e0\u5e72\u6270\u8ba1\u7b97\u548c\u901a\u4fe1\u65e0\u5173\u6027\uff0c\u5bfc\u81f4\u6548\u7387\u4e0d\u8db3\u3002", "method": "FlashOverlap\u91c7\u7528\u65b0\u578b\u4fe1\u53f7\u673a\u5236\u8bc6\u522b\u5206\u5757\u6570\u636e\u4f9d\u8d56\uff0c\u4e0d\u4e2d\u65ad\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u5e76\u91cd\u65b0\u6392\u5217\u6570\u636e\u81f3\u8fde\u7eed\u5730\u5740\uff0c\u901a\u8fc7\u8c03\u7528NCCL API\u5b9e\u73b0\u901a\u4fe1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u8bbe\u8ba1\u6700\u9ad8\u5b9e\u73b01.65\u500d\u52a0\u901f\uff0c\u4f18\u4e8e\u73b0\u6709\u591a\u6570\u65b9\u6848\u3002", "conclusion": "FlashOverlap\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u591aGPU\u901a\u4fe1\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2504.18814", "pdf": "https://arxiv.org/pdf/2504.18814", "abs": "https://arxiv.org/abs/2504.18814", "authors": ["Abdelaziz Amara korba", "Nour Elislem Karabadji", "Yacine Ghamri-Doudane"], "title": "Zero-Day Botnet Attack Detection in IoV: A Modular Approach Using Isolation Forests and Particle Swarm Optimization", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The Internet of Vehicles (IoV) is transforming transportation by enhancing\nconnectivity and enabling autonomous driving. However, this increased\ninterconnectivity introduces new security vulnerabilities. Bot malware and\ncyberattacks pose significant risks to Connected and Autonomous Vehicles\n(CAVs), as demonstrated by real-world incidents involving remote vehicle system\ncompromise. To address these challenges, we propose an edge-based Intrusion\nDetection System (IDS) that monitors network traffic to and from CAVs. Our\ndetection model is based on a meta-ensemble classifier capable of recognizing\nknown (Nday) attacks and detecting previously unseen (zero-day) attacks. The\napproach involves training multiple Isolation Forest (IF) models on\nMulti-access Edge Computing (MEC) servers, with each IF specialized in\nidentifying a specific type of botnet attack. These IFs, either trained locally\nor shared by other MEC nodes, are then aggregated using a Particle Swarm\nOptimization (PSO) based stacking strategy to construct a robust\nmeta-classifier. The proposed IDS has been evaluated on a vehicular botnet\ndataset, achieving an average detection rate of 92.80% for N-day attacks and\n77.32% for zero-day attacks. These results highlight the effectiveness of our\nsolution in detecting both known and emerging threats, providing a scalable and\nadaptive defense mechanism for CAVs within the IoV ecosystem.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fb9\u7f18\u7684\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff08IDS\uff09\uff0c\u901a\u8fc7\u5143\u96c6\u6210\u5206\u7c7b\u5668\u68c0\u6d4b\u8f66\u8054\u7f51\uff08IoV\uff09\u4e2d\u7684\u5df2\u77e5\u548c\u672a\u77e5\u653b\u51fb\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a92.8%\u7684\u5df2\u77e5\u653b\u51fb\u68c0\u6d4b\u7387\u548c77.32%\u7684\u672a\u77e5\u653b\u51fb\u68c0\u6d4b\u7387\u3002", "motivation": "\u968f\u7740\u8f66\u8054\u7f51\uff08IoV\uff09\u7684\u53d1\u5c55\uff0c\u4e92\u8054\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08CAV\uff09\u9762\u4e34\u65e5\u76ca\u4e25\u91cd\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u5982\u50f5\u5c38\u7f51\u7edc\u653b\u51fb\u3002\u73b0\u6709\u7cfb\u7edf\u96be\u4ee5\u540c\u65f6\u5e94\u5bf9\u5df2\u77e5\u548c\u672a\u77e5\u653b\u51fb\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u9632\u62a4\u65b9\u6848\u3002", "method": "\u5229\u7528\u591a\u63a5\u5165\u8fb9\u7f18\u8ba1\u7b97\uff08MEC\uff09\u670d\u52a1\u5668\u8bad\u7ec3\u591a\u4e2a\u9694\u79bb\u68ee\u6797\uff08IF\uff09\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u4e13\u6ce8\u4e8e\u68c0\u6d4b\u7279\u5b9a\u7c7b\u578b\u7684\u50f5\u5c38\u7f51\u7edc\u653b\u51fb\uff0c\u5e76\u901a\u8fc7\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u7684\u5806\u53e0\u7b56\u7565\u6784\u5efa\u5143\u5206\u7c7b\u5668\u3002", "result": "\u5728\u8f66\u7528\u50f5\u5c38\u7f51\u7edc\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5bf9\u5df2\u77e5\u653b\u51fb\u7684\u5e73\u5747\u68c0\u6d4b\u7387\u4e3a92.80%\uff0c\u5bf9\u672a\u77e5\u653b\u51fb\u4e3a77.32%\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5IDS\u65b9\u6848\u4e3aIoV\u4e2d\u7684CAV\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u81ea\u9002\u5e94\u7684\u5b89\u5168\u9632\u5fa1\uff0c\u80fd\u591f\u540c\u65f6\u5e94\u5bf9\u5df2\u77e5\u548c\u65b0\u5174\u5a01\u80c1\u3002"}}
{"id": "2504.19538", "pdf": "https://arxiv.org/pdf/2504.19538", "abs": "https://arxiv.org/abs/2504.19538", "authors": ["Yasir Ghunaim", "Andr\u00e9s Villa", "Gergo Ignacz", "Gyorgy Szekely", "Motasem Alfarra", "Bernard Ghanem"], "title": "Towards Faster and More Compact Foundation Models for Molecular Property Prediction", "categories": ["cs.LG", "q-bio.BM"], "comment": null, "summary": "Advancements in machine learning for molecular property prediction have\nimproved accuracy but at the expense of higher computational cost and longer\ntraining times. Recently, the Joint Multi-domain Pre-training (JMP) foundation\nmodel has demonstrated strong performance across various downstream tasks with\nreduced training time over previous models. Despite JMP's advantages,\nfine-tuning it on molecular datasets ranging from small-scale to large-scale\nrequires considerable time and computational resources. In this work, we\ninvestigate strategies to enhance efficiency by reducing model size while\npreserving performance. To better understand the model's efficiency, we analyze\nthe layer contributions of JMP and find that later interaction blocks provide\ndiminishing returns, suggesting an opportunity for model compression. We\nexplore block reduction strategies by pruning the pre-trained model and\nevaluating its impact on efficiency and accuracy during fine-tuning. Our\nanalysis reveals that removing two interaction blocks results in a minimal\nperformance drop, reducing the model size by 32% while increasing inference\nthroughput by 1.3x. These results suggest that JMP-L is over-parameterized and\nthat a smaller, more efficient variant can achieve comparable performance with\nlower computational cost. Our study provides insights for developing lighter,\nfaster, and more scalable foundation models for molecular and materials\ndiscovery. The code is publicly available at:\nhttps://github.com/Yasir-Ghunaim/efficient-jmp.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u526a\u679d\u9884\u8bad\u7ec3\u6a21\u578b\u6765\u63d0\u5347\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u6548\u7387\u7684\u65b9\u6cd5\uff0c\u7814\u7a76\u53d1\u73b0\u51cf\u5c11\u4e24\u4e2a\u4ea4\u4e92\u6a21\u5757\u540e\u6a21\u578b\u5927\u5c0f\u964d\u4f4e32%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53471.3\u500d\uff0c\u4e14\u6027\u80fd\u635f\u5931\u6781\u5c0f\u3002", "motivation": "\u5c3d\u7ba1JMP\u57fa\u7840\u6a21\u578b\u5728\u591a\u9886\u57df\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u5206\u5b50\u6570\u636e\u96c6\u4e0a\u7684\u5fae\u8c03\u4ecd\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u95f4\u3002\u4e3a\u4e86\u63d0\u5347\u6548\u7387\uff0c\u4f5c\u8005\u5e0c\u671b\u5728\u4e0d\u663e\u8457\u635f\u5931\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u3002", "method": "\u901a\u8fc7\u5206\u6790JMP\u6a21\u578b\u4e2d\u5404\u5c42\u7684\u8d21\u732e\uff0c\u53d1\u73b0\u540e\u671f\u4ea4\u4e92\u6a21\u5757\u7684\u6536\u76ca\u9012\u51cf\uff0c\u56e0\u6b64\u91c7\u7528\u526a\u679d\u7b56\u7565\u51cf\u5c11\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u53c2\u6570\uff0c\u5e76\u5728\u5fae\u8c03\u9636\u6bb5\u8bc4\u4f30\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "result": "\u53bb\u9664\u4e24\u4e2a\u4ea4\u4e92\u6a21\u5757\u540e\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c11\u4e8632%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u4e861.3\u500d\uff0c\u4e14\u6027\u80fd\u4e0b\u964d\u6781\u5c0f\uff0c\u8868\u660eJMP-L\u5b58\u5728\u53c2\u6570\u5197\u4f59\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u526a\u679d\u53ef\u4ee5\u5f00\u53d1\u66f4\u8f7b\u91cf\u3001\u5feb\u901f\u4e14\u53ef\u6269\u5c55\u7684\u5206\u5b50\u548c\u6750\u6599\u53d1\u73b0\u57fa\u7840\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.19583", "pdf": "https://arxiv.org/pdf/2504.19583", "abs": "https://arxiv.org/abs/2504.19583", "authors": ["Hanlu Zhang", "Yumeng Ma", "Shuo Wang", "Guiran Liu", "Binrong Zhu"], "title": "Graph-Based Spectral Decomposition for Parameter Coordination in Language Model Fine-Tuning", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "This paper proposes a parameter collaborative optimization algorithm for\nlarge language models, enhanced with graph spectral analysis. The goal is to\nimprove both fine-tuning efficiency and structural awareness during training.\nIn the proposed method, the parameters of a pre-trained language model are\ntreated as nodes in a graph. A weighted graph is constructed, and Laplacian\nspectral decomposition is applied to enable frequency-domain modeling and\nstructural representation of the parameter space. Based on this structure, a\njoint loss function is designed. It combines the task loss with a spectral\nregularization term to facilitate collaborative updates among parameters. In\naddition, a spectral filtering mechanism is introduced during the optimization\nphase. This mechanism adjusts gradients in a structure-aware manner, enhancing\nthe model's training stability and convergence behavior. The method is\nevaluated on multiple tasks, including traditional fine-tuning comparisons,\nfew-shot generalization tests, and convergence speed analysis. In all settings,\nthe proposed approach demonstrates superior performance. The experimental\nresults confirm that the spectral collaborative optimization framework\neffectively reduces parameter perturbations and improves fine-tuning quality\nwhile preserving overall model performance. This work contributes significantly\nto the field of artificial intelligence by advancing parameter-efficient\ntraining methodologies for large-scale models, reinforcing the importance of\nstructural signal processing in deep learning optimization, and offering a\nrobust, generalizable framework for enhancing language model adaptability and\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u8c31\u5206\u6790\u7684\u5927\u8bed\u8a00\u6a21\u578b\u53c2\u6570\u534f\u540c\u4f18\u5316\u7b97\u6cd5\uff0c\u65e8\u5728\u63d0\u5347\u5fae\u8c03\u6548\u7387\u548c\u8bad\u7ec3\u4e2d\u7684\u7ed3\u6784\u611f\u77e5\u80fd\u529b\uff0c\u901a\u8fc7\u9891\u57df\u5efa\u6a21\u548c\u8c31\u6b63\u5219\u5316\u5b9e\u73b0\u53c2\u6570\u534f\u540c\u66f4\u65b0\uff0c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5fae\u8c03\u6548\u7387\u548c\u7ed3\u6784\u611f\u77e5\u80fd\u529b\uff0c\u89e3\u51b3\u8bad\u7ec3\u4e2d\u7684\u53c2\u6570\u6270\u52a8\u95ee\u9898\uff0c\u589e\u5f3a\u6a21\u578b\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002", "method": "\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\u89c6\u4e3a\u56fe\u8282\u70b9\uff0c\u6784\u5efa\u52a0\u6743\u56fe\u5e76\u5e94\u7528\u62c9\u666e\u62c9\u65af\u8c31\u5206\u89e3\uff0c\u8bbe\u8ba1\u7ed3\u5408\u4efb\u52a1\u635f\u5931\u548c\u8c31\u6b63\u5219\u5316\u7684\u8054\u5408\u635f\u5931\u51fd\u6570\uff0c\u5e76\u5728\u4f18\u5316\u9636\u6bb5\u5f15\u5165\u8c31\u6ee4\u6ce2\u673a\u5236\u3002", "result": "\u5728\u591a\u4efb\u52a1\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6709\u6548\u51cf\u5c11\u53c2\u6570\u6270\u52a8\u5e76\u63d0\u5347\u5fae\u8c03\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u8c31\u534f\u540c\u4f18\u5316\u6846\u67b6\u63a8\u52a8\u4e86\u5927\u89c4\u6a21\u6a21\u578b\u7684\u9ad8\u6548\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5f3a\u8c03\u4e86\u7ed3\u6784\u4fe1\u53f7\u5904\u7406\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u63d0\u4f9b\u4e86\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u9002\u5e94\u6027\u548c\u6027\u80fd\u7684\u901a\u7528\u6846\u67b6\u3002"}}
{"id": "2504.19561", "pdf": "https://arxiv.org/pdf/2504.19561", "abs": "https://arxiv.org/abs/2504.19561", "authors": ["Rom N. Parnichkun", "Neehal Tumma", "Armin W. Thomas", "Alessandro Moro", "Qi An", "Taiji Suzuki", "Atsushi Yamashita", "Michael Poli", "Stefano Massaroli"], "title": "Quantifying Memory Utilization with Effective State-Size", "categories": ["cs.LG"], "comment": null, "summary": "The need to develop a general framework for architecture analysis is becoming\nincreasingly important, given the expanding design space of sequence models. To\nthis end, we draw insights from classical signal processing and control theory,\nto develop a quantitative measure of \\textit{memory utilization}: the internal\nmechanisms through which a model stores past information to produce future\noutputs. This metric, which we call \\textbf{\\textit{effective state-size}}\n(ESS), is tailored to the fundamental class of systems with\n\\textit{input-invariant} and \\textit{input-varying linear operators},\nencompassing a variety of computational units such as variants of attention,\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\neither relies on raw operator visualizations (e.g. attention maps), or simply\nthe total \\textit{memory capacity} (i.e. cache size) of a model, our metrics\nprovide highly interpretable and actionable measurements. In particular, we\nshow how ESS can be leveraged to improve initialization strategies, inform\nnovel regularizers and advance the performance-efficiency frontier through\nmodel distillation. Furthermore, we demonstrate that the effect of context\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\ndifferences in how large language models utilize their available memory to\nrecall information. Overall, we find that ESS provides valuable insights into\nthe dynamics that dictate memory utilization, enabling the design of more\nefficient and effective sequence models.", "AI": {"tldr": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u2018\u6709\u6548\u72b6\u6001\u5927\u5c0f\u2019\uff08ESS\uff09\u7684\u5b9a\u91cf\u6307\u6807\uff0c\u7528\u4e8e\u6d4b\u91cf\u5e8f\u5217\u6a21\u578b\u4e2d\u7684\u5185\u5b58\u5229\u7528\u7387\uff0c\u4ece\u800c\u5e2e\u52a9\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u67b6\u6784\u3002", "motivation": "\u968f\u7740\u5e8f\u5217\u6a21\u578b\u8bbe\u8ba1\u7a7a\u95f4\u7684\u6269\u5c55\uff0c\u9700\u8981\u4e00\u79cd\u901a\u7528\u6846\u67b6\u6765\u5206\u6790\u5176\u67b6\u6784\uff0c\u5c24\u5176\u662f\u5185\u5b58\u5229\u7528\u673a\u5236\u3002\u57fa\u4e8e\u4fe1\u53f7\u5904\u7406\u548c\u63a7\u5236\u7406\u8bba\u7684\u89c1\u89e3\uff0c\u7814\u7a76\u8005\u65e8\u5728\u91cf\u5316\u6a21\u578b\u5982\u4f55\u5b58\u50a8\u8fc7\u53bb\u4fe1\u606f\u4ee5\u751f\u6210\u672a\u6765\u8f93\u51fa\u3002", "method": "\u7814\u7a76\u8005\u5f00\u53d1\u4e86ESS\u6307\u6807\uff0c\u9002\u7528\u4e8e\u5177\u6709\u8f93\u5165\u4e0d\u53d8\u548c\u8f93\u5165\u53d8\u5316\u7ebf\u6027\u7b97\u5b50\u7684\u7cfb\u7edf\uff0c\u6db5\u76d6\u6ce8\u610f\u529b\u3001\u5377\u79ef\u548c\u5faa\u73af\u5355\u5143\u7b49\u8ba1\u7b97\u5355\u5143\u3002\u4e0e\u4f20\u7edf\u7684\u53ef\u89c6\u5316\u65b9\u6cd5\u6216\u7b80\u5355\u5185\u5b58\u5bb9\u91cf\u6d4b\u91cf\u4e0d\u540c\uff0cESS\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u4e14\u53ef\u64cd\u4f5c\u7684\u91cf\u5316\u7ed3\u679c\u3002", "result": "ESS\u53ef\u7528\u4e8e\u6539\u8fdb\u521d\u59cb\u5316\u7b56\u7565\u3001\u8bbe\u8ba1\u65b0\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u84b8\u998f\u63d0\u5347\u6027\u80fd\u6548\u7387\u3002\u6b64\u5916\uff0cESS\u63ed\u793a\u4e86\u4e0d\u540c\u67b6\u6784\u4e0b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5229\u7528\u5185\u5b58\u7684\u5dee\u5f02\uff0c\u5c24\u5176\u662f\u5728\u4e0a\u4e0b\u6587\u5206\u9694\u7b26\uff08\u5982\u53e5\u5c3e\u6807\u8bb0\uff09\u7684\u5f71\u54cd\u65b9\u9762\u3002", "conclusion": "ESS\u4e3a\u7406\u89e3\u5185\u5b58\u5229\u7528\u7684\u52a8\u6001\u673a\u5236\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6d1e\u89c1\uff0c\u6709\u52a9\u4e8e\u8bbe\u8ba1\u66f4\u9ad8\u6548\u3001\u66f4\u6709\u6548\u7684\u5e8f\u5217\u6a21\u578b\u3002"}}
{"id": "2504.19730", "pdf": "https://arxiv.org/pdf/2504.19730", "abs": "https://arxiv.org/abs/2504.19730", "authors": ["Wenhan Mu", "Ling Xu", "Shuren Pei", "Le Mi", "Huichi Zhou"], "title": "Evaluate-and-Purify: Fortifying Code Language Models Against Adversarial Attacks Using LLM-as-a-Judge", "categories": ["cs.SE", "cs.CL"], "comment": "25 pages, 6 figures", "summary": "The widespread adoption of code language models in software engineering tasks\nhas exposed vulnerabilities to adversarial attacks, especially the identifier\nsubstitution attacks. Although existing identifier substitution attackers\ndemonstrate high success rates, they often produce adversarial examples with\nunnatural code patterns. In this paper, we systematically assess the quality of\nadversarial examples using LLM-as-a-Judge. Our analysis reveals that over 80%\nof adversarial examples generated by state-of-the-art identifier substitution\nattackers (e.g., ALERT) are actually detectable. Based on this insight, we\npropose EP-Shield, a unified framework for evaluating and purifying identifier\nsubstitution attacks via naturalness-aware reasoning. Specifically, we first\nevaluate the naturalness of code and identify the perturbed adversarial code,\nthen purify it so that the victim model can restore correct prediction.\nExtensive experiments demonstrate the superiority of EP-Shield over adversarial\nfine-tuning (up to 83.36% improvement) and its lightweight design 7B\nparameters) with GPT-4-level performance.", "AI": {"tldr": "EP-Shield\u6846\u67b6\u901a\u8fc7\u81ea\u7136\u611f\u77e5\u63a8\u7406\u8bc4\u4f30\u5e76\u51c0\u5316\u6807\u8bc6\u7b26\u66ff\u6362\u653b\u51fb\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff08\u5982ALERT\uff09\uff0c\u80fd\u68c0\u6d4b80%\u4ee5\u4e0a\u7684\u5bf9\u6297\u6837\u672c\uff0c\u5e76\u5728\u8f7b\u91cf\u8bbe\u8ba1\u4e0b\u5b9e\u73b0GPT-4\u7ea7\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6807\u8bc6\u7b26\u66ff\u6362\u653b\u51fb\u751f\u6210\u7684\u5bf9\u6297\u6837\u672c\u5b58\u5728\u4e0d\u81ea\u7136\u4ee3\u7801\u6a21\u5f0f\uff0c\u5bb9\u6613\u88ab\u68c0\u6d4b\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u8bc4\u4f30\u5e76\u51c0\u5316\u8fd9\u4e9b\u653b\u51fb\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faEP-Shield\u6846\u67b6\uff1a1) \u8bc4\u4f30\u4ee3\u7801\u81ea\u7136\u6027\u5e76\u8bc6\u522b\u5bf9\u6297\u4ee3\u7801\uff1b2) \u51c0\u5316\u5bf9\u6297\u6837\u672c\u4ee5\u4f7f\u53d7\u5bb3\u8005\u6a21\u578b\u6062\u590d\u6b63\u786e\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eEP-Shield\u4f18\u4e8e\u5bf9\u6297\u5fae\u8c03\uff08\u63d0\u5347\u8fbe83.36%\uff09\uff0c\u4e14\u8f7b\u91cf\u8bbe\u8ba1\uff087B\u53c2\u6570\uff09\u6027\u80fd\u8fbeGPT-4\u7ea7\u522b\u3002", "conclusion": "EP-Shield\u6709\u6548\u63d0\u5347\u5bf9\u6297\u6837\u672c\u7684\u81ea\u7136\u6027\u548c\u68c0\u6d4b\u80fd\u529b\uff0c\u4e3a\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u63d0\u4f9b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18827", "pdf": "https://arxiv.org/pdf/2504.18827", "abs": "https://arxiv.org/abs/2504.18827", "authors": ["Teeradaj Racharak", "Chaiyong Ragkhitwetsagul", "Chommakorn Sontesadisai", "Thanwadee Sunetnanta"], "title": "Test It Before You Trust It: Applying Software Testing for Trustworthy In-context Learning", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "In-context learning (ICL) has emerged as a powerful capability of large\nlanguage models (LLMs), enabling them to perform new tasks based on a few\nprovided examples without explicit fine-tuning. Despite their impressive\nadaptability, these models remain vulnerable to subtle adversarial\nperturbations and exhibit unpredictable behavior when faced with linguistic\nvariations. Inspired by software testing principles, we introduce a software\ntesting-inspired framework, called MMT4NL, for evaluating the trustworthiness\nof in-context learning by utilizing adversarial perturbations and software\ntesting techniques. It includes diverse evaluation aspects of linguistic\ncapabilities for testing the ICL capabilities of LLMs. MMT4NL is built around\nthe idea of crafting metamorphic adversarial examples from a test set in order\nto quantify and pinpoint bugs in the designed prompts of ICL. Our philosophy is\nto treat any LLM as software and validate its functionalities just like testing\nthe software. Finally, we demonstrate applications of MMT4NL on the sentiment\nanalysis and question-answering tasks. Our experiments could reveal various\nlinguistic bugs in state-of-the-art LLMs.", "AI": {"tldr": "MMT4NL\u662f\u4e00\u4e2a\u57fa\u4e8e\u8f6f\u4ef6\u6d4b\u8bd5\u539f\u5219\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u7684\u53ef\u4fe1\u5ea6\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u6270\u52a8\u548c\u8f6f\u4ef6\u6d4b\u8bd5\u6280\u672f\u6765\u53d1\u73b0\u6a21\u578b\u5728\u8bed\u8a00\u53d8\u4f53\u4e2d\u7684\u6f0f\u6d1e\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u80fd\u529b\uff0c\u4f46\u5176\u5728\u9762\u5bf9\u7ec6\u5fae\u5bf9\u6297\u6027\u6270\u52a8\u6216\u8bed\u8a00\u53d8\u4f53\u65f6\u4ecd\u8868\u73b0\u51fa\u4e0d\u53ef\u9884\u6d4b\u7684\u884c\u4e3a\uff0c\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u5316\u65b9\u6cd5\u6765\u8bc4\u4f30\u5176\u53ef\u4fe1\u5ea6\u3002", "method": "MMT4NL\u5229\u7528\u5bf9\u6297\u6027\u6270\u52a8\u548c\u8f6f\u4ef6\u6d4b\u8bd5\u6280\u672f\uff0c\u901a\u8fc7\u6784\u5efa\u53d8\u5f62\u5bf9\u6297\u6837\u672c\uff0c\u4ece\u591a\u6837\u5316\u8bed\u8a00\u80fd\u529b\u89d2\u5ea6\u6d4b\u8bd5LLMs\u7684ICL\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u5728\u60c5\u611f\u5206\u6790\u548c\u95ee\u7b54\u4efb\u52a1\u4e2d\u5e94\u7528MMT4NL\uff0c\u6210\u529f\u63ed\u793a\u4e86\u5f53\u524d\u5148\u8fdbLLMs\u4e2d\u7684\u591a\u79cd\u8bed\u8a00\u6f0f\u6d1e\u3002", "conclusion": "MMT4NL\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdbLLMs\u7684ICL\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u51f8\u663e\u4e86\u8f6f\u4ef6\u6d4b\u8bd5\u65b9\u6cd5\u5728\u9a8c\u8bc1\u6a21\u578b\u529f\u80fd\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2504.19754", "pdf": "https://arxiv.org/pdf/2504.19754", "abs": "https://arxiv.org/abs/2504.19754", "authors": ["Carlo Merola", "Jaspinder Singh"], "title": "Reconstructing Context: Evaluating Advanced Chunking Strategies for Retrieval-Augmented Generation", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "13 pages, 2 figures, Second Workshop on Knowledge-Enhanced\n  Information Retrieval, ECIR 2025", "summary": "Retrieval-augmented generation (RAG) has become a transformative approach for\nenhancing large language models (LLMs) by grounding their outputs in external\nknowledge sources. Yet, a critical question persists: how can vast volumes of\nexternal knowledge be managed effectively within the input constraints of LLMs?\nTraditional methods address this by chunking external documents into smaller,\nfixed-size segments. While this approach alleviates input limitations, it often\nfragments context, resulting in incomplete retrieval and diminished coherence\nin generation. To overcome these shortcomings, two advanced techniques, late\nchunking and contextual retrieval, have been introduced, both aiming to\npreserve global context. Despite their potential, their comparative strengths\nand limitations remain unclear. This study presents a rigorous analysis of late\nchunking and contextual retrieval, evaluating their effectiveness and\nefficiency in optimizing RAG systems. Our results indicate that contextual\nretrieval preserves semantic coherence more effectively but requires greater\ncomputational resources. In contrast, late chunking offers higher efficiency\nbut tends to sacrifice relevance and completeness.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5bf9\u2018\u5ef6\u8fdf\u5206\u5757\u2019\u548c\u2018\u4e0a\u4e0b\u6587\u68c0\u7d22\u2019\u4e24\u79cd\u6280\u672f\u7684\u5bf9\u6bd4\u5206\u6790\uff0c\u8bc4\u4f30\u4e86\u5b83\u4eec\u5728\u4f18\u5316\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u4e2d\u7684\u6548\u679c\u548c\u6548\u7387\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0a\u4e0b\u6587\u68c0\u7d22\u5728\u4fdd\u6301\u8bed\u4e49\u8fde\u8d2f\u6027\u4e0a\u66f4\u6709\u6548\uff0c\u4f46\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u66f4\u9ad8\uff1b\u800c\u5ef6\u8fdf\u5206\u5757\u6548\u7387\u66f4\u9ad8\uff0c\u4f46\u4f1a\u727a\u7272\u76f8\u5173\u6027\u548c\u5b8c\u6574\u6027\u3002", "motivation": "\u89e3\u51b3RAG\u7cfb\u7edf\u4e2d\u5982\u4f55\u6709\u6548\u7ba1\u7406\u5927\u91cf\u5916\u90e8\u77e5\u8bc6\uff0c\u4ee5\u514b\u670d\u4f20\u7edf\u5206\u5757\u65b9\u6cd5\u5bfc\u81f4\u7684\u4fe1\u606f\u788e\u7247\u5316\u95ee\u9898\uff0c\u5e76\u6bd4\u8f83\u4e24\u79cd\u65b0\u5174\u6280\u672f\uff08\u5ef6\u8fdf\u5206\u5757\u548c\u4e0a\u4e0b\u6587\u68c0\u7d22\uff09\u7684\u4f18\u7f3a\u70b9\u3002", "method": "\u91c7\u7528\u4e25\u683c\u7684\u5bf9\u6bd4\u5206\u6790\u65b9\u6cd5\uff0c\u8bc4\u4f30\u5ef6\u8fdf\u5206\u5757\u548c\u4e0a\u4e0b\u6587\u68c0\u7d22\u5728RAG\u7cfb\u7edf\u4e2d\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u6548\u679c\u548c\u6548\u7387\u3002", "result": "\u4e0a\u4e0b\u6587\u68c0\u7d22\u5728\u4fdd\u6301\u8bed\u4e49\u8fde\u8d2f\u6027\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\uff1b\u5ef6\u8fdf\u5206\u5757\u6548\u7387\u66f4\u9ad8\uff0c\u4f46\u4f1a\u964d\u4f4e\u76f8\u5173\u6027\u548c\u5b8c\u6574\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3aRAG\u7cfb\u7edf\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5f3a\u8c03\u6839\u636e\u5b9e\u9645\u9700\u6c42\u5728\u8bed\u4e49\u8fde\u8d2f\u6027\u548c\u6548\u7387\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002"}}
{"id": "2504.19602", "pdf": "https://arxiv.org/pdf/2504.19602", "abs": "https://arxiv.org/abs/2504.19602", "authors": ["Kitsuya Azuma", "Takayuki Nishio", "Yuichi Kitagawa", "Wakako Nakano", "Takahito Tanimura"], "title": "Soft-Label Caching and Sharpening for Communication-Efficient Federated Distillation", "categories": ["cs.LG"], "comment": null, "summary": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.", "AI": {"tldr": "SCARLET\u6846\u67b6\u901a\u8fc7\u540c\u6b65\u8f6f\u6807\u7b7e\u7f13\u5b58\u548c\u589e\u5f3a\u7684\u71b5\u51cf\u5c11\u805a\u5408\u673a\u5236\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8054\u90a6\u5b66\u4e60\u7684\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u5728\u901a\u4fe1\u5f00\u9500\u9ad8\u548c\u6a21\u578b\u5f02\u6784\u6027\u53d7\u9650\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u84b8\u998f\u65b9\u6cd5\u4e2d\u5197\u4f59\u4f20\u8f93\u7684\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u540c\u6b65\u8f6f\u6807\u7b7e\u7f13\u5b58\u548c\u589e\u5f3a\u7684\u71b5\u51cf\u5c11\u805a\u5408\uff08Enhanced ERA\uff09\u673a\u5236\uff0c\u51cf\u5c11\u5197\u4f59\u901a\u4fe1\u3002", "result": "\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u901a\u4fe1\u6210\u672c\u964d\u4f4e50%\uff0c\u5e76\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u573a\u666f\u4e2d\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "SCARLET\u5728\u901a\u4fe1\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u84b8\u998f\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18847", "pdf": "https://arxiv.org/pdf/2504.18847", "abs": "https://arxiv.org/abs/2504.18847", "authors": ["Hidayet Ersin Dursun", "Yusuf G\u00fcven", "Tufan Kumbasar"], "title": "Imitation Learning for Autonomous Driving: Insights from Real-World Testing", "categories": ["cs.RO", "cs.AI"], "comment": "In International Congress on Human-Computer Interaction, Optimization\n  and Robotic Applications, 2025", "summary": "This work focuses on the design of a deep learning-based autonomous driving\nsystem deployed and tested on the real-world MIT Racecar to assess its\neffectiveness in driving scenarios. The Deep Neural Network (DNN) translates\nraw image inputs into real-time steering commands in an end-to-end learning\nfashion, following the imitation learning framework. The key design challenge\nis to ensure that DNN predictions are accurate and fast enough, at a high\nsampling frequency, and result in smooth vehicle operation under different\noperating conditions. In this study, we design and compare various DNNs, to\nidentify the most effective approach for real-time autonomous driving. In\ndesigning the DNNs, we adopted an incremental design approach that involved\nenhancing the model capacity and dataset to address the challenges of\nreal-world driving scenarios. We designed a PD system, CNN, CNN-LSTM, and\nCNN-NODE, and evaluated their performance on the real-world MIT Racecar. While\nthe PD system handled basic lane following, it struggled with sharp turns and\nlighting variations. The CNN improved steering but lacked temporal awareness,\nwhich the CNN-LSTM addressed as it resulted in smooth driving performance. The\nCNN-NODE performed similarly to the CNN-LSTM in handling driving dynamics, yet\nwith slightly better driving performance. The findings of this research\nhighlight the importance of iterative design processes in developing robust\nDNNs for autonomous driving applications. The experimental video is available\nat https://www.youtube.com/watch?v=FNNYgU--iaY.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728MIT Racecar\u4e0a\u7684\u5b9e\u65f6\u6027\u80fd\uff0c\u901a\u8fc7\u6bd4\u8f83\u591a\u79cdDNN\u6a21\u578b\uff08\u5982PD\u7cfb\u7edf\u3001CNN\u3001CNN-LSTM\u548cCNN-NODE\uff09\uff0c\u53d1\u73b0CNN-LSTM\u548cCNN-NODE\u5728\u52a8\u6001\u9a7e\u9a76\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u4e3a\u4e86\u5f00\u53d1\u80fd\u591f\u5728\u590d\u6742\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\u4e2d\u5b9e\u65f6\u3001\u51c6\u786e\u4e14\u7a33\u5b9a\u8fd0\u884c\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u8bbe\u8ba1\u5e76\u6bd4\u8f83\u4e86\u591a\u79cdDNN\u6a21\u578b\uff08PD\u7cfb\u7edf\u3001CNN\u3001CNN-LSTM\u3001CNN-NODE\uff09\uff0c\u901a\u8fc7\u589e\u91cf\u5f0f\u8bbe\u8ba1\u9010\u6b65\u4f18\u5316\u6a21\u578b\u80fd\u529b\u3002", "result": "CNN-LSTM\u548cCNN-NODE\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u52a8\u6001\u9a7e\u9a76\u4e2d\uff1b\u800cPD\u7cfb\u7edf\u4ec5\u9002\u5408\u57fa\u7840\u8f66\u9053\u8ddf\u968f\uff0cCNN\u5219\u7f3a\u4e4f\u65f6\u95f4\u5e8f\u5217\u611f\u77e5\u3002", "conclusion": "\u8fed\u4ee3\u8bbe\u8ba1\u8fc7\u7a0b\u5bf9\u5f00\u53d1\u7a33\u5065\u7684\u81ea\u52a8\u9a7e\u9a76DNN\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0cCNN-LSTM\u548cCNN-NODE\u5728\u6b64\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2504.19621", "pdf": "https://arxiv.org/pdf/2504.19621", "abs": "https://arxiv.org/abs/2504.19621", "authors": ["Haroui Ma", "Francesco Quinzan", "Theresa Willem", "Stefan Bauer"], "title": "AI Alignment in Medical Imaging: Unveiling Hidden Biases Through Counterfactual Analysis", "categories": ["cs.LG", "eess.IV", "stat.ML"], "comment": null, "summary": "Machine learning (ML) systems for medical imaging have demonstrated\nremarkable diagnostic capabilities, but their susceptibility to biases poses\nsignificant risks, since biases may negatively impact generalization\nperformance. In this paper, we introduce a novel statistical framework to\nevaluate the dependency of medical imaging ML models on sensitive attributes,\nsuch as demographics. Our method leverages the concept of counterfactual\ninvariance, measuring the extent to which a model's predictions remain\nunchanged under hypothetical changes to sensitive attributes. We present a\npractical algorithm that combines conditional latent diffusion models with\nstatistical hypothesis testing to identify and quantify such biases without\nrequiring direct access to counterfactual data. Through experiments on\nsynthetic datasets and large-scale real-world medical imaging datasets,\nincluding \\textsc{cheXpert} and MIMIC-CXR, we demonstrate that our approach\naligns closely with counterfactual fairness principles and outperforms standard\nbaselines. This work provides a robust tool to ensure that ML diagnostic\nsystems generalize well, e.g., across demographic groups, offering a critical\nstep towards AI safety in healthcare. Code:\nhttps://github.com/Neferpitou3871/AI-Alignment-Medical-Imaging.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7edf\u8ba1\u6846\u67b6\u8bc4\u4f30\u533b\u5b66\u5f71\u50cfML\u6a21\u578b\u5bf9\u654f\u611f\u5c5e\u6027\u7684\u4f9d\u8d56\u6027\uff0c\u5229\u7528\u53cd\u4e8b\u5b9e\u4e0d\u53d8\u6027\u6982\u5ff5\uff0c\u7ed3\u5408\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u7edf\u8ba1\u5047\u8bbe\u68c0\u9a8c\uff0c\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u63d0\u5347AI\u5728\u533b\u7597\u9886\u57df\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u533b\u5b66\u5f71\u50cfML\u6a21\u578b\u7684\u8bca\u65ad\u80fd\u529b\u5f3a\uff0c\u4f46\u5bf9\u504f\u89c1\u7684\u654f\u611f\u6027\u53ef\u80fd\u5f71\u54cd\u5176\u6cdb\u5316\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8bc4\u4f30\u5176\u5bf9\u654f\u611f\u5c5e\u6027\uff08\u5982\u4eba\u53e3\u7edf\u8ba1\uff09\u7684\u4f9d\u8d56\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u6a21\u578b\u4e0e\u7edf\u8ba1\u5047\u8bbe\u68c0\u9a8c\u7684\u7b97\u6cd5\uff0c\u65e0\u9700\u76f4\u63a5\u83b7\u53d6\u53cd\u4e8b\u5b9e\u6570\u636e\u5373\u53ef\u91cf\u5316\u504f\u89c1\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u53ca\u771f\u5b9e\u533b\u5b66\u5f71\u50cf\u6570\u636e\uff08\u5982cheXpert\u548cMIMIC-CXR\uff09\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u7b26\u5408\u53cd\u4e8b\u5b9e\u516c\u5e73\u539f\u5219\u4e14\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u533b\u5b66\u5f71\u50cfML\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u5de5\u5177\uff0c\u786e\u4fdd\u5176\u5728\u8de8\u4eba\u53e3\u7edf\u8ba1\u7ec4\u7b49\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u662fAI\u533b\u7597\u5b89\u5168\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2504.18854", "pdf": "https://arxiv.org/pdf/2504.18854", "abs": "https://arxiv.org/abs/2504.18854", "authors": ["Tengfei Xing", "Xiaodan Ren", "Jie Li"], "title": "Predicting Stress in Two-phase Random Materials and Super-Resolution Method for Stress Images by Embedding Physical Information", "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.LG"], "comment": null, "summary": "Stress analysis is an important part of material design. For materials with\ncomplex microstructures, such as two-phase random materials (TRMs), material\nfailure is often accompanied by stress concentration. Phase interfaces in\ntwo-phase materials are critical for stress concentration. Therefore, the\nprediction error of stress at phase boundaries is crucial. In practical\nengineering, the pixels of the obtained material microstructure images are\nlimited, which limits the resolution of stress images generated by deep\nlearning methods, making it difficult to observe stress concentration regions.\nExisting Image Super-Resolution (ISR) technologies are all based on data-driven\nsupervised learning. However, stress images have natural physical constraints,\nwhich provide new ideas for new ISR technologies. In this study, we constructed\na stress prediction framework for TRMs. First, the framework uses a proposed\nMultiple Compositions U-net (MC U-net) to predict stress in low-resolution\nmaterial microstructures. By considering the phase interface information of the\nmicrostructure, the MC U-net effectively reduces the problem of excessive\nprediction errors at phase boundaries. Secondly, a Mixed Physics-Informed\nNeural Network (MPINN) based method for stress ISR (SRPINN) was proposed. By\nintroducing the constraints of physical information, the new method does not\nrequire paired stress images for training and can increase the resolution of\nstress images to any multiple. This enables a multiscale analysis of the stress\nconcentration regions at phase boundaries. Finally, we performed stress\nanalysis on TRMs with different phase volume fractions and loading states\nthrough transfer learning. The results show the proposed stress prediction\nframework has satisfactory accuracy and generalization ability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u4e24\u76f8\u968f\u673a\u6750\u6599\uff08TRMs\uff09\u7684\u5e94\u529b\u9884\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u591a\u6210\u5206U-net\u548c\u6df7\u5408\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08MPINN\uff09\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u5e94\u529b\u9884\u6d4b\u7cbe\u5ea6\u548c\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u9002\u7528\u4e8e\u591a\u5c3a\u5ea6\u5206\u6790\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u590d\u6742\u5fae\u7ed3\u6784\u6750\u6599\u4e2d\u56e0\u5206\u8fa8\u7387\u9650\u5236\u5bfc\u81f4\u7684\u5e94\u529b\u96c6\u4e2d\u533a\u57df\u89c2\u6d4b\u56f0\u96be\u95ee\u9898\uff0c\u5e76\u5229\u7528\u5e94\u529b\u56fe\u50cf\u7684\u7269\u7406\u7ea6\u675f\u5f00\u53d1\u65b0\u7684\u8d85\u5206\u8fa8\u7387\u6280\u672f\u3002", "method": "1. \u4f7f\u7528\u591a\u6210\u5206U-net\uff08MC U-net\uff09\u9884\u6d4b\u4f4e\u5206\u8fa8\u7387\u6750\u6599\u5fae\u7ed3\u6784\u7684\u5e94\u529b\uff0c\u51cf\u5c11\u76f8\u754c\u9762\u5904\u7684\u8bef\u5dee\uff1b2. \u63d0\u51fa\u57fa\u4e8e\u6df7\u5408\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08MPINN\uff09\u7684\u5e94\u529b\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff08SRPINN\uff09\uff0c\u65e0\u9700\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u4efb\u610f\u500d\u6570\u63d0\u5347\u5206\u8fa8\u7387\u3002", "result": "\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u5bf9\u4e0d\u540c\u76f8\u4f53\u79ef\u5206\u6570\u548c\u52a0\u8f7d\u72b6\u6001\u7684TRMs\u8fdb\u884c\u5e94\u529b\u5206\u6790\uff0c\u8bc1\u660e\u6846\u67b6\u5177\u6709\u9ad8\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u7269\u7406\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e94\u529b\u9884\u6d4b\u7684\u7cbe\u5ea6\u548c\u591a\u5c3a\u5ea6\u5206\u6790\u80fd\u529b\uff0c\u4e3a\u5de5\u7a0b\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.19638", "pdf": "https://arxiv.org/pdf/2504.19638", "abs": "https://arxiv.org/abs/2504.19638", "authors": ["Biqing Duan", "Qing Wang", "Di Liu", "Wei Zhou", "Zhenli He", "Shengfa Miao"], "title": "LODAP: On-Device Incremental Learning Via Lightweight Operations and Data Pruning", "categories": ["cs.LG", "cs.ET"], "comment": null, "summary": "Incremental learning that learns new classes over time after the model's\ndeployment is becoming increasingly crucial, particularly for industrial edge\nsystems, where it is difficult to communicate with a remote server to conduct\ncomputation-intensive learning. As more classes are expected to learn after\ntheir execution for edge devices. In this paper, we propose LODAP, a new\non-device incremental learning framework for edge systems. The key part of\nLODAP is a new module, namely Efficient Incremental Module (EIM). EIM is\ncomposed of normal convolutions and lightweight operations. During incremental\nlearning, EIM exploits some lightweight operations, called adapters, to\neffectively and efficiently learn features for new classes so that it can\nimprove the accuracy of incremental learning while reducing model complexity as\nwell as training overhead. The efficiency of LODAP is further enhanced by a\ndata pruning strategy that significantly reduces the training data, thereby\nlowering the training overhead. We conducted extensive experiments on the\nCIFAR-100 and Tiny- ImageNet datasets. Experimental results show that LODAP\nimproves the accuracy by up to 4.32\\% over existing methods while reducing\naround 50\\% of model complexity. In addition, evaluations on real edge systems\ndemonstrate its applicability for on-device machine learning. The code is\navailable at https://github.com/duanbiqing/LODAP.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLODAP\u7684\u589e\u91cf\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u9ad8\u6548\u5b66\u4e60\u65b0\u7c7b\u522b\uff0c\u901a\u8fc7\u5f15\u5165\u8f7b\u91cf\u7ea7\u64cd\u4f5c\u6a21\u5757\uff08EIM\uff09\u548c\u6570\u636e\u526a\u679d\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cbe\u5ea6\u5e76\u964d\u4f4e\u4e86\u6a21\u578b\u590d\u6742\u5ea6\u548c\u8bad\u7ec3\u5f00\u9500\u3002", "motivation": "\u5de5\u4e1a\u8fb9\u7f18\u7cfb\u7edf\u9700\u8981\u5728\u4e0d\u4f9d\u8d56\u8fdc\u7a0b\u670d\u52a1\u5668\u7684\u60c5\u51b5\u4e0b\u589e\u91cf\u5b66\u4e60\u65b0\u7c7b\u522b\uff0c\u89e3\u51b3\u6a21\u578b\u590d\u6742\u5ea6\u548c\u8bad\u7ec3\u5f00\u9500\u95ee\u9898\u3002", "method": "\u63d0\u51faLODAP\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u9ad8\u6548\u589e\u91cf\u6a21\u5757\uff08EIM\uff09\u548c\u6570\u636e\u526a\u679d\u7b56\u7565\u3002EIM\u4f7f\u7528\u9002\u914d\u5668\u7b49\u8f7b\u91cf\u64cd\u4f5c\u5b66\u4e60\u65b0\u7c7b\u522b\u7279\u5f81\uff0c\u6570\u636e\u526a\u679d\u51cf\u5c11\u8bad\u7ec3\u6570\u636e\u91cf\u3002", "result": "\u5728CIFAR-100\u548cTiny-ImageNet\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0cLODAP\u6bd4\u73b0\u6709\u65b9\u6cd5\u7cbe\u5ea6\u63d0\u5347\u6700\u591a4.32%\uff0c\u6a21\u578b\u590d\u6742\u5ea6\u964d\u4f4e\u7ea650%\u3002", "conclusion": "LODAP\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u7684\u673a\u5668\u5b66\u4e60\uff0c\u5e73\u8861\u4e86\u7cbe\u5ea6\u3001\u590d\u6742\u5ea6\u548c\u8bad\u7ec3\u5f00\u9500\u3002"}}
{"id": "2504.19639", "pdf": "https://arxiv.org/pdf/2504.19639", "abs": "https://arxiv.org/abs/2504.19639", "authors": ["Youngjoon Lee", "Jinu Gong", "Joonhyuk Kang"], "title": "A Unified Benchmark of Federated Learning with Kolmogorov-Arnold Networks for Medical Imaging", "categories": ["cs.LG", "eess.SP"], "comment": "5 pages", "summary": "Federated Learning (FL) enables model training across decentralized devices\nwithout sharing raw data, thereby preserving privacy in sensitive domains like\nhealthcare. In this paper, we evaluate Kolmogorov-Arnold Networks (KAN)\narchitectures against traditional MLP across six state-of-the-art FL algorithms\non a blood cell classification dataset. Notably, our experiments demonstrate\nthat KAN can effectively replace MLP in federated environments, achieving\nsuperior performance with simpler architectures. Furthermore, we analyze the\nimpact of key hyperparameters-grid size and network architecture-on KAN\nperformance under varying degrees of Non-IID data distribution. Additionally,\nour ablation studies reveal that optimizing KAN width while maintaining minimal\ndepth yields the best performance in federated settings. As a result, these\nfindings establish KAN as a promising alternative for privacy-preserving\nmedical imaging applications in distributed healthcare. To the best of our\nknowledge, this is the first comprehensive benchmark of KAN in FL settings for\nmedical imaging task.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86Kolmogorov-Arnold Networks (KAN)\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u80fd\u66ff\u4ee3\u4f20\u7edfMLP\u5e76\u8868\u73b0\u66f4\u4f18\uff0c\u5c24\u5176\u5728\u533b\u7597\u6210\u50cf\u4efb\u52a1\u4e2d\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22KAN\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u4f5c\u4e3a\u9690\u79c1\u4fdd\u62a4\u5de5\u5177\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u533b\u7597\u9886\u57df\u7684\u6570\u636e\u654f\u611f\u6027\u9700\u6c42\u4e0b\u3002", "method": "\u901a\u8fc7\u516d\u79cd\u5148\u8fdb\u7684\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\u5728\u8840\u7ec6\u80de\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u6bd4\u8f83KAN\u4e0e\u4f20\u7edfMLP\uff0c\u5206\u6790\u8d85\u53c2\u6570\u5bf9KAN\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "KAN\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u5316\u5bbd\u5ea6\u548c\u6700\u5c0f\u6df1\u5ea6\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5206\u5e03\u5f0f\u533b\u7597\u5e94\u7528\u3002", "conclusion": "KAN\u662f\u8054\u90a6\u5b66\u4e60\u4e2d\u4e00\u4e2a\u9ad8\u6548\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u533b\u7597\u6210\u50cf\u4efb\u52a1\u3002"}}
{"id": "2504.18858", "pdf": "https://arxiv.org/pdf/2504.18858", "abs": "https://arxiv.org/abs/2504.18858", "authors": ["Vahid Garousi"], "title": "Why you shouldn't fully trust ChatGPT: A synthesis of this AI tool's error rates across disciplines and the software engineering lifecycle", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Context: ChatGPT and other large language models (LLMs) are widely used\nacross healthcare, business, economics, engineering, and software engineering\n(SE). Despite their popularity, concerns persist about their reliability,\nespecially their error rates across domains and the software development\nlifecycle (SDLC).\n  Objective: This study synthesizes and quantifies ChatGPT's reported error\nrates across major domains and SE tasks aligned with SDLC phases. It provides\nan evidence-based view of where ChatGPT excels, where it fails, and how\nreliability varies by task, domain, and model version (GPT-3.5, GPT-4,\nGPT-4-turbo, GPT-4o).\n  Method: A Multivocal Literature Review (MLR) was conducted, gathering data\nfrom academic studies, reports, benchmarks, and grey literature up to 2025.\nFactual, reasoning, coding, and interpretive errors were considered. Data were\ngrouped by domain and SE phase and visualized using boxplots to show error\ndistributions.\n  Results: Error rates vary across domains and versions. In healthcare, rates\nranged from 8% to 83%. Business and economics saw error rates drop from ~50%\nwith GPT-3.5 to 15-20% with GPT-4. Engineering tasks averaged 20-30%.\nProgramming success reached 87.5%, though complex debugging still showed over\n50% errors. In SE, requirements and design phases showed lower error rates\n(~5-20%), while coding, testing, and maintenance phases had higher variability\n(10-50%). Upgrades from GPT-3.5 to GPT-4 improved reliability.\n  Conclusion: Despite improvements, ChatGPT still exhibits non-negligible error\nrates varying by domain, task, and SDLC phase. Full reliance without human\noversight remains risky, especially in critical settings. Continuous evaluation\nand critical validation are essential to ensure reliability and\ntrustworthiness.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u591a\u89c6\u89d2\u6587\u732e\u7efc\u8ff0\uff08MLR\uff09\u91cf\u5316\u4e86ChatGPT\u5728\u4e0d\u540c\u9886\u57df\u548c\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\uff08SDLC\uff09\u4e2d\u7684\u9519\u8bef\u7387\uff0c\u53d1\u73b0\u9519\u8bef\u7387\u56e0\u9886\u57df\u3001\u4efb\u52a1\u548c\u6a21\u578b\u7248\u672c\u800c\u5f02\uff0c\u5347\u7ea7\u6a21\u578b\u53ef\u63d0\u5347\u53ef\u9760\u6027\uff0c\u4f46\u4ecd\u9700\u4eba\u5de5\u76d1\u7763\u3002", "motivation": "\u5c3d\u7ba1ChatGPT\u7b49\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u591a\u4e2a\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u53ef\u9760\u6027\u95ee\u9898\uff08\u5c24\u5176\u662f\u9519\u8bef\u7387\uff09\u4ecd\u5907\u53d7\u5173\u6ce8\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u5176\u5728\u4e0d\u540c\u9886\u57df\u548cSDLC\u9636\u6bb5\u7684\u8868\u73b0\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u91c7\u7528\u591a\u89c6\u89d2\u6587\u732e\u7efc\u8ff0\uff08MLR\uff09\u65b9\u6cd5\uff0c\u6536\u96c6\u622a\u81f32025\u5e74\u7684\u5b66\u672f\u7814\u7a76\u3001\u62a5\u544a\u3001\u57fa\u51c6\u6d4b\u8bd5\u53ca\u7070\u8272\u6587\u732e\u6570\u636e\uff0c\u6309\u9886\u57df\u548cSDLC\u9636\u6bb5\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7\u7bb1\u7ebf\u56fe\u53ef\u89c6\u5316\u9519\u8bef\u7387\u5206\u5e03\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u9519\u8bef\u7387\u56e0\u9886\u57df\u548c\u6a21\u578b\u7248\u672c\u5dee\u5f02\u663e\u8457\uff1a\u533b\u7597\u9886\u57df8%-83%\uff0c\u5546\u4e1a/\u7ecf\u6d4e\u5b66\u9886\u57df\u4eceGPT-3.5\u768450%\u964d\u81f3GPT-4\u768415-20%\u3002\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\uff0c\u9700\u6c42/\u8bbe\u8ba1\u9636\u6bb5\u9519\u8bef\u7387\u8f83\u4f4e\uff085-20%\uff09\uff0c\u800c\u7f16\u7801/\u6d4b\u8bd5\u9636\u6bb5\u8f83\u9ad8\uff0810-50%\uff09\u3002\u6a21\u578b\u5347\u7ea7\uff08\u5982GPT-4\uff09\u663e\u8457\u63d0\u5347\u53ef\u9760\u6027\u3002", "conclusion": "\u5c3d\u7ba1\u6a21\u578b\u6539\u8fdb\u964d\u4f4e\u4e86\u9519\u8bef\u7387\uff0c\u4f46\u5728\u5173\u952e\u9886\u57df\u6216\u4efb\u52a1\u4e2d\u5b8c\u5168\u4f9d\u8d56ChatGPT\u4ecd\u5b58\u5728\u98ce\u9669\uff0c\u9700\u6301\u7eed\u8bc4\u4f30\u548c\u4eba\u5de5\u9a8c\u8bc1\u4ee5\u786e\u4fdd\u53ef\u9760\u6027\u3002"}}
{"id": "2504.19649", "pdf": "https://arxiv.org/pdf/2504.19649", "abs": "https://arxiv.org/abs/2504.19649", "authors": ["Lei Xu", "Shanshan Wang", "Emmanuel Casseau", "Chenglong Xiao"], "title": "Intelligent4DSE: Optimizing High-Level Synthesis Design Space Exploration with Graph Neural Networks and Large Language Models", "categories": ["cs.LG", "cs.AR"], "comment": null, "summary": "High-level synthesis (HLS) design space exploration (DSE) is an optimization\nprocess in electronic design automation (EDA) that systematically explores\nhigh-level design configurations to achieve Pareto-optimal hardware\nimplementations balancing performance, area, and power (PPA). To optimize this\nprocess, HLS prediction tasks often employ message-passing neural networks\n(MPNNs), leveraging complex architectures to achieve high accuracy. These\npredictors serve as evaluators in the DSE process, effectively bypassing the\ntime-consuming estimations traditionally required by HLS tools. However,\nexisting models often prioritize structural complexity and minimization of\ntraining loss, overlooking task-specific characteristics. Additionally, while\nevolutionary algorithms are widely used in DSE, they typically require\nextensive domain-specific knowledge to design effective crossover and mutation\noperators. To address these limitations, we propose CoGNNs-LLMEA, a framework\nthat integrates a graph neural network with task-adaptive message passing and a\nlarge language model-enhanced evolutionary algorithm. As a predictive model,\nCoGNNs directly leverages intermediate representations generated from source\ncode after compiler front-end processing, enabling prediction of quality of\nresults (QoR) without invoking HLS tools. Due to its strong adaptability to\ntasks, CoGNNs can be tuned to predict post-HLS and post-implementation\noutcomes, effectively bridging the gap between high-level abstractions and\nphysical implementation characteristics. CoGNNs achieves state-of-the-art\nprediction accuracy in post-HLS QoR prediction, reducing mean prediction errors\nby 2.8$\\times$ for latency and 3.4$\\times$ for resource utilization compared to\nbaseline models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoGNNs-LLMEA\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u8fdb\u5316\u7b97\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86HLS\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0c\u51cf\u5c11\u4e86\u5b9e\u73b0\u8bef\u5dee\u3002", "motivation": "\u73b0\u6709HLS\u9884\u6d4b\u6a21\u578b\u8fc7\u4e8e\u5173\u6ce8\u7ed3\u6784\u590d\u6742\u6027\u548c\u8bad\u7ec3\u635f\u5931\uff0c\u5ffd\u7565\u4efb\u52a1\u7279\u6027\uff0c\u4e14\u8fdb\u5316\u7b97\u6cd5\u9700\u5927\u91cf\u9886\u57df\u77e5\u8bc6\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faCoGNNs-LLMEA\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u4efb\u52a1\u81ea\u9002\u5e94\u6d88\u606f\u4f20\u9012\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u5316\u7b97\u6cd5\uff0c\u76f4\u63a5\u5229\u7528\u6e90\u4ee3\u7801\u4e2d\u95f4\u8868\u793a\u9884\u6d4b\u7ed3\u679c\u3002", "result": "CoGNNs\u5728HLS\u540eQoR\u9884\u6d4b\u4e2d\u8fbe\u5230\u6700\u4f73\u7cbe\u5ea6\uff0c\u5ef6\u8fdf\u548c\u8d44\u6e90\u5229\u7528\u7387\u7684\u5e73\u5747\u9884\u6d4b\u8bef\u5dee\u5206\u522b\u964d\u4f4e\u4e862.8\u500d\u548c3.4\u500d\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u8fde\u63a5\u9ad8\u5c42\u62bd\u8c61\u4e0e\u7269\u7406\u5b9e\u73b0\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u4e3aEDA\u4f18\u5316\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2504.19659", "pdf": "https://arxiv.org/pdf/2504.19659", "abs": "https://arxiv.org/abs/2504.19659", "authors": ["Muhammad Sabih", "Abrarul Karim", "Jakob Wittmann", "Frank Hannig", "J\u00fcrgen Teich"], "title": "Hardware/Software Co-Design of RISC-V Extensions for Accelerating Sparse DNNs on FPGAs", "categories": ["cs.LG", "cs.AI", "cs.AR"], "comment": null, "summary": "The customizability of RISC-V makes it an attractive choice for accelerating\ndeep neural networks (DNNs). It can be achieved through instruction set\nextensions and corresponding custom functional units. Yet, efficiently\nexploiting these opportunities requires a hardware/software co-design approach\nin which the DNN model, software, and hardware are designed together. In this\npaper, we propose novel RISC-V extensions for accelerating DNN models\ncontaining semi-structured and unstructured sparsity. While the idea of\naccelerating structured and unstructured pruning is not new, our novel design\noffers various advantages over other designs. To exploit semi-structured\nsparsity, we take advantage of the fine-grained (bit-level) configurability of\nFPGAs and suggest reserving a few bits in a block of DNN weights to encode the\ninformation about sparsity in the succeeding blocks. The proposed custom\nfunctional unit utilizes this information to skip computations. To exploit\nunstructured sparsity, we propose a variable cycle sequential\nmultiply-and-accumulate unit that performs only as many multiplications as the\nnon-zero weights. Our implementation of unstructured and semi-structured\npruning accelerators can provide speedups of up to a factor of 3 and 4,\nrespectively. We then propose a combined design that can accelerate both types\nof sparsities, providing speedups of up to a factor of 5. Our designs consume a\nsmall amount of additional FPGA resources such that the resulting co-designs\nenable the acceleration of DNNs even on small FPGAs. We benchmark our designs\non standard TinyML applications such as keyword spotting, image classification,\nand person detection.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684RISC-V\u6269\u5c55\uff0c\u7528\u4e8e\u52a0\u901f\u5305\u542b\u534a\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u7a00\u758f\u6027\u7684DNN\u6a21\u578b\uff0c\u901a\u8fc7\u786c\u4ef6/\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u6548\u679c\u3002", "motivation": "RISC-V\u7684\u53ef\u5b9a\u5236\u6027\u4f7f\u5176\u6210\u4e3a\u52a0\u901fDNN\u7684\u6709\u529b\u9009\u62e9\uff0c\u4f46\u9700\u901a\u8fc7\u6307\u4ee4\u96c6\u6269\u5c55\u548c\u786c\u4ef6/\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u6765\u9ad8\u6548\u5229\u7528\u8fd9\u4e9b\u673a\u4f1a\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u52a0\u901f\u7a00\u758f\u6027\u7684\u65b9\u6cd5\uff1a\u5229\u7528FPGA\u7ec6\u7c92\u5ea6\u914d\u7f6e\u7684\u534a\u7ed3\u6784\u5316\u7a00\u758f\u6027\u8df3\u8fc7\u8ba1\u7b97\uff0c\u4ee5\u53ca\u9488\u5bf9\u975e\u7ed3\u6784\u5316\u7a00\u758f\u6027\u7684\u53ef\u53d8\u5468\u671f\u4e58\u6cd5\u7d2f\u52a0\u5355\u5143\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u975e\u7ed3\u6784\u5316\u548c\u534a\u7ed3\u6784\u5316\u7a00\u758f\u6027\u52a0\u901f\u5668\u5206\u522b\u5b9e\u73b0\u4e863\u500d\u548c4\u500d\u7684\u52a0\u901f\u6548\u679c\uff0c\u800c\u7ed3\u5408\u8bbe\u8ba1\u5219\u8fbe\u5230\u4e865\u500d\u52a0\u901f\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u5728\u5360\u7528\u5c11\u91cf\u989d\u5916FPGA\u8d44\u6e90\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86DNN\u6a21\u578b\u7684\u52a0\u901f\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5c0f\u578bFPGA\u7684TinyML\u5e94\u7528\u3002"}}
{"id": "2504.19667", "pdf": "https://arxiv.org/pdf/2504.19667", "abs": "https://arxiv.org/abs/2504.19667", "authors": ["Michael Banf", "Johannes Kuhn"], "title": "A Tripartite Perspective on GraphRAG", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious domains, yet they struggle with knowledge-intensive tasks in areas that\ndemand factual accuracy, e.g. industrial automation and healthcare. Key\nlimitations include their tendency to hallucinate, lack of source traceability\n(provenance), and challenges in timely knowledge updates. Combining language\nmodels with knowledge graphs (GraphRAG) offers promising avenues for overcoming\nthese deficits. However, a major challenge lies in creating such a knowledge\ngraph in the first place. Here, we propose a novel approach that combines LLMs\nwith a tripartite knowledge graph representation, which is constructed by\nconnecting complex, domain-specific objects via a curated ontology of\ncorresponding, domain-specific concepts to relevant sections within chunks of\ntext through a concept-anchored pre-analysis of source documents starting from\nan initial lexical graph. As a consequence, our Tripartite-GraphRAG approach\nimplements: i) a concept-specific, information-preserving pre-compression of\ntextual chunks; ii) allows for the formation of a concept-specific relevance\nestimation of embedding similarities grounded in statistics; and iii) avoids\ncommon challenges w.r.t. continuous extendability, such as the need for entity\nresolution and deduplication. By applying a transformation to the knowledge\ngraph, we formulate LLM prompt creation as an unsupervised node classification\nproblem, drawing on ideas from Markov Random Fields. We evaluate our approach\non a healthcare use case, involving multi-faceted analyses of patient anamneses\ngiven a set of medical concepts as well as clinical literature. Experiments\nindicate that it can optimize information density, coverage, and arrangement of\nLLM prompts while reducing their lengths, which may lead to reduced costs and\nmore consistent and reliable LLM outputs.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u4e09\u65b9\u77e5\u8bc6\u56fe\u8c31\uff08Tripartite-GraphRAG\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u9488\u5bf9\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u3001\u6765\u6e90\u8ffd\u6eaf\u548c\u77e5\u8bc6\u66f4\u65b0\u95ee\u9898\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u6982\u5ff5\u951a\u5b9a\u7684\u6587\u672c\u9884\u5904\u7406\u548c\u7edf\u8ba1\u9a71\u52a8\u5d4c\u5165\u76f8\u4f3c\u6027\u8bc4\u4f30\uff0c\u4f18\u5316\u63d0\u793a\u4fe1\u606f\u5bc6\u5ea6\u3001\u8986\u76d6\u7387\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709LLMs\u5728\u5de5\u4e1a\u81ea\u52a8\u5316\u548c\u533b\u7597\u7b49\u9700\u8981\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u9886\u57df\u5b58\u5728\u5e7b\u89c9\u3001\u6765\u6e90\u4e0d\u53ef\u8ffd\u6eaf\u548c\u77e5\u8bc6\u66f4\u65b0\u6ede\u540e\u7684\u5c40\u9650\uff0c\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u53ef\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u672c\u8eab\u662f\u4e00\u5927\u6311\u6218\u3002", "method": "\u63d0\u51faTripartite-GraphRAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u6982\u5ff5\u951a\u5b9a\u7684\u6e90\u6587\u6863\u9884\u5904\u7406\u6784\u5efa\u4e09\u65b9\u77e5\u8bc6\u56fe\u8c31\uff0c\u5c06LLM\u63d0\u793a\u521b\u5efa\u8f6c\u5316\u4e3a\u65e0\u76d1\u7763\u8282\u70b9\u5206\u7c7b\u95ee\u9898\uff0c\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u968f\u673a\u573a\u601d\u60f3\u3002", "result": "\u5728\u533b\u7597\u7528\u4f8b\u4e2d\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u4f18\u5316\u63d0\u793a\u7684\u4fe1\u606f\u5bc6\u5ea6\u3001\u8986\u76d6\u7387\u548c\u6392\u5217\uff0c\u7f29\u77ed\u63d0\u793a\u957f\u5ea6\uff0c\u4ece\u800c\u53ef\u80fd\u964d\u4f4e\u6210\u672c\u5e76\u63d0\u9ad8LLM\u8f93\u51fa\u7684\u53ef\u9760\u6027\u4e0e\u4e00\u81f4\u6027\u3002", "conclusion": "Tripartite-GraphRAG\u4e3a\u89e3\u51b3LLMs\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u9014\u5f84\uff0c\u5c24\u5176\u5728\u533b\u7597\u9886\u57df\u5c55\u73b0\u51fa\u6f5c\u529b\u3002"}}
{"id": "2504.19740", "pdf": "https://arxiv.org/pdf/2504.19740", "abs": "https://arxiv.org/abs/2504.19740", "authors": ["Yonghui Zhai", "Yang Zhang", "Minghao Shang", "Lihua Pang", "Yaxin Ren"], "title": "Graph Fourier Transformer with Structure-Frequency Information", "categories": ["cs.LG", "cs.GR"], "comment": null, "summary": "Graph Transformers (GTs) have shown advantages in numerous graph structure\ntasks but their self-attention mechanism ignores the generalization bias of\ngraphs, with existing methods mainly compensating for this bias from aspects\nlike position encoding, attention bias and relative distance yet still having\nsub-optimal performance and being insufficient by only considering the\nstructural perspective of generalization bias. To address this, this paper\nproposes Grafourierformer, which innovatively combines GT with inductive bias\ncontaining Frequency-Structure information by applying Graph Fourier Transform\nto the Attention Matrix: specifically, eigenvalues from the Graph Laplacian\nmatrix are used to construct an Eigenvalue matrix mask (reflecting node\npositions and structural relationships with neighboring nodes to enable\nconsideration of node range structural characteristics and focus on local graph\ndetails), and inverse Fourier transform is employed to extract node\nhigh-frequency and low-frequency features, calculate low-frequency and\nhigh-frequency energy, and construct a node frequency-energy matrix to filter\nthe eigenvalue matrix mask, allowing attention heads to incorporate both graph\nstructural information and node frequency information optimization, adaptively\ndistinguish global trends from local details, and effectively suppress\nredundant information interference. Extensive experiments on various benchmarks\nshow Grafourierformer consistently outperforms GNN and GT-based models in graph\nclassification and node classification tasks, with ablation experiments further\nvalidating the effectiveness and necessity of the method. Codes are available\nat https://github.com/Arichibald/Grafourierformer.git", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGrafourierformer\uff0c\u901a\u8fc7\u7ed3\u5408\u56fe\u5085\u91cc\u53f6\u53d8\u6362\u4e0e\u56fe\u53d8\u6362\u5668\uff0c\u4f18\u5316\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u540c\u65f6\u8003\u8651\u56fe\u7684\u7ed3\u6784\u548c\u8282\u70b9\u9891\u7387\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u56fe\u5206\u7c7b\u548c\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u53d8\u6362\u5668\uff08GT\uff09\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5ffd\u7565\u56fe\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u4ec5\u4ece\u7ed3\u6784\u89d2\u5ea6\u8865\u507f\uff0c\u6027\u80fd\u6b20\u4f73\u3002\u9700\u540c\u65f6\u5229\u7528\u7ed3\u6784\u548c\u9891\u7387\u4fe1\u606f\u4ee5\u66f4\u597d\u5730\u4f18\u5316\u6a21\u578b\u3002", "method": "\u63d0\u51faGrafourierformer\uff1a1\uff09\u7528\u56fe\u62c9\u666e\u62c9\u65af\u77e9\u9635\u7279\u5f81\u503c\u6784\u5efa\u7279\u5f81\u77e9\u9635\u63a9\u7801\uff1b2\uff09\u7ed3\u5408\u5085\u91cc\u53f6\u6b63/\u9006\u53d8\u6362\u63d0\u53d6\u8282\u70b9\u9ad8\u4f4e\u9891\u7279\u5f81\u53ca\u80fd\u91cf\uff1b3\uff09\u901a\u8fc7\u9891\u7387-\u80fd\u91cf\u77e9\u9635\u8fc7\u6ee4\u63a9\u7801\uff0c\u4f7f\u6ce8\u610f\u529b\u5934\u81ea\u9002\u5e94\u533a\u5206\u5168\u5c40\u8d8b\u52bf\u4e0e\u5c40\u90e8\u7ec6\u8282\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGrafourierformer\u5728\u56fe\u5206\u7c7b\u548c\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8eGNN\u548cGT\u6a21\u578b\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5fc5\u8981\u6027\u3002", "conclusion": "\u901a\u8fc7\u878d\u5408\u56fe\u7ed3\u6784\u4e0e\u9891\u7387\u4fe1\u606f\uff0cGrafourierformer\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u56fe\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.18886", "pdf": "https://arxiv.org/pdf/2504.18886", "abs": "https://arxiv.org/abs/2504.18886", "authors": ["Simone Maurizio La Cava", "Roberto Casula", "Sara Concas", "Giulia Orr\u00f9", "Ruben Tolosana", "Martin Drahansky", "Julian Fierrez", "Gian Luca Marcialis"], "title": "Exploiting Multiple Representations: 3D Face Biometrics Fusion with Application to Surveillance", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "3D face reconstruction (3DFR) algorithms are based on specific assumptions\ntailored to the limits and characteristics of the different application\nscenarios. In this study, we investigate how multiple state-of-the-art 3DFR\nalgorithms can be used to generate a better representation of subjects, with\nthe final goal of improving the performance of face recognition systems in\nchallenging uncontrolled scenarios. We also explore how different parametric\nand non-parametric score-level fusion methods can exploit the unique strengths\nof multiple 3DFR algorithms to enhance biometric recognition robustness. With\nthis goal, we propose a comprehensive analysis of several face recognition\nsystems across diverse conditions, such as varying distances and camera setups,\nintra-dataset and cross-dataset, to assess the robustness of the proposed\nensemble method. The results demonstrate that the distinct information provided\nby different 3DFR algorithms can alleviate the problem of generalizing over\nmultiple application scenarios. In addition, the present study highlights the\npotential of advanced fusion strategies to enhance the reliability of\n3DFR-based face recognition systems, providing the research community with key\ninsights to exploit them in real-world applications effectively. Although the\nexperiments are carried out in a specific face verification setup, our proposed\nfusion-based 3DFR methods may be applied to other tasks around face biometrics\nthat are not strictly related to identity recognition.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u5982\u4f55\u7ed3\u5408\u591a\u79cd3D\u4eba\u8138\u91cd\u5efa\u7b97\u6cd5\u63d0\u5347\u65e0\u7ea6\u675f\u573a\u666f\u4e0b\u7684\u4eba\u8138\u8bc6\u522b\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u53c2\u6570\u4e0e\u975e\u53c2\u6570\u878d\u5408\u65b9\u6cd5\u589e\u5f3a\u751f\u7269\u8bc6\u522b\u9c81\u68d2\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b33D\u4eba\u8138\u91cd\u5efa\u7b97\u6cd5\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e2d\u7684\u5c40\u9650\uff0c\u63d0\u5347\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5728\u6311\u6218\u6027\u65e0\u7ea6\u675f\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u878d\u5408\u591a\u79cd3D\u4eba\u8138\u91cd\u5efa\u7b97\u6cd5\uff0c\u91c7\u7528\u53c2\u6570\u4e0e\u975e\u53c2\u6570\u5206\u6570\u7ea7\u878d\u5408\u65b9\u6cd5\uff0c\u5206\u6790\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u7cfb\u7edf\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u591a\u7b97\u6cd5\u878d\u5408\u80fd\u7f13\u89e3\u8de8\u573a\u666f\u6cdb\u5316\u95ee\u9898\uff0c\u9ad8\u7ea7\u878d\u5408\u7b56\u7565\u53ef\u63d0\u53473D\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u878d\u5408\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u8eab\u4efd\u8bc6\u522b\uff0c\u8fd8\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u4eba\u8138\u751f\u7269\u8bc6\u522b\u4efb\u52a1\u3002"}}
{"id": "2504.19746", "pdf": "https://arxiv.org/pdf/2504.19746", "abs": "https://arxiv.org/abs/2504.19746", "authors": ["Xilong Xie", "Liang Wang", "Limin Xiao", "Meng Han", "Lin Sun", "Shuai Zheng", "Xiangrong Xu"], "title": "FineQ: Software-Hardware Co-Design for Low-Bit Fine-Grained Mixed-Precision Quantization of LLMs", "categories": ["cs.LG", "cs.AR"], "comment": "DATE 2025", "summary": "Large language models (LLMs) have significantly advanced the natural language\nprocessing paradigm but impose substantial demands on memory and computational\nresources. Quantization is one of the most effective ways to reduce memory\nconsumption of LLMs. However, advanced single-precision quantization methods\nexperience significant accuracy degradation when quantizing to ultra-low bits.\nExisting mixed-precision quantization methods are quantized by groups with\ncoarse granularity. Employing high precision for group data leads to\nsubstantial memory overhead, whereas low precision severely impacts model\naccuracy. To address this issue, we propose FineQ, software-hardware co-design\nfor low-bit fine-grained mixed-precision quantization of LLMs. First, FineQ\npartitions the weights into finer-grained clusters and considers the\ndistribution of outliers within these clusters, thus achieving a balance\nbetween model accuracy and memory overhead. Then, we propose an outlier\nprotection mechanism within clusters that uses 3 bits to represent outliers and\nintroduce an encoding scheme for index and data concatenation to enable aligned\nmemory access. Finally, we introduce an accelerator utilizing temporal coding\nthat effectively supports the quantization algorithm while simplifying the\nmultipliers in the systolic array. FineQ achieves higher model accuracy\ncompared to the SOTA mixed-precision quantization algorithm at a close average\nbit-width. Meanwhile, the accelerator achieves up to 1.79x energy efficiency\nand reduces the area of the systolic array by 61.2%.", "AI": {"tldr": "FineQ\u63d0\u51fa\u4e86\u4e00\u79cd\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u7684\u4f4e\u6bd4\u7279\u7ec6\u7c92\u5ea6\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u91cf\u5316\u4e2d\u5185\u5b58\u4e0e\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5355\u7cbe\u5ea6\u91cf\u5316\u65b9\u6cd5\u5728\u8d85\u4f4e\u4f4d\u5bbd\u4e0b\u7cbe\u5ea6\u635f\u5931\u4e25\u91cd\uff0c\u800c\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6cd5\u56e0\u7c97\u7c92\u5ea6\u5206\u7ec4\u5bfc\u81f4\u5185\u5b58\u5f00\u9500\u5927\u6216\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "FineQ\u901a\u8fc7\u7ec6\u7c92\u5ea6\u6743\u91cd\u5206\u533a\u3001\u5f02\u5e38\u503c\u4fdd\u62a4\u673a\u5236\u548c\u7f16\u7801\u65b9\u6848\uff0c\u914d\u5408\u57fa\u4e8e\u65f6\u5e8f\u7f16\u7801\u7684\u52a0\u901f\u5668\u8bbe\u8ba1\uff0c\u5e73\u8861\u4e86\u7cbe\u5ea6\u4e0e\u5185\u5b58\u5f00\u9500\u3002", "result": "\u5728\u76f8\u8fd1\u5e73\u5747\u4f4d\u5bbd\u4e0b\uff0cFineQ\u7684\u6a21\u578b\u7cbe\u5ea6\u4f18\u4e8e\u73b0\u6709\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6cd5\uff0c\u52a0\u901f\u5668\u80fd\u6548\u63d0\u53471.79\u500d\uff0c\u8109\u52a8\u9635\u5217\u9762\u79ef\u51cf\u5c1161.2%\u3002", "conclusion": "FineQ\u901a\u8fc7\u7ec6\u7c92\u5ea6\u91cf\u5316\u4e0e\u786c\u4ef6\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u7684\u6548\u7387\u4e0e\u7cbe\u5ea6\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18902", "pdf": "https://arxiv.org/pdf/2504.18902", "abs": "https://arxiv.org/abs/2504.18902", "authors": ["Cyril Shih-Huan Hsu", "Anestis Dalgkitsis", "Chrysa Papagianni", "Paola Grosso"], "title": "Transformer-Empowered Actor-Critic Reinforcement Learning for Sequence-Aware Service Function Chain Partitioning", "categories": ["cs.NI", "cs.AI", "cs.LG", "cs.NE"], "comment": null, "summary": "In the forthcoming era of 6G networks, characterized by unprecedented data\nrates, ultra-low latency, and extensive connectivity, effective management of\nVirtualized Network Functions (VNFs) is essential. VNFs are software-based\ncounterparts of traditional hardware devices that facilitate flexible and\nscalable service provisioning. Service Function Chains (SFCs), structured as\nordered sequences of VNFs, are pivotal in orchestrating complex network\nservices. Nevertheless, partitioning SFCs across multi-domain network\ninfrastructures presents substantial challenges due to stringent latency\nconstraints and limited resource availability. Conventional optimization-based\nmethods typically exhibit low scalability, whereas existing data-driven\napproaches often fail to adequately balance computational efficiency with the\ncapability to effectively account for dependencies inherent in SFCs. To\novercome these limitations, we introduce a Transformer-empowered actor-critic\nframework specifically designed for sequence-aware SFC partitioning. By\nutilizing the self-attention mechanism, our approach effectively models complex\ninter-dependencies among VNFs, facilitating coordinated and parallelized\ndecision-making processes. Additionally, we enhance training stability and\nconvergence using $\\epsilon$-LoPe exploration strategy as well as Asymptotic\nReturn Normalization. Comprehensive simulation results demonstrate that the\nproposed methodology outperforms existing state-of-the-art solutions in terms\nof long-term acceptance rates, resource utilization efficiency, and\nscalability, while achieving rapid inference. This study not only advances\nintelligent network orchestration by delivering a scalable and robust solution\nfor SFC partitioning within emerging 6G environments, but also bridging recent\nadvancements in Large Language Models (LLMs) with the optimization of\nnext-generation networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684actor-critic\u6846\u67b6\uff0c\u7528\u4e8e6G\u7f51\u7edc\u4e2d\u670d\u52a1\u529f\u80fd\u94fe\uff08SFC\uff09\u7684\u5206\u533a\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5efa\u6a21VNF\u95f4\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u7ed3\u5408\u63a2\u7d22\u7b56\u7565\u548c\u5f52\u4e00\u5316\u6280\u672f\u63d0\u5347\u6027\u80fd\u3002\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u957f\u671f\u63a5\u53d7\u7387\u3001\u8d44\u6e90\u5229\u7528\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "6G\u7f51\u7edc\u4e2d\u865a\u62df\u5316\u7f51\u7edc\u529f\u80fd\uff08VNF\uff09\u7684\u7075\u6d3b\u7ba1\u7406\u662f\u5173\u952e\uff0c\u4f46\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u53ef\u6269\u5c55\u6027\u5dee\uff0c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u6548\u7387\u4e0eSFC\u7684\u4f9d\u8d56\u5173\u7cfb\u3002\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u91c7\u7528Transformer\u589e\u5f3a\u7684actor-critic\u6846\u67b6\uff0c\u5229\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5efa\u6a21VNF\u95f4\u4f9d\u8d56\uff0c\u914d\u5408\u03b5-LoPe\u63a2\u7d22\u7b56\u7565\u548c\u6e10\u8fdb\u56de\u62a5\u5f52\u4e00\u5316\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u957f\u671f\u63a5\u53d7\u7387\u3001\u8d44\u6e90\u5229\u7528\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u5feb\u901f\u63a8\u7406\u3002", "conclusion": "\u7814\u7a76\u4e0d\u4ec5\u4e3a6G\u73af\u5883\u4e2d\u7684SFC\u5206\u533a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8fd8\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8fdb\u5c55\u4e0e\u4e0b\u4e00\u4ee3\u7f51\u7edc\u4f18\u5316\u8054\u7cfb\u8d77\u6765\u3002"}}
{"id": "2504.19774", "pdf": "https://arxiv.org/pdf/2504.19774", "abs": "https://arxiv.org/abs/2504.19774", "authors": ["Nicola Debole", "Pietro Barbiero", "Francesco Giannini", "Andrea Passeggini", "Stefano Teso", "Emanuele Marconato"], "title": "If Concept Bottlenecks are the Question, are Foundation Models the Answer?", "categories": ["cs.LG"], "comment": null, "summary": "Concept Bottleneck Models (CBMs) are neural networks designed to conjoin high\nperformance with ante-hoc interpretability. CBMs work by first mapping inputs\n(e.g., images) to high-level concepts (e.g., visible objects and their\nproperties) and then use these to solve a downstream task (e.g., tagging or\nscoring an image) in an interpretable manner. Their performance and\ninterpretability, however, hinge on the quality of the concepts they learn. The\ngo-to strategy for ensuring good quality concepts is to leverage expert\nannotations, which are expensive to collect and seldom available in\napplications. Researchers have recently addressed this issue by introducing\n\"VLM-CBM\" architectures that replace manual annotations with weak supervision\nfrom foundation models. It is however unclear what is the impact of doing so on\nthe quality of the learned concepts. To answer this question, we put\nstate-of-the-art VLM-CBMs to the test, analyzing their learned concepts\nempirically using a selection of significant metrics. Our results show that,\ndepending on the task, VLM supervision can sensibly differ from expert\nannotations, and that concept accuracy and quality are not strongly correlated.\nOur code is available at https://github.com/debryu/CQA.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u57fa\u7840\u6a21\u578b\uff08\u5982VLM\uff09\u66ff\u4ee3\u4e13\u5bb6\u6807\u6ce8\u5bf9\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBM\uff09\u6982\u5ff5\u5b66\u4e60\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6982\u5ff5\u51c6\u786e\u6027\u4e0e\u8d28\u91cf\u5e76\u4e0d\u5f3a\u76f8\u5173\u3002", "motivation": "\u4f20\u7edfCBM\u4f9d\u8d56\u4e13\u5bb6\u6807\u6ce8\u6765\u786e\u4fdd\u9ad8\u8d28\u91cf\u6982\u5ff5\uff0c\u4f46\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u4e0d\u6613\u83b7\u53d6\u3002\u7814\u7a76\u63a2\u7d22\u7528\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u5f31\u76d1\u7763\u7684\u53ef\u884c\u6027\u53ca\u5176\u5f71\u54cd\u3002", "method": "\u901a\u8fc7VLM-CBM\u67b6\u6784\uff0c\u7528\u57fa\u7840\u6a21\u578b\u751f\u6210\u7684\u5f31\u76d1\u7763\u66ff\u4ee3\u4e13\u5bb6\u6807\u6ce8\uff0c\u5e76\u8bbe\u8ba1\u5b9e\u9a8c\u5ea6\u91cf\u5b66\u4e60\u5230\u7684\u6982\u5ff5\u8d28\u91cf\u3002", "result": "\u53d1\u73b0VLM\u76d1\u7763\u4e0e\u4e13\u5bb6\u6807\u6ce8\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u5dee\u5f02\u663e\u8457\uff0c\u4e14\u6982\u5ff5\u51c6\u786e\u6027\u4e0e\u8d28\u91cf\u65e0\u660e\u663e\u76f8\u5173\u6027\u3002", "conclusion": "\u867d\u7136VLM-CBM\u80fd\u51cf\u5c11\u5bf9\u4e13\u5bb6\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u786e\u4fdd\u6982\u5ff5\u8d28\u91cf\u3002"}}
{"id": "2504.18910", "pdf": "https://arxiv.org/pdf/2504.18910", "abs": "https://arxiv.org/abs/2504.18910", "authors": ["Ali Nazari", "Mohsen Ebrahimi Moghaddam", "Omidreza Borzoei"], "title": "Kinship Verification through a Forest Neural Network", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Early methods used face representations in kinship verification, which are\nless accurate than joint representations of parents' and children's facial\nimages learned from scratch. We propose an approach featuring graph neural\nnetwork concepts to utilize face representations and have comparable results to\njoint representation algorithms. Moreover, we designed the structure of the\nclassification module and introduced a new combination of losses to engage the\ncenter loss gradually in training our network. Additionally, we conducted\nexperiments on KinFaceW-I and II, demonstrating the effectiveness of our\napproach. We achieved the best result on KinFaceW-II, an average improvement of\nnearly 1.6 for all kinship types, and we were near the best on KinFaceW-I. The\ncode is available at https://github.com/ali-nazari/Kinship-Verification", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u4eba\u8138\u8868\u793a\u7684\u4eb2\u5c5e\u5173\u7cfb\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u8868\u73b0\u4f18\u4e8e\u65e9\u671f\u65b9\u6cd5\uff0c\u5e76\u5728KinFaceW\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u65e9\u671f\u57fa\u4e8e\u4eba\u8138\u8868\u793a\u7684\u4eb2\u5c5e\u5173\u7cfb\u9a8c\u8bc1\u65b9\u6cd5\u51c6\u786e\u6027\u8f83\u4f4e\uff0c\u800c\u4ece\u5934\u5b66\u4e60\u7684\u8054\u5408\u8868\u793a\u65b9\u6cd5\u6548\u679c\u66f4\u597d\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u6982\u5ff5\u63d0\u5347\u4eba\u8138\u8868\u793a\u7684\u6548\u80fd\uff0c\u4f7f\u5176\u80fd\u4e0e\u8054\u5408\u8868\u793a\u7b97\u6cd5\u5ab2\u7f8e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u4eba\u8138\u8868\u793a\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e86\u5206\u7c7b\u6a21\u5757\u7ed3\u6784\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u635f\u5931\u7ec4\u5408\uff08\u5982\u4e2d\u5fc3\u635f\u5931\uff09\u9010\u6b65\u4f18\u5316\u7f51\u7edc\u8bad\u7ec3\u3002", "result": "\u5728KinFaceW-I\u548cII\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5176\u4e2dKinFaceW-II\u7684\u5e73\u5747\u6027\u80fd\u63d0\u53471.6\uff0c\u63a5\u8fd1KinFaceW-I\u7684\u6700\u4f73\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u4f18\u5316\u635f\u5931\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eb2\u5c5e\u5173\u7cfb\u9a8c\u8bc1\u7684\u51c6\u786e\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.19779", "pdf": "https://arxiv.org/pdf/2504.19779", "abs": "https://arxiv.org/abs/2504.19779", "authors": ["Claudia Drygala", "Hanno Gottschalk", "Thomas Kruse", "S\u00e9gol\u00e8ne Martin", "Annika M\u00fctze"], "title": "Learning Brenier Potentials with Convex Generative Adversarial Neural Networks", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Brenier proved that under certain conditions on a source and a target\nprobability measure there exists a strictly convex function such that its\ngradient is a transport map from the source to the target distribution. This\nfunction is called the Brenier potential. Furthermore, detailed information on\nthe H\\\"older regularity of the Brenier potential is available. In this work we\ndevelop the statistical learning theory of generative adversarial neural\nnetworks that learn the Brenier potential. As by the transformation of\ndensities formula, the density of the generated measure depends on the second\nderivative of the Brenier potential, we develop the universal approximation\ntheory of ReCU networks with cubic activation $\\mathtt{ReCU}(x)=\\max\\{0,x\\}^3$\nthat combines the favorable approximation properties of H\\\"older functions with\na Lipschitz continuous density. In order to assure the convexity of such\ngeneral networks, we introduce an adversarial training procedure for a\npotential function represented by the ReCU networks that combines the classical\ndiscriminator cross entropy loss with a penalty term that enforces (strict)\nconvexity. We give a detailed decomposition of learning errors and show that\nfor a suitable high penalty parameter all networks chosen in the adversarial\nmin-max optimization problem are strictly convex. This is further exploited to\nprove the consistency of the learning procedure for (slowly) expanding network\ncapacity. We also implement the described learning algorithm and apply it to a\nnumber of standard test cases from Gaussian mixture to image data as target\ndistributions. As predicted in theory, we observe that the convexity loss\nbecomes inactive during the training process and the potentials represented by\nthe neural networks have learned convexity.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u751f\u6210\u5bf9\u6297\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u5b66\u4e60Brenier\u52bf\u80fd\uff0c\u7ed3\u5408ReCU\u7f51\u7edc\u548c\u5bf9\u6297\u8bad\u7ec3\uff0c\u786e\u4fdd\u7f51\u7edc\u4e25\u683c\u51f8\u6027\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "Brenier\u52bf\u80fd\u5728\u6982\u7387\u6d4b\u5ea6\u95f4\u7684\u4f20\u8f93\u6620\u5c04\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u5176\u7edf\u8ba1\u5b66\u4e60\u7406\u8bba\u5c1a\u672a\u5145\u5206\u5f00\u53d1\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60Brenier\u52bf\u80fd\uff0c\u5e76\u786e\u4fdd\u5b66\u4e60\u7684\u4e25\u683c\u51f8\u6027\u3002", "method": "\u91c7\u7528ReCU\u7f51\u7edc\uff08\u7acb\u65b9\u6fc0\u6d3b\u51fd\u6570\uff09\u8fd1\u4f3cBrenier\u52bf\u80fd\uff0c\u63d0\u51fa\u5bf9\u6297\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u5224\u522b\u5668\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u548c\u51f8\u6027\u60e9\u7f5a\u9879\uff0c\u4fdd\u8bc1\u7f51\u7edc\u7684\u4e25\u683c\u51f8\u6027\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5b66\u4e60\u4e25\u683c\u51f8\u7684Brenier\u52bf\u80fd\uff0c\u51f8\u6027\u60e9\u7f5a\u9879\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9010\u6e10\u5931\u6548\uff0c\u7f51\u7edc\u81ea\u52a8\u5b66\u4e60\u5230\u51f8\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\u80fd\u591f\u7a33\u5b9a\u5b66\u4e60Brenier\u52bf\u80fd\uff0c\u4e3a\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u7406\u8bba\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.18916", "pdf": "https://arxiv.org/pdf/2504.18916", "abs": "https://arxiv.org/abs/2504.18916", "authors": ["Sarang S", "Druva Dhakshinamoorthy", "Aditya Shiva Sharma", "Yuvraj Singh Bhadauria", "Siddharth Chaitra Vivek", "Arihant Bansal", "Arnab K. Paul"], "title": "UnifyFL: Enabling Decentralized Cross-Silo Federated Learning", "categories": ["cs.DC", "cs.AI"], "comment": "12 pages, 7 figures, 7 tables. Accepted at the 26th ACM/IFIP\n  International Middleware Conference (MIDDLEWARE 2025)", "summary": "Federated Learning (FL) is a decentralized machine learning (ML) paradigm in\nwhich models are trained on private data across several devices called clients\nand combined at a single node called an aggregator rather than aggregating the\ndata itself. Many organizations employ FL to have better privacy-aware\nML-driven decision-making capabilities. However, organizations often operate\nindependently rather than collaborate to enhance their FL capabilities due to\nthe lack of an effective mechanism for collaboration. The challenge lies in\nbalancing trust and resource efficiency. One approach relies on trusting a\nthird-party aggregator to consolidate models from all organizations (multilevel\nFL), but this requires trusting an entity that may be biased or unreliable.\nAlternatively, organizations can bypass a third party by sharing their local\nmodels directly, which requires significant computational resources for\nvalidation. Both approaches reflect a fundamental trade-off between trust and\nresource constraints, with neither offering an ideal solution. In this work, we\ndevelop a trust-based cross-silo FL framework called \\proj, which uses\ndecentralized orchestration and distributed storage. \\proj provides flexibility\nto the participating organizations and presents synchronous and asynchronous\nmodes to handle stragglers. Our evaluation on a diverse testbed shows that\n\\proj achieves a performance comparable to the ideal multilevel centralized FL\nwhile allowing trust and optimal use of resources.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\\proj\u7684\u57fa\u4e8e\u4fe1\u4efb\u7684\u8de8\u673a\u6784\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53bb\u4e2d\u5fc3\u5316\u7f16\u6392\u548c\u5206\u5e03\u5f0f\u5b58\u50a8\uff0c\u5e73\u8861\u4e86\u4fe1\u4efb\u4e0e\u8d44\u6e90\u6548\u7387\uff0c\u6027\u80fd\u63a5\u8fd1\u7406\u60f3\u7684\u591a\u7ea7\u96c6\u4e2d\u5f0f\u8054\u90a6\u5b66\u4e60\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u9690\u79c1\u4fdd\u62a4\u4e0b\u7684\u673a\u5668\u5b66\u4e60\u51b3\u7b56\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7ec4\u7ec7\u95f4\u7f3a\u4e4f\u6709\u6548\u534f\u4f5c\u673a\u5236\uff0c\u9762\u4e34\u4fe1\u4efb\u4e0e\u8d44\u6e90\u6548\u7387\u7684\u6743\u8861\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u4f9d\u8d56\u7b2c\u4e09\u65b9\u805a\u5408\u5668\u6216\u76f4\u63a5\u5171\u4eab\u6a21\u578b\uff09\u5747\u5b58\u5728\u4e0d\u8db3\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\\proj\u6846\u67b6\uff0c\u91c7\u7528\u53bb\u4e2d\u5fc3\u5316\u7f16\u6392\u548c\u5206\u5e03\u5f0f\u5b58\u50a8\uff0c\u652f\u6301\u540c\u6b65\u548c\u5f02\u6b65\u6a21\u5f0f\u4ee5\u5904\u7406\u5ef6\u8fdf\u8282\u70b9\uff0c\u786e\u4fdd\u7075\u6d3b\u6027\u4e0e\u8d44\u6e90\u9ad8\u6548\u5229\u7528\u3002", "result": "\u5728\u591a\u6837\u5316\u6d4b\u8bd5\u5e73\u53f0\u4e0a\uff0c\\proj\u6027\u80fd\u63a5\u8fd1\u7406\u60f3\u7684\u591a\u7ea7\u96c6\u4e2d\u5f0f\u8054\u90a6\u5b66\u4e60\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u4fe1\u4efb\u4e0e\u8d44\u6e90\u7684\u6700\u4f18\u5e73\u8861\u3002", "conclusion": "\\proj\u4e3a\u8de8\u673a\u6784\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u517c\u987e\u4fe1\u4efb\u3001\u8d44\u6e90\u6548\u7387\u548c\u6027\u80fd\u7684\u53ef\u884c\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2504.19785", "pdf": "https://arxiv.org/pdf/2504.19785", "abs": "https://arxiv.org/abs/2504.19785", "authors": ["Haishan Wang", "Arno Solin", "Vikas Garg"], "title": "Heterophily-informed Message Passing", "categories": ["cs.LG"], "comment": "Appearing in Transactions on Machine Learning Research (TMLR) 2025", "summary": "Graph neural networks (GNNs) are known to be vulnerable to oversmoothing due\nto their implicit homophily assumption. We mitigate this problem with a novel\nscheme that regulates the aggregation of messages, modulating the type and\nextent of message passing locally thereby preserving both the low and\nhigh-frequency components of information. Our approach relies solely on learnt\nembeddings, obviating the need for auxiliary labels, thus extending the\nbenefits of heterophily-aware embeddings to broader applications, e.g.,\ngenerative modelling. Our experiments, conducted across various data sets and\nGNN architectures, demonstrate performance enhancements and reveal heterophily\npatterns across standard classification benchmarks. Furthermore, application to\nmolecular generation showcases notable performance improvements on\nchemoinformatics benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u65b9\u6848\uff0c\u901a\u8fc7\u8c03\u8282\u6d88\u606f\u805a\u5408\u7684\u7c7b\u578b\u548c\u8303\u56f4\u6765\u89e3\u51b3GNN\u56e0\u540c\u8d28\u6027\u5047\u8bbe\u800c\u5bfc\u81f4\u7684\u8fc7\u5ea6\u5e73\u6ed1\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u7559\u4fe1\u606f\u7684\u4f4e\u9891\u548c\u9ad8\u9891\u6210\u5206\u3002\u8be5\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u5b66\u4e60\u5d4c\u5165\uff0c\u65e0\u9700\u8f85\u52a9\u6807\u7b7e\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548cGNN\u67b6\u6784\u4e0a\u9a8c\u8bc1\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u8fd8\u5c55\u793a\u4e86\u5728\u5206\u5b50\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u540c\u8d28\u6027\u5047\u8bbe\u5bb9\u6613\u5bfc\u81f4\u8fc7\u5ea6\u5e73\u6ed1\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u5f02\u8d28\u6027\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u5c40\u90e8\u8c03\u8282\u6d88\u606f\u4f20\u9012\u7b56\u7565\uff0c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u5e76\u6269\u5c55GNN\u7684\u5e94\u7528\u8303\u56f4\uff0c\u5982\u751f\u6210\u5efa\u6a21\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u5d4c\u5165\u7684\u6d88\u606f\u805a\u5408\u8c03\u8282\u65b9\u6848\uff0c\u901a\u8fc7\u5c40\u90e8\u63a7\u5236\u6d88\u606f\u4f20\u9012\u7684\u7c7b\u578b\u548c\u8303\u56f4\uff0c\u4fdd\u7559\u4fe1\u606f\u7684\u4f4e\u9891\u548c\u9ad8\u9891\u6210\u5206\uff0c\u65e0\u9700\u4f9d\u8d56\u8f85\u52a9\u6807\u7b7e\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548cGNN\u67b6\u6784\u4e0a\u9a8c\u8bc1\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u63ed\u793a\u4e86\u6807\u51c6\u5206\u7c7b\u57fa\u51c6\u4e2d\u7684\u5f02\u8d28\u6027\u6a21\u5f0f\uff0c\u5e76\u5728\u5206\u5b50\u751f\u6210\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6848\u6709\u6548\u7f13\u89e3\u4e86GNN\u7684\u8fc7\u5ea6\u5e73\u6ed1\u95ee\u9898\uff0c\u6269\u5c55\u4e86\u5176\u5728\u5f02\u8d28\u6027\u573a\u666f\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2504.19792", "pdf": "https://arxiv.org/pdf/2504.19792", "abs": "https://arxiv.org/abs/2504.19792", "authors": ["Runtian Zhai"], "title": "Contextures: The Mechanism of Representation Learning", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "PhD Dissertation", "summary": "This dissertation establishes the contexture theory to mathematically\ncharacterize the mechanism of representation learning, or pretraining. Despite\nthe remarkable empirical success of foundation models, it is not very clear\nwhat representations they learn, and why these representations are useful for\nvarious downstream tasks. A scientific understanding of representation learning\nis critical, especially at this point when scaling up the model size is\nproducing diminishing returns, and designing new pretraining methods is\nimperative for further progress.\n  Prior work treated different representation learning methods quite\ndifferently, whereas the contexture theory provides a unified framework for\nanalyzing these methods. The central argument is that a representation is\nlearned from the association between the input X and a context variable A. We\nprove that if an encoder captures the maximum information of this association,\nin which case we say that the encoder learns the contexture, then it will be\noptimal on the class of tasks that are compatible with the context. We also\nshow that a context is the most useful when the association between X and A is\nneither too strong nor too weak. The important implication of the contexture\ntheory is that increasing the model size alone will achieve diminishing\nreturns, and further advancements require better contexts.\n  We demonstrate that many pretraining objectives can learn the contexture,\nincluding supervised learning, self-supervised learning, generative models,\netc. Then, we introduce two general objectives -- SVME and KISE, for learning\nthe contexture. We also show how to mix multiple contexts together, an\neffortless way to create better contexts from existing ones. Then, we prove\nstatistical learning bounds for representation learning. Finally, we discuss\nthe effect of the data distribution shift from pretraining to the downstream\ntask.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\"\u4e0a\u4e0b\u6587\u7406\u8bba\"\u6765\u6570\u5b66\u5316\u8868\u5f81\u5b66\u4e60\u7684\u673a\u5236\uff0c\u6307\u51fa\u5355\u7eaf\u589e\u52a0\u6a21\u578b\u89c4\u6a21\u4f1a\u5bfc\u81f4\u6536\u76ca\u9012\u51cf\uff0c\u9700\u901a\u8fc7\u8bbe\u8ba1\u66f4\u597d\u7684\u4e0a\u4e0b\u6587\uff08context\uff09\u63a8\u52a8\u8fdb\u5c55\u3002", "motivation": "\u5f53\u524d\u57fa\u7840\u6a21\u578b\u5728\u8868\u5f81\u5b66\u4e60\u65b9\u9762\u867d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5b66\u4e60\u673a\u5236\u7f3a\u4e4f\u79d1\u5b66\u7406\u89e3\uff0c\u5c24\u5176\u5728\u6a21\u578b\u89c4\u6a21\u6269\u5927\u6536\u76ca\u9012\u51cf\u7684\u80cc\u666f\u4e0b\uff0c\u4e9f\u9700\u7406\u8bba\u6307\u5bfc\u4ee5\u4f18\u5316\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\"\u4e0a\u4e0b\u6587\u7406\u8bba\"\uff0c\u5b9a\u4e49\u8868\u5f81\u901a\u8fc7\u8f93\u5165X\u4e0e\u4e0a\u4e0b\u6587\u53d8\u91cfA\u7684\u5173\u8054\u5b66\u4e60\uff1b\u63d0\u51fa\u4e24\u79cd\u901a\u7528\u76ee\u6807\uff08SVME\u548cKISE\uff09\u5b66\u4e60\u4e0a\u4e0b\u6587\uff0c\u5e76\u8bc1\u660e\u7edf\u8ba1\u5b66\u4e60\u8fb9\u754c\u3002", "result": "\u8bc1\u660e\u5f53\u7f16\u7801\u5668\u6355\u83b7X\u4e0eA\u7684\u6700\u5927\u5173\u8054\u4fe1\u606f\uff08\u5373\u5b66\u4e60\u5230\"\u4e0a\u4e0b\u6587\"\uff09\u65f6\uff0c\u5bf9\u517c\u5bb9\u4efb\u52a1\u6700\u4f18\uff1b\u4e0a\u4e0b\u6587\u6548\u7528\u5728\u5176\u5173\u8054\u5f3a\u5ea6\u9002\u4e2d\u65f6\u6700\u9ad8\u3002", "conclusion": "\u6a21\u578b\u89c4\u6a21\u6269\u5927\u9700\u914d\u5408\u66f4\u597d\u7684\u4e0a\u4e0b\u6587\u8bbe\u8ba1\uff0c\u8bba\u6587\u4e3a\u591a\u7c7b\u9884\u8bad\u7ec3\u65b9\u6cd5\u63d0\u4f9b\u7406\u8bba\u652f\u6301\uff0c\u5e76\u63d0\u51fa\u4e86\u6df7\u5408\u4e0a\u4e0b\u6587\u53ca\u5e94\u5bf9\u6570\u636e\u5206\u5e03\u8fc1\u79fb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2504.19820", "pdf": "https://arxiv.org/pdf/2504.19820", "abs": "https://arxiv.org/abs/2504.19820", "authors": ["Yoonhyuk Choi", "Chong-Kwon Kim"], "title": "Hierarchical Uncertainty-Aware Graph Neural Network", "categories": ["cs.LG", "cs.IR"], "comment": null, "summary": "Recent research on graph neural networks (GNNs) has explored mechanisms for\ncapturing local uncertainty and exploiting graph hierarchies to mitigate data\nsparsity and leverage structural properties. However, the synergistic\nintegration of these two approaches remains underexplored. In this work, we\nintroduce a novel architecture, the Hierarchical Uncertainty-Aware Graph Neural\nNetwork (HU-GNN), which unifies multi-scale representation learning, principled\nuncertainty estimation, and self-supervised embedding diversity within a single\nend-to-end framework. Specifically, HU-GNN adaptively forms node clusters and\nestimates uncertainty at multiple structural scales from individual nodes to\nhigher levels. These uncertainty estimates guide a robust message-passing\nmechanism and attention weighting, effectively mitigating noise and adversarial\nperturbations while preserving predictive accuracy on both node- and\ngraph-level tasks. We also offer key theoretical contributions, including a\nprobabilistic formulation, rigorous uncertainty-calibration guarantees, and\nformal robustness bounds. Finally, by incorporating recent advances in graph\ncontrastive learning, HU-GNN maintains diverse, structurally faithful\nembeddings. Extensive experiments on standard benchmarks demonstrate that our\nmodel achieves state-of-the-art robustness and interpretability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aHU-GNN\u7684\u65b0\u578b\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u5c3a\u5ea6\u8868\u5f81\u5b66\u4e60\u3001\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u81ea\u76d1\u7763\u5d4c\u5165\u591a\u6837\u6027\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5f53\u524d\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u7814\u7a76\u5c40\u90e8\u4e0d\u786e\u5b9a\u6027\u548c\u5229\u7528\u56fe\u5c42\u6b21\u7ed3\u6784\u65b9\u9762\u7f3a\u4e4f\u534f\u540c\u6574\u5408\uff0c\u56e0\u6b64\u4f5c\u8005\u65e8\u5728\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "HU-GNN\u901a\u8fc7\u81ea\u9002\u5e94\u8282\u70b9\u805a\u7c7b\u548c\u591a\u5c3a\u5ea6\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u7ed3\u5408\u9c81\u68d2\u7684\u6d88\u606f\u4f20\u9012\u673a\u5236\u548c\u6ce8\u610f\u529b\u52a0\u6743\uff0c\u6709\u6548\u51cf\u5c11\u566a\u58f0\u548c\u5bf9\u6297\u6270\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHU-GNN\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "HU-GNN\u901a\u8fc7\u7edf\u4e00\u591a\u5c3a\u5ea6\u5b66\u4e60\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u4e3a\u56fe\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18931", "pdf": "https://arxiv.org/pdf/2504.18931", "abs": "https://arxiv.org/abs/2504.18931", "authors": ["Dianwei Chen", "Yaobang Gong", "Xianfeng Yang"], "title": "Advanced Longitudinal Control and Collision Avoidance for High-Risk Edge Cases in Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Advanced Driver Assistance Systems (ADAS) and Advanced Driving Systems (ADS)\nare key to improving road safety, yet most existing implementations focus\nprimarily on the vehicle ahead, neglecting the behavior of following vehicles.\nThis shortfall often leads to chain reaction collisions in high speed, densely\nspaced traffic particularly when a middle vehicle suddenly brakes and trailing\nvehicles cannot respond in time. To address this critical gap, we propose a\nnovel longitudinal control and collision avoidance algorithm that integrates\nadaptive cruising with emergency braking. Leveraging deep reinforcement\nlearning, our method simultaneously accounts for both leading and following\nvehicles. Through a data preprocessing framework that calibrates real-world\nsensor data, we enhance the robustness and reliability of the training process,\nensuring the learned policy can handle diverse driving conditions. In simulated\nhigh risk scenarios (e.g., emergency braking in dense traffic), the algorithm\neffectively prevents potential pile up collisions, even in situations involving\nheavy duty vehicles. Furthermore, in typical highway scenarios where three\nvehicles decelerate, the proposed DRL approach achieves a 99% success rate far\nsurpassing the standard Federal Highway Administration speed concepts guide,\nwhich reaches only 36.77% success under the same conditions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7eb5\u5411\u63a7\u5236\u548c\u78b0\u649e\u907f\u514d\u7b97\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u5f3a\u5316\u6846\u67b6\u540c\u65f6\u8003\u8651\u524d\u8f66\u548c\u540e\u8f66\u884c\u4e3a\uff0c\u6709\u6548\u9632\u6b62\u9ad8\u901f\u5bc6\u96c6\u4ea4\u901a\u4e2d\u7684\u8fde\u9501\u78b0\u649e\u3002", "motivation": "\u73b0\u6709ADAS\u548cADS\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u524d\u65b9\u8f66\u8f86\uff0c\u5ffd\u7565\u4e86\u540e\u65b9\u8f66\u8f86\u884c\u4e3a\uff0c\u5bfc\u81f4\u9ad8\u901f\u5bc6\u96c6\u4ea4\u901a\u4e2d\u8fde\u9501\u78b0\u649e\u9891\u53d1\uff0c\u7279\u522b\u662f\u4e2d\u95f4\u8f66\u8f86\u7a81\u7136\u5239\u8f66\u65f6\u3002", "method": "\u7ed3\u5408\u81ea\u9002\u5e94\u5de1\u822a\u4e0e\u7d27\u6025\u5236\u52a8\uff0c\u5229\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u6846\u67b6\uff0c\u540c\u65f6\u5904\u7406\u524d\u8f66\u548c\u540e\u8f66\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u9884\u5904\u7406\u63d0\u5347\u8bad\u7ec3\u8fc7\u7a0b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u9ad8\u98ce\u9669\u573a\u666f\uff08\u5982\u5bc6\u96c6\u4ea4\u901a\u4e2d\u7684\u7d27\u6025\u5236\u52a8\uff09\u4e2d\uff0c\u7b97\u6cd5\u6210\u529f\u907f\u514d\u8fde\u73af\u78b0\u649e\uff1b\u5728\u5178\u578b\u9ad8\u901f\u8def\u6bb5\u4e09\u8f66\u51cf\u901f\u6d4b\u8bd5\u4e2d\uff0c\u6210\u529f\u7387\u9ad8\u8fbe99%\uff0c\u8fdc\u8d85\u884c\u4e1a\u6807\u51c6\uff0836.77%\uff09\u3002", "conclusion": "\u8be5DRL\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u901f\u5bc6\u96c6\u4ea4\u901a\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u4e3aADAS/ADS\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u7eb5\u5411\u63a7\u5236\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.19822", "pdf": "https://arxiv.org/pdf/2504.19822", "abs": "https://arxiv.org/abs/2504.19822", "authors": ["Minjong Cheon"], "title": "Mj\u00f6lnir: A Deep Learning Parametrization Framework for Global Lightning Flash Density", "categories": ["cs.LG", "cs.AI", "cs.CV", "physics.ao-ph"], "comment": null, "summary": "Recent advances in AI-based weather forecasting models, such as FourCastNet,\nPangu-Weather, and GraphCast, have demonstrated the remarkable ability of deep\nlearning to emulate complex atmospheric dynamics. Building on this momentum, we\npropose Mj\\\"olnir, a novel deep learning-based framework for global lightning\nflash density parameterization. Trained on ERA5 atmospheric predictors and\nWorld Wide Lightning Location Network (WWLLN) observations at a daily temporal\nresolution and 1 degree spatial resolution, Mj\\\"olnir captures the nonlinear\nmapping between large-scale environmental conditions and lightning activity.\nThe model architecture is based on the InceptionNeXt backbone with SENet, and a\nmulti-task learning strategy to simultaneously predict lightning occurrence and\nmagnitude. Extensive evaluations yield that Mollnir accurately reproduces the\nglobal distribution, seasonal variability, and regional characteristics of\nlightning activity, achieving a global Pearson correlation coefficient of 0.96\nfor annual mean fields. These results suggest that Mj\\\"olnir serves not only as\nan effective data-driven global lightning parameterization but also as a\npromising AI-based scheme for next-generation Earth system models (AI-ESMs).", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Mj\"olnir\uff0c\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5168\u7403\u95ea\u7535\u5bc6\u5ea6\u53c2\u6570\u5316\u6846\u67b6\uff0c\u901a\u8fc7InceptionNeXt\u67b6\u6784\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u7b56\u7565\uff0c\u51c6\u786e\u9884\u6d4b\u95ea\u7535\u6d3b\u52a8\uff0c\u5e76\u5728\u5168\u7403\u8303\u56f4\u5185\u53d6\u5f97\u4e86\u9ad8\u76f8\u5173\u6027\u7ed3\u679c\u3002", "motivation": "\u8fd1\u5e74\u6765AI\u5929\u6c14\u9884\u6d4b\u6a21\u578b\u7684\u6210\u529f\u6fc0\u53d1\u4e86\u5c06\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u4e8e\u5730\u7403\u7cfb\u7edf\u5efa\u6a21\u7684\u5174\u8da3\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u5168\u7403\u95ea\u7535\u53c2\u6570\u5316\u65b9\u6848\uff0c\u4ee5\u63d0\u5347\u4e0b\u4e00\u4ee3AI\u5730\u7403\u7cfb\u7edf\u6a21\u578b\uff08AI-ESMs\uff09\u7684\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u4f7f\u7528ERA5\u5927\u6c14\u9884\u6d4b\u56e0\u5b50\u548cWWLLN\u95ea\u7535\u89c2\u6d4b\u6570\u636e\uff0c\u4ee5InceptionNeXt\u4e3a\u6a21\u578b\u4e3b\u5e72\uff0c\u7ed3\u5408SENet\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u7b56\u7565\uff0c\u540c\u65f6\u9884\u6d4b\u95ea\u7535\u53d1\u751f\u548c\u5f3a\u5ea6\u3002", "result": "Mj\"olnir\u80fd\u51c6\u786e\u91cd\u73b0\u95ea\u7535\u6d3b\u52a8\u7684\u5168\u7403\u5206\u5e03\u3001\u5b63\u8282\u53d8\u5316\u548c\u533a\u57df\u7279\u5f81\uff0c\u5e74\u5e73\u5747\u503c\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u8fbe0.96\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e0d\u4ec5\u662f\u4e00\u79cd\u6709\u6548\u7684\u5168\u7403\u95ea\u7535\u6570\u636e\u9a71\u52a8\u53c2\u6570\u5316\u5de5\u5177\uff0c\u8fd8\u4e3a\u4e0b\u4e00\u4ee3AI\u5730\u7403\u7cfb\u7edf\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u6848\u3002"}}
{"id": "2504.18932", "pdf": "https://arxiv.org/pdf/2504.18932", "abs": "https://arxiv.org/abs/2504.18932", "authors": ["Dong Whi Yoo", "Jiayue Melissa Shi", "Violeta J. Rodriguez", "Koustuv Saha"], "title": "AI Chatbots for Mental Health: Values and Harms from Lived Experiences of Depression", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Recent advancements in LLMs enable chatbots to interact with individuals on a\nrange of queries, including sensitive mental health contexts. Despite\nuncertainties about their effectiveness and reliability, the development of\nLLMs in these areas is growing, potentially leading to harms. To better\nidentify and mitigate these harms, it is critical to understand how the values\nof people with lived experiences relate to the harms. In this study, we\ndeveloped a technology probe, a GPT-4o based chatbot called Zenny, enabling\nparticipants to engage with depression self-management scenarios informed by\nprevious research. We used Zenny to interview 17 individuals with lived\nexperiences of depression. Our thematic analysis revealed key values:\ninformational support, emotional support, personalization, privacy, and crisis\nmanagement. This work explores the relationship between lived experience\nvalues, potential harms, and design recommendations for mental health AI\nchatbots, aiming to enhance self-management support while minimizing risks.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8eGPT-4\u7684\u804a\u5929\u673a\u5668\u4ebaZenny\u5728\u6291\u90c1\u75c7\u81ea\u6211\u7ba1\u7406\u4e2d\u7684\u4f5c\u7528\uff0c\u901a\u8fc7\u4e0e17\u540d\u6291\u90c1\u75c7\u60a3\u8005\u8bbf\u8c08\uff0c\u63ed\u793a\u4e86\u5176\u5728\u4fe1\u606f\u652f\u6301\u3001\u60c5\u611f\u652f\u6301\u7b49\u65b9\u9762\u7684\u4ef7\u503c\u53ca\u6f5c\u5728\u98ce\u9669\u3002", "motivation": "\u968f\u7740LLMs\u5728\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684\u5e94\u7528\u589e\u957f\uff0c\u4e86\u89e3\u5176\u6f5c\u5728\u5371\u5bb3\u53ca\u5982\u4f55\u4e0e\u60a3\u8005\u4ef7\u503c\u89c2\u5173\u8054\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u4f18\u5316\u8bbe\u8ba1\u5e76\u51cf\u5c11\u98ce\u9669\u3002", "method": "\u5f00\u53d1\u4e86GPT-4o\u804a\u5929\u673a\u5668\u4ebaZenny\uff0c\u901a\u8fc7\u6a21\u62df\u6291\u90c1\u75c7\u81ea\u6211\u7ba1\u7406\u573a\u666f\uff0c\u8bbf\u8c0817\u540d\u60a3\u8005\uff0c\u5e76\u8fdb\u884c\u4e3b\u9898\u5206\u6790\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u4e94\u5927\u5173\u952e\u4ef7\u503c\uff1a\u4fe1\u606f\u652f\u6301\u3001\u60c5\u611f\u652f\u6301\u3001\u4e2a\u6027\u5316\u3001\u9690\u79c1\u548c\u5371\u673a\u7ba1\u7406\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u4e0e\u6f5c\u5728\u5371\u5bb3\u7684\u5173\u7cfb\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5fc3\u7406\u5065\u5eb7AI\u804a\u5929\u673a\u5668\u4eba\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5efa\u8bae\uff0c\u65e8\u5728\u589e\u5f3a\u652f\u6301\u540c\u65f6\u964d\u4f4e\u98ce\u9669\uff0c\u5f3a\u8c03\u60a3\u8005\u4ef7\u503c\u89c2\u4e0e\u6280\u672f\u7684\u7ed3\u5408\u3002"}}
{"id": "2504.19874", "pdf": "https://arxiv.org/pdf/2504.19874", "abs": "https://arxiv.org/abs/2504.19874", "authors": ["Amir Zandieh", "Majid Daliri", "Majid Hadian", "Vahab Mirrokni"], "title": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate", "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.DS"], "comment": "25 pages", "summary": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero.", "AI": {"tldr": "TurboQuant\u662f\u4e00\u79cd\u65b0\u578b\u7684\u5411\u91cf\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u968f\u673a\u65cb\u8f6c\u548c\u9ad8\u7ef4\u5750\u6807\u72ec\u7acb\u6027\u5b9e\u73b0\u8fd1\u6700\u4f18\u7684\u5931\u771f\u7387\uff0c\u6539\u8fdb\u4e86\u73b0\u6709\u65b9\u6cd5\u5728MSE\u548c\u5185\u79ef\u5931\u771f\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u5411\u91cf\u91cf\u5316\u65b9\u6cd5\u5728\u591a\u7ef4\u6570\u636e\u4e2d\u7684\u5931\u771f\u7387\u4e0d\u7406\u60f3\uff0c\u5c24\u5176\u662f\u5728MSE\u548c\u5185\u79ef\u5931\u771f\u4e0a\u5b58\u5728\u77db\u76fe\u6216\u6027\u80fd\u4e0d\u8db3\u3002TurboQuant\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u4f18\u7684\u91cf\u5316\u65b9\u6848\u3002", "method": "TurboQuant\u91c7\u7528\u6570\u636e\u65e0\u5173\u7684\u968f\u673a\u65cb\u8f6c\u6280\u672f\uff0c\u5229\u7528\u9ad8\u7ef4\u5750\u6807\u7684\u72ec\u7acb\u6027\u548cBeta\u5206\u5e03\u7279\u6027\uff0c\u5148\u4f7f\u7528MSE\u6700\u4f18\u91cf\u5316\u5668\uff0c\u518d\u901a\u8fc71\u4f4dQJL\u53d8\u6362\u6821\u6b63\u5185\u79ef\u504f\u5dee\u3002", "result": "TurboQuant\u5728KV\u7f13\u5b58\u91cf\u5316\u4e2d\u5b9e\u73b03.5\u4f4d/\u901a\u9053\u7684\u7edd\u5bf9\u8d28\u91cf\u4e2d\u6027\uff0c2.5\u4f4d/\u901a\u9053\u8fd1\u4e4e\u65e0\u635f\uff1b\u5728\u6700\u8fd1\u90bb\u641c\u7d22\u4efb\u52a1\u4e2d\uff0c\u53ec\u56de\u7387\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u7d22\u5f15\u65f6\u95f4\u8d8b\u8fd1\u4e8e\u96f6\u3002", "conclusion": "TurboQuant\u4ee5\u8fd1\u6700\u4f18\u5931\u771f\u7387\u548c\u65e0\u504f\u5185\u79ef\u4f30\u8ba1\u4e3a\u7279\u70b9\uff0c\u4e3a\u9ad8\u7ef4\u5411\u91cf\u91cf\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7406\u8bba\u548c\u5b9e\u9a8c\u8868\u73b0\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2504.19901", "pdf": "https://arxiv.org/pdf/2504.19901", "abs": "https://arxiv.org/abs/2504.19901", "authors": ["Hude Liu", "Jerry Yao-Chieh Hu", "Zhao Song", "Han Liu"], "title": "Attention Mechanism, Max-Affine Partition, and Universal Approximation", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "We establish the universal approximation capability of single-layer,\nsingle-head self- and cross-attention mechanisms with minimal attached\nstructures. Our key insight is to interpret single-head attention as an input\ndomain-partition mechanism that assigns distinct values to subregions. This\nallows us to engineer the attention weights such that this assignment imitates\nthe target function. Building on this, we prove that a single self-attention\nlayer, preceded by sum-of-linear transformations, is capable of approximating\nany continuous function on a compact domain under the $L_\\infty$-norm.\nFurthermore, we extend this construction to approximate any Lebesgue integrable\nfunction under $L_p$-norm for $1\\leq p <\\infty$. Lastly, we also extend our\ntechniques and show that, for the first time, single-head cross-attention\nachieves the same universal approximation guarantees.", "AI": {"tldr": "\u5355\u5c42\u5355\u5934\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5177\u6709\u901a\u7528\u903c\u8fd1\u80fd\u529b\uff0c\u4ec5\u9700\u6700\u5c0f\u9644\u52a0\u7ed3\u6784\u5373\u53ef\u8fd1\u4f3c\u8fde\u7eed\u51fd\u6570\u6216\u52d2\u8d1d\u683c\u53ef\u79ef\u51fd\u6570\u3002", "motivation": "\u63a2\u7d22\u5355\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u901a\u7528\u903c\u8fd1\u6f5c\u529b\uff0c\u9a8c\u8bc1\u5176\u5728\u6570\u5b66\u4e0a\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u6700\u5c0f\u7ed3\u6784\u4e0b\u7684\u529f\u80fd\u3002", "method": "\u901a\u8fc7\u5c06\u5355\u5934\u6ce8\u610f\u529b\u89e3\u91ca\u4e3a\u8f93\u5165\u57df\u5206\u533a\u673a\u5236\uff0c\u8bbe\u8ba1\u6ce8\u610f\u529b\u6743\u91cd\u4ee5\u6a21\u62df\u76ee\u6807\u51fd\u6570\uff0c\u7ed3\u5408\u7ebf\u6027\u53d8\u6362\u548c\u6570\u5b66\u8bc1\u660e\u3002", "result": "\u5355\u5c42\u81ea\u6ce8\u610f\u529b\u53ef\u4ee5$L_\\infty$-\u8303\u6570\u903c\u8fd1\u8fde\u7eed\u51fd\u6570\uff0c$L_p$-\u8303\u6570\u903c\u8fd1\u52d2\u8d1d\u683c\u53ef\u79ef\u51fd\u6570\uff1b\u4ea4\u53c9\u6ce8\u610f\u529b\u9996\u6b21\u8fbe\u5230\u76f8\u540c\u6548\u679c\u3002", "conclusion": "\u5355\u5934\u6ce8\u610f\u529b\u673a\u5236\u5728\u7406\u8bba\u4e0a\u662f\u901a\u7528\u903c\u8fd1\u5668\uff0c\u4e3a\u7b80\u5316\u6ce8\u610f\u529b\u67b6\u6784\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2504.18943", "pdf": "https://arxiv.org/pdf/2504.18943", "abs": "https://arxiv.org/abs/2504.18943", "authors": ["Martin Berger", "Nathana\u00ebl Fijalkow", "Mojtaba Valizadeh"], "title": "GPU accelerated program synthesis: Enumerate semantics, not syntax!", "categories": ["cs.PL", "cs.AI", "cs.LO", "68", "D.3"], "comment": "10 pages", "summary": "Program synthesis is an umbrella term for generating programs and logical\nformulae from specifications. With the remarkable performance improvements that\nGPUs enable for deep learning, a natural question arose: can we also implement\na search-based program synthesiser on GPUs to achieve similar performance\nimprovements? In this article we discuss our insights on this question, based\non recent works~. The goal is to build a synthesiser running on GPUs which\ntakes as input positive and negative example traces and returns a logical\nformula accepting the positive and rejecting the negative traces. With\nGPU-friendly programming techniques -- using the semantics of formulae to\nminimise data movement and reduce data-dependent branching -- our synthesiser\nscales to significantly larger synthesis problems, and operates much faster\nthan the previous CPU-based state-of-the-art. We believe the insights that make\nour approach GPU-friendly have wide potential for enhancing the performance of\nother formal methods (FM) workloads.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528GPU\u4f18\u5316\u7a0b\u5e8f\u5408\u6210\u6280\u672f\uff0c\u901a\u8fc7\u51cf\u5c11\u6570\u636e\u79fb\u52a8\u548c\u4f9d\u8d56\u5206\u652f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5408\u6210\u5668\u7684\u6027\u80fd\u548c\u89c4\u6a21\u3002", "motivation": "\u968f\u7740GPU\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u5c06\u5176\u5e94\u7528\u4e8e\u7a0b\u5e8f\u5408\u6210\uff0c\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "\u91c7\u7528GPU\u53cb\u597d\u7684\u7f16\u7a0b\u6280\u672f\uff0c\u5229\u7528\u516c\u5f0f\u8bed\u4e49\u51cf\u5c11\u6570\u636e\u79fb\u52a8\u548c\u4f9d\u8d56\u5206\u652f\u3002", "result": "GPU\u5408\u6210\u7684\u6027\u80fd\u663e\u8457\u4f18\u4e8eCPU\uff0c\u80fd\u591f\u5904\u7406\u66f4\u5927\u89c4\u6a21\u7684\u95ee\u9898\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u7a0b\u5e8f\u5408\u6210\uff0c\u8fd8\u53ef\u80fd\u63d0\u5347\u5176\u4ed6\u5f62\u5f0f\u5316\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2504.19903", "pdf": "https://arxiv.org/pdf/2504.19903", "abs": "https://arxiv.org/abs/2504.19903", "authors": ["Diying Yang", "Yingwei Hou", "Danyang Xiao", "Weigang Wu"], "title": "Convergence Analysis of Asynchronous Federated Learning with Gradient Compression for Non-Convex Optimization", "categories": ["cs.LG"], "comment": null, "summary": "Gradient compression is an effective technique for reducing communication\ncosts in federated learning (FL), and error feedback (EF) is usually adopted to\nremedy the compression errors. However, there remains a lack of systematic\nstudy on these techniques in asynchronous FL. In this paper, we fill this gap\nby analyzing the convergence behaviors of FL under different frameworks. We\nfirstly consider a basic asynchronous FL framework AsynFL, and provide an\nimproved convergence analysis that relies on fewer assumptions and yields a\nsuperior convergence rate than prior studies. Then, we consider a variant\nframework with gradient compression, AsynFLC. We show sufficient conditions for\nits convergence to the optimum, indicating the interaction between asynchronous\ndelay and compression rate. Our analysis also demonstrates that asynchronous\ndelay amplifies the variance caused by compression, thereby hindering\nconvergence, and such an impact is exacerbated by high data heterogeneity.\nFurthermore, we study the convergence of AsynFLC-EF, the framework that further\nintegrates EF. We prove that EF can effectively reduce the variance of gradient\nestimation despite asynchronous delay, which enables AsynFLC-EF to match the\nconvergence rate of AsynFL. We also show that the impact of asynchronous delay\non EF is limited to slowing down the higher-order convergence term.\nExperimental results substantiate our analytical findings very well.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u68af\u5ea6\u538b\u7f29\u5728\u5f02\u6b65\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4e2d\u7684\u6536\u655b\u884c\u4e3a\uff0c\u5206\u6790\u4e86\u5f02\u6b65\u6846\u67b6AsynFL\u548c\u6539\u8fdb\u6846\u67b6AsynFLC\u53caAsynFLC-EF\u7684\u6536\u655b\u6027\uff0c\u53d1\u73b0\u8bef\u5dee\u53cd\u9988\uff08EF\uff09\u80fd\u6709\u6548\u51cf\u5c11\u5f02\u6b65\u5ef6\u8fdf\u5bf9\u68af\u5ea6\u4f30\u8ba1\u7684\u5f71\u54cd\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u5bf9\u5f02\u6b65FL\u4e2d\u68af\u5ea6\u538b\u7f29\u548c\u8bef\u5dee\u53cd\u9988\u6280\u672f\u7684\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5206\u6790\u4e0d\u540c\u6846\u67b6\u4e0b\u7684\u6536\u655b\u884c\u4e3a\u3002", "method": "\u7814\u7a76\u4e86\u5f02\u6b65FL\u7684\u4e24\u79cd\u6846\u67b6\uff08AsynFL\u548cAsynFLC\uff09\uff0c\u5e76\u8fdb\u4e00\u6b65\u6574\u5408\u4e86\u8bef\u5dee\u53cd\u9988\uff08EF\uff09\u4ee5\u6539\u8fdb\u68af\u5ea6\u538b\u7f29\u7684\u6548\u679c\u3002", "result": "\u5206\u6790\u8868\u660e\u5f02\u6b65\u5ef6\u8fdf\u4f1a\u653e\u5927\u538b\u7f29\u5e26\u6765\u7684\u65b9\u5dee\uff0cEF\u80fd\u6709\u6548\u51cf\u5c11\u8fd9\u79cd\u5f71\u54cd\uff0c\u4f7fAsynFLC-EF\u7684\u6536\u655b\u7387\u4e0eAsynFL\u76f8\u5f53\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5206\u6790\u7ed3\u679c\u3002", "conclusion": "\u8bef\u5dee\u53cd\u9988\u80fd\u591f\u663e\u8457\u51cf\u8f7b\u5f02\u6b65\u5ef6\u8fdf\u5bf9\u68af\u5ea6\u538b\u7f29\u7684\u5f71\u54cd\uff0c\u63d0\u5347FL\u7684\u6536\u655b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6570\u636e\u5f02\u6784\u6027\u9ad8\u7684\u60c5\u51b5\u4e0b\u6548\u679c\u66f4\u660e\u663e\u3002"}}
{"id": "2504.18953", "pdf": "https://arxiv.org/pdf/2504.18953", "abs": "https://arxiv.org/abs/2504.18953", "authors": ["Sahar Ramezani Jolfaei", "Sepehr Khodadadi Hossein Abadi"], "title": "Application of the Brain Drain Optimization Algorithm to the N-Queens Problem", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "This paper introduces the application of the Brain Drain Optimization\nalgorithm -- a swarm-based metaheuristic inspired by the emigration of\nintellectual elites -- to the N-Queens problem. The N-Queens problem, a classic\ncombinatorial optimization problem, serves as a challenge for applying the\nBRADO. A designed cost function guides the search, and the configurations are\ntuned using a TOPSIS-based multicriteria decision making process. BRADO\nconsistently outperforms alternatives in terms of solution quality, achieving\nfewer threats and better objective function values. To assess BRADO's efficacy,\nit is benchmarked against several established metaheuristic algorithms,\nincluding Particle Swarm Optimization (PSO), Genetic Algorithm (GA),\nImperialist Competitive Algorithm (ICA), Iterated Local Search (ILS), and basic\nLocal Search (LS). The study highlights BRADO's potential as a general-purpose\nsolver for combinatorial problems, opening pathways for future applications in\nother domains of artificial intelligence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7fa4\u4f53\u667a\u80fd\u7684Brain Drain Optimization\uff08BRADO\uff09\u7b97\u6cd5\uff0c\u5e94\u7528\u4e8eN\u7687\u540e\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u591a\u51c6\u5219\u51b3\u7b56\u4f18\u5316\u914d\u7f6e\u3002BRADO\u5728\u89e3\u8d28\u91cf\u548c\u76ee\u6807\u51fd\u6570\u503c\u4e0a\u4f18\u4e8e\u5176\u4ed6\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5c06\u53d7\u77e5\u8bc6\u7cbe\u82f1\u79fb\u6c11\u542f\u53d1\u7684BRADO\u7b97\u6cd5\u5e94\u7528\u4e8e\u7ecf\u5178\u7ec4\u5408\u4f18\u5316\u95ee\u9898N\u7687\u540e\u95ee\u9898\uff0c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8eTOPSIS\u7684\u591a\u51c6\u5219\u51b3\u7b56\u65b9\u6cd5\u8c03\u6574\u914d\u7f6e\uff0c\u8bbe\u8ba1\u6210\u672c\u51fd\u6570\u5f15\u5bfc\u641c\u7d22\u3002", "result": "BRADO\u5728\u89e3\u8d28\u91cf\u4e0a\u4f18\u4e8ePSO\u3001GA\u3001ICA\u3001ILS\u548cLS\u7b49\u7b97\u6cd5\uff0c\u8868\u73b0\u51fa\u66f4\u5c11\u7684\u5a01\u80c1\u548c\u66f4\u4f18\u7684\u76ee\u6807\u51fd\u6570\u503c\u3002", "conclusion": "BRADO\u5c55\u73b0\u51fa\u4f5c\u4e3a\u901a\u7528\u7ec4\u5408\u95ee\u9898\u6c42\u89e3\u5668\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u5728AI\u5176\u4ed6\u9886\u57df\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2504.19955", "pdf": "https://arxiv.org/pdf/2504.19955", "abs": "https://arxiv.org/abs/2504.19955", "authors": ["Malhar A. Managoli", "Vinod M. Prabhakaran", "Suhas Diggavi"], "title": "Robust Federated Personalised Mean Estimation for the Gaussian Mixture Model", "categories": ["cs.LG", "cs.IT", "math.IT"], "comment": null, "summary": "Federated learning with heterogeneous data and personalization has received\nsignificant recent attention. Separately, robustness to corrupted data in the\ncontext of federated learning has also been studied. In this paper we explore\ncombining personalization for heterogeneous data with robustness, where a\nconstant fraction of the clients are corrupted. Motivated by this broad\nproblem, we formulate a simple instantiation which captures some of its\ndifficulty. We focus on the specific problem of personalized mean estimation\nwhere the data is drawn from a Gaussian mixture model. We give an algorithm\nwhose error depends almost linearly on the ratio of corrupted to uncorrupted\nsamples, and show a lower bound with the same behavior, albeit with a gap of a\nconstant factor.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e2a\u6027\u5316\u5747\u503c\u4f30\u8ba1\u7b97\u6cd5\uff0c\u9488\u5bf9\u8054\u90a6\u5b66\u4e60\u4e2d\u5f02\u6784\u6570\u636e\u548c\u90e8\u5206\u5ba2\u6237\u7aef\u6570\u636e\u635f\u574f\u7684\u95ee\u9898\uff0c\u7ed9\u51fa\u4e86\u8bef\u5dee\u8fd1\u4f3c\u7ebf\u6027\u4f9d\u8d56\u4e8e\u635f\u574f\u4e0e\u672a\u635f\u574f\u6837\u672c\u6bd4\u4f8b\u7684\u7b97\u6cd5\uff0c\u5e76\u8fdb\u884c\u4e86\u7406\u8bba\u4e0b\u754c\u5206\u6790\u3002", "motivation": "\u7814\u7a76\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7ed3\u5408\u4e2a\u6027\u5316\u5904\u7406\u5f02\u6784\u6570\u636e\u548c\u6570\u636e\u635f\u574f\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u90e8\u5206\u5ba2\u6237\u7aef\u6570\u636e\u635f\u574f\u7684\u60c5\u51b5\uff0c\u4ee5\u89e3\u51b3\u5b9e\u9645\u5e94\u7528\u4e2d\u5e38\u89c1\u7684\u6311\u6218\u3002", "method": "\u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7684\u4e2a\u6027\u5316\u5747\u503c\u4f30\u8ba1\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7b97\u6cd5\uff0c\u5176\u8bef\u5dee\u4e0e\u635f\u574f\u4e0e\u672a\u635f\u574f\u6837\u672c\u7684\u6bd4\u4f8b\u51e0\u4e4e\u5448\u7ebf\u6027\u5173\u7cfb\u3002", "result": "\u7b97\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5c55\u793a\u4e86\u8bef\u5dee\u4e0e\u635f\u574f\u6bd4\u4f8b\u7684\u7ebf\u6027\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u4e0b\u754c\u8bc1\u660e\u4e86\u5176\u6027\u80fd\u63a5\u8fd1\u6700\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5f02\u6784\u6570\u636e\u548c\u6570\u636e\u635f\u574f\u95ee\u9898\u4e0a\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u7f29\u5c0f\u7406\u8bba\u4e0e\u5b9e\u9645\u4e4b\u95f4\u7684\u5e38\u6570\u5dee\u8ddd\u3002"}}
{"id": "2504.18954", "pdf": "https://arxiv.org/pdf/2504.18954", "abs": "https://arxiv.org/abs/2504.18954", "authors": ["Marco Mezzina", "Pieter De Backer", "Tom Vercauteren", "Matthew Blaschko", "Alexandre Mottrie", "Tinne Tuytelaars"], "title": "Surgeons vs. Computer Vision: A comparative analysis on surgical phase recognition capabilities", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Purpose: Automated Surgical Phase Recognition (SPR) uses Artificial\nIntelligence (AI) to segment the surgical workflow into its key events,\nfunctioning as a building block for efficient video review, surgical education\nas well as skill assessment. Previous research has focused on short and linear\nsurgical procedures and has not explored if temporal context influences\nexperts' ability to better classify surgical phases. This research addresses\nthese gaps, focusing on Robot-Assisted Partial Nephrectomy (RAPN) as a highly\nnon-linear procedure. Methods: Urologists of varying expertise were grouped and\ntasked to indicate the surgical phase for RAPN on both single frames and video\nsnippets using a custom-made web platform. Participants reported their\nconfidence levels and the visual landmarks used in their decision-making. AI\narchitectures without and with temporal context as trained and benchmarked on\nthe Cholec80 dataset were subsequently trained on this RAPN dataset. Results:\nVideo snippets and presence of specific visual landmarks improved phase\nclassification accuracy across all groups. Surgeons displayed high confidence\nin their classifications and outperformed novices, who struggled discriminating\nphases. The performance of the AI models is comparable to the surgeons in the\nsurvey, with improvements when temporal context was incorporated in both cases.\nConclusion: SPR is an inherently complex task for expert surgeons and computer\nvision, where both perform equally well when given the same context.\nPerformance increases when temporal information is provided. Surgical tools and\norgans form the key landmarks for human interpretation and are expected to\nshape the future of automated SPR.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u81ea\u52a8\u5316\u624b\u672f\u9636\u6bb5\u8bc6\u522b\uff08SPR\uff09\u5728\u673a\u5668\u4eba\u8f85\u52a9\u90e8\u5206\u80be\u5207\u9664\u672f\uff08RAPN\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u89c6\u9891\u7247\u6bb5\u548c\u89c6\u89c9\u6807\u5fd7\u7269\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u672a\u63a2\u7d22\u65f6\u95f4\u80cc\u666f\u5bf9\u4e13\u5bb6\u5206\u7c7b\u624b\u672f\u9636\u6bb5\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u5e76\u586b\u8865\u7ebf\u6027\u624b\u672f\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5b9a\u5236\u7f51\u7edc\u5e73\u53f0\uff0c\u4e0d\u540c\u7ecf\u9a8c\u7684\u6ccc\u5c3f\u79d1\u533b\u751f\u5bf9RAPN\u7684\u5355\u5e27\u548c\u89c6\u9891\u7247\u6bb5\u8fdb\u884c\u9636\u6bb5\u6807\u6ce8\uff0c\u5e76\u8bad\u7ec3\u5e26/\u4e0d\u5e26\u65f6\u95f4\u80cc\u666f\u7684AI\u6a21\u578b\u3002", "result": "\u89c6\u9891\u7247\u6bb5\u548c\u89c6\u89c9\u6807\u5fd7\u7269\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u4e13\u5bb6\u8868\u73b0\u4f18\u4e8e\u65b0\u624b\uff0cAI\u6a21\u578b\u4e0e\u4e13\u5bb6\u8868\u73b0\u76f8\u5f53\uff0c\u52a0\u5165\u65f6\u95f4\u80cc\u666f\u540e\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "SPR\u5bf9\u4e13\u5bb6\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u5747\u5177\u6311\u6218\u6027\uff0c\u65f6\u95f4\u4fe1\u606f\u7684\u52a0\u5165\u80fd\u63d0\u5347\u8868\u73b0\uff0c\u624b\u672f\u5de5\u5177\u548c\u5668\u5b98\u662f\u5173\u952e\u6807\u5fd7\u7269\uff0c\u5c06\u63a8\u52a8\u81ea\u52a8\u5316SPR\u53d1\u5c55\u3002"}}
{"id": "2504.19979", "pdf": "https://arxiv.org/pdf/2504.19979", "abs": "https://arxiv.org/abs/2504.19979", "authors": ["Liyuan Wang", "Jiachen Chen", "Kathryn L. Lunetta", "Danyang Huang", "Huimin Cheng", "Debarghya Mukherjee"], "title": "Transfer Learning Under High-Dimensional Network Convolutional Regression Model", "categories": ["cs.LG", "stat.ME"], "comment": null, "summary": "Transfer learning enhances model performance by utilizing knowledge from\nrelated domains, particularly when labeled data is scarce. While existing\nresearch addresses transfer learning under various distribution shifts in\nindependent settings, handling dependencies in networked data remains\nchallenging. To address this challenge, we propose a high-dimensional transfer\nlearning framework based on network convolutional regression (NCR), inspired by\nthe success of graph convolutional networks (GCNs). The NCR model incorporates\nrandom network structure by allowing each node's response to depend on its\nfeatures and the aggregated features of its neighbors, capturing local\ndependencies effectively. Our methodology includes a two-step transfer learning\nalgorithm that addresses domain shift between source and target networks, along\nwith a source detection mechanism to identify informative domains.\nTheoretically, we analyze the lasso estimator in the context of a random graph\nbased on the Erdos-Renyi model assumption, demonstrating that transfer learning\nimproves convergence rates when informative sources are present. Empirical\nevaluations, including simulations and a real-world application using Sina\nWeibo data, demonstrate substantial improvements in prediction accuracy,\nparticularly when labeled data in the target domain is limited.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f51\u7edc\u5377\u79ef\u56de\u5f52\uff08NCR\uff09\u7684\u9ad8\u7ef4\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u7f51\u7edc\u5316\u6570\u636e\u4e2d\u7684\u4f9d\u8d56\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u5904\u7406\u72ec\u7acb\u8bbe\u7f6e\u4e0b\u7684\u5206\u5e03\u504f\u79fb\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u7f51\u7edc\u5316\u6570\u636e\u4e2d\u7684\u4f9d\u8d56\u6027\u95ee\u9898\u7814\u7a76\u4e0d\u8db3\u3002\u8be5\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u5347\u5728\u76ee\u6807\u9886\u57df\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u65f6\u7684\u6a21\u578b\u6027\u80fd\u3002", "method": "\u91c7\u7528\u7f51\u7edc\u5377\u79ef\u56de\u5f52\uff08NCR\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u6b65\u8fc1\u79fb\u5b66\u4e60\u7b97\u6cd5\u89e3\u51b3\u6e90\u7f51\u7edc\u4e0e\u76ee\u6807\u7f51\u7edc\u7684\u57df\u504f\u79fb\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u6e90\u68c0\u6d4b\u673a\u5236\u8bc6\u522b\u4fe1\u606f\u4e30\u5bcc\u7684\u9886\u57df\u3002\u7406\u8bba\u5206\u6790\u57fa\u4e8eErdos-Renyi\u6a21\u578b\u5047\u8bbe\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\uff08\u5305\u62ec\u6a21\u62df\u5b9e\u9a8c\u548c\u5fae\u535a\u6570\u636e\u5e94\u7528\uff09\u8868\u660e\uff0c\u5f53\u5b58\u5728\u4fe1\u606f\u4e30\u5bcc\u7684\u6e90\u9886\u57df\u65f6\uff0c\u8fc1\u79fb\u5b66\u4e60\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u76ee\u6807\u9886\u57df\u6807\u6ce8\u6570\u636e\u6709\u9650\u65f6\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "NCR\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u7f51\u7edc\u5316\u6570\u636e\u4e2d\u7684\u8fc1\u79fb\u5b66\u4e60\u95ee\u9898\uff0c\u7406\u8bba\u548c\u5b9e\u8bc1\u7ed3\u679c\u5747\u652f\u6301\u5176\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2504.18961", "pdf": "https://arxiv.org/pdf/2504.18961", "abs": "https://arxiv.org/abs/2504.18961", "authors": ["Junjie Zhou"], "title": "Feature Fusion Revisited: Multimodal CTR Prediction for MMCTR Challenge", "categories": ["cs.IR", "cs.AI"], "comment": "A technical report for the MMCTR Challenge held by EReL@MIR Workshop\n  at WWW 2025", "summary": "With the rapid advancement of Multimodal Large Language Models (MLLMs), an\nincreasing number of researchers are exploring their application in\nrecommendation systems. However, the high latency associated with large models\npresents a significant challenge for such use cases. The EReL@MIR workshop\nprovided a valuable opportunity to experiment with various approaches aimed at\nimproving the efficiency of multimodal representation learning for information\nretrieval tasks. As part of the competition's requirements, participants were\nmandated to submit a technical report detailing their methodologies and\nfindings. Our team was honored to receive the award for Task 2 - Winner\n(Multimodal CTR Prediction). In this technical report, we present our methods\nand key findings. Additionally, we propose several directions for future work,\nparticularly focusing on how to effectively integrate recommendation signals\ninto multimodal representations. The codebase for our implementation is\npublicly available at: https://github.com/Lattice-zjj/MMCTR_Code, and the\ntrained model weights can be accessed at:\nhttps://huggingface.co/FireFlyCourageous/MMCTR_DIN_MicroLens_1M_x1.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u9488\u5bf9\u5927\u6a21\u578b\u7684\u9ad8\u5ef6\u8fdf\u95ee\u9898\u63d0\u51fa\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5728\u7ade\u8d5b\u4e2d\u83b7\u5956\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u5e94\u7528\u65f6\u7684\u9ad8\u5ef6\u8fdf\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u91c7\u7528\u4e86\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u7ade\u8d5b\u8981\u6c42\u63d0\u4ea4\u4e86\u6280\u672f\u62a5\u544a\uff0c\u8be6\u7ec6\u63cf\u8ff0\u4e86\u65b9\u6cd5\u8bba\u3002", "result": "\u56e2\u961f\u5728Task 2 - Winner (Multimodal CTR Prediction)\u4e2d\u83b7\u5956\uff0c\u5e76\u516c\u5f00\u4e86\u4ee3\u7801\u548c\u6a21\u578b\u6743\u91cd\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u65b9\u6cd5\u53ca\u6210\u679c\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u91cd\u70b9\u662f\u63a8\u8350\u4fe1\u53f7\u4e0e\u591a\u6a21\u6001\u8868\u793a\u7684\u6709\u6548\u6574\u5408\u3002"}}
{"id": "2504.19981", "pdf": "https://arxiv.org/pdf/2504.19981", "abs": "https://arxiv.org/abs/2504.19981", "authors": ["Adam Younsi", "Abdalgader Abubaker", "Mohamed El Amine Seddik", "Hakim Hacid", "Salem Lahlou"], "title": "Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided GFlowNets", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Achieving both accuracy and diverse reasoning remains challenging for Large\nLanguage Models (LLMs) in complex domains like mathematics. A key bottleneck is\nevaluating intermediate reasoning steps to guide generation without costly\nhuman annotations. To address this, we first introduce a novel Process Reward\nModel (PRM) trained automatically using Monte Carlo Tree Search coupled with a\nsimilarity-based data augmentation technique, effectively capturing step-level\nreasoning quality. Leveraging this PRM, we then adapt Generative Flow Networks\n(GFlowNets) to operate at the reasoning step level. Unlike traditional\nreinforcement learning focused on maximizing a single reward, GFlowNets\nnaturally sample diverse, high-quality solutions proportional to their rewards,\nas measured by our PRM. Empirical evaluation shows strong improvements in both\naccuracy and solution diversity on challenging mathematical benchmarks (e.g.,\n+2.59% absolute accuracy on MATH Level 5 for Llama3.2-3B), with effective\ngeneralization to unseen datasets (+9.4% absolute on SAT MATH). Our work\ndemonstrates the potential of PRM-guided, step-level GFlowNets for developing\nmore robust and versatile mathematical reasoning in LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6d41\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u7ed3\u5408\u751f\u6210\u6d41\u7f51\u7edc\uff08GFlowNets\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u590d\u6742\u9886\u57df\uff08\u5982\u6570\u5b66\uff09\u4e2d\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u96be\u4ee5\u517c\u987e\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5982\u4f55\u5728\u4e0d\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u3002", "method": "\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\u8bad\u7ec3PRM\uff0c\u5e76\u4f7f\u7528GFlowNets\u5728\u63a8\u7406\u6b65\u9aa4\u5c42\u7ea7\u751f\u6210\u591a\u6837\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5728\u6570\u5b66\u8bc4\u6d4b\u57fa\u51c6\u4e0a\u5b9e\u73b0\u663e\u8457\u6539\u8fdb\uff08\u5982Llama3.2-3B\u5728MATH Level 5\u4e0a\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u53472.59%\uff09\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u65b0\u6570\u636e\u96c6\uff08SAT MATH\u4e0a\u63d0\u53479.4%\uff09\u3002", "conclusion": "PRM\u5f15\u5bfc\u7684GFlowNets\u5728\u63d0\u5347LLMs\u6570\u5b66\u63a8\u7406\u80fd\u529b\u7684\u7a33\u5065\u6027\u548c\u901a\u7528\u6027\u65b9\u9762\u5177\u6709\u6f5c\u5728\u4ef7\u503c\u3002"}}
{"id": "2504.19983", "pdf": "https://arxiv.org/pdf/2504.19983", "abs": "https://arxiv.org/abs/2504.19983", "authors": ["Yunwei Ren", "Eshaan Nichani", "Denny Wu", "Jason D. Lee"], "title": "Emergence and scaling laws in SGD learning of shallow neural networks", "categories": ["cs.LG", "stat.ML"], "comment": "100 pages", "summary": "We study the complexity of online stochastic gradient descent (SGD) for\nlearning a two-layer neural network with $P$ neurons on isotropic Gaussian\ndata: $f_*(\\boldsymbol{x}) = \\sum_{p=1}^P a_p\\cdot\n\\sigma(\\langle\\boldsymbol{x},\\boldsymbol{v}_p^*\\rangle)$, $\\boldsymbol{x} \\sim\n\\mathcal{N}(0,\\boldsymbol{I}_d)$, where the activation\n$\\sigma:\\mathbb{R}\\to\\mathbb{R}$ is an even function with information exponent\n$k_*>2$ (defined as the lowest degree in the Hermite expansion),\n$\\{\\boldsymbol{v}^*_p\\}_{p\\in[P]}\\subset \\mathbb{R}^d$ are orthonormal signal\ndirections, and the non-negative second-layer coefficients satisfy $\\sum_{p}\na_p^2=1$. We focus on the challenging ``extensive-width'' regime $P\\gg 1$ and\npermit diverging condition number in the second-layer, covering as a special\ncase the power-law scaling $a_p\\asymp p^{-\\beta}$ where\n$\\beta\\in\\mathbb{R}_{\\ge 0}$. We provide a precise analysis of SGD dynamics for\nthe training of a student two-layer network to minimize the mean squared error\n(MSE) objective, and explicitly identify sharp transition times to recover each\nsignal direction. In the power-law setting, we characterize scaling law\nexponents for the MSE loss with respect to the number of training samples and\nSGD steps, as well as the number of parameters in the student neural network.\nOur analysis entails that while the learning of individual teacher neurons\nexhibits abrupt transitions, the juxtaposition of $P\\gg 1$ emergent learning\ncurves at different timescales leads to a smooth scaling law in the cumulative\nobjective.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u7ebf\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\u5b66\u4e60\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc\u7684\u590d\u6742\u6027\uff0c\u7279\u522b\u662f\u5728\u201c\u6269\u5c55\u5bbd\u5ea6\u201d\u6761\u4ef6\u4e0b\uff0c\u5206\u6790\u4e86\u8bad\u7ec3\u52a8\u6001\u548c\u4fe1\u53f7\u65b9\u5411\u6062\u590d\u7684\u4e34\u754c\u65f6\u95f4\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u5728\u5927\u91cf\u795e\u7ecf\u5143\uff08P\u226b1\uff09\u548c\u53d1\u6563\u6761\u4ef6\u6570\u4e0b\uff0cSGD\u8bad\u7ec3\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc\u7684\u52a8\u6001\u884c\u4e3a\u53ca\u5176\u4f18\u5316\u6548\u679c\uff0c\u5c24\u5176\u662f\u5728\u529f\u7387\u5f8b\u7f29\u653e\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\u5bf9SGD\u52a8\u6001\u7684\u7cbe\u786e\u5206\u6790\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u76ee\u6807\uff0c\u660e\u786e\u8bc6\u522b\u4fe1\u53f7\u65b9\u5411\u6062\u590d\u7684\u4e34\u754c\u65f6\u95f4\uff0c\u5e76\u63a2\u8ba8\u529f\u7387\u5f8b\u8bbe\u5b9a\u4e0b\u7684\u6807\u5ea6\u5f8b\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5355\u4e2a\u6559\u5e08\u795e\u7ecf\u5143\u7684\u5b66\u4e60\u5448\u73b0\u51fa\u7a81\u7136\u7684\u8f6c\u53d8\uff0c\u4f46\u5728\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u5927\u91cf\u5b66\u4e60\u66f2\u7ebf\u7684\u53e0\u52a0\u5bfc\u81f4\u4e86\u7d2f\u79ef\u76ee\u6807\u7684\u5e73\u6ed1\u6807\u5ea6\u5f8b\u3002", "conclusion": "\u8bba\u6587\u7684\u7ed3\u8bba\u5f3a\u8c03\u4e86\u5728\u5927\u89c4\u6a21\u795e\u7ecf\u5143\u7f51\u7edc\u4e2d\uff0cSGD\u7684\u52a8\u6001\u884c\u4e3a\u5bfc\u81f4\u4e86\u590d\u6742\u4f46\u53ef\u9884\u6d4b\u7684\u4f18\u5316\u6548\u679c\uff0c\u4e3a\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2504.20019", "pdf": "https://arxiv.org/pdf/2504.20019", "abs": "https://arxiv.org/abs/2504.20019", "authors": ["Abdelhakim Amer", "David Felsager", "Yury Brodskiy", "Andriy Sarabakha"], "title": "Modelling of Underwater Vehicles using Physics-Informed Neural Networks with Control", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "This paper has been accepted for presentation at the International\n  Joint Conference on Neural Networks (IJCNN) 2025. The final version consists\n  of 8 pages", "summary": "Physics-informed neural networks (PINNs) integrate physical laws with\ndata-driven models to improve generalization and sample efficiency. This work\nintroduces an open-source implementation of the Physics-Informed Neural Network\nwith Control (PINC) framework, designed to model the dynamics of an underwater\nvehicle. Using initial states, control actions, and time inputs, PINC extends\nPINNs to enable physically consistent transitions beyond the training domain.\nVarious PINC configurations are tested, including differing loss functions,\ngradient-weighting schemes, and hyperparameters. Validation on a simulated\nunderwater vehicle demonstrates more accurate long-horizon predictions compared\nto a non-physics-informed baseline", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86PINC\u6846\u67b6\uff0c\u5b83\u662fPINNs\u7684\u4e00\u4e2a\u5f00\u6e90\u5b9e\u73b0\uff0c\u4e13\u4e3a\u6c34\u4e0b\u8f66\u8f86\u52a8\u529b\u5b66\u5efa\u6a21\u8bbe\u8ba1\u3002\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u5b9a\u5f8b\u548c\u521d\u59cb\u72b6\u6001\u3001\u63a7\u5236\u52a8\u4f5c\u53ca\u65f6\u95f4\u8f93\u5165\uff0cPINC\u6269\u5c55\u4e86PINNs\u7684\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u8d85\u51fa\u8bad\u7ec3\u57df\u7684\u7269\u7406\u4e00\u81f4\u6027\u9884\u6d4b\u3002", "motivation": "\u63d0\u9ad8\u6570\u636e\u9a71\u52a8\u6a21\u578b\u5728\u5efa\u6a21\u6c34\u4e0b\u8f66\u8f86\u52a8\u529b\u5b66\u65f6\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6837\u672c\u6548\u7387\uff0c\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u5b9a\u5f8b\u786e\u4fdd\u9884\u6d4b\u7684\u7269\u7406\u4e00\u81f4\u6027\u3002", "method": "PINC\u6846\u67b6\u6269\u5c55\u4e86PINNs\uff0c\u5229\u7528\u521d\u59cb\u72b6\u6001\u3001\u63a7\u5236\u52a8\u4f5c\u548c\u65f6\u95f4\u8f93\u5165\uff0c\u6d4b\u8bd5\u4e86\u4e0d\u540c\u7684\u635f\u5931\u51fd\u6570\u3001\u68af\u5ea6\u52a0\u6743\u65b9\u6848\u548c\u8d85\u53c2\u6570\u914d\u7f6e\u3002", "result": "\u5728\u6a21\u62df\u6c34\u4e0b\u8f66\u8f86\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0c\u76f8\u8f83\u4e8e\u975e\u7269\u7406\u4fe1\u606f\u7684\u57fa\u7ebf\u6a21\u578b\uff0cPINC\u80fd\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u957f\u65f6\u9884\u6d4b\u3002", "conclusion": "PINC\u6846\u67b6\u901a\u8fc7\u6574\u5408\u7269\u7406\u5b9a\u5f8b\u548c\u6570\u636e\u9a71\u52a8\u6a21\u578b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6c34\u4e0b\u8f66\u8f86\u52a8\u529b\u5b66\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\u548c\u6cdb\u5316\u6027\u3002"}}
{"id": "2504.20020", "pdf": "https://arxiv.org/pdf/2504.20020", "abs": "https://arxiv.org/abs/2504.20020", "authors": ["Xin Wang", "Haoyang Li", "Zeyang Zhang", "Haibo Chen", "Wenwu Zhu"], "title": "Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "11 pages, 3 figures", "summary": "Large language models (LLMs) have dramatically advanced machine learning\nresearch including natural language processing, computer vision, data mining,\netc., yet they still exhibit critical limitations in reasoning, factual\nconsistency, and interpretability. In this paper, we introduce a novel learning\nparadigm -- Modular Machine Learning (MML) -- as an essential approach toward\nnew-generation LLMs. MML decomposes the complex structure of LLMs into three\ninterdependent components: modular representation, modular model, and modular\nreasoning, aiming to enhance LLMs' capability of counterfactual reasoning,\nmitigating hallucinations, as well as promoting fairness, safety, and\ntransparency. Specifically, the proposed MML paradigm can: i) clarify the\ninternal working mechanism of LLMs through the disentanglement of semantic\ncomponents; ii) allow for flexible and task-adaptive model design; iii) enable\ninterpretable and logic-driven decision-making process. We present a feasible\nimplementation of MML-based LLMs via leveraging advanced techniques such as\ndisentangled representation learning, neural architecture search and\nneuro-symbolic learning. We critically identify key challenges, such as the\nintegration of continuous neural and discrete symbolic processes, joint\noptimization, and computational scalability, present promising future research\ndirections that deserve further exploration. Ultimately, the integration of the\nMML paradigm with LLMs has the potential to bridge the gap between statistical\n(deep) learning and formal (logical) reasoning, thereby paving the way for\nrobust, adaptable, and trustworthy AI systems across a wide range of real-world\napplications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5b66\u4e60\u8303\u5f0f\u2014\u2014\u6a21\u5757\u5316\u673a\u5668\u5b66\u4e60\uff08MML\uff09\uff0c\u65e8\u5728\u901a\u8fc7\u5206\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u590d\u6742\u7ed3\u6784\uff0c\u63d0\u5347\u5176\u53cd\u4e8b\u5b9e\u63a8\u7406\u80fd\u529b\u3001\u51cf\u5c11\u5e7b\u89c9\uff0c\u5e76\u589e\u5f3a\u516c\u5e73\u6027\u3001\u5b89\u5168\u6027\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u3001\u4e8b\u5b9e\u4e00\u81f4\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "MML\u5c06LLMs\u5206\u89e3\u4e3a\u4e09\u4e2a\u76f8\u4e92\u4f9d\u8d56\u7684\u7ec4\u4ef6\uff1a\u6a21\u5757\u5316\u8868\u793a\u3001\u6a21\u5757\u5316\u6a21\u578b\u548c\u6a21\u5757\u5316\u63a8\u7406\uff0c\u5e76\u7ed3\u5408\u89e3\u8026\u8868\u793a\u5b66\u4e60\u3001\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u548c\u795e\u7ecf\u7b26\u53f7\u5b66\u4e60\u7b49\u6280\u672f\u5b9e\u73b0\u3002", "result": "MML\u8303\u5f0f\u80fd\u660e\u786eLLMs\u7684\u5185\u90e8\u5de5\u4f5c\u673a\u5236\uff0c\u652f\u6301\u7075\u6d3b\u7684\u4efb\u52a1\u81ea\u9002\u5e94\u6a21\u578b\u8bbe\u8ba1\uff0c\u5e76\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u903b\u8f91\u9a71\u52a8\u51b3\u7b56\u8fc7\u7a0b\u3002", "conclusion": "MML\u4e0eLLMs\u7684\u7ed3\u5408\u6709\u671b\u5f25\u5408\u7edf\u8ba1\u5b66\u4e60\u4e0e\u5f62\u5f0f\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63a8\u52a8\u6784\u5efa\u7a33\u5065\u3001\u9002\u5e94\u6027\u5f3a\u4e14\u53ef\u4fe1\u7684AI\u7cfb\u7edf\u3002"}}
{"id": "2504.19030", "pdf": "https://arxiv.org/pdf/2504.19030", "abs": "https://arxiv.org/abs/2504.19030", "authors": ["Sidahmed Lachenani", "Hamza Kheddar", "Mohamed Ouldzmirli"], "title": "Improving Pretrained YAMNet for Enhanced Speech Command Detection via Transfer Learning", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "This work addresses the need for enhanced accuracy and efficiency in speech\ncommand recognition systems, a critical component for improving user\ninteraction in various smart applications. Leveraging the robust pretrained\nYAMNet model and transfer learning, this study develops a method that\nsignificantly improves speech command recognition. We adapt and train a YAMNet\ndeep learning model to effectively detect and interpret speech commands from\naudio signals. Using the extensively annotated Speech Commands dataset\n(speech_commands_v0.01), our approach demonstrates the practical application of\ntransfer learning to accurately recognize a predefined set of speech commands.\nThe dataset is meticulously augmented, and features are strategically extracted\nto boost model performance. As a result, the final model achieved a recognition\naccuracy of 95.28%, underscoring the impact of advanced machine learning\ntechniques on speech command recognition. This achievement marks substantial\nprogress in audio processing technologies and establishes a new benchmark for\nfuture research in the field.", "AI": {"tldr": "\u5229\u7528\u9884\u8bad\u7ec3\u7684YAMNet\u6a21\u578b\u548c\u8fc1\u79fb\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u8bed\u97f3\u547d\u4ee4\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u6700\u7ec8\u6a21\u578b\u8bc6\u522b\u51c6\u786e\u7387\u8fbe95.28%\u3002", "motivation": "\u63d0\u5347\u8bed\u97f3\u547d\u4ee4\u8bc6\u522b\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4ee5\u6539\u5584\u667a\u80fd\u5e94\u7528\u4e2d\u7684\u7528\u6237\u4ea4\u4e92\u4f53\u9a8c\u3002", "method": "\u91c7\u7528YAMNet\u6df1\u5ea6\u6a21\u578b\u548c\u8fc1\u79fb\u5b66\u4e60\u6280\u672f\uff0c\u5bf9\u8bed\u97f3\u547d\u4ee4\u6570\u636e\u96c6\u8fdb\u884c\u7cbe\u7ec6\u5316\u589e\u5f3a\u548c\u7279\u5f81\u63d0\u53d6\u3002", "result": "\u6a21\u578b\u5728\u8bed\u97f3\u547d\u4ee4\u8bc6\u522b\u4efb\u52a1\u4e2d\u8fbe\u523095.28%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5148\u8fdb\u673a\u5668\u5b66\u4e60\u6280\u672f\u5728\u8bed\u97f3\u547d\u4ee4\u8bc6\u522b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u97f3\u9891\u5904\u7406\u6280\u672f\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2504.18539", "pdf": "https://arxiv.org/pdf/2504.18539", "abs": "https://arxiv.org/abs/2504.18539", "authors": ["Sungnyun Kim", "Sungwoo Cho", "Sangmin Bae", "Kangwook Jang", "Se-Young Yun"], "title": "Multi-Task Corrupted Prediction for Learning Robust Audio-Visual Speech Representation", "categories": ["eess.AS", "cs.LG", "cs.MM", "cs.SD"], "comment": "22 pages, 6 figures, 14 tables", "summary": "Audio-visual speech recognition (AVSR) incorporates auditory and visual\nmodalities to improve recognition accuracy, particularly in noisy environments\nwhere audio-only speech systems are insufficient. While previous research has\nlargely addressed audio disruptions, few studies have dealt with visual\ncorruptions, e.g., lip occlusions or blurred videos, which are also\ndetrimental. To address this real-world challenge, we propose CAV2vec, a novel\nself-supervised speech representation learning framework particularly designed\nto handle audio-visual joint corruption. CAV2vec employs a self-distillation\napproach with a corrupted prediction task, where the student model learns to\npredict clean targets, generated by the teacher model, with corrupted input\nframes. Specifically, we suggest a unimodal multi-task learning, which distills\ncross-modal knowledge and aligns the corrupted modalities, by predicting clean\naudio targets with corrupted videos, and clean video targets with corrupted\naudios. This strategy mitigates the dispersion in the representation space\ncaused by corrupted modalities, leading to more reliable and robust\naudio-visual fusion. Our experiments on robust AVSR benchmarks demonstrate that\nthe corrupted representation learning method significantly enhances recognition\naccuracy across generalized environments involving various types of corruption.", "AI": {"tldr": "CAV2vec\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u8bed\u97f3\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u5904\u7406\u97f3\u9891-\u89c6\u89c9\u8054\u5408\u635f\u574f\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u84b8\u998f\u65b9\u6cd5\u63d0\u5347\u5728\u591a\u635f\u574f\u73af\u5883\u4e0b\u7684\u8bed\u97f3\u8bc6\u522b\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u5608\u6742\u73af\u5883\u4e2d\uff0c\u97f3\u9891-\u89c6\u89c9\u8bed\u97f3\u8bc6\u522b\uff08AVSR\uff09\u56e0\u89c6\u89c9\u635f\u574f\uff08\u5982\u5507\u90e8\u906e\u6321\u6216\u89c6\u9891\u6a21\u7cca\uff09\u96be\u4ee5\u5904\u7406\uff0c\u5bfc\u81f4\u8bc6\u522b\u6027\u80fd\u4e0b\u964d\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u73b0\u5b9e\u6311\u6218\u3002", "method": "\u63d0\u51faCAV2vec\uff0c\u901a\u8fc7\u81ea\u84b8\u998f\u65b9\u6cd5\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u9884\u6d4b\u6559\u5e08\u6a21\u578b\u751f\u6210\u7684\u5e72\u51c0\u76ee\u6807\uff0c\u540c\u65f6\u91c7\u7528\u5355\u6a21\u6001\u591a\u4efb\u52a1\u5b66\u4e60\u5bf9\u9f50\u635f\u574f\u6a21\u6001\u3002", "result": "\u5728\u591a\u79cd\u635f\u574f\u7c7b\u578b\u7684AVSR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8bc6\u522b\u51c6\u786e\u6027\u3002", "conclusion": "CAV2vec\u901a\u8fc7\u81ea\u84b8\u998f\u548c\u8de8\u6a21\u6001\u77e5\u8bc6\u4f20\u9012\uff0c\u6709\u6548\u589e\u5f3a\u4e86AVSR\u5728\u590d\u6742\u635f\u574f\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.19032", "pdf": "https://arxiv.org/pdf/2504.19032", "abs": "https://arxiv.org/abs/2504.19032", "authors": ["Niaz Ahmad", "Youngmoon Lee", "Guanghui Wang"], "title": "VISUALCENT: Visual Human Analysis using Dynamic Centroid Representation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce VISUALCENT, a unified human pose and instance segmentation\nframework to address generalizability and scalability limitations to multi\nperson visual human analysis. VISUALCENT leverages centroid based bottom up\nkeypoint detection paradigm and uses Keypoint Heatmap incorporating Disk\nRepresentation and KeyCentroid to identify the optimal keypoint coordinates.\nFor the unified segmentation task, an explicit keypoint is defined as a dynamic\ncentroid called MaskCentroid to swiftly cluster pixels to specific human\ninstance during rapid changes in human body movement or significantly occluded\nenvironment. Experimental results on COCO and OCHuman datasets demonstrate\nVISUALCENTs accuracy and real time performance advantages, outperforming\nexisting methods in mAP scores and execution frame rate per second. The\nimplementation is available on the project page.", "AI": {"tldr": "VISUALCENT\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u59ff\u6001\u548c\u5b9e\u4f8b\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u8d28\u5fc3\u7684\u81ea\u5e95\u5411\u4e0a\u5173\u952e\u70b9\u68c0\u6d4b\u8303\u5f0f\uff0c\u7ed3\u5408Disk Representation\u548cKeyCentroid\uff0c\u63d0\u5347\u4e86\u591a\u4eba\u89c6\u89c9\u5206\u6790\u7684\u6cdb\u5316\u6027\u548c\u6269\u5c55\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u4eba\u89c6\u89c9\u5206\u6790\u4e2d\u6cdb\u5316\u6027\u548c\u6269\u5c55\u6027\u7684\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u8d28\u5fc3\u7684\u81ea\u5e95\u5411\u4e0a\u5173\u952e\u70b9\u68c0\u6d4b\uff0c\u5305\u62ecKeypoint Heatmap\u3001Disk Representation\u548cKeyCentroid\uff1b\u540c\u65f6\u5f15\u5165\u52a8\u6001\u8d28\u5fc3MaskCentroid\u5feb\u901f\u805a\u7c7b\u50cf\u7d20\u5230\u7279\u5b9a\u4eba\u5b9e\u4f8b\u3002", "result": "\u5728COCO\u548cOCHuman\u6570\u636e\u96c6\u4e0a\uff0cVISUALCENT\u5728mAP\u5206\u6570\u548c\u6267\u884c\u5e27\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "VISUALCENT\u5c55\u793a\u4e86\u5728\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u80fd\u4e0a\u7684\u4f18\u52bf\uff0c\u4e3a\u591a\u4eba\u89c6\u89c9\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18545", "pdf": "https://arxiv.org/pdf/2504.18545", "abs": "https://arxiv.org/abs/2504.18545", "authors": ["Geethu Joy", "Christian Huyck", "Xin-She Yang"], "title": "Parameter Tuning of the Firefly Algorithm by Three Tuning Methods: Standard Monte Carlo, Quasi-Monte Carlo and Latin Hypercube Sampling Methods", "categories": ["stat.CO", "cs.LG", "cs.NE", "68T05, 90C31"], "comment": "21 pages", "summary": "There are many different nature-inspired algorithms in the literature, and\nalmost all such algorithms have algorithm-dependent parameters that need to be\ntuned. The proper setting and parameter tuning should be carried out to\nmaximize the performance of the algorithm under consideration. This work is the\nextension of the recent work on parameter tuning by Joy et al. (2024) presented\nat the International Conference on Computational Science (ICCS 2024), and the\nFirefly Algorithm (FA) is tuned using three different methods: the Monte Carlo\nmethod, the Quasi-Monte Carlo method and the Latin Hypercube Sampling. The FA\nwith the tuned parameters is then used to solve a set of six different\noptimization problems, and the possible effect of parameter setting on the\nquality of the optimal solutions is analyzed. Rigorous statistical hypothesis\ntests have been carried out, including Student's t-tests, F-tests,\nnon-parametric Friedman tests and ANOVA. Results show that the performance of\nthe FA is not influenced by the tuning methods used. In addition, the tuned\nparameter values are largely independent of the tuning methods used. This\nindicates that the FA can be flexible and equally effective in solving\noptimization problems, and any of the three tuning methods can be used to tune\nits parameters effectively.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8424\u706b\u866b\u7b97\u6cd5\uff08FA\uff09\u7684\u53c2\u6570\u8c03\u4f18\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u79cd\u4e0d\u540c\u65b9\u6cd5\uff08\u8499\u7279\u5361\u6d1b\u3001\u62df\u8499\u7279\u5361\u6d1b\u548c\u62c9\u4e01\u8d85\u7acb\u65b9\u91c7\u6837\uff09\u8fdb\u884c\u8c03\u4f18\uff0c\u5e76\u8fdb\u884c\u4f18\u5316\u95ee\u9898\u6c42\u89e3\u548c\u7edf\u8ba1\u5206\u6790\u3002\u7ed3\u679c\u663e\u793a\uff0cFA\u7684\u6027\u80fd\u4e0d\u53d7\u8c03\u4f18\u65b9\u6cd5\u5f71\u54cd\uff0c\u8868\u660e\u5176\u7075\u6d3b\u6027\u548c\u901a\u7528\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e3a\u4e86\u63a2\u7d22\u4e0d\u540c\u53c2\u6570\u8c03\u4f18\u65b9\u6cd5\u5bf9\u8424\u706b\u866b\u7b97\u6cd5\uff08FA\uff09\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ee5\u786e\u5b9a\u5176\u5728\u4e0d\u540c\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u9002\u7528\u6027\u548c\u7075\u6d3b\u6027\u3002", "method": "\u8bba\u6587\u91c7\u7528\u4e09\u79cd\u53c2\u6570\u8c03\u4f18\u65b9\u6cd5\uff08\u8499\u7279\u5361\u6d1b\u3001\u62df\u8499\u7279\u5361\u6d1b\u548c\u62c9\u4e01\u8d85\u7acb\u65b9\u91c7\u6837\uff09\u5bf9FA\u8fdb\u884c\u8c03\u4f18\uff0c\u5e76\u4f7f\u7528\u8c03\u4f18\u540e\u7684FA\u89e3\u51b3\u516d\u4e2a\u4f18\u5316\u95ee\u9898\uff0c\u518d\u901a\u8fc7\u591a\u79cd\u7edf\u8ba1\u5047\u8bbe\u68c0\u9a8c\uff08\u5982t\u68c0\u9a8c\u3001F\u68c0\u9a8c\u3001Friedman\u68c0\u9a8c\u548cANOVA\uff09\u5206\u6790\u8c03\u4f18\u65b9\u6cd5\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cFA\u7684\u6027\u80fd\u4e0d\u53d7\u8c03\u4f18\u65b9\u6cd5\u5f71\u54cd\uff0c\u4e14\u53c2\u6570\u8c03\u4f18\u7ed3\u679c\u4e0e\u8c03\u4f18\u65b9\u6cd5\u65e0\u5173\uff0c\u8bf4\u660eFA\u5728\u4e0d\u540c\u8c03\u4f18\u65b9\u6cd5\u4e0b\u90fd\u80fd\u6709\u6548\u89e3\u51b3\u95ee\u9898\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u8424\u706b\u866b\u7b97\u6cd5\uff08FA\uff09\u5177\u6709\u7075\u6d3b\u6027\u548c\u901a\u7528\u6027\uff0c\u4e09\u79cd\u8c03\u4f18\u65b9\u6cd5\u5747\u53ef\u7528\u4e8e\u5176\u53c2\u6570\u8c03\u4f18\uff0c\u4e14\u6548\u679c\u76f8\u5f53\u3002"}}
{"id": "2504.19042", "pdf": "https://arxiv.org/pdf/2504.19042", "abs": "https://arxiv.org/abs/2504.19042", "authors": ["James Giroux", "Michael Martinez", "Cristiano Fanelli"], "title": "Generative Models for Fast Simulation of Cherenkov Detectors at the Electron-Ion Collider", "categories": ["physics.ins-det", "cs.AI", "cs.LG", "hep-ex", "nucl-ex"], "comment": "45 pages, 27 figures", "summary": "The integration of Deep Learning (DL) into experimental nuclear and particle\nphysics has driven significant progress in simulation and reconstruction\nworkflows. However, traditional simulation frameworks such as Geant4 remain\ncomputationally intensive, especially for Cherenkov detectors, where simulating\noptical photon transport through complex geometries and reflective surfaces\nintroduces a major bottleneck. To address this, we present an open, standalone\nfast simulation tool for Detection of Internally Reflected Cherenkov Light\n(DIRC) detectors, with a focus on the High-Performance DIRC (hpDIRC) at the\nfuture Electron-Ion Collider (EIC). Our framework incorporates a suite of\ngenerative models tailored to accelerate particle identification (PID) tasks by\noffering a scalable, GPU-accelerated alternative to full Geant4-based\nsimulations. Designed with accessibility in mind, our simulation package\nenables both DL researchers and physicists to efficiently generate\nhigh-fidelity large-scale datasets on demand, without relying on complex\ntraditional simulation stacks. This flexibility supports the development and\nbenchmarking of novel DL-driven PID methods. Moreover, this fast simulation\npipeline represents a critical step toward enabling EIC-wide PID strategies\nthat depend on virtually unlimited simulated samples, spanning the full\nacceptance of the hpDIRC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9DIRC\u63a2\u6d4b\u5668\u7684\u5feb\u901f\u4eff\u771f\u5de5\u5177\uff0c\u4f7f\u7528\u751f\u6210\u6a21\u578b\u52a0\u901f\u7c92\u5b50\u8bc6\u522b\u4efb\u52a1\uff0c\u4ee3\u66ff\u4f20\u7edf\u7684Geant4\u4eff\u771f\u6846\u67b6\u3002", "motivation": "\u4f20\u7edf\u4eff\u771f\u6846\u67b6\uff08\u5982Geant4\uff09\u5728\u6a21\u62dfCherenkov\u63a2\u6d4b\u5668\u65f6\u5149\u5b50\u4f20\u8f93\u8ba1\u7b97\u91cf\u5927\uff0c\u6210\u4e3a\u6027\u80fd\u74f6\u9888\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u9700\u5f00\u53d1\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u72ec\u7acb\u7684\u5feb\u901f\u4eff\u771f\u5de5\u5177\uff0c\u7ed3\u5408\u751f\u6210\u6a21\u578b\u548cGPU\u52a0\u901f\uff0c\u4e3ahpDIRC\u63a2\u6d4b\u5668\u751f\u6210\u9ad8\u4fdd\u771f\u6570\u636e\u96c6\u3002", "result": "\u8be5\u5de5\u5177\u80fd\u591f\u9ad8\u6548\u751f\u6210\u5927\u89c4\u6a21\u4eff\u771f\u6570\u636e\uff0c\u652f\u6301\u5f00\u53d1\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u9a71\u52a8\u7684\u7c92\u5b50\u8bc6\u522b\u65b9\u6cd5\uff0c\u5e76\u63a8\u52a8EIC\u8303\u56f4\u5185\u7684PID\u7b56\u7565\u3002", "conclusion": "\u8be5\u5feb\u901f\u4eff\u771f\u5de5\u5177\u4e3a\u7269\u7406\u5b66\u5bb6\u548cDL\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u7075\u6d3b\u9ad8\u6548\u7684\u6570\u636e\u751f\u6210\u65b9\u6848\uff0c\u51cf\u5c11\u4e86\u5bf9\u4f20\u7edf\u4eff\u771f\u6846\u67b6\u7684\u4f9d\u8d56\u3002"}}
{"id": "2504.19046", "pdf": "https://arxiv.org/pdf/2504.19046", "abs": "https://arxiv.org/abs/2504.19046", "authors": ["Billel Essaid", "Hamza Kheddar", "Noureddine Batel"], "title": "Enhancing Cochlear Implant Signal Coding with Scaled Dot-Product Attention", "categories": ["eess.AS", "cs.AI", "cs.SD"], "comment": null, "summary": "Cochlear implants (CIs) play a vital role in restoring hearing for\nindividuals with severe to profound sensorineural hearing loss by directly\nstimulating the auditory nerve with electrical signals. While traditional\ncoding strategies, such as the advanced combination encoder (ACE), have proven\neffective, they are constrained by their adaptability and precision. This paper\ninvestigates the use of deep learning (DL) techniques to generate\nelectrodograms for CIs, presenting our model as an advanced alternative. We\ncompared the performance of our model with the ACE strategy by evaluating the\nintelligibility of reconstructed audio signals using the short-time objective\nintelligibility (STOI) metric. The results indicate that our model achieves a\nSTOI score of 0.6031, closely approximating the 0.6126 score of the ACE\nstrategy, and offers potential advantages in flexibility and adaptability. This\nstudy underscores the benefits of incorporating artificial intelligent (AI)\ninto CI technology, such as enhanced personalization and efficiency.", "AI": {"tldr": "TL;DR: \u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u4e3a\u4eba\u5de5\u8033\u8717\u751f\u6210\u7535\u7801\u56fe, \u5176\u6027\u80fd\u63a5\u8fd1\u4f20\u7edfACE\u65b9\u6cd5, \u5e76\u5c55\u793a\u4e86\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u6f5c\u529b\u3002", "motivation": "\u63a2\u8ba8\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5728\u4eba\u5de5\u8033\u8717\u7f16\u7801\u4e2d\u7684\u5e94\u7528, \u5f25\u8865\u4f20\u7edf\u65b9\u6cd5\u5728\u9002\u5e94\u6027\u548c\u7cbe\u786e\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u6bd4\u8f83\u6df1\u5ea6\u5b66\u4e60\u751f\u6210\u7684\u7535\u7801\u56fe\u4e0eACE\u7b56\u7565, \u4f7f\u7528STOI\u8bc4\u5206\u8bc4\u4f30\u97f3\u9891\u4fe1\u53f7\u7684\u53ef\u61c2\u5ea6\u3002", "result": "\u6a21\u578bSTOI\u5f97\u5206\u4e3a0.6031, \u63a5\u8fd1ACE\u5f97\u52060.6126, \u4e14\u5728\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u4e0a\u66f4\u5177\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9eAI\u6280\u672f\u80fd\u4e3a\u4eba\u5de5\u8033\u8717\u5e26\u6765\u66f4\u4e2a\u6027\u5316\u548c\u9ad8\u6548\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2504.18570", "pdf": "https://arxiv.org/pdf/2504.18570", "abs": "https://arxiv.org/abs/2504.18570", "authors": ["Sabrina Bruckmeier", "Huadong Mo", "James Qin"], "title": "Residual-Evasive Attacks on ADMM in Distributed Optimization", "categories": ["cs.CR", "cs.DC", "cs.LG", "math.OC"], "comment": "10 pages, 12 figures, 2 tables", "summary": "This paper presents two attack strategies designed to evade detection in\nADMM-based systems by preventing significant changes to the residual during the\nattacked iteration. While many detection algorithms focus on identifying false\ndata injection through residual changes, we show that our attacks remain\nundetected by keeping the residual largely unchanged. The first strategy uses a\nrandom starting point combined with Gram-Schmidt orthogonalization to ensure\nstealth, with potential for refinement by enhancing the orthogonal component to\nincrease system disruption. The second strategy builds on the first, targeting\nfinancial gains by manipulating reactive power and pushing the system to its\nupper voltage limit, exploiting operational constraints. The effectiveness of\nthe proposed attack-resilient mechanism is demonstrated through case studies on\nthe IEEE 14-bus system. A comparison of the two strategies, along with commonly\nused naive attacks, reveals trade-offs between simplicity, detectability, and\neffectiveness, providing insights into ADMM system vulnerabilities. These\nfindings underscore the need for more robust monitoring algorithms to protect\nagainst advanced attack strategies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u9488\u5bf9ADMM\u7cfb\u7edf\u7684\u653b\u51fb\u7b56\u7565\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u6b8b\u5dee\u53d8\u5316\u4ee5\u89c4\u907f\u68c0\u6d4b\uff0c\u5e76\u901a\u8fc7IEEE 14-bus\u7cfb\u7edf\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u8bb8\u591a\u68c0\u6d4b\u7b97\u6cd5\u4f9d\u8d56\u6b8b\u5dee\u53d8\u5316\u8bc6\u522b\u865a\u5047\u6570\u636e\u6ce8\u5165\u653b\u51fb\uff0c\u4f46\u4f5c\u8005\u53d1\u73b0\u53ef\u901a\u8fc7\u4fdd\u6301\u6b8b\u5dee\u4e0d\u53d8\u7684\u65b9\u5f0f\u7ed5\u8fc7\u68c0\u6d4b\uff0c\u56e0\u6b64\u7814\u7a76\u66f4\u9690\u853d\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u7b2c\u4e00\u79cd\u7b56\u7565\u4f7f\u7528\u968f\u673a\u8d77\u70b9\u4e0eGram-Schmidt\u6b63\u4ea4\u5316\u786e\u4fdd\u9690\u853d\u6027\uff0c\u5e76\u53ef\u4f18\u5316\u6b63\u4ea4\u5206\u91cf\u4ee5\u589e\u5f3a\u7834\u574f\u6027\uff1b\u7b2c\u4e8c\u79cd\u7b56\u7565\u5728\u6b64\u57fa\u7840\u4e0a\u901a\u8fc7\u64cd\u7eb5\u65e0\u529f\u529f\u7387\u5c06\u7cfb\u7edf\u63a8\u5411\u7535\u538b\u4e0a\u9650\u4ee5\u83b7\u53d6\u7ecf\u6d4e\u5229\u76ca\u3002", "result": "\u5728IEEE 14-bus\u7cfb\u7edf\u4e2d\u7684\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u4e24\u79cd\u7b56\u7565\u5728\u9690\u853d\u6027\u548c\u7834\u574f\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u653b\u51fb\uff0c\u63ed\u793a\u4e86ADMM\u7cfb\u7edf\u7684\u8106\u5f31\u6027\u3002", "conclusion": "\u7814\u7a76\u51f8\u663e\u4e86\u73b0\u6709\u76d1\u63a7\u7b97\u6cd5\u7684\u4e0d\u8db3\uff0c\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u9632\u5fa1\u673a\u5236\u5e94\u5bf9\u9ad8\u7ea7\u653b\u51fb\u3002"}}
{"id": "2504.19047", "pdf": "https://arxiv.org/pdf/2504.19047", "abs": "https://arxiv.org/abs/2504.19047", "authors": ["David Almog"], "title": "AI Recommendations and Non-instrumental Image Concerns", "categories": ["econ.GN", "cs.AI", "cs.HC", "q-fin.EC"], "comment": null, "summary": "There is growing enthusiasm about the potential for humans and AI to\ncollaborate by leveraging their respective strengths. Yet in practice, this\npromise often falls short. This paper uses an online experiment to identify\nnon-instrumental image concerns as a key reason individuals underutilize AI\nrecommendations. I show that concerns about how one is perceived, even when\nthose perceptions carry no monetary consequences, lead participants to\ndisregard AI advice and reduce task performance.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7fAI\u5efa\u8bae\u65e0\u5b9e\u9645\u8d1f\u9762\u5f71\u54cd\uff0c\u4eba\u4eec\u5bf9\u81ea\u6211\u5f62\u8c61\u7684\u62c5\u5fe7\u4ecd\u5bfc\u81f4\u5176\u5ffd\u89c6AI\u5efa\u8bae\u5e76\u964d\u4f4e\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22\u4eba\u7c7b\u4e0eAI\u5408\u4f5c\u7684\u5b9e\u9645\u969c\u788d\uff0c\u91cd\u70b9\u5173\u6ce8\u975e\u5de5\u5177\u6027\u56e0\u7d20\uff08\u5982\u81ea\u6211\u5f62\u8c61\uff09\u5982\u4f55\u5f71\u54cdAI\u5efa\u8bae\u7684\u91c7\u7eb3\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u5b9e\u9a8c\u5206\u6790\u53c2\u4e0e\u8005\u5728\u65e0\u7ecf\u6d4e\u540e\u679c\u60c5\u51b5\u4e0b\u5bf9AI\u5efa\u8bae\u7684\u6001\u5ea6\u53ca\u884c\u4e3a\u53cd\u5e94\u3002", "result": "\u53c2\u4e0e\u8005\u56e0\u62c5\u5fc3\u4ed6\u4eba\u8bc4\u4ef7\uff08\u975e\u7ecf\u6d4e\u56e0\u7d20\uff09\u800c\u62d2\u7eddAI\u5efa\u8bae\uff0c\u5bfc\u81f4\u4efb\u52a1\u8868\u73b0\u4e0b\u964d\u3002", "conclusion": "\u975e\u5de5\u5177\u6027\u5fc3\u7406\u56e0\u7d20\u662f\u963b\u788d\u4eba\u673a\u534f\u4f5c\u7684\u91cd\u8981\u969c\u788d\uff0c\u672a\u6765\u9700\u8bbe\u8ba1\u66f4\u5173\u6ce8\u7528\u6237\u5fc3\u7406\u7684AI\u7cfb\u7edf\u3002"}}
{"id": "2504.18571", "pdf": "https://arxiv.org/pdf/2504.18571", "abs": "https://arxiv.org/abs/2504.18571", "authors": ["Fabio Palmese", "Anna Maria Mandalari", "Hamed Haddadi", "Alessandro Enrico Cesare Redondi"], "title": "Intelligent Detection of Non-Essential IoT Traffic on the Home Gateway", "categories": ["cs.CR", "cs.LG"], "comment": "Paper accepted for publication at 10th International Workshop on\n  Traffic Measurements for Cybersecurity (WTMC 2025)", "summary": "The rapid expansion of Internet of Things (IoT) devices, particularly in\nsmart home environments, has introduced considerable security and privacy\nconcerns due to their persistent connectivity and interaction with cloud\nservices. Despite advancements in IoT security, effective privacy measures\nremain uncovered, with existing solutions often relying on cloud-based threat\ndetection that exposes sensitive data or outdated allow-lists that inadequately\nrestrict non-essential network traffic. This work presents ML-IoTrim, a system\nfor detecting and mitigating non-essential IoT traffic (i.e., not influencing\nthe device operations) by analyzing network behavior at the edge, leveraging\nMachine Learning to classify network destinations. Our approach includes\nbuilding a labeled dataset based on IoT device behavior and employing a\nfeature-extraction pipeline to enable a binary classification of essential vs.\nnon-essential network destinations. We test our framework in a consumer smart\nhome setup with IoT devices from five categories, demonstrating that the model\ncan accurately identify and block non-essential traffic, including previously\nunseen destinations, without relying on traditional allow-lists. We implement\nour solution on a home access point, showing the framework has strong potential\nfor scalable deployment, supporting near-real-time traffic classification in\nlarge-scale IoT environments with hundreds of devices. This research advances\nprivacy-aware traffic control in smart homes, paving the way for future\ndevelopments in IoT device privacy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86ML-IoTrim\u7cfb\u7edf\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u5728\u8fb9\u7f18\u5206\u6790\u7f51\u7edc\u884c\u4e3a\u4ee5\u68c0\u6d4b\u548c\u963b\u6b62\u667a\u80fd\u5bb6\u5c45\u4e2d\u975e\u5fc5\u8981\u7684IoT\u6d41\u91cf\uff0c\u65e0\u9700\u4f9d\u8d56\u4f20\u7edf\u5141\u8bb8\u5217\u8868\u3002", "motivation": "\u968f\u7740IoT\u8bbe\u5907\u5728\u667a\u80fd\u5bb6\u5c45\u4e2d\u7684\u5feb\u901f\u666e\u53ca\uff0c\u5176\u6301\u7eed\u7684\u4e91\u670d\u52a1\u8fde\u63a5\u5f15\u53d1\u4e86\u5b89\u5168\u548c\u9690\u79c1\u95ee\u9898\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff08\u5982\u57fa\u4e8e\u4e91\u7684\u5a01\u80c1\u68c0\u6d4b\u6216\u8fc7\u65f6\u7684\u5141\u8bb8\u5217\u8868\uff09\u4ecd\u5b58\u5728\u654f\u611f\u6570\u636e\u66b4\u9732\u6216\u6d41\u91cf\u9650\u5236\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efa\u57fa\u4e8eIoT\u8bbe\u5907\u884c\u4e3a\u7684\u6807\u8bb0\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u7279\u5f81\u63d0\u53d6\u6d41\u7a0b\u4ee5\u5b9e\u73b0\u5bf9\u7f51\u7edc\u76ee\u7684\u5730\u7684\u4e8c\u5143\u5206\u7c7b\uff08\u5fc5\u8981\u4e0e\u975e\u5fc5\u8981\uff09\uff0c\u5e76\u5728\u771f\u5b9e\u667a\u80fd\u5bb6\u5c45\u73af\u5883\u4e2d\u6d4b\u8bd5\u6a21\u578b\u3002", "result": "\u6a21\u578b\u80fd\u7cbe\u786e\u8bc6\u522b\u5e76\u963b\u6b62\u975e\u5fc5\u8981\u6d41\u91cf\uff08\u5305\u62ec\u672a\u89c1\u8fc7\u76ee\u7684\u5730\uff09\uff0c\u4e14\u53ef\u5728\u5bb6\u5ead\u63a5\u5165\u70b9\u5b9e\u73b0\u8fd1\u5b9e\u65f6\u5206\u7c7b\uff0c\u652f\u6301\u5927\u89c4\u6a21IoT\u90e8\u7f72\u3002", "conclusion": "ML-IoTrim\u4e3a\u667a\u80fd\u5bb6\u5c45\u9690\u79c1\u6d41\u91cf\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86IoT\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u7684\u672a\u6765\u53d1\u5c55\u3002"}}
{"id": "2504.18605", "pdf": "https://arxiv.org/pdf/2504.18605", "abs": "https://arxiv.org/abs/2504.18605", "authors": ["Baimam Boukar Jean Jacques"], "title": "Explainable Deep-Learning Based Potentially Hazardous Asteroids Classification Using Graph Neural Networks", "categories": ["astro-ph.EP", "astro-ph.IM", "cs.LG"], "comment": null, "summary": "Classifying potentially hazardous asteroids (PHAs) is crucial for planetary\ndefense and deep space navigation, yet traditional methods often overlook the\ndynamical relationships among asteroids. We introduce a Graph Neural Network\n(GNN) approach that models asteroids as nodes with orbital and physical\nfeatures, connected by edges representing their similarities, using a NASA\ndataset of 958,524 records. Despite an extreme class imbalance with only 0.22%\nof the dataset with the hazardous label, our model achieves an overall accuracy\nof 99% and an AUC of 0.99, with a recall of 78% and an F1-score of 37% for\nhazardous asteroids after applying the Synthetic Minority Oversampling\nTechnique. Feature importance analysis highlights albedo, perihelion distance,\nand semi-major axis as main predictors. This framework supports planetary\ndefense missions and confirms AI's potential in enabling autonomous navigation\nfor future missions such as NASA's NEO Surveyor and ESA's Ramses, offering an\ninterpretable and scalable solution for asteroid hazard assessment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u7c7b\u6f5c\u5728\u5371\u9669\u5c0f\u884c\u661f\uff08PHAs\uff09\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5ffd\u89c6\u5c0f\u884c\u661f\u95f4\u52a8\u529b\u5b66\u5173\u7cfb\u7684\u95ee\u9898\uff0c\u5e76\u5728NASA\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u9ad8\u7cbe\u5ea6\u548cAUC\u503c\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u884c\u661f\u9632\u5fa1\u548c\u6df1\u7a7a\u5bfc\u822a\u7684\u51c6\u786e\u6027\uff0c\u7814\u7a76\u65e8\u5728\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u5c0f\u884c\u661f\u95f4\u52a8\u529b\u5b66\u5173\u7cfb\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u6a21\u578b\uff0c\u5c06\u5c0f\u884c\u661f\u8868\u793a\u4e3a\u8282\u70b9\uff0c\u57fa\u4e8e\u8f68\u9053\u548c\u7269\u7406\u7279\u5f81\uff0c\u901a\u8fc7\u76f8\u4f3c\u6027\u8fb9\u8fde\u63a5\uff0c\u5e76\u4f7f\u7528\u5408\u6210\u5c11\u6570\u7c7b\u8fc7\u91c7\u6837\u6280\u672f\uff08SMOTE\uff09\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u3002", "result": "\u6a21\u578b\u5728NASA\u6570\u636e\u96c6\uff08958,524\u6761\u8bb0\u5f55\uff09\u4e0a\u603b\u4f53\u51c6\u786e\u7387\u8fbe99%\uff0cAUC\u4e3a0.99\uff0c\u5bf9\u5371\u9669\u5c0f\u884c\u661f\u7684\u53ec\u56de\u7387\u4e3a78%\uff0cF1\u5206\u6570\u4e3a37%\uff0c\u4e3b\u8981\u9884\u6d4b\u56e0\u5b50\u4e3a\u53cd\u7167\u7387\u3001\u8fd1\u65e5\u70b9\u8ddd\u79bb\u548c\u534a\u957f\u8f74\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u884c\u661f\u9632\u5fa1\u4efb\u52a1\u548c\u81ea\u4e3b\u5bfc\u822a\uff08\u5982NASA\u7684NEO Surveyor\u548cESA\u7684Ramses\uff09\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86AI\u5728\u5c0f\u884c\u661f\u98ce\u9669\u8bc4\u4f30\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.18624", "pdf": "https://arxiv.org/pdf/2504.18624", "abs": "https://arxiv.org/abs/2504.18624", "authors": ["Ali SaraerToosi", "Avery Broderick"], "title": "Validation and Calibration of Semi-Analytical Models for the Event Horizon Telescope Observations of Sagittarius A*", "categories": ["astro-ph.HE", "astro-ph.IM", "cs.CV", "cs.LG", "85A99, 35Q75, 65C60, 62F15", "I.2.6; G.1.10; I.4.10"], "comment": "26 pages, 9 figures, 2 tables, submitted to ApJ", "summary": "The Event Horizon Telescope (EHT) enables the exploration of black hole\naccretion flows at event-horizon scales. Fitting ray-traced physical models to\nEHT observations requires the generation of synthetic images, a task that is\ncomputationally demanding. This study leverages \\alinet, a generative machine\nlearning model, to efficiently produce radiatively inefficient accretion flow\n(RIAF) images as a function of the specified physical parameters. \\alinet has\npreviously been shown to be able to interpolate black hole images and their\nassociated physical parameters after training on a computationally tractable\nset of library images. We utilize this model to estimate the uncertainty\nintroduced by a number of anticipated unmodeled physical effects, including\ninterstellar scattering and intrinsic source variability. We then use this to\ncalibrate physical parameter estimates and their associated uncertainties from\nRIAF model fits to mock EHT data via a library of general relativistic\nmagnetohydrodynamics models.", "AI": {"tldr": "\u4f7f\u7528\u751f\u6210\u5f0f\u673a\u5668\u5b66\u4e60\u6a21\u578b\\alinet\u9ad8\u6548\u751f\u6210RIAF\u56fe\u50cf\uff0c\u8bc4\u4f30\u672a\u5efa\u6a21\u7269\u7406\u6548\u5e94\u7684\u5f71\u54cd\uff0c\u5e76\u6821\u51c6EHT\u89c2\u6d4b\u7684\u7269\u7406\u53c2\u6570\u4f30\u8ba1\u53ca\u5176\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u4e8b\u4ef6\u89c6\u754c\u671b\u8fdc\u955c\uff08EHT\uff09\u53ef\u4ee5\u63a2\u7d22\u9ed1\u6d1e\u5438\u79ef\u6d41\uff0c\u4f46\u751f\u6210\u5408\u6210\u56fe\u50cf\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\\alinet\u63d0\u9ad8\u6548\u7387\uff0c\u5e76\u8bc4\u4f30\u672a\u5efa\u6a21\u7269\u7406\u6548\u5e94\u5bf9\u53c2\u6570\u4f30\u8ba1\u7684\u5f71\u54cd\u3002", "method": "\u5229\u7528\\alinet\u6a21\u578b\u751f\u6210RIAF\u56fe\u50cf\uff0c\u7ed3\u5408\u5e7f\u4e49\u76f8\u5bf9\u8bba\u78c1\u6d41\u4f53\u52a8\u529b\u5b66\u6a21\u578b\u5e93\uff0c\u8bc4\u4f30\u661f\u9645\u6563\u5c04\u548c\u6e90\u5185\u53d8\u7387\u7b49\u672a\u5efa\u6a21\u6548\u5e94\u5f15\u8d77\u7684\u8bef\u5dee\u3002", "result": "\\alinet\u80fd\u591f\u9ad8\u6548\u751f\u6210\u56fe\u50cf\u5e76\u91cf\u5316\u672a\u5efa\u6a21\u6548\u5e94\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u6821\u51c6EHT\u89c2\u6d4b\u4e2d\u7684\u7269\u7406\u53c2\u6570\u4f30\u8ba1\u53ca\u5176\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\\alinet\u5728\u9ed1\u6d1e\u5438\u79ef\u6d41\u6a21\u578b\u62df\u5408\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3aEHT\u89c2\u6d4b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u53c2\u6570\u6821\u51c6\u65b9\u6cd5\u3002"}}
{"id": "2504.19080", "pdf": "https://arxiv.org/pdf/2504.19080", "abs": "https://arxiv.org/abs/2504.19080", "authors": ["Zhenkai Qin", "Jiaquan Liang", "Qiao Fang"], "title": "MIA-Mind: A Multidimensional Interactive Attention Mechanism Based on MindSpore", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Attention mechanisms have significantly advanced deep learning by enhancing\nfeature representation through selective focus. However, existing approaches\noften independently model channel importance and spatial saliency, overlooking\ntheir inherent interdependence and limiting their effectiveness. To address\nthis limitation, we propose MIA-Mind, a lightweight and modular\nMultidimensional Interactive Attention Mechanism, built upon the MindSpore\nframework. MIA-Mind jointly models spatial and channel features through a\nunified cross-attentive fusion strategy, enabling fine-grained feature\nrecalibration with minimal computational overhead. Extensive experiments are\nconducted on three representative datasets: on CIFAR-10, MIA-Mind achieves an\naccuracy of 82.9\\%; on ISBI2012, it achieves an accuracy of 78.7\\%; and on\nCIC-IDS2017, it achieves an accuracy of 91.9\\%. These results validate the\nversatility, lightweight design, and generalization ability of MIA-Mind across\nheterogeneous tasks. Future work will explore the extension of MIA-Mind to\nlarge-scale datasets, the development of ada,ptive attention fusion strategies,\nand distributed deployment to further enhance scalability and robustness.", "AI": {"tldr": "MIA-Mind\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u591a\u7ef4\u4ea4\u4e92\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8054\u5408\u5efa\u6a21\u7a7a\u95f4\u4e0e\u901a\u9053\u7279\u5f81\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u6ce8\u610f\u529b\u673a\u5236\u72ec\u7acb\u5efa\u6a21\u901a\u9053\u4e0e\u7a7a\u95f4\u7279\u5f81\uff0c\u5ffd\u7565\u5176\u5185\u5728\u5173\u8054\u6027\uff0c\u9650\u5236\u4e86\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86MIA-Mind\uff0c\u57fa\u4e8eMindSpore\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u878d\u5408\u7b56\u7565\u8054\u5408\u5efa\u6a21\u7a7a\u95f4\u4e0e\u901a\u9053\u7279\u5f81\u3002", "result": "\u5728CIFAR-10\u3001ISBI2012\u548cCIC-IDS2017\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523082.9%\u300178.7%\u548c91.9%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86MIA-Mind\u7684\u901a\u7528\u6027\u3001\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u672a\u6765\u5c06\u6269\u5c55\u81f3\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0e\u5206\u5e03\u5f0f\u90e8\u7f72\u3002"}}
{"id": "2504.18628", "pdf": "https://arxiv.org/pdf/2504.18628", "abs": "https://arxiv.org/abs/2504.18628", "authors": ["Christodoulos Peltekis", "Chrysostomos Nicopoulos", "Giorgos Dimitrakopoulos"], "title": "Periodic Online Testing for Sparse Systolic Tensor Arrays", "categories": ["cs.AR", "cs.LG"], "comment": "International Conference on Modern Circuits and Systems Technologies\n  (MOCAST) 2025", "summary": "Modern Machine Learning (ML) applications often benefit from structured\nsparsity, a technique that efficiently reduces model complexity and simplifies\nhandling of sparse data in hardware. Sparse systolic tensor arrays -\nspecifically designed to accelerate these structured-sparse ML models - play a\npivotal role in enabling efficient computations. As ML is increasingly\nintegrated into safety-critical systems, it is of paramount importance to\nensure the reliability of these systems. This paper introduces an online\nerror-checking technique capable of detecting and locating permanent faults\nwithin sparse systolic tensor arrays before computation begins. The new\ntechnique relies on merely four test vectors and exploits the weight values\nalready loaded within the systolic array to comprehensively test the system.\nFault-injection campaigns within the gate-level netlist, while executing three\nwell-established Convolutional Neural Networks (CNN), validate the efficiency\nof the proposed approach, which is shown to achieve very high fault coverage,\nwhile incurring minimal performance and area overheads.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7ebf\u9519\u8bef\u68c0\u6d4b\u6280\u672f\uff0c\u7528\u4e8e\u5728\u8ba1\u7b97\u5f00\u59cb\u524d\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7a00\u758f\u8109\u52a8\u5f20\u91cf\u9635\u5217\u4e2d\u7684\u6c38\u4e45\u6027\u6545\u969c\uff0c\u4ec5\u9700\u56db\u4e2a\u6d4b\u8bd5\u5411\u91cf\uff0c\u5e76\u901a\u8fc7\u6545\u969c\u6ce8\u5165\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u5728\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u786e\u4fdd\u8fd9\u4e9b\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7a00\u758f\u8109\u52a8\u5f20\u91cf\u9635\u5217\u5728\u52a0\u901f\u7ed3\u6784\u5316\u7a00\u758fML\u6a21\u578b\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\uff0c\u4f46\u5176\u53ef\u9760\u6027\u9700\u8981\u5f97\u5230\u4fdd\u969c\u3002", "method": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u56db\u4e2a\u6d4b\u8bd5\u5411\u91cf\u7684\u5728\u7ebf\u9519\u8bef\u68c0\u67e5\u6280\u672f\uff0c\u5229\u7528\u8109\u52a8\u9635\u5217\u4e2d\u5df2\u52a0\u8f7d\u7684\u6743\u91cd\u503c\u8fdb\u884c\u5168\u9762\u6d4b\u8bd5\u3002", "result": "\u5728\u4e09\u4e2a\u6210\u719f\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u4e0a\u8fdb\u884c\u7684\u6545\u969c\u6ce8\u5165\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5b9e\u73b0\u6781\u9ad8\u7684\u6545\u969c\u8986\u76d6\u7387\uff0c\u540c\u65f6\u6027\u80fd\u548c\u9762\u79ef\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6280\u672f\u4e3a\u7a00\u758f\u8109\u52a8\u5f20\u91cf\u9635\u5217\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u5f00\u9500\u7684\u53ef\u9760\u6027\u4fdd\u969c\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b89\u5168\u5173\u952e\u7684ML\u5e94\u7528\u3002"}}
{"id": "2504.19093", "pdf": "https://arxiv.org/pdf/2504.19093", "abs": "https://arxiv.org/abs/2504.19093", "authors": ["Yu Li", "Qizhi Pei", "Mengyuan Sun", "Honglin Lin", "Chenlin Ming", "Xin Gao", "Jiang Wu", "Conghui He", "Lijun Wu"], "title": "CipherBank: Exploring the Boundary of LLM Reasoning Capabilities through Cryptography Challenges", "categories": ["cs.CR", "cs.AI", "cs.PF"], "comment": "Work in progress", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities,\nespecially the recent advancements in reasoning, such as o1 and o3, pushing the\nboundaries of AI. Despite these impressive achievements in mathematics and\ncoding, the reasoning abilities of LLMs in domains requiring cryptographic\nexpertise remain underexplored. In this paper, we introduce CipherBank, a\ncomprehensive benchmark designed to evaluate the reasoning capabilities of LLMs\nin cryptographic decryption tasks. CipherBank comprises 2,358 meticulously\ncrafted problems, covering 262 unique plaintexts across 5 domains and 14\nsubdomains, with a focus on privacy-sensitive and real-world scenarios that\nnecessitate encryption. From a cryptographic perspective, CipherBank\nincorporates 3 major categories of encryption methods, spanning 9 distinct\nalgorithms, ranging from classical ciphers to custom cryptographic techniques.\nWe evaluate state-of-the-art LLMs on CipherBank, e.g., GPT-4o, DeepSeek-V3, and\ncutting-edge reasoning-focused models such as o1 and DeepSeek-R1. Our results\nreveal significant gaps in reasoning abilities not only between general-purpose\nchat LLMs and reasoning-focused LLMs but also in the performance of current\nreasoning-focused models when applied to classical cryptographic decryption\ntasks, highlighting the challenges these models face in understanding and\nmanipulating encrypted data. Through detailed analysis and error\ninvestigations, we provide several key observations that shed light on the\nlimitations and potential improvement areas for LLMs in cryptographic\nreasoning. These findings underscore the need for continuous advancements in\nLLM reasoning capabilities.", "AI": {"tldr": "CipherBank\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u52a0\u5bc6\u89e3\u5bc6\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u5bc6\u7801\u5b66\u9886\u57df\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u548c\u7f16\u7801\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u9700\u8981\u5bc6\u7801\u5b66\u77e5\u8bc6\u7684\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u56e2\u961f\u8bbe\u8ba1\u4e86CipherBank\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b2,358\u4e2a\u95ee\u9898\uff0c\u6db5\u76d614\u4e2a\u5b50\u9886\u57df\u548c9\u79cd\u52a0\u5bc6\u7b97\u6cd5\uff0c\u8bc4\u4f30\u4e86\u5305\u62ecGPT-4o\u548cDeepSeek-V3\u5728\u5185\u7684\u591a\u79cd\u6a21\u578b\u3002", "result": "\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\uff0c\u901a\u7528\u804a\u5929\u6a21\u578b\u548c\u4e13\u6ce8\u4e8e\u63a8\u7406\u7684\u6a21\u578b\u5728\u5bc6\u7801\u5b66\u4efb\u52a1\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u5c24\u5176\u5728\u7ecf\u5178\u52a0\u5bc6\u89e3\u5bc6\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5bc6\u7801\u5b66\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u7684\u65b9\u5411\u3002"}}
{"id": "2504.18633", "pdf": "https://arxiv.org/pdf/2504.18633", "abs": "https://arxiv.org/abs/2504.18633", "authors": ["Nguyen Thi Minh Phu", "Duong Tan Loc", "Vo Nguyen Le Duy"], "title": "Statistical Inference for Clustering-based Anomaly Detection", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Unsupervised anomaly detection (AD) is a fundamental problem in machine\nlearning and statistics. A popular approach to unsupervised AD is\nclustering-based detection. However, this method lacks the ability to guarantee\nthe reliability of the detected anomalies. In this paper, we propose SI-CLAD\n(Statistical Inference for CLustering-based Anomaly Detection), a novel\nstatistical framework for testing the clustering-based AD results. The key\nstrength of SI-CLAD lies in its ability to rigorously control the probability\nof falsely identifying anomalies, maintaining it below a pre-specified\nsignificance level $\\alpha$ (e.g., $\\alpha = 0.05$). By analyzing the selection\nmechanism inherent in clustering-based AD and leveraging the Selective\nInference (SI) framework, we prove that false detection control is attainable.\nMoreover, we introduce a strategy to boost the true detection rate, enhancing\nthe overall performance of SI-CLAD. Extensive experiments on synthetic and\nreal-world datasets provide strong empirical support for our theoretical\nfindings, showcasing the superior performance of the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSI-CLAD\u7684\u65b0\u578b\u7edf\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u9a8c\u57fa\u4e8e\u805a\u7c7b\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u7ed3\u679c\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u63a8\u65ad\uff08SI\uff09\u6846\u67b6\u4e25\u683c\u63a7\u5236\u8bef\u68c0\u6982\u7387\uff0c\u5e76\u63d0\u5347\u771f\u68c0\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u805a\u7c7b\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u68c0\u6d4b\u7ed3\u679c\u53ef\u9760\u6027\u7684\u4fdd\u8bc1\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63a7\u5236\u8bef\u68c0\u6982\u7387\u5e76\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u7684\u7edf\u8ba1\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u9009\u62e9\u6027\u63a8\u65ad\uff08SI\uff09\u6846\u67b6\uff0c\u5206\u6790\u805a\u7c7b\u5f02\u5e38\u68c0\u6d4b\u7684\u9009\u62e9\u673a\u5236\uff0c\u63d0\u51faSI-CLAD\u6846\u67b6\uff0c\u63a7\u5236\u8bef\u68c0\u6982\u7387\u4f4e\u4e8e\u9884\u8bbe\u663e\u8457\u6027\u6c34\u5e73\u03b1\uff0c\u5e76\u4f18\u5316\u771f\u68c0\u7387\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SI-CLAD\u7684\u7406\u8bba\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "SI-CLAD\u4e3a\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u7684\u7edf\u8ba1\u68c0\u9a8c\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2504.19099", "pdf": "https://arxiv.org/pdf/2504.19099", "abs": "https://arxiv.org/abs/2504.19099", "authors": ["Ning Wang", "Bingkun Yao", "Jie Zhou", "Yuchen Hu", "Xi Wang", "Nan Guan", "Zhe Jiang"], "title": "VeriDebug: A Unified LLM for Verilog Debugging via Contrastive Embedding and Guided Correction", "categories": ["cs.SE", "cs.AI", "cs.AR"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable potential in\ndebugging for various programming languages. However, the application of LLMs\nto Verilog debugging remains insufficiently explored. Here, we present\nVeriDebug, an approach that integrates contrastive representation and guided\ncorrection capabilities for automated Verilog debugging. Unlike existing\nmethods, VeriDebug employs an embedding-based technique to accurately retrieve\ninternal information, followed by bug-fixing. VeriDebug unifies Verilog bug\ndetection and correction through a shared parameter space. By simultaneously\nlearning bug patterns and fixes, it streamlines debugging via contrastive\nembedding and guided correction. Empirical results show the efficacy of\nVeriDebug in enhancing Verilog debugging. Our VeriDebugLoc, Type model achieves\n64.7 accuracy in bug fixing (Acc1), a significant improvement from the existing\nopen-source SOTAs 11.3. This performance not only outperforms open-source\nalternatives but also exceeds larger closed-source models like GPT-3.5-turbo\n(36.6), offering a more accurate alternative to conventional debugging methods.", "AI": {"tldr": "VeriDebug\u662f\u4e00\u79cd\u96c6\u6210\u5bf9\u6bd4\u8868\u793a\u548c\u5f15\u5bfc\u4fee\u6b63\u7684\u81ea\u52a8\u5316Verilog\u8c03\u8bd5\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u8c03\u8bd5\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22LLMs\u5728Verilog\u8c03\u8bd5\u4e2d\u7684\u5e94\u7528\uff0c\u5f25\u8865\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5d4c\u5165\u7684\u6280\u672f\u68c0\u7d22\u5185\u90e8\u4fe1\u606f\u5e76\u7ed3\u5408\u5f15\u5bfc\u4fee\u6b63\uff0c\u7edf\u4e00\u68c0\u6d4b\u4e0e\u4fee\u590d\u3002", "result": "VeriDebugLoc\u6a21\u578b\u5728\u4fee\u590d\u51c6\u786e\u7387\uff08Acc1\uff09\u8fbe\u523064.7\uff0c\u8fdc\u8d85\u5f00\u6e90\u548cGPT-3.5-turbo\u3002", "conclusion": "VeriDebug\u4e3aVerilog\u8c03\u8bd5\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18657", "pdf": "https://arxiv.org/pdf/2504.18657", "abs": "https://arxiv.org/abs/2504.18657", "authors": ["Benjamin Schiffer", "Lucas Janson"], "title": "Foundations of Safe Online Reinforcement Learning in the Linear Quadratic Regulator: $\\sqrt{T}$-Regret", "categories": ["stat.ML", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Understanding how to efficiently learn while adhering to safety constraints\nis essential for using online reinforcement learning in practical applications.\nHowever, proving rigorous regret bounds for safety-constrained reinforcement\nlearning is difficult due to the complex interaction between safety,\nexploration, and exploitation. In this work, we seek to establish foundations\nfor safety-constrained reinforcement learning by studying the canonical problem\nof controlling a one-dimensional linear dynamical system with unknown dynamics.\nWe study the safety-constrained version of this problem, where the state must\nwith high probability stay within a safe region, and we provide the first safe\nalgorithm that achieves regret of $\\tilde{O}_T(\\sqrt{T})$. Furthermore, the\nregret is with respect to the baseline of truncated linear controllers, a\nnatural baseline of non-linear controllers that are well-suited for\nsafety-constrained linear systems. In addition to introducing this new\nbaseline, we also prove several desirable continuity properties of the optimal\ncontroller in this baseline. In showing our main result, we prove that whenever\nthe constraints impact the optimal controller, the non-linearity of our\ncontroller class leads to a faster rate of learning than in the unconstrained\nsetting.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5b89\u5168\u7ea6\u675f\u4e0b\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u9488\u5bf9\u4e00\u7ef4\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u7684\u63a7\u5236\u95ee\u9898\uff0c\u63d0\u51fa\u9996\u4e2a\u5b9e\u73b0$\tilde{O}_T(\\\\sqrt{T})$\u9057\u61be\u7684\u5b89\u5168\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u5b89\u5168\u7ea6\u675f\u4e0b\u9ad8\u6548\u5b66\u4e60\uff0c\u4ee5\u89e3\u51b3\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u6027\u95ee\u9898\u3002", "method": "\u7814\u7a76\u4e00\u7ef4\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u7684\u5b89\u5168\u7ea6\u675f\u63a7\u5236\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u622a\u65ad\u7ebf\u6027\u63a7\u5236\u5668\u7684\u5b89\u5168\u7b97\u6cd5\u3002", "result": "\u7b97\u6cd5\u5b9e\u73b0\u4e86$\tilde{O}_T(\\\\sqrt{T})$\u7684\u9057\u61be\uff0c\u5e76\u5728\u7ea6\u675f\u5f71\u54cd\u6700\u4f18\u63a7\u5236\u5668\u65f6\u8868\u73b0\u51fa\u6bd4\u65e0\u7ea6\u675f\u73af\u5883\u4e0b\u66f4\u5feb\u7684\u5b66\u4e60\u7387\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u57fa\u51c6\u548c\u8bc1\u660e\u6700\u4f18\u63a7\u5236\u5668\u7684\u8fde\u7eed\u6027\uff0c\u4e3a\u5b89\u5168\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2504.19120", "pdf": "https://arxiv.org/pdf/2504.19120", "abs": "https://arxiv.org/abs/2504.19120", "authors": ["Gaojian Huang", "Yantong Jin", "Wei-Hsiang Lo"], "title": "Beyond Levels of Driving Automation: A Triadic Framework of Human-AI Collaboration in On-Road Mobility", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The goal of the current study is to introduce a triadic human-AI\ncollaboration framework for the automated vehicle domain. Previous\nclassifications (e.g., SAE Levels of Automation) focus on defining automation\nlevels based on who controls the vehicle. However, it remains unclear how human\nusers and AI should collaborate in real-time, especially in dynamic driving\ncontexts, where roles can shift frequently. To fill the gap, this study\nproposes a triadic human-AI collaboration framework with three AI roles (i.e.,\nAdvisor, Co-Pilot, and Guardian) that dynamically adapt to human needs.\nOverall, the study lays a foundation for developing adaptive, role-based\nhuman-AI collaboration strategies in automated vehicles.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u5143\u4eba\u673a\u534f\u4f5c\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u9a7e\u9a76\u573a\u666f\u4e2d\u4eba\u4e0eAI\u7684\u5b9e\u65f6\u534f\u4f5c\u3002", "motivation": "\u73b0\u6709\u5206\u7c7b\uff08\u5982SAE\u81ea\u52a8\u5316\u7b49\u7ea7\uff09\u4ec5\u5173\u6ce8\u8f66\u8f86\u63a7\u5236\u8005\uff0c\u7f3a\u4e4f\u5bf9\u52a8\u6001\u9a7e\u9a76\u573a\u666f\u4e2d\u4eba\u4e0eAI\u5982\u4f55\u5b9e\u65f6\u534f\u4f5c\u7684\u6307\u5bfc\u3002", "method": "\u8bbe\u8ba1\u4e86\u5305\u542b\u4e09\u79cdAI\u89d2\u8272\uff08\u987e\u95ee\u3001\u526f\u9a7e\u9a76\u3001\u5b88\u62a4\u8005\uff09\u7684\u6846\u67b6\uff0c\u6839\u636e\u4eba\u7c7b\u9700\u6c42\u52a8\u6001\u8c03\u6574\u89d2\u8272\u3002", "result": "\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e2d\u57fa\u4e8e\u89d2\u8272\u7684\u81ea\u9002\u5e94\u4eba\u673a\u534f\u4f5c\u7b56\u7565\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u4e09\u5143\u6846\u67b6\u586b\u8865\u4e86\u52a8\u6001\u573a\u666f\u4e2d\u4eba\u673a\u534f\u4f5c\u7684\u7a7a\u767d\uff0c\u652f\u6301\u89d2\u8272\u7075\u6d3b\u5207\u6362\u3002"}}
{"id": "2504.19136", "pdf": "https://arxiv.org/pdf/2504.19136", "abs": "https://arxiv.org/abs/2504.19136", "authors": ["Huiling Zheng", "Xian Zhong", "Bin Liu", "Yi Xiao", "Bihan Wen", "Xiaofeng Li"], "title": "PAD: Phase-Amplitude Decoupling Fusion for Multi-Modal Land Cover Classification", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "13 pages, 8 figures", "summary": "The fusion of Synthetic Aperture Radar (SAR) and RGB imagery for land cover\nclassification remains challenging due to modality heterogeneity and the\nunderutilization of spectral complementarity. Existing methods often fail to\ndecouple shared structural features from modality-specific radiometric\nattributes, leading to feature conflicts and information loss. To address this\nissue, we propose Phase-Amplitude Decoupling (PAD), a frequency-aware framework\nthat separates phase (modality-shared) and amplitude (modality-specific)\ncomponents in the Fourier domain. Specifically, PAD consists of two key\ncomponents: 1) Phase Spectrum Correction (PSC), which aligns cross-modal phase\nfeatures through convolution-guided scaling to enhance geometric consistency,\nand 2) Amplitude Spectrum Fusion (ASF), which dynamically integrates\nhigh-frequency details and low-frequency structures using frequency-adaptive\nmultilayer perceptrons. This approach leverages SAR's sensitivity to\nmorphological features and RGB's spectral richness. Extensive experiments on\nWHU-OPT-SAR and DDHR-SK datasets demonstrate state-of-the-art performance. Our\nwork establishes a new paradigm for physics-aware multi-modal fusion in remote\nsensing. The code will be available at https://github.com/RanFeng2/PAD.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPAD\u7684\u9891\u7387\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u5085\u91cc\u53f6\u57df\u4e2d\u7684\u76f8\u4f4d\uff08\u6a21\u6001\u5171\u4eab\uff09\u548c\u5e45\u5ea6\uff08\u6a21\u6001\u7279\u5b9a\uff09\u6210\u5206\uff0c\u89e3\u51b3\u4e86SAR\u548cRGB\u56fe\u50cf\u5728\u571f\u5730\u8986\u76d6\u5206\u7c7b\u4e2d\u7684\u6a21\u6001\u5f02\u8d28\u6027\u548c\u5149\u8c31\u4e92\u8865\u6027\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u89e3\u8026\u5171\u4eab\u7ed3\u6784\u7279\u5f81\u548c\u6a21\u6001\u7279\u5b9a\u7684\u8f90\u5c04\u5c5e\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u7279\u5f81\u51b2\u7a81\u548c\u4fe1\u606f\u4e22\u5931\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86PAD\u6846\u67b6\u3002", "method": "PAD\u6846\u67b6\u5305\u62ec\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1\uff09\u76f8\u4f4d\u8c31\u6821\u6b63\uff08PSC\uff09\uff0c\u901a\u8fc7\u5377\u79ef\u5f15\u5bfc\u7684\u7f29\u653e\u5bf9\u9f50\u8de8\u6a21\u6001\u76f8\u4f4d\u7279\u5f81\uff1b2\uff09\u5e45\u5ea6\u8c31\u878d\u5408\uff08ASF\uff09\uff0c\u4f7f\u7528\u9891\u7387\u81ea\u9002\u5e94\u591a\u5c42\u611f\u77e5\u5668\u52a8\u6001\u6574\u5408\u9ad8\u3001\u4f4e\u9891\u7ec6\u8282\u3002", "result": "\u5728WHU-OPT-SAR\u548cDDHR-SK\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7269\u7406\u611f\u77e5\u7684\u591a\u6a21\u6001\u878d\u5408\u5728\u9065\u611f\u9886\u57df\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2504.18695", "pdf": "https://arxiv.org/pdf/2504.18695", "abs": "https://arxiv.org/abs/2504.18695", "authors": ["Ladan Tazik", "James Stafford", "John Braun"], "title": "Local Polynomial Lp-norm Regression", "categories": ["stat.ML", "cs.LG", "stat.OT"], "comment": null, "summary": "The local least squares estimator for a regression curve cannot provide\noptimal results when non-Gaussian noise is present. Both theoretical and\nempirical evidence suggests that residuals often exhibit distributional\nproperties different from those of a normal distribution, making it worthwhile\nto consider estimation based on other norms. It is suggested that $L_p$-norm\nestimators be used to minimize the residuals when these exhibit non-normal\nkurtosis. In this paper, we propose a local polynomial $L_p$-norm regression\nthat replaces weighted least squares estimation with weighted $L_p$-norm\nestimation for fitting the polynomial locally. We also introduce a new method\nfor estimating the parameter $p$ from the residuals, enhancing the adaptability\nof the approach. Through numerical and theoretical investigation, we\ndemonstrate our method's superiority over local least squares in\none-dimensional data and show promising outcomes for higher dimensions,\nspecifically in 2D.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e$L_p$\u8303\u6570\u7684\u5c40\u90e8\u591a\u9879\u5f0f\u56de\u5f52\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a0\u6743$L_p$\u8303\u6570\u4f30\u8ba1\u66ff\u4ee3\u4f20\u7edf\u7684\u52a0\u6743\u6700\u5c0f\u4e8c\u4e58\u4f30\u8ba1\uff0c\u7528\u4e8e\u5904\u7406\u975e\u9ad8\u65af\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u56de\u5f52\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u4e00\u7ef4\u548c\u4e8c\u7ef4\u6570\u636e\u4e0a\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edf\u6700\u5c0f\u4e8c\u4e58\u4f30\u8ba1\u5728\u975e\u9ad8\u65af\u566a\u58f0\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u8003\u8651\u57fa\u4e8e\u5176\u4ed6\u8303\u6570\u7684\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u6b8b\u5dee\u7684\u975e\u6b63\u6001\u5206\u5e03\u7279\u6027\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u5c40\u90e8\u591a\u9879\u5f0f$L_p$\u8303\u6570\u56de\u5f52\uff0c\u4f7f\u7528\u52a0\u6743$L_p$\u8303\u6570\u4f30\u8ba1\u66ff\u4ee3\u52a0\u6743\u6700\u5c0f\u4e8c\u4e58\u4f30\u8ba1\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u53c2\u6570$p$\u4f30\u8ba1\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u6570\u503c\u548c\u7406\u8bba\u5206\u6790\uff0c\u8bba\u6587\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u4e00\u7ef4\u6570\u636e\u4e0a\u4f18\u4e8e\u5c40\u90e8\u6700\u5c0f\u4e8c\u4e58\u4f30\u8ba1\uff0c\u5e76\u5728\u4e8c\u7ef4\u6570\u636e\u4e0a\u5c55\u793a\u4e86\u826f\u597d\u7684\u524d\u666f\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u56de\u5f52\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u9002\u5e94\u975e\u9ad8\u65af\u566a\u58f0\uff0c\u672a\u6765\u53ef\u5728\u9ad8\u7ef4\u6570\u636e\u4e2d\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u5176\u6027\u80fd\u3002"}}
{"id": "2504.19142", "pdf": "https://arxiv.org/pdf/2504.19142", "abs": "https://arxiv.org/abs/2504.19142", "authors": ["Chenhao Xu", "Chunyu Chen", "Jinglin Peng", "Jiannan Wang", "Jun Gao"], "title": "BQSched: A Non-intrusive Scheduler for Batch Concurrent Queries via Reinforcement Learning", "categories": ["cs.DB", "cs.AI"], "comment": "Accepted by ICDE '25", "summary": "Most large enterprises build predefined data pipelines and execute them\nperiodically to process operational data using SQL queries for various tasks. A\nkey issue in minimizing the overall makespan of these pipelines is the\nefficient scheduling of concurrent queries within the pipelines. Existing tools\nmainly rely on simple heuristic rules due to the difficulty of expressing the\ncomplex features and mutual influences of queries. The latest reinforcement\nlearning (RL) based methods have the potential to capture these patterns from\nfeedback, but it is non-trivial to apply them directly due to the large\nscheduling space, high sampling cost, and poor sample utilization.\n  Motivated by these challenges, we propose BQSched, a non-intrusive Scheduler\nfor Batch concurrent Queries via reinforcement learning. Specifically, BQSched\ndesigns an attention-based state representation to capture the complex query\npatterns, and proposes IQ-PPO, an auxiliary task-enhanced proximal policy\noptimization (PPO) algorithm, to fully exploit the rich signals of Individual\nQuery completion in logs. Based on the RL framework above, BQSched further\nintroduces three optimization strategies, including adaptive masking to prune\nthe action space, scheduling gain-based query clustering to deal with large\nquery sets, and an incremental simulator to reduce sampling cost. To our\nknowledge, BQSched is the first non-intrusive batch query scheduler via RL.\nExtensive experiments show that BQSched can significantly improve the\nefficiency and stability of batch query scheduling, while also achieving\nremarkable scalability and adaptability in both data and queries. For example,\nacross all DBMSs and scales tested, BQSched reduces the overall makespan of\nbatch queries on TPC-DS benchmark by an average of 34% and 13%, compared with\nthe commonly used heuristic strategy and the adapted RL-based scheduler,\nrespectively.", "AI": {"tldr": "BQSched\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u975e\u4fb5\u5165\u5f0f\u6279\u5904\u7406\u67e5\u8be2\u8c03\u5ea6\u5668\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u548cIQ-PPO\u7b97\u6cd5\u4f18\u5316\u67e5\u8be2\u8c03\u5ea6\uff0c\u663e\u8457\u7f29\u77ed\u4e86\u603b\u4f53\u6267\u884c\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u4f9d\u8d56\u7b80\u5355\u542f\u53d1\u5f0f\u89c4\u5219\u5904\u7406\u5e76\u53d1\u67e5\u8be2\u8c03\u5ea6\uff0c\u4f46\u96be\u4ee5\u6355\u6349\u590d\u6742\u67e5\u8be2\u7279\u5f81\u548c\u76f8\u4e92\u5f71\u54cd\uff0c\u5f3a\u5316\u5b66\u4e60\u867d\u5177\u6f5c\u529b\u4f46\u9762\u4e34\u8c03\u5ea6\u7a7a\u95f4\u5927\u3001\u91c7\u6837\u6210\u672c\u9ad8\u7b49\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u72b6\u6001\u8868\u793a\u548c\u8f85\u52a9\u4efb\u52a1\u589e\u5f3a\u7684IQ-PPO\u7b97\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u81ea\u9002\u5e94\u63a9\u7801\u3001\u8c03\u5ea6\u589e\u76ca\u805a\u7c7b\u548c\u589e\u91cf\u6a21\u62df\u5668\u7b49\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728TPC-DS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBQSched\u5e73\u5747\u7f29\u77ed\u4e8634%\u548c13%\u7684\u603b\u4f53\u6267\u884c\u65f6\u95f4\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u7b56\u7565\u548c\u73b0\u6709RL\u8c03\u5ea6\u5668\u3002", "conclusion": "BQSched\u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8eRL\u7684\u975e\u4fb5\u5165\u5f0f\u6279\u5904\u7406\u67e5\u8be2\u8c03\u5ea6\uff0c\u5c55\u73b0\u4e86\u9ad8\u6548\u7387\u3001\u7a33\u5b9a\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2504.19155", "pdf": "https://arxiv.org/pdf/2504.19155", "abs": "https://arxiv.org/abs/2504.19155", "authors": ["Hussein Harb", "Didier Benoit", "Axel Rannou", "Chi-Hieu Pham", "Valentin Tissot", "Bahaa Nasr", "Julien Bert"], "title": "Machine Learning-Based Modeling of the Anode Heel Effect in X-ray Beam Monte Carlo Simulations", "categories": ["physics.med-ph", "cs.AI"], "comment": "15 pages, 8 figures", "summary": "This study enhances Monte Carlo simulation accuracy in X-ray imaging by\ndeveloping an AI-driven model for the anode heel effect, achieving improved\nbeam intensity distribution and dosimetric precision. Through dynamic\nadjustments to beam weights on the anode and cathode sides of the X-ray tube,\nour machine learning model effectively replicates the asymmetry characteristic\nof clinical X-ray beams. Experimental results reveal dose rate increases of up\nto 9.6% on the cathode side and reductions of up to 12.5% on the anode side,\nfor energy levels between 50 and 120 kVp. These experimentally optimized beam\nweights were integrated into the OpenGATE and GGEMS Monte Carlo toolkits,\nsignificantly advancing dosimetric simulation accuracy and the image quality\nwhich closely resembles the clinical imaging. Validation with fluence and dose\nactors demonstrated that the AI-based model closely mirrors clinical beam\nbehavior, providing substantial improvements in dose consistency and accuracy\nover conventional X-ray models. This approach provides a robust framework for\nimproving X-ray dosimetry, with potential applications in dose optimization,\nimaging quality enhancement, and radiation safety in both clinical and research\nsettings.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5f00\u53d1\u4e00\u79cdAI\u9a71\u52a8\u7684\u6a21\u578b\u6765\u589e\u5f3aX\u5c04\u7ebf\u6210\u50cf\u4e2d\u8499\u7279\u5361\u6d1b\u6a21\u62df\u7684\u51c6\u786e\u6027\uff0c\u91cd\u70b9\u6539\u8fdb\u4e86\u9633\u6781\u8db3\u8ddf\u6548\u5e94\u7684\u6a21\u62df\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u51c6\u7684\u5c04\u675f\u5f3a\u5ea6\u5206\u5e03\u548c\u5242\u91cf\u8ba1\u7b97\u3002", "motivation": "\u4f20\u7edfX\u5c04\u7ebf\u6a21\u578b\u5728\u6a21\u62df\u9633\u6781\u8db3\u8ddf\u6548\u5e94\u65f6\u5b58\u5728\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u5242\u91cf\u8ba1\u7b97\u548c\u56fe\u50cf\u8d28\u91cf\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7AI\u6280\u672f\u4f18\u5316\u8fd9\u4e00\u6a21\u62df\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u52a8\u6001\u8c03\u6574X\u5c04\u7ebf\u7ba1\u9633\u6781\u548c\u9634\u6781\u4fa7\u7684\u5c04\u675f\u6743\u91cd\uff0c\u4ee5\u590d\u73b0\u4e34\u5e8aX\u5c04\u7ebf\u675f\u7684\u4e0d\u5bf9\u79f0\u7279\u6027\u3002\u4f18\u5316\u540e\u7684\u5c04\u675f\u6743\u91cd\u88ab\u96c6\u6210\u5230OpenGATE\u548cGGEMS\u8499\u7279\u5361\u6d1b\u5de5\u5177\u5305\u4e2d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u572850\u81f3120 kVp\u80fd\u91cf\u8303\u56f4\u5185\uff0c\u9634\u6781\u4fa7\u7684\u5242\u91cf\u7387\u63d0\u5347\u4e869.6%\uff0c\u9633\u6781\u4fa7\u964d\u4f4e\u4e8612.5%\u3002\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u5242\u91cf\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5AI\u6a21\u578b\u4e3aX\u5c04\u7ebf\u5242\u91cf\u5b66\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u6a21\u62df\u6846\u67b6\uff0c\u6709\u671b\u5728\u4e34\u5e8a\u548c\u79d1\u7814\u4e2d\u5e94\u7528\u4e8e\u5242\u91cf\u4f18\u5316\u3001\u6210\u50cf\u8d28\u91cf\u63d0\u5347\u53ca\u8f90\u5c04\u5b89\u5168\u3002"}}
{"id": "2504.18791", "pdf": "https://arxiv.org/pdf/2504.18791", "abs": "https://arxiv.org/abs/2504.18791", "authors": ["Uday Kiran Reddy Tadipatri", "Benjamin D. Haeffele", "Joshua Agterberg", "Ingvar Ziemann", "Ren\u00e9 Vidal"], "title": "Nonconvex Linear System Identification with Minimal State Representation", "categories": ["eess.SY", "cs.LG", "cs.SY", "eess.SP", "stat.ML"], "comment": "Accepted to 7th Annual Conference on Learning for Dynamics and\n  Control (L4DC) 2025. The full version including appendix", "summary": "Low-order linear System IDentification (SysID) addresses the challenge of\nestimating the parameters of a linear dynamical system from finite samples of\nobservations and control inputs with minimal state representation. Traditional\napproaches often utilize Hankel-rank minimization, which relies on convex\nrelaxations that can require numerous, costly singular value decompositions\n(SVDs) to optimize. In this work, we propose two nonconvex reformulations to\ntackle low-order SysID (i) Burer-Monterio (BM) factorization of the Hankel\nmatrix for efficient nuclear norm minimization, and (ii) optimizing directly\nover system parameters for real, diagonalizable systems with an atomic norm\nstyle decomposition. These reformulations circumvent the need for repeated\nheavy SVD computations, significantly improving computational efficiency.\nMoreover, we prove that optimizing directly over the system parameters yields\nlower statistical error rates, and lower sample complexities that do not scale\nlinearly with trajectory length like in Hankel-nuclear norm minimization.\nAdditionally, while our proposed formulations are nonconvex, we provide\ntheoretical guarantees of achieving global optimality in polynomial time.\nFinally, we demonstrate algorithms that solve these nonconvex programs and\nvalidate our theoretical claims on synthetic data.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u975e\u51f8\u91cd\u6784\u65b9\u6cd5\uff08Burer-Monterio\u5206\u89e3\u548c\u7cfb\u7edf\u53c2\u6570\u76f4\u63a5\u4f18\u5316\uff09\u4ee5\u9ad8\u6548\u89e3\u51b3\u4f4e\u9636\u7ebf\u6027\u7cfb\u7edf\u8bc6\u522b\u95ee\u9898\uff0c\u907f\u514d\u4e86\u4f20\u7edfHankel\u79e9\u6700\u5c0f\u5316\u65b9\u6cd5\u4e2d\u91cd\u590dSVD\u8ba1\u7b97\u7684\u9ad8\u6210\u672c\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u7edf\u8ba1\u8bef\u5dee\u548c\u6837\u672c\u590d\u6742\u5ea6\u4e0a\u7684\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edfHankel\u79e9\u6700\u5c0f\u5316\u65b9\u6cd5\u4f9d\u8d56\u51f8\u677e\u5f1b\uff0c\u9700\u591a\u6b21\u6602\u8d35SVD\u8ba1\u7b97\uff0c\u8ba1\u7b97\u6548\u7387\u4f4e\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u975e\u51f8\u91cd\u6784\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u6539\u5584\u7edf\u8ba1\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a(i) Hankel\u77e9\u9635\u7684Burer-Monterio\u5206\u89e3\u4ee5\u5b9e\u73b0\u9ad8\u6548\u6838\u8303\u6570\u6700\u5c0f\u5316\uff0c(ii) \u5bf9\u53ef\u5bf9\u89d2\u5316\u7cfb\u7edf\u76f4\u63a5\u4f18\u5316\u7cfb\u7edf\u53c2\u6570\uff0c\u91c7\u7528\u539f\u5b50\u8303\u6570\u5206\u89e3\u3002", "result": "\u65b0\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u964d\u4f4e\u4e86\u7edf\u8ba1\u8bef\u5dee\u548c\u6837\u672c\u590d\u6742\u5ea6\uff08\u4e0d\u4e0e\u8f68\u8ff9\u957f\u5ea6\u7ebf\u6027\u76f8\u5173\uff09\uff0c\u4e14\u7406\u8bba\u4fdd\u8bc1\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u5168\u5c40\u6700\u4f18\u3002", "conclusion": "\u975e\u51f8\u91cd\u6784\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u7edf\u8ba1\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4f4e\u9636\u7ebf\u6027\u7cfb\u7edf\u8bc6\u522b\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u6570\u636e\u9a8c\u8bc1\u4e86\u7406\u8bba\u3002"}}
{"id": "2504.18802", "pdf": "https://arxiv.org/pdf/2504.18802", "abs": "https://arxiv.org/abs/2504.18802", "authors": ["Xiren Zhou", "Shikang Liu", "Xinyu Yan", "Yizhan Fan", "Xiangyu Wang", "Yu Kang", "Jian Cheng", "Huanhuan Chen"], "title": "Reservoir-enhanced Segment Anything Model for Subsurface Diagnosis", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Urban roads and infrastructure, vital to city operations, face growing\nthreats from subsurface anomalies like cracks and cavities. Ground Penetrating\nRadar (GPR) effectively visualizes underground conditions employing\nelectromagnetic (EM) waves; however, accurate anomaly detection via GPR remains\nchallenging due to limited labeled data, varying subsurface conditions, and\nindistinct target boundaries. Although visually image-like, GPR data\nfundamentally represent EM waves, with variations within and between waves\ncritical for identifying anomalies. Addressing these, we propose the\nReservoir-enhanced Segment Anything Model (Res-SAM), an innovative framework\nexploiting both visual discernibility and wave-changing properties of GPR data.\nRes-SAM initially identifies apparent candidate anomaly regions given minimal\nprompts, and further refines them by analyzing anomaly-induced changing\ninformation within and between EM waves in local GPR data, enabling precise and\ncomplete anomaly region extraction and category determination. Real-world\nexperiments demonstrate that Res-SAM achieves high detection accuracy (>85%)\nand outperforms state-of-the-art. Notably, Res-SAM requires only minimal\naccessible non-target data, avoids intensive training, and incorporates simple\nhuman interaction to enhance reliability. Our research provides a scalable,\nresource-efficient solution for rapid subsurface anomaly detection across\ndiverse environments, improving urban safety monitoring while reducing manual\neffort and computational cost.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRes-SAM\u7684\u521b\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5730\u4e0b\u5f02\u5e38\uff08\u5982\u88c2\u7f1d\u548c\u7a7a\u6d1e\uff09\u68c0\u6d4b\u7684\u6311\u6218\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u89c6\u89c9\u53ef\u8fa8\u8bc6\u6027\u548c\u7535\u78c1\u6ce2\u53d8\u5316\u7279\u6027\uff0c\u80fd\u591f\u5728\u6709\u9650\u7684\u6807\u8bb0\u6570\u636e\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u3002", "motivation": "\u57ce\u5e02\u9053\u8def\u548c\u57fa\u7840\u8bbe\u65bd\u9762\u4e34\u5730\u4e0b\u5f02\u5e38\u7684\u5a01\u80c1\uff0c\u800c\u73b0\u6709\u7684\u5730\u9762\u7a7f\u900f\u96f7\u8fbe\uff08GPR\uff09\u68c0\u6d4b\u65b9\u6cd5\u56e0\u6807\u8bb0\u6570\u636e\u4e0d\u8db3\u548c\u5730\u4e0b\u6761\u4ef6\u591a\u53d8\u800c\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51faRes-SAM\u6846\u67b6\uff0c\u9996\u5148\u901a\u8fc7\u5c11\u91cf\u63d0\u793a\u8bc6\u522b\u5019\u9009\u5f02\u5e38\u533a\u57df\uff0c\u7136\u540e\u5206\u6790\u7535\u78c1\u6ce2\u7684\u5c40\u90e8\u53d8\u5316\u4fe1\u606f\u8fdb\u884c\u7ec6\u5316\uff0c\u5b9e\u73b0\u7cbe\u786e\u68c0\u6d4b\u3002", "result": "\u5b9e\u9645\u5b9e\u9a8c\u8868\u660e\uff0cRes-SAM\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u8d85\u8fc785%\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e14\u53ea\u9700\u5c11\u91cf\u975e\u76ee\u6807\u6570\u636e\u548c\u7b80\u5355\u4eba\u5de5\u4ea4\u4e92\u3002", "conclusion": "Res-SAM\u4e3a\u57ce\u5e02\u5730\u4e0b\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u548c\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2504.19197", "pdf": "https://arxiv.org/pdf/2504.19197", "abs": "https://arxiv.org/abs/2504.19197", "authors": ["Sandipan Dhar", "Nanda Dulal Jana", "Swagatam Das"], "title": "Generative Adversarial Network based Voice Conversion: Techniques, Challenges, and Recent Advancements", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "19 pages, 12 figures, 1 table", "summary": "Voice conversion (VC) stands as a crucial research area in speech synthesis,\nenabling the transformation of a speaker's vocal characteristics to resemble\nanother while preserving the linguistic content. This technology has broad\napplications, including automated movie dubbing, speech-to-singing conversion,\nand assistive devices for pathological speech rehabilitation. With the\nincreasing demand for high-quality and natural-sounding synthetic voices,\nresearchers have developed a wide range of VC techniques. Among these,\ngenerative adversarial network (GAN)-based approaches have drawn considerable\nattention for their powerful feature-mapping capabilities and potential to\nproduce highly realistic speech. Despite notable advancements, challenges such\nas ensuring training stability, maintaining linguistic consistency, and\nachieving perceptual naturalness continue to hinder progress in GAN-based VC\nsystems. This systematic review presents a comprehensive analysis of the voice\nconversion landscape, highlighting key techniques, key challenges, and the\ntransformative impact of GANs in the field. The survey categorizes existing\nmethods, examines technical obstacles, and critically evaluates recent\ndevelopments in GAN-based VC. By consolidating and synthesizing research\nfindings scattered across the literature, this review provides a structured\nunderstanding of the strengths and limitations of different approaches. The\nsignificance of this survey lies in its ability to guide future research by\nidentifying existing gaps, proposing potential directions, and offering\ninsights for building more robust and efficient VC systems. Overall, this work\nserves as an essential resource for researchers, developers, and practitioners\naiming to advance the state-of-the-art (SOTA) in voice conversion technology.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7efc\u8ff0\u4e86\u8bed\u97f3\u8f6c\u6362\u6280\u672f\uff08VC\uff09\u7684\u73b0\u72b6\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u7684\u65b9\u6cd5\uff0c\u603b\u7ed3\u4e86\u5173\u952e\u6311\u6218\u548c\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u8bed\u97f3\u8f6c\u6362\u6280\u672f\u5728\u81ea\u52a8\u7535\u5f71\u914d\u97f3\u3001\u8bed\u97f3\u8f6c\u6b4c\u5531\u548c\u75c5\u7406\u8bed\u97f3\u5eb7\u590d\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u6280\u672f\u4ecd\u9762\u4e34\u8bad\u7ec3\u7a33\u5b9a\u6027\u3001\u8bed\u8a00\u4e00\u81f4\u6027\u548c\u611f\u77e5\u81ea\u7136\u6027\u7b49\u6311\u6218\uff0c\u9700\u8981\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u7efc\u8ff0\u6307\u5bfc\u672a\u6765\u7814\u7a76\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5206\u7c7b\u73b0\u6709\u65b9\u6cd5\u3001\u5206\u6790\u6280\u672f\u969c\u788d\u548c\u8bc4\u4f30GAN\u5728\u8bed\u97f3\u8f6c\u6362\u4e2d\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u7efc\u8ff0\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86GAN\u5728\u8bed\u97f3\u8f6c\u6362\u4e2d\u7684\u6f5c\u529b\uff0c\u4f46\u540c\u65f6\u4e5f\u6307\u51fa\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5982\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u81ea\u7136\u6027\u4e0d\u8db3\u3002", "conclusion": "\u8fd9\u7bc7\u7efc\u8ff0\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u7684\u77e5\u8bc6\u6846\u67b6\uff0c\u5e2e\u52a9\u8bc6\u522b\u73b0\u6709\u6280\u672f\u7f3a\u53e3\u5e76\u6307\u5bfc\u672a\u6765\u7814\u7a76\uff0c\u63a8\u52a8\u8bed\u97f3\u8f6c\u6362\u6280\u672f\u7684\u8fdb\u6b65\u3002"}}
{"id": "2504.19212", "pdf": "https://arxiv.org/pdf/2504.19212", "abs": "https://arxiv.org/abs/2504.19212", "authors": ["Tuan Nguyen", "Naseem Khan", "Issa Khalil"], "title": "CapsFake: A Multimodal Capsule Network for Detecting Instruction-Guided Deepfakes", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages", "summary": "The rapid evolution of deepfake technology, particularly in\ninstruction-guided image editing, threatens the integrity of digital images by\nenabling subtle, context-aware manipulations. Generated conditionally from real\nimages and textual prompts, these edits are often imperceptible to both humans\nand existing detection systems, revealing significant limitations in current\ndefenses. We propose a novel multimodal capsule network, CapsFake, designed to\ndetect such deepfake image edits by integrating low-level capsules from visual,\ntextual, and frequency-domain modalities. High-level capsules, predicted\nthrough a competitive routing mechanism, dynamically aggregate local features\nto identify manipulated regions with precision. Evaluated on diverse datasets,\nincluding MagicBrush, Unsplash Edits, Open Images Edits, and Multi-turn Edits,\nCapsFake outperforms state-of-the-art methods by up to 20% in detection\naccuracy. Ablation studies validate its robustness, achieving detection rates\nabove 94% under natural perturbations and 96% against adversarial attacks, with\nexcellent generalization to unseen editing scenarios. This approach establishes\na powerful framework for countering sophisticated image manipulations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u540d\u4e3aCapsFake\u7684\u591a\u6a21\u6001\u80f6\u56ca\u7f51\u7edc\uff0c\u7528\u4e8e\u68c0\u6d4b\u7531\u6307\u4ee4\u5f15\u5bfc\u7684\u6df1\u5ea6\u4f2a\u9020\u56fe\u50cf\u7f16\u8f91\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u534720%\uff0c\u6297\u5e72\u6270\u548c\u5bf9\u6297\u653b\u51fb\u80fd\u529b\u5f3a\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u5728\u6307\u4ee4\u5f15\u5bfc\u7684\u56fe\u50cf\u7f16\u8f91\u4e2d\u5feb\u901f\u53d1\u5c55\uff0c\u73b0\u6709\u7684\u68c0\u6d4b\u7cfb\u7edf\u96be\u4ee5\u5bdf\u89c9\u5176\u7ec6\u5fae\u4e14\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7be1\u6539\uff0c\u4e9f\u9700\u66f4\u5f3a\u5927\u7684\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u80f6\u56ca\u7f51\u7edc\uff08CapsFake\uff09\uff0c\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u3001\u6587\u672c\u548c\u9891\u57df\u6a21\u6001\u7684\u4f4e\u7ea7\u80f6\u56ca\uff0c\u5e76\u5229\u7528\u7ade\u4e89\u6027\u8def\u7531\u673a\u5236\u52a8\u6001\u805a\u5408\u5c40\u90e8\u7279\u5f81\uff0c\u7cbe\u51c6\u8bc6\u522b\u7be1\u6539\u533a\u57df\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cCapsFake\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u9ad8\u51fa20%\uff0c\u5728\u81ea\u7136\u6270\u52a8\u548c\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u68c0\u6d4b\u7387\u5206\u522b\u8d85\u8fc794%\u548c96%\uff0c\u4e14\u5bf9\u672a\u89c1\u8fc7\u7684\u7f16\u8f91\u573a\u666f\u6cdb\u5316\u80fd\u529b\u5f3a\u3002", "conclusion": "CapsFake\u4e3a\u5bf9\u6297\u590d\u6742\u56fe\u50cf\u7be1\u6539\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u6846\u67b6\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u68c0\u6d4b\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.18830", "pdf": "https://arxiv.org/pdf/2504.18830", "abs": "https://arxiv.org/abs/2504.18830", "authors": ["Fran\u00e7ois-Xavier Briol", "Alexandra Gessner", "Toni Karvonen", "Maren Mahsereci"], "title": "A Dictionary of Closed-Form Kernel Mean Embeddings", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA", "stat.CO"], "comment": null, "summary": "Kernel mean embeddings -- integrals of a kernel with respect to a probability\ndistribution -- are essential in Bayesian quadrature, but also widely used in\nother computational tools for numerical integration or for statistical\ninference based on the maximum mean discrepancy. These methods often require,\nor are enhanced by, the availability of a closed-form expression for the kernel\nmean embedding. However, deriving such expressions can be challenging, limiting\nthe applicability of kernel-based techniques when practitioners do not have\naccess to a closed-form embedding. This paper addresses this limitation by\nproviding a comprehensive dictionary of known kernel mean embeddings, along\nwith practical tools for deriving new embeddings from known ones. We also\nprovide a Python library that includes minimal implementations of the\nembeddings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u603b\u7ed3\u4e86\u6838\u5747\u503c\u5d4c\u5165\u7684\u5df2\u77e5\u516c\u5f0f\uff0c\u5e76\u63d0\u4f9b\u4e86\u5de5\u5177\u6765\u63a8\u5bfc\u65b0\u5d4c\u5165\uff0c\u540c\u65f6\u53d1\u5e03\u4e86\u76f8\u5173Python\u5e93\u3002", "motivation": "\u6838\u5747\u503c\u5d4c\u5165\u5728\u8d1d\u53f6\u65af\u79ef\u5206\u7b49\u8ba1\u7b97\u5de5\u5177\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u63a8\u5bfc\u5176\u95ed\u5f0f\u8868\u8fbe\u5f0f\u5177\u6311\u6218\u6027\uff0c\u9650\u5236\u4e86\u6838\u65b9\u6cd5\u7684\u9002\u7528\u8303\u56f4\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6574\u7406\u5df2\u77e5\u6838\u5747\u503c\u5d4c\u5165\uff0c\u5e76\u5f00\u53d1\u5de5\u5177\u652f\u6301\u4ece\u73b0\u6709\u5d4c\u5165\u63a8\u5bfc\u65b0\u5d4c\u5165\uff0c\u540c\u65f6\u5b9e\u73b0Python\u5e93\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u5957\u5b8c\u6574\u7684\u6838\u5747\u503c\u5d4c\u5165\u5b57\u5178\u53ca\u5b9e\u7528\u5de5\u5177\uff0c\u5e76\u5f00\u6e90\u4e86\u8f7b\u91cf\u7ea7Python\u5b9e\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u964d\u4f4e\u4e86\u6838\u65b9\u6cd5\u7684\u4f7f\u7528\u95e8\u69db\uff0c\u4e3a\u6570\u503c\u79ef\u5206\u548c\u7edf\u8ba1\u63a8\u65ad\u63d0\u4f9b\u4e86\u66f4\u4fbf\u6377\u7684\u5de5\u5177\u652f\u6301\u3002"}}
{"id": "2504.19223", "pdf": "https://arxiv.org/pdf/2504.19223", "abs": "https://arxiv.org/abs/2504.19223", "authors": ["Alexander Baumann", "Leonardo Ayala", "Silvia Seidlitz", "Jan Sellner", "Alexander Studier-Fischer", "Berkin \u00d6zdemir", "Lena Maier-Hein", "Slobodan Ilic"], "title": "CARL: Camera-Agnostic Representation Learning for Spectral Image Analysis", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Spectral imaging offers promising applications across diverse domains,\nincluding medicine and urban scene understanding, and is already established as\na critical modality in remote sensing. However, variability in channel\ndimensionality and captured wavelengths among spectral cameras impede the\ndevelopment of AI-driven methodologies, leading to camera-specific models with\nlimited generalizability and inadequate cross-camera applicability. To address\nthis bottleneck, we introduce $\\textbf{CARL}$, a model for\n$\\textbf{C}$amera-$\\textbf{A}$gnostic $\\textbf{R}$epresentation\n$\\textbf{L}$earning across RGB, multispectral, and hyperspectral imaging\nmodalities. To enable the conversion of a spectral image with any channel\ndimensionality to a camera-agnostic embedding, we introduce wavelength\npositional encoding and a self-attention-cross-attention mechanism to compress\nspectral information into learned query representations. Spectral-spatial\npre-training is achieved with a novel spectral self-supervised JEPA-inspired\nstrategy tailored to CARL. Large-scale experiments across the domains of\nmedical imaging, autonomous driving, and satellite imaging demonstrate our\nmodel's unique robustness to spectral heterogeneity, outperforming on datasets\nwith simulated and real-world cross-camera spectral variations. The scalability\nand versatility of the proposed approach position our model as a backbone for\nfuture spectral foundation models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCARL\u7684\u76f8\u673a\u65e0\u5173\u8868\u5f81\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u5149\u8c31\u6210\u50cf\u4e2d\u56e0\u76f8\u673a\u5dee\u5f02\u5bfc\u81f4\u7684AI\u6a21\u578b\u6cdb\u5316\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u6ce2\u957f\u4f4d\u7f6e\u7f16\u7801\u548c\u81ea\u6ce8\u610f\u529b-\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u8de8\u6a21\u6001\u5b66\u4e60\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u5149\u8c31\u6210\u50cf\u5728\u4e0d\u540c\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u7531\u4e8e\u76f8\u673a\u901a\u9053\u7ef4\u5ea6\u548c\u6ce2\u957f\u6355\u6349\u7684\u5dee\u5f02\uff0c\u73b0\u6709AI\u65b9\u6cd5\u6cdb\u5316\u6027\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u8de8\u76f8\u673a\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u9002\u5e94\u4e0d\u540c\u5149\u8c31\u6210\u50cf\u6a21\u6001\u7684\u901a\u7528\u8868\u5f81\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCARL\u6a21\u578b\uff0c\u91c7\u7528\u6ce2\u957f\u4f4d\u7f6e\u7f16\u7801\u548c\u81ea\u6ce8\u610f\u529b-\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5c06\u4efb\u610f\u901a\u9053\u7ef4\u5ea6\u7684\u5149\u8c31\u56fe\u50cf\u538b\u7f29\u4e3a\u76f8\u673a\u65e0\u5173\u7684\u5d4c\u5165\u8868\u793a\u3002\u901a\u8fc7\u65b0\u578b\u5149\u8c31\u81ea\u76d1\u7763JEPA\u7b56\u7565\u8fdb\u884c\u8c31\u7a7a\u95f4\u9884\u8bad\u7ec3\u3002", "result": "\u5728\u533b\u5b66\u6210\u50cf\u3001\u81ea\u52a8\u9a7e\u9a76\u548c\u536b\u661f\u6210\u50cf\u7b49\u9886\u57df\u7684\u5927\u89c4\u6a21\u5b9e\u9a8c\u4e2d\uff0cCARL\u8868\u73b0\u51fa\u5bf9\u5149\u8c31\u5f02\u8d28\u6027\u7684\u72ec\u7279\u9c81\u68d2\u6027\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u8de8\u76f8\u673a\u5149\u8c31\u53d8\u5316\u7684\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "CARL\u7684\u6269\u5c55\u6027\u548c\u591a\u529f\u80fd\u6027\u4f7f\u5176\u6210\u4e3a\u672a\u6765\u5149\u8c31\u57fa\u7840\u6a21\u578b\u7684\u9aa8\u5e72\uff0c\u4e3a\u8de8\u76f8\u673a\u548c\u8de8\u6a21\u6001\u7684\u5149\u8c31\u6210\u50cfAI\u5e94\u7528\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18860", "pdf": "https://arxiv.org/pdf/2504.18860", "abs": "https://arxiv.org/abs/2504.18860", "authors": ["Ken-Joel Simmoteit", "Philipp Schillinger", "Leonel Rozo"], "title": "Diffeomorphic Obstacle Avoidance for Contractive Dynamical Systems via Implicit Representations", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "comment": "Accepted at R:SS 2025", "summary": "Ensuring safety and robustness of robot skills is becoming crucial as robots\nare required to perform increasingly complex and dynamic tasks. The former is\nessential when performing tasks in cluttered environments, while the latter is\nrelevant to overcome unseen task situations. This paper addresses the challenge\nof ensuring both safety and robustness in dynamic robot skills learned from\ndemonstrations. Specifically, we build on neural contractive dynamical systems\nto provide robust extrapolation of the learned skills, while designing a\nfull-body obstacle avoidance strategy that preserves contraction stability via\ndiffeomorphic transforms. This is particularly crucial in complex environments\nwhere implicit scene representations, such as Signed Distance Fields (SDFs),\nare necessary. To this end, our framework called Signed Distance Field\nDiffeomorphic Transform, leverages SDFs and flow-based diffeomorphisms to\nachieve contraction-preserving obstacle avoidance. We thoroughly evaluate our\nframework on synthetic datasets and several real-world robotic tasks in a\nkitchen environment. Our results show that our approach locally adapts the\nlearned contractive vector field while staying close to the learned dynamics\nand without introducing highly-curved motion paths, thus outperforming several\nstate-of-the-art methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u795e\u7ecf\u6536\u7f29\u52a8\u529b\u7cfb\u7edf\u548c\u7b26\u53f7\u8ddd\u79bb\u573a\u5fae\u5206\u53d8\u6362\u7684\u6846\u67b6\uff0c\u65e8\u5728\u4fdd\u8bc1\u52a8\u6001\u673a\u5668\u4eba\u6280\u80fd\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u73af\u5883\u4e2d\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u6267\u884c\u4efb\u52a1\u7684\u590d\u6742\u6027\u548c\u52a8\u6001\u6027\u589e\u52a0\uff0c\u786e\u4fdd\u5176\u6280\u80fd\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u6742\u4e71\u73af\u5883\u548c\u672a\u77e5\u4efb\u52a1\u60c5\u5883\u4e0b\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u6536\u7f29\u52a8\u529b\u7cfb\u7edf\u8fdb\u884c\u6280\u80fd\u5916\u63a8\uff0c\u5e76\u7ed3\u5408\u7b26\u53f7\u8ddd\u79bb\u573a\uff08SDF\uff09\u5fae\u5206\u53d8\u6362\u8bbe\u8ba1\u5168\u969c\u788d\u7269\u907f\u969c\u7b56\u7565\uff0c\u4ee5\u4fdd\u6301\u6536\u7f29\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u53a8\u623f\u73af\u5883\u4efb\u52a1\u4e2d\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5c40\u90e8\u8c03\u6574\u5b66\u4e60\u7684\u6536\u7f29\u5411\u91cf\u573a\uff0c\u540c\u65f6\u907f\u514d\u9ad8\u66f2\u7387\u8fd0\u52a8\u8def\u5f84\uff0c\u4f18\u4e8e\u591a\u79cd\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u4e86\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u52a8\u6001\u673a\u5668\u4eba\u6280\u80fd\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18868", "pdf": "https://arxiv.org/pdf/2504.18868", "abs": "https://arxiv.org/abs/2504.18868", "authors": ["David Sychrovsk\u00fd", "Christopher Solinas", "Revan MacQueen", "Kevin Wang", "James R. Wright", "Nathan R. Sturtevant", "Michael Bowling"], "title": "Approximating Nash Equilibria in General-Sum Games via Meta-Learning", "categories": ["cs.GT", "cs.LG"], "comment": null, "summary": "Nash equilibrium is perhaps the best-known solution concept in game theory.\nSuch a solution assigns a strategy to each player which offers no incentive to\nunilaterally deviate. While a Nash equilibrium is guaranteed to always exist,\nthe problem of finding one in general-sum games is PPAD-complete, generally\nconsidered intractable. Regret minimization is an efficient framework for\napproximating Nash equilibria in two-player zero-sum games. However, in\ngeneral-sum games, such algorithms are only guaranteed to converge to a\ncoarse-correlated equilibrium (CCE), a solution concept where players can\ncorrelate their strategies. In this work, we use meta-learning to minimize the\ncorrelations in strategies produced by a regret minimizer. This encourages the\nregret minimizer to find strategies that are closer to a Nash equilibrium. The\nmeta-learned regret minimizer is still guaranteed to converge to a CCE, but we\ngive a bound on the distance to Nash equilibrium in terms of our meta-loss. We\nevaluate our approach in general-sum imperfect information games. Our\nalgorithms provide significantly better approximations of Nash equilibria than\nstate-of-the-art regret minimization techniques.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5143\u5b66\u4e60\u4f18\u5316\u9057\u61be\u6700\u5c0f\u5316\u7b97\u6cd5\uff0c\u51cf\u5c11\u7b56\u7565\u95f4\u7684\u76f8\u5173\u6027\u4ee5\u66f4\u63a5\u8fd1\u7eb3\u4ec0\u5747\u8861\uff0c\u5728\u5e7f\u4e49\u4e0d\u5b8c\u7f8e\u4fe1\u606f\u535a\u5f08\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u7eb3\u4ec0\u5747\u8861\u5728\u5e7f\u4e49\u535a\u5f08\u4e2d\u96be\u4ee5\u9ad8\u6548\u6c42\u89e3\uff0c\u73b0\u6709\u9057\u61be\u6700\u5c0f\u5316\u7b97\u6cd5\u53ea\u80fd\u6536\u655b\u81f3\u7c97\u76f8\u5173\u5747\u8861\uff08CCE\uff09\uff0c\u800cCCE\u5141\u8bb8\u7b56\u7565\u76f8\u5173\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5143\u5b66\u4e60\u964d\u4f4e\u76f8\u5173\u6027\uff0c\u4f7f\u7ed3\u679c\u66f4\u63a5\u8fd1\u7eb3\u4ec0\u5747\u8861\u3002", "method": "\u91c7\u7528\u5143\u5b66\u4e60\u6846\u67b6\u8c03\u6574\u9057\u61be\u6700\u5c0f\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u7b56\u7565\u76f8\u5173\u6027\u4f18\u5316\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u7559\u6536\u655b\u81f3CCE\u7684\u4fdd\u8bc1\u3002\u7406\u8bba\u5206\u6790\u4e2d\u7ed9\u51fa\u4e86\u5143\u635f\u5931\u4e0e\u7eb3\u4ec0\u5747\u8861\u8ddd\u79bb\u7684\u8fb9\u754c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5e7f\u4e49\u4e0d\u5b8c\u7f8e\u4fe1\u606f\u535a\u5f08\u4e2d\uff0c\u65b0\u7b97\u6cd5\u6bd4\u73b0\u6709\u9057\u61be\u6700\u5c0f\u5316\u6280\u672f\u80fd\u663e\u8457\u66f4\u597d\u5730\u903c\u8fd1\u7eb3\u4ec0\u5747\u8861\u3002", "conclusion": "\u5143\u5b66\u4e60\u53ef\u6709\u6548\u51cf\u5c11\u7b56\u7565\u76f8\u5173\u6027\uff0c\u63d0\u5347\u9057\u61be\u6700\u5c0f\u5316\u7b97\u6cd5\u5bf9\u7eb3\u4ec0\u5747\u8861\u7684\u903c\u8fd1\u6548\u679c\uff0c\u4e3a\u590d\u6742\u535a\u5f08\u6c42\u89e3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.19275", "pdf": "https://arxiv.org/pdf/2504.19275", "abs": "https://arxiv.org/abs/2504.19275", "authors": ["Yiren Xu"], "title": "Balancing Creativity and Automation: The Influence of AI on Modern Film Production and Dissemination", "categories": ["cs.CY", "cs.AI", "I.5.m"], "comment": "19 pages, 1 figures, 2 tables", "summary": "The integration of Artificial Intelligence(AI) into film production has\nrevolutionized efficiency and creativity, yet it simultaneously raises critical\nethical and practical challenges. This study explores the dual impact of AI on\nmodern cinema through three objectives: defining the optimal human-AI\nrelationship, balancing creativity with automation, and developing ethical\nguidelines. By employing a mixed-method approach combining theoretical\nframeworks (auteur theory, human-technology relations) and case studies (The\nSafe Zone, Fast & Furious 7, The Brutalist), the research reveals that\npositioning AI as an \"embodiment tool\" rather than an independent \"alterity\npartner\" preserves human authorship and artistic integrity. Key findings\nhighlight the risks of surveillance capitalism in AI-driven markets and the\nethical dilemmas of deepfake technology. The study concludes with actionable\nrecommendations, including international regulatory frameworks and a Human\nControl Index (HCI) to quantify AI involvement. These insights aim to guide\nfilmmakers, policymakers, and scholars in navigating the evolving AI-cinema\nlandscape while safeguarding cultural diversity and ethical standards.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8AI\u5bf9\u73b0\u4ee3\u7535\u5f71\u7684\u53cc\u91cd\u5f71\u54cd\uff0c\u63d0\u51fa\u4eba\u7c7b\u4e0eAI\u5e94\u4ee5AI\u4f5c\u4e3a\u5de5\u5177\u800c\u975e\u72ec\u7acb\u5408\u4f5c\u8005\u7684\u5173\u7cfb\uff0c\u5e76\u5f3a\u8c03\u4f26\u7406\u6311\u6218\u4e0e\u76d1\u7ba1\u5efa\u8bae\u3002", "motivation": "\u63a2\u7d22AI\u5728\u7535\u5f71\u5236\u4f5c\u4e2d\u7684\u9ad8\u6548\u4e0e\u521b\u610f\u4f5c\u7528\uff0c\u540c\u65f6\u89e3\u51b3\u5176\u5f15\u53d1\u7684\u4f26\u7406\u4e0e\u5b9e\u8df5\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u7406\u8bba\u6846\u67b6\uff08\u4f5c\u8005\u8bba\u3001\u4eba\u673a\u5173\u7cfb\uff09\u548c\u6848\u4f8b\u7814\u7a76\uff08\u5982\u300a\u5b89\u5168\u5730\u5e26\u300b\u300a\u901f\u5ea6\u4e0e\u6fc0\u60c57\u300b\uff09\u3002", "result": "\u5c06AI\u5b9a\u4f4d\u4e3a\u5de5\u5177\u80fd\u4fdd\u62a4\u4eba\u7c7b\u521b\u4f5c\u6743\uff1b\u63ed\u793aAI\u5e02\u573a\u7684\u76d1\u63a7\u8d44\u672c\u4e3b\u4e49\u98ce\u9669\u53ca\u6df1\u5ea6\u4f2a\u9020\u7684\u4f26\u7406\u56f0\u5883\u3002", "conclusion": "\u63d0\u51fa\u56fd\u9645\u76d1\u7ba1\u6846\u67b6\u548c\u4eba\u7c7b\u63a7\u5236\u6307\u6570\uff08HCI\uff09\uff0c\u4ee5\u5e73\u8861AI\u5e94\u7528\u4e0e\u6587\u5316\u4f26\u7406\u7684\u591a\u6837\u6027\u3002"}}
{"id": "2504.18897", "pdf": "https://arxiv.org/pdf/2504.18897", "abs": "https://arxiv.org/abs/2504.18897", "authors": ["Yuha Park", "Kunwoong Kim", "Insung Kong", "Yongdai Kim"], "title": "ReLU integral probability metric and its applications", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": "49 pages, 9 figures", "summary": "We propose a parametric integral probability metric (IPM) to measure the\ndiscrepancy between two probability measures. The proposed IPM leverages a\nspecific parametric family of discriminators, such as single-node neural\nnetworks with ReLU activation, to effectively distinguish between\ndistributions, making it applicable in high-dimensional settings. By optimizing\nover the parameters of the chosen discriminator class, the proposed IPM\ndemonstrates that its estimators have good convergence rates and can serve as a\nsurrogate for other IPMs that use smooth nonparametric discriminator classes.\nWe present an efficient algorithm for practical computation, offering a simple\nimplementation and requiring fewer hyperparameters. Furthermore, we explore its\napplications in various tasks, such as covariate balancing for causal inference\nand fair representation learning. Across such diverse applications, we\ndemonstrate that the proposed IPM provides strong theoretical guarantees, and\nempirical experiments show that it achieves comparable or even superior\nperformance to other methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u5316\u79ef\u5206\u6982\u7387\u5ea6\u91cf\uff08IPM\uff09\uff0c\u7528\u4e8e\u8861\u91cf\u4e24\u4e2a\u6982\u7387\u5206\u5e03\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u901a\u8fc7\u4f18\u5316\u7279\u5b9a\u53c2\u6570\u5316\u5224\u522b\u5668\uff08\u5982ReLU\u5355\u8282\u70b9\u795e\u7ecf\u7f51\u7edc\uff09\u5b9e\u73b0\u9ad8\u6548\u7684\u5206\u5e03\u5224\u522b\uff0c\u5e76\u5728\u9ad8\u7ef4\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002\u8be5\u65b9\u6cd5\u6536\u655b\u901f\u5ea6\u5feb\uff0c\u53ef\u4f5c\u4e3a\u5176\u4ed6IPM\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4e14\u7b97\u6cd5\u5b9e\u73b0\u7b80\u5355\u3001\u8d85\u53c2\u6570\u5c11\u3002\u5728\u56e0\u679c\u63a8\u65ad\u548c\u516c\u5e73\u8868\u793a\u5b66\u4e60\u7b49\u4efb\u52a1\u4e2d\uff0c\u8be5IPM\u8868\u73b0\u51fa\u826f\u597d\u7684\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684IPM\u65b9\u6cd5\u5728\u975e\u53c2\u6570\u5316\u5224\u522b\u5668\u4e0b\u53ef\u80fd\u8ba1\u7b97\u590d\u6742\u6216\u96be\u4ee5\u6269\u5c55\u81f3\u9ad8\u7ef4\u6570\u636e\uff0c\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u53c2\u6570\u5316IPM\uff0c\u517c\u5177\u9ad8\u6548\u6027\u548c\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "\u91c7\u7528\u53c2\u6570\u5316\u5224\u522b\u5668\uff08\u5982ReLU\u5355\u8282\u70b9\u795e\u7ecf\u7f51\u7edc\uff09\uff0c\u901a\u8fc7\u4f18\u5316\u5224\u522b\u5668\u53c2\u6570\u6765\u5ea6\u91cf\u5206\u5e03\u5dee\u5f02\uff0c\u5e76\u8bbe\u8ba1\u9ad8\u6548\u7b97\u6cd5\u5b9e\u73b0\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u5feb\u901f\u6536\u655b\u7387\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u6027\u80fd\u4f18\u4e8e\u6216\u5ab2\u7f8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u53c2\u6570\u5316IPM\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5206\u5e03\u5dee\u5f02\u5ea6\u91cf\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u6570\u636e\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2504.19323", "pdf": "https://arxiv.org/pdf/2504.19323", "abs": "https://arxiv.org/abs/2504.19323", "authors": ["Hanchen Yang", "Zishen Wan", "Ritik Raj", "Joongun Park", "Ziwei Li", "Ananda Samajdar", "Arijit Raychowdhury", "Tushar Krishna"], "title": "NSFlow: An End-to-End FPGA Framework with Scalable Dataflow Architecture for Neuro-Symbolic AI", "categories": ["cs.AR", "cs.AI", "cs.LG", "cs.PF"], "comment": null, "summary": "Neuro-Symbolic AI (NSAI) is an emerging paradigm that integrates neural\nnetworks with symbolic reasoning to enhance the transparency, reasoning\ncapabilities, and data efficiency of AI systems. Recent NSAI systems have\ngained traction due to their exceptional performance in reasoning tasks and\nhuman-AI collaborative scenarios. Despite these algorithmic advancements,\nexecuting NSAI tasks on existing hardware (e.g., CPUs, GPUs, TPUs) remains\nchallenging, due to their heterogeneous computing kernels, high memory\nintensity, and unique memory access patterns. Moreover, current NSAI algorithms\nexhibit significant variation in operation types and scales, making them\nincompatible with existing ML accelerators. These challenges highlight the need\nfor a versatile and flexible acceleration framework tailored to NSAI workloads.\nIn this paper, we propose NSFlow, an FPGA-based acceleration framework designed\nto achieve high efficiency, scalability, and versatility across NSAI systems.\nNSFlow features a design architecture generator that identifies workload data\ndependencies and creates optimized dataflow architectures, as well as a\nreconfigurable array with flexible compute units, re-organizable memory, and\nmixed-precision capabilities. Evaluating across NSAI workloads, NSFlow achieves\n31x speedup over Jetson TX2, more than 2x over GPU, 8x speedup over TPU-like\nsystolic array, and more than 3x over Xilinx DPU. NSFlow also demonstrates\nenhanced scalability, with only 4x runtime increase when symbolic workloads\nscale by 150x. To the best of our knowledge, NSFlow is the first framework to\nenable real-time generalizable NSAI algorithms acceleration, demonstrating a\npromising solution for next-generation cognitive systems.", "AI": {"tldr": "NSFlow\u662f\u4e00\u79cd\u57fa\u4e8eFPGA\u7684\u52a0\u901f\u6846\u67b6\uff0c\u4e13\u4e3aNeuro-Symbolic AI\u8bbe\u8ba1\uff0c\u901a\u8fc7\u4f18\u5316\u6570\u636e\u6d41\u67b6\u6784\u548c\u53ef\u91cd\u6784\u8ba1\u7b97\u5355\u5143\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u5728\u591a\u79cd\u786c\u4ef6\u4e0a\u5b9e\u73b0\u5927\u5e45\u52a0\u901f\u3002", "motivation": "\u5f53\u524d\u786c\u4ef6\u5bf9Neuro-Symbolic AI\u4efb\u52a1\u7684\u6267\u884c\u6548\u7387\u4e0d\u9ad8\uff0c\u56e0\u5176\u8ba1\u7b97\u5185\u6838\u5f02\u6784\u3001\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u72ec\u7279\u4e14\u8d1f\u8f7d\u7c7b\u578b\u591a\u6837\u3002NSFlow\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u63d0\u4f9b\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u52a0\u901f\u65b9\u6848\u3002", "method": "NSFlow\u5305\u542b\u4e00\u4e2a\u8bbe\u8ba1\u67b6\u6784\u751f\u6210\u5668\uff08\u8bc6\u522b\u6570\u636e\u4f9d\u8d56\u5e76\u4f18\u5316\u6570\u636e\u6d41\uff09\u548c\u4e00\u4e2a\u53ef\u91cd\u6784\u8ba1\u7b97\u9635\u5217\uff08\u652f\u6301\u7075\u6d3b\u8ba1\u7b97\u5355\u5143\u3001\u5185\u5b58\u91cd\u7ec4\u548c\u6df7\u5408\u7cbe\u5ea6\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cNSFlow\u6bd4Jetson TX2\u5feb31\u500d\uff0c\u6bd4GPU\u5feb2\u500d\uff0c\u6bd4TPU\u7c7b\u8109\u52a8\u9635\u5217\u5feb8\u500d\uff0c\u4e14\u7b26\u53f7\u8d1f\u8f7d\u6269\u5c55150\u500d\u65f6\u4ec5\u589e\u52a04\u500d\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "NSFlow\u9996\u6b21\u5b9e\u73b0\u4e86\u5b9e\u65f6\u901a\u7528\u7684Neuro-Symbolic AI\u52a0\u901f\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u8ba4\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2504.18911", "pdf": "https://arxiv.org/pdf/2504.18911", "abs": "https://arxiv.org/abs/2504.18911", "authors": ["Benedict Leimkuhler", "Ren\u00e9 Lohmann", "Peter Whalley"], "title": "A Langevin sampling algorithm inspired by the Adam optimizer", "categories": ["stat.CO", "cs.LG", "stat.ML", "60J22, 62-08, 82C31, 65C05, 65C30, 65C40"], "comment": null, "summary": "We present a framework for adaptive-stepsize MCMC sampling based on\ntime-rescaled Langevin dynamics, in which the stepsize variation is dynamically\ndriven by an additional degree of freedom. Our approach augments the phase\nspace by an additional variable which in turn defines a time\nreparameterization. The use of an auxiliary relaxation equation allows\naccumulation of a moving average of a local monitor function and provides for\nprecise control of the timestep while circumventing the need to modify the\ndrift term in the physical system. Our algorithm is straightforward to\nimplement and can be readily combined with any off-the-peg fixed-stepsize\nLangevin integrator. As a particular example, we consider control of the\nstepsize by monitoring the norm of the log-posterior gradient, which takes\ninspiration from the Adam optimizer, the stepsize being automatically reduced\nin regions of steep change of the log posterior and increased on plateaus,\nimproving numerical stability and convergence speed. As in Adam, the stepsize\nvariation depends on the recent history of the gradient norm, which enhances\nstability and improves accuracy compared to more immediate control approaches.\nWe demonstrate the potential benefit of this method--both in accuracy and in\nstability--in numerical experiments including Neal's funnel and a Bayesian\nneural network for classification of MNIST data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u95f4\u91cd\u6807\u5ea6\u6717\u4e4b\u4e07\u52a8\u529b\u5b66\u7684\u81ea\u9002\u5e94\u6b65\u957fMCMC\u91c7\u6837\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6b65\u957f\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027\u548c\u6536\u655b\u901f\u5ea6\uff0c\u9002\u7528\u4e8e\u590d\u6742\u540e\u9a8c\u5206\u5e03\u91c7\u6837\u3002", "motivation": "\u4e3a\u89e3\u51b3\u56fa\u5b9a\u6b65\u957fMCMC\u91c7\u6837\u5728\u590d\u6742\u540e\u9a8c\u5206\u5e03\u4e2d\u7684\u7a33\u5b9a\u6027\u4e0e\u6548\u7387\u95ee\u9898\uff0c\u63d0\u51fa\u52a8\u6001\u8c03\u6574\u6b65\u957f\u7684\u65b9\u6cd5\uff0c\u517c\u987e\u68af\u5ea6\u5267\u70c8\u53d8\u5316\u533a\u57df\u548c\u5e73\u7a33\u533a\u57df\u7684\u91c7\u6837\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u8f85\u52a9\u53d8\u91cf\u5b9e\u73b0\u65f6\u95f4\u91cd\u6807\u5ea6\uff0c\u5229\u7528\u677e\u5f1b\u65b9\u7a0b\u79ef\u7d2f\u5c40\u90e8\u76d1\u63a7\u51fd\u6570\u7684\u79fb\u52a8\u5e73\u5747\u503c\uff0c\u52a8\u6001\u63a7\u5236\u6b65\u957f\uff0c\u65e0\u9700\u4fee\u6539\u7269\u7406\u7cfb\u7edf\u7684\u6f02\u79fb\u9879\u3002\u7b97\u6cd5\u53ef\u517c\u5bb9\u73b0\u6709\u56fa\u5b9a\u6b65\u957f\u6717\u4e4b\u4e07\u79ef\u5206\u5668\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\uff08\u5982Neal\u6f0f\u6597\u548cMNIST\u6570\u636e\u5206\u7c7b\u7684\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\uff09\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u4e0a\u5747\u4f18\u4e8e\u76f4\u63a5\u63a7\u5236\u6b65\u957f\u7684\u65b9\u6848\u3002", "conclusion": "\u81ea\u9002\u5e94\u6b65\u957f\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86MCMC\u91c7\u6837\u7684\u6548\u7387\u4e0e\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9ad8\u7ef4\u6216\u975e\u89c4\u5219\u540e\u9a8c\u5206\u5e03\u573a\u666f\u3002"}}
{"id": "2504.19327", "pdf": "https://arxiv.org/pdf/2504.19327", "abs": "https://arxiv.org/abs/2504.19327", "authors": ["Moulik Choraria", "Xinbo Wu", "Akhil Bhimaraju", "Nitesh Sekhar", "Yue Wu", "Xu Zhang", "Prateek Singhal", "Lav R. Varshney"], "title": "Platonic Grounding for Efficient Multimodal Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The hyperscaling of data and parameter count in Transformer-based models is\nyielding diminishing performance improvement, especially when weighed against\ntraining costs. Such plateauing indicates the importance of methods for more\nefficient finetuning and inference, while retaining similar performance. This\nis especially relevant for multimodal learning paradigms, where inference costs\nof processing multimodal tokens can determine the model's practical viability.\nAt the same time, research on representations and mechanistic interpretability\nhas improved our understanding of the inner workings of Transformer-based\nmodels; one such line of work reveals an implicit alignment in the deeper\nlayers of pretrained models, across modalities. Taking inspiration from this,\nwe motivate and propose a simple modification to existing multimodal frameworks\nthat rely on aligning pretrained models. We demonstrate that our approach\nmaintains and, in some cases, even improves performance of baseline methods\nwhile achieving significant gains in both training and inference-time compute.\nOur work also has implications for combining pretrained models into larger\nsystems efficiently.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u591a\u6a21\u6001\u6846\u67b6\u7684\u9ad8\u6548\u5fae\u8c03\u548c\u63a8\u7406\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u6df1\u5c42\u5bf9\u9f50\u7279\u6027\uff0c\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u968f\u7740Transformer\u6a21\u578b\u6570\u636e\u548c\u53c2\u6570\u89c4\u6a21\u7684\u6269\u5927\uff0c\u6027\u80fd\u63d0\u5347\u9010\u6e10\u51cf\u5f31\u4e14\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\uff0c\u5c24\u5176\u662f\u5728\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\uff0c\u63a8\u7406\u6210\u672c\u76f4\u63a5\u5f71\u54cd\u6a21\u578b\u5b9e\u7528\u6027\u3002", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u6df1\u5c42\u8de8\u6a21\u6001\u9690\u5f0f\u5bf9\u9f50\u7684\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u591a\u6a21\u6001\u6846\u67b6\u6539\u8fdb\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6216\u63d0\u5347\u57fa\u7ebf\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u7684\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e0d\u4ec5\u4f18\u5316\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u6548\u7387\uff0c\u8fd8\u4e3a\u9ad8\u6548\u6574\u5408\u9884\u8bad\u7ec3\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.18917", "pdf": "https://arxiv.org/pdf/2504.18917", "abs": "https://arxiv.org/abs/2504.18917", "authors": ["David Sychrovsk\u00fd", "Martin Schmid", "Michal \u0160ustr", "Michael Bowling"], "title": "Meta-Learning in Self-Play Regret Minimization", "categories": ["cs.GT", "cs.LG"], "comment": null, "summary": "Regret minimization is a general approach to online optimization which plays\na crucial role in many algorithms for approximating Nash equilibria in\ntwo-player zero-sum games. The literature mainly focuses on solving individual\ngames in isolation. However, in practice, players often encounter a\ndistribution of similar but distinct games. For example, when trading\ncorrelated assets on the stock market, or when refining the strategy in\nsubgames of a much larger game. Recently, offline meta-learning was used to\naccelerate one-sided equilibrium finding on such distributions. We build upon\nthis, extending the framework to the more challenging self-play setting, which\nis the basis for most state-of-the-art equilibrium approximation algorithms for\ndomains at scale. When selecting the strategy, our method uniquely integrates\ninformation across all decision states, promoting global communication as\nopposed to the traditional local regret decomposition. Empirical evaluation on\nnormal-form games and river poker subgames shows our meta-learned algorithms\nconsiderably outperform other state-of-the-art regret minimization algorithms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u7684\u81ea\u5bf9\u5f08\u5143\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a0\u901f\u5728\u7c7b\u4f3c\u6e38\u620f\u5206\u5e03\u4e0a\u7684\u5747\u8861\u6c42\u89e3\uff0c\u901a\u8fc7\u5728\u7b56\u7565\u9009\u62e9\u65f6\u5168\u5c40\u6574\u5408\u4fe1\u606f\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u9057\u61be\u6700\u5c0f\u5316\u7b97\u6cd5\u3002", "motivation": "\u5b9e\u8df5\u4e2d\u73a9\u5bb6\u5e38\u9762\u4e34\u76f8\u4f3c\u4f46\u4e0d\u540c\u7684\u6e38\u620f\u5206\u5e03\uff08\u5982\u80a1\u7968\u4ea4\u6613\u6216\u5b50\u6e38\u620f\u7b56\u7565\u7ec6\u5316\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u4e13\u6ce8\u4e8e\u5355\u72ec\u6c42\u89e3\u5355\u4e2a\u6e38\u620f\uff0c\u800c\u79bb\u7ebf\u5143\u5b66\u4e60\u53ef\u52a0\u901f\u6b64\u7c7b\u5206\u5e03\u7684\u5747\u8861\u6c42\u89e3\u3002", "method": "\u6269\u5c55\u81ea\u5bf9\u5f08\u6846\u67b6\uff0c\u5168\u5c40\u6574\u5408\u6240\u6709\u51b3\u7b56\u72b6\u6001\u4fe1\u606f\uff0c\u800c\u975e\u4f20\u7edf\u5c40\u90e8\u9057\u61be\u5206\u89e3\uff0c\u5e94\u7528\u4e8e\u6b63\u6001\u535a\u5f08\u548c\u6cb3\u6d41\u6251\u514b\u5b50\u6e38\u620f\u3002", "result": "\u5728\u6b63\u6001\u535a\u5f08\u548c\u6cb3\u6d41\u6251\u514b\u5b50\u6e38\u620f\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u9057\u61be\u6700\u5c0f\u5316\u7b97\u6cd5\u3002", "conclusion": "\u5168\u5c40\u4fe1\u606f\u6574\u5408\u7684\u5143\u5b66\u4e60\u65b9\u6cd5\u5728\u81ea\u5bf9\u5f08\u573a\u666f\u4e2d\u9ad8\u6548\uff0c\u4e3a\u5927\u89c4\u6a21\u9886\u57df\u5747\u8861\u8fd1\u4f3c\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.18950", "pdf": "https://arxiv.org/pdf/2504.18950", "abs": "https://arxiv.org/abs/2504.18950", "authors": ["Erfan Loweimi", "Mengjie Qian", "Kate Knill", "Mark Gales"], "title": "Speaker Retrieval in the Wild: Challenges, Effectiveness and Robustness", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "comment": "13 pages, 10 figures, 10 tables, 76 references", "summary": "There is a growing abundance of publicly available or company-owned\naudio/video archives, highlighting the increasing importance of efficient\naccess to desired content and information retrieval from these archives. This\npaper investigates the challenges, solutions, effectiveness, and robustness of\nspeaker retrieval systems developed \"in the wild\" which involves addressing two\nprimary challenges: extraction of task-relevant labels from limited metadata\nfor system development and evaluation, as well as the unconstrained acoustic\nconditions encountered in the archive, ranging from quiet studios to adverse\nnoisy environments. While we focus on the publicly-available BBC Rewind archive\n(spanning 1948 to 1979), our framework addresses the broader issue of speaker\nretrieval on extensive and possibly aged archives with no control over the\ncontent and acoustic conditions. Typically, these archives offer a brief and\ngeneral file description, mostly inadequate for specific applications like\nspeaker retrieval, and manual annotation of such large-scale archives is\nunfeasible. We explore various aspects of system development (e.g., speaker\ndiarisation, embedding extraction, query selection) and analyse the challenges,\npossible solutions, and their functionality. To evaluate the performance, we\nconduct systematic experiments in both clean setup and against various\ndistortions simulating real-world applications. Our findings demonstrate the\neffectiveness and robustness of the developed speaker retrieval systems,\nestablishing the versatility and scalability of the proposed framework for a\nwide range of applications beyond the BBC Rewind corpus.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u975e\u53d7\u63a7\u73af\u5883\u4e0b\u5f00\u53d1\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u8bf4\u8bdd\u4eba\u68c0\u7d22\u7cfb\u7edf\u7684\u6311\u6218\u4e0e\u65b9\u6cd5\uff0c\u5229\u7528BBC Rewind\u6863\u6848\u4f5c\u4e3a\u6848\u4f8b\uff0c\u5c55\u793a\u4e86\u5176\u6846\u67b6\u7684\u901a\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u968f\u7740\u97f3\u89c6\u9891\u6863\u6848\u7684\u6fc0\u589e\uff0c\u5982\u4f55\u9ad8\u6548\u68c0\u7d22\u7279\u5b9a\u5185\u5bb9\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002\u73b0\u6709\u6863\u6848\u7684\u5143\u6570\u636e\u5f80\u5f80\u4e0d\u8db3\uff0c\u4e14\u624b\u52a8\u6807\u6ce8\u5927\u89c4\u6a21\u6863\u6848\u4e0d\u5207\u5b9e\u9645\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u81ea\u52a8\u5316\u4e14\u9c81\u68d2\u7684\u8bf4\u8bdd\u4eba\u68c0\u7d22\u7cfb\u7edf\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u6db5\u76d6\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u3001\u5d4c\u5165\u63d0\u53d6\u548c\u67e5\u8be2\u9009\u62e9\u7b49\u6280\u672f\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\uff08BBC Rewind\u6863\u6848\uff09\u548c\u6a21\u62df\u626d\u66f2\u73af\u5883\u4e0b\u8fdb\u884c\u7cfb\u7edf\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6240\u5f00\u53d1\u7cfb\u7edf\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5728\u65e0\u63a7\u5236\u5185\u5bb9\u548c\u58f0\u5b66\u6761\u4ef6\u4e0b\u4f9d\u7136\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u6846\u67b6\u5177\u6709\u901a\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8eBBC Rewind\u6863\u6848\u4ee5\u5916\u7684\u591a\u79cd\u5e94\u7528\u573a\u666f\uff0c\u4e3a\u5927\u89c4\u6a21\u6863\u6848\u7684\u8bf4\u8bdd\u4eba\u68c0\u7d22\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2504.19341", "pdf": "https://arxiv.org/pdf/2504.19341", "abs": "https://arxiv.org/abs/2504.19341", "authors": ["Jialiang Zhao", "Naveen Kuppuswamy", "Siyuan Feng", "Benjamin Burchfiel", "Edward Adelson"], "title": "PolyTouch: A Robust Multi-Modal Tactile Sensor for Contact-rich Manipulation Using Tactile-Diffusion Policies", "categories": ["cs.RO", "cs.AI"], "comment": "Nominated for the best paper award at ICRA 2025", "summary": "Achieving robust dexterous manipulation in unstructured domestic environments\nremains a significant challenge in robotics. Even with state-of-the-art robot\nlearning methods, haptic-oblivious control strategies (i.e. those relying only\non external vision and/or proprioception) often fall short due to occlusions,\nvisual complexities, and the need for precise contact interaction control. To\naddress these limitations, we introduce PolyTouch, a novel robot finger that\nintegrates camera-based tactile sensing, acoustic sensing, and peripheral\nvisual sensing into a single design that is compact and durable. PolyTouch\nprovides high-resolution tactile feedback across multiple temporal scales,\nwhich is essential for efficiently learning complex manipulation tasks.\nExperiments demonstrate an at least 20-fold increase in lifespan over\ncommercial tactile sensors, with a design that is both easy to manufacture and\nscalable. We then use this multi-modal tactile feedback along with\nvisuo-proprioceptive observations to synthesize a tactile-diffusion policy from\nhuman demonstrations; the resulting contact-aware control policy significantly\noutperforms haptic-oblivious policies in multiple contact-aware manipulation\npolicies. This paper highlights how effectively integrating multi-modal contact\nsensing can hasten the development of effective contact-aware manipulation\npolicies, paving the way for more reliable and versatile domestic robots. More\ninformation can be found at https://polytouch.alanz.info/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u673a\u5668\u4eba\u624b\u6307PolyTouch\uff0c\u96c6\u6210\u4e86\u89e6\u89c9\u3001\u542c\u89c9\u548c\u89c6\u89c9\u4f20\u611f\uff0c\u663e\u8457\u63d0\u5347\u63a5\u89e6\u611f\u77e5\u64cd\u63a7\u7b56\u7565\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u975e\u7ed3\u6784\u5316\u5bb6\u5ead\u73af\u5883\u4e2d\u673a\u5668\u4eba\u64cd\u63a7\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u548c\u672c\u4f53\u611f\u77e5\u7684\u65b9\u6cd5\u56e0\u906e\u6321\u548c\u63a5\u89e6\u63a7\u5236\u4e0d\u8db3\u800c\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u8bbe\u8ba1\u4e86PolyTouch\u624b\u6307\uff0c\u7ed3\u5408\u89e6\u89c9\u3001\u542c\u89c9\u548c\u89c6\u89c9\u4f20\u611f\uff0c\u5e76\u5229\u7528\u591a\u6a21\u6001\u53cd\u9988\u8bad\u7ec3\u89e6\u89c9\u6269\u6563\u7b56\u7565\u3002", "result": "PolyTouch\u5bff\u547d\u63d0\u534720\u500d\u4ee5\u4e0a\uff0c\u89e6\u89c9\u611f\u77e5\u7b56\u7565\u663e\u8457\u4f18\u4e8e\u65e0\u89e6\u89c9\u65b9\u6cd5\u3002", "conclusion": "\u591a\u6a21\u6001\u63a5\u89e6\u4f20\u611f\u6709\u6548\u52a0\u901f\u4e86\u63a5\u89e6\u611f\u77e5\u64cd\u63a7\u7b56\u7565\u7684\u53d1\u5c55\uff0c\u4e3a\u66f4\u53ef\u9760\u7684\u5bb6\u5ead\u673a\u5668\u4eba\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2504.18958", "pdf": "https://arxiv.org/pdf/2504.18958", "abs": "https://arxiv.org/abs/2504.18958", "authors": ["Masoud Ataei"], "title": "Modeling Regime Structure and Informational Drivers of Stock Market Volatility via the Financial Chaos Index", "categories": ["q-fin.ST", "cs.LG"], "comment": null, "summary": "This paper investigates the structural dynamics of stock market volatility\nthrough the Financial Chaos Index, a tensor- and eigenvalue-based measure\ndesigned to capture realized volatility via mutual fluctuations among asset\nprices. Motivated by empirical evidence of regime-dependent volatility behavior\nand perceptual time dilation during financial crises, we develop a\nregime-switching framework based on the Modified Lognormal Power-Law\ndistribution. Analysis of the FCIX from January 1990 to December 2023\nidentifies three distinct market regimes, low-chaos, intermediate-chaos, and\nhigh-chaos, each characterized by differing levels of systemic stress,\nstatistical dispersion and persistence characteristics. Building upon the\nsegmented regime structure, we further examine the informational forces that\nshape forward-looking market expectations. Using sentiment-based predictors\nderived from the Equity Market Volatility tracker, we employ an elastic net\nregression model to forecast implied volatility, as proxied by the VIX index.\nOur findings indicate that shifts in macroeconomic, financial, policy, and\ngeopolitical uncertainty exhibit strong predictive power for volatility\ndynamics across regimes. Together, these results offer a unified empirical\nperspective on how systemic uncertainty governs both the realized evolution of\nfinancial markets and the anticipatory behavior embedded in implied volatility\nmeasures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u91d1\u878d\u6df7\u6c8c\u6307\u6570\uff08FCIX\uff09\u7814\u7a76\u4e86\u80a1\u7968\u5e02\u573a\u6ce2\u52a8\u7684\u7ed3\u6784\u52a8\u6001\uff0c\u4f7f\u7528\u57fa\u4e8e\u5f20\u91cf\u548c\u7279\u5f81\u503c\u7684\u65b9\u6cd5\u6355\u6349\u8d44\u4ea7\u4ef7\u683c\u7684\u76f8\u4e92\u6ce2\u52a8\u3002\u7814\u7a76\u53d1\u73b0\u4e09\u79cd\u4e0d\u540c\u7684\u5e02\u573a\u72b6\u6001\uff08\u4f4e\u6df7\u6c8c\u3001\u4e2d\u6df7\u6c8c\u548c\u9ad8\u6df7\u6c8c\uff09\uff0c\u5e76\u901a\u8fc7\u5f39\u6027\u7f51\u7edc\u56de\u5f52\u6a21\u578b\u9884\u6d4b\u4e86VIX\u6307\u6570\u7684\u9690\u542b\u6ce2\u52a8\u7387\uff0c\u8868\u660e\u5b8f\u89c2\u7ecf\u6d4e\u3001\u91d1\u878d\u3001\u653f\u7b56\u548c\u5730\u7f18\u653f\u6cbb\u4e0d\u786e\u5b9a\u6027\u5bf9\u6ce2\u52a8\u7387\u52a8\u6001\u6709\u5f3a\u9884\u6d4b\u529b\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u91d1\u878d\u5371\u673a\u7684\u7ecf\u9a8c\u8bc1\u636e\uff0c\u663e\u793a\u6ce2\u52a8\u884c\u4e3a\u5177\u6709\u72b6\u6001\u4f9d\u8d56\u6027\uff0c\u4e14\u5728\u5371\u673a\u671f\u95f4\u5b58\u5728\u611f\u77e5\u65f6\u95f4\u6269\u5f20\u73b0\u8c61\u3002\u4e3a\u7406\u89e3\u8fd9\u4e00\u73b0\u8c61\u5e76\u6355\u6349\u7cfb\u7edf\u6027\u4e0d\u786e\u5b9a\u6027\u5bf9\u5e02\u573a\u7684\u5f71\u54cd\uff0c\u4f5c\u8005\u5f00\u53d1\u4e86\u57fa\u4e8e\u4fee\u6b63\u5bf9\u6570\u6b63\u6001\u5e42\u5f8b\u5206\u5e03\u7684\u72b6\u6001\u8f6c\u6362\u6846\u67b6\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a\uff081\uff09\u4f7f\u7528\u91d1\u878d\u6df7\u6c8c\u6307\u6570\uff08FCIX\uff09\u6d4b\u91cf\u5df2\u5b9e\u73b0\u6ce2\u52a8\u7387\uff1b\uff082\uff09\u57fa\u4e8e\u4fee\u6b63\u5bf9\u6570\u6b63\u6001\u5e42\u5f8b\u5206\u5e03\u7684\u72b6\u6001\u8f6c\u6362\u6846\u67b6\u5212\u5206\u5e02\u573a\u72b6\u6001\uff1b\uff083\uff09\u5229\u7528\u5f39\u6027\u7f51\u7edc\u56de\u5f52\u6a21\u578b\u5206\u6790\u60c5\u7eea\u9884\u6d4b\u56e0\u5b50\u5bf9VIX\u6307\u6570\u7684\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u7ed3\u679c\u663e\u793a\u4e09\u79cd\u5e02\u573a\u72b6\u6001\uff08\u4f4e\u3001\u4e2d\u3001\u9ad8\u6df7\u6c8c\uff09\uff0c\u6bcf\u79cd\u72b6\u6001\u7684\u7cfb\u7edf\u6027\u538b\u529b\u3001\u7edf\u8ba1\u5206\u6563\u6027\u548c\u6301\u7eed\u6027\u7279\u5f81\u4e0d\u540c\u3002\u60c5\u7eea\u9884\u6d4b\u56e0\u5b50\uff08\u5982\u5b8f\u89c2\u7ecf\u6d4e\u548c\u653f\u7b56\u4e0d\u786e\u5b9a\u6027\uff09\u5bf9\u9690\u542b\u6ce2\u52a8\u7387\uff08VIX\uff09\u7684\u9884\u6d4b\u80fd\u529b\u663e\u8457\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u8868\u660e\u7cfb\u7edf\u6027\u4e0d\u786e\u5b9a\u6027\u4e0d\u4ec5\u5f71\u54cd\u91d1\u878d\u5e02\u573a\u7684\u5df2\u5b9e\u73b0\u6ce2\u52a8\uff0c\u8fd8\u901a\u8fc7\u9690\u542b\u6ce2\u52a8\u7387\u53cd\u6620\u5e02\u573a\u7684\u524d\u77bb\u6027\u884c\u4e3a\uff0c\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u5b9e\u8bc1\u89c6\u89d2\u3002"}}
{"id": "2504.18989", "pdf": "https://arxiv.org/pdf/2504.18989", "abs": "https://arxiv.org/abs/2504.18989", "authors": ["Gal Almog", "Ariel Shamir", "Ohad Fried"], "title": "REED-VAE: RE-Encode Decode Training for Iterative Image Editing with Diffusion Models", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted to Eurographics 2025. Project page:\n  https://reed-vae.github.io/", "summary": "While latent diffusion models achieve impressive image editing results, their\napplication to iterative editing of the same image is severely restricted. When\ntrying to apply consecutive edit operations using current models, they\naccumulate artifacts and noise due to repeated transitions between pixel and\nlatent spaces. Some methods have attempted to address this limitation by\nperforming the entire edit chain within the latent space, sacrificing\nflexibility by supporting only a limited, predetermined set of diffusion\nediting operations. We present a RE-encode decode (REED) training scheme for\nvariational autoencoders (VAEs), which promotes image quality preservation even\nafter many iterations. Our work enables multi-method iterative image editing:\nusers can perform a variety of iterative edit operations, with each operation\nbuilding on the output of the previous one using both diffusion-based\noperations and conventional editing techniques. We demonstrate the advantage of\nREED-VAE across a range of image editing scenarios, including text-based and\nmask-based editing frameworks. In addition, we show how REED-VAE enhances the\noverall editability of images, increasing the likelihood of successful and\nprecise edit operations. We hope that this work will serve as a benchmark for\nthe newly introduced task of multi-method image editing. Our code and models\nwill be available at https://github.com/galmog/REED-VAE", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86REED-VAE\u8bad\u7ec3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u9690\u6269\u6563\u6a21\u578b\u5728\u8fed\u4ee3\u7f16\u8f91\u540c\u4e00\u56fe\u50cf\u65f6\u7684\u566a\u58f0\u548c\u4f2a\u5f71\u7d2f\u79ef\u95ee\u9898\uff0c\u652f\u6301\u591a\u79cd\u7f16\u8f91\u65b9\u6cd5\u7684\u8fed\u4ee3\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709\u9690\u6269\u6563\u6a21\u578b\u5728\u8fed\u4ee3\u7f16\u8f91\u540c\u4e00\u56fe\u50cf\u65f6\uff0c\u7531\u4e8e\u50cf\u7d20\u548c\u6f5c\u5728\u7a7a\u95f4\u4e4b\u95f4\u7684\u91cd\u590d\u8f6c\u6362\u4f1a\u5bfc\u81f4\u566a\u58f0\u548c\u4f2a\u5f71\u7d2f\u79ef\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdRE-encode decode (REED)\u8bad\u7ec3\u65b9\u6848\uff0c\u4f18\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668(VAE)\uff0c\u4ee5\u5728\u591a\u6b21\u8fed\u4ee3\u540e\u4ecd\u80fd\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "result": "REED-VAE\u5728\u591a\u79cd\u56fe\u50cf\u7f16\u8f91\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5305\u62ec\u57fa\u4e8e\u6587\u672c\u548c\u63a9\u7801\u7684\u7f16\u8f91\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f16\u8f91\u7684\u6210\u529f\u7387\u548c\u7cbe\u786e\u5ea6\u3002", "conclusion": "REED-VAE\u4e3a\u591a\u65b9\u6cd5\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\uff0c\u5e76\u6709\u671b\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.19362", "pdf": "https://arxiv.org/pdf/2504.19362", "abs": "https://arxiv.org/abs/2504.19362", "authors": ["Yunxuan Wang", "Ray Yin", "Yumei Tan", "Hao Chen", "Haiying Xia"], "title": "Low-Rank Adaptive Structural Priors for Generalizable Diabetic Retinopathy Grading", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Accepted by IJCNN 2025", "summary": "Diabetic retinopathy (DR), a serious ocular complication of diabetes, is one\nof the primary causes of vision loss among retinal vascular diseases. Deep\nlearning methods have been extensively applied in the grading of diabetic\nretinopathy (DR). However, their performance declines significantly when\napplied to data outside the training distribution due to domain shifts. Domain\ngeneralization (DG) has emerged as a solution to this challenge. However, most\nexisting DG methods overlook lesion-specific features, resulting in\ninsufficient accuracy. In this paper, we propose a novel approach that enhances\nexisting DG methods by incorporating structural priors, inspired by the\nobservation that DR grading is heavily dependent on vessel and lesion\nstructures. We introduce Low-rank Adaptive Structural Priors (LoASP), a\nplug-and-play framework designed for seamless integration with existing DG\nmodels. LoASP improves generalization by learning adaptive structural\nrepresentations that are finely tuned to the complexities of DR diagnosis.\nExtensive experiments on eight diverse datasets validate its effectiveness in\nboth single-source and multi-source domain scenarios. Furthermore,\nvisualizations reveal that the learned structural priors intuitively align with\nthe intricate architecture of the vessels and lesions, providing compelling\ninsights into their interpretability and diagnostic relevance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLoASP\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u6027\u5148\u9a8c\u589e\u5f3a\u73b0\u6709\u57df\u6cdb\u5316\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u5206\u7ea7\u4e2d\u7684\u57df\u504f\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u4e4b\u5916\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u662f\u5bfc\u81f4\u89c6\u529b\u4e27\u5931\u7684\u4e3b\u8981\u539f\u56e0\u4e4b\u4e00\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u57df\u504f\u79fb\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u5927\u591a\u6570\u57df\u6cdb\u5316\u65b9\u6cd5\u5ffd\u7565\u75c5\u7076\u7279\u5b9a\u7279\u5f81\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u6784\u6027\u5148\u9a8c\u6539\u8fdb\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86LoASP\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u4e0eDR\u8bca\u65ad\u590d\u6742\u6027\u76f8\u9002\u5e94\u7684\u7ed3\u6784\u6027\u8868\u5f81\uff0c\u589e\u5f3a\u73b0\u6709\u57df\u6cdb\u5316\u6a21\u578b\uff0c\u91cd\u70b9\u5173\u6ce8\u8840\u7ba1\u548c\u75c5\u7076\u7ed3\u6784\u3002", "result": "\u5728\u516b\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86LoASP\u5728\u5355\u6e90\u548c\u591a\u6e90\u57df\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7ed3\u6784\u5148\u9a8c\u53ef\u89c6\u5316\u4e5f\u76f4\u89c2\u5c55\u793a\u4e86\u5176\u4e0e\u8840\u7ba1\u548c\u75c5\u7076\u7ed3\u6784\u7684\u5bf9\u5e94\u5173\u7cfb\u3002", "conclusion": "LoASP\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u6027\u5148\u9a8c\u663e\u8457\u63d0\u5347\u4e86DR\u5206\u7ea7\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2504.19007", "pdf": "https://arxiv.org/pdf/2504.19007", "abs": "https://arxiv.org/abs/2504.19007", "authors": ["Jinghao Lyu", "Kyle J. Ray", "James P. Crutchfield"], "title": "Learning Stochastic Thermodynamics Directly from Correlation and Trajectory-Fluctuation Currents", "categories": ["cond-mat.stat-mech", "cond-mat.dis-nn", "cs.LG", "nlin.AO", "stat.ML"], "comment": "11 pages, 6 appendices (10 pages), 4 figures;\n  https://csc.ucdavis.edu/~cmg/compmech/pubs/currents.htm", "summary": "Markedly increased computational power and data acquisition have led to\ngrowing interest in data-driven inverse dynamics problems. These seek to answer\na fundamental question: What can we learn from time series measurements of a\ncomplex dynamical system? For small systems interacting with external\nenvironments, the effective dynamics are inherently stochastic, making it\ncrucial to properly manage noise in data. Here, we explore this for systems\nobeying Langevin dynamics and, using currents, we construct a learning\nframework for stochastic modeling. Currents have recently gained increased\nattention for their role in bounding entropy production (EP) from thermodynamic\nuncertainty relations (TURs). We introduce a fundamental relationship between\nthe cumulant currents there and standard machine-learning loss functions. Using\nthis, we derive loss functions for several key thermodynamic functions directly\nfrom the system dynamics without the (common) intermediate step of deriving a\nTUR. These loss functions reproduce results derived both from TURs and other\nmethods. More significantly, they open a path to discover new loss functions\nfor previously inaccessible quantities. Notably, this includes access to\nper-trajectory entropy production, even if the observed system is driven far\nfrom its steady-state. We also consider higher order estimation. Our method is\nstraightforward and unifies dynamic inference with recent approaches to entropy\nproduction estimation. Taken altogether, this reveals a deep connection between\ndiffusion models in machine learning and entropy production estimation in\nstochastic thermodynamics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6717\u4e4b\u4e07\u52a8\u529b\u5b66\u7684\u6570\u636e\u9a71\u52a8\u9006\u52a8\u529b\u5b66\u95ee\u9898\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u7535\u6d41\u6784\u5efa\u4e86\u968f\u673a\u5efa\u6a21\u7684\u635f\u5931\u51fd\u6570\uff0c\u5e76\u63ed\u793a\u4e86\u673a\u5668\u5b66\u4e60\u6269\u6563\u6a21\u578b\u4e0e\u968f\u673a\u70ed\u529b\u5b66\u71b5\u4ea7\u751f\u4f30\u8ba1\u4e4b\u95f4\u7684\u6df1\u523b\u8054\u7cfb\u3002", "motivation": "\u968f\u7740\u8ba1\u7b97\u80fd\u529b\u548c\u6570\u636e\u83b7\u53d6\u80fd\u529b\u7684\u663e\u8457\u63d0\u5347\uff0c\u6570\u636e\u9a71\u52a8\u7684\u9006\u52a8\u529b\u5b66\u95ee\u9898\u7814\u7a76\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\u3002\u5982\u4f55\u4ece\u590d\u6742\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u65f6\u95f4\u5e8f\u5217\u6d4b\u91cf\u4e2d\u63d0\u53d6\u6709\u6548\u4fe1\u606f\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u53d7\u566a\u58f0\u5f71\u54cd\u7684\u5c0f\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u6717\u4e4b\u4e07\u52a8\u529b\u5b66\u7684\u968f\u673a\u5efa\u6a21\uff0c\u5e76\u5229\u7528\u7535\u6d41\u6784\u5efa\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u8bba\u6587\u5229\u7528\u7535\u6d41\u4e0e\u71b5\u4ea7\u751f\u7684\u5173\u7cfb\uff0c\u76f4\u63a5\u4ece\u7cfb\u7edf\u52a8\u529b\u5b66\u63a8\u5bfc\u51fa\u635f\u5931\u51fd\u6570\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u70ed\u529b\u5b66\u4e0d\u786e\u5b9a\u6027\u5173\u7cfb\uff08TUR\uff09\u7684\u4e2d\u95f4\u6b65\u9aa4\u3002\u8fd9\u4e00\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u7a33\u6001\u7cfb\u7edf\uff0c\u8fd8\u80fd\u5904\u7406\u8fdc\u79bb\u7a33\u6001\u7684\u9a71\u52a8\u7cfb\u7edf\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u63a2\u8ba8\u4e86\u9ad8\u9636\u4f30\u8ba1\u7684\u95ee\u9898\u3002", "result": "\u63d0\u51fa\u7684\u635f\u5931\u51fd\u6570\u4e0d\u4ec5\u590d\u73b0\u4e86\u57fa\u4e8eTUR\u548c\u5176\u4ed6\u65b9\u6cd5\u7684\u7ed3\u679c\uff0c\u8fd8\u4e3a\u4e4b\u524d\u65e0\u6cd5\u76f4\u63a5\u8ba1\u7b97\u7684\u91cf\uff08\u5982\u5355\u8f68\u8ff9\u71b5\u4ea7\u751f\uff09\u63d0\u4f9b\u4e86\u65b0\u7684\u8ba1\u7b97\u8def\u5f84\u3002\u8be5\u65b9\u6cd5\u5c06\u52a8\u6001\u63a8\u65ad\u4e0e\u71b5\u4ea7\u751f\u4f30\u8ba1\u7edf\u4e00\u8d77\u6765\uff0c\u5c55\u793a\u4e86\u673a\u5668\u5b66\u4e60\u6269\u6563\u6a21\u578b\u4e0e\u968f\u673a\u70ed\u529b\u5b66\u71b5\u4ea7\u751f\u4f30\u8ba1\u7684\u6df1\u523b\u8054\u7cfb\u3002", "conclusion": "\u901a\u8fc7\u5c06\u7535\u6d41\u4e0e\u673a\u5668\u5b66\u4e60\u635f\u5931\u51fd\u6570\u7ed3\u5408\uff0c\u8bba\u6587\u4e3a\u968f\u673a\u52a8\u529b\u5b66\u5efa\u6a21\u548c\u71b5\u4ea7\u751f\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7edf\u4e00\u7684\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2504.19370", "pdf": "https://arxiv.org/pdf/2504.19370", "abs": "https://arxiv.org/abs/2504.19370", "authors": ["Jean-R\u00e9my Conti", "St\u00e9phan Cl\u00e9men\u00e7on"], "title": "Mitigating Bias in Facial Recognition Systems: Centroid Fairness Loss Optimization", "categories": ["cs.CV", "cs.AI", "cs.LG", "stat.ML"], "comment": "Accepted at both the AFME and RegML Workshops at NeurIPS 2024. A\n  preliminary version has been accepted for publication by Springer Nature, in\n  the context of the ICPR 2024 conference", "summary": "The urging societal demand for fair AI systems has put pressure on the\nresearch community to develop predictive models that are not only globally\naccurate but also meet new fairness criteria, reflecting the lack of disparate\nmistreatment with respect to sensitive attributes ($\\textit{e.g.}$ gender,\nethnicity, age). In particular, the variability of the errors made by certain\nFacial Recognition (FR) systems across specific segments of the population\ncompromises the deployment of the latter, and was judged unacceptable by\nregulatory authorities. Designing fair FR systems is a very challenging\nproblem, mainly due to the complex and functional nature of the performance\nmeasure used in this domain ($\\textit{i.e.}$ ROC curves) and because of the\nhuge heterogeneity of the face image datasets usually available for training.\nIn this paper, we propose a novel post-processing approach to improve the\nfairness of pre-trained FR models by optimizing a regression loss which acts on\ncentroid-based scores. Beyond the computational advantages of the method, we\npresent numerical experiments providing strong empirical evidence of the gain\nin fairness and of the ability to preserve global accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u57fa\u4e8e\u8d28\u5fc3\u7684\u5206\u6570\u56de\u5f52\u635f\u5931\uff0c\u63d0\u9ad8\u9884\u8bad\u7ec3\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u7684\u516c\u5e73\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u51c6\u786e\u6027\u3002", "motivation": "\u793e\u4f1a\u5bf9\u516c\u5e73AI\u7cfb\u7edf\u7684\u9700\u6c42\u63a8\u52a8\u4e86\u7814\u7a76\uff0c\u5c24\u5176\u662f\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5728\u4e0d\u540c\u4eba\u7fa4\u4e2d\u7684\u9519\u8bef\u7387\u5dee\u5f02\u95ee\u9898\uff0c\u9700\u8981\u6ee1\u8db3\u516c\u5e73\u6027\u6807\u51c6\u3002", "method": "\u91c7\u7528\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u4f18\u5316\u57fa\u4e8e\u8d28\u5fc3\u7684\u5206\u6570\u56de\u5f52\u635f\u5931\uff0c\u6539\u8fdb\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u516c\u5e73\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u516c\u5e73\u6027\uff0c\u5e76\u4fdd\u6301\u4e86\u5168\u5c40\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u516c\u5e73\u6027\u63d0\u5347\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4e3a\u516c\u5e73\u4eba\u8138\u8bc6\u522b\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.19012", "pdf": "https://arxiv.org/pdf/2504.19012", "abs": "https://arxiv.org/abs/2504.19012", "authors": ["Xizhuo", "Zhang", "Bing Yao"], "title": "Geometry-aware Active Learning of Spatiotemporal Dynamic Systems", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Rapid developments in advanced sensing and imaging have significantly\nenhanced information visibility, opening opportunities for predictive modeling\nof complex dynamic systems. However, sensing signals acquired from such complex\nsystems are often distributed across 3D geometries and rapidly evolving over\ntime, posing significant challenges in spatiotemporal predictive modeling. This\npaper proposes a geometry-aware active learning framework for modeling\nspatiotemporal dynamic systems. Specifically, we propose a geometry-aware\nspatiotemporal Gaussian Process (G-ST-GP) to effectively integrate the temporal\ncorrelations and geometric manifold features for reliable prediction of\nhigh-dimensional dynamic behaviors. In addition, we develop an adaptive active\nlearning strategy to strategically identify informative spatial locations for\ndata collection and further maximize the prediction accuracy. This strategy\nachieves the adaptive trade-off between the prediction uncertainty in the\nG-ST-GP model and the space-filling design guided by the geodesic distance\nacross the 3D geometry. We implement the proposed framework to model the\nspatiotemporal electrodynamics in a 3D heart geometry. Numerical experiments\nshow that our framework outperforms traditional methods lacking the mechanism\nof geometric information incorporation or effective data collection.", "AI": {"tldr": "\u8fd9\u7bc7\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u4e2a\u51e0\u4f55\u611f\u77e5\u7684\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6G-ST-GP\uff0c\u7528\u4e8e\u4e09\u7ef4\u52a8\u6001\u7cfb\u7edf\u7684\u65f6\u7a7a\u9884\u6d4b\u5efa\u6a21\uff0c\u901a\u8fc7\u7ed3\u5408\u65f6\u95f4\u76f8\u5173\u6027\u548c\u51e0\u4f55\u6d41\u5f62\u7279\u5f81\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u57283D\u5fc3\u810f\u51e0\u4f55\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u5148\u8fdb\u7684\u4f20\u611f\u6280\u672f\u867d\u7136\u63d0\u9ad8\u4e86\u4fe1\u606f\u53ef\u89c1\u6027\uff0c\u4f46\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u7684\u4e09\u7ef4\u51e0\u4f55\u5206\u5e03\u548c\u5feb\u901f\u6f14\u53d8\u7684\u65f6\u7a7a\u4fe1\u53f7\u7ed9\u9884\u6d4b\u5efa\u6a21\u5e26\u6765\u4e86\u6311\u6218\uff0c\u6587\u7ae0\u8bd5\u56fe\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u548c\u4e3b\u52a8\u5b66\u4e60\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u51e0\u4f55\u611f\u77e5\u7684\u65f6\u7a7a\u9ad8\u65af\u8fc7\u7a0b\uff08G-ST-GP\uff09\uff0c\u6574\u5408\u65f6\u95f4\u76f8\u5173\u6027\u4e0e\u51e0\u4f55\u6d41\u5f62\u7279\u5f81\uff1b\u5e76\u8bbe\u8ba1\u81ea\u9002\u5e94\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\uff0c\u57fa\u4e8e\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u548c\u6d4b\u5730\u8ddd\u79bb\u4f18\u5316\u6570\u636e\u91c7\u96c6\u4f4d\u7f6e\u3002", "result": "\u57283D\u5fc3\u810f\u51e0\u4f55\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u9ad8\u7ef4\u52a8\u6001\u884c\u4e3a\u4e0a\u4f18\u4e8e\u7f3a\u4e4f\u51e0\u4f55\u4fe1\u606f\u6574\u5408\u6216\u6709\u6548\u6570\u636e\u91c7\u96c6\u7684\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u51e0\u4f55\u611f\u77e5\u7684G-ST-GP\u6846\u67b6\u548c\u81ea\u9002\u5e94\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u7684\u65f6\u7a7a\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u4e3a\u7c7b\u4f3c\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.19373", "pdf": "https://arxiv.org/pdf/2504.19373", "abs": "https://arxiv.org/abs/2504.19373", "authors": ["Weidi Luo", "Qiming Zhang", "Tianyu Lu", "Xiaogeng Liu", "Yue Zhao", "Zhen Xiang", "Chaowei Xiao"], "title": "Doxing via the Lens: Revealing Privacy Leakage in Image Geolocation for Agentic Multi-Modal Large Reasoning Model", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The increasing capabilities of agentic multi-modal large reasoning models,\nsuch as ChatGPT o3, have raised critical concerns regarding privacy leakage\nthrough inadvertent image geolocation. In this paper, we conduct the first\nsystematic and controlled study on the potential privacy risks associated with\nvisual reasoning abilities of ChatGPT o3. We manually collect and construct a\ndataset comprising 50 real-world images that feature individuals alongside\nprivacy-relevant environmental elements, capturing realistic and sensitive\nscenarios for analysis. Our experimental evaluation reveals that ChatGPT o3 can\npredict user locations with high precision, achieving street-level accuracy\n(within one mile) in 60% of cases. Through analysis, we identify key visual\ncues, including street layout and front yard design, that significantly\ncontribute to the model inference success. Additionally, targeted occlusion\nexperiments demonstrate that masking critical features effectively mitigates\ngeolocation accuracy, providing insights into potential defense mechanisms. Our\nfindings highlight an urgent need for privacy-aware development for agentic\nmulti-modal large reasoning models, particularly in applications involving\nprivate imagery.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86ChatGPT o3\u5728\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u4e0b\u53ef\u80fd\u5bfc\u81f4\u7684\u9690\u79c1\u98ce\u9669\uff0c\u53d1\u73b0\u5176\u80fd\u901a\u8fc7\u56fe\u50cf\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u7528\u6237\u4f4d\u7f6e\uff0c\u5e76\u63d0\u51fa\u9632\u5fa1\u673a\u5236\u3002", "motivation": "\u63a2\u8ba8\u591a\u6a21\u6001\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08\u5982ChatGPT o3\uff09\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u662f\u5426\u4f1a\u5bfc\u81f4\u9690\u79c1\u6cc4\u9732\uff0c\u91cd\u70b9\u5173\u6ce8\u56fe\u50cf\u5730\u7406\u4f4d\u7f6e\u63a8\u65ad\u7684\u98ce\u9669\u3002", "method": "\u624b\u52a8\u6784\u5efa\u5305\u542b50\u5f20\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u7684\u9690\u79c1\u76f8\u5173\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30ChatGPT o3\u7684\u5b9a\u4f4d\u80fd\u529b\uff0c\u5e76\u8fdb\u884c\u906e\u6321\u5b9e\u9a8c\u9a8c\u8bc1\u5173\u952e\u7279\u5f81\u7684\u4f5c\u7528\u3002", "result": "ChatGPT o3\u572860%\u7684\u6848\u4f8b\u4e2d\u80fd\u5b9e\u73b0\u8857\u9053\u7ea7\uff08\u4e00\u82f1\u91cc\u5185\uff09\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u5173\u952e\u89c6\u89c9\u7ebf\u7d22\uff08\u5982\u8857\u9053\u5e03\u5c40\u548c\u5ead\u9662\u8bbe\u8ba1\uff09\u5bf9\u63a8\u65ad\u6210\u529f\u8d21\u732e\u663e\u8457\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u9700\u5728\u5f00\u53d1\u591a\u6a21\u6001\u5927\u578b\u63a8\u7406\u6a21\u578b\u65f6\u6ce8\u91cd\u9690\u79c1\u4fdd\u62a4\uff0c\u5c24\u5176\u662f\u6d89\u53ca\u79c1\u4eba\u56fe\u50cf\u7684\u573a\u666f\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u906e\u6321\u5173\u952e\u7279\u5f81\u964d\u4f4e\u5b9a\u4f4d\u7cbe\u5ea6\u7684\u9632\u5fa1\u65b9\u6848\u3002"}}
{"id": "2504.19384", "pdf": "https://arxiv.org/pdf/2504.19384", "abs": "https://arxiv.org/abs/2504.19384", "authors": ["Syed Tauhid Ullah Shah", "Mohamad Hussein", "Ann Barcomb", "Mohammad Moshirpour"], "title": "From Inductive to Deductive: LLMs-Based Qualitative Data Analysis in Requirements Engineering", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Requirements Engineering (RE) is essential for developing complex and\nregulated software projects. Given the challenges in transforming stakeholder\ninputs into consistent software designs, Qualitative Data Analysis (QDA)\nprovides a systematic approach to handling free-form data. However, traditional\nQDA methods are time-consuming and heavily reliant on manual effort. In this\npaper, we explore the use of Large Language Models (LLMs), including GPT-4,\nMistral, and LLaMA-2, to improve QDA tasks in RE. Our study evaluates LLMs'\nperformance in inductive (zero-shot) and deductive (one-shot, few-shot)\nannotation tasks, revealing that GPT-4 achieves substantial agreement with\nhuman analysts in deductive settings, with Cohen's Kappa scores exceeding 0.7,\nwhile zero-shot performance remains limited. Detailed, context-rich prompts\nsignificantly improve annotation accuracy and consistency, particularly in\ndeductive scenarios, and GPT-4 demonstrates high reliability across repeated\nruns. These findings highlight the potential of LLMs to support QDA in RE by\nreducing manual effort while maintaining annotation quality. The structured\nlabels automatically provide traceability of requirements and can be directly\nutilized as classes in domain models, facilitating systematic software design.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u9700\u6c42\u5de5\u7a0b\u4e2d\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4\u3001Mistral\u548cLLaMA-2\uff09\u6539\u8fdb\u5b9a\u6027\u6570\u636e\u5206\u6790\uff08QDA\uff09\u4efb\u52a1\uff0c\u53d1\u73b0GPT-4\u5728\u6f14\u7ece\u6807\u6ce8\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4eba\u5de5\u5de5\u4f5c\u91cf\u3002", "motivation": "\u4f20\u7edf\u7684QDA\u65b9\u6cd5\u8017\u65f6\u4e14\u4f9d\u8d56\u4eba\u5de5\uff0c\u96be\u4ee5\u9ad8\u6548\u5904\u7406\u590d\u6742\u8f6f\u4ef6\u9879\u76ee\u7684\u81ea\u7531\u5f62\u5f0f\u6570\u636e\uff0c\u56e0\u6b64\u63a2\u7d22\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u5347QDA\u6548\u7387\u548c\u8d28\u91cf\u3002", "method": "\u8bc4\u4f30\u4e86LLMs\u5728\u5f52\u7eb3\uff08\u96f6\u6837\u672c\uff09\u548c\u6f14\u7ece\uff08\u4e00\u6837\u672c\u3001\u591a\u6837\u672c\uff09\u6807\u6ce8\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u5173\u6ce8GPT-4\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\uff0c\u5e76\u5206\u6790\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u63d0\u793a\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "GPT-4\u5728\u6f14\u7ece\u6807\u6ce8\u4efb\u52a1\u4e2d\u4e0e\u4eba\u7c7b\u5206\u6790\u5e08\u7684\u4e00\u81f4\u6027\u8f83\u9ad8\uff08Cohen's Kappa > 0.7\uff09\uff0c\u800c\u96f6\u6837\u672c\u8868\u73b0\u8f83\u5f31\uff1b\u8be6\u7ec6\u7684\u63d0\u793a\u663e\u8457\u63d0\u5347\u4e86\u6807\u6ce8\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "LLMs\uff08\u5c24\u5176\u662fGPT-4\uff09\u80fd\u591f\u6709\u6548\u51cf\u5c11QDA\u7684\u4eba\u5de5\u5de5\u4f5c\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6807\u6ce8\u8d28\u91cf\uff0c\u5e76\u4e3a\u9700\u6c42\u8ddf\u8e2a\u548c\u9886\u57df\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u7ed3\u6784\u5316\u7684\u6807\u7b7e\u652f\u6301\u3002"}}
{"id": "2504.19394", "pdf": "https://arxiv.org/pdf/2504.19394", "abs": "https://arxiv.org/abs/2504.19394", "authors": ["Toby Simonds"], "title": "LLMs for Engineering: Teaching Models to Design High Powered Rockets", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have transformed software engineering, but their\napplication to physical engineering domains remains underexplored. This paper\nevaluates LLMs' capabilities in high-powered rocketry design through\nRocketBench, a benchmark connecting LLMs to high-fidelity rocket simulations.\nWe test models on two increasingly complex design tasks: target altitude\noptimization and precision landing challenges. Our findings reveal that while\nstate-of-the-art LLMs demonstrate strong baseline engineering knowledge, they\nstruggle to iterate on their designs when given simulation results and\nultimately plateau below human performance levels. However, when enhanced with\nreinforcement learning (RL), we show that a 7B parameter model outperforms both\nSoTA foundation models and human experts. This research demonstrates that\nRL-trained LLMs can serve as effective tools for complex engineering\noptimization, potentially transforming engineering domains beyond software\ndevelopment.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u706b\u7bad\u8bbe\u8ba1\u4e2d\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u57fa\u7840LLMs\u8868\u73b0\u6709\u9650\uff0c\u4f46\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u589e\u5f3a\u540e\u53ef\u4ee5\u8d85\u8d8a\u4eba\u7c7b\u4e13\u5bb6\u548cSoTA\u6a21\u578b\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u7269\u7406\u5de5\u7a0b\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u586b\u8865\u5176\u5728\u706b\u7bad\u8bbe\u8ba1\u9886\u57df\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u7528RocketBench\u57fa\u51c6\u6d4b\u8bd5\u8fde\u63a5LLMs\u548c\u9ad8\u4fdd\u771f\u706b\u7bad\u6a21\u62df\uff0c\u6d4b\u8bd5\u76ee\u6807\u9ad8\u5ea6\u4f18\u5316\u548c\u7cbe\u51c6\u7740\u9646\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u589e\u5f3a\u6a21\u578b\u3002", "result": "\u57fa\u7840LLMs\u867d\u5177\u5907\u5de5\u7a0b\u77e5\u8bc6\u4f46\u96be\u4ee5\u8fed\u4ee3\u4f18\u5316\uff0cRL\u589e\u5f3a\u540e\u76847B\u53c2\u6570\u6a21\u578b\u4f18\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u548c\u5176\u4ed6SoTA\u6a21\u578b\u3002", "conclusion": "RL\u8bad\u7ec3\u7684LLMs\u53ef\u4f5c\u4e3a\u590d\u6742\u5de5\u7a0b\u4f18\u5316\u7684\u6709\u6548\u5de5\u5177\uff0c\u6709\u671b\u62d3\u5c55\u81f3\u8f6f\u4ef6\u5f00\u53d1\u4ee5\u5916\u7684\u5de5\u7a0b\u9886\u57df\u3002"}}
{"id": "2504.19409", "pdf": "https://arxiv.org/pdf/2504.19409", "abs": "https://arxiv.org/abs/2504.19409", "authors": ["Zuxing Lu", "Xin Yuan", "Shaowen Yang", "Jingyu Liu", "Jiawei Wang", "Changyin Sun"], "title": "GSFF-SLAM: 3D Semantic Gaussian Splatting SLAM via Feature Field", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Semantic-aware 3D scene reconstruction is essential for autonomous robots to\nperform complex interactions. Semantic SLAM, an online approach, integrates\npose tracking, geometric reconstruction, and semantic mapping into a unified\nframework, shows significant potential. However, existing systems, which rely\non 2D ground truth priors for supervision, are often limited by the sparsity\nand noise of these signals in real-world environments. To address this\nchallenge, we propose GSFF-SLAM, a novel dense semantic SLAM system based on 3D\nGaussian Splatting that leverages feature fields to achieve joint rendering of\nappearance, geometry, and N-dimensional semantic features. By independently\noptimizing feature gradients, our method supports semantic reconstruction using\nvarious forms of 2D priors, particularly sparse and noisy signals. Experimental\nresults demonstrate that our approach outperforms previous methods in both\ntracking accuracy and photorealistic rendering quality. When utilizing 2D\nground truth priors, GSFF-SLAM achieves state-of-the-art semantic segmentation\nperformance with 95.03\\% mIoU, while achieving up to 2.9$\\times$ speedup with\nonly marginal performance degradation.", "AI": {"tldr": "GSFF-SLAM\u662f\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u65b0\u578b\u5bc6\u96c6\u8bed\u4e49SLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u7279\u5f81\u573a\u5b9e\u73b0\u5916\u89c2\u3001\u51e0\u4f55\u548cN\u7ef4\u8bed\u4e49\u7279\u5f81\u7684\u8054\u5408\u6e32\u67d3\uff0c\u652f\u6301\u7a00\u758f\u548c\u5608\u6742\u76842D\u5148\u9a8c\u4fe1\u53f7\uff0c\u5e76\u5728\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u903c\u771f\u6e32\u67d3\u8d28\u91cf\u4e0a\u8d85\u8d8a\u5148\u524d\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49SLAM\u7cfb\u7edf\u4f9d\u8d562D\u771f\u5b9e\u5148\u9a8c\u8fdb\u884c\u76d1\u7763\uff0c\u4f46\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8fd9\u4e9b\u4fe1\u53f7\u7a00\u758f\u4e14\u5608\u6742\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u9ad8\u6548\u5229\u7528\u5404\u7c7b2D\u5148\u9a8c\u7684\u65b9\u6cd5\u3002", "method": "GSFF-SLAM\u91c7\u75283D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u901a\u8fc7\u72ec\u7acb\u4f18\u5316\u7279\u5f81\u68af\u5ea6\uff0c\u5b9e\u73b0\u4e86\u5916\u89c2\u3001\u51e0\u4f55\u548c\u8bed\u4e49\u7279\u5f81\u7684\u8054\u5408\u6e32\u67d3\uff0c\u652f\u6301\u591a\u79cd2D\u5148\u9a8c\u4fe1\u53f7\u3002", "result": "\u57282D\u771f\u5b9e\u5148\u9a8c\u4e0b\uff0cGSFF-SLAM\u8fbe\u523095.03%\u7684mIoU\u8bed\u4e49\u5206\u5272\u6027\u80fd\uff0c\u901f\u5ea6\u63d0\u53472.9\u500d\u4e14\u6027\u80fd\u635f\u5931\u5fae\u5c0f\u3002", "conclusion": "GSFF-SLAM\u901a\u8fc73D\u9ad8\u65af\u6cfc\u6e85\u548c\u7279\u5f81\u573a\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u4e49SLAM\u7684\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u590d\u6742\u771f\u5b9e\u73af\u5883\u3002"}}
{"id": "2504.19053", "pdf": "https://arxiv.org/pdf/2504.19053", "abs": "https://arxiv.org/abs/2504.19053", "authors": ["Hongni Jin", "Gurinder Singh", "Kenneth M. Merz Jr"], "title": "QFGN: A Quantum Approach to High-Fidelity Implicit Neural Representations", "categories": ["quant-ph", "cs.LG"], "comment": null, "summary": "Implicit neural representations have shown potential in various applications.\nHowever, accurately reconstructing the image or providing clear details via\nimage super-resolution remains challenging. This paper introduces Quantum\nFourier Gaussian Network (QFGN), a quantum-based machine learning model for\nbetter signal representations. The frequency spectrum is well balanced by\npenalizing the low-frequency components, leading to the improved expressivity\nof quantum circuits. The results demonstrate that with minimal parameters, QFGN\noutperforms the current state-of-the-art (SOTA) models. Despite noise on\nhardware, the model achieves accuracy comparable to that of SIREN, highlighting\nthe potential applications of quantum machine learning in this field.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u7684Quantum Fourier Gaussian Network (QFGN)\uff0c\u7528\u4e8e\u6539\u8fdb\u4fe1\u53f7\u8868\u793a\u548c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u3002\u5c3d\u7ba1\u786c\u4ef6\u5b58\u5728\u566a\u58f0\uff0cQFGN\u5728\u53c2\u6570\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u4e8e\u5f53\u524dSOTA\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\u96be\u4ee5\u51c6\u786e\u91cd\u5efa\u6216\u63d0\u4f9b\u6e05\u6670\u7ec6\u8282\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4fe1\u53f7\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u91cf\u5b50\u5085\u91cc\u53f6\u9ad8\u65af\u7f51\u7edc(QFGN)\uff0c\u901a\u8fc7\u60e9\u7f5a\u4f4e\u9891\u5206\u91cf\u5e73\u8861\u9891\u8c31\uff0c\u63d0\u5347\u91cf\u5b50\u7535\u8def\u7684\u8868\u8fbe\u80fd\u529b\u3002", "result": "QFGN\u5728\u5c11\u91cf\u53c2\u6570\u4e0b\u4f18\u4e8e\u73b0\u6709SOTA\u6a21\u578b\uff0c\u566a\u58f0\u73af\u5883\u4e0b\u4ecd\u80fd\u8fbe\u5230\u4e0eSIREN\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002", "conclusion": "QFGN\u5c55\u793a\u4e86\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u4e3a\u4fe1\u53f7\u8868\u793a\u548c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.19426", "pdf": "https://arxiv.org/pdf/2504.19426", "abs": "https://arxiv.org/abs/2504.19426", "authors": ["Steffen Dereich", "Arnulf Jentzen", "Adrian Riekert"], "title": "Sharp higher order convergence rates for the Adam optimizer", "categories": ["math.OC", "cs.AI", "68T05, 65K05, 90C25", "I.2.0"], "comment": "27 pages", "summary": "Gradient descent based optimization methods are the methods of choice to\ntrain deep neural networks in machine learning. Beyond the standard gradient\ndescent method, also suitable modified variants of standard gradient descent\ninvolving acceleration techniques such as the momentum method and/or adaptivity\ntechniques such as the RMSprop method are frequently considered optimization\nmethods. These days the most popular of such sophisticated optimization schemes\nis presumably the Adam optimizer that has been proposed in 2014 by Kingma and\nBa. A highly relevant topic of research is to investigate the speed of\nconvergence of such optimization methods. In particular, in 1964 Polyak showed\nthat the standard gradient descent method converges in a neighborhood of a\nstrict local minimizer with rate (x - 1)(x + 1)^{-1} while momentum achieves\nthe (optimal) strictly faster convergence rate (\\sqrt{x} - 1)(\\sqrt{x} +\n1)^{-1} where x \\in (1,\\infty) is the condition number (the ratio of the\nlargest and the smallest eigenvalue) of the Hessian of the objective function\nat the local minimizer. It is the key contribution of this work to reveal that\nAdam also converges with the strictly faster convergence rate (\\sqrt{x} -\n1)(\\sqrt{x} + 1)^{-1} while RMSprop only converges with the convergence rate (x\n- 1)(x + 1)^{-1}.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86Adam\u4f18\u5316\u5668\u5728\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u7684\u6536\u655b\u901f\u5ea6\uff0c\u53d1\u73b0\u5176\u4e0e\u52a8\u91cf\u6cd5\u4e00\u6837\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u7387(\u221ax-1)/(\u221ax+1)\uff0c\u800cRMSprop\u5219\u4e0e\u6807\u51c6\u68af\u5ea6\u4e0b\u964d\u6cd5\u76f8\u540c\uff0c\u6536\u655b\u901f\u7387\u4e3a(x-1)/(x+1)\u3002", "motivation": "\u7814\u7a76\u4e0d\u540c\u4f18\u5316\u65b9\u6cd5\uff08\u5982Adam\u3001RMSprop\uff09\u5728\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65f6\u7684\u6536\u655b\u901f\u5ea6\uff0c\u4e3a\u4f18\u5316\u7b97\u6cd5\u9009\u62e9\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u5bf9\u6bd4\u6807\u51c6\u68af\u5ea6\u4e0b\u964d\u3001\u52a8\u91cf\u6cd5\u3001Adam\u548cRMSprop\u5728\u5c40\u90e8\u6781\u5c0f\u503c\u9644\u8fd1\u7684\u6536\u655b\u901f\u7387\u3002", "result": "Adam\u4f18\u5316\u5668\u6536\u655b\u901f\u7387\u4e0e\u52a8\u91cf\u6cd5\u76f8\u540c\uff0c(\u221ax-1)/(\u221ax+1)\uff0c\u4f18\u4e8eRMSprop\u548c\u6807\u51c6\u68af\u5ea6\u4e0b\u964d\u6cd5\uff08(x-1)/(x+1)\uff09\u3002", "conclusion": "Adam\u4f18\u5316\u5668\u5728\u6536\u655b\u901f\u5ea6\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u590d\u6742\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u9ad8\u6548\u6027\u3002"}}
{"id": "2504.19432", "pdf": "https://arxiv.org/pdf/2504.19432", "abs": "https://arxiv.org/abs/2504.19432", "authors": ["Zhe Dong", "Yuzhe Sun", "Tianzhu Liu", "Wangmeng Zuo", "Yanfeng Gu"], "title": "EarthMapper: Visual Autoregressive Models for Controllable Bidirectional Satellite-Map Translation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Satellite imagery and maps, as two fundamental data modalities in remote\nsensing, offer direct observations of the Earth's surface and\nhuman-interpretable geographic abstractions, respectively. The task of\nbidirectional translation between satellite images and maps (BSMT) holds\nsignificant potential for applications in urban planning and disaster response.\nHowever, this task presents two major challenges: first, the absence of precise\npixel-wise alignment between the two modalities substantially complicates the\ntranslation process; second, it requires achieving both high-level abstraction\nof geographic features and high-quality visual synthesis, which further\nelevates the technical complexity. To address these limitations, we introduce\nEarthMapper, a novel autoregressive framework for controllable bidirectional\nsatellite-map translation. EarthMapper employs geographic coordinate embeddings\nto anchor generation, ensuring region-specific adaptability, and leverages\nmulti-scale feature alignment within a geo-conditioned joint scale\nautoregression (GJSA) process to unify bidirectional translation in a single\ntraining cycle. A semantic infusion (SI) mechanism is introduced to enhance\nfeature-level consistency, while a key point adaptive guidance (KPAG) mechanism\nis proposed to dynamically balance diversity and precision during inference. We\nfurther contribute CNSatMap, a large-scale dataset comprising 302,132 precisely\naligned satellite-map pairs across 38 Chinese cities, enabling robust\nbenchmarking. Extensive experiments on CNSatMap and the New York dataset\ndemonstrate EarthMapper's superior performance, achieving significant\nimprovements in visual realism, semantic consistency, and structural fidelity\nover state-of-the-art methods. Additionally, EarthMapper excels in zero-shot\ntasks like in-painting, out-painting and coordinate-conditional generation,\nunderscoring its versatility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEarthMapper\u7684\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u7528\u4e8e\u53ef\u63a7\u7684\u536b\u661f\u56fe\u50cf\u4e0e\u5730\u56fe\u53cc\u5411\u7ffb\u8bd1\uff0c\u89e3\u51b3\u4e86\u6a21\u6001\u95f4\u50cf\u7d20\u5bf9\u9f50\u4e0d\u8db3\u548c\u9ad8\u5c42\u6b21\u62bd\u8c61/\u9ad8\u8d28\u91cf\u89c6\u89c9\u5408\u6210\u7684\u6280\u672f\u6311\u6218\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u536b\u661f\u56fe\u50cf\u4e0e\u5730\u56fe\u7684\u53cc\u5411\u7ffb\u8bd1\u5728\u57ce\u89c4\u548c\u707e\u96be\u54cd\u5e94\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5b58\u5728\u50cf\u7d20\u5bf9\u9f50\u4e0d\u8db3\u548c\u62bd\u8c61/\u5408\u6210\u6280\u672f\u590d\u6742\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u5730\u7406\u5750\u6807\u5d4c\u5165\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u5bf9\u9f50\uff0c\u7ed3\u5408\u8bed\u4e49\u6ce8\u5165\u548c\u5173\u952e\u70b9\u81ea\u9002\u5e94\u6307\u5bfc\u673a\u5236\uff0c\u7edf\u4e00\u53cc\u5411\u7ffb\u8bd1\u4e8e\u5355\u6b21\u8bad\u7ec3\u3002", "result": "\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u5353\u8d8a\uff0c\u89c6\u89c9\u771f\u5b9e\u611f\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u7ed3\u6784\u4fdd\u771f\u5ea6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u5c55\u793a\u591a\u80fd\u6027\u3002", "conclusion": "EarthMapper\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u53cc\u5411\u7ffb\u8bd1\u7684\u6838\u5fc3\u96be\u9898\uff0c\u5c55\u73b0\u4e86\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.19074", "pdf": "https://arxiv.org/pdf/2504.19074", "abs": "https://arxiv.org/abs/2504.19074", "authors": ["Anyong Qin", "Chaoqi Yuan", "Qiang Li", "Feng Yang", "Tiecheng Song", "Chenqiang Gao"], "title": "Dual-Branch Residual Network for Cross-Domain Few-Shot Hyperspectral Image Classification with Refined Prototype", "categories": ["cs.CV", "cs.LG"], "comment": "5 pages, 2 figures. IEEE Geoscience and Remote Sensing Letters (2025)", "summary": "Convolutional neural networks (CNNs) are effective for hyperspectral image\n(HSI) classification, but their 3D convolutional structures introduce high\ncomputational costs and limited generalization in few-shot scenarios. Domain\nshifts caused by sensor differences and environmental variations further hinder\ncross-dataset adaptability. Metric-based few-shot learning (FSL) prototype\nnetworks mitigate this problem, yet their performance is sensitive to prototype\nquality, especially with limited samples. To overcome these challenges, a\ndual-branch residual network that integrates spatial and spectral features via\nparallel branches is proposed in this letter. Additionally, more robust refined\nprototypes are obtained through a regulation term. Furthermore, a kernel\nprobability matching strategy aligns source and target domain features,\nalleviating domain shift. Experiments on four publicly available HSI datasets\nillustrate that the proposal achieves superior performance compared to other\nmethods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5206\u652f\u6b8b\u5dee\u7f51\u7edc\uff0c\u901a\u8fc7\u6574\u5408\u7a7a\u95f4\u548c\u5149\u8c31\u7279\u5f81\u6765\u89e3\u51b3\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u7684\u539f\u578b\u8d28\u91cf\u548c\u57df\u9002\u5e94\u7b56\u7565\u63d0\u5347\u4e86\u5c11\u6837\u672c\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u4e2d3D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u5c11\u6837\u672c\u5b66\u4e60\u548c\u57df\u9002\u5e94\u5e26\u6765\u7684\u6311\u6218\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u6b8b\u5dee\u7f51\u7edc\u7ed3\u6784\uff0c\u5e76\u884c\u6574\u5408\u7a7a\u95f4\u548c\u5149\u8c31\u7279\u5f81\uff1b\u901a\u8fc7\u6539\u8fdb\u7684\u539f\u578b\u751f\u6210\u65b9\u6cd5\u548c\u6838\u6982\u7387\u5339\u914d\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u5c11\u6837\u672c\u5b66\u4e60\u548c\u8de8\u57df\u9002\u5e94\u80fd\u529b\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u7684\u9ad8\u5149\u8c31\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u5176\u4ed6\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53cc\u5206\u652f\u6b8b\u5dee\u7f51\u7edc\u548c\u57df\u9002\u5e94\u7b56\u7565\u5728\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u5c11\u6837\u672c\u548c\u8de8\u57df\u573a\u666f\u4e0b\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2504.19443", "pdf": "https://arxiv.org/pdf/2504.19443", "abs": "https://arxiv.org/abs/2504.19443", "authors": ["Yejin Jeong", "Donghun Lee"], "title": "CLIP-KOA: Enhancing Knee Osteoarthritis Diagnosis with Multi-Modal Learning and Symmetry-Aware Loss Functions", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 2 figures", "summary": "Knee osteoarthritis (KOA) is a universal chronic musculoskeletal disorders\nworldwide, making early diagnosis crucial. Currently, the Kellgren and Lawrence\n(KL) grading system is widely used to assess KOA severity. However, its high\ninter-observer variability and subjectivity hinder diagnostic consistency. To\naddress these limitations, automated diagnostic techniques using deep learning\nhave been actively explored in recent years. In this study, we propose a\nCLIP-based framework (CLIP-KOA) to enhance the consistency and reliability of\nKOA grade prediction. To achieve this, we introduce a learning approach that\nintegrates image and text information and incorporate Symmetry Loss and\nConsistency Loss to ensure prediction consistency between the original and\nflipped images. CLIP-KOA achieves state-of-the-art accuracy of 71.86\\% on KOA\nseverity prediction task, and ablation studies show that CLIP-KOA has 2.36\\%\nimprovement in accuracy over the standard CLIP model due to our contribution.\nThis study shows a novel direction for data-driven medical prediction not only\nto improve reliability of fine-grained diagnosis and but also to explore\nmultimodal methods for medical image analysis. Our code is available at\nhttps://github.com/anonymized-link.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCLIP\u7684\u6846\u67b6(CLIP-KOA)\uff0c\u901a\u8fc7\u7ed3\u5408\u56fe\u50cf\u548c\u6587\u672c\u4fe1\u606f\u4ee5\u53ca\u5f15\u5165\u5bf9\u79f0\u6027\u548c\u4e00\u81f4\u6027\u635f\u5931\uff0c\u63d0\u5347\u4e86\u819d\u9aa8\u5173\u8282\u708e(KOA)\u4e25\u91cd\u7a0b\u5ea6\u9884\u6d4b\u7684\u4e00\u81f4\u6027\u548c\u53ef\u9760\u6027\uff0c\u51c6\u786e\u7387\u8fbe71.86%\u3002", "motivation": "\u819d\u9aa8\u5173\u8282\u708e(KOA)\u662f\u4e00\u79cd\u5168\u7403\u5e38\u89c1\u7684\u6162\u6027\u808c\u8089\u9aa8\u9abc\u75be\u75c5\uff0c\u65e9\u671f\u8bca\u65ad\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684KL\u8bc4\u5206\u7cfb\u7edf\u56e0\u5176\u9ad8\u4e3b\u89c2\u6027\u548c\u89c2\u5bdf\u8005\u95f4\u5dee\u5f02\u5bfc\u81f4\u8bca\u65ad\u4e0d\u4e00\u81f4\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u81ea\u52a8\u5316\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528CLIP\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u50cf\u548c\u6587\u672c\u4fe1\u606f\uff0c\u5e76\u5f15\u5165\u5bf9\u79f0\u6027\u635f\u5931(Symmetry Loss)\u548c\u4e00\u81f4\u6027\u635f\u5931(Consistency Loss)\uff0c\u786e\u4fdd\u539f\u59cb\u56fe\u50cf\u4e0e\u7ffb\u8f6c\u56fe\u50cf\u7684\u9884\u6d4b\u7ed3\u679c\u4e00\u81f4\u3002", "result": "CLIP-KOA\u5728KOA\u4e25\u91cd\u7a0b\u5ea6\u9884\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u523071.86%\u7684\u51c6\u786e\u7387\uff0c\u6bd4\u6807\u51c6CLIP\u6a21\u578b\u63d0\u53472.36%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u533b\u5b66\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u7ec6\u7c92\u5ea6\u8bca\u65ad\u7684\u53ef\u9760\u6027\uff0c\u8fd8\u63a2\u7d22\u4e86\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2504.19113", "pdf": "https://arxiv.org/pdf/2504.19113", "abs": "https://arxiv.org/abs/2504.19113", "authors": ["Satwik Kundu", "Swaroop Ghosh"], "title": "Inverse-Transpilation: Reverse-Engineering Quantum Compiler Optimization Passes from Circuit Snapshots", "categories": ["quant-ph", "cs.LG"], "comment": null, "summary": "Circuit compilation, a crucial process for adapting quantum algorithms to\nhardware constraints, often operates as a ``black box,'' with limited\nvisibility into the optimization techniques used by proprietary systems or\nadvanced open-source frameworks. Due to fundamental differences in qubit\ntechnologies, efficient compiler design is an expensive process, further\nexposing these systems to various security threats. In this work, we take a\nfirst step toward evaluating one such challenge affecting compiler\nconfidentiality, specifically, reverse-engineering compilation methodologies.\nWe propose a simple ML-based framework to infer underlying optimization\ntechniques by leveraging structural differences observed between original and\ncompiled circuits. The motivation is twofold: (1) enhancing transparency in\ncircuit optimization for improved cross-platform debugging and performance\ntuning, and (2) identifying potential intellectual property (IP)-protected\noptimizations employed by commercial systems. Our extensive evaluation across\nthousands of quantum circuits shows that a neural network performs the best in\ndetecting optimization passes, with individual pass F1-scores reaching as high\nas 0.96. Thus, our initial study demonstrates the viability of this threat to\ncompiler confidentiality and underscores the need for active research in this\narea.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u6bd4\u8f83\u539f\u59cb\u548c\u7f16\u8bd1\u540e\u7684\u91cf\u5b50\u7535\u8def\u7ed3\u6784\u5dee\u5f02\u6765\u63a8\u65ad\u5e95\u5c42\u4f18\u5316\u6280\u672f\uff0c\u65e8\u5728\u63d0\u5347\u7f16\u8bd1\u900f\u660e\u5ea6\u548c\u8bc6\u522b\u5546\u4e1a\u7cfb\u7edf\u7684\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u4f18\u5316\u3002", "motivation": "\u589e\u5f3a\u91cf\u5b50\u7535\u8def\u4f18\u5316\u7684\u900f\u660e\u5ea6\u4ee5\u6539\u8fdb\u8de8\u5e73\u53f0\u8c03\u8bd5\u548c\u6027\u80fd\u8c03\u4f18\uff0c\u540c\u65f6\u8bc6\u522b\u5546\u4e1a\u7cfb\u7edf\u4e2d\u53ef\u80fd\u53d7\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u7684\u4f18\u5316\u6280\u672f\u3002", "method": "\u91c7\u7528\u7b80\u5355\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u5206\u6790\u539f\u59cb\u548c\u7f16\u8bd1\u540e\u91cf\u5b50\u7535\u8def\u7684\u7ed3\u6784\u5dee\u5f02\uff0c\u4ee5\u63a8\u65ad\u4f18\u5316\u6280\u672f\u3002", "result": "\u5728\u5927\u91cf\u91cf\u5b50\u7535\u8def\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u795e\u7ecf\u7f51\u7edc\u5728\u68c0\u6d4b\u4f18\u5316\u6b65\u9aa4\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u5355\u4e2a\u4f18\u5316\u6b65\u9aa4\u7684F1\u5206\u6570\u9ad8\u8fbe0.96\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u8fd9\u79cd\u5a01\u80c1\u5bf9\u7f16\u8bd1\u5668\u4fdd\u5bc6\u6027\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u5728\u8fd9\u4e00\u9886\u57df\u5f00\u5c55\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2504.19145", "pdf": "https://arxiv.org/pdf/2504.19145", "abs": "https://arxiv.org/abs/2504.19145", "authors": ["Abhishek Pasula", "Deepak N. Subramani"], "title": "Global Climate Model Bias Correction Using Deep Learning", "categories": ["physics.ao-ph", "cs.LG", "stat.AP"], "comment": "40 pages, 15 figures", "summary": "Climate change affects ocean temperature, salinity and sea level, impacting\nmonsoons and ocean productivity. Future projections by Global Climate Models\nbased on shared socioeconomic pathways from the Coupled Model Intercomparison\nProject (CMIP) are widely used to understand the effects of climate change.\nHowever, CMIP models have significant bias compared to reanalysis in the Bay of\nBengal for the time period when both projections and reanalysis are available.\nFor example, there is a 1.5C root mean square error (RMSE) in the sea surface\ntemperature (SST) projections of the climate model CNRM-CM6 compared to the\nOcean Reanalysis System (ORAS5). We develop a suite of data-driven deep\nlearning models for bias correction of climate model projections and apply it\nto correct SST projections of the Bay of Bengal. We propose the use of three\ndifferent deep neural network architectures: convolutional encoder-decoder\nUNet, Bidirectional LSTM and ConvLSTM. We also use a baseline linear regression\nmodel and the Equi-Distant Cumulative Density Function (EDCDF) bias correction\nmethod for comparison and evaluating the impact of the new deep learning\nmodels. All bias correction models are trained using pairs of monthly CMIP6\nprojections and the corresponding month's ORAS5 as input and output. Historical\ndata (1950-2014) and future projection data (2015-2020) of CNRM-CM6 are used\nfor training and validation, including hyperparameter tuning. Testing is\nperformed on future projection data from 2021 to 2024. Detailed analysis of the\nthree deep neural models has been completed. We found that the UNet\narchitecture trained using a climatology-removed CNRM-CM6 projection as input\nand climatology-removed ORAS5 as output gives the best bias-corrected\nprojections. Our novel deep learning-based method for correcting CNRM-CM6 data\nhas a 15% reduction in RMSE compared EDCDF.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u504f\u5dee\u6821\u6b63\u65b9\u6cd5\uff0c\u7528\u4e8e\u7ea0\u6b63CMIP6\u6c14\u5019\u6a21\u578b\u5728\u5b5f\u52a0\u62c9\u6e7e\u6d77\u9762\u6e29\u5ea6\uff08SST\uff09\u9884\u6d4b\u4e2d\u7684\u504f\u5dee\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\uff08EDCDF\uff09\u964d\u4f4e\u4e8615%\u7684RMSE\u3002", "motivation": "CMIP\u6c14\u5019\u6a21\u578b\u5728\u5b5f\u52a0\u62c9\u6e7e\u7684SST\u9884\u6d4b\u4e2d\u5b58\u5728\u663e\u8457\u504f\u5dee\uff0c\u5f71\u54cd\u6c14\u5019\u53d8\u5316\u7684\u51c6\u786e\u5206\u6790\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u504f\u5dee\u6821\u6b63\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff08UNet\u3001\u53cc\u5411LSTM\u548cConvLSTM\uff09\u8fdb\u884c\u504f\u5dee\u6821\u6b63\uff0c\u5e76\u4e0e\u7ebf\u6027\u56de\u5f52\u548cEDCDF\u65b9\u6cd5\u5bf9\u6bd4\u3002\u8bad\u7ec3\u6570\u636e\u4e3a1950-2014\u5e74\u5386\u53f2\u6570\u636e\u548c2015-2020\u5e74\u9884\u6d4b\u6570\u636e\uff0c\u6d4b\u8bd5\u6570\u636e\u4e3a2021-2024\u5e74\u3002", "result": "UNet\u67b6\u6784\u5728\u53bb\u9664\u6c14\u5019\u80cc\u666f\u540e\u8868\u73b0\u6700\u4f73\uff0c\u5176\u504f\u5dee\u6821\u6b63\u6548\u679c\u6bd4EDCDF\u65b9\u6cd5\u63d0\u5347\u4e8615%\u7684RMSE\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u6c14\u5019\u6a21\u578b\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0cUNet\u662f\u6821\u6b63SST\u504f\u5dee\u7684\u6700\u4f18\u9009\u62e9\u3002"}}
{"id": "2504.19460", "pdf": "https://arxiv.org/pdf/2504.19460", "abs": "https://arxiv.org/abs/2504.19460", "authors": ["Mahya Khazaei", "Ali Bahrani", "George Tzanetakis"], "title": "A Real-Time Gesture-Based Control Framework", "categories": ["cs.HC", "cs.AI"], "comment": "8 pages, 4 figures, 2025 International Computer Music Conference", "summary": "We introduce a real-time, human-in-the-loop gesture control framework that\ncan dynamically adapt audio and music based on human movement by analyzing live\nvideo input. By creating a responsive connection between visual and auditory\nstimuli, this system enables dancers and performers to not only respond to\nmusic but also influence it through their movements. Designed for live\nperformances, interactive installations, and personal use, it offers an\nimmersive experience where users can shape the music in real time.\n  The framework integrates computer vision and machine learning techniques to\ntrack and interpret motion, allowing users to manipulate audio elements such as\ntempo, pitch, effects, and playback sequence. With ongoing training, it\nachieves user-independent functionality, requiring as few as 50 to 80 samples\nto label simple gestures. This framework combines gesture training, cue\nmapping, and audio manipulation to create a dynamic, interactive experience.\nGestures are interpreted as input signals, mapped to sound control commands,\nand used to naturally adjust music elements, showcasing the seamless interplay\nbetween human interaction and machine response.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5b9e\u65f6\u7684\u624b\u52bf\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u5b9e\u65f6\u89c6\u9891\u8f93\u5165\u52a8\u6001\u8c03\u6574\u97f3\u9891\u548c\u97f3\u4e50\uff0c\u5b9e\u73b0\u4eba\u673a\u4ea4\u4e92\u3002", "motivation": "\u65e8\u5728\u4e3a\u821e\u8e48\u3001\u8868\u6f14\u7b49\u573a\u666f\u63d0\u4f9b\u6c89\u6d78\u5f0f\u4f53\u9a8c\uff0c\u8ba9\u7528\u6237\u901a\u8fc7\u52a8\u4f5c\u5b9e\u65f6\u5f71\u54cd\u97f3\u4e50\u3002", "method": "\u6574\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u901a\u8fc7\u624b\u52bf\u8bad\u7ec3\u3001\u4fe1\u53f7\u6620\u5c04\u548c\u97f3\u9891\u5904\u7406\u5b9e\u73b0\u52a8\u6001\u4ea4\u4e92\u3002", "result": "\u7cfb\u7edf\u80fd\u72ec\u7acb\u4e8e\u7528\u6237\u5de5\u4f5c\uff0c\u4ec5\u970050-80\u4e2a\u6837\u672c\u5373\u53ef\u8bc6\u522b\u7b80\u5355\u624b\u52bf\uff0c\u5b9e\u65f6\u8c03\u6574\u97f3\u4e50\u5143\u7d20\u3002", "conclusion": "\u6846\u67b6\u5b9e\u73b0\u4e86\u4eba\u673a\u65e0\u7f1d\u4e92\u52a8\uff0c\u5c55\u793a\u4e86\u52a8\u4f5c\u4e0e\u97f3\u4e50\u63a7\u5236\u7684\u81ea\u7136\u7ed3\u5408\u3002"}}
{"id": "2504.19475", "pdf": "https://arxiv.org/pdf/2504.19475", "abs": "https://arxiv.org/abs/2504.19475", "authors": ["Sonia Joseph", "Praneet Suresh", "Lorenz Hufe", "Edward Stevinson", "Robert Graham", "Yash Vadi", "Danilo Bzdok", "Sebastian Lapuschkin", "Lee Sharkey", "Blake Aaron Richards"], "title": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "4 pages, 3 figures, 9 tables. Oral and Tutorial at the CVPR\n  Mechanistic Interpretability for Vision (MIV) Workshop", "summary": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field.", "AI": {"tldr": "Prisma\u662f\u4e00\u4e2a\u52a0\u901f\u89c6\u89c9\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u63d0\u4f9b75+\u89c6\u89c9\u548c\u89c6\u9891Transformer\u5de5\u5177\u5305\u300180+\u9884\u8bad\u7ec3\u7a00\u758f\u81ea\u7f16\u7801\u5668\u6743\u91cd\u53ca\u5206\u6790\u5de5\u5177\uff0c\u7814\u7a76\u53d1\u73b0\u89c6\u89c9SAE\u53ef\u5448\u73b0\u6bd4\u8bed\u8a00SAE\u66f4\u4f4e\u7684\u7a00\u758f\u6027\uff0c\u4e14\u67d0\u4e9b\u60c5\u51b5\u4e0b\u80fd\u964d\u4f4e\u6a21\u578b\u635f\u5931\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u6613\u7528\u6846\u67b6\u548c\u9884\u8bad\u7ec3\u6743\u91cd\uff0c\u89c6\u89c9\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u8fdb\u5c55\u7f13\u6162\uff0cPrisma\u65e8\u5728\u901a\u8fc7\u5f00\u6e90\u5de5\u5177\u548c\u8d44\u6e90\u964d\u4f4e\u8be5\u9886\u57df\u95e8\u69db\u3002", "method": "\u5f00\u53d1\u7edf\u4e00\u5de5\u5177\u5305\uff08\u652f\u6301SAE\u3001\u8f6c\u7801\u5668/\u8de8\u7801\u5668\u8bad\u7ec3\uff09\u3001\u9884\u8bad\u7ec3\u6743\u91cd\u3001\u6fc0\u6d3b\u7f13\u5b58\u3001\u7535\u8def\u5206\u6790\u548c\u53ef\u89c6\u5316\u5de5\u5177\u3002", "result": "\u53d1\u73b0\u89c6\u89c9SAE\u53ef\u5c55\u73b0\u66f4\u4f4e\u7a00\u758f\u6027\uff0c\u90e8\u5206SAE\u91cd\u5efa\u80fd\u51cf\u5c11\u6a21\u578b\u635f\u5931\u3002", "conclusion": "Prisma\u4e3a\u7406\u89e3\u89c6\u89c9\u6a21\u578b\u5185\u90e8\u673a\u5236\u5f00\u8f9f\u65b0\u65b9\u5411\uff0c\u540c\u65f6\u964d\u4f4e\u7814\u7a76\u95e8\u69db\u3002"}}
{"id": "2504.19231", "pdf": "https://arxiv.org/pdf/2504.19231", "abs": "https://arxiv.org/abs/2504.19231", "authors": ["Alexander Dubbs"], "title": "Test Set Sizing for the Ridge Regression", "categories": ["stat.ML", "cs.LG", "math.PR"], "comment": null, "summary": "We derive the ideal train/test split for the ridge regression to high\naccuracy in the limit that the number of training rows m becomes large. The\nsplit must depend on the ridge tuning parameter, alpha, but we find that the\ndependence is weak and can asymptotically be ignored; all parameters vanish\nexcept for m and the number of features, n. This is the first time that such a\nsplit is calculated mathematically for a machine learning model in the large\ndata limit. The goal of the calculations is to maximize \"integrity,\" so that\nthe measured error in the trained model is as close as possible to what it\ntheoretically should be. This paper's result for the ridge regression split\nmatches prior art for the plain vanilla linear regression split to the first\ntwo terms asymptotically, and it appears that practically there is no\ndifference.", "AI": {"tldr": "\u8bba\u6587\u63a8\u5bfc\u4e86\u5cad\u56de\u5f52\u5728\u8bad\u7ec3\u6837\u672c\u6570m\u8d8b\u8fd1\u4e8e\u65e0\u7a77\u5927\u65f6\u7684\u7406\u60f3\u8bad\u7ec3/\u6d4b\u8bd5\u5206\u5272\u6bd4\u4f8b\uff0c\u7ed3\u679c\u8868\u660e\u5206\u5272\u6bd4\u4f8b\u5bf9\u5cad\u53c2\u6570\u03b1\u7684\u4f9d\u8d56\u8f83\u5f31\u4e14\u53ef\u6e10\u8fd1\u5ffd\u7565\uff0c\u4ec5\u4f9d\u8d56m\u548c\u7279\u5f81\u6570n\u3002\u8fd9\u662f\u9996\u6b21\u5728\u5927\u6570\u636e\u6781\u9650\u4e0b\u5bf9\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5206\u5272\u6bd4\u4f8b\u8fdb\u884c\u6570\u5b66\u8ba1\u7b97\u3002", "motivation": "\u7814\u7a76\u7684\u76ee\u7684\u662f\u6700\u5927\u5316\u6a21\u578b\u7684\u2018\u5b8c\u6574\u6027\u2019\uff0c\u5373\u786e\u4fdd\u8bad\u7ec3\u6a21\u578b\u7684\u6d4b\u91cf\u8bef\u5dee\u5c3d\u53ef\u80fd\u63a5\u8fd1\u5176\u7406\u8bba\u503c\u3002\u901a\u8fc7\u6570\u5b66\u8ba1\u7b97\u786e\u5b9a\u5cad\u56de\u5f52\u7684\u7406\u60f3\u8bad\u7ec3/\u6d4b\u8bd5\u5206\u5272\u6bd4\u4f8b\u3002", "method": "\u8bba\u6587\u5728\u8bad\u7ec3\u6837\u672c\u6570m\u8d8b\u8fd1\u4e8e\u65e0\u7a77\u5927\u7684\u6781\u9650\u4e0b\uff0c\u63a8\u5bfc\u4e86\u5cad\u56de\u5f52\u7684\u7406\u60f3\u8bad\u7ec3/\u6d4b\u8bd5\u5206\u5272\u6bd4\u4f8b\uff0c\u5e76\u5206\u6790\u4e86\u5206\u5272\u6bd4\u4f8b\u5bf9\u5cad\u53c2\u6570\u03b1\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5206\u5272\u6bd4\u4f8b\u5bf9\u5cad\u53c2\u6570\u03b1\u7684\u4f9d\u8d56\u8f83\u5f31\u4e14\u53ef\u6e10\u8fd1\u5ffd\u7565\uff0c\u4ec5\u4f9d\u8d56m\u548c\u7279\u5f81\u6570n\u3002\u8fd9\u4e00\u7ed3\u679c\u4e0e\u666e\u901a\u7ebf\u6027\u56de\u5f52\u7684\u5206\u5272\u6bd4\u4f8b\u5728\u524d\u4e24\u9879\u6e10\u8fd1\u9879\u4e0a\u4e00\u81f4\uff0c\u5b9e\u8df5\u4e2d\u51e0\u4e4e\u65e0\u5dee\u522b\u3002", "conclusion": "\u8bba\u6587\u9996\u6b21\u5728\u5927\u6570\u636e\u6781\u9650\u4e0b\u4e3a\u5cad\u56de\u5f52\u63d0\u4f9b\u4e86\u6570\u5b66\u4e0a\u7684\u7406\u60f3\u8bad\u7ec3/\u6d4b\u8bd5\u5206\u5272\u6bd4\u4f8b\uff0c\u7ed3\u679c\u8868\u660e\u5176\u5bf9\u5cad\u53c2\u6570\u7684\u4f9d\u8d56\u53ef\u5ffd\u7565\uff0c\u4e14\u4e0e\u666e\u901a\u7ebf\u6027\u56de\u5f52\u7ed3\u679c\u4e00\u81f4\u3002"}}
{"id": "2504.19239", "pdf": "https://arxiv.org/pdf/2504.19239", "abs": "https://arxiv.org/abs/2504.19239", "authors": ["Yoshiaki Kawase"], "title": "The effect of the number of parameters and the number of local feature patches on loss landscapes in distributed quantum neural networks", "categories": ["quant-ph", "cs.LG"], "comment": "9 pages + Appendices", "summary": "Quantum neural networks hold promise for tackling computationally challenging\ntasks that are intractable for classical computers. However, their practical\napplication is hindered by significant optimization challenges, arising from\ncomplex loss landscapes characterized by barren plateaus and numerous local\nminima. These problems become more severe as the number of parameters or qubits\nincreases, hampering effective training. To mitigate these optimization\nchallenges, particularly for quantum machine learning applied to classical\ndata, we employ an approach of distributing overlapping local patches across\nmultiple quantum neural networks, processing each patch with an independent\nquantum neural network, and aggregating their outputs for prediction. In this\nstudy, we investigate how the number of parameters and patches affects the loss\nlandscape geometry of this distributed quantum neural network architecture via\nHessian analysis and loss landscape visualization. Our results confirm that\nincreasing the number of parameters tends to lead to deeper and sharper loss\nlandscapes. Crucially, we demonstrate that increasing the number of patches\nsignificantly reduces the largest Hessian eigenvalue at minima. This finding\nsuggests that our distributed patch approach acts as a form of implicit\nregularization, promoting optimization stability and potentially enhancing\ngeneralization. Our study provides valuable insights into optimization\nchallenges and highlights that the distributed patch approach is a promising\nstrategy for developing more trainable and practical quantum machine learning\nmodels for classical data tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u5c06\u6570\u636e\u5206\u5272\u4e3a\u5c40\u90e8\u5757\u5e76\u72ec\u7acb\u5904\u7406\uff0c\u4ee5\u7f13\u89e3\u4f18\u5316\u95ee\u9898\uff08\u5982\u8d2b\u7620\u9ad8\u539f\u548c\u5c40\u90e8\u6781\u5c0f\u503c\uff09\uff0c\u5e76\u901a\u8fc7Hessian\u5206\u6790\u548c\u635f\u5931\u666f\u89c2\u53ef\u89c6\u5316\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u5728\u5904\u7406\u7ecf\u5178\u6570\u636e\u65f6\u9762\u4e34\u4f18\u5316\u96be\u9898\uff08\u5982\u8d2b\u7620\u9ad8\u539f\u548c\u5c40\u90e8\u6781\u5c0f\u503c\uff09\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5206\u5e03\u5f0f\u5757\u5904\u7406\u65b9\u6cd5\u63d0\u5347\u4f18\u5316\u7a33\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u5206\u5e03\u5f0f\u5757\u7b56\u7565\uff0c\u5c06\u6570\u636e\u5206\u5272\u4e3a\u91cd\u53e0\u5c40\u90e8\u5757\uff0c\u7531\u72ec\u7acb\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u5904\u7406\uff0c\u5e76\u805a\u5408\u8f93\u51fa\u3002\u901a\u8fc7Hessian\u5206\u6790\u548c\u635f\u5931\u666f\u89c2\u53ef\u89c6\u5316\u7814\u7a76\u53c2\u6570\u4e0e\u5757\u6570\u91cf\u7684\u5f71\u54cd\u3002", "result": "\u589e\u52a0\u53c2\u6570\u4f1a\u52a0\u6df1\u635f\u5931\u666f\u89c2\uff0c\u800c\u589e\u52a0\u5757\u6570\u91cf\u663e\u8457\u964d\u4f4eHessian\u6700\u5927\u7279\u5f81\u503c\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u9690\u5f0f\u6b63\u5219\u5316\u6548\u679c\uff0c\u63d0\u5347\u4f18\u5316\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5206\u5e03\u5f0f\u5757\u7b56\u7565\u662f\u89e3\u51b3\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4e3a\u7ecf\u5178\u6570\u636e\u4efb\u52a1\u4e2d\u7684\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u53ef\u884c\u7684\u8bad\u7ec3\u65b9\u6848\u3002"}}
{"id": "2504.19545", "pdf": "https://arxiv.org/pdf/2504.19545", "abs": "https://arxiv.org/abs/2504.19545", "authors": ["Zezeng Li", "Zhihui Qi", "Weimin Wang", "Ziliang Wang", "Junyi Duan", "Na Lei"], "title": "Point2Quad: Generating Quad Meshes from Point Clouds via Face Prediction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Quad meshes are essential in geometric modeling and computational mechanics.\nAlthough learning-based methods for triangle mesh demonstrate considerable\nadvancements, quad mesh generation remains less explored due to the challenge\nof ensuring coplanarity, convexity, and quad-only meshes. In this paper, we\npresent Point2Quad, the first learning-based method for quad-only mesh\ngeneration from point clouds. The key idea is learning to identify quad mesh\nwith fused pointwise and facewise features. Specifically, Point2Quad begins\nwith a k-NN-based candidate generation considering the coplanarity and\nsquareness. Then, two encoders are followed to extract geometric and\ntopological features that address the challenge of quad-related constraints,\nespecially by combining in-depth quadrilaterals-specific characteristics.\nSubsequently, the extracted features are fused to train the classifier with a\ndesigned compound loss. The final results are derived after the refinement by a\nquad-specific post-processing. Extensive experiments on both clear and noise\ndata demonstrate the effectiveness and superiority of Point2Quad, compared to\nbaseline methods under comprehensive metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPoint2Quad\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u4ece\u70b9\u4e91\u751f\u6210\u7eaf\u56db\u8fb9\u5f62\u7f51\u683c\uff0c\u901a\u8fc7\u878d\u5408\u70b9\u7ea7\u548c\u9762\u7ea7\u7279\u5f81\u89e3\u51b3\u56db\u8fb9\u5f62\u7f51\u683c\u751f\u6210\u4e2d\u7684\u5171\u9762\u6027\u3001\u51f8\u6027\u548c\u7eaf\u56db\u8fb9\u5f62\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u56db\u8fb9\u5f62\u7f51\u683c\u5728\u51e0\u4f55\u5efa\u6a21\u548c\u8ba1\u7b97\u529b\u5b66\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u591a\u96c6\u4e2d\u4e8e\u4e09\u89d2\u7f51\u683c\uff0c\u7eaf\u56db\u8fb9\u5f62\u7f51\u683c\u751f\u6210\u7684\u7814\u7a76\u8f83\u5c11\uff0c\u4e3b\u8981\u7531\u4e8e\u5176\u5171\u9762\u6027\u3001\u51f8\u6027\u548c\u7eaf\u56db\u8fb9\u5f62\u9650\u5236\u7684\u6280\u672f\u6311\u6218\u3002", "method": "Point2Quad\u91c7\u7528k-NN\u5019\u9009\u751f\u6210\u7b56\u7565\uff0c\u7ed3\u5408\u5171\u9762\u6027\u548c\u65b9\u5f62\u5ea6\uff0c\u968f\u540e\u901a\u8fc7\u4e24\u4e2a\u7f16\u7801\u5668\u63d0\u53d6\u51e0\u4f55\u548c\u62d3\u6251\u7279\u5f81\uff0c\u878d\u5408\u540e\u8bad\u7ec3\u5206\u7c7b\u5668\uff0c\u5e76\u8bbe\u8ba1\u4e86\u590d\u5408\u635f\u5931\u51fd\u6570\uff0c\u6700\u540e\u901a\u8fc7\u56db\u8fb9\u5f62\u4e13\u7528\u540e\u5904\u7406\u4f18\u5316\u7ed3\u679c\u3002", "result": "\u5728\u6e05\u6670\u548c\u566a\u58f0\u6570\u636e\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cPoint2Quad\u5728\u7efc\u5408\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u73b0\u4e86\u5176\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "Point2Quad\u662f\u7b2c\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u7eaf\u56db\u8fb9\u5f62\u7f51\u683c\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u7279\u5f81\u878d\u5408\u548c\u635f\u5931\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u56db\u8fb9\u5f62\u7f51\u683c\u751f\u6210\u7684\u96be\u9898\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2504.19264", "pdf": "https://arxiv.org/pdf/2504.19264", "abs": "https://arxiv.org/abs/2504.19264", "authors": ["Angel Mary John", "Jerrin Thomas Panachakel", "Anusha S. P"], "title": "Navigating AI Policy Landscapes: Insights into Human Rights Considerations Across IEEE Regions", "categories": ["cs.CY", "cs.LG"], "comment": "2024 IEEE 12th Region 10 Humanitarian Technology Conference\n  (R10-HTC). IEEE, 2024", "summary": "This paper explores the integration of human rights considerations into AI\nregulatory frameworks across different IEEE regions - specifically the United\nStates (Region 1-6), Europe (Region 8), China (part of Region 10), and\nSingapore (part of Region 10). While all acknowledge the transformative\npotential of AI and the necessity of ethical guidelines, their regulatory\napproaches significantly differ. Europe exhibits a rigorous framework with\nstringent protections for individual rights, while the U.S. promotes innovation\nwith less restrictive regulations. China emphasizes state control and societal\norder in its AI strategies. In contrast, Singapore's advisory framework\nencourages self-regulation and aligns closely with international norms. This\ncomparative analysis underlines the need for ongoing global dialogue to\nharmonize AI regulations that safeguard human rights while promoting\ntechnological advancement, reflecting the diverse perspectives and priorities\nof each region.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u7f8e\u56fd\u3001\u6b27\u6d32\u3001\u4e2d\u56fd\u548c\u65b0\u52a0\u5761\u5728AI\u76d1\u7ba1\u6846\u67b6\u4e2d\u7eb3\u5165\u4eba\u6743\u8003\u91cf\u7684\u5dee\u5f02\uff0c\u5f3a\u8c03\u5168\u7403\u5bf9\u8bdd\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u63a2\u8ba8\u4e0d\u540c\u5730\u533a\u5982\u4f55\u5c06\u4eba\u6743\u7eb3\u5165AI\u76d1\u7ba1\uff0c\u63ed\u793a\u5176\u5dee\u5f02\u80cc\u540e\u7684\u52a8\u673a\u548c\u4f18\u5148\u4e8b\u9879\u3002", "method": "\u901a\u8fc7\u5bf9IEEE\u533a\u57df\uff08\u7f8e\u56fd\u3001\u6b27\u6d32\u3001\u4e2d\u56fd\u3001\u65b0\u52a0\u5761\uff09\u7684AI\u76d1\u7ba1\u6846\u67b6\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u6b27\u6d32\u76d1\u7ba1\u4e25\u683c\u4fdd\u62a4\u4e2a\u4eba\u6743\u5229\uff0c\u7f8e\u56fd\u5bbd\u677e\u4ee5\u4fc3\u8fdb\u521b\u65b0\uff0c\u4e2d\u56fd\u4fa7\u91cd\u56fd\u5bb6\u63a7\u5236\u4e0e\u793e\u4f1a\u79e9\u5e8f\uff0c\u65b0\u52a0\u5761\u63d0\u5021\u81ea\u6211\u76d1\u7ba1\u4e0e\u56fd\u9645\u63a5\u8f68\u3002", "conclusion": "\u9700\u5168\u7403\u534f\u4f5c\u4ee5\u5e73\u8861\u4eba\u6743\u4fdd\u62a4\u4e0e\u6280\u672f\u8fdb\u6b65\uff0c\u5c0a\u91cd\u5404\u5730\u533a\u7684\u591a\u6837\u6027\u548c\u4f18\u5148\u7ea7\u3002"}}
{"id": "2504.19592", "pdf": "https://arxiv.org/pdf/2504.19592", "abs": "https://arxiv.org/abs/2504.19592", "authors": ["Roman Malashin", "Daniil Ilyukhin"], "title": "Neural network task specialization via domain constraining", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper introduces a concept of neural network specialization via\ntask-specific domain constraining, aimed at enhancing network performance on\ndata subspace in which the network operates. The study presents experiments on\ntraining specialists for image classification and object detection tasks. The\nresults demonstrate that specialization can enhance a generalist's accuracy\neven without additional data or changing training regimes: solely by\nconstraining class label space in which the network performs. Theoretical and\nexperimental analyses indicate that effective specialization requires modifying\ntraditional fine-tuning methods and constraining data space to semantically\ncoherent subsets. The specialist extraction phase before tuning the network is\nproposed for maximal performance gains. We also provide analysis of the\nevolution of the feature space during specialization. This study paves way to\nfuture research for developing more advanced dynamically configurable image\nanalysis systems, where computations depend on the specific input.\nAdditionally, the proposed methods can help improve system performance in\nscenarios where certain data domains should be excluded from consideration of\nthe generalist network.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u9886\u57df\u7ea6\u675f\u6765\u589e\u5f3a\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u8bc1\u660e\u5728\u4e0d\u589e\u52a0\u6570\u636e\u6216\u6539\u53d8\u8bad\u7ec3\u673a\u5236\u7684\u60c5\u51b5\u4e0b\uff0c\u4ec5\u901a\u8fc7\u7ea6\u675f\u7c7b\u522b\u6807\u7b7e\u7a7a\u95f4\u5373\u53ef\u63d0\u5347\u901a\u7528\u7f51\u7edc\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u795e\u7ecf\u7f51\u7edc\u5728\u7279\u5b9a\u6570\u636e\u5b50\u7a7a\u95f4\u4e0a\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u6539\u53d8\u73b0\u6709\u8bad\u7ec3\u6846\u67b6\u6216\u589e\u52a0\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u7ea6\u675f\u7c7b\u522b\u6807\u7b7e\u7a7a\u95f4\u6765\u5b9e\u73b0\u7f51\u7edc\u7684\u4e13\u4e1a\u5316\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4fee\u6539\u4f20\u7edf\u7684\u5fae\u8c03\u65b9\u6cd5\u3001\u7ea6\u675f\u6570\u636e\u7a7a\u95f4\u5230\u8bed\u4e49\u8fde\u8d2f\u7684\u5b50\u96c6\uff0c\u5e76\u5728\u7f51\u7edc\u8c03\u4f18\u524d\u8fdb\u884c\u4e13\u5bb6\u63d0\u53d6\u9636\u6bb5\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u901a\u7528\u7f51\u7edc\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u5206\u6790\u4e86\u7279\u5f81\u7a7a\u95f4\u5728\u4e13\u4e1a\u5316\u8fc7\u7a0b\u4e2d\u7684\u6f14\u53d8\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u5148\u8fdb\u7684\u52a8\u6001\u53ef\u914d\u7f6e\u56fe\u50cf\u5206\u6790\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5e76\u53ef\u5728\u7279\u5b9a\u6570\u636e\u57df\u6392\u9664\u573a\u666f\u4e2d\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2504.19594", "pdf": "https://arxiv.org/pdf/2504.19594", "abs": "https://arxiv.org/abs/2504.19594", "authors": ["Lorenzo Alvisi", "Serena Tardelli", "Maurizio Tesconi"], "title": "Mapping the Italian Telegram Ecosystem", "categories": ["cs.SI", "cs.AI"], "comment": null, "summary": "Telegram has become a major space for political discourse and alternative\nmedia. However, its lack of moderation allows misinformation, extremism, and\ntoxicity to spread. While prior research focused on these particular phenomena\nor topics, these have mostly been examined separately, and a broader\nunderstanding of the Telegram ecosystem is still missing. In this work, we fill\nthis gap by conducting a large-scale analysis of the Italian Telegram sphere,\nleveraging a dataset of 186 million messages from 13,151 chats collected in\n2023. Using network analysis, Large Language Models, and toxicity detection\ntools, we examine how different thematic communities form, align ideologically,\nand engage in harmful discourse within the Italian cultural context. Results\nshow strong thematic and ideological homophily. We also identify mixed\nideological communities where far-left and far-right rhetoric coexist on\nparticular geopolitical issues. Beyond political analysis, we find that\ntoxicity, rather than being isolated in a few extreme chats, appears widely\nnormalized within highly toxic communities. Moreover, we find that Italian\ndiscourse primarily targets Black people, Jews, and gay individuals\nindependently of the topic. Finally, we uncover common trend of intra-national\nhostility, where Italians often attack other Italians, reflecting regional and\nintra-regional cultural conflicts that can be traced back to old historical\ndivisions. This study provides the first large-scale mapping of the Italian\nTelegram ecosystem, offering insights into ideological interactions, toxicity,\nand identity-targets of hate and contributing to research on online toxicity\nacross different cultural and linguistic contexts on Telegram.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790\u610f\u5927\u5229Telegram\u5e73\u53f0\u4e0a\u76841.86\u4ebf\u6761\u6d88\u606f\uff0c\u63ed\u793a\u4e86\u8be5\u5e73\u53f0\u7684\u653f\u6cbb\u751f\u6001\u3001\u610f\u8bc6\u5f62\u6001\u540c\u8d28\u6027\u3001\u6bd2\u6027\u8a00\u8bba\u7684\u666e\u904d\u6027\u4ee5\u53ca\u5bf9\u7279\u5b9a\u7fa4\u4f53\u7684\u4ec7\u6068\u8a00\u8bba\uff0c\u586b\u8865\u4e86\u76f8\u5173\u7814\u7a76\u7a7a\u767d\u3002", "motivation": "Telegram\u5df2\u6210\u4e3a\u653f\u6cbb\u8ba8\u8bba\u548c\u53e6\u7c7b\u5a92\u4f53\u7684\u91cd\u8981\u7a7a\u95f4\uff0c\u4f46\u5176\u7f3a\u4e4f\u5185\u5bb9\u5ba1\u6838\u5bfc\u81f4\u9519\u8bef\u4fe1\u606f\u3001\u6781\u7aef\u4e3b\u4e49\u548c\u6bd2\u6027\u8a00\u8bba\u6cdb\u6ee5\u3002\u4ee5\u5f80\u7814\u7a76\u591a\u805a\u7126\u4e8e\u5355\u4e00\u73b0\u8c61\uff0c\u7f3a\u4e4f\u5bf9Telegram\u751f\u6001\u7684\u6574\u4f53\u7406\u89e3\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5229\u7528\u7f51\u7edc\u5206\u6790\u3001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6bd2\u6027\u68c0\u6d4b\u5de5\u5177\uff0c\u5bf92023\u5e74\u6536\u96c6\u768413,151\u4e2a\u804a\u5929\u4e2d\u76841.86\u4ebf\u6761\u6d88\u606f\u8fdb\u884c\u5206\u6790\uff0c\u7814\u7a76\u610f\u5927\u5229Telegram\u4e2d\u7684\u4e3b\u9898\u793e\u533a\u5f62\u6210\u3001\u610f\u8bc6\u5f62\u6001\u5bf9\u9f50\u53ca\u6709\u5bb3\u8a00\u8bba\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f3a\u70c8\u7684\u4e3b\u9898\u548c\u610f\u8bc6\u5f62\u6001\u540c\u8d28\u6027\uff1b\u6df7\u5408\u610f\u8bc6\u5f62\u6001\u793e\u533a\u4e2d\u5b58\u5728\u6781\u5de6\u548c\u6781\u53f3\u8a00\u8bba\u5171\u5b58\uff1b\u6bd2\u6027\u8a00\u8bba\u5728\u9ad8\u5ea6\u6bd2\u6027\u793e\u533a\u4e2d\u88ab\u5e7f\u6cdb\u6b63\u5e38\u5316\uff1b\u4ec7\u6068\u8a00\u8bba\u4e3b\u8981\u9488\u5bf9\u9ed1\u4eba\u3001\u72b9\u592a\u4eba\u548c\u540c\u6027\u604b\u8005\uff1b\u610f\u5927\u5229\u4eba\u4e4b\u95f4\u5b58\u5728\u5730\u57df\u6027\u654c\u5bf9\u884c\u4e3a\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u5927\u89c4\u6a21\u7ed8\u5236\u4e86\u610f\u5927\u5229Telegram\u751f\u6001\uff0c\u4e3a\u7814\u7a76\u4e0d\u540c\u6587\u5316\u548c\u8bed\u8a00\u80cc\u666f\u4e0b\u7684\u5728\u7ebf\u6bd2\u6027\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2504.19595", "pdf": "https://arxiv.org/pdf/2504.19595", "abs": "https://arxiv.org/abs/2504.19595", "authors": ["Pietro Bongini", "Sara Mandelli", "Andrea Montibeller", "Mirko Casu", "Orazio Pontorno", "Claudio Ragaglia", "Luca Zanchetta", "Mattia Aquilina", "Taiba Majid Wani", "Luca Guarnera", "Benedetta Tondi", "Paolo Bestagini", "Irene Amerini", "Francesco Denatale", "Sebastiano Battiato", "Mauro Barni"], "title": "WILD: a new in-the-Wild Image Linkage Dataset for synthetic image attribution", "categories": ["cs.MM", "cs.AI", "cs.CV"], "comment": null, "summary": "Synthetic image source attribution is an open challenge, with an increasing\nnumber of image generators being released yearly. The complexity and the sheer\nnumber of available generative techniques, as well as the scarcity of\nhigh-quality open source datasets of diverse nature for this task, make\ntraining and benchmarking synthetic image source attribution models very\nchallenging. WILD is a new in-the-Wild Image Linkage Dataset designed to\nprovide a powerful training and benchmarking tool for synthetic image\nattribution models. The dataset is built out of a closed set of 10 popular\ncommercial generators, which constitutes the training base of attribution\nmodels, and an open set of 10 additional generators, simulating a real-world\nin-the-wild scenario. Each generator is represented by 1,000 images, for a\ntotal of 10,000 images in the closed set and 10,000 images in the open set.\nHalf of the images are post-processed with a wide range of operators. WILD\nallows benchmarking attribution models in a wide range of tasks, including\nclosed and open set identification and verification, and robust attribution\nwith respect to post-processing and adversarial attacks. Models trained on WILD\nare expected to benefit from the challenging scenario represented by the\ndataset itself. Moreover, an assessment of seven baseline methodologies on\nclosed and open set attribution is presented, including robustness tests with\nrespect to post-processing.", "AI": {"tldr": "WILD\u6570\u636e\u96c6\u4e3a\u5408\u6210\u56fe\u50cf\u6765\u6e90\u8bc6\u522b\u6a21\u578b\u63d0\u4f9b\u4e86\u8bad\u7ec3\u548c\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u5305\u542b\u5c01\u95ed\u96c6\u548c\u5f00\u653e\u96c6\u7684\u56fe\u50cf\uff0c\u652f\u6301\u591a\u79cd\u4efb\u52a1\u6d4b\u8bd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e03\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5408\u6210\u56fe\u50cf\u6765\u6e90\u8bc6\u522b\u662f\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\uff0c\u73b0\u6709\u751f\u6210\u6280\u672f\u590d\u6742\u4e14\u591a\u6837\uff0c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7a00\u7f3a\uff0cWILD\u6570\u636e\u96c6\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "WILD\u6570\u636e\u96c6\u5305\u542b\u5c01\u95ed\u96c6\uff0810\u4e2a\u751f\u6210\u5668\uff0c10,000\u5f20\u56fe\u50cf\uff09\u548c\u5f00\u653e\u96c6\uff0810\u4e2a\u751f\u6210\u5668\uff0c10,000\u5f20\u56fe\u50cf\uff09\uff0c\u90e8\u5206\u56fe\u50cf\u7ecf\u8fc7\u540e\u5904\u7406\u3002", "result": "\u8be5\u6570\u636e\u96c6\u652f\u6301\u591a\u79cd\u4efb\u52a1\u6d4b\u8bd5\uff0c\u6a21\u578b\u5728\u6b64\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6709\u671b\u63d0\u5347\u6027\u80fd\u3002\u4e03\u79cd\u57fa\u7ebf\u65b9\u6cd5\u5728\u5c01\u95ed\u548c\u5f00\u653e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5305\u62ec\u540e\u5904\u7406\u9c81\u68d2\u6027\u6d4b\u8bd5\u3002", "conclusion": "WILD\u4e3a\u5408\u6210\u56fe\u50cf\u6765\u6e90\u8bc6\u522b\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u548c\u6a21\u578b\u5f00\u53d1\u3002"}}
{"id": "2504.19342", "pdf": "https://arxiv.org/pdf/2504.19342", "abs": "https://arxiv.org/abs/2504.19342", "authors": ["Nan Lu", "Ethan X. Fang", "Junwei Lu"], "title": "Contextual Online Uncertainty-Aware Preference Learning for Human Feedback", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) has become a pivotal\nparadigm in artificial intelligence to align large models with human\npreferences. In this paper, we propose a novel statistical framework to\nsimultaneously conduct the online decision-making and statistical inference on\nthe optimal model using human preference data based on dynamic contextual\ninformation. Our approach introduces an efficient decision strategy that\nachieves both the optimal regret bound and the asymptotic distribution of the\nestimators. A key challenge in RLHF is handling the dependent online human\npreference outcomes with dynamic contexts. To address this, in the\nmethodological aspect, we propose a two-stage algorithm starting with\n$\\epsilon$-greedy followed by exploitations; in the theoretical aspect, we\ntailor anti-concentration inequalities and matrix martingale concentration\ntechniques to derive the uniform estimation rate and asymptotic normality of\nthe estimators using dependent samples from both stages. Extensive simulation\nresults demonstrate that our method outperforms state-of-the-art strategies. We\napply the proposed framework to analyze the human preference data for ranking\nlarge language models on the Massive Multitask Language Understanding dataset,\nyielding insightful results on the performance of different large language\nmodels for medical anatomy knowledge.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u7edf\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u57fa\u4e8e\u52a8\u6001\u4e0a\u4e0b\u6587\u7684\u4eba\u7c7b\u504f\u597d\u6570\u636e\u540c\u65f6\u8fdb\u884c\u5728\u7ebf\u51b3\u7b56\u548c\u7edf\u8ba1\u63a8\u65ad\uff0c\u4f18\u5316\u4e86\u9057\u61be\u8fb9\u754c\u548c\u4f30\u8ba1\u91cf\u7684\u6e10\u8fd1\u5206\u5e03\u3002", "motivation": "\u4e3a\u89e3\u51b3RLHF\u4e2d\u5904\u7406\u52a8\u6001\u4e0a\u4e0b\u6587\u4e0b\u4f9d\u8d56\u7684\u5728\u7ebf\u4eba\u7c7b\u504f\u597d\u7ed3\u679c\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u7b97\u6cd5\uff08\u03b5-\u8d2a\u5a6a\u7b56\u7565\u540e\u63a5\u5f00\u53d1\u9636\u6bb5\uff09\uff0c\u5e76\u8fd0\u7528\u6297\u96c6\u4e2d\u4e0d\u7b49\u5f0f\u548c\u77e9\u9635\u9785\u96c6\u4e2d\u6280\u672f\u8fdb\u884c\u7406\u8bba\u5206\u6790\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u591a\u4efb\u52a1\u8bed\u8a00\u7406\u89e3\u6570\u636e\u96c6\u7684\u5206\u6790\u3002", "conclusion": "\u65b0\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u52a8\u6001\u4e0a\u4e0b\u6587\u7684\u4eba\u7c7b\u504f\u597d\u6570\u636e\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u65b0\u5de5\u5177\u3002"}}
{"id": "2504.19598", "pdf": "https://arxiv.org/pdf/2504.19598", "abs": "https://arxiv.org/abs/2504.19598", "authors": ["Dou Quan", "Rufan Zhou", "Shuang Wang", "Ning Huyan", "Dong Zhao", "Yunan Li", "Licheng Jiao"], "title": "Lightweight Adapter Learning for More Generalized Remote Sensing Change Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep learning methods have shown promising performances in remote sensing\nimage change detection (CD). However, existing methods usually train a\ndataset-specific deep network for each dataset. Due to the significant\ndifferences in the data distribution and labeling between various datasets, the\ntrained dataset-specific deep network has poor generalization performances on\nother datasets. To solve this problem, this paper proposes a change adapter\nnetwork (CANet) for a more universal and generalized CD. CANet contains\ndataset-shared and dataset-specific learning modules. The former explores the\ndiscriminative features of images, and the latter designs a lightweight adapter\nmodel, to deal with the characteristics of different datasets in data\ndistribution and labeling. The lightweight adapter can quickly generalize the\ndeep network for new CD tasks with a small computation cost. Specifically, this\npaper proposes an interesting change region mask (ICM) in the adapter, which\ncan adaptively focus on interested change objects and decrease the influence of\nlabeling differences in various datasets. Moreover, CANet adopts a unique batch\nnormalization layer for each dataset to deal with data distribution\ndifferences. Compared with existing deep learning methods, CANet can achieve\nsatisfactory CD performances on various datasets simultaneously. Experimental\nresults on several public datasets have verified the effectiveness and\nadvantages of the proposed CANet on CD. CANet has a stronger generalization\nability, smaller training costs (merely updating 4.1%-7.7% parameters), and\nbetter performances under limited training datasets than other deep learning\nmethods, which also can be flexibly inserted with existing deep models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u53d8\u5316\u68c0\u6d4b\u7f51\u7edcCANet\uff0c\u5305\u542b\u6570\u636e\u96c6\u5171\u4eab\u548c\u7279\u5b9a\u6a21\u5757\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u548c\u5c0f\u8ba1\u7b97\u6210\u672c\u89e3\u51b3\u4e0d\u540c\u6570\u636e\u96c6\u95f4\u7684\u5206\u5e03\u548c\u6807\u6ce8\u5dee\u5f02\uff0c\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u9065\u611f\u56fe\u50cf\u53d8\u5316\u68c0\u6d4b\u4e2d\u56e0\u6570\u636e\u96c6\u5dee\u5f02\u5bfc\u81f4\u7684\u6cdb\u5316\u80fd\u529b\u5dee\u95ee\u9898\uff0c\u63d0\u51fa\u66f4\u901a\u7528\u7684\u68c0\u6d4b\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4e86CANet\u7f51\u7edc\uff0c\u5305\u542b\u6570\u636e\u96c6\u5171\u4eab\u5b66\u4e60\u6a21\u5757\u548c\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\uff0c\u5f15\u5165\u53d8\u5316\u533a\u57df\u63a9\u7801\uff08ICM\uff09\u548c\u72ec\u7279\u6279\u5f52\u4e00\u5316\u5c42\u5904\u7406\u6570\u636e\u5206\u5e03\u4e0e\u6807\u6ce8\u5dee\u5f02\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86CANet\u7684\u6709\u6548\u6027\uff0c\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff0c\u8bad\u7ec3\u6210\u672c\u66f4\u4f4e\uff08\u4ec5\u66f4\u65b04.1%-7.7%\u53c2\u6570\uff09\uff0c\u5e76\u4e14\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "CANet\u901a\u8fc7\u9002\u914d\u5668\u548c\u5171\u4eab\u6a21\u5757\u7684\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53d8\u5316\u68c0\u6d4b\u7684\u901a\u7528\u6027\u548c\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6570\u636e\u96c6\uff0c\u4e14\u517c\u5bb9\u73b0\u6709\u6df1\u5ea6\u6a21\u578b\u3002"}}
{"id": "2504.19351", "pdf": "https://arxiv.org/pdf/2504.19351", "abs": "https://arxiv.org/abs/2504.19351", "authors": ["Chathurika S Abeykoon", "Aleksandr Beknazaryan", "Hailin Sang"], "title": "The Double Descent Behavior in Two Layer Neural Network for Binary Classification", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Recent studies observed a surprising concept on model test error called the\ndouble descent phenomenon, where the increasing model complexity decreases the\ntest error first and then the error increases and decreases again. To observe\nthis, we work on a two layer neural network model with a ReLU activation\nfunction designed for binary classification under supervised learning. Our aim\nis to observe and investigate the mathematical theory behind the double descent\nbehavior of model test error for varying model sizes. We quantify the model\nsize by the ratio of number of training samples to the dimension of the model.\nDue to the complexity of the empirical risk minimization procedure, we use the\nConvex Gaussian Min Max Theorem to find a suitable candidate for the global\ntraining loss.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6a21\u578b\u6d4b\u8bd5\u8bef\u5dee\u4e2d\u7684\u201c\u53cc\u4e0b\u964d\u201d\u73b0\u8c61\uff0c\u5373\u6a21\u578b\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u8bef\u5dee\u5148\u964d\u540e\u5347\u518d\u964d\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u4e0b\u4e24\u5c42ReLU\u7f51\u7edc\u7684\u6570\u5b66\u7406\u8bba\u3002", "motivation": "\u76ee\u6807\u662f\u63a2\u7a76\u4e0d\u540c\u6a21\u578b\u5c3a\u5bf8\u4e0b\u6d4b\u8bd5\u8bef\u5dee\u53cc\u4e0b\u964d\u884c\u4e3a\u7684\u6570\u5b66\u7406\u8bba\uff0c\u5c24\u5176\u5173\u6ce8\u8bad\u7ec3\u6837\u672c\u6570\u4e0e\u6a21\u578b\u7ef4\u5ea6\u6bd4\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u4e24\u5c42ReLU\u795e\u7ecf\u7f51\u7edc\u548c\u51f8\u9ad8\u65af\u6781\u5c0f\u6781\u5927\u5b9a\u7406\uff08CGMT\uff09\u5206\u6790\u5168\u5c40\u8bad\u7ec3\u635f\u5931\u7684\u5019\u9009\u89e3\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u91cf\u5316\u4e86\u6a21\u578b\u5c3a\u5bf8\u5bf9\u6d4b\u8bd5\u8bef\u5dee\u53cc\u4e0b\u964d\u73b0\u8c61\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3\u590d\u6742\u6a21\u578b\u5728\u8bad\u7ec3\u4e2d\u7684\u8bef\u5dee\u884c\u4e3a\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5c3a\u5bf8\u4e0e\u6d4b\u8bd5\u8bef\u5dee\u7684\u975e\u5355\u8c03\u5173\u7cfb\u3002"}}
{"id": "2504.19600", "pdf": "https://arxiv.org/pdf/2504.19600", "abs": "https://arxiv.org/abs/2504.19600", "authors": ["Pengfei Zhang", "Shouqing Jia"], "title": "Image Generation Method Based on Heat Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Denoising Diffusion Probabilistic Models (DDPMs) achieve high-quality image\ngeneration without adversarial training, but they process images as a whole.\nSince adjacent pixels are highly likely to belong to the same object, we\npropose the Heat Diffusion Model (HDM) to further preserve image details and\ngenerate more realistic images. HDM is a model that incorporates pixel-level\noperations while maintaining the same training process as DDPM. In HDM, the\ndiscrete form of the two-dimensional heat equation is integrated into the\ndiffusion and generation formulas of DDPM, enabling the model to compute\nrelationships between neighboring pixels during image processing. Our\nexperiments demonstrate that HDM can generate higher-quality samples compared\nto models such as DDPM, Consistency Diffusion Models (CDM), Latent Diffusion\nModels (LDM), and Vector Quantized Generative Adversarial Networks (VQGAN).", "AI": {"tldr": "HDM\u662f\u4e00\u79cd\u57fa\u4e8e\u70ed\u6269\u6563\u65b9\u7a0b\u6539\u8fdb\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u8003\u8651\u76f8\u90bb\u50cf\u7d20\u5173\u7cfb\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u3002", "motivation": "\u4f20\u7edfDDPM\u5c06\u56fe\u50cf\u89c6\u4e3a\u6574\u4f53\u5904\u7406\uff0c\u5ffd\u7565\u4e86\u76f8\u90bb\u50cf\u7d20\u7684\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u7ec6\u8282\u4e22\u5931\u3002", "method": "\u5c06\u4e8c\u7ef4\u70ed\u65b9\u7a0b\u7684\u79bb\u6563\u5f62\u5f0f\u96c6\u6210\u5230DDPM\u7684\u6269\u6563\u548c\u751f\u6210\u516c\u5f0f\u4e2d\uff0c\u5b9e\u73b0\u50cf\u7d20\u7ea7\u64cd\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHDM\u5728\u56fe\u50cf\u8d28\u91cf\u4e0a\u4f18\u4e8eDDPM\u3001CDM\u3001LDM\u548cVQGAN\u7b49\u6a21\u578b\u3002", "conclusion": "\u5f15\u5165\u70ed\u65b9\u7a0b\u80fd\u6709\u6548\u63d0\u5347\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u751f\u6210\u8d28\u91cf\uff0c\u4fdd\u7559\u66f4\u591a\u7ec6\u8282\u3002"}}
{"id": "2504.19355", "pdf": "https://arxiv.org/pdf/2504.19355", "abs": "https://arxiv.org/abs/2504.19355", "authors": ["Gionni Marchetti"], "title": "Metric Similarity and Manifold Learning of Circular Dichroism Spectra of Proteins", "categories": ["physics.soc-ph", "cs.LG"], "comment": "13 pages, 16 figures", "summary": "We present a machine learning analysis of circular dichroism spectra of\nglobular proteins from the SP175 database, using the optimal transport-based\n$1$-Wasserstein distance $\\mathcal{W}_1$ (with order $p=1$) and the manifold\nlearning algorithm $t$-SNE. Our results demonstrate that $\\mathcal{W}_1$ is\nconsistent with both Euclidean and Manhattan metrics while exhibiting\nrobustness to noise. On the other hand, $t$-SNE uncovers meaningful structure\nin the high-dimensional data. The clustering in the $t$-SNE embedding is\nprimarily determined by proteins with distinct secondary structure\ncompositions: one cluster predominantly contains $\\beta$-rich proteins, while\nthe other consists mainly of proteins with mixed $\\alpha/\\beta$ and\n$\\alpha$-helical content.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u548c\u6700\u4f18\u8fd0\u8f93\u8ddd\u79bb\u5206\u6790SP175\u6570\u636e\u5e93\u4e2d\u7684\u86cb\u767d\u8d28\u5706\u4e8c\u8272\u5149\u8c31\uff0c\u8bc1\u660e$\\mathcal{W}_1$\u8ddd\u79bb\u5728\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u800c$t$-SNE\u63ed\u793a\u4e86\u9ad8\u7ef4\u6570\u636e\u4e2d\u86cb\u767d\u8d28\u7684\u4e8c\u7ea7\u7ed3\u6784\u805a\u7c7b\u7279\u5f81\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1$\\mathcal{W}_1$\u8ddd\u79bb\u5728\u86cb\u767d\u8d28\u5149\u8c31\u5206\u6790\u4e2d\u7684\u7a33\u5065\u6027\uff0c\u5e76\u5229\u7528\u964d\u7ef4\u65b9\u6cd5$t$-SNE\u63ed\u793a\u86cb\u767d\u8d28\u4e8c\u7ea7\u7ed3\u6784\u7684\u6f5c\u5728\u805a\u7c7b\u6a21\u5f0f\u3002", "method": "\u91c7\u7528$\\mathcal{W}_1$\u8ddd\u79bb\uff08$p=1$\u9636\uff09\u548c$t$-SNE\u6d41\u5f62\u5b66\u4e60\u7b97\u6cd5\u5206\u6790SP175\u6570\u636e\u5e93\u4e2d\u7684\u86cb\u767d\u8d28\u5706\u4e8c\u8272\u5149\u8c31\u6570\u636e\u3002", "result": "$\\mathcal{W}_1$\u8ddd\u79bb\u4e0e\u6b27\u51e0\u91cc\u5f97\u548c\u66fc\u54c8\u987f\u5ea6\u91cf\u4e00\u81f4\u4e14\u6297\u566a\uff0c$t$-SNE\u5d4c\u5165\u663e\u793a\u86cb\u767d\u8d28\u6309\u4e8c\u7ea7\u7ed3\u6784\uff08\u4e3b\u8981\u4e3a$\\beta$-\u5bcc\u96c6\u548c$\\alpha/\\beta$\u6df7\u5408\u578b\uff09\u805a\u7c7b\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86$\\mathcal{W}_1$\u5728\u5149\u8c31\u5206\u6790\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u540c\u65f6$t$-SNE\u6709\u6548\u63ed\u793a\u4e86\u86cb\u767d\u8d28\u7ed3\u6784\u7684\u5929\u7136\u805a\u7c7b\u7279\u5f81\u3002"}}
{"id": "2504.19653", "pdf": "https://arxiv.org/pdf/2504.19653", "abs": "https://arxiv.org/abs/2504.19653", "authors": ["Leon Davies", "Baihua Li", "Mohamad Saada", "Simon S\u00f8lvsten", "Qinggang Meng"], "title": "GAN-SLAM: Real-Time GAN Aided Floor Plan Creation Through SLAM", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "10 pages, preprint conference submission", "summary": "SLAM is a fundamental component of modern autonomous systems, providing\nrobots and their operators with a deeper understanding of their environment.\nSLAM systems often encounter challenges due to the dynamic nature of robotic\nmotion, leading to inaccuracies in mapping quality, particularly in 2D\nrepresentations such as Occupancy Grid Maps. These errors can significantly\ndegrade map quality, hindering the effectiveness of specific downstream tasks\nsuch as floor plan creation. To address this challenge, we introduce our novel\n'GAN-SLAM', a new SLAM approach that leverages Generative Adversarial Networks\nto clean and complete occupancy grids during the SLAM process, reducing the\nimpact of noise and inaccuracies introduced on the output map. We adapt and\nintegrate accurate pose estimation techniques typically used for 3D SLAM into a\n2D form. This enables the quality improvement 3D LiDAR-odometry has seen in\nrecent years to be effective for 2D representations. Our results demonstrate\nsubstantial improvements in map fidelity and quality, with minimal noise and\nerrors, affirming the effectiveness of GAN-SLAM for real-world mapping\napplications within large-scale complex environments. We validate our approach\non real-world data operating in real-time, and on famous examples of 2D maps.\nThe improved quality of the output map enables new downstream tasks, such as\nfloor plan drafting, further enhancing the capabilities of autonomous systems.\nOur novel approach to SLAM offers a significant step forward in the field,\nimproving the usability for SLAM in mapping-based tasks, and offers insight\ninto the usage of GANs for OGM error correction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a'GAN-SLAM'\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u5728SLAM\u8fc7\u7a0b\u4e2d\u6e05\u7406\u548c\u5b8c\u5584\u5360\u7528\u7f51\u683c\uff0c\u51cf\u5c11\u566a\u58f0\u548c\u4e0d\u51c6\u786e\u6027\u5bf9\u8f93\u51fa\u5730\u56fe\u7684\u5f71\u54cd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5730\u56fe\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u7684SLAM\u7cfb\u7edf\u7531\u4e8e\u673a\u5668\u4eba\u8fd0\u52a8\u7684\u52a8\u6001\u6027\uff0c\u57282D\u8868\u793a\uff08\u5982\u5360\u7528\u7f51\u683c\u5730\u56fe\uff09\u4e2d\u5bb9\u6613\u4ea7\u751f\u566a\u58f0\u548c\u4e0d\u51c6\u786e\u6027\uff0c\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u5e73\u9762\u56fe\u521b\u5efa\uff09\u7684\u6548\u679c\u3002\u8fd9\u4fc3\u4f7f\u4f5c\u8005\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u63d0\u5347\u5730\u56fe\u8d28\u91cf\u3002", "method": "\u4f5c\u8005\u63d0\u51fa'GAN-SLAM'\uff0c\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u5728SLAM\u8fc7\u7a0b\u4e2d\u6e05\u7406\u548c\u8865\u5168\u5360\u7528\u7f51\u683c\uff0c\u5e76\u501f\u92743D SLAM\u4e2d\u9ad8\u7cbe\u5ea6\u7684\u4f4d\u59ff\u4f30\u8ba1\u6280\u672f\uff0c\u5c06\u5176\u9002\u914d\u52302D\u5f62\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5730\u56fe\u7684\u4fdd\u771f\u5ea6\u548c\u8d28\u91cf\uff0c\u51cf\u5c11\u4e86\u566a\u58f0\u548c\u9519\u8bef\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u590d\u6742\u73af\u5883\u4e0b\u7684\u5b9e\u65f6\u64cd\u4f5c\uff0c\u5e76\u652f\u6301\u65b0\u7684\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u5e73\u9762\u56fe\u7ed8\u5236\uff09\u3002", "conclusion": "GAN-SLAM\u4e3aSLAM\u9886\u57df\u5e26\u6765\u91cd\u8981\u8fdb\u5c55\uff0c\u63d0\u5347\u4e86\u5176\u5728\u57fa\u4e8e\u5730\u56fe\u7684\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u540c\u65f6\u4e5f\u4e3a\u4f7f\u7528GAN\u8fdb\u884c\u5360\u7528\u7f51\u683c\u5730\u56fe\uff08OGM\uff09\u9519\u8bef\u6821\u6b63\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.19372", "pdf": "https://arxiv.org/pdf/2504.19372", "abs": "https://arxiv.org/abs/2504.19372", "authors": ["Weishi Wang", "Mark K. Transtrum", "Vincenzo Lordi", "Vasily V. Bulatov", "Amit Samanta"], "title": "Composable and adaptive design of machine learning interatomic potentials guided by Fisher-information analysis", "categories": ["cond-mat.mtrl-sci", "cs.LG", "cs.NA", "math.NA", "physics.app-ph", "physics.comp-ph"], "comment": "18 pages, 7 figures, and 6 tables", "summary": "An adaptive physics-informed model design strategy for machine-learning\ninteratomic potentials (MLIPs) is proposed. This strategy follows an iterative\nreconfiguration of composite models from single-term models, followed by a\nunified training procedure. A model evaluation method based on the Fisher\ninformation matrix (FIM) and multiple-property error metrics is proposed to\nguide model reconfiguration and hyperparameter optimization. Combining the\nmodel reconfiguration and the model evaluation subroutines, we provide an\nadaptive MLIP design strategy that balances flexibility and extensibility. In a\ncase study of designing models against a structurally diverse niobium dataset,\nwe managed to obtain an optimal configuration with 75 parameters generated by\nour framework that achieved a force RMSE of 0.172 eV/{\\AA} and an energy RMSE\nof 0.013 eV/atom.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u7269\u7406\u4fe1\u606f\u7684\u673a\u5668\u5b66\u4e60\u539f\u5b50\u95f4\u52bf(MLIPs)\u8bbe\u8ba1\u7b56\u7565\uff0c\u901a\u8fc7\u8fed\u4ee3\u91cd\u6784\u590d\u5408\u6a21\u578b\u548c\u7edf\u4e00\u8bad\u7ec3\uff0c\u7ed3\u5408Fisher\u4fe1\u606f\u77e9\u9635(FIM)\u548c\u591a\u5c5e\u6027\u8bef\u5dee\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\uff0c\u6700\u7ec8\u5728\u94cc\u6570\u636e\u6848\u4f8b\u4e2d\u53d6\u5f97\u826f\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8MLIPs\u7684\u9002\u5e94\u6027\u548c\u6269\u5c55\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u8c03\u6574\u6a21\u578b\u7ed3\u6784\u5e76\u4f18\u5316\u6027\u80fd\u7684\u8bbe\u8ba1\u7b56\u7565\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u91cd\u6784\u590d\u5408\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408FIM\u548c\u591a\u5c5e\u6027\u8bef\u5dee\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\uff0c\u6307\u5bfc\u6a21\u578b\u91cd\u6784\u548c\u8d85\u53c2\u6570\u4f18\u5316\u3002", "result": "\u5728\u94cc\u6570\u636e\u96c6\u6848\u4f8b\u4e2d\uff0c\u4f18\u5316\u540e\u7684\u6a21\u578b\u4ec5\u970075\u4e2a\u53c2\u6570\uff0c\u5b9e\u73b0\u4e86\u529bRMSE 0.172 eV/\u00c5\u548c\u80fd\u91cfRMSE 0.013 eV/atom\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u81ea\u9002\u5e94\u8bbe\u8ba1\u7b56\u7565\u5728\u7075\u6d3b\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u4e3aMLIPs\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2504.19654", "pdf": "https://arxiv.org/pdf/2504.19654", "abs": "https://arxiv.org/abs/2504.19654", "authors": ["Leon Davies", "Baihua Li", "Mohamad Saada", "Simon S\u00f8lvsten", "Qinggang Meng"], "title": "Transformation & Translation Occupancy Grid Mapping: 2-Dimensional Deep Learning Refined SLAM", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "12 pages, preprint, submitted to Robotics And Autonomous Systems", "summary": "SLAM (Simultaneous Localisation and Mapping) is a crucial component for\nrobotic systems, providing a map of an environment, the current location and\nprevious trajectory of a robot. While 3D LiDAR SLAM has received notable\nimprovements in recent years, 2D SLAM lags behind. Gradual drifts in odometry\nand pose estimation inaccuracies hinder modern 2D LiDAR-odometry algorithms in\nlarge complex environments. Dynamic robotic motion coupled with inherent\nestimation based SLAM processes introduce noise and errors, degrading map\nquality. Occupancy Grid Mapping (OGM) produces results that are often noisy and\nunclear. This is due to the fact that evidence based mapping represents maps\naccording to uncertain observations. This is why OGMs are so popular in\nexploration or navigation tasks. However, this also limits OGMs' effectiveness\nfor specific mapping based tasks such as floor plan creation in complex scenes.\nTo address this, we propose our novel Transformation and Translation Occupancy\nGrid Mapping (TT-OGM). We adapt and enable accurate and robust pose estimation\ntechniques from 3D SLAM to the world of 2D and mitigate errors to improve map\nquality using Generative Adversarial Networks (GANs). We introduce a novel data\ngeneration method via deep reinforcement learning (DRL) to build datasets large\nenough for training a GAN for SLAM error correction. We demonstrate our SLAM in\nreal-time on data collected at Loughborough University. We also prove its\ngeneralisability on a variety of large complex environments on a collection of\nlarge scale well-known 2D occupancy maps. Our novel approach enables the\ncreation of high quality OGMs in complex scenes, far surpassing the\ncapabilities of current SLAM algorithms in terms of quality, accuracy and\nreliability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTT-OGM\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c063D SLAM\u4e2d\u7684\u7cbe\u786e\u59ff\u6001\u4f30\u8ba1\u6280\u672f\u5e94\u7528\u52302D SLAM\u4e2d\uff0c\u5e76\u7ed3\u5408GAN\u548cDRL\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u4e2d\u7684\u5730\u56fe\u8d28\u91cf\u3001\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "2D SLAM\u5728\u590d\u6742\u5927\u573a\u666f\u4e2d\u56e0\u91cc\u7a0b\u8ba1\u6f02\u79fb\u548c\u59ff\u6001\u4f30\u8ba1\u4e0d\u51c6\u786e\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u4f20\u7edf\u7684OGM\u751f\u6210\u7684\u5730\u56fe\u5b58\u5728\u566a\u58f0\u4e14\u4e0d\u6e05\u6670\uff0c\u96be\u4ee5\u6ee1\u8db3\u9ad8\u7cbe\u5ea6\u5730\u56fe\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86TT-OGM\u65b9\u6cd5\uff0c\u7ed3\u54083D SLAM\u7684\u7cbe\u786e\u59ff\u6001\u4f30\u8ba1\u6280\u672f\uff0c\u4f7f\u7528GAN\u8fdb\u884c\u8bef\u5dee\u6821\u6b63\uff0c\u5e76\u901a\u8fc7DRL\u751f\u6210\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4ee5\u8bad\u7ec3GAN\u3002", "result": "\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u5b9e\u65f6\u8fd0\u884cSLAM\uff0c\u5e76\u5728\u591a\u79cd\u590d\u6742\u573a\u666f\u4e0b\u9a8c\u8bc1\u4e86\u5176\u901a\u7528\u6027\uff0c\u751f\u6210\u7684\u5730\u56fe\u8d28\u91cf\u8fdc\u8d85\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TT-OGM\u663e\u8457\u63d0\u5347\u4e862D SLAM\u7684\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684\u9ad8\u7cbe\u5ea6\u5730\u56fe\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2504.19464", "pdf": "https://arxiv.org/pdf/2504.19464", "abs": "https://arxiv.org/abs/2504.19464", "authors": ["Junting Ren", "Armin Schwartzman"], "title": "Model uncertainty quantification using feature confidence sets for outcome excursions", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "When implementing prediction models for high-stakes real-world applications\nsuch as medicine, finance, and autonomous systems, quantifying prediction\nuncertainty is critical for effective risk management. Traditional approaches\nto uncertainty quantification, such as confidence and prediction intervals,\nprovide probability coverage guarantees for the expected outcomes\n$f(\\boldsymbol{x})$ or the realized outcomes $f(\\boldsymbol{x})+\\epsilon$.\nInstead, this paper introduces a novel, model-agnostic framework for\nquantifying uncertainty in continuous and binary outcomes using confidence sets\nfor outcome excursions, where the goal is to identify a subset of the feature\nspace where the expected or realized outcome exceeds a specific value. The\nproposed method constructs data-dependent inner and outer confidence sets that\naim to contain the true feature subset for which the expected or realized\noutcomes of these features exceed a specified threshold. We establish\ntheoretical guarantees for the probability that these confidence sets contain\nthe true feature subset, both asymptotically and for finite sample sizes. The\nframework is validated through simulations and applied to real-world datasets,\ndemonstrating its utility in contexts such as housing price prediction and time\nto sepsis diagnosis in healthcare. This approach provides a unified method for\nuncertainty quantification that is broadly applicable across various continuous\nand binary prediction models.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u6570\u636e\u4f9d\u8d56\u7684\u5185\u5916\u7f6e\u4fe1\u96c6\u6765\u91cf\u5316\u8fde\u7eed\u548c\u4e8c\u5143\u7ed3\u679c\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u65e8\u5728\u8bc6\u522b\u7279\u5f81\u7a7a\u95f4\u4e2d\u9884\u671f\u6216\u5b9e\u9645\u7ed3\u679c\u8d85\u8fc7\u7279\u5b9a\u503c\u7684\u5b50\u96c6\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\u548c\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff09\uff0c\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u5bf9\u98ce\u9669\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u7f6e\u4fe1\u533a\u95f4\uff09\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7f6e\u4fe1\u96c6\u7684\u6a21\u578b\u65e0\u5173\u6846\u67b6\uff0c\u6784\u5efa\u6570\u636e\u4f9d\u8d56\u7684\u5185\u5916\u7f6e\u4fe1\u96c6\uff0c\u786e\u4fdd\u5305\u542b\u771f\u5b9e\u7279\u5f81\u5b50\u96c6\u7684\u6982\u7387\u7406\u8bba\u4fdd\u8bc1\uff08\u6e10\u8fd1\u548c\u6709\u9650\u6837\u672c\uff09\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u548c\u5b9e\u9645\u6570\u636e\uff08\u5982\u623f\u4ef7\u9884\u6d4b\u3001\u8d25\u8840\u75c7\u8bca\u65ad\u65f6\u95f4\uff09\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8fde\u7eed\u548c\u4e8c\u5143\u9884\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u3001\u53ef\u5e7f\u6cdb\u5e94\u7528\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002"}}
{"id": "2504.19673", "pdf": "https://arxiv.org/pdf/2504.19673", "abs": "https://arxiv.org/abs/2504.19673", "authors": ["Stefanie Krause", "Ashish Dalvi", "Syed Khubaib Zaidi"], "title": "Generative AI in Education: Student Skills and Lecturer Roles", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Generative Artificial Intelligence (GenAI) tools such as ChatGPT are emerging\nas a revolutionary tool in education that brings both positive aspects and\nchallenges for educators and students, reshaping how learning and teaching are\napproached. This study aims to identify and evaluate the key competencies\nstudents need to effectively engage with GenAI in education and to provide\nstrategies for lecturers to integrate GenAI into teaching practices. The study\napplied a mixed method approach with a combination of a literature review and a\nquantitative survey involving 130 students from South Asia and Europe to obtain\nits findings. The literature review identified 14 essential student skills for\nGenAI engagement, with AI literacy, critical thinking, and ethical AI practices\nemerging as the most critical. The student survey revealed gaps in prompt\nengineering, bias awareness, and AI output management. In our study of lecturer\nstrategies, we identified six key areas, with GenAI Integration and Curriculum\nDesign being the most emphasised. Our findings highlight the importance of\nincorporating GenAI into education. While literature prioritized ethics and\npolicy development, students favour hands-on, project-based learning and\npractical AI applications. To foster inclusive and responsible GenAI adoption,\ninstitutions should ensure equitable access to GenAI tools, establish clear\nacademic integrity policies, and advocate for global GenAI research\ninitiatives.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u53ca\u6240\u9700\u5b66\u751f\u80fd\u529b\uff0c\u8bc6\u522b\u4e8614\u9879\u5173\u952e\u6280\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u8bb2\u5e08\u6574\u5408GenAI\u7684\u516d\u5927\u7b56\u7565\u3002\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u548c\u5b66\u751f\u8c03\u67e5\uff0c\u53d1\u73b0AI\u7d20\u517b\u3001\u6279\u5224\u6027\u601d\u7ef4\u548c\u4f26\u7406\u5b9e\u8df5\u6700\u4e3a\u91cd\u8981\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5e2e\u52a9\u6559\u80b2\u8005\u548c\u5b66\u751f\u6709\u6548\u5e94\u5bf9GenAI\u5728\u6559\u80b2\u4e2d\u7684\u6311\u6218\uff0c\u91cd\u5851\u6559\u5b66\u65b9\u5f0f\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u6587\u732e\u7efc\u8ff0\u548c\u5bf9130\u540d\u6b27\u4e9a\u5b66\u751f\u7684\u5b9a\u91cf\u8c03\u67e5\u3002", "result": "\u7814\u7a76\u53d1\u73b0AI\u7d20\u517b\u3001\u6279\u5224\u6027\u601d\u7ef4\u548c\u4f26\u7406\u5b9e\u8df5\u662f\u5173\u952e\u6280\u80fd\uff1b\u5b66\u751f\u5728\u63d0\u793a\u5de5\u7a0b\u3001\u504f\u89c1\u610f\u8bc6\u7b49\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff1b\u8bb2\u5e08\u7b56\u7565\u4e2dGenAI\u6574\u5408\u548c\u8bfe\u7a0b\u8bbe\u8ba1\u6700\u53d7\u91cd\u89c6\u3002", "conclusion": "\u6559\u80b2\u673a\u6784\u9700\u63a8\u52a8\u5305\u5bb9\u6027GenAI\u5e94\u7528\uff0c\u786e\u4fdd\u5de5\u5177\u516c\u5e73\u83b7\u53d6\uff0c\u660e\u786e\u5b66\u672f\u8bda\u4fe1\u653f\u7b56\uff0c\u5e76\u652f\u6301\u5168\u7403\u7814\u7a76\u3002"}}
{"id": "2504.19476", "pdf": "https://arxiv.org/pdf/2504.19476", "abs": "https://arxiv.org/abs/2504.19476", "authors": ["Mina Karzand", "Guy Bresler"], "title": "Optimal Sequential Recommendations: Exploiting User and Item Structure", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT"], "comment": "91 pages, 7 figures", "summary": "We consider an online model for recommendation systems, with each user being\nrecommended an item at each time-step and providing 'like' or 'dislike'\nfeedback. A latent variable model specifies the user preferences: both users\nand items are clustered into types. The model captures structure in both the\nitem and user spaces, as used by item-item and user-user collaborative\nfiltering algorithms. We study the situation in which the type preference\nmatrix has i.i.d. entries. Our main contribution is an algorithm that\nsimultaneously uses both item and user structures, proved to be near-optimal\nvia corresponding information-theoretic lower bounds. In particular, our\nanalysis highlights the sub-optimality of using only one of item or user\nstructure (as is done in most collaborative filtering algorithms).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u7ebf\u63a8\u8350\u7cfb\u7edf\u6a21\u578b\uff0c\u901a\u8fc7\u7528\u6237\u548c\u7269\u54c1\u7684\u805a\u7c7b\u7c7b\u578b\uff0c\u5229\u7528\u9690\u53d8\u91cf\u6a21\u578b\u6355\u6349\u7528\u6237\u504f\u597d\u3002\u7b97\u6cd5\u7ed3\u5408\u4e86\u7528\u6237\u548c\u7269\u54c1\u7ed3\u6784\uff0c\u8bc1\u660e\u63a5\u8fd1\u6700\u4f18\uff0c\u5e76\u6307\u51fa\u4ec5\u4f7f\u7528\u5355\u4e00\u7ed3\u6784\u7684\u4e0d\u8db3\u3002", "motivation": "\u7814\u7a76\u5728\u7ebf\u63a8\u8350\u7cfb\u7edf\u4e2d\u7528\u6237\u548c\u7269\u54c1\u7684\u7ed3\u6784\u5bf9\u63a8\u8350\u6548\u679c\u7684\u5f71\u54cd\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u534f\u540c\u8fc7\u6ee4\u7b97\u6cd5\u4e2d\u4ec5\u4f9d\u8d56\u5355\u4e00\u7ed3\u6784\uff08\u7528\u6237\u6216\u7269\u54c1\uff09\u5bfc\u81f4\u7684\u6b21\u4f18\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u540c\u65f6\u5229\u7528\u7528\u6237\u548c\u7269\u54c1\u805a\u7c7b\u7c7b\u578b\u7684\u7b97\u6cd5\uff0c\u9690\u53d8\u91cf\u6a21\u578b\u63cf\u8ff0\u7528\u6237\u504f\u597d\uff0c\u5e76\u901a\u8fc7\u4fe1\u606f\u8bba\u4e0b\u754c\u5206\u6790\u5176\u6700\u4f18\u6027\u3002", "result": "\u7b97\u6cd5\u5728\u7ed3\u5408\u7528\u6237\u548c\u7269\u54c1\u7ed3\u6784\u65f6\u8868\u73b0\u63a5\u8fd1\u6700\u4f18\uff0c\u5206\u6790\u8868\u660e\u4ec5\u4f9d\u8d56\u5355\u4e00\u7ed3\u6784\u7684\u4f20\u7edf\u65b9\u6cd5\u6548\u679c\u6b21\u4f18\u3002", "conclusion": "\u63a8\u8350\u7cfb\u7edf\u5e94\u540c\u65f6\u5229\u7528\u7528\u6237\u548c\u7269\u54c1\u7ed3\u6784\u4ee5\u63d0\u9ad8\u6027\u80fd\uff0c\u4ec5\u4f9d\u8d56\u5355\u4e00\u7ed3\u6784\u53ef\u80fd\u5bfc\u81f4\u6548\u679c\u4e0d\u4f73\u3002"}}
{"id": "2504.19674", "pdf": "https://arxiv.org/pdf/2504.19674", "abs": "https://arxiv.org/abs/2504.19674", "authors": ["Madhur Jindal", "Hari Shrawgi", "Parag Agrawal", "Sandipan Dandapat"], "title": "$\\texttt{SAGE}$: A Generic Framework for LLM Safety Evaluation", "categories": ["cs.CR", "cs.AI"], "comment": "24 pages, 9 main pages excluding references and appendix", "summary": "Safety evaluation of Large Language Models (LLMs) has made progress and\nattracted academic interest, but it remains challenging to keep pace with the\nrapid integration of LLMs across diverse applications. Different applications\nexpose users to various harms, necessitating application-specific safety\nevaluations with tailored harms and policies. Another major gap is the lack of\nfocus on the dynamic and conversational nature of LLM systems. Such potential\noversights can lead to harms that go unnoticed in standard safety benchmarks.\nThis paper identifies the above as key requirements for robust LLM safety\nevaluation and recognizing that current evaluation methodologies do not satisfy\nthese, we introduce the $\\texttt{SAGE}$ (Safety AI Generic Evaluation)\nframework. $\\texttt{SAGE}$ is an automated modular framework designed for\ncustomized and dynamic harm evaluations. It utilizes adversarial user models\nthat are system-aware and have unique personalities, enabling a holistic\nred-teaming evaluation. We demonstrate $\\texttt{SAGE}$'s effectiveness by\nevaluating seven state-of-the-art LLMs across three applications and harm\npolicies. Our experiments with multi-turn conversational evaluations revealed a\nconcerning finding that harm steadily increases with conversation length.\nFurthermore, we observe significant disparities in model behavior when exposed\nto different user personalities and scenarios. Our findings also reveal that\nsome models minimize harmful outputs by employing severe refusal tactics that\ncan hinder their usefulness. These insights highlight the necessity of adaptive\nand context-specific testing to ensure better safety alignment and safer\ndeployment of LLMs in real-world scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u81ea\u52a8\u5316\u6a21\u5757\u5316\u6846\u67b6$\texttt{SAGE}$\uff0c\u7528\u4e8e\u5b9a\u5236\u5316\u548c\u52a8\u6001\u7684\u5371\u5bb3\u8bc4\u4f30\uff0c\u4ee5\u5e94\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u6837\u5316\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u6027\u6311\u6218\u3002", "motivation": "\u5f53\u524dLLMs\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u5e94\u7528\u5b9a\u5236\u5316\u548c\u52a8\u6001\u5bf9\u8bdd\u7684\u9700\u6c42\uff0c\u53ef\u80fd\u5bfc\u81f4\u6f5c\u5728\u5371\u5bb3\u88ab\u5ffd\u89c6\u3002", "method": "\u5f15\u5165$\texttt{SAGE}$\u6846\u67b6\uff0c\u5229\u7528\u7cfb\u7edf\u611f\u77e5\u7684\u5bf9\u6297\u6027\u7528\u6237\u6a21\u578b\u8fdb\u884c\u591a\u8f6e\u5bf9\u8bdd\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u5371\u5bb3\u968f\u5bf9\u8bdd\u957f\u5ea6\u589e\u52a0\u800c\u4e0a\u5347\uff0c\u6a21\u578b\u884c\u4e3a\u56e0\u7528\u6237\u4e2a\u6027\u548c\u573a\u666f\u5dee\u5f02\u663e\u8457\uff0c\u67d0\u4e9b\u6a21\u578b\u901a\u8fc7\u4e25\u5389\u62d2\u7edd\u7b56\u7565\u51cf\u5c11\u5371\u5bb3\u4f46\u53ef\u80fd\u5f71\u54cd\u5b9e\u7528\u6027\u3002", "conclusion": "\u9002\u5e94\u6027\u5f3a\u7684\u4e0a\u4e0b\u6587\u7279\u5b9a\u6d4b\u8bd5\u662f\u786e\u4fddLLMs\u5b89\u5168\u90e8\u7f72\u7684\u5173\u952e\u3002"}}
{"id": "2504.19488", "pdf": "https://arxiv.org/pdf/2504.19488", "abs": "https://arxiv.org/abs/2504.19488", "authors": ["Vijay Prakash S"], "title": "Two-parameter superposable S-curves", "categories": ["stat.ME", "cs.LG"], "comment": null, "summary": "Straight line equation $y=mx$ with slope $m$, when singularly perturbed as\n$ay^3+y=mx$ with a positive parameter $a$, results in S-shaped curves or\nS-curves on a real plane. As $a\\rightarrow 0$, we get back $y=mx$ which is a\ncumulative distribution function of a continuous uniform distribution that\ndescribes the occurrence of every event in an interval to be equally probable.\nAs $a\\rightarrow\\infty$, the derivative of $y$ has finite support only at $y=0$\nresembling a degenerate distribution. Based on these arguments, in this work,\nwe propose that these S-curves can represent maximum entropy uniform\ndistribution to a zero entropy single value. We also argue that these S-curves\nare superposable as they are only parametrically nonlinear but fundamentally\nlinear. So far, the superposed forms have been used to capture the patterns of\nnatural systems such as nonlinear dynamics of biological growth and kinetics of\nenzyme reactions. Here, we attempt to use the S-curve and its superposed form\nas a statistical model. We fit the models on a classical dataset containing\nflower measurements of iris plants and analyze their usefulness in pattern\nrecognition. Based on these models, we claim that any non-uniform pattern can\nbe represented as a singular perturbation to uniform distribution. However, our\nparametric estimation procedure have some limitations such as sensitivity to\ninitial conditions depending on the data at hand.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faS\u5f62\u66f2\u7ebf\uff08S-curves\uff09\u53ef\u4f5c\u4e3a\u7edf\u8ba1\u6a21\u578b\uff0c\u4ece\u5747\u5300\u5206\u5e03\u5230\u9000\u5316\u5206\u5e03\u63cf\u8ff0\u6570\u636e\u6a21\u5f0f\uff0c\u5e76\u5e94\u7528\u4e8e\u9e22\u5c3e\u82b1\u6570\u636e\u5206\u7c7b\uff0c\u4f46\u53c2\u6570\u4f30\u8ba1\u5bf9\u521d\u59cb\u6761\u4ef6\u654f\u611f\u3002", "motivation": "\u7814\u7a76S\u5f62\u66f2\u7ebf\u662f\u5426\u80fd\u4f5c\u4e3a\u7edf\u8ba1\u6a21\u578b\uff0c\u4ece\u6700\u5927\u71b5\u5747\u5300\u5206\u5e03\u8fc7\u6e21\u5230\u96f6\u71b5\u5355\u503c\u5206\u5e03\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u81ea\u7136\u7cfb\u7edf\uff08\u5982\u751f\u7269\u751f\u957f\u548c\u9176\u53cd\u5e94\u52a8\u529b\u5b66\uff09\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u901a\u8fc7\u5947\u5f02\u6270\u52a8\u76f4\u7ebf\u65b9\u7a0b$y=mx$\u751f\u6210S\u5f62\u66f2\u7ebf\uff0c\u5206\u6790\u5176\u53e0\u52a0\u6027\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u7edf\u8ba1\u6a21\u578b\u62df\u5408\u7ecf\u5178\u9e22\u5c3e\u82b1\u6570\u636e\u96c6\u3002", "result": "S\u5f62\u66f2\u7ebf\u80fd\u6709\u6548\u6355\u6349\u975e\u5747\u5300\u6a21\u5f0f\uff0c\u4f46\u5728\u53c2\u6570\u4f30\u8ba1\u65f6\u5bf9\u521d\u59cb\u6761\u4ef6\u654f\u611f\uff0c\u6570\u636e\u4f9d\u8d56\u6027\u8f83\u5f3a\u3002", "conclusion": "S\u5f62\u66f2\u7ebf\u53ef\u4f5c\u4e3a\u63cf\u8ff0\u975e\u5747\u5300\u6a21\u5f0f\u7684\u7edf\u8ba1\u6a21\u578b\uff0c\u4f46\u9700\u6539\u8fdb\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\u4ee5\u51cf\u5c11\u5bf9\u521d\u59cb\u6761\u4ef6\u7684\u654f\u611f\u6027\u3002"}}
{"id": "2504.19497", "pdf": "https://arxiv.org/pdf/2504.19497", "abs": "https://arxiv.org/abs/2504.19497", "authors": ["Kanghong Shi", "Ruigang Wang", "Ian R. Manchester"], "title": "Negative Imaginary Neural ODEs: Learning to Control Mechanical Systems with Stability Guarantees", "categories": ["eess.SY", "cs.LG", "cs.SY", "math.OC"], "comment": null, "summary": "We propose a neural control method to provide guaranteed stabilization for\nmechanical systems using a novel negative imaginary neural ordinary\ndifferential equation (NINODE) controller. Specifically, we employ neural\nnetworks with desired properties as state-space function matrices within a\nHamiltonian framework to ensure the system possesses the NI property. This\nNINODE system can serve as a controller that asymptotically stabilizes an NI\nplant under certain conditions. For mechanical plants with colocated force\nactuators and position sensors, we demonstrate that all the conditions required\nfor stability can be translated into regularity constraints on the neural\nnetworks used in the controller. We illustrate the utility, effectiveness, and\nstability guarantees of the NINODE controller through an example involving a\nnonlinear mass-spring system.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8d1f\u865a\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\uff08NINODE\uff09\u7684\u795e\u7ecf\u63a7\u5236\u65b9\u6cd5\uff0c\u786e\u4fdd\u673a\u68b0\u7cfb\u7edf\u7684\u7a33\u5b9a\u5316\u3002", "motivation": "\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5728\u54c8\u5bc6\u987f\u6846\u67b6\u4e2d\u5b9e\u73b0\u8d1f\u865a\u7279\u6027\uff0c\u4e3a\u673a\u68b0\u7cfb\u7edf\u63d0\u4f9b\u7a33\u5b9a\u63a7\u5236\u3002", "method": "\u5229\u7528\u5177\u6709\u7279\u5b9a\u5c5e\u6027\u7684\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u72b6\u6001\u7a7a\u95f4\u51fd\u6570\u77e9\u9635\uff0c\u6784\u5efaNINODE\u63a7\u5236\u5668\u3002", "result": "\u5728\u975e\u7ebf\u6027\u5f39\u7c27\u8d28\u91cf\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u63a7\u5236\u5668\u7684\u6709\u6548\u6027\u548c\u7a33\u5b9a\u6027\u4fdd\u8bc1\u3002", "conclusion": "NINODE\u63a7\u5236\u5668\u5728\u6ee1\u8db3\u6761\u4ef6\u65f6\u53ef\u7a33\u5b9a\u5177\u6709\u5171\u4f4d\u529b\u6267\u884c\u5668\u548c\u4f4d\u7f6e\u4f20\u611f\u5668\u7684\u673a\u68b0\u7cfb\u7edf\u3002"}}
{"id": "2504.19715", "pdf": "https://arxiv.org/pdf/2504.19715", "abs": "https://arxiv.org/abs/2504.19715", "authors": ["Heisei Yonezawa", "Ansei Yonezawa", "Itsuro Kajiwara"], "title": "Model-based controller assisted domain randomization in deep reinforcement learning: application to nonlinear powertrain control", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "comment": null, "summary": "Complex mechanical systems such as vehicle powertrains are inherently subject\nto multiple nonlinearities and uncertainties arising from parametric\nvariations. Modeling and calibration errors are therefore unavoidable, making\nthe transfer of control systems from simulation to real-world systems a\ncritical challenge. Traditional robust controls have limitations in handling\ncertain types of nonlinearities and uncertainties, requiring a more practical\napproach capable of comprehensively compensating for these various constraints.\nThis study proposes a new robust control approach using the framework of deep\nreinforcement learning (DRL). The key strategy lies in the synergy among domain\nrandomization-based DRL, long short-term memory (LSTM)-based actor and critic\nnetworks, and model-based control (MBC). The problem setup is modeled via the\nlatent Markov decision process (LMDP), a set of vanilla MDPs, for a controlled\nsystem subject to uncertainties and nonlinearities. In LMDP, the dynamics of an\nenvironment simulator is randomized during training to improve the robustness\nof the control system to real testing environments. The randomization increases\ntraining difficulties as well as conservativeness of the resultant control\nsystem; therefore, progress is assisted by concurrent use of a model-based\ncontroller based on a nominal system model. Compared to traditional DRL-based\ncontrols, the proposed controller design is smarter in that we can achieve a\nhigh level of generalization ability with a more compact neural network\narchitecture and a smaller amount of training data. The proposed approach is\nverified via practical application to active damping for a complex powertrain\nsystem with nonlinearities and parametric variations. Comparative tests\ndemonstrate the high robustness of the proposed approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u65b0\u578b\u9c81\u68d2\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u57df\u968f\u673a\u5316DRL\u3001LSTM\u7f51\u7edc\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u63a7\u5236\uff08MBC\uff09\uff0c\u6709\u6548\u5904\u7406\u975e\u7ebf\u6027\u4e0e\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u9ad8\u4e86\u63a7\u5236\u7cfb\u7edf\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u9c81\u68d2\u63a7\u5236\u5728\u5904\u7406\u67d0\u4e9b\u975e\u7ebf\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u65b9\u6cd5\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7DRL\u6846\u67b6\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u63d0\u9ad8\u63a7\u5236\u7cfb\u7edf\u7684\u5b9e\u9645\u5e94\u7528\u80fd\u529b\u3002", "method": "\u91c7\u7528\u57df\u968f\u673a\u5316\u7684DRL\u6846\u67b6\uff0c\u7ed3\u5408LSTM\u7f51\u7edc\u7684\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7ed3\u6784\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u63a7\u5236\uff0c\u901a\u8fc7LMDP\u5efa\u6a21\u95ee\u9898\uff0c\u5e76\u5728\u8bad\u7ec3\u4e2d\u5bf9\u73af\u5883\u52a8\u6001\u8fdb\u884c\u968f\u673a\u5316\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "result": "\u63d0\u51fa\u7684\u63a7\u5236\u5668\u5728\u590d\u6742\u52a8\u529b\u7cfb\u7edf\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u9ad8\u9c81\u68d2\u6027\uff0c\u4e14\u6240\u9700\u7684\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u66f4\u7d27\u51d1\u3001\u8bad\u7ec3\u6570\u636e\u66f4\u5c11\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u975e\u7ebf\u6027\u4e0e\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u590d\u6742\u673a\u68b0\u7cfb\u7edf\u7684\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.19596", "pdf": "https://arxiv.org/pdf/2504.19596", "abs": "https://arxiv.org/abs/2504.19596", "authors": ["Xi Fu", "Wei-Bang Jiang", "Yi Ding", "Cuntai Guan"], "title": "Towards Robust Multimodal Physiological Foundation Models: Handling Arbitrary Missing Modalities", "categories": ["eess.SP", "cs.LG"], "comment": "19 pages, 5 figures", "summary": "Multimodal physiological signals, such as EEG, ECG, EOG, and EMG, are crucial\nfor healthcare and brain-computer interfaces. While existing methods rely on\nspecialized architectures and dataset-specific fusion strategies, they struggle\nto learn universal representations that generalize across datasets and handle\nmissing modalities at inference time. To address these issues, we propose\nPhysioOmni, a foundation model for multimodal physiological signal analysis\nthat models both homogeneous and heterogeneous features to decouple multimodal\nsignals and extract generic representations while maintaining compatibility\nwith arbitrary missing modalities. PhysioOmni trains a decoupled multimodal\ntokenizer, enabling masked signal pre-training via modality-invariant and\nmodality-specific objectives. To ensure adaptability to diverse and incomplete\nmodality combinations, the pre-trained encoders undergo resilient fine-tuning\nwith prototype alignment on downstream datasets. Extensive experiments on four\ndownstream tasks, emotion recognition, sleep stage classification, motor\nprediction, and mental workload detection, demonstrate that PhysioOmni achieves\nstate-of-the-art performance while maintaining strong robustness to missing\nmodalities. Our code and model weights will be released.", "AI": {"tldr": "PhysioOmni\u662f\u4e00\u79cd\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u751f\u7406\u4fe1\u53f7\u5206\u6790\uff0c\u901a\u8fc7\u5b66\u4e60\u901a\u7528\u8868\u793a\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u548c\u7f3a\u5931\u6a21\u6001\u5904\u7406\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5b66\u4e60\u901a\u7528\u8868\u793a\u4e14\u65e0\u6cd5\u5904\u7406\u63a8\u7406\u65f6\u7684\u7f3a\u5931\u6a21\u6001\uff0cPhysioOmni\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faPhysioOmni\u6a21\u578b\uff0c\u901a\u8fc7\u89e3\u8026\u591a\u6a21\u6001\u4fe1\u53f7\u5e76\u9884\u8bad\u7ec3\u6a21\u6001\u4e0d\u53d8\u548c\u6a21\u6001\u7279\u5b9a\u7684\u76ee\u6807\uff0c\u4f7f\u7528\u539f\u578b\u5bf9\u9f50\u5bf9\u4e0b\u6e38\u4efb\u52a1\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u60c5\u611f\u8bc6\u522b\u3001\u7761\u7720\u9636\u6bb5\u5206\u7c7b\u7b49\u56db\u4e2a\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4e14\u5bf9\u7f3a\u5931\u6a21\u6001\u5177\u6709\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "PhysioOmni\u5728\u591a\u6a21\u6001\u751f\u7406\u4fe1\u53f7\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u901a\u7528\u8868\u793a\u548c\u7f3a\u5931\u6a21\u6001\u95ee\u9898\uff0c\u5177\u5907\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2504.19755", "pdf": "https://arxiv.org/pdf/2504.19755", "abs": "https://arxiv.org/abs/2504.19755", "authors": ["Kapil Kashyap", "Sean Fargose", "Chrisil Dabre", "Fatema Dolaria", "Nilesh Patil", "Aniket Kore"], "title": "Hybrid Approach Combining Ultrasound and Blood Test Analysis with a Voting Classifier for Accurate Liver Fibrosis and Cirrhosis Assessment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Liver cirrhosis is an insidious condition involving the substitution of\nnormal liver tissue with fibrous scar tissue and causing major health\ncomplications. The conventional method of diagnosis using liver biopsy is\ninvasive and, therefore, inconvenient for use in regular screening. In this\npaper,we present a hybrid model that combines machine learning techniques with\nclinical data and ultrasoundscans to improve liver fibrosis and cirrhosis\ndetection accuracy is presented. The model integrates fixed blood test\nprobabilities with deep learning model predictions (DenseNet-201) for\nultrasonic images. The combined hybrid model achieved an accuracy of 92.5%. The\nfindings establish the viability of the combined model in enhancing diagnosis\naccuracy and supporting early intervention in liver disease care.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548c\u4e34\u5e8a\u6570\u636e\u7684\u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u9ad8\u809d\u7ea4\u7ef4\u5316\u548c\u809d\u786c\u5316\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u4fb5\u5165\u6027\u809d\u6d3b\u68c0\u3002", "motivation": "\u4f20\u7edf\u7684\u809d\u6d3b\u68c0\u8bca\u65ad\u65b9\u6cd5\u4fb5\u5165\u6027\u5f3a\uff0c\u4e0d\u4fbf\u4e8e\u5e38\u89c4\u7b5b\u67e5\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u4fbf\u6377\u4e14\u51c6\u786e\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u8840\u6db2\u68c0\u6d4b\u6570\u636e\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08DenseNet-201\uff09\u5206\u6790\u8d85\u58f0\u56fe\u50cf\uff0c\u6784\u5efa\u6df7\u5408\u6a21\u578b\u3002", "result": "\u6df7\u5408\u6a21\u578b\u7684\u51c6\u786e\u7387\u8fbe\u523092.5%\u3002", "conclusion": "\u8be5\u6df7\u5408\u6a21\u578b\u80fd\u6709\u6548\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u652f\u6301\u809d\u75c5\u7684\u65e9\u671f\u5e72\u9884\u3002"}}
{"id": "2504.19625", "pdf": "https://arxiv.org/pdf/2504.19625", "abs": "https://arxiv.org/abs/2504.19625", "authors": ["Massimo Fioravanti", "Samuele Pasini", "Giovanni Agosta"], "title": "Rulebook: bringing co-routines to reinforcement learning environments", "categories": ["cs.PL", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) algorithms, due to their reliance on external\nsystems to learn from, require digital environments (e.g., simulators) with\nvery simple interfaces, which in turn constrain significantly the\nimplementation of such environments. In particular, these environments are\nimplemented either as separate processes or as state machines, leading to\nsynchronization and communication overheads in the first case, and to\nunstructured programming in the second.\n  We propose a new domain-specific, co-routine-based, compiled language, called\nRulebook, designed to automatically generate the state machine required to\ninteract with machine learning (ML) algorithms and similar applications, with\nno performance overhead. Rulebook allows users to express programs without\nneeding to be aware of the specific interface required by the ML components. By\ndecoupling the execution model of the program from the syntactical encoding of\nthe program, and thus without the need for manual state management, Rulebook\nallows to create larger and more sophisticated environments at a lower\ndevelopment cost.", "AI": {"tldr": "Rulebook\u662f\u4e00\u79cd\u65b0\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff0c\u65e8\u5728\u901a\u8fc7\u534f\u7a0b\u81ea\u52a8\u751f\u6210\u4e0e\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u4ea4\u4e92\u6240\u9700\u7684\u72b6\u6001\u673a\uff0c\u964d\u4f4e\u5f00\u53d1\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4f9d\u8d56\u5916\u90e8\u7cfb\u7edf\uff0c\u5bfc\u81f4\u6570\u5b57\u73af\u5883\u7684\u5b9e\u73b0\u53d7\u9650\u4e8e\u540c\u6b65\u3001\u901a\u4fe1\u548c\u4ee3\u7801\u7ed3\u6784\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51faRulebook\uff0c\u4e00\u79cd\u57fa\u4e8e\u534f\u7a0b\u7684\u7f16\u8bd1\u8bed\u8a00\uff0c\u81ea\u52a8\u751f\u6210\u72b6\u6001\u673a\uff0c\u89e3\u8026\u7a0b\u5e8f\u6267\u884c\u6a21\u578b\u4e0e\u8bed\u6cd5\u7f16\u7801\u3002", "result": "Rulebook\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u6027\u80fd\u5f00\u9500\u7684ML\u63a5\u53e3\uff0c\u652f\u6301\u66f4\u590d\u6742\u73af\u5883\u7684\u4f4e\u6210\u672c\u5f00\u53d1\u3002", "conclusion": "Rulebook\u901a\u8fc7\u7b80\u5316\u72b6\u6001\u7ba1\u7406\uff0c\u63d0\u5347\u4e86\u5f00\u53d1\u6548\u7387\uff0c\u9002\u7528\u4e8e\u590d\u6742ML\u73af\u5883\u7684\u6784\u5efa\u3002"}}
{"id": "2504.19818", "pdf": "https://arxiv.org/pdf/2504.19818", "abs": "https://arxiv.org/abs/2504.19818", "authors": ["Feng Chen", "Ilias Stogiannidis", "Andrew Wood", "Danilo Bueno", "Dominic Williams", "Fraser Macfarlane", "Bruce Grieve", "Darren Wells", "Jonathan A. Atkinson", "Malcolm J. Hawkesford", "Stephen A. Rolfe", "Tracy Lawson", "Tony Pridmore", "Mario Valerio Giuffrida", "Sotirios A. Tsaftaris"], "title": "PhenoAssistant: A Conversational Multi-Agent AI System for Automated Plant Phenotyping", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Plant phenotyping increasingly relies on (semi-)automated image-based\nanalysis workflows to improve its accuracy and scalability. However, many\nexisting solutions remain overly complex, difficult to reimplement and\nmaintain, and pose high barriers for users without substantial computational\nexpertise. To address these challenges, we introduce PhenoAssistant: a\npioneering AI-driven system that streamlines plant phenotyping via intuitive\nnatural language interaction. PhenoAssistant leverages a large language model\nto orchestrate a curated toolkit supporting tasks including automated phenotype\nextraction, data visualisation and automated model training. We validate\nPhenoAssistant through several representative case studies and a set of\nevaluation tasks. By significantly lowering technical hurdles, PhenoAssistant\nunderscores the promise of AI-driven methodologies to democratising AI adoption\nin plant biology.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86PhenoAssistant\uff0c\u4e00\u4e2a\u57fa\u4e8eAI\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u7b80\u5316\u690d\u7269\u8868\u578b\u5206\u6790\uff0c\u964d\u4f4e\u6280\u672f\u95e8\u69db\uff0c\u63d0\u5347\u6548\u7387\u548c\u53ef\u53ca\u6027\u3002", "motivation": "\u73b0\u6709\u690d\u7269\u8868\u578b\u5206\u6790\u5de5\u5177\u590d\u6742\u4e14\u96be\u4ee5\u7ef4\u62a4\uff0c\u6280\u672f\u95e8\u69db\u9ad8\uff0c\u9650\u5236\u4e86\u975e\u4e13\u4e1a\u7528\u6237\u7684\u4f7f\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u5f00\u53d1\u4e86PhenoAssistant\u3002", "method": "PhenoAssistant\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u534f\u8c03\u4e00\u5957\u5de5\u5177\u5305\uff0c\u652f\u6301\u81ea\u52a8\u5316\u8868\u578b\u63d0\u53d6\u3001\u6570\u636e\u53ef\u89c6\u5316\u548c\u6a21\u578b\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548c\u8bc4\u4f30\u4efb\u52a1\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "PhenoAssistant\u5728\u591a\u4e2a\u6848\u4f8b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u6280\u672f\u95e8\u69db\uff0c\u8bc1\u660e\u4e86AI\u5728\u690d\u7269\u751f\u7269\u5b66\u4e2d\u7684\u666e\u53ca\u6f5c\u529b\u3002", "conclusion": "PhenoAssistant\u5c55\u793a\u4e86AI\u9a71\u52a8\u65b9\u6cd5\u5728\u690d\u7269\u8868\u578b\u5206\u6790\u4e2d\u7684\u524d\u666f\uff0c\u901a\u8fc7\u7b80\u5316\u6d41\u7a0b\uff0c\u63a8\u52a8\u4e86AI\u5728\u751f\u7269\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2504.19632", "pdf": "https://arxiv.org/pdf/2504.19632", "abs": "https://arxiv.org/abs/2504.19632", "authors": ["Subham Das", "Ashtakala Meghanath", "Bikash K. Behera", "Shahid Mumtaz", "Saif Al-Kuwari", "Ahmed Farouk"], "title": "QFDNN: A Resource-Efficient Variational Quantum Feature Deep Neural Networks for Fraud Detection and Loan Prediction", "categories": ["quant-ph", "cs.LG"], "comment": "12 pages, 6 figures, 8 tables", "summary": "Social financial technology focuses on trust, sustainability, and social\nresponsibility, which require advanced technologies to address complex\nfinancial tasks in the digital era. With the rapid growth in online\ntransactions, automating credit card fraud detection and loan eligibility\nprediction has become increasingly challenging. Classical machine learning (ML)\nmodels have been used to solve these challenges; however, these approaches\noften encounter scalability, overfitting, and high computational costs due to\ncomplexity and high-dimensional financial data. Quantum computing (QC) and\nquantum machine learning (QML) provide a promising solution to efficiently\nprocessing high-dimensional datasets and enabling real-time identification of\nsubtle fraud patterns. However, existing quantum algorithms lack robustness in\nnoisy environments and fail to optimize performance with reduced feature sets.\nTo address these limitations, we propose a quantum feature deep neural network\n(QFDNN), a novel, resource efficient, and noise-resilient quantum model that\noptimizes feature representation while requiring fewer qubits and simpler\nvariational circuits. The model is evaluated using credit card fraud detection\nand loan eligibility prediction datasets, achieving competitive accuracies of\n82.2% and 74.4%, respectively, with reduced computational overhead.\nFurthermore, we test QFDNN against six noise models, demonstrating its\nrobustness across various error conditions. Our findings highlight QFDNN\npotential to enhance trust and security in social financial technology by\naccurately detecting fraudulent transactions while supporting sustainability\nthrough its resource-efficient design and minimal computational overhead.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQFDNN\u7684\u65b0\u578b\u91cf\u5b50\u7279\u5f81\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u65e8\u5728\u89e3\u51b3\u91cf\u5b50\u8ba1\u7b97\u5728\u91d1\u878d\u79d1\u6280\u9886\u57df\u4e2d\u9047\u5230\u7684\u566a\u58f0\u73af\u5883\u548c\u7279\u5f81\u4f18\u5316\u95ee\u9898\u3002QFDNN\u5728\u4fe1\u7528\u5361\u6b3a\u8bc8\u68c0\u6d4b\u548c\u8d37\u6b3e\u8d44\u683c\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u51c6\u786e\u7387\u5206\u522b\u4e3a82.2%\u548c74.4%\uff0c\u540c\u65f6\u5177\u6709\u8d44\u6e90\u9ad8\u6548\u548c\u566a\u58f0\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5904\u7406\u91d1\u878d\u6570\u636e\u65f6\u9762\u4e34\u53ef\u6269\u5c55\u6027\u3001\u8fc7\u62df\u5408\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u7b49\u95ee\u9898\uff1b\u73b0\u6709\u91cf\u5b50\u7b97\u6cd5\u5728\u566a\u58f0\u73af\u5883\u4e2d\u9c81\u68d2\u6027\u4e0d\u8db3\u4e14\u96be\u4ee5\u4f18\u5316\u7279\u5f81\u3002\u793e\u4f1a\u91d1\u878d\u79d1\u6280\u9700\u8981\u9ad8\u6548\u7387\u548c\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u4ee5\u589e\u5f3a\u4fe1\u4efb\u548c\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u4e14\u566a\u58f0\u9c81\u68d2\u7684\u91cf\u5b50\u7279\u5f81\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08QFDNN\uff09\uff0c\u901a\u8fc7\u4f18\u5316\u7279\u5f81\u8868\u793a\u5e76\u4f7f\u7528\u66f4\u5c11\u7684\u91cf\u5b50\u6bd4\u7279\u548c\u66f4\u7b80\u5355\u7684\u53d8\u5206\u7535\u8def\u6765\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728\u4fe1\u7528\u5361\u6b3a\u8bc8\u68c0\u6d4b\u548c\u8d37\u6b3e\u8d44\u683c\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cQFDNN\u5206\u522b\u53d6\u5f97\u4e8682.2%\u548c74.4%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728\u516d\u79cd\u566a\u58f0\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "QFDNN\u5728\u91d1\u878d\u79d1\u6280\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u80fd\u591f\u9ad8\u6548\u51c6\u786e\u5730\u68c0\u6d4b\u6b3a\u8bc8\u4ea4\u6613\uff0c\u540c\u65f6\u901a\u8fc7\u5176\u8d44\u6e90\u9ad8\u6548\u7684\u8bbe\u8ba1\u652f\u6301\u53ef\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2504.19847", "pdf": "https://arxiv.org/pdf/2504.19847", "abs": "https://arxiv.org/abs/2504.19847", "authors": ["Juhan Park", "Kyungjae Lee", "Hyung Jin Chang", "Jungchan Cho"], "title": "Foundation Model-Driven Framework for Human-Object Interaction Prediction with Segmentation Mask Integration", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this work, we introduce Segmentation to Human-Object Interaction\n(\\textit{\\textbf{Seg2HOI}}) approach, a novel framework that integrates\nsegmentation-based vision foundation models with the human-object interaction\ntask, distinguished from traditional detection-based Human-Object Interaction\n(HOI) methods. Our approach enhances HOI detection by not only predicting the\nstandard triplets but also introducing quadruplets, which extend HOI triplets\nby including segmentation masks for human-object pairs. More specifically,\nSeg2HOI inherits the properties of the vision foundation model (e.g.,\npromptable and interactive mechanisms) and incorporates a decoder that applies\nthese attributes to HOI task. Despite training only for HOI, without additional\ntraining mechanisms for these properties, the framework demonstrates that such\nfeatures still operate efficiently. Extensive experiments on two public\nbenchmark datasets demonstrate that Seg2HOI achieves performance comparable to\nstate-of-the-art methods, even in zero-shot scenarios. Lastly, we propose that\nSeg2HOI can generate HOI quadruplets and interactive HOI segmentation from\nnovel text and visual prompts that were not used during training, making it\nversatile for a wide range of applications by leveraging this flexibility.", "AI": {"tldr": "Seg2HOI: \u4e00\u4e2a\u5c06\u5206\u5272\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e0e\u4eba\u7c7b-\u7269\u4f53\u4ea4\u4e92\u4efb\u52a1\u7ed3\u5408\u7684\u65b0\u6846\u67b6\uff0c\u53ef\u751f\u6210\u56db\u5143\u7ec4\u548c\u4ea4\u4e92\u5f0f\u5206\u5272\uff0c\u6027\u80fd\u63a5\u8fd1\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u4eba\u7c7b-\u7269\u4f53\u4ea4\u4e92\uff08HOI\uff09\u65b9\u6cd5\u4f9d\u8d56\u68c0\u6d4b\uff0c\u800cSeg2HOI\u65e8\u5728\u5229\u7528\u5206\u5272\u80fd\u529b\u63d0\u5347HOI\u4efb\u52a1\uff0c\u589e\u52a0\u51c6\u786e\u6027\u548c\u7075\u6d3b\u6027\u3002", "method": "\u7ed3\u5408\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5982\u652f\u6301\u63d0\u793a\u548c\u4ea4\u4e92\u673a\u5236\u7684\u5206\u5272\u6a21\u578b\uff09\uff0c\u5f15\u5165\u89e3\u7801\u5668\u5c06\u5206\u5272\u5c5e\u6027\u5e94\u7528\u4e8eHOI\uff0c\u5e76\u6269\u5c55\u5230\u751f\u6210\u56db\u5143\u7ec4\uff08\u542b\u5206\u5272\u63a9\u7801\uff09\u3002\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u529f\u80fd\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u63a5\u8fd1SOTA\uff0c\u96f6\u6837\u672c\u573a\u666f\u4e0b\u4e5f\u80fd\u9ad8\u6548\u8fd0\u4f5c\uff0c\u4e14\u80fd\u901a\u8fc7\u65b0\u6587\u672c\u6216\u89c6\u89c9\u63d0\u793a\u751f\u6210\u672a\u8bad\u7ec3\u8fc7\u7684HOI\u56db\u5143\u7ec4\u548c\u4ea4\u4e92\u5206\u5272\u3002", "conclusion": "Seg2HOI\u5c55\u793a\u4e86\u5206\u5272\u6a21\u578b\u5728HOI\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7075\u6d3b\u6027\u3002"}}
{"id": "2504.19635", "pdf": "https://arxiv.org/pdf/2504.19635", "abs": "https://arxiv.org/abs/2504.19635", "authors": ["Yike Zhao", "Haoyuan Cai", "Ali H. Sayed"], "title": "Diffusion Stochastic Learning Over Adaptive Competing Networks", "categories": ["cs.MA", "cs.LG", "math.OC", "stat.ML"], "comment": null, "summary": "This paper studies a stochastic dynamic game between two competing teams,\neach consisting of a network of collaborating agents. Unlike fully cooperative\nsettings, where all agents share a common objective, each team in this game\naims to minimize its own distinct objective. In the adversarial setting, their\nobjectives could be conflicting as in zero-sum games. Throughout the\ncompetition, agents share strategic information within their own team while\nsimultaneously inferring and adapting to the strategies of the opposing team.\nWe propose diffusion learning algorithms to address two important classes of\nthis network game: i) a zero-sum game characterized by weak cross-team subgraph\ninteractions, and ii) a general non-zero-sum game exhibiting strong cross-team\nsubgraph interactions. We analyze the stability performance of the proposed\nalgorithms under reasonable assumptions and illustrate the theoretical results\nthrough experiments on Cournot team competition and decentralized GAN training.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e24\u4e2a\u7ade\u4e89\u56e2\u961f\u95f4\u968f\u673a\u52a8\u6001\u535a\u5f08\uff0c\u5176\u4e2d\u56e2\u961f\u6210\u5458\u534f\u4f5c\u4f46\u76ee\u6807\u5bf9\u7acb\uff1b\u63d0\u51fa\u6269\u6563\u5b66\u4e60\u7b97\u6cd5\u5e94\u5bf9\u5f31/\u5f3a\u8de8\u56e2\u961f\u4ea4\u4e92\u60c5\u666f\uff0c\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u7a33\u5b9a\u6027\u3002", "motivation": "\u7814\u7a76\u5728\u7ade\u4e89\u73af\u5883\u4e2d\uff0c\u534f\u4f5c\u56e2\u961f\u95f4\u5982\u4f55\u901a\u8fc7\u4fe1\u606f\u5171\u4eab\u4e0e\u5bf9\u624b\u7b56\u7565\u63a8\u65ad\u5b9e\u73b0\u76ee\u6807\u4f18\u5316\uff0c\u586b\u8865\u4e86\u56e2\u961f\u5bf9\u6297\u52a8\u6001\u535a\u5f08\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u6269\u6563\u5b66\u4e60\u7b97\u6cd5\uff0c\u9488\u5bf9\u96f6\u548c\uff08\u5f31\u4ea4\u4e92\uff09\u4e0e\u975e\u96f6\u548c\uff08\u5f3a\u4ea4\u4e92\uff09\u4e24\u7c7b\u56e2\u961f\u535a\u5f08\u8bbe\u8ba1\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u7b97\u6cd5\u5728\u5408\u7406\u5047\u8bbe\u4e0b\u5177\u6709\u7a33\u5b9a\u6027\uff0cCournot\u56e2\u961f\u7ade\u4e89\u548c\u53bb\u4e2d\u5fc3\u5316GAN\u8bad\u7ec3\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u6269\u6563\u5b66\u4e60\u7b97\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u534f\u4f5c\u56e2\u961f\u95f4\u7684\u52a8\u6001\u5bf9\u6297\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\uff08\u5982\u7ecf\u6d4e\u7ade\u4e89\u3001\u591a\u667a\u80fd\u4f53\u5bf9\u6297\uff09\u63d0\u4f9b\u4e86\u7406\u8bba\u5de5\u5177\u3002"}}
{"id": "2504.19848", "pdf": "https://arxiv.org/pdf/2504.19848", "abs": "https://arxiv.org/abs/2504.19848", "authors": ["Simona Casini", "Pietro Ducange", "Francesco Marcelloni", "Lorenzo Pollini"], "title": "Human-Centered AI and Autonomy in Robotics: Insights from a Bibliometric Study", "categories": ["cs.RO", "cs.AI"], "comment": "International Joint Conference on Neural Network 2025 - Accepted", "summary": "The development of autonomous robotic systems offers significant potential\nfor performing complex tasks with precision and consistency. Recent advances in\nArtificial Intelligence (AI) have enabled more capable intelligent automation\nsystems, addressing increasingly complex challenges. However, this progress\nraises questions about human roles in such systems. Human-Centered AI (HCAI)\naims to balance human control and automation, ensuring performance enhancement\nwhile maintaining creativity, mastery, and responsibility. For real-world\napplications, autonomous robots must balance task performance with reliability,\nsafety, and trustworthiness. Integrating HCAI principles enhances human-robot\ncollaboration and ensures responsible operation.\n  This paper presents a bibliometric analysis of intelligent autonomous robotic\nsystems, utilizing SciMAT and VOSViewer to examine data from the Scopus\ndatabase. The findings highlight academic trends, emerging topics, and AI's\nrole in self-adaptive robotic behaviour, with an emphasis on HCAI architecture.\nThese insights are then projected onto the IBM MAPE-K architecture, with the\ngoal of identifying how these research results map into actual robotic\nautonomous systems development efforts for real-world scenarios.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6587\u732e\u8ba1\u91cf\u5206\u6790\u63a2\u8ba8\u4e86\u667a\u80fd\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5b66\u672f\u8d8b\u52bf\u548cAI\u5728\u81ea\u9002\u5e94\u884c\u4e3a\u4e2d\u7684\u4f5c\u7528\uff0c\u91cd\u70b9\u7ed3\u5408\u4ee5\u4eba\u4e3a\u672c\u7684AI\u67b6\u6784\uff0c\u5e76\u5c06\u7814\u7a76\u6210\u679c\u6620\u5c04\u5230IBM MAPE-K\u67b6\u6784\u4e2d\u3002", "motivation": "\u968f\u7740AI\u6280\u672f\u7684\u53d1\u5c55\uff0c\u667a\u80fd\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u4e5f\u5f15\u53d1\u4e86\u5bf9\u4eba\u7c7b\u89d2\u8272\u7684\u601d\u8003\u3002HCAI\u65e8\u5728\u5e73\u8861\u4eba\u7c7b\u63a7\u5236\u4e0e\u81ea\u52a8\u5316\uff0c\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u7684\u540c\u65f6\u786e\u4fdd\u5b89\u5168\u6027\u548c\u53ef\u4fe1\u8d56\u6027\u3002", "method": "\u4f7f\u7528SciMAT\u548cVOSViewer\u5bf9Scopus\u6570\u636e\u5e93\u4e2d\u7684\u6587\u732e\u8fdb\u884c\u8ba1\u91cf\u5206\u6790\uff0c\u7ed3\u5408HCAI\u67b6\u6784\u548cIBM MAPE-K\u67b6\u6784\u8fdb\u884c\u6620\u5c04\u3002", "result": "\u5206\u6790\u7ed3\u679c\u63ed\u793a\u4e86\u5b66\u672f\u8d8b\u52bf\u3001\u65b0\u5174\u4e3b\u9898\u53caAI\u5728\u673a\u5668\u4eba\u81ea\u9002\u5e94\u884c\u4e3a\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u5b9e\u9645\u7684\u81ea\u4e3b\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86HCAI\u5728\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u7684\u81ea\u4e3b\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6846\u67b6\u548c\u65b9\u5411\u3002"}}
{"id": "2504.19854", "pdf": "https://arxiv.org/pdf/2504.19854", "abs": "https://arxiv.org/abs/2504.19854", "authors": ["Chia-Yu Hung", "Qi Sun", "Pengfei Hong", "Amir Zadeh", "Chuan Li", "U-Xuan Tan", "Navonil Majumder", "Soujanya Poria"], "title": "NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Existing Visual-Language-Action (VLA) models have shown promising performance\nin zero-shot scenarios, demonstrating impressive task execution and reasoning\ncapabilities. However, a significant challenge arises from the limitations of\nvisual encoding, which can result in failures during tasks such as object\ngrasping. Moreover, these models typically suffer from high computational\noverhead due to their large sizes, often exceeding 7B parameters. While these\nmodels excel in reasoning and task planning, the substantial computational\noverhead they incur makes them impractical for real-time robotic environments,\nwhere speed and efficiency are paramount. To address the limitations of\nexisting VLA models, we propose NORA, a 3B-parameter model designed to reduce\ncomputational overhead while maintaining strong task performance. NORA adopts\nthe Qwen-2.5-VL-3B multimodal model as its backbone, leveraging its superior\nvisual-semantic understanding to enhance visual reasoning and action grounding.\nAdditionally, our \\model{} is trained on 970k real-world robot demonstrations\nand equipped with the FAST+ tokenizer for efficient action sequence generation.\nExperimental results demonstrate that NORA outperforms existing large-scale VLA\nmodels, achieving better task performance with significantly reduced\ncomputational overhead, making it a more practical solution for real-time\nrobotic autonomy.", "AI": {"tldr": "NORA\u662f\u4e00\u79cd3B\u53c2\u6570\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\uff0c\u65e8\u5728\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u5e76\u4fdd\u6301\u9ad8\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u673a\u5668\u4eba\u73af\u5883\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u96f6\u6837\u672c\u573a\u666f\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u89c6\u89c9\u7f16\u7801\u9650\u5236\u548c\u9ad8\u8ba1\u7b97\u5f00\u9500\uff08\u59827B\u53c2\u6570\uff09\u59a8\u788d\u4e86\u5b9e\u65f6\u673a\u5668\u4eba\u5e94\u7528\u7684\u5b9e\u7528\u6027\u3002", "method": "\u91c7\u7528Qwen-2.5-VL-3B\u591a\u6a21\u6001\u6a21\u578b\u4f5c\u4e3a\u4e3b\u5e72\uff0c\u7ed3\u540897\u4e07\u771f\u5b9e\u673a\u5668\u4eba\u6f14\u793a\u6570\u636e\u8bad\u7ec3\uff0c\u914d\u5907FAST+\u5206\u8bcd\u5668\u4ee5\u9ad8\u6548\u751f\u6210\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "NORA\u5728\u4efb\u52a1\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u5927\u578bVLA\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u66f4\u9002\u5408\u5b9e\u65f6\u5e94\u7528\u3002", "conclusion": "NORA\u4e3aVLA\u6a21\u578b\u7684\u9ad8\u6548\u5b9e\u7528\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2504.19863", "pdf": "https://arxiv.org/pdf/2504.19863", "abs": "https://arxiv.org/abs/2504.19863", "authors": ["Daniel Kienzle", "Robin Sch\u00f6n", "Rainer Lienhart", "Shin'Ichi Satoh"], "title": "Towards Ball Spin and Trajectory Analysis in Table Tennis Broadcast Videos via Physically Grounded Synthetic-to-Real Transfer", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "To be published in 2025 IEEE/CVF International Conference on Computer\n  Vision and Pattern Recognition Workshops (CVPRW)", "summary": "Analyzing a player's technique in table tennis requires knowledge of the\nball's 3D trajectory and spin. While, the spin is not directly observable in\nstandard broadcasting videos, we show that it can be inferred from the ball's\ntrajectory in the video. We present a novel method to infer the initial spin\nand 3D trajectory from the corresponding 2D trajectory in a video. Without\nground truth labels for broadcast videos, we train a neural network solely on\nsynthetic data. Due to the choice of our input data representation, physically\ncorrect synthetic training data, and using targeted augmentations, the network\nnaturally generalizes to real data. Notably, these simple techniques are\nsufficient to achieve generalization. No real data at all is required for\ntraining. To the best of our knowledge, we are the first to present a method\nfor spin and trajectory prediction in simple monocular broadcast videos,\nachieving an accuracy of 92.0% in spin classification and a 2D reprojection\nerror of 0.19% of the image diagonal.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4e52\u4e53\u7403\u76842D\u8f68\u8ff9\u89c6\u9891\u63a8\u65ad\u5176\u521d\u59cb\u65cb\u8f6c\u548c3D\u8f68\u8ff9\u7684\u65b0\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u65e0\u9700\u771f\u5b9e\u6570\u636e\uff0c\u5b9e\u73b0\u4e8692.0%\u7684\u65cb\u8f6c\u5206\u7c7b\u51c6\u786e\u7387\u548c0.19%\u7684\u56fe\u50cf\u5bf9\u89d2\u7ebf2D\u91cd\u6295\u5f71\u8bef\u5dee\u3002", "motivation": "\u5728\u6807\u51c6\u5e7f\u64ad\u89c6\u9891\u4e2d\uff0c\u4e52\u4e53\u7403\u7684\u65cb\u8f6c\u65e0\u6cd5\u76f4\u63a5\u89c2\u5bdf\uff0c\u4f46\u901a\u8fc7\u5206\u6790\u7403\u7684\u8f68\u8ff9\u53ef\u4ee5\u95f4\u63a5\u63a8\u65ad\u65cb\u8f6c\uff0c\u8fd9\u5bf9\u4e8e\u5206\u6790\u8fd0\u52a8\u5458\u7684\u6280\u672f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u65b9\u6cd5\uff0c\u4ece\u89c6\u9891\u4e2d\u76842D\u8f68\u8ff9\u63a8\u65ad\u521d\u59cb\u65cb\u8f6c\u548c3D\u8f68\u8ff9\u3002\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u901a\u8fc7\u7269\u7406\u6b63\u786e\u7684\u5408\u6210\u6570\u636e\u548c\u6709\u9488\u5bf9\u6027\u7684\u589e\u5f3a\u5b9e\u73b0\u6cdb\u5316\u3002", "result": "\u65b9\u6cd5\u5728\u7b80\u5355\u5355\u76ee\u5e7f\u64ad\u89c6\u9891\u4e2d\u5b9e\u73b0\u4e8692.0%\u7684\u65cb\u8f6c\u5206\u7c7b\u51c6\u786e\u7387\u548c0.19%\u7684\u56fe\u50cf\u5bf9\u89d2\u7ebf2D\u91cd\u6295\u5f71\u8bef\u5dee\uff0c\u65e0\u9700\u4efb\u4f55\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5728\u5355\u76ee\u5e7f\u64ad\u89c6\u9891\u4e2d\u5b9e\u73b0\u4e86\u65cb\u8f6c\u548c\u8f68\u8ff9\u9884\u6d4b\uff0c\u4ec5\u901a\u8fc7\u7b80\u5355\u6280\u672f\u5c31\u80fd\u6cdb\u5316\u5230\u771f\u5b9e\u6570\u636e\uff0c\u5c55\u793a\u4e86\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.19657", "pdf": "https://arxiv.org/pdf/2504.19657", "abs": "https://arxiv.org/abs/2504.19657", "authors": ["Shotaro Takasu", "Toshio Aoyagi"], "title": "Neuronal correlations shape the scaling behavior of memory capacity and nonlinear computational capability of recurrent neural networks", "categories": ["cond-mat.dis-nn", "cs.LG", "q-bio.NC"], "comment": "19 pages, 8 figures", "summary": "Reservoir computing is a powerful framework for real-time information\nprocessing, characterized by its high computational ability and quick learning,\nwith applications ranging from machine learning to biological systems. In this\npaper, we demonstrate that the memory capacity of a reservoir recurrent neural\nnetwork scales sublinearly with the number of readout neurons. To elucidate\nthis phenomenon, we develop a theoretical framework for analytically deriving\nmemory capacity, attributing the decaying growth of memory capacity to neuronal\ncorrelations. In addition, numerical simulations reveal that once memory\ncapacity becomes sublinear, increasing the number of readout neurons\nsuccessively enables nonlinear processing at progressively higher polynomial\norders. Furthermore, our theoretical framework suggests that neuronal\ncorrelations govern not only memory capacity but also the sequential growth of\nnonlinear computational capabilities. Our findings establish a foundation for\ndesigning scalable and cost-effective reservoir computing, providing novel\ninsights into the interplay among neuronal correlations, linear memory, and\nnonlinear processing.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u50a8\u5c42\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7684\u8bb0\u5fc6\u5bb9\u91cf\u968f\u8bfb\u51fa\u795e\u7ecf\u5143\u6570\u91cf\u5448\u6b21\u7ebf\u6027\u589e\u957f\uff0c\u5e76\u63ed\u793a\u4e86\u795e\u7ecf\u5143\u76f8\u5173\u6027\u5bf9\u8bb0\u5fc6\u548c\u975e\u7ebf\u6027\u8ba1\u7b97\u80fd\u529b\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u50a8\u5c42\u8ba1\u7b97\u6846\u67b6\u4e2d\u8bb0\u5fc6\u5bb9\u91cf\u7684\u589e\u957f\u89c4\u5f8b\u53ca\u5176\u4e0e\u795e\u7ecf\u5143\u76f8\u5173\u6027\u7684\u5173\u7cfb\uff0c\u4e3a\u8bbe\u8ba1\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u7684\u50a8\u5c42\u8ba1\u7b97\u5960\u5b9a\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u6846\u67b6\u5206\u6790\u8bb0\u5fc6\u5bb9\u91cf\uff0c\u7ed3\u5408\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u6b21\u7ebf\u6027\u589e\u957f\u73b0\u8c61\uff0c\u5e76\u7814\u7a76\u795e\u7ecf\u5143\u76f8\u5173\u6027\u5bf9\u975e\u7ebf\u6027\u8ba1\u7b97\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u8bb0\u5fc6\u5bb9\u91cf\u968f\u8bfb\u51fa\u795e\u7ecf\u5143\u6570\u91cf\u6b21\u7ebf\u6027\u589e\u957f\uff0c\u4e14\u795e\u7ecf\u5143\u76f8\u5173\u6027\u9a71\u52a8\u975e\u7ebf\u6027\u8ba1\u7b97\u80fd\u529b\u7684\u9010\u6b65\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u4e3a\u50a8\u5c42\u8ba1\u7b97\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u63ed\u793a\u4e86\u795e\u7ecf\u5143\u76f8\u5173\u6027\u3001\u7ebf\u6027\u8bb0\u5fc6\u4e0e\u975e\u7ebf\u6027\u5904\u7406\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002"}}
{"id": "2504.19900", "pdf": "https://arxiv.org/pdf/2504.19900", "abs": "https://arxiv.org/abs/2504.19900", "authors": ["Han Chen", "Anne L. Martel"], "title": "Breast Cancer Detection from Multi-View Screening Mammograms with Visual Prompt Tuning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate detection of breast cancer from high-resolution mammograms is\ncrucial for early diagnosis and effective treatment planning. Previous studies\nhave shown the potential of using single-view mammograms for breast cancer\ndetection. However, incorporating multi-view data can provide more\ncomprehensive insights. Multi-view classification, especially in medical\nimaging, presents unique challenges, particularly when dealing with\nlarge-scale, high-resolution data. In this work, we propose a novel Multi-view\nVisual Prompt Tuning Network (MVPT-NET) for analyzing multiple screening\nmammograms. We first pretrain a robust single-view classification model on\nhigh-resolution mammograms and then innovatively adapt multi-view feature\nlearning into a task-specific prompt tuning process. This technique selectively\ntunes a minimal set of trainable parameters (7\\%) while retaining the\nrobustness of the pre-trained single-view model, enabling efficient integration\nof multi-view data without the need for aggressive downsampling. Our approach\noffers an efficient alternative to traditional feature fusion methods,\nproviding a more robust, scalable, and efficient solution for high-resolution\nmammogram analysis. Experimental results on a large multi-institution dataset\ndemonstrate that our method outperforms conventional approaches while\nmaintaining detection efficiency, achieving an AUROC of 0.852 for\ndistinguishing between Benign, DCIS, and Invasive classes. This work highlights\nthe potential of MVPT-NET for medical imaging tasks and provides a scalable\nsolution for integrating multi-view data in breast cancer detection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u89c6\u89d2\u89c6\u89c9\u63d0\u793a\u8c03\u4f18\u7f51\u7edc\uff08MVPT-NET\uff09\uff0c\u7528\u4e8e\u9ad8\u6548\u6574\u5408\u591a\u89c6\u89d2\u4e73\u817aX\u5149\u7247\u6570\u636e\uff0c\u4ee5\u63d0\u5347\u4e73\u817a\u764c\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u89c6\u89d2\u4e73\u817aX\u5149\u7247\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u591a\u89c6\u89d2\u6570\u636e\u80fd\u591f\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u4fe1\u606f\uff0c\u4f46\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u65f6\u5b58\u5728\u6311\u6218\u3002", "method": "\u5148\u9884\u8bad\u7ec3\u4e00\u4e2a\u5355\u89c6\u89d2\u5206\u7c7b\u6a21\u578b\uff0c\u7136\u540e\u5c06\u591a\u89c6\u89d2\u7279\u5f81\u5b66\u4e60\u878d\u5165\u4efb\u52a1\u7279\u5b9a\u7684\u63d0\u793a\u8c03\u4f18\u8fc7\u7a0b\uff0c\u4ec5\u8c03\u6574\u5c11\u91cf\u53c2\u6570\uff087%\uff09\u3002", "result": "\u5728\u5927\u578b\u591a\u673a\u6784\u6570\u636e\u96c6\u4e0a\uff0cMVPT-NET\u7684AUROC\u8fbe\u52300.852\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "MVPT-NET\u4e3a\u9ad8\u5206\u8fa8\u7387\u4e73\u817aX\u5149\u7247\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u591a\u89c6\u89d2\u6574\u5408\u65b9\u6848\u3002"}}
{"id": "2504.19682", "pdf": "https://arxiv.org/pdf/2504.19682", "abs": "https://arxiv.org/abs/2504.19682", "authors": ["Nikolaos Chaidos", "Angeliki Dimitriou", "Nikolaos Spanos", "Athanasios Voulodimos", "Giorgos Stamou"], "title": "Explaining Vision GNNs: A Semantic and Visual Analysis of Graph-based Image Classification", "categories": ["cs.CV", "cs.LG"], "comment": "13 pages, 3 figures, accepted for presentation at\n  xAI-World-Conference 2025, code is available at\n  https://github.com/nickhaidos/Vision-GNNs-Explainer", "summary": "Graph Neural Networks (GNNs) have emerged as an efficient alternative to\nconvolutional approaches for vision tasks such as image classification,\nleveraging patch-based representations instead of raw pixels. These methods\nconstruct graphs where image patches serve as nodes, and edges are established\nbased on patch similarity or classification relevance. Despite their\nefficiency, the explainability of GNN-based vision models remains\nunderexplored, even though graphs are naturally interpretable. In this work, we\nanalyze the semantic consistency of the graphs formed at different layers of\nGNN-based image classifiers, focusing on how well they preserve object\nstructures and meaningful relationships. A comprehensive analysis is presented\nby quantifying the extent to which inter-layer graph connections reflect\nsemantic similarity and spatial coherence. Explanations from standard and\nadversarial settings are also compared to assess whether they reflect the\nclassifiers' robustness. Additionally, we visualize the flow of information\nacross layers through heatmap-based visualization techniques, thereby\nhighlighting the models' explainability. Our findings demonstrate that the\ndecision-making processes of these models can be effectively explained, while\nalso revealing that their reasoning does not necessarily align with human\nperception, especially in deeper layers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u56fe\u50cf\u5206\u7c7b\u5668\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u5c42\u4e2d\u56fe\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u53ca\u5176\u5bf9\u5bf9\u8c61\u7ed3\u6784\u548c\u5173\u7cfb\u7684\u4fdd\u7559\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u91cf\u5316\u8bc4\u4f30\u548c\u53ef\u89c6\u5316\u6280\u672f\u63ed\u793a\u4e86\u6a21\u578b\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002", "motivation": "\u5c3d\u7ba1GNN\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\uff0c\u4f46\u5176\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u4ecd\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u56fe\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\u662f\u5426\u80fd\u4e0e\u4eba\u7c7b\u611f\u77e5\u4e00\u81f4\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u901a\u8fc7\u91cf\u5316\u5206\u6790\u4e0d\u540c\u5c42\u95f4\u56fe\u7684\u8fde\u63a5\u662f\u5426\u53cd\u6620\u8bed\u4e49\u76f8\u4f3c\u6027\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u5e76\u7ed3\u5408\u6807\u51c6\u4e0e\u5bf9\u6297\u8bbe\u7f6e\u4e0b\u7684\u89e3\u91ca\u5bf9\u6bd4\uff0c\u4ee5\u53ca\u70ed\u529b\u56fe\u53ef\u89c6\u5316\u6280\u672f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u7684\u51b3\u7b56\u8fc7\u7a0b\u53ef\u4ee5\u6709\u6548\u89e3\u91ca\uff0c\u4f46\u6df1\u5c42\u7f51\u7edc\u7684\u63a8\u7406\u4e0e\u4eba\u7c7b\u611f\u77e5\u5e76\u4e0d\u5b8c\u5168\u4e00\u81f4\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86GNN\u56fe\u50cf\u5206\u7c7b\u5668\u7684\u53ef\u89e3\u91ca\u6027\u6f5c\u529b\uff0c\u540c\u65f6\u6307\u51fa\u5176\u6df1\u5c42\u63a8\u7406\u4e0e\u4eba\u7c7b\u7406\u89e3\u7684\u5dee\u5f02\uff0c\u4e3a\u8fdb\u4e00\u6b65\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.19918", "pdf": "https://arxiv.org/pdf/2504.19918", "abs": "https://arxiv.org/abs/2504.19918", "authors": ["Hugo Georgenthum", "Cristian Cosentino", "Fabrizio Marozzo", "Pietro Li\u00f2"], "title": "Enhancing Surgical Documentation through Multimodal Visual-Temporal Transformers and Generative AI", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The automatic summarization of surgical videos is essential for enhancing\nprocedural documentation, supporting surgical training, and facilitating\npost-operative analysis. This paper presents a novel method at the intersection\nof artificial intelligence and medicine, aiming to develop machine learning\nmodels with direct real-world applications in surgical contexts. We propose a\nmulti-modal framework that leverages recent advancements in computer vision and\nlarge language models to generate comprehensive video summaries. % The approach\nis structured in three key stages. First, surgical videos are divided into\nclips, and visual features are extracted at the frame level using visual\ntransformers. This step focuses on detecting tools, tissues, organs, and\nsurgical actions. Second, the extracted features are transformed into\nframe-level captions via large language models. These are then combined with\ntemporal features, captured using a ViViT-based encoder, to produce clip-level\nsummaries that reflect the broader context of each video segment. Finally, the\nclip-level descriptions are aggregated into a full surgical report using a\ndedicated LLM tailored for the summarization task. % We evaluate our method on\nthe CholecT50 dataset, using instrument and action annotations from 50\nlaparoscopic videos. The results show strong performance, achieving 96\\%\nprecision in tool detection and a BERT score of 0.74 for temporal context\nsummarization. This work contributes to the advancement of AI-assisted tools\nfor surgical reporting, offering a step toward more intelligent and reliable\nclinical documentation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7ed3\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u81ea\u52a8\u751f\u6210\u624b\u672f\u89c6\u9891\u6458\u8981\uff0c\u5728\u5de5\u5177\u68c0\u6d4b\u548c\u65f6\u95f4\u4e0a\u4e0b\u6587\u6458\u8981\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u63d0\u5347\u624b\u672f\u89c6\u9891\u81ea\u52a8\u6458\u8981\u80fd\u529b\uff0c\u4ee5\u6539\u5584\u624b\u672f\u8bb0\u5f55\u3001\u652f\u6301\u57f9\u8bad\u5e76\u4fc3\u8fdb\u672f\u540e\u5206\u6790\uff0c\u63a8\u52a8AI\u5728\u533b\u7597\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5206\u4e09\u9636\u6bb5\uff1a1) \u5206\u5272\u89c6\u9891\u5e76\u63d0\u53d6\u89c6\u89c9\u7279\u5f81\uff1b2) \u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5e27\u7ea7\u63cf\u8ff0\u5e76\u6574\u5408\u65f6\u95f4\u7279\u5f81\uff1b3) \u7528\u4e13\u7528LLM\u805a\u5408\u751f\u6210\u5b8c\u6574\u624b\u672f\u62a5\u544a\u3002", "result": "\u5728CholecT50\u6570\u636e\u96c6\u4e0a\uff0c\u5de5\u5177\u68c0\u6d4b\u7cbe\u5ea6\u8fbe96%\uff0c\u65f6\u95f4\u4e0a\u4e0b\u6587\u6458\u8981BERT\u5206\u6570\u4e3a0.74\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aAI\u8f85\u52a9\u624b\u672f\u62a5\u544a\u63d0\u4f9b\u4e86\u667a\u80fd\u5316\u3001\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u4e34\u5e8a\u6587\u6863\u6280\u672f\u7684\u8fdb\u6b65\u3002"}}
{"id": "2504.19684", "pdf": "https://arxiv.org/pdf/2504.19684", "abs": "https://arxiv.org/abs/2504.19684", "authors": ["Anush Lakshman Sivaraman", "Kojo Adu-Gyamfi", "Ibne Farabi Shihab", "Anuj Sharma"], "title": "ClearVision: Leveraging CycleGAN and SigLIP-2 for Robust All-Weather Classification in Traffic Camera Imagery", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Accurate weather classification from low-quality traffic camera imagery\nremains a challenging task, particularly under adverse nighttime conditions. In\nthis study, we propose a scalable framework that combines generative domain\nadaptation with efficient contrastive learning to enhance classification\nperformance. Using CycleGAN-based domain translation, we improve the quality of\nnighttime images, enabling better feature extraction by downstream models.\nWhile the baseline EVA-02 model employing CLIP-based contrastive loss achieves\nan overall accuracy of 96.55\\%, it exhibits a significant performance gap\nbetween daytime (97.21\\%) and nighttime conditions (63.40\\%). Replacing CLIP\nwith the lightweight SigLIP-2 (Sigmoid contrastive loss) achieves a competitive\noverall accuracy of 94.00\\%, with substantial improvements in nighttime\nperformance (85.90\\% accuracy). The combination of Vision-SigLIP-2,\nText-SigLIP-2, CycleGAN, and contrastive training achieves the best nighttime\naccuracy (85.90\\%) among all models tested, while EVA-02 with CycleGAN\nmaintains the highest overall accuracy (97.01\\%) and per-class accuracies.\nThese findings demonstrate the potential of combining domain adaptation and\nefficient contrastive learning to build practical, resource-efficient weather\nclassification systems for intelligent transportation infrastructure.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u751f\u6210\u57df\u9002\u5e94\u548c\u9ad8\u6548\u5bf9\u6bd4\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u8d28\u91cf\u4ea4\u901a\u6444\u50cf\u5934\u56fe\u50cf\u5728\u6076\u52a3\u591c\u95f4\u6761\u4ef6\u4e0b\u7684\u5929\u6c14\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u591c\u95f4\u6761\u4ef6\u4e0b\u4f4e\u8d28\u91cf\u7684\u4ea4\u901a\u6444\u50cf\u5934\u56fe\u50cf\u5bfc\u81f4\u5929\u6c14\u5206\u7c7b\u6027\u80fd\u4e0b\u964d\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u57df\u9002\u5e94\u548c\u5bf9\u6bd4\u5b66\u4e60\u6539\u8fdb\u5206\u7c7b\u6548\u679c\u3002", "method": "\u91c7\u7528CycleGAN\u8fdb\u884c\u57df\u8f6c\u6362\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\uff0c\u7ed3\u5408SigLIP-2\u5bf9\u6bd4\u635f\u5931\u548cVision-SigLIP-2\u3001Text-SigLIP-2\u8fdb\u884c\u5bf9\u6bd4\u8bad\u7ec3\u3002", "result": "\u6700\u4f73\u591c\u95f4\u5206\u7c7b\u51c6\u786e\u7387\u8fbe85.90%\uff0c\u6574\u4f53\u51c6\u786e\u7387\u4e3a97.01%\uff0c\u591c\u95f4\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u7ed3\u5408\u57df\u9002\u5e94\u548c\u9ad8\u6548\u5bf9\u6bd4\u5b66\u4e60\u53ef\u6784\u5efa\u8d44\u6e90\u9ad8\u6548\u7684\u5929\u6c14\u5206\u7c7b\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u667a\u80fd\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2504.19944", "pdf": "https://arxiv.org/pdf/2504.19944", "abs": "https://arxiv.org/abs/2504.19944", "authors": ["Markus Bl\u00e4ser", "Julian D\u00f6rfler", "Maciej Li\u015bkiewicz", "Benito van der Zander"], "title": "Probabilistic and Causal Satisfiability: Constraining the Model", "categories": ["cs.CC", "cs.AI", "cs.LO"], "comment": "accepted at ICALP 25", "summary": "We study the complexity of satisfiability problems in probabilistic and\ncausal reasoning. Given random variables $X_1, X_2,\\ldots$ over finite domains,\nthe basic terms are probabilities of propositional formulas over atomic events\n$X_i = x_i$, such as $P(X_1 = x_1)$ or $P(X_1 = x_1 \\vee X_2 = x_2)$. The basic\nterms can be combined using addition (yielding linear terms) or multiplication\n(polynomial terms). The probabilistic satisfiability problem asks whether a\njoint probability distribution satisfies a Boolean combination of\n(in)equalities over such terms. Fagin et al. (1990) showed that for basic and\nlinear terms, this problem is NP-complete, making it no harder than Boolean\nsatisfiability, while Moss\\'e et al. (2022) proved that for polynomial terms,\nit is complete for the existential theory of the reals.\n  Pearl's Causal Hierarchy (PCH) extends the probabilistic setting with\ninterventional and counterfactual reasoning, enriching the expressiveness of\nlanguages. However, Moss\\'e et al. (2022) found that satisfiability complexity\nremains unchanged. Van der Zander et al. (2023) showed that introducing a\nmarginalization operator to languages induces a significant increase in\ncomplexity.\n  We extend this line of work by adding two new dimensions to the problem by\nconstraining the models. First, we fix the graph structure of the underlying\nstructural causal model, motivated by settings like Pearl's do-calculus, and\ngive a nearly complete landscape across different arithmetics and PCH levels.\nSecond, we study small models. While earlier work showed that satisfiable\ninstances admit polynomial-size models, this is no longer guaranteed with\ncompact marginalization. We characterize the complexities of satisfiability\nunder small-model constraints across different settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6982\u7387\u548c\u56e0\u679c\u63a8\u7406\u4e2d\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\u7684\u590d\u6742\u6027\uff0c\u6269\u5c55\u4e86\u5148\u524d\u5de5\u4f5c\uff0c\u589e\u52a0\u6a21\u578b\u7ea6\u675f\u6761\u4ef6\u5e76\u5206\u6790\u5176\u590d\u6742\u6027\u3002", "motivation": "\u63a2\u8ba8\u7ea6\u675f\u6761\u4ef6\u4e0b\uff08\u5982\u56fa\u5b9a\u56e0\u679c\u6a21\u578b\u56fe\u7ed3\u6784\u548c\u5c0f\u6a21\u578b\u8981\u6c42\uff09\u6982\u7387\u548c\u56e0\u679c\u63a8\u7406\u4e2d\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\u7684\u590d\u6742\u6027\u53d8\u5316\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u6269\u5c55\u7814\u7a76\u7ef4\u5ea6\uff1a1) \u56fa\u5b9a\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u7684\u56fe\u7ed3\u6784\uff1b2) \u7814\u7a76\u5c0f\u6a21\u578b\u7ea6\u675f\u4e0b\u7684\u590d\u6742\u6027\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u5bf9\u6bd4\u4e0d\u540c\u7b97\u672f\u548cPCH\u5c42\u7ea7\u4e0b\u7684\u590d\u6742\u6027\u3002", "result": "\u63ed\u793a\u5728\u56fa\u5b9a\u56fe\u7ed3\u6784\u548c\u5c0f\u6a21\u578b\u7ea6\u675f\u4e0b\uff0c\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\u7684\u590d\u6742\u6027\u53d8\u5316\uff0c\u5e76\u63d0\u4f9b\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u590d\u6742\u6027\u5206\u7c7b\u3002", "conclusion": "\u6a21\u578b\u7ea6\u675f\uff08\u5982\u56fa\u5b9a\u56fe\u7ed3\u6784\u6216\u5c0f\u6a21\u578b\uff09\u663e\u8457\u5f71\u54cd\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\u7684\u590d\u6742\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2504.19949", "pdf": "https://arxiv.org/pdf/2504.19949", "abs": "https://arxiv.org/abs/2504.19949", "authors": ["Aydo\u011fan Soylu", "Tufan Kumbasar"], "title": "Capturing Aerodynamic Characteristics of ATTAS Aircraft with Evolving Intelligent System", "categories": ["eess.SY", "cs.AI", "cs.SY"], "comment": "in International Congress on Human-Computer Interaction, Optimization\n  and Robotic Applications, 2025", "summary": "Accurate modeling of aerodynamic coefficients is crucial for understanding\nand optimizing the performance of modern aircraft systems. This paper presents\nthe novel deployment of an Evolving Type-2 Quantum Fuzzy Neural Network\n(eT2QFNN) for modeling the aerodynamic coefficients of the ATTAS aircraft to\nexpress the aerodynamic characteristics. eT2QFNN can represent the nonlinear\naircraft model by creating multiple linear submodels with its rule-based\nstructure through an incremental learning strategy rather than a traditional\nbatch learning approach. Moreover, it enhances robustness to uncertainties and\ndata noise through its quantum membership functions, as well as its automatic\nrule-learning and parameter-tuning capabilities. During the estimation of the\naerodynamic coefficients via the flight data of the ATTAS, two different\nstudies are conducted in the training phase: one with a large amount of data\nand the other with a limited amount of data. The results show that the modeling\nperformance of the eT2QFNN is superior in comparison to baseline counterparts.\nFurthermore, eT2QFNN estimated the aerodynamic model with fewer rules compared\nto Type-1 fuzzy counterparts. In addition, by applying the Delta method to the\nproposed approach, the stability and control derivatives of the aircraft are\nanalyzed. The results prove the superiority of the proposed eT2QFNN in\nrepresenting aerodynamic coefficients.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684Evolving Type-2 Quantum Fuzzy Neural Network (eT2QFNN)\u7528\u4e8e\u7cbe\u786e\u5efa\u6a21ATTAS\u98de\u673a\u7684\u6c14\u52a8\u7cfb\u6570\uff0c\u901a\u8fc7\u589e\u91cf\u5b66\u4e60\u548c\u91cf\u5b50\u96b6\u5c5e\u51fd\u6570\u63d0\u5347\u5bf9\u4e0d\u786e\u5b9a\u6027\u548c\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u8bad\u7ec3\u9636\u6bb5\u8fdb\u884c\u4e86\u5927\u91cf\u6570\u636e\u548c\u6709\u9650\u6570\u636e\u7684\u5bf9\u6bd4\u7814\u7a76\uff0c\u7ed3\u679c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u7cbe\u786e\u5efa\u6a21\u6c14\u52a8\u7cfb\u6570\u5bf9\u73b0\u4ee3\u822a\u7a7a\u7cfb\u7edf\u7684\u6027\u80fd\u7406\u89e3\u548c\u4f18\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u6570\u636e\u566a\u58f0\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528eT2QFNN\uff0c\u901a\u8fc7\u589e\u91cf\u5b66\u4e60\u7b56\u7565\u548c\u91cf\u5b50\u96b6\u5c5e\u51fd\u6570\uff0c\u81ea\u52a8\u5b66\u4e60\u89c4\u5219\u548c\u8c03\u6574\u53c2\u6570\uff0c\u751f\u6210\u591a\u4e2a\u7ebf\u6027\u5b50\u6a21\u578b\u6765\u8868\u5f81\u975e\u7ebf\u6027\u98de\u673a\u7279\u6027\uff0c\u5e76\u5728ATTAS\u98de\u884c\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u4e24\u79cd\u6570\u636e\u89c4\u6a21\u7684\u8bad\u7ec3\u7814\u7a76\u3002", "result": "eT2QFNN\u5728\u6c14\u52a8\u7cfb\u6570\u5efa\u6a21\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u89c4\u5219\u6570\u91cf\u5c11\u4e8eType-1\u6a21\u7cca\u65b9\u6cd5\uff0c\u901a\u8fc7Delta\u65b9\u6cd5\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u5728\u7a33\u5b9a\u6027\u4e0e\u63a7\u5236\u5bfc\u6570\u5206\u6790\u4e2d\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "eT2QFNN\u5728\u6c14\u52a8\u7cfb\u6570\u5efa\u6a21\u4e2d\u5c55\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u590d\u6742\u975e\u7ebf\u6027\u7cfb\u7edf\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.19722", "pdf": "https://arxiv.org/pdf/2504.19722", "abs": "https://arxiv.org/abs/2504.19722", "authors": ["Rupert Polley", "Nikolai Polley", "Dominik Heid", "Marc Heinrich", "Sven Ochs", "J. Marius Z\u00f6llner"], "title": "The ATLAS of Traffic Lights: A Reliable Perception Framework for Autonomous Driving", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at IEEE Intelligent Vehicles Symposium (IV 2025). Dataset\n  link: https://url.fzi.de/ATLAS", "summary": "Traffic light perception is an essential component of the camera-based\nperception system for autonomous vehicles, enabling accurate detection and\ninterpretation of traffic lights to ensure safe navigation through complex\nurban environments. In this work, we propose a modularized perception framework\nthat integrates state-of-the-art detection models with a novel real-time\nassociation and decision framework, enabling seamless deployment into an\nautonomous driving stack. To address the limitations of existing public\ndatasets, we introduce the ATLAS dataset, which provides comprehensive\nannotations of traffic light states and pictograms across diverse environmental\nconditions and camera setups. This dataset is publicly available at\nhttps://url.fzi.de/ATLAS. We train and evaluate several state-of-the-art\ntraffic light detection architectures on ATLAS, demonstrating significant\nperformance improvements in both accuracy and robustness. Finally, we evaluate\nthe framework in real-world scenarios by deploying it in an autonomous vehicle\nto make decisions at traffic light-controlled intersections, highlighting its\nreliability and effectiveness for real-time operation.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u4ea4\u901a\u706f\u611f\u77e5\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5148\u8fdb\u7684\u68c0\u6d4b\u6a21\u578b\u548c\u5b9e\u65f6\u5173\u8054\u51b3\u7b56\u7b97\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86ATLAS\u6570\u636e\u96c6\u4ee5\u63d0\u5347\u6027\u80fd\u9a8c\u8bc1\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u4ea4\u901a\u706f\u611f\u77e5\u7684\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6027\u95ee\u9898\uff0c\u540c\u65f6\u5f25\u8865\u73b0\u6709\u516c\u5171\u6570\u636e\u96c6\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u6a21\u5757\u5316\u611f\u77e5\u6846\u67b6\uff0c\u96c6\u6210\u68c0\u6d4b\u6a21\u578b\u4e0e\u5b9e\u65f6\u5173\u8054\u51b3\u7b56\u7b97\u6cd5\uff0c\u5e76\u4f7f\u7528\u81ea\u7814ATLAS\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728ATLAS\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5728\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u6846\u67b6\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u901a\u706f\u611f\u77e5\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9002\u5408\u5b9e\u65f6\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u3002"}}
{"id": "2504.19951", "pdf": "https://arxiv.org/pdf/2504.19951", "abs": "https://arxiv.org/abs/2504.19951", "authors": ["Vineeth Sai Narajala", "Ken Huang", "Idan Habler"], "title": "Securing GenAI Multi-Agent Systems Against Tool Squatting: A Zero Trust Registry-Based Approach", "categories": ["cs.CR", "cs.AI"], "comment": "12 pages, 4 figures, 1 table", "summary": "The rise of generative AI (GenAI) multi-agent systems (MAS) necessitates\nstandardized protocols enabling agents to discover and interact with external\ntools. However, these protocols introduce new security challenges,\nparticularly; tool squatting; the deceptive registration or representation of\ntools. This paper analyzes tool squatting threats within the context of\nemerging interoperability standards, such as Model Context Protocol (MCP) or\nseamless communication between agents protocols. It introduces a comprehensive\nTool Registry system designed to mitigate these risks. We propose a\nsecurity-focused architecture featuring admin-controlled registration,\ncentralized tool discovery, fine grained access policies enforced via dedicated\nAgent and Tool Registry services, a dynamic trust scoring mechanism based on\ntool versioning and known vulnerabilities, and just in time credential\nprovisioning. Based on its design principles, the proposed registry framework\naims to effectively prevent common tool squatting vectors while preserving the\nflexibility and power of multi-agent systems. This work addresses a critical\nsecurity gap in the rapidly evolving GenAI ecosystem and provides a foundation\nfor secure tool integration in production environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5de5\u5177\u6ce8\u518c\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u751f\u6210\u5f0fAI\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u5de5\u5177\u6b3a\u9a97\u6ce8\u518c\u95ee\u9898\uff0c\u901a\u8fc7\u5b89\u5168\u67b6\u6784\u548c\u52a8\u6001\u4fe1\u4efb\u8bc4\u5206\u6765\u9884\u9632\u98ce\u9669\u3002", "motivation": "\u751f\u6210\u5f0fAI\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u5de5\u5177\u4ea4\u4e92\u7684\u5b89\u5168\u6027\u6311\u6218\uff0c\u5c24\u5176\u662f\u5de5\u5177\u6b3a\u9a97\u6ce8\u518c\u95ee\u9898\uff0c\u9700\u8981\u6807\u51c6\u5316\u534f\u8bae\u6765\u4fdd\u969c\u5b89\u5168\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b89\u5168\u67b6\u6784\uff0c\u5305\u62ec\u7ba1\u7406\u5458\u63a7\u5236\u7684\u6ce8\u518c\u3001\u96c6\u4e2d\u5f0f\u5de5\u5177\u53d1\u73b0\u3001\u7ec6\u7c92\u5ea6\u8bbf\u95ee\u7b56\u7565\u3001\u52a8\u6001\u4fe1\u4efb\u8bc4\u5206\u673a\u5236\u548c\u5373\u65f6\u51ed\u8bc1\u5206\u914d\u3002", "result": "\u7cfb\u7edf\u8bbe\u8ba1\u80fd\u6709\u6548\u9884\u9632\u5de5\u5177\u6b3a\u9a97\u6ce8\u518c\uff0c\u540c\u65f6\u4fdd\u6301\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u548c\u529f\u80fd\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u751f\u6210\u5f0fAI\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u7a7a\u767d\uff0c\u4e3a\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5de5\u5177\u96c6\u6210\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2504.19956", "pdf": "https://arxiv.org/pdf/2504.19956", "abs": "https://arxiv.org/abs/2504.19956", "authors": ["Vineeth Sai Narajala", "Om Narayan"], "title": "Securing Agentic AI: A Comprehensive Threat Model and Mitigation Framework for Generative AI Agents", "categories": ["cs.CR", "cs.AI"], "comment": "12 pages, 2 figures, 1 table", "summary": "As generative AI (GenAI) agents become more common in enterprise settings,\nthey introduce security challenges that differ significantly from those posed\nby traditional systems. These agents are not just LLMs; they reason, remember,\nand act, often with minimal human oversight. This paper introduces a\ncomprehensive threat model tailored specifically for GenAI agents, focusing on\nhow their autonomy, persistent memory access, complex reasoning, and tool\nintegration create novel risks. This research work identifies 9 primary threats\nand organizes them across five key domains: cognitive architecture\nvulnerabilities, temporal persistence threats, operational execution\nvulnerabilities, trust boundary violations, and governance circumvention. These\nthreats are not just theoretical they bring practical challenges such as\ndelayed exploitability, cross-system propagation, cross system lateral\nmovement, and subtle goal misalignments that are hard to detect with existing\nframeworks and standard approaches. To help address this, the research work\npresent two complementary frameworks: ATFAA - Advanced Threat Framework for\nAutonomous AI Agents, which organizes agent-specific risks, and SHIELD, a\nframework proposing practical mitigation strategies designed to reduce\nenterprise exposure. While this work builds on existing work in LLM and AI\nsecurity, the focus is squarely on what makes agents different and why those\ndifferences matter. Ultimately, this research argues that GenAI agents require\na new lens for security. If we fail to adapt our threat models and defenses to\naccount for their unique architecture and behavior, we risk turning a powerful\nnew tool into a serious enterprise liability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u4ee3\u7406\u7684\u5168\u9762\u5a01\u80c1\u6a21\u578b\uff0c\u8bc6\u522b\u4e869\u79cd\u4e3b\u8981\u5a01\u80c1\uff0c\u5e76\u5206\u4e3a\u4e94\u5927\u9886\u57df\u3002\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86ATFAA\u548cSHIELD\u4e24\u4e2a\u6846\u67b6\uff0c\u5206\u522b\u7528\u4e8e\u7ec4\u7ec7\u98ce\u9669\u548c\u5236\u5b9a\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u968f\u7740GenAI\u4ee3\u7406\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u666e\u53ca\uff0c\u5176\u81ea\u4e3b\u6027\u3001\u8bb0\u5fc6\u8bbf\u95ee\u548c\u590d\u6742\u63a8\u7406\u80fd\u529b\u5e26\u6765\u4e86\u4e0e\u4f20\u7edf\u7cfb\u7edf\u4e0d\u540c\u7684\u5b89\u5168\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u5a01\u80c1\u6a21\u578b\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86\u4e13\u95e8\u7684\u5a01\u80c1\u6a21\u578b\uff0c\u8bc6\u522b\u5e76\u5206\u7c7b\u5a01\u80c1\uff0c\u5e76\u63d0\u51fa\u4e86ATFAA\uff08\u98ce\u9669\u7ec4\u7ec7\u6846\u67b6\uff09\u548cSHIELD\uff08\u7f13\u89e3\u7b56\u7565\u6846\u67b6\uff09\u3002", "result": "\u7814\u7a76\u53d1\u73b0GenAI\u4ee3\u7406\u5b58\u57289\u79cd\u65b0\u578b\u5a01\u80c1\uff0c\u4f20\u7edf\u5b89\u5168\u6846\u67b6\u96be\u4ee5\u5e94\u5bf9\u5176\u72ec\u7279\u98ce\u9669\uff0c\u9700\u4e13\u7528\u5de5\u5177\u548c\u7b56\u7565\u3002", "conclusion": "GenAI\u4ee3\u7406\u7684\u5b89\u5168\u9700\u8981\u5168\u65b0\u7684\u89c6\u89d2\u548c\u6846\u67b6\uff0c\u5426\u5219\u53ef\u80fd\u4ece\u5f3a\u5927\u5de5\u5177\u53d8\u4e3a\u4f01\u4e1a\u5b89\u5168\u9690\u60a3\u3002"}}
{"id": "2504.19787", "pdf": "https://arxiv.org/pdf/2504.19787", "abs": "https://arxiv.org/abs/2504.19787", "authors": ["Aditi Nachnani", "Kai K. Li-Caldwell", "Saptarshi Biswas", "Prince Sharma", "Gaoyuan Ouyang", "Prashant Singh"], "title": "Interpretable machine learning-guided design of Fe-based soft magnetic alloys", "categories": ["cond-mat.mtrl-sci", "cond-mat.other", "cs.LG"], "comment": "24 Pages, 6 Figure, 1 Table", "summary": "We present a machine-learning guided approach to predict saturation\nmagnetization (MS) and coercivity (HC) in Fe-rich soft magnetic alloys,\nparticularly Fe-Si-B systems. ML models trained on experimental data reveals\nthat increasing Si and B content reduces MS from 1.81T (DFT~2.04 T) to ~1.54 T\n(DFT~1.56T) in Fe-Si-B, which is attributed to decreased magnetic density and\nstructural modifications. Experimental validation of ML predicted magnetic\nsaturation on Fe-1Si-1B (2.09T), Fe-5Si-5B (2.01T) and Fe-10Si-10B (1.54T)\nalloy compositions further support our findings. These trends are consistent\nwith density functional theory (DFT) predictions, which link increased\nelectronic disorder and band broadening to lower MS values. Experimental\nvalidation on selected alloys confirms the predictive accuracy of the ML model,\nwith good agreement across compositions. Beyond predictive accuracy, detailed\nuncertainty quantification and model interpretability including through feature\nimportance and partial dependence analysis reveals that MS is governed by a\nnonlinear interplay between Fe content, early transition metal ratios, and\nannealing temperature, while HC is more sensitive to processing conditions such\nas ribbon thickness and thermal treatment windows. The ML framework was further\napplied to Fe-Si-B/Cr/Cu/Zr/Nb alloys in a pseudo-quaternary compositional\nspace, which shows comparable magnetic properties to NANOMET\n(Fe84.8Si0.5B9.4Cu0.8 P3.5C1), FINEMET (Fe73.5Si13.5B9 Cu1Nb3), NANOPERM\n(Fe88Zr7B4Cu1), and HITPERM (Fe44Co44Zr7B4Cu1. Our fundings demonstrate the\npotential of ML framework for accelerated search of high-performance, Co- and\nNi-free, soft magnetic materials.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u5bcc\u94c1\u8f6f\u78c1\u5408\u91d1\uff08\u7279\u522b\u662fFe-Si-B\u7cfb\u7edf\uff09\u7684\u9971\u548c\u78c1\u5316\u5f3a\u5ea6\uff08MS\uff09\u548c\u77eb\u987d\u529b\uff08HC\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u548c\u7406\u8bba\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u3002", "motivation": "\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u52a0\u901f\u9ad8\u6027\u80fd\u3001\u4e0d\u542b\u94b4\u548c\u954d\u7684\u8f6f\u78c1\u6750\u6599\u7684\u641c\u7d22\uff0c\u540c\u65f6\u51cf\u5c11\u5b9e\u9a8c\u6210\u672c\u548c\u65f6\u95f4\u3002", "method": "\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u5b9e\u9a8c\u6570\u636e\uff0c\u7ed3\u5408\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u9884\u6d4b\uff0c\u5206\u6790\u5408\u91d1\u6210\u5206\u5bf9\u78c1\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u589e\u52a0Si\u548cB\u542b\u91cf\u4f1a\u964d\u4f4e\u9971\u548c\u78c1\u5316\u5f3a\u5ea6\uff0c\u800c\u77eb\u987d\u529b\u66f4\u53d7\u5904\u7406\u6761\u4ef6\u5f71\u54cd\u3002\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u4e0e\u5b9e\u9a8c\u6570\u636e\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u8be5\u673a\u5668\u5b66\u4e60\u6846\u67b6\u5728\u9884\u6d4b\u548c\u4f18\u5316\u8f6f\u78c1\u5408\u91d1\u6027\u80fd\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u5f00\u53d1\u65b0\u578b\u65e0\u94b4\u3001\u65e0\u954d\u6750\u6599\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u3002"}}
{"id": "2504.19967", "pdf": "https://arxiv.org/pdf/2504.19967", "abs": "https://arxiv.org/abs/2504.19967", "authors": ["Adway Das", "Agnimitra Sengupta", "S. Ilgin Guler"], "title": "Enhancing short-term traffic prediction by integrating trends and fluctuations with attention mechanism", "categories": ["cs.ET", "cs.AI", "cs.LG", "stat.AP"], "comment": null, "summary": "Traffic flow prediction is a critical component of intelligent transportation\nsystems, yet accurately forecasting traffic remains challenging due to the\ninteraction between long-term trends and short-term fluctuations. Standard deep\nlearning models often struggle with these challenges because their\narchitectures inherently smooth over fine-grained fluctuations while focusing\non general trends. This limitation arises from low-pass filtering effects, gate\nbiases favoring stability, and memory update mechanisms that prioritize\nlong-term information retention. To address these shortcomings, this study\nintroduces a hybrid deep learning framework that integrates both long-term\ntrend and short-term fluctuation information using two input features processed\nin parallel, designed to capture complementary aspects of traffic flow\ndynamics. Further, our approach leverages attention mechanisms, specifically\nBahdanau attention, to selectively focus on critical time steps within traffic\ndata, enhancing the model's ability to predict congestion and other transient\nphenomena. Experimental results demonstrate that features learned from both\nbranches are complementary, significantly improving the goodness-of-fit\nstatistics across multiple prediction horizons compared to a baseline model.\nNotably, the attention mechanism enhances short-term forecast accuracy by\ndirectly targeting immediate fluctuations, though challenges remain in fully\nintegrating long-term trends. This framework can contribute to more effective\ncongestion mitigation and urban mobility planning by advancing the robustness\nand precision of traffic prediction models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u957f\u671f\u8d8b\u52bf\u548c\u77ed\u671f\u6ce2\u52a8\u4fe1\u606f\uff0c\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u6d41\u91cf\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\u4e2d\u56e0\u5e73\u6ed1\u7ec6\u7c92\u5ea6\u6ce2\u52a8\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u89e3\u51b3\u957f\u77ed\u671f\u4fe1\u606f\u878d\u5408\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5e76\u884c\u53cc\u5206\u652f\u7ed3\u6784\u5206\u522b\u5904\u7406\u957f\u77ed\u671f\u7279\u5f81\uff0c\u5e76\u5f15\u5165Bahdanau\u6ce8\u610f\u529b\u673a\u5236\u805a\u7126\u5173\u952e\u65f6\u95f4\u6b65\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u53cc\u5206\u652f\u7279\u5f81\u4e92\u8865\uff0c\u663e\u8457\u63d0\u5347\u591a\u65f6\u95f4\u5c3a\u5ea6\u7684\u62df\u5408\u5ea6\uff0c\u6ce8\u610f\u529b\u673a\u5236\u5c24\u5176\u6539\u5584\u4e86\u77ed\u671f\u9884\u6d4b\u3002", "conclusion": "\u8be5\u6846\u67b6\u589e\u5f3a\u4e86\u9884\u6d4b\u9c81\u68d2\u6027\uff0c\u4f46\u5bf9\u957f\u671f\u8d8b\u52bf\u7684\u5b8c\u5168\u6574\u5408\u4ecd\u9700\u6539\u8fdb\uff0c\u53ef\u52a9\u529b\u62e5\u5835\u7f13\u89e3\u4e0e\u57ce\u5e02\u4ea4\u901a\u89c4\u5212\u3002"}}
{"id": "2504.19797", "pdf": "https://arxiv.org/pdf/2504.19797", "abs": "https://arxiv.org/abs/2504.19797", "authors": ["Gang Mao", "Tousif Rahman", "Sidharth Maheshwari", "Bob Pattison", "Zhuang Shao", "Rishad Shafik", "Alex Yakovlev"], "title": "Dynamic Tsetlin Machine Accelerators for On-Chip Training at the Edge using FPGAs", "categories": ["cs.AR", "cs.LG"], "comment": null, "summary": "The increased demand for data privacy and security in machine learning (ML)\napplications has put impetus on effective edge training on Internet-of-Things\n(IoT) nodes. Edge training aims to leverage speed, energy efficiency and\nadaptability within the resource constraints of the nodes. Deploying and\ntraining Deep Neural Networks (DNNs)-based models at the edge, although\naccurate, posit significant challenges from the back-propagation algorithm's\ncomplexity, bit precision trade-offs, and heterogeneity of DNN layers. This\npaper presents a Dynamic Tsetlin Machine (DTM) training accelerator as an\nalternative to DNN implementations. DTM utilizes logic-based on-chip inference\nwith finite-state automata-driven learning within the same Field Programmable\nGate Array (FPGA) package. Underpinned on the Vanilla and Coalesced Tsetlin\nMachine algorithms, the dynamic aspect of the accelerator design allows for a\nrun-time reconfiguration targeting different datasets, model architectures, and\nmodel sizes without resynthesis. This makes the DTM suitable for targeting\nmultivariate sensor-based edge tasks. Compared to DNNs, DTM trains with fewer\nmultiply-accumulates, devoid of derivative computation. It is a data-centric ML\nalgorithm that learns by aligning Tsetlin automata with input data to form\nlogical propositions enabling efficient Look-up-Table (LUT) mapping and frugal\nBlock RAM usage in FPGA training implementations. The proposed accelerator\noffers 2.54x more Giga operations per second per Watt (GOP/s per W) and uses 6x\nless power than the next-best comparable design.", "AI": {"tldr": "\u6458\u8981\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001Tsetlin\u673a\u5668\uff08DTM\uff09\u8bad\u7ec3\u52a0\u901f\u5668\uff0c\u4ee5\u66ff\u4ee3DNN\u5b9e\u73b0\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\uff0c\u5177\u5907\u9ad8\u6548\u80fd\u548c\u4f4e\u529f\u8017\u7279\u70b9\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u5e94\u7528\u5bf9\u6570\u636e\u9690\u79c1\u548c\u5b89\u5168\u9700\u6c42\u7684\u589e\u52a0\uff0c\u8fb9\u7f18\u8bad\u7ec3\u7684\u9700\u6c42\u4e0a\u5347\uff0c\u4f46DNN\u5728\u8fb9\u7f18\u8282\u70b9\u4e0a\u8bad\u7ec3\u9762\u4e34\u590d\u6742\u6027\u548c\u8d44\u6e90\u9650\u5236\u7684\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6709\u9650\u72b6\u6001\u81ea\u52a8\u673a\u7684DTM\u52a0\u901f\u5668\u8bbe\u8ba1\uff0c\u652f\u6301\u8fd0\u884c\u65f6\u52a8\u6001\u91cd\u6784\uff0c\u65e0\u9700\u91cd\u65b0\u7efc\u5408\u5373\u53ef\u9002\u5e94\u4e0d\u540c\u6570\u636e\u96c6\u548c\u6a21\u578b\u67b6\u6784\uff0c\u4e14\u91c7\u7528\u903b\u8f91\u63a8\u7406\u548c\u9ad8\u6548LUT\u6620\u5c04\u3002", "result": "\u4e0eDNN\u76f8\u6bd4\uff0cDTM\u80fd\u51cf\u5c11\u4e58\u7d2f\u52a0\u64cd\u4f5c\u548c\u5bfc\u6570\u8ba1\u7b97\uff0c\u63d0\u9ad8\u80fd\u6548\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176GOP/s\u6bcf\u74e6\u7279\u63d0\u53472.54\u500d\uff0c\u529f\u8017\u964d\u4f4e6\u500d\u3002", "conclusion": "DTM\u662f\u4e00\u79cd\u9002\u5408\u8fb9\u7f18\u4efb\u52a1\u7684\u6570\u636e\u9ad8\u6548\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u8282\u70b9\u3002"}}
{"id": "2504.19816", "pdf": "https://arxiv.org/pdf/2504.19816", "abs": "https://arxiv.org/abs/2504.19816", "authors": ["Erblin Isaku", "Hassan Sartaj", "Shaukat Ali"], "title": "Digital Twin-based Out-of-Distribution Detection in Autonomous Vessels", "categories": ["eess.SY", "cs.LG", "cs.SE", "cs.SY"], "comment": "34 pages, 12 figures, 11 tables", "summary": "An autonomous vessel (AV) is a complex cyber-physical system (CPS) with\nsoftware enabling many key functionalities, e.g., navigation software enables\nan AV to autonomously or semi-autonomously follow a path to its destination.\nDigital twins of such AVs enable advanced functionalities such as running\nwhat-if scenarios, performing predictive maintenance, and enabling fault\ndiagnosis. Due to technological improvements, real-time analyses using\ncontinuous data from vessels' real-time operations have become increasingly\npossible. However, the literature has little explored developing advanced\nanalyses in real-time data in AVs with digital twins built with machine\nlearning techniques. To this end, we present a novel digital twin-based\napproach (ODDIT) to detect future out-of-distribution (OOD) states of an AV\nbefore reaching them, enabling proactive intervention. Such states may indicate\nanomalies requiring attention (e.g., manual correction by the ship master) and\nassist testers in scenario-centered testing. The digital twin consists of two\nmachine-learning models predicting future vessel states and whether the\npredicted state will be OOD. We evaluated ODDIT with five vessels across\nwaypoint and zigzag maneuvering under simulated conditions, including sensor\nand actuator noise and environmental disturbances i.e., ocean current. ODDIT\nachieved high accuracy in detecting OOD states, with AUROC and TNR@TPR95 scores\nreaching 99\\% across multiple vessels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aODDIT\u7684\u6570\u5b57\u5b6a\u751f\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u524d\u68c0\u6d4b\u81ea\u4e3b\u8239\u8236\uff08AV\uff09\u7684\u5f02\u5e38\u72b6\u6001\uff0c\u652f\u6301\u4e3b\u52a8\u5e72\u9884\u548c\u9ad8\u7cbe\u5ea6\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u6587\u732e\u5bf9\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5b9e\u65f6\u6570\u636e\u9ad8\u7ea7\u5206\u6790\u5728\u81ea\u4e3b\u8239\u8236\u6570\u5b57\u5b6a\u751f\u4e2d\u7684\u5e94\u7528\u63a2\u7d22\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u63d0\u524d\u68c0\u6d4b\u5f02\u5e38\u72b6\u6001\u7684\u65b9\u6cd5\u3002", "method": "ODDIT\u65b9\u6cd5\u7ed3\u5408\u4e86\u4e24\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff1a\u4e00\u4e2a\u9884\u6d4b\u672a\u6765\u8239\u8236\u72b6\u6001\uff0c\u53e6\u4e00\u4e2a\u5224\u65ad\u9884\u6d4b\u72b6\u6001\u662f\u5426\u4e3a\u5f02\u5e38\uff08OOD\uff09\u3002", "result": "\u5728\u6a21\u62df\u6761\u4ef6\u4e0b\uff08\u5305\u62ec\u4f20\u611f\u5668\u566a\u58f0\u548c\u73af\u5883\u5e72\u6270\uff09\uff0cODDIT\u5bf9\u4e94\u79cd\u8239\u8236\u7684\u5f02\u5e38\u72b6\u6001\u68c0\u6d4b\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\uff08AUROC\u548cTNR@TPR95\u8fbe99%\uff09\u3002", "conclusion": "ODDIT\u8bc1\u660e\u4e86\u6570\u5b57\u5b6a\u751f\u5728\u81ea\u4e3b\u8239\u8236\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5b9e\u65f6\u5e72\u9884\u548c\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2504.19985", "pdf": "https://arxiv.org/pdf/2504.19985", "abs": "https://arxiv.org/abs/2504.19985", "authors": ["Keyhan Rayati", "Amirhossein Feizi", "Alireza Beigy", "Pourya Shahverdi", "Mehdi Tale Masouleh", "Ahmad Kalhor"], "title": "Real-Time Imitation of Human Head Motions, Blinks and Emotions by Nao Robot: A Closed-Loop Approach", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "This paper introduces a novel approach for enabling real-time imitation of\nhuman head motion by a Nao robot, with a primary focus on elevating human-robot\ninteractions. By using the robust capabilities of the MediaPipe as a computer\nvision library and the DeepFace as an emotion recognition library, this\nresearch endeavors to capture the subtleties of human head motion, including\nblink actions and emotional expressions, and seamlessly incorporate these\nindicators into the robot's responses. The result is a comprehensive framework\nwhich facilitates precise head imitation within human-robot interactions,\nutilizing a closed-loop approach that involves gathering real-time feedback\nfrom the robot's imitation performance. This feedback loop ensures a high\ndegree of accuracy in modeling head motion, as evidenced by an impressive R2\nscore of 96.3 for pitch and 98.9 for yaw. Notably, the proposed approach holds\npromise in improving communication for children with autism, offering them a\nvaluable tool for more effective interaction. In essence, proposed work\nexplores the integration of real-time head imitation and real-time emotion\nrecognition to enhance human-robot interactions, with potential benefits for\nindividuals with unique communication needs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7Nao\u673a\u5668\u4eba\u5b9e\u65f6\u6a21\u4eff\u4eba\u7c7b\u5934\u90e8\u8fd0\u52a8\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u4f53\u9a8c\u3002\u7ed3\u5408MediaPipe\u89c6\u89c9\u5e93\u548cDeepFace\u60c5\u611f\u8bc6\u522b\u6280\u672f\uff0c\u80fd\u591f\u6355\u6349\u5934\u90e8\u52a8\u4f5c\uff08\u5982\u7728\u773c\uff09\u548c\u8868\u60c5\uff0c\u5e76\u901a\u8fc7\u95ed\u73af\u53cd\u9988\u63d0\u9ad8\u6a21\u4eff\u7cbe\u5ea6\u3002\u5728\u5b9e\u9a8c\u4e2d\uff0cPitch\u548cYaw\u7684R2\u5206\u6570\u5206\u522b\u8fbe\u523096.3\u548c98.9\u3002\u8be5\u65b9\u6cd5\u5c24\u5176\u5bf9\u81ea\u95ed\u75c7\u513f\u7ae5\u7684\u4ea4\u6d41\u6709\u6f5c\u5728\u5e2e\u52a9\u3002", "motivation": "\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u7684\u81ea\u7136\u6027\u548c\u7cbe\u786e\u6027\uff0c\u7279\u522b\u662f\u4e3a\u6709\u7279\u6b8a\u6c9f\u901a\u9700\u6c42\u7684\u7fa4\u4f53\uff08\u5982\u81ea\u95ed\u75c7\u513f\u7ae5\uff09\u63d0\u4f9b\u66f4\u597d\u7684\u4e92\u52a8\u5de5\u5177\u3002", "method": "\u91c7\u7528MediaPipe\u5b9e\u65f6\u6355\u6349\u5934\u90e8\u8fd0\u52a8\u548cDeepFace\u8bc6\u522b\u60c5\u611f\uff0c\u7ed3\u5408\u95ed\u73af\u53cd\u9988\u673a\u5236\u4f18\u5316\u673a\u5668\u4eba\u54cd\u5e94\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5934\u90e8\u8fd0\u52a8\u6a21\u4eff\uff08Pitch R2=96.3, Yaw R2=98.9\uff09\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u5408\u5b9e\u65f6\u5934\u90e8\u6a21\u4eff\u548c\u60c5\u611f\u8bc6\u522b\u7684\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u4eba\u673a\u4ea4\u4e92\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u8f85\u52a9\u6c9f\u901a\u969c\u788d\u7fa4\u4f53\u65b9\u9762\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.19990", "pdf": "https://arxiv.org/pdf/2504.19990", "abs": "https://arxiv.org/abs/2504.19990", "authors": ["Salem Lahlou"], "title": "Mitigating Societal Cognitive Overload in the Age of AI: Challenges and Directions", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Societal cognitive overload, driven by the deluge of information and\ncomplexity in the AI age, poses a critical challenge to human well-being and\nsocietal resilience. This paper argues that mitigating cognitive overload is\nnot only essential for improving present-day life but also a crucial\nprerequisite for navigating the potential risks of advanced AI, including\nexistential threats. We examine how AI exacerbates cognitive overload through\nvarious mechanisms, including information proliferation, algorithmic\nmanipulation, automation anxieties, deregulation, and the erosion of meaning.\nThe paper reframes the AI safety debate to center on cognitive overload,\nhighlighting its role as a bridge between near-term harms and long-term risks.\nIt concludes by discussing potential institutional adaptations, research\ndirections, and policy considerations that arise from adopting an\noverload-resilient perspective on human-AI alignment, suggesting pathways for\nfuture exploration rather than prescribing definitive solutions.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u65f6\u4ee3\u4fe1\u606f\u8fc7\u8f7d\u5bf9\u4eba\u7c7b\u798f\u7949\u548c\u793e\u4f1a\u97e7\u6027\u7684\u6311\u6218\uff0c\u63d0\u51fa\u7f13\u89e3\u8ba4\u77e5\u8fc7\u8f7d\u5bf9\u5e94\u5bf9AI\u957f\u671f\u98ce\u9669\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u5728AI\u65f6\u4ee3\uff0c\u4fe1\u606f\u7206\u70b8\u548c\u590d\u6742\u6027\u5bfc\u81f4\u793e\u4f1a\u8ba4\u77e5\u8fc7\u8f7d\uff0c\u5a01\u80c1\u4eba\u7c7b\u798f\u7949\u548c\u793e\u4f1a\u97e7\u6027\uff0c\u9700\u91cd\u65b0\u5ba1\u89c6AI\u5b89\u5168\u95ee\u9898\u7684\u6838\u5fc3\u3002", "method": "\u901a\u8fc7\u5206\u6790AI\u52a0\u5267\u8ba4\u77e5\u8fc7\u8f7d\u7684\u673a\u5236\uff08\u5982\u4fe1\u606f\u6cdb\u6ee5\u3001\u7b97\u6cd5\u64cd\u63a7\u7b49\uff09\uff0c\u5c06AI\u5b89\u5168\u8fa9\u8bba\u91cd\u65b0\u805a\u7126\u4e8e\u8ba4\u77e5\u8fc7\u8f7d\u95ee\u9898\u3002", "result": "\u8ba4\u77e5\u8fc7\u8f7d\u662f\u8fde\u63a5AI\u77ed\u671f\u5371\u5bb3\u4e0e\u957f\u671f\u98ce\u9669\u7684\u6865\u6881\uff0c\u9700\u4ece\u5236\u5ea6\u3001\u7814\u7a76\u548c\u653f\u7b56\u89d2\u5ea6\u63d0\u5347\u8fc7\u8f7d\u97e7\u6027\u3002", "conclusion": "\u672a\u6765\u9700\u63a2\u7d22\u66f4\u5177\u9002\u5e94\u6027\u7684\u5236\u5ea6\u8bbe\u8ba1\u548c\u653f\u7b56\u6846\u67b6\uff0c\u800c\u975e\u63d0\u4f9b\u660e\u786e\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u5e94\u5bf9AI\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684\u6311\u6218\u3002"}}
{"id": "2504.19996", "pdf": "https://arxiv.org/pdf/2504.19996", "abs": "https://arxiv.org/abs/2504.19996", "authors": ["Andreas Kalogeras", "Dimitrios Bormpoudakis", "Iason Tsardanidis", "Dimitra A. Loka", "Charalampos Kontoes"], "title": "Monitoring digestate application on agricultural crops using Sentinel-2 Satellite imagery", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The widespread use of Exogenous Organic Matter in agriculture necessitates\nmonitoring to assess its effects on soil and crop health. This study evaluates\noptical Sentinel-2 satellite imagery for detecting digestate application, a\npractice that enhances soil fertility but poses environmental risks like\nmicroplastic contamination and nitrogen losses. In the first instance,\nSentinel-2 satellite image time series (SITS) analysis of specific indices\n(EOMI, NDVI, EVI) was used to characterize EOM's spectral behavior after\napplication on the soils of four different crop types in Thessaly, Greece.\nFurthermore, Machine Learning (ML) models (namely Random Forest, k-NN, Gradient\nBoosting and a Feed-Forward Neural Network), were used to investigate digestate\npresence detection, achieving F1-scores up to 0.85. The findings highlight the\npotential of combining remote sensing and ML for scalable and cost-effective\nmonitoring of EOM applications, supporting precision agriculture and\nsustainability.", "AI": {"tldr": "\u4f7f\u7528Sentinel-2\u536b\u661f\u5f71\u50cf\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u68c0\u6d4b\u519c\u4e1a\u4e2d\u5916\u6e90\u6709\u673a\u7269\u8d28\u7684\u5e94\u7528\u6548\u679c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u9ad8\u6548\u76d1\u6d4b\u6f5c\u529b\u3002", "motivation": "\u5916\u6e90\u6709\u673a\u7269\u8d28\u5728\u519c\u4e1a\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u9700\u8981\u5bf9\u5176\u5bf9\u571f\u58e4\u548c\u4f5c\u7269\u5065\u5eb7\u7684\u5f71\u54cd\u8fdb\u884c\u76d1\u6d4b\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5982\u4f55\u5229\u7528\u9065\u611f\u6280\u672f\u548c\u673a\u5668\u5b66\u4e60\u9ad8\u6548\u76d1\u6d4b\u8fd9\u79cd\u5e94\u7528\u3002", "method": "\u901a\u8fc7Sentinel-2\u536b\u661f\u5f71\u50cf\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u7279\u5b9a\u6307\u6570\uff08EOMI\u3001NDVI\u3001EVI\uff09\uff0c\u5e76\u7ed3\u5408\u56db\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u968f\u673a\u68ee\u6797\u3001k-NN\u3001\u68af\u5ea6\u63d0\u5347\u548c\u524d\u9988\u795e\u7ecf\u7f51\u7edc\uff09\u68c0\u6d4b\u6d88\u5316\u7269\u7684\u5b58\u5728\u3002", "result": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u68c0\u6d4b\u6d88\u5316\u7269\u5b58\u5728\u65f6\u8fbe\u5230\u4e86\u6700\u9ad80.85\u7684F1\u5206\u6570\uff0c\u663e\u793a\u4e86\u9065\u611f\u4e0e\u673a\u5668\u5b66\u4e60\u7ed3\u5408\u7684\u6f5c\u529b\u3002", "conclusion": "\u7ed3\u5408\u9065\u611f\u548c\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\u4e3a\u76d1\u6d4b\u5916\u6e90\u6709\u673a\u7269\u8d28\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u7cbe\u51c6\u519c\u4e1a\u548c\u53ef\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2504.19925", "pdf": "https://arxiv.org/pdf/2504.19925", "abs": "https://arxiv.org/abs/2504.19925", "authors": ["Athinagoras Skiadopoulos", "Mark Zhao", "Swapnil Gandhi", "Thomas Norrie", "Shrijeet Mukherjee", "Christos Kozyrakis"], "title": "Accelerating Mixture-of-Experts Training with Adaptive Expert Replication", "categories": ["cs.DC", "cs.LG"], "comment": "Preprint. Under review", "summary": "Mixture-of-Experts (MoE) models have become a widely adopted solution to\ncontinue scaling model sizes without a corresponding linear increase in\ncompute. During MoE model training, each input token is dynamically routed to a\nsubset of experts -- sparsely-activated feed-forward networks -- within each\ntransformer layer. The distribution of tokens assigned to each expert varies\nwidely and rapidly over the course of training. To handle the wide load\nimbalance across experts, current systems are forced to either drop tokens\nassigned to popular experts, degrading convergence, or frequently rebalance\nresources allocated to each expert based on popularity, incurring high state\nmigration overheads.\n  To break this performance-accuracy tradeoff, we introduce SwiftMoE, an\nadaptive MoE training system. The key insight of SwiftMoE is to decouple the\nplacement of expert parameters from their large optimizer state. SwiftMoE\nstatically partitions the optimizer of each expert across all training nodes.\nMeanwhile, SwiftMoE dynamically adjusts the placement of expert parameters by\nrepurposing existing weight updates, avoiding migration overheads. In doing so,\nSwiftMoE right-sizes the GPU resources allocated to each expert, on a\nper-iteration basis, with minimal overheads. Compared to state-of-the-art MoE\ntraining systems, DeepSpeed and FlexMoE, SwiftMoE is able to achieve a 30.5%\nand 25.9% faster time-to-convergence, respectively.", "AI": {"tldr": "SwiftMoE \u662f\u4e00\u79cd\u81ea\u9002\u5e94 MoE \u8bad\u7ec3\u7cfb\u7edf\uff0c\u901a\u8fc7\u89e3\u8026\u4e13\u5bb6\u53c2\u6570\u548c\u4f18\u5316\u5668\u72b6\u6001\u7684\u5206\u533a\uff0c\u51cf\u5c11\u8fc1\u79fb\u5f00\u9500\u5e76\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff0c\u6bd4\u5176\u4ed6\u5148\u8fdb\u7cfb\u7edf\u5feb 25.9-30.5%\u3002", "motivation": "\u73b0\u6709\u7684 MoE \u8bad\u7ec3\u7cfb\u7edf\u5728\u8d1f\u8f7d\u4e0d\u5747\u65f6\uff0c\u8981\u4e48\u727a\u7272\u6536\u655b\u6027\u4e22\u5f03\u70ed\u95e8\u4e13\u5bb6\u7684\u4ee4\u724c\uff0c\u8981\u4e48\u9891\u7e41\u91cd\u65b0\u5206\u914d\u8d44\u6e90\u5bfc\u81f4\u9ad8\u8fc1\u79fb\u5f00\u9500\u3002", "method": "SwiftMoE \u901a\u8fc7\u9759\u6001\u5206\u533a\u4f18\u5316\u5668\u72b6\u6001\u3001\u52a8\u6001\u8c03\u6574\u4e13\u5bb6\u53c2\u6570\u5e03\u5c40\uff0c\u5229\u7528\u73b0\u6709\u6743\u91cd\u66f4\u65b0\u907f\u514d\u8fc1\u79fb\u5f00\u9500\uff0c\u5b9e\u73b0\u9ad8\u6548\u8d44\u6e90\u5206\u914d\u3002", "result": "SwiftMoE \u5728\u6536\u655b\u65f6\u95f4\u4e0a\u6bd4 DeepSpeed \u548c FlexMoE \u5206\u522b\u5feb 30.5% \u548c 25.9%\u3002", "conclusion": "SwiftMoE \u7a81\u7834\u4e86\u6027\u80fd\u4e0e\u51c6\u786e\u6027\u7684\u6743\u8861\uff0c\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86 MoE \u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2504.19997", "pdf": "https://arxiv.org/pdf/2504.19997", "abs": "https://arxiv.org/abs/2504.19997", "authors": ["Ivo Brett"], "title": "Simplified and Secure MCP Gateways for Enterprise AI Integration", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The increased adoption of the Model Context Protocol (MCP) for AI Agents\nnecessitates robust security for Enterprise integrations. This paper introduces\nthe MCP Gateway to simplify self-hosted MCP server integration. The proposed\narchitecture integrates security principles, authentication, intrusion\ndetection, and secure tunneling, enabling secure self-hosting without exposing\ninfrastructure. Key contributions include a reference architecture, threat\nmodel mapping, simplified integration strategies, and open-source\nimplementation recommendations. This work focuses on the unique challenges of\nenterprise-centric, self-hosted AI integrations, unlike existing public MCP\nserver solutions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMCP Gateway\uff0c\u4e3a\u4f01\u4e1a\u81ea\u6258\u7ba1MCP\u670d\u52a1\u5668\u96c6\u6210\u63d0\u4f9b\u5b89\u5168\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u542b\u53c2\u8003\u67b6\u6784\u3001\u5a01\u80c1\u6a21\u578b\u548c\u5f00\u6e90\u5b9e\u73b0\u5efa\u8bae\u3002", "motivation": "\u968f\u7740MCP\u5728AI\u4ee3\u7406\u4e2d\u7684\u5e7f\u6cdb\u91c7\u7528\uff0c\u4f01\u4e1a\u96c6\u6210\u9700\u8981\u66f4\u5f3a\u7684\u5b89\u5168\u4fdd\u969c\uff0c\u800c\u73b0\u6709\u516c\u5171MCP\u89e3\u51b3\u65b9\u6848\u65e0\u6cd5\u6ee1\u8db3\u81ea\u6258\u7ba1\u573a\u666f\u7684\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1MCP Gateway\u67b6\u6784\uff0c\u6574\u5408\u5b89\u5168\u539f\u5219\u3001\u8ba4\u8bc1\u3001\u5165\u4fb5\u68c0\u6d4b\u548c\u5b89\u5168\u96a7\u9053\u6280\u672f\uff0c\u5b9e\u73b0\u5b89\u5168\u7684\u81ea\u6258\u7ba1\u96c6\u6210\u3002", "result": "\u63d0\u51fa\u4e86\u53c2\u8003\u67b6\u6784\u3001\u5a01\u80c1\u6a21\u578b\u6620\u5c04\u548c\u7b80\u5316\u96c6\u6210\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u5b9e\u73b0\u9a8c\u8bc1\u5176\u53ef\u884c\u6027\u3002", "conclusion": "MCP Gateway\u6709\u6548\u89e3\u51b3\u4e86\u4f01\u4e1a\u81ea\u6258\u7ba1AI\u96c6\u6210\u7684\u72ec\u7279\u5b89\u5168\u6311\u6218\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2504.20018", "pdf": "https://arxiv.org/pdf/2504.20018", "abs": "https://arxiv.org/abs/2504.20018", "authors": ["Jiongli Zhu", "Yue Wang", "Bailu Ding", "Philip A. Bernstein", "Vivek Narasayya", "Surajit Chaudhuri"], "title": "MINT: Multi-Vector Search Index Tuning", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "Vector search plays a crucial role in many real-world applications. In\naddition to single-vector search, multi-vector search becomes important for\nmulti-modal and multi-feature scenarios today. In a multi-vector database, each\nrow is an item, each column represents a feature of items, and each cell is a\nhigh-dimensional vector. In multi-vector databases, the choice of indexes can\nhave a significant impact on performance. Although index tuning for relational\ndatabases has been extensively studied, index tuning for multi-vector search\nremains unclear and challenging. In this paper, we define multi-vector search\nindex tuning and propose a framework to solve it. Specifically, given a\nmulti-vector search workload, we develop algorithms to find indexes that\nminimize latency and meet storage and recall constraints. Compared to the\nbaseline, our latency achieves 2.1X to 8.3X speedup.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5411\u91cf\u641c\u7d22\u7d22\u5f15\u8c03\u4f18\u6846\u67b6\uff0c\u901a\u8fc7\u7b97\u6cd5\u4f18\u5316\uff0c\u5728\u6ee1\u8db3\u5b58\u50a8\u548c\u53ec\u56de\u7387\u7ea6\u675f\u4e0b\u6700\u5c0f\u5316\u5ef6\u8fdf\uff0c\u6027\u80fd\u63d0\u53472.1X\u81f38.3X\u3002", "motivation": "\u591a\u5411\u91cf\u641c\u7d22\u5728\u591a\u6a21\u6001\u548c\u591a\u7279\u5f81\u573a\u666f\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7d22\u5f15\u9009\u62e9\u5bf9\u6027\u80fd\u5f71\u54cd\u663e\u8457\uff0c\u76ee\u524d\u7f3a\u5c11\u9488\u5bf9\u591a\u5411\u91cf\u641c\u7d22\u7684\u7d22\u5f15\u8c03\u4f18\u65b9\u6cd5\u3002", "method": "\u5b9a\u4e49\u591a\u5411\u91cf\u641c\u7d22\u7d22\u5f15\u8c03\u4f18\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u6846\u67b6\uff0c\u5f00\u53d1\u7b97\u6cd5\u4ee5\u5728\u7ed9\u5b9a\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u4f18\u5316\u7d22\u5f15\u9009\u62e9\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebf\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5ef6\u8fdf\u4e0a\u5b9e\u73b0\u4e862.1\u500d\u52308.3\u500d\u7684\u52a0\u901f\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u5411\u91cf\u641c\u7d22\u4e2d\u7684\u7d22\u5f15\u8c03\u4f18\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u641c\u7d22\u6027\u80fd\u3002"}}
{"id": "2504.19952", "pdf": "https://arxiv.org/pdf/2504.19952", "abs": "https://arxiv.org/abs/2504.19952", "authors": ["Shubhada Agrawal", "Aaditya Ramdas"], "title": "On Stopping Times of Power-one Sequential Tests: Tight Lower and Upper Bounds", "categories": ["math.ST", "cs.LG", "stat.ML", "stat.TH"], "comment": "36 pages", "summary": "We prove two lower bounds for stopping times of sequential tests between\ngeneral composite nulls and alternatives. The first lower bound is for the\nsetting where the type-1 error level $\\alpha$ approaches zero, and equals\n$\\log(1/\\alpha)$ divided by a certain infimum KL divergence, termed\n$\\operatorname{KL_{inf}}$. The second lower bound applies to the setting where\n$\\alpha$ is fixed and $\\operatorname{KL_{inf}}$ approaches 0 (meaning that the\nnull and alternative sets are not separated) and equals $c\n\\operatorname{KL_{inf}}^{-1} \\log \\log \\operatorname{KL_{inf}}^{-1}$ for a\nuniversal constant $c > 0$. We also provide a sufficient condition for matching\nthe upper bounds and show that this condition is met in several special cases.\nGiven past work, these upper and lower bounds are unsurprising in their form;\nour main contribution is the generality in which they hold, for example, not\nrequiring reference measures or compactness of the classes.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u4e86\u5728\u5e7f\u4e49\u590d\u5408\u96f6\u5047\u8bbe\u548c\u66ff\u4ee3\u5047\u8bbe\u4e4b\u95f4\u987a\u5e8f\u6d4b\u8bd5\u505c\u6b62\u65f6\u95f4\u7684\u4e24\u4e2a\u4e0b\u754c\u3002\u7b2c\u4e00\u4e2a\u4e0b\u754c\u9002\u7528\u4e8e\u7c7b\u578b1\u9519\u8bef\u6c34\u5e73\u03b1\u8d8b\u8fd1\u4e8e\u96f6\u7684\u60c5\u5f62\uff0c\u7ed3\u679c\u4e3alog(1/\u03b1)\u9664\u4ee5\u4e00\u4e2a\u7279\u5b9a\u7684KL\u6563\u5ea6\u4e0b\u754c\u3002\u7b2c\u4e8c\u4e2a\u4e0b\u754c\u9002\u7528\u4e8e\u03b1\u56fa\u5b9a\u4e14KL\u6563\u5ea6\u4e0b\u754c\u8d8b\u8fd1\u4e8e\u96f6\u7684\u60c5\u5f62\uff0c\u7ed3\u679c\u4e3ac\u00b7KL_inf^{-1}\u00b7loglog(KL_inf^{-1})\u3002\u8bba\u6587\u8fd8\u63d0\u4f9b\u4e86\u5339\u914d\u4e0a\u754c\u7684\u5145\u5206\u6761\u4ef6\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u6761\u4ef6\u5728\u591a\u4e2a\u7279\u6b8a\u60c5\u51b5\u4e0b\u6210\u7acb\u3002\u4e3b\u8981\u8d21\u732e\u5728\u4e8e\u8fd9\u4e9b\u7ed3\u679c\u7684\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u4f8b\u5982\u65e0\u9700\u53c2\u8003\u6d4b\u5ea6\u6216\u7c7b\u7684\u7d27\u6027\u3002", "motivation": "\u7814\u7a76\u5e7f\u4e49\u590d\u5408\u5047\u8bbe\u4e0b\u987a\u5e8f\u6d4b\u8bd5\u505c\u6b62\u65f6\u95f4\u7684\u4e0b\u754c\uff0c\u6269\u5c55\u73b0\u6709\u7ed3\u679c\u7684\u9002\u7528\u6027\uff0c\u65e0\u9700\u4f9d\u8d56\u53c2\u8003\u6d4b\u5ea6\u6216\u7d27\u6027\u5047\u8bbe\u3002", "method": "\u8bc1\u660e\u4e86\u4e24\u4e2a\u4e0b\u754c\u5b9a\u7406\uff1a\u7b2c\u4e00\u4e2a\u9488\u5bf9\u03b1\u8d8b\u8fd1\u4e8e\u96f6\u7684\u60c5\u5f62\uff0c\u7b2c\u4e8c\u4e2a\u9488\u5bf9KL_inf\u8d8b\u8fd1\u4e8e\u96f6\u7684\u60c5\u5f62\u3002\u540c\u65f6\u63d0\u4f9b\u4e86\u5339\u914d\u4e0a\u754c\u7684\u5145\u5206\u6761\u4ef6\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u591a\u4e2a\u7279\u4f8b\u4e2d\u7684\u9002\u7528\u6027\u3002", "result": "\u5f97\u5230\u4e86\u4e24\u4e2a\u4e0b\u754c\u8868\u8fbe\u5f0f\uff0c\u5e76\u8bc1\u660e\u4e86\u5b83\u4eec\u5728\u5e7f\u6cdb\u6761\u4ef6\u4e0b\u7684\u9002\u7528\u6027\uff0c\u5305\u62ec\u65e0\u9700\u53c2\u8003\u6d4b\u5ea6\u6216\u7d27\u6027\u5047\u8bbe\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u5e7f\u6cdb\u9002\u7528\u7684\u4e0b\u754c\u7ed3\u679c\u6269\u5c55\u4e86\u987a\u5e8f\u6d4b\u8bd5\u505c\u6b62\u65f6\u95f4\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4e3a\u590d\u5408\u5047\u8bbe\u4e0b\u7684\u7edf\u8ba1\u63a8\u65ad\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u7684\u5de5\u5177\u3002"}}
{"id": "2504.19987", "pdf": "https://arxiv.org/pdf/2504.19987", "abs": "https://arxiv.org/abs/2504.19987", "authors": ["Yomn Alkabakibi", "Congwei Xie", "Artem R. Oganov"], "title": "Graph Neural Network Prediction of Nonlinear Optical Properties", "categories": ["cond-mat.mtrl-sci", "cs.LG", "physics.optics"], "comment": "7 pages, 2 figures, 2 tables", "summary": "Nonlinear optical (NLO) materials for generating lasers via second harmonic\ngeneration (SHG) are highly sought in today's technology. However, discovering\nnovel materials with considerable SHG is challenging due to the time-consuming\nand costly nature of both experimental methods and first-principles\ncalculations. In this study, we present a deep learning approach using the\nAtomistic Line Graph Neural Network (ALIGNN) to predict NLO properties.\nSourcing data from the Novel Opto-Electronic Materials Discovery (NOEMD)\ndatabase and using the Kurtz-Perry (KP) coefficient as the key target, we\ndeveloped a robust model capable of accurately estimating nonlinear optical\nresponses. Our results demonstrate that the model achieves 82.5% accuracy at a\ntolerated absolute error up to 1 pm/V and relative error not exceeding 0.5.\nThis work highlights the potential of deep learning in accelerating the\ndiscovery and design of advanced optical materials with desired properties.", "AI": {"tldr": "\u6458\u8981\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684Atomistic Line Graph Neural Network\uff08ALIGNN\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u975e\u7ebf\u6027\u5149\u5b66\uff08NLO\uff09\u6750\u6599\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u4e8c\u6b21\u8c10\u6ce2\u4ea7\u751f\uff08SHG\uff09\u3002\u8be5\u65b9\u6cd5\u5229\u7528NOEMD\u6570\u636e\u5e93\u548cKurtz-Perry\u7cfb\u6570\u4f5c\u4e3a\u5173\u952e\u76ee\u6807\uff0c\u5b9e\u73b0\u4e8682.5%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u7531\u4e8e\u5b9e\u9a8c\u548c\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u8017\u65f6\u4e14\u6602\u8d35\uff0c\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u65b9\u6cd5\u6765\u9884\u6d4b\u5177\u6709\u663e\u8457SHG\u7684\u65b0\u578b\u975e\u7ebf\u6027\u5149\u5b66\u6750\u6599\u6210\u4e3a\u8feb\u5207\u9700\u6c42\u3002", "method": "\u7814\u7a76\u4f7f\u7528Atomistic Line Graph Neural Network\uff08ALIGNN\uff09\u7ed3\u5408NOEMD\u6570\u636e\u5e93\u548cKurtz-Perry\u7cfb\u6570\u8fdb\u884c\u6df1\u5ea6\u5b66\u4e60\u5efa\u6a21\u3002", "result": "\u6a21\u578b\u5728\u7edd\u5bf9\u8bef\u5dee1 pm/V\u548c\u76f8\u5bf9\u8bef\u5dee0.5\u8303\u56f4\u5185\u8fbe\u5230\u4e8682.5%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u6df1\u5ea6\u5b66\u4e60\u5728\u52a0\u901f\u53d1\u73b0\u548c\u8bbe\u8ba1\u5177\u6709\u6240\u9700\u6027\u80fd\u7684\u5148\u8fdb\u5149\u5b66\u6750\u6599\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2504.20026", "pdf": "https://arxiv.org/pdf/2504.20026", "abs": "https://arxiv.org/abs/2504.20026", "authors": ["Zhengqin Li", "Dilin Wang", "Ka Chen", "Zhaoyang Lv", "Thu Nguyen-Phuoc", "Milim Lee", "Jia-Bin Huang", "Lei Xiao", "Cheng Zhang", "Yufeng Zhu", "Carl S. Marshall", "Yufeng Ren", "Richard Newcombe", "Zhao Dong"], "title": "LIRM: Large Inverse Rendering Model for Progressive Reconstruction of Shape, Materials and View-dependent Radiance Fields", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025", "summary": "We present Large Inverse Rendering Model (LIRM), a transformer architecture\nthat jointly reconstructs high-quality shape, materials, and radiance fields\nwith view-dependent effects in less than a second. Our model builds upon the\nrecent Large Reconstruction Models (LRMs) that achieve state-of-the-art\nsparse-view reconstruction quality. However, existing LRMs struggle to\nreconstruct unseen parts accurately and cannot recover glossy appearance or\ngenerate relightable 3D contents that can be consumed by standard Graphics\nengines. To address these limitations, we make three key technical\ncontributions to build a more practical multi-view 3D reconstruction framework.\nFirst, we introduce an update model that allows us to progressively add more\ninput views to improve our reconstruction. Second, we propose a hexa-plane\nneural SDF representation to better recover detailed textures, geometry and\nmaterial parameters. Third, we develop a novel neural directional-embedding\nmechanism to handle view-dependent effects. Trained on a large-scale shape and\nmaterial dataset with a tailored coarse-to-fine training scheme, our model\nachieves compelling results. It compares favorably to optimization-based\ndense-view inverse rendering methods in terms of geometry and relighting\naccuracy, while requiring only a fraction of the inference time.", "AI": {"tldr": "LIRM\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u89c6\u89d23D\u91cd\u5efa\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5148\u8fdb\u7684\u5f62\u72b6\u3001\u6750\u8d28\u548c\u8f90\u5c04\u573a\u91cd\u5efa\u6280\u672f\uff0c\u5728\u4e0d\u5230\u4e00\u79d2\u5185\u5b8c\u6210\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709LRMs\u5728\u7a00\u758f\u89c6\u89d2\u91cd\u5efa\u4e2d\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u5728\u672a\u89c1\u8fc7\u90e8\u5206\u7684\u51c6\u786e\u91cd\u5efa\u3001\u5149\u6ed1\u5916\u89c2\u6062\u590d\u53ca\u53ef\u91cd\u5149\u71673D\u5185\u5bb9\u751f\u6210\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5173\u952e\u8d21\u732e\u5305\u62ec\u589e\u91cf\u8f93\u5165\u89c6\u89d2\u7684\u66f4\u65b0\u6a21\u578b\u3001\u516d\u5e73\u9762\u795e\u7ecfSDF\u8868\u793a\u548c\u795e\u7ecf\u65b9\u5411\u5d4c\u5165\u673a\u5236\uff0c\u7ed3\u5408\u5206\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728\u51e0\u4f55\u548c\u91cd\u5149\u7167\u7cbe\u5ea6\u4e0a\u4e0e\u4f18\u5316\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "LIRM\u4e3a\u591a\u89c6\u89d23D\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u5907\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.19991", "pdf": "https://arxiv.org/pdf/2504.19991", "abs": "https://arxiv.org/abs/2504.19991", "authors": ["Ioannis Kontogiorgakis", "Iason Tsardanidis", "Dimitrios Bormpoudakis", "Ilias Tsoumas", "Dimitra A. Loka", "Christos Noulas", "Alexandros Tsitouras", "Charalampos Kontoes"], "title": "Mapping of Weed Management Methods in Orchards using Sentinel-2 and PlanetScope Data", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Effective weed management is crucial for improving agricultural productivity,\nas weeds compete with crops for vital resources like nutrients and water.\nAccurate maps of weed management methods are essential for policymakers to\nassess farmer practices, evaluate impacts on vegetation health, biodiversity,\nand climate, as well as ensure compliance with policies and subsidies. However,\nmonitoring weed management methods is challenging as commonly rely on on-ground\nfield surveys, which are often costly, time-consuming and subject to delays. In\norder to tackle this problem, we leverage Earth Observation (EO) data and\nMachine Learning (ML). Specifically, we developed an ML approach for mapping\nfour distinct weed management methods (Mowing, Tillage, Chemical-spraying, and\nNo practice) in orchards using satellite image time series (SITS) data from two\ndifferent sources: Sentinel-2 (S2) and PlanetScope (PS). The findings\ndemonstrate the potential of ML-driven remote sensing to enhance the efficiency\nand accuracy of weed management mapping in orchards.", "AI": {"tldr": "\u5229\u7528\u536b\u661f\u56fe\u50cf\u65f6\u95f4\u5e8f\u5217\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u9ad8\u6548\u7ed8\u5236\u679c\u56ed\u6742\u8349\u7ba1\u7406\u5730\u56fe\uff0c\u63d0\u9ad8\u519c\u4e1a\u751f\u4ea7\u529b\u3002", "motivation": "\u6742\u8349\u7ba1\u7406\u5bf9\u519c\u4e1a\u751f\u4ea7\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5730\u9762\u8c03\u67e5\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u76d1\u6d4b\u624b\u6bb5\u3002", "method": "\u7ed3\u5408Sentinel-2\u548cPlanetScope\u536b\u661f\u6570\u636e\uff0c\u5f00\u53d1\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u8bc6\u522b\u56db\u79cd\u6742\u8349\u7ba1\u7406\u65b9\u6cd5\uff08\u5272\u8349\u3001\u8015\u4f5c\u3001\u5316\u5b66\u55b7\u6d12\u548c\u65e0\u63aa\u65bd\uff09\u3002", "result": "\u9a8c\u8bc1\u4e86\u673a\u5668\u5b66\u4e60\u4e0e\u9065\u611f\u6280\u672f\u5728\u679c\u56ed\u6742\u8349\u7ba1\u7406\u5236\u56fe\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u9a71\u52a8\u7684\u9065\u611f\u6280\u672f\u4e3a\u6742\u8349\u7ba1\u7406\u76d1\u6d4b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u7cbe\u51c6\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.20004", "pdf": "https://arxiv.org/pdf/2504.20004", "abs": "https://arxiv.org/abs/2504.20004", "authors": ["Jing Wang", "Yan Jin", "Hamid Taghavifar", "Fei Ding", "Chongfeng Wei"], "title": "Socially-Aware Autonomous Driving: Inferring Yielding Intentions for Safer Interactions", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Since the emergence of autonomous driving technology, it has advanced rapidly\nover the past decade. It is becoming increasingly likely that autonomous\nvehicles (AVs) would soon coexist with human-driven vehicles (HVs) on the\nroads. Currently, safety and reliable decision-making remain significant\nchallenges, particularly when AVs are navigating lane changes and interacting\nwith surrounding HVs. Therefore, precise estimation of the intentions of\nsurrounding HVs can assist AVs in making more reliable and safe lane change\ndecision-making. This involves not only understanding their current behaviors\nbut also predicting their future motions without any direct communication.\nHowever, distinguishing between the passing and yielding intentions of\nsurrounding HVs still remains ambiguous. To address the challenge, we propose a\nsocial intention estimation algorithm rooted in Directed Acyclic Graph (DAG),\ncoupled with a decision-making framework employing Deep Reinforcement Learning\n(DRL) algorithms. To evaluate the method's performance, the proposed framework\ncan be tested and applied in a lane-changing scenario within a simulated\nenvironment. Furthermore, the experiment results demonstrate how our approach\nenhances the ability of AVs to navigate lane changes safely and efficiently on\nroads.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u7684\u793e\u4f1a\u610f\u56fe\u4f30\u8ba1\u7b97\u6cd5\uff0c\u5e76\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u51b3\u7b56\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AV\uff09\u5728\u8f66\u9053\u53d8\u6362\u4e2d\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AVs\uff09\u4e0e\u4eba\u5de5\u9a7e\u9a76\u8f66\u8f86\uff08HVs\uff09\u5171\u5b58\u65f6\uff0c\u7f3a\u4e4f\u76f4\u63a5\u901a\u4fe1\u5bfc\u81f4\u610f\u56fe\u96be\u4ee5\u9884\u6d4b\uff0c\u5c24\u5176\u662f\u8f66\u9053\u53d8\u6362\u4e2d\u7684\u901a\u8fc7\u548c\u8ba9\u884c\u610f\u56fe\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7cbe\u51c6\u7684\u610f\u56fe\u4f30\u8ba1\u548c\u53ef\u9760\u51b3\u7b56\u3002", "method": "\u91c7\u7528\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u8fdb\u884c\u793e\u4f1a\u610f\u56fe\u4f30\u8ba1\uff0c\u5e76\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u6784\u5efa\u51b3\u7b56\u6846\u67b6\uff0c\u901a\u8fc7\u4eff\u771f\u73af\u5883\u6d4b\u8bd5\u8f66\u9053\u53d8\u6362\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86AV\u5728\u8f66\u9053\u53d8\u6362\u4e2d\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aAV\u4e0eHV\u5171\u5b58\u7684\u590d\u6742\u4ea4\u901a\u573a\u666f\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u610f\u56fe\u4f30\u8ba1\u548c\u51b3\u7b56\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.20011", "pdf": "https://arxiv.org/pdf/2504.20011", "abs": "https://arxiv.org/abs/2504.20011", "authors": ["Aditya Vatsavai", "Ganesh Narasimha", "Yongtao Liu", "Jan-Chi Yang", "Hiroshu Funakubo", "Maxim Ziatdinov", "Rama Vasudevan"], "title": "Curiosity Driven Exploration to Optimize Structure-Property Learning in Microscopy", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "comment": "12 pages, 8 figures", "summary": "Rapidly determining structure-property correlations in materials is an\nimportant challenge in better understanding fundamental mechanisms and greatly\nassists in materials design. In microscopy, imaging data provides a direct\nmeasurement of the local structure, while spectroscopic measurements provide\nrelevant functional property information. Deep kernel active learning\napproaches have been utilized to rapidly map local structure to functional\nproperties in microscopy experiments, but are computationally expensive for\nmulti-dimensional and correlated output spaces. Here, we present an alternative\nlightweight curiosity algorithm which actively samples regions with unexplored\nstructure-property relations, utilizing a deep-learning based surrogate model\nfor error prediction. We show that the algorithm outperforms random sampling\nfor predicting properties from structures, and provides a convenient tool for\nefficient mapping of structure-property relationships in materials science.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u597d\u5947\u5fc3\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u6750\u6599\u79d1\u5b66\u4e2d\u9ad8\u6548\u6620\u5c04\u7ed3\u6784-\u6027\u80fd\u5173\u7cfb\uff0c\u4f18\u4e8e\u968f\u673a\u91c7\u6837\uff0c\u5e76\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u5feb\u901f\u786e\u5b9a\u6750\u6599\u7684\u7ed3\u6784-\u6027\u80fd\u5173\u7cfb\u5bf9\u7406\u89e3\u57fa\u7840\u673a\u5236\u548c\u8f85\u52a9\u6750\u6599\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u591a\u7ef4\u548c\u76f8\u5173\u8f93\u51fa\u7a7a\u95f4\u65f6\u5f00\u9500\u8f83\u5927\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u597d\u5947\u5fc3\u7b97\u6cd5\uff0c\u4e3b\u52a8\u91c7\u6837\u672a\u63a2\u7d22\u7684\u7ed3\u6784-\u6027\u80fd\u5173\u7cfb\u533a\u57df\uff0c\u5229\u7528\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4ee3\u7406\u6a21\u578b\u8fdb\u884c\u8bef\u5dee\u9884\u6d4b\u3002", "result": "\u7b97\u6cd5\u5728\u9884\u6d4b\u7ed3\u6784-\u6027\u80fd\u5173\u7cfb\u65b9\u9762\u4f18\u4e8e\u968f\u673a\u91c7\u6837\uff0c\u4e3a\u6750\u6599\u79d1\u5b66\u63d0\u4f9b\u4e86\u9ad8\u6548\u6620\u5c04\u5de5\u5177\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u6750\u6599\u79d1\u5b66\u4e2d\u7684\u7ed3\u6784-\u6027\u80fd\u5173\u7cfb\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u65b0\u65b9\u6cd5\u3002"}}
