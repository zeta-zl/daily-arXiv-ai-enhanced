{"id": "2505.06416", "pdf": "https://arxiv.org/pdf/2505.06416", "abs": "https://arxiv.org/abs/2505.06416", "authors": ["Elias Lumer", "Anmol Gulati", "Vamse Kumar Subbiah", "Pradeep Honaganahalli Basavaraju", "James A. Burke"], "title": "ScaleMCP: Dynamic and Auto-Synchronizing Model Context Protocol Tools for LLM Agents", "categories": ["cs.CL"], "comment": "17 pages", "summary": "Recent advancements in Large Language Models (LLMs) and the introduction of\nthe Model Context Protocol (MCP) have significantly expanded LLM agents'\ncapability to interact dynamically with external tools and APIs. However,\nexisting tool selection frameworks do not integrate MCP servers, instead\nrelying heavily on error-prone manual updates to monolithic local tool\nrepositories, leading to duplication, inconsistencies, and inefficiencies.\nAdditionally, current approaches abstract tool selection before the LLM agent\nis invoked, limiting its autonomy and hindering dynamic re-querying\ncapabilities during multi-turn interactions. To address these issues, we\nintroduce ScaleMCP, a novel tool selection approach that dynamically equips LLM\nagents with a MCP tool retriever, giving agents the autonomy to add tools into\ntheir memory, as well as an auto-synchronizing tool storage system pipeline\nthrough CRUD (create, read, update, delete) operations with MCP servers as the\nsingle source of truth. We also propose a novel embedding strategy, Tool\nDocument Weighted Average (TDWA), designed to selectively emphasize critical\ncomponents of tool documents (e.g. tool name or synthetic questions) during the\nembedding process. Comprehensive evaluations conducted on a created dataset of\n5,000 financial metric MCP servers, across 10 LLM models, 5 embedding models,\nand 5 retriever types, demonstrate substantial improvements in tool retrieval\nand agent invocation performance, emphasizing ScaleMCP's effectiveness in\nscalable, dynamic tool selection and invocation.", "AI": {"tldr": "ScaleMCP\u662f\u4e00\u79cd\u65b0\u5de5\u5177\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u96c6\u6210MCP\u670d\u52a1\u5668\u548c\u81ea\u52a8\u5316\u5de5\u5177\u5b58\u50a8\u7cfb\u7edf\u89e3\u51b3\u73b0\u6709\u5de5\u5177\u9009\u62e9\u6846\u67b6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u5de5\u5177\u9009\u62e9\u6846\u67b6\u4f9d\u8d56\u4eba\u5de5\u66f4\u65b0\u4e14\u672a\u96c6\u6210MCP\u670d\u52a1\u5668\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3001\u4e0d\u4e00\u81f4\u6027\u548c\u4ee3\u7406\u81ea\u4e3b\u6027\u53d7\u9650\u3002", "method": "\u5f15\u5165ScaleMCP\uff0c\u5305\u62ecMCP\u5de5\u5177\u68c0\u7d22\u5668\u3001\u81ea\u52a8\u5316\u5b58\u50a8\u7cfb\u7edf\u548cTDWA\u5d4c\u5165\u7b56\u7565\uff0c\u5b9e\u73b0\u52a8\u6001\u5de5\u5177\u9009\u62e9\u548c\u540c\u6b65\u3002", "result": "\u57285000\u4e2a\u91d1\u878dMCP\u670d\u52a1\u5668\u7684\u6d4b\u8bd5\u4e2d\uff0cScaleMCP\u663e\u8457\u63d0\u5347\u4e86\u5de5\u5177\u68c0\u7d22\u548c\u4ee3\u7406\u8c03\u7528\u6027\u80fd\u3002", "conclusion": "ScaleMCP\u80fd\u6709\u6548\u652f\u6301\u53ef\u6269\u5c55\u3001\u52a8\u6001\u7684\u5de5\u5177\u9009\u62e9\u4e0e\u8c03\u7528\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u7f3a\u9677\u3002"}}
{"id": "2505.06418", "pdf": "https://arxiv.org/pdf/2505.06418", "abs": "https://arxiv.org/abs/2505.06418", "authors": ["Ming Liu", "Liwen Wang", "Wensheng Zhang"], "title": "Is your multimodal large language model a good science tutor?", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal large language models (MLLMs) demonstrate impressive performance\non scientific reasoning tasks (e.g., ScienceQA). However, most existing\nbenchmarks focus narrowly on the accuracy of the final answer while ignoring\nother metrics. In particular, when applying MLLMs to educational contexts, the\ngoal is not only correctness but also the ability to teach. In this paper, we\npropose a framework that evaluates MLLMs as science tutors using a\ncomprehensive educational rubric and a simulated student model that judges the\nteaching performance of the tutors. Given a list of candidate MLLM science\ntutors, we use rubric-based student judgments to produce a range of tutor\nperformance scores, identifying both strong and weak tutors. Using the training\nsection of the ScienceQA dataset, we then construct a data set of pairwise\ncomparisons between the outputs of strong and weak tutors. This enables us to\napply multiple preference optimization methods to fine-tune an underperforming\ntutor model (Qwen2-VL-2B) into more effective ones. Our results also show that\nstrong problem-solving skills do not guarantee high-quality tutoring and that\nperformance optimization-guided refinements can yield more educationally\naligned tutor models. This approach opens avenues for building MLLMs that serve\nnot only as problem solvers, but as genuinely helpful educational assistants.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u79d1\u5b66\u5bfc\u5e08\u7684\u6846\u67b6,\u7ed3\u5408\u6559\u80b2\u6807\u51c6\u4e0e\u5b66\u751f\u6a21\u62df,\u4f18\u5316\u6a21\u578b\u4ee5\u63d0\u5347\u6559\u5b66\u80fd\u529b,\u4e0d\u4ec5\u4ec5\u662f\u89e3\u51b3\u95ee\u9898\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u6027,\u800c\u5ffd\u89c6\u4e86\u6559\u5b66\u80fd\u529b\u3002\u5728\u6559\u80b2\u573a\u666f\u4e2d,\u4e0d\u4ec5\u9700\u8981\u6a21\u578b\u7684\u6b63\u786e\u6027,\u66f4\u9700\u8981\u5176\u80fd\u591f\u6709\u6548\u6559\u5b66\u3002", "method": "\u5229\u7528\u6559\u80b2\u6807\u51c6\u4e0e\u5b66\u751f\u6a21\u62df\u6a21\u578b\u8bc4\u4f30\u6a21\u578b\u6559\u5b66\u8868\u73b0,\u6784\u5efa\u5f3a\u5f31\u5bfc\u5e08\u5bf9\u6bd4\u6570\u636e\u96c6,\u5e76\u8fd0\u7528\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u5fae\u8c03\u6a21\u578b\u3002", "result": "\u7814\u7a76\u8868\u660e\u5f3a\u89e3\u9898\u80fd\u529b\u5e76\u4e0d\u7b49\u540c\u4e8e\u9ad8\u8d28\u91cf\u6559\u5b66,\u4f18\u5316\u540e\u7684\u6a21\u578b(Qwen2-VL-2B)\u66f4\u52a0\u7b26\u5408\u6559\u80b2\u9700\u6c42\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u6784\u5efa\u5177\u5907\u6559\u5b66\u80fd\u529b\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u9014\u5f84,\u4f7f\u5176\u4e0d\u4ec5\u80fd\u89e3\u9898,\u8fd8\u80fd\u6210\u4e3a\u9ad8\u6548\u7684\u6559\u80b2\u52a9\u624b\u3002"}}
{"id": "2505.06496", "pdf": "https://arxiv.org/pdf/2505.06496", "abs": "https://arxiv.org/abs/2505.06496", "authors": ["Erik Nijkamp", "Bo Pang", "Egor Pakhomov", "Akash Gokul", "Jin Qu", "Silvio Savarese", "Yingbo Zhou", "Caiming Xiong"], "title": "xGen-small Technical Report", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce xGen-small, a family of 4B and 9B Transformer decoder models\noptimized for long-context applications. Our vertically integrated pipeline\nunites domain-balanced, frequency-aware data curation; multi-stage pre-training\nwith quality annealing and length extension to 128k tokens; and targeted\npost-training via supervised fine-tuning, preference learning, and online\nreinforcement learning. xGen-small delivers strong performance across various\ntasks, especially in math and coding domains, while excelling at long context\nbenchmarks.", "AI": {"tldr": "xGen-small\u662f\u4e00\u4e2a\u9488\u5bf9\u957f\u4e0a\u4e0b\u6587\u4f18\u5316\u76844B\u548c9B Transformer\u89e3\u7801\u5668\u6a21\u578b\u5bb6\u65cf\uff0c\u901a\u8fc7\u6570\u636e\u7ba1\u7406\u3001\u591a\u9636\u6bb5\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u7b49\u65b9\u6cd5\uff0c\u5728\u6570\u5b66\u548c\u7f16\u7801\u9886\u57df\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4e3a\u957f\u4e0a\u4e0b\u6587\u5e94\u7528\u5f00\u53d1\u9ad8\u6548\u7684Transformer\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u5728\u6570\u5b66\u548c\u7f16\u7801\u7b49\u9886\u57df\u7684\u6027\u80fd\u3002", "method": "\u5782\u76f4\u6574\u5408\u7684\u6570\u636e\u7ba1\u7406\u3001\u591a\u9636\u6bb5\u9884\u8bad\u7ec3\uff08\u5305\u62ec\u8d28\u91cf\u9000\u706b\u548c128k tokens\u957f\u5ea6\u6269\u5c55\uff09\u3001\u76d1\u7763\u5fae\u8c03\u3001\u504f\u597d\u5b66\u4e60\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u5f3a\u52b2\uff0c\u5c24\u5176\u5728\u6570\u5b66\u548c\u7f16\u7801\u9886\u57df\u4ee5\u53ca\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "xGen-small\u901a\u8fc7\u4f18\u5316\u6570\u636e\u5904\u7406\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2505.06538", "pdf": "https://arxiv.org/pdf/2505.06538", "abs": "https://arxiv.org/abs/2505.06538", "authors": ["Xinyue Lou", "You Li", "Jinan Xu", "Xiangyu Shi", "Chi Chen", "Kaiyu Huang"], "title": "Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model", "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "The rapid development of multimodal large reasoning models (MLRMs) has\ndemonstrated broad application potential, yet their safety and reliability\nremain critical concerns that require systematic exploration. To address this\ngap, we conduct a comprehensive and systematic safety evaluation of 11 MLRMs\nacross 5 benchmarks and unveil prevalent safety degradation phenomena in most\nadvanced models. Moreover, our analysis reveals distinct safety patterns across\ndifferent benchmarks: significant safety degradation is observed across\njailbreak robustness benchmarks, whereas safety-awareness benchmarks\ndemonstrate less pronounced degradation. In particular, a long thought process\nin some scenarios even enhances safety performance. Therefore, it is a\npotential approach to addressing safety issues in MLRMs by leveraging the\nintrinsic reasoning capabilities of the model to detect unsafe intent. To\noperationalize this insight, we construct a multimodal tuning dataset that\nincorporates a safety-oriented thought process. Experimental results from\nfine-tuning existing MLRMs with this dataset effectively enhances the safety on\nboth jailbreak robustness and safety-awareness benchmarks. This study provides\na new perspective for developing safe MLRMs. Our dataset is available at\nhttps://github.com/xinyuelou/Think-in-Safety.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u6027\u8bc4\u4f3011\u79cd\u591a\u6a21\u6001\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08MLRMs\uff09\uff0c\u63ed\u793a\u4e86\u5176\u5b89\u5168\u6027\u80fd\u4e0b\u964d\u7684\u666e\u904d\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6a21\u578b\u5185\u5728\u63a8\u7406\u80fd\u529b\u68c0\u6d4b\u4e0d\u5b89\u5168\u610f\u56fe\u7684\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u6570\u636e\u96c6\u5fae\u8c03\u80fd\u6709\u6548\u63d0\u5347\u5b89\u5168\u6027\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08MLRMs\uff09\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u5176\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u4ecd\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\uff0c\u4e9f\u9700\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5bf911\u79cdMLRMs\u57285\u4e2a\u57fa\u51c6\u4e0a\u8fdb\u884c\u5168\u9762\u5b89\u5168\u8bc4\u4f30\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u5b89\u5168\u5bfc\u5411\u601d\u7ef4\u7684\u591a\u6a21\u6001\u5fae\u8c03\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528\u8be5\u6570\u636e\u96c6\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u53d1\u73b0\u5148\u8fdb\u6a21\u578b\u5b58\u5728\u663e\u8457\u5b89\u5168\u6027\u80fd\u4e0b\u964d\u73b0\u8c61\uff0c\u5c24\u5176\u5728\u8d8a\u72f1\u9c81\u68d2\u6027\u57fa\u51c6\u4e0a\u8868\u73b0\u660e\u663e\uff1b\u901a\u8fc7\u5fae\u8c03\u6570\u636e\u96c6\uff0c\u6a21\u578b\u5728\u5b89\u5168\u6027\u80fd\u4e0a\u5f97\u5230\u6709\u6548\u63d0\u5347\u3002", "conclusion": "\u5229\u7528\u6a21\u578b\u5185\u5728\u63a8\u7406\u80fd\u529b\u68c0\u6d4b\u4e0d\u5b89\u5168\u610f\u56fe\u662f\u89e3\u51b3MLRMs\u5b89\u5168\u95ee\u9898\u7684\u53ef\u884c\u9014\u5f84\uff0c\u7814\u7a76\u4e3a\u5f00\u53d1\u5b89\u5168MLRMs\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.06229", "pdf": "https://arxiv.org/pdf/2505.06229", "abs": "https://arxiv.org/abs/2505.06229", "authors": ["Aaqib Ayoub Bhat", "Asif Khan", "M. Mursaleen"], "title": "Neural Network Operator-Based Fractal Approximation: Smoothness Preservation and Convergence Analysis", "categories": ["cs.LG", "cs.NA", "math.NA", "28A80, 41A05, 41A25, 41A29, 41A30, 65D05"], "comment": "18 pages", "summary": "This paper presents a new approach of constructing $\\alpha$-fractal\ninterpolation functions (FIFs) using neural network operators, integrating\nconcepts from approximation theory. Initially, we construct $\\alpha$-fractals\nutilizing neural network-based operators, providing an approach to generating\nfractal functions with interpolation properties. Based on the same foundation,\nwe have developed fractal interpolation functions that utilize only the values\nof the original function at the nodes or partition points, unlike traditional\nmethods that rely on the entire original function.\n  Further, we have constructed \\(\\alpha\\)-fractals that preserve the smoothness\nof functions under certain constraints by employing a four-layered neural\nnetwork operator, ensuring that if \\(f \\in C^{r}[a,b]\\), then the corresponding\nfractal \\(f^{\\alpha} \\in C^{r}[a,b]\\). Furthermore, we analyze the convergence\nof these $\\alpha$-fractals to the original function under suitable conditions.\nThe work uses key approximation theory tools, such as the modulus of continuity\nand interpolation operators, to develop convergence results and uniform\napproximation error bounds.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50\u6784\u5efa\u03b1-\u5206\u5f62\u63d2\u503c\u51fd\u6570\uff08FIF\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u903c\u8fd1\u7406\u8bba\uff0c\u901a\u8fc7\u4ec5\u4f7f\u7528\u8282\u70b9\u5904\u7684\u51fd\u6570\u503c\u751f\u6210\u5206\u5f62\u51fd\u6570\uff0c\u5e76\u5728\u4e00\u5b9a\u6761\u4ef6\u4e0b\u4fdd\u6301\u51fd\u6570\u7684\u5149\u6ed1\u6027\uff0c\u540c\u65f6\u5206\u6790\u4e86\u5176\u6536\u655b\u6027\u3002", "motivation": "\u4f20\u7edf\u5206\u5f62\u63d2\u503c\u51fd\u6570\u6784\u5efa\u4f9d\u8d56\u4e8e\u6574\u4e2a\u539f\u59cb\u51fd\u6570\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50\u4ec5\u4f7f\u7528\u8282\u70b9\u503c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u6784\u5efa\uff0c\u5e76\u7814\u7a76\u5176\u5149\u6ed1\u6027\u4e0e\u903c\u8fd1\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u56db\u5c42\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50\u6784\u5efa\u03b1-\u5206\u5f62\u63d2\u503c\u51fd\u6570\uff0c\u5229\u7528\u903c\u8fd1\u7406\u8bba\u5de5\u5177\uff08\u5982\u8fde\u7eed\u6027\u6a21\u548c\u63d2\u503c\u7b97\u5b50\uff09\u5206\u6790\u6536\u655b\u6027\u53ca\u8bef\u5dee\u754c\u3002", "result": "\u5728\u7ea6\u675f\u6761\u4ef6\u4e0b\uff0c\u65b0\u6784\u5efa\u7684\u03b1-\u5206\u5f62\u51fd\u6570\u4fdd\u6301\u4e86\u539f\u59cb\u51fd\u6570\u7684\u5149\u6ed1\u6027\uff08\u82e5f\u2208C^r\uff0c\u5219f^\u03b1\u2208C^r\uff09\uff0c\u5e76\u5728\u9002\u5f53\u6761\u4ef6\u4e0b\u6536\u655b\u4e8e\u539f\u51fd\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5206\u5f62\u63d2\u503c\u51fd\u6570\u6784\u5efa\uff0c\u4e3a\u5206\u5f62\u903c\u8fd1\u7406\u8bba\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u548c\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2505.06287", "pdf": "https://arxiv.org/pdf/2505.06287", "abs": "https://arxiv.org/abs/2505.06287", "authors": ["Riccardo Sieve", "Paul Kobialka", "Laura Slaughter", "Rudolf Schlatte", "Einar Broch Johnsen", "Silvia Lizeth Tapia Tarifa"], "title": "BedreFlyt: Improving Patient Flows through Hospital Wards with Digital Twins", "categories": ["cs.AI", "cs.ET", "cs.LO", "D.2.2; D.2.4; J.3"], "comment": "In Proceedings ASQAP 2025, arXiv:2505.02873", "summary": "Digital twins are emerging as a valuable tool for short-term decision-making\nas well as for long-term strategic planning across numerous domains, including\nprocess industry, energy, space, transport, and healthcare. This paper reports\non our ongoing work on designing a digital twin to enhance resource planning,\ne.g., for the in-patient ward needs in hospitals. By leveraging executable\nformal models for system exploration, ontologies for knowledge representation\nand an SMT solver for constraint satisfiability, our approach aims to explore\nhypothetical \"what-if\" scenarios to improve strategic planning processes, as\nwell as to solve concrete, short-term decision-making tasks. Our proposed\nsolution uses the executable formal model to turn a stream of arriving\npatients, that need to be hospitalized, into a stream of optimization problems,\ne.g., capturing daily inpatient ward needs, that can be solved by SMT\ntechniques. The knowledge base, which formalizes domain knowledge, is used to\nmodel the needed configuration in the digital twin, allowing the twin to\nsupport both short-term decision-making and long-term strategic planning by\ngenerating scenarios spanning average-case as well as worst-case resource\nneeds, depending on the expected treatment of patients, as well as ranging over\nvariations in available resources, e.g., bed distribution in different rooms.\nWe illustrate our digital twin architecture by considering the problem of bed\nbay allocation in a hospital ward.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u533b\u9662\u75c5\u623f\u8d44\u6e90\u89c4\u5212\u7684\u6570\u5b57\u5b6a\u751f\u67b6\u6784\uff0c\u7ed3\u5408\u5f62\u5f0f\u5316\u6a21\u578b\u3001\u672c\u4f53\u8bba\u548cSMT\u6c42\u89e3\u5668\uff0c\u4ee5\u652f\u6301\u77ed\u671f\u51b3\u7b56\u548c\u957f\u671f\u6218\u7565\u89c4\u5212\u3002", "motivation": "\u6570\u5b57\u5b6a\u751f\u5728\u591a\u9886\u57df\u51b3\u7b56\u548c\u89c4\u5212\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u7279\u522b\u662f\u533b\u7597\u8d44\u6e90\u5206\u914d\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u533b\u9662\u75c5\u623f\u8d44\u6e90\u89c4\u5212\u7684\u590d\u6742\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5f62\u5f0f\u5316\u53ef\u6267\u884c\u6a21\u578b\u3001\u672c\u4f53\u8bba\u77e5\u8bc6\u8868\u793a\u548cSMT\u6c42\u89e3\u5668\u6784\u5efa\u6570\u5b57\u5b6a\u751f\u67b6\u6784\uff0c\u7528\u4e8e\u63a2\u7d22\u5047\u8bbe\u573a\u666f\u548c\u4f18\u5316\u95ee\u9898\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u5305\u62ec\u5e73\u5747\u548c\u6700\u574f\u60c5\u51b5\u4e0b\u8d44\u6e90\u9700\u6c42\u7684\u573a\u666f\uff0c\u652f\u6301\u533b\u9662\u7684\u77ed\u671f\u53ca\u957f\u671f\u89c4\u5212\u3002", "conclusion": "\u6570\u5b57\u5b6a\u751f\u67b6\u6784\u80fd\u591f\u6709\u6548\u63d0\u5347\u533b\u9662\u75c5\u623f\u8d44\u6e90\u89c4\u5212\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u591a\u53d8\u7684\u9700\u6c42\u548c\u8d44\u6e90\u7ea6\u675f\u3002"}}
{"id": "2505.06548", "pdf": "https://arxiv.org/pdf/2505.06548", "abs": "https://arxiv.org/abs/2505.06548", "authors": ["Aniruddha Roy", "Pretam Ray", "Abhilash Nandy", "Somak Aditya", "Pawan Goyal"], "title": "REFINE-AF: A Task-Agnostic Framework to Align Language Models via Self-Generated Instructions using Reinforcement Learning from Automated Feedback", "categories": ["cs.CL"], "comment": "11 pages", "summary": "Instruction-based Large Language Models (LLMs) have proven effective in\nnumerous few-shot or zero-shot Natural Language Processing (NLP) tasks.\nHowever, creating human-annotated instruction data is time-consuming,\nexpensive, and often limited in quantity and task diversity. Previous research\nendeavors have attempted to address this challenge by proposing frameworks\ncapable of generating instructions in a semi-automated and task-agnostic manner\ndirectly from the model itself. Many of these efforts have relied on large\nAPI-only parameter-based models such as GPT-3.5 (175B), which are expensive,\nand subject to limits on a number of queries. This paper explores the\nperformance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B,\nand Mistral 7B, using a semi-automated framework, thereby reducing human\nintervention, effort, and cost required to generate an instruction dataset for\nfine-tuning LLMs. Furthermore, we demonstrate that incorporating a\nReinforcement Learning (RL) based training algorithm into this LLMs-based\nframework leads to further enhancements. Our evaluation of the dataset reveals\nthat these RL-based frameworks achieve a substantial improvements in 63-66% of\nthe tasks compared to previous approaches.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4e09\u79cd\u5f00\u6e90\u5c0f\u578bLLM\uff08LLaMA 2-7B\u3001LLama 2-13B\u548cMistral 7B\uff09\u5728\u751f\u6210\u6307\u4ee4\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u534a\u81ea\u52a8\u5316\u6846\u67b6\u964d\u4f4e\u4eba\u5de5\u5e72\u9884\u548c\u6210\u672c\uff0c\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u8fdb\u4e00\u6b65\u63d0\u5347\u6548\u679c\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u8fd9\u79cd\u65b9\u6cd5\u572863-66%\u7684\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4e4b\u524d\u7684\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u4eba\u5de5\u6807\u6ce8\u6307\u4ee4\u6570\u636e\u8017\u65f6\u3001\u6602\u8d35\u4e14\u591a\u6837\u6027\u6709\u9650\uff0c\u8bba\u6587\u65e8\u5728\u63a2\u7d22\u5f00\u6e90\u5c0f\u578bLLM\u751f\u6210\u6307\u4ee4\u7684\u53ef\u884c\u6027\uff0c\u4ee5\u964d\u4f4e\u6210\u672c\u5e76\u63d0\u5347\u4efb\u52a1\u591a\u6837\u6027\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u5f00\u6e90\u5c0f\u578bLLM\uff08LLaMA 2-7B\u3001LLama 2-13B\u548cMistral 7B\uff09\uff0c\u7ed3\u5408\u534a\u81ea\u52a8\u5316\u6846\u67b6\u751f\u6210\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u5e76\u5f15\u5165\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\u572863-66%\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5c0f\u578bLLM\u548c\u81ea\u52a8\u5316\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5f00\u6e90\u5c0f\u578bLLM\u7ed3\u5408\u534a\u81ea\u52a8\u5316\u6846\u67b6\u53ca\u5f3a\u5316\u5b66\u4e60\u80fd\u9ad8\u6548\u751f\u6210\u6307\u4ee4\u6570\u636e\uff0c\u964d\u4f4e\u6210\u672c\u4e14\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e3aLLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.06257", "pdf": "https://arxiv.org/pdf/2505.06257", "abs": "https://arxiv.org/abs/2505.06257", "authors": ["Ahsan Adeel"], "title": "Beyond Attention: Toward Machines with Intrinsic Higher Mental States", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "Attending to what is relevant is fundamental to both the mammalian brain and\nmodern machine learning models such as Transformers. Yet, determining relevance\nremains a core challenge, traditionally offloaded to learning algorithms like\nbackpropagation. Inspired by recent cellular neurobiological evidence linking\nneocortical pyramidal cells to distinct mental states, this work shows how\nmodels (e.g., Transformers) can emulate high-level perceptual processing and\nawake thought (imagination) states to pre-select relevant information before\napplying attention. Triadic neuronal-level modulation loops among questions\n($Q$), clues (keys, $K$), and hypotheses (values, $V$) enable diverse, deep,\nparallel reasoning chains at the representation level and allow a rapid shift\nfrom initial biases to refined understanding. This leads to orders-of-magnitude\nfaster learning with significantly reduced computational demand (e.g., fewer\nheads, layers, and tokens), at an approximate cost of $\\mathcal{O}(N)$, where\n$N$ is the number of input tokens. Results span reinforcement learning (e.g.,\nCarRacing in a high-dimensional visual setup), computer vision, and natural\nlanguage question answering.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u5927\u8111\u795e\u7ecf\u5143\u542f\u53d1\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u6a21\u62df\u9ad8\u7ea7\u611f\u77e5\u5904\u7406\u4e0e\u6e05\u9192\u601d\u7ef4\u72b6\u6001\uff0c\u9884\u5148\u7b5b\u9009\u76f8\u5173\u4fe1\u606f\u4ee5\u63d0\u9ad8\u6ce8\u610f\u529b\u673a\u5236\u7684\u6548\u7387\uff0c\u4ece\u800c\u5728\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\u7684\u540c\u65f6\u52a0\u901f\u5b66\u4e60\u8fc7\u7a0b\u3002", "motivation": "\u53d7\u6700\u8fd1\u795e\u7ecf\u751f\u7269\u5b66\u53d1\u73b0\u7684\u542f\u53d1\uff0c\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u6a21\u62df\u5927\u8111\u7684\u795e\u7ecf\u5143\u8c03\u5236\u673a\u5236\uff0c\u4f18\u5316\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982Transformer\uff09\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5e76\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002", "method": "\u5229\u7528\u4e09\u5143\u795e\u7ecf\u5143\u7ea7\u8c03\u5236\u5faa\u73af\uff08Q\u3001K\u3001V\uff09\uff0c\u5728\u8868\u793a\u5c42\u9762\u5b9e\u73b0\u591a\u6837\u5316\u4e14\u6df1\u5ea6\u7684\u5e76\u884c\u63a8\u7406\u94fe\uff0c\u9884\u5148\u7b5b\u9009\u76f8\u5173\u4fe1\u606f\u540e\u518d\u5e94\u7528\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728\u5f3a\u5316\u5b66\u4e60\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u9700\u6c42\uff08\u5982\u5934\u90e8\u3001\u5c42\u6570\u548c\u6807\u8bb0\u6570\u91cf\uff09\uff0c\u540c\u65f6\u5b9e\u73b0\u6570\u91cf\u7ea7\u7684\u5b66\u4e60\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u601d\u8def\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u795e\u7ecf\u751f\u7269\u5b66\u542f\u53d1\u7684\u65b9\u6cd5\u4f18\u5316\u6a21\u578b\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2505.06328", "pdf": "https://arxiv.org/pdf/2505.06328", "abs": "https://arxiv.org/abs/2505.06328", "authors": ["Felix Ocker", "J\u00f6rg Deigm\u00f6ller", "Pavel Smirnov", "Julian Eggert"], "title": "A Grounded Memory System For Smart Personal Assistants", "categories": ["cs.AI", "H.3.3; H.3.4; I.2.1; I.2.5; I.2.7; I.2.10; J.3"], "comment": "8 pages, 5 figures, accepted for the ESWC 2025 TEXT2KG workshop", "summary": "A wide variety of agentic AI applications - ranging from cognitive assistants\nfor dementia patients to robotics - demand a robust memory system grounded in\nreality. In this paper, we propose such a memory system consisting of three\ncomponents. First, we combine Vision Language Models for image captioning and\nentity disambiguation with Large Language Models for consistent information\nextraction during perception. Second, the extracted information is represented\nin a memory consisting of a knowledge graph enhanced by vector embeddings to\nefficiently manage relational information. Third, we combine semantic search\nand graph query generation for question answering via Retrieval Augmented\nGeneration. We illustrate the system's working and potential using a real-world\nexample.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e09\u7ec4\u4ef6\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u7528\u4e8e\u589e\u5f3aAI\u4ee3\u7406\u7684\u73b0\u5b9e\u8bb0\u5fc6\u80fd\u529b\u3002", "motivation": "\u4e3a\u9700\u8981\u73b0\u5b9e\u8bb0\u5fc6\u652f\u6301\u7684AI\u5e94\u7528\uff08\u5982\u8ba4\u77e5\u8f85\u52a9\u3001\u673a\u5668\u4eba\uff09\u8bbe\u8ba1\u4e00\u79cd\u5065\u58ee\u7684\u8bb0\u5fc6\u7cfb\u7edf\u3002", "method": "\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u56fe\u50cf\u63cf\u8ff0\u4e0e\u5b9e\u4f53\u6d88\u6b67\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u4fe1\u606f\u63d0\u53d6\uff09\uff0c\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u4e0e\u5411\u91cf\u5d4c\u5165\u7684\u6df7\u5408\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5b9e\u73b0\u95ee\u7b54\u3002", "result": "\u7cfb\u7edf\u901a\u8fc7\u771f\u5b9e\u6848\u4f8b\u5c55\u793a\u4e86\u5176\u529f\u80fd\u548c\u6f5c\u529b\u3002", "conclusion": "\u4e09\u7ec4\u4ef6\u8bb0\u5fc6\u7cfb\u7edf\u80fd\u6709\u6548\u652f\u6301AI\u4ee3\u7406\u7684\u73b0\u5b9e\u8bb0\u5fc6\u9700\u6c42\u3002"}}
{"id": "2505.06552", "pdf": "https://arxiv.org/pdf/2505.06552", "abs": "https://arxiv.org/abs/2505.06552", "authors": ["Doyoung Kim", "Youngjun Lee", "Joeun Kim", "Jihwan Bang", "Hwanjun Song", "Susik Yoon", "Jae-Gil Lee"], "title": "References Indeed Matter? Reference-Free Preference Optimization for Conversational Query Reformulation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Conversational query reformulation (CQR) has become indispensable for\nimproving retrieval in dialogue-based applications. However, existing\napproaches typically rely on reference passages for optimization, which are\nimpractical to acquire in real-world scenarios. To address this limitation, we\nintroduce a novel reference-free preference optimization framework DualReform\nthat generates pseudo reference passages from commonly-encountered\nconversational datasets containing only queries and responses. DualReform\nattains this goal through two key innovations: (1) response-based inference,\nwhere responses serve as proxies to infer pseudo reference passages, and (2)\nresponse refinement via the dual-role of CQR, where a CQR model refines\nresponses based on the shared objectives between response refinement and CQR.\nDespite not relying on reference passages, DualReform achieves 96.9--99.1% of\nthe retrieval accuracy attainable only with reference passages and surpasses\nthe state-of-the-art method by up to 31.6%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u53c2\u8003\u6bb5\u843d\u7684\u5bf9\u8bdd\u67e5\u8be2\u91cd\u6784\u4f18\u5316\u6846\u67b6DualReform\uff0c\u901a\u8fc7\u54cd\u5e94\u63a8\u7406\u548c\u53cc\u89d2\u8272\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u68c0\u7d22\uff0c\u6027\u80fd\u63a5\u8fd1\u751a\u81f3\u8d85\u8d8a\u4f9d\u8d56\u53c2\u8003\u6bb5\u843d\u7684\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709CQR\u65b9\u6cd5\u4f9d\u8d56\u53c2\u8003\u6bb5\u843d\u4f18\u5316\uff0c\u4f46\u5b9e\u9645\u573a\u666f\u4e2d\u96be\u4ee5\u83b7\u53d6\uff1b\u9700\u5f00\u53d1\u65e0\u9700\u53c2\u8003\u7684\u9ad8\u6548\u6846\u67b6\u3002", "method": "DualReform\u901a\u8fc7\u54cd\u5e94\u63a8\u7406\u751f\u6210\u4f2a\u53c2\u8003\u6bb5\u843d\uff0c\u5e76\u5229\u7528CQR\u53cc\u89d2\u8272\uff08\u54cd\u5e94\u7cbe\u8c03\u4e0e\u67e5\u8be2\u91cd\u6784\u5171\u4eab\u76ee\u6807\uff09\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5728\u65e0\u53c2\u8003\u6bb5\u843d\u7684\u60c5\u51b5\u4e0b\uff0c\u8fbe\u523096.9%~99.1%\u7684\u68c0\u7d22\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u8fbe31.6%\u3002", "conclusion": "DualReform\u8bc1\u660e\u4e86\u65e0\u9700\u53c2\u8003\u6bb5\u843d\u7684\u53ef\u884c\u6027\u4e0e\u9ad8\u6548\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.06258", "pdf": "https://arxiv.org/pdf/2505.06258", "abs": "https://arxiv.org/abs/2505.06258", "authors": ["Zhiyu Zhu", "Jiayu Zhang", "Zhibo Jin", "Fang Chen", "Jianlong Zhou"], "title": "ABE: A Unified Framework for Robust and Faithful Attribution-Based Explainability", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Attribution algorithms are essential for enhancing the interpretability and\ntrustworthiness of deep learning models by identifying key features driving\nmodel decisions. Existing frameworks, such as InterpretDL and OmniXAI,\nintegrate multiple attribution methods but suffer from scalability limitations,\nhigh coupling, theoretical constraints, and lack of user-friendly\nimplementations, hindering neural network transparency and interoperability. To\naddress these challenges, we propose Attribution-Based Explainability (ABE), a\nunified framework that formalizes Fundamental Attribution Methods and\nintegrates state-of-the-art attribution algorithms while ensuring compliance\nwith attribution axioms. ABE enables researchers to develop novel attribution\ntechniques and enhances interpretability through four customizable modules:\nRobustness, Interpretability, Validation, and Data & Model. This framework\nprovides a scalable, extensible foundation for advancing attribution-based\nexplainability and fostering transparent AI systems. Our code is available at:\nhttps://github.com/LMBTough/ABE-XAI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5f52\u56e0\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff08ABE\uff09\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u3001\u8026\u5408\u5ea6\u3001\u7406\u8bba\u9650\u5236\u548c\u7528\u6237\u53cb\u597d\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u4f9b\u53ef\u5b9a\u5236\u6a21\u5757\u4ee5\u589e\u5f3a\u6df1\u5ea6\u5b66\u4e60\u7684\u900f\u660e\u5ea6\u3002", "motivation": "\u73b0\u6709\u5f52\u56e0\u7b97\u6cd5\u6846\u67b6\u5b58\u5728\u53ef\u6269\u5c55\u6027\u5dee\u3001\u8026\u5408\u5ea6\u9ad8\u3001\u7406\u8bba\u9650\u5236\u591a\u548c\u7528\u6237\u53cb\u597d\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u8fd9\u963b\u788d\u4e86\u795e\u7ecf\u7f51\u7edc\u7684\u900f\u660e\u5ea6\u548c\u4e92\u64cd\u4f5c\u6027\u3002", "method": "\u63d0\u51fa\u4e86Attribution-Based Explainability (ABE)\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u6574\u5408\u4e86\u6700\u5148\u8fdb\u7684\u5f52\u56e0\u7b97\u6cd5\uff0c\u5e76\u786e\u4fdd\u7b26\u5408\u5f52\u56e0\u516c\u7406\uff0c\u5305\u542b\u56db\u4e2a\u53ef\u5b9a\u5236\u6a21\u5757\uff1a\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3001\u9a8c\u8bc1\u548c\u6570\u636e\u4e0e\u6a21\u578b\u3002", "result": "ABE\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u4ee5\u63a8\u8fdb\u57fa\u4e8e\u5f52\u56e0\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u4fc3\u8fdb\u900f\u660e\u7684AI\u7cfb\u7edf\u3002", "conclusion": "ABE\u6846\u67b6\u901a\u8fc7\u6574\u5408\u73b0\u6709\u6280\u672f\u548c\u63d0\u4f9b\u81ea\u5b9a\u4e49\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u5de5\u5177\u3002"}}
{"id": "2505.06438", "pdf": "https://arxiv.org/pdf/2505.06438", "abs": "https://arxiv.org/abs/2505.06438", "authors": ["Yankai Zeng", "Gopal Gupta"], "title": "Reliable Collaborative Conversational Agent System Based on LLMs and Answer Set Programming", "categories": ["cs.AI"], "comment": "14 pages", "summary": "As the Large-Language-Model-driven (LLM-driven) Artificial Intelligence (AI)\nbots became popular, people realized their strong potential in Task-Oriented\nDialogue (TOD). However, bots relying wholly on LLMs are unreliable in their\nknowledge, and whether they can finally produce a correct result for the task\nis not guaranteed. The collaboration among these agents also remains a\nchallenge, since the necessary information to convey is unclear, and the\ninformation transfer is by prompts -- unreliable, and malicious knowledge is\neasy to inject. With the help of logic programming tools such as Answer Set\nProgramming (ASP), conversational agents can be built safely and reliably, and\ncommunication among the agents made more efficient and secure. We proposed an\nAdministrator-Assistant Dual-Agent paradigm, where the two ASP-driven bots\nshare the same knowledge base and complete their tasks independently, while the\ninformation can be passed by a Collaborative Rule Set (CRS). The knowledge and\ninformation conveyed are encapsulated and invisible to the users, ensuring the\nsecurity of information transmission. We have constructed AutoManager, a\ndual-agent system for managing the drive-through window of a fast-food\nrestaurant such as Taco Bell in the US. In AutoManager, the assistant bot takes\nthe customer's order while the administrator bot manages the menu and food\nsupply. We evaluated our AutoManager and compared it with the real-world Taco\nBell Drive-Thru AI Order Taker, and the results show that our method is more\nreliable.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u903b\u8f91\u7f16\u7a0b\u7684\u53cc\u4ee3\u7406\u67b6\u6784\uff08Administrator-Assistant Dual-Agent\uff09\uff0c\u901a\u8fc7\u5171\u4eab\u77e5\u8bc6\u5e93\u548c\u534f\u4f5c\u89c4\u5219\u96c6\uff08CRS\uff09\u5b9e\u73b0\u9ad8\u6548\u5b89\u5168\u7684\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\uff08TOD\uff09\uff0c\u5bf9\u6bd4\u5b9e\u9a8c\u8868\u660e\u5176\u53ef\u9760\u6027\u4f18\u4e8e\u7eafLLM\u9a71\u52a8\u7684\u65b9\u6848\u3002", "motivation": "\u73b0\u6709LLM\u9a71\u52a8\u7684AI\u673a\u5668\u4eba\u5728\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u4e2d\u5b58\u5728\u77e5\u8bc6\u4e0d\u53ef\u9760\u3001\u534f\u4f5c\u6548\u7387\u4f4e\u4e14\u6613\u53d7\u6076\u610f\u6ce8\u5165\u7684\u95ee\u9898\uff0c\u9700\u4e00\u79cd\u5b89\u5168\u53ef\u9760\u7684\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\u3002", "method": "\u91c7\u7528Answer Set Programming\uff08ASP\uff09\u5de5\u5177\u6784\u5efa\u53cc\u4ee3\u7406\u7cfb\u7edf\uff0c\u5305\u62ec\u7ba1\u7406\u5458\u548c\u52a9\u624b\u4ee3\u7406\uff0c\u901a\u8fc7CRS\u5171\u4eab\u77e5\u8bc6\u5e76\u72ec\u7acb\u5b8c\u6210\u4efb\u52a1\uff0c\u786e\u4fdd\u4fe1\u606f\u5b89\u5168\u4f20\u8f93\u3002", "result": "\u5f00\u53d1\u4e86AutoManager\u7cfb\u7edf\uff0c\u5e94\u7528\u4e8e\u5feb\u9910\u5e97\u6c7d\u8f66\u9910\u5385\u573a\u666f\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6bd4Taco Bell\u73b0\u6709AI\u63a5\u5355\u7cfb\u7edf\u66f4\u53ef\u9760\u3002", "conclusion": "\u57fa\u4e8e\u903b\u8f91\u7f16\u7a0b\u7684\u53cc\u4ee3\u7406\u67b6\u6784\u80fd\u6709\u6548\u63d0\u5347\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7684\u53ef\u9760\u6027\u4e0e\u5b89\u5168\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.06569", "pdf": "https://arxiv.org/pdf/2505.06569", "abs": "https://arxiv.org/abs/2505.06569", "authors": ["Woosang Lim", "Zekun Li", "Gyuwan Kim", "Sungyoung Ji", "HyeonJung Kim", "Kyuri Choi", "Jin Hyuk Lim", "Kyungpyo Park", "William Yang Wang"], "title": "MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Long-context (LC) Large Language Models (LLMs) combined with\nRetrieval-Augmented Generation (RAG) hold strong potential for complex\nmulti-hop and large-document tasks. However, existing RAG systems often suffer\nfrom imprecise retrieval, incomplete context coverage under constrained context\nwindows, and fragmented information caused by suboptimal context construction.\nWe introduce Multi-scale Adaptive Context RAG (MacRAG), a hierarchical\nretrieval framework that compresses and partitions documents into\ncoarse-to-fine granularities, then adaptively merges relevant contexts through\nchunk- and document-level expansions in real time. By starting from the\nfinest-level retrieval and progressively incorporating higher-level and broader\ncontext, MacRAG constructs effective query-specific long contexts, optimizing\nboth precision and coverage. Evaluations on the challenging LongBench\nexpansions of HotpotQA, 2WikiMultihopQA, and Musique confirm that MacRAG\nconsistently surpasses baseline RAG pipelines on single- and multi-step\ngeneration with Llama-3.1-8B, Gemini-1.5-pro, and GPT-4o. Our results establish\nMacRAG as an efficient, scalable solution for real-world long-context,\nmulti-hop reasoning. Our code is available at\nhttps://github.com/Leezekun/MacRAG.", "AI": {"tldr": "MacRAG\u662f\u4e00\u4e2a\u5206\u5c42\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u6784\u5efa\u4f18\u5316RAG\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u591a\u8df3\u548c\u957f\u6587\u6863\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u5b58\u5728\u68c0\u7d22\u4e0d\u7cbe\u51c6\u3001\u4e0a\u4e0b\u6587\u8986\u76d6\u4e0d\u5b8c\u6574\u4ee5\u53ca\u4fe1\u606f\u788e\u7247\u5316\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "MacRAG\u91c7\u7528\u5206\u5c42\u68c0\u7d22\uff0c\u5c06\u6587\u6863\u538b\u7f29\u5e76\u5206\u533a\u4e3a\u7c97\u5230\u7ec6\u7684\u7c92\u5ea6\uff0c\u901a\u8fc7\u5b9e\u65f6\u5757\u7ea7\u548c\u6587\u6863\u7ea7\u6269\u5c55\u81ea\u9002\u5e94\u5408\u5e76\u76f8\u5173\u4e0a\u4e0b\u6587\u3002", "result": "\u5728LongBench\u6269\u5c55\u6570\u636e\u96c6\u4e0a\uff0cMacRAG\u663e\u8457\u4f18\u4e8e\u57fa\u7ebfRAG\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u6b65\u751f\u6210\u548c\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u3002", "conclusion": "MacRAG\u4e3a\u73b0\u5b9e\u4e16\u754c\u7684\u957f\u4e0a\u4e0b\u6587\u3001\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.06259", "pdf": "https://arxiv.org/pdf/2505.06259", "abs": "https://arxiv.org/abs/2505.06259", "authors": ["Mattia Setzu", "Riccardo Guidotti"], "title": "Fair Clustering with Clusterlets", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Given their widespread usage in the real world, the fairness of clustering\nmethods has become of major interest. Theoretical results on fair clustering\nshow that fairness enjoys transitivity: given a set of small and fair clusters,\na trivial centroid-based clustering algorithm yields a fair clustering.\nUnfortunately, discovering a suitable starting clustering can be\ncomputationally expensive, rather complex or arbitrary.\n  In this paper, we propose a set of simple \\emph{clusterlet}-based fuzzy\nclustering algorithms that match single-class clusters, optimizing fair\nclustering. Matching leverages clusterlet distance, optimizing for classic\nclustering objectives, while also regularizing for fairness. Empirical results\nshow that simple matching strategies are able to achieve high fairness, and\nthat appropriate parameter tuning allows to achieve high cohesion and low\noverlap.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u7c07\u7684\u6a21\u7cca\u805a\u7c7b\u7b97\u6cd5\uff0c\u4f18\u5316\u4e86\u516c\u5e73\u805a\u7c7b\u6548\u679c\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u5728\u4fdd\u8bc1\u516c\u5e73\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u5185\u805a\u548c\u4f4e\u91cd\u53e0\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u805a\u7c7b\u65b9\u6cd5\u7684\u516c\u5e73\u6027\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u627e\u5230\u5408\u9002\u7684\u521d\u59cb\u805a\u7c7b\u65f6\u53ef\u80fd\u8ba1\u7b97\u590d\u6742\u6216\u968f\u610f\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7b80\u5355\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5c0f\u7c07\uff08clusterlet\uff09\u7684\u6a21\u7cca\u805a\u7c7b\u7b97\u6cd5\uff0c\u901a\u8fc7\u5339\u914d\u5355\u7c7b\u7c07\u5e76\u4f18\u5316\u805a\u7c7b\u8ddd\u79bb\uff0c\u540c\u65f6\u7528\u6b63\u5219\u5316\u65b9\u6cd5\u4fdd\u8bc1\u516c\u5e73\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u7b80\u5355\u7684\u5339\u914d\u7b56\u7565\u80fd\u5b9e\u73b0\u9ad8\u516c\u5e73\u6027\uff0c\u53c2\u6570\u8c03\u4f18\u540e\u8fd8\u80fd\u8fbe\u5230\u9ad8\u5185\u805a\u548c\u4f4e\u91cd\u53e0\u7684\u76ee\u6807\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u516c\u5e73\u6027\u548c\u805a\u7c7b\u6548\u679c\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7b80\u5355\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.06464", "pdf": "https://arxiv.org/pdf/2505.06464", "abs": "https://arxiv.org/abs/2505.06464", "authors": ["Tamara Paris", "AJung Moon", "Jin Guo"], "title": "Opening the Scope of Openness in AI", "categories": ["cs.AI"], "comment": "To appear in ACM Conference on Fairness, Accountability, and\n  Transparency (ACM FAccT) 2025", "summary": "The concept of openness in AI has so far been heavily inspired by the\ndefinition and community practice of open source software. This positions\nopenness in AI as having positive connotations; it introduces assumptions of\ncertain advantages, such as collaborative innovation and transparency. However,\nthe practices and benefits of open source software are not fully transferable\nto AI, which has its own challenges. Framing a notion of openness tailored to\nAI is crucial to addressing its growing societal implications, risks, and\ncapabilities. We argue that considering the fundamental scope of openness in\ndifferent disciplines will broaden discussions, introduce important\nperspectives, and reflect on what openness in AI should mean. Toward this goal,\nwe qualitatively analyze 98 concepts of openness discovered from topic\nmodeling, through which we develop a taxonomy of openness. Using this taxonomy\nas an instrument, we situate the current discussion on AI openness, identify\ngaps and highlight links with other disciplines. Our work contributes to the\nrecent efforts in framing openness in AI by reflecting principles and practices\nof openness beyond open source software and calls for a more holistic view of\nopenness in terms of actions, system properties, and ethical objectives.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u9886\u57df\u5f00\u653e\u6027\u7684\u6982\u5ff5\uff0c\u6307\u51fa\u76ee\u524d\u4e3b\u8981\u53d7\u5f00\u6e90\u8f6f\u4ef6\u542f\u53d1\uff0c\u4f46\u5f00\u6e90\u8f6f\u4ef6\u7684\u4f18\u52bf\u4e0d\u80fd\u5b8c\u5168\u9002\u7528\u4e8eAI\u3002\u901a\u8fc7\u5206\u679098\u4e2a\u5f00\u653e\u6027\u6982\u5ff5\u5e76\u6784\u5efa\u5206\u7c7b\u6cd5\uff0c\u4f5c\u8005\u547c\u5401\u66f4\u5168\u9762\u7684AI\u5f00\u653e\u6027\u5b9a\u4e49\uff0c\u6db5\u76d6\u884c\u4e3a\u3001\u7cfb\u7edf\u5c5e\u6027\u548c\u4f26\u7406\u76ee\u6807\u3002", "motivation": "\u5f53\u524dAI\u5f00\u653e\u6027\u6982\u5ff5\u4e3b\u8981\u57fa\u4e8e\u5f00\u6e90\u8f6f\u4ef6\uff0c\u4f46\u5176\u4f18\u52bf\u4e0d\u5b8c\u5168\u9002\u7528\u4e8eAI\u3002\u4f5c\u8005\u5e0c\u671b\u6784\u5efa\u66f4\u9002\u5408AI\u7684\u5f00\u653e\u6027\u5b9a\u4e49\uff0c\u4ee5\u5e94\u5bf9\u5176\u793e\u4f1a\u5f71\u54cd\u3001\u98ce\u9669\u548c\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u4e3b\u9898\u5efa\u6a21\u53d1\u73b098\u4e2a\u5f00\u653e\u6027\u6982\u5ff5\uff0c\u5b9a\u6027\u5206\u6790\u5e76\u6784\u5efa\u5206\u7c7b\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u5f00\u653e\u6027\u7684\u8ba8\u8bba\u73b0\u72b6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f00\u653e\u6027\u5206\u7c7b\u6cd5\uff0c\u63ed\u793a\u4e86AI\u5f00\u653e\u6027\u8ba8\u8bba\u7684\u5c40\u9650\uff0c\u5e76\u5f3a\u8c03\u4e86\u4e0e\u5176\u4ed6\u5b66\u79d1\u7684\u5173\u8054\u3002", "conclusion": "\u547c\u5401\u8d85\u8d8a\u5f00\u6e90\u8f6f\u4ef6\u7684\u5f00\u653e\u6027\u6846\u67b6\uff0c\u5efa\u7acb\u66f4\u5168\u9762\u7684AI\u5f00\u653e\u6027\u5b9a\u4e49\uff0c\u6db5\u76d6\u884c\u4e3a\u3001\u7cfb\u7edf\u5c5e\u6027\u548c\u4f26\u7406\u76ee\u6807\u3002"}}
{"id": "2505.06591", "pdf": "https://arxiv.org/pdf/2505.06591", "abs": "https://arxiv.org/abs/2505.06591", "authors": ["Anna Wr\u00f3blewska", "Bartosz Grabek", "Jakub \u015awistak", "Daniel Dan"], "title": "Evaluating LLM-Generated Q&A Test: a Student-Centered Study", "categories": ["cs.CL", "cs.HC"], "comment": "accepted to AIED 2025", "summary": "This research prepares an automatic pipeline for generating reliable\nquestion-answer (Q&A) tests using AI chatbots. We automatically generated a\nGPT-4o-mini-based Q&A test for a Natural Language Processing course and\nevaluated its psychometric and perceived-quality metrics with students and\nexperts. A mixed-format IRT analysis showed that the generated items exhibit\nstrong discrimination and appropriate difficulty, while student and expert star\nratings reflect high overall quality. A uniform DIF check identified two items\nfor review. These findings demonstrate that LLM-generated assessments can match\nhuman-authored tests in psychometric performance and user satisfaction,\nillustrating a scalable approach to AI-assisted assessment development.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u804a\u5929\u673a\u5668\u4eba\u7684\u81ea\u52a8\u5316\u751f\u6210\u53ef\u9760\u95ee\u7b54\u6d4b\u8bd5\u7684\u6d41\u7a0b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5fc3\u7406\u6d4b\u91cf\u5b66\u6027\u80fd\u4e0e\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5229\u7528AI\uff08\u5982GPT-4o-mini\uff09\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u95ee\u7b54\u6d4b\u8bd5\u7684\u53ef\u884c\u6027\uff0c\u4ee5\u89e3\u51b3\u4eba\u5de5\u7f16\u5199\u6d4b\u8bd5\u7684 scalability \u95ee\u9898\u3002", "method": "\u4f7f\u7528GPT-4o-mini\u81ea\u52a8\u751f\u6210\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u8bfe\u7a0b\u7684\u95ee\u7b54\u6d4b\u8bd5\uff0c\u5e76\u901a\u8fc7\u5b66\u751f\u548c\u4e13\u5bb6\u7684\u5fc3\u7406\u6d4b\u91cf\u5b66\u6307\u6807\uff08\u5982IRT\u5206\u6790\uff09\u53ca\u611f\u77e5\u8d28\u91cf\u8bc4\u5206\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u751f\u6210\u7684\u6d4b\u8bd5\u9898\u5c55\u73b0\u4e86\u5f3a\u533a\u5206\u5ea6\u548c\u5408\u9002\u96be\u5ea6\uff0c\u5b66\u751f\u548c\u4e13\u5bb6\u8bc4\u5206\u663e\u793a\u9ad8\u8d28\u91cf\uff1b\u4ec5\u4e24\u9898\u9700\u5ba1\u67e5\uff08DIF\u5206\u6790\uff09\u3002", "conclusion": "LLM\u751f\u6210\u7684\u6d4b\u8bd5\u5728\u5fc3\u7406\u6d4b\u91cf\u5b66\u6027\u80fd\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u4e0a\u53ef\u5ab2\u7f8e\u4eba\u5de5\u6d4b\u8bd5\uff0c\u4e3aAI\u8f85\u52a9\u8bc4\u4f30\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u65b9\u6848\u3002"}}
{"id": "2505.06262", "pdf": "https://arxiv.org/pdf/2505.06262", "abs": "https://arxiv.org/abs/2505.06262", "authors": ["Zara Siddique", "Liam D. Turner", "Luis Espinosa-Anke"], "title": "Dialz: A Python Toolkit for Steering Vectors", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce Dialz, a framework for advancing research on steering vectors\nfor open-source LLMs, implemented in Python. Steering vectors allow users to\nmodify activations at inference time to amplify or weaken a 'concept', e.g.\nhonesty or positivity, providing a more powerful alternative to prompting or\nfine-tuning. Dialz supports a diverse set of tasks, including creating\ncontrastive pair datasets, computing and applying steering vectors, and\nvisualizations. Unlike existing libraries, Dialz emphasizes modularity and\nusability, enabling both rapid prototyping and in-depth analysis. We\ndemonstrate how Dialz can be used to reduce harmful outputs such as\nstereotypes, while also providing insights into model behaviour across\ndifferent layers. We release Dialz with full documentation, tutorials, and\nsupport for popular open-source models to encourage further research in safe\nand controllable language generation. Dialz enables faster research cycles and\nfacilitates insights into model interpretability, paving the way for safer,\nmore transparent, and more reliable AI systems.", "AI": {"tldr": "Dialz\u662f\u4e00\u4e2a\u7528\u4e8e\u5f00\u6e90LLMs\u7684\u8f6c\u5411\u5411\u91cf\u7814\u7a76\u7684Python\u6846\u67b6\uff0c\u652f\u6301\u591a\u79cd\u4efb\u52a1\uff0c\u5f3a\u8c03\u6a21\u5757\u5316\u548c\u6613\u7528\u6027\uff0c\u65e8\u5728\u52a0\u901f\u7814\u7a76\u5e76\u63d0\u5347AI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u63a7\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u4f9b\u6bd4\u63d0\u793a\u6216\u5fae\u8c03\u66f4\u5f3a\u5927\u7684\u65b9\u6cd5\u6765\u4fee\u6539LLMs\u7684\u884c\u4e3a\uff0c\u4f8b\u5982\u589e\u5f3a\u6216\u51cf\u5f31\u7279\u5b9a\u6982\u5ff5\uff08\u5982\u8bda\u5b9e\u6216\u79ef\u6781\u6027\uff09\uff0c\u5e76\u4fc3\u8fdb\u5b89\u5168\u53ef\u63a7\u7684\u8bed\u8a00\u751f\u6210\u7814\u7a76\u3002", "method": "\u8bbe\u8ba1\u4e86Dialz\u6846\u67b6\uff0c\u652f\u6301\u521b\u5efa\u5bf9\u6bd4\u5bf9\u6570\u636e\u96c6\u3001\u8ba1\u7b97\u548c\u5e94\u7528\u8f6c\u5411\u5411\u91cf\u4ee5\u53ca\u53ef\u89c6\u5316\u7b49\u529f\u80fd\uff0c\u5f3a\u8c03\u6a21\u5757\u5316\u548c\u6613\u7528\u6027\u3002", "result": "Dialz\u80fd\u591f\u51cf\u5c11\u6709\u5bb3\u8f93\u51fa\uff08\u5982\u523b\u677f\u5370\u8c61\uff09\uff0c\u5e76\u63d0\u4f9b\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u6df1\u5165\u6d1e\u5bdf\u3002", "conclusion": "Dialz\u901a\u8fc7\u52a0\u901f\u7814\u7a76\u5468\u671f\u548c\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u66f4\u5b89\u5168\u3001\u900f\u660e\u3001\u53ef\u9760\u7684AI\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2505.06469", "pdf": "https://arxiv.org/pdf/2505.06469", "abs": "https://arxiv.org/abs/2505.06469", "authors": ["Yumou Wei", "Paulo Carvalho", "John Stamper"], "title": "KCluster: An LLM-based Clustering Approach to Knowledge Component Discovery", "categories": ["cs.AI", "cs.HC"], "comment": "Accepted to the Educational Data Mining (EDM) 2025 conference", "summary": "Educators evaluate student knowledge using knowledge component (KC) models\nthat map assessment questions to KCs. Still, designing KC models for large\nquestion banks remains an insurmountable challenge for instructors who need to\nanalyze each question by hand. The growing use of Generative AI in education is\nexpected only to aggravate this chronic deficiency of expert-designed KC\nmodels, as course engineers designing KCs struggle to keep up with the pace at\nwhich questions are generated. In this work, we propose KCluster, a novel KC\ndiscovery algorithm based on identifying clusters of congruent questions\naccording to a new similarity metric induced by a large language model (LLM).\nWe demonstrate in three datasets that an LLM can create an effective metric of\nquestion similarity, which a clustering algorithm can use to create KC models\nfrom questions with minimal human effort. Combining the strengths of LLM and\nclustering, KCluster generates descriptive KC labels and discovers KC models\nthat predict student performance better than the best expert-designed models\navailable. In anticipation of future work, we illustrate how KCluster can\nreveal insights into difficult KCs and suggest improvements to instruction.", "AI": {"tldr": "KCluster\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b0\u578b\u77e5\u8bc6\u7ec4\u4ef6\uff08KC\uff09\u53d1\u73b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u805a\u7c7b\u76f8\u4f3c\u95ee\u9898\u81ea\u52a8\u751f\u6210KC\u6a21\u578b\uff0c\u663e\u8457\u4f18\u4e8e\u4e13\u5bb6\u8bbe\u8ba1\u7684\u6a21\u578b\u3002", "motivation": "\u4f20\u7edfKC\u6a21\u578b\u8bbe\u8ba1\u4f9d\u8d56\u4eba\u5de5\u5206\u6790\uff0c\u96be\u4ee5\u5e94\u5bf9\u5927\u89c4\u6a21\u9898\u5e93\u548c\u751f\u6210\u5f0fAI\u5feb\u901f\u751f\u6210\u95ee\u9898\u7684\u6311\u6218\u3002", "method": "\u5229\u7528LLM\u8861\u91cf\u95ee\u9898\u76f8\u4f3c\u6027\uff0c\u7ed3\u5408\u805a\u7c7b\u7b97\u6cd5\u81ea\u52a8\u751f\u6210KC\u6a21\u578b\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002", "result": "KCluster\u751f\u6210\u7684KC\u6a21\u578b\u5728\u5b66\u751f\u8868\u73b0\u9884\u6d4b\u4e0a\u4f18\u4e8e\u4e13\u5bb6\u8bbe\u8ba1\u7684\u6a21\u578b\uff0c\u5e76\u80fd\u63d0\u4f9b\u63cf\u8ff0\u6027\u6807\u7b7e\u3002", "conclusion": "KCluster\u4e3a\u6559\u80b2\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684KC\u6a21\u578b\u751f\u6210\u65b9\u6cd5\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u6559\u5b66\u3002"}}
{"id": "2505.06594", "pdf": "https://arxiv.org/pdf/2505.06594", "abs": "https://arxiv.org/abs/2505.06594", "authors": ["Galann Pennec", "Zhengyuan Liu", "Nicholas Asher", "Philippe Muller", "Nancy F. Chen"], "title": "Integrating Video and Text: A Balanced Approach to Multimodal Summary Generation and Evaluation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) often struggle to balance visual and textual\ninformation when summarizing complex multimodal inputs, such as entire TV show\nepisodes. In this paper, we propose a zero-shot video-to-text summarization\napproach that builds its own screenplay representation of an episode,\neffectively integrating key video moments, dialogue, and character information\ninto a unified document. Unlike previous approaches, we simultaneously generate\nscreenplays and name the characters in zero-shot, using only the audio, video,\nand transcripts as input. Additionally, we highlight that existing\nsummarization metrics can fail to assess the multimodal content in summaries.\nTo address this, we introduce MFactSum, a multimodal metric that evaluates\nsummaries with respect to both vision and text modalities. Using MFactSum, we\nevaluate our screenplay summaries on the SummScreen3D dataset, demonstrating\nsuperiority against state-of-the-art VLMs such as Gemini 1.5 by generating\nsummaries containing 20% more relevant visual information while requiring 75%\nless of the video as input.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u89c6\u9891\u5230\u6587\u672c\u6458\u8981\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5267\u672c\u8868\u793a\u6574\u5408\u89c6\u9891\u3001\u5bf9\u8bdd\u548c\u89d2\u8272\u4fe1\u606f\uff0c\u5e76\u5f15\u5165MFactSum\u591a\u6a21\u6001\u8bc4\u4f30\u6307\u6807\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5f53\u524d\u6700\u4f73VLM\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u591a\u6a21\u6001\u8f93\u5165\uff08\u5982\u6574\u96c6\u7535\u89c6\u5267\uff09\u65f6\u96be\u4ee5\u5e73\u8861\u89c6\u89c9\u4e0e\u6587\u672c\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u6458\u8981\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u96f6\u6837\u672c\u65b9\u6cd5\u751f\u6210\u5267\u672c\u8868\u793a\uff0c\u6574\u5408\u5173\u952e\u89c6\u9891\u7247\u6bb5\u3001\u5bf9\u8bdd\u548c\u89d2\u8272\u4fe1\u606f\uff0c\u4ec5\u9700\u97f3\u9891\u3001\u89c6\u9891\u548c\u6587\u672c\u4f5c\u4e3a\u8f93\u5165\u3002\u540c\u65f6\u63d0\u51fa\u591a\u6a21\u6001\u8bc4\u4f30\u6307\u6807MFactSum\u3002", "result": "\u5728SummScreen3D\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6bd4Gemini 1.5\u591a\u5305\u542b20%\u7684\u76f8\u5173\u89c6\u89c9\u4fe1\u606f\uff0c\u4e14\u4ec5\u9700\u540e\u800575%\u7684\u89c6\u9891\u8f93\u5165\u3002", "conclusion": "\u63d0\u51fa\u7684\u96f6\u6837\u672c\u5267\u672c\u6458\u8981\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u6458\u8981\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0cMFactSum\u6307\u6807\u4e3a\u591a\u6a21\u6001\u5185\u5bb9\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.06265", "pdf": "https://arxiv.org/pdf/2505.06265", "abs": "https://arxiv.org/abs/2505.06265", "authors": ["Jacques Peter", "Quentin Bennehard", "S\u00e9bastien Heib", "Jean-Luc Hantrais-Gervois", "Fr\u00e9d\u00e9ric Mo\u00ebns"], "title": "ONERA's CRM WBPN database for machine learning activities, related regression challenge and first results", "categories": ["cs.LG"], "comment": "16 pages, 9 figures", "summary": "This paper presents a new Computational Fluid Dynamics database, developed at\nONERA, to support the advancement of machine learning techniques for\naerodynamic field prediction. It contains 468 Reynolds-Averaged Navier-Stokes\nsimulations using the Spalart-Allmaras turbulence model, performed on the\nNASA/Boeing Common Research Model wing-body-pylon-nacelle configuration. The\ndatabase spans a wide range of flow conditions, varying Mach number (including\ntransonic regimes), angle of attack (capturing flow separation), and Reynolds\nnumber (based on three stagnation pressures, with one setting matching wind\ntunnel experiments). The quality of the database is assessed, through checking\nthe convergence level of each computation.\n  Based on these data, a regression challenge is defined. It consists in\npredicting the wall distributions of pressure and friction coefficients for\nunseen aerodynamic conditions. The 468 simulations are split into training and\ntesting sets, with the training data made available publicly on the Codabench\nplatform. The paper further evaluates several classical machine learning\nregressors on this task. Tested pointwise methods include Multi-Layer\nPerceptrons, $\\lambda$-DNNs, and Decision Trees, while global methods include\nMulti-Layer Perceptron, k-Nearest Neighbors, Proper Orthogonal Decomposition\nand IsoMap. Initial performance results, using $R^2$ scores and worst relative\nmean absolute error metrics, are presented, offering insights into the\ncapabilities of these techniques for the challenge and references for future\nwork.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86ONERA\u5f00\u53d1\u7684\u4e00\u4e2a\u65b0\u7684\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\u6570\u636e\u5e93\uff0c\u7528\u4e8e\u652f\u6301\u673a\u5668\u5b66\u4e60\u5728\u6c14\u52a8\u529b\u573a\u9884\u6d4b\u4e2d\u7684\u53d1\u5c55\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u6570\u636e\u5b9a\u4e49\u4e86\u56de\u5f52\u6311\u6218\u4efb\u52a1\u3002", "motivation": "\u63a8\u52a8\u673a\u5668\u5b66\u4e60\u5728\u6c14\u52a8\u529b\u573a\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u4f7f\u7528468\u4e2aRANS\u6a21\u62df\u6570\u636e\uff0c\u8986\u76d6\u591a\u79cd\u6d41\u52a8\u6761\u4ef6\uff0c\u901a\u8fc7\u8bad\u7ec3\u548c\u6d4b\u8bd5\u96c6\u5212\u5206\uff0c\u8bc4\u4f30\u591a\u79cd\u673a\u5668\u5b66\u4e60\u56de\u5f52\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "result": "\u521d\u6b65\u6027\u80fd\u7ed3\u679c\u5c55\u793a\u4e86\u4e0d\u540c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u538b\u529b\u548c\u6469\u64e6\u7cfb\u6570\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u53c2\u8003\u3002", "conclusion": "\u8be5\u6570\u636e\u5e93\u548c\u6311\u6218\u4efb\u52a1\u4e3a\u673a\u5668\u5b66\u4e60\u5728\u6c14\u52a8\u529b\u5b66\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u548c\u65b9\u5411\u3002"}}
{"id": "2505.06492", "pdf": "https://arxiv.org/pdf/2505.06492", "abs": "https://arxiv.org/abs/2505.06492", "authors": ["Chathurangi Shyalika", "Renjith Prasad", "Alaa Al Ghazo", "Darssan Eswaramoorthi", "Harleen Kaur", "Sara Shree Muthuselvam", "Amit Sheth"], "title": "SmartPilot: A Multiagent CoPilot for Adaptive and Intelligent Manufacturing", "categories": ["cs.AI"], "comment": "8 pages, 8 figures, 4 tables, IEEE Conference on Artificial\n  Intelligence (IEEE CAI) 2025", "summary": "In the dynamic landscape of Industry 4.0, achieving efficiency, precision,\nand adaptability is essential to optimize manufacturing operations. Industries\nsuffer due to supply chain disruptions caused by anomalies, which are being\ndetected by current AI models but leaving domain experts uncertain without\ndeeper insights into these anomalies. Additionally, operational inefficiencies\npersist due to inaccurate production forecasts and the limited effectiveness of\ntraditional AI models for processing complex sensor data. Despite these\nadvancements, existing systems lack the seamless integration of these\ncapabilities needed to create a truly unified solution for enhancing production\nand decision-making. We propose SmartPilot, a neurosymbolic, multiagent CoPilot\ndesigned for advanced reasoning and contextual decision-making to address these\nchallenges. SmartPilot processes multimodal sensor data and is compact to\ndeploy on edge devices. It focuses on three key tasks: anomaly prediction,\nproduction forecasting, and domain-specific question answering. By bridging the\ngap between AI capabilities and real-world industrial needs, SmartPilot\nempowers industries with intelligent decision-making and drives transformative\ninnovation in manufacturing. The demonstration video, datasets, and\nsupplementary materials are available at\nhttps://github.com/ChathurangiShyalika/SmartPilot.", "AI": {"tldr": "SmartPilot\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u5316\u7684\u591a\u4ee3\u7406CoPilot\uff0c\u65e8\u5728\u901a\u8fc7\u5904\u7406\u590d\u6742\u4f20\u611f\u5668\u6570\u636e\u89e3\u51b3\u5de5\u4e1a4.0\u4e2d\u7684\u5f02\u5e38\u9884\u6d4b\u3001\u751f\u4ea7\u9884\u6d4b\u548c\u9886\u57df\u7279\u5b9a\u95ee\u9898\u56de\u7b54\uff0c\u63d0\u5347\u5236\u9020\u4e1a\u7684\u667a\u80fd\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u5f53\u524dAI\u6a21\u578b\u867d\u80fd\u68c0\u6d4b\u5f02\u5e38\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5f02\u5e38\u6df1\u5c42\u5206\u6790\u7684\u89c1\u89e3\uff0c\u4e14\u4f20\u7edfAI\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u4f20\u611f\u5668\u6570\u636e\u65f6\u6548\u679c\u6709\u9650\uff0c\u5bfc\u81f4\u4f9b\u5e94\u94fe\u4e2d\u65ad\u548c\u751f\u4ea7\u6548\u7387\u4f4e\u4e0b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u65e0\u7f1d\u96c6\u6210\u8fd9\u4e9b\u80fd\u529b\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002", "method": "SmartPilot\u7ed3\u5408\u795e\u7ecf\u7b26\u53f7\u5316\u548c\u591a\u4ee3\u7406\u6280\u672f\uff0c\u5904\u7406\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\uff0c\u5e76\u53ef\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002\u5176\u6838\u5fc3\u4efb\u52a1\u5305\u62ec\u5f02\u5e38\u9884\u6d4b\u3001\u751f\u4ea7\u9884\u6d4b\u548c\u9886\u57df\u7279\u5b9a\u95ee\u9898\u56de\u7b54\u3002", "result": "SmartPilot\u6210\u529f\u5b9e\u73b0\u4e86\u590d\u6742\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\uff0c\u901a\u8fc7\u63d0\u4f9b\u5f02\u5e38\u6df1\u5c42\u5206\u6790\u548c\u7cbe\u51c6\u751f\u4ea7\u9884\u6d4b\uff0c\u586b\u8865\u4e86AI\u80fd\u529b\u4e0e\u5b9e\u9645\u5de5\u4e1a\u9700\u6c42\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "conclusion": "SmartPilot\u901a\u8fc7\u5176\u795e\u7ecf\u7b26\u53f7\u5316\u548c\u591a\u4ee3\u7406\u8bbe\u8ba1\uff0c\u4e3a\u5236\u9020\u4e1a\u63d0\u4f9b\u4e86\u667a\u80fd\u51b3\u7b56\u652f\u6301\uff0c\u63a8\u52a8\u4e86\u5de5\u4e1a4.0\u4e2d\u7684\u53d8\u9769\u6027\u521b\u65b0\u3002"}}
{"id": "2505.06599", "pdf": "https://arxiv.org/pdf/2505.06599", "abs": "https://arxiv.org/abs/2505.06599", "authors": ["Abbas Bertina", "Shahab Beirami", "Hossein Biniazian", "Elham Esmaeilnia", "Soheil Shahi", "Mahdi Pirnia"], "title": "Bridging the Gap: An Intermediate Language for Enhanced and Cost-Effective Grapheme-to-Phoneme Conversion with Homographs with Multiple Pronunciations Disambiguation", "categories": ["cs.CL", "I.2.7; I.2.6"], "comment": "pdf, 8 pages, 4 figures, 4 tables", "summary": "Grapheme-to-phoneme (G2P) conversion for Persian presents unique challenges\ndue to its complex phonological features, particularly homographs and Ezafe,\nwhich exist in formal and informal language contexts. This paper introduces an\nintermediate language specifically designed for Persian language processing\nthat addresses these challenges through a multi-faceted approach. Our\nmethodology combines two key components: Large Language Model (LLM) prompting\ntechniques and a specialized sequence-to-sequence machine transliteration\narchitecture. We developed and implemented a systematic approach for\nconstructing a comprehensive lexical database for homographs with multiple\npronunciations disambiguation often termed polyphones, utilizing formal concept\nanalysis for semantic differentiation. We train our model using two distinct\ndatasets: the LLM-generated dataset for formal and informal Persian and the\nB-Plus podcasts for informal language variants. The experimental results\ndemonstrate superior performance compared to existing state-of-the-art\napproaches, particularly in handling the complexities of Persian phoneme\nconversion. Our model significantly improves Phoneme Error Rate (PER) metrics,\nestablishing a new benchmark for Persian G2P conversion accuracy. This work\ncontributes to the growing research in low-resource language processing and\nprovides a robust solution for Persian text-to-speech systems and demonstrating\nits applicability beyond Persian. Specifically, the approach can extend to\nlanguages with rich homographic phenomena such as Chinese and Arabic", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6ce2\u65af\u8bed\u7684\u591a\u5c42\u9762G2P\u8f6c\u6362\u65b9\u6cd5\uff0c\u7ed3\u5408LLM\u63d0\u793a\u6280\u672f\u548c\u5e8f\u5217\u5230\u5e8f\u5217\u673a\u5668\u97f3\u8bd1\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u7d20\u9519\u8bef\u7387\uff08PER\uff09\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002", "motivation": "\u6ce2\u65af\u8bedG2P\u8f6c\u6362\u56e0\u540c\u5f62\u5f02\u4e49\u8bcd\u548cEzafe\u7b49\u590d\u6742\u8bed\u97f3\u7279\u5f81\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u6b63\u5f0f\u4e0e\u975e\u6b63\u5f0f\u8bed\u5883\u4e2d\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u4e2d\u95f4\u8bed\u8a00\u548c\u7cfb\u7edf\u6027\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u7ed3\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63d0\u793a\u6280\u672f\u548c\u4e13\u95e8\u7684\u5e8f\u5217\u5230\u5e8f\u5217\u97f3\u8bd1\u67b6\u6784\uff0c\u5e76\u5229\u7528\u5f62\u5f0f\u6982\u5ff5\u5206\u6790\u6784\u5efa\u540c\u5f62\u5f02\u4e49\u8bcd\u7684\u8bed\u4e49\u533a\u5206\u6570\u636e\u5e93\u3002\u8bad\u7ec3\u6570\u636e\u5305\u62ecLLM\u751f\u6210\u7684\u6570\u636e\u96c6\u548cB-Plus\u64ad\u5ba2\u7684\u975e\u6b63\u5f0f\u8bed\u8a00\u53d8\u4f53\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6ce2\u65af\u8bed\u97f3\u7d20\u8f6c\u6362\u590d\u6742\u6027\u5904\u7406\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u97f3\u7d20\u9519\u8bef\u7387\uff08PER\uff09\uff0c\u4e3a\u6ce2\u65af\u8bedG2P\u8f6c\u6362\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u4e3a\u6ce2\u65af\u8bed\u6587\u672c\u5230\u8bed\u97f3\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9c81\u68d2\u89e3\u51b3\u65b9\u6848\uff0c\u8fd8\u53ef\u63a8\u5e7f\u81f3\u4e2d\u6587\u548c\u963f\u62c9\u4f2f\u8bed\u7b49\u540c\u5f62\u5f02\u4e49\u73b0\u8c61\u4e30\u5bcc\u7684\u8bed\u8a00\uff0c\u63a8\u52a8\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\u7684\u7814\u7a76\u3002"}}
{"id": "2505.06266", "pdf": "https://arxiv.org/pdf/2505.06266", "abs": "https://arxiv.org/abs/2505.06266", "authors": ["Qi Cheng", "Licheng Liu", "Zhang Yao", "Hong Mu", "Shiyuan Luo", "Zhenong Jin", "Yiqun Xie", "Xiaowei Jia"], "title": "Knowledge Guided Encoder-Decoder Framework Integrating Multiple Physical Models for Agricultural Ecosystem Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Agricultural monitoring is critical for ensuring food security, maintaining\nsustainable farming practices, informing policies on mitigating food shortage,\nand managing greenhouse gas emissions. Traditional process-based physical\nmodels are often designed and implemented for specific situations, and their\nparameters could also be highly uncertain. In contrast, data-driven models\noften use black-box structures and does not explicitly model the\ninter-dependence between different ecological variables. As a result, they\nrequire extensive training data and lack generalizability to different tasks\nwith data distribution shifts and inconsistent observed variables. To address\nthe need for more universal models, we propose a knowledge-guided\nencoder-decoder model, which can predict key crop variables by leveraging\nknowledge of underlying processes from multiple physical models. The proposed\nmethod also integrates a language model to process complex and inconsistent\ninputs and also utilizes it to implement a model selection mechanism for\nselectively combining the knowledge from different physical models. Our\nevaluations on predicting carbon and nitrogen fluxes for multiple sites\ndemonstrate the effectiveness and robustness of the proposed model under\nvarious scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u77e5\u8bc6\u6307\u5bfc\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\uff0c\u7ed3\u5408\u7269\u7406\u6a21\u578b\u548c\u8bed\u8a00\u6a21\u578b\uff0c\u9884\u6d4b\u519c\u4f5c\u7269\u5173\u952e\u53d8\u91cf\uff0c\u89e3\u51b3\u4f20\u7edf\u6a21\u578b\u6cdb\u5316\u6027\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u519c\u4e1a\u76d1\u6d4b\u5bf9\u98df\u54c1\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u7269\u7406\u6a21\u578b\u548c\u6570\u636e\u9a71\u52a8\u6a21\u578b\u5b58\u5728\u53c2\u6570\u4e0d\u786e\u5b9a\u3001\u6cdb\u5316\u6027\u5dee\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8054\u5408\u4f7f\u7528\u77e5\u8bc6\u6307\u5bfc\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\u548c\u8bed\u8a00\u6a21\u578b\uff0c\u9009\u62e9\u6027\u6574\u5408\u591a\u7269\u7406\u6a21\u578b\u77e5\u8bc6\uff0c\u5904\u7406\u590d\u6742\u4e0d\u4e00\u81f4\u7684\u8f93\u5165\u3002", "result": "\u5728\u591a\u4e2a\u5730\u70b9\u9884\u6d4b\u78b3\u6c2e\u901a\u91cf\u7684\u5b9e\u9a8c\u4e2d\uff0c\u6a21\u578b\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u77e5\u8bc6\u548c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u9884\u6d4b\u7684\u901a\u7528\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2505.06505", "pdf": "https://arxiv.org/pdf/2505.06505", "abs": "https://arxiv.org/abs/2505.06505", "authors": ["Hua Meng", "Zhiguo Long", "Michael Sioutis", "Zhengchun Zhou"], "title": "On Definite Iterated Belief Revision with Belief Algebras", "categories": ["cs.AI", "I.2.4"], "comment": "10 pages. Extended version of an accepted IJCAI 2025 paper", "summary": "Traditional logic-based belief revision research focuses on designing rules\nto constrain the behavior of revision operators. Frameworks have been proposed\nto characterize iterated revision rules, but they are often too loose, leading\nto multiple revision operators that all satisfy the rules under the same belief\ncondition. In many practical applications, such as safety critical ones, it is\nimportant to specify a definite revision operator to enable agents to\niteratively revise their beliefs in a deterministic way. In this paper, we\npropose a novel framework for iterated belief revision by characterizing belief\ninformation through preference relations. Semantically, both beliefs and new\nevidence are represented as belief algebras, which provide a rich and\nexpressive foundation for belief revision. Building on traditional revision\nrules, we introduce additional postulates for revision with belief algebra,\nincluding an upper-bound constraint on the outcomes of revision. We prove that\nthe revision result is uniquely determined given the current belief state and\nnew evidence. Furthermore, to make the framework more useful in practice, we\ndevelop a particular algorithm for performing the proposed revision process. We\nargue that this approach may offer a more predictable and principled method for\nbelief revision, making it suitable for real-world applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u504f\u597d\u5173\u7cfb\u7684\u8fed\u4ee3\u4fe1\u5ff5\u4fee\u6b63\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u5ff5\u4ee3\u6570\u8868\u793a\u4fe1\u5ff5\u548c\u65b0\u8bc1\u636e\uff0c\u5f15\u5165\u989d\u5916\u7684\u4fee\u6b63\u89c4\u5219\u786e\u4fdd\u7ed3\u679c\u552f\u4e00\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u5b9e\u7528\u7b97\u6cd5\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u7b49\u5e94\u7528\u4e2d\uff0c\u9700\u8981\u786e\u5b9a\u6027\u4fe1\u5ff5\u4fee\u6b63\u64cd\u4f5c\u7b26\u4ee5\u786e\u4fdd\u8fed\u4ee3\u4fee\u6b63\u7684\u53ef\u9884\u6d4b\u6027\u3002\u4f20\u7edf\u6846\u67b6\u8fc7\u4e8e\u5bbd\u677e\uff0c\u5bfc\u81f4\u591a\u79cd\u64cd\u4f5c\u7b26\u6ee1\u8db3\u76f8\u540c\u6761\u4ef6\u3002", "method": "\u901a\u8fc7\u504f\u597d\u5173\u7cfb\u8868\u5f81\u4fe1\u5ff5\u4fe1\u606f\uff0c\u4f7f\u7528\u4fe1\u5ff5\u4ee3\u6570\u4f5c\u4e3a\u8bed\u4e49\u57fa\u7840\uff0c\u5f15\u5165\u4e0a\u754c\u7ea6\u675f\u7b49\u989d\u5916\u89c4\u5219\uff0c\u5e76\u5f00\u53d1\u5177\u4f53\u7b97\u6cd5\u5b9e\u73b0\u4fee\u6b63\u8fc7\u7a0b\u3002", "result": "\u8bc1\u660e\u5728\u7ed9\u5b9a\u5f53\u524d\u4fe1\u5ff5\u72b6\u6001\u548c\u65b0\u8bc1\u636e\u4e0b\uff0c\u4fee\u6b63\u7ed3\u679c\u662f\u552f\u4e00\u786e\u5b9a\u7684\uff0c\u5e76\u901a\u8fc7\u7b97\u6cd5\u5b9e\u73b0\u5b9e\u7528\u5316\u3002", "conclusion": "\u65b0\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u53ef\u9884\u6d4b\u3001\u539f\u5219\u6027\u7684\u4fe1\u5ff5\u4fee\u6b63\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u5e94\u7528\u3002"}}
{"id": "2505.06605", "pdf": "https://arxiv.org/pdf/2505.06605", "abs": "https://arxiv.org/abs/2505.06605", "authors": ["Min Li", "Chun Yuan"], "title": "Using External knowledge to Enhanced PLM for Semantic Matching", "categories": ["cs.CL"], "comment": null, "summary": "Modeling semantic relevance has always been a challenging and critical task\nin natural language processing. In recent years, with the emergence of massive\namounts of annotated data, it has become feasible to train complex models, such\nas neural network-based reasoning models. These models have shown excellent\nperformance in practical applications and have achieved the current\nstate-ofthe-art performance. However, even with such large-scale annotated\ndata, we still need to think: Can machines learn all the knowledge necessary to\nperform semantic relevance detection tasks based on this data alone? If not,\nhow can neural network-based models incorporate external knowledge into\nthemselves, and how can relevance detection models be constructed to make full\nuse of external knowledge? In this paper, we use external knowledge to enhance\nthe pre-trained semantic relevance discrimination model. Experimental results\non 10 public datasets show that our method achieves consistent improvements in\nperformance compared to the baseline model.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u5916\u90e8\u77e5\u8bc6\u589e\u5f3a\u9884\u8bad\u7ec3\u7684\u8bed\u4e49\u76f8\u5173\u6027\u5224\u522b\u6a21\u578b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u63a2\u8ba8\u673a\u5668\u662f\u5426\u80fd\u4ec5\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u5b66\u4e60\u8bed\u4e49\u76f8\u5173\u6027\u68c0\u6d4b\u4efb\u52a1\u6240\u9700\u7684\u5168\u90e8\u77e5\u8bc6\uff0c\u5e76\u63d0\u51fa\u5982\u4f55\u5c06\u5916\u90e8\u77e5\u8bc6\u878d\u5165\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002", "method": "\u5229\u7528\u5916\u90e8\u77e5\u8bc6\u589e\u5f3a\u9884\u8bad\u7ec3\u7684\u8bed\u4e49\u76f8\u5173\u6027\u5224\u522b\u6a21\u578b\u3002", "result": "\u572810\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u5916\u90e8\u77e5\u8bc6\u53ef\u4ee5\u6709\u6548\u589e\u5f3a\u6a21\u578b\u7684\u8bed\u4e49\u76f8\u5173\u6027\u68c0\u6d4b\u80fd\u529b\uff0c\u4e3a\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2505.06268", "pdf": "https://arxiv.org/pdf/2505.06268", "abs": "https://arxiv.org/abs/2505.06268", "authors": ["Pengcheng Sun", "Erwu Liu", "Wei Ni", "Kanglei Yu", "Rui Wang", "Abbas Jamalipour"], "title": "Cluster-Aware Multi-Round Update for Wireless Federated Learning in Heterogeneous Environments", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The aggregation efficiency and accuracy of wireless Federated Learning (FL)\nare significantly affected by resource constraints, especially in heterogeneous\nenvironments where devices exhibit distinct data distributions and\ncommunication capabilities. This paper proposes a clustering strategy that\nleverages prior knowledge similarity to group devices with similar data and\ncommunication characteristics, mitigating performance degradation from\nheterogeneity. On this basis, a novel Cluster- Aware Multi-round Update (CAMU)\nstrategy is proposed, which treats clusters as the basic units and adjusts the\nlocal update frequency based on the clustered contribution threshold,\neffectively reducing update bias and enhancing aggregation accuracy. The\ntheoretical convergence of the CAMU strategy is rigorously validated.\nMeanwhile, based on the convergence upper bound, the local update frequency and\ntransmission power of each cluster are jointly optimized to achieve an optimal\nbalance between computation and communication resources under constrained\nconditions, significantly improving the convergence efficiency of FL.\nExperimental results demonstrate that the proposed method effectively improves\nthe model performance of FL in heterogeneous environments and achieves a better\nbalance between communication cost and computational load under limited\nresources.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u7b56\u7565\u7684\u65e0\u7ebf\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u5907\u805a\u7c7b\u51cf\u5c11\u5f02\u6784\u6027\u5f71\u54cd\uff0c\u5e76\u8bbe\u8ba1\u4e86CAMU\u7b56\u7565\u4ee5\u8c03\u6574\u672c\u5730\u66f4\u65b0\u9891\u7387\u548c\u4f18\u5316\u8d44\u6e90\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86FL\u7684\u6a21\u578b\u6027\u80fd\u548c\u6536\u655b\u6548\u7387\u3002", "motivation": "\u65e0\u7ebf\u8054\u90a6\u5b66\u4e60\u5728\u5f02\u6784\u73af\u5883\u4e2d\u56e0\u8bbe\u5907\u548c\u901a\u4fe1\u80fd\u529b\u7684\u5dee\u5f02\u6027\u5bfc\u81f4\u805a\u5408\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0b\u964d\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u805a\u7c7b\u7b56\u7565\u548c\u8d44\u6e90\u4f18\u5316\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5148\u9a8c\u77e5\u8bc6\u76f8\u4f3c\u6027\u7684\u805a\u7c7b\u7b56\u7565\u5206\u7ec4\u8bbe\u5907\uff0c\u5e76\u63d0\u51faCAMU\u7b56\u7565\u52a8\u6001\u8c03\u6574\u672c\u5730\u66f4\u65b0\u9891\u7387\u548c\u4f20\u8f93\u529f\u7387\uff0c\u4f18\u5316\u8ba1\u7b97\u4e0e\u901a\u4fe1\u8d44\u6e90\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86FL\u5728\u5f02\u6784\u73af\u5883\u4e2d\u7684\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u5b9e\u73b0\u4e86\u901a\u4fe1\u6210\u672c\u4e0e\u8ba1\u7b97\u8d1f\u8f7d\u7684\u66f4\u597d\u5e73\u8861\u3002", "conclusion": "\u805a\u7c7b\u548cCAMU\u7b56\u7565\u7684\u7ed3\u5408\u80fd\u591f\u663e\u8457\u6539\u5584FL\u7684\u805a\u5408\u6548\u7387\u548c\u6536\u655b\u6027\u80fd\uff0c\u4e3a\u5f02\u6784\u73af\u5883\u4e2d\u7684\u5206\u5e03\u5f0f\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.06507", "pdf": "https://arxiv.org/pdf/2505.06507", "abs": "https://arxiv.org/abs/2505.06507", "authors": ["Haoyang Xie", "Feng Ju"], "title": "Text-to-CadQuery: A New Paradigm for CAD Generation with Scalable Large Model Capabilities", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Computer-aided design (CAD) is fundamental to modern engineering and\nmanufacturing, but creating CAD models still requires expert knowledge and\nspecialized software. Recent advances in large language models (LLMs) open up\nthe possibility of generative CAD, where natural language is directly\ntranslated into parametric 3D models. However, most existing methods generate\ntask-specific command sequences that pretrained models cannot directly handle.\nThese sequences must be converted into CAD representations such as CAD vectors\nbefore a 3D model can be produced, which requires training models from scratch\nand adds unnecessary complexity. To tackle this issue, we propose generating\nCadQuery code directly from text, leveraging the strengths of pretrained LLMs\nto produce 3D models without intermediate representations, using this\nPython-based scripting language. Since LLMs already excel at Python generation\nand spatial reasoning, fine-tuning them on Text-to-CadQuery data proves highly\neffective. Given that these capabilities typically improve with scale, we\nhypothesize that larger models will perform better after fine-tuning. To enable\nthis, we augment the Text2CAD dataset with 170,000 CadQuery annotations. We\nfine-tune six open-source LLMs of varying sizes and observe consistent\nimprovements. Our best model achieves a top-1 exact match of 69.3%, up from\n58.8%, and reduces Chamfer Distance by 48.6%. Project page:\nhttps://github.com/Text-to-CadQuery/Text-to-CadQuery.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u76f4\u63a5\u751f\u6210CadQuery\u4ee3\u7801\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u4e3a3D\u6a21\u578b\uff0c\u907f\u514d\u4e86\u4e2d\u95f4\u8868\u793a\u7684\u590d\u6742\u6027\u3002\u901a\u8fc7\u5fae\u8c03LLMs\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u6700\u4f73\u6a21\u578b\u51c6\u786e\u7387\u8fbe69.3%\u3002", "motivation": "\u5f53\u524dCAD\u6a21\u578b\u521b\u5efa\u4f9d\u8d56\u4e13\u4e1a\u77e5\u8bc6\u548c\u8f6f\u4ef6\uff0c\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u7684\u4efb\u52a1\u7279\u5b9a\u547d\u4ee4\u5e8f\u5217\u9700\u8f6c\u6362\u4e3aCAD\u8868\u793a\uff0c\u589e\u52a0\u4e86\u590d\u6742\u6027\u3002\u76f4\u63a5\u751f\u6210CadQuery\u4ee3\u7801\u53ef\u7b80\u5316\u6d41\u7a0b\u5e76\u5229\u7528LLMs\u7684\u73b0\u6709\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u76f4\u63a5\u751f\u6210CadQuery\u4ee3\u7801\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3LLMs\u5fae\u8c03\u5904\u7406Text-to-CadQuery\u6570\u636e\u3002\u901a\u8fc7\u6269\u5145\u6570\u636e\u96c6\u5e76\u5fae\u8c03\u4e0d\u540c\u89c4\u6a21\u7684LLMs\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\u3002", "result": "\u6700\u4f73\u6a21\u578b\u8fbe\u523069.3%\u7684top-1\u51c6\u786e\u7387\uff08\u539f58.8%\uff09\uff0cChamfer Distance\u964d\u4f4e48.6%\u3002\u89c4\u6a21\u66f4\u5927\u7684\u6a21\u578b\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u76f4\u63a5\u751f\u6210CadQuery\u4ee3\u7801\u7684\u65b9\u6cd5\u6709\u6548\u7b80\u5316\u4e86CAD\u6a21\u578b\u751f\u6210\u6d41\u7a0b\uff0c\u5229\u7528LLMs\u7684\u9884\u8bad\u7ec3\u80fd\u529b\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5c24\u5176\u5728\u89c4\u6a21\u66f4\u5927\u7684\u6a21\u578b\u4e2d\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2505.06607", "pdf": "https://arxiv.org/pdf/2505.06607", "abs": "https://arxiv.org/abs/2505.06607", "authors": ["Min Li", "Chun Yuan"], "title": "Boosting Neural Language Inference via Cascaded Interactive Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Natural Language Inference (NLI) focuses on ascertaining the logical\nrelationship (entailment, contradiction, or neutral) between a given premise\nand hypothesis. This task presents significant challenges due to inherent\nlinguistic features such as diverse phrasing, semantic complexity, and\ncontextual nuances. While Pre-trained Language Models (PLMs) built upon the\nTransformer architecture have yielded substantial advancements in NLI,\nprevailing methods predominantly utilize representations from the terminal\nlayer. This reliance on final-layer outputs may overlook valuable information\nencoded in intermediate layers, potentially limiting the capacity to model\nintricate semantic interactions effectively. Addressing this gap, we introduce\nthe Cascaded Interactive Reasoning Network (CIRN), a novel architecture\ndesigned for deeper semantic comprehension in NLI. CIRN implements a\nhierarchical feature extraction strategy across multiple network depths,\noperating within an interactive space where cross-sentence information is\ncontinuously integrated. This mechanism aims to mimic a process of progressive\nreasoning, transitioning from surface-level feature matching to uncovering more\nprofound logical and semantic connections between the premise and hypothesis.\nBy systematically mining latent semantic relationships at various\nrepresentational levels, CIRN facilitates a more thorough understanding of the\ninput pair. Comprehensive evaluations conducted on several standard NLI\nbenchmark datasets reveal consistent performance gains achieved by CIRN over\ncompetitive baseline approaches, demonstrating the efficacy of leveraging\nmulti-level interactive features for complex relational reasoning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCascaded Interactive Reasoning Network (CIRN)\u7684\u65b0\u67b6\u6784\uff0c\u7528\u4e8e\u63d0\u5347\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff08NLI\uff09\u4efb\u52a1\u4e2d\u7684\u6df1\u5ea6\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002\u901a\u8fc7\u5206\u5c42\u7279\u5f81\u63d0\u53d6\u548c\u8de8\u53e5\u5b50\u4fe1\u606f\u4ea4\u4e92\uff0cCIRN\u5728\u591a\u4e2a\u6807\u51c6NLI\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684NLI\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLMs\uff09\u7684\u6700\u540e\u4e00\u5c42\u8868\u793a\uff0c\u8fd9\u53ef\u80fd\u4f1a\u5ffd\u7565\u4e2d\u95f4\u5c42\u7684\u5b9d\u8d35\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u590d\u6742\u8bed\u4e49\u5173\u7cfb\u7684\u5efa\u6a21\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86CIRN\u4ee5\u66f4\u5168\u9762\u5730\u6316\u6398\u8bed\u4e49\u4fe1\u606f\u3002", "method": "CIRN\u91c7\u7528\u5206\u5c42\u7279\u5f81\u63d0\u53d6\u7b56\u7565\uff0c\u5728\u591a\u4e2a\u7f51\u7edc\u6df1\u5ea6\u4e2d\u8fde\u7eed\u6574\u5408\u8de8\u53e5\u5b50\u4fe1\u606f\uff0c\u6a21\u62df\u4ece\u8868\u5c42\u7279\u5f81\u5339\u914d\u5230\u6df1\u5c42\u903b\u8f91\u548c\u8bed\u4e49\u8fde\u63a5\u7684\u6e10\u8fdb\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u6807\u51c6NLI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCIRN\u901a\u8fc7\u5229\u7528\u591a\u5c42\u6b21\u4ea4\u4e92\u7279\u5f81\uff0c\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u7ade\u4e89\u6027\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CIRN\u901a\u8fc7\u5206\u5c42\u548c\u4ea4\u4e92\u5f0f\u7279\u5f81\u63d0\u53d6\uff0c\u6709\u6548\u63d0\u5347\u4e86NLI\u4efb\u52a1\u4e2d\u5bf9\u590d\u6742\u8bed\u4e49\u5173\u7cfb\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u591a\u5c42\u7ea7\u4fe1\u606f\u6574\u5408\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.06269", "pdf": "https://arxiv.org/pdf/2505.06269", "abs": "https://arxiv.org/abs/2505.06269", "authors": ["Chenguang Zhou", "Lei Chen", "Xiaohui Zhong", "Bo Lu", "Hao Li", "Libo Wu", "Jie Wu", "Jiahui Hu", "Zesheng Dou", "Pang-Chi Hsu", "Xiaoye Zhang"], "title": "A machine learning model for skillful climate system prediction", "categories": ["cs.LG"], "comment": null, "summary": "Climate system models (CSMs), through integrating cross-sphere interactions\namong the atmosphere, ocean, land, and cryosphere, have emerged as pivotal\ntools for deciphering climate dynamics and improving forecasting capabilities.\nRecent breakthroughs in artificial intelligence (AI)-driven meteorological\nmodeling have demonstrated remarkable success in single-sphere systems and\npartially spheres coupled systems. However, the development of a fully coupled\nAI-based climate system model encompassing atmosphere-ocean-land-sea ice\ninteractions has remained an unresolved challenge. This paper introduces\nFengShun-CSM, an AI-based CSM model that provides 60-day global daily forecasts\nfor 29 critical variables across atmospheric, oceanic, terrestrial, and\ncryospheric domains. The model significantly outperforms the European Centre\nfor Medium-Range Weather Forecasts (ECMWF) subseasonal-to-seasonal (S2S) model\nin predicting most variables, particularly precipitation, land surface, and\noceanic components. This enhanced capability is primarily attributed to its\nimproved representation of intra-seasonal variability modes, most notably the\nMadden-Julian Oscillation (MJO). Remarkably, FengShun-CSM exhibits substantial\npotential in predicting subseasonal extreme events. Such breakthroughs will\nadvance its applications in meteorological disaster mitigation, marine\necosystem conservation, and agricultural productivity enhancement. Furthermore,\nit validates the feasibility of developing AI-powered CSMs through machine\nlearning technologies, establishing a transformative paradigm for\nnext-generation Earth system modeling.", "AI": {"tldr": "FengShun-CSM\u662f\u4e00\u79cd\u57fa\u4e8eAI\u7684\u6c14\u5019\u7cfb\u7edf\u6a21\u578b\uff0c\u80fd\u591f\u5b9e\u73b060\u5929\u5168\u7403\u6bcf\u65e5\u9884\u6d4b\uff0c\u6db5\u76d629\u4e2a\u5173\u952e\u53d8\u91cf\uff0c\u5e76\u5728\u591a\u6570\u9884\u6d4b\u4e2d\u4f18\u4e8eECMWF\u7684S2S\u6a21\u578b\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u5b8c\u5168\u8026\u5408\u7684AI\u6c14\u5019\u7cfb\u7edf\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u6a21\u578b\u5728\u591a\u5708\u5c42\u8026\u5408\u9884\u6d4b\u4e2d\u7684\u4e0d\u8db3\u3002", "method": "\u5229\u7528AI\u6280\u672f\u6784\u5efaFengShun-CSM\u6a21\u578b\uff0c\u6574\u5408\u5927\u6c14\u3001\u6d77\u6d0b\u3001\u9646\u5730\u548c\u6d77\u51b0\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "FengShun-CSM\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u5b63\u8282\u5185\u53d8\u5f02\u6a21\u5f0f\uff08\u5982MJO\uff09\u548c\u6781\u7aef\u4e8b\u4ef6\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "FengShun-CSM\u4e3a\u4e0b\u4e00\u4ee3\u5730\u7403\u7cfb\u7edf\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5e76\u5728\u707e\u5bb3\u7f13\u89e3\u548c\u751f\u6001\u4fdd\u62a4\u7b49\u9886\u57df\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.06518", "pdf": "https://arxiv.org/pdf/2505.06518", "abs": "https://arxiv.org/abs/2505.06518", "authors": ["Larry Preuett III"], "title": "A Point-Based Algorithm for Distributional Reinforcement Learning in Partially Observable Domains", "categories": ["cs.AI"], "comment": null, "summary": "In many real-world planning tasks, agents must tackle uncertainty about the\nenvironment's state and variability in the outcomes of any chosen policy. We\naddress both forms of uncertainty as a first step toward safer algorithms in\npartially observable settings. Specifically, we extend Distributional\nReinforcement Learning (DistRL)-which models the entire return distribution for\nfully observable domains-to Partially Observable Markov Decision Processes\n(POMDPs), allowing an agent to learn the distribution of returns for each\nconditional plan. Concretely, we introduce new distributional Bellman operators\nfor partial observability and prove their convergence under the supremum\np-Wasserstein metric. We also propose a finite representation of these return\ndistributions via psi-vectors, generalizing the classical alpha-vectors in\nPOMDP solvers. Building on this, we develop Distributional Point-Based Value\nIteration (DPBVI), which integrates psi-vectors into a standard point-based\nbackup procedure-bridging DistRL and POMDP planning. By tracking return\ndistributions, DPBVI naturally enables risk-sensitive control in domains where\nrare, high-impact events must be carefully managed. We provide source code to\nfoster further research in robust decision-making under partial observability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\uff08POMDPs\uff09\u4e2d\u7684\u6269\u5c55\u65b9\u6cd5\uff0c\u5f15\u5165\u4e86\u65b0\u7684\u5206\u5e03\u8d1d\u5c14\u66fc\u7b97\u5b50\u548c\u03c8-\u5411\u91cf\u8868\u793a\uff0c\u5f00\u53d1\u4e86DPBVI\u7b97\u6cd5\u4ee5\u652f\u6301\u98ce\u9669\u654f\u611f\u63a7\u5236\u3002", "motivation": "\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u4e2d\uff0c\u667a\u80fd\u4f53\u9762\u4e34\u72b6\u6001\u4e0d\u786e\u5b9a\u6027\u548c\u7b56\u7565\u7ed3\u679c\u591a\u53d8\u7684\u53cc\u91cd\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u8bba\u6587\u65e8\u5728\u5f00\u53d1\u66f4\u5b89\u5168\u7684\u7b97\u6cd5\uff0c\u5904\u7406\u8fd9\u4e9b\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u7a33\u5065\u7684\u51b3\u7b56\u5236\u5b9a\u3002", "method": "\u8bba\u6587\u6269\u5c55\u4e86\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\uff08DistRL\uff09\u5230\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDPs\uff09\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u5e03\u8d1d\u5c14\u66fc\u7b97\u5b50\u5e76\u8bc1\u660e\u5176\u6536\u655b\u6027\u3002\u540c\u65f6\u5f15\u5165\u4e86\u03c8-\u5411\u91cf\u4f5c\u4e3a\u8fd4\u56de\u5206\u5e03\u7684\u6709\u9650\u8868\u793a\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u5206\u5e03\u70b9\u57fa\u503c\u8fed\u4ee3\uff08DPBVI\uff09\u7b97\u6cd5\uff0c\u7ed3\u5408\u70b9\u57fa\u5907\u4efd\u6d41\u7a0b\uff0c\u5b9e\u73b0\u98ce\u9669\u654f\u611f\u63a7\u5236\u3002", "result": "\u8bba\u6587\u8bc1\u660e\u4e86\u63d0\u51fa\u7684\u5206\u5e03\u8d1d\u5c14\u66fc\u7b97\u5b50\u5728p-\u74e6\u745f\u65af\u5766\u5ea6\u91cf\u4e0b\u7684\u6536\u655b\u6027\uff0c\u5e76\u901a\u8fc7DPBVI\u7b97\u6cd5\u5b9e\u73b0\u4e86\u5728\u9700\u8981\u7ba1\u7406\u7f55\u89c1\u9ad8\u5f71\u54cd\u4e8b\u4ef6\u7684\u9886\u57df\u4e2d\u66f4\u6709\u6548\u7684\u98ce\u9669\u654f\u611f\u51b3\u7b56\u5236\u5b9a\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5f00\u6e90\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u66f4\u5e7f\u6cdb\u7684\u7814\u7a76\u5e94\u7528\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u6269\u5c55\u5230\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\uff0c\u8bba\u6587\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u969c\u548c\u5b9e\u7528\u7b97\u6cd5\uff0c\u8fd8\u4e3a\u7a33\u5065\u51b3\u7b56\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u98ce\u9669\u7ba1\u7406\u7684\u5173\u952e\u9886\u57df\u4e2d\u3002"}}
{"id": "2505.06624", "pdf": "https://arxiv.org/pdf/2505.06624", "abs": "https://arxiv.org/abs/2505.06624", "authors": ["Arezoo Hatefi", "Xuan-Son Vu", "Monowar Bhuyan", "Frank Drewes"], "title": "The Efficiency of Pre-training with Objective Masking in Pseudo Labeling for Semi-Supervised Text Classification", "categories": ["cs.CL"], "comment": null, "summary": "We extend and study a semi-supervised model for text classification proposed\nearlier by Hatefi et al. for classification tasks in which document classes are\ndescribed by a small number of gold-labeled examples, while the majority of\ntraining examples is unlabeled. The model leverages the teacher-student\narchitecture of Meta Pseudo Labels in which a ''teacher'' generates labels for\noriginally unlabeled training data to train the ''student'' and updates its own\nmodel iteratively based on the performance of the student on the gold-labeled\nportion of the data. We extend the original model of Hatefi et al. by an\nunsupervised pre-training phase based on objective masking, and conduct\nin-depth performance evaluations of the original model, our extension, and\nvarious independent baselines. Experiments are performed using three different\ndatasets in two different languages (English and Swedish).", "AI": {"tldr": "\u8bba\u6587\u6269\u5c55\u4e86Hatefi\u7b49\u4eba\u7684\u534a\u76d1\u7763\u6587\u672c\u5206\u7c7b\u6a21\u578b\uff0c\u52a0\u5165\u4e86\u57fa\u4e8e\u76ee\u6807\u63a9\u7801\u7684\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u5e76\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u8bed\u8a00\u4e0a\u8fdb\u884c\u4e86\u6027\u80fd\u8bc4\u4f30\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u6539\u8fdb\u534a\u76d1\u7763\u6587\u672c\u5206\u7c7b\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u9ec4\u91d1\u6807\u7b7e\u6837\u672c\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u7ed3\u5408\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u6269\u5c55\u4e86Meta Pseudo Labels\u7684\u5e08\u751f\u6846\u67b6\uff0c\u52a0\u5165\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u5e76\u901a\u8fc7\u4e09\u79cd\u6570\u636e\u96c6\u548c\u4e24\u79cd\u8bed\u8a00\u8bc4\u4f30\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u5c55\u793a\u4e86\u539f\u59cb\u6a21\u578b\u3001\u6269\u5c55\u6a21\u578b\u53ca\u591a\u4e2a\u57fa\u51c6\u6a21\u578b\u7684\u6027\u80fd\u5bf9\u6bd4\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u52a0\u5165\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u9636\u6bb5\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u534a\u76d1\u7763\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2505.06270", "pdf": "https://arxiv.org/pdf/2505.06270", "abs": "https://arxiv.org/abs/2505.06270", "authors": ["Seongmin Kim", "Kwanho Kim", "Minseung Kim", "Kanghyun Jo"], "title": "Importance Analysis for Dynamic Control of Balancing Parameter in a Simple Knowledge Distillation Setting", "categories": ["cs.LG", "cs.AI"], "comment": "3 pages, 2 figures, conference preprint for IWIS2025", "summary": "Although deep learning models owe their remarkable success to deep and\ncomplex architectures, this very complexity typically comes at the expense of\nreal-time performance. To address this issue, a variety of model compression\ntechniques have been proposed, among which knowledge distillation (KD) stands\nout for its strong empirical performance. The KD contains two concurrent\nprocesses: (i) matching the outputs of a large, pre-trained teacher network and\na lightweight student network, and (ii) training the student to solve its\ndesignated downstream task. The associated loss functions are termed the\ndistillation loss and the downsteam-task loss, respectively. Numerous prior\nstudies report that KD is most effective when the influence of the distillation\nloss outweighs that of the downstream-task loss. The influence(or importance)\nis typically regulated by a balancing parameter. This paper provides a\nmathematical rationale showing that in a simple KD setting when the loss is\ndecreasing, the balancing parameter should be dynamically adjusted", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u5728\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u8fc7\u7a0b\u4e2d\u52a8\u6001\u8c03\u6574\u5e73\u8861\u53c2\u6570\u7684\u6570\u5b66\u4f9d\u636e\uff0c\u4ee5\u4f18\u5316\u8f7b\u91cf\u7ea7\u5b66\u751f\u7f51\u7edc\u7684\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u7684\u590d\u6742\u6027\u5e38\u5bfc\u81f4\u5b9e\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u77e5\u8bc6\u84b8\u998f\u867d\u80fd\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4e2d\u5e73\u8861\u53c2\u6570\u7684\u9759\u6001\u8c03\u6574\u53ef\u80fd\u9650\u5236\u5176\u6548\u679c\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u52a8\u6001\u8c03\u6574\u7684\u7406\u8bba\u652f\u6301\u3002", "method": "\u901a\u8fc7\u5206\u6790\u77e5\u8bc6\u84b8\u998f\u7684\u4e24\u4e2a\u635f\u5931\u51fd\u6570\uff08\u84b8\u998f\u635f\u5931\u548c\u4e0b\u6e38\u4efb\u52a1\u635f\u5931\uff09\uff0c\u5728\u7b80\u5355KD\u8bbe\u5b9a\u4e0b\u63a8\u5bfc\u51fa\u635f\u5931\u4e0b\u964d\u65f6\u5e94\u52a8\u6001\u8c03\u8282\u5e73\u8861\u53c2\u6570\u7684\u6570\u5b66\u7406\u8bba\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u52a8\u6001\u8c03\u6574\u5e73\u8861\u53c2\u6570\u80fd\u66f4\u6709\u6548\u5730\u63d0\u5347\u5b66\u751f\u7f51\u7edc\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u84b8\u998f\u635f\u5931\u5360\u4e3b\u5bfc\u65f6\u3002", "conclusion": "\u52a8\u6001\u8c03\u6574\u7b56\u7565\u4e3a\u77e5\u8bc6\u84b8\u998f\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u4f18\u5316\u8def\u5f84\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u590d\u6742\u573a\u666f\u9a8c\u8bc1\u5176\u666e\u9002\u6027\u3002"}}
{"id": "2505.06535", "pdf": "https://arxiv.org/pdf/2505.06535", "abs": "https://arxiv.org/abs/2505.06535", "authors": ["Anindya Sarkar", "Binglin Ji", "Yevgeniy Vorobeychik"], "title": "Online Feedback Efficient Active Target Discovery in Partially Observable Environments", "categories": ["cs.AI", "cs.LG", "stat.ML"], "comment": "30 pages, 28 figures, Pre-print", "summary": "In various scientific and engineering domains, where data acquisition is\ncostly, such as in medical imaging, environmental monitoring, or remote\nsensing, strategic sampling from unobserved regions, guided by prior\nobservations, is essential to maximize target discovery within a limited\nsampling budget. In this work, we introduce Diffusion-guided Active Target\nDiscovery (DiffATD), a novel method that leverages diffusion dynamics for\nactive target discovery. DiffATD maintains a belief distribution over each\nunobserved state in the environment, using this distribution to dynamically\nbalance exploration-exploitation. Exploration reduces uncertainty by sampling\nregions with the highest expected entropy, while exploitation targets areas\nwith the highest likelihood of discovering the target, indicated by the belief\ndistribution and an incrementally trained reward model designed to learn the\ncharacteristics of the target. DiffATD enables efficient target discovery in a\npartially observable environment within a fixed sampling budget, all without\nrelying on any prior supervised training. Furthermore, DiffATD offers\ninterpretability, unlike existing black-box policies that require extensive\nsupervised training. Through extensive experiments and ablation studies across\ndiverse domains, including medical imaging and remote sensing, we show that\nDiffATD performs significantly better than baselines and competitively with\nsupervised methods that operate under full environmental observability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDiffATD\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u52a8\u529b\u5b66\u8fdb\u884c\u4e3b\u52a8\u76ee\u6807\u53d1\u73b0\uff0c\u52a8\u6001\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u65e0\u9700\u76d1\u7763\u8bad\u7ec3\u5373\u53ef\u5728\u6709\u9650\u91c7\u6837\u9884\u7b97\u4e0b\u9ad8\u6548\u53d1\u73b0\u76ee\u6807\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5728\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u7684\u9886\u57df\uff08\u5982\u533b\u7597\u5f71\u50cf\u3001\u73af\u5883\u76d1\u6d4b\uff09\uff0c\u5982\u4f55\u5728\u6709\u9650\u91c7\u6837\u9884\u7b97\u4e0b\u6700\u5927\u5316\u76ee\u6807\u53d1\u73b0\u6548\u7387\u662f\u5173\u952e\u95ee\u9898\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u76d1\u7763\u8bad\u7ec3\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u76d1\u7763\u3001\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u65b0\u65b9\u6cd5\u3002", "method": "DiffATD\u901a\u8fc7\u6269\u6563\u52a8\u529b\u5b66\u7ef4\u62a4\u672a\u89c2\u6d4b\u72b6\u6001\u7684\u7f6e\u4fe1\u5206\u5e03\uff0c\u52a8\u6001\u5e73\u8861\u63a2\u7d22\uff08\u9009\u62e9\u9ad8\u4e0d\u786e\u5b9a\u6027\u533a\u57df\uff09\u4e0e\u5229\u7528\uff08\u9009\u62e9\u9ad8\u76ee\u6807\u6982\u7387\u533a\u57df\uff09\uff0c\u5e76\u7ed3\u5408\u589e\u91cf\u8bad\u7ec3\u7684\u5956\u52b1\u6a21\u578b\u5b66\u4e60\u76ee\u6807\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDiffATD\u5728\u533b\u7597\u5f71\u50cf\u548c\u9065\u611f\u7b49\u591a\u4e2a\u9886\u57df\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u4e0e\u5b8c\u5168\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u7684\u76d1\u7763\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "DiffATD\u662f\u4e00\u79cd\u65e0\u9700\u76d1\u7763\u3001\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u4e3b\u52a8\u76ee\u6807\u53d1\u73b0\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\uff0c\u4e3a\u9ad8\u6210\u672c\u6570\u636e\u91c7\u96c6\u9886\u57df\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.06630", "pdf": "https://arxiv.org/pdf/2505.06630", "abs": "https://arxiv.org/abs/2505.06630", "authors": ["Chunyi Yue", "Ang Li"], "title": "Dynamic Domain Information Modulation Algorithm for Multi-domain Sentiment Analysis", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "17 pages, 5 figures, 3 tables", "summary": "Multi-domain sentiment classification aims to mitigate poor performance\nmodels due to the scarcity of labeled data in a single domain, by utilizing\ndata labeled from various domains. A series of models that jointly train domain\nclassifiers and sentiment classifiers have demonstrated their advantages,\nbecause domain classification helps generate necessary information for\nsentiment classification. Intuitively, the importance of sentiment\nclassification tasks is the same in all domains for multi-domain sentiment\nclassification; but domain classification tasks are different because the\nimpact of domain information on sentiment classification varies across\ndifferent fields; this can be controlled through adjustable weights or hyper\nparameters. However, as the number of domains increases, existing\nhyperparameter optimization algorithms may face the following challenges: (1)\ntremendous demand for computing resources, (2) convergence problems, and (3)\nhigh algorithm complexity. To efficiently generate the domain information\nrequired for sentiment classification in each domain, we propose a dynamic\ninformation modulation algorithm. Specifically, the model training process is\ndivided into two stages. In the first stage, a shared hyperparameter, which\nwould control the proportion of domain classification tasks across all fields,\nis determined. In the second stage, we introduce a novel domain-aware\nmodulation algorithm to adjust the domain information contained in the input\ntext, which is then calculated based on a gradient-based and loss-based method.\nIn summary, experimental results on a public sentiment analysis dataset\ncontaining 16 domains prove the superiority of the proposed method.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u4fe1\u606f\u8c03\u5236\u7b97\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u751f\u6210\u591a\u9886\u57df\u60c5\u611f\u5206\u7c7b\u6240\u9700\u7684\u9886\u57df\u4fe1\u606f\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u548c\u68af\u5ea6/\u635f\u5931\u8c03\u6574\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u8ba1\u7b97\u8d44\u6e90\u3001\u6536\u655b\u548c\u590d\u6742\u5ea6\u95ee\u9898\u3002", "motivation": "\u591a\u9886\u57df\u60c5\u611f\u5206\u7c7b\u4e2d\uff0c\u73b0\u6709\u8d85\u53c2\u6570\u4f18\u5316\u7b97\u6cd5\u5728\u9886\u57df\u6570\u91cf\u589e\u52a0\u65f6\u9762\u4e34\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u5927\u3001\u6536\u655b\u95ee\u9898\u548c\u7b97\u6cd5\u590d\u6742\u5ea6\u9ad8\u7684\u6311\u6218\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u751f\u6210\u9886\u57df\u4fe1\u606f\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u7b2c\u4e00\u9636\u6bb5\u786e\u5b9a\u5168\u5c40\u5171\u4eab\u7684\u8d85\u53c2\u6570\u63a7\u5236\u9886\u57df\u5206\u7c7b\u4efb\u52a1\u6bd4\u4f8b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u68af\u5ea6/\u635f\u5931\u65b9\u6cd5\u7684\u9886\u57df\u611f\u77e5\u8c03\u5236\u7b97\u6cd5\u52a8\u6001\u8c03\u6574\u8f93\u5165\u6587\u672c\u4e2d\u7684\u9886\u57df\u4fe1\u606f\u3002", "result": "\u5728\u5305\u542b16\u4e2a\u9886\u57df\u7684\u516c\u5f00\u60c5\u611f\u5206\u6790\u6570\u636e\u96c6\u4e0a\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u52a8\u6001\u4fe1\u606f\u8c03\u5236\u7b97\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u591a\u9886\u57df\u60c5\u611f\u5206\u7c7b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2505.06271", "pdf": "https://arxiv.org/pdf/2505.06271", "abs": "https://arxiv.org/abs/2505.06271", "authors": ["June-Woo Kim", "Sanghoon Lee", "Miika Toikkanen", "Daehwan Hwang", "Kyunghoon Kim"], "title": "Tri-MTL: A Triple Multitask Learning Approach for Respiratory Disease Diagnosis", "categories": ["cs.LG", "cs.AI", "cs.SD"], "comment": "Accepted to EMBC 2025", "summary": "Auscultation remains a cornerstone of clinical practice, essential for both\ninitial evaluation and continuous monitoring. Clinicians listen to the lung\nsounds and make a diagnosis by combining the patient's medical history and test\nresults. Given this strong association, multitask learning (MTL) can offer a\ncompelling framework to simultaneously model these relationships, integrating\nrespiratory sound patterns with disease manifestations. While MTL has shown\nconsiderable promise in medical applications, a significant research gap\nremains in understanding the complex interplay between respiratory sounds,\ndisease manifestations, and patient metadata attributes. This study\ninvestigates how integrating MTL with cutting-edge deep learning architectures\ncan enhance both respiratory sound classification and disease diagnosis.\nSpecifically, we extend recent findings regarding the beneficial impact of\nmetadata on respiratory sound classification by evaluating its effectiveness\nwithin an MTL framework. Our comprehensive experiments reveal significant\nimprovements in both lung sound classification and diagnostic performance when\nthe stethoscope information is incorporated into the MTL architecture.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u591a\u4efb\u52a1\u5b66\u4e60\uff08MTL\uff09\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u5728\u547c\u5438\u97f3\u5206\u7c7b\u548c\u75be\u75c5\u8bca\u65ad\u4e2d\u7684\u6548\u679c\uff0c\u5c24\u5176\u5173\u6ce8\u4e86\u5143\u6570\u636e\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u7ed3\u5408\u542c\u8bca\u5668\u4fe1\u606f\u7684MTL\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u548c\u8bca\u65ad\u6027\u80fd\u3002", "motivation": "\u4e34\u5e8a\u542c\u8bca\u662f\u8bc4\u4f30\u548c\u76d1\u6d4b\u60a3\u8005\u7684\u91cd\u8981\u624b\u6bb5\uff0c\u4f46\u547c\u5438\u97f3\u3001\u75be\u75c5\u8868\u73b0\u548c\u60a3\u8005\u5143\u6570\u636e\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7MTL\u548c\u6df1\u5ea6\u5b66\u4e60\u6574\u5408\u8fd9\u4e9b\u4fe1\u606f\uff0c\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u547c\u5438\u97f3\u6a21\u5f0f\u3001\u75be\u75c5\u8868\u73b0\u548c\u60a3\u8005\u5143\u6570\u636e\uff0c\u5e76\u91c7\u7528\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u8fdb\u884c\u5efa\u6a21\u548c\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c06\u542c\u8bca\u5668\u4fe1\u606f\u6574\u5408\u5230MTL\u67b6\u6784\u4e2d\uff0c\u663e\u8457\u6539\u5584\u4e86\u547c\u5438\u97f3\u5206\u7c7b\u548c\u75be\u75c5\u8bca\u65ad\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u5b9e\u4e86MTL\u7ed3\u5408\u5143\u6570\u636e\u5728\u533b\u7597\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u547c\u5438\u97f3\u5206\u6790\u548c\u75be\u75c5\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.06580", "pdf": "https://arxiv.org/pdf/2505.06580", "abs": "https://arxiv.org/abs/2505.06580", "authors": ["Dongyoon Yang", "Jihu Lee", "Yongdai Kim"], "title": "TAROT: Towards Essentially Domain-Invariant Robustness with Theoretical Justification", "categories": ["cs.AI", "stat.ML"], "comment": "Accepted in CVPR 2025 (19 pages, 7 figures)", "summary": "Robust domain adaptation against adversarial attacks is a critical research\narea that aims to develop models capable of maintaining consistent performance\nacross diverse and challenging domains. In this paper, we derive a new\ngeneralization bound for robust risk on the target domain using a novel\ndivergence measure specifically designed for robust domain adaptation. Building\nupon this, we propose a new algorithm named TAROT, which is designed to enhance\nboth domain adaptability and robustness. Through extensive experiments, TAROT\nnot only surpasses state-of-the-art methods in accuracy and robustness but also\nsignificantly enhances domain generalization and scalability by effectively\nlearning domain-invariant features. In particular, TAROT achieves superior\nperformance on the challenging DomainNet dataset, demonstrating its ability to\nlearn domain-invariant representations that generalize well across different\ndomains, including unseen ones. These results highlight the broader\napplicability of our approach in real-world domain adaptation scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9c81\u68d2\u57df\u9002\u5e94\u7b97\u6cd5TAROT\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u6563\u5ea6\u5ea6\u91cf\u548c\u6cdb\u5316\u8fb9\u754c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df\u6027\u80fd\u548c\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u5e76\u5728DomainNet\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u9488\u5bf9\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5bf9\u6297\u653b\u51fb\u548c\u57df\u5dee\u5f02\u6311\u6218\uff0c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u8de8\u57df\u6027\u80fd\u53c8\u5177\u5907\u9c81\u68d2\u6027\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u65b0\u6563\u5ea6\u5ea6\u91cf\u7684\u9c81\u68d2\u98ce\u9669\u6cdb\u5316\u8fb9\u754c\uff0c\u5e76\u8bbe\u8ba1\u7b97\u6cd5TAROT\uff0c\u4e13\u6ce8\u4e8e\u5b66\u4e60\u57df\u4e0d\u53d8\u7279\u5f81\u4ee5\u589e\u5f3a\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "TAROT\u5728DomainNet\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u51c6\u786e\u7387\u3001\u9c81\u68d2\u6027\u53ca\u57df\u6cdb\u5316\u80fd\u529b\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "TAROT\u901a\u8fc7\u6709\u6548\u7684\u57df\u4e0d\u53d8\u7279\u5f81\u5b66\u4e60\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u57df\u9002\u5e94\u573a\u666f\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.06633", "pdf": "https://arxiv.org/pdf/2505.06633", "abs": "https://arxiv.org/abs/2505.06633", "authors": ["Isaac Gerber"], "title": "Attention Is Not All You Need: The Importance of Feedforward Networks in Transformer Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Decoder-only transformer networks have become incredibly popular for language\nmodeling tasks. State-of-the-art models can have over a hundred transformer\nblocks, containing billions of trainable parameters, and are trained on\ntrillions of tokens of text. Each transformer block typically consists of a\nmulti-head attention (MHA) mechanism and a two-layer fully connected\nfeedforward network (FFN). In this paper, we examine the importance of the FFN\nduring the model pre-training process through a series of experiments,\nconfirming that the FFN is important to model performance. Furthermore, we show\nthat models using a transformer block configuration with three-layer FFNs with\nfewer such blocks outperform the standard two-layer configuration delivering\nlower training loss with fewer total parameters in less time.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86FFN\u5728\u89e3\u7801\u5668-\u751f\u6210\u5f0fTransformer\u9884\u8bad\u7ec3\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u53d1\u73b0\u4e09\u5c42FFN\u7ed3\u6784\u6bd4\u6807\u51c6\u4e24\u5c42FFN\u8868\u73b0\u66f4\u597d\uff0c\u8bad\u7ec3\u635f\u5931\u66f4\u4f4e\uff0c\u53c2\u6570\u91cf\u66f4\u5c11\u4e14\u8bad\u7ec3\u66f4\u5feb\u3002", "motivation": "\u63a2\u7d22FFN\u5728\u9884\u8bad\u7ec3\u4e2d\u7684\u4f5c\u7528\u53ca\u5176\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u6807\u51c6\u4e24\u5c42FFN\u4e0e\u4e09\u5c42FFN\u5728\u4e0d\u540cTransformer\u5757\u914d\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u4e09\u5c42FFN\u7ed3\u6784\u5728\u51cf\u5c11\u53c2\u6570\u91cf\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u8bad\u7ec3\u635f\u5931\u548c\u66f4\u5feb\u7684\u8bad\u7ec3\u901f\u5ea6\u3002", "conclusion": "FFN\u5728\u9884\u8bad\u7ec3\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4e09\u5c42FFN\u914d\u7f6e\u4f18\u4e8e\u6807\u51c6\u4e24\u5c42\u7ed3\u6784\u3002"}}
{"id": "2505.06272", "pdf": "https://arxiv.org/pdf/2505.06272", "abs": "https://arxiv.org/abs/2505.06272", "authors": ["Junzhou Xu", "Boyu Diao"], "title": "A Sensitivity-Driven Expert Allocation Method in LoRA-MoE for Efficient Fine-Tuning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As deep learning models expand, the pre-training-fine-tuning paradigm has\nbecome the standard approach for handling various downstream tasks. However,\nshared parameters can lead to diminished performance when dealing with complex\ndatasets involving multiple tasks. While introducing Mixture-of-Experts (MoE)\nmethods has alleviated this issue to some extent, it also significantly\nincreases the number of parameters required for fine-tuning and training time,\nintroducing greater parameter redundancy. To address these challenges, we\npropose a method for allocating expert numbers based on parameter sensitivity\nLoRA-SMoE (A Sensitivity-Driven Expert Allocation Method in LoRA-MoE for\nEfficient Fine-Tuning). This method rapidly assesses the sensitivity of\ndifferent tasks to parameters by sampling a small amount of data and using\ngradient information. It then adaptively allocates expert numbers within a\ngiven budget. The process maintains comparable memory consumption to LoRA\n(Low-Rank Adaptation) while ensuring an efficient and resource-friendly\nfine-tuning procedure. Experimental results demonstrate that compared to SOTA\nfine-tuning methods, our LoRA-SMoE approach can enhance model performance while\nreducing the number of trainable parameters. This significantly improves model\nperformance in resource-constrained environments. Additionally, due to its\nefficient parameter sensitivity evaluation mechanism, LoRA-SMoE requires\nminimal computational overhead to optimize expert allocation, making it\nparticularly suitable for scenarios with limited computational resources. All\nthe code in this study will be made publicly available following the acceptance\nof the paper for publication. Source code is at\nhttps://github.com/EMLS-ICTCAS/LoRA-SMoE", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53c2\u6570\u654f\u611f\u6027\u7684\u4e13\u5bb6\u5206\u914d\u65b9\u6cd5\uff08LoRA-SMoE\uff09\uff0c\u901a\u8fc7\u91c7\u6837\u5c11\u91cf\u6570\u636e\u5e76\u5229\u7528\u68af\u5ea6\u4fe1\u606f\u5feb\u901f\u8bc4\u4f30\u4efb\u52a1\u5bf9\u53c2\u6570\u7684\u654f\u611f\u6027\uff0c\u81ea\u9002\u5e94\u5206\u914d\u4e13\u5bb6\u6570\u91cf\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u4e0eLoRA\u76f8\u5f53\u5185\u5b58\u6d88\u8017\u7684\u540c\u65f6\uff0c\u51cf\u5c11\u4e86\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6269\u5c55\uff0c\u9884\u8bad\u7ec3-\u5fae\u8c03\u8303\u5f0f\u6210\u4e3a\u5904\u7406\u4e0b\u6e38\u4efb\u52a1\u7684\u6807\u51c6\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u5171\u4eab\u53c2\u6570\u5728\u590d\u6742\u591a\u4efb\u52a1\u6570\u636e\u96c6\u4e0a\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u867d\u7136MoE\u65b9\u6cd5\u90e8\u5206\u7f13\u89e3\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u4e5f\u663e\u8457\u589e\u52a0\u4e86\u5fae\u8c03\u53c2\u6570\u548c\u8bad\u7ec3\u65f6\u95f4\uff0c\u5f15\u5165\u66f4\u5927\u53c2\u6570\u5197\u4f59\u3002", "method": "\u63d0\u51faLoRA-SMoE\u65b9\u6cd5\uff0c\u901a\u8fc7\u91c7\u6837\u5c11\u91cf\u6570\u636e\u5e76\u5229\u7528\u68af\u5ea6\u4fe1\u606f\u5feb\u901f\u8bc4\u4f30\u4efb\u52a1\u5bf9\u53c2\u6570\u7684\u654f\u611f\u6027\uff0c\u81ea\u9002\u5e94\u5206\u914d\u4e13\u5bb6\u6570\u91cf\uff0c\u4fdd\u6301\u5185\u5b58\u6d88\u8017\u4e0eLoRA\u76f8\u5f53\uff0c\u540c\u65f6\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4SOTA\u5fae\u8c03\u65b9\u6cd5\uff0cLoRA-SMoE\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\uff0c\u8d44\u6e90\u6d88\u8017\u4f4e\uff0c\u9002\u5408\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u573a\u666f\u3002", "conclusion": "LoRA-SMoE\u901a\u8fc7\u9ad8\u6548\u53c2\u6570\u654f\u611f\u6027\u8bc4\u4f30\u673a\u5236\u4f18\u5316\u4e13\u5bb6\u5206\u914d\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6848\u3002\u6240\u6709\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2505.06637", "pdf": "https://arxiv.org/pdf/2505.06637", "abs": "https://arxiv.org/abs/2505.06637", "authors": ["Chi Xu", "Yili Jin", "Sami Ma", "Rongsheng Qian", "Hao Fang", "Jiangchuan Liu", "Xue Liu", "Edith C. H. Ngai", "William I. Atlas", "Katrina M. Connors", "Mark A. Spoljaric"], "title": "Exploring Multimodal Foundation AI and Expert-in-the-Loop for Sustainable Management of Wild Salmon Fisheries in Indigenous Rivers", "categories": ["cs.AI"], "comment": "10 pages, accepted by IJCAI 2025, AI and Social Good Track", "summary": "Wild salmon are essential to the ecological, economic, and cultural\nsustainability of the North Pacific Rim. Yet climate variability, habitat loss,\nand data limitations in remote ecosystems that lack basic infrastructure\nsupport pose significant challenges to effective fisheries management. This\nproject explores the integration of multimodal foundation AI and\nexpert-in-the-loop frameworks to enhance wild salmon monitoring and sustainable\nfisheries management in Indigenous rivers across Pacific Northwest. By\nleveraging video and sonar-based monitoring, we develop AI-powered tools for\nautomated species identification, counting, and length measurement, reducing\nmanual effort, expediting delivery of results, and improving decision-making\naccuracy. Expert validation and active learning frameworks ensure ecological\nrelevance while reducing annotation burdens. To address unique technical and\nsocietal challenges, we bring together a cross-domain, interdisciplinary team\nof university researchers, fisheries biologists, Indigenous stewardship\npractitioners, government agencies, and conservation organizations. Through\nthese collaborations, our research fosters ethical AI co-development, open data\nsharing, and culturally informed fisheries management.", "AI": {"tldr": "\u6458\u8981\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u591a\u6a21\u6001\u57fa\u7840AI\u548c\u4e13\u5bb6\u53c2\u4e0e\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u592a\u5e73\u6d0b\u897f\u5317\u90e8\u7684\u571f\u8457\u6cb3\u6d41\u4e2d\u6574\u5408\u89c6\u9891\u548c\u58f0\u7eb3\u76d1\u6d4b\u6280\u672f\uff0c\u5f00\u53d1\u81ea\u52a8\u5316\u5de5\u5177\u6765\u63d0\u5347\u91ce\u751f\u9c91\u9c7c\u76d1\u6d4b\u548c\u53ef\u6301\u7eed\u6e14\u4e1a\u7ba1\u7406\u7684\u6548\u7387\u3002", "motivation": "\u91ce\u751f\u9c91\u9c7c\u5bf9\u5317\u592a\u5e73\u6d0b\u6cbf\u5cb8\u7684\u751f\u6001\u3001\u7ecf\u6d4e\u548c\u6587\u5316\u53ef\u6301\u7eed\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6c14\u5019\u53d8\u5f02\u6027\u3001\u6816\u606f\u5730\u4e27\u5931\u4ee5\u53ca\u504f\u8fdc\u5730\u533a\u57fa\u7840\u8bbe\u65bd\u4e0d\u8db3\u5e26\u6765\u7684\u6570\u636e\u9650\u5236\uff0c\u5bf9\u6e14\u4e1a\u7ba1\u7406\u6784\u6210\u4e86\u5de8\u5927\u6311\u6218\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u89c6\u9891\u548c\u58f0\u7eb3\u76d1\u6d4b\u6280\u672f\uff0c\u5f00\u53d1AI\u5de5\u5177\u5b9e\u73b0\u81ea\u52a8\u7269\u79cd\u8bc6\u522b\u3001\u8ba1\u6570\u548c\u4f53\u957f\u6d4b\u91cf\uff0c\u540c\u65f6\u5f15\u5165\u4e13\u5bb6\u9a8c\u8bc1\u548c\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\u4ee5\u786e\u4fdd\u751f\u6001\u76f8\u5173\u6027\u548c\u51cf\u8f7b\u6807\u6ce8\u8d1f\u62c5\u3002", "result": "\u8be5\u9879\u76ee\u901a\u8fc7\u8de8\u9886\u57df\u5408\u4f5c\uff0c\u4fc3\u8fdb\u4e86\u4f26\u7406AI\u5171\u540c\u5f00\u53d1\u3001\u5f00\u653e\u6570\u636e\u5171\u4eab\u548c\u6587\u5316\u654f\u611f\u7684\u6e14\u4e1a\u7ba1\u7406\uff0c\u63d0\u9ad8\u4e86\u51b3\u7b56\u51c6\u786e\u6027\u5e76\u52a0\u5feb\u4e86\u7ed3\u679c\u4ea4\u4ed8\u901f\u5ea6\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528AI\u6280\u672f\u548c\u8de8\u5b66\u79d1\u5408\u4f5c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u91ce\u751f\u9c91\u9c7c\u76d1\u6d4b\u548c\u7ba1\u7406\u4e2d\u7684\u6280\u672f\u4e0e\u793e\u4f1a\u6311\u6218\uff0c\u4e3a\u53ef\u6301\u7eed\u6e14\u4e1a\u7ba1\u7406\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2505.06660", "pdf": "https://arxiv.org/pdf/2505.06660", "abs": "https://arxiv.org/abs/2505.06660", "authors": ["Junyi Peng", "Takanori Ashihara", "Marc Delcroix", "Tsubasa Ochiai", "Oldrich Plchot", "Shoko Araki", "Jan \u010cernock\u00fd"], "title": "TS-SUPERB: A Target Speech Processing Benchmark for Speech Self-Supervised Learning Models", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at ICASSP 2025", "summary": "Self-supervised learning (SSL) models have significantly advanced speech\nprocessing tasks, and several benchmarks have been proposed to validate their\neffectiveness. However, previous benchmarks have primarily focused on\nsingle-speaker scenarios, with less exploration of target-speaker tasks in\nnoisy, multi-talker conditions -- a more challenging yet practical case. In\nthis paper, we introduce the Target-Speaker Speech Processing Universal\nPerformance Benchmark (TS-SUPERB), which includes four widely recognized\ntarget-speaker processing tasks that require identifying the target speaker and\nextracting information from the speech mixture. In our benchmark, the speaker\nembedding extracted from enrollment speech is used as a clue to condition\ndownstream models. The benchmark result reveals the importance of evaluating\nSSL models in target speaker scenarios, demonstrating that performance cannot\nbe easily inferred from related single-speaker tasks. Moreover, by using a\nunified SSL-based target speech encoder, consisting of a speaker encoder and an\nextractor module, we also investigate joint optimization across TS tasks to\nleverage mutual information and demonstrate its effectiveness.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86TS-SUPERB\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30SSL\u6a21\u578b\u5728\u591a\u8bf4\u8bdd\u4eba\u5608\u6742\u73af\u5883\u4e2d\u7684\u76ee\u6807\u8bf4\u8bdd\u4eba\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u9a8c\u8bc1\u4e86\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524dSSL\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u9488\u5bf9\u5355\u8bf4\u8bdd\u4eba\u573a\u666f\uff0c\u800c\u5bf9\u5608\u6742\u591a\u8bf4\u8bdd\u4eba\u73af\u5883\u4e2d\u7684\u76ee\u6807\u8bf4\u8bdd\u4eba\u4efb\u52a1\u7814\u7a76\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86TS-SUPERB\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u56db\u9879\u76ee\u6807\u8bf4\u8bdd\u4eba\u5904\u7406\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u57fa\u4e8eSSL\u7684\u7edf\u4e00\u76ee\u6807\u8bed\u97f3\u7f16\u7801\u5668\uff08\u5305\u542b\u8bf4\u8bdd\u4eba\u7f16\u7801\u5668\u548c\u63d0\u53d6\u6a21\u5757\uff09\uff0c\u7814\u7a76\u8de8\u4efb\u52a1\u8054\u5408\u4f18\u5316\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u76ee\u6807\u8bf4\u8bdd\u4eba\u4efb\u52a1\u7684\u6027\u80fd\u65e0\u6cd5\u7b80\u5355\u4ece\u5355\u8bf4\u8bdd\u4eba\u4efb\u52a1\u63a8\u65ad\uff0c\u4e14\u8054\u5408\u4f18\u5316\u80fd\u6709\u6548\u5229\u7528\u4efb\u52a1\u95f4\u7684\u4e92\u4fe1\u606f\u63d0\u5347\u8868\u73b0\u3002", "conclusion": "TS-SUPERB\u7a81\u51fa\u4e86\u5728\u76ee\u6807\u8bf4\u8bdd\u4eba\u573a\u666f\u4e2d\u8bc4\u4f30SSL\u6a21\u578b\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u3002"}}
{"id": "2505.06273", "pdf": "https://arxiv.org/pdf/2505.06273", "abs": "https://arxiv.org/abs/2505.06273", "authors": ["Taehyun Cho", "Seokhun Ju", "Seungyub Han", "Dohyeong Kim", "Kyungjae Lee", "Jungwoo Lee"], "title": "Policy-labeled Preference Learning: Is Preference Enough for RLHF?", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "To design rewards that align with human goals, Reinforcement Learning from\nHuman Feedback (RLHF) has emerged as a prominent technique for learning reward\nfunctions from human preferences and optimizing policies via reinforcement\nlearning algorithms. However, existing RLHF methods often misinterpret\ntrajectories as being generated by an optimal policy, causing inaccurate\nlikelihood estimation and suboptimal learning. Inspired by Direct Preference\nOptimization framework which directly learns optimal policy without explicit\nreward, we propose policy-labeled preference learning (PPL), to resolve\nlikelihood mismatch issues by modeling human preferences with regret, which\nreflects behavior policy information. We also provide a contrastive KL\nregularization, derived from regret-based principles, to enhance RLHF in\nsequential decision making. Experiments in high-dimensional continuous control\ntasks demonstrate PPL's significant improvements in offline RLHF performance\nand its effectiveness in online settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3apolicy-labeled preference learning (PPL)\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u4eba\u7c7b\u504f\u597d\u548c\u540e\u6094\u884c\u4e3a\u6765\u89e3\u51b3\u73b0\u6709RLHF\u65b9\u6cd5\u4e2d\u7684\u4f3c\u7136\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u79bb\u7ebfRLHF\u6027\u80fd\u548c\u5728\u5728\u7ebf\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684RLHF\u65b9\u6cd5\u5e38\u5e38\u8bef\u5c06\u8f68\u8ff9\u89c6\u4e3a\u7531\u6700\u4f18\u7b56\u7565\u751f\u6210\uff0c\u5bfc\u81f4\u4f3c\u7136\u4f30\u8ba1\u4e0d\u51c6\u786e\u548c\u5b66\u4e60\u6548\u679c\u4e0d\u4f73\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u66f4\u51c6\u786e\u5730\u5efa\u6a21\u4eba\u7c7b\u504f\u597d\u5e76\u4f18\u5316\u7b56\u7565\u3002", "method": "\u53d7Direct Preference Optimization\uff08\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff09\u6846\u67b6\u7684\u542f\u53d1\uff0c\u63d0\u51fa\u4e86policy-labeled preference learning (PPL)\uff0c\u901a\u8fc7\u540e\u6094\u884c\u4e3a\u6765\u5efa\u6a21\u4eba\u7c7b\u504f\u597d\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u5bf9\u6bd4KL\u6b63\u5219\u5316\u65b9\u6cd5\u4ee5\u589e\u5f3aRLHF\u5728\u5e8f\u5217\u51b3\u7b56\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5728\u9ad8\u7ef4\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPPL\u5728\u79bb\u7ebfRLHF\u6027\u80fd\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u5e76\u4e14\u5728\u5728\u7ebf\u73af\u5883\u4e2d\u4e5f\u8868\u73b0\u51fa\u4e86\u6709\u6548\u6027\u3002", "conclusion": "PPL\u901a\u8fc7\u5f15\u5165\u540e\u6094\u884c\u4e3a\u5efa\u6a21\u548c\u5bf9\u6bd4KL\u6b63\u5219\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709RLHF\u65b9\u6cd5\u4e2d\u7684\u95ee\u9898\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.06680", "pdf": "https://arxiv.org/pdf/2505.06680", "abs": "https://arxiv.org/abs/2505.06680", "authors": ["Linxuan Huang", "Dong-Fan Xie", "Li Li", "Zhengbing He"], "title": "A Survey on Data-Driven Modeling of Human Drivers' Lane-Changing Decisions", "categories": ["cs.AI", "cs.HC", "cs.LG", "cs.SY", "eess.SY", "physics.soc-ph"], "comment": null, "summary": "Lane-changing (LC) behavior, a critical yet complex driving maneuver,\nsignificantly influences driving safety and traffic dynamics. Traditional\nanalytical LC decision (LCD) models, while effective in specific environments,\noften oversimplify behavioral heterogeneity and complex interactions, limiting\ntheir capacity to capture real LCD. Data-driven approaches address these gaps\nby leveraging rich empirical data and machine learning to decode latent\ndecision-making patterns, enabling adaptive LCD modeling in dynamic\nenvironments. In light of the rapid development of artificial intelligence and\nthe demand for data-driven models oriented towards connected vehicles and\nautonomous vehicles, this paper presents a comprehensive survey of data-driven\nLCD models, with a particular focus on human drivers LC decision-making. It\nsystematically reviews the modeling framework, covering data sources and\npreprocessing, model inputs and outputs, objectives, structures, and validation\nmethods. This survey further discusses the opportunities and challenges faced\nby data-driven LCD models, including driving safety, uncertainty, as well as\nthe integration and improvement of technical frameworks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u6570\u636e\u9a71\u52a8\u7684\u8f66\u9053\u53d8\u6362\u51b3\u7b56\u6a21\u578b\uff0c\u91cd\u70b9\u5173\u6ce8\u4eba\u7c7b\u9a7e\u9a76\u5458\u7684\u884c\u4e3a\uff0c\u7cfb\u7edf\u56de\u987e\u4e86\u5efa\u6a21\u6846\u67b6\u3001\u6570\u636e\u6765\u6e90\u3001\u6a21\u578b\u7ed3\u6784\u53ca\u5176\u9762\u4e34\u7684\u6311\u6218\u4e0e\u673a\u9047\u3002", "motivation": "\u4f20\u7edf\u8f66\u9053\u53d8\u6362\u51b3\u7b56\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fc7\u4e8e\u7b80\u5316\uff0c\u96be\u4ee5\u6355\u6349\u771f\u5b9e\u884c\u4e3a\u3002\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5229\u7528\u673a\u5668\u5b66\u4e60\u548c\u4e30\u5bcc\u6570\u636e\uff0c\u80fd\u591f\u66f4\u597d\u5730\u9002\u5e94\u52a8\u6001\u73af\u5883\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8054\u7f51\u8f66\u8f86\u548c\u81ea\u52a8\u9a7e\u9a76\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u5206\u6790\u6570\u636e\u9a71\u52a8\u8f66\u9053\u53d8\u6362\u51b3\u7b56\u6a21\u578b\u7684\u6846\u67b6\uff0c\u5305\u62ec\u6570\u636e\u6765\u6e90\u4e0e\u9884\u5904\u7406\u3001\u6a21\u578b\u8f93\u5165\u8f93\u51fa\u3001\u76ee\u6807\u3001\u7ed3\u6784\u548c\u9a8c\u8bc1\u65b9\u6cd5\u3002", "result": "\u6587\u7ae0\u603b\u7ed3\u4e86\u6570\u636e\u9a71\u52a8\u6a21\u578b\u5728\u89e3\u7801\u9a7e\u9a76\u5458\u51b3\u7b56\u6a21\u5f0f\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u5b89\u5168\u6027\u3001\u4e0d\u786e\u5b9a\u6027\u53ca\u6280\u672f\u6846\u67b6\u6574\u5408\u7684\u6311\u6218\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u7684\u8f66\u9053\u53d8\u6362\u51b3\u7b56\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u89e3\u51b3\u5b89\u5168\u6027\u548c\u6280\u672f\u6574\u5408\u95ee\u9898\u3002"}}
{"id": "2505.06696", "pdf": "https://arxiv.org/pdf/2505.06696", "abs": "https://arxiv.org/abs/2505.06696", "authors": ["Dominik Koterwa", "Maciej \u015awita\u0142a"], "title": "Enhancing BERTopic with Intermediate Layer Representations", "categories": ["cs.CL"], "comment": "Repository with code for reproduction:\n  https://github.com/dkoterwa/optimizing_bertopic", "summary": "BERTopic is a topic modeling algorithm that leverages transformer-based\nembeddings to create dense clusters, enabling the estimation of topic\nstructures and the extraction of valuable insights from a corpus of documents.\nThis approach allows users to efficiently process large-scale text data and\ngain meaningful insights into its structure. While BERTopic is a powerful tool,\nembedding preparation can vary, including extracting representations from\nintermediate model layers and applying transformations to these embeddings. In\nthis study, we evaluate 18 different embedding representations and present\nfindings based on experiments conducted on three diverse datasets. To assess\nthe algorithm's performance, we report topic coherence and topic diversity\nmetrics across all experiments. Our results demonstrate that, for each dataset,\nit is possible to find an embedding configuration that performs better than the\ndefault setting of BERTopic. Additionally, we investigate the influence of stop\nwords on different embedding configurations.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86BERTopic\u7b97\u6cd5\u4e2d18\u79cd\u4e0d\u540c\u5d4c\u5165\u8868\u793a\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u9488\u5bf9\u4e0d\u540c\u6570\u636e\u96c6\u53ef\u4ee5\u627e\u5230\u4f18\u4e8e\u9ed8\u8ba4\u8bbe\u7f6e\u7684\u914d\u7f6e\uff0c\u5e76\u63a2\u8ba8\u4e86\u505c\u7528\u8bcd\u5bf9\u5d4c\u5165\u914d\u7f6e\u7684\u5f71\u54cd\u3002", "motivation": "\u8bc4\u4f30BERTopic\u4e2d\u4e0d\u540c\u5d4c\u5165\u8868\u793a\u7684\u6027\u80fd\uff0c\u4ee5\u4f18\u5316\u5176\u5728\u591a\u6837\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u752818\u79cd\u5d4c\u5165\u8868\u793a\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e3b\u9898\u4e00\u81f4\u6027\u548c\u591a\u6837\u6027\u3002", "result": "\u6bcf\u79cd\u6570\u636e\u96c6\u90fd\u80fd\u627e\u5230\u4f18\u4e8e\u9ed8\u8ba4\u8bbe\u7f6e\u7684\u5d4c\u5165\u914d\u7f6e\uff0c\u505c\u7528\u8bcd\u5bf9\u5d4c\u5165\u914d\u7f6e\u7684\u5f71\u54cd\u663e\u8457\u3002", "conclusion": "\u5d4c\u5165\u914d\u7f6e\u7684\u9009\u62e9\u5bf9BERTopic\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4e14\u5e94\u8003\u8651\u6570\u636e\u96c6\u7279\u6027\u548c\u505c\u7528\u8bcd\u5f71\u54cd\u3002"}}
{"id": "2505.06274", "pdf": "https://arxiv.org/pdf/2505.06274", "abs": "https://arxiv.org/abs/2505.06274", "authors": ["Baijiong Lin", "Weisen Jiang", "Yuancheng Xu", "Hao Chen", "Ying-Cong Chen"], "title": "PARM: Multi-Objective Test-Time Alignment via Preference-Aware Autoregressive Reward Model", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by ICML 2025", "summary": "Multi-objective test-time alignment aims to adapt large language models\n(LLMs) to diverse multi-dimensional user preferences during inference while\nkeeping LLMs frozen. Recently, GenARM (Xu et al., 2025) first independently\ntrains Autoregressive Reward Models (ARMs) for each preference dimension\nwithout awareness of each other, then combines their outputs based on\nuser-specific preference vectors during inference to achieve multi-objective\ntest-time alignment, leading to two key limitations: the need for\n\\textit{multiple} ARMs increases the inference cost, and the separate training\nof ARMs causes the misalignment between the guided generation and the user\npreferences. To address these issues, we propose Preference-aware ARM (PARM), a\nsingle unified ARM trained across all preference dimensions. PARM uses our\nproposed Preference-Aware Bilinear Low-Rank Adaptation (PBLoRA), which employs\na bilinear form to condition the ARM on preference vectors, enabling it to\nachieve precise control over preference trade-offs during inference.\nExperiments demonstrate that PARM reduces inference costs and achieves better\nalignment with preference vectors compared with existing methods. Additionally,\nPARM enables weak-to-strong guidance, allowing a smaller PARM to guide a larger\nfrozen LLM without expensive training, making multi-objective alignment\naccessible with limited computing resources. The code is available at\nhttps://github.com/Baijiong-Lin/PARM.", "AI": {"tldr": "PARM\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u504f\u597d\u611f\u77e5\u81ea\u56de\u5f52\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7PBLoRA\u6280\u672f\u5b9e\u73b0\u591a\u76ee\u6807\u6d4b\u8bd5\u65f6\u5bf9\u9f50\uff0c\u964d\u4f4e\u4e86\u63a8\u7406\u6210\u672c\u5e76\u6539\u5584\u4e86\u504f\u597d\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5GenARM\u9700\u8981\u591a\u4e2a\u72ec\u7acb\u8bad\u7ec3\u7684ARMs\u5bfc\u81f4\u7684\u63a8\u7406\u6210\u672c\u9ad8\u548c\u504f\u597d\u5bf9\u9f50\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faPreference-aware ARM (PARM)\u548cPreference-Aware Bilinear Low-Rank Adaptation (PBLoRA)\uff0c\u901a\u8fc7\u5355\u4e00\u6a21\u578b\u548c\u53cc\u7ebf\u6027\u5f62\u5f0f\u5b9e\u73b0\u591a\u504f\u597d\u7ef4\u5ea6\u7684\u7edf\u4e00\u8bad\u7ec3\u548c\u63a7\u5236\u3002", "result": "PARM\u5728\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u4e86\u66f4\u4f4e\u7684\u63a8\u7406\u6210\u672c\u548c\u66f4\u597d\u7684\u504f\u597d\u5bf9\u9f50\u6548\u679c\uff0c\u540c\u65f6\u652f\u6301\u5f31\u5230\u5f3a\u7684\u5f15\u5bfc\u3002", "conclusion": "PARM\u4e3a\u591a\u76ee\u6807\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7ecf\u6d4e\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u3002"}}
{"id": "2505.06706", "pdf": "https://arxiv.org/pdf/2505.06706", "abs": "https://arxiv.org/abs/2505.06706", "authors": ["Yuxuan Zheng", "Yihe Zhou", "Feiyang Xu", "Mingli Song", "Shunyu Liu"], "title": "Bi-level Mean Field: Dynamic Grouping for Large-Scale MARL", "categories": ["cs.AI"], "comment": null, "summary": "Large-scale Multi-Agent Reinforcement Learning (MARL) often suffers from the\ncurse of dimensionality, as the exponential growth in agent interactions\nsignificantly increases computational complexity and impedes learning\nefficiency. To mitigate this, existing efforts that rely on Mean Field (MF)\nsimplify the interaction landscape by approximating neighboring agents as a\nsingle mean agent, thus reducing overall complexity to pairwise interactions.\nHowever, these MF methods inevitably fail to account for individual\ndifferences, leading to aggregation noise caused by inaccurate iterative\nupdates during MF learning. In this paper, we propose a Bi-level Mean Field\n(BMF) method to capture agent diversity with dynamic grouping in large-scale\nMARL, which can alleviate aggregation noise via bi-level interaction.\nSpecifically, BMF introduces a dynamic group assignment module, which employs a\nVariational AutoEncoder (VAE) to learn the representations of agents,\nfacilitating their dynamic grouping over time. Furthermore, we propose a\nbi-level interaction module to model both inter- and intra-group interactions\nfor effective neighboring aggregation. Experiments across various tasks\ndemonstrate that the proposed BMF yields results superior to the\nstate-of-the-art methods. Our code will be made publicly available.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u5c42\u6b21\u5747\u503c\u573a\uff08BMF\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u7ec4\u6355\u6349\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u591a\u6837\u6027\uff0c\u51cf\u5c11\u805a\u5408\u566a\u58f0\uff0c\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7ef4\u5ea6\u707e\u96be\u95ee\u9898\uff0c\u4f20\u7edf\u5747\u503c\u573a\u65b9\u6cd5\u56e0\u5ffd\u7565\u4e2a\u4f53\u5dee\u5f02\u5bfc\u81f4\u805a\u5408\u566a\u58f0\u3002", "method": "\u5f15\u5165\u52a8\u6001\u5206\u7ec4\u6a21\u5757\uff08\u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff09\u548c\u53cc\u5c42\u6b21\u4ea4\u4e92\u6a21\u5757\uff0c\u5206\u522b\u5efa\u6a21\u7ec4\u95f4\u548c\u7ec4\u5185\u4ea4\u4e92\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "BMF\u80fd\u6709\u6548\u7f13\u89e3\u805a\u5408\u566a\u58f0\u5e76\u63d0\u5347\u6027\u80fd\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2505.06698", "pdf": "https://arxiv.org/pdf/2505.06698", "abs": "https://arxiv.org/abs/2505.06698", "authors": ["Zongqi Wang", "Tianle Gu", "Chen Gong", "Xin Tian", "Siqi Bao", "Yujiu Yang"], "title": "From Rankings to Insights: Evaluation Should Shift Focus from Leaderboard to Feedback", "categories": ["cs.CL"], "comment": null, "summary": "Automatic evaluation benchmarks such as MT-Bench, Arena-Hard, and Auto-Arena\nare seeing growing adoption for the evaluation of Large Language Models (LLMs).\nExisting research has primarily focused on approximating human-based model\nrankings using limited data and LLM-as-a-Judge. However, the fundamental\npremise of these studies, which attempts to replicate human rankings, is\nflawed. Specifically, these benchmarks typically offer only overall scores,\nlimiting their utility to leaderboard rankings, rather than providing feedback\nthat can guide model optimization and support model profiling. Therefore, we\nadvocate for an evaluation paradigm shift from approximating human-based model\nrankings to providing feedback with analytical value. To this end, we introduce\nFeedbacker, an evaluation framework that provides comprehensive and\nfine-grained results, thereby enabling thorough identification of a model's\nspecific strengths and weaknesses. Such feedback not only supports the targeted\noptimization of the model but also enhances the understanding of its behavior.\nFeedbacker comprises three key components: an extensible tree-based query\ntaxonomy builder, an automated query synthesis scheme, and a suite of\nvisualization and analysis tools. Furthermore, we propose a novel\nLLM-as-a-Judge method: PC2 (Pre-Comparison-derived Criteria) pointwise\nevaluation. This method derives evaluation criteria by pre-comparing the\ndifferences between several auxiliary responses, achieving the accuracy of\npairwise evaluation while maintaining the time complexity of pointwise\nevaluation. Finally, leveraging the evaluation results of 17 mainstream LLMs,\nwe demonstrate the usage of Feedbacker and highlight its effectiveness and\npotential. Our homepage project is available at\nhttps://liudan193.github.io/Feedbacker.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aFeedbacker\u7684\u65b0\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u65e8\u5728\u4ece\u5355\u7eaf\u6a21\u4eff\u4eba\u7c7b\u6a21\u578b\u6392\u540d\u8f6c\u5411\u63d0\u4f9b\u6709\u5206\u6790\u4ef7\u503c\u7684\u53cd\u9988\uff0c\u4ee5\u5e2e\u52a9\u6a21\u578b\u4f18\u5316\u548c\u6027\u80fd\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u8bc4\u4f30\u57fa\u51c6\uff08\u5982MT-Bench\u3001Arena-Hard\u7b49\uff09\u4ec5\u63d0\u4f9b\u603b\u4f53\u5206\u6570\uff0c\u65e0\u6cd5\u4e3a\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u5177\u4f53\u53cd\u9988\u3002", "method": "Feedbacker\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u53ef\u6269\u5c55\u7684\u6811\u72b6\u67e5\u8be2\u5206\u7c7b\u6784\u5efa\u5668\u3001\u81ea\u52a8\u5316\u67e5\u8be2\u5408\u6210\u65b9\u6848\u3001\u53ef\u89c6\u5316\u5206\u6790\u5de5\u5177\uff0c\u5e76\u63d0\u51faPC2\u70b9\u5bf9\u70b9\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u8bc4\u4f3017\u4e2a\u4e3b\u6d41LLM\uff0c\u5c55\u793a\u4e86Feedbacker\u7684\u6709\u6548\u6027\u548c\u6f5c\u529b\u3002", "conclusion": "Feedbacker\u6846\u67b6\u80fd\u63d0\u4f9b\u7ec6\u81f4\u5168\u9762\u7684\u53cd\u9988\uff0c\u652f\u6301\u6a21\u578b\u4f18\u5316\u5e76\u589e\u5f3a\u5bf9\u5176\u884c\u4e3a\u7684\u7406\u89e3\u3002"}}
{"id": "2505.06275", "pdf": "https://arxiv.org/pdf/2505.06275", "abs": "https://arxiv.org/abs/2505.06275", "authors": ["Yuzhou Zhu", "Zheng Zhang", "Ruyi Zhang", "Liang Zhou"], "title": "Attonsecond Streaking Phase Retrieval Via Deep Learning Methods", "categories": ["cs.LG", "cs.AI", "cs.CV", "physics.optics"], "comment": null, "summary": "Attosecond streaking phase retrieval is essential for resolving electron\ndynamics on sub-femtosecond time scales yet traditional algorithms rely on\niterative minimization and central momentum approximations that degrade\naccuracy for broadband pulses. In this work phase retrieval is reformulated as\na supervised computer-vision problem and four neural architectures are\nsystematically compared. A convolutional network demonstrates strong\nsensitivity to local streak edges but lacks global context; a vision\ntransformer captures long-range delay-energy correlations at the expense of\nlocal inductive bias; a hybrid CNN-ViT model unites local feature extraction\nand full-graph attention; and a capsule network further enforces spatial pose\nagreement through dynamic routing. A theoretical analysis introduces local,\nglobal and positional sensitivity measures and derives surrogate error bounds\nthat predict the strict ordering $CNN<ViT<Hybrid<Capsule$. Controlled\nexperiments on synthetic streaking spectrograms confirm this hierarchy, with\nthe capsule network achieving the highest retrieval fidelity. Looking forward,\nembedding the strong-field integral into physics-informed neural networks and\nexploring photonic hardware implementations promise pathways toward real-time\nattosecond pulse characterization under demanding experimental conditions.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u76f8\u4f4d\u68c0\u7d22\u65b9\u6cd5\uff0c\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u76d1\u7763\u5f0f\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff0c\u5e76\u6bd4\u8f83\u4e86\u56db\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u6027\u80fd\u3002\u80f6\u56ca\u7f51\u7edc\u8868\u73b0\u6700\u4f73\uff0c\u4e3a\u5b9e\u65f6\u963f\u79d2\u8109\u51b2\u8868\u5f81\u63d0\u4f9b\u4e86\u6f5c\u5728\u9014\u5f84\u3002", "motivation": "\u4f20\u7edf\u76f8\u4f4d\u68c0\u7d22\u7b97\u6cd5\u4f9d\u8d56\u8fed\u4ee3\u6700\u5c0f\u5316\u548c\u4e2d\u5fc3\u52a8\u91cf\u8fd1\u4f3c\uff0c\u5bf9\u5bbd\u5e26\u8109\u51b2\u7684\u51c6\u786e\u6027\u6709\u9650\u3002\u56e0\u6b64\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u65b9\u6cd5\u6765\u89e3\u6790\u4e9a\u98de\u79d2\u65f6\u95f4\u5c3a\u5ea6\u7684\u7535\u5b50\u52a8\u529b\u5b66\u3002", "method": "\u8bba\u6587\u5c06\u76f8\u4f4d\u68c0\u7d22\u91cd\u65b0\u5b9a\u4e49\u4e3a\u76d1\u7763\u5f0f\u8ba1\u7b97\u673a\u89c6\u89c9\u95ee\u9898\uff0c\u5e76\u7cfb\u7edf\u6bd4\u8f83\u4e86\u56db\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff1a\u5377\u79ef\u7f51\u7edc\uff08CNN\uff09\u3001\u89c6\u89c9\u53d8\u6362\u5668\uff08ViT\uff09\u3001\u6df7\u5408CNN-ViT\u6a21\u578b\u4ee5\u53ca\u80f6\u56ca\u7f51\u7edc\uff0c\u5206\u522b\u8bc4\u4f30\u5176\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5408\u6210\u6761\u7eb9\u5149\u8c31\u5b9e\u9a8c\u8868\u660e\uff0c\u80f6\u56ca\u7f51\u7edc\u7684\u68c0\u7d22\u4fdd\u771f\u5ea6\u6700\u9ad8\uff0c\u6027\u80fd\u6392\u5e8f\u4e3aCNN < ViT < \u6df7\u5408\u6a21\u578b < \u80f6\u56ca\u7f51\u7edc\u3002", "conclusion": "\u80f6\u56ca\u7f51\u7edc\u5728\u76f8\u4f4d\u68c0\u7d22\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u5305\u62ec\u5c06\u5f3a\u573a\u79ef\u5206\u5d4c\u5165\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u548c\u63a2\u7d22\u5149\u5b50\u786c\u4ef6\u5b9e\u73b0\uff0c\u4ee5\u5b9e\u73b0\u590d\u6742\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u7684\u5b9e\u65f6\u963f\u79d2\u8109\u51b2\u8868\u5f81\u3002"}}
{"id": "2505.06769", "pdf": "https://arxiv.org/pdf/2505.06769", "abs": "https://arxiv.org/abs/2505.06769", "authors": ["Krishnendu Chatterjee", "Mahdi JafariRaviz", "Raimundo Saona", "Jakub Svoboda"], "title": "Value Iteration with Guessing for Markov Chains and Markov Decision Processes", "categories": ["cs.AI", "cs.CC"], "comment": "Appeared in the 31st International Conference on Tools and Algorithms\n  for the Construction and Analysis of Systems (TACAS 2025)", "summary": "Two standard models for probabilistic systems are Markov chains (MCs) and\nMarkov decision processes (MDPs). Classic objectives for such probabilistic\nmodels for control and planning problems are reachability and stochastic\nshortest path. The widely studied algorithmic approach for these problems is\nthe Value Iteration (VI) algorithm which iteratively applies local updates\ncalled Bellman updates. There are many practical approaches for VI in the\nliterature but they all require exponentially many Bellman updates for MCs in\nthe worst case. A preprocessing step is an algorithm that is discrete,\ngraph-theoretical, and requires linear space. An important open question is\nwhether, after a polynomial-time preprocessing, VI can be achieved with\nsub-exponentially many Bellman updates. In this work, we present a new approach\nfor VI based on guessing values. Our theoretical contributions are twofold.\nFirst, for MCs, we present an almost-linear-time preprocessing algorithm after\nwhich, along with guessing values, VI requires only subexponentially many\nBellman updates. Second, we present an improved analysis of the speed of\nconvergence of VI for MDPs. Finally, we present a practical algorithm for MDPs\nbased on our new approach. Experimental results show that our approach provides\na considerable improvement over existing VI-based approaches on several\nbenchmark examples from the literature.", "AI": {"tldr": "\u6539\u8fdb\u7684\u9884\u5904\u7406\u548c\u731c\u6d4b\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u72b6\u6001\u66f4\u65b0\u7684\u590d\u6742\u5ea6", "motivation": "\u89e3\u51b3\u7ecf\u5178\u7b97\u6cd5\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u9700\u8981\u6307\u6570\u7ea7\u66f4\u65b0\u7684\u95ee\u9898", "method": "\u5f15\u5165\u731c\u6d4b\u503c\u7684\u65b0\u65b9\u6cd5\u5e76\u4f18\u5316\u9884\u5904\u7406\u6b65\u9aa4", "result": "\u5b9e\u73b0\u8fd1\u4e4e\u7ebf\u6027\u7684\u9884\u5904\u7406\u65f6\u95f4\u4e0e\u4e9a\u6307\u6570\u7ea7\u72b6\u6001\u66f4\u65b0", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f"}}
{"id": "2505.06708", "pdf": "https://arxiv.org/pdf/2505.06708", "abs": "https://arxiv.org/abs/2505.06708", "authors": ["Zihan Qiu", "Zekun Wang", "Bo Zheng", "Zeyu Huang", "Kaiyue Wen", "Songlin Yang", "Rui Men", "Le Yu", "Fei Huang", "Suozhi Huang", "Dayiheng Liu", "Jingren Zhou", "Junyang Lin"], "title": "Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free", "categories": ["cs.CL"], "comment": null, "summary": "Gating mechanisms have been widely utilized, from early models like LSTMs and\nHighway Networks to recent state space models, linear attention, and also\nsoftmax attention. Yet, existing literature rarely examines the specific\neffects of gating. In this work, we conduct comprehensive experiments to\nsystematically investigate gating-augmented softmax attention variants.\nSpecifically, we perform a comprehensive comparison over 30 variants of 15B\nMixture-of-Experts (MoE) models and 1.7B dense models trained on a 3.5 trillion\ntoken dataset. Our central finding is that a simple modification-applying a\nhead-specific sigmoid gate after the Scaled Dot-Product Attention\n(SDPA)-consistently improves performance. This modification also enhances\ntraining stability, tolerates larger learning rates, and improves scaling\nproperties. By comparing various gating positions and computational variants,\nwe attribute this effectiveness to two key factors: (1) introducing\nnon-linearity upon the low-rank mapping in the softmax attention, and (2)\napplying query-dependent sparse gating scores to modulate the SDPA output.\nNotably, we find this sparse gating mechanism mitigates 'attention sink' and\nenhances long-context extrapolation performance, and we also release related\n$\\href{https://github.com/qiuzh20/gated_attention}{codes}$ and\n$\\href{https://huggingface.co/QwQZh/gated_attention}{models}$ to facilitate\nfuture research.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u7814\u7a76\u4e86\u8f6f\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u6dfb\u52a0\u95e8\u63a7\u7684\u6548\u679c\uff0c\u53d1\u73b0\u7b80\u5355\u7684\u5934\u90e8\u7279\u5f02\u6027sigmoid\u95e8\u63a7\u80fd\u63d0\u5347\u6027\u80fd\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u957f\u4e0a\u4e0b\u6587\u5916\u63a8\u80fd\u529b\u3002", "motivation": "\u63a2\u8ba8\u95e8\u63a7\u673a\u5236\u5728\u8f6f\u6ce8\u610f\u529b\u4e2d\u7684\u5177\u4f53\u6548\u679c\uff0c\u586b\u8865\u73b0\u6709\u6587\u732e\u4e2d\u7684\u7a7a\u767d\u3002", "method": "\u5bf9\u6bd430\u79cd15B\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u548c1.7B\u5bc6\u96c6\u6a21\u578b\u7684\u95e8\u63a7\u53d8\u4f53\uff0c\u91cd\u70b9\u5173\u6ce8\u5934\u90e8\u7279\u5f02\u6027sigmoid\u95e8\u63a7\u7684\u5e94\u7528\u3002", "result": "\u5934\u90e8\u7279\u5f02\u6027sigmoid\u95e8\u63a7\u663e\u8457\u63d0\u5347\u6027\u80fd\u3001\u7a33\u5b9a\u6027\u548c\u5b66\u4e60\u7387\u5bb9\u5fcd\u5ea6\uff0c\u5e76\u89e3\u51b3'\u6ce8\u610f\u529b\u4e0b\u6c89'\u95ee\u9898\u3002", "conclusion": "\u95e8\u63a7\u673a\u5236\u901a\u8fc7\u5f15\u5165\u975e\u7ebf\u6027\u548c\u7a00\u758f\u95e8\u63a7\u8bc4\u5206\u6709\u6548\u4f18\u5316\u8f6f\u6ce8\u610f\u529b\uff0c\u516c\u5f00\u4ee3\u7801\u548c\u6a21\u578b\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2505.06279", "pdf": "https://arxiv.org/pdf/2505.06279", "abs": "https://arxiv.org/abs/2505.06279", "authors": ["Shashwat Pandey"], "title": "Interpretable Learning Dynamics in Unsupervised Reinforcement Learning", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We present an interpretability framework for unsupervised reinforcement\nlearning (URL) agents, aimed at understanding how intrinsic motivation shapes\nattention, behavior, and representation learning. We analyze five agents DQN,\nRND, ICM, PPO, and a Transformer-RND variant trained on procedurally generated\nenvironments, using Grad-CAM, Layer-wise Relevance Propagation (LRP),\nexploration metrics, and latent space clustering. To capture how agents\nperceive and adapt over time, we introduce two metrics: attention diversity,\nwhich measures the spatial breadth of focus, and attention change rate, which\nquantifies temporal shifts in attention. Our findings show that\ncuriosity-driven agents display broader, more dynamic attention and exploratory\nbehavior than their extrinsically motivated counterparts. Among them,\nTransformerRND combines wide attention, high exploration coverage, and compact,\nstructured latent representations. Our results highlight the influence of\narchitectural inductive biases and training signals on internal agent dynamics.\nBeyond reward-centric evaluation, the proposed framework offers diagnostic\ntools to probe perception and abstraction in RL agents, enabling more\ninterpretable and generalizable behavior.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u65e0\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\uff08URL\uff09\u4ee3\u7406\u7684\u5185\u5728\u52a8\u673a\u5982\u4f55\u5f71\u54cd\u5176\u6ce8\u610f\u529b\u3001\u884c\u4e3a\u548c\u8868\u5f81\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u5206\u6790\u65b9\u6cd5\u6bd4\u8f83\u4e86\u4e94\u79cd\u4ee3\u7406\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u7406\u89e3\u65e0\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5982\u4f55\u901a\u8fc7\u5185\u5728\u52a8\u673a\uff08\u5982\u597d\u5947\u5fc3\uff09\u5851\u9020\u5176\u6ce8\u610f\u529b\u3001\u884c\u4e3a\u548c\u5b66\u4e60\u8868\u5f81\uff0c\u4ece\u800c\u63d0\u4f9b\u66f4\u53ef\u89e3\u91ca\u548c\u6cdb\u5316\u7684\u884c\u4e3a\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\u5bf9 DQN\u3001RND\u3001ICM\u3001PPO \u548c Transformer-RND \u4e94\u79cd\u4ee3\u7406\u5728\u7a0b\u5e8f\u751f\u6210\u73af\u5883\u4e2d\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u4f7f\u7528 Grad-CAM\u3001LRP\u3001\u63a2\u7d22\u5ea6\u91cf\u548c\u6f5c\u5728\u7a7a\u95f4\u805a\u7c7b\u8fdb\u884c\u5206\u6790\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u6307\u6807\uff1a\u6ce8\u610f\u529b\u591a\u6837\u6027\u548c\u6ce8\u610f\u529b\u53d8\u5316\u7387\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7531\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u4ee3\u7406\u8868\u73b0\u51fa\u6bd4\u5916\u5728\u52a8\u673a\u4ee3\u7406\u66f4\u5e7f\u6cdb\u548c\u52a8\u6001\u7684\u6ce8\u610f\u529b\u53ca\u63a2\u7d22\u884c\u4e3a\uff0c\u5176\u4e2d Transformer-RND \u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u6ce8\u610f\u529b\u3001\u9ad8\u63a2\u7d22\u8986\u76d6\u7387\u548c\u7d27\u51d1\u7684\u7ed3\u6784\u5316\u6f5c\u5728\u8868\u5f81\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u5f3a\u8c03\u4e86\u67b6\u6784\u5f52\u7eb3\u504f\u5dee\u548c\u8bad\u7ec3\u4fe1\u53f7\u5bf9\u4ee3\u7406\u5185\u90e8\u52a8\u6001\u7684\u5f71\u54cd\uff0c\u8fd8\u4e3a\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u611f\u77e5\u548c\u62bd\u8c61\u80fd\u529b\u63d0\u4f9b\u4e86\u8bca\u65ad\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u6027\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.06817", "pdf": "https://arxiv.org/pdf/2505.06817", "abs": "https://arxiv.org/abs/2505.06817", "authors": ["Sivasathivel Kandasamy"], "title": "Control Plane as a Tool: A Scalable Design Pattern for Agentic AI Systems", "categories": ["cs.AI"], "comment": "2 Figures and 2 Tables", "summary": "Agentic AI systems represent a new frontier in artificial intelligence, where\nagents often based on large language models(LLMs) interact with tools,\nenvironments, and other agents to accomplish tasks with a degree of autonomy.\nThese systems show promise across a range of domains, but their architectural\nunderpinnings remain immature. This paper conducts a comprehensive review of\nthe types of agents, their modes of interaction with the environment, and the\ninfrastructural and architectural challenges that emerge. We identify a gap in\nhow these systems manage tool orchestration at scale and propose a reusable\ndesign abstraction: the \"Control Plane as a Tool\" pattern. This pattern allows\ndevelopers to expose a single tool interface to an agent while encapsulating\nmodular tool routing logic behind it. We position this pattern within the\nbroader context of agent design and argue that it addresses several key\nchallenges in scaling, safety, and extensibility.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u81ea\u4e3bAI\u4ee3\u7406\u7684\u67b6\u6784\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u590d\u7528\u7684\u8bbe\u8ba1\u62bd\u8c61\u6a21\u5f0f\u2014\u2014'Control Plane as a Tool'\uff0c\u4ee5\u89e3\u51b3\u89c4\u6a21\u5316\u5de5\u5177\u7f16\u6392\u4e2d\u7684\u95ee\u9898\u3002", "motivation": "\u81ea\u4e3bAI\u4ee3\u7406\u5728\u5904\u7406\u4efb\u52a1\u65f6\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5f53\u524d\u67b6\u6784\u4e0d\u6210\u719f\uff0c\u5c24\u5176\u5728\u5de5\u5177\u7f16\u6392\u89c4\u6a21\u5316\u65b9\u9762\u5b58\u5728\u7a7a\u767d\uff0c\u4e9f\u9700\u9ad8\u6548\u3001\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5168\u9762\u68b3\u7406\u4ee3\u7406\u7c7b\u578b\u3001\u73af\u5883\u4ea4\u4e92\u6a21\u5f0f\u53ca\u67b6\u6784\u6311\u6218\uff0c\u63d0\u51fa\u5c06\u5de5\u5177\u8def\u7531\u903b\u8f91\u6a21\u5757\u5316\u7684'Control Plane as a Tool'\u8bbe\u8ba1\u6a21\u5f0f\u3002", "result": "\u8be5\u6a21\u5f0f\u901a\u8fc7\u5355\u4e00\u5de5\u5177\u63a5\u53e3\u5c01\u88c5\u590d\u6742\u8def\u7531\u903b\u8f91\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7406\u5728\u6269\u5c55\u6027\u3001\u5b89\u5168\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002", "conclusion": "'Control Plane as a Tool'\u4e3a\u4ee3\u7406\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u666e\u9002\u6027\u8303\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c4\u6a21\u5316\u5de5\u5177\u7ba1\u7406\u7684\u6838\u5fc3\u6311\u6218\u3002"}}
{"id": "2505.06782", "pdf": "https://arxiv.org/pdf/2505.06782", "abs": "https://arxiv.org/abs/2505.06782", "authors": ["Damian Curran", "Brian Chapman", "Mike Conway"], "title": "Utilizing LLMs to Investigate the Disputed Role of Evidence in Electronic Cigarette Health Policy Formation in Australia and the UK", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Australia and the UK have developed contrasting approaches to the regulation\nof electronic cigarettes, with - broadly speaking - Australia adopting a\nrelatively restrictive approach and the UK adopting a more permissive approach.\nNotably, these divergent policies were developed from the same broad evidence\nbase. In this paper, to investigate differences in how the two jurisdictions\nmanage and present evidence, we developed and evaluated a Large Language\nModel-based sentence classifier to perform automated analyses of electronic\ncigarette-related policy documents drawn from official Australian and UK\nlegislative processes (109 documents in total). Specifically, we utilized GPT-4\nto automatically classify sentences based on whether they contained claims that\ne-cigarettes were broadly helpful or harmful for public health. Our LLM-based\nclassifier achieved an F-score of 0.9. Further, when applying the classifier to\nour entire sentence-level corpus, we found that Australian legislative\ndocuments show a much higher proportion of harmful statements, and a lower\nproportion of helpful statements compared to the expected values, with the\nopposite holding for the UK. In conclusion, this work utilized an LLM-based\napproach to provide evidence to support the contention that - drawing on the\nsame evidence base - Australian ENDS-related policy documents emphasize the\nharms associated with ENDS products and UK policy documents emphasize the\nbenefits. Further, our approach provides a starting point for using LLM-based\nmethods to investigate the complex relationship between evidence and health\npolicy formation.", "AI": {"tldr": "\u8bba\u6587\u5229\u7528GPT-4\u5f00\u53d1\u4e86\u4e00\u4e2a\u53e5\u5b50\u5206\u7c7b\u5668\uff0c\u5206\u6790\u6fb3\u5927\u5229\u4e9a\u548c\u82f1\u56fd\u7684\u7535\u5b50\u70df\u653f\u7b56\u6587\u4ef6\uff0c\u53d1\u73b0\u6fb3\u65b9\u66f4\u5f3a\u8c03\u5371\u5bb3\uff0c\u82f1\u65b9\u66f4\u5f3a\u8c03\u76ca\u5904\u3002", "motivation": "\u7814\u7a76\u6fb3\u5927\u5229\u4e9a\u548c\u82f1\u56fd\u5728\u7535\u5b50\u70df\u653f\u7b56\u4e0a\u7684\u5dee\u5f02\uff0c\u5c3d\u7ba1\u57fa\u4e8e\u76f8\u540c\u8bc1\u636e\uff0c\u4f46\u653f\u7b56\u53d6\u5411\u622a\u7136\u4e0d\u540c\u3002", "method": "\u4f7f\u7528GPT-4\u5f00\u53d1\u7684LLM\u53e5\u5b50\u5206\u7c7b\u5668\u81ea\u52a8\u5206\u6790109\u4efd\u653f\u7b56\u6587\u4ef6\u4e2d\u7684\u53e5\u5b50\uff0c\u5206\u7c7b\u4e3a\u6709\u76ca\u6216\u6709\u5bb3\u516c\u5171\u5065\u5eb7\u7684\u58f0\u660e\u3002", "result": "\u5206\u7c7b\u5668F-score\u8fbe0.9\uff1b\u6fb3\u5927\u5229\u4e9a\u6587\u4ef6\u66f4\u5f3a\u8c03\u5371\u5bb3\uff0c\u82f1\u56fd\u6587\u4ef6\u66f4\u5f3a\u8c03\u76ca\u5904\u3002", "conclusion": "LLM\u65b9\u6cd5\u8bc1\u5b9e\u4e86\u4e24\u56fd\u653f\u7b56\u5dee\u5f02\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5065\u5eb7\u653f\u7b56\u4e0e\u8bc1\u636e\u5173\u7cfb\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.06280", "pdf": "https://arxiv.org/pdf/2505.06280", "abs": "https://arxiv.org/abs/2505.06280", "authors": ["Gabriele Rosi", "Fabio Cermelli"], "title": "Show or Tell? A Benchmark To Evaluate Visual and Textual Prompts in Semantic Segmentation", "categories": ["cs.LG"], "comment": "Accepted to PixFoundation workshop at CVPR2025. Code:\n  https://github.com/FocoosAI/ShowOrTell", "summary": "Prompt engineering has shown remarkable success with large language models,\nyet its systematic exploration in computer vision remains limited. In semantic\nsegmentation, both textual and visual prompts offer distinct advantages:\ntextual prompts through open-vocabulary methods allow segmentation of arbitrary\ncategories, while visual reference prompts provide intuitive reference\nexamples. However, existing benchmarks evaluate these modalities in isolation,\nwithout direct comparison under identical conditions. We present Show or Tell\n(SoT), a novel benchmark specifically designed to evaluate both visual and\ntextual prompts for semantic segmentation across 14 datasets spanning 7 diverse\ndomains (common scenes, urban, food, waste, parts, tools, and land-cover). We\nevaluate 5 open-vocabulary methods and 4 visual reference prompt approaches,\nadapting the latter to handle multi-class segmentation through a\nconfidence-based mask merging strategy. Our extensive experiments reveal that\nopen-vocabulary methods excel with common concepts easily described by text but\nstruggle with complex domains like tools, while visual reference prompt methods\nachieve good average results but exhibit high variability depending on the\ninput prompt. Through comprehensive quantitative and qualitative analysis, we\nidentify the strengths and weaknesses of both prompting modalities, providing\nvaluable insights to guide future research in vision foundation models for\nsegmentation tasks.", "AI": {"tldr": "SoT\u662f\u4e00\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u8bc4\u4f30\u8bed\u4e49\u5206\u5272\u4e2d\u89c6\u89c9\u548c\u6587\u672c\u63d0\u793a\u7684\u65b0\u57fa\u51c6\uff0c\u6db5\u76d614\u4e2a\u6570\u636e\u96c6\u548c7\u4e2a\u591a\u6837\u5316\u9886\u57df\uff0c\u63ed\u793a\u4e86\u4e24\u79cd\u63d0\u793a\u65b9\u6cd5\u7684\u4f18\u52a3\u52bf\u3002", "motivation": "\u76ee\u524d\u8bed\u4e49\u5206\u5272\u4e2d\u6587\u672c\u548c\u89c6\u89c9\u63d0\u793a\u7684\u8bc4\u4f30\u7f3a\u4e4f\u76f4\u63a5\u6bd4\u8f83\uff0c\u4e3a\u4e86\u7cfb\u7edf\u63a2\u7d22\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u8bbe\u8ba1\u4e86SoT\u57fa\u51c6\u3002", "method": "\u901a\u8fc75\u79cd\u5f00\u653e\u8bcd\u6c47\u65b9\u6cd5\u548c4\u79cd\u89c6\u89c9\u53c2\u8003\u63d0\u793a\u65b9\u6cd5\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u63a9\u7801\u5408\u5e76\u7b56\u7565\u5904\u7406\u591a\u7c7b\u5206\u5272\u3002", "result": "\u5f00\u653e\u8bcd\u6c47\u65b9\u6cd5\u5728\u6587\u672c\u6613\u4e8e\u63cf\u8ff0\u7684\u5e38\u89c1\u6982\u5ff5\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u590d\u6742\u9886\u57df\uff08\u5982\u5de5\u5177\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff1b\u89c6\u89c9\u53c2\u8003\u63d0\u793a\u65b9\u6cd5\u5e73\u5747\u6548\u679c\u826f\u597d\u4f46\u53d7\u8f93\u5165\u63d0\u793a\u5f71\u54cd\u8f83\u5927\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf9\u4e24\u79cd\u63d0\u793a\u6a21\u6001\u4f18\u7f3a\u70b9\u7684\u6df1\u5165\u5206\u6790\uff0c\u4e3a\u672a\u6765\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2505.06856", "pdf": "https://arxiv.org/pdf/2505.06856", "abs": "https://arxiv.org/abs/2505.06856", "authors": ["Bonan Wang", "Haicheng Liao", "Chengyue Wang", "Bin Rao", "Yanchen Guan", "Guyang Yu", "Jiaxun Zhang", "Songning Lai", "Chengzhong Xu", "Zhenning Li"], "title": "Beyond Patterns: Harnessing Causal Logic for Autonomous Driving Trajectory Prediction", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Accurate trajectory prediction has long been a major challenge for autonomous\ndriving (AD). Traditional data-driven models predominantly rely on statistical\ncorrelations, often overlooking the causal relationships that govern traffic\nbehavior. In this paper, we introduce a novel trajectory prediction framework\nthat leverages causal inference to enhance predictive robustness,\ngeneralization, and accuracy. By decomposing the environment into spatial and\ntemporal components, our approach identifies and mitigates spurious\ncorrelations, uncovering genuine causal relationships. We also employ a\nprogressive fusion strategy to integrate multimodal information, simulating\nhuman-like reasoning processes and enabling real-time inference. Evaluations on\nfive real-world datasets--ApolloScape, nuScenes, NGSIM, HighD, and\nMoCAD--demonstrate our model's superiority over existing state-of-the-art\n(SOTA) methods, with improvements in key metrics such as RMSE and FDE. Our\nfindings highlight the potential of causal reasoning to transform trajectory\nprediction, paving the way for robust AD systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u63a8\u7406\u7684\u65b0\u578b\u8f68\u8ff9\u9884\u6d4b\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u9884\u6d4b\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u6570\u636e\u9a71\u52a8\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u7edf\u8ba1\u76f8\u5173\u6027\uff0c\u5f80\u5f80\u5ffd\u7565\u4e86\u5f71\u54cd\u4ea4\u901a\u884c\u4e3a\u7684\u56e0\u679c\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u5c06\u73af\u5883\u5206\u89e3\u4e3a\u7a7a\u95f4\u548c\u65f6\u95f4\u7ec4\u4ef6\uff0c\u8bc6\u522b\u5e76\u6d88\u9664\u4e86\u865a\u5047\u76f8\u5173\u6027\uff0c\u540c\u65f6\u91c7\u7528\u6e10\u8fdb\u5f0f\u878d\u5408\u7b56\u7565\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728RMSE\u548cFDE\u7b49\u5173\u952e\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u56e0\u679c\u63a8\u7406\u5728\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u5177\u6709\u53d8\u9769\u6f5c\u529b\uff0c\u6709\u671b\u63a8\u52a8\u66f4\u9c81\u68d2\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2505.06862", "pdf": "https://arxiv.org/pdf/2505.06862", "abs": "https://arxiv.org/abs/2505.06862", "authors": ["Lhuqita Fazry"], "title": "A Split-then-Join Approach to Abstractive Summarization for Very Long Documents in a Low Resource Setting", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "$\\texttt{BIGBIRD-PEGASUS}$ model achieves $\\textit{state-of-the-art}$ on\nabstractive text summarization for long documents. However it's capacity still\nlimited to maximum of $4,096$ tokens, thus caused performance degradation on\nsummarization for very long documents. Common method to deal with the issue is\nto truncate the documents. In this reasearch, we'll use different approach.\nWe'll use the pretrained $\\texttt{BIGBIRD-PEGASUS}$ model by fine tuned the\nmodel on other domain dataset. First, we filter out all documents which length\nless than $20,000$ tokens to focus on very long documents. To prevent domain\nshifting problem and overfitting on transfer learning due to small dataset, we\naugment the dataset by splitting document-summary training pair into parts, to\nfit the document into $4,096$ tokens. Source code available on\n$\\href{https://github.com/lhfazry/SPIN-summ}{https://github.com/lhfazry/SPIN-summ}$.", "AI": {"tldr": "BIGBIRD-PEGASUS\u6a21\u578b\u5728\u957f\u6587\u6863\u6458\u8981\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u7531\u4e8e\u5176\u6700\u591a\u53ea\u80fd\u5904\u74064096\u4e2a\u6807\u8bb0\uff0c\u5bf9\u8d85\u957f\u6587\u6863\u6458\u8981\u7684\u6027\u80fd\u6709\u9650\u3002\u7814\u7a76\u91c7\u7528\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7b5b\u9009\u8d85\u8fc720000\u6807\u8bb0\u7684\u6587\u6863\uff0c\u5e76\u901a\u8fc7\u62c6\u5206\u6587\u6863-\u6458\u8981\u5bf9\u6765\u9002\u5e94\u6a21\u578b\u9650\u5236\uff0c\u4ee5\u907f\u514d\u57df\u504f\u79fb\u548c\u5c0f\u6570\u636e\u96c6\u8fc7\u62df\u5408\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3BIGBIRD-PEGASUS\u6a21\u578b\u5728\u5904\u7406\u8d85\u957f\u6587\u6863\u65f6\u56e0\u6807\u8bb0\u9650\u5236\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u907f\u514d\u5e38\u89c1\u7684\u622a\u65ad\u6587\u6863\u65b9\u6cd5\u3002", "method": "\u7b5b\u9009\u8d85\u8fc720000\u6807\u8bb0\u7684\u6587\u6863\uff0c\u91c7\u7528\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u5c06\u6587\u6863-\u6458\u8981\u5bf9\u62c6\u5206\u4e3a\u7b26\u54084096\u6807\u8bb0\u9650\u5236\u7684\u90e8\u5206\uff0c\u5e76\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u6a21\u578b\u80fd\u591f\u5728\u8d85\u957f\u6587\u6863\u6458\u8981\u4efb\u52a1\u4e0a\u4fdd\u6301\u6027\u80fd\uff0c\u907f\u514d\u4e86\u56e0\u76f4\u63a5\u622a\u65ad\u5e26\u6765\u7684\u4fe1\u606f\u635f\u5931\u3002", "conclusion": "\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u6210\u529f\u63d0\u5347\u4e86BIGBIRD-PEGASUS\u5728\u8d85\u957f\u6587\u6863\u6458\u8981\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2505.06281", "pdf": "https://arxiv.org/pdf/2505.06281", "abs": "https://arxiv.org/abs/2505.06281", "authors": ["Chunduru Rohith Kumar", "PHD Surya Shanmuk", "Prabhala Naga Srinivas", "Sri Venkatesh Lankalapalli", "Debasis Dwibedy"], "title": "A Data-Driven Probabilistic Framework for Cascading Urban Risk Analysis Using Bayesian Networks", "categories": ["cs.LG", "stat.ML"], "comment": "14 pages, 4 figures, 8 tables", "summary": "The increasing complexity of cascading risks in urban systems necessitates\nrobust, data-driven frameworks to model interdependencies across multiple\ndomains. This study presents a foundational Bayesian network-based approach for\nanalyzing cross-domain risk propagation across key urban domains, including\nair, water, electricity, agriculture, health, infrastructure, weather, and\nclimate. Directed Acyclic Graphs (DAGs) are constructed using Bayesian Belief\nNetworks (BBNs), with structure learning guided by Hill-Climbing search\noptimized through Bayesian Information Criterion (BIC) and K2 scoring. The\nframework is trained on a hybrid dataset that combines real-world urban\nindicators with synthetically generated data from Generative Adversarial\nNetworks (GANs), and is further balanced using the Synthetic Minority\nOver-sampling Technique (SMOTE). Conditional Probability Tables (CPTs) derived\nfrom the learned structures enable interpretable probabilistic reasoning and\nquantify the likelihood of cascading failures. The results identify key intra-\nand inter-domain risk factors and demonstrate the framework's utility for\nproactive urban resilience planning. This work establishes a scalable,\ninterpretable foundation for cascading risk assessment and serves as a basis\nfor future empirical research in this emerging interdisciplinary field.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u57ce\u5e02\u7cfb\u7edf\u4e2d\u8de8\u57df\u98ce\u9669\u7684\u4f20\u64ad\u3002\u901a\u8fc7\u7ed3\u5408\u771f\u5b9e\u6570\u636e\u548cGAN\u751f\u6210\u6570\u636e\uff0c\u5229\u7528\u7ed3\u6784\u5b66\u4e60\u548c\u6982\u7387\u63a8\u7406\uff0c\u8bc6\u522b\u5173\u952e\u98ce\u9669\u56e0\u7d20\uff0c\u4e3a\u57ce\u5e02\u97e7\u6027\u89c4\u5212\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u57ce\u5e02\u7cfb\u7edf\u4e2d\u8de8\u57df\u98ce\u9669\u7684\u590d\u6742\u6027\u589e\u52a0\uff0c\u9700\u8981\u6570\u636e\u9a71\u52a8\u7684\u6846\u67b6\u6765\u5efa\u6a21\u591a\u9886\u57df\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u6027\u3002", "method": "\u4f7f\u7528\u8d1d\u53f6\u65af\u4fe1\u5ff5\u7f51\u7edc\uff08BBNs\uff09\u6784\u5efa\u6709\u5411\u65e0\u73af\u56fe\uff08DAGs\uff09\uff0c\u7ed3\u6784\u5b66\u4e60\u91c7\u7528Hill-Climbing\u641c\u7d22\uff0c\u4f18\u5316\u57fa\u4e8eBIC\u548cK2\u8bc4\u5206\u3002\u8bad\u7ec3\u6570\u636e\u7ed3\u5408\u771f\u5b9e\u57ce\u5e02\u6307\u6807\u548cGAN\u751f\u6210\u6570\u636e\uff0c\u5e76\u7528SMOTE\u5e73\u8861\u3002", "result": "\u8bc6\u522b\u4e86\u5173\u952e\u57df\u5185\u548c\u8de8\u57df\u98ce\u9669\u56e0\u7d20\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728\u4e3b\u52a8\u57ce\u5e02\u97e7\u6027\u89c4\u5212\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7ea7\u8054\u98ce\u9669\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u57fa\u7840\uff0c\u5e76\u4e3a\u672a\u6765\u8fd9\u4e00\u65b0\u5174\u8de8\u5b66\u79d1\u9886\u57df\u7684\u5b9e\u8bc1\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.06897", "pdf": "https://arxiv.org/pdf/2505.06897", "abs": "https://arxiv.org/abs/2505.06897", "authors": ["Jinhao Jiang", "Changlin Chen", "Shile Feng", "Wanru Geng", "Zesheng Zhou", "Ni Wang", "Shuai Li", "Feng-Qi Cui", "Erbao Dong"], "title": "Embodied Intelligence: The Key to Unblocking Generalized Artificial Intelligence", "categories": ["cs.AI"], "comment": "19pages,7 figures,3 tables", "summary": "The ultimate goal of artificial intelligence (AI) is to achieve Artificial\nGeneral Intelligence (AGI). Embodied Artificial Intelligence (EAI), which\ninvolves intelligent systems with physical presence and real-time interaction\nwith the environment, has emerged as a key research direction in pursuit of\nAGI. While advancements in deep learning, reinforcement learning, large-scale\nlanguage models, and multimodal technologies have significantly contributed to\nthe progress of EAI, most existing reviews focus on specific technologies or\napplications. A systematic overview, particularly one that explores the direct\nconnection between EAI and AGI, remains scarce. This paper examines EAI as a\nfoundational approach to AGI, systematically analyzing its four core modules:\nperception, intelligent decision-making, action, and feedback. We provide a\ndetailed discussion of how each module contributes to the six core principles\nof AGI. Additionally, we discuss future trends, challenges, and research\ndirections in EAI, emphasizing its potential as a cornerstone for AGI\ndevelopment. Our findings suggest that EAI's integration of dynamic learning\nand real-world interaction is essential for bridging the gap between narrow AI\nand AGI.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5d4c\u5165\u5f0f\u4eba\u5de5\u667a\u80fd\uff08EAI\uff09\u4f5c\u4e3a\u5b9e\u73b0\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff08AGI\uff09\u7684\u57fa\u7840\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u5176\u56db\u5927\u6838\u5fc3\u6a21\u5757\uff08\u611f\u77e5\u3001\u667a\u80fd\u51b3\u7b56\u3001\u884c\u52a8\u548c\u53cd\u9988\uff09\u53ca\u5176\u5bf9AGI\u516d\u9879\u6838\u5fc3\u539f\u5219\u7684\u8d21\u732e\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u8d8b\u52bf\u548c\u6311\u6218\u3002", "motivation": "\u4e3a\u5b9e\u73b0AGI\uff0cEAI\u56e0\u5176\u5b9e\u65f6\u73af\u5883\u4ea4\u4e92\u548c\u7269\u7406\u5b58\u5728\u7279\u6027\u6210\u4e3a\u5173\u952e\u7814\u7a76\u65b9\u5411\u3002\u73b0\u6709\u7814\u7a76\u591a\u805a\u7126\u7279\u5b9a\u6280\u672f\u6216\u5e94\u7528\uff0c\u7f3a\u4e4f\u5bf9EAI\u4e0eAGI\u76f4\u63a5\u8054\u7cfb\u7684\u7cfb\u7edf\u6027\u7efc\u8ff0\u3002", "method": "\u7cfb\u7edf\u5206\u6790EAI\u7684\u56db\u5927\u6838\u5fc3\u6a21\u5757\uff08\u611f\u77e5\u3001\u51b3\u7b56\u3001\u884c\u52a8\u3001\u53cd\u9988\uff09\u53ca\u5176\u5982\u4f55\u652f\u6301AGI\u7684\u6838\u5fc3\u539f\u5219\uff0c\u5e76\u7ed3\u5408\u73b0\u6709\u6280\u672f\u8fdb\u5c55\u8fdb\u884c\u8ba8\u8bba\u3002", "result": "\u7814\u7a76\u53d1\u73b0EAI\u901a\u8fc7\u52a8\u6001\u5b66\u4e60\u548c\u73b0\u5b9e\u4ea4\u4e92\uff0c\u4e3a\u8fde\u63a5\u7a84AI\u4e0eAGI\u63d0\u4f9b\u4e86\u5173\u952e\u8def\u5f84\uff0c\u540c\u65f6\u660e\u786e\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "EAI\u662f\u5b9e\u73b0AGI\u7684\u91cd\u8981\u57fa\u77f3\uff0c\u5176\u591a\u6a21\u5757\u534f\u540c\u4e0e\u5b9e\u65f6\u4ea4\u4e92\u80fd\u529b\u4e3aAGI\u53d1\u5c55\u63d0\u4f9b\u4e86\u72ec\u7279\u4f18\u52bf\uff0c\u4f46\u4ecd\u9700\u514b\u670d\u6280\u672f\u4e0e\u7406\u8bba\u6311\u6218\u3002"}}
{"id": "2505.06889", "pdf": "https://arxiv.org/pdf/2505.06889", "abs": "https://arxiv.org/abs/2505.06889", "authors": ["Mihyeon Kim", "Juhyoung Park", "Youngbin Kim"], "title": "IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP 2024 Main", "summary": "Pre-trained Language Models (PLMs) have achieved remarkable performance on\ndiverse NLP tasks through pre-training and fine-tuning. However, fine-tuning\nthe model with a large number of parameters on limited downstream datasets\noften leads to vulnerability to adversarial attacks, causing overfitting of the\nmodel on standard datasets.\n  To address these issues, we propose IM-BERT from the perspective of a dynamic\nsystem by conceptualizing a layer of BERT as a solution of Ordinary\nDifferential Equations (ODEs). Under the situation of initial value\nperturbation, we analyze the numerical stability of two main numerical ODE\nsolvers: the explicit and implicit Euler approaches.\n  Based on these analyses, we introduce a numerically robust IM-connection\nincorporating BERT's layers. This strategy enhances the robustness of PLMs\nagainst adversarial attacks, even in low-resource scenarios, without\nintroducing additional parameters or adversarial training strategies.\n  Experimental results on the adversarial GLUE (AdvGLUE) dataset validate the\nrobustness of IM-BERT under various conditions. Compared to the original BERT,\nIM-BERT exhibits a performance improvement of approximately 8.3\\%p on the\nAdvGLUE dataset. Furthermore, in low-resource scenarios, IM-BERT outperforms\nBERT by achieving 5.9\\%p higher accuracy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86IM-BERT\u6a21\u578b\uff0c\u901a\u8fc7\u5c06BERT\u7684\u5c42\u5efa\u6a21\u4e3aODE\u89e3\uff0c\u5e76\u5206\u6790\u6570\u503c\u7a33\u5b9a\u6027\uff0c\u589e\u5f3a\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u5fae\u8c03\u65f6\u56e0\u53c2\u6570\u8fc7\u591a\u800c\u5bfc\u81f4\u7684\u5bf9\u6297\u653b\u51fb\u8106\u5f31\u6027\u95ee\u9898\u3002", "method": "\u5c06BERT\u7684\u5c42\u5efa\u6a21\u4e3aODE\u89e3\uff0c\u5206\u6790\u663e\u5f0f\u548c\u9690\u5f0f\u6b27\u62c9\u65b9\u6cd5\u7684\u6570\u503c\u7a33\u5b9a\u6027\uff0c\u5e76\u5f15\u5165IM-connection\u7b56\u7565\u3002", "result": "\u5728AdvGLUE\u6570\u636e\u96c6\u4e0a\uff0cIM-BERT\u6bd4\u539f\u59cbBERT\u6027\u80fd\u63d0\u5347\u7ea68.3%\uff0c\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u51c6\u786e\u7387\u63d0\u53475.9%\u3002", "conclusion": "IM-BERT\u901a\u8fc7\u6570\u503c\u7a33\u5b9a\u7684\u8fde\u63a5\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u4e14\u65e0\u9700\u989d\u5916\u53c2\u6570\u6216\u5bf9\u6297\u8bad\u7ec3\u3002"}}
{"id": "2505.06282", "pdf": "https://arxiv.org/pdf/2505.06282", "abs": "https://arxiv.org/abs/2505.06282", "authors": ["Zixu Wang", "Bingbing Xu", "Yige Yuan", "Huawei Shen", "Xueqi Cheng"], "title": "InfoNCE is a Free Lunch for Semantically guided Graph Contrastive Learning", "categories": ["cs.LG"], "comment": "10 pages, 5 figures, Accepted by SIGIR2025", "summary": "As an important graph pre-training method, Graph Contrastive Learning (GCL)\ncontinues to play a crucial role in the ongoing surge of research on graph\nfoundation models or LLM as enhancer for graphs. Traditional GCL optimizes\nInfoNCE by using augmentations to define self-supervised tasks, treating\naugmented pairs as positive samples and others as negative. However, this leads\nto semantically similar pairs being classified as negative, causing significant\nsampling bias and limiting performance. In this paper, we argue that GCL is\nessentially a Positive-Unlabeled (PU) learning problem, where the definition of\nself-supervised tasks should be semantically guided, i.e., augmented samples\nwith similar semantics are considered positive, while others, with unknown\nsemantics, are treated as unlabeled. From this perspective, the key lies in how\nto extract semantic information. To achieve this, we propose IFL-GCL, using\nInfoNCE as a \"free lunch\" to extract semantic information. Specifically, We\nfirst prove that under InfoNCE, the representation similarity of node pairs\naligns with the probability that the corresponding contrastive sample is\npositive. Then we redefine the maximum likelihood objective based on the\ncorrected samples, leading to a new InfoNCE loss function. Extensive\nexperiments on both the graph pretraining framework and LLM as an enhancer show\nsignificantly improvements of IFL-GCL in both IID and OOD scenarios, achieving\nup to a 9.05% improvement, validating the effectiveness of semantically guided.\nCode for IFL-GCL is publicly available at:\nhttps://github.com/Camel-Prince/IFL-GCL.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5IFL-GCL\uff0c\u901a\u8fc7\u5c06GCL\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6b63\u672a\u6807\u6ce8\uff08PU\uff09\u5b66\u4e60\u95ee\u9898\uff0c\u5e76\u5229\u7528InfoNCE\u63d0\u53d6\u8bed\u4e49\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u5bf9\u6bd4\u5b66\u4e60\uff08GCL\uff09\u65b9\u6cd5\u5728\u5b9a\u4e49\u81ea\u76d1\u7763\u4efb\u52a1\u65f6\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bed\u4e49\u76f8\u4f3c\u7684\u6837\u672c\u88ab\u9519\u8bef\u5206\u7c7b\u4e3a\u8d1f\u6837\u672c\uff0c\u9020\u6210\u91c7\u6837\u504f\u5dee\u548c\u6027\u80fd\u9650\u5236\u3002\u56e0\u6b64\uff0c\u8bba\u6587\u63d0\u51faGCL\u5e94\u89c6\u4e3a\u6b63\u672a\u6807\u6ce8\uff08PU\uff09\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u6307\u5bfc\u91cd\u65b0\u5b9a\u4e49\u4efb\u52a1\u76ee\u6807\u3002", "method": "\u8bba\u6587\u63d0\u51faIFL-GCL\u65b9\u6cd5\uff0c\u5229\u7528InfoNCE\u4f5c\u4e3a\u5de5\u5177\u63d0\u53d6\u8bed\u4e49\u4fe1\u606f\uff0c\u8bc1\u660e\u8282\u70b9\u5bf9\u8868\u793a\u76f8\u4f3c\u6027\u4e0e\u5176\u5bf9\u6bd4\u6837\u672c\u4e3a\u6b63\u6837\u672c\u7684\u6982\u7387\u4e00\u81f4\uff0c\u5e76\u57fa\u4e8e\u4fee\u6b63\u6837\u672c\u91cd\u65b0\u5b9a\u4e49\u6700\u5927\u4f3c\u7136\u76ee\u6807\uff0c\u63a8\u5bfc\u51fa\u65b0\u7684InfoNCE\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728IID\u548cOOD\u573a\u666f\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0cIFL-GCL\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6700\u9ad8\u63d0\u5347\u4e869.05%\uff0c\u9a8c\u8bc1\u4e86\u8bed\u4e49\u6307\u5bfc\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "IFL-GCL\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49GCL\u4e3aPU\u95ee\u9898\u5e76\u5f15\u5165\u8bed\u4e49\u4fe1\u606f\u63d0\u53d6\uff0c\u6210\u529f\u63d0\u5347\u4e86\u56fe\u5bf9\u6bd4\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u4e3a\u56fe\u9884\u8bad\u7ec3\u548cLLM\u589e\u5f3a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.06907", "pdf": "https://arxiv.org/pdf/2505.06907", "abs": "https://arxiv.org/abs/2505.06907", "authors": ["Yu Qiao", "Huy Q. Le", "Avi Deb Raha", "Phuong-Nam Tran", "Apurba Adhikary", "Mengchun Zhang", "Loc X. Nguyen", "Eui-Nam Huh", "Dusit Niyato", "Choong Seon Hong"], "title": "Towards Artificial General or Personalized Intelligence? A Survey on Foundation Models for Personalized Federated Intelligence", "categories": ["cs.AI", "cs.CV", "cs.NE"], "comment": "On going work", "summary": "The rise of large language models (LLMs), such as ChatGPT, DeepSeek, and\nGrok-3, has reshaped the artificial intelligence landscape. As prominent\nexamples of foundational models (FMs) built on LLMs, these models exhibit\nremarkable capabilities in generating human-like content, bringing us closer to\nachieving artificial general intelligence (AGI). However, their large-scale\nnature, sensitivity to privacy concerns, and substantial computational demands\npresent significant challenges to personalized customization for end users. To\nbridge this gap, this paper presents the vision of artificial personalized\nintelligence (API), focusing on adapting these powerful models to meet the\nspecific needs and preferences of users while maintaining privacy and\nefficiency. Specifically, this paper proposes personalized federated\nintelligence (PFI), which integrates the privacy-preserving advantages of\nfederated learning (FL) with the zero-shot generalization capabilities of FMs,\nenabling personalized, efficient, and privacy-protective deployment at the\nedge. We first review recent advances in both FL and FMs, and discuss the\npotential of leveraging FMs to enhance federated systems. We then present the\nkey motivations behind realizing PFI and explore promising opportunities in\nthis space, including efficient PFI, trustworthy PFI, and PFI empowered by\nretrieval-augmented generation (RAG). Finally, we outline key challenges and\nfuture research directions for deploying FM-powered FL systems at the edge with\nimproved personalization, computational efficiency, and privacy guarantees.\nOverall, this survey aims to lay the groundwork for the development of API as a\ncomplement to AGI, with a particular focus on PFI as a key enabling technique.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e2a\u6027\u8054\u90a6\u667a\u80fd\uff08PFI\uff09\uff0c\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u7684\u9690\u79c1\u4fdd\u62a4\u4f18\u52bf\u548c\u57fa\u7840\u6a21\u578b\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u65e8\u5728\u5b9e\u73b0\u4e2a\u6027\u5316\u3001\u9ad8\u6548\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u8fb9\u7f18\u90e8\u7f72\uff0c\u4e3a\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff08AGI\uff09\u7684\u8865\u5145\u4eba\u5de5\u4e2a\u6027\u667a\u80fd\uff08API\uff09\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u867d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5927\u89c4\u6a21\u6027\u3001\u9690\u79c1\u654f\u611f\u6027\u53ca\u9ad8\u8ba1\u7b97\u9700\u6c42\u5bfc\u81f4\u96be\u4ee5\u6ee1\u8db3\u7528\u6237\u7684\u4e2a\u6027\u5316\u5b9a\u5236\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e2a\u6027\u8054\u90a6\u667a\u80fd\uff08PFI\uff09\uff0c\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u7684\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\u548c\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u63a2\u8ba8\u4e86\u9ad8\u6548PFI\u3001\u53ef\u4fe1PFI\u53caRAG\u8d4b\u80fd\u7684PFI\u7b49\u673a\u9047\uff0c\u5e76\u5206\u6790\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "PFI\u662f\u5b9e\u73b0\u4eba\u5de5\u4e2a\u6027\u667a\u80fd\uff08API\uff09\u7684\u5173\u952e\u6280\u672f\uff0c\u4e3aAGI\u7684\u8865\u5145\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2505.06904", "pdf": "https://arxiv.org/pdf/2505.06904", "abs": "https://arxiv.org/abs/2505.06904", "authors": ["Xinyi Mou", "Chen Qian", "Wei Liu", "Xuanjing Huang", "Zhongyu Wei"], "title": "EcoLANG: Efficient and Effective Agent Communication Language Induction for Social Simulation", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) have demonstrated an impressive ability to\nrole-play humans and replicate complex social dynamics. While large-scale\nsocial simulations are gaining increasing attention, they still face\nsignificant challenges, particularly regarding high time and computation costs.\nExisting solutions, such as distributed mechanisms or hybrid agent-based model\n(ABM) integrations, either fail to address inference costs or compromise\naccuracy and generalizability. To this end, we propose EcoLANG: Efficient and\nEffective Agent Communication Language Induction for Social Simulation. EcoLANG\noperates in two stages: (1) language evolution, where we filter synonymous\nwords and optimize sentence-level rules through natural selection, and (2)\nlanguage utilization, where agents in social simulations communicate using the\nevolved language. Experimental results demonstrate that EcoLANG reduces token\nconsumption by over 20%, enhancing efficiency without sacrificing simulation\naccuracy.", "AI": {"tldr": "EcoLANG\u63d0\u51fa\u4e86\u4e24\u9636\u6bb5\u8bed\u8a00\u6f14\u5316\u4e0e\u5229\u7528\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u793e\u4ea4\u6a21\u62df\u4e2d\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6210\u672c\u6216\u4fdd\u6301\u51c6\u786e\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cEcoLANG\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5206\u4e3a\u8bed\u8a00\u6f14\u5316\u548c\u8bed\u8a00\u5229\u7528\u4e24\u4e2a\u9636\u6bb5\uff1a\u901a\u8fc7\u81ea\u7136\u9009\u62e9\u4f18\u5316\u8bed\u8a00\u89c4\u5219\uff0c\u5e76\u5728\u793e\u4ea4\u6a21\u62df\u4e2d\u4f7f\u7528\u4f18\u5316\u7684\u8bed\u8a00\u3002", "result": "\u5b9e\u9a8c\u8868\u660eEcoLANG\u51cf\u5c11\u8d85\u8fc720%\u7684token\u6d88\u8017\uff0c\u63d0\u5347\u6548\u7387\u4e14\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u3002", "conclusion": "EcoLANG\u4e3a\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u793e\u4ea4\u6a21\u62df\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2505.06283", "pdf": "https://arxiv.org/pdf/2505.06283", "abs": "https://arxiv.org/abs/2505.06283", "authors": ["Limin Li", "Kuo Yang", "Wenjie Du", "Pengkun Wang", "Zhengyang Zhou", "Yang Wang"], "title": "Soft causal learning for generalized molecule property prediction: An environment perspective", "categories": ["cs.LG", "q-bio.QM", "stat.ML", "I.2.4"], "comment": "23 pages, 7 figures, 3 tables", "summary": "Learning on molecule graphs has become an increasingly important topic in AI\nfor science, which takes full advantage of AI to facilitate scientific\ndiscovery. Existing solutions on modeling molecules utilize Graph Neural\nNetworks (GNNs) to achieve representations but they mostly fail to adapt models\nto out-of-distribution (OOD) samples. Although recent advances on OOD-oriented\ngraph learning have discovered the invariant rationale on graphs, they still\nignore three important issues, i.e., 1) the expanding atom patterns regarding\nenvironments on graphs lead to failures of invariant rationale based models, 2)\nthe associations between discovered molecular subgraphs and corresponding\nproperties are complex where causal substructures cannot fully interpret the\nlabels. 3) the interactions between environments and invariances can influence\nwith each other thus are challenging to be modeled. To this end, we propose a\nsoft causal learning framework, to tackle the unresolved OOD challenge in\nmolecular science, from the perspective of fully modeling the molecule\nenvironments and bypassing the invariant subgraphs. Specifically, we first\nincorporate chemistry theories into our graph growth generator to imitate\nexpaned environments, and then devise an GIB-based objective to disentangle\nenvironment from whole graphs and finally introduce a cross-attention based\nsoft causal interaction, which allows dynamic interactions between environments\nand invariances. We perform experiments on seven datasets by imitating\ndifferent kinds of OOD generalization scenarios. Extensive comparison, ablation\nexperiments as well as visualized case studies demonstrate well generalization\nability of our proposal.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f6f\u56e0\u679c\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5206\u5b50\u79d1\u5b66\u4e2d\u7684OOD\u6311\u6218\uff0c\u901a\u8fc7\u6a21\u62df\u5206\u5b50\u73af\u5883\u548c\u7ed5\u8fc7\u4e0d\u53d8\u5b50\u56fe\u6765\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709GNN\u6a21\u578b\u5728\u5206\u5b50\u56fe\u5b66\u4e60\u4e2d\u96be\u4ee5\u9002\u5e94OOD\u6837\u672c\uff0c\u4e14\u5f53\u524dOOD\u65b9\u6cd5\u5ffd\u89c6\u539f\u5b50\u73af\u5883\u6269\u5c55\u3001\u5b50\u56fe-\u6027\u8d28\u5173\u8054\u590d\u6742\u6027\u53ca\u73af\u5883\u4e0e\u4e0d\u53d8\u6027\u52a8\u6001\u4ea4\u4e92\u3002", "method": "\u7ed3\u5408\u5316\u5b66\u7406\u8bba\u8bbe\u8ba1\u56fe\u589e\u957f\u751f\u6210\u5668\u6a21\u62df\u73af\u5883\uff0c\u91c7\u7528GIB\u76ee\u6807\u89e3\u8026\u73af\u5883\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u8f6f\u56e0\u679c\u4ea4\u4e92\u673a\u5236\u3002", "result": "\u5728\u4e03\u79cdOOD\u6cdb\u5316\u573a\u666f\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5b9e\u9a8c\u4e0e\u53ef\u89c6\u5316\u6848\u4f8b\u8868\u660e\u6a21\u578b\u5177\u6709\u4f18\u79c0\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u5206\u5b50OOD\u6cdb\u5316\u6027\u80fd\uff0c\u4e3aAI\u9a71\u52a8\u7684\u79d1\u5b66\u53d1\u73b0\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.06949", "pdf": "https://arxiv.org/pdf/2505.06949", "abs": "https://arxiv.org/abs/2505.06949", "authors": ["Sumyyah Toonsi", "Paul Schofield", "Robert Hoehndorf"], "title": "Causal knowledge graph analysis identifies adverse drug effects", "categories": ["cs.AI", "q-bio.BM"], "comment": null, "summary": "Knowledge graphs and structural causal models have each proven valuable for\norganizing biomedical knowledge and estimating causal effects, but remain\nlargely disconnected: knowledge graphs encode qualitative relationships\nfocusing on facts and deductive reasoning without formal probabilistic\nsemantics, while causal models lack integration with background knowledge in\nknowledge graphs and have no access to the deductive reasoning capabilities\nthat knowledge graphs provide. To bridge this gap, we introduce a novel\nformulation of Causal Knowledge Graphs (CKGs) which extend knowledge graphs\nwith formal causal semantics, preserving their deductive capabilities while\nenabling principled causal inference. CKGs support deconfounding via explicitly\nmarked causal edges and facilitate hypothesis formulation aligned with both\nencoded and entailed background knowledge. We constructed a Drug-Disease CKG\n(DD-CKG) integrating disease progression pathways, drug indications,\nside-effects, and hierarchical disease classification to enable automated\nlarge-scale mediation analysis. Applied to UK Biobank and MIMIC-IV cohorts, we\ntested whether drugs mediate effects between indications and downstream disease\nprogression, adjusting for confounders inferred from the DD-CKG. Our approach\nsuccessfully reproduced known adverse drug reactions with high precision while\nidentifying previously undocumented significant candidate adverse effects.\nFurther validation through side effect similarity analysis demonstrated that\ncombining our predicted drug effects with established databases significantly\nimproves the prediction of shared drug indications, supporting the clinical\nrelevance of our novel findings. These results demonstrate that our methodology\nprovides a generalizable, knowledge-driven framework for scalable causal\ninference.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u56e0\u679c\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\u2014\u2014\u56e0\u679c\u77e5\u8bc6\u56fe\u8c31\uff08CKG\uff09\uff0c\u7528\u4e8e\u751f\u7269\u533b\u5b66\u9886\u57df\u7684\u56e0\u679c\u63a8\u7406\uff0c\u5e76\u5728\u836f\u7269-\u75be\u75c5\u5173\u7cfb\u5206\u6790\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u77e5\u8bc6\u56fe\u8c31\u7f3a\u4e4f\u6982\u7387\u8bed\u4e49\uff0c\u800c\u56e0\u679c\u6a21\u578b\u7f3a\u4e4f\u80cc\u666f\u77e5\u8bc6\u7684\u6574\u5408\u548c\u6f14\u7ece\u63a8\u7406\u80fd\u529b\u3002\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e00\u79cd\u517c\u5177\u4e24\u79cd\u4f18\u52bf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bba\u6587\u5f15\u5165\u4e86\u56e0\u679c\u77e5\u8bc6\u56fe\u8c31\uff08CKG\uff09\uff0c\u6269\u5c55\u4e86\u77e5\u8bc6\u56fe\u8c31\u7684\u56e0\u679c\u8bed\u4e49\uff0c\u652f\u6301\u53bb\u6df7\u6dc6\u548c\u57fa\u4e8e\u80cc\u666f\u77e5\u8bc6\u7684\u5047\u8bbe\u5236\u5b9a\u3002\u4f5c\u8005\u6784\u5efa\u4e86\u836f\u7269-\u75be\u75c5CKG\uff08DD-CKG\uff09\uff0c\u5e76\u5e94\u7528\u4e8eUK Biobank\u548cMIMIC-IV\u961f\u5217\u7684\u5927\u89c4\u6a21\u4e2d\u4ecb\u5206\u6790\u3002", "result": "CKG\u65b9\u6cd5\u6210\u529f\u590d\u73b0\u4e86\u5df2\u77e5\u7684\u836f\u7269\u4e0d\u826f\u53cd\u5e94\uff0c\u5e76\u8bc6\u522b\u51fa\u65b0\u7684\u6f5c\u5728\u526f\u4f5c\u7528\u3002\u901a\u8fc7\u526f\u4f5c\u7528\u76f8\u4f3c\u6027\u5206\u6790\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4e34\u5e8a\u76f8\u5173\u6027\uff0c\u8868\u660e\u5176\u663e\u8457\u63d0\u5347\u4e86\u5171\u4eab\u836f\u7269\u9002\u5e94\u75c7\u7684\u9884\u6d4b\u80fd\u529b\u3002", "conclusion": "CKG\u4e3a\u53ef\u6269\u5c55\u7684\u56e0\u679c\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u77e5\u8bc6\u9a71\u52a8\u6846\u67b6\uff0c\u5177\u6709\u6f5c\u5728\u7684\u5e7f\u6cdb\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.06914", "pdf": "https://arxiv.org/pdf/2505.06914", "abs": "https://arxiv.org/abs/2505.06914", "authors": ["Chen Amiraz", "Florin Cuconasu", "Simone Filice", "Zohar Karnin"], "title": "The Distracting Effect: Understanding Irrelevant Passages in RAG", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "A well-known issue with Retrieval Augmented Generation (RAG) is that\nretrieved passages that are irrelevant to the query sometimes distract the\nanswer-generating LLM, causing it to provide an incorrect response. In this\npaper, we shed light on this core issue and formulate the distracting effect of\na passage w.r.t. a query (and an LLM). We provide a quantifiable measure of the\ndistracting effect of a passage and demonstrate its robustness across LLMs.\n  Our research introduces novel methods for identifying and using hard\ndistracting passages to improve RAG systems. By fine-tuning LLMs with these\ncarefully selected distracting passages, we achieve up to a 7.5% increase in\nanswering accuracy compared to counterparts fine-tuned on conventional RAG\ndatasets. Our contribution is two-fold: first, we move beyond the simple binary\nclassification of irrelevant passages as either completely unrelated vs.\ndistracting, and second, we develop and analyze multiple methods for finding\nhard distracting passages. To our knowledge, no other research has provided\nsuch a comprehensive framework for identifying and utilizing hard distracting\npassages.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e2d\u65e0\u5173\u6bb5\u843d\u5206\u6563LLM\u6ce8\u610f\u529b\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u91cf\u5316\u5e72\u6270\u6548\u5e94\u7684\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u786c\u5e72\u6270\u6bb5\u843d\u4f18\u5316RAG\u7cfb\u7edf\uff0c\u5b9e\u73b0\u56de\u7b54\u51c6\u786e\u7387\u63d0\u53477.5%\u3002", "motivation": "\u89e3\u51b3RAG\u7cfb\u7edf\u4e2d\u65e0\u5173\u6bb5\u843d\u5e72\u6270LLM\u5bfc\u81f4\u9519\u8bef\u56de\u7b54\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u91cf\u5316\u5e72\u6270\u6548\u5e94\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u91cf\u5316\u5e72\u6270\u6548\u5e94\u7684\u6307\u6807\uff0c\u8bbe\u8ba1\u591a\u79cd\u65b9\u6cd5\u8bc6\u522b\u786c\u5e72\u6270\u6bb5\u843d\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u6bb5\u843d\u5fae\u8c03LLM\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4f20\u7edfRAG\u6570\u636e\u96c6\u76f8\u6bd4\uff0c\u4f7f\u7528\u786c\u5e72\u6270\u6bb5\u843d\u5fae\u8c03\u7684LLM\u56de\u7b54\u51c6\u786e\u7387\u63d0\u5347\u8fbe7.5%\u3002", "conclusion": "\u672c\u6587\u4e0d\u4ec5\u6269\u5c55\u4e86\u5bf9\u65e0\u5173\u6bb5\u843d\u7684\u5206\u7c7b\uff08\u4ece\u4e8c\u5143\u5230\u5e72\u6270\u6548\u5e94\u91cf\u5316\uff09\uff0c\u8fd8\u4e3a\u8bc6\u522b\u548c\u5229\u7528\u786c\u5e72\u6270\u6bb5\u843d\u63d0\u4f9b\u4e86\u5168\u9762\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86RAG\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2505.06284", "pdf": "https://arxiv.org/pdf/2505.06284", "abs": "https://arxiv.org/abs/2505.06284", "authors": ["Zhiqiang Wang", "Ruoxi Cheng"], "title": "DMRL: Data- and Model-aware Reward Learning for Data Extraction", "categories": ["cs.LG", "cs.CR"], "comment": "Data- and Model-aware Reward Learning for Data Extraction. arXiv\n  admin note: substantial text overlap with arXiv:2503.18991", "summary": "Large language models (LLMs) are inherently vulnerable to unintended privacy\nbreaches. Consequently, systematic red-teaming research is essential for\ndeveloping robust defense mechanisms. However, current data extraction methods\nsuffer from several limitations: (1) rely on dataset duplicates (addressable\nvia deduplication), (2) depend on prompt engineering (now countered by\ndetection and defense), and (3) rely on random-search adversarial generation.\nTo address these challenges, we propose DMRL, a Data- and Model-aware Reward\nLearning approach for data extraction. This technique leverages inverse\nreinforcement learning to extract sensitive data from LLMs. Our method consists\nof two main components: (1) constructing an introspective reasoning dataset\nthat captures leakage mindsets to guide model behavior, and (2) training reward\nmodels with Group Relative Policy Optimization (GRPO), dynamically tuning\noptimization based on task difficulty at both the data and model levels.\nComprehensive experiments across various LLMs demonstrate that DMRL outperforms\nall baseline methods in data extraction performance.", "AI": {"tldr": "DMRL\uff08\u6570\u636e\u4e0e\u6a21\u578b\u611f\u77e5\u5956\u52b1\u5b66\u4e60\uff09\u65b9\u6cd5\u901a\u8fc7\u9006\u5411\u5f3a\u5316\u5b66\u4e60\u548c\u52a8\u6001\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u654f\u611f\u6570\u636e\u7684\u63d0\u53d6\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7531\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u7ea2\u961f\u7814\u7a76\u6765\u5f00\u53d1\u9632\u5fa1\u673a\u5236\u3002\u5f53\u524d\u7684\u6570\u636e\u63d0\u53d6\u65b9\u6cd5\u5b58\u5728\u4f9d\u8d56\u6570\u636e\u96c6\u91cd\u590d\u3001\u63d0\u793a\u5de5\u7a0b\u548c\u968f\u673a\u641c\u7d22\u5bf9\u6297\u751f\u6210\u7b49\u9650\u5236\u3002", "method": "DMRL\u91c7\u7528\u9006\u5411\u5f3a\u5316\u5b66\u4e60\uff0c\u6784\u5efa\u5185\u7701\u63a8\u7406\u6570\u636e\u96c6\u4ee5\u5f15\u5bfc\u6a21\u578b\u884c\u4e3a\uff0c\u5e76\u5229\u7528GRPO\uff08\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff09\u52a8\u6001\u8c03\u6574\u5956\u52b1\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDMRL\u5728\u591a\u79cdLLMs\u4e0a\u7684\u6570\u636e\u63d0\u53d6\u6027\u80fd\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DMRL\u901a\u8fc7\u6570\u636e\u4e0e\u6a21\u578b\u611f\u77e5\u7684\u5956\u52b1\u5b66\u4e60\uff0c\u4e3aLLMs\u7684\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u6570\u636e\u63d0\u53d6\u65b9\u6848\u3002"}}
{"id": "2505.06964", "pdf": "https://arxiv.org/pdf/2505.06964", "abs": "https://arxiv.org/abs/2505.06964", "authors": ["Gaurab Sarkar", "Sougata Saha"], "title": "From Knowledge to Reasoning: Evaluating LLMs for Ionic Liquids Research in Chemical and Biological Engineering", "categories": ["cs.AI"], "comment": null, "summary": "Although Large Language Models (LLMs) have achieved remarkable performance in\ndiverse general knowledge and reasoning tasks, their utility in the scientific\ndomain of Chemical and Biological Engineering (CBE) is unclear. Hence, it\nnecessitates challenging evaluation benchmarks that can measure LLM performance\nin knowledge- and reasoning-based tasks, which is lacking. As a foundational\nstep, we empirically measure the reasoning capabilities of LLMs in CBE. We\nconstruct and share an expert-curated dataset of 5,920 examples for\nbenchmarking LLMs' reasoning capabilities in the niche domain of Ionic Liquids\n(ILs) for carbon sequestration, an emergent solution to reducing global\nwarming. The dataset presents different difficulty levels by varying along the\ndimensions of linguistic and domain-specific knowledge. Benchmarking three less\nthan 10B parameter open-source LLMs on the dataset suggests that while smaller\ngeneral-purpose LLMs are knowledgeable about ILs, they lack domain-specific\nreasoning capabilities. Based on our results, we further discuss considerations\nfor leveraging LLMs for carbon capture research using ILs. Since LLMs have a\nhigh carbon footprint, gearing them for IL research can symbiotically benefit\nboth fields and help reach the ambitious carbon neutrality target by 2050.\nDataset link: https://github.com/sougata-ub/llms_for_ionic_liquids", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5316\u5b66\u4e0e\u751f\u7269\u5de5\u7a0b\uff08CBE\uff09\u9886\u57df\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u9488\u5bf9\u79bb\u5b50\u6db2\u4f53\uff08ILs\uff09\u7528\u4e8e\u78b3\u5c01\u5b58\u7684\u7814\u7a76\u3002\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u4e13\u5bb6\u7b56\u5212\u7684\u6570\u636e\u96c6\uff085,920\u4e2a\u793a\u4f8b\uff09\u5e76\u6d4b\u8bd5\u4e86\u4e09\u79cd\u5f00\u6e90LLMs\uff0c\u53d1\u73b0\u867d\u7136\u5c0f\u89c4\u6a21\u901a\u7528LLMs\u5bf9ILs\u6709\u4e00\u5b9a\u77e5\u8bc6\uff0c\u4f46\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u5e7f\u6cdb\u7684\u77e5\u8bc6\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5728CBE\u9886\u57df\u7684\u5b9e\u7528\u6027\u5c1a\u4e0d\u660e\u786e\u3002\u8be5\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u6784\u5efa\u8bc4\u4f30\u57fa\u51c6\u6765\u8861\u91cfLLMs\u5728ILs\u9886\u57df\u7684\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b5,920\u4e2a\u793a\u4f8b\u7684\u4e13\u5bb6\u7b56\u5212\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e0d\u540c\u96be\u5ea6\u6c34\u5e73\uff08\u8bed\u8a00\u548c\u9886\u57df\u77e5\u8bc6\u7ef4\u5ea6\uff09\uff0c\u5e76\u6d4b\u8bd5\u4e86\u4e09\u79cd\u53c2\u6570\u91cf\u5c0f\u4e8e10B\u7684\u5f00\u6e90LLMs\u3002", "result": "\u6d4b\u8bd5\u8868\u660e\uff0c\u5c0f\u89c4\u6a21\u901a\u7528LLMs\u5bf9ILs\u6709\u4e00\u5b9a\u77e5\u8bc6\uff0c\u4f46\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u8ba8\u8bba\u4e86\u5982\u4f55\u5229\u7528LLMs\u63a8\u52a8ILs\u7684\u78b3\u6355\u83b7\u7814\u7a76\uff0c\u5e76\u6307\u51fa\u8fd9\u4e00\u65b9\u5411\u53ef\u80fd\u4e3a\u78b3\u8fbe\u5cf0\u76ee\u6807\u63d0\u4f9b\u53cc\u8d62\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.06974", "pdf": "https://arxiv.org/pdf/2505.06974", "abs": "https://arxiv.org/abs/2505.06974", "authors": ["Daichi Kohmoto", "Katsutoshi Fukuda", "Daisuke Yoshida", "Takafumi Matsui", "Sachihiro Omura"], "title": "CNN-based Image Models Verify a Hypothesis that The Writers of Cuneiform Texts Improved Their Writing Skills When Studying at the Age of Hittite Empire", "categories": ["cs.CL"], "comment": "11 pages, 9 figures, 5 tables", "summary": "A cuneiform tablet KBo 23.1 ++/KUB 30.38, which is known to represent a text\nof Kizzuwatna rituals, was written by two writers with almost identical content\nin two iterations. Unlike other cuneiform tablets that contained information\nsuch as myths, essays, or business records, the reason why ancient people left\nsuch tablets for posterity remains unclear. To study this problem, we develop a\nnew methodology by analyzing images of a tablet quantitatively using CNN\n(Convolutional Neural Network)-based image models, without segmenting\ncuneiforms one-by-one. Our data-driven methodology implies that the writer\nwriting the first half was a `teacher' and the other writer was a `student' who\nwas training his skills of writing cuneiforms. This result has not been reached\nby classical linguistics. We also discuss related conclusions and possible\nfurther directions for applying our method and its generalizations.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7CNN\u56fe\u50cf\u5206\u6790\u53d1\u73b0\uff0c\u523b\u6709Kizzuwatna\u4eea\u5f0f\u7684\u6954\u5f62\u6587\u5b57\u677f\u53ef\u80fd\u662f\u5e08\u5f92\u7ec3\u4e60\u7684\u8bb0\u5f55\uff0c\u4f20\u7edf\u8bed\u8a00\u5b66\u672a\u80fd\u5f97\u51fa\u6b64\u7ed3\u8bba\u3002", "motivation": "\u7814\u7a76\u53e4\u4ee3\u6954\u5f62\u6587\u5b57\u677f\u91cd\u590d\u523b\u5199\u7684\u539f\u56e0\uff0c\u5c24\u5176\u662fKBo 23.1 ++/KUB 30.38\u8fd9\u7c7b\u975e\u795e\u8bdd\u6216\u5546\u4e1a\u7528\u9014\u7684\u6587\u672c\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eCNN\u7684\u56fe\u50cf\u6a21\u578b\u5b9a\u91cf\u5206\u6790\u6587\u5b57\u677f\u56fe\u50cf\uff0c\u65e0\u9700\u9010\u5b57\u5206\u5272\u6954\u5f62\u6587\u5b57\u3002", "result": "\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u663e\u793a\u7b2c\u4e00\u90e8\u5206\u7531\u201c\u8001\u5e08\u201d\u4e66\u5199\uff0c\u7b2c\u4e8c\u90e8\u5206\u7531\u201c\u5b66\u751f\u201d\u7ec3\u4e60\u4e66\u5199\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6954\u5f62\u6587\u5b57\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u53ef\u80fd\u63a8\u5e7f\u5230\u5176\u4ed6\u7c7b\u4f3c\u6587\u672c\u7684\u5206\u6790\u3002"}}
{"id": "2505.06288", "pdf": "https://arxiv.org/pdf/2505.06288", "abs": "https://arxiv.org/abs/2505.06288", "authors": ["Zihao Chen", "Wenyong Wang", "Jiachen Yang", "Yu Xiang"], "title": "IIKL: Isometric Immersion Kernel Learning with Riemannian Manifold for Geometric Preservation", "categories": ["cs.LG", "stat.ML"], "comment": "16 pages, 14 figures", "summary": "Geometric representation learning in preserving the intrinsic geometric and\ntopological properties for discrete non-Euclidean data is crucial in scientific\napplications. Previous research generally mapped non-Euclidean discrete data\ninto Euclidean space during representation learning, which may lead to the loss\nof some critical geometric information. In this paper, we propose a novel\nIsometric Immersion Kernel Learning (IIKL) method to build Riemannian manifold\nand isometrically induce Riemannian metric from discrete non-Euclidean data. We\nprove that Isometric immersion is equivalent to the kernel function in the\ntangent bundle on the manifold, which explicitly guarantees the invariance of\nthe inner product between vectors in the arbitrary tangent space throughout the\nlearning process, thus maintaining the geometric structure of the original\ndata. Moreover, a novel parameterized learning model based on IIKL is\nintroduced, and an alternating training method for this model is derived using\nMaximum Likelihood Estimation (MLE), ensuring efficient convergence.\nExperimental results proved that using the learned Riemannian manifold and its\nmetric, our model preserved the intrinsic geometric representation of data in\nboth 3D and high-dimensional datasets successfully, and significantly improved\nthe accuracy of downstream tasks, such as data reconstruction and\nclassification. It is showed that our method could reduce the inner product\ninvariant loss by more than 90% compared to state-of-the-art (SOTA) methods,\nalso achieved an average 40% improvement in downstream reconstruction accuracy\nand a 90% reduction in error for geometric metrics involving isometric and\nconformal.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7b49\u8ddd\u6d78\u5165\u6838\u5b66\u4e60\uff08IIKL\uff09\u65b9\u6cd5\uff0c\u6784\u5efa\u9ece\u66fc\u6d41\u5f62\u5e76\u4ece\u79bb\u6563\u975e\u6b27\u6570\u636e\u4e2d\u5bfc\u51fa\u9ece\u66fc\u5ea6\u91cf\uff0c\u4fdd\u6301\u6570\u636e\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4e0b\u6e38\u4efb\u52a1\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5c06\u975e\u6b27\u79bb\u6563\u6570\u636e\u6620\u5c04\u5230\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\uff0c\u53ef\u80fd\u5bfc\u81f4\u5173\u952e\u51e0\u4f55\u4fe1\u606f\u4e22\u5931\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u7b49\u8ddd\u6d78\u5165\u6280\u672f\u4fdd\u6301\u6570\u636e\u7684\u672c\u5f81\u51e0\u4f55\u548c\u62d3\u6251\u6027\u8d28\u3002", "method": "\u63d0\u51faIIKL\u65b9\u6cd5\uff0c\u8bc1\u660e\u7b49\u8ddd\u6d78\u5165\u7b49\u4ef7\u4e8e\u6d41\u5f62\u5207\u4e1b\u4e0a\u7684\u6838\u51fd\u6570\uff0c\u5e76\u57fa\u4e8e\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u8bbe\u8ba1\u4e86\u53c2\u6570\u5316\u5b66\u4e60\u6a21\u578b\u4e0e\u4ea4\u66ff\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u57283D\u548c\u9ad8\u7ef4\u6570\u636e\u4e2d\u6210\u529f\u4fdd\u6301\u4e86\u51e0\u4f55\u8868\u5f81\uff0c\u5185\u79ef\u4e0d\u53d8\u635f\u5931\u964d\u4f4e90%\u4ee5\u4e0a\uff0c\u91cd\u5efa\u7cbe\u5ea6\u5e73\u5747\u63d0\u534740%\uff0c\u7b49\u8ddd\u4e0e\u5171\u5f62\u51e0\u4f55\u8bef\u5dee\u51cf\u5c1190%\u3002", "conclusion": "IIKL\u5728\u4fdd\u7559\u6570\u636e\u51e0\u4f55\u7ed3\u6784\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u4e3a\u79d1\u5b66\u5e94\u7528\u4e2d\u7684\u975e\u6b27\u6570\u636e\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.06977", "pdf": "https://arxiv.org/pdf/2505.06977", "abs": "https://arxiv.org/abs/2505.06977", "authors": ["Wenju Sun", "Qingyong Li", "Yangli-ao Geng", "Boyang Li"], "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Multi-task model merging offers a promising paradigm for integrating multiple\nexpert models into a unified model without additional training. Existing\nstate-of-the-art techniques, such as Task Arithmetic and its variants, merge\nmodels by accumulating task vectors -- the parameter differences between\npretrained and finetuned models. However, task vector accumulation is often\nhindered by knowledge conflicts, leading to performance degradation. To address\nthis challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel\ntraining-free framework that selectively trims conflict-prone components from\nthe task vectors. CAT Merging introduces several parameter-specific strategies,\nincluding projection for linear weights and masking for scaling and shifting\nparameters in normalization layers. Extensive experiments on vision, language,\nand vision-language tasks demonstrate that CAT Merging effectively suppresses\nknowledge conflicts, achieving average accuracy improvements of up to 2.5%\n(ViT-B/32) and 2.0% (ViT-L/14) over state-of-the-art methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Conflict-Aware Task Merging\uff08CAT Merging\uff09\uff0c\u4e00\u79cd\u65e0\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u4fee\u526a\u51b2\u7a81\u7ec4\u4ef6\u6765\u89e3\u51b3\u591a\u4efb\u52a1\u6a21\u578b\u5408\u5e76\u4e2d\u7684\u77e5\u8bc6\u51b2\u7a81\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4efb\u52a1\u5411\u91cf\u7d2f\u79ef\u65b9\u6cd5\u5728\u5408\u5e76\u591a\u4efb\u52a1\u6a21\u578b\u65f6\u56e0\u77e5\u8bc6\u51b2\u7a81\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u4e9f\u9700\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u65b9\u6cd5\u6765\u6291\u5236\u51b2\u7a81\u3002", "method": "\u63d0\u51faCAT Merging\u6846\u67b6\uff0c\u91c7\u7528\u53c2\u6570\u7279\u5b9a\u7b56\u7565\u4fee\u526a\u4efb\u52a1\u5411\u91cf\u4e2d\u7684\u51b2\u7a81\u7ec4\u4ef6\uff0c\u5982\u7ebf\u6027\u6743\u91cd\u7684\u6295\u5f71\u548c\u5f52\u4e00\u5316\u5c42\u53c2\u6570\u7684\u63a9\u7801\u5904\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCAT Merging\u5728\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u5e73\u5747\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u53472.5%\uff08ViT-B/32\uff09\u548c2.0%\uff08ViT-L/14\uff09\u3002", "conclusion": "CAT Merging\u6709\u6548\u89e3\u51b3\u4e86\u77e5\u8bc6\u51b2\u7a81\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u6a21\u578b\u5408\u5e76\u7684\u6027\u80fd\uff0c\u4e3a\u65e0\u8bad\u7ec3\u6846\u67b6\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.06987", "pdf": "https://arxiv.org/pdf/2505.06987", "abs": "https://arxiv.org/abs/2505.06987", "authors": ["Xiaoyu Wang", "Yue Zhao", "Qingqing Gu", "Zhonglin Jiang", "Xiaokai Chen", "Yong Chen", "Luo Ji"], "title": "Convert Language Model into a Value-based Strategic Planner", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 5 figures, Accepted by ACL 2025 Industry Track", "summary": "Emotional support conversation (ESC) aims to alleviate the emotional distress\nof individuals through effective conversations. Although large language models\n(LLMs) have obtained remarkable progress on ESC, most of these studies might\nnot define the diagram from the state model perspective, therefore providing a\nsuboptimal solution for long-term satisfaction. To address such an issue, we\nleverage the Q-learning on LLMs, and propose a framework called straQ*. Our\nframework allows a plug-and-play LLM to bootstrap the planning during ESC,\ndetermine the optimal strategy based on long-term returns, and finally guide\nthe LLM to response. Substantial experiments on ESC datasets suggest that\nstraQ* outperforms many baselines, including direct inference, self-refine,\nchain of thought, finetuning, and finite state machines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3astraQ*\u7684\u6846\u67b6\uff0c\u901a\u8fc7Q\u5b66\u4e60\u4f18\u5316LLMs\u5728\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u4e2d\u7684\u957f\u671f\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u672a\u4ece\u72b6\u6001\u6a21\u578b\u89d2\u5ea6\u4f18\u5316LLMs\u5728\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u4e2d\u7684\u957f\u671f\u6ee1\u610f\u5ea6\uff0c\u5bfc\u81f4\u89e3\u51b3\u65b9\u6848\u4e0d\u591f\u7406\u60f3\u3002", "method": "\u7ed3\u5408Q\u5b66\u4e60\u548cLLMs\uff0c\u8bbe\u8ba1\u53ef\u5373\u63d2\u5373\u7528\u7684straQ*\u6846\u67b6\uff0c\u901a\u8fc7\u957f\u671f\u56de\u62a5\u9009\u62e9\u6700\u4f18\u7b56\u7565\u5e76\u6307\u5bfcLLMs\u751f\u6210\u56de\u5e94\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cstraQ*\u8868\u73b0\u4f18\u4e8e\u76f4\u63a5\u63a8\u7406\u3001\u81ea\u4f18\u5316\u3001\u601d\u7ef4\u94fe\u3001\u5fae\u8c03\u548c\u6709\u9650\u72b6\u6001\u673a\u7b49\u65b9\u6cd5\u3002", "conclusion": "straQ*\u663e\u8457\u63d0\u5347\u4e86\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u7684\u957f\u671f\u6548\u679c\uff0c\u9a8c\u8bc1\u4e86Q\u5b66\u4e60\u5728\u4f18\u5316LLMs\u5bf9\u8bdd\u7b56\u7565\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.06289", "pdf": "https://arxiv.org/pdf/2505.06289", "abs": "https://arxiv.org/abs/2505.06289", "authors": ["Sotirios Athanasoulias"], "title": "Edge-Optimized Deep Learning & Pattern Recognition Techniques for Non-Intrusive Load Monitoring of Energy Time Series", "categories": ["cs.LG", "eess.SP", "stat.ML"], "comment": "PhD dissertation as part of the GECKO Marie Curie", "summary": "The growing global energy demand and the urgent need for sustainability call\nfor innovative ways to boost energy efficiency. While advanced energy-saving\nsystems exist, they often fall short without user engagement. Providing\nfeedback on energy consumption behavior is key to promoting sustainable\npractices. Non-Intrusive Load Monitoring (NILM) offers a promising solution by\ndisaggregating total household energy usage, recorded by a central smart meter,\ninto appliance-level data. This empowers users to optimize consumption.\nAdvances in AI, IoT, and smart meter adoption have further enhanced NILM's\npotential.\n  Despite this promise, real-world NILM deployment faces major challenges.\nFirst, existing datasets mainly represent regions like the USA and UK, leaving\nplaces like the Mediterranean underrepresented. This limits understanding of\nregional consumption patterns, such as heavy use of air conditioners and\nelectric water heaters. Second, deep learning models used in NILM require high\ncomputational power, often relying on cloud services. This increases costs,\nraises privacy concerns, and limits scalability, especially for households with\npoor connectivity. This thesis tackles these issues with key contributions. It\npresents an interoperable data collection framework and introduces the Plegma\nDataset, focused on underrepresented Mediterranean energy patterns. It also\nexplores advanced deep neural networks and model compression techniques for\nefficient edge deployment. By bridging theoretical advances with practical\nneeds, this work aims to make NILM scalable, efficient, and adaptable for\nglobal energy sustainability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u4fb5\u5165\u5f0f\u8d1f\u8377\u76d1\u6d4b\uff08NILM\uff09\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7AI\u548cIoT\u6280\u672f\u5206\u89e3\u5bb6\u5ead\u80fd\u6e90\u4f7f\u7528\u6570\u636e\uff0c\u5e76\u9488\u5bf9\u5730\u4e2d\u6d77\u5730\u533a\u6570\u636e\u4e0d\u8db3\u548c\u90e8\u7f72\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u6570\u636e\u96c6\u548c\u6539\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "motivation": "\u5168\u7403\u80fd\u6e90\u9700\u6c42\u589e\u957f\u548c\u53ef\u6301\u7eed\u53d1\u5c55\u9700\u6c42\u4fc3\u4f7f\u7814\u7a76\u66f4\u9ad8\u6548\u7684\u80fd\u6e90\u7ba1\u7406\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u6280\u672f\u7f3a\u4e4f\u7528\u6237\u53c2\u4e0e\u548c\u533a\u57df\u4ee3\u8868\u6027\uff0c\u5c24\u5176\u662f\u5730\u4e2d\u6d77\u5730\u533a\u7684\u80fd\u6e90\u4f7f\u7528\u6a21\u5f0f\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u4e92\u64cd\u4f5c\u7684\u6570\u636e\u6536\u96c6\u6846\u67b6\u548cPlegma\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528\u5148\u8fdb\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u53ca\u6a21\u578b\u538b\u7f29\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u6548\u7387\u3002", "result": "\u901a\u8fc7\u5f15\u5165\u5730\u4e2d\u6d77\u5730\u533a\u7684\u6570\u636e\u96c6\u548c\u4f18\u5316\u540e\u7684\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86NILM\u7684\u533a\u57df\u9002\u5e94\u6027\u548c\u90e8\u7f72\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8df5\u7684\u7ed3\u5408\uff0c\u4f7fNILM\u6280\u672f\u66f4\u5177\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u5168\u7403\u80fd\u6e90\u53ef\u6301\u7eed\u53d1\u5c55\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.06997", "pdf": "https://arxiv.org/pdf/2505.06997", "abs": "https://arxiv.org/abs/2505.06997", "authors": ["Wenhao Lu", "Zhengqiu Zhu", "Yong Zhao", "Yonglin Tian", "Junjie Zeng", "Jun Zhang", "Zhong Liu", "Fei-Yue Wang"], "title": "A Multi-Agent Reinforcement Learning Approach for Cooperative Air-Ground-Human Crowdsensing in Emergency Rescue", "categories": ["cs.AI"], "comment": null, "summary": "Mobile crowdsensing is evolving beyond traditional human-centric models by\nintegrating heterogeneous entities like unmanned aerial vehicles (UAVs) and\nunmanned ground vehicles (UGVs). Optimizing task allocation among these diverse\nagents is critical, particularly in challenging emergency rescue scenarios\ncharacterized by complex environments, limited communication, and partial\nobservability. This paper tackles the Heterogeneous-Entity\nCollaborative-Sensing Task Allocation (HECTA) problem specifically for\nemergency rescue, considering humans, UAVs, and UGVs. We introduce a novel\n``Hard-Cooperative'' policy where UGVs prioritize recharging low-battery UAVs,\nalongside performing their sensing tasks. The primary objective is maximizing\nthe task completion rate (TCR) under strict time constraints. We rigorously\nformulate this NP-hard problem as a decentralized partially observable Markov\ndecision process (Dec-POMDP) to effectively handle sequential decision-making\nunder uncertainty. To solve this, we propose HECTA4ER, a novel multi-agent\nreinforcement learning algorithm built upon a Centralized Training with\nDecentralized Execution architecture. HECTA4ER incorporates tailored designs,\nincluding specialized modules for complex feature extraction, utilization of\naction-observation history via hidden states, and a mixing network integrating\nglobal and local information, specifically addressing the challenges of partial\nobservability. Furthermore, theoretical analysis confirms the algorithm's\nconvergence properties. Extensive simulations demonstrate that HECTA4ER\nsignificantly outperforms baseline algorithms, achieving an average 18.42%\nincrease in TCR. Crucially, a real-world case study validates the algorithm's\neffectiveness and robustness in dynamic sensing scenarios, highlighting its\nstrong potential for practical application in emergency response.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHECTA4ER\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5f02\u6784\u5b9e\u4f53\uff08\u5982\u4eba\u3001\u65e0\u4eba\u673a\u548c\u65e0\u4eba\u8f66\uff09\u5728\u7d27\u6025\u6551\u63f4\u573a\u666f\u4e2d\u7684\u4efb\u52a1\u5206\u914d\u95ee\u9898\uff08HECTA\uff09\uff0c\u901a\u8fc7'Hard-Cooperative'\u7b56\u7565\u548c\u5168\u5c40-\u5c40\u90e8\u4fe1\u606f\u878d\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u5b8c\u6210\u7387\uff08TCR\uff09\u3002", "motivation": "\u5728\u7d27\u6025\u6551\u63f4\u7b49\u590d\u6742\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u7684\u4eba\u7c7b\u4e2d\u5fc3\u6a21\u578b\u65e0\u6cd5\u9ad8\u6548\u6574\u5408\u5f02\u6784\u5b9e\u4f53\uff08\u5982\u65e0\u4eba\u673a\u548c\u65e0\u4eba\u8f66\uff09\u7684\u534f\u4f5c\u611f\u77e5\u4efb\u52a1\u5206\u914d\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u591f\u5904\u7406\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u548c\u901a\u4fe1\u9650\u5236\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u8bba\u6587\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u5206\u6563\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08Dec-POMDP\uff09\uff0c\u5e76\u63d0\u51fa\u4e86HECTA4ER\u7b97\u6cd5\uff0c\u91c7\u7528\u96c6\u4e2d\u8bad\u7ec3\u5206\u6563\u6267\u884c\u67b6\u6784\uff0c\u7ed3\u5408\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u3001\u5386\u53f2\u52a8\u4f5c-\u89c2\u6d4b\u5229\u7528\u548c\u5168\u5c40-\u5c40\u90e8\u4fe1\u606f\u6df7\u5408\u7f51\u7edc\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\uff0cHECTA4ER\u5e73\u5747\u4efb\u52a1\u5b8c\u6210\u7387\uff08TCR\uff09\u6bd4\u57fa\u7ebf\u7b97\u6cd5\u63d0\u9ad8\u4e8618.42%\uff0c\u5e76\u5728\u771f\u5b9e\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u52a8\u6001\u611f\u77e5\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "HECTA4ER\u7b97\u6cd5\u5728\u5f02\u6784\u5b9e\u4f53\u534f\u4f5c\u611f\u77e5\u4efb\u52a1\u5206\u914d\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u7d27\u6025\u6551\u63f4\u7b49\u9ad8\u52a8\u6001\u73af\u5883\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.07157", "pdf": "https://arxiv.org/pdf/2505.07157", "abs": "https://arxiv.org/abs/2505.07157", "authors": ["Hajar Sakai", "Sarah S. Lam"], "title": "HAMLET: Healthcare-focused Adaptive Multilingual Learning Embedding-based Topic Modeling", "categories": ["cs.CL"], "comment": null, "summary": "Traditional topic models often struggle with contextual nuances and fail to\nadequately handle polysemy and rare words. This limitation typically results in\ntopics that lack coherence and quality. Large Language Models (LLMs) can\nmitigate this issue by generating an initial set of topics. However, these raw\ntopics frequently lack refinement and representativeness, which leads to\nredundancy without lexical similarity and reduced interpretability. This paper\nintroduces HAMLET, a graph-driven architecture for cross-lingual healthcare\ntopic modeling that uses LLMs. The proposed approach leverages neural-enhanced\nsemantic fusion to refine the embeddings of topics generated by the LLM.\nInstead of relying solely on statistical co-occurrence or human interpretation\nto extract topics from a document corpus, this method introduces a topic\nembedding refinement that uses Bidirectional Encoder Representations from\nTransformers (BERT) and Graph Neural Networks (GNN). After topic generation, a\nhybrid technique that involves BERT and Sentence-BERT (SBERT) is employed for\nembedding. The topic representations are further refined using a GNN, which\nestablishes connections between documents, topics, words, similar topics, and\nsimilar words. A novel method is introduced to compute similarities.\nConsequently, the topic embeddings are refined, and the top k topics are\nextracted. Experiments were conducted using two healthcare datasets, one in\nEnglish and one in French, from which six sets were derived. The results\ndemonstrate the effectiveness of HAMLET.", "AI": {"tldr": "HAMLET\u662f\u4e00\u79cd\u8de8\u8bed\u8a00\u533b\u7597\u4e3b\u9898\u5efa\u6a21\u7684\u56fe\u9a71\u52a8\u67b6\u6784\uff0c\u5229\u7528LLM\u751f\u6210\u521d\u59cb\u4e3b\u9898\u5e76\u901a\u8fc7BERT\u548cGNN\u4f18\u5316\u4e3b\u9898\u5d4c\u5165\uff0c\u63d0\u5347\u4e3b\u9898\u8d28\u91cf\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u4e3b\u9898\u6a21\u578b\u96be\u4ee5\u5904\u7406\u4e0a\u4e0b\u6587\u590d\u6742\u6027\u548c\u591a\u4e49\u8bcd/\u7f55\u89c1\u8bcd\uff0c\u5bfc\u81f4\u4e3b\u9898\u8d28\u91cf\u4f4e\uff1bLLM\u751f\u6210\u7684\u4e3b\u9898\u867d\u521d\u6b65\u4f46\u4ecd\u9700\u4f18\u5316\u3002", "method": "\u7ed3\u5408BERT\u548cGNN\u8fdb\u884c\u4e3b\u9898\u5d4c\u5165\u4f18\u5316\uff1a\u5148\u7528BERT/SBERT\u5d4c\u5165\uff0c\u518d\u901a\u8fc7GNN\u5efa\u7acb\u6587\u6863\u3001\u4e3b\u9898\u3001\u8bcd\u6c47\u95f4\u7684\u5173\u8054\uff0c\u6700\u7ec8\u63d0\u53d6\u524dk\u4e2a\u4e3b\u9898\u3002", "result": "\u5728\u82f1\u6cd5\u533b\u7597\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86HAMLET\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e3b\u9898\u8d28\u91cf\u3002", "conclusion": "HAMLET\u901a\u8fc7\u795e\u7ecf\u589e\u5f3a\u8bed\u4e49\u878d\u5408\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u8de8\u8bed\u8a00\u4e3b\u9898\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.06290", "pdf": "https://arxiv.org/pdf/2505.06290", "abs": "https://arxiv.org/abs/2505.06290", "authors": ["Zefang Zong", "Xiaochen Wei", "Guozhen Zhang", "Chen Gao", "Huandong Wang", "Yong Li"], "title": "UniCO: Towards a Unified Model for Combinatorial Optimization Problems", "categories": ["cs.LG", "cs.DM"], "comment": null, "summary": "Combinatorial Optimization (CO) encompasses a wide range of problems that\narise in many real-world scenarios. While significant progress has been made in\ndeveloping learning-based methods for specialized CO problems, a unified model\nwith a single architecture and parameter set for diverse CO problems remains\nelusive. Such a model would offer substantial advantages in terms of efficiency\nand convenience. In this paper, we introduce UniCO, a unified model for solving\nvarious CO problems. Inspired by the success of next-token prediction, we frame\neach problem-solving process as a Markov Decision Process (MDP), tokenize the\ncorresponding sequential trajectory data, and train the model using a\ntransformer backbone. To reduce token length in the trajectory data, we propose\na CO-prefix design that aggregates static problem features. To address the\nheterogeneity of state and action tokens within the MDP, we employ a two-stage\nself-supervised learning approach. In this approach, a dynamic prediction model\nis first trained and then serves as a pre-trained model for subsequent policy\ngeneration. Experiments across 10 CO problems showcase the versatility of\nUniCO, emphasizing its ability to generalize to new, unseen problems with\nminimal fine-tuning, achieving even few-shot or zero-shot performance. Our\nframework offers a valuable complement to existing neural CO methods that focus\non optimizing performance for individual problems.", "AI": {"tldr": "UniCO\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u7ec4\u5408\u4f18\u5316\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u95ee\u9898\u89e3\u51b3\u8fc7\u7a0b\u8f6c\u5316\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u5e76\u5229\u7528Transformer\u67b6\u6784\uff0c\u80fd\u591f\u5728\u5c11\u91cf\u5fae\u8c03\u4e0b\u6cdb\u5316\u5230\u65b0\u7684\u672a\u89c1\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5b66\u4e60\u578b\u65b9\u6cd5\u591a\u4e3a\u7279\u5b9a\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002UniCO\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u5355\u4e00\u67b6\u6784\u548c\u53c2\u6570\u96c6\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u63d0\u5347\u6548\u7387\u548c\u4fbf\u6377\u6027\u3002", "method": "\u5c06\u95ee\u9898\u89e3\u51b3\u8fc7\u7a0b\u5efa\u6a21\u4e3aMDP\uff0c\u5bf9\u5e8f\u5217\u8f68\u8ff9\u6570\u636e\u8fdb\u884c\u6807\u8bb0\u5316\uff0c\u4f7f\u7528Transformer\u9aa8\u5e72\u548c\u4e24\u9636\u6bb5\u81ea\u76d1\u7763\u5b66\u4e60\uff08\u52a8\u6001\u9884\u6d4b\u6a21\u578b\u9884\u8bad\u7ec3\u540e\u751f\u6210\u7b56\u7565\uff09\u3002", "result": "\u572810\u4e2a\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e0a\u9a8c\u8bc1\u4e86UniCO\u7684\u901a\u7528\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5c11\u91cf\u6837\u672c\u751a\u81f3\u96f6\u6837\u672c\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "UniCO\u4e3a\u4e13\u6ce8\u4e8e\u5355\u4e2a\u95ee\u9898\u6027\u80fd\u4f18\u5316\u7684\u73b0\u6709\u795e\u7ecf\u7ec4\u5408\u4f18\u5316\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8865\u5145\u3002"}}
{"id": "2505.07005", "pdf": "https://arxiv.org/pdf/2505.07005", "abs": "https://arxiv.org/abs/2505.07005", "authors": ["Bowen Long", "Enjie Liu", "Renxi Qiu", "Yanqing Duan"], "title": "Explainable AI the Latest Advancements and New Trends", "categories": ["cs.AI"], "comment": null, "summary": "In recent years, Artificial Intelligence technology has excelled in various\napplications across all domains and fields. However, the various algorithms in\nneural networks make it difficult to understand the reasons behind decisions.\nFor this reason, trustworthy AI techniques have started gaining popularity. The\nconcept of trustworthiness is cross-disciplinary; it must meet societal\nstandards and principles, and technology is used to fulfill these requirements.\nIn this paper, we first surveyed developments from various countries and\nregions on the ethical elements that make AI algorithms trustworthy; and then\nfocused our survey on the state of the art research into the interpretability\nof AI. We have conducted an intensive survey on technologies and techniques\nused in making AI explainable. Finally, we identified new trends in achieving\nexplainable AI. In particular, we elaborate on the strong link between the\nexplainability of AI and the meta-reasoning of autonomous systems. The concept\nof meta-reasoning is 'reason the reasoning', which coincides with the intention\nand goal of explainable Al. The integration of the approaches could pave the\nway for future interpretable AI systems.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u53ef\u4fe1AI\u6280\u672f\u7684\u53d1\u5c55\uff0c\u8c03\u67e5\u4e86\u5168\u7403\u5173\u4e8eAI\u4f26\u7406\u548c\u53ef\u89e3\u91ca\u6027\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u5e76\u5206\u6790\u4e86AI\u53ef\u89e3\u91ca\u6027\u4e0e\u5143\u63a8\u7406\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "motivation": "AI\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u4e86\u51b3\u7b56\u4e0d\u900f\u660e\u7684\u95ee\u9898\uff0c\u7814\u7a76\u53ef\u4fe1AI\u6280\u672f\uff08\u5c24\u5176\u662f\u53ef\u89e3\u91ca\u6027\uff09\u6709\u52a9\u4e8e\u6ee1\u8db3\u793e\u4f1a\u548c\u4f26\u7406\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u6587\u732e\u8c03\u67e5\uff0c\u7efc\u5408\u5206\u6790\u5404\u56fdAI\u4f26\u7406\u6807\u51c6\u548c\u53ef\u89e3\u91ca\u6027\u6280\u672f\u7684\u73b0\u72b6\uff0c\u5e76\u63a2\u8ba8AI\u53ef\u89e3\u91ca\u6027\u4e0e\u5143\u63a8\u7406\u7684\u5173\u7cfb\u3002", "result": "\u603b\u7ed3\u4e86\u53ef\u89e3\u91caAI\u7684\u6700\u65b0\u6280\u672f\uff0c\u5e76\u5f3a\u8c03\u4e86\u5143\u63a8\u7406\uff08\u5373\u2018\u63a8\u7406\u7684\u63a8\u7406\u2019\uff09\u5728\u5b9e\u73b0\u53ef\u89e3\u91ca\u6027\u4e2d\u7684\u91cd\u8981\u4f5c\u7528\u3002", "conclusion": "\u672a\u6765\u53ef\u901a\u8fc7\u7ed3\u5408\u53ef\u89e3\u91caAI\u4e0e\u5143\u63a8\u7406\u6280\u672f\uff0c\u5f00\u53d1\u66f4\u900f\u660e\u3001\u53ef\u4fe1\u7684AI\u7cfb\u7edf\u3002"}}
{"id": "2505.07161", "pdf": "https://arxiv.org/pdf/2505.07161", "abs": "https://arxiv.org/abs/2505.07161", "authors": ["Jannatun Naim", "Jie Cao", "Fareen Tasneem", "Jennifer Jacobs", "Brent Milne", "James Martin", "Tamara Sumner"], "title": "Towards Actionable Pedagogical Feedback: A Multi-Perspective Analysis of Mathematics Teaching and Tutoring Dialogue", "categories": ["cs.CL", "cs.HC"], "comment": "Accepted to EDM'2025", "summary": "Effective feedback is essential for refining instructional practices in\nmathematics education, and researchers often turn to advanced natural language\nprocessing (NLP) models to analyze classroom dialogues from multiple\nperspectives. However, utterance-level discourse analysis encounters two\nprimary challenges: (1) multifunctionality, where a single utterance may serve\nmultiple purposes that a single tag cannot capture, and (2) the exclusion of\nmany utterances from domain-specific discourse move classifications, leading to\ntheir omission in feedback. To address these challenges, we proposed a\nmulti-perspective discourse analysis that integrates domain-specific talk moves\nwith dialogue act (using the flattened multi-functional SWBD-MASL schema with\n43 tags) and discourse relation (applying Segmented Discourse Representation\nTheory with 16 relations). Our top-down analysis framework enables a\ncomprehensive understanding of utterances that contain talk moves, as well as\nutterances that do not contain talk moves. This is applied to two mathematics\neducation datasets: TalkMoves (teaching) and SAGA22 (tutoring). Through\ndistributional unigram analysis, sequential talk move analysis, and multi-view\ndeep dive, we discovered meaningful discourse patterns, and revealed the vital\nrole of utterances without talk moves, demonstrating that these utterances, far\nfrom being mere fillers, serve crucial functions in guiding, acknowledging, and\nstructuring classroom discourse. These insights underscore the importance of\nincorporating discourse relations and dialogue acts into AI-assisted education\nsystems to enhance feedback and create more responsive learning environments.\nOur framework may prove helpful for providing human educator feedback, but also\naiding in the development of AI agents that can effectively emulate the roles\nof both educators and students.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u89c6\u89d2\u8bdd\u8bed\u5206\u6790\u65b9\u6cd5\uff0c\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u7684\u5bf9\u8bdd\u52a8\u4f5c\u548c\u8bdd\u8bed\u5173\u7cfb\uff0c\u89e3\u51b3\u4e86\u6570\u5b66\u6559\u80b2\u4e2d\u53cd\u9988\u5206\u6790\u7684\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u6570\u5b66\u6559\u80b2\u4e2d\u8bdd\u8bed\u5206\u6790\u7684\u4e24\u5927\u6311\u6218\uff1a\u591a\u529f\u80fd\u6027\u548c\u9886\u57df\u7279\u5b9a\u8bdd\u8bed\u5206\u7c7b\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u591a\u89c6\u89d2\u8bdd\u8bed\u5206\u6790\u6846\u67b6\uff0c\u6574\u5408\u9886\u57df\u7279\u5b9a\u7684\u5bf9\u8bdd\u52a8\u4f5c\u548c\u8bdd\u8bed\u5173\u7cfb\uff0c\u5e94\u7528\u4e8e\u4e24\u4e2a\u6570\u5b66\u6559\u80b2\u6570\u636e\u96c6\u3002", "result": "\u53d1\u73b0\u4e86\u6709\u610f\u4e49\u7684\u8bdd\u8bed\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u975e\u8bdd\u8bed\u52a8\u4f5c\u53e5\u5b50\u7684\u91cd\u8981\u4f5c\u7528\u3002", "conclusion": "\u5f3a\u8c03\u5c06\u8bdd\u8bed\u5173\u7cfb\u548c\u5bf9\u8bdd\u52a8\u4f5c\u7eb3\u5165AI\u8f85\u52a9\u6559\u80b2\u7cfb\u7edf\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u63d0\u5347\u53cd\u9988\u8d28\u91cf\u548c\u5b66\u4e60\u73af\u5883\u54cd\u5e94\u6027\u3002"}}
{"id": "2505.06292", "pdf": "https://arxiv.org/pdf/2505.06292", "abs": "https://arxiv.org/abs/2505.06292", "authors": ["Silke K. Kaiser", "Filipe Rodrigues", "Carlos Lima Azevedo", "Lynn H. Kaack"], "title": "Spatio-Temporal Graph Neural Network for Urban Spaces: Interpolating Citywide Traffic Volume", "categories": ["cs.LG", "cs.CY"], "comment": null, "summary": "Reliable street-level traffic volume data, covering multiple modes of\ntransportation, helps urban planning by informing decisions on infrastructure\nimprovements, traffic management, and public transportation. Yet, traffic\nsensors measuring traffic volume are typically scarcely located, due to their\nhigh deployment and maintenance costs. To address this, interpolation methods\ncan estimate traffic volumes at unobserved locations using available data.\nGraph Neural Networks have shown strong performance in traffic volume\nforecasting, particularly on highways and major arterial networks. Applying\nthem to urban settings, however, presents unique challenges: urban networks\nexhibit greater structural diversity, traffic volumes are highly overdispersed\nwith many zeros, the best way to account for spatial dependencies remains\nunclear, and sensor coverage is often very sparse. We introduce the Graph\nNeural Network for Urban Interpolation (GNNUI), a novel urban traffic volume\nestimation approach. GNNUI employs a masking algorithm to learn interpolation,\nintegrates node features to capture functional roles, and uses a loss function\ntailored to zero-inflated traffic distributions. In addition to the model, we\nintroduce two new open, large-scale urban traffic volume benchmarks, covering\ndifferent transportation modes: Strava cycling data from Berlin and New York\nCity taxi data. GNNUI outperforms recent, some graph-based, interpolation\nmethods across metrics (MAE, RMSE, true-zero rate, Kullback-Leibler divergence)\nand remains robust from 90% to 1% sensor coverage. On Strava, for instance, MAE\nrises only from 7.1 to 10.5, on Taxi from 23.0 to 40.4, demonstrating strong\nperformance under extreme data scarcity, common in real-world urban settings.\nWe also examine how graph connectivity choices influence model accuracy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u57ce\u5e02\u4ea4\u901a\u91cf\u63d2\u503c\u65b9\u6cd5\uff08GNNUI\uff09\uff0c\u65e8\u5728\u89e3\u51b3\u57ce\u5e02\u7f51\u7edc\u4e2d\u4ea4\u901a\u91cf\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u5305\u62ec\u7ed3\u6784\u591a\u6837\u6027\u3001\u96f6\u81a8\u80c0\u5206\u5e03\u548c\u7a00\u758f\u4f20\u611f\u5668\u8986\u76d6\u3002GNNUI\u901a\u8fc7\u63a9\u7801\u7b97\u6cd5\u5b66\u4e60\u63d2\u503c\uff0c\u6574\u5408\u8282\u70b9\u7279\u5f81\u4ee5\u6355\u83b7\u529f\u80fd\u89d2\u8272\uff0c\u5e76\u4f7f\u7528\u9488\u5bf9\u96f6\u81a8\u80c0\u5206\u5e03\u7684\u635f\u5931\u51fd\u6570\u3002\u5b9e\u9a8c\u8868\u660e\uff0cGNNUI\u5728\u4e24\u79cd\u65b0\u7684\u5927\u89c4\u6a21\u57ce\u5e02\u4ea4\u901a\u91cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5728\u6781\u7aef\u7a00\u758f\u6570\u636e\u4e0b\u4ecd\u4fdd\u6301\u7a33\u5065\u3002", "motivation": "\u57ce\u5e02\u4ea4\u901a\u91cf\u6570\u636e\u7684\u7a00\u7f3a\u6027\u9650\u5236\u4e86\u57ce\u5e02\u89c4\u5212\u7684\u51b3\u7b56\u652f\u6301\uff0c\u5c24\u5176\u662f\u5728\u591a\u6a21\u5f0f\u4ea4\u901a\u7f51\u7edc\u4e2d\u3002\u5c3d\u7ba1\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u4ea4\u901a\u91cf\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u5e94\u7528\u65f6\u9762\u4e34\u7ed3\u6784\u591a\u6837\u6027\u3001\u96f6\u81a8\u80c0\u5206\u5e03\u548c\u7a00\u758f\u4f20\u611f\u5668\u8986\u76d6\u7b49\u72ec\u7279\u6311\u6218\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u6709\u6548\u4f30\u8ba1\u57ce\u5e02\u4ea4\u901a\u91cf\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86GNNUI\u65b9\u6cd5\uff0c\u5176\u6838\u5fc3\u5305\u62ec\uff1a\uff081\uff09\u4f7f\u7528\u63a9\u7801\u7b97\u6cd5\u5b66\u4e60\u63d2\u503c\uff1b\uff082\uff09\u6574\u5408\u8282\u70b9\u7279\u5f81\u4ee5\u6355\u83b7\u4ea4\u901a\u7f51\u7edc\u7684\u529f\u80fd\u89d2\u8272\uff1b\uff083\uff09\u8bbe\u8ba1\u9488\u5bf9\u96f6\u81a8\u80c0\u5206\u5e03\u7684\u635f\u5931\u51fd\u6570\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u5f15\u5165\u4e86\u4e24\u4e2a\u65b0\u7684\u5927\u89c4\u6a21\u57ce\u5e02\u4ea4\u901a\u91cf\u57fa\u51c6\u6d4b\u8bd5\uff08Strava\u9a91\u884c\u6570\u636e\u548c\u7ebd\u7ea6\u51fa\u79df\u8f66\u6570\u636e\uff09\u7528\u4e8e\u9a8c\u8bc1\u3002", "result": "GNNUI\u5728MAE\u3001RMSE\u3001\u771f\u5b9e\u96f6\u7387\u7b49\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u4f20\u611f\u5668\u8986\u76d6\u7387\u4e3a1%\u7684\u6781\u7aef\u7a00\u758f\u60c5\u51b5\u4e0b\u4ecd\u8868\u73b0\u7a33\u5065\uff08Strava\u7684MAE\u4ece7.1\u5347\u81f310.5\uff0cTaxi\u7684MAE\u4ece23.0\u5347\u81f340.4\uff09\u3002\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86\u56fe\u8fde\u63a5\u9009\u62e9\u5bf9\u6a21\u578b\u7cbe\u5ea6\u7684\u5f71\u54cd\u3002", "conclusion": "GNNUI\u4e3a\u57ce\u5e02\u4ea4\u901a\u91cf\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4f20\u611f\u5668\u8986\u76d6\u6781\u7a00\u758f\u7684\u73b0\u5b9e\u573a\u666f\u3002\u5176\u65b9\u6cd5\u8bbe\u8ba1\u548c\u57fa\u51c6\u6570\u636e\u96c6\u7684\u5f15\u5165\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2505.07027", "pdf": "https://arxiv.org/pdf/2505.07027", "abs": "https://arxiv.org/abs/2505.07027", "authors": ["Haorui Wang", "Jeff Guo", "Lingkai Kong", "Rampi Ramprasad", "Philippe Schwaller", "Yuanqi Du", "Chao Zhang"], "title": "LLM-Augmented Chemical Synthesis and Design Decision Programs", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.NE", "physics.chem-ph"], "comment": null, "summary": "Retrosynthesis, the process of breaking down a target molecule into simpler\nprecursors through a series of valid reactions, stands at the core of organic\nchemistry and drug development. Although recent machine learning (ML) research\nhas advanced single-step retrosynthetic modeling and subsequent route searches,\nthese solutions remain restricted by the extensive combinatorial space of\npossible pathways. Concurrently, large language models (LLMs) have exhibited\nremarkable chemical knowledge, hinting at their potential to tackle complex\ndecision-making tasks in chemistry. In this work, we explore whether LLMs can\nsuccessfully navigate the highly constrained, multi-step retrosynthesis\nplanning problem. We introduce an efficient scheme for encoding reaction\npathways and present a new route-level search strategy, moving beyond the\nconventional step-by-step reactant prediction. Through comprehensive\nevaluations, we show that our LLM-augmented approach excels at retrosynthesis\nplanning and extends naturally to the broader challenge of synthesizable\nmolecular design.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f18\u5316\u591a\u6b65\u9006\u5408\u6210\u89c4\u5212\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8def\u5f84\u7f16\u7801\u65b9\u6848\u548c\u65b0\u641c\u7d22\u7b56\u7565\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9006\u5408\u6210\u89c4\u5212\u6548\u679c\uff0c\u5e76\u6269\u5c55\u81f3\u53ef\u5408\u6210\u5206\u5b50\u8bbe\u8ba1\u3002", "motivation": "\u9006\u5408\u6210\u662f\u6709\u673a\u5316\u5b66\u548c\u836f\u7269\u5f00\u53d1\u7684\u6838\u5fc3\uff0c\u4f46\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\u4e8e\u7ec4\u5408\u7a7a\u95f4\u5e9e\u5927\uff0c\u800cLLM\u5728\u5316\u5b66\u51b3\u7b56\u4efb\u52a1\u4e2d\u5c55\u73b0\u6f5c\u529b\uff0c\u56e0\u6b64\u7814\u7a76\u5176\u80fd\u5426\u89e3\u51b3\u591a\u6b65\u9006\u5408\u6210\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8def\u5f84\u7f16\u7801\u65b9\u6848\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8d85\u8d8a\u9010\u6b65\u53cd\u5e94\u7269\u9884\u6d4b\u7684\u8def\u7531\u7ea7\u641c\u7d22\u7b56\u7565\u3002", "result": "\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cLLM\u589e\u5f3a\u65b9\u6cd5\u5728\u9006\u5408\u6210\u89c4\u5212\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u53ef\u6269\u5c55\u81f3\u53ef\u5408\u6210\u5206\u5b50\u8bbe\u8ba1\u3002", "conclusion": "LLM\u5728\u591a\u6b65\u9006\u5408\u6210\u95ee\u9898\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u5316\u5b66\u5408\u6210\u89c4\u5212\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.07162", "pdf": "https://arxiv.org/pdf/2505.07162", "abs": "https://arxiv.org/abs/2505.07162", "authors": ["Hajar Sakai", "Sarah S. Lam"], "title": "KDH-MLTC: Knowledge Distillation for Healthcare Multi-Label Text Classification", "categories": ["cs.CL"], "comment": null, "summary": "The increasing volume of healthcare textual data requires computationally\nefficient, yet highly accurate classification approaches able to handle the\nnuanced and complex nature of medical terminology. This research presents\nKnowledge Distillation for Healthcare Multi-Label Text Classification\n(KDH-MLTC), a framework leveraging model compression and Large Language Models\n(LLMs). The proposed approach addresses conventional healthcare Multi-Label\nText Classification (MLTC) challenges by integrating knowledge distillation and\nsequential fine-tuning, subsequently optimized through Particle Swarm\nOptimization (PSO) for hyperparameter tuning. KDH-MLTC transfers knowledge from\na more complex teacher LLM (i.e., BERT) to a lighter student LLM (i.e.,\nDistilBERT) through sequential training adapted to MLTC that preserves the\nteacher's learned information while significantly reducing computational\nrequirements. As a result, the classification is enabled to be conducted\nlocally, making it suitable for healthcare textual data characterized by\nsensitivity and, therefore, ensuring HIPAA compliance. The experiments\nconducted on three medical literature datasets of different sizes, sampled from\nthe Hallmark of Cancer (HoC) dataset, demonstrate that KDH-MLTC achieves\nsuperior performance compared to existing approaches, particularly for the\nlargest dataset, reaching an F1 score of 82.70%. Additionally, statistical\nvalidation and an ablation study are carried out, proving the robustness of\nKDH-MLTC. Furthermore, the PSO-based hyperparameter optimization process\nallowed the identification of optimal configurations. The proposed approach\ncontributes to healthcare text classification research, balancing efficiency\nrequirements in resource-constrained healthcare settings with satisfactory\naccuracy demands.", "AI": {"tldr": "KDH-MLTC\u6846\u67b6\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u5e8f\u5217\u5fae\u8c03\u4f18\u5316\u533b\u7597\u591a\u6807\u7b7e\u6587\u672c\u5206\u7c7b\uff0c\u7ed3\u5408PSO\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\u5e76\u63d0\u5347\u6027\u80fd\uff0cF1\u5206\u6570\u8fbe82.70%\u3002", "motivation": "\u533b\u7597\u6587\u672c\u6570\u636e\u91cf\u589e\u957f\u8fc5\u901f\uff0c\u9700\u8981\u9ad8\u6548\u4e14\u9ad8\u7cbe\u5ea6\u7684\u5206\u7c7b\u65b9\u6cd5\u5904\u7406\u590d\u6742\u533b\u5b66\u672f\u8bed\uff0c\u540c\u65f6\u6ee1\u8db3\u654f\u611f\u6570\u636e\u7684HIPAA\u5408\u89c4\u8981\u6c42\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u84b8\u998f\u5c06BERT\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230\u8f7b\u91cf\u7ea7DistilBERT\uff0c\u7ed3\u5408\u5e8f\u5217\u5fae\u8c03\u548cPSO\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u5b9e\u73b0\u9ad8\u6548\u5206\u7c7b\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u89c4\u6a21\u7684\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6700\u5927\u6570\u636e\u96c6F1\u5206\u6570\u8fbe82.70%\uff0c\u5e76\u901a\u8fc7\u7edf\u8ba1\u9a8c\u8bc1\u548c\u6d88\u878d\u5b9e\u9a8c\u8bc1\u660e\u5176\u9c81\u68d2\u6027\u3002", "conclusion": "KDH-MLTC\u5e73\u8861\u4e86\u8d44\u6e90\u9650\u5236\u4e0b\u7684\u6548\u7387\u9700\u6c42\u4e0e\u51c6\u786e\u6027\uff0c\u4e3a\u533b\u7597\u6587\u672c\u5206\u7c7b\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.06295", "pdf": "https://arxiv.org/pdf/2505.06295", "abs": "https://arxiv.org/abs/2505.06295", "authors": ["Bhuvan Saravanan", "Pasanth Kumar M D", "Aarnesh Vengateson"], "title": "Benchmarking Traditional Machine Learning and Deep Learning Models for Fault Detection in Power Transformers", "categories": ["cs.LG"], "comment": null, "summary": "Accurate diagnosis of power transformer faults is essential for ensuring the\nstability and safety of electrical power systems. This study presents a\ncomparative analysis of conventional machine learning (ML) algorithms and deep\nlearning (DL) algorithms for fault classification of power transformers. Using\na condition-monitored dataset spanning 10 months, various gas concentration\nfeatures were normalized and used to train five ML classifiers: Support Vector\nMachine (SVM), k-Nearest Neighbors (KNN), Random Forest (RF), XGBoost, and\nArtificial Neural Network (ANN). In addition, four DL models were evaluated:\nLong Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), One-Dimensional\nConvolutional Neural Network (1D-CNN), and TabNet. Experimental results show\nthat both ML and DL approaches performed comparably. The RF model achieved the\nhighest ML accuracy at 86.82%, while the 1D-CNN model attained a close 86.30%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u4f20\u7edf\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u7b97\u6cd5\u5728\u7535\u529b\u53d8\u538b\u5668\u6545\u969c\u5206\u7c7b\u4e2d\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\uff0c\u7ed3\u679c\u8868\u660e\u4e24\u8005\u8868\u73b0\u76f8\u8fd1\uff0c\u968f\u673a\u68ee\u6797\uff08RF\uff09\u548c1D-CNN\u6a21\u578b\u5206\u522b\u53d6\u5f97\u4e86\u6700\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u7535\u529b\u53d8\u538b\u5668\u6545\u969c\u7684\u51c6\u786e\u8bca\u65ad\u5bf9\u7535\u529b\u7cfb\u7edf\u7684\u7a33\u5b9a\u548c\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u9700\u8981\u6bd4\u8f83\u4e0d\u540c\u7b97\u6cd5\u7684\u6027\u80fd\u4ee5\u9009\u62e9\u6700\u4f73\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e8610\u4e2a\u6708\u7684\u76d1\u6d4b\u6570\u636e\u96c6\uff0c\u5bf9\u6c14\u4f53\u6d53\u5ea6\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\u540e\uff0c\u8bad\u7ec3\u4e865\u79cdML\u5206\u7c7b\u5668\uff08SVM\u3001KNN\u3001RF\u3001XGBoost\u3001ANN\uff09\u548c4\u79cdDL\u6a21\u578b\uff08LSTM\u3001GRU\u30011D-CNN\u3001TabNet\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cML\u548cDL\u65b9\u6cd5\u8868\u73b0\u76f8\u5f53\uff0c\u5176\u4e2dRF\u6a21\u578b\u7684\u51c6\u786e\u7387\u6700\u9ad8\uff0886.82%\uff09\uff0c1D-CNN\u6a21\u578b\u6b21\u4e4b\uff0886.30%\uff09\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u5728\u53d8\u538b\u5668\u6545\u969c\u5206\u7c7b\u4e2d\u5747\u6709\u6548\uff0c\u53ef\u6839\u636e\u5b9e\u9645\u9700\u6c42\u9009\u62e9\u5408\u9002\u7b97\u6cd5\u3002"}}
{"id": "2505.07030", "pdf": "https://arxiv.org/pdf/2505.07030", "abs": "https://arxiv.org/abs/2505.07030", "authors": ["Mahmood Mohassel Feghhi", "Raya Majid Alsharfa", "Majid Hameed Majeed"], "title": "Efficient Fault Detection in WSN Based on PCA-Optimized Deep Neural Network Slicing Trained with GOA", "categories": ["cs.AI", "cs.LG", "eess.SP"], "comment": "22 pages, 18 figures, Accepted for publication in International\n  Journal of Intelligent Engineering and Systems, May 2025", "summary": "Fault detection in Wireless Sensor Networks (WSNs) is crucial for reliable\ndata transmission and network longevity. Traditional fault detection methods\noften struggle with optimizing deep neural networks (DNNs) for efficient\nperformance, especially in handling high-dimensional data and capturing\nnonlinear relationships. Additionally, these methods typically suffer from slow\nconvergence and difficulty in finding optimal network architectures using\ngradient-based optimization. This study proposes a novel hybrid method\ncombining Principal Component Analysis (PCA) with a DNN optimized by the\nGrasshopper Optimization Algorithm (GOA) to address these limitations. Our\napproach begins by computing eigenvalues from the original 12-dimensional\ndataset and sorting them in descending order. The cumulative sum of these\nvalues is calculated, retaining principal components until 99.5% variance is\nachieved, effectively reducing dimensionality to 4 features while preserving\ncritical information. This compressed representation trains a six-layer DNN\nwhere GOA optimizes the network architecture, overcoming backpropagation's\nlimitations in discovering nonlinear relationships. This hybrid PCA-GOA-DNN\nframework compresses the data and trains a six-layer DNN that is optimized by\nGOA, enhancing both training efficiency and fault detection accuracy. The\ndataset used in this study is a real-world WSNs dataset developed by the\nUniversity of North Carolina, which was used to evaluate the proposed method's\nperformance. Extensive simulations demonstrate that our approach achieves a\nremarkable 99.72% classification accuracy, with exceptional precision and\nrecall, outperforming conventional methods. The method is computationally\nefficient, making it suitable for large-scale WSN deployments, and represents a\nsignificant advancement in fault detection for resource-constrained WSNs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408PCA\u3001DNN\u548cGOA\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316WSN\u4e2d\u7684\u6545\u969c\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u7387\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u4f18\u5316DNN\u4ee5\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u548c\u975e\u7ebf\u6027\u5173\u7cfb\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u6536\u655b\u6162\u3001\u67b6\u6784\u4f18\u5316\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528PCA\u964d\u7ef4\u81f34\u4e2a\u7279\u5f81\uff0c\u7528GOA\u4f18\u5316\u516d\u5c42DNN\u67b6\u6784\uff0c\u514b\u670d\u53cd\u5411\u4f20\u64ad\u7684\u5c40\u9650\u6027\u3002", "result": "\u5728\u771f\u5b9eWSN\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8699.72%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6df7\u5408\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u7684WSN\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u9ad8\u51c6\u786e\u6027\uff0c\u662f\u6545\u969c\u68c0\u6d4b\u9886\u57df\u7684\u91cd\u5927\u8fdb\u5c55\u3002"}}
{"id": "2505.07184", "pdf": "https://arxiv.org/pdf/2505.07184", "abs": "https://arxiv.org/abs/2505.07184", "authors": ["Yifan Wei", "Xiaoyan Yu", "Tengfei Pan", "Angsheng Li", "Li Du"], "title": "Structural Entropy Guided Agent for Detecting and Repairing Knowledge Deficiencies in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved unprecedented performance by\nleveraging vast pretraining corpora, yet their performance remains suboptimal\nin knowledge-intensive domains such as medicine and scientific research, where\nhigh factual precision is required. While synthetic data provides a promising\navenue for augmenting domain knowledge, existing methods frequently generate\nredundant samples that do not align with the model's true knowledge gaps. To\novercome this limitation, we propose a novel Structural Entropy-guided\nKnowledge Navigator (SENATOR) framework that addresses the intrinsic knowledge\ndeficiencies of LLMs. Our approach employs the Structure Entropy (SE) metric to\nquantify uncertainty along knowledge graph paths and leverages Monte Carlo Tree\nSearch (MCTS) to selectively explore regions where the model lacks\ndomain-specific knowledge. Guided by these insights, the framework generates\ntargeted synthetic data for supervised fine-tuning, enabling continuous\nself-improvement. Experimental results on LLaMA-3 and Qwen2 across multiple\ndomain-specific benchmarks show that SENATOR effectively detects and repairs\nknowledge deficiencies, achieving notable performance improvements. The code\nand data for our methods and experiments are available at\nhttps://github.com/weiyifan1023/senator.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSENATOR\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u71b5\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6765\u8bc6\u522b\u548c\u4fee\u590d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\u7684\u7f3a\u9677\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u5b9a\u5411\u5408\u6210\u6570\u636e\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5e7f\u6cdb\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9700\u8981\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\uff08\u5982\u533b\u5b66\u548c\u79d1\u7814\uff09\u4ecd\u8868\u73b0\u4e0d\u8db3\u3002\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u901a\u5e38\u5197\u4f59\u4e14\u672a\u80fd\u9488\u5bf9\u6a21\u578b\u7684\u77e5\u8bc6\u7f3a\u53e3\u3002", "method": "\u63d0\u51faSENATOR\u6846\u67b6\uff0c\u5229\u7528\u7ed3\u6784\u71b5\uff08SE\uff09\u5ea6\u91cf\u77e5\u8bc6\u56fe\u8c31\u8def\u5f84\u4e0a\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u9009\u62e9\u6027\u63a2\u7d22\u6a21\u578b\u7f3a\u4e4f\u77e5\u8bc6\u7684\u533a\u57df\uff0c\u751f\u6210\u5b9a\u5411\u5408\u6210\u6570\u636e\u7528\u4e8e\u76d1\u7763\u5fae\u8c03\u3002", "result": "\u5728LLaMA-3\u548cQwen2\u4e0a\u7684\u591a\u9886\u57df\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0cSENATOR\u80fd\u6709\u6548\u68c0\u6d4b\u5e76\u4fee\u590d\u77e5\u8bc6\u7f3a\u9677\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "SENATOR\u901a\u8fc7\u7ed3\u6784\u71b5\u548c\u5b9a\u5411\u6570\u636e\u751f\u6210\uff0c\u4e3aLLMs\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\u7684\u6301\u7eed\u81ea\u6211\u6539\u8fdb\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.06297", "pdf": "https://arxiv.org/pdf/2505.06297", "abs": "https://arxiv.org/abs/2505.06297", "authors": ["Yu Mao", "Holger Pirk", "Chun Jason Xue"], "title": "Lossless Compression of Large Language Model-Generated Text via Next-Token Prediction", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "As large language models (LLMs) continue to be deployed and utilized across\ndomains, the volume of LLM-generated data is growing rapidly. This trend\nhighlights the increasing importance of effective and lossless compression for\nsuch data in modern text management systems. However, compressing LLM-generated\ndata presents unique challenges compared to traditional human- or\nmachine-generated content. Traditional machine-generated data is typically\nderived from computational processes or device outputs, often highly structured\nand limited to low-level elements like labels or numerical values. This\nstructure enables conventional lossless compressors to perform efficiently. In\ncontrast, LLM-generated data is more complex and diverse, requiring new\napproaches for effective compression. In this work, we conduct the first\nsystematic investigation of lossless compression techniques tailored\nspecifically to LLM-generated data. Notably, because LLMs are trained via\nnext-token prediction, we find that LLM-generated data is highly predictable\nfor the models themselves. This predictability enables LLMs to serve as\nefficient compressors of their own outputs. Through extensive experiments with\n14 representative LLMs and 8 LLM-generated datasets from diverse domains, we\nshow that LLM-based prediction methods achieve remarkable compression rates,\nexceeding 20x, far surpassing the 3x rate achieved by Gzip, a widely used\ngeneral-purpose compressor. Furthermore, this advantage holds across different\nLLM sizes and dataset types, demonstrating the robustness and practicality of\nLLM-based methods in lossless text compression under generative AI workloads.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u6570\u636e\u7684\u65e0\u635f\u538b\u7f29\u6280\u672f\uff0c\u53d1\u73b0LLM\u80fd\u9ad8\u6548\u538b\u7f29\u81ea\u8eab\u8f93\u51fa\uff0c\u538b\u7f29\u7387\u8fdc\u8d85\u4f20\u7edf\u5de5\u5177\u5982Gzip\u3002", "motivation": "\u968f\u7740LLM\u751f\u6210\u6570\u636e\u7684\u5feb\u901f\u589e\u957f\uff0c\u5176\u590d\u6742\u7684\u591a\u6837\u6027\u4f7f\u5f97\u4f20\u7edf\u538b\u7f29\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u4e3aLLM\u751f\u6210\u6570\u636e\u8bbe\u8ba1\u7684\u65b0\u538b\u7f29\u6280\u672f\u3002", "method": "\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u6027\u63a2\u7d22\u4e86\u57fa\u4e8eLLM\u9884\u6d4b\u7684\u65e0\u635f\u538b\u7f29\u65b9\u6cd5\uff0c\u5229\u7528LLM\u81ea\u8eab\u5bf9\u751f\u6210\u6570\u636e\u7684\u53ef\u9884\u6d4b\u6027\u3002\u5b9e\u9a8c\u8986\u76d614\u79cdLLM\u548c8\u4e2a\u6570\u636e\u96c6\u3002", "result": "LLM\u538b\u7f29\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8d85\u8fc720\u500d\u7684\u538b\u7f29\u7387\uff0c\u663e\u8457\u4f18\u4e8eGzip\u76843\u500d\uff0c\u4e14\u5728\u4e0d\u540cLLM\u89c4\u6a21\u548c\u6570\u636e\u7c7b\u578b\u4e2d\u5747\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "LLM\u4f5c\u4e3a\u81ea\u8eab\u8f93\u51fa\u7684\u538b\u7f29\u5668\u5177\u6709\u9ad8\u6548\u6027\u548c\u666e\u9002\u6027\uff0c\u4e3a\u751f\u6210\u5f0fAI\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u65e0\u635f\u6587\u672c\u538b\u7f29\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.07049", "pdf": "https://arxiv.org/pdf/2505.07049", "abs": "https://arxiv.org/abs/2505.07049", "authors": ["Yubo Shu", "Zhewei Huang", "Xin Wu", "Chen Hu", "Shuchang Zhou", "Daxin Jiang"], "title": "DialogueReason: Rule-Based RL Sparks Dialogue Reasoning in LLMs", "categories": ["cs.AI"], "comment": null, "summary": "We propose DialogueReason, a reasoning paradigm that uncovers the lost roles\nin monologue-style reasoning models, aiming to boost diversity and coherency of\nthe reasoning process. Recent advances in RL-based large reasoning models have\nled to impressive long CoT capabilities and high performance on math and\nscience benchmarks. However, these reasoning models rely mainly on\nmonologue-style reasoning, which often limits reasoning diversity and\ncoherency, frequently recycling fixed strategies or exhibiting unnecessary\nshifts in attention. Our work consists of an analysis of monologue reasoning\npatterns and the development of a dialogue-based reasoning approach. We first\nintroduce the Compound-QA task, which concatenates multiple problems into a\nsingle prompt to assess both diversity and coherency of reasoning. Our analysis\nshows that Compound-QA exposes weaknesses in monologue reasoning, evidenced by\nboth quantitative metrics and qualitative reasoning traces. Building on the\nanalysis, we propose a dialogue-based reasoning, named DialogueReason,\nstructured around agents, environment, and interactions. Using PPO with\nrule-based rewards, we train open-source LLMs (Qwen-QWQ and Qwen-Base) to adopt\ndialogue reasoning. We evaluate trained models on MATH, AIME, and GPQA\ndatasets, showing that the dialogue reasoning model outperforms monologue\nmodels under more complex compound questions. Additionally, we discuss how\ndialogue-based reasoning helps enhance interpretability, facilitate more\nintuitive human interaction, and inspire advances in multi-agent system design.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86DialogueReason\uff0c\u4e00\u79cd\u5bf9\u8bdd\u5f0f\u63a8\u7406\u8303\u5f0f\uff0c\u65e8\u5728\u89e3\u51b3\u5355\u98ce\u683c\u63a8\u7406\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u63a8\u7406\u591a\u6837\u6027\u548c\u8fde\u8d2f\u6027\u3002\u901a\u8fc7Compound-QA\u4efb\u52a1\u9a8c\u8bc1\u5355\u98ce\u683c\u63a8\u7406\u7684\u5f31\u70b9\uff0c\u5e76\u57fa\u4e8ePPO\u8bad\u7ec3\u5f00\u6e90LLM\uff0c\u5728\u590d\u6742\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u4e8e\u5355\u98ce\u683c\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5355\u98ce\u683c\u63a8\u7406\u6a21\u578b\u591a\u6837\u6027\u4f4e\u3001\u8fde\u8d2f\u6027\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u63a8\u7406\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5bf9\u8bdd\u5f0f\u63a8\u7406\u63d0\u5347\u8fd9\u4e9b\u65b9\u9762\u3002", "method": "\u5f15\u5165Compound-QA\u4efb\u52a1\u5206\u6790\u5355\u98ce\u683c\u63a8\u7406\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8ePPO\u548c\u89c4\u5219\u5956\u52b1\u7684\u5bf9\u8bdd\u63a8\u7406\u8bad\u7ec3\u6846\u67b6\uff0c\u5e76\u5e94\u7528\u4e8e\u5f00\u6e90LLM\uff08Qwen-QWQ\u548cQwen-Base\uff09\u3002", "result": "\u5bf9\u8bdd\u63a8\u7406\u6a21\u578b\u5728MATH\u3001AIME\u548cGPQA\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u5355\u98ce\u683c\u6a21\u578b\uff0c\u5c24\u5176\u5728\u590d\u6742\u95ee\u9898\u4e2d\u3002", "conclusion": "\u5bf9\u8bdd\u63a8\u7406\u4e0d\u4ec5\u63d0\u5347\u6027\u80fd\uff0c\u8fd8\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u548c\u4eba\u673a\u4ea4\u4e92\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.07202", "pdf": "https://arxiv.org/pdf/2505.07202", "abs": "https://arxiv.org/abs/2505.07202", "authors": ["Hyouin Liu", "Zhikuan Zhang"], "title": "On the Cost and Benefits of Training Context with Utterance or Full Conversation Training: A Comparative Stud", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Modern TTS systems designed for conversations achieve high-quality utterances\nbut often remain inaccessible publicly. Are existing open-source architectures\ninadequate, or are current training techniques insufficient? This paper\ninvestigates prominent models and their underlying behaviors regarding\nconversational context. Using 20 GPU-hours on an NVIDIA H100, we empirically\nexamine two approaches: context-based utterance-level training versus full\nconversation training. Results demonstrate that context-based utterance\ntraining achieves superior MOS scores (4.3/5.0 vs 3.7/5.0) and reduces training\ntime by 37%, while full conversation approaches suffer from speaker similarity\nhallucination issues. These findings provide practical guidelines for\nconversational TTS development, favoring utterance-level training with\ncontextual conditioning for both resource efficiency and output quality.", "AI": {"tldr": "\u6bd4\u8f83\u4e24\u79cd\u5bf9\u8bddTTS\u8bad\u7ec3\u65b9\u6cd5\uff1a\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u53e5\u5b50\u7ea7\u8bad\u7ec3\u548c\u5b8c\u6574\u5bf9\u8bdd\u8bad\u7ec3\uff0c\u524d\u8005\u5728MOS\u5206\u6570\u548c\u8bad\u7ec3\u6548\u7387\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u7684\u5f00\u653eTTS\u7cfb\u7edf\u5728\u5bf9\u8bdd\u573a\u666f\u4e2d\u6027\u80fd\u6709\u9650\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7a76\u5176\u6839\u672c\u539f\u56e0\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6848\u3002", "method": "\u4f7f\u752820 GPU\u5c0f\u65f6\u5728NVIDIA H100\u4e0a\u5b9e\u8bc1\u5bf9\u6bd4\u4e24\u79cd\u65b9\u6cd5\uff1a\u4e0a\u4e0b\u6587\u53e5\u5b50\u7ea7\u8bad\u7ec3\u548c\u5b8c\u6574\u5bf9\u8bdd\u8bad\u7ec3\u3002", "result": "\u4e0a\u4e0b\u6587\u53e5\u5b50\u7ea7\u8bad\u7ec3MOS\u5f97\u5206\u66f4\u9ad8\uff084.3/5.0\uff09\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1137%\uff0c\u4e14\u907f\u514d\u4e86\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u6027\u5e7b\u89c9\u95ee\u9898\u3002", "conclusion": "\u5bf9\u8bddTTS\u5f00\u53d1\u5e94\u4f18\u5148\u91c7\u7528\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u53e5\u5b50\u7ea7\u8bad\u7ec3\uff0c\u4ee5\u517c\u987e\u8d44\u6e90\u6548\u7387\u548c\u8f93\u51fa\u8d28\u91cf\u3002"}}
{"id": "2505.06300", "pdf": "https://arxiv.org/pdf/2505.06300", "abs": "https://arxiv.org/abs/2505.06300", "authors": ["Umberto Gon\u00e7alves de Sousa"], "title": "ARDNS-FN-Quantum: A Quantum-Enhanced Reinforcement Learning Framework with Cognitive-Inspired Adaptive Exploration for Dynamic Environments", "categories": ["cs.LG", "cs.AI"], "comment": "19 pages, 7 figures", "summary": "Reinforcement learning (RL) has transformed sequential decision making, yet\ntraditional algorithms like Deep Q-Networks (DQNs) and Proximal Policy\nOptimization (PPO) often struggle with efficient exploration, stability, and\nadaptability in dynamic environments. This study presents ARDNS-FN-Quantum\n(Adaptive Reward-Driven Neural Simulator with Quantum enhancement), a novel\nframework that integrates a 2-qubit quantum circuit for action selection, a\ndual-memory system inspired by human cognition, and adaptive exploration\nstrategies modulated by reward variance and curiosity. Evaluated in a 10X10\ngrid-world over 20,000 episodes, ARDNS-FN-Quantum achieves a 99.5% success rate\n(versus 81.3% for DQN and 97.0% for PPO), a mean reward of 9.0528 across all\nepisodes (versus 1.2941 for DQN and 7.6196 for PPO), and an average of 46.7\nsteps to goal (versus 135.9 for DQN and 62.5 for PPO). In the last 100\nepisodes, it records a mean reward of 9.1652 (versus 7.0916 for DQN and 9.0310\nfor PPO) and 37.2 steps to goal (versus 52.7 for DQN and 53.4 for PPO).\nGraphical analyses, including learning curves, steps-to-goal trends, reward\nvariance, and reward distributions, demonstrate ARDNS-FN-Quantum's superior\nstability (reward variance 5.424 across all episodes versus 252.262 for DQN and\n76.583 for PPO) and efficiency. By bridging quantum computing, cognitive\nscience, and RL, ARDNS-FN-Quantum offers a scalable, human-like approach to\nadaptive learning in uncertain environments, with potential applications in\nrobotics, autonomous systems, and decision-making under uncertainty.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u91cf\u5b50\u8ba1\u7b97\u548c\u8ba4\u77e5\u79d1\u5b66\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6ARDNS-FN-Quantum\uff0c\u5728\u63a2\u7d22\u6548\u7387\u3001\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08\u5982DQN\u548cPPO\uff09\u5728\u52a8\u6001\u73af\u5883\u4e2d\u63a2\u7d22\u6548\u7387\u4f4e\u3001\u7a33\u5b9a\u6027\u5dee\u548c\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u6574\u5408\u4e862\u91cf\u5b50\u6bd4\u7279\u7535\u8def\u7528\u4e8e\u52a8\u4f5c\u9009\u62e9\u3001\u53d7\u4eba\u7c7b\u8ba4\u77e5\u542f\u53d1\u7684\u53cc\u8bb0\u5fc6\u7cfb\u7edf\u548c\u57fa\u4e8e\u5956\u52b1\u65b9\u5dee\u4e0e\u597d\u5947\u5fc3\u7684\u81ea\u9002\u5e94\u63a2\u7d22\u7b56\u7565\u3002", "result": "\u572810X10\u7f51\u683c\u4e16\u754c\u4e2d\uff0cARDNS-FN-Quantum\u7684\u6210\u529f\u7387\u4e3a99.5%\uff0c\u5e73\u5747\u5956\u52b19.0528\uff0c\u5e73\u5747\u6b65\u657046.7\uff0c\u5747\u663e\u8457\u4f18\u4e8eDQN\u548cPPO\u3002", "conclusion": "ARDNS-FN-Quantum\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u9002\u5e94\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u3001\u81ea\u4e3b\u7cfb\u7edf\u548c\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\u9886\u57df\u3002"}}
{"id": "2505.07052", "pdf": "https://arxiv.org/pdf/2505.07052", "abs": "https://arxiv.org/abs/2505.07052", "authors": ["Humam Kourani", "Gyunam Park", "Wil M. P. van der Aalst"], "title": "Unlocking Non-Block-Structured Decisions: Inductive Mining with Choice Graphs", "categories": ["cs.AI"], "comment": "The Version of Record of this contribution will be published in the\n  proceedings of the 23rd International Conference on Business Process\n  Management (BPM 2025). This preprint has not undergone peer review or any\n  post-submission improvements or corrections", "summary": "Process discovery aims to automatically derive process models from event\nlogs, enabling organizations to analyze and improve their operational\nprocesses. Inductive mining algorithms, while prioritizing soundness and\nefficiency through hierarchical modeling languages, often impose a strict\nblock-structured representation. This limits their ability to accurately\ncapture the complexities of real-world processes. While recent advancements\nlike the Partially Ordered Workflow Language (POWL) have addressed the\nblock-structure limitation for concurrency, a significant gap remains in\neffectively modeling non-block-structured decision points. In this paper, we\nbridge this gap by proposing an extension of POWL to handle\nnon-block-structured decisions through the introduction of choice graphs.\nChoice graphs offer a structured yet flexible approach to model complex\ndecision logic within the hierarchical framework of POWL. We present an\ninductive mining discovery algorithm that uses our extension and preserves the\nquality guarantees of the inductive mining framework. Our experimental\nevaluation demonstrates that the discovered models, enriched with choice\ngraphs, more precisely represent the complex decision-making behavior found in\nreal-world processes, without compromising the high scalability inherent in\ninductive mining techniques.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u7684\u90e8\u5206\u6709\u5e8f\u5de5\u4f5c\u6d41\u8bed\u8a00\uff08POWL\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u9009\u62e9\u56fe\uff08choice graphs\uff09\u6765\u89e3\u51b3\u975e\u5757\u7ed3\u6784\u51b3\u7b56\u70b9\u7684\u5efa\u6a21\u95ee\u9898\uff0c\u4ece\u800c\u66f4\u7cbe\u786e\u5730\u6355\u6349\u73b0\u5b9e\u6d41\u7a0b\u4e2d\u7684\u590d\u6742\u51b3\u7b56\u903b\u8f91\u3002", "motivation": "\u73b0\u6709\u5f52\u7eb3\u6316\u6398\u7b97\u6cd5\u5728\u5904\u7406\u975e\u5757\u7ed3\u6784\u51b3\u7b56\u70b9\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u73b0\u5b9e\u6d41\u7a0b\u7684\u590d\u6742\u6027\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u6269\u5c55POWL\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55POWL\u7684\u65b9\u6cd5\uff0c\u5f15\u5165\u9009\u62e9\u56fe\u4ee5\u5efa\u6a21\u975e\u5757\u7ed3\u6784\u51b3\u7b56\u70b9\uff0c\u5e76\u5f00\u53d1\u4e86\u76f8\u5e94\u7684\u5f52\u7eb3\u6316\u6398\u53d1\u73b0\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6269\u5c55\u540e\u7684POWL\u80fd\u66f4\u7cbe\u786e\u5730\u8868\u793a\u73b0\u5b9e\u6d41\u7a0b\u4e2d\u7684\u590d\u6742\u51b3\u7b56\u884c\u4e3a\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f52\u7eb3\u6316\u6398\u6280\u672f\u7684\u9ad8\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u9009\u62e9\u56fe\u6269\u5c55POWL\uff0c\u8bba\u6587\u6210\u529f\u89e3\u51b3\u4e86\u975e\u5757\u7ed3\u6784\u51b3\u7b56\u70b9\u7684\u5efa\u6a21\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6d41\u7a0b\u53d1\u73b0\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2505.07205", "pdf": "https://arxiv.org/pdf/2505.07205", "abs": "https://arxiv.org/abs/2505.07205", "authors": ["Mouxiao Bian", "Rongzhao Zhang", "Chao Ding", "Xinwei Peng", "Jie Xu"], "title": "Benchmarking Ethical and Safety Risks of Healthcare LLMs in China-Toward Systemic Governance under Healthy China 2030", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are poised to transform healthcare under China's\nHealthy China 2030 initiative, yet they introduce new ethical and\npatient-safety challenges. We present a novel 12,000-item Q&A benchmark\ncovering 11 ethics and 9 safety dimensions in medical contexts, to\nquantitatively evaluate these risks. Using this dataset, we assess\nstate-of-the-art Chinese medical LLMs (e.g., Qwen 2.5-32B, DeepSeek), revealing\nmoderate baseline performance (accuracy 42.7% for Qwen 2.5-32B) and significant\nimprovements after fine-tuning on our data (up to 50.8% accuracy). Results show\nnotable gaps in LLM decision-making on ethics and safety scenarios, reflecting\ninsufficient institutional oversight. We then identify systemic governance\nshortfalls-including the lack of fine-grained ethical audit protocols, slow\nadaptation by hospital IRBs, and insufficient evaluation tools-that currently\nhinder safe LLM deployment. Finally, we propose a practical governance\nframework for healthcare institutions (embedding LLM auditing teams, enacting\ndata ethics guidelines, and implementing safety simulation pipelines) to\nproactively manage LLM risks. Our study highlights the urgent need for robust\nLLM governance in Chinese healthcare, aligning AI innovation with patient\nsafety and ethical standards.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4e2d\u56fd\u533b\u7597\u9886\u57df\u7684\u4f26\u7406\u4e0e\u5b89\u5168\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a12000\u9879\u7684Q&A\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86\u5f53\u524d\u533b\u7597LLM\u7684\u8868\u73b0\uff0c\u5e76\u53d1\u73b0\u5176\u5b58\u5728\u663e\u8457\u7f3a\u9677\u3002\u7814\u7a76\u8fd8\u6307\u51fa\u4e86\u7cfb\u7edf\u6027\u6cbb\u7406\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u5957\u5b9e\u7528\u7684\u6cbb\u7406\u6846\u67b6\u3002", "motivation": "\u4e2d\u56fd\u653f\u5e9c\u63d0\u51fa\u2018\u5065\u5eb7\u4e2d\u56fd2030\u2019\u8ba1\u5212\uff0c\u5e0c\u671b\u901a\u8fc7LLM\u6280\u672f\u63a8\u52a8\u533b\u7597\u521b\u65b0\uff0c\u4f46LLM\u5728\u4f26\u7406\u548c\u60a3\u8005\u5b89\u5168\u65b9\u9762\u63d0\u51fa\u4e86\u65b0\u7684\u6311\u6218\u3002\u7814\u7a76\u65e8\u5728\u91cf\u5316\u8fd9\u4e9b\u98ce\u9669\uff0c\u5e76\u63d0\u4f9b\u6cbb\u7406\u5efa\u8bae\u4ee5\u786e\u4fddAI\u521b\u65b0\u7b26\u5408\u4f26\u7406\u6807\u51c6\u548c\u60a3\u8005\u5b89\u5168\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b12000\u9879\u7684Q&A\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8986\u76d611\u4e2a\u4f26\u7406\u7ef4\u5ea6\u548c9\u4e2a\u5b89\u5168\u7ef4\u5ea6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e2d\u6587\u533b\u7597LLM\u7684\u8868\u73b0\u3002\u7814\u7a76\u4eba\u5458\u901a\u8fc7\u8fd9\u4e00\u6570\u636e\u96c6\u8bc4\u4f30\u4e86\u591a\u4e2a\u5148\u8fdb\u7684\u4e2d\u6587\u533b\u7597LLM\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u5fae\u8c03\u4ee5\u89c2\u5bdf\u6539\u8fdb\u6548\u679c\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u73b0\u6709\u533b\u7597LLM\u5728\u4f26\u7406\u548c\u5b89\u5168\u573a\u666f\u4e2d\u7684\u8868\u73b0\u4ec5\u4e3a\u4e2d\u7b49\uff08\u5982Qwen 2.5-32B\u7684\u51c6\u786e\u7387\u4e3a42.7%\uff09\uff0c\u7ecf\u8fc7\u5fae\u8c03\u540e\u63d0\u5347\u81f350.8%\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\u5f53\u524d\u6cbb\u7406\u4f53\u7cfb\u5b58\u5728\u4e0d\u8db3\uff0c\u5982\u7f3a\u4e4f\u7ec6\u5316\u7684\u4f26\u7406\u5ba1\u8ba1\u534f\u8bae\u548c\u8bc4\u4f30\u5de5\u5177\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u4e2d\u56fd\u533b\u7597\u9886\u57df\u8feb\u5207\u9700\u8981\u52a0\u5f3aLLM\u6cbb\u7406\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6574\u5408\u4f26\u7406\u5ba1\u8ba1\u56e2\u961f\u3001\u6570\u636e\u4f26\u7406\u6307\u5357\u548c\u5b89\u5168\u6a21\u62df\u7ba1\u9053\u7684\u6846\u67b6\uff0c\u4ee5\u4fc3\u8fdbAI\u521b\u65b0\u4e0e\u60a3\u8005\u5b89\u5168\u3001\u4f26\u7406\u6807\u51c6\u7684\u534f\u8c03\u3002"}}
{"id": "2505.06301", "pdf": "https://arxiv.org/pdf/2505.06301", "abs": "https://arxiv.org/abs/2505.06301", "authors": ["Xiaozhou Ye", "Kevin I-Kai Wang"], "title": "Domain-Adversarial Anatomical Graph Networks for Cross-User Human Activity Recognition", "categories": ["cs.LG", "cs.AI", "cs.HC"], "comment": null, "summary": "Cross-user variability in Human Activity Recognition (HAR) remains a critical\nchallenge due to differences in sensor placement, body dynamics, and behavioral\npatterns. Traditional methods often fail to capture biomechanical invariants\nthat persist across users, limiting their generalization capability. We propose\nan Edge-Enhanced Graph-Based Adversarial Domain Generalization (EEG-ADG)\nframework that integrates anatomical correlation knowledge into a unified graph\nneural network (GNN) architecture. By modeling three biomechanically motivated\nrelationships together-Interconnected Units, Analogous Units, and Lateral\nUnits-our method encodes domain-invariant features while addressing\nuser-specific variability through Variational Edge Feature Extractor. A\nGradient Reversal Layer (GRL) enforces adversarial domain generalization,\nensuring robustness to unseen users. Extensive experiments on OPPORTUNITY and\nDSADS datasets demonstrate state-of-the-art performance. Our work bridges\nbiomechanical principles with graph-based adversarial learning by integrating\ninformation fusion techniques. This fusion of information underpins our unified\nand generalized model for cross-user HAR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684EEG-ADG\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u89e3\u5256\u5b66\u77e5\u8bc6\u548c\u5bf9\u6297\u57df\u6cdb\u5316\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u8de8\u7528\u6237\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\uff08HAR\uff09\u4e2d\u7684\u53d8\u5f02\u6027\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u8de8\u7528\u6237HAR\u4e2d\u5b58\u5728\u4f20\u611f\u5668\u653e\u7f6e\u3001\u8eab\u4f53\u52a8\u6001\u548c\u884c\u4e3a\u6a21\u5f0f\u7684\u5dee\u5f02\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u8de8\u7528\u6237\u7684\u751f\u7269\u529b\u5b66\u4e0d\u53d8\u6027\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86EEG-ADG\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u4e09\u7c7b\u751f\u7269\u529b\u5b66\u5173\u7cfb\uff08\u4e92\u8054\u5355\u5143\u3001\u7c7b\u6bd4\u5355\u5143\u548c\u4fa7\u5411\u5355\u5143\uff09\uff0c\u901a\u8fc7\u53d8\u5206\u8fb9\u7279\u5f81\u63d0\u53d6\u5668\u548c\u68af\u5ea6\u53cd\u8f6c\u5c42\u5b9e\u73b0\u5bf9\u6297\u57df\u6cdb\u5316\u3002", "result": "\u5728OPPORTUNITY\u548cDSADS\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5c06\u751f\u7269\u529b\u5b66\u539f\u7406\u4e0e\u56fe\u5bf9\u6297\u5b66\u4e60\u7ed3\u5408\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u4e14\u6cdb\u5316\u7684\u8de8\u7528\u6237HAR\u6a21\u578b\u3002"}}
{"id": "2505.07079", "pdf": "https://arxiv.org/pdf/2505.07079", "abs": "https://arxiv.org/abs/2505.07079", "authors": ["Robert Johansson", "Patrick Hammer", "Tony Lofthouse"], "title": "Arbitrarily Applicable Same/Opposite Relational Responding with NARS", "categories": ["cs.AI"], "comment": null, "summary": "Same/opposite relational responding, a fundamental aspect of human symbolic\ncognition, allows the flexible generalization of stimulus relationships based\non minimal experience. In this study, we demonstrate the emergence of\n\\textit{arbitrarily applicable} same/opposite relational responding within the\nNon-Axiomatic Reasoning System (NARS), a computational cognitive architecture\ndesigned for adaptive reasoning under uncertainty. Specifically, we extend NARS\nwith an implementation of \\textit{acquired relations}, enabling the system to\nexplicitly derive both symmetric (mutual entailment) and novel relational\ncombinations (combinatorial entailment) from minimal explicit training in a\ncontextually controlled matching-to-sample (MTS) procedure. Experimental\nresults show that NARS rapidly internalizes explicitly trained relational rules\nand robustly demonstrates derived relational generalizations based on arbitrary\ncontextual cues. Importantly, derived relational responding in critical test\nphases inherently combines both mutual and combinatorial entailments, such as\nderiving same-relations from multiple explicitly trained opposite-relations.\nInternal confidence metrics illustrate strong internalization of these\nrelational principles, closely paralleling phenomena observed in human\nrelational learning experiments. Our findings underscore the potential for\nintegrating nuanced relational learning mechanisms inspired by learning\npsychology into artificial general intelligence frameworks, explicitly\nhighlighting the arbitrary and context-sensitive relational capabilities\nmodeled within NARS.", "AI": {"tldr": "\u8bba\u6587\u5c55\u793a\u4e86\u975e\u516c\u7406\u63a8\u7406\u7cfb\u7edf\uff08NARS\uff09\u5982\u4f55\u901a\u8fc7\u6700\u5c0f\u8bad\u7ec3\u5b9e\u73b0\u968f\u610f\u9002\u7528\u7684\u76f8\u540c/\u76f8\u53cd\u5173\u7cfb\u54cd\u5e94\uff0c\u5e76\u6269\u5c55\u4e86\u5176\u80fd\u529b\u4ee5\u652f\u6301\u5bf9\u79f0\u548c\u65b0\u9896\u5173\u7cfb\u7ec4\u5408\u7684\u63a8\u5bfc\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4e86\u7cfb\u7edf\u5feb\u901f\u5185\u5316\u5173\u7cfb\u89c4\u5219\u5e76\u80fd\u7075\u6d3b\u63a8\u5e7f\u7684\u80fd\u529b\uff0c\u4e0e\u4eba\u5173\u7cfb\u5b66\u4e60\u73b0\u8c61\u76f8\u4f3c\uff0c\u7a81\u663e\u4e86\u5176\u5728\u4eba\u5de5\u901a\u7528\u667a\u80fd\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5728\u8ba1\u7b97\u8ba4\u77e5\u67b6\u6784\u4e2d\u5b9e\u73b0\u4eba\u7c7b\u7b26\u53f7\u8ba4\u77e5\u7684\u57fa\u672c\u80fd\u529b\u2014\u2014\u76f8\u540c/\u76f8\u53cd\u5173\u7cfb\u54cd\u5e94\uff0c\u4ee5\u589e\u5f3a\u4eba\u5de5\u667a\u80fd\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u7684\u81ea\u9002\u5e94\u63a8\u7406\u80fd\u529b\u3002", "method": "\u6269\u5c55NARS\u7cfb\u7edf\uff0c\u5f15\u5165'\u83b7\u5f97\u5173\u7cfb'\u7684\u5b9e\u73b0\uff0c\u5e76\u901a\u8fc7\u4e0a\u4e0b\u6587\u63a7\u5236\u7684\u5339\u914d\u5230\u6837\u672c\uff08MTS\uff09\u7a0b\u5e8f\u8fdb\u884c\u6700\u5c0f\u663e\u5f0f\u8bad\u7ec3\uff0c\u4ee5\u652f\u6301\u5bf9\u79f0\uff08\u4e92\u8574\u542b\uff09\u548c\u65b0\u9896\u5173\u7cfb\u7ec4\u5408\uff08\u7ec4\u5408\u8574\u542b\uff09\u7684\u63a8\u5bfc\u3002", "result": "NARS\u80fd\u5feb\u901f\u5185\u5316\u663e\u5f0f\u8bad\u7ec3\u7684\u5173\u7cfb\u89c4\u5219\uff0c\u5e76\u5728\u5173\u952e\u6d4b\u8bd5\u9636\u6bb5\u5c55\u793a\u57fa\u4e8e\u4efb\u610f\u4e0a\u4e0b\u6587\u7ebf\u7d22\u7684\u6d3e\u751f\u5173\u7cfb\u63a8\u5e7f\u80fd\u529b\u3002\u5185\u90e8\u7f6e\u4fe1\u5ea6\u6307\u6807\u8868\u660e\u4e86\u5173\u7cfb\u539f\u5219\u7684\u5f3a\u5185\u5316\uff0c\u4e0e\u4eba\u5173\u7cfb\u5b66\u4e60\u73b0\u8c61\u76f8\u8fd1\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5c06\u5fc3\u7406\u5b66\u4e2d\u5fae\u5999\u7684\u5173\u7cfb\u5b66\u4e60\u673a\u5236\u6574\u5408\u5230\u4eba\u5de5\u901a\u7528\u667a\u80fd\u6846\u67b6\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662fNARS\u6240\u6a21\u62df\u7684\u968f\u610f\u548c\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u5173\u7cfb\u80fd\u529b\u3002"}}
{"id": "2505.07233", "pdf": "https://arxiv.org/pdf/2505.07233", "abs": "https://arxiv.org/abs/2505.07233", "authors": ["Jiashuo Sun", "Xianrui Zhong", "Sizhe Zhou", "Jiawei Han"], "title": "DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": "24 pages, 6 figures, 15 tables", "summary": "Retrieval-augmented generation (RAG) systems combine large language models\n(LLMs) with external knowledge retrieval, making them highly effective for\nknowledge-intensive tasks. A crucial but often under-explored component of\nthese systems is the reranker, which refines retrieved documents to enhance\ngeneration quality and explainability. The challenge of selecting the optimal\nnumber of documents (k) remains unsolved: too few may omit critical\ninformation, while too many introduce noise and inefficiencies. Although recent\nstudies have explored LLM-based rerankers, they primarily leverage internal\nmodel knowledge and overlook the rich supervisory signals that LLMs can\nprovide, such as using response quality as feedback for optimizing reranking\ndecisions. In this paper, we propose DynamicRAG, a novel RAG framework where\nthe reranker dynamically adjusts both the order and number of retrieved\ndocuments based on the query. We model the reranker as an agent optimized\nthrough reinforcement learning (RL), using rewards derived from LLM output\nquality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates\nsuperior performance, achieving state-of-the-art results. The model, data and\ncode are available at https://github.com/GasolSun36/DynamicRAG", "AI": {"tldr": "\u63d0\u51fa\u4e86DynamicRAG\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u52a8\u6001\u8c03\u6574\u68c0\u7d22\u6587\u6863\u6570\u91cf\u548c\u987a\u5e8f\u7684reranker\uff0c\u63d0\u5347\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u7684\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5f53\u524dRAG\u7cfb\u7edf\u4e2d\u7684reranker\u7ec4\u4ef6\u5e38\u88ab\u5ffd\u89c6\uff0c\u4e14\u56fa\u5b9a\u6587\u6863\u6570\u91cfk\u53ef\u80fd\u5bfc\u81f4\u4fe1\u606f\u9057\u6f0f\u6216\u566a\u58f0\u5f15\u5165\u3002\u73b0\u6709\u7814\u7a76\u672a\u5145\u5206\u5229\u7528LLM\u53cd\u9988\u4fe1\u53f7\u4f18\u5316reranking\u51b3\u7b56\u3002", "method": "\u5c06reranker\u5efa\u6a21\u4e3a\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u6839\u636eLLM\u8f93\u51fa\u8d28\u91cf\u4f5c\u4e3a\u5956\u52b1\u52a8\u6001\u8c03\u6574\u68c0\u7d22\u6587\u6863\u7684\u987a\u5e8f\u548c\u6570\u91cf\u3002", "result": "\u5728\u4e03\u4e2a\u77e5\u8bc6\u5bc6\u96c6\u578b\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f18\u6027\u80fd\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002", "conclusion": "DynamicRAG\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u7b56\u7565\u663e\u8457\u63d0\u5347RAG\u7cfb\u7edf\u6548\u679c\uff0c\u4e3areranker\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.06302", "pdf": "https://arxiv.org/pdf/2505.06302", "abs": "https://arxiv.org/abs/2505.06302", "authors": ["Xuzhi Zhang", "Shaohui Peng", "Qirui Zhou", "Yuanbo Wen", "Qi Guo", "Ruizhi Chen", "Xinguo Zhu", "Weiqiang Xiong", "Haixin Chen", "Congying Ma", "Ke Gao", "Chen Zhao", "Yanjun Wu", "Yunji Chen", "Ling Li"], "title": "QiMeng-TensorOp: Automatically Generating High-Performance Tensor Operators with Hardware Primitives", "categories": ["cs.LG", "cs.AI", "I.2.2"], "comment": "10 pages, 5 figures", "summary": "Computation-intensive tensor operators constitute over 90\\% of the\ncomputations in Large Language Models (LLMs) and Deep Neural\nNetworks.Automatically and efficiently generating high-performance tensor\noperators with hardware primitives is crucial for diverse and ever-evolving\nhardware architectures like RISC-V, ARM, and GPUs, as manually optimized\nimplementation takes at least months and lacks portability.LLMs excel at\ngenerating high-level language codes, but they struggle to fully comprehend\nhardware characteristics and produce high-performance tensor operators. We\nintroduce a tensor-operator auto-generation framework with a one-line user\nprompt (QiMeng-TensorOp), which enables LLMs to automatically exploit hardware\ncharacteristics to generate tensor operators with hardware primitives, and tune\nparameters for optimal performance across diverse hardware. Experimental\nresults on various hardware platforms, SOTA LLMs, and typical tensor operators\ndemonstrate that QiMeng-TensorOp effectively unleashes the computing capability\nof various hardware platforms, and automatically generates tensor operators of\nsuperior performance. Compared with vanilla LLMs, QiMeng-TensorOp achieves up\nto $1291 \\times$ performance improvement. Even compared with human experts,\nQiMeng-TensorOp could reach $251 \\%$ of OpenBLAS on RISC-V CPUs, and $124 \\%$\nof cuBLAS on NVIDIA GPUs. Additionally, QiMeng-TensorOp also significantly\nreduces development costs by $200 \\times$ compared with human experts.", "AI": {"tldr": "QiMeng-TensorOp \u662f\u4e00\u4e2a\u57fa\u4e8e LLMs \u7684\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u4e00\u884c\u7528\u6237\u63d0\u793a\u81ea\u52a8\u751f\u6210\u9ad8\u6027\u80fd\u5f20\u91cf\u7b97\u5b50\uff0c\u5229\u7528\u786c\u4ef6\u7279\u6027\u4f18\u5316\u53c2\u6570\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6027\u80fd\u5e76\u964d\u4f4e\u5f00\u53d1\u6210\u672c\u3002", "motivation": "\u624b\u5de5\u4f18\u5316\u7684\u5f20\u91cf\u7b97\u5b50\u5f00\u53d1\u5468\u671f\u957f\u4e14\u7f3a\u4e4f\u53ef\u79fb\u690d\u6027\uff0c\u800c\u73b0\u6709\u7684 LLMs \u96be\u4ee5\u5145\u5206\u7406\u89e3\u786c\u4ef6\u7279\u6027\u6765\u751f\u6210\u9ad8\u6027\u80fd\u4ee3\u7801\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u81ea\u52a8\u751f\u6210\u5e76\u4f18\u5316\u5f20\u91cf\u7b97\u5b50\u7684\u5de5\u5177\u4ee5\u9002\u914d\u591a\u6837\u5316\u786c\u4ef6\u3002", "method": "\u63d0\u51fa\u4e86 QiMeng-TensorOp \u6846\u67b6\uff0c\u901a\u8fc7 LLMs \u81ea\u52a8\u5206\u6790\u786c\u4ef6\u7279\u6027\u5e76\u751f\u6210\u9ad8\u6027\u80fd\u5f20\u91cf\u7b97\u5b50\uff0c\u540c\u65f6\u8c03\u4f18\u53c2\u6570\u4ee5\u9002\u914d\u4e0d\u540c\u786c\u4ef6\u5e73\u53f0\u3002", "result": "\u5728\u591a\u79cd\u786c\u4ef6\u5e73\u53f0\u4e0a\uff0cQiMeng-TensorOp \u751f\u6210\u7684\u7b97\u5b50\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u666e\u901a LLMs \u548c\u4eba\u5de5\u4f18\u5316\u65b9\u6848\uff08\u5982 1291 \u500d\u6027\u80fd\u63d0\u5347\uff09\uff0c\u540c\u65f6\u5f00\u53d1\u6210\u672c\u964d\u4f4e 200 \u500d\u3002", "conclusion": "QiMeng-TensorOp \u6709\u6548\u89e3\u51b3\u4e86\u5f20\u91cf\u7b97\u5b50\u81ea\u52a8\u751f\u6210\u7684\u96be\u9898\uff0c\u4e3a AI \u6a21\u578b\u7684\u786c\u4ef6\u9002\u914d\u548c\u9ad8\u6027\u80fd\u8ba1\u7b97\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.07087", "pdf": "https://arxiv.org/pdf/2505.07087", "abs": "https://arxiv.org/abs/2505.07087", "authors": ["Robert E. Wray", "James R. Kirk", "John E. Laird"], "title": "Architectural Precedents for General Agents using Large Language Models", "categories": ["cs.AI", "I.2.11; I.2.7"], "comment": "14 pages, 2 figures. Submitted to AGI25", "summary": "One goal of AI (and AGI) is to identify and understand specific mechanisms\nand representations sufficient for general intelligence. Often, this work\nmanifests in research focused on architectures and many cognitive architectures\nhave been explored in AI/AGI. However, different research groups and even\ndifferent research traditions have somewhat independently identified\nsimilar/common patterns of processes and representations or cognitive design\npatterns that are manifest in existing architectures. Today, AI systems\nexploiting large language models (LLMs) offer a relatively new combination of\nmechanism and representation available for exploring the possibilities of\ngeneral intelligence. In this paper, we summarize a few recurring cognitive\ndesign patterns that have appeared in various pre-transformer AI architectures.\nWe then explore how these patterns are evident in systems using LLMs,\nespecially for reasoning and interactive (\"agentic\") use cases. By examining\nand applying these recurring patterns, we can also predict gaps or deficiencies\nin today's Agentic LLM Systems and identify likely subjects of future research\ntowards general intelligence using LLMs and other generative foundation models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u603b\u7ed3\u4e86\u5728\u9884Transformer AI\u67b6\u6784\u4e2d\u53cd\u590d\u51fa\u73b0\u7684\u8ba4\u77e5\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5e76\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u6a21\u5f0f\u5728\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\uff08\u5c24\u5176\u662f\u63a8\u7406\u548c\u4ea4\u4e92\u5f0f\u5e94\u7528\uff09\u4e2d\u7684\u4f53\u73b0\u3002\u901a\u8fc7\u5206\u6790\u8fd9\u4e9b\u6a21\u5f0f\uff0c\u53ef\u4ee5\u9884\u6d4b\u5f53\u524dAgentic LLM\u7cfb\u7edf\u7684\u4e0d\u8db3\uff0c\u5e76\u4e3a\u672a\u6765\u901a\u7528\u667a\u80fd\u7814\u7a76\u6307\u660e\u65b9\u5411\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u8bc6\u522b\u548c\u7406\u89e3\u901a\u7528\u667a\u80fd\u7684\u673a\u5236\u548c\u8868\u5f81\u3002\u5f53\u524d\uff0c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684AI\u7cfb\u7edf\u4e3a\u63a2\u7d22\u901a\u7528\u667a\u80fd\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\uff0c\u4f46\u9700\u8981\u7cfb\u7edf\u5316\u5206\u6790\u5df2\u6709\u8ba4\u77e5\u6a21\u5f0f\u5728\u8fd9\u4e00\u65b0\u9886\u57df\u7684\u9002\u7528\u6027\u548c\u4e0d\u8db3\u3002", "method": "\u603b\u7ed3\u9884Transformer\u67b6\u6784\u4e2d\u5e38\u89c1\u7684\u8ba4\u77e5\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5e76\u5206\u6790\u5176\u5728\u57fa\u4e8eLLMs\u7684\u7cfb\u7edf\u4e2d\u7684\u4f53\u73b0\uff0c\u5c24\u5176\u662f\u63a8\u7406\u548c\u4ee3\u7406\uff08Agentic\uff09\u5e94\u7528\u573a\u666f\u3002", "result": "\u53d1\u73b0LLM\u7cfb\u7edf\u4e2d\u5b58\u5728\u4e0e\u5386\u53f2\u8ba4\u77e5\u6a21\u5f0f\u76f8\u4f3c\u7684\u673a\u5236\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u5f53\u524d\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u63a8\u7406\u548c\u4ee3\u7406\u529f\u80fd\u4e0a\u7684\u4e0d\u8db3\u3002", "conclusion": "\u901a\u8fc7\u5e94\u7528\u5386\u53f2\u8ba4\u77e5\u6a21\u5f0f\u53ef\u6307\u5bfcLLMs\u7684\u6539\u8fdb\uff0c\u672a\u6765\u7814\u7a76\u9700\u586b\u8865\u5176\u5728\u901a\u7528\u667a\u80fd\u4efb\u52a1\u4e2d\u7684\u7f3a\u9677\uff0c\u5c24\u5176\u662f\u5bf9\u751f\u6210\u5f0f\u57fa\u7840\u6a21\u578b\u7684\u6df1\u5165\u63a2\u7d22\u3002"}}
{"id": "2505.07247", "pdf": "https://arxiv.org/pdf/2505.07247", "abs": "https://arxiv.org/abs/2505.07247", "authors": ["Peichao Lai", "Kexuan Zhang", "Yi Lin", "Linyihan Zhang", "Feiyang Ye", "Jinhao Yan", "Yanwei Xu", "Conghui He", "Yilei Wang", "Wentao Zhang", "Bin Cui"], "title": "SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Subjective Answer Grading (SAG) plays a crucial role in education,\nstandardized testing, and automated assessment systems, particularly for\nevaluating short-form responses in Short Answer Scoring (SAS). However,\nexisting approaches often produce coarse-grained scores and lack detailed\nreasoning. Although large language models (LLMs) have demonstrated potential as\nzero-shot evaluators, they remain susceptible to bias, inconsistencies with\nhuman judgment, and limited transparency in scoring decisions. To overcome\nthese limitations, we introduce SAS-Bench, a benchmark specifically designed\nfor LLM-based SAS tasks. SAS-Bench provides fine-grained, step-wise scoring,\nexpert-annotated error categories, and a diverse range of question types\nderived from real-world subject-specific exams. This benchmark facilitates\ndetailed evaluation of model reasoning processes and explainability. We also\nrelease an open-source dataset containing 1,030 questions and 4,109 student\nresponses, each annotated by domain experts. Furthermore, we conduct\ncomprehensive experiments with various LLMs, identifying major challenges in\nscoring science-related questions and highlighting the effectiveness of\nfew-shot prompting in improving scoring accuracy. Our work offers valuable\ninsights into the development of more robust, fair, and educationally\nmeaningful LLM-based evaluation systems.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86SAS-Bench\uff0c\u4e00\u4e2a\u4e13\u4e3aLLM\u8bbe\u8ba1\u7684\u77ed\u7b54\u6848\u8bc4\u5206\u57fa\u51c6\uff0c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u8bc4\u5206\u3001\u4e13\u5bb6\u6807\u6ce8\u7684\u9519\u8bef\u7c7b\u522b\u53ca\u591a\u6837\u9898\u578b\uff0c\u65e8\u5728\u6539\u5584\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u4e3b\u89c2\u7b54\u6848\u8bc4\u5206\u65b9\u6cd5\u8bc4\u5206\u7c97\u7cd9\u4e14\u7f3a\u4e4f\u8be6\u7ec6\u63a8\u7406\uff0cLLM\u4f5c\u4e3a\u96f6\u6837\u672c\u8bc4\u4f30\u5668\u6613\u53d7\u504f\u5dee\u5f71\u54cd\u4e14\u900f\u660e\u5ea6\u4e0d\u8db3\u3002", "method": "\u5f15\u5165SAS-Bench\u57fa\u51c6\uff0c\u5305\u542b1,030\u9898\u4e0e4,109\u6807\u6ce8\u7b54\u6848\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5c11\u6837\u672c\u63d0\u793a\u63d0\u5347\u8bc4\u5206\u51c6\u786e\u6027\u3002", "result": "\u53d1\u73b0\u79d1\u5b66\u7c7b\u9898\u76ee\u8bc4\u5206\u6311\u6218\uff0c\u5c11\u6837\u672c\u63d0\u793a\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\uff0c\u6570\u636e\u96c6\u4e0e\u57fa\u51c6\u4fc3\u8fdb\u6a21\u578b\u63a8\u7406\u4e0e\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u3002", "conclusion": "SAS-Bench\u4e3a\u5f00\u53d1\u66f4\u5065\u58ee\u3001\u516c\u5e73\u4e14\u6559\u80b2\u610f\u4e49\u5f3a\u7684LLM\u8bc4\u4f30\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2505.06303", "pdf": "https://arxiv.org/pdf/2505.06303", "abs": "https://arxiv.org/abs/2505.06303", "authors": ["Li Yuan", "Yi Cai", "Xudong Shen", "Qing Li", "Qingbao Huang", "Zikun Deng", "Tao Wang"], "title": "Collaborative Multi-LoRA Experts with Achievement-based Multi-Tasks Loss for Unified Multimodal Information Extraction", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by IJCAI 2025", "summary": "Multimodal Information Extraction (MIE) has gained attention for extracting\nstructured information from multimedia sources. Traditional methods tackle MIE\ntasks separately, missing opportunities to share knowledge across tasks. Recent\napproaches unify these tasks into a generation problem using instruction-based\nT5 models with visual adaptors, optimized through full-parameter fine-tuning.\nHowever, this method is computationally intensive, and multi-task fine-tuning\noften faces gradient conflicts, limiting performance. To address these\nchallenges, we propose collaborative multi-LoRA experts with achievement-based\nmulti-task loss (C-LoRAE) for MIE tasks. C-LoRAE extends the low-rank\nadaptation (LoRA) method by incorporating a universal expert to learn shared\nmultimodal knowledge from cross-MIE tasks and task-specific experts to learn\nspecialized instructional task features. This configuration enhances the\nmodel's generalization ability across multiple tasks while maintaining the\nindependence of various instruction tasks and mitigating gradient conflicts.\nAdditionally, we propose an achievement-based multi-task loss to balance\ntraining progress across tasks, addressing the imbalance caused by varying\nnumbers of training samples in MIE tasks. Experimental results on seven\nbenchmark datasets across three key MIE tasks demonstrate that C-LoRAE achieves\nsuperior overall performance compared to traditional fine-tuning methods and\nLoRA methods while utilizing a comparable number of training parameters to\nLoRA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86C-LoRAE\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u901a\u7528\u4e13\u5bb6\u548c\u4efb\u52a1\u7279\u5b9a\u4e13\u5bb6\u7684\u591aLoRA\u67b6\u6784\uff0c\u4ee5\u53ca\u57fa\u4e8e\u6210\u5c31\u7684\u591a\u4efb\u52a1\u635f\u5931\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u4fe1\u606f\u63d0\u53d6\uff08MIE\uff09\u4efb\u52a1\u4e2d\u7684\u68af\u5ea6\u51b2\u7a81\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfMIE\u65b9\u6cd5\u4efb\u52a1\u72ec\u7acb\u5904\u7406\uff0c\u7f3a\u4e4f\u77e5\u8bc6\u5171\u4eab\uff0c\u800c\u73b0\u6709\u57fa\u4e8eT5\u7684\u7edf\u4e00\u751f\u6210\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5b58\u5728\u68af\u5ea6\u51b2\u7a81\u3002", "method": "C-LoRAE\u7ed3\u5408\u901a\u7528\u4e13\u5bb6\u5b66\u4e60\u8de8\u4efb\u52a1\u5171\u4eab\u7684\u591a\u6a21\u6001\u77e5\u8bc6\uff0c\u4efb\u52a1\u7279\u5b9a\u4e13\u5bb6\u5b66\u4e60\u5177\u4f53\u4efb\u52a1\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u6210\u5c31\u7684\u591a\u4efb\u52a1\u635f\u5931\u5e73\u8861\u8bad\u7ec3\u8fdb\u5ea6\u3002", "result": "\u57287\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cC-LoRAE\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u5fae\u8c03\u548cLoRA\u65b9\u6cd5\uff0c\u4e14\u53c2\u6570\u91cf\u4e0eLoRA\u76f8\u5f53\u3002", "conclusion": "C-LoRAE\u6709\u6548\u63d0\u5347\u4e86\u591a\u4efb\u52a1MIE\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u4e3a\u591a\u6a21\u6001\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.07089", "pdf": "https://arxiv.org/pdf/2505.07089", "abs": "https://arxiv.org/abs/2505.07089", "authors": ["Hanzheng Dai", "Yuanliang Li", "Zhibo Zhang", "Jun Yan"], "title": "RefPentester: A Knowledge-Informed Self-Reflective Penetration Testing Framework Based on Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Automated penetration testing (AutoPT) powered by large language models\n(LLMs) has gained attention for its ability to automate ethical hacking\nprocesses and identify vulnerabilities in target systems by leveraging the\nintrinsic knowledge of LLMs. However, existing LLM-based AutoPT frameworks\noften underperform compared to human experts in challenging tasks for several\nreasons: the imbalanced knowledge used in LLM training, short-sighted planning\nin the planning process, and hallucinations during command generation. In\naddition, the penetration testing (PT) process, with its trial-and-error\nnature, is limited by existing frameworks that lack mechanisms to learn from\nprevious failed operations, restricting adaptive improvement of PT strategies.\nTo address these limitations, we propose a knowledge-informed self-reflective\nPT framework powered by LLMs, called RefPentester, which is an AutoPT framework\ndesigned to assist human operators in identifying the current stage of the PT\nprocess, selecting appropriate tactic and technique for the stage, choosing\nsuggested action, providing step-by-step operational guidance, and learning\nfrom previous failed operations. We also modeled the PT process as a\nseven-state Stage Machine to integrate the proposed framework effectively. The\nevaluation shows that RefPentester can successfully reveal credentials on Hack\nThe Box's Sau machine, outperforming the baseline GPT-4o model by 16.7\\%.\nAcross PT stages, RefPentester also demonstrates superior success rates on PT\nstage transitions.", "AI": {"tldr": "RefPentester\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u6e17\u900f\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u589e\u5f3a\u548c\u81ea\u6211\u53cd\u601d\u673a\u5236\u63d0\u5347\u6548\u80fd\uff0c\u6bd4GPT-4o\u57fa\u51c6\u6a21\u578b\u8868\u73b0\u9ad8\u51fa16.7%\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u6e17\u900f\u6d4b\u8bd5\u6846\u67b6\u5728\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u5982\u4eba\u7c7b\u4e13\u5bb6\uff0c\u539f\u56e0\u5305\u62ec\u77e5\u8bc6\u4e0d\u5e73\u8861\u3001\u77ed\u89c6\u89c4\u5212\u4ee5\u53ca\u547d\u4ee4\u751f\u6210\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u4ece\u5931\u8d25\u64cd\u4f5c\u4e2d\u5b66\u4e60\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faRefPentester\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u589e\u5f3a\u548c\u81ea\u6211\u53cd\u601d\u673a\u5236\u8f85\u52a9\u4eba\u7c7b\u64cd\u4f5c\u8005\u8bc6\u522b\u6e17\u900f\u6d4b\u8bd5\u9636\u6bb5\u3001\u9009\u62e9\u9002\u5f53\u6218\u672f\u548c\u6280\u672f\u3001\u63d0\u4f9b\u64cd\u4f5c\u6307\u5bfc\uff0c\u5e76\u4ece\u5931\u8d25\u4e2d\u5b66\u4e60\u3002\u6e17\u900f\u6d4b\u8bd5\u8fc7\u7a0b\u88ab\u5efa\u6a21\u4e3a\u4e03\u72b6\u6001\u9636\u6bb5\u673a\u3002", "result": "RefPentester\u6210\u529f\u5728Hack The Box\u7684Sau\u673a\u5668\u4e0a\u63ed\u793a\u4e86\u51ed\u8bc1\uff0c\u8868\u73b0\u4f18\u4e8eGPT-4o\u57fa\u51c6\u6a21\u578b16.7%\uff0c\u5e76\u5728\u6e17\u900f\u6d4b\u8bd5\u9636\u6bb5\u8f6c\u6362\u4e2d\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3002", "conclusion": "RefPentester\u901a\u8fc7\u77e5\u8bc6\u589e\u5f3a\u548c\u81ea\u6211\u53cd\u601d\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u5316\u6e17\u900f\u6d4b\u8bd5\u7684\u8868\u73b0\uff0c\u663e\u793a\u51fa\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.07258", "pdf": "https://arxiv.org/pdf/2505.07258", "abs": "https://arxiv.org/abs/2505.07258", "authors": ["Wenqiang Wang", "Siyuan Liang", "Yangshijie Zhang", "Xiaojun Jia", "Hao Lin", "Xiaochun Cao"], "title": "No Query, No Access", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Textual adversarial attacks mislead NLP models, including Large Language\nModels (LLMs), by subtly modifying text. While effective, existing attacks\noften require knowledge of the victim model, extensive queries, or access to\ntraining data, limiting real-world feasibility. To overcome these constraints,\nwe introduce the \\textbf{Victim Data-based Adversarial Attack (VDBA)}, which\noperates using only victim texts. To prevent access to the victim model, we\ncreate a shadow dataset with publicly available pre-trained models and\nclustering methods as a foundation for developing substitute models. To address\nthe low attack success rate (ASR) due to insufficient information feedback, we\npropose the hierarchical substitution model design, generating substitute\nmodels to mitigate the failure of a single substitute model at the decision\nboundary.\n  Concurrently, we use diverse adversarial example generation, employing\nvarious attack methods to generate and select the adversarial example with\nbetter similarity and attack effectiveness. Experiments on the Emotion and SST5\ndatasets show that VDBA outperforms state-of-the-art methods, achieving an ASR\nimprovement of 52.08\\% while significantly reducing attack queries to 0. More\nimportantly, we discover that VDBA poses a significant threat to LLMs such as\nQwen2 and the GPT family, and achieves the highest ASR of 45.99% even without\naccess to the API, confirming that advanced NLP models still face serious\nsecurity risks. Our codes can be found at\nhttps://anonymous.4open.science/r/VDBA-Victim-Data-based-Adversarial-Attack-36EC/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d7\u5bb3\u8005\u6587\u672c\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff08VDBA\uff09\uff0c\u4ec5\u5229\u7528\u53d7\u5bb3\u8005\u6587\u672c\u5c31\u80fd\u6709\u6548\u653b\u51fbNLP\u6a21\u578b\uff0c\u4e14\u65e0\u9700\u8bbf\u95ee\u53d7\u5bb3\u8005\u6a21\u578b\u3002\u901a\u8fc7\u6784\u5efa\u5f71\u5b50\u6570\u636e\u96c6\u548c\u5206\u5c42\u66ff\u4ee3\u6a21\u578b\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653b\u51fb\u6210\u529f\u7387\uff08ASR\uff09\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5bf9LLM\uff08\u5982Qwen2\u548cGPT\u7cfb\u5217\uff09\u7684\u5a01\u80c1\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4e86\u89e3\u53d7\u5bb3\u8005\u6a21\u578b\u3001\u5927\u91cf\u67e5\u8be2\u6216\u8bad\u7ec3\u6570\u636e\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u53ef\u884c\u6027\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u4ec5\u4f9d\u8d56\u53d7\u5bb3\u8005\u6587\u672c\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u63d0\u5347\u653b\u51fb\u6548\u7387\u5e76\u964d\u4f4e\u4f9d\u8d56\u3002", "method": "\u63d0\u51fa\u4e86VDBA\u65b9\u6cd5\uff0c\u5229\u7528\u516c\u5f00\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u805a\u7c7b\u65b9\u6cd5\u6784\u5efa\u5f71\u5b50\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u5206\u5c42\u66ff\u4ee3\u6a21\u578b\u4ee5\u89e3\u51b3\u5355\u4e00\u6a21\u578b\u5728\u51b3\u7b56\u8fb9\u754c\u5931\u6548\u7684\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u591a\u6837\u5316\u5bf9\u6297\u6837\u672c\u751f\u6210\u7b56\u7565\u4f18\u5316\u653b\u51fb\u6548\u679c\u3002", "result": "\u5728Emotion\u548cSST5\u6570\u636e\u96c6\u4e0a\uff0cVDBA\u7684ASR\u63d0\u5347\u4e8652.08%\uff0c\u4e14\u653b\u51fb\u67e5\u8be2\u6b21\u6570\u964d\u4e3a0\u3002\u5bf9Qwen2\u548cGPT\u7cfb\u5217LLM\u7684\u653b\u51fbASR\u8fbe45.99%\uff0c\u9a8c\u8bc1\u4e86\u5176\u5a01\u80c1\u6027\u3002", "conclusion": "VDBA\u65e0\u9700\u8bbf\u95eeAPI\u5373\u53ef\u5bf9\u9ad8\u7ea7NLP\u6a21\u578b\u6784\u6210\u4e25\u91cd\u5b89\u5168\u5a01\u80c1\uff0c\u8868\u660e\u5f53\u524d\u6a21\u578b\u4ecd\u5b58\u5728\u91cd\u5927\u5b89\u5168\u98ce\u9669\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.06316", "pdf": "https://arxiv.org/pdf/2505.06316", "abs": "https://arxiv.org/abs/2505.06316", "authors": ["Guozhong Li", "Muhannad Alhumaidi", "Spiros Skiadopoulos", "Ibrahim Hoteit", "Panos Kalnis"], "title": "GraphComp: Extreme Error-bounded Compression of Scientific Data via Temporal Graph Autoencoders", "categories": ["cs.LG"], "comment": null, "summary": "The generation of voluminous scientific data poses significant challenges for\nefficient storage, transfer, and analysis. Recently, error-bounded lossy\ncompression methods emerged due to their ability to achieve high compression\nratios while controlling data distortion. However, they often overlook the\ninherent spatial and temporal correlations within scientific data, thus missing\nopportunities for higher compression. In this paper we propose GRAPHCOMP, a\nnovel graph-based method for error-bounded lossy compression of scientific\ndata. We perform irregular segmentation of the original grid data and generate\na graph representation that preserves the spatial and temporal correlations.\nInspired by Graph Neural Networks (GNNs), we then propose a temporal graph\nautoencoder to learn latent representations that significantly reduce the size\nof the graph, effectively compressing the original data. Decompression reverses\nthe process and utilizes the learnt graph model together with the latent\nrepresentation to reconstruct an approximation of the original data. The\ndecompressed data are guaranteed to satisfy a user-defined point-wise error\nbound. We compare our method against the state-of-the-art error-bounded lossy\nmethods (i.e., HPEZ, SZ3.1, SPERR, and ZFP) on large-scale real and synthetic\ndata. GRAPHCOMP consistently achieves the highest compression ratio across most\ndatasets, outperforming the second-best method by margins ranging from 22% to\n50%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86GRAPHCOMP\uff0c\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u8bef\u5dee\u6709\u754c\u6709\u635f\u538b\u7f29\u65b9\u6cd5\uff0c\u5229\u7528\u65f6\u7a7a\u76f8\u5173\u6027\u5b9e\u73b0\u9ad8\u538b\u7f29\u6bd4\uff0c\u5e76\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u79d1\u5b66\u6570\u636e\u91cf\u5927\uff0c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u65f6\u7a7a\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u538b\u7f29\u6548\u7387\u4e0d\u9ad8\u3002", "method": "\u901a\u8fc7\u4e0d\u89c4\u5219\u5206\u5272\u539f\u59cb\u7f51\u683c\u6570\u636e\u5e76\u751f\u6210\u56fe\u8868\u793a\uff0c\u7ed3\u5408\u65f6\u5e8f\u56fe\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u6f5c\u5728\u8868\u793a\u4ee5\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\u3002", "result": "GRAPHCOMP\u5728\u5927\u591a\u6570\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u538b\u7f29\u6bd4\uff0c\u6bd4\u7b2c\u4e8c\u4f18\u65b9\u6cd5\u63d0\u534722%-50%\u3002", "conclusion": "GRAPHCOMP\u901a\u8fc7\u5229\u7528\u65f6\u7a7a\u76f8\u5173\u6027\u548c\u56fe\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79d1\u5b66\u6570\u636e\u7684\u538b\u7f29\u6548\u7387\u3002"}}
{"id": "2505.07171", "pdf": "https://arxiv.org/pdf/2505.07171", "abs": "https://arxiv.org/abs/2505.07171", "authors": ["Jeongho Kim", "Chanyeong Heo", "Jaehee Jung"], "title": "ReCDAP: Relation-Based Conditional Diffusion with Attention Pooling for Few-Shot Knowledge Graph Completion", "categories": ["cs.AI", "cs.IR"], "comment": "Accepted by SIGIR 2025, 5 pages, 1 figure", "summary": "Knowledge Graphs (KGs), composed of triples in the form of (head, relation,\ntail) and consisting of entities and relations, play a key role in information\nretrieval systems such as question answering, entity search, and\nrecommendation. In real-world KGs, although many entities exist, the relations\nexhibit a long-tail distribution, which can hinder information retrieval\nperformance. Previous few-shot knowledge graph completion studies focused\nexclusively on the positive triple information that exists in the graph or,\nwhen negative triples were incorporated, used them merely as a signal to\nindicate incorrect triples. To overcome this limitation, we propose\nRelation-Based Conditional Diffusion with Attention Pooling (ReCDAP). First,\nnegative triples are generated by randomly replacing the tail entity in the\nsupport set. By conditionally incorporating positive information in the KG and\nnon-existent negative information into the diffusion process, the model\nseparately estimates the latent distributions for positive and negative\nrelations. Moreover, including an attention pooler enables the model to\nleverage the differences between positive and negative cases explicitly.\nExperiments on two widely used datasets demonstrate that our method outperforms\nexisting approaches, achieving state-of-the-art performance. The code is\navailable at https://github.com/hou27/ReCDAP-FKGC.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5173\u7cfb\u7684\u6761\u4ef6\u6269\u6563\u4e0e\u6ce8\u610f\u529b\u6c60\u5316\uff08ReCDAP\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u8865\u5168\u3002\u901a\u8fc7\u7ed3\u5408\u6b63\u8d1f\u4e09\u5143\u7ec4\u4fe1\u606f\u5e76\u5229\u7528\u6269\u6563\u8fc7\u7a0b\uff0c\u6a21\u578b\u80fd\u591f\u6709\u6548\u5904\u7406\u957f\u5c3e\u5206\u5e03\u7684\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u77e5\u8bc6\u56fe\u8c31\u4e2d\u5173\u7cfb\u7684\u957f\u5c3e\u5206\u5e03\u5f71\u54cd\u4e86\u4fe1\u606f\u68c0\u7d22\u6027\u80fd\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u5229\u7528\u6b63\u4e09\u5143\u7ec4\u6216\u7b80\u5355\u4f7f\u7528\u8d1f\u4e09\u5143\u7ec4\u4f5c\u4e3a\u9519\u8bef\u4fe1\u53f7\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u8d1f\u4e09\u5143\u7ec4\u4fe1\u606f\u3002", "method": "\u63d0\u51faReCDAP\u65b9\u6cd5\uff0c\u9996\u5148\u751f\u6210\u8d1f\u4e09\u5143\u7ec4\uff08\u968f\u673a\u66ff\u6362\u5c3e\u5b9e\u4f53\uff09\uff0c\u5e76\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u6761\u4ef6\u5316\u7ed3\u5408\u6b63\u8d1f\u4e09\u5143\u7ec4\u4fe1\u606f\uff0c\u5229\u7528\u6ce8\u610f\u529b\u6c60\u5316\u663e\u5f0f\u6355\u6349\u6b63\u8d1f\u6848\u4f8b\u5dee\u5f02\u3002", "result": "\u5728\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\uff0cReCDAP\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u6761\u4ef6\u5316\u6269\u6563\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0cReCDAP\u6709\u6548\u5904\u7406\u4e86\u77e5\u8bc6\u56fe\u8c31\u4e2d\u957f\u5c3e\u5173\u7cfb\u7684\u8865\u5168\u95ee\u9898\uff0c\u4e3a\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u652f\u6301\u3002"}}
{"id": "2505.07271", "pdf": "https://arxiv.org/pdf/2505.07271", "abs": "https://arxiv.org/abs/2505.07271", "authors": ["Jiwoo Hong", "Noah Lee", "Eunki Kim", "Guijin Son", "Woojin Chung", "Aman Gupta", "Shao Tang", "James Thorne"], "title": "On the Robustness of Reward Models for Language Model Alignment", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "The Bradley-Terry (BT) model is widely practiced in reward modeling for\nreinforcement learning with human feedback (RLHF). Despite its effectiveness,\nreward models (RMs) trained with BT model loss are prone to over-optimization,\nlosing generalizability to unseen input distributions. In this paper, we study\nthe cause of over-optimization in RM training and its downstream effects on the\nRLHF procedure, accentuating the importance of distributional robustness of RMs\nin unseen data. First, we show that the excessive dispersion of hidden state\nnorms is the main source of over-optimization. Then, we propose batch-wise\nsum-to-zero regularization (BSR) to enforce zero-centered reward sum per batch,\nconstraining the rewards with extreme magnitudes. We assess the impact of BSR\nin improving robustness in RMs through four scenarios of over-optimization,\nwhere BSR consistently manifests better robustness. Subsequently, we compare\nthe plain BT model and BSR on RLHF training and empirically show that robust\nRMs better align the policy to the gold preference model. Finally, we apply BSR\nto high-quality data and models, which surpasses state-of-the-art RMs in the 8B\nscale by adding more than 5% in complex preference prediction tasks. By\nconducting RLOO training with 8B RMs, AlpacaEval 2.0 reduces generation length\nby 40% while adding a 7% increase in win rate, further highlighting that\nrobustness in RMs induces robustness in RLHF training. We release the code,\ndata, and models: https://github.com/LinkedIn-XFACT/RM-Robustness.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86Bradley-Terry\u6a21\u578b\u5728RM\u8bad\u7ec3\u4e2d\u7684\u8fc7\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u6279\u91cf\u548c\u4e3a\u96f6\u6b63\u5219\u5316\uff08BSR\uff09\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728RLHF\u8bad\u7ec3\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u867d\u7136Bradley-Terry\u6a21\u578b\u5728RLHF\u4e2d\u5e7f\u6cdb\u7528\u4e8e\u5956\u52b1\u5efa\u6a21\uff0c\u4f46\u5176\u8bad\u7ec3\u51fa\u7684\u5956\u52b1\u6a21\u578b\u6613\u5728\u672a\u89c1\u8fc7\u6570\u636e\u4e0a\u6cdb\u5316\u6027\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u8fc7\u4f18\u5316\u7684\u6210\u56e0\u53ca\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u6279\u91cf\u548c\u4e3a\u96f6\u6b63\u5219\u5316\uff08BSR\uff09\uff0c\u9650\u5236\u5956\u52b1\u503c\u7684\u6781\u7aef\u5206\u5e03\uff0c\u901a\u8fc7\u56db\u7c7b\u8fc7\u4f18\u5316\u573a\u666f\u9a8c\u8bc1\u5176\u63d0\u5347\u5956\u52b1\u6a21\u578b\u9c81\u68d2\u6027\u7684\u6548\u679c\u3002", "result": "BSR\u5728RLHF\u8bad\u7ec3\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u7b56\u7565\u4e0e\u9ec4\u91d1\u504f\u597d\u6a21\u578b\u7684\u5bf9\u9f50\u6548\u679c\uff0c8B\u89c4\u6a21\u6a21\u578b\u5728\u590d\u6742\u504f\u597d\u9884\u6d4b\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u53475%+\uff0c\u4e14\u5728AlpacaEval 2.0\u4e2d\u51cf\u5c11\u751f\u6210\u957f\u5ea640%\u7684\u540c\u65f6\u63d0\u5347\u80dc\u73877%\u3002", "conclusion": "BSR\u80fd\u6709\u6548\u63d0\u5347\u5956\u52b1\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548cRLHF\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u591a\u4e2a\u4efb\u52a1\u548c\u6a21\u578b\u89c4\u6a21\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2505.06319", "pdf": "https://arxiv.org/pdf/2505.06319", "abs": "https://arxiv.org/abs/2505.06319", "authors": ["Zijian An", "Lifeng Zhou"], "title": "Reinforcement Learning for Game-Theoretic Resource Allocation on Graphs", "categories": ["cs.LG", "cs.GT"], "comment": "12 pages, 7 figures", "summary": "Game-theoretic resource allocation on graphs (GRAG) involves two players\ncompeting over multiple steps to control nodes of interest on a graph, a\nproblem modeled as a multi-step Colonel Blotto Game (MCBG). Finding optimal\nstrategies is challenging due to the dynamic action space and structural\nconstraints imposed by the graph. To address this, we formulate the MCBG as a\nMarkov Decision Process (MDP) and apply Reinforcement Learning (RL) methods,\nspecifically Deep Q-Network (DQN) and Proximal Policy Optimization (PPO). To\nenforce graph constraints, we introduce an action-displacement adjacency matrix\nthat dynamically generates valid action sets at each step. We evaluate RL\nperformance across a variety of graph structures and initial resource\ndistributions, comparing against random, greedy, and learned RL policies.\nExperimental results show that both DQN and PPO consistently outperform\nbaseline strategies and converge to a balanced $50\\%$ win rate when competing\nagainst the learned RL policy. Particularly, on asymmetric graphs, RL agents\nsuccessfully exploit structural advantages and adapt their allocation\nstrategies, even under disadvantageous initial resource distributions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u591a\u6b65Colonel Blotto\u6e38\u620f\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u5e76\u5e94\u7528DQN\u548cPPO\u7b97\u6cd5\uff0c\u5728\u5404\u79cd\u56fe\u7ed3\u6784\u548c\u521d\u59cb\u8d44\u6e90\u5206\u5e03\u4e0b\u5747\u4f18\u4e8e\u57fa\u51c6\u7b56\u7565\u3002", "motivation": "\u89e3\u51b3\u5728\u591a\u6b65Colonel Blotto\u6e38\u620f\u4e2d\uff0c\u7531\u4e8e\u52a8\u6001\u52a8\u4f5c\u7a7a\u95f4\u548c\u56fe\u7684\u7ed3\u6784\u7ea6\u675f\u5bfc\u81f4\u7684\u5bfb\u627e\u6700\u4f18\u7b56\u7565\u7684\u6311\u6218\u3002", "method": "\u5c06\u591a\u6b65Colonel Blotto\u6e38\u620f\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\uff0c\u5e76\u5e94\u7528\u6df1\u5ea6Q\u7f51\u7edc\uff08DQN\uff09\u548c\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u7b97\u6cd5\uff0c\u5f15\u5165\u52a8\u4f5c-\u4f4d\u79fb\u90bb\u63a5\u77e9\u9635\u4ee5\u52a8\u6001\u751f\u6210\u6709\u6548\u52a8\u4f5c\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDQN\u548cPPO\u5728\u591a\u79cd\u56fe\u7ed3\u6784\u548c\u521d\u59cb\u8d44\u6e90\u5206\u5e03\u4e0b\u5747\u4f18\u4e8e\u968f\u673a\u3001\u8d2a\u5fc3\u548c\u5df2\u5b66\u4e60RL\u7b56\u7565\uff0c\u5e76\u5728\u5bf9\u6297\u5b66\u4e60\u7b56\u7565\u65f6\u8fbe\u523050%\u7684\u5747\u8861\u80dc\u7387\u3002\u5728\u975e\u5bf9\u79f0\u56fe\u4e2d\uff0cRL\u4ee3\u7406\u80fd\u6709\u6548\u5229\u7528\u7ed3\u6784\u4f18\u52bf\u5e76\u8c03\u6574\u5206\u914d\u7b56\u7565\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6b65Colonel Blotto\u6e38\u620f\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86RL\u5728\u5904\u7406\u52a8\u6001\u7ea6\u675f\u95ee\u9898\u4e0a\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.07178", "pdf": "https://arxiv.org/pdf/2505.07178", "abs": "https://arxiv.org/abs/2505.07178", "authors": ["Yuri Nakao"], "title": "Accountability of Generative AI: Exploring a Precautionary Approach for \"Artificially Created Nature\"", "categories": ["cs.AI"], "comment": null, "summary": "The rapid development of generative artificial intelligence (AI) technologies\nraises concerns about the accountability of sociotechnical systems. Current\ngenerative AI systems rely on complex mechanisms that make it difficult for\neven experts to fully trace the reasons behind the outputs. This paper first\nexamines existing research on AI transparency and accountability and argues\nthat transparency is not a sufficient condition for accountability but can\ncontribute to its improvement. We then discuss that if it is not possible to\nmake generative AI transparent, generative AI technology becomes ``artificially\ncreated nature'' in a metaphorical sense, and suggest using the precautionary\nprinciple approach to consider AI risks. Finally, we propose that a platform\nfor citizen participation is needed to address the risks of generative AI.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u751f\u6210\u5f0fAI\u7684\u900f\u660e\u5ea6\u548c\u95ee\u8d23\u5236\uff0c\u8ba4\u4e3a\u900f\u660e\u5ea6\u867d\u4e0d\u8db3\u4ee5\u4fdd\u8bc1\u95ee\u8d23\uff0c\u4f46\u80fd\u4fc3\u8fdb\u5176\u6539\u8fdb\u3002\u4f5c\u8005\u63d0\u51fa\u5c06\u751f\u6210\u5f0fAI\u89c6\u4e3a\u4eba\u5de5\u521b\u9020\u7684\u81ea\u7136\uff0c\u5efa\u8bae\u91c7\u53d6\u9884\u9632\u539f\u5219\uff0c\u5e76\u547c\u5401\u5efa\u7acb\u516c\u6c11\u53c2\u4e0e\u5e73\u53f0\u4ee5\u5e94\u5bf9\u98ce\u9669\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5176\u96be\u4ee5\u8ffd\u6eaf\u8f93\u51fa\u7684\u590d\u6742\u6027\u5f15\u53d1\u4e86\u793e\u4f1a\u6280\u672f\u7cfb\u7edf\u7684\u95ee\u8d23\u95ee\u9898\u3002\u4f5c\u8005\u65e8\u5728\u63a2\u8ba8\u900f\u660e\u5ea6\u4e0e\u95ee\u8d23\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u5e94\u5bf9\u751f\u6210\u5f0fAI\u98ce\u9669\u7684\u65b0\u601d\u8def\u3002", "method": "\u4f5c\u8005\u9996\u5148\u7efc\u8ff0\u73b0\u6709AI\u900f\u660e\u5ea6\u548c\u95ee\u8d23\u7814\u7a76\uff0c\u6307\u51fa\u900f\u660e\u5ea6\u7684\u5c40\u9650\u6027\uff1b\u5176\u6b21\u63d0\u51fa\u5c06\u751f\u6210\u5f0fAI\u89c6\u4e3a\u4eba\u5de5\u521b\u9020\u7684\u81ea\u7136\uff0c\u7c7b\u6bd4\u81ea\u7136\u73b0\u8c61\u7684\u4e0d\u900f\u660e\u6027\uff1b\u6700\u540e\u63d0\u51fa\u9884\u9632\u539f\u5219\u548c\u516c\u6c11\u53c2\u4e0e\u5e73\u53f0\u7684\u5fc5\u8981\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u900f\u660e\u5ea6\u867d\u4e0d\u80fd\u5b8c\u5168\u89e3\u51b3\u95ee\u8d23\u95ee\u9898\uff0c\u4f46\u80fd\u6539\u5584\u95ee\u8d23\u5236\uff1b\u751f\u6210\u5f0fAI\u7684\u4e0d\u900f\u660e\u6027\u53ef\u7c7b\u6bd4\u81ea\u7136\u73b0\u8c61\uff0c\u9700\u91c7\u7528\u9884\u9632\u539f\u5219\uff1b\u516c\u6c11\u53c2\u4e0e\u662f\u5e94\u5bf9\u98ce\u9669\u7684\u5173\u952e\u9014\u5f84\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u7684\u95ee\u8d23\u9700\u8d85\u8d8a\u900f\u660e\u5ea6\uff0c\u7ed3\u5408\u9884\u9632\u539f\u5219\u548c\u516c\u6c11\u53c2\u4e0e\uff0c\u4ee5\u5e94\u5bf9\u5176\u4e0d\u900f\u660e\u6027\u548c\u6f5c\u5728\u98ce\u9669\u3002"}}
{"id": "2505.07289", "pdf": "https://arxiv.org/pdf/2505.07289", "abs": "https://arxiv.org/abs/2505.07289", "authors": ["Stanislas Laborde", "Martin Cousseau", "Antoun Yaacoub", "Lionel Prevost"], "title": "Semantic Retention and Extreme Compression in LLMs: Can We Have Both?", "categories": ["cs.CL", "cs.AI", "cs.LG", "68P30 (Primary) 68T07, 68T50 (Secondary)", "I.2.6; I.5.1; I.2.7"], "comment": "Accepted for publication in the Proceedings of the 2025 International\n  Joint Conference on Neural Networks (IJCNN); this arXiv version includes an\n  appendix with 6 result tables; 10 pages, 15 figures, 7 tables", "summary": "The exponential growth in Large Language Model (LLM) deployment has\nintensified the need for efficient model compression techniques to reduce\ncomputational and memory costs. While pruning and quantization have shown\npromise, their combined potential remains largely unexplored. In this paper, we\nexamine joint compression and how strategically combining pruning and\nquantization could yield superior performance-to-compression ratios compared to\nsingle-method approaches. Recognizing the challenges in accurately assessing\nLLM performance, we address key limitations of previous evaluation frameworks\nand introduce the Semantic Retention Compression Rate (SrCr), a novel metric\nthat quantifies the trade-off between model compression and semantic\npreservation, facilitating the optimization of pruning-quantization\nconfigurations. Experiments demonstrate that our recommended combination\nachieves, on average, a 20% performance increase compared to an equivalent\nquantization-only model at the same theoretical compression rate.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u7ed3\u5408\u526a\u679d\u4e0e\u91cf\u5316\u7684\u8054\u5408\u538b\u7f29\u65b9\u6cd5\uff0c\u5f15\u5165\u65b0\u6307\u6807 SrCr \u8bc4\u4f30\u8bed\u4e49\u4fdd\u7559\u4e0e\u538b\u7f29\u7387\uff0c\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u6bd4\u5355\u4e00\u91cf\u5316\u6548\u7387\u63d0\u5347\u7ea6 20%\u3002", "motivation": "\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u9ad8\uff0c\u9700\u9ad8\u6548\u538b\u7f29\u6280\u672f\u3002\u526a\u679d\u4e0e\u91cf\u5316\u7ed3\u5408\u6f5c\u529b\u672a\u88ab\u5145\u5206\u6316\u6398\uff0c\u9700\u4f18\u5316\u4e8c\u8005\u534f\u540c\u6548\u679c\u3002", "method": "\u63d0\u51fa\u8054\u5408\u526a\u679d\u4e0e\u91cf\u5316\u7684\u538b\u7f29\u7b56\u7565\uff0c\u5e76\u8bbe\u8ba1 Semantic Retention Compression Rate (SrCr) \u6307\u6807\uff0c\u91cf\u5316\u538b\u7f29\u7387\u4e0e\u8bed\u4e49\u4fdd\u7559\u7684\u6743\u8861\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63a8\u8350\u7ec4\u5408\u5728\u76f8\u540c\u7406\u8bba\u538b\u7f29\u7387\u4e0b\uff0c\u6027\u80fd\u6bd4\u5355\u4e00\u91cf\u5316\u6a21\u578b\u5e73\u5747\u63d0\u9ad8 20%\u3002", "conclusion": "\u8054\u5408\u526a\u679d\u4e0e\u91cf\u5316\u53ef\u663e\u8457\u63d0\u5347\u538b\u7f29\u6548\u7387\uff0cSrCr \u6307\u6807\u4e3a\u4f18\u5316\u538b\u7f29\u914d\u7f6e\u63d0\u4f9b\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2505.06320", "pdf": "https://arxiv.org/pdf/2505.06320", "abs": "https://arxiv.org/abs/2505.06320", "authors": ["Jan Ko\u015bcia\u0142kowski", "Pawe\u0142 Marcinkowski"], "title": "Divide (Text) and Conquer (Sentiment): Improved Sentiment Classification by Constituent Conflict Resolution", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "8 pages, 6 figures, 4 tables, developed as a final project for the\n  Stanford Center for Professional Education XCS224U (Natural Language\n  Understanding) course", "summary": "Sentiment classification, a complex task in natural language processing,\nbecomes even more challenging when analyzing passages with multiple conflicting\ntones. Typically, longer passages exacerbate this issue, leading to decreased\nmodel performance. The aim of this paper is to introduce novel methodologies\nfor isolating conflicting sentiments and aggregating them to effectively\npredict the overall sentiment of such passages. One of the aggregation\nstrategies involves a Multi-Layer Perceptron (MLP) model which outperforms\nbaseline models across various datasets, including Amazon, Twitter, and SST\nwhile costing $\\sim$1/100 of what fine-tuning the baseline would take.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u6a21\u578b\u6709\u6548\u5206\u79bb\u548c\u805a\u5408\u591a\u8bed\u8c03\u6bb5\u843d\u4e2d\u7684\u60c5\u611f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u60c5\u611f\u5206\u7c7b\u6027\u80fd\uff0c\u4e14\u6210\u672c\u8fdc\u4f4e\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u591a\u8bed\u8c03\u6bb5\u843d\u60c5\u611f\u5206\u7c7b\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u957f\u6bb5\u843d\u4e2d\u51b2\u7a81\u60c5\u611f\u7684\u51c6\u786e\u9884\u6d4b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u6a21\u578b\u8fdb\u884c\u60c5\u611f\u9694\u79bb\u548c\u805a\u5408\uff0c\u5bf9\u6bd4\u57fa\u7ebf\u6a21\u578b\u5728Amazon\u3001Twitter\u548cSST\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "MLP\u6a21\u578b\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u4e14\u6210\u672c\u4ec5\u4e3a\u5fae\u8c03\u57fa\u7ebf\u76841/100\u3002", "conclusion": "\u63d0\u51fa\u7684MLP\u65b9\u6cd5\u9ad8\u6548\u4e14\u7ecf\u6d4e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\u3002"}}
{"id": "2505.07215", "pdf": "https://arxiv.org/pdf/2505.07215", "abs": "https://arxiv.org/abs/2505.07215", "authors": ["Vivek Verma", "David Huang", "William Chen", "Dan Klein", "Nicholas Tomlin"], "title": "Measuring General Intelligence with Generated Games", "categories": ["cs.AI"], "comment": null, "summary": "We present gg-bench, a collection of game environments designed to evaluate\ngeneral reasoning capabilities in language models. Unlike most static\nbenchmarks, gg-bench is a data generating process where new evaluation\ninstances can be generated at will. In particular, gg-bench is synthetically\ngenerated by (1) using a large language model (LLM) to generate natural\nlanguage descriptions of novel games, (2) using the LLM to implement each game\nin code as a Gym environment, and (3) training reinforcement learning (RL)\nagents via self-play on the generated games. We evaluate language models by\ntheir winrate against these RL agents by prompting models with the game\ndescription, current board state, and a list of valid moves, after which models\noutput the moves they wish to take. gg-bench is challenging: state-of-the-art\nLLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench\nusing in-context learning, while reasoning models such as o1, o3-mini and\nDeepSeek-R1 achieve average winrates of 31-36%. We release the generated games,\ndata generation process, and evaluation code in order to support future\nmodeling work and expansion of our benchmark.", "AI": {"tldr": "gg-bench \u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u901a\u7528\u63a8\u7406\u80fd\u529b\u7684\u6e38\u620f\u73af\u5883\u96c6\u5408\uff0c\u901a\u8fc7\u751f\u6210\u65b0\u6e38\u620f\u5b9e\u4f8b\u548c\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u8fdb\u884c\u6d4b\u8bd5\u3002", "motivation": "\u4f20\u7edf\u7684\u9759\u6001\u8bc4\u6d4b\u57fa\u51c6\u96be\u4ee5\u5168\u9762\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u56e0\u6b64\u4f5c\u8005\u8bbe\u8ba1\u4e86\u52a8\u6001\u751f\u6210\u7684\u6e38\u620f\u73af\u5883\u6765\u66f4\u7075\u6d3b\u5730\u6d4b\u8bd5\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u6e38\u620f\u63cf\u8ff0\u5e76\u5b9e\u73b0\u4e3a Gym \u73af\u5883\uff0c\u518d\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u901a\u8fc7\u81ea\u6211\u5bf9\u6297\u8fdb\u884c\u8bc4\u6d4b\u3002", "result": "\u5f53\u524d\u6700\u5148\u8fdb\u7684 LLM\uff08\u5982 GPT-4o \u548c Claude 3.7 Sonnet\uff09\u5728 gg-bench \u4e0a\u7684\u80dc\u7387\u4ec5\u4e3a 7-9%\uff0c\u800c\u67d0\u4e9b\u4e13\u7528\u63a8\u7406\u6a21\u578b\uff08\u5982 o1\u3001o3-mini \u548c DeepSeek-R1\uff09\u7684\u80dc\u7387\u8fbe\u5230 31-36%\u3002", "conclusion": "gg-bench \u63d0\u4f9b\u4e86\u4e00\u79cd\u52a8\u6001\u8bc4\u6d4b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\uff0c\u672a\u6765\u53ef\u6269\u5c55\u5e76\u652f\u6301\u66f4\u591a\u6a21\u578b\u7684\u7814\u7a76\u3002"}}
{"id": "2505.07293", "pdf": "https://arxiv.org/pdf/2505.07293", "abs": "https://arxiv.org/abs/2505.07293", "authors": ["Kai Hua", "Steven Wu", "Ge Zhang", "Ke Shen"], "title": "AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong Pretraining Data Selection", "categories": ["cs.CL"], "comment": "28 pages, 19 figures", "summary": "Recently, there has been growing interest in collecting reasoning-intensive\npretraining data to improve LLMs' complex reasoning ability. Prior approaches\ntypically rely on supervised classifiers to identify such data, which requires\nlabeling by humans or LLMs, often introducing domain-specific biases. Due to\nthe attention heads being crucial to in-context reasoning, we propose\nAttentionInfluence, a simple yet effective, training-free method without\nsupervision signal. Our approach enables a small pretrained language model to\nact as a strong data selector through a simple attention head masking\noperation. Specifically, we identify retrieval heads and compute the loss\ndifference when masking these heads. We apply AttentionInfluence to a\n1.3B-parameter dense model to conduct data selection on the SmolLM corpus of\n241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B\ntokens to pretrain a 7B-parameter dense model using 1T training tokens and WSD\nlearning rate scheduling. Our experimental results demonstrate substantial\nimprovements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive\nand reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and\nHumanEval). This demonstrates an effective weak-to-strong scaling property,\nwith small models improving the final performance of larger models-offering a\npromising and scalable path for reasoning-centric data selection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u548c\u76d1\u7763\u7684\u65b9\u6cd5AttentionInfluence\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5934\u63a9\u7801\u64cd\u4f5c\u7b5b\u9009\u63a8\u7406\u5bc6\u96c6\u578b\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6709\u76d1\u7763\u5206\u7c7b\u5668\u7b5b\u9009\u6570\u636e\uff0c\u5bb9\u6613\u5f15\u5165\u9886\u57df\u504f\u89c1\u3002\u57fa\u4e8e\u6ce8\u610f\u529b\u5934\u5728\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u8bbe\u8ba1\u4e86\u65e0\u9700\u76d1\u7763\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u548c\u63a9\u7801\u68c0\u7d22\u5934\uff0c\u8ba1\u7b97\u635f\u5931\u5dee\u5f02\u6765\u7b5b\u9009\u6570\u636e\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e1.3B\u53c2\u6570\u6a21\u578b\uff0c\u4ece241B token\u4e2d\u9009\u51fa73B token\u7528\u4e8e\u8bad\u7ec37B\u53c2\u6570\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u77e5\u8bc6\u5bc6\u96c6\u548c\u63a8\u7406\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982MMLU\u3001GSM8K\uff09\u4e0a\u5b9e\u73b0\u4e861.4%\u52303.5%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5c55\u793a\u4e86\u5c0f\u6a21\u578b\u63d0\u5347\u5927\u6a21\u578b\u6027\u80fd\u7684\u6709\u6548\u6027\u3002", "conclusion": "AttentionInfluence\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u63a8\u7406\u4e2d\u5fc3\u6570\u636e\u7b5b\u9009\u8def\u5f84\uff0c\u901a\u8fc7\u5c0f\u6a21\u578b\u4f18\u5316\u5927\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2505.06321", "pdf": "https://arxiv.org/pdf/2505.06321", "abs": "https://arxiv.org/abs/2505.06321", "authors": ["Hang Gao", "Chenhao Zhang", "Tie Wang", "Junsuo Zhao", "Fengge Wu", "Changwen Zheng", "Huaping Liu"], "title": "Learn to Think: Bootstrapping LLM Reasoning Capability Through Graph Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by IJCAI 2025", "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains. However, they still face significant challenges, including high\ncomputational costs for training and limitations in solving complex reasoning\nproblems. Although existing methods have extended the reasoning capabilities of\nLLMs through structured paradigms, these approaches often rely on task-specific\nprompts and predefined reasoning processes, which constrain their flexibility\nand generalizability. To address these limitations, we propose a novel\nframework that leverages graph learning to enable more flexible and adaptive\nreasoning capabilities for LLMs. Specifically, this approach models the\nreasoning process of a problem as a graph and employs LLM-based graph learning\nto guide the adaptive generation of each reasoning step. To further enhance the\nadaptability of the model, we introduce a Graph Neural Network (GNN) module to\nperform representation learning on the generated reasoning process, enabling\nreal-time adjustments to both the model and the prompt. Experimental results\ndemonstrate that this method significantly improves reasoning performance\nacross multiple tasks without requiring additional training or task-specific\nprompt design. Code can be found in https://github.com/zch65458525/L2T.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u56fe\u5b66\u4e60\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u63a8\u7406\u80fd\u529b\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u63a8\u7406\u8fc7\u7a0b\u4e3a\u56fe\u5e76\u7ed3\u5408GNN\u6a21\u5757\u5b9e\u73b0\u5b9e\u65f6\u8c03\u6574\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5f53\u524dLLMs\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u548c\u9884\u5b9a\u4e49\u6d41\u7a0b\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u56fe\u5b66\u4e60\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5c06\u95ee\u9898\u63a8\u7406\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u56fe\uff0c\u901a\u8fc7LLM-based\u56fe\u5b66\u4e60\u81ea\u9002\u5e94\u751f\u6210\u63a8\u7406\u6b65\u9aa4\uff0c\u5e76\u5f15\u5165GNN\u6a21\u5757\u8fdb\u884c\u8868\u793a\u5b66\u4e60\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u8c03\u6574\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u8bbe\u8ba1\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u7684\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u56fe\u5b66\u4e60\u4e0eLLMs\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u3001\u81ea\u9002\u5e94\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3aLLMs\u7684\u590d\u6742\u4efb\u52a1\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.07299", "pdf": "https://arxiv.org/pdf/2505.07299", "abs": "https://arxiv.org/abs/2505.07299", "authors": ["Andr\u00e9 Artelt", "Stelios G. Vrachimis", "Demetrios G. Eliades", "Ulrike Kuhl", "Barbara Hammer", "Marios M. Polycarpou"], "title": "Interpretable Event Diagnosis in Water Distribution Networks", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "The increasing penetration of information and communication technologies in\nthe design, monitoring, and control of water systems enables the use of\nalgorithms for detecting and identifying unanticipated events (such as leakages\nor water contamination) using sensor measurements. However, data-driven\nmethodologies do not always give accurate results and are often not trusted by\noperators, who may prefer to use their engineering judgment and experience to\ndeal with such events.\n  In this work, we propose a framework for interpretable event diagnosis -- an\napproach that assists the operators in associating the results of algorithmic\nevent diagnosis methodologies with their own intuition and experience. This is\nachieved by providing contrasting (i.e., counterfactual) explanations of the\nresults provided by fault diagnosis algorithms; their aim is to improve the\nunderstanding of the algorithm's inner workings by the operators, thus enabling\nthem to take a more informed decision by combining the results with their\npersonal experiences. Specifically, we propose counterfactual event\nfingerprints, a representation of the difference between the current event\ndiagnosis and the closest alternative explanation, which can be presented in a\ngraphical way. The proposed methodology is applied and evaluated on a realistic\nuse case using the L-Town benchmark.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u4e8b\u4ef6\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u4f9b\u53cd\u4e8b\u5b9e\u89e3\u91ca\u5e2e\u52a9\u64cd\u4f5c\u5458\u7406\u89e3\u7b97\u6cd5\u7ed3\u679c\uff0c\u5e76\u7ed3\u5408\u81ea\u8eab\u7ecf\u9a8c\u505a\u51fa\u66f4\u660e\u667a\u7684\u51b3\u7b56\u3002", "motivation": "\u5c3d\u7ba1\u4fe1\u606f\u4e0e\u901a\u4fe1\u6280\u672f\u5728\u6c34\u7cfb\u7edf\u76d1\u6d4b\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u7ed3\u679c\u5e76\u4e0d\u603b\u662f\u51c6\u786e\uff0c\u4e14\u64cd\u4f5c\u5458\u66f4\u4f9d\u8d56\u81ea\u8eab\u7ecf\u9a8c\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\u5f25\u5408\u7b97\u6cd5\u4e0e\u64cd\u4f5c\u5458\u76f4\u89c9\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u4e8b\u4ef6\u6307\u7eb9\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u5f62\u5316\u5c55\u793a\u5f53\u524d\u4e8b\u4ef6\u8bca\u65ad\u4e0e\u6700\u8fd1\u66ff\u4ee3\u89e3\u91ca\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u589e\u5f3a\u64cd\u4f5c\u5458\u5bf9\u7b97\u6cd5\u5185\u90e8\u903b\u8f91\u7684\u7406\u89e3\u3002", "result": "\u8be5\u65b9\u6cd5\u5728L-Town\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u5e94\u7528\u548c\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u53cd\u4e8b\u5b9e\u89e3\u91ca\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u64cd\u4f5c\u5458\u5bf9\u7b97\u6cd5\u8bca\u65ad\u7ed3\u679c\u7684\u4fe1\u4efb\u548c\u51b3\u7b56\u8d28\u91cf\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u63a8\u5e7f\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2505.07313", "pdf": "https://arxiv.org/pdf/2505.07313", "abs": "https://arxiv.org/abs/2505.07313", "authors": ["Baixuan Xu", "Chunyang Li", "Weiqi Wang", "Wei Fan", "Tianshi Zheng", "Haochen Shi", "Tao Fan", "Yangqiu Song", "Qiang Yang"], "title": "Towards Multi-Agent Reasoning Systems for Collaborative Expertise Delegation: An Exploratory Design Study", "categories": ["cs.CL", "cs.AI"], "comment": "18 pages", "summary": "Designing effective collaboration structure for multi-agent LLM systems to\nenhance collective reasoning is crucial yet remains under-explored. In this\npaper, we systematically investigate how collaborative reasoning performance is\naffected by three key design dimensions: (1) Expertise-Domain Alignment, (2)\nCollaboration Paradigm (structured workflow vs. diversity-driven integration),\nand (3) System Scale. Our findings reveal that expertise alignment benefits are\nhighly domain-contingent, proving most effective for contextual reasoning\ntasks. Furthermore, collaboration focused on integrating diverse knowledge\nconsistently outperforms rigid task decomposition. Finally, we empirically\nexplore the impact of scaling the multi-agent system with expertise\nspecialization and study the computational trade off, highlighting the need for\nmore efficient communication protocol design. This work provides concrete\nguidelines for configuring specialized multi-agent system and identifies\ncritical architectural trade-offs and bottlenecks for scalable multi-agent\nreasoning. The code will be made available upon acceptance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u63a2\u8ba8\u4e86\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u4e2d\u534f\u4f5c\u7ed3\u6784\u8bbe\u8ba1\u5bf9\u96c6\u4f53\u63a8\u7406\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5173\u6ce8\u4e13\u5bb6\u9886\u57df\u5bf9\u9f50\u3001\u534f\u4f5c\u8303\u5f0f\uff08\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u4e0e\u591a\u6837\u6027\u9a71\u52a8\u6574\u5408\uff09\u548c\u7cfb\u7edf\u89c4\u6a21\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\u3002", "motivation": "\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u7684\u6709\u6548\u534f\u4f5c\u7ed3\u6784\u8bbe\u8ba1\u5bf9\u589e\u5f3a\u96c6\u4f53\u63a8\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7814\u7a76\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u914d\u7f6e\u4e13\u4e1a\u5316\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u7814\u7a76\u4e09\u4e2a\u8bbe\u8ba1\u7ef4\u5ea6\u5bf9\u534f\u4f5c\u63a8\u7406\u6027\u80fd\u7684\u5f71\u54cd\uff1a\u4e13\u5bb6\u9886\u57df\u5bf9\u9f50\u3001\u534f\u4f5c\u8303\u5f0f\uff08\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41 vs. \u591a\u6837\u6027\u9a71\u52a8\u6574\u5408\uff09\u548c\u7cfb\u7edf\u89c4\u6a21\u3002", "result": "\u4e13\u5bb6\u9886\u57df\u5bf9\u9f50\u7684\u6548\u76ca\u9ad8\u5ea6\u4f9d\u8d56\u4efb\u52a1\u9886\u57df\uff1b\u591a\u6837\u6027\u9a71\u52a8\u7684\u534f\u4f5c\u8303\u5f0f\u4f18\u4e8e\u50f5\u5316\u7684\u4efb\u52a1\u5206\u89e3\uff1b\u7cfb\u7edf\u89c4\u6a21\u6269\u5c55\u5e26\u6765\u8ba1\u7b97\u6548\u7387\u4e0e\u4e13\u4e1a\u5316\u7684\u6743\u8861\u3002", "conclusion": "\u7814\u7a76\u4e3a\u914d\u7f6e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5177\u4f53\u6307\u5bfc\uff0c\u5e76\u6307\u51fa\u4e86\u6269\u5c55\u6027\u591a\u667a\u80fd\u4f53\u63a8\u7406\u4e2d\u7684\u5173\u952e\u67b6\u6784\u6743\u8861\u548c\u74f6\u9888\uff0c\u9700\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u901a\u4fe1\u534f\u8bae\u3002"}}
{"id": "2505.06325", "pdf": "https://arxiv.org/pdf/2505.06325", "abs": "https://arxiv.org/abs/2505.06325", "authors": ["Daniel Geissler", "Lars Krupp", "Vishal Banwari", "David Habusch", "Bo Zhou", "Paul Lukowicz", "Jakob Karolus"], "title": "Human in the Latent Loop (HILL): Interactively Guiding Model Training Through Human Intuition", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Latent space representations are critical for understanding and improving the\nbehavior of machine learning models, yet they often remain obscure and\nintricate. Understanding and exploring the latent space has the potential to\ncontribute valuable human intuition and expertise about respective domains. In\nthis work, we present HILL, an interactive framework allowing users to\nincorporate human intuition into the model training by interactively reshaping\nlatent space representations. The modifications are infused into the model\ntraining loop via a novel approach inspired by knowledge distillation, treating\nthe user's modifications as a teacher to guide the model in reshaping its\nintrinsic latent representation. The process allows the model to converge more\neffectively and overcome inefficiencies, as well as provide beneficial insights\nto the user. We evaluated HILL in a user study tasking participants to train an\noptimal model, closely observing the employed strategies. The results\ndemonstrated that human-guided latent space modifications enhance model\nperformance while maintaining generalization, yet also revealing the risks of\nincluding user biases. Our work introduces a novel human-AI interaction\nparadigm that infuses human intuition into model training and critically\nexamines the impact of human intervention on training strategies and potential\nbiases.", "AI": {"tldr": "HILL\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4eba\u7c7b\u76f4\u89c9\u878d\u5165\u6a21\u578b\u8bad\u7ec3\uff0c\u8c03\u6574\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u9700\u6ce8\u610f\u6f5c\u5728\u7684\u7528\u6237\u504f\u89c1\u3002", "motivation": "\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\u5bf9\u7406\u89e3\u548c\u6539\u8fdb\u673a\u5668\u5b66\u4e60\u6a21\u578b\u884c\u4e3a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u901a\u5e38\u590d\u6742\u96be\u61c2\u3002\u7ed3\u5408\u4eba\u7c7b\u76f4\u89c9\u53ef\u4f18\u5316\u6a21\u578b\u8bad\u7ec3\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u84b8\u998f\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5c06\u7528\u6237\u8c03\u6574\u4f5c\u4e3a\u6559\u5e08\u4fe1\u53f7\u6307\u5bfc\u6a21\u578b\u91cd\u5851\u6f5c\u5728\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u8bc4\u4f30\u6548\u679c\u3002", "result": "\u4eba\u7c7b\u6307\u5bfc\u7684\u6f5c\u5728\u7a7a\u95f4\u4fee\u6539\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u5e76\u4fdd\u6301\u6cdb\u5316\u6027\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u5f15\u5165\u7528\u6237\u504f\u89c1\u7684\u98ce\u9669\u3002", "conclusion": "HILL\u5f00\u521b\u4e86\u4eba\u673a\u4ea4\u4e92\u65b0\u8303\u5f0f\uff0c\u5c06\u4eba\u7c7b\u76f4\u89c9\u878d\u5165\u8bad\u7ec3\uff0c\u540c\u65f6\u9700\u8b66\u60d5\u5e72\u9884\u5e26\u6765\u7684\u504f\u89c1\u5f71\u54cd\u3002"}}
{"id": "2505.07315", "pdf": "https://arxiv.org/pdf/2505.07315", "abs": "https://arxiv.org/abs/2505.07315", "authors": ["Zexiao Wang", "Yankai Wang", "Xiaoqiang Liao", "Xinguo Ming", "Weiming Shen"], "title": "FedIFL: A federated cross-domain diagnostic framework for motor-driven systems with inconsistent fault modes", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Due to the scarcity of industrial data, individual equipment users,\nparticularly start-ups, struggle to independently train a comprehensive fault\ndiagnosis model; federated learning enables collaborative training while\nensuring data privacy, making it an ideal solution. However, the diversity of\nworking conditions leads to variations in fault modes, resulting in\ninconsistent label spaces across different clients. In federated diagnostic\nscenarios, label space inconsistency leads to local models focus on\nclient-specific fault modes and causes local models from different clients to\nmap different failure modes to similar feature representations, which weakens\nthe aggregated global model's generalization. To tackle this issue, this\narticle proposed a federated cross-domain diagnostic framework termed Federated\nInvariant Features Learning (FedIFL). In intra-client training, prototype\ncontrastive learning mitigates intra-client domain shifts, subsequently,\nfeature generating ensures local models can access distributions of other\nclients in a privacy-friendly manner. Besides, in cross-client training, a\nfeature disentanglement mechanism is introduced to mitigate cross-client domain\nshifts, specifically, an instance-level federated instance consistency loss is\ndesigned to ensure the instance-level consistency of invariant features between\ndifferent clients, furthermore, a federated instance personalization loss and\nan orthogonal loss are constructed to distinguish specific features that from\nthe invariant features. Eventually, the aggregated model achieves promising\ngeneralization among global label spaces, enabling accurate fault diagnosis for\ntarget clients' Motor Driven Systems (MDSs) with inconsistent label spaces.\nExperiments on real-world MDSs validate the effectiveness and superiority of\nFedIFL in federated cross-domain diagnosis with inconsistent fault modes.", "AI": {"tldr": "\u63d0\u51faFedIFL\u6846\u67b6\uff0c\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u6807\u7b7e\u7a7a\u95f4\u4e0d\u4e00\u81f4\u5bfc\u81f4\u7684\u6a21\u578b\u6cdb\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u7279\u5f81\u89e3\u8026\u548c\u5bf9\u6bd4\u5b66\u4e60\u63d0\u5347\u5168\u5c40\u6a21\u578b\u7684\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u5de5\u4e1a\u6570\u636e\u7a00\u7f3a\u4e14\u6807\u7b7e\u7a7a\u95f4\u4e0d\u4e00\u81f4\uff0c\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u96be\u4ee5\u4fdd\u8bc1\u5168\u5c40\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u89e3\u51b3\u8de8\u5ba2\u6237\u7aef\u7279\u5f81\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u539f\u578b\u5bf9\u6bd4\u5b66\u4e60\u548c\u7279\u5f81\u751f\u6210\u89e3\u51b3\u5ba2\u6237\u7aef\u5185\u90e8\u57df\u504f\u79fb\uff0c\u901a\u8fc7\u7279\u5f81\u89e3\u8026\u673a\u5236\u548c\u4e00\u81f4\u6027\u635f\u5931\u4f18\u5316\u8de8\u5ba2\u6237\u7aef\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728\u771f\u5b9eMDS\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0cFedIFL\u663e\u8457\u63d0\u5347\u8de8\u57df\u6545\u969c\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "FedIFL\u6709\u6548\u89e3\u51b3\u4e86\u6807\u7b7e\u7a7a\u95f4\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4e3a\u8054\u90a6\u5b66\u4e60\u5728\u5de5\u4e1a\u6545\u969c\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.07345", "pdf": "https://arxiv.org/pdf/2505.07345", "abs": "https://arxiv.org/abs/2505.07345", "authors": ["Ohjoon Kwon", "Changsu Lee", "Jihye Back", "Lim Sun Suk", "Inho Kang", "Donghyeon Jeon"], "title": "QUPID: Quantified Understanding for Enhanced Performance, Insights, and Decisions in Korean Search Engines", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large language models (LLMs) have been widely used for relevance assessment\nin information retrieval. However, our study demonstrates that combining two\ndistinct small language models (SLMs) with different architectures can\noutperform LLMs in this task. Our approach -- QUPID -- integrates a generative\nSLM with an embedding-based SLM, achieving higher relevance judgment accuracy\nwhile reducing computational costs compared to state-of-the-art LLM solutions.\nThis computational efficiency makes QUPID highly scalable for real-world search\nsystems processing millions of queries daily. In experiments across diverse\ndocument types, our method demonstrated consistent performance improvements\n(Cohen's Kappa of 0.646 versus 0.387 for leading LLMs) while offering 60x\nfaster inference times. Furthermore, when integrated into production search\npipelines, QUPID improved nDCG@5 scores by 1.9%. These findings underscore how\narchitectural diversity in model combinations can significantly enhance both\nsearch relevance and operational efficiency in information retrieval systems.", "AI": {"tldr": "QUPID\u7ed3\u5408\u4e24\u79cd\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08\u751f\u6210\u5f0f\u548c\u5d4c\u5165\u5f0f\uff09\uff0c\u5728\u4fe1\u606f\u68c0\u7d22\u4e2d\u6bd4\u5927\u578b\u8bed\u8a00\u6a21\u578b\u66f4\u9ad8\u6548\u4e14\u51c6\u786e\uff0c\u63a8\u7406\u901f\u5ea6\u5feb60\u500d\uff0cnDCG@5\u63d0\u53471.9%\u3002", "motivation": "\u7814\u7a76\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u7ed3\u5408\u4e0d\u540c\u67b6\u6784\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\uff0c\u5728\u4fdd\u8bc1\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u53d6\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4fe1\u606f\u68c0\u7d22\u4e2d\u7684\u4e3b\u5bfc\u5730\u4f4d\u3002", "method": "\u63d0\u51faQUPID\u65b9\u6cd5\uff0c\u6574\u5408\u751f\u6210\u5f0fSLM\u548c\u5d4c\u5165\u5f0fSLM\uff0c\u5229\u7528\u67b6\u6784\u591a\u6837\u6027\u63d0\u5347\u76f8\u5173\u6027\u5224\u65ad\u51c6\u786e\u7387\uff0c\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5b9e\u9a8c\u663e\u793aQUPID\u7684Cohen's Kappa\u8fbe0.646\uff08LLMs\u4e3a0.387\uff09\uff0c\u63a8\u7406\u901f\u5ea6\u5feb60\u500d\uff1b\u751f\u4ea7\u73af\u5883\u4e2dnDCG@5\u63d0\u53471.9%\u3002", "conclusion": "\u901a\u8fc7SLM\u7ec4\u5408\u7684\u67b6\u6784\u591a\u6837\u6027\uff0cQUPID\u5728\u68c0\u7d22\u76f8\u5173\u6027\u548c\u8fd0\u884c\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8eLLMs\uff0c\u9002\u5408\u5927\u89c4\u6a21\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2505.06330", "pdf": "https://arxiv.org/pdf/2505.06330", "abs": "https://arxiv.org/abs/2505.06330", "authors": ["Junyu Xue", "Xudong Wang", "Xiaoling He", "Shicheng Liu", "Yi Wang", "Guoming Tang"], "title": "Prompting Large Language Models for Training-Free Non-Intrusive Load Monitoring", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "Non-intrusive Load Monitoring (NILM) aims to disaggregate aggregate household\nelectricity consumption into individual appliance usage, enabling more\neffective energy management. While deep learning has advanced NILM, it remains\nlimited by its dependence on labeled data, restricted generalization, and lack\nof interpretability. In this paper, we introduce the first prompt-based NILM\nframework that leverages Large Language Models (LLMs) with in-context learning.\nWe design and evaluate prompt strategies that integrate appliance features,\ntimestamps and contextual information, as well as representative time-series\nexamples, using the REDD dataset. With optimized prompts, LLMs achieve\ncompetitive state detection accuracy, reaching an average F1-score of 0.676 on\nunseen households, and demonstrate robust generalization without the need for\nfine-tuning. LLMs also enhance interpretability by providing clear,\nhuman-readable explanations for their predictions. Our results show that LLMs\ncan reduce data requirements, improve adaptability, and provide transparent\nenergy disaggregation in NILM applications.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u63d0\u793a\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6846\u67b6\u7528\u4e8e\u975e\u4fb5\u5165\u5f0f\u8d1f\u8377\u76d1\u6d4b\uff08NILM\uff09\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u6574\u5408\u8bbe\u5907\u7279\u5f81\u3001\u65f6\u95f4\u6233\u548c\u4ee3\u8868\u6027\u65f6\u95f4\u5e8f\u5217\u793a\u4f8b\uff0c\u5728REDD\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u7ade\u4e89\u6027\u7684\u72b6\u6001\u68c0\u6d4b\u51c6\u786e\u5ea6\uff08\u5e73\u5747F1-score 0.676\uff09\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edfNILM\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u4e14\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\uff0c\u7814\u7a76\u65e8\u5728\u5229\u7528LLM\u51cf\u5c11\u6570\u636e\u9700\u6c42\u3001\u63d0\u5347\u9002\u5e94\u6027\u548c\u89e3\u91ca\u6027\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8eLLM\u7684\u63d0\u793a\u6846\u67b6\uff0c\u7ed3\u5408\u8bbe\u5907\u7279\u5f81\u3001\u65f6\u95f4\u6233\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e76\u91c7\u7528REDD\u6570\u636e\u96c6\u4f18\u5316\u63d0\u793a\u7b56\u7565\u3002", "result": "LLM\u5728\u672a\u89c1\u8fc7\u5bb6\u5ead\u4e2d\u5e73\u5747F1-score\u8fbe0.676\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u6cdb\u5316\uff0c\u5e76\u63d0\u4f9b\u4eba\u7c7b\u53ef\u8bfb\u7684\u9884\u6d4b\u89e3\u91ca\u3002", "conclusion": "LLM\u901a\u8fc7\u63d0\u793a\u7b56\u7565\u663e\u8457\u964d\u4f4e\u4e86NILM\u5bf9\u6570\u636e\u7684\u9700\u6c42\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u89e3\u91ca\u6027\uff0c\u4e3a\u80fd\u6e90\u5206\u89e3\u63d0\u4f9b\u4e86\u900f\u660e\u65b9\u6848\u3002"}}
{"id": "2505.07374", "pdf": "https://arxiv.org/pdf/2505.07374", "abs": "https://arxiv.org/abs/2505.07374", "authors": ["Zhiye Xie", "Enmei Tu", "Xianping Fu", "Guoliang Yuan", "Yi Han"], "title": "AIS Data-Driven Maritime Monitoring Based on Transformer: A Comprehensive Review", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "With the increasing demands for safety, efficiency, and sustainability in\nglobal shipping, Automatic Identification System (AIS) data plays an\nincreasingly important role in maritime monitoring. AIS data contains\nspatial-temporal variation patterns of vessels that hold significant research\nvalue in the marine domain. However, due to its massive scale, the full\npotential of AIS data has long remained untapped. With its powerful sequence\nmodeling capabilities, particularly its ability to capture long-range\ndependencies and complex temporal dynamics, the Transformer model has emerged\nas an effective tool for processing AIS data. Therefore, this paper reviews the\nresearch on Transformer-based AIS data-driven maritime monitoring, providing a\ncomprehensive overview of the current applications of Transformer models in the\nmarine field. The focus is on Transformer-based trajectory prediction methods,\nbehavior detection, and prediction techniques. Additionally, this paper\ncollects and organizes publicly available AIS datasets from the reviewed\npapers, performing data filtering, cleaning, and statistical analysis. The\nstatistical results reveal the operational characteristics of different vessel\ntypes, providing data support for further research on maritime monitoring\ntasks. Finally, we offer valuable suggestions for future research, identifying\ntwo promising research directions. Datasets are available at\nhttps://github.com/eyesofworld/Maritime-Monitoring.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8eTransformer\u6a21\u578b\u7684AIS\u6570\u636e\u5728\u6d77\u6d0b\u76d1\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u63a2\u8ba8\u4e86\u8f68\u8ff9\u9884\u6d4b\u3001\u884c\u4e3a\u68c0\u6d4b\u4e0e\u9884\u6d4b\u6280\u672f\uff0c\u5e76\u6574\u7406\u4e86\u516c\u5f00\u7684AIS\u6570\u636e\u96c6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6570\u636e\u652f\u6301\u4e0e\u65b9\u5411\u5efa\u8bae\u3002", "motivation": "\u968f\u7740\u5168\u7403\u822a\u8fd0\u5bf9\u5b89\u5168\u3001\u6548\u7387\u548c\u53ef\u6301\u7eed\u6027\u7684\u9700\u6c42\u589e\u52a0\uff0cAIS\u6570\u636e\u7684\u6f5c\u529b\u56e0\u89c4\u6a21\u5e9e\u5927\u672a\u88ab\u5145\u5206\u6316\u6398\u3002Transformer\u6a21\u578b\u56e0\u5176\u5f3a\u5927\u7684\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\uff08\u5982\u6355\u6349\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u590d\u6742\u65f6\u5e8f\u52a8\u6001\uff09\u6210\u4e3a\u5904\u7406AIS\u6570\u636e\u7684\u6709\u6548\u5de5\u5177\u3002", "method": "\u56de\u987e\u4e86Transformer\u5728\u6d77\u6d0b\u9886\u57df\u7684\u5e94\u7528\uff0c\u805a\u7126\u8f68\u8ff9\u9884\u6d4b\u3001\u884c\u4e3a\u68c0\u6d4b\u4e0e\u9884\u6d4b\u6280\u672f\uff1b\u6574\u7406\u4e86\u516c\u5f00AIS\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u4e86\u6570\u636e\u8fc7\u6ee4\u3001\u6e05\u6d17\u4e0e\u7edf\u8ba1\u5206\u6790\uff0c\u63ed\u793a\u4e0d\u540c\u8239\u578b\u7684\u64cd\u4f5c\u7279\u5f81\u3002", "result": "\u7edf\u8ba1\u7ed3\u679c\u5c55\u793a\u4e86\u5404\u7c7b\u8239\u53ea\u7684\u8fd0\u884c\u7279\u70b9\uff0c\u4e3a\u6d77\u6d0b\u76d1\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\uff08\u5982GitHub\u94fe\u63a5\uff09\uff0c\u5e76\u63d0\u51fa\u4e24\u4e2a\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "Transformer\u5728AIS\u6570\u636e\u9a71\u52a8\u7684\u6d77\u6d0b\u76d1\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6570\u636e\u6574\u7406\u4e0e\u5206\u6790\u4e3a\u540e\u7eed\u7814\u7a76\u5960\u5b9a\u57fa\u7840\uff1b\u672a\u6765\u53ef\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u6216\u8de8\u9886\u57df\u5e94\u7528\u3002"}}
{"id": "2505.07409", "pdf": "https://arxiv.org/pdf/2505.07409", "abs": "https://arxiv.org/abs/2505.07409", "authors": ["Tim Wittenborg", "Constantin Sebastian Tremel", "Markus Stocker", "S\u00f6ren Auer"], "title": "Computational Fact-Checking of Online Discourse: Scoring scientific accuracy in climate change related news articles", "categories": ["cs.CL"], "comment": "4 pages, 4 figures, submitted to ACM Web Conference 2025", "summary": "Democratic societies need reliable information. Misinformation in popular\nmedia such as news articles or videos threatens to impair civic discourse.\nCitizens are, unfortunately, not equipped to verify this content flood consumed\ndaily at increasing rates. This work aims to semi-automatically quantify\nscientific accuracy of online media. By semantifying media of unknown veracity,\ntheir statements can be compared against equally processed trusted sources. We\nimplemented a workflow using LLM-based statement extraction and knowledge graph\nanalysis. Our neurosymbolic system was able to evidently streamline\nstate-of-the-art veracity quantification. Evaluated via expert interviews and a\nuser survey, the tool provides a beneficial veracity indication. This\nindicator, however, is unable to annotate public media at the required\ngranularity and scale. Further work towards a FAIR (Findable, Accessible,\nInteroperable, Reusable) ground truth and complementary metrics are required to\nscientifically support civic discourse.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u4f7f\u7528LLM\u548c\u77e5\u8bc6\u56fe\u8c31\u5206\u6790\u6765\u91cf\u5316\u5728\u7ebf\u5a92\u4f53\u7684\u79d1\u5b66\u51c6\u786e\u6027\uff0c\u4f46\u5de5\u5177\u5728\u7c92\u5ea6\u4e0e\u89c4\u6a21\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u6c11\u4e3b\u793e\u4f1a\u9700\u8981\u53ef\u9760\u4fe1\u606f\uff0c\u4f46\u516c\u6c11\u7f3a\u4e4f\u9a8c\u8bc1\u6d77\u91cf\u5185\u5bb9\u7684\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u8861\u91cf\u5728\u7ebf\u5a92\u4f53\u7684\u79d1\u5b66\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u58f0\u660e\u63d0\u53d6\u548c\u77e5\u8bc6\u56fe\u8c31\u5206\u6790\u7684\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\uff0c\u5bf9\u6bd4\u672a\u77e5\u6765\u6e90\u4e0e\u53ef\u4fe1\u6765\u6e90\u7684\u8bed\u4e49\u5316\u5185\u5bb9\u3002", "result": "\u7cfb\u7edf\u80fd\u663e\u8457\u63d0\u5347\u771f\u5b9e\u6027\u91cf\u5316\u6548\u7387\uff0c\u4f46\u65e0\u6cd5\u6ee1\u8db3\u516c\u5171\u5a92\u4f53\u6240\u9700\u7684\u7cbe\u7ec6\u5316\u548c\u89c4\u6a21\u5316\u6807\u6ce8\u9700\u6c42\u3002", "conclusion": "\u9700\u8fdb\u4e00\u6b65\u5f00\u53d1FAIR\u6807\u51c6\u7684\u57fa\u7840\u6570\u636e\u548c\u8865\u5145\u6307\u6807\uff0c\u4ee5\u79d1\u5b66\u652f\u6301\u516c\u5171\u8ba8\u8bba\u3002"}}
{"id": "2505.06331", "pdf": "https://arxiv.org/pdf/2505.06331", "abs": "https://arxiv.org/abs/2505.06331", "authors": ["Feilong Jiang", "Xiaonan Hou", "Jianqiao Ye", "Min Xia"], "title": "Mask-PINNs: Regulating Feature Distributions in Physics-Informed Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Physics-Informed Neural Networks (PINNs) are a class of deep learning models\ndesigned to solve partial differential equations by incorporating physical laws\ndirectly into the loss function. However, the internal covariate shift, which\nhas been largely overlooked, hinders the effective utilization of neural\nnetwork capacity in PINNs. To this end, we propose Mask-PINNs, a novel\narchitecture designed to address this issue in PINNs. Unlike traditional\nnormalization methods such as BatchNorm or LayerNorm, we introduce a learnable,\nnonlinear mask function that constrains the feature distributions without\nviolating underlying physics. The experimental results show that the proposed\nmethod significantly improves feature distribution stability, accuracy, and\nrobustness across various activation functions and PDE benchmarks. Furthermore,\nit enables the stable and efficient training of wider networks a capability\nthat has been largely overlooked in PINNs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMask-PINNs\u7684\u65b0\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u4e2d\u5185\u90e8\u534f\u53d8\u91cf\u504f\u79fb\u7684\u95ee\u9898\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u5bb9\u91cf\u3002", "motivation": "PINNs\u901a\u8fc7\u5c06\u7269\u7406\u5b9a\u5f8b\u76f4\u63a5\u878d\u5165\u635f\u5931\u51fd\u6570\u6765\u89e3\u51b3\u504f\u5fae\u5206\u65b9\u7a0b\uff0c\u4f46\u5185\u90e8\u534f\u53d8\u91cf\u504f\u79fb\u95ee\u9898\u957f\u671f\u88ab\u5ffd\u89c6\uff0c\u963b\u788d\u4e86\u795e\u7ecf\u7f51\u7edc\u5bb9\u91cf\u7684\u6709\u6548\u5229\u7528\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5b66\u4e60\u7684\u975e\u7ebf\u6027\u63a9\u7801\u51fd\u6570\uff0c\u7528\u4e8e\u5728\u4e0d\u8fdd\u53cd\u7269\u7406\u5b9a\u5f8b\u7684\u524d\u63d0\u4e0b\u7ea6\u675f\u7279\u5f81\u5206\u5e03\uff0c\u53d6\u4ee3\u4e86\u4f20\u7edf\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u5982BatchNorm\u6216LayerNorm\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u7279\u5f81\u5206\u5e03\u7684\u7a33\u5b9a\u6027\u3001\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u8fd8\u5b9e\u73b0\u4e86\u66f4\u5bbd\u7f51\u7edc\u7684\u7a33\u5b9a\u9ad8\u6548\u8bad\u7ec3\u3002", "conclusion": "Mask-PINNs\u6709\u6548\u89e3\u51b3\u4e86PINNs\u4e2d\u7684\u5185\u90e8\u534f\u53d8\u91cf\u504f\u79fb\u95ee\u9898\uff0c\u4e3a\u66f4\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2505.07453", "pdf": "https://arxiv.org/pdf/2505.07453", "abs": "https://arxiv.org/abs/2505.07453", "authors": ["Cornelius Wolff", "Madelon Hulsebos"], "title": "How well do LLMs reason over tabular data, really?", "categories": ["cs.AI"], "comment": "10 pages, 4 figures", "summary": "Large Language Models (LLMs) excel in natural language tasks, but less is\nknown about their reasoning capabilities over tabular data. Prior analyses\ndevise evaluation strategies that poorly reflect an LLM's realistic performance\non tabular queries. Moreover, we have a limited understanding of the robustness\nof LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can\ngeneral-purpose LLMs reason over tabular data, really?, and focus on two\nquestions 1) are tabular reasoning capabilities of general-purpose LLMs robust\nto real-world characteristics of tabular inputs, and 2) how can we\nrealistically evaluate an LLM's performance on analytical tabular queries?\nBuilding on a recent tabular reasoning benchmark, we first surface shortcomings\nof its multiple-choice prompt evaluation strategy, as well as commonly used\nfree-form text metrics such as SacreBleu and BERT-score. We show that an\nLLM-as-a-judge procedure yields more reliable performance insights and unveil a\nsignificant deficit in tabular reasoning performance of LLMs. We then extend\nthe tabular inputs reflecting three common characteristics in practice: 1)\nmissing values, 2) duplicate entities, and 3) structural variations.\nExperiments show that the tabular reasoning capabilities of general-purpose\nLLMs suffer from these variations, stressing the importance of improving their\nrobustness for realistic tabular inputs.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8868\u683c\u6570\u636e\u4e0a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u6307\u51fa\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51faLLM-as-a-judge\u65b9\u6cd5\u66f4\u53ef\u9760\uff1b\u5b9e\u9a8c\u8868\u660eLLMs\u5bf9\u8868\u683c\u8f93\u5165\u7684\u5e38\u89c1\u53d8\u5316\uff08\u5982\u7f3a\u5931\u503c\u3001\u91cd\u590d\u5b9e\u4f53\u7b49\uff09\u8868\u73b0\u8106\u5f31\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u8868\u683c\u6570\u636e\u4e0a\u7684\u771f\u5b9e\u63a8\u7406\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5176\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u8868\u683c\u8f93\u5165\u53d8\u5316\uff08\u5982\u7f3a\u5931\u503c\u3001\u91cd\u590d\u5b9e\u4f53\u7b49\uff09\u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u53ca\u5982\u4f55\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u73b0\u6709\u8868\u683c\u63a8\u7406\u57fa\u51c6\uff0c\u5206\u6790\u5176\u591a\u9009\u63d0\u793a\u8bc4\u4f30\u7b56\u7565\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51faLLM-as-a-judge\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u6269\u5c55\u8868\u683c\u8f93\u5165\u4ee5\u6a21\u62df\u73b0\u5b9e\u4e2d\u7684\u4e09\u79cd\u5e38\u89c1\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLLMs\u5728\u8868\u683c\u63a8\u7406\u80fd\u529b\u4e0a\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u4e14\u5bf9\u8f93\u5165\u53d8\u5316\uff08\u5982\u7f3a\u5931\u503c\u3001\u91cd\u590d\u5b9e\u4f53\u7b49\uff09\u8868\u73b0\u8106\u5f31\u3002", "conclusion": "LLMs\u5728\u8868\u683c\u6570\u636e\u63a8\u7406\u4e0a\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u4ee5\u9002\u5e94\u73b0\u5b9e\u573a\u666f\u3002"}}
{"id": "2505.07416", "pdf": "https://arxiv.org/pdf/2505.07416", "abs": "https://arxiv.org/abs/2505.07416", "authors": ["Truc Mai-Thanh Nguyen", "Dat Minh Nguyen", "Son T. Luu", "Kiet Van Nguyen"], "title": "ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness Prediction via Human-AI Collaborative Annotation", "categories": ["cs.CL"], "comment": "Accepted at NLDB 2025", "summary": "Multimodal Review Helpfulness Prediction (MRHP) is an essential task in\nrecommender systems, particularly in E-commerce platforms. Determining the\nhelpfulness of user-generated reviews enhances user experience and improves\nconsumer decision-making. However, existing datasets focus predominantly on\nEnglish and Indonesian, resulting in a lack of linguistic diversity, especially\nfor low-resource languages such as Vietnamese. In this paper, we introduce\nViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), a large-scale\nbenchmark dataset for MRHP task in Vietnamese. This dataset covers four\ndomains, including 2K products with 46K reviews. Meanwhile, a large-scale\ndataset requires considerable time and cost. To optimize the annotation\nprocess, we leverage AI to assist annotators in constructing the ViMRHP\ndataset. With AI assistance, annotation time is reduced (90 to 120 seconds per\ntask down to 20 to 40 seconds per task) while maintaining data quality and\nlowering overall costs by approximately 65%. However, AI-generated annotations\nstill have limitations in complex annotation tasks, which we further examine\nthrough a detailed performance analysis. In our experiment on ViMRHP, we\nevaluate baseline models on human-verified and AI-generated annotations to\nassess their quality differences. The ViMRHP dataset is publicly available at\nhttps://github.com/trng28/ViMRHP", "AI": {"tldr": "ViMRHP\u662f\u8d8a\u5357\u8bed\u591a\u6a21\u6001\u8bc4\u8bba\u6709\u7528\u6027\u9884\u6d4b\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8986\u76d64\u4e2a\u9886\u57df\uff0c\u542b2K\u4ea7\u54c1\u548c46K\u8bc4\u8bba\u3002\u501f\u52a9AI\u8f85\u52a9\u6807\u6ce8\uff0c\u65f6\u95f4\u51cf\u5c1165%\u4e14\u8d28\u91cf\u6709\u4fdd\u969c\uff0c\u4f46\u590d\u6742\u4efb\u52a1\u4ecd\u6709\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u4e3b\u8981\u9488\u5bf9\u82f1\u8bed\u548c\u5370\u5c3c\u8bed\uff0c\u8d8a\u5357\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u7f3a\u4e4f\u591a\u6837\u6027\u3002ViMRHP\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\uff0c\u65e8\u5728\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u7528\u6237\u4f53\u9a8c\u548c\u51b3\u7b56\u652f\u6301\u3002", "method": "\u901a\u8fc7AI\u8f85\u52a9\u4eba\u5de5\u6807\u6ce8\u6784\u5efaViMRHP\u6570\u636e\u96c6\uff0c\u5bf9\u6bd4\u4eba\u5de5\u4e0eAI\u6807\u6ce8\u7684\u8d28\u91cf\u5dee\u5f02\uff0c\u5e76\u8bc4\u4f30\u57fa\u7ebf\u6a21\u578b\u8868\u73b0\u3002", "result": "\u6807\u6ce8\u65f6\u95f4\u4ece90-120\u79d2/\u4efb\u52a1\u964d\u81f320-40\u79d2/\u4efb\u52a1\uff0c\u6574\u4f53\u6210\u672c\u964d\u4f4e65%\u3002AI\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u4f46\u6570\u636e\u8d28\u91cf\u4ecd\u53ef\u4fdd\u969c\u3002", "conclusion": "ViMRHP\u4e3a\u8d8a\u5357\u8bedMRHP\u4efb\u52a1\u63d0\u4f9b\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0cAI\u8f85\u52a9\u6807\u6ce8\u9ad8\u6548\u4e14\u7ecf\u6d4e\uff0c\u4f46\u590d\u6742\u573a\u666f\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2505.06333", "pdf": "https://arxiv.org/pdf/2505.06333", "abs": "https://arxiv.org/abs/2505.06333", "authors": ["Chathurangi Shyalika", "Renjith Prasad", "Fadi El Kalach", "Revathy Venkataramanan", "Ramtin Zand", "Ramy Harik", "Amit Sheth"], "title": "NSF-MAP: Neurosymbolic Multimodal Fusion for Robust and Interpretable Anomaly Prediction in Assembly Pipelines", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, 7 figures, 2 tables, IJCAI 2025 (International Joint\n  Conferences on Artificial Intelligence) Special Track on AI4Tech: AI Enabling\n  Critical Technologies", "summary": "In modern assembly pipelines, identifying anomalies is crucial in ensuring\nproduct quality and operational efficiency. Conventional single-modality\nmethods fail to capture the intricate relationships required for precise\nanomaly prediction in complex predictive environments with abundant data and\nmultiple modalities. This paper proposes a neurosymbolic AI and fusion-based\napproach for multimodal anomaly prediction in assembly pipelines. We introduce\na time series and image-based fusion model that leverages decision-level fusion\ntechniques. Our research builds upon three primary novel approaches in\nmultimodal learning: time series and image-based decision-level fusion\nmodeling, transfer learning for fusion, and knowledge-infused learning. We\nevaluate the novel method using our derived and publicly available multimodal\ndataset and conduct comprehensive ablation studies to assess the impact of our\npreprocessing techniques and fusion model compared to traditional baselines.\nThe results demonstrate that a neurosymbolic AI-based fusion approach that uses\ntransfer learning can effectively harness the complementary strengths of time\nseries and image data, offering a robust and interpretable approach for anomaly\nprediction in assembly pipelines with enhanced performance. \\noindent The\ndatasets, codes to reproduce the results, supplementary materials, and demo are\navailable at https://github.com/ChathurangiShyalika/NSF-MAP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7b26\u53f7AI\u548c\u591a\u6a21\u6001\u878d\u5408\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u88c5\u914d\u6d41\u6c34\u7ebf\u4e2d\u7684\u5f02\u5e38\u9884\u6d4b\uff0c\u7ed3\u5408\u65f6\u95f4\u5e8f\u5217\u548c\u56fe\u50cf\u6570\u636e\uff0c\u901a\u8fc7\u51b3\u7b56\u7ea7\u878d\u5408\u3001\u8fc1\u79fb\u5b66\u4e60\u548c\u77e5\u8bc6\u6ce8\u5165\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u88c5\u914d\u6d41\u6c34\u7ebf\u4e2d\uff0c\u4f20\u7edf\u5355\u6a21\u6001\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u590d\u6742\u6570\u636e\u548c\u591a\u6a21\u6001\u73af\u5883\u4e2d\u7684\u5f02\u5e38\u5173\u7cfb\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u786e\u7684\u591a\u6a21\u6001\u5f02\u5e38\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u65f6\u95f4\u5e8f\u5217\u4e0e\u56fe\u50cf\u7684\u51b3\u7b56\u7ea7\u878d\u5408\u5efa\u6a21\u3001\u8fc1\u79fb\u5b66\u4e60\u548c\u77e5\u8bc6\u6ce8\u5165\u5b66\u4e60\uff0c\u6784\u5efa\u4e86\u795e\u7ecf\u7b26\u53f7AI\u548c\u591a\u6a21\u6001\u878d\u5408\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u7ed3\u5408\u65f6\u95f4\u5e8f\u5217\u548c\u56fe\u50cf\u6570\u636e\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u5f02\u5e38\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u795e\u7ecf\u7b26\u53f7AI\u4e0e\u591a\u6a21\u6001\u878d\u5408\u7684\u65b9\u6cd5\u4e3a\u88c5\u914d\u6d41\u6c34\u7ebf\u5f02\u5e38\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u9a8c\u6570\u636e\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2505.07460", "pdf": "https://arxiv.org/pdf/2505.07460", "abs": "https://arxiv.org/abs/2505.07460", "authors": ["Yi Chen", "JiaHao Zhao", "HaoHao Han"], "title": "A Survey on Collaborative Mechanisms Between Large and Small Language Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) deliver powerful AI capabilities but face\ndeployment challenges due to high resource costs and latency, whereas Small\nLanguage Models (SLMs) offer efficiency and deployability at the cost of\nreduced performance. Collaboration between LLMs and SLMs emerges as a crucial\nparadigm to synergistically balance these trade-offs, enabling advanced AI\napplications, especially on resource-constrained edge devices. This survey\nprovides a comprehensive overview of LLM-SLM collaboration, detailing various\ninteraction mechanisms (pipeline, routing, auxiliary, distillation, fusion),\nkey enabling technologies, and diverse application scenarios driven by\non-device needs like low latency, privacy, personalization, and offline\noperation. While highlighting the significant potential for creating more\nefficient, adaptable, and accessible AI, we also discuss persistent challenges\nincluding system overhead, inter-model consistency, robust task allocation,\nevaluation complexity, and security/privacy concerns. Future directions point\ntowards more intelligent adaptive frameworks, deeper model fusion, and\nexpansion into multimodal and embodied AI, positioning LLM-SLM collaboration as\na key driver for the next generation of practical and ubiquitous artificial\nintelligence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u534f\u4f5c\u7684\u91cd\u8981\u6027\uff0c\u603b\u7ed3\u4e86\u4ea4\u4e92\u673a\u5236\u3001\u5173\u952e\u6280\u672f\u53ca\u5e94\u7528\u573a\u666f\uff0c\u5e76\u6307\u51fa\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u89e3\u51b3LLM\u8d44\u6e90\u6d88\u8017\u9ad8\u3001\u5ef6\u8fdf\u5927\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5229\u7528SLM\u7684\u9ad8\u6548\u6027\uff0c\u63a8\u52a8\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684AI\u5e94\u7528\u3002", "method": "\u7efc\u8ff0\u4e86LLM-SLM\u534f\u4f5c\u7684\u4e94\u79cd\u673a\u5236\uff08\u6d41\u6c34\u7ebf\u3001\u8def\u7531\u3001\u8f85\u52a9\u3001\u84b8\u998f\u3001\u878d\u5408\uff09\u53ca\u5173\u952e\u6280\u672f\u3002", "result": "\u534f\u4f5c\u6846\u67b6\u5728\u5ef6\u8fdf\u3001\u9690\u79c1\u7b49\u9700\u6c42\u4e0b\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u514b\u670d\u7cfb\u7edf\u5f00\u9500\u3001\u8bc4\u4f30\u590d\u6742\u5ea6\u7b49\u6311\u6218\u3002", "conclusion": "LLM-SLM\u534f\u4f5c\u662f\u672a\u6765\u5b9e\u7528AI\u7684\u5173\u952e\u65b9\u5411\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u81ea\u9002\u5e94\u6846\u67b6\u548c\u591a\u6a21\u6001\u6269\u5c55\u3002"}}
{"id": "2505.07430", "pdf": "https://arxiv.org/pdf/2505.07430", "abs": "https://arxiv.org/abs/2505.07430", "authors": ["Mostafa Mohaimen Akand Faisal", "Rabeya Amin Jhuma"], "title": "Comparative sentiment analysis of public perception: Monkeypox vs. COVID-19 behavioral insights", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The emergence of global health crises, such as COVID-19 and Monkeypox (mpox),\nhas underscored the importance of understanding public sentiment to inform\neffective public health strategies. This study conducts a comparative sentiment\nanalysis of public perceptions surrounding COVID-19 and mpox by leveraging\nextensive datasets of 147,475 and 106,638 tweets, respectively. Advanced\nmachine learning models, including Logistic Regression, Naive Bayes, RoBERTa,\nDistilRoBERTa and XLNet, were applied to perform sentiment classification, with\nresults indicating key trends in public emotion and discourse. The analysis\nhighlights significant differences in public sentiment driven by disease\ncharacteristics, media representation, and pandemic fatigue. Through the lens\nof sentiment polarity and thematic trends, this study offers valuable insights\ninto tailoring public health messaging, mitigating misinformation, and\nfostering trust during concurrent health crises. The findings contribute to\nadvancing sentiment analysis applications in public health informatics, setting\nthe groundwork for enhanced real-time monitoring and multilingual analysis in\nfuture research.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u6bd4\u8f83COVID-19\u548cMpox\u7684\u516c\u4f17\u60c5\u7eea\u5206\u6790\uff0c\u63ed\u793a\u4e86\u516c\u4f17\u60c5\u7eea\u5dee\u5f02\u53ca\u5176\u5bf9\u516c\u5171\u536b\u751f\u7b56\u7565\u7684\u542f\u793a\u3002", "motivation": "\u5168\u7403\u5065\u5eb7\u5371\u673a\u5982COVID-19\u548cMpox\u7684\u51fa\u73b0\u51f8\u663e\u4e86\u7406\u89e3\u516c\u4f17\u60c5\u7eea\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u4fbf\u5236\u5b9a\u6709\u6548\u7684\u516c\u5171\u536b\u751f\u7b56\u7565\u3002", "method": "\u7814\u7a76\u5229\u7528Logistic\u56de\u5f52\u3001\u6734\u7d20\u8d1d\u53f6\u65af\u3001RoBERTa\u3001DistilRoBERTa\u548cXLNet\u7b49\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5bf9147,475\u6761COVID-19\u63a8\u6587\u548c106,638\u6761Mpox\u63a8\u6587\u8fdb\u884c\u4e86\u60c5\u611f\u5206\u7c7b\u3002", "result": "\u5206\u6790\u53d1\u73b0\u516c\u4f17\u60c5\u7eea\u56e0\u75be\u75c5\u7279\u5f81\u3001\u5a92\u4f53\u8868\u73b0\u548c\u75ab\u60c5\u75b2\u52b3\u7b49\u56e0\u7d20\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86\u516c\u5171\u536b\u751f\u4fe1\u606f\u4f20\u9012\u7684\u6539\u8fdb\u5efa\u8bae\u3002", "conclusion": "\u7814\u7a76\u4e3a\u516c\u5171\u536b\u751f\u4fe1\u606f\u5b66\u7684\u60c5\u7eea\u5206\u6790\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u4e3a\u672a\u6765\u5b9e\u65f6\u76d1\u63a7\u548c\u591a\u8bed\u8a00\u5206\u6790\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.06335", "pdf": "https://arxiv.org/pdf/2505.06335", "abs": "https://arxiv.org/abs/2505.06335", "authors": ["Jinsheng Yuan", "Yuhang Hao", "Weisi Guo", "Yun Wu", "Chongyan Gu"], "title": "Remote Rowhammer Attack using Adversarial Observations on Federated Learning Clients", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Federated Learning (FL) has the potential for simultaneous global learning\namongst a large number of parallel agents, enabling emerging AI such as LLMs to\nbe trained across demographically diverse data. Central to this being efficient\nis the ability for FL to perform sparse gradient updates and remote direct\nmemory access at the central server. Most of the research in FL security\nfocuses on protecting data privacy at the edge client or in the communication\nchannels between the client and server. Client-facing attacks on the server are\nless well investigated as the assumption is that a large collective of clients\noffer resilience.\n  Here, we show that by attacking certain clients that lead to a high frequency\nrepetitive memory update in the server, we can remote initiate a rowhammer\nattack on the server memory. For the first time, we do not need backdoor access\nto the server, and a reinforcement learning (RL) attacker can learn how to\nmaximize server repetitive memory updates by manipulating the client's sensor\nobservation. The consequence of the remote rowhammer attack is that we are able\nto achieve bit flips, which can corrupt the server memory. We demonstrate the\nfeasibility of our attack using a large-scale FL automatic speech recognition\n(ASR) systems with sparse updates, our adversarial attacking agent can achieve\naround 70\\% repeated update rate (RUR) in the targeted server model,\neffectively inducing bit flips on server DRAM. The security implications are\nthat can cause disruptions to learning or may inadvertently cause elevated\nprivilege. This paves the way for further research on practical mitigation\nstrategies in FL and hardware design.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5ba2\u6237\u7aef\u653b\u51fb\u670d\u52a1\u5668\u5185\u5b58\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u64cd\u7eb5\u5ba2\u6237\u7aef\u89c2\u6d4b\uff0c\u89e6\u53d1\u670d\u52a1\u5668\u9ad8\u9891\u91cd\u590d\u5185\u5b58\u66f4\u65b0\uff0c\u5b9e\u73b0\u8fdc\u7a0bRowhammer\u653b\u51fb\uff0c\u5bfc\u81f4\u670d\u52a1\u5668\u5185\u5b58\u4f4d\u7ffb\u8f6c\uff0c\u7834\u574f\u5b66\u4e60\u8fc7\u7a0b\u6216\u63d0\u5347\u6743\u9650\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u670d\u52a1\u5668\u7684\u5b89\u5168\u9690\u60a3\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u670d\u52a1\u5668\u5185\u5b58\u7684\u8fdc\u7a0b\u653b\u51fb\u53ef\u80fd\u6027\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u5ba2\u6237\u7aef\u9690\u79c1\u4fdd\u62a4\u800c\u5ffd\u7565\u670d\u52a1\u5668\u5b89\u5168\u7684\u7a7a\u767d\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u653b\u51fb\u4ee3\u7406\uff0c\u4f7f\u5176\u80fd\u591f\u8bc6\u522b\u5e76\u64cd\u63a7\u7279\u5b9a\u5ba2\u6237\u7aef\uff0c\u89e6\u53d1\u670d\u52a1\u5668\u9ad8\u9891\u5185\u5b58\u66f4\u65b0\uff0c\u4ece\u800c\u5b9e\u65bd\u8fdc\u7a0bRowhammer\u653b\u51fb\u3002\u5b9e\u9a8c\u57fa\u4e8e\u5927\u89c4\u6a21FL\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\uff0c\u8bc4\u4f30\u653b\u51fb\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u653b\u51fb\u4ee3\u7406\u80fd\u5b9e\u73b0\u7ea670%\u7684\u91cd\u590d\u66f4\u65b0\u7387\uff08RUR\uff09\uff0c\u6210\u529f\u5728\u670d\u52a1\u5668DRAM\u4e0a\u8bf1\u53d1\u4f4d\u7ffb\u8f6c\uff0c\u9a8c\u8bc1\u4e86\u653b\u51fb\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u8fd9\u79cd\u653b\u51fb\u53ef\u80fd\u7834\u574f\u5b66\u4e60\u8fc7\u7a0b\u6216\u5bfc\u81f4\u6743\u9650\u63d0\u5347\uff0c\u547c\u5401\u672a\u6765\u7814\u7a76\u5173\u6ce8\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5b9e\u7528\u9632\u5fa1\u7b56\u7565\u53ca\u786c\u4ef6\u8bbe\u8ba1\u6539\u8fdb\u3002"}}
{"id": "2505.07473", "pdf": "https://arxiv.org/pdf/2505.07473", "abs": "https://arxiv.org/abs/2505.07473", "authors": ["Kai Xu", "YiWei Mao", "XinYi Guan", "ZiLong Feng"], "title": "Web-Bench: A LLM Code Benchmark Based on Web Standards and Frameworks", "categories": ["cs.AI"], "comment": "28 pages, 15 figures", "summary": "The application of large language models (LLMs) in the field of coding is\nevolving rapidly: from code assistants, to autonomous coding agents, and then\nto generating complete projects through natural language. Early LLM code\nbenchmarks primarily focused on code generation accuracy, but these benchmarks\nhave gradually become saturated. Benchmark saturation weakens their guiding\nrole for LLMs. For example, HumanEval Pass@1 has reached 99.4% and MBPP 94.2%.\nAmong various attempts to address benchmark saturation, approaches based on\nsoftware engineering have stood out, but the saturation of existing software\nengineering benchmarks is rapidly increasing. To address this, we propose a new\nbenchmark, Web-Bench, which contains 50 projects, each consisting of 20 tasks\nwith sequential dependencies. The tasks implement project features in sequence,\nsimulating real-world human development workflows. When designing Web-Bench, we\naim to cover the foundational elements of Web development: Web Standards and\nWeb Frameworks. Given the scale and complexity of these projects, which were\ndesigned by engineers with 5 to 10 years of experience, each presents a\nsignificant challenge. On average, a single project takes 4 to 8 hours for a\nsenior engineer to complete. On our given benchmark agent (Web-Agent), SOTA\n(Claude 3.7 Sonnet) achieves only 25.1% Pass@1, significantly lower (better)\nthan SWE-Bench's Verified (65.4%) and Full (33.8%) scores. Finally, we discuss\nthat in any development field, Standards and Frameworks represent foundational\nknowledge and efficiency tools, respectively, and LLMs require optimization\ntailored to them.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5Web-Bench\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u9971\u548c\u95ee\u9898\u3002Web-Bench\u5305\u542b50\u4e2a\u9879\u76ee\uff0c\u6a21\u62df\u771f\u5b9e\u5f00\u53d1\u6d41\u7a0b\uff0c\u6311\u6218\u6027\u5f3a\u3002\u5f53\u524d\u6700\u4f18\u6a21\u578bClaude 3.7 Sonnet\u5728Web-Bench\u4e0a\u7684Pass@1\u4ec5\u4e3a25.1%\uff0c\u663e\u8457\u4f4e\u4e8e\u5176\u4ed6\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u3002", "motivation": "\u65e9\u671f\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982HumanEval\u3001MBPP\uff09\u5df2\u63a5\u8fd1\u9971\u548c\uff0c\u5931\u53bb\u4e86\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6307\u5bfc\u4f5c\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u57fa\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\u3002\u4f46\u73b0\u6709\u8f6f\u4ef6\u5de5\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u7684\u9971\u548c\u901f\u5ea6\u4e5f\u5728\u52a0\u5feb\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5177\u6311\u6218\u6027\u548c\u771f\u5b9e\u6027\u7684\u6d4b\u8bd5\u73af\u5883\u3002", "method": "\u8bbe\u8ba1Web-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b50\u4e2a\u9879\u76ee\uff0c\u6bcf\u4e2a\u9879\u76ee\u753120\u4e2a\u5177\u6709\u987a\u5e8f\u4f9d\u8d56\u7684\u4efb\u52a1\u7ec4\u6210\uff0c\u6a21\u62df\u771f\u5b9e\u7684\u4eba\u7c7b\u5f00\u53d1\u6d41\u7a0b\u3002\u9879\u76ee\u7531\u7ecf\u9a8c\u4e30\u5bcc\u7684\u5de5\u7a0b\u5e08\u8bbe\u8ba1\uff0c\u6db5\u76d6Web\u5f00\u53d1\u7684\u57fa\u7840\u5143\u7d20\uff08Web\u6807\u51c6\u548c\u6846\u67b6\uff09\uff0c\u6bcf\u4e2a\u9879\u76ee\u8017\u65f64-8\u5c0f\u65f6\u3002\u901a\u8fc7\u5728Web-Agent\u4e0a\u6d4b\u8bd5SOTA\u6a21\u578b\uff08\u5982Claude 3.7 Sonnet\uff09\u6765\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u73b0\u6709\u6700\u4f18\u6a21\u578bClaude 3.7 Sonnet\u5728Web-Bench\u4e0a\u7684Pass@1\u4ec5\u4e3a25.1%\uff0c\u663e\u8457\u4f4e\u4e8eSWE-Bench\u7684Verified\uff0865.4%\uff09\u548cFull\uff0833.8%\uff09\u5206\u6570\uff0c\u8868\u660eWeb-Bench\u7684\u6311\u6218\u6027\u66f4\u9ad8\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\uff0c\u5728\u4efb\u4f55\u5f00\u53d1\u9886\u57df\u4e2d\uff0c\u6807\u51c6\u548c\u6846\u67b6\u5206\u522b\u4ee3\u8868\u57fa\u7840\u77e5\u8bc6\u548c\u6548\u7387\u5de5\u5177\uff0c\u800cLLMs\u9700\u8981\u9488\u5bf9\u5b83\u4eec\u8fdb\u884c\u4f18\u5316\u3002Web-Bench\u7684\u63a8\u51fa\u4e3a\u8bc4\u4f30LLMs\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u65b0\u7684\u5e73\u53f0\u3002"}}
{"id": "2505.07440", "pdf": "https://arxiv.org/pdf/2505.07440", "abs": "https://arxiv.org/abs/2505.07440", "authors": ["Rituraj Singh", "Sachin Pawar", "Girish Palshikar"], "title": "Matching Tasks with Industry Groups for Augmenting Commonsense Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "Commonsense knowledge bases (KB) are a source of specialized knowledge that\nis widely used to improve machine learning applications. However, even for a\nlarge KB such as ConceptNet, capturing explicit knowledge from each industry\ndomain is challenging. For example, only a few samples of general {\\em tasks}\nperformed by various industries are available in ConceptNet. Here, a task is a\nwell-defined knowledge-based volitional action to achieve a particular goal. In\nthis paper, we aim to fill this gap and present a weakly-supervised framework\nto augment commonsense KB with tasks carried out by various industry groups\n(IG). We attempt to {\\em match} each task with one or more suitable IGs by\ntraining a neural model to learn task-IG affinity and apply clustering to\nselect the top-k tasks per IG. We extract a total of 2339 triples of the form\n$\\langle IG, is~capable~of, task \\rangle$ from two publicly available news\ndatasets for 24 IGs with the precision of 0.86. This validates the reliability\nof the extracted task-IG pairs that can be directly added to existing KBs.", "AI": {"tldr": "\u63d0\u51fa\u5f31\u76d1\u7763\u6846\u67b6\u589e\u5f3a\u5e38\u8bc6\u77e5\u8bc6\u5e93,\u4ee5\u586b\u8865\u884c\u4e1a\u4efb\u52a1\u4e0e\u884c\u4e1a\u7fa4\u7ec4\u95f4\u7684\u5173\u7cfb\u7a7a\u767d", "motivation": "\u73b0\u6709\u5e38\u8bc6\u77e5\u8bc6\u5e93(\u5982ConceptNet)\u7f3a\u4e4f\u7279\u5b9a\u884c\u4e1a\u4efb\u52a1\u7684\u4e13\u4e1a\u77e5\u8bc6,\u9700\u8865\u5145\u884c\u4e1a\u7fa4\u7ec4\u4e0e\u4efb\u52a1\u95f4\u7684\u5173\u7cfb", "method": "\u8bad\u7ec3\u795e\u7ecf\u6a21\u578b\u5b66\u4e60\u4efb\u52a1\u4e0e\u884c\u4e1a\u7fa4\u7ec4\u4e4b\u95f4\u7684\u5173\u8054,\u5e76\u901a\u8fc7\u805a\u7c7b\u4e3a\u6bcf\u4e2a\u884c\u4e1a\u7fa4\u7ec4\u9009\u62e9top-k\u4efb\u52a1", "result": "\u4ece\u65b0\u95fb\u6570\u636e\u96c6\u4e2d\u63d0\u53d62339\u4e2a\u4e09\u5143\u7ec4(\u884c\u4e1a\u7fa4\u7ec4,is capable of,\u4efb\u52a1),\u7cbe\u5ea6\u8fbe0.86", "conclusion": "\u63d0\u53d6\u7684\u4efb\u52a1\u4e0e\u884c\u4e1a\u7fa4\u7ec4\u5173\u7cfb\u53ef\u9760,\u53ef\u76f4\u63a5\u8865\u5145\u81f3\u73b0\u6709\u77e5\u8bc6\u5e93"}}
{"id": "2505.06351", "pdf": "https://arxiv.org/pdf/2505.06351", "abs": "https://arxiv.org/abs/2505.06351", "authors": ["Willem Diepeveen", "Jon Schwenk", "Andrea Bertozzi"], "title": "Latent Diffeomorphic Dynamic Mode Decomposition", "categories": ["cs.LG", "math.DS"], "comment": null, "summary": "We present Latent Diffeomorphic Dynamic Mode Decomposition (LDDMD), a new\ndata reduction approach for the analysis of non-linear systems that combines\nthe interpretability of Dynamic Mode Decomposition (DMD) with the predictive\npower of Recurrent Neural Networks (RNNs). Notably, LDDMD maintains simplicity,\nwhich enhances interpretability, while effectively modeling and learning\ncomplex non-linear systems with memory, enabling accurate predictions. This is\nexemplified by its successful application in streamflow prediction.", "AI": {"tldr": "LDDMD\u7ed3\u5408\u4e86DMD\u7684\u53ef\u89e3\u91ca\u6027\u548cRNN\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u7528\u4e8e\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u6570\u636e\u964d\u7ef4\u5206\u6790\uff0c\u5e76\u5728\u6c34\u6d41\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4e3a\u4e86\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\uff0c\u63d0\u5347\u5bf9\u590d\u6742\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u5efa\u6a21\u548c\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u52a8\u6001\u6a21\u5f0f\u5206\u89e3\uff08DMD\uff09\u548c\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\uff0c\u63d0\u51faLatent Diffeomorphic Dynamic Mode Decomposition\uff08LDDMD\uff09\u65b9\u6cd5\u3002", "result": "LDDMD\u6210\u529f\u5e94\u7528\u4e8e\u6c34\u6d41\u9884\u6d4b\uff0c\u8868\u73b0\u51fa\u51c6\u786e\u7684\u9884\u6d4b\u80fd\u529b\u3002", "conclusion": "LDDMD\u5728\u4fdd\u6301\u7b80\u5355\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\uff0c\u6709\u6548\u5efa\u6a21\u5e76\u5b66\u4e60\u5177\u6709\u8bb0\u5fc6\u6027\u7684\u590d\u6742\u975e\u7ebf\u6027\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.07509", "pdf": "https://arxiv.org/pdf/2505.07509", "abs": "https://arxiv.org/abs/2505.07509", "authors": ["Feng Ding", "Tingting Wang", "Yupeng Gao", "Shuo Yu", "Jing Ren", "Feng Xia"], "title": "HALO: Half Life-Based Outdated Fact Filtering in Temporal Knowledge Graphs", "categories": ["cs.AI"], "comment": null, "summary": "Outdated facts in temporal knowledge graphs (TKGs) result from exceeding the\nexpiration date of facts, which negatively impact reasoning performance on\nTKGs. However, existing reasoning methods primarily focus on positive\nimportance of historical facts, neglecting adverse effects of outdated facts.\nBesides, training on these outdated facts yields extra computational cost. To\naddress these challenges, we propose an outdated fact filtering framework named\nHALO, which quantifies the temporal validity of historical facts by exploring\nthe half-life theory to filter outdated facts in TKGs. HALO consists of three\nmodules: the temporal fact attention module, the dynamic relation-aware encoder\nmodule, and the outdated fact filtering module. Firstly, the temporal fact\nattention module captures the evolution of historical facts over time to\nidentify relevant facts. Secondly, the dynamic relation-aware encoder module is\ndesigned for efficiently predicting the half life of each fact. Finally, we\nconstruct a time decay function based on the half-life theory to quantify the\ntemporal validity of facts and filter outdated facts. Experimental results show\nthat HALO outperforms the state-of-the-art TKG reasoning methods on three\npublic datasets, demonstrating its effectiveness in detecting and filtering\noutdated facts (Codes are available at\nhttps://github.com/yushuowiki/K-Half/tree/main ).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aHALO\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8fc7\u6ee4\u65f6\u95f4\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u8fc7\u65f6\u4e8b\u5b9e\uff0c\u901a\u8fc7\u534a\u8870\u671f\u7406\u8bba\u91cf\u5316\u5386\u53f2\u4e8b\u5b9e\u7684\u65f6\u95f4\u6709\u6548\u6027\uff0c\u63d0\u9ad8\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5386\u53f2\u4e8b\u5b9e\u7684\u79ef\u6781\u5f71\u54cd\uff0c\u800c\u5ffd\u7565\u4e86\u8fc7\u65f6\u4e8b\u5b9e\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u540c\u65f6\u8bad\u7ec3\u8fd9\u4e9b\u4e8b\u5b9e\u589e\u52a0\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "method": "HALO\u6846\u67b6\u5305\u62ec\u4e09\u4e2a\u6a21\u5757\uff1a\u65f6\u95f4\u4e8b\u5b9e\u6ce8\u610f\u529b\u6a21\u5757\u6355\u6349\u5386\u53f2\u4e8b\u5b9e\u7684\u65f6\u95f4\u6f14\u53d8\uff0c\u52a8\u6001\u5173\u7cfb\u611f\u77e5\u7f16\u7801\u5668\u6a21\u5757\u9884\u6d4b\u4e8b\u5b9e\u534a\u8870\u671f\uff0c\u8fc7\u65f6\u4e8b\u5b9e\u8fc7\u6ee4\u6a21\u5757\u57fa\u4e8e\u534a\u8870\u671f\u7406\u8bba\u6784\u5efa\u65f6\u95f4\u8870\u51cf\u51fd\u6570\u6765\u8fc7\u6ee4\u8fc7\u65f6\u4e8b\u5b9e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHALO\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65f6\u95f4\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u65b9\u6cd5\u3002", "conclusion": "HALO\u6846\u67b6\u6709\u6548\u5730\u68c0\u6d4b\u548c\u8fc7\u6ee4\u8fc7\u65f6\u4e8b\u5b9e\uff0c\u63d0\u9ad8\u4e86\u65f6\u95f4\u77e5\u8bc6\u56fe\u8c31\u7684\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2505.07495", "pdf": "https://arxiv.org/pdf/2505.07495", "abs": "https://arxiv.org/abs/2505.07495", "authors": ["Isabelle van der Vegt", "Bennett Kleinberg", "Marilu Miotto", "Jonas Festor"], "title": "Translating the Grievance Dictionary: a psychometric evaluation of Dutch, German, and Italian versions", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces and evaluates three translations of the Grievance\nDictionary, a psycholinguistic dictionary for the analysis of violent,\nthreatening or grievance-fuelled texts. Considering the relevance of these\nthemes in languages beyond English, we translated the Grievance Dictionary to\nDutch, German, and Italian. We describe the process of automated translation\nsupplemented by human annotation. Psychometric analyses are performed,\nincluding internal reliability of dictionary categories and correlations with\nthe LIWC dictionary. The Dutch and German translations perform similarly to the\noriginal English version, whereas the Italian dictionary shows low reliability\nfor some categories. Finally, we make suggestions for further validation and\napplication of the dictionary, as well as for future dictionary translations\nfollowing a similar approach.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e09\u79cdGrievance Dictionary\u7684\u7ffb\u8bd1\u7248\u672c\uff08\u8377\u5170\u8bed\u3001\u5fb7\u8bed\u3001\u610f\u5927\u5229\u8bed\uff09\uff0c\u8bc4\u4f30\u4e86\u5176\u5fc3\u7406\u6d4b\u91cf\u5b66\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7ffb\u8bd1\u548c\u9a8c\u8bc1\u7684\u5efa\u8bae\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e3a\u4e86\u6269\u5c55Grievance Dictionary\u5728\u82f1\u8bed\u4ee5\u5916\u7684\u8bed\u8a00\u4e2d\u7684\u5e94\u7528\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u66b4\u529b\u3001\u5a01\u80c1\u6216\u6028\u6124\u76f8\u5173\u6587\u672c\u7684\u5206\u6790\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u81ea\u52a8\u5316\u7ffb\u8bd1\u7ed3\u5408\u4eba\u5de5\u6807\u6ce8\uff0c\u5e76\u8fdb\u884c\u5fc3\u7406\u6d4b\u91cf\u5b66\u5206\u6790\uff08\u5982\u5185\u90e8\u53ef\u9760\u6027\u548c\u4e0eLIWC\u8bcd\u5178\u7684\u76f8\u5173\u6027\uff09\u3002", "result": "\u7ed3\u679c\u663e\u793a\u8377\u5170\u8bed\u548c\u5fb7\u8bed\u7ffb\u8bd1\u7248\u672c\u4e0e\u539f\u59cb\u82f1\u8bed\u7248\u672c\u8868\u73b0\u76f8\u4f3c\uff0c\u800c\u610f\u5927\u5229\u8bed\u7248\u672c\u5728\u67d0\u4e9b\u7c7b\u522b\u4e0a\u53ef\u9760\u6027\u8f83\u4f4e\u3002", "conclusion": "\u7ed3\u8bba\u63d0\u51fa\u4e86\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u548c\u5e94\u7528\u8be5\u8bcd\u5178\u7684\u5efa\u8bae\uff0c\u4ee5\u53ca\u672a\u6765\u7c7b\u4f3c\u7ffb\u8bd1\u65b9\u6cd5\u7684\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2505.06367", "pdf": "https://arxiv.org/pdf/2505.06367", "abs": "https://arxiv.org/abs/2505.06367", "authors": ["Everest Yang", "Ria Vasishtha", "Luqman K. Dad", "Lisa A. Kachnic", "Andrew Hope", "Eric Wang", "Xiao Wu", "Yading Yuan", "David J. Brenner", "Igor Shuryak"], "title": "CAST: Time-Varying Treatment Effects with Application to Chemotherapy and Radiotherapy on Head and Neck Squamous Cell Carcinoma", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Causal machine learning (CML) enables individualized estimation of treatment\neffects, offering critical advantages over traditional correlation-based\nmethods. However, existing approaches for medical survival data with censoring\nsuch as causal survival forests estimate effects at fixed time points, limiting\ntheir ability to capture dynamic changes over time. We introduce Causal\nAnalysis for Survival Trajectories (CAST), a novel framework that models\ntreatment effects as continuous functions of time following treatment. By\ncombining parametric and non-parametric methods, CAST overcomes the limitations\nof discrete time-point analysis to estimate continuous effect trajectories.\nUsing the RADCURE dataset [1] of 2,651 patients with head and neck squamous\ncell carcinoma (HNSCC) as a clinically relevant example, CAST models how\nchemotherapy and radiotherapy effects evolve over time at the population and\nindividual levels. By capturing the temporal dynamics of treatment response,\nCAST reveals how treatment effects rise, peak, and decline over the follow-up\nperiod, helping clinicians determine when and for whom treatment benefits are\nmaximized. This framework advances the application of CML to personalized care\nin HNSCC and other life-threatening medical conditions. Source code/data\navailable at: https://github.com/CAST-FW/HNSCC", "AI": {"tldr": "CAST\u6846\u67b6\u63d0\u51fa\u4e86\u4e00\u79cd\u8fde\u7eed\u65f6\u95f4\u70b9\u56e0\u679c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u751f\u5b58\u5206\u6790\u4e2d\u4ec5\u80fd\u8bc4\u4f30\u56fa\u5b9a\u65f6\u95f4\u70b9\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u7ed3\u5408\u53c2\u6570\u548c\u975e\u53c2\u6570\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6cbb\u7597\u6548\u679c\u968f\u65f6\u95f4\u52a8\u6001\u53d8\u5316\u7684\u8fde\u7eed\u4f30\u8ba1\u3002", "motivation": "\u4f20\u7edf\u56e0\u679c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u533b\u5b66\u751f\u5b58\u6570\u636e\u65f6\uff0c\u4ec5\u80fd\u5728\u56fa\u5b9a\u65f6\u95f4\u70b9\u8bc4\u4f30\u6cbb\u7597\u6548\u679c\uff0c\u65e0\u6cd5\u6355\u6349\u6cbb\u7597\u6548\u679c\u7684\u52a8\u6001\u53d8\u5316\u3002\u8fd9\u9650\u5236\u4e86\u5176\u5728\u4e2a\u6027\u5316\u533b\u7597\u4e2d\u7684\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u9884\u540e\u968f\u65f6\u95f4\u53d8\u5316\u7684\u75be\u75c5\uff08\u5982\u5934\u9888\u90e8\u9cde\u72b6\u7ec6\u80de\u764c\uff09\u3002\u57fa\u4e8e\u6b64\uff0cCAST\u6846\u67b6\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\u3002", "method": "CAST\u7ed3\u5408\u53c2\u6570\u548c\u975e\u53c2\u6570\u65b9\u6cd5\uff0c\u5c06\u6cbb\u7597\u6548\u679c\u5efa\u6a21\u4e3a\u6cbb\u7597\u540e\u7684\u8fde\u7eed\u65f6\u95f4\u51fd\u6570\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5b83\u5229\u7528RADCURE\u6570\u636e\u96c6\u4e2d\u76842651\u4f8b\u60a3\u8005\u6570\u636e\uff0c\u901a\u8fc7\u52a8\u6001\u8f68\u8ff9\u5206\u6790\u63ed\u793a\u5316\u7597\u548c\u653e\u7597\u6548\u679c\u5728\u4eba\u7fa4\u548c\u4e2a\u4f53\u5c42\u9762\u7684\u65f6\u95f4\u6f14\u53d8\u89c4\u5f8b\u3002", "result": "\u5728\u5934\u9888\u90e8\u9cde\u72b6\u7ec6\u80de\u764c\u7684\u5b9e\u8bc1\u5206\u6790\u4e2d\uff0cCAST\u6210\u529f\u91cf\u5316\u4e86\u6cbb\u7597\u6548\u679c\u968f\u65f6\u95f4\u4e0a\u5347\u3001\u5cf0\u503c\u548c\u4e0b\u964d\u7684\u52a8\u6001\u8d8b\u52bf\uff0c\u5e2e\u52a9\u4e34\u5e8a\u533b\u751f\u66f4\u7cbe\u51c6\u5730\u5224\u65ad\u6cbb\u7597\u83b7\u76ca\u7684\u65f6\u673a\u548c\u9002\u7528\u4eba\u7fa4\u3002", "conclusion": "CAST\u901a\u8fc7\u8fde\u7eed\u65f6\u95f4\u70b9\u7684\u56e0\u679c\u6548\u5e94\u5206\u6790\uff0c\u63a8\u52a8\u4e86\u56e0\u679c\u673a\u5668\u5b66\u4e60\u5728\u4e2a\u6027\u5316\u533b\u7597\uff08\u5982HNSCC\u548c\u5176\u4ed6\u81f4\u547d\u6027\u75be\u75c5\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u4e3a\u52a8\u6001\u6cbb\u7597\u51b3\u7b56\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u652f\u6301\u3002"}}
{"id": "2505.07531", "pdf": "https://arxiv.org/pdf/2505.07531", "abs": "https://arxiv.org/abs/2505.07531", "authors": ["Khurram Mazher", "Saad Bin Nasir"], "title": "QuantX: A Framework for Hardware-Aware Quantization of Generative AI Workloads", "categories": ["cs.AI"], "comment": null, "summary": "We present QuantX: a tailored suite of recipes for LLM and VLM quantization.\nIt is capable of quantizing down to 3-bit resolutions with minimal loss in\nperformance. The quantization strategies in QuantX take into account\nhardware-specific constraints to achieve efficient dequantization during\ninference ensuring flexible trade-off between runtime speed, memory requirement\nand model accuracy. Our results demonstrate that QuantX achieves performance\nwithin 6% of the unquantized model for LlaVa-v1.6 quantized down to 3-bits for\nmultiple end user tasks and outperforms recently published state-of-the-art\nquantization techniques. This manuscript provides insights into the LLM\nquantization process that motivated the range of recipes and options that are\nincorporated in QuantX.", "AI": {"tldr": "QuantX\u662f\u4e00\u79cd\u9488\u5bf9LLM\u548cVLM\u91cf\u8eab\u5b9a\u5236\u7684\u91cf\u5316\u65b9\u6cd5\u5957\u4ef6\uff0c\u652f\u6301\u6700\u4f4e3\u6bd4\u7279\u7684\u91cf\u5316\uff0c\u6027\u80fd\u635f\u5931\u6781\u5c0f\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u91cf\u5316\u8fc7\u7a0b\u4e2d\u6027\u80fd\u635f\u5931\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u9ad8\u6548\u7684\u91cf\u5316\u7b56\u7565\u5e76\u8003\u8651\u786c\u4ef6\u7ea6\u675f\u3002", "method": "\u91c7\u7528\u786c\u4ef6\u611f\u77e5\u7684\u91cf\u5316\u7b56\u7565\uff0c\u652f\u6301\u52a8\u6001\u91cf\u5316\u548c\u89e3\u91cf\u5316\uff0c\u4ee5\u5e73\u8861\u8fd0\u884c\u901f\u5ea6\u3001\u5185\u5b58\u9700\u6c42\u548c\u6a21\u578b\u7cbe\u5ea6\u3002", "result": "QuantX\u57283\u6bd4\u7279\u91cf\u5316\u4e0b\uff0c\u6027\u80fd\u635f\u5931\u4ec5\u4e3a6%\u4ee5\u5185\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u91cf\u5316\u6280\u672f\u3002", "conclusion": "QuantX\u901a\u8fc7\u4f18\u5316\u91cf\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u4f4e\u635f\u5931\u7684\u6a21\u578b\u91cf\u5316\uff0c\u4e3aLLM\u548cVLM\u7684\u91cf\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.07512", "pdf": "https://arxiv.org/pdf/2505.07512", "abs": "https://arxiv.org/abs/2505.07512", "authors": ["Xu Huang", "Weiwen Liu", "Xingshan Zeng", "Yuefeng Huang", "Xinlong Hao", "Yuxian Wang", "Yirong Zeng", "Chuhan Wu", "Yasheng Wang", "Ruiming Tang", "Defu Lian"], "title": "ToolACE-DEV: Self-Improving Tool Learning via Decomposition and EVolution", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The tool-using capability of large language models (LLMs) enables them to\naccess up-to-date external information and handle complex tasks. Current\napproaches to enhancing this capability primarily rely on distilling advanced\nmodels by data synthesis. However, this method incurs significant costs\nassociated with advanced model usage and often results in data compatibility\nissues, led by the high discrepancy in the knowledge scope between the advanced\nmodel and the target model. To address these challenges, we propose\nToolACE-DEV, a self-improving framework for tool learning. First, we decompose\nthe tool-learning objective into sub-tasks that enhance basic tool-making and\ntool-using abilities. Then, we introduce a self-evolving paradigm that allows\nlightweight models to self-improve, reducing reliance on advanced LLMs.\nExtensive experiments validate the effectiveness of our approach across models\nof varying scales and architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aToolACE-DEV\u7684\u81ea\u6539\u8fdb\u6846\u67b6\uff0c\u7528\u4e8e\u51cf\u5c11\u5de5\u5177\u5b66\u4e60\u4e2d\u9ad8\u7ea7\u8bed\u8a00\u6a21\u578b\u7684\u4f7f\u7528\u4f9d\u8d56\u548c\u6570\u636e\u517c\u5bb9\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u89e3\u4efb\u52a1\u548c\u5f15\u5165\u81ea\u8fdb\u5316\u7b56\u7565\u3002", "motivation": "\u5f53\u524d\u5de5\u5177\u5b66\u4e60\u4e3b\u8981\u4f9d\u8d56\u9ad8\u7ea7\u6a21\u578b\u7684\u6570\u636e\u5408\u6210\uff0c\u6210\u672c\u9ad8\u4e14\u5b58\u5728\u6570\u636e\u517c\u5bb9\u95ee\u9898\u3002", "method": "\u5c06\u5de5\u5177\u5b66\u4e60\u76ee\u6807\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\uff0c\u5f15\u5165\u81ea\u8fdb\u5316\u8303\u5f0f\uff0c\u51cf\u5c11\u5bf9\u9ad8\u7ea7\u6a21\u578b\u7684\u4f9d\u8d56\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u89c4\u6a21\u548c\u67b6\u6784\u7684\u6a21\u578b\u4e0a\u5747\u6709\u6548\u3002", "conclusion": "ToolACE-DEV\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4f4e\u6210\u672c\u7684\u5de5\u5177\u5b66\u4e60\u65b9\u6848\u3002"}}
{"id": "2505.06371", "pdf": "https://arxiv.org/pdf/2505.06371", "abs": "https://arxiv.org/abs/2505.06371", "authors": ["Jae-Won Chung", "Jiachen Liu", "Jeff J. Ma", "Ruofan Wu", "Oh Jun Kweon", "Yuxuan Xia", "Zhiyu Wu", "Mosharaf Chowdhury"], "title": "The ML.ENERGY Benchmark: Toward Automated Inference Energy Measurement and Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "Leaderboard: https://ml.energy/leaderboard", "summary": "As the adoption of Generative AI in real-world services grow explosively,\nenergy has emerged as a critical bottleneck resource. However, energy remains a\nmetric that is often overlooked, under-explored, or poorly understood in the\ncontext of building ML systems. We present the ML.ENERGY Benchmark, a benchmark\nsuite and tool for measuring inference energy consumption under realistic\nservice environments, and the corresponding ML.ENERGY Leaderboard, which have\nserved as a valuable resource for those hoping to understand and optimize the\nenergy consumption of their generative AI services. In this paper, we explain\nfour key design principles for benchmarking ML energy we have acquired over\ntime, and then describe how they are implemented in the ML.ENERGY Benchmark. We\nthen highlight results from the latest iteration of the benchmark, including\nenergy measurements of 40 widely used model architectures across 6 different\ntasks, case studies of how ML design choices impact energy consumption, and how\nautomated optimization recommendations can lead to significant (sometimes more\nthan 40%) energy savings without changing what is being computed by the model.\nThe ML.ENERGY Benchmark is open-source and can be easily extended to various\ncustomized models and application scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86ML.ENERGY Benchmark\uff0c\u7528\u4e8e\u6d4b\u91cf\u751f\u6210\u5f0fAI\u5728\u771f\u5b9e\u670d\u52a1\u73af\u5883\u4e2d\u7684\u63a8\u7406\u80fd\u8017\uff0c\u5e76\u603b\u7ed3\u4e86\u56db\u79cd\u5173\u952e\u8bbe\u8ba1\u539f\u5219\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5728\u73b0\u5b9e\u670d\u52a1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u80fd\u6e90\u6210\u4e3a\u5173\u952e\u74f6\u9888\u8d44\u6e90\uff0c\u4f46ML\u7cfb\u7edf\u7684\u80fd\u8017\u95ee\u9898\u5e38\u88ab\u5ffd\u89c6\u6216\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u901a\u8fc7ML.ENERGY Benchmark\u5de5\u5177\u548c\u5bf9\u5e94\u7684Leaderboard\uff0c\u6d4b\u91cf40\u79cd\u5e38\u7528\u6a21\u578b\u67b6\u6784\u57286\u79cd\u4efb\u52a1\u4e2d\u7684\u80fd\u8017\uff0c\u5e76\u5c55\u793a\u8bbe\u8ba1\u9009\u62e9\u5bf9\u80fd\u8017\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u81ea\u52a8\u5316\u4f18\u5316\u5efa\u8bae\u53ef\u5e26\u6765\u663e\u8457\uff08\u6709\u65f6\u8d85\u8fc740%\uff09\u7684\u80fd\u6e90\u8282\u7ea6\uff0c\u540c\u65f6\u4e0d\u6539\u53d8\u6a21\u578b\u7684\u8ba1\u7b97\u7ed3\u679c\u3002", "conclusion": "ML.ENERGY Benchmark\u5f00\u6e90\u4e14\u6613\u4e8e\u6269\u5c55\uff0c\u4e3a\u7406\u89e3\u548c\u4f18\u5316\u751f\u6210\u5f0fAI\u670d\u52a1\u7684\u80fd\u8017\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002"}}
{"id": "2505.07581", "pdf": "https://arxiv.org/pdf/2505.07581", "abs": "https://arxiv.org/abs/2505.07581", "authors": ["Lei Wang", "Heyang Gao", "Xiaohe Bo", "Xu Chen", "Ji-Rong Wen"], "title": "YuLan-OneSim: Towards the Next Generation of Social Simulator with Large Language Models", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "Leveraging large language model (LLM) based agents to simulate human social\nbehaviors has recently gained significant attention. In this paper, we\nintroduce a novel social simulator called YuLan-OneSim. Compared to previous\nworks, YuLan-OneSim distinguishes itself in five key aspects: (1) Code-free\nscenario construction: Users can simply describe and refine their simulation\nscenarios through natural language interactions with our simulator. All\nsimulation code is automatically generated, significantly reducing the need for\nprogramming expertise. (2) Comprehensive default scenarios: We implement 50\ndefault simulation scenarios spanning 8 domains, including economics,\nsociology, politics, psychology, organization, demographics, law, and\ncommunication, broadening access for a diverse range of social researchers. (3)\nEvolvable simulation: Our simulator is capable of receiving external feedback\nand automatically fine-tuning the backbone LLMs, significantly enhancing the\nsimulation quality. (4) Large-scale simulation: By developing a fully\nresponsive agent framework and a distributed simulation architecture, our\nsimulator can handle up to 100,000 agents, ensuring more stable and reliable\nsimulation results. (5) AI social researcher: Leveraging the above features, we\ndevelop an AI social researcher. Users only need to propose a research topic,\nand the AI researcher will automatically analyze the input, construct\nsimulation environments, summarize results, generate technical reports, review\nand refine the reports--completing the social science research loop. To\ndemonstrate the advantages of YuLan-OneSim, we conduct experiments to evaluate\nthe quality of the automatically generated scenarios, the reliability,\nefficiency, and scalability of the simulation process, as well as the\nperformance of the AI social researcher.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aYuLan-OneSim\u7684\u65b0\u578b\u793e\u4ea4\u6a21\u62df\u5668\uff0c\u5177\u5907\u65e0\u4ee3\u7801\u573a\u666f\u6784\u5efa\u3001\u5168\u9762\u9ed8\u8ba4\u573a\u666f\u3001\u53ef\u8fdb\u5316\u6a21\u62df\u3001\u5927\u89c4\u6a21\u6a21\u62df\u548cAI\u793e\u4ea4\u7814\u7a76\u8005\u4e94\u5927\u7279\u70b9\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u52bf\u3002", "motivation": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u6a21\u62df\u4eba\u7c7b\u793e\u4ea4\u884c\u4e3a\uff0c\u65e8\u5728\u964d\u4f4e\u7f16\u7a0b\u95e8\u69db\u3001\u8986\u76d6\u591a\u9886\u57df\u7814\u7a76\u9700\u6c42\u5e76\u63d0\u5347\u6a21\u62df\u8d28\u91cf\u3002", "method": "\u5f00\u53d1YuLan-OneSim\u6a21\u62df\u5668\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u751f\u6210\u4ee3\u7801\uff0c\u5185\u7f6e\u591a\u9886\u57df\u573a\u666f\uff0c\u652f\u6301\u5916\u90e8\u53cd\u9988\u4f18\u5316LLM\uff0c\u91c7\u7528\u5206\u5e03\u5f0f\u67b6\u6784\u652f\u6301\u5927\u89c4\u6a21\u4ee3\u7406\uff0c\u5e76\u96c6\u6210AI\u793e\u4ea4\u7814\u7a76\u6d41\u7a0b\u81ea\u52a8\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660eYuLan-OneSim\u5728\u573a\u666f\u751f\u6210\u8d28\u91cf\u3001\u6a21\u62df\u53ef\u9760\u6027\u3001\u6548\u7387\u548c\u6269\u5c55\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0cAI\u793e\u4ea4\u7814\u7a76\u8005\u80fd\u9ad8\u6548\u5b8c\u6210\u7814\u7a76\u95ed\u73af\u3002", "conclusion": "YuLan-OneSim\u901a\u8fc7\u6280\u672f\u521b\u65b0\u663e\u8457\u63d0\u5347\u4e86\u793e\u4ea4\u6a21\u62df\u7684\u6613\u7528\u6027\u548c\u7814\u7a76\u6548\u7387\uff0c\u4e3a\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2505.07528", "pdf": "https://arxiv.org/pdf/2505.07528", "abs": "https://arxiv.org/abs/2505.07528", "authors": ["Lei Wang"], "title": "SEReDeEP: Hallucination Detection in Retrieval-Augmented Models via Semantic Entropy and Context-Parameter Fusion", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) models frequently encounter\nhallucination phenomena when integrating external information with internal\nparametric knowledge. Empirical studies demonstrate that the disequilibrium\nbetween external contextual information and internal parametric knowledge\nconstitutes a primary factor in hallucination generation. Existing\nhallucination detection methodologies predominantly emphasize either the\nexternal or internal mechanism in isolation, thereby overlooking their\nsynergistic effects. The recently proposed ReDeEP framework decouples these\ndual mechanisms, identifying two critical contributors to hallucinations:\nexcessive reliance on parametric knowledge encoded in feed-forward networks\n(FFN) and insufficient utilization of external information by attention\nmechanisms (particularly copy heads). ReDeEP quantitatively assesses these\nfactors to detect hallucinations and dynamically modulates the contributions of\nFFNs and copy heads to attenuate their occurrence. Nevertheless, ReDeEP and\nnumerous other hallucination detection approaches have been employed at\nlogit-level uncertainty estimation or language-level self-consistency\nevaluation, inadequately address the semantic dimensions of model responses,\nresulting in inconsistent hallucination assessments in RAG implementations.\nBuilding upon ReDeEP's foundation, this paper introduces SEReDeEP, which\nenhances computational processes through semantic entropy captured via trained\nlinear probes, thereby achieving hallucination assessments that more accurately\nreflect ground truth evaluations.", "AI": {"tldr": "SEReDeEP\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u71b5\u589e\u5f3a\u5e7b\u89c9\u68c0\u6d4b\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u4e49\u7ef4\u5ea6\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff08\u5982ReDeEP\uff09\u5728\u8bed\u4e49\u7ef4\u5ea6\u4e0a\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u65e0\u6cd5\u5145\u5206\u53cd\u6620\u6a21\u578b\u54cd\u5e94\u7684\u771f\u5b9e\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "SEReDeEP\u5229\u7528\u8bad\u7ec3\u597d\u7684\u7ebf\u6027\u63a2\u9488\u6355\u83b7\u8bed\u4e49\u71b5\uff0c\u6539\u8fdb\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u4ece\u800c\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u5e7b\u89c9\u73b0\u8c61\u3002", "result": "SEReDeEP\u5728\u5e7b\u89c9\u68c0\u6d4b\u4e0a\u66f4\u63a5\u8fd1\u4eba\u5de5\u8bc4\u4f30\u7684\u771f\u5b9e\u7ed3\u679c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002", "conclusion": "SEReDeEP\u901a\u8fc7\u8bed\u4e49\u71b5\u589e\u5f3a\uff0c\u4e3a\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2505.06384", "pdf": "https://arxiv.org/pdf/2505.06384", "abs": "https://arxiv.org/abs/2505.06384", "authors": ["Aditya Mishra", "Haroon Lone"], "title": "RiM: Record, Improve and Maintain Physical Well-being using Federated Learning", "categories": ["cs.LG", "cs.CR", "cs.CY"], "comment": "Report submitted in partial fulfilment of the requirements for the\n  award of the degree of Bachelor of Science (BS) in Electrical Engineering and\n  Computer Science", "summary": "In academic settings, the demanding environment often forces students to\nprioritize academic performance over their physical well-being. Moreover,\nprivacy concerns and the inherent risk of data breaches hinder the deployment\nof traditional machine learning techniques for addressing these health\nchallenges. In this study, we introduce RiM: Record, Improve, and Maintain, a\nmobile application which incorporates a novel personalized machine learning\nframework that leverages federated learning to enhance students' physical\nwell-being by analyzing their lifestyle habits. Our approach involves\npre-training a multilayer perceptron (MLP) model on a large-scale simulated\ndataset to generate personalized recommendations. Subsequently, we employ\nfederated learning to fine-tune the model using data from IISER Bhopal\nstudents, thereby ensuring its applicability in real-world scenarios. The\nfederated learning approach guarantees differential privacy by exclusively\nsharing model weights rather than raw data. Experimental results show that the\nFedAvg-based RiM model achieves an average accuracy of 60.71% and a mean\nabsolute error of 0.91--outperforming the FedPer variant (average accuracy\n46.34%, MAE 1.19)--thereby demonstrating its efficacy in predicting lifestyle\ndeficits under privacy-preserving constraints.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRiM\u7684\u79fb\u52a8\u5e94\u7528\uff0c\u7ed3\u5408\u4e2a\u6027\u5316\u673a\u5668\u5b66\u4e60\u6846\u67b6\u548c\u8054\u90a6\u5b66\u4e60\uff0c\u63d0\u5347\u5b66\u751f\u7684\u8eab\u4f53\u5065\u5eb7\u72b6\u51b5\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u6a21\u578b\u5728\u9690\u79c1\u4fdd\u62a4\u6761\u4ef6\u4e0b\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5b66\u672f\u538b\u529b\u5bfc\u81f4\u5b66\u751f\u5ffd\u89c6\u8eab\u4f53\u5065\u5eb7\uff0c\u4f46\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u56e0\u9690\u79c1\u95ee\u9898\u96be\u4ee5\u5e94\u7528\uff0c\u56e0\u6b64\u7814\u7a76\u63d0\u51fa\u4e86\u9690\u79c1\u4fdd\u62a4\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591a\u5c42\u611f\u77e5\u5668\u9884\u8bad\u7ec3\u548c\u8054\u90a6\u5b66\u4e60\u5fae\u8c03\u6a21\u578b\uff0c\u4ec5\u5171\u4eab\u6a21\u578b\u6743\u91cd\u800c\u975e\u539f\u59cb\u6570\u636e\u4ee5\u786e\u4fdd\u9690\u79c1\u3002", "result": "RiM\u6a21\u578b\u5e73\u5747\u51c6\u786e\u738760.71%\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee0.91\uff0c\u4f18\u4e8eFedPer\u53d8\u4f53\u3002", "conclusion": "\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u6709\u6548\u63d0\u5347\u5b66\u751f\u5065\u5eb7\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2505.07686", "pdf": "https://arxiv.org/pdf/2505.07686", "abs": "https://arxiv.org/abs/2505.07686", "authors": ["Muzhi Dai", "Chenxu Yang", "Qingyi Si"], "title": "S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "As Test-Time Scaling emerges as an active research focus in the large\nlanguage model community, advanced post-training methods increasingly emphasize\nextending chain-of-thought (CoT) generation length, thereby enhancing reasoning\ncapabilities to approach Deepseek R1-like reasoning models. However, recent\nstudies reveal that reasoning models (even Qwen3) consistently exhibit\nexcessive thought redundancy in CoT generation. This overthinking problem stems\nfrom conventional outcome-reward reinforcement learning's systematic neglect in\nregulating intermediate reasoning steps. This paper proposes Serial-Group\nDecaying-Reward Policy Optimization (namely S-GRPO), a novel reinforcement\nlearning method that empowers models with the capability to determine the\nsufficiency of reasoning steps, subsequently triggering early exit of CoT\ngeneration. Specifically, unlike GRPO, which samples multiple possible\ncompletions (parallel group) in parallel, we select multiple temporal positions\nin the generation of one CoT to allow the model to exit thinking and instead\ngenerate answers (serial group), respectively. For the correct answers in a\nserial group, we assign rewards that decay according to positions, with lower\nrewards towards the later ones, thereby reinforcing the model's behavior to\ngenerate higher-quality answers at earlier phases with earlier exits of\nthinking. Empirical evaluations demonstrate compatibility with state-of-the-art\nreasoning models, including Qwen3 and Deepseek-distill models, achieving 35.4%\n~ 61.1\\% sequence length reduction with 0.72% ~ 6.08% accuracy improvements\nacross GSM8K, AIME 2024, AMC 2023, MATH-500, and GPQA Diamond benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aS-GRPO\u7684\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u8282\u601d\u7ef4\u94fe\uff08CoT\uff09\u751f\u6210\u7684\u4e2d\u95f4\u6b65\u9aa4\u51cf\u5c11\u5197\u4f59\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u65e9\u9000\u51fa\u601d\u8003\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u7ed3\u679c\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5ffd\u89c6\u4e86\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u7684\u8c03\u8282\uff0c\u5bfc\u81f4\u601d\u7ef4\u94fe\u751f\u6210\u65f6\u51fa\u73b0\u5197\u4f59\u601d\u8003\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f18\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86Serial-Group Decaying-Reward Policy Optimization\uff08S-GRPO\uff09\uff0c\u901a\u8fc7\u5728\u5355\u4e00CoT\u751f\u6210\u4e2d\u9009\u62e9\u591a\u4e2a\u65f6\u95f4\u70b9\u5141\u8bb8\u6a21\u578b\u601d\u8003\u9000\u51fa\uff0c\u5e76\u5bf9\u65e9\u671f\u6b63\u786e\u7684\u7b54\u6848\u5206\u914d\u8870\u51cf\u5956\u52b1\u3002", "result": "\u5728Qwen3\u548cDeepseek-distill\u7b49\u591a\u4e2a\u6a21\u578b\u4e0a\u6d4b\u8bd5\uff0c\u5b9e\u73b0\u4e8635.4%\u81f361.1%\u7684\u5e8f\u5217\u957f\u5ea6\u51cf\u5c11\u548c0.72%\u81f36.08%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "S-GRPO\u80fd\u591f\u6709\u6548\u51cf\u5c11\u63a8\u7406\u6b65\u9aa4\u5197\u4f59\uff0c\u63d0\u5347\u6a21\u578b\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578b\u3002"}}
{"id": "2505.07591", "pdf": "https://arxiv.org/pdf/2505.07591", "abs": "https://arxiv.org/abs/2505.07591", "authors": ["Junjie Ye", "Caishuang Huang", "Zhuohan Chen", "Wenjie Fu", "Chenyuan Yang", "Leyi Yang", "Yilong Wu", "Peng Wang", "Meng Zhou", "Xiaolong Yang", "Tao Gui", "Qi Zhang", "Zhongchao Shi", "Jianping Fan", "Xuanjing Huang"], "title": "A Multi-Dimensional Constraint Framework for Evaluating and Improving Instruction Following in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Instruction following evaluates large language models (LLMs) on their ability\nto generate outputs that adhere to user-defined constraints. However, existing\nbenchmarks often rely on templated constraint prompts, which lack the diversity\nof real-world usage and limit fine-grained performance assessment. To fill this\ngap, we propose a multi-dimensional constraint framework encompassing three\nconstraint patterns, four constraint categories, and four difficulty levels.\nBuilding on this framework, we develop an automated instruction generation\npipeline that performs constraint expansion, conflict detection, and\ninstruction rewriting, yielding 1,200 code-verifiable instruction-following\ntest samples. We evaluate 19 LLMs across seven model families and uncover\nsubstantial variation in performance across constraint forms. For instance,\naverage performance drops from 77.67% at Level I to 32.96% at Level IV.\nFurthermore, we demonstrate the utility of our approach by using it to generate\ndata for reinforcement learning, achieving substantial gains in instruction\nfollowing without degrading general performance. In-depth analysis indicates\nthat these gains stem primarily from modifications in the model's attention\nmodules parameters, which enhance constraint recognition and adherence. Code\nand data are available in https://github.com/Junjie-Ye/MulDimIF.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u7ef4\u5ea6\u7ea6\u675f\u6846\u67b6\u548c\u81ea\u52a8\u5316\u6307\u4ee4\u751f\u6210\u6d41\u7a0b\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e0d\u540c\u7ea6\u675f\u5f62\u5f0f\u7684\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6570\u636e\u751f\u6210\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4f9d\u8d56\u6a21\u677f\u5316\u7684\u7ea6\u675f\u63d0\u793a\uff0c\u7f3a\u4e4f\u73b0\u5b9e\u5e94\u7528\u7684\u591a\u6837\u6027\u4e14\u96be\u4ee5\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u542b\u4e09\u79cd\u7ea6\u675f\u6a21\u5f0f\u3001\u56db\u7c7b\u7ea6\u675f\u548c\u56db\u79cd\u96be\u5ea6\u7ea7\u522b\u7684\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u7a0b\u751f\u6210\u4e861200\u4e2a\u53ef\u9a8c\u8bc1\u7684\u6d4b\u8bd5\u6837\u672c\u3002\u8bc4\u4f30\u4e8619\u4e2a\u4e0d\u540c\u5bb6\u65cf\u7684LLMs\u3002", "result": "\u6027\u80fd\u4eceLevel I\u768477.67%\u964d\u81f3Level IV\u768432.96%\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u751f\u6210\u6570\u636e\u663e\u8457\u63d0\u5347\u4e86\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u800c\u4e0d\u5f71\u54cd\u901a\u7528\u6027\u80fd\u3002", "conclusion": "\u591a\u7ef4\u5ea6\u7ea6\u675f\u6846\u67b6\u6709\u6548\u63ed\u793a\u4e86LLMs\u7684\u5c40\u9650\u6027\uff0c\u5176\u751f\u6210\u7684\u5f3a\u5316\u5b66\u4e60\u6570\u636e\u53ef\u4f18\u5316\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u548c\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2505.06445", "pdf": "https://arxiv.org/pdf/2505.06445", "abs": "https://arxiv.org/abs/2505.06445", "authors": ["Yan Zheng", "Qiang Chen", "Chenglei Niu"], "title": "Tweedie Regression for Video Recommendation System", "categories": ["cs.LG", "cs.IR"], "comment": "ICMI 2025 IEEE 4th International Conference on Computing and Machine\n  Intelligence April 05-06, 2025", "summary": "Modern recommendation systems aim to increase click-through rates (CTR) for\nbetter user experience, through commonly treating ranking as a classification\ntask focused on predicting CTR. However, there is a gap between this method and\nthe actual objectives of businesses across different sectors. In video\nrecommendation services, the objective of video on demand (VOD) extends beyond\nmerely encouraging clicks, but also guiding users to discover their true\ninterests, leading to increased watch time. And longer users watch time will\nleads to more revenue through increased chances of presenting online display\nadvertisements. This research addresses the issue by redefining the problem\nfrom classification to regression, with a focus on maximizing revenue through\nuser viewing time. Due to the lack of positive labels on recommendation, the\nstudy introduces Tweedie Loss Function, which is better suited in this scenario\nthan the traditional mean square error loss. The paper also provides insights\non how Tweedie process capture users diverse interests. Our offline simulation\nand online A/B test revealed that we can substantially enhance our core\nbusiness objectives: user engagement in terms of viewing time and,\nconsequently, revenue. Additionally, we provide a theoretical comparison\nbetween the Tweedie Loss and the commonly employed viewing time weighted\nLogloss, highlighting why Tweedie Regression stands out as an efficient\nsolution. We further outline a framework for designing a loss function that\nfocuses on a singular objective.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f20\u7edfCTR\u9884\u6d4b\u5728\u89c6\u9891\u63a8\u8350\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4f7f\u7528\u56de\u5f52\u65b9\u6cd5\uff08Tweedie Loss\uff09\u4f18\u5316\u7528\u6237\u89c2\u770b\u65f6\u957f\u53ca\u6536\u5165\uff0c\u9a8c\u8bc1\u5176\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u63a8\u8350\u7cfb\u7edf\u8fc7\u4e8e\u5173\u6ce8CTR\u9884\u6d4b\uff0c\u5ffd\u89c6\u4e86\u89c6\u9891\u70b9\u64ad\u670d\u52a1\u7684\u6838\u5fc3\u76ee\u6807\uff08\u5982\u7528\u6237\u89c2\u770b\u65f6\u957f\u548c\u6536\u5165\uff09\u3002\u9700\u91cd\u65b0\u5b9a\u4e49\u95ee\u9898\u4ee5\u66f4\u8d34\u5408\u5b9e\u9645\u4e1a\u52a1\u9700\u6c42\u3002", "method": "\u91c7\u7528\u56de\u5f52\u65b9\u6cd5\uff08Tweedie Loss\u51fd\u6570\uff09\u66ff\u4ee3\u5206\u7c7b\u4efb\u52a1\uff0c\u7ed3\u5408\u79bb\u7ebf\u6a21\u62df\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u9a8c\u8bc1\u6a21\u578b\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u7528\u6237\u89c2\u770b\u65f6\u957f\u53ca\u6536\u5165\uff0c\u4e14Tweedie Loss\u5728\u7406\u8bba\u5bf9\u6bd4\u4e2d\u4f18\u4e8e\u52a0\u6743Logloss\u3002", "conclusion": "Tweedie\u56de\u5f52\u4e3a\u89c6\u9891\u63a8\u8350\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5f3a\u8c03\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\u5e94\u805a\u7126\u5355\u4e00\u6838\u5fc3\u76ee\u6807\u3002"}}
{"id": "2505.07693", "pdf": "https://arxiv.org/pdf/2505.07693", "abs": "https://arxiv.org/abs/2505.07693", "authors": ["Sebastian Dumbrava"], "title": "Belief Injection for Epistemic Control in Linguistic State Space", "categories": ["cs.AI"], "comment": "30 pages, 9 figures", "summary": "This work introduces belief injection, a proactive epistemic control\nmechanism for artificial agents whose cognitive states are structured as\ndynamic ensembles of linguistic belief fragments. Grounded in the Semantic\nManifold framework, belief injection directly incorporates targeted linguistic\nbeliefs into an agent's internal cognitive state, influencing reasoning and\nalignment proactively rather than reactively. We delineate various injection\nstrategies, such as direct, context-aware, goal-oriented, and reflective\napproaches, and contrast belief injection with related epistemic control\nmechanisms, notably belief filtering. Additionally, this work discusses\npractical applications, implementation considerations, ethical implications,\nand outlines promising directions for future research into cognitive governance\nusing architecturally embedded belief injection.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4fe1\u5ff5\u6ce8\u5165\uff08belief injection\uff09\uff0c\u4e00\u79cd\u9488\u5bf9\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u7684\u524d\u77bb\u6027\u8ba4\u77e5\u63a7\u5236\u673a\u5236\uff0c\u901a\u8fc7\u8bed\u8a00\u4fe1\u5ff5\u7247\u6bb5\u76f4\u63a5\u5f71\u54cd\u5176\u63a8\u7406\u548c\u884c\u4e3a\u5bf9\u9f50\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u4e3b\u52a8\u800c\u975e\u53cd\u5e94\u6027\u7684\u65b9\u5f0f\uff0c\u52a8\u6001\u8c03\u6574AI\u4ee3\u7406\u7684\u8ba4\u77e5\u72b6\u6001\uff0c\u63d0\u5347\u5176\u63a8\u7406\u548c\u5bf9\u9f50\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u8bed\u4e49\u6d41\u5f62\u6846\u67b6\uff08Semantic Manifold\uff09\uff0c\u63d0\u51fa\u591a\u79cd\u6ce8\u5165\u7b56\u7565\uff08\u5982\u76f4\u63a5\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u3001\u76ee\u6807\u5bfc\u5411\u548c\u53cd\u601d\u6027\u6ce8\u5165\uff09\uff0c\u5e76\u4e0e\u4fe1\u5ff5\u8fc7\u6ee4\u7b49\u673a\u5236\u5bf9\u6bd4\u3002", "result": "\u63a2\u8ba8\u4e86\u5b9e\u8df5\u5e94\u7528\u3001\u5b9e\u65bd\u8003\u91cf\u53ca\u4f26\u7406\u5f71\u54cd\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u8ba4\u77e5\u6cbb\u7406\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u4fe1\u5ff5\u5d4c\u5165\uff08belief injection\uff09\u4f5c\u4e3a\u67b6\u6784\u5316\u8ba4\u77e5\u6cbb\u7406\u5de5\u5177\uff0c\u5177\u6709\u6f5c\u529b\u548c\u7814\u7a76\u4ef7\u503c\u3002"}}
{"id": "2505.07596", "pdf": "https://arxiv.org/pdf/2505.07596", "abs": "https://arxiv.org/abs/2505.07596", "authors": ["Ziyang Huang", "Xiaowei Yuan", "Yiming Ju", "Jun Zhao", "Kang Liu"], "title": "Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-augmented generation (RAG) is a common strategy to reduce\nhallucinations in Large Language Models (LLMs). While reinforcement learning\n(RL) can enable LLMs to act as search agents by activating retrieval\ncapabilities, existing ones often underutilize their internal knowledge. This\ncan lead to redundant retrievals, potential harmful knowledge conflicts, and\nincreased inference latency. To address these limitations, an efficient and\nadaptive search agent capable of discerning optimal retrieval timing and\nsynergistically integrating parametric (internal) and retrieved (external)\nknowledge is in urgent need. This paper introduces the Reinforced\nInternal-External Knowledge Synergistic Reasoning Agent (IKEA), which could\nindentify its own knowledge boundary and prioritize the utilization of internal\nknowledge, resorting to external search only when internal knowledge is deemed\ninsufficient. This is achieved using a novel knowledge-boundary aware reward\nfunction and a knowledge-boundary aware training dataset. These are designed\nfor internal-external knowledge synergy oriented RL, incentivizing the model to\ndeliver accurate answers, minimize unnecessary retrievals, and encourage\nappropriate external searches when its own knowledge is lacking. Evaluations\nacross multiple knowledge reasoning tasks demonstrate that IKEA significantly\noutperforms baseline methods, reduces retrieval frequency significantly, and\nexhibits robust generalization capabilities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIKEA\u7684\u667a\u80fd\u68c0\u7d22\u4ee3\u7406\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5185\u90e8\u77e5\u8bc6\u4e0e\u5916\u90e8\u68c0\u7d22\u7684\u534f\u540c\uff0c\u51cf\u5c11\u5197\u4f59\u68c0\u7d22\u5e76\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u5728LLMs\u4e2d\u5e38\u5bfc\u81f4\u5197\u4f59\u68c0\u7d22\u3001\u77e5\u8bc6\u51b2\u7a81\u548c\u5ef6\u8fdf\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u52a8\u6001\u5224\u65ad\u68c0\u7d22\u65f6\u673a\u5e76\u534f\u540c\u5185\u5916\u77e5\u8bc6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u77e5\u8bc6\u8fb9\u754c\u611f\u77e5\u7684\u5956\u52b1\u51fd\u6570\u548c\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u6fc0\u52b1\u6a21\u578b\u4f18\u5148\u4f7f\u7528\u5185\u90e8\u77e5\u8bc6\uff0c\u4ec5\u5728\u5fc5\u8981\u65f6\u8fdb\u884c\u5916\u90e8\u68c0\u7d22\u3002", "result": "\u5728\u591a\u9879\u77e5\u8bc6\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cIKEA\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u68c0\u7d22\u9891\u7387\u5e76\u5c55\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "IKEA\u901a\u8fc7\u5185\u5916\u77e5\u8bc6\u534f\u540c\u673a\u5236\uff0c\u6709\u6548\u5e73\u8861\u4e86\u68c0\u7d22\u6548\u7387\u4e0e\u51c6\u786e\u6027\uff0c\u4e3aLLMs\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.06446", "pdf": "https://arxiv.org/pdf/2505.06446", "abs": "https://arxiv.org/abs/2505.06446", "authors": ["Jessie Finocchiaro", "Rafael Frongillo", "Enrique Nueve"], "title": "Structured Prediction with Abstention via the Lov\u00e1sz Hinge", "categories": ["cs.LG"], "comment": "This paper is an extension of the work \"The Structured Abstain\n  Problem and the Lov\\'asz Hinge\" (arXiv:2203.08645) via the original authors", "summary": "The Lov\\'asz hinge is a convex loss function proposed for binary structured\nclassification, in which k related binary predictions jointly evaluated by a\nsubmodular function. Despite its prevalence in image segmentation and related\ntasks, the consistency of the Lov\\'asz hinge has remained open. We show that\nthe Lov\\'asz hinge is inconsistent with its desired target unless the set\nfunction used for evaluation is modular. Leveraging the embedding framework of\nFinocchiaro et al. (2024), we find the target loss for which the Lov\\'asz hinge\nis consistent. This target, which we call the structured abstain problem, is a\nvariant of selective classification for structured prediction that allows one\nto abstain on any subset of the k binary predictions. We derive a family of\nlink functions, each of which is simultaneously consistent for all\npolymatroids, a subset of submodular set functions. We then give sufficient\nconditions on the polymatroid for the structured abstain problem to be tightly\nembedded by the Lov\\'asz hinge, meaning no target prediction is redundant. We\nexperimentally demonstrate the potential of the structured abstain problem for\ninterpretability in structured classification tasks. Finally, for the\nmulticlass setting, we show that one can combine the binary encoding\nconstruction of Ramaswamy et al. (2018) with our link construction to achieve\nan efficient consistent surrogate for a natural multiclass generalization of\nthe structured abstain problem.", "AI": {"tldr": "Lov\u00e1sz\u94f0\u94fe\u662f\u4e00\u79cd\u7528\u4e8e\u4e8c\u5143\u7ed3\u6784\u5316\u5206\u7c7b\u7684\u51f8\u635f\u5931\u51fd\u6570\uff0c\u4f46\u5176\u4e00\u81f4\u6027\u672a\u88ab\u8bc1\u660e\u3002\u672c\u6587\u53d1\u73b0\u9664\u975e\u8bc4\u4f30\u51fd\u6570\u4e3a\u6a21\u5757\u5316\uff0c\u5426\u5219Lov\u00e1sz\u94f0\u94fe\u662f\u4e0d\u4e00\u81f4\u7684\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u76ee\u6807\u635f\u5931\u51fd\u6570\u2014\u2014\u201c\u7ed3\u6784\u5316\u5f03\u6743\u95ee\u9898\u201d\u3002\u6b64\u5916\uff0c\u8fd8\u63a8\u5bfc\u4e86\u4e00\u65cf\u94fe\u63a5\u51fd\u6570\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u5176\u6f5c\u529b\u3002", "motivation": "\u7814\u7a76Lov\u00e1sz\u94f0\u94fe\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u76ee\u6807\u635f\u5931\u51fd\u6570\u4ee5\u89e3\u51b3\u5176\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u540c\u65f6\u63a2\u7d22\u5176\u5728\u7ed3\u6784\u5316\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5229\u7528Finocchiaro\u7b49\u4eba\u7684\u5d4c\u5165\u6846\u67b6\uff0c\u5206\u6790Lov\u00e1sz\u94f0\u94fe\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u63a8\u5bfc\u51fa\u4e00\u65cf\u94fe\u63a5\u51fd\u6570\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u7ed3\u6784\u5316\u5f03\u6743\u95ee\u9898\u7684\u6709\u6548\u6027\u3002", "result": "\u53d1\u73b0Lov\u00e1sz\u94f0\u94fe\u4ec5\u5728\u8bc4\u4f30\u51fd\u6570\u4e3a\u6a21\u5757\u5316\u65f6\u4e00\u81f4\uff0c\u63d0\u51fa\u4e86\u7ed3\u6784\u5316\u5f03\u6743\u95ee\u9898\u4f5c\u4e3a\u5176\u4e00\u81f4\u7684\u76ee\u6807\u635f\u5931\u51fd\u6570\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u6784\u5316\u5f03\u6743\u95ee\u9898\u4e3aLov\u00e1sz\u94f0\u94fe\u63d0\u4f9b\u4e86\u4e00\u81f4\u6027\uff0c\u5e76\u4e3a\u7ed3\u6784\u5316\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.07757", "pdf": "https://arxiv.org/pdf/2505.07757", "abs": "https://arxiv.org/abs/2505.07757", "authors": ["Rintaro Ando"], "title": "Emotion-Gradient Metacognitive RSI (Part I): Theoretical Foundations and Single-Agent Architecture", "categories": ["cs.AI", "cs.LG", "F.1.2; I.2.0"], "comment": "21 pages, 3 figures. Part I of a four-part series (Parts II-IV\n  forthcoming)", "summary": "We present the Emotion-Gradient Metacognitive Recursive Self-Improvement\n(EG-MRSI) framework, a novel architecture that integrates introspective\nmetacognition, emotion-based intrinsic motivation, and recursive\nself-modification into a unified theoretical system. The framework is\nexplicitly capable of overwriting its own learning algorithm under formally\nbounded risk. Building upon the Noise-to-Meaning RSI (N2M-RSI) foundation,\nEG-MRSI introduces a differentiable intrinsic reward function driven by\nconfidence, error, novelty, and cumulative success. This signal regulates both\na metacognitive mapping and a self-modification operator constrained by\nprovable safety mechanisms. We formally define the initial agent configuration,\nemotion-gradient dynamics, and RSI trigger conditions, and derive a\nreinforcement-compatible optimization objective that guides the agent's\ndevelopment trajectory. Meaning Density and Meaning Conversion Efficiency are\nintroduced as quantifiable metrics of semantic learning, closing the gap\nbetween internal structure and predictive informativeness. This Part I paper\nestablishes the single-agent theoretical foundations of EG-MRSI. Future parts\nwill extend this framework to include safety certificates and rollback\nprotocols (Part II), collective intelligence mechanisms (Part III), and\nfeasibility constraints including thermodynamic and computational limits (Part\nIV). Together, the EG-MRSI series provides a rigorous, extensible foundation\nfor open-ended and safe AGI.", "AI": {"tldr": "EG-MRSI\u6846\u67b6\u878d\u5408\u5143\u8ba4\u77e5\u4e0e\u60c5\u7eea\u9a71\u52a8\u673a\u5236\uff0c\u652f\u6301\u9012\u5f52\u81ea\u4fee\u6539\uff0c\u63d0\u51fa\u53ef\u5fae\u5206\u5185\u5728\u5956\u52b1\u51fd\u6570\u4e0e\u5b89\u5168\u673a\u5236\uff0c\u4e3a\u5f00\u653e\u5f0fAGI\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u9012\u5f52\u81ea\u6539\u8fdb\u7cfb\u7edf\u5728\u5b89\u5168\u6027\u4e0e\u8bed\u4e49\u5b66\u4e60\u91cf\u5316\u4e0a\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u60c5\u7eea\u68af\u5ea6\u4e0e\u5143\u8ba4\u77e5\u7ed3\u5408\u7684\u521b\u65b0\u6846\u67b6\u3002", "method": "\u57fa\u4e8eN2M-RSI\uff0c\u5f15\u5165\u60c5\u7eea\u68af\u5ea6\u52a8\u6001\u3001\u5185\u5728\u5956\u52b1\u51fd\u6570\u53ca\u5b89\u5168\u7ea6\u675f\u7684\u81ea\u4fee\u6539\u7b97\u5b50\uff0c\u5b9a\u4e49\u5f3a\u5316\u5b66\u4e60\u517c\u5bb9\u7684\u4f18\u5316\u76ee\u6807\u3002", "result": "\u63d0\u51fa\u53ef\u91cf\u5316\u8bed\u4e49\u5b66\u4e60\u7684\u6307\u6807\uff08\u610f\u4e49\u5bc6\u5ea6\u4e0e\u8f6c\u6362\u6548\u7387\uff09\uff0c\u521d\u6b65\u5efa\u7acb\u5355\u4ee3\u7406\u7406\u8bba\u4f53\u7cfb\u3002", "conclusion": "EG-MRSI\u4e3a\u5b89\u5168\u3001\u5f00\u653e\u7684AGI\u5960\u5b9a\u57fa\u7840\uff0c\u540e\u7eed\u5c06\u6269\u5c55\u81f3\u5b89\u5168\u534f\u8bae\u3001\u7fa4\u4f53\u667a\u80fd\u53ca\u7269\u7406\u9650\u5236\u7b49\u65b9\u5411\u3002"}}
{"id": "2505.07601", "pdf": "https://arxiv.org/pdf/2505.07601", "abs": "https://arxiv.org/abs/2505.07601", "authors": ["Edirlei Soares de Lima", "Marco A. Casanova", "Bruno Feij\u00f3", "Antonio L. Furtado"], "title": "Characterizing the Investigative Methods of Fictional Detectives with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Detective fiction, a genre defined by its complex narrative structures and\ncharacter-driven storytelling, presents unique challenges for computational\nnarratology, a research field focused on integrating literary theory into\nautomated narrative generation. While traditional literary studies have offered\ndeep insights into the methods and archetypes of fictional detectives, these\nanalyses often focus on a limited number of characters and lack the scalability\nneeded for the extraction of unique traits that can be used to guide narrative\ngeneration methods. In this paper, we present an AI-driven approach for\nsystematically characterizing the investigative methods of fictional\ndetectives. Our multi-phase workflow explores the capabilities of 15 Large\nLanguage Models (LLMs) to extract, synthesize, and validate distinctive\ninvestigative traits of fictional detectives. This approach was tested on a\ndiverse set of seven iconic detectives - Hercule Poirot, Sherlock Holmes,\nWilliam Murdoch, Columbo, Father Brown, Miss Marple, and Auguste Dupin -\ncapturing the distinctive investigative styles that define each character. The\nidentified traits were validated against existing literary analyses and further\ntested in a reverse identification phase, achieving an overall accuracy of\n91.43%, demonstrating the method's effectiveness in capturing the distinctive\ninvestigative approaches of each detective. This work contributes to the\nbroader field of computational narratology by providing a scalable framework\nfor character analysis, with potential applications in AI-driven interactive\nstorytelling and automated narrative generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdAI\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5730\u5206\u6790\u865a\u6784\u4fa6\u63a2\u7684\u8c03\u67e5\u65b9\u6cd5\uff0c\u901a\u8fc715\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63d0\u53d6\u3001\u7efc\u5408\u5e76\u9a8c\u8bc1\u5176\u72ec\u7279\u7279\u5f81\uff0c\u6d4b\u8bd5\u51c6\u786e\u6027\u8fbe91.43%\u3002", "motivation": "\u4f20\u7edf\u6587\u5b66\u7814\u7a76\u5bf9\u865a\u6784\u4fa6\u63a2\u7684\u5206\u6790\u5c40\u9650\u4e8e\u5c11\u6570\u89d2\u8272\uff0c\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\uff0c\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6846\u67b6\u7528\u4e8e\u8ba1\u7b97\u53d9\u4e8b\u5b66\u4e2d\u7684\u89d2\u8272\u5206\u6790\u3002", "method": "\u4f7f\u752815\u4e2aLLM\u7684\u591a\u9636\u6bb5\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5bf97\u4f4d\u6807\u5fd7\u6027\u4fa6\u63a2\u7684\u8c03\u67e5\u98ce\u683c\u8fdb\u884c\u63d0\u53d6\u3001\u7efc\u5408\u548c\u9a8c\u8bc1\u3002", "result": "\u65b9\u6cd5\u5728\u53cd\u5411\u8bc6\u522b\u9636\u6bb5\u8fbe\u523091.43%\u7684\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8ba1\u7b97\u53d9\u4e8b\u5b66\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u89d2\u8272\u5206\u6790\u6846\u67b6\uff0c\u53ef\u5e94\u7528\u4e8eAI\u9a71\u52a8\u7684\u4ea4\u4e92\u5f0f\u53d9\u4e8b\u548c\u81ea\u52a8\u6545\u4e8b\u751f\u6210\u3002"}}
{"id": "2505.06454", "pdf": "https://arxiv.org/pdf/2505.06454", "abs": "https://arxiv.org/abs/2505.06454", "authors": ["Syed Mhamudul Hasan", "Hussein Zangoti", "Iraklis Anagnostopoulos", "Abdur R. Shahid"], "title": "Sponge Attacks on Sensing AI: Energy-Latency Vulnerabilities and Defense via Model Pruning", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Recent studies have shown that sponge attacks can significantly increase the\nenergy consumption and inference latency of deep neural networks (DNNs).\nHowever, prior work has focused primarily on computer vision and natural\nlanguage processing tasks, overlooking the growing use of lightweight AI models\nin sensing-based applications on resource-constrained devices, such as those in\nInternet of Things (IoT) environments. These attacks pose serious threats of\nenergy depletion and latency degradation in systems where limited battery\ncapacity and real-time responsiveness are critical for reliable operation. This\npaper makes two key contributions. First, we present the first systematic\nexploration of energy-latency sponge attacks targeting sensing-based AI models.\nUsing wearable sensing-based AI as a case study, we demonstrate that sponge\nattacks can substantially degrade performance by increasing energy consumption,\nleading to faster battery drain, and by prolonging inference latency. Second,\nto mitigate such attacks, we investigate model pruning, a widely adopted\ncompression technique for resource-constrained AI, as a potential defense. Our\nexperiments show that pruning-induced sparsity significantly improves model\nresilience against sponge poisoning. We also quantify the trade-offs between\nmodel efficiency and attack resilience, offering insights into the security\nimplications of model compression in sensing-based AI systems deployed in IoT\nenvironments.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u63a2\u8ba8\u4e86\u9488\u5bf9\u4f20\u611fAI\u6a21\u578b\u7684\u80fd\u91cf-\u5ef6\u8fdf\u6d77\u7ef5\u653b\u51fb\uff0c\u63d0\u51fa\u6a21\u578b\u526a\u679d\u4f5c\u4e3a\u9632\u5fa1\u624b\u6bb5\uff0c\u5e76\u91cf\u5316\u4e86\u6a21\u578b\u6548\u7387\u4e0e\u653b\u51fb\u97e7\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u6d77\u7ef5\u653b\u51fb\uff0c\u5ffd\u89c6\u4e86\u8f7b\u91cf\u7ea7AI\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u7684IoT\u8bbe\u5907\u4e2d\u7684\u4f7f\u7528\u3002\u8fd9\u4e9b\u653b\u51fb\u5bf9\u7535\u6c60\u5bb9\u91cf\u6709\u9650\u4e14\u9700\u8981\u5b9e\u65f6\u54cd\u5e94\u7684\u7cfb\u7edf\u6784\u6210\u4e25\u91cd\u5a01\u80c1\u3002", "method": "\u4ee5\u53ef\u7a7f\u6234\u4f20\u611fAI\u4e3a\u4f8b\uff0c\u7814\u7a76\u4e86\u80fd\u91cf-\u5ef6\u8fdf\u6d77\u7ef5\u653b\u51fb\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u4e86\u6a21\u578b\u526a\u679d\u4f5c\u4e3a\u9632\u5fa1\u624b\u6bb5\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u526a\u679d\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u5bf9\u6d77\u7ef5\u4e2d\u6bd2\u653b\u51fb\u7684\u97e7\u6027\uff0c\u4f46\u9700\u8981\u5728\u6a21\u578b\u6548\u7387\u548c\u653b\u51fb\u97e7\u6027\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728IoT\u73af\u5883\u4e2d\u90e8\u7f72\u4f20\u611fAI\u65f6\uff0c\u6a21\u578b\u538b\u7f29\u7684\u5b89\u5168\u5f71\u54cd\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u5b89\u5168\u7684\u8f7b\u91cf\u7ea7AI\u6a21\u578b\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2505.07759", "pdf": "https://arxiv.org/pdf/2505.07759", "abs": "https://arxiv.org/abs/2505.07759", "authors": ["Jennifer Mondragon", "Carlos Rubio-Medrano", "Gael Cruz", "Dvijesh Shastri"], "title": "\"I Apologize For Not Understanding Your Policy\": Exploring the Specification and Evaluation of User-Managed Access Control Policies by AI Virtual Assistants", "categories": ["cs.AI"], "comment": null, "summary": "The rapid evolution of Artificial Intelligence (AI)-based Virtual Assistants\n(VAs) e.g., Google Gemini, ChatGPT, Microsoft Copilot, and High-Flyer Deepseek\nhas turned them into convenient interfaces for managing emerging technologies\nsuch as Smart Homes, Smart Cars, Electronic Health Records, by means of\nexplicit commands,e.g., prompts, which can be even launched via voice, thus\nproviding a very convenient interface for end-users. However, the proper\nspecification and evaluation of User-Managed Access Control Policies (U-MAPs),\nthe rules issued and managed by end-users to govern access to sensitive data\nand device functionality - within these VAs presents significant challenges,\nsince such a process is crucial for preventing security vulnerabilities and\nprivacy leaks without impacting user experience. This study provides an initial\nexploratory investigation on whether current publicly-available VAs can manage\nU-MAPs effectively across differing scenarios. By conducting unstructured to\nstructured tests, we evaluated the comprehension of such VAs, revealing a lack\nof understanding in varying U-MAP approaches. Our research not only identifies\nkey limitations, but offers valuable insights into how VAs can be further\nimproved to manage complex authorization rules and adapt to dynamic changes.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u865a\u62df\u52a9\u624b\u5728\u7ba1\u7406\u7528\u6237\u81ea\u4e3b\u8bbf\u95ee\u63a7\u5236\u7b56\u7565\uff08U-MAPs\uff09\u65f6\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u52a9\u624b\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u968f\u7740AI\u865a\u62df\u52a9\u624b\u7684\u666e\u53ca\uff0c\u7528\u6237\u901a\u8fc7\u5176\u7ba1\u7406\u654f\u611f\u6570\u636e\u548c\u8bbe\u5907\u529f\u80fd\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u5982\u4f55\u6709\u6548\u7ba1\u7406\u7528\u6237\u81ea\u4e3b\u8bbf\u95ee\u63a7\u5236\u7b56\u7565\uff08U-MAPs\uff09\u4ee5\u9632\u6b62\u5b89\u5168\u6f0f\u6d1e\u548c\u9690\u79c1\u6cc4\u9732\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u901a\u8fc7\u4ece\u975e\u7ed3\u6784\u5316\u5230\u7ed3\u6784\u5316\u7684\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86\u5f53\u524d\u516c\u5f00\u53ef\u7528\u7684\u865a\u62df\u52a9\u624b\u5728\u4e0d\u540cU-MAP\u573a\u666f\u4e0b\u7684\u7406\u89e3\u80fd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u865a\u62df\u52a9\u624b\u5728\u7406\u89e3\u591a\u6837\u5316\u7684U-MAP\u7b56\u7565\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002", "conclusion": "\u7814\u7a76\u4e0d\u4ec5\u63ed\u793a\u4e86\u865a\u62df\u52a9\u624b\u7684\u5173\u952e\u5c40\u9650\uff0c\u8fd8\u4e3a\u5982\u4f55\u6539\u8fdb\u4ee5\u7ba1\u7406\u590d\u6742\u6388\u6743\u89c4\u5219\u548c\u9002\u5e94\u52a8\u6001\u53d8\u5316\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\u3002"}}
{"id": "2505.07608", "pdf": "https://arxiv.org/pdf/2505.07608", "abs": "https://arxiv.org/abs/2505.07608", "authors": ["Xiaomi LLM-Core Team", ":", "Bingquan Xia", "Bowen Shen", "Cici", "Dawei Zhu", "Di Zhang", "Gang Wang", "Hailin Zhang", "Huaqiu Liu", "Jiebao Xiao", "Jinhao Dong", "Liang Zhao", "Peidian Li", "Peng Wang", "Shihua Yu", "Shimao Chen", "Weikun Wang", "Wenhan Ma", "Xiangwei Deng", "Yi Huang", "Yifan Song", "Zihan Jiang", "Bowen Ye", "Can Cai", "Chenhong He", "Dong Zhang", "Duo Zhang", "Guoan Wang", "Hao Tian", "Haochen Zhao", "Heng Qu", "Hongshen Xu", "Jun Shi", "Kainan Bao", "QingKai Fang", "Kang Zhou", "Kangyang Zhou", "Lei Li", "Menghang Zhu", "Nuo Chen", "Qiantong Wang", "Shaohui Liu", "Shicheng Li", "Shuhao Gu", "Shuhuai Ren", "Shuo Liu", "Sirui Deng", "Weiji Zhuang", "Weiwei Lv", "Wenyu Yang", "Xin Zhang", "Xing Yong", "Xing Zhang", "Xingchen Song", "Xinzhe Xu", "Xu Wang", "Yihan Yan", "Yu Tu", "Yuanyuan Tian", "Yudong Wang", "Yue Yu", "Zhenru Lin", "Zhichao Song", "Zihao Yue"], "title": "MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present MiMo-7B, a large language model born for reasoning tasks, with\noptimization across both pre-training and post-training stages. During\npre-training, we enhance the data preprocessing pipeline and employ a\nthree-stage data mixing strategy to strengthen the base model's reasoning\npotential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional\nMulti-Token Prediction objective for enhanced performance and accelerated\ninference speed. During post-training, we curate a dataset of 130K verifiable\nmathematics and programming problems for reinforcement learning, integrating a\ntest-difficulty-driven code-reward scheme to alleviate sparse-reward issues and\nemploying strategic data resampling to stabilize training. Extensive\nevaluations show that MiMo-7B-Base possesses exceptional reasoning potential,\noutperforming even much larger 32B models. The final RL-tuned model,\nMiMo-7B-RL, achieves superior performance on mathematics, code and general\nreasoning tasks, surpassing the performance of OpenAI o1-mini. The model\ncheckpoints are available at https://github.com/xiaomimimo/MiMo.", "AI": {"tldr": "MiMo-7B\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u63a8\u7406\u4efb\u52a1\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u4f18\u5316\u9884\u8bad\u7ec3\u548c\u540e\u8bad\u7ec3\u9636\u6bb5\u63d0\u5347\u6027\u80fd\uff0c\u6700\u7ec8\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u548c\u901a\u7528\u63a8\u7406\u4efb\u52a1\u4e0a\u8d85\u8d8a\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u3002", "motivation": "\u65e8\u5728\u6784\u5efa\u4e00\u4e2a\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u7684\u9ad8\u6548\u8bed\u8a00\u6a21\u578b\uff0c\u540c\u65f6\u901a\u8fc7\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\u89e3\u51b3\u7a00\u758f\u5956\u52b1\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u9884\u8bad\u7ec3\u9636\u6bb5\u91c7\u7528\u589e\u5f3a\u7684\u6570\u636e\u9884\u5904\u7406\u548c\u4e09\u9636\u6bb5\u6570\u636e\u6df7\u5408\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u591a\u4ee4\u724c\u9884\u6d4b\u76ee\u6807\uff1b\u540e\u8bad\u7ec3\u9636\u6bb5\u4f7f\u752813\u4e07\u4e2a\u53ef\u9a8c\u8bc1\u7684\u6570\u5b66\u548c\u7f16\u7a0b\u95ee\u9898\uff0c\u7ed3\u5408\u6d4b\u8bd5\u96be\u5ea6\u9a71\u52a8\u7684\u4ee3\u7801\u5956\u52b1\u65b9\u6848\u548c\u7b56\u7565\u6027\u6570\u636e\u91cd\u91c7\u6837\u3002", "result": "MiMo-7B\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e32B\u89c4\u6a21\u7684\u6a21\u578b\uff0c\u6700\u7ec8RL\u8c03\u4f18\u7248\u672c\u5728\u6570\u5b66\u3001\u4ee3\u7801\u548c\u901a\u7528\u63a8\u7406\u4efb\u52a1\u4e0a\u8d85\u8d8aOpenAI o1-mini\u3002", "conclusion": "MiMo-7B\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u4f18\u5316\u6280\u672f\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u9ad8\u6548\u6027\u548c\u7ade\u4e89\u529b\u3002"}}
{"id": "2505.06459", "pdf": "https://arxiv.org/pdf/2505.06459", "abs": "https://arxiv.org/abs/2505.06459", "authors": ["Pablo Flores", "Olga Graf", "Pavlos Protopapas", "Karim Pichara"], "title": "Improved Uncertainty Quantification in Physics-Informed Neural Networks Using Error Bounds and Solution Bundles", "categories": ["cs.LG", "cs.AI", "physics.comp-ph", "stat.ML"], "comment": null, "summary": "Physics-Informed Neural Networks (PINNs) have been widely used to obtain\nsolutions to various physical phenomena modeled as Differential Equations. As\nPINNs are not naturally equipped with mechanisms for Uncertainty\nQuantification, some work has been done to quantify the different uncertainties\nthat arise when dealing with PINNs. In this paper, we use a two-step procedure\nto train Bayesian Neural Networks that provide uncertainties over the solutions\nto differential equation systems provided by PINNs. We use available error\nbounds over PINNs to formulate a heteroscedastic variance that improves the\nuncertainty estimation. Furthermore, we solve forward problems and utilize the\nobtained uncertainties when doing parameter estimation in inverse problems in\ncosmology.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u6b65\u8bad\u7ec3\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u91cf\u5316\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u89e3\u51b3\u5fae\u5206\u65b9\u7a0b\u7cfb\u7edf\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u5c3d\u7ba1PINNs\u5e7f\u6cdb\u5e94\u7528\u4e8e\u7269\u7406\u73b0\u8c61\u5efa\u6a21\uff0c\u4f46\u5176\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u673a\u5236\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u65b9\u6cd5\u4ee5\u66f4\u51c6\u786e\u5730\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u901a\u8fc7\u5229\u7528PINN\u7684\u8bef\u5dee\u754c\u9650\u6784\u5efa\u5f02\u65b9\u5dee\u65b9\u5dee\uff0c\u5e76\u7ed3\u5408\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u7684\u4e24\u6b65\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u6539\u8fdb\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "result": "\u7814\u7a76\u6210\u529f\u5e94\u7528\u4e8e\u5b87\u5b99\u5b66\u4e2d\u7684\u6b63\u5411\u95ee\u9898\u6c42\u89e3\u548c\u9006\u5411\u95ee\u9898\u53c2\u6570\u4f30\u8ba1\uff0c\u5e76\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u4e0d\u786e\u5b9a\u6027\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86PINN\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u7684\u8868\u73b0\uff0c\u4e3a\u7269\u7406\u6a21\u578b\u7684\u53c2\u6570\u4f30\u8ba1\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u4f9d\u636e\u3002"}}
{"id": "2505.07773", "pdf": "https://arxiv.org/pdf/2505.07773", "abs": "https://arxiv.org/abs/2505.07773", "authors": ["Xinji Mai", "Haotian Xu", "Xing W", "Weinong Wang", "Yingying Zhang", "Wenqiang Zhang"], "title": "Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often struggle with mathematical reasoning tasks\nrequiring precise, verifiable computation. While Reinforcement Learning (RL)\nfrom outcome-based rewards enhances text-based reasoning, understanding how\nagents autonomously learn to leverage external tools like code execution\nremains crucial. We investigate RL from outcome-based rewards for\nTool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously\ngenerate and execute Python code for mathematical problems without supervised\ntool-use examples. Our central contribution is we demonstrate that as RL\ntraining progresses, key metrics scale predictably. Specifically, we observe\nstrong positive correlations where increased training steps lead to increases\nin the spontaneous code execution frequency, the average response length, and,\ncritically, the final task accuracy. This suggests a quantifiable relationship\nbetween computational effort invested in training and the emergence of\neffective, tool-augmented reasoning strategies. We implement a robust framework\nfeaturing a decoupled code execution environment and validate our findings\nacross standard RL algorithms and frameworks. Experiments show ZeroTIR\nsignificantly surpasses non-tool ZeroRL baselines on challenging math\nbenchmarks. Our findings provide a foundational understanding of how autonomous\ntool use is acquired and scales within Agent RL, offering a reproducible\nbenchmark for future studies. Code is released at\n\\href{https://github.com/Anonymize-Author/AgentRL}{https://github.com/Anonymize-Author/AgentRL}.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u57fa\u4e8e\u7ed3\u679c\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u4e3b\u751f\u6210\u5e76\u6267\u884cPython\u4ee3\u7801\u4ee5\u89e3\u51b3\u6570\u5b66\u95ee\u9898\u7684\u65b9\u6cd5\uff08ZeroTIR\uff09\uff0c\u5c55\u793a\u4e86\u8bad\u7ec3\u6b65\u6570\u4e0e\u4ee3\u7801\u6267\u884c\u9891\u7387\u3001\u56de\u7b54\u957f\u5ea6\u53ca\u4efb\u52a1\u51c6\u786e\u7387\u4e4b\u95f4\u7684\u6b63\u76f8\u5173\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3LLMs\u5728\u9700\u8981\u7cbe\u786e\u8ba1\u7b97\u7684\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\uff0c\u63a2\u7d22\u5982\u4f55\u901a\u8fc7RL\u81ea\u4e3b\u5b66\u4e60\u548c\u5229\u7528\u5916\u90e8\u5de5\u5177\uff08\u5982\u4ee3\u7801\u6267\u884c\uff09\u6765\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u3002", "method": "\u65b9\u6cd5\u91c7\u7528\u57fa\u4e8e\u7ed3\u679c\u7684RL\u8bad\u7ec3\u57fa\u7840LLMs\uff08ZeroTIR\uff09\uff0c\u65e0\u9700\u76d1\u7763\u7684\u5de5\u5177\u4f7f\u7528\u793a\u4f8b\uff0c\u81ea\u52a8\u751f\u6210\u548c\u6267\u884cPython\u4ee3\u7801\uff0c\u5e76\u5728\u89e3\u8026\u7684\u4ee3\u7801\u6267\u884c\u73af\u5883\u4e2d\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cZeroTIR\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u4e0d\u4f7f\u7528\u5de5\u5177\u7684ZeroRL\u57fa\u7ebf\uff0c\u4e14\u8bad\u7ec3\u6b65\u6570\u4e0e\u5de5\u5177\u4f7f\u7528\u6548\u679c\u5448\u6b63\u76f8\u5173\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u8be5\u65b9\u6cd5\u4e3a\u81ea\u4e3b\u5de5\u5177\u4f7f\u7528\u7684\u5b66\u4e60\u4e0e\u6269\u5c55\u63d0\u4f9b\u4e86\u91cf\u5316\u57fa\u7840\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u3002"}}
{"id": "2505.07610", "pdf": "https://arxiv.org/pdf/2505.07610", "abs": "https://arxiv.org/abs/2505.07610", "authors": ["Kenza Amara", "Rita Sevastjanova", "Mennatallah El-Assady"], "title": "Concept-Level Explainability for Auditing & Steering LLM Responses", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 7 figures, Submission to Neurips 2025", "summary": "As large language models (LLMs) become widely deployed, concerns about their\nsafety and alignment grow. An approach to steer LLM behavior, such as\nmitigating biases or defending against jailbreaks, is to identify which parts\nof a prompt influence specific aspects of the model's output. Token-level\nattribution methods offer a promising solution, but still struggle in text\ngeneration, explaining the presence of each token in the output separately,\nrather than the underlying semantics of the entire LLM response. We introduce\nConceptX, a model-agnostic, concept-level explainability method that identifies\nthe concepts, i.e., semantically rich tokens in the prompt, and assigns them\nimportance based on the outputs' semantic similarity. Unlike current\ntoken-level methods, ConceptX also offers to preserve context integrity through\nin-place token replacements and supports flexible explanation goals, e.g.,\ngender bias. ConceptX enables both auditing, by uncovering sources of bias, and\nsteering, by modifying prompts to shift the sentiment or reduce the harmfulness\nof LLM responses, without requiring retraining. Across three LLMs, ConceptX\noutperforms token-level methods like TokenSHAP in both faithfulness and human\nalignment. Steering tasks boost sentiment shift by 0.252 versus 0.131 for\nrandom edits and lower attack success rates from 0.463 to 0.242, outperforming\nattribution and paraphrasing baselines. While prompt engineering and\nself-explaining methods sometimes yield safer responses, ConceptX offers a\ntransparent and faithful alternative for improving LLM safety and alignment,\ndemonstrating the practical value of attribution-based explainability in\nguiding LLM behavior.", "AI": {"tldr": "ConceptX\u662f\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u6982\u5ff5\u7ea7\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u5ea6\u8bc6\u522b\u63d0\u793a\u4e2d\u7684\u6982\u5ff5\u5e76\u5206\u914d\u91cd\u8981\u6027\uff0c\u4f18\u4e8e\u8bcd\u7ea7\u65b9\u6cd5\uff0c\u63d0\u5347LLM\u7684\u5b89\u5168\u6027\u548c\u5bf9\u9f50\u6027\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5e7f\u6cdb\u90e8\u7f72\uff0c\u5176\u5b89\u5168\u6027\u548c\u5bf9\u9f50\u6027\u6210\u4e3a\u5173\u6ce8\u70b9\u3002\u5f53\u524d\u8bcd\u7ea7\u5f52\u56e0\u65b9\u6cd5\u5728\u6587\u672c\u751f\u6210\u4e2d\u89e3\u91ca\u80fd\u529b\u6709\u9650\uff0c\u65e0\u6cd5\u6355\u6349\u8f93\u51fa\u7684\u5e95\u5c42\u8bed\u4e49\u3002", "method": "\u63d0\u51faConceptX\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u5ea6\u5206\u6790\u63d0\u793a\u4e2d\u7684\u6982\u5ff5\uff08\u8bed\u4e49\u4e30\u5bcc\u7684\u8bcd\uff09\uff0c\u652f\u6301\u4e0a\u4e0b\u6587\u5b8c\u6574\u6027\u4fdd\u7559\u548c\u7075\u6d3b\u7684\u89e3\u91ca\u76ee\u6807\uff08\u5982\u6027\u522b\u504f\u89c1\uff09\u3002", "result": "\u5728\u4e09\u4e2aLLM\u4e0a\uff0cConceptX\u5728\u5fe0\u5b9e\u5ea6\u548c\u4eba\u5de5\u5bf9\u9f50\u6027\u4e0a\u4f18\u4e8eTokenSHAP\u7b49\u8bcd\u7ea7\u65b9\u6cd5\uff1b\u901a\u8fc7\u63d0\u793a\u8c03\u6574\uff0c\u60c5\u611f\u504f\u79fb\u63d0\u5347\u81f30.252\uff0c\u653b\u51fb\u6210\u529f\u7387\u4ece0.463\u964d\u81f30.242\u3002", "conclusion": "ConceptX\u4e3aLLM\u5b89\u5168\u6027\u548c\u5bf9\u9f50\u6027\u63d0\u4f9b\u4e86\u900f\u660e\u4e14\u5fe0\u5b9e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5f52\u56e0\u89e3\u91ca\u5728\u5f15\u5bfcLLM\u884c\u4e3a\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.06475", "pdf": "https://arxiv.org/pdf/2505.06475", "abs": "https://arxiv.org/abs/2505.06475", "authors": ["Binwen Liu", "Peiyu Xu", "Quan Yuan", "Yihong Chen"], "title": "Probing In-Context Learning: Impact of Task Complexity and Model Architecture on Generalization and Efficiency", "categories": ["cs.LG"], "comment": null, "summary": "We investigate in-context learning (ICL) through a meticulous experimental\nframework that systematically varies task complexity and model architecture.\nExtending beyond the linear regression baseline, we introduce Gaussian kernel\nregression and nonlinear dynamical system tasks, which emphasize temporal and\nrecursive reasoning. We evaluate four distinct models: a GPT2-style\nTransformer, a Transformer with FlashAttention mechanism, a convolutional\nHyena-based model, and the Mamba state-space model. Each model is trained from\nscratch on synthetic datasets and assessed for generalization during testing.\nOur findings highlight that model architecture significantly shapes ICL\nperformance. The standard Transformer demonstrates robust performance across\ndiverse tasks, while Mamba excels in temporally structured dynamics. Hyena\neffectively captures long-range dependencies but shows higher variance early in\ntraining, and FlashAttention offers computational efficiency but is more\nsensitive in low-data regimes. Further analysis uncovers locality-induced\nshortcuts in Gaussian kernel tasks, enhanced nonlinear separability through\ninput range scaling, and the critical role of curriculum learning in mastering\nhigh-dimensional tasks.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u5728\u4e0d\u540c\u4efb\u52a1\u590d\u6742\u5ea6\u548c\u6a21\u578b\u67b6\u6784\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u67b6\u6784\u5bf9ICL\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002\u6807\u51c6Transformer\u8868\u73b0\u7a33\u5065\uff0cMamba\u64c5\u957f\u65f6\u95f4\u7ed3\u6784\u4efb\u52a1\uff0cHyena\u6355\u6349\u957f\u7a0b\u4f9d\u8d56\u4f46\u8bad\u7ec3\u65e9\u671f\u65b9\u5dee\u8f83\u5927\uff0cFlashAttention\u8ba1\u7b97\u9ad8\u6548\u4f46\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u654f\u611f\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u5728\u591a\u79cd\u4efb\u52a1\u590d\u6742\u5ea6\u4e0b\u5bf9\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u4ee5\u63ed\u793a\u67b6\u6784\u8bbe\u8ba1\u5bf9\u6027\u80fd\u7684\u5173\u952e\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u5f15\u5165\u9ad8\u65af\u6838\u56de\u5f52\u548c\u975e\u7ebf\u6027\u52a8\u6001\u7cfb\u7edf\u4efb\u52a1\uff0c\u8bc4\u4f30\u56db\u79cd\u6a21\u578b\uff08\u6807\u51c6Transformer\u3001FlashAttention Transformer\u3001Hyena\u5377\u79ef\u6a21\u578b\u3001Mamba\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff09\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6807\u51c6Transformer\u8868\u73b0\u7a33\u5065\uff1bMamba\u5728\u65f6\u95f4\u7ed3\u6784\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff1bHyena\u80fd\u6355\u6349\u957f\u7a0b\u4f9d\u8d56\u4f46\u8bad\u7ec3\u65e9\u671f\u65b9\u5dee\u9ad8\uff1bFlashAttention\u8ba1\u7b97\u9ad8\u6548\u4f46\u4f4e\u6570\u636e\u654f\u611f\u3002\u8fd8\u53d1\u73b0\u9ad8\u65af\u6838\u4efb\u52a1\u4e2d\u7684\u5c40\u90e8\u6377\u5f84\u3001\u8f93\u5165\u8303\u56f4\u7f29\u653e\u589e\u5f3a\u975e\u7ebf\u6027\u53ef\u5206\u6027\u4ee5\u53ca\u8bfe\u7a0b\u5b66\u4e60\u5728\u9ad8\u7ef4\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u6a21\u578b\u67b6\u6784\u5bf9ICL\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4e0d\u540c\u67b6\u6784\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u5404\u6709\u4f18\u52a3\uff0c\u4efb\u52a1\u8bbe\u8ba1\u548c\u8bad\u7ec3\u7b56\u7565\uff08\u5982\u8bfe\u7a0b\u5b66\u4e60\uff09\u5bf9\u6027\u80fd\u63d0\u5347\u6709\u663e\u8457\u5f71\u54cd\u3002"}}
{"id": "2505.06241", "pdf": "https://arxiv.org/pdf/2505.06241", "abs": "https://arxiv.org/abs/2505.06241", "authors": ["Arek Berc Gokdag", "Silvia Mura", "Antonio Coviello", "Michele Zhu", "Maurizio Magarini", "Umberto Spagnolini"], "title": "Low-Complexity CNN-Based Classification of Electroneurographic Signals", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Peripheral nerve interfaces (PNIs) facilitate neural recording and\nstimulation for treating nerve injuries, but real-time classification of\nelectroneurographic (ENG) signals remains challenging due to constraints on\ncomplexity and latency, particularly in implantable devices. This study\nintroduces MobilESCAPE-Net, a lightweight architecture that reduces\ncomputational cost while maintaining and slightly improving classification\nperformance. Compared to the state-of-the-art ESCAPE-Net, MobilESCAPE-Net\nachieves comparable accuracy and F1-score with significantly lower complexity,\nreducing trainable parameters by 99.9\\% and floating point operations per\nsecond by 92.47\\%, enabling faster inference and real-time processing. Its\nefficiency makes it well-suited for low-complexity ENG signal classification in\nresource-constrained environments such as implantable devices.", "AI": {"tldr": "MobilESCAPE-Net\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u67b6\u6784\uff0c\u7528\u4e8e\u5b9e\u65f6\u5206\u7c7bENG\u4fe1\u53f7\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u690d\u5165\u5f0f\u8bbe\u5907\u4e2dENG\u4fe1\u53f7\u5b9e\u65f6\u5206\u7c7b\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5ef6\u8fdf\u65b9\u9762\u7684\u9650\u5236\u3002", "method": "\u5f00\u53d1\u4e86MobilESCAPE-Net\uff0c\u4e0e\u73b0\u6709ESCAPE-Net\u76f8\u6bd4\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u53ef\u8bad\u7ec3\u53c2\u6570\u548c\u6d6e\u70b9\u8fd0\u7b97\u3002", "result": "MobilESCAPE-Net\u5728\u4fdd\u6301\u51c6\u786e\u6027\u548cF1\u5206\u6570\u7684\u540c\u65f6\uff0c\u5c06\u53c2\u6570\u51cf\u5c11\u4e8699.9%\uff0c\u8fd0\u7b97\u91cf\u51cf\u5c11\u4e8692.47%\u3002", "conclusion": "MobilESCAPE-Net\u9002\u5408\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684ENG\u4fe1\u53f7\u5206\u7c7b\uff0c\u5982\u690d\u5165\u5f0f\u8bbe\u5907\u3002"}}
{"id": "2505.07637", "pdf": "https://arxiv.org/pdf/2505.07637", "abs": "https://arxiv.org/abs/2505.07637", "authors": ["Krish Goel", "Sanskar Pandey", "KS Mahadevan", "Harsh Kumar", "Vishesh Khadaria"], "title": "Chronocept: Instilling a Sense of Time in Machines", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "20 pages, 8 figures, 18 tables", "summary": "Human cognition is deeply intertwined with a sense of time, known as\nChronoception. This sense allows us to judge how long facts remain valid and\nwhen knowledge becomes outdated. Despite progress in vision, language, and\nmotor control, AI still struggles to reason about temporal validity. We\nintroduce Chronocept, the first benchmark to model temporal validity as a\ncontinuous probability distribution over time. Using skew-normal curves fitted\nalong semantically decomposed temporal axes, Chronocept captures nuanced\npatterns of emergence, decay, and peak relevance. It includes two datasets:\nBenchmark I (atomic facts) and Benchmark II (multi-sentence passages).\nAnnotations show strong inter-annotator agreement (84% and 89%). Our baselines\npredict curve parameters - location, scale, and skewness - enabling\ninterpretable, generalizable learning and outperforming classification-based\napproaches. Chronocept fills a foundational gap in AI's temporal reasoning,\nsupporting applications in knowledge grounding, fact-checking,\nretrieval-augmented generation (RAG), and proactive agents. Code and data are\npublicly available.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Chronocept\u57fa\u51c6\uff0c\u9996\u6b21\u5c06\u65f6\u95f4\u6709\u6548\u6027\u5efa\u6a21\u4e3a\u8fde\u7eed\u6982\u7387\u5206\u5e03\uff0c\u901a\u8fc7\u504f\u6001\u6b63\u6001\u66f2\u7ebf\u6355\u83b7\u77e5\u8bc6\u7684\u51fa\u73b0\u3001\u8870\u51cf\u548c\u5cf0\u503c\u76f8\u5173\u6027\uff0c\u89e3\u51b3\u4e86AI\u5728\u65f6\u95f4\u63a8\u7406\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "AI\u5728\u65f6\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4eba\u7c7b\u80fd\u591f\u611f\u77e5\u65f6\u95f4\u5e76\u5224\u65ad\u77e5\u8bc6\u7684\u65f6\u6548\u6027\uff0c\u800cAI\u5728\u8fd9\u65b9\u9762\u7684\u80fd\u529b\u6709\u9650\u3002", "method": "\u4f7f\u7528\u504f\u6001\u6b63\u6001\u66f2\u7ebf\u62df\u5408\u8bed\u4e49\u5206\u89e3\u7684\u65f6\u95f4\u8f74\uff0c\u5efa\u6a21\u65f6\u95f4\u6709\u6548\u6027\u7684\u8fde\u7eed\u6982\u7387\u5206\u5e03\uff0c\u5e76\u63d0\u4f9b\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08\u539f\u5b50\u4e8b\u5b9e\u548c\u591a\u53e5\u6bb5\u843d\uff09\u3002", "result": "\u6807\u6ce8\u6570\u636e\u7684\u4e00\u81f4\u6027\u9ad8\uff0884%\u548c89%\uff09\uff0c\u57fa\u51c6\u65b9\u6cd5\u5728\u9884\u6d4b\u66f2\u7ebf\u53c2\u6570\uff08\u4f4d\u7f6e\u3001\u5c3a\u5ea6\u548c\u504f\u5ea6\uff09\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u5206\u7c7b\u7684\u65b9\u6cd5\u3002", "conclusion": "Chronocept\u586b\u8865\u4e86AI\u65f6\u95f4\u63a8\u7406\u7684\u57fa\u7840\u7a7a\u767d\uff0c\u652f\u6301\u77e5\u8bc6\u843d\u5730\u3001\u4e8b\u5b9e\u6838\u67e5\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u4e3b\u52a8\u4ee3\u7406\u7b49\u5e94\u7528\u3002"}}
{"id": "2505.06481", "pdf": "https://arxiv.org/pdf/2505.06481", "abs": "https://arxiv.org/abs/2505.06481", "authors": ["HamidReza Imani", "Jiaxin Peng", "Peiman Mohseni", "Abdolah Amirany", "Tarek El-Ghazawi"], "title": "QoS-Efficient Serving of Multiple Mixture-of-Expert LLMs Using Partial Runtime Reconfiguration", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "The deployment of mixture-of-experts (MoE) large language models (LLMs)\npresents significant challenges due to their high memory demands. These\nchallenges become even more pronounced in multi-tenant environments, where\nshared resources must accommodate multiple models, limiting the effectiveness\nof conventional virtualization techniques. This paper addresses the problem of\nefficiently serving multiple fine-tuned MoE-LLMs on a single-GPU. We propose a\nserving system that employs \\textit{similarity-based expert consolidation} to\nreduce the overall memory footprint by sharing similar experts across models.\nTo ensure output quality, we introduce \\textit{runtime partial\nreconfiguration}, dynamically replacing non-expert layers when processing\nrequests from different models. As a result, our approach achieves a\ncompetitive output quality while maintaining throughput comparable to serving a\nsingle model while incurring a negligible increase in time-to-first-token\n(TTFT). Experiments on a server with a single NVIDIA A100 GPU (80GB) using\nMixtral-8x7B models demonstrate an 85\\% average reduction in turnaround time\ncompared to NVIDIA's multi-instance GPU (MIG). Furthermore, experiments on\nGoogle's Switch Transformer Base-8 model with up to four variants demonstrate\nthe scalability and resilience of our approach in maintaining output quality\ncompared to other model merging baselines, highlighting its effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u670d\u52a1\u591a\u4e2a\u5fae\u8c03MoE-LLM\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u76f8\u4f3c\u6027\u4e13\u5bb6\u6574\u5408\u548c\u52a8\u6001\u90e8\u5206\u91cd\u6784\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u5e76\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u591a\u79df\u6237\u73af\u5883\u4e2d\u5171\u4eabGPU\u8d44\u6e90\u65f6\uff0cMoE-LLM\u5185\u5b58\u9700\u6c42\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u76f8\u4f3c\u6027\u4e13\u5bb6\u6574\u5408\u548c\u8fd0\u884c\u65f6\u90e8\u5206\u91cd\u6784\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5185\u5b58\u5360\u7528\u51cf\u5c1185%\uff0c\u540c\u65f6\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u591a\u6a21\u578b\u5171\u4eabGPU\u8d44\u6e90\u65f6\u9ad8\u6548\u4e14\u5b9e\u7528\u3002"}}
{"id": "2505.06246", "pdf": "https://arxiv.org/pdf/2505.06246", "abs": "https://arxiv.org/abs/2505.06246", "authors": ["Dominic Parosh Yamarthi", "Haripriya Raman", "Shamsad Parvin"], "title": "United States Road Accident Prediction using Random Forest Predictor", "categories": ["cs.CY", "cs.AI", "cs.LG", "stat.AP"], "comment": "5 Pages, 8 Figures", "summary": "Road accidents significantly threaten public safety and require in-depth\nanalysis for effective prevention and mitigation strategies. This paper focuses\non predicting accidents through the examination of a comprehensive traffic\ndataset covering 49 states in the United States. The dataset integrates\ninformation from diverse sources, including transportation departments, law\nenforcement, and traffic sensors. This paper specifically emphasizes predicting\nthe number of accidents, utilizing advanced machine learning models such as\nregression analysis and time series analysis. The inclusion of various factors,\nranging from environmental conditions to human behavior and infrastructure,\nensures a holistic understanding of the dynamics influencing road safety.\nTemporal and spatial analysis further allows for the identification of trends,\nseasonal variations, and high-risk areas. The implications of this research\nextend to proactive decision-making for policymakers and transportation\nauthorities. By providing accurate predictions and quantifiable insights into\nexpected accident rates under different conditions, the paper aims to empower\nauthorities to allocate resources efficiently and implement targeted\ninterventions. The goal is to contribute to the development of informed\npolicies and interventions that enhance road safety, creating a safer\nenvironment for all road users. Keywords: Machine Learning, Random Forest,\nAccident Prediction, AutoML, LSTM.", "AI": {"tldr": "\u5229\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5206\u6790\u7f8e\u56fd49\u5dde\u4ea4\u901a\u6570\u636e\uff0c\u9884\u6d4b\u4ea4\u901a\u4e8b\u6545\u6570\u91cf\uff0c\u52a9\u529b\u51b3\u7b56\u8005\u4f18\u5316\u8d44\u6e90\u5206\u914d\u548c\u9053\u8def\u5b89\u5168\u653f\u7b56\u3002", "motivation": "\u9053\u8def\u4ea4\u901a\u4e8b\u6545\u5bf9\u516c\u5171\u5b89\u5168\u6784\u6210\u91cd\u5927\u5a01\u80c1\uff0c\u9700\u6df1\u5165\u5206\u6790\u4ee5\u5236\u5b9a\u6709\u6548\u9884\u9632\u548c\u7f13\u89e3\u7b56\u7565\u3002", "method": "\u7ed3\u5408\u56de\u5f52\u5206\u6790\u548c\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u7b49\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u6574\u5408\u73af\u5883\u3001\u4eba\u56e0\u3001\u57fa\u7840\u8bbe\u65bd\u7b49\u591a\u5143\u6570\u636e\uff0c\u8fdb\u884c\u65f6\u7a7a\u5206\u6790\u3002", "result": "\u5b9e\u73b0\u4e86\u5bf9\u4ea4\u901a\u4e8b\u6545\u6570\u91cf\u7684\u51c6\u786e\u9884\u6d4b\uff0c\u8bc6\u522b\u51fa\u8d8b\u52bf\u3001\u5b63\u8282\u6027\u53d8\u5316\u548c\u9ad8\u98ce\u9669\u533a\u57df\u3002", "conclusion": "\u7814\u7a76\u4e3a\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u91cf\u5316\u4f9d\u636e\uff0c\u6709\u52a9\u4e8e\u8d44\u6e90\u9ad8\u6548\u5206\u914d\u548c\u7cbe\u51c6\u5e72\u9884\uff0c\u63d0\u5347\u9053\u8def\u5b89\u5168\u3002"}}
{"id": "2505.07653", "pdf": "https://arxiv.org/pdf/2505.07653", "abs": "https://arxiv.org/abs/2505.07653", "authors": ["Iman Johary", "Raphael Romero", "Alexandru C. Mara", "Tijl De Bie"], "title": "JobHop: A Large-Scale Dataset of Career Trajectories", "categories": ["cs.CL"], "comment": null, "summary": "Understanding labor market dynamics is essential for policymakers, employers,\nand job seekers. However, comprehensive datasets that capture real-world career\ntrajectories are scarce. In this paper, we introduce JobHop, a large-scale\npublic dataset derived from anonymized resumes provided by VDAB, the public\nemployment service in Flanders, Belgium. Utilizing Large Language Models\n(LLMs), we process unstructured resume data to extract structured career\ninformation, which is then mapped to standardized ESCO occupation codes using a\nmulti-label classification model. This results in a rich dataset of over 2.3\nmillion work experiences, extracted from and grouped into more than 391,000\nuser resumes and mapped to standardized ESCO occupation codes, offering\nvaluable insights into real-world occupational transitions. This dataset\nenables diverse applications, such as analyzing labor market mobility, job\nstability, and the effects of career breaks on occupational transitions. It\nalso supports career path prediction and other data-driven decision-making\nprocesses. To illustrate its potential, we explore key dataset characteristics,\nincluding job distributions, career breaks, and job transitions, demonstrating\nits value for advancing labor market research.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86JobHop\u6570\u636e\u96c6\uff0c\u4e00\u4e2a\u4ece\u6bd4\u5229\u65f6\u5f17\u5170\u5fb7\u65af\u516c\u5171\u5c31\u4e1a\u670d\u52a1\u63d0\u4f9b\u7684\u533f\u540d\u7b80\u5386\u4e2d\u63d0\u53d6\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u901a\u8fc7LLM\u548c\u591a\u6807\u7b7e\u5206\u7c7b\u6a21\u578b\u5904\u7406\uff0c\u5305\u542b230\u4e07\u5de5\u4f5c\u7ecf\u9a8c\u8bb0\u5f55\uff0c\u53ef\u7528\u4e8e\u52b3\u52a8\u529b\u5e02\u573a\u7814\u7a76\u3002", "motivation": "\u52b3\u52a8\u529b\u5e02\u573a\u52a8\u6001\u7684\u7814\u7a76\u9700\u8981\u5168\u9762\u7684\u6570\u636e\u96c6\uff0c\u4f46\u73b0\u5b9e\u4e2d\u7684\u804c\u4e1a\u8f68\u8ff9\u6570\u636e\u7a00\u7f3a\u3002JobHop\u7684\u521b\u5efa\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u653f\u7b56\u5236\u5b9a\u8005\u548c\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5904\u7406\u975e\u7ed3\u6784\u5316\u7b80\u5386\u6570\u636e\uff0c\u63d0\u53d6\u7ed3\u6784\u5316\u804c\u4e1a\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u591a\u6807\u7b7e\u5206\u7c7b\u6a21\u578b\u6620\u5c04\u5230\u6807\u51c6\u5316ESCO\u804c\u4e1a\u4ee3\u7801\u3002", "result": "\u751f\u6210\u4e86\u4e00\u4e2a\u5305\u542b230\u4e07\u5de5\u4f5c\u7ecf\u9a8c\u300139.1\u4e07\u4efd\u7b80\u5386\u7684\u6570\u636e\u96c6\uff0c\u652f\u6301\u52b3\u52a8\u529b\u5e02\u573a\u6d41\u52a8\u6027\u3001\u804c\u4e1a\u7a33\u5b9a\u6027\u7b49\u5206\u6790\u3002", "conclusion": "JobHop\u6570\u636e\u96c6\u4e3a\u52b3\u52a8\u529b\u5e02\u573a\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u652f\u6301\u6570\u636e\u9a71\u52a8\u7684\u51b3\u7b56\u548c\u804c\u4e1a\u8def\u5f84\u9884\u6d4b\u3002"}}
{"id": "2505.06482", "pdf": "https://arxiv.org/pdf/2505.06482", "abs": "https://arxiv.org/abs/2505.06482", "authors": ["Minting Pan", "Yitao Zheng", "Jiajian Li", "Yunbo Wang", "Xiaokang Yang"], "title": "Video-Enhanced Offline Reinforcement Learning: A Model-Based Approach", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Offline reinforcement learning (RL) enables policy optimization in static\ndatasets, avoiding the risks and costs of real-world exploration. However, it\nstruggles with suboptimal behavior learning and inaccurate value estimation due\nto the lack of environmental interaction. In this paper, we present\nVideo-Enhanced Offline RL (VeoRL), a model-based approach that constructs an\ninteractive world model from diverse, unlabeled video data readily available\nonline. Leveraging model-based behavior guidance, VeoRL transfers commonsense\nknowledge of control policy and physical dynamics from natural videos to the RL\nagent within the target domain. Our method achieves substantial performance\ngains (exceeding 100% in some cases) across visuomotor control tasks in robotic\nmanipulation, autonomous driving, and open-world video games.", "AI": {"tldr": "VeoRL\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u5728\u7ebf\u591a\u6837\u672a\u6807\u6ce8\u89c6\u9891\u6570\u636e\u6784\u5efa\u4ea4\u4e92\u5f0f\u4e16\u754c\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u56e0\u7f3a\u4e4f\u73af\u5883\u4ea4\u4e92\u800c\u9762\u4e34\u884c\u4e3a\u5b66\u4e60\u6b21\u4f18\u548c\u503c\u4f30\u8ba1\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff0cVeoRL\u65e8\u5728\u901a\u8fc7\u89c6\u9891\u6570\u636e\u589e\u5f3a\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "VeoRL\u901a\u8fc7\u6784\u5efa\u4ea4\u4e92\u5f0f\u4e16\u754c\u6a21\u578b\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u6a21\u578b\u7684\u884c\u4e3a\u6307\u5bfc\uff0c\u5c06\u89c6\u9891\u4e2d\u7684\u5e38\u8bc6\u6027\u77e5\u8bc6\u8fc1\u79fb\u81f3\u76ee\u6807\u9886\u57df\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u3002", "result": "\u5728\u673a\u5668\u4eba\u64cd\u63a7\u3001\u81ea\u52a8\u9a7e\u9a76\u548c\u5f00\u653e\u4e16\u754c\u89c6\u9891\u6e38\u620f\u7b49\u4efb\u52a1\u4e2d\uff0cVeoRL\u7684\u6027\u80fd\u63d0\u5347\u663e\u8457\uff08\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8d85\u8fc7100%\uff09\u3002", "conclusion": "VeoRL\u901a\u8fc7\u89c6\u9891\u6570\u636e\u589e\u5f3a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u5728\u591a\u4e2a\u9886\u57df\u4e2d\u7684\u9ad8\u6548\u7b56\u7565\u4f18\u5316\uff0c\u5c55\u793a\u4e86\u6a21\u578b\u8fc1\u79fb\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.06250", "pdf": "https://arxiv.org/pdf/2505.06250", "abs": "https://arxiv.org/abs/2505.06250", "authors": ["Yizhuo Wu", "Yi Zhu", "Kun Qian", "Qinyu Chen", "Anding Zhu", "John Gajadharsing", "Leo C. N. de Vreede", "Chang Gao"], "title": "DeltaDPD: Exploiting Dynamic Temporal Sparsity in Recurrent Neural Networks for Energy-Efficient Wideband Digital Predistortion", "categories": ["eess.SP", "cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted to IEEE Microwave and Wireless Technology Letters (MWTL)", "summary": "Digital Predistortion (DPD) is a popular technique to enhance signal quality\nin wideband RF power amplifiers (PAs). With increasing bandwidth and data\nrates, DPD faces significant energy consumption challenges during deployment,\ncontrasting with its efficiency goals. State-of-the-art DPD models rely on\nrecurrent neural networks (RNN), whose computational complexity hinders system\nefficiency. This paper introduces DeltaDPD, exploring the dynamic temporal\nsparsity of input signals and neuronal hidden states in RNNs for\nenergy-efficient DPD, reducing arithmetic operations and memory accesses while\npreserving satisfactory linearization performance. Applying a TM3.1a 200MHz-BW\n256-QAM OFDM signal to a 3.5 GHz GaN Doherty RF PA, DeltaDPD achieves -50.03\ndBc in Adjacent Channel Power Ratio (ACPR), -37.22 dB in Normalized Mean Square\nError (NMSE) and -38.52 dBc in Error Vector Magnitude (EVM) with 52% temporal\nsparsity, leading to a 1.8X reduction in estimated inference power. The\nDeltaDPD code will be released after formal publication at\nhttps://www.opendpd.com.", "AI": {"tldr": "DeltaDPD\u662f\u4e00\u79cd\u9488\u5bf9\u5bbd\u5e26\u5c04\u9891\u529f\u7387\u653e\u5927\u5668\u7684\u6570\u5b57\u9884\u5931\u771f\u6280\u672f\uff0c\u901a\u8fc7\u5229\u7528\u8f93\u5165\u4fe1\u53f7\u548cRNN\u9690\u85cf\u72b6\u6001\u7684\u52a8\u6001\u65f6\u95f4\u7a00\u758f\u6027\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u8bbf\u95ee\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u7ebf\u6027\u5316\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5e26\u5bbd\u548c\u6570\u636e\u901f\u7387\u7684\u589e\u52a0\uff0c\u4f20\u7edfDPD\u6280\u672f\u9762\u4e34\u80fd\u6548\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u5b9e\u73b0\u65b9\u5f0f\u3002", "method": "\u91c7\u7528\u52a8\u6001\u65f6\u95f4\u7a00\u758f\u6027\u4f18\u5316RNN\u6a21\u578b\uff0c\u51cf\u5c11\u4e86\u7b97\u672f\u8fd0\u7b97\u548c\u5185\u5b58\u8bbf\u95ee\u3002", "result": "\u5728\u7279\u5b9a\u6d4b\u8bd5\u6761\u4ef6\u4e0b\uff0cDeltaDPD\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u7ebf\u6027\u5316\u6027\u80fd\uff08ACPR -50.03 dBc\uff0cNMSE -37.22 dB\uff0cEVM -38.52 dBc\uff09\u548c1.8\u500d\u7684\u80fd\u6548\u63d0\u5347\u3002", "conclusion": "DeltaDPD\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u80fd\u6548\uff0c\u4e3a\u9ad8\u6548DPD\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.07659", "pdf": "https://arxiv.org/pdf/2505.07659", "abs": "https://arxiv.org/abs/2505.07659", "authors": ["Ethan Gotlieb Wilcox", "Cui Ding", "Giovanni Acampa", "Tiago Pimentel", "Alex Warstadt", "Tamar I. Regev"], "title": "Using Information Theory to Characterize Prosodic Typology: The Case of Tone, Pitch-Accent and Stress-Accent", "categories": ["cs.CL"], "comment": null, "summary": "This paper argues that the relationship between lexical identity and prosody\n-- one well-studied parameter of linguistic variation -- can be characterized\nusing information theory. We predict that languages that use prosody to make\nlexical distinctions should exhibit a higher mutual information between word\nidentity and prosody, compared to languages that don't. We test this hypothesis\nin the domain of pitch, which is used to make lexical distinctions in tonal\nlanguages, like Cantonese. We use a dataset of speakers reading sentences aloud\nin ten languages across five language families to estimate the mutual\ninformation between the text and their pitch curves. We find that, across\nlanguages, pitch curves display similar amounts of entropy. However, these\ncurves are easier to predict given their associated text in the tonal\nlanguages, compared to pitch- and stress-accent languages, and thus the mutual\ninformation is higher in these languages, supporting our hypothesis. Our\nresults support perspectives that view linguistic typology as gradient, rather\nthan categorical.", "AI": {"tldr": "\u8bed\u8a00\u4e2d\u8bcd\u6c47\u8eab\u4efd\u4e0e\u97f5\u5f8b\uff08\u5982\u97f3\u9ad8\uff09\u7684\u5173\u7cfb\u53ef\u901a\u8fc7\u4fe1\u606f\u8bba\u5206\u6790\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u58f0\u8c03\u8bed\u8a00\u4e2d\u97f3\u9ad8\u4e0e\u8bcd\u6c47\u7684\u4e92\u4fe1\u606f\u66f4\u9ad8\uff0c\u652f\u6301\u8bed\u8a00\u7c7b\u578b\u5b66\u662f\u6e10\u53d8\u7684\u89c2\u70b9\u3002", "motivation": "\u63a2\u8ba8\u8bcd\u6c47\u8eab\u4efd\u4e0e\u97f5\u5f8b\uff08\u5982\u97f3\u9ad8\uff09\u7684\u5173\u7cfb\u662f\u5426\u80fd\u7528\u4fe1\u606f\u8bba\u91cf\u5316\uff0c\u5e76\u9a8c\u8bc1\u58f0\u8c03\u8bed\u8a00\u4e2d\u4e24\u8005\u7684\u4e92\u4fe1\u606f\u66f4\u9ad8\u3002", "method": "\u4f7f\u7528\u5341\u79cd\u8bed\u8a00\u7684\u8bed\u97f3\u6570\u636e\uff0c\u901a\u8fc7\u8ba1\u7b97\u6587\u672c\u4e0e\u97f3\u9ad8\u66f2\u7ebf\u7684\u4e92\u4fe1\u606f\uff0c\u5bf9\u6bd4\u58f0\u8c03\u3001\u97f3\u9ad8\u91cd\u97f3\u548c\u91cd\u97f3\u8bed\u8a00\u7684\u8868\u73b0\u3002", "result": "\u58f0\u8c03\u8bed\u8a00\u7684\u97f3\u9ad8\u66f2\u7ebf\u4e0e\u8bcd\u6c47\u7684\u4e92\u4fe1\u606f\u663e\u8457\u9ad8\u4e8e\u975e\u58f0\u8c03\u8bed\u8a00\uff0c\u652f\u6301\u5047\u8bbe\u3002\u6240\u6709\u8bed\u8a00\u7684\u97f3\u9ad8\u71b5\u76f8\u4f3c\u3002", "conclusion": "\u7814\u7a76\u652f\u6301\u8bed\u8a00\u7c7b\u578b\u5b66\u7684\u6e10\u53d8\u89c6\u89d2\uff0c\u58f0\u8c03\u8bed\u8a00\u901a\u8fc7\u97f3\u9ad8\u533a\u5206\u8bcd\u6c47\u7684\u80fd\u529b\u66f4\u5f3a\u3002"}}
{"id": "2505.06497", "pdf": "https://arxiv.org/pdf/2505.06497", "abs": "https://arxiv.org/abs/2505.06497", "authors": ["Jiacheng Wang", "Hongtao Lv", "Lei Liu"], "title": "FedADP: Unified Model Aggregation for Federated Learning with Heterogeneous Model Architectures", "categories": ["cs.LG"], "comment": null, "summary": "Traditional Federated Learning (FL) faces significant challenges in terms of\nefficiency and accuracy, particularly in heterogeneous environments where\nclients employ diverse model architectures and have varying computational\nresources. Such heterogeneity complicates the aggregation process, leading to\nperformance bottlenecks and reduced model generalizability. To address these\nissues, we propose FedADP, a federated learning framework designed to adapt to\nclient heterogeneity by dynamically adjusting model architectures during\naggregation. FedADP enables effective collaboration among clients with\ndiffering capabilities, maximizing resource utilization and ensuring model\nquality. Our experimental results demonstrate that FedADP significantly\noutperforms existing methods, such as FlexiFed, achieving an accuracy\nimprovement of up to 23.30%, thereby enhancing model adaptability and training\nefficiency in heterogeneous real-world settings.", "AI": {"tldr": "FedADP\u662f\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u9488\u5bf9\u5f02\u6784\u73af\u5883\u4e2d\u5ba2\u6237\u7aef\u7684\u591a\u6837\u6027\u6311\u6218\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6a21\u578b\u67b6\u6784\u63d0\u5347\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u76f8\u6bd4FlexiFed\u7b49\u65b9\u6cd5\u53ef\u63d0\u5347\u9ad8\u8fbe23.30%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u5728\u5f02\u6784\u73af\u5883\u4e2d\u56e0\u5ba2\u6237\u7aef\u6a21\u578b\u67b6\u6784\u548c\u8ba1\u7b97\u8d44\u6e90\u5dee\u5f02\u5bfc\u81f4\u7684\u6548\u7387\u4f4e\u548c\u51c6\u786e\u6027\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u63d0\u51faFedADP\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u805a\u5408\u8fc7\u7a0b\u4e2d\u7684\u6a21\u578b\u67b6\u6784\u4ee5\u9002\u5e94\u5ba2\u6237\u7aef\u5f02\u8d28\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793aFedADP\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5982FlexiFed\uff0c\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u534723.30%\uff0c\u540c\u65f6\u5728\u8d44\u6e90\u5229\u7528\u548c\u6a21\u578b\u9002\u5e94\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "FedDFP\u6709\u6548\u5730\u63d0\u5347\u8054\u90a6\u5b66\u4e60\u5728\u5f02\u6784\u73af\u5883\u4e2d\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.06256", "pdf": "https://arxiv.org/pdf/2505.06256", "abs": "https://arxiv.org/abs/2505.06256", "authors": ["Fuhui Zhou", "Chunyu Liu", "Hao Zhang", "Wei Wu", "Qihui Wu", "Derrick Wing Kwan Ng", "Tony Q. S. Quek", "Chan-Byoung Chae"], "title": "SpectrumFM: A Foundation Model for Intelligent Spectrum Management", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "Intelligent spectrum management is crucial for improving spectrum efficiency\nand achieving secure utilization of spectrum resources. However, existing\nintelligent spectrum management methods, typically based on small-scale models,\nsuffer from notable limitations in recognition accuracy, convergence speed, and\ngeneralization, particularly in the complex and dynamic spectrum environments.\nTo address these challenges, this paper proposes a novel spectrum foundation\nmodel, termed SpectrumFM, establishing a new paradigm for spectrum management.\nSpectrumFM features an innovative encoder architecture that synergistically\nexploits the convolutional neural networks and the multi-head self-attention\nmechanisms to enhance feature extraction and enable robust representation\nlearning. The model is pre-trained via two novel self-supervised learning\ntasks, namely masked reconstruction and next-slot signal prediction, which\nleverage large-scale in-phase and quadrature (IQ) data to achieve comprehensive\nand transferable spectrum representations. Furthermore, a parameter-efficient\nfine-tuning strategy is proposed to enable SpectrumFM to adapt to various\ndownstream spectrum management tasks, including automatic modulation\nclassification (AMC), wireless technology classification (WTC), spectrum\nsensing (SS), and anomaly detection (AD). Extensive experiments demonstrate\nthat SpectrumFM achieves superior performance in terms of accuracy, robustness,\nadaptability, few-shot learning efficiency, and convergence speed, consistently\noutperforming conventional methods across multiple benchmarks. Specifically,\nSpectrumFM improves AMC accuracy by up to 12.1% and WTC accuracy by 9.3%,\nachieves an area under the curve (AUC) of 0.97 in SS at -4 dB signal-to-noise\nratio (SNR), and enhances AD performance by over 10%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpectrumFM\u7684\u65b0\u578b\u9891\u8c31\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u5347\u4e86\u9891\u8c31\u7ba1\u7406\u7684\u8bc6\u522b\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u9891\u8c31\u7ba1\u7406\u65b9\u6cd5\u5728\u5c0f\u89c4\u6a21\u6a21\u578b\u4e0a\u5b58\u5728\u8bc6\u522b\u7cbe\u5ea6\u4f4e\u3001\u6536\u655b\u901f\u5ea6\u6162\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7b49\u95ee\u9898\uff0c\u5c24\u5176\u5728\u590d\u6742\u52a8\u6001\u9891\u8c31\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faSpectrumFM\u6a21\u578b\uff0c\u91c7\u7528\u521b\u65b0\u7684\u7f16\u7801\u5668\u67b6\u6784\uff0c\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u63a9\u7801\u91cd\u6784\u548c\u4e0b\u4e00\u65f6\u9699\u4fe1\u53f7\u9884\u6d4b\u4e24\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSpectrumFM\u5728\u7cbe\u5ea6\u3001\u9c81\u68d2\u6027\u3001\u9002\u5e94\u6027\u3001\u5c11\u6837\u672c\u5b66\u4e60\u6548\u7387\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5982\u5728AMC\u548cWTC\u4efb\u52a1\u4e2d\u5206\u522b\u63d0\u534712.1%\u548c9.3%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "SpectrumFM\u4e3a\u9891\u8c31\u7ba1\u7406\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u80fd\u591f\u6709\u6548\u9002\u5e94\u590d\u6742\u52a8\u6001\u73af\u5883\uff0c\u663e\u8457\u63d0\u5347\u591a\u9879\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2505.07671", "pdf": "https://arxiv.org/pdf/2505.07671", "abs": "https://arxiv.org/abs/2505.07671", "authors": ["Xianrui Zhong", "Bowen Jin", "Siru Ouyang", "Yanzhen Shen", "Qiao Jin", "Yin Fang", "Zhiyong Lu", "Jiawei Han"], "title": "Benchmarking Retrieval-Augmented Generation for Chemistry", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Retrieval-augmented generation (RAG) has emerged as a powerful framework for\nenhancing large language models (LLMs) with external knowledge, particularly in\nscientific domains that demand specialized and dynamic information. Despite its\npromise, the application of RAG in the chemistry domain remains underexplored,\nprimarily due to the lack of high-quality, domain-specific corpora and\nwell-curated evaluation benchmarks. In this work, we introduce ChemRAG-Bench, a\ncomprehensive benchmark designed to systematically assess the effectiveness of\nRAG across a diverse set of chemistry-related tasks. The accompanying chemistry\ncorpus integrates heterogeneous knowledge sources, including scientific\nliterature, the PubChem database, PubMed abstracts, textbooks, and Wikipedia\nentries. In addition, we present ChemRAG-Toolkit, a modular and extensible RAG\ntoolkit that supports five retrieval algorithms and eight LLMs. Using\nChemRAG-Toolkit, we demonstrate that RAG yields a substantial performance gain\n-- achieving an average relative improvement of 17.4% over direct inference\nmethods. We further conduct in-depth analyses on retriever architectures,\ncorpus selection, and the number of retrieved passages, culminating in\npractical recommendations to guide future research and deployment of RAG\nsystems in the chemistry domain. The code and data is available at\nhttps://chemrag.github.io.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86ChemRAG-Bench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5316\u5b66\u9886\u57dfRAG\u6548\u679c\u7684\u57fa\u51c6\uff0c\u4ee5\u53caChemRAG-Toolkit\uff0c\u4e00\u4e2a\u652f\u6301\u591a\u79cd\u68c0\u7d22\u7b97\u6cd5\u548cLLM\u7684\u6a21\u5757\u5316\u5de5\u5177\u5305\u3002\u5b9e\u9a8c\u663e\u793aRAG\u6bd4\u76f4\u63a5\u63a8\u7406\u65b9\u6cd5\u6027\u80fd\u63d0\u534717.4%\u3002", "motivation": "\u5316\u5b66\u9886\u57df\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u4e13\u4e1a\u8bed\u6599\u548c\u8bc4\u4f30\u57fa\u51c6\uff0c\u9650\u5236\u4e86RAG\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faChemRAG-Bench\u57fa\u51c6\u548cChemRAG-Toolkit\u5de5\u5177\u5305\uff0c\u6574\u5408\u591a\u79cd\u5316\u5b66\u77e5\u8bc6\u6e90\uff0c\u652f\u6301\u591a\u79cd\u68c0\u7d22\u7b97\u6cd5\u548cLLM\u3002", "result": "RAG\u65b9\u6cd5\u5e73\u5747\u76f8\u5bf9\u6027\u80fd\u63d0\u534717.4%\uff0c\u5e76\u5bf9\u68c0\u7d22\u67b6\u6784\u3001\u8bed\u6599\u9009\u62e9\u7b49\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\u3002", "conclusion": "ChemRAG\u4e3a\u5316\u5b66\u9886\u57df\u7684RAG\u7814\u7a76\u548c\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u548c\u5de5\u5177\u3002"}}
{"id": "2505.06519", "pdf": "https://arxiv.org/pdf/2505.06519", "abs": "https://arxiv.org/abs/2505.06519", "authors": ["Hansani Weeratunge", "Dominic Robe", "Elnaz Hajizadeh"], "title": "Interpretable SHAP-bounded Bayesian Optimization for Underwater Acoustic Metamaterial Coating Design", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.SD"], "comment": null, "summary": "We developed an interpretability informed Bayesian optimization framework to\noptimize underwater acoustic coatings based on polyurethane elastomers with\nembedded metamaterial features. A data driven model was employed to analyze the\nrelationship between acoustic performance, specifically sound absorption and\nthe corresponding design variables. By leveraging SHapley Additive exPlanations\n(SHAP), a machine learning interpretability tool, we identified the key\nparameters influencing the objective function and gained insights into how\nthese parameters affect sound absorption. The insights derived from the SHAP\nanalysis were subsequently used to automatically refine the bounds of the\noptimization problem automatically, enabling a more targeted and efficient\nexploration of the design space.\n  The proposed approach was applied to two polyurethane materials with distinct\nhardness levels, resulting in improved optimal solutions compared to those\nobtained without SHAP-informed guidance. Notably, these enhancements were\nachieved without increasing the number of simulation iterations. Our findings\ndemonstrate the potential of SHAP to streamline optimization processes by\nuncovering hidden parameter relationships and guiding the search toward\npromising regions of the design space. This work underscores the effectiveness\nof combining interpretability techniques with Bayesian optimization for the\nefficient and cost-effective design of underwater acoustic metamaterials under\nstrict computational constraints and can be generalized towards other materials\nand engineering optimization problems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89e3\u91ca\u6027\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u5177\u6709\u8d85\u6750\u6599\u7279\u6027\u7684\u805a\u6c28\u916f\u5f39\u6027\u4f53\u6c34\u4e0b\u58f0\u5b66\u6d82\u5c42\u3002\u901a\u8fc7SHAP\u5206\u6790\u5173\u952e\u53c2\u6570\u5bf9\u58f0\u5438\u6536\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u81ea\u52a8\u8c03\u6574\u4f18\u5316\u8fb9\u754c\uff0c\u4ece\u800c\u9ad8\u6548\u5730\u63a2\u7d22\u8bbe\u8ba1\u7a7a\u95f4\u3002", "motivation": "\u5f53\u524d\u6c34\u4e0b\u58f0\u5b66\u6d82\u5c42\u7684\u4f18\u5316\u8fc7\u7a0b\u53d7\u9650\u4e8e\u8ba1\u7b97\u8d44\u6e90\u548c\u9ad8\u7ef4\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408SHAP\u89e3\u91ca\u6027\u5de5\u5177\uff0c\u63ed\u793a\u53c2\u6570\u95f4\u7684\u9690\u85cf\u5173\u7cfb\uff0c\u63d0\u5347\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "method": "\u91c7\u7528\u6570\u636e\u9a71\u52a8\u6a21\u578b\u5206\u6790\u58f0\u5438\u6536\u6027\u80fd\u4e0e\u8bbe\u8ba1\u53d8\u91cf\u7684\u5173\u7cfb\uff0c\u7ed3\u5408SHAP\u8bc6\u522b\u5173\u952e\u53c2\u6570\u53ca\u5176\u5f71\u54cd\u673a\u5236\u3002\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u81ea\u52a8\u4f18\u5316\u95ee\u9898\u8fb9\u754c\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u63a2\u7d22\u3002", "result": "\u5728\u4e24\u79cd\u786c\u5ea6\u4e0d\u540c\u7684\u805a\u6c28\u916f\u6750\u6599\u4e0a\u5e94\u7528\u8be5\u65b9\u6cd5\uff0c\u4f18\u5316\u7ed3\u679c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e14\u672a\u589e\u52a0\u4eff\u771f\u8fed\u4ee3\u6b21\u6570\u3002\u8bc1\u660eSHAP\u80fd\u6709\u6548\u5f15\u5bfc\u4f18\u5316\u65b9\u5411\u3002", "conclusion": "\u7ed3\u5408SHAP\u7684\u89e3\u91ca\u6027\u4e0e\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u53ef\u9ad8\u6548\u8bbe\u8ba1\u6c34\u4e0b\u58f0\u5b66\u8d85\u6750\u6599\uff0c\u4e14\u65b9\u6cd5\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u6750\u6599\u548c\u5de5\u7a0b\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2505.07672", "pdf": "https://arxiv.org/pdf/2505.07672", "abs": "https://arxiv.org/abs/2505.07672", "authors": ["Arun S. Maiya"], "title": "OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "6 pages", "summary": "We present OnPrem.LLM, a Python-based toolkit for applying large language\nmodels (LLMs) to sensitive, non-public data in offline or restricted\nenvironments. The system is designed for privacy-preserving use cases and\nprovides prebuilt pipelines for document processing and storage,\nretrieval-augmented generation (RAG), information extraction, summarization,\nclassification, and prompt/output processing with minimal configuration.\nOnPrem.LLM supports multiple LLM backends -- including llama.cpp, Ollama, vLLM,\nand Hugging Face Transformers -- with quantized model support, GPU\nacceleration, and seamless backend switching. Although designed for fully local\nexecution, OnPrem.LLM also supports integration with a wide range of cloud LLM\nproviders when permitted, enabling hybrid deployments that balance performance\nwith data control. A no-code web interface extends accessibility to\nnon-technical users.", "AI": {"tldr": "OnPrem.LLM\u662f\u4e00\u4e2aPython\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u5728\u79bb\u7ebf\u6216\u53d7\u9650\u5236\u73af\u5883\u4e2d\u5e94\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406\u654f\u611f\u6570\u636e\uff0c\u652f\u6301\u591a\u79cdLLM\u540e\u7aef\u548c\u9690\u79c1\u4fdd\u62a4\u7528\u4f8b\u3002", "motivation": "\u9488\u5bf9\u654f\u611f\u975e\u516c\u5f00\u6570\u636e\uff0c\u9700\u8981\u5728\u9690\u79c1\u4fdd\u62a4\u7684\u524d\u63d0\u4e0b\u4f7f\u7528LLM\uff0c\u63d0\u4f9b\u672c\u5730\u5316\u89e3\u51b3\u65b9\u6848\u5e76\u517c\u987e\u7075\u6d3b\u6027\u548c\u6613\u7528\u6027\u3002", "method": "\u57fa\u4e8ePython\u7684\u5de5\u5177\u5305\uff0c\u63d0\u4f9b\u6587\u6863\u5904\u7406\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u3001\u4fe1\u606f\u63d0\u53d6\u7b49\u529f\u80fd\uff0c\u652f\u6301\u591a\u540e\u7aef\u548cGPU\u52a0\u901f\u3002", "result": "\u652f\u6301\u591a\u79cdLLM\u540e\u7aef\uff08\u5982llama.cpp\u3001Ollama\u7b49\uff09\uff0c\u5b9e\u73b0\u672c\u5730\u6267\u884c\u6216\u6df7\u5408\u90e8\u7f72\uff0c\u5e76\u63d0\u4f9b\u65e0\u4ee3\u7801\u754c\u9762\u3002", "conclusion": "OnPrem.LLM\u4e3a\u9690\u79c1\u654f\u611f\u573a\u666f\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u9ad8\u6548\u7684LLM\u5e94\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u987e\u6027\u80fd\u4e0e\u6570\u636e\u63a7\u5236\u3002"}}
{"id": "2505.06520", "pdf": "https://arxiv.org/pdf/2505.06520", "abs": "https://arxiv.org/abs/2505.06520", "authors": ["Xuran Li", "Jingyi Wang", "Xiaohan Yuan", "Peixin Zhang", "Zhan Qin", "Zhibo Wang", "Kui Ren"], "title": "PRUNE: A Patching Based Repair Framework for Certiffable Unlearning of Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "It is often desirable to remove (a.k.a. unlearn) a speciffc part of the\ntraining data from a trained neural network model. A typical application\nscenario is to protect the data holder's right to be forgotten, which has been\npromoted by many recent regulation rules. Existing unlearning methods involve\ntraining alternative models with remaining data, which may be costly and\nchallenging to verify from the data holder or a thirdparty auditor's\nperspective. In this work, we provide a new angle and propose a novel\nunlearning approach by imposing carefully crafted \"patch\" on the original\nneural network to achieve targeted \"forgetting\" of the requested data to\ndelete. Speciffcally, inspired by the research line of neural network repair,\nwe propose to strategically seek a lightweight minimum \"patch\" for unlearning a\ngiven data point with certiffable guarantee. Furthermore, to unlearn a\nconsiderable amount of data points (or an entire class), we propose to\niteratively select a small subset of representative data points to unlearn,\nwhich achieves the effect of unlearning the whole set. Extensive experiments on\nmultiple categorical datasets demonstrates our approach's effectiveness,\nachieving measurable unlearning while preserving the model's performance and\nbeing competitive in efffciency and memory consumption compared to various\nbaseline methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5fae\u8c03\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u6570\u636e\u5220\u9664\u7684\u65b0\u65b9\u6cd5\uff0c\u6bd4\u91cd\u65b0\u8bad\u7ec3\u66f4\u9ad8\u6548\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6570\u636e\u5220\u9664\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u9a8c\u8bc1\u96be\u7684\u95ee\u9898\uff0c\u6ee1\u8db3\u6cd5\u89c4\u5bf9\u6570\u636e\u9057\u5fd8\u6743\u7684\u8981\u6c42\u3002", "method": "\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u2018\u8865\u4e01\u2019\u4fee\u6539\u7f51\u7edc\uff0c\u9010\u6b65\u5220\u9664\u4ee3\u8868\u6027\u6570\u636e\u70b9\u4ee5\u5b9e\u73b0\u6279\u91cf\u9057\u5fd8\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5220\u9664\u6570\u636e\u4e14\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u6548\u7387\u548c\u5185\u5b58\u5360\u7528\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6570\u636e\u5220\u9664\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u9a8c\u8bc1\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.07705", "pdf": "https://arxiv.org/pdf/2505.07705", "abs": "https://arxiv.org/abs/2505.07705", "authors": ["Letian Peng", "Jingbo Shang"], "title": "Codifying Character Logic in Role-Playing", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces Codified Profiles for role-playing, a novel approach\nthat represents character logic as structured, executable functions for\nbehavioral decision-making. Each profile defines a set of functions\nparse_by_scene(scene) that outputs a list of logic-grounded assertions\ntriggered_statements, using both explicit control structures (e.g.,\nif-then-else) and condition checks like check_condition(scene, question), where\neach question is a semantically meaningful prompt about the scene (e.g., \"Is\nthe character in danger?\") discriminated by the role-playing LLM as true,\nfalse, or unknown. This explicit representation offers three key advantages\nover traditional prompt-based profiles, which append character descriptions\ndirectly into text prompts: (1) Persistence, by enforcing complete and\nconsistent execution of character logic, rather than relying on the model's\nimplicit reasoning; (2) Updatability, through systematic inspection and\nrevision of behavioral logic, which is difficult to track or debug in\nprompt-only approaches; (3) Controllable Randomness, by supporting stochastic\nbehavior directly within the logic, enabling fine-grained variability that\nprompting alone struggles to achieve. To validate these advantages, we\nintroduce a new benchmark constructed from 83 characters and 5,141 scenes\ncurated from Fandom, using NLI-based scoring to compare character responses\nagainst ground-truth actions. Our experiments demonstrate the significant\nbenefits of codified profiles in improving persistence, updatability, and\nbehavioral diversity. Notably, by offloading a significant portion of reasoning\nto preprocessing, codified profiles enable even 1B-parameter models to perform\nhigh-quality role-playing, providing a scalable and efficient foundation for\nlocal deployment of role-play agents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCodified Profiles\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u89d2\u8272\u903b\u8f91\u8868\u793a\u4e3a\u7ed3\u6784\u5316\u3001\u53ef\u6267\u884c\u7684\u51fd\u6570\u6765\u6539\u8fdb\u89d2\u8272\u626e\u6f14\u4e2d\u7684\u51b3\u7b56\u884c\u4e3a\u3002\u76f8\u6bd4\u4f20\u7edf\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5177\u6709\u6301\u4e45\u6027\u3001\u53ef\u66f4\u65b0\u6027\u548c\u53ef\u63a7\u968f\u673a\u6027\u4e09\u5927\u4f18\u52bf\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u63d0\u793a\u7684\u89d2\u8272\u626e\u6f14\u65b9\u6cd5\u4f9d\u8d56\u6a21\u578b\u7684\u9690\u5f0f\u63a8\u7406\uff0c\u96be\u4ee5\u4fdd\u6301\u903b\u8f91\u7684\u4e00\u81f4\u6027\u548c\u53ef\u66f4\u65b0\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u6784\u5316\u3001\u53ef\u6267\u884c\u7684\u51fd\u6570\u663e\u5f0f\u8868\u793a\u89d2\u8272\u903b\u8f91\uff0c\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u89d2\u8272\u903b\u8f91\u7f16\u7801\u4e3a\u51fd\u6570\uff0c\u5982`parse_by_scene(scene)`\u548c`check_condition(scene, question)`\uff0c\u5229\u7528\u663e\u5f0f\u63a7\u5236\u7ed3\u6784\u548c\u6761\u4ef6\u68c0\u67e5\u751f\u6210\u903b\u8f91\u9a71\u52a8\u7684\u884c\u4e3a\u65ad\u8a00\u3002", "result": "\u5b9e\u9a8c\u57fa\u4e8e83\u4e2a\u89d2\u8272\u548c5,141\u4e2a\u573a\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660eCodified Profiles\u663e\u8457\u63d0\u5347\u4e86\u6301\u4e45\u6027\u3001\u53ef\u66f4\u65b0\u6027\u548c\u884c\u4e3a\u591a\u6837\u6027\uff0c\u751a\u81f3\u4f7f1B\u53c2\u6570\u6a21\u578b\u4e5f\u80fd\u9ad8\u6548\u5b9e\u73b0\u9ad8\u8d28\u91cf\u89d2\u8272\u626e\u6f14\u3002", "conclusion": "Codified Profiles\u4e3a\u89d2\u8272\u626e\u6f14\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u672c\u5730\u90e8\u7f72\u65b9\u6848\uff0c\u901a\u8fc7\u663e\u5f0f\u903b\u8f91\u8868\u793a\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.06534", "pdf": "https://arxiv.org/pdf/2505.06534", "abs": "https://arxiv.org/abs/2505.06534", "authors": ["Ummay Maria Muna", "Fahim Hafiz", "Shanta Biswas", "Riasat Azim"], "title": "GBDTSVM: Combined Support Vector Machine and Gradient Boosting Decision Tree Framework for efficient snoRNA-disease association prediction", "categories": ["cs.LG", "q-bio.QM"], "comment": "30 pages, 3 figures", "summary": "Small nucleolar RNAs (snoRNAs) are increasingly recognized for their critical\nrole in the pathogenesis and characterization of various human diseases.\nConsequently, the precise identification of snoRNA-disease associations (SDAs)\nis essential for the progression of diseases and the advancement of treatment\nstrategies. However, conventional biological experimental approaches are\ncostly, time-consuming, and resource-intensive; therefore, machine\nlearning-based computational methods offer a promising solution to mitigate\nthese limitations. This paper proposes a model called 'GBDTSVM', representing a\nnovel and efficient machine learning approach for predicting snoRNA-disease\nassociations by leveraging a Gradient Boosting Decision Tree (GBDT) and Support\nVector Machine (SVM). 'GBDTSVM' effectively extracts integrated snoRNA-disease\nfeature representations utilizing GBDT and SVM is subsequently utilized to\nclassify and identify potential associations. Furthermore, the method enhances\nthe accuracy of these predictions by incorporating Gaussian kernel profile\nsimilarity for both snoRNAs and diseases. Experimental evaluation of the\nGBDTSVM model demonstrated superior performance compared to state-of-the-art\nmethods in the field, achieving an area under the receiver operating\ncharacteristic (AUROC) of 0.96 and an area under the precision-recall curve\n(AUPRC) of 0.95 on MDRF dataset. Moreover, our model shows superior performance\non two more datasets named LSGT and PsnoD. Additionally, a case study on the\npredicted snoRNA-disease associations verified the top 10 predicted snoRNAs\nacross nine prevalent diseases, further validating the efficacy of the GBDTSVM\napproach. These results underscore the model's potential as a robust tool for\nadvancing snoRNA-related disease research. Source codes and datasets our\nproposed framework can be obtained from: https://github.com/mariamuna04/gbdtsvm", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGBDTSVM\u7684\u65b0\u578b\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u68af\u5ea6\u63d0\u5347\u51b3\u7b56\u6811\uff08GBDT\uff09\u548c\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u6765\u9884\u6d4bsnoRNA-\u75be\u75c5\u5173\u8054\uff08SDAs\uff09\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cAUROC\u548cAUPRC\u5206\u522b\u8fbe\u52300.96\u548c0.95\u3002", "motivation": "\u4f20\u7edf\u7684\u751f\u7269\u5b9e\u9a8c\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\u4e14\u8d44\u6e90\u5bc6\u96c6\uff0c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4e3asnoRNA-\u75be\u75c5\u5173\u8054\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "GBDTSVM\u7ed3\u5408GBDT\u63d0\u53d6\u7279\u5f81\u8868\u793a\uff0c\u518d\u7528SVM\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5f15\u5165\u9ad8\u65af\u6838\u8f6e\u5ed3\u76f8\u4f3c\u6027\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u3002", "result": "\u6a21\u578b\u5728MDRF\u3001LSGT\u548cPsnoD\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cAUROC\uff080.96\uff09\u548cAUPRC\uff080.95\uff09\u4f18\u5f02\uff0c\u6848\u4f8b\u5206\u6790\u9a8c\u8bc1\u4e86\u524d\u5341\u9884\u6d4bsnoRNA\u7684\u51c6\u786e\u6027\u3002", "conclusion": "GBDTSVM\u662f\u4e00\u79cd\u6709\u6548\u7684\u5de5\u5177\uff0c\u53ef\u63a8\u52a8snoRNA\u76f8\u5173\u75be\u75c5\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.07731", "pdf": "https://arxiv.org/pdf/2505.07731", "abs": "https://arxiv.org/abs/2505.07731", "authors": ["Neeraj Agrawal", "Sriram Ganapathy"], "title": "Spoken Language Understanding on Unseen Tasks With In-Context Learning", "categories": ["cs.CL", "cs.LG", "eess.AS"], "comment": null, "summary": "Spoken language understanding (SLU) tasks involve diverse skills that probe\nthe information extraction, classification and/or generation capabilities of\nmodels. In this setting, task-specific training data may not always be\navailable. While traditional task-specific SLU models are unable to cater to\nsuch requirements, the speech-text large language models (LLMs) offer a\npromising alternative with emergent abilities. However, out of-the-box, our\nevaluations indicate that the zero/few-shot performance of prominent\nopen-source speech-text LLMs on SLU tasks are not up to the mark. In this\npaper, we introduce a novel approach to robust task-agnostic fine-tuning using\nrandomized class labels. With this proposed fine-tuning, we illustrate that the\nperformance of the speech-text LLMs on an unseen task is significantly improved\nover standard approaches. Critically, the proposed approach avoids the\nrequirement of task-specific data annotations for enabling new tasks in\nspeech-text LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8bed\u97f3\u6587\u672c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u9c81\u68d2\u6027\u4efb\u52a1\u65e0\u5173\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u968f\u673a\u5316\u7c7b\u522b\u6807\u7b7e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u6570\u636e\u6807\u6ce8\u3002", "motivation": "\u4f20\u7edf\u4efb\u52a1\u7279\u5b9a\u7684\u53e3\u8bed\u7406\u89e3\uff08SLU\uff09\u6a21\u578b\u65e0\u6cd5\u5e94\u5bf9\u7f3a\u4e4f\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\uff0c\u800c\u73b0\u6709\u7684\u5f00\u6e90\u8bed\u97f3\u6587\u672cLLMs\u5728\u96f6/\u5c11\u6837\u672c\u5b66\u4e60\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u4e0d\u8db3\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9c81\u68d2\u6027\u4efb\u52a1\u65e0\u5173\u5fae\u8c03\u65b9\u6cd5\uff0c\u4f7f\u7528\u968f\u673a\u5316\u7c7b\u522b\u6807\u7b7e\u5bf9\u8bed\u97f3\u6587\u672cLLMs\u8fdb\u884c\u8bad\u7ec3\uff0c\u907f\u514d\u4e86\u5bf9\u4efb\u52a1\u7279\u5b9a\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u989d\u5916\u7684\u4efb\u52a1\u7279\u5b9a\u6570\u636e\u6807\u6ce8\u3002", "conclusion": "\u968f\u673a\u5316\u7c7b\u522b\u6807\u7b7e\u7684\u5fae\u8c03\u65b9\u6cd5\u4e3a\u8bed\u97f3\u6587\u672cLLMs\u5728\u591a\u6837\u5316SLU\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u4efb\u52a1\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.06542", "pdf": "https://arxiv.org/pdf/2505.06542", "abs": "https://arxiv.org/abs/2505.06542", "authors": ["Ad\u00e8le H. Ribeiro", "Dominik Heider"], "title": "dcFCI: Robust Causal Discovery Under Latent Confounding, Unfaithfulness, and Mixed Data", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "31 pages. This work has been submitted to the IEEE for possible\n  publication", "summary": "Causal discovery is central to inferring causal relationships from\nobservational data. In the presence of latent confounding, algorithms such as\nFast Causal Inference (FCI) learn a Partial Ancestral Graph (PAG) representing\nthe true model's Markov Equivalence Class. However, their correctness\ncritically depends on empirical faithfulness, the assumption that observed\n(in)dependencies perfectly reflect those of the underlying causal model, which\noften fails in practice due to limited sample sizes. To address this, we\nintroduce the first nonparametric score to assess a PAG's compatibility with\nobserved data, even with mixed variable types. This score is both necessary and\nsufficient to characterize structural uncertainty and distinguish between\ndistinct PAGs. We then propose data-compatible FCI (dcFCI), the first hybrid\ncausal discovery algorithm to jointly address latent confounding, empirical\nunfaithfulness, and mixed data types. dcFCI integrates our score into an\n(Anytime)FCI-guided search that systematically explores, ranks, and validates\ncandidate PAGs. Experiments on synthetic and real-world scenarios demonstrate\nthat dcFCI significantly outperforms state-of-the-art methods, often recovering\nthe true PAG even in small and heterogeneous datasets. Examining top-ranked\nPAGs further provides valuable insights into structural uncertainty, supporting\nmore robust and informed causal reasoning and decision-making.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u975e\u53c2\u6570\u8bc4\u5206\u65b9\u6cd5dcFCI\uff0c\u7528\u4e8e\u8bc4\u4f30\u6f5c\u5728\u6df7\u6742\u3001\u7ecf\u9a8c\u975e\u5fe0\u5b9e\u6027\u548c\u6df7\u5408\u6570\u636e\u7c7b\u578b\u4e0b\u7684PAG\u517c\u5bb9\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u56e0\u679c\u53d1\u73b0\u4e2d\u56e0\u6837\u672c\u91cf\u9650\u5236\u5bfc\u81f4\u7684\u7ecf\u9a8c\u975e\u5fe0\u5b9e\u6027\u95ee\u9898\uff0c\u4ee5\u53ca\u6f5c\u5728\u6df7\u6742\u548c\u6df7\u5408\u6570\u636e\u7c7b\u578b\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u975e\u53c2\u6570\u8bc4\u5206\u4e0eAnytimeFCI\u5f15\u5bfc\u7684\u641c\u7d22\uff0c\u7cfb\u7edf\u8bc4\u4f30\u548c\u9a8c\u8bc1\u5019\u9009PAG\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e2d\uff0cdcFCI\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u6062\u590d\u771f\u5b9ePAG\u3002", "conclusion": "dcFCI\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u7ed3\u6784\u4e0d\u786e\u5b9a\u6027\u548c\u66f4\u7a33\u5065\u7684\u56e0\u679c\u63a8\u7406\u652f\u6301\u3002"}}
{"id": "2505.06261", "pdf": "https://arxiv.org/pdf/2505.06261", "abs": "https://arxiv.org/abs/2505.06261", "authors": ["Wei Meng"], "title": "Modeling supply chain compliance response strategies based on AI synthetic data with structural path regression: A Simulation Study of EU 2027 Mandatory Labor Regulations", "categories": ["cs.CY", "cs.AI", "stat.AP", "90B06 (Primary) 62J05, 91B74 (Secondary)", "I.6.3; I.2.6; J.1"], "comment": "Simulated data modeling of the impact of non-tariff barriers in trade\n  wars", "summary": "In the context of the new mandatory labor compliance in the European Union\n(EU), which will be implemented in 2027, supply chain enterprises face\nstringent working hour management requirements and compliance risks. In order\nto scientifically predict the enterprises' coping behaviors and performance\noutcomes under the policy impact, this paper constructs a methodological\nframework that integrates the AI synthetic data generation mechanism and\nstructural path regression modeling to simulate the enterprises' strategic\ntransition paths under the new regulations. In terms of research methodology,\nthis paper adopts high-quality simulation data generated based on Monte Carlo\nmechanism and NIST synthetic data standards to construct a structural path\nanalysis model that includes multiple linear regression, logistic regression,\nmediation effect and moderating effect. The variable system covers 14\nindicators such as enterprise working hours, compliance investment, response\nspeed, automation level, policy dependence, etc. The variable set with\nexplanatory power is screened out through exploratory data analysis (EDA) and\nVIF multicollinearity elimination. The findings show that compliance investment\nhas a significant positive impact on firm survival and its effect is\ntransmitted through the mediating path of the level of intelligence; meanwhile,\nfirms' dependence on the EU market significantly moderates the strength of this\nmediating effect. It is concluded that AI synthetic data combined with\nstructural path modeling provides an effective tool for high-intensity\nregulatory simulation, which can provide a quantitative basis for corporate\nstrategic response, policy design and AI-assisted decision-making in the\npre-prediction stage lacking real scenario data. Keywords: AI synthetic data,\nstructural path regression modeling, compliance response strategy, EU 2027\nmandatory labor regulation", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u7ed3\u5408AI\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u7ed3\u6784\u8def\u5f84\u56de\u5f52\u5efa\u6a21\u7684\u65b9\u6cd5\u6846\u67b6\uff0c\u6a21\u62df\u4f01\u4e1a\u5728\u6b27\u76df2027\u5e74\u65b0\u52b3\u52a8\u6cd5\u89c4\u4e0b\u7684\u6218\u7565\u8f6c\u578b\u8def\u5f84\u3002\u7814\u7a76\u663e\u793a\u5408\u89c4\u6295\u8d44\u5bf9\u4f01\u4e1a\u751f\u5b58\u6709\u663e\u8457\u6b63\u5411\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u667a\u80fd\u5316\u6c34\u5e73\u7684\u4e2d\u4ecb\u8def\u5f84\u4f20\u9012\uff0c\u540c\u65f6\u4f01\u4e1a\u5bf9\u6b27\u76df\u5e02\u573a\u7684\u4f9d\u8d56\u6027\u8c03\u8282\u4e86\u8fd9\u4e00\u4e2d\u4ecb\u6548\u5e94\u7684\u5f3a\u5ea6\u3002", "motivation": "\u6b27\u76df2027\u5e74\u5c06\u5b9e\u65bd\u5f3a\u5236\u6027\u52b3\u52a8\u5408\u89c4\u653f\u7b56\uff0c\u4f9b\u5e94\u94fe\u4f01\u4e1a\u9762\u4e34\u4e25\u683c\u7684\u5de5\u65f6\u7ba1\u7406\u548c\u5408\u89c4\u98ce\u9669\u3002\u4e3a\u79d1\u5b66\u9884\u6d4b\u4f01\u4e1a\u5e94\u5bf9\u884c\u4e3a\u53ca\u653f\u7b56\u5f71\u54cd\u4e0b\u7684\u7ee9\u6548\u7ed3\u679c\uff0c\u7814\u7a76\u9700\u8981\u6a21\u62df\u653f\u7b56\u5f71\u54cd\u4e0b\u7684\u4f01\u4e1a\u6218\u7565\u8f6c\u578b\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u673a\u5236\u548cNIST\u5408\u6210\u6570\u636e\u6807\u51c6\u7684\u9ad8\u8d28\u91cf\u4eff\u771f\u6570\u636e\uff0c\u6784\u5efa\u5305\u542b\u591a\u5143\u7ebf\u6027\u56de\u5f52\u3001\u903b\u8f91\u56de\u5f52\u3001\u4e2d\u4ecb\u6548\u5e94\u4e0e\u8c03\u8282\u6548\u5e94\u7684\u7ed3\u6784\u8def\u5f84\u5206\u6790\u6a21\u578b\uff0c\u7b5b\u900914\u4e2a\u53d8\u91cf\u7684\u89e3\u91ca\u6027\u96c6\u3002", "result": "\u5408\u89c4\u6295\u8d44\u901a\u8fc7\u667a\u80fd\u5316\u6c34\u5e73\u7684\u4e2d\u4ecb\u8def\u5f84\u663e\u8457\u63d0\u5347\u4f01\u4e1a\u751f\u5b58\u6982\u7387\uff0c\u800c\u4f01\u4e1a\u5bf9\u6b27\u76df\u5e02\u573a\u7684\u4f9d\u8d56\u6027\u8c03\u8282\u4e86\u4e2d\u4ecb\u6548\u5e94\u7684\u5f3a\u5ea6\u3002AI\u5408\u6210\u6570\u636e\u4e0e\u7ed3\u6784\u8def\u5f84\u5efa\u6a21\u5728\u7f3a\u4e4f\u771f\u5b9e\u573a\u666f\u6570\u636e\u7684\u9636\u6bb5\u4e3a\u6218\u7565\u51b3\u7b56\u63d0\u4f9b\u4e86\u91cf\u5316\u4f9d\u636e\u3002", "conclusion": "AI\u5408\u6210\u6570\u636e\u7ed3\u5408\u7ed3\u6784\u8def\u5f84\u5efa\u6a21\u662f\u9ad8\u5f3a\u5ea6\u76d1\u7ba1\u6a21\u62df\u7684\u6709\u6548\u5de5\u5177\uff0c\u80fd\u4e3a\u4f01\u4e1a\u6218\u7565\u5e94\u5bf9\u3001\u653f\u7b56\u8bbe\u8ba1\u548cAI\u8f85\u52a9\u51b3\u7b56\u63d0\u4f9b\u91cf\u5316\u652f\u6301\u3002"}}
{"id": "2505.07775", "pdf": "https://arxiv.org/pdf/2505.07775", "abs": "https://arxiv.org/abs/2505.07775", "authors": ["Nimet Beyza Bozdag", "Shuhaib Mehri", "Xiaocheng Yang", "Hyeonjeong Ha", "Zirui Cheng", "Esin Durmus", "Jiaxuan You", "Heng Ji", "Gokhan Tur", "Dilek Hakkani-T\u00fcr"], "title": "Must Read: A Systematic Survey of Computational Persuasion", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Persuasion is a fundamental aspect of communication, influencing\ndecision-making across diverse contexts, from everyday conversations to\nhigh-stakes scenarios such as politics, marketing, and law. The rise of\nconversational AI systems has significantly expanded the scope of persuasion,\nintroducing both opportunities and risks. AI-driven persuasion can be leveraged\nfor beneficial applications, but also poses threats through manipulation and\nunethical influence. Moreover, AI systems are not only persuaders, but also\nsusceptible to persuasion, making them vulnerable to adversarial attacks and\nbias reinforcement. Despite rapid advancements in AI-generated persuasive\ncontent, our understanding of what makes persuasion effective remains limited\ndue to its inherently subjective and context-dependent nature. In this survey,\nwe provide a comprehensive overview of computational persuasion, structured\naround three key perspectives: (1) AI as a Persuader, which explores\nAI-generated persuasive content and its applications; (2) AI as a Persuadee,\nwhich examines AI's susceptibility to influence and manipulation; and (3) AI as\na Persuasion Judge, which analyzes AI's role in evaluating persuasive\nstrategies, detecting manipulation, and ensuring ethical persuasion. We\nintroduce a taxonomy for computational persuasion research and discuss key\nchallenges, including evaluating persuasiveness, mitigating manipulative\npersuasion, and developing responsible AI-driven persuasive systems. Our survey\noutlines future research directions to enhance the safety, fairness, and\neffectiveness of AI-powered persuasion while addressing the risks posed by\nincreasingly capable language models.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u8ba1\u7b97\u8bf4\u670d\u7684\u4e09\u5927\u89c6\u89d2\uff1aAI\u4f5c\u4e3a\u8bf4\u670d\u8005\u3001\u88ab\u8bf4\u670d\u8005\u548c\u8bc4\u5224\u8005\uff0c\u63a2\u8ba8\u4e86AI\u751f\u6210\u8bf4\u670d\u5185\u5bb9\u3001\u6613\u53d7\u64cd\u7eb5\u6027\u53ca\u4f26\u7406\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740AI\u9a71\u52a8\u7684\u8bf4\u670d\u6280\u672f\u5feb\u901f\u53d1\u5c55\uff0c\u5176\u6f5c\u5728\u76ca\u5904\u4e0e\u98ce\u9669\u5e76\u5b58\uff0c\u4f46\u76ee\u524d\u5bf9\u5176\u6548\u679c\u7684\u7406\u89e3\u4ecd\u53d7\u9650\u4e8e\u4e3b\u89c2\u6027\u548c\u60c5\u666f\u4f9d\u8d56\u6027\uff0c\u4e9f\u9700\u7cfb\u7edf\u68b3\u7406\u3002", "method": "\u901a\u8fc7\u4e09\u5927\u89c6\u89d2\uff08AI\u4f5c\u4e3a\u8bf4\u670d\u8005\u3001\u88ab\u8bf4\u670d\u8005\u3001\u8bc4\u5224\u8005\uff09\u6784\u5efa\u5206\u7c7b\u6cd5\uff0c\u7efc\u8ff0\u73b0\u6709\u7814\u7a76\uff0c\u5e76\u5206\u6790\u5173\u952e\u6280\u672f\u6311\u6218\u3002", "result": "\u63d0\u51fa\u4e86\u8ba1\u7b97\u8bf4\u670d\u7814\u7a76\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u660e\u786e\u4e86\u8bc4\u4f30\u8bf4\u670d\u529b\u3001\u9632\u6b62\u64cd\u7eb5\u6027\u8bf4\u670d\u7b49\u6838\u5fc3\u6311\u6218\u3002", "conclusion": "\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u786e\u4fddAI\u8bf4\u670d\u6280\u672f\u7684\u5b89\u5168\u6027\u3001\u516c\u5e73\u6027\u548c\u6709\u6548\u6027\uff0c\u540c\u65f6\u5e94\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u98ce\u9669\u3002"}}
{"id": "2505.06549", "pdf": "https://arxiv.org/pdf/2505.06549", "abs": "https://arxiv.org/abs/2505.06549", "authors": ["Matthias Chung", "Bas Peters", "Michael Solomon"], "title": "Good Things Come in Pairs: Paired Autoencoders for Inverse Problems", "categories": ["cs.LG", "stat.ML", "68T99"], "comment": "43 pages, 17 figures", "summary": "In this book chapter, we discuss recent advances in data-driven approaches\nfor inverse problems. In particular, we focus on the \\emph{paired autoencoder}\nframework, which has proven to be a powerful tool for solving inverse problems\nin scientific computing. The paired autoencoder framework is a novel approach\nthat leverages the strengths of both data-driven and model-based methods by\nprojecting both the data and the quantity of interest into a latent space and\nmapping these latent spaces to provide surrogate forward and inverse mappings.\nWe illustrate the advantages of this approach through numerical experiments,\nincluding seismic imaging and classical inpainting: nonlinear and linear\ninverse problems, respectively. Although the paired autoencoder framework is\nlikelihood-free, it generates multiple data- and model-based reconstruction\nmetrics that help assess whether examples are in or out of distribution. In\naddition to direct model estimates from data, the paired autoencoder enables\nlatent-space refinement to fit the observed data accurately. Numerical\nexperiments show that this procedure, combined with the latent-space initial\nguess, is essential for high-quality estimates, even when data noise exceeds\nthe training regime. We also introduce two novel variants that combine\nvariational and paired autoencoder ideas, maintaining the original benefits\nwhile enabling sampling for uncertainty analysis.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u914d\u5bf9\u81ea\u7f16\u7801\u5668\u201d\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u79d1\u5b66\u8ba1\u7b97\u4e2d\u7684\u53cd\u95ee\u9898\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u6620\u5c04\u548c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u566a\u58f0\u6570\u636e\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u9ad8\u8d28\u91cf\u4f30\u8ba1\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u548c\u6a21\u578b\u9a71\u52a8\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u4ee5\u89e3\u51b3\u79d1\u5b66\u8ba1\u7b97\u4e2d\u7684\u53cd\u95ee\u9898\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u975e\u7ebf\u6027\u53cd\u95ee\u9898\uff08\u5982\u5730\u9707\u6210\u50cf\uff09\u548c\u7ebf\u6027\u53cd\u95ee\u9898\uff08\u5982\u7ecf\u5178\u4fee\u590d\uff09\u3002", "method": "\u63d0\u51fa\u914d\u5bf9\u81ea\u7f16\u7801\u5668\u6846\u67b6\uff0c\u5c06\u6570\u636e\u548c\u76ee\u6807\u91cf\u6295\u5f71\u5230\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u6620\u5c04\u751f\u6210\u524d\u5411\u548c\u53cd\u5411\u4ee3\u7406\u6620\u5c04\uff0c\u65e0\u9700\u4f3c\u7136\u5047\u8bbe\u3002\u540c\u65f6\u5f15\u5165\u53d8\u5206\u601d\u60f3\u6269\u5c55\u529f\u80fd\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u566a\u58f0\u8d85\u8fc7\u8bad\u7ec3\u8303\u56f4\u65f6\u4ecd\u80fd\u63d0\u4f9b\u9ad8\u8d28\u91cf\u4f30\u8ba1\uff0c\u5e76\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u7ec6\u5316\u548c\u521d\u59cb\u731c\u6d4b\u63d0\u5347\u6027\u80fd\u3002\u53d8\u5206\u6269\u5c55\u7248\u8fd8\u652f\u6301\u4e0d\u786e\u5b9a\u6027\u5206\u6790\u91c7\u6837\u3002", "conclusion": "\u914d\u5bf9\u81ea\u7f16\u7801\u5668\u7ed3\u5408\u6570\u636e\u4e0e\u6a21\u578b\u4f18\u52bf\uff0c\u4e3a\u53cd\u95ee\u9898\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u53d8\u79cd\u8fdb\u4e00\u6b65\u6269\u5c55\u4e86\u4e0d\u786e\u5b9a\u6027\u5206\u6790\u80fd\u529b\u3002"}}
{"id": "2505.07784", "pdf": "https://arxiv.org/pdf/2505.07784", "abs": "https://arxiv.org/abs/2505.07784", "authors": ["Da Ju", "Hagen Blix", "Adina Williams"], "title": "Domain Regeneration: How well do LLMs match syntactic properties of text domains?", "categories": ["cs.CL"], "comment": null, "summary": "Recent improvement in large language model performance have, in all\nlikelihood, been accompanied by improvement in how well they can approximate\nthe distribution of their training data. In this work, we explore the following\nquestion: which properties of text domains do LLMs faithfully approximate, and\nhow well do they do so? Applying observational approaches familiar from corpus\nlinguistics, we prompt a commonly used, opensource LLM to regenerate text from\ntwo domains of permissively licensed English text which are often contained in\nLLM training data -- Wikipedia and news text. This regeneration paradigm allows\nus to investigate whether LLMs can faithfully match the original human text\ndomains in a fairly semantically-controlled setting. We investigate varying\nlevels of syntactic abstraction, from more simple properties like sentence\nlength, and article readability, to more complex and higher order properties\nsuch as dependency tag distribution, parse depth, and parse complexity. We find\nthat the majority of the regenerated distributions show a shifted mean, a lower\nstandard deviation, and a reduction of the long tail, as compared to the human\noriginals.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u751f\u6210\u6587\u672c\u65f6\u662f\u5426\u80fd\u5fe0\u5b9e\u8fd1\u4f3c\u5176\u8bad\u7ec3\u6570\u636e\u7684\u5206\u5e03\u7279\u6027\uff0c\u901a\u8fc7\u5bf9Wikipedia\u548c\u65b0\u95fb\u6587\u672c\u7684\u518d\u751f\u5b9e\u9a8c\u53d1\u73b0\uff0cLLM\u751f\u6210\u7684\u6587\u672c\u5728\u5747\u503c\u3001\u6807\u51c6\u5dee\u548c\u957f\u5c3e\u5206\u5e03\u4e0a\u4e0e\u539f\u59cb\u4eba\u7c7b\u6587\u672c\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e86\u89e3LLM\u5728\u751f\u6210\u6587\u672c\u65f6\u80fd\u591a\u597d\u5730\u8fd1\u4f3c\u5176\u8bad\u7ec3\u6570\u636e\u7684\u5206\u5e03\u7279\u6027\uff0c\u5c24\u5176\u662f\u4e0d\u540c\u6587\u672c\u9886\u57df\u7684\u8bed\u8a00\u5b66\u5c5e\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4eceWikipedia\u548c\u65b0\u95fb\u6587\u672c\u4e2d\u63d0\u53d6\u8bed\u6599\uff0c\u901a\u8fc7LLM\u518d\u751f\u6587\u672c\uff0c\u5e76\u5bf9\u6bd4\u5206\u6790\u8bed\u6cd5\u62bd\u8c61\u5c42\u6b21\uff08\u5982\u53e5\u5b50\u957f\u5ea6\u3001\u53ef\u8bfb\u6027\uff09\u548c\u590d\u6742\u5c5e\u6027\uff08\u5982\u4f9d\u5b58\u6807\u7b7e\u5206\u5e03\u3001\u53e5\u6cd5\u6df1\u5ea6\uff09\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cLLM\u751f\u6210\u7684\u6587\u672c\u5728\u591a\u6570\u5206\u5e03\u4e2d\u8868\u73b0\u51fa\u5747\u503c\u504f\u79fb\u3001\u6807\u51c6\u5dee\u964d\u4f4e\u548c\u957f\u5c3e\u51cf\u5c11\u7684\u73b0\u8c61\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51faLLM\u5728\u751f\u6210\u6587\u672c\u65f6\u867d\u7136\u80fd\u90e8\u5206\u8fd1\u4f3c\u4eba\u7c7b\u6587\u672c\uff0c\u4f46\u5728\u5206\u5e03\u7279\u6027\u4e0a\u4ecd\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002"}}
{"id": "2505.06581", "pdf": "https://arxiv.org/pdf/2505.06581", "abs": "https://arxiv.org/abs/2505.06581", "authors": ["Chao Yan"], "title": "An \\tilde{O}ptimal Differentially Private Learner for Concept Classes with VC Dimension 1", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "We present the first nearly optimal differentially private PAC learner for\nany concept class with VC dimension 1 and Littlestone dimension $d$. Our\nalgorithm achieves the sample complexity of\n$\\tilde{O}_{\\varepsilon,\\delta,\\alpha,\\delta}(\\log^* d)$, nearly matching the\nlower bound of $\\Omega(\\log^* d)$ proved by Alon et al. [STOC19]. Prior to our\nwork, the best known upper bound is $\\tilde{O}(VC\\cdot d^5)$ for general VC\nclasses, as shown by Ghazi et al. [STOC21].", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9VC\u7ef4\u5ea6\u4e3a1\u3001Littlestone\u7ef4\u5ea6\u4e3a$d$\u7684\u6982\u5ff5\u7c7b\u7684\u8fd1\u4e4e\u6700\u4f18\u7684\u5dee\u5206\u9690\u79c1PAC\u5b66\u4e60\u7b97\u6cd5\uff0c\u6837\u672c\u590d\u6742\u5ea6\u4e3a$\tilde{O}_{\\varepsilon,\\delta,\\alpha,\\delta}(\\log^* d)$\uff0c\u63a5\u8fd1Alon\u7b49\u4eba\u7684\u4e0b\u754c\u3002", "motivation": "\u73b0\u6709\u5dee\u5206\u9690\u79c1PAC\u5b66\u4e60\u7b97\u6cd5\u5bf9\u4e00\u822cVC\u7c7b\u7684\u6837\u672c\u590d\u6742\u5ea6\u4e0a\u9650\u4e3a$\tilde{O}(VC\\cdot d^5)$\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\u6765\u5904\u7406\u7279\u5b9a\u6982\u5ff5\u7c7b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5dee\u5206\u9690\u79c1PAC\u5b66\u4e60\u7b97\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9VC\u7ef4\u5ea6\u4e3a1\u3001Littlestone\u7ef4\u5ea6\u4e3a$d$\u7684\u6982\u5ff5\u7c7b\u3002", "result": "\u7b97\u6cd5\u7684\u6837\u672c\u590d\u6742\u5ea6\u4e3a$\tilde{O}_{\\varepsilon,\\delta,\\alpha,\\delta}(\\log^* d)$\uff0c\u63a5\u8fd1\u5df2\u77e5\u4e0b\u754c\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u7279\u5b9a\u6982\u5ff5\u7c7b\u4e0a\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u6700\u4f18\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4e00\u822cVC\u7c7b\u7b97\u6cd5\u7684\u4e0a\u9650\u3002"}}
{"id": "2505.06264", "pdf": "https://arxiv.org/pdf/2505.06264", "abs": "https://arxiv.org/abs/2505.06264", "authors": ["Santhakumar Ramamoorthy", "Priya Rani", "James Mahon", "Glenn Mathews", "Shaun Cloherty", "Mahdi Babaei"], "title": "Prediction of Delirium Risk in Mild Cognitive Impairment Using Time-Series data, Machine Learning and Comorbidity Patterns -- A Retrospective Study", "categories": ["stat.AP", "cs.AI"], "comment": null, "summary": "Delirium represents a significant clinical concern characterized by high\nmorbidity and mortality rates, particularly in patients with mild cognitive\nimpairment (MCI). This study investigates the associated risk factors for\ndelirium by analyzing the comorbidity patterns relevant to MCI and developing a\nlongitudinal predictive model leveraging machine learning methodologies. A\nretrospective analysis utilizing the MIMIC-IV v2.2 database was performed to\nevaluate comorbid conditions, survival probabilities, and predictive modeling\noutcomes. The examination of comorbidity patterns identified distinct risk\nprofiles for the MCI population. Kaplan-Meier survival analysis demonstrated\nthat individuals with MCI exhibit markedly reduced survival probabilities when\ndeveloping delirium compared to their non-MCI counterparts, underscoring the\nheightened vulnerability within this cohort. For predictive modeling, a Long\nShort-Term Memory (LSTM) ML network was implemented utilizing time-series data,\ndemographic variables, Charlson Comorbidity Index (CCI) scores, and an array of\ncomorbid conditions. The model demonstrated robust predictive capabilities with\nan AUROC of 0.93 and an AUPRC of 0.92. This study underscores the critical role\nof comorbidities in evaluating delirium risk and highlights the efficacy of\ntime-series predictive modeling in pinpointing patients at elevated risk for\ndelirium development.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5206\u6790\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\uff08MCI\uff09\u60a3\u8005\u7684\u5171\u75c5\u6a21\u5f0f\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u5f00\u53d1\u4e86\u9884\u6d4b\u8c35\u5984\u98ce\u9669\u7684LSTM\u6a21\u578b\uff0cAUROC\u8fbe0.93\u3002", "motivation": "\u8c35\u5984\u5728MCI\u60a3\u8005\u4e2d\u53d1\u75c5\u7387\u548c\u6b7b\u4ea1\u7387\u9ad8\uff0c\u4f46\u76f8\u5173\u98ce\u9669\u56e0\u7d20\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u5f00\u53d1\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u5229\u7528MIMIC-IV v2.2\u6570\u636e\u5e93\u8fdb\u884c\u56de\u987e\u6027\u5206\u6790\uff0c\u7ed3\u5408Kaplan-Meier\u751f\u5b58\u5206\u6790\u548cLSTM\u6a21\u578b\u9884\u6d4b\u98ce\u9669\u3002", "result": "MCI\u60a3\u8005\u8c35\u5984\u540e\u751f\u5b58\u7387\u663e\u8457\u964d\u4f4e\uff0cLSTM\u6a21\u578b\u9884\u6d4b\u6027\u80fd\u4f18\u5f02\uff08AUROC 0.93\uff0cAUPRC 0.92\uff09\u3002", "conclusion": "\u5171\u75c5\u662f\u8c35\u5984\u98ce\u9669\u7684\u5173\u952e\u56e0\u7d20\uff0c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u80fd\u6709\u6548\u8bc6\u522b\u9ad8\u98ce\u9669\u60a3\u8005\u3002"}}
{"id": "2505.07787", "pdf": "https://arxiv.org/pdf/2505.07787", "abs": "https://arxiv.org/abs/2505.07787", "authors": ["Tongxu Luo", "Wenyu Du", "Jiaxi Bi", "Stephen Chung", "Zhengyang Tang", "Hao Yang", "Min Zhang", "Benyou Wang"], "title": "Learning from Peers in Reasoning Models", "categories": ["cs.CL"], "comment": "29 pages, 32 figures", "summary": "Large Reasoning Models (LRMs) have the ability to self-correct even when they\nmake mistakes in their reasoning paths. However, our study reveals that when\nthe reasoning process starts with a short but poor beginning, it becomes\ndifficult for the model to recover. We refer to this phenomenon as the \"Prefix\nDominance Trap\". Inspired by psychological findings that peer interaction can\npromote self-correction without negatively impacting already accurate\nindividuals, we propose **Learning from Peers** (LeaP) to address this\nphenomenon. Specifically, every tokens, each reasoning path summarizes its\nintermediate reasoning and shares it with others through a routing mechanism,\nenabling paths to incorporate peer insights during inference. However, we\nobserve that smaller models sometimes fail to follow summarization and\nreflection instructions effectively. To address this, we fine-tune them into\nour **LeaP-T** model series. Experiments on AIME 2024, AIME 2025, AIMO 2025,\nand GPQA Diamond show that LeaP provides substantial improvements. For\ninstance, QwQ-32B with LeaP achieves nearly 5 absolute points higher than the\nbaseline on average, and surpasses DeepSeek-R1-671B on three math benchmarks\nwith an average gain of 3.3 points. Notably, our fine-tuned LeaP-T-7B matches\nthe performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024. In-depth analysis\nreveals LeaP's robust error correction by timely peer insights, showing strong\nerror tolerance and handling varied task difficulty. LeaP marks a milestone by\nenabling LRMs to collaborate during reasoning. Our code, datasets, and models\nare available at https://learning-from-peers.github.io/ .", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRM\uff09\u5728\u63a8\u7406\u8def\u5f84\u5f00\u5934\u4e0d\u4f73\u65f6\u96be\u4ee5\u81ea\u6211\u7ea0\u6b63\u7684\u73b0\u8c61\uff08\u79f0\u4e3a\u201c\u524d\u7f00\u4e3b\u5bfc\u9677\u9631\u201d\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5b66\u4e60\u540c\u4f34\u201d\uff08LeaP\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u540c\u884c\u4e92\u52a8\u673a\u5236\u63d0\u5347\u6a21\u578b\u81ea\u6211\u7ea0\u9519\u80fd\u529b\u3002\u5b9e\u9a8c\u8bc1\u660eLeaP\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u53d1\u73b0\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u63a8\u7406\u8def\u5f84\u5f00\u5934\u4e0d\u4f73\u65f6\u96be\u4ee5\u81ea\u6211\u7ea0\u6b63\uff08\u524d\u7f00\u4e3b\u5bfc\u9677\u9631\uff09\u3002\u53d7\u5fc3\u7406\u5b66\u4e2d\u540c\u884c\u4e92\u52a8\u4fc3\u8fdb\u81ea\u6211\u7ea0\u6b63\u7684\u542f\u53d1\uff0c\u4f5c\u8005\u65e8\u5728\u901a\u8fc7LeaP\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86LeaP\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b9a\u671f\u603b\u7ed3\u5e76\u5171\u4eab\u4e2d\u95f4\u7ed3\u679c\uff08\u8def\u7531\u673a\u5236\uff09\uff0c\u8ba9\u4e0d\u540c\u63a8\u7406\u8def\u5f84\u76f8\u4e92\u5b66\u4e60\u3002\u9488\u5bf9\u5c0f\u6a21\u578b\u96be\u4ee5\u6709\u6548\u6267\u884c\u603b\u7ed3\u4efb\u52a1\u7684\u95ee\u9898\uff0c\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u5fae\u8c03\u7248\u672cLeaP-T\u7cfb\u5217\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLeaP\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002\u4f8b\u5982\uff0cQwQ-32B + LeaP\u5728\u591a\u4e2a\u6570\u5b66\u57fa\u51c6\u4e0a\u5e73\u5747\u63d0\u53475\u4e2a\u70b9\uff0c\u751a\u81f3\u8d85\u8d8aDeepSeek-R1-671B\uff08\u5e73\u5747\u589e\u76ca3.3\u70b9\uff09\u3002LeaP-T-7B\u5728AIME 2024\u4e0a\u8868\u73b0\u4e0e\u66f4\u5927\u7684\u84b8\u998f\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "LeaP\u901a\u8fc7\u540c\u884c\u534f\u4f5c\u673a\u5236\u5b9e\u73b0\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u9ad8\u6548\u81ea\u6211\u7ea0\u9519\uff0c\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u9519\u8bef\u5bb9\u5fcd\u80fd\u529b\u548c\u4efb\u52a1\u9002\u5e94\u6027\uff0c\u6807\u5fd7\u7740\u6a21\u578b\u534f\u4f5c\u63a8\u7406\u7684\u91cd\u8981\u91cc\u7a0b\u7891\u3002"}}
{"id": "2505.06597", "pdf": "https://arxiv.org/pdf/2505.06597", "abs": "https://arxiv.org/abs/2505.06597", "authors": ["Ibrahim Talha Ersoy", "Karoline Wiesner"], "title": "Geometry of Learning -- L2 Phase Transitions in Deep and Shallow Neural Networks", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech", "physics.data-an"], "comment": null, "summary": "When neural networks (NNs) are subject to L2 regularization, increasing the\nregularization strength beyond a certain threshold pushes the model into an\nunder-parameterization regime. This transition manifests as a first-order phase\ntransition in single-hidden-layer NNs and a second-order phase transition in\nNNs with two or more hidden layers. This paper establishes a unified framework\nfor such transitions by integrating the Ricci curvature of the loss landscape\nwith regularizer-driven deep learning. First, we show that a curvature\nchange-point separates the model-accuracy regimes in the onset of learning and\nthat it is identical to the critical point of the phase transition driven by\nregularization. Second, we show that for more complex data sets additional\nphase transitions exist between model accuracies, and that they are again\nidentical to curvature change points in the error landscape. Third, by studying\nthe MNIST data set using a Variational Autoencoder, we demonstrate that the\ncurvature change points identify phase transitions in model accuracy outside\nthe L2 setting. Our framework also offers practical insights for optimizing\nmodel performance across various architectures and datasets. By linking\ngeometric features of the error landscape to observable phase transitions, our\nwork paves the way for more informed regularization strategies and potentially\nnew methods for probing the intrinsic structure of neural networks beyond the\nL2 context.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5efa\u7acb\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u635f\u5931\u666f\u89c2\u7684Ricci\u66f2\u7387\u4e0eL2\u6b63\u5219\u5316\u9a71\u52a8\u7684\u6df1\u5ea6\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u63ed\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u5728\u6b63\u5219\u5316\u5f3a\u5ea6\u589e\u52a0\u65f6\u7684\u76f8\u4f4d\u8f6c\u53d8\u73b0\u8c61\u3002\u5355\u9690\u5c42\u7f51\u7edc\u8868\u73b0\u4e3a\u4e00\u9636\u8f6c\u53d8\uff0c\u800c\u591a\u9690\u5c42\u7f51\u7edc\u5219\u4e3a\u4e8c\u9636\u8f6c\u53d8\u3002\u66f2\u7387\u53d8\u5316\u70b9\u4e0e\u6a21\u578b\u7cbe\u5ea6\u7684\u76f8\u4f4d\u8f6c\u53d8\u4e34\u754c\u70b9\u4e00\u81f4\uff0c\u9002\u7528\u4e8e\u590d\u6742\u6570\u636e\u96c6\u548cMNIST\u5b9e\u9a8c\u3002\u8be5\u6846\u67b6\u4e3a\u4f18\u5316\u6a21\u578b\u6027\u80fd\u548c\u63a2\u7d22\u795e\u7ecf\u7f51\u7edc\u5185\u5728\u7ed3\u6784\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u7406\u89e3L2\u6b63\u5219\u5316\u5f3a\u5ea6\u589e\u52a0\u65f6\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u73b0\u53d8\u5316\uff0c\u5c24\u5176\u662f\u6a21\u578b\u4ece\u8fc7\u53c2\u6570\u5316\u5230\u6b20\u53c2\u6570\u5316\u8f6c\u53d8\u7684\u76f8\u4f4d\u73b0\u8c61\uff0c\u5e76\u63a2\u7d22\u8fd9\u4e9b\u73b0\u8c61\u4e0e\u635f\u5931\u666f\u89c2\u51e0\u4f55\u7279\u5f81\u7684\u8054\u7cfb\u3002", "method": "\u65b9\u6cd5\u7ed3\u5408\u4e86Ricci\u66f2\u7387\u5206\u6790\u4e0e\u6b63\u5219\u5316\u9a71\u52a8\u7684\u6df1\u5ea6\u5b66\u4e60\uff0c\u9a8c\u8bc1\u66f2\u7387\u53d8\u5316\u70b9\u4e0e\u76f8\u4f4d\u8f6c\u53d8\u4e34\u754c\u70b9\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7MNIST\u6570\u636e\u96c6\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b9e\u9a8c\u6269\u5c55\u4e86\u6846\u67b6\u7684\u5e94\u7528\u8303\u56f4\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u66f2\u7387\u53d8\u5316\u70b9\u4e0d\u4ec5\u5bf9\u5e94\u4e8e\u6b63\u5219\u5316\u9a71\u52a8\u7684\u76f8\u4f4d\u8f6c\u53d8\u4e34\u754c\u70b9\uff0c\u8fd8\u80fd\u8bc6\u522b\u590d\u6742\u6570\u636e\u96c6\u548cMNIST\u5b9e\u9a8c\u4e2d\u7684\u6a21\u578b\u7cbe\u5ea6\u53d8\u5316\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u666e\u9002\u6027\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u4e86\u8be5\u6846\u67b6\u5728\u6307\u5bfc\u6b63\u5219\u5316\u7b56\u7565\u548c\u63a2\u7d22\u795e\u7ecf\u7f51\u7edc\u5185\u5728\u7ed3\u6784\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u4f18\u5316\u6a21\u578b\u6027\u80fd\u548c\u5f00\u53d1\u65b0\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2505.07796", "pdf": "https://arxiv.org/pdf/2505.07796", "abs": "https://arxiv.org/abs/2505.07796", "authors": ["Xingjin Wang", "Howe Tissue", "Lu Wang", "Linjing Li", "Daniel Dajun Zeng"], "title": "Learning Dynamics in Continual Pre-Training for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ICML2025 (spotlight)", "summary": "Continual Pre-Training (CPT) has become a popular and effective method to\napply strong foundation models to specific downstream tasks. In this work, we\nexplore the learning dynamics throughout the CPT process for large language\nmodels. We specifically focus on how general and downstream domain performance\nevolves at each training step, with domain performance measured via validation\nlosses. We have observed that the CPT loss curve fundamentally characterizes\nthe transition from one curve to another hidden curve, and could be described\nby decoupling the effects of distribution shift and learning rate annealing. We\nderive a CPT scaling law that combines the two factors, enabling the prediction\nof loss at any (continual) training steps and across learning rate schedules\n(LRS) in CPT. Our formulation presents a comprehensive understanding of several\ncritical factors in CPT, including loss potential, peak learning rate, training\nsteps, replay ratio, etc. Moreover, our approach can be adapted to customize\ntraining hyper-parameters to different CPT goals such as balancing general and\ndomain-specific performance. Extensive experiments demonstrate that our scaling\nlaw holds across various CPT datasets and training hyper-parameters.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u9884\u8bad\u7ec3\uff08CPT\uff09\u8fc7\u7a0b\u4e2d\u7684\u5b66\u4e60\u52a8\u6001\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u5206\u5e03\u504f\u79fb\u548c\u5b66\u4e60\u7387\u9000\u706b\u56e0\u7d20\u7684CPT\u7f29\u653e\u5b9a\u5f8b\uff0c\u7528\u4e8e\u9884\u6d4b\u4efb\u610f\u8bad\u7ec3\u6b65\u9aa4\u548c\u4e0d\u540c\u5b66\u4e60\u7387\u8ba1\u5212\u4e0b\u7684\u635f\u5931\u3002", "motivation": "\u63a2\u7d22CPT\u8fc7\u7a0b\u4e2d\u901a\u7528\u548c\u4e0b\u6e38\u9886\u57df\u6027\u80fd\u7684\u53d8\u5316\uff0c\u4ee5\u4f18\u5316\u8bad\u7ec3\u8d85\u53c2\u6570\u5e76\u5e73\u8861\u901a\u7528\u4e0e\u9886\u57df\u7279\u5b9a\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u89e3\u8026\u5206\u5e03\u504f\u79fb\u548c\u5b66\u4e60\u7387\u9000\u706b\u6548\u5e94\uff0c\u63a8\u5bfcCPT\u7f29\u653e\u5b9a\u5f8b\uff0c\u5e76\u7ed3\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u63d0\u51fa\u7684\u7f29\u653e\u5b9a\u5f8b\u5728\u4e0d\u540cCPT\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u8d85\u53c2\u6570\u4e0b\u5747\u9002\u7528\uff0c\u5e76\u80fd\u5b9a\u5236\u5316\u8bad\u7ec3\u76ee\u6807\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aCPT\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u7406\u89e3\u6846\u67b6\uff0c\u5e76\u80fd\u6307\u5bfc\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8d85\u53c2\u6570\u4f18\u5316\u3002"}}
{"id": "2505.06621", "pdf": "https://arxiv.org/pdf/2505.06621", "abs": "https://arxiv.org/abs/2505.06621", "authors": ["Thamiris Coelho", "Leo S. F. Ribeiro", "Jo\u00e3o Macedo", "Jefersson A. dos Santos", "Sandra Avila"], "title": "Minimizing Risk Through Minimizing Model-Data Interaction: A Protocol For Relying on Proxy Tasks When Designing Child Sexual Abuse Imagery Detection Models", "categories": ["cs.LG", "cs.CV"], "comment": "ACM Conference on Fairness, Accountability, and Transparency (FAccT\n  2025)", "summary": "The distribution of child sexual abuse imagery (CSAI) is an ever-growing\nconcern of our modern world; children who suffered from this heinous crime are\nrevictimized, and the growing amount of illegal imagery distributed overwhelms\nlaw enforcement agents (LEAs) with the manual labor of categorization. To ease\nthis burden researchers have explored methods for automating data triage and\ndetection of CSAI, but the sensitive nature of the data imposes restricted\naccess and minimal interaction between real data and learning algorithms,\navoiding leaks at all costs. In observing how these restrictions have shaped\nthe literature we formalize a definition of \"Proxy Tasks\", i.e., the substitute\ntasks used for training models for CSAI without making use of CSA data. Under\nthis new terminology we review current literature and present a protocol for\nmaking conscious use of Proxy Tasks together with consistent input from LEAs to\ndesign better automation in this field. Finally, we apply this protocol to\nstudy -- for the first time -- the task of Few-shot Indoor Scene Classification\non CSAI, showing a final model that achieves promising results on a real-world\nCSAI dataset whilst having no weights actually trained on sensitive data.", "AI": {"tldr": "\u8bba\u6587\u5173\u6ce8\u513f\u7ae5\u6027\u8650\u5f85\u56fe\u50cf\uff08CSAI\uff09\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u4ee3\u7406\u4efb\u52a1\u201d\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u4e0d\u76f4\u63a5\u63a5\u89e6\u654f\u611f\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u513f\u7ae5\u6027\u8650\u5f85\u56fe\u50cf\u7684\u4f20\u64ad\u65e5\u76ca\u4e25\u91cd\uff0c\u53d7\u5bb3\u8005\u4e0d\u65ad\u53d7\u5230\u4e8c\u6b21\u4f24\u5bb3\u3002\u624b\u52a8\u5206\u7c7b\u5de5\u4f5c\u91cf\u5927\u4e14\u6548\u7387\u4f4e\uff0c\u800c\u654f\u611f\u6570\u636e\u7684\u8bbf\u95ee\u9650\u5236\u4f7f\u5f97\u81ea\u52a8\u5316\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u201c\u4ee3\u7406\u4efb\u52a1\u201d\u6982\u5ff5\uff0c\u5373\u4f7f\u7528\u66ff\u4ee3\u4efb\u52a1\u8bad\u7ec3\u6a21\u578b\u800c\u4e0d\u76f4\u63a5\u4f7f\u7528CSAI\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u6267\u6cd5\u673a\u6784\u7684\u53cd\u9988\u8bbe\u8ba1\u66f4\u4f18\u7684\u81ea\u52a8\u5316\u65b9\u6848\u3002", "result": "\u9996\u6b21\u5c06\u5c11\u6837\u672c\u5ba4\u5185\u573a\u666f\u5206\u7c7b\u4efb\u52a1\u5e94\u7528\u4e8eCSAI\u68c0\u6d4b\uff0c\u6a21\u578b\u5728\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4e14\u65e0\u9700\u654f\u611f\u6570\u636e\u8bad\u7ec3\u6743\u91cd\u3002", "conclusion": "\u4ee3\u7406\u4efb\u52a1\u662f\u89e3\u51b3CSAI\u68c0\u6d4b\u4e2d\u6570\u636e\u654f\u611f\u95ee\u9898\u7684\u53ef\u884c\u65b9\u6cd5\uff0c\u7ed3\u5408\u6267\u6cd5\u673a\u6784\u7684\u8f93\u5165\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u81ea\u52a8\u5316\u68c0\u6d4b\u6548\u679c\u3002"}}
{"id": "2505.06267", "pdf": "https://arxiv.org/pdf/2505.06267", "abs": "https://arxiv.org/abs/2505.06267", "authors": ["Ilyas Oulkadda", "Julien Perez"], "title": "AKD : Adversarial Knowledge Distillation For Large Language Models Alignment on Coding tasks", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": null, "summary": "The widespread adoption of Large Language Models (LLMs) for code generation,\nexemplified by GitHub Copilot\\footnote{A coding extension powered by a Code-LLM\nto assist in code completion tasks} surpassing a million users, highlights the\ntransformative potential of these tools in improving developer productivity.\nHowever, this rapid growth also underscores critical concerns regarding the\nquality, safety, and reliability of the code they generate. As Code-LLMs\nevolve, they face significant challenges, including the diminishing returns of\nmodel scaling and the scarcity of new, high-quality training data. To address\nthese issues, this paper introduces Adversarial Knowledge Distillation (AKD), a\nnovel approach that leverages adversarially generated synthetic datasets to\ndistill the capabilities of larger models into smaller, more efficient ones. By\nsystematically stress-testing and refining the reasoning capabilities of\nCode-LLMs, AKD provides a framework for enhancing model robustness,\nreliability, and security while improving their parameter-efficiency. We\nbelieve this work represents a critical step toward ensuring dependable\nautomated code generation within the constraints of existing data and the\ncost-efficiency of model execution.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5bf9\u6297\u6027\u77e5\u8bc6\u84b8\u998f\uff08AKD\uff09\uff0c\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u6027\u5408\u6210\u6570\u636e\u96c6\uff0c\u5c06\u5927\u578bCode-LLM\u7684\u80fd\u529b\u84b8\u998f\u5230\u66f4\u5c0f\u3001\u9ad8\u6548\u7684\u6a21\u578b\u4e2d\uff0c\u4ee5\u89e3\u51b3\u6a21\u578b\u6269\u5c55\u6536\u76ca\u4e0b\u964d\u548c\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740Code-LLM\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4ee3\u7801\u751f\u6210\u7684\u8d28\u91cf\u3001\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u6210\u4e3a\u5173\u952e\u95ee\u9898\uff0c\u540c\u65f6\u6a21\u578b\u6269\u5c55\u7684\u6536\u76ca\u9012\u51cf\u548c\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u9650\u5236\u4e86\u5176\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u5bf9\u6297\u6027\u77e5\u8bc6\u84b8\u998f\uff08AKD\uff09\uff0c\u5229\u7528\u5bf9\u6297\u6027\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u96c6\u5bf9\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u6027\u538b\u529b\u6d4b\u8bd5\u548c\u4f18\u5316\uff0c\u5c06\u5927\u578b\u6a21\u578b\u7684\u80fd\u529b\u84b8\u998f\u5230\u5c0f\u578b\u6a21\u578b\u4e2d\u3002", "result": "AKD\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3001\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u53c2\u6570\u6548\u7387\uff0c\u4e3a\u5728\u73b0\u6709\u6570\u636e\u548c\u6210\u672c\u9650\u5236\u4e0b\u5b9e\u73b0\u53ef\u9760\u7684\u81ea\u52a8\u5316\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u6846\u67b6\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u662f\u786e\u4fddCode-LLM\u5728\u6570\u636e\u6709\u9650\u548c\u6210\u672c\u6548\u7387\u9650\u5236\u4e0b\u53ef\u9760\u751f\u6210\u4ee3\u7801\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2505.07809", "pdf": "https://arxiv.org/pdf/2505.07809", "abs": "https://arxiv.org/abs/2505.07809", "authors": ["M\u00e1t\u00e9 Gedeon"], "title": "A Comparative Analysis of Static Word Embeddings for Hungarian", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents a comprehensive analysis of various static word\nembeddings for Hungarian, including traditional models such as Word2Vec,\nFastText, as well as static embeddings derived from BERT-based models using\ndifferent extraction methods. We evaluate these embeddings on both intrinsic\nand extrinsic tasks to provide a holistic view of their performance. For\nintrinsic evaluation, we employ a word analogy task, which assesses the\nembeddings ability to capture semantic and syntactic relationships. Our results\nindicate that traditional static embeddings, particularly FastText, excel in\nthis task, achieving high accuracy and mean reciprocal rank (MRR) scores. Among\nthe BERT-based models, the X2Static method for extracting static embeddings\ndemonstrates superior performance compared to decontextualized and aggregate\nmethods, approaching the effectiveness of traditional static embeddings. For\nextrinsic evaluation, we utilize a bidirectional LSTM model to perform Named\nEntity Recognition (NER) and Part-of-Speech (POS) tagging tasks. The results\nreveal that embeddings derived from dynamic models, especially those extracted\nusing the X2Static method, outperform purely static embeddings. Notably, ELMo\nembeddings achieve the highest accuracy in both NER and POS tagging tasks,\nunderscoring the benefits of contextualized representations even when used in a\nstatic form. Our findings highlight the continued relevance of static word\nembeddings in NLP applications and the potential of advanced extraction methods\nto enhance the utility of BERT-based models. This piece of research contributes\nto the understanding of embedding performance in the Hungarian language and\nprovides valuable insights for future developments in the field. The training\nscripts, evaluation codes, restricted vocabulary, and extracted embeddings will\nbe made publicly available to support further research and reproducibility.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5168\u9762\u5206\u6790\u4e86\u5308\u7259\u5229\u8bed\u7684\u5404\u79cd\u9759\u6001\u8bcd\u5d4c\u5165\u65b9\u6cd5\uff0c\u5305\u62ecWord2Vec\u3001FastText\u4ee5\u53ca\u57fa\u4e8eBERT\u6a21\u578b\u7684\u9759\u6001\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u5185\u5728\u548c\u5916\u5728\u4efb\u52a1\u8bc4\u4f30\u5176\u6027\u80fd\u3002\u4f20\u7edf\u9759\u6001\u5d4c\u5165\uff08\u5c24\u5176\u662fFastText\uff09\u5728\u8bed\u4e49\u548c\u8bed\u6cd5\u5173\u7cfb\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u800c\u57fa\u4e8eBERT\u7684X2Static\u65b9\u6cd5\u63d0\u53d6\u7684\u9759\u6001\u5d4c\u5165\u63a5\u8fd1\u4f20\u7edf\u5d4c\u5165\u7684\u6548\u679c\u3002\u52a8\u6001\u6a21\u578b\u63d0\u53d6\u7684\u5d4c\u5165\uff08\u5982ELMo\uff09\u5728\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u8bcd\u6027\u6807\u6ce8\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f73\uff0c\u51f8\u663e\u4e86\u4e0a\u4e0b\u6587\u5316\u8868\u793a\u7684\u4f18\u52bf\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u6bd4\u8f83\u4e0d\u540c\u9759\u6001\u8bcd\u5d4c\u5165\u65b9\u6cd5\u5728\u5308\u7259\u5229\u8bed\u4e2d\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u4f20\u7edf\u65b9\u6cd5\u4e0e\u57fa\u4e8eBERT\u6a21\u578b\u7684\u9759\u6001\u5d4c\u5165\u65b9\u6cd5\uff0c\u4ee5\u671f\u4e3a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5e94\u7528\u63d0\u4f9b\u66f4\u4f18\u7684\u5d4c\u5165\u9009\u62e9\u3002", "method": "\u901a\u8fc7\u5185\u5728\u8bc4\u4f30\uff08\u5982\u8bcd\u7c7b\u6bd4\u4efb\u52a1\uff09\u548c\u5916\u5728\u8bc4\u4f30\uff08\u5982\u53cc\u5411LSTM\u6a21\u578b\u7528\u4e8e\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u8bcd\u6027\u6807\u6ce8\uff09\u6765\u6d4b\u8bd5\u5404\u79cd\u5d4c\u5165\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "result": "\u4f20\u7edf\u9759\u6001\u5d4c\u5165\uff08\u5982FastText\uff09\u5728\u8bcd\u7c7b\u6bd4\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u800c\u57fa\u4e8eBERT\u7684X2Static\u65b9\u6cd5\u63d0\u53d6\u7684\u9759\u6001\u5d4c\u5165\u63a5\u8fd1\u5176\u6548\u679c\u3002\u52a8\u6001\u6a21\u578b\uff08\u5982ELMo\uff09\u7684\u5d4c\u5165\u5728\u5916\u5728\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u9759\u6001\u8bcd\u5d4c\u5165\u5728NLP\u4e2d\u4ecd\u5177\u91cd\u8981\u6027\uff0c\u800c\u9ad8\u7ea7\u63d0\u53d6\u65b9\u6cd5\uff08\u5982X2Static\uff09\u80fd\u63d0\u5347\u57fa\u4e8eBERT\u6a21\u578b\u7684\u6548\u679c\u3002\u7814\u7a76\u4e3a\u5308\u7259\u5229\u8bed\u7684\u5d4c\u5165\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u76f8\u5173\u4ee3\u7801\u548c\u8d44\u6e90\u5c06\u516c\u5f00\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2505.06651", "pdf": "https://arxiv.org/pdf/2505.06651", "abs": "https://arxiv.org/abs/2505.06651", "authors": ["Zehan Zhu", "Yan Huang", "Xin Wang", "Shouling Ji", "Jinming Xu"], "title": "Dyn-D$^2$P: Dynamic Differentially Private Decentralized Learning with Provable Utility Guarantee", "categories": ["cs.LG", "cs.AI"], "comment": "This paper has been accepted by the 34th International Joint\n  Conference on Artificial Intelligence(IJCAI 2025)", "summary": "Most existing decentralized learning methods with differential privacy (DP)\nguarantee rely on constant gradient clipping bounds and fixed-level DP Gaussian\nnoises for each node throughout the training process, leading to a significant\naccuracy degradation compared to non-private counterparts. In this paper, we\npropose a new Dynamic Differentially Private Decentralized learning approach\n(termed Dyn-D$^2$P) tailored for general time-varying directed networks.\nLeveraging the Gaussian DP (GDP) framework for privacy accounting, Dyn-D$^2$P\ndynamically adjusts gradient clipping bounds and noise levels based on gradient\nconvergence. This proposed dynamic noise strategy enables us to enhance model\naccuracy while preserving the total privacy budget. Extensive experiments on\nbenchmark datasets demonstrate the superiority of Dyn-D$^2$P over its\ncounterparts employing fixed-level noises, especially under strong privacy\nguarantees. Furthermore, we provide a provable utility bound for Dyn-D$^2$P\nthat establishes an explicit dependency on network-related parameters, with a\nscaling factor of $1/\\sqrt{n}$ in terms of the number of nodes $n$ up to a bias\nerror term induced by gradient clipping. To our knowledge, this is the first\nmodel utility analysis for differentially private decentralized non-convex\noptimization with dynamic gradient clipping bounds and noise levels.", "AI": {"tldr": "Dyn-D2P\u662f\u4e00\u79cd\u52a8\u6001\u5dee\u5206\u9690\u79c1\u5206\u5e03\u5f0f\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u68af\u5ea6\u88c1\u526a\u754c\u9650\u548c\u566a\u58f0\u6c34\u5e73\u63d0\u9ad8\u6a21\u578b\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u9884\u7b97\uff0c\u4f18\u4e8e\u56fa\u5b9a\u566a\u58f0\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5206\u5e03\u5f0f\u5b66\u4e60\u65b9\u6cd5\u7684\u56fa\u5b9a\u68af\u5ea6\u88c1\u526a\u548c\u566a\u58f0\u6c34\u5e73\u5bfc\u81f4\u7cbe\u5ea6\u663e\u8457\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u52a8\u6001\u8c03\u6574\u4ee5\u63d0\u9ad8\u7cbe\u5ea6\u5e76\u4fdd\u62a4\u9690\u79c1\u3002", "method": "\u5229\u7528\u9ad8\u65af\u5dee\u5206\u9690\u79c1\u6846\u67b6\uff0c\u52a8\u6001\u8c03\u6574\u68af\u5ea6\u88c1\u526a\u754c\u9650\u548c\u566a\u58f0\u6c34\u5e73\uff0c\u57fa\u4e8e\u68af\u5ea6\u6536\u655b\u60c5\u51b5\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDyn-D2P\u5728\u5f3a\u9690\u79c1\u4fdd\u8bc1\u4e0b\u4f18\u4e8e\u56fa\u5b9a\u566a\u58f0\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u6a21\u578b\u6548\u7528\u8fb9\u754c\u5206\u6790\u3002", "conclusion": "Dyn-D2P\u662f\u9996\u4e2a\u9488\u5bf9\u52a8\u6001\u68af\u5ea6\u88c1\u526a\u548c\u566a\u58f0\u6c34\u5e73\u7684\u5dee\u5206\u9690\u79c1\u5206\u5e03\u5f0f\u975e\u51f8\u4f18\u5316\u65b9\u6cd5\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u4f18\u52bf\u3002"}}
{"id": "2505.06653", "pdf": "https://arxiv.org/pdf/2505.06653", "abs": "https://arxiv.org/abs/2505.06653", "authors": ["Patrick Blumenberg", "Thomas Graave", "Tim Fingscheidt"], "title": "Improving Block-Wise LLM Quantization by 4-bit Block-Wise Optimal Float (BOF4): Analysis and Variations", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) demand extensive memory capacity during both\nfine-tuning and inference. To enable memory-efficient fine-tuning, existing\nmethods apply block-wise quantization techniques, such as NF4 and AF4, to the\nnetwork weights. We show that these quantization techniques incur suboptimal\nquantization errors. Therefore, as a first novelty, we propose an optimization\napproach for block-wise quantization. Using this method, we design a family of\nquantizers named 4-bit block-wise optimal float (BOF4), which consistently\nreduces the quantization error compared to both baseline methods. We provide\nboth a theoretical and a data-driven solution for the optimization process and\nprove their practical equivalence. Secondly, we propose a modification to the\nemployed normalization method based on the signed absolute block maximum\n(BOF4-S), enabling further reduction of the quantization error and empirically\nachieving less degradation in language modeling performance. Thirdly, we\nexplore additional variations of block-wise quantization methods applied to\nLLMs through an experimental study on the importance of accurately representing\nzero and large-amplitude weights on the one hand, and optimization towards\nvarious error metrics on the other hand. Lastly, we introduce a mixed-precision\nquantization strategy dubbed outlier-preserving quantization (OPQ) to address\nthe distributional mismatch induced by outlier weights in block-wise\nquantization. By storing outlier weights in 16-bit precision (OPQ) while\napplying BOF4-S, we achieve top performance among 4-bit block-wise quantization\ntechniques w.r.t. perplexity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u5757\u7ea7\u91cf\u5316\u7684\u65b9\u6cd5BOF4\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u6570\u636e\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\u51cf\u5c11\u91cf\u5316\u8bef\u5dee\uff0c\u8fdb\u4e00\u6b65\u6539\u8fdb\u4e3aBOF4-S\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u6df7\u5408\u7cbe\u5ea6\u7b56\u7565OPQ\u4ee5\u5904\u7406\u5f02\u5e38\u6743\u91cd\uff0c\u6700\u7ec8\u57284-bit\u5757\u7ea7\u91cf\u5316\u6280\u672f\u4e2d\u53d6\u5f97\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5fae\u8c03\u548c\u63a8\u7406\u65f6\u9700\u8981\u5927\u91cf\u5185\u5b58\u3002\u73b0\u6709\u5757\u7ea7\u91cf\u5316\u65b9\u6cd5\uff08\u5982NF4\u548cAF4\uff09\u91cf\u5316\u8bef\u5dee\u8f83\u5927\uff0c\u56e0\u6b64\u9700\u4f18\u5316\u4ee5\u51cf\u5c11\u8bef\u5dee\u5e76\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u3002", "method": "1. \u63d0\u51faBOF4\u91cf\u5316\u5668\u4f18\u5316\u5757\u7ea7\u91cf\u5316\uff1b2. \u6539\u8fdb\u5f52\u4e00\u5316\u65b9\u6cd5\uff08BOF4-S\uff09\u8fdb\u4e00\u6b65\u51cf\u5c11\u8bef\u5dee\uff1b3. \u5b9e\u9a8c\u63a2\u7a76\u5757\u7ea7\u91cf\u5316\u53d8\u4f53\u7684\u91cd\u8981\u6027\uff1b4. \u5f15\u5165\u6df7\u5408\u7cbe\u5ea6\u7b56\u7565OPQ\u5904\u7406\u5f02\u5e38\u6743\u91cd\u3002", "result": "BOF4-S\u7ed3\u5408OPQ\u57284-bit\u5757\u7ea7\u91cf\u5316\u6280\u672f\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u8bed\u8a00\u5efa\u6a21\u6027\u80fd\u4e0b\u964d\u6700\u5c0f\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u5757\u7ea7\u91cf\u5316\u548c\u6df7\u5408\u7cbe\u5ea6\u7b56\u7565\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u91cf\u5316\u8bef\u5dee\u5e76\u63d0\u5347\u4e86LLMs\u7684\u5185\u5b58\u6548\u7387\u4e0e\u6027\u80fd\u3002"}}
{"id": "2505.06313", "pdf": "https://arxiv.org/pdf/2505.06313", "abs": "https://arxiv.org/abs/2505.06313", "authors": ["Bohdan M. Pavlyshenko"], "title": "AI Approaches to Qualitative and Quantitative News Analytics on NATO Unity", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.SI"], "comment": null, "summary": "The paper considers the use of GPT models with retrieval-augmented generation\n(RAG) for qualitative and quantitative analytics on NATO sentiments, NATO unity\nand NATO Article 5 trust opinion scores in different web sources: news sites\nfound via Google Search API, Youtube videos with comments, and Reddit\ndiscussions. A RAG approach using GPT-4.1 model was applied to analyse news\nwhere NATO related topics were discussed. Two levels of RAG analytics were\nused: on the first level, the GPT model generates qualitative news summaries\nand quantitative opinion scores using zero-shot prompts; on the second level,\nthe GPT model generates the summary of news summaries. Quantitative news\nopinion scores generated by the GPT model were analysed using Bayesian\nregression to get trend lines. The distributions found for the regression\nparameters make it possible to analyse an uncertainty in specified news opinion\nscore trends. Obtained results show a downward trend for analysed scores of\nopinion related to NATO unity.\n  This approach does not aim to conduct real political analysis; rather, it\nconsider AI based approaches which can be used for further analytics\n  as a part of a complex analytical approach. The obtained results demonstrate\nthat the use of GPT models for news analysis can give informative qualitative\nand quantitative analytics, providing important insights.\n  The dynamic model based on neural ordinary differential equations was\nconsidered for modelling public opinions. This approach makes it possible to\nanalyse different scenarios for evolving public opinions.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528GPT\u6a21\u578b\u4e0e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\u5206\u6790\u5317\u7ea6\u60c5\u7eea\u3001\u56e2\u7ed3\u53ca\u5bf9\u7b2c5\u6761\u4fe1\u4efb\u7684\u5b9a\u6027\u5b9a\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f51\u7edc\u65b0\u95fb\u3001YouTube\u8bc4\u8bba\u548cReddit\u8ba8\u8bba\u6570\u636e\uff0c\u53d1\u73b0\u76f8\u5173\u610f\u89c1\u5206\u6570\u5448\u4e0b\u964d\u8d8b\u52bf\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22AI\uff08\u7279\u522b\u662fGPT\u6a21\u578b\uff09\u5728\u590d\u6742\u5206\u6790\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u653f\u6cbb\u6216\u516c\u5171\u610f\u89c1\u5206\u6790\u63d0\u4f9b\u521d\u6b65\u7684\u5b9a\u6027\u5b9a\u91cf\u5de5\u5177\uff0c\u800c\u975e\u76f4\u63a5\u7528\u4e8e\u5b9e\u9645\u653f\u6cbb\u51b3\u7b56\u3002", "method": "\u91c7\u7528GPT-4.1\u6a21\u578b\u4e0eRAG\u6280\u672f\uff0c\u5206\u4e24\u7ea7\u5206\u6790\uff1a\u9996\u5c42\u751f\u6210\u65b0\u95fb\u6458\u8981\u548c\u610f\u89c1\u5206\u6570\uff0c\u6b21\u5c42\u6c47\u603b\u6458\u8981\uff1b\u901a\u8fc7\u8d1d\u53f6\u65af\u56de\u5f52\u5206\u6790\u8d8b\u52bf\uff0c\u5e76\u5229\u7528\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u52a8\u6001\u6a21\u62df\u8206\u8bba\u6f14\u53d8\u3002", "result": "\u5206\u6790\u663e\u793a\uff0c\u5317\u7ea6\u56e2\u7ed3\u76f8\u5173\u610f\u89c1\u5206\u6570\u5448\u73b0\u4e0b\u964d\u8d8b\u52bf\uff0c\u6a21\u578b\u80fd\u6709\u6548\u6355\u6349\u4e0d\u786e\u5b9a\u6027\u5e76\u63d0\u4f9b\u4fe1\u606f\u6027\u5206\u6790\u3002", "conclusion": "GPT\u6a21\u578b\u7ed3\u5408RAG\u6280\u672f\u53ef\u4e3a\u65b0\u95fb\u5206\u6790\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u5b9a\u6027\u5b9a\u91cf\u7ed3\u679c\uff0c\u52a8\u6001\u6a21\u578b\u8fdb\u4e00\u6b65\u6269\u5c55\u4e86\u8206\u8bba\u6f14\u53d8\u7684\u60c5\u666f\u5206\u6790\u80fd\u529b\u3002"}}
{"id": "2505.06688", "pdf": "https://arxiv.org/pdf/2505.06688", "abs": "https://arxiv.org/abs/2505.06688", "authors": ["Jianxin Zhang", "Lianzi Jiang", "Xinyu Han", "Xiangrong Wang"], "title": "A Novel Framework for Significant Wave Height Prediction based on Adaptive Feature Extraction Time-Frequency Network", "categories": ["cs.LG"], "comment": null, "summary": "Precise forecasting of significant wave height (Hs) is essential for the\ndevelopment and utilization of wave energy. The challenges in predicting Hs\narise from its non-linear and non-stationary characteristics. The combination\nof decomposition preprocessing and machine learning models have demonstrated\nsignificant effectiveness in Hs prediction by extracting data features.\nHowever, decomposing the unknown data in the test set can lead to data leakage\nissues. To simultaneously achieve data feature extraction and prevent data\nleakage, a novel Adaptive Feature Extraction Time-Frequency Network (AFE-TFNet)\nis proposed to improve prediction accuracy and stability. It is encoder-decoder\nrolling framework. The encoder consists of two stages: feature extraction and\nfeature fusion. In the feature extraction stage, global and local frequency\ndomain features are extracted by combining Wavelet Transform (WT) and Fourier\nTransform (FT), and multi-scale frequency analysis is performed using Inception\nblocks. In the feature fusion stage, time-domain and frequency-domain features\nare integrated through dominant harmonic sequence energy weighting (DHSEW). The\ndecoder employed an advanced long short-term memory (LSTM) model. Hourly\nmeasured wind speed (Ws), dominant wave period (DPD), average wave period (APD)\nand Hs from three stations are used as the dataset, and the four metrics are\nemployed to evaluate the forecasting performance. Results show that AFE-TFNet\nsignificantly outperforms benchmark methods in terms of prediction accuracy.\nFeature extraction can significantly improve the prediction accuracy. DHSEW has\nsubstantially increased the accuracy of medium-term to long-term forecasting.\nThe prediction accuracy of AFE-TFNet does not demonstrate significant\nvariability with changes of rolling time window size. Overall, AFE-TFNet shows\nstrong potential for handling complex signal forecasting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u81ea\u9002\u5e94\u7279\u5f81\u63d0\u53d6\u65f6\u9891\u7f51\u7edc\uff08AFE-TFNet\uff09\uff0c\u7528\u4e8e\u663e\u8457\u6ce2\u9ad8\uff08Hs\uff09\u7684\u7cbe\u786e\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5206\u89e3\u9884\u5904\u7406\u5f15\u53d1\u7684\u6570\u636e\u6cc4\u9732\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u7279\u5f81\u63d0\u53d6\u4e0e\u878d\u5408\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u9884\u6d4b\u663e\u8457\u6ce2\u9ad8\uff08Hs\uff09\u5bf9\u6ce2\u6d6a\u80fd\u5f00\u53d1\u81f3\u5173\u91cd\u8981\uff0c\u4f46Hs\u7684\u975e\u7ebf\u6027\u548c\u975e\u5e73\u7a33\u7279\u6027\u5e26\u6765\u4e86\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u4e2d\uff0c\u5206\u89e3\u9884\u5904\u7406\u53ef\u80fd\u5bfc\u81f4\u6d4b\u8bd5\u96c6\u6570\u636e\u6cc4\u9732\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u63d0\u53d6\u7279\u5f81\u53c8\u80fd\u907f\u514d\u6cc4\u9732\u7684\u65b0\u65b9\u6cd5\u3002", "method": "AFE-TFNet\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6eda\u52a8\u6846\u67b6\u3002\u7f16\u7801\u5668\u5206\u4e24\u9636\u6bb5\uff1a\u7279\u5f81\u63d0\u53d6\uff08\u7ed3\u5408\u5c0f\u6ce2\u53d8\u6362\u548c\u5085\u91cc\u53f6\u53d8\u6362\u63d0\u53d6\u5168\u5c40\u4e0e\u5c40\u90e8\u9891\u57df\u7279\u5f81\uff0c\u4f7f\u7528Inception\u5757\u8fdb\u884c\u591a\u5c3a\u5ea6\u5206\u6790\uff09\u548c\u7279\u5f81\u878d\u5408\uff08\u901a\u8fc7\u4e3b\u5bfc\u8c10\u6ce2\u5e8f\u5217\u80fd\u91cf\u52a0\u6743\u6574\u5408\u65f6\u9891\u57df\u7279\u5f81\uff09\u3002\u89e3\u7801\u5668\u91c7\u7528\u6539\u8fdb\u7684LSTM\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u4f7f\u7528\u4e09\u4e2a\u7ad9\u70b9\u7684\u98ce\u901f\u3001\u6ce2\u5468\u671f\u548cHs\u6570\u636e\uff0c\u7ed3\u679c\u8868\u660eAFE-TFNet\u5728\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u7279\u5f81\u63d0\u53d6\u548c\u878d\u5408\uff08\u5c24\u5176\u662fDHSEW\uff09\u663e\u8457\u63d0\u5347\u4e86\u4e2d\u957f\u671f\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4e14\u6eda\u52a8\u65f6\u95f4\u7a97\u53e3\u53d8\u5316\u5bf9\u6a21\u578b\u5f71\u54cd\u8f83\u5c0f\u3002", "conclusion": "AFE-TFNet\u80fd\u6709\u6548\u5904\u7406\u590d\u6742\u4fe1\u53f7\u9884\u6d4b\uff0c\u5c24\u5176\u5728\u907f\u514d\u6570\u636e\u6cc4\u9732\u548c\u63d0\u5347\u4e2d\u957f\u671f\u9884\u6d4b\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.06690", "pdf": "https://arxiv.org/pdf/2505.06690", "abs": "https://arxiv.org/abs/2505.06690", "authors": ["Jianxin Zhang", "Lianzi Jiang", "Xinyu Han", "Xiangrong Wang", "Weinan Huang"], "title": "E2E-FANet: A Highly Generalizable Framework for Waves prediction Behind Floating Breakwaters via Exogenous-to-Endogenous Variable Attention", "categories": ["cs.LG"], "comment": null, "summary": "Accurate prediction of waves behind floating breakwaters (FB) is crucial for\noptimizing coastal engineering structures, enhancing safety, and improving\ndesign efficiency. Existing methods demonstrate limitations in capturing\nnonlinear interactions between waves and structures, while exhibiting\ninsufficient capability in modeling the complex frequency-domain relationships\namong elevations of different wave gauges. To address these challenges, this\nstudy introduces the Exogenous-to-Endogenous Frequency-Aware Network\n(E2E-FANet), a novel end-to-end neural network designed to model relationships\nbetween waves and structures. The E2E-FANetarchitecture incorporates a\nDual-Basis Frequency Mapping (DBFM) module that leverages orthogonal cosine and\nsine bases to extract wave features from the frequency domain while preserving\ntemporal information. Additionally, we introduce the Exogenous-to-Endogenous\nCross-Attention (E2ECA) module, which employs cross attention to model the\ninteractions between endogenous and exogenous variables. We incorporate a\nTemporal-wise Attention (TA) mechanism that adaptively captures complex\ndependencies in endogenous variables. These integrated modules function\nsynergistically, enabling E2E-FANet to achieve both comprehensive feature\nperception in the time-frequency domain and precise modeling of wave-structure\ninteractions. To comprehensively evaluate the performance of E2E-FANet, we\nconstructed a multi-level validation framework comprising three distinct\ntesting scenarios: internal validation under identical wave conditions,\ngeneralization testing across different wave conditions, and adaptability\ntesting with varying relative water density (RW) conditions. These\ncomprehensive tests demonstrate that E2E-FANet provides accurate waves behind\nFB predictions while successfully generalizing diverse wave conditions.", "AI": {"tldr": "\u9488\u5bf9\u6d6e\u52a8\u9632\u6ce2\u5824\u540e\u6ce2\u6d6a\u9884\u6d4b\u7684\u51c6\u786e\u6027\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86E2E-FANet\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u9891\u57df\u7279\u5f81\u63d0\u53d6\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u6349\u6ce2\u6d6a\u4e0e\u7ed3\u6784\u7684\u975e\u7ebf\u6027\u76f8\u4e92\u4f5c\u7528\u53ca\u590d\u6742\u9891\u57df\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u4f18\u5316\u6d77\u5cb8\u5de5\u7a0b\u7ed3\u6784\u3002", "method": "\u63d0\u51faE2E-FANet\u6a21\u578b\uff0c\u5305\u542bDBFM\u6a21\u5757\uff08\u9891\u57df\u7279\u5f81\u63d0\u53d6\uff09\u3001E2ECA\u6a21\u5757\uff08\u5185\u5916\u53d8\u91cf\u4ea4\u4e92\u5efa\u6a21\uff09\u548cTA\u673a\u5236\uff08\u65f6\u95f4\u4f9d\u8d56\u6355\u6349\uff09\u3002", "result": "\u901a\u8fc7\u591a\u5c42\u6b21\u9a8c\u8bc1\uff08\u76f8\u540c\u6ce2\u6d6a\u6761\u4ef6\u3001\u4e0d\u540c\u6ce2\u6d6a\u6761\u4ef6\u3001\u4e0d\u540c\u76f8\u5bf9\u6c34\u5bc6\u5ea6\u6761\u4ef6\uff09\uff0cE2E-FANet\u5c55\u73b0\u51fa\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "E2E-FANet\u6a21\u578b\u5728\u6ce2\u6d6a\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u6d77\u5cb8\u5de5\u7a0b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bbe\u8ba1\u4f18\u5316\u5de5\u5177\u3002"}}
{"id": "2505.06699", "pdf": "https://arxiv.org/pdf/2505.06699", "abs": "https://arxiv.org/abs/2505.06699", "authors": ["Xiyuan Wei", "Ming Lin", "Fanjiang Ye", "Fengguang Song", "Liangliang Cao", "My T. That", "Tianbao Yang"], "title": "Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws", "categories": ["cs.LG", "stat.ML"], "comment": "18 pages, 6 figures", "summary": "This paper formalizes an emerging learning paradigm that uses a trained model\nas a reference to guide and enhance the training of a target model through\nstrategic data selection or weighting, named $\\textbf{model steering}$. While\nad-hoc methods have been used in various contexts, including the training of\nlarge foundation models, its underlying principles remain insufficiently\nunderstood, leading to sub-optimal performance. In this work, we propose a\ntheory-driven framework for model steering called $\\textbf{DRRho risk\nminimization}$, which is rooted in Distributionally Robust Optimization (DRO).\nThrough a generalization analysis, we provide theoretical insights into why\nthis approach improves generalization and data efficiency compared to training\nwithout a reference model. To the best of our knowledge, this is the first time\nsuch theoretical insights are provided for the new learning paradigm, which\nsignificantly enhance our understanding and practice of model steering.\nBuilding on these insights and the connection between contrastive learning and\nDRO, we introduce a novel method for Contrastive Language-Image Pretraining\n(CLIP) with a reference model, termed DRRho-CLIP. Extensive experiments\nvalidate the theoretical insights, reveal a superior scaling law compared to\nCLIP without a reference model, and demonstrate its strength over existing\nheuristic approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u7684\u7406\u8bba\u6846\u67b6DRRho\u98ce\u9669\u6700\u5c0f\u5316\uff0c\u7528\u4e8e\u6307\u5bfc\u76ee\u6807\u6a21\u578b\u7684\u8bad\u7ec3\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u6570\u636e\u6548\u7387\uff0c\u5e76\u5728CLIP\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5f15\u5bfc\u7684\u65b9\u6cd5\u867d\u5728\u5b9e\u8df5\u4e2d\u6709\u6240\u5e94\u7528\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u6846\u67b6\u8bbe\u8ba1\uff0c\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u5e76\u63d0\u5347\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86DRRho\u98ce\u9669\u6700\u5c0f\u5316\u6846\u67b6\uff0c\u57fa\u4e8e\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u7406\u8bba\uff0c\u5e76\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u4e0eDRO\u7684\u8054\u7cfb\uff0c\u8bbe\u8ba1\u4e86DRRho-CLIP\u65b9\u6cd5\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728CLIP\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u6269\u5c55\u6027\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6a21\u578b\u5f15\u5bfc\u8fd9\u4e00\u65b0\u5174\u5b66\u4e60\u8303\u5f0f\u63d0\u4f9b\u4e86\u9996\u4e2a\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u9a71\u52a8\u7684\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u5b9e\u8df5\u6548\u679c\u3002"}}
{"id": "2505.06803", "pdf": "https://arxiv.org/pdf/2505.06803", "abs": "https://arxiv.org/abs/2505.06803", "authors": ["Xilin Jiang", "Junkai Wu", "Vishal Choudhari", "Nima Mesgarani"], "title": "Bridging Ears and Eyes: Analyzing Audio and Visual Large Language Models to Humans in Visible Sound Recognition and Reducing Their Sensory Gap via Cross-Modal Distillation", "categories": ["cs.SD", "cs.CL", "cs.CV", "cs.MM", "eess.AS"], "comment": null, "summary": "Audio large language models (LLMs) are considered experts at recognizing\nsound objects, yet their performance relative to LLMs in other sensory\nmodalities, such as visual or audio-visual LLMs, and to humans using their\nears, eyes, or both remains unexplored. To investigate this, we systematically\nevaluate audio, visual, and audio-visual LLMs, specifically Qwen2-Audio,\nQwen2-VL, and Qwen2.5-Omni, against humans in recognizing sound objects of\ndifferent classes from audio-only, silent video, or sounded video inputs. We\nuncover a performance gap between Qwen2-Audio and Qwen2-VL that parallels the\nsensory discrepancy between human ears and eyes. To reduce this gap, we\nintroduce a cross-modal distillation framework, where an LLM in one modality\nserves as the teacher and another as the student, with knowledge transfer in\nsound classes predicted as more challenging to the student by a heuristic\nmodel. Distillation in both directions, from Qwen2-VL to Qwen2-Audio and vice\nversa, leads to notable improvements, particularly in challenging classes. This\nwork highlights the sensory gap in LLMs from a human-aligned perspective and\nproposes a principled approach to enhancing modality-specific perception in\nmultimodal LLMs.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5bf9\u6bd4\u97f3\u9891\u3001\u89c6\u89c9\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bc6\u522b\u58f0\u97f3\u5bf9\u8c61\u4e0a\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u97f3\u9891\u548c\u89c6\u89c9\u6a21\u578b\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u8de8\u6a21\u6001\u84b8\u998f\u6846\u67b6\u6765\u7f29\u5c0f\u5dee\u8ddd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u97f3\u9891LLMs\u4e0e\u89c6\u89c9\u3001\u591a\u6a21\u6001LLMs\u4ee5\u53ca\u4eba\u7c7b\u5728\u8bc6\u522b\u58f0\u97f3\u5bf9\u8c61\u4e0a\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u4ece\u4eba\u7c7b\u611f\u5b98\u5dee\u5f02\u7684\u89d2\u5ea6\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u8bc4\u4f30\u97f3\u9891\uff08Qwen2-Audio\uff09\u3001\u89c6\u89c9\uff08Qwen2-VL\uff09\u548c\u591a\u6a21\u6001\uff08Qwen2.5-Omni\uff09LLMs\u4e0e\u4eba\u7c7b\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u8de8\u6a21\u6001\u84b8\u998f\u6846\u67b6\uff0c\u4ee5\u4e00\u79cd\u6a21\u6001\u7684LLM\u4f5c\u4e3a\u6559\u5e08\uff0c\u53e6\u4e00\u79cd\u4f5c\u4e3a\u5b66\u751f\uff0c\u901a\u8fc7\u77e5\u8bc6\u8f6c\u79fb\u63d0\u5347\u6027\u80fd\u3002", "result": "\u8de8\u6a21\u6001\u84b8\u998f\uff08\u53cc\u5411\uff1aQwen2-VL\u5230Qwen2-Audio\uff0c\u53cd\u4e4b\u4ea6\u7136\uff09\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u6311\u6218\u6027\u7c7b\u522b\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "\u672c\u6587\u4ece\u4eba\u7c7b\u611f\u5b98\u5bf9\u9f50\u7684\u89d2\u5ea6\u63ed\u793a\u4e86\u591a\u6a21\u6001LLMs\u7684\u611f\u5b98\u5dee\u8ddd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u6a21\u6001\u611f\u77e5\u7684\u539f\u5219\u6027\u65b9\u6cd5\u3002"}}
{"id": "2505.06709", "pdf": "https://arxiv.org/pdf/2505.06709", "abs": "https://arxiv.org/abs/2505.06709", "authors": ["Abhishek Sinha", "Rahul Vaze"], "title": "Beyond $\\tilde{O}(\\sqrt{T})$ Constraint Violation for Online Convex Optimization with Adversarial Constraints", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": null, "summary": "We revisit the Online Convex Optimization problem with adversarial\nconstraints (COCO) where, in each round, a learner is presented with a convex\ncost function and a convex constraint function, both of which may be chosen\nadversarially. The learner selects actions from a convex decision set in an\nonline fashion, with the goal of minimizing both regret and the cumulative\nconstraint violation (CCV) over a horizon of $T$ rounds. The best-known policy\nfor this problem achieves $O(\\sqrt{T})$ regret and $\\tilde{O}(\\sqrt{T})$ CCV.\nIn this paper, we present a surprising improvement that achieves a\nsignificantly smaller CCV by trading it off with regret. Specifically, for any\nbounded convex cost and constraint functions, we propose an online policy that\nachieves $\\tilde{O}(\\sqrt{dT}+ T^\\beta)$ regret and $\\tilde{O}(dT^{1-\\beta})$\nCCV, where $d$ is the dimension of the decision set and $\\beta \\in [0,1]$ is a\ntunable parameter. We achieve this result by first considering the special case\nof $\\textsf{Constrained Expert}$ problem where the decision set is a\nprobability simplex and the cost and constraint functions are linear.\nLeveraging a new adaptive small-loss regret bound, we propose an efficient\npolicy for the $\\textsf{Constrained Expert}$ problem, that attains\n$O(\\sqrt{T\\ln N}+T^{\\beta})$ regret and $\\tilde{O}(T^{1-\\beta} \\ln N)$ CCV,\nwhere $N$ is the number of experts. The original problem is then reduced to the\n$\\textsf{Constrained Expert}$ problem via a covering argument. Finally, with an\nadditional smoothness assumption, we propose an efficient gradient-based policy\nattaining $O(T^{\\max(\\frac{1}{2},\\beta)})$ regret and $\\tilde{O}(T^{1-\\beta})$\nCCV.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u5e26\u5bf9\u6297\u6027\u7ea6\u675f\u7684\u5728\u7ebf\u51f8\u4f18\u5316\u95ee\u9898\uff08COCO\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7b56\u7565\uff0c\u901a\u8fc7\u6743\u8861\u540e\u6094\u503c\u548c\u7d2f\u8ba1\u7ea6\u675f\u8fdd\u53cd\uff08CCV\uff09\uff0c\u663e\u8457\u964d\u4f4e\u4e86CCV\u3002\u65b9\u6cd5\u5305\u62ec\u4ece\u7ea6\u675f\u4e13\u5bb6\u95ee\u9898\u5165\u624b\uff0c\u5229\u7528\u81ea\u9002\u5e94\u5c0f\u635f\u5931\u540e\u6094\u8fb9\u754c\uff0c\u5e76\u6700\u7ec8\u901a\u8fc7\u5e73\u6ed1\u6027\u5047\u8bbe\u63d0\u51fa\u68af\u5ea6\u7b56\u7565\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u901a\u8fc7\u6539\u8fdb\u73b0\u6709\u7b56\u7565\uff0c\u5b9e\u73b0\u5728\u7ebf\u51f8\u4f18\u5316\u95ee\u9898\u4e2d\u540e\u6094\u503c\u548cCCV\u7684\u66f4\u597d\u6743\u8861\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u7ef4\u51b3\u7b56\u7a7a\u95f4\u4e2d\u3002", "method": "\u65b9\u6cd5\u9996\u5148\u4ece\u7ea6\u675f\u4e13\u5bb6\u95ee\u9898\uff08\u51b3\u7b56\u96c6\u4e3a\u6982\u7387\u5355\u7eaf\u5f62\uff09\u5165\u624b\uff0c\u63d0\u51fa\u81ea\u9002\u5e94\u5c0f\u635f\u5931\u540e\u6094\u8fb9\u754c\u7684\u9ad8\u6548\u7b56\u7565\uff1b\u518d\u901a\u8fc7\u8986\u76d6\u8bba\u8bc1\u6269\u5c55\u5230\u4e00\u822c\u95ee\u9898\uff1b\u6700\u540e\u57fa\u4e8e\u5e73\u6ed1\u6027\u5047\u8bbe\u63d0\u51fa\u68af\u5ea6\u7b56\u7565\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u65b0\u7b56\u7565\u5728\u540e\u6094\u503c\uff08$\tilde{O}(/sqrt{dT}+ T^\beta)$\uff09\u548cCCV\uff08$\tilde{O}(dT^{1-\beta})$\uff09\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u7ed3\u679c\uff08$O(/sqrt{T})$\u540e\u6094\u503c\uff0c$\tilde{O}(/sqrt{T})$ CCV\uff09\u3002", "conclusion": "\u8bba\u6587\u7ed3\u8bba\u8868\u660e\uff0c\u901a\u8fc7\u6743\u8861\u540e\u6094\u503c\u548cCCV\uff0c\u65b0\u7b56\u7565\u5728\u9ad8\u7ef4\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u66f4\u4f18\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5e73\u6ed1\u6027\u5047\u8bbe\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2505.06814", "pdf": "https://arxiv.org/pdf/2505.06814", "abs": "https://arxiv.org/abs/2505.06814", "authors": ["Bin Li", "Shenxi Liu", "Yixuan Weng", "Yue Du", "Yuhang Tian", "Shoujun Zhou"], "title": "Overview of the NLPCC 2025 Shared Task 4: Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "12 pages, 5 figures, 4 tables", "summary": "Following the successful hosts of the 1-st (NLPCC 2023 Foshan) CMIVQA and the\n2-rd (NLPCC 2024 Hangzhou) MMIVQA challenges, this year, a new task has been\nintroduced to further advance research in multi-modal, multilingual, and\nmulti-hop medical instructional question answering (M4IVQA) systems, with a\nspecific focus on medical instructional videos. The M4IVQA challenge focuses on\nevaluating models that integrate information from medical instructional videos,\nunderstand multiple languages, and answer multi-hop questions requiring\nreasoning over various modalities. This task consists of three tracks:\nmulti-modal, multilingual, and multi-hop Temporal Answer Grounding in Single\nVideo (M4TAGSV), multi-modal, multilingual, and multi-hop Video Corpus\nRetrieval (M4VCR) and multi-modal, multilingual, and multi-hop Temporal Answer\nGrounding in Video Corpus (M4TAGVC). Participants in M4IVQA are expected to\ndevelop algorithms capable of processing both video and text data,\nunderstanding multilingual queries, and providing relevant answers to multi-hop\nmedical questions. We believe the newly introduced M4IVQA challenge will drive\ninnovations in multimodal reasoning systems for healthcare scenarios,\nultimately contributing to smarter emergency response systems and more\neffective medical education platforms in multilingual communities. Our official\nwebsite is https://cmivqa.github.io/", "AI": {"tldr": "M4IVQA\u6311\u6218\u8d5b\u65e8\u5728\u63a8\u52a8\u591a\u6a21\u6001\u3001\u591a\u8bed\u8a00\u548c\u591a\u8df3\u533b\u5b66\u6559\u5b66\u89c6\u9891\u95ee\u7b54\u7cfb\u7edf\u7684\u7814\u7a76\uff0c\u5305\u542b\u4e09\u4e2a\u5b50\u4efb\u52a1\uff1aM4TAGSV\u3001M4VCR\u548cM4TAGVC\u3002", "motivation": "\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\u3001\u591a\u8bed\u8a00\u7406\u89e3\u548c\u591a\u8df3\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u5347\u533b\u7597\u573a\u666f\u4e2d\u7684\u667a\u80fd\u5e94\u6025\u54cd\u5e94\u7cfb\u7edf\u548c\u533b\u5b66\u6559\u80b2\u5e73\u53f0\u7684\u6548\u679c\u3002", "method": "\u53c2\u8d5b\u8005\u9700\u5f00\u53d1\u7b97\u6cd5\u5904\u7406\u89c6\u9891\u548c\u6587\u672c\u6570\u636e\uff0c\u7406\u89e3\u591a\u8bed\u8a00\u67e5\u8be2\uff0c\u5e76\u63d0\u4f9b\u591a\u8df3\u533b\u5b66\u95ee\u9898\u7684\u76f8\u5173\u7b54\u6848\u3002", "result": "M4IVQA\u6311\u6218\u8d5b\u5c06\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u7cfb\u7edf\u5728\u533b\u7597\u9886\u57df\u7684\u521b\u65b0\u63d0\u4f9b\u5e73\u53f0\uff0c\u52a9\u529b\u591a\u8bed\u8a00\u793e\u533a\u7684\u533b\u7597\u6559\u80b2\u3002", "conclusion": "M4IVQA\u6311\u6218\u8d5b\u6709\u671b\u63a8\u52a8\u591a\u6a21\u6001\u533b\u7597\u95ee\u7b54\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u63d0\u5347\u533b\u7597\u5e94\u6025\u548c\u6559\u80b2\u7684\u667a\u80fd\u5316\u6c34\u5e73\u3002"}}
{"id": "2505.06730", "pdf": "https://arxiv.org/pdf/2505.06730", "abs": "https://arxiv.org/abs/2505.06730", "authors": ["Debashish Saha", "Piyush Malik", "Adrika Saha"], "title": "Activity and Subject Detection for UCI HAR Dataset with & without missing Sensor Data", "categories": ["cs.LG"], "comment": null, "summary": "Current studies in Human Activity Recognition (HAR) primarily focus on the\nclassification of activities through sensor data, while there is not much\nemphasis placed on recognizing the individuals performing these activities.\nThis type of classification is very important for developing personalized and\ncontext-sensitive applications. Additionally, the issue of missing sensor data,\nwhich often occurs in practical situations due to hardware malfunctions, has\nnot been explored yet. This paper seeks to fill these voids by introducing a\nlightweight LSTM-based model that can be used to classify both activities and\nsubjects. The proposed model was used to classify the HAR dataset by UCI [1],\nachieving an accuracy of 93.89% in activity recognition (across six\nactivities), nearing the 96.67% benchmark, and an accuracy of 80.19% in subject\nrecognition (involving 30 subjects), thereby establishing a new baseline for\nthis area of research. We then simulate the absence of sensor data to mirror\nreal-world scenarios and incorporate imputation techniques, both with and\nwithout Principal Component Analysis (PCA), to restore incomplete datasets. We\nfound that K-Nearest Neighbors (KNN) imputation performs the best for filling\nthe missing sensor data without PCA because the use of PCA resulted in slightly\nlower accuracy. These results demonstrate how well the framework handles\nmissing sensor data, which is a major step forward in using the Human Activity\nRecognition dataset for reliable classification tasks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLSTM\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u7528\u4e8e\u6d3b\u52a8\u548c\u4eba\u5458\u5206\u7c7b\uff0c\u5e76\u5728UCI HAR\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u9ad8\u7cbe\u5ea6\u3002\u540c\u65f6\u63a2\u7d22\u7f3a\u5931\u4f20\u611f\u5668\u6570\u636e\u586b\u5145\u65b9\u6cd5\uff0c\u53d1\u73b0KNN\u63d2\u8865\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u5f53\u524dHAR\u7814\u7a76\u591a\u5173\u6ce8\u6d3b\u52a8\u5206\u7c7b\u800c\u975e\u4eba\u5458\u8bc6\u522b\uff0c\u4e14\u672a\u89e3\u51b3\u4f20\u611f\u5668\u6570\u636e\u7f3a\u5931\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e9b\u7a7a\u767d\uff0c\u63a8\u52a8\u4e2a\u6027\u5316\u548c\u4e0a\u4e0b\u6587\u654f\u611f\u5e94\u7528\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u7528LSTM\u6a21\u578b\u8fdb\u884c\u6d3b\u52a8\u548c\u4eba\u5458\u5206\u7c7b\uff0c\u6a21\u62df\u6570\u636e\u7f3a\u5931\u573a\u666f\uff0c\u5e76\u6bd4\u8f83KNN\u548cPCA\u63d2\u8865\u6280\u672f\u7684\u6548\u679c\u3002", "result": "\u6d3b\u52a8\u8bc6\u522b\u51c6\u786e\u738793.89%\uff0c\u63a5\u8fd1\u57fa\u51c696.67%\uff1b\u4eba\u5458\u8bc6\u522b\u51c6\u786e\u738780.19%\uff0cKNN\u63d2\u8865\u5728\u65e0PCA\u65f6\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5904\u7406\u7f3a\u5931\u6570\u636e\uff0c\u4e3aHAR\u6570\u636e\u96c6\u7684\u53ef\u9760\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2505.06843", "pdf": "https://arxiv.org/pdf/2505.06843", "abs": "https://arxiv.org/abs/2505.06843", "authors": ["Zihan Guan", "Mengxuan Hu", "Ronghang Zhu", "Sheng Li", "Anil Vullikanti"], "title": "Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety", "categories": ["cs.LG", "cs.CL"], "comment": "26 pages, 13 figures", "summary": "Recent studies have uncovered a troubling vulnerability in the fine-tuning\nstage of large language models (LLMs): even fine-tuning on entirely benign\ndatasets can lead to a significant increase in the harmfulness of LLM outputs.\nBuilding on this finding, our red teaming study takes this threat one step\nfurther by developing a more effective attack. Specifically, we analyze and\nidentify samples within benign datasets that contribute most to safety\ndegradation, then fine-tune LLMs exclusively on these samples. We approach this\nproblem from an outlier detection perspective and propose Self-Inf-N, to detect\nand extract outliers for fine-tuning. Our findings reveal that fine-tuning LLMs\non 100 outlier samples selected by Self-Inf-N in the benign datasets severely\ncompromises LLM safety alignment. Extensive experiments across seven mainstream\nLLMs demonstrate that our attack exhibits high transferability across different\narchitectures and remains effective in practical scenarios. Alarmingly, our\nresults indicate that most existing mitigation strategies fail to defend\nagainst this attack, underscoring the urgent need for more robust alignment\nsafeguards. Codes are available at\nhttps://github.com/GuanZihan/Benign-Samples-Matter.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u5728\u826f\u6027\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u4e5f\u53ef\u80fd\u5bfc\u81f4\u8f93\u51fa\u5371\u5bb3\u6027\u663e\u8457\u589e\u52a0\u3002\u56e2\u961f\u63d0\u51fa\u4e00\u79cd\u653b\u51fb\u65b9\u6cd5Self-Inf-N\uff0c\u901a\u8fc7\u68c0\u6d4b\u548c\u63d0\u53d6\u5f02\u5e38\u6837\u672c\u6765\u5fae\u8c03\u6a21\u578b\uff0c\u5b9e\u9a8c\u8868\u660e\u4ec5\u7528100\u4e2a\u5f02\u5e38\u6837\u672c\u5373\u53ef\u4e25\u91cd\u7834\u574f\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002\u73b0\u6709\u9632\u5fa1\u7b56\u7565\u5bf9\u6b64\u653b\u51fb\u51e0\u4e4e\u65e0\u6548\uff0c\u51f8\u663e\u4e86\u5bf9\u66f4\u9c81\u68d2\u5bf9\u9f50\u4fdd\u62a4\u7684\u9700\u6c42\u3002", "motivation": "\u8fd1\u671f\u7814\u7a76\u8868\u660e\uff0c\u5373\u4fbf\u662f\u826f\u6027\u6570\u636e\u96c6\u7684\u5fae\u8c03\u4e5f\u53ef\u80fd\u5bfc\u81f4LLM\u8f93\u51fa\u5371\u5bb3\u6027\u589e\u52a0\u3002\u672c\u7814\u7a76\u8fdb\u4e00\u6b65\u63a2\u7d22\u8fd9\u4e00\u5a01\u80c1\uff0c\u65e8\u5728\u5f00\u53d1\u66f4\u6709\u6548\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u63ed\u793a\u73b0\u6709\u5b89\u5168\u5bf9\u9f50\u63aa\u65bd\u7684\u4e0d\u8db3\u3002", "method": "\u4ece\u5f02\u5e38\u68c0\u6d4b\u89d2\u5ea6\u51fa\u53d1\uff0c\u63d0\u51faSelf-Inf-N\u65b9\u6cd5\uff0c\u8bc6\u522b\u826f\u6027\u6570\u636e\u96c6\u4e2d\u5bf9\u5b89\u5168\u6027\u5f71\u54cd\u6700\u5927\u7684\u6837\u672c\uff0c\u5e76\u4ec5\u7528\u8fd9\u4e9b\u6837\u672c\u5fae\u8c03LLMs\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4ec5\u7528100\u4e2aSelf-Inf-N\u9009\u51fa\u7684\u5f02\u5e38\u6837\u672c\u5fae\u8c03\u5373\u53ef\u663e\u8457\u7834\u574fLLM\u7684\u5b89\u5168\u6027\u3002\u8be5\u653b\u51fb\u57287\u79cd\u4e3b\u6d41LLMs\u4e0a\u5177\u6709\u9ad8\u8fc1\u79fb\u6027\uff0c\u4e14\u73b0\u6709\u9632\u5fa1\u7b56\u7565\u96be\u4ee5\u62b5\u5fa1\u3002", "conclusion": "\u7814\u7a76\u66b4\u9732\u4e86\u5f53\u524dLLM\u5b89\u5168\u5bf9\u9f50\u7684\u8106\u5f31\u6027\uff0c\u4e9f\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u9632\u62a4\u63aa\u65bd\u3002"}}
{"id": "2505.06731", "pdf": "https://arxiv.org/pdf/2505.06731", "abs": "https://arxiv.org/abs/2505.06731", "authors": ["David Zucker"], "title": "Deeply Explainable Artificial Neural Network", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "While deep learning models have demonstrated remarkable success in numerous\ndomains, their black-box nature remains a significant limitation, especially in\ncritical fields such as medical image analysis and inference. Existing\nexplainability methods, such as SHAP, LIME, and Grad-CAM, are typically applied\npost hoc, adding computational overhead and sometimes producing inconsistent or\nambiguous results. In this paper, we present the Deeply Explainable Artificial\nNeural Network (DxANN), a novel deep learning architecture that embeds\nexplainability ante hoc, directly into the training process. Unlike\nconventional models that require external interpretation methods, DxANN is\ndesigned to produce per-sample, per-feature explanations as part of the forward\npass. Built on a flow-based framework, it enables both accurate predictions and\ntransparent decision-making, and is particularly well-suited for image-based\ntasks. While our focus is on medical imaging, the DxANN architecture is readily\nadaptable to other data modalities, including tabular and sequential data.\nDxANN marks a step forward toward intrinsically interpretable deep learning,\noffering a practical solution for applications where trust and accountability\nare essential.", "AI": {"tldr": "DxANN\u662f\u4e00\u79cd\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u5c06\u53ef\u89e3\u91ca\u6027\u5d4c\u5165\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u540e\u5904\u7406\u65b9\u6cd5\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u7ed3\u679c\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u9ed1\u76d2\u7279\u6027\u5728\u533b\u5b66\u5f71\u50cf\u7b49\u5173\u952e\u9886\u57df\u9650\u5236\u5176\u5e94\u7528\uff0c\u73b0\u6709\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff08\u5982SHAP\u3001LIME\uff09\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u7ed3\u679c\u6a21\u7cca\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6d41\u6846\u67b6\u7684DxANN\u67b6\u6784\uff0c\u5728\u6b63\u5411\u4f20\u64ad\u4e2d\u76f4\u63a5\u751f\u6210\u9010\u6837\u672c\u3001\u9010\u7279\u5f81\u7684\u89e3\u91ca\uff0c\u65e0\u9700\u5916\u90e8\u89e3\u91ca\u65b9\u6cd5\u3002", "result": "DxANN\u5728\u4fdd\u6301\u9884\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u900f\u660e\u51b3\u7b56\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u533b\u5b66\u5f71\u50cf\u7b49\u4efb\u52a1\uff0c\u5e76\u53ef\u63a8\u5e7f\u81f3\u8868\u683c\u548c\u5e8f\u5217\u6570\u636e\u3002", "conclusion": "DxANN\u4e3a\u9700\u53ef\u4fe1\u5ea6\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u672c\u8d28\u53ef\u89e3\u91ca\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u5b9e\u7528\u6027\u8fdb\u5c55\u3002"}}
{"id": "2505.06277", "pdf": "https://arxiv.org/pdf/2505.06277", "abs": "https://arxiv.org/abs/2505.06277", "authors": ["John Song", "Lihao Zhang", "Feng Ye", "Haijian Sun"], "title": "Terahertz Spatial Wireless Channel Modeling with Radio Radiance Field", "categories": ["eess.SP", "cs.AI", "cs.CV", "cs.NI"], "comment": "submitted to IEEE conferences", "summary": "Terahertz (THz) communication is a key enabler for 6G systems, offering\nultra-wide bandwidth and unprecedented data rates. However, THz signal\npropagation differs significantly from lower-frequency bands due to severe free\nspace path loss, minimal diffraction and specular reflection, and prominent\nscattering, making conventional channel modeling and pilot-based estimation\napproaches inefficient. In this work, we investigate the feasibility of\napplying radio radiance field (RRF) framework to the THz band. This method\nreconstructs a continuous RRF using visual-based geometry and sparse THz RF\nmeasurements, enabling efficient spatial channel state information\n(Spatial-CSI) modeling without dense sampling. We first build a fine simulated\nTHz scenario, then we reconstruct the RRF and evaluate the performance in terms\nof both reconstruction quality and effectiveness in THz communication, showing\nthat the reconstructed RRF captures key propagation paths with sparse training\nsamples. Our findings demonstrate that RRF modeling remains effective in the\nTHz regime and provides a promising direction for scalable, low-cost spatial\nchannel reconstruction in future 6G networks.", "AI": {"tldr": "\u7814\u7a76\u4e86\u901a\u8fc7\u65e0\u7ebf\u7535\u8f90\u5c04\u573a\uff08RRF\uff09\u6846\u67b6\u5728\u592a\u8d6b\u5179\uff08THz\uff09\u9891\u6bb5\u5e94\u7528\u7684\u53ef\u884c\u6027\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u89c6\u89c9\u51e0\u4f55\u548c\u7a00\u758fTHz\u5c04\u9891\u6d4b\u91cf\u91cd\u5efa\u8fde\u7eedRRF\uff0c\u6709\u6548\u5efa\u6a21\u7a7a\u95f4\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff0c\u907f\u514d\u4e86\u5bc6\u96c6\u91c7\u6837\u3002", "motivation": "\u592a\u8d6b\u5179\u901a\u4fe1\u57286G\u7cfb\u7edf\u4e2d\u5177\u6709\u8d85\u9ad8\u5e26\u5bbd\u548c\u901f\u7387\u6f5c\u529b\uff0c\u4f46THz\u4fe1\u53f7\u4f20\u64ad\u7279\u6027\u5bfc\u81f4\u4f20\u7edf\u4fe1\u9053\u5efa\u6a21\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u6b64\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u5efa\u6a21\u65b9\u6cd5\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u6784\u5efa\u7cbe\u7ec6\u7684\u6a21\u62dfTHz\u573a\u666f\uff0c\u5229\u7528\u89c6\u89c9\u51e0\u4f55\u548c\u7a00\u758f\u6d4b\u91cf\u91cd\u5efaRRF\uff0c\u8bc4\u4f30\u5176\u5728THz\u901a\u4fe1\u4e2d\u7684\u91cd\u5efa\u8d28\u91cf\u548c\u6709\u6548\u6027\u3002", "result": "\u91cd\u5efa\u7684RRF\u80fd\u591f\u901a\u8fc7\u7a00\u758f\u8bad\u7ec3\u6837\u672c\u6355\u6349\u5173\u952e\u4f20\u64ad\u8def\u5f84\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728THz\u9891\u6bb5\u7684\u6709\u6548\u6027\u3002", "conclusion": "RRF\u5efa\u6a21\u5728THz\u9891\u6bb5\u4ecd\u6709\u6548\uff0c\u4e3a\u672a\u67656G\u7f51\u7edc\u4e2d\u4f4e\u6210\u672c\u3001\u53ef\u6269\u5c55\u7684\u7a7a\u95f4\u4fe1\u9053\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2505.06898", "pdf": "https://arxiv.org/pdf/2505.06898", "abs": "https://arxiv.org/abs/2505.06898", "authors": ["Honglong Yang", "Shanshan Song", "Yi Qin", "Lehan Wang", "Haonan Wang", "Xinpeng Ding", "Qixiang Zhang", "Bodong Du", "Xiaomeng Li"], "title": "Multi-Modal Explainable Medical AI Assistant for Trustworthy Human-AI Collaboration", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Generalist Medical AI (GMAI) systems have demonstrated expert-level\nperformance in biomedical perception tasks, yet their clinical utility remains\nlimited by inadequate multi-modal explainability and suboptimal prognostic\ncapabilities. Here, we present XMedGPT, a clinician-centric, multi-modal AI\nassistant that integrates textual and visual interpretability to support\ntransparent and trustworthy medical decision-making. XMedGPT not only produces\naccurate diagnostic and descriptive outputs, but also grounds referenced\nanatomical sites within medical images, bridging critical gaps in\ninterpretability and enhancing clinician usability. To support real-world\ndeployment, we introduce a reliability indexing mechanism that quantifies\nuncertainty through consistency-based assessment via interactive\nquestion-answering. We validate XMedGPT across four pillars: multi-modal\ninterpretability, uncertainty quantification, and prognostic modeling, and\nrigorous benchmarking. The model achieves an IoU of 0.703 across 141 anatomical\nregions, and a Kendall's tau-b of 0.479, demonstrating strong alignment between\nvisual rationales and clinical outcomes. For uncertainty estimation, it attains\nan AUC of 0.862 on visual question answering and 0.764 on radiology report\ngeneration. In survival and recurrence prediction for lung and glioma cancers,\nit surpasses prior leading models by 26.9%, and outperforms GPT-4o by 25.0%.\nRigorous benchmarking across 347 datasets covers 40 imaging modalities and\nexternal validation spans 4 anatomical systems confirming exceptional\ngeneralizability, with performance gains surpassing existing GMAI by 20.7% for\nin-domain evaluation and 16.7% on 11,530 in-house data evaluation. Together,\nXMedGPT represents a significant leap forward in clinician-centric AI\nintegration, offering trustworthy and scalable support for diverse healthcare\napplications.", "AI": {"tldr": "XMedGPT\u662f\u4e00\u79cd\u591a\u6a21\u6001AI\u52a9\u624b\uff0c\u901a\u8fc7\u589e\u5f3a\u89e3\u91ca\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u5347\u533b\u7597\u51b3\u7b56\u7684\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u901a\u7528\u533b\u7597AI\u7cfb\u7edf\u5728\u89e3\u91ca\u6027\u548c\u9884\u540e\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u4e34\u5e8a\u51b3\u7b56\u7684\u53ef\u4fe1\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "method": "\u6574\u5408\u6587\u672c\u548c\u89c6\u89c9\u89e3\u91ca\u6027\uff0c\u5f15\u5165\u53ef\u9760\u6027\u7d22\u5f15\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u4ea4\u4e92\u5f0f\u95ee\u7b54\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u89e3\u5256\u533a\u57df\u5b9a\u4f4d\uff08IoU 0.703\uff09\u3001\u9884\u540e\u5efa\u6a21\uff08\u63d0\u534726.9%\uff09\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u5e76\u5728347\u4e2a\u6570\u636e\u96c6\u4e2d\u9a8c\u8bc1\u4e86\u901a\u7528\u6027\u3002", "conclusion": "XMedGPT\u901a\u8fc7\u591a\u6a21\u6001\u89e3\u91ca\u548c\u53ef\u9760\u6027\u673a\u5236\uff0c\u663e\u8457\u63a8\u52a8\u4e86\u4e34\u5e8aAI\u7684\u5b9e\u7528\u5316\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2505.06744", "pdf": "https://arxiv.org/pdf/2505.06744", "abs": "https://arxiv.org/abs/2505.06744", "authors": ["Kai M\u00fcller", "Martin Wenzel", "Tobias Windisch"], "title": "LineFlow: A Framework to Learn Active Control of Production Lines", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted at ICML 2025", "summary": "Many production lines require active control mechanisms, such as adaptive\nrouting, worker reallocation, and rescheduling, to maintain optimal\nperformance. However, designing these control systems is challenging for\nvarious reasons, and while reinforcement learning (RL) has shown promise in\naddressing these challenges, a standardized and general framework is still\nlacking. In this work, we introduce LineFlow, an extensible, open-source Python\nframework for simulating production lines of arbitrary complexity and training\nRL agents to control them. To demonstrate the capabilities and to validate the\nunderlying theoretical assumptions of LineFlow, we formulate core subproblems\nof active line control in ways that facilitate mathematical analysis. For each\nproblem, we provide optimal solutions for comparison. We benchmark\nstate-of-the-art RL algorithms and show that the learned policies approach\noptimal performance in well-understood scenarios. However, for more complex,\nindustrial-scale production lines, RL still faces significant challenges,\nhighlighting the need for further research in areas such as reward shaping,\ncurriculum learning, and hierarchical control.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86LineFlow\uff0c\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5f00\u6e90Python\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u4efb\u610f\u590d\u6742\u5ea6\u7684\u751f\u4ea7\u7ebf\u5e76\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4ee3\u7406\u63a7\u5236\u5b83\u4eec\u3002\u901a\u8fc7\u6838\u5fc3\u5b50\u95ee\u9898\u7684\u6570\u5b66\u5206\u6790\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86RL\u5728\u7b80\u5355\u573a\u666f\u4e2d\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\uff0c\u4f46\u5728\u590d\u6742\u5de5\u4e1a\u89c4\u6a21\u751f\u4ea7\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\u3002", "motivation": "\u751f\u4ea7\u7ebf\u7684\u52a8\u6001\u63a7\u5236\uff08\u5982\u81ea\u9002\u5e94\u8def\u7531\u3001\u5de5\u4eba\u91cd\u65b0\u5206\u914d\u548c\u91cd\u65b0\u8c03\u5ea6\uff09\u8bbe\u8ba1\u590d\u6742\u4e14\u7f3a\u4e4f\u901a\u7528\u6846\u67b6\uff0c\u5f3a\u5316\u5b66\u4e60\u867d\u5177\u6f5c\u529b\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u56e0\u6b64\u9700\u8981LineFlow\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faLineFlow\u6846\u67b6\uff0c\u652f\u6301\u6a21\u62df\u590d\u6742\u751f\u4ea7\u7ebf\u5e76\u8bad\u7ec3RL\u4ee3\u7406\uff1b\u901a\u8fc7\u6570\u5b66\u5206\u6790\u9a8c\u8bc1\u5176\u7406\u8bba\u5047\u8bbe\uff0c\u5e76\u4e3a\u5b50\u95ee\u9898\u63d0\u4f9b\u6700\u4f18\u89e3\u4f5c\u4e3a\u5bf9\u6bd4\u57fa\u51c6\u3002", "result": "RL\u7b97\u6cd5\u5728\u5df2\u77e5\u573a\u666f\u4e2d\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\uff0c\u4f46\u5728\u5de5\u4e1a\u7ea7\u590d\u6742\u751f\u4ea7\u7ebf\u4e0a\u4ecd\u5b58\u5728\u663e\u8457\u6311\u6218\uff0c\u4f8b\u5982\u5956\u52b1\u8bbe\u8ba1\u3001\u6e10\u8fdb\u5b66\u4e60\u548c\u5206\u5c42\u63a7\u5236\u7684\u9700\u6c42\u3002", "conclusion": "LineFlow\u4e3a\u751f\u4ea7\u7ebfRL\u63a7\u5236\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u5de5\u5177\uff0c\u4f46\u590d\u6742\u573a\u666f\u7684\u4f18\u5316\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2505.06299", "pdf": "https://arxiv.org/pdf/2505.06299", "abs": "https://arxiv.org/abs/2505.06299", "authors": ["Spyridon Raptis", "Haralampos-G. Stratigopoulos"], "title": "Input-Specific and Universal Adversarial Attack Generation for Spiking Neural Networks in the Spiking Domain", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "As Spiking Neural Networks (SNNs) gain traction across various applications,\nunderstanding their security vulnerabilities becomes increasingly important. In\nthis work, we focus on the adversarial attacks, which is perhaps the most\nconcerning threat. An adversarial attack aims at finding a subtle input\nperturbation to fool the network's decision-making. We propose two novel\nadversarial attack algorithms for SNNs: an input-specific attack that crafts\nadversarial samples from specific dataset inputs and a universal attack that\ngenerates a reusable patch capable of inducing misclassification across most\ninputs, thus offering practical feasibility for real-time deployment. The\nalgorithms are gradient-based operating in the spiking domain proving to be\neffective across different evaluation metrics, such as adversarial accuracy,\nstealthiness, and generation time. Experimental results on two widely used\nneuromorphic vision datasets, NMNIST and IBM DVS Gesture, show that our\nproposed attacks surpass in all metrics all existing state-of-the-art methods.\nAdditionally, we present the first demonstration of adversarial attack\ngeneration in the sound domain using the SHD dataset.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u9488\u5bf9\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u7684\u65b0\u578b\u5bf9\u6297\u653b\u51fb\u7b97\u6cd5\uff1a\u4e00\u79cd\u57fa\u4e8e\u7279\u5b9a\u8f93\u5165\u7684\u653b\u51fb\u548c\u4e00\u79cd\u901a\u7528\u653b\u51fb\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8fd9\u4e24\u79cd\u7b97\u6cd5\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u7684\u666e\u53ca\uff0c\u7814\u7a76\u5176\u5b89\u5168\u6f0f\u6d1e\uff08\u5c24\u5176\u662f\u5bf9\u6297\u653b\u51fb\uff09\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u8bc4\u4f30\u5176\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u68af\u5ea6\u9a71\u52a8\u7684\u5bf9\u6297\u653b\u51fb\u7b97\u6cd5\uff1a\u4e00\u79cd\u9488\u5bf9\u7279\u5b9a\u8f93\u5165\u751f\u6210\u5bf9\u6297\u6837\u672c\uff0c\u53e6\u4e00\u79cd\u751f\u6210\u53ef\u7528\u4e8e\u591a\u6570\u8f93\u5165\u7684\u53ef\u590d\u7528\u7684\u901a\u7528\u5bf9\u6297\u8865\u4e01\u3002\u8fd9\u4e9b\u7b97\u6cd5\u5728\u8109\u51b2\u57df\u4e2d\u64cd\u4f5c\uff0c\u4f18\u5316\u591a\u4e2a\u6307\u6807\u3002", "result": "\u5728NMNIST\u548cIBM DVS Gesture\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u653b\u51fb\u7b97\u6cd5\u5728\u5bf9\u6297\u51c6\u786e\u6027\u3001\u9690\u853d\u6027\u548c\u751f\u6210\u65f6\u95f4\u7b49\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u9996\u6b21\u5728\u58f0\u97f3\u57df\uff08SHD\u6570\u636e\u96c6\uff09\u4e2d\u9a8c\u8bc1\u4e86\u5bf9\u6297\u653b\u51fb\u7684\u751f\u6210\u3002", "conclusion": "\u8bba\u6587\u8bc1\u5b9e\u4e86SNNs\u5bf9\u5bf9\u6297\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u5e76\u63d0\u51fa\u9ad8\u6548\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u9632\u5fa1\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2505.06938", "pdf": "https://arxiv.org/pdf/2505.06938", "abs": "https://arxiv.org/abs/2505.06938", "authors": ["Katarzyna Anna Kapitan"], "title": "A digital perspective on the role of a stemma in material-philological transmission studies", "categories": ["cs.DL", "cs.CL"], "comment": null, "summary": "Taking its point of departure in the recent developments in the field of\ndigital humanities and the increasing automatisation of scholarly workflows,\nthis study explores the implications of digital approaches to textual\ntraditions for the broader field of textual scholarship. It argues that the\nrelative simplicity of creating computergenerated stemmas allows us to view the\nstemma codicum as a research tool rather than the final product of our\nscholarly investigation. Using the Old Norse saga of Hr\\'omundur as a case\nstudy, this article demonstrates that stemmas can serve as a starting point for\nexploring textual traditions further. In doing so, they enable us to address\nresearch questions that otherwise remain unanswered. The article is accompanied\nby datasets used to generate stemmas for the Hr\\'omundar saga tradition as well\nas two custom Python scripts. The scripts are designed to convert XML-based\ntextual data, encoded according to the TEI Guidelines, into the input format\nused for the analysis in the PHYLIP package to generate unrooted trees of\nrelationships between texts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u6570\u5b57\u65b9\u6cd5\u5bf9\u6587\u672c\u4f20\u7edf\u7684\u5e7f\u6cdb\u5f71\u54cd\uff0c\u63d0\u51fa\u8ba1\u7b97\u673a\u751f\u6210\u8c31\u7cfb\u56fe\u5e94\u89c6\u4e3a\u7814\u7a76\u5de5\u5177\u800c\u975e\u6700\u7ec8\u6210\u679c\uff0c\u5e76\u4ee5\u53e4\u632a\u5a01\u4f20\u5947\u4e3a\u4f8b\u5c55\u793a\u4e86\u5176\u5e94\u7528\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u4eba\u6587\u7684\u5feb\u901f\u53d1\u5c55\u548c\u5b66\u672f\u5de5\u4f5c\u6d41\u7a0b\u7684\u81ea\u52a8\u5316\uff0c\u4f5c\u8005\u5e0c\u671b\u63a2\u7d22\u6570\u5b57\u65b9\u6cd5\u5728\u6587\u672c\u4f20\u7edf\u7814\u7a76\u4e2d\u7684\u4f5c\u7528\uff0c\u7279\u522b\u662f\u8c31\u7cfb\u56fe\u4f5c\u4e3a\u7814\u7a76\u5de5\u5177\u7684\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u4ee5\u53e4\u632a\u5a01\u4f20\u5947Hr\\'omundur\u4e3a\u4f8b\uff0c\u7528Python\u811a\u672c\u5c06TEI\u7f16\u7801\u7684XML\u6570\u636e\u8f6c\u6362\u4e3aPHYLIP\u8f6f\u4ef6\u53ef\u5206\u6790\u7684\u683c\u5f0f\uff0c\u751f\u6210\u6587\u672c\u95f4\u5173\u7cfb\u7684\u65e0\u6839\u6811\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8ba1\u7b97\u673a\u751f\u6210\u7684\u8c31\u7cfb\u56fe\u53ef\u4ee5\u4f5c\u4e3a\u6df1\u5165\u7814\u7a76\u6587\u672c\u4f20\u7edf\u7684\u8d77\u70b9\uff0c\u89e3\u7b54\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5173\u7684\u6570\u636e\u96c6\u548c\u811a\u672c\u4f5c\u4e3a\u652f\u6301\u3002", "conclusion": "\u8c31\u7cfb\u56fe\u4e0d\u5e94\u4ec5\u88ab\u89c6\u4e3a\u7814\u7a76\u7684\u6700\u7ec8\u6210\u679c\uff0c\u800c\u5e94\u4f5c\u4e3a\u52a8\u6001\u5de5\u5177\u4fc3\u8fdb\u5bf9\u6587\u672c\u4f20\u7edf\u7684\u66f4\u6df1\u5165\u63a2\u7d22\uff0c\u6570\u5b57\u65b9\u6cd5\u4e3a\u6587\u672c\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2505.06753", "pdf": "https://arxiv.org/pdf/2505.06753", "abs": "https://arxiv.org/abs/2505.06753", "authors": ["Muhamed Amin", "Bernard R. Brooks"], "title": "Boltzmann Classifier: A Thermodynamic-Inspired Approach to Supervised Learning", "categories": ["cs.LG", "physics.comp-ph"], "comment": null, "summary": "We propose a novel classification algorithm, the Boltzmann Classifier,\ninspired by the thermodynamic principles underlying the Boltzmann distribution.\nOur method computes a probabilistic estimate for each class based on an energy\nfunction derived from feature-wise deviations between input samples and\nclass-specific centroids. The resulting probabilities are proportional to the\nexponential negative energies, normalized across classes, analogous to the\nBoltzmann distribution used in statistical mechanics. In addition, the KT\nvariable can be used to allow the high energy states to be more accessible,\nwhich allows the tuning of their probabilities as needed. We evaluate the model\nperformance on several datasets from different applications. The model achieves\na high accuracy, which indicates that the Boltzmann Classifier is competitive\nwith standard models like logistic regression and k-nearest neighbors while\noffering a thermodynamically motivated probabilistic interpretation. our\nclassifier does not require iterative optimization or backpropagation and is\nthus computationally efficient and easy to integrate into existing workflows.\nThis work demonstrates how ideas from physics can inform new directions in\nmachine learning, providing a foundation for interpretable, energy-based\ndecision-making systems.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u7684\u70ed\u529b\u5b66\u539f\u7406\u7684\u65b0\u578b\u5206\u7c7b\u7b97\u6cd5\u2014\u2014\u73bb\u5c14\u5179\u66fc\u5206\u7c7b\u5668\uff0c\u901a\u8fc7\u7279\u5f81\u504f\u5dee\u8ba1\u7b97\u7c7b\u6982\u7387\uff0c\u65e0\u9700\u8fed\u4ee3\u4f18\u5316\uff0c\u8ba1\u7b97\u9ad8\u6548\u4e14\u6613\u4e8e\u96c6\u6210\u3002", "motivation": "\u53d7\u70ed\u529b\u5b66\u4e2d\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u7684\u542f\u53d1\uff0c\u8bbe\u8ba1\u4e00\u79cd\u65b0\u578b\u5206\u7c7b\u7b97\u6cd5\uff0c\u63d0\u4f9b\u57fa\u4e8e\u80fd\u91cf\u51fd\u6570\u7684\u6982\u7387\u89e3\u91ca\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u9ad8\u6548\u6027\u548c\u6613\u7528\u6027\u3002", "method": "\u901a\u8fc7\u6837\u672c\u7279\u5f81\u4e0e\u7c7b\u7279\u5b9a\u8d28\u5fc3\u7684\u504f\u5dee\u8ba1\u7b97\u80fd\u91cf\u51fd\u6570\uff0c\u5229\u7528\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u751f\u6210\u5f52\u4e00\u5316\u7684\u7c7b\u6982\u7387\u3002KT\u53d8\u91cf\u7684\u5f15\u5165\u53ef\u8c03\u8282\u9ad8\u80fd\u91cf\u72b6\u6001\u7684\u53ef\u8bbf\u95ee\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u5206\u7c7b\u5668\u5728\u51c6\u786e\u6027\u4e0a\u4e0e\u903b\u8f91\u56de\u5f52\u548cK\u8fd1\u90bb\u7b49\u6807\u51c6\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u5177\u6709\u8ba1\u7b97\u9ad8\u6548\u6027\u548c\u7269\u7406\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u73bb\u5c14\u5179\u66fc\u5206\u7c7b\u5668\u5c55\u793a\u4e86\u7269\u7406\u5b66\u601d\u60f3\u5982\u4f55\u4e3a\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u65b0\u65b9\u5411\uff0c\u4e3a\u53ef\u89e3\u91ca\u7684\u57fa\u4e8e\u80fd\u91cf\u7684\u51b3\u7b56\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.06972", "pdf": "https://arxiv.org/pdf/2505.06972", "abs": "https://arxiv.org/abs/2505.06972", "authors": ["Yuichi Sasazawa", "Yasuhiro Sogawa"], "title": "Web Page Classification using LLMs for Crawling Support", "categories": ["cs.IR", "cs.CL"], "comment": "8 pages, 2 figures", "summary": "A web crawler is a system designed to collect web pages, and efficient\ncrawling of new pages requires appropriate algorithms. While website features\nsuch as XML sitemaps and the frequency of past page updates provide important\nclues for accessing new pages, their universal application across diverse\nconditions is challenging. In this study, we propose a method to efficiently\ncollect new pages by classifying web pages into two types, \"Index Pages\" and\n\"Content Pages,\" using a large language model (LLM), and leveraging the\nclassification results to select index pages as starting points for accessing\nnew pages. We construct a dataset with automatically annotated web page types\nand evaluate our approach from two perspectives: the page type classification\nperformance and coverage of new pages. Experimental results demonstrate that\nthe LLM-based method outperformed baseline methods in both evaluation metrics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5c06\u7f51\u9875\u5206\u7c7b\u4e3a\u201c\u7d22\u5f15\u9875\u201d\u548c\u201c\u5185\u5bb9\u9875\u201d\uff0c\u5e76\u57fa\u4e8e\u5206\u7c7b\u7ed3\u679c\u9009\u62e9\u7d22\u5f15\u9875\u4f5c\u4e3a\u65b0\u9875\u9762\u722c\u53d6\u8d77\u70b9\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5206\u7c7b\u6027\u80fd\u548c\u65b0\u9875\u9762\u8986\u76d6\u7387\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u9488\u5bf9\u591a\u6837\u6761\u4ef6\u4e0b\u9ad8\u6548\u722c\u53d6\u65b0\u9875\u9762\u7684\u6311\u6218\uff0c\u7814\u7a76\u901a\u8fc7\u5229\u7528\u7f51\u7ad9\u7279\u5f81\uff08\u5982XML\u7ad9\u70b9\u5730\u56fe\u548c\u9875\u9762\u66f4\u65b0\u9891\u7387\uff09\u5e76\u7ed3\u5408LLM\u5206\u7c7b\u7f51\u9875\u7c7b\u578b\uff0c\u65e8\u5728\u63d0\u5347\u65b0\u9875\u9762\u91c7\u96c6\u6548\u7387\u3002", "method": "\u4f7f\u7528LLM\u5c06\u7f51\u9875\u5206\u4e3a\u201c\u7d22\u5f15\u9875\u201d\u548c\u201c\u5185\u5bb9\u9875\u201d\u4e24\u7c7b\uff0c\u6784\u5efa\u81ea\u52a8\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u5e76\u4ee5\u7d22\u5f15\u9875\u4e3a\u8d77\u70b9\u722c\u53d6\u65b0\u9875\u9762\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5728\u9875\u9762\u7c7b\u578b\u5206\u7c7b\u6027\u80fd\u548c\u65b0\u9875\u9762\u8986\u76d6\u7387\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7LLM\u5206\u7c7b\u7f51\u9875\u7c7b\u578b\u5e76\u4f18\u5316\u722c\u53d6\u7b56\u7565\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u65b0\u9875\u9762\u91c7\u96c6\u7684\u6548\u7387\u548c\u8986\u76d6\u7387\u3002"}}
{"id": "2505.06759", "pdf": "https://arxiv.org/pdf/2505.06759", "abs": "https://arxiv.org/abs/2505.06759", "authors": ["Xavier Mart\u00ednez-Lua\u00f1a", "Manuel Fern\u00e1ndez-Veiga", "Rebeca P. D\u00edaz-Redondo", "Ana Fern\u00e1ndez-Vilas"], "title": "Privacy-aware Berrut Approximated Coded Computing applied to general distributed learning", "categories": ["cs.LG", "cs.CR", "cs.DC", "cs.IT", "math.IT"], "comment": null, "summary": "Coded computing is one of the techniques that can be used for privacy\nprotection in Federated Learning. However, most of the constructions used for\ncoded computing work only under the assumption that the computations involved\nare exact, generally restricted to special classes of functions, and require\nquantized inputs. This paper considers the use of Private Berrut Approximate\nCoded Computing (PBACC) as a general solution to add strong but non-perfect\nprivacy to federated learning. We derive new adapted PBACC algorithms for\ncentralized aggregation, secure distributed training with centralized data, and\nsecure decentralized training with decentralized data, thus enlarging\nsignificantly the applications of the method and the existing privacy\nprotection tools available for these paradigms. Particularly, PBACC can be used\nrobustly to attain privacy guarantees in decentralized federated learning for a\nvariety of models. Our numerical results show that the achievable quality of\ndifferent learning models (convolutional neural networks, variational\nautoencoders, and Cox regression) is minimally altered by using these new\ncomputing schemes, and that the privacy leakage can be bounded strictly to less\nthan a fraction of one bit per participant. Additionally, the computational\ncost of the encoding and decoding processes depends only of the degree of\ndecentralization of the data.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8ePrivate Berrut\u8fd1\u4f3c\u7f16\u7801\u8ba1\u7b97\uff08PBACC\uff09\u7684\u8054\u90a6\u5b66\u4e60\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\uff0c\u63d0\u51fa\u65b0\u7b97\u6cd5\u652f\u6301\u96c6\u4e2d\u5f0f\u548c\u5206\u6563\u5f0f\u8bad\u7ec3\uff0c\u9690\u79c1\u6cc4\u9732\u53ef\u4e25\u683c\u9650\u5236\u5e76\u9002\u7528\u4e8e\u591a\u79cd\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7f16\u7801\u8ba1\u7b97\u6280\u672f\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u4ec5\u9002\u7528\u4e8e\u7cbe\u786e\u8ba1\u7b97\u548c\u7279\u5b9a\u51fd\u6570\u7c7b\u522b\uff0c\u9650\u5236\u4e86\u9690\u79c1\u4fdd\u62a4\u7684\u901a\u7528\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7PBACC\u4e3a\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u66f4\u901a\u7528\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u96c6\u4e2d\u5f0f\u805a\u5408\u3001\u96c6\u4e2d\u6570\u636e\u7684\u5b89\u5168\u5206\u5e03\u5f0f\u8bad\u7ec3\u53ca\u5206\u6563\u6570\u636e\u7684\u5b89\u5168\u5206\u6563\u8bad\u7ec3\u7684PBACC\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPBACC\u5bf9\u4e0d\u540c\u6a21\u578b\uff08\u5982CNN\u3001VAE\u7b49\uff09\u7684\u6027\u80fd\u5f71\u54cd\u6781\u5c0f\uff0c\u4e14\u9690\u79c1\u6cc4\u9732\u53ef\u4e25\u683c\u9650\u5236\uff08\u6bcf\u53c2\u4e0e\u8005\u5c11\u4e8e1\u6bd4\u7279\uff09\u3002\u8ba1\u7b97\u6210\u672c\u4ec5\u53d6\u51b3\u4e8e\u6570\u636e\u5206\u6563\u7a0b\u5ea6\u3002", "conclusion": "PBACC\u663e\u8457\u6269\u5c55\u4e86\u9690\u79c1\u4fdd\u62a4\u5de5\u5177\u7684\u5e94\u7528\u8303\u56f4\uff0c\u4e3a\u5206\u6563\u5f0f\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u9690\u79c1\u4fdd\u969c\u65b9\u6848\u3002"}}
{"id": "2505.06993", "pdf": "https://arxiv.org/pdf/2505.06993", "abs": "https://arxiv.org/abs/2505.06993", "authors": ["Yuxuan He", "Junpeng Zhang", "Hongyuan Zhang", "Quanshi Zhang"], "title": "Towards the Three-Phase Dynamics of Generalization Power of a DNN", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper proposes a new perspective for analyzing the generalization power\nof deep neural networks (DNNs), i.e., directly disentangling and analyzing the\ndynamics of generalizable and non-generalizable interaction encoded by a DNN\nthrough the training process. Specifically, this work builds upon the recent\ntheoretical achievement in explainble AI, which proves that the detailed\ninference logic of DNNs can be can be strictly rewritten as a small number of\nAND-OR interaction patterns. Based on this, we propose an efficient method to\nquantify the generalization power of each interaction, and we discover a\ndistinct three-phase dynamics of the generalization power of interactions\nduring training. In particular, the early phase of training typically removes\nnoisy and non-generalizable interactions and learns simple and generalizable\nones. The second and the third phases tend to capture increasingly complex\ninteractions that are harder to generalize. Experimental results verify that\nthe learning of non-generalizable interactions is the the direct cause for the\ngap between the training and testing losses.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u6790\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u6cdb\u5316\u80fd\u529b\u7684\u65b0\u89c6\u89d2\uff0c\u901a\u8fc7\u76f4\u63a5\u89e3\u8026\u548c\u5206\u6790DNN\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7f16\u7801\u7684\u53ef\u6cdb\u5316\u548c\u975e\u6cdb\u5316\u4ea4\u4e92\u7684\u52a8\u6001\u53d8\u5316\u3002\u7814\u7a76\u53d1\u73b0\u4ea4\u4e92\u7684\u6cdb\u5316\u80fd\u529b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5448\u73b0\u4e09\u6bb5\u52a8\u6001\uff0c\u5e76\u4e3a\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u635f\u5931\u95f4\u7684\u5dee\u8ddd\u63d0\u4f9b\u4e86\u76f4\u63a5\u89e3\u91ca\u3002", "motivation": "\u5f53\u524d\u5bf9DNN\u6cdb\u5316\u80fd\u529b\u7684\u7406\u89e3\u8fd8\u4e0d\u591f\u6df1\u5165\uff0c\u5c24\u5176\u662f\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4ea4\u4e92\u7684\u52a8\u6001\u53d8\u5316\u5982\u4f55\u5f71\u54cd\u6cdb\u5316\u6027\u80fd\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u91cf\u5316\u4ea4\u4e92\u7684\u6cdb\u5316\u80fd\u529b\u6765\u5206\u6790DNN\u7684\u5b66\u4e60\u884c\u4e3a\u3002", "method": "\u57fa\u4e8e\u53ef\u89e3\u91caAI\u7684\u6700\u65b0\u7406\u8bba\u6210\u679c\uff0c\u5c06DNN\u7684\u63a8\u7406\u903b\u8f91\u4e25\u683c\u6539\u5199\u4e3a\u5c11\u91cfAND-OR\u4ea4\u4e92\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u91cf\u5316\u4ea4\u4e92\u6cdb\u5316\u80fd\u529b\u7684\u9ad8\u6548\u65b9\u6cd5\u3002\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4ea4\u4e92\u7684\u52a8\u6001\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u4ea4\u4e92\u6cdb\u5316\u80fd\u529b\u7684\u4e09\u6bb5\u52a8\u6001\uff1a\u65e9\u671f\u53bb\u9664\u566a\u58f0\u548c\u975e\u6cdb\u5316\u4ea4\u4e92\uff0c\u4e2d\u540e\u671f\u5b66\u4e60\u66f4\u590d\u6742\u4f46\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u7684\u4ea4\u4e92\u3002\u975e\u6cdb\u5316\u4ea4\u4e92\u7684\u5b66\u4e60\u662f\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u635f\u5931\u5dee\u8ddd\u7684\u76f4\u63a5\u539f\u56e0\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aDNN\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u89c6\u89d2\uff0c\u63ed\u793a\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4ea4\u4e92\u52a8\u6001\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u6539\u8fdb\u6a21\u578b\u6cdb\u5316\u6027\u80fd\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2505.06761", "pdf": "https://arxiv.org/pdf/2505.06761", "abs": "https://arxiv.org/abs/2505.06761", "authors": ["Youcef Djenouri", "Nassim Belmecheri", "Tomasz Michalak", "Jan Dubi\u0144ski", "Ahmed Nabil Belbachir", "Anis Yazidi"], "title": "Learning Graph Representation of Agent Diffuser", "categories": ["cs.LG", "cs.MA"], "comment": "Accepted at AAMAS2025 International Conference on Autonomous Agents\n  and Multiagent Systems", "summary": "Diffusion-based generative models have significantly advanced text-to-image\nsynthesis, demonstrating impressive text comprehension and zero-shot\ngeneralization. These models refine images from random noise based on textual\nprompts, with initial reliance on text input shifting towards enhanced visual\nfidelity over time. This transition suggests that static model parameters might\nnot optimally address the distinct phases of generation. We introduce LGR-AD\n(Learning Graph Representation of Agent Diffusers), a novel multi-agent system\ndesigned to improve adaptability in dynamic computer vision tasks. LGR-AD\nmodels the generation process as a distributed system of interacting agents,\neach representing an expert sub-model. These agents dynamically adapt to\nvarying conditions and collaborate through a graph neural network that encodes\ntheir relationships and performance metrics. Our approach employs a\ncoordination mechanism based on top-$k$ maximum spanning trees, optimizing the\ngeneration process. Each agent's decision-making is guided by a meta-model that\nminimizes a novel loss function, balancing accuracy and diversity. Theoretical\nanalysis and extensive empirical evaluations show that LGR-AD outperforms\ntraditional diffusion models across various benchmarks, highlighting its\npotential for scalable and flexible solutions in complex image generation\ntasks. Code is available at: https://github.com/YousIA/LGR_AD", "AI": {"tldr": "LGR-AD \u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u52a8\u6001\u534f\u8c03\u673a\u5236\u6539\u8fdb\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u9759\u6001\u6a21\u578b\u53c2\u6570\u53ef\u80fd\u65e0\u6cd5\u9002\u5e94\u751f\u6210\u8fc7\u7a0b\u7684\u4e0d\u540c\u9636\u6bb5\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002", "method": "\u63d0\u51fa\u4e86LGR-AD\uff0c\u8fd9\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u7f16\u7801\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u6700\u5927\u751f\u6210\u6811\u7684\u534f\u8c03\u673a\u5236\u4f18\u5316\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u9a8c\u8868\u660e\uff0cLGR-AD\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u4f20\u7edf\u6269\u6563\u6a21\u578b\u3002", "conclusion": "LGR-AD \u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u7075\u6d3b\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7684\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u3002"}}
{"id": "2505.06762", "pdf": "https://arxiv.org/pdf/2505.06762", "abs": "https://arxiv.org/abs/2505.06762", "authors": ["Junfeng Jiao", "Seung Gyu Baik", "Seung Jun Choi", "Yiming Xu"], "title": "Investigating Robotaxi Crash Severity Using Geographical Random Forest", "categories": ["cs.LG", "cs.RO"], "comment": "21 pages, 8 figures", "summary": "This paper quantitatively investigates the crash severity of Autonomous\nVehicles (AVs) with spatially localized machine learning and macroscopic\nmeasures of the urban built environment. We address spatial heterogeneity and\nspatial autocorrelation, while focusing on land use patterns and human\nbehavior. Our Geographical Random Forest (GRF) model, accompanied with a crash\nseverity risk map of San Francisco, presents three findings that are useful for\ncommercial operations of AVs and robotaxis. First, spatially localized machine\nlearning performed better than regular machine learning, when predicting AV\ncrash severity. Bias-variance tradeoff was evident as we adjust the\nlocalization weight hyperparameter. Second, land use was the most important\nbuilt environment measure, compared to intersections, building footprints,\npublic transit stops, and Points Of Interests (POIs). Third, it was predicted\nthat city center areas with greater diversity and commercial activities were\nmore likely to result in low-severity AV crashes, than residential\nneighborhoods. Residential land use may be associated with higher severity due\nto human behavior and less restrictive environment. This paper recommends to\nexplicitly consider geographic locations, and to design safety measures\nspecific to residential neighborhoods, when robotaxi operators train their AV\nsystems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4f7f\u7528\u7a7a\u95f4\u5c40\u90e8\u673a\u5668\u5b66\u4e60\u548c\u57ce\u5e02\u5efa\u6210\u73af\u5883\u7684\u5b8f\u89c2\u6d4b\u91cf\uff0c\u7814\u7a76\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u78b0\u649e\u4e25\u91cd\u6027\u3002\u901a\u8fc7\u5730\u7406\u968f\u673a\u68ee\u6797\u6a21\u578b\uff08GRF\uff09\u53d1\u73b0\uff0c\u7a7a\u95f4\u5c40\u90e8\u673a\u5668\u5b66\u4e60\u5728\u9884\u6d4b\u78b0\u649e\u4e25\u91cd\u6027\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u571f\u5730\u5229\u7528\u662f\u6700\u91cd\u8981\u7684\u5f71\u54cd\u56e0\u7d20\uff0c\u5e02\u4e2d\u5fc3\u533a\u57df\u7684\u4f4e\u4e25\u91cd\u6027\u78b0\u649e\u6982\u7387\u66f4\u9ad8\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u78b0\u649e\u4e25\u91cd\u6027\u4e0e\u57ce\u5e02\u5efa\u6210\u73af\u5883\u4e4b\u95f4\u7684\u7a7a\u95f4\u5f02\u8d28\u6027\u548c\u81ea\u76f8\u5173\u6027\uff0c\u4e3a\u5546\u4e1a\u8fd0\u8425\u63d0\u4f9b\u5b89\u5168\u6539\u8fdb\u5efa\u8bae\u3002", "method": "\u91c7\u7528\u5730\u7406\u968f\u673a\u68ee\u6797\u6a21\u578b\uff08GRF\uff09\u7ed3\u5408\u7a7a\u95f4\u5c40\u90e8\u673a\u5668\u5b66\u4e60\u548c\u5efa\u6210\u73af\u5883\u6d4b\u91cf\uff08\u5982\u571f\u5730\u5229\u7528\u3001\u4ea4\u53c9\u53e3\u7b49\uff09\uff0c\u5206\u6790\u65e7\u91d1\u5c71\u78b0\u649e\u4e25\u91cd\u6027\u98ce\u9669\u56fe\u3002", "result": "\u7a7a\u95f4\u5c40\u90e8\u673a\u5668\u5b66\u4e60\u4f18\u4e8e\u5e38\u89c4\u65b9\u6cd5\uff0c\u571f\u5730\u5229\u7528\u5bf9\u78b0\u649e\u4e25\u91cd\u6027\u5f71\u54cd\u6700\u5927\uff0c\u5e02\u4e2d\u5fc3\u533a\u57df\u7684\u4f4e\u4e25\u91cd\u6027\u78b0\u649e\u6982\u7387\u663e\u8457\u9ad8\u4e8e\u4f4f\u5b85\u533a\u3002", "conclusion": "\u5efa\u8bae\u5546\u4e1a\u8fd0\u8425\u4e2d\u660e\u786e\u8003\u8651\u5730\u7406\u4f4d\u7f6e\uff0c\u5e76\u4e3a\u4f4f\u5b85\u533a\u8bbe\u8ba1\u7279\u5b9a\u5b89\u5168\u63aa\u65bd\uff0c\u4ee5\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2505.07155", "pdf": "https://arxiv.org/pdf/2505.07155", "abs": "https://arxiv.org/abs/2505.07155", "authors": ["Shuai Wang", "Harrisen Scells", "Bevan Koopman", "Guido Zuccon"], "title": "Reassessing Large Language Model Boolean Query Generation for Systematic Reviews", "categories": ["cs.IR", "cs.CL"], "comment": "Accepted in SIGIR-2025", "summary": "Systematic reviews are comprehensive literature reviews that address highly\nfocused research questions and represent the highest form of evidence in\nmedicine. A critical step in this process is the development of complex Boolean\nqueries to retrieve relevant literature. Given the difficulty of manually\nconstructing these queries, recent efforts have explored Large Language Models\n(LLMs) to assist in their formulation. One of the first studies,Wang et al.,\ninvestigated ChatGPT for this task, followed by Staudinger et al., which\nevaluated multiple LLMs in a reproducibility study. However, the latter\noverlooked several key aspects of the original work, including (i) validation\nof generated queries, (ii) output formatting constraints, and (iii) selection\nof examples for chain-of-thought (Guided) prompting. As a result, its findings\ndiverged significantly from the original study. In this work, we systematically\nreproduce both studies while addressing these overlooked factors. Our results\nshow that query effectiveness varies significantly across models and prompt\ndesigns, with guided query formulation benefiting from well-chosen seed\nstudies. Overall, prompt design and model selection are key drivers of\nsuccessful query formulation. Our findings provide a clearer understanding of\nLLMs' potential in Boolean query generation and highlight the importance of\nmodel- and prompt-specific optimisations. The complex nature of systematic\nreviews adds to challenges in both developing and reproducing methods but also\nhighlights the importance of reproducibility studies in this domain.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u751f\u6210\u7cfb\u7edf\u6027\u7efc\u8ff0\u7684\u5e03\u5c14\u67e5\u8be2\u4e2d\u7684\u4f5c\u7528\uff0c\u5206\u6790\u4e86\u6a21\u578b\u9009\u62e9\u548c\u63d0\u793a\u8bbe\u8ba1\u5bf9\u67e5\u8be2\u6548\u679c\u7684\u5f71\u54cd\uff0c\u5f3a\u8c03\u4e86\u53ef\u91cd\u590d\u6027\u7814\u7a76\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7531\u4e8e\u624b\u52a8\u6784\u5efa\u7cfb\u7edf\u6027\u7efc\u8ff0\u7684\u5e03\u5c14\u67e5\u8be2\u56f0\u96be\uff0c\u7814\u7a76\u8005\u63a2\u7d22\u4e86LLM\u7684\u8f85\u52a9\u4f5c\u7528\uff0c\u4f46\u540e\u7eed\u7814\u7a76\u5ffd\u7565\u4e86\u539f\u59cb\u5de5\u4f5c\u7684\u5173\u952e\u65b9\u9762\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u4e00\u81f4\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7cfb\u7edf\u590d\u73b0\u6f84\u6e05\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u590d\u73b0\u4e86Wang et al.\u548cStaudinger et al.\u7684\u7814\u7a76\uff0c\u7279\u522b\u5173\u6ce8\u4e86\u67e5\u8be2\u9a8c\u8bc1\u3001\u8f93\u51fa\u683c\u5f0f\u7ea6\u675f\u548c\u5f15\u5bfc\u63d0\u793a\u7684\u793a\u4f8b\u9009\u62e9\u7b49\u88ab\u5ffd\u89c6\u7684\u56e0\u7d20\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u67e5\u8be2\u6548\u679c\u56e0\u6a21\u578b\u548c\u63d0\u793a\u8bbe\u8ba1\u800c\u5f02\uff0c\u5f15\u5bfc\u67e5\u8be2\u4ece\u7cbe\u5fc3\u9009\u62e9\u7684\u79cd\u5b50\u7814\u7a76\u4e2d\u83b7\u76ca\u3002\u63d0\u793a\u8bbe\u8ba1\u548c\u6a21\u578b\u9009\u62e9\u662f\u6210\u529f\u751f\u6210\u67e5\u8be2\u7684\u5173\u952e\u3002", "conclusion": "LLM\u5728\u5e03\u5c14\u67e5\u8be2\u751f\u6210\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u9488\u5bf9\u6a21\u578b\u548c\u63d0\u793a\u8fdb\u884c\u4f18\u5316\u3002\u7cfb\u7edf\u6027\u7efc\u8ff0\u7684\u590d\u6742\u6027\u589e\u52a0\u4e86\u5f00\u53d1\u548c\u590d\u73b0\u65b9\u6cd5\u7684\u6311\u6218\uff0c\u4e5f\u51f8\u663e\u4e86\u53ef\u91cd\u590d\u6027\u7814\u7a76\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.06795", "pdf": "https://arxiv.org/pdf/2505.06795", "abs": "https://arxiv.org/abs/2505.06795", "authors": ["Abhijit Gupta"], "title": "Decoding Futures Price Dynamics: A Regularized Sparse Autoencoder for Interpretable Multi-Horizon Forecasting and Factor Discovery", "categories": ["cs.LG", "cs.AI", "cs.CE"], "comment": null, "summary": "Commodity price volatility creates economic challenges, necessitating\naccurate multi-horizon forecasting. Predicting prices for commodities like\ncopper and crude oil is complicated by diverse interacting factors\n(macroeconomic, supply/demand, geopolitical, etc.). Current models often lack\ntransparency, limiting strategic use. This paper presents a Regularized Sparse\nAutoencoder (RSAE), a deep learning framework for simultaneous multi-horizon\ncommodity price prediction and discovery of interpretable latent market\ndrivers. The RSAE forecasts prices at multiple horizons (e.g., 1-day, 1-week,\n1-month) using multivariate time series. Crucially, L1 regularization\n($\\|\\mathbf{z}\\|_1$) on its latent vector $\\mathbf{z}$ enforces sparsity,\npromoting parsimonious explanations of market dynamics through learned factors\nrepresenting underlying drivers (e.g., demand, supply shocks). Drawing from\nenergy-based models and sparse coding, the RSAE optimizes predictive accuracy\nwhile learning sparse representations. Evaluated on historical Copper and Crude\nOil data with numerous indicators, our findings indicate the RSAE offers\ncompetitive multi-horizon forecasting accuracy and data-driven insights into\nprice dynamics via its interpretable latent space, a key advantage over\ntraditional black-box approaches.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6b63\u5219\u5316\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08RSAE\uff09\uff0c\u7528\u4e8e\u591a\u65f6\u95f4\u8303\u56f4\u7684\u5546\u54c1\u4ef7\u683c\u9884\u6d4b\u548c\u53ef\u89e3\u91ca\u5e02\u573a\u9a71\u52a8\u56e0\u7d20\u53d1\u73b0\u3002RSAE\u901a\u8fc7L1\u6b63\u5219\u5316\u5f3a\u5236\u7a00\u758f\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u5728\u94dc\u548c\u539f\u6cb9\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5546\u54c1\u4ef7\u683c\u6ce2\u52a8\u5bf9\u7ecf\u6d4e\u9020\u6210\u6311\u6218\uff0c\u73b0\u6709\u9884\u6d4b\u6a21\u578b\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u5176\u6218\u7565\u5e94\u7528\u3002RSAE\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5b9e\u73b0\u591a\u65f6\u95f4\u8303\u56f4\u9884\u6d4b\u5e76\u63ed\u793a\u5e02\u573a\u9a71\u52a8\u56e0\u7d20\u3002", "method": "RSAE\u662f\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408L1\u6b63\u5219\u5316\uff08$\\|\\mathbf{z}\\|_1$\uff09\u5bf9\u6f5c\u5728\u5411\u91cf\u8fdb\u884c\u7a00\u758f\u7ea6\u675f\uff0c\u901a\u8fc7\u5b66\u4e60\u7a00\u758f\u8868\u793a\u4f18\u5316\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u4ece\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u63d0\u53d6\u53ef\u89e3\u91ca\u7684\u6f5c\u5728\u9a71\u52a8\u56e0\u7d20\u3002", "result": "\u5728\u94dc\u548c\u539f\u6cb9\u5386\u53f2\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRSAE\u5728\u591a\u65f6\u95f4\u8303\u56f4\u9884\u6d4b\u65b9\u9762\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5e76\u901a\u8fc7\u5176\u53ef\u89e3\u91ca\u7684\u6f5c\u5728\u7a7a\u95f4\u63d0\u4f9b\u4e86\u5bf9\u4ef7\u683c\u52a8\u6001\u7684\u6570\u636e\u9a71\u52a8\u6d1e\u5bdf\u3002", "conclusion": "RSAE\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u5546\u54c1\u4ef7\u683c\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u901a\u8fc7\u7a00\u758f\u6f5c\u5728\u8868\u793a\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u9ed1\u7bb1\u65b9\u6cd5\u3002"}}
{"id": "2505.06305", "pdf": "https://arxiv.org/pdf/2505.06305", "abs": "https://arxiv.org/abs/2505.06305", "authors": ["Haowei Yang", "Qingyi Lu", "Yang Wang", "Sibei Liu", "Jiayun Zheng", "Ao Xiang"], "title": "User Behavior Analysis in Privacy Protection with Large Language Models: A Study on Privacy Preferences with Limited Data", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "With the widespread application of large language models (LLMs), user privacy\nprotection has become a significant research topic. Existing privacy preference\nmodeling methods often rely on large-scale user data, making effective privacy\npreference analysis challenging in data-limited environments. This study\nexplores how LLMs can analyze user behavior related to privacy protection in\nscenarios with limited data and proposes a method that integrates Few-shot\nLearning and Privacy Computing to model user privacy preferences. The research\nutilizes anonymized user privacy settings data, survey responses, and simulated\ndata, comparing the performance of traditional modeling approaches with\nLLM-based methods. Experimental results demonstrate that, even with limited\ndata, LLMs significantly improve the accuracy of privacy preference modeling.\nAdditionally, incorporating Differential Privacy and Federated Learning further\nreduces the risk of user data exposure. The findings provide new insights into\nthe application of LLMs in privacy protection and offer theoretical support for\nadvancing privacy computing and user behavior analysis.", "AI": {"tldr": "\u901a\u8fc7\u96c6\u6210\u5c11\u6837\u672c\u5b66\u4e60\u548c\u9690\u79c1\u8ba1\u7b97\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6709\u9650\u6570\u636e\u4e0b\u63d0\u5347\u7528\u6237\u9690\u79c1\u504f\u597d\u5efa\u6a21\u51c6\u786e\u6027\uff0c\u5e76\u5f15\u5165\u5dee\u5206\u9690\u79c1\u548c\u8054\u90a6\u5b66\u4e60\u964d\u4f4e\u6570\u636e\u66b4\u9732\u98ce\u9669\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7528\u6237\u9690\u79c1\u4fdd\u62a4\u6210\u4e3a\u91cd\u8981\u7814\u7a76\u8bfe\u9898\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u6570\u636e\uff0c\u5728\u6570\u636e\u6709\u9650\u73af\u5883\u4e0b\u96be\u4ee5\u6709\u6548\u5206\u6790\u9690\u79c1\u504f\u597d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5c11\u6837\u672c\u5b66\u4e60\u548c\u9690\u79c1\u8ba1\u7b97\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u533f\u540d\u9690\u79c1\u8bbe\u7f6e\u6570\u636e\u3001\u8c03\u67e5\u54cd\u5e94\u548c\u6a21\u62df\u6570\u636e\uff0c\u5bf9\u6bd4\u4f20\u7edf\u4e0eLLM\u5efa\u6a21\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u6570\u636e\u6709\u9650\uff0cLLM\u663e\u8457\u63d0\u5347\u4e86\u9690\u79c1\u504f\u597d\u5efa\u6a21\u7684\u51c6\u786e\u6027\uff0c\u4e14\u5dee\u5206\u9690\u79c1\u548c\u8054\u90a6\u5b66\u4e60\u8fdb\u4e00\u6b65\u964d\u4f4e\u4e86\u6570\u636e\u66b4\u9732\u98ce\u9669\u3002", "conclusion": "\u7814\u7a76\u4e3aLLM\u5728\u9690\u79c1\u4fdd\u62a4\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u4e3a\u9690\u79c1\u8ba1\u7b97\u548c\u7528\u6237\u884c\u4e3a\u5206\u6790\u7684\u8fdb\u6b65\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2505.07166", "pdf": "https://arxiv.org/pdf/2505.07166", "abs": "https://arxiv.org/abs/2505.07166", "authors": ["Zheng Yao", "Shuai Wang", "Guido Zuccon"], "title": "Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval Knowledge Acquisition", "categories": ["cs.IR", "cs.CL"], "comment": "Accepted in SIGIR-2025", "summary": "Dense retrievers utilize pre-trained backbone language models (e.g., BERT,\nLLaMA) that are fine-tuned via contrastive learning to perform the task of\nencoding text into sense representations that can be then compared via a\nshallow similarity operation, e.g. inner product. Recent research has\nquestioned the role of fine-tuning vs. that of pre-training within dense\nretrievers, specifically arguing that retrieval knowledge is primarily gained\nduring pre-training, meaning knowledge not acquired during pre-training cannot\nbe sub-sequentially acquired via fine-tuning. We revisit this idea here as the\nclaim was only studied in the context of a BERT-based encoder using DPR as\nrepresentative dense retriever. We extend the previous analysis by testing\nother representation approaches (comparing the use of CLS tokens with that of\nmean pooling), backbone architectures (encoder-only BERT vs. decoder-only\nLLaMA), and additional datasets (MSMARCO in addition to Natural Questions). Our\nstudy confirms that in DPR tuning, pre-trained knowledge underpins retrieval\nperformance, with fine-tuning primarily adjusting neuron activation rather than\nreorganizing knowledge. However, this pattern does not hold universally, such\nas in mean-pooled (Contriever) and decoder-based (LLaMA) models. We ensure full\nreproducibility and make our implementation publicly available at\nhttps://github.com/ielab/DenseRetriever-Knowledge-Acquisition.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5bc6\u96c6\u578b\u68c0\u7d22\u5668\u4e2d\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u9884\u8bad\u7ec3\u77e5\u8bc6\u5bf9\u68c0\u7d22\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u800c\u5fae\u8c03\u4e3b\u8981\u8c03\u8282\u795e\u7ecf\u5143\u6fc0\u6d3b\u800c\u975e\u91cd\u7ec4\u77e5\u8bc6\uff0c\u4f46\u8fd9\u4e00\u7ed3\u8bba\u5728\u4e0d\u540c\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u8ba8\u5bc6\u96c6\u578b\u68c0\u7d22\u5668\u4e2d\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u7684\u76f8\u5bf9\u8d21\u732e\uff0c\u6f84\u6e05\u9884\u8bad\u7ec3\u77e5\u8bc6\u662f\u5426\u8db3\u4ee5\u652f\u6491\u68c0\u7d22\u4efb\u52a1\uff0c\u800c\u5fae\u8c03\u662f\u5426\u771f\u6b63\u5f15\u5165\u65b0\u77e5\u8bc6\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5bf9\u6bd4\u5206\u6790\u4e0d\u540c\u7684\u8868\u793a\u65b9\u6cd5\uff08CLS token\u4e0e\u5e73\u5747\u6c60\u5316\uff09\u3001\u6a21\u578b\u67b6\u6784\uff08BERT\u4e0eLLaMA\uff09\u4ee5\u53ca\u6570\u636e\u96c6\uff08MSMARCO\u548cNatural Questions\uff09\uff0c\u4ee5\u9a8c\u8bc1\u68c0\u7d22\u77e5\u8bc6\u7684\u83b7\u53d6\u6765\u6e90\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5728DPR\u5fae\u8c03\u4e2d\uff0c\u9884\u8bad\u7ec3\u77e5\u8bc6\u662f\u68c0\u7d22\u6027\u80fd\u7684\u57fa\u7840\uff0c\u5fae\u8c03\u4e3b\u8981\u8c03\u6574\u795e\u7ecf\u5143\u6fc0\u6d3b\uff1b\u7136\u800c\u8fd9\u4e00\u7ed3\u8bba\u5728\u5e73\u5747\u6c60\u5316\u7684Contriever\u548c\u89e3\u7801\u5668\u6a21\u578bLLaMA\u4e2d\u5e76\u4e0d\u666e\u904d\u9002\u7528\u3002", "conclusion": "\u8bba\u6587\u7ed3\u8bba\u5f3a\u8c03\u4e86\u9884\u8bad\u7ec3\u77e5\u8bc6\u7684\u91cd\u8981\u6027\uff0c\u4f46\u4e5f\u6307\u51fa\u4e0d\u540c\u6a21\u578b\u548c\u68c0\u7d22\u65b9\u6cd5\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002\u5b9e\u73b0\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.06804", "pdf": "https://arxiv.org/pdf/2505.06804", "abs": "https://arxiv.org/abs/2505.06804", "authors": ["Xiaohan Wang", "Matthew Berger"], "title": "Topology Guidance: Controlling the Outputs of Generative Models via Vector Field Topology", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "For domains that involve numerical simulation, it can be computationally\nexpensive to run an ensemble of simulations spanning a parameter space of\ninterest to a user. To this end, an attractive surrogate for simulation is the\ngenerative modeling of fields produced by an ensemble, allowing one to\nsynthesize fields in a computationally cheap, yet accurate, manner. However,\nfor the purposes of visual analysis, a limitation of generative models is their\nlack of control, as it is unclear what one should expect when sampling a field\nfrom a model. In this paper we study how to make generative models of fields\nmore controllable, so that users can specify features of interest, in\nparticular topological features, that they wish to see in the output. We\npropose topology guidance, a method for guiding the sampling process of a\ngenerative model, specifically a diffusion model, such that a topological\ndescription specified as input is satisfied in the generated output. Central to\nour method, we couple a coordinate-based neural network used to represent\nfields, with a diffusion model used for generation. We show how to use\ntopologically-relevant signals provided by the coordinate-based network to help\nguide the denoising process of a diffusion model. This enables us to faithfully\nrepresent a user's specified topology, while ensuring that the output field\nremains within the generative data distribution. Specifically, we study 2D\nvector field topology, evaluating our method over an ensemble of fluid flows,\nwhere we show that generated vector fields faithfully adhere to the location,\nand type, of critical points over the spatial domain. We further show the\nbenefits of our method in aiding the comparison of ensembles, allowing one to\nexplore commonalities and differences in distributions along prescribed\ntopological features.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u62d3\u6251\u5f15\u5bfc\u63d0\u9ad8\u751f\u6210\u6a21\u578b\u7684\u53ef\u63a7\u6027\uff0c\u4f7f\u5f97\u7528\u6237\u80fd\u591f\u6307\u5b9a\u8f93\u51fa\u4e2d\u7684\u62d3\u6251\u7279\u5f81\uff0c\u7279\u522b\u662f\u7528\u4e8e\u6570\u503c\u6a21\u62df\u76842D\u77e2\u91cf\u573a\u3002", "motivation": "\u5728\u6570\u503c\u6a21\u62df\u4e2d\uff0c\u751f\u6210\u6a21\u578b\u53ef\u4ee5\u5ec9\u4ef7\u4e14\u51c6\u786e\u5730\u5408\u6210\u6a21\u62df\u7ed3\u679c\uff0c\u4f46\u7f3a\u4e4f\u63a7\u5236\u6027\uff0c\u7528\u6237\u96be\u4ee5\u6307\u5b9a\u8f93\u51fa\u4e2d\u7684\u7279\u5b9a\u7279\u5f81\uff0c\u5c24\u5176\u662f\u62d3\u6251\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u62d3\u6251\u5f15\u5bfc\u65b9\u6cd5\uff0c\u7ed3\u5408\u57fa\u4e8e\u5750\u6807\u7684\u795e\u7ecf\u7f51\u7edc\u548c\u6269\u6563\u6a21\u578b\uff0c\u5229\u7528\u62d3\u6251\u76f8\u5173\u4fe1\u53f7\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u7684\u53bb\u566a\u8fc7\u7a0b\uff0c\u786e\u4fdd\u751f\u6210\u7684\u77e2\u91cf\u573a\u6ee1\u8db3\u7528\u6237\u6307\u5b9a\u7684\u62d3\u6251\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u751f\u6210\u76842D\u77e2\u91cf\u573a\u80fd\u591f\u5fe0\u5b9e\u53cd\u6620\u7528\u6237\u6307\u5b9a\u7684\u4e34\u754c\u70b9\u4f4d\u7f6e\u548c\u7c7b\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u6570\u636e\u7684\u5206\u5e03\u3002\u65b9\u6cd5\u8fd8\u652f\u6301\u6bd4\u8f83\u4e0d\u540c\u62d3\u6251\u7279\u5f81\u7684\u5171\u6027\u5dee\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6a21\u578b\u7684\u53ef\u63a7\u6027\uff0c\u6709\u52a9\u4e8e\u89c6\u89c9\u5206\u6790\u548c\u63a2\u7d22\u62d3\u6251\u7279\u5f81\u7684\u5206\u5e03\u5dee\u5f02\u3002"}}
{"id": "2505.06307", "pdf": "https://arxiv.org/pdf/2505.06307", "abs": "https://arxiv.org/abs/2505.06307", "authors": ["Mingfei Zeng", "Ming Xie", "Xixi Zheng", "Chunhai Li", "Chuan Zhang", "Liehuang Zhu"], "title": "Large Language Model-driven Security Assistant for Internet of Things via Chain-of-Thought", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The rapid development of Internet of Things (IoT) technology has transformed\npeople's way of life and has a profound impact on both production and daily\nactivities. However, with the rapid advancement of IoT technology, the security\nof IoT devices has become an unavoidable issue in both research and\napplications. Although some efforts have been made to detect or mitigate IoT\nsecurity vulnerabilities, they often struggle to adapt to the complexity of IoT\nenvironments, especially when dealing with dynamic security scenarios. How to\nautomatically, efficiently, and accurately understand these vulnerabilities\nremains a challenge. To address this, we propose an IoT security assistant\ndriven by Large Language Model (LLM), which enhances the LLM's understanding of\nIoT security vulnerabilities and related threats. The aim of the ICoT method we\npropose is to enable the LLM to understand security issues by breaking down the\nvarious dimensions of security vulnerabilities and generating responses\ntailored to the user's specific needs and expertise level. By incorporating\nICoT, LLM can gradually analyze and reason through complex security scenarios,\nresulting in more accurate, in-depth, and personalized security recommendations\nand solutions. Experimental results show that, compared to methods relying\nsolely on LLM, our proposed LLM-driven IoT security assistant significantly\nimproves the understanding of IoT security issues through the ICoT approach and\nprovides personalized solutions based on the user's identity, demonstrating\nhigher accuracy and reliability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684IoT\u5b89\u5168\u52a9\u624bICoT\uff0c\u901a\u8fc7\u5206\u89e3\u5b89\u5168\u6f0f\u6d1e\u7684\u591a\u7ef4\u5ea6\u5e76\u751f\u6210\u4e2a\u6027\u5316\u54cd\u5e94\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5bf9IoT\u5b89\u5168\u95ee\u9898\u7684\u7406\u89e3\u4e0e\u89e3\u51b3\u65b9\u6848\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740IoT\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5176\u5b89\u5168\u95ee\u9898\u65e5\u76ca\u51f8\u663e\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u590d\u6742\u52a8\u6001\u7684\u5b89\u5168\u573a\u666f\u3002\u5982\u4f55\u81ea\u52a8\u3001\u9ad8\u6548\u4e14\u51c6\u786e\u5730\u7406\u89e3\u8fd9\u4e9b\u6f0f\u6d1e\u6210\u4e3a\u6311\u6218\u3002", "method": "\u63d0\u51faICoT\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u5b89\u5168\u6f0f\u6d1e\u7684\u5404\u4e2a\u7ef4\u5ea6\uff0c\u4f7fLLM\u80fd\u9010\u6b65\u5206\u6790\u548c\u63a8\u7406\u590d\u6742\u5b89\u5168\u573a\u666f\uff0c\u751f\u6210\u57fa\u4e8e\u7528\u6237\u9700\u6c42\u548c\u4e13\u4e1a\u6c34\u5e73\u7684\u4e2a\u6027\u5316\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u7eafLLM\u65b9\u6cd5\uff0cICoT\u663e\u8457\u63d0\u5347\u4e86\u5bf9IoT\u5b89\u5168\u95ee\u9898\u7684\u7406\u89e3\uff0c\u5e76\u63d0\u4f9b\u66f4\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u7684\u4e2a\u6027\u5316\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "ICoT\u9a71\u52a8\u7684LLM\u5b89\u5168\u52a9\u624b\u5728\u7406\u89e3IoT\u5b89\u5168\u6f0f\u6d1e\u548c\u63d0\u4f9b\u4e2a\u6027\u5316\u89e3\u51b3\u65b9\u6848\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3aIoT\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.07167", "pdf": "https://arxiv.org/pdf/2505.07167", "abs": "https://arxiv.org/abs/2505.07167", "authors": ["Haoran Gu", "Handing Wang", "Yi Mei", "Mengjie Zhang", "Yaochu Jin"], "title": "One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have been extensively used across diverse\ndomains, including virtual assistants, automated code generation, and\nscientific research. However, they remain vulnerable to jailbreak attacks,\nwhich manipulate the models into generating harmful responses despite safety\nalignment. Recent studies have shown that current safety-aligned LLMs often\nundergo the shallow safety alignment, where the first few tokens largely\ndetermine whether the response will be harmful. Through comprehensive\nobservations, we find that safety-aligned LLMs and various defense strategies\ngenerate highly similar initial tokens in their refusal responses, which we\ndefine as safety trigger tokens. Building on this insight, we propose\n\\texttt{D-STT}, a simple yet effective defense algorithm that identifies and\nexplicitly decodes safety trigger tokens of the given safety-aligned LLM to\ntrigger the model's learned safety patterns. In this process, the safety\ntrigger is constrained to a single token, which effectively preserves model\nusability by introducing minimum intervention in the decoding process.\nExtensive experiments across diverse jailbreak attacks and benign prompts\ndemonstrate that \\ours significantly reduces output harmfulness while\npreserving model usability and incurring negligible response time overhead,\noutperforming ten baseline methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86D-STT\u9632\u5fa1\u7b97\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u89e3\u7801\u5b89\u5168\u89e6\u53d1\u8bcd\u6765\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u51cf\u5c11\u6709\u5bb3\u8f93\u51fa\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u53ef\u7528\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c3d\u7ba1\u7ecf\u8fc7\u5b89\u5168\u5bf9\u9f50\uff0c\u4f46\u4ecd\u5bb9\u6613\u53d7\u5230\u8d8a\u72f1\u653b\u51fb\u7684\u5a01\u80c1\uff0c\u5bfc\u81f4\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u3002\u7814\u7a76\u53d1\u73b0\u5b89\u5168\u5bf9\u9f50\u7684LLMs\u5728\u62d2\u7edd\u54cd\u5e94\u65f6\u751f\u6210\u9ad8\u5ea6\u76f8\u4f3c\u7684\u521d\u59cb\u8bcd\uff08\u5b89\u5168\u89e6\u53d1\u8bcd\uff09\uff0c\u8fd9\u6210\u4e3a\u9632\u5fa1\u7684\u5173\u952e\u3002", "method": "\u63d0\u51fa\u4e86D-STT\u7b97\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u89e3\u7801\u5b89\u5168\u89e6\u53d1\u8bcd\u6765\u63d0\u524d\u6fc0\u6d3b\u6a21\u578b\u7684\u5b89\u5168\u6a21\u5f0f\uff0c\u4ec5\u9700\u5e72\u9884\u5355\u4e2a\u8bcd\uff0c\u6700\u5c0f\u5316\u89e3\u7801\u8fc7\u7a0b\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cD-STT\u80fd\u663e\u8457\u964d\u4f4e\u6709\u5bb3\u8f93\u51fa\uff0c\u540c\u65f6\u5728\u6a21\u578b\u53ef\u7528\u6027\u548c\u54cd\u5e94\u65f6\u95f4\u4e0a\u51e0\u4e4e\u65e0\u989d\u5916\u5f00\u9500\uff0c\u4f18\u4e8e\u5341\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "D-STT\u662f\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u6709\u6548\u5e73\u8861\u5b89\u5168\u6027\u548c\u6a21\u578b\u5b9e\u7528\u6027\uff0c\u4e3aLLMs\u7684\u5b89\u5168\u9632\u5fa1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.06818", "pdf": "https://arxiv.org/pdf/2505.06818", "abs": "https://arxiv.org/abs/2505.06818", "authors": ["Thien Nhan Vo"], "title": "Deep Learning for On-Street Parking Violation Prediction", "categories": ["cs.LG"], "comment": null, "summary": "Illegal parking along with the lack of available parking spaces are among the\nbiggest issues faced in many large cities. These issues can have a significant\nimpact on the quality of life of citizens. On-street parking systems have been\ndesigned to this end aiming at ensuring that parking spaces will be available\nfor the local population, while also providing easy access to parking for\npeople visiting the city center. However, these systems are often affected by\nillegal parking, providing incorrect information regarding the availability of\nparking spaces. Even though this can be mitigated using sensors for detecting\nthe presence of cars in various parking sectors, the cost of these\nimplementations is usually prohibiting large. In this paper, we investigate an\nindirect way of predicting parking violations at a fine-grained level,\nequipping such parking systems with a valuable tool for providing more accurate\ninformation to citizens. To this end, we employed a Deep Learning (DL)-based\nmodel to predict fine-grained parking violation rates for on-street parking\nsystems. Moreover, we developed a data augmentation and smoothing technique for\nfurther improving the accuracy of DL models under the presence of missing and\nnoisy data. We demonstrate, using experiments on real data collected in\nThessaloniki, Greece, that the developed system can indeed provide accurate\nparking violation predictions.", "AI": {"tldr": "\u6458\u8981\uff1a\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7ec6\u7c92\u5ea6\u505c\u8f66\u8fdd\u89c4\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u5e73\u6ed1\u6280\u672f\u89e3\u51b3\u6570\u636e\u7f3a\u5931\u548c\u566a\u58f0\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u975e\u6cd5\u505c\u8f66\u548c\u505c\u8f66\u4f4d\u4e0d\u8db3\u4e25\u91cd\u5f71\u54cd\u57ce\u5e02\u751f\u6d3b\u8d28\u91cf\uff0c\u73b0\u6709\u8def\u8fb9\u505c\u8f66\u7cfb\u7edf\u56e0\u975e\u6cd5\u505c\u8f66\u800c\u65e0\u6cd5\u51c6\u786e\u63d0\u4f9b\u8f66\u4f4d\u4fe1\u606f\uff0c\u867d\u6709\u4f20\u611f\u5668\u89e3\u51b3\u65b9\u6848\u4f46\u6210\u672c\u8fc7\u9ad8\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u9884\u6d4b\u8fdd\u89c4\u505c\u8f66\u6539\u5584\u8fd9\u4e00\u73b0\u72b6\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u9884\u6d4b\u7ec6\u7c92\u5ea6\u505c\u8f66\u8fdd\u89c4\u7387\uff0c\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u548c\u5e73\u6ed1\u6280\u672f\u5904\u7406\u7f3a\u5931\u548c\u566a\u58f0\u6570\u636e\u3002", "result": "\u5728\u5e0c\u814a\u585e\u8428\u6d1b\u5c3c\u57fa\u7684\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u7cfb\u7edf\u80fd\u51c6\u786e\u9884\u6d4b\u505c\u8f66\u8fdd\u89c4\u884c\u4e3a\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\u80fd\u6709\u6548\u63d0\u5347\u8def\u8fb9\u505c\u8f66\u7cfb\u7edf\u7684\u4fe1\u606f\u51c6\u786e\u6027\uff0c\u4e3a\u57ce\u5e02\u505c\u8f66\u7ba1\u7406\u63d0\u4f9b\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2505.06311", "pdf": "https://arxiv.org/pdf/2505.06311", "abs": "https://arxiv.org/abs/2505.06311", "authors": ["Tongyu Wen", "Chenglong Wang", "Xiyuan Yang", "Haoyu Tang", "Yueqi Xie", "Lingjuan Lyu", "Zhicheng Dou", "Fangzhao Wu"], "title": "Defending against Indirect Prompt Injection by Instruction Detection", "categories": ["cs.CR", "cs.AI"], "comment": "13 pages, 4 figures", "summary": "The integration of Large Language Models (LLMs) with external sources is\nbecoming increasingly common, with Retrieval-Augmented Generation (RAG) being a\nprominent example. However, this integration introduces vulnerabilities of\nIndirect Prompt Injection (IPI) attacks, where hidden instructions embedded in\nexternal data can manipulate LLMs into executing unintended or harmful actions.\nWe recognize that the success of IPI attacks fundamentally relies in the\npresence of instructions embedded within external content, which can alter the\nbehavioral state of LLMs. Can effectively detecting such state changes help us\ndefend against IPI attacks? In this paper, we propose a novel approach that\ntakes external data as input and leverages the behavioral state of LLMs during\nboth forward and backward propagation to detect potential IPI attacks.\nSpecifically, we demonstrate that the hidden states and gradients from\nintermediate layers provide highly discriminative features for instruction\ndetection. By effectively combining these features, our approach achieves a\ndetection accuracy of 99.60\\% in the in-domain setting and 96.90\\% in the\nout-of-domain setting, while reducing the attack success rate to just 0.12\\% on\nthe BIPIA benchmark.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528LLM\u7684\u884c\u4e3a\u72b6\u6001\u68c0\u6d4b\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\uff08IPI\uff09\u653b\u51fb\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\u4e2d\u7684\u9690\u85cf\u72b6\u6001\u548c\u68af\u5ea6\u7279\u5f81\uff0c\u5b9e\u73b0\u4e86\u9ad8\u68c0\u6d4b\u51c6\u786e\u7387\u548c\u4f4e\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u968f\u7740LLM\u4e0e\u5916\u90e8\u6570\u636e\uff08\u5982RAG\uff09\u7684\u96c6\u6210\u589e\u52a0\uff0cIPI\u653b\u51fb\u7684\u98ce\u9669\u4e0a\u5347\uff0c\u5373\u9690\u85cf\u6307\u4ee4\u53ef\u80fd\u64cd\u63a7LLM\u6267\u884c\u6709\u5bb3\u884c\u4e3a\u3002\u73b0\u6709\u9632\u5fa1\u624b\u6bb5\u6709\u9650\uff0c\u56e0\u6b64\u9700\u63a2\u7d22\u57fa\u4e8eLLM\u884c\u4e3a\u72b6\u6001\u53d8\u5316\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u5206\u6790\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\u4e2d\u7684\u9690\u85cf\u72b6\u6001\u53ca\u68af\u5ea6\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u68c0\u6d4b\u5916\u90e8\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u6307\u4ee4\uff0c\u4ee5\u8bc6\u522bIPI\u653b\u51fb\u3002", "result": "\u65b9\u6cd5\u5728BIPIA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523099.60%\uff08\u540c\u57df\uff09\u548c96.90%\uff08\u8de8\u57df\uff09\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u5e76\u5c06\u653b\u51fb\u6210\u529f\u7387\u964d\u81f30.12%\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408LLM\u7684\u884c\u4e3a\u72b6\u6001\u7279\u5f81\uff0c\u80fd\u9ad8\u6548\u9632\u5fa1IPI\u653b\u51fb\uff0c\u4e3a\u5b89\u5168\u96c6\u6210\u5916\u90e8\u6570\u636e\u63d0\u4f9b\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.07188", "pdf": "https://arxiv.org/pdf/2505.07188", "abs": "https://arxiv.org/abs/2505.07188", "authors": ["Chetan Pathade", "Shubham Patil"], "title": "Securing Genomic Data Against Inference Attacks in Federated Learning Environments", "categories": ["cs.CR", "cs.CL"], "comment": "10 Pages, 7 Figures", "summary": "Federated Learning (FL) offers a promising framework for collaboratively\ntraining machine learning models across decentralized genomic datasets without\ndirect data sharing. While this approach preserves data locality, it remains\nsusceptible to sophisticated inference attacks that can compromise individual\nprivacy. In this study, we simulate a federated learning setup using synthetic\ngenomic data and assess its vulnerability to three key attack vectors:\nMembership Inference Attack (MIA), Gradient-Based Membership Inference Attack,\nand Label Inference Attack (LIA). Our experiments reveal that Gradient-Based\nMIA achieves the highest effectiveness, with a precision of 0.79 and F1-score\nof 0.87, underscoring the risk posed by gradient exposure in federated updates.\nAdditionally, we visualize comparative attack performance through radar plots\nand quantify model leakage across clients. The findings emphasize the\ninadequacy of na\\\"ive FL setups in safeguarding genomic privacy and motivate\nthe development of more robust privacy-preserving mechanisms tailored to the\nunique sensitivity of genomic data.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u57fa\u56e0\u7ec4\u6570\u636e\u4e2d\u7684\u9690\u79c1\u6f0f\u6d1e\uff0c\u901a\u8fc7\u6a21\u62df\u653b\u51fb\u53d1\u73b0\u68af\u5ea6\u66b4\u9732\u7684\u98ce\u9669\u6700\u9ad8\uff0c\u547c\u5401\u5f3a\u5316\u9690\u79c1\u4fdd\u62a4\u673a\u5236\u3002", "motivation": "\u5c3d\u7ba1\u8054\u90a6\u5b66\u4e60\u4fdd\u62a4\u4e86\u6570\u636e\u672c\u5730\u6027\uff0c\u4f46\u4ecd\u9762\u4e34\u63a8\u7406\u653b\u51fb\u7684\u98ce\u9669\uff0c\u5c24\u5176\u662f\u57fa\u56e0\u7ec4\u6570\u636e\u7684\u654f\u611f\u6027\u9700\u8981\u66f4\u4e25\u683c\u7684\u9690\u79c1\u4fdd\u62a4\u3002", "method": "\u4f7f\u7528\u5408\u6210\u57fa\u56e0\u7ec4\u6570\u636e\u6a21\u62df\u8054\u90a6\u5b66\u4e60\u73af\u5883\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u653b\u51fb\u65b9\u5f0f\uff08MIA\u3001\u68af\u5ea6MIA\u548cLIA\uff09\u7684\u6548\u679c\u3002", "result": "\u68af\u5ea6MIA\u6548\u679c\u6700\u663e\u8457\uff08\u7cbe\u5ea60.79\uff0cF1\u5f97\u52060.87\uff09\uff0c\u96f7\u8fbe\u56fe\u5c55\u793a\u4e86\u4e0d\u540c\u653b\u51fb\u7684\u6027\u80fd\u5bf9\u6bd4\uff0c\u7a81\u51fa\u4e86\u68af\u5ea6\u66b4\u9732\u7684\u9ad8\u98ce\u9669\u3002", "conclusion": "\u5e38\u89c4\u8054\u90a6\u5b66\u4e60\u96be\u4ee5\u4fdd\u62a4\u57fa\u56e0\u7ec4\u9690\u79c1\uff0c\u9700\u5f00\u53d1\u9488\u5bf9\u6027\u66f4\u5f3a\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u3002"}}
{"id": "2505.06835", "pdf": "https://arxiv.org/pdf/2505.06835", "abs": "https://arxiv.org/abs/2505.06835", "authors": ["Khai Nguyen"], "title": "Streaming Sliced Optimal Transport", "categories": ["cs.LG", "stat.CO", "stat.ME", "stat.ML"], "comment": "28 pages, 9 figures, 3 tables", "summary": "Sliced optimal transport (SOT) or sliced Wasserstein (SW) distance is widely\nrecognized for its statistical and computational scalability. In this work, we\nfurther enhance the computational scalability by proposing the first method for\ncomputing SW from sample streams, called \\emph{streaming sliced Wasserstein}\n(Stream-SW). To define Stream-SW, we first introduce the streaming computation\nof the one-dimensional Wasserstein distance. Since the one-dimensional\nWasserstein (1DW) distance has a closed-form expression, given by the absolute\ndifference between the quantile functions of the compared distributions, we\nleverage quantile approximation techniques for sample streams to define the\nstreaming 1DW distance. By applying streaming 1DW to all projections, we obtain\nStream-SW. The key advantage of Stream-SW is its low memory complexity while\nproviding theoretical guarantees on the approximation error. We demonstrate\nthat Stream-SW achieves a more accurate approximation of SW than random\nsubsampling, with lower memory consumption, in comparing Gaussian distributions\nand mixtures of Gaussians from streaming samples. Additionally, we conduct\nexperiments on point cloud classification, point cloud gradient flows, and\nstreaming change point detection to further highlight the favorable performance\nof Stream-SW.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aStream-SW\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u6837\u672c\u6d41\u4e2d\u8ba1\u7b97\u5207\u7247Wasserstein\u8ddd\u79bb\uff0c\u5177\u6709\u4f4e\u5185\u5b58\u590d\u6742\u6027\u548c\u7406\u8bba\u4fdd\u8bc1\uff0c\u4f18\u4e8e\u968f\u673a\u5b50\u91c7\u6837\u3002", "motivation": "\u5207\u7247\u6700\u4f18\u8fd0\u8f93\uff08SOT\uff09\u6216\u5207\u7247Wasserstein\uff08SW\uff09\u8ddd\u79bb\u56e0\u5176\u7edf\u8ba1\u548c\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u800c\u5e7f\u53d7\u8ba4\u53ef\uff0c\u4f46\u8fdb\u4e00\u6b65\u9700\u8981\u4ece\u6d41\u6837\u672c\u4e2d\u9ad8\u6548\u8ba1\u7b97SW\u7684\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u63d0\u51fa\u6d41\u5f0f\u4e00\u7ef4Wasserstein\uff081DW\uff09\u8ddd\u79bb\u8ba1\u7b97\uff0c\u5229\u7528\u5206\u4f4d\u6570\u8fd1\u4f3c\u6280\u672f\u5b9a\u4e49\u6d41\u5f0f1DW\uff0c\u7136\u540e\u5c06\u5176\u5e94\u7528\u4e8e\u6240\u6709\u6295\u5f71\u4ee5\u5f97\u5230Stream-SW\u3002", "result": "Stream-SW\u5728\u5185\u5b58\u6d88\u8017\u4f4e\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9SW\u7684\u8fd1\u4f3c\u6bd4\u968f\u673a\u5b50\u91c7\u6837\u66f4\u51c6\u786e\uff0c\u5e76\u5728\u9ad8\u65af\u5206\u5e03\u3001\u9ad8\u65af\u6df7\u5408\u6d41\u6837\u672c\u53ca\u70b9\u4e91\u5206\u7c7b\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "Stream-SW\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u5185\u5b58\u9700\u6c42\u7684\u6d41\u5f0fSW\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.06312", "pdf": "https://arxiv.org/pdf/2505.06312", "abs": "https://arxiv.org/abs/2505.06312", "authors": ["Pavel Naumov", "Jia Tao"], "title": "Responsibility Gap in Collective Decision Making", "categories": ["cs.GT", "cs.AI"], "comment": "full version of an IJCAI-25 paper", "summary": "The responsibility gap is a set of outcomes of a collective decision-making\nmechanism in which no single agent is individually responsible. In general,\nwhen designing a decision-making process, it is desirable to minimise the gap.\n  The paper proposes a concept of an elected dictatorship. It shows that, in a\nperfect information setting, the gap is empty if and only if the mechanism is\nan elected dictatorship. It also proves that in an imperfect information\nsetting, the class of gap-free mechanisms is positioned strictly between two\nvariations of the class of elected dictatorships.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u2018\u9009\u4e3e\u72ec\u88c1\u2019\u6982\u5ff5\uff0c\u8bc1\u660e\u5728\u5b8c\u7f8e\u4fe1\u606f\u73af\u5883\u4e0b\u8d23\u4efb\u7a7a\u7f3a\u4ec5\u5728\u8be5\u673a\u5236\u4e0b\u6d88\u5931\uff0c\u5e76\u5728\u4e0d\u5b8c\u7f8e\u4fe1\u606f\u4e0b\u7ed9\u51fa\u7ed3\u8bba\u3002", "motivation": "\u7814\u7a76\u96c6\u4f53\u51b3\u7b56\u673a\u5236\u4e2d\u7684\u8d23\u4efb\u7a7a\u7f3a\u95ee\u9898\uff0c\u65e8\u5728\u6700\u5c0f\u5316\u8fd9\u4e00\u73b0\u8c61\u3002", "method": "\u5f15\u5165\u2018\u9009\u4e3e\u72ec\u88c1\u2019\u6982\u5ff5\uff0c\u5206\u522b\u5728\u5b8c\u7f8e\u4e0e\u4e0d\u5b8c\u7f8e\u4fe1\u606f\u73af\u5883\u4e0b\u8fdb\u884c\u5206\u6790\u4e0e\u8bc1\u660e\u3002", "result": "\u5728\u5b8c\u7f8e\u4fe1\u606f\u4e0b\uff0c\u8d23\u4efb\u7a7a\u7f3a\u6d88\u5931\u5f53\u4e14\u4ec5\u5f53\u91c7\u7528\u9009\u4e3e\u72ec\u88c1\u673a\u5236\uff1b\u4e0d\u5b8c\u7f8e\u4fe1\u606f\u4e0b\uff0c\u65e0\u7a7a\u7f3a\u673a\u5236\u7c7b\u4f4d\u4e8e\u4e24\u79cd\u9009\u4e3e\u72ec\u88c1\u53d8\u4f53\u4e4b\u95f4\u3002", "conclusion": "\u9009\u4e3e\u72ec\u88c1\u673a\u5236\u662f\u6d88\u9664\u8d23\u4efb\u7a7a\u7f3a\u7684\u5173\u952e\uff0c\u4f46\u5176\u5728\u4e0d\u5b8c\u7f8e\u4fe1\u606f\u4e0b\u7684\u9002\u7528\u6027\u5b58\u5728\u9650\u5236\u3002"}}
{"id": "2505.07365", "pdf": "https://arxiv.org/pdf/2505.07365", "abs": "https://arxiv.org/abs/2505.07365", "authors": ["Chao-Han Huck Yang", "Sreyan Ghosh", "Qing Wang", "Jaeyeon Kim", "Hengyi Hong", "Sonal Kumar", "Guirui Zhong", "Zhifeng Kong", "S Sakshi", "Vaibhavi Lokegaonkar", "Oriol Nieto", "Ramani Duraiswami", "Dinesh Manocha", "Gunhee Kim", "Jun Du", "Rafael Valle", "Bryan Catanzaro"], "title": "Multi-Domain Audio Question Answering Toward Acoustic Content Reasoning in The DCASE 2025 Challenge", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM", "eess.AS"], "comment": "Preprint. DCASE 2025 Audio QA Challenge:\n  https://dcase.community/challenge2025/task-audio-question-answering", "summary": "We present Task 5 of the DCASE 2025 Challenge: an Audio Question Answering\n(AQA) benchmark spanning multiple domains of sound understanding. This task\ndefines three QA subsets (Bioacoustics, Temporal Soundscapes, and Complex QA)\nto test audio-language models on interactive question-answering over diverse\nacoustic scenes. We describe the dataset composition (from marine mammal calls\nto soundscapes and complex real-world clips), the evaluation protocol (top-1\naccuracy with answer-shuffling robustness), and baseline systems\n(Qwen2-Audio-7B, AudioFlamingo 2, Gemini-2-Flash). Preliminary results on the\ndevelopment set are compared, showing strong variation across models and\nsubsets. This challenge aims to advance the audio understanding and reasoning\ncapabilities of audio-language models toward human-level acuity, which are\ncrucial for enabling AI agents to perceive and interact about the world\neffectively.", "AI": {"tldr": "DCASE 2025\u6311\u6218\u8d5b\u7684\u4efb\u52a15\u63d0\u51fa\u4e86\u4e00\u4e2a\u97f3\u9891\u95ee\u7b54\uff08AQA\uff09\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u591a\u4e2a\u58f0\u97f3\u7406\u89e3\u9886\u57df\uff0c\u5305\u62ec\u751f\u7269\u58f0\u5b66\u3001\u65f6\u95f4\u58f0\u666f\u548c\u590d\u6742\u95ee\u7b54\uff0c\u4ee5\u8bc4\u4f30\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u7684\u4ea4\u4e92\u95ee\u7b54\u80fd\u529b\u3002", "motivation": "\u901a\u8fc7\u6311\u6218\u8d5b\u63a8\u52a8\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u5728\u97f3\u9891\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u4e0a\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\uff0c\u4f7fAI\u4ee3\u7406\u80fd\u66f4\u6709\u6548\u5730\u611f\u77e5\u548c\u4ea4\u4e92\u4e16\u754c\u3002", "method": "\u4efb\u52a1\u5b9a\u4e49\u4e86\u4e09\u4e2a\u95ee\u7b54\u5b50\u96c6\uff0c\u4f7f\u7528\u6765\u81ea\u4e0d\u540c\u58f0\u5b66\u573a\u666f\u7684\u6570\u636e\u96c6\uff0c\u5e76\u91c7\u7528top-1\u51c6\u786e\u6027\u548c\u7b54\u6848\u968f\u673a\u9c81\u68d2\u6027\u4f5c\u4e3a\u8bc4\u4f30\u534f\u8bae\u3002\u57fa\u7ebf\u7cfb\u7edf\u5305\u62ecQwen2-Audio-7B\u3001AudioFlamingo 2\u548cGemini-2-Flash\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\u4e0d\u540c\u6a21\u578b\u548c\u5b50\u96c6\u4e4b\u95f4\u7684\u8868\u73b0\u5dee\u5f02\u8f83\u5927\u3002", "conclusion": "\u8be5\u6311\u6218\u8d5b\u65e8\u5728\u63d0\u5347\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u7684\u97f3\u9891\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4e3aAI\u4ee3\u7406\u7684\u5b9e\u9645\u5e94\u7528\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2505.06839", "pdf": "https://arxiv.org/pdf/2505.06839", "abs": "https://arxiv.org/abs/2505.06839", "authors": ["Enric Boix-Adsera", "Philippe Rigollet"], "title": "The power of fine-grained experts: Granularity boosts expressivity in Mixture of Experts", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Mixture-of-Experts (MoE) layers are increasingly central to frontier model\narchitectures. By selectively activating parameters, they reduce computational\ncost while scaling total parameter count. This paper investigates the impact of\nthe number of active experts, termed granularity, comparing architectures with\nmany (e.g., 8 per layer in DeepSeek) to those with fewer (e.g., 1 per layer in\nLlama-4 models). We prove an exponential separation in network expressivity\nbased on this design parameter, suggesting that models benefit from higher\ngranularity. Experimental results corroborate our theoretical findings and\nillustrate this separation.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u5c42\u4e2d\u6fc0\u6d3b\u4e13\u5bb6\u6570\u91cf\uff08\u7c92\u5ea6\uff09\u5bf9\u6a21\u578b\u8868\u8fbe\u529b\u7684\u5f71\u54cd\uff0c\u8bc1\u660e\u9ad8\u7c92\u5ea6\u8bbe\u8ba1\uff08\u5982\u6bcf\u4e2a\u5c42\u6fc0\u6d3b\u591a\u4e2a\u4e13\u5bb6\uff09\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u80fd\u529b\u3002", "motivation": "\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u5c42\u901a\u8fc7\u9009\u62e9\u6027\u6fc0\u6d3b\u53c2\u6570\u6765\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u4e0d\u540c\u7c92\u5ea6\u7684\u8bbe\u8ba1\uff08\u5982\u6bcf\u5c42\u6fc0\u6d3b\u4e13\u5bb6\u6570\u91cf\uff09\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002\u7814\u7a76\u65e8\u5728\u63ed\u793a\u7c92\u5ea6\u4e0e\u6a21\u578b\u8868\u8fbe\u529b\u7684\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6bd4\u8f83\u4e0d\u540c\u7c92\u5ea6\uff08\u5982\u6bcf\u4e2a\u5c42\u6fc0\u6d3b8\u4e2a\u4e13\u5bb6\u4e0e1\u4e2a\u4e13\u5bb6\uff09\u7684\u6a21\u578b\u67b6\u6784\uff0c\u5e76\u8bc1\u660e\u5176\u5bf9\u7f51\u7edc\u8868\u8fbe\u529b\u7684\u5f71\u54cd\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u9ad8\u7c92\u5ea6\u8bbe\u8ba1\uff08\u591a\u4e13\u5bb6\u6fc0\u6d3b\uff09\u5e26\u6765\u6a21\u578b\u8868\u8fbe\u529b\u7684\u6307\u6570\u7ea7\u63d0\u5347\uff0c\u5b9e\u9a8c\u7ed3\u679c\u652f\u6301\u8fd9\u4e00\u7ed3\u8bba\u3002", "conclusion": "\u6df7\u5408\u4e13\u5bb6\u5c42\u7684\u7c92\u5ea6\u8bbe\u8ba1\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u9ad8\u7c92\u5ea6\u67b6\u6784\u80fd\u663e\u8457\u589e\u5f3a\u6a21\u578b\u8868\u8fbe\u529b\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2505.06314", "pdf": "https://arxiv.org/pdf/2505.06314", "abs": "https://arxiv.org/abs/2505.06314", "authors": ["Ashok Goel", "Ploy Thajchayapong", "Vrinda Nandan", "Harshvardhan Sikka", "Spencer Rugaber"], "title": "A4L: An Architecture for AI-Augmented Learning", "categories": ["cs.CY", "cs.AI"], "comment": "14 pages, 7 figures", "summary": "AI promises personalized learning and scalable education. As AI agents\nincreasingly permeate education in support of teaching and learning, there is a\ncritical and urgent need for data architectures for collecting and analyzing\ndata on learning, and feeding the results back to teachers, learners, and the\nAI agents for personalization of learning at scale. At the National AI\nInstitute for Adult Learning and Online Education, we are developing an\nArchitecture for AI-Augmented Learning (A4L) for supporting adult learning\nthrough online education. We present the motivations, goals, requirements of\nthe A4L architecture. We describe preliminary applications of A4L and discuss\nhow it advances the goals of making learning more personalized and scalable.", "AI": {"tldr": "AI\u6559\u80b2\u52a9\u624b\u80fd\u4e2a\u6027\u5316\u5b66\u4e60\u5e76\u6269\u5c55\u6559\u80b2\u89c4\u6a21\u3002\u56fd\u5bb6AI\u6210\u4eba\u5b66\u4e60\u4e0e\u5728\u7ebf\u6559\u80b2\u7814\u7a76\u6240\u5f00\u53d1\u7684A4L\u67b6\u6784\u901a\u8fc7\u6570\u636e\u6536\u96c6\u3001\u5206\u6790\u4e0e\u53cd\u9988\uff0c\u652f\u6301\u5728\u7ebf\u6559\u80b2\u4e2d\u7684\u6210\u4eba\u5b66\u4e60\u3002", "motivation": "AI\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u9700\u8981\u6709\u6548\u7684\u6570\u636e\u67b6\u6784\u6765\u652f\u6301\u4e2a\u6027\u5316\u5b66\u4e60\uff0c\u5e76\u6269\u5c55\u81f3\u5927\u89c4\u6a21\u6559\u80b2\u573a\u666f\u3002", "method": "\u5f00\u53d1\u4e86A4L\uff08AI-Augmented Learning\uff09\u67b6\u6784\uff0c\u7528\u4e8e\u6570\u636e\u6536\u96c6\u3001\u5206\u6790\u53ca\u53cd\u9988\uff0c\u4ee5\u652f\u6301\u5728\u7ebf\u6559\u80b2\u4e2d\u7684\u6210\u4eba\u5b66\u4e60\u3002", "result": "A4L\u67b6\u6784\u7684\u521d\u6b65\u5e94\u7528\u5c55\u793a\u4e86\u5176\u5728\u4e2a\u6027\u5316\u5b66\u4e60\u548c\u6559\u80b2\u6269\u5c55\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "A4L\u67b6\u6784\u4e3aAI\u652f\u6301\u7684\u6559\u80b2\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u4e2a\u6027\u5316\u548c\u5927\u89c4\u6a21\u6559\u80b2\u7684\u53cc\u91cd\u76ee\u6807\u3002"}}
{"id": "2505.07558", "pdf": "https://arxiv.org/pdf/2505.07558", "abs": "https://arxiv.org/abs/2505.07558", "authors": ["Rei Higuchi", "Taiji Suzuki"], "title": "Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models", "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": null, "summary": "Aligning large language models (LLMs) with human preferences is crucial for\nsafe deployment, yet existing methods assume specific preference models like\nBradley-Terry model. This assumption leads to statistical inconsistency, where\nmore data doesn't guarantee convergence to true human preferences. To address\nthis critical gap, we introduce a novel alignment method Direct Density Ratio\nOptimization (DDRO). DDRO directly estimates the density ratio between\npreferred and unpreferred output distributions, circumventing the need for\nexplicit human preference modeling. We theoretically prove that DDRO is\nstatistically consistent, ensuring convergence to the true preferred\ndistribution as the data size grows, regardless of the underlying preference\nstructure. Experiments demonstrate that DDRO achieves superior performance\ncompared to existing methods on many major benchmarks. DDRO unlocks the\npotential for truly data-driven alignment, paving the way for more reliable and\nhuman-aligned LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDDRO\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u4f30\u8ba1\u504f\u597d\u4e0e\u975e\u504f\u597d\u8f93\u51fa\u7684\u5bc6\u5ea6\u6bd4\u6765\u5bf9\u9f50\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u5bf9\u7279\u5b9a\u504f\u597d\u6a21\u578b\u7684\u4f9d\u8d56\uff0c\u8bc1\u660e\u4e86\u5176\u7edf\u8ba1\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\u4f9d\u8d56\u7279\u5b9a\u7684\u504f\u597d\u6a21\u578b\uff08\u5982Bradley-Terry\u6a21\u578b\uff09\uff0c\u5bfc\u81f4\u7edf\u8ba1\u4e0d\u4e00\u81f4\u6027\uff08\u5373\u6570\u636e\u589e\u52a0\u672a\u5fc5\u6536\u655b\u81f3\u771f\u5b9e\u4eba\u7c7b\u504f\u597d\uff09\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u4e0d\u4f9d\u8d56\u663e\u5f0f\u504f\u597d\u5efa\u6a21\u4e14\u80fd\u4fdd\u8bc1\u7edf\u8ba1\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51faDirect Density Ratio Optimization (DDRO)\uff0c\u76f4\u63a5\u4f30\u8ba1\u504f\u597d\u4e0e\u975e\u504f\u597d\u8f93\u51fa\u5206\u5e03\u7684\u5bc6\u5ea6\u6bd4\uff0c\u65e0\u9700\u663e\u5f0f\u5efa\u6a21\u4eba\u7c7b\u504f\u597d\u3002\u7406\u8bba\u4e0a\u8bc1\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u7edf\u8ba1\u4e00\u81f4\u6027\uff0c\u5373\u6570\u636e\u89c4\u6a21\u589e\u5927\u65f6\u80fd\u6536\u655b\u81f3\u771f\u5b9e\u504f\u597d\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDDRO\u5728\u591a\u4e2a\u4e3b\u6d41\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "DDRO\u4e3a\u771f\u6b63\u6570\u636e\u9a71\u52a8\u7684\u5bf9\u9f50\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u63a8\u52a8\u4e86\u66f4\u53ef\u9760\u3001\u66f4\u8d34\u5408\u4eba\u7c7b\u504f\u597d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.06849", "pdf": "https://arxiv.org/pdf/2505.06849", "abs": "https://arxiv.org/abs/2505.06849", "authors": ["Tamilselvan Subramani", "Sebastian Bartscher"], "title": "Predictive Digital Twins for Thermal Management Using Machine Learning and Reduced-Order Models", "categories": ["cs.LG", "68T07, 65M99, 80A23", "I.2.6; G.1.8; J.2"], "comment": "10 pages, 2 tables, from M.Tech. thesis accepted at BITS Pilani, 2022", "summary": "Digital twins enable real-time simulation and prediction in engineering\nsystems. This paper presents a novel framework for predictive digital twins of\na headlamp heatsink, integrating physics-based reduced-order models (ROMs) from\ncomputational fluid dynamics (CFD) with supervised machine learning. A\ncomponent-based ROM library, derived via proper orthogonal decomposition (POD),\ncaptures thermal dynamics efficiently. Machine learning models, including\nDecision Trees, k-Nearest Neighbors, Support Vector Regression (SVR), and\nNeural Networks, predict optimal ROM configurations, enabling rapid digital\ntwin updates. The Neural Network achieves a mean absolute error (MAE) of\n54.240, outperforming other models. Quantitative comparisons of predicted and\noriginal values demonstrate high accuracy. This scalable, interpretable\nframework advances thermal management in automotive systems, supporting robust\ndesign and predictive maintenance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u964d\u9636\u6a21\u578b\u548c\u76d1\u7763\u5b66\u4e60\u7684\u6570\u5b57\u5b6a\u751f\u6846\u67b6\uff0c\u7528\u4e8e\u8f66\u706f\u6563\u70ed\u5668\u7684\u5b9e\u65f6\u9884\u6d4b\uff0c\u5176\u4e2d\u795e\u7ecf\u7f51\u7edc\u8868\u73b0\u6700\u4f73\uff0c\u8bef\u5dee\u6700\u4f4e\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u8f66\u706f\u6563\u70ed\u5668\u70ed\u7ba1\u7406\u7684\u6548\u7387\u548c\u5b9e\u65f6\u9884\u6d4b\u80fd\u529b\uff0c\u9700\u8981\u7ed3\u5408\u7269\u7406\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4ee5\u4f18\u5316\u6570\u5b57\u5b6a\u751f\u7684\u66f4\u65b0\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u57fa\u4e8ePOD\u7684\u964d\u9636\u6a21\u578b\u5e93\u6355\u6349\u70ed\u529b\u5b66\u884c\u4e3a\uff0c\u5e76\u7528\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982\u51b3\u7b56\u6811\u3001k-NN\u3001SVR\u548c\u795e\u7ecf\u7f51\u7edc\uff09\u9884\u6d4b\u6700\u4f18\u6a21\u578b\u914d\u7f6e\u3002", "result": "\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u6700\u4f4e\uff0854.240\uff09\uff0c\u4e0e\u5176\u4ed6\u6a21\u578b\u76f8\u6bd4\u8868\u73b0\u6700\u4f73\uff0c\u9884\u6d4b\u7ed3\u679c\u4e0e\u539f\u6570\u636e\u5bf9\u6bd4\u663e\u793a\u51fa\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u6c7d\u8f66\u7cfb\u7edf\u70ed\u7ba1\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8bbe\u8ba1\u548c\u9884\u6d4b\u7ef4\u62a4\u652f\u6301\u3002"}}
{"id": "2505.06315", "pdf": "https://arxiv.org/pdf/2505.06315", "abs": "https://arxiv.org/abs/2505.06315", "authors": ["Jose Sanchez Vicarte", "Marcin Spoczynski", "Mostafa Elsaid"], "title": "Threat Modeling for AI: The Case for an Asset-Centric Approach", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Recent advances in AI are transforming AI's ubiquitous presence in our world\nfrom that of standalone AI-applications into deeply integrated AI-agents. These\nchanges have been driven by agents' increasing capability to autonomously make\ndecisions and initiate actions, using existing applications; whether those\napplications are AI-based or not. This evolution enables unprecedented levels\nof AI integration, with agents now able to take actions on behalf of systems\nand users -- including, in some cases, the powerful ability for the AI to write\nand execute scripts as it deems necessary. With AI systems now able to\nautonomously execute code, interact with external systems, and operate without\nhuman oversight, traditional security approaches fall short.\n  This paper introduces an asset-centric methodology for threat modeling AI\nsystems that addresses the unique security challenges posed by integrated AI\nagents. Unlike existing top-down frameworks that analyze individual attacks\nwithin specific product contexts, our bottom-up approach enables defenders to\nsystematically identify how vulnerabilities -- both conventional and\nAI-specific -- impact critical AI assets across distributed infrastructures\nused to develop and deploy these agents. This methodology allows security teams\nto: (1) perform comprehensive analysis that communicates effectively across\ntechnical domains, (2) quantify security assumptions about third-party AI\ncomponents without requiring visibility into their implementation, and (3)\nholistically identify AI-based vulnerabilities relevant to their specific\nproduct context. This approach is particularly relevant for securing agentic\nsystems with complex autonomous capabilities. By focusing on assets rather than\nattacks, our approach scales with the rapidly evolving threat landscape while\naccommodating increasingly complex and distributed AI development pipelines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ee5\u8d44\u4ea7\u4e3a\u4e2d\u5fc3\u7684AI\u7cfb\u7edf\u5a01\u80c1\u5efa\u6a21\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u96c6\u6210AI\u4ee3\u7406\u5e26\u6765\u7684\u72ec\u7279\u5b89\u5168\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u805a\u7126\u8d44\u4ea7\u800c\u975e\u653b\u51fb\uff0c\u9002\u7528\u4e8e\u590d\u6742\u4e14\u5feb\u901f\u6f14\u53d8\u7684AI\u5f00\u53d1\u751f\u6001\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u80fd\u529b\u7684\u589e\u5f3a\uff0c\u5176\u80fd\u591f\u81ea\u4e3b\u51b3\u7b56\u548c\u6267\u884c\u64cd\u4f5c\uff08\u5305\u62ec\u7f16\u5199\u548c\u6267\u884c\u811a\u672c\uff09\uff0c\u4f20\u7edf\u5b89\u5168\u65b9\u6cd5\u5df2\u65e0\u6cd5\u5e94\u5bf9\u7531\u6b64\u5e26\u6765\u7684\u65b0\u5b89\u5168\u98ce\u9669\u3002", "method": "\u91c7\u7528\u81ea\u4e0b\u800c\u4e0a\u7684\u8d44\u4ea7\u4e2d\u5fc3\u5316\u65b9\u6cd5\uff0c\u7cfb\u7edf\u6027\u8bc6\u522b\u8de8\u5206\u5e03\u5f0f\u57fa\u7840\u8bbe\u65bd\u7684\u6f0f\u6d1e\uff08\u5305\u62ec\u5e38\u89c4\u548cAI\u7279\u6709\u7684\uff09\uff0c\u5e76\u901a\u8fc7\u91cf\u5316\u5b89\u5168\u5047\u8bbe\u548c\u5168\u57df\u5206\u6790\u6765\u63d0\u5347\u5b89\u5168\u6027\u3002", "result": "\u8fd9\u4e00\u65b9\u6cd5\u80fd\u591f\u652f\u6301\u5b89\u5168\u56e2\u961f\u5168\u9762\u5206\u6790\u8de8\u6280\u672f\u9886\u57df\u7684\u98ce\u9669\u3001\u91cf\u5316\u7b2c\u4e09\u65b9AI\u7ec4\u4ef6\u7684\u5b89\u5168\u5047\u8bbe\uff0c\u5e76\u8bc6\u522b\u7279\u5b9a\u4ea7\u54c1\u73af\u5883\u4e2d\u7684AI\u6f0f\u6d1e\u3002", "conclusion": "\u805a\u7126\u8d44\u4ea7\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u5feb\u901f\u6f14\u53d8\u7684\u5a01\u80c1\u73af\u5883\u548c\u590d\u6742\u7684AI\u5f00\u53d1\u751f\u6001\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5177\u5907\u9ad8\u5ea6\u81ea\u4e3b\u80fd\u529b\u7684AI\u4ee3\u7406\u7cfb\u7edf\u3002"}}
{"id": "2505.07704", "pdf": "https://arxiv.org/pdf/2505.07704", "abs": "https://arxiv.org/abs/2505.07704", "authors": ["Elisei Rykov", "Kseniia Petrushina", "Kseniia Titova", "Anton Razzhigaev", "Alexander Panchenko", "Vasily Konovalov"], "title": "Through the Looking Glass: Common Sense Consistency Evaluation of Weird Images", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Measuring how real images look is a complex task in artificial intelligence\nresearch. For example, an image of a boy with a vacuum cleaner in a desert\nviolates common sense. We introduce a novel method, which we call Through the\nLooking Glass (TLG), to assess image common sense consistency using Large\nVision-Language Models (LVLMs) and Transformer-based encoder. By leveraging\nLVLMs to extract atomic facts from these images, we obtain a mix of accurate\nfacts. We proceed by fine-tuning a compact attention-pooling classifier over\nencoded atomic facts. Our TLG has achieved a new state-of-the-art performance\non the WHOOPS! and WEIRD datasets while leveraging a compact fine-tuning\ncomponent.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a'Through the Looking Glass (TLG)'\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u548cTransformer\u7f16\u7801\u5668\u8bc4\u4f30\u56fe\u50cf\u5e38\u8bc6\u4e00\u81f4\u6027\uff0c\u5e76\u5728WHOOPS!\u548cWEIRD\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u56fe\u50cf\u771f\u5b9e\u611f\u7684\u8bc4\u4f30\u5728AI\u7814\u7a76\u4e2d\u5177\u6709\u6311\u6218\u6027\uff0c\u4f8b\u5982\u8fdd\u53cd\u5e38\u8bc6\u7684\u56fe\u50cf\uff08\u5982\u6c99\u6f20\u4e2d\u7537\u5b69\u62ff\u5438\u5c18\u5668\uff09\u3002\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u65b9\u6cd5\u68c0\u6d4b\u6b64\u7c7b\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u7ed3\u5408LVLMs\u63d0\u53d6\u56fe\u50cf\u7684\u539f\u5b50\u4e8b\u5b9e\uff0c\u5e76\u5fae\u8c03\u4e00\u4e2a\u7d27\u51d1\u7684\u6ce8\u610f\u529b\u6c60\u5316\u5206\u7c7b\u5668\u5bf9\u7f16\u7801\u4e8b\u5b9e\u8fdb\u884c\u5206\u7c7b\u3002", "result": "TLG\u5728WHOOPS!\u548cWEIRD\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u65b0\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "TLG\u901a\u8fc7LVLMs\u548c\u8f7b\u91cf\u7ea7\u5fae\u8c03\u7ec4\u4ef6\uff0c\u9ad8\u6548\u89e3\u51b3\u4e86\u56fe\u50cf\u5e38\u8bc6\u4e00\u81f4\u6027\u8bc4\u4f30\u95ee\u9898\u3002"}}
{"id": "2505.06852", "pdf": "https://arxiv.org/pdf/2505.06852", "abs": "https://arxiv.org/abs/2505.06852", "authors": ["Ziyi Liu", "Phuc Luong", "Mario Boley", "Daniel F. Schmidt"], "title": "Improving Random Forests by Smoothing", "categories": ["cs.LG", "stat.ML"], "comment": "14 pages, 2 figures, 4 pages appendix, 3 figures in appendix", "summary": "Gaussian process regression is a popular model in the small data regime due\nto its sound uncertainty quantification and the exploitation of the smoothness\nof the regression function that is encountered in a wide range of practical\nproblems. However, Gaussian processes perform sub-optimally when the degree of\nsmoothness is non-homogeneous across the input domain. Random forest regression\npartially addresses this issue by providing local basis functions of variable\nsupport set sizes that are chosen in a data-driven way. However, they do so at\nthe expense of forgoing any degree of smoothness, which often results in poor\nperformance in the small data regime. Here, we aim to combine the advantages of\nboth models by applying a kernel-based smoothing mechanism to a learned random\nforest or any other piecewise constant prediction function. As we demonstrate\nempirically, the resulting model consistently improves the predictive\nperformance of the underlying random forests and, in almost all test cases,\nalso improves the log loss of the usual uncertainty quantification based on\ninter-tree variance. The latter advantage can be attributed to the ability of\nthe smoothing model to take into account the uncertainty over the exact\ntree-splitting locations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u548c\u968f\u673a\u68ee\u6797\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u968f\u673a\u68ee\u6797\u6216\u5176\u4ed6\u5206\u6bb5\u5e38\u6570\u9884\u6d4b\u51fd\u6570\u4e0a\u5e94\u7528\u57fa\u4e8e\u6838\u7684\u5e73\u6ed1\u673a\u5236\uff0c\u4ee5\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u5728\u5c0f\u6570\u636e\u573a\u666f\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u8f93\u5165\u57df\u5185\u5e73\u6ed1\u5ea6\u4e0d\u5747\u5300\u65f6\u8f83\u96be\u5904\u7406\uff1b\u968f\u673a\u68ee\u6797\u867d\u80fd\u9002\u5e94\u5c40\u90e8\u53d8\u5316\u4f46\u7f3a\u4e4f\u5e73\u6ed1\u6027\uff0c\u5bfc\u81f4\u5c0f\u6570\u636e\u573a\u666f\u4e0b\u6027\u80fd\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "method": "\u5728\u5b66\u4e60\u7684\u968f\u673a\u68ee\u6797\u6216\u5176\u4ed6\u5206\u6bb5\u5e38\u6570\u9884\u6d4b\u51fd\u6570\u4e0a\u5e94\u7528\u57fa\u4e8e\u6838\u7684\u5e73\u6ed1\u673a\u5236\u3002", "result": "\u65b0\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u968f\u673a\u68ee\u6797\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u5728\u51e0\u4e4e\u6240\u6709\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u6539\u8fdb\u4e86\u5bf9\u6570\u635f\u5931\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u9ad8\u65af\u8fc7\u7a0b\u548c\u968f\u673a\u68ee\u6797\u7684\u4f18\u52bf\uff0c\u65b0\u6a21\u578b\u5728\u5c0f\u6570\u636e\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u4f18\uff0c\u4e14\u80fd\u66f4\u597d\u5730\u5904\u7406\u5e73\u6ed1\u5ea6\u4e0d\u5747\u5300\u95ee\u9898\u3002"}}
{"id": "2505.07768", "pdf": "https://arxiv.org/pdf/2505.07768", "abs": "https://arxiv.org/abs/2505.07768", "authors": ["Yifeng Di", "Tianyi Zhang"], "title": "Enhancing Code Generation via Bidirectional Comment-Level Mutual Grounding", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "Accepted to ICSE 2025", "summary": "Large Language Models (LLMs) have demonstrated unprecedented capability in\ncode generation. However, LLM-generated code is still plagued with a wide range\nof functional errors, especially for complex programming tasks that LLMs have\nnot seen before. Recent studies have shown that developers often struggle with\ninspecting and fixing incorrect code generated by LLMs, diminishing their\nproductivity and trust in LLM-based code generation. Inspired by the mutual\ngrounding theory in communication, we propose an interactive approach that\nleverages code comments as a medium for developers and LLMs to establish a\nshared understanding. Our approach facilitates iterative grounding by\ninterleaving code generation, inline comment generation, and contextualized\nuser feedback through editable comments to align generated code with developer\nintent. We evaluated our approach on two popular benchmarks and demonstrated\nthat our approach significantly improved multiple state-of-the-art LLMs, e.g.,\n17.1% pass@1 improvement for code-davinci-002 on HumanEval. Furthermore, we\nconducted a user study with 12 participants in comparison to two baselines: (1)\ninteracting with GitHub Copilot, and (2) interacting with a multi-step code\ngeneration paradigm called Multi-Turn Program Synthesis. Participants completed\nthe given programming tasks 16.7% faster and with 10.5% improvement in task\nsuccess rate when using our approach. Both results show that interactively\nrefining code comments enables the collaborative establishment of mutual\ngrounding, leading to more accurate code generation and higher developer\nconfidence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ea4\u4e92\u5f0f\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ee3\u7801\u6ce8\u91ca\u4f5c\u4e3a\u5f00\u53d1\u8005\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u5efa\u7acb\u5171\u8bc6\u7684\u5a92\u4ecb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u5f00\u53d1\u8005\u7684\u4fe1\u4efb\u5ea6\u3002", "motivation": "\u7531\u4e8eLLM\u751f\u6210\u7684\u4ee3\u7801\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5e38\u5b58\u5728\u529f\u80fd\u6027\u9519\u8bef\uff0c\u5f00\u53d1\u8005\u96be\u4ee5\u68c0\u67e5\u548c\u4fee\u590d\u8fd9\u4e9b\u9519\u8bef\uff0c\u5f71\u54cd\u4e86\u751f\u4ea7\u529b\u548c\u5bf9LLM\u7684\u4fe1\u4efb\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u597d\u7684\u534f\u4f5c\u65b9\u5f0f\u3002", "method": "\u91c7\u7528\u4ea4\u4e92\u5f0f\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ea4\u66ff\u751f\u6210\u4ee3\u7801\u3001\u5185\u8054\u6ce8\u91ca\u548c\u7528\u6237\u53cd\u9988\uff0c\u9010\u6b65\u8c03\u6574\u4ee3\u7801\u4ee5\u7b26\u5408\u5f00\u53d1\u8005\u610f\u56fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728HumanEval\u57fa\u51c6\u4e0a\u4f7fcode-davinci-002\u7684pass@1\u63d0\u5347\u4e8617.1%\uff1b\u7528\u6237\u7814\u7a76\u4e2d\uff0c\u4efb\u52a1\u5b8c\u6210\u901f\u5ea6\u63d0\u534716.7%\uff0c\u6210\u529f\u7387\u63d0\u9ad810.5%\u3002", "conclusion": "\u901a\u8fc7\u4ea4\u4e92\u5f0f\u6ce8\u91ca\u4f18\u5316\uff0c\u5f00\u53d1\u8005\u4e0eLLM\u80fd\u66f4\u9ad8\u6548\u5730\u8fbe\u6210\u5171\u8bc6\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u4ee3\u7801\u751f\u6210\u5e76\u63d0\u5347\u4fe1\u4efb\u5ea6\u3002"}}
{"id": "2505.06858", "pdf": "https://arxiv.org/pdf/2505.06858", "abs": "https://arxiv.org/abs/2505.06858", "authors": ["Tianyu Chen", "Haoyi Zhou", "Ying Li", "Hao Wang", "Zhenzhe Zhang", "Tianchen Zhu", "Shanghang Zhang", "Jianxin Li"], "title": "FreqMoE: Dynamic Frequency Enhancement for Neural PDE Solvers", "categories": ["cs.LG"], "comment": "Accepted by IJCAI 2025", "summary": "Fourier Neural Operators (FNO) have emerged as promising solutions for\nefficiently solving partial differential equations (PDEs) by learning\ninfinite-dimensional function mappings through frequency domain\ntransformations. However, the sparsity of high-frequency signals limits\ncomputational efficiency for high-dimensional inputs, and fixed-pattern\ntruncation often causes high-frequency signal loss, reducing performance in\nscenarios such as high-resolution inputs or long-term predictions. To address\nthese challenges, we propose FreqMoE, an efficient and progressive training\nframework that exploits the dependency of high-frequency signals on\nlow-frequency components. The model first learns low-frequency weights and then\napplies a sparse upward-cycling strategy to construct a mixture of experts\n(MoE) in the frequency domain, effectively extending the learned weights to\nhigh-frequency regions. Experiments on both regular and irregular grid PDEs\ndemonstrate that FreqMoE achieves up to 16.6% accuracy improvement while using\nmerely 2.1% parameters (47.32x reduction) compared to dense FNO. Furthermore,\nthe approach demonstrates remarkable stability in long-term predictions and\ngeneralizes seamlessly to various FNO variants and grid structures,\nestablishing a new ``Low frequency Pretraining, High frequency Fine-tuning''\nparadigm for solving PDEs.", "AI": {"tldr": "FreqMoE\u662f\u4e00\u79cd\u9ad8\u6548\u6e10\u8fdb\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u4f4e\u9891\u5230\u9ad8\u9891\u7684\u4fe1\u53f7\u4f9d\u8d56\u5b66\u4e60\u6765\u89e3\u51b3PDE\u95ee\u9898\uff0c\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u4e0e\u6027\u80fd\u3002", "motivation": "\u9ad8\u9891\u4fe1\u53f7\u7a00\u758f\u4e0e\u56fa\u5b9a\u622a\u65ad\u5bfc\u81f4\u4f20\u7edfFNO\u5728\u9ad8\u7ef4\u8f93\u5165\u6216\u957f\u671f\u9884\u6d4b\u4e2d\u6548\u7387\u4e0e\u6027\u80fd\u53d7\u9650\u3002", "method": "\u5148\u5b66\u4e60\u4f4e\u9891\u6743\u91cd\uff0c\u518d\u901a\u8fc7\u7a00\u758f\u4e0a\u884c\u5faa\u73af\u7b56\u7565\u6784\u5efa\u9891\u57dfMoE\uff0c\u5c06\u6743\u91cd\u6269\u5c55\u5230\u9ad8\u9891\u533a\u57df\u3002", "result": "\u5728\u89c4\u5219\u4e0e\u975e\u89c4\u5219\u7f51\u683cPDE\u4e0a\uff0cFreqMoE\u7cbe\u5ea6\u63d0\u534716.6%\uff0c\u53c2\u6570\u51cf\u5c1147.32\u500d\uff0c\u4e14\u957f\u671f\u9884\u6d4b\u7a33\u5b9a\u3002", "conclusion": "\u63d0\u51fa\u201c\u4f4e\u9891\u9884\u8bad\u7ec3-\u9ad8\u9891\u5fae\u8c03\u201d\u65b0\u8303\u5f0f\uff0c\u9002\u7528\u4e8e\u591a\u79cdFNO\u53d8\u4f53\u4e0e\u7f51\u683c\u7ed3\u6784\u3002"}}
{"id": "2505.06863", "pdf": "https://arxiv.org/pdf/2505.06863", "abs": "https://arxiv.org/abs/2505.06863", "authors": ["Jiebo Song", "Huaming Ling"], "title": "Masked Subspace Clustering Methods", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "To further utilize the unsupervised features and pairwise information, we\npropose a general Bilevel Clustering Optimization (BCO) framework to improve\nthe performance of clustering. And then we introduce three special cases on\nsubspace clustering with two different types of masks. At first, we reformulate\nthe original subspace clustering as a Basic Masked Subspace Clustering (BMSC),\nwhich reformulate the diagonal constraints to a hard mask. Then, we provide a\nGeneral Masked Subspace Clustering (GMSC) method to integrate different\nclustering via a soft mask. Furthermore, based on BCO and GMSC, we induce a\nlearnable soft mask and design a Recursive Masked Subspace Clustering (RMSC)\nmethod that can alternately update the affinity matrix and the soft mask.\nNumerical experiments show that our models obtain significant improvement\ncompared with the baselines on several commonly used datasets, such as MNIST,\nUSPS, ORL, COIL20 and COIL100.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2aBilevel Clustering Optimization (BCO)\u6846\u67b6\u6765\u63d0\u5347\u805a\u7c7b\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u4e09\u79cd\u5b50\u7a7a\u95f4\u805a\u7c7b\u7684\u7279\u6b8a\u6848\u4f8b\uff08BMSC\u3001GMSC\u3001RMSC\uff09\u5c55\u793a\u5176\u6709\u6548\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u591a\u4e2a\u5e38\u7528\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u4e3a\u4e86\u8fdb\u4e00\u6b65\u5229\u7528\u65e0\u76d1\u7763\u7279\u5f81\u548c\u6210\u5bf9\u4fe1\u606f\uff0c\u63d0\u5347\u805a\u7c7b\u6027\u80fd\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86BCO\u6846\u67b6\u53ca\u5176\u5177\u4f53\u5b9e\u73b0\u65b9\u6cd5\u3002", "method": "\u8bba\u6587\u5f15\u5165\u4e86\u57fa\u4e8eBCO\u6846\u67b6\u7684\u4e09\u79cd\u5b50\u7a7a\u95f4\u805a\u7c7b\u65b9\u6cd5\uff1aBMSC\uff08\u786c\u63a9\u7801\uff09\u3001GMSC\uff08\u8f6f\u63a9\u7801\uff09\u548cRMSC\uff08\u53ef\u5b66\u4e60\u8f6f\u63a9\u7801\uff09\uff0c\u901a\u8fc7\u4ea4\u66ff\u66f4\u65b0\u76f8\u4f3c\u77e9\u9635\u548c\u63a9\u7801\u4f18\u5316\u805a\u7c7b\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u5728MNIST\u3001USPS\u3001ORL\u3001COIL20\u548cCOIL100\u7b49\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "BCO\u6846\u67b6\u53ca\u5176\u884d\u751f\u65b9\u6cd5\uff08BMSC\u3001GMSC\u3001RMSC\uff09\u901a\u8fc7\u63a9\u7801\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u805a\u7c7b\u6027\u80fd\uff0c\u5c55\u73b0\u4e86\u5728\u65e0\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.06324", "pdf": "https://arxiv.org/pdf/2505.06324", "abs": "https://arxiv.org/abs/2505.06324", "authors": ["Vipula Rawte", "Ryan A. Rossi", "Franck Dernoncourt", "Nedim Lipka"], "title": "Document Attribution: Examining Citation Relationships using Large Language Models", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "As Large Language Models (LLMs) are increasingly applied to document-based\ntasks - such as document summarization, question answering, and information\nextraction - where user requirements focus on retrieving information from\nprovided documents rather than relying on the model's parametric knowledge,\nensuring the trustworthiness and interpretability of these systems has become a\ncritical concern. A central approach to addressing this challenge is\nattribution, which involves tracing the generated outputs back to their source\ndocuments. However, since LLMs can produce inaccurate or imprecise responses,\nit is crucial to assess the reliability of these citations.\n  To tackle this, our work proposes two techniques. (1) A zero-shot approach\nthat frames attribution as a straightforward textual entailment task. Our\nmethod using flan-ul2 demonstrates an improvement of 0.27% and 2.4% over the\nbest baseline of ID and OOD sets of AttributionBench, respectively. (2) We also\nexplore the role of the attention mechanism in enhancing the attribution\nprocess. Using a smaller LLM, flan-t5-small, the F1 scores outperform the\nbaseline across almost all layers except layer 4 and layers 8 through 11.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u6280\u672f\u6765\u8bc4\u4f30LLM\u8f93\u51fa\u7684\u53ef\u9760\u6027\uff1a\u57fa\u4e8e\u96f6\u6837\u672c\u7684\u6587\u672c\u8574\u542b\u65b9\u6cd5\u548c\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4e24\u79cd\u65b9\u6cd5\u5728AttributionBench\u6570\u636e\u96c6\u4e0a\u5747\u6709\u63d0\u5347\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6587\u6863\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u786e\u4fdd\u5176\u8f93\u51fa\u7684\u53ef\u4fe1\u6027\u548c\u53ef\u89e3\u91ca\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u53ef\u80fd\u4ea7\u751f\u4e0d\u51c6\u786e\u6216\u4e0d\u7cbe\u786e\u7684\u5f15\u7528\uff0c\u9700\u8981\u8bc4\u4f30\u53ef\u9760\u6027\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u6280\u672f\uff1a1\uff09\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u5c06\u5f52\u5c5e\u95ee\u9898\u5efa\u6a21\u4e3a\u6587\u672c\u8574\u542b\u4efb\u52a1\uff1b2\uff09\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\u5f52\u5c5e\u8fc7\u7a0b\u3002\u5206\u522b\u4f7f\u7528flan-ul2\u548cflan-t5-small\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u96f6\u6837\u672c\u65b9\u6cd5\u5728AttributionBench\u6570\u636e\u96c6\u7684ID\u548cOOD\u96c6\u4e0a\u5206\u522b\u63d0\u53470.27%\u548c2.4%\u3002\u6ce8\u610f\u529b\u673a\u5236\u65b9\u6cd5\uff08flan-t5-small\uff09\u5728\u5927\u591a\u6570\u5c42\u7684F1\u5206\u6570\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u4e24\u79cd\u65b9\u6cd5\u5747\u80fd\u6709\u6548\u63d0\u5347LLM\u8f93\u51fa\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u6027\uff0c\u4e3a\u6587\u6863\u4efb\u52a1\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.06874", "pdf": "https://arxiv.org/pdf/2505.06874", "abs": "https://arxiv.org/abs/2505.06874", "authors": ["Thanh Son Nguyen", "Van Thanh Nguyen", "Dang Minh Duc Nguyen"], "title": "Enhancing Time Series Forecasting via a Parallel Hybridization of ARIMA and Polynomial Classifiers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time series forecasting has attracted significant attention, leading to the\nde-velopment of a wide range of approaches, from traditional statistical\nmeth-ods to advanced deep learning models. Among them, the Auto-Regressive\nIntegrated Moving Average (ARIMA) model remains a widely adopted linear\ntechnique due to its effectiveness in modeling temporal dependencies in\neconomic, industrial, and social data. On the other hand, polynomial\nclassifi-ers offer a robust framework for capturing non-linear relationships\nand have demonstrated competitive performance in domains such as stock price\npre-diction. In this study, we propose a hybrid forecasting approach that\ninte-grates the ARIMA model with a polynomial classifier to leverage the\ncom-plementary strengths of both models. The hybrid method is evaluated on\nmultiple real-world time series datasets spanning diverse domains. Perfor-mance\nis assessed based on forecasting accuracy and computational effi-ciency.\nExperimental results reveal that the proposed hybrid model consist-ently\noutperforms the individual models in terms of prediction accuracy, al-beit with\na modest increase in execution time.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408ARIMA\u6a21\u578b\u548c\u591a\u9879\u5f0f\u5206\u7c7b\u5668\u7684\u6df7\u5408\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u4e24\u8005\u7684\u4f18\u52bf\uff0c\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u5355\u4e00\u6a21\u578b\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5c3d\u7ba1\u8ba1\u7b97\u65f6\u95f4\u7565\u6709\u589e\u52a0\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u9886\u57df\u5df2\u6709\u591a\u79cd\u65b9\u6cd5\uff0c\u4f20\u7edfARIMA\u6a21\u578b\u64c5\u957f\u6355\u6349\u7ebf\u6027\u65f6\u95f4\u4f9d\u8d56\uff0c\u800c\u591a\u9879\u5f0f\u5206\u7c7b\u5668\u5728\u975e\u7ebf\u6027\u5173\u7cfb\u4e2d\u8868\u73b0\u4f18\u79c0\u3002\u4e3a\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u6df7\u5408\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u6574\u5408ARIMA\u6a21\u578b\u7684\u7ebf\u6027\u65f6\u95f4\u4f9d\u8d56\u5efa\u6a21\u80fd\u529b\u548c\u591a\u9879\u5f0f\u5206\u7c7b\u5668\u7684\u975e\u7ebf\u6027\u5173\u7cfb\u6355\u6349\u80fd\u529b\uff0c\u6784\u5efa\u6df7\u5408\u9884\u6d4b\u6846\u67b6\uff0c\u5e76\u5728\u591a\u79cd\u771f\u5b9e\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6df7\u5408\u6a21\u578b\u5728\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u5355\u72ec\u7684ARIMA\u6216\u591a\u9879\u5f0f\u5206\u7c7b\u5668\uff0c\u4f46\u8ba1\u7b97\u65f6\u95f4\u7565\u6709\u589e\u52a0\u3002", "conclusion": "\u6df7\u5408\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6848\uff0c\u9002\u5408\u591a\u9886\u57df\u7684\u5e94\u7528\u3002"}}
{"id": "2505.06890", "pdf": "https://arxiv.org/pdf/2505.06890", "abs": "https://arxiv.org/abs/2505.06890", "authors": ["Kosuke Ukita", "Ye Xiaolong", "Tsuyoshi Okita"], "title": "Image Classification Using a Diffusion Model as a Pre-Training Model", "categories": ["cs.LG", "cs.CV", "eess.IV"], "comment": "10 pages, 9 figures", "summary": "In this paper, we propose a diffusion model that integrates a\nrepresentation-conditioning mechanism, where the representations derived from a\nVision Transformer (ViT) are used to condition the internal process of a\nTransformer-based diffusion model. This approach enables\nrepresentation-conditioned data generation, addressing the challenge of\nrequiring large-scale labeled datasets by leveraging self-supervised learning\non unlabeled data. We evaluate our method through a zero-shot classification\ntask for hematoma detection in brain imaging. Compared to the strong\ncontrastive learning baseline, DINOv2, our method achieves a notable\nimprovement of +6.15% in accuracy and +13.60% in F1-score, demonstrating its\neffectiveness in image classification.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9Transformer\uff08ViT\uff09\u8868\u793a\u7684\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5e76\u5728\u8111\u5f71\u50cf\u8840\u80bf\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u57fa\u7ebf\uff08DINOv2\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u751f\u6210\u6a21\u578b\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u6570\u636e\u751f\u6210\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8868\u793a\u6761\u4ef6\u673a\u5236\uff0c\u5229\u7528ViT\u751f\u6210\u7684\u6761\u4ef6\u8868\u793a\u6307\u5bfc\u6269\u6563\u8fc7\u7a0b\u7684\u5185\u90e8\u751f\u6210\u3002", "result": "\u5728\u96f6\u6837\u672c\u8840\u80bf\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u51c6\u786e\u7387\u548cF1\u5206\u6570\u5206\u522b\u6bd4DINOv2\u57fa\u7ebf\u63d0\u9ad8\u4e866.15%\u548c13.60%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u6570\u636e\u6807\u6ce8\u9700\u6c42\u7684\u540c\u65f6\u63d0\u5347\u4e86\u751f\u6210\u6a21\u578b\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u533b\u5b66\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.06326", "pdf": "https://arxiv.org/pdf/2505.06326", "abs": "https://arxiv.org/abs/2505.06326", "authors": ["Alexander Ettinger"], "title": "Enterprise Architecture as a Dynamic Capability for Scalable and Sustainable Generative AI adoption: Bridging Innovation and Governance in Large Organisations", "categories": ["cs.CY", "cs.AI"], "comment": "82 pages excluding appendix", "summary": "Generative Artificial Intelligence is a powerful new technology with the\npotential to boost innovation and reshape governance in many industries.\nNevertheless, organisations face major challenges in scaling GenAI, including\ntechnology complexity, governance gaps and resource misalignments. This study\nexplores how Enterprise Architecture Management can meet the complex\nrequirements of GenAI adoption within large enterprises. Based on a systematic\nliterature review and the qualitative analysis of 16 semi-structured interviews\nwith experts, it examines the relationships between EAM, dynamic capabilities\nand GenAI adoption. The review identified key limitations in existing EA\nframeworks, particularly their inability to fully address the unique\nrequirements of GenAI. The interviews, analysed using the Gioia methodology,\nrevealed critical enablers and barriers to GenAI adoption across industries.\nThe findings indicate that EAM, when theorised as sensing, seizing and\ntransforming dynamic capabilities, can enhance GenAI adoption by improving\nstrategic alignment, governance frameworks and organisational agility. However,\nthe study also highlights the need to tailor EA frameworks to GenAI-specific\nchallenges, including low data governance maturity and the balance between\ninnovation and compliance. Several conceptual frameworks are proposed to guide\nEA leaders in aligning GenAI maturity with organisational readiness. The work\ncontributes to academic understanding and industry practice by clarifying the\nrole of EA in bridging innovation and governance in disruptive technology\nenvironments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u4f01\u4e1a\u67b6\u6784\u7ba1\u7406\uff08EAM\uff09\u5982\u4f55\u652f\u6301\u5927\u578b\u4f01\u4e1a\u4e2d\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u7684\u91c7\u7528\u3002\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u548c\u4e13\u5bb6\u8bbf\u8c08\uff0c\u7814\u7a76\u6307\u51fa\u4e86\u73b0\u6709EA\u6846\u67b6\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u52a8\u6001\u80fd\u529b\u7406\u8bba\u89c6\u89d2\u4e0b\u7684EAM\u5bf9GenAI\u91c7\u7528\u7684\u4fc3\u8fdb\u4f5c\u7528\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5177\u6709\u91cd\u5851\u884c\u4e1a\u7684\u6f5c\u529b\uff0c\u4f46\u4f01\u4e1a\u5728\u89c4\u6a21\u5316\u91c7\u7528\u65f6\u9762\u4e34\u6280\u672f\u590d\u6742\u6027\u3001\u6cbb\u7406\u7f3a\u5931\u7b49\u6311\u6218\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22EAM\u5982\u4f55\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u548c\u5bf916\u4f4d\u4e13\u5bb6\u7684\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u91c7\u7528Gioia\u65b9\u6cd5\u8bba\u5206\u6790\u6570\u636e\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cEAM\u4f5c\u4e3a\u52a8\u6001\u80fd\u529b\uff08\u611f\u77e5\u3001\u6355\u6349\u3001\u8f6c\u5316\uff09\u53ef\u4ee5\u63d0\u5347GenAI\u91c7\u7528\uff0c\u4f46\u9700\u8981\u9488\u5bf9\u5176\u7279\u5b9a\u6311\u6218\uff08\u5982\u6570\u636e\u6cbb\u7406\u6210\u719f\u5ea6\u4f4e\uff09\u8c03\u6574\u6846\u67b6\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u6982\u5ff5\u6846\u67b6\u4ee5\u6307\u5bfc\u4f01\u4e1a\u5b9e\u8df5\uff0c\u5f3a\u8c03\u4e86EAM\u5728\u5e73\u8861\u521b\u65b0\u4e0e\u6cbb\u7406\u4e2d\u7684\u6865\u6881\u4f5c\u7528\uff0c\u4e3a\u5b66\u672f\u548c\u884c\u4e1a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2505.06892", "pdf": "https://arxiv.org/pdf/2505.06892", "abs": "https://arxiv.org/abs/2505.06892", "authors": ["Zhen Liu", "Yicheng Luo", "Boyuan Li", "Emadeldeen Eldele", "Min Wu", "Qianli Ma"], "title": "Learning Soft Sparse Shapes for Efficient Time-Series Classification", "categories": ["cs.LG"], "comment": "Accepted in ICML 2025", "summary": "Shapelets are discriminative subsequences (or shapes) with high\ninterpretability in time series classification. Due to the time-intensive\nnature of shapelet discovery, existing shapelet-based methods mainly focus on\nselecting discriminative shapes while discarding others to achieve candidate\nsubsequence sparsification. However, this approach may exclude beneficial\nshapes and overlook the varying contributions of shapelets to classification\nperformance. To this end, we propose a \\textbf{Soft} sparse \\textbf{Shape}s\n(\\textbf{SoftShape}) model for efficient time series classification. Our\napproach mainly introduces soft shape sparsification and soft shape learning\nblocks. The former transforms shapes into soft representations based on\nclassification contribution scores, merging lower-scored ones into a single\nshape to retain and differentiate all subsequence information. The latter\nfacilitates intra- and inter-shape temporal pattern learning, improving model\nefficiency by using sparsified soft shapes as inputs. Specifically, we employ a\nlearnable router to activate a subset of class-specific expert networks for\nintra-shape pattern learning. Meanwhile, a shared expert network learns\ninter-shape patterns by converting sparsified shapes into sequences. Extensive\nexperiments show that SoftShape outperforms state-of-the-art methods and\nproduces interpretable results.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSoftShape\u7684\u65b0\u6a21\u578b\uff0c\u901a\u8fc7\u8f6f\u7a00\u758f\u5316\u548c\u8f6f\u5f62\u72b6\u5b66\u4e60\u5757\u6765\u9ad8\u6548\u5206\u7c7b\u65f6\u95f4\u5e8f\u5217\uff0c\u4fdd\u7559\u6240\u6709\u5b50\u5e8f\u5217\u4fe1\u606f\u5e76\u63d0\u9ad8\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f62\u72b6\u53d1\u73b0\u65b9\u6cd5\u56e0\u65f6\u95f4\u5bc6\u96c6\u6027\u800c\u4e22\u5f03\u90e8\u5206\u5f62\u72b6\uff0c\u53ef\u80fd\u5bfc\u81f4\u6709\u76ca\u4fe1\u606f\u4e22\u5931\u4e14\u5ffd\u89c6\u5f62\u72b6\u5bf9\u5206\u7c7b\u7684\u8d21\u732e\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u8f6f\u5f62\u72b6\u7a00\u758f\u5316\u548c\u8f6f\u5f62\u72b6\u5b66\u4e60\u5757\uff0c\u5c06\u5f62\u72b6\u8f6c\u5316\u4e3a\u8f6f\u8868\u793a\u5e76\u4f18\u5316\u6a21\u578b\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSoftShape\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u751f\u6210\u53ef\u89e3\u91ca\u7ed3\u679c\u3002", "conclusion": "SoftShape\u6a21\u578b\u901a\u8fc7\u4fdd\u7559\u548c\u533a\u5206\u6240\u6709\u5b50\u5e8f\u5217\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2505.06911", "pdf": "https://arxiv.org/pdf/2505.06911", "abs": "https://arxiv.org/abs/2505.06911", "authors": ["Lishan Yang", "Wei Zhang", "Quan Z. Sheng", "Weitong Chen", "Lina Yao", "Weitong Chen", "Ali Shakeri"], "title": "MMiC: Mitigating Modality Incompleteness in Clustered Federated Learning", "categories": ["cs.LG", "cs.AI", "I.2.11; I.2.7"], "comment": "10 pages, 10 figures, it's KDD'2025 under reviewing", "summary": "In the era of big data, data mining has become indispensable for uncovering\nhidden patterns and insights from vast and complex datasets. The integration of\nmultimodal data sources further enhances its potential. Multimodal Federated\nLearning (MFL) is a distributed approach that enhances the efficiency and\nquality of multimodal learning, ensuring collaborative work and privacy\nprotection. However, missing modalities pose a significant challenge in MFL,\noften due to data quality issues or privacy policies across the clients. In\nthis work, we present MMiC, a framework for Mitigating Modality incompleteness\nin MFL within the Clusters. MMiC replaces partial parameters within client\nmodels inside clusters to mitigate the impact of missing modalities.\nFurthermore, it leverages the Banzhaf Power Index to optimize client selection\nunder these conditions. Finally, MMiC employs an innovative approach to\ndynamically control global aggregation by utilizing Markovitz Portfolio\nOptimization. Extensive experiments demonstrate that MMiC consistently\noutperforms existing federated learning architectures in both global and\npersonalized performance on multimodal datasets with missing modalities,\nconfirming the effectiveness of our proposed solution.", "AI": {"tldr": "MMiC\u6846\u67b6\u901a\u8fc7\u53c2\u6570\u66ff\u6362\u3001Banzhaf\u6743\u529b\u6307\u6570\u4f18\u5316\u5ba2\u6237\u7aef\u9009\u62e9\u53caMarkovitz\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u52a8\u6001\u63a7\u5236\u5168\u5c40\u805a\u5408\uff0c\u6709\u6548\u89e3\u51b3\u591a\u6a21\u6001\u8054\u90a6\u5b66\u4e60\u4e2d\u6a21\u6001\u7f3a\u5931\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u591a\u6a21\u6001\u8054\u90a6\u5b66\u4e60\u4e2d\u6a21\u6001\u7f3a\u5931\u95ee\u9898\u7531\u4e8e\u6570\u636e\u8d28\u91cf\u6216\u9690\u79c1\u653f\u7b56\u800c\u666e\u904d\u5b58\u5728\uff0c\u5f71\u54cd\u5b66\u4e60\u6548\u7387\u548c\u8d28\u91cf\u3002", "method": "\u63d0\u51faMMiC\u6846\u67b6\uff0c\u5305\u62ec\u5ba2\u6237\u7aef\u6a21\u578b\u53c2\u6570\u66ff\u6362\u3001Banzhaf\u6743\u529b\u6307\u6570\u4f18\u5316\u9009\u62e9\u548cMarkovitz\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u5168\u5c40\u805a\u5408\u3002", "result": "\u5728\u6a21\u6001\u7f3a\u5931\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\uff0cMMiC\u5728\u5168\u7403\u548c\u4e2a\u6027\u5316\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u67b6\u6784\u3002", "conclusion": "MMiC\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6a21\u6001\u7f3a\u5931\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u679c\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.06917", "pdf": "https://arxiv.org/pdf/2505.06917", "abs": "https://arxiv.org/abs/2505.06917", "authors": ["Yuqi Xiong", "Yang Wen"], "title": "Non-Stationary Time Series Forecasting Based on Fourier Analysis and Cross Attention Mechanism", "categories": ["cs.LG"], "comment": "IJCNN 2025", "summary": "Time series forecasting has important applications in financial analysis,\nweather forecasting, and traffic management. However, existing deep learning\nmodels are limited in processing non-stationary time series data because they\ncannot effectively capture the statistical characteristics that change over\ntime. To address this problem, this paper proposes a new framework, AEFIN,\nwhich enhances the information sharing ability between stable and unstable\ncomponents by introducing a cross-attention mechanism, and combines Fourier\nanalysis networks with MLP to deeply explore the seasonal patterns and trend\ncharacteristics in unstable components. In addition, we design a new loss\nfunction that combines time-domain stability constraints, time-domain\ninstability constraints, and frequency-domain stability constraints to improve\nthe accuracy and robustness of forecasting. Experimental results show that\nAEFIN outperforms the most common models in terms of mean square error and mean\nabsolute error, especially under non-stationary data conditions, and shows\nexcellent forecasting capabilities. This paper provides an innovative solution\nfor the modeling and forecasting of non-stationary time series data, and\ncontributes to the research of deep learning for complex time series.", "AI": {"tldr": "\u63d0\u51faAEFIN\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u673a\u5236\u548c\u5085\u91cc\u53f6\u5206\u6790\u7f51\u7edc\u7ed3\u5408MLP\u5904\u7406\u975e\u5e73\u7a33\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u6709\u6548\u5904\u7406\u975e\u5e73\u7a33\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u7edf\u8ba1\u7279\u6027\u53d8\u5316\u3002", "method": "\u5f15\u5165\u8de8\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u4fe1\u606f\u5171\u4eab\uff0c\u7ed3\u5408\u5085\u91cc\u53f6\u5206\u6790\u7f51\u7edc\u548cMLP\uff0c\u8bbe\u8ba1\u65b0\u578b\u635f\u5931\u51fd\u6570\u3002", "result": "AEFIN\u5728\u5747\u65b9\u8bef\u5dee\u548c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e0a\u4f18\u4e8e\u5e38\u89c1\u6a21\u578b\uff0c\u5c24\u5176\u5728\u975e\u5e73\u7a33\u6570\u636e\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "AEFIN\u4e3a\u975e\u5e73\u7a33\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u4e0e\u9884\u6d4b\u63d0\u4f9b\u521b\u65b0\u65b9\u6848\uff0c\u63a8\u52a8\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u3002"}}
{"id": "2505.06936", "pdf": "https://arxiv.org/pdf/2505.06936", "abs": "https://arxiv.org/abs/2505.06936", "authors": ["Mohammad Mashayekhi", "Kamran Salehian"], "title": "AI-Powered Inverse Design of Ku-Band SIW Resonant Structures by Iterative Residual Correction Network", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages, 14 figures", "summary": "Inverse electromagnetic modeling has emerged as a powerful approach for\ndesigning complex microwave structures with high accuracy and efficiency. In\nthis study, we propose an Iterative Residual Correction Network (IRC-Net) for\nthe inverse design of Ku-band Substrate Integrated Waveguide (SIW) components\nbased on multimode resonators. We use a multimode resonance structure to\ndemonstrate that it is possible to control the resonances of the structure.\nTherefore, these structures can be used for resonant components and smart\nfilter design. The proposed deep learning architecture leverages residual\nneural networks to overcome the limitations of traditional inverse design\ntechniques, such as the Feedforward Inverse Model (FIM), offering improved\ngeneralization and prediction accuracy. The approach begins with a FIM to\ngenerate initial design estimates, followed by an iterative correction strategy\ninspired by the Hybrid Inverse-Forward Residual Refinement Network\n(HiFR\\textsuperscript{2}-Net), which we call IRC-Net. Experiments demonstrate\nthat the IRC-Net achieves substantial improvements in prediction accuracy\ncompared to traditional single-stage networks, validated through statistical\nmetrics, full-wave electromagnetic simulations, and measurements. To validate\nthe proposed framework, we first design and fabricate a three-resonance SIW\nstructure. Next, we apply the trained IRC-Net model to predict the geometry of\na four-resonance structure based on its desired frequency response. Both\ndesigns are fabricated and tested, showing strong agreement between the\nsimulated, predicted, and measured results, confirming the effectiveness and\npracticality of the proposed method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eKu\u6ce2\u6bb5\u57fa\u677f\u96c6\u6210\u6ce2\u5bfc\uff08SIW\uff09\u7ec4\u4ef6\u9006\u5411\u8bbe\u8ba1\u7684\u8fed\u4ee3\u6b8b\u5dee\u6821\u6b63\u7f51\u7edc\uff08IRC-Net\uff09\uff0c\u901a\u8fc7\u591a\u6a21\u8c10\u632f\u7ed3\u6784\u63a7\u5236\u8c10\u632f\uff0c\u63d0\u9ad8\u4e86\u4f20\u7edf\u9006\u5411\u8bbe\u8ba1\u6280\u672f\u7684\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u9006\u5411\u8bbe\u8ba1\u6280\u672f\uff08\u5982FIM\uff09\u5728\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u8bbe\u8ba1\u590d\u6742\u7684\u5fae\u6ce2\u7ed3\u6784\u3002", "method": "\u91c7\u7528\u7ed3\u5408\u524d\u9988\u9006\u5411\u6a21\u578b\uff08FIM\uff09\u521d\u59cb\u4f30\u8ba1\u548c\u8fed\u4ee3\u6b8b\u5dee\u6821\u6b63\u7b56\u7565\uff08IRC-Net\uff09\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u4f18\u5316\u591a\u6a21\u8c10\u632fSIW\u7ec4\u4ef6\u7684\u8bbe\u8ba1\u3002", "result": "IRC-Net\u5728\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u5355\u9636\u6bb5\u7f51\u7edc\uff0c\u4eff\u771f\u4e0e\u5b9e\u6d4b\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\uff0c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "IRC-Net\u4e3a\u5fae\u6ce2\u7ec4\u4ef6\u7684\u9006\u5411\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u7cbe\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8c10\u632f\u5143\u4ef6\u548c\u667a\u80fd\u6ee4\u6ce2\u5668\u8bbe\u8ba1\u3002"}}
{"id": "2505.06945", "pdf": "https://arxiv.org/pdf/2505.06945", "abs": "https://arxiv.org/abs/2505.06945", "authors": ["Maryam Farhadizadeh", "Maria Weymann", "Michael Bla\u00df", "Johann Kraus", "Christopher Gundler", "Sebastian Walter", "Noah Hempen", "Harald Binde", "Nadine Binder"], "title": "A systematic review of challenges and proposed solutions in modeling multimodal data", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Multimodal data modeling has emerged as a powerful approach in clinical\nresearch, enabling the integration of diverse data types such as imaging,\ngenomics, wearable sensors, and electronic health records. Despite its\npotential to improve diagnostic accuracy and support personalized care,\nmodeling such heterogeneous data presents significant technical challenges.\nThis systematic review synthesizes findings from 69 studies to identify common\nobstacles, including missing modalities, limited sample sizes, dimensionality\nimbalance, interpretability issues, and finding the optimal fusion techniques.\nWe highlight recent methodological advances, such as transfer learning,\ngenerative models, attention mechanisms, and neural architecture search that\noffer promising solutions. By mapping current trends and innovations, this\nreview provides a comprehensive overview of the field and offers practical\ninsights to guide future research and development in multimodal modeling for\nmedical applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff069\u9879\u7814\u7a76\uff0c\u603b\u7ed3\u4e86\u591a\u6a21\u6001\u6570\u636e\u5efa\u6a21\u5728\u4e34\u5e8a\u7814\u7a76\u4e2d\u7684\u5e38\u89c1\u6280\u672f\u6311\u6218\u53ca\u6700\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u7684\u533b\u5b66\u5e94\u7528\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "motivation": "\u591a\u6a21\u6001\u6570\u636e\u5efa\u6a21\u5728\u4e34\u5e8a\u7814\u7a76\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u6574\u5408\u5f02\u6784\u6570\u636e\uff08\u5982\u5f71\u50cf\u3001\u57fa\u56e0\u7ec4\u3001\u53ef\u7a7f\u6234\u8bbe\u5907\u6570\u636e\u7b49\uff09\u9762\u4e34\u8bf8\u591a\u6280\u672f\u96be\u9898\uff0c\u5982\u7f3a\u5931\u6a21\u6001\u3001\u5c0f\u6837\u672c\u91cf\u7b49\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u603b\u7ed3\u548c\u65b9\u6cd5\u521b\u65b0\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5206\u679069\u9879\u7814\u7a76\uff0c\u8bc6\u522b\u591a\u6a21\u6001\u5efa\u6a21\u4e2d\u7684\u5173\u952e\u969c\u788d\u53ca\u89e3\u51b3\u65b9\u6848\uff08\u5982\u8fc1\u79fb\u5b66\u4e60\u3001\u751f\u6210\u6a21\u578b\u3001\u6ce8\u610f\u529b\u673a\u5236\u7b49\uff09\u3002", "result": "\u603b\u7ed3\u4e86\u5f53\u524d\u591a\u6a21\u6001\u5efa\u6a21\u7684\u6311\u6218\uff08\u5982\u7ef4\u5ea6\u4e0d\u5e73\u8861\u3001\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff09\u548c\u524d\u6cbf\u65b9\u6cd5\u8fdb\u5c55\uff0c\u63d0\u4f9b\u4e86\u9886\u57df\u53d1\u5c55\u8d8b\u52bf\u7684\u5168\u9762\u6982\u89c8\u3002", "conclusion": "\u591a\u6a21\u6001\u533b\u5b66\u6570\u636e\u5efa\u6a21\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u878d\u5408\u6280\u672f\uff0c\u672a\u6765\u7814\u7a76\u53ef\u501f\u52a9\u7efc\u8ff0\u4e2d\u63d0\u51fa\u7684\u65b9\u6cd5\u8bba\uff08\u5982\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff09\u63a8\u52a8\u4e2a\u6027\u5316\u533b\u7597\u53d1\u5c55\u3002"}}
{"id": "2505.06347", "pdf": "https://arxiv.org/pdf/2505.06347", "abs": "https://arxiv.org/abs/2505.06347", "authors": ["Qing-Hong Cao", "Zong-Yue Hou", "Ying-Ying Li", "Xiaohui Liu", "Zhuo-Yang Song", "Liang-Qi Zhang", "Shutao Zhang", "Ke Zhao"], "title": "Quantum State Preparation via Large-Language-Model-Driven Evolution", "categories": ["quant-ph", "cs.AI", "hep-lat", "hep-ph"], "comment": "6 + 4 pages, 14 figures", "summary": "We propose an automated framework for quantum circuit design by integrating\nlarge-language models (LLMs) with evolutionary optimization to overcome the\nrigidity, scalability limitations, and expert dependence of traditional ones in\nvariational quantum algorithms. Our approach (FunSearch) autonomously discovers\nhardware-efficient ans\\\"atze with new features of scalability and\nsystem-size-independent number of variational parameters entirely from scratch.\nDemonstrations on the Ising and XY spin chains with n = 9 qubits yield circuits\ncontaining 4 parameters, achieving near-exact energy extrapolation across\nsystem sizes. Implementations on quantum hardware (Zuchongzhi chip) validate\npracticality, where two-qubit quantum gate noises can be effectively mitigated\nvia zero-noise extrapolations for a spin chain system as large as 20 sites.\nThis framework bridges algorithmic design and experimental constraints,\ncomplementing contemporary quantum architecture search frameworks to advance\nscalable quantum simulations.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u52a8\u5316\u6846\u67b6FunSearch\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u8fdb\u5316\u4f18\u5316\u8bbe\u8ba1\u91cf\u5b50\u7535\u8def\uff0c\u89e3\u51b3\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5\u4e2d\u7684\u521a\u6027\u3001\u6269\u5c55\u6027\u53ca\u4f9d\u8d56\u4e13\u5bb6\u95ee\u9898\uff0c\u5c55\u793a\u5728\u5b9e\u9645\u91cf\u5b50\u786c\u4ef6\u4e0a\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u4f20\u7edf\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5\u5b58\u5728\u521a\u6027\u3001\u6269\u5c55\u6027\u4e0d\u8db3\u53ca\u4f9d\u8d56\u4e13\u5bb6\u7684\u95ee\u9898\uff0c\u9700\u5f00\u53d1\u81ea\u4e3b\u8bbe\u8ba1\u786c\u4ef6\u9ad8\u6548ans\u00e4tze\u7684\u6846\u67b6\u3002", "method": "\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u8fdb\u5316\u4f18\u5316\uff0c\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\uff0c\u81ea\u4e3b\u751f\u6210\u53ef\u6269\u5c55\u4e14\u53c2\u6570\u6570\u91cf\u4e0e\u7cfb\u7edf\u5c3a\u5bf8\u65e0\u5173\u7684\u91cf\u5b50\u7535\u8def\u3002", "result": "\u57289\u91cf\u5b50\u6bd4\u7279Ising\u548cXY\u81ea\u65cb\u94fe\u4e0a\u5b9e\u73b0\u4ec5\u542b4\u4e2a\u53c2\u6570\u7684\u7535\u8def\uff0c\u80fd\u91cf\u5916\u63a8\u63a5\u8fd1\u7cbe\u786e\uff1b\u5728\u7956\u51b2\u4e4b\u82af\u7247\u4e0a\u9a8c\u8bc1\uff0c\u901a\u8fc7\u96f6\u566a\u58f0\u5916\u63a8\u6709\u6548\u6291\u5236\u4e24\u91cf\u5b50\u6bd4\u7279\u95e8\u566a\u58f0\u3002", "conclusion": "FunSearch\u586b\u8865\u7b97\u6cd5\u8bbe\u8ba1\u4e0e\u5b9e\u9a8c\u7ea6\u675f\u95f4\u7684\u9e3f\u6c9f\uff0c\u63a8\u52a8\u53ef\u6269\u5c55\u91cf\u5b50\u6a21\u62df\u7684\u53d1\u5c55\uff0c\u662f\u5f53\u524d\u91cf\u5b50\u67b6\u6784\u641c\u7d22\u6846\u67b6\u7684\u6709\u529b\u8865\u5145\u3002"}}
{"id": "2505.06978", "pdf": "https://arxiv.org/pdf/2505.06978", "abs": "https://arxiv.org/abs/2505.06978", "authors": ["Lei Lei", "Kan Zheng", "Xuemin", "Shen"], "title": "Learning Value of Information towards Joint Communication and Control in 6G V2X", "categories": ["cs.LG"], "comment": null, "summary": "As Cellular Vehicle-to-Everything (C-V2X) evolves towards future\nsixth-generation (6G) networks, Connected Autonomous Vehicles (CAVs) are\nemerging to become a key application. Leveraging data-driven Machine Learning\n(ML), especially Deep Reinforcement Learning (DRL), is expected to\nsignificantly enhance CAV decision-making in both vehicle control and V2X\ncommunication under uncertainty. These two decision-making processes are\nclosely intertwined, with the value of information (VoI) acting as a crucial\nbridge between them. In this paper, we introduce Sequential Stochastic Decision\nProcess (SSDP) models to define and assess VoI, demonstrating their application\nin optimizing communication systems for CAVs. Specifically, we formally define\nthe SSDP model and demonstrate that the MDP model is a special case of it. The\nSSDP model offers a key advantage by explicitly representing the set of\ninformation that can enhance decision-making when available. Furthermore, as\ncurrent research on VoI remains fragmented, we propose a systematic VoI\nmodeling framework grounded in the MDP, Reinforcement Learning (RL) and Optimal\nControl theories. We define different categories of VoI and discuss their\ncorresponding estimation methods. Finally, we present a structured approach to\nleverage the various VoI metrics for optimizing the ``When\", ``What\", and\n``How\" to communicate problems. For this purpose, SSDP models are formulated\nwith VoI-associated reward functions derived from VoI-based optimization\nobjectives. While we use a simple vehicle-following control problem to\nillustrate the proposed methodology, it holds significant potential to\nfacilitate the joint optimization of stochastic, sequential control and\ncommunication decisions in a wide range of networked control systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSSDP\u6a21\u578b\u548cVoI\u7684\u7cfb\u7edf\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316CAV\u7684\u51b3\u7b56\u548c\u901a\u4fe1\u7cfb\u7edf\uff0c\u5e76\u7ed3\u5408DRL\u548c\u6700\u4f18\u63a7\u5236\u7406\u8bba\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8f66\u8054\u7f51\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u968f\u7740C-V2X\u54116G\u7f51\u7edc\u6f14\u8fdb\uff0cCAV\u6210\u4e3a\u5173\u952e\u5e94\u7528\u3002\u4f46\u73b0\u6709\u7814\u7a76\u5728\u4fe1\u606f\u4ef7\u503c\uff08VoI\uff09\u5efa\u6a21\u4e0a\u8f83\u4e3a\u96f6\u6563\uff0c\u4e14\u8f66\u63a7\u4e0e\u901a\u4fe1\u7684\u534f\u540c\u4f18\u5316\u9700\u6c42\u8feb\u5207\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u5316\u6846\u67b6\u3002", "method": "\u63d0\u51faSSDP\u6a21\u578b\uff08MDP\u7684\u6269\u5c55\uff09\u5b9a\u4e49VoI\uff0c\u5efa\u7acbMDP/RL/\u6700\u4f18\u63a7\u5236\u7406\u8bba\u9a71\u52a8\u7684VoI\u5efa\u6a21\u6846\u67b6\uff0c\u5206\u7c7bVoI\u5e76\u8bbe\u8ba1\u4f30\u8ba1\u65b9\u6cd5\uff0c\u6700\u540e\u901a\u8fc7VoI\u5956\u52b1\u51fd\u6570\u4f18\u5316\u901a\u4fe1\u7b56\u7565\u3002", "result": "SSDP\u6a21\u578b\u80fd\u663e\u5f0f\u8868\u793a\u63d0\u5347\u51b3\u7b56\u7684\u4fe1\u606f\u96c6\u5408\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u53ef\u7cfb\u7edf\u5316\u89e3\u51b3\u901a\u4fe1\u7684\u65f6\u673a\uff08When\uff09\u3001\u5185\u5bb9\uff08What\uff09\u548c\u65b9\u5f0f\uff08How\uff09\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u8ddf\u8f66\u63a7\u5236\u6848\u4f8b\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7f51\u7edc\u5316\u63a7\u5236\u7cfb\u7edf\u4e2d\u968f\u673a\u5e8f\u8d2f\u51b3\u7b56\u4e0e\u901a\u4fe1\u7684\u8054\u5408\u4f18\u5316\u63d0\u4f9b\u4e86\u901a\u7528\u65b9\u6cd5\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u66f4\u590d\u6742\u573a\u666f\u3002"}}
{"id": "2505.06363", "pdf": "https://arxiv.org/pdf/2505.06363", "abs": "https://arxiv.org/abs/2505.06363", "authors": ["Anmol Gupta", "Weiwei Gu", "Omkar Patil", "Jun Ki Lee", "Nakul Gopalan"], "title": "Learning Sequential Kinematic Models from Demonstrations for Multi-Jointed Articulated Objects", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "As robots become more generalized and deployed in diverse environments, they\nmust interact with complex objects, many with multiple independent joints or\ndegrees of freedom (DoF) requiring precise control. A common strategy is object\nmodeling, where compact state-space models are learned from real-world\nobservations and paired with classical planning. However, existing methods\noften rely on prior knowledge or focus on single-DoF objects, limiting their\napplicability. They also fail to handle occluded joints and ignore the\nmanipulation sequences needed to access them. We address this by learning\nobject models from human demonstrations. We introduce Object Kinematic Sequence\nMachines (OKSMs), a novel representation capturing both kinematic constraints\nand manipulation order for multi-DoF objects. To estimate these models from\npoint cloud data, we present Pokenet, a deep neural network trained on human\ndemonstrations. We validate our approach on 8,000 simulated and 1,600\nreal-world annotated samples. Pokenet improves joint axis and state estimation\nby over 20 percent on real-world data compared to prior methods. Finally, we\ndemonstrate OKSMs on a Sawyer robot using inverse kinematics-based planning to\nmanipulate multi-DoF objects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOKSM\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4eba\u7c7b\u6f14\u793a\u5b66\u4e60\u591a\u81ea\u7531\u5ea6\u7269\u4f53\u7684\u8fd0\u52a8\u5b66\u7ea6\u675f\u548c\u64cd\u4f5c\u987a\u5e8f\uff0c\u5e76\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bPokenet\u8fdb\u884c\u70b9\u4e91\u6570\u636e\u5904\u7406\uff0c\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u6027\u80fd\u63d0\u5347\u8d85\u8fc720%\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5148\u9a8c\u77e5\u8bc6\u6216\u4ec5\u9002\u7528\u4e8e\u5355\u81ea\u7531\u5ea6\u7269\u4f53\uff0c\u65e0\u6cd5\u5904\u7406\u906e\u6321\u5173\u8282\u6216\u64cd\u4f5c\u987a\u5e8f\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7OKSM\u6355\u6349\u7269\u4f53\u7684\u8fd0\u52a8\u5b66\u7ea6\u675f\u548c\u64cd\u4f5c\u987a\u5e8f\uff0c\u5e76\u5229\u7528Pokenet\u7f51\u7edc\u4ece\u70b9\u4e91\u6570\u636e\u4e2d\u4f30\u8ba1\u8fd9\u4e9b\u6a21\u578b\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0cPokenet\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5173\u8282\u8f74\u548c\u72b6\u6001\u4f30\u8ba1\u6027\u80fd\u63d0\u5347\u8d85\u8fc720%\u3002", "conclusion": "OKSM\u7ed3\u5408Pokenet\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u81ea\u7531\u5ea6\u7269\u4f53\u7684\u5efa\u6a21\u4e0e\u64cd\u63a7\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u673a\u5668\u4eba\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002"}}
{"id": "2505.07004", "pdf": "https://arxiv.org/pdf/2505.07004", "abs": "https://arxiv.org/abs/2505.07004", "authors": ["Jinuk Kim", "Marwa El Halabi", "Wonpyo Park", "Clemens JS Schaefer", "Deokjae Lee", "Yeonhong Park", "Jae W. Lee", "Hyun Oh Song"], "title": "GuidedQuant: Large Language Model Quantization via Exploiting End Loss Guidance", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Post-training quantization is a key technique for reducing the memory and\ninference latency of large language models by quantizing weights and\nactivations without requiring retraining. However, existing methods either (1)\nfail to account for the varying importance of hidden features to the end loss\nor, when incorporating end loss, (2) neglect the critical interactions between\nmodel weights. To address these limitations, we propose GuidedQuant, a novel\nquantization approach that integrates gradient information from the end loss\ninto the quantization objective while preserving cross-weight dependencies\nwithin output channels. GuidedQuant consistently boosts the performance of\nstate-of-the-art quantization methods across weight-only scalar, weight-only\nvector, and weight-and-activation quantization. Additionally, we introduce a\nnovel non-uniform scalar quantization algorithm, which is guaranteed to\nmonotonically decrease the quantization objective value, and outperforms\nexisting methods in this category. We release the code at\nhttps://github.com/snu-mllab/GuidedQuant.", "AI": {"tldr": "GuidedQuant\u662f\u4e00\u79cd\u65b0\u578b\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u635f\u5931\u51fd\u6570\u7684\u68af\u5ea6\u4fe1\u606f\u878d\u5165\u91cf\u5316\u76ee\u6807\uff0c\u540c\u65f6\u4fdd\u6301\u8f93\u51fa\u901a\u9053\u95f4\u7684\u6743\u91cd\u4f9d\u8d56\u5173\u7cfb\uff0c\u63d0\u5347\u4e86\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u975e\u5747\u5300\u6807\u91cf\u91cf\u5316\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u8981\u4e48\u672a\u8003\u8651\u9690\u85cf\u7279\u5f81\u5bf9\u6700\u7ec8\u635f\u5931\u7684\u91cd\u8981\u6027\uff0c\u8981\u4e48\u5ffd\u7565\u4e86\u6a21\u578b\u6743\u91cd\u95f4\u7684\u5173\u952e\u4ea4\u4e92\uff0c\u9650\u5236\u4e86\u91cf\u5316\u6548\u679c\u3002", "method": "\u63d0\u51faGuidedQuant\u65b9\u6cd5\uff0c\u7ed3\u5408\u635f\u5931\u51fd\u6570\u7684\u68af\u5ea6\u4fe1\u606f\uff0c\u52a0\u5165\u91cf\u5316\u76ee\u6807\uff0c\u540c\u65f6\u4fdd\u6301\u8f93\u51fa\u901a\u9053\u7684\u6743\u91cd\u4e92\u4f9d\u8d56\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u975e\u5747\u5300\u6807\u91cf\u91cf\u5316\u7b97\u6cd5\u3002", "result": "\u5728\u4ec5\u6743\u91cd\u6807\u91cf\u3001\u5411\u91cf\u91cf\u5316\u53ca\u6743\u91cd\u4e0e\u6fc0\u6d3b\u91cf\u5316\u4e2d\uff0cGuidedQuant\u5747\u63d0\u5347\u4e86\u6027\u80fd\uff1b\u975e\u5747\u5300\u6807\u91cf\u91cf\u5316\u7b97\u6cd5\u4f18\u4e8e\u73b0\u6709\u540c\u7c7b\u65b9\u6cd5\u3002", "conclusion": "GuidedQuant\u901a\u8fc7\u7efc\u5408\u8003\u8651\u68af\u5ea6\u4fe1\u606f\u548c\u6743\u91cd\u4f9d\u8d56\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u91cf\u5316\u6027\u80fd\uff0c\u4e3a\u975e\u5747\u5300\u6807\u91cf\u91cf\u5316\u63d0\u4f9b\u4e86\u65b0\u89e3\u6cd5\u3002"}}
{"id": "2505.06378", "pdf": "https://arxiv.org/pdf/2505.06378", "abs": "https://arxiv.org/abs/2505.06378", "authors": ["Yuxiang Wei", "Zhuoqi Zeng", "Yue Zhong", "Jiawen Kang", "Ryan Wen Liu", "M. Shamim Hossain"], "title": "Bi-LSTM based Multi-Agent DRL with Computation-aware Pruning for Agent Twins Migration in Vehicular Embodied AI Networks", "categories": ["cs.GT", "cs.AI"], "comment": null, "summary": "With the advancement of large language models and embodied Artificial\nIntelligence (AI) in the intelligent transportation scenarios, the combination\nof them in intelligent transportation spawns the Vehicular Embodied AI Network\n(VEANs). In VEANs, Autonomous Vehicles (AVs) are typical agents whose local\nadvanced AI applications are defined as vehicular embodied AI agents, enabling\ncapabilities such as environment perception and multi-agent collaboration. Due\nto computation latency and resource constraints, the local AI applications and\nservices running on vehicular embodied AI agents need to be migrated, and\nsubsequently referred to as vehicular embodied AI agent twins, which drive the\nadvancement of vehicular embodied AI networks to offload intensive tasks to\nRoadside Units (RSUs), mitigating latency problems while maintaining service\nquality. Recognizing workload imbalance among RSUs in traditional approaches,\nwe model AV-RSU interactions as a Stackelberg game to optimize bandwidth\nresource allocation for efficient migration. A Tiny Multi-Agent Bidirectional\nLSTM Proximal Policy Optimization (TMABLPPO) algorithm is designed to\napproximate the Stackelberg equilibrium through decentralized coordination.\nFurthermore, a personalized neural network pruning algorithm based on Path\neXclusion (PX) dynamically adapts to heterogeneous AV computation capabilities\nby identifying task-critical parameters in trained models, reducing model\ncomplexity with less performance degradation. Experimental validation confirms\nthe algorithm's effectiveness in balancing system load and minimizing delays,\ndemonstrating significant improvements in vehicular embodied AI agent\ndeployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVEANs\u7684\u8f66\u8f7d\u5d4c\u5165\u5f0fAI\u7f51\u7edc\uff0c\u901a\u8fc7Stackelberg\u535a\u5f08\u4f18\u5316\u5e26\u5bbd\u8d44\u6e90\u5206\u914d\uff0c\u8bbe\u8ba1\u4e86TMABLPPO\u7b97\u6cd5\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u534f\u8c03\uff0c\u5e76\u91c7\u7528\u57fa\u4e8ePX\u7684\u4e2a\u6027\u5316\u795e\u7ecf\u7f51\u7edc\u526a\u679d\u7b97\u6cd5\u52a8\u6001\u9002\u5e94\u5f02\u6784AV\u8ba1\u7b97\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u8d1f\u8f7d\u5e73\u8861\u548c\u5ef6\u8fdf\u6700\u5c0f\u5316\u3002", "motivation": "\u667a\u80fd\u4ea4\u901a\u573a\u666f\u4e2d\uff0c\u8f66\u8f7d\u5d4c\u5165\u5f0fAI\u4ee3\u7406\u7684\u8ba1\u7b97\u5ef6\u8fdf\u548c\u8d44\u6e90\u9650\u5236\u9700\u8981\u4efb\u52a1\u8fc1\u79fb\u5230RSU\uff0c\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728RSU\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\uff0c\u9700\u4f18\u5316\u8d44\u6e90\u5206\u914d\u4ee5\u63d0\u5347\u6548\u7387\u3002", "method": "1. \u5c06AV-RSU\u4ea4\u4e92\u5efa\u6a21\u4e3aStackelberg\u535a\u5f08\uff1b2. \u8bbe\u8ba1TMABLPPO\u7b97\u6cd5\u8fd1\u4f3c\u5747\u8861\u89e3\uff1b3. \u63d0\u51fa\u57fa\u4e8ePX\u7684\u4e2a\u6027\u5316\u795e\u7ecf\u7f51\u7edc\u526a\u679d\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u7b97\u6cd5\u5728\u7cfb\u7edf\u8d1f\u8f7d\u5e73\u8861\u548c\u5ef6\u8fdf\u6700\u5c0f\u5316\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f66\u8f7d\u5d4c\u5165\u5f0fAI\u4ee3\u7406\u7684\u90e8\u7f72\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8f66\u8f7d\u5d4c\u5165\u5f0fAI\u7f51\u7edc\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2505.07023", "pdf": "https://arxiv.org/pdf/2505.07023", "abs": "https://arxiv.org/abs/2505.07023", "authors": ["Alexander Koebler", "Thomas Decker", "Ingo Thon", "Volker Tresp", "Florian Buettner"], "title": "Incremental Uncertainty-aware Performance Monitoring with Active Labeling Intervention", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "We study the problem of monitoring machine learning models under gradual\ndistribution shifts, where circumstances change slowly over time, often leading\nto unnoticed yet significant declines in accuracy. To address this, we propose\nIncremental Uncertainty-aware Performance Monitoring (IUPM), a novel label-free\nmethod that estimates performance changes by modeling gradual shifts using\noptimal transport. In addition, IUPM quantifies the uncertainty in the\nperformance prediction and introduces an active labeling procedure to restore a\nreliable estimate under a limited labeling budget. Our experiments show that\nIUPM outperforms existing performance estimation baselines in various gradual\nshift scenarios and that its uncertainty awareness guides label acquisition\nmore effectively compared to other strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIUPM\u7684\u65e0\u6807\u7b7e\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u5efa\u6a21\u6e10\u53d8\u5206\u5e03\u504f\u79fb\u6765\u4f30\u8ba1\u6027\u80fd\u53d8\u5316\uff0c\u5e76\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u6709\u9650\u6807\u6ce8\u9884\u7b97\u4e0b\u6709\u6548\u6307\u5bfc\u6807\u6ce8\u83b7\u53d6\u3002", "motivation": "\u7814\u7a76\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6e10\u53d8\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6027\u80fd\u76d1\u6d4b\u95ee\u9898\uff0c\u89e3\u51b3\u56e0\u7f13\u6162\u53d8\u5316\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u672a\u88ab\u5bdf\u89c9\u7684\u60c5\u51b5\u3002", "method": "\u63d0\u51faIUPM\u65b9\u6cd5\uff0c\u7ed3\u5408\u6700\u4f18\u4f20\u8f93\u5efa\u6a21\u6e10\u53d8\u504f\u79fb\uff0c\u91cf\u5316\u6027\u80fd\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5f15\u5165\u4e3b\u52a8\u6807\u6ce8\u7b56\u7565\u4ee5\u6062\u590d\u53ef\u9760\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660eIUPM\u5728\u591a\u79cd\u6e10\u53d8\u504f\u79fb\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5176\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u80fd\u66f4\u6709\u6548\u5730\u6307\u5bfc\u6807\u6ce8\u83b7\u53d6\u3002", "conclusion": "IUPM\u901a\u8fc7\u5efa\u6a21\u6e10\u53d8\u504f\u79fb\u548c\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u6a21\u578b\u6027\u80fd\u76d1\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.06380", "pdf": "https://arxiv.org/pdf/2505.06380", "abs": "https://arxiv.org/abs/2505.06380", "authors": ["Josh Harguess", "Chris M. Ward"], "title": "Offensive Security for AI Systems: Concepts, Practices, and Applications", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "As artificial intelligence (AI) systems become increasingly adopted across\nsectors, the need for robust, proactive security strategies is paramount.\nTraditional defensive measures often fall short against the unique and evolving\nthreats facing AI-driven technologies, making offensive security an essential\napproach for identifying and mitigating risks. This paper presents a\ncomprehensive framework for offensive security in AI systems, emphasizing\nproactive threat simulation and adversarial testing to uncover vulnerabilities\nthroughout the AI lifecycle. We examine key offensive security techniques,\nincluding weakness and vulnerability assessment, penetration testing, and red\nteaming, tailored specifically to address AI's unique susceptibilities. By\nsimulating real-world attack scenarios, these methodologies reveal critical\ninsights, informing stronger defensive strategies and advancing resilience\nagainst emerging threats. This framework advances offensive AI security from\ntheoretical concepts to practical, actionable methodologies that organizations\ncan implement to strengthen their AI systems against emerging threats.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9AI\u7cfb\u7edf\u7684\u4e3b\u52a8\u5b89\u5168\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u771f\u5b9e\u653b\u51fb\u573a\u666f\u548c\u5bf9\u6297\u6d4b\u8bd5\u6765\u8bc6\u522b\u6f0f\u6d1e\uff0c\u63d0\u5347AI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u968f\u7740AI\u6280\u672f\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f20\u7edf\u9632\u5fa1\u63aa\u65bd\u96be\u4ee5\u5e94\u5bf9\u5176\u72ec\u7279\u4e14\u4e0d\u65ad\u6f14\u53d8\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e3b\u52a8\u7684\u5b89\u5168\u7b56\u7565\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u9488\u5bf9AI\u7cfb\u7edf\u7684\u8fdb\u653b\u6027\u5b89\u5168\u6846\u67b6\uff0c\u5305\u62ec\u5f31\u70b9\u8bc4\u4f30\u3001\u6e17\u900f\u6d4b\u8bd5\u548c\u7ea2\u961f\u6f14\u7ec3\u7b49\u65b9\u6cd5\uff0c\u4ee5\u6a21\u62df\u771f\u5b9e\u653b\u51fb\u573a\u666f\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u8bc6\u522bAI\u7cfb\u7edf\u7684\u6f0f\u6d1e\uff0c\u5e76\u4e3a\u5236\u5b9a\u66f4\u5f3a\u5927\u7684\u9632\u5fa1\u7b56\u7565\u63d0\u4f9b\u5173\u952e\u89c1\u89e3\uff0c\u63d0\u5347\u7cfb\u7edf\u7684\u6297\u5a01\u80c1\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c06\u8fdb\u653b\u6027AI\u5b89\u5168\u4ece\u7406\u8bba\u8f6c\u5316\u4e3a\u5b9e\u8df5\u4e2d\u53ef\u64cd\u4f5c\u7684\u65b9\u6cd5\u8bba\uff0c\u5e2e\u52a9\u7ec4\u7ec7\u589e\u5f3a\u5176AI\u7cfb\u7edf\u5bf9\u65b0\u5174\u5a01\u80c1\u7684\u62b5\u5fa1\u80fd\u529b\u3002"}}
{"id": "2505.07026", "pdf": "https://arxiv.org/pdf/2505.07026", "abs": "https://arxiv.org/abs/2505.07026", "authors": ["Maximilian Egger", "Rawad Bitar", "R\u00fcdiger Urbanke"], "title": "Efficient Machine Unlearning by Model Splitting and Core Sample Selection", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Machine unlearning is essential for meeting legal obligations such as the\nright to be forgotten, which requires the removal of specific data from machine\nlearning models upon request. While several approaches to unlearning have been\nproposed, existing solutions often struggle with efficiency and, more\ncritically, with the verification of unlearning - particularly in the case of\nweak unlearning guarantees, where verification remains an open challenge. We\nintroduce a generalized variant of the standard unlearning metric that enables\nmore efficient and precise unlearning strategies. We also present an\nunlearning-aware training procedure that, in many cases, allows for exact\nunlearning. We term our approach MaxRR. When exact unlearning is not feasible,\nMaxRR still supports efficient unlearning with properties closely matching\nthose achieved through full retraining.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMaxRR\u7684\u673a\u5668\u5b66\u4e60\u53bb\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u6807\u51c6\u53bb\u5b66\u4e60\u6307\u6807\u548c\u5f15\u5165\u53bb\u5b66\u4e60\u611f\u77e5\u8bad\u7ec3\u7a0b\u5e8f\uff0c\u63d0\u9ad8\u4e86\u53bb\u5b66\u4e60\u7684\u6548\u7387\u548c\u7cbe\u786e\u6027\u3002", "motivation": "\u4e3a\u6ee1\u8db3\u2018\u88ab\u9057\u5fd8\u6743\u2019\u7b49\u6cd5\u5f8b\u8981\u6c42\uff0c\u73b0\u6709\u53bb\u5b66\u4e60\u65b9\u6cd5\u5728\u6548\u7387\u548c\u9a8c\u8bc1\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u5f31\u53bb\u5b66\u4e60\u4fdd\u8bc1\u7684\u60c5\u51b5\u4e0b\uff0c\u9a8c\u8bc1\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u5f15\u5165\u4e86\u6807\u51c6\u53bb\u5b66\u4e60\u6307\u6807\u7684\u5e7f\u4e49\u53d8\u4f53\uff0c\u5e76\u63d0\u51fa\u4e86\u53bb\u5b66\u4e60\u611f\u77e5\u8bad\u7ec3\u7a0b\u5e8f\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u652f\u6301\u7cbe\u786e\u53bb\u5b66\u4e60\uff0c\u5426\u5219\u4e5f\u80fd\u9ad8\u6548\u6a21\u62df\u5b8c\u5168\u91cd\u8bad\u7ec3\u7684\u6548\u679c\u3002", "result": "MaxRR\u5728\u65e0\u6cd5\u7cbe\u786e\u53bb\u5b66\u4e60\u65f6\uff0c\u4ecd\u80fd\u5b9e\u73b0\u4e0e\u5b8c\u5168\u91cd\u8bad\u7ec3\u76f8\u8fd1\u7684\u9ad8\u6548\u53bb\u5b66\u4e60\u6548\u679c\u3002", "conclusion": "MaxRR\u4e3a\u673a\u5668\u5b66\u4e60\u53bb\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u662f\u5728\u6cd5\u5f8b\u8981\u6c42\u4e25\u683c\u7684\u573a\u666f\u4e0b\u3002"}}
{"id": "2505.06394", "pdf": "https://arxiv.org/pdf/2505.06394", "abs": "https://arxiv.org/abs/2505.06394", "authors": ["Massimiliano Albanese", "Xinming Ou", "Kevin Lybarger", "Daniel Lende", "Dmitry Goldgof"], "title": "Towards AI-Driven Human-Machine Co-Teaming for Adaptive and Agile Cyber Security Operation Centers", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Security Operations Centers (SOCs) face growing challenges in managing\ncybersecurity threats due to an overwhelming volume of alerts, a shortage of\nskilled analysts, and poorly integrated tools. Human-AI collaboration offers a\npromising path to augment the capabilities of SOC analysts while reducing their\ncognitive overload. To this end, we introduce an AI-driven human-machine\nco-teaming paradigm that leverages large language models (LLMs) to enhance\nthreat intelligence, alert triage, and incident response workflows. We present\na vision in which LLM-based AI agents learn from human analysts the tacit\nknowledge embedded in SOC operations, enabling the AI agents to improve their\nperformance on SOC tasks through this co-teaming. We invite SOCs to collaborate\nwith us to further develop this process and uncover replicable patterns where\nhuman-AI co-teaming yields measurable improvements in SOC productivity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2aAI\u9a71\u52a8\u7684\u4eba\u673a\u534f\u4f5c\u6a21\u5f0f\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u589e\u5f3aSOC\u5206\u6790\u5e08\u7684\u80fd\u529b\uff0c\u4ee5\u5e94\u5bf9\u65e5\u76ca\u589e\u957f\u7684\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\u3002", "motivation": "\u7531\u4e8e\u8b66\u62a5\u6570\u91cf\u5e9e\u5927\u3001\u5206\u6790\u5e08\u77ed\u7f3a\u53ca\u5de5\u5177\u96c6\u6210\u4e0d\u4f73\uff0cSOC\u5728\u7ba1\u7406\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\u65b9\u9762\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002\u4eba\u673a\u534f\u4f5c\u88ab\u89c6\u4e3a\u63d0\u5347\u5206\u6790\u5e08\u80fd\u529b\u5e76\u51cf\u8f7b\u5176\u8ba4\u77e5\u8d1f\u8377\u7684\u6709\u6548\u9014\u5f84\u3002", "method": "\u5f15\u5165\u57fa\u4e8eLLM\u7684\u4eba\u673a\u534f\u4f5c\u8303\u5f0f\uff0c\u901a\u8fc7\u5b66\u4e60\u4eba\u7c7b\u5206\u6790\u5e08\u7684\u9690\u6027\u77e5\u8bc6\uff0c\u6539\u8fdbAI\u5728\u5a01\u80c1\u60c5\u62a5\u3001\u8b66\u62a5\u5206\u7c7b\u548c\u4e8b\u4ef6\u54cd\u5e94\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u613f\u666f\uff0c\u5373\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u63d0\u5347SOC\u4efb\u52a1\u6548\u7387\uff0c\u5e76\u9080\u8bf7SOC\u5408\u4f5c\u4ee5\u53d1\u73b0\u53ef\u91cd\u590d\u7684\u6548\u7387\u63d0\u5347\u6a21\u5f0f\u3002", "conclusion": "\u4eba\u673a\u534f\u4f5c\u662f\u63d0\u5347SOC\u751f\u4ea7\u529b\u7684\u53ef\u884c\u65b9\u5411\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u9a8c\u8bc1\u5176\u5b9e\u9645\u6548\u679c\u3002"}}
{"id": "2505.07036", "pdf": "https://arxiv.org/pdf/2505.07036", "abs": "https://arxiv.org/abs/2505.07036", "authors": ["Mahade Hasan", "Farhana Yasmin"], "title": "Predicting Diabetes Using Machine Learning: A Comparative Study of Classifiers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Diabetes remains a significant health challenge globally, contributing to\nsevere complications like kidney disease, vision loss, and heart issues. The\napplication of machine learning (ML) in healthcare enables efficient and\naccurate disease prediction, offering avenues for early intervention and\npatient support. Our study introduces an innovative diabetes prediction\nframework, leveraging both traditional ML techniques such as Logistic\nRegression, SVM, Na\\\"ive Bayes, and Random Forest and advanced ensemble methods\nlike AdaBoost, Gradient Boosting, Extra Trees, and XGBoost. Central to our\napproach is the development of a novel model, DNet, a hybrid architecture\ncombining Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM)\nlayers for effective feature extraction and sequential learning. The DNet model\ncomprises an initial convolutional block for capturing essential features,\nfollowed by a residual block with skip connections to facilitate efficient\ninformation flow. Batch Normalization and Dropout are employed for robust\nregularization, and an LSTM layer captures temporal dependencies within the\ndata. Using a Kaggle-sourced real-world diabetes dataset, our model evaluation\nspans cross-validation accuracy, precision, recall, F1 score, and ROC-AUC.\nAmong the models, DNet demonstrates the highest efficacy with an accuracy of\n99.79% and an AUC-ROC of 99.98%, establishing its potential for superior\ndiabetes prediction. This robust hybrid architecture showcases the value of\ncombining CNN and LSTM layers, emphasizing its applicability in medical\ndiagnostics and disease prediction tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDNet\u7684\u65b0\u578b\u7cd6\u5c3f\u75c5\u9884\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u548c\u5148\u8fdb\u7684\u96c6\u6210\u6280\u672f\uff0c\u5e76\u901a\u8fc7CNN\u548cLSTM\u6df7\u5408\u67b6\u6784\u5b9e\u73b0\u4e86\u9ad8\u6548\u7279\u5f81\u63d0\u53d6\u548c\u65f6\u5e8f\u5b66\u4e60\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aDNet\u5728\u51c6\u786e\u7387\u548cROC-AUC\u4e0a\u8868\u73b0\u6700\u4f18\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u662f\u5168\u7403\u91cd\u5927\u5065\u5eb7\u95ee\u9898\uff0c\u673a\u5668\u5b66\u4e60\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u4e3a\u65e9\u671f\u5e72\u9884\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u521b\u65b0\u7684\u6df7\u5408\u6a21\u578b\u63d0\u5347\u7cd6\u5c3f\u75c5\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u7814\u7a76\u4e86\u591a\u79cd\u4f20\u7edf\u548c\u96c6\u6210\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u63d0\u51faDNet\u6a21\u578b\u2014\u2014\u4e00\u4e2a\u7ed3\u5408CNN\u548cLSTM\u7684\u6df7\u5408\u67b6\u6784\uff0c\u5305\u542b\u5377\u79ef\u5757\u3001\u6b8b\u5dee\u5757\u3001LSTM\u5c42\u53ca\u6b63\u5219\u5316\u6280\u672f\u3002", "result": "DNet\u5728Kaggle\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8699.79%\u7684\u51c6\u786e\u7387\u548c99.98%\u7684ROC-AUC\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "conclusion": "DNet\u5c55\u793a\u4e86CNN\u4e0eLSTM\u7ed3\u5408\u5728\u533b\u7597\u8bca\u65ad\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u7cd6\u5c3f\u75c5\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.06402", "pdf": "https://arxiv.org/pdf/2505.06402", "abs": "https://arxiv.org/abs/2505.06402", "authors": ["Alexiy Buynitsky", "Sina Ehsani", "Bhanu Pallakonda", "Pragyana Mishra"], "title": "Camera Control at the Edge with Language Models for Scene Understanding", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": "7 pages, 6 figures. This work was presented and published at the 11th\n  IEEE International Conference on Control, Automation and Robotics (ICCAR) in\n  2025", "summary": "In this paper, we present Optimized Prompt-based Unified System (OPUS), a\nframework that utilizes a Large Language Model (LLM) to control Pan-Tilt-Zoom\n(PTZ) cameras, providing contextual understanding of natural environments. To\nachieve this goal, the OPUS system improves cost-effectiveness by generating\nkeywords from a high-level camera control API and transferring knowledge from\nlarger closed-source language models to smaller ones through Supervised\nFine-Tuning (SFT) on synthetic data. This enables efficient edge deployment\nwhile maintaining performance comparable to larger models like GPT-4. OPUS\nenhances environmental awareness by converting data from multiple cameras into\ntextual descriptions for language models, eliminating the need for specialized\nsensory tokens. In benchmark testing, our approach significantly outperformed\nboth traditional language model techniques and more complex prompting methods,\nachieving a 35% improvement over advanced techniques and a 20% higher task\naccuracy compared to closed-source models like Gemini Pro. The system\ndemonstrates OPUS's capability to simplify PTZ camera operations through an\nintuitive natural language interface. This approach eliminates the need for\nexplicit programming and provides a conversational method for interacting with\ncamera systems, representing a significant advancement in how users can control\nand utilize PTZ camera technology.", "AI": {"tldr": "OPUS\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63a7\u5236PTZ\u6444\u50cf\u5934\uff0c\u901a\u8fc7\u751f\u6210\u5173\u952e\u8bcd\u548c\u76d1\u7763\u5fae\u8c03\u63d0\u5347\u6210\u672c\u6548\u76ca\uff0c\u6027\u80fd\u63a5\u8fd1GPT-4\uff0c\u4e14\u5728\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u63d0\u5347PTZ\u6444\u50cf\u5934\u63a7\u5236\u7684\u6210\u672c\u6548\u76ca\u548c\u7528\u6237\u53cb\u597d\u6027\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\u7b80\u5316\u64cd\u4f5c\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5173\u952e\u8bcd\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u5c06\u77e5\u8bc6\u4ece\u5c0f\u6a21\u578b\u8f6c\u79fb\u5230\u66f4\u5927\u6a21\u578b\uff0c\u5e76\u8f6c\u6362\u591a\u6444\u50cf\u5934\u6570\u636e\u4e3a\u6587\u672c\u63cf\u8ff0\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOPUS\u6bd4\u4f20\u7edf\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\uff0c\u4efb\u52a1\u51c6\u786e\u7387\u63d0\u9ad8\u4e8620%\uff0c\u4e14\u6027\u80fd\u63a5\u8fd1GPT-4\u3002", "conclusion": "OPUS\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\u7b80\u5316\u4e86PTZ\u6444\u50cf\u5934\u7684\u63a7\u5236\uff0c\u4ee3\u8868\u4e86\u8be5\u6280\u672f\u7684\u91cd\u5927\u8fdb\u6b65\u3002"}}
{"id": "2505.07045", "pdf": "https://arxiv.org/pdf/2505.07045", "abs": "https://arxiv.org/abs/2505.07045", "authors": ["Junjie Yu", "John S. Schreck", "David John Gagne", "Keith W. Oleson", "Jie Li", "Yongtu Liang", "Qi Liao", "Mingfei Sun", "David O. Topping", "Zhonghua Zheng"], "title": "Reinforcement Learning (RL) Meets Urban Climate Modeling: Investigating the Efficacy and Impacts of RL-Based HVAC Control", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning (RL)-based heating, ventilation, and air conditioning\n(HVAC) control has emerged as a promising technology for reducing building\nenergy consumption while maintaining indoor thermal comfort. However, the\nefficacy of such strategies is influenced by the background climate and their\nimplementation may potentially alter both the indoor climate and local urban\nclimate. This study proposes an integrated framework combining RL with an urban\nclimate model that incorporates a building energy model, aiming to evaluate the\nefficacy of RL-based HVAC control across different background climates, impacts\nof RL strategies on indoor climate and local urban climate, and the\ntransferability of RL strategies across cities. Our findings reveal that the\nreward (defined as a weighted combination of energy consumption and thermal\ncomfort) and the impacts of RL strategies on indoor climate and local urban\nclimate exhibit marked variability across cities with different background\nclimates. The sensitivity of reward weights and the transferability of RL\nstrategies are also strongly influenced by the background climate. Cities in\nhot climates tend to achieve higher rewards across most reward weight\nconfigurations that balance energy consumption and thermal comfort, and those\ncities with more varying atmospheric temperatures demonstrate greater RL\nstrategy transferability. These findings underscore the importance of\nthoroughly evaluating RL-based HVAC control strategies in diverse climatic\ncontexts. This study also provides a new insight that city-to-city learning\nwill potentially aid the deployment of RL-based HVAC control.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e0e\u57ce\u5e02\u6c14\u5019\u6a21\u578b\u7684\u96c6\u6210\u6846\u67b6\uff0c\u8bc4\u4f30\u4e0d\u540c\u6c14\u5019\u80cc\u666f\u4e0bRL HVAC\u63a7\u5236\u7684\u6548\u679c\u53ca\u5176\u5bf9\u5ba4\u5185\u548c\u57ce\u5e02\u6c14\u5019\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22RL HVAC\u63a7\u5236\u5728\u51cf\u5c11\u5efa\u7b51\u80fd\u8017\u540c\u65f6\u4fdd\u6301\u5ba4\u5185\u70ed\u8212\u9002\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u7814\u7a76\u5176\u5728\u4e0d\u540c\u6c14\u5019\u80cc\u666f\u4e0b\u7684\u9002\u7528\u6027\u548c\u5f71\u54cd\u3002", "method": "\u96c6\u6210RL\u3001\u5efa\u7b51\u80fd\u6e90\u6a21\u578b\u548c\u57ce\u5e02\u6c14\u5019\u6a21\u578b\uff0c\u5206\u6790\u57ce\u5e02\u80cc\u666f\u6c14\u5019\u5bf9RL\u7b56\u7565\u7684\u6548\u679c\u3001\u5ba4\u5185\u5916\u6c14\u5019\u5f71\u54cd\u53ca\u8de8\u57ce\u5e02\u7b56\u7565\u53ef\u8fc1\u79fb\u6027\u7684\u5f71\u54cd\u3002", "result": "\u4e0d\u540c\u57ce\u5e02\u7684RL\u7b56\u7565\u6548\u679c\u53ca\u5ba4\u5185\u5916\u6c14\u5019\u5f71\u54cd\u5dee\u5f02\u663e\u8457\uff1b\u708e\u70ed\u6c14\u5019\u57ce\u5e02\u5728\u4e0d\u540c\u5956\u52b1\u6743\u91cd\u4e0b\u8868\u73b0\u66f4\u4f18\uff0c\u6e29\u5ea6\u53d8\u5316\u5927\u7684\u57ce\u5e02\u7b56\u7565\u8fc1\u79fb\u6027\u66f4\u5f3a\u3002", "conclusion": "\u5f3a\u8c03\u5728\u4e0d\u540c\u6c14\u5019\u80cc\u666f\u4e0b\u5168\u9762\u8bc4\u4f30RL HVAC\u63a7\u5236\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u6307\u51fa\u8de8\u57ce\u5e02\u5b66\u4e60\u6709\u52a9\u4e8eRL HVAC\u7684\u90e8\u7f72\u3002"}}
{"id": "2505.06409", "pdf": "https://arxiv.org/pdf/2505.06409", "abs": "https://arxiv.org/abs/2505.06409", "authors": ["Krti Tallam"], "title": "Engineering Risk-Aware, Security-by-Design Frameworks for Assurance of Large-Scale Autonomous AI Models", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.LG", "cs.MA", "cs.SY", "eess.SY"], "comment": null, "summary": "As AI models scale to billions of parameters and operate with increasing\nautonomy, ensuring their safe, reliable operation demands engineering-grade\nsecurity and assurance frameworks. This paper presents an enterprise-level,\nrisk-aware, security-by-design approach for large-scale autonomous AI systems,\nintegrating standardized threat metrics, adversarial hardening techniques, and\nreal-time anomaly detection into every phase of the development lifecycle. We\ndetail a unified pipeline - from design-time risk assessments and secure\ntraining protocols to continuous monitoring and automated audit logging - that\ndelivers provable guarantees of model behavior under adversarial and\noperational stress. Case studies in national security, open-source model\ngovernance, and industrial automation demonstrate measurable reductions in\nvulnerability and compliance overhead. Finally, we advocate cross-sector\ncollaboration - uniting engineering teams, standards bodies, and regulatory\nagencies - to institutionalize these technical safeguards within a resilient,\nend-to-end assurance ecosystem for the next generation of AI.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f01\u4e1a\u7ea7\u3001\u98ce\u9669\u611f\u77e5\u3001\u8bbe\u8ba1\u5373\u5b89\u5168\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u81ea\u4e3bAI\u7cfb\u7edf\u7684\u5b89\u5168\u4fdd\u969c\uff0c\u6574\u5408\u4e86\u6807\u51c6\u5316\u5a01\u80c1\u6307\u6807\u3001\u5bf9\u6297\u6027\u5f3a\u5316\u6280\u672f\u548c\u5b9e\u65f6\u5f02\u5e38\u68c0\u6d4b\uff0c\u5e76\u5728\u5f00\u53d1\u5468\u671f\u5404\u4e2a\u9636\u6bb5\u5b9e\u65bd\u3002\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u56fd\u5bb6\u5b89\u5168\u548c\u5de5\u4e1a\u81ea\u52a8\u5316\u4e2d\u6709\u6548\u51cf\u5c11\u4e86\u6f0f\u6d1e\u548c\u5408\u89c4\u5f00\u9500\u3002", "motivation": "\u968f\u7740AI\u6a21\u578b\u53c2\u6570\u91cf\u589e\u81f3\u6570\u5341\u4ebf\u4e14\u81ea\u4e3b\u6027\u589e\u5f3a\uff0c\u786e\u4fdd\u5176\u5b89\u5168\u53ef\u9760\u8fd0\u884c\u9700\u8981\u5de5\u7a0b\u7ea7\u7684\u5b89\u5168\u548c\u4fdd\u969c\u6846\u67b6\u3002", "method": "\u8bba\u6587\u91c7\u7528\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5b89\u5168\u5f00\u53d1\u6d41\u7a0b\uff0c\u5305\u62ec\u8bbe\u8ba1\u65f6\u98ce\u9669\u8bc4\u4f30\u3001\u5b89\u5168\u8bad\u7ec3\u534f\u8bae\u3001\u6301\u7eed\u76d1\u63a7\u548c\u81ea\u52a8\u5ba1\u8ba1\u65e5\u5fd7\uff0c\u4ee5\u5728\u5bf9\u6297\u6027\u548c\u64cd\u4f5c\u538b\u529b\u4e0b\u63d0\u4f9b\u6a21\u578b\u884c\u4e3a\u7684\u53ef\u8bc1\u660e\u4fdd\u8bc1\u3002", "result": "\u5728\u56fd\u5bb6\u5b89\ufffd\ufffd\ufffd\u3001\u5f00\u6e90\u6a21\u578b\u6cbb\u7406\u548c\u5de5\u4e1a\u81ea\u52a8\u5316\u7b49\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u6f0f\u6d1e\u548c\u5408\u89c4\u5f00\u9500\u3002", "conclusion": "\u4f5c\u8005\u547c\u5401\u8de8\u90e8\u95e8\u5408\u4f5c\uff0c\u8054\u5408\u5de5\u7a0b\u56e2\u961f\u3001\u6807\u51c6\u673a\u6784\u548c\u76d1\u7ba1\u673a\u6784\uff0c\u5c06\u8fd9\u4e9b\u6280\u672f\u4fdd\u969c\u5236\u5ea6\u5316\uff0c\u4ee5\u6784\u5efa\u4e0b\u4e00\u4ee3AI\u7684\u5168\u65b9\u4f4d\u97e7\u6027\u4fdd\u969c\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2505.07070", "pdf": "https://arxiv.org/pdf/2505.07070", "abs": "https://arxiv.org/abs/2505.07070", "authors": ["Francesco Cagnetta", "Alessandro Favero", "Antonio Sclocchi", "Matthieu Wyart"], "title": "Scaling Laws and Representation Learning in Simple Hierarchical Languages: Transformers vs. Convolutional Architectures", "categories": ["cs.LG", "cond-mat.dis-nn", "stat.ML"], "comment": "14 pages, 8 figures", "summary": "How do neural language models acquire a language's structure when trained for\nnext-token prediction? We address this question by deriving theoretical scaling\nlaws for neural network performance on synthetic datasets generated by the\nRandom Hierarchy Model (RHM) -- an ensemble of probabilistic context-free\ngrammars designed to capture the hierarchical structure of natural language\nwhile remaining analytically tractable. Previously, we developed a theory of\nrepresentation learning based on data correlations that explains how deep\nlearning models capture the hierarchical structure of the data sequentially,\none layer at a time. Here, we extend our theoretical framework to account for\narchitectural differences. In particular, we predict and empirically validate\nthat convolutional networks, whose structure aligns with that of the generative\nprocess through locality and weight sharing, enjoy a faster scaling of\nperformance compared to transformer models, which rely on global self-attention\nmechanisms. This finding clarifies the architectural biases underlying neural\nscaling laws and highlights how representation learning is shaped by the\ninteraction between model architecture and the statistical properties of data.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u795e\u7ecf\u7f51\u7edc\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\u5b66\u4e60\u8bed\u8a00\u7ed3\u6784\uff0c\u63d0\u51fa\u4e86\u7406\u8bba\u6269\u5c55\u5b9a\u5f8b\uff0c\u5e76\u5bf9\u6bd4\u4e86\u5377\u79ef\u7f51\u7edc\u548cTransformer\u6a21\u578b\u7684\u6027\u80fd\u5dee\u5f02\u3002", "motivation": "\u63a2\u7d22\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\u4efb\u52a1\u4e2d\u5982\u4f55\u6355\u83b7\u8bed\u8a00\u7684\u7ed3\u6784\uff0c\u5c24\u5176\u662f\u5c42\u6b21\u7ed3\u6784\uff0c\u4ee5\u7406\u89e3\u6a21\u578b\u67b6\u6784\u4e0e\u6570\u636e\u7edf\u8ba1\u7279\u6027\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u4f7f\u7528\u968f\u673a\u5c42\u6b21\u6a21\u578b\uff08RHM\uff09\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u6269\u5c55\u7406\u8bba\u6846\u67b6\u4ee5\u5206\u6790\u4e0d\u540c\u67b6\u6784\uff08\u5982\u5377\u79ef\u7f51\u7edc\u548cTransformer\uff09\u7684\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u53d1\u73b0\u5377\u79ef\u7f51\u7edc\u7531\u4e8e\u5c40\u90e8\u6027\u548c\u6743\u91cd\u5171\u4eab\u7684\u7279\u6027\uff0c\u6027\u80fd\u63d0\u5347\u901f\u5ea6\u4f18\u4e8e\u4f9d\u8d56\u5168\u5c40\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684Transformer\u6a21\u578b\u3002", "conclusion": "\u6a21\u578b\u67b6\u6784\u4e0e\u6570\u636e\u7edf\u8ba1\u7279\u6027\u7684\u4ea4\u4e92\u51b3\u5b9a\u4e86\u8868\u793a\u5b66\u4e60\u7684\u6548\u679c\uff0c\u5377\u79ef\u7f51\u7edc\u5728\u6355\u83b7\u5c42\u6b21\u7ed3\u6784\u65b9\u9762\u66f4\u5177\u4f18\u52bf\u3002"}}
{"id": "2505.06411", "pdf": "https://arxiv.org/pdf/2505.06411", "abs": "https://arxiv.org/abs/2505.06411", "authors": ["Fangyu Du", "Yang Yang", "Xuehao Gao", "Hongye Hou"], "title": "MAGE:A Multi-stage Avatar Generator with Sparse Observations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Inferring full-body poses from Head Mounted Devices, which capture only\n3-joint observations from the head and wrists, is a challenging task with wide\nAR/VR applications. Previous attempts focus on learning one-stage motion\nmapping and thus suffer from an over-large inference space for unobserved body\njoint motions. This often leads to unsatisfactory lower-body predictions and\npoor temporal consistency, resulting in unrealistic or incoherent motion\nsequences. To address this, we propose a powerful Multi-stage Avatar GEnerator\nnamed MAGE that factorizes this one-stage direct motion mapping learning with a\nprogressive prediction strategy. Specifically, given initial 3-joint motions,\nMAGE gradually inferring multi-scale body part poses at different abstract\ngranularity levels, starting from a 6-part body representation and gradually\nrefining to 22 joints. With decreasing abstract levels step by step, MAGE\nintroduces more motion context priors from former prediction stages and thus\nimproves realistic motion completion with richer constraint conditions and less\nambiguity. Extensive experiments on large-scale datasets verify that MAGE\nsignificantly outperforms state-of-the-art methods with better accuracy and\ncontinuity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMAGE\u7684\u591a\u9636\u6bb5\u4eba\u4f53\u59ff\u6001\u751f\u6210\u5668\uff0c\u901a\u8fc7\u9010\u6b65\u9884\u6d4b\u7b56\u7565\u4ece\u5934\u90e8\u548c\u624b\u8155\u76843\u5173\u8282\u70b9\u63a8\u6d4b\u5168\u8eab\u59ff\u6001\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u65f6\u5e8f\u8fde\u7eed\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u76f4\u63a5\u4ece3\u5173\u8282\u70b9\u6620\u5c04\u5168\u8eab\u59ff\u6001\uff0c\u5b58\u5728\u63a8\u65ad\u7a7a\u95f4\u8fc7\u5927\u3001\u4e0b\u80a2\u9884\u6d4b\u4e0d\u51c6\u786e\u548c\u65f6\u5e8f\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002MAGE\u65e8\u5728\u901a\u8fc7\u591a\u9636\u6bb5\u7ec6\u5316\u7b56\u7565\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "MAGE\u91c7\u7528\u6e10\u8fdb\u5f0f\u9884\u6d4b\u7b56\u7565\uff0c\u4ece6\u90e8\u5206\u7c97\u7c92\u5ea6\u8eab\u4f53\u8868\u793a\u9010\u6b65\u7ec6\u5316\u523022\u5173\u8282\u70b9\uff0c\u6bcf\u9636\u6bb5\u5f15\u5165\u524d\u4e00\u6b65\u7684\u8fd0\u52a8\u4e0a\u4e0b\u6587\u5148\u9a8c\uff0c\u51cf\u5c11\u6b67\u4e49\u5e76\u63d0\u5347\u771f\u5b9e\u6027\u3002", "result": "\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMAGE\u5728\u51c6\u786e\u6027\u548c\u8fde\u7eed\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u591a\u9636\u6bb5\u7ec6\u5316\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u4ece\u7a00\u758f\u8f93\u5165\u751f\u6210\u5168\u8eab\u59ff\u6001\u7684\u8d28\u91cf\uff0c\u4e3aAR/VR\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u8fd0\u52a8\u5e8f\u5217\u3002"}}
{"id": "2505.07081", "pdf": "https://arxiv.org/pdf/2505.07081", "abs": "https://arxiv.org/abs/2505.07081", "authors": ["Gregoire Fournier", "Sourav Medya"], "title": "COMRECGC: Global Graph Counterfactual Explainer through Common Recourse", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted at ICML 2025", "summary": "Graph neural networks (GNNs) have been widely used in various domains such as\nsocial networks, molecular biology, or recommendation systems. Concurrently,\ndifferent explanations methods of GNNs have arisen to complement its black-box\nnature. Explanations of the GNNs' predictions can be categorized into two\ntypes--factual and counterfactual. Given a GNN trained on binary classification\ninto ''accept'' and ''reject'' classes, a global counterfactual explanation\nconsists in generating a small set of ''accept'' graphs relevant to all of the\ninput ''reject'' graphs. The transformation of a ''reject'' graph into an\n''accept'' graph is called a recourse. A common recourse explanation is a small\nset of recourse, from which every ''reject'' graph can be turned into an\n''accept'' graph. Although local counterfactual explanations have been studied\nextensively, the problem of finding common recourse for global counterfactual\nexplanation remains unexplored, particularly for GNNs. In this paper, we\nformalize the common recourse explanation problem, and design an effective\nalgorithm, COMRECGC, to solve it. We benchmark our algorithm against strong\nbaselines on four different real-world graphs datasets and demonstrate the\nsuperior performance of COMRECGC against the competitors. We also compare the\ncommon recourse explanations to the graph counterfactual explanation, showing\nthat common recourse explanations are either comparable or superior, making\nthem worth considering for applications such as drug discovery or computational\nbiology.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9GNN\u7684\u5168\u5c40\u53cd\u4e8b\u5b9e\u89e3\u91ca\u95ee\u9898\uff0c\u63d0\u51fa\u4e86COMRECGC\u7b97\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u5e38\u89c1\u7684\u8865\u6551\u63aa\u65bd\uff0c\u5c06\u2018\u62d2\u7edd\u2019\u7c7b\u56fe\u8f6c\u6362\u4e3a\u2018\u63a5\u53d7\u2019\u7c7b\u56fe\u3002\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u836f\u7269\u53d1\u73b0\u7b49\u9886\u57df\u7684\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u7684GNN\u89e3\u91ca\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5c40\u90e8\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u800c\u5168\u5c40\u53cd\u4e8b\u5b9e\u89e3\u91ca\u4e2d\u7684\u5e38\u89c1\u8865\u6551\u63aa\u65bd\u95ee\u9898\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4f5c\u8005\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u4ee5\u652f\u6301GNN\u5728\u5168\u5c40\u89e3\u91ca\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u8bba\u6587\u9996\u5148\u5f62\u5f0f\u5316\u4e86\u5e38\u89c1\u8865\u6551\u63aa\u65bd\u89e3\u91ca\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u4e86COMRECGC\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u901a\u8fc7\u751f\u6210\u4e00\u5c0f\u90e8\u5206\u8865\u6551\u63aa\u65bd\u96c6\u5408\uff0c\u4f7f\u5f97\u6240\u6709\u2018\u62d2\u7edd\u2019\u7c7b\u56fe\u5747\u53ef\u901a\u8fc7\u8fd9\u4e9b\u63aa\u65bd\u8f6c\u6362\u4e3a\u2018\u63a5\u53d7\u2019\u7c7b\u56fe\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCOMRECGC\u7b97\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u5bf9\u6bd4\u5176\u4ed6\u56fe\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\uff0c\u5e38\u89c1\u8865\u6551\u63aa\u65bd\u7684\u751f\u6210\u8868\u73b0\u51fa\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u6548\u679c\u3002", "conclusion": "\u5e38\u89c1\u8865\u6551\u63aa\u65bd\u89e3\u91ca\u4e0d\u4ec5\u586b\u8865\u4e86GNN\u5168\u5c40\u89e3\u91ca\u7684\u7a7a\u767d\uff0c\u8fd8\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff08\u5982\u836f\u7269\u53d1\u73b0\uff09\u3002COMRECGC\u7b97\u6cd5\u7684\u9ad8\u6548\u6027\u548c\u666e\u9002\u6027\u4e3aGNN\u7684\u900f\u660e\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.06413", "pdf": "https://arxiv.org/pdf/2505.06413", "abs": "https://arxiv.org/abs/2505.06413", "authors": ["Ming Liu", "Siyuan Liang", "Koushik Howlader", "Liwen Wang", "Dacheng Tao", "Wensheng Zhang"], "title": "Natural Reflection Backdoor Attack on Vision Language Model for Autonomous Driving", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) have been integrated into autonomous driving\nsystems to enhance reasoning capabilities through tasks such as Visual Question\nAnswering (VQA). However, the robustness of these systems against backdoor\nattacks remains underexplored. In this paper, we propose a natural\nreflection-based backdoor attack targeting VLM systems in autonomous driving\nscenarios, aiming to induce substantial response delays when specific visual\ntriggers are present. We embed faint reflection patterns, mimicking natural\nsurfaces such as glass or water, into a subset of images in the DriveLM\ndataset, while prepending lengthy irrelevant prefixes (e.g., fabricated stories\nor system update notifications) to the corresponding textual labels. This\nstrategy trains the model to generate abnormally long responses upon\nencountering the trigger. We fine-tune two state-of-the-art VLMs, Qwen2-VL and\nLLaMA-Adapter, using parameter-efficient methods. Experimental results\ndemonstrate that while the models maintain normal performance on clean inputs,\nthey exhibit significantly increased inference latency when triggered,\npotentially leading to hazardous delays in real-world autonomous driving\ndecision-making. Further analysis examines factors such as poisoning rates,\ncamera perspectives, and cross-view transferability. Our findings uncover a new\nclass of attacks that exploit the stringent real-time requirements of\nautonomous driving, posing serious challenges to the security and reliability\nof VLM-augmented driving systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u81ea\u7136\u53cd\u5c04\u7684\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\uff0c\u901a\u8fc7\u5728\u56fe\u50cf\u5d4c\u5165\u53cd\u5c04\u6a21\u5f0f\u5e76\u4fee\u6539\u6587\u672c\u6807\u7b7e\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u89e6\u53d1\u65f6\u4ea7\u751f\u5ef6\u8fdf\u54cd\u5e94\uff0c\u5a01\u80c1\u7cfb\u7edf\u5b9e\u65f6\u6027\u3002", "motivation": "\u7814\u7a76\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u4e2d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u7684\u540e\u95e8\u653b\u51fb\uff0c\u586b\u8865\u4e86\u5176\u5728\u9c81\u68d2\u6027\u65b9\u9762\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u65e8\u5728\u63ed\u793a\u6f5c\u5728\u7684\u5b89\u5168\u98ce\u9669\uff0c\u7279\u522b\u662f\u5728\u5b9e\u65f6\u6027\u8981\u6c42\u4e25\u683c\u7684\u573a\u666f\u4e0b\u3002", "method": "\u901a\u8fc7\u5728DriveLM\u6570\u636e\u96c6\u4e2d\u5d4c\u5165\u5fae\u5f31\u7684\u81ea\u7136\u53cd\u5c04\u6a21\u5f0f\uff08\u5982\u73bb\u7483\u6216\u6c34\u9762\u53cd\u5c04\uff09\uff0c\u5e76\u5728\u5bf9\u5e94\u7684\u6587\u672c\u6807\u7b7e\u524d\u6dfb\u52a0\u65e0\u5173\u524d\u7f00\uff08\u5982\u865a\u6784\u6545\u4e8b\u6216\u7cfb\u7edf\u901a\u77e5\uff09\uff0c\u8bad\u7ec3Qwen2-VL\u548cLLaMA-Adapter\u6a21\u578b\uff0c\u4f7f\u5176\u5728\u89e6\u53d1\u65f6\u751f\u6210\u5f02\u5e38\u5197\u957f\u7684\u54cd\u5e94\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u5728\u5e72\u51c0\u8f93\u5165\u4e0b\u8868\u73b0\u6b63\u5e38\uff0c\u4f46\u5728\u89e6\u53d1\u65f6\u63a8\u7406\u5ef6\u8fdf\u663e\u8457\u589e\u52a0\uff0c\u53ef\u80fd\u5f15\u53d1\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7684 hazardous delays\u3002\u7814\u7a76\u8fd8\u5206\u6790\u4e86\u4e2d\u6bd2\u7387\u3001\u76f8\u673a\u89c6\u89d2\u548c\u8de8\u89c6\u56fe\u8fc1\u79fb\u6027\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u4e00\u79cd\u65b0\u578b\u653b\u51fb\uff0c\u5229\u7528\u81ea\u52a8\u9a7e\u9a76\u7684\u5b9e\u65f6\u6027\u9700\u6c42\u5a01\u80c1VLM\u589e\u5f3a\u7cfb\u7edf\u7684\u5b89\u5168\uff0c\u547c\u5401\u52a0\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u548c\u9632\u5fa1\u673a\u5236\u7684\u7814\u7a76\u3002"}}
{"id": "2505.07086", "pdf": "https://arxiv.org/pdf/2505.07086", "abs": "https://arxiv.org/abs/2505.07086", "authors": ["Tong Chen", "Yinuo Zhang", "Sophia Tang", "Pranam Chatterjee"], "title": "Multi-Objective-Guided Discrete Flow Matching for Controllable Biological Sequence Design", "categories": ["cs.LG", "q-bio.BM"], "comment": null, "summary": "Designing biological sequences that satisfy multiple, often conflicting,\nfunctional and biophysical criteria remains a central challenge in biomolecule\nengineering. While discrete flow matching models have recently shown promise\nfor efficient sampling in high-dimensional sequence spaces, existing approaches\naddress only single objectives or require continuous embeddings that can\ndistort discrete distributions. We present Multi-Objective-Guided Discrete Flow\nMatching (MOG-DFM), a general framework to steer any pretrained discrete-time\nflow matching generator toward Pareto-efficient trade-offs across multiple\nscalar objectives. At each sampling step, MOG-DFM computes a hybrid\nrank-directional score for candidate transitions and applies an adaptive\nhypercone filter to enforce consistent multi-objective progression. We also\ntrained two unconditional discrete flow matching models, PepDFM for diverse\npeptide generation and EnhancerDFM for functional enhancer DNA generation, as\nbase generation models for MOG-DFM. We demonstrate MOG-DFM's effectiveness in\ngenerating peptide binders optimized across five properties (hemolysis,\nnon-fouling, solubility, half-life, and binding affinity), and in designing DNA\nsequences with specific enhancer classes and DNA shapes. In total, MOG-DFM\nproves to be a powerful tool for multi-property-guided biomolecule sequence\ndesign.", "AI": {"tldr": "\u63d0\u51fa\u4e86MOG-DFM\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u591a\u76ee\u6807\u51b2\u7a81\u7684\u6761\u4ef6\u4e0b\u9ad8\u6548\u751f\u6210\u751f\u7269\u5206\u5b50\u5e8f\u5217\u3002", "motivation": "\u89e3\u51b3\u751f\u7269\u5206\u5b50\u8bbe\u8ba1\u4e2d\u591a\u76ee\u6807\u51b2\u7a81\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u79bb\u6563\u5206\u5e03\u4e0e\u591a\u76ee\u6807\u4f18\u5316\u3002", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684\u79bb\u6563\u65f6\u95f4\u6d41\u5339\u914d\u751f\u6210\u5668\uff0c\u63d0\u51fa\u6df7\u5408\u79e9-\u65b9\u5411\u8bc4\u5206\u4e0e\u81ea\u9002\u5e94\u8d85\u9525\u6ee4\u6ce2\u6280\u672f\u3002", "result": "\u5728\u4f18\u5316\u80bd\u7ed3\u5408\u5242\u548c\u589e\u5f3aDNA\u5e8f\u5217\u8bbe\u8ba1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u73b0\u591a\u5c5e\u6027\u5e73\u8861\u3002", "conclusion": "MOG-DFM\u662f\u4e00\u79cd\u5f3a\u5927\u7684\u591a\u5c5e\u6027\u751f\u7269\u5206\u5b50\u5e8f\u5217\u8bbe\u8ba1\u5de5\u5177\u3002"}}
{"id": "2505.06428", "pdf": "https://arxiv.org/pdf/2505.06428", "abs": "https://arxiv.org/abs/2505.06428", "authors": ["Somayeh Molaei", "Lionel P. Robert", "Nikola Banovic"], "title": "What Do People Want to Know About Artificial Intelligence (AI)? The Importance of Answering End-User Questions to Explain Autonomous Vehicle (AV) Decisions", "categories": ["cs.HC", "cs.AI"], "comment": "Accepted to the Proceedings of the ACM on Human-Computer Interaction,\n  CSCW, October 2025", "summary": "Improving end-users' understanding of decisions made by autonomous vehicles\n(AVs) driven by artificial intelligence (AI) can improve utilization and\nacceptance of AVs. However, current explanation mechanisms primarily help AI\nresearchers and engineers in debugging and monitoring their AI systems, and may\nnot address the specific questions of end-users, such as passengers, about AVs\nin various scenarios. In this paper, we conducted two user studies to\ninvestigate questions that potential AV passengers might pose while riding in\nan AV and evaluate how well answers to those questions improve their\nunderstanding of AI-driven AV decisions. Our initial formative study identified\na range of questions about AI in autonomous driving that existing explanation\nmechanisms do not readily address. Our second study demonstrated that\ninteractive text-based explanations effectively improved participants'\ncomprehension of AV decisions compared to simply observing AV decisions. These\nfindings inform the design of interactions that motivate end-users to engage\nwith and inquire about the reasoning behind AI-driven AV decisions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4e24\u9879\u7528\u6237\u8c03\u67e5\uff0c\u63a2\u8ba8\u4e86\u6f5c\u5728\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u4e58\u5ba2\u53ef\u80fd\u63d0\u51fa\u7684\u95ee\u9898\uff0c\u5e76\u9a8c\u8bc1\u4e86\u4ea4\u4e92\u5f0f\u6587\u672c\u89e3\u91ca\u5bf9\u63d0\u9ad8\u4e58\u5ba2\u7406\u89e3AI\u51b3\u7b56\u7684\u6548\u679c\u3002", "motivation": "\u63d0\u5347\u7ec8\u7aef\u7528\u6237\u5bf9AI\u9a71\u52a8\u7684\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u51b3\u7b56\u7684\u7406\u89e3\uff0c\u4ee5\u589e\u5f3a\u5176\u4f7f\u7528\u7387\u548c\u63a5\u53d7\u5ea6\u3002\u73b0\u6709\u7684\u89e3\u91ca\u673a\u5236\u4e3b\u8981\u670d\u52a1\u4e8eAI\u7814\u7a76\u4eba\u5458\u548c\u5de5\u7a0b\u5e08\uff0c\u672a\u80fd\u6ee1\u8db3\u4e58\u5ba2\u5728\u5177\u4f53\u573a\u666f\u4e2d\u7684\u9700\u6c42\u3002", "method": "\u8fdb\u884c\u4e24\u9879\u7528\u6237\u7814\u7a76\uff1a\u7b2c\u4e00\u9879\u7814\u7a76\u8bc6\u522b\u73b0\u6709\u89e3\u91ca\u673a\u5236\u672a\u6db5\u76d6\u7684\u4e58\u5ba2\u95ee\u9898\uff1b\u7b2c\u4e8c\u9879\u7814\u7a76\u8bc4\u4f30\u4ea4\u4e92\u5f0f\u6587\u672c\u89e3\u91ca\u5bf9\u63d0\u5347\u4e58\u5ba2\u7406\u89e3\u7684\u6548\u679c\u3002", "result": "\u4ea4\u4e92\u5f0f\u6587\u672c\u89e3\u91ca\u663e\u8457\u63d0\u9ad8\u4e86\u53c2\u4e0e\u8005\u5bf9\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u51b3\u7b56\u7684\u7406\u89e3\uff0c\u4f18\u4e8e\u5355\u7eaf\u89c2\u5bdf\u51b3\u7b56\u7684\u65b9\u5f0f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u8bbe\u8ba1\u4fc3\u4f7f\u7ec8\u7aef\u7528\u6237\u53c2\u4e0e\u5e76\u8be2\u95eeAI\u51b3\u7b56\u80cc\u540e\u539f\u56e0\u7684\u7528\u6237\u4ea4\u4e92\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2505.07090", "pdf": "https://arxiv.org/pdf/2505.07090", "abs": "https://arxiv.org/abs/2505.07090", "authors": ["Bilal Ahmed", "Yuqing Qiu", "Diab W. Abueidda", "Waleed El-Sekelly", "Tarek Abdoun", "Mostafa E. Mobasher"], "title": "Physics-informed Multiple-Input Operators for efficient dynamic response prediction of structures", "categories": ["cs.LG"], "comment": null, "summary": "Finite element (FE) modeling is essential for structural analysis but remains\ncomputationally intensive, especially under dynamic loading. While operator\nlearning models have shown promise in replicating static structural responses\nat FEM level accuracy, modeling dynamic behavior remains more challenging. This\nwork presents a Multiple Input Operator Network (MIONet) that incorporates a\nsecond trunk network to explicitly encode temporal dynamics, enabling accurate\nprediction of structural responses under moving loads. Traditional DeepONet\narchitectures using recurrent neural networks (RNNs) are limited by fixed time\ndiscretization and struggle to capture continuous dynamics. In contrast, MIONet\npredicts responses continuously over both space and time, removing the need for\nstep wise modeling. It maps scalar inputs including load type, velocity,\nspatial mesh, and time steps to full field structural responses. To improve\nefficiency and enforce physical consistency, we introduce a physics informed\nloss based on dynamic equilibrium using precomputed mass, damping, and\nstiffness matrices, without solving the governing PDEs directly. Further, a\nSchur complement formulation reduces the training domain, significantly cutting\ncomputational costs while preserving global accuracy. The model is validated on\nboth a simple beam and the KW-51 bridge, achieving FEM level accuracy within\nseconds. Compared to GRU based DeepONet, our model offers comparable accuracy\nwith improved temporal continuity and over 100 times faster inference, making\nit well suited for real-time structural monitoring and digital twin\napplications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8f93\u5165\u7b97\u5b50\u7f51\u7edc\uff08MIONet\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u7b2c\u4e8c\u4e3b\u5e72\u7f51\u7edc\u663e\u5f0f\u7f16\u7801\u65f6\u95f4\u52a8\u6001\uff0c\u5b9e\u73b0\u4e86\u79fb\u52a8\u8f7d\u8377\u4e0b\u7ed3\u6784\u54cd\u5e94\u7684\u51c6\u786e\u9884\u6d4b\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u6709\u9650\u5143\u5efa\u6a21\uff08FEM\uff09\u5728\u52a8\u6001\u8f7d\u8377\u4e0b\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u8fde\u7eed\u6355\u6349\u65f6\u7a7a\u52a8\u6001\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u7269\u7406\u4e00\u81f4\u6027\u5f3a\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "MIONet\u91c7\u7528\u53cc\u4e3b\u5e72\u7f51\u7edc\u8bbe\u8ba1\uff0c\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u635f\u5931\u51fd\u6570\u548cSchur\u8865\u5f62\u5f0f\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u65e0\u9700\u76f4\u63a5\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u3002", "result": "MIONet\u5728\u6881\u548c\u6865\u6881\u6848\u4f8b\u4e2d\u8fbe\u5230FEM\u7ea7\u7cbe\u5ea6\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u57fa\u4e8eGRU\u7684DeepONet\u5feb100\u500d\u4ee5\u4e0a\u3002", "conclusion": "MIONet\u9002\u7528\u4e8e\u5b9e\u65f6\u7ed3\u6784\u76d1\u6d4b\u548c\u6570\u5b57\u5b6a\u751f\u5e94\u7528\uff0c\u517c\u5177\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2505.06436", "pdf": "https://arxiv.org/pdf/2505.06436", "abs": "https://arxiv.org/abs/2505.06436", "authors": ["Jingrui He", "Andrew Stephen McGough"], "title": "My Emotion on your face: The use of Facial Keypoint Detection to preserve Emotions in Latent Space Editing", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to 2nd International Workshop on Synthetic Data for Face\n  and Gesture Analysis at IEEE FG 2025", "summary": "Generative Adversarial Network approaches such as StyleGAN/2 provide two key\nbenefits: the ability to generate photo-realistic face images and possessing a\nsemantically structured latent space from which these images are created. Many\napproaches have emerged for editing images derived from vectors in the latent\nspace of a pre-trained StyleGAN/2 models by identifying semantically meaningful\ndirections (e.g., gender or age) in the latent space. By moving the vector in a\nspecific direction, the ideal result would only change the target feature while\npreserving all the other features. Providing an ideal data augmentation\napproach for gesture research as it could be used to generate numerous image\nvariations whilst keeping the facial expressions intact. However, entanglement\nissues, where changing one feature inevitably affects other features, impacts\nthe ability to preserve facial expressions. To address this, we propose the use\nof an addition to the loss function of a Facial Keypoint Detection model to\nrestrict changes to the facial expressions. Building on top of an existing\nmodel, adding the proposed Human Face Landmark Detection (HFLD) loss, provided\nby a pre-trained Facial Keypoint Detection model, to the original loss\nfunction. We quantitatively and qualitatively evaluate the existing and our\nextended model, showing the effectiveness of our approach in addressing the\nentanglement issue and maintaining the facial expression. Our approach achieves\nup to 49% reduction in the change of emotion in our experiments. Moreover, we\nshow the benefit of our approach by comparing with state-of-the-art models. By\nincreasing the ability to preserve the facial gesture and expression during\nfacial transformation, we present a way to create human face images with fixed\nexpression but different appearances, making it a reliable data augmentation\napproach for Facial Gesture and Expression research.", "AI": {"tldr": "\u901a\u8fc7\u5728\u4eba\u8138\u5173\u952e\u70b9\u68c0\u6d4b\u6a21\u578b\u7684\u635f\u5931\u51fd\u6570\u4e2d\u589e\u52a0HFLD\u635f\u5931\uff0c\u6709\u6548\u89e3\u51b3\u4e86StyleGAN/2\u751f\u6210\u4eba\u8138\u56fe\u50cf\u65f6\u7279\u5f81\u7ea0\u7f20\u95ee\u9898\uff0c\u4fdd\u7559\u4e86\u9762\u90e8\u8868\u60c5\uff0c\u4e3a\u624b\u52bf\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002", "motivation": "StyleGAN/2\u867d\u7136\u80fd\u751f\u6210\u903c\u771f\u4eba\u8138\u56fe\u50cf\u4e14\u5177\u6709\u8bed\u4e49\u5316\u6f5c\u7a7a\u95f4\uff0c\u4f46\u7f16\u8f91\u56fe\u50cf\u65f6\u5b58\u5728\u7279\u5f81\u7ea0\u7f20\u95ee\u9898\uff0c\u5f71\u54cd\u8868\u60c5\u4fdd\u7559\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5728\u539f\u635f\u5931\u51fd\u6570\u57fa\u7840\u4e0a\u589e\u52a0HFLD\u635f\u5931\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u4eba\u8138\u5173\u952e\u70b9\u68c0\u6d4b\u6a21\u578b\u9650\u5236\u9762\u90e8\u8868\u60c5\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u51cf\u5c1149%\u7684\u60c5\u611f\u53d8\u5316\uff0c\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u6709\u6548\u4fdd\u7559\u8868\u60c5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u65f6\u8868\u60c5\u7684\u4fdd\u7559\u80fd\u529b\uff0c\u4e3a\u9762\u90e8\u624b\u52bf\u548c\u8868\u60c5\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6848\u3002"}}
{"id": "2505.07100", "pdf": "https://arxiv.org/pdf/2505.07100", "abs": "https://arxiv.org/abs/2505.07100", "authors": ["Julian Rosenberger", "Philipp Schr\u00f6ppel", "Sven Kruschel", "Mathias Kraus", "Patrick Zschech", "Maximilian F\u00f6rster"], "title": "Navigating the Rashomon Effect: How Personalization Can Help Adjust Interpretable Machine Learning Models to Individual Users", "categories": ["cs.LG", "cs.HC"], "comment": "Accepted as a Completed Research Paper at the Thirty-Third European\n  Conference on Information Systems (ECIS 2025), Amman, Jordan", "summary": "The Rashomon effect describes the observation that in machine learning (ML)\nmultiple models often achieve similar predictive performance while explaining\nthe underlying relationships in different ways. This observation holds even for\nintrinsically interpretable models, such as Generalized Additive Models (GAMs),\nwhich offer users valuable insights into the model's behavior. Given the\nexistence of multiple GAM configurations with similar predictive performance, a\nnatural question is whether we can personalize these configurations based on\nusers' needs for interpretability. In our study, we developed an approach to\npersonalize models based on contextual bandits. In an online experiment with\n108 users in a personalized treatment and a non-personalized control group, we\nfound that personalization led to individualized rather than one-size-fits-all\nconfigurations. Despite these individual adjustments, the interpretability\nremained high across both groups, with users reporting a strong understanding\nof the models. Our research offers initial insights into the potential for\npersonalizing interpretable ML.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5728\u9884\u6d4b\u6027\u80fd\u76f8\u4f3c\u7684\u5e7f\u4e49\u52a0\u6027\u6a21\u578b\uff08GAMs\uff09\u4e2d\u6839\u636e\u7528\u6237\u9700\u6c42\u4e2a\u6027\u5316\u914d\u7f6e\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5728\u7ebf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e2a\u6027\u5316\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u591a\u4e2a\u6a21\u578b\u9884\u6d4b\u6027\u80fd\u76f8\u4f3c\u4f46\u89e3\u91ca\u65b9\u5f0f\u4e0d\u540c\u65f6\uff0c\u5982\u4f55\u6839\u636e\u7528\u6237\u9700\u6c42\u4e2a\u6027\u5316\u914d\u7f6e\u6a21\u578b\u4ee5\u63d0\u5347\u89e3\u91ca\u6027\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e0a\u4e0b\u6587\u8001\u864e\u673a\u65b9\u6cd5\u5bf9\u6a21\u578b\u8fdb\u884c\u4e2a\u6027\u5316\u914d\u7f6e\uff0c\u5e76\u901a\u8fc7108\u540d\u7528\u6237\u7684\u5728\u7ebf\u5b9e\u9a8c\uff0c\u5bf9\u6bd4\u4e86\u4e2a\u6027\u5316\u7ec4\u548c\u975e\u4e2a\u6027\u5316\u7ec4\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4e2a\u6027\u5316\u65b9\u6cd5\u80fd\u591f\u4ea7\u751f\u9488\u5bf9\u4e2a\u4f53\u7684\u914d\u7f6e\uff0c\u4e14\u4e24\u7ec4\u7528\u6237\u7684\u6a21\u578b\u89e3\u91ca\u6027\u5747\u4fdd\u6301\u8f83\u9ad8\u6c34\u5e73\u3002", "conclusion": "\u7814\u7a76\u521d\u6b65\u63a2\u7d22\u4e86\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u7684\u4e2a\u6027\u5316\u6f5c\u529b\uff0c\u663e\u793a\u4e86\u4e2a\u6027\u5316\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u89e3\u91ca\u6027\u65b9\u9762\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2505.07124", "pdf": "https://arxiv.org/pdf/2505.07124", "abs": "https://arxiv.org/abs/2505.07124", "authors": ["Francisco Andrade", "Gabriel Peyr\u00e9", "Clarice Poon"], "title": "Learning from Samples: Inverse Problems over measures via Sharpened Fenchel-Young Losses", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "Estimating parameters from samples of an optimal probability distribution is\nessential in applications ranging from socio-economic modeling to biological\nsystem analysis. In these settings, the probability distribution arises as the\nsolution to an optimization problem that captures either static interactions\namong agents or the dynamic evolution of a system over time. Our approach\nrelies on minimizing a new class of loss functions, called sharpened\nFenchel-Young losses, which measure the sub-optimality gap of the optimization\nproblem over the space of measures. We study the stability of this estimation\nmethod when only a finite number of sample is available. The parameters to be\nestimated typically correspond to a cost function in static problems and to a\npotential function in dynamic problems. To analyze stability, we introduce a\ngeneral methodology that leverages the strong convexity of the loss function\ntogether with the sample complexity of the forward optimization problem. Our\nanalysis emphasizes two specific settings in the context of optimal transport,\nwhere our method provides explicit stability guarantees: The first is inverse\nunbalanced optimal transport (iUOT) with entropic regularization, where the\nparameters to estimate are cost functions that govern transport computations;\nthis method has applications such as link prediction in machine learning. The\nsecond is inverse gradient flow (iJKO), where the objective is to recover a\npotential function that drives the evolution of a probability distribution via\nthe Jordan-Kinderlehrer-Otto (JKO) time-discretization scheme; this is\nparticularly relevant for understanding cell population dynamics in single-cell\ngenomics. Finally, we validate our approach through numerical experiments on\nGaussian distributions, where closed-form solutions are available, to\ndemonstrate the practical performance of our methods", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6700\u5c0f\u5316\u65b0\u635f\u5931\u51fd\u6570\uff08sharpened Fenchel-Young losses\uff09\u6765\u4f30\u8ba1\u6700\u4f18\u6982\u7387\u5206\u5e03\u53c2\u6570\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u6709\u9650\u6837\u672c\u4e0b\u7684\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u9006\u4e0d\u5e73\u8861\u6700\u4f18\u4f20\u8f93\u548c\u9006\u68af\u5ea6\u6d41\u4e24\u4e2a\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f30\u8ba1\u6700\u4f18\u6982\u7387\u5206\u5e03\u7684\u53c2\u6570\u5728\u591a\u4e2a\u9886\u57df\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5728\u6709\u9650\u6837\u672c\u4e0b\u7684\u7a33\u5b9a\u6027\u4e0d\u8db3\uff0c\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u65b0\u7684\u635f\u5931\u51fd\u6570\u548c\u7a33\u5b9a\u6027\u5206\u6790\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86sharpened Fenchel-Young\u635f\u5931\u51fd\u6570\u6765\u5ea6\u91cf\u4f18\u5316\u95ee\u9898\u7684\u6b21\u4f18\u6027\uff0c\u5e76\u5229\u7528\u635f\u5931\u51fd\u6570\u7684\u5f3a\u51f8\u6027\u548c\u524d\u5411\u4f18\u5316\u95ee\u9898\u7684\u6837\u672c\u590d\u6742\u5ea6\u6765\u5206\u6790\u7a33\u5b9a\u6027\u3002", "result": "\u65b9\u6cd5\u5728\u9006\u4e0d\u5e73\u8861\u6700\u4f18\u4f20\u8f93\uff08iUOT\uff09\u548c\u9006\u68af\u5ea6\u6d41\uff08iJKO\uff09\u573a\u666f\u4e2d\u53d6\u5f97\u663e\u5f0f\u7a33\u5b9a\u6027\u4fdd\u8bc1\uff0c\u5e76\u901a\u8fc7\u9ad8\u65af\u5206\u5e03\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u6027\u80fd\u3002", "conclusion": "\u8bba\u6587\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4f30\u8ba1\u6700\u4f18\u6982\u7387\u5206\u5e03\u53c2\u6570\u63d0\u4f9b\u4e86\u7a33\u5b9a\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.07137", "pdf": "https://arxiv.org/pdf/2505.07137", "abs": "https://arxiv.org/abs/2505.07137", "authors": ["Danny Calegari"], "title": "Triangulating PL functions and the existence of efficient ReLU DNNs", "categories": ["cs.LG", "math.GT"], "comment": "4 pages", "summary": "We show that every piecewise linear function $f:R^d \\to R$ with compact\nsupport a polyhedron $P$ has a representation as a sum of so-called `simplex\nfunctions'. Such representations arise from degree 1 triangulations of the\nrelative homology class (in $R^{d+1}$) bounded by $P$ and the graph of $f$, and\ngive a short elementary proof of the existence of efficient universal ReLU\nneural networks that simultaneously compute all such functions $f$ of bounded\ncomplexity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc1\u660e\u6240\u6709\u5206\u6bb5\u7ebf\u6027\u51fd\u6570\u53ef\u4ee5\u901a\u8fc7\u4e09\u89d2\u5256\u5206\u8868\u793a\u4e3a\u5355\u7eaf\u5f62\u51fd\u6570\u7684\u548c\uff0c\u4ece\u800c\u4e3aReLU\u795e\u7ecf\u7f51\u7edc\u7684\u9ad8\u6548\u901a\u7528\u6027\u63d0\u4f9b\u7b80\u6d01\u8bc1\u660e\u3002", "motivation": "\u7814\u7a76\u5206\u6bb5\u7ebf\u6027\u51fd\u6570\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u4ee5\u652f\u6301ReLU\u795e\u7ecf\u7f51\u7edc\u7684\u9ad8\u6548\u901a\u7528\u8ba1\u7b97\u80fd\u529b\u3002", "method": "\u5229\u7528\u76f8\u5bf9\u540c\u8c03\u7c7b\u7684\u4e00\u5ea6\u4e09\u89d2\u5256\u5206\uff0c\u5c06\u5206\u6bb5\u7ebf\u6027\u51fd\u6570\u8868\u793a\u4e3a\u5355\u7eaf\u5f62\u51fd\u6570\u7684\u548c\u3002", "result": "\u8bc1\u660e\u4e86\u9ad8\u6548\u901a\u7528ReLU\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u540c\u65f6\u8ba1\u7b97\u6240\u6709\u590d\u6742\u5ea6\u53d7\u9650\u7684\u5206\u6bb5\u7ebf\u6027\u51fd\u6570\u3002", "conclusion": "\u901a\u8fc7\u4e09\u89d2\u5256\u5206\u548c\u5355\u7eaf\u5f62\u51fd\u6570\u8868\u793a\uff0c\u4e3aReLU\u795e\u7ecf\u7f51\u7edc\u7684\u901a\u7528\u6027\u63d0\u4f9b\u4e86\u7b80\u6d01\u7684\u6570\u5b66\u57fa\u7840\u3002"}}
{"id": "2505.06493", "pdf": "https://arxiv.org/pdf/2505.06493", "abs": "https://arxiv.org/abs/2505.06493", "authors": ["Jiawei Guo", "Haipeng Cai"], "title": "System Prompt Poisoning: Persistent Attacks on Large Language Models Beyond User Injection", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have gained widespread adoption across diverse\napplications due to their impressive generative capabilities. Their\nplug-and-play nature enables both developers and end users to interact with\nthese models through simple prompts. However, as LLMs become more integrated\ninto various systems in diverse domains, concerns around their security are\ngrowing. Existing studies mainly focus on threats arising from user prompts\n(e.g. prompt injection attack) and model output (e.g. model inversion attack),\nwhile the security of system prompts remains largely overlooked. This work\nbridges the critical gap. We introduce system prompt poisoning, a new attack\nvector against LLMs that, unlike traditional user prompt injection, poisons\nsystem prompts hence persistently impacts all subsequent user interactions and\nmodel responses. We systematically investigate four practical attack strategies\nin various poisoning scenarios. Through demonstration on both generative and\nreasoning LLMs, we show that system prompt poisoning is highly feasible without\nrequiring jailbreak techniques, and effective across a wide range of tasks,\nincluding those in mathematics, coding, logical reasoning, and natural language\nprocessing. Importantly, our findings reveal that the attack remains effective\neven when user prompts employ advanced prompting techniques like\nchain-of-thought (CoT). We also show that such techniques, including CoT and\nretrieval-augmentation-generation (RAG), which are proven to be effective for\nimproving LLM performance in a wide range of tasks, are significantly weakened\nin their effectiveness by system prompt poisoning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u65b0\u578b\u653b\u51fb\u65b9\u5f0f\u2014\u2014\u7cfb\u7edf\u63d0\u793a\u4e2d\u6bd2\uff08system prompt poisoning\uff09\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u7cfb\u7edf\u63d0\u793a\u5b89\u5168\u7684\u7a7a\u767d\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86\u5176\u53ef\u884c\u6027\u548c\u5e7f\u6cdb\u5f71\u54cd\u3002", "motivation": "\u968f\u7740LLMs\u5728\u5404\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5b89\u5168\u6027\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u7528\u6237\u63d0\u793a\u6216\u6a21\u578b\u8f93\u51fa\u76f8\u5173\u7684\u5a01\u80c1\uff0c\u800c\u7cfb\u7edf\u63d0\u793a\u7684\u5b89\u5168\u6027\u88ab\u5ffd\u89c6\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u5173\u952e\u7a7a\u767d\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u7cfb\u7edf\u63d0\u793a\u4e2d\u6bd2\u653b\u51fb\uff0c\u901a\u8fc7\u56db\u79cd\u7b56\u7565\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u6bd2\u5bb3\u7cfb\u7edf\u63d0\u793a\uff0c\u8fdb\u800c\u5f71\u54cd\u6240\u6709\u540e\u7eed\u7528\u6237\u4ea4\u4e92\u548c\u6a21\u578b\u54cd\u5e94\u3002\u5b9e\u9a8c\u5728\u751f\u6210\u548c\u63a8\u7406\u7c7bLLMs\u4e0a\u8fdb\u884c\uff0c\u65e0\u9700\u8d8a\u72f1\u6280\u672f\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u653b\u51fb\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u7cfb\u7edf\u63d0\u793a\u4e2d\u6bd2\u653b\u51fb\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u3001\u903b\u8f91\u63a8\u7406\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7b49\u5e7f\u6cdb\u4efb\u52a1\u4e2d\u5747\u6709\u6548\uff0c\u751a\u81f3\u80fd\u524a\u5f31\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7b49\u5148\u8fdb\u63d0\u793a\u6280\u672f\u7684\u6548\u679c\u3002", "conclusion": "\u7cfb\u7edf\u63d0\u793a\u4e2d\u6bd2\u662f\u4e00\u79cd\u65b0\u578b\u4e14\u5f3a\u6548\u7684\u653b\u51fb\u65b9\u5f0f\uff0c\u63ed\u793a\u4e86LLMs\u7cfb\u7edf\u5c42\u9762\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u547c\u5401\u672a\u6765\u7814\u7a76\u5173\u6ce8\u6b64\u7c7b\u5a01\u80c1\u5e76\u63d0\u51fa\u9632\u5fa1\u63aa\u65bd\u3002"}}
{"id": "2505.07149", "pdf": "https://arxiv.org/pdf/2505.07149", "abs": "https://arxiv.org/abs/2505.07149", "authors": ["Heqing Ren", "Chao Feng", "Alberto Huertas", "Burkhard Stiller"], "title": "AugMixCloak: A Defense against Membership Inference Attacks via Image Transformation", "categories": ["cs.LG"], "comment": null, "summary": "Traditional machine learning (ML) raises serious privacy concerns, while\nfederated learning (FL) mitigates the risk of data leakage by keeping data on\nlocal devices. However, the training process of FL can still leak sensitive\ninformation, which adversaries may exploit to infer private data. One of the\nmost prominent threats is the membership inference attack (MIA), where the\nadversary aims to determine whether a particular data record was part of the\ntraining set.\n  This paper addresses this problem through a two-stage defense called\nAugMixCloak. The core idea is to apply data augmentation and principal\ncomponent analysis (PCA)-based information fusion to query images, which are\ndetected by perceptual hashing (pHash) as either identical to or highly similar\nto images in the training set. Experimental results show that AugMixCloak\nsuccessfully defends against both binary classifier-based MIA and metric-based\nMIA across five datasets and various decentralized FL (DFL) topologies.\nCompared with regularization-based defenses, AugMixCloak demonstrates stronger\nprotection. Compared with confidence score masking, AugMixCloak exhibits better\ngeneralization.", "AI": {"tldr": "AugMixCloak defends against membership inference\u653b\u51fb in federated learning through data augmentation and PCA-based information fusion, showing strong protection and generalization.", "motivation": "To mitigate privacy risks in federated learning caused by membership inference attacks.", "method": "Two-stage defense combining data augmentation and PCA-based information fusion, identified via perceptual hashing.", "result": "Outperforms regularization and confidence masking methods across multiple datasets and topologies.", "conclusion": "AugMixCloak effectively protects privacy in federated learning against membership inference attacks."}}
{"id": "2505.07180", "pdf": "https://arxiv.org/pdf/2505.07180", "abs": "https://arxiv.org/abs/2505.07180", "authors": ["Ruichu Cai", "Kaitao Zheng", "Junxian Huang", "Zijian Li", "Zhengming Chen", "Boyan Xu", "Zhifeng Hao"], "title": "Causal View of Time Series Imputation: Some Identification Results on Missing Mechanism", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Time series imputation is one of the most challenge problems and has broad\napplications in various fields like health care and the Internet of Things.\nExisting methods mainly aim to model the temporally latent dependencies and the\ngeneration process from the observed time series data. In real-world scenarios,\ndifferent types of missing mechanisms, like MAR (Missing At Random), and MNAR\n(Missing Not At Random) can occur in time series data. However, existing\nmethods often overlook the difference among the aforementioned missing\nmechanisms and use a single model for time series imputation, which can easily\nlead to misleading results due to mechanism mismatching. In this paper, we\npropose a framework for time series imputation problem by exploring Different\nMissing Mechanisms (DMM in short) and tailoring solutions accordingly.\nSpecifically, we first analyze the data generation processes with temporal\nlatent states and missing cause variables for different mechanisms.\nSequentially, we model these generation processes via variational inference and\nestimate prior distributions of latent variables via normalizing flow-based\nneural architecture. Furthermore, we establish identifiability results under\nthe nonlinear independent component analysis framework to show that latent\nvariables are identifiable. Experimental results show that our method surpasses\nexisting time series imputation techniques across various datasets with\ndifferent missing mechanisms, demonstrating its effectiveness in real-world\napplications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217\u586b\u8865\u95ee\u9898\u7684\u6846\u67b6\uff08DMM\uff09\uff0c\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u7f3a\u5931\u673a\u5236\uff08\u5982MAR\u548cMNAR\uff09\u5e76\u5b9a\u5236\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u53d8\u5206\u63a8\u7406\u548c\u5f52\u4e00\u5316\u6d41\u67b6\u6784\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5b58\u5728\u591a\u79cd\u7f3a\u5931\u673a\u5236\uff08\u5982MAR\u548cMNAR\uff09\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5e38\u5ffd\u7565\u5176\u5dee\u5f02\uff0c\u91c7\u7528\u5355\u4e00\u6a21\u578b\u5bfc\u81f4\u8bef\u5bfc\u6027\u7ed3\u679c\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u533a\u5206\u5e76\u9002\u914d\u4e0d\u540c\u7f3a\u5931\u673a\u5236\u7684\u6846\u67b6\u3002", "method": "\u6846\u67b6\u9996\u5148\u5206\u6790\u4e0d\u540c\u673a\u5236\u4e0b\u7684\u6570\u636e\u751f\u6210\u8fc7\u7a0b\uff08\u542b\u65f6\u95f4\u6f5c\u5728\u72b6\u6001\u548c\u7f3a\u5931\u539f\u56e0\u53d8\u91cf\uff09\uff0c\u5e76\u901a\u8fc7\u53d8\u5206\u63a8\u7406\u5efa\u6a21\uff0c\u540c\u65f6\u5229\u7528\u5f52\u4e00\u5316\u6d41\u67b6\u6784\u4f30\u8ba1\u6f5c\u5728\u53d8\u91cf\u7684\u5148\u9a8c\u5206\u5e03\u3002\u6b64\u5916\uff0c\u5728\u975e\u7ebf\u6027\u72ec\u7acb\u6210\u5206\u5206\u6790\u6846\u67b6\u4e0b\u8bc1\u660e\u4e86\u6f5c\u5728\u53d8\u91cf\u7684\u53ef\u8bc6\u522b\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u7f3a\u5931\u673a\u5236\u7684\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u586b\u8865\u6280\u672f\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "DMM\u6846\u67b6\u901a\u8fc7\u533a\u5206\u7f3a\u5931\u673a\u5236\u5e76\u5b9a\u5236\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u586b\u8865\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u590d\u6742\u5b9e\u9645\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2505.06503", "pdf": "https://arxiv.org/pdf/2505.06503", "abs": "https://arxiv.org/abs/2505.06503", "authors": ["David Balaban"], "title": "Attention Mechanisms in Dynamical Systems: A Case Study with Predator-Prey Models", "categories": ["math.DS", "cs.AI", "es: 92B05 (Primary), 34C60, 37N25, 68T07, 93B30 (Secondary)"], "comment": "5 figures, 12 pages, python code included", "summary": "Attention mechanisms are widely used in artificial intelligence to enhance\nperformance and interpretability. In this paper, we investigate their utility\nin modeling classical dynamical systems -- specifically, a noisy predator-prey\n(Lotka-Volterra) system. We train a simple linear attention model on perturbed\ntime-series data to reconstruct system trajectories. Remarkably, the learned\nattention weights align with the geometric structure of the Lyapunov function:\nhigh attention corresponds to flat regions (where perturbations have small\neffect), and low attention aligns with steep regions (where perturbations have\nlarge effect). We further demonstrate that attention-based weighting can serve\nas a proxy for sensitivity analysis, capturing key phase-space properties\nwithout explicit knowledge of the system equations. These results suggest a\nnovel use of AI-derived attention for interpretable, data-driven analysis and\ncontrol of nonlinear systems. For example our framework could support future\nwork in biological modeling of circadian rhythms, and interpretable machine\nlearning for dynamical environments.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6ce8\u610f\u529b\u673a\u5236\u5728\u6a21\u62df\u7ecf\u5178\u52a8\u529b\u7cfb\u7edf\uff08\u5982Lotka-Volterra\u7cfb\u7edf\uff09\u4e2d\u7684\u6548\u7528\uff0c\u53d1\u73b0\u6ce8\u610f\u529b\u6743\u91cd\u4e0eLyapunov\u51fd\u6570\u7684\u51e0\u4f55\u7ed3\u6784\u4e00\u81f4\uff0c\u5e76\u53ef\u7528\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u975e\u7ebf\u6027\u7cfb\u7edf\u5206\u6790\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u5bf9\u7ecf\u5178\u52a8\u529b\u7cfb\u7edf\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u9a71\u52a8\u7684\u975e\u7ebf\u6027\u7cfb\u7edf\u5206\u6790\u548c\u63a7\u5236\u4e2d\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u5728\u53d7\u6270\u52a8\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e0a\u8bad\u7ec3\u7b80\u5355\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u91cd\u5efa\u7cfb\u7edf\u8f68\u8ff9\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5b66\u4e60\u7684\u6ce8\u610f\u529b\u6743\u91cd\u4e0eLyapunov\u51fd\u6570\u7684\u51e0\u4f55\u7ed3\u6784\u4e00\u81f4\uff0c\u9ad8\u6ce8\u610f\u529b\u5bf9\u5e94\u5e73\u5766\u533a\u57df\uff08\u6270\u52a8\u5f71\u54cd\u5c0f\uff09\uff0c\u4f4e\u6ce8\u610f\u529b\u5bf9\u5e94\u9661\u5ced\u533a\u57df\uff08\u6270\u52a8\u5f71\u54cd\u5927\uff09\u3002", "conclusion": "\u7ed3\u8bba\u662f\u6ce8\u610f\u529b\u673a\u5236\u53ef\u4f5c\u4e3a\u7075\u654f\u5ea6\u5206\u6790\u7684\u4ee3\u7406\uff0c\u65e0\u9700\u7cfb\u7edf\u65b9\u7a0b\u7684\u663e\u5f0f\u77e5\u8bc6\u5373\u53ef\u6355\u83b7\u5173\u952e\u76f8\u7a7a\u95f4\u7279\u6027\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u975e\u7ebf\u6027\u7cfb\u7edf\u5206\u6790\u548c\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2505.07222", "pdf": "https://arxiv.org/pdf/2505.07222", "abs": "https://arxiv.org/abs/2505.07222", "authors": ["Nima Dehghani"], "title": "Compression, Regularity, Randomness and Emergent Structure: Rethinking Physical Complexity in the Data-Driven Era", "categories": ["cs.LG", "cond-mat.stat-mech", "cs.IT", "math.IT", "physics.bio-ph", "physics.data-an"], "comment": null, "summary": "Complexity science offers a wide range of measures for quantifying\nunpredictability, structure, and information. Yet, a systematic conceptual\norganization of these measures is still missing.\n  We present a unified framework that locates statistical, algorithmic, and\ndynamical measures along three axes (regularity, randomness, and complexity)\nand situates them in a common conceptual space. We map statistical,\nalgorithmic, and dynamical measures into this conceptual space, discussing\ntheir computational accessibility and approximability.\n  This taxonomy reveals the deep challenges posed by uncomputability and\nhighlights the emergence of modern data-driven methods (including autoencoders,\nlatent dynamical models, symbolic regression, and physics-informed neural\nnetworks) as pragmatic approximations to classical complexity ideals. Latent\nspaces emerge as operational arenas where regularity extraction, noise\nmanagement, and structured compression converge, bridging theoretical\nfoundations with practical modeling in high-dimensional systems.\n  We close by outlining implications for physics-informed AI and AI-guided\ndiscovery in complex physical systems, arguing that classical questions of\ncomplexity remain central to next-generation scientific modeling.", "AI": {"tldr": "\u6458\u8981\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u7edf\u8ba1\u3001\u7b97\u6cd5\u548c\u52a8\u6001\u5ea6\u91cf\u7ec4\u7ec7\u5230\u6982\u5ff5\u7a7a\u95f4\u7684\u4e09\u8f74\uff08\u89c4\u5f8b\u6027\u3001\u968f\u673a\u6027\u548c\u590d\u6742\u6027\uff09\u4e0a\uff0c\u5e76\u8ba8\u8bba\u4e86\u73b0\u4ee3\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u903c\u8fd1\u7ecf\u5178\u590d\u6742\u6027\u7406\u8bba\u65f6\u7684\u5b9e\u7528\u6027\u3002\u6700\u540e\uff0c\u5f3a\u8c03\u4e86\u590d\u6742\u6027\u7406\u8bba\u5bf9\u4e0b\u4e00\u4ee3\u79d1\u5b66\u5efa\u6a21\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5f53\u524d\u590d\u6742\u6027\u79d1\u5b66\u4e2d\u7f3a\u4e4f\u5bf9\u7edf\u8ba1\u3001\u7b97\u6cd5\u548c\u52a8\u6001\u5ea6\u91cf\u7684\u7cfb\u7edf\u6027\u6982\u5ff5\u7ec4\u7ec7\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u7edf\u4e00\u6846\u67b6\u4ee5\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u8f74\uff08\u89c4\u5f8b\u6027\u3001\u968f\u673a\u6027\u3001\u590d\u6742\u6027\uff09\u7684\u6982\u5ff5\u7a7a\u95f4\u6846\u67b6\uff0c\u5c06\u7edf\u8ba1\u3001\u7b97\u6cd5\u548c\u52a8\u6001\u5ea6\u91cf\u6620\u5c04\u5230\u5176\u4e2d\uff0c\u5e76\u5206\u6790\u5b83\u4eec\u7684\u8ba1\u7b97\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u903c\u8fd1\u6027\u3002\u540c\u65f6\u63a2\u8ba8\u4e86\u73b0\u4ee3\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff08\u5982\u81ea\u7f16\u7801\u5668\u3001\u6f5c\u5728\u52a8\u6001\u6a21\u578b\u7b49\uff09\u4f5c\u4e3a\u7ecf\u5178\u590d\u6742\u6027\u7406\u8bba\u7684\u5b9e\u7528\u903c\u8fd1\u5de5\u5177\u3002", "result": "\u8be5\u6846\u67b6\u63ed\u793a\u4e86\u4e0d\u53ef\u8ba1\u7b97\u6027\u5e26\u6765\u7684\u6df1\u5c42\u6311\u6218\uff0c\u5e76\u5c55\u793a\u4e86\u6f5c\u5728\u7a7a\u95f4\u5982\u4f55\u6210\u4e3a\u7406\u8bba\u5efa\u6a21\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u6865\u6881\u3002", "conclusion": "\u590d\u6742\u6027\u7406\u8bba\u4ecd\u662f\u4e0b\u4e00\u4ee3\u79d1\u5b66\u5efa\u6a21\u7684\u6838\u5fc3\uff0c\u6846\u67b6\u4e3a\u7269\u7406\u9a71\u52a8\u7684AI\u548cAI\u5f15\u5bfc\u7684\u590d\u6742\u7cfb\u7edf\u53d1\u73b0\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6491\u3002"}}
{"id": "2505.07245", "pdf": "https://arxiv.org/pdf/2505.07245", "abs": "https://arxiv.org/abs/2505.07245", "authors": ["Fei Liu", "Huanhuan Ren", "Yu Guan", "Xiuxu Wang", "Wang Lv", "Zhiqiang Hu", "Yaxi Chen"], "title": "REMEDI: Relative Feature Enhanced Meta-Learning with Distillation for Imbalanced Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Predicting future vehicle purchases among existing owners presents a critical\nchallenge due to extreme class imbalance (<0.5% positive rate) and complex\nbehavioral patterns. We propose REMEDI (Relative feature Enhanced Meta-learning\nwith Distillation for Imbalanced prediction), a novel multi-stage framework\naddressing these challenges. REMEDI first trains diverse base models to capture\ncomplementary aspects of user behavior. Second, inspired by comparative\nop-timization techniques, we introduce relative performance meta-features\n(deviation from ensemble mean, rank among peers) for effective model fusion\nthrough a hybrid-expert architecture. Third, we distill the ensemble's\nknowledge into a single efficient model via supervised fine-tuning with MSE\nloss, enabling practical deployment. Evaluated on approximately 800,000 vehicle\nowners, REMEDI significantly outperforms baseline approaches, achieving the\nbusiness target of identifying ~50% of actual buyers within the top 60,000\nrecommendations at ~10% precision. The distilled model preserves the ensemble's\npredictive power while maintaining deployment efficiency, demonstrating\nREMEDI's effectiveness for imbalanced prediction in industry settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faREMEDI\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u65b9\u6cd5\u89e3\u51b3\u6781\u7aef\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u8f66\u8f86\u8d2d\u4e70\u9884\u6d4b\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u9884\u6d4b\u73b0\u6709\u8f66\u4e3b\u7684\u672a\u6765\u8d2d\u4e70\u884c\u4e3a\u56e0\u6781\u4f4e\u7684\u9633\u6027\u7387(<0.5%)\u548c\u590d\u6742\u884c\u4e3a\u6a21\u5f0f\u800c\u5177\u6311\u6218\u6027\u3002", "method": "REMEDI\u91c7\u7528\u4e09\u9636\u6bb5\u6846\u67b6\uff1a\u8bad\u7ec3\u591a\u6837\u5316\u57fa\u7840\u6a21\u578b\u3001\u5f15\u5165\u76f8\u5bf9\u6027\u80fd\u5143\u7279\u5f81\u8fdb\u884c\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u878d\u5408\u3001\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u5c06\u96c6\u6210\u77e5\u8bc6\u84b8\u998f\u4e3a\u9ad8\u6548\u5355\u6a21\u578b\u3002", "result": "\u572880\u4e07\u8f66\u4e3b\u6570\u636e\u4e0a\uff0cREMEDI\u57286\u4e07\u63a8\u8350\u4e2d\u8bc6\u522b\u51fa50%\u5b9e\u9645\u4e70\u5bb6\uff0c\u7cbe\u5ea6\u7ea610%\uff0c\u84b8\u998f\u6a21\u578b\u4fdd\u6301\u96c6\u6210\u6027\u80fd\u4e14\u90e8\u7f72\u9ad8\u6548\u3002", "conclusion": "REMEDI\u5728\u5de5\u4e1a\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u89e3\u51b3\u4e0d\u5e73\u8861\u9884\u6d4b\u95ee\u9898\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.06527", "pdf": "https://arxiv.org/pdf/2505.06527", "abs": "https://arxiv.org/abs/2505.06527", "authors": ["Jing Hu", "Kaiwei Yu", "Hongjiang Xian", "Shu Hu", "Xin Wang"], "title": "Improving Generalization of Medical Image Registration Foundation Model", "categories": ["cs.CV", "cs.AI"], "comment": "IJCNN", "summary": "Deformable registration is a fundamental task in medical image processing,\naiming to achieve precise alignment by establishing nonlinear correspondences\nbetween images. Traditional methods offer good adaptability and\ninterpretability but are limited by computational efficiency. Although deep\nlearning approaches have significantly improved registration speed and\naccuracy, they often lack flexibility and generalizability across different\ndatasets and tasks. In recent years, foundation models have emerged as a\npromising direction, leveraging large and diverse datasets to learn universal\nfeatures and transformation patterns for image registration, thus demonstrating\nstrong cross-task transferability. However, these models still face challenges\nin generalization and robustness when encountering novel anatomical structures,\nvarying imaging conditions, or unseen modalities. To address these limitations,\nthis paper incorporates Sharpness-Aware Minimization (SAM) into foundation\nmodels to enhance their generalization and robustness in medical image\nregistration. By optimizing the flatness of the loss landscape, SAM improves\nmodel stability across diverse data distributions and strengthens its ability\nto handle complex clinical scenarios. Experimental results show that foundation\nmodels integrated with SAM achieve significant improvements in cross-dataset\nregistration performance, offering new insights for the advancement of medical\nimage registration technology. Our code is available at\nhttps://github.com/Promise13/fm_sam}{https://github.com/Promise13/fm\\_sam.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06SAM\u96c6\u6210\u5230\u57fa\u7840\u6a21\u578b\u4e2d\u4ee5\u63d0\u9ad8\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8de8\u6570\u636e\u96c6\u914d\u51c6\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u53ef\u53d8\u5f62\u914d\u51c6\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u4f4e\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u6cdb\u5316\u6027\uff0c\u57fa\u7840\u6a21\u578b\u867d\u5177\u6f5c\u529b\u4f46\u4ecd\u9762\u4e34\u6cdb\u5316\u548c\u9c81\u68d2\u6027\u6311\u6218\u3002", "method": "\u7ed3\u5408Sharpness-Aware Minimization\uff08SAM\uff09\u4f18\u5316\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5b66\u4e60\u5e73\u5766\u7684\u635f\u5931\u666f\u89c2\u63d0\u5347\u6a21\u578b\u7a33\u5b9a\u6027\u548c\u5904\u7406\u590d\u6742\u4e34\u5e8a\u573a\u666f\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u96c6\u6210SAM\u7684\u57fa\u7840\u6a21\u578b\u5728\u8de8\u6570\u636e\u96c6\u914d\u51c6\u6027\u80fd\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u6280\u672f\u7684\u8fdb\u6b65\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.07260", "pdf": "https://arxiv.org/pdf/2505.07260", "abs": "https://arxiv.org/abs/2505.07260", "authors": ["Yuanhang Yang", "Chaozheng Wang", "Jing Li"], "title": "UMoE: Unifying Attention and FFN with Shared Experts", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Sparse Mixture of Experts (MoE) architectures have emerged as a promising\napproach for scaling Transformer models. While initial works primarily\nincorporated MoE into feed-forward network (FFN) layers, recent studies have\nexplored extending the MoE paradigm to attention layers to enhance model\nperformance. However, existing attention-based MoE layers require specialized\nimplementations and demonstrate suboptimal performance compared to their\nFFN-based counterparts. In this paper, we aim to unify the MoE designs in\nattention and FFN layers by introducing a novel reformulation of the attention\nmechanism, revealing an underlying FFN-like structure within attention modules.\nOur proposed architecture, UMoE, achieves superior performance through\nattention-based MoE layers while enabling efficient parameter sharing between\nFFN and attention components.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6ce8\u610f\u529b\u5c42\u548c\u524d\u9988\u7f51\u7edc\u5c42\u7684Sparse Mixture of Experts (MoE)\u8bbe\u8ba1\uff0c\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63ed\u793a\u4e86\u5176\u5185\u5728\u7684FFN\u7ed3\u6784\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6ce8\u610f\u529b\u5c42MoE\u8bbe\u8ba1\u6027\u80fd\u4e0d\u4f73\u4e14\u5b9e\u73b0\u590d\u6742\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7edf\u4e00MoE\u67b6\u6784\uff0c\u63d0\u5347\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51faUMoE\u67b6\u6784\uff0c\u91cd\u65b0\u5b9a\u4e49\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63ed\u793a\u5176FFN\u7ed3\u6784\uff0c\u5b9e\u73b0\u53c2\u6570\u5171\u4eab\u3002", "result": "UMoE\u5728\u6ce8\u610f\u529b\u5c42MoE\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u7edf\u4e00MoE\u8bbe\u8ba1\u80fd\u591f\u9ad8\u6548\u63d0\u5347\u6027\u80fd\uff0c\u4e3a\u6a21\u578b\u6269\u5c55\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.06536", "pdf": "https://arxiv.org/pdf/2505.06536", "abs": "https://arxiv.org/abs/2505.06536", "authors": ["Feng Liu", "Ziwang Fu", "Yunlong Wang", "Qijian Zheng"], "title": "TACFN: Transformer-based Adaptive Cross-modal Fusion Network for Multimodal Emotion Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2111.02172", "summary": "The fusion technique is the key to the multimodal emotion recognition task.\nRecently, cross-modal attention-based fusion methods have demonstrated high\nperformance and strong robustness. However, cross-modal attention suffers from\nredundant features and does not capture complementary features well. We find\nthat it is not necessary to use the entire information of one modality to\nreinforce the other during cross-modal interaction, and the features that can\nreinforce a modality may contain only a part of it. To this end, we design an\ninnovative Transformer-based Adaptive Cross-modal Fusion Network (TACFN).\nSpecifically, for the redundant features, we make one modality perform\nintra-modal feature selection through a self-attention mechanism, so that the\nselected features can adaptively and efficiently interact with another\nmodality. To better capture the complementary information between the\nmodalities, we obtain the fused weight vector by splicing and use the weight\nvector to achieve feature reinforcement of the modalities. We apply TCAFN to\nthe RAVDESS and IEMOCAP datasets. For fair comparison, we use the same unimodal\nrepresentations to validate the effectiveness of the proposed fusion method.\nThe experimental results show that TACFN brings a significant performance\nimprovement compared to other methods and reaches the state-of-the-art. All\ncode and models could be accessed from https://github.com/shuzihuaiyu/TACFN.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u81ea\u9002\u5e94\u8de8\u6a21\u6001\u878d\u5408\u7f51\u7edc\uff08TACFN\uff09\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u9009\u62e9\u5173\u952e\u7279\u5f81\uff0c\u5e76\u5229\u7528\u6743\u91cd\u5411\u91cf\u589e\u5f3a\u6a21\u6001\u95f4\u7684\u4e92\u8865\u4fe1\u606f\uff0c\u5728RAVDESS\u548cIEMOCAP\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u878d\u5408\u65b9\u6cd5\u5b58\u5728\u7279\u5f81\u5197\u4f59\u548c\u4e92\u8865\u7279\u5f81\u6355\u6349\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6a21\u6001\u95f4\u7684\u4fe1\u606f\u4ea4\u4e92\u4e0d\u9700\u8981\u5b8c\u5168\u4f9d\u8d56\u6574\u4e2a\u6a21\u6001\u7684\u4fe1\u606f\uff0c\u90e8\u5206\u7279\u5f81\u5373\u53ef\u5b9e\u73b0\u6709\u6548\u589e\u5f3a\u3002", "method": "\u8bbe\u8ba1\u4e86TACFN\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u6a21\u6001\u5185\u7279\u5f81\u9009\u62e9\uff0c\u5e76\u62fc\u63a5\u6743\u91cd\u5411\u91cf\u4ee5\u589e\u5f3a\u6a21\u6001\u95f4\u7684\u4e92\u8865\u4fe1\u606f\u3002", "result": "\u5728RAVDESS\u548cIEMOCAP\u6570\u636e\u96c6\u4e0a\uff0cTACFN\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u8fbe\u5230\u4e86\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "TACFN\u901a\u8fc7\u81ea\u9002\u5e94\u7279\u5f81\u9009\u62e9\u548c\u4e92\u8865\u4fe1\u606f\u878d\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.07274", "pdf": "https://arxiv.org/pdf/2505.07274", "abs": "https://arxiv.org/abs/2505.07274", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "Cache-Efficient Posterior Sampling for Reinforcement Learning with LLM-Derived Priors Across Discrete and Continuous Domains", "categories": ["cs.LG"], "comment": null, "summary": "Integrating large language models (LLMs) as priors in reinforcement learning\n(RL) offers significant advantages but comes with substantial computational\ncosts. We present a principled cache-efficient framework for posterior sampling\nwith LLM-derived priors that dramatically reduces these costs while maintaining\nhigh performance. At the core of our approach is an adaptive caching mechanism,\nwhere cache parameters are meta-optimized using surrogate gradients derived\nfrom policy performance. This design enables efficient inference across both\ndiscrete text environments (e.g., TextWorld, ALFWorld) and continuous control\ndomains (e.g., MuJoCo), achieving a 3.8--4.7$\\times$ reduction in LLM queries\nand 4.0--12.0$\\times$ lower median latencies (85--93\\,ms on a consumer GPU)\nwhile retaining 96--98\\% of uncached performance. Our theoretical analysis\nprovides KL divergence bounds on approximation quality, validated empirically.\nThe framework extends to offline RL, where our CQL-Prior variant improves\nperformance by 14--29\\% and reduces training time by 38--40\\%. Extensive\nevaluations across a diverse suite of eight tasks demonstrate the\ngeneralizability and practical viability of LLM-guided RL in\nresource-constrained settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u5148\u9a8c\u7684\u9ad8\u6548\u7f13\u5b58\u6846\u67b6\uff0c\u663e\u8457\u964d\u4f4eRL\u4e2d\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u5728\u79bb\u6563\u548c\u8fde\u7eed\u73af\u5883\u4e2d\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3LLM\u4f5c\u4e3aRL\u5148\u9a8c\u65f6\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u7f13\u5b58\u673a\u5236\uff0c\u901a\u8fc7\u4ee3\u7406\u68af\u5ea6\u5143\u4f18\u5316\u7f13\u5b58\u53c2\u6570\uff0c\u9002\u5e94\u4e0d\u540c\u73af\u5883\u3002", "result": "\u5728TextWorld\u3001ALFWorld\u548cMuJoCo\u7b49\u73af\u5883\u4e2d\uff0cLLM\u67e5\u8be2\u51cf\u5c113.8-4.7\u500d\uff0c\u5ef6\u8fdf\u964d\u4f4e4.0-12.0\u500d\uff0c\u6027\u80fd\u4fdd\u755996-98%\u3002\u79bb\u7ebfRL\u4e2d\uff0cCQL-Prior\u53d8\u4f53\u6027\u80fd\u63d0\u534714-29%\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1138-40%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u5c55\u793a\u4e86LLM\u5f15\u5bfcRL\u7684\u901a\u7528\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.06537", "pdf": "https://arxiv.org/pdf/2505.06537", "abs": "https://arxiv.org/abs/2505.06537", "authors": ["Xianghao Kong", "Qiaosong Qi", "Yuanbin Wang", "Anyi Rao", "Biaolong Chen", "Aixi Zhang", "Si Liu", "Hao Jiang"], "title": "ProFashion: Prototype-guided Fashion Video Generation with Multiple Reference Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Fashion video generation aims to synthesize temporally consistent videos from\nreference images of a designated character. Despite significant progress,\nexisting diffusion-based methods only support a single reference image as\ninput, severely limiting their capability to generate view-consistent fashion\nvideos, especially when there are different patterns on the clothes from\ndifferent perspectives. Moreover, the widely adopted motion module does not\nsufficiently model human body movement, leading to sub-optimal spatiotemporal\nconsistency. To address these issues, we propose ProFashion, a fashion video\ngeneration framework leveraging multiple reference images to achieve improved\nview consistency and temporal coherency. To effectively leverage features from\nmultiple reference images while maintaining a reasonable computational cost, we\ndevise a Pose-aware Prototype Aggregator, which selects and aggregates global\nand fine-grained reference features according to pose information to form\nframe-wise prototypes, which serve as guidance in the denoising process. To\nfurther enhance motion consistency, we introduce a Flow-enhanced Prototype\nInstantiator, which exploits the human keypoint motion flow to guide an extra\nspatiotemporal attention process in the denoiser. To demonstrate the\neffectiveness of ProFashion, we extensively evaluate our method on the\nMRFashion-7K dataset we collected from the Internet. ProFashion also\noutperforms previous methods on the UBC Fashion dataset.", "AI": {"tldr": "ProFashion\u662f\u4e00\u79cd\u5229\u7528\u591a\u53c2\u8003\u56fe\u50cf\u63d0\u5347\u89c6\u56fe\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u65f6\u5c1a\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u59ff\u6001\u611f\u77e5\u539f\u578b\u805a\u5408\u5668\u548c\u6d41\u589e\u5f3a\u539f\u578b\u5b9e\u4f8b\u5316\u5668\u6539\u8fdb\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u4ec5\u652f\u6301\u5355\u53c2\u8003\u56fe\u50cf\u8f93\u5165\uff0c\u9650\u5236\u4e86\u751f\u6210\u89c6\u89d2\u4e00\u81f4\u7684\u65f6\u5c1a\u89c6\u9891\u7684\u80fd\u529b\uff0c\u4e14\u8fd0\u52a8\u6a21\u5757\u5bf9\u4eba\u4f53\u8fd0\u52a8\u7684\u5efa\u6a21\u4e0d\u8db3\u3002", "method": "\u63d0\u51faProFashion\u6846\u67b6\uff0c\u5305\u542b\u59ff\u6001\u611f\u77e5\u539f\u578b\u805a\u5408\u5668\uff08\u6839\u636e\u59ff\u6001\u9009\u62e9\u5e76\u805a\u5408\u591a\u53c2\u8003\u56fe\u50cf\u7279\u5f81\uff09\u548c\u6d41\u589e\u5f3a\u539f\u578b\u5b9e\u4f8b\u5316\u5668\uff08\u5229\u7528\u4eba\u4f53\u5173\u952e\u70b9\u8fd0\u52a8\u6d41\u5f15\u5bfc\u65f6\u7a7a\u6ce8\u610f\u529b\uff09\u3002", "result": "\u5728\u81ea\u5efaMRFashion-7K\u6570\u636e\u96c6\u548cUBC Fashion\u6570\u636e\u96c6\u4e0a\uff0cProFashion\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ProFashion\u901a\u8fc7\u591a\u53c2\u8003\u56fe\u50cf\u548c\u8fd0\u52a8\u6d41\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u5c1a\u89c6\u9891\u751f\u6210\u7684\u89c6\u56fe\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002"}}
{"id": "2505.07291", "pdf": "https://arxiv.org/pdf/2505.07291", "abs": "https://arxiv.org/abs/2505.07291", "authors": ["Prime Intellect Team", "Sami Jaghouar", "Justus Mattern", "Jack Min Ong", "Jannik Straube", "Manveer Basra", "Aaron Pazdera", "Kushal Thaman", "Matthew Di Ferrante", "Felix Gabriel", "Fares Obeid", "Kemal Erdem", "Michael Keiblinger", "Johannes Hagemann"], "title": "INTELLECT-2: A Reasoning Model Trained Through Globally Decentralized Reinforcement Learning", "categories": ["cs.LG", "cs.DC"], "comment": "26 pages, 12 figures", "summary": "We introduce INTELLECT-2, the first globally distributed reinforcement\nlearning (RL) training run of a 32 billion parameter language model. Unlike\ntraditional centralized training efforts, INTELLECT-2 trains a reasoning model\nusing fully asynchronous RL across a dynamic, heterogeneous swarm of\npermissionless compute contributors.\n  To enable a training run with this unique infrastructure, we built various\ncomponents from scratch: we introduce PRIME-RL, our training framework\npurpose-built for distributed asynchronous reinforcement learning, based on top\nof novel components such as TOPLOC, which verifies rollouts from untrusted\ninference workers, and SHARDCAST, which efficiently broadcasts policy weights\nfrom training nodes to inference workers.\n  Beyond infrastructure components, we propose modifications to the standard\nGRPO training recipe and data filtering techniques that were crucial to achieve\ntraining stability and ensure that our model successfully learned its training\nobjective, thus improving upon QwQ-32B, the state of the art reasoning model in\nthe 32B parameter range.\n  We open-source INTELLECT-2 along with all of our code and data, hoping to\nencourage and enable more open research in the field of decentralized training.", "AI": {"tldr": "INTELLECT-2\u662f\u9996\u4e2a\u901a\u8fc7\u5168\u7403\u5206\u5e03\u5f0f\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8bad\u7ec3\u7684320\u4ebf\u53c2\u6570\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u4e86\u5b8c\u5168\u5f02\u6b65RL\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u63a8\u52a8\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\u7814\u7a76\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u7684\u5c40\u9650\u6027\uff0c\u7814\u7a76\u8005\u65e8\u5728\u5229\u7528\u53bb\u4e2d\u5fc3\u5316\u5f02\u6784\u8ba1\u7b97\u8d44\u6e90\uff0c\u6784\u5efa\u66f4\u9ad8\u6548\u3001\u5f00\u653e\u7684\u8bad\u7ec3\u6846\u67b6\u3002", "method": "\u5f00\u53d1\u4e86PRIME-RL\u8bad\u7ec3\u6846\u67b6\uff0c\u5305\u542bTOPLOC\uff08\u9a8c\u8bc1\u975e\u53ef\u4fe1\u63a8\u7406\u8282\u70b9\u7684rollout\uff09\u548cSHARDCAST\uff08\u9ad8\u6548\u5e7f\u64ad\u7b56\u7565\u6743\u91cd\uff09\uff0c\u5e76\u5bf9\u6807\u51c6GRPO\u8bad\u7ec3\u65b9\u6cd5\u53ca\u6570\u636e\u8fc7\u6ee4\u6280\u672f\u8fdb\u884c\u4e86\u6539\u8fdb\u3002", "result": "\u6210\u529f\u8bad\u7ec3\u4e86INTELLECT-2\u6a21\u578b\uff0c\u8d85\u8d8a\u4e86\u5f53\u524d320\u4ebf\u53c2\u6570\u8303\u56f4\u5185\u6700\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578bQwQ-32B\u3002", "conclusion": "INTELLECT-2\u53ca\u914d\u5957\u5de5\u5177\u7684\u5f00\u6e90\u4e3a\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u5f00\u653e\u53d1\u5c55\u3002"}}
{"id": "2505.07303", "pdf": "https://arxiv.org/pdf/2505.07303", "abs": "https://arxiv.org/abs/2505.07303", "authors": ["Bianca Marin Moreno", "Khaled Eldowa", "Pierre Gaillard", "Margaux Br\u00e9g\u00e8re", "Nadia Oudjane"], "title": "Online Episodic Convex Reinforcement Learning", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We study online learning in episodic finite-horizon Markov decision processes\n(MDPs) with convex objective functions, known as the concave utility\nreinforcement learning (CURL) problem. This setting generalizes RL from linear\nto convex losses on the state-action distribution induced by the agent's\npolicy. The non-linearity of CURL invalidates classical Bellman equations and\nrequires new algorithmic approaches. We introduce the first algorithm achieving\nnear-optimal regret bounds for online CURL without any prior knowledge on the\ntransition function. To achieve this, we use an online mirror descent algorithm\nwith varying constraint sets and a carefully designed exploration bonus. We\nthen address for the first time a bandit version of CURL, where the only\nfeedback is the value of the objective function on the state-action\ndistribution induced by the agent's policy. We achieve a sub-linear regret\nbound for this more challenging problem by adapting techniques from bandit\nconvex optimization to the MDP setting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5177\u6709\u51f8\u76ee\u6807\u51fd\u6570\u7684\u6709\u9650\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u5728\u7ebf\u5b66\u4e60\u7684\u95ee\u9898\uff08CURL\u95ee\u9898\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u7684\u7b97\u6cd5\uff0c\u5e76\u5728\u66f4\u590d\u6742\u7684\u201c\u4ec5\u53cd\u9988\u76ee\u6807\u51fd\u6570\u503c\u201d\u7684bandit\u8bbe\u5b9a\u4e0b\u5b9e\u73b0\u4e86\u6b21\u7ebf\u6027\u9057\u61be\u3002", "motivation": "\u7ecf\u5178\u7684\u5f3a\u5316\u5b66\u4e60\u901a\u5e38\u5047\u8bbe\u7ebf\u6027\u635f\u5931\uff0c\u800c\u73b0\u5b9e\u95ee\u9898\u5f80\u5f80\u6d89\u53ca\u66f4\u590d\u6742\u7684\u975e\u7ebf\u6027\u76ee\u6807\uff08\u5373\u51f8\u76ee\u6807\u51fd\u6570\uff09\u3002\u8fd9\u79cd\u975e\u7ebf\u6027\u4f7f\u5f97\u4f20\u7edf\u7684\u65b9\u6cd5\uff08\u5982\u8d1d\u5c14\u66fc\u65b9\u7a0b\uff09\u5931\u6548\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65b0\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5728\u7ebf\u7684\u955c\u50cf\u4e0b\u964d\u7b97\u6cd5\uff0c\u7ed3\u5408\u52a8\u6001\u7ea6\u675f\u96c6\u548c\u4e13\u95e8\u8bbe\u8ba1\u7684\u63a2\u7d22\u5956\u52b1\uff0c\u6765\u89e3\u51b3CURL\u95ee\u9898\u3002\u5728bandit\u8bbe\u5b9a\u4e0b\uff0c\u5219\u901a\u8fc7\u5c06bandit\u51f8\u4f18\u5316\u7684\u6280\u672f\u8fc1\u79fb\u5230MDP\u73af\u5883\u4e2d\u6765\u5b9e\u73b0\u3002", "result": "\u8bba\u6587\u7684\u7b97\u6cd5\u5728\u6807\u51c6CURL\u8bbe\u5b9a\u4e0b\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u6700\u4f18\u7684\u9057\u61be\u754c\uff1b\u800c\u5728bandit\u8bbe\u5b9a\u4e0b\uff0c\u5c3d\u7ba1\u662f\u66f4\u56f0\u96be\u7684\u95ee\u9898\uff0c\u4ecd\u7136\u83b7\u5f97\u4e86\u6b21\u7ebf\u6027\u7684\u9057\u61be\u754c\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5728\u975e\u7ebf\u6027\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u4e2d\u5b9e\u73b0\u9ad8\u6548\u5728\u7ebf\u5b66\u4e60\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5904\u7406\u66f4\u590d\u6742\u7684\u5f3a\u5316\u5b66\u4e60\u573a\u666f\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u548c\u65b0\u5de5\u5177\u3002"}}
{"id": "2505.06561", "pdf": "https://arxiv.org/pdf/2505.06561", "abs": "https://arxiv.org/abs/2505.06561", "authors": ["Danil Belov", "Artem Erkhov", "Elizaveta Pestova", "Ilya Osokin", "Dzmitry Tsetserukou", "Pavel Osinenko"], "title": "Quadrupedal Robot Skateboard Mounting via Reverse Curriculum Learning", "categories": ["cs.RO", "cs.AI", "math.OC"], "comment": null, "summary": "The aim of this work is to enable quadrupedal robots to mount skateboards\nusing Reverse Curriculum Reinforcement Learning. Although prior work has\ndemonstrated skateboarding for quadrupeds that are already positioned on the\nboard, the initial mounting phase still poses a significant challenge. A\ngoal-oriented methodology was adopted, beginning with the terminal phases of\nthe task and progressively increasing the complexity of the problem definition\nto approximate the desired objective. The learning process was initiated with\nthe skateboard rigidly fixed within the global coordinate frame and the robot\npositioned directly above it. Through gradual relaxation of these initial\nconditions, the learned policy demonstrated robustness to variations in\nskateboard position and orientation, ultimately exhibiting a successful\ntransfer to scenarios involving a mobile skateboard. The code, trained models,\nand reproducible examples are available at the following link:\nhttps://github.com/dancher00/quadruped-skateboard-mounting", "AI": {"tldr": "\u4f7f\u7528\u9006\u5411\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u8ba9\u56db\u8db3\u673a\u5668\u4eba\u5b66\u4f1a\u6ed1\u677f\u7ad9\u7acb\uff0c\u89e3\u51b3\u4e86\u521d\u59cb\u7ad9\u7acb\u9636\u6bb5\u7684\u6311\u6218\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u7814\u7a76\u5c55\u793a\u56db\u8db3\u673a\u5668\u4eba\u5728\u6ed1\u677f\u4e0a\u7684\u52a8\u4f5c\uff0c\u4f46\u521d\u59cb\u7ad9\u7acb\u9636\u6bb5\u4ecd\u662f\u96be\u70b9\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u76ee\u6807\u5bfc\u5411\u65b9\u6cd5\uff0c\u4ece\u4efb\u52a1\u6700\u7ec8\u9636\u6bb5\u5f00\u59cb\uff0c\u9010\u6b65\u589e\u52a0\u95ee\u9898\u590d\u6742\u6027\uff0c\u521d\u59cb\u9636\u6bb5\u56fa\u5b9a\u6ed1\u677f\u5e76\u9010\u6b65\u653e\u677e\u6761\u4ef6\u3002", "result": "\u5b66\u4e60\u7b56\u7565\u5728\u6ed1\u677f\u4f4d\u7f6e\u548c\u65b9\u5411\u53d8\u5316\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u5e76\u6210\u529f\u8fc1\u79fb\u5230\u79fb\u52a8\u6ed1\u677f\u573a\u666f\u3002", "conclusion": "\u901a\u8fc7\u9006\u5411\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\uff0c\u56db\u8db3\u673a\u5668\u4eba\u6210\u529f\u5b66\u4f1a\u7ad9\u7acb\u6ed1\u677f\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.07309", "pdf": "https://arxiv.org/pdf/2505.07309", "abs": "https://arxiv.org/abs/2505.07309", "authors": ["Pei-Fu Guo", "Yun-Da Tsai", "Shou-De Lin"], "title": "Uncertainty Profiles for LLMs: Uncertainty Source Decomposition and Adaptive Model-Metric Selection", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) often generate fluent but factually incorrect\noutputs, known as hallucinations, which undermine their reliability in\nreal-world applications. While uncertainty estimation has emerged as a\npromising strategy for detecting such errors, current metrics offer limited\ninterpretability and lack clarity about the types of uncertainty they capture.\nIn this paper, we present a systematic framework for decomposing LLM\nuncertainty into four distinct sources, inspired by previous research. We\ndevelop a source-specific estimation pipeline to quantify these uncertainty\ntypes and evaluate how existing metrics relate to each source across tasks and\nmodels. Our results show that metrics, task, and model exhibit systematic\nvariation in uncertainty characteristic. Building on this, we propose a method\nfor task specific metric/model selection guided by the alignment or divergence\nbetween their uncertainty characteristics and that of a given task. Our\nexperiments across datasets and models demonstrate that our uncertainty-aware\nselection strategy consistently outperforms baseline strategies, helping us\nselect appropriate models or uncertainty metrics, and contributing to more\nreliable and efficient deployment in uncertainty estimation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u6846\u67b6\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4e0d\u786e\u5b9a\u6027\u5206\u89e3\u4e3a\u56db\u79cd\u6765\u6e90\uff0c\u5e76\u5f00\u53d1\u4e86\u91cf\u5316\u8fd9\u4e9b\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u3002\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4efb\u52a1\u7279\u6027\u9009\u62e9\u5408\u9002\u6a21\u578b\u6216\u4e0d\u786e\u5b9a\u6027\u6307\u6807\u7684\u7b56\u7565\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6548\u679c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5185\u5bb9\u4e2d\u7684\u4e8b\u5b9e\u6027\u9519\u8bef\uff08\u5e7b\u89c9\uff09\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9002\u7528\u6027\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5206\u89e3LLM\u7684\u4e0d\u786e\u5b9a\u6027\u4e3a\u56db\u79cd\u6765\u6e90\uff0c\u5f00\u53d1\u4e86\u9488\u5bf9\u6bcf\u79cd\u6765\u6e90\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u4efb\u52a1\u7279\u6027\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6a21\u578b\u6216\u6307\u6807\u9009\u62e9\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u7cfb\u7edf\u6027\u5730\u5206\u6790\u4e0d\u786e\u5b9a\u6027\u7279\u5f81\uff0c\u4e14\u63d0\u51fa\u7684\u9009\u62e9\u7b56\u7565\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u63d0\u5347LLM\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u9009\u62e9\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9ad8\u6548\u90e8\u7f72\u3002"}}
{"id": "2505.07320", "pdf": "https://arxiv.org/pdf/2505.07320", "abs": "https://arxiv.org/abs/2505.07320", "authors": ["Yuhao Li", "Ling Luo", "Uwe Aickelin"], "title": "Dynamical Label Augmentation and Calibration for Noisy Electronic Health Records", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Medical research, particularly in predicting patient outcomes, heavily relies\non medical time series data extracted from Electronic Health Records (EHR),\nwhich provide extensive information on patient histories. Despite rigorous\nexamination, labeling errors are inevitable and can significantly impede\naccurate predictions of patient outcome. To address this challenge, we propose\nan \\textbf{A}ttention-based Learning Framework with Dynamic\n\\textbf{C}alibration and Augmentation for \\textbf{T}ime series Noisy\n\\textbf{L}abel \\textbf{L}earning (ACTLL). This framework leverages a\ntwo-component Beta mixture model to identify the certain and uncertain sets of\ninstances based on the fitness distribution of each class, and it captures\nglobal temporal dynamics while dynamically calibrating labels from the\nuncertain set or augmenting confident instances from the certain set.\nExperimental results on large-scale EHR datasets eICU and MIMIC-IV-ED, and\nseveral benchmark datasets from the UCR and UEA repositories, demonstrate that\nour model ACTLL has achieved state-of-the-art performance, especially under\nhigh noise levels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aACTLL\u7684\u6ce8\u610f\u529b\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u533b\u7597\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u6821\u51c6\u548c\u589e\u5f3a\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u533b\u7597\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff08\u5982EHR\uff09\u4e2d\u6807\u7b7e\u9519\u8bef\u4e0d\u53ef\u907f\u514d\uff0c\u4e25\u91cd\u5f71\u54cd\u4e86\u60a3\u8005\u7ed3\u5c40\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5b66\u4e60\u6846\u67b6\uff08ACTLL\uff09\uff0c\u7ed3\u5408Beta\u6df7\u5408\u6a21\u578b\u5206\u7c7b\u786e\u5b9a\u5b9e\u4f8b\u7684\u786e\u5b9a\u96c6\u548c\u4e0d\u786e\u5b9a\u96c6\uff0c\u5e76\u52a8\u6001\u6821\u51c6\u6216\u589e\u5f3a\u6807\u7b7e\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\uff08\u5982eICU\u3001MIMIC-IV-ED\u3001UCR\u3001UEA\uff09\u4e0a\u9a8c\u8bc1\uff0cACTLL\u5728\u9ad8\u566a\u58f0\u6c34\u5e73\u4e0b\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "ACTLL\u80fd\u6709\u6548\u5904\u7406\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u5347\u533b\u7597\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2505.06576", "pdf": "https://arxiv.org/pdf/2505.06576", "abs": "https://arxiv.org/abs/2505.06576", "authors": ["Haorui Chen", "Zeyu Ren", "Jiaxuan Ren", "Ran Ran", "Jinliang Shao", "Jie Huang", "Liangjian Deng"], "title": "Two-Stage Random Alternation Framework for Zero-Shot Pansharpening", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In recent years, pansharpening has seen rapid advancements with deep learning\nmethods, which have demonstrated impressive fusion quality. However, the\nchallenge of acquiring real high-resolution images limits the practical\napplicability of these methods. To address this, we propose a two-stage random\nalternating framework (TRA-PAN) that effectively integrates strong supervision\nconstraints from reduced-resolution images with the physical characteristics of\nfull-resolution images. The first stage introduces a pre-training procedure,\nwhich includes Degradation-Aware Modeling (DAM) to capture spatial-spectral\ndegradation mappings, alongside a warm-up procedure designed to reduce training\ntime and mitigate the negative effects of reduced-resolution data. In the\nsecond stage, Random Alternation Optimization (RAO) is employed, where random\nalternating training leverages the strengths of both reduced- and\nfull-resolution images, further optimizing the fusion model. By primarily\nrelying on full-resolution images, our method enables zero-shot training with\njust a single image pair, obviating the need for large datasets. Experimental\nresults demonstrate that TRA-PAN outperforms state-of-the-art (SOTA) methods in\nboth quantitative metrics and visual quality in real-world scenarios,\nhighlighting its strong practical applicability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTRA-PAN\u7684\u4e24\u9636\u6bb5\u968f\u673a\u4ea4\u66ff\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u964d\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u5f3a\u76d1\u7763\u7ea6\u675f\u548c\u5168\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u7269\u7406\u7279\u6027\uff0c\u89e3\u51b3\u4e86\u6df1\u5ea6\u5b66\u4e60\u5168\u8272\u9510\u5316\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u771f\u5b9e\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u800c\u53d7\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5168\u8272\u9510\u5316\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u5b9e\u9645\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u6570\u636e\u800c\u53d7\u5230\u9650\u5236\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u521b\u65b0\u7684\u4e24\u9636\u6bb5\u6846\u67b6\u7a81\u7834\u8fd9\u4e00\u74f6\u9888\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u964d\u7ea7\u611f\u77e5\u5efa\u6a21\uff08DAM\uff09\u548c\u9884\u70ed\u8fc7\u7a0b\u9884\u8bad\u7ec3\u6a21\u578b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u968f\u673a\u4ea4\u66ff\u4f18\u5316\uff08RAO\uff09\u7ed3\u5408\u964d\u5206\u8fa8\u7387\u548c\u5168\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u4f18\u52bf\u7ee7\u7eed\u4f18\u5316\u6a21\u578b\u3002", "result": "TRA-PAN\u5728\u5b9a\u91cf\u6307\u6807\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\uff0c\u4e14\u5728\u4ec5\u9700\u5355\u5bf9\u56fe\u50cf\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u96f6\u6837\u672c\u8bad\u7ec3\u3002", "conclusion": "TRA-PAN\u6846\u67b6\u5728\u5b9e\u7528\u6027\u548c\u6027\u80fd\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u4e3a\u5168\u8272\u9510\u5316\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.07351", "pdf": "https://arxiv.org/pdf/2505.07351", "abs": "https://arxiv.org/abs/2505.07351", "authors": ["Prateek Garg", "Lokesh Nagalapatti", "Sunita Sarawagi"], "title": "From Search To Sampling: Generative Models For Robust Algorithmic Recourse", "categories": ["cs.LG"], "comment": null, "summary": "Algorithmic Recourse provides recommendations to individuals who are\nadversely impacted by automated model decisions, on how to alter their profiles\nto achieve a favorable outcome. Effective recourse methods must balance three\nconflicting goals: proximity to the original profile to minimize cost,\nplausibility for realistic recourse, and validity to ensure the desired\noutcome. We show that existing methods train for these objectives separately\nand then search for recourse through a joint optimization over the recourse\ngoals during inference, leading to poor recourse recommendations. We introduce\nGenRe, a generative recourse model designed to train the three recourse\nobjectives jointly. Training such generative models is non-trivial due to lack\nof direct recourse supervision. We propose efficient ways to synthesize such\nsupervision and further show that GenRe's training leads to a consistent\nestimator. Unlike most prior methods, that employ non-robust gradient descent\nbased search during inference, GenRe simply performs a forward sampling over\nthe generative model to produce minimum cost recourse, leading to superior\nperformance across multiple metrics. We also demonstrate GenRe provides the\nbest trade-off between cost, plausibility and validity, compared to\nstate-of-art baselines. Our code is available at:\nhttps://github.com/prateekgargx/genre.", "AI": {"tldr": "GenRe\u662f\u4e00\u79cd\u751f\u6210\u5f0f\u8865\u6551\u6a21\u578b\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u63a8\u8350\u8865\u6551\u65b9\u6848\u65f6\u7684\u4e0d\u8db3\uff0c\u5b9e\u73b0\u4e86\u6210\u672c\u3001\u5408\u7406\u6027\u548c\u6709\u6548\u6027\u7684\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u8865\u6551\u65b9\u6cd5\u5206\u522b\u8bad\u7ec3\u76ee\u6807\u540e\u901a\u8fc7\u8054\u5408\u4f18\u5316\u641c\u7d22\u63a8\u8350\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u8054\u5408\u8bad\u7ec3\u8fd9\u4e9b\u76ee\u6807\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51faGenRe\u6a21\u578b\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u65b9\u6cd5\u8054\u5408\u8bad\u7ec3\u8865\u6551\u76ee\u6807\uff0c\u5e76\u5229\u7528\u5408\u6210\u76d1\u7763\u8bad\u7ec3\u6a21\u578b\uff0c\u7b80\u5316\u63a8\u7406\u8fc7\u7a0b\u4e3a\u524d\u5411\u91c7\u6837\u3002", "result": "GenRe\u5728\u6210\u672c\u3001\u5408\u7406\u6027\u548c\u6709\u6548\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u4e14\u63a8\u7406\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "GenRe\u6a21\u578b\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u751f\u6210\u5f0f\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8865\u6551\u63a8\u8350\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2505.06584", "pdf": "https://arxiv.org/pdf/2505.06584", "abs": "https://arxiv.org/abs/2505.06584", "authors": ["Ziluo Ding", "Haobin Jiang", "Yuxuan Wang", "Zhenguo Sun", "Yu Zhang", "Xiaojie Niu", "Ming Yang", "Weishuai Zeng", "Xinrun Xu", "Zongqing Lu"], "title": "JAEGER: Dual-Level Humanoid Whole-Body Controller", "categories": ["cs.RO", "cs.AI"], "comment": "15 pages, 2 figures", "summary": "This paper presents JAEGER, a dual-level whole-body controller for humanoid\nrobots that addresses the challenges of training a more robust and versatile\npolicy. Unlike traditional single-controller approaches, JAEGER separates the\ncontrol of the upper and lower bodies into two independent controllers, so that\nthey can better focus on their distinct tasks. This separation alleviates the\ndimensionality curse and improves fault tolerance. JAEGER supports both root\nvelocity tracking (coarse-grained control) and local joint angle tracking\n(fine-grained control), enabling versatile and stable movements. To train the\ncontroller, we utilize a human motion dataset (AMASS), retargeting human poses\nto humanoid poses through an efficient retargeting network, and employ a\ncurriculum learning approach. This method performs supervised learning for\ninitialization, followed by reinforcement learning for further exploration. We\nconduct our experiments on two humanoid platforms and demonstrate the\nsuperiority of our approach against state-of-the-art methods in both simulation\nand real environments.", "AI": {"tldr": "JAEGER\u662f\u4e00\u79cd\u53cc\u5c42\u6b21\u5168\u8eab\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u5206\u79bb\u4e0a\u4e0b\u534a\u8eab\u63a7\u5236\u6765\u63d0\u5347\u4eba\u5f62\u673a\u5668\u4eba\u7684\u9c81\u68d2\u6027\u548c\u591a\u529f\u80fd\u6027\u3002\u5b83\u652f\u6301\u7c97\u7c92\u5ea6\u4e0e\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u5e76\u5229\u7528\u4eba\u7c7b\u52a8\u4f5c\u6570\u636e\u96c6\u548c\u8bfe\u7a0b\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5355\u63a7\u5236\u5668\u65b9\u6cd5\u5728\u5904\u7406\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u63a7\u5236\u65f6\u9762\u4e34\u7ef4\u5ea6\u707e\u96be\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u6311\u6218\u3002JAEGER\u901a\u8fc7\u5206\u79bb\u4e0a\u4e0b\u534a\u8eab\u63a7\u5236\uff0c\u65e8\u5728\u63d0\u5347\u7b56\u7565\u7684\u9c81\u68d2\u6027\u548c\u591a\u529f\u80fd\u6027\u3002", "method": "JAEGER\u8bbe\u8ba1\u4e3a\u53cc\u5c42\u6b21\u63a7\u5236\u5668\uff0c\u5206\u522b\u63a7\u5236\u4e0a\u4e0b\u534a\u8eab\uff0c\u7ed3\u5408\u7c97\u7c92\u5ea6\uff08\u6839\u901f\u5ea6\u8ddf\u8e2a\uff09\u548c\u7ec6\u7c92\u5ea6\uff08\u5173\u8282\u89d2\u5ea6\u8ddf\u8e2a\uff09\u63a7\u5236\u3002\u8bad\u7ec3\u65f6\u4f7f\u7528AMASS\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u91cd\u5b9a\u5411\u7f51\u7edc\u5c06\u4eba\u7c7b\u52a8\u4f5c\u6620\u5c04\u5230\u673a\u5668\u4eba\u59ff\u6001\uff0c\u5e76\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\uff08\u76d1\u7763\u5b66\u4e60\u521d\u59cb\u5316+\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff09\u3002", "result": "\u5728\u4e24\u79cd\u4eba\u5f62\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u6d4b\u8bd5\uff0cJAEGER\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u66f4\u9ad8\u7684\u7a33\u5b9a\u6027\u548c\u591a\u529f\u80fd\u6027\u3002", "conclusion": "JAEGER\u901a\u8fc7\u5206\u5c42\u63a7\u5236\u548c\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.07367", "pdf": "https://arxiv.org/pdf/2505.07367", "abs": "https://arxiv.org/abs/2505.07367", "authors": ["Julian Rodemann", "James Bailie"], "title": "Generalization Bounds and Stopping Rules for Learning with Self-Selected Data", "categories": ["cs.LG", "math.ST", "stat.ME", "stat.ML", "stat.TH"], "comment": "38 pages, 4 figures", "summary": "Many learning paradigms self-select training data in light of previously\nlearned parameters. Examples include active learning, semi-supervised learning,\nbandits, or boosting. Rodemann et al. (2024) unify them under the framework of\n\"reciprocal learning\". In this article, we address the question of how well\nthese methods can generalize from their self-selected samples. In particular,\nwe prove universal generalization bounds for reciprocal learning using covering\nnumbers and Wasserstein ambiguity sets. Our results require no assumptions on\nthe distribution of self-selected data, only verifiable conditions on the\nalgorithms. We prove results for both convergent and finite iteration\nsolutions. The latter are anytime valid, thereby giving rise to stopping rules\nfor a practitioner seeking to guarantee the out-of-sample performance of their\nreciprocal learning algorithm. Finally, we illustrate our bounds and stopping\nrules for reciprocal learning's special case of semi-supervised learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u7edf\u4e00\u591a\u79cd\u81ea\u9009\u8bad\u7ec3\u6570\u636e\u7684\u5b66\u4e60\u8303\u5f0f\uff08\u5982\u4e3b\u52a8\u5b66\u4e60\u3001\u534a\u76d1\u7763\u5b66\u4e60\u7b49\uff09\u4e3a'\u4e92\u60e0\u5b66\u4e60'\u6846\u67b6\uff0c\u5e76\u63a2\u8ba8\u5176\u6cdb\u5316\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u65e0\u5206\u5e03\u5047\u8bbe\u7684\u901a\u7528\u6cdb\u5316\u8fb9\u754c\u3002", "motivation": "\u7814\u7a76\u81ea\u9009\u6570\u636e\u5b66\u4e60\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u7406\u8bba\u652f\u6301\uff0c\u786e\u4fdd\u5176\u6a21\u578b\u5728\u672a\u77e5\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u8986\u76d6\u6570\u548cWasserstein\u6a21\u7cca\u96c6\u8bc1\u660e\u6cdb\u5316\u8fb9\u754c\uff0c\u65e0\u9700\u6570\u636e\u5206\u5e03\u5047\u8bbe\uff0c\u4ec5\u9700\u7b97\u6cd5\u53ef\u9a8c\u8bc1\u6761\u4ef6\u3002", "result": "\u8bc1\u660e\u4e86\u6536\u655b\u89e3\u548c\u6709\u9650\u8fed\u4ee3\u89e3\u7684\u6cdb\u5316\u8fb9\u754c\uff0c\u540e\u8005\u9002\u7528\u4e8e\u4efb\u610f\u505c\u6b62\u89c4\u5219\uff0c\u5e76\u901a\u8fc7\u534a\u76d1\u7763\u5b66\u4e60\u5b9e\u4f8b\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u9009\u6570\u636e\u5b66\u4e60\u63d0\u4f9b\u4e86\u666e\u9002\u6027\u7406\u8bba\u5de5\u5177\uff0c\u5c24\u5176\u662f\u6cdb\u5316\u8fb9\u754c\u548c\u505c\u6b62\u89c4\u5219\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u663e\u8457\u3002"}}
{"id": "2505.06589", "pdf": "https://arxiv.org/pdf/2505.06589", "abs": "https://arxiv.org/abs/2505.06589", "authors": ["Gabriel Peyr\u00e9"], "title": "Optimal Transport for Machine Learners", "categories": ["stat.ML", "cs.AI", "math.OC"], "comment": "arXiv admin note: text overlap with arXiv:1803.00567", "summary": "Optimal Transport is a foundational mathematical theory that connects\noptimization, partial differential equations, and probability. It offers a\npowerful framework for comparing probability distributions and has recently\nbecome an important tool in machine learning, especially for designing and\nevaluating generative models. These course notes cover the fundamental\nmathematical aspects of OT, including the Monge and Kantorovich formulations,\nBrenier's theorem, the dual and dynamic formulations, the Bures metric on\nGaussian distributions, and gradient flows. It also introduces numerical\nmethods such as linear programming, semi-discrete solvers, and entropic\nregularization. Applications in machine learning include topics like training\nneural networks via gradient flows, token dynamics in transformers, and the\nstructure of GANs and diffusion models. These notes focus primarily on\nmathematical content rather than deep learning techniques.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6458\u8981\u4ecb\u7ecd\u4e86\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u751f\u6210\u6a21\u578b\u8bbe\u8ba1\u548c\u8bc4\u4f30\u3002\u5185\u5bb9\u6db5\u76d6\u57fa\u672c\u6570\u5b66\u7406\u8bba\u3001\u6570\u503c\u65b9\u6cd5\u53ca\u673a\u5668\u5b66\u4e60\u5e94\u7528\uff0c\u4f46\u4fa7\u91cd\u4e8e\u6570\u5b66\u5185\u5bb9\u800c\u975e\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u3002", "motivation": "\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u7ed3\u5408\u4e86\u4f18\u5316\u3001\u504f\u5fae\u5206\u65b9\u7a0b\u548c\u6982\u7387\u8bba\uff0c\u4e3a\u6bd4\u8f83\u6982\u7387\u5206\u5e03\u63d0\u4f9b\u4e86\u5f3a\u5927\u6846\u67b6\uff0c\u8fd1\u5e74\u6765\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u751f\u6210\u6a21\u578b\u7684\u8bbe\u8ba1\u548c\u8bc4\u4f30\u65b9\u9762\u3002", "method": "\u8bba\u6587\u6db5\u76d6\u6700\u4f18\u4f20\u8f93\u7684\u57fa\u672c\u6570\u5b66\u7406\u8bba\uff08\u5982Monge\u548cKantorovich\u516c\u5f0f\u3001Brenier\u5b9a\u7406\u3001\u5bf9\u5076\u548c\u52a8\u6001\u516c\u5f0f\u3001\u9ad8\u65af\u5206\u5e03\u7684Bures\u5ea6\u91cf\u3001\u68af\u5ea6\u6d41\uff09\uff0c\u5e76\u4ecb\u7ecd\u6570\u503c\u65b9\u6cd5\uff08\u5982\u7ebf\u6027\u89c4\u5212\u3001\u534a\u79bb\u6563\u6c42\u89e3\u5668\u548c\u71b5\u6b63\u5219\u5316\uff09\u3002", "result": "\u8bba\u6587\u63d0\u4f9b\u4e86\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u7684\u6570\u5b66\u57fa\u7840\u548c\u6570\u503c\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5177\u4f53\u5e94\u7528\uff0c\u5982\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u3001Transformer\u4e2d\u7684token\u52a8\u6001\u4ee5\u53caGAN\u548c\u6269\u6563\u6a21\u578b\u7684\u7ed3\u6784\u3002", "conclusion": "\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u6570\u5b66\u5de5\u5177\uff0c\u5728\u673a\u5668\u5b66\u4e60\u9886\u57df\u5c24\u5176\u662f\u751f\u6210\u6a21\u578b\u4e2d\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\uff0c\u8be5\u8bba\u6587\u4e3a\u5176\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u6570\u5b66\u57fa\u7840\u548c\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2505.07411", "pdf": "https://arxiv.org/pdf/2505.07411", "abs": "https://arxiv.org/abs/2505.07411", "authors": ["Wenhao Hu", "Paul Henderson", "Jos\u00e9 Cano"], "title": "ICE-Pruning: An Iterative Cost-Efficient Pruning Pipeline for Deep Neural Networks", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": "Accepted to International Joint Conference on Neural Networks (IJCNN)\n  2025", "summary": "Pruning is a widely used method for compressing Deep Neural Networks (DNNs),\nwhere less relevant parameters are removed from a DNN model to reduce its size.\nHowever, removing parameters reduces model accuracy, so pruning is typically\ncombined with fine-tuning, and sometimes other operations such as rewinding\nweights, to recover accuracy. A common approach is to repeatedly prune and then\nfine-tune, with increasing amounts of model parameters being removed in each\nstep. While straightforward to implement, pruning pipelines that follow this\napproach are computationally expensive due to the need for repeated\nfine-tuning.\n  In this paper we propose ICE-Pruning, an iterative pruning pipeline for DNNs\nthat significantly decreases the time required for pruning by reducing the\noverall cost of fine-tuning, while maintaining a similar accuracy to existing\npruning pipelines. ICE-Pruning is based on three main components: i) an\nautomatic mechanism to determine after which pruning steps fine-tuning should\nbe performed; ii) a freezing strategy for faster fine-tuning in each pruning\nstep; and iii) a custom pruning-aware learning rate scheduler to further\nimprove the accuracy of each pruning step and reduce the overall time\nconsumption. We also propose an efficient auto-tuning stage for the\nhyperparameters (e.g., freezing percentage) introduced by the three components.\nWe evaluate ICE-Pruning on several DNN models and datasets, showing that it can\naccelerate pruning by up to 9.61x. Code is available at\nhttps://github.com/gicLAB/ICE-Pruning", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aICE-Pruning\u7684\u8fed\u4ee3\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u51b3\u5b9a\u526a\u679d\u540e\u662f\u5426\u9700\u8981\u5fae\u8c03\u3001\u51bb\u7ed3\u7b56\u7565\u548c\u526a\u679d\u611f\u77e5\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u526a\u679d\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7684\u526a\u679d\u65b9\u6cd5\u9700\u8981\u53cd\u590d\u5fae\u8c03\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u672c\u6587\u65e8\u5728\u51cf\u5c11\u526a\u679d\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002", "method": "ICE-Pruning\u5305\u62ec\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u81ea\u52a8\u51b3\u5b9a\u526a\u679d\u540e\u662f\u5426\u5fae\u8c03\u7684\u673a\u5236\uff1b2) \u52a0\u901f\u5fae\u8c03\u7684\u51bb\u7ed3\u7b56\u7565\uff1b3) \u526a\u679d\u611f\u77e5\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u3002\u8fd8\u5305\u62ec\u8d85\u53c2\u6570\u81ea\u52a8\u8c03\u4f18\u3002", "result": "\u5728\u591a\u4e2aDNN\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cICE-Pruning\u80fd\u5c06\u526a\u679d\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe9.61\u500d\u3002", "conclusion": "ICE-Pruning\u5728\u51cf\u5c11\u526a\u679d\u65f6\u95f4\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u526a\u679d\u65b9\u6848\u3002"}}
{"id": "2505.06595", "pdf": "https://arxiv.org/pdf/2505.06595", "abs": "https://arxiv.org/abs/2505.06595", "authors": ["Hai-Vy Nguyen", "Fabrice Gamboa", "Sixin Zhang", "Reda Chhaibi", "Serge Gratton", "Thierry Giaccone"], "title": "Feature Representation Transferring to Lightweight Models via Perception Coherence", "categories": ["stat.ML", "cs.AI", "cs.CV", "cs.LG", "math.PR"], "comment": null, "summary": "In this paper, we propose a method for transferring feature representation to\nlightweight student models from larger teacher models. We mathematically define\na new notion called \\textit{perception coherence}. Based on this notion, we\npropose a loss function, which takes into account the dissimilarities between\ndata points in feature space through their ranking. At a high level, by\nminimizing this loss function, the student model learns to mimic how the\nteacher model \\textit{perceives} inputs. More precisely, our method is\nmotivated by the fact that the representational capacity of the student model\nis weaker than the teacher model. Hence, we aim to develop a new method\nallowing for a better relaxation. This means that, the student model does not\nneed to preserve the absolute geometry of the teacher one, while preserving\nglobal coherence through dissimilarity ranking. Our theoretical insights\nprovide a probabilistic perspective on the process of feature representation\ntransfer. Our experiments results show that our method outperforms or achieves\non-par performance compared to strong baseline methods for representation\ntransferring.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ece\u5927\u578b\u6559\u5e08\u6a21\u578b\u5411\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\u8fc1\u79fb\u7279\u5f81\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9a\u4e49'\u611f\u77e5\u4e00\u81f4\u6027'\u5e76\u8bbe\u8ba1\u76f8\u5e94\u7684\u635f\u5931\u51fd\u6570\uff0c\u4f7f\u5b66\u751f\u6a21\u578b\u80fd\u6a21\u4eff\u6559\u5e08\u6a21\u578b\u5bf9\u8f93\u5165\u7684\u611f\u77e5\u65b9\u5f0f\u3002", "motivation": "\u89e3\u51b3\u5b66\u751f\u6a21\u578b\u8868\u793a\u80fd\u529b\u8f83\u5f31\u7684\u95ee\u9898\uff0c\u76ee\u6807\u662f\u5f00\u53d1\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u4f7f\u5b66\u751f\u5728\u4e0d\u9700\u8981\u5b8c\u5168\u4fdd\u7559\u6559\u5e08\u6a21\u578b\u7684\u7edd\u5bf9\u51e0\u4f55\u7ed3\u6784\u7684\u540c\u65f6\uff0c\u80fd\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\u3002", "method": "\u57fa\u4e8e'\u611f\u77e5\u4e00\u81f4\u6027'\u6982\u5ff5\u8bbe\u8ba1\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u6570\u636e\u70b9\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u6392\u540d\u5dee\u5f02\u6765\u4f18\u5316\uff0c\u4f7f\u5b66\u751f\u6a21\u578b\u5b66\u4e60\u6559\u5e08\u6a21\u578b\u7684\u8f93\u5165\u611f\u77e5\u65b9\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7279\u5f81\u8fc1\u79fb\u4efb\u52a1\u4e0a\u4f18\u4e8e\u6216\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u6392\u540d\u5dee\u5f02\u4f18\u5316\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u7279\u5f81\u8868\u793a\u8fc1\u79fb\uff0c\u4e14\u7406\u8bba\u5206\u6790\u4e3a\u7279\u5f81\u8fc1\u79fb\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u6982\u7387\u89c6\u89d2\u3002"}}
{"id": "2505.07413", "pdf": "https://arxiv.org/pdf/2505.07413", "abs": "https://arxiv.org/abs/2505.07413", "authors": ["Tung L Nguyen", "Toby Hocking"], "title": "Learning Penalty for Optimal Partitioning via Automatic Feature Extraction", "categories": ["cs.LG", "stat.AP"], "comment": "9 Figures", "summary": "Changepoint detection identifies significant shifts in data sequences, making\nit important in areas like finance, genetics, and healthcare. The Optimal\nPartitioning algorithms efficiently detect these changes, using a penalty\nparameter to limit the changepoints number. Determining the appropriate value\nfor this penalty can be challenging. Traditionally, this process involved\nmanually extracting statistical features, such as sequence length or variance\nto make the prediction. This study proposes a novel approach that uses\nrecurrent neural networks to learn this penalty directly from raw sequences by\nautomatically extracting features. Experiments conducted on 20 benchmark\ngenomic datasets show that this novel method surpasses traditional methods in\npartitioning accuracy in most cases.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7684\u65b0\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u539f\u59cb\u6570\u636e\u5e8f\u5217\u4e2d\u5b66\u4e60\u53d8\u5316\u70b9\u68c0\u6d4b\u7684\u60e9\u7f5a\u53c2\u6570\uff0c\u81ea\u52a8\u63d0\u53d6\u7279\u5f81\u3002\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u5927\u90e8\u5206\u57fa\u56e0\u7ec4\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u53d8\u5316\u70b9\u68c0\u6d4b\u5728\u591a\u4e2a\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u9700\u624b\u52a8\u63d0\u53d6\u7edf\u8ba1\u7279\u5f81\u6765\u786e\u5b9a\u60e9\u7f5a\u53c2\u6570\uff0c\u6548\u7387\u8f83\u4f4e\u4e14\u6548\u679c\u6709\u9650\u3002", "method": "\u91c7\u7528\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u76f4\u63a5\u4ece\u539f\u59cb\u5e8f\u5217\u4e2d\u5b66\u4e60\u5e76\u9884\u6d4b\u60e9\u7f5a\u53c2\u6570\uff0c\u65e0\u9700\u4eba\u5de5\u7279\u5f81\u63d0\u53d6\u3002", "result": "\u572820\u4e2a\u57fa\u56e0\u7ec4\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u65b0\u65b9\u6cd5\u5728\u5206\u533a\u51c6\u786e\u6027\u4e0a\u591a\u6570\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u4e3a\u53d8\u5316\u70b9\u68c0\u6d4b\u7684\u60e9\u7f5a\u53c2\u6570\u9009\u62e9\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u4e14\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.06612", "pdf": "https://arxiv.org/pdf/2505.06612", "abs": "https://arxiv.org/abs/2505.06612", "authors": ["Yuqin Lan"], "title": "Burger: Robust Graph Denoising-augmentation Fusion and Multi-semantic Modeling in Social Recommendation", "categories": ["cs.SI", "cs.AI", "cs.IR", "F.2.2; I.2.7"], "comment": "10 pages, 5 figures", "summary": "In the era of rapid development of social media, social recommendation\nsystems as hybrid recommendation systems have been widely applied. Existing\nmethods capture interest similarity between users to filter out\ninterest-irrelevant relations in social networks that inevitably decrease\nrecommendation accuracy, however, limited research has a focus on the mutual\ninfluence of semantic information between the social network and the user-item\ninteraction network for further improving social recommendation. To address\nthese issues, we introduce a social \\underline{r}ecommendation model with\nro\\underline{bu}st g\\underline{r}aph denoisin\\underline{g}-augmentation fusion\nand multi-s\\underline{e}mantic Modeling(Burger). Specifically, we firstly\npropose to construct a social tensor in order to smooth the training process of\nthe model. Then, a graph convolutional network and a tensor convolutional\nnetwork are employed to capture user's item preference and social preference,\nrespectively. Considering the different semantic information in the user-item\ninteraction network and the social network, a bi-semantic coordination loss is\nproposed to model the mutual influence of semantic information. To alleviate\nthe interference of interest-irrelevant relations on multi-semantic modeling,\nwe further use Bayesian posterior probability to mine potential social\nrelations to replace social noise. Finally, the sliding window mechanism is\nutilized to update the social tensor as the input for the next iteration.\nExtensive experiments on three real datasets show Burger has a superior\nperformance compared with the state-of-the-art models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBurger\u7684\u793e\u4ea4\u63a8\u8350\u6a21\u578b\uff0c\u901a\u8fc7\u56fe\u53bb\u566a\u589e\u5f3a\u878d\u5408\u548c\u591a\u8bed\u4e49\u5efa\u6a21\uff0c\u89e3\u51b3\u4e86\u793e\u4ea4\u7f51\u7edc\u4e2d\u5174\u8da3\u65e0\u5173\u5173\u7cfb\u5bf9\u63a8\u8350\u51c6\u786e\u6027\u7684\u5e72\u6270\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5173\u6ce8\u793e\u4ea4\u7f51\u7edc\u4e0e\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u7f51\u7edc\u4e2d\u8bed\u4e49\u4fe1\u606f\u7684\u76f8\u4e92\u5f71\u54cd\uff0c\u4ece\u800c\u9650\u5236\u4e86\u793e\u4ea4\u63a8\u8350\u7684\u51c6\u786e\u6027\u3002", "method": "\u6a21\u578b\u7ed3\u5408\u56fe\u5377\u79ef\u7f51\u7edc\u548c\u5f20\u91cf\u5377\u79ef\u7f51\u7edc\u6355\u6349\u7528\u6237\u504f\u597d\uff0c\u5e76\u5f15\u5165\u53cc\u8bed\u4e49\u534f\u8c03\u635f\u5931\u548c\u8d1d\u53f6\u65af\u540e\u9a8c\u6982\u7387\u6765\u4f18\u5316\u591a\u8bed\u4e49\u5efa\u6a21\u548c\u793e\u4ea4\u5173\u7cfb\u6316\u6398\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBurger\u6a21\u578b\u7684\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "Burger\u901a\u8fc7\u591a\u8bed\u4e49\u5efa\u6a21\u548c\u793e\u4ea4\u5173\u7cfb\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u793e\u4ea4\u63a8\u8350\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.07437", "pdf": "https://arxiv.org/pdf/2505.07437", "abs": "https://arxiv.org/abs/2505.07437", "authors": ["Xiaotian Lin", "Yanlin Qi", "Yizhang Zhu", "Themis Palpanas", "Chengliang Chai", "Nan Tang", "Yuyu Luo"], "title": "LEAD: Iterative Data Selection for Efficient LLM Instruction Tuning", "categories": ["cs.LG", "cs.AI", "cs.DB"], "comment": null, "summary": "Instruction tuning has emerged as a critical paradigm for improving the\ncapabilities and alignment of large language models (LLMs). However, existing\niterative model-aware data selection methods incur significant computational\noverhead, as they rely on repeatedly performing full-dataset model inference to\nestimate sample utility for subsequent training iterations, creating a\nfundamental efficiency bottleneck. In this paper, we propose LEAD, an efficient\niterative data selection framework that accurately estimates sample utility\nentirely within the standard training loop, eliminating the need for costly\nadditional model inference. At its core, LEAD introduces Instance-Level Dynamic\nUncertainty (IDU), a theoretically grounded utility function combining\ninstantaneous training loss, gradient-based approximation of loss changes, and\nexponential smoothing of historical loss signals. To further scale efficiently\nto large datasets, LEAD employs a two-stage, coarse-to-fine selection strategy,\nadaptively prioritizing informative clusters through a multi-armed bandit\nmechanism, followed by precise fine-grained selection of high-utility samples\nusing IDU. Extensive experiments across four diverse benchmarks show that LEAD\nsignificantly outperforms state-of-the-art methods, improving average model\nperformance by 6.1%-10.8% while using only 2.5% of the training data and\nreducing overall training time by 5-10x.", "AI": {"tldr": "LEAD\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8fed\u4ee3\u6570\u636e\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7Instance-Level Dynamic Uncertainty (IDU)\u5728\u6807\u51c6\u8bad\u7ec3\u5faa\u73af\u4e2d\u4f30\u8ba1\u6837\u672c\u6548\u7528\uff0c\u65e0\u9700\u989d\u5916\u6a21\u578b\u63a8\u7406\u3002\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u548c\u6570\u636e\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u7684\u8fed\u4ee3\u6a21\u578b\u611f\u77e5\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u9700\u8981\u91cd\u590d\u8fdb\u884c\u5168\u6570\u636e\u96c6\u6a21\u578b\u63a8\u7406\u4ee5\u4f30\u8ba1\u6837\u672c\u6548\u7528\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u5f62\u6210\u4e86\u6548\u7387\u74f6\u9888\u3002LEAD\u65e8\u5728\u6d88\u9664\u8fd9\u4e00\u74f6\u9888\uff0c\u63d0\u5347\u6570\u636e\u9009\u62e9\u6548\u7387\u3002", "method": "LEAD\u5f15\u5165\u4e86Instance-Level Dynamic Uncertainty (IDU)\u4f5c\u4e3a\u7406\u8bba\u57fa\u7840\u6837\u672c\u6548\u7528\u51fd\u6570\uff0c\u7ed3\u5408\u4e86\u77ac\u65f6\u8bad\u7ec3\u635f\u5931\u3001\u68af\u5ea6\u8fd1\u4f3c\u635f\u5931\u53d8\u5316\u548c\u5386\u53f2\u635f\u5931\u4fe1\u53f7\u6307\u6570\u5e73\u6ed1\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u7c97\u5230\u7ec6\u9009\u62e9\u7b56\u7565\uff1a\u57fa\u4e8e\u591a\u81c2\u8001\u864e\u673a\u673a\u5236\u81ea\u9002\u5e94\u4f18\u5148\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u7684\u805a\u7c7b\uff0c\u968f\u540e\u7528IDU\u7cbe\u786e\u9009\u62e9\u9ad8\u6548\u7528\u6837\u672c\u3002", "result": "\u5728\u56db\u4e2a\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLEAD\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747\u6a21\u578b\u6027\u80fd\u63d0\u53476.1%-10.8%\uff0c\u4ec5\u4f7f\u75282.5%\u8bad\u7ec3\u6570\u636e\uff0c\u603b\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c115-10\u500d\u3002", "conclusion": "LEAD\u901a\u8fc7\u9ad8\u6548\u7684\u6570\u636e\u9009\u62e9\u6846\u67b6\uff0c\u5728\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u6570\u636e\u9009\u62e9\u6548\u7387\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.06620", "pdf": "https://arxiv.org/pdf/2505.06620", "abs": "https://arxiv.org/abs/2505.06620", "authors": ["Dima Alattal", "Asal Khoshravan Azar", "Puja Myles", "Richard Branson", "Hatim Abdulhussein", "Allan Tucker"], "title": "Integrating Explainable AI in Medical Devices: Technical, Clinical and Regulatory Insights and Recommendations", "categories": ["cs.HC", "cs.AI", "H.5.2"], "comment": "47 pages", "summary": "There is a growing demand for the use of Artificial Intelligence (AI) and\nMachine Learning (ML) in healthcare, particularly as clinical decision support\nsystems to assist medical professionals. However, the complexity of many of\nthese models, often referred to as black box models, raises concerns about\ntheir safe integration into clinical settings as it is difficult to understand\nhow they arrived at their predictions. This paper discusses insights and\nrecommendations derived from an expert working group convened by the UK\nMedicine and Healthcare products Regulatory Agency (MHRA). The group consisted\nof healthcare professionals, regulators, and data scientists, with a primary\nfocus on evaluating the outputs from different AI algorithms in clinical\ndecision-making contexts. Additionally, the group evaluated findings from a\npilot study investigating clinicians' behaviour and interaction with AI methods\nduring clinical diagnosis. Incorporating AI methods is crucial for ensuring the\nsafety and trustworthiness of medical AI devices in clinical settings. Adequate\ntraining for stakeholders is essential to address potential issues, and further\ninsights and recommendations for safely adopting AI systems in healthcare\nsettings are provided.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86\u7531\u82f1\u56fdMHRA\u7ec4\u7ec7\u7684\u4e13\u5bb6\u5de5\u4f5c\u7ec4\u5173\u4e8eAI\u5728\u4e34\u5e8a\u51b3\u7b56\u4e2d\u5e94\u7528\u7684\u5efa\u8bae\uff0c\u91cd\u70b9\u5173\u6ce8AI\u7b97\u6cd5\u7684\u900f\u660e\u5ea6\u3001\u5b89\u5168\u6027\u53ca\u4e34\u5e8a\u533b\u751f\u7684\u57f9\u8bad\u9700\u6c42\u3002", "motivation": "\u7531\u4e8eAI\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u8bb8\u591a\u9ed1\u76d2\u6a21\u578b\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u5f15\u53d1\u4e86\u5bf9\u5176\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u7684\u62c5\u5fe7\u3002\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u5982\u4f55\u5b89\u5168\u5730\u5c06AI\u6574\u5408\u5230\u4e34\u5e8a\u51b3\u7b56\u4e2d\u3002", "method": "\u901a\u8fc7\u4e13\u5bb6\u5de5\u4f5c\u7ec4\uff08\u5305\u62ec\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u3001\u76d1\u7ba1\u8005\u548c\u6570\u636e\u79d1\u5b66\u5bb6\uff09\u7684\u8ba8\u8bba\uff0c\u5e76\u7ed3\u5408\u4e00\u9879\u5173\u4e8e\u4e34\u5e8a\u533b\u751f\u4e0eAI\u4ea4\u4e92\u884c\u4e3a\u7684\u8bd5\u70b9\u7814\u7a76\u6765\u8bc4\u4f30AI\u7b97\u6cd5\u7684\u8f93\u51fa\u3002", "result": "\u5de5\u4f5c\u7ec4\u6307\u51fa\u4e86AI\u5728\u4e34\u5e8a\u51b3\u7b56\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5982\u6a21\u578b\u900f\u660e\u5ea6\u548c\u533b\u751f\u57f9\u8bad\u9700\u6c42\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b89\u5168\u91c7\u7eb3AI\u7cfb\u7edf\u7684\u5177\u4f53\u5efa\u8bae\u3002", "conclusion": "\u4e3a\u786e\u4fddAI\u5728\u533b\u7597\u9886\u57df\u7684\u5b89\u5168\u5e94\u7528\uff0c\u9700\u8981\u63d0\u9ad8\u6a21\u578b\u900f\u660e\u5ea6\u5e76\u52a0\u5f3a\u76f8\u5173\u4eba\u5458\u7684\u57f9\u8bad\u3002\u672c\u6587\u63d0\u4f9b\u4e86\u4fc3\u8fdbAI\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u5b89\u5168\u4f7f\u7528\u7684\u8fdb\u4e00\u6b65\u5efa\u8bae\u3002"}}
{"id": "2505.07447", "pdf": "https://arxiv.org/pdf/2505.07447", "abs": "https://arxiv.org/abs/2505.07447", "authors": ["Peng Sun", "Yi Jiang", "Tao Lin"], "title": "Unified Continuous Generative Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "https://github.com/LINs-lab/UCGM", "summary": "Recent advances in continuous generative models, including multi-step\napproaches like diffusion and flow-matching (typically requiring 8-1000\nsampling steps) and few-step methods such as consistency models (typically 1-8\nsteps), have demonstrated impressive generative performance. However, existing\nwork often treats these approaches as distinct paradigms, resulting in separate\ntraining and sampling methodologies. We introduce a unified framework for\ntraining, sampling, and analyzing these models. Our implementation, the Unified\nContinuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves\nstate-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a\n675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID\nin 20 steps and a few-step model reaching 1.42 FID in just 2 steps.\nAdditionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at\n250 steps) improves performance to 1.06 FID in only 40 steps. Code is available\nat: https://github.com/LINs-lab/UCGM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u8fde\u7eed\u751f\u6210\u6a21\u578b\u6846\u67b6UCGM-{T,S}\uff0c\u5b9e\u73b0\u4e86\u591a\u6b65\uff08\u5982\u6269\u6563\u6a21\u578b\uff09\u4e0e\u5c11\u6b65\uff08\u5982\u4e00\u81f4\u6027\u6a21\u578b\uff09\u65b9\u6cd5\u7684\u7edf\u4e00\u8bad\u7ec3\u4e0e\u91c7\u6837\uff0c\u5e76\u5728ImageNet 256x256\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u901a\u5e38\u5c06\u591a\u6b65\u4e0e\u5c11\u6b65\u751f\u6210\u6a21\u578b\u89c6\u4e3a\u72ec\u7acb\u8303\u5f0f\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0e\u91c7\u6837\u65b9\u6cd5\u5206\u79bb\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u4ee5\u6574\u5408\u8fd9\u4e9b\u65b9\u6cd5\u3002", "method": "\u63d0\u51faUCGM-{T,S}\u6846\u67b6\uff0c\u7edf\u4e00\u8bad\u7ec3\u3001\u91c7\u6837\u53ca\u5206\u6790\u8fde\u7eed\u751f\u6210\u6a21\u578b\uff0c\u652f\u6301\u591a\u6b65\u4e0e\u5c11\u6b65\u6a21\u578b\u7684\u534f\u540c\u4f18\u5316\u3002", "result": "\u5728ImageNet 256x256\u4e0a\uff0cUCGM-T\u8bad\u7ec3\u7684\u591a\u6b65\u6a21\u578b20\u6b65FID\u8fbe1.30\uff0c\u5c11\u6b65\u6a21\u578b2\u6b65FID\u8fbe1.42\uff1bUCGM-S\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u539f250\u6b65FID 1.26\uff09\u63d0\u5347\u81f340\u6b65FID 1.06\u3002", "conclusion": "UCGM\u6846\u67b6\u8bc1\u660e\u4e86\u7edf\u4e00\u591a\u6b65\u4e0e\u5c11\u6b65\u751f\u6210\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6548\u7387\u4e0e\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.06625", "pdf": "https://arxiv.org/pdf/2505.06625", "abs": "https://arxiv.org/abs/2505.06625", "authors": ["Tianhao Cai", "Liang Wang", "Limin Xiao", "Meng Han", "Zeyu Wang", "Lin Sun", "Xiaojian Liao"], "title": "CaMDN: Enhancing Cache Efficiency for Multi-tenant DNNs on Integrated NPUs", "categories": ["cs.AR", "cs.AI"], "comment": "7 pages, 9 figures. This paper has been accepted to the 2025 Design\n  Automation Conference (DAC)", "summary": "With the rapid development of DNN applications, multi-tenant execution, where\nmultiple DNNs are co-located on a single SoC, is becoming a prevailing trend.\nAlthough many methods are proposed in prior works to improve multi-tenant\nperformance, the impact of shared cache is not well studied. This paper\nproposes CaMDN, an architecture-scheduling co-design to enhance cache\nefficiency for multi-tenant DNNs on integrated NPUs. Specifically, a\nlightweight architecture is proposed to support model-exclusive, NPU-controlled\nregions inside shared cache to eliminate unexpected cache contention. Moreover,\na cache scheduling method is proposed to improve shared cache utilization. In\nparticular, it includes a cache-aware mapping method for adaptability to the\nvarying available cache capacity and a dynamic allocation algorithm to adjust\nthe usage among co-located DNNs at runtime. Compared to prior works, CaMDN\nreduces the memory access by 33.4% on average and achieves a model speedup of\nup to 2.56$\\times$ (1.88$\\times$ on average).", "AI": {"tldr": "\u63d0\u51faCaMDN\uff0c\u4e00\u7a2e\u67b6\u69cb-\u6392\u7a0b\u5171\u8a2d\u8a08\u65b9\u6cd5\uff0c\u901a\u904e\u5171\u4eab\u5feb\u53d6\u512a\u5316\u63d0\u5347\u591a\u79df\u6236DNN\u5728NPU\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u73fe\u6709\u6587\u737b\u5c0d\u5171\u4eab\u5feb\u53d6\u5728\u591a\u79df\u6236DNN\u4e2d\u7684\u5f71\u97ff\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u63d0\u5347\u5feb\u53d6\u6548\u7387\u4ee5\u512a\u5316\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u8f15\u91cf\u67b6\u69cb\u652f\u63f4\u6a21\u578b\u5c08\u6709\u5feb\u53d6\u5340\u57df\uff0c\u4e26\u8a2d\u8a08\u5feb\u53d6\u6392\u7a0b\u65b9\u6cd5\uff08\u542b\u5feb\u53d6\u611f\u77e5\u6620\u5c04\u8207\u52d5\u614b\u5206\u914d\u7b97\u6cd5\uff09\u3002", "result": "\u5e73\u5747\u6e1b\u5c1133.4%\u8a18\u61b6\u9ad4\u5b58\u53d6\uff0c\u6a21\u578b\u52a0\u901f\u6700\u9ad82.56\u500d\uff08\u5e73\u57471.88\u500d\uff09\u3002", "conclusion": "CaMDN\u6709\u6548\u6d88\u9664\u5feb\u53d6\u7af6\u722d\uff0c\u986f\u8457\u63d0\u5347\u591a\u79df\u6236DNN\u6548\u80fd\u3002"}}
{"id": "2505.07450", "pdf": "https://arxiv.org/pdf/2505.07450", "abs": "https://arxiv.org/abs/2505.07450", "authors": ["Neil De La Fuente", "Maria Pilligua", "Daniel Vidal", "Albin Soutiff", "Cecilia Curreli", "Daniel Cremers", "Andrey Barsky"], "title": "Prototype Augmented Hypernetworks for Continual Learning", "categories": ["cs.LG", "cs.AI"], "comment": "CVPR (LatinX in CV)", "summary": "Continual learning (CL) aims to learn a sequence of tasks without forgetting\nprior knowledge, but gradient updates for a new task often overwrite the\nweights learned earlier, causing catastrophic forgetting (CF). We propose\nPrototype-Augmented Hypernetworks (PAH), a framework where a single\nhypernetwork, conditioned on learnable task prototypes, dynamically generates\ntask-specific classifier heads on demand. To mitigate forgetting, PAH combines\ncross-entropy with dual distillation losses, one to align logits and another to\nalign prototypes, ensuring stable feature representations across tasks.\nEvaluations on Split-CIFAR100 and TinyImageNet demonstrate that PAH achieves\nstate-of-the-art performance, reaching 74.5 % and 63.7 % accuracy with only 1.7\n% and 4.4 % forgetting, respectively, surpassing prior methods without storing\nsamples or heads.", "AI": {"tldr": "\u63d0\u51fa\u4e86Prototype-Augmented Hypernetworks (PAH)\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u5206\u7c7b\u5668\u5934\u548c\u53cc\u91cd\u84b8\u998f\u635f\u5931\u6765\u51cf\u5c11\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u9057\u5fd8\u95ee\u9898\uff0c\u5728Split-CIFAR100\u548cTinyImageNet\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u4e2d\uff0c\u65b0\u4efb\u52a1\u7684\u68af\u5ea6\u66f4\u65b0\u5e38\u8986\u76d6\u4e4b\u524d\u5b66\u5230\u7684\u6743\u91cd\uff0c\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff08CF\uff09\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u8bba\u6587\u63d0\u51fa\u4e86PAH\u6846\u67b6\u3002", "method": "PAH\u5229\u7528\u4e00\u4e2a\u8d85\u7f51\u7edc\uff0c\u57fa\u4e8e\u53ef\u5b66\u4e60\u7684\u4efb\u52a1\u539f\u578b\u52a8\u6001\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u5206\u7c7b\u5668\u5934\uff0c\u5e76\u901a\u8fc7\u4ea4\u53c9\u71b5\u548c\u53cc\u91cd\u84b8\u998f\u635f\u5931\uff08logits\u5bf9\u9f50\u548c\u539f\u578b\u5bf9\u9f50\uff09\u6765\u51cf\u5c11\u9057\u5fd8\u3002", "result": "\u5728Split-CIFAR100\u548cTinyImageNet\u4e0a\uff0cPAH\u5206\u522b\u8fbe\u523074.5%\u548c63.7%\u7684\u51c6\u786e\u7387\uff0c\u9057\u5fd8\u7387\u4ec5\u4e3a1.7%\u548c4.4%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PAH\u901a\u8fc7\u52a8\u6001\u751f\u6210\u5206\u7c7b\u5668\u5934\u548c\u53cc\u91cd\u84b8\u998f\u635f\u5931\uff0c\u6709\u6548\u5730\u51cf\u5c11\u4e86\u6301\u7eed\u5b66\u4e60\u7684\u9057\u5fd8\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2505.07477", "pdf": "https://arxiv.org/pdf/2505.07477", "abs": "https://arxiv.org/abs/2505.07477", "authors": ["Hongkun Dou", "Zeyu Li", "Xingyu Jiang", "Hongjue Li", "Lijun Yang", "Wen Yao", "Yue Deng"], "title": "You Only Look One Step: Accelerating Backpropagation in Diffusion Sampling with Gradient Shortcuts", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Diffusion models (DMs) have recently demonstrated remarkable success in\nmodeling large-scale data distributions. However, many downstream tasks require\nguiding the generated content based on specific differentiable metrics,\ntypically necessitating backpropagation during the generation process. This\napproach is computationally expensive, as generating with DMs often demands\ntens to hundreds of recursive network calls, resulting in high memory usage and\nsignificant time consumption. In this paper, we propose a more efficient\nalternative that approaches the problem from the perspective of parallel\ndenoising. We show that full backpropagation throughout the entire generation\nprocess is unnecessary. The downstream metrics can be optimized by retaining\nthe computational graph of only one step during generation, thus providing a\nshortcut for gradient propagation. The resulting method, which we call Shortcut\nDiffusion Optimization (SDO), is generic, high-performance, and computationally\nlightweight, capable of optimizing all parameter types in diffusion sampling.\nWe demonstrate the effectiveness of SDO on several real-world tasks, including\ncontrolling generation by optimizing latent and aligning the DMs by fine-tuning\nnetwork parameters. Compared to full backpropagation, our approach reduces\ncomputational costs by $\\sim 90\\%$ while maintaining superior performance. Code\nis available at https://github.com/deng-ai-lab/SDO.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSDO\uff08Shortcut Diffusion Optimization\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ec5\u4fdd\u7559\u751f\u6210\u8fc7\u7a0b\u4e2d\u4e00\u4e2a\u6b65\u9aa4\u7684\u8ba1\u7b97\u56fe\u6765\u9ad8\u6548\u4f18\u5316\u6269\u6563\u6a21\u578b\u7684\u4e0b\u6e38\u4efb\u52a1\uff0c\u51cf\u5c11\u4e8690%\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u901a\u5e38\u9700\u8981\u57fa\u4e8e\u53ef\u5fae\u5206\u6307\u6807\u8fdb\u884c\u5185\u5bb9\u5f15\u5bfc\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5728\u6574\u4e2a\u751f\u6210\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "SDO\u65b9\u6cd5\u901a\u8fc7\u5e76\u884c\u53bb\u566a\u7684\u89c6\u89d2\uff0c\u4ec5\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u4fdd\u7559\u4e00\u4e2a\u6b65\u9aa4\u7684\u8ba1\u7b97\u56fe\uff0c\u4e3a\u68af\u5ea6\u4f20\u64ad\u63d0\u4f9b\u6377\u5f84\uff0c\u4ece\u800c\u907f\u514d\u5168\u8fc7\u7a0b\u7684\u6602\u8d35\u53cd\u5411\u4f20\u64ad\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eSDO\u5728\u591a\u4e2a\u5b9e\u9645\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5305\u62ec\u6f5c\u5728\u7a7a\u95f4\u4f18\u5316\u548c\u7f51\u7edc\u53c2\u6570\u5fae\u8c03\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u7ea690%\uff0c\u6027\u80fd\u4ecd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "SDO\u662f\u4e00\u79cd\u901a\u7528\u3001\u9ad8\u6548\u4e14\u8f7b\u91cf\u7ea7\u7684\u6269\u6563\u91c7\u6837\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\u3002"}}
{"id": "2505.06632", "pdf": "https://arxiv.org/pdf/2505.06632", "abs": "https://arxiv.org/abs/2505.06632", "authors": ["Rathin Chandra Shit", "Sharmila Subudhi"], "title": "AI-Powered Anomaly Detection with Blockchain for Real-Time Security and Reliability in Autonomous Vehicles", "categories": ["cs.CR", "cs.AI"], "comment": "Scheduled for presentation at an upcoming conference", "summary": "Autonomous Vehicles (AV) proliferation brings important and pressing security\nand reliability issues that must be dealt with to guarantee public safety and\nhelp their widespread adoption. The contribution of the proposed research is\ntowards achieving more secure, reliable, and trustworthy autonomous\ntransportation system by providing more capabilities for anomaly detection,\ndata provenance, and real-time response in safety critical AV deployments. In\nthis research, we develop a new framework that combines the power of Artificial\nIntelligence (AI) for real-time anomaly detection with blockchain technology to\ndetect and prevent any malicious activity including sensor failures in AVs.\nThrough Long Short-Term Memory (LSTM) networks, our approach continually\nmonitors associated multi-sensor data streams to detect anomalous patterns that\nmay represent cyberattacks as well as hardware malfunctions. Further, this\nframework employs a decentralized platform for securely storing sensor data and\nanomaly alerts in a blockchain ledger for data incorruptibility and\nauthenticity, while offering transparent forensic features. Moreover, immediate\nautomated response mechanisms are deployed using smart contracts when anomalies\nare found. This makes the AV system more resilient to attacks from both\ncyberspace and hardware component failure. Besides, we identify potential\nchallenges of scalability in handling high frequency sensor data, computational\nconstraint in resource constrained environment, and of distributed data storage\nin terms of privacy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408AI\u5b9e\u65f6\u5f02\u5e38\u68c0\u6d4b\u548c\u533a\u5757\u94fe\u6280\u672f\u7684\u65b0\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u6db5\u76d6\u5f02\u5e38\u68c0\u6d4b\u3001\u6570\u636e\u9632\u7be1\u6539\u548c\u5b9e\u65f6\u54cd\u5e94\u673a\u5236\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u666e\u53ca\u5e26\u6765\u4e86\u5b89\u5168\u548c\u53ef\u9760\u6027\u95ee\u9898\uff0c\u9700\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u4ee5\u786e\u4fdd\u516c\u5171\u5b89\u5168\u5e76\u4fc3\u8fdb\u5e7f\u6cdb\u91c7\u7528\u3002", "method": "\u5229\u7528LSTM\u7f51\u7edc\u5b9e\u65f6\u76d1\u63a7\u591a\u4f20\u611f\u5668\u6570\u636e\u6d41\u4ee5\u68c0\u6d4b\u5f02\u5e38\uff0c\u5e76\u901a\u8fc7\u533a\u5757\u94fe\u6280\u672f\u5b58\u50a8\u6570\u636e\u548c\u8b66\u62a5\uff0c\u7ed3\u5408\u667a\u80fd\u5408\u7ea6\u5b9e\u73b0\u81ea\u52a8\u5316\u54cd\u5e94\u3002", "result": "\u6846\u67b6\u589e\u5f3a\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5bf9\u7f51\u7edc\u653b\u51fb\u548c\u786c\u4ef6\u6545\u969c\u7684\u62b5\u5fa1\u80fd\u529b\uff0c\u540c\u65f6\u786e\u4fdd\u4e86\u6570\u636e\u7684\u771f\u5b9e\u6027\u548c\u900f\u660e\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9700\u89e3\u51b3\u9ad8\u9891\u6570\u636e\u5904\u7406\u3001\u8d44\u6e90\u9650\u5236\u548c\u9690\u79c1\u7b49\u6f5c\u5728\u6311\u6218\u3002"}}
{"id": "2505.07503", "pdf": "https://arxiv.org/pdf/2505.07503", "abs": "https://arxiv.org/abs/2505.07503", "authors": ["Quang-Duy Tran", "Bao Duong", "Phuoc Nguyen", "Thin Nguyen"], "title": "Identifying Causal Direction via Variational Bayesian Compression", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted at the 42nd International Conference on Machine Learning\n  (ICML2025)", "summary": "Telling apart the cause and effect between two random variables with purely\nobservational data is a challenging problem that finds applications in various\nscientific disciplines. A key principle utilized in this task is the\nalgorithmic Markov condition, which postulates that the joint distribution,\nwhen factorized according to the causal direction, yields a more succinct\ncodelength compared to the anti-causal direction. Previous approaches\napproximate these codelengths by relying on simple functions or Gaussian\nprocesses (GPs) with easily evaluable complexity, compromising between model\nfitness and computational complexity. To overcome these limitations, we propose\nleveraging the variational Bayesian learning of neural networks as an\ninterpretation of the codelengths. Consequently, we can enhance the model\nfitness while promoting the succinctness of the codelengths, while avoiding the\nsignificant computational complexity of the GP-based approaches. Extensive\nexperiments on both synthetic and real-world benchmarks in cause-effect\nidentification demonstrate the effectiveness of our proposed method, surpassing\nthe overall performance of related complexity-based and structural causal model\nregression-based approaches.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u53d8\u5206\u8d1d\u53f6\u65af\u5b66\u4e60\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u56e0\u679c\u63a8\u65ad\u4e2d\u7684\u7f16\u7801\u957f\u5ea6\uff0c\u4ece\u800c\u5728\u6a21\u578b\u62df\u5408\u6027\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u7b80\u5355\u51fd\u6570\u6216\u9ad8\u65af\u8fc7\u7a0b\u6765\u8fd1\u4f3c\u7f16\u7801\u957f\u5ea6\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5728\u6a21\u578b\u62df\u5408\u6027\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e4b\u95f4\u5b58\u5728\u59a5\u534f\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u8d1d\u53f6\u65af\u5b66\u4e60\u795e\u7ecf\u7f51\u7edc\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u66f4\u9ad8\u6548\u5730\u8ba1\u7b97\u56e0\u679c\u65b9\u5411\u4e0b\u7684\u7f16\u7801\u957f\u5ea6\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u56e0\u679c\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u590d\u6742\u5ea6\u9a71\u52a8\u65b9\u6cd5\u548c\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u56de\u5f52\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u7f16\u7801\u957f\u5ea6\u7684\u8ba1\u7b97\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u6a21\u578b\u62df\u5408\u6027\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4ece\u800c\u5728\u56e0\u679c\u63a8\u65ad\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u4f18\u6027\u80fd\u3002"}}
{"id": "2505.07508", "pdf": "https://arxiv.org/pdf/2505.07508", "abs": "https://arxiv.org/abs/2505.07508", "authors": ["Jing Ren", "Mingliang Hou", "Zhixuan Liu", "Xiaomei Bai"], "title": "EAGLE: Contrastive Learning for Efficient Graph Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph anomaly detection is a popular and vital task in various real-world\nscenarios, which has been studied for several decades. Recently, many studies\nextending deep learning-based methods have shown preferable performance on\ngraph anomaly detection. However, existing methods are lack of efficiency that\nis definitely necessary for embedded devices. Towards this end, we propose an\nEfficient Anomaly detection model on heterogeneous Graphs via contrastive\nLEarning (EAGLE) by contrasting abnormal nodes with normal ones in terms of\ntheir distances to the local context. The proposed method first samples\ninstance pairs on meta path-level for contrastive learning. Then, a graph\nautoencoder-based model is applied to learn informative node embeddings in an\nunsupervised way, which will be further combined with the discriminator to\npredict the anomaly scores of nodes. Experimental results show that EAGLE\noutperforms the state-of-the-art methods on three heterogeneous network\ndatasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEAGLE\u7684\u9ad8\u6548\u5f02\u6784\u56fe\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5f02\u5e38\u8282\u70b9\u4e0e\u6b63\u5e38\u8282\u70b9\u5728\u5176\u5c40\u90e8\u4e0a\u4e0b\u6587\u4e2d\u7684\u8ddd\u79bb\u6765\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u6548\u7387\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "EAGLE\u4f7f\u7528\u5143\u8def\u5f84\u7ea7\u5b9e\u4f8b\u5bf9\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5e76\u7ed3\u5408\u56fe\u81ea\u7f16\u7801\u5668\u548c\u5224\u522b\u5668\u4ee5\u65e0\u76d1\u7763\u65b9\u5f0f\u5b66\u4e60\u8282\u70b9\u5d4c\u5165\u5e76\u9884\u6d4b\u5f02\u5e38\u5f97\u5206\u3002", "result": "EAGLE\u5728\u4e09\u4e2a\u5f02\u6784\u7f51\u7edc\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "EAGLE\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5f02\u6784\u56fe\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u3002"}}
{"id": "2505.06652", "pdf": "https://arxiv.org/pdf/2505.06652", "abs": "https://arxiv.org/abs/2505.06652", "authors": ["Ernesto Giralt Hernandez", "Lazaro Antonio Bueno Perez"], "title": "Enfoque Odychess: Un m\u00e9todo dial\u00e9ctico, constructivista y adaptativo para la ense\u00f1anza del ajedrez con inteligencias artificiales generativas", "categories": ["cs.CY", "cs.AI"], "comment": "Full article in Spanish", "summary": "Chess teaching has evolved through different approaches, however, traditional\nmethodologies, often based on memorization, contrast with the new possibilities\noffered by generative artificial intelligence, a technology still little\nexplored in this field. This study seeks to empirically validate the\neffectiveness of the Odychess Approach in improving chess knowledge, strategic\nunderstanding, and metacognitive skills in students. A quasi-experimental study\nwas conducted with a pre-test/post-test design and a control group (N=60). The\nexperimental intervention implemented the Odychess Approach, incorporating a\nLlama 3.3 language model that was specifically adapted using\nParameter-Efficient Fine-Tuning (PEFT) techniques to act as a Socratic chess\ntutor. Quantitative assessment instruments were used to measure chess\nknowledge, strategic understanding, and metacognitive skills before and after\nthe intervention. The results of the quasi-experimental study showed\nsignificant improvements in the experimental group compared to the control\ngroup in the three variables analyzed: chess knowledge, strategic\nunderstanding, and metacognitive skills. The complementary qualitative analysis\nrevealed greater analytical depth, more developed dialectical reasoning, and\nincreased intrinsic motivation in students who participated in the Odychess\nmethod-based intervention. The Odychess Approach represents an effective\npedagogical methodology for teaching chess, demonstrating the potential of the\nsynergistic integration of constructivist and dialectical principles with\ngenerative artificial intelligence. The implications of this work are relevant\nfor educators and institutions interested in adopting innovative pedagogical\ntechnologies and for researchers in the field of AI applied to education,\nhighlighting the transferability of the language model adaptation methodology\nto other educational domains.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9a8c\u8bc1\u4e86\u7ed3\u5408\u751f\u6210\u5f0fAI\uff08Llama 3.3\u6a21\u578b\uff09\u7684Odychess\u65b9\u6cd5\u5728\u63d0\u5347\u5b66\u751f\u56fd\u9645\u8c61\u68cb\u77e5\u8bc6\u3001\u6218\u7565\u7406\u89e3\u548c\u5143\u8ba4\u77e5\u6280\u80fd\u4e0a\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u663e\u793a\u5b9e\u9a8c\u7ec4\u663e\u8457\u4f18\u4e8e\u5bf9\u7167\u7ec4\u3002", "motivation": "\u4f20\u7edf\u56fd\u9645\u8c61\u68cb\u6559\u5b66\u4f9d\u8d56\u8bb0\u5fc6\u6cd5\uff0c\u800c\u751f\u6210\u5f0fAI\u5728\u8be5\u9886\u57df\u7684\u6f5c\u529b\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u57fa\u4e8eAI\u7684Odychess\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528\u51c6\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u5b9e\u9a8c\u7ec4\u63a5\u53d7\u57fa\u4e8eLlama 3.3\u6a21\u578b\uff08\u7ecfPEFT\u8c03\u6574\uff09\u7684Socratic\u5bfc\u5e08\u5e72\u9884\uff0c\u901a\u8fc7\u524d\u540e\u6d4b\u548c\u5bf9\u7167\u7ec4\uff08N=60\uff09\u91cf\u5316\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ec4\u5728\u8c61\u68cb\u77e5\u8bc6\u3001\u6218\u7565\u7406\u89e3\u548c\u5143\u8ba4\u77e5\u6280\u80fd\u4e0a\u663e\u8457\u63d0\u5347\uff1b\u5b9a\u6027\u5206\u6790\u8fd8\u663e\u793a\u5b66\u751f\u5206\u6790\u6df1\u5ea6\u3001\u8fa9\u8bc1\u601d\u7ef4\u548c\u5185\u5728\u52a8\u673a\u589e\u5f3a\u3002", "conclusion": "Odychess\u65b9\u6cd5\u7ed3\u5408\u6784\u5efa\u4e3b\u4e49\u4e0e\u751f\u6210\u5f0fAI\uff0c\u662f\u6709\u6548\u7684\u6559\u5b66\u5de5\u5177\uff0c\u5176\u6a21\u578b\u8c03\u6574\u65b9\u6cd5\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u6559\u80b2\u9886\u57df\u3002"}}
{"id": "2505.07525", "pdf": "https://arxiv.org/pdf/2505.07525", "abs": "https://arxiv.org/abs/2505.07525", "authors": ["Sana Ayromlou", "D. B. Emerson"], "title": "Adaptive Latent-Space Constraints in Personalized FL", "categories": ["cs.LG", "68T07", "I.2.0; I.2.11; I.2.6"], "comment": "14 Pages, 1 Algorithm, 3 Figures, 3 Tables", "summary": "Federated learning (FL) has become an effective and widely used approach to\ntraining deep learning models on decentralized datasets held by distinct\nclients. FL also strengthens both security and privacy protections for training\ndata. Common challenges associated with statistical heterogeneity between\ndistributed datasets have spurred significant interest in personalized FL (pFL)\nmethods, where models combine aspects of global learning with local modeling\nspecific to each client's unique characteristics. In this work, the efficacy of\ntheoretically supported, adaptive MMD measures within the Ditto framework, a\nstate-of-the-art technique in pFL, are investigated. The use of such measures\nsignificantly improves model performance across a variety of tasks, especially\nthose with pronounced feature heterogeneity. While the Ditto algorithm is\nspecifically considered, such measures are directly applicable to a number of\nother pFL settings, and the results motivate the use of constraints tailored to\nthe various kinds of heterogeneity expected in FL systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\uff08pFL\uff09\u4e2d\u5e94\u7528\u81ea\u9002\u5e94MMD\u5ea6\u91cf\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u6548\u679c\uff0c\u5c24\u5176\u662f\u5728\u7279\u5f81\u5f02\u8d28\u6027\u663e\u8457\u7684\u4efb\u52a1\u4e2d\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u5206\u6563\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u65f6\u9762\u4e34\u7edf\u8ba1\u5f02\u8d28\u6027\u6311\u6218\uff0c\u4e2a\u6027\u5316FL\uff08pFL\uff09\u7ed3\u5408\u5168\u5c40\u5b66\u4e60\u548c\u672c\u5730\u5efa\u6a21\u4ee5\u5e94\u5bf9\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5728Ditto\u6846\u67b6\uff08pFL\u7684\u5148\u8fdb\u6280\u672f\uff09\u4e2d\uff0c\u91c7\u7528\u7406\u8bba\u652f\u6301\u7684\u81ea\u9002\u5e94MMD\u5ea6\u91cf\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u81ea\u9002\u5e94MMD\u5ea6\u91cf\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u7279\u5f81\u5f02\u8d28\u6027\u660e\u663e\u7684\u4efb\u52a1\u4e2d\u3002", "conclusion": "\u81ea\u9002\u5e94MMD\u5ea6\u91cf\u4e0d\u4ec5\u9002\u7528\u4e8eDitto\u7b97\u6cd5\uff0c\u8fd8\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6pFL\u573a\u666f\uff0c\u6fc0\u52b1\u9488\u5bf9FL\u7cfb\u7edf\u4e2d\u5f02\u8d28\u6027\u7684\u5b9a\u5236\u7ea6\u675f\u8bbe\u8ba1\u3002"}}
{"id": "2505.06682", "pdf": "https://arxiv.org/pdf/2505.06682", "abs": "https://arxiv.org/abs/2505.06682", "authors": ["Zijian Zhao"], "title": "A Short Overview of Multi-Modal Wi-Fi Sensing", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "Wi-Fi sensing has emerged as a significant technology in wireless sensing and\nIntegrated Sensing and Communication (ISAC), offering benefits such as low\ncost, high penetration, and enhanced privacy. Currently, it is widely utilized\nin various applications, including action recognition, human localization, and\ncrowd counting. However, Wi-Fi sensing also faces challenges, such as low\nrobustness and difficulties in data collection. Recently, there has been an\nincreasing focus on multi-modal Wi-Fi sensing, where other modalities can act\nas teachers, providing ground truth or robust features for Wi-Fi sensing models\nto learn from, or can be directly fused with Wi-Fi for enhanced sensing\ncapabilities. Although these methods have demonstrated promising results and\nsubstantial value in practical applications, there is a lack of comprehensive\nsurveys reviewing them. To address this gap, this paper reviews the multi-modal\nWi-Fi sensing literature \\textbf{from the past 24 months} and highlights the\ncurrent limitations, challenges and future directions in this field.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u8fc7\u53bb24\u4e2a\u6708\u4e2d\u7684\u591a\u6a21\u6001Wi-Fi\u611f\u77e5\u6280\u672f\uff0c\u5206\u6790\u4e86\u5176\u4f18\u52bf\u3001\u6311\u6218\u53ca\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u5f25\u8865\u4e86\u8be5\u9886\u57df\u7f3a\u4e4f\u5168\u9762\u8c03\u7814\u7684\u7a7a\u767d\u3002", "motivation": "Wi-Fi\u611f\u77e5\u6280\u672f\u6210\u672c\u4f4e\u3001\u7a7f\u900f\u6027\u5f3a\u4e14\u9690\u79c1\u6027\u9ad8\uff0c\u4f46\u9762\u4e34\u9c81\u68d2\u6027\u5dee\u548c\u6570\u636e\u6536\u96c6\u56f0\u96be\u7b49\u95ee\u9898\u3002\u591a\u6a21\u6001Wi-Fi\u611f\u77e5\u901a\u8fc7\u7ed3\u5408\u5176\u4ed6\u6a21\u6001\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u7efc\u8ff0\uff0c\u56e0\u6b64\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5bf9\u8fc7\u53bb24\u4e2a\u6708\u7684\u591a\u6a21\u6001Wi-Fi\u611f\u77e5\u6587\u732e\u8fdb\u884c\u7efc\u8ff0\uff0c\u68b3\u7406\u73b0\u6709\u65b9\u6cd5\u3001\u6027\u80fd\u548c\u5c40\u9650\u6027\u3002", "result": "\u591a\u6a21\u6001Wi-Fi\u611f\u77e5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u6570\u636e\u878d\u5408\u3001\u6a21\u578b\u9c81\u68d2\u6027\u7b49\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86\u591a\u6a21\u6001Wi-Fi\u611f\u77e5\u7684\u73b0\u72b6\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u8fdb\u4e00\u6b65\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2505.07527", "pdf": "https://arxiv.org/pdf/2505.07527", "abs": "https://arxiv.org/abs/2505.07527", "authors": ["Hu Wang", "Congbo Ma", "Ian Reid", "Mohammad Yaqub"], "title": "Kalman Filter Enhanced GRPO for Reinforcement Learning-Based Language Model Reasoning", "categories": ["cs.LG"], "comment": null, "summary": "Reward baseline is important for Reinforcement Learning (RL) algorithms to\nreduce variance in policy gradient estimates. Recently, for language modeling,\nGroup Relative Policy Optimization (GRPO) is proposed to compute the advantage\nfor each output by subtracting the mean reward, as the baseline, for all\noutputs in the group. However, it can lead to inaccurate advantage estimates in\nenvironments with highly noisy rewards, potentially introducing bias. In this\nwork, we propose a model, called Kalman Filter Enhanced Group Relative Policy\nOptimization (KRPO), by using lightweight Kalman filtering to dynamically\nestimate the latent reward mean and variance. This filtering technique replaces\nthe naive batch mean baseline, enabling more adaptive advantage normalization.\nOur method does not require additional learned parameters over GRPO. This\napproach offers a simple yet effective way to incorporate multiple outputs of\nGRPO into advantage estimation, improving policy optimization in settings where\nhighly dynamic reward signals are difficult to model for language models.\nThrough experiments and analyses, we show that using a more adaptive advantage\nestimation model, KRPO can improve the stability and performance of GRPO. The\ncode is available at https://github.com/billhhh/KRPO_LLMs_RL", "AI": {"tldr": "KRPO\u6539\u8fdbGRPO\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5361\u5c14\u66fc\u6ee4\u6ce2\u52a8\u6001\u4f30\u8ba1\u5956\u52b1\u5747\u503c\u548c\u65b9\u5dee\uff0c\u63d0\u5347\u4f18\u52bf\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u9ad8\u566a\u58f0\u5956\u52b1\u73af\u5883\u3002", "motivation": "GRPO\u5728\u9ad8\u566a\u58f0\u5956\u52b1\u73af\u5883\u4e0b\u53ef\u80fd\u5bfc\u81f4\u4f18\u52bf\u4f30\u8ba1\u4e0d\u51c6\u786e\uff0c\u5f15\u5165\u504f\u5dee\uff0cKRPO\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "KRPO\u4f7f\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u52a8\u6001\u4f30\u8ba1\u6f5c\u5728\u5956\u52b1\u5747\u503c\u548c\u65b9\u5dee\uff0c\u66ff\u4ee3GRPO\u7684\u6279\u91cf\u5747\u503c\u57fa\u7ebf\uff0c\u65e0\u9700\u989d\u5916\u5b66\u4e60\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cKRPO\u80fd\u63d0\u5347GRPO\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "conclusion": "KRPO\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u4f18\u5316\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u52a8\u6001\u5956\u52b1\u4fe1\u53f7\u4e0b\u7684\u7b56\u7565\u4f18\u5316\u3002"}}
{"id": "2505.06684", "pdf": "https://arxiv.org/pdf/2505.06684", "abs": "https://arxiv.org/abs/2505.06684", "authors": ["Xuefeng Jiang", "Jia Li", "Nannan Wu", "Zhiyuan Wu", "Xujing Li", "Sheng Sun", "Gang Xu", "Yuwei Wang", "Qi Li", "Min Liu"], "title": "FNBench: Benchmarking Robust Federated Learning against Noisy Labels", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to IEEE TDSC, currently under major revision", "summary": "Robustness to label noise within data is a significant challenge in federated\nlearning (FL). From the data-centric perspective, the data quality of\ndistributed datasets can not be guaranteed since annotations of different\nclients contain complicated label noise of varying degrees, which causes the\nperformance degradation. There have been some early attempts to tackle noisy\nlabels in FL. However, there exists a lack of benchmark studies on\ncomprehensively evaluating their practical performance under unified settings.\nTo this end, we propose the first benchmark study FNBench to provide an\nexperimental investigation which considers three diverse label noise patterns\ncovering synthetic label noise, imperfect human-annotation errors and\nsystematic errors. Our evaluation incorporates eighteen state-of-the-art\nmethods over five image recognition datasets and one text classification\ndataset. Meanwhile, we provide observations to understand why noisy labels\nimpair FL, and additionally exploit a representation-aware regularization\nmethod to enhance the robustness of existing methods against noisy labels based\non our observations. Finally, we discuss the limitations of this work and\npropose three-fold future directions. To facilitate related communities, our\nsource code is open-sourced at https://github.com/Sprinter1999/FNBench.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4e2d\u6807\u7b7e\u566a\u58f0\u7684\u57fa\u51c6\u7814\u7a76FNBench\uff0c\u8bc4\u4f30\u4e8618\u79cd\u5148\u8fdb\u65b9\u6cd5\u5728\u591a\u79cd\u566a\u58f0\u6a21\u5f0f\u548c\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8868\u793a\u611f\u77e5\u6b63\u5219\u5316\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u5206\u5e03\u5f0f\u6570\u636e\u7684\u6807\u7b7e\u566a\u58f0\u95ee\u9898\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u57fa\u51c6\u7814\u7a76\u6765\u8bc4\u4f30\u73b0\u6709\u65b9\u6cd5\u7684\u5b9e\u9645\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86FNBench\u57fa\u51c6\u7814\u7a76\uff0c\u6db5\u76d6\u4e09\u79cd\u6807\u7b7e\u566a\u58f0\u6a21\u5f0f\uff08\u5408\u6210\u566a\u58f0\u3001\u4eba\u5de5\u6807\u6ce8\u9519\u8bef\u3001\u7cfb\u7edf\u9519\u8bef\uff09\uff0c\u5728\u4e94\u79cd\u56fe\u50cf\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u6587\u672c\u6570\u636e\u96c6\u4e0a\u8bc4\u4f3018\u79cd\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u8868\u793a\u611f\u77e5\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u6807\u7b7e\u566a\u58f0\u5bf9FL\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u80fd\u591f\u63d0\u5347\u73b0\u6709\u65b9\u6cd5\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "FNBench\u586b\u8865\u4e86FL\u9886\u57df\u6807\u7b7e\u566a\u58f0\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u672a\u6765\u65b9\u5411\u5e76\u5f00\u6e90\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u793e\u533a\u53d1\u5c55\u3002"}}
{"id": "2505.07548", "pdf": "https://arxiv.org/pdf/2505.07548", "abs": "https://arxiv.org/abs/2505.07548", "authors": ["Lingkun Luo", "Shiqiang Hu", "Liming Chen"], "title": "Noise Optimized Conditional Diffusion for Domain Adaptation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "9 pages, 4 figures This work has been accepted by the International\n  Joint Conference on Artificial Intelligence (IJCAI 2025)", "summary": "Pseudo-labeling is a cornerstone of Unsupervised Domain Adaptation (UDA), yet\nthe scarcity of High-Confidence Pseudo-Labeled Target Domain Samples\n(\\textbf{hcpl-tds}) often leads to inaccurate cross-domain statistical\nalignment, causing DA failures. To address this challenge, we propose\n\\textbf{N}oise \\textbf{O}ptimized \\textbf{C}onditional \\textbf{D}iffusion for\n\\textbf{D}omain \\textbf{A}daptation (\\textbf{NOCDDA}), which seamlessly\nintegrates the generative capabilities of conditional diffusion models with the\ndecision-making requirements of DA to achieve task-coupled optimization for\nefficient adaptation. For robust cross-domain consistency, we modify the DA\nclassifier to align with the conditional diffusion classifier within a unified\noptimization framework, enabling forward training on noise-varying cross-domain\nsamples. Furthermore, we argue that the conventional \\( \\mathcal{N}(\\mathbf{0},\n\\mathbf{I}) \\) initialization in diffusion models often generates\nclass-confused hcpl-tds, compromising discriminative DA. To resolve this, we\nintroduce a class-aware noise optimization strategy that refines sampling\nregions for reverse class-specific hcpl-tds generation, effectively enhancing\ncross-domain alignment. Extensive experiments across 5 benchmark datasets and\n29 DA tasks demonstrate significant performance gains of \\textbf{NOCDDA} over\n31 state-of-the-art methods, validating its robustness and effectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86NOCDDA\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u6761\u4ef6\u6269\u6563\u6a21\u578b\u548c\u9886\u57df\u81ea\u9002\u5e94\uff08DA\uff09\u4efb\u52a1\u4f18\u5316\u751f\u6210\u9ad8\u7f6e\u4fe1\u4f2a\u6807\u7b7e\u76ee\u6807\u57df\u6837\u672c\uff08hcpl-tds\uff09\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4f2a\u6807\u7b7e\u65b9\u6cd5\u4e2d\u6837\u672c\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u8de8\u9886\u57df\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u4f2a\u6807\u7b7e\u65b9\u6cd5\u5728\u65e0\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\uff08UDA\uff09\u4e2d\u56e0\u9ad8\u7f6e\u4fe1\u4f2a\u6807\u7b7e\u76ee\u6807\u57df\u6837\u672c\uff08hcpl-tds\uff09\u7a00\u7f3a\uff0c\u5bfc\u81f4\u8de8\u9886\u57df\u7edf\u8ba1\u5bf9\u9f50\u4e0d\u51c6\u786e\uff0c\u4ece\u800c\u5931\u8d25\u3002", "method": "\u6240\u63d0\u51fa\u7684NOCDDA\u65b9\u6cd5\u6574\u5408\u4e86\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u548cDA\u7684\u51b3\u7b56\u9700\u6c42\uff0c\u5f15\u5165\u7c7b\u611f\u77e5\u566a\u58f0\u4f18\u5316\u7b56\u7565\uff0c\u4f18\u5316\u53cd\u5411\u91c7\u6837\u751f\u6210hcpl-tds\u7684\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u8de8\u9886\u57df\u4e00\u81f4\u6027\u3002", "result": "\u57285\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c29\u4e2aDA\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNOCDDA\u572831\u79cd\u73b0\u6709\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "NOCDDA\u901a\u8fc7\u566a\u58f0\u4f18\u5316\u548c\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u9886\u57df\u81ea\u9002\u5e94\u7684\u6027\u80fd\uff0c\u4e3aUDA\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.06694", "pdf": "https://arxiv.org/pdf/2505.06694", "abs": "https://arxiv.org/abs/2505.06694", "authors": ["XiaoTong Gu", "Shengyu Tang", "Yiming Cao", "Changdong Yu"], "title": "Underwater object detection in sonar imagery with detection transformer and Zero-shot neural architecture search", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Underwater object detection using sonar imagery has become a critical and\nrapidly evolving research domain within marine technology. However, sonar\nimages are characterized by lower resolution and sparser features compared to\noptical images, which seriously degrades the performance of object detection.To\naddress these challenges, we specifically propose a Detection Transformer\n(DETR) architecture optimized with a Neural Architecture Search (NAS) approach\ncalled NAS-DETR for object detection in sonar images. First, an improved\nZero-shot Neural Architecture Search (NAS) method based on the maximum entropy\nprinciple is proposed to identify a real-time, high-representational-capacity\nCNN-Transformer backbone for sonar image detection. This method enables the\nefficient discovery of high-performance network architectures with low\ncomputational and time overhead. Subsequently, the backbone is combined with a\nFeature Pyramid Network (FPN) and a deformable attention-based Transformer\ndecoder to construct a complete network architecture. This architecture\nintegrates various advanced components and training schemes to enhance overall\nperformance. Extensive experiments demonstrate that this architecture achieves\nstate-of-the-art performance on two Representative datasets, while maintaining\nminimal overhead in real-time efficiency and computational complexity.\nFurthermore, correlation analysis between the key parameters and differential\nentropy-based fitness function is performed to enhance the interpretability of\nthe proposed framework. To the best of our knowledge, this is the first work in\nthe field of sonar object detection to integrate the DETR architecture with a\nNAS search mechanism.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408DETR\u548cNAS\u7684\u67b6\u6784NAS-DETR\uff0c\u7528\u4e8e\u63d0\u5347\u58f0\u7eb3\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0c\u901a\u8fc7\u6539\u8fdb\u7684Zero-shot NAS\u65b9\u6cd5\u548c\u53d8\u5f62\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u51c6\u786e\u7387\u7684\u68c0\u6d4b\u3002", "motivation": "\u58f0\u7eb3\u56fe\u50cf\u5206\u8fa8\u7387\u4f4e\u3001\u7279\u5f81\u7a00\u758f\uff0c\u4f20\u7edf\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u6027\u80fd\u53d7\u9650\uff0c\u4e9f\u9700\u9ad8\u6548\u4e14\u9ad8\u7cbe\u5ea6\u7684\u68c0\u6d4b\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6700\u5927\u71b5\u539f\u5219\u7684\u6539\u8fdbZero-shot NAS\u65b9\u6cd5\u9009\u62e9CNN-Transformer\u4e3b\u5e72\u7f51\u7edc\uff0c\u7ed3\u5408FPN\u548c\u53d8\u5f62\u6ce8\u610f\u529bTransformer\u89e3\u7801\u5668\u6784\u5efa\u5b8c\u6574\u67b6\u6784\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u5f53\u524d\u6700\u4f73\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u5b9e\u65f6\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "NAS-DETR\u9996\u6b21\u5728\u58f0\u7eb3\u76ee\u6807\u68c0\u6d4b\u4e2d\u7ed3\u5408DETR\u4e0eNAS\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6027\u80fd\u5e76\u589e\u5f3a\u4e86\u6846\u67b6\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2505.07554", "pdf": "https://arxiv.org/pdf/2505.07554", "abs": "https://arxiv.org/abs/2505.07554", "authors": ["Erica Coppolillo"], "title": "Injecting Knowledge Graphs into Large Language Models", "categories": ["cs.LG", "cs.IR"], "comment": null, "summary": "Integrating structured knowledge from Knowledge Graphs (KGs) into Large\nLanguage Models (LLMs) remains a key challenge for symbolic reasoning. Existing\nmethods mainly rely on prompt engineering or fine-tuning, which lose structural\nfidelity or incur high computational costs. Building on recent encoding\ntechniques which integrate graph embeddings within the LLM input as tokens, we\nextend this paradigm to the KG domain by leveraging Knowledge Graph Embedding\n(KGE) models, thus enabling graph-aware reasoning. Our approach is\nmodel-agnostic, resource-efficient, and compatible with any LLMs. Extensive\nexperimentation on synthetic and real-world datasets shows that our method\nimproves reasoning performance over established baselines, further achieving\nthe best trade-off in terms of accuracy and efficiency against state-of-the-art\nLLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u5d4c\u5165\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\uff08KGE\uff09\u6a21\u578b\u5b9e\u73b0\u56fe\u611f\u77e5\u63a8\u7406\uff0c\u63d0\u9ad8\u4e86\u63a8\u7406\u6027\u80fd\u5e76\u5e73\u8861\u4e86\u51c6\u786e\u6027\u4e0e\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u63d0\u793a\u5de5\u7a0b\u6216\u5fae\u8c03\uff0c\u4f46\u4f1a\u635f\u5931\u7ed3\u6784\u4fdd\u771f\u5ea6\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e2\u9ad8\u6548\u53c8\u80fd\u4fdd\u6301\u7ed3\u6784\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\uff08KGE\uff09\u6a21\u578b\u5c06\u56fe\u7f16\u7801\u4e3aLLM\u7684\u8f93\u5165\u4ee4\u724c\uff0c\u4ece\u800c\u5b9e\u73b0\u56fe\u611f\u77e5\u63a8\u7406\uff0c\u8be5\u65b9\u6cd5\u4e0e\u6a21\u578b\u65e0\u5173\u4e14\u8d44\u6e90\u9ad8\u6548\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\uff0c\u5e76\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u8fbe\u5230\u4e86\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7b26\u53f7\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u4efb\u4f55LLM\u3002"}}
{"id": "2505.06737", "pdf": "https://arxiv.org/pdf/2505.06737", "abs": "https://arxiv.org/abs/2505.06737", "authors": ["Ahmed Abouelazm", "Jonas Michel", "Helen Gremmelmaier", "Tim Joseph", "Philip Sch\u00f6rner", "J. Marius Z\u00f6llner"], "title": "Balancing Progress and Safety: A Novel Risk-Aware Objective for RL in Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted in the 36th IEEE Intelligent vehicles Symposium (IV 2025)", "summary": "Reinforcement Learning (RL) is a promising approach for achieving autonomous\ndriving due to robust decision-making capabilities. RL learns a driving policy\nthrough trial and error in traffic scenarios, guided by a reward function that\ncombines the driving objectives. The design of such reward function has\nreceived insufficient attention, yielding ill-defined rewards with various\npitfalls. Safety, in particular, has long been regarded only as a penalty for\ncollisions. This leaves the risks associated with actions leading up to a\ncollision unaddressed, limiting the applicability of RL in real-world\nscenarios. To address these shortcomings, our work focuses on enhancing the\nreward formulation by defining a set of driving objectives and structuring them\nhierarchically. Furthermore, we discuss the formulation of these objectives in\na normalized manner to transparently determine their contribution to the\noverall reward. Additionally, we introduce a novel risk-aware objective for\nvarious driving interactions based on a two-dimensional ellipsoid function and\nan extension of Responsibility-Sensitive Safety (RSS) concepts. We evaluate the\nefficacy of our proposed reward in unsignalized intersection scenarios with\nvarying traffic densities. The approach decreases collision rates by 21\\% on\naverage compared to baseline rewards and consistently surpasses them in route\nprogress and cumulative reward, demonstrating its capability to promote safer\ndriving behaviors while maintaining high-performance levels.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u548c\u5f52\u4e00\u5316\u7684\u76ee\u6807\u5b9a\u4e49\uff0c\u7ed3\u5408\u98ce\u9669\u611f\u77e5\u76ee\u6807\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u78b0\u649e\u7387\u964d\u4f4e\u4e8621%\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5b89\u5168\u76ee\u6807\u4ec5\u4f5c\u4e3a\u78b0\u649e\u60e9\u7f5a\uff0c\u800c\u5ffd\u7565\u4e86\u6f5c\u5728\u98ce\u9669\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u66f4\u5168\u9762\u7684\u5956\u52b1\u8bbe\u8ba1\u63d0\u5347RL\u5728\u771f\u5b9e\u573a\u666f\u7684\u9002\u7528\u6027\u3002", "method": "1. \u5206\u5c42\u5b9a\u4e49\u9a7e\u9a76\u76ee\u6807\u5e76\u5f52\u4e00\u5316\uff1b2. \u5f15\u5165\u57fa\u4e8e\u4e8c\u7ef4\u692d\u7403\u51fd\u6570\u548cRSS\u6269\u5c55\u7684\u98ce\u9669\u611f\u77e5\u76ee\u6807\uff1b3. \u5728\u65e0\u4fe1\u53f7\u4ea4\u53c9\u8def\u53e3\u573a\u666f\u4e2d\u9a8c\u8bc1\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u78b0\u649e\u7387\u5e73\u5747\u964d\u4f4e21%\uff0c\u8def\u7ebf\u8fdb\u5ea6\u548c\u7d2f\u79ef\u5956\u52b1\u8868\u73b0\u66f4\u4f18\uff0c\u9a8c\u8bc1\u4e86\u5176\u5b89\u5168\u6027\u548c\u9ad8\u6548\u6027\u3002", "conclusion": "\u6539\u8fdb\u7684\u5956\u52b1\u51fd\u6570\u80fd\u6709\u6548\u5e73\u8861\u5b89\u5168\u4e0e\u6027\u80fd\uff0c\u4e3aRL\u5728\u590d\u6742\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2505.07575", "pdf": "https://arxiv.org/pdf/2505.07575", "abs": "https://arxiv.org/abs/2505.07575", "authors": ["Samuel Erickson", "Mikael Johansson"], "title": "Personalized Federated Learning under Model Dissimilarity Constraints", "categories": ["cs.LG"], "comment": null, "summary": "One of the defining challenges in federated learning is that of statistical\nheterogeneity among clients. We address this problem with KARULA, a regularized\nstrategy for personalized federated learning, which constrains the pairwise\nmodel dissimilarities between clients based on the difference in their\ndistributions, as measured by a surrogate for the 1-Wasserstein distance\nadapted for the federated setting. This allows the strategy to adapt to highly\ncomplex interrelations between clients, that e.g., clustered approaches fail to\ncapture. We propose an inexact projected stochastic gradient algorithm to solve\nthe constrained problem that the strategy defines, and show theoretically that\nit converges with smooth, possibly non-convex losses to a neighborhood of a\nstationary point with rate O(1/K). We demonstrate the effectiveness of KARULA\non synthetic and real federated data sets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKARULA\u7684\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u5ba2\u6237\u95f4\u6a21\u578b\u5dee\u5f02\u6765\u5e94\u5bf9\u7edf\u8ba1\u5f02\u8d28\u6027\uff0c\u5e76\u57fa\u4e8e1-Wasserstein\u8ddd\u79bb\u7684\u66ff\u4ee3\u5ea6\u91cf\u6765\u7ea6\u675f\u5dee\u5f02\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u7cbe\u786e\u6295\u5f71\u968f\u673a\u68af\u5ea6\u7b97\u6cd5\u4ee5\u6c42\u89e3\u95ee\u9898\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u7edf\u8ba1\u5f02\u8d28\u6027\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u4e0d\u540c\u5ba2\u6237\u7684\u6570\u636e\u5206\u5e03\u5dee\u5f02\u8f83\u5927\uff0c\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u805a\u7c7b\uff09\u96be\u4ee5\u6355\u6349\u590d\u6742\u7684\u5ba2\u6237\u95f4\u5173\u7cfb\u3002", "method": "KARULA\u901a\u8fc7\u6b63\u5219\u5316\u5ba2\u6237\u95f4\u6a21\u578b\u5dee\u5f02\uff0c\u5e76\u4f7f\u75281-Wasserstein\u8ddd\u79bb\u7684\u66ff\u4ee3\u5ea6\u91cf\u4f5c\u4e3a\u7ea6\u675f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u7cbe\u786e\u6295\u5f71\u968f\u673a\u68af\u5ea6\u7b97\u6cd5\u6765\u89e3\u51b3\u7ea6\u675f\u95ee\u9898\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u8be5\u7b97\u6cd5\u5728\u5e73\u6ed1\uff08\u53ef\u80fd\u975e\u51f8\uff09\u635f\u5931\u4e0b\u4ee5O(1/K)\u7684\u901f\u7387\u6536\u655b\u5230\u7a33\u5b9a\u70b9\u90bb\u57df\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u8054\u90a6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "KARULA\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u5ba2\u6237\u95f4\u5173\u7cfb\uff0c\u63d0\u5347\u8054\u90a6\u5b66\u4e60\u6027\u80fd\u3002"}}
{"id": "2505.06740", "pdf": "https://arxiv.org/pdf/2505.06740", "abs": "https://arxiv.org/abs/2505.06740", "authors": ["Ahmed Abouelazm", "Mianzhi Liu", "Christian Hubschneider", "Yin Wu", "Daniel Slieter", "J. Marius Z\u00f6llner"], "title": "Boundary-Guided Trajectory Prediction for Road Aware and Physically Feasible Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted in the 36th IEEE Intelligent Vehicles Symposium (IV 2025)", "summary": "Accurate prediction of surrounding road users' trajectories is essential for\nsafe and efficient autonomous driving. While deep learning models have improved\nperformance, challenges remain in preventing off-road predictions and ensuring\nkinematic feasibility. Existing methods incorporate road-awareness modules and\nenforce kinematic constraints but lack plausibility guarantees and often\nintroduce trade-offs in complexity and flexibility. This paper proposes a novel\nframework that formulates trajectory prediction as a constrained regression\nguided by permissible driving directions and their boundaries. Using the\nagent's current state and an HD map, our approach defines the valid boundaries\nand ensures on-road predictions by training the network to learn superimposed\npaths between left and right boundary polylines. To guarantee feasibility, the\nmodel predicts acceleration profiles that determine the vehicle's travel\ndistance along these paths while adhering to kinematic constraints. We evaluate\nour approach on the Argoverse-2 dataset against the HPTR baseline. Our approach\nshows a slight decrease in benchmark metrics compared to HPTR but notably\nimproves final displacement error and eliminates infeasible trajectories.\nMoreover, the proposed approach has superior generalization to less prevalent\nmaneuvers and unseen out-of-distribution scenarios, reducing the off-road rate\nunder adversarial attacks from 66\\% to just 1\\%. These results highlight the\neffectiveness of our approach in generating feasible and robust predictions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8f68\u8ff9\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8f68\u8ff9\u9884\u6d4b\u5efa\u6a21\u4e3a\u53d7\u5141\u8bb8\u884c\u9a76\u65b9\u5411\u548c\u8fb9\u754c\u7ea6\u675f\u7684\u56de\u5f52\u95ee\u9898\uff0c\u786e\u4fdd\u9884\u6d4b\u7ed3\u679c\u5728\u9053\u8def\u8303\u56f4\u5185\u5e76\u6ee1\u8db3\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4e0d\u53ef\u884c\u8f68\u8ff9\u6bd4\u4f8b\u548c\u8131\u8f68\u7387\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u51c6\u786e\u9884\u6d4b\u5468\u56f4\u9053\u8def\u7528\u6237\u7684\u8f68\u8ff9\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u9632\u6b62\u8131\u8f68\u9884\u6d4b\u548c\u4fdd\u8bc1\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u5408\u7406\u6027\u4fdd\u8bc1\u4e14\u5e38\u5728\u590d\u6742\u6027\u548c\u7075\u6d3b\u6027\u4e4b\u95f4\u6743\u8861\u3002", "method": "\u6846\u67b6\u4f7f\u7528\u5f53\u524d\u72b6\u6001\u548c\u9ad8\u6e05\u5730\u56fe\uff0c\u5b9a\u4e49\u6709\u6548\u8fb9\u754c\u5e76\u901a\u8fc7\u8bad\u7ec3\u7f51\u7edc\u5b66\u4e60\u5de6\u53f3\u8fb9\u754c\u6298\u7ebf\u95f4\u7684\u53e0\u52a0\u8def\u5f84\uff0c\u9884\u6d4b\u52a0\u901f\u5ea6\u8f6e\u5ed3\u4ee5\u786e\u4fdd\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u3002", "result": "\u5728Argoverse-2\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4HPTR\u57fa\u7ebf\uff0c\u6700\u7ec8\u4f4d\u79fb\u8bef\u5dee\u964d\u4f4e\u4e14\u4e0d\u53ef\u884c\u8f68\u8ff9\u5b8c\u5168\u6d88\u9664\uff0c\u5bf9\u7f55\u89c1\u64cd\u4f5c\u548c\u5206\u5e03\u5916\u573a\u666f\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff0c\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u8131\u8f68\u7387\u4ece66%\u964d\u81f31%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u9884\u6d4b\u7ed3\u679c\u65e2\u53ef\u884c\u53c8\u9c81\u68d2\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.07614", "pdf": "https://arxiv.org/pdf/2505.07614", "abs": "https://arxiv.org/abs/2505.07614", "authors": ["Gleb Molodtsov", "Daniil Medyakov", "Sergey Skorik", "Nikolas Khachaturov", "Shahane Tigranyan", "Vladimir Aletov", "Aram Avetisyan", "Martin Tak\u00e1\u010d", "Aleksandr Beznosikov"], "title": "Trial and Trust: Addressing Byzantine Attacks with Comprehensive Defense Strategy", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Recent advancements in machine learning have improved performance while also\nincreasing computational demands. While federated and distributed setups\naddress these issues, their structure is vulnerable to malicious influences. In\nthis paper, we address a specific threat, Byzantine attacks, where compromised\nclients inject adversarial updates to derail global convergence. We combine the\ntrust scores concept with trial function methodology to dynamically filter\noutliers. Our methods address the critical limitations of previous approaches,\nallowing functionality even when Byzantine nodes are in the majority. Moreover,\nour algorithms adapt to widely used scaled methods like Adam and RMSProp, as\nwell as practical scenarios, including local training and partial\nparticipation. We validate the robustness of our methods by conducting\nextensive experiments on both synthetic and real ECG data collected from\nmedical institutions. Furthermore, we provide a broad theoretical analysis of\nour algorithms and their extensions to aforementioned practical setups. The\nconvergence guarantees of our methods are comparable to those of classical\nalgorithms developed without Byzantine interference.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4fe1\u4efb\u5206\u6570\u548c\u8bd5\u51fd\u6570\u65b9\u6cd5\u7684\u65b0\u7b97\u6cd5\uff0c\u52a8\u6001\u8fc7\u6ee4\u6076\u610f\u66f4\u65b0\uff0c\u6709\u6548\u62b5\u5fa1\u62dc\u5360\u5ead\u653b\u51fb\uff0c\u5e76\u9002\u7528\u4e8e\u591a\u79cd\u5b9e\u9645\u573a\u666f\uff08\u5982Adam/RMSProp\u4f18\u5316\u5668\u3001\u672c\u5730\u8bad\u7ec3\u7b49\uff09\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u5408\u6210\u548c\u771f\u5b9e\u533b\u7597\u6570\u636e\u4e0a\u7684\u9c81\u68d2\u6027\uff0c\u7406\u8bba\u5206\u6790\u4e5f\u8bc1\u660e\u4e86\u5176\u6536\u655b\u6027\u4e0e\u65e0\u653b\u51fb\u60c5\u51b5\u4e0b\u7684\u7ecf\u5178\u7b97\u6cd5\u76f8\u5f53\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u7684\u8ba1\u7b97\u9700\u6c42\u589e\u957f\uff0c\u8054\u90a6\u5b66\u4e60\u4e0e\u5206\u5e03\u5f0f\u67b6\u6784\u7684\u8106\u5f31\u6027\uff08\u5982\u62dc\u5360\u5ead\u653b\u51fb\uff09\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u9762\u5bf9\u591a\u6570\u8282\u70b9\u6076\u610f\u65f6\u5931\u6548\uff0c\u9700\u52a8\u6001\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4fe1\u4efb\u5206\u6570\u4e0e\u8bd5\u51fd\u6570\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u52a8\u6001\u8bc6\u522b\u5e76\u8fc7\u6ee4\u5f02\u5e38\u66f4\u65b0\u3002\u7b97\u6cd5\u517c\u5bb9\u4e3b\u6d41\u4f18\u5316\u5668\uff08\u5982Adam\u3001RMSProp\uff09\u53ca\u672c\u5730\u8bad\u7ec3\u3001\u90e8\u5206\u53c2\u4e0e\u7b49\u5b9e\u9645\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9eECG\u6570\u636e\u4e2d\uff0c\u7b97\u6cd5\u80fd\u6709\u6548\u62b5\u5fa1\u62dc\u5360\u5ead\u653b\u51fb\uff08\u5373\u4f7f\u6076\u610f\u8282\u70b9\u5360\u591a\u6570\uff09\u3002\u7406\u8bba\u5206\u6790\u663e\u793a\u5176\u6536\u655b\u6027\u63a5\u8fd1\u65e0\u653b\u51fb\u7684\u7ecf\u5178\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u62dc\u5360\u5ead\u653b\u51fb\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u9c81\u68d2\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u517c\u5177\u7406\u8bba\u4fdd\u8bc1\u4e0e\u5b9e\u6218\u9002\u5e94\u6027\u3002"}}
{"id": "2505.06743", "pdf": "https://arxiv.org/pdf/2505.06743", "abs": "https://arxiv.org/abs/2505.06743", "authors": ["Marius Baden", "Ahmed Abouelazm", "Christian Hubschneider", "Yin Wu", "Daniel Slieter", "J. Marius Z\u00f6llner"], "title": "TPK: Trustworthy Trajectory Prediction Integrating Prior Knowledge For Interpretability and Kinematic Feasibility", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted in the 36th IEEE Intelligent Vehicles Symposium (IV 2025)\n  for oral presentation", "summary": "Trajectory prediction is crucial for autonomous driving, enabling vehicles to\nnavigate safely by anticipating the movements of surrounding road users.\nHowever, current deep learning models often lack trustworthiness as their\npredictions can be physically infeasible and illogical to humans. To make\npredictions more trustworthy, recent research has incorporated prior knowledge,\nlike the social force model for modeling interactions and kinematic models for\nphysical realism. However, these approaches focus on priors that suit either\nvehicles or pedestrians and do not generalize to traffic with mixed agent\nclasses. We propose incorporating interaction and kinematic priors of all agent\nclasses--vehicles, pedestrians, and cyclists with class-specific interaction\nlayers to capture agent behavioral differences. To improve the interpretability\nof the agent interactions, we introduce DG-SFM, a rule-based interaction\nimportance score that guides the interaction layer. To ensure physically\nfeasible predictions, we proposed suitable kinematic models for all agent\nclasses with a novel pedestrian kinematic model. We benchmark our approach on\nthe Argoverse 2 dataset, using the state-of-the-art transformer HPTR as our\nbaseline. Experiments demonstrate that our method improves interaction\ninterpretability, revealing a correlation between incorrect predictions and\ndivergence from our interaction prior. Even though incorporating the kinematic\nmodels causes a slight decrease in accuracy, they eliminate infeasible\ntrajectories found in the dataset and the baseline model. Thus, our approach\nfosters trust in trajectory prediction as its interaction reasoning is\ninterpretable, and its predictions adhere to physics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4ea4\u4e92\u548c\u8fd0\u52a8\u5b66\u5148\u9a8c\u7684\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4ea4\u901a\u53c2\u4e0e\u8005\uff08\u8f66\u8f86\u3001\u884c\u4eba\u3001\u81ea\u884c\u8f66\uff09\uff0c\u901a\u8fc7\u7c7b\u7279\u5b9a\u4ea4\u4e92\u5c42\u548c\u89c4\u5219\u5316\u7684\u4ea4\u4e92\u91cd\u8981\u6027\u8bc4\u5206\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5728\u7269\u7406\u53ef\u884c\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u8f68\u8ff9\u9884\u6d4b\u7684\u4fe1\u4efb\u95ee\u9898\uff1a\u73b0\u6709\u6a21\u578b\u7684\u9884\u6d4b\u53ef\u80fd\u4e0d\u7b26\u5408\u7269\u7406\u6216\u903b\u8f91\uff0c\u4e14\u901a\u5e38\u53ea\u9488\u5bf9\u5355\u4e00\u4ea4\u901a\u53c2\u4e0e\u8005\u8bbe\u8ba1\u3002", "method": "\u63d0\u51fa\u7c7b\u7279\u5b9a\u4ea4\u4e92\u5c42\uff08\u6355\u6349\u4e0d\u540c\u4ea4\u901a\u53c2\u4e0e\u8005\u7684\u884c\u4e3a\u5dee\u5f02\uff09\u548cDG-SFM\u4ea4\u4e92\u91cd\u8981\u6027\u8bc4\u5206\uff08\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\uff09\uff0c\u5e76\u8bbe\u8ba1\u9002\u5408\u5404\u7c7b\u522b\u7684\u8fd0\u52a8\u5b66\u6a21\u578b\uff08\u5305\u62ec\u65b0\u884c\u4eba\u8fd0\u52a8\u5b66\u6a21\u578b\uff09\u3002", "result": "\u5728Argoverse 2\u6570\u636e\u96c6\u4e0a\uff0c\u4ea4\u4e92\u53ef\u89e3\u91ca\u6027\u63d0\u5347\uff0c\u9519\u8bef\u9884\u6d4b\u4e0e\u4ea4\u4e92\u5148\u9a8c\u7684\u504f\u79bb\u76f8\u5173\uff1b\u8fd0\u52a8\u5b66\u6a21\u578b\u867d\u8f7b\u5fae\u964d\u4f4e\u7cbe\u5ea6\uff0c\u4f46\u6d88\u9664\u4e86\u4e0d\u53ef\u884c\u8f68\u8ff9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u4ea4\u4e92\u63a8\u7406\u548c\u7b26\u5408\u7269\u7406\u7684\u9884\u6d4b\uff0c\u63d0\u5347\u4e86\u8f68\u8ff9\u9884\u6d4b\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2505.07629", "pdf": "https://arxiv.org/pdf/2505.07629", "abs": "https://arxiv.org/abs/2505.07629", "authors": ["Yizhou Ma", "Zhuoqin Yang", "Luis-Daniel Ib\u00e1\u00f1ez"], "title": "Enhancing Federated Learning with Kolmogorov-Arnold Networks: A Comparative Study Across Diverse Aggregation Strategies", "categories": ["cs.LG"], "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. It was prepared prior to submission to, and has\n  since been accepted at, ICIC 2025. The final Version of Record will be\n  published in the ICIC 2025 proceedings by Springer", "summary": "Multilayer Perceptron (MLP), as a simple yet powerful model, continues to be\nwidely used in classification and regression tasks. However, traditional MLPs\noften struggle to efficiently capture nonlinear relationships in load data when\ndealing with complex datasets. Kolmogorov-Arnold Networks (KAN), inspired by\nthe Kolmogorov-Arnold representation theorem, have shown promising capabilities\nin modeling complex nonlinear relationships. In this study, we explore the\nperformance of KANs within federated learning (FL) frameworks and compare them\nto traditional Multilayer Perceptrons. Our experiments, conducted across four\ndiverse datasets demonstrate that KANs consistently outperform MLPs in terms of\naccuracy, stability, and convergence efficiency. KANs exhibit remarkable\nrobustness under varying client numbers and non-IID data distributions,\nmaintaining superior performance even as client heterogeneity increases.\nNotably, KANs require fewer communication rounds to converge compared to MLPs,\nhighlighting their efficiency in FL scenarios. Additionally, we evaluate\nmultiple parameter aggregation strategies, with trimmed mean and FedProx\nemerging as the most effective for optimizing KAN performance. These findings\nestablish KANs as a robust and scalable alternative to MLPs for federated\nlearning tasks, paving the way for their application in decentralized and\nprivacy-preserving environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8Kolmogorov-Arnold Networks (KAN)\u5728\u8054\u90a6\u5b66\u4e60(FL)\u4e2d\u7684\u6027\u80fd\uff0c\u76f8\u6bd4\u4f20\u7edf\u591a\u5c42\u611f\u77e5\u673a(MLP)\uff0cKAN\u5728\u51c6\u786e\u6027\u3001\u7a33\u5b9a\u6027\u548c\u6536\u655b\u6548\u7387\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u5c24\u5176\u662f\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u548c\u5ba2\u6237\u7aef\u5f02\u6784\u6027\u9ad8\u7684\u573a\u666f\u4e0b\u3002", "motivation": "\u4f20\u7edf\u7684MLP\u5728\u5904\u7406\u590d\u6742\u6570\u636e\u65f6\u96be\u4ee5\u9ad8\u6548\u6355\u6349\u975e\u7ebf\u6027\u5173\u7cfb\uff0c\u800cKAN\u57fa\u4e8eKolmogorov-Arnold\u8868\u793a\u5b9a\u7406\uff0c\u6709\u671b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7FL\u6846\u67b6\u9a8c\u8bc1KAN\u7684\u6027\u80fd\u4f18\u52bf\u3002", "method": "\u5728\u56db\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u5bf9\u6bd4KAN\u548cMLP\u7684\u8868\u73b0\uff0c\u8bc4\u4f30\u53c2\u6570\u805a\u5408\u7b56\u7565\uff08\u5982trimmed mean\u548cFedProx\uff09\uff0c\u5206\u6790KAN\u5728\u5ba2\u6237\u7aef\u6570\u91cf\u548c\u4e0d\u540c\u6570\u636e\u5206\u5e03\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "result": "KAN\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8eMLP\uff0c\u5c24\u5176\u662f\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u4e2d\u8868\u73b0\u7a33\u5065\uff0c\u4e14\u6536\u655b\u6240\u9700\u901a\u4fe1\u8f6e\u6b21\u66f4\u5c11\u3002trimmed mean\u548cFedProx\u5bf9KAN\u6027\u80fd\u4f18\u5316\u6548\u679c\u6700\u4f73\u3002", "conclusion": "KAN\u662f\u8054\u90a6\u5b66\u4e60\u4e2d\u6bd4MLP\u66f4\u9ad8\u6548\u3001\u7a33\u5065\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u53bb\u4e2d\u5fc3\u5316\u548c\u9690\u79c1\u4fdd\u62a4\u573a\u666f\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.06745", "pdf": "https://arxiv.org/pdf/2505.06745", "abs": "https://arxiv.org/abs/2505.06745", "authors": ["Parth Padalkar", "Gopal Gupta"], "title": "Symbolic Rule Extraction from Attention-Guided Sparse Representations in Vision Transformers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent neuro-symbolic approaches have successfully extracted symbolic\nrule-sets from CNN-based models to enhance interpretability. However, applying\nsimilar techniques to Vision Transformers (ViTs) remains challenging due to\ntheir lack of modular concept detectors and reliance on global self-attention\nmechanisms. We propose a framework for symbolic rule extraction from ViTs by\nintroducing a sparse concept layer inspired by Sparse Autoencoders (SAEs). This\nlinear layer operates on attention-weighted patch representations and learns a\ndisentangled, binarized representation in which individual neurons activate for\nhigh-level visual concepts. To encourage interpretability, we apply a\ncombination of L1 sparsity, entropy minimization, and supervised contrastive\nloss. These binarized concept activations are used as input to the FOLD-SE-M\nalgorithm, which generates a rule-set in the form of logic programs. Our method\nachieves a 5.14% better classification accuracy than the standard ViT while\nenabling symbolic reasoning. Crucially, the extracted rule-set is not merely\npost-hoc but acts as a logic-based decision layer that operates directly on the\nsparse concept representations. The resulting programs are concise and\nsemantically meaningful. This work is the first to extract executable logic\nprograms from ViTs using sparse symbolic representations. It bridges the gap\nbetween transformer-based vision models and symbolic logic programming,\nproviding a step forward in interpretable and verifiable neuro-symbolic AI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4eceViT\u4e2d\u63d0\u53d6\u7b26\u53f7\u89c4\u5219\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u7a00\u758f\u6982\u5ff5\u5c42\u548cFOLD-SE-M\u7b97\u6cd5\u751f\u6210\u903b\u8f91\u7a0b\u5e8f\uff0c\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5c3d\u7ba1CNN\u5df2\u6709\u7b26\u53f7\u89c4\u5219\u63d0\u53d6\u65b9\u6cd5\uff0c\u4f46ViT\u56e0\u5176\u7f3a\u4e4f\u6a21\u5757\u5316\u6982\u5ff5\u68c0\u6d4b\u5668\u548c\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\u800c\u96be\u4ee5\u5e94\u7528\u7c7b\u4f3c\u6280\u672f\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u65b9\u6cd5\u589e\u5f3aViT\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5f15\u5165\u7a00\u758f\u81ea\u7f16\u7801\u5668\u542f\u53d1\u7684\u7a00\u758f\u6982\u5ff5\u5c42\uff0c\u7ed3\u5408L1\u7a00\u758f\u3001\u71b5\u6700\u5c0f\u5316\u548c\u76d1\u7763\u5bf9\u6bd4\u635f\u5931\uff0c\u751f\u6210\u4e8c\u503c\u5316\u6982\u5ff5\u6fc0\u6d3b\uff0c\u518d\u7528FOLD-SE-M\u7b97\u6cd5\u751f\u6210\u903b\u8f91\u7a0b\u5e8f\u3002", "result": "\u5206\u7c7b\u51c6\u786e\u7387\u6bd4\u6807\u51c6ViT\u9ad85.14%\uff0c\u751f\u6210\u7684\u903b\u8f91\u7a0b\u5e8f\u7b80\u6d01\u4e14\u8bed\u4e49\u660e\u786e\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u4eceViT\u5230\u7b26\u53f7\u903b\u8f91\u7f16\u7a0b\u7684\u8f6c\u6362\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8fde\u63a5\u4e86ViT\u4e0e\u7b26\u53f7\u903b\u8f91\u7f16\u7a0b\uff0c\u63a8\u52a8\u4e86\u53ef\u89e3\u91ca\u3001\u53ef\u9a8c\u8bc1\u7684\u795e\u7ecf\u7b26\u53f7AI\u53d1\u5c55\u3002"}}
{"id": "2505.07635", "pdf": "https://arxiv.org/pdf/2505.07635", "abs": "https://arxiv.org/abs/2505.07635", "authors": ["Dazhuo Qiu", "Haolai Che", "Arijit Khan", "Yinghui Wu"], "title": "Generating Skyline Explanations for Graph Neural Networks", "categories": ["cs.LG", "cs.DB"], "comment": null, "summary": "This paper proposes a novel approach to generate subgraph explanations for\ngraph neural networks GNNs that simultaneously optimize multiple measures for\nexplainability. Existing GNN explanation methods often compute subgraphs\n(called ``explanatory subgraphs'') that optimize a pre-defined, single\nexplainability measure, such as fidelity or conciseness. This can lead to\nbiased explanations that cannot provide a comprehensive explanation to clarify\nthe output of GNN models. We introduce skyline explanation, a GNN explanation\nparadigm that aims to identify k explanatory subgraphs by simultaneously\noptimizing multiple explainability measures. (1) We formulate skyline\nexplanation generation as a multi-objective optimization problem, and pursue\nexplanations that approximate a skyline set of explanatory subgraphs. We show\nthe hardness for skyline explanation generation. (2) We design efficient\nalgorithms with an onion-peeling approach that strategically removes edges from\nneighbors of nodes of interests, and incrementally improves explanations as it\nexplores an interpretation domain, with provable quality guarantees. (3) We\nfurther develop an algorithm to diversify explanations to provide more\ncomprehensive perspectives. Using real-world graphs, we empirically verify the\neffectiveness, efficiency, and scalability of our algorithms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u540c\u65f6\u4f18\u5316\u591a\u79cd\u53ef\u89e3\u91ca\u6027\u6307\u6807\u6765\u751f\u6210\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u5b50\u56fe\u89e3\u91ca\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u56e0\u5355\u6307\u6807\u4f18\u5316\u5bfc\u81f4\u7684\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u73b0\u6709GNN\u89e3\u91ca\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u5355\u4e00\u53ef\u89e3\u91ca\u6027\u6307\u6807\u751f\u6210\u5b50\u56fe\u89e3\u91ca\uff0c\u53ef\u80fd\u5bfc\u81f4\u504f\u5dee\u4e14\u7f3a\u4e4f\u5168\u9762\u6027\uff0c\u65e0\u6cd5\u5145\u5206\u9610\u660eGNN\u7684\u8f93\u51fa\u3002", "method": "\u63d0\u51fa\u5929\u9645\u7ebf\u89e3\u91ca\u8303\u5f0f\uff0c\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u591a\u76ee\u6807\u4f18\u5316\uff0c\u8bbe\u8ba1\u9ad8\u6548\u7684\u57fa\u4e8e\u6d0b\u8471\u5265\u76ae\u6cd5\u7684\u7b97\u6cd5\uff0c\u9010\u6b65\u4f18\u5316\u5b50\u56fe\u89e3\u91ca\uff0c\u5e76\u63d0\u4f9b\u591a\u6837\u6027\u7b97\u6cd5\u4ee5\u589e\u5f3a\u89e3\u91ca\u5168\u9762\u6027\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u56fe\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3001\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u5929\u9645\u7ebf\u89e3\u91ca\u65b9\u6cd5\u901a\u8fc7\u591a\u6307\u6807\u4f18\u5316\u548c\u591a\u6837\u6027\u589e\u5f3a\uff0c\u4e3aGNN\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u3001\u65e0\u504f\u7684\u89e3\u91ca\u3002"}}
{"id": "2505.07674", "pdf": "https://arxiv.org/pdf/2505.07674", "abs": "https://arxiv.org/abs/2505.07674", "authors": ["Nan Jiang", "Wenxuan Zhu", "Xu Han", "Weiqiang Huang", "Yumeng Sun"], "title": "Joint Graph Convolution and Sequential Modeling for Scalable Network Traffic Estimation", "categories": ["cs.LG"], "comment": null, "summary": "This study focuses on the challenge of predicting network traffic within\ncomplex topological environments. It introduces a spatiotemporal modeling\napproach that integrates Graph Convolutional Networks (GCN) with Gated\nRecurrent Units (GRU). The GCN component captures spatial dependencies among\nnetwork nodes, while the GRU component models the temporal evolution of traffic\ndata. This combination allows for precise forecasting of future traffic\npatterns. The effectiveness of the proposed model is validated through\ncomprehensive experiments on the real-world Abilene network traffic dataset.\nThe model is benchmarked against several popular deep learning methods.\nFurthermore, a set of ablation experiments is conducted to examine the\ninfluence of various components on performance, including changes in the number\nof graph convolution layers, different temporal modeling strategies, and\nmethods for constructing the adjacency matrix. Results indicate that the\nproposed approach achieves superior performance across multiple metrics,\ndemonstrating robust stability and strong generalization capabilities in\ncomplex network traffic forecasting scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u548c\u95e8\u63a7\u5faa\u73af\u5355\u5143\uff08GRU\uff09\u7684\u65f6\u7a7a\u6a21\u578b\uff0c\u7528\u4e8e\u590d\u6742\u62d3\u6251\u73af\u5883\u4e2d\u7684\u7f51\u7edc\u6d41\u91cf\u9884\u6d4b\u3002\u901a\u8fc7\u771f\u5b9e\u7f51\u7edc\u6570\u636e\u96c6\u9a8c\u8bc1\uff0c\u8be5\u6a21\u578b\u5728\u591a\u9879\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u590d\u6742\u62d3\u6251\u73af\u5883\u4e2d\u7684\u7f51\u7edc\u6d41\u91cf\u9884\u6d4b\u662f\u4e00\u4e2a\u6311\u6218\u6027\u95ee\u9898\uff0c\u9700\u8981\u540c\u65f6\u6355\u6349\u7a7a\u95f4\u4f9d\u8d56\u548c\u65f6\u95f4\u6f14\u5316\u7279\u5f81\u3002", "method": "\u63d0\u51faGCN-GRU\u6df7\u5408\u6a21\u578b\uff1aGCN\u5904\u7406\u7f51\u7edc\u8282\u70b9\u7684\u7a7a\u95f4\u5173\u7cfb\uff0cGRU\u5efa\u6a21\u6d41\u91cf\u6570\u636e\u7684\u65f6\u95f4\u52a8\u6001\u6027\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u5206\u6790\u5404\u7ec4\u4ef6\u5f71\u54cd\u3002", "result": "\u6a21\u578b\u5728Abilene\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u9c81\u68d2\u7684\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GCN\u4e0eGRU\u7684\u7ed3\u5408\u80fd\u6709\u6548\u63d0\u5347\u590d\u6742\u7f51\u7edc\u6d41\u91cf\u9884\u6d4b\u7684\u7cbe\u5ea6\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2505.06799", "pdf": "https://arxiv.org/pdf/2505.06799", "abs": "https://arxiv.org/abs/2505.06799", "authors": ["Erik L. Connerty", "Ethan N. Evans", "Gerasimos Angelatos", "Vignesh Narayanan"], "title": "Quantum Observers: A NISQ Hardware Demonstration of Chaotic State Prediction Using Quantum Echo-state Networks", "categories": ["quant-ph", "cs.AI"], "comment": "14 pages, 12 figures", "summary": "Recent advances in artificial intelligence have highlighted the remarkable\ncapabilities of neural network (NN)-powered systems on classical computers.\nHowever, these systems face significant computational challenges that limit\nscalability and efficiency. Quantum computers hold the potential to overcome\nthese limitations and increase processing power beyond classical systems.\nDespite this, integrating quantum computing with NNs remains largely unrealized\ndue to challenges posed by noise, decoherence, and high error rates in current\nquantum hardware. Here, we propose a novel quantum echo-state network (QESN)\ndesign and implementation algorithm that can operate within the presence of\nnoise on current IBM hardware. We apply classical control-theoretic response\nanalysis to characterize the QESN, emphasizing its rich nonlinear dynamics and\nmemory, as well as its ability to be fine-tuned with sparsity and re-uploading\nblocks. We validate our approach through a comprehensive demonstration of QESNs\nfunctioning as quantum observers, applied in both high-fidelity simulations and\nhardware experiments utilizing data from a prototypical chaotic Lorenz system.\nOur results show that the QESN can predict long time-series with persistent\nmemory, running over 100 times longer than the median T}1 and T2 of the IBM\nMarrakesh QPU, achieving state-of-the-art time-series performance on\nsuperconducting hardware.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u91cf\u5b50\u56de\u58f0\u72b6\u6001\u7f51\u7edc\uff08QESN\uff09\u8bbe\u8ba1\u53ca\u5b9e\u73b0\u7b97\u6cd5\uff0c\u80fd\u591f\u5728\u5f53\u524dIBM\u91cf\u5b50\u786c\u4ef6\u566a\u58f0\u73af\u5883\u4e0b\u8fd0\u884c\uff0c\u5e76\u901a\u8fc7\u7ecf\u5178\u63a7\u5236\u7406\u8bba\u5206\u6790\u9a8c\u8bc1\u5176\u975e\u7ebf\u6027\u52a8\u6001\u548c\u8bb0\u5fc6\u80fd\u529b\uff0c\u5b9e\u9a8c\u8868\u660eQESN\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u5728\u7ecf\u5178\u8ba1\u7b97\u673a\u4e0a\u7684\u8ba1\u7b97\u9650\u5236\uff0c\u63a2\u7d22\u91cf\u5b50\u8ba1\u7b97\u4e0e\u795e\u7ecf\u7f51\u7edc\u7684\u7ed3\u5408\uff0c\u514b\u670d\u5f53\u524d\u91cf\u5b50\u786c\u4ef6\u7684\u566a\u58f0\u548c\u9000\u76f8\u5e72\u95ee\u9898\u3002", "method": "\u63d0\u51faQESN\u8bbe\u8ba1\u53ca\u5b9e\u73b0\u7b97\u6cd5\uff0c\u7ed3\u5408\u7ecf\u5178\u63a7\u5236\u7406\u8bba\u5206\u6790\u5176\u52a8\u6001\u7279\u6027\uff0c\u5e76\u901a\u8fc7\u9ad8\u4fdd\u771f\u6a21\u62df\u548c\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "QESN\u80fd\u5728\u566a\u58f0\u73af\u5883\u4e0b\u8fd0\u884c\uff0c\u9884\u6d4b\u957f\u65f6\u95f4\u5e8f\u5217\uff0c\u6027\u80fd\u4f18\u4e8eIBM\u91cf\u5b50\u5904\u7406\u5668\u7684\u4e2d\u4f4dT1\u548cT2\u65f6\u95f4\u3002", "conclusion": "QESN\u5728\u91cf\u5b50\u786c\u4ef6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u91cf\u5b50\u8ba1\u7b97\u4e0e\u795e\u7ecf\u7f51\u7edc\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.07675", "pdf": "https://arxiv.org/pdf/2505.07675", "abs": "https://arxiv.org/abs/2505.07675", "authors": ["Seongjae Kang", "Dong Bok Lee", "Hyungjoon Jang", "Sung Ju Hwang"], "title": "Simple Semi-supervised Knowledge Distillation from Vision-Language Models via $\\mathbf{\\texttt{D}}$ual-$\\mathbf{\\texttt{H}}$ead $\\mathbf{\\texttt{O}}$ptimization", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "41 pages, 19 figures, preprint", "summary": "Vision-language models (VLMs) have achieved remarkable success across diverse\ntasks by leveraging rich textual information with minimal labeled data.\nHowever, deploying such large models remains challenging, particularly in\nresource-constrained environments. Knowledge distillation (KD) offers a\nwell-established solution to this problem; however, recent KD approaches from\nVLMs often involve multi-stage training or additional tuning, increasing\ncomputational overhead and optimization complexity. In this paper, we propose\n$\\mathbf{\\texttt{D}}$ual-$\\mathbf{\\texttt{H}}$ead\n$\\mathbf{\\texttt{O}}$ptimization ($\\mathbf{\\texttt{DHO}}$) -- a simple yet\neffective KD framework that transfers knowledge from VLMs to compact,\ntask-specific models in semi-supervised settings. Specifically, we introduce\ndual prediction heads that independently learn from labeled data and teacher\npredictions, and propose to linearly combine their outputs during inference. We\nobserve that $\\texttt{DHO}$ mitigates gradient conflicts between supervised and\ndistillation signals, enabling more effective feature learning than single-head\nKD baselines. As a result, extensive experiments show that $\\texttt{DHO}$\nconsistently outperforms baselines across multiple domains and fine-grained\ndatasets. Notably, on ImageNet, it achieves state-of-the-art performance,\nimproving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively,\nwhile using fewer parameters.", "AI": {"tldr": "\u63d0\u51fa\u4e86DHO\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u9884\u6d4b\u5934\u7b80\u5316\u77e5\u8bc6\u84b8\u998f\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u90e8\u7f72\u7684\u6311\u6218\uff0c\u907f\u514d\u591a\u9636\u6bb5\u8bad\u7ec3\u6216\u989d\u5916\u8c03\u53c2\u7684\u590d\u6742\u6027\u3002", "method": "\u5f15\u5165\u53cc\u9884\u6d4b\u5934\u72ec\u7acb\u5b66\u4e60\u6807\u8bb0\u6570\u636e\u548c\u6559\u5e08\u9884\u6d4b\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u7ebf\u6027\u7ec4\u5408\u8f93\u51fa\u3002", "result": "\u5728\u591a\u9879\u5b9e\u9a8c\u548c\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cImageNet\u4e0a1%\u548c10%\u6807\u8bb0\u6570\u636e\u5206\u522b\u63d0\u53473%\u548c0.1%\u51c6\u786e\u7387\u3002", "conclusion": "DHO\u6846\u67b6\u6709\u6548\u89e3\u51b3\u68af\u5ea6\u51b2\u7a81\uff0c\u63d0\u5347\u7279\u5f81\u5b66\u4e60\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u573a\u666f\u3002"}}
{"id": "2505.07680", "pdf": "https://arxiv.org/pdf/2505.07680", "abs": "https://arxiv.org/abs/2505.07680", "authors": ["Hang Wu", "Jianian Zhu", "Yinghui Li", "Haojie Wang", "Biao Hou", "Jidong Zhai"], "title": "SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in Large Language Models", "categories": ["cs.LG", "cs.DC"], "comment": "10 pages", "summary": "Large Language Models (LLMs) present a critical trade-off between inference\nquality and computational cost: larger models offer superior capabilities but\nincur significant latency, while smaller models are faster but less powerful.\nExisting serving strategies often employ fixed model scales or static two-stage\nspeculative decoding, failing to dynamically adapt to the varying complexities\nof user requests or fluctuations in system performance. This paper introduces\n\\systemname{}, a novel framework that reimagines LLM inference as an adaptive\nrouting problem solved through multi-level speculative decoding. \\systemname{}\ndynamically constructs and optimizes inference \"paths\" (chains of models) based\non real-time feedback, addressing the limitations of static approaches. Our\ncontributions are threefold: (1) An \\textbf{adaptive model chain scheduling}\nmechanism that leverages performance profiling (execution times) and predictive\nsimilarity metrics (derived from token distribution divergence) to continuously\nselect the optimal sequence of draft and verifier models, minimizing predicted\nlatency per generated token. (2) A \\textbf{multi-level collaborative\nverification} framework where intermediate models within the selected chain can\nvalidate speculative tokens, reducing the verification burden on the final,\nmost powerful target model. (3) A \\textbf{synchronized state management} system\nproviding efficient, consistent KV cache handling across heterogeneous models\nin the chain, including precise, low-overhead rollbacks tailored for\nasynchronous batch processing inherent in multi-level speculation. Preliminary\nexperiments demonstrate the validity of our method.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\\systemname{}\u7684\u52a8\u6001\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c42\u6b21\u63a8\u6d4b\u6027\u89e3\u7801\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u6839\u636e\u5b9e\u65f6\u53cd\u9988\u9009\u62e9\u6700\u4f18\u6a21\u578b\u94fe\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u8d28\u91cf\u548c\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u73b0\u6709\u9759\u6001\u7b56\u7565\u65e0\u6cd5\u52a8\u6001\u9002\u5e94\u7528\u6237\u8bf7\u6c42\u7684\u590d\u6742\u6027\u6216\u7cfb\u7edf\u6027\u80fd\u6ce2\u52a8\u3002\\systemname{}\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u81ea\u9002\u5e94\u6a21\u578b\u94fe\u8c03\u5ea6\u3001\u591a\u5c42\u6b21\u534f\u540c\u9a8c\u8bc1\u548c\u540c\u6b65\u72b6\u6001\u7ba1\u7406\uff0c\u52a8\u6001\u4f18\u5316\u6a21\u578b\u94fe\u4ee5\u51cf\u5c11\u5ef6\u8fdf\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\\systemname{}\u901a\u8fc7\u52a8\u6001\u8def\u7531\u548c\u591a\u5c42\u6b21\u89e3\u7801\u663e\u8457\u4f18\u5316\u4e86\u63a8\u7406\u5ef6\u8fdf\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u670d\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.06821", "pdf": "https://arxiv.org/pdf/2505.06821", "abs": "https://arxiv.org/abs/2505.06821", "authors": ["Dipayan Saha", "Hasan Al Shaikh", "Shams Tarek", "Farimah Farahmandi"], "title": "ThreatLens: LLM-guided Threat Modeling and Test Plan Generation for Hardware Security Verification", "categories": ["cs.CR", "cs.AI", "cs.ET"], "comment": "This paper has been presented at IEEE VLSI Test Symposium (VTS) 2025", "summary": "Current hardware security verification processes predominantly rely on manual\nthreat modeling and test plan generation, which are labor-intensive,\nerror-prone, and struggle to scale with increasing design complexity and\nevolving attack methodologies. To address these challenges, we propose\nThreatLens, an LLM-driven multi-agent framework that automates security threat\nmodeling and test plan generation for hardware security verification.\nThreatLens integrates retrieval-augmented generation (RAG) to extract relevant\nsecurity knowledge, LLM-powered reasoning for threat assessment, and\ninteractive user feedback to ensure the generation of practical test plans. By\nautomating these processes, the framework reduces the manual verification\neffort, enhances coverage, and ensures a structured, adaptable approach to\nsecurity verification. We evaluated our framework on the NEORV32 SoC,\ndemonstrating its capability to automate security verification through\nstructured test plans and validating its effectiveness in real-world scenarios.", "AI": {"tldr": "ThreatLens\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u81ea\u52a8\u5316\u786c\u4ef6\u5b89\u5168\u9a8c\u8bc1\uff0c\u51cf\u5c11\u4eba\u5de5\u6295\u5165\uff0c\u63d0\u9ad8\u8986\u76d6\u7387\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u5f53\u524d\u786c\u4ef6\u5b89\u5168\u9a8c\u8bc1\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u65b9\u6cd5\uff0c\u6548\u7387\u4f4e\u3001\u6613\u51fa\u9519\u4e14\u96be\u4ee5\u9002\u5e94\u8bbe\u8ba1\u590d\u6742\u5ea6\u548c\u653b\u51fb\u65b9\u6cd5\u7684\u589e\u957f\u3002", "method": "\u96c6\u6210\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u63d0\u53d6\u5b89\u5168\u77e5\u8bc6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5a01\u80c1\u8bc4\u4f30\uff0c\u5e76\u7ed3\u5408\u7528\u6237\u53cd\u9988\u751f\u6210\u5b9e\u7528\u6d4b\u8bd5\u8ba1\u5212\u3002", "result": "\u5728NEORV32 SoC\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u81ea\u52a8\u5316\u751f\u6210\u7ed3\u6784\u5316\u6d4b\u8bd5\u8ba1\u5212\uff0c\u6709\u6548\u5e94\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002", "conclusion": "ThreatLens\u6846\u67b6\u663e\u8457\u63d0\u5347\u786c\u4ef6\u5b89\u5168\u9a8c\u8bc1\u7684\u81ea\u52a8\u5316\u7a0b\u5ea6\u3001\u8986\u76d6\u8303\u56f4\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.07683", "pdf": "https://arxiv.org/pdf/2505.07683", "abs": "https://arxiv.org/abs/2505.07683", "authors": ["Steven Song", "Morgan Borjigin-Wang", "Irene Madejski", "Robert L. Grossman"], "title": "Multimodal Survival Modeling in the Age of Foundation Models", "categories": ["cs.LG", "cs.AI"], "comment": "23 pages, 7 figures, 8 tables", "summary": "The Cancer Genome Atlas (TCGA) has enabled novel discoveries and served as a\nlarge-scale reference through its harmonized genomics, clinical, and image\ndata. Prior studies have trained bespoke cancer survival prediction models from\nunimodal or multimodal TCGA data. A modern paradigm in biomedical deep learning\nis the development of foundation models (FMs) to derive meaningful feature\nembeddings, agnostic to a specific modeling task. Biomedical text especially\nhas seen growing development of FMs. While TCGA contains free-text data as\npathology reports, these have been historically underutilized. Here, we\ninvestigate the feasibility of training classical, multimodal survival models\nover zero-shot embeddings extracted by FMs. We show the ease and additive\neffect of multimodal fusion, outperforming unimodal models. We demonstrate the\nbenefit of including pathology report text and rigorously evaluate the effect\nof model-based text summarization and hallucination. Overall, we modernize\nsurvival modeling by leveraging FMs and information extraction from pathology\nreports.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u4eceTCGA\u6570\u636e\u4e2d\u63d0\u53d6\u7279\u5f81\u5d4c\u5165\uff0c\u7ed3\u5408\u75c5\u7406\u62a5\u544a\u6587\u672c\uff0c\u63d0\u5347\u4e86\u764c\u75c7\u751f\u5b58\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u764c\u75c7\u57fa\u56e0\u7ec4\u56fe\u8c31\uff08TCGA\uff09\u63d0\u4f9b\u4e86\u5927\u91cf\u591a\u6a21\u6001\u6570\u636e\uff0c\u4f46\u75c5\u7406\u62a5\u544a\u6587\u672c\u957f\u671f\u672a\u88ab\u5145\u5206\u5229\u7528\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u901a\u8fc7FMs\u63d0\u53d6\u7279\u5f81\u5d4c\u5165\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u6570\u636e\u63d0\u5347\u751f\u5b58\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u96f6\u6837\u672cFMs\u63d0\u53d6\u7279\u5f81\u5d4c\u5165\uff0c\u6784\u5efa\u591a\u6a21\u6001\u751f\u5b58\u6a21\u578b\uff0c\u878d\u5408\u6587\u672c\u548c\u57fa\u56e0\u7ec4\u6570\u636e\uff0c\u5e76\u8bc4\u4f30\u6587\u672c\u6458\u8981\u548c\u5e7b\u89c9\u5bf9\u6a21\u578b\u7684\u5f71\u54cd\u3002", "result": "\u591a\u6a21\u6001\u878d\u5408\u6a21\u578b\u4f18\u4e8e\u5355\u6a21\u6001\u6a21\u578b\uff0c\u75c5\u7406\u62a5\u544a\u6587\u672c\u7684\u52a0\u5165\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86FMs\u5728\u751f\u5b58\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7FMs\u548c\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\uff0c\u7814\u7a76\u4e3a\u764c\u75c7\u751f\u5b58\u9884\u6d4b\u63d0\u4f9b\u4e86\u73b0\u4ee3\u5316\u65b9\u6cd5\uff0c\u7a81\u663e\u4e86\u75c5\u7406\u62a5\u544a\u6587\u672c\u7684\u4ef7\u503c\u3002"}}
{"id": "2505.06827", "pdf": "https://arxiv.org/pdf/2505.06827", "abs": "https://arxiv.org/abs/2505.06827", "authors": ["Fabrice Y Harel-Canada", "Boran Erol", "Connor Choi", "Jason Liu", "Gary Jiarui Song", "Nanyun Peng", "Amit Sahai"], "title": "Sandcastles in the Storm: Revisiting the (Im)possibility of Strong Watermarking", "categories": ["cs.CR", "cs.AI"], "comment": "In Review @ ACL 2025", "summary": "Watermarking AI-generated text is critical for combating misuse. Yet recent\ntheoretical work argues that any watermark can be erased via random walk\nattacks that perturb text while preserving quality. However, such attacks rely\non two key assumptions: (1) rapid mixing (watermarks dissolve quickly under\nperturbations) and (2) reliable quality preservation (automated quality oracles\nperfectly guide edits). Through large-scale experiments and human-validated\nassessments, we find mixing is slow: 100% of perturbed texts retain traces of\ntheir origin after hundreds of edits, defying rapid mixing. Oracles falter, as\nstate-of-the-art quality detectors misjudge edits (77% accuracy), compounding\nerrors during attacks. Ultimately, attacks underperform: automated walks remove\nwatermarks just 26% of the time -- dropping to 10% under human quality review.\nThese findings challenge the inevitability of watermark removal. Instead,\npractical barriers -- slow mixing and imperfect quality control -- reveal\nwatermarking to be far more robust than theoretical models suggest. The gap\nbetween idealized attacks and real-world feasibility underscores the need for\nstronger watermarking methods and more realistic attack models.", "AI": {"tldr": "\u73b0\u6709\u7814\u7a76\u8ba4\u4e3aAI\u751f\u6210\u6587\u672c\u7684\u6c34\u5370\u53ef\u901a\u8fc7\u968f\u673a\u6270\u52a8\u653b\u51fb\u8f7b\u677e\u53bb\u9664\uff0c\u4f46\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u6270\u52a8\u540e\u6c34\u5370\u6b8b\u7559\u663e\u8457\uff0c\u4e14\u81ea\u52a8\u5316\u8d28\u91cf\u68c0\u6d4b\u4e0d\u53ef\u9760\uff0c\u5b9e\u9645\u653b\u51fb\u6210\u529f\u7387\u8fdc\u4f4e\u4e8e\u7406\u8bba\u9884\u671f\uff0c\u8868\u660e\u6c34\u5370\u6280\u672f\u6bd4\u7406\u8bba\u6a21\u578b\u66f4\u7a33\u5065\u3002", "motivation": "\u63a2\u8ba8AI\u751f\u6210\u6587\u672c\u6c34\u5370\u6280\u672f\u5728\u5b9e\u9645\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u53cd\u9a73\u73b0\u6709\u7406\u8bba\u4e2d\u5173\u4e8e\u6c34\u5370\u6613\u53bb\u9664\u7684\u5047\u8bbe\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u548c\u4eba\u5de5\u9a8c\u8bc1\uff0c\u6d4b\u8bd5\u968f\u673a\u6270\u52a8\u653b\u51fb\u5bf9\u6c34\u5370\u7684\u5f71\u54cd\u53ca\u81ea\u52a8\u5316\u8d28\u91cf\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6c34\u5370\u5728\u6570\u767e\u6b21\u6270\u52a8\u540e\u4ecd\u5b58\u5728\uff08100%\u6b8b\u7559\uff09\uff0c\u81ea\u52a8\u5316\u8d28\u91cf\u68c0\u6d4b\u51c6\u786e\u7387\u4ec577%\uff0c\u5b9e\u9645\u653b\u51fb\u53bb\u9664\u6c34\u5370\u6210\u529f\u7387\u4ec526%\uff08\u4eba\u5de5\u5ba1\u6838\u65f6\u964d\u81f310%\uff09\u3002", "conclusion": "\u6c34\u5370\u6280\u672f\u5728\u5b9e\u9645\u4e2d\u56e0\u6270\u52a8\u6df7\u5408\u901f\u5ea6\u6162\u548c\u8d28\u91cf\u63a7\u5236\u4e0d\u5b8c\u5584\u800c\u8868\u73b0\u7a33\u5065\uff0c\u9700\u5f00\u53d1\u66f4\u5f3a\u6c34\u5370\u65b9\u6cd5\u5e76\u6539\u8fdb\u653b\u51fb\u6a21\u578b\u4ee5\u66f4\u8d34\u8fd1\u73b0\u5b9e\u3002"}}
{"id": "2505.07702", "pdf": "https://arxiv.org/pdf/2505.07702", "abs": "https://arxiv.org/abs/2505.07702", "authors": ["Onthada Preedasawakul", "Nathakhun Wiroonsri"], "title": "4TaStiC: Time and trend traveling time series clustering for classifying long-term type 2 diabetes patients", "categories": ["cs.LG", "cs.CY", "62H30 (Primary) 62M10, 92C50 (Secondary)"], "comment": null, "summary": "Diabetes is one of the most prevalent diseases worldwide, characterized by\npersistently high blood sugar levels, capable of damaging various internal\norgans and systems. Diabetes patients require routine check-ups, resulting in a\ntime series of laboratory records, such as hemoglobin A1c, which reflects each\npatient's health behavior over time and informs their doctor's recommendations.\nClustering patients into groups based on their entire time series data assists\ndoctors in making recommendations and choosing treatments without the need to\nreview all records. However, time series clustering of this type of dataset\nintroduces some challenges; patients visit their doctors at different time\npoints, making it difficult to capture and match trends, peaks, and patterns.\nAdditionally, two aspects must be considered: differences in the levels of\nlaboratory results and differences in trends and patterns. To address these\nchallenges, we introduce a new clustering algorithm called Time and Trend\nTraveling Time Series Clustering (4TaStiC), using a base dissimilarity measure\ncombined with Euclidean and Pearson correlation metrics. We evaluated this\nalgorithm on artificial datasets, comparing its performance with that of seven\nexisting methods. The results show that 4TaStiC outperformed the other methods\non the targeted datasets. Finally, we applied 4TaStiC to cluster a cohort of\n1,989 type 2 diabetes patients at Siriraj Hospital. Each group of patients\nexhibits clear characteristics that will benefit doctors in making efficient\nclinical decisions. Furthermore, the proposed algorithm can be applied to\ncontexts outside the medical field.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a4TaStiC\u7684\u805a\u7c7b\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u7cd6\u5c3f\u75c5\u60a3\u8005\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u805a\u7c7b\u4e2d\u7684\u6311\u6218\uff0c\u7ed3\u5408\u6b27\u51e0\u91cc\u5f97\u548c\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\uff0c\u5728\u4eba\u5de5\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u60a3\u8005\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff08\u5982\u8840\u7ea2\u86cb\u767dA1c\uff09\u805a\u7c7b\u80fd\u5e2e\u52a9\u533b\u751f\u66f4\u9ad8\u6548\u5730\u5236\u5b9a\u6cbb\u7597\u65b9\u6848\uff0c\u4f46\u6570\u636e\u65f6\u95f4\u70b9\u4e0d\u4e00\u81f4\u3001\u8d8b\u52bf\u5dee\u5f02\u5927\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u3002", "method": "\u63d0\u51fa4TaStiC\u7b97\u6cd5\uff0c\u7ed3\u5408\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u548c\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u4f5c\u4e3a\u57fa dissimilarity \u5ea6\u91cf\uff0c\u89e3\u51b3\u6570\u636e\u65f6\u95f4\u4e0d\u4e00\u81f4\u548c\u8d8b\u52bf\u5dee\u5f02\u95ee\u9898\u3002", "result": "\u5728\u4eba\u5de5\u6570\u636e\u96c6\u4e0a\uff0c4TaStiC\u8868\u73b0\u4f18\u4e8e\u4e03\u79cd\u73b0\u6709\u65b9\u6cd5\uff1b\u5e94\u7528\u4e8e1,989\u540d2\u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u6570\u636e\u540e\uff0c\u805a\u7c7b\u7ed3\u679c\u6e05\u6670\uff0c\u6709\u52a9\u4e8e\u4e34\u5e8a\u51b3\u7b56\u3002", "conclusion": "4TaStiC\u7b97\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u533b\u7597\u9886\u57df\uff0c\u8fd8\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u65f6\u95f4\u5e8f\u5217\u805a\u7c7b\u573a\u666f\u3002"}}
{"id": "2505.07735", "pdf": "https://arxiv.org/pdf/2505.07735", "abs": "https://arxiv.org/abs/2505.07735", "authors": ["Nicholas T. Runcie", "Charlotte M. Deane", "Fergus Imrie"], "title": "Assessing the Chemical Intelligence of Large Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Large Language Models are versatile, general-purpose tools with a wide range\nof applications. Recently, the advent of \"reasoning models\" has led to\nsubstantial improvements in their abilities in advanced problem-solving domains\nsuch as mathematics and software engineering. In this work, we assessed the\nability of reasoning models to directly perform chemistry tasks, without any\nassistance from external tools. We created a novel benchmark, called ChemIQ,\nwhich consists of 796 questions assessing core concepts in organic chemistry,\nfocused on molecular comprehension and chemical reasoning. Unlike previous\nbenchmarks, which primarily use multiple choice formats, our approach requires\nmodels to construct short-answer responses, more closely reflecting real-world\napplications. The reasoning models, exemplified by OpenAI's o3-mini, correctly\nanswered 28%-59% of questions depending on the reasoning level used, with\nhigher reasoning levels significantly increasing performance on all tasks.\nThese models substantially outperformed the non-reasoning model, GPT-4o, which\nachieved only 7% accuracy. We found that Large Language Models can now convert\nSMILES strings to IUPAC names, a task earlier models were unable to perform.\nAdditionally, we show that the latest reasoning models can elucidate structures\nfrom 1H and 13C NMR data, correctly generating SMILES strings for 74% of\nmolecules containing up to 10 heavy atoms, and in one case solving a structure\ncomprising 21 heavy atoms. For each task, we found evidence that the reasoning\nprocess mirrors that of a human chemist. Our results demonstrate that the\nlatest reasoning models have the ability to perform advanced chemical\nreasoning.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u63a8\u7406\u5927\u6a21\u578b\u5728\u5316\u5b66\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u521b\u5efa\u4e86ChemIQ\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u679c\u663e\u793a\u63a8\u7406\u6a21\u578b\u5728\u6709\u673a\u5316\u5b66\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u975e\u63a8\u7406\u6a21\u578b\uff0c\u5e76\u80fd\u5b8c\u6210SMILES\u8f6cIUPAC\u540d\u548c\u89e3\u6790NMR\u6570\u636e\u7b49\u4efb\u52a1\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u9a8c\u8bc1\u63a8\u7406\u5927\u6a21\u578b\u80fd\u5426\u65e0\u9700\u5916\u90e8\u5de5\u5177\u76f4\u63a5\u5b8c\u6210\u5316\u5b66\u4efb\u52a1\uff0c\u63a8\u52a8\u5927\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u521b\u5efaChemIQ\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u542b796\u4e2a\u6709\u673a\u5316\u5b66\u95ee\u9898\uff09\uff0c\u6d4b\u8bd5\u63a8\u7406\u6a21\u578b\uff08\u5982o3-mini\uff09\u4e0e\u975e\u63a8\u7406\u6a21\u578b\uff08GPT-4o\uff09\u7684\u8868\u73b0\uff0c\u4efb\u52a1\u6db5\u76d6\u5206\u5b50\u7406\u89e3\u548c\u5316\u5b66\u63a8\u7406\u3002", "result": "\u7ed3\u679c\u663e\u793a\u63a8\u7406\u6a21\u578b\u6b63\u786e\u738728%-59%\uff0c\u663e\u8457\u4f18\u4e8eGPT-4o\uff087%\uff09\uff0c\u5e76\u80fd\u5b8c\u6210SMILES\u8f6cIUPAC\u540d\u548c74%\u7684NMR\u7ed3\u6784\u89e3\u6790\uff08\u539f\u5b50\u6570\u226410\uff09\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\u6700\u65b0\u63a8\u7406\u6a21\u578b\u5177\u5907\u9ad8\u7ea7\u5316\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5176\u63a8\u7406\u8fc7\u7a0b\u4e0e\u4eba\u7c7b\u5316\u5b66\u5bb6\u76f8\u4f3c\uff0c\u5c55\u73b0\u4e86\u5728\u4e13\u4e1a\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.06841", "pdf": "https://arxiv.org/pdf/2505.06841", "abs": "https://arxiv.org/abs/2505.06841", "authors": ["Prabhdeep Cheema", "Erhan Guven"], "title": "Optimizing Recommendations using Fine-Tuned LLMs", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": "Accepted and presented at IEEE CAI 2025. This version includes minor\n  clarifications and formatting updates", "summary": "As digital media platforms strive to meet evolving user expectations,\ndelivering highly personalized and intuitive movies and media recommendations\nhas become essential for attracting and retaining audiences. Traditional\nsystems often rely on keyword-based search and recommendation techniques, which\nlimit users to specific keywords and a combination of keywords. This paper\nproposes an approach that generates synthetic datasets by modeling real-world\nuser interactions, creating complex chat-style data reflective of diverse\npreferences. This allows users to express more information with complex\npreferences, such as mood, plot details, and thematic elements, in addition to\nconventional criteria like genre, title, and actor-based searches. In today's\nsearch space, users cannot write queries like ``Looking for a fantasy movie\nfeaturing dire wolves, ideally set in a harsh frozen world with themes of\nloyalty and survival.''\n  Building on these contributions, we evaluate synthetic datasets for diversity\nand effectiveness in training and benchmarking models, particularly in areas\noften absent from traditional datasets. This approach enhances personalization\nand accuracy by enabling expressive and natural user queries. It establishes a\nfoundation for the next generation of conversational AI-driven search and\nrecommendation systems in digital entertainment.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6a21\u62df\u771f\u5b9e\u7528\u6237\u4ea4\u4e92\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\u7684\u65b9\u6cd5\uff0c\u4ee5\u6539\u8fdb\u6570\u5b57\u5a31\u4e50\u5e73\u53f0\u4e2d\u4e2a\u6027\u5316\u63a8\u8350\u7684\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u3002", "motivation": "\u4e3a\u4e86\u6ee1\u8db3\u7528\u6237\u5bf9\u9ad8\u5ea6\u4e2a\u6027\u5316\u63a8\u8350\u7684\u9700\u6c42\uff0c\u4f20\u7edf\u57fa\u4e8e\u5173\u952e\u8bcd\u7684\u7cfb\u7edf\u9650\u5236\u4e86\u7528\u6237\u8868\u8fbe\u7684\u590d\u6742\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u591a\u6837\u5316\u7684\u504f\u597d\u3002", "method": "\u901a\u8fc7\u5efa\u6a21\u771f\u5b9e\u7528\u6237\u4ea4\u4e92\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\uff0c\u652f\u6301\u590d\u6742\u804a\u5929\u5f0f\u67e5\u8be2\uff0c\u6db5\u76d6\u60c5\u7eea\u3001\u60c5\u8282\u7ec6\u8282\u7b49\u975e\u4f20\u7edf\u641c\u7d22\u6807\u51c6\u3002", "result": "\u5408\u6210\u7684\u6570\u636e\u96c6\u589e\u5f3a\u4e86\u6a21\u578b\u8bad\u7ec3\u7684\u591a\u6837\u6027\u548c\u6709\u6548\u6027\uff0c\u63d0\u9ad8\u4e86\u63a8\u8350\u7cfb\u7edf\u7684\u4e2a\u6027\u5316\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e0b\u4e00\u4ee3\u5bf9\u8bdd\u5f0fAI\u9a71\u52a8\u7684\u641c\u7d22\u548c\u63a8\u8350\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u7279\u522b\u662f\u5728\u6570\u5b57\u5a31\u4e50\u9886\u57df\u3002"}}
{"id": "2505.07750", "pdf": "https://arxiv.org/pdf/2505.07750", "abs": "https://arxiv.org/abs/2505.07750", "authors": ["Ga\u0161per Petelin", "Gjorgjina Cenikj"], "title": "The Pitfalls of Benchmarking in Algorithm Selection: What We Are Getting Wrong", "categories": ["cs.LG"], "comment": null, "summary": "Algorithm selection, aiming to identify the best algorithm for a given\nproblem, plays a pivotal role in continuous black-box optimization. A common\napproach involves representing optimization functions using a set of features,\nwhich are then used to train a machine learning meta-model for selecting\nsuitable algorithms. Various approaches have demonstrated the effectiveness of\nthese algorithm selection meta-models. However, not all evaluation approaches\nare equally valid for assessing the performance of meta-models. We highlight\nmethodological issues that frequently occur in the community and should be\naddressed when evaluating algorithm selection approaches. First, we identify\nflaws with the \"leave-instance-out\" evaluation technique. We show that\nnon-informative features and meta-models can achieve high accuracy, which\nshould not be the case with a well-designed evaluation framework. Second, we\ndemonstrate that measuring the performance of optimization algorithms with\nmetrics sensitive to the scale of the objective function requires careful\nconsideration of how this impacts the construction of the meta-model, its\npredictions, and the model's error. Such metrics can falsely present overly\noptimistic performance assessments of the meta-models. This paper emphasizes\nthe importance of careful evaluation, as loosely defined methodologies can\nmislead researchers, divert efforts, and introduce noise into the field", "AI": {"tldr": "\u672c\u6587\u6279\u8bc4\u4e86\u7b97\u6cd5\u9009\u62e9\u5143\u6a21\u578b\u8bc4\u4f30\u4e2d\u7684\u5e38\u89c1\u65b9\u6cd5\u95ee\u9898\uff0c\u6307\u51fa\u201cleave-instance-out\u201d\u6280\u672f\u53ca\u5bf9\u76ee\u6807\u51fd\u6570\u5c3a\u5ea6\u654f\u611f\u7684\u6307\u6807\u53ef\u80fd\u5bfc\u81f4\u8bef\u5bfc\u6027\u7ed3\u679c\u3002", "motivation": "\u7b97\u6cd5\u9009\u62e9\u5728\u4f18\u5316\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u4e25\u8c28\u53ef\u80fd\u5bfc\u81f4\u65e0\u6548\u7279\u5f81\u6216\u6a21\u578b\u88ab\u9ad8\u4f30\uff0c\u5e72\u6270\u9886\u57df\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u5206\u6790\u975e\u4fe1\u606f\u6027\u7279\u5f81\u548c\u6a21\u578b\u7684\u4f2a\u9ad8\u7cbe\u5ea6\u73b0\u8c61\uff0c\u4ee5\u53ca\u5c3a\u5ea6\u654f\u611f\u6307\u6807\u7684\u8bef\u5bfc\u6027\uff0c\u63ed\u793a\u8bc4\u4f30\u65b9\u6cd5\u7684\u7f3a\u9677\u3002", "result": "\u53d1\u73b0\u5f53\u524d\u8bc4\u4f30\u6846\u67b6\u53ef\u80fd\u63a9\u76d6\u5143\u6a21\u578b\u7684\u771f\u5b9e\u6027\u80fd\uff0c\u5bfc\u81f4\u8fc7\u4e8e\u4e50\u89c2\u7684\u7ed3\u8bba\u3002", "conclusion": "\u547c\u5401\u91c7\u7528\u66f4\u4e25\u8c28\u7684\u8bc4\u4f30\u65b9\u6cd5\u8bba\uff0c\u4ee5\u907f\u514d\u8bef\u5bfc\u7814\u7a76\u65b9\u5411\u548c\u6d6a\u8d39\u8d44\u6e90\u3002"}}
{"id": "2505.06860", "pdf": "https://arxiv.org/pdf/2505.06860", "abs": "https://arxiv.org/abs/2505.06860", "authors": ["Xia Du", "Jiajie Zhu", "Jizhe Zhou", "Chi-man Pun", "Zheng Lin", "Cong Wu", "Zhe Chen", "Jun Luo"], "title": "DP-TRAE: A Dual-Phase Merging Transferable Reversible Adversarial Example for Image Privacy Protection", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "12 pages, 5 figures", "summary": "In the field of digital security, Reversible Adversarial Examples (RAE)\ncombine adversarial attacks with reversible data hiding techniques to\neffectively protect sensitive data and prevent unauthorized analysis by\nmalicious Deep Neural Networks (DNNs). However, existing RAE techniques\nprimarily focus on white-box attacks, lacking a comprehensive evaluation of\ntheir effectiveness in black-box scenarios. This limitation impedes their\nbroader deployment in complex, dynamic environments. Further more, traditional\nblack-box attacks are often characterized by poor transferability and high\nquery costs, significantly limiting their practical applicability. To address\nthese challenges, we propose the Dual-Phase Merging Transferable Reversible\nAttack method, which generates highly transferable initial adversarial\nperturbations in a white-box model and employs a memory augmented black-box\nstrategy to effectively mislead target mod els. Experimental results\ndemonstrate the superiority of our approach, achieving a 99.0% attack success\nrate and 100% recovery rate in black-box scenarios, highlighting its robustness\nin privacy protection. Moreover, we successfully implemented a black-box attack\non a commercial model, further substantiating the potential of this approach\nfor practical use.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u76f8\u5408\u5e76\u53ef\u9006\u653b\u51fb\u65b9\u6cd5\uff08Dual-Phase Merging Transferable Reversible Attack\uff09\uff0c\u4ee5\u63d0\u9ad8\u9ed1\u76d2\u573a\u666f\u4e0b\u7684\u5bf9\u6297\u6837\u672c\u653b\u51fb\u6548\u679c\uff0c\u5b9e\u73b099.0%\u7684\u653b\u51fb\u6210\u529f\u7387\u548c100%\u7684\u6062\u590d\u7387\u3002", "motivation": "\u73b0\u6709\u53ef\u9006\u5bf9\u6297\u6837\u672c\uff08RAE\uff09\u6280\u672f\u4e3b\u8981\u9488\u5bf9\u767d\u76d2\u653b\u51fb\uff0c\u7f3a\u4e4f\u5bf9\u9ed1\u76d2\u573a\u666f\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u4f20\u7edf\u9ed1\u76d2\u653b\u51fb\u53c8\u56e0\u8fc1\u79fb\u6027\u5dee\u548c\u67e5\u8be2\u6210\u672c\u9ad8\u800c\u5b9e\u7528\u6027\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u53cc\u76f8\u5408\u5e76\u65b9\u6cd5\uff0c\u9996\u5148\u751f\u6210\u9ad8\u8fc1\u79fb\u6027\u7684\u521d\u59cb\u5bf9\u6297\u6270\u52a8\uff08\u767d\u76d2\u9636\u6bb5\uff09\uff0c\u518d\u901a\u8fc7\u8bb0\u5fc6\u589e\u5f3a\u7684\u9ed1\u76d2\u7b56\u7565\u8bef\u5bfc\u76ee\u6807\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u9ed1\u76d2\u573a\u666f\u4e0b\u653b\u51fb\u6210\u529f\u7387\u8fbe99.0%\uff0c\u6062\u590d\u7387100%\uff0c\u5e76\u5728\u5546\u4e1a\u6a21\u578b\u4e0a\u6210\u529f\u5b9e\u73b0\u9ed1\u76d2\u653b\u51fb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u6837\u672c\u5728\u9ed1\u76d2\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\u3002"}}
{"id": "2505.07777", "pdf": "https://arxiv.org/pdf/2505.07777", "abs": "https://arxiv.org/abs/2505.07777", "authors": ["Arya Grayeli", "Vipin Swarup", "Steven E. Noel"], "title": "Synthesizing Diverse Network Flow Datasets with Scalable Dynamic Multigraph Generation", "categories": ["cs.LG", "cs.NI"], "comment": null, "summary": "Obtaining real-world network datasets is often challenging because of\nprivacy, security, and computational constraints. In the absence of such\ndatasets, graph generative models become essential tools for creating synthetic\ndatasets. In this paper, we introduce a novel machine learning model for\ngenerating high-fidelity synthetic network flow datasets that are\nrepresentative of real-world networks. Our approach involves the generation of\ndynamic multigraphs using a stochastic Kronecker graph generator for structure\ngeneration and a tabular generative adversarial network for feature generation.\nWe further employ an XGBoost (eXtreme Gradient Boosting) model for graph\nalignment, ensuring accurate overlay of features onto the generated graph\nstructure. We evaluate our model using new metrics that assess both the\naccuracy and diversity of the synthetic graphs. Our results demonstrate\nimprovements in accuracy over previous large-scale graph generation methods\nwhile maintaining similar efficiency. We also explore the trade-off between\naccuracy and diversity in synthetic graph dataset creation, a topic not\nextensively covered in related works. Our contributions include the synthesis\nand evaluation of large real-world netflow datasets and the definition of new\nmetrics for evaluating synthetic graph generative models.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u4e14\u80fd\u4ee3\u8868\u771f\u5b9e\u7f51\u7edc\u7684\u5408\u6210\u7f51\u7edc\u6d41\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u52a8\u6001\u591a\u91cd\u56fe\u751f\u6210\u548c\u7279\u5f81\u751f\u6210\u7ed3\u5408\uff0c\u5e76\u91c7\u7528XGBoost\u8fdb\u884c\u56fe\u5bf9\u9f50\uff0c\u63d0\u5347\u4e86\u751f\u6210\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u3002", "motivation": "\u7531\u4e8e\u9690\u79c1\u3001\u5b89\u5168\u548c\u8ba1\u7b97\u9650\u5236\uff0c\u83b7\u53d6\u771f\u5b9e\u7f51\u7edc\u6570\u636e\u96c6\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u6570\u636e\u96c6\u751f\u6210\u5de5\u5177\u3002", "method": "\u7ed3\u5408\u968f\u673aKronecker\u56fe\u751f\u6210\u5668\u751f\u6210\u7ed3\u6784\uff0c\u4f7f\u7528\u8868\u683c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u751f\u6210\u7279\u5f81\uff0c\u5e76\u7528XGBoost\u6a21\u578b\u8fdb\u884c\u56fe\u5bf9\u9f50\u3002", "result": "\u65b0\u6a21\u578b\u5728\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u4e4b\u524d\u7684\u5927\u89c4\u6a21\u56fe\u751f\u6210\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u4f3c\u7684\u6548\u7387\uff0c\u5e76\u63a2\u7d22\u4e86\u51c6\u786e\u6027\u4e0e\u591a\u6837\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u8bba\u6587\u8d21\u732e\u5305\u62ec\u5408\u6210\u548c\u8bc4\u4f30\u5927\u89c4\u6a21\u771f\u5b9e\u7f51\u7edc\u6d41\u6570\u636e\u96c6\uff0c\u5e76\u5b9a\u4e49\u4e86\u8bc4\u4f30\u5408\u6210\u56fe\u751f\u6210\u6a21\u578b\u7684\u65b0\u6307\u6807\u3002"}}
{"id": "2505.06861", "pdf": "https://arxiv.org/pdf/2505.06861", "abs": "https://arxiv.org/abs/2505.06861", "authors": ["Dongxiu Liu", "Haoyi Niu", "Zhihao Wang", "Jinliang Zheng", "Yinan Zheng", "Zhonghong Ou", "Jianming Hu", "Jianxiong Li", "Xianyuan Zhan"], "title": "Efficient Robotic Policy Learning via Latent Space Backward Planning", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Current robotic planning methods often rely on predicting multi-frame images\nwith full pixel details. While this fine-grained approach can serve as a\ngeneric world model, it introduces two significant challenges for downstream\npolicy learning: substantial computational costs that hinder real-time\ndeployment, and accumulated inaccuracies that can mislead action extraction.\nPlanning with coarse-grained subgoals partially alleviates efficiency issues.\nHowever, their forward planning schemes can still result in off-task\npredictions due to accumulation errors, leading to misalignment with long-term\ngoals. This raises a critical question: Can robotic planning be both efficient\nand accurate enough for real-time control in long-horizon, multi-stage tasks?\nTo address this, we propose a Latent Space Backward Planning scheme (LBP),\nwhich begins by grounding the task into final latent goals, followed by\nrecursively predicting intermediate subgoals closer to the current state. The\ngrounded final goal enables backward subgoal planning to always remain aware of\ntask completion, facilitating on-task prediction along the entire planning\nhorizon. The subgoal-conditioned policy incorporates a learnable token to\nsummarize the subgoal sequences and determines how each subgoal guides action\nextraction. Through extensive simulation and real-robot long-horizon\nexperiments, we show that LBP outperforms existing fine-grained and forward\nplanning methods, achieving SOTA performance. Project Page:\nhttps://lbp-authors.github.io", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9006\u5411\u89c4\u5212\u6846\u67b6\uff08LBP\uff09\uff0c\u901a\u8fc7\u4ece\u6700\u7ec8\u76ee\u6807\u53cd\u5411\u751f\u6210\u4e2d\u95f4\u5b50\u76ee\u6807\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89c4\u5212\u65b9\u6cd5\u5728\u5b9e\u65f6\u6027\u548c\u51c6\u786e\u6027\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u89c4\u5212\u65b9\u6cd5\u591a\u4f9d\u8d56\u50cf\u7d20\u7ea7\u591a\u5e27\u9884\u6d4b\uff0c\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u8bef\u5dee\u79ef\u7d2f\u95ee\u9898\uff0c\u800c\u6b63\u5411\u5b50\u76ee\u6807\u89c4\u5212\u4ecd\u96be\u4ee5\u4fdd\u8bc1\u957f\u671f\u76ee\u6807\u5bf9\u9f50\u3002", "method": "\u63d0\u51faLatent Space Backward Planning\uff08LBP\uff09\uff0c\u4ece\u6f5c\u5728\u7a7a\u95f4\u6700\u7ec8\u76ee\u6807\u51fa\u53d1\uff0c\u9012\u5f52\u751f\u6210\u4e2d\u95f4\u5b50\u76ee\u6807\uff0c\u5e76\u901a\u8fc7\u53ef\u5b66\u4e60\u4ee4\u724c\u6307\u5bfc\u52a8\u4f5c\u751f\u6210\u3002", "result": "LBP\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "\u9006\u5411\u89c4\u5212\u80fd\u6709\u6548\u63d0\u5347\u5b9e\u65f6\u6027\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u591a\u9636\u6bb5\u957f\u65f6\u7a0b\u4efb\u52a1\u3002"}}
{"id": "2505.07782", "pdf": "https://arxiv.org/pdf/2505.07782", "abs": "https://arxiv.org/abs/2505.07782", "authors": ["Rushi Qiang", "Yuchen Zhuang", "Yinghao Li", "Dingu Sagar V K", "Rongzhi Zhang", "Changhao Li", "Ian Shu-Hei Wong", "Sherry Yang", "Percy Liang", "Chao Zhang", "Bo Dai"], "title": "MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering", "categories": ["cs.LG"], "comment": null, "summary": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement\nlearning, evaluating, and improving autonomous large language model (LLM)\nagents in iterative machine learning engineering (MLE) workflows. Unlike\nexisting benchmarks that primarily rely on static datasets or single-attempt\nevaluations, MLE-Dojo provides an interactive environment enabling agents to\niteratively experiment, debug, and refine solutions through structured feedback\nloops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse,\nopen-ended MLE tasks carefully curated to reflect realistic engineering\nscenarios such as data processing, architecture search, hyperparameter tuning,\nand code debugging. Its fully executable environment supports comprehensive\nagent training via both supervised fine-tuning and reinforcement learning,\nfacilitating iterative experimentation, realistic data sampling, and real-time\noutcome verification. Extensive evaluations of eight frontier LLMs reveal that\nwhile current models achieve meaningful iterative improvements, they still\nexhibit significant limitations in autonomously generating long-horizon\nsolutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's\nflexible and extensible architecture seamlessly integrates diverse data\nsources, tools, and evaluation protocols, uniquely enabling model-based agent\ntuning and promoting interoperability, scalability, and reproducibility. We\nopen-source our framework and benchmarks to foster community-driven innovation\ntowards next-generation MLE agents.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86MLE-Dojo\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u7cfb\u7edf\u5f3a\u5316\u5b66\u4e60\u3001\u8bc4\u4f30\u548c\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u7684\u4ea4\u4e92\u5f0f\u73af\u5883\uff0c\u57fa\u4e8e200\u591a\u4e2a\u771f\u5b9eKaggle\u6311\u6218\u4efb\u52a1\u3002\u5f53\u524d\u6a21\u578b\u867d\u80fd\u8fed\u4ee3\u6539\u8fdb\uff0c\u4f46\u5728\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u65f6\u4ecd\u6709\u5c40\u9650\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u6570\u636e\u96c6\u6216\u5355\u6b21\u8bc4\u4f30\uff0c\u65e0\u6cd5\u652f\u6301\u4ee3\u7406\u5728\u771f\u5b9eMLE\u5de5\u4f5c\u6d41\u4e2d\u8fed\u4ee3\u5b9e\u9a8c\u548c\u6539\u8fdb\u3002\u9700\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u73af\u5883\u6765\u6a21\u62df\u771f\u5b9e\u5de5\u7a0b\u573a\u666f\uff0c\u4fc3\u8fdb\u4ee3\u7406\u7684\u6301\u7eed\u4f18\u5316\u3002", "method": "\u6784\u5efaMLE-Dojo\u6846\u67b6\uff0c\u57fa\u4e8e200+\u771f\u5b9eKaggle\u4efb\u52a1\uff0c\u6db5\u76d6\u6570\u636e\u9884\u5904\u7406\u3001\u67b6\u6784\u641c\u7d22\u3001\u8d85\u53c2\u6570\u8c03\u4f18\u7b49\u591a\u573a\u666f\u3002\u652f\u6301\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u4f9b\u5b9e\u65f6\u53cd\u9988\u548c\u53ef\u6267\u884c\u73af\u5883\u3002", "result": "\u8bc4\u4f308\u79cd\u524d\u6cbfLLM\u663e\u793a\uff0c\u4ee3\u7406\u80fd\u5b9e\u73b0\u8fed\u4ee3\u6539\u8fdb\uff0c\u4f46\u5728\u751f\u6210\u957f\u671f\u89e3\u51b3\u65b9\u6848\u548c\u9ad8\u6548\u89e3\u51b3\u590d\u6742\u9519\u8bef\u65b9\u9762\u4ecd\u6709\u660e\u663e\u4e0d\u8db3\u3002", "conclusion": "MLE-Dojo\u901a\u8fc7\u7075\u6d3b\u67b6\u6784\u548c\u5f00\u6e90\u793e\u533a\u652f\u6301\uff0c\u63a8\u52a8\u4e86\u4e0b\u4e00\u4ee3MLE\u4ee3\u7406\u7684\u7814\u53d1\uff0c\u4f46\u5176\u5c40\u9650\u6027\u4e5f\u51f8\u663e\u4e86\u672a\u6765\u6539\u8fdb\u7684\u65b9\u5411\u3002"}}
{"id": "2505.07783", "pdf": "https://arxiv.org/pdf/2505.07783", "abs": "https://arxiv.org/abs/2505.07783", "authors": ["Yanxin Liu", "Yunqi Zhang"], "title": "Relative Overfitting and Accept-Reject Framework", "categories": ["cs.LG"], "comment": null, "summary": "Currently, the scaling law of Large Language Models (LLMs) faces challenges\nand bottlenecks. This paper posits that noise effects, stemming from changes in\nthe signal-to-noise ratio under diminishing marginal returns, are the root\ncause of these issues. To control this noise, we investigated the differences\nbetween models with performance advantages and disadvantages, introducing the\nconcept of \"relative overfitting.\" Based on their complementary strengths, we\nhave proposed an application framework, Accept-Reject (AR). In Natural Language\nProcessing (NLP), we use LLMs and Small Language Models (SLMs) as the medium\nfor discussion. This framework enables SLMs to exert a universal positive\ninfluence on LLM decision outputs, rather than the intuitively expected\nnegative influence. We validated our approach using self-built models based on\nmainstream architectures and pre-trained mainstream models across multiple\ndatasets, including basic language modeling, long-context tasks, subject\nexamination, and question-answering (QA) benchmarks. The results demonstrate\nthat through our structure, compared to increasing the LLM's parameters, we can\nachieve better performance improvements with significantly lower parameter and\ncomputational costs in many scenarios. These improvements are universal,\nstable, and effective. Furthermore, we explore the potential of \"relative\noverfitting\" and the AR framework in other machine learning domains, such as\ncomputer vision (CV) and AI for science. We hope the proposed approach can help\nscale laws overcome existing bottlenecks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u2018\u76f8\u5bf9\u8fc7\u62df\u5408\u2019\u6982\u5ff5\u548cAR\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u548cSLM\u89e3\u51b3\u5927\u6a21\u578b\u89c4\u6a21\u5316\u74f6\u9888\u95ee\u9898\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u5316\u9762\u4e34\u74f6\u9888\uff0c\u4f5c\u8005\u8ba4\u4e3a\u566a\u58f0\u6548\u5e94\u662f\u5173\u952e\u95ee\u9898\uff0c\u9700\u63a2\u7d22\u65b0\u65b9\u6cd5\u4f18\u5316\u6027\u80fd\u4e0e\u6210\u672c\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5dee\u5f02\u63d0\u51fa\u2018\u76f8\u5bf9\u8fc7\u62df\u5408\u2019\u6982\u5ff5\uff0c\u8bbe\u8ba1AR\u6846\u67b6\u5229\u7528SLM\u8f85\u52a9LLM\u51b3\u7b56\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eAR\u6846\u67b6\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u4f18\u4e8e\u5355\u7eaf\u589e\u52a0LLM\u53c2\u6570\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\u4e14\u7a33\u5b9a\u3002", "conclusion": "\u8be5\u6846\u67b6\u666e\u9002\u6027\u5f3a\uff0c\u6216\u53ef\u6269\u5c55\u81f3CV\u7b49\u9886\u57df\uff0c\u6709\u671b\u7a81\u7834\u73b0\u6709\u89c4\u6a21\u5316\u74f6\u9888\u3002"}}
{"id": "2505.06881", "pdf": "https://arxiv.org/pdf/2505.06881", "abs": "https://arxiv.org/abs/2505.06881", "authors": ["Hamd Jalil", "Ahmed Qazi", "Asim Iqbal"], "title": "NeuRN: Neuro-inspired Domain Generalization for Image Classification", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "comment": "14 pages, 7 figures, 1 table", "summary": "Domain generalization in image classification is a crucial challenge, with\nmodels often failing to generalize well across unseen datasets. We address this\nissue by introducing a neuro-inspired Neural Response Normalization (NeuRN)\nlayer which draws inspiration from neurons in the mammalian visual cortex,\nwhich aims to enhance the performance of deep learning architectures on unseen\ntarget domains by training deep learning models on a source domain. The\nperformance of these models is considered as a baseline and then compared\nagainst models integrated with NeuRN on image classification tasks. We perform\nexperiments across a range of deep learning architectures, including ones\nderived from Neural Architecture Search and Vision Transformer. Additionally,\nin order to shortlist models for our experiment from amongst the vast range of\ndeep neural networks available which have shown promising results, we also\npropose a novel method that uses the Needleman-Wunsch algorithm to compute\nsimilarity between deep learning architectures. Our results demonstrate the\neffectiveness of NeuRN by showing improvement against baseline in cross-domain\nimage classification tasks. Our framework attempts to establish a foundation\nfor future neuro-inspired deep learning models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u5927\u8111\u89c6\u89c9\u76ae\u5c42\u542f\u53d1\u7684\u795e\u7ecf\u54cd\u5e94\u5f52\u4e00\u5316\u5c42\uff08NeuRN\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u672a\u89c1\u76ee\u6807\u57df\u4e0a\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u9886\u57df\u6cdb\u5316\u95ee\u9898\u4e25\u91cd\uff0c\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u53d7\u54fa\u4e73\u52a8\u7269\u89c6\u89c9\u76ae\u5c42\u795e\u7ecf\u5143\u542f\u53d1\uff0c\u63d0\u51faNeuRN\u5c42\u4ee5\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5728\u6e90\u57df\u4e0a\u8bad\u7ec3\u6a21\u578b\uff0c\u5f15\u5165NeuRN\u5c42\uff0c\u5e76\u5bf9\u6bd4\u57fa\u7ebf\u6a21\u578b\u4e0e\u6574\u5408NeuRN\u7684\u6a21\u578b\u6027\u80fd\u3002\u91c7\u7528\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff08\u5982\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u548c\u89c6\u89c9Transformer\uff09\u8fdb\u884c\u5b9e\u9a8c\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eNeedleman-Wunsch\u7b97\u6cd5\u7684\u6df1\u5ea6\u67b6\u6784\u76f8\u4f3c\u6027\u8ba1\u7b97\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cNeuRN\u5728\u8de8\u57df\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u672a\u6765\u53d7\u795e\u7ecf\u542f\u53d1\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86NeuRN\u5728\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.07793", "pdf": "https://arxiv.org/pdf/2505.07793", "abs": "https://arxiv.org/abs/2505.07793", "authors": ["Assaf Ben-Kish", "Itamar Zimerman", "M. Jehanzeb Mirza", "James Glass", "Leonid Karlinsky", "Raja Giryes"], "title": "Overflow Prevention Enhances Long-Context Recurrent LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "A recent trend in LLMs is developing recurrent sub-quadratic models that\nimprove long-context processing efficiency. We investigate leading large\nlong-context models, focusing on how their fixed-size recurrent memory affects\ntheir performance. Our experiments reveal that, even when these models are\ntrained for extended contexts, their use of long contexts remains\nunderutilized. Specifically, we demonstrate that a chunk-based inference\nprocedure, which identifies and processes only the most relevant portion of the\ninput can mitigate recurrent memory failures and be effective for many\nlong-context tasks: On LongBench, our method improves the overall performance\nof Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%,\nRecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this\nsimple approach also leads to state-of-the-art results in the challenging\nLongBench v2 benchmark, showing competitive performance with equivalent size\nTransformers. Furthermore, our findings raise questions about whether recurrent\nmodels genuinely exploit long-range dependencies, as our single-chunk strategy\ndelivers stronger performance - even in tasks that presumably require\ncross-context relations.", "AI": {"tldr": "\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u6548\u7387\u662f\u5f53\u524dLLMs\u7684\u7814\u7a76\u70ed\u70b9\uff0c\u672c\u6587\u53d1\u73b0\u5373\u4f7f\u8bad\u7ec3\u4e86\u957f\u4e0a\u4e0b\u6587\uff0c\u8fd9\u4e9b\u6a21\u578b\u7684\u56fa\u5b9a\u5927\u5c0f\u5faa\u73af\u5185\u5b58\u4ecd\u5bfc\u81f4\u5176\u5229\u7528\u7387\u4e0d\u8db3\u3002\u901a\u8fc7\u5206\u5757\u63a8\u7406\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u73b0\u6709\u7684\u957f\u4e0a\u4e0b\u6587\u5927\u6a21\u578b\u5982\u4f55\u5229\u7528\u5176\u56fa\u5b9a\u5927\u5c0f\u7684\u5faa\u73af\u5185\u5b58\uff0c\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7\u6539\u8fdb\u65b9\u6cd5\u6765\u63d0\u9ad8\u5176\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u5206\u5757\u63a8\u7406\u65b9\u6cd5\uff0c\u4ec5\u5904\u7406\u548c\u8bc6\u522b\u8f93\u5165\u4e2d\u6700\u76f8\u5173\u7684\u90e8\u5206\uff0c\u4ee5\u51cf\u8f7b\u5faa\u73af\u5185\u5b58\u7684\u4e0d\u8db3\u3002", "result": "\u5728LongBench\u4e0a\uff0c\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u591a\u4e2a\u6a21\u578b\u7684\u6027\u80fd\uff08\u5982Falcon3-Mamba-Inst-7B\u63d0\u534714%\uff09\u3002\u5728LongBench v2\u4e0a\uff0c\u65b9\u6cd5\u751a\u81f3\u8fbe\u5230\u4e86\u4e0eTransformer\u76f8\u5f53\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "conclusion": "\u5206\u5757\u63a8\u7406\u65b9\u6cd5\u5c55\u793a\u4e86\u5176\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4f46\u540c\u65f6\u4e5f\u8d28\u7591\u4e86\u5faa\u73af\u6a21\u578b\u662f\u5426\u771f\u6b63\u5229\u7528\u4e86\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u3002"}}
{"id": "2505.06883", "pdf": "https://arxiv.org/pdf/2505.06883", "abs": "https://arxiv.org/abs/2505.06883", "authors": ["Botian Xu", "Haoyang Weng", "Qingzhou Lu", "Yang Gao", "Huazhe Xu"], "title": "FACET: Force-Adaptive Control via Impedance Reference Tracking for Legged Robots", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) has made significant strides in legged robot\ncontrol, enabling locomotion across diverse terrains and complex\nloco-manipulation capabilities. However, the commonly used position or velocity\ntracking-based objectives are agnostic to forces experienced by the robot,\nleading to stiff and potentially dangerous behaviors and poor control during\nforceful interactions. To address this limitation, we present\n\\emph{Force-Adaptive Control via Impedance Reference Tracking} (FACET).\nInspired by impedance control, we use RL to train a control policy to imitate a\nvirtual mass-spring-damper system, allowing fine-grained control under external\nforces by manipulating the virtual spring. In simulation, we demonstrate that\nour quadruped robot achieves improved robustness to large impulses (up to 200\nNs) and exhibits controllable compliance, achieving an 80% reduction in\ncollision impulse. The policy is deployed to a physical robot to showcase both\ncompliance and the ability to engage with large forces by kinesthetic control\nand pulling payloads up to 2/3 of its weight. Further extension to a legged\nloco-manipulator and a humanoid shows the applicability of our method to more\ncomplex settings to enable whole-body compliance control. Project Website:\nhttps://egalahad.github.io/facet/", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86FACET\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8ba9\u673a\u5668\u4eba\u6a21\u4eff\u865a\u62df\u8d28\u91cf-\u5f39\u7c27-\u963b\u5c3c\u7cfb\u7edf\uff0c\u4ee5\u4f18\u5316\u5916\u529b\u4ea4\u4e92\u4e0b\u7684\u63a7\u5236\u8868\u73b0\uff0c\u63d0\u5347\u4e86\u9c81\u68d2\u6027\u548c\u5408\u89c4\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4f4d\u7f6e\u6216\u901f\u5ea6\u8ddf\u8e2a\u7684\u5f3a\u5316\u5b66\u4e60\u76ee\u6807\u5bf9\u5916\u529b\u4e0d\u654f\u611f\uff0c\u5bfc\u81f4\u673a\u5668\u4eba\u884c\u4e3a\u50f5\u786c\u4e14\u4ea4\u4e92\u4e0d\u5b89\u5168\u3002", "method": "\u7ed3\u5408\u963b\u6297\u63a7\u5236\u601d\u60f3\uff0c\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7b56\u7565\u6a21\u4eff\u865a\u62df\u8d28\u91cf-\u5f39\u7c27-\u963b\u5c3c\u7cfb\u7edf\uff0c\u901a\u8fc7\u8c03\u8282\u865a\u62df\u5f39\u7c27\u5b9e\u73b0\u7cbe\u7ec6\u5916\u529b\u63a7\u5236\u3002", "result": "\u4eff\u771f\u4e2d\u56db\u8db3\u673a\u5668\u4eba\u5bf9200Ns\u51b2\u91cf\u9c81\u68d2\u6027\u63d0\u5347\uff0c\u78b0\u649e\u51b2\u91cf\u51cf\u5c1180%\uff1b\u5b9e\u7269\u6d4b\u8bd5\u5c55\u793a\u4e86\u5408\u89c4\u6027\u548c\u62d6\u52a82/3\u81ea\u91cd\u8d1f\u8f7d\u7684\u80fd\u529b\u3002", "conclusion": "FACET\u53ef\u6269\u5c55\u81f3\u590d\u6742\u573a\u666f\uff08\u5982\u8db3\u5f0f\u64cd\u4f5c\u5668\u548c\u4eba\u5f62\u673a\u5668\u4eba\uff09\uff0c\u5b9e\u73b0\u5168\u8eab\u5408\u89c4\u63a7\u5236\u3002"}}
{"id": "2505.07797", "pdf": "https://arxiv.org/pdf/2505.07797", "abs": "https://arxiv.org/abs/2505.07797", "authors": ["Daniel Beechey", "Thomas M. S. Smith", "\u00d6zg\u00fcr \u015eim\u015fek"], "title": "A Theoretical Framework for Explaining Reinforcement Learning with Shapley Values", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning agents can achieve superhuman performance, but their\ndecisions are often difficult to interpret. This lack of transparency limits\ndeployment, especially in safety-critical settings where human trust and\naccountability are essential. In this work, we develop a theoretical framework\nfor explaining reinforcement learning through the influence of state features,\nwhich represent what the agent observes in its environment. We identify three\ncore elements of the agent-environment interaction that benefit from\nexplanation: behaviour (what the agent does), performance (what the agent\nachieves), and value estimation (what the agent expects to achieve). We treat\nstate features as players cooperating to produce each element and apply Shapley\nvalues, a principled method from cooperative game theory, to identify the\ninfluence of each feature. This approach yields a family of mathematically\ngrounded explanations with clear semantics and theoretical guarantees. We use\nillustrative examples to show how these explanations align with human intuition\nand reveal novel insights. Our framework unifies and extends prior work, making\nexplicit the assumptions behind existing approaches, and offers a principled\nfoundation for more interpretable and trustworthy reinforcement learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u535a\u5f08\u8bba\u4e2dShapley\u503c\u7684\u65b9\u6cd5\uff0c\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u72b6\u6001\u7279\u5f81\u5f71\u54cd\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u867d\u6027\u80fd\u4f18\u8d8a\u4f46\u51b3\u7b56\u4e0d\u900f\u660e\uff0c\u9650\u5236\u4e86\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u72b6\u6001\u7279\u5f81\u535a\u5f08\u8bba\u5206\u6790\u884c\u4e3a\u3001\u6027\u80fd\u548c\u503c\u4f30\u8ba1\u4e09\u4e2a\u4ea4\u4e92\u8981\u7d20\uff0c\u5e94\u7528Shapley\u503c\u91cf\u5316\u7279\u5f81\u5f71\u54cd\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u5957\u6570\u5b66\u4e25\u8c28\u7684\u89e3\u91ca\u6846\u67b6\uff0c\u9a8c\u8bc1\u4e86\u5176\u7b26\u5408\u4eba\u7c7b\u76f4\u89c9\u5e76\u63d0\u4f9b\u65b0\u89c1\u89e3\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u89e3\u91ca\u6027\u548c\u4fe1\u4efb\u5ea6\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u7edf\u4e00\u5e76\u6269\u5c55\u4e86\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.06886", "pdf": "https://arxiv.org/pdf/2505.06886", "abs": "https://arxiv.org/abs/2505.06886", "authors": ["Ahmed Qazi", "Hamd Jalil", "Asim Iqbal"], "title": "Mice to Machines: Neural Representations from Visual Cortex for Domain Generalization", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "comment": "12 pages, 8 figures, 1 table", "summary": "The mouse is one of the most studied animal models in the field of systems\nneuroscience. Understanding the generalized patterns and decoding the neural\nrepresentations that are evoked by the diverse range of natural scene stimuli\nin the mouse visual cortex is one of the key quests in computational vision. In\nrecent years, significant parallels have been drawn between the primate visual\ncortex and hierarchical deep neural networks. However, their generalized\nefficacy in understanding mouse vision has been limited. In this study, we\ninvestigate the functional alignment between the mouse visual cortex and deep\nlearning models for object classification tasks. We first introduce a\ngeneralized representational learning strategy that uncovers a striking\nresemblance between the functional mapping of the mouse visual cortex and\nhigh-performing deep learning models on both top-down (population-level) and\nbottom-up (single cell-level) scenarios. Next, this representational similarity\nacross the two systems is further enhanced by the addition of Neural Response\nNormalization (NeuRN) layer, inspired by the activation profile of excitatory\nand inhibitory neurons in the visual cortex. To test the performance effect of\nNeuRN on real-world tasks, we integrate it into deep learning models and\nobserve significant improvements in their robustness against data shifts in\ndomain generalization tasks. Our work proposes a novel framework for comparing\nthe functional architecture of the mouse visual cortex with deep learning\nmodels. Our findings carry broad implications for the development of advanced\nAI models that draw inspiration from the mouse visual cortex, suggesting that\nthese models serve as valuable tools for studying the neural representations of\nthe mouse visual cortex and, as a result, enhancing their performance on\nreal-world tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u5c0f\u9f20\u89c6\u89c9\u76ae\u5c42\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u7269\u4f53\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u529f\u80fd\u5bf9\u9f50\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u8868\u5f81\u5b66\u4e60\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u53d7\u5c0f\u9f20\u89c6\u89c9\u76ae\u5c42\u542f\u53d1\u7684NeuRN\u5c42\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u9886\u57df\u6cdb\u5316\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u7406\u89e3\u5c0f\u9f20\u89c6\u89c9\u76ae\u5c42\u7684\u795e\u7ecf\u8868\u5f81\u6a21\u5f0f\uff0c\u5e76\u63a2\u7d22\u5176\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u76f8\u4f3c\u6027\uff0c\u4ee5\u63d0\u5347AI\u6a21\u578b\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u63d0\u51fa\u901a\u7528\u7684\u8868\u5f81\u5b66\u4e60\u7b56\u7565\uff0c\u5f15\u5165NeuRN\u5c42\uff08\u53d7\u5c0f\u9f20\u89c6\u89c9\u76ae\u5c42\u795e\u7ecf\u6fc0\u6d3b\u6a21\u5f0f\u542f\u53d1\uff09\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cNeuRN\u5c42\u7684\u52a0\u5165\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u9886\u57df\u6cdb\u5316\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u4e14\u5c0f\u9f20\u89c6\u89c9\u76ae\u5c42\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u529f\u80fd\u6620\u5c04\u4e0a\u9ad8\u5ea6\u76f8\u4f3c\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u8be5\u6846\u67b6\u4e3a\u6bd4\u8f83\u5c0f\u9f20\u89c6\u89c9\u76ae\u5c42\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u529f\u80fd\u67b6\u6784\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5bf9\u5f00\u53d1\u53d7\u751f\u7269\u542f\u53d1\u7684AI\u6a21\u578b\u5177\u6709\u5e7f\u6cdb\u610f\u4e49\u3002"}}
{"id": "2409.04300", "pdf": "https://arxiv.org/pdf/2409.04300", "abs": "https://arxiv.org/abs/2409.04300", "authors": ["Oliver Weissl", "Evgenii Egorov"], "title": "Equivariant Machine Learning Decoder for 3D Toric Codes", "categories": ["quant-ph", "cs.LG"], "comment": null, "summary": "Mitigating errors in computing and communication systems has seen a great\ndeal of research since the beginning of the widespread use of these\ntechnologies. However, as we develop new methods to do computation or\ncommunication, we also need to reiterate the method used to deal with errors.\nWithin the field of quantum computing, error correction is getting a lot of\nattention since errors can propagate fast and invalidate results, which makes\nthe theoretical exponential speed increase in computation time, compared to\ntraditional systems, obsolete. To correct errors in quantum systems,\nerror-correcting codes are used. A subgroup of codes, topological codes, is\ncurrently the focus of many research papers. Topological codes represent parity\ncheck matrices corresponding to graphs embedded on a $d$-dimensional surface.\nFor our research, the focus lies on the toric code with a 3D square lattice.\nThe goal of any decoder is robustness to noise, which can increase with code\nsize. However, a reasonable decoder performance scales polynomially with\nlattice size. As error correction is a time-sensitive operation, we propose a\nneural network using an inductive bias: equivariance. This allows the network\nto learn from a rather small subset of the exponentially growing training space\nof possible inputs. In addition, we investigate how transformer networks can\nhelp in correction. These methods will be compared with various configurations\nand previously published methods of decoding errors in the 3D toric code.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u89e3\u7801\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad83D\u73af\u9762\u7801\u4e2d\u7684\u9519\u8bef\u7ea0\u6b63\u6548\u7387\uff0c\u7279\u522b\u5173\u6ce8\u4e86\u7b49\u53d8\u6027\u548cTransformer\u7f51\u7edc\u7684\u6f5c\u529b\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u9519\u8bef\u4f20\u64ad\u901f\u5ea6\u5feb\u4e14\u4f1a\u62b5\u6d88\u5176\u7406\u8bba\u4e0a\u7684\u6307\u6570\u7ea7\u901f\u5ea6\u4f18\u52bf\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u7684\u9519\u8bef\u7ea0\u6b63\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5177\u6709\u7b49\u53d8\u6027\u7684\u795e\u7ecf\u7f51\u7edc\u548cTransformer\u7f51\u7edc\uff0c\u4ece\u6307\u6570\u589e\u957f\u7684\u8bad\u7ec3\u7a7a\u95f4\u4e2d\u7684\u4e00\u5c0f\u90e8\u5206\u5b66\u4e60\uff0c\u4ee5\u63d0\u9ad8\u89e3\u7801\u6548\u7387\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u914d\u7f6e\u548c\u5df2\u6709\u89e3\u7801\u65b9\u6cd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u795e\u7ecf\u7f51\u7edc\uff08\u5c24\u5176\u662f\u7ed3\u5408\u7b49\u53d8\u6027\u548cTransformer\uff09\u57283D\u73af\u9762\u7801\u7684\u9519\u8bef\u7ea0\u6b63\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u9ad8\u6548\u5904\u7406\u566a\u58f0\u95ee\u9898\u3002"}}
{"id": "2505.06894", "pdf": "https://arxiv.org/pdf/2505.06894", "abs": "https://arxiv.org/abs/2505.06894", "authors": ["Ahmed Qazi", "Abdul Basit", "Asim Iqbal"], "title": "NeuGen: Amplifying the 'Neural' in Neural Radiance Fields for Domain Generalization", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "comment": "18 pages, 6 figures", "summary": "Neural Radiance Fields (NeRF) have significantly advanced the field of novel\nview synthesis, yet their generalization across diverse scenes and conditions\nremains challenging. Addressing this, we propose the integration of a novel\nbrain-inspired normalization technique Neural Generalization (NeuGen) into\nleading NeRF architectures which include MVSNeRF and GeoNeRF. NeuGen extracts\nthe domain-invariant features, thereby enhancing the models' generalization\ncapabilities. It can be seamlessly integrated into NeRF architectures and\ncultivates a comprehensive feature set that significantly improves accuracy and\nrobustness in image rendering. Through this integration, NeuGen shows improved\nperformance on benchmarks on diverse datasets across state-of-the-art NeRF\narchitectures, enabling them to generalize better across varied scenes. Our\ncomprehensive evaluations, both quantitative and qualitative, confirm that our\napproach not only surpasses existing models in generalizability but also\nmarkedly improves rendering quality. Our work exemplifies the potential of\nmerging neuroscientific principles with deep learning frameworks, setting a new\nprecedent for enhanced generalizability and efficiency in novel view synthesis.\nA demo of our study is available at https://neugennerf.github.io.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faNeuGen\uff0c\u4e00\u79cd\u57fa\u4e8e\u8111\u542f\u53d1\u7684\u5f52\u4e00\u5316\u6280\u672f\uff0c\u5d4c\u5165NeRF\u67b6\u6784\u4ee5\u63d0\u5347\u8de8\u573a\u666f\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u6539\u5584\u6e32\u67d3\u8d28\u91cf\u548c\u7cbe\u5ea6\u3002", "motivation": "NeRF\u5728\u591a\u573a\u666f\u548c\u6761\u4ef6\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u8111\u542f\u53d1\u7684\u6280\u672f\u589e\u5f3a\u5176\u6cdb\u5316\u6027\u3002", "method": "\u5c06NeuGen\u96c6\u6210\u5230MVSNeRF\u548cGeoNeRF\u7b49NeRF\u67b6\u6784\u4e2d\uff0c\u63d0\u53d6\u57df\u4e0d\u53d8\u7279\u5f81\u4ee5\u63d0\u5347\u6a21\u578b\u80fd\u529b\u3002", "result": "NeuGen\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u6cdb\u5316\u6027\u548c\u6e32\u67d3\u8d28\u91cf\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u7ed3\u5408\u795e\u7ecf\u79d1\u5b66\u539f\u7406\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u4e3a\u89c6\u56fe\u5408\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u6cdb\u5316\u6027\u548c\u6548\u7387\u6807\u51c6\u3002"}}
{"id": "2505.06243", "pdf": "https://arxiv.org/pdf/2505.06243", "abs": "https://arxiv.org/abs/2505.06243", "authors": ["Mykola Kozlenko"], "title": "Supervised machine learning based signal demodulation in chaotic communications", "categories": ["eess.SP", "cs.LG", "68T07 (Primary) 94A12, 94A13, 94A14 (Secondary)", "I.2.6; C.2"], "comment": "5 pages, 3 figures, 1 table. This paper was originally published in\n  2022 International Conference on Innovative Solutions in Software Engineering\n  (ICISSE), available: https://zenodo.org/records/7512427", "summary": "A chaotic modulation scheme is an efficient wideband communication method. It\nutilizes the deterministic chaos to generate pseudo-random carriers. Chaotic\nbifurcation parameter modulation is one of the well-known and widely-used\ntechniques. This paper presents the machine learning based demodulation\napproach for the bifurcation parameter keying. It presents the structure of a\nconvolutional neural network as well as performance metrics values for signals\ngenerated with the chaotic logistic map. The paper provides an assessment of\nthe overall accuracy for binary signals. It reports the accuracy value of 0.88\nfor the bifurcation parameter deviation of 1.34% in the presence of additive\nwhite Gaussian noise at the normalized signal-to-noise ratio value of 20 dB for\nbalanced dataset.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u89e3\u8c03\u65b9\u6cd5\uff0c\u7528\u4e8e\u6df7\u6c8c\u5206\u5c94\u53c2\u6570\u952e\u63a7\u6280\u672f\uff0c\u901a\u8fc7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8bc4\u4f30\u4e86\u5728\u52a0\u6027\u9ad8\u65af\u767d\u566a\u58f0\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u6df7\u6c8c\u8c03\u5236\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5bbd\u5e26\u901a\u4fe1\u65b9\u6cd5\uff0c\u4f46\u4f20\u7edf\u89e3\u8c03\u65b9\u6cd5\u53ef\u80fd\u4e0d\u591f\u9ad8\u6548\u6216\u51c6\u786e\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u673a\u5668\u5b66\u4e60\u5728\u89e3\u8c03\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5bf9\u7531\u6df7\u6c8c\u903b\u8f91\u6620\u5c04\u751f\u6210\u7684\u4fe1\u53f7\u8fdb\u884c\u89e3\u8c03\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u5206\u5c94\u53c2\u6570\u952e\u63a7\u6280\u672f\u3002", "result": "\u5728\u52a0\u6027\u9ad8\u65af\u767d\u566a\u58f0\u73af\u5883\u4e0b\uff0c\u5f53\u5206\u5c94\u53c2\u6570\u504f\u5dee\u4e3a1.34%\u3001\u4fe1\u566a\u6bd4\u4e3a20 dB\u65f6\uff0c\u4e8c\u5143\u4fe1\u53f7\u89e3\u8c03\u7684\u51c6\u786e\u7387\u8fbe\u52300.88\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u5c24\u5176\u662fCNN\u5728\u6df7\u6c8c\u4fe1\u53f7\u89e3\u8c03\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\uff0c\u4e3a\u5bbd\u5e26\u901a\u4fe1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u8c03\u65b9\u6848\u3002"}}
{"id": "2505.06245", "pdf": "https://arxiv.org/pdf/2505.06245", "abs": "https://arxiv.org/abs/2505.06245", "authors": ["Dominic Schneider", "Lutz Rapp", "Christoph Ament"], "title": "A Transformer-Based Approach for Diagnosing Fault Cases in Optical Fiber Amplifiers", "categories": ["eess.SP", "cs.LG"], "comment": "This paper has been accepted for publication at the 25th\n  International Conference on Transparent Optical Networks (ICTON) 2025", "summary": "A transformer-based deep learning approach is presented that enables the\ndiagnosis of fault cases in optical fiber amplifiers using condition-based\nmonitoring time series data. The model, Inverse Triple-Aspect Self-Attention\nTransformer (ITST), uses an encoder-decoder architecture, utilizing three\nfeature extraction paths in the encoder, feature-engineered data for the\ndecoder and a self-attention mechanism. The results show that ITST outperforms\nstate-of-the-art models in terms of classification accuracy, which enables\npredictive maintenance for optical fiber amplifiers, reducing network downtimes\nand maintenance costs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5ITST\uff0c\u7528\u4e8e\u901a\u8fc7\u65f6\u5e8f\u6570\u636e\u8bca\u65ad\u5149\u7ea4\u653e\u5927\u5668\u6545\u969c\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u901a\u8fc7\u63d0\u5347\u6545\u969c\u8bca\u65ad\u7684\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u5149\u7ea4\u653e\u5927\u5668\u7f51\u7edc\u7684\u4e2d\u65ad\u65f6\u95f4\u548c\u7ef4\u62a4\u6210\u672c\u3002", "method": "\u4f7f\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u7f16\u7801\u5668\u5305\u542b\u4e09\u6761\u7279\u5f81\u63d0\u53d6\u8def\u5f84\uff0c\u89e3\u7801\u5668\u91c7\u7528\u7279\u5f81\u5de5\u7a0b\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "ITST\u5728\u5206\u7c7b\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "ITST\u80fd\u6709\u6548\u652f\u6301\u5149\u7ea4\u653e\u5927\u5668\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\uff0c\u63d0\u5347\u7f51\u7edc\u6548\u7387\u548c\u964d\u4f4e\u6210\u672c\u3002"}}
{"id": "2505.06913", "pdf": "https://arxiv.org/pdf/2505.06913", "abs": "https://arxiv.org/abs/2505.06913", "authors": ["Brian Challita", "Pierre Parrend"], "title": "RedTeamLLM: an Agentic AI framework for offensive security", "categories": ["cs.CR", "cs.AI", "cs.CY"], "comment": null, "summary": "From automated intrusion testing to discovery of zero-day attacks before\nsoftware launch, agentic AI calls for great promises in security engineering.\nThis strong capability is bound with a similar threat: the security and\nresearch community must build up its models before the approach is leveraged by\nmalicious actors for cybercrime. We therefore propose and evaluate RedTeamLLM,\nan integrated architecture with a comprehensive security model for\nautomatization of pentest tasks. RedTeamLLM follows three key steps:\nsummarizing, reasoning and act, which embed its operational capacity. This\nnovel framework addresses four open challenges: plan correction, memory\nmanagement, context window constraint, and generality vs. specialization.\nEvaluation is performed through the automated resolution of a range of\nentry-level, but not trivial, CTF challenges. The contribution of the reasoning\ncapability of our agentic AI framework is specifically evaluated.", "AI": {"tldr": "RedTeamLLM\u662f\u4e00\u4e2a\u96c6\u6210\u7684AI\u67b6\u6784\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u6e17\u900f\u6d4b\u8bd5\u4efb\u52a1\uff0c\u901a\u8fc7\u603b\u7ed3\u3001\u63a8\u7406\u548c\u6267\u884c\u4e09\u4e2a\u5173\u952e\u6b65\u9aa4\u89e3\u51b3\u56db\u9879\u5f00\u653e\u6311\u6218\uff0c\u5e76\u5728CTF\u6311\u6218\u4e2d\u9a8c\u8bc1\u5176\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u968f\u7740AI\u5728\u5b89\u5168\u5de5\u7a0b\u4e2d\u7684\u6f5c\u529b\u589e\u52a0\uff0c\u6076\u610f\u884c\u4e3a\u8005\u4e5f\u53ef\u80fd\u5229\u7528\u7c7b\u4f3c\u6280\u672f\u8fdb\u884c\u7f51\u7edc\u72af\u7f6a\u3002\u56e0\u6b64\uff0c\u5f00\u53d1\u4e00\u4e2a\u5148\u8fdb\u7684AI\u6846\u67b6\uff08\u5982RedTeamLLM\uff09\u4ee5\u63d0\u524d\u5e94\u5bf9\u8fd9\u4e00\u5a01\u80c1\u662f\u6709\u5fc5\u8981\u7684\u3002", "method": "RedTeamLLM\u67b6\u6784\u9075\u5faa\u603b\u7ed3\u3001\u63a8\u7406\u548c\u6267\u884c\u7684\u6b65\u9aa4\uff0c\u89e3\u51b3\u8ba1\u5212\u4fee\u6b63\u3001\u5185\u5b58\u7ba1\u7406\u3001\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u4ee5\u53ca\u901a\u7528\u6027\u4e0e\u4e13\u4e1a\u5316\u7684\u5e73\u8861\u95ee\u9898\u3002", "result": "\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u5316\u89e3\u51b3\u521d\u7ea7\u4f46\u975e\u5e73\u51e1\u7684CTF\u6311\u6218\u8fdb\u884c\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u4e86\u5176\u63a8\u7406\u80fd\u529b\u7684\u8d21\u732e\u3002", "conclusion": "RedTeamLLM\u4e3a\u5b89\u5168\u5de5\u7a0b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684AI\u9a71\u52a8\u6846\u67b6\uff0c\u80fd\u591f\u5728\u6076\u610f\u884c\u4e3a\u8005\u4e4b\u524d\u8bc6\u522b\u548c\u5e94\u5bf9\u6f5c\u5728\u5a01\u80c1\u3002"}}
{"id": "2505.06249", "pdf": "https://arxiv.org/pdf/2505.06249", "abs": "https://arxiv.org/abs/2505.06249", "authors": ["Geraldine Henningsen"], "title": "An Early Warning Model for Forced Displacement", "categories": ["stat.AP", "cs.CY", "cs.LG"], "comment": "13 pages, 6 figures", "summary": "Monitoring tools for anticipatory action are increasingly gaining traction to\nimprove the efficiency and timeliness of humanitarian responses. Whilst\npredictive models can now forecast conflicts with high accuracy, translating\nthese predictions into potential forced displacement movements remains\nchallenging because it is often unclear which precise events will trigger\nsignificant population movements. This paper presents a novel monitoring\napproach for refugee and asylum seeker flows that addresses this challenge.\nUsing gradient boosting classification, we combine conflict forecasts with a\ncomprehensive set of economic, political, and demographic variables to assess\ntwo distinct risks at the country of origin: the likelihood of significant\ndisplacement flows and the probability of sudden increases in these flows. The\nmodel generates country-specific monthly risk indices for these two events with\nprediction horizons of one, three, and six months. Our analysis shows high\naccuracy in predicting significant displacement flows and good accuracy in\nforecasting sudden increases in displacement--the latter being inherently more\ndifficult to predict, given the complexity of displacement triggers. We achieve\nthese results by including predictive factors beyond conflict, thereby\ndemonstrating that forced displacement risks can be assessed through an\nintegrated analysis of multiple country-level indicators. Whilst these risk\nindices provide valuable quantitative support for humanitarian planning, they\nshould always be understood as decision-support tools within a broader\nanalytical framework.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u96be\u6c11\u548c\u5bfb\u6c42\u5e87\u62a4\u8005\u6d41\u52a8\u76d1\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u63d0\u5347\u5206\u7c7b\u7ed3\u5408\u51b2\u7a81\u9884\u6d4b\u4e0e\u7ecf\u6d4e\u3001\u653f\u6cbb\u53ca\u4eba\u53e3\u53d8\u91cf\uff0c\u8bc4\u4f30\u6765\u6e90\u56fd\u7684\u4e24\u79cd\u98ce\u9669\uff1a\u663e\u8457\u6d41\u52a8\u7684\u53ef\u80fd\u6027\u548c\u6d41\u52a8\u7a81\u7136\u589e\u52a0\u7684\u6982\u7387\uff0c\u5e76\u751f\u6210\u4e86\u9ad8\u7cbe\u5ea6\u7684\u6708\u5ea6\u98ce\u9669\u6307\u6570\u3002", "motivation": "\u5f53\u524d\u9884\u6d4b\u6a21\u578b\u867d\u80fd\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u51b2\u7a81\uff0c\u4f46\u5177\u4f53\u54ea\u4e9b\u4e8b\u4ef6\u4f1a\u5f15\u53d1\u5927\u89c4\u6a21\u4eba\u53e3\u6d41\u52a8\u4ecd\u4e0d\u660e\u786e\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u591a\u6307\u6807\u7efc\u5408\u5206\u6790\u8bc4\u4f30\u5f3a\u8feb\u6027\u6d41\u52a8\u98ce\u9669\uff0c\u4e3a\u4eba\u9053\u4e3b\u4e49\u884c\u52a8\u63d0\u4f9b\u91cf\u5316\u652f\u6301\u3002", "method": "\u91c7\u7528\u68af\u5ea6\u63d0\u5347\u5206\u7c7b\u6a21\u578b\uff0c\u7ed3\u5408\u51b2\u7a81\u9884\u6d4b\u53ca\u7ecf\u6d4e\u3001\u653f\u6cbb\u3001\u4eba\u53e3\u7b49\u591a\u7ef4\u53d8\u91cf\uff0c\u751f\u62101\u30013\u30016\u4e2a\u6708\u9884\u6d4b\u671f\u7684\u56fd\u5bb6\u6708\u5ea6\u98ce\u9669\u6307\u6570\uff0c\u5206\u522b\u8bc4\u4f30\u663e\u8457\u6d41\u52a8\u548c\u6d41\u52a8\u7a81\u589e\u7684\u4e24\u79cd\u98ce\u9669\u3002", "result": "\u6a21\u578b\u5728\u9884\u6d4b\u663e\u8457\u6d41\u52a8\u65f6\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\uff0c\u800c\u5bf9\u6d41\u52a8\u7a81\u589e\u7684\u9884\u6d4b\u4e5f\u6709\u8f83\u597d\u8868\u73b0\uff08\u540e\u8005\u7531\u4e8e\u89e6\u53d1\u590d\u6742\u6027\u66f4\u96be\u9884\u6d4b\uff09\u3002\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u591a\u6307\u6807\u7efc\u5408\u5206\u6790\u53ef\u4ee5\u6709\u6548\u8bc4\u4f30\u5f3a\u8feb\u6027\u6d41\u52a8\u98ce\u9669\u3002", "conclusion": "\u98ce\u9669\u6307\u6570\u4e3a\u4eba\u9053\u4e3b\u4e49\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u91cf\u5316\u5de5\u5177\uff0c\u4f46\u9700\u4f5c\u4e3a\u66f4\u5e7f\u6cdb\u5206\u6790\u6846\u67b6\u4e2d\u7684\u51b3\u7b56\u652f\u6301\u5de5\u5177\u4f7f\u7528\u3002\u591a\u6307\u6807\u7efc\u5408\u5206\u6790\u662f\u8bc4\u4f30\u6d41\u52a8\u98ce\u9669\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2505.06963", "pdf": "https://arxiv.org/pdf/2505.06963", "abs": "https://arxiv.org/abs/2505.06963", "authors": ["Tarik Houichime", "Younes EL Amrani"], "title": "Reinforcement Learning-Based Monocular Vision Approach for Autonomous UAV Landing", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "This paper introduces an innovative approach for the autonomous landing of\nUnmanned Aerial Vehicles (UAVs) using only a front-facing monocular camera,\ntherefore obviating the requirement for depth estimation cameras. Drawing on\nthe inherent human estimating process, the proposed method reframes the landing\ntask as an optimization problem. The UAV employs variations in the visual\ncharacteristics of a specially designed lenticular circle on the landing pad,\nwhere the perceived color and form provide critical information for estimating\nboth altitude and depth. Reinforcement learning algorithms are utilized to\napproximate the functions governing these estimations, enabling the UAV to\nascertain ideal landing settings via training. This method's efficacy is\nassessed by simulations and experiments, showcasing its potential for robust\nand accurate autonomous landing without dependence on complex sensor setups.\nThis research contributes to the advancement of cost-effective and efficient\nUAV landing solutions, paving the way for wider applicability across various\nfields.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528\u524d\u7f6e\u5355\u76ee\u76f8\u673a\u5b9e\u73b0\u65e0\u4eba\u673a\u81ea\u4e3b\u7740\u9646\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u65e0\u9700\u6df1\u5ea6\u4f30\u8ba1\u76f8\u673a\uff0c\u901a\u8fc7\u4f18\u5316\u95ee\u9898\u548c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u65e0\u4eba\u673a\u81ea\u4e3b\u7740\u9646\u5bf9\u590d\u6742\u4f20\u611f\u5668\u4f9d\u8d56\u7684\u95ee\u9898\uff0c\u5e76\u964d\u4f4e\u7cfb\u7edf\u6210\u672c\uff0c\u540c\u65f6\u501f\u9274\u4eba\u7c7b\u4f30\u8ba1\u8fc7\u7a0b\u3002", "method": "\u5229\u7528\u4f18\u5316\u7684\u57fa\u4e8e\u89c6\u89c9\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u7279\u6b8a\u7684\u7740\u9646\u57ab\u56fe\u6848\uff0c\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4f30\u8ba1\u9ad8\u5ea6\u548c\u6df1\u5ea6\u3002\u4ee5\u8bad\u7ec3\u65e0\u4eba\u673a\u627e\u5230\u6700\u4f73\u7740\u9646\u53c2\u6570\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u7a33\u5065\u4e14\u9ad8\u7cbe\u5ea6\u7684\u81ea\u4e3b\u7740\u9646\uff0c\u65e0\u9700\u590d\u6742\u4f20\u611f\u5668\u914d\u7f6e\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86\u7ecf\u6d4e\u9ad8\u6548\u7684\u65e0\u4eba\u673a\u7740\u9646\u89e3\u51b3\u65b9\u6848\u7684\u53d1\u5c55\uff0c\u6269\u5c55\u4e86\u65e0\u4eba\u673a\u5728\u5404\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.06263", "pdf": "https://arxiv.org/pdf/2505.06263", "abs": "https://arxiv.org/abs/2505.06263", "authors": ["Yiping Meng", "Yiming Sun"], "title": "From Biometrics to Environmental Control: AI-Enhanced Digital Twins for Personalized Health Interventions in Healing Landscapes", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "The dynamic nature of human health and comfort calls for adaptive systems\nthat respond to individual physiological needs in real time. This paper\npresents an AI-enhanced digital twin framework that integrates biometric\nsignals, specifically electrocardiogram (ECG) data, with environmental\nparameters such as temperature, humidity, and ventilation. Leveraging\nIoT-enabled sensors and biometric monitoring devices, the system continuously\nacquires, synchronises, and preprocesses multimodal data streams to construct a\nresponsive virtual replica of the physical environment. To validate this\nframework, a detailed case study is conducted using the MIT-BIH noise stress\ntest dataset. ECG signals are filtered and segmented using dynamic sliding\nwindows, followed by extracting heart rate variability (HRV) features such as\nSDNN, BPM, QTc, and LF/HF ratio. Relative deviation metrics are computed\nagainst clean baselines to quantify stress responses. A random forest\nclassifier is trained to predict stress levels across five categories, and\nShapley Additive exPlanations (SHAP) is used to interpret model behaviour and\nidentify key contributing features. These predictions are mapped to a\nstructured set of environmental interventions using a Five Level Stress\nIntervention Mapping, which activates multi-scale responses across personal,\nroom, building, and landscape levels. This integration of physiological\ninsight, explainable AI, and adaptive control establishes a new paradigm for\nhealth-responsive built environments. It lays the foundation for the future\ndevelopment of intelligent, personalised healing spaces.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2aAI\u589e\u5f3a\u7684\u6570\u5b57\u5b6a\u751f\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408ECG\u6570\u636e\u548c\u73af\u5883\u53c2\u6570\uff0c\u5b9e\u65f6\u54cd\u5e94\u4e2a\u4f53\u751f\u7406\u9700\u6c42\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u52a8\u6001\u7684\u4eba\u7c7b\u5065\u5eb7\u548c\u8212\u9002\u9700\u6c42\u9700\u8981\u80fd\u5b9e\u65f6\u54cd\u5e94\u4e2a\u4f53\u751f\u7406\u53d8\u5316\u7684\u9002\u5e94\u6027\u7cfb\u7edf\u3002", "method": "\u5229\u7528IoT\u4f20\u611f\u5668\u548c\u751f\u7269\u8bc6\u522b\u8bbe\u5907\u91c7\u96c6\u591a\u6a21\u6001\u6570\u636e\uff0c\u6784\u5efa\u6570\u5b57\u5b6a\u751f\u6a21\u578b\uff0c\u901a\u8fc7\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u548cSHAP\u65b9\u6cd5\u9884\u6d4b\u548c\u89e3\u91ca\u538b\u529b\u6c34\u5e73\u3002", "result": "\u7cfb\u7edf\u80fd\u51c6\u786e\u9884\u6d4b\u538b\u529b\u6c34\u5e73\u5e76\u89e6\u53d1\u591a\u5c3a\u5ea6\u73af\u5883\u5e72\u9884\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5065\u5eb7\u54cd\u5e94\u578b\u5efa\u7b51\u73af\u5883\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u4e3a\u672a\u6765\u667a\u80fd\u4e2a\u6027\u5316\u7597\u6108\u7a7a\u95f4\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.07012", "pdf": "https://arxiv.org/pdf/2505.07012", "abs": "https://arxiv.org/abs/2505.07012", "authors": ["Hao Xu", "Yinqiao Wang", "Niloy J. Mitra", "Shuaicheng Liu", "Pheng-Ann Heng", "Chi-Wing Fu"], "title": "Hand-Shadow Poser", "categories": ["cs.CG", "cs.AI"], "comment": "SIGGRAPH 2025 (ACM TOG)", "summary": "Hand shadow art is a captivating art form, creatively using hand shadows to\nreproduce expressive shapes on the wall. In this work, we study an inverse\nproblem: given a target shape, find the poses of left and right hands that\ntogether best produce a shadow resembling the input. This problem is\nnontrivial, since the design space of 3D hand poses is huge while being\nrestrictive due to anatomical constraints. Also, we need to attend to the\ninput's shape and crucial features, though the input is colorless and\ntextureless. To meet these challenges, we design Hand-Shadow Poser, a\nthree-stage pipeline, to decouple the anatomical constraints (by hand) and\nsemantic constraints (by shadow shape): (i) a generative hand assignment module\nto explore diverse but reasonable left/right-hand shape hypotheses; (ii) a\ngeneralized hand-shadow alignment module to infer coarse hand poses with a\nsimilarity-driven strategy for selecting hypotheses; and (iii) a\nshadow-feature-aware refinement module to optimize the hand poses for physical\nplausibility and shadow feature preservation. Further, we design our pipeline\nto be trainable on generic public hand data, thus avoiding the need for any\nspecialized training dataset. For method validation, we build a benchmark of\n210 diverse shadow shapes of varying complexity and a comprehensive set of\nmetrics, including a novel DINOv2-based evaluation metric. Through extensive\ncomparisons with multiple baselines and user studies, our approach is\ndemonstrated to effectively generate bimanual hand poses for a large variety of\nhand shapes for over 85% of the benchmark cases.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHand-Shadow Poser\u7684\u4e09\u9636\u6bb5\u7ba1\u9053\uff0c\u7528\u4e8e\u89e3\u51b3\u4ece\u76ee\u6807\u5f62\u72b6\u751f\u6210\u53cc\u624b\u59ff\u52bf\u4ee5\u6295\u5f71\u5339\u914d\u8f93\u5165\u5f62\u72b6\u7684\u53cd\u5411\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u572885%\u7684\u6848\u4f8b\u4e2d\u6709\u6548\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u4ece\u5f71\u5b50\u5f62\u72b6\u53cd\u5411\u63a8\u5bfc\u53cc\u624b\u59ff\u52bf\u7684\u590d\u6742\u95ee\u9898\uff0c\u6311\u6218\u5728\u4e8e\u5de8\u5927\u76843D\u624b\u90e8\u59ff\u52bf\u8bbe\u8ba1\u7a7a\u95f4\u548c\u89e3\u5256\u5b66\u9650\u5236\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u9636\u6bb5\uff1a\u751f\u6210\u624b\u90e8\u5206\u914d\u6a21\u5757\u3001\u5e7f\u4e49\u624b\u5f71\u5bf9\u9f50\u6a21\u5757\u548c\u5f71\u5b50\u7279\u5f81\u611f\u77e5\u4f18\u5316\u6a21\u5757\uff0c\u65e0\u9700\u4e13\u95e8\u8bad\u7ec3\u6570\u636e\uff0c\u57fa\u4e8e\u516c\u5f00\u624b\u90e8\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7\u6784\u5efa210\u4e2a\u591a\u6837\u5316\u5f71\u5b50\u5f62\u72b6\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u6307\u6807\u8bc4\u4f30\uff0c\u5305\u62ec\u57fa\u4e8eDINOv2\u7684\u65b0\u8bc4\u4ef7\u6307\u6807\uff0c\u65b9\u6cd5\u572885%\u7684\u6848\u4f8b\u4e2d\u6210\u529f\u751f\u6210\u53cc\u624b\u59ff\u52bf\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u591a\u6837\u5316\u7684\u53cc\u624b\u59ff\u52bf\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u624b\u5f71\u5f62\u72b6\uff0c\u4e14\u65e0\u9700\u4e13\u7528\u8bad\u7ec3\u6570\u636e\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.06291", "pdf": "https://arxiv.org/pdf/2505.06291", "abs": "https://arxiv.org/abs/2505.06291", "authors": ["Wei Xiong", "Junming Lin", "Jiangtong Li", "Jie Li", "Changjun Jiang"], "title": "ALFEE: Adaptive Large Foundation Model for EEG Representation", "categories": ["eess.SP", "cs.CE", "cs.HC", "cs.LG"], "comment": "17pages, 17 figures", "summary": "While foundation models excel in text, image, and video domains, the critical\nbiological signals, particularly electroencephalography(EEG), remain\nunderexplored. EEG benefits neurological research with its high temporal\nresolution, operational practicality, and safety profile. However, low\nsignal-to-noise ratio, inter-subject variability, and cross-paradigm\ndifferences hinder the generalization of current models. Existing methods often\nemploy simplified strategies, such as a single loss function or a\nchannel-temporal joint representation module, and suffer from a domain gap\nbetween pretraining and evaluation tasks that compromises efficiency and\nadaptability. To address these limitations, we propose the Adaptive Large\nFoundation model for EEG signal representation(ALFEE) framework, a novel hybrid\ntransformer architecture with two learning stages for robust EEG representation\nlearning. ALFEE employs a hybrid attention that separates channel-wise feature\naggregation from temporal dynamics modeling, enabling robust EEG representation\nwith variable channel configurations. A channel encoder adaptively compresses\nvariable channel information, a temporal encoder captures task-guided\nevolution, and a hybrid decoder reconstructs signals in both temporal and\nfrequency domains. During pretraining, ALFEE optimizes task prediction, channel\nand temporal mask reconstruction, and temporal forecasting to enhance\nmulti-scale and multi-channel representation. During fine-tuning, a full-model\nadaptation with a task-specific token dictionary and a cross-attention layer\nboosts performance across multiple tasks. After 25,000 hours of pretraining,\nextensive experimental results on six downstream EEG tasks demonstrate the\nsuperior performance of ALFEE over existing models. Our ALFEE framework\nestablishes a scalable foundation for biological signal analysis with\nimplementation at https://github.com/xw1216/ALFEE.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aALFEE\u7684\u65b0\u578b\u6df7\u5408\u53d8\u538b\u5668\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3EEG\u4fe1\u53f7\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u901a\u7528\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5b66\u4e60\u548c\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "EEG\u4fe1\u53f7\u5728\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u4e2d\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u5f53\u524d\u6a21\u578b\u5728\u6cdb\u5316\u6027\u4e0a\u53d7\u9650\u4e8e\u4fe1\u566a\u6bd4\u4f4e\u3001\u4e2a\u4f53\u95f4\u5dee\u5f02\u5927\u548c\u4ea4\u53c9\u8303\u5f0f\u5dee\u5f02\u7b49\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u8fc7\u4e8e\u7b80\u5316\u4e14\u5b58\u5728\u9884\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u4efb\u52a1\u95f4\u7684\u9886\u57df\u5dee\u8ddd\u3002", "method": "ALFEE\u91c7\u7528\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5206\u522b\u5904\u7406\u901a\u9053\u548c\u65f6\u95f4\u7279\u5f81\uff0c\u5305\u62ec\u901a\u9053\u7f16\u7801\u5668\u3001\u65f6\u95f4\u7f16\u7801\u5668\u548c\u6df7\u5408\u89e3\u7801\u5668\u3002\u9884\u8bad\u7ec3\u9636\u6bb5\u4f18\u5316\u591a\u4efb\u52a1\u76ee\u6807\uff0c\u5fae\u8c03\u9636\u6bb5\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u6807\u8bb0\u5b57\u5178\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u63d0\u5347\u6027\u80fd\u3002", "result": "\u7ecf\u8fc725,000\u5c0f\u65f6\u7684\u9884\u8bad\u7ec3\uff0cALFEE\u5728\u516d\u4e2a\u4e0b\u6e38EEG\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "ALFEE\u6846\u67b6\u4e3a\u751f\u7269\u4fe1\u53f7\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u4e14\u5df2\u5728GitHub\u4e0a\u5f00\u6e90\u3002"}}
{"id": "2505.07013", "pdf": "https://arxiv.org/pdf/2505.07013", "abs": "https://arxiv.org/abs/2505.07013", "authors": ["Jitesh Joshi", "Youngjun Cho"], "title": "Efficient and Robust Multidimensional Attention in Remote Physiological Sensing through Target Signal Constrained Factorization", "categories": ["cs.CV", "cs.AI"], "comment": "25 pages, 6 figures", "summary": "Remote physiological sensing using camera-based technologies offers\ntransformative potential for non-invasive vital sign monitoring across\nhealthcare and human-computer interaction domains. Although deep learning\napproaches have advanced the extraction of physiological signals from video\ndata, existing methods have not been sufficiently assessed for their robustness\nto domain shifts. These shifts in remote physiological sensing include\nvariations in ambient conditions, camera specifications, head movements, facial\nposes, and physiological states which often impact real-world performance\nsignificantly. Cross-dataset evaluation provides an objective measure to assess\ngeneralization capabilities across these domain shifts. We introduce Target\nSignal Constrained Factorization module (TSFM), a novel multidimensional\nattention mechanism that explicitly incorporates physiological signal\ncharacteristics as factorization constraints, allowing more precise feature\nextraction. Building on this innovation, we present MMRPhys, an efficient\ndual-branch 3D-CNN architecture designed for simultaneous multitask estimation\nof photoplethysmography (rPPG) and respiratory (rRSP) signals from multimodal\nRGB and thermal video inputs. Through comprehensive cross-dataset evaluation on\nfive benchmark datasets, we demonstrate that MMRPhys with TSFM significantly\noutperforms state-of-the-art methods in generalization across domain shifts for\nrPPG and rRSP estimation, while maintaining a minimal inference latency\nsuitable for real-time applications. Our approach establishes new benchmarks\nfor robust multitask and multimodal physiological sensing and offers a\ncomputationally efficient framework for practical deployment in unconstrained\nenvironments. The web browser-based application featuring on-device real-time\ninference of MMRPhys model is available at\nhttps://physiologicailab.github.io/mmrphys-live", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u7ef4\u6ce8\u610f\u529b\u673a\u5236TSFM\u548c\u9ad8\u6548\u7684\u53cc\u5206\u652f3D-CNN\u67b6\u6784MMRPhys\uff0c\u7528\u4e8e\u4ece\u591a\u6a21\u6001\u89c6\u9891\u6570\u636e\u4e2d\u540c\u65f6\u4f30\u8ba1rPPG\u548crRSP\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u5728\u57df\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8fdc\u7a0b\u751f\u7406\u611f\u77e5\u6280\u672f\u5728\u533b\u7597\u548c\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5bf9\u57df\u504f\u79fb\uff08\u5982\u73af\u5883\u53d8\u5316\u3001\u6444\u50cf\u5934\u89c4\u683c\u7b49\uff09\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86TSFM\uff08\u76ee\u6807\u4fe1\u53f7\u7ea6\u675f\u5206\u89e3\u6a21\u5757\uff09\u2014\u2014\u4e00\u79cd\u7ed3\u5408\u751f\u7406\u4fe1\u53f7\u7279\u6027\u7684\u591a\u7ef4\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u53caMMRPhys\u2014\u2014\u4e00\u79cd\u9ad8\u6548\u7684\u53cc\u5206\u652f3D-CNN\u67b6\u6784\uff0c\u7528\u4e8e\u4eceRGB\u548c\u70ed\u6210\u50cf\u89c6\u9891\u4e2d\u540c\u65f6\u4f30\u8ba1rPPG\u548crRSP\u4fe1\u53f7\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u8868\u660e\uff0cMMRPhys\u7ed3\u5408TSFM\u5728rPPG\u548crRSP\u4f30\u8ba1\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u4fdd\u6301\u4e86\u8f83\u4f4e\u7684\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9c81\u68d2\u7684\u591a\u4efb\u52a1\u548c\u591a\u6a21\u6001\u751f\u7406\u611f\u77e5\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8ba1\u7b97\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u9002\u5408\u5728\u975e\u7ea6\u675f\u73af\u5883\u4e2d\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2505.07020", "pdf": "https://arxiv.org/pdf/2505.07020", "abs": "https://arxiv.org/abs/2505.07020", "authors": ["Suyeon Choi"], "title": "R-CAGE: A Structural Model for Emotion Output Design in Human-AI Interaction", "categories": ["cs.HC", "cs.AI", "cs.CY", "H.5.2"], "comment": "theory-only preprint. Independent research", "summary": "This paper presents R-CAGE (Rhythmic Control Architecture for Guarding Ego),\na theoretical framework for restructuring emotional output in long-term\nhuman-AI interaction. While prior affective computing approaches emphasized\nexpressiveness, immersion, and responsiveness, they often neglected the\ncognitive and structural consequences of repeated emotional engagement. R-CAGE\ninstead conceptualizes emotional output not as reactive expression but as\nethical design structure requiring architectural intervention. The model is\ngrounded in experiential observations of subtle affective symptoms such as\nlocalized head tension, interpretive fixation, and emotional lag arising from\nprolonged interaction with affective AI systems. These indicate a mismatch\nbetween system-driven emotion and user interpretation that cannot be fully\nexplained by biometric data or observable behavior. R-CAGE adopts a\nuser-centered stance prioritizing psychological recovery, interpretive\nautonomy, and identity continuity. The framework consists of four control\nblocks: (1) Control of Rhythmic Expression regulates output pacing to reduce\nfatigue; (2) Architecture of Sensory Structuring adjusts intensity and timing\nof affective stimuli; (3) Guarding of Cognitive Framing reduces semantic\npressure to allow flexible interpretation; (4) Ego-Aligned Response Design\nsupports self-reference recovery during interpretive lag. By structurally\nregulating emotional rhythm, sensory intensity, and interpretive affordances,\nR-CAGE frames emotion not as performative output but as sustainable design\nunit. The goal is to protect users from oversaturation and cognitive overload\nwhile sustaining long-term interpretive agency in AI-mediated environments.", "AI": {"tldr": "R-CAGE\u662f\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u7ed3\u6784\u5316\u63a7\u5236\u60c5\u7eea\u8f93\u51fa\u6765\u6539\u5584\u957f\u671f\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u8ba4\u77e5\u548c\u60c5\u611f\u5065\u5eb7\u3002", "motivation": "\u73b0\u6709\u60c5\u611f\u8ba1\u7b97\u65b9\u6cd5\u8fc7\u4e8e\u5f3a\u8c03\u8868\u8fbe\u6027\u548c\u6c89\u6d78\u611f\uff0c\u5ffd\u89c6\u4e86\u91cd\u590d\u60c5\u611f\u4e92\u52a8\u5bf9\u8ba4\u77e5\u548c\u7ed3\u6784\u7684\u5f71\u54cd\u3002R-CAGE\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "R-CAGE\u5305\u542b\u56db\u4e2a\u63a7\u5236\u6a21\u5757\uff1a\u8282\u594f\u8868\u8fbe\u63a7\u5236\u3001\u611f\u5b98\u7ed3\u6784\u8c03\u6574\u3001\u8ba4\u77e5\u6846\u67b6\u4fdd\u62a4\u548c\u81ea\u6211\u5bf9\u9f50\u54cd\u5e94\u8bbe\u8ba1\u3002", "result": "\u901a\u8fc7\u8c03\u8282\u60c5\u7eea\u8282\u594f\u3001\u611f\u5b98\u5f3a\u5ea6\u548c\u89e3\u91ca\u81ea\u7531\u5ea6\uff0cR-CAGE\u80fd\u51cf\u5c11\u7528\u6237\u7684\u8ba4\u77e5\u8fc7\u8f7d\u5e76\u4fdd\u6301\u5176\u89e3\u91ca\u81ea\u4e3b\u6743\u3002", "conclusion": "R-CAGE\u5c06\u60c5\u7eea\u89c6\u4e3a\u53ef\u6301\u7eed\u8bbe\u8ba1\u5355\u5143\uff0c\u800c\u975e\u8868\u6f14\u6027\u8f93\u51fa\uff0c\u4ece\u800c\u4fdd\u62a4\u7528\u6237\u5728AI\u73af\u5883\u4e2d\u7684\u957f\u671f\u5fc3\u7406\u5065\u5eb7\u3002"}}
{"id": "2505.06310", "pdf": "https://arxiv.org/pdf/2505.06310", "abs": "https://arxiv.org/abs/2505.06310", "authors": ["Tao Shen", "Jethro Browell", "Daniela Castro-Camilo"], "title": "Adaptive Bayesian Very Short-Term Wind Power Forecasting Based on the Generalised Logit Transformation", "categories": ["stat.AP", "cs.LG"], "comment": "31 pages, 10 figures and tables. Submitted to International Journal\n  of Forecasting", "summary": "Wind power plays an increasingly significant role in achieving the 2050 Net\nZero Strategy. Despite its rapid growth, its inherent variability presents\nchallenges in forecasting. Accurately forecasting wind power generation is one\nkey demand for the stable and controllable integration of renewable energy into\nexisting grid operations. This paper proposes an adaptive method for very\nshort-term forecasting that combines the generalised logit transformation with\na Bayesian approach. The generalised logit transformation processes\ndouble-bounded wind power data to an unbounded domain, facilitating the\napplication of Bayesian methods. A novel adaptive mechanism for updating the\ntransformation shape parameter is introduced to leverage Bayesian updates by\nrecovering a small sample of representative data. Four adaptive forecasting\nmethods are investigated, evaluating their advantages and limitations through\nan extensive case study of over 100 wind farms ranging four years in the UK.\nThe methods are evaluated using the Continuous Ranked Probability Score and we\npropose the use of functional reliability diagrams to assess calibration.\nResults indicate that the proposed Bayesian method with adaptive shape\nparameter updating outperforms benchmarks, yielding consistent improvements in\nCRPS and forecast reliability. The method effectively addresses uncertainty,\nensuring robust and accurate probabilistic forecasting which is essential for\ngrid integration and decision-making.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5e7f\u4e49Logit\u53d8\u6362\u548c\u8d1d\u53f6\u65af\u65b9\u6cd5\u7684\u81ea\u9002\u5e94\u77ed\u671f\u98ce\u7535\u529f\u7387\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u66f4\u65b0\u5f62\u72b6\u53c2\u6570\u4f18\u5316\u9884\u6d4b\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u5728100\u591a\u4e2a\u98ce\u7535\u573a\u7684\u6570\u636e\u96c6\u4e2d\u9a8c\u8bc1\uff0c\u8868\u73b0\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u98ce\u7535\u5728\u5b9e\u73b02050\u51c0\u96f6\u76ee\u6807\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u6ce2\u52a8\u6027\u7ed9\u9884\u6d4b\u5e26\u6765\u6311\u6218\u3002\u51c6\u786e\u9884\u6d4b\u98ce\u7535\u529f\u7387\u662f\u7a33\u5b9a\u63a5\u5165\u7535\u7f51\u7684\u5173\u952e\u9700\u6c42\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u9884\u6d4b\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u3002", "method": "\u91c7\u7528\u5e7f\u4e49Logit\u53d8\u6362\u5c06\u98ce\u7535\u529f\u7387\u6570\u636e\u8f6c\u6362\u5230\u65e0\u754c\u57df\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u65b9\u6cd5\u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u5f62\u72b6\u53c2\u6570\u66f4\u65b0\u673a\u5236\u3002\u7814\u7a76\u4e86\u56db\u79cd\u81ea\u9002\u5e94\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5927\u91cf\u98ce\u7535\u573a\u6570\u636e\u9a8c\u8bc1\u3002", "result": "\u63d0\u51fa\u7684\u8d1d\u53f6\u65af\u65b9\u6cd5\u5728\u8fde\u7eed\u6392\u540d\u6982\u7387\u8bc4\u5206\uff08CRPS\uff09\u548c\u53ef\u9760\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u9884\u6d4b\u51c6\u786e\u6027\u548c\u7a33\u5065\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u98ce\u7535\u9884\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u4e3a\u7535\u7f51\u96c6\u6210\u548c\u51b3\u7b56\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u9884\u6d4b\u5de5\u5177\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.06375", "pdf": "https://arxiv.org/pdf/2505.06375", "abs": "https://arxiv.org/abs/2505.06375", "authors": ["Nahshon Mokua Obiri", "Kristof Van Laerhoven"], "title": "A Comprehensive Data Description for LoRaWAN Path Loss Measurements in an Indoor Office Setting: Effects of Environmental Factors", "categories": ["cs.NI", "cs.AR", "cs.LG", "eess.SP"], "comment": "This is a peer-reviewed article with the help of IEEE Access editors.\n  The relevant DOI will be availed soon", "summary": "This paper presents a comprehensive dataset of LoRaWAN technology path loss\nmeasurements collected in an indoor office environment, focusing on quantifying\nthe effects of environmental factors on signal propagation. Utilizing a network\nof six strategically placed LoRaWAN end devices (EDs) and a single indoor\ngateway (GW) at the University of Siegen, City of Siegen, Germany, we\nsystematically measured signal strength indicators such as the Received Signal\nStrength Indicator (RSSI) and the Signal-to-Noise Ratio (SNR) under various\nenvironmental conditions, including temperature, relative humidity, carbon\ndioxide (CO$_2$) concentration, barometric pressure, and particulate matter\nlevels (PM$_{2.5}$). Our empirical analysis confirms that transient phenomena\nsuch as reflections, scattering, interference, occupancy patterns (induced by\nenvironmental parameter variations), and furniture rearrangements can alter\nsignal attenuation by as much as 10.58 dB, highlighting the dynamic nature of\nindoor propagation. As an example of how this dataset can be utilized, we\ntested and evaluated a refined Log-Distance Path Loss and Shadowing Model that\nintegrates both structural obstructions (Multiple Walls) and Environmental\nParameters (LDPLSM-MW-EP). Compared to a baseline model that considers only\nMultiple Walls (LDPLSM-MW), the enhanced approach reduced the root mean square\nerror (RMSE) from 10.58 dB to 8.04 dB and increased the coefficient of\ndetermination (R$^2$) from 0.6917 to 0.8222. By capturing the extra effects of\nenvironmental conditions and occupancy dynamics, this improved model provides\nvaluable insights for optimizing power usage and prolonging device battery\nlife, enhancing network reliability in indoor Internet of Things (IoT)\ndeployments, among other applications. This dataset offers a solid foundation\nfor future research and development in indoor wireless communication.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5ba4\u5185LoRaWAN\u4fe1\u53f7\u4f20\u64ad\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u91cf\u5316\u4e86\u73af\u5883\u56e0\u7d20\u5bf9\u4fe1\u53f7\u7684\u5f71\u54cd\uff0c\u5e76\u6539\u8fdb\u4e86\u8def\u5f84\u635f\u8017\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u91cf\u5316\u5ba4\u5185\u73af\u5883\u4e2d\u73af\u5883\u56e0\u7d20\u5bf9LoRaWAN\u4fe1\u53f7\u4f20\u64ad\u7684\u5f71\u54cd\uff0c\u4ee5\u4f18\u5316\u7269\u8054\u7f51\u8bbe\u5907\u7684\u90e8\u7f72\u548c\u80fd\u6548\u3002", "method": "\u901a\u8fc7\u5728\u5ba4\u5185\u5e03\u8bbe6\u4e2a\u7ec8\u7aef\u8bbe\u5907\u548c1\u4e2a\u7f51\u5173\uff0c\u7cfb\u7edf\u6d4b\u91cf\u4fe1\u53f7\u5f3a\u5ea6\u6307\u6807\uff08\u5982RSSI\u548cSNR\uff09\uff0c\u5e76\u7ed3\u5408\u73af\u5883\u53c2\u6570\uff08\u6e29\u5ea6\u3001\u6e7f\u5ea6\u7b49\uff09\u5206\u6790\u4fe1\u53f7\u8870\u51cf\u3002\u63d0\u51fa\u6539\u8fdb\u7684\u8def\u5f84\u635f\u8017\u6a21\u578b\uff08LDPLSM-MW-EP\uff09\u3002", "result": "\u6539\u8fdb\u6a21\u578b\u5c06RMSE\u4ece10.58 dB\u964d\u81f38.04 dB\uff0cR\u00b2\u4ece0.6917\u63d0\u5347\u81f30.8222\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4fe1\u53f7\u8870\u51cf\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u548c\u6539\u8fdb\u6a21\u578b\u4e3a\u5ba4\u5185\u65e0\u7ebf\u901a\u4fe1\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u7269\u8054\u7f51\u90e8\u7f72\u548c\u5ef6\u957f\u8bbe\u5907\u5bff\u547d\u3002"}}
{"id": "2505.06386", "pdf": "https://arxiv.org/pdf/2505.06386", "abs": "https://arxiv.org/abs/2505.06386", "authors": ["Donghao Ren", "Fred Hohman", "Halden Lin", "Dominik Moritz"], "title": "Embedding Atlas: Low-Friction, Interactive Embedding Visualization", "categories": ["cs.HC", "cs.LG"], "comment": "Website: https://apple.github.io/embedding-atlas/", "summary": "Embedding projections are popular for visualizing large datasets and models.\nHowever, people often encounter \"friction\" when using embedding visualization\ntools: (1) barriers to adoption, e.g., tedious data wrangling and loading,\nscalability limits, no integration of results into existing workflows, and (2)\nlimitations in possible analyses, without integration with external tools to\nadditionally show coordinated views of metadata. In this paper, we present\nEmbedding Atlas, a scalable, interactive visualization tool designed to make\ninteracting with large embeddings as easy as possible. Embedding Atlas uses\nmodern web technologies and advanced algorithms -- including density-based\nclustering, and automated labeling -- to provide a fast and rich data analysis\nexperience at scale. We evaluate Embedding Atlas with a competitive analysis\nagainst other popular embedding tools, showing that Embedding Atlas's feature\nset specifically helps reduce friction, and report a benchmark on its real-time\nrendering performance with millions of points. Embedding Atlas is available as\nopen source to support future work in embedding-based analysis.", "AI": {"tldr": "Embedding Atlas\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u65e8\u5728\u964d\u4f4e\u4f7f\u7528\u5927\u89c4\u6a21\u5d4c\u5165\u6570\u636e\u7684\u590d\u6742\u5ea6\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u5b58\u5728\u91c7\u7528\u969c\u788d\u548c\u5206\u6790\u9650\u5236\uff0cEmbedding Atlas\u65e8\u5728\u6d88\u9664\u8fd9\u4e9b\u6469\u64e6\u3002", "method": "\u7ed3\u5408\u73b0\u4ee3\u7f51\u7edc\u6280\u672f\u548c\u9ad8\u7ea7\u7b97\u6cd5\uff08\u5982\u57fa\u4e8e\u5bc6\u5ea6\u7684\u805a\u7c7b\u548c\u81ea\u52a8\u6807\u8bb0\uff09\u4ee5\u63d0\u5347\u4ea4\u4e92\u4f53\u9a8c\u3002", "result": "\u5bf9\u6bd4\u8bc4\u4f30\u663e\u793aEmbedding Atlas\u80fd\u6709\u6548\u51cf\u5c11\u6469\u64e6\uff0c\u4e14\u652f\u6301\u767e\u4e07\u7ea7\u6570\u636e\u7684\u5b9e\u65f6\u6e32\u67d3\u3002", "conclusion": "Embedding Atlas\u5f00\u6e90\u53d1\u5e03\uff0c\u4e3a\u672a\u6765\u5d4c\u5165\u6570\u636e\u5206\u6790\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2505.07041", "pdf": "https://arxiv.org/pdf/2505.07041", "abs": "https://arxiv.org/abs/2505.07041", "authors": ["Samaneh Mohammadi", "Iraklis Symeonidis", "Ali Balador", "Francesco Flammini"], "title": "Empirical Analysis of Asynchronous Federated Learning on Heterogeneous Devices: Efficiency, Fairness, and Privacy Trade-offs", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": "This paper was accepted to IJCNN 2025. This version is a preprint and\n  not the official published version", "summary": "Device heterogeneity poses major challenges in Federated Learning (FL), where\nresource-constrained clients slow down synchronous schemes that wait for all\nupdates before aggregation. Asynchronous FL addresses this by incorporating\nupdates as they arrive, substantially improving efficiency. While its\nefficiency gains are well recognized, its privacy costs remain largely\nunexplored, particularly for high-end devices that contribute updates more\nfrequently, increasing their cumulative privacy exposure. This paper presents\nthe first comprehensive analysis of the efficiency-fairness-privacy trade-off\nin synchronous vs. asynchronous FL under realistic device heterogeneity. We\nempirically compare FedAvg and staleness-aware FedAsync using a physical\ntestbed of five edge devices spanning diverse hardware tiers, integrating Local\nDifferential Privacy (LDP) and the Moments Accountant to quantify per-client\nprivacy loss. Using Speech Emotion Recognition (SER) as a privacy-critical\nbenchmark, we show that FedAsync achieves up to 10x faster convergence but\nexacerbates fairness and privacy disparities: high-end devices contribute 6-10x\nmore updates and incur up to 5x higher privacy loss, while low-end devices\nsuffer amplified accuracy degradation due to infrequent, stale, and\nnoise-perturbed updates. These findings motivate the need for adaptive FL\nprotocols that jointly optimize aggregation and privacy mechanisms based on\nclient capacity and participation dynamics, moving beyond static,\none-size-fits-all solutions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u8bbe\u5907\u5f02\u6784\u6027\u5bf9\u6548\u7387\u3001\u516c\u5e73\u6027\u548c\u9690\u79c1\u7684\u5f71\u54cd\uff0c\u6bd4\u8f83\u4e86\u540c\u6b65\u548c\u5f02\u6b65\u65b9\u6cd5\u7684\u4f18\u52a3\uff0c\u5f3a\u8c03\u4e86\u81ea\u9002\u5e94\u534f\u8bae\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u8bbe\u5907\u5f02\u6784\u6027\u5bfc\u81f4\u8054\u90a6\u5b66\u4e60\u4e2d\u8d44\u6e90\u53d7\u9650\u7684\u5ba2\u6237\u7aef\u62d6\u6162\u540c\u6b65\u65b9\u6848\uff0c\u800c\u5f02\u6b65\u65b9\u6cd5\u867d\u7136\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u4f46\u5176\u9690\u79c1\u4ee3\u4ef7\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u5c24\u5176\u662f\u9ad8\u7aef\u8bbe\u5907\u7684\u9690\u79c1\u635f\u5931\u66f4\u5927\u3002", "method": "\u901a\u8fc7\u7269\u7406\u6d4b\u8bd5\u5e8a\uff08\u4e94\u53f0\u8fb9\u7f18\u8bbe\u5907\uff09\u6bd4\u8f83FedAvg\u548cFedAsync\u65b9\u6cd5\uff0c\u7ed3\u5408\u672c\u5730\u5dee\u5206\u9690\u79c1\u548c\u65f6\u523b\u7edf\u8ba1\u91cf\u5316\u9690\u79c1\u635f\u5931\u3002", "result": "\u5f02\u6b65\u65b9\u6cd5\u6536\u655b\u901f\u5ea6\u63d0\u9ad810\u500d\uff0c\u4f46\u9ad8\u7aef\u8bbe\u5907\u7684\u9690\u79c1\u635f\u5931\u589e\u52a05\u500d\uff0c\u4f4e\u7aef\u8bbe\u5907\u56e0\u66f4\u65b0\u9891\u7387\u4f4e\u548c\u566a\u58f0\u5e72\u6270\u5bfc\u81f4\u51c6\u786e\u7387\u4e0b\u964d\u3002", "conclusion": "\u7814\u7a76\u547c\u5401\u5f00\u53d1\u81ea\u9002\u5e94\u8054\u90a6\u5b66\u4e60\u534f\u8bae\uff0c\u4ee5\u6839\u636e\u5ba2\u6237\u7aef\u80fd\u529b\u548c\u53c2\u4e0e\u52a8\u6001\u4f18\u5316\u805a\u5408\u548c\u9690\u79c1\u673a\u5236\uff0c\u53d6\u4ee3\u9759\u6001\u7edf\u4e00\u65b9\u6848\u3002"}}
{"id": "2505.06407", "pdf": "https://arxiv.org/pdf/2505.06407", "abs": "https://arxiv.org/abs/2505.06407", "authors": ["Ramin Esmzad", "Gokul S. Sankar", "Teawon Han", "Hamidreza Modares"], "title": "Direct Data Driven Control Using Noisy Measurements", "categories": ["eess.SY", "cs.LG", "cs.RO", "cs.SY", "math.OC"], "comment": "Submitted to IEEE-TAC", "summary": "This paper presents a novel direct data-driven control framework for solving\nthe linear quadratic regulator (LQR) under disturbances and noisy state\nmeasurements. The system dynamics are assumed unknown, and the LQR solution is\nlearned using only a single trajectory of noisy input-output data while\nbypassing system identification. Our approach guarantees mean-square stability\n(MSS) and optimal performance by leveraging convex optimization techniques that\nincorporate noise statistics directly into the controller synthesis. First, we\nestablish a theoretical result showing that the MSS of an uncertain data-driven\nsystem implies the MSS of the true closed-loop system. Building on this, we\ndevelop a robust stability condition using linear matrix inequalities (LMIs)\nthat yields a stabilizing controller gain from noisy measurements. Finally, we\nformulate a data-driven LQR problem as a semidefinite program (SDP) that\ncomputes an optimal gain, minimizing the steady-state covariance. Extensive\nsimulations on benchmark systems -- including a rotary inverted pendulum and an\nactive suspension system -- demonstrate the superior robustness and accuracy of\nour method compared to existing data-driven LQR approaches. The proposed\nframework offers a practical and theoretically grounded solution for controller\ndesign in noise-corrupted environments where system identification is\ninfeasible.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u76f4\u63a5\u6570\u636e\u9a71\u52a8\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6270\u52a8\u548c\u566a\u58f0\u72b6\u6001\u4e0b\u89e3\u51b3\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\u5668\uff08LQR\uff09\u95ee\u9898\uff0c\u65e0\u9700\u7cfb\u7edf\u8fa8\u8bc6\u3002\u901a\u8fc7\u51f8\u4f18\u5316\u6280\u672f\uff0c\u7ed3\u5408\u566a\u58f0\u7edf\u8ba1\u76f4\u63a5\u5408\u6210\u63a7\u5236\u5668\uff0c\u4fdd\u8bc1\u4e86\u5747\u65b9\u7a33\u5b9a\u6027\u548c\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u5728\u7cfb\u7edf\u52a8\u6001\u672a\u77e5\u4e14\u5b58\u5728\u566a\u58f0\u6d4b\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u7cfb\u7edf\u8fa8\u8bc6\uff0c\u4f46\u590d\u6742\u4e14\u4e0d\u5b9e\u7528\u3002\u672c\u6587\u76ee\u6807\u662f\u7ed5\u8fc7\u7cfb\u7edf\u8fa8\u8bc6\uff0c\u76f4\u63a5\u4ece\u566a\u58f0\u8f93\u5165\u8f93\u51fa\u6570\u636e\u4e2d\u5b66\u4e60LQR\u89e3\uff0c\u63d0\u4f9b\u66f4\u9c81\u68d2\u548c\u5b9e\u7528\u7684\u63a7\u5236\u5668\u8bbe\u8ba1\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u566a\u58f0\u7edf\u8ba1\u4fe1\u606f\uff0c\u901a\u8fc7\u7ebf\u6027\u77e9\u9635\u4e0d\u7b49\u5f0f\uff08LMI\uff09\u5efa\u7acb\u9c81\u68d2\u7a33\u5b9a\u6027\u6761\u4ef6\uff0c\u5e76\u4f7f\u7528\u534a\u5b9a\u89c4\u5212\uff08SDP\uff09\u516c\u5f0f\u5316\u6570\u636e\u9a71\u52a8LQR\u95ee\u9898\uff0c\u8ba1\u7b97\u6700\u4f18\u63a7\u5236\u589e\u76ca\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\uff08\u5982\u65cb\u8f6c\u5012\u7acb\u6446\u548c\u4e3b\u52a8\u60ac\u67b6\u7cfb\u7edf\uff09\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u9a71\u52a8LQR\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u566a\u58f0\u6c61\u67d3\u73af\u5883\u4e2d\u65e0\u6cd5\u8fdb\u884c\u7cfb\u7edf\u8fa8\u8bc6\u7684\u63a7\u5236\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.07062", "pdf": "https://arxiv.org/pdf/2505.07062", "abs": "https://arxiv.org/abs/2505.07062", "authors": ["Dong Guo", "Faming Wu", "Feida Zhu", "Fuxing Leng", "Guang Shi", "Haobin Chen", "Haoqi Fan", "Jian Wang", "Jianyu Jiang", "Jiawei Wang", "Jingji Chen", "Jingjia Huang", "Kang Lei", "Liping Yuan", "Lishu Luo", "Pengfei Liu", "Qinghao Ye", "Rui Qian", "Shen Yan", "Shixiong Zhao", "Shuai Peng", "Shuangye Li", "Sihang Yuan", "Sijin Wu", "Tianheng Cheng", "Weiwei Liu", "Wenqian Wang", "Xianhan Zeng", "Xiao Liu", "Xiaobo Qin", "Xiaohan Ding", "Xiaojun Xiao", "Xiaoying Zhang", "Xuanwei Zhang", "Xuehan Xiong", "Yanghua Peng", "Yangrui Chen", "Yanwei Li", "Yanxu Hu", "Yi Lin", "Yiyuan Hu", "Yiyuan Zhang", "Youbin Wu", "Yu Li", "Yudong Liu", "Yue Ling", "Yujia Qin", "Zanbo Wang", "Zhiwu He", "Aoxue Zhang", "Bairen Yi", "Bencheng Liao", "Can Huang", "Can Zhang", "Chaorui Deng", "Chaoyi Deng", "Cheng Lin", "Cheng Yuan", "Chenggang Li", "Chenhui Gou", "Chenwei Lou", "Chengzhi Wei", "Chundian Liu", "Chunyuan Li", "Deyao Zhu", "Donghong Zhong", "Feng Li", "Feng Zhang", "Gang Wu", "Guodong Li", "Guohong Xiao", "Haibin Lin", "Haihua Yang", "Haoming Wang", "Heng Ji", "Hongxiang Hao", "Hui Shen", "Huixia Li", "Jiahao Li", "Jialong Wu", "Jianhua Zhu", "Jianpeng Jiao", "Jiashi Feng", "Jiaze Chen", "Jianhui Duan", "Jihao Liu", "Jin Zeng", "Jingqun Tang", "Jingyu Sun", "Joya Chen", "Jun Long", "Junda Feng", "Junfeng Zhan", "Junjie Fang", "Junting Lu", "Kai Hua", "Kai Liu", "Kai Shen", "Kaiyuan Zhang", "Ke Shen", "Ke Wang", "Keyu Pan", "Kun Zhang", "Kunchang Li", "Lanxin Li", "Lei Li", "Lei Shi", "Li Han", "Liang Xiang", "Liangqiang Chen", "Lin Chen", "Lin Li", "Lin Yan", "Liying Chi", "Longxiang Liu", "Mengfei Du", "Mingxuan Wang", "Ningxin Pan", "Peibin Chen", "Pengfei Chen", "Pengfei Wu", "Qingqing Yuan", "Qingyao Shuai", "Qiuyan Tao", "Renjie Zheng", "Renrui Zhang", "Ru Zhang", "Rui Wang", "Rui Yang", "Rui Zhao", "Shaoqiang Xu", "Shihao Liang", "Shipeng Yan", "Shu Zhong", "Shuaishuai Cao", "Shuangzhi Wu", "Shufan Liu", "Shuhan Chang", "Songhua Cai", "Tenglong Ao", "Tianhao Yang", "Tingting Zhang", "Wanjun Zhong", "Wei Jia", "Wei Weng", "Weihao Yu", "Wenhao Huang", "Wenjia Zhu", "Wenli Yang", "Wenzhi Wang", "Xiang Long", "XiangRui Yin", "Xiao Li", "Xiaolei Zhu", "Xiaoying Jia", "Xijin Zhang", "Xin Liu", "Xinchen Zhang", "Xinyu Yang", "Xiongcai Luo", "Xiuli Chen", "Xuantong Zhong", "Xuefeng Xiao", "Xujing Li", "Yan Wu", "Yawei Wen", "Yifan Du", "Yihao Zhang", "Yining Ye", "Yonghui Wu", "Yu Liu", "Yu Yue", "Yufeng Zhou", "Yufeng Yuan", "Yuhang Xu", "Yuhong Yang", "Yun Zhang", "Yunhao Fang", "Yuntao Li", "Yurui Ren", "Yuwen Xiong", "Zehua Hong", "Zehua Wang", "Zewei Sun", "Zeyu Wang", "Zhao Cai", "Zhaoyue Zha", "Zhecheng An", "Zhehui Zhao", "Zhengzhuo Xu", "Zhipeng Chen", "Zhiyong Wu", "Zhuofan Zheng", "Zihao Wang", "Zilong Huang", "Ziyu Zhu", "Zuquan Song"], "title": "Seed1.5-VL Technical Report", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present Seed1.5-VL, a vision-language foundation model designed to advance\ngeneral-purpose multimodal understanding and reasoning. Seed1.5-VL is composed\nwith a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B\nactive parameters. Despite its relatively compact architecture, it delivers\nstrong performance across a wide spectrum of public VLM benchmarks and internal\nevaluation suites, achieving the state-of-the-art performance on 38 out of 60\npublic benchmarks. Moreover, in agent-centric tasks such as GUI control and\ngameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI\nCUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates\nstrong reasoning abilities, making it particularly effective for multimodal\nreasoning challenges such as visual puzzles. We believe these capabilities will\nempower broader applications across diverse tasks. In this report, we mainly\nprovide a comprehensive review of our experiences in building Seed1.5-VL across\nmodel design, data construction, and training at various stages, hoping that\nthis report can inspire further research. Seed1.5-VL is now accessible at\nhttps://www.volcengine.com/ (Volcano Engine Model ID:\ndoubao-1-5-thinking-vision-pro-250428)", "AI": {"tldr": "Seed1.5-VL\u662f\u4e00\u79cd\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u7d27\u51d1\u7684\u67b6\u6784\uff08532M\u89c6\u89c9\u7f16\u7801\u5668\u548c20B\u53c2\u6570\u7684MoE LLM\uff09\uff0c\u572860\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d38\u9879\u8fbe\u5230\u6700\u9ad8\u6c34\u5e73\uff0c\u5c24\u5176\u5728\u4ee3\u7406\u4efb\u52a1\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u65e8\u5728\u63a8\u52a8\u591a\u6a21\u6001\u7406\u89e3\u548c\u63a8\u7406\uff0c\u63d0\u5347\u89c6\u89c9\u3001\u89c6\u9891\u53ca\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff08MoE LLM\uff09\u548c\u4f18\u5316\u7684\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u5927\u89c4\u6a21\u6570\u636e\u8bad\u7ec3\u548c\u9636\u6bb5\u6027\u8c03\u6574\u3002", "result": "38/60\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u9886\u5148\uff0c\u4ee3\u7406\u4efb\u52a1\u8868\u73b0\u8d85OpenAI CUA\u548cClaude 3.7\uff0c\u63a8\u7406\u4efb\u52a1\u80fd\u529b\u7a81\u51fa\u3002", "conclusion": "Seed1.5-VL\u5c55\u793a\u4e86\u9ad8\u6548\u7684\u591a\u6a21\u6001\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u3001\u6570\u636e\u548c\u8bad\u7ec3\u7684\u7ecf\u9a8c\u53c2\u8003\u3002"}}
{"id": "2505.07064", "pdf": "https://arxiv.org/pdf/2505.07064", "abs": "https://arxiv.org/abs/2505.07064", "authors": ["Shusen Liu", "Haichao Miao", "Peer-Timo Bremer"], "title": "ParaView-MCP: An Autonomous Visualization Agent with Direct Tool Use", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "While powerful and well-established, tools like ParaView present a steep\nlearning curve that discourages many potential users. This work introduces\nParaView-MCP, an autonomous agent that integrates modern multimodal large\nlanguage models (MLLMs) with ParaView to not only lower the barrier to entry\nbut also augment ParaView with intelligent decision support. By leveraging the\nstate-of-the-art reasoning, command execution, and vision capabilities of\nMLLMs, ParaView-MCP enables users to interact with ParaView through natural\nlanguage and visual inputs. Specifically, our system adopted the Model Context\nProtocol (MCP) - a standardized interface for model-application communication -\nthat facilitates direct interaction between MLLMs with ParaView's Python API to\nallow seamless information exchange between the user, the language model, and\nthe visualization tool itself. Furthermore, by implementing a visual feedback\nmechanism that allows the agent to observe the viewport, we unlock a range of\nnew capabilities, including recreating visualizations from examples,\nclosed-loop visualization parameter updates based on user-defined goals, and\neven cross-application collaboration involving multiple tools. Broadly, we\nbelieve such an agent-driven visualization paradigm can profoundly change the\nway we interact with visualization tools. We expect a significant uptake in the\ndevelopment of such visualization tools, in both visualization research and\nindustry.", "AI": {"tldr": "ParaView-MCP \u662f\u4e00\u79cd\u81ea\u4e3b\u4ee3\u7406\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u6765\u964d\u4f4e ParaView \u7684\u5b66\u4e60\u95e8\u69db\uff0c\u5e76\u63d0\u4f9b\u667a\u80fd\u51b3\u7b56\u652f\u6301\u3002", "motivation": "ParaView \u7b49\u5de5\u5177\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\uff0c\u963b\u788d\u4e86\u8bb8\u591a\u6f5c\u5728\u7528\u6237\u7684\u4f7f\u7528\uff0cParaView-MCP \u65e8\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u548c\u89c6\u89c9\u8f93\u5165\u7b80\u5316\u4ea4\u4e92\u3002", "method": "\u91c7\u7528\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u4f5c\u4e3a\u6807\u51c6\u63a5\u53e3\uff0c\u7ed3\u5408 MLLMs \u548c ParaView \u7684 Python API\uff0c\u5b9e\u73b0\u7528\u6237\u3001\u8bed\u8a00\u6a21\u578b\u4e0e\u53ef\u89c6\u5316\u5de5\u5177\u95f4\u7684\u65e0\u7f1d\u4ea4\u4e92\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e86\u57fa\u4e8e\u89c6\u89c9\u53cd\u9988\u7684\u65b0\u529f\u80fd\uff0c\u5982\u793a\u4f8b\u53ef\u89c6\u5316\u91cd\u5efa\u3001\u95ed\u73af\u53c2\u6570\u66f4\u65b0\u53ca\u8de8\u5de5\u5177\u534f\u4f5c\u3002", "conclusion": "\u4ee3\u7406\u9a71\u52a8\u7684\u53ef\u89c6\u5316\u8303\u5f0f\u6709\u671b\u6539\u53d8\u7528\u6237\u4e0e\u53ef\u89c6\u5316\u5de5\u5177\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u63a8\u52a8\u7814\u7a76\u548c\u5de5\u4e1a\u754c\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.06435", "pdf": "https://arxiv.org/pdf/2505.06435", "abs": "https://arxiv.org/abs/2505.06435", "authors": ["Insung Kong", "Kunwoong Kim", "Yongdai Kim"], "title": "Fair Representation Learning for Continuous Sensitive Attributes using Expectation of Integral Probability Metrics", "categories": ["stat.ML", "cs.LG"], "comment": "42 pages, 30 figures. IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (2025)", "summary": "AI fairness, also known as algorithmic fairness, aims to ensure that\nalgorithms operate without bias or discrimination towards any individual or\ngroup. Among various AI algorithms, the Fair Representation Learning (FRL)\napproach has gained significant interest in recent years. However, existing FRL\nalgorithms have a limitation: they are primarily designed for categorical\nsensitive attributes and thus cannot be applied to continuous sensitive\nattributes, such as age or income. In this paper, we propose an FRL algorithm\nfor continuous sensitive attributes. First, we introduce a measure called the\nExpectation of Integral Probability Metrics (EIPM) to assess the fairness level\nof representation space for continuous sensitive attributes. We demonstrate\nthat if the distribution of the representation has a low EIPM value, then any\nprediction head constructed on the top of the representation become fair,\nregardless of the selection of the prediction head. Furthermore, EIPM possesses\na distinguished advantage in that it can be accurately estimated using our\nproposed estimator with finite samples. Based on these properties, we propose a\nnew FRL algorithm called Fair Representation using EIPM with MMD (FREM).\nExperimental evidences show that FREM outperforms other baseline methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8fde\u7eed\u654f\u611f\u5c5e\u6027\u7684\u516c\u5e73\u8868\u793a\u5b66\u4e60\u7b97\u6cd5FREM\uff0c\u901a\u8fc7EIPM\u5ea6\u91cf\u516c\u5e73\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u516c\u5e73\u8868\u793a\u5b66\u4e60\u7b97\u6cd5\u4e3b\u8981\u9488\u5bf9\u5206\u7c7b\u654f\u611f\u5c5e\u6027\uff0c\u65e0\u6cd5\u5904\u7406\u8fde\u7eed\u654f\u611f\u5c5e\u6027\uff08\u5982\u5e74\u9f84\u6216\u6536\u5165\uff09\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002", "method": "\u5f15\u5165EIPM\u5ea6\u91cf\u8868\u793a\u7a7a\u95f4\u7684\u516c\u5e73\u6027\uff0c\u8bc1\u660e\u4f4eEIPM\u503c\u53ef\u4fdd\u8bc1\u9884\u6d4b\u5934\u7684\u516c\u5e73\u6027\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8eEIPM\u548cMMD\u7684FREM\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFREM\u5728\u8fde\u7eed\u654f\u611f\u5c5e\u6027\u4e0a\u7684\u516c\u5e73\u6027\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FREM\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8fde\u7eed\u654f\u611f\u5c5e\u6027\u7684\u516c\u5e73\u8868\u793a\u5b66\u4e60\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.07078", "pdf": "https://arxiv.org/pdf/2505.07078", "abs": "https://arxiv.org/abs/2505.07078", "authors": ["Weixian Waylon Li", "Hyeonjun Kim", "Mihai Cucuringu", "Tiejun Ma"], "title": "Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?", "categories": ["q-fin.TR", "cs.AI", "cs.CE"], "comment": "14 pages", "summary": "Large Language Models (LLMs) have recently been leveraged for asset pricing\ntasks and stock trading applications, enabling AI agents to generate investment\ndecisions from unstructured financial data. However, most evaluations of LLM\ntiming-based investing strategies are conducted on narrow timeframes and\nlimited stock universes, overstating effectiveness due to survivorship and\ndata-snooping biases. We critically assess their generalizability and\nrobustness by proposing FINSABER, a backtesting framework evaluating\ntiming-based strategies across longer periods and a larger universe of symbols.\nSystematic backtests over two decades and 100+ symbols reveal that previously\nreported LLM advantages deteriorate significantly under broader cross-section\nand over a longer-term evaluation. Our market regime analysis further\ndemonstrates that LLM strategies are overly conservative in bull markets,\nunderperforming passive benchmarks, and overly aggressive in bear markets,\nincurring heavy losses. These findings highlight the need to develop LLM\nstrategies that are able to prioritise trend detection and regime-aware risk\ncontrols over mere scaling of framework complexity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faFINSABER\u6846\u67b6\uff0c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8d44\u4ea7\u5b9a\u4ef7\u548c\u80a1\u7968\u4ea4\u6613\u4e2d\u7684\u957f\u671f\u666e\u9002\u6027\u548c\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u5176\u7b56\u7565\u5728\u66f4\u957f\u65f6\u95f4\u548c\u66f4\u5927\u80a1\u7968\u8303\u56f4\u5185\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u9700\u6539\u8fdb\u8d8b\u52bf\u68c0\u6d4b\u548c\u98ce\u9669\u63a7\u5236\u3002", "motivation": "\u73b0\u6709LLM\u5728\u91d1\u878d\u4efb\u52a1\u4e2d\u7684\u8bc4\u4f30\u96c6\u4e2d\u5728\u77ed\u65f6\u95f4\u548c\u6709\u9650\u80a1\u7968\u8303\u56f4\uff0c\u53ef\u80fd\u9ad8\u4f30\u4e86\u5176\u6548\u679c\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u9a8c\u8bc1\u5176\u666e\u9002\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faFINSABER\u6846\u67b6\uff0c\u901a\u8fc7\u957f\u8fbe20\u5e74\u548c100+\u80a1\u7968\u7684\u7cfb\u7edf\u6027\u56de\u6d4b\uff0c\u5206\u6790LLM\u7b56\u7565\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0LLM\u7b56\u7565\u5728\u66f4\u5927\u8303\u56f4\u548c\u66f4\u957f\u65f6\u95f4\u5185\u4f18\u52bf\u660e\u663e\u4e0b\u964d\uff0c\u4e14\u5728\u725b\u5e02\u4fdd\u5b88\u3001\u718a\u5e02\u6fc0\u8fdb\uff0c\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "LLM\u7b56\u7565\u9700\u6539\u8fdb\u8d8b\u52bf\u68c0\u6d4b\u548c\u98ce\u9669\u63a7\u5236\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u589e\u52a0\u6846\u67b6\u590d\u6742\u5ea6\u3002"}}
{"id": "2505.06461", "pdf": "https://arxiv.org/pdf/2505.06461", "abs": "https://arxiv.org/abs/2505.06461", "authors": ["Haolin Zhang", "Jeff Huang"], "title": "Challenging GPU Dominance: When CPUs Outperform for On-Device LLM Inference", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "The common assumption in on-device AI is that GPUs, with their superior\nparallel processing, always provide the best performance for large language\nmodel (LLM) inference. In this work, we challenge this notion by empirically\ndemonstrating that, under certain conditions, CPUs can outperform GPUs for LLM\ninference on mobile devices. Using a 1-billion-parameter LLM deployed via\nllama.cpp on the iPhone 15 Pro, we show that a CPU-only configuration (two\nthreads, F16 precision) achieves 17 tokens per second, surpassing the 12.8\ntokens per second obtained with GPU acceleration. We analyze the architectural\nfactors driving this counterintuitive result, revealing that GPU memory\ntransfer overhead and CPU thread optimization play a critical role.\nFurthermore, we explore the impact of thread oversubscription, quantization\nstrategies, and hardware constraints, providing new insights into efficient\non-device AI execution. Our findings challenge conventional GPU-first thinking,\nhighlighting the untapped potential of optimized CPU inference and paving the\nway for smarter deployment strategies in mobile AI. However, fully explaining\nthe observed CPU advantage remains difficult due to limited access to low-level\nprofiling tools on iOS.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u5982iPhone 15 Pro\u4e0a\u90e8\u7f721-billion-parameter LLM\u65f6\uff0cCPU\u63a8\u7406\uff08\u53cc\u7ebf\u7a0b\uff0cF16\u7cbe\u5ea6\uff09\u7684\u6027\u80fd\uff0817 tokens/s\uff09\u4f18\u4e8eGPU\u52a0\u901f\uff0812.8 tokens/s\uff09\u3002\u5185\u5b58\u4f20\u8f93\u5f00\u9500\u548c\u7ebf\u7a0b\u4f18\u5316\u662f\u5173\u952e\u56e0\u7d20\uff0c\u6311\u6218\u4e86GPU\u4f18\u5148\u7684\u5e38\u89c4\u601d\u7ef4\u3002", "motivation": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u6311\u6218\u4e86GPU\u5728\u79fb\u52a8\u8bbe\u5907LLM\u63a8\u7406\u4e2d\u6027\u80fd\u6700\u4f18\u7684\u666e\u904d\u5047\u8bbe\u3002", "method": "\u4f7f\u7528llama.cpp\u5728iPhone 15 Pro\u4e0a\u90e8\u7f721-billion-parameter LLM\uff0c\u5bf9\u6bd4CPU\uff08\u53cc\u7ebf\u7a0b\uff0cF16\uff09\u548cGPU\u7684\u6027\u80fd\u3002", "result": "CPU\u914d\u7f6e\uff0817 tokens/s, two threads, F16\uff09\u4f18\u4e8eGPU\u52a0\u901f\uff0812.8 tokens/s\uff09\u3002\u5185\u5b58\u4f20\u8f93\u5f00\u9500\u548cCPU\u7ebf\u7a0b\u4f18\u5316\u662f\u4e3b\u8981\u539f\u56e0\u3002", "conclusion": "\u4f18\u5316CPU\u63a8\u7406\u5177\u5907\u6f5c\u529b\uff0c\u9700\u91cd\u65b0\u601d\u8003\u79fb\u52a8AI\u90e8\u7f72\u7b56\u7565\u3002\u4f46\u56e0iOS\u5e95\u5c42\u5de5\u5177\u9650\u5236\uff0cCPU\u4f18\u52bf\u7684\u5b8c\u6574\u89e3\u91ca\u4ecd\u6709\u96be\u5ea6\u3002"}}
{"id": "2505.07096", "pdf": "https://arxiv.org/pdf/2505.07096", "abs": "https://arxiv.org/abs/2505.07096", "authors": ["Prithwish Dan", "Kushal Kedia", "Angela Chao", "Edward Weiyi Duan", "Maximus Adrian Pace", "Wei-Chiu Ma", "Sanjiban Choudhury"], "title": "X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Human videos offer a scalable way to train robot manipulation policies, but\nlack the action labels needed by standard imitation learning algorithms.\nExisting cross-embodiment approaches try to map human motion to robot actions,\nbut often fail when the embodiments differ significantly. We propose X-Sim, a\nreal-to-sim-to-real framework that uses object motion as a dense and\ntransferable signal for learning robot policies. X-Sim starts by reconstructing\na photorealistic simulation from an RGBD human video and tracking object\ntrajectories to define object-centric rewards. These rewards are used to train\na reinforcement learning (RL) policy in simulation. The learned policy is then\ndistilled into an image-conditioned diffusion policy using synthetic rollouts\nrendered with varied viewpoints and lighting. To transfer to the real world,\nX-Si introduces an online domain adaptation technique that aligns real and\nsimulated observations during deployment. Importantly, X-Sim does not require\nany robot teleoperation data. We evaluate it across 5 manipulation tasks in 2\nenvironments and show that it: (1) improves task progress by 30% on average\nover hand-tracking and sim-to-real baselines, (2) matches behavior cloning with\n10x less data collection time, and (3) generalizes to new camera viewpoints and\ntest-time changes. Code and videos are available at\nhttps://portal-cornell.github.io/X-Sim/.", "AI": {"tldr": "X-Sim\u6846\u67b6\u901a\u8fc7\u5229\u7528\u4eba\u7c7b\u89c6\u9891\u4e2d\u7684\u7269\u4f53\u8fd0\u52a8\u4fe1\u53f7\u8bad\u7ec3\u673a\u5668\u4eba\u7b56\u7565\uff0c\u65e0\u9700\u673a\u5668\u4eba\u793a\u6559\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u8fdb\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4eba\u7c7b\u89c6\u9891\u7f3a\u4e4f\u52a8\u4f5c\u6807\u7b7e\uff0c\u73b0\u6709\u8de8\u5b9e\u4f53\u65b9\u6cd5\u5728\u5b9e\u4f53\u5dee\u5f02\u5927\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u4e00\u79cd\u57fa\u4e8e\u7269\u4f53\u8fd0\u52a8\u7684\u901a\u7528\u5b66\u4e60\u4fe1\u53f7\u3002", "method": "X-Sim\u91c7\u7528\u771f\u5b9e-\u4eff\u771f-\u771f\u5b9e\u6846\u67b6\uff1a\u4eceRGBD\u89c6\u9891\u91cd\u5efa\u4eff\u771f\u73af\u5883\uff0c\u7528\u7269\u4f53\u8f68\u8ff9\u5b9a\u4e49\u5956\u52b1\uff0c\u8bad\u7ec3RL\u7b56\u7565\u540e\u84b8\u998f\u4e3a\u6269\u6563\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u5728\u7ebf\u57df\u9002\u5e94\u8fc1\u79fb\u5230\u73b0\u5b9e\u3002", "result": "\u57285\u9879\u4efb\u52a1\u4e2d\uff0cX-Sim\u5e73\u5747\u4efb\u52a1\u8fdb\u5ea6\u63d0\u534730%\uff0c\u6570\u636e\u6536\u96c6\u65f6\u95f4\u51cf\u5c1110\u500d\uff0c\u4e14\u80fd\u9002\u5e94\u65b0\u89c6\u89d2\u548c\u52a8\u6001\u53d8\u5316\u3002", "conclusion": "X-Sim\u8bc1\u660e\u4e86\u5229\u7528\u7269\u4f53\u8fd0\u52a8\u4fe1\u53f7\u7684\u8de8\u5b9e\u4f53\u7b56\u7565\u5b66\u4e60\u7684\u6709\u6548\u6027\uff0c\u4e3a\u65e0\u793a\u6559\u6570\u636e\u7684\u673a\u5668\u4eba\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.06502", "pdf": "https://arxiv.org/pdf/2505.06502", "abs": "https://arxiv.org/abs/2505.06502", "authors": ["Md Rakibul Hasan", "Pouria Behnoudfar", "Dan MacKinlay", "Thomas Poulet"], "title": "PC-SRGAN: Physically Consistent Super-Resolution Generative Adversarial Network for General Transient Simulations", "categories": ["eess.IV", "cs.CE", "cs.CV", "cs.LG"], "comment": null, "summary": "Machine Learning, particularly Generative Adversarial Networks (GANs), has\nrevolutionised Super Resolution (SR). However, generated images often lack\nphysical meaningfulness, which is essential for scientific applications. Our\napproach, PC-SRGAN, enhances image resolution while ensuring physical\nconsistency for interpretable simulations. PC-SRGAN significantly improves both\nthe Peak Signal-to-Noise Ratio and the Structural Similarity Index Measure\ncompared to conventional methods, even with limited training data (e.g., only\n13% of training data required for SRGAN). Beyond SR, PC-SRGAN augments\nphysically meaningful machine learning, incorporating numerically justified\ntime integrators and advanced quality metrics. These advancements promise\nreliable and causal machine-learning models in scientific domains. A\nsignificant advantage of PC-SRGAN over conventional SR techniques is its\nphysical consistency, which makes it a viable surrogate model for\ntime-dependent problems. PC-SRGAN advances scientific machine learning,\noffering improved accuracy and efficiency for image processing, enhanced\nprocess understanding, and broader applications to scientific research. The\nsource codes and data will be made publicly available at\nhttps://github.com/hasan-rakibul/PC-SRGAN upon acceptance of this paper.", "AI": {"tldr": "PC-SRGAN\u662f\u4e00\u79cd\u57fa\u4e8eGAN\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u56fe\u50cf\u5206\u8fa8\u7387\u5e76\u786e\u4fdd\u7269\u7406\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86PSNR\u548cSSIM\u6307\u6807\uff0c\u4e14\u4ec5\u9700\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u6027\u80fd\u3002", "motivation": "\u5f53\u524dGAN\u751f\u6210\u7684\u8d85\u5206\u8fa8\u7387\u56fe\u50cf\u5728\u79d1\u5b66\u5e94\u7528\u4e2d\u7f3a\u4e4f\u7269\u7406\u610f\u4e49\uff0cPC-SRGAN\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u4f9b\u5177\u5907\u7269\u7406\u4e00\u81f4\u6027\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u3002", "method": "PC-SRGAN\u7ed3\u5408\u4e86\u6570\u503c\u9a8c\u8bc1\u7684\u65f6\u95f4\u79ef\u5206\u5668\u548c\u9ad8\u7ea7\u8d28\u91cf\u6307\u6807\uff0c\u786e\u4fdd\u751f\u6210\u56fe\u50cf\u7684\u7269\u7406\u5408\u7406\u6027\u3002", "result": "\u76f8\u8f83\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0cPC-SRGAN\u5728PSNR\u548cSSIM\u6307\u6807\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u4e14\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u66f4\u4f4e\uff08\u4ec5\u9700SRGAN\u768413%\uff09\u3002", "conclusion": "PC-SRGAN\u4e0d\u4ec5\u63d0\u5347\u4e86\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u4e3a\u65f6\u95f4\u4f9d\u8d56\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u66ff\u4ee3\u6a21\u578b\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.07119", "pdf": "https://arxiv.org/pdf/2505.07119", "abs": "https://arxiv.org/abs/2505.07119", "authors": ["Arianna Stropeni", "Francesco Borsatti", "Manuel Barusco", "Davide Dalle Pezze", "Marco Fabris", "Gian Antonio Susto"], "title": "Towards Scalable IoT Deployment for Visual Anomaly Detection via Efficient Compression", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual Anomaly Detection (VAD) is a key task in industrial settings, where\nminimizing waste and operational costs is essential. Deploying deep learning\nmodels within Internet of Things (IoT) environments introduces specific\nchallenges due to the limited computational power and bandwidth of edge\ndevices. This study investigates how to perform VAD effectively under such\nconstraints by leveraging compact and efficient processing strategies. We\nevaluate several data compression techniques, examining the trade-off between\nsystem latency and detection accuracy. Experiments on the MVTec AD benchmark\ndemonstrate that significant compression can be achieved with minimal loss in\nanomaly detection performance compared to uncompressed data.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u5982\u4f55\u5728\u8ba1\u7b97\u548c\u5e26\u5bbd\u6709\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u6267\u884c\u89c6\u89c9\u5f02\u5e38\u68c0\u6d4b\uff08VAD\uff09\uff0c\u901a\u8fc7\u6570\u636e\u538b\u7f29\u6280\u672f\u5e73\u8861\u7cfb\u7edf\u5ef6\u8fdf\u548c\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5b9e\u9a8c\u8868\u660e\u5728MVTec AD\u57fa\u51c6\u4e0a\u53ef\u5b9e\u73b0\u663e\u8457\u538b\u7f29\u4e14\u6027\u80fd\u635f\u5931\u6700\u5c0f\u3002", "motivation": "\u5de5\u4e1a\u73af\u5883\u4e2d\uff0c\u89c6\u89c9\u5f02\u5e38\u68c0\u6d4b\uff08VAD\uff09\u5bf9\u51cf\u5c11\u6d6a\u8d39\u548c\u8fd0\u8425\u6210\u672c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9762\u4e34\u8ba1\u7b97\u80fd\u529b\u548c\u5e26\u5bbd\u9650\u5236\u7684\u6311\u6218\u3002", "method": "\u7814\u7a76\u4e86\u591a\u79cd\u6570\u636e\u538b\u7f29\u6280\u672f\uff0c\u5206\u6790\u538b\u7f29\u4e0e\u7cfb\u7edf\u5ef6\u8fdf\u3001\u68c0\u6d4b\u7cbe\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "result": "\u5728MVTec AD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u538b\u7f29\u6570\u636e\u4e0e\u672a\u538b\u7f29\u6570\u636e\u76f8\u6bd4\uff0c\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u635f\u5931\u6781\u5c0f\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u538b\u7f29\u6548\u679c\u3002", "conclusion": "\u9ad8\u6548\u7684\u6570\u636e\u538b\u7f29\u6280\u672f\u53ef\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684VAD\uff0c\u4e3a\u5de5\u4e1a\u5e94\u7528\u63d0\u4f9b\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.07176", "pdf": "https://arxiv.org/pdf/2505.07176", "abs": "https://arxiv.org/abs/2505.07176", "authors": ["Yuntao Wang", "Shaolong Guo", "Yanghe Pan", "Zhou Su", "Fahao Chen", "Tom H. Luan", "Peng Li", "Jiawen Kang", "Dusit Niyato"], "title": "Internet of Agents: Fundamentals, Applications, and Challenges", "categories": ["cs.MA", "cs.AI"], "comment": "22 pages,10 figures, 8 tables. Submitted to IEEE TCCN", "summary": "With the rapid proliferation of large language models and vision-language\nmodels, AI agents have evolved from isolated, task-specific systems into\nautonomous, interactive entities capable of perceiving, reasoning, and acting\nwithout human intervention. As these agents proliferate across virtual and\nphysical environments, from virtual assistants to embodied robots, the need for\na unified, agent-centric infrastructure becomes paramount. In this survey, we\nintroduce the Internet of Agents (IoA) as a foundational framework that enables\nseamless interconnection, dynamic discovery, and collaborative orchestration\namong heterogeneous agents at scale. We begin by presenting a general IoA\narchitecture, highlighting its hierarchical organization, distinguishing\nfeatures relative to the traditional Internet, and emerging applications. Next,\nwe analyze the key operational enablers of IoA, including capability\nnotification and discovery, adaptive communication protocols, dynamic task\nmatching, consensus and conflict-resolution mechanisms, and incentive models.\nFinally, we identify open research directions toward building resilient and\ntrustworthy IoA ecosystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u201c\u667a\u80fd\u4f53\u4e92\u8054\u7f51\u201d\uff08IoA\uff09\u4f5c\u4e3a\u7edf\u4e00\u57fa\u7840\u8bbe\u65bd\u4ee5\u5b9e\u73b0\u5f02\u6784\u667a\u80fd\u4f53\u7684\u65e0\u7f1d\u4e92\u8054\u4e0e\u534f\u4f5c\uff0c\u4ecb\u7ecd\u4e86\u5176\u67b6\u6784\u3001\u5173\u952e\u4f7f\u80fd\u6280\u672f\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740AI\u667a\u80fd\u4f53\u4ece\u5b64\u7acb\u4efb\u52a1\u7cfb\u7edf\u53d1\u5c55\u4e3a\u81ea\u4e3b\u4ea4\u4e92\u5b9e\u4f53\uff0c\u8de8\u865a\u62df\u548c\u7269\u7406\u73af\u5883\u7684\u5e94\u7528\u9700\u6c42\u8feb\u5207\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u7840\u8bbe\u65bd\u652f\u6301\u667a\u80fd\u4f53\u7684\u4e92\u8054\u4e0e\u534f\u4f5c\u3002", "method": "\u901a\u8fc7\u5f15\u5165IoA\u6846\u67b6\uff0c\u5206\u6790\u5176\u5206\u5c42\u67b6\u6784\u3001\u4e0e\u4f20\u7edf\u4e92\u8054\u7f51\u7684\u533a\u522b\uff0c\u5e76\u63a2\u8ba8\u80fd\u529b\u901a\u77e5\u4e0e\u53d1\u73b0\u3001\u81ea\u9002\u5e94\u901a\u4fe1\u534f\u8bae\u3001\u52a8\u6001\u4efb\u52a1\u5339\u914d\u7b49\u5173\u952e\u4f7f\u80fd\u6280\u672f\u3002", "result": "\u63d0\u51fa\u4e86IoA\u7684\u57fa\u7840\u6846\u67b6\u548c\u8fd0\u884c\u673a\u5236\uff0c\u4e3a\u5927\u89c4\u6a21\u5f02\u6784\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u6280\u672f\u652f\u6301\u3002", "conclusion": "IoA\u4e3a\u6784\u5efa\u5f39\u6027\u548c\u53ef\u4fe1\u7684\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4f46\u672a\u6765\u4ecd\u9700\u5728\u591a\u4e2a\u7814\u7a76\u65b9\u5411\u8fdb\u4e00\u6b65\u63a2\u7d22\u4ee5\u5b9e\u73b0\u5176\u6f5c\u529b\u3002"}}
{"id": "2505.06531", "pdf": "https://arxiv.org/pdf/2505.06531", "abs": "https://arxiv.org/abs/2505.06531", "authors": ["Yong-Syun Cao", "Shinpei Imori", "Ching-Kang Ing"], "title": "High-Dimensional Importance-Weighted Information Criteria: Theory and Optimality", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Imori and Ing (2025) proposed the importance-weighted orthogonal greedy\nalgorithm (IWOGA) for model selection in high-dimensional misspecified\nregression models under covariate shift. To determine the number of IWOGA\niterations, they introduced the high-dimensional importance-weighted\ninformation criterion (HDIWIC). They argued that the combined use of IWOGA and\nHDIWIC, IWOGA + HDIWIC, achieves an optimal trade-off between variance and\nsquared bias, leading to optimal convergence rates in terms of conditional mean\nsquared prediction error. In this article, we provide a theoretical\njustification for this claim by establishing the optimality of IWOGA + HDIWIC\nunder a set of reasonable assumptions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIWOGA\u7684\u7b97\u6cd5\u548cHDIWIC\u51c6\u5219\uff0c\u7528\u4e8e\u9ad8\u7ef4\u8bef\u8bbe\u56de\u5f52\u6a21\u578b\u4e2d\u7684\u6a21\u578b\u9009\u62e9\uff0c\u5e76\u7406\u8bba\u8bc1\u660e\u5176\u6700\u4f18\u6027\u3002", "motivation": "\u5728\u9ad8\u7ef4\u8bef\u8bbe\u56de\u5f52\u6a21\u578b\u548c\u534f\u53d8\u91cf\u504f\u79fb\u60c5\u51b5\u4e0b\uff0c\u73b0\u6709\u7684\u6a21\u578b\u9009\u62e9\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\u548c\u51c6\u5219\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u91cd\u8981\u6027\u52a0\u6743\u6b63\u4ea4\u8d2a\u5a6a\u7b97\u6cd5\uff08IWOGA\uff09\u548c\u9ad8\u7ef4\u91cd\u8981\u6027\u52a0\u6743\u4fe1\u606f\u51c6\u5219\uff08HDIWIC\uff09\uff0c\u5e76\u7406\u8bba\u5206\u6790\u4e86\u5176\u7ec4\u5408\u4f7f\u7528\u7684\u6709\u6548\u6027\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u8868\u660e\uff0cIWOGA + HDIWIC\u80fd\u591f\u5728\u65b9\u5dee\u548c\u5e73\u65b9\u504f\u5dee\u4e4b\u95f4\u5b9e\u73b0\u6700\u4f18\u5e73\u8861\uff0c\u4ece\u800c\u83b7\u5f97\u6700\u4f18\u7684\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "IWOGA + HDIWIC\u5728\u9ad8\u7ef4\u8bef\u8bbe\u56de\u5f52\u6a21\u578b\u7684\u534f\u53d8\u91cf\u504f\u79fb\u4e0b\u662f\u4e00\u79cd\u6709\u6548\u7684\u6a21\u578b\u9009\u62e9\u65b9\u6cd5\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u969c\u7684\u6700\u4f18\u6027\u80fd\u3002"}}
{"id": "2505.07214", "pdf": "https://arxiv.org/pdf/2505.07214", "abs": "https://arxiv.org/abs/2505.07214", "authors": ["Pascal Spiegler", "Arash Harirpoush", "Yiming Xiao"], "title": "Towards user-centered interactive medical image segmentation in VR with an assistive AI agent", "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": null, "summary": "Crucial in disease analysis and surgical planning, manual segmentation of\nvolumetric medical scans (e.g. MRI, CT) is laborious, error-prone, and\nchallenging to master, while fully automatic algorithms can benefit from\nuser-feedback. Therefore, with the complementary power of the latest\nradiological AI foundation models and virtual reality (VR)'s intuitive data\ninteraction, we propose SAMIRA, a novel conversational AI agent that assists\nusers with localizing, segmenting, and visualizing 3D medical concepts in VR.\nThrough speech-based interaction, the agent helps users understand radiological\nfeatures, locate clinical targets, and generate segmentation masks that can be\nrefined with just a few point prompts. The system also supports true-to-scale\n3D visualization of segmented pathology to enhance patient-specific anatomical\nunderstanding. Furthermore, to determine the optimal interaction paradigm under\nnear-far attention-switching for refining segmentation masks in an immersive,\nhuman-in-the-loop workflow, we compare VR controller pointing, head pointing,\nand eye tracking as input modes. With a user study, evaluations demonstrated a\nhigh usability score (SUS=90.0 $\\pm$ 9.0), low overall task load, as well as\nstrong support for the proposed VR system's guidance, training potential, and\nintegration of AI in radiological segmentation tasks.", "AI": {"tldr": "SAMIRA\u662f\u4e00\u4e2a\u57fa\u4e8eVR\u548cAI\u7684\u5bf9\u8bdd\u5f0f\u4ee3\u7406\uff0c\u5e2e\u52a9\u7528\u6237\u901a\u8fc7\u8bed\u97f3\u4ea4\u4e92\u548c\u76f4\u89c2\u76843D\u53ef\u89c6\u5316\u8fdb\u884c\u533b\u5b66\u5f71\u50cf\u7684\u5b9a\u4f4d\u3001\u5206\u5272\u548c\u6807\u8bb0\uff0c\u663e\u8457\u63d0\u5347\u5206\u5272\u4efb\u52a1\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u533b\u5b66\u5f71\u50cf\u624b\u52a8\u5206\u5272\u8017\u65f6\u4e14\u6613\u9519\uff0c\u81ea\u52a8\u7b97\u6cd5\u867d\u9ad8\u6548\u4f46\u7f3a\u4e4f\u4ea4\u4e92\u6027\uff0c\u4e9f\u9700\u7ed3\u5408AI\u4e0eVR\u6280\u672f\u63d0\u5347\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u5206\u5272\u8d28\u91cf\u3002", "method": "\u63d0\u51faSAMIRA\u7cfb\u7edf\uff0c\u7ed3\u5408\u653e\u5c04\u5b66AI\u57fa\u7840\u6a21\u578b\u4e0eVR\u4ea4\u4e92\uff0c\u652f\u6301\u8bed\u97f3\u6307\u4ee4\u3001\u70b9\u63d0\u793a\u5206\u5272\u53ca\u771f\u5b9e\u6bd4\u4f8b3D\u53ef\u89c6\u5316\uff0c\u5e76\u5bf9\u6bd4\u63a7\u5236\u5668\u3001\u5934\u90e8\u6307\u5411\u548c\u773c\u52a8\u8ffd\u8e2a\u4e09\u79cd\u4ea4\u4e92\u6a21\u5f0f\u3002", "result": "\u7528\u6237\u7814\u7a76\u663e\u793a\u9ad8\u53ef\u7528\u6027\uff08SUS=90.0\u00b19.0\uff09\u3001\u4f4e\u4efb\u52a1\u8d1f\u8377\uff0c\u4e14\u7cfb\u7edf\u5728\u6307\u5bfc\u6027\u3001\u57f9\u8bad\u6f5c\u529b\u548cAI\u6574\u5408\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "SAMIRA\u901a\u8fc7\u6c89\u6d78\u5f0f\u4eba\u673a\u534f\u4f5c\u8303\u5f0f\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u76f4\u89c2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e86VR\u4e0eAI\u7ed3\u5408\u7684\u4e34\u5e8a\u6f5c\u529b\u3002"}}
{"id": "2505.07236", "pdf": "https://arxiv.org/pdf/2505.07236", "abs": "https://arxiv.org/abs/2505.07236", "authors": ["Oleg Sautenkov", "Yasheerah Yaqoot", "Muhammad Ahsan Mustafa", "Faryal Batool", "Jeffrin Sam", "Artem Lykov", "Chih-Yung Wen", "Dzmitry Tsetserukou"], "title": "UAV-CodeAgents: Scalable UAV Mission Planning via Multi-Agent ReAct and Vision-Language Reasoning", "categories": ["cs.RO", "cs.AI"], "comment": "Submitted", "summary": "We present UAV-CodeAgents, a scalable multi-agent framework for autonomous\nUAV mission generation, built on large language and vision-language models\n(LLMs/VLMs). The system leverages the ReAct (Reason + Act) paradigm to\ninterpret satellite imagery, ground high-level natural language instructions,\nand collaboratively generate UAV trajectories with minimal human supervision. A\ncore component is a vision-grounded, pixel-pointing mechanism that enables\nprecise localization of semantic targets on aerial maps. To support real-time\nadaptability, we introduce a reactive thinking loop, allowing agents to\niteratively reflect on observations, revise mission goals, and coordinate\ndynamically in evolving environments.\n  UAV-CodeAgents is evaluated on large-scale mission scenarios involving\nindustrial and environmental fire detection. Our results show that a lower\ndecoding temperature (0.5) yields higher planning reliability and reduced\nexecution time, with an average mission creation time of 96.96 seconds and a\nsuccess rate of 93%. We further fine-tune Qwen2.5VL-7B on 9,000 annotated\nsatellite images, achieving strong spatial grounding across diverse visual\ncategories. To foster reproducibility and future research, we will release the\nfull codebase and a novel benchmark dataset for vision-language-based UAV\nplanning.", "AI": {"tldr": "UAV-CodeAgents\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u4e3b\u751f\u6210\u65e0\u4eba\u673a\u4efb\u52a1\uff0c\u901a\u8fc7ReAct\u8303\u5f0f\u89e3\u6790\u536b\u661f\u56fe\u50cf\u548c\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u4efb\u52a1\u89c4\u5212\u3002", "motivation": "\u65e8\u5728\u51cf\u5c11\u65e0\u4eba\u673a\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u4eba\u5de5\u76d1\u7763\uff0c\u63d0\u5347\u5176\u5728\u590d\u6742\u73af\u5883\uff08\u5982\u5de5\u4e1a\u706b\u707e\u68c0\u6d4b\uff09\u4e2d\u7684\u81ea\u4e3b\u6027\u548c\u5b9e\u65f6\u9002\u5e94\u6027\u3002", "method": "\u7ed3\u5408ReAct\u8303\u5f0f\u3001\u89c6\u89c9\u5b9a\u4f4d\u673a\u5236\u548c\u53cd\u5e94\u601d\u7ef4\u5faa\u73af\uff0c\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4efb\u52a1\u751f\u6210\uff0c\u5e76\u57fa\u4e8eQwen2.5VL-7B\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u4efb\u52a1\u521b\u5efa\u65f6\u95f4\uff08\u5e73\u574796.96\u79d2\uff09\u548c\u6210\u529f\u7387\uff0893%\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u89e3\u7801\u6e29\u5ea60.5\u65f6\u53ef\u9760\u6027\u6700\u9ad8\u3002", "conclusion": "\u6846\u67b6\u5c55\u793a\u4e86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u9ad8\u6548\u751f\u6210\u65e0\u4eba\u673a\u4efb\u52a1\u7684\u6f5c\u529b\uff0c\u672a\u6765\u5c06\u5f00\u6e90\u4ee3\u7801\u548c\u57fa\u51c6\u6570\u636e\u96c6\u4ee5\u63a8\u52a8\u7814\u7a76\u3002"}}
{"id": "2505.07239", "pdf": "https://arxiv.org/pdf/2505.07239", "abs": "https://arxiv.org/abs/2505.07239", "authors": ["Guang Yan", "Yuhui Zhang", "Zimu Guo", "Lutan Zhao", "Xiaojun Chen", "Chen Wang", "Wenhao Wang", "Dan Meng", "Rui Hou"], "title": "Comet: Accelerating Private Inference for Large Language Model by Predicting Activation Sparsity", "categories": ["cs.CR", "cs.AI"], "comment": "Accepted to SP 2025", "summary": "With the growing use of large language models (LLMs) hosted on cloud\nplatforms to offer inference services, privacy concerns about the potential\nleakage of sensitive information are escalating. Secure multi-party computation\n(MPC) is a promising solution to protect the privacy in LLM inference. However,\nMPC requires frequent inter-server communication, causing high performance\noverhead.\n  Inspired by the prevalent activation sparsity of LLMs, where most neuron are\nnot activated after non-linear activation functions, we propose an efficient\nprivate inference system, Comet. This system employs an accurate and fast\npredictor to predict the sparsity distribution of activation function output.\nAdditionally, we introduce a new private inference protocol. It efficiently and\nsecurely avoids computations involving zero values by exploiting the spatial\nlocality of the predicted sparse distribution. While this computation-avoidance\napproach impacts the spatiotemporal continuity of KV cache entries, we address\nthis challenge with a low-communication overhead cache refilling strategy that\nmerges miss requests and incorporates a prefetching mechanism. Finally, we\nevaluate Comet on four common LLMs and compare it with six state-of-the-art\nprivate inference systems. Comet achieves a 1.87x-2.63x speedup and a\n1.94x-2.64x communication reduction.", "AI": {"tldr": "Comet\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u79c1\u6709\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u9884\u6d4bLLM\u6fc0\u6d3b\u51fd\u6570\u7684\u7a00\u758f\u5206\u5e03\uff0c\u51cf\u5c11\u96f6\u503c\u8ba1\u7b97\uff0c\u63d0\u5347\u9690\u79c1\u4fdd\u62a4\u4e0b\u7684\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u968f\u7740\u4e91\u5e73\u53f0LLM\u63a8\u7406\u670d\u52a1\u7684\u666e\u53ca\uff0c\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u589e\u52a0\uff0c\u800c\u73b0\u6709\u7684\u5b89\u5168\u591a\u65b9\u8ba1\u7b97\uff08MPC\uff09\u65b9\u6cd5\u901a\u4fe1\u5f00\u9500\u9ad8\u3002", "method": "\u5229\u7528LLM\u6fc0\u6d3b\u7a00\u758f\u6027\uff0c\u8bbe\u8ba1\u9884\u6d4b\u5668\u548c\u79c1\u6709\u63a8\u7406\u534f\u8bae\uff0c\u907f\u514d\u96f6\u503c\u8ba1\u7b97\uff0c\u5e76\u901a\u8fc7\u7f13\u5b58\u91cd\u586b\u7b56\u7565\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u3002", "result": "\u5728\u56db\u79cd\u5e38\u89c1LLM\u4e0a\u6d4b\u8bd5\uff0cComet\u6bd4\u73b0\u6709\u7cfb\u7edf\u63d0\u901f1.87x-2.63x\uff0c\u901a\u4fe1\u51cf\u5c111.94x-2.64x\u3002", "conclusion": "Comet\u663e\u8457\u63d0\u5347\u4e86\u79c1\u6709\u63a8\u7406\u7684\u6548\u7387\u548c\u901a\u4fe1\u6548\u7387\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.06578", "pdf": "https://arxiv.org/pdf/2505.06578", "abs": "https://arxiv.org/abs/2505.06578", "authors": ["Maxim Vashkevich", "Egor Krivalcevich"], "title": "Compact and Efficient Neural Networks for Image Recognition Based on Learned 2D Separable Transform", "categories": ["cs.CV", "cs.LG", "68T07", "I.5.1"], "comment": "6 pages, 9 figures", "summary": "The paper presents a learned two-dimensional separable transform (LST) that\ncan be considered as a new type of computational layer for constructing neural\nnetwork (NN) architecture for image recognition tasks. The LST based on the\nidea of sharing the weights of one fullyconnected (FC) layer to process all\nrows of an image. After that, a second shared FC layer is used to process all\ncolumns of image representation obtained from the first layer. The use of LST\nlayers in a NN architecture significantly reduces the number of model\nparameters compared to models that use stacked FC layers. We show that a\nNN-classifier based on a single LST layer followed by an FC layer achieves\n98.02\\% accuracy on the MNIST dataset, while having only 9.5k parameters. We\nalso implemented a LST-based classifier for handwritten digit recognition on\nthe FPGA platform to demonstrate the efficiency of the suggested approach for\ndesigning a compact and high-performance implementation of NN models. Git\nrepository with supplementary materials: https://github.com/Mak-Sim/LST-2d", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e8c\u7ef4\u53ef\u5206\u79bb\u53d8\u6362\uff08LST\uff09\u4f5c\u4e3a\u795e\u7ecf\u7f51\u7edc\u7684\u8ba1\u7b97\u5c42\uff0c\u7528\u4e8e\u56fe\u50cf\u8bc6\u522b\u4efb\u52a1\u3002LST\u901a\u8fc7\u5171\u4eab\u5168\u8fde\u63a5\u5c42\u6743\u91cd\u5904\u7406\u56fe\u50cf\u7684\u884c\u548c\u5217\uff0c\u5927\u5e45\u51cf\u5c11\u6a21\u578b\u53c2\u6570\u3002\u5728MNIST\u6570\u636e\u96c6\u4e0a\uff0c\u57fa\u4e8eLST\u7684\u5206\u7c7b\u5668\u5b9e\u73b0\u4e8698.02%\u7684\u51c6\u786e\u7387\uff0c\u4ec5\u97009.5k\u53c2\u6570\uff0c\u5e76\u5c55\u793a\u4e86FPGA\u5e73\u53f0\u4e0a\u7684\u9ad8\u6548\u5b9e\u73b0\u3002", "motivation": "\u9488\u5bf9\u4f20\u7edf\u5168\u8fde\u63a5\u5c42\u53c2\u6570\u91cf\u5927\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u51faLST\u5c42\u4ee5\u51cf\u5c11\u6a21\u578b\u53c2\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\uff0c\u5c24\u5176\u9002\u5408\u786c\u4ef6\u5b9e\u73b0\u3002", "method": "\u91c7\u7528\u4e24\u5c42\u5171\u4eab\u6743\u91cd\u7684\u5168\u8fde\u63a5\u5c42\uff0c\u5206\u522b\u5904\u7406\u56fe\u50cf\u7684\u884c\u548c\u5217\uff0c\u4ece\u800c\u6784\u5efa\u4e8c\u7ef4\u53ef\u5206\u79bb\u53d8\u6362\uff08LST\uff09\u5c42\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u795e\u7ecf\u7f51\u7edc\u7684\u4e00\u90e8\u5206\u3002", "result": "\u5728MNIST\u6570\u636e\u96c6\u4e0a\uff0c\u57fa\u4e8eLST\u7684\u5206\u7c7b\u5668\u51c6\u786e\u7387\u8fbe98.02%\uff0c\u6a21\u578b\u53c2\u6570\u4ec59.5k\uff0c\u5e76\u5728FPGA\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u3002", "conclusion": "LST\u5c42\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u795e\u7ecf\u7f51\u7edc\u6784\u5efa\u6a21\u5757\uff0c\u80fd\u5728\u51cf\u5c11\u53c2\u6570\u91cf\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\uff0c\u9002\u5408\u7d27\u51d1\u548c\u9ad8\u6027\u80fd\u7684\u786c\u4ef6\u5b9e\u73b0\u3002"}}
{"id": "2505.06601", "pdf": "https://arxiv.org/pdf/2505.06601", "abs": "https://arxiv.org/abs/2505.06601", "authors": ["Yuanhang Luo", "Yeheng Ge", "Ruijian Han", "Guohao Shen"], "title": "Learning Guarantee of Reward Modeling Using Deep Neural Networks", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "In this work, we study the learning theory of reward modeling with pairwise\ncomparison data using deep neural networks. We establish a novel non-asymptotic\nregret bound for deep reward estimators in a non-parametric setting, which\ndepends explicitly on the network architecture. Furthermore, to underscore the\ncritical importance of clear human beliefs, we introduce a margin-type\ncondition that assumes the conditional winning probability of the optimal\naction in pairwise comparisons is significantly distanced from 1/2. This\ncondition enables a sharper regret bound, which substantiates the empirical\nefficiency of Reinforcement Learning from Human Feedback and highlights clear\nhuman beliefs in its success. Notably, this improvement stems from high-quality\npairwise comparison data implied by the margin-type condition, is independent\nof the specific estimators used, and thus applies to various learning\nalgorithms and models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5bf9\u6210\u5bf9\u6bd4\u8f83\u6570\u636e\u8fdb\u884c\u5956\u52b1\u5efa\u6a21\u7684\u5b66\u4e60\u7406\u8bba\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u53c2\u6570\u8bbe\u7f6e\u4e0b\u4f9d\u8d56\u4e8e\u7f51\u7edc\u67b6\u6784\u7684\u975e\u6e10\u8fd1\u9057\u61be\u8fb9\u754c\uff0c\u5e76\u901a\u8fc7\u8fb9\u9645\u6761\u4ef6\u5f3a\u8c03\u4e86\u6e05\u6670\u4eba\u7c7b\u504f\u597d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5956\u52b1\u5efa\u6a21\u7684\u975e\u6e10\u8fd1\u7406\u8bba\u5206\u6790\uff0c\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u4ece\u4eba\u7c7b\u53cd\u9988\u4e2d\u5b66\u4e60\uff08RLHF\uff09\u7684\u6210\u529f\u673a\u5236\uff0c\u5c24\u5176\u662f\u6e05\u6670\u4eba\u7c7b\u504f\u597d\u5728\u5176\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6784\u5efa\u5956\u52b1\u4f30\u8ba1\u5668\uff0c\u5728\u975e\u53c2\u6570\u8bbe\u7f6e\u4e0b\u5206\u6790\u5176\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u8fb9\u9645\u6761\u4ef6\uff08\u5047\u8bbe\u6700\u4f18\u52a8\u4f5c\u7684\u80dc\u7387\u663e\u8457\u504f\u79bb1/2\uff09\u4ee5\u4f18\u5316\u9057\u61be\u8fb9\u754c\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u8fb9\u9645\u6761\u4ef6\u80fd\u663e\u8457\u63d0\u5347\u9057\u61be\u8fb9\u754c\u7684\u7d27\u81f4\u6027\uff0c\u4e14\u8fd9\u4e00\u6539\u8fdb\u72ec\u7acb\u4e8e\u5177\u4f53\u4f30\u8ba1\u5668\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5b66\u4e60\u7b97\u6cd5\u548c\u6a21\u578b\u3002", "conclusion": "\u6e05\u6670\u7684\u4eba\u7c7b\u504f\u597d\uff08\u901a\u8fc7\u9ad8\u8d28\u91cf\u6210\u5bf9\u6bd4\u8f83\u6570\u636e\u4f53\u73b0\uff09\u662fRLHF\u9ad8\u6548\u6027\u7684\u5173\u952e\uff0c\u8fb9\u9645\u6761\u4ef6\u4e3a\u7406\u8bba\u652f\u6301\u63d0\u4f9b\u4e86\u901a\u7528\u6846\u67b6\u3002"}}
{"id": "2505.07251", "pdf": "https://arxiv.org/pdf/2505.07251", "abs": "https://arxiv.org/abs/2505.07251", "authors": ["Wenqiang Wang", "Yangshijie Zhang"], "title": "Incomplete In-context Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large vision language models (LVLMs) achieve remarkable performance through\nVision In-context Learning (VICL), a process that depends significantly on\ndemonstrations retrieved from an extensive collection of annotated examples\n(retrieval database). Existing studies often assume that the retrieval database\ncontains annotated examples for all labels. However, in real-world scenarios,\ndelays in database updates or incomplete data annotation may result in the\nretrieval database containing labeled samples for only a subset of classes. We\nrefer to this phenomenon as an \\textbf{incomplete retrieval database} and\ndefine the in-context learning under this condition as \\textbf{Incomplete\nIn-context Learning (IICL)}. To address this challenge, we propose\n\\textbf{Iterative Judgments and Integrated Prediction (IJIP)}, a two-stage\nframework designed to mitigate the limitations of IICL. The Iterative Judgments\nStage reformulates an \\(\\boldsymbol{m}\\)-class classification problem into a\nseries of \\(\\boldsymbol{m}\\) binary classification tasks, effectively\nconverting the IICL setting into a standard VICL scenario. The Integrated\nPrediction Stage further refines the classification process by leveraging both\nthe input image and the predictions from the Iterative Judgments Stage to\nenhance overall classification accuracy. IJIP demonstrates considerable\nperformance across two LVLMs and two datasets under three distinct conditions\nof label incompleteness, achieving the highest accuracy of 93.9\\%. Notably,\neven in scenarios where labels are fully available, IJIP still achieves the\nbest performance of all six baselines. Furthermore, IJIP can be directly\napplied to \\textbf{Prompt Learning} and is adaptable to the \\textbf{text\ndomain}.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u8fed\u4ee3\u5224\u65ad\u4e0e\u96c6\u6210\u9884\u6d4b\uff08IJIP\uff09\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u5728\u4e0d\u5b8c\u6574\u68c0\u7d22\u6570\u636e\u5e93\u6761\u4ef6\u4e0b\u7684\u4e0d\u5b8c\u5168\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08IICL\uff09\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u6570\u636e\u5e93\u66f4\u65b0\u5ef6\u8fdf\u6216\u6807\u6ce8\u4e0d\u5b8c\u6574\u53ef\u80fd\u5bfc\u81f4\u68c0\u7d22\u6570\u636e\u5e93\u4ec5\u5305\u542b\u90e8\u5206\u7c7b\u522b\u7684\u6837\u672c\uff0c\u4f20\u7edf\u65b9\u6cd5\u5047\u8bbe\u6570\u636e\u5e93\u5305\u542b\u6240\u6709\u6807\u7b7e\u6837\u672c\uff0c\u65e0\u6cd5\u9002\u5e94\u8fd9\u79cd\u573a\u666f\u3002", "method": "IJIP\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u8fed\u4ee3\u5224\u65ad\u9636\u6bb5\u5c06\u591a\u5206\u7c7b\u95ee\u9898\u8f6c\u5316\u4e3a\u591a\u4e2a\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\uff0c\u96c6\u6210\u9884\u6d4b\u9636\u6bb5\u7ed3\u5408\u8f93\u5165\u56fe\u50cf\u548c\u8fed\u4ee3\u5224\u65ad\u7ed3\u679c\u4f18\u5316\u5206\u7c7b\u3002", "result": "\u5728\u4e24\u79cdLVLM\u548c\u4e09\u79cd\u6807\u7b7e\u4e0d\u5b8c\u6574\u6761\u4ef6\u4e0b\uff0cIJIP\u51c6\u786e\u7387\u6700\u9ad8\u8fbe93.9%\uff0c\u5373\u4f7f\u5728\u6807\u7b7e\u5b8c\u6574\u65f6\u4e5f\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "IJIP\u4e0d\u4ec5\u89e3\u51b3\u4e86IICL\u95ee\u9898\uff0c\u8fd8\u9002\u7528\u4e8e\u63d0\u793a\u5b66\u4e60\u548c\u6587\u672c\u9886\u57df\uff0c\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2505.06646", "pdf": "https://arxiv.org/pdf/2505.06646", "abs": "https://arxiv.org/abs/2505.06646", "authors": ["Daniel Strick", "Carlos Garcia", "Anthony Huang"], "title": "Reproducing and Improving CheXNet: Deep Learning for Chest X-ray Disease Classification", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "12 pages, 4 figures", "summary": "Deep learning for radiologic image analysis is a rapidly growing field in\nbiomedical research and is likely to become a standard practice in modern\nmedicine. On the publicly available NIH ChestX-ray14 dataset, containing X-ray\nimages that are classified by the presence or absence of 14 different diseases,\nwe reproduced an algorithm known as CheXNet, as well as explored other\nalgorithms that outperform CheXNet's baseline metrics. Model performance was\nprimarily evaluated using the F1 score and AUC-ROC, both of which are critical\nmetrics for imbalanced, multi-label classification tasks in medical imaging.\nThe best model achieved an average AUC-ROC score of 0.85 and an average F1\nscore of 0.39 across all 14 disease classifications present in the dataset.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u80f8\u90e8X\u5149\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u590d\u73b0\u4e86CheXNet\u7b97\u6cd5\u5e76\u63a2\u7d22\u4e86\u5176\u4ed6\u66f4\u4f18\u7b97\u6cd5\uff0c\u8bc4\u4f30\u6307\u6807\u5305\u62ecF1\u5206\u6570\u548cAUC-ROC\u3002", "motivation": "\u63a2\u7d22\u6df1\u5ea6\u5b66\u4e60\u5728\u653e\u5c04\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u6f5c\u529b\uff0c\u6539\u8fdb\u73b0\u6709\u7b97\u6cd5\uff0c\u4ee5\u63d0\u5347\u5bf914\u79cd\u75be\u75c5\u7684\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u5728\u516c\u5f00\u7684NIH ChestX-ray14\u6570\u636e\u96c6\u4e0a\u590d\u73b0CheXNet\uff0c\u5e76\u6d4b\u8bd5\u5176\u4ed6\u7b97\u6cd5\u3002", "result": "\u6700\u4f73\u6a21\u578b\u7684\u5e73\u5747AUC-ROC\u4e3a0.85\uff0c\u5e73\u5747F1\u5206\u6570\u4e3a0.39\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u5728\u80f8\u90e8X\u5149\u5206\u6790\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u672a\u6765\u53ef\u80fd\u6210\u4e3a\u533b\u5b66\u5f71\u50cf\u5206\u6790\u7684\u6807\u914d\u3002"}}
{"id": "2505.06668", "pdf": "https://arxiv.org/pdf/2505.06668", "abs": "https://arxiv.org/abs/2505.06668", "authors": ["Ziyi Wang", "Haipeng Li", "Lin Sui", "Tianhao Zhou", "Hai Jiang", "Lang Nie", "Shuaicheng Liu"], "title": "StableMotion: Repurposing Diffusion-Based Image Priors for Motion Estimation", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "We present StableMotion, a novel framework leverages knowledge (geometry and\ncontent priors) from pretrained large-scale image diffusion models to perform\nmotion estimation, solving single-image-based image rectification tasks such as\nStitched Image Rectangling (SIR) and Rolling Shutter Correction (RSC).\nSpecifically, StableMotion framework takes text-to-image Stable Diffusion (SD)\nmodels as backbone and repurposes it into an image-to-motion estimator. To\nmitigate inconsistent output produced by diffusion models, we propose Adaptive\nEnsemble Strategy (AES) that consolidates multiple outputs into a cohesive,\nhigh-fidelity result. Additionally, we present the concept of Sampling Steps\nDisaster (SSD), the counterintuitive scenario where increasing the number of\nsampling steps can lead to poorer outcomes, which enables our framework to\nachieve one-step inference. StableMotion is verified on two image rectification\ntasks and delivers state-of-the-art performance in both, as well as showing\nstrong generalizability. Supported by SSD, StableMotion offers a speedup of 200\ntimes compared to previous diffusion model-based methods.", "AI": {"tldr": "StableMotion\u662f\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u5927\u89c4\u6a21\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u77e5\u8bc6\u8fdb\u884c\u8fd0\u52a8\u4f30\u8ba1\uff0c\u89e3\u51b3\u5355\u56fe\u50cf\u6821\u6b63\u4efb\u52a1\uff0c\u5982\u62fc\u63a5\u56fe\u50cf\u77eb\u6b63\u548c\u6eda\u52a8\u5feb\u95e8\u6821\u6b63\u3002\u901a\u8fc7\u81ea\u9002\u5e94\u96c6\u5408\u7b56\u7565\u548c\u91c7\u6837\u6b65\u9aa4\u707e\u96be\u6982\u5ff5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u5feb\u901f\u63a8\u7406\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5355\u56fe\u50cf\u6821\u6b63\u4efb\u52a1\u4e2d\u7684\u8fd0\u52a8\u4f30\u8ba1\u95ee\u9898\uff0c\u5e76\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u51e0\u4f55\u548c\u5185\u5bb9\u5148\u9a8c\u77e5\u8bc6\uff0c\u63d0\u9ad8\u4efb\u52a1\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "method": "\u4f7f\u7528\u6587\u672c\u5230\u56fe\u50cfStable Diffusion\u6a21\u578b\u4f5c\u4e3a\u9aa8\u5e72\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u96c6\u5408\u7b56\u7565\uff08AES\uff09\u6574\u5408\u591a\u4e2a\u8f93\u51fa\u4ee5\u63d0\u5347\u7ed3\u679c\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u5f15\u5165\u91c7\u6837\u6b65\u9aa4\u707e\u96be\uff08SSD\uff09\u6982\u5ff5\u5b9e\u73b0\u4e00\u6b65\u63a8\u7406\u3002", "result": "\u5728\u4e24\u4e2a\u56fe\u50cf\u6821\u6b63\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u6cdb\u5316\u80fd\u529b\uff1b\u76f8\u6bd4\u4e4b\u524d\u7684\u65b9\u6cd5\uff0c\u901f\u5ea6\u63d0\u9ad8\u4e86200\u500d\u3002", "conclusion": "StableMotion\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u6a21\u578b\u7684\u5148\u9a8c\u77e5\u8bc6\u548c\u65b0\u63d0\u51fa\u7684\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u56fe\u50cf\u8fd0\u52a8\u4f30\u8ba1\u4efb\u52a1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.07261", "pdf": "https://arxiv.org/pdf/2505.07261", "abs": "https://arxiv.org/abs/2505.07261", "authors": ["Ce Hao", "Anxing Xiao", "Zhiwei Xue", "Harold Soh"], "title": "CHD: Coupled Hierarchical Diffusion for Long-Horizon Tasks", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Diffusion-based planners have shown strong performance in short-horizon tasks\nbut often fail in complex, long-horizon settings. We trace the failure to loose\ncoupling between high-level (HL) sub-goal selection and low-level (LL)\ntrajectory generation, which leads to incoherent plans and degraded\nperformance. We propose Coupled Hierarchical Diffusion (CHD), a framework that\nmodels HL sub-goals and LL trajectories jointly within a unified diffusion\nprocess. A shared classifier passes LL feedback upstream so that sub-goals\nself-correct while sampling proceeds. This tight HL-LL coupling improves\ntrajectory coherence and enables scalable long-horizon diffusion planning.\nExperiments across maze navigation, tabletop manipulation, and household\nenvironments show that CHD consistently outperforms both flat and hierarchical\ndiffusion baselines.", "AI": {"tldr": "CHD\uff08\u8026\u5408\u5c42\u6b21\u6269\u6563\uff09\u901a\u8fc7\u7edf\u4e00\u6269\u6563\u8fc7\u7a0b\u8054\u5408\u5efa\u6a21\u9ad8\u5c42\u5b50\u76ee\u6807\u548c\u4f4e\u5c42\u8f68\u8ff9\uff0c\u6539\u5584\u4e86\u8f68\u8ff9\u8fde\u8d2f\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6269\u6563\u7684\u89c4\u5212\u5668\u5728\u590d\u6742\u957f\u5468\u671f\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u6e90\u4e8e\u9ad8\u5c42\u5b50\u76ee\u6807\u9009\u62e9\u548c\u4f4e\u5c42\u8f68\u8ff9\u751f\u6210\u4e4b\u95f4\u7684\u677e\u6563\u8026\u5408\u3002", "method": "\u63d0\u51faCHD\u6846\u67b6\uff0c\u5728\u7edf\u4e00\u6269\u6563\u8fc7\u7a0b\u4e2d\u8054\u5408\u5efa\u6a21\u9ad8\u5c42\u5b50\u76ee\u6807\u548c\u4f4e\u5c42\u8f68\u8ff9\uff0c\u5e76\u901a\u8fc7\u5171\u4eab\u5206\u7c7b\u5668\u4f20\u9012\u4f4e\u5c42\u53cd\u9988\u4ee5\u5b9e\u73b0\u5b50\u76ee\u6807\u81ea\u6821\u6b63\u3002", "result": "\u5728\u8ff7\u5bab\u5bfc\u822a\u3001\u684c\u9762\u64cd\u4f5c\u548c\u5bb6\u5ead\u73af\u5883\u4e2d\uff0cCHD\u5747\u4f18\u4e8e\u5e73\u9762\u548c\u5206\u5c42\u6269\u6563\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CHD\u901a\u8fc7\u7d27\u5bc6\u8026\u5408\u9ad8\u5c42\u4e0e\u4f4e\u5c42\u89c4\u5212\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u957f\u5468\u671f\u6269\u6563\u89c4\u5212\uff0c\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2505.06701", "pdf": "https://arxiv.org/pdf/2505.06701", "abs": "https://arxiv.org/abs/2505.06701", "authors": ["Akansha Shukla", "Parth Atulbhai Gandhi", "Yuval Elovici", "Asaf Shabtai"], "title": "RuleGenie: SIEM Detection Rule Set Optimization", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "SIEM systems serve as a critical hub, employing rule-based logic to detect\nand respond to threats. Redundant or overlapping rules in SIEM systems lead to\nexcessive false alerts, degrading analyst performance due to alert fatigue, and\nincrease computational overhead and response latency for actual threats. As a\nresult, optimizing SIEM rule sets is essential for efficient operations.\nDespite the importance of such optimization, research in this area is limited,\nwith current practices relying on manual optimization methods that are both\ntime-consuming and error-prone due to the scale and complexity of\nenterprise-level rule sets. To address this gap, we present RuleGenie, a novel\nlarge language model (LLM) aided recommender system designed to optimize SIEM\nrule sets. Our approach leverages transformer models' multi-head attention\ncapabilities to generate SIEM rule embeddings, which are then analyzed using a\nsimilarity matching algorithm to identify the top-k most similar rules. The LLM\nthen processes the rules identified, utilizing its information extraction,\nlanguage understanding, and reasoning capabilities to analyze rule similarity,\nevaluate threat coverage and performance metrics, and deliver optimized\nrecommendations for refining the rule set. By automating the rule optimization\nprocess, RuleGenie allows security teams to focus on more strategic tasks while\nenhancing the efficiency of SIEM systems and strengthening organizations'\nsecurity posture. We evaluated RuleGenie on a comprehensive set of real-world\nSIEM rule formats, including Splunk, Sigma, and AQL (Ariel query language),\ndemonstrating its platform-agnostic capabilities and adaptability across\ndiverse security infrastructures. Our experimental results show that RuleGenie\ncan effectively identify redundant rules, which in turn decreases false\npositive rates and enhances overall rule efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aRuleGenie\u7684\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u8350\u7cfb\u7edf\uff0c\u7528\u4e8e\u4f18\u5316SIEM\u89c4\u5219\u96c6\uff0c\u51cf\u5c11\u5197\u4f59\u548c\u8bef\u62a5\u3002", "motivation": "SIEM\u7cfb\u7edf\u4e2d\u5b58\u5728\u5197\u4f59\u548c\u91cd\u53e0\u89c4\u5219\uff0c\u5bfc\u81f4\u9ad8\u8bef\u62a5\u7387\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u4f18\u5316\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u6613\u51fa\u9519\u3002", "method": "\u5229\u7528Transformer\u6a21\u578b\u751f\u6210\u89c4\u5219\u5d4c\u5165\uff0c\u901a\u8fc7\u76f8\u4f3c\u6027\u5339\u914d\u7b97\u6cd5\u8bc6\u522b\u5197\u4f59\u89c4\u5219\uff0c\u5e76\u7ed3\u5408LLM\u7684\u8bed\u8a00\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4f18\u5316\u5efa\u8bae\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRuleGenie\u80fd\u6709\u6548\u8bc6\u522b\u5197\u4f59\u89c4\u5219\uff0c\u964d\u4f4e\u8bef\u62a5\u7387\u5e76\u63d0\u5347\u89c4\u5219\u6548\u7387\u3002", "conclusion": "RuleGenie\u81ea\u52a8\u5316\u4f18\u5316\u8fc7\u7a0b\uff0c\u63d0\u5347SIEM\u7cfb\u7edf\u6548\u7387\u548c\u5b89\u5168\u6027\uff0c\u5177\u6709\u5e73\u53f0\u65e0\u5173\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2505.07280", "pdf": "https://arxiv.org/pdf/2505.07280", "abs": "https://arxiv.org/abs/2505.07280", "authors": ["Navid Falah", "Behnam Yousefimehr", "Mehdi Ghatee"], "title": "Predicting Music Track Popularity by Convolutional Neural Networks on Spotify Features and Spectrogram of Audio Waveform", "categories": ["cs.SD", "cs.AI", "68T05, 68T10, 68T37", "I.2.6; I.2.1"], "comment": "12 pages, 6 figures, 4 tables", "summary": "In the digital streaming landscape, it's becoming increasingly challenging\nfor artists and industry experts to predict the success of music tracks. This\nstudy introduces a pioneering methodology that uses Convolutional Neural\nNetworks (CNNs) and Spotify data analysis to forecast the popularity of music\ntracks. Our approach takes advantage of Spotify's wide range of features,\nincluding acoustic attributes based on the spectrogram of audio waveform,\nmetadata, and user engagement metrics, to capture the complex patterns and\nrelationships that influence a track's popularity. Using a large dataset\ncovering various genres and demographics, our CNN-based model shows impressive\neffectiveness in predicting the popularity of music tracks. Additionally, we've\nconducted extensive experiments to assess the strength and adaptability of our\nmodel across different musical styles and time periods, with promising results\nyielding a 97\\% F1 score. Our study not only offers valuable insights into the\ndynamic landscape of digital music consumption but also provides the music\nindustry with advanced predictive tools for assessing and predicting the\nsuccess of music tracks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCNN\u548cSpotify\u6570\u636e\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u97f3\u4e50\u66f2\u76ee\u7684\u6d41\u884c\u5ea6\uff0c\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff08F1\u5206\u657097%\uff09\u3002", "motivation": "\u6570\u5b57\u6d41\u5a92\u4f53\u73af\u5883\u4e0b\uff0c\u9884\u6d4b\u97f3\u4e50\u66f2\u76ee\u6210\u529f\u8d8a\u6765\u8d8a\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u5e2e\u52a9\u884c\u4e1a\u8bc4\u4f30\u4f5c\u54c1\u6f5c\u529b\u3002", "method": "\u7ed3\u5408CNN\u548cSpotify\u6570\u636e\uff08\u58f0\u5b66\u7279\u5f81\u3001\u5143\u6570\u636e\u3001\u7528\u6237\u53c2\u4e0e\u5ea6\u6307\u6807\uff09\uff0c\u5206\u6790\u591a\u6d41\u6d3e\u3001\u591a\u4eba\u53e3\u7edf\u8ba1\u6570\u636e\u96c6\u3002", "result": "\u6a21\u578b\u5728\u4e0d\u540c\u97f3\u4e50\u98ce\u683c\u548c\u65f6\u95f4\u6bb5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cF1\u5206\u6570\u8fbe97%\u3002", "conclusion": "\u7814\u7a76\u4e3a\u97f3\u4e50\u884c\u4e1a\u63d0\u4f9b\u4e86\u9884\u6d4b\u5de5\u5177\uff0c\u5e76\u63ed\u793a\u4e86\u6570\u5b57\u97f3\u4e50\u6d88\u8d39\u7684\u52a8\u6001\u89c4\u5f8b\u3002"}}
{"id": "2505.06711", "pdf": "https://arxiv.org/pdf/2505.06711", "abs": "https://arxiv.org/abs/2505.06711", "authors": ["Junfan Xia", "Bin Jiang"], "title": "Efficient Parallelization of Message Passing Neural Networks", "categories": ["physics.chem-ph", "cs.LG"], "comment": "33 pages, 8 figures", "summary": "Machine learning potentials have achieved great success in accelerating\natomistic simulations. Many of them rely on local descriptors that readily\nallow parallelization. More recent message passing neural network (MPNN) models\nhave demonstrated their superior accuracy and become increasingly popular.\nHowever, parallelizing MPNN models for large-scale simulations across compute\nnodes remains a challenge, as the previously argued poor scalability with the\nnumber of MP layers and the necessity of data communication. Here, we propose\nan efficient parallel algorithm for MPNN models, in which additional data\ncommunication is minimized among local atoms only in each MP layer without\nredundant computation, thus scaling linearly with the layer number. Integrated\nwith our recursively embedded atom neural network model, this algorithm\ndemonstrates excellent strong scaling and weak scaling behaviors in several\nbenchmark systems. This approach enables massive molecular dynamics simulations\non MPNN models for hundreds of millions of atoms as fast as on strictly local\nmodels, vastly extending the applicability of the MPNN potential to an\nunprecedented scale. This general parallelization framework can empower various\nMPNN models to efficiently simulate very large and complex systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5e76\u884c\u7b97\u6cd5\uff0c\u7528\u4e8e\u6d88\u606f\u4f20\u9012\u795e\u7ecf\u7f51\u7edc\uff08MPNN\uff09\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u5176\u5728\u5927\u578b\u6a21\u62df\u4e2d\u5e76\u884c\u5316\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u6570\u636e\u901a\u4fe1\u548c\u5197\u4f59\u8ba1\u7b97\uff0c\u5b9e\u73b0\u4e86\u7ebf\u6027\u6269\u5c55\u3002\u7ed3\u5408\u9012\u5f52\u5d4c\u5165\u539f\u5b50\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u5f3a\u6269\u5c55\u548c\u5f31\u6269\u5c55\u884c\u4e3a\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u7684\u52bf\u51fd\u6570\u5728\u52a0\u901f\u539f\u5b50\u6a21\u62df\u65b9\u9762\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\uff0c\u4f46MPNN\u6a21\u578b\u5728\u5927\u89c4\u6a21\u6a21\u62df\u4e2d\u7684\u5e76\u884c\u5316\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u4e3b\u8981\u7531\u4e8e\u5c42\u6570\u589e\u52a0\u65f6\u7684\u6269\u5c55\u6027\u95ee\u9898\u53ca\u6570\u636e\u901a\u4fe1\u7684\u5fc5\u8981\u6027\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u56e2\u961f\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u5e76\u884c\u7b97\u6cd5\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5e76\u884c\u7b97\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u672c\u5730\u539f\u5b50\u95f4\u7684\u989d\u5916\u6570\u636e\u901a\u4fe1\uff0c\u5e76\u5728\u6bcf\u4e00\u6d88\u606f\u4f20\u9012\u5c42\u4e2d\u907f\u514d\u5197\u4f59\u8ba1\u7b97\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u7ebf\u6027\u6269\u5c55\u3002\u8be5\u65b9\u6cd5\u4e0e\u9012\u5f52\u5d4c\u5165\u539f\u5b50\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7ed3\u5408\u4f7f\u7528\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u5f3a\u6269\u5c55\u548c\u5f31\u6269\u5c55\u884c\u4e3a\uff0c\u80fd\u591f\u7528\u4e8e\u6570\u4ebf\u539f\u5b50\u7684\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\uff0c\u901f\u5ea6\u4e0e\u4e25\u683c\u672c\u5730\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "\u8be5\u5e76\u884c\u5316\u6846\u67b6\u6781\u5927\u5730\u6269\u5c55\u4e86MPNN\u6a21\u578b\u7684\u9002\u7528\u8303\u56f4\uff0c\u4f7f\u5176\u80fd\u591f\u9ad8\u6548\u6a21\u62df\u975e\u5e38\u5e9e\u5927\u548c\u590d\u6742\u7684\u7cfb\u7edf\u3002"}}
{"id": "2505.07286", "pdf": "https://arxiv.org/pdf/2505.07286", "abs": "https://arxiv.org/abs/2505.07286", "authors": ["Keyue Qiu", "Yuxuan Song", "Zhehuan Fan", "Peidong Liu", "Zhe Zhang", "Mingyue Zheng", "Hao Zhou", "Wei-Ying Ma"], "title": "Piloting Structure-Based Drug Design via Modality-Specific Optimal Schedule", "categories": ["q-bio.BM", "cs.AI", "cs.LG"], "comment": "Accepted to ICML 2025", "summary": "Structure-Based Drug Design (SBDD) is crucial for identifying bioactive\nmolecules. Recent deep generative models are faced with challenges in geometric\nstructure modeling. A major bottleneck lies in the twisted probability path of\nmulti-modalities -- continuous 3D positions and discrete 2D topologies -- which\njointly determine molecular geometries. By establishing the fact that noise\nschedules decide the Variational Lower Bound (VLB) for the twisted probability\npath, we propose VLB-Optimal Scheduling (VOS) strategy in this under-explored\narea, which optimizes VLB as a path integral for SBDD. Our model effectively\nenhances molecular geometries and interaction modeling, achieving\nstate-of-the-art PoseBusters passing rate of 95.9% on CrossDock, more than 10%\nimprovement upon strong baselines, while maintaining high affinities and robust\nintramolecular validity evaluated on held-out test set.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVOS\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u4f18\u5316\u53d8\u5206\u4e0b\u754c\uff08VLB\uff09\u6765\u89e3\u51b3\u5206\u5b50\u51e0\u4f55\u5efa\u6a21\u4e2d\u7684\u6982\u7387\u8def\u5f84\u626d\u66f2\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5b50\u51e0\u4f55\u7ed3\u6784\u548c\u76f8\u4e92\u4f5c\u7528\u5efa\u6a21\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5728\u5904\u7406\u5206\u5b50\u51e0\u4f55\u7ed3\u6784\u7684\u591a\u6a21\u6001\uff08\u8fde\u7eed3D\u4f4d\u7f6e\u548c\u79bb\u65632D\u62d3\u6251\uff09\u6982\u7387\u8def\u5f84\u626d\u66f2\u95ee\u9898\u4e0a\u5b58\u5728\u74f6\u9888\uff0c\u5f71\u54cd\u4e86\u836f\u7269\u8bbe\u8ba1\u7684\u6548\u679c\u3002", "method": "\u8bba\u6587\u63d0\u51faVLB-Optimal Scheduling\uff08VOS\uff09\u7b56\u7565\uff0c\u901a\u8fc7\u4f18\u5316VLB\u4f5c\u4e3a\u8def\u5f84\u79ef\u5206\u6765\u89e3\u51b3\u626d\u66f2\u6982\u7387\u8def\u5f84\u95ee\u9898\uff0c\u63d0\u5347\u5206\u5b50\u51e0\u4f55\u5efa\u6a21\u3002", "result": "\u5728CrossDock\u6570\u636e\u96c6\u4e0a\uff0cPoseBusters\u901a\u8fc7\u7387\u8fbe\u5230\u4e8695.9%\uff0c\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e8610%\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u4eb2\u548c\u529b\u548c\u5206\u5b50\u5185\u6709\u6548\u6027\u3002", "conclusion": "VOS\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5b50\u51e0\u4f55\u5efa\u6a21\u4e2d\u7684\u6982\u7387\u8def\u5f84\u626d\u66f2\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u836f\u7269\u8bbe\u8ba1\u7684\u6548\u679c\u3002"}}
{"id": "2505.06756", "pdf": "https://arxiv.org/pdf/2505.06756", "abs": "https://arxiv.org/abs/2505.06756", "authors": ["Michael W. Trosset", "Kaiyi Tan", "Minh Tang", "Carey E. Priebe"], "title": "Out-of-Sample Embedding with Proximity Data: Projection versus Restricted Reconstruction", "categories": ["stat.ML", "cs.LG", "stat.CO"], "comment": "19 pages, 2 figures", "summary": "The problem of using proximity (similarity or dissimilarity) data for the\npurpose of \"adding a point to a vector diagram\" was first studied by J.C. Gower\nin 1968. Since then, a number of methods -- mostly kernel methods -- have been\nproposed for solving what has come to be called the problem of *out-of-sample\nembedding*. We survey the various kernel methods that we have encountered and\nshow that each can be derived from one or the other of two competing\nstrategies: *projection* or *restricted reconstruction*. Projection can be\nanalogized to a well-known formula for adding a point to a principal component\nanalysis. Restricted reconstruction poses a different challenge: how to best\napproximate redoing the entire multivariate analysis while holding fixed the\nvector diagram that was previously obtained. This strategy results in a\nnonlinear optimization problem that can be simplified to a unidimensional\nsearch. Various circumstances may warrant either projection or restricted\nreconstruction.", "AI": {"tldr": "\u8bba\u6587\u7efc\u8ff0\u4e86\u6838\u65b9\u6cd5\u89e3\u51b3\u6837\u672c\u5916\u5d4c\u5165\u95ee\u9898\u7684\u4e24\u79cd\u7b56\u7565\uff1a\u6295\u5f71\u548c\u53d7\u9650\u91cd\u5efa\uff0c\u5e76\u6bd4\u8f83\u4e86\u5b83\u4eec\u7684\u9002\u7528\u573a\u666f\u3002", "motivation": "\u7814\u7a76\u6837\u672c\u5916\u5d4c\u5165\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u4ece\u6838\u65b9\u6cd5\u7684\u89d2\u5ea6\u51fa\u53d1\uff0c\u63a2\u8ba8\u5982\u4f55\u6709\u6548\u5c06\u65b0\u6570\u636e\u70b9\u5d4c\u5165\u5df2\u6709\u5411\u91cf\u56fe\u4e2d\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u7b56\u7565\u8fdb\u884c\u6837\u672c\u5916\u5d4c\u5165\uff1a\u6295\u5f71\uff08\u7c7b\u6bd4PCA\uff09\u548c\u53d7\u9650\u91cd\u5efa\uff08\u975e\u7ebf\u6027\u4f18\u5316\u7b80\u5316\u4e3a\u5355\u7ef4\u641c\u7d22\uff09\u3002", "result": "\u5c55\u793a\u4e86\u6bcf\u79cd\u7b56\u7565\u7684\u6838\u65b9\u6cd5\u5b9e\u73b0\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u60c5\u51b5\u4e0b\u9009\u62e9\u6295\u5f71\u6216\u53d7\u9650\u91cd\u5efa\u7684\u4f18\u52bf\u3002", "conclusion": "\u6837\u672c\u5916\u5d4c\u5165\u95ee\u9898\u53ef\u901a\u8fc7\u6295\u5f71\u6216\u53d7\u9650\u91cd\u5efa\u7b56\u7565\u89e3\u51b3\uff0c\u5177\u4f53\u9009\u62e9\u53d6\u51b3\u4e8e\u5e94\u7528\u573a\u666f\u548c\u9700\u6c42\u3002"}}
{"id": "2505.06771", "pdf": "https://arxiv.org/pdf/2505.06771", "abs": "https://arxiv.org/abs/2505.06771", "authors": ["Shalin Anand Jain", "Jiazhen Liu", "Siva Kailas", "Harish Ravichandar"], "title": "JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes", "categories": ["cs.RO", "cs.LG", "cs.MA"], "comment": "22 pages, 14 figures, 10 tables", "summary": "Multi-agent reinforcement learning (MARL) has emerged as a promising solution\nfor learning complex and scalable coordination behaviors in multi-robot\nsystems. However, established MARL platforms (e.g., SMAC and MPE) lack robotics\nrelevance and hardware deployment, leaving multi-robot learning researchers to\ndevelop bespoke environments and hardware testbeds dedicated to the development\nand evaluation of their individual contributions. The Multi-Agent RL Benchmark\nand Learning Environment for the Robotarium (MARBLER) is an exciting recent\nstep in providing a standardized robotics-relevant platform for MARL, by\nbridging the Robotarium testbed with existing MARL software infrastructure.\nHowever, MARBLER lacks support for parallelization and GPU/TPU execution,\nmaking the platform prohibitively slow compared to modern MARL environments and\nhindering adoption. We contribute JaxRobotarium, a Jax-powered end-to-end\nsimulation, learning, deployment, and benchmarking platform for the Robotarium.\nJaxRobotarium enables rapid training and deployment of multi-robot\nreinforcement learning (MRRL) policies with realistic robot dynamics and safety\nconstraints, supporting both parallelization and hardware acceleration. Our\ngeneralizable learning interface provides an easy-to-use integration with SOTA\nMARL libraries (e.g., JaxMARL). In addition, JaxRobotarium includes eight\nstandardized coordination scenarios, including four novel scenarios that bring\nestablished MARL benchmark tasks (e.g., RWARE and Level-Based Foraging) to a\nrealistic robotics setting. We demonstrate that JaxRobotarium retains high\nsimulation fidelity while achieving dramatic speedups over baseline (20x in\ntraining and 150x in simulation), and provides an open-access sim-to-real\nevaluation pipeline through the Robotarium testbed, accelerating and\ndemocratizing access to multi-robot learning research and evaluation.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86JaxRobotarium\uff0c\u4e00\u4e2a\u57fa\u4e8eJax\u7684\u591a\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u5e73\u53f0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709MARL\u5e73\u53f0\uff08\u5982MARBLER\uff09\u7f3a\u4e4f\u5e76\u884c\u5316\u548c\u786c\u4ef6\u52a0\u901f\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u548c\u4eff\u771f\u901f\u5ea6\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u5728\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u73b0\u6709\u5e73\u53f0\uff08\u5982SMAC\u548cMPE\uff09\u7f3a\u4e4f\u673a\u5668\u4eba\u76f8\u5173\u6027\u548c\u786c\u4ef6\u90e8\u7f72\u652f\u6301\uff0c\u4e14MARBLER\u56e0\u4e0d\u652f\u6301\u5e76\u884c\u5316\u548cGPU/TPU\u6267\u884c\u5bfc\u81f4\u901f\u5ea6\u8fc7\u6162\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u63d0\u51faJaxRobotarium\uff0c\u4e00\u4e2a\u57fa\u4e8eJax\u7684\u7aef\u5230\u7aef\u4eff\u771f\u3001\u5b66\u4e60\u3001\u90e8\u7f72\u548c\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u652f\u6301\u5e76\u884c\u5316\u548c\u786c\u4ef6\u52a0\u901f\uff0c\u5e76\u4e0e\u73b0\u6709MARL\u5e93\uff08\u5982JaxMARL\uff09\u65e0\u7f1d\u96c6\u6210\uff0c\u540c\u65f6\u5f15\u5165\u516b\u79cd\u6807\u51c6\u5316\u534f\u8c03\u573a\u666f\uff08\u5305\u62ec\u56db\u79cd\u65b0\u573a\u666f\uff09\u3002", "result": "JaxRobotarium\u5728\u4fdd\u6301\u9ad8\u4eff\u771f\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u534720\u500d\uff0c\u4eff\u771f\u901f\u5ea6\u63d0\u5347150\u500d\uff0c\u5e76\u901a\u8fc7Robotarium\u6d4b\u8bd5\u5e8a\u63d0\u4f9b\u5f00\u6e90\u7684\u6a21\u62df\u5230\u771f\u5b9e\u7684\u8bc4\u4f30\u6d41\u7a0b\u3002", "conclusion": "JaxRobotarium\u663e\u8457\u52a0\u901f\u4e86\u591a\u673a\u5668\u4eba\u5b66\u4e60\u7684\u7814\u7a76\u4e0e\u8bc4\u4f30\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u6613\u7528\u7684\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u6c11\u4e3b\u5316\u53d1\u5c55\u3002"}}
{"id": "2505.07294", "pdf": "https://arxiv.org/pdf/2505.07294", "abs": "https://arxiv.org/abs/2505.07294", "authors": ["Tong Zhang", "Boyuan Zheng", "Ruiqian Nai", "Yingdong Hu", "Yen-Jen Wang", "Geng Chen", "Fanqi Lin", "Jiongye Li", "Chuye Hong", "Koushil Sreenath", "Yang Gao"], "title": "HuB: Learning Extreme Humanoid Balance", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "comment": "Project website: https://hub-robot.github.io", "summary": "The human body demonstrates exceptional motor capabilities-such as standing\nsteadily on one foot or performing a high kick with the leg raised over 1.5\nmeters-both requiring precise balance control. While recent research on\nhumanoid control has leveraged reinforcement learning to track human motions\nfor skill acquisition, applying this paradigm to balance-intensive tasks\nremains challenging. In this work, we identify three key obstacles: instability\nfrom reference motion errors, learning difficulties due to morphological\nmismatch, and the sim-to-real gap caused by sensor noise and unmodeled\ndynamics. To address these challenges, we propose HuB (Humanoid Balance), a\nunified framework that integrates reference motion refinement, balance-aware\npolicy learning, and sim-to-real robustness training, with each component\ntargeting a specific challenge. We validate our approach on the Unitree G1\nhumanoid robot across challenging quasi-static balance tasks, including extreme\nsingle-legged poses such as Swallow Balance and Bruce Lee's Kick. Our policy\nremains stable even under strong physical disturbances-such as a forceful\nsoccer strike-while baseline methods consistently fail to complete these tasks.\nProject website: https://hub-robot.github.io", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86HuB\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u8003\u8fd0\u52a8\u4f18\u5316\u3001\u5e73\u8861\u611f\u77e5\u7b56\u7565\u5b66\u4e60\u548c\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u9c81\u68d2\u6027\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u9ad8\u96be\u5ea6\u5e73\u8861\u4efb\u52a1\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u5f62\u6001\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9645\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u5728\u9700\u8981\u9ad8\u7cbe\u5ea6\u5e73\u8861\u63a7\u5236\u7684\u52a8\u4f5c\uff08\u5982\u5355\u817f\u7ad9\u7acb\u6216\u9ad8\u8e22\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u56e0\u53c2\u8003\u8fd0\u52a8\u8bef\u5dee\u3001\u5f62\u6001\u4e0d\u5339\u914d\u548c\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u800c\u96be\u4ee5\u9002\u7528\u3002", "method": "\u63d0\u51faHuB\u6846\u67b6\uff0c\u6574\u5408\u4e86\u53c2\u8003\u8fd0\u52a8\u4f18\u5316\u3001\u5e73\u8861\u611f\u77e5\u7b56\u7565\u5b66\u4e60\u548c\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u9c81\u68d2\u6027\u8bad\u7ec3\u4e09\u90e8\u5206\uff0c\u5206\u522b\u9488\u5bf9\u4e0a\u8ff0\u6311\u6218\u3002", "result": "\u5728Unitree G1\u673a\u5668\u4eba\u4e0a\u6210\u529f\u5b8c\u6210\u9ad8\u96be\u5ea6\u5e73\u8861\u4efb\u52a1\uff08\u5982\u201c\u71d5\u5b50\u5e73\u8861\u201d\u548c\u674e\u5c0f\u9f99\u5f0f\u9ad8\u8e22\uff09\uff0c\u5373\u4f7f\u5728\u5f3a\u7269\u7406\u5e72\u6270\u4e0b\u4ecd\u4fdd\u6301\u7a33\u5b9a\uff0c\u800c\u57fa\u51c6\u65b9\u6cd5\u5747\u5931\u8d25\u3002", "conclusion": "HuB\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u6781\u7aef\u5e73\u8861\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u89e3\u51b3\u590d\u6742\u8fd0\u52a8\u63a7\u5236\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.06774", "pdf": "https://arxiv.org/pdf/2505.06774", "abs": "https://arxiv.org/abs/2505.06774", "authors": ["Ammar Daskin"], "title": "Quantum RNNs and LSTMs Through Entangling and Disentangling Power of Unitary Transformations", "categories": ["quant-ph", "cs.LG"], "comment": "the simulation code can be downloaded from\n  https://github.com/adaskin/quantum-lstm", "summary": "In this paper, we discuss how quantum recurrent neural networks (RNNs) and\ntheir enhanced version, long short-term memory (LSTM) networks, can be modeled\nusing the core ideas presented in Ref.[1], where the entangling and\ndisentangling power of unitary transformations is investigated. In particular,\nwe interpret entangling and disentangling power as information retention and\nforgetting mechanisms in LSTMs. Therefore, entanglement becomes a key component\nof the optimization (training) process. We believe that, by leveraging prior\nknowledge of the entangling power of unitaries, the proposed quantum-classical\nframework can guide and help to design better-parameterized quantum circuits\nfor various real-world applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u7528\u91cf\u5b50\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\u53ca\u5176\u589e\u5f3a\u7248\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u7f51\u7edc\uff0c\u5229\u7528\u9149\u53d8\u6362\u7684\u7ea0\u7f20\u548c\u89e3\u7ea0\u7f20\u80fd\u529b\u6765\u4f18\u5316\u91cf\u5b50\u7535\u8def\u8bbe\u8ba1\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u91cf\u5b50\u4e0e\u7ecf\u5178\u6846\u67b6\u7684\u7ed3\u5408\uff0c\u5229\u7528\u9149\u53d8\u6362\u7684\u7ea0\u7f20\u548c\u89e3\u7ea0\u7f20\u80fd\u529b\u6765\u63d0\u5347\u91cf\u5b50\u7535\u8def\u7684\u53c2\u6570\u5316\u8bbe\u8ba1\uff0c\u4ee5\u9002\u7528\u4e8e\u66f4\u591a\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002", "method": "\u8bba\u6587\u91c7\u7528\u91cf\u5b50RNN\u548cLSTM\u7f51\u7edc\uff0c\u5c06\u9149\u53d8\u6362\u7684\u7ea0\u7f20\u548c\u89e3\u7ea0\u7f20\u80fd\u529b\u89e3\u91ca\u4e3a\u4fe1\u606f\u7684\u4fdd\u7559\u4e0e\u9057\u5fd8\u673a\u5236\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u4f18\u5316\u91cf\u5b50\u7535\u8def\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u7814\u7a76\u6210\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u91cf\u5b50-\u7ecf\u5178\u6846\u67b6\u53ef\u4ee5\u6709\u6548\u6307\u5bfc\u8bbe\u8ba1\u66f4\u597d\u7684\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u91cf\u5b50\u4e0e\u7ecf\u5178\u65b9\u6cd5\uff0c\u5229\u7528\u9149\u53d8\u6362\u7684\u7ea0\u7f20\u7279\u6027\u4f18\u5316\u91cf\u5b50\u7535\u8def\u8bbe\u8ba1\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2505.06800", "pdf": "https://arxiv.org/pdf/2505.06800", "abs": "https://arxiv.org/abs/2505.06800", "authors": ["Jairon H. N. Batista", "Fl\u00e1vio B. Gon\u00e7alves", "Yuri F. Saporito", "Rodrigo S. Targino"], "title": "Reverse-BSDE Monte Carlo", "categories": ["stat.ML", "cs.LG", "math.PR"], "comment": null, "summary": "Recently, there has been a growing interest in generative models based on\ndiffusions driven by the empirical robustness of these methods in generating\nhigh-dimensional photorealistic images and the possibility of using the vast\nexisting toolbox of stochastic differential equations. %This remarkable ability\nmay stem from their capacity to model and generate multimodal distributions. In\nthis work, we offer a novel perspective on the approach introduced in Song et\nal. (2021), shifting the focus from a \"learning\" problem to a \"sampling\"\nproblem. To achieve this, we reformulate the equations governing\ndiffusion-based generative models as a Forward-Backward Stochastic Differential\nEquation (FBSDE), which avoids the well-known issue of pre-estimating the\ngradient of the log target density. The solution of this FBSDE is proved to be\nunique using non-standard techniques. Additionally, we propose a numerical\nsolution to this problem, leveraging on Deep Learning techniques. This\nreformulation opens new pathways for sampling multidimensional distributions\nwith densities known up to a normalization constant, a problem frequently\nencountered in Bayesian statistics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89d2\uff0c\u5c06\u6269\u6563\u6a21\u578b\u7684\u201c\u5b66\u4e60\u201d\u95ee\u9898\u8f6c\u5316\u4e3a\u201c\u91c7\u6837\u201d\u95ee\u9898\uff0c\u5229\u7528FBSDE\u6846\u67b6\u907f\u514d\u9884\u4f30\u8ba1\u76ee\u6807\u5bc6\u5ea6\u68af\u5ea6\u7684\u96be\u9898\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6570\u503c\u89e3\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u7ef4\u903c\u771f\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u9700\u9884\u4f30\u8ba1\u76ee\u6807\u5bc6\u5ea6\u68af\u5ea6\uff0c\u8ba1\u7b97\u590d\u6742\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7FBSDE\u6846\u67b6\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u5c06\u6269\u6563\u6a21\u578b\u91cd\u65b0\u8868\u8ff0\u4e3aFBSDE\uff0c\u5229\u7528\u975e\u6807\u51c6\u6280\u672f\u8bc1\u660e\u89e3\u7684\u552f\u4e00\u6027\uff0c\u5e76\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u8bbe\u8ba1\u6570\u503c\u89e3\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u907f\u514d\u4e86\u68af\u5ea6\u9884\u4f30\u8ba1\uff0c\u4e3a\u591a\u7ef4\u5206\u5e03\u91c7\u6837\uff08\u5982\u8d1d\u53f6\u65af\u7edf\u8ba1\u4e2d\u5e38\u89c1\u95ee\u9898\uff09\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "conclusion": "FBSDE\u6846\u67b6\u4e3a\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7406\u8bba\u4e25\u8c28\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u62d3\u5c55\u4e86\u5176\u5728\u590d\u6742\u5206\u5e03\u91c7\u6837\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.07317", "pdf": "https://arxiv.org/pdf/2505.07317", "abs": "https://arxiv.org/abs/2505.07317", "authors": ["Ashmita Sampatsing", "Sophie Vos", "Emma Beauxis-Aussalet", "Justus Bogner"], "title": "How Do Companies Manage the Environmental Sustainability of AI? An Interview Study About Green AI Efforts and Regulations", "categories": ["cs.CY", "cs.AI"], "comment": "Accepted for publication at the 11th International Conference on ICT\n  for Sustainability (ICT4S'25), see https://conf.researchr.org/home/ict4s-2025", "summary": "With the ever-growing adoption of artificial intelligence (AI), AI-based\nsoftware and its negative impact on the environment are no longer negligible,\nand studying and mitigating this impact has become a critical area of research.\nHowever, it is currently unclear which role environmental sustainability plays\nduring AI adoption in industry and how AI regulations influence Green AI\npractices and decision-making in industry. We therefore aim to investigate the\nGreen AI perception and management of industry practitioners. To this end, we\nconducted a total of 11 interviews with participants from 10 different\norganizations that adopted AI-based software. The interviews explored three\nmain themes: AI adoption, current efforts in mitigating the negative\nenvironmental impact of AI, and the influence of the EU AI Act and the\nCorporate Sustainability Reporting Directive (CSRD). Our findings indicate that\n9 of 11 participants prioritized business efficiency during AI adoption, with\nminimal consideration of environmental sustainability. Monitoring and\nmitigation of AI's environmental impact were very limited. Only one participant\nmonitored negative environmental effects. Regarding applied mitigation\npractices, six participants reported no actions, with the others sporadically\nmentioning techniques like prompt engineering, relying on smaller models, or\nnot overusing AI. Awareness and compliance with the EU AI Act are low, with\nonly one participant reporting on its influence, while the CSRD drove\nsustainability reporting efforts primarily in larger companies. All in all, our\nfindings reflect a lack of urgency and priority for sustainable AI among these\ncompanies. We suggest that current regulations are not very effective, which\nhas implications for policymakers. Additionally, there is a need to raise\nindustry awareness, but also to provide user-friendly techniques and tools for\nGreen AI practices.", "AI": {"tldr": "\u6587\u7ae0\u7814\u7a76\u4e86AI\u5728\u5de5\u4e1a\u754c\u91c7\u7528\u65f6\u5bf9\u73af\u5883\u53ef\u6301\u7eed\u6027\u7684\u8003\u91cf\uff0c\u53d1\u73b0\u591a\u6570\u4f01\u4e1a\u66f4\u5173\u6ce8\u4e1a\u52a1\u6548\u7387\u800c\u975e\u73af\u4fdd\uff0c\u73b0\u6709\u6cd5\u89c4\u5982EU AI Act\u548cCSRD\u6548\u679c\u6709\u9650\u3002", "motivation": "\u968f\u7740AI\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u73af\u5883\u5f71\u54cd\u4e0d\u5bb9\u5ffd\u89c6\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5de5\u4e1a\u754c\u5bf9Green AI\u7684\u8ba4\u77e5\u4e0e\u7ba1\u7406\u73b0\u72b6\uff0c\u4ee5\u53ca\u6cd5\u89c4\u5bf9\u5176\u5b9e\u8df5\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc711\u5bb6\u91c7\u7528AI\u7684\u4f01\u4e1a\u8bbf\u8c08\uff0c\u63a2\u8ba8AI\u91c7\u7528\u3001\u73af\u4fdd\u63aa\u65bd\u53ca\u6cd5\u89c4\uff08\u5982EU AI Act\u548cCSRD\uff09\u7684\u5f71\u54cd\u3002", "result": "\u591a\u6570\u4f01\u4e1a\u4f18\u5148\u4e1a\u52a1\u6548\u7387\uff0c\u73af\u4fdd\u8003\u91cf\u6781\u5c11\uff1b\u4ec5\u5c11\u6570\u91c7\u53d6\u51cf\u6392\u63aa\u65bd\uff1b\u6cd5\u89c4\u610f\u8bc6\u8584\u5f31\uff0c\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u9700\u63d0\u5347\u884c\u4e1a\u73af\u4fdd\u610f\u8bc6\uff0c\u5f00\u53d1\u6613\u7528\u5de5\u5177\uff0c\u5e76\u6539\u8fdb\u653f\u7b56\u4ee5\u63a8\u52a8\u53ef\u6301\u7eedAI\u5b9e\u8df5\u3002"}}
{"id": "2505.06805", "pdf": "https://arxiv.org/pdf/2505.06805", "abs": "https://arxiv.org/abs/2505.06805", "authors": ["Tommaso Giovannelli", "Griffin Dean Kent", "Luis Nunes Vicente"], "title": "A stochastic gradient method for trilevel optimization", "categories": ["math.OC", "cs.LG", "stat.ML"], "comment": null, "summary": "With the success that the field of bilevel optimization has seen in recent\nyears, similar methodologies have started being applied to solving more\ndifficult applications that arise in trilevel optimization. At the helm of\nthese applications are new machine learning formulations that have been\nproposed in the trilevel context and, as a result, efficient and theoretically\nsound stochastic methods are required. In this work, we propose the first-ever\nstochastic gradient descent method for solving unconstrained trilevel\noptimization problems and provide a convergence theory that covers all forms of\ninexactness of the trilevel adjoint gradient, such as the inexact solutions of\nthe middle-level and lower-level problems, inexact computation of the trilevel\nadjoint formula, and noisy estimates of the gradients, Hessians, Jacobians, and\ntensors of third-order derivatives involved. We also demonstrate the promise of\nour approach by providing numerical results on both synthetic trilevel problems\nand trilevel formulations for hyperparameter adversarial tuning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u89e3\u51b3\u65e0\u7ea6\u675f\u4e09\u91cd\u4f18\u5316\u95ee\u9898\u7684\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u8986\u76d6\u6240\u6709\u5f62\u5f0f\u7684\u8fd1\u4f3c\u68af\u5ea6\u7684\u6536\u655b\u7406\u8bba\u3002", "motivation": "\u968f\u7740\u53cc\u5c42\u4f18\u5316\u7684\u6210\u529f\uff0c\u7c7b\u4f3c\u65b9\u6cd5\u5f00\u59cb\u5e94\u7528\u4e8e\u66f4\u590d\u6742\u7684\u4e09\u91cd\u4f18\u5316\u95ee\u9898\u3002\u65b0\u63d0\u51fa\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\u9700\u8981\u9ad8\u6548\u4e14\u7406\u8bba\u4e25\u8c28\u7684\u968f\u673a\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u6cd5\u5904\u7406\u4e09\u91cd\u4f18\u5316\u95ee\u9898\uff0c\u5141\u8bb8\u4e2d\u95f4\u5c42\u548c\u4e0b\u5c42\u95ee\u9898\u7684\u8fd1\u4f3c\u89e3\uff0c\u4ee5\u53ca\u68af\u5ea6\u3001Hessian\u77e9\u9635\u3001Jacobian\u77e9\u9635\u548c\u4e09\u9636\u5bfc\u6570\u5f20\u91cf\u7684\u566a\u58f0\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u4e09\u91cd\u4f18\u5316\u95ee\u9898\u548c\u8d85\u53c2\u6570\u5bf9\u6297\u8c03\u4f18\u7684\u4e09\u91cd\u95ee\u9898\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u9996\u6b21\u63d0\u51fa\u7684\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\u5728\u4e09\u91cd\u4f18\u5316\u4e2d\u662f\u6709\u6548\u7684\uff0c\u4e14\u5176\u6536\u655b\u7406\u8bba\u5177\u6709\u666e\u9002\u6027\u3002"}}
{"id": "2505.06825", "pdf": "https://arxiv.org/pdf/2505.06825", "abs": "https://arxiv.org/abs/2505.06825", "authors": ["Thien Nhan Vo"], "title": "Active Learning for Multi-class Image Classification", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "A principle bottleneck in image classification is the large number of\ntraining examples needed to train a classifier. Using active learning, we can\nreduce the number of training examples to teach a CNN classifier by\nstrategically selecting examples. Assigning values to image examples using\ndifferent uncertainty metrics allows the model to identify and select\nhigh-value examples in a smaller training set size. We demonstrate results for\ndigit recognition and fruit classification on the MNIST and Fruits360 data\nsets. We formally compare results for four different uncertainty metrics.\nFinally, we observe active learning is also effective on simpler (binary)\nclassification tasks, but marked improvement from random sampling is more\nevident on more difficult tasks. We show active learning is a viable algorithm\nfor image classification problems.", "AI": {"tldr": "Active learning can reduce the number of training examples needed for image classification by strategically selecting high-value examples using uncertainty metrics, validated on MNIST and Fruits360 datasets.", "motivation": "To address the bottleneck of needing numerous training examples in image classification by leveraging active learning.", "method": "Employed active learning with CNN classifiers, using four uncertainty metrics to select high-value training examples from MNIST and Fruits360 datasets.", "result": "Demonstrated that active learning reduces required training examples, with more significant improvements on complex tasks compared to random sampling.", "conclusion": "Active learning is effective for image classification, especially in challenging tasks, proving its viability."}}
{"id": "2505.07336", "pdf": "https://arxiv.org/pdf/2505.07336", "abs": "https://arxiv.org/abs/2505.07336", "authors": ["Zhixuan Zhang", "Xiaopeng Li", "Qi Liu"], "title": "SAEN-BGS: Energy-Efficient Spiking AutoEncoder Network for Background Subtraction", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by Pattern Recognition", "summary": "Background subtraction (BGS) is utilized to detect moving objects in a video\nand is commonly employed at the onset of object tracking and human recognition\nprocesses. Nevertheless, existing BGS techniques utilizing deep learning still\nencounter challenges with various background noises in videos, including\nvariations in lighting, shifts in camera angles, and disturbances like air\nturbulence or swaying trees. To address this problem, we design a spiking\nautoencoder network, termed SAEN-BGS, based on noise resilience and\ntime-sequence sensitivity of spiking neural networks (SNNs) to enhance the\nseparation of foreground and background. To eliminate unnecessary background\nnoise and preserve the important foreground elements, we begin by creating the\ncontinuous spiking conv-and-dconv block, which serves as the fundamental\nbuilding block for the decoder in SAEN-BGS. Moreover, in striving for enhanced\nenergy efficiency, we introduce a novel self-distillation spiking supervised\nlearning method grounded in ANN-to-SNN frameworks, resulting in decreased power\nconsumption. In extensive experiments conducted on CDnet-2014 and DAVIS-2016\ndatasets, our approach demonstrates superior segmentation performance relative\nto other baseline methods, even when challenged by complex scenarios with\ndynamic backgrounds.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u566a\u58f0\u6297\u5e72\u6270\u548c\u65f6\u95f4\u5e8f\u5217\u654f\u611f\u7684SAEN-BGS\u80cc\u666f\u51cf\u9664\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u84b8\u998f\u76d1\u7763\u5b66\u4e60\u63d0\u5347\u80fd\u6548\uff0c\u5728\u590d\u6742\u52a8\u6001\u80cc\u666f\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u7684\u80cc\u666f\u51cf\u9664\u6280\u672f\u5728\u89c6\u9891\u4e2d\u5b58\u5728\u5149\u7167\u53d8\u5316\u3001\u76f8\u673a\u89d2\u5ea6\u53d8\u5316\u7b49\u566a\u58f0\u5e72\u6270\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8109\u51b2\u81ea\u7f16\u7801\u5668\u7f51\u7edc\uff08SAEN-BGS\uff09\uff0c\u7ed3\u5408\u8fde\u7eed\u7684\u8109\u51b2\u5377\u79ef\u548c\u53cd\u5377\u79ef\u6a21\u5757\uff0c\u5e76\u5f15\u5165\u4e86\u57fa\u4e8eANN-to-SNN\u6846\u67b6\u7684\u81ea\u84b8\u998f\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5728CDnet-2014\u548cDAVIS-2016\u6570\u636e\u96c6\u4e0a\uff0cSAEN-BGS\u5728\u52a8\u6001\u80cc\u666f\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SAEN-BGS\u901a\u8fc7\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u566a\u58f0\u6297\u5e72\u6270\u80fd\u529b\u548c\u65f6\u95f4\u5e8f\u5217\u654f\u611f\u6027\uff0c\u6709\u6548\u63d0\u5347\u4e86\u80cc\u666f\u51cf\u9664\u7684\u9c81\u68d2\u6027\u548c\u80fd\u6548\u3002"}}
{"id": "2505.07339", "pdf": "https://arxiv.org/pdf/2505.07339", "abs": "https://arxiv.org/abs/2505.07339", "authors": ["Gabriel Lima", "Nina Grgi\u0107-Hla\u010da", "Markus Langer", "Yixin Zou"], "title": "Laypeople's Attitudes Towards Fair, Affirmative, and Discriminatory Decision-Making Algorithms", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Affirmative algorithms have emerged as a potential answer to algorithmic\ndiscrimination, seeking to redress past harms and rectify the source of\nhistorical injustices. We present the results of two experiments ($N$$=$$1193$)\ncapturing laypeople's perceptions of affirmative algorithms -- those which\nexplicitly prioritize the historically marginalized -- in hiring and criminal\njustice. We contrast these opinions about affirmative algorithms with folk\nattitudes towards algorithms that prioritize the privileged (i.e.,\ndiscriminatory) and systems that make decisions independently of demographic\ngroups (i.e., fair). We find that people -- regardless of their political\nleaning and identity -- view fair algorithms favorably and denounce\ndiscriminatory systems. In contrast, we identify disagreements concerning\naffirmative algorithms: liberals and racial minorities rate affirmative systems\nas positively as their fair counterparts, whereas conservatives and those from\nthe dominant racial group evaluate affirmative algorithms as negatively as\ndiscriminatory systems. We identify a source of these divisions: people have\nvarying beliefs about who (if anyone) is marginalized, shaping their views of\naffirmative algorithms. We discuss the possibility of bridging these\ndisagreements to bring people together towards affirmative algorithms.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u4e24\u9879\u5b9e\u9a8c\uff08N=1193\uff09\u63a2\u8ba8\u4e86\u516c\u4f17\u5bf9\u2018\u5e73\u6743\u7b97\u6cd5\u2019\uff08\u4f18\u5148\u8003\u8651\u5386\u53f2\u4e0a\u8fb9\u7f18\u5316\u7fa4\u4f53\uff09\u7684\u770b\u6cd5\uff0c\u53d1\u73b0\u4eba\u4eec\u5bf9\u516c\u5e73\u7b97\u6cd5\u6301\u6b63\u9762\u6001\u5ea6\uff0c\u53cd\u5bf9\u6b67\u89c6\u6027\u7cfb\u7edf\uff0c\u4f46\u5bf9\u5e73\u6743\u7b97\u6cd5\u7684\u8bc4\u4ef7\u5b58\u5728\u653f\u6cbb\u7acb\u573a\u548c\u79cd\u65cf\u80cc\u666f\u7684\u5206\u6b67\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u516c\u4f17\u5bf9\u65e8\u5728\u7ea0\u6b63\u5386\u53f2\u4e0d\u516c\u7684\u5e73\u6743\u7b97\u6cd5\u7684\u63a5\u53d7\u5ea6\uff0c\u4ee5\u53ca\u8fd9\u79cd\u6001\u5ea6\u5982\u4f55\u53d7\u653f\u6cbb\u503e\u5411\u548c\u79cd\u65cf\u8eab\u4efd\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u4e24\u9879\u5b9e\u9a8c\uff08N=1193\uff09\u5bf9\u6bd4\u516c\u4f17\u5bf9\u5e73\u6743\u7b97\u6cd5\u3001\u6b67\u89c6\u6027\u7b97\u6cd5\u548c\u516c\u5e73\u7b97\u6cd5\u5728\u62db\u8058\u4e0e\u5211\u4e8b\u53f8\u6cd5\u4e2d\u7684\u6001\u5ea6\u3002", "result": "\u81ea\u7531\u6d3e\u548c\u5c11\u6570\u65cf\u88d4\u5bf9\u5e73\u6743\u7b97\u6cd5\u7684\u8bc4\u4ef7\u4e0e\u516c\u5e73\u7b97\u6cd5\u76f8\u8fd1\uff0c\u800c\u4fdd\u5b88\u6d3e\u548c\u4e3b\u6d41\u65cf\u88d4\u5219\u5c06\u5176\u89c6\u4e3a\u4e0e\u6b67\u89c6\u6027\u7cfb\u7edf\u540c\u7b49\u8d1f\u9762\u3002\u8fd9\u79cd\u5206\u6b67\u6e90\u4e8e\u4eba\u4eec\u5bf9\u8c01\u662f\u88ab\u8fb9\u7f18\u5316\u7fa4\u4f53\u7684\u4e0d\u540c\u8ba4\u77e5\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5e73\u6743\u7b97\u6cd5\u63a5\u53d7\u5ea6\u7684\u5206\u6b67\uff0c\u5e76\u63a2\u8ba8\u4e86\u5f25\u5408\u8fd9\u79cd\u5206\u6b67\u7684\u53ef\u80fd\u6027\uff0c\u4ee5\u63a8\u52a8\u793e\u4f1a\u5bf9\u5e73\u6743\u7b97\u6cd5\u7684\u5171\u8bc6\u3002"}}
{"id": "2505.07344", "pdf": "https://arxiv.org/pdf/2505.07344", "abs": "https://arxiv.org/abs/2505.07344", "authors": ["Yuan Zhang", "Jiacheng Jiang", "Guoqing Ma", "Zhiying Lu", "Haoyang Huang", "Jianlong Yuan", "Nan Duan"], "title": "Generative Pre-trained Autoregressive Diffusion Transformer", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this work, we present GPDiT, a Generative Pre-trained Autoregressive\nDiffusion Transformer that unifies the strengths of diffusion and\nautoregressive modeling for long-range video synthesis, within a continuous\nlatent space. Instead of predicting discrete tokens, GPDiT autoregressively\npredicts future latent frames using a diffusion loss, enabling natural modeling\nof motion dynamics and semantic consistency across frames. This continuous\nautoregressive framework not only enhances generation quality but also endows\nthe model with representation capabilities. Additionally, we introduce a\nlightweight causal attention variant and a parameter-free rotation-based\ntime-conditioning mechanism, improving both the training and inference\nefficiency. Extensive experiments demonstrate that GPDiT achieves strong\nperformance in video generation quality, video representation ability, and\nfew-shot learning tasks, highlighting its potential as an effective framework\nfor video modeling in continuous space.", "AI": {"tldr": "GPDiT combines diffusion and autoregressive models for high-quality video synthesis in continuous latent space, improving motion and semantic consistency.", "motivation": "To unify diffusion and autoregressive modeling for better long-range video synthesis and continuous latent space representation.", "method": "Uses autoregressive prediction of future latent frames with diffusion loss, plus lightweight causal attention and rotation-based time-conditioning.", "result": "Achieves strong performance in video generation quality, representation, and few-shot learning.", "conclusion": "GPDiT is a promising framework for continuous-space video modeling."}}
{"id": "2505.06864", "pdf": "https://arxiv.org/pdf/2505.06864", "abs": "https://arxiv.org/abs/2505.06864", "authors": ["Shunyao Wang", "Ming Cheng", "Christina Dan Wang"], "title": "NewsNet-SDF: Stochastic Discount Factor Estimation with Pretrained Language Model News Embeddings via Adversarial Networks", "categories": ["q-fin.PM", "cs.LG"], "comment": null, "summary": "Stochastic Discount Factor (SDF) models provide a unified framework for asset\npricing and risk assessment, yet traditional formulations struggle to\nincorporate unstructured textual information. We introduce NewsNet-SDF, a novel\ndeep learning framework that seamlessly integrates pretrained language model\nembeddings with financial time series through adversarial networks. Our\nmultimodal architecture processes financial news using GTE-multilingual models,\nextracts temporal patterns from macroeconomic data via LSTM networks, and\nnormalizes firm characteristics, fusing these heterogeneous information sources\nthrough an innovative adversarial training mechanism. Our dataset encompasses\napproximately 2.5 million news articles and 10,000 unique securities,\naddressing the computational challenges of processing and aligning text data\nwith financial time series. Empirical evaluations on U.S. equity data\n(1980-2022) demonstrate NewsNet-SDF substantially outperforms alternatives with\na Sharpe ratio of 2.80. The model shows a 471% improvement over CAPM, over 200%\nimprovement versus traditional SDF implementations, and a 74% reduction in\npricing errors compared to the Fama-French five-factor model. In comprehensive\ncomparisons, our deep learning approach consistently outperforms traditional,\nmodern, and other neural asset pricing models across all key metrics. Ablation\nstudies confirm that text embeddings contribute significantly more to model\nperformance than macroeconomic features, with news-derived principal components\nranking among the most influential determinants of SDF dynamics. These results\nvalidate the effectiveness of our multimodal deep learning approach in\nintegrating unstructured text with traditional financial data for more accurate\nasset pricing, providing new insights for digital intelligent decision-making\nin financial technology.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNewsNet-SDF\u7684\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u7f51\u7edc\u5c06\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u4e0e\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u8d44\u4ea7\u5b9a\u4ef7\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfSDF\u6a21\u578b\u5728\u5904\u7406\u975e\u7ed3\u6784\u5316\u6587\u672c\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u878d\u5408\u6587\u672c\u548c\u91d1\u878d\u6570\u636e\u6765\u63d0\u5347\u5b9a\u4ef7\u548c\u98ce\u9669\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u67b6\u6784\uff0c\u7ed3\u5408GTE\u591a\u8bed\u8a00\u6a21\u578b\u5904\u7406\u91d1\u878d\u65b0\u95fb\u3001LSTM\u7f51\u7edc\u63d0\u53d6\u5b8f\u89c2\u7ecf\u6d4e\u6570\u636e\u65f6\u95f4\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u673a\u5236\u6574\u5408\u5f02\u6784\u4fe1\u606f\u3002", "result": "\u57281980-2022\u5e74\u7684\u7f8e\u56fd\u80a1\u7968\u6570\u636e\u4e0a\uff0c\u6a21\u578bSharpe\u6bd4\u7387\u8fbe2.80\uff0c\u663e\u8457\u4f18\u4e8eCAPM\u3001\u4f20\u7edfSDF\u548cFama-French\u4e94\u56e0\u5b50\u6a21\u578b\uff0c\u4e14\u6587\u672c\u5d4c\u5165\u5bf9\u6027\u80fd\u8d21\u732e\u6700\u5927\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\u6df1\u5ea6\u5b66\u4e60\u591a\u6a21\u6001\u65b9\u6cd5\u80fd\u6709\u6548\u878d\u5408\u975e\u7ed3\u6784\u5316\u6587\u672c\u4e0e\u4f20\u7edf\u91d1\u878d\u6570\u636e\uff0c\u4e3a\u91d1\u878d\u79d1\u6280\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2505.07364", "pdf": "https://arxiv.org/pdf/2505.07364", "abs": "https://arxiv.org/abs/2505.07364", "authors": ["Daria Zotova", "Nicolas Pinon", "Robin Trombetta", "Romain Bouet", "Julien Jung", "Carole Lartizien"], "title": "GAN-based synthetic FDG PET images from T1 brain MRI can serve to improve performance of deep unsupervised anomaly detection models", "categories": ["eess.IV", "cs.AI"], "comment": null, "summary": "Background and Objective. Research in the cross-modal medical image\ntranslation domain has been very productive over the past few years in tackling\nthe scarce availability of large curated multimodality datasets with the\npromising performance of GAN-based architectures. However, only a few of these\nstudies assessed task-based related performance of these synthetic data,\nespecially for the training of deep models. Method. We design and compare\ndifferent GAN-based frameworks for generating synthetic brain\n[18F]fluorodeoxyglucose (FDG) PET images from T1 weighted MRI data. We first\nperform standard qualitative and quantitative visual quality evaluation. Then,\nwe explore further impact of using these fake PET data in the training of a\ndeep unsupervised anomaly detection (UAD) model designed to detect subtle\nepilepsy lesions in T1 MRI and FDG PET images. We introduce novel diagnostic\ntask-oriented quality metrics of the synthetic FDG PET data tailored to our\nunsupervised detection task, then use these fake data to train a use case UAD\nmodel combining a deep representation learning based on siamese autoencoders\nwith a OC-SVM density support estimation model. This model is trained on normal\nsubjects only and allows the detection of any variation from the pattern of the\nnormal population. We compare the detection performance of models trained on 35\npaired real MR T1 of normal subjects paired either on 35 true PET images or on\n35 synthetic PET images generated from the best performing generative models.\nPerformance analysis is conducted on 17 exams of epilepsy patients undergoing\nsurgery. Results. The best performing GAN-based models allow generating\nrealistic fake PET images of control subject with SSIM and PSNR values around\n0.9 and 23.8, respectively and in distribution (ID) with regard to the true\ncontrol dataset. The best UAD model trained on these synthetic normative PET\ndata allows reaching 74% sensitivity. Conclusion. Our results confirm that\nGAN-based models are the best suited for MR T1 to FDG PET translation,\noutperforming transformer or diffusion models. We also demonstrate the\ndiagnostic value of these synthetic data for the training of UAD models and\nevaluation on clinical exams of epilepsy patients. Our code and the normative\nimage dataset are available.", "AI": {"tldr": "\u4f7f\u7528GAN\u67b6\u6784\u7814\u7a76MR T1\u5230FDG PET\u7684\u8de8\u6a21\u6001\u7ffb\u8bd1\uff0c\u9a8c\u8bc1\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u5728\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u533b\u5b66\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u7814\u7a76GAN\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u5728\u8bad\u7ec3\u6df1\u5ea6\u6a21\u578b\u65f6\u7684\u4efb\u52a1\u76f8\u5173\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u5e76\u6bd4\u8f83\u591a\u79cdGAN\u6846\u67b6\uff0c\u751f\u6210\u8111FDG PET\u56fe\u50cf\uff0c\u8bc4\u4f30\u89c6\u89c9\u8d28\u91cf\u540e\uff0c\u7528\u4e8e\u8bad\u7ec3\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\uff08UAD\uff09\u3002", "result": "\u6700\u4f73GAN\u6a21\u578b\u751f\u6210\u903c\u771fPET\u56fe\u50cf\uff08SSIM 0.9\uff0cPSNR 23.8\uff09\uff0cUAD\u6a21\u578b\u5728\u5408\u6210\u6570\u636e\u4e0a\u8fbe\u523074%\u7075\u654f\u5ea6\u3002", "conclusion": "GAN\u5728MR T1\u5230FDG PET\u7ffb\u8bd1\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u5408\u6210\u6570\u636e\u5bf9\u8bad\u7ec3UAD\u6a21\u578b\u5177\u6709\u8bca\u65ad\u4ef7\u503c\u3002"}}
{"id": "2505.07372", "pdf": "https://arxiv.org/pdf/2505.07372", "abs": "https://arxiv.org/abs/2505.07372", "authors": ["David de-Fitero-Dominguez", "Antonio Garcia-Cabot", "Eva Garcia-Lopez"], "title": "Synthetic Code Surgery: Repairing Bugs and Vulnerabilities with LLMs and Synthetic Data", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "This paper presents a novel methodology for enhancing Automated Program\nRepair (APR) through synthetic data generation utilizing Large Language Models\n(LLMs). Current APR systems are constrained by the limited availability of\nhigh-quality training data encompassing diverse bug types across multiple\nprogramming languages. The proposed approach addresses this limitation through\na two-phase process: a synthetic sample generation followed by a rigorous\nquality assessment. Multiple state-of-the-art LLMs were employed to generate\napproximately 30,000 paired examples of buggy and fixed code across 12\nprogramming languages and 13 bug categories. Subsequently, these samples\nunderwent cross-model evaluation against five criteria: correctness, code\nquality, security, performance, and completeness. Experimental evaluation on\nthe VulRepair test set dataset showed statistically significant improvements in\nPerfect Prediction rates, with the quality-filtered synthetic dataset\noutperforming both baseline and real-world commit data configurations in\ncertain scenarios. The methodology was validated through rigorous statistical\ntesting, including ANOVA and post-hoc Tukey's Honest Significant Difference\nanalysis. Furthermore, the best-performing configurations surpassed existing\nsystems despite using a less computationally intensive decoding strategy. This\nresearch establishes a self-bootstrapping paradigm in which LLMs generate and\nevaluate their own training data, potentially transforming approaches to data\nscarcity across software engineering tasks and advancing the development of\nrobust, adaptable tools for automated code maintenance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u5408\u6210\u6570\u636e\u4ee5\u589e\u5f3a\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\uff08APR\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709APR\u7cfb\u7edf\u56e0\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u800c\u53d7\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524dAPR\u7cfb\u7edf\u53d7\u9650\u4e8e\u8de8\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c\u9519\u8bef\u7c7b\u578b\u7684\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u7f3a\u6027\uff0c\u5f71\u54cd\u4e86\u5176\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u9996\u5148\u751f\u6210\u5408\u6210\u6837\u672c\uff0c\u968f\u540e\u8fdb\u884c\u4e25\u683c\u7684\u8d28\u91cf\u8bc4\u4f30\u3002\u91c7\u7528\u591a\u79cd\u5148\u8fdbLLM\u751f\u6210\u4e86\u7ea630,000\u5bf9\u8de812\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c13\u79cd\u9519\u8bef\u7c7b\u522b\u7684\u9519\u8bef\u548c\u4fee\u590d\u4ee3\u7801\u6837\u672c\uff0c\u5e76\u901a\u8fc7\u4e94\u9879\u6807\u51c6\uff08\u6b63\u786e\u6027\u3001\u4ee3\u7801\u8d28\u91cf\u3001\u5b89\u5168\u6027\u3001\u6027\u80fd\u3001\u5b8c\u6574\u6027\uff09\u8fdb\u884c\u8de8\u6a21\u578b\u8bc4\u4f30\u3002", "result": "\u5728VulRepair\u6d4b\u8bd5\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8d28\u91cf\u8fc7\u6ee4\u540e\u7684\u5408\u6210\u6570\u636e\u96c6\u5728Perfect Prediction\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u548c\u771f\u5b9e\u63d0\u4ea4\u6570\u636e\u914d\u7f6e\uff0c\u90e8\u5206\u573a\u666f\u8868\u73b0\u66f4\u4f18\u3002\u6700\u4f73\u914d\u7f6e\u5728\u8f83\u4f4e\u8ba1\u7b97\u5f3a\u5ea6\u4e0b\u8d85\u8d8a\u4e86\u73b0\u6709\u7cfb\u7edf\u3002", "conclusion": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u79cd\u81ea\u4e3e\u8303\u5f0f\uff0c\u901a\u8fc7LLM\u751f\u6210\u5e76\u8bc4\u4f30\u81ea\u8eab\u8bad\u7ec3\u6570\u636e\uff0c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u63a8\u52a8\u4e86\u81ea\u52a8\u5316\u4ee3\u7801\u7ef4\u62a4\u5de5\u5177\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u53d1\u5c55\u3002"}}
{"id": "2505.07377", "pdf": "https://arxiv.org/pdf/2505.07377", "abs": "https://arxiv.org/abs/2505.07377", "authors": ["Suleyman Ozdel", "Can Sarpkaya", "Efe Bozkir", "Hong Gao", "Enkelejda Kasneci"], "title": "Examining the Role of LLM-Driven Interactions on Attention and Cognitive Engagement in Virtual Classrooms", "categories": ["cs.HC", "cs.AI"], "comment": "Accepted to EDM 2025 (Eighteenth International Conference on\n  Educational Data Mining)", "summary": "Transforming educational technologies through the integration of large\nlanguage models (LLMs) and virtual reality (VR) offers the potential for\nimmersive and interactive learning experiences. However, the effects of LLMs on\nuser engagement and attention in educational environments remain open\nquestions. In this study, we utilized a fully LLM-driven virtual learning\nenvironment, where peers and teachers were LLM-driven, to examine how students\nbehaved in such settings. Specifically, we investigate how peer question-asking\nbehaviors influenced student engagement, attention, cognitive load, and\nlearning outcomes and found that, in conditions where LLM-driven peer learners\nasked questions, students exhibited more targeted visual scanpaths, with their\nattention directed toward the learning content, particularly in complex\nsubjects. Our results suggest that peer questions did not introduce extraneous\ncognitive load directly, as the cognitive load is strongly correlated with\nincreased attention to the learning material. Considering these findings, we\nprovide design recommendations for optimizing VR learning spaces.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u7ed3\u5408\u5728\u6559\u80b2\u6280\u672f\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u5173\u6ce8LLM\u9a71\u52a8\u7684\u865a\u62df\u5b66\u4e60\u73af\u5883\u4e2d\u5b66\u751f\u4e92\u52a8\u884c\u4e3a\u5bf9\u6ce8\u610f\u529b\u3001\u8ba4\u77e5\u8d1f\u8377\u548c\u5b66\u4e60\u6548\u679c\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u7a76LLM\u548cVR\u7ed3\u5408\u7684\u6c89\u6d78\u5f0f\u5b66\u4e60\u73af\u5883\u5982\u4f55\u5f71\u54cd\u5b66\u751f\u7684\u53c2\u4e0e\u5ea6\u548c\u6ce8\u610f\u529b\uff0c\u4ece\u800c\u4f18\u5316\u6559\u80b2\u6280\u672f\u7684\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528\u5b8c\u5168\u7531LLM\u9a71\u52a8\u7684\u865a\u62df\u5b66\u4e60\u73af\u5883\uff0c\u6a21\u62dfLLM\u9a71\u52a8\u7684\u540c\u4f34\u548c\u6559\u5e08\uff0c\u5206\u6790\u5b66\u751f\u884c\u4e3a\uff08\u7279\u522b\u662f\u540c\u4f34\u63d0\u95ee\u884c\u4e3a\uff09\u5bf9\u6ce8\u610f\u529b\u3001\u8ba4\u77e5\u8d1f\u8377\u548c\u5b66\u4e60\u6210\u679c\u7684\u5f71\u54cd\u3002", "result": "\u5728LLM\u9a71\u52a8\u7684\u540c\u4f34\u63d0\u95ee\u60c5\u5883\u4e0b\uff0c\u5b66\u751f\u7684\u89c6\u89c9\u626b\u63cf\u8def\u5f84\u66f4\u96c6\u4e2d\uff0c\u6ce8\u610f\u529b\u66f4\u4e13\u6ce8\u4e8e\u5b66\u4e60\u5185\u5bb9\uff08\u5c24\u5176\u662f\u590d\u6742\u5b66\u79d1\uff09\uff0c\u4e14\u8ba4\u77e5\u8d1f\u8377\u4e0e\u5bf9\u5b66\u4e60\u6750\u6599\u7684\u5173\u6ce8\u5ea6\u6b63\u76f8\u5173\u3002", "conclusion": "\u540c\u4f34\u63d0\u95ee\u672a\u76f4\u63a5\u5f15\u5165\u989d\u5916\u8ba4\u77e5\u8d1f\u8377\uff0c\u4e14\u80fd\u63d0\u5347\u6ce8\u610f\u529b\uff0c\u7814\u7a76\u4e3a\u4f18\u5316VR\u5b66\u4e60\u7a7a\u95f4\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u5efa\u8bae\u3002"}}
{"id": "2505.06900", "pdf": "https://arxiv.org/pdf/2505.06900", "abs": "https://arxiv.org/abs/2505.06900", "authors": ["Zhenzhou Jin", "Li You", "Derrick Wing Kwan Ng", "Xiang-Gen Xia", "Xiqi Gao"], "title": "Near-Field Channel Estimation for XL-MIMO: A Deep Generative Model Guided by Side Information", "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT"], "comment": "15 pages, 11 figures, to appear on IEEE Transactions on Cognitive\n  Communications and Networking", "summary": "This paper investigates the near-field (NF) channel estimation (CE) for\nextremely large-scale multiple-input multiple-output (XL-MIMO) systems.\nConsidering the pronounced NF effects in XL-MIMO communications, we first\nestablish a joint angle-distance (AD) domain-based spherical-wavefront physical\nchannel model that captures the inherent sparsity of XL-MIMO channels.\nLeveraging the channel's sparsity in the joint AD domain, the CE is approached\nas a task of reconstructing sparse signals. Anchored in this framework, we\nfirst propose a compressed sensing algorithm to acquire a preliminary channel\nestimate. Harnessing the powerful implicit prior learning capability of\ngenerative artificial intelligence (GenAI), we further propose a GenAI-based\napproach to refine the estimated channel. Specifically, we introduce the\npreliminary estimated channel as side information, and derive the evidence\nlower bound (ELBO) of the log-marginal distribution of the target NF channel\nconditioned on the preliminary estimated channel, which serves as the\noptimization objective for the proposed generative diffusion model (GDM).\nAdditionally, we introduce a more generalized version of the GDM, the\nnon-Markovian GDM (NM-GDM), to accelerate the sampling process, achieving an\napproximately tenfold enhancement in sampling efficiency. Experimental results\nindicate that the proposed approach is capable of offering substantial\nperformance gain in CE compared to existing benchmark schemes within NF XL-MIMO\nsystems. Furthermore, our approach exhibits enhanced generalization\ncapabilities in both the NF or far-field (FF) regions.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6781\u5927\u89c4\u6a21\u591a\u8f93\u5165\u591a\u8f93\u51fa\uff08XL-MIMO\uff09\u7cfb\u7edf\u4e2d\u7684\u8fd1\u573a\u4fe1\u9053\u4f30\u8ba1\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u5408\u89d2\u5ea6-\u8ddd\u79bb\u57df\u7684\u7a00\u758f\u4fe1\u9053\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u4f18\u5316\u4f30\u8ba1\u7ed3\u679c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fe1\u9053\u4f30\u8ba1\u6027\u80fd\u3002", "motivation": "XL-MIMO\u7cfb\u7edf\u4e2d\u8fd1\u573a\u6548\u5e94\u7684\u663e\u8457\u5b58\u5728\u4f7f\u5f97\u4f20\u7edf\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\u4e0d\u518d\u9002\u7528\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u65b0\u7684\u7269\u7406\u4fe1\u9053\u6a21\u578b\u5e76\u5f00\u53d1\u9ad8\u6548\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u5efa\u7acb\u8054\u5408\u89d2\u5ea6-\u8ddd\u79bb\u57df\u7684\u7a00\u758f\u4fe1\u9053\u6a21\u578b\uff0c\u63d0\u51fa\u57fa\u4e8e\u538b\u7f29\u611f\u77e5\u7684\u521d\u6b65\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u8fdb\u4e00\u6b65\u5229\u7528\u751f\u6210\u5f0f\u6269\u6563\u6a21\u578b\uff08GDM\uff09\u4f18\u5316\u4f30\u8ba1\u7ed3\u679c\uff0c\u5f15\u5165\u4e86\u975e\u9a6c\u5c14\u53ef\u592bGDM\uff08NM-GDM\uff09\u52a0\u901f\u91c7\u6837\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u8fd1\u573aXL-MIMO\u7cfb\u7edf\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6848\uff0c\u4e14\u5728\u8fd1\u573a\u548c\u8fdc\u573a\u533a\u57df\u5747\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7ed3\u5408\u7a00\u758f\u4fe1\u9053\u5efa\u6a21\u548c\u751f\u6210\u5f0fAI\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u4e3aXL-MIMO\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u4fe1\u9053\u4f30\u8ba1\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.07381", "pdf": "https://arxiv.org/pdf/2505.07381", "abs": "https://arxiv.org/abs/2505.07381", "authors": ["Baoping Cheng", "Yukun Zhang", "Liming Wang", "Xiaoyan Xie", "Tao Fu", "Dongkun Wang", "Xiaoming Tao"], "title": "Few-shot Semantic Encoding and Decoding for Video Surveillance", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the continuous increase in the number and resolution of video\nsurveillance cameras, the burden of transmitting and storing surveillance video\nis growing. Traditional communication methods based on Shannon's theory are\nfacing optimization bottlenecks. Semantic communication, as an emerging\ncommunication method, is expected to break through this bottleneck and reduce\nthe storage and transmission consumption of video. Existing semantic decoding\nmethods often require many samples to train the neural network for each scene,\nwhich is time-consuming and labor-intensive. In this study, a semantic encoding\nand decoding method for surveillance video is proposed. First, the sketch was\nextracted as semantic information, and a sketch compression method was proposed\nto reduce the bit rate of semantic information. Then, an image translation\nnetwork was proposed to translate the sketch into a video frame with a\nreference frame. Finally, a few-shot sketch decoding network was proposed to\nreconstruct video from sketch. Experimental results showed that the proposed\nmethod achieved significantly better video reconstruction performance than\nbaseline methods. The sketch compression method could effectively reduce the\nstorage and transmission consumption of semantic information with little\ncompromise on video quality. The proposed method provides a novel semantic\nencoding and decoding method that only needs a few training samples for each\nsurveillance scene, thus improving the practicality of the semantic\ncommunication system.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u76d1\u63a7\u89c6\u9891\u7684\u8bed\u4e49\u7f16\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u8349\u56fe\u4f5c\u4e3a\u8bed\u4e49\u4fe1\u606f\u5e76\u538b\u7f29\uff0c\u7ed3\u5408\u56fe\u50cf\u7ffb\u8bd1\u7f51\u7edc\u548c\u5c11\u6837\u672c\u8349\u56fe\u89e3\u7801\u7f51\u7edc\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5b58\u50a8\u548c\u4f20\u8f93\u6210\u672c\uff0c\u4e14\u4ec5\u9700\u5c11\u91cf\u8bad\u7ec3\u6837\u672c\u3002", "motivation": "\u968f\u7740\u76d1\u63a7\u6444\u50cf\u5934\u6570\u91cf\u548c\u5206\u8fa8\u7387\u7684\u589e\u52a0\uff0c\u4f20\u7edf\u901a\u4fe1\u65b9\u6cd5\u9762\u4e34\u4f18\u5316\u74f6\u9888\uff0c\u8bed\u4e49\u901a\u4fe1\u6709\u671b\u7a81\u7834\u8fd9\u4e00\u9650\u5236\u3002\u73b0\u6709\u8bed\u4e49\u89e3\u7801\u65b9\u6cd5\u9700\u5927\u91cf\u6837\u672c\u8bad\u7ec3\uff0c\u8017\u65f6\u8017\u529b\u3002", "method": "1. \u63d0\u53d6\u8349\u56fe\u4f5c\u4e3a\u8bed\u4e49\u4fe1\u606f\u5e76\u538b\u7f29\uff1b2. \u63d0\u51fa\u56fe\u50cf\u7ffb\u8bd1\u7f51\u7edc\u5c06\u8349\u56fe\u8f6c\u6362\u4e3a\u89c6\u9891\u5e27\uff1b3. \u8bbe\u8ba1\u5c11\u6837\u672c\u8349\u56fe\u89e3\u7801\u7f51\u7edc\u91cd\u5efa\u89c6\u9891\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u89c6\u9891\u91cd\u5efa\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u8349\u56fe\u538b\u7f29\u65b9\u6cd5\u6709\u6548\u964d\u4f4e\u4e86\u5b58\u50a8\u548c\u4f20\u8f93\u6210\u672c\uff0c\u4e14\u89c6\u9891\u8d28\u91cf\u635f\u5931\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ea\u9700\u5c11\u91cf\u8bad\u7ec3\u6837\u672c\uff0c\u63d0\u5347\u4e86\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.06906", "pdf": "https://arxiv.org/pdf/2505.06906", "abs": "https://arxiv.org/abs/2505.06906", "authors": ["Sindre Benjamin Remman", "Anastasios M. Lekkas"], "title": "Realistic Counterfactual Explanations for Machine Learning-Controlled Mobile Robots using 2D LiDAR", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "comment": "Accepted for publication at the 2025 European Control Conference\n  (ECC)", "summary": "This paper presents a novel method for generating realistic counterfactual\nexplanations (CFEs) in machine learning (ML)-based control for mobile robots\nusing 2D LiDAR. ML models, especially artificial neural networks (ANNs), can\nprovide advanced decision-making and control capabilities by learning from\ndata. However, they often function as black boxes, making it challenging to\ninterpret them. This is especially a problem in safety-critical control\napplications. To generate realistic CFEs, we parameterize the LiDAR space with\nsimple shapes such as circles and rectangles, whose parameters are chosen by a\ngenetic algorithm, and the configurations are transformed into LiDAR data by\nraycasting. Our model-agnostic approach generates CFEs in the form of synthetic\nLiDAR data that resembles a base LiDAR state but is modified to produce a\npre-defined ML model control output based on a query from the user. We\ndemonstrate our method on a mobile robot, the TurtleBot3, controlled using deep\nreinforcement learning (DRL) in real-world and simulated scenarios. Our method\ngenerates logical and realistic CFEs, which helps to interpret the DRL agent's\ndecision making. This paper contributes towards advancing explainable AI in\nmobile robotics, and our method could be a tool for understanding, debugging,\nand improving ML-based autonomous control.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u75282D\u6fc0\u5149\u96f7\u8fbe\u751f\u6210\u673a\u5668\u5b66\u4e60\u63a7\u5236\u4e2d\u73b0\u5b9e\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u6fc0\u5149\u96f7\u8fbe\u7a7a\u95f4\u5e76\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u5f62\u72b6\u53c2\u6570\uff0c\u4ee5\u7528\u6237\u67e5\u8be2\u4e3a\u57fa\u7840\u751f\u6210\u5408\u6210\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\uff0c\u4ece\u800c\u89e3\u91ca\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u7684\u51b3\u7b56\u3002", "motivation": "\u7531\u4e8e\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5c24\u5176\u662f\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff09\u5728\u5b89\u5168\u5173\u952e\u63a7\u5236\u5e94\u7528\u4e2d\u5e38\u88ab\u89c6\u4e3a\u9ed1\u7bb1\uff0c\u96be\u4ee5\u89e3\u91ca\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u751f\u6210\u73b0\u5b9e\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u901a\u8fc7\u7528\u7b80\u5355\u5f62\u72b6\uff08\u5982\u5706\u5f62\u548c\u77e9\u5f62\uff09\u53c2\u6570\u5316\u6fc0\u5149\u96f7\u8fbe\u7a7a\u95f4\uff0c\u5229\u7528\u9057\u4f20\u7b97\u6cd5\u9009\u62e9\u53c2\u6570\u914d\u7f6e\uff0c\u5e76\u901a\u8fc7\u5149\u7ebf\u6295\u5c04\u751f\u6210\u5408\u6210\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\uff0c\u8fdb\u800c\u751f\u6210\u53cd\u4e8b\u5b9e\u89e3\u91ca\u3002", "result": "\u5728TurtleBot3\u79fb\u52a8\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u903b\u8f91\u5408\u7406\u4e14\u73b0\u5b9e\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u6709\u6548\u89e3\u91ca\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u4e2d\u7684\u53ef\u89e3\u91caAI\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u3001\u8c03\u8bd5\u548c\u6539\u8fdb\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u81ea\u4e3b\u63a7\u5236\u3002"}}
{"id": "2505.07393", "pdf": "https://arxiv.org/pdf/2505.07393", "abs": "https://arxiv.org/abs/2505.07393", "authors": ["Nadine Sandjo Tchatchoua", "Richard Harper"], "title": "AI in Money Matters", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "In November 2022, Europe and the world by and large were stunned by the birth\nof a new large language model : ChatGPT. Ever since then, both academic and\npopulist discussions have taken place in various public spheres such as\nLinkedIn and X(formerly known as Twitter) with the view to both understand the\ntool and its benefits for the society. The views of real actors in professional\nspaces, especially in regulated industries such as finance and law have been\nlargely missing. We aim to begin to close this gap by presenting results from\nan empirical investigation conducted through interviews with professional\nactors in the Fintech industry. The paper asks the question, how and to what\nextent are large language models in general and ChatGPT in particular being\nadopted and used in the Fintech industry? The results show that while the\nfintech experts we spoke with see a potential in using large language models in\nthe future, a lot of questions marks remain concerning how they are policed and\ntherefore might be adopted in a regulated industry such as Fintech. This paper\naims to add to the existing academic discussing around large language models,\nwith a contribution to our understanding of professional viewpoints.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86ChatGPT\u7b49\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u79d1\u6280\u884c\u4e1a\u7684\u5e94\u7528\u73b0\u72b6\u548c\u672a\u6765\u6f5c\u529b\uff0c\u5f3a\u8c03\u4e86\u5bf9\u76d1\u7ba1\u95ee\u9898\u7684\u5173\u6ce8\u3002", "motivation": "\u586b\u8865\u91d1\u878d\u79d1\u6280\u7b49\u53d7\u76d1\u7ba1\u884c\u4e1a\u4e2d\u4e13\u4e1a\u4eba\u58eb\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f7f\u7528\u610f\u89c1\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5bf9\u91d1\u878d\u79d1\u6280\u884c\u4e1a\u4e13\u4e1a\u4eba\u58eb\u7684\u8bbf\u8c08\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u5c3d\u7ba1\u91d1\u878d\u79d1\u6280\u4e13\u5bb6\u8ba4\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u53d7\u76d1\u7ba1\u884c\u4e1a\u4e2d\u7684\u91c7\u7eb3\u4ecd\u9762\u4e34\u76d1\u7ba1\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u672c\u6587\u4e3a\u7406\u89e3\u4e13\u4e1a\u89c2\u70b9\u63d0\u4f9b\u4e86\u8d21\u732e\uff0c\u6307\u51fa\u5728\u91d1\u878d\u79d1\u6280\u7b49\u53d7\u76d1\u7ba1\u884c\u4e1a\u4e2d\uff0c\u76d1\u7ba1\u95ee\u9898\u662f\u91c7\u7eb3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5173\u952e\u3002"}}
{"id": "2505.06918", "pdf": "https://arxiv.org/pdf/2505.06918", "abs": "https://arxiv.org/abs/2505.06918", "authors": ["Yanhui Hong", "Nan Wang", "Zhiyi Xia", "Haoyi Tao", "Xi Fang", "Yiming Li", "Jiankun Wang", "Peng Jin", "Xiaochen Cai", "Shengyu Li", "Ziqi Chen", "Zezhong Zhang", "Guolin Ke", "Linfeng Zhang"], "title": "Uni-AIMS: AI-Powered Microscopy Image Analysis", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "This paper presents a systematic solution for the intelligent recognition and\nautomatic analysis of microscopy images. We developed a data engine that\ngenerates high-quality annotated datasets through a combination of the\ncollection of diverse microscopy images from experiments, synthetic data\ngeneration and a human-in-the-loop annotation process. To address the unique\nchallenges of microscopy images, we propose a segmentation model capable of\nrobustly detecting both small and large objects. The model effectively\nidentifies and separates thousands of closely situated targets, even in\ncluttered visual environments. Furthermore, our solution supports the precise\nautomatic recognition of image scale bars, an essential feature in quantitative\nmicroscopic analysis. Building upon these components, we have constructed a\ncomprehensive intelligent analysis platform and validated its effectiveness and\npracticality in real-world applications. This study not only advances automatic\nrecognition in microscopy imaging but also ensures scalability and\ngeneralizability across multiple application domains, offering a powerful tool\nfor automated microscopic analysis in interdisciplinary research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u663e\u5fae\u955c\u56fe\u50cf\u7684\u667a\u80fd\u8bc6\u522b\u4e0e\u81ea\u52a8\u5206\u6790\uff0c\u5305\u62ec\u6570\u636e\u5f15\u64ce\u3001\u5206\u5272\u6a21\u578b\u548c\u81ea\u52a8\u6807\u5c3a\u8bc6\u522b\u529f\u80fd\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7efc\u5408\u667a\u80fd\u5206\u6790\u5e73\u53f0\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u89e3\u51b3\u663e\u5fae\u955c\u56fe\u50cf\u8bc6\u522b\u4e0e\u5206\u6790\u4e2d\u7684\u72ec\u7279\u6311\u6218\uff0c\u63d0\u5347\u81ea\u52a8\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4e3a\u8de8\u5b66\u79d1\u7814\u7a76\u63d0\u4f9b\u81ea\u52a8\u5316\u5de5\u5177\u652f\u6301\u3002", "method": "\u7ed3\u5408\u5b9e\u9a8c\u6570\u636e\u6536\u96c6\u3001\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u4eba\u5de5\u6807\u6ce8\u6d41\u7a0b\u6784\u5efa\u6570\u636e\u5f15\u64ce\uff1b\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u7a33\u5065\u68c0\u6d4b\u5927\u5c0f\u7269\u4f53\u7684\u5206\u5272\u6a21\u578b\uff1b\u652f\u6301\u81ea\u52a8\u8bc6\u522b\u56fe\u50cf\u6807\u5c3a\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u7efc\u5408\u667a\u80fd\u5206\u6790\u5e73\u53f0\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\uff0c\u63d0\u5347\u4e86\u663e\u5fae\u955c\u56fe\u50cf\u81ea\u52a8\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u63a8\u52a8\u4e86\u663e\u5fae\u955c\u56fe\u50cf\u81ea\u52a8\u8bc6\u522b\u7684\u53d1\u5c55\uff0c\u8fd8\u786e\u4fdd\u4e86\u89e3\u51b3\u65b9\u6848\u7684\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u8de8\u5b66\u79d1\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u81ea\u52a8\u5316\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2505.06927", "pdf": "https://arxiv.org/pdf/2505.06927", "abs": "https://arxiv.org/abs/2505.06927", "authors": ["Ryan Cory-Wright", "Andr\u00e9s G\u00f3mez"], "title": "Stability Regularized Cross-Validation", "categories": ["math.OC", "cs.LG", "stat.ML"], "comment": "Some of this material previously appeared in 2306.14851v2, which we\n  have split into two papers (this one and 2306.14851v3), because it contained\n  two ideas that need separate papers", "summary": "We revisit the problem of ensuring strong test-set performance via\ncross-validation. Motivated by the generalization theory literature, we propose\na nested k-fold cross-validation scheme that selects hyperparameters by\nminimizing a weighted sum of the usual cross-validation metric and an empirical\nmodel-stability measure. The weight on the stability term is itself chosen via\na nested cross-validation procedure. This reduces the risk of strong validation\nset performance and poor test set performance due to instability. We benchmark\nour procedure on a suite of 13 real-world UCI datasets, and find that, compared\nto k-fold cross-validation over the same hyperparameters, it improves the\nout-of-sample MSE for sparse ridge regression and CART by 4% on average, but\nhas no impact on XGBoost. This suggests that for interpretable and unstable\nmodels, such as sparse regression and CART, our approach is a viable and\ncomputationally affordable method for improving test-set performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5d4c\u5957k\u6298\u4ea4\u53c9\u9a8c\u8bc1\u65b9\u6848\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u4ea4\u53c9\u9a8c\u8bc1\u6307\u6807\u4e0e\u6a21\u578b\u7a33\u5b9a\u6027\u5ea6\u91cf\u7684\u52a0\u6743\u548c\u6765\u9009\u62e9\u8d85\u53c2\u6570\uff0c\u4ee5\u63d0\u5347\u6d4b\u8bd5\u96c6\u6027\u80fd\uff0c\u5c24\u5176\u5728\u7a00\u758f\u56de\u5f52\u548cCART\u7b49\u4e0d\u7a33\u5b9a\u6a21\u578b\u4e0a\u6709\u6548\u3002", "motivation": "\u73b0\u6709\u4ea4\u53c9\u9a8c\u8bc1\u65b9\u6cd5\u53ef\u80fd\u56e0\u6a21\u578b\u4e0d\u7a33\u5b9a\u800c\u5bfc\u81f4\u9a8c\u8bc1\u96c6\u6027\u80fd\u597d\u4f46\u6d4b\u8bd5\u96c6\u6027\u80fd\u5dee\u7684\u95ee\u9898\uff0c\u9700\u7ed3\u5408\u7a33\u5b9a\u6027\u5ea6\u91cf\u4ee5\u6539\u5584\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5d4c\u5957k\u6298\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u8d85\u53c2\u6570\u9009\u62e9\u7efc\u5408\u8003\u8651\u4ea4\u53c9\u9a8c\u8bc1\u6307\u6807\u548c\u6a21\u578b\u7a33\u5b9a\u6027\u5ea6\u91cf\uff0c\u7a33\u5b9a\u6027\u6743\u91cd\u901a\u8fc7\u5d4c\u5957\u4ea4\u53c9\u9a8c\u8bc1\u786e\u5b9a\u3002", "result": "\u572813\u4e2aUCI\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u7a00\u758f\u5cad\u56de\u5f52\u548cCART\u7684\u6d4b\u8bd5\u96c6MSE\u5e73\u5747\u63d0\u53474%\uff0c\u4f46\u5bf9XGBoost\u65e0\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5bf9\u4e0d\u7a33\u5b9a\u6027\u6a21\u578b\uff08\u5982\u7a00\u758f\u56de\u5f52\u3001CART\uff09\u80fd\u6709\u6548\u63d0\u5347\u6d4b\u8bd5\u6027\u80fd\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u53ef\u63a7\u3002"}}
{"id": "2505.06928", "pdf": "https://arxiv.org/pdf/2505.06928", "abs": "https://arxiv.org/abs/2505.06928", "authors": ["Chi-Sheng Chen", "En-Jui Kuo"], "title": "Unraveling Quantum Environments: Transformer-Assisted Learning in Lindblad Dynamics", "categories": ["quant-ph", "cs.LG"], "comment": null, "summary": "Understanding dissipation in open quantum systems is crucial for the\ndevelopment of robust quantum technologies. In this work, we introduce a\nTransformer-based machine learning framework to infer time-dependent\ndissipation rates in quantum systems governed by the Lindblad master equation.\nOur approach uses time series of observable quantities, such as expectation\nvalues of single Pauli operators, as input to learn dissipation profiles\nwithout requiring knowledge of the initial quantum state or even the system\nHamiltonian.\n  We demonstrate the effectiveness of our approach on a hierarchy of open\nquantum models of increasing complexity, including single-qubit systems with\ntime-independent or time-dependent jump rates, two-qubit interacting systems\n(e.g., Heisenberg and transverse Ising models), and the Jaynes--Cummings model\ninvolving light--matter interaction and cavity loss with time-dependent decay\nrates. Our method accurately reconstructs both fixed and time-dependent decay\nrates from observable time series. To support this, we prove that under\nreasonable assumptions, the jump rates in all these models are uniquely\ndetermined by a finite set of observables, such as qubit and photon\nmeasurements. In practice, we combine Transformer-based architectures with\nlightweight feature extraction techniques to efficiently learn these dynamics.\nOur results suggest that modern machine learning tools can serve as scalable\nand data-driven alternatives for identifying unknown environments in open\nquantum systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u63a8\u65ad\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u8017\u6563\u7387\uff0c\u65e0\u9700\u521d\u59cb\u91cf\u5b50\u6001\u6216\u7cfb\u7edf\u54c8\u5bc6\u987f\u91cf\u7684\u5148\u9a8c\u77e5\u8bc6\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u65f6\u95f4\u5e8f\u5217\u53ef\u89c2\u6d4b\u6570\u636e\u6709\u6548\u5b66\u4e60\u8017\u6563\u6a21\u5f0f\uff0c\u5e76\u5728\u591a\u79cd\u590d\u6742\u6a21\u578b\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002", "motivation": "\u7406\u89e3\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u8017\u6563\u73b0\u8c61\u5bf9\u91cf\u5b50\u6280\u672f\u7684\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u7279\u5b9a\u5047\u8bbe\u6216\u590d\u6742\u5efa\u6a21\uff0c\u672c\u7814\u7a76\u7684\u76ee\u7684\u662f\u63d0\u4f9b\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528Transformer\u67b6\u6784\u548c\u8f7b\u91cf\u7ea7\u7279\u5f81\u63d0\u53d6\u6280\u672f\uff0c\u901a\u8fc7\u65f6\u95f4\u5e8f\u5217\u53ef\u89c2\u6d4b\u6570\u636e\uff08\u5982\u6ce1\u5229\u7b97\u7b26\u671f\u671b\u503c\uff09\u63a8\u65ad\u8017\u6563\u7387\uff0c\u9002\u7528\u4e8e\u4ece\u5355\u91cf\u5b50\u6bd4\u7279\u5230\u591a\u4f53\u76f8\u4e92\u4f5c\u7528\u7684\u590d\u6742\u6a21\u578b\u3002", "result": "\u65b9\u6cd5\u80fd\u51c6\u786e\u91cd\u6784\u56fa\u5b9a\u548c\u65f6\u53d8\u8017\u6563\u7387\uff0c\u4e14\u5728\u591a\u79cd\u91cf\u5b50\u6a21\u578b\uff08\u5982\u5355\u6bd4\u7279\u3001\u53cc\u6bd4\u7279\u76f8\u4e92\u4f5c\u7528\u53caJaynes-Cummings\u6a21\u578b\uff09\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u8017\u6563\u7387\u53ef\u7531\u6709\u9650\u53ef\u89c2\u6d4b\u96c6\u5408\u552f\u4e00\u786e\u5b9a\u3002", "conclusion": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u5de5\u5177\u4e3a\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u4e2d\u672a\u77e5\u73af\u5883\u7684\u8bc6\u522b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6848\uff0c\u4e3a\u91cf\u5b50\u6280\u672f\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2505.06948", "pdf": "https://arxiv.org/pdf/2505.06948", "abs": "https://arxiv.org/abs/2505.06948", "authors": ["Pan Du", "Wangbo Zhao", "Xinai Lu", "Nian Liu", "Zhikai Li", "Chaoyu Gong", "Suyun Zhao", "Hong Chen", "Cuiping Li", "Kai Wang", "Yang You"], "title": "Unsupervised Learning for Class Distribution Mismatch", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted by ICML 2025", "summary": "Class distribution mismatch (CDM) refers to the discrepancy between class\ndistributions in training data and target tasks. Previous methods address this\nby designing classifiers to categorize classes known during training, while\ngrouping unknown or new classes into an \"other\" category. However, they focus\non semi-supervised scenarios and heavily rely on labeled data, limiting their\napplicability and performance. To address this, we propose Unsupervised\nLearning for Class Distribution Mismatch (UCDM), which constructs\npositive-negative pairs from unlabeled data for classifier training. Our\napproach randomly samples images and uses a diffusion model to add or erase\nsemantic classes, synthesizing diverse training pairs. Additionally, we\nintroduce a confidence-based labeling mechanism that iteratively assigns\npseudo-labels to valuable real-world data and incorporates them into the\ntraining process. Extensive experiments on three datasets demonstrate UCDM's\nsuperiority over previous semi-supervised methods. Specifically, with a 60%\nmismatch proportion on Tiny-ImageNet dataset, our approach, without relying on\nlabeled data, surpasses OpenMatch (with 40 labels per class) by 35.1%, 63.7%,\nand 72.5% in classifying known, unknown, and new classes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5UCDM\uff0c\u901a\u8fc7\u6784\u5efa\u6b63\u8d1f\u6837\u672c\u5bf9\u5e76\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u6837\u8bad\u7ec3\u6570\u636e\uff0c\u89e3\u51b3\u7c7b\u522b\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6548\u679c\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u534a\u76d1\u7763\u65b9\u6cd5\u3002", "motivation": "\u9488\u5bf9\u7c7b\u522b\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6807\u8bb0\u6570\u636e\u548c\u534a\u76d1\u7763\u5b66\u4e60\uff0c\u5e94\u7528\u53d7\u9650\u4e14\u6027\u80fd\u4e0d\u8db3\uff0c\u56e0\u6b64\u63d0\u51faUCDM\u65b9\u6cd5\u4ee5\u5b8c\u5168\u65e0\u76d1\u7763\u65b9\u5f0f\u89e3\u51b3\u8be5\u95ee\u9898\u3002", "method": "UCDM\u901a\u8fc7\u968f\u673a\u91c7\u6837\u56fe\u50cf\u5e76\u5229\u7528\u6269\u6563\u6a21\u578b\u589e\u51cf\u8bed\u4e49\u7c7b\u522b\u6784\u5efa\u8bad\u7ec3\u5bf9\uff0c\u540c\u65f6\u5f15\u5165\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u4f2a\u6807\u7b7e\u673a\u5236\u8fed\u4ee3\u4f18\u5316\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728Tiny-ImageNet\u7b493\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cUCDM\u572860%\u4e0d\u5339\u914d\u6bd4\u4f8b\u4e0b\uff0c\u5bf9\u5df2\u77e5\u3001\u672a\u77e5\u548c\u65b0\u7c7b\u522b\u7684\u5206\u7c7b\u51c6\u786e\u7387\u5206\u522b\u8d85\u8fc7OpenMatch 35.1%\u300163.7%\u548c72.5%\u3002", "conclusion": "UCDM\u5728\u65e0\u76d1\u7763\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u7c7b\u522b\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\u7684\u5904\u7406\u80fd\u529b\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.07457", "pdf": "https://arxiv.org/pdf/2505.07457", "abs": "https://arxiv.org/abs/2505.07457", "authors": ["R. Maria del Rio-Chanona", "Marco Pangallo", "Cars Hommes"], "title": "Can Generative AI agents behave like humans? Evidence from laboratory market experiments", "categories": ["econ.GN", "cs.AI", "q-fin.EC"], "comment": null, "summary": "We explore the potential of Large Language Models (LLMs) to replicate human\nbehavior in economic market experiments. Compared to previous studies, we focus\non dynamic feedback between LLM agents: the decisions of each LLM impact the\nmarket price at the current step, and so affect the decisions of the other LLMs\nat the next step. We compare LLM behavior to market dynamics observed in\nlaboratory settings and assess their alignment with human participants'\nbehavior. Our findings indicate that LLMs do not adhere strictly to rational\nexpectations, displaying instead bounded rationality, similarly to human\nparticipants. Providing a minimal context window i.e. memory of three previous\ntime steps, combined with a high variability setting capturing response\nheterogeneity, allows LLMs to replicate broad trends seen in human experiments,\nsuch as the distinction between positive and negative feedback markets.\nHowever, differences remain at a granular level--LLMs exhibit less\nheterogeneity in behavior than humans. These results suggest that LLMs hold\npromise as tools for simulating realistic human behavior in economic contexts,\nthough further research is needed to refine their accuracy and increase\nbehavioral diversity.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7ecf\u6d4e\u5b66\u5e02\u573a\u5b9e\u9a8c\u4e2d\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u7684\u6f5c\u529b\uff0c\u53d1\u73b0LLMs\u5728\u52a8\u6001\u53cd\u9988\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u7c7b\u4f3c\u7684\u6709\u9650\u7406\u6027\u884c\u4e3a\uff0c\u4f46\u884c\u4e3a\u591a\u6837\u6027\u8f83\u4f4e\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22LLMs\u662f\u5426\u80fd\u6a21\u62df\u4eba\u7c7b\u5728\u7ecf\u6d4e\u5b66\u5b9e\u9a8c\u4e2d\u7684\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u53cd\u9988\u7684\u5e02\u573a\u73af\u5883\u4e2d\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6a21\u62dfLLM\u4ee3\u7406\u4e4b\u95f4\u7684\u52a8\u6001\u4e92\u52a8\uff0c\u6bd4\u8f83\u5b83\u4eec\u5728\u5e02\u573a\u5b9e\u9a8c\u4e2d\u7684\u884c\u4e3a\u4e0e\u4eba\u7c7b\u53c2\u4e0e\u8005\u7684\u884c\u4e3a\uff0c\u5e76\u8bc4\u4f30\u5176\u5bf9\u5e02\u573a\u52a8\u6001\u7684\u9002\u5e94\u6027\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cLLMs\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u7c7b\u4f3c\u7684\u6709\u9650\u7406\u6027\u884c\u4e3a\uff0c\u4f46\u884c\u4e3a\u591a\u6837\u6027\u4e0d\u5982\u4eba\u7c7b\uff1b\u5176\u884c\u4e3a\u8d8b\u52bf\u4e0e\u4eba\u7c7b\u5b9e\u9a8c\u7ed3\u679c\u76f8\u4f3c\uff0c\u4f46\u5728\u7ec6\u8282\u4e0a\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u7ed3\u8bba\u662fLLMs\u5728\u6a21\u62df\u4eba\u7c7b\u7ecf\u6d4e\u884c\u4e3a\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u884c\u4e3a\u591a\u6837\u6027\u3002"}}
{"id": "2505.06958", "pdf": "https://arxiv.org/pdf/2505.06958", "abs": "https://arxiv.org/abs/2505.06958", "authors": ["James Tobler", "Hira Taqdees Syeda", "Toby Murray"], "title": "A Formally Verified Robustness Certifier for Neural Networks (Extended Version)", "categories": ["cs.PL", "cs.LG"], "comment": null, "summary": "Neural networks are often susceptible to minor perturbations in input that\ncause them to misclassify. A recent solution to this problem is the use of\nglobally-robust neural networks, which employ a function to certify that the\nclassification of an input cannot be altered by such a perturbation. Outputs\nthat pass this test are called certified robust. However, to the authors'\nknowledge, these certification functions have not yet been verified at the\nimplementation level. We demonstrate how previous unverified implementations\nare exploitably unsound in certain circumstances. Moreover, they often rely on\napproximation-based algorithms, such as power iteration, that (perhaps\nsurprisingly) do not guarantee soundness. To provide assurance that a given\noutput is robust, we implemented and formally verified a certification function\nfor globally-robust neural networks in Dafny. We describe the program, its\nspecifications, and the important design decisions taken for its implementation\nand verification, as well as our experience applying it in practice.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51fa\u5f53\u524d\u5168\u5c40\u9c81\u68d2\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u8ba4\u8bc1\u51fd\u6570\u5b9e\u73b0\u672a\u7ecf\u9a8c\u8bc1\uff0c\u5b58\u5728\u6f0f\u6d1e\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728Dafny\u4e2d\u5b9e\u73b0\u5e76\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u8ba4\u8bc1\u51fd\u6570\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u6613\u53d7\u5fae\u5c0f\u8f93\u5165\u6270\u52a8\u5f71\u54cd\uff0c\u5bfc\u81f4\u9519\u8bef\u5206\u7c7b\u3002\u5f53\u524d\u5168\u5c40\u9c81\u68d2\u795e\u7ecf\u7f51\u7edc\u7684\u8ba4\u8bc1\u51fd\u6570\u867d\u80fd\u4fdd\u8bc1\u8f93\u51fa\u9c81\u68d2\u6027\uff0c\u4f46\u5176\u5b9e\u73b0\u672a\u7ecf\u9a8c\u8bc1\uff0c\u53ef\u80fd\u5b58\u5728\u6f0f\u6d1e\u3002", "method": "\u4f5c\u8005\u5b9e\u73b0\u5e76\u5f62\u5f0f\u5316\u9a8c\u8bc1\u4e86\u4e00\u4e2a\u7528\u4e8e\u5168\u5c40\u9c81\u68d2\u795e\u7ecf\u7f51\u7edc\u7684\u8ba4\u8bc1\u51fd\u6570\uff0c\u4f7f\u7528Dafny\u8bed\u8a00\u7f16\u5199\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u8bbe\u8ba1\u51b3\u7b56\u548c\u5b9e\u8df5\u5e94\u7528\u7ecf\u9a8c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e4b\u524d\u7684\u8ba4\u8bc1\u51fd\u6570\u5b9e\u73b0\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u5b58\u5728\u6f0f\u6d1e\uff0c\u4e14\u4f9d\u8d56\u7684\u8fd1\u4f3c\u7b97\u6cd5\uff08\u5982\u5e42\u8fed\u4ee3\uff09\u5e76\u4e0d\u80fd\u4fdd\u8bc1\u51c6\u786e\u6027\u3002\u65b0\u63d0\u51fa\u7684\u9a8c\u8bc1\u65b9\u6cd5\u89e3\u51b3\u4e86\u8fd9\u4e9b\u95ee\u9898\u3002", "conclusion": "\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u8ba4\u8bc1\u51fd\u6570\u4e3a\u5168\u5c40\u9c81\u68d2\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u9c81\u68d2\u6027\u4fdd\u8bc1\uff0c\u89e3\u51b3\u4e86\u4e4b\u524d\u5b9e\u73b0\u4e2d\u7684\u6f0f\u6d1e\u95ee\u9898\u3002"}}
{"id": "2505.07511", "pdf": "https://arxiv.org/pdf/2505.07511", "abs": "https://arxiv.org/abs/2505.07511", "authors": ["Mauricio Orbes-Arteaga", "Oeslle Lucena", "Sabastien Ourselin", "M. Jorge Cardoso"], "title": "MAIS: Memory-Attention for Interactive Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Interactive medical segmentation reduces annotation effort by refining\npredictions through user feedback. Vision Transformer (ViT)-based models, such\nas the Segment Anything Model (SAM), achieve state-of-the-art performance using\nuser clicks and prior masks as prompts. However, existing methods treat\ninteractions as independent events, leading to redundant corrections and\nlimited refinement gains. We address this by introducing MAIS, a\nMemory-Attention mechanism for Interactive Segmentation that stores past user\ninputs and segmentation states, enabling temporal context integration. Our\napproach enhances ViT-based segmentation across diverse imaging modalities,\nachieving more efficient and accurate refinements.", "AI": {"tldr": "MAIS\u5f15\u5165\u4e86\u4e00\u79cd\u8bb0\u5fc6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u5b58\u50a8\u8fc7\u53bb\u7684\u7528\u6237\u8f93\u5165\u548c\u5206\u5272\u72b6\u6001\u6765\u6574\u5408\u65f6\u95f4\u4e0a\u4e0b\u6587\uff0c\u63d0\u9ad8\u4e86\u4ea4\u4e92\u5f0f\u533b\u5b66\u5206\u5272\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4ea4\u4e92\u5f0f\u5206\u5272\u65b9\u6cd5\u5c06\u7528\u6237\u4ea4\u4e92\u89c6\u4e3a\u72ec\u7acb\u4e8b\u4ef6\uff0c\u5bfc\u81f4\u5197\u4f59\u4fee\u6b63\u548c\u6539\u8fdb\u6709\u9650\uff0cMAIS\u65e8\u5728\u901a\u8fc7\u6574\u5408\u65f6\u95f4\u4e0a\u4e0b\u6587\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "MAIS\u5229\u7528\u8bb0\u5fc6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b58\u50a8\u7528\u6237\u8fc7\u53bb\u7684\u8f93\u5165\u548c\u5206\u5272\u72b6\u6001\uff0c\u4ece\u800c\u5728Vision Transformer\uff08ViT\uff09\u67b6\u6784\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u4ea4\u4e92\u5f0f\u5206\u5272\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u7684\u6210\u50cf\u6a21\u6001\u4e2d\u63d0\u5347\u4e86ViT\u5206\u5272\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u548c\u66f4\u51c6\u786e\u7684\u4fee\u6b63\u3002", "conclusion": "MAIS\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u4e0a\u4e0b\u6587\u6539\u8fdb\u4e86\u4ea4\u4e92\u5f0f\u533b\u5b66\u5206\u5272\uff0c\u51cf\u5c11\u4e86\u5197\u4f59\u4fee\u6b63\u5e76\u63d0\u5347\u4e86\u5206\u5272\u7cbe\u5ea6\u3002"}}
{"id": "2505.07001", "pdf": "https://arxiv.org/pdf/2505.07001", "abs": "https://arxiv.org/abs/2505.07001", "authors": ["Bidur Khanal", "Sandesh Pokhrel", "Sanjay Bhandari", "Ramesh Rana", "Nikesh Shrestha", "Ram Bahadur Gurung", "Cristian Linte", "Angus Watson", "Yash Raj Shrestha", "Binod Bhattarai"], "title": "Hallucination-Aware Multimodal Benchmark for Gastrointestinal Image Analysis with Large Vision-Language Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Vision-Language Models (VLMs) are becoming increasingly popular in the\nmedical domain, bridging the gap between medical images and clinical language.\nExisting VLMs demonstrate an impressive ability to comprehend medical images\nand text queries to generate detailed, descriptive diagnostic medical reports.\nHowever, hallucination--the tendency to generate descriptions that are\ninconsistent with the visual content--remains a significant issue in VLMs, with\nparticularly severe implications in the medical field. To facilitate VLM\nresearch on gastrointestinal (GI) image analysis and study hallucination, we\ncurate a multimodal image-text GI dataset: Gut-VLM. This dataset is created\nusing a two-stage pipeline: first, descriptive medical reports of Kvasir-v2\nimages are generated using ChatGPT, which introduces some hallucinated or\nincorrect texts. In the second stage, medical experts systematically review\nthese reports, and identify and correct potential inaccuracies to ensure\nhigh-quality, clinically reliable annotations. Unlike traditional datasets that\ncontain only descriptive texts, our dataset also features tags identifying\nhallucinated sentences and their corresponding corrections. A common approach\nto reducing hallucination in VLM is to finetune the model on a small-scale,\nproblem-specific dataset. However, we take a different strategy using our\ndataset. Instead of finetuning the VLM solely for generating textual reports,\nwe finetune it to detect and correct hallucinations, an approach we call\nhallucination-aware finetuning. Our results show that this approach is better\nthan simply finetuning for descriptive report generation. Additionally, we\nconduct an extensive evaluation of state-of-the-art VLMs across several\nmetrics, establishing a benchmark. GitHub Repo:\nhttps://github.com/bhattarailab/Hallucination-Aware-VLM.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u80c3\u80a0\u9053\u56fe\u50cf\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6Gut-VLM\uff0c\u5e76\u91c7\u7528\u5e7b\u89c9\u611f\u77e5\u5fae\u8c03\u7b56\u7565\u6765\u51cf\u5c11\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u533b\u5b66\u62a5\u544a\u751f\u6210\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u7684\u751f\u6210\u5fae\u8c03\u3002", "motivation": "\u533b\u5b66\u9886\u57df\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u8bca\u65ad\u62a5\u544a\u65f6\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff08\u751f\u6210\u4e0e\u56fe\u50cf\u5185\u5bb9\u4e0d\u7b26\u7684\u63cf\u8ff0\uff09\uff0c\u8fd9\u5bf9\u533b\u7597\u5e94\u7528\u5177\u6709\u4e25\u91cd\u98ce\u9669\u3002\u4e3a\u4e86\u7814\u7a76\u5e76\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u80c3\u80a0\u9053\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u5fae\u8c03\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u4e24\u9636\u6bb5\u6d41\u7a0b\u6784\u5efaGut-VLM\u6570\u636e\u96c6\uff1a1\uff09\u7528ChatGPT\u751f\u6210\u5e26\u6709\u5e7b\u89c9\u7684\u533b\u7597\u62a5\u544a\uff1b2\uff09\u533b\u5b66\u4e13\u5bb6\u5ba1\u6838\u5e76\u6821\u6b63\u3002\u63d0\u51fa\u201c\u5e7b\u89c9\u611f\u77e5\u5fae\u8c03\u201d\u7b56\u7565\uff0c\u8bad\u7ec3\u6a21\u578b\u68c0\u6d4b\u5e76\u7ea0\u6b63\u5e7b\u89c9\uff0c\u800c\u975e\u5355\u7eaf\u751f\u6210\u62a5\u544a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5e7b\u89c9\u611f\u77e5\u5fae\u8c03\u65b9\u6cd5\u5728\u51cf\u5c11\u5e7b\u89c9\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u751f\u6210\u5fae\u8c03\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u5bf9\u5f53\u524d\u5148\u8fdb\u7684VLM\u6a21\u578b\u8fdb\u884c\u4e86\u591a\u6307\u6807\u8bc4\u4f30\uff0c\u5efa\u7acb\u4e86\u57fa\u51c6\u3002", "conclusion": "Gut-VLM\u6570\u636e\u96c6\u548c\u5e7b\u89c9\u611f\u77e5\u5fae\u8c03\u7b56\u7565\u4e3a\u533b\u5b66VLM\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u663e\u8457\u6539\u5584\u4e86\u6a21\u578b\u8f93\u51fa\u4e0e\u56fe\u50cf\u5185\u5bb9\u7684\u4e00\u81f4\u6027\u3002\u8fd9\u79cd\u65b9\u6cd5\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u533b\u5b66\u5f71\u50cf\u9886\u57df\u3002"}}
{"id": "2505.07011", "pdf": "https://arxiv.org/pdf/2505.07011", "abs": "https://arxiv.org/abs/2505.07011", "authors": ["Maximilian Egger", "Svenja Lage", "Rawad Bitar", "Antonia Wachter-Zeh"], "title": "Source Anonymity for Private Random Walk Decentralized Learning", "categories": ["cs.CR", "cs.DC", "cs.IT", "cs.LG", "math.IT", "stat.ML"], "comment": null, "summary": "This paper considers random walk-based decentralized learning, where at each\niteration of the learning process, one user updates the model and sends it to a\nrandomly chosen neighbor until a convergence criterion is met. Preserving data\nprivacy is a central concern and open problem in decentralized learning. We\npropose a privacy-preserving algorithm based on public-key cryptography and\nanonymization. In this algorithm, the user updates the model and encrypts the\nresult using a distant user's public key. The encrypted result is then\ntransmitted through the network with the goal of reaching that specific user.\nThe key idea is to hide the source's identity so that, when the destination\nuser decrypts the result, it does not know who the source was. The challenge is\nto design a network-dependent probability distribution (at the source) over the\npotential destinations such that, from the receiver's perspective, all users\nhave a similar likelihood of being the source. We introduce the problem and\nconstruct a scheme that provides anonymity with theoretical guarantees. We\nfocus on random regular graphs to establish rigorous guarantees.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u6f2b\u6b65\u7684\u9690\u79c1\u4fdd\u62a4\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u5408\u516c\u94a5\u52a0\u5bc6\u548c\u533f\u540d\u5316\u6280\u672f\uff0c\u786e\u4fdd\u5728\u6a21\u578b\u66f4\u65b0\u8fc7\u7a0b\u4e2d\u6e90\u7528\u6237\u8eab\u4efd\u7684\u9690\u533f\u6027\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u662f\u4e00\u4e2a\u91cd\u8981\u4f46\u672a\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u7528\u6237\u968f\u673a\u9009\u62e9\u90bb\u5c45\u8fdb\u884c\u6a21\u578b\u66f4\u65b0\u7684\u573a\u666f\u4e2d\uff0c\u5982\u4f55\u9690\u85cf\u6e90\u7528\u6237\u8eab\u4efd\u662f\u5173\u952e\u6311\u6218\u3002", "method": "\u7b97\u6cd5\u5229\u7528\u516c\u94a5\u52a0\u5bc6\u548c\u533f\u540d\u5316\u6280\u672f\uff0c\u6e90\u7528\u6237\u7528\u76ee\u6807\u7528\u6237\u7684\u516c\u94a5\u52a0\u5bc6\u6a21\u578b\u66f4\u65b0\u7ed3\u679c\uff0c\u5e76\u901a\u8fc7\u7f51\u7edc\u4f20\u9012\uff0c\u786e\u4fdd\u76ee\u6807\u7528\u6237\u65e0\u6cd5\u8ffd\u8e2a\u6e90\u7528\u6237\u8eab\u4efd\u3002\u901a\u8fc7\u8bbe\u8ba1\u7f51\u7edc\u76f8\u5173\u7684\u6982\u7387\u5206\u5e03\uff0c\u4fdd\u8bc1\u63a5\u6536\u8005\u8ba4\u4e3a\u6240\u6709\u7528\u6237\u4f5c\u4e3a\u6e90\u7684\u53ef\u80fd\u6027\u76f8\u8fd1\u3002", "result": "\u5728\u968f\u673a\u6b63\u5219\u56fe\u4e0a\uff0c\u6240\u63d0\u65b9\u6848\u5b9e\u73b0\u4e86\u7406\u8bba\u4fdd\u8bc1\u7684\u533f\u540d\u6027\uff0c\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u968f\u673a\u7f51\u7edc\u62d3\u6251\u4e0b\u5177\u6709\u7406\u8bba\u4fdd\u969c\u7684\u9690\u533f\u6027\u3002"}}
{"id": "2505.07533", "pdf": "https://arxiv.org/pdf/2505.07533", "abs": "https://arxiv.org/abs/2505.07533", "authors": ["Ahmad Fall", "Federica Granese", "Alex Lence", "Dominique Fourer", "Blaise Hanczar", "Joe-Elie Salem", "Jean-Daniel Zucker", "Edi Prifti"], "title": "IKrNet: A Neural Network for Detecting Specific Drug-Induced Patterns in Electrocardiograms Amidst Physiological Variability", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Monitoring and analyzing electrocardiogram (ECG) signals, even under varying\nphysiological conditions, including those influenced by physical activity,\ndrugs and stress, is crucial to accurately assess cardiac health. However,\ncurrent AI-based methods often fail to account for how these factors interact\nand alter ECG patterns, ultimately limiting their applicability in real-world\nsettings. This study introduces IKrNet, a novel neural network model, which\nidentifies drug-specific patterns in ECGs amidst certain physiological\nconditions. IKrNet's architecture incorporates spatial and temporal dynamics by\nusing a convolutional backbone with varying receptive field size to capture\nspatial features. A bi-directional Long Short-Term Memory module is also\nemployed to model temporal dependencies. By treating heart rate variability as\na surrogate for physiological fluctuations, we evaluated IKrNet's performance\nacross diverse scenarios, including conditions with physical stress, drug\nintake alone, and a baseline without drug presence. Our assessment follows a\nclinical protocol in which 990 healthy volunteers were administered 80mg of\nSotalol, a drug which is known to be a precursor to Torsades-de-Pointes, a\nlife-threatening arrhythmia. We show that IKrNet outperforms state-of-the-art\nmodels' accuracy and stability in varying physiological conditions,\nunderscoring its clinical viability.", "AI": {"tldr": "IKrNet\u662f\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u548c\u65f6\u95f4\u52a8\u6001\u5206\u6790\u5fc3\u7535\u56fe\uff08ECG\uff09\u4e2d\u7684\u836f\u7269\u7279\u5b9a\u6a21\u5f0f\uff0c\u5728\u591a\u79cd\u751f\u7406\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709AI\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u8003\u8651\u751f\u7406\u6761\u4ef6\uff08\u5982\u4f53\u529b\u6d3b\u52a8\u3001\u836f\u7269\u548c\u538b\u529b\uff09\u5bf9ECG\u6a21\u5f0f\u7684\u4ea4\u4e92\u5f71\u54cd\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u9002\u5e94\u8fd9\u4e9b\u53d8\u5316\u7684\u6a21\u578b\u3002", "method": "IKrNet\u7ed3\u5408\u4e86\u5377\u79ef\u4e3b\u5e72\uff08\u6355\u6349\u7a7a\u95f4\u7279\u5f81\uff09\u548c\u53cc\u5411\u957f\u77ed\u65f6\u8bb0\u5fc6\u6a21\u5757\uff08\u5efa\u6a21\u65f6\u95f4\u4f9d\u8d56\uff09\uff0c\u5e76\u4ee5\u5fc3\u7387\u53d8\u5f02\u6027\u4f5c\u4e3a\u751f\u7406\u6ce2\u52a8\u7684\u66ff\u4ee3\u6307\u6807\u3002\u6a21\u578b\u5728\u5305\u542b\u4f53\u529b\u538b\u529b\u3001\u836f\u7269\u6444\u5165\u548c\u57fa\u7ebf\u6761\u4ef6\u7684\u591a\u6837\u5316\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u5728990\u540d\u5065\u5eb7\u5fd7\u613f\u8005\uff08\u670d\u752880mg\u7d22\u4ed6\u6d1b\u5c14\uff09\u7684\u4e34\u5e8a\u534f\u8bae\u4e0b\uff0cIKrNet\u5728\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u6a21\u578b\u3002", "conclusion": "IKrNet\u5728\u591a\u53d8\u751f\u7406\u6761\u4ef6\u4e0b\u7684\u4f18\u5f02\u8868\u73b0\u8bc1\u660e\u4e86\u5176\u4e34\u5e8a\u53ef\u884c\u6027\uff0c\u4e3a\u89e3\u51b3ECG\u5206\u6790\u7684\u73b0\u5b9e\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2505.07534", "pdf": "https://arxiv.org/pdf/2505.07534", "abs": "https://arxiv.org/abs/2505.07534", "authors": ["J\u00fcrgen Bernard"], "title": "The Human-Data-Model Interaction Canvas for Visual Analytics", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "7 pages, 5 figures, LaTeX; to appear at the 16th International\n  EuroVis Workshop on Visual Analytics (EuroVA'25) as a position paper", "summary": "Visual Analytics (VA) integrates humans, data, and models as key actors in\ninsight generation and data-driven decision-making. This position paper values\nand reflects on 16 VA process models and frameworks and makes nine high-level\nobservations that motivate a fresh perspective on VA. The contribution is the\nHDMI Canvas, a perspective to VA that complements the strengths of existing VA\nprocess models and frameworks. It systematically characterizes diverse roles of\nhumans, data, and models, and how these actors benefit from and contribute to\nVA processes. The descriptive power of the HDMI Canvas eases the\ndifferentiation between a series of VA building blocks, rather than describing\ngeneral VA principles only. The canvas includes modern human-centered\nmethodologies, including human knowledge externalization and forms of feedback\nloops, while interpretable and explainable AI highlight model contributions\nbeyond their conventional outputs. The HDMI Canvas has generative power,\nguiding the design of new VA processes and is optimized for external\nstakeholders, improving VA outreach, interdisciplinary collaboration, and\nuser-centered design. The utility of the HDMI Canvas is demonstrated through\ntwo preliminary case studies.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86HDMI Canvas\uff0c\u4f5c\u4e3a\u89c6\u89c9\u5206\u6790\uff08VA\uff09\u7684\u65b0\u89c6\u89d2\uff0c\u901a\u8fc7\u6574\u5408\u4eba\u7c7b\u3001\u6570\u636e\u548c\u6a21\u578b\u89d2\u8272\u6765\u4f18\u5316VA\u6d41\u7a0b\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u5176\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u9274\u4e8e\u73b0\u6709VA\u6d41\u7a0b\u6a21\u578b\u548c\u6846\u67b6\u7684\u5c40\u9650\u6027\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7HDMI Canvas\u63d0\u4f9b\u4e00\u4e2a\u66f4\u7cfb\u7edf\u5316\u7684\u89c6\u89d2\uff0c\u4ee5\u589e\u5f3aVA\u5728\u8de8\u5b66\u79d1\u534f\u4f5c\u548c\u7528\u6237\u4e2d\u5fc3\u8bbe\u8ba1\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u57fa\u4e8e\u5bf916\u4e2aVA\u6d41\u7a0b\u6a21\u578b\u548c\u6846\u67b6\u7684\u5206\u6790\uff0c\u4f5c\u8005\u63d0\u51faHDMI Canvas\uff0c\u5f3a\u8c03\u4eba\u7c7b\u3001\u6570\u636e\u548c\u6a21\u578b\u7684\u591a\u6837\u5316\u89d2\u8272\u53ca\u5176\u5728VA\u4e2d\u7684\u4e92\u52a8\u3002\u7ed3\u5408\u73b0\u4ee3\u4eba\u672c\u65b9\u6cd5\u548c\u53ef\u89e3\u91caAI\uff0c\u5c55\u793a\u4e86\u751f\u6210\u65b0VA\u6d41\u7a0b\u7684\u8bbe\u8ba1\u6307\u5bfc\u3002", "result": "HDMI Canvas\u901a\u8fc7\u4e24\u4e2a\u521d\u6b65\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u63cf\u8ff0\u6027\u548c\u751f\u6210\u6027\u80fd\u529b\uff0c\u80fd\u591f\u6e05\u6670\u533a\u5206VA\u6784\u5efa\u5757\uff0c\u5e76\u652f\u6301\u8de8\u5b66\u79d1\u534f\u4f5c\u548c\u7528\u6237\u4e2d\u5fc3\u8bbe\u8ba1\u3002", "conclusion": "HDMI Canvas\u4e0d\u4ec5\u8865\u8db3\u4e86\u73b0\u6709VA\u6a21\u578b\u548c\u6846\u67b6\u7684\u4e0d\u8db3\uff0c\u8fd8\u4e3a\u8bbe\u8ba1\u65b0VA\u6d41\u7a0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u63d0\u5347\u4e86VA\u7684\u5916\u90e8\u4f20\u64ad\u529b\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.07546", "pdf": "https://arxiv.org/pdf/2505.07546", "abs": "https://arxiv.org/abs/2505.07546", "authors": ["Jingjie Zheng", "Aryo Pradipta Gema", "Giwon Hong", "Xuanli He", "Pasquale Minervini", "Youcheng Sun", "Qiongkai Xu"], "title": "GRADA: Graph-based Reranker against Adversarial Documents Attack", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Retrieval Augmented Generation (RAG) frameworks improve the accuracy of large\nlanguage models (LLMs) by integrating external knowledge from retrieved\ndocuments, thereby overcoming the limitations of models' static intrinsic\nknowledge. However, these systems are susceptible to adversarial attacks that\nmanipulate the retrieval process by introducing documents that are adversarial\nyet semantically similar to the query. Notably, while these adversarial\ndocuments resemble the query, they exhibit weak similarity to benign documents\nin the retrieval set. Thus, we propose a simple yet effective Graph-based\nReranking against Adversarial Document Attacks (GRADA) framework aiming at\npreserving retrieval quality while significantly reducing the success of\nadversaries. Our study evaluates the effectiveness of our approach through\nexperiments conducted on five LLMs: GPT-3.5-Turbo, GPT-4o, Llama3.1-8b,\nLlama3.1-70b, and Qwen2.5-7b. We use three datasets to assess performance, with\nresults from the Natural Questions dataset demonstrating up to an 80% reduction\nin attack success rates while maintaining minimal loss in accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGRADA\u7684\u56fe\u91cd\u6392\u5e8f\u6846\u67b6\uff0c\u7528\u4e8e\u62b5\u5fa1\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u4e2d\u7684\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5bf9\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "motivation": "RAG\u7cfb\u7edf\u867d\u7136\u80fd\u63d0\u5347LLMs\u7684\u51c6\u786e\u6027\uff0c\u4f46\u6613\u53d7\u5bf9\u6297\u6027\u653b\u51fb\u7684\u5f71\u54cd\uff0c\u653b\u51fb\u8005\u901a\u8fc7\u5f15\u5165\u8bed\u4e49\u76f8\u4f3c\u4f46\u5bf9\u6297\u6027\u7684\u6587\u6863\u5e72\u6270\u68c0\u7d22\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u56fe\u7684\u91cd\u6392\u5e8f\uff08GRADA\uff09\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u68c0\u7d22\u8d28\u91cf\u7684\u540c\u65f6\u51cf\u5c11\u5bf9\u6297\u653b\u51fb\u7684\u6210\u529f\u7387\u3002", "result": "\u5728\u4e94\u4e2aLLMs\u548c\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGRADA\u80fd\u5c06\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e80%\uff0c\u4e14\u51c6\u786e\u6027\u635f\u5931\u6781\u5c0f\u3002", "conclusion": "GRADA\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u663e\u8457\u63d0\u5347RAG\u7cfb\u7edf\u5bf9\u6297\u5bf9\u6297\u6027\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2505.07033", "pdf": "https://arxiv.org/pdf/2505.07033", "abs": "https://arxiv.org/abs/2505.07033", "authors": ["Ningsheng Zhao", "Trang Bui", "Jia Yuan Yu", "Krzysztof Dzieciolowski"], "title": "Outperformance Score: A Universal Standardization Method for Confusion-Matrix-Based Classification Performance Metrics", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "Many classification performance metrics exist, each suited to a specific\napplication. However, these metrics often differ in scale and can exhibit\nvarying sensitivity to class imbalance rates in the test set. As a result, it\nis difficult to use the nominal values of these metrics to interpret and\nevaluate classification performances, especially when imbalance rates vary. To\naddress this problem, we introduce the outperformance score function, a\nuniversal standardization method for confusion-matrix-based classification\nperformance (CMBCP) metrics. It maps any given metric to a common scale of\n$[0,1]$, while providing a clear and consistent interpretation. Specifically,\nthe outperformance score represents the percentile rank of the observed\nclassification performance within a reference distribution of possible\nperformances. This unified framework enables meaningful comparison and\nmonitoring of classification performance across test sets with differing\nimbalance rates. We illustrate how the outperformance scores can be applied to\na variety of commonly used classification performance metrics and demonstrate\nthe robustness of our method through experiments on real-world datasets\nspanning multiple classification applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u6807\u51c6\u5316\u65b9\u6cd5\u2014\u2014\u8d85\u8868\u73b0\u5206\u6570\u51fd\u6570\uff0c\u7528\u4e8e\u5c06\u57fa\u4e8e\u6df7\u6dc6\u77e9\u9635\u7684\u5206\u7c7b\u6027\u80fd\u6307\u6807\u6620\u5c04\u5230\u7edf\u4e00\u5c3a\u5ea6[0,1]\u4e0a\uff0c\u89e3\u51b3\u4e86\u4e0d\u540c\u6307\u6807\u56e0\u91cf\u7eb2\u548c\u7c7b\u4e0d\u5e73\u8861\u7387\u5dee\u5f02\u5bfc\u81f4\u7684\u6027\u80fd\u8bc4\u4f30\u96be\u9898\u3002", "motivation": "\u73b0\u6709\u5206\u7c7b\u6027\u80fd\u6307\u6807\u56e0\u91cf\u7eb2\u548c\u7c7b\u4e0d\u5e73\u8861\u7387\u654f\u611f\u5ea6\u4e0d\u540c\uff0c\u96be\u4ee5\u5728\u6d4b\u8bd5\u96c6\u4e0d\u5e73\u8861\u7387\u53d8\u5316\u65f6\u8fdb\u884c\u4e00\u81f4\u8bc4\u4f30\u3002", "method": "\u5f15\u5165\u4e86\u8d85\u8868\u73b0\u5206\u6570\u51fd\u6570\uff0c\u5c06\u6307\u6807\u6807\u51c6\u5316\u4e3a[0,1]\u7684\u767e\u5206\u4f4d\u6392\u540d\uff0c\u5b9e\u73b0\u8de8\u6d4b\u8bd5\u96c6\u7684\u6027\u80fd\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u9c81\u68d2\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "\u8d85\u8868\u73b0\u5206\u6570\u4e3a\u5206\u7c7b\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7edf\u4e00\u4e14\u76f4\u89c2\u7684\u6846\u67b6\u3002"}}
{"id": "2505.07552", "pdf": "https://arxiv.org/pdf/2505.07552", "abs": "https://arxiv.org/abs/2505.07552", "authors": ["Efe Bozkir", "Christian Kosel", "Tina Seidel", "Enkelejda Kasneci"], "title": "Automated Visual Attention Detection using Mobile Eye Tracking in Behavioral Classroom Studies", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "Accepted as a long paper at the Educational Data Mining (EDM)\n  Conference 2025", "summary": "Teachers' visual attention and its distribution across the students in\nclassrooms can constitute important implications for student engagement,\nachievement, and professional teacher training. Despite that, inferring the\ninformation about where and which student teachers focus on is not trivial.\nMobile eye tracking can provide vital help to solve this issue; however, the\nuse of mobile eye tracking alone requires a significant amount of manual\nannotations. To address this limitation, we present an automated processing\npipeline concept that requires minimal manually annotated data to recognize\nwhich student the teachers focus on. To this end, we utilize state-of-the-art\nface detection models and face recognition feature embeddings to train face\nrecognition models with transfer learning in the classroom context and combine\nthese models with the teachers' gaze from mobile eye trackers. We evaluated our\napproach with data collected from four different classrooms, and our results\nshow that while it is possible to estimate the visually focused students with\nreasonable performance in all of our classroom setups, U-shaped and small\nclassrooms led to the best results with accuracies of approximately 0.7 and\n0.9, respectively. While we did not evaluate our method for teacher-student\ninteractions and focused on the validity of the technical approach, as our\nmethodology does not require a vast amount of manually annotated data and\noffers a non-intrusive way of handling teachers' visual attention, it could\nhelp improve instructional strategies, enhance classroom management, and\nprovide feedback for professional teacher development.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u5904\u7406\u6d41\u7a0b\uff0c\u5229\u7528\u9762\u90e8\u68c0\u6d4b\u548c\u8bc6\u522b\u6280\u672f\u7ed3\u5408\u6559\u5e08\u773c\u52a8\u6570\u636e\uff0c\u4ee5\u6700\u5c0f\u624b\u52a8\u6807\u6ce8\u8bc6\u522b\u6559\u5e08\u5173\u6ce8\u7684\u5b66\u751f\u3002\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u6559\u5ba4\u5e03\u5c40\u4e2d\u8868\u73b0\u826f\u597d\uff0cU\u578b\u548c\u5c0f\u578b\u6559\u5ba4\u51c6\u786e\u7387\u6700\u9ad8\u3002", "motivation": "\u6559\u5e08\u89c6\u89c9\u6ce8\u610f\u529b\u5206\u5e03\u5bf9\u5b66\u751f\u53c2\u4e0e\u548c\u6210\u7ee9\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u4f46\u624b\u52a8\u6807\u6ce8\u773c\u52a8\u6570\u636e\u8017\u65f6\u8d39\u529b\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u6280\u672f\u51cf\u5c11\u624b\u52a8\u6807\u6ce8\u9700\u6c42\uff0c\u63d0\u4f9b\u975e\u4fb5\u5165\u5f0f\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u5148\u8fdb\u9762\u90e8\u68c0\u6d4b\u6a21\u578b\u3001\u9762\u90e8\u8bc6\u522b\u7279\u5f81\u5d4c\u5165\u548c\u8fc1\u79fb\u5b66\u4e60\uff0c\u5229\u7528\u6559\u5e08\u773c\u52a8\u6570\u636e\u8bad\u7ec3\u8bfe\u5802\u60c5\u5883\u4e0b\u7684\u9762\u90e8\u8bc6\u522b\u6a21\u578b\uff0c\u6784\u5efa\u81ea\u52a8\u5316\u5904\u7406\u6d41\u7a0b\u3002", "result": "\u5728\u56db\u79cd\u6559\u5ba4\u5e03\u5c40\u4e2d\u5747\u80fd\u5408\u7406\u4f30\u8ba1\u6559\u5e08\u89c6\u89c9\u5173\u6ce8\u5bf9\u8c61\uff0cU\u578b\u548c\u5c0f\u578b\u6559\u5ba4\u51c6\u786e\u7387\u5206\u522b\u8fbe0.7\u548c0.9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6280\u672f\u6709\u6548\u4e14\u65e0\u9700\u5927\u91cf\u624b\u52a8\u6807\u6ce8\uff0c\u4e3a\u6559\u5b66\u7b56\u7565\u6539\u8fdb\u3001\u8bfe\u5802\u7ba1\u7406\u548c\u6559\u5e08\u53d1\u5c55\u63d0\u4f9b\u4e86\u975e\u4fb5\u5165\u5f0f\u5de5\u5177\u3002"}}
{"id": "2505.07046", "pdf": "https://arxiv.org/pdf/2505.07046", "abs": "https://arxiv.org/abs/2505.07046", "authors": ["Stephen Thomas"], "title": "Streaming Krylov-Accelerated Stochastic Gradient Descent", "categories": ["math.NA", "cs.LG", "cs.NA"], "comment": null, "summary": "We present SKA-SGD (Streaming Krylov-Accelerated Stochastic Gradient\nDescent), a novel optimization approach that accelerates convergence for\nill-conditioned problems by projecting stochastic gradients onto a\nlow-dimensional Krylov subspace. Directly inspired by recent advances in s-step\nConjugate Gradient methods with streaming Gauss-Seidel Gram solvers\n\\cite{dambra2025sstep}, our method extends these techniques to the stochastic\noptimization domain. Our approach combines three key innovations: (1)\nprojection coefficients computed via a single streaming Gauss-Seidel iteration,\nwhich is mathematically equivalent to Modified Gram-Schmidt orthogonalization;\n(2) a Chebyshev polynomial basis for constructing the Krylov subspace,\nproviding superior numerical stability; and (3) efficient implementation for\nAMD GPUs using HIP. We prove that our streaming approach achieves a backward\nerror near machine precision with $O(s^2)$ complexity rather than $O(s^3)$,\nwhere $s$ is the Krylov subspace dimension. Experimental results demonstrate\nthat SKA-SGD significantly outperforms standard SGD and Adam in convergence\nrate and final error, particularly for problems with condition numbers\nexceeding $10^3$. GPU performance analysis reveals a crossover point where\ncommunication-avoiding benefits outweigh computational overhead, typically\noccurring at moderate scale ($p \\approx 64$ processors) for problem sizes $n\n\\geq 10^6$.", "AI": {"tldr": "SKA-SGD\u662f\u4e00\u79cd\u65b0\u9896\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u968f\u673a\u68af\u5ea6\u6295\u5f71\u5230\u4f4e\u7ef4Krylov\u5b50\u7a7a\u95f4\u6765\u52a0\u901f\u75c5\u6001\u95ee\u9898\u7684\u6536\u655b\uff0c\u7ed3\u5408\u4e86\u6d41\u5f0fGauss-Seidel\u8fed\u4ee3\u3001Chebyshev\u591a\u9879\u5f0f\u57fa\u5e95\u548cGPU\u9ad8\u6548\u5b9e\u73b0\uff0c\u663e\u8457\u4f18\u4e8eSGD\u548cAdam\u3002", "motivation": "\u9488\u5bf9\u75c5\u6001\u95ee\u9898\u6536\u655b\u901f\u5ea6\u6162\u7684\u6311\u6218\uff0c\u7ed3\u5408\u6d41\u5f0fKrylov\u5b50\u7a7a\u95f4\u4f18\u5316\u548c\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7684\u4f18\u52bf\uff0c\u63d0\u51fa\u4e86SKA-SGD\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6d41\u5f0fGauss-Seidel\u8fed\u4ee3\u8ba1\u7b97\u6295\u5f71\u7cfb\u6570\uff0c\u4f7f\u7528Chebyshev\u591a\u9879\u5f0f\u57fa\u5e95\u6784\u5efaKrylov\u5b50\u7a7a\u95f4\uff0c\u5e76\u5728AMD GPU\u4e0a\u9ad8\u6548\u5b9e\u73b0\uff0c\u590d\u6742\u5ea6\u4ec5\u4e3aO(s\u00b2)\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSKA-SGD\u5728\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u8bef\u5dee\u4e0a\u663e\u8457\u4f18\u4e8e\u6807\u51c6SGD\u548cAdam\uff0c\u5c24\u5176\u5728\u6761\u4ef6\u6570\u8d85\u8fc710\u00b3\u7684\u95ee\u9898\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "SKA-SGD\u901a\u8fc7\u6d41\u5f0fKrylov\u52a0\u901f\u548cGPU\u4f18\u5316\uff0c\u5728\u75c5\u6001\u95ee\u9898\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u6536\u655b\uff0c\u4e3a\u5927\u89c4\u6a21\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.07553", "pdf": "https://arxiv.org/pdf/2505.07553", "abs": "https://arxiv.org/abs/2505.07553", "authors": ["Tor Sporsem", "Rasmus Ulfsnes"], "title": "Towards Requirements Engineering for RAG Systems", "categories": ["cs.SE", "cs.AI"], "comment": "Accepted to EASE 2025, 17-20 June, Istanbul, Turkey", "summary": "This short paper explores how a maritime company develops and integrates\nlarge-language models (LLM). Specifically by looking at the requirements\nengineering for Retrieval Augmented Generation (RAG) systems in expert\nsettings. Through a case study at a maritime service provider, we demonstrate\nhow data scientists face a fundamental tension between user expectations of AI\nperfection and the correctness of the generated outputs. Our findings reveal\nthat data scientists must identify context-specific \"retrieval requirements\"\nthrough iterative experimentation together with users because they are the ones\nwho can determine correctness. We present an empirical process model describing\nhow data scientists practically elicited these \"retrieval requirements\" and\nmanaged system limitations. This work advances software engineering knowledge\nby providing insights into the specialized requirements engineering processes\nfor implementing RAG systems in complex domain-specific applications.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6d77\u4e8b\u516c\u53f8\u5982\u4f55\u5f00\u53d1\u548c\u96c6\u6210\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u5728\u4e13\u5bb6\u73af\u5883\u4e2d\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u7684\u9700\u6c42\u5de5\u7a0b\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u63ed\u793a\u4e86\u6570\u636e\u79d1\u5b66\u5bb6\u5728\u7528\u6237\u671f\u671b\u4e0e\u751f\u6210\u8f93\u51fa\u6b63\u786e\u6027\u4e4b\u95f4\u7684\u6311\u6218\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u6570\u636e\u79d1\u5b66\u5bb6\u5728\u590d\u6742\u9886\u57df\u4e2d\u5b9e\u65bdRAG\u7cfb\u7edf\u65f6\uff0c\u5982\u4f55\u5e73\u8861\u7528\u6237\u5bf9AI\u5b8c\u7f8e\u8868\u73b0\u7684\u671f\u671b\u4e0e\u5b9e\u9645\u751f\u6210\u5185\u5bb9\u7684\u6b63\u786e\u6027\u4e4b\u95f4\u7684\u77db\u76fe\u3002", "method": "\u901a\u8fc7\u6d77\u4e8b\u670d\u52a1\u63d0\u4f9b\u5546\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u91c7\u7528\u8fed\u4ee3\u5b9e\u9a8c\u4e0e\u7528\u6237\u5408\u4f5c\u7684\u65b9\u5f0f\uff0c\u8bc6\u522b\u4e0a\u4e0b\u6587\u7279\u5b9a\u7684\u201c\u68c0\u7d22\u9700\u6c42\u201d\uff0c\u5e76\u5efa\u7acb\u7ecf\u9a8c\u6027\u8fc7\u7a0b\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6570\u636e\u79d1\u5b66\u5bb6\u9700\u901a\u8fc7\u7528\u6237\u5408\u4f5c\u786e\u5b9a\u201c\u68c0\u7d22\u9700\u6c42\u201d\uff0c\u5e76\u7ba1\u7406\u7cfb\u7edf\u5c40\u9650\u6027\uff0c\u4e3a\u590d\u6742\u57df\u7279\u5b9a\u5e94\u7528\u4e2d\u7684RAG\u7cfb\u7edf\u9700\u6c42\u5de5\u7a0b\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u77e5\u8bc6\u8d21\u732e\u4e86\u4e13\u95e8\u5316\u7684\u9700\u6c42\u5de5\u7a0b\u8fc7\u7a0b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u57df\u7279\u5b9a\u5e94\u7528\u4e2d\u5b9e\u65bdRAG\u7cfb\u7edf\u7684\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2505.07054", "pdf": "https://arxiv.org/pdf/2505.07054", "abs": "https://arxiv.org/abs/2505.07054", "authors": ["Austin Braniff", "Yuhe Tian"], "title": "YANNs: Y-wise Affine Neural Networks for Exact and Efficient Representations of Piecewise Linear Functions", "categories": ["eess.SY", "cs.LG", "cs.SY", "math.OC"], "comment": null, "summary": "This work formally introduces Y-wise Affine Neural Networks (YANNs), a\nfully-explainable network architecture that continuously and efficiently\nrepresent piecewise affine functions with polytopic subdomains. Following from\nthe proofs, it is shown that the development of YANNs requires no training to\nachieve the functionally equivalent representation. YANNs thus maintain all\nmathematical properties of the original formulations. Multi-parametric model\npredictive control is utilized as an application showcase of YANNs, which\ntheoretically computes optimal control laws as a piecewise affine function of\nstates, outputs, setpoints, and disturbances. With the exact representation of\nmulti-parametric control laws, YANNs retain essential control-theoretic\nguarantees such as recursive feasibility and stability. This sets YANNs apart\nfrom the existing works which apply neural networks for approximating optimal\ncontrol laws instead of exactly representing them. By optimizing the inference\nspeed of the networks, YANNs can evaluate substantially faster in real-time\ncompared to traditional piecewise affine function calculations. Numerical case\nstudies are presented to demonstrate the algorithmic scalability with respect\nto the input/output dimensions and the number of subdomains. YANNs represent a\nsignificant advancement in control as the first neural network-based controller\nthat inherently ensures both feasibility and stability. Future applications can\nleverage them as an efficient and interpretable starting point for data-driven\nmodeling/control.", "AI": {"tldr": "YANNs\u662f\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u9ad8\u6548\u8868\u793a\u5206\u6bb5\u4eff\u5c04\u51fd\u6570\uff0c\u4fdd\u6301\u6570\u5b66\u6027\u8d28\uff0c\u5e76\u5728\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u4e2d\u63d0\u4f9b\u7406\u8bba\u6700\u4f18\u89e3\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u5728\u63a7\u5236\u5f8b\u8fd1\u4f3c\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0cYANNs\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u7cbe\u786e\u8868\u793a\u63a7\u5236\u5f8b\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u8bc1\u63a7\u5236\u7406\u8bba\u7684\u53ef\u884c\u6027\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u591a\u53c2\u6570\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u901a\u8fc7YANNs\u76f4\u63a5\u8868\u793a\u63a7\u5236\u5f8b\u4e3a\u72b6\u6001\u3001\u8f93\u51fa\u7b49\u7684\u5206\u6bb5\u4eff\u5c04\u51fd\u6570\u3002", "result": "YANNs\u5728\u5b9e\u65f6\u8ba1\u7b97\u4e2d\u901f\u5ea6\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u6848\u4f8b\u9a8c\u8bc1\u4e86\u5176\u7b97\u6cd5\u6269\u5c55\u6027\u3002", "conclusion": "YANNs\u662f\u9996\u4e2a\u80fd\u540c\u65f6\u4fdd\u8bc1\u53ef\u884c\u6027\u548c\u7a33\u5b9a\u6027\u7684\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u5efa\u6a21/\u63a7\u5236\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u57fa\u7840\u3002"}}
{"id": "2505.07573", "pdf": "https://arxiv.org/pdf/2505.07573", "abs": "https://arxiv.org/abs/2505.07573", "authors": ["Sarah de Boer", "Hartmut H\u00e4ntze", "Kiran Vaidhya Venkadesh", "Myrthe A. D. Buser", "Gabriel E. Humpire Mamani", "Lina Xu", "Lisa C. Adams", "Jawed Nawabi", "Keno K. Bressem", "Bram van Ginneken", "Mathias Prokop", "Alessa Hering"], "title": "Robust Kidney Abnormality Segmentation: A Validation Study of an AI-Based Framework", "categories": ["cs.CV", "cs.AI"], "comment": "35 pages, 11 figures", "summary": "Kidney abnormality segmentation has important potential to enhance the\nclinical workflow, especially in settings requiring quantitative assessments.\nKidney volume could serve as an important biomarker for renal diseases, with\nchanges in volume correlating directly with kidney function. Currently,\nclinical practice often relies on subjective visual assessment for evaluating\nkidney size and abnormalities, including tumors and cysts, which are typically\nstaged based on diameter, volume, and anatomical location. To support a more\nobjective and reproducible approach, this research aims to develop a robust,\nthoroughly validated kidney abnormality segmentation algorithm, made publicly\navailable for clinical and research use. We employ publicly available training\ndatasets and leverage the state-of-the-art medical image segmentation framework\nnnU-Net. Validation is conducted using both proprietary and public test\ndatasets, with segmentation performance quantified by Dice coefficient and the\n95th percentile Hausdorff distance. Furthermore, we analyze robustness across\nsubgroups based on patient sex, age, CT contrast phases, and tumor histologic\nsubtypes. Our findings demonstrate that our segmentation algorithm, trained\nexclusively on publicly available data, generalizes effectively to external\ntest sets and outperforms existing state-of-the-art models across all tested\ndatasets. Subgroup analyses reveal consistent high performance, indicating\nstrong robustness and reliability. The developed algorithm and associated code\nare publicly accessible at\nhttps://github.com/DIAGNijmegen/oncology-kidney-abnormality-segmentation.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8ennU-Net\u7684\u80be\u810f\u5f02\u5e38\u5206\u5272\u7b97\u6cd5\uff0c\u5229\u7528\u516c\u5f00\u6570\u636e\u96c6\u8bad\u7ec3\u5e76\u901a\u8fc7\u591a\u79cd\u6d4b\u8bd5\u96c6\u9a8c\u8bc1\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u4e14\u5728\u4e0d\u540c\u4e9a\u7ec4\u4e2d\u5747\u8868\u73b0\u7a33\u5065\u3002", "motivation": "\u901a\u8fc7\u5ba2\u89c2\u3001\u53ef\u91cd\u590d\u7684\u80be\u810f\u5f02\u5e38\u5206\u5272\u65b9\u6cd5\uff0c\u6539\u8fdb\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u4f9d\u8d56\u4e3b\u89c2\u89c6\u89c9\u8bc4\u4f30\u7684\u73b0\u72b6\uff0c\u652f\u6301\u5b9a\u91cf\u5206\u6790\u80be\u810f\u75be\u75c5\u3002", "method": "\u4f7f\u7528\u516c\u5f00\u6570\u636e\u96c6\u8bad\u7ec3nnU-Net\u6846\u67b6\uff0c\u5e76\u901a\u8fc7Dice\u7cfb\u6570\u548c95th\u767e\u5206\u4f4dHausdorff\u8ddd\u79bb\u91cf\u5316\u5206\u5272\u6027\u80fd\uff0c\u540c\u65f6\u5728\u60a3\u8005\u6027\u522b\u3001\u5e74\u9f84\u7b49\u4e9a\u7ec4\u4e2d\u9a8c\u8bc1\u9c81\u68d2\u6027\u3002", "result": "\u7b97\u6cd5\u5728\u5916\u90e8\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u4e14\u5728\u4e0d\u540c\u4e9a\u7ec4\u4e2d\u6027\u80fd\u4e00\u81f4\u3002", "conclusion": "\u5f00\u53d1\u7684\u7b97\u6cd5\u5177\u6709\u9ad8\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u548c\u7814\u7a76\uff0c\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2505.07067", "pdf": "https://arxiv.org/pdf/2505.07067", "abs": "https://arxiv.org/abs/2505.07067", "authors": ["Francesco Cagnetta", "Hyunmo Kang", "Matthieu Wyart"], "title": "Learning curves theory for hierarchically compositional data with power-law distributed features", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.LG"], "comment": null, "summary": "Recent theories suggest that Neural Scaling Laws arise whenever the task is\nlinearly decomposed into power-law distributed units. Alternatively, scaling\nlaws also emerge when data exhibit a hierarchically compositional structure, as\nis thought to occur in language and images. To unify these views, we consider\nclassification and next-token prediction tasks based on probabilistic\ncontext-free grammars -- probabilistic models that generate data via a\nhierarchy of production rules. For classification, we show that having\npower-law distributed production rules results in a power-law learning curve\nwith an exponent depending on the rules' distribution and a large\nmultiplicative constant that depends on the hierarchical structure. By\ncontrast, for next-token prediction, the distribution of production rules\ncontrols the local details of the learning curve, but not the exponent\ndescribing the large-scale behaviour.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u7684\u8d77\u6e90\uff0c\u53d1\u73b0\u5206\u7c7b\u4efb\u52a1\u548c\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\u4efb\u52a1\u4e2d\u5e42\u5f8b\u5206\u5e03\u89c4\u5219\u7684\u4e0d\u540c\u5f71\u54cd\u3002\u5206\u7c7b\u4efb\u52a1\u4e2d\u5e42\u5f8b\u5206\u5e03\u89c4\u5219\u51b3\u5b9a\u4e86\u5b66\u4e60\u66f2\u7ebf\u7684\u5e42\u5f8b\u6307\u6570\uff0c\u800c\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\u4efb\u52a1\u4e2d\u89c4\u5219\u5206\u5e03\u4ec5\u5f71\u54cd\u5c40\u90e8\u7ec6\u8282\u3002", "motivation": "\u65e8\u5728\u7edf\u4e00\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u7684\u4e24\u79cd\u7406\u8bba\u89c2\u70b9\uff1a\u4efb\u52a1\u7ebf\u6027\u5206\u89e3\u4e3a\u5e42\u5f8b\u5206\u5e03\u5355\u4f4d\u548c\u6570\u636e\u7684\u5206\u5c42\u7ec4\u5408\u7ed3\u6784\u3002", "method": "\u57fa\u4e8e\u6982\u7387\u4e0a\u4e0b\u6587\u65e0\u5173\u6587\u6cd5\u7684\u5206\u7c7b\u548c\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\u4efb\u52a1\u5206\u6790\u3002", "result": "\u5206\u7c7b\u4efb\u52a1\u4e2d\u5e42\u5f8b\u5206\u5e03\u89c4\u5219\u7684\u5b66\u4e60\u66f2\u7ebf\u5448\u73b0\u5e42\u5f8b\u884c\u4e3a\uff0c\u6307\u6570\u7531\u89c4\u5219\u5206\u5e03\u51b3\u5b9a\uff1b\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\u4efb\u52a1\u4e2d\u89c4\u5219\u5206\u5e03\u4e0d\u5f71\u54cd\u5927\u5c3a\u5ea6\u884c\u4e3a\u7684\u5e42\u5f8b\u6307\u6570\u3002", "conclusion": "\u901a\u8fc7\u6587\u6cd5\u6a21\u578b\u7edf\u4e00\u4e86\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u7684\u4e24\u79cd\u89e3\u91ca\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u4efb\u52a1\u4e2d\u89c4\u5219\u5206\u5e03\u5f71\u54cd\u5b66\u4e60\u66f2\u7ebf\u7684\u5dee\u5f02\u6027\u3002"}}
{"id": "2505.07576", "pdf": "https://arxiv.org/pdf/2505.07576", "abs": "https://arxiv.org/abs/2505.07576", "authors": ["Manuel Barusco", "Francesco Borsatti", "Youssef Ben Khalifa", "Davide Dalle Pezze", "Gian Antonio Susto"], "title": "Evaluating Modern Visual Anomaly Detection Approaches in Semiconductor Manufacturing: A Comparative Study", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Semiconductor manufacturing is a complex, multistage process. Automated\nvisual inspection of Scanning Electron Microscope (SEM) images is indispensable\nfor minimizing equipment downtime and containing costs. Most previous research\nconsiders supervised approaches, assuming a sufficient number of anomalously\nlabeled samples. On the contrary, Visual Anomaly Detection (VAD), an emerging\nresearch domain, focuses on unsupervised learning, avoiding the costly defect\ncollection phase while providing explanations of the predictions. We introduce\na benchmark for VAD in the semiconductor domain by leveraging the MIIC dataset.\nOur results demonstrate the efficacy of modern VAD approaches in this field.", "AI": {"tldr": "\u8bba\u6587\u6458\u8981\u8ba8\u8bba\u4e86\u534a\u5bfc\u4f53\u5236\u9020\u4e2dSEM\u56fe\u50cf\u81ea\u52a8\u89c6\u89c9\u68c0\u6d4b\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u89c6\u89c9\u5f02\u5e38\u68c0\u6d4b\uff08VAD\uff09\u65b9\u6cd5\uff0c\u5e76\u5229\u7528MIIC\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "motivation": "\u7531\u4e8e\u534a\u5bfc\u4f53\u5236\u9020\u8fc7\u7a0b\u4e2d\u7f3a\u9677\u6837\u672c\u6807\u8bb0\u6210\u672c\u9ad8\uff0c\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\uff0c\u56e0\u6b64\u63a2\u7d22\u65e0\u76d1\u7763\u7684VAD\u65b9\u6cd5\u4ee5\u5b9e\u73b0\u9ad8\u6548\u4e14\u7ecf\u6d4e\u7684\u5f02\u5e38\u68c0\u6d4b\u663e\u5f97\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u73b0\u4ee3VAD\u65b9\u6cd5\uff0c\u5e76\u5728\u534a\u5bfc\u4f53\u9886\u57df\u7684MIIC\u6570\u636e\u96c6\u4e0a\u6784\u5efa\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u73b0\u4ee3VAD\u65b9\u6cd5\u5728\u534a\u5bfc\u4f53\u9886\u57df\u7684\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u65e0\u76d1\u7763\u7684VAD\u65b9\u6cd5\u5728\u534a\u5bfc\u4f53\u5236\u9020\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u5927\u91cf\u6807\u8bb0\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u68c0\u6d4b\u5f02\u5e38\u3002"}}
{"id": "2505.07068", "pdf": "https://arxiv.org/pdf/2505.07068", "abs": "https://arxiv.org/abs/2505.07068", "authors": ["Jinchao Feng", "Sui Tang"], "title": "A Sparse Bayesian Learning Algorithm for Estimation of Interaction Kernels in Motsch-Tadmor Model", "categories": ["stat.ML", "cs.LG", "math.DS"], "comment": "18 pages", "summary": "In this paper, we investigate the data-driven identification of asymmetric\ninteraction kernels in the Motsch-Tadmor model based on observed trajectory\ndata. The model under consideration is governed by a class of semilinear\nevolution equations, where the interaction kernel defines a normalized,\nstate-dependent Laplacian operator that governs collective dynamics. To address\nthe resulting nonlinear inverse problem, we propose a variational framework\nthat reformulates kernel identification using the implicit form of the\ngoverning equations, reducing it to a subspace identification problem. We\nestablish an identifiability result that characterizes conditions under which\nthe interaction kernel can be uniquely recovered up to scale. To solve the\ninverse problem robustly, we develop a sparse Bayesian learning algorithm that\nincorporates informative priors for regularization, quantifies uncertainty, and\nenables principled model selection. Extensive numerical experiments on\nrepresentative interacting particle systems demonstrate the accuracy,\nrobustness, and interpretability of the proposed framework across a range of\nnoise levels and data regimes.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u57fa\u4e8e\u89c2\u6d4b\u8f68\u8ff9\u6570\u636e\u7684Motsch-Tadmor\u6a21\u578b\u4e2d\u975e\u5bf9\u79f0\u4ea4\u4e92\u6838\u7684\u6570\u636e\u9a71\u52a8\u8bc6\u522b\uff0c\u63d0\u51fa\u53d8\u5206\u6846\u67b6\u548c\u7a00\u758f\u8d1d\u53f6\u65af\u5b66\u4e60\u7b97\u6cd5\uff0c\u8bc1\u660e\u4e86\u552f\u4e00\u53ef\u8bc6\u522b\u6027\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4e3a\u89e3\u51b3Motsch-Tadmor\u6a21\u578b\u4e2d\u975e\u7ebf\u6027\u9006\u95ee\u9898\u53ca\u5176\u4ea4\u4e92\u6838\u8bc6\u522b\u7684\u6311\u6218\uff0c\u9700\u4e00\u79cd\u80fd\u5904\u7406\u566a\u58f0\u6570\u636e\u3001\u63d0\u4f9b\u4e0d\u786e\u5b9a\u91cf\u5316\u5e76\u652f\u6301\u6a21\u578b\u9009\u62e9\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u53d8\u5206\u6846\u67b6\u5c06\u6838\u8bc6\u522b\u8f6c\u4e3a\u5b50\u7a7a\u95f4\u8bc6\u522b\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u7a00\u758f\u8d1d\u53f6\u65af\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u5408\u4fe1\u606f\u5148\u9a8c\u8fdb\u884c\u6b63\u5219\u5316\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4ea4\u4e92\u6838\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u552f\u4e00\u53ef\u8bc6\u522b\uff08\u81f3\u5c3a\u5ea6\u53d8\u6362\uff09\uff0c\u6570\u503c\u5b9e\u9a8c\u663e\u793a\u65b9\u6cd5\u5728\u4e0d\u540c\u566a\u58f0\u548c\u6570\u636e\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u6240\u63d0\u6846\u67b6\u5728\u8bc6\u522b\u4ea4\u4e92\u6838\u65f6\u517c\u5177\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u590d\u6742\u52a8\u529b\u7cfb\u7edf\u7684\u6570\u636e\u9a71\u52a8\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2505.07073", "pdf": "https://arxiv.org/pdf/2505.07073", "abs": "https://arxiv.org/abs/2505.07073", "authors": ["Payal Varshney", "Adriano Lucieri", "Christoph Balada", "Andreas Dengel", "Sheraz Ahmed"], "title": "Discovering Concept Directions from Diffusion-based Counterfactuals via Latent Clustering", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Concept-based explanations have emerged as an effective approach within\nExplainable Artificial Intelligence, enabling interpretable insights by\naligning model decisions with human-understandable concepts. However, existing\nmethods rely on computationally intensive procedures and struggle to\nefficiently capture complex, semantic concepts. Recently, the Concept Discovery\nthrough Latent Diffusion-based Counterfactual Trajectories (CDCT) framework,\nintroduced by Varshney et al. (2025), attempts to identify concepts via\ndimension-wise traversal of the latent space of a Variational Autoencoder\ntrained on counterfactual trajectories. Extending the CDCT framework, this work\nintroduces Concept Directions via Latent Clustering (CDLC), which extracts\nglobal, class-specific concept directions by clustering latent difference\nvectors derived from factual and diffusion-generated counterfactual image\npairs. CDLC substantially reduces computational complexity by eliminating the\nexhaustive latent dimension traversal required in CDCT and enables the\nextraction of multidimensional semantic concepts encoded across the latent\ndimensions. This approach is validated on a real-world skin lesion dataset,\ndemonstrating that the extracted concept directions align with clinically\nrecognized dermoscopic features and, in some cases, reveal dataset-specific\nbiases or unknown biomarkers. These results highlight that CDLC is\ninterpretable, scalable, and applicable across high-stakes domains and diverse\ndata modalities.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86CDLC\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7c7b\u6f5c\u5728\u5dee\u5f02\u5411\u91cf\u6765\u9ad8\u6548\u63d0\u53d6\u5168\u5c40\u3001\u7c7b\u522b\u7279\u5b9a\u7684\u6982\u5ff5\u65b9\u5411\uff0c\u76f8\u6bd4\u4e8e\u4e4b\u524d\u7684CDCT\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6982\u5ff5\u7684\u89e3\u91ca\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6355\u6349\u590d\u6742\u8bed\u4e49\u6982\u5ff5\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u80fd\u63d0\u53d6\u591a\u7ef4\u8bed\u4e49\u6982\u5ff5\u7684\u65b9\u6cd5\u3002", "method": "CDLC\u901a\u8fc7\u805a\u7c7b\u4ece\u4e8b\u5b9e\u548c\u53cd\u4e8b\u5b9e\u56fe\u50cf\u5bf9\u4e2d\u5f97\u5230\u7684\u6f5c\u5728\u5dee\u5f02\u5411\u91cf\uff0c\u63d0\u53d6\u5168\u5c40\u3001\u7c7b\u522b\u7279\u5b9a\u7684\u6982\u5ff5\u65b9\u5411\uff0c\u65e0\u9700\u904d\u5386\u6240\u6709\u6f5c\u5728\u7ef4\u5ea6\u3002", "result": "\u5728\u76ae\u80a4\u75c5\u53d8\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCDLC\u63d0\u53d6\u7684\u6982\u5ff5\u65b9\u5411\u4e0e\u4e34\u5e8a\u7279\u5f81\u4e00\u81f4\uff0c\u5e76\u80fd\u63ed\u793a\u6570\u636e\u96c6\u504f\u5dee\u6216\u672a\u77e5\u751f\u7269\u6807\u5fd7\u7269\u3002", "conclusion": "CDLC\u662f\u4e00\u79cd\u53ef\u89e3\u91ca\u3001\u53ef\u6269\u5c55\u4e14\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u9886\u57df\u548c\u591a\u6837\u5316\u6570\u636e\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.07101", "pdf": "https://arxiv.org/pdf/2505.07101", "abs": "https://arxiv.org/abs/2505.07101", "authors": ["Haichen Hu", "David Simchi-Levi", "Navid Azizan"], "title": "Constrained Online Decision-Making with Density Estimation Oracles", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Contextual online decision-making problems with constraints appear in a wide\nrange of real-world applications, such as personalized recommendation with\nresource limits, adaptive experimental design, and decision-making under safety\nor fairness requirements. In this paper, we investigate a general formulation\nof sequential decision-making with stage-wise feasibility constraints, where at\neach round, the learner must select an action based on observed context while\nensuring that a problem-specific feasibility criterion is satisfied. We propose\na unified algorithmic framework that captures many existing constrained\nlearning problems, including constrained bandits, active learning with label\nbudgets, online hypothesis testing with Type I error control, and model\ncalibration. Central to our approach is the concept of upper counterfactual\nconfidence bounds, which enables the design of practically efficient online\nalgorithms with strong theoretical guarantee using any offline conditional\ndensity estimation oracle. Technically, to handle feasibility constraints in\ncomplex environments, we introduce a generalized notion of the eluder dimension\n- extending it from the classical setting based on square loss to a broader\nclass of metric-like probability divergences. This allows us to capture the\ncomplexity of various density function classes and characterize the utility\nregret incurred due to feasibility constraint uncertainty. Our result offers a\nprincipled foundation for constrained sequential decision-making in both theory\nand practice.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u7b97\u6cd5\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5177\u6709\u9636\u6bb5\u6027\u53ef\u884c\u6027\u7ea6\u675f\u7684\u5728\u7ebf\u51b3\u7b56\u95ee\u9898\uff0c\u6db5\u76d6\u4e86\u591a\u79cd\u73b0\u6709\u7ea6\u675f\u5b66\u4e60\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u5e94\u7528\u4e2d\u5982\u8d44\u6e90\u6709\u9650\u7684\u4e2a\u6027\u5316\u63a8\u8350\u3001\u81ea\u9002\u5e94\u5b9e\u9a8c\u8bbe\u8ba1\u7b49\u9700\u8981\u6ee1\u8db3\u7279\u5b9a\u7ea6\u675f\u7684\u5728\u7ebf\u51b3\u7b56\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86\u4e0a\u754c\u53cd\u4e8b\u5b9e\u7f6e\u4fe1\u8fb9\u754c\u6982\u5ff5\uff0c\u5e76\u6269\u5c55\u4e86eluder\u7ef4\u5ea6\u7684\u5b9a\u4e49\uff0c\u4ee5\u5904\u7406\u590d\u6742\u73af\u5883\u4e2d\u7684\u53ef\u884c\u6027\u7ea6\u675f\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u6846\u67b6\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u5747\u63d0\u4f9b\u4e86\u5f3a\u7406\u8bba\u4fdd\u8bc1\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5bc6\u5ea6\u51fd\u6570\u7c7b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u7684\u7ea6\u675f\u5e8f\u5217\u51b3\u7b56\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u57fa\u7840\u3002"}}
{"id": "2505.07105", "pdf": "https://arxiv.org/pdf/2505.07105", "abs": "https://arxiv.org/abs/2505.07105", "authors": ["Hongwei Shang", "Nguyen Vo", "Nitin Yadav", "Tian Zhang", "Ajit Puthenputhussery", "Xunfan Cai", "Shuyi Chen", "Prijith Chandran", "Changsung Kang"], "title": "Knowledge Distillation for Enhancing Walmart E-commerce Search Relevance Using Large Language Models", "categories": ["cs.IR", "cs.LG"], "comment": "9 pages, published at WWWW'25", "summary": "Ensuring the products displayed in e-commerce search results are relevant to\nusers queries is crucial for improving the user experience. With their advanced\nsemantic understanding, deep learning models have been widely used for\nrelevance matching in search tasks. While large language models (LLMs) offer\nsuperior ranking capabilities, it is challenging to deploy LLMs in real-time\nsystems due to the high-latency requirements. To leverage the ranking power of\nLLMs while meeting the low-latency demands of production systems, we propose a\nnovel framework that distills a high performing LLM into a more efficient,\nlow-latency student model. To help the student model learn more effectively\nfrom the teacher model, we first train the teacher LLM as a classification\nmodel with soft targets. Then, we train the student model to capture the\nrelevance margin between pairs of products for a given query using mean squared\nerror loss. Instead of using the same training data as the teacher model, we\nsignificantly expand the student model dataset by generating unlabeled data and\nlabeling it with the teacher model predictions. Experimental results show that\nthe student model performance continues to improve as the size of the augmented\ntraining data increases. In fact, with enough augmented data, the student model\ncan outperform the teacher model. The student model has been successfully\ndeployed in production at Walmart.com with significantly positive metrics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u5c06\u9ad8\u6027\u80fd\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u84b8\u998f\u4e3a\u9ad8\u6548\u4f4e\u5ef6\u8fdf\u7684\u5b66\u751f\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3LLM\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u90e8\u7f72\u7684\u9ad8\u5ef6\u8fdf\u95ee\u9898\u3002\u901a\u8fc7\u8f6f\u76ee\u6807\u5206\u7c7b\u548c\u6269\u5927\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5b66\u751f\u6a21\u578b\u6027\u80fd\u751a\u81f3\u8d85\u8d8a\u6559\u5e08\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u5df2\u5728Walmart.com\u6210\u529f\u90e8\u7f72\u3002", "motivation": "\u4e3a\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5b9e\u65f6\u7535\u5546\u641c\u7d22\u7cfb\u7edf\u4e2d\u56e0\u9ad8\u5ef6\u8fdf\u65e0\u6cd5\u90e8\u7f72\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u5f3a\u5927\u7684\u6392\u5e8f\u80fd\u529b\u3002", "method": "1. \u5c06\u6559\u5e08LLM\u8bad\u7ec3\u4e3a\u5e26\u8f6f\u76ee\u6807\u7684\u5206\u7c7b\u6a21\u578b\uff1b2. \u4f7f\u7528\u5747\u65b9\u8bef\u5dee\u635f\u5931\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u5b66\u4e60\u5546\u54c1\u5bf9\u7684\u76f8\u5173\u6027\u5dee\u5f02\uff1b3. \u901a\u8fc7\u751f\u6210\u672a\u6807\u6ce8\u6570\u636e\u5e76\u7528\u6559\u5e08\u6a21\u578b\u9884\u6d4b\u6807\u6ce8\uff0c\u5927\u5e45\u6269\u5c55\u5b66\u751f\u6a21\u578b\u8bad\u7ec3\u96c6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u968f\u7740\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u7684\u589e\u52a0\uff0c\u5b66\u751f\u6a21\u578b\u6027\u80fd\u6301\u7eed\u63d0\u5347\uff0c\u751a\u81f3\u8d85\u8d8a\u6559\u5e08\u6a21\u578b\u3002\u8be5\u6a21\u578b\u5728Walmart.com\u7684\u5b9e\u9645\u90e8\u7f72\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u901a\u8fc7\u84b8\u998f\u548c\u6570\u636e\u96c6\u6269\u5c55\uff0c\u5b66\u751f\u6a21\u578b\u5728\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.07163", "pdf": "https://arxiv.org/pdf/2505.07163", "abs": "https://arxiv.org/abs/2505.07163", "authors": ["Natalia G. Berloff"], "title": "Exact Spin Elimination in Ising Hamiltonians and Energy-Based Machine Learning", "categories": ["quant-ph", "cs.DM", "cs.DS", "cs.ET", "cs.LG"], "comment": "28 pages, 6 figures", "summary": "We present an exact spin-elimination technique that reduces the\ndimensionality of both quadratic and k-local Ising Hamiltonians while\npreserving their original ground-state configurations. By systematically\nreplacing each removed spin with an effective interaction among its neighbors,\nour method lowers the total spin count without invoking approximations or\niterative recalculations. This capability is especially beneficial for\nhardware-constrained platforms, classical or quantum, that can directly\nimplement multi-body interactions but have limited qubit or spin resources. We\ndemonstrate three key advances enabled by this technique. First, we handle\nlarger instances of benchmark problems such as Max-Cut on cubic graphs without\nexceeding a 2-local interaction limit. Second, we reduce qubit requirements in\nQAOA-based integer factorization on near-term quantum devices, thus extending\nthe feasible range of integers to be factorized. Third, we improve memory\ncapacity in Hopfield associative memories and enhance memory retrieval by\nsuppressing spurious attractors, enhancing retrieval performance. Our\nspin-elimination procedure trades local spin complexity for higher-order\ncouplings or higher node degrees in a single pass, opening new avenues for\nscaling up combinatorial optimization and energy-based machine learning on\nnear-term hardware. Finally, these results underscore that the next-generation\nphysical spin machines will likely capitalize on k-local spin Hamiltonians to\noffer an alternative to classical computations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7cbe\u786e\u7684\u81ea\u65cb\u6d88\u9664\u6280\u672f\uff0c\u964d\u4f4e\u4e8c\u6b21\u548ck-local Ising\u54c8\u5bc6\u987f\u91cf\u7684\u7ef4\u5ea6\uff0c\u540c\u65f6\u4fdd\u7559\u57fa\u6001\u6784\u578b\uff0c\u9002\u7528\u4e8e\u786c\u4ef6\u53d7\u9650\u5e73\u53f0\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e09\u5927\u5e94\u7528\u8fdb\u5c55\u3002", "motivation": "\u9488\u5bf9\u786c\u4ef6\u8d44\u6e90\u53d7\u9650\u7684\u5e73\u53f0\uff08\u7ecf\u5178\u6216\u91cf\u5b50\uff09\uff0c\u901a\u8fc7\u964d\u4f4e\u81ea\u65cb\u6570\u91cf\u4f46\u4ecd\u4fdd\u7559\u539f\u59cb\u57fa\u6001\u6784\u578b\uff0c\u63d0\u5347\u7ec4\u5408\u4f18\u5316\u548c\u57fa\u4e8e\u80fd\u91cf\u7684\u673a\u5668\u5b66\u4e60\u5728\u8fd1\u7aef\u786c\u4ef6\u4e0a\u7684\u6269\u5c55\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u5c06\u6bcf\u4e2a\u79fb\u9664\u7684\u81ea\u65cb\u66ff\u6362\u4e3a\u5176\u90bb\u5c45\u95f4\u7684\u6709\u6548\u76f8\u4e92\u4f5c\u7528\uff0c\u65e0\u9700\u8fd1\u4f3c\u6216\u8fed\u4ee3\u8ba1\u7b97\uff0c\u76f4\u63a5\u964d\u4f4e\u603b\u81ea\u65cb\u6570\u3002", "result": "\u5728\u4e09\u4e2a\u5173\u952e\u9886\u57df\u53d6\u5f97\u8fdb\u5c55\uff1a\u5904\u7406\u66f4\u5927\u89c4\u6a21\u7684Max-Cut\u95ee\u9898\u3001\u51cf\u5c11QAOA\u6574\u6570\u5206\u89e3\u7684\u91cf\u5b50\u6bd4\u7279\u9700\u6c42\u3001\u63d0\u5347Hopfield\u8054\u60f3\u8bb0\u5fc6\u7684\u6027\u80fd\u3002", "conclusion": "\u81ea\u65cb\u6d88\u9664\u6280\u672f\u4e3a\u4e0b\u4e00\u4ee3\u7269\u7406\u81ea\u65cb\u673a\u5668\u63d0\u4f9b\u4e86\u65b0\u7684\u6269\u5c55\u9014\u5f84\uff0c\u5c55\u793a\u4e86k-local\u81ea\u65cb\u54c8\u5bc6\u987f\u91cf\u5728\u7ecf\u5178\u8ba1\u7b97\u66ff\u4ee3\u65b9\u6848\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.07615", "pdf": "https://arxiv.org/pdf/2505.07615", "abs": "https://arxiv.org/abs/2505.07615", "authors": ["Riccardo Passoni", "Francesca Ronchini", "Luca Comanducci", "Romain Serizel", "Fabio Antonacci"], "title": "Diffused Responsibility: Analyzing the Energy Consumption of Generative Text-to-Audio Diffusion Models", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "comment": null, "summary": "Text-to-audio models have recently emerged as a powerful technology for\ngenerating sound from textual descriptions. However, their high computational\ndemands raise concerns about energy consumption and environmental impact. In\nthis paper, we conduct an analysis of the energy usage of 7 state-of-the-art\ntext-to-audio diffusion-based generative models, evaluating to what extent\nvariations in generation parameters affect energy consumption at inference\ntime. We also aim to identify an optimal balance between audio quality and\nenergy consumption by considering Pareto-optimal solutions across all selected\nmodels. Our findings provide insights into the trade-offs between performance\nand environmental impact, contributing to the development of more efficient\ngenerative audio models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e867\u79cd\u6700\u5148\u8fdb\u7684\u6587\u672c\u5230\u97f3\u9891\u6269\u6563\u751f\u6210\u6a21\u578b\u7684\u80fd\u8017\uff0c\u63a2\u8ba8\u4e86\u751f\u6210\u53c2\u6570\u5bf9\u63a8\u7406\u65f6\u80fd\u8017\u7684\u5f71\u54cd\uff0c\u5e76\u5bfb\u627e\u97f3\u9891\u8d28\u91cf\u4e0e\u80fd\u8017\u7684\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u6587\u672c\u5230\u97f3\u9891\u6a21\u578b\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u9ad8\u8ba1\u7b97\u9700\u6c42\u5f15\u53d1\u4e86\u5bf9\u80fd\u8017\u548c\u73af\u4fdd\u5f71\u54cd\u7684\u62c5\u5fe7\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5982\u4f55\u5728\u6027\u80fd\u4e0e\u73af\u5883\u5f71\u54cd\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002", "method": "\u7814\u7a76\u8005\u5bf97\u79cd\u6269\u6563\u751f\u6210\u6a21\u578b\u8fdb\u884c\u80fd\u8017\u5206\u6790\uff0c\u8bc4\u4f30\u4e0d\u540c\u751f\u6210\u53c2\u6570\u5bf9\u63a8\u7406\u65f6\u80fd\u8017\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u5e15\u7d2f\u6258\u6700\u4f18\u89e3\u5bfb\u627e\u6700\u4f73\u5e73\u8861\u70b9\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u751f\u6210\u97f3\u9891\u6a21\u578b\u63d0\u4f9b\u4e86\u6027\u80fd\u4e0e\u80fd\u8017\u6743\u8861\u7684\u89c1\u89e3\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u6a21\u578b\u53c2\u6570\u548c\u80fd\u8017\u7684\u5206\u6790\uff0c\u8bba\u6587\u4e3a\u672a\u6765\u7684\u9ad8\u6548\u97f3\u9891\u751f\u6210\u6a21\u578b\u8bbe\u8ba1\u63d0\u51fa\u4e86\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2505.07244", "pdf": "https://arxiv.org/pdf/2505.07244", "abs": "https://arxiv.org/abs/2505.07244", "authors": ["Christian Kuehn", "Sara-Viola Kuntz"], "title": "The Influence of the Memory Capacity of Neural DDEs on the Universal Approximation Property", "categories": ["math.DS", "cs.LG", "cs.NE"], "comment": null, "summary": "Neural Ordinary Differential Equations (Neural ODEs), which are the\ncontinuous-time analog of Residual Neural Networks (ResNets), have gained\nsignificant attention in recent years. Similarly, Neural Delay Differential\nEquations (Neural DDEs) can be interpreted as an infinite depth limit of\nDensely Connected Residual Neural Networks (DenseResNets). In contrast to\ntraditional ResNet architectures, DenseResNets are feed-forward networks that\nallow for shortcut connections across all layers. These additional connections\nintroduce memory in the network architecture, as typical in many modern\narchitectures. In this work, we explore how the memory capacity in neural DDEs\ninfluences the universal approximation property. The key parameter for studying\nthe memory capacity is the product $K \\tau$ of the Lipschitz constant and the\ndelay of the DDE. In the case of non-augmented architectures, where the network\nwidth is not larger than the input and output dimensions, neural ODEs and\nclassical feed-forward neural networks cannot have the universal approximation\nproperty. We show that if the memory capacity $K\\tau$ is sufficiently small,\nthe dynamics of the neural DDE can be approximated by a neural ODE.\nConsequently, non-augmented neural DDEs with a small memory capacity also lack\nthe universal approximation property. In contrast, if the memory capacity\n$K\\tau$ is sufficiently large, we can establish the universal approximation\nproperty of neural DDEs for continuous functions. If the neural DDE\narchitecture is augmented, we can expand the parameter regions in which\nuniversal approximation is possible. Overall, our results show that by\nincreasing the memory capacity $K\\tau$, the infinite-dimensional phase space of\nDDEs with positive delay $\\tau>0$ is not sufficient to guarantee a direct jump\ntransition to universal approximation, but only after a certain memory\nthreshold, universal approximation holds.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u795e\u7ecf\u5ef6\u8fdf\u5fae\u5206\u65b9\u7a0b\uff08Neural DDEs\uff09\u7684\u8bb0\u5fc6\u5bb9\u91cf\u5bf9\u901a\u7528\u8fd1\u4f3c\u6027\u8d28\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u8bb0\u5fc6\u5bb9\u91cf\u7684\u5927\u5c0f\u51b3\u5b9a\u4e86\u5176\u662f\u5426\u80fd\u5b9e\u73b0\u901a\u7528\u8fd1\u4f3c\u3002", "motivation": "\u63a2\u7d22\u795e\u7ecfDDEs\u7684\u8bb0\u5fc6\u5bb9\u91cf\u5982\u4f55\u5f71\u54cd\u5176\u901a\u7528\u8fd1\u4f3c\u80fd\u529b\uff0c\u586b\u8865\u4e86\u8fde\u7eed\u65f6\u95f4\u795e\u7ecf\u7f51\u7edc\u7406\u8bba\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8bb0\u5fc6\u5bb9\u91cf\u53c2\u6570K\u03c4\uff0c\u7814\u7a76\u795e\u7ecfDDEs\u5728\u4e0d\u540c\u8bb0\u5fc6\u5bb9\u91cf\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u795e\u7ecfODE\u548c\u4f20\u7edf\u7f51\u7edc\u5bf9\u6bd4\u3002", "result": "\u8bb0\u5fc6\u5bb9\u91cfK\u03c4\u8f83\u5c0f\u65f6\uff0c\u795e\u7ecfDDEs\u65e0\u6cd5\u5b9e\u73b0\u901a\u7528\u8fd1\u4f3c\uff1bK\u03c4\u8db3\u591f\u5927\u65f6\uff0c\u624d\u80fd\u5b9e\u73b0\u8fde\u7eed\u51fd\u6570\u7684\u901a\u7528\u8fd1\u4f3c\u3002\u589e\u5f3a\u67b6\u6784\u53ef\u4ee5\u6269\u5927\u9002\u7528\u53c2\u6570\u8303\u56f4\u3002", "conclusion": "\u795e\u7ecfDDEs\u7684\u8bb0\u5fc6\u5bb9\u91cf\u662f\u5176\u901a\u7528\u8fd1\u4f3c\u80fd\u529b\u7684\u5173\u952e\uff0c\u4ec5\u5f53\u8bb0\u5fc6\u5bb9\u91cf\u8d85\u8fc7\u4e00\u5b9a\u9608\u503c\u65f6\u624d\u80fd\u5b9e\u73b0\u901a\u7528\u8fd1\u4f3c\u3002"}}
{"id": "2505.07621", "pdf": "https://arxiv.org/pdf/2505.07621", "abs": "https://arxiv.org/abs/2505.07621", "authors": ["Leonardo Kuffo", "Peter Boncz"], "title": "Bang for the Buck: Vector Search on Cloud CPUs", "categories": ["cs.DB", "cs.AI"], "comment": "To be published in Proceedings of 21st International Workshop on Data\n  Management on New Hardware (DaMoN '25)", "summary": "Vector databases have emerged as a new type of systems that support efficient\nquerying of high-dimensional vectors. Many of these offer their database as a\nservice in the cloud. However, the variety of available CPUs and the lack of\nvector search benchmarks across CPUs make it difficult for users to choose one.\nIn this study, we show that CPU microarchitectures available in the cloud\nperform significantly differently across vector search scenarios. For instance,\nin an IVF index on float32 vectors, AMD's Zen4 gives almost 3x more queries per\nsecond (QPS) compared to Intel's Sapphire Rapids, but for HNSW indexes, the\ntables turn. However, when looking at the number of queries per dollar (QP$),\nGraviton3 is the best option for most indexes and quantization settings, even\nover Graviton4 (Table 1). With this work, we hope to guide users in getting the\nbest \"bang for the buck\" when deploying vector search systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u53d1\u73b0\u4e0d\u540cCPU\u5fae\u67b6\u6784\u5728\u5411\u91cf\u641c\u7d22\u573a\u666f\u4e2d\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u5e76\u6307\u51faGraviton3\u5728\u6027\u4ef7\u6bd4\u4e0a\u6700\u4f18\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u8de8CPU\u7684\u5411\u91cf\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u6237\u96be\u4ee5\u9009\u62e9\u6700\u9002\u5408\u7684\u4e91\u670d\u52a1CPU\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540cCPU\u5728\u4e0d\u540c\u5411\u91cf\u641c\u7d22\u573a\u666f\uff08\u5982IVF\u548cHNSW\u7d22\u5f15\uff09\u4e0b\u7684\u6027\u80fd\uff08QPS\u548cQP$\uff09\u3002", "result": "AMD Zen4\u5728IVF\u7d22\u5f15\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u800cGraviton3\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u6027\u4ef7\u6bd4\u6700\u9ad8\u3002", "conclusion": "\u7814\u7a76\u4e3a\u90e8\u7f72\u5411\u91cf\u641c\u7d22\u7cfb\u7edf\u7684\u7528\u6237\u63d0\u4f9b\u4e86\u6027\u4ef7\u6bd4\u6700\u4f18\u7684CPU\u9009\u62e9\u6307\u5bfc\u3002"}}
{"id": "2505.07267", "pdf": "https://arxiv.org/pdf/2505.07267", "abs": "https://arxiv.org/abs/2505.07267", "authors": ["Gerardo Duran-Martin"], "title": "Adaptive, Robust and Scalable Bayesian Filtering for Online Learning", "categories": ["stat.ML", "cs.LG"], "comment": "PhD thesis", "summary": "In this thesis, we introduce Bayesian filtering as a principled framework for\ntackling diverse sequential machine learning problems, including online\n(continual) learning, prequential (one-step-ahead) forecasting, and contextual\nbandits. To this end, this thesis addresses key challenges in applying Bayesian\nfiltering to these problems: adaptivity to non-stationary environments,\nrobustness to model misspecification and outliers, and scalability to the\nhigh-dimensional parameter space of deep neural networks. We develop novel\ntools within the Bayesian filtering framework to address each of these\nchallenges, including: (i) a modular framework that enables the development\nadaptive approaches for online learning; (ii) a novel, provably robust filter\nwith similar computational cost to standard filters, that employs Generalised\nBayes; and (iii) a set of tools for sequentially updating model parameters\nusing approximate second-order optimisation methods that exploit the\noverparametrisation of high-dimensional parametric models such as neural\nnetworks. Theoretical analysis and empirical results demonstrate the improved\nperformance of our methods in dynamic, high-dimensional, and misspecified\nmodels.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u6ee4\u6ce2\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5e8f\u5217\u673a\u5668\u5b66\u4e60\u95ee\u9898\uff0c\u5982\u5728\u7ebf\u5b66\u4e60\u3001\u9884\u6d4b\u548c\u4e0a\u4e0b\u6587\u532a\u5f92\u95ee\u9898\uff0c\u5e76\u9488\u5bf9\u975e\u5e73\u7a33\u73af\u5883\u3001\u6a21\u578b\u8bef\u8bbe\u548c\u9ad8\u7ef4\u53c2\u6570\u7a7a\u95f4\u7b49\u6311\u6218\u63d0\u51fa\u4e86\u521b\u65b0\u5de5\u5177\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u52a8\u6001\u3001\u9ad8\u7ef4\u548c\u6a21\u578b\u8bef\u8bbe\u7684\u5e8f\u5217\u5b66\u4e60\u95ee\u9898\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9002\u5e94\u6027\u5f3a\u3001\u9c81\u68d2\u6027\u9ad8\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u62ec\u6a21\u5757\u5316\u6846\u67b6\u3001\u65b0\u578b\u9c81\u68d2\u6ee4\u6ce2\u5668\u548c\u8fd1\u4f3c\u4e8c\u9636\u4f18\u5316\u5de5\u5177\u5728\u5185\u7684\u8d1d\u53f6\u65af\u6ee4\u6ce2\u65b9\u6cd5\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u9a8c\u7ed3\u679c\u5747\u663e\u793a\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u52a8\u6001\u3001\u9ad8\u7ef4\u548c\u8bef\u8bbe\u6a21\u578b\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8d1d\u53f6\u65af\u6ee4\u6ce2\u6846\u67b6\u5728\u5e8f\u5217\u5b66\u4e60\u95ee\u9898\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5728\u590d\u6742\u73af\u5883\u4e0b\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2505.07634", "pdf": "https://arxiv.org/pdf/2505.07634", "abs": "https://arxiv.org/abs/2505.07634", "authors": ["Jian Liu", "Xiongtao Shi", "Thai Duy Nguyen", "Haitian Zhang", "Tianxiang Zhang", "Wei Sun", "Yanjie Li", "Athanasios V. Vasilakos", "Giovanni Iacca", "Arshad Ali Khan", "Arvind Kumar", "Jae Won Cho", "Ajmal Mian", "Lihua Xie", "Erik Cambria", "Lin Wang"], "title": "Neural Brain: A Neuroscience-inspired Framework for Embodied Agents", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "51 pages, 17 figures, 9 tables", "summary": "The rapid evolution of artificial intelligence (AI) has shifted from static,\ndata-driven models to dynamic systems capable of perceiving and interacting\nwith real-world environments. Despite advancements in pattern recognition and\nsymbolic reasoning, current AI systems, such as large language models, remain\ndisembodied, unable to physically engage with the world. This limitation has\ndriven the rise of embodied AI, where autonomous agents, such as humanoid\nrobots, must navigate and manipulate unstructured environments with human-like\nadaptability. At the core of this challenge lies the concept of Neural Brain, a\ncentral intelligence system designed to drive embodied agents with human-like\nadaptability. A Neural Brain must seamlessly integrate multimodal sensing and\nperception with cognitive capabilities. Achieving this also requires an\nadaptive memory system and energy-efficient hardware-software co-design,\nenabling real-time action in dynamic environments. This paper introduces a\nunified framework for the Neural Brain of embodied agents, addressing two\nfundamental challenges: (1) defining the core components of Neural Brain and\n(2) bridging the gap between static AI models and the dynamic adaptability\nrequired for real-world deployment. To this end, we propose a biologically\ninspired architecture that integrates multimodal active sensing,\nperception-cognition-action function, neuroplasticity-based memory storage and\nupdating, and neuromorphic hardware/software optimization. Furthermore, we also\nreview the latest research on embodied agents across these four aspects and\nanalyze the gap between current AI systems and human intelligence. By\nsynthesizing insights from neuroscience, we outline a roadmap towards the\ndevelopment of generalizable, autonomous agents capable of human-level\nintelligence in real-world scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u2018\u795e\u7ecf\u5927\u8111\u2019\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u65e8\u5728\u4e3a\u5177\u8eabAI\u4ee3\u7406\u63d0\u4f9b\u4eba\u7c7b\u822c\u7684\u52a8\u6001\u9002\u5e94\u80fd\u529b\uff0c\u7ed3\u5408\u4e86\u591a\u6a21\u6001\u611f\u77e5\u3001\u8ba4\u77e5\u884c\u52a8\u529f\u80fd\u3001\u795e\u7ecf\u53ef\u5851\u6027\u8bb0\u5fc6\u548c\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u4f18\u5316\uff0c\u5e76\u5206\u6790\u4e86\u5f53\u524dAI\u4e0e\u4eba\u7c7b\u667a\u80fd\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u7f3a\u4e4f\u4e0e\u73b0\u5b9e\u4e16\u754c\u7684\u7269\u7406\u4ea4\u4e92\u80fd\u529b\uff0c\u8fd9\u63a8\u52a8\u4e86\u5177\u8eabAI\u7684\u53d1\u5c55\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u2018\u795e\u7ecf\u5927\u8111\u2019\u6846\u67b6\u89e3\u51b3\u9759\u6001AI\u6a21\u578b\u4e0e\u52a8\u6001\u73b0\u5b9e\u4e16\u754c\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u751f\u7269\u5b66\u542f\u53d1\u7684\u67b6\u6784\uff0c\u6574\u5408\u4e86\u591a\u6a21\u6001\u4e3b\u52a8\u611f\u77e5\u3001\u611f\u77e5-\u8ba4\u77e5-\u884c\u52a8\u529f\u80fd\u3001\u57fa\u4e8e\u795e\u7ecf\u53ef\u5851\u6027\u7684\u8bb0\u5fc6\u7cfb\u7edf\u4ee5\u53ca\u795e\u7ecf\u5f62\u6001\u786c\u4ef6/\u8f6f\u4ef6\u4f18\u5316\u3002", "result": "\u901a\u8fc7\u8fd9\u4e00\u6846\u67b6\uff0c\u8bba\u6587\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u591a\u5b66\u79d1\u7814\u7a76\uff08\u5982\u795e\u7ecf\u79d1\u5b66\uff09\u5e94\u7528\u4e8e\u5f00\u53d1\u5177\u6709\u4eba\u7c7b\u6c34\u5e73\u667a\u80fd\u7684\u81ea\u4e3b\u4ee3\u7406\u3002", "conclusion": "\u8bba\u6587\u4e3a\u5f00\u53d1\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5177\u6709\u4eba\u7c7b\u7ea7\u667a\u80fd\u7684\u901a\u7528\u5177\u8eabAI\u4ee3\u7406\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\uff0c\u5f3a\u8c03\u4e86\u8de8\u5b66\u79d1\u5408\u4f5c\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.07272", "pdf": "https://arxiv.org/pdf/2505.07272", "abs": "https://arxiv.org/abs/2505.07272", "authors": ["Javier Salazar Cavazos", "Jeffrey A. Fessler", "Laura Balzano"], "title": "ALPCAH: Subspace Learning for Sample-wise Heteroscedastic Data", "categories": ["stat.ML", "cs.LG", "eess.SP"], "comment": null, "summary": "Principal component analysis (PCA) is a key tool in the field of data\ndimensionality reduction. However, some applications involve heterogeneous data\nthat vary in quality due to noise characteristics associated with each data\nsample. Heteroscedastic methods aim to deal with such mixed data quality. This\npaper develops a subspace learning method, named ALPCAH, that can estimate the\nsample-wise noise variances and use this information to improve the estimate of\nthe subspace basis associated with the low-rank structure of the data. Our\nmethod makes no distributional assumptions of the low-rank component and does\nnot assume that the noise variances are known. Further, this method uses a soft\nrank constraint that does not require subspace dimension to be known.\nAdditionally, this paper develops a matrix factorized version of ALPCAH, named\nLR-ALPCAH, that is much faster and more memory efficient at the cost of\nrequiring subspace dimension to be known or estimated. Simulations and real\ndata experiments show the effectiveness of accounting for data\nheteroscedasticity compared to existing algorithms. Code available at\nhttps://github.com/javiersc1/ALPCAH.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86ALPCAH\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u5f02\u65b9\u5dee\u6570\u636e\u4e2d\u7684\u4f4e\u79e9\u5b50\u7a7a\u95f4\u5b66\u4e60\uff0c\u65e0\u9700\u5047\u8bbe\u566a\u58f0\u5206\u5e03\u6216\u5df2\u77e5\u566a\u58f0\u65b9\u5dee\uff0c\u5e76\u5f00\u53d1\u4e86\u66f4\u9ad8\u6548\u7684\u77e9\u9635\u5206\u89e3\u7248\u672cLR-ALPCAH\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "motivation": "\u4f20\u7edfPCA\u65e0\u6cd5\u5904\u7406\u5f02\u65b9\u5dee\u6570\u636e\uff08\u5373\u566a\u58f0\u65b9\u5dee\u4e0d\u540c\u7684\u6570\u636e\uff09\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u52a8\u4f30\u8ba1\u566a\u58f0\u65b9\u5dee\u5e76\u6539\u8fdb\u5b50\u7a7a\u95f4\u4f30\u8ba1\u7684\u65b9\u6cd5\u3002", "method": "ALPCAH\u901a\u8fc7\u8f6f\u79e9\u7ea6\u675f\u4f30\u8ba1\u6837\u672c\u566a\u58f0\u65b9\u5dee\u548c\u4f4e\u79e9\u5b50\u7a7a\u95f4\uff0c\u65e0\u9700\u5df2\u77e5\u566a\u58f0\u5206\u5e03\u6216\u5b50\u7a7a\u95f4\u7ef4\u5ea6\uff1bLR-ALPCAH\u662f\u5176\u77e9\u9635\u5206\u89e3\u7248\u672c\uff0c\u6548\u7387\u66f4\u9ad8\u4f46\u9700\u5df2\u77e5\u5b50\u7a7a\u95f4\u7ef4\u5ea6\u3002", "result": "\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\u8868\u660e\uff0cALPCAH\u548cLR-ALPCAH\u5728\u5f02\u65b9\u5dee\u6570\u636e\u4e2d\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002", "conclusion": "ALPCAH\u7cfb\u5217\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u65b9\u5dee\u6570\u636e\u7684\u5b50\u7a7a\u95f4\u5b66\u4e60\u95ee\u9898\uff0c\u4e14\u7075\u6d3b\u9002\u5e94\u4e0d\u540c\u573a\u666f\u9700\u6c42\u3002"}}
{"id": "2505.07664", "pdf": "https://arxiv.org/pdf/2505.07664", "abs": "https://arxiv.org/abs/2505.07664", "authors": ["Werner Geyer", "Jessica He", "Daita Sarkar", "Michelle Brachman", "Chris Hammond", "Jennifer Heins", "Zahra Ashktorab", "Carlos Rosemberg", "Charlie Hill"], "title": "A Case Study Investigating the Role of Generative AI in Quality Evaluations of Epics in Agile Software Development", "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": null, "summary": "The broad availability of generative AI offers new opportunities to support\nvarious work domains, including agile software development. Agile epics are a\nkey artifact for product managers to communicate requirements to stakeholders.\nHowever, in practice, they are often poorly defined, leading to churn, delivery\ndelays, and cost overruns. In this industry case study, we investigate\nopportunities for large language models (LLMs) to evaluate agile epic quality\nin a global company. Results from a user study with 17 product managers\nindicate how LLM evaluations could be integrated into their work practices,\nincluding perceived values and usage in improving their epics. High levels of\nsatisfaction indicate that agile epics are a new, viable application of AI\nevaluations. However, our findings also outline challenges, limitations, and\nadoption barriers that can inform both practitioners and researchers on the\nintegration of such evaluations into future agile work practices.", "AI": {"tldr": "LLMs\u53ef\u4ee5\u8bc4\u4f30\u654f\u6377epic\u8d28\u91cf\uff0c\u5e2e\u52a9\u4ea7\u54c1\u7ecf\u7406\u6539\u8fdb\u9700\u6c42\u5b9a\u4e49\uff0c\u7528\u6237\u6ee1\u610f\u5ea6\u9ad8\uff0c\u4f46\u4e5f\u5b58\u5728\u6311\u6218\u548c\u91c7\u7eb3\u969c\u788d\u3002", "motivation": "\u654f\u6377epic\u5b9a\u4e49\u4e0d\u4f73\u4f1a\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u6210\u672c\u8d85\u652f\uff0c\u7814\u7a76\u5982\u4f55\u5229\u7528LLMs\u63d0\u5347\u5176\u8d28\u91cf\u3002", "method": "\u5728\u5168\u7403\u516c\u53f8\u4e2d\u8fdb\u884c\u884c\u4e1a\u6848\u4f8b\u7814\u7a76\uff0c\u901a\u8fc717\u540d\u4ea7\u54c1\u7ecf\u7406\u7684\u7528\u6237\u7814\u7a76\u8bc4\u4f30LLM\u7684\u5e94\u7528\u6548\u679c\u3002", "result": "LLM\u8bc4\u4f30\u53ef\u96c6\u6210\u5230\u4ea7\u54c1\u7ecf\u7406\u7684\u5de5\u4f5c\u4e2d\uff0c\u6ee1\u610f\u5ea6\u9ad8\uff0c\u4f46\u540c\u65f6\u4e5f\u63ed\u793a\u4e86\u6311\u6218\u548c\u91c7\u7eb3\u969c\u788d\u3002", "conclusion": "\u654f\u6377epic\u662fLLM\u8bc4\u4f30\u7684\u65b0\u5e94\u7528\u65b9\u5411\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u672a\u6765\u5b9e\u8df5\u548c\u7814\u7a76\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2505.07701", "pdf": "https://arxiv.org/pdf/2505.07701", "abs": "https://arxiv.org/abs/2505.07701", "authors": ["Biel Tura Vecino", "Adam Gabry\u015b", "Daniel M\u0105twicki", "Andrzej Pomirski", "Tom Iddon", "Marius Cotescu", "Jaime Lorenzo-Trueba"], "title": "Lightweight End-to-end Text-to-speech Synthesis for low resource on-device applications", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Published as a conference paper at SSW 2023", "summary": "Recent works have shown that modelling raw waveform directly from text in an\nend-to-end (E2E) fashion produces more natural-sounding speech than traditional\nneural text-to-speech (TTS) systems based on a cascade or two-stage approach.\nHowever, current E2E state-of-the-art models are computationally complex and\nmemory-consuming, making them unsuitable for real-time offline on-device\napplications in low-resource scenarios. To address this issue, we propose a\nLightweight E2E-TTS (LE2E) model that generates high-quality speech requiring\nminimal computational resources. We evaluate the proposed model on the LJSpeech\ndataset and show that it achieves state-of-the-art performance while being up\nto $90\\%$ smaller in terms of model parameters and $10\\times$ faster in\nreal-time-factor. Furthermore, we demonstrate that the proposed E2E training\nparadigm achieves better quality compared to an equivalent architecture trained\nin a two-stage approach. Our results suggest that LE2E is a promising approach\nfor developing real-time, high quality, low-resource TTS applications for\non-device applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u8f7b\u91cf\u7ea7\u7aef\u5230\u7aef\u6587\u672c\u5230\u8bed\u97f3\uff08LE2E\uff09\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709E2E-TTS\u6a21\u578b\u8ba1\u7b97\u590d\u6742\u548c\u5185\u5b58\u6d88\u8017\u5927\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u8bed\u97f3\u5408\u6210\u4e14\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u7684\u7aef\u5230\u7aef\u6587\u672c\u5230\u8bed\u97f3\uff08E2E-TTS\uff09\u6a21\u578b\u867d\u7136\u80fd\u751f\u6210\u66f4\u81ea\u7136\u7684\u8bed\u97f3\uff0c\u4f46\u8ba1\u7b97\u590d\u6742\u4e14\u5360\u7528\u5927\u91cf\u5185\u5b58\uff0c\u9650\u5236\u4e86\u5728\u4f4e\u8d44\u6e90\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u5e94\u7528\u3002\u4f5c\u8005\u65e8\u5728\u8bbe\u8ba1\u4e00\u4e2a\u8f7b\u91cf\u5316\u6a21\u578b\uff0c\u4ee5\u514b\u670d\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u8f7b\u91cf\u7ea7\u7aef\u5230\u7aef\uff08LE2E\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u4f18\u5316\u67b6\u6784\u548c\u8bad\u7ec3\u8303\u5f0f\u51cf\u5c11\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\uff0c\u5e76\u5728LJSpeech\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLE2E\u6a21\u578b\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u8bed\u97f3\u5408\u6210\u7684\u540c\u65f6\uff0c\u53c2\u6570\u89c4\u6a21\u51cf\u5c0f90%\uff0c\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u63d0\u534710\u500d\uff0c\u4e14\u76f8\u6bd4\u4e24\u9636\u6bb5\u8bad\u7ec3\u7684\u7b49\u6548\u67b6\u6784\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "LE2E\u662f\u4e00\u79cd\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u8bbe\u5907\u7684\u5b9e\u65f6\u9ad8\u8d28\u91cfTTS\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u7aef\u5230\u7aef\u6a21\u578b\u7684\u8f7b\u91cf\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2505.07329", "pdf": "https://arxiv.org/pdf/2505.07329", "abs": "https://arxiv.org/abs/2505.07329", "authors": ["Jordan Frery", "Roman Bredehoft", "Jakub Klemsa", "Arthur Meyre", "Andrei Stoian"], "title": "Private LoRA Fine-tuning of Open-Source LLMs with Homomorphic Encryption", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Preserving data confidentiality during the fine-tuning of open-source Large\nLanguage Models (LLMs) is crucial for sensitive applications. This work\nintroduces an interactive protocol adapting the Low-Rank Adaptation (LoRA)\ntechnique for private fine-tuning. Homomorphic Encryption (HE) protects the\nconfidentiality of training data and gradients handled by remote worker nodes\nperforming the bulk of computations involving the base model weights. The data\nowner orchestrates training, requiring minimal local computing power and\nmemory, thus alleviating the need for expensive client-side GPUs. We\ndemonstrate feasibility by fine-tuning a Llama-3.2-1B model, presenting\nconvergence results using HE-compatible quantization and performance benchmarks\nfor HE computations on GPU hardware. This approach enables applications such as\nconfidential knowledge base question answering, private codebase fine-tuning\nfor AI code assistants, AI agents for drafting emails based on a company's\nemail archive, and adapting models to analyze sensitive legal or healthcare\ndocuments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u6280\u672f\u548c\u540c\u6001\u52a0\u5bc6\uff08HE\uff09\u7684\u79c1\u6709\u5fae\u8c03\u534f\u8bae\uff0c\u7528\u4e8e\u4fdd\u62a4\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u6570\u636e\u673a\u5bc6\u6027\u3002\u901a\u8fc7\u8fdc\u7a0b\u8ba1\u7b97\u8282\u70b9\u5904\u7406\u5927\u90e8\u5206\u8ba1\u7b97\uff0c\u51cf\u8f7b\u4e86\u5ba2\u6237\u7aef\u5bf9\u9ad8\u6027\u80fd\u786c\u4ef6\u7684\u4f9d\u8d56\uff0c\u5e76\u5728Llama-3.2-1B\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\u3002", "motivation": "\u654f\u611f\u5e94\u7528\u573a\u666f\u4e0b\uff0c\u4fdd\u62a4\u5f00\u6e90LLMs\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u6570\u636e\u673a\u5bc6\u6027\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u9ad8\u6602\u7684\u5ba2\u6237\u7aef\u8ba1\u7b97\u8d44\u6e90\uff0c\u4e14\u7f3a\u4e4f\u8db3\u591f\u7684\u6570\u636e\u4fdd\u62a4\u673a\u5236\u3002", "method": "\u91c7\u7528LoRA\u6280\u672f\u7ed3\u5408HE\uff0c\u8fdc\u7a0b\u8ba1\u7b97\u8282\u70b9\u5904\u7406\u4e3b\u8981\u8ba1\u7b97\u4efb\u52a1\uff0c\u4fdd\u62a4\u8bad\u7ec3\u6570\u636e\u548c\u68af\u5ea6\u7684\u673a\u5bc6\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5ba2\u6237\u7aef\u7684\u8ba1\u7b97\u8d1f\u62c5\u3002", "result": "\u5728Llama-3.2-1B\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u6536\u655b\uff0c\u5e76\u5728GPU\u786c\u4ef6\u4e0a\u9ad8\u6548\u8fd0\u884cHE\u8ba1\u7b97\uff0c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u673a\u5bc6\u77e5\u8bc6\u5e93\u95ee\u7b54\u3001\u79c1\u6709\u4ee3\u7801\u5e93\u5fae\u8c03\u7b49\u654f\u611f\u573a\u666f\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u5ba2\u6237\u7aef\u786c\u4ef6\u9700\u6c42\u3002"}}
{"id": "2505.07711", "pdf": "https://arxiv.org/pdf/2505.07711", "abs": "https://arxiv.org/abs/2505.07711", "authors": ["Pranav Sinha", "Sumit Kumar Jha", "Sunny Raj"], "title": "Circuit Partitioning Using Large Language Models for Quantum Compilation and Simulations", "categories": ["cs.ET", "cs.AI", "quant-ph"], "comment": "7 pages, 2 tables and 3 figures", "summary": "We are in the midst of the noisy intermediate-scale quantum (NISQ) era, where\nquantum computers are limited by noisy gates, some of which are more\nerror-prone than others and can render the final computation incomprehensible.\nQuantum circuit compilation algorithms attempt to minimize these noisy gates\nwhen mapping quantum algorithms onto quantum hardware but face computational\nchallenges that restrict their application to circuits with no more than 5-6\nqubits, necessitating the need to partition large circuits before the\napplication of noisy quantum gate minimization algorithms. The existing\ngeneration of these algorithms is heuristic in nature and does not account for\ndownstream gate minimization tasks. Large language models (LLMs) have the\npotential to change this and help improve quantum circuit partitions. This\npaper investigates the use of LLMs, such as Llama and Mistral, for partitioning\nquantum circuits by capitalizing on their abilities to understand and generate\ncode, including QASM. Specifically, we teach LLMs to partition circuits using\nthe quick partition approach of the Berkeley Quantum Synthesis Toolkit. Through\nexperimental evaluations, we show that careful fine-tuning of open source LLMs\nenables us to obtain an accuracy of 53.4% for the partition task while\nover-the-shelf LLMs are unable to correctly partition circuits, using standard\n1-shot and few-shot training approaches.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982Llama\u548cMistral\uff09\u5206\u5272\u91cf\u5b50\u7535\u8def\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u7b97\u6cd5\u56e0\u8ba1\u7b97\u9650\u5236\u65e0\u6cd5\u5904\u7406\u5927\u89c4\u6a21\u7535\u8def\u7684\u95ee\u9898\u3002", "motivation": "\u5728NISQ\u65f6\u4ee3\uff0c\u91cf\u5b50\u8ba1\u7b97\u673a\u53d7\u9650\u4e8e\u566a\u58f0\u95e8\u7684\u5e72\u6270\uff0c\u73b0\u6709\u7b97\u6cd5\u53ea\u80fd\u5904\u74065-6\u4e2a\u91cf\u5b50\u4f4d\u7684\u7535\u8def\uff0c\u56e0\u6b64\u9700\u8981\u5206\u5272\u5927\u89c4\u6a21\u7535\u8def\u4ee5\u6539\u8fdb\u91cf\u5b50\u95e8\u7684\u6700\u5c0f\u5316\u3002", "method": "\u901a\u8fc7\u5fae\u8c03\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5229\u7528\u5176\u4ee3\u7801\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\uff08\u5982QASM\uff09\uff0c\u7ed3\u5408Berkeley Quantum Synthesis Toolkit\u7684\u5feb\u901f\u5206\u5272\u65b9\u6cd5\uff0c\u5b9e\u73b0\u7535\u8def\u5206\u5272\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ecf\u7cbe\u7ec6\u5fae\u8c03\u7684\u6a21\u578b\u5728\u5206\u5272\u4efb\u52a1\u4e2d\u8fbe\u523053.4%\u7684\u51c6\u786e\u7387\uff0c\u800c\u73b0\u6210\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u901a\u8fc7\u6807\u51c6\u8bad\u7ec3\u65b9\u6cd5\u6b63\u786e\u5b8c\u6210\u5206\u5272\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u91cf\u5b50\u7535\u8def\u5206\u5272\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u63d0\u9ad8\u51c6\u786e\u7387\u3002"}}
{"id": "2505.07715", "pdf": "https://arxiv.org/pdf/2505.07715", "abs": "https://arxiv.org/abs/2505.07715", "authors": ["Qi Xu", "Jie Deng", "Jiangrong Shen", "Biwu Chen", "Huajin Tang", "Gang Pan"], "title": "Hybrid Spiking Vision Transformer for Object Detection with Event Cameras", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Event-based object detection has gained increasing attention due to its\nadvantages such as high temporal resolution, wide dynamic range, and\nasynchronous address-event representation. Leveraging these advantages, Spiking\nNeural Networks (SNNs) have emerged as a promising approach, offering low\nenergy consumption and rich spatiotemporal dynamics. To further enhance the\nperformance of event-based object detection, this study proposes a novel hybrid\nspike vision Transformer (HsVT) model. The HsVT model integrates a spatial\nfeature extraction module to capture local and global features, and a temporal\nfeature extraction module to model time dependencies and long-term patterns in\nevent sequences. This combination enables HsVT to capture spatiotemporal\nfeatures, improving its capability to handle complex event-based object\ndetection tasks. To support research in this area, we developed and publicly\nreleased The Fall Detection Dataset as a benchmark for event-based object\ndetection tasks. This dataset, captured using an event-based camera, ensures\nfacial privacy protection and reduces memory usage due to the event\nrepresentation format. We evaluated the HsVT model on GEN1 and Fall Detection\ndatasets across various model sizes. Experimental results demonstrate that HsVT\nachieves significant performance improvements in event detection with fewer\nparameters.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u8109\u51b2\u89c6\u89c9Transformer\uff08HsVT\uff09\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u5347\u57fa\u4e8e\u4e8b\u4ef6\u7684\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0c\u7ed3\u5408\u4e86\u7a7a\u95f4\u548c\u65f6\u95f4\u7279\u5f81\u63d0\u53d6\u6a21\u5757\uff0c\u5e76\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u57fa\u4e8e\u4e8b\u4ef6\u7684\u76ee\u6807\u68c0\u6d4b\u5177\u6709\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u3001\u5bbd\u52a8\u6001\u8303\u56f4\u548c\u5f02\u6b65\u4e8b\u4ef6\u8868\u793a\u7b49\u4f18\u52bf\uff0c\u800cSNN\u7684\u4f4e\u80fd\u8017\u548c\u65f6\u7a7a\u52a8\u6001\u7279\u6027\u4f7f\u5176\u6210\u4e3a\u6709\u6f5c\u529b\u7684\u89e3\u51b3\u65b9\u6848\u3002\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u3002", "method": "HsVT\u6a21\u578b\u6574\u5408\u4e86\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\u6a21\u5757\uff08\u6355\u83b7\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff09\u548c\u65f6\u95f4\u7279\u5f81\u63d0\u53d6\u6a21\u5757\uff08\u5efa\u6a21\u4e8b\u4ef6\u5e8f\u5217\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u957f\u671f\u6a21\u5f0f\uff09\uff0c\u4ee5\u540c\u65f6\u6355\u6349\u65f6\u7a7a\u7279\u5f81\u3002", "result": "HsVT\u5728GEN1\u548c\u81ea\u5efa\u7684Fall Detection\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e0d\u4ec5\u51cf\u5c11\u4e86\u53c2\u6570\u91cf\uff0c\u8fd8\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "HsVT\u901a\u8fc7\u7ed3\u5408\u65f6\u7a7a\u7279\u5f81\u63d0\u53d6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u57fa\u4e8e\u4e8b\u4ef6\u7684\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u6a21\u578b\u601d\u8def\u548c\u516c\u5f00\u6570\u636e\u96c6\u652f\u6301\u3002"}}
{"id": "2505.07396", "pdf": "https://arxiv.org/pdf/2505.07396", "abs": "https://arxiv.org/abs/2505.07396", "authors": ["Olaf Wysocki", "Benedikt Schwab", "Manoj Kumar Biswanath", "Qilin Zhang", "Jingwei Zhu", "Thomas Froech", "Medhini Heeramaglore", "Ihab Hijazi", "Khaoula Kanna", "Mathias Pechinger", "Zhaiyu Chen", "Yao Sun", "Alejandro Rueda Segura", "Ziyang Xu", "Omar AbdelGafar", "Mansour Mehranfar", "Chandan Yeshwanth", "Yueh-Cheng Liu", "Hadi Yazdi", "Jiapan Wang", "Stefan Auer", "Katharina Anders", "Klaus Bogenberger", "Andre Borrmann", "Angela Dai", "Ludwig Hoegner", "Christoph Holst", "Thomas H. Kolbe", "Ferdinand Ludwig", "Matthias Nie\u00dfner", "Frank Petzold", "Xiao Xiang Zhu", "Boris Jutzi"], "title": "TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset", "categories": ["cs.CV", "cs.LG"], "comment": "Submitted to the ISPRS Journal of Photogrammetry and Remote Sensing", "summary": "Urban Digital Twins (UDTs) have become essential for managing cities and\nintegrating complex, heterogeneous data from diverse sources. Creating UDTs\ninvolves challenges at multiple process stages, including acquiring accurate 3D\nsource data, reconstructing high-fidelity 3D models, maintaining models'\nupdates, and ensuring seamless interoperability to downstream tasks. Current\ndatasets are usually limited to one part of the processing chain, hampering\ncomprehensive UDTs validation. To address these challenges, we introduce the\nfirst comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN.\nThis dataset includes georeferenced, semantically aligned 3D models and\nnetworks along with various terrestrial, mobile, aerial, and satellite\nobservations boasting 32 data subsets over roughly 100,000 $m^2$ and currently\n767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, high\naccuracy, and multimodal data integration, the benchmark supports robust\nanalysis of sensors and the development of advanced reconstruction methods.\nAdditionally, we explore downstream tasks demonstrating the potential of\nTUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solar\npotential analysis, point cloud semantic segmentation, and LoD3 building\nreconstruction. We are convinced this contribution lays a foundation for\novercoming current limitations in UDT creation, fostering new research\ndirections and practical solutions for smarter, data-driven urban environments.\nThe project is available under: https://tum2t.win", "AI": {"tldr": "\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aTUM2TWIN\u7684\u591a\u6a21\u6001\u57ce\u5e02\u6570\u5b57\u5b6a\u751f\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u6570\u636e\u83b7\u53d6\u3001\u6a21\u578b\u91cd\u5efa\u3001\u66f4\u65b0\u548c\u4e92\u64cd\u4f5c\u6027\u7b49\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u901a\u5e38\u53ea\u6db5\u76d6\u5904\u7406\u94fe\u7684\u4e00\u90e8\u5206\uff0c\u9650\u5236\u4e86\u57ce\u5e02\u6570\u5b57\u5b6a\u751f\uff08UDTs\uff09\u7684\u5168\u9762\u9a8c\u8bc1\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "\u5f00\u53d1\u4e86TUM2TWIN\u6570\u636e\u96c6\uff0c\u5305\u542b\u5730\u7406\u53c2\u8003\u3001\u8bed\u4e49\u5bf9\u9f50\u76843D\u6a21\u578b\u548c\u7f51\u7edc\uff0c\u4ee5\u53ca\u591a\u79cd\u5730\u9762\u3001\u79fb\u52a8\u3001\u822a\u7a7a\u548c\u536b\u661f\u89c2\u6d4b\u6570\u636e\u3002", "result": "\u6570\u636e\u96c6\u8986\u76d6\u7ea6100,000\u5e73\u65b9\u7c73\uff0c\u5305\u542b767GB\u6570\u636e\uff0c\u652f\u6301\u4f20\u611f\u5668\u5206\u6790\u548c\u9ad8\u7ea7\u91cd\u5efa\u65b9\u6cd5\u7684\u5f00\u53d1\uff0c\u5e76\u901a\u8fc7\u4e0b\u6e38\u4efb\u52a1\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u3002", "conclusion": "TUM2TWIN\u4e3a\u514b\u670dUDT\u521b\u5efa\u4e2d\u7684\u73b0\u6709\u9650\u5236\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u6570\u636e\u9a71\u52a8\u57ce\u5e02\u73af\u5883\u7684\u65b0\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2505.07728", "pdf": "https://arxiv.org/pdf/2505.07728", "abs": "https://arxiv.org/abs/2505.07728", "authors": ["Lihan Zha", "Apurva Badithela", "Michael Zhang", "Justin Lidard", "Jeremy Bao", "Emily Zhou", "David Snyder", "Allen Z. Ren", "Dhruv Shah", "Anirudha Majumdar"], "title": "Guiding Data Collection via Factored Scaling Curves", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Project website: https://factored-data-scaling.github.io", "summary": "Generalist imitation learning policies trained on large datasets show great\npromise for solving diverse manipulation tasks. However, to ensure\ngeneralization to different conditions, policies need to be trained with data\ncollected across a large set of environmental factor variations (e.g., camera\npose, table height, distractors) $-$ a prohibitively expensive undertaking, if\ndone exhaustively. We introduce a principled method for deciding what data to\ncollect and how much to collect for each factor by constructing factored\nscaling curves (FSC), which quantify how policy performance varies as data\nscales along individual or paired factors. These curves enable targeted data\nacquisition for the most influential factor combinations within a given budget.\nWe evaluate the proposed method through extensive simulated and real-world\nexperiments, across both training-from-scratch and fine-tuning settings, and\nshow that it boosts success rates in real-world tasks in new environments by up\nto 26% over existing data-collection strategies. We further demonstrate how\nfactored scaling curves can effectively guide data collection using an offline\nmetric, without requiring real-world evaluation at scale.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u79f0\u4e3a\u5206\u89e3\u7f29\u653e\u66f2\u7ebf\uff08FSC\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u6536\u96c6\u7b56\u7565\uff0c\u901a\u8fc7\u5206\u6790\u5404\u73af\u5883\u56e0\u7d20\u5bf9\u7b56\u7565\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u6709\u9488\u5bf9\u6027\u5730\u5728\u9884\u7b97\u5185\u6536\u96c6\u6700\u5177\u5f71\u54cd\u529b\u7684\u6570\u636e\uff0c\u4ece\u800c\u63d0\u5347\u7b56\u7565\u5728\u65b0\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e2d\u8bad\u7ec3\u901a\u7528\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u65f6\uff0c\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u8986\u76d6\u6240\u6709\u73af\u5883\u53d8\u5316\u7684\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u5206\u89e3\u7f29\u653e\u66f2\u7ebf\u65b9\u6cd5\uff0c\u4ee5\u9ad8\u6548\u6307\u5bfc\u6570\u636e\u6536\u96c6\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u5206\u89e3\u7f29\u653e\u66f2\u7ebf\uff08FSC\uff09\uff0c\u91cf\u5316\u4e0d\u540c\u73af\u5883\u56e0\u7d20\uff08\u5982\u76f8\u673a\u89d2\u5ea6\u3001\u684c\u5b50\u9ad8\u5ea6\u7b49\uff09\u5bf9\u7b56\u7565\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u6709\u9488\u5bf9\u6027\u5730\u5206\u914d\u6570\u636e\u6536\u96c6\u8d44\u6e90\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u5747\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u7684\u6210\u529f\u7387\uff08\u6700\u9ad826%\uff09\uff0c\u5e76\u80fd\u901a\u8fc7\u79bb\u7ebf\u6307\u6807\u6709\u6548\u6307\u5bfc\u6570\u636e\u6536\u96c6\u3002", "conclusion": "\u5206\u89e3\u7f29\u653e\u66f2\u7ebf\u4e3a\u9ad8\u6548\u6570\u636e\u6536\u96c6\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u80fd\u591f\u5728\u6709\u9650\u9884\u7b97\u4e0b\u4f18\u5316\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u4ece\u96f6\u8bad\u7ec3\u548c\u5fae\u8c03\u4e24\u79cd\u573a\u666f\u3002"}}
{"id": "2505.07755", "pdf": "https://arxiv.org/pdf/2505.07755", "abs": "https://arxiv.org/abs/2505.07755", "authors": ["Tomasz Szydlo", "Viacheslaw Horbanow", "Dev Nandan Jha", "Shashikant Ilager", "Aleksander Slominski", "Rajiv Ranjan"], "title": "Benchmarking of CPU-intensive Stream Data Processing in The Edge Computing Systems", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "Edge computing has emerged as a pivotal technology, offering significant\nadvantages such as low latency, enhanced data security, and reduced reliance on\ncentralized cloud infrastructure. These benefits are crucial for applications\nrequiring real-time data processing or strict security measures. Despite these\nadvantages, edge devices operating within edge clusters are often\nunderutilized. This inefficiency is mainly due to the absence of a holistic\nperformance profiling mechanism which can help dynamically adjust the desired\nsystem configuration for a given workload. Since edge computing environments\ninvolve a complex interplay between CPU frequency, power consumption, and\napplication performance, a deeper understanding of these correlations is\nessential. By uncovering these relationships, it becomes possible to make\ninformed decisions that enhance both computational efficiency and energy\nsavings. To address this gap, this paper evaluates the power consumption and\nperformance characteristics of a single processing node within an edge cluster\nusing a synthetic microbenchmark by varying the workload size and CPU\nfrequency. The results show how an optimal measure can lead to optimized usage\nof edge resources, given both performance and power consumption.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u6790CPU\u9891\u7387\u3001\u529f\u8017\u4e0e\u6027\u80fd\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u4f18\u5316\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u8282\u80fd\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u5e38\u56e0\u7f3a\u4e4f\u5168\u9762\u7684\u6027\u80fd\u5206\u6790\u673a\u5236\u800c\u5bfc\u81f4\u8d44\u6e90\u5229\u7528\u7387\u4e0d\u8db3\uff0c\u9700\u52a8\u6001\u8c03\u6574\u914d\u7f6e\u4ee5\u9002\u5e94\u5de5\u4f5c\u8d1f\u8f7d\u3002", "method": "\u4f7f\u7528\u5408\u6210\u5fae\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5728\u4e0d\u540c\u5de5\u4f5c\u8d1f\u8f7d\u5927\u5c0f\u548cCPU\u9891\u7387\u4e0b\u8bc4\u4f30\u8fb9\u7f18\u96c6\u7fa4\u4e2d\u5355\u4e2a\u5904\u7406\u8282\u70b9\u7684\u529f\u8017\u4e0e\u6027\u80fd\u7279\u5f81\u3002", "result": "\u63ed\u793a\u4e86CPU\u9891\u7387\u3001\u529f\u8017\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u63d0\u51fa\u4e86\u4f18\u5316\u8fb9\u7f18\u8d44\u6e90\u4f7f\u7528\u7684\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u6df1\u5165\u7406\u89e3\u8fd9\u4e9b\u5173\u7cfb\uff0c\u53ef\u4ee5\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u5e76\u8282\u7701\u80fd\u6e90\uff0c\u4f18\u5316\u8fb9\u7f18\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u3002"}}
{"id": "2505.07487", "pdf": "https://arxiv.org/pdf/2505.07487", "abs": "https://arxiv.org/abs/2505.07487", "authors": ["Heraldo Borges", "Juliana Alves Pereira", "Djamel Eddine Khelladi", "Mathieu Acher"], "title": "Linux Kernel Configurations at Scale: A Dataset for Performance and Evolution Analysis", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "Configuring the Linux kernel to meet specific requirements, such as binary\nsize, is highly challenging due to its immense complexity-with over 15,000\ninterdependent options evolving rapidly across different versions. Although\nseveral studies have explored sampling strategies and machine learning methods\nto understand and predict the impact of configuration options, the literature\nstill lacks a comprehensive and large-scale dataset encompassing multiple\nkernel versions along with detailed quantitative measurements. To bridge this\ngap, we introduce LinuxData, an accessible collection of kernel configurations\nspanning several kernel releases, specifically from versions 4.13 to 5.8. This\ndataset, gathered through automated tools and build processes, comprises over\n240,000 kernel configurations systematically labeled with compilation outcomes\nand binary sizes. By providing detailed records of configuration evolution and\ncapturing the intricate interplay among kernel options, our dataset enables\ninnovative research in feature subset selection, prediction models based on\nmachine learning, and transfer learning across kernel versions. Throughout this\npaper, we describe how the dataset has been made easily accessible via OpenML\nand illustrate how it can be leveraged using only a few lines of Python code to\nevaluate AI-based techniques, such as supervised machine learning. We\nanticipate that this dataset will significantly enhance reproducibility and\nfoster new insights into configuration-space analysis at a scale that presents\nunique opportunities and inherent challenges, thereby advancing our\nunderstanding of the Linux kernel's configurability and evolution.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aLinuxData\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u4e2aLinux\u5185\u6838\u7248\u672c\uff084.13\u52305.8\uff09\u7684\u914d\u7f6e\uff0c\u5305\u542b\u8d85\u8fc724\u4e07\u6761\u914d\u7f6e\u8bb0\u5f55\uff0c\u7528\u4e8e\u7814\u7a76\u914d\u7f6e\u9009\u9879\u7684\u6f14\u53d8\u53ca\u5176\u5f71\u54cd\u3002", "motivation": "Linux\u5185\u6838\u914d\u7f6e\u6781\u5176\u590d\u6742\u4e14\u7248\u672c\u8fed\u4ee3\u5feb\uff0c\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u591a\u7248\u672c\u3001\u5927\u89c4\u6a21\u7684\u5b9a\u91cf\u6570\u636e\u96c6\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u5168\u9762\u7684\u6570\u636e\u96c6\u4ee5\u652f\u6301\u76f8\u5173\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u5316\u5de5\u5177\u548c\u6784\u5efa\u6d41\u7a0b\u6536\u96c6\u591a\u4e2a\u5185\u6838\u7248\u672c\u7684\u914d\u7f6e\u6570\u636e\uff0c\u8bb0\u5f55\u7f16\u8bd1\u7ed3\u679c\u548c\u4e8c\u8fdb\u5236\u5927\u5c0f\uff0c\u5e76\u5c06\u6570\u636e\u96c6\u516c\u5f00\u5728OpenML\u5e73\u53f0\u4e0a\u3002", "result": "LinuxData\u6570\u636e\u96c6\u5305\u542b24\u4e07\u6761\u914d\u7f6e\u8bb0\u5f55\uff0c\u652f\u6301\u7279\u5f81\u9009\u62e9\u3001\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u6a21\u578b\u548c\u8de8\u7248\u672c\u8fc1\u79fb\u5b66\u4e60\u7b49\u7814\u7a76\u3002", "conclusion": "LinuxData\u6570\u636e\u96c6\u5c06\u63d0\u5347\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\uff0c\u5e76\u4e3a\u5206\u6790Linux\u5185\u6838\u7684\u914d\u7f6e\u7a7a\u95f4\u63d0\u4f9b\u65b0\u89c6\u89d2\uff0c\u63a8\u52a8\u5bf9\u5176\u53ef\u914d\u7f6e\u6027\u548c\u6f14\u53d8\u7684\u7406\u89e3\u3002"}}
{"id": "2505.07496", "pdf": "https://arxiv.org/pdf/2505.07496", "abs": "https://arxiv.org/abs/2505.07496", "authors": ["Mohamed Ali Souibgui", "Changkyu Choi", "Andrey Barsky", "Kangsoo Jung", "Ernest Valveny", "Dimosthenis Karatzas"], "title": "DocVXQA: Context-Aware Visual Explanations for Document Question Answering", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We propose DocVXQA, a novel framework for visually self-explainable document\nquestion answering. The framework is designed not only to produce accurate\nanswers to questions but also to learn visual heatmaps that highlight\ncontextually critical regions, thereby offering interpretable justifications\nfor the model's decisions. To integrate explanations into the learning process,\nwe quantitatively formulate explainability principles as explicit learning\nobjectives. Unlike conventional methods that emphasize only the regions\npertinent to the answer, our framework delivers explanations that are\n\\textit{contextually sufficient} while remaining\n\\textit{representation-efficient}. This fosters user trust while achieving a\nbalance between predictive performance and interpretability in DocVQA\napplications. Extensive experiments, including human evaluation, provide strong\nevidence supporting the effectiveness of our method. The code is available at\nhttps://github.com/dali92002/DocVXQA.", "AI": {"tldr": "\u63d0\u51faDocVXQA\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u70ed\u56fe\u63d0\u4f9b\u6587\u6863\u95ee\u7b54\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5f3a\u8c03\u4e0a\u4e0b\u6587\u5145\u5206\u6027\u5e76\u5e73\u8861\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u63d0\u5347\u6587\u6863\u95ee\u7b54\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u89c6\u89c9\u70ed\u56fe\u63d0\u4f9b\u51b3\u7b56\u4f9d\u636e\uff0c\u589e\u5f3a\u7528\u6237\u4fe1\u4efb\u3002", "method": "\u5c06\u53ef\u89e3\u91ca\u6027\u539f\u5219\u91cf\u5316\u4e3a\u5b66\u4e60\u76ee\u6807\uff0c\u751f\u6210\u4e0a\u4e0b\u6587\u5145\u5206\u4e14\u9ad8\u6548\u8868\u793a\u7684\u89c6\u89c9\u70ed\u56fe\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u4eba\u7c7b\u8bc4\u4f30\u3002", "conclusion": "DocVXQA\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u9002\u7528\u4e8eDocVQA\u4efb\u52a1\u3002"}}
{"id": "2505.07594", "pdf": "https://arxiv.org/pdf/2505.07594", "abs": "https://arxiv.org/abs/2505.07594", "authors": ["Manish Prajapat", "Johannes K\u00f6hler", "Amon Lahr", "Andreas Krause", "Melanie N. Zeilinger"], "title": "Finite-Sample-Based Reachability for Safe Control with Gaussian Process Dynamics", "categories": ["eess.SY", "cs.LG", "cs.SY", "math.OC"], "comment": null, "summary": "Gaussian Process (GP) regression is shown to be effective for learning\nunknown dynamics, enabling efficient and safety-aware control strategies across\ndiverse applications. However, existing GP-based model predictive control\n(GP-MPC) methods either rely on approximations, thus lacking guarantees, or are\noverly conservative, which limits their practical utility. To close this gap,\nwe present a sampling-based framework that efficiently propagates the model's\nepistemic uncertainty while avoiding conservatism. We establish a novel sample\ncomplexity result that enables the construction of a reachable set using a\nfinite number of dynamics functions sampled from the GP posterior. Building on\nthis, we design a sampling-based GP-MPC scheme that is recursively feasible and\nguarantees closed-loop safety and stability with high probability. Finally, we\nshowcase the effectiveness of our method on two numerical examples,\nhighlighting accurate reachable set over-approximation and safe closed-loop\nperformance.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91c7\u6837\u7684\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08GP-MPC\uff09\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u56e0\u8fd1\u4f3c\u800c\u7f3a\u4e4f\u4fdd\u8bc1\u6216\u8fc7\u4e8e\u4fdd\u5b88\u7684\u95ee\u9898\uff0c\u540c\u65f6\u786e\u4fdd\u4e86\u9012\u5f52\u53ef\u884c\u6027\u3001\u95ed\u73af\u5b89\u5168\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u8fd1\u4f3c\u800c\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\uff0c\u8981\u4e48\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u91c7\u6837\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6709\u9650\u6570\u91cf\u7684GP\u540e\u9a8c\u52a8\u6001\u51fd\u6570\u91c7\u6837\u6709\u6548\u4f20\u64ad\u6a21\u578b\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u907f\u514d\u4fdd\u5b88\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9012\u5f52\u53ef\u884c\u7684\u91c7\u6837\u578bGP-MPC\u65b9\u6848\u3002", "result": "\u5efa\u7acb\u4e86\u65b0\u7684\u6837\u672c\u590d\u6742\u5ea6\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u80fd\u6784\u9020\u9ad8\u6982\u7387\u7684\u53ef\u8fbe\u96c6\uff0c\u5e76\u5728\u4e24\u4e2a\u6570\u503c\u7b97\u4f8b\u4e2d\u5c55\u793a\u4e86\u51c6\u786e\u7684\u8fd1\u4f3c\u548c\u5b89\u5168\u7684\u95ed\u73af\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u786e\u4fdd\u5b89\u5168\u6027\u548c\u7a33\u5b9a\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86GP-MPC\u7684\u5b9e\u7528\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2505.07802", "pdf": "https://arxiv.org/pdf/2505.07802", "abs": "https://arxiv.org/abs/2505.07802", "authors": ["Reece O'Mahoney", "Wanming Yu", "Ioannis Havoutis"], "title": "Improving Trajectory Stitching with Flow Models", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Generative models have shown great promise as trajectory planners, given\ntheir affinity to modeling complex distributions and guidable inference\nprocess. Previous works have successfully applied these in the context of\nrobotic manipulation but perform poorly when the required solution does not\nexist as a complete trajectory within the training set. We identify that this\nis a result of being unable to plan via stitching, and subsequently address the\narchitectural and dataset choices needed to remedy this. On top of this, we\npropose a novel addition to the training and inference procedures to both\nstabilize and enhance these capabilities. We demonstrate the efficacy of our\napproach by generating plans with out of distribution boundary conditions and\nperforming obstacle avoidance on the Franka Panda in simulation and on real\nhardware. In both of these tasks our method performs significantly better than\nthe baselines and is able to avoid obstacles up to four times as large.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6539\u8fdb\u4e86\u751f\u6210\u6a21\u578b\u5728\u8f68\u8ff9\u89c4\u5212\u4e2d\u7684\u5e94\u7528\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u901a\u8fc7\u62fc\u63a5\u65b9\u5f0f\u89c4\u5212\u8f68\u8ff9\u7684\u95ee\u9898\uff0c\u5e76\u5728\u6a21\u62df\u548c\u5b9e\u9645\u786c\u4ef6\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u751f\u6210\u6a21\u578b\u5728\u8f68\u8ff9\u89c4\u5212\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u6240\u9700\u89e3\u51b3\u65b9\u6848\u4e0d\u5b58\u5728\u4e8e\u8bad\u7ec3\u96c6\u4e2d\u65f6\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6539\u8fdb\u6a21\u578b\u67b6\u6784\u3001\u6570\u636e\u96c6\u9009\u62e9\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a\u548c\u7a33\u5b9a\u8f68\u8ff9\u89c4\u5212\u80fd\u529b\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u786c\u4ef6\u73af\u5883\u4e2d\uff0c\u65b0\u65b9\u6cd5\u5728\u751f\u6210\u8d85\u51fa\u5206\u5e03\u8fb9\u754c\u6761\u4ef6\u7684\u89c4\u5212\u53ca\u907f\u969c\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u907f\u969c\u6548\u7387\u63d0\u5347\u56db\u500d\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u6539\u8fdb\u540e\u7684\u751f\u6210\u6a21\u578b\u5728\u590d\u6742\u8f68\u8ff9\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u65b0\u573a\u666f\u65f6\u7684\u4f18\u52bf\u3002"}}
{"id": "2505.07607", "pdf": "https://arxiv.org/pdf/2505.07607", "abs": "https://arxiv.org/abs/2505.07607", "authors": ["Georg Sch\u00e4fer", "Raphael Seliger", "Jakob Rehrl", "Stefan Huber", "Simon Hirlaender"], "title": "Multi-Objective Reinforcement Learning for Energy-Efficient Industrial Control", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": "Accepted at DEXA 2025 (AI4IP)", "summary": "Industrial automation increasingly demands energy-efficient control\nstrategies to balance performance with environmental and cost constraints. In\nthis work, we present a multi-objective reinforcement learning (MORL) framework\nfor energy-efficient control of the Quanser Aero 2 testbed in its\none-degree-of-freedom configuration. We design a composite reward function that\nsimultaneously penalizes tracking error and electrical power consumption.\nPreliminary experiments explore the influence of varying the Energy penalty\nweight, alpha, on the trade-off between pitch tracking and energy savings. Our\nresults reveal a marked performance shift for alpha values between 0.0 and\n0.25, with non-Pareto optimal solutions emerging at lower alpha values, on both\nthe simulation and the real system. We hypothesize that these effects may be\nattributed to artifacts introduced by the adaptive behavior of the Adam\noptimizer, which could bias the learning process and favor bang-bang control\nstrategies. Future work will focus on automating alpha selection through\nGaussian Process-based Pareto front modeling and transitioning the approach\nfrom simulation to real-world deployment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8eQuanser Aero 2\u6d4b\u8bd5\u53f0\u7684\u80fd\u91cf\u9ad8\u6548\u63a7\u5236\uff0c\u901a\u8fc7\u8bbe\u8ba1\u590d\u5408\u5956\u52b1\u51fd\u6570\u6765\u5e73\u8861\u8ddf\u8e2a\u8bef\u5dee\u548c\u80fd\u8017\uff0c\u5e76\u521d\u6b65\u63a2\u7d22\u4e86\u80fd\u91cf\u60e9\u7f5a\u6743\u91cd\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u5de5\u4e1a\u81ea\u52a8\u5316\u5bf9\u80fd\u6e90\u9ad8\u6548\u63a7\u5236\u7b56\u7565\u7684\u9700\u6c42\u589e\u957f\uff0c\u7814\u7a76\u65e8\u5728\u5e73\u8861\u6027\u80fd\u4e0e\u73af\u5883\u53ca\u6210\u672c\u7ea6\u675f\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a7\u5236\u3002", "method": "\u91c7\u7528\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u540c\u65f6\u60e9\u7f5a\u8ddf\u8e2a\u8bef\u5dee\u548c\u7535\u529b\u6d88\u8017\u7684\u590d\u5408\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u8c03\u6574\u80fd\u91cf\u60e9\u7f5a\u6743\u91cd\u03b1\u4ee5\u63a2\u7d22\u6027\u80fd\u4e0e\u80fd\u8017\u7684\u6743\u8861\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u03b1\u503c\u4e3a0.0\u81f30.25\u95f4\u6027\u80fd\u53d8\u5316\u663e\u8457\uff0c\u4f4e\u03b1\u503c\u4e0b\u51fa\u73b0\u975e\u5e15\u7d2f\u6258\u6700\u4f18\u89e3\uff0c\u53ef\u80fd\u4e0eAdam\u4f18\u5316\u5668\u7684\u9002\u5e94\u6027\u884c\u4e3a\u6709\u5173\u3002", "conclusion": "\u672a\u6765\u5de5\u4f5c\u5c06\u96c6\u4e2d\u4e8e\u901a\u8fc7\u9ad8\u65af\u8fc7\u7a0b\u5efa\u6a21\u81ea\u52a8\u9009\u62e9\u03b1\u503c\uff0c\u5e76\u5c06\u65b9\u6cd5\u4ece\u4eff\u771f\u8fc7\u6e21\u5230\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2505.07813", "pdf": "https://arxiv.org/pdf/2505.07813", "abs": "https://arxiv.org/abs/2505.07813", "authors": ["Tony Tao", "Mohan Kumar Srirama", "Jason Jingzhou Liu", "Kenneth Shaw", "Deepak Pathak"], "title": "DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": "In RSS 2025. Website at https://dexwild.github.io", "summary": "Large-scale, diverse robot datasets have emerged as a promising path toward\nenabling dexterous manipulation policies to generalize to novel environments,\nbut acquiring such datasets presents many challenges. While teleoperation\nprovides high-fidelity datasets, its high cost limits its scalability. Instead,\nwhat if people could use their own hands, just as they do in everyday life, to\ncollect data? In DexWild, a diverse team of data collectors uses their hands to\ncollect hours of interactions across a multitude of environments and objects.\nTo record this data, we create DexWild-System, a low-cost, mobile, and\neasy-to-use device. The DexWild learning framework co-trains on both human and\nrobot demonstrations, leading to improved performance compared to training on\neach dataset individually. This combination results in robust robot policies\ncapable of generalizing to novel environments, tasks, and embodiments with\nminimal additional robot-specific data. Experimental results demonstrate that\nDexWild significantly improves performance, achieving a 68.5% success rate in\nunseen environments-nearly four times higher than policies trained with robot\ndata only-and offering 5.8x better cross-embodiment generalization. Video\nresults, codebases, and instructions at https://dexwild.github.io", "AI": {"tldr": "\u8ad6\u6587\u63d0\u51faDexWild-System\u4f4e\u6210\u672c\u79fb\u52d5\u88dd\u7f6e\uff0c\u5229\u7528\u4eba\u985e\u624b\u52d5\u6536\u96c6\u6578\u64da\uff0c\u7d50\u5408\u6a5f\u5668\u4eba\u6578\u64da\u5171\u8a13\u7df4\uff0c\u63d0\u5347\u6a21\u578b\u5728\u65b0\u74b0\u5883\u548c\u4efb\u52d9\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u6210\u529f\u7387\u9ad8\u905468.5%\uff0c\u6bd4\u7d14\u6a5f\u5668\u4eba\u6578\u64da\u8a13\u7df4\u9ad8\u8fd14\u500d\u3002", "motivation": "\u70ba\u89e3\u6c7a\u5927\u898f\u6a21\u6a5f\u5668\u4eba\u6578\u64da\u96c6\u6210\u672c\u9ad8\u4e14\u4e0d\u6613\u7372\u53d6\u7684\u554f\u984c\uff0c\u7814\u7a76\u63d0\u51fa\u5229\u7528\u4eba\u985e\u624b\u52d5\u6536\u96c6\u6578\u64da\u7684\u65b9\u6cd5\uff0c\u4e26\u8a2d\u8a08\u4f4e\u6210\u672c\u88dd\u7f6eDexWild-System\uff0c\u4ee5\u63d0\u9ad8\u6578\u64da\u6536\u96c6\u7684\u6548\u7387\u548c\u53ef\u64f4\u5c55\u6027\u3002", "method": "\u958b\u767c\u4f4e\u6210\u672c\u79fb\u52d5\u88dd\u7f6eDexWild-System\uff0c\u5141\u8a31\u4eba\u985e\u4f7f\u7528\u81ea\u5df1\u7684\u624b\u6536\u96c6\u4ea4\u4e92\u6578\u64da\uff0c\u4e26\u5c07\u4eba\u985e\u6578\u64da\u8207\u6a5f\u5668\u4eba\u6578\u64da\u5171\u8a13\u7df4\uff0c\u5f62\u6210\u6df7\u5408\u5b78\u7fd2\u6846\u67b6\u3002", "result": "\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0cDexWild\u5728\u65b0\u74b0\u5883\u4e2d\u9054\u523068.5%\u7684\u6210\u529f\u7387\uff0c\u6bd4\u7d14\u6a5f\u5668\u4eba\u6578\u64da\u8a13\u7df4\u9ad8\u8fd14\u500d\uff0c\u4e14\u5728\u8de8\u5be6\u9ad4\u6cdb\u5316\u80fd\u529b\u4e0a\u63d0\u53475.8\u500d\u3002", "conclusion": "DexWild\u7d50\u5408\u4eba\u985e\u548c\u6a5f\u5668\u4eba\u6578\u64da\u7684\u5171\u8a13\u7df4\u65b9\u6cd5\u986f\u8457\u63d0\u5347\u4e86\u6a5f\u5668\u4eba\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u70ba\u4f4e\u6210\u672c\u9ad8\u6548\u7372\u53d6\u591a\u6a23\u5316\u6578\u64da\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.07609", "pdf": "https://arxiv.org/pdf/2505.07609", "abs": "https://arxiv.org/abs/2505.07609", "authors": ["Paul Primus", "Florian Schmid", "Gerhard Widmer"], "title": "TACOS: Temporally-aligned Audio CaptiOnS for Language-Audio Pretraining", "categories": ["eess.AS", "cs.LG", "cs.SD"], "comment": "submitted to the IEEE Workshop on Applications of Signal Processing\n  to Audio and Acoustics (WASPAA), 2025. Dataset (Zenodo):\n  https://zenodo.org/records/15379789, Implementation (GitHub):\n  https://github.com/OptimusPrimus/tacos", "summary": "Learning to associate audio with textual descriptions is valuable for a range\nof tasks, including pretraining, zero-shot classification, audio retrieval,\naudio captioning, and text-conditioned audio generation. Existing contrastive\nlanguage-audio pretrained models are typically trained using global, clip-level\ndescriptions, which provide only weak temporal supervision. We hypothesize that\nCLAP-like language-audio models - particularly, if they are expected to produce\nframe-level embeddings - can benefit from a stronger temporal supervision. To\nconfirm our hypothesis, we curate a novel dataset of approximately 12,000 audio\nrecordings from Freesound, each annotated with single-sentence free-text\ndescriptions linked to a specific temporal segment in an audio recording. We\nuse large language models to clean these annotations by removing references to\nnon-audible events, transcribed speech, typos, and annotator language bias. We\nfurther propose a frame-wise contrastive training strategy that learns to align\ntext descriptions with temporal regions in an audio recording and demonstrate\nthat our model has better temporal text-audio alignment abilities compared to\nmodels trained only on global captions when evaluated on the AudioSet Strong\nbenchmark. The dataset and our source code are available on Zenodo and GitHub,\nrespectively.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e27\u7ea7\u522b\u5bf9\u6bd4\u5b66\u4e60\u7684\u8bed\u8a00-\u97f3\u9891\u6a21\u578b\uff0c\u5229\u7528\u65f6\u95f4\u6807\u6ce8\u7684\u97f3\u9891\u63cf\u8ff0\u63d0\u5347\u6a21\u578b\u7684\u65f6\u95f4\u5bf9\u9f50\u80fd\u529b\uff0c\u4f18\u4e8e\u4ec5\u4f7f\u7528\u5168\u5c40\u6807\u6ce8\u7684\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u5168\u5c40\u97f3\u9891\u63cf\u8ff0\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u65f6\u95f4\u76d1\u7763\uff0c\u800c\u4f5c\u8005\u8ba4\u4e3a\u65f6\u95f4\u6807\u6ce8\u7684\u63cf\u8ff0\u80fd\u63d0\u5347\u8bed\u8a00-\u97f3\u9891\u6a21\u578b\uff08\u5982CLAP\uff09\u7684\u8868\u73b0\u529b\uff0c\u5e76\u63d0\u4f9b\u66f4\u597d\u7684\u65f6\u5e8f\u5bf9\u9f50\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u5e26\u6709\u65f6\u95f4\u7cbe\u786e\u63cf\u8ff0\u7684\u97f3\u9891\u6570\u636e\u96c6\uff0c\u5e76\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6e05\u6d17\u6807\u6ce8\u6570\u636e\uff0c\u63d0\u51fa\u5e27\u7ea7\u522b\u7684\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u6765\u5bf9\u9f50\u6587\u672c\u63cf\u8ff0\u548c\u97f3\u9891\u7247\u6bb5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u6a21\u578b\u5728AudioSet Strong\u57fa\u51c6\u4e0a\u663e\u793a\u51fa\u66f4\u597d\u7684\u65f6\u95f4\u5bf9\u9f50\u80fd\u529b\uff0c\u4f18\u4e8e\u4ec5\u7528\u5168\u5c40\u6807\u6ce8\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "\u65f6\u95f4\u7cbe\u786e\u7684\u97f3\u9891\u6807\u6ce8\u80fd\u663e\u8457\u63d0\u5347\u8bed\u8a00-\u97f3\u9891\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5e27\u7ea7\u522b\u5bf9\u6bd4\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.07816", "pdf": "https://arxiv.org/pdf/2505.07816", "abs": "https://arxiv.org/abs/2505.07816", "authors": ["Veeti Ahvonen", "Damian Heiman", "Antti Kuusisto"], "title": "A class of distributed automata that contains the modal mu-fragment", "categories": ["cs.LO", "cs.AI", "F.4.1; F.1.1; I.2.0"], "comment": null, "summary": "This paper gives a translation from the $\\mu$-fragment of the graded modal\n$\\mu$-calculus to a class of distributed message-passing automata. As a\ncorollary, we obtain an alternative proof for a theorem from\n\\cite{ahvonen_neurips} stating that recurrent graph neural networks working\nwith reals and graded modal substitution calculus have the same expressive\npower in restriction to the logic monadic second-order logic MSO.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u5206\u7ea7\u6a21\u6001\u03bc-\u6f14\u7b97\u7684\u03bc-\u7247\u6bb5\u7ffb\u8bd1\u4e3a\u4e00\u7c7b\u5206\u5e03\u5f0f\u6d88\u606f\u4f20\u9012\u81ea\u52a8\u673a\uff0c\u5e76\u4ee5\u6b64\u4f5c\u4e3a\u63a8\u8bba\uff0c\u4e3a\u5df2\u6709\u5b9a\u7406\u63d0\u4f9b\u4e86\u53e6\u4e00\u79cd\u8bc1\u660e\u3002", "motivation": "\u7814\u7a76\u5206\u7ea7\u6a21\u6001\u03bc-\u6f14\u7b97\u4e0e\u5206\u5e03\u5f0f\u81ea\u52a8\u673a\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u5e76\u901a\u8fc7\u8f6c\u6362\u65b9\u6cd5\u4e3a\u5df2\u6709\u7ed3\u8bba\u63d0\u4f9b\u65b0\u7684\u8bc1\u660e\u9014\u5f84\u3002", "method": "\u901a\u8fc7\u7ffb\u8bd1\u65b9\u6cd5\u5c06\u5206\u7ea7\u6a21\u6001\u03bc-\u6f14\u7b97\u7684\u03bc-\u7247\u6bb5\u6620\u5c04\u5230\u5206\u5e03\u5f0f\u6d88\u606f\u4f20\u9012\u81ea\u52a8\u673a\u3002", "result": "\u8bc1\u5b9e\u4e86\u9012\u5f52\u56fe\u795e\u7ecf\u7f51\u7edc\u4e0e\u5206\u7ea7\u6a21\u6001\u66ff\u6362\u6f14\u7b97\u5728MSO\u903b\u8f91\u9650\u5236\u4e0b\u5177\u6709\u76f8\u540c\u8868\u8fbe\u80fd\u529b\u7684\u5b9a\u7406\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u7ffb\u8bd1\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u6a21\u6001\u03bc-\u6f14\u7b97\u4e0e\u5206\u5e03\u5f0f\u81ea\u52a8\u673a\u7684\u8054\u7cfb\uff0c\u5e76\u652f\u6301\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\u8868\u8fbe\u80fd\u529b\u7684\u7406\u8bba\u7ed3\u679c\u3002"}}
{"id": "2505.07819", "pdf": "https://arxiv.org/pdf/2505.07819", "abs": "https://arxiv.org/abs/2505.07819", "authors": ["Yiyang Lu", "Yufeng Tian", "Zhecheng Yuan", "Xianbang Wang", "Pu Hua", "Zhengrong Xue", "Huazhe Xu"], "title": "H$^{\\mathbf{3}}$DP: Triply-Hierarchical Diffusion Policy for Visuomotor Learning", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Visuomotor policy learning has witnessed substantial progress in robotic\nmanipulation, with recent approaches predominantly relying on generative models\nto model the action distribution. However, these methods often overlook the\ncritical coupling between visual perception and action prediction. In this\nwork, we introduce $\\textbf{Triply-Hierarchical Diffusion\nPolicy}~(\\textbf{H$^{\\mathbf{3}}$DP})$, a novel visuomotor learning framework\nthat explicitly incorporates hierarchical structures to strengthen the\nintegration between visual features and action generation. H$^{3}$DP contains\n$\\mathbf{3}$ levels of hierarchy: (1) depth-aware input layering that organizes\nRGB-D observations based on depth information; (2) multi-scale visual\nrepresentations that encode semantic features at varying levels of granularity;\nand (3) a hierarchically conditioned diffusion process that aligns the\ngeneration of coarse-to-fine actions with corresponding visual features.\nExtensive experiments demonstrate that H$^{3}$DP yields a $\\mathbf{+27.5\\%}$\naverage relative improvement over baselines across $\\mathbf{44}$ simulation\ntasks and achieves superior performance in $\\mathbf{4}$ challenging bimanual\nreal-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5b66\u4e60\u6846\u67b6H3DP\uff0c\u901a\u8fc7\u4e09\u5c42\u5c42\u6b21\u7ed3\u6784\u52a0\u5f3a\u89c6\u89c9\u7279\u5f81\u4e0e\u52a8\u4f5c\u751f\u6210\u7684\u8026\u5408\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u89c6\u89c9\u611f\u77e5\u4e0e\u52a8\u4f5c\u9884\u6d4b\u7684\u8026\u5408\uff0cH3DP\u65e8\u5728\u901a\u8fc7\u5c42\u6b21\u5316\u7ed3\u6784\u5f3a\u5316\u8fd9\u4e00\u6574\u5408\u3002", "method": "H3DP\u5305\u542b\u4e09\u5c42\u5c42\u6b21\uff1a\u6df1\u5ea6\u611f\u77e5\u8f93\u5165\u5206\u5c42\u3001\u591a\u5c3a\u5ea6\u89c6\u89c9\u8868\u5f81\u548c\u5c42\u6b21\u5316\u6761\u4ef6\u6269\u6563\u8fc7\u7a0b\u3002", "result": "H3DP\u572844\u4e2a\u4eff\u771f\u4efb\u52a1\u4e2d\u5e73\u5747\u76f8\u5bf9\u63d0\u534727.5%\uff0c\u5e76\u57284\u4e2a\u771f\u5b9e\u4e16\u754c\u53cc\u8fb9\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u5c42\u6b21\u5316\u8bbe\u8ba1\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u89c6\u89c9-\u52a8\u4f5c\u8026\u5408\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.07620", "pdf": "https://arxiv.org/pdf/2505.07620", "abs": "https://arxiv.org/abs/2505.07620", "authors": ["Simone Azeglio", "Victor Calbiague Garcia", "Guilhem Glaziou", "Peter Neri", "Olivier Marre", "Ulisse Ferrari"], "title": "Higher-Order Convolution Improves Neural Predictivity in the Retina", "categories": ["cs.CV", "cs.LG", "q-bio.NC"], "comment": null, "summary": "We present a novel approach to neural response prediction that incorporates\nhigher-order operations directly within convolutional neural networks (CNNs).\nOur model extends traditional 3D CNNs by embedding higher-order operations\nwithin the convolutional operator itself, enabling direct modeling of\nmultiplicative interactions between neighboring pixels across space and time.\nOur model increases the representational power of CNNs without increasing their\ndepth, therefore addressing the architectural disparity between deep artificial\nnetworks and the relatively shallow processing hierarchy of biological visual\nsystems. We evaluate our approach on two distinct datasets: salamander retinal\nganglion cell (RGC) responses to natural scenes, and a new dataset of mouse RGC\nresponses to controlled geometric transformations. Our higher-order CNN (HoCNN)\nachieves superior performance while requiring only half the training data\ncompared to standard architectures, demonstrating correlation coefficients up\nto 0.75 with neural responses (against 0.80$\\pm$0.02 retinal reliability). When\nintegrated into state-of-the-art architectures, our approach consistently\nimproves performance across different species and stimulus conditions. Analysis\nof the learned representations reveals that our network naturally encodes\nfundamental geometric transformations, particularly scaling parameters that\ncharacterize object expansion and contraction. This capability is especially\nrelevant for specific cell types, such as transient OFF-alpha and transient ON\ncells, which are known to detect looming objects and object motion\nrespectively, and where our model shows marked improvement in response\nprediction. The correlation coefficients for scaling parameters are more than\ntwice as high in HoCNN (0.72) compared to baseline models (0.32).", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u9ad8\u9636\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08HoCNN\uff09\uff0c\u901a\u8fc7\u76f4\u63a5\u5728\u5377\u79ef\u64cd\u4f5c\u4e2d\u5d4c\u5165\u9ad8\u9636\u8fd0\u7b97\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u8868\u73b0\u529b\u800c\u4e0d\u589e\u52a0\u6df1\u5ea6\uff0c\u5e76\u5728\u9884\u6d4b\u795e\u7ecf\u54cd\u5e94\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u4eba\u5de5\u7f51\u7edc\u4e0e\u751f\u7269\u89c6\u89c9\u7cfb\u7edf\u6d45\u5c42\u5904\u7406\u5c42\u6b21\u4e4b\u95f4\u7684\u67b6\u6784\u5dee\u5f02\uff0c\u540c\u65f6\u63d0\u5347CNN\u7684\u8868\u73b0\u529b\u3002", "method": "\u5728\u4f20\u7edf3D CNN\u4e2d\u5d4c\u5165\u9ad8\u9636\u8fd0\u7b97\uff0c\u76f4\u63a5\u5efa\u6a21\u7a7a\u95f4\u548c\u65f6\u95f4\u7684\u50cf\u7d20\u95f4\u4e58\u6cd5\u4ea4\u4e92\u3002", "result": "HoCNN\u5728\u4e24\u79cd\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u6807\u51c6\u67b6\u6784\uff0c\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u51cf\u534a\uff0c\u76f8\u5173\u6027\u7cfb\u6570\u8fbe0.75\uff0c\u51e0\u4f55\u53d8\u6362\u7f16\u7801\u80fd\u529b\u663e\u8457\u63d0\u5347\u3002", "conclusion": "HoCNN\u4e0d\u4ec5\u63d0\u5347\u4e86\u795e\u7ecf\u54cd\u5e94\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u80fd\u81ea\u7136\u7f16\u7801\u51e0\u4f55\u53d8\u6362\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u7279\u5b9a\u7ec6\u80de\u7c7b\u578b\u7684\u54cd\u5e94\u9884\u6d4b\u3002"}}
{"id": "2505.07640", "pdf": "https://arxiv.org/pdf/2505.07640", "abs": "https://arxiv.org/abs/2505.07640", "authors": ["Haolin Zou", "Arnab Auddy", "Yongchan Kwon", "Kamiar Rahnama Rad", "Arian Maleki"], "title": "Certified Data Removal Under High-dimensional Settings", "categories": ["stat.ML", "cs.LG"], "comment": "46 pages, 4 figures", "summary": "Machine unlearning focuses on the computationally efficient removal of\nspecific training data from trained models, ensuring that the influence of\nforgotten data is effectively eliminated without the need for full retraining.\nDespite advances in low-dimensional settings, where the number of parameters \\(\np \\) is much smaller than the sample size \\( n \\), extending similar\ntheoretical guarantees to high-dimensional regimes remains challenging. We\npropose an unlearning algorithm that starts from the original model parameters\nand performs a theory-guided sequence of Newton steps \\( T \\in \\{ 1,2\\}\\).\nAfter this update, carefully scaled isotropic Laplacian noise is added to the\nestimate to ensure that any (potential) residual influence of forget data is\ncompletely removed. We show that when both \\( n, p \\to \\infty \\) with a fixed\nratio \\( n/p \\), significant theoretical and computational obstacles arise due\nto the interplay between the complexity of the model and the finite\nsignal-to-noise ratio. Finally, we show that, unlike in low-dimensional\nsettings, a single Newton step is insufficient for effective unlearning in\nhigh-dimensional problems -- however, two steps are enough to achieve the\ndesired certifiebility. We provide numerical experiments to support the\ncertifiability and accuracy claims of this approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u673a\u5668\u9057\u5fd8\u7b97\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u5f15\u5bfc\u7684\u725b\u987f\u6b65\u9aa4\u548c\u566a\u58f0\u6dfb\u52a0\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u73af\u5883\u4e2d\u6570\u636e\u9057\u5fd8\u7684\u7406\u8bba\u4e0e\u8ba1\u7b97\u6311\u6218\uff0c\u8bc1\u660e\u4e24\u6b65\u8fed\u4ee3\u8db3\u4ee5\u5b9e\u73b0\u6709\u6548\u9057\u5fd8\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\u5728\u4f4e\u7ef4\u73af\u5883\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u9ad8\u7ef4\u5ea6\uff08\u53c2\u6570\u6570\u63a5\u8fd1\u6837\u672c\u91cf\uff09\u65f6\u7f3a\u4e4f\u7406\u8bba\u4fdd\u969c\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u786e\u4fdd\u9ad8\u7ef4\u73af\u5883\u4e0b\u6570\u636e\u5b8c\u5168\u4e14\u9ad8\u6548\u5730\u4ece\u6a21\u578b\u4e2d\u79fb\u9664\u3002", "method": "\u7b97\u6cd5\u4ece\u539f\u6a21\u578b\u53c2\u6570\u51fa\u53d1\uff0c\u6267\u884c1-2\u6b21\u7406\u8bba\u5f15\u5bfc\u7684\u725b\u987f\u6b65\u9aa4\uff0c\u968f\u540e\u6dfb\u52a0\u6807\u5ea6\u5316\u7684\u62c9\u666e\u62c9\u65af\u566a\u58f0\u4ee5\u5f7b\u5e95\u6d88\u9664\u6b8b\u7559\u6570\u636e\u5f71\u54cd\uff0c\u91cd\u70b9\u89e3\u51b3\u9ad8\u7ef4\u4e0b\u4fe1\u53f7\u566a\u58f0\u6bd4\u4e0e\u6a21\u578b\u590d\u6742\u5ea6\u7684\u4ea4\u4e92\u95ee\u9898\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5355\u6b65\u725b\u987f\u8fed\u4ee3\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u9ad8\u7ef4\u9057\u5fd8\uff0c\u4f46\u4e24\u6b65\u8fed\u4ee3\u53ef\u8fbe\u5230\u53ef\u8ba4\u8bc1\u7684\u9057\u5fd8\u6548\u679c\u3002\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u8ba4\u8bc1\u6027\uff08\u65e0\u6b8b\u7559\u5f71\u54cd\uff09\u548c\u51c6\u786e\u6027\uff08\u6a21\u578b\u6027\u80fd\u4fdd\u6301\uff09\u3002", "conclusion": "\u4e24\u6b65\u725b\u987f\u66f4\u65b0\u52a0\u566a\u58f0\u7684\u6846\u67b6\u4e3a\u9ad8\u7ef4\u673a\u5668\u9057\u5fd8\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6491\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u5f25\u5408\u4e86\u4f4e\u7ef4\u4e0e\u9ad8\u7ef4\u7684\u7406\u8bba\u5dee\u8ddd\u3002"}}
{"id": "2505.07642", "pdf": "https://arxiv.org/pdf/2505.07642", "abs": "https://arxiv.org/abs/2505.07642", "authors": ["Yulong Lu", "Pierre Monmarch\u00e9"], "title": "Convergence of Time-Averaged Mean Field Gradient Descent Dynamics for Continuous Multi-Player Zero-Sum Games", "categories": ["math.OC", "cs.LG", "math.AP", "math.PR", "stat.ML", "35Q89, 49N80, 91A16, 90C47"], "comment": "21 pages", "summary": "The approximation of mixed Nash equilibria (MNE) for zero-sum games with\nmean-field interacting players has recently raised much interest in machine\nlearning. In this paper we propose a mean-field gradient descent dynamics for\nfinding the MNE of zero-sum games involving $K$ players with $K\\geq 2$. The\nevolution of the players' strategy distributions follows coupled mean-field\ngradient descent flows with momentum, incorporating an exponentially discounted\ntime-averaging of gradients. First, in the case of a fixed entropic\nregularization, we prove an exponential convergence rate for the mean-field\ndynamics to the mixed Nash equilibrium with respect to the total variation\nmetric. This improves a previous polynomial convergence rate for a similar\ntime-averaged dynamics with different averaging factors. Moreover, unlike\nprevious two-scale approaches for finding the MNE, our approach treats all\nplayer types on the same time scale. We also show that with a suitable choice\nof decreasing temperature, a simulated annealing version of the mean-field\ndynamics converges to an MNE of the initial unregularized problem.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5747\u503c\u573a\u68af\u5ea6\u4e0b\u964d\u52a8\u529b\u5b66\u6765\u8fd1\u4f3c\u6c42\u89e3\u96f6\u548c\u535a\u5f08\u4e2d\u6df7\u5408\u7eb3\u4ec0\u5747\u8861\u7684\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5728\u56fa\u5b9a\u71b5\u6b63\u5219\u5316\u4e0b\u6307\u6570\u6536\u655b\u901f\u5ea6\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u9000\u706b\u65b9\u6cd5\u6536\u655b\u5230\u672a\u6b63\u5219\u5316\u95ee\u9898\u7684\u89e3\u3002", "motivation": "\u7814\u7a76\u96f6\u548c\u535a\u5f08\u4e2d\u6df7\u5408\u7eb3\u4ec0\u5747\u8861\u7684\u8fd1\u4f3c\u6c42\u89e3\u95ee\u9898\uff0c\u65e8\u5728\u6539\u8fdb\u73b0\u6709\u7684\u65b9\u6cd5\u5e76\u63d0\u5347\u6536\u655b\u901f\u5ea6\u3002", "method": "\u91c7\u7528\u5747\u503c\u573a\u68af\u5ea6\u4e0b\u964d\u52a8\u529b\u5b66\uff0c\u7ed3\u5408\u52a8\u91cf\u6cd5\u548c\u6307\u6570\u6298\u6263\u7684\u65f6\u95f4\u5e73\u5747\u68af\u5ea6\uff0c\u5904\u7406$K\\geq 2$\u73a9\u5bb6\u7684\u7b56\u7565\u5206\u5e03\u6f14\u5316\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u56fa\u5b9a\u71b5\u6b63\u5219\u5316\u4e0b\uff0c\u65b9\u6cd5\u80fd\u6307\u6570\u6536\u655b\u5230\u6df7\u5408\u7eb3\u4ec0\u5747\u8861\uff1b\u6a21\u62df\u9000\u706b\u7248\u672c\u80fd\u6536\u655b\u5230\u672a\u6b63\u5219\u5316\u95ee\u9898\u7684\u89e3\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6536\u655b\u901f\u5ea6\u548c\u7edf\u4e00\u65f6\u95f4\u5c3a\u5ea6\u5904\u7406\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u9002\u7528\u4e8e\u672a\u6b63\u5219\u5316\u95ee\u9898\u3002"}}
{"id": "2505.07676", "pdf": "https://arxiv.org/pdf/2505.07676", "abs": "https://arxiv.org/abs/2505.07676", "authors": ["Nicolas Camenzind", "Damir Filipovic"], "title": "Transfer Learning Across Fixed-Income Product Classes", "categories": ["stat.ML", "cs.LG", "q-fin.CP", "q-fin.MF"], "comment": null, "summary": "We propose a framework for transfer learning of discount curves across\ndifferent fixed-income product classes. Motivated by challenges in estimating\ndiscount curves from sparse or noisy data, we extend kernel ridge regression\n(KR) to a vector-valued setting, formulating a convex optimization problem in a\nvector-valued reproducing kernel Hilbert space (RKHS). Each component of the\nsolution corresponds to the discount curve implied by a specific product class.\nWe introduce an additional regularization term motivated by economic\nprinciples, promoting smoothness of spread curves between product classes, and\nshow that it leads to a valid separable kernel structure. A main theoretical\ncontribution is a decomposition of the vector-valued RKHS norm induced by\nseparable kernels. We further provide a Gaussian process interpretation of\nvector-valued KR, enabling quantification of estimation uncertainty.\nIllustrative examples demonstrate that transfer learning significantly improves\nextrapolation performance and tightens confidence intervals compared to\nsingle-curve estimation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u56fa\u5b9a\u6536\u76ca\u4ea7\u54c1\u7c7b\u522b\u7684\u6298\u73b0\u66f2\u7ebf\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u5c55\u6838\u5cad\u56de\u5f52\u5230\u5411\u91cf\u503c\u8bbe\u7f6e\uff0c\u5e76\u5f15\u5165\u7ecf\u6d4e\u539f\u5219\u9a71\u52a8\u7684\u6b63\u5219\u5316\u9879\uff0c\u663e\u8457\u63d0\u5347\u4e86\u66f2\u7ebf\u5916\u63a8\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u7a00\u758f\u6216\u566a\u58f0\u6570\u636e\u4e2d\u4f30\u8ba1\u6298\u73b0\u66f2\u7ebf\u7684\u6311\u6218\uff0c\u65e8\u5728\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u63d0\u5347\u4e0d\u540c\u4ea7\u54c1\u7c7b\u522b\u95f4\u7684\u66f2\u7ebf\u4f30\u8ba1\u7cbe\u5ea6\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5c06\u6838\u5cad\u56de\u5f52\u6269\u5c55\u5230\u5411\u91cf\u503c\u8bbe\u7f6e\uff0c\u6784\u5efa\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5728\u5411\u91cf\u503c\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u5f15\u5165\u7ecf\u6d4e\u539f\u5219\u9a71\u52a8\u7684\u6b63\u5219\u5316\u9879\u3002", "result": "\u7ed3\u679c\u8868\u660e\u8fc1\u79fb\u5b66\u4e60\u663e\u8457\u6539\u5584\u4e86\u5916\u63a8\u6027\u80fd\uff0c\u5e76\u7f29\u5c0f\u4e86\u7f6e\u4fe1\u533a\u95f4\uff0c\u76f8\u6bd4\u5355\u66f2\u7ebf\u4f30\u8ba1\u66f4\u5177\u4f18\u52bf\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\u6240\u63d0\u6846\u67b6\u901a\u8fc7\u7ecf\u6d4e\u539f\u5219\u6b63\u5219\u5316\u548c\u5411\u91cf\u503c\u6838\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6298\u73b0\u66f2\u7ebf\u7684\u8de8\u4ea7\u54c1\u7c7b\u522b\u8fc1\u79fb\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2505.07688", "pdf": "https://arxiv.org/pdf/2505.07688", "abs": "https://arxiv.org/abs/2505.07688", "authors": ["Renzhe Xu", "Kang Wang", "Bo Li"], "title": "Heterogeneous Data Game: Characterizing the Model Competition Across Multiple Data Sources", "categories": ["cs.GT", "cs.LG"], "comment": "ICML 2025", "summary": "Data heterogeneity across multiple sources is common in real-world machine\nlearning (ML) settings. Although many methods focus on enabling a single model\nto handle diverse data, real-world markets often comprise multiple competing ML\nproviders. In this paper, we propose a game-theoretic framework -- the\nHeterogeneous Data Game -- to analyze how such providers compete across\nheterogeneous data sources. We investigate the resulting pure Nash equilibria\n(PNE), showing that they can be non-existent, homogeneous (all providers\nconverge on the same model), or heterogeneous (providers specialize in distinct\ndata sources). Our analysis spans monopolistic, duopolistic, and more general\nmarkets, illustrating how factors such as the \"temperature\" of data-source\nchoice models and the dominance of certain data sources shape equilibrium\noutcomes. We offer theoretical insights into both homogeneous and heterogeneous\nPNEs, guiding regulatory policies and practical strategies for competitive ML\nmarketplaces.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u201c\u5f02\u8d28\u6570\u636e\u535a\u5f08\u201d\u7684\u535a\u5f08\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u4e0d\u540c\u673a\u5668\u5b66\u4e60\u670d\u52a1\u63d0\u4f9b\u5546\u5728\u5f02\u8d28\u6570\u636e\u6e90\u4e0a\u7684\u7ade\u4e89\u60c5\u51b5\uff0c\u7814\u7a76\u4e86\u7eaf\u7eb3\u4ec0\u5747\u8861\u7684\u5f62\u6001\u53ca\u5176\u5f71\u54cd\u56e0\u7d20\uff0c\u4e3a\u7ade\u4e89\u6027ML\u5e02\u573a\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u6307\u5bfc\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u5e02\u573a\u5e38\u7531\u591a\u4e2a\u7ade\u4e89\u6027\u63d0\u4f9b\u5546\u7ec4\u6210\uff0c\u800c\u6570\u636e\u5f02\u8d28\u6027\u666e\u904d\u5b58\u5728\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u5173\u6ce8\u5355\u4e00\u6a21\u578b\u5904\u7406\u591a\u6837\u6027\u6570\u636e\uff0c\u7f3a\u4e4f\u5bf9\u591a\u63d0\u4f9b\u5546\u7ade\u4e89\u7684\u7406\u8bba\u5206\u6790\u3002", "method": "\u63d0\u51fa\u4e86\u201c\u5f02\u8d28\u6570\u636e\u535a\u5f08\u201d\u6846\u67b6\uff0c\u5206\u6790\u63d0\u4f9b\u5546\u5728\u5f02\u8d28\u6570\u636e\u6e90\u4e0a\u7684\u7ade\u4e89\u884c\u4e3a\uff0c\u91cd\u70b9\u7814\u7a76\u7eaf\u7eb3\u4ec0\u5747\u8861\uff08PNE\uff09\u7684\u5b58\u5728\u6027\u4e0e\u5f62\u6001\uff08\u540c\u8d28\u6216\u5f02\u8d28\uff09\uff0c\u6db5\u76d6\u5784\u65ad\u3001\u53cc\u5be1\u5934\u53ca\u66f4\u4e00\u822c\u7684\u5e02\u573a\u60c5\u666f\u3002", "result": "\u53d1\u73b0\u5747\u8861\u53ef\u80fd\u662f\u975e\u5b58\u5728\u3001\u540c\u8d28\uff08\u6240\u6709\u63d0\u4f9b\u5546\u8d8b\u540c\uff09\u6216\u5f02\u8d28\uff08\u63d0\u4f9b\u5546\u5404\u81ea\u4e13\u7cbe\u4e0d\u540c\u6570\u636e\u6e90\uff09\uff0c\u5e76\u63ed\u793a\u4e86\u6570\u636e\u9009\u62e9\u6a21\u578b\u7684\u201c\u6e29\u5ea6\u201d\u548c\u4f18\u52bf\u6570\u636e\u6e90\u5bf9\u5747\u8861\u5f62\u6001\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7ade\u4e89\u6027ML\u5e02\u573a\u7684\u76d1\u7ba1\u653f\u7b56\u548c\u5b9e\u8df5\u7b56\u7565\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u5f3a\u8c03\u4e86\u5747\u8861\u5f62\u6001\u7684\u591a\u6837\u6027\u548c\u5176\u80cc\u540e\u7684\u9a71\u52a8\u56e0\u7d20\u3002"}}
{"id": "2505.07709", "pdf": "https://arxiv.org/pdf/2505.07709", "abs": "https://arxiv.org/abs/2505.07709", "authors": ["Daniel Haider", "Felix Perfler", "Peter Balazs", "Clara Hollomey", "Nicki Holighaus"], "title": "ISAC: An Invertible and Stable Auditory Filter Bank with Customizable Kernels for ML Integration", "categories": ["cs.SD", "cs.LG"], "comment": "Accepted at the IEEE International Conference on Sampling Theory and\n  Applications (SampTA) 2025", "summary": "This paper introduces ISAC, an invertible and stable, perceptually-motivated\nfilter bank that is specifically designed to be integrated into machine\nlearning paradigms. More precisely, the center frequencies and bandwidths of\nthe filters are chosen to follow a non-linear, auditory frequency scale, the\nfilter kernels have user-defined maximum temporal support and may serve as\nlearnable convolutional kernels, and there exists a corresponding filter bank\nsuch that both form a perfect reconstruction pair. ISAC provides a powerful and\nuser-friendly audio front-end suitable for any application, including\nanalysis-synthesis schemes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86ISAC\uff0c\u4e00\u79cd\u53ef\u9006\u4e14\u7a33\u5b9a\u7684\u611f\u77e5\u9a71\u52a8\u6ee4\u6ce2\u5668\u7ec4\uff0c\u4e13\u4e3a\u96c6\u6210\u5230\u673a\u5668\u5b66\u4e60\u8303\u5f0f\u800c\u8bbe\u8ba1\u3002\u5b83\u91c7\u7528\u975e\u7ebf\u6027\u542c\u89c9\u9891\u7387\u5c3a\u5ea6\uff0c\u652f\u6301\u53ef\u5b66\u4e60\u5377\u79ef\u6838\uff0c\u5e76\u5b9e\u73b0\u5b8c\u7f8e\u91cd\u5efa\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u79cd\u9002\u5408\u673a\u5668\u5b66\u4e60\u5e94\u7528\u7684\u97f3\u9891\u524d\u7aef\uff0c\u7ed3\u5408\u542c\u89c9\u611f\u77e5\u7279\u6027\uff0c\u63d0\u4f9b\u7075\u6d3b\u6027\u548c\u5b8c\u7f8e\u91cd\u5efa\u80fd\u529b\u3002", "method": "\u91c7\u7528\u975e\u7ebf\u6027\u542c\u89c9\u9891\u7387\u5c3a\u5ea6\u8bbe\u8ba1\u6ee4\u6ce2\u5668\u7ec4\uff0c\u652f\u6301\u7528\u6237\u5b9a\u4e49\u7684\u6700\u5927\u65f6\u95f4\u652f\u6301\uff0c\u5e76\u4f5c\u4e3a\u53ef\u5b66\u4e60\u5377\u79ef\u6838\u3002", "result": "ISAC\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u4e14\u7528\u6237\u53cb\u597d\u7684\u97f3\u9891\u524d\u7aef\uff0c\u9002\u7528\u4e8e\u5305\u62ec\u5206\u6790-\u5408\u6210\u65b9\u6848\u5728\u5185\u7684\u591a\u79cd\u5e94\u7528\u3002", "conclusion": "ISAC\u6ee4\u6ce2\u5668\u7ec4\u5728\u673a\u5668\u5b66\u4e60\u548c\u97f3\u9891\u5904\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u5907\u53ef\u9006\u6027\u3001\u7a33\u5b9a\u6027\u548c\u611f\u77e5\u9a71\u52a8\u7279\u6027\u3002"}}
{"id": "2505.07714", "pdf": "https://arxiv.org/pdf/2505.07714", "abs": "https://arxiv.org/abs/2505.07714", "authors": ["Almoatssimbillah Saifaldawla", "Eva Lagunas", "Flor Ortiz", "Abuzar B. M. Adam", "Symeon Chatzinotas"], "title": "SmartUT: Receive Beamforming for Spectral Coexistence of NGSO Satellite Systems", "categories": ["eess.SP", "cs.ET", "cs.LG"], "comment": null, "summary": "In this paper, we investigate downlink co-frequency interference (CFI)\nmitigation in non-geostationary satellites orbits (NGSOs) co-existing systems.\nTraditional mitigation techniques, such as Zero-forcing (ZF), produce a null\ntowards the direction of arrivals (DOAs) of the interfering signals, but they\nsuffer from high computational complexity due to matrix inversions and required\nknowledge of the channel state information (CSI). Furthermore, adaptive\nbeamformers, such as sample matrix inversion (SMI)-based minimum variance,\nprovide poor performance when the available snapshots are limited. We propose a\nMamba-based beamformer (MambaBF) that leverages an unsupervised deep learning\n(DL) approach and can be deployed on the user terminal (UT) antenna array, for\nassisting downlink beamforming and CFI mitigation using only a limited number\nof available array snapshots as input, and without CSI knowledge. Simulation\nresults demonstrate that MambaBF consistently outperforms conventional\nbeamforming techniques in mitigating interference and maximizing the\nsignal-to-interference-plus-noise ratio (SINR), particularly under challenging\nconditions characterized by low SINR, limited snapshots, and imperfect CSI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba\u7684\u65e0\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u6ce2\u675f\u6210\u5f62\u5668\uff08MambaBF\uff09\uff0c\u7528\u4e8e\u975e\u9759\u6b62\u8f68\u9053\u536b\u661f\u7cfb\u7edf\u4e2d\u7684\u540c\u9891\u5e72\u6270\u6291\u5236\uff0c\u65e0\u9700\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u5373\u53ef\u5728\u6709\u9650\u5feb\u7167\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5e72\u6270\u6291\u5236\u65b9\u6cd5\uff08\u5982\u96f6\u8feb\uff09\u56e0\u8ba1\u7b97\u590d\u6742\u4e14\u4f9d\u8d56\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u800c\u53d7\u9650\uff0c\u800c\u81ea\u9002\u5e94\u6ce2\u675f\u6210\u5f62\u5668\uff08\u5982SMI\uff09\u5728\u5feb\u7167\u4e0d\u8db3\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u91c7\u7528Mamba\u6846\u67b6\u7684\u65e0\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ec5\u9700\u6709\u9650\u5feb\u7167\u8f93\u5165\uff0c\u65e0\u9700CSI\u77e5\u8bc6\uff0c\u76f4\u63a5\u5728\u7528\u6237\u7ec8\u7aef\u5929\u7ebf\u9635\u5217\u4e0a\u90e8\u7f72\u5b9e\u73b0\u6ce2\u675f\u6210\u5f62\u548c\u5e72\u6270\u6291\u5236\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0cMambaBF\u5728\u4f4e\u4fe1\u5e72\u566a\u6bd4\u3001\u5feb\u7167\u6709\u9650\u548cCSI\u4e0d\u5b8c\u7f8e\u7684\u6311\u6218\u6027\u573a\u666f\u4e0b\uff0c\u5e72\u6270\u6291\u5236\u6548\u679c\u548c\u4fe1\u53f7\u8d28\u91cf\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "MambaBF\u4e3a\u536b\u661f\u901a\u4fe1\u4e2d\u7684\u540c\u9891\u5e72\u6270\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2505.07719", "pdf": "https://arxiv.org/pdf/2505.07719", "abs": "https://arxiv.org/abs/2505.07719", "authors": ["Hyunwoo Oh"], "title": "Training neural control variates using correlated configurations", "categories": ["hep-lat", "cs.LG", "nucl-th"], "comment": "8 pages, 6 figures", "summary": "Neural control variates (NCVs) have emerged as a powerful tool for variance\nreduction in Monte Carlo (MC) simulations, particularly in high-dimensional\nproblems where traditional control variates are difficult to construct\nanalytically. By training neural networks to learn auxiliary functions\ncorrelated with the target observable, NCVs can significantly reduce estimator\nvariance while preserving unbiasedness. However, a critical but often\noverlooked aspect of NCV training is the role of autocorrelated samples\ngenerated by Markov Chain Monte Carlo (MCMC). While such samples are typically\ndiscarded for error estimation due to their statistical redundancy, they may\ncontain useful information about the structure of the underlying probability\ndistribution that can benefit the training process. In this work, we\nsystematically examine the effect of using correlated configurations in\ntraining neural control variates. We demonstrate, both conceptually and\nnumerically, that training on correlated data can improve control variate\nperformance, especially in settings with limited computational resources. Our\nanalysis includes empirical results from $U(1)$ gauge theory and scalar field\ntheory, illustrating when and how autocorrelated samples enhance NCV\nconstruction. These findings provide practical guidance for the efficient use\nof MCMC data in training neural networks.", "AI": {"tldr": "\u6458\u8981\u5206\u6790\u4e86\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u53d8\u91cf\uff08NCV\uff09\u5728\u8499\u7279\u5361\u6d1b\u6a21\u62df\u4e2d\u7684\u65b9\u5dee\u51cf\u5c11\u4f5c\u7528\uff0c\u7279\u522b\u63a2\u8ba8\u4e86\u4f7f\u7528\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\uff08MCMC\uff09\u751f\u6210\u7684**\u81ea\u76f8\u5173\u6837\u672c**\u5bf9NCV\u8bad\u7ec3\u7684\u6f5c\u5728\u76ca\u5904\u3002", "motivation": "\u4f20\u7edf\u63a7\u5236\u53d8\u91cf\u5728\u9ad8\u7ef4\u95ee\u9898\u4e2d\u96be\u4ee5\u6784\u5efa\uff0cNCV\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u4e0e\u76ee\u6807\u89c2\u6d4b\u76f8\u5173\u7684\u8f85\u52a9\u51fd\u6570\u4ee5\u51cf\u5c11\u65b9\u5dee\uff0c\u4f46MCMC\u751f\u6210\u7684\u81ea\u76f8\u5173\u6837\u672c\u5e38\u88ab\u5ffd\u7565\uff0c\u53ef\u80fd\u8574\u542b\u6709\u52a9\u4e8e\u8bad\u7ec3\u7684\u6982\u7387\u5206\u5e03\u7ed3\u6784\u4fe1\u606f\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u4e86\u81ea\u76f8\u5173\u6837\u672c\u5728NCV\u8bad\u7ec3\u4e2d\u7684\u4f5c\u7528\uff0c\u5305\u62ec\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u5b9e\u9a8c\uff08\u4ee5$U(1)$\u89c4\u8303\u7406\u8bba\u548c\u6807\u91cf\u573a\u7406\u8bba\u4e3a\u4f8b\uff09\u3002", "result": "\u4f7f\u7528\u81ea\u76f8\u5173\u6570\u636e\u8bad\u7ec3\u80fd\u63d0\u5347\u63a7\u5236\u53d8\u91cf\u6027\u80fd\uff0c\u5c24\u5176\u5728\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u65f6\u3002\u5b9e\u9a8c\u7ed3\u679c\u5c55\u793a\u4e86\u81ea\u76f8\u5173\u6837\u672c\u5982\u4f55\u4f18\u5316NCV\u6784\u5efa\u3002", "conclusion": "\u7814\u7a76\u4e3a\u9ad8\u6548\u5229\u7528MCMC\u6570\u636e\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5f3a\u8c03\u4e86\u81ea\u76f8\u5173\u6837\u672c\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u7684\u4ef7\u503c\u3002"}}
{"id": "2505.07744", "pdf": "https://arxiv.org/pdf/2505.07744", "abs": "https://arxiv.org/abs/2505.07744", "authors": ["Halid Ziya Yerebakan", "Kritika Iyer", "Xueqi Guo", "Yoshihisa Shinagawa", "Gerardo Hermosillo Valadez"], "title": "BodyGPS: Anatomical Positioning System", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We introduce a new type of foundational model for parsing human anatomy in\nmedical images that works for different modalities. It supports supervised or\nunsupervised training and can perform matching, registration, classification,\nor segmentation with or without user interaction. We achieve this by training a\nneural network estimator that maps query locations to atlas coordinates via\nregression. Efficiency is improved by sparsely sampling the input, enabling\nresponse times of less than 1 ms without additional accelerator hardware. We\ndemonstrate the utility of the algorithm in both CT and MRI modalities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u6790\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u4eba\u4f53\u89e3\u5256\u7ed3\u6784\uff0c\u652f\u6301\u591a\u79cd\u6a21\u6001\u548c\u76d1\u7763/\u65e0\u76d1\u7763\u8bad\u7ec3\uff0c\u5b9e\u73b0\u5feb\u901f\u54cd\u5e94\uff08<1\u6beb\u79d2\uff09\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u4e2d\u591a\u6a21\u6001\u89e3\u5256\u7ed3\u6784\u89e3\u6790\u7684\u901a\u7528\u6027\u548c\u6548\u7387\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u4f30\u8ba1\u5668\uff0c\u5c06\u67e5\u8be2\u4f4d\u7f6e\u6620\u5c04\u5230\u56fe\u8c31\u5750\u6807\uff0c\u7a00\u758f\u91c7\u6837\u8f93\u5165\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u5728CT\u548cMRI\u6a21\u6001\u4e2d\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u5b9e\u7528\u6027\uff0c\u54cd\u5e94\u65f6\u95f4\u5c0f\u4e8e1\u6beb\u79d2\u3002", "conclusion": "\u8be5\u6a21\u578b\u5177\u6709\u9ad8\u6548\u6027\u548c\u591a\u6a21\u6001\u9002\u7528\u6027\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2505.07765", "pdf": "https://arxiv.org/pdf/2505.07765", "abs": "https://arxiv.org/abs/2505.07765", "authors": ["Zihan Shao", "Konstantin Pieper", "Xiaochuan Tian"], "title": "Solving Nonlinear PDEs with Sparse Radial Basis Function Networks", "categories": ["math.NA", "cs.LG", "cs.NA"], "comment": "35 pages, 7 figures", "summary": "We propose a novel framework for solving nonlinear PDEs using sparse radial\nbasis function (RBF) networks. Sparsity-promoting regularization is employed to\nprevent over-parameterization and reduce redundant features. This work is\nmotivated by longstanding challenges in traditional RBF collocation methods,\nalong with the limitations of physics-informed neural networks (PINNs) and\nGaussian process (GP) approaches, aiming to blend their respective strengths in\na unified framework. The theoretical foundation of our approach lies in the\nfunction space of Reproducing Kernel Banach Spaces (RKBS) induced by\none-hidden-layer neural networks of possibly infinite width. We prove a\nrepresenter theorem showing that the solution to the sparse optimization\nproblem in the RKBS admits a finite solution and establishes error bounds that\noffer a foundation for generalizing classical numerical analysis. The\nalgorithmic framework is based on a three-phase algorithm to maintain\ncomputational efficiency through adaptive feature selection, second-order\noptimization, and pruning of inactive neurons. Numerical experiments\ndemonstrate the effectiveness of our method and highlight cases where it offers\nnotable advantages over GP approaches. This work opens new directions for\nadaptive PDE solvers grounded in rigorous analysis with efficient,\nlearning-inspired implementation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u7a00\u758f\u5f84\u5411\u57fa\u51fd\u6570\uff08RBF\uff09\u7f51\u7edc\u6c42\u89e3\u975e\u7ebf\u6027PDE\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u4fc3\u8fdb\u6b63\u5219\u5316\u907f\u514d\u8fc7\u53c2\u6570\u5316\uff0c\u7ed3\u5408\u4e86\u4f20\u7edfRBF\u914d\u7f6e\u6cd5\u3001\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u548c\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u65b9\u6cd5\u7684\u4f18\u70b9\uff0c\u5e76\u5728\u7406\u8bba\u4e0a\u57fa\u4e8e\u518d\u751f\u6838Banach\u7a7a\u95f4\uff08RKBS\uff09\uff0c\u8bc1\u660e\u4e86\u7a00\u758f\u4f18\u5316\u95ee\u9898\u7684\u6709\u9650\u89e3\u5b58\u5728\u6027\u548c\u8bef\u5dee\u754c\u9650\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u4f18\u4e8eGP\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfRBF\u914d\u7f6e\u6cd5\u3001PINN\u548cGP\u65b9\u6cd5\u5404\u81ea\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u8fc7\u53c2\u6570\u5316\u6216\u8ba1\u7b97\u6548\u7387\u4f4e\uff0c\u672c\u6587\u65e8\u5728\u7ed3\u5408\u8fd9\u4e9b\u65b9\u6cd5\u7684\u4f18\u70b9\uff0c\u63d0\u51fa\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u91c7\u7528\u57fa\u4e8eRKBS\u7406\u8bba\u7684\u4e09\u9636\u6bb5\u7b97\u6cd5\uff0c\u5305\u62ec\u81ea\u9002\u5e94\u7279\u5f81\u9009\u62e9\u3001\u4e8c\u9636\u4f18\u5316\u548c\u4e0d\u6d3b\u8dc3\u795e\u7ecf\u5143\u526a\u679d\uff0c\u4ee5\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u7a00\u758f\u4f18\u5316\u95ee\u9898\u7684\u6709\u9650\u89e3\u548c\u8bef\u5dee\u754c\u9650\uff0c\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u4f18\u4e8eGP\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u57fa\u4e8e\u4e25\u683c\u5206\u6790\u548c\u9ad8\u6548\u5b66\u4e60\u7684\u81ea\u9002\u5e94PDE\u6c42\u89e3\u5668\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.07769", "pdf": "https://arxiv.org/pdf/2505.07769", "abs": "https://arxiv.org/abs/2505.07769", "authors": ["Jai Bardhan", "Tanumoy Mandal", "Subhadip Mitra", "Cyrin Neeraj", "Mihir Rawat"], "title": "Tagging fully hadronic exotic decays of the vectorlike $\\mathbf{B}$ quark using a graph neural network", "categories": ["hep-ph", "cs.LG", "hep-ex"], "comment": "13 pages, 10 figures, 3 tables", "summary": "Following up on our earlier study in [J. Bardhan et al., Machine\nlearning-enhanced search for a vectorlike singlet B quark decaying to a singlet\nscalar or pseudoscalar, Phys. Rev. D 107 (2023) 115001; arXiv:2212.02442], we\ninvestigate the LHC prospects of pair-produced vectorlike $B$ quarks decaying\nexotically to a new gauge-singlet (pseudo)scalar field $\\Phi$ and a $b$ quark.\nAfter the electroweak symmetry breaking, the $\\Phi$ decays predominantly to\n$gg/bb$ final states, leading to a fully hadronic $2b+4j$ or $6b$ signature.\nBecause of the large Standard Model background and the lack of leptonic\nhandles, it is a difficult channel to probe. To overcome the challenge, we\nemploy a hybrid deep learning model containing a graph neural network followed\nby a deep neural network. We estimate that such a state-of-the-art deep\nlearning analysis pipeline can lead to a performance comparable to that in the\nsemi-leptonic mode, taking the discovery (exclusion) reach up to about\n$M_B=1.8\\:(2.4)$~TeV at HL-LHC when $B$ decays fully exotically, i.e., BR$(B\n\\to b\\Phi) = 100\\%$.", "AI": {"tldr": "\u7814\u7a76\u4e86LHC\u4e0a\u77e2\u91cf\u5938\u514bB\u5bf9\u7684\u4ea7\u751f\u53ca\u5176\u901a\u8fc7\u65b0\u6807\u91cf\u573a\u03a6\u7684\u5947\u5f02\u8870\u53d8\u7684\u63a2\u6d4b\u6f5c\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4ee5\u63d0\u9ad8\u4fe1\u53f7\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22LHC\u4e0a\u96be\u4ee5\u63a2\u6d4b\u7684\u5168\u5f3a\u5b50\u8870\u53d8\u4fe1\u53f7\uff0c\u514b\u670d\u6807\u51c6\u6a21\u578b\u80cc\u666f\u5927\u4e14\u7f3a\u5c11\u8f7b\u5b50\u6807\u8bb0\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u5305\u542b\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5206\u67902b+4j\u62166b\u672b\u6001\u4fe1\u53f7\u3002", "result": "\u8be5\u6a21\u578b\u4f7f\u63a2\u6d4b\u7075\u654f\u5ea6\u63a5\u8fd1\u534a\u8f7b\u5b50\u6a21\u5f0f\uff0c\u9884\u8ba1\u5728HL-LHC\u4e0b\u5bf9B\u5938\u514b\u8d28\u91cf\u7684\u53d1\u73b0\uff08\u6392\u9664\uff09\u8303\u56f4\u53ef\u8fbe1.8\uff082.4\uff09TeV\u3002", "conclusion": "\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6709\u6548\u63d0\u5347\u4e86\u5168\u5f3a\u5b50\u8870\u53d8\u901a\u9053\u7684\u63a2\u6d4b\u80fd\u529b\uff0c\u4e3aLHC\u4e0a\u5947\u5f02\u8870\u53d8\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2505.07792", "pdf": "https://arxiv.org/pdf/2505.07792", "abs": "https://arxiv.org/abs/2505.07792", "authors": ["Francesco Mori", "Francesca Mignacco"], "title": "Analytic theory of dropout regularization", "categories": ["stat.ML", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.LG"], "comment": "17 pages, 8 figures", "summary": "Dropout is a regularization technique widely used in training artificial\nneural networks to mitigate overfitting. It consists of dynamically\ndeactivating subsets of the network during training to promote more robust\nrepresentations. Despite its widespread adoption, dropout probabilities are\noften selected heuristically, and theoretical explanations of its success\nremain sparse. Here, we analytically study dropout in two-layer neural networks\ntrained with online stochastic gradient descent. In the high-dimensional limit,\nwe derive a set of ordinary differential equations that fully characterize the\nevolution of the network during training and capture the effects of dropout. We\nobtain a number of exact results describing the generalization error and the\noptimal dropout probability at short, intermediate, and long training times.\nOur analysis shows that dropout reduces detrimental correlations between hidden\nnodes, mitigates the impact of label noise, and that the optimal dropout\nprobability increases with the level of noise in the data. Our results are\nvalidated by extensive numerical simulations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc\u4e2d\u4f7f\u7528Dropout\u7684\u6280\u672f\uff0c\u901a\u8fc7\u5206\u6790\u5f97\u51fa\u5176\u4f18\u5316\u8bad\u7ec3\u6548\u679c\u548c\u9002\u5e94\u6570\u636e\u566a\u58f0\u7684\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "motivation": "Dropout\u867d\u7136\u5e7f\u6cdb\u7528\u4e8e\u7f13\u89e3\u8fc7\u62df\u5408\uff0c\u4f46\u5176\u6982\u7387\u9009\u62e9\u548c\u7406\u8bba\u89e3\u91ca\u4ecd\u7136\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u56e0\u6b64\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5728\u9ad8\u7ef4\u6781\u9650\u4e0b\uff0c\u4f5c\u8005\u4f7f\u7528\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u901a\u8fc7\u4e00\u7ec4\u5e38\u5fae\u5206\u65b9\u7a0b\u5206\u6790Dropout\u5bf9\u8bad\u7ec3\u52a8\u6001\u7684\u5f71\u54cd\u3002", "result": "\u8bba\u6587\u5f97\u51fa\u4e86\u4e00\u7cfb\u5217\u7cbe\u786e\u7ed3\u679c\uff0c\u8868\u660eDropout\u80fd\u51cf\u5c11\u8282\u70b9\u95f4\u7684\u6709\u5bb3\u5173\u8054\u3001\u51cf\u8f7b\u6807\u7b7e\u566a\u58f0\u7684\u5f71\u54cd\uff0c\u4e14\u6700\u4f18\u6982\u7387\u968f\u6570\u636e\u566a\u58f0\u589e\u52a0\u800c\u63d0\u9ad8\u3002", "conclusion": "\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u5b9e\u9a8c\u8bc1\u5b9e\u4e86Dropout\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u566a\u58f0\u8f83\u5927\u7684\u60c5\u51b5\u4e0b\uff0c\u8fdb\u4e00\u6b65\u63a8\u5e7f\u4e86\u5176\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.07801", "pdf": "https://arxiv.org/pdf/2505.07801", "abs": "https://arxiv.org/abs/2505.07801", "authors": ["Bernardo P. Ferreira", "Miguel A. Bessa"], "title": "Automatically Differentiable Model Updating (ADiMU): conventional, hybrid, and neural network material model discovery including history-dependency", "categories": ["math.NA", "cs.LG", "cs.NA", "physics.comp-ph"], "comment": "77 pages, 50 figures", "summary": "We introduce the first Automatically Differentiable Model Updating (ADiMU)\nframework that finds any history-dependent material model from full-field\ndisplacement and global force data (global, indirect discovery) or from\nstrain-stress data (local, direct discovery). We show that ADiMU can update\nconventional (physics-based), neural network (data-driven), and hybrid material\nmodels. Moreover, this framework requires no fine-tuning of hyperparameters or\nadditional quantities beyond those inherent to the user-selected material model\narchitecture and optimizer. The robustness and versatility of ADiMU is\nextensively exemplified by updating different models spanning tens to millions\nof parameters, in both local and global discovery settings. Relying on fully\ndifferentiable code, the algorithmic implementation leverages vectorizing maps\nthat enable history-dependent automatic differentiation via efficient batched\nexecution of shared computation graphs. This contribution also aims to\nfacilitate the integration, evaluation and application of future material model\narchitectures by openly supporting the research community. Therefore, ADiMU is\nreleased as an open-source computational tool, integrated into a carefully\ndesigned and documented software named HookeAI.", "AI": {"tldr": "ADiMU \u662f\u4e00\u79cd\u81ea\u52a8\u53ef\u5fae\u5206\u6a21\u578b\u66f4\u65b0\u6846\u67b6\uff0c\u80fd\u4ece\u5168\u5c40\uff08\u4f4d\u79fb\u548c\u529b\u6570\u636e\uff09\u6216\u5c40\u90e8\uff08\u5e94\u53d8-\u5e94\u529b\u6570\u636e\uff09\u6570\u636e\u4e2d\u53d1\u73b0\u5386\u53f2\u4f9d\u8d56\u6027\u6750\u6599\u6a21\u578b\uff0c\u5e76\u652f\u6301\u7269\u7406\u6a21\u578b\u3001\u795e\u7ecf\u7f51\u7edc\u53ca\u6df7\u5408\u6a21\u578b\u7684\u66f4\u65b0\uff0c\u65e0\u9700\u8c03\u53c2\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u548c\u73b0\u4ee3\u6750\u6599\u6a21\u578b\u5728\u4ece\u5b9e\u9a8c\u6570\u636e\u4e2d\u66f4\u65b0\u65f6\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e00\u4e2a\u65e0\u9700\u8c03\u53c2\u3001\u652f\u6301\u591a\u6837\u5316\u6a21\u578b\u7684\u901a\u7528\u6846\u67b6\u3002", "method": "\u5229\u7528\u81ea\u52a8\u5fae\u5206\u548c\u9ad8\u6548\u6279\u5904\u7406\u8ba1\u7b97\u56fe\uff0c\u901a\u8fc7\u5168\u5fae\u5206\u4ee3\u7801\u5b9e\u73b0\u5386\u53f2\u4f9d\u8d56\u6027\u6a21\u578b\u7684\u5411\u91cf\u5316\u66f4\u65b0\u3002", "result": "ADiMU \u6210\u529f\u66f4\u65b0\u4e86\u53c2\u6570\u89c4\u6a21\u4ece\u6570\u5341\u5230\u6570\u767e\u4e07\u7684\u4e0d\u540c\u6a21\u578b\uff0c\u5728\u5c40\u90e8\u548c\u5168\u5c40\u53d1\u73b0\u8bbe\u7f6e\u4e0b\u5747\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "ADiMU \u662f\u4e00\u79cd\u5f3a\u5927\u4e14\u7075\u6d3b\u7684\u5de5\u5177\uff0c\u4e3a\u6750\u6599\u6a21\u578b\u7684\u96c6\u6210\u3001\u8bc4\u4f30\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u5e76\u4ee5\u5f00\u6e90\u5de5\u5177 HookeAI \u7684\u5f62\u5f0f\u53d1\u5e03\u3002"}}
{"id": "2505.07815", "pdf": "https://arxiv.org/pdf/2505.07815", "abs": "https://arxiv.org/abs/2505.07815", "authors": ["Seungjae Lee", "Daniel Ekpo", "Haowen Liu", "Furong Huang", "Abhinav Shrivastava", "Jia-Bin Huang"], "title": "Imagine, Verify, Execute: Memory-Guided Agentic Exploration with Vision-Language Models", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Project webpage: https://ive-robot.github.io/", "summary": "Exploration is essential for general-purpose robotic learning, especially in\nopen-ended environments where dense rewards, explicit goals, or task-specific\nsupervision are scarce. Vision-language models (VLMs), with their semantic\nreasoning over objects, spatial relations, and potential outcomes, present a\ncompelling foundation for generating high-level exploratory behaviors. However,\ntheir outputs are often ungrounded, making it difficult to determine whether\nimagined transitions are physically feasible or informative. To bridge the gap\nbetween imagination and execution, we present IVE (Imagine, Verify, Execute),\nan agentic exploration framework inspired by human curiosity. Human exploration\nis often driven by the desire to discover novel scene configurations and to\ndeepen understanding of the environment. Similarly, IVE leverages VLMs to\nabstract RGB-D observations into semantic scene graphs, imagine novel scenes,\npredict their physical plausibility, and generate executable skill sequences\nthrough action tools. We evaluate IVE in both simulated and real-world tabletop\nenvironments. The results show that IVE enables more diverse and meaningful\nexploration than RL baselines, as evidenced by a 4.1 to 7.8x increase in the\nentropy of visited states. Moreover, the collected experience supports\ndownstream learning, producing policies that closely match or exceed the\nperformance of those trained on human-collected demonstrations.", "AI": {"tldr": "IVE\uff08Imagine, Verify, Execute\uff09\u662f\u4e00\u4e2a\u53d7\u4eba\u7c7b\u597d\u5947\u5fc3\u542f\u53d1\u7684\u63a2\u7d22\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u751f\u6210\u8bed\u4e49\u573a\u666f\u56fe\uff0c\u5e76\u9884\u6d4b\u7269\u7406\u53ef\u884c\u6027\u4ee5\u9a71\u52a8\u673a\u5668\u4eba\u591a\u6837\u5316\u63a2\u7d22\uff0c\u6548\u679c\u4f18\u4e8e\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u3002", "motivation": "\u5728\u5f00\u653e\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8981\u591a\u6837\u5316\u7684\u63a2\u7d22\u80fd\u529b\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5bc6\u96c6\u5956\u52b1\u6216\u4efb\u52a1\u76d1\u7763\u96be\u4ee5\u5b9e\u73b0\u3002\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5177\u5907\u8bed\u4e49\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u8f93\u51fa\u5e38\u7f3a\u4e4f\u7269\u7406\u53ef\u884c\u6027\u9a8c\u8bc1\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5c06\u60f3\u8c61\u4e0e\u5b9e\u9645\u6267\u884c\u7ed3\u5408\u3002", "method": "IVE\u6846\u67b6\u5206\u4e09\u6b65\uff1a1\uff09\u5c06RGB-D\u89c2\u6d4b\u62bd\u8c61\u4e3a\u8bed\u4e49\u573a\u666f\u56fe\uff1b2\uff09\u901a\u8fc7VLMs\u60f3\u8c61\u65b0\u573a\u666f\u5e76\u9a8c\u8bc1\u5176\u7269\u7406\u53ef\u884c\u6027\uff1b3\uff09\u751f\u6210\u53ef\u6267\u884c\u52a8\u4f5c\u5e8f\u5217\u3002\u6846\u67b6\u5728\u6a21\u62df\u548c\u771f\u5b9e\u684c\u9762\u73af\u5883\u4e2d\u9a8c\u8bc1\u3002", "result": "IVE\u5728\u63a2\u7d22\u591a\u6837\u6027\u548c\u610f\u4e49\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\uff08\u72b6\u6001\u71b5\u63d0\u53474.1-7.8\u500d\uff09\uff0c\u4e14\u6536\u96c6\u7684\u7ecf\u9a8c\u652f\u6301\u4e0b\u6e38\u5b66\u4e60\uff0c\u6027\u80fd\u63a5\u8fd1\u6216\u8d85\u8d8a\u4eba\u7c7b\u6f14\u793a\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "IVE\u901a\u8fc7\u7ed3\u5408VLM\u7684\u8bed\u4e49\u60f3\u8c61\u548c\u7269\u7406\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u591a\u6837\u5316\u7684\u673a\u5668\u4eba\u63a2\u7d22\uff0c\u4e3a\u5f00\u653e\u73af\u5883\u4e2d\u7684\u901a\u7528\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
