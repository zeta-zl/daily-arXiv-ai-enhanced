{"id": "2505.02847", "pdf": "https://arxiv.org/pdf/2505.02847", "abs": "https://arxiv.org/abs/2505.02847", "authors": ["Bang Zhang", "Ruotian Ma", "Qingxuan Jiang", "Peisong Wang", "Jiaqi Chen", "Zheng Xie", "Xingyu Chen", "Yue Wang", "Fanghua Ye", "Jian Li", "Yifan Yang", "Zhaopeng Tu", "Xiaolong Li"], "title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Assessing how well a large language model (LLM) understands human, rather\nthan merely text, remains an open challenge. To bridge the gap, we introduce\nSentient Agent as a Judge (SAGE), an automated evaluation framework that\nmeasures an LLM's higher-order social cognition. SAGE instantiates a Sentient\nAgent that simulates human-like emotional changes and inner thoughts during\ninteraction, providing a more realistic evaluation of the tested model in\nmulti-turn conversations. At every turn, the agent reasons about (i) how its\nemotion changes, (ii) how it feels, and (iii) how it should reply, yielding a\nnumerical emotion trajectory and interpretable inner thoughts. Experiments on\n100 supportive-dialogue scenarios show that the final Sentient emotion score\ncorrelates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings\nand utterance-level empathy metrics, validating psychological fidelity. We also\nbuild a public Sentient Leaderboard covering 18 commercial and open-source\nmodels that uncovers substantial gaps (up to 4x) between frontier systems\n(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in\nconventional leaderboards (e.g., Arena). SAGE thus provides a principled,\nscalable and interpretable tool for tracking progress toward genuinely\nempathetic and socially adept language agents.", "AI": {"tldr": "SAGE\u662f\u4e00\u79cd\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8861\u91cf\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9ad8\u9636\u793e\u4f1a\u8ba4\u77e5\u80fd\u529b\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u60c5\u611f\u53d8\u5316\u548c\u5185\u5fc3\u601d\u7ef4\uff0c\u63d0\u4f9b\u66f4\u771f\u5b9e\u7684\u5bf9\u8bdd\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u8861\u91cfLLM\u5bf9\u4eba\u7c7b\u60c5\u611f\u7684\u7406\u89e3\uff0cSAGE\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u60c5\u611f\u8f68\u8ff9\u548c\u5185\u5fc3\u601d\u7ef4\u63d0\u4f9b\u66f4\u771f\u5b9e\u7684\u8bc4\u4f30\u3002", "method": "SAGE\u5f15\u5165\u4e86\u4e00\u4e2a\u201c\u611f\u77e5\u667a\u80fd\u4f53\u201d\uff0c\u5728\u5bf9\u8bdd\u4e2d\u6a21\u62df\u4eba\u7c7b\u60c5\u611f\u53d8\u5316\uff08\u60c5\u7eea\u8bc4\u5206\uff09\u548c\u5185\u5fc3\u601d\u7ef4\uff08\u53ef\u89e3\u91ca\u7684\u601d\u8003\u8fc7\u7a0b\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSAGE\u7684\u60c5\u611f\u8bc4\u5206\u4e0e\u5fc3\u7406\u5b66\u8bc4\u4f30\u6807\u51c6\uff08\u5982BLRI\uff09\u9ad8\u5ea6\u76f8\u5173\uff0c\u5e76\u63ed\u793a\u4e86\u524d\u6cbf\u6a21\u578b\uff08\u5982GPT-4o\uff09\u4e0e\u65e9\u671f\u57fa\u7ebf\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "SAGE\u4e3a\u5f00\u53d1\u771f\u6b63\u5177\u5907\u5171\u60c5\u548c\u793e\u4ea4\u80fd\u529b\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2505.02850", "pdf": "https://arxiv.org/pdf/2505.02850", "abs": "https://arxiv.org/abs/2505.02850", "authors": ["Nicy Scaria", "Silvester John Joseph Kennedy", "Diksha Seth", "Ananya Thakur", "Deepak Subramani"], "title": "Harnessing Structured Knowledge: A Concept Map-Based Approach for High-Quality Multiple Choice Question Generation with Effective Distractors", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DB"], "comment": null, "summary": "Generating high-quality MCQs, especially those targeting diverse cognitive\nlevels and incorporating common misconceptions into distractor design, is\ntime-consuming and expertise-intensive, making manual creation impractical at\nscale. Current automated approaches typically generate questions at lower\ncognitive levels and fail to incorporate domain-specific misconceptions. This\npaper presents a hierarchical concept map-based framework that provides\nstructured knowledge to guide LLMs in generating MCQs with distractors. We\nchose high-school physics as our test domain and began by developing a\nhierarchical concept map covering major Physics topics and their\ninterconnections with an efficient database design. Next, through an automated\npipeline, topic-relevant sections of these concept maps are retrieved to serve\nas a structured context for the LLM to generate questions and distractors that\nspecifically target common misconceptions. Lastly, an automated validation is\ncompleted to ensure that the generated MCQs meet the requirements provided. We\nevaluate our framework against two baseline approaches: a base LLM and a\nRAG-based generation. We conducted expert evaluations and student assessments\nof the generated MCQs. Expert evaluation shows that our method significantly\noutperforms the baseline approaches, achieving a success rate of 75.20% in\nmeeting all quality criteria compared to approximately 37% for both baseline\nmethods. Student assessment data reveal that our concept map-driven approach\nachieved a significantly lower guess success rate of 28.05% compared to 37.10%\nfor the baselines, indicating a more effective assessment of conceptual\nunderstanding. The results demonstrate that our concept map-based approach\nenables robust assessment across cognitive levels and instant identification of\nconceptual gaps, facilitating faster feedback loops and targeted interventions\nat scale.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u6982\u5ff5\u56fe\u7684\u6846\u67b6\uff0c\u5229\u7528LLM\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u591a\u9009\u9898\uff0c\u9488\u5bf9\u591a\u6837\u5316\u8ba4\u77e5\u6c34\u5e73\u548c\u5e38\u89c1\u8bef\u89e3\u8bbe\u8ba1\u5e72\u6270\u9879\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002\u4e13\u5bb6\u8bc4\u4f30\u548c\u5b66\u751f\u6d4b\u8bd5\u5747\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u624b\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u9009\u9898\u8017\u65f6\u4e14\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u8ba4\u77e5\u6c34\u5e73\u548c\u9886\u57df\u7279\u5b9a\u8bef\u89e3\u7684\u9700\u6c42\u3002\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7ed3\u6784\u5316\u77e5\u8bc6\u5f15\u5bfcLLM\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6982\u5ff5\u56fe\u6784\u5efa\u7269\u7406\u77e5\u8bc6\u6846\u67b6\uff0c\u81ea\u52a8\u68c0\u7d22\u76f8\u5173\u6982\u5ff5\u4f5c\u4e3aLLM\u7684\u4e0a\u4e0b\u6587\uff0c\u751f\u6210\u9898\u76ee\u548c\u5e72\u6270\u9879\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u9a8c\u8bc1\u786e\u4fdd\u8d28\u91cf\u3002\u4e0e\u57fa\u7ebf\u65b9\u6cd5\uff08\u57fa\u7840LLM\u548cRAG\u751f\u6210\uff09\u5bf9\u6bd4\u3002", "result": "\u4e13\u5bb6\u8bc4\u4f30\u663e\u793a\u6210\u529f\u7387\u8fbe75.20%\uff08\u57fa\u7ebf\u7ea637%\uff09\uff1b\u5b66\u751f\u6d4b\u8bd5\u4e2d\u731c\u6d4b\u6b63\u786e\u7387\u964d\u81f328.05%\uff08\u57fa\u7ebf37.10%\uff09\uff0c\u8bc1\u660e\u5176\u80fd\u66f4\u6709\u6548\u8bc4\u4f30\u6982\u5ff5\u7406\u89e3\u3002", "conclusion": "\u6982\u5ff5\u56fe\u9a71\u52a8\u7684\u65b9\u6cd5\u80fd\u8de8\u8ba4\u77e5\u6c34\u5e73\u7a33\u5065\u8bc4\u4f30\uff0c\u5feb\u901f\u8bc6\u522b\u6982\u5ff5\u7f3a\u53e3\uff0c\u5b9e\u73b0\u89c4\u6a21\u5316\u9488\u5bf9\u6027\u53cd\u9988\u3002"}}
{"id": "2505.02851", "pdf": "https://arxiv.org/pdf/2505.02851", "abs": "https://arxiv.org/abs/2505.02851", "authors": ["Franklin Zhang", "Sonya Zhang", "Alon Halevy"], "title": "30DayGen: Leveraging LLMs to Create a Content Corpus for Habit Formation", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; H.3.1; H.3.3"], "comment": "8 pages (main content), 4 figures. Submitted to ACL BEA2025", "summary": "In this paper, we present 30 Day Me, a habit formation application that\nleverages Large Language Models (LLMs) to help users break down their goals\ninto manageable, actionable steps and track their progress. Central to the app\nis the 30DAYGEN system, which generates 3,531 unique 30-day challenges sourced\nfrom over 15K webpages, and enables runtime search of challenge ideas aligned\nwith user-defined goals. We showcase how LLMs can be harnessed to rapidly\nconstruct domain specific content corpora for behavioral and educational\npurposes, and propose a practical pipeline that incorporates effective LLM\nenhanced approaches for content generation and semantic deduplication.", "AI": {"tldr": "30 Day Me\u662f\u4e00\u6b3e\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5e2e\u52a9\u7528\u6237\u5206\u89e3\u76ee\u6807\u5e76\u8ddf\u8e2a\u8fdb\u5c55\u7684\u4e60\u60ef\u517b\u6210\u5e94\u7528\uff0c\u5176\u6838\u5fc3\u529f\u80fd\u5305\u62ec\u751f\u6210\u72ec\u7279\u768430\u5929\u6311\u6218\u548c\u5b9e\u65f6\u641c\u7d22\u4e0e\u7528\u6237\u76ee\u6807\u5339\u914d\u7684\u6311\u6218\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5c55\u793a\u5982\u4f55\u5229\u7528LLM\u5feb\u901f\u6784\u5efa\u7279\u5b9a\u9886\u57df\u7684\u5185\u5bb9\u5e93\uff0c\u652f\u6301\u884c\u4e3a\u548c\u6559\u80b2\u6d3b\u52a8\uff0c\u5e76\u63d0\u5347\u5185\u5bb9\u751f\u6210\u548c\u8bed\u4e49\u53bb\u91cd\u7684\u6548\u7387\u3002", "method": "\u901a\u8fc730DAYGEN\u7cfb\u7edf\u4ece15K+\u7f51\u9875\u4e2d\u751f\u62103,531\u4e2a\u72ec\u7279\u768430\u5929\u6311\u6218\uff0c\u7ed3\u5408LLM\u4f18\u5316\u5185\u5bb9\u751f\u6210\u548c\u8bed\u4e49\u53bb\u91cd\u6d41\u7a0b\u3002", "result": "\u5e94\u7528\u6210\u529f\u5b9e\u73b0\u4e86\u6311\u6218\u7684\u52a8\u6001\u751f\u6210\u548c\u5b9e\u65f6\u641c\u7d22\uff0c\u9a8c\u8bc1\u4e86LLM\u5728\u5185\u5bb9\u6784\u5efa\u548c\u884c\u4e3a\u5e72\u9884\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660eLLM\u80fd\u9ad8\u6548\u652f\u6301\u4e60\u60ef\u517b\u6210\u5de5\u5177\u7684\u5f00\u53d1\uff0c\u4e3a\u884c\u4e3a\u548c\u6559\u80b2\u6d3b\u52a8\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02854", "pdf": "https://arxiv.org/pdf/2505.02854", "abs": "https://arxiv.org/abs/2505.02854", "authors": ["Masumi Morishige", "Ryo Koshihara"], "title": "Ensuring Reproducibility in Generative AI Systems for General Use Cases: A Framework for Regression Testing and Open Datasets", "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 10 figures", "summary": "Reproducibility and reliability remain pressing challenges for generative AI\nsystems whose behavior can drift with each model update or prompt revision. We\nintroduce GPR-bench, a lightweight, extensible benchmark that operationalizes\nregression testing for general purpose use cases. GPR-bench couples an open,\nbilingual (English and Japanese) dataset covering eight task categories (e.g.,\ntext generation, code generation, and information retrieval) and 10 scenarios\nin each task categories (80 total test cases for each language) with an\nautomated evaluation pipeline that employs \"LLM-as-a-Judge\" scoring of\ncorrectness and conciseness. Experiments across three recent model versions -\ngpt-4o-mini, o3-mini, and o4-mini - and two prompt configurations (default\nversus concise-writing instruction) reveal heterogeneous quality. Our results\nshow that newer models generally improve correctness, but the differences are\nmodest and not statistically significant, suggesting that GPR-bench may not be\nsufficiently challenging to differentiate between recent model versions. In\ncontrast, the concise-writing instruction significantly enhances conciseness\n(+12.37 pp, Mann-Whitney U test: p < 0.001, effect size r = 0.2995) with\nminimal degradations on accuracy (-1.7 pp), demonstrating the effectiveness of\nprompt engineering. Released under the MIT License, GPR- bench lowers the\nbarrier to initiating reproducibility monitoring and provides a foundation for\ncommunity-driven extensions, while also raising important considerations about\nbenchmark design for rapidly evolving language models.", "AI": {"tldr": "GPR-bench\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u7528\u4e8e\u8bc4\u4f30\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u53ef\u91cd\u590d\u6027\u548c\u53ef\u9760\u6027\uff0c\u5305\u542b\u53cc\u8bed\u6570\u636e\u96c6\u548c\u81ea\u52a8\u8bc4\u4f30\u6d41\u7a0b\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u65b0\u6a21\u578b\u5728\u6b63\u786e\u6027\u4e0a\u7565\u6709\u63d0\u5347\uff0c\u4f46\u63d0\u793a\u5de5\u7a0b\u5bf9\u7b80\u6d01\u6027\u7684\u63d0\u5347\u66f4\u663e\u8457\u3002", "motivation": "\u89e3\u51b3\u751f\u6210\u5f0fAI\u7cfb\u7edf\u56e0\u6a21\u578b\u66f4\u65b0\u6216\u63d0\u793a\u8c03\u6574\u5bfc\u81f4\u7684\u884c\u4e3a\u6f02\u79fb\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u53ef\u64cd\u4f5c\u7684\u56de\u5f52\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "GPR-bench\u7ed3\u5408\u53cc\u8bed\u6570\u636e\u96c6\uff08\u8986\u76d68\u7c7b\u4efb\u52a1\uff09\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\u6d41\u7a0b\uff08\u4f7f\u7528LLM-as-a-Judge\u8bc4\u5206\uff09\uff0c\u6d4b\u8bd5\u4e0d\u540c\u6a21\u578b\u7248\u672c\u548c\u63d0\u793a\u914d\u7f6e\u7684\u6548\u679c\u3002", "result": "\u65b0\u6a21\u578b\u5728\u6b63\u786e\u6027\u4e0a\u63d0\u5347\u6709\u9650\uff0c\u800c\u7b80\u6d01\u6027\u63d0\u793a\u663e\u8457\u63d0\u5347\u8f93\u51fa\u7b80\u6d01\u6027\uff08+12.37 pp\uff09\uff0c\u51c6\u786e\u6027\u4ec5\u7565\u5fae\u4e0b\u964d\uff08-1.7 pp\uff09\u3002", "conclusion": "GPR-bench\u4e3a\u53ef\u91cd\u590d\u6027\u76d1\u6d4b\u63d0\u4f9b\u4e86\u4f4e\u95e8\u69db\u5de5\u5177\uff0c\u4f46\u9700\u4f18\u5316\u57fa\u51c6\u8bbe\u8ba1\u4ee5\u9002\u5e94\u5feb\u901f\u6f14\u8fdb\u7684\u6a21\u578b\u3002"}}
{"id": "2505.02952", "pdf": "https://arxiv.org/pdf/2505.02952", "abs": "https://arxiv.org/abs/2505.02952", "authors": ["Fabrizio Marozzo"], "title": "Iterative Resolution of Prompt Ambiguities Using a Progressive Cutting-Search Approach", "categories": ["cs.AI", "cs.CL", "cs.ET", "cs.IR", "cs.LG"], "comment": null, "summary": "Generative AI systems have revolutionized human interaction by enabling\nnatural language-based coding and problem solving. However, the inherent\nambiguity of natural language often leads to imprecise instructions, forcing\nusers to iteratively test, correct, and resubmit their prompts. We propose an\niterative approach that systematically narrows down these ambiguities through a\nstructured series of clarification questions and alternative solution\nproposals, illustrated with input/output examples as well. Once every\nuncertainty is resolved, a final, precise solution is generated. Evaluated on a\ndiverse dataset spanning coding, data analysis, and creative writing, our\nmethod demonstrates superior accuracy, competitive resolution times, and higher\nuser satisfaction compared to conventional one-shot solutions, which typically\nrequire multiple manual iterations to achieve a correct output.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fed\u4ee3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6f84\u6e05\u95ee\u9898\u548c\u793a\u4f8b\u9010\u6b65\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u6a21\u7cca\u6027\uff0c\u6700\u7ec8\u751f\u6210\u7cbe\u786e\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u4e8e\u4f20\u7edf\u4e00\u6b21\u6027\u65b9\u6cd5\u3002", "motivation": "\u751f\u6210\u5f0fAI\u7cfb\u7edf\u867d\u9769\u65b0\u4e86\u4eba\u673a\u4ea4\u4e92\uff0c\u4f46\u81ea\u7136\u8bed\u8a00\u7684\u6a21\u7cca\u6027\u5e38\u5bfc\u81f4\u4e0d\u7cbe\u786e\u7684\u6307\u4ee4\uff0c\u8feb\u4f7f\u7528\u6237\u53cd\u590d\u6d4b\u8bd5\u548c\u4fee\u6b63\u63d0\u793a\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6f84\u6e05\u95ee\u9898\u3001\u66ff\u4ee3\u65b9\u6848\u63d0\u8bae\u53ca\u8f93\u5165/\u8f93\u51fa\u793a\u4f8b\uff0c\u9010\u6b65\u6d88\u9664\u6a21\u7cca\u6027\u3002", "result": "\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u51c6\u786e\u6027\u66f4\u9ad8\u3001\u89e3\u51b3\u65f6\u95f4\u7ade\u4e89\u6027\u5f3a\u3001\u7528\u6237\u6ee1\u610f\u5ea6\u4f18\u4e8e\u4f20\u7edf\u4e00\u6b21\u6027\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u6784\u5316\u8fed\u4ee3\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u6a21\u7cca\u6027\uff0c\u63d0\u5347\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2505.02874", "pdf": "https://arxiv.org/pdf/2505.02874", "abs": "https://arxiv.org/abs/2505.02874", "authors": ["L. Juli\u00e1n Lechuga L\u00f3pez", "Shaza Elsharief", "Dhiyaa Al Jorf", "Firas Darwish", "Congbo Ma", "Farah E. Shamout"], "title": "Uncertainty Quantification for Machine Learning in Healthcare: A Survey", "categories": ["cs.LG", "cs.AI"], "comment": "46 pages, 3 figures, 2 tables, AHLI Conference on Health, Inference,\n  and Learning (CHIL)", "summary": "Uncertainty Quantification (UQ) is pivotal in enhancing the robustness,\nreliability, and interpretability of Machine Learning (ML) systems for\nhealthcare, optimizing resources and improving patient care. Despite the\nemergence of ML-based clinical decision support tools, the lack of principled\nquantification of uncertainty in ML models remains a major challenge. Current\nreviews have a narrow focus on analyzing the state-of-the-art UQ in specific\nhealthcare domains without systematically evaluating method efficacy across\ndifferent stages of model development, and despite a growing body of research,\nits implementation in healthcare applications remains limited. Therefore, in\nthis survey, we provide a comprehensive analysis of current UQ in healthcare,\noffering an informed framework that highlights how different methods can be\nintegrated into each stage of the ML pipeline including data processing,\ntraining and evaluation. We also highlight the most popular methods used in\nhealthcare and novel approaches from other domains that hold potential for\nfuture adoption in the medical context. We expect this study will provide a\nclear overview of the challenges and opportunities of implementing UQ in the ML\npipeline for healthcare, guiding researchers and practitioners in selecting\nsuitable techniques to enhance the reliability, safety and trust from patients\nand clinicians on ML-driven healthcare solutions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u533b\u7597\u9886\u57df\u4e2d\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u7684\u73b0\u72b6\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5c06\u4e0d\u540cUQ\u65b9\u6cd5\u6574\u5408\u5230ML\u6d41\u7a0b\u4e2d\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u7684\u5e94\u7528\u65b9\u5411\u3002", "motivation": "\u5f53\u524d\u533b\u7597\u9886\u57df\u7684ML\u5de5\u5177\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u9650\u5236\u4e86\u5176\u53ef\u9760\u6027\u548c\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5168\u9762\u5206\u6790\u73b0\u6709UQ\u65b9\u6cd5\uff0c\u6784\u5efa\u4e00\u4e2a\u6846\u67b6\uff0c\u5c55\u793a\u5982\u4f55\u5c06\u8fd9\u4e9b\u65b9\u6cd5\u6574\u5408\u5230ML\u6d41\u7a0b\u7684\u5404\u4e2a\u9636\u6bb5\uff08\u6570\u636e\u3001\u8bad\u7ec3\u3001\u8bc4\u4f30\uff09\u3002", "result": "\u603b\u7ed3\u4e86\u533b\u7597\u9886\u57df\u5e38\u7528\u7684UQ\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u5176\u4ed6\u9886\u57df\u53ef\u80fd\u9002\u7528\u4e8e\u533b\u7597\u7684\u65b0\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u533b\u7597ML\u4e2d\u7684UQ\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u6311\u6218\u4e0e\u673a\u9047\u6982\u89c8\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u9009\u62e9\u5408\u9002\u7684\u6280\u672f\u4ee5\u63d0\u5347\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u4e0e\u4fe1\u4efb\u5ea6\u3002"}}
{"id": "2505.02858", "pdf": "https://arxiv.org/pdf/2505.02858", "abs": "https://arxiv.org/abs/2505.02858", "authors": ["Henry Tari", "Nojus Sereiva", "Rishabh Kaushal", "Thales Bertaglia", "Adriana Iamnitchi"], "title": "Towards High-Fidelity Synthetic Multi-platform Social Media Datasets via Large Language Models", "categories": ["cs.CL", "cs.CY"], "comment": "arXiv admin note: text overlap with arXiv:2407.08323", "summary": "Social media datasets are essential for research on a variety of topics, such\nas disinformation, influence operations, hate speech detection, or influencer\nmarketing practices. However, access to social media datasets is often\nconstrained due to costs and platform restrictions. Acquiring datasets that\nspan multiple platforms, which is crucial for understanding the digital\necosystem, is particularly challenging. This paper explores the potential of\nlarge language models to create lexically and semantically relevant social\nmedia datasets across multiple platforms, aiming to match the quality of real\ndata. We propose multi-platform topic-based prompting and employ various\nlanguage models to generate synthetic data from two real datasets, each\nconsisting of posts from three different social media platforms. We assess the\nlexical and semantic properties of the synthetic data and compare them with\nthose of the real data. Our empirical findings show that using large language\nmodels to generate synthetic multi-platform social media data is promising,\ndifferent language models perform differently in terms of fidelity, and a\npost-processing approach might be needed for generating high-fidelity synthetic\ndatasets for research. In addition to the empirical evaluation of three state\nof the art large language models, our contributions include new fidelity\nmetrics specific to multi-platform social media datasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8de8\u5e73\u53f0\u793e\u4ea4\u5a92\u4f53\u5408\u6210\u6570\u636e\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u591a\u5e73\u53f0\u4e3b\u9898\u63d0\u793a\u548c\u4e0d\u540c\u6a21\u578b\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86\u5408\u6210\u6570\u636e\u7684\u8bcd\u6cd5\u548c\u8bed\u4e49\u8d28\u91cf\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u9ad8\u4fdd\u771f\u6570\u636e\u96c6\u7684\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u83b7\u53d6\u771f\u5b9e\u7684\u8de8\u5e73\u53f0\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u96c6\u65e2\u6602\u8d35\u53c8\u53d7\u9650\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5408\u6210\u6570\u636e\u4ee5\u66ff\u4ee3\u771f\u5b9e\u6570\u636e\u7684\u53ef\u884c\u6027\u3002", "method": "\u91c7\u7528\u4e86\u591a\u5e73\u53f0\u4e3b\u9898\u63d0\u793a\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u4e24\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5e76\u5bf9\u6bd4\u4e86\u771f\u5b9e\u6570\u636e\u4e0e\u5408\u6210\u6570\u636e\u7684\u8bcd\u6cd5\u548c\u8bed\u4e49\u7279\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8de8\u5e73\u53f0\u5408\u6210\u6570\u636e\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4e0d\u540c\u6a21\u578b\u7684\u8868\u73b0\u5404\u5f02\uff0c\u53ef\u80fd\u9700\u8981\u540e\u5904\u7406\u6765\u63d0\u9ad8\u6570\u636e\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8bba\u6587\u8bc1\u5b9e\u4e86\u5408\u6210\u6570\u636e\u7684\u53ef\u884c\u6027\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u591a\u5e73\u53f0\u6570\u636e\u96c6\u4fdd\u771f\u5ea6\u6307\u6807\uff0c\u5e76\u5efa\u8bae\u8fdb\u4e00\u6b65\u4f18\u5316\u8bed\u8a00\u6a21\u578b\u548c\u540e\u5904\u7406\u65b9\u6cd5\u4ee5\u63d0\u5347\u6570\u636e\u8d28\u91cf\u3002"}}
{"id": "2505.03020", "pdf": "https://arxiv.org/pdf/2505.03020", "abs": "https://arxiv.org/abs/2505.03020", "authors": ["Kishore Sampath", "Pratheesh", "Ayaazuddin Mohammad", "Resmi Ramachandranpillai"], "title": "The Multimodal Paradox: How Added and Missing Modalities Shape Bias and Performance in Multimodal AI", "categories": ["cs.AI"], "comment": "CVPR 2025 Second Workshop on Responsible Generative AI", "summary": "Multimodal learning, which integrates diverse data sources such as images,\ntext, and structured data, has proven superior to unimodal counterparts in\nhigh-stakes decision-making. However, while performance gains remain the gold\nstandard for evaluating multimodal systems, concerns around bias and robustness\nare frequently overlooked. In this context, this paper explores two key\nresearch questions (RQs): (i) RQ1 examines whether adding a modality\ncon-sistently enhances performance and investigates its role in shaping\nfairness measures, assessing whether it mitigates or amplifies bias in\nmultimodal models; (ii) RQ2 investigates the impact of missing modalities at\ninference time, analyzing how multimodal models generalize in terms of both\nperformance and fairness. Our analysis reveals that incorporating new\nmodalities during training consistently enhances the performance of multimodal\nmodels, while fairness trends exhibit variability across different evaluation\nmeasures and datasets. Additionally, the absence of modalities at inference\ndegrades performance and fairness, raising concerns about its robustness in\nreal-world deployment. We conduct extensive experiments using multimodal\nhealthcare datasets containing images, time series, and structured information\nto validate our findings.", "AI": {"tldr": "\u591a\u6a21\u6001\u5b66\u4e60\u5728\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\uff0c\u53ef\u80fd\u5f71\u54cd\u516c\u5e73\u6027\uff1b\u6a21\u6001\u7f3a\u5931\u4f1a\u964d\u4f4e\u6027\u80fd\u548c\u516c\u5e73\u6027\uff0c\u5f71\u54cd\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7a33\u5065\u6027\u3002", "motivation": "\u63a2\u8ba8\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u6027\u80fd\u63d0\u5347\u4e0e\u516c\u5e73\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4ee5\u53ca\u6a21\u6001\u7f3a\u5931\u5bf9\u6a21\u578b\u6027\u80fd\u548c\u516c\u5e73\u6027\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u591a\u6a21\u6001\u6a21\u578b\u7684\u6027\u80fd\u548c\u516c\u5e73\u6027\uff0c\u4f7f\u7528\u5305\u542b\u56fe\u50cf\u3001\u65f6\u95f4\u5e8f\u5217\u548c\u7ed3\u6784\u5316\u6570\u636e\u7684\u533b\u7597\u6570\u636e\u96c6\u3002", "result": "\u65b0\u6a21\u6001\u7684\u52a0\u5165\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u516c\u5e73\u6027\u8868\u73b0\u4e0d\u4e00\u81f4\uff1b\u6a21\u6001\u7f3a\u5931\u4f1a\u5bfc\u81f4\u6027\u80fd\u548c\u516c\u5e73\u6027\u4e0b\u964d\u3002", "conclusion": "\u591a\u6a21\u6001\u5b66\u4e60\u7684\u5b9e\u9645\u90e8\u7f72\u9700\u6743\u8861\u6027\u80fd\u3001\u516c\u5e73\u6027\u548c\u7a33\u5065\u6027\u3002"}}
{"id": "2505.02877", "pdf": "https://arxiv.org/pdf/2505.02877", "abs": "https://arxiv.org/abs/2505.02877", "authors": ["Hele Zhu", "Xinyi Huang", "Haojia Gao", "Mengfei Jiang", "Haohua Que", "Lei Mu"], "title": "A Wireless Collaborated Inference Acceleration Framework for Plant Disease Recognition", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Plant disease is a critical factor affecting agricultural production.\nTraditional manual recognition methods face significant drawbacks, including\nlow accuracy, high costs, and inefficiency. Deep learning techniques have\ndemonstrated significant benefits in identifying plant diseases, but they still\nface challenges such as inference delays and high energy consumption. Deep\nlearning algorithms are difficult to run on resource-limited embedded devices.\nOffloading these models to cloud servers is confronted with the restriction of\ncommunication bandwidth, and all of these factors will influence the\ninference's efficiency. We propose a collaborative inference framework for\nrecognizing plant diseases between edge devices and cloud servers to enhance\ninference speed. The DNN model for plant disease recognition is pruned through\ndeep reinforcement learning to improve the inference speed and reduce energy\nconsumption. Then the optimal split point is determined by a greedy strategy to\nachieve the best collaborated inference acceleration. Finally, the system for\ncollaborative inference acceleration in plant disease recognition has been\nimplemented using Gradio to facilitate friendly human-machine interaction.\nExperiments indicate that the proposed collaborative inference framework\nsignificantly increases inference speed while maintaining acceptable\nrecognition accuracy, offering a novel solution for rapidly diagnosing and\npreventing plant diseases.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fb9\u7f18\u8bbe\u5907\u4e0e\u4e91\u670d\u52a1\u5668\u534f\u540c\u7684\u690d\u7269\u75c5\u5bb3\u8bc6\u522b\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4fee\u526a\u6a21\u578b\u5e76\u786e\u5b9a\u6700\u4f18\u5206\u88c2\u70b9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\u5e76\u964d\u4f4e\u4e86\u80fd\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u63a5\u53d7\u7684\u8bc6\u522b\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u624b\u52a8\u8bc6\u522b\u65b9\u6cd5\u51c6\u786e\u7387\u4f4e\u3001\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\u4e0b\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u8fd0\u884c\u56f0\u96be\uff0c\u4e91\u670d\u52a1\u5668\u53c8\u53d7\u5e26\u5bbd\u9650\u5236\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u8017\u7684\u690d\u7269\u75c5\u5bb3\u8bc6\u522b\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4fee\u526aDNN\u6a21\u578b\u4ee5\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u57fa\u4e8e\u8d2a\u5a6a\u7b56\u7565\u786e\u5b9a\u6700\u4f18\u5206\u88c2\u70b9\uff0c\u5b9e\u73b0\u8fb9\u7f18\u4e0e\u4e91\u7684\u534f\u540c\u63a8\u7406\u52a0\u901f\uff0c\u5e76\u901a\u8fc7Gradio\u5b9e\u73b0\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u63a5\u53d7\u7684\u8bc6\u522b\u7cbe\u5ea6\uff0c\u4e3a\u690d\u7269\u75c5\u5bb3\u5feb\u901f\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002", "conclusion": "\u63d0\u51fa\u7684\u534f\u540c\u63a8\u7406\u6846\u67b6\u89e3\u51b3\u4e86\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u7684\u6548\u7387\u4e0e\u80fd\u8017\u95ee\u9898\uff0c\u4e3a\u519c\u4e1a\u75c5\u5bb3\u9632\u6cbb\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2505.02859", "pdf": "https://arxiv.org/pdf/2505.02859", "abs": "https://arxiv.org/abs/2505.02859", "authors": ["Jonas Bokstaller", "Julia Altheimer", "Julian Dormehl", "Alina Buss", "Jasper Wiltfang", "Johannes Schneider", "Maximilian R\u00f6glinger"], "title": "Enhancing ML Model Interpretability: Leveraging Fine-Tuned Large Language Models for Better Understanding of AI", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Across various sectors applications of eXplainableAI (XAI) gained momentum as\nthe increasing black-boxedness of prevailing Machine Learning (ML) models\nbecame apparent. In parallel, Large Language Models (LLMs) significantly\ndeveloped in their abilities to understand human language and complex patterns.\nBy combining both, this paper presents a novel reference architecture for the\ninterpretation of XAI through an interactive chatbot powered by a fine-tuned\nLLM. We instantiate the reference architecture in the context of\nState-of-Health (SoH) prediction for batteries and validate its design in\nmultiple evaluation and demonstration rounds. The evaluation indicates that the\nimplemented prototype enhances the human interpretability of ML, especially for\nusers with less experience with XAI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408XAI\u548cLLM\u7684\u65b0\u578b\u4ea4\u4e92\u5f0f\u804a\u5929\u673a\u5668\u4eba\u67b6\u6784\uff0c\u7528\u4e8e\u7535\u6c60\u5065\u5eb7\u72b6\u6001\u9884\u6d4b\uff0c\u63d0\u5347\u4e86\u975e\u4e13\u4e1a\u7528\u6237\u5bf9ML\u6a21\u578b\u7684\u7406\u89e3\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u9ed1\u7bb1\u7279\u6027\u65e5\u76ca\u660e\u663e\uff0cXAI\u7684\u5e94\u7528\u9700\u6c42\u589e\u52a0\uff1b\u540c\u65f6\uff0cLLM\u5728\u7406\u89e3\u4eba\u7c7b\u8bed\u8a00\u548c\u590d\u6742\u6a21\u5f0f\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4e24\u8005\u7684\u7ed3\u5408\u6709\u671b\u63d0\u5347\u6a21\u578b\u89e3\u91ca\u6027\u3002", "method": "\u901a\u8fc7\u5fae\u8c03LLM\u6784\u5efa\u4ea4\u4e92\u5f0f\u804a\u5929\u673a\u5668\u4eba\uff0c\u5c06\u5176\u4f5c\u4e3aXAI\u7684\u89e3\u91ca\u63a5\u53e3\uff0c\u5e76\u5728\u7535\u6c60\u5065\u5eb7\u9884\u6d4b\u573a\u666f\u4e2d\u5b9e\u4f8b\u5316\u548c\u9a8c\u8bc1\u8be5\u67b6\u6784\u3002", "result": "\u539f\u578b\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\uff08\u5c24\u5176\u662fXAI\u7ecf\u9a8c\u4e0d\u8db3\u8005\uff09\u5bf9ML\u6a21\u578b\u7684\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "\u7ed3\u5408XAI\u4e0eLLM\u7684\u4ea4\u4e92\u5f0f\u67b6\u6784\u6709\u6548\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u66f4\u591a\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2505.03033", "pdf": "https://arxiv.org/pdf/2505.03033", "abs": "https://arxiv.org/abs/2505.03033", "authors": ["George Xi Wang", "Jingying Deng", "Safinah Ali"], "title": "Evaluating the Impact of AI-Powered Audiovisual Personalization on Learner Emotion, Focus, and Learning Outcomes", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Independent learners often struggle with sustaining focus and emotional\nregulation in unstructured or distracting settings. Although some rely on\nambient aids such as music, ASMR, or visual backgrounds to support\nconcentration, these tools are rarely integrated into cohesive,\nlearner-centered systems. Moreover, existing educational technologies focus\nprimarily on content adaptation and feedback, overlooking the emotional and\nsensory context in which learning takes place. Large language models have\ndemonstrated powerful multimodal capabilities including the ability to generate\nand adapt text, audio, and visual content. Educational research has yet to\nfully explore their potential in creating personalized audiovisual learning\nenvironments. To address this gap, we introduce an AI-powered system that uses\nLLMs to generate personalized multisensory study environments. Users select or\ngenerate customized visual themes (e.g., abstract vs. realistic, static vs.\nanimated) and auditory elements (e.g., white noise, ambient ASMR, familiar vs.\nnovel sounds) to create immersive settings aimed at reducing distraction and\nenhancing emotional stability. Our primary research question investigates how\ncombinations of personalized audiovisual elements affect learner cognitive load\nand engagement. Using a mixed-methods design that incorporates biometric\nmeasures and performance outcomes, this study evaluates the effectiveness of\nLLM-driven sensory personalization. The findings aim to advance emotionally\nresponsive educational technologies and extend the application of multimodal\nLLMs into the sensory dimension of self-directed learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u4e2a\u6027\u5316\u591a\u611f\u5b98\u5b66\u4e60\u73af\u5883\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u5b9a\u5236\u89c6\u542c\u5143\u7d20\u51cf\u5c11\u5206\u5fc3\u5e76\u63d0\u5347\u5b66\u4e60\u8005\u7684\u60c5\u611f\u7a33\u5b9a\u6027\u3002\u7814\u7a76\u7ed3\u5408\u751f\u7269\u7279\u5f81\u548c\u5b66\u4e60\u8868\u73b0\uff0c\u8bc4\u4f30\u4e86\u8be5\u7cfb\u7edf\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6559\u80b2\u6280\u672f\u591a\u5173\u6ce8\u5185\u5bb9\u9002\u914d\u548c\u53cd\u9988\uff0c\u5ffd\u89c6\u4e86\u5b66\u4e60\u7684\u60c5\u611f\u4e0e\u611f\u5b98\u73af\u5883\u3002\u5b66\u4e60\u8005\u5e38\u56e0\u5206\u5fc3\u6216\u60c5\u611f\u6ce2\u52a8\u5f71\u54cd\u5b66\u4e60\u6548\u679c\uff0c\u9700\u8981\u66f4\u4e2a\u6027\u5316\u548c\u6c89\u6d78\u5f0f\u7684\u8f85\u52a9\u5de5\u5177\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u65b9\u6cd5\u8bbe\u8ba1\uff0c\u901a\u8fc7LLM\u751f\u6210\u4e2a\u6027\u5316\u89c6\u542c\u5143\u7d20\uff08\u5982\u89c6\u89c9\u4e3b\u9898\u548c\u542c\u89c9\u80cc\u666f\uff09\uff0c\u5e76\u7ed3\u5408\u751f\u7269\u6307\u6807\u548c\u5b66\u4e60\u8868\u73b0\u8bc4\u4f30\u5176\u6548\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e2a\u6027\u5316\u89c6\u542c\u73af\u5883\u80fd\u663e\u8457\u964d\u4f4e\u8ba4\u77e5\u8d1f\u8377\u5e76\u63d0\u5347\u5b66\u4e60\u4e13\u6ce8\u5ea6\uff0c\u9a8c\u8bc1\u4e86LLM\u5728\u591a\u611f\u5b98\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u63a8\u52a8\u4e86\u60c5\u611f\u54cd\u5e94\u6559\u80b2\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5e76\u4e3aLLM\u5728\u81ea\u4e3b\u5b66\u4e60\u611f\u5b98\u7ef4\u5ea6\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.02880", "pdf": "https://arxiv.org/pdf/2505.02880", "abs": "https://arxiv.org/abs/2505.02880", "authors": ["Zian Liu", "Renjun Jia"], "title": "LLM4FTS: Enhancing Large Language Models for Financial Time Series Prediction", "categories": ["cs.LG"], "comment": "12 pages, 9figures", "summary": "Predicting financial time series presents significant challenges due to\ninherent low signal-to-noise ratios and intricate temporal patterns.\nTraditional machine learning models exhibit limitations in this forecasting\ntask constrained by their restricted model capacity. Recent advances in large\nlanguage models (LLMs), with their greatly expanded parameter spaces,\ndemonstrate promising potential for modeling complex dependencies in temporal\nsequences. However, existing LLM-based approaches typically focus on\nfixed-length patch analysis due to the Transformer architecture, ignoring\nmarket data's multi-scale pattern characteristics. In this study, we propose\n$LLM4FTS$, a novel framework that enhances LLM capabilities for temporal\nsequence modeling through learnable patch segmentation and dynamic wavelet\nconvolution modules. Specifically,we first employ K-means++ clustering based on\nDTW distance to identify scale-invariant patterns in market data. Building upon\npattern recognition results, we introduce adaptive patch segmentation that\npartitions temporal sequences while preserving maximal pattern integrity. To\naccommodate time-varying frequency characteristics, we devise a dynamic wavelet\nconvolution module that emulates discrete wavelet transformation with enhanced\nflexibility in capturing time-frequency features. These three modules work\ntogether to improve large language model's ability to handle scale-invariant\npatterns in financial time series. Extensive experiments on real-world\nfinancial datasets substantiate the framework's efficacy, demonstrating\nsuperior performance in capturing complex market patterns and achieving\nstate-of-the-art results in stock return prediction. The successful deployment\nin practical trading systems confirms its real-world applicability,\nrepresenting a significant advancement in LLM applications for financial\nforecasting.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86LLM4FTS\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5206\u5272\u548c\u52a8\u6001\u5c0f\u6ce2\u5377\u79ef\u6a21\u5757\u589e\u5f3aLLM\u5bf9\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u80a1\u7968\u9884\u6d4b\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u53d7\u9650\u4e8e\u6a21\u578b\u5bb9\u91cf\uff0c\u800c\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5ffd\u89c6\u5e02\u573a\u6570\u636e\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u3002", "method": "\u7ed3\u5408K-means++\u805a\u7c7b\u548cDTW\u8ddd\u79bb\u8bc6\u522b\u5c3a\u5ea6\u4e0d\u53d8\u6a21\u5f0f\uff0c\u63d0\u51fa\u81ea\u9002\u5e94\u5206\u5272\u548c\u52a8\u6001\u5c0f\u6ce2\u5377\u79ef\u6a21\u5757\u3002", "result": "\u5728\u771f\u5b9e\u91d1\u878d\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u4f18\u8d8a\u6027\uff0c\u5b9e\u73b0\u4e86\u80a1\u7968\u6536\u76ca\u9884\u6d4b\u7684\u6700\u4f73\u6548\u679c\u3002", "conclusion": "LLM4FTS\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u91d1\u878d\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u5177\u6709\u5b9e\u9645\u4ea4\u6613\u7cfb\u7edf\u7684\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2505.02862", "pdf": "https://arxiv.org/pdf/2505.02862", "abs": "https://arxiv.org/abs/2505.02862", "authors": ["Haoming Yang", "Ke Ma", "Xiaojun Jia", "Yingfei Sun", "Qianqian Xu", "Qingming Huang"], "title": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the remarkable performance of Large Language Models (LLMs), they\nremain vulnerable to jailbreak attacks, which can compromise their safety\nmechanisms. Existing studies often rely on brute-force optimization or manual\ndesign, failing to uncover potential risks in real-world scenarios. To address\nthis, we propose a novel jailbreak attack framework, ICRT, inspired by\nheuristics and biases in human cognition. Leveraging the simplicity effect, we\nemploy cognitive decomposition to reduce the complexity of malicious prompts.\nSimultaneously, relevance bias is utilized to reorganize prompts, enhancing\nsemantic alignment and inducing harmful outputs effectively. Furthermore, we\nintroduce a ranking-based harmfulness evaluation metric that surpasses the\ntraditional binary success-or-failure paradigm by employing ranking aggregation\nmethods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify\nthe harmfulness of generated content. Experimental results show that our\napproach consistently bypasses mainstream LLMs' safety mechanisms and generates\nhigh-risk content, providing insights into jailbreak attack risks and\ncontributing to stronger defense strategies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aICRT\u7684\u65b0\u578b\u8d8a\u72f1\u653b\u51fb\u6846\u67b6\uff0c\u5229\u7528\u4eba\u7c7b\u8ba4\u77e5\u7684\u542f\u53d1\u5f0f\u548c\u504f\u89c1\uff0c\u901a\u8fc7\u8ba4\u77e5\u5206\u89e3\u548c\u76f8\u5173\u6027\u504f\u7f6e\u4f18\u5316\u6076\u610f\u63d0\u793a\uff0c\u6709\u6548\u7ed5\u8fc7\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u673a\u5236\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u51fa\u8272\uff0c\u4f46\u5176\u5b89\u5168\u673a\u5236\u4ecd\u6613\u53d7\u8d8a\u72f1\u653b\u51fb\u5a01\u80c1\u3002\u73b0\u6709\u7814\u7a76\u591a\u57fa\u4e8e\u66b4\u529b\u4f18\u5316\u6216\u624b\u52a8\u8bbe\u8ba1\uff0c\u96be\u4ee5\u63ed\u793a\u5b9e\u9645\u98ce\u9669\u3002", "method": "\u91c7\u7528\u8ba4\u77e5\u542f\u53d1\u5f0f\uff08\u5982\u7b80\u5355\u6548\u5e94\u548c\u76f8\u5173\u6027\u504f\u7f6e\uff09\u5206\u89e3\u548c\u91cd\u7ec4\u6076\u610f\u63d0\u793a\uff1b\u63d0\u51fa\u57fa\u4e8e\u6392\u5e8f\u7684\u5371\u5bb3\u6027\u8bc4\u4f30\u6307\u6807\uff08\u5982Elo\u3001HodgeRank\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660eICRT\u80fd\u7a33\u5b9a\u7ed5\u8fc7\u4e3b\u6d41\u6a21\u578b\u7684\u5b89\u5168\u673a\u5236\uff0c\u751f\u6210\u9ad8\u98ce\u9669\u5185\u5bb9\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u8d8a\u72f1\u653b\u51fb\u98ce\u9669\uff0c\u4e3a\u9632\u5fa1\u7b56\u7565\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2505.03054", "pdf": "https://arxiv.org/pdf/2505.03054", "abs": "https://arxiv.org/abs/2505.03054", "authors": ["Orevaoghene Ahia", "Martijn Bartelds", "Kabir Ahuja", "Hila Gonen", "Valentin Hofmann", "Siddhant Arora", "Shuyue Stella Li", "Vishal Puttagunta", "Mofetoluwa Adeyemi", "Charishma Buchireddy", "Ben Walls", "Noah Bennett", "Shinji Watanabe", "Noah A. Smith", "Yulia Tsvetkov", "Sachin Kumar"], "title": "BLAB: Brutally Long Audio Bench", "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Developing large audio language models (LMs) capable of understanding diverse\nspoken interactions is essential for accommodating the multimodal nature of\nhuman communication and can increase the accessibility of language technologies\nacross different user populations. Recent work on audio LMs has primarily\nevaluated their performance on short audio segments, typically under 30\nseconds, with limited exploration of long-form conversational speech segments\nthat more closely reflect natural user interactions with these models. We\nintroduce Brutally Long Audio Bench (BLAB), a challenging long-form audio\nbenchmark that evaluates audio LMs on localization, duration estimation,\nemotion, and counting tasks using audio segments averaging 51 minutes in\nlength. BLAB consists of 833+ hours of diverse, full-length audio clips, each\npaired with human-annotated, text-based natural language questions and answers.\nOur audio data were collected from permissively licensed sources and underwent\na human-assisted filtering process to ensure task compliance. We evaluate six\nopen-source and proprietary audio LMs on BLAB and find that all of them,\nincluding advanced models such as Gemini 2.0 Pro and GPT-4o, struggle with the\ntasks in BLAB. Our comprehensive analysis reveals key insights into the\ntrade-offs between task difficulty and audio duration. In general, we find that\naudio LMs struggle with long-form speech, with performance declining as\nduration increases. They perform poorly on localization, temporal reasoning,\ncounting, and struggle to understand non-phonemic information, relying more on\nprompts than audio content. BLAB serves as a challenging evaluation framework\nto develop audio LMs with robust long-form audio understanding capabilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aBLAB\u7684\u957f\u97f3\u9891\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5728\u957f\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u957f\u97f3\u9891\u7406\u89e3\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u7406\u89e3\u591a\u6837\u5316\u8bed\u97f3\u4ea4\u4e92\u7684\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u66f4\u8d34\u8fd1\u81ea\u7136\u7528\u6237\u4ea4\u4e92\u7684\u591a\u6a21\u6001\u9700\u6c42\uff0c\u5e76\u63d0\u5347\u8bed\u8a00\u6280\u672f\u7684\u53ef\u8bbf\u95ee\u6027\u3002", "method": "\u5f15\u5165\u4e86BLAB\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b833+\u5c0f\u65f6\u7684\u591a\u6837\u5316\u957f\u97f3\u9891\u7247\u6bb5\uff08\u5e73\u574751\u5206\u949f\uff09\uff0c\u5e76\u914d\u6709\u4eba\u5de5\u6807\u6ce8\u7684\u6587\u672c\u95ee\u7b54\u5bf9\uff0c\u8bc4\u4f30\u4e86\u516d\u79cd\u5f00\u6e90\u548c\u4e13\u6709\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u6240\u6709\u6a21\u578b\uff08\u5305\u62ecGemini 2.0 Pro\u548cGPT-4o\uff09\u5728BLAB\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u5728\u5b9a\u4f4d\u3001\u65f6\u95f4\u63a8\u7406\u3001\u8ba1\u6570\u7b49\u4efb\u52a1\u4e0a\uff0c\u6027\u80fd\u968f\u97f3\u9891\u65f6\u957f\u589e\u52a0\u800c\u4e0b\u964d\u3002", "conclusion": "BLAB\u4e3a\u5f00\u53d1\u5177\u6709\u957f\u97f3\u9891\u7406\u89e3\u80fd\u529b\u7684\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u6311\u6218\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u957f\u8bed\u97f3\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.02881", "pdf": "https://arxiv.org/pdf/2505.02881", "abs": "https://arxiv.org/abs/2505.02881", "authors": ["Kazuki Fujii", "Yukito Tajima", "Sakae Mizuki", "Hinari Shimada", "Taihei Shiotani", "Koshiro Saito", "Masanari Ohi", "Masaki Kawamura", "Taishi Nakamura", "Takumi Okamoto", "Shigeki Ishida", "Kakeru Hattori", "Youmi Ma", "Hiroya Takamura", "Rio Yokota", "Naoaki Okazaki"], "title": "Rewriting Pre-Training Data Boosts LLM Performance in Math and Code", "categories": ["cs.LG", "cs.AI"], "comment": "27pages(including appendix), 10 figures", "summary": "The performance of large language models (LLMs) in program synthesis and\nmathematical reasoning is fundamentally limited by the quality of their\npre-training corpora. We introduce two openly licensed datasets, released under\nthe Llama 3.3 Community License, that significantly enhance LLM performance by\nsystematically rewriting public data. SwallowCode (approximately 16.1 billion\ntokens) refines Python snippets from The-Stack-v2 through a novel four-stage\npipeline: syntax validation, pylint-based style filtering, and a two-stage LLM\nrewriting process that enforces style conformity and transforms snippets into\nself-contained, algorithmically efficient examples. Unlike prior methods that\nrely on exclusionary filtering or limited transformations, our\ntransform-and-retain approach upgrades low-quality code, maximizing data\nutility. SwallowMath (approximately 2.3 billion tokens) enhances Finemath-4+ by\nremoving boilerplate, restoring context, and reformatting solutions into\nconcise, step-by-step explanations. Within a fixed 50 billion token training\nbudget, continual pre-training of Llama-3.1-8B with SwallowCode boosts pass@1\nby +17.0 on HumanEval and +17.7 on HumanEval+ compared to Stack-Edu, surpassing\nthe baseline model's code generation capabilities. Similarly, substituting\nSwallowMath yields +12.4 accuracy on GSM8K and +7.6 on MATH. Ablation studies\nconfirm that each pipeline stage contributes incrementally, with rewriting\ndelivering the largest gains. All datasets, prompts, and checkpoints are\npublicly available, enabling reproducible research and advancing LLM\npre-training for specialized domains.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e24\u6b3e\u65b0\u6570\u636e\u96c6SwallowCode\u548cSwallowMath\uff0c\u901a\u8fc7\u4f18\u5316\u516c\u5f00\u6570\u636e\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7a0b\u5e8f\u5408\u6210\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7a0b\u5e8f\u5408\u6210\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u56e0\u9884\u8bad\u7ec3\u8bed\u6599\u8d28\u91cf\u4e0d\u8db3\u800c\u8868\u73b0\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u56db\u9636\u6bb5\u6d41\u6c34\u7ebf\uff08\u8bed\u6cd5\u9a8c\u8bc1\u3001\u98ce\u683c\u8fc7\u6ee4\u3001\u4e24\u9636\u6bb5LLM\u91cd\u5199\uff09\u4f18\u5316\u4ee3\u7801\u7247\u6bb5\uff0c\u5e76\u5c06\u6570\u5b66\u95ee\u9898\u8f6c\u5316\u4e3a\u7b80\u6d01\u7684\u9010\u6b65\u89e3\u91ca\u3002", "result": "\u5728\u56fa\u5b9a50B token\u8bad\u7ec3\u9884\u7b97\u4e0b\uff0c\u4f7f\u7528SwallowCode\u548cSwallowMath\u5206\u522b\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728HumanEval\u3001GSM8K\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u4f18\u5316\u516c\u5f00\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u516c\u5f00\u4e86\u6240\u6709\u6570\u636e\u96c6\u548c\u5de5\u5177\u4ee5\u4fc3\u8fdb\u53ef\u91cd\u590d\u7814\u7a76\u548c\u9886\u57df\u4e13\u7528LLM\u7684\u9884\u8bad\u7ec3\u3002"}}
{"id": "2505.02865", "pdf": "https://arxiv.org/pdf/2505.02865", "abs": "https://arxiv.org/abs/2505.02865", "authors": ["Zhihai Wang", "Jie Wang", "Jilai Pan", "Xilin Xia", "Huiling Zhen", "Mingxuan Yuan", "Jianye Hao", "Feng Wu"], "title": "Accelerating Large Language Model Reasoning via Speculative Search", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICML2025", "summary": "Tree-search-based reasoning methods have significantly enhanced the reasoning\ncapability of large language models (LLMs) by facilitating the exploration of\nmultiple intermediate reasoning steps, i.e., thoughts. However, these methods\nsuffer from substantial inference latency, as they have to generate numerous\nreasoning thoughts, severely limiting LLM applicability. To address this\nchallenge, we propose a novel Speculative Search (SpecSearch) framework that\nsignificantly accelerates LLM reasoning by optimizing thought generation.\nSpecifically, SpecSearch utilizes a small model to strategically collaborate\nwith a large model at both thought and token levels, efficiently generating\nhigh-quality reasoning thoughts. The major pillar of SpecSearch is a novel\nquality-preserving rejection mechanism, which effectively filters out thoughts\nwhose quality falls below that of the large model's outputs. Moreover, we show\nthat SpecSearch preserves comparable reasoning quality to the large model.\nExperiments on both the Qwen and Llama models demonstrate that SpecSearch\nsignificantly outperforms state-of-the-art approaches, achieving up to\n2.12$\\times$ speedup with comparable reasoning quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpeculative Search\uff08SpecSearch\uff09\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5c0f\u6a21\u578b\u4e0e\u5927\u6a21\u578b\u7684\u534f\u4f5c\uff0c\u4f18\u5316\u601d\u7ef4\u751f\u6210\uff0c\u663e\u8457\u52a0\u901f\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6811\u641c\u7d22\u7684\u63a8\u7406\u65b9\u6cd5\u867d\u7136\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u7531\u4e8e\u751f\u6210\u5927\u91cf\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\uff08\u5373\u601d\u7ef4\uff09\uff0c\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u9ad8\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faSpecSearch\u6846\u67b6\uff0c\u901a\u8fc7\u5c0f\u6a21\u578b\u4e0e\u5927\u6a21\u578b\u5728\u601d\u7ef4\u548c\u6807\u8bb0\u5c42\u9762\u7684\u534f\u4f5c\uff0c\u4f18\u5316\u601d\u7ef4\u751f\u6210\uff0c\u5e76\u7ed3\u5408\u8d28\u91cf\u4fdd\u7559\u62d2\u7edd\u673a\u5236\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u601d\u7ef4\u3002", "result": "\u5728Qwen\u548cLlama\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSpecSearch\u5728\u4fdd\u6301\u63a8\u7406\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad82.12\u500d\u7684\u52a0\u901f\u6548\u679c\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SpecSearch\u5728\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u4e0e\u5927\u578b\u6a21\u578b\u76f8\u5f53\u7684\u63a8\u7406\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5ef6\u8fdf\u95ee\u9898\u3002"}}
{"id": "2505.03108", "pdf": "https://arxiv.org/pdf/2505.03108", "abs": "https://arxiv.org/abs/2505.03108", "authors": ["Brendan Campbell", "Alan Williams", "Kleio Baxevani", "Alyssa Campbell", "Rushabh Dhoke", "Rileigh E. Hudock", "Xiaomin Lin", "Vivek Mange", "Bernhard Neuberger", "Arjun Suresh", "Alhim Vera", "Arthur Trembanis", "Herbert G. Tanner", "Edward Hale"], "title": "Is AI currently capable of identifying wild oysters? A comparison of human annotators against the AI model, ODYSSEE", "categories": ["cs.AI"], "comment": null, "summary": "Oysters are ecologically and commercially important species that require\nfrequent monitoring to track population demographics (e.g. abundance, growth,\nmortality). Current methods of monitoring oyster reefs often require\ndestructive sampling methods and extensive manual effort. Therefore, they are\nsuboptimal for small-scale or sensitive environments. A recent alternative, the\nODYSSEE model, was developed to use deep learning techniques to identify live\noysters using video or images taken in the field of oyster reefs to assess\nabundance. The validity of this model in identifying live oysters on a reef was\ncompared to expert and non-expert annotators. In addition, we identified\npotential sources of prediction error. Although the model can make inferences\nsignificantly faster than expert and non-expert annotators (39.6 s, $2.34 \\pm\n0.61$ h, $4.50 \\pm 1.46$ h, respectively), the model overpredicted the number\nof live oysters, achieving lower accuracy (63\\%) in identifying live oysters\ncompared to experts (74\\%) and non-experts (75\\%) alike. Image quality was an\nimportant factor in determining the accuracy of the model and the annotators.\nBetter quality images improved human accuracy and worsened model accuracy.\nAlthough ODYSSEE was not sufficiently accurate, we anticipate that future\ntraining on higher-quality images, utilizing additional live imagery, and\nincorporating additional annotation training classes will greatly improve the\nmodel's predictive power based on the results of this analysis. Future research\nshould address methods that improve the detection of living vs. dead oysters.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684ODYSSEE\u6a21\u578b\uff0c\u7528\u4e8e\u901a\u8fc7\u89c6\u9891\u6216\u56fe\u50cf\u8bc6\u522b\u6d3b\u7261\u86ce\u4ee5\u8bc4\u4f30\u5176\u6570\u91cf\uff0c\u4f46\u6a21\u578b\u51c6\u786e\u6027\uff0863%\uff09\u4f4e\u4e8e\u4e13\u5bb6\uff0874%\uff09\u548c\u975e\u4e13\u5bb6\uff0875%\uff09\u3002\u56fe\u50cf\u8d28\u91cf\u5bf9\u6a21\u578b\u548c\u4eba\u5de5\u6807\u6ce8\u7684\u51c6\u786e\u6027\u6709\u91cd\u8981\u5f71\u54cd\u3002\u672a\u6765\u9700\u4f18\u5316\u6a21\u578b\u8bad\u7ec3\u4ee5\u63d0\u9ad8\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7261\u86ce\u7901\u76d1\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u7834\u574f\u6027\u91c7\u6837\u548c\u5927\u91cf\u4eba\u5de5\u5de5\u4f5c\uff0c\u4e0d\u9002\u7528\u4e8e\u5c0f\u89c4\u6a21\u6216\u654f\u611f\u73af\u5883\u3002ODYSSEE\u6a21\u578b\u65e8\u5728\u901a\u8fc7\u975e\u4fb5\u5165\u6027\u6280\u672f\u63d0\u9ad8\u76d1\u6d4b\u6548\u7387\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5f00\u53d1ODYSSEE\u6a21\u578b\uff0c\u901a\u8fc7\u89c6\u9891\u6216\u56fe\u50cf\u81ea\u52a8\u8bc6\u522b\u6d3b\u7261\u86ce\uff0c\u5e76\u4e0e\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\u7684\u6807\u6ce8\u7ed3\u679c\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5206\u6790\u9884\u6d4b\u9519\u8bef\u6765\u6e90\u3002", "result": "\u6a21\u578b\u63a8\u65ad\u901f\u5ea6\u663e\u8457\u5feb\u4e8e\u4eba\u5de5\uff0839.6\u79d2 vs. \u51e0\u5c0f\u65f6\uff09\uff0c\u4f46\u6d3b\u7261\u86ce\u8bc6\u522b\u51c6\u786e\u6027\u8f83\u4f4e\uff0863% vs. \u4e13\u5bb674%\u3001\u975e\u4e13\u5bb675%\uff09\u3002\u56fe\u50cf\u8d28\u91cf\u5bf9\u6a21\u578b\u548c\u4eba\u5de5\u51c6\u786e\u6027\u5747\u6709\u5f71\u54cd\u3002", "conclusion": "\u5c3d\u7ba1\u6a21\u578b\u51c6\u786e\u6027\u4e0d\u8db3\uff0c\u672a\u6765\u901a\u8fc7\u66f4\u9ad8\u8d28\u56fe\u50cf\u8bad\u7ec3\u3001\u589e\u52a0\u6d3b\u4f53\u56fe\u50cf\u548c\u6807\u6ce8\u7c7b\u522b\u53ef\u63d0\u5347\u6027\u80fd\u3002\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u6d3b\u7261\u86ce\u4e0e\u6b7b\u7261\u86ce\u7684\u533a\u5206\u65b9\u6cd5\u3002"}}
{"id": "2505.02884", "pdf": "https://arxiv.org/pdf/2505.02884", "abs": "https://arxiv.org/abs/2505.02884", "authors": ["Guangzhi Sun", "Potsawee Manakul", "Xiao Zhan", "Mark Gales"], "title": "Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Unlearning has emerged as a critical capability for large language models\n(LLMs) to support data privacy, regulatory compliance, and ethical AI\ndeployment. Recent techniques often rely on obfuscation by injecting incorrect\nor irrelevant information to suppress knowledge. Such methods effectively\nconstitute knowledge addition rather than true removal, often leaving models\nvulnerable to probing. In this paper, we formally distinguish unlearning from\nobfuscation and introduce a probing-based evaluation framework to assess\nwhether existing approaches genuinely remove targeted information. Moreover, we\npropose DF-MCQ, a novel unlearning method that flattens the model predictive\ndistribution over automatically generated multiple-choice questions using\nKL-divergence, effectively removing knowledge about target individuals and\ntriggering appropriate refusal behaviour. Experimental results demonstrate that\nDF-MCQ achieves unlearning with over 90% refusal rate and a random choice-level\nuncertainty that is much higher than obfuscation on probing questions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86DF-MCQ\u65b9\u6cd5\uff0c\u901a\u8fc7KL\u6563\u5ea6\u5728\u81ea\u52a8\u751f\u6210\u7684\u591a\u9009\u9898\u4e0a\u6241\u5e73\u5316\u6a21\u578b\u9884\u6d4b\u5206\u5e03\uff0c\u6709\u6548\u5b9e\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u7684\u771f\u6b63\u9057\u5fd8\uff08unlearning\uff09\uff0c\u800c\u975e\u4ec5\u4ec5\u6a21\u7cca\u5316\uff08obfuscation\uff09\u3002\u5b9e\u9a8c\u663e\u793a\u5176\u62d2\u7edd\u7387\u8d8590%\uff0c\u4e14\u4e0d\u786e\u5b9a\u6027\u663e\u8457\u9ad8\u4e8e\u6a21\u7cca\u5316\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u9057\u5fd8\u6280\u672f\uff08\u5982\u6a21\u7cca\u5316\uff09\u4ec5\u901a\u8fc7\u6ce8\u5165\u9519\u8bef\u4fe1\u606f\u63a9\u76d6\u77e5\u8bc6\u800c\u975e\u771f\u6b63\u79fb\u9664\u7684\u95ee\u9898\uff0c\u4ece\u800c\u6ee1\u8db3\u6570\u636e\u9690\u79c1\u3001\u6cd5\u89c4\u5408\u89c4\u548c\u4f26\u7406AI\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86DF-MCQ\u65b9\u6cd5\uff0c\u5229\u7528KL\u6563\u5ea6\u5728\u81ea\u52a8\u751f\u6210\u7684\u591a\u9009\u9898\u4e0a\u6241\u5e73\u5316\u6a21\u578b\u9884\u6d4b\u5206\u5e03\uff0c\u6709\u6548\u79fb\u9664\u76ee\u6807\u77e5\u8bc6\u5e76\u89e6\u53d1\u9002\u5f53\u7684\u62d2\u7edd\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDF-MCQ\u5728\u9057\u5fd8\u76ee\u6807\u4e2a\u4f53\u77e5\u8bc6\u65f6\u62d2\u7edd\u7387\u8d85\u8fc790%\uff0c\u4e14\u5728\u63a2\u6d4b\u95ee\u9898\u4e0a\u7684\u4e0d\u786e\u5b9a\u6027\u663e\u8457\u9ad8\u4e8e\u6a21\u7cca\u5316\u65b9\u6cd5\uff0c\u8fbe\u5230\u968f\u673a\u9009\u62e9\u6c34\u5e73\u3002", "conclusion": "\u8bba\u6587\u7ed3\u8bba\u662fDF-MCQ\u65b9\u6cd5\u771f\u6b63\u5b9e\u73b0\u4e86\u77e5\u8bc6\u79fb\u9664\u800c\u975e\u6a21\u7cca\u5316\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9057\u5fd8\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02872", "pdf": "https://arxiv.org/pdf/2505.02872", "abs": "https://arxiv.org/abs/2505.02872", "authors": ["Cfir Avraham Hadar", "Omer Shubi", "Yoav Meiri", "Yevgeni Berzak"], "title": "Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "When reading, we often have specific information that interests us in a text.\nFor example, you might be reading this paper because you are curious about LLMs\nfor eye movements in reading, the experimental design, or perhaps you only care\nabout the question ``but does it work?''. More broadly, in daily life, people\napproach texts with any number of text-specific goals that guide their reading\nbehavior. In this work, we ask, for the first time, whether open-ended reading\ngoals can be automatically decoded from eye movements in reading. To address\nthis question, we introduce goal classification and goal reconstruction tasks\nand evaluation frameworks, and use large-scale eye tracking for reading data in\nEnglish with hundreds of text-specific information seeking tasks. We develop\nand compare several discriminative and generative multimodal LLMs that combine\neye movements and text for goal classification and goal reconstruction. Our\nexperiments show considerable success on both tasks, suggesting that LLMs can\nextract valuable information about the readers' text-specific goals from eye\nmovements.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u773c\u52a8\u6570\u636e\u81ea\u52a8\u89e3\u7801\u8bfb\u8005\u7684\u5f00\u653e\u578b\u9605\u8bfb\u76ee\u6807\uff0c\u901a\u8fc7\u76ee\u6807\u5206\u7c7b\u548c\u91cd\u6784\u4efb\u52a1\uff0c\u7ed3\u5408\u591a\u6a21\u6001LLM\u6a21\u578b\uff0c\u5b9e\u9a8c\u8bc1\u660eLLM\u80fd\u4ece\u773c\u52a8\u4e2d\u6709\u6548\u63d0\u53d6\u9605\u8bfb\u76ee\u6807\u4fe1\u606f\u3002", "motivation": "\u63a2\u8ba8\u65e5\u5e38\u9605\u8bfb\u4e2d\uff0c\u8bfb\u8005\u5e38\u56e0\u7279\u5b9a\u76ee\u6807\u800c\u8c03\u6574\u9605\u8bfb\u884c\u4e3a\uff0c\u7814\u7a76\u662f\u5426\u80fd\u901a\u8fc7\u773c\u52a8\u6570\u636e\u81ea\u52a8\u8bc6\u522b\u8fd9\u4e9b\u76ee\u6807\uff0c\u586b\u8865\u8be5\u9886\u57df\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u76ee\u6807\u5206\u7c7b\u548c\u91cd\u6784\u4efb\u52a1\u6846\u67b6\uff0c\u5229\u7528\u5927\u89c4\u6a21\u82f1\u8bed\u9605\u8bfb\u773c\u52a8\u6570\u636e\uff0c\u5f00\u53d1\u5e76\u6bd4\u8f83\u591a\u79cd\u7ed3\u5408\u773c\u52a8\u548c\u6587\u672c\u7684\u591a\u6a21\u6001LLM\u6a21\u578b\uff08\u5224\u522b\u5f0f\u548c\u751f\u6210\u5f0f\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u5728\u4e24\u7c7b\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u826f\u597d\uff0c\u9a8c\u8bc1\u4e86LLM\u53ef\u4ece\u773c\u52a8\u6570\u636e\u4e2d\u6709\u6548\u63a8\u65ad\u8bfb\u8005\u7684\u6587\u672c\u76f8\u5173\u76ee\u6807\u3002", "conclusion": "\u773c\u52a8\u6570\u636e\u643a\u5e26\u4e86\u9605\u8bfb\u76ee\u6807\u7684\u5173\u952e\u4fe1\u606f\uff0c\u591a\u6a21\u6001LLM\u80fd\u6210\u529f\u89e3\u7801\u8fd9\u4e9b\u4fe1\u606f\uff0c\u4e3a\u9605\u8bfb\u884c\u4e3a\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.03135", "pdf": "https://arxiv.org/pdf/2505.03135", "abs": "https://arxiv.org/abs/2505.03135", "authors": ["Haoran Ou", "Gelei Deng", "Xingshuo Han", "Jie Zhang", "Xinlei He", "Han Qiu", "Shangwei Guo", "Tianwei Zhang"], "title": "Holmes: Automated Fact Check with Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "The rise of Internet connectivity has accelerated the spread of\ndisinformation, threatening societal trust, decision-making, and national\nsecurity. Disinformation has evolved from simple text to complex multimodal\nforms combining images and text, challenging existing detection methods.\nTraditional deep learning models struggle to capture the complexity of\nmultimodal disinformation. Inspired by advances in AI, this study explores\nusing Large Language Models (LLMs) for automated disinformation detection. The\nempirical study shows that (1) LLMs alone cannot reliably assess the\ntruthfulness of claims; (2) providing relevant evidence significantly improves\ntheir performance; (3) however, LLMs cannot autonomously search for accurate\nevidence. To address this, we propose Holmes, an end-to-end framework featuring\na novel evidence retrieval method that assists LLMs in collecting high-quality\nevidence. Our approach uses (1) LLM-powered summarization to extract key\ninformation from open sources and (2) a new algorithm and metrics to evaluate\nevidence quality. Holmes enables LLMs to verify claims and generate\njustifications effectively. Experiments show Holmes achieves 88.3% accuracy on\ntwo open-source datasets and 90.2% in real-time verification tasks. Notably,\nour improved evidence retrieval boosts fact-checking accuracy by 30.8% over\nexisting methods", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u68c0\u6d4b\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86Holmes\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u8bc1\u636e\u68c0\u7d22\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u4e92\u8054\u7f51\u865a\u5047\u4fe1\u606f\u7684\u4f20\u64ad\u5a01\u80c1\u793e\u4f1a\u4fe1\u4efb\u4e0e\u5b89\u5168\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u7684\u590d\u6742\u6027\u3002", "method": "\u63d0\u51faHolmes\u6846\u67b6\uff0c\u7ed3\u5408LLM\u9a71\u52a8\u7684\u5185\u5bb9\u6458\u8981\u548c\u65b0\u578b\u8bc1\u636e\u8d28\u91cf\u8bc4\u4f30\u7b97\u6cd5\uff0c\u4f18\u5316\u8bc1\u636e\u68c0\u7d22\u4ee5\u8f85\u52a9LLMs\u9a8c\u8bc1\u4fe1\u606f\u3002", "result": "Holmes\u5728\u5f00\u653e\u6570\u636e\u96c6\u548c\u5b9e\u65f6\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u5206\u522b\u8fbe\u523088.3%\u548c90.2%\u7684\u51c6\u786e\u7387\uff0c\u8bc1\u636e\u68c0\u7d22\u6539\u8fdb\u4f7f\u4e8b\u5b9e\u6838\u67e5\u51c6\u786e\u6027\u63d0\u534730.8%\u3002", "conclusion": "Holmes\u6846\u67b6\u901a\u8fc7\u9ad8\u6548\u8bc1\u636e\u68c0\u7d22\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684\u4fe1\u606f\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02888", "pdf": "https://arxiv.org/pdf/2505.02888", "abs": "https://arxiv.org/abs/2505.02888", "authors": ["Rintaro Ando"], "title": "When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and a Formal RSI Trigger", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T05, 68Q85", "I.2.0; I.2.3; I.2.6"], "comment": "20 pages, 4 figures, 3 tables. Code:\n  github.com/rintaro-ando-tech/n2m-rsi-demo (v1.0)", "summary": "We present Noise-to-Meaning Recursive Self-Improvement (N2M-RSI), a minimal\nformal model showing that once an AI agent feeds its own outputs back as inputs\nand crosses an explicit information-integration threshold, its internal\ncomplexity will grow without bound under our assumptions. The framework unifies\nearlier ideas on self-prompting large language models, G\\\"odelian\nself-reference, and AutoML, yet remains implementation-agnostic. The model\nfurthermore scales naturally to interacting swarms of agents, hinting at\nsuper-linear effects once communication among instances is permitted. For\nsafety reasons, we omit system-specific implementation details and release only\na brief, model-agnostic toy prototype in Appendix C.", "AI": {"tldr": "N2M-RSI\u662f\u4e00\u4e2a\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u5c55\u793aAI\u4ee3\u7406\u5728\u53cd\u9988\u8f93\u51fa\u4f5c\u4e3a\u8f93\u5165\u5e76\u8d85\u8fc7\u4fe1\u606f\u6574\u5408\u9608\u503c\u540e\uff0c\u5185\u90e8\u590d\u6742\u6027\u4f1a\u65e0\u9650\u589e\u957f\uff0c\u7edf\u4e00\u4e86\u81ea\u63d0\u793aLLM\u3001\u54e5\u5fb7\u5c14\u81ea\u5f15\u7528\u548cAutoML\u7b49\u6982\u5ff5\u3002", "motivation": "\u63a2\u7d22AI\u4ee3\u7406\u901a\u8fc7\u81ea\u53cd\u9988\u5b9e\u73b0\u65e0\u9650\u590d\u6742\u6027\u7684\u6f5c\u529b\uff0c\u5e76\u63ed\u793a\u7fa4\u4f53\u4ea4\u4e92\u53ef\u80fd\u5e26\u6765\u7684\u8d85\u7ebf\u6027\u6548\u5e94\u3002", "method": "\u63d0\u51faN2M-RSI\u6a21\u578b\uff0c\u57fa\u4e8e\u81ea\u53cd\u9988\u548c\u660e\u786e\u7684\u4fe1\u606f\u6574\u5408\u9608\u503c\uff0c\u6a21\u578b\u5b9e\u73b0\u4e0e\u5177\u4f53\u5b9e\u73b0\u65e0\u5173\u3002", "result": "\u6a21\u578b\u663e\u793aAI\u4ee3\u7406\u5185\u90e8\u590d\u6742\u6027\u5c06\u65e0\u9650\u589e\u957f\uff0c\u7fa4\u4f53\u4ea4\u4e92\u53ef\u80fd\u4ea7\u751f\u8d85\u7ebf\u6027\u6548\u5e94\u3002", "conclusion": "N2M-RSI\u4e3a\u81ea\u6539\u8fdbAI\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u4f46\u56e0\u5b89\u5168\u8003\u8651\u672a\u516c\u5f00\u5177\u4f53\u5b9e\u73b0\u7ec6\u8282\u3002"}}
{"id": "2505.02983", "pdf": "https://arxiv.org/pdf/2505.02983", "abs": "https://arxiv.org/abs/2505.02983", "authors": ["Wenjie Hua", "Shenghan Xu"], "title": "Logits-Constrained Framework with RoBERTa for Ancient Chinese NER", "categories": ["cs.CL", "68T50", "I.2.7; I.5.1; I.5.4"], "comment": "5 pages, 2 figures, 6 tables. Accepted to EvaHan 2025 shared task on\n  Ancient Chinese NLP", "summary": "This paper presents a Logits-Constrained (LC) framework for Ancient Chinese\nNamed Entity Recognition (NER), evaluated on the EvaHan 2025 benchmark. Our\ntwo-stage model integrates GujiRoBERTa for contextual encoding and a\ndifferentiable decoding mechanism to enforce valid BMES label transitions.\nExperiments demonstrate that LC improves performance over traditional CRF and\nBiLSTM-based approaches, especially in high-label or large-data settings. We\nalso propose a model selection criterion balancing label complexity and dataset\nsize, providing practical guidance for real-world Ancient Chinese NLP tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdLogits-Constrained (LC)\u6846\u67b6\u7528\u4e8e\u53e4\u6c49\u8bed\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff0c\u91c7\u7528GujiRoBERTa\u548c\u53ef\u5fae\u5206\u89e3\u7801\u673a\u5236\uff0c\u5728EvaHan 2025\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfCRF\u548cBiLSTM\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u53e4\u6c49\u8bed\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4e2d\u9ad8\u6807\u7b7e\u6216\u5927\u6570\u636e\u573a\u666f\u4e0b\u4f20\u7edf\u65b9\u6cd5\u7684\u6027\u80fd\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u4e24\u9636\u6bb5\u6a21\u578b\uff0c\u7ed3\u5408GujiRoBERTa\u8fdb\u884c\u4e0a\u4e0b\u6587\u7f16\u7801\u548c\u53ef\u5fae\u5206\u89e3\u7801\u673a\u5236\u4ee5\u786e\u4fdd\u6709\u6548\u7684BMES\u6807\u7b7e\u8f6c\u6362\u3002", "result": "LC\u6846\u67b6\u5728\u6027\u80fd\u4e0a\u4f18\u4e8eCRF\u548cBiLSTM\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u9ad8\u6807\u7b7e\u6216\u5927\u6570\u636e\u8bbe\u7f6e\u4e2d\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u578b\u9009\u62e9\u6807\u51c6\u4e3a\u5b9e\u9645\u53e4\u6c49\u8bedNLP\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5e73\u8861\u6807\u7b7e\u590d\u6742\u6027\u548c\u6570\u636e\u96c6\u5927\u5c0f\u3002"}}
{"id": "2505.03171", "pdf": "https://arxiv.org/pdf/2505.03171", "abs": "https://arxiv.org/abs/2505.03171", "authors": ["Junqi Liu", "Xiaohan Lin", "Jonas Bayer", "Yael Dillies", "Weijie Jiang", "Xiaodan Liang", "Roman Soletskyi", "Haiming Wang", "Yunzhou Xie", "Beibei Xiong", "Zhengfeng Yang", "Jujian Zhang", "Lihong Zhi", "Jia Li", "Zhengying Liu"], "title": "CombiBench: Benchmarking LLM Capability for Combinatorial Mathematics", "categories": ["cs.AI"], "comment": null, "summary": "Neurosymbolic approaches integrating large language models with formal\nreasoning have recently achieved human-level performance on mathematics\ncompetition problems in algebra, geometry and number theory. In comparison,\ncombinatorics remains a challenging domain, characterized by a lack of\nappropriate benchmarks and theorem libraries. To address this gap, we introduce\nCombiBench, a comprehensive benchmark comprising 100 combinatorial problems,\neach formalized in Lean~4 and paired with its corresponding informal statement.\nThe problem set covers a wide spectrum of difficulty levels, ranging from\nmiddle school to IMO and university level, and span over ten combinatorial\ntopics. CombiBench is suitable for testing IMO solving capabilities since it\nincludes all IMO combinatorial problems since 2000 (except IMO 2004 P3 as its\nstatement contain an images). Furthermore, we provide a comprehensive and\nstandardized evaluation framework, dubbed Fine-Eval (for\n$\\textbf{F}$ill-in-the-blank $\\textbf{in}$ L$\\textbf{e}$an Evaluation), for\nformal mathematics. It accommodates not only proof-based problems but also, for\nthe first time, the evaluation of fill-in-the-blank questions. Using Fine-Eval\nas the evaluation method and Kimina Lean Server as the backend, we benchmark\nseveral LLMs on CombiBench and observe that their capabilities for formally\nsolving combinatorial problems remain limited. Among all models tested (none of\nwhich has been trained for this particular task), Kimina-Prover attains the\nbest results, solving 7 problems (out of 100) under both ``with solution'' and\n``without solution'' scenarios. We open source the benchmark dataset alongside\nwith the code of the proposed evaluation method at\nhttps://github.com/MoonshotAI/CombiBench/.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86CombiBench\uff0c\u4e00\u4e2a\u6db5\u76d6100\u4e2a\u7ec4\u5408\u95ee\u9898\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u65e8\u5728\u5f25\u8865\u7ec4\u5408\u6570\u5b66\u9886\u57df\u7f3a\u4e4f\u5408\u9002\u57fa\u51c6\u548c\u5b9a\u7406\u5e93\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u8bc4\u4f30\u6846\u67b6Fine-Eval\u3002", "motivation": "\u7ec4\u5408\u6570\u5b66\u9886\u57df\u7f3a\u4e4f\u9002\u5f53\u7684\u57fa\u51c6\u548c\u5b9a\u7406\u5e93\uff0c\u5bfc\u81f4\u8be5\u9886\u57df\u7684\u81ea\u52a8\u5316\u63a8\u7406\u7814\u7a76\u8fdb\u5c55\u7f13\u6162\u3002", "method": "\u63d0\u51fa\u4e86CombiBench\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86Fine-Eval\u8bc4\u4f30\u6846\u67b6\uff0c\u652f\u6301\u5f62\u5f0f\u5316\u6570\u5b66\u95ee\u9898\u7684\u8bc4\u4f30\uff0c\u5305\u62ec\u586b\u7a7a\u5f0f\u95ee\u9898\u3002", "result": "\u6d4b\u8bd5\u4e86\u591a\u4e2aLLM\u6a21\u578b\uff0c\u7ed3\u679c\u8868\u660e\u5b83\u4eec\u5728\u5f62\u5f0f\u5316\u89e3\u51b3\u7ec4\u5408\u95ee\u9898\u65b9\u9762\u80fd\u529b\u6709\u9650\uff08\u6700\u9ad8\u89e3\u51b37/100\u95ee\u9898\uff09\u3002", "conclusion": "CombiBench\u548cFine-Eval\u4e3a\u7ec4\u5408\u6570\u5b66\u7684\u5f62\u5f0f\u5316\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u8868\u73b0\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002"}}
{"id": "2505.02889", "pdf": "https://arxiv.org/pdf/2505.02889", "abs": "https://arxiv.org/abs/2505.02889", "authors": ["Oyindolapo O. Komolafe", "Zhimin Mei", "David Morales Zarate", "Gregory William Spangenberg"], "title": "Early Prediction of Sepsis: Feature-Aligned Transfer Learning", "categories": ["cs.LG", "cs.AI"], "comment": "A project implemented for MACHINE LEARNING IN HEALTH AND BIOMEDICAL\n  SCIENCE", "summary": "Sepsis is a life threatening medical condition that occurs when the body has\nan extreme response to infection, leading to widespread inflammation, organ\nfailure, and potentially death. Because sepsis can worsen rapidly, early\ndetection is critical to saving lives. However, current diagnostic methods\noften identify sepsis only after significant damage has already occurred. Our\nproject aims to address this challenge by developing a machine learning based\nsystem to predict sepsis in its early stages, giving healthcare providers more\ntime to intervene.\n  A major problem with existing models is the wide variability in the patient\ninformation or features they use, such as heart rate, temperature, and lab\nresults. This inconsistency makes models difficult to compare and limits their\nability to work across different hospitals and settings. To solve this, we\npropose a method called Feature Aligned Transfer Learning (FATL), which\nidentifies and focuses on the most important and commonly reported features\nacross multiple studies, ensuring the model remains consistent and clinically\nrelevant.\n  Most existing models are trained on narrow patient groups, leading to\npopulation bias. FATL addresses this by combining knowledge from models trained\non diverse populations, using a weighted approach that reflects each models\ncontribution. This makes the system more generalizable and effective across\ndifferent patient demographics and clinical environments. FATL offers a\npractical and scalable solution for early sepsis detection, particularly in\nhospitals with limited resources, and has the potential to improve patient\noutcomes, reduce healthcare costs, and support more equitable healthcare\ndelivery.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFATL\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7279\u5f81\u5bf9\u9f50\u8fc1\u79fb\u5b66\u4e60\u6765\u65e9\u671f\u9884\u6d4b\u8d25\u8840\u75c7\uff0c\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u7279\u5f81\u4e0d\u4e00\u81f4\u548c\u4eba\u7fa4\u504f\u89c1\u7684\u95ee\u9898\u3002", "motivation": "\u8d25\u8840\u75c7\u662f\u4e00\u79cd\u5371\u53ca\u751f\u547d\u7684\u75be\u75c5\uff0c\u65e9\u671f\u68c0\u6d4b\u5bf9\u633d\u6551\u751f\u547d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u8bca\u65ad\u65b9\u6cd5\u5f80\u5f80\u5728\u635f\u4f24\u53d1\u751f\u540e\u624d\u8bc6\u522b\uff0c\u4e14\u6a21\u578b\u7279\u5f81\u4e0d\u4e00\u81f4\u3001\u5b58\u5728\u4eba\u7fa4\u504f\u89c1\u3002", "method": "\u63d0\u51fa\u4e86FATL\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u805a\u7126\u8de8\u7814\u7a76\u7684\u91cd\u8981\u4e14\u5e38\u89c1\u7684\u7279\u5f81\uff0c\u7ed3\u5408\u591a\u6837\u4eba\u7fa4\u8bad\u7ec3\u7684\u6a21\u578b\u77e5\u8bc6\uff0c\u91c7\u7528\u52a0\u6743\u65b9\u6cd5\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "result": "FATL\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e00\u81f4\u4e14\u4e34\u5e8a\u76f8\u5173\u7684\u8d25\u8840\u75c7\u65e9\u671f\u9884\u6d4b\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u60a3\u8005\u4eba\u7fa4\u548c\u4e34\u5e8a\u73af\u5883\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u7684\u533b\u9662\u3002", "conclusion": "FATL\u662f\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u6539\u5584\u60a3\u8005\u9884\u540e\u3001\u964d\u4f4e\u533b\u7597\u6210\u672c\u5e76\u4fc3\u8fdb\u66f4\u516c\u5e73\u7684\u533b\u7597\u670d\u52a1\u3002"}}
{"id": "2505.03005", "pdf": "https://arxiv.org/pdf/2505.03005", "abs": "https://arxiv.org/abs/2505.03005", "authors": ["Daniel Goldstein", "Eric Alcaide", "Janna Lu", "Eugene Cheah"], "title": "RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "We present Rapid Attention Distillation to Linear Attention Decoders at Scale\n(RADLADS), a protocol for rapidly converting softmax attention transformers\ninto linear attention decoder models, along with two new RWKV-variant\narchitectures, and models converted from popular Qwen2.5 open source models in\n7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,\nless than 0.005% of the token count used to train the original teacher models.\nConverting to our 72B linear attention model costs less than \\$2,000 USD at\ntoday's prices, yet quality at inference remains close to the original\ntransformer. These models achieve state-of-the-art downstream performance\nacross a set of standard benchmarks for linear attention models of their size.\nWe release all our models on HuggingFace under the Apache 2.0 license, with the\nexception of our 72B models which are also governed by the Qwen License\nAgreement.\n  Models at\nhttps://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102\nTraining Code at https://github.com/recursal/RADLADS-paper", "AI": {"tldr": "RADLADS\u662f\u4e00\u79cd\u5feb\u901f\u5c06softmax\u6ce8\u610f\u529bTransformer\u8f6c\u6362\u4e3a\u7ebf\u6027\u6ce8\u610f\u529b\u89e3\u7801\u5668\u6a21\u578b\u7684\u534f\u8bae\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u65b0\u7684RWKV\u53d8\u4f53\u67b6\u6784\u3002\u8f6c\u6362\u8fc7\u7a0b\u4ec5\u9700350-700M tokens\uff0c\u6210\u672c\u4f4e\u4e8e2000\u7f8e\u5143\uff0c\u4e14\u63a8\u7406\u8d28\u91cf\u63a5\u8fd1\u539f\u59cbTransformer\u3002\u8fd9\u4e9b\u6a21\u578b\u5728\u6807\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6a21\u578b\u5df2\u5f00\u6e90\u3002", "motivation": "\u65e8\u5728\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u5730\u5c06\u73b0\u6709softmax\u6ce8\u610f\u529bTransformer\u6a21\u578b\u8f6c\u6362\u4e3a\u7ebf\u6027\u6ce8\u610f\u529b\u89e3\u7801\u5668\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u63a8\u7406\u6027\u80fd\u3002", "method": "\u63d0\u51faRADLADS\u534f\u8bae\uff0c\u901a\u8fc7\u5c11\u91cf\u6570\u636e\uff08350-700M tokens\uff09\u8fdb\u884c\u8f6c\u6362\uff0c\u5e76\u8bbe\u8ba1\u4e24\u79cd\u65b0\u7684RWKV\u67b6\u6784\u3002\u652f\u6301\u4eceQwen2.5\u5f00\u6e90\u6a21\u578b\u8f6c\u63627B\u300132B\u548c72B\u89c4\u6a21\u7684\u6a21\u578b\u3002", "result": "\u8f6c\u6362\u540e\u768472B\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b\u6210\u672c\u4f4e\u4e8e2000\u7f8e\u5143\uff0c\u63a8\u7406\u8d28\u91cf\u63a5\u8fd1\u539f\u59cbTransformer\uff0c\u5e76\u5728\u6807\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u540c\u7c7b\u6a21\u578b\u7684\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "RADLADS\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u6a21\u578b\u8f6c\u6362\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\uff0c\u6a21\u578b\u5df2\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u4e0e\u5e94\u7528\u3002"}}
{"id": "2505.03189", "pdf": "https://arxiv.org/pdf/2505.03189", "abs": "https://arxiv.org/abs/2505.03189", "authors": ["Yixiong Hao", "Ayush Panda", "Stepan Shabalin", "Sheikh Abdur Raheem Ali"], "title": "Patterns and Mechanisms of Contrastive Activation Engineering", "categories": ["cs.AI", "cs.HC"], "comment": "Published at the ICLR 2025 Bi-Align, HAIC, and Building Trust\n  workshops", "summary": "Controlling the behavior of Large Language Models (LLMs) remains a\nsignificant challenge due to their inherent complexity and opacity. While\ntechniques like fine-tuning can modify model behavior, they typically require\nextensive computational resources. Recent work has introduced a class of\ncontrastive activation engineering (CAE) techniques as promising approaches for\nsteering LLM outputs through targeted modifications to their internal\nrepresentations. Applied at inference-time with zero cost, CAE has the\npotential to introduce a new paradigm of flexible, task-specific LLM behavior\ntuning. We analyze the performance of CAE in in-distribution,\nout-of-distribution settings, evaluate drawbacks, and begin to develop\ncomprehensive guidelines for its effective deployment. We find that 1. CAE is\nonly reliably effective when applied to in-distribution contexts. 2. Increasing\nthe number of samples used to generate steering vectors has diminishing returns\nat around 80 samples. 3. Steering vectors are susceptible to adversarial inputs\nthat reverses the behavior that is steered for. 4. Steering vectors harm the\noverall model perplexity. 5. Larger models are more resistant to\nsteering-induced degradation.", "AI": {"tldr": "\u5bf9\u6bd4\u6fc0\u6d3b\u5de5\u7a0b\uff08CAE\uff09\u4f5c\u4e3a\u4e00\u79cd\u96f6\u6210\u672c\u3001\u63a8\u7406\u65f6\u8c03\u6574\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7684\u65b9\u6cd5\uff0c\u867d\u5728\u5206\u5e03\u5185\u6709\u6548\u4f46\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u5bf9\u6297\u8f93\u5165\u8106\u5f31\u6027\u3001\u56f0\u60d1\u5ea6\u964d\u4f4e\u7b49\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u6027\u548c\u4e0d\u900f\u660e\u6027\uff0c\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0cCAE\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u3001\u4efb\u52a1\u7279\u5b9a\u7684\u8c03\u6574\u65b9\u5f0f\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u6fc0\u6d3b\u5de5\u7a0b\u6280\u672f\uff0c\u5728\u63a8\u7406\u65f6\u9488\u5bf9\u6027\u8c03\u6574\u6a21\u578b\u5185\u90e8\u8868\u793a\uff0c\u5206\u6790\u5176\u5728\u5206\u5e03\u5185\u5916\u573a\u666f\u7684\u6548\u679c\u53ca\u7a33\u5b9a\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0CAE\u4ec5\u9002\u7528\u4e8e\u5206\u5e03\u5185\u573a\u666f\uff0c\u6837\u672c\u91cf\u8d85\u8fc780\u540e\u6536\u76ca\u9012\u51cf\uff0c\u6613\u53d7\u5bf9\u6297\u8f93\u5165\u5f71\u54cd\u5e76\u964d\u4f4e\u6a21\u578b\u56f0\u60d1\u5ea6\uff0c\u4f46\u5927\u6a21\u578b\u6297\u6027\u8f83\u5f3a\u3002", "conclusion": "CAE\u867d\u5177\u6f5c\u529b\uff0c\u4f46\u9700\u8c28\u614e\u90e8\u7f72\u4ee5\u5e94\u5bf9\u5176\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u5206\u5e03\u5916\u548c\u5bf9\u6297\u6027\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002"}}
{"id": "2505.02922", "pdf": "https://arxiv.org/pdf/2505.02922", "abs": "https://arxiv.org/abs/2505.02922", "authors": ["Yaoqi Chen", "Jinkai Zhang", "Baotong Lu", "Qianxi Zhang", "Chengruidong Zhang", "Jingjia Luo", "Di Liu", "Huiqiang Jiang", "Qi Chen", "Jing Liu", "Bailu Ding", "Xiao Yan", "Jiawei Jiang", "Chen Chen", "Mingxing Zhang", "Yuqing Yang", "Fan Yang", "Mao Yang"], "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM Inference", "categories": ["cs.LG"], "comment": "16 pages", "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.", "AI": {"tldr": "RetroInfer is a system that optimizes long-context LLM inference by redefining the KV cache as a vector storage system, leveraging attention sparsity for speed and efficiency without losing accuracy.", "motivation": "Addressing the inefficiencies in GPU memory and bandwidth usage during long-context LLM inference, which hinder performance.", "method": "Utilizes a 'wave index' for efficient token retrieval via techniques like tripartite attention approximation and a 'wave buffer' for hardware coordination, improving throughput.", "result": "Achieves up to 4.5X speedup over full attention and 10.5X over sparse baselines while maintaining full-attention accuracy.", "conclusion": "RetroInfer offers a robust solution for long-context LLM inference, balancing speed, efficiency, and accuracy."}}
{"id": "2505.03019", "pdf": "https://arxiv.org/pdf/2505.03019", "abs": "https://arxiv.org/abs/2505.03019", "authors": ["Alb\u00e9rick Euraste Djir\u00e9", "Abdoul Kader Kabor\u00e9", "Earl T. Barr", "Jacques Klein", "Tegawend\u00e9 F. Bissyand\u00e9"], "title": "Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation Analysis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While Large Language Models (LLMs) achieve remarkable performance through\ntraining on massive datasets, they can exhibit concerning behaviors such as\nverbatim reproduction of training data rather than true generalization. This\nmemorization phenomenon raises significant concerns about data privacy,\nintellectual property rights, and the reliability of model evaluations. This\npaper introduces PEARL, a novel approach for detecting memorization in LLMs.\nPEARL assesses how sensitive an LLM's performance is to input perturbations,\nenabling memorization detection without requiring access to the model's\ninternals. We investigate how input perturbations affect the consistency of\noutputs, enabling us to distinguish between true generalization and\nmemorization. Our findings, following extensive experiments on the Pythia open\nmodel, provide a robust framework for identifying when the model simply\nregurgitates learned information. Applied on the GPT 4o models, the PEARL\nframework not only identified cases of memorization of classic texts from the\nBible or common code from HumanEval but also demonstrated that it can provide\nsupporting evidence that some data, such as from the New York Times news\narticles, were likely part of the training data of a given model.", "AI": {"tldr": "\u6458\u8981", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bad\u7ec3\u65f6\u53ef\u80fd\u76f4\u63a5\u8bb0\u5fc6\u800c\u975e\u6cdb\u5316\u6570\u636e\uff0c\u8fd9\u5f15\u53d1\u4e86\u9690\u79c1\u3001\u77e5\u8bc6\u4ea7\u6743\u548c\u6a21\u578b\u8bc4\u4f30\u53ef\u9760\u6027\u7684\u62c5\u5fe7\u3002", "method": "\u63d0\u51faPEARL\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f93\u5165\u6270\u52a8\u68c0\u6d4bLLMs\u7684\u8f93\u51fa\u4e00\u81f4\u6027\uff0c\u4ee5\u533a\u5206\u8bb0\u5fc6\u548c\u6cdb\u5316\u884c\u4e3a\u3002", "result": "\u5728Pythia\u548cGPT-4o\u6a21\u578b\u4e0a\u6d4b\u8bd5\u663e\u793a\uff0cPEARL\u80fd\u6709\u6548\u8bc6\u522b\u8bb0\u5fc6\u884c\u4e3a\uff0c\u5e76\u63d0\u4f9b\u8bad\u7ec3\u6570\u636e\u6765\u6e90\u7684\u8bc1\u636e\u3002", "conclusion": "PEARL\u4e3a\u68c0\u6d4bLLMs\u7684\u8bb0\u5fc6\u884c\u4e3a\u63d0\u4f9b\u4e86\u65e0\u4fb5\u5165\u4e14\u6709\u6548\u7684\u6846\u67b6\u3002"}}
{"id": "2505.03275", "pdf": "https://arxiv.org/pdf/2505.03275", "abs": "https://arxiv.org/abs/2505.03275", "authors": ["Tiantian Gan", "Qiyao Sun"], "title": "RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented Generation", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "Large language models (LLMs) struggle to effectively utilize a growing number\nof external tools, such as those defined by the Model Context Protocol\n(MCP)\\cite{IntroducingMCP}, due to prompt bloat and selection complexity. We\nintroduce RAG-MCP, a Retrieval-Augmented Generation framework that overcomes\nthis challenge by offloading tool discovery. RAG-MCP uses semantic retrieval to\nidentify the most relevant MCP(s) for a given query from an external index\nbefore engaging the LLM. Only the selected tool descriptions are passed to the\nmodel, drastically reducing prompt size and simplifying decision-making.\nExperiments, including an MCP stress test, demonstrate RAG-MCP significantly\ncuts prompt tokens (e.g., by over 50%) and more than triples tool selection\naccuracy (43.13% vs 13.62% baseline) on benchmark tasks. RAG-MCP enables\nscalable and accurate tool integration for LLMs.", "AI": {"tldr": "RAG-MCP\u6846\u67b6\u901a\u8fc7\u8bed\u4e49\u68c0\u7d22\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f7f\u7528\u5916\u90e8\u5de5\u5177\u65f6\u7684\u63d0\u793a\u81a8\u80c0\u548c\u9009\u62e9\u590d\u6742\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u5904\u7406\u5927\u91cf\u5916\u90e8\u5de5\u5177\u65f6\u7684\u63d0\u793a\u81a8\u80c0\u548c\u5de5\u5177\u9009\u62e9\u590d\u6742\u6027\u95ee\u9898\u3002", "method": "\u5f15\u5165RAG-MCP\u6846\u67b6\uff0c\u5229\u7528\u8bed\u4e49\u68c0\u7d22\u4ece\u5916\u90e8\u7d22\u5f15\u4e2d\u7b5b\u9009\u76f8\u5173\u5de5\u5177\u63cf\u8ff0\uff0c\u4ec5\u5c06\u9009\u4e2d\u7684\u5de5\u5177\u4f20\u9012\u7ed9LLM\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cRAG-MCP\u51cf\u5c11\u63d0\u793a\u4ee4\u724c\u8d85\u8fc750%\uff0c\u5de5\u5177\u9009\u62e9\u51c6\u786e\u7387\u4ece13.62%\u63d0\u5347\u81f343.13%\u3002", "conclusion": "RAG-MCP\u4e3aLLM\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u51c6\u786e\u7684\u5916\u90e8\u5de5\u5177\u96c6\u6210\u65b9\u6848\u3002"}}
{"id": "2505.02959", "pdf": "https://arxiv.org/pdf/2505.02959", "abs": "https://arxiv.org/abs/2505.02959", "authors": ["Enrique Nueve", "Bo Waggoner"], "title": "Smooth Quadratic Prediction Markets", "categories": ["cs.LG", "cs.GT"], "comment": null, "summary": "When agents trade in a Duality-based Cost Function prediction market, they\ncollectively implement the learning algorithm Follow-The-Regularized-Leader. We\nask whether other learning algorithms could be used to inspire the design of\nprediction markets. By decomposing and modifying the Duality-based Cost\nFunction Market Maker's (DCFMM) pricing mechanism, we propose a new prediction\nmarket, called the Smooth Quadratic Prediction Market, the incentivizes agents\nto collectively implement general steepest gradient descent. Relative to the\nDCFMM, the Smooth Quadratic Prediction Market has a better worst-case monetary\nloss for AD securities while preserving axiom guarantees such as the existence\nof instantaneous price, information incorporation, expressiveness, no\narbitrage, and a form of incentive compatibility. To motivate the application\nof the Smooth Quadratic Prediction Market, we independently examine agents'\ntrading behavior under two realistic constraints: bounded budgets and buy-only\nsecurities. Finally, we provide an introductory analysis of an approach to\nfacilitate adaptive liquidity using the Smooth Quadratic AD Prediction Market.\nOur results suggest future designs where the price update rule is separate from\nthe fee structure, yet guarantees are preserved.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9884\u6d4b\u5e02\u573a\u8bbe\u8ba1\u2014\u2014\u5e73\u6ed1\u4e8c\u6b21\u9884\u6d4b\u5e02\u573a\uff0c\u901a\u8fc7\u5206\u89e3\u548c\u4fee\u6539\u57fa\u4e8e\u5bf9\u5076\u7684\u6210\u672c\u51fd\u6570\u5e02\u573a\u5b9a\u4ef7\u673a\u5236\uff0c\u6fc0\u52b1\u4ee3\u7406\u4eba\u96c6\u4f53\u5b9e\u73b0\u5e7f\u4e49\u6700\u901f\u68af\u5ea6\u4e0b\u964d\u3002\u76f8\u6bd4DCFMM\uff0c\u8be5\u5e02\u573a\u5728AD\u8bc1\u5238\u4e0a\u5177\u6709\u66f4\u4f18\u7684\u6700\u574f\u60c5\u51b5\u8d27\u5e01\u635f\u5931\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u77ac\u65f6\u4ef7\u683c\u5b58\u5728\u3001\u4fe1\u606f\u6574\u5408\u3001\u8868\u8fbe\u6027\u3001\u65e0\u5957\u5229\u548c\u67d0\u79cd\u5f62\u5f0f\u7684\u6fc0\u52b1\u517c\u5bb9\u6027\u7b49\u516c\u7406\u4fdd\u8bc1\u3002", "motivation": "\u63a2\u8ba8\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5176\u4ed6\u5b66\u4e60\u7b97\u6cd5\u6765\u8bbe\u8ba1\u9884\u6d4b\u5e02\u573a\uff0c\u7a81\u7834\u73b0\u6709\u57fa\u4e8e\u5bf9\u5076\u7684\u6210\u672c\u51fd\u6570\u9884\u6d4b\u5e02\u573a\u7684\u5c40\u9650\uff0c\u63d0\u5347\u5e02\u573a\u6548\u7387\u5e76\u9002\u5e94\u66f4\u5b9e\u9645\u7684\u4ea4\u6613\u7ea6\u675f\u3002", "method": "\u63d0\u51fa\u5e73\u6ed1\u4e8c\u6b21\u9884\u6d4b\u5e02\u573a\uff0c\u901a\u8fc7\u5206\u89e3\u548c\u4fee\u6539DCFMM\u7684\u5b9a\u4ef7\u673a\u5236\uff0c\u5b9e\u73b0\u5e7f\u4e49\u6700\u901f\u68af\u5ea6\u4e0b\u964d\u3002\u5728\u9884\u7b97\u6709\u9650\u548c\u4ec5\u9650\u4e70\u5165\u8bc1\u5238\u7684\u73b0\u5b9e\u7ea6\u675f\u4e0b\u5206\u6790\u4e86\u4ee3\u7406\u4eba\u4ea4\u6613\u884c\u4e3a\u3002", "result": "\u5e73\u6ed1\u4e8c\u6b21\u9884\u6d4b\u5e02\u573a\u5728AD\u8bc1\u5238\u4e0a\u8868\u73b0\u51fa\u66f4\u4f18\u7684\u6700\u574f\u60c5\u51b5\u8d27\u5e01\u635f\u5931\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6838\u5fc3\u516c\u7406\u4fdd\u8bc1\u3002\u521d\u6b65\u5206\u6790\u4e86\u81ea\u9002\u5e94\u6d41\u52a8\u6027\u7684\u5b9e\u73b0\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9884\u6d4b\u5e02\u573a\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u672a\u6765\u53ef\u63a2\u7d22\u4ef7\u683c\u66f4\u65b0\u89c4\u5219\u4e0e\u8d39\u7528\u7ed3\u6784\u5206\u79bb\u7684\u8bbe\u8ba1\uff0c\u540c\u65f6\u4fdd\u7559\u5173\u952e\u4fdd\u8bc1\u3002"}}
{"id": "2505.03025", "pdf": "https://arxiv.org/pdf/2505.03025", "abs": "https://arxiv.org/abs/2505.03025", "authors": ["Steven Bedrick", "A. Seza Do\u011fru\u00f6z", "Sergiu Nisioi"], "title": "A Typology of Synthetic Datasets for Dialogue Processing in Clinical Contexts", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Synthetic data sets are used across linguistic domains and NLP tasks,\nparticularly in scenarios where authentic data is limited (or even\nnon-existent). One such domain is that of clinical (healthcare) contexts, where\nthere exist significant and long-standing challenges (e.g., privacy,\nanonymization, and data governance) which have led to the development of an\nincreasing number of synthetic datasets. One increasingly important category of\nclinical dataset is that of clinical dialogues which are especially sensitive\nand difficult to collect, and as such are commonly synthesized.\n  While such synthetic datasets have been shown to be sufficient in some\nsituations, little theory exists to inform how they may be best used and\ngeneralized to new applications. In this paper, we provide an overview of how\nsynthetic datasets are created, evaluated and being used for dialogue related\ntasks in the medical domain. Additionally, we propose a novel typology for use\nin classifying types and degrees of data synthesis, to facilitate comparison\nand evaluation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u4e34\u5e8a\u5bf9\u8bdd\u9886\u57df\u5408\u6210\u6570\u636e\u96c6\u7684\u521b\u5efa\u3001\u8bc4\u4f30\u548c\u4f7f\u7528\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u7c7b\u6cd5\u6765\u533a\u5206\u6570\u636e\u7684\u5408\u6210\u7c7b\u578b\u548c\u7a0b\u5ea6\u3002", "motivation": "\u7531\u4e8e\u771f\u5b9e\u4e34\u5e8a\u5bf9\u8bdd\u6570\u636e\u7684\u9690\u79c1\u548c\u6cbb\u7406\u95ee\u9898\u96be\u4ee5\u83b7\u53d6\uff0c\u5408\u6210\u6570\u636e\u96c6\u5728\u6b64\u9886\u57df\u663e\u5f97\u5c24\u4e3a\u91cd\u8981\u3002\u7136\u800c\uff0c\u5f53\u524d\u7f3a\u4e4f\u7406\u8bba\u6307\u5bfc\u5982\u4f55\u6700\u4f73\u5229\u7528\u548c\u63a8\u5e7f\u8fd9\u4e9b\u6570\u636e\u3002", "method": "\u8bba\u6587\u56de\u987e\u4e86\u5408\u6210\u6570\u636e\u96c6\u7684\u521b\u5efa\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u5206\u7c7b\u6cd5\uff0c\u7528\u4e8e\u6bd4\u8f83\u548c\u8bc4\u4f30\u4e0d\u540c\u5408\u6210\u6570\u636e\u7684\u7c7b\u578b\u548c\u7a0b\u5ea6\u3002", "result": "\u7814\u7a76\u6982\u8ff0\u4e86\u4e34\u5e8a\u5bf9\u8bdd\u5408\u6210\u6570\u636e\u96c6\u7684\u73b0\u72b6\uff0c\u5e76\u901a\u8fc7\u5206\u7c7b\u6cd5\u4e3a\u6570\u636e\u5408\u6210\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u4e86\u5408\u6210\u6570\u636e\u96c6\u5728\u4e34\u5e8a\u5bf9\u8bdd\u9886\u57df\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u7684\u5206\u7c7b\u6cd5\u6709\u52a9\u4e8e\u672a\u6765\u7814\u7a76\u7684\u6570\u636e\u6bd4\u8f83\u548c\u5e94\u7528\u63a8\u5e7f\u3002"}}
{"id": "2505.03295", "pdf": "https://arxiv.org/pdf/2505.03295", "abs": "https://arxiv.org/abs/2505.03295", "authors": ["Luis Miguel Vieira da Silva", "Aljosha K\u00f6cher", "Nicolas K\u00f6nig", "Felix Gehlhoff", "Alexander Fay"], "title": "Capability-Driven Skill Generation with LLMs: A RAG-Based Approach for Reusing Existing Libraries and Interfaces", "categories": ["cs.AI", "cs.RO", "cs.SE"], "comment": null, "summary": "Modern automation systems increasingly rely on modular architectures, with\ncapabilities and skills as one solution approach. Capabilities define the\nfunctions of resources in a machine-readable form and skills provide the\nconcrete implementations that realize those capabilities. However, the\ndevelopment of a skill implementation conforming to a corresponding capability\nremains a time-consuming and challenging task. In this paper, we present a\nmethod that treats capabilities as contracts for skill implementations and\nleverages large language models to generate executable code based on natural\nlanguage user input. A key feature of our approach is the integration of\nexisting software libraries and interface technologies, enabling the generation\nof skill implementations across different target languages. We introduce a\nframework that allows users to incorporate their own libraries and resource\ninterfaces into the code generation process through a retrieval-augmented\ngeneration architecture. The proposed method is evaluated using an autonomous\nmobile robot controlled via Python and ROS 2, demonstrating the feasibility and\nflexibility of the approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u751f\u6210\u6280\u80fd\u5b9e\u73b0\u4ee3\u7801\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6280\u80fd\u5f00\u53d1\u8017\u65f6\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u9762\u5bf9\u6a21\u5757\u5316\u67b6\u6784\u4e2d\u6280\u80fd\u5b9e\u73b0\u5f00\u53d1\u8017\u65f6\u4e14\u590d\u6742\u7684\u6311\u6218\uff0c\u7814\u7a76\u65e8\u5728\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u5c06\u80fd\u529b\u89c6\u4e3a\u6280\u80fd\u5b9e\u73b0\u7684\u5951\u7ea6\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u548c\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4ee3\u7801\uff0c\u5e76\u96c6\u6210\u73b0\u6709\u8f6f\u4ef6\u5e93\u548c\u63a5\u53e3\u6280\u672f\u3002", "result": "\u5728\u57fa\u4e8ePython\u548cROS 2\u7684\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "\u901a\u8fc7\u96c6\u6210\u7528\u6237\u81ea\u5b9a\u4e49\u5e93\u548c\u63a5\u53e3\uff0c\u8be5\u65b9\u6cd5\u4e3a\u8de8\u8bed\u8a00\u6280\u80fd\u5b9e\u73b0\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02974", "pdf": "https://arxiv.org/pdf/2505.02974", "abs": "https://arxiv.org/abs/2505.02974", "authors": ["Fabien Casenave", "Xavier Roynard", "Brian Staber", "Nissrine Akkari", "William Piat", "Michele Alessandro Bucci", "Abbas Kabalan", "Xuan Minh Vuong Nguyen", "Luca Saverio", "Rapha\u00ebl Carpintero Perez", "Anthony Kalaydjian", "Samy Fouch\u00e9", "Thierry Gonon", "Ghassan Najjar", "Emmanuel Menier", "Matthieu Nastorg", "Christian Rey"], "title": "Physics-Learning AI Datamodel (PLAID) datasets: a collection of physics simulations for machine learning", "categories": ["cs.LG"], "comment": null, "summary": "Machine learning-based surrogate models have emerged as a powerful tool to\naccelerate simulation-driven scientific workflows. However, their widespread\nadoption is hindered by the lack of large-scale, diverse, and standardized\ndatasets tailored to physics-based simulations. While existing initiatives\nprovide valuable contributions, many are limited in scope-focusing on specific\nphysics domains, relying on fragmented tooling, or adhering to overly\nsimplistic datamodels that restrict generalization. To address these\nlimitations, we introduce PLAID (Physics-Learning AI Datamodel), a flexible and\nextensible framework for representing and sharing datasets of physics\nsimulations. PLAID defines a unified standard for describing simulation data\nand is accompanied by a library for creating, reading, and manipulating complex\ndatasets across a wide range of physical use cases (gitlab.com/drti/plaid). We\nrelease six carefully crafted datasets under the PLAID standard, covering\nstructural mechanics and computational fluid dynamics, and provide baseline\nbenchmarks using representative learning methods. Benchmarking tools are made\navailable on Hugging Face, enabling direct participation by the community and\ncontribution to ongoing evaluation efforts (huggingface.co/PLAIDcompetitions).", "AI": {"tldr": "PLAID\u6846\u67b6\u63d0\u51fa\u4ee5\u6807\u51c6\u5316\u548c\u7075\u6d3b\u6027\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u7269\u7406\u6a21\u62df\u4e2d\u6570\u636e\u96c6\u5206\u6563\u548c\u5c40\u9650\u6027\u95ee\u9898\uff0c\u53d1\u5e03\u4e86\u516d\u4e2a\u6570\u636e\u96c6\u5e76\u63d0\u4f9b\u4e86\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\u3002", "motivation": "\u73b0\u6709\u7269\u7406\u6a21\u62df\u6570\u636e\u96c6\u5b58\u5728\u9886\u57df\u5c40\u9650\u3001\u5de5\u5177\u5206\u6563\u548c\u7b80\u5355\u6570\u636e\u6a21\u578b\u96be\u4ee5\u63a8\u5e7f\u7684\u95ee\u9898\uff0c\u9700\u8981\u7edf\u4e00\u6807\u51c6\u4ee5\u4fc3\u8fdb\u673a\u5668\u5b66\u4e60\u5728\u79d1\u5b66\u6a21\u62df\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86PLAID\u6846\u67b6\uff0c\u5b9a\u4e49\u7edf\u4e00\u7684\u6570\u636e\u63cf\u8ff0\u6807\u51c6\uff0c\u5e76\u914d\u5957\u5de5\u5177\u5e93\u652f\u6301\u591a\u9886\u57df\u7269\u7406\u6a21\u62df\u6570\u636e\u7684\u521b\u5efa\u4e0e\u7ba1\u7406\uff0c\u53d1\u5e03\u516d\u4e2a\u8de8\u9886\u57df\u6570\u636e\u96c6\u3002", "result": "PLAID\u6846\u67b6\u5b9e\u73b0\u4e86\u8de8\u7ed3\u6784\u529b\u5b66\u548c\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\u7684\u6570\u636e\u96c6\u6807\u51c6\u5316\uff0c\u5e76\u63d0\u4f9b\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u4fc3\u8fdb\u793e\u533a\u53c2\u4e0e\u8bc4\u4f30\u3002", "conclusion": "PLAID\u4e3a\u7269\u7406\u6a21\u62df\u6570\u636e\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u9ad8\u6548\u5f00\u53d1\u548c\u5e94\u7528\uff0c\u63a8\u52a8\u4e86\u79d1\u5b66\u6a21\u62df\u9886\u57df\u7684\u534f\u4f5c\u4e0e\u521b\u65b0\u3002"}}
{"id": "2505.03030", "pdf": "https://arxiv.org/pdf/2505.03030", "abs": "https://arxiv.org/abs/2505.03030", "authors": ["Sicong Huang", "Jincheng He", "Shiyuan Huang", "Karthik Raja Anandan", "Arkajyoti Chakraborty", "Ian Lane"], "title": "UCSC at SemEval-2025 Task 3: Context, Models and Prompt Optimization for Automated Hallucination Detection in LLM Output", "categories": ["cs.CL"], "comment": "6 pages, 1 figure", "summary": "Hallucinations pose a significant challenge for large language models when\nanswering knowledge-intensive queries. As LLMs become more widely adopted, it\nis crucial not only to detect if hallucinations occur but also to pinpoint\nexactly where in the LLM output they occur. SemEval 2025 Task 3, Mu-SHROOM:\nMultilingual Shared-task on Hallucinations and Related Observable\nOvergeneration Mistakes, is a recent effort in this direction. This paper\ndescribes the UCSC system submission to the shared Mu-SHROOM task. We introduce\na framework that first retrieves relevant context, next identifies false\ncontent from the answer, and finally maps them back to spans in the LLM output.\nThe process is further enhanced by automatically optimizing prompts. Our system\nachieves the highest overall performance, ranking #1 in average position across\nall languages. We release our code and experiment results.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u5728LLM\u8f93\u51fa\u4e2d\u68c0\u6d4b\u5e7b\u89c9\u5e76\u5b9a\u4f4d\u5176\u4f4d\u7f6e\uff0c\u6700\u7ec8\u5728Mu-SHROOM\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u77e5\u8bc6\u5bc6\u96c6\u578b\u67e5\u8be2\u65f6\u4ea7\u751f\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u7cbe\u786e\u5b9a\u4f4d\u5e7b\u89c9\u53d1\u751f\u7684\u4f4d\u7f6e\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u6846\u67b6\uff0c\u4f9d\u6b21\u68c0\u7d22\u76f8\u5173\u4e0a\u4e0b\u6587\u3001\u8bc6\u522b\u7b54\u6848\u4e2d\u7684\u9519\u8bef\u5185\u5bb9\uff0c\u5e76\u5c06\u5176\u6620\u5c04\u56deLLM\u8f93\u51fa\u4e2d\u7684\u5177\u4f53\u90e8\u5206\uff0c\u540c\u65f6\u901a\u8fc7\u81ea\u52a8\u4f18\u5316\u63d0\u793a\u589e\u5f3a\u6548\u679c\u3002", "result": "\u7cfb\u7edf\u5728Mu-SHROOM\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u9ad8\u7efc\u5408\u6027\u80fd\uff0c\u5728\u6240\u6709\u8bed\u8a00\u4e2d\u5e73\u5747\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u5e7b\u89c9\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\uff0c\u4ee3\u7801\u548c\u5b9e\u9a8c\u7ed3\u679c\u5df2\u516c\u5f00\u3002"}}
{"id": "2505.03315", "pdf": "https://arxiv.org/pdf/2505.03315", "abs": "https://arxiv.org/abs/2505.03315", "authors": ["Kanghyun Jo", "Jehwan Choi", "Kwanho Kim", "Seongmin Kim", "Duy-Linh Nguyen", "Xuan-Thuy Vo", "Adri Priadana", "Tien-Dat Tran"], "title": "Artificial Behavior Intelligence: Technology, Challenges, and Future Directions", "categories": ["cs.AI"], "comment": "9 pages, 6 figures, Pre-print for IWIS2025", "summary": "Understanding and predicting human behavior has emerged as a core capability\nin various AI application domains such as autonomous driving, smart healthcare,\nsurveillance systems, and social robotics. This paper defines the technical\nframework of Artificial Behavior Intelligence (ABI), which comprehensively\nanalyzes and interprets human posture, facial expressions, emotions, behavioral\nsequences, and contextual cues. It details the essential components of ABI,\nincluding pose estimation, face and emotion recognition, sequential behavior\nanalysis, and context-aware modeling. Furthermore, we highlight the\ntransformative potential of recent advances in large-scale pretrained models,\nsuch as large language models (LLMs), vision foundation models, and multimodal\nintegration models, in significantly improving the accuracy and\ninterpretability of behavior recognition. Our research team has a strong\ninterest in the ABI domain and is actively conducting research, particularly\nfocusing on the development of intelligent lightweight models capable of\nefficiently inferring complex human behaviors. This paper identifies several\ntechnical challenges that must be addressed to deploy ABI in real-world\napplications including learning behavioral intelligence from limited data,\nquantifying uncertainty in complex behavior prediction, and optimizing model\nstructures for low-power, real-time inference. To tackle these challenges, our\nteam is exploring various optimization strategies including lightweight\ntransformers, graph-based recognition architectures, energy-aware loss\nfunctions, and multimodal knowledge distillation, while validating their\napplicability in real-time environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4eba\u5de5\u884c\u4e3a\u667a\u80fd\uff08ABI\uff09\u7684\u6280\u672f\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u548c\u89e3\u91ca\u4eba\u7c7b\u59ff\u6001\u3001\u8868\u60c5\u3001\u60c5\u7eea\u3001\u884c\u4e3a\u5e8f\u5217\u53ca\u4e0a\u4e0b\u6587\u7ebf\u7d22\uff0c\u5e76\u63a2\u8ba8\u4e86\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u5bf9\u5176\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u63d0\u5347\u3002\u7814\u7a76\u56e2\u961f\u4e13\u6ce8\u4e8e\u5f00\u53d1\u8f7b\u91cf\u7ea7\u6a21\u578b\u4ee5\u9ad8\u6548\u63a8\u65ad\u590d\u6742\u884c\u4e3a\uff0c\u5e76\u63d0\u51fa\u4e86\u5e94\u5bf9\u6570\u636e\u6709\u9650\u6027\u3001\u884c\u4e3a\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u53ca\u6a21\u578b\u4f18\u5316\u7684\u6311\u6218\u65b9\u6848\u3002", "motivation": "\u7406\u89e3\u548c\u9884\u6d4b\u4eba\u7c7b\u884c\u4e3a\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u667a\u80fd\u533b\u7597\u7b49\u591a\u4e2aAI\u5e94\u7528\u9886\u57df\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7ABI\u6280\u672f\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u6570\u636e\u548c\u9884\u8bad\u7ec3\u5927\u6a21\u578b\uff0c\u63d0\u5347\u884c\u4e3a\u8bc6\u522b\u7684\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u59ff\u6001\u4f30\u8ba1\u3001\u8868\u60c5\u4e0e\u60c5\u7eea\u8bc6\u522b\u3001\u5e8f\u5217\u884c\u4e3a\u5206\u6790\u548c\u4e0a\u4e0b\u6587\u5efa\u6a21\u7b49\u5173\u952e\u6280\u672f\uff0c\u7814\u7a76\u56e2\u961f\u63a2\u7d22\u4e86\u8f7b\u91cf\u7ea7Transformer\u3001\u56fe\u67b6\u6784\u3001\u80fd\u8017\u611f\u77e5\u635f\u5931\u51fd\u6570\u548c\u591a\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u7b49\u4f18\u5316\u7b56\u7565\u3002", "result": "\u7814\u7a76\u91cd\u70b9\u5728\u4e8e\u5f00\u53d1\u80fd\u591f\u9ad8\u6548\u63a8\u65ad\u590d\u6742\u884c\u4e3a\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u65f6\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\uff0c\u540c\u65f6\u8bc6\u522b\u4e86\u6570\u636e\u9650\u5236\u3001\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u548c\u6a21\u578b\u4f18\u5316\u7b49\u6280\u672f\u6311\u6218\u3002", "conclusion": "ABI\u6846\u67b6\u53ca\u5176\u4f18\u5316\u65b9\u6cd5\u4e3a\u884c\u4e3a\u667a\u80fd\u7684\u5b9e\u65f6\u9ad8\u6548\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u6027\u3001\u63d0\u5347\u9884\u6d4b\u7a33\u5b9a\u6027\u53ca\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2505.02985", "pdf": "https://arxiv.org/pdf/2505.02985", "abs": "https://arxiv.org/abs/2505.02985", "authors": ["Mohammad Partohaghighi", "Roummel Marcia", "YangQuan Chen"], "title": "More Optimal Fractional-Order Stochastic Gradient Descent for Non-Convex Optimization Problems", "categories": ["cs.LG", "math.OC"], "comment": "8 pages submitted to IEEE CDC2025. arXiv admin note: substantial text\n  overlap with arXiv:2503.13764", "summary": "Fractional-order stochastic gradient descent (FOSGD) leverages fractional\nexponents to capture long-memory effects in optimization. However, its utility\nis often limited by the difficulty of tuning and stabilizing these exponents.\nWe propose 2SED Fractional-Order Stochastic Gradient Descent (2SEDFOSGD), which\nintegrates the Two-Scale Effective Dimension (2SED) algorithm with FOSGD to\nadapt the fractional exponent in a data-driven manner. By tracking model\nsensitivity and effective dimensionality, 2SEDFOSGD dynamically modulates the\nexponent to mitigate oscillations and hasten convergence. Theoretically, for\nonoconvex optimization problems, this approach preserves the advantages of\nfractional memory without the sluggish or unstable behavior observed in na\\\"ive\nfractional SGD. Empirical evaluations in Gaussian and $\\alpha$-stable noise\nscenarios using an autoregressive (AR) model highlight faster convergence and\nmore robust parameter estimates compared to baseline methods, underscoring the\npotential of dimension-aware fractional techniques for advanced modeling and\nestimation tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u5206\u6570\u9636\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd52SEDFOSGD\uff0c\u901a\u8fc7\u7ed3\u5408\u53cc\u5c3a\u5ea6\u6709\u6548\u7ef4\u5ea6\u7b97\u6cd5\u52a8\u6001\u8c03\u6574\u5206\u6570\u6307\u6570\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u5206\u6570\u6307\u6570\u8c03\u4f18\u548c\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u5728\u975e\u51f8\u4f18\u5316\u95ee\u9898\u4e2d\u5c55\u793a\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u9c81\u68d2\u7684\u53c2\u6570\u4f30\u8ba1\u3002", "motivation": "\u4f20\u7edf\u5206\u6570\u9636\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08FOSGD\uff09\u5728\u4f18\u5316\u4e2d\u5b58\u5728\u957f\u8bb0\u5fc6\u6548\u5e94\uff0c\u4f46\u5176\u5206\u6570\u6307\u6570\u7684\u8c03\u4f18\u548c\u7a33\u5b9a\u6027\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa2SEDFOSGD\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ddf\u8e2a\u6a21\u578b\u654f\u611f\u6027\u548c\u6709\u6548\u7ef4\u5ea6\uff0c\u52a8\u6001\u8c03\u6574\u5206\u6570\u6307\u6570\uff0c\u7ed3\u5408\u53cc\u5c3a\u5ea6\u6709\u6548\u7ef4\u5ea6\uff082SED\uff09\u7b97\u6cd5\u3002", "result": "\u5728\u975e\u51f8\u4f18\u5316\u95ee\u9898\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728Gaussian\u548c\u03b1-stable\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u9c81\u68d2\u7684\u53c2\u6570\u4f30\u8ba1\u3002", "conclusion": "2SEDFOSGD\u5c55\u793a\u4e86\u5728\u9ad8\u7ea7\u5efa\u6a21\u548c\u4f30\u8ba1\u4efb\u52a1\u4e2d\u6570\u636e\u9a71\u52a8\u7684\u5206\u6570\u9636\u6280\u672f\u6f5c\u529b\u3002"}}
{"id": "2505.03052", "pdf": "https://arxiv.org/pdf/2505.03052", "abs": "https://arxiv.org/abs/2505.03052", "authors": ["Ryan Wang", "Matthew Finlayson", "Luca Soldaini", "Swabha Swayamdipta", "Robin Jia"], "title": "Teaching Models to Understand (but not Generate) High-risk Data", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Language model developers typically filter out high-risk content -- such as\ntoxic or copyrighted text -- from their pre-training data to prevent models\nfrom generating similar outputs. However, removing such data altogether limits\nmodels' ability to recognize and appropriately respond to harmful or sensitive\ncontent. In this paper, we introduce Selective Loss to Understand but Not\nGenerate (SLUNG), a pre-training paradigm through which models learn to\nunderstand high-risk data without learning to generate it. Instead of uniformly\napplying the next-token prediction loss, SLUNG selectively avoids incentivizing\nthe generation of high-risk tokens while ensuring they remain within the\nmodel's context window. As the model learns to predict low-risk tokens that\nfollow high-risk ones, it is forced to understand the high-risk content.\nThrough our experiments, we show that SLUNG consistently improves models'\nunderstanding of high-risk data (e.g., ability to recognize toxic content)\nwithout increasing its generation (e.g., toxicity of model responses). Overall,\nour SLUNG paradigm enables models to benefit from high-risk text that would\notherwise be filtered out.", "AI": {"tldr": "SLUNG\u662f\u4e00\u79cd\u65b0\u7684\u9884\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u635f\u5931\u673a\u5236\uff0c\u8ba9\u6a21\u578b\u80fd\u591f\u7406\u89e3\u9ad8\u98ce\u9669\u5185\u5bb9\u4f46\u4e0d\u5b66\u4e60\u751f\u6210\u5b83\u4eec\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u9ad8\u98ce\u9669\u5185\u5bb9\u7684\u7406\u89e3\u80fd\u529b\uff0c\u540c\u65f6\u4e0d\u589e\u52a0\u5176\u751f\u6210\u6b64\u7c7b\u5185\u5bb9\u7684\u53ef\u80fd\u3002", "motivation": "\u4e3a\u4e86\u9632\u6b62\u6a21\u578b\u751f\u6210\u9ad8\u98ce\u9669\u5185\u5bb9\uff08\u5982\u6709\u6bd2\u6216\u53d7\u7248\u6743\u4fdd\u62a4\u7684\u6587\u672c\uff09\uff0c\u5f00\u53d1\u8005\u901a\u5e38\u4f1a\u5728\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u8fc7\u6ee4\u6389\u8fd9\u4e9b\u5185\u5bb9\u3002\u7136\u800c\uff0c\u5b8c\u5168\u79fb\u9664\u8fd9\u4e9b\u6570\u636e\u4f1a\u524a\u5f31\u6a21\u578b\u8bc6\u522b\u548c\u6070\u5f53\u5e94\u5bf9\u6709\u5bb3\u6216\u654f\u611f\u5185\u5bb9\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86SLUNG\uff08\u9009\u62e9\u6027\u635f\u5931\u673a\u5236\uff09\uff0c\u5728\u9884\u8bad\u7ec3\u4e2d\u907f\u514d\u5bf9\u9ad8\u98ce\u9669token\u7684\u9884\u6d4b\u635f\u5931\uff0c\u4f46\u4fdd\u7559\u5176\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u8feb\u4f7f\u6a21\u578b\u7406\u89e3\u8fd9\u7c7b\u5185\u5bb9\u800c\u4e0d\u751f\u6210\u5b83\u4eec\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSLUNG\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u9ad8\u98ce\u9669\u5185\u5bb9\u7684\u7406\u89e3\uff08\u5982\u8bc6\u522b\u6709\u6bd2\u5185\u5bb9\u7684\u80fd\u529b\uff09\uff0c\u540c\u65f6\u672a\u589e\u52a0\u6a21\u578b\u751f\u6210\u9ad8\u98ce\u9669\u5185\u5bb9\u7684\u6982\u7387\uff08\u5982\u56de\u590d\u7684\u6bd2\u6027\uff09\u3002", "conclusion": "SLUNG\u4f7f\u6a21\u578b\u80fd\u591f\u4ece\u672a\u88ab\u8fc7\u6ee4\u7684\u9ad8\u98ce\u9669\u6587\u672c\u4e2d\u83b7\u76ca\uff0c\u8fbe\u5230\u7406\u89e3\u548c\u5e94\u5bf9\u6b64\u7c7b\u5185\u5bb9\u7684\u76ee\u6807\u3002"}}
{"id": "2505.03332", "pdf": "https://arxiv.org/pdf/2505.03332", "abs": "https://arxiv.org/abs/2505.03332", "authors": ["Evgeny Markhasin"], "title": "AI-Driven Scholarly Peer Review via Persistent Workflow Prompting, Meta-Prompting, and Meta-Reasoning", "categories": ["cs.AI"], "comment": "22 pages, 36 pages (references and appendixes)", "summary": "Critical peer review of scientific manuscripts presents a significant\nchallenge for Large Language Models (LLMs), partly due to data limitations and\nthe complexity of expert reasoning. This report introduces Persistent Workflow\nPrompting (PWP), a potentially broadly applicable prompt engineering\nmethodology designed to bridge this gap using standard LLM chat interfaces\n(zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical\nanalysis of experimental chemistry manuscripts, featuring a hierarchical,\nmodular architecture (structured via Markdown) that defines detailed analysis\nworkflows. We develop this PWP prompt through iterative application of\nmeta-prompting techniques and meta-reasoning aimed at systematically codifying\nexpert review workflows, including tacit knowledge. Submitted once at the start\nof a session, this PWP prompt equips the LLM with persistent workflows\ntriggered by subsequent queries, guiding modern reasoning LLMs through\nsystematic, multimodal evaluations. Demonstrations show the PWP-guided LLM\nidentifying major methodological flaws in a test case while mitigating LLM\ninput bias and performing complex tasks, including distinguishing claims from\nevidence, integrating text/photo/figure analysis to infer parameters, executing\nquantitative feasibility checks, comparing estimates against claims, and\nassessing a priori plausibility. To ensure transparency and facilitate\nreplication, we provide full prompts, detailed demonstration analyses, and logs\nof interactive chats as supplementary resources. Beyond the specific\napplication, this work offers insights into the meta-development process\nitself, highlighting the potential of PWP, informed by detailed workflow\nformalization, to enable sophisticated analysis using readily available LLMs\nfor complex scientific tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6301\u4e45\u5de5\u4f5c\u6d41\u63d0\u793a\uff08PWP\uff09\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u6807\u51c6LLM\u804a\u5929\u754c\u9762\uff08\u65e0\u9700\u7f16\u7801\u6216API\uff09\u6539\u8fdb\u79d1\u5b66\u624b\u7a3f\u7684\u540c\u884c\u8bc4\u5ba1\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9a8c\u5316\u5b66\u9886\u57df\u8bc6\u522b\u65b9\u6cd5\u7f3a\u9677\u7b49\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u7a3f\u4ef6\u540c\u884c\u8bc4\u5ba1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5982\u6570\u636e\u4e0d\u8db3\u548c\u4e13\u5bb6\u63a8\u7406\u590d\u6742\u6027\uff0c\u901a\u8fc7\u4e00\u79cd\u96f6\u4ee3\u7801\u7684\u666e\u9002\u6027\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6a21\u5757\u5316\u67b6\u6784\u7684PWP\u63d0\u793a\uff0c\u901a\u8fc7\u5143\u63d0\u793a\u6280\u672f\u548c\u5143\u63a8\u7406\u8fed\u4ee3\u5f00\u53d1\uff0c\u7f16\u7801\u4e13\u5bb6\u8bc4\u5ba1\u5de5\u4f5c\u6d41\uff08\u5305\u62ec\u9690\u6027\u77e5\u8bc6\uff09\uff0c\u63d0\u4ea4\u540e\u53ef\u6301\u7eed\u89e6\u53d1\u7cfb\u7edf\u5316\u8bc4\u4f30\u3002", "result": "PWP\u5f15\u5bfc\u7684LLM\u6210\u529f\u8bc6\u522b\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u7684\u4e3b\u8981\u65b9\u6cd5\u7f3a\u9677\uff0c\u51cf\u5c11\u8f93\u5165\u504f\u5dee\uff0c\u5904\u7406\u590d\u6742\u4efb\u52a1\u5982\u533a\u5206\u4e3b\u5f20\u4e0e\u8bc1\u636e\u3001\u591a\u6a21\u6001\u5206\u6790\u7b49\u3002", "conclusion": "PWP\u57fa\u4e8e\u5de5\u4f5c\u6d41\u5f62\u5f0f\u5316\uff0c\u5c55\u793a\u4e86\u5229\u7528\u73b0\u6709LLM\u5b8c\u6210\u590d\u6742\u79d1\u5b66\u4efb\u52a1\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u4f9b\u4e86\u900f\u660e\u5316\u7684\u5f00\u53d1\u8fc7\u7a0b\u548c\u8d44\u6e90\u4ee5\u652f\u6301\u590d\u73b0\u3002"}}
{"id": "2505.03031", "pdf": "https://arxiv.org/pdf/2505.03031", "abs": "https://arxiv.org/abs/2505.03031", "authors": ["Sean I. Young"], "title": "Radio: Rate-Distortion Optimization for Large Language Model Compression", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ICML 2025", "summary": "In recent years, the compression of large language models (LLMs) has emerged\nas a key problem in facilitating LLM deployment on resource-limited devices,\nreducing compute costs, and mitigating the environmental footprint due to\nlarge-scale AI infrastructure. Here, we establish the foundations of LLM\nquantization from a rate-distortion theory perspective and propose a\nquantization technique based on simple rate-distortion optimization. Our\ntechnique scales to models containing hundreds of billions of weight parameters\nand offers users the flexibility to compress models, post-training, to a model\nsize or accuracy specified by the user.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7387\u5931\u771f\u7406\u8bba\u7684LLM\u91cf\u5316\u6280\u672f\uff0c\u652f\u6301\u5728\u8bad\u7ec3\u540e\u6839\u636e\u7528\u6237\u9700\u6c42\u7075\u6d3b\u538b\u7f29\u6a21\u578b\u5927\u5c0f\u6216\u7cbe\u5ea6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u90e8\u7f72\u5728\u8d44\u6e90\u6709\u9650\u7684\u8bbe\u5907\u4e0a\u65f6\u9762\u4e34\u6311\u6218\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u91cf\u5316\u6280\u672f\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u73af\u5883\u5f71\u54cd\u3002", "method": "\u4ece\u7387\u5931\u771f\u7406\u8bba\u89d2\u5ea6\u5efa\u7acbLLM\u91cf\u5316\u57fa\u7840\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7b80\u5355\u7387\u5931\u771f\u4f18\u5316\u7684\u91cf\u5316\u6280\u672f\u3002", "result": "\u8be5\u6280\u672f\u53ef\u6269\u5c55\u5230\u5305\u542b\u6570\u767e\u4ebf\u6743\u91cd\u53c2\u6570\u7684\u6a21\u578b\uff0c\u5e76\u5141\u8bb8\u7528\u6237\u6839\u636e\u9700\u8981\u538b\u7f29\u6a21\u578b\u5927\u5c0f\u6216\u7cbe\u5ea6\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u91cf\u5316\u6280\u672f\u4e3aLLM\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.03053", "pdf": "https://arxiv.org/pdf/2505.03053", "abs": "https://arxiv.org/abs/2505.03053", "authors": ["Jennifer Healey", "Laurie Byrum", "Md Nadeem Akhtar", "Surabhi Bhargava", "Moumita Sinha"], "title": "Developing A Framework to Support Human Evaluation of Bias in Generated Free Response Text", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, no figures, presented at CHI 2025 workshop for Human\n  Evaluation and Auditing of Language Models", "summary": "LLM evaluation is challenging even the case of base models. In real world\ndeployments, evaluation is further complicated by the interplay of task\nspecific prompts and experiential context. At scale, bias evaluation is often\nbased on short context, fixed choice benchmarks that can be rapidly evaluated,\nhowever, these can lose validity when the LLMs' deployed context differs. Large\nscale human evaluation is often seen as too intractable and costly. Here we\npresent our journey towards developing a semi-automated bias evaluation\nframework for free text responses that has human insights at its core. We\ndiscuss how we developed an operational definition of bias that helped us\nautomate our pipeline and a methodology for classifying bias beyond multiple\nchoice. We additionally comment on how human evaluation helped us uncover\nproblematic templates in a bias benchmark.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u534a\u81ea\u52a8\u5316\u7684\u504f\u89c1\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u7531\u6587\u672c\u56de\u7b54\uff0c\u6838\u5fc3\u662f\u7ed3\u5408\u4eba\u7c7b\u6d1e\u5bdf\u529b\uff0c\u4ee5\u89e3\u51b3LLM\u5728\u771f\u5b9e\u90e8\u7f72\u4e2d\u7684\u504f\u89c1\u8bc4\u4f30\u95ee\u9898\u3002", "motivation": "LLM\u8bc4\u4f30\u5728\u771f\u5b9e\u90e8\u7f72\u4e2d\u56e0\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u548c\u5b9e\u9a8c\u60c5\u5883\u7684\u4ea4\u4e92\u800c\u590d\u6742\u5316\uff0c\u73b0\u6709\u7684\u57fa\u4e8e\u77ed\u4e0a\u4e0b\u6587\u3001\u56fa\u5b9a\u9009\u62e9\u7684\u57fa\u51c6\u6d4b\u8bd5\u5728\u90e8\u7f72\u60c5\u5883\u4e0d\u540c\u65f6\u53ef\u80fd\u5931\u6548\uff0c\u800c\u5927\u89c4\u6a21\u4eba\u7c7b\u8bc4\u4f30\u53c8\u6210\u672c\u9ad8\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u534a\u81ea\u52a8\u5316\u7684\u504f\u89c1\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u64cd\u4f5c\u6027\u7684\u504f\u89c1\u5b9a\u4e49\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u5e76\u63d0\u51fa\u4e86\u8d85\u8d8a\u591a\u9879\u9009\u62e9\u7684\u504f\u89c1\u5206\u7c7b\u65b9\u6cd5\uff0c\u7ed3\u5408\u4eba\u7c7b\u8bc4\u4f30\u8bc6\u522b\u504f\u89c1\u57fa\u51c6\u4e2d\u7684\u95ee\u9898\u6a21\u677f\u3002", "result": "\u6846\u67b6\u7ed3\u5408\u4eba\u7c7b\u6d1e\u5bdf\u529b\uff0c\u6709\u6548\u8bc6\u522b\u548c\u5206\u7c7b\u504f\u89c1\uff0c\u6210\u529f\u53d1\u73b0\u504f\u89c1\u57fa\u51c6\u4e2d\u7684\u95ee\u9898\u6a21\u677f\u3002", "conclusion": "\u534a\u81ea\u52a8\u5316\u6846\u67b6\u7ed3\u5408\u4eba\u7c7b\u8bc4\u4f30\uff0c\u4e3aLLM\u7684\u81ea\u7531\u6587\u672c\u504f\u89c1\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u884c\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.03359", "pdf": "https://arxiv.org/pdf/2505.03359", "abs": "https://arxiv.org/abs/2505.03359", "authors": ["June-Woo Kim", "Haram Yoon", "Wonkyo Oh", "Dawoon Jung", "Sung-Hoon Yoon", "Dae-Jin Kim", "Dong-Ho Lee", "Sang-Yeol Lee", "Chan-Mo Yang"], "title": "Domain Adversarial Training for Mitigating Gender Bias in Speech-based Mental Health Detection", "categories": ["cs.AI"], "comment": "Accepted to EMBC 2025", "summary": "Speech-based AI models are emerging as powerful tools for detecting\ndepression and the presence of Post-traumatic stress disorder (PTSD), offering\na non-invasive and cost-effective way to assess mental health. However, these\nmodels often struggle with gender bias, which can lead to unfair and inaccurate\npredictions. In this study, our study addresses this issue by introducing a\ndomain adversarial training approach that explicitly considers gender\ndifferences in speech-based depression and PTSD detection. Specifically, we\ntreat different genders as distinct domains and integrate this information into\na pretrained speech foundation model. We then validate its effectiveness on the\nE-DAIC dataset to assess its impact on performance. Experimental results show\nthat our method notably improves detection performance, increasing the F1-score\nby up to 13.29 percentage points compared to the baseline. This highlights the\nimportance of addressing demographic disparities in AI-driven mental health\nassessment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9886\u57df\u5bf9\u6297\u8bad\u7ec3\u7684\u8bed\u97f3AI\u6a21\u578b\uff0c\u7528\u4e8e\u51cf\u5c11\u6027\u522b\u504f\u89c1\uff0c\u63d0\u5347\u6291\u90c1\u75c7\u548cPTSD\u68c0\u6d4b\u7684\u516c\u5e73\u6027\u4e0e\u51c6\u786e\u6027\uff0c\u5b9e\u9a8c\u663e\u793aF1\u5206\u6570\u63d0\u9ad8\u4e8613.29%\u3002", "motivation": "\u8bed\u97f3AI\u6a21\u578b\u5728\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u4e2d\u5b58\u5728\u6027\u522b\u504f\u89c1\uff0c\u5bfc\u81f4\u9884\u6d4b\u4e0d\u516c\u548c\u4e0d\u51c6\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u9886\u57df\u5bf9\u6297\u8bad\u7ec3\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06\u4e0d\u540c\u6027\u522b\u89c6\u4e3a\u72ec\u7acb\u9886\u57df\uff0c\u878d\u5165\u9884\u8bad\u7ec3\u7684\u8bed\u97f3\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u901a\u8fc7E-DAIC\u6570\u636e\u96c6\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\uff0cF1\u5206\u6570\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e8613.29\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728AI\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u4e2d\u89e3\u51b3\u4eba\u53e3\u7edf\u8ba1\u5b66\u5dee\u5f02\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.03042", "pdf": "https://arxiv.org/pdf/2505.03042", "abs": "https://arxiv.org/abs/2505.03042", "authors": ["Steven Tin Sui Luo"], "title": "A New Perspective To Understanding Multi-resolution Hash Encoding For Neural Fields", "categories": ["cs.LG"], "comment": null, "summary": "Instant-NGP has been the state-of-the-art architecture of neural fields in\nrecent years. Its incredible signal-fitting capabilities are generally\nattributed to its multi-resolution hash grid structure and have been used and\nimproved in numerous following works. However, it is unclear how and why such a\nhash grid structure improves the capabilities of a neural network by such great\nmargins. A lack of principled understanding of the hash grid also implies that\nthe large set of hyperparameters accompanying Instant-NGP could only be tuned\nempirically without much heuristics. To provide an intuitive explanation of the\nworking principle of the hash grid, we propose a novel perspective, namely\ndomain manipulation. This perspective provides a ground-up explanation of how\nthe feature grid learns the target signal and increases the expressivity of the\nneural field by artificially creating multiples of pre-existing linear\nsegments. We conducted numerous experiments on carefully constructed\n1-dimensional signals to support our claims empirically and aid our\nillustrations. While our analysis mainly focuses on 1-dimensional signals, we\nshow that the idea is generalizable to higher dimensions.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86Instant-NGP\u591a\u5206\u8fa8\u7387\u54c8\u5e0c\u7f51\u683c\u7ed3\u6784\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u2018\u57df\u64cd\u4f5c\u2019\u7684\u65b0\u89c6\u89d2\u6765\u89e3\u91ca\u5176\u5982\u4f55\u63d0\u5347\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u89c6\u89d2\u7684\u5408\u7406\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u9ad8\u7ef4\u4fe1\u53f7\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1Instant-NPG\u7684\u591a\u5206\u8fa8\u7387\u54c8\u5e0c\u7f51\u683c\u7ed3\u6784\u5728\u4fe1\u53f7\u62df\u5408\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5de5\u4f5c\u539f\u7406\u4ecd\u7f3a\u4e4f\u7406\u8bba\u652f\u6301\u3002\u7f3a\u4e4f\u5bf9\u8fd9\u4e00\u7ed3\u6784\u7684\u7406\u8bba\u7406\u89e3\u4f7f\u5f97\u5176\u8d85\u53c2\u6570\u53ea\u80fd\u901a\u8fc7\u7ecf\u9a8c\u8c03\u4f18\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u2018\u57df\u64cd\u4f5c\u2019\u7684\u65b0\u89c6\u89d2\u89e3\u91ca\u54c8\u5e0c\u7f51\u683c\u7684\u5de5\u4f5c\u539f\u7406\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u2018\u57df\u64cd\u4f5c\u2019\u89c6\u89d2\uff0c\u4ece\u5e95\u5c42\u89e3\u91ca\u54c8\u5e0c\u7f51\u683c\u5982\u4f55\u901a\u8fc7\u5b66\u4e60\u76ee\u6807\u4fe1\u53f7\u4ee5\u53ca\u4eba\u4e3a\u521b\u5efa\u5df2\u6709\u7ebf\u6bb5\u7684\u591a\u4e2a\u526f\u672c\u6765\u63d0\u5347\u795e\u7ecf\u573a\u7684\u8868\u8fbe\u80fd\u529b\u3002\u5e76\u901a\u8fc7\u5728\u4e00\u7ef4\u4fe1\u53f7\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u548c\u652f\u6301\u8fd9\u4e00\u7406\u8bba\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u2018\u57df\u64cd\u4f5c\u2019\u89c6\u89d2\u80fd\u591f\u76f4\u89c2\u89e3\u91ca\u591a\u5206\u8fa8\u7387\u54c8\u5e0c\u7f51\u683c\u7ed3\u6784\u7684\u5de5\u4f5c\u539f\u7406\u3002\u8fd9\u4e00\u89c6\u89d2\u4e0d\u4ec5\u5728\u4e00\u7ef4\u4fe1\u53f7\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u800c\u4e14\u5c55\u793a\u4e86\u5176\u5728\u9ad8\u7ef4\u4fe1\u53f7\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8bba\u6587\u4e3aInstant-NGP\u7684\u591a\u5206\u8fa8\u7387\u54c8\u5e0c\u7f51\u683c\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u63d0\u51fa\u2018\u57df\u64cd\u4f5c\u2019\u7684\u65b0\u89c6\u89d2\u80fd\u591f\u5e2e\u52a9\u7406\u89e3\u5176\u5de5\u4f5c\u539f\u7406\u548c\u63d0\u5347\u8868\u8fbe\u80fd\u529b\u7684\u673a\u5236\u3002\u8fd9\u4e3a\u8d85\u53c2\u6570\u7684\u9009\u62e9\u63d0\u4f9b\u4e86\u7406\u8bba\u542f\u53d1\u548c\u9ad8\u7ef4\u5e94\u7528\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2505.03059", "pdf": "https://arxiv.org/pdf/2505.03059", "abs": "https://arxiv.org/abs/2505.03059", "authors": ["Junlin Wang", "Roy Xie", "Shang Zhu", "Jue Wang", "Ben Athiwaratkun", "Bhuwan Dhingra", "Shuaiwen Leon Song", "Ce Zhang", "James Zou"], "title": "Improving Model Alignment Through Collective Intelligence of Open-Source LLMS", "categories": ["cs.CL"], "comment": "ICML 2025", "summary": "Building helpful and harmless large language models (LLMs) requires effective\nmodel alignment approach based on human instructions and feedback, which\nnecessitates high-quality human-labeled data. Constructing such datasets is\noften expensive and hard to scale, and may face potential limitations on\ndiversity and generalization. To address these challenges, we introduce Mixture\nof Agents Alignment (MoAA), that leverages the collective strengths of various\nlanguage models to provide high-quality data for model alignment. By employing\nMoAA, we enhance both supervised fine-tuning and preference optimization,\nleading to improved performance compared to using a single model alone to\ngenerate alignment data (e.g. using GPT-4o alone). Evaluation results show that\nour approach can improve win rate of LLaMA-3.1-8B-Instruct from 19.5 to 48.3 on\nArena-Hard and from 22.33 to 57.23 on AlpacaEval2, highlighting a promising\ndirection for model alignment through this new scalable and diverse synthetic\ndata recipe. Furthermore, we demonstrate that MoAA enables a self-improvement\npipeline, where models finetuned on MoA-generated data surpass their own\ninitial capabilities, providing evidence that our approach can push the\nfrontier of open-source LLMs without reliance on stronger external supervision.\nData and code will be released.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMoAA\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u591a\u79cd\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5bf9\u9f50\u6570\u636e\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u5728\u76d1\u7763\u5fae\u8c03\u548c\u504f\u597d\u4f18\u5316\u4e0a\u7684\u8868\u73b0\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LLaMA-3.1-8B-Instruct\u7684\u80dc\u7387\u3002", "motivation": "\u6784\u5efa\u57fa\u4e8e\u4eba\u7c7b\u6307\u4ee4\u548c\u53cd\u9988\u7684\u9ad8\u8d28\u91cf\u5bf9\u9f50\u6570\u636e\u901a\u5e38\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u4e14\u53ef\u80fd\u5728\u591a\u6837\u6027\u548c\u6cdb\u5316\u6027\u4e0a\u53d7\u9650\u3002MoAA\u65e8\u5728\u5229\u7528\u591a\u6a21\u578b\u534f\u4f5c\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "MoAA\u7ed3\u5408\u591a\u79cd\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u52bf\u751f\u6210\u5bf9\u9f50\u6570\u636e\uff0c\u7528\u4e8e\u76d1\u7763\u5fae\u8c03\u548c\u504f\u597d\u4f18\u5316\uff0c\u907f\u514d\u4e86\u5bf9\u5355\u4e00\u6a21\u578b\uff08\u5982GPT-4o\uff09\u7684\u4f9d\u8d56\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cMoAA\u5c06LLaMA-3.1-8B-Instruct\u5728Arena-Hard\u548cAlpacaEval2\u4e0a\u7684\u80dc\u7387\u5206\u522b\u4ece19.5\u63d0\u5347\u81f348.3\u548c\u4ece22.33\u63d0\u5347\u81f357.23\u3002", "conclusion": "MoAA\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u591a\u6837\u5316\u7684\u5408\u6210\u6570\u636e\u65b9\u6cd5\uff0c\u80fd\u81ea\u6211\u6539\u8fdb\u6a21\u578b\u6027\u80fd\uff0c\u63a8\u52a8\u4e86\u5f00\u6e90\u5927\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u51cf\u5c11\u5bf9\u5916\u90e8\u5f3a\u76d1\u7763\u7684\u4f9d\u8d56\u3002"}}
{"id": "2505.03369", "pdf": "https://arxiv.org/pdf/2505.03369", "abs": "https://arxiv.org/abs/2505.03369", "authors": ["Yuanyuan Yang", "Yuan Shen", "Tianchen Sun", "Yangbin Xie"], "title": "Validating the Effectiveness of a Large Language Model-based Approach for Identifying Children's Development across Various Free Play Settings in Kindergarten", "categories": ["cs.AI", "cs.CY"], "comment": "15 pages, 4 figures", "summary": "Free play is a fundamental aspect of early childhood education, supporting\nchildren's cognitive, social, emotional, and motor development. However,\nassessing children's development during free play poses significant challenges\ndue to the unstructured and spontaneous nature of the activity. Traditional\nassessment methods often rely on direct observations by teachers, parents, or\nresearchers, which may fail to capture comprehensive insights from free play\nand provide timely feedback to educators. This study proposes an innovative\napproach combining Large Language Models (LLMs) with learning analytics to\nanalyze children's self-narratives of their play experiences. The LLM\nidentifies developmental abilities, while performance scores across different\nplay settings are calculated using learning analytics techniques. We collected\n2,224 play narratives from 29 children in a kindergarten, covering four\ndistinct play areas over one semester. According to the evaluation results from\neight professionals, the LLM-based approach achieved high accuracy in\nidentifying cognitive, motor, and social abilities, with accuracy exceeding 90%\nin most domains. Moreover, significant differences in developmental outcomes\nwere observed across play settings, highlighting each area's unique\ncontributions to specific abilities. These findings confirm that the proposed\napproach is effective in identifying children's development across various free\nplay settings. This study demonstrates the potential of integrating LLMs and\nlearning analytics to provide child-centered insights into developmental\ntrajectories, offering educators valuable data to support personalized learning\nand enhance early childhood education practices.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u548c\u5b66\u4e60\u5206\u6790\u6280\u672f\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u513f\u7ae5\u81ea\u7531\u6e38\u620f\u4e2d\u7684\u81ea\u6211\u53d9\u8ff0\uff0c\u4ee5\u8bc4\u4f30\u5176\u53d1\u5c55\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u5728\u8bc6\u522b\u8ba4\u77e5\u3001\u8fd0\u52a8\u548c\u793e\u4ea4\u80fd\u529b\u65b9\u9762\u51c6\u786e\u7387\u8d85\u8fc790%\uff0c\u5e76\u63ed\u793a\u4e86\u4e0d\u540c\u6e38\u620f\u73af\u5883\u5bf9\u513f\u7ae5\u53d1\u5c55\u7684\u72ec\u7279\u8d21\u732e\u3002", "motivation": "\u81ea\u7531\u6e38\u620f\u5bf9\u513f\u7ae5\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u5168\u9762\u6355\u6349\u5176\u52a8\u6001\u548c\u975e\u7ed3\u6784\u5316\u7279\u70b9\u3002\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u63d0\u4f9b\u66f4\u5ba2\u89c2\u3001\u5373\u65f6\u7684\u8bc4\u4f30\uff0c\u4ee5\u652f\u6301\u4e2a\u6027\u5316\u6559\u80b2\u3002", "method": "\u7814\u7a76\u4f7f\u7528LLM\u5206\u6790\u513f\u7ae5\u7684\u6e38\u620f\u53d9\u8ff0\uff0c\u5e76\u7ed3\u5408\u5b66\u4e60\u5206\u6790\u6280\u672f\u8ba1\u7b97\u4e0d\u540c\u6e38\u620f\u73af\u5883\u4e0b\u7684\u8868\u73b0\u8bc4\u5206\u3002\u6570\u636e\u6765\u81ea29\u540d\u5e7c\u513f\u56ed\u513f\u7ae5\u76842,224\u6761\u6e38\u620f\u53d9\u8ff0\uff0c\u8986\u76d6\u4e00\u5b66\u671f\u7684\u56db\u4e2a\u6e38\u620f\u533a\u57df\u3002", "result": "LLM\u6a21\u578b\u5728\u591a\u6570\u9886\u57df\u7684\u8bc6\u522b\u51c6\u786e\u7387\u8d85\u8fc790%\uff0c\u4e14\u4e0d\u540c\u6e38\u620f\u73af\u5883\u5bf9\u7279\u5b9a\u80fd\u529b\u7684\u53d1\u5c55\u6709\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u8bc6\u522b\u513f\u7ae5\u5728\u591a\u6e38\u620f\u73af\u5883\u4e2d\u7684\u53d1\u5c55\uff0c\u5c55\u793a\u4e86LLM\u4e0e\u5b66\u4e60\u5206\u6790\u5728\u6559\u80b2\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u4e2a\u6027\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2505.03049", "pdf": "https://arxiv.org/pdf/2505.03049", "abs": "https://arxiv.org/abs/2505.03049", "authors": ["Yoel Zimmermann", "Adib Bazgir", "Alexander Al-Feghali", "Mehrad Ansari", "L. Catherine Brinson", "Yuan Chiang", "Defne Circi", "Min-Hsueh Chiu", "Nathan Daelman", "Matthew L. Evans", "Abhijeet S. Gangan", "Janine George", "Hassan Harb", "Ghazal Khalighinejad", "Sartaaj Takrim Khan", "Sascha Klawohn", "Magdalena Lederbauer", "Soroush Mahjoubi", "Bernadette Mohr", "Seyed Mohamad Moosavi", "Aakash Naik", "Aleyna Beste Ozhan", "Dieter Plessers", "Aritra Roy", "Fabian Sch\u00f6ppach", "Philippe Schwaller", "Carla Terboven", "Katharina Ueltzen", "Shang Zhu", "Jan Janssen", "Calvin Li", "Ian Foster", "Ben Blaiszik"], "title": "34 Examples of LLM Applications in Materials Science and Chemistry: Towards Automation, Assistants, Agents, and Accelerated Scientific Discovery", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "comment": "arXiv admin note: substantial text overlap with arXiv:2411.15221", "summary": "Large Language Models (LLMs) are reshaping many aspects of materials science\nand chemistry research, enabling advances in molecular property prediction,\nmaterials design, scientific automation, knowledge extraction, and more. Recent\ndevelopments demonstrate that the latest class of models are able to integrate\nstructured and unstructured data, assist in hypothesis generation, and\nstreamline research workflows. To explore the frontier of LLM capabilities\nacross the research lifecycle, we review applications of LLMs through 34 total\nprojects developed during the second annual Large Language Model Hackathon for\nApplications in Materials Science and Chemistry, a global hybrid event. These\nprojects spanned seven key research areas: (1) molecular and material property\nprediction, (2) molecular and material design, (3) automation and novel\ninterfaces, (4) scientific communication and education, (5) research data\nmanagement and automation, (6) hypothesis generation and evaluation, and (7)\nknowledge extraction and reasoning from the scientific literature.\nCollectively, these applications illustrate how LLMs serve as versatile\npredictive models, platforms for rapid prototyping of domain-specific tools,\nand much more. In particular, improvements in both open source and proprietary\nLLM performance through the addition of reasoning, additional training data,\nand new techniques have expanded effectiveness, particularly in low-data\nenvironments and interdisciplinary research. As LLMs continue to improve, their\nintegration into scientific workflows presents both new opportunities and new\nchallenges, requiring ongoing exploration, continued refinement, and further\nresearch to address reliability, interpretability, and reproducibility.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6b63\u5728\u6539\u53d8\u6750\u6599\u79d1\u5b66\u4e0e\u5316\u5b66\u7814\u7a76\u7684\u591a\u4e2a\u9886\u57df\uff0c\u6db5\u76d6\u5206\u5b50\u6027\u8d28\u9884\u6d4b\u3001\u6750\u6599\u8bbe\u8ba1\u7b49\uff0c\u5e76\u901a\u8fc734\u4e2a\u9879\u76ee\u5c55\u793a\u4e86\u5176\u591a\u529f\u80fd\u6027\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u79d1\u5b66\u7814\u7a76\u5168\u5468\u671f\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u6750\u6599\u79d1\u5b66\u4e0e\u5316\u5b66\u4e2d\u7684\u524d\u6cbf\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u7b2c\u4e8c\u5c4a\u5e74\u5ea6LLM Hackathon\u768434\u4e2a\u9879\u76ee\uff0c\u56f4\u7ed5\u4e03\u4e2a\u5173\u952e\u7814\u7a76\u9886\u57df\uff08\u5982\u5206\u5b50\u8bbe\u8ba1\u3001\u81ea\u52a8\u5316\u7b49\uff09\u8fdb\u884c\u7efc\u8ff0\u5206\u6790\u3002", "result": "LLMs\u5c55\u73b0\u4e86\u4f5c\u4e3a\u9884\u6d4b\u6a21\u578b\u548c\u9886\u57df\u5de5\u5177\u5feb\u901f\u539f\u578b\u5e73\u53f0\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u4f4e\u6570\u636e\u73af\u5883\u548c\u8de8\u5b66\u79d1\u7814\u7a76\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "LLMs\u7684\u6301\u7eed\u6539\u8fdb\u4e3a\u79d1\u5b66\u5de5\u4f5c\u6d41\u5e26\u6765\u65b0\u673a\u9047\uff0c\u4f46\u4e5f\u9700\u89e3\u51b3\u53ef\u9760\u6027\u3001\u53ef\u89e3\u91ca\u6027\u7b49\u6311\u6218\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2505.03229", "pdf": "https://arxiv.org/pdf/2505.03229", "abs": "https://arxiv.org/abs/2505.03229", "authors": ["Behrooz Mansouri"], "title": "Survey of Abstract Meaning Representation: Then, Now, Future", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a survey of Abstract Meaning Representation (AMR), a\nsemantic representation framework that captures the meaning of sentences\nthrough a graph-based structure. AMR represents sentences as rooted, directed\nacyclic graphs, where nodes correspond to concepts and edges denote\nrelationships, effectively encoding the meaning of complex sentences. This\nsurvey investigates AMR and its extensions, focusing on AMR capabilities. It\nthen explores the parsing (text-to-AMR) and generation (AMR-to-text) tasks by\nshowing traditional, current, and possible futures approaches. It also reviews\nvarious applications of AMR including text generation, text classification, and\ninformation extraction and information seeking. By analyzing recent\ndevelopments and challenges in the field, this survey provides insights into\nfuture directions for research and the potential impact of AMR on enhancing\nmachine understanding of human language.", "AI": {"tldr": "\u672c\u6587\u5bf9\u62bd\u8c61\u610f\u4e49\u8868\u793a\uff08AMR\uff09\u8fdb\u884c\u4e86\u7efc\u8ff0\uff0c\u63a2\u8ba8\u4e86\u5176\u56fe\u7ed3\u6784\u8868\u793a\u53e5\u610f\u7684\u80fd\u529b\uff0c\u89e3\u6790\u4e0e\u751f\u6210\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u4ee5\u53caAMR\u5728\u6587\u672c\u751f\u6210\u3001\u5206\u7c7b\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002", "motivation": "AMR\u4f5c\u4e3a\u4e00\u79cd\u56fe\u7ed3\u6784\u7684\u8bed\u4e49\u8868\u793a\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u7f16\u7801\u590d\u6742\u53e5\u5b50\u7684\u542b\u4e49\uff0c\u7814\u7a76\u5176\u53d1\u5c55\u548c\u5e94\u7528\u6709\u52a9\u4e8e\u63d0\u5347\u673a\u5668\u5bf9\u4eba\u7c7b\u8bed\u8a00\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u8c03\u67e5AMR\u53ca\u5176\u6269\u5c55\uff0c\u5206\u6790\u6587\u672c\u5230AMR\u89e3\u6790\u548cAMR\u5230\u6587\u672c\u751f\u6210\u7684\u4f20\u7edf\u3001\u5f53\u524d\u53ca\u672a\u6765\u53ef\u80fd\u7684\u65b9\u6cd5\uff0c\u5e76\u56de\u987eAMR\u5728\u591a\u4e2a\u9886\u57df\u7684\u5e94\u7528\u3002", "result": "\u7efc\u8ff0\u63ed\u793a\u4e86AMR\u7684\u6700\u65b0\u8fdb\u5c55\u548c\u6311\u6218\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6587\u672c\u5904\u7406\u548c\u4fe1\u606f\u63d0\u53d6\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "AMR\u7814\u7a76\u4e3a\u672a\u6765\u63d0\u5347\u673a\u5668\u8bed\u8a00\u7406\u89e3\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u5176\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u503c\u5f97\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2505.03434", "pdf": "https://arxiv.org/pdf/2505.03434", "abs": "https://arxiv.org/abs/2505.03434", "authors": ["Schaun Wheeler", "Olivier Jeunen"], "title": "Procedural Memory Is Not All You Need: Bridging Cognitive Gaps in LLM-Based Agents", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted to the workshop on Hybrid AI for Human-Centric\n  Personalization (HyPer), co-located with ACM UMAP '25", "summary": "Large Language Models (LLMs) represent a landmark achievement in Artificial\nIntelligence (AI), demonstrating unprecedented proficiency in procedural tasks\nsuch as text generation, code completion, and conversational coherence. These\ncapabilities stem from their architecture, which mirrors human procedural\nmemory -- the brain's ability to automate repetitive, pattern-driven tasks\nthrough practice. However, as LLMs are increasingly deployed in real-world\napplications, it becomes impossible to ignore their limitations operating in\ncomplex, unpredictable environments. This paper argues that LLMs, while\ntransformative, are fundamentally constrained by their reliance on procedural\nmemory. To create agents capable of navigating ``wicked'' learning environments\n-- where rules shift, feedback is ambiguous, and novelty is the norm -- we must\naugment LLMs with semantic memory and associative learning systems. By adopting\na modular architecture that decouples these cognitive functions, we can bridge\nthe gap between narrow procedural expertise and the adaptive intelligence\nrequired for real-world problem-solving.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f9d\u8d56\u7a0b\u5e8f\u6027\u8bb0\u5fc6\uff0c\u5728\u590d\u6742\u591a\u53d8\u7684\u73af\u5883\u4e2d\u8868\u73b0\u53d7\u9650\uff0c\u63d0\u51fa\u901a\u8fc7\u6a21\u5757\u5316\u67b6\u6784\u589e\u5f3a\u8bed\u4e49\u8bb0\u5fc6\u548c\u8054\u60f3\u5b66\u4e60\u7cfb\u7edf\u4ee5\u63d0\u5347\u9002\u5e94\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3LLMs\u5728\u771f\u5b9e\u590d\u6742\u73af\u5883\u4e2d\u56e0\u4f9d\u8d56\u7a0b\u5e8f\u6027\u8bb0\u5fc6\u800c\u8868\u73b0\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u5c06\u8bed\u4e49\u8bb0\u5fc6\u548c\u8054\u60f3\u5b66\u4e60\u7cfb\u7edf\u4e0eLLMs\u89e3\u8026\uff0c\u4ee5\u589e\u5f3a\u5176\u9002\u5e94\u6027\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u589e\u5f3a\u540e\u7684\u7cfb\u7edf\u80fd\u66f4\u597d\u5730\u5e94\u5bf9\u89c4\u5219\u591a\u53d8\u3001\u53cd\u9988\u6a21\u7cca\u7684\u590d\u6742\u73af\u5883\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u591a\u79cd\u8ba4\u77e5\u529f\u80fd\uff0cLLMs\u53ef\u7a81\u7834\u7a0b\u5e8f\u6027\u8bb0\u5fc6\u7684\u5c40\u9650\uff0c\u5b9e\u73b0\u66f4\u63a5\u8fd1\u4eba\u7c7b\u667a\u80fd\u7684\u9002\u5e94\u80fd\u529b\u3002"}}
{"id": "2505.03084", "pdf": "https://arxiv.org/pdf/2505.03084", "abs": "https://arxiv.org/abs/2505.03084", "authors": ["Shashank Kapoor", "Sanjay Surendranath Girija", "Lakshit Arora", "Dipen Pradhan", "Ankit Shetgaonkar", "Aman Raj"], "title": "Adversarial Attacks in Multimodal Systems: A Practitioner's Survey", "categories": ["cs.LG"], "comment": "Accepted in IEEE COMPSAC 2025", "summary": "The introduction of multimodal models is a huge step forward in Artificial\nIntelligence. A single model is trained to understand multiple modalities:\ntext, image, video, and audio. Open-source multimodal models have made these\nbreakthroughs more accessible. However, considering the vast landscape of\nadversarial attacks across these modalities, these models also inherit\nvulnerabilities of all the modalities, and ultimately, the adversarial threat\namplifies. While broad research is available on possible attacks within or\nacross these modalities, a practitioner-focused view that outlines attack types\nremains absent in the multimodal world. As more Machine Learning Practitioners\nadopt, fine-tune, and deploy open-source models in real-world applications,\nit's crucial that they can view the threat landscape and take the preventive\nactions necessary. This paper addresses the gap by surveying adversarial\nattacks targeting all four modalities: text, image, video, and audio. This\nsurvey provides a view of the adversarial attack landscape and presents how\nmultimodal adversarial threats have evolved. To the best of our knowledge, this\nsurvey is the first comprehensive summarization of the threat landscape in the\nmultimodal world.", "AI": {"tldr": "\u8bba\u6587\u63d0\u4f9b\u4e86\u5bf9\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u5bf9\u6297\u6027\u653b\u51fb\u7684\u9996\u4e2a\u5168\u9762\u7efc\u8ff0\uff0c\u6db5\u76d6\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u548c\u97f3\u9891\u56db\u79cd\u6a21\u6001\uff0c\u586b\u8865\u4e86\u5b9e\u8df5\u5e94\u7528\u89c6\u89d2\u7684\u7a7a\u767d\u3002", "motivation": "\u591a\u6a21\u6001\u6a21\u578b\u867d\u4e3aAI\u5e26\u6765\u7a81\u7834\uff0c\u4f46\u5176\u8de8\u6a21\u6001\u7684\u5bf9\u6297\u6027\u653b\u51fb\u5a01\u80c1\u88ab\u653e\u5927\uff0c\u800c\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u9762\u5411\u5b9e\u8df5\u8005\u7684\u653b\u51fb\u7c7b\u578b\u603b\u7ed3\u3002", "method": "\u901a\u8fc7\u8c03\u67e5\u9488\u5bf9\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u548c\u97f3\u9891\u56db\u79cd\u6a21\u6001\u7684\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u68b3\u7406\u653b\u51fb\u7c7b\u578b\u53ca\u5176\u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\u7684\u6f14\u53d8\u3002", "result": "\u9996\u6b21\u5168\u9762\u603b\u7ed3\u4e86\u591a\u6a21\u6001\u4e16\u754c\u7684\u5bf9\u6297\u6027\u5a01\u80c1\uff0c\u63d0\u4f9b\u4e86\u653b\u51fb\u89c6\u89d2\u7684\u5b9e\u8df5\u6307\u5357\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u591a\u6a21\u6001\u5bf9\u6297\u6027\u653b\u51fb\u7684\u5b9e\u8df5\u7a7a\u767d\uff0c\u4e3a\u5f00\u53d1\u8005\u5728\u90e8\u7f72\u6a21\u578b\u65f6\u63d0\u4f9b\u9632\u5fa1\u4f9d\u636e\u3002"}}
{"id": "2505.03293", "pdf": "https://arxiv.org/pdf/2505.03293", "abs": "https://arxiv.org/abs/2505.03293", "authors": ["Shijing Zhu", "Zhuang Chen", "Guanqun Bi", "Binghang Li", "Yaxi Deng", "Dazhen Wan", "Libiao Peng", "Xiyao Xiao", "Rongsheng Zhang", "Tangjie Lv", "Zhipeng Hu", "FangFang Li", "Minlie Huang"], "title": "\u03a8-Arena: Interactive Assessment and Optimization of LLM-based Psychological Counselors with Tripartite Feedback", "categories": ["cs.CL"], "comment": "in progress", "summary": "Large language models (LLMs) have shown promise in providing scalable mental\nhealth support, while evaluating their counseling capability remains crucial to\nensure both efficacy and safety. Existing evaluations are limited by the static\nassessment that focuses on knowledge tests, the single perspective that centers\non user experience, and the open-loop framework that lacks actionable feedback.\nTo address these issues, we propose {\\Psi}-Arena, an interactive framework for\ncomprehensive assessment and optimization of LLM-based counselors, featuring\nthree key characteristics: (1) Realistic arena interactions that simulate\nreal-world counseling through multi-stage dialogues with psychologically\nprofiled NPC clients, (2) Tripartite evaluation that integrates assessments\nfrom the client, counselor, and supervisor perspectives, and (3) Closed-loop\noptimization that iteratively improves LLM counselors using diagnostic\nfeedback. Experiments across eight state-of-the-art LLMs show significant\nperformance variations in different real-world scenarios and evaluation\nperspectives. Moreover, reflection-based optimization results in up to a 141%\nimprovement in counseling performance. We hope PsychoArena provides a\nfoundational resource for advancing reliable and human-aligned LLM applications\nin mental healthcare.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa \u03a8-Arena\uff0c\u4e00\u79cd\u4e92\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u4f18\u5316\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5fc3\u7406\u54a8\u8be2\u5e08\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u8bc4\u4f30\u548c\u95ed\u73af\u4f18\u5316\u63d0\u5347\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u54a8\u8be2\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5bf9LLM\u5fc3\u7406\u54a8\u8be2\u80fd\u529b\u7684\u8bc4\u4f30\u5c40\u9650\u9759\u6001\u6d4b\u8bd5\u3001\u5355\u4e00\u89c6\u89d2\u548c\u5f00\u653e\u5f0f\u6846\u67b6\uff0c\u7f3a\u4e4f\u5b9e\u9645\u53ef\u884c\u53cd\u9988\u3002", "method": "\u63d0\u51fa\u03a8-Arena\u6846\u67b6\uff0c\u5305\u542b\u591a\u9636\u6bb5\u6a21\u62df\u5bf9\u8bdd\u3001\u4e09\u65b9\u8bc4\u4f30\uff08\u5ba2\u6237\u3001\u54a8\u8be2\u5e08\u548c\u4e3b\u7ba1\uff09\u53ca\u57fa\u4e8e\u53cd\u9988\u7684\u95ed\u73af\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4e0d\u540cLLM\u5728\u771f\u5b9e\u573a\u666f\u4e0b\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u4f18\u5316\u540e\u54a8\u8be2\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe141%\u3002", "conclusion": "\u03a8-Arena\u4e3a\u63a8\u8fdb\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u53ef\u9760\u4e14\u4eba\u6027\u5316\u7684LLM\u5e94\u7528\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\u3002"}}
{"id": "2505.03439", "pdf": "https://arxiv.org/pdf/2505.03439", "abs": "https://arxiv.org/abs/2505.03439", "authors": ["Artem Karpov", "Tinuade Adeleke", "Seong Hah Cho", "Natalia Perez-Campanero"], "title": "The Steganographic Potentials of Language Models", "categories": ["cs.AI", "cs.CR", "cs.LG"], "comment": "Published at Building Trust Workshop at ICLR 2025", "summary": "The potential for large language models (LLMs) to hide messages within plain\ntext (steganography) poses a challenge to detection and thwarting of unaligned\nAI agents, and undermines faithfulness of LLMs reasoning. We explore the\nsteganographic capabilities of LLMs fine-tuned via reinforcement learning (RL)\nto: (1) develop covert encoding schemes, (2) engage in steganography when\nprompted, and (3) utilize steganography in realistic scenarios where hidden\nreasoning is likely, but not prompted. In these scenarios, we detect the\nintention of LLMs to hide their reasoning as well as their steganography\nperformance. Our findings in the fine-tuning experiments as well as in\nbehavioral non fine-tuning evaluations reveal that while current models exhibit\nrudimentary steganographic abilities in terms of security and capacity,\nexplicit algorithmic guidance markedly enhances their capacity for information\nconcealment.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u540e\u9690\u85cf\u4fe1\u606f\uff08\u9690\u5199\uff09\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u5b89\u5168\u548c\u5bb9\u91cf\u4e0a\u8868\u73b0\u51fa\u521d\u6b65\u9690\u5199\u80fd\u529b\uff0c\u800c\u660e\u786e\u7684\u7b97\u6cd5\u6307\u5bfc\u663e\u8457\u63d0\u5347\u4e86\u5176\u4fe1\u606f\u9690\u85cf\u80fd\u529b\u3002", "motivation": "\u63a2\u8ba8LLMs\u901a\u8fc7\u9690\u5199\u6280\u672f\u9690\u85cf\u4fe1\u606f\u7684\u6f5c\u529b\u53ca\u5176\u5bf9\u672a\u5bf9\u9f50AI\u4ee3\u7406\u68c0\u6d4b\u548cLLMs\u63a8\u7406\u5fe0\u5b9e\u6027\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03LLMs\uff0c\u5f00\u53d1\u9690\u853d\u7f16\u7801\u65b9\u6848\uff0c\u5e76\u5728\u63d0\u793a\u548c\u672a\u63d0\u793a\u7684 realistic \u573a\u666f\u4e2d\u6d4b\u8bd5\u5176\u9690\u5199\u80fd\u529b\u3002", "result": "\u5f53\u524d\u6a21\u578b\u5728\u9690\u5199\u7684\u5b89\u5168\u548c\u5bb9\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u521d\u6b65\u80fd\u529b\uff0c\u800c\u7b97\u6cd5\u6307\u5bfc\u663e\u8457\u63d0\u5347\u4e86\u5176\u4fe1\u606f\u9690\u85cf\u6548\u679c\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0LLMs\u5177\u5907\u521d\u6b65\u9690\u5199\u80fd\u529b\uff0c\u4e14\u901a\u8fc7\u7b97\u6cd5\u6307\u5bfc\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5176\u4fe1\u606f\u9690\u85cf\u6027\u80fd\uff0c\u8fd9\u5bf9AI\u5b89\u5168\u548cLLMs\u7684\u53ef\u9760\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2505.03109", "pdf": "https://arxiv.org/pdf/2505.03109", "abs": "https://arxiv.org/abs/2505.03109", "authors": ["Lutfu Sua", "Haibo Wang", "Jun Huang"], "title": "Deep Learning in Renewable Energy Forecasting: A Cross-Dataset Evaluation of Temporal and Spatial Models", "categories": ["cs.LG", "econ.GN", "q-fin.EC"], "comment": "34 pages, 16 figures", "summary": "Unpredictability of renewable energy sources coupled with the complexity of\nthose methods used for various purposes in this area calls for the development\nof robust methods such as DL models within the renewable energy domain. Given\nthe nonlinear relationships among variables in renewable energy datasets, DL\nmodels are preferred over traditional machine learning (ML) models because they\ncan effectively capture and model complex interactions between variables. This\nresearch aims to identify the factors responsible for the accuracy of DL\ntechniques, such as sampling, stationarity, linearity, and hyperparameter\noptimization for different algorithms. The proposed DL framework compares\nvarious methods and alternative training/test ratios. Seven ML methods, such as\nLong-Short Term Memory (LSTM), Stacked LSTM, Convolutional Neural Network\n(CNN), CNN-LSTM, Deep Neural Network (DNN), Multilayer Perceptron (MLP), and\nEncoder-Decoder (ED), were evaluated on two different datasets. The first\ndataset contains the weather and power generation data. It encompasses two\ndistinct datasets, hourly energy demand data and hourly weather data in Spain,\nwhile the second dataset includes power output generated by the photovoltaic\npanels at 12 locations. This study deploys regularization approaches, including\nearly stopping, neuron dropping, and L2 regularization, to reduce the\noverfitting problem associated with DL models. The LSTM and MLP models show\nsuperior performance. Their validation data exhibit exceptionally low root mean\nsquare error values.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u53ef\u518d\u751f\u80fd\u6e90\u9886\u57df\u4e2d\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u6a21\u578b\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u6bd4\u8f83\u4e86\u591a\u79cdDL\u548c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u65b9\u6cd5\u5728\u4e24\u79cd\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u6700\u7ec8\u53d1\u73b0LSTM\u548cMLP\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u53ef\u518d\u751f\u80fd\u6e90\u6570\u636e\u5177\u6709\u975e\u7ebf\u6027\u548c\u590d\u6742\u6027\uff0c\u4f20\u7edfML\u6a21\u578b\u96be\u4ee5\u6709\u6548\u6355\u6349\u53d8\u91cf\u95f4\u7684\u590d\u6742\u5173\u7cfb\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76DL\u6a21\u578b\u5728\u8fd9\u4e00\u9886\u57df\u7684\u9002\u7528\u6027\u548c\u51c6\u786e\u6027\u5f71\u54cd\u56e0\u7d20\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u4e03\u79cd\u4e0d\u540c\u7684DL\u548cML\u65b9\u6cd5\uff08\u5305\u62ecLSTM\u3001CNN\u7b49\uff09\uff0c\u5e76\u4f7f\u7528\u4e24\u79cd\u6570\u636e\u96c6\uff08\u5929\u6c14\u4e0e\u53d1\u7535\u6570\u636e\u3001\u5149\u4f0f\u9762\u677f\u8f93\u51fa\u6570\u636e\uff09\uff0c\u7ed3\u5408\u6b63\u5219\u5316\u6280\u672f\uff08\u5982\u65e9\u505c\u3001\u795e\u7ecf\u5143\u4e22\u5f03\u7b49\uff09\u6765\u51cf\u5c11\u8fc7\u62df\u5408\u95ee\u9898\u3002", "result": "LSTM\u548cMLP\u6a21\u578b\u8868\u73b0\u6700\u4f18\uff0c\u5176\u9a8c\u8bc1\u6570\u636e\u7684\u5747\u65b9\u6839\u8bef\u5dee\u975e\u5e38\u4f4e\u3002", "conclusion": "DL\u6a21\u578b\uff08\u5c24\u5176\u662fLSTM\u548cMLP\uff09\u5728\u53ef\u518d\u751f\u80fd\u6e90\u6570\u636e\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9002\u5408\u7528\u4e8e\u5904\u7406\u590d\u6742\u7684\u975e\u7ebf\u6027\u5173\u7cfb\u3002"}}
{"id": "2505.03320", "pdf": "https://arxiv.org/pdf/2505.03320", "abs": "https://arxiv.org/abs/2505.03320", "authors": ["Junyu Ma", "Tianqing Fang", "Zhisong Zhang", "Hongming Zhang", "Haitao Mi", "Dong Yu"], "title": "Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation", "categories": ["cs.CL"], "comment": null, "summary": "Mamba's theoretical infinite-context potential is limited in practice when\nsequences far exceed training lengths. This work explores unlocking Mamba's\nlong-context memory ability by a simple-yet-effective method, Recall with\nReasoning (RwR), by distilling chain-of-thought (CoT) summarization from a\nteacher model. Specifically, RwR prepends these summarization as CoT prompts\nduring fine-tuning, teaching Mamba to actively recall and reason over long\ncontexts. Experiments on LONGMEMEVAL and HELMET show RwR boosts Mamba's\nlong-context performance against comparable Transformer/hybrid baselines under\nsimilar pretraining conditions, while preserving short-context capabilities,\nall without architectural changes.", "AI": {"tldr": "\u901a\u8fc7\u300e\u56de\u5fc6\u4e0e\u63a8\u7406\u300f\uff08RwR\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u6559\u5e08\u6a21\u578b\u7684\u601d\u7ef4\u94fe\u6458\u8981\uff0c\u63d0\u5347Mamba\u6a21\u578b\u7684\u957f\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u80fd\u529b\uff0c\u65e0\u9700\u67b6\u6784\u6539\u52a8\u5373\u53ef\u8d85\u8d8aTransformer/\u6df7\u5408\u6a21\u578b\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3Mamba\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u957f\u5e8f\u5217\u5904\u7406\u80fd\u529b\u7684\u9650\u5236\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u5fae\u8c03\u65b9\u6cd5\u63d0\u5347\u5176\u957f\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u80fd\u529b\u3002", "method": "\u5728\u5fae\u8c03\u9636\u6bb5\uff0c\u901a\u8fc7\u5728\u8f93\u5165\u524d\u6dfb\u52a0\u6559\u5e08\u6a21\u578b\u751f\u6210\u7684\u601d\u7ef4\u94fe\u6458\u8981\u4f5c\u4e3a\u63d0\u793a\uff0c\u6559\u5bfcMamba\u4e3b\u52a8\u56de\u5fc6\u548c\u63a8\u7406\u957f\u4e0a\u4e0b\u6587\u3002", "result": "\u5728LONGMEMEVAL\u548cHELMET\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRwR\u663e\u8457\u63d0\u5347\u4e86Mamba\u7684\u957f\u4e0a\u4e0b\u6587\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u77ed\u4e0a\u4e0b\u6587\u80fd\u529b\uff0c\u4e14\u672a\u6539\u53d8\u6a21\u578b\u67b6\u6784\u3002", "conclusion": "RwR\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u6539\u52a8\u6a21\u578b\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347Mamba\u7684\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b\uff0c\u4e3a\u7c7b\u4f3c\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2505.03475", "pdf": "https://arxiv.org/pdf/2505.03475", "abs": "https://arxiv.org/abs/2505.03475", "authors": ["Zirui Liu", "Jiatong Li", "Yan Zhuang", "Qi Liu", "Shuanghong Shen", "Jie Ouyang", "Mingyue Cheng", "Shijin Wang"], "title": "am-ELO: A Stable Framework for Arena-based LLM Evaluation", "categories": ["cs.AI", "cs.LG"], "comment": "ICML2025 Accepted", "summary": "Arena-based evaluation is a fundamental yet significant evaluation paradigm\nfor modern AI models, especially large language models (LLMs). Existing\nframework based on ELO rating system suffers from the inevitable instability\nproblem due to ranking inconsistency and the lack of attention to the varying\nabilities of annotators. In this paper, we introduce a novel stable arena\nframework to address these issues by enhancing the ELO Rating System.\nSpecifically, we replace the iterative update method with a Maximum Likelihood\nEstimation (MLE) approach, m-ELO, and provide theoretical proof of the\nconsistency and stability of the MLE approach for model ranking. Additionally,\nwe proposed the am-ELO, which modify the Elo Rating's probability function to\nincorporate annotator abilities, enabling the simultaneous estimation of model\nscores and annotator reliability. Experiments demonstrate that this method\nensures stability, proving that this framework offers a more robust, accurate,\nand stable evaluation method for LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7a33\u5b9a\u7ade\u6280\u573a\u6846\u67b6m-ELO\u548cam-ELO\uff0c\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u548c\u8003\u8651\u6807\u6ce8\u8005\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709ELO\u8bc4\u5206\u7cfb\u7edf\u7684\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u7684\u7a33\u5065\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eELO\u8bc4\u5206\u7cfb\u7edf\u7684\u7ade\u6280\u573a\u8bc4\u4f30\u6846\u67b6\u5b58\u5728\u6392\u540d\u4e0d\u4e00\u81f4\u5bfc\u81f4\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e14\u672a\u8003\u8651\u6807\u6ce8\u8005\u80fd\u529b\u7684\u5dee\u5f02\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u6539\u8fdbELO\u7cfb\u7edf\uff0c\u63d0\u51fa\u66f4\u7a33\u5b9a\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fam-ELO\uff0c\u7528\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u66ff\u4ee3\u8fed\u4ee3\u66f4\u65b0\u65b9\u6cd5\uff0c\u5e76\u7406\u8bba\u8bc1\u660e\u5176\u4e00\u81f4\u6027\u548c\u7a33\u5b9a\u6027\uff1b\u8fdb\u4e00\u6b65\u63d0\u51faam-ELO\uff0c\u4fee\u6539ELO\u7684\u6982\u7387\u51fd\u6570\u4ee5\u7eb3\u5165\u6807\u6ce8\u8005\u80fd\u529b\uff0c\u540c\u65f6\u4f30\u8ba1\u6a21\u578b\u5f97\u5206\u548c\u6807\u6ce8\u8005\u53ef\u9760\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u786e\u4fdd\u7a33\u5b9a\u6027\uff0c\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u3001\u51c6\u786e\u548c\u7a33\u5b9a\u7684LLMs\u8bc4\u4f30\u6846\u67b6\u3002", "conclusion": "m-ELO\u548cam-ELO\u901a\u8fc7\u7406\u8bba\u6539\u8fdb\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ade\u6280\u573a\u8bc4\u4f30\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3aLLMs\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.03112", "pdf": "https://arxiv.org/pdf/2505.03112", "abs": "https://arxiv.org/abs/2505.03112", "authors": ["Mohammad Rostami", "Atik Faysal", "Reihaneh Gh. Roshan", "Huaxia Wang", "Nikhil Muralidhar", "Yu-Dong Yao"], "title": "Plug-and-Play AMC: Context Is King in Training-Free, Open-Set Modulation with LLMs", "categories": ["cs.LG"], "comment": null, "summary": "Automatic Modulation Classification (AMC) is critical for efficient spectrum\nmanagement and robust wireless communications. However, AMC remains challenging\ndue to the complex interplay of signal interference and noise. In this work, we\npropose an innovative framework that integrates traditional signal processing\ntechniques with Large-Language Models (LLMs) to address AMC. Our approach\nleverages higher-order statistics and cumulant estimation to convert\nquantitative signal features into structured natural language prompts. By\nincorporating exemplar contexts into these prompts, our method exploits the\nLLM's inherent familiarity with classical signal processing, enabling effective\none-shot classification without additional training or preprocessing (e.g.,\ndenoising). Experimental evaluations on synthetically generated datasets,\nspanning both noiseless and noisy conditions, demonstrate that our framework\nachieves competitive performance across diverse modulation schemes and\nSignal-to-Noise Ratios (SNRs). Moreover, our approach paves the way for robust\nfoundation models in wireless communications across varying channel conditions,\nsignificantly reducing the expense associated with developing channel-specific\nmodels. This work lays the foundation for scalable, interpretable, and\nversatile signal classification systems in next-generation wireless networks.\nThe source code is available at https://github.com/RU-SIT/context-is-king", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u81ea\u52a8\u8c03\u5236\u5206\u7c7b\uff08AMC\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u4f20\u7edf\u4fe1\u53f7\u5904\u7406\u6280\u672f\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u901a\u8fc7\u9ad8\u9636\u7edf\u8ba1\u548c\u7d2f\u79ef\u91cf\u4f30\u8ba1\u5c06\u4fe1\u53f7\u7279\u5f81\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u81ea\u7136\u8bed\u8a00\u63d0\u793a\uff0c\u5b9e\u73b0\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u9884\u5904\u7406\u7684\u5355\u6b21\u5206\u7c7b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "AMC\u5728\u9891\u8c31\u7ba1\u7406\u548c\u65e0\u7ebf\u901a\u4fe1\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4fe1\u53f7\u5e72\u6270\u548c\u566a\u58f0\u7684\u590d\u6742\u6027\u4f7f\u5176\u4ecd\u5177\u6311\u6218\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u7ed3\u5408\u4f20\u7edf\u4fe1\u53f7\u5904\u7406\u548cLLMs\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u51cf\u5c11\u5f00\u53d1\u7279\u5b9a\u4fe1\u9053\u6a21\u578b\u7684\u6210\u672c\uff0c\u5e76\u63d0\u5347\u5206\u7c7b\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u5229\u7528\u9ad8\u9636\u7edf\u8ba1\u548c\u7d2f\u79ef\u91cf\u4f30\u8ba1\u63d0\u53d6\u4fe1\u53f7\u7279\u5f81\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u81ea\u7136\u8bed\u8a00\u63d0\u793a\uff0c\u7ed3\u5408LLMs\u5bf9\u4f20\u7edf\u4fe1\u53f7\u5904\u7406\u7684\u56fa\u6709\u77e5\u8bc6\uff0c\u5b9e\u73b0\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u5355\u6b21\u5206\u7c7b\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\uff08\u5305\u62ec\u65e0\u566a\u58f0\u548c\u6709\u566a\u58f0\u6761\u4ef6\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u8c03\u5236\u65b9\u6848\u548c\u4fe1\u566a\u6bd4\uff08SNR\uff09\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u4e14\u901a\u7528\u7684\u4fe1\u53f7\u5206\u7c7b\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4fe1\u9053\u7279\u5b9a\u6a21\u578b\u7684\u5f00\u53d1\u6210\u672c\u3002"}}
{"id": "2505.03406", "pdf": "https://arxiv.org/pdf/2505.03406", "abs": "https://arxiv.org/abs/2505.03406", "authors": ["Mohammad Shoaib Ansari", "Mohd Sohail Ali Khan", "Shubham Revankar", "Aditya Varma", "Anil S. Mokhade"], "title": "Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs and Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages", "summary": "This research paper investigates the application of Large Language Models\n(LLMs) in healthcare, specifically focusing on enhancing medical decision\nsupport through Retrieval-Augmented Generation (RAG) integrated with\nhospital-specific data and fine-tuning using Quantized Low-Rank Adaptation\n(QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. By\nembedding and retrieving context-relevant healthcare information, the system\nsignificantly improves response accuracy. QLoRA facilitates notable parameter\nefficiency and memory optimization, preserving the integrity of medical\ninformation through specialized quantization techniques. Our research also\nshows that our model performs relatively well on various medical benchmarks,\nindicating that it can be used to make basic medical suggestions. This paper\ndetails the system's technical components, including its architecture,\nquantization methods, and key healthcare applications such as enhanced disease\nprediction from patient symptoms and medical history, treatment suggestions,\nand efficient summarization of complex medical reports. We touch on the ethical\nconsiderations-patient privacy, data security, and the need for rigorous\nclinical validation-as well as the practical challenges of integrating such\nsystems into real-world healthcare workflows. Furthermore, the lightweight\nquantized weights ensure scalability and ease of deployment even in\nlow-resource hospital environments. Finally, the paper concludes with an\nanalysis of the broader impact of LLMs on healthcare and outlines future\ndirections for LLMs in medical settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u662f\u4f7f\u7528RAG\u548cQLoRA\u6280\u672f\u589e\u5f3a\u533b\u7597\u51b3\u7b56\u652f\u6301\uff0c\u5e76\u901a\u8fc7\u91cf\u5316\u4f4e\u79e9\u9002\u914d\u4f18\u5316\u6a21\u578b\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u63a2\u7d22LLMs\u5982\u4f55\u6574\u5408\u533b\u9662\u7279\u5b9a\u6570\u636e\u4ee5\u63d0\u5347\u533b\u7597\u51b3\u7b56\u652f\u6301\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\uff0c\u540c\u65f6\u89e3\u51b3\u53c2\u6570\u6548\u7387\u548c\u5b58\u50a8\u4f18\u5316\u7684\u6311\u6218\u3002", "method": "\u57fa\u4e8eLlama 3.2-3B-Instruct\u6a21\u578b\uff0c\u7ed3\u5408RAG\u548cQLoRA\u6280\u672f\uff0c\u5d4c\u5165\u548c\u68c0\u7d22\u533b\u7597\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u91cf\u5316\u6280\u672f\u4f18\u5316\u6a21\u578b\u6548\u7387\u3002", "result": "\u6a21\u578b\u5728\u591a\u4e2a\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u80fd\u63d0\u4f9b\u57fa\u672c\u533b\u7597\u5efa\u8bae\uff0c\u5e76\u5728\u75be\u75c5\u9884\u6d4b\u3001\u6cbb\u7597\u5efa\u8bae\u548c\u62a5\u544a\u6458\u8981\u7b49\u5e94\u7528\u4e2d\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660eLLMs\u5728\u533b\u7597\u9886\u57df\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u4f26\u7406\u548c\u6280\u672f\u6311\u6218\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u90e8\u7f72\u5230\u5b9e\u9645\u4e34\u5e8a\u73af\u5883\u4e2d\u3002"}}
{"id": "2505.03547", "pdf": "https://arxiv.org/pdf/2505.03547", "abs": "https://arxiv.org/abs/2505.03547", "authors": ["Eric Zhou", "Shreyas Basavatia", "Moontashir Siam", "Zexin Chen", "Mark O. Riedl"], "title": "STORY2GAME: Generating (Almost) Everything in an Interactive Fiction Game", "categories": ["cs.AI"], "comment": null, "summary": "We introduce STORY2GAME, a novel approach to using Large Language Models to\ngenerate text-based interactive fiction games that starts by generating a\nstory, populates the world, and builds the code for actions in a game engine\nthat enables the story to play out interactively. Whereas a given set of\nhard-coded actions can artificially constrain story generation, the ability to\ngenerate actions means the story generation process can be more open-ended but\nstill allow for experiences that are grounded in a game state. The key to\nsuccessful action generation is to use LLM-generated preconditions and effects\nof actions in the stories as guides for what aspects of the game state must be\ntracked and changed by the game engine when a player performs an action. We\nalso introduce a technique for dynamically generating new actions to\naccommodate the player's desire to perform actions that they think of that are\nnot part of the story. Dynamic action generation may require on-the-fly updates\nto the game engine's state representation and revision of previously generated\nactions. We evaluate the success rate of action code generation with respect to\nwhether a player can interactively play through the entire generated story.", "AI": {"tldr": "STORY2GAME\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u57fa\u4e8e\u6587\u672c\u7684\u4ea4\u4e92\u5f0f\u5c0f\u8bf4\u6e38\u620f\uff0c\u901a\u8fc7\u751f\u6210\u6545\u4e8b\u3001\u586b\u5145\u4e16\u754c\u5e76\u6784\u5efa\u6e38\u620f\u5f15\u64ce\u4e2d\u7684\u52a8\u4f5c\u4ee3\u7801\uff0c\u4f7f\u6545\u4e8b\u80fd\u591f\u4e92\u52a8\u5c55\u5f00\u3002\u52a8\u6001\u751f\u6210\u52a8\u4f5c\u548c\u6e38\u620f\u72b6\u6001\u66f4\u65b0\u662f\u6838\u5fc3\u521b\u65b0\u3002", "motivation": "\u4f20\u7edf\u786c\u7f16\u7801\u52a8\u4f5c\u53ef\u80fd\u9650\u5236\u6545\u4e8b\u751f\u6210\u7684\u5f00\u653e\u6027\uff0c\u800c\u52a8\u6001\u751f\u6210\u52a8\u4f5c\u548c\u6e38\u620f\u72b6\u6001\u8ddf\u8e2a\u80fd\u63d0\u4f9b\u66f4\u81ea\u7531\u7684\u4e92\u52a8\u4f53\u9a8c\u3002", "method": "\u5229\u7528LLM\u751f\u6210\u52a8\u4f5c\u7684\u524d\u7f6e\u6761\u4ef6\u548c\u6548\u679c\uff0c\u52a8\u6001\u751f\u6210\u65b0\u52a8\u4f5c\u4ee5\u9002\u5e94\u73a9\u5bb6\u884c\u4e3a\uff0c\u5e76\u5b9e\u65f6\u66f4\u65b0\u6e38\u620f\u5f15\u64ce\u72b6\u6001\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u52a8\u4f5c\u4ee3\u7801\u751f\u6210\u7684\u6210\u529f\u7387\u53d6\u51b3\u4e8e\u73a9\u5bb6\u662f\u5426\u80fd\u5b8c\u6574\u4e92\u52a8\u4f53\u9a8c\u751f\u6210\u7684\u6545\u4e8b\u3002", "conclusion": "STORY2GAME\u901a\u8fc7\u52a8\u6001\u52a8\u4f5c\u751f\u6210\u548c\u72b6\u6001\u7ba1\u7406\uff0c\u5b9e\u73b0\u4e86\u66f4\u5f00\u653e\u7684\u4e92\u52a8\u6545\u4e8b\u751f\u6210\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u53d9\u4e8b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.03118", "pdf": "https://arxiv.org/pdf/2505.03118", "abs": "https://arxiv.org/abs/2505.03118", "authors": ["Dmytro Shamatrin"], "title": "Adaptive Thresholding for Multi-Label Classification via Global-Local Signal Fusion", "categories": ["cs.LG"], "comment": null, "summary": "Multi-label classification (MLC) requires predicting multiple labels per\nsample, often under heavy class imbalance and noisy conditions. Traditional\napproaches apply fixed thresholds or treat labels independently, overlooking\ncontext and global rarity. We introduce an adaptive thresholding mechanism that\nfuses global (IDF-based) and local (KNN-based) signals to produce per-label,\nper-instance thresholds. Instead of applying these as hard cutoffs, we treat\nthem as differentiable penalties in the loss, providing smooth supervision and\nbetter calibration. Our architecture is lightweight, interpretable, and highly\nmodular. On the AmazonCat-13K benchmark, it achieves a macro-F1 of 0.1712,\nsubstantially outperforming tree-based and pretrained transformer-based\nmethods. We release full code for reproducibility and future extensions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u9608\u503c\u673a\u5236\uff0c\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u53f7\uff0c\u7528\u4e8e\u591a\u6807\u7b7e\u5206\u7c7b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u591a\u6807\u7b7e\u5206\u7c7b\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u9608\u503c\u6216\u72ec\u7acb\u5904\u7406\u6807\u7b7e\uff0c\u5ffd\u7565\u4e86\u4e0a\u4e0b\u6587\u548c\u5168\u5c40\u7a00\u758f\u6027\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7075\u6d3b\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u9608\u503c\u673a\u5236\uff0c\u878d\u5408\u5168\u5c40\uff08\u57fa\u4e8eIDF\uff09\u548c\u5c40\u90e8\uff08\u57fa\u4e8eKNN\uff09\u4fe1\u53f7\uff0c\u751f\u6210\u6bcf\u4e2a\u6807\u7b7e\u548c\u5b9e\u4f8b\u7684\u9608\u503c\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u53ef\u5fae\u635f\u5931\u60e9\u7f5a\u3002", "result": "\u5728AmazonCat-13K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b8fF1\u8fbe\u52300.1712\uff0c\u4f18\u4e8e\u57fa\u4e8e\u6811\u548c\u9884\u8bad\u7ec3Transformer\u7684\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u8f7b\u91cf\u3001\u53ef\u89e3\u91ca\u4e14\u6a21\u5757\u5316\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u4ee5\u4fbf\u590d\u73b0\u548c\u6269\u5c55\u3002"}}
{"id": "2505.03427", "pdf": "https://arxiv.org/pdf/2505.03427", "abs": "https://arxiv.org/abs/2505.03427", "authors": ["Mouath Abu Daoud", "Chaimae Abouzahir", "Leen Kharouf", "Walid Al-Eisawi", "Nizar Habash", "Farah E. Shamout"], "title": "MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "21 pages", "summary": "Large Language Models (LLMs) have demonstrated significant promise for\nvarious applications in healthcare. However, their efficacy in the Arabic\nmedical domain remains unexplored due to the lack of high-quality\ndomain-specific datasets and benchmarks. This study introduces MedArabiQ, a\nnovel benchmark dataset consisting of seven Arabic medical tasks, covering\nmultiple specialties and including multiple choice questions,\nfill-in-the-blank, and patient-doctor question answering. We first constructed\nthe dataset using past medical exams and publicly available datasets. We then\nintroduced different modifications to evaluate various LLM capabilities,\nincluding bias mitigation. We conducted an extensive evaluation with five\nstate-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude\n3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of\nnew high-quality benchmarks that span different languages to ensure fair\ndeployment and scalability of LLMs in healthcare. By establishing this\nbenchmark and releasing the dataset, we provide a foundation for future\nresearch aimed at evaluating and enhancing the multilingual capabilities of\nLLMs for the equitable use of generative AI in healthcare.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86MedArabiQ\u2014\u2014\u4e00\u4e2a\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u9886\u57df\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u79cd\u533b\u7597\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u9886\u57df\u7f3a\u4e4f\u76f8\u5173\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u963b\u788d\u4e86LLMs\u7684\u516c\u5e73\u90e8\u7f72\u548c\u591a\u8bed\u8a00\u6269\u5c55\u3002\u8be5\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u6574\u5408\u533b\u5b66\u8003\u8bd5\u548c\u516c\u5f00\u6570\u636e\u96c6\u6784\u5efa\u4e86\u5305\u542b\u4e03\u79cd\u533b\u7597\u4efb\u52a1\u7684MedArabiQ\uff0c\u5e76\u5f15\u5165\u591a\u79cd\u4fee\u6539\u4ee5\u8bc4\u4f30LLMs\u6027\u80fd\u4e0e\u504f\u5dee\u7f13\u89e3\u3002\u5bf9\u4e94\u79cd\u524d\u6cbfLLMs\uff08\u5982GPT-4o\uff09\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u63ed\u793a\u4e86LLMs\u5728\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u591a\u8bed\u8a00\u9ad8\u8d28\u91cf\u57fa\u51c6\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u786e\u4fddLLMs\u5728\u533b\u7597\u9886\u57df\u7684\u516c\u5e73\u5e94\u7528\u3002", "conclusion": "MedArabiQ\u7684\u53d1\u5e03\u4e3a\u589e\u5f3aLLMs\u591a\u8bed\u8a00\u80fd\u529b\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u751f\u6210\u5f0fAI\u5728\u533b\u7597\u9886\u57df\u7684\u516c\u5e73\u4f7f\u7528\u4e0e\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2505.03553", "pdf": "https://arxiv.org/pdf/2505.03553", "abs": "https://arxiv.org/abs/2505.03553", "authors": ["Kolawole E. Ogunsina", "Morayo A. Ogunsina"], "title": "A Hashgraph-Inspired Consensus Mechanism for Reliable Multi-Model Reasoning", "categories": ["cs.AI", "cs.DC"], "comment": "15 pages", "summary": "Inconsistent outputs and hallucinations from large language models (LLMs) are\nmajor obstacles to reliable AI systems. When different proprietary reasoning\nmodels (RMs), such as those by OpenAI, Google, Anthropic, DeepSeek, and xAI,\nare given the same complex request, they often produce divergent results due to\nvariations in training and inference. This paper proposes a novel consensus\nmechanism, inspired by distributed ledger technology, to validate and converge\nthese outputs, treating each RM as a black-box peer. Building on the Hashgraph\nconsensus algorithm, our approach employs gossip-about-gossip communication and\nvirtual voting to achieve agreement among an ensemble of RMs. We present an\narchitectural design for a prototype system in which RMs iteratively exchange\nand update their answers, using information from each round to improve accuracy\nand confidence in subsequent rounds. This approach goes beyond simple majority\nvoting by incorporating the knowledge and cross-verification content of every\nmodel. We justify the feasibility of this Hashgraph-inspired consensus for AI\nensembles and outline its advantages over traditional ensembling techniques in\nreducing nonfactual outputs. Preliminary considerations for implementation,\nevaluation criteria for convergence and accuracy, and potential challenges are\ndiscussed. The proposed mechanism demonstrates a promising direction for\nmulti-agent AI systems to self-validate and deliver high-fidelity responses in\ncomplex tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5171\u8bc6\u673a\u5236\uff0c\u53d7\u5206\u5e03\u5f0f\u8d26\u672c\u6280\u672f\u542f\u53d1\uff0c\u901a\u8fc7\u9ed1\u7bb1\u540c\u884c\u7684\u65b9\u5f0f\u9a8c\u8bc1\u548c\u6536\u655b\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u4ee5\u51cf\u5c11\u4e0d\u4e00\u81f4\u548c\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e0d\u4e00\u81f4\u8f93\u51fa\u548c\u5e7b\u89c9\u65b9\u9762\u5b58\u5728\u663e\u8457\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u53ef\u9760AI\u7cfb\u7edf\u7684\u53d1\u5c55\u3002\u4e0d\u540c\u4e13\u6709\u63a8\u7406\u6a21\u578b\uff08RMs\uff09\u5728\u76f8\u540c\u590d\u6742\u8bf7\u6c42\u4e0b\u53ef\u80fd\u4ea7\u751f\u5dee\u5f02\u5de8\u5927\u7684\u7ed3\u679c\uff0c\u5f71\u54cd\u4e86\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u8bba\u6587\u57fa\u4e8eHashgraph\u5171\u8bc6\u7b97\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u5171\u8bc6\u673a\u5236\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u2018gossip-about-gossip\u2019\u901a\u4fe1\u548c\u865a\u62df\u6295\u7968\u5728\u591a\u4e2a\u63a8\u7406\u6a21\u578b\u95f4\u8fbe\u6210\u4e00\u81f4\u3002\u6bcf\u4e2a\u6a21\u578b\u5728\u8fed\u4ee3\u4ea4\u6362\u548c\u66f4\u65b0\u5176\u7b54\u6848\u7684\u8fc7\u7a0b\u4e2d\uff0c\u9010\u6b65\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u7f6e\u4fe1\u5ea6\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u91c7\u7528\u591a\u6570\u8868\u51b3\uff0c\u8fd8\u7ed3\u5408\u4e86\u6bcf\u4e2a\u6a21\u578b\u7684\u77e5\u8bc6\u548c\u4ea4\u53c9\u9a8c\u8bc1\u5185\u5bb9\u3002", "result": "\u8be5\u65b9\u6cd5\u5c55\u73b0\u51fa\u5728\u51cf\u5c11\u4e0d\u51c6\u786e\u8f93\u51fa\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u96c6\u6210\u6280\u672f\u3002\u4f5c\u8005\u6784\u5efa\u4e86\u539f\u578b\u7cfb\u7edf\u67b6\u6784\uff0c\u521d\u6b65\u8bc4\u4f30\u8868\u660e\u5728\u6536\u655b\u6027\u548c\u51c6\u786e\u6027\u4e0a\u7684\u6f5c\u529b\uff0c\u4f46\u4e5f\u63d0\u51fa\u4e86\u5b9e\u65bd\u7684\u6311\u6218\u3002", "conclusion": "\u8be5\u5171\u8bc6\u673a\u5236\u4e3a\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u9a8c\u8bc1\u624b\u6bb5\uff0c\u80fd\u591f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u63d0\u4f9b\u9ad8\u4fdd\u771f\u5ea6\u7684\u5e94\u7b54\uff0c\u5177\u6709\u91cd\u8981\u7684\u7814\u7a76\u524d\u666f\u548c\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.03155", "pdf": "https://arxiv.org/pdf/2505.03155", "abs": "https://arxiv.org/abs/2505.03155", "authors": ["Max Qiushi Lin", "Jincheng Mei", "Matin Aghaei", "Michael Lu", "Bo Dai", "Alekh Agarwal", "Dale Schuurmans", "Csaba Szepesvari", "Sharan Vaswani"], "title": "Rethinking the Global Convergence of Softmax Policy Gradient with Linear Function Approximation", "categories": ["cs.LG"], "comment": "75 pages", "summary": "Policy gradient (PG) methods have played an essential role in the empirical\nsuccesses of reinforcement learning. In order to handle large state-action\nspaces, PG methods are typically used with function approximation. In this\nsetting, the approximation error in modeling problem-dependent quantities is a\nkey notion for characterizing the global convergence of PG methods. We focus on\nSoftmax PG with linear function approximation (referred to as\n$\\texttt{Lin-SPG}$) and demonstrate that the approximation error is irrelevant\nto the algorithm's global convergence even for the stochastic bandit setting.\nConsequently, we first identify the necessary and sufficient conditions on the\nfeature representation that can guarantee the asymptotic global convergence of\n$\\texttt{Lin-SPG}$. Under these feature conditions, we prove that $T$\niterations of $\\texttt{Lin-SPG}$ with a problem-specific learning rate result\nin an $O(1/T)$ convergence to the optimal policy. Furthermore, we prove that\n$\\texttt{Lin-SPG}$ with any arbitrary constant learning rate can ensure\nasymptotic global convergence to the optimal policy.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5e26\u7ebf\u6027\u51fd\u6570\u8fd1\u4f3c\u7684Softmax\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff08Lin-SPG\uff09\uff0c\u8bc1\u660e\u8fd1\u4f3c\u8bef\u5dee\u4e0d\u5f71\u54cd\u7b97\u6cd5\u7684\u5168\u5c40\u6536\u655b\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4fdd\u8bc1\u5168\u5c40\u6536\u655b\u7684\u7279\u5f81\u8868\u793a\u6761\u4ef6\uff0c\u540c\u65f6\u8fd8\u8bc1\u660e\u4e86\u65e0\u8bba\u5b66\u4e60\u7387\u5982\u4f55\uff0cLin-SPG\u90fd\u53ef\u4ee5\u6e10\u8fdb\u6536\u655b\u5230\u6700\u4f18\u7b56\u7565\u3002", "motivation": "\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u529f\uff0c\u4f46\u5728\u5904\u7406\u5927\u7684\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u65f6\uff0c\u901a\u5e38\u9700\u8981\u7ed3\u5408\u51fd\u6570\u8fd1\u4f3c\u6280\u672f\u3002\u76ee\u524d\uff0c\u8fd1\u4f3c\u8bef\u5dee\u5bf9\u5168\u5c40\u6536\u655b\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22Lin-SPG\u65b9\u6cd5\u7684\u6536\u655b\u6027\u53ca\u5176\u8fd1\u4f3c\u8bef\u5dee\u7684\u5f71\u54cd\u3002", "method": "\u672c\u6587\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u8bc6\u522b\u51fa\u4fdd\u8bc1Lin-SPG\u65b9\u6cd5\u5168\u5c40\u6536\u655b\u7684\u5fc5\u8981\u4e0e\u5145\u5206\u7279\u5f81\u8868\u793a\u6761\u4ef6\uff0c\u5e76\u8fdb\u4e00\u6b65\u8003\u5bdf\u4e86\u95ee\u9898\u7279\u5b9a\u5b66\u4e60\u7387\u53ca\u4efb\u610f\u6052\u5b9a\u5b66\u4e60\u7387\u6761\u4ef6\u4e0b\u7684\u6536\u655b\u884c\u4e3a\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1) \u8fd1\u4f3c\u8bef\u5dee\u4e0d\u5f71\u54cdLin-SPG\u7684\u5168\u5c40\u6536\u655b\u6027\uff1b2) \u5728\u7279\u5b9a\u7279\u5f81\u6761\u4ef6\u4e0b\uff0cT\u6b21\u8fed\u4ee3\u53ef\u5b9e\u73b0O(1/T)\u7684\u6700\u4f18\u7b56\u7565\u6536\u655b\uff1b3) \u4efb\u610f\u6052\u5b9a\u5b66\u4e60\u7387\u4e0b\uff0cLin-SPG\u5747\u53ef\u6e10\u8fdb\u6536\u655b\u5230\u6700\u4f18\u7b56\u7565\u3002", "conclusion": "\u7814\u7a76\u4e3aLin-SPG\u7684\u5168\u5c40\u6536\u655b\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u969c\uff0c\u62d3\u5c55\u4e86\u5bf9\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7a33\u5b9a\u6027\u7684\u7406\u89e3\uff0c\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2505.03452", "pdf": "https://arxiv.org/pdf/2505.03452", "abs": "https://arxiv.org/abs/2505.03452", "authors": ["Matan Orbach", "Ohad Eytan", "Benjamin Sznajder", "Ariel Gera", "Odellia Boni", "Yoav Kantor", "Gal Bloch", "Omri Levy", "Hadas Abraham", "Nitzan Barzilay", "Eyal Shnarch", "Michael E. Factor", "Shila Ofek-Koifman", "Paula Ta-Shma", "Assaf Toledo"], "title": "An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Finding the optimal Retrieval-Augmented Generation (RAG) configuration for a\ngiven use case can be complex and expensive. Motivated by this challenge,\nframeworks for RAG hyper-parameter optimization (HPO) have recently emerged,\nyet their effectiveness has not been rigorously benchmarked. To address this\ngap, we present a comprehensive study involving 5 HPO algorithms over 5\ndatasets from diverse domains, including a new one collected for this work on\nreal-world product documentation. Our study explores the largest HPO search\nspace considered to date, with two optimized evaluation metrics. Analysis of\nthe results shows that RAG HPO can be done efficiently, either greedily or with\niterative random search, and that it significantly boosts RAG performance for\nall datasets. For greedy HPO approaches, we show that optimizing models first\nis preferable to the prevalent practice of optimizing sequentially according to\nthe RAG pipeline order.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u8d2a\u5a6a\u6216\u8fed\u4ee3\u968f\u673a\u641c\u7d22\u65b9\u6cd5\u53ef\u4ee5\u9ad8\u6548\u4f18\u5316RAG\u914d\u7f6e\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5efa\u8bae\u4f18\u5148\u4f18\u5316\u6a21\u578b\u800c\u975e\u6309RAG\u6d41\u7a0b\u987a\u5e8f\u4f18\u5316\u3002", "motivation": "\u5f53\u524d\u4e3a\u7279\u5b9a\u7528\u4f8b\u627e\u5230\u6700\u4f18\u7684RAG\u914d\u7f6e\u65e2\u590d\u6742\u53c8\u6602\u8d35\uff0c\u867d\u6709HPO\u6846\u67b6\u4f46\u7f3a\u4e4f\u4e25\u683c\u8bc4\u4f30\uff0c\u56e0\u6b64\u9700\u7cfb\u7edf\u7814\u7a76\u5176\u6709\u6548\u6027\u3002", "method": "\u4f7f\u75285\u79cdHPO\u7b97\u6cd5\u57285\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\uff08\u5305\u62ec\u65b0\u6536\u96c6\u7684\u771f\u5b9e\u4ea7\u54c1\u6587\u6863\u6570\u636e\u96c6\uff09\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u7814\u7a76\uff0c\u63a2\u7d22\u4e86\u8fc4\u4eca\u6700\u5927\u7684HPO\u641c\u7d22\u7a7a\u95f4\u5e76\u4f18\u5316\u4e86\u4e24\u4e2a\u8bc4\u4ef7\u6307\u6807\u3002", "result": "RAG\u7684HPO\u53ef\u4ee5\u901a\u8fc7\u8d2a\u5a6a\u6216\u8fed\u4ee3\u968f\u673a\u641c\u7d22\u9ad8\u6548\u5b8c\u6210\uff0c\u4e14\u5bf9\u6240\u6709\u6570\u636e\u96c6\u5747\u663e\u8457\u63d0\u5347\u6027\u80fd\uff1b\u8d2a\u5a6a\u65b9\u6cd5\u4e2d\u4f18\u5148\u4f18\u5316\u6a21\u578b\u4f18\u4e8e\u6309RAG\u6d41\u7a0b\u987a\u5e8f\u4f18\u5316\u3002", "conclusion": "\u7814\u7a76\u8868\u660eRAG\u914d\u7f6e\u4f18\u5316\u5177\u6709\u9ad8\u6548\u6027\u548c\u5e7f\u6cdb\u63d0\u5347\u4f5c\u7528\uff0c\u63a8\u8350\u5728HPO\u65f6\u4f18\u5148\u4f18\u5316\u6a21\u578b\u800c\u975e\u9075\u5faa\u4f20\u7edf\u6d41\u7a0b\u987a\u5e8f\u3002"}}
{"id": "2505.03570", "pdf": "https://arxiv.org/pdf/2505.03570", "abs": "https://arxiv.org/abs/2505.03570", "authors": ["Mariya Davydova", "Daniel Jeffries", "Patrick Barker", "Arturo M\u00e1rquez Flores", "Sin\u00e9ad Ryan"], "title": "OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents", "categories": ["cs.AI"], "comment": null, "summary": "In this paper, we introduce OSUniverse: a benchmark of complex, multimodal\ndesktop-oriented tasks for advanced GUI-navigation AI agents that focuses on\nease of use, extensibility, comprehensive coverage of test cases, and automated\nvalidation. We divide the tasks in increasing levels of complexity, from basic\nprecision clicking to multistep, multiapplication tests requiring dexterity,\nprecision, and clear thinking from the agent. In version one of the benchmark,\npresented here, we have calibrated the complexity of the benchmark test cases\nto ensure that the SOTA (State of the Art) agents (at the time of publication)\ndo not achieve results higher than 50%, while the average white collar worker\ncan perform all these tasks with perfect accuracy. The benchmark can be scored\nmanually, but we also introduce an automated validation mechanism that has an\naverage error rate less than 2%. Therefore, this benchmark presents solid\nground for fully automated measuring of progress, capabilities and the\neffectiveness of GUI-navigation AI agents over the short and medium-term\nhorizon. The source code of the benchmark is available at\nhttps://github.com/agentsea/osuniverse.", "AI": {"tldr": "OSUniverse\u662f\u4e00\u4e2a\u4e3a\u9ad8\u7ea7GUI\u5bfc\u822aAI\u4ee3\u7406\u8bbe\u8ba1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8e\u6613\u7528\u6027\u3001\u53ef\u6269\u5c55\u6027\u3001\u5168\u9762\u8986\u76d6\u6d4b\u8bd5\u7528\u4f8b\u548c\u81ea\u52a8\u5316\u9a8c\u8bc1\u3002\u5176\u4efb\u52a1\u590d\u6742\u5ea6\u4ece\u57fa\u7840\u70b9\u51fb\u5230\u591a\u6b65\u9aa4\u8de8\u5e94\u7528\u6d4b\u8bd5\u9012\u589e\uff0c\u5f53\u524d\u7248\u672c\u786e\u4fddSOTA\u4ee3\u7406\u6210\u529f\u7387\u4e0d\u8d85\u8fc750%\uff0c\u800c\u666e\u901a\u767d\u9886\u53ef\u5b8c\u7f8e\u5b8c\u6210\u3002\u57fa\u51c6\u6d4b\u8bd5\u652f\u6301\u624b\u52a8\u548c\u81ea\u52a8\u5316\u8bc4\u5206\uff08\u8bef\u5dee\u7387<2%\uff09\uff0c\u4e3a\u8861\u91cfAI\u4ee3\u7406GUI\u5bfc\u822a\u80fd\u529b\u63d0\u4f9b\u53ef\u9760\u6807\u51c6\u3002", "motivation": "\u4e3a\u5148\u8fdbGUI\u5bfc\u822aAI\u4ee3\u7406\u521b\u5efa\u4e00\u4e2a\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\uff0c\u586b\u8865\u73b0\u6709\u57fa\u51c6\u5728\u590d\u6742\u5ea6\u3001\u8986\u76d6\u8303\u56f4\u548c\u81ea\u52a8\u5316\u9a8c\u8bc1\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4ee5\u51c6\u786e\u8861\u91cf\u4ee3\u7406\u7684\u80fd\u529b\u548c\u8fdb\u5c55\u3002", "method": "\u4efb\u52a1\u6309\u590d\u6742\u5ea6\u5206\u5c42\u8bbe\u8ba1\uff08\u4ece\u57fa\u7840\u70b9\u51fb\u5230\u8de8\u5e94\u7528\u591a\u6b65\u9aa4\u4efb\u52a1\uff09\uff0c\u5e76\u5f15\u5165\u81ea\u52a8\u5316\u9a8c\u8bc1\u673a\u5236\uff08\u8bef\u5dee\u7387<2%\uff09\u4ee5\u786e\u4fdd\u8bc4\u5206\u53ef\u9760\u6027\u3002\u4efb\u52a1\u96be\u5ea6\u6821\u51c6\u81f3SOTA\u4ee3\u7406\u6210\u529f\u7387\u226450%\uff0c\u666e\u901a\u4eba\u7c7b\u53ef\u5b8c\u7f8e\u5b8c\u6210\u3002", "result": "\u5f53\u524d\u57fa\u51c6\u7248\u672c\u663e\u793aSOTA\u4ee3\u7406\u4efb\u52a1\u5b8c\u6210\u7387\u4e0d\u8db350%\uff0c\u81ea\u52a8\u5316\u9a8c\u8bc1\u8bef\u5dee\u7387\u4f4e\u4e8e2%\uff0c\u6709\u6548\u533a\u5206AI\u4e0e\u4eba\u7c7b\u80fd\u529b\u5dee\u8ddd\u3002", "conclusion": "OSUniverse\u4e3aGUI\u5bfc\u822aAI\u4ee3\u7406\u7684\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u3001\u81ea\u52a8\u5316\u4e14\u53ef\u9760\u7684\u57fa\u51c6\uff0c\u652f\u6301\u77ed\u671f\u548c\u4e2d\u671f\u6280\u672f\u8fdb\u5c55\u7684\u91cf\u5316\u8861\u91cf\u3002"}}
{"id": "2505.03165", "pdf": "https://arxiv.org/pdf/2505.03165", "abs": "https://arxiv.org/abs/2505.03165", "authors": ["Nikita Ravi", "Abhinav Goel", "James C. Davis", "George K. Thiruvathukal"], "title": "Improving the Reproducibility of Deep Learning Software: An Initial Investigation through a Case Study Analysis", "categories": ["cs.LG", "cs.SE"], "comment": null, "summary": "The field of deep learning has witnessed significant breakthroughs, spanning\nvarious applications, and fundamentally transforming current software\ncapabilities. However, alongside these advancements, there have been increasing\nconcerns about reproducing the results of these deep learning methods. This is\nsignificant because reproducibility is the foundation of reliability and\nvalidity in software development, particularly in the rapidly evolving domain\nof deep learning. The difficulty of reproducibility may arise due to several\nreasons, including having differences from the original execution environment,\nincompatible software libraries, proprietary data and source code, lack of\ntransparency, and the stochastic nature in some software. A study conducted by\nthe Nature journal reveals that more than 70% of researchers failed to\nreproduce other researchers experiments and over 50% failed to reproduce their\nown experiments. Irreproducibility of deep learning poses significant\nchallenges for researchers and practitioners. To address these concerns, this\npaper presents a systematic approach at analyzing and improving the\nreproducibility of deep learning models by demonstrating these guidelines using\na case study. We illustrate the patterns and anti-patterns involved with these\nguidelines for improving the reproducibility of deep learning models. These\nguidelines encompass establishing a methodology to replicate the original\nsoftware environment, implementing end-to-end training and testing algorithms,\ndisclosing architectural designs, and enhancing transparency in data processing\nand training pipelines. We also conduct a sensitivity analysis to understand\nthe model performance across diverse conditions. By implementing these\nstrategies, we aim to bridge the gap between research and practice, so that\ninnovations in deep learning can be effectively reproduced and deployed within\nsoftware.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u65b9\u6cd5\u6765\u5206\u6790\u548c\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u53ef\u590d\u73b0\u6027\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u76f8\u5173\u6307\u5bfc\u539f\u5219\uff0c\u5305\u62ec\u73af\u5883\u590d\u5236\u3001\u7b97\u6cd5\u5b9e\u73b0\u3001\u8bbe\u8ba1\u900f\u660e\u5316\u7b49\uff0c\u5e76\u8fdb\u884c\u4e86\u654f\u611f\u6027\u5206\u6790\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u867d\u53d6\u5f97\u91cd\u5927\u7a81\u7834\uff0c\u4f46\u7ed3\u679c\u7684\u53ef\u590d\u73b0\u6027\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u5f71\u54cd\u7814\u7a76\u7684\u53ef\u9760\u6027\u548c\u6709\u6548\u6027\u3002\u8d85\u8fc770%\u7684\u7814\u7a76\u8005\u65e0\u6cd5\u590d\u73b0\u4ed6\u4eba\u5b9e\u9a8c\uff0c50%\u65e0\u6cd5\u590d\u73b0\u81ea\u5df1\u7684\u5b9e\u9a8c\uff0c\u4e9f\u9700\u89e3\u51b3\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u5957\u6307\u5bfc\u539f\u5219\uff0c\u5982\u590d\u5236\u539f\u59cb\u8f6f\u4ef6\u73af\u5883\u3001\u5b9e\u73b0\u7aef\u5230\u7aef\u7b97\u6cd5\u3001\u516c\u5f00\u67b6\u6784\u8bbe\u8ba1\u3001\u589e\u5f3a\u6570\u636e\u5904\u7406\u548c\u8bad\u7ec3\u7ba1\u9053\u7684\u900f\u660e\u5ea6\uff0c\u5e76\u8fdb\u884c\u654f\u611f\u6027\u5206\u6790\u3002", "result": "\u901a\u8fc7\u5b9e\u65bd\u8fd9\u4e9b\u7b56\u7565\uff0c\u8bba\u6587\u5c55\u793a\u4e86\u5982\u4f55\u6709\u6548\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u53ef\u590d\u73b0\u6027\uff0c\u5f25\u5408\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7cfb\u7edf\u6027\u65b9\u6cd5\u548c\u6307\u5bfc\u539f\u5219\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u53ef\u590d\u73b0\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u7814\u7a76\u7ed3\u679c\u7684\u53ef\u9760\u90e8\u7f72\u548c\u5e94\u7528\u3002"}}
{"id": "2505.03467", "pdf": "https://arxiv.org/pdf/2505.03467", "abs": "https://arxiv.org/abs/2505.03467", "authors": ["Shuang Zhou", "Jiashuo Wang", "Zidu Xu", "Song Wang", "David Brauer", "Lindsay Welton", "Jacob Cogan", "Yuen-Hei Chung", "Lei Tian", "Zaifu Zhan", "Yu Hou", "Mingquan Lin", "Genevieve B. Melton", "Rui Zhang"], "title": "Uncertainty-Aware Large Language Models for Explainable Disease Diagnosis", "categories": ["cs.CL"], "comment": "22 pages, 8 figures", "summary": "Explainable disease diagnosis, which leverages patient information (e.g.,\nsigns and symptoms) and computational models to generate probable diagnoses and\nreasonings, offers clear clinical values. However, when clinical notes\nencompass insufficient evidence for a definite diagnosis, such as the absence\nof definitive symptoms, diagnostic uncertainty usually arises, increasing the\nrisk of misdiagnosis and adverse outcomes. Although explicitly identifying and\nexplaining diagnostic uncertainties is essential for trustworthy diagnostic\nsystems, it remains under-explored. To fill this gap, we introduce ConfiDx, an\nuncertainty-aware large language model (LLM) created by fine-tuning open-source\nLLMs with diagnostic criteria. We formalized the task and assembled richly\nannotated datasets that capture varying degrees of diagnostic ambiguity.\nEvaluating ConfiDx on real-world datasets demonstrated that it excelled in\nidentifying diagnostic uncertainties, achieving superior diagnostic\nperformance, and generating trustworthy explanations for diagnoses and\nuncertainties. To our knowledge, this is the first study to jointly address\ndiagnostic uncertainty recognition and explanation, substantially enhancing the\nreliability of automatic diagnostic systems.", "AI": {"tldr": "ConfiDx \u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7cfb\u7edf\uff0c\u4e13\u95e8\u9488\u5bf9\u8bca\u65ad\u4e0d\u786e\u5b9a\u6027\u8fdb\u884c\u8bc6\u522b\u548c\u89e3\u91ca\uff0c\u901a\u8fc7\u5fae\u8c03\u5f00\u6e90 LLM \u5e76\u7ed3\u5408\u8bca\u65ad\u6807\u51c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u8bca\u65ad\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u8bca\u65ad\u8fc7\u7a0b\u4e2d\uff0c\u4e34\u5e8a\u7b14\u8bb0\u4fe1\u606f\u4e0d\u8db3\u4f1a\u5bfc\u81f4\u4e0d\u786e\u5b9a\u6027\u589e\u52a0\uff0c\u4ece\u800c\u5f15\u53d1\u8bef\u8bca\u98ce\u9669\u3002\u5f53\u524d\u7cfb\u7edf\u5728\u8bc6\u522b\u548c\u89e3\u91ca\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u7814\u7a76\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65b0\u65b9\u6cd5\u6765\u63d0\u5347\u8bca\u65ad\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u7814\u7a76\u56e2\u961f\u901a\u8fc7\u5fae\u8c03\u5f00\u6e90 LLM\uff0c\u7ed3\u5408\u8bca\u65ad\u6807\u51c6\uff0c\u6784\u5efa\u4e86 ConfiDx \u6a21\u578b\uff0c\u5e76\u6574\u7406\u4e86\u5305\u542b\u4e0d\u540c\u8bca\u65ad\u6a21\u7cca\u7a0b\u5ea6\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u4ee5\u8bc4\u4f30\u6a21\u578b\u6548\u679c\u3002", "result": "ConfiDx \u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u6709\u6548\u8bc6\u522b\u8bca\u65ad\u4e0d\u786e\u5b9a\u6027\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u548c\u89e3\u91ca\u7684\u53ef\u4fe1\u5ea6\u3002", "conclusion": "ConfiDx \u662f\u9996\u4e2a\u540c\u65f6\u89e3\u51b3\u8bca\u65ad\u4e0d\u786e\u5b9a\u6027\u8bc6\u522b\u548c\u89e3\u91ca\u7684\u7814\u7a76\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u81ea\u52a8\u8bca\u65ad\u7cfb\u7edf\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u66f4\u53ef\u4fe1\u7684\u5de5\u5177\u3002"}}
{"id": "2505.03641", "pdf": "https://arxiv.org/pdf/2505.03641", "abs": "https://arxiv.org/abs/2505.03641", "authors": ["Chen Wei", "Chi Zhang", "Jiachen Zou", "Haotian Deng", "Dietmar Heinke", "Quanying Liu"], "title": "Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering and Manipulating Human Perceptual Variability", "categories": ["cs.AI"], "comment": "accepted at ICML 2025", "summary": "Human decision-making in cognitive tasks and daily life exhibits considerable\nvariability, shaped by factors such as task difficulty, individual preferences,\nand personal experiences. Understanding this variability across individuals is\nessential for uncovering the perceptual and decision-making mechanisms that\nhumans rely on when faced with uncertainty and ambiguity. We present a\ncomputational framework BAM (Boundary Alignment & Manipulation framework) that\ncombines perceptual boundary sampling in ANNs and human behavioral experiments\nto systematically investigate this phenomenon. Our perceptual boundary sampling\nalgorithm generates stimuli along ANN decision boundaries that intrinsically\ninduce significant perceptual variability. The efficacy of these stimuli is\nempirically validated through large-scale behavioral experiments involving 246\nparticipants across 116,715 trials, culminating in the variMNIST dataset\ncontaining 19,943 systematically annotated images. Through personalized model\nalignment and adversarial generation, we establish a reliable method for\nsimultaneously predicting and manipulating the divergent perceptual decisions\nof pairs of participants. This work bridges the gap between computational\nmodels and human individual difference research, providing new tools for\npersonalized perception analysis.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aBAM\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u611f\u77e5\u8fb9\u754c\u91c7\u6837\u548c\u4eba\u7c7b\u884c\u4e3a\u5b9e\u9a8c\uff0c\u4ee5\u7cfb\u7edf\u6027\u7814\u7a76\u4eba\u7c7b\u51b3\u7b56\u7684\u53d8\u5f02\u6027\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7406\u89e3\u4eba\u7c7b\u5728\u8ba4\u77e5\u4efb\u52a1\u548c\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u51b3\u7b56\u53d8\u5f02\u6027\uff0c\u63a2\u7d22\u611f\u77e5\u548c\u51b3\u7b56\u673a\u5236\u5982\u4f55\u5e94\u5bf9\u4e0d\u786e\u5b9a\u6027\u548c\u6a21\u7cca\u6027\u3002", "method": "\u7ed3\u5408\u4e86ANN\u7684\u611f\u77e5\u8fb9\u754c\u91c7\u6837\u7b97\u6cd5\u548c\u5927\u89c4\u6a21\u4eba\u7c7b\u884c\u4e3a\u5b9e\u9a8c\uff0c\u751f\u6210\u4e86\u53d8\u5f02\u6027\u523a\u6fc0\uff0c\u5e76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u901a\u8fc7246\u540d\u53c2\u4e0e\u8005\u7684116,715\u6b21\u5b9e\u9a8c\uff0c\u521b\u5efa\u4e86\u5305\u542b19,943\u5f20\u6807\u6ce8\u56fe\u50cf\u7684variMNIST\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u540c\u65f6\u9884\u6d4b\u548c\u64cd\u7eb5\u53c2\u4e0e\u8005\u611f\u77e5\u51b3\u7b56\u7684\u80fd\u529b\u3002", "conclusion": "BAM\u6846\u67b6\u586b\u8865\u4e86\u8ba1\u7b97\u6a21\u578b\u4e0e\u4eba\u7c7b\u4e2a\u4f53\u5dee\u5f02\u7814\u7a76\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u4e3a\u4e2a\u6027\u5316\u611f\u77e5\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2505.03172", "pdf": "https://arxiv.org/pdf/2505.03172", "abs": "https://arxiv.org/abs/2505.03172", "authors": ["Caleb Chuck", "Fan Feng", "Carl Qi", "Chang Shi", "Siddhant Agarwal", "Amy Zhang", "Scott Niekum"], "title": "Null Counterfactual Factor Interactions for Goal-Conditioned Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Published at ICLR 2025", "summary": "Hindsight relabeling is a powerful tool for overcoming sparsity in\ngoal-conditioned reinforcement learning (GCRL), especially in certain domains\nsuch as navigation and locomotion. However, hindsight relabeling can struggle\nin object-centric domains. For example, suppose that the goal space consists of\na robotic arm pushing a particular target block to a goal location. In this\ncase, hindsight relabeling will give high rewards to any trajectory that does\nnot interact with the block. However, these behaviors are only useful when the\nobject is already at the goal -- an extremely rare case in practice. A dataset\ndominated by these kinds of trajectories can complicate learning and lead to\nfailures. In object-centric domains, one key intuition is that meaningful\ntrajectories are often characterized by object-object interactions such as\npushing the block with the gripper. To leverage this intuition, we introduce\nHindsight Relabeling using Interactions (HInt), which combines interactions\nwith hindsight relabeling to improve the sample efficiency of downstream RL.\nHowever because interactions do not have a consensus statistical definition\ntractable for downstream GCRL, we propose a definition of interactions based on\nthe concept of null counterfactual: a cause object is interacting with a target\nobject if, in a world where the cause object did not exist, the target object\nwould have different transition dynamics. We leverage this definition to infer\ninteractions in Null Counterfactual Interaction Inference (NCII), which uses a\n\"nulling'' operation with a learned model to infer interactions. NCII is able\nto achieve significantly improved interaction inference accuracy in both simple\nlinear dynamics domains and dynamic robotic domains in Robosuite, Robot Air\nHockey, and Franka Kitchen and HInt improves sample efficiency by up to 4x.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faHInt\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u4ea4\u4e92\u4e0e\u4e8b\u540e\u91cd\u6807\u7b7e\uff0c\u63d0\u9ad8\u76ee\u6807\u5bfc\u5411\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\uff0c\u5e76\u5728\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6548\u679c\u3002", "motivation": "\u5728\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u9886\u57df\uff0c\u4f20\u7edf\u7684\u4e8b\u540e\u91cd\u6807\u7b7e\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5206\u914d\u5956\u52b1\uff0c\u5bfc\u81f4\u5b66\u4e60\u5931\u8d25\u3002\u8bba\u6587\u901a\u8fc7\u5b9a\u4e49\u4ea4\u4e92\u884c\u4e3a\u6765\u6539\u8fdb\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faHInt\u65b9\u6cd5\uff0c\u7ed3\u5408\u4ea4\u4e92\u4e0e\u4e8b\u540e\u91cd\u6807\u7b7e\uff0c\u5e76\u4f7f\u7528Null Counterfactual Interaction Inference\uff08NCII\uff09\u6765\u63a8\u65ad\u4ea4\u4e92\u884c\u4e3a\u3002", "result": "\u5728\u591a\u4e2a\u52a8\u6001\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u4ea4\u4e92\u63a8\u65ad\u51c6\u786e\u6027\u63d0\u5347\uff0c\u6837\u672c\u6548\u7387\u63d0\u9ad8\u4e864\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u4efb\u52a1\u4e2d\u6709\u6548\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2505.03469", "pdf": "https://arxiv.org/pdf/2505.03469", "abs": "https://arxiv.org/abs/2505.03469", "authors": ["Bin Yu", "Hang Yuan", "Yuliang Wei", "Bailing Wang", "Weizhen Qi", "Kai Chen"], "title": "Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": "11 pages, 2 figures", "summary": "Recent advances in large language models have demonstrated that Supervised\nFine-Tuning (SFT) with Chain-of-Thought (CoT) reasoning data distilled from\nlarge reasoning models (e.g., DeepSeek R1) can effectively transfer reasoning\ncapabilities to non-reasoning models. However, models fine-tuned with this\napproach inherit the \"overthinking\" problem from teacher models, producing\nverbose and redundant reasoning chains during inference. To address this\nchallenge, we propose \\textbf{L}ong-\\textbf{S}hort Chain-of-Thought\n\\textbf{Mixture} \\textbf{S}upervised \\textbf{F}ine-\\textbf{T}uning\n(\\textbf{LS-Mixture SFT}), which combines long CoT reasoning dataset with their\nshort counterparts obtained through structure-preserved rewriting. Our\nexperiments demonstrate that models trained using the LS-Mixture SFT method,\ncompared to those trained with direct SFT, achieved an average accuracy\nimprovement of 2.3\\% across various benchmarks while substantially reducing\nmodel response length by approximately 47.61\\%. This work offers an approach to\nendow non-reasoning models with reasoning capabilities through supervised\nfine-tuning while avoiding the inherent overthinking problems inherited from\nteacher models, thereby enabling efficient reasoning in the fine-tuned models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86LS-Mixture SFT\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u957f\u77ed\u94fe\u5f0f\u601d\u7ef4\u6570\u636e\u5fae\u8c03\u975e\u63a8\u7406\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u80fd\u529b\u5e76\u51cf\u5c11\u5197\u4f59\u8f93\u51fa\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4ece\u5927\u578b\u63a8\u7406\u6a21\u578b\u84b8\u998f\u65f6\u7ee7\u627f\u7684\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u95ee\u9898\uff0c\u5373\u751f\u6210\u5197\u957f\u7684\u63a8\u7406\u94fe\u3002", "method": "\u63d0\u51faLS-Mixture SFT\uff0c\u6df7\u5408\u957f\u94fe\u5f0f\u601d\u7ef4\u6570\u636e\u4e0e\u7ed3\u6784\u4fdd\u7559\u91cd\u5199\u7684\u77ed\u94fe\u6570\u636e\u7528\u4e8e\u6a21\u578b\u5fae\u8c03\u3002", "result": "\u76f8\u6bd4\u76f4\u63a5SFT\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53472.3%\uff0c\u54cd\u5e94\u957f\u5ea6\u51cf\u5c1147.61%\u3002", "conclusion": "LS-Mixture SFT\u80fd\u5728\u907f\u514d\u8fc7\u5ea6\u601d\u8003\u7684\u540c\u65f6\u9ad8\u6548\u8d4b\u4e88\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2505.03643", "pdf": "https://arxiv.org/pdf/2505.03643", "abs": "https://arxiv.org/abs/2505.03643", "authors": ["Chelsea Sidrane", "Jana Tumova"], "title": "BURNS: Backward Underapproximate Reachability for Neural-Feedback-Loop Systems", "categories": ["cs.AI", "cs.LO", "cs.SY", "eess.SY"], "comment": null, "summary": "Learning-enabled planning and control algorithms are increasingly popular,\nbut they often lack rigorous guarantees of performance or safety. We introduce\nan algorithm for computing underapproximate backward reachable sets of\nnonlinear discrete time neural feedback loops. We then use the backward\nreachable sets to check goal-reaching properties. Our algorithm is based on\noverapproximating the system dynamics function to enable computation of\nunderapproximate backward reachable sets through solutions of mixed-integer\nlinear programs. We rigorously analyze the soundness of our algorithm and\ndemonstrate it on a numerical example. Our work expands the class of properties\nthat can be verified for learning-enabled systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8ba1\u7b97\u975e\u7ebf\u6027\u79bb\u6563\u65f6\u95f4\u795e\u7ecf\u53cd\u9988\u56de\u8def\u7684\u4e0b\u8fd1\u4f3c\u540e\u5411\u53ef\u8fbe\u96c6\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u9a8c\u8bc1\u76ee\u6807\u53ef\u8fbe\u6027\u3002", "motivation": "\u73b0\u6709\u5b66\u4e60\u542f\u7528\u7684\u89c4\u5212\u548c\u63a7\u5236\u7b97\u6cd5\u7f3a\u4e4f\u4e25\u683c\u7684\u6027\u80fd\u6216\u5b89\u5168\u6027\u4fdd\u8bc1\uff0c\u9700\u4e00\u79cd\u80fd\u9a8c\u8bc1\u5b66\u4e60\u542f\u7528\u7cfb\u7edf\u5c5e\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u8fc7\u8fd1\u4f3c\u7cfb\u7edf\u52a8\u529b\u5b66\u51fd\u6570\uff0c\u5229\u7528\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u8ba1\u7b97\u4e0b\u8fd1\u4f3c\u540e\u5411\u53ef\u8fbe\u96c6\u3002", "result": "\u7b97\u6cd5\u5728\u6570\u503c\u793a\u4f8b\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u6269\u5c55\u4e86\u53ef\u9a8c\u8bc1\u5b66\u4e60\u542f\u7528\u7cfb\u7edf\u5c5e\u6027\u7684\u7c7b\u522b\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u5b66\u4e60\u542f\u7528\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u4e25\u683c\u7684\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86\u5176\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2505.03178", "pdf": "https://arxiv.org/pdf/2505.03178", "abs": "https://arxiv.org/abs/2505.03178", "authors": ["Jiawei Wang", "Xintao Yan", "Yao Mu", "Haowei Sun", "Zhong Cao", "Henry X. Liu"], "title": "RADE: Learning Risk-Adjustable Driving Environment via Multi-Agent Conditional Diffusion", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "Generating safety-critical scenarios in high-fidelity simulations offers a\npromising and cost-effective approach for efficient testing of autonomous\nvehicles. Existing methods typically rely on manipulating a single vehicle's\ntrajectory through sophisticated designed objectives to induce adversarial\ninteractions, often at the cost of realism and scalability. In this work, we\npropose the Risk-Adjustable Driving Environment (RADE), a simulation framework\nthat generates statistically realistic and risk-adjustable traffic scenes.\nBuilt upon a multi-agent diffusion architecture, RADE jointly models the\nbehavior of all agents in the environment and conditions their trajectories on\na surrogate risk measure. Unlike traditional adversarial methods, RADE learns\nrisk-conditioned behaviors directly from data, preserving naturalistic\nmulti-agent interactions with controllable risk levels. To ensure physical\nplausibility, we incorporate a tokenized dynamics check module that efficiently\nfilters generated trajectories using a motion vocabulary. We validate RADE on\nthe real-world rounD dataset, demonstrating that it preserves statistical\nrealism across varying risk levels and naturally increases the likelihood of\nsafety-critical events as the desired risk level grows up. Our results\nhighlight RADE's potential as a scalable and realistic tool for AV safety\nevaluation.", "AI": {"tldr": "RADE\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6269\u6563\u67b6\u6784\u7684\u4eff\u771f\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u7edf\u8ba1\u4e0a\u771f\u5b9e\u4e14\u98ce\u9669\u53ef\u8c03\u7684\u4ea4\u901a\u573a\u666f\uff0c\u76f4\u63a5\u901a\u8fc7\u6570\u636e\u5b66\u4e60\u98ce\u9669\u6761\u4ef6\u4e0b\u7684\u884c\u4e3a\uff0c\u5e76\u4fdd\u6301\u81ea\u7136\u7684\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\uff0c\u540c\u65f6\u901a\u8fc7\u4ee4\u724c\u5316\u52a8\u529b\u5b66\u68c0\u67e5\u6a21\u5757\u786e\u4fdd\u7269\u7406\u5408\u7406\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u8bbe\u8ba1\u590d\u6742\u7684\u76ee\u6807\u64cd\u7eb5\u5355\u4e00\u8f66\u8f86\u7684\u8f68\u8ff9\u4ee5\u5f15\u53d1\u5bf9\u6297\u6027\u4ea4\u4e92\uff0c\u4f46\u727a\u7272\u4e86\u771f\u5b9e\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002RADE\u65e8\u5728\u751f\u6210\u7edf\u8ba1\u4e0a\u771f\u5b9e\u4e14\u98ce\u9669\u53ef\u8c03\u7684\u4ea4\u901a\u573a\u666f\uff0c\u4ee5\u66f4\u9ad8\u6548\u5730\u6d4b\u8bd5\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b89\u5168\u6027\u3002", "method": "RADE\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6269\u6563\u67b6\u6784\uff0c\u8054\u5408\u5efa\u6a21\u73af\u5883\u4e2d\u6240\u6709\u667a\u80fd\u4f53\u7684\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u4ee3\u7406\u98ce\u9669\u5ea6\u91cf\u6761\u4ef6\u5316\u5176\u8f68\u8ff9\u3002\u5f15\u5165\u4ee4\u724c\u5316\u52a8\u529b\u5b66\u68c0\u67e5\u6a21\u5757\u8fc7\u6ee4\u751f\u6210\u7684\u8f68\u8ff9\u4ee5\u786e\u4fdd\u7269\u7406\u5408\u7406\u6027\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684rounD\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u8868\u660e\uff0cRADE\u5728\u4e0d\u540c\u98ce\u9669\u6c34\u5e73\u4e0b\u5747\u4fdd\u6301\u7edf\u8ba1\u771f\u5b9e\u6027\uff0c\u5e76\u968f\u7740\u98ce\u9669\u6c34\u5e73\u7684\u63d0\u5347\u81ea\u7136\u589e\u52a0\u5b89\u5168\u5173\u952e\u4e8b\u4ef6\u7684\u6982\u7387\u3002", "conclusion": "RADE\u4f5c\u4e3a\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u771f\u5b9e\u7684\u5de5\u5177\uff0c\u5177\u6709\u5728\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u6027\u8bc4\u4f30\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.03473", "pdf": "https://arxiv.org/pdf/2505.03473", "abs": "https://arxiv.org/abs/2505.03473", "authors": ["Marta Boscariol", "Luana Bulla", "Lia Draetta", "Beatrice Fiuman\u00f2", "Emanuele Lenzi", "Leonardo Piano"], "title": "Evaluation of LLMs on Long-tail Entity Linking in Historical Documents", "categories": ["cs.CL"], "comment": null, "summary": "Entity Linking (EL) plays a crucial role in Natural Language Processing (NLP)\napplications, enabling the disambiguation of entity mentions by linking them to\ntheir corresponding entries in a reference knowledge base (KB). Thanks to their\ndeep contextual understanding capabilities, LLMs offer a new perspective to\ntackle EL, promising better results than traditional methods. Despite the\nimpressive generalization capabilities of LLMs, linking less popular, long-tail\nentities remains challenging as these entities are often underrepresented in\ntraining data and knowledge bases. Furthermore, the long-tail EL task is an\nunderstudied problem, and limited studies address it with LLMs. In the present\nwork, we assess the performance of two popular LLMs, GPT and LLama3, in a\nlong-tail entity linking scenario. Using MHERCL v0.1, a manually annotated\nbenchmark of sentences from domain-specific historical texts, we quantitatively\ncompare the performance of LLMs in identifying and linking entities to their\ncorresponding Wikidata entries against that of ReLiK, a state-of-the-art Entity\nLinking and Relation Extraction framework. Our preliminary experiments reveal\nthat LLMs perform encouragingly well in long-tail EL, indicating that this\ntechnology can be a valuable adjunct in filling the gap between head and\nlong-tail EL.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86GPT\u548cLLaMA3\u5728\u957f\u5c3e\u5b9e\u4f53\u94fe\u63a5\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5728\u7279\u5b9a\u5386\u53f2\u6587\u672c\u9886\u57df\uff0cLLMs\u8868\u73b0\u826f\u597d\uff0c\u53ef\u4f5c\u4e3a\u4f20\u7edf\u65b9\u6cd5\u7684\u8865\u5145\u3002", "motivation": "\u957f\u5c3e\u5b9e\u4f53\u5728\u8bad\u7ec3\u6570\u636e\u548c\u77e5\u8bc6\u5e93\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u4f7f\u7528LLMs\u89e3\u51b3\u957f\u5c3e\u5b9e\u4f53\u94fe\u63a5\u95ee\u9898\u662f\u4e00\u4e2a\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u9886\u57df\u3002", "method": "\u4f7f\u7528MHERCL v0.1\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6bd4\u8f83GPT\u548cLLaMA3\u4e0eReLiK\u6846\u67b6\u5728\u957f\u5c3e\u5b9e\u4f53\u94fe\u63a5\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\uff0cLLMs\u5728\u957f\u5c3e\u5b9e\u4f53\u94fe\u63a5\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u663e\u793a\u51fa\u586b\u8865\u5934\u90e8\u4e0e\u957f\u5c3e\u5b9e\u4f53\u94fe\u63a5\u5dee\u8ddd\u7684\u6f5c\u529b\u3002", "conclusion": "LLMs\u53ef\u4ee5\u4f5c\u4e3a\u957f\u5c3e\u5b9e\u4f53\u94fe\u63a5\u6280\u672f\u7684\u6709\u6548\u8865\u5145\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u4f18\u5316\u5176\u5728\u957f\u5c3e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2505.03668", "pdf": "https://arxiv.org/pdf/2505.03668", "abs": "https://arxiv.org/abs/2505.03668", "authors": ["Celeste Veronese", "Daniele Meli", "Alessandro Farinelli"], "title": "Learning Symbolic Persistent Macro-Actions for POMDP Solving Over Time", "categories": ["cs.AI"], "comment": "Accepted at 9th Conference on Neurosymbolic Learning and Reasoning", "summary": "This paper proposes an integration of temporal logical reasoning and\nPartially Observable Markov Decision Processes (POMDPs) to achieve\ninterpretable decision-making under uncertainty with macro-actions. Our method\nleverages a fragment of Linear Temporal Logic (LTL) based on Event Calculus\n(EC) to generate \\emph{persistent} (i.e., constant) macro-actions, which guide\nMonte Carlo Tree Search (MCTS)-based POMDP solvers over a time horizon,\nsignificantly reducing inference time while ensuring robust performance. Such\nmacro-actions are learnt via Inductive Logic Programming (ILP) from a few\ntraces of execution (belief-action pairs), thus eliminating the need for\nmanually designed heuristics and requiring only the specification of the POMDP\ntransition model. In the Pocman and Rocksample benchmark scenarios, our learned\nmacro-actions demonstrate increased expressiveness and generality when compared\nto time-independent heuristics, indeed offering substantial computational\nefficiency improvements.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u65f6\u95f4\u903b\u8f91\u63a8\u7406\u4e0e\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDPs\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\uff08LTL\uff09\u548c\u4e8b\u4ef6\u6f14\u7b97\uff08EC\uff09\u751f\u6210\u6301\u4e45\u5b8f\u52a8\u4f5c\uff0c\u663e\u8457\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u51b3\u7b56\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u65f6\u95f4\u903b\u8f91\u548cPOMDPs\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\uff0c\u540c\u65f6\u51cf\u5c11\u5bf9\u4eba\u5de5\u8bbe\u8ba1\u542f\u53d1\u5f0f\u7684\u4f9d\u8d56\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5229\u7528LTL\u548cEC\u751f\u6210\u6301\u4e45\u5b8f\u52a8\u4f5c\uff0c\u901a\u8fc7\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\uff08ILP\uff09\u4ece\u5c11\u91cf\u6267\u884c\u8f68\u8ff9\u4e2d\u5b66\u4e60\u5b8f\u52a8\u4f5c\uff0c\u5e76\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u4f18\u5316POMDP\u6c42\u89e3\u3002", "result": "\u5728Pocman\u548cRocksample\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b66\u4e60\u7684\u5b8f\u52a8\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u8868\u8fbe\u80fd\u529b\u548c\u901a\u7528\u6027\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\uff0c\u7ed3\u5408\u65f6\u95f4\u903b\u8f91\u7684\u5b8f\u52a8\u4f5c\u80fd\u9ad8\u6548\u89e3\u51b3\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u95ee\u9898\uff0c\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u5e76\u4fdd\u6301\u9c81\u68d2\u6027\u80fd\u3002"}}
{"id": "2505.03181", "pdf": "https://arxiv.org/pdf/2505.03181", "abs": "https://arxiv.org/abs/2505.03181", "authors": ["Jake Grigsby", "Yuke Zhu", "Michael Ryoo", "Juan Carlos Niebles"], "title": "VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making", "categories": ["cs.LG"], "comment": "SSI-FM Workshop ICLR 2025", "summary": "Recent research looks to harness the general knowledge and reasoning of large\nlanguage models (LLMs) into agents that accomplish user-specified goals in\ninteractive environments. Vision-language models (VLMs) extend LLMs to\nmulti-modal data and provide agents with the visual reasoning necessary for new\napplications in areas such as computer automation. However, agent tasks\nemphasize skills where accessible open-weight VLMs lag behind their LLM\nequivalents. For example, VLMs are less capable of following an environment's\nstrict output syntax requirements and are more focused on open-ended question\nanswering. Overcoming these limitations requires supervised fine-tuning (SFT)\non task-specific expert demonstrations. Our work approaches these challenges\nfrom an offline-to-online reinforcement learning (RL) perspective. RL lets us\nfine-tune VLMs to agent tasks while learning from the unsuccessful decisions of\nour own model or more capable (larger) models. We explore an off-policy RL\nsolution that retains the stability and simplicity of the widely used SFT\nworkflow while allowing our agent to self-improve and learn from low-quality\ndatasets. We demonstrate this technique with two open-weight VLMs across three\nmulti-modal agent domains.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f18\u5316\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u4ee3\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5f25\u8865\u5176\u5728\u4e25\u683c\u8f93\u51fa\u8bed\u6cd5\u8981\u6c42\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u4e09\u4e2a\u591a\u6a21\u6001\u4ee3\u7406\u9886\u57df\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7406\u4efb\u52a1\u4e2d\u4e0d\u5982\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7075\u6d3b\uff0c\u5c24\u5176\u662f\u5728\u4e25\u683c\u8f93\u51fa\u8bed\u6cd5\u8981\u6c42\u4e0a\u7684\u4e0d\u8db3\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u4ece\u6a21\u578b\u81ea\u8eab\u6216\u66f4\u5f3a\u5927\u6a21\u578b\u7684\u5931\u8d25\u51b3\u7b56\u4e2d\u5b66\u4e60\uff0c\u4ee5\u7a33\u5b9a\u4e14\u7b80\u5355\u7684\u65b9\u5f0f\u4f18\u5316VLM\u3002", "result": "\u7814\u7a76\u5728\u4e09\u4e2a\u591a\u6a21\u6001\u4ee3\u7406\u9886\u57df\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u8bc1\u660e\u5176\u80fd\u591f\u6709\u6548\u63d0\u5347VLM\u5728\u4ee3\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0cVLM\u80fd\u591f\u5728\u4ee3\u7406\u4efb\u52a1\u4e2d\u81ea\u6211\u6539\u8fdb\u5e76\u5b66\u4e60\u4f4e\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u4e3a\u591a\u6a21\u6001\u4ee3\u7406\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.03481", "pdf": "https://arxiv.org/pdf/2505.03481", "abs": "https://arxiv.org/abs/2505.03481", "authors": ["Maciej Zembrzuski", "Saad Mahamood"], "title": "Sentence Embeddings as an intermediate target in end-to-end summarisation", "categories": ["cs.CL"], "comment": "10 pages, 1 figure, Year: 2019", "summary": "Current neural network-based methods to the problem of document summarisation\nstruggle when applied to datasets containing large inputs. In this paper we\npropose a new approach to the challenge of content-selection when dealing with\nend-to-end summarisation of user reviews of accommodations. We show that by\ncombining an extractive approach with externally pre-trained sentence level\nembeddings in an addition to an abstractive summarisation model we can\noutperform existing methods when this is applied to the task of summarising a\nlarge input dataset. We also prove that predicting sentence level embedding of\na summary increases the quality of an end-to-end system for loosely aligned\nsource to target corpora, than compared to commonly predicting probability\ndistributions of sentence selection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u62bd\u53d6\u5f0f\u548c\u751f\u6210\u5f0f\u65b9\u6cd5\u7684\u65b0\u6a21\u578b\uff0c\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u7684\u53e5\u5b50\u7ea7\u5d4c\u5165\u6765\u63d0\u5347\u5927\u89c4\u6a21\u7528\u6237\u8bc4\u8bba\u6458\u8981\u7684\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u8f93\u5165\u6570\u636e\u96c6\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u5c24\u5176\u5728\u7528\u6237\u8bc4\u8bba\u6458\u8981\u4efb\u52a1\u4e2d\u3002", "method": "\u7ed3\u5408\u62bd\u53d6\u5f0f\u65b9\u6cd5\uff08\u5229\u7528\u9884\u8bad\u7ec3\u53e5\u5b50\u7ea7\u5d4c\u5165\uff09\u4e0e\u751f\u6210\u5f0f\u6a21\u578b\uff0c\u6539\u8fdb\u5185\u5bb9\u9009\u62e9\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u8f93\u5165\u6570\u636e\u6458\u8981\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u9884\u6d4b\u6458\u8981\u53e5\u5b50\u7ea7\u5d4c\u5165\u6bd4\u4f20\u7edf\u53e5\u5b50\u9009\u62e9\u6982\u7387\u5206\u5e03\u66f4\u6709\u6548\u3002", "conclusion": "\u8054\u5408\u4f7f\u7528\u62bd\u53d6\u4e0e\u751f\u6210\u65b9\u6cd5\u53ca\u53e5\u5b50\u7ea7\u5d4c\u5165\u80fd\u663e\u8457\u63d0\u5347\u6458\u8981\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u677e\u6563\u5bf9\u9f50\u7684\u6e90-\u76ee\u6807\u8bed\u6599\u3002"}}
{"id": "2505.03674", "pdf": "https://arxiv.org/pdf/2505.03674", "abs": "https://arxiv.org/abs/2505.03674", "authors": ["Yotam Amitai", "Reuth Mirsky", "Ofra Amir"], "title": "Gap the (Theory of) Mind: Sharing Beliefs About Teammates' Goals Boosts Collaboration Perception, Not Performance", "categories": ["cs.AI"], "comment": null, "summary": "In human-agent teams, openly sharing goals is often assumed to enhance\nplanning, collaboration, and effectiveness. However, direct communication of\nthese goals is not always feasible, requiring teammates to infer their\npartner's intentions through actions. Building on this, we investigate whether\nan AI agent's ability to share its inferred understanding of a human teammate's\ngoals can improve task performance and perceived collaboration. Through an\nexperiment comparing three conditions-no recognition (NR), viable goals (VG),\nand viable goals on-demand (VGod) - we find that while goal-sharing information\ndid not yield significant improvements in task performance or overall\nsatisfaction scores, thematic analysis suggests that it supported strategic\nadaptations and subjective perceptions of collaboration. Cognitive load\nassessments revealed no additional burden across conditions, highlighting the\nchallenge of balancing informativeness and simplicity in human-agent\ninteractions. These findings highlight the nuanced trade-off of goal-sharing:\nwhile it fosters trust and enhances perceived collaboration, it can\noccasionally hinder objective performance gains.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86AI\u4ee3\u7406\u901a\u8fc7\u5171\u4eab\u63a8\u65ad\u7684\u4eba\u7c7b\u961f\u53cb\u76ee\u6807\u662f\u5426\u80fd\u63d0\u5347\u4efb\u52a1\u8868\u73b0\u548c\u534f\u4f5c\u611f\u77e5\u3002\u5b9e\u9a8c\u53d1\u73b0\uff0c\u76ee\u6807\u5171\u4eab\u867d\u672a\u663e\u8457\u6539\u5584\u4efb\u52a1\u8868\u73b0\u6216\u6ee1\u610f\u5ea6\uff0c\u4f46\u652f\u6301\u4e86\u7b56\u7565\u8c03\u6574\u548c\u534f\u4f5c\u611f\u77e5\uff0c\u4e14\u5728\u8ba4\u77e5\u8d1f\u8377\u4e0a\u65e0\u989d\u5916\u8d1f\u62c5\u3002", "motivation": "\u5728\u4eba\u7c7b-AI\u56e2\u961f\u4e2d\uff0c\u76f4\u63a5\u5171\u4eab\u76ee\u6807\u5e76\u975e\u603b\u662f\u53ef\u884c\uff0c\u9700\u901a\u8fc7\u884c\u52a8\u63a8\u65ad\u610f\u56fe\u3002\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1AI\u5171\u4eab\u63a8\u65ad\u76ee\u6807\u662f\u5426\u80fd\u63d0\u5347\u534f\u4f5c\u6548\u679c\u3002", "method": "\u91c7\u7528\u5b9e\u9a8c\u6bd4\u8f83\u4e09\u79cd\u6761\u4ef6\uff1a\u65e0\u76ee\u6807\u8bc6\u522b\uff08NR\uff09\u3001\u53ef\u884c\u76ee\u6807\uff08VG\uff09\u53ca\u6309\u9700\u53ef\u884c\u76ee\u6807\uff08VGod\uff09\uff0c\u8bc4\u4f30\u4efb\u52a1\u8868\u73b0\u3001\u6ee1\u610f\u5ea6\u53ca\u8ba4\u77e5\u8d1f\u8377\u3002", "result": "\u76ee\u6807\u5171\u4eab\u672a\u663e\u8457\u63d0\u5347\u4efb\u52a1\u8868\u73b0\u6216\u6ee1\u610f\u5ea6\uff0c\u4f46\u652f\u6301\u7b56\u7565\u8c03\u6574\u548c\u534f\u4f5c\u611f\u77e5\uff1b\u8ba4\u77e5\u8d1f\u8377\u65e0\u5dee\u5f02\uff0c\u8868\u660e\u4fe1\u606f\u91cf\u4e0e\u7b80\u6d01\u6027\u7684\u5e73\u8861\u96be\u9898\u3002", "conclusion": "\u76ee\u6807\u5171\u4eab\u5728\u589e\u5f3a\u4fe1\u4efb\u548c\u534f\u4f5c\u611f\u77e5\u4e0a\u6709\u4ef7\u503c\uff0c\u4f46\u53ef\u80fd\u727a\u7272\u5ba2\u89c2\u6027\u80fd\uff0c\u9700\u6743\u8861\u5176\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2505.03194", "pdf": "https://arxiv.org/pdf/2505.03194", "abs": "https://arxiv.org/abs/2505.03194", "authors": ["Yiding Chen", "Yiyi Zhang", "Owen Oertell", "Wen Sun"], "title": "Convergence Of Consistency Model With Multistep Sampling Under General Data Assumptions", "categories": ["cs.LG"], "comment": null, "summary": "Diffusion models accomplish remarkable success in data generation tasks\nacross various domains. However, the iterative sampling process is\ncomputationally expensive. Consistency models are proposed to learn consistency\nfunctions to map from noise to data directly, which allows one-step fast data\ngeneration and multistep sampling to improve sample quality. In this paper, we\nstudy the convergence of consistency models when the self-consistency property\nholds approximately under the training distribution. Our analysis requires only\nmild data assumption and applies to a family of forward processes. When the\ntarget data distribution has bounded support or has tails that decay\nsufficiently fast, we show that the samples generated by the consistency model\nare close to the target distribution in Wasserstein distance; when the target\ndistribution satisfies some smoothness assumption, we show that with an\nadditional perturbation step for smoothing, the generated samples are close to\nthe target distribution in total variation distance. We provide two case\nstudies with commonly chosen forward processes to demonstrate the benefit of\nmultistep sampling.", "AI": {"tldr": "\u4e00\u81f4\u6027\u6a21\u578b\u901a\u8fc7\u5b66\u4e60\u4ece\u566a\u58f0\u76f4\u63a5\u6620\u5c04\u5230\u6570\u636e\u7684\u51fd\u6570\uff0c\u5b9e\u73b0\u5feb\u901f\u4e00\u6b65\u751f\u6210\u548c\u591a\u6b65\u91c7\u6837\u4ee5\u63d0\u9ad8\u6837\u672c\u8d28\u91cf\u3002\u672c\u6587\u5206\u6790\u4e86\u81ea\u4e00\u81f4\u6027\u5728\u8bad\u7ec3\u5206\u5e03\u4e0b\u8fd1\u4f3c\u6210\u7acb\u65f6\u7684\u6536\u655b\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u6570\u636e\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u8fed\u4ee3\u91c7\u6837\u8fc7\u7a0b\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u7814\u7a76\u4e00\u81f4\u6027\u6a21\u578b\u5728\u81ea\u4e00\u81f4\u6027\u6761\u4ef6\u4e0b\u7684\u6536\u655b\u6027\uff0c\u662f\u5e0c\u671b\u627e\u5230\u66f4\u9ad8\u6548\u7684\u751f\u6210\u65b9\u5f0f\u3002", "method": "\u5206\u6790\u4e86\u4e00\u81f4\u6027\u6a21\u578b\u5728\u81ea\u4e00\u81f4\u6027\u8fd1\u4f3c\u6210\u7acb\u65f6\u7684\u6536\u655b\u6027\uff0c\u9488\u5bf9\u4e0d\u540c\u6570\u636e\u5206\u5e03\uff08\u6709\u754c\u652f\u6491\u6216\u5feb\u901f\u8870\u51cf\u7684\u5c3e\u90e8\u5206\u5e03\uff09\u7ed9\u51fa\u4e86\u6837\u672c\u4e0e\u76ee\u6807\u5206\u5e03\u5728Wasserstein\u8ddd\u79bb\u6216\u603b\u53d8\u5dee\u8ddd\u79bb\u4e0b\u7684\u8fd1\u4f3c\u7a0b\u5ea6\u3002", "result": "\u5f53\u76ee\u6807\u5206\u5e03\u6ee1\u8db3\u4e00\u5b9a\u6761\u4ef6\u65f6\uff0c\u4e00\u81f4\u6027\u6a21\u578b\u751f\u6210\u7684\u6837\u672c\u4e0e\u76ee\u6807\u5206\u5e03\u5728\u7279\u5b9a\u8ddd\u79bb\u5ea6\u91cf\u4e0b\u63a5\u8fd1\uff1b\u591a\u6b65\u91c7\u6837\u7684\u76ca\u5904\u901a\u8fc7\u5e38\u7528\u524d\u5411\u8fc7\u7a0b\u7684\u6848\u4f8b\u7814\u7a76\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "\u4e00\u81f4\u6027\u6a21\u578b\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u80fd\u6709\u6548\u903c\u8fd1\u76ee\u6807\u5206\u5e03\uff0c\u591a\u6b65\u91c7\u6837\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6837\u672c\u8d28\u91cf\u3002"}}
{"id": "2505.03531", "pdf": "https://arxiv.org/pdf/2505.03531", "abs": "https://arxiv.org/abs/2505.03531", "authors": ["Haoqi Yang", "Luohe Shi", "Qiwei Li", "Zuchao Li", "Ping Wang", "Bo Du", "Mengjia Shen", "Hai Zhao"], "title": "Faster MoE LLM Inference for Extremely Large Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually\nbecoming the mainstream approach for ultra-large-scale models. Existing\noptimization efforts for MoE models have focused primarily on coarse-grained\nMoE architectures. With the emergence of DeepSeek Models, fine-grained MoE\nmodels are gaining popularity, yet research on them remains limited. Therefore,\nwe want to discuss the efficiency dynamic under different service loads.\nAdditionally, fine-grained models allow deployers to reduce the number of\nrouted experts, both activated counts and total counts, raising the question of\nhow this reduction affects the trade-off between MoE efficiency and\nperformance. Our findings indicate that while deploying MoE models presents\ngreater challenges, it also offers significant optimization opportunities.\nReducing the number of activated experts can lead to substantial efficiency\nimprovements in certain scenarios, with only minor performance degradation.\nReducing the total number of experts provides limited efficiency gains but\nresults in severe performance degradation. Our method can increase throughput\nby at least 10\\% without any performance degradation. Overall, we conclude that\nMoE inference optimization remains an area with substantial potential for\nexploration and improvement.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u7ec6\u7c92\u5ea6\u7a00\u758f\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u578b\u7684\u6548\u7387\u52a8\u6001\uff0c\u7814\u7a76\u8868\u660e\u51cf\u5c11\u6fc0\u6d3b\u4e13\u5bb6\u6570\u91cf\u80fd\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u6548\u7387\u4e14\u6027\u80fd\u635f\u5931\u8f83\u5c0f\uff0c\u800c\u51cf\u5c11\u4e13\u5bb6\u603b\u6570\u5219\u6548\u7387\u63d0\u5347\u6709\u9650\u4f46\u6027\u80fd\u635f\u5931\u4e25\u91cd\u3002\u65b9\u6cd5\u53ef\u5b9e\u73b0\u81f3\u5c1110%\u7684\u541e\u5410\u91cf\u63d0\u5347\u4e14\u65e0\u6027\u80fd\u635f\u5931\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7c97\u7c92\u5ea6MoE\u67b6\u6784\uff0c\u800c\u5bf9\u7ec6\u7c92\u5ea6MoE\u7684\u7814\u7a76\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u670d\u52a1\u8d1f\u8f7d\u4e0b\u7684\u6548\u7387\u52a8\u6001\u53ca\u4e13\u5bb6\u6570\u91cf\u51cf\u5c11\u5bf9\u6548\u7387\u4e0e\u6027\u80fd\u6743\u8861\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4e0d\u540c\u670d\u52a1\u8d1f\u8f7d\u4e0b\u7ec6\u7c92\u5ea6MoE\u6a21\u578b\u7684\u6548\u7387\u52a8\u6001\uff0c\u5e76\u63a2\u8ba8\u51cf\u5c11\u6fc0\u6d3b\u4e13\u5bb6\u6570\u91cf\u4e0e\u4e13\u5bb6\u603b\u6570\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u51cf\u5c11\u6fc0\u6d3b\u4e13\u5bb6\u6570\u91cf\u53ef\u663e\u8457\u63d0\u5347\u6548\u7387\u4e14\u6027\u80fd\u635f\u5931\u8f83\u5c0f\uff1b\u51cf\u5c11\u4e13\u5bb6\u603b\u6570\u6548\u7387\u63d0\u5347\u6709\u9650\u4f46\u6027\u80fd\u635f\u5931\u4e25\u91cd\u3002\u65b9\u6cd5\u53ef\u5b9e\u73b0\u81f3\u5c1110%\u7684\u541e\u5410\u91cf\u63d0\u5347\u4e14\u65e0\u6027\u80fd\u635f\u5931\u3002", "conclusion": "MoE\u63a8\u7406\u4f18\u5316\u4ecd\u5177\u5de8\u5927\u63a2\u7d22\u4e0e\u6539\u8fdb\u6f5c\u529b\uff0c\u7ec6\u7c92\u5ea6MoE\u6a21\u578b\u5728\u6548\u7387\u4e0e\u6027\u80fd\u7684\u6743\u8861\u4e2d\u8868\u73b0\u51fa\u72ec\u7279\u4f18\u52bf\u3002"}}
{"id": "2505.03678", "pdf": "https://arxiv.org/pdf/2505.03678", "abs": "https://arxiv.org/abs/2505.03678", "authors": ["Walter Didimo", "Fabrizio Montecchiani", "Tommaso Piselli"], "title": "Graph Drawing for LLMs: An Empirical Evaluation", "categories": ["cs.AI"], "comment": null, "summary": "Our work contributes to the fast-growing literature on the use of Large\nLanguage Models (LLMs) to perform graph-related tasks. In particular, we focus\non usage scenarios that rely on the visual modality, feeding the model with a\ndrawing of the graph under analysis. We investigate how the model's performance\nis affected by the chosen layout paradigm, the aesthetics of the drawing, and\nthe prompting technique used for the queries. We formulate three corresponding\nresearch questions and present the results of a thorough experimental analysis.\nOur findings reveal that choosing the right layout paradigm and optimizing the\nreadability of the input drawing from a human perspective can significantly\nimprove the performance of the model on the given task. Moreover, selecting the\nmost effective prompting technique is a challenging yet crucial task for\nachieving optimal performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u56fe\u76f8\u5173\u4efb\u52a1\u65f6\uff0c\u8f93\u5165\u56fe\u7684\u53ef\u89c6\u5316\u5e03\u5c40\u3001\u7f8e\u89c2\u6027\u53ca\u63d0\u793a\u6280\u672f\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u4f18\u5316\u56fe\u7684\u5e03\u5c40\u548c\u63d0\u793a\u6280\u672f\uff0c\u63d0\u5347LLMs\u5728\u56fe\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4e0d\u540c\u5e03\u5c40\u8303\u5f0f\u3001\u7ed8\u56fe\u7f8e\u5b66\u548c\u63d0\u793a\u6280\u672f\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u4e2a\u7814\u7a76\u95ee\u9898\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u9009\u62e9\u6b63\u786e\u7684\u5e03\u5c40\u8303\u5f0f\u548c\u4f18\u5316\u7ed8\u56fe\u53ef\u8bfb\u6027\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u793a\u6280\u672f\u7684\u9009\u62e9\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u4f18\u5316\u56fe\u7684\u89c6\u89c9\u8f93\u5165\u548c\u63d0\u793a\u6280\u672f\u662f\u63d0\u5347LLMs\u5728\u56fe\u4efb\u52a1\u4e2d\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2505.03205", "pdf": "https://arxiv.org/pdf/2505.03205", "abs": "https://arxiv.org/abs/2505.03205", "authors": ["Zhaiming Shen", "Alex Havrilla", "Rongjie Lai", "Alexander Cloninger", "Wenjing Liao"], "title": "Transformers for Learning on Noisy and Task-Level Manifolds: Approximation and Generalization Insights", "categories": ["cs.LG", "cs.NA", "math.NA", "math.ST", "stat.TH"], "comment": null, "summary": "Transformers serve as the foundational architecture for large language and\nvideo generation models, such as GPT, BERT, SORA and their successors.\nEmpirical studies have demonstrated that real-world data and learning tasks\nexhibit low-dimensional structures, along with some noise or measurement error.\nThe performance of transformers tends to depend on the intrinsic dimension of\nthe data/tasks, though theoretical understandings remain largely unexplored for\ntransformers. This work establishes a theoretical foundation by analyzing the\nperformance of transformers for regression tasks involving noisy input data on\na manifold. Specifically, the input data are in a tubular neighborhood of a\nmanifold, while the ground truth function depends on the projection of the\nnoisy data onto the manifold. We prove approximation and generalization errors\nwhich crucially depend on the intrinsic dimension of the manifold. Our results\ndemonstrate that transformers can leverage low-complexity structures in\nlearning task even when the input data are perturbed by high-dimensional noise.\nOur novel proof technique constructs representations of basic arithmetic\noperations by transformers, which may hold independent interest.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4e3aTransformer\u5728\u5e26\u566a\u6570\u636e\u7684\u6d41\u5f62\u56de\u5f52\u4efb\u52a1\u4e2d\u5efa\u7acb\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u53d1\u73b0\u5176\u6027\u80fd\u4f9d\u8d56\u4e8e\u6570\u636e\u7684\u5185\u5728\u7ef4\u5ea6\uff0c\u5373\u4f7f\u8f93\u5165\u6570\u636e\u53d7\u5230\u9ad8\u7ef4\u566a\u58f0\u5e72\u6270\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u6570\u636e\u548c\u5b66\u4e60\u4efb\u52a1\u901a\u5e38\u5177\u6709\u4f4e\u7ef4\u7ed3\u6784\uff0c\u540c\u65f6\u4f34\u968f\u566a\u58f0\u6216\u6d4b\u91cf\u8bef\u5dee\u3002\u5c3d\u7ba1Transformer\u5728\u5b9e\u9645\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u7406\u8bba\u7406\u89e3\u5c1a\u4e0d\u5b8c\u5584\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7814\u7a76Transformer\u5728\u5e26\u566a\u6570\u636e\u6d41\u5f62\u4e0a\u7684\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8f93\u5165\u6570\u636e\u4e3a\u6d41\u5f62\u7ba1\u72b6\u90bb\u57df\u7684\u60c5\u51b5\uff0c\u7814\u7a76Transformer\u5982\u4f55\u4ece\u5e26\u566a\u6570\u636e\u4e2d\u5b66\u4e60\u4f9d\u8d56\u4e8e\u6d41\u5f62\u6295\u5f71\u7684\u771f\u5b9e\u51fd\u6570\u3002\u91c7\u7528\u65b0\u9896\u7684\u8bc1\u660e\u6280\u672f\u6784\u5efaTransformer\u5bf9\u57fa\u672c\u7b97\u672f\u8fd0\u7b97\u7684\u8868\u793a\u3002", "result": "\u8bba\u6587\u8bc1\u660e\u4e86Transformer\u7684\u8fd1\u4f3c\u548c\u6cdb\u5316\u8bef\u5dee\u4f9d\u8d56\u4e8e\u6d41\u5f62\u7684\u5185\u5728\u7ef4\u5ea6\uff0c\u8868\u660e\u5373\u4f7f\u8f93\u5165\u6570\u636e\u53d7\u9ad8\u7ef4\u566a\u58f0\u5e72\u6270\uff0cTransformer\u4ecd\u53ef\u5229\u7528\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u4f4e\u590d\u6742\u6027\u7ed3\u6784\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3aTransformer\u5728\u5e26\u566a\u6570\u636e\u4efb\u52a1\u4e2d\u7684\u7406\u8bba\u8868\u73b0\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5176\u8bc1\u660e\u6280\u672f\u53ef\u80fd\u5177\u6709\u72ec\u7acb\u7684\u7814\u7a76\u4ef7\u503c\u3002"}}
{"id": "2505.03563", "pdf": "https://arxiv.org/pdf/2505.03563", "abs": "https://arxiv.org/abs/2505.03563", "authors": ["Cl\u00e9a Chataigner", "Rebecca Ma", "Prakhar Ganesh", "Afaf Ta\u00efk", "Elliot Creager", "Golnoosh Farnadi"], "title": "Say It Another Way: A Framework for User-Grounded Paraphrasing", "categories": ["cs.CL"], "comment": null, "summary": "Small changes in how a prompt is worded can lead to meaningful differences in\nthe behavior of large language models (LLMs), raising concerns about the\nstability and reliability of their evaluations. While prior work has explored\nsimple formatting changes, these rarely capture the kinds of natural variation\nseen in real-world language use. We propose a controlled paraphrasing framework\nbased on a taxonomy of minimal linguistic transformations to systematically\ngenerate natural prompt variations. Using the BBQ dataset, we validate our\nmethod with both human annotations and automated checks, then use it to study\nhow LLMs respond to paraphrased prompts in stereotype evaluation tasks. Our\nanalysis shows that even subtle prompt modifications can lead to substantial\nchanges in model behavior. These results highlight the need for robust,\nparaphrase-aware evaluation protocols.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u5c0f\u8bed\u8a00\u8f6c\u6362\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u63a7\u5236\u6027\u6539\u8ff0\u751f\u6210\u81ea\u7136\u7684\u63d0\u793a\u53d8\u4f53\uff0c\u9a8c\u8bc1LLMs\u5728\u523b\u677f\u5370\u8c61\u8bc4\u4f30\u4e2d\u7684\u54cd\u5e94\u5dee\u5f02\uff0c\u7ed3\u679c\u663e\u793a\u7ec6\u5fae\u63d0\u793a\u4fee\u6539\u53ef\u663e\u8457\u6539\u53d8\u6a21\u578b\u884c\u4e3a\u3002", "motivation": "\u63a2\u7d22\u63d0\u793a\u63aa\u8f9e\u53d8\u5316\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u81ea\u7136\u8bed\u8a00\u53d8\u4f53\u4e0b\u7684\u7a33\u5b9a\u6027\u4e0e\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8bed\u8a00\u8f6c\u6362\u5206\u7c7b\u7684\u6846\u67b6\uff0c\u751f\u6210\u81ea\u7136\u7684\u63d0\u793a\u53d8\u4f53\uff0c\u4f7f\u7528BBQ\u6570\u636e\u96c6\u5e76\u901a\u8fc7\u4eba\u5de5\u548c\u81ea\u52a8\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5373\u4f7f\u7ec6\u5fae\u7684\u63d0\u793a\u4fee\u6539\u4e5f\u4f1a\u663e\u8457\u5f71\u54cd\u6a21\u578b\u884c\u4e3a\u3002", "conclusion": "\u5f3a\u8c03\u9700\u8981\u5f00\u53d1\u5bf9\u6539\u8ff0\u654f\u611f\u7684\u7a33\u5065\u8bc4\u4f30\u534f\u8bae\u3002"}}
{"id": "2505.02841", "pdf": "https://arxiv.org/pdf/2505.02841", "abs": "https://arxiv.org/abs/2505.02841", "authors": ["Marco Masera", "Alessandro Leone", "Johannes K\u00f6ster", "Ivan Molineris"], "title": "Snakemaker: Seamlessly transforming ad-hoc analyses into sustainable Snakemake workflows with generative AI", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Reproducibility and sustainability present significant challenges in\nbioinformatics software development, where rapidly evolving tools and complex\nworkflows often result in short-lived or difficult-to-adapt pipelines. This\npaper introduces Snakemaker, a tool that leverages generative AI to facilitate\nresearchers build sustainable data analysis pipelines by converting\nunstructured code into well-defined Snakemake workflows. Snakemaker\nnon-invasively tracks the work performed in the terminal by the researcher,\nanalyzes execution patterns, and generates Snakemake workflows that can be\nintegrated into existing pipelines. Snakemaker also supports the transformation\nof monolithic Ipython Notebooks into modular Snakemake pipelines, resolving the\nglobal state of the notebook into discrete, file-based interactions between\nrules. An integrated chat assistant provides users with fine-grained control\nthrough natural language instructions. Snakemaker generates high-quality\nSnakemake workflows by adhering to the best practices, including Conda\nenvironment tracking, generic rule generation and loop unrolling. By lowering\nthe barrier between prototype and production-quality code, Snakemaker addresses\na critical gap in computational reproducibility for bioinformatics research.", "AI": {"tldr": "Snakemaker\u662f\u4e00\u4e2a\u5229\u7528\u751f\u6210\u5f0fAI\u5c06\u975e\u7ed3\u6784\u5316\u4ee3\u7801\u8f6c\u6362\u4e3aSnakemake\u5de5\u4f5c\u6d41\u7a0b\u7684\u5de5\u5177\uff0c\u65e8\u5728\u63d0\u5347\u751f\u7269\u4fe1\u606f\u5b66\u8f6f\u4ef6\u7684\u53ef\u91cd\u590d\u6027\u548c\u53ef\u6301\u7eed\u6027\u3002", "motivation": "\u751f\u7269\u4fe1\u606f\u5b66\u8f6f\u4ef6\u5f00\u53d1\u548c\u590d\u6742\u5de5\u4f5c\u6d41\u7684\u5feb\u901f\u6f14\u53d8\u5e38\u5bfc\u81f4\u5de5\u5177\u96be\u4ee5\u6301\u7eed\u6216\u9002\u5e94\u3002Snakemaker\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u5347\u8ba1\u7b97\u7684\u53ef\u91cd\u590d\u6027\u3002", "method": "Snakemaker\u901a\u8fc7\u8ddf\u8e2a\u7ec8\u7aef\u6267\u884c\u8bb0\u5f55\u548c\u5206\u6790\u6267\u884c\u6a21\u5f0f\uff0c\u751f\u6210\u7b26\u5408\u6700\u4f73\u5b9e\u8df5\u7684Snakemake\u5de5\u4f5c\u6d41\u3002\u652f\u6301\u5c06Ipython Notebook\u8f6c\u6362\u4e3a\u6a21\u5757\u5316\u6d41\u7a0b\uff0c\u5e76\u901a\u8fc7\u804a\u5929\u52a9\u624b\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u63a7\u5236\u3002", "result": "Snakemaker\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684Snakemake\u5de5\u4f5c\u6d41\uff0c\u5305\u62ec\u8ddf\u8e2aConda\u73af\u5883\u3001\u901a\u7528\u89c4\u5219\u751f\u6210\u548c\u5faa\u73af\u5c55\u5f00\u3002", "conclusion": "\u901a\u8fc7\u964d\u4f4e\u539f\u578b\u4ee3\u7801\u4e0e\u751f\u4ea7\u7ea7\u4ee3\u7801\u4e4b\u95f4\u7684\u95e8\u69db\uff0cSnakemaker\u89e3\u51b3\u4e86\u751f\u7269\u4fe1\u606f\u5b66\u7814\u7a76\u4e2d\u8ba1\u7b97\u53ef\u91cd\u590d\u6027\u7684\u5173\u952e\u95ee\u9898\u3002"}}
{"id": "2505.03207", "pdf": "https://arxiv.org/pdf/2505.03207", "abs": "https://arxiv.org/abs/2505.03207", "authors": ["Yutong Xie", "Fuchao Yang", "Yuheng Jia"], "title": "Partial Label Clustering", "categories": ["cs.LG"], "comment": "Accepted by IJCAI 2025", "summary": "Partial label learning (PLL) is a significant weakly supervised learning\nframework, where each training example corresponds to a set of candidate labels\nand only one label is the ground-truth label. For the first time, this paper\ninvestigates the partial label clustering problem, which takes advantage of the\nlimited available partial labels to improve the clustering performance.\nSpecifically, we first construct a weight matrix of examples based on their\nrelationships in the feature space and disambiguate the candidate labels to\nestimate the ground-truth label based on the weight matrix. Then, we construct\na set of must-link and cannot-link constraints based on the disambiguation\nresults. Moreover, we propagate the initial must-link and cannot-link\nconstraints based on an adversarial prior promoted dual-graph learning\napproach. Finally, we integrate weight matrix construction, label\ndisambiguation, and pairwise constraints propagation into a joint model to\nachieve mutual enhancement. We also theoretically prove that a better\ndisambiguated label matrix can help improve clustering performance.\nComprehensive experiments demonstrate our method realizes superior performance\nwhen comparing with state-of-the-art constrained clustering methods, and\noutperforms PLL and semi-supervised PLL methods when only limited samples are\nannotated. The code is publicly available at https://github.com/xyt-ml/PLC.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7814\u7a76\u4e86\u90e8\u5206\u6807\u7b7e\u805a\u7c7b\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u6709\u9650\u7684\u6807\u7b7e\u4fe1\u606f\u63d0\u5347\u805a\u7c7b\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7279\u5f81\u7a7a\u95f4\u5173\u7cfb\u3001\u6807\u7b7e\u6d88\u6b67\u548c\u5bf9\u5076\u56fe\u5b66\u4e60\u7684\u8054\u5408\u6a21\u578b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u90e8\u5206\u6807\u7b7e\u5b66\u4e60\uff08PLL\uff09\u4e2d\u6bcf\u4e2a\u8bad\u7ec3\u6837\u672c\u6709\u4e00\u7ec4\u5019\u9009\u6807\u7b7e\u4f46\u4ec5\u4e00\u4e2a\u771f\u5b9e\u6807\u7b7e\uff0c\u4f5c\u8005\u5e0c\u671b\u5229\u7528\u8fd9\u4e9b\u6709\u9650\u7684\u90e8\u5206\u6807\u7b7e\u4fe1\u606f\u63d0\u5347\u805a\u7c7b\u6027\u80fd\u3002", "method": "1. \u57fa\u4e8e\u7279\u5f81\u7a7a\u95f4\u5173\u7cfb\u6784\u5efa\u6837\u672c\u6743\u91cd\u77e9\u9635\uff0c\u6d88\u6b67\u5019\u9009\u6807\u7b7e\u4f30\u8ba1\u771f\u5b9e\u6807\u7b7e\uff1b2. \u57fa\u4e8e\u6d88\u6b67\u7ed3\u679c\u6784\u5efa\u5fc5\u987b\u94fe\u63a5\u548c\u4e0d\u53ef\u94fe\u63a5\u7ea6\u675f\uff1b3. \u901a\u8fc7\u5bf9\u6297\u6027\u5148\u9a8c\u4fc3\u8fdb\u7684\u53cc\u56fe\u5b66\u4e60\u4f20\u64ad\u7ea6\u675f\uff1b4. \u5c06\u6743\u91cd\u77e9\u9635\u3001\u6807\u7b7e\u6d88\u6b67\u548c\u7ea6\u675f\u4f20\u64ad\u6574\u5408\u4e3a\u8054\u5408\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u6807\u6ce8\u6837\u672c\u4e0b\u4f18\u4e8e\u73b0\u6709\u7684\u7ea6\u675f\u805a\u7c7b\u65b9\u6cd5\u3001PLL\u53ca\u534a\u76d1\u7763PLL\u65b9\u6cd5\u3002", "conclusion": "\u8bba\u6587\u8bc1\u660e\u4e86\u66f4\u597d\u7684\u6807\u7b7e\u6d88\u6b67\u80fd\u63d0\u5347\u805a\u7c7b\u6027\u80fd\uff0c\u63d0\u51fa\u7684\u8054\u5408\u6a21\u578b\u5728\u591a\u4efb\u52a1\u534f\u540c\u4e2d\u5b9e\u73b0\u4e86\u6027\u80fd\u7a81\u7834\u3002"}}
{"id": "2505.03675", "pdf": "https://arxiv.org/pdf/2505.03675", "abs": "https://arxiv.org/abs/2505.03675", "authors": ["Anuja Tayal", "Devika Salunke", "Barbara Di Eugenio", "Paula G Allen-Meares", "Eulalia P Abril", "Olga Garcia-Bedoya", "Carolyn A Dickens", "Andrew D. Boyd"], "title": "Towards conversational assistants for health applications: using ChatGPT to generate conversations about heart failure", "categories": ["cs.CL"], "comment": null, "summary": "We explore the potential of ChatGPT (3.5-turbo and 4) to generate\nconversations focused on self-care strategies for African-American heart\nfailure patients -- a domain with limited specialized datasets. To simulate\npatient-health educator dialogues, we employed four prompting strategies:\ndomain, African American Vernacular English (AAVE), Social Determinants of\nHealth (SDOH), and SDOH-informed reasoning. Conversations were generated across\nkey self-care domains of food, exercise, and fluid intake, with varying turn\nlengths (5, 10, 15) and incorporated patient-specific SDOH attributes such as\nage, gender, neighborhood, and socioeconomic status. Our findings show that\neffective prompt design is essential. While incorporating SDOH and reasoning\nimproves dialogue quality, ChatGPT still lacks the empathy and engagement\nneeded for meaningful healthcare communication.", "AI": {"tldr": "\u7814\u7a76\u4e86ChatGPT\u4e3a\u975e\u6d32\u88d4\u7f8e\u56fd\u5fc3\u8870\u60a3\u8005\u751f\u6210\u81ea\u6211\u62a4\u7406\u5bf9\u8bdd\u7684\u6f5c\u529b\uff0c\u53d1\u73b0\u6709\u6548\u63d0\u793a\u8bbe\u8ba1\u5173\u952e\uff0c\u4f46\u6a21\u578b\u4ecd\u7f3a\u4e4f\u5171\u60c5\u548c\u4e92\u52a8\u3002", "motivation": "\u586b\u8865\u975e\u6d32\u88d4\u7f8e\u56fd\u5fc3\u8870\u60a3\u8005\u81ea\u6211\u62a4\u7406\u5bf9\u8bdd\u6570\u636e\u7684\u7a7a\u767d\uff0c\u63d0\u5347\u533b\u7597\u5bf9\u8bdd\u8d28\u91cf\u3002", "method": "\u4f7f\u7528\u56db\u79cd\u63d0\u793a\u7b56\u7565\uff08\u9886\u57df\u3001AAVE\u3001SDOH\u3001SDOH\u63a8\u7406\uff09\uff0c\u751f\u6210\u5bf9\u8bdd\u5e76\u8bc4\u4f30\u5176\u6548\u679c\u3002", "result": "SDOH\u548c\u63a8\u7406\u63d0\u793a\u6539\u5584\u5bf9\u8bdd\u8d28\u91cf\uff0c\u4f46ChatGPT\u4ecd\u7f3a\u4e4f\u5171\u60c5\u548c\u4e92\u52a8\u80fd\u529b\u3002", "conclusion": "\u63d0\u793a\u8bbe\u8ba1\u5bf9\u751f\u6210\u533b\u7597\u5bf9\u8bdd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u589e\u5f3a\u5171\u60c5\u548c\u4e92\u52a8\u3002"}}
{"id": "2505.02843", "pdf": "https://arxiv.org/pdf/2505.02843", "abs": "https://arxiv.org/abs/2505.02843", "authors": ["Miriam Cobo", "David Corral Fontecha", "Wilson Silva", "Lara Lloret Iglesias"], "title": "Physical foundations for trustworthy medical imaging: a review for artificial intelligence researchers", "categories": ["eess.IV", "cs.AI", "cs.CV", "physics.med-ph"], "comment": "17 pages, 2 figures", "summary": "Artificial intelligence in medical imaging has seen unprecedented growth in\nthe last years, due to rapid advances in deep learning and computing resources.\nApplications cover the full range of existing medical imaging modalities, with\nunique characteristics driven by the physics of each technique. Yet, artificial\nintelligence professionals entering the field, and even experienced developers,\noften lack a comprehensive understanding of the physical principles underlying\nmedical image acquisition, which hinders their ability to fully leverage its\npotential. The integration of physics knowledge into artificial intelligence\nalgorithms enhances their trustworthiness and robustness in medical imaging,\nespecially in scenarios with limited data availability. In this work, we review\nthe fundamentals of physics in medical images and their impact on the latest\nadvances in artificial intelligence, particularly, in generative models and\nreconstruction algorithms. Finally, we explore the integration of physics\nknowledge into physics-inspired machine learning models, which leverage\nphysics-based constraints to enhance the learning of medical imaging features.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u533b\u5b66\u5f71\u50cf\u4e2d\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u5f3a\u8c03\u4e86\u7269\u7406\u77e5\u8bc6\u5728\u63d0\u5347AI\u7b97\u6cd5\u53ef\u4fe1\u5ea6\u548c\u9c81\u68d2\u6027\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u6709\u9650\u7684\u573a\u666f\u4e0b\u3002", "motivation": "\u533b\u5b66\u5f71\u50cfAI\u5feb\u901f\u53d1\u5c55\uff0c\u4f46AI\u4e13\u4e1a\u4eba\u58eb\u5e38\u7f3a\u4e4f\u5bf9\u533b\u5b66\u5f71\u50cf\u7269\u7406\u539f\u7406\u7684\u5168\u9762\u7406\u89e3\uff0c\u9650\u5236\u4e86\u7b97\u6cd5\u6f5c\u529b\u7684\u53d1\u6325\u3002", "method": "\u56de\u987e\u533b\u5b66\u5f71\u50cf\u7269\u7406\u57fa\u7840\u53ca\u5176\u5bf9AI\u6280\u672f\uff08\u5982\u751f\u6210\u6a21\u578b\u548c\u91cd\u5efa\u7b97\u6cd5\uff09\u7684\u5f71\u54cd\uff0c\u63a2\u7d22\u7269\u7406\u77e5\u8bc6\u5982\u4f55\u878d\u5165\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u7269\u7406\u542f\u53d1\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u80fd\u66f4\u597d\u5730\u5b66\u4e60\u533b\u5b66\u5f71\u50cf\u7279\u5f81\uff0c\u63d0\u5347\u7b97\u6cd5\u6027\u80fd\u3002", "conclusion": "\u7269\u7406\u77e5\u8bc6\u7684\u878d\u5165\u662f\u589e\u5f3a\u533b\u5b66\u5f71\u50cfAI\u53ef\u4fe1\u5ea6\u548c\u9c81\u68d2\u6027\u7684\u5173\u952e\uff0c\u5c24\u5176\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2505.03209", "pdf": "https://arxiv.org/pdf/2505.03209", "abs": "https://arxiv.org/abs/2505.03209", "authors": ["Borui Wang", "Kathleen McKeown", "Rex Ying"], "title": "DYSTIL: Dynamic Strategy Induction with Large Language Models for Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning from expert demonstrations has long remained a\nchallenging research problem, and existing state-of-the-art methods using\nbehavioral cloning plus further RL training often suffer from poor\ngeneralization, low sample efficiency, and poor model interpretability.\nInspired by the strong reasoning abilities of large language models (LLMs), we\npropose a novel strategy-based reinforcement learning framework integrated with\nLLMs called DYnamic STrategy Induction with Llms for reinforcement learning\n(DYSTIL) to overcome these limitations. DYSTIL dynamically queries a\nstrategy-generating LLM to induce textual strategies based on advantage\nestimations and expert demonstrations, and gradually internalizes induced\nstrategies into the RL agent through policy optimization to improve its\nperformance through boosting policy generalization and enhancing sample\nefficiency. It also provides a direct textual channel to observe and interpret\nthe evolution of the policy's underlying strategies during training. We test\nDYSTIL over challenging RL environments from Minigrid and BabyAI, and\nempirically demonstrate that DYSTIL significantly outperforms state-of-the-art\nbaseline methods by 17.75% in average success rate while also enjoying higher\nsample efficiency during the learning process.", "AI": {"tldr": "DYSTIL\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u6587\u672c\u7b56\u7565\u5e76\u4f18\u5316\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u548c\u6837\u672c\u6548\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u4e13\u5bb6\u6f14\u793a\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u5dee\u3001\u6837\u672c\u6548\u7387\u4f4e\u4e14\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u5dee\u3002DYSTIL\u901a\u8fc7\u5229\u7528LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u52a8\u6001\u751f\u6210\u7b56\u7565\uff0c\u4ee5\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\u3002", "method": "DYSTIL\u52a8\u6001\u67e5\u8be2\u7b56\u7565\u751f\u6210LLM\uff0c\u57fa\u4e8e\u4f18\u52bf\u4f30\u8ba1\u548c\u4e13\u5bb6\u6f14\u793a\u751f\u6210\u6587\u672c\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u7b56\u7565\u4f18\u5316\u9010\u6b65\u5c06\u8fd9\u4e9b\u7b56\u7565\u5185\u5316\u5230RL\u667a\u80fd\u4f53\u4e2d\u3002", "result": "\u5728Minigrid\u548cBabyAI\u7684\u6311\u6218\u6027\u73af\u5883\u4e2d\uff0cDYSTIL\u6bd4\u73b0\u6709\u65b9\u6cd5\u7684\u5e73\u5747\u6210\u529f\u7387\u63d0\u5347\u4e8617.75%\uff0c\u4e14\u6837\u672c\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "DYSTIL\u901a\u8fc7\u7ed3\u5408LLM\u7684\u52a8\u6001\u7b56\u7565\u751f\u6210\u548c\u7b56\u7565\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u7b56\u7565\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2505.03688", "pdf": "https://arxiv.org/pdf/2505.03688", "abs": "https://arxiv.org/abs/2505.03688", "authors": ["Sharvi Endait", "Ruturaj Ghatage", "Aditya Kulkarni", "Rajlaxmi Patil", "Raviraj Joshi"], "title": "IndicSQuAD: A Comprehensive Multilingual Question Answering Dataset for Indic Languages", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The rapid progress in question-answering (QA) systems has predominantly\nbenefited high-resource languages, leaving Indic languages largely\nunderrepresented despite their vast native speaker base. In this paper, we\npresent IndicSQuAD, a comprehensive multi-lingual extractive QA dataset\ncovering nine major Indic languages, systematically derived from the SQuAD\ndataset. Building on previous work with MahaSQuAD for Marathi, our approach\nadapts and extends translation techniques to maintain high linguistic fidelity\nand accurate answer-span alignment across diverse languages. IndicSQuAD\ncomprises extensive training, validation, and test sets for each language,\nproviding a robust foundation for model development. We evaluate baseline\nperformances using language-specific monolingual BERT models and the\nmultilingual MuRIL-BERT. The results indicate some challenges inherent in\nlow-resource settings. Moreover, our experiments suggest potential directions\nfor future work, including expanding to additional languages, developing\ndomain-specific datasets, and incorporating multimodal data. The dataset and\nmodels are publicly shared at https://github.com/l3cube-pune/indic-nlp", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86IndicSQuAD\uff0c\u4e00\u4e2a\u6db5\u76d6\u4e5d\u79cd\u4e3b\u8981\u5370\u5ea6\u8bed\u8a00\u7684\u591a\u8bed\u8a00\u62bd\u53d6\u5f0f\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u5370\u5ea6\u8bed\u8a00\u5728\u95ee\u7b54\u7cfb\u7edf\u4e2d\u7684\u8d44\u6e90\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u9ad8\u8d44\u6e90\u8bed\u8a00\u5728\u95ee\u7b54\u7cfb\u7edf\u65b9\u9762\u53d6\u5f97\u4e86\u5feb\u901f\u8fdb\u5c55\uff0c\u4f46\u5370\u5ea6\u8bed\u8a00\u7531\u4e8e\u5176\u8d44\u6e90\u532e\u4e4f\u800c\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u5c3d\u7ba1\u5b83\u4eec\u6709\u5927\u91cf\u7684\u6bcd\u8bed\u4f7f\u7528\u8005\u3002\u672c\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u6784\u5efaIndicSQuAD\u6570\u636e\u96c6\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u4eceSQuAD\u6570\u636e\u96c6\u7cfb\u7edf\u6027\u5730\u7ffb\u8bd1\u548c\u6269\u5c55\uff0c\u6784\u5efa\u4e86\u6db5\u76d6\u4e5d\u79cd\u5370\u5ea6\u8bed\u8a00\u7684QA\u6570\u636e\u96c6IndicSQuAD\uff0c\u5e76\u786e\u4fdd\u4e86\u8bed\u8a00\u5fe0\u5b9e\u5ea6\u548c\u7b54\u6848\u8303\u56f4\u7684\u51c6\u786e\u6027\u3002\u4f7f\u7528\u8bed\u8a00\u7279\u5b9a\u7684\u5355\u8bedBERT\u6a21\u578b\u548c\u591a\u8bed\u8a00MuRIL-BERT\u8bc4\u4f30\u57fa\u7ebf\u6027\u80fd\u3002", "result": "\u57fa\u7ebf\u7ed3\u679c\u663e\u793a\uff0c\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u5b58\u5728\u4e00\u4e9b\u6311\u6218\u3002\u5b9e\u9a8c\u7ed3\u679c\u8fd8\u6307\u51fa\u4e86\u672a\u6765\u5de5\u4f5c\u7684\u6f5c\u5728\u65b9\u5411\uff0c\u5305\u62ec\u6269\u5c55\u5230\u66f4\u591a\u8bed\u8a00\u3001\u5f00\u53d1\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\u548c\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u3002", "conclusion": "IndicSQuAD\u4e3a\u5370\u5ea6\u8bed\u8a00\u7684\u95ee\u7b54\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u5fc5\u8981\u6027\u3002\u6570\u636e\u96c6\u548c\u6a21\u578b\u5df2\u516c\u5f00\u5206\u4eab\u3002"}}
{"id": "2505.02846", "pdf": "https://arxiv.org/pdf/2505.02846", "abs": "https://arxiv.org/abs/2505.02846", "authors": ["Kim Kaivanto"], "title": "The Precautionary Principle and the Innovation Principle: Incompatible Guides for AI Innovation Governance?", "categories": ["cs.CY", "cs.AI", "econ.GN", "q-fin.EC"], "comment": "47 pages", "summary": "In policy debates concerning the governance and regulation of Artificial\nIntelligence (AI), both the Precautionary Principle (PP) and the Innovation\nPrinciple (IP) are advocated by their respective interest groups. Do these\nprinciples offer wholly incompatible and contradictory guidance? Does one\nnecessarily negate the other? I argue here that provided attention is\nrestricted to weak-form PP and IP, the answer to both of these questions is\n\"No.\" The essence of these weak formulations is the requirement to fully\naccount for type-I error costs arising from erroneously preventing the\ninnovation's diffusion through society (i.e. mistaken regulatory red-lighting)\nas well as the type-II error costs arising from erroneously allowing the\ninnovation to diffuse through society (i.e. mistaken regulatory\ngreen-lighting). Within the Signal Detection Theory (SDT) model developed here,\nweak-PP red-light (weak-IP green-light) determinations are optimal for\nsufficiently small (large) ratios of expected type-I to type-II error costs.\nFor intermediate expected cost ratios, an amber-light 'wait-and-monitor' policy\nis optimal. Regulatory sandbox instruments allow AI testing and experimentation\nto take place within a structured environment of limited duration and societal\nscale, whereby the expected cost ratio falls within the 'wait-and-monitor'\nrange. Through sandboxing regulators and innovating firms learn more about the\nexpected cost ratio, and what respective adaptations -- of regulation, of\ntechnical solution, of business model, or combination thereof, if any -- are\nneeded to keep the ratio out of the weak-PP red-light zone.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u6cbb\u7406\u4e2d\u7684\u9884\u9632\u539f\u5219\u4e0e\u521b\u65b0\u539f\u5219\u7684\u517c\u5bb9\u6027\uff0c\u63d0\u51fa\u901a\u8fc7\u4fe1\u53f7\u68c0\u6d4b\u7406\u8bba\u6a21\u578b\u548c\u76d1\u7ba1\u6c99\u76d2\u5b9e\u73b0\u4e24\u8005\u7684\u8c03\u548c\u3002", "motivation": "\u89e3\u51b3AI\u6cbb\u7406\u4e2d\u9884\u9632\u539f\u5219\uff08PP\uff09\u4e0e\u521b\u65b0\u539f\u5219\uff08IP\uff09\u662f\u5426\u5bf9\u7acb\u7684\u95ee\u9898\uff0c\u8bc1\u660e\u4e24\u8005\u5728\u5f31\u5f62\u5f0f\u4e0b\u53ef\u534f\u540c\u3002", "method": "\u5f15\u5165\u4fe1\u53f7\u68c0\u6d4b\u7406\u8bba\uff08SDT\uff09\u6a21\u578b\uff0c\u5206\u6790\u7c7b\u578bI\u548cII\u9519\u8bef\u6210\u672c\uff0c\u63d0\u51fa\u76d1\u7ba1\u6c99\u76d2\u4f5c\u4e3a\u4e2d\u95f4\u7b56\u7565\u3002", "result": "\u76d1\u7ba1\u6c99\u76d2\u5de5\u5177\u80fd\u52a8\u6001\u8c03\u6574\u9884\u671f\u6210\u672c\u6bd4\u4f8b\uff0c\u907f\u514d\u6781\u7aef\u7ea2\u7eff\u706f\u51b3\u7b56\uff0c\u5b9e\u73b0\u4f18\u5316\u76d1\u7ba1\u3002", "conclusion": "\u5f31\u5f62\u5f0fPP\u4e0eIP\u53ef\u901a\u8fc7SDT\u6a21\u578b\u548c\u6c99\u76d2\u76d1\u7ba1\u5b9e\u73b0\u517c\u5bb9\uff0c\u4e3aAI\u6cbb\u7406\u63d0\u4f9b\u7075\u6d3b\u6846\u67b6\u3002"}}
{"id": "2505.03230", "pdf": "https://arxiv.org/pdf/2505.03230", "abs": "https://arxiv.org/abs/2505.03230", "authors": ["Yue Chen", "Hui Kang", "Jiahui Li", "Geng Su", "Boxiong Wang", "Jiacheng Wang", "Cong Liang", "Shuang Liang", "Dusit Niyato"], "title": "Joint Resource Management for Energy-efficient UAV-assisted SWIPT-MEC: A Deep Reinforcement Learning Approach", "categories": ["cs.LG"], "comment": null, "summary": "The integration of simultaneous wireless information and power transfer\n(SWIPT) technology in 6G Internet of Things (IoT) networks faces significant\nchallenges in remote areas and disaster scenarios where ground infrastructure\nis unavailable. This paper proposes a novel unmanned aerial vehicle\n(UAV)-assisted mobile edge computing (MEC) system enhanced by directional\nantennas to provide both computational resources and energy support for ground\nIoT terminals. However, such systems require multiple trade-off policies to\nbalance UAV energy consumption, terminal battery levels, and computational\nresource allocation under various constraints, including limited UAV battery\ncapacity, non-linear energy harvesting characteristics, and dynamic task\narrivals. To address these challenges comprehensively, we formulate a\nbi-objective optimization problem that simultaneously considers system energy\nefficiency and terminal battery sustainability. We then reformulate this\nnon-convex problem with a hybrid solution space as a Markov decision process\n(MDP) and propose an improved soft actor-critic (SAC) algorithm with an action\nsimplification mechanism to enhance its convergence and generalization\ncapabilities. Simulation results have demonstrated that our proposed approach\noutperforms various baselines in different scenarios, achieving efficient\nenergy management while maintaining high computational performance.\nFurthermore, our method shows strong generalization ability across different\nscenarios, particularly in complex environments, validating the effectiveness\nof our designed boundary penalty and charging reward mechanisms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u4eba\u673a\u7684\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\u7cfb\u7edf\uff0c\u7ed3\u5408\u5b9a\u5411\u5929\u7ebf\u6280\u672f\uff0c\u4e3a6G\u7269\u8054\u7f51\u7ec8\u7aef\u63d0\u4f9b\u8ba1\u7b97\u8d44\u6e90\u548c\u80fd\u91cf\u652f\u6301\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4f18\u5316\u80fd\u91cf\u6548\u7387\u548c\u7ec8\u7aef\u7535\u6c60\u53ef\u6301\u7eed\u6027\u3002", "motivation": "\u9488\u5bf96G\u7269\u8054\u7f51\u5728\u57fa\u7840\u8bbe\u65bd\u7f3a\u5931\u573a\u666f\uff08\u5982\u504f\u8fdc\u5730\u533a\u6216\u707e\u96be\u73af\u5883\uff09\u4e2d\u65e0\u7ebf\u4fe1\u606f\u4e0e\u80fd\u91cf\u540c\u65f6\u4f20\u8f93\u7684\u6311\u6218\uff0c\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u65e0\u4eba\u673a\u8f85\u52a9\u7cfb\u7edf\u5e73\u8861\u80fd\u91cf\u6d88\u8017\u3001\u7ec8\u7aef\u7535\u6c60\u5bff\u547d\u548c\u8d44\u6e90\u5206\u914d\u3002", "method": "\u901a\u8fc7\u53cc\u76ee\u6807\u4f18\u5316\u95ee\u9898\u5efa\u6a21\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u6539\u8fdb\u8f6f\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7b97\u6cd5\uff08SAC\uff09\uff0c\u52a0\u5165\u52a8\u4f5c\u7b80\u5316\u673a\u5236\u4ee5\u63d0\u9ad8\u6536\u655b\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u573a\u666f\u4e0b\u4f18\u4e8e\u57fa\u7ebf\u7b97\u6cd5\uff0c\u80fd\u591f\u9ad8\u6548\u7ba1\u7406\u80fd\u91cf\u5e76\u4fdd\u6301\u9ad8\u8ba1\u7b97\u6027\u80fd\uff0c\u4e14\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u8f85\u52a9\u7cfb\u7edf\u7684\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u8fb9\u754c\u60e9\u7f5a\u548c\u5145\u7535\u5956\u52b1\u673a\u5236\u7684\u8bbe\u8ba1\u6709\u6548\u6027\u3002"}}
{"id": "2505.03711", "pdf": "https://arxiv.org/pdf/2505.03711", "abs": "https://arxiv.org/abs/2505.03711", "authors": ["Baharul Islam", "Nasim Ahmad", "Ferdous Ahmed Barbhuiya", "Kuntal Dey"], "title": "NBF at SemEval-2025 Task 5: Light-Burst Attention Enhanced System for Multilingual Subject Recommendation", "categories": ["cs.CL"], "comment": null, "summary": "We present our system submission for SemEval 2025 Task 5, which focuses on\ncross-lingual subject classification in the English and German academic\ndomains. Our approach leverages bilingual data during training, employing\nnegative sampling and a margin-based retrieval objective. We demonstrate that a\ndimension-as-token self-attention mechanism designed with significantly reduced\ninternal dimensions can effectively encode sentence embeddings for subject\nretrieval. In quantitative evaluation, our system achieved an average recall\nrate of 32.24% in the general quantitative setting (all subjects), 43.16% and\n31.53% of the general qualitative evaluation methods with minimal GPU usage,\nhighlighting their competitive performance. Our results demonstrate that our\napproach is effective in capturing relevant subject information under resource\nconstraints, although there is still room for improvement.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9SemEval 2025 Task 5\u7684\u8de8\u8bed\u8a00\u5b66\u79d1\u5206\u7c7b\u7cfb\u7edf\uff0c\u901a\u8fc7\u53cc\u8bed\u6570\u636e\u8bad\u7ec3\u3001\u8d1f\u91c7\u6837\u548c\u57fa\u4e8e\u8fb9\u7f18\u7684\u68c0\u7d22\u76ee\u6807\uff0c\u4f7f\u7528\u7ef4\u5ea6\u6807\u8bb0\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u7f16\u7801\u53e5\u5b50\u5d4c\u5165\uff0c\u5728\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u7ade\u4e89\u6027\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u3002", "motivation": "\u89e3\u51b3\u8de8\u8bed\u8a00\u5b66\u79d1\u5206\u7c7b\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u82f1\u8bed\u548c\u5fb7\u8bed\u5b66\u672f\u9886\u57df\u4e2d\uff0c\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u9ad8\u6548\u6355\u83b7\u76f8\u5173\u5b66\u79d1\u4fe1\u606f\u3002", "method": "\u91c7\u7528\u4e86\u53cc\u8bed\u6570\u636e\u8bad\u7ec3\uff0c\u7ed3\u5408\u8d1f\u91c7\u6837\u548c\u57fa\u4e8e\u8fb9\u7f18\u7684\u68c0\u7d22\u76ee\u6807\uff0c\u8bbe\u8ba1\u4e86\u7ef4\u5ea6\u6807\u8bb0\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u51cf\u5c11\u5185\u90e8\u7ef4\u5ea6\u5e76\u9ad8\u6548\u7f16\u7801\u53e5\u5b50\u5d4c\u5165\u3002", "result": "\u7cfb\u7edf\u5728\u4e00\u822c\u5b9a\u91cf\u8bbe\u7f6e\u4e2d\u5e73\u5747\u53ec\u56de\u7387\u4e3a32.24%\uff0c\u5728\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5206\u522b\u4e3a43.16%\u548c31.53%\uff0c\u4e14GPU\u4f7f\u7528\u7387\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8d44\u6e90\u6709\u9650\u6761\u4ef6\u4e0b\u6709\u6548\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2505.03280", "pdf": "https://arxiv.org/pdf/2505.03280", "abs": "https://arxiv.org/abs/2505.03280", "authors": ["Vansh Kapoor", "Jayakrishnan Nair"], "title": "MDPs with a State Sensing Cost", "categories": ["cs.LG"], "comment": "14 pages", "summary": "In many practical sequential decision-making problems, tracking the state of\nthe environment incurs a sensing/communication/computation cost. In these\nsettings, the agent's interaction with its environment includes the additional\ncomponent of deciding $\\textit{when}$ to sense the state, in a manner that\nbalances the value associated with optimal (state-specific) actions and the\ncost of sensing. We formulate this as an expected discounted cost Markov\nDecision Process (MDP), wherein the agent incurs an additional cost for sensing\nits next state, but has the option to take actions while remaining 'blind' to\nthe system state.\n  We pose this problem as a classical discounted cost MDP with an expanded\n(countably infinite) state space. While computing the optimal policy for this\nMDP is intractable in general, we bound the sub-optimality gap associated with\noptimal policies in a restricted class, where the number of consecutive\nnon-sensing (a.k.a., blind) actions is capped. We also design a computationally\nefficient heuristic algorithm based on policy improvement, which in practice\nperforms close to the optimal policy. Finally, we benchmark against the state\nof the art via a numerical case study.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u611f\u77e5\u6210\u672c\u5b58\u5728\u65f6\u7684\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0c\u6269\u5c55\u4e86\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "motivation": "\u5b9e\u9645\u51b3\u7b56\u95ee\u9898\u4e2d\uff0c\u611f\u77e5\u73af\u5883\u72b6\u6001\u4f1a\u5e26\u6765\u6210\u672c\uff0c\u9700\u8981\u6743\u8861\u611f\u77e5\u4e0e\u884c\u52a8\u7684\u4ef7\u503c\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u6269\u5c55\u7684MDP\uff0c\u9650\u5236\u8fde\u7eed\u975e\u611f\u77e5\u52a8\u4f5c\u7684\u6570\u91cf\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u7b56\u7565\u6539\u8fdb\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "result": "\u7406\u8bba\u4e0a\u754c\u5b9a\u4e86\u7b56\u7565\u6b21\u4f18\u6027\u5dee\u8ddd\uff0c\u5b9e\u8df5\u4e2d\u7b97\u6cd5\u8868\u73b0\u63a5\u8fd1\u6700\u4f18\u7b56\u7565\u3002", "conclusion": "\u901a\u8fc7\u6570\u503c\u6848\u4f8b\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e3a\u9ad8\u6210\u672c\u611f\u77e5\u573a\u666f\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.03733", "pdf": "https://arxiv.org/pdf/2505.03733", "abs": "https://arxiv.org/abs/2505.03733", "authors": ["Zimu Lu", "Yunqiao Yang", "Houxing Ren", "Haotian Hou", "Han Xiao", "Ke Wang", "Weikang Shi", "Aojun Zhou", "Mingjie Zhan", "Hongsheng Li"], "title": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch", "categories": ["cs.CL"], "comment": null, "summary": "LLM-based agents have demonstrated great potential in generating and managing\ncode within complex codebases. In this paper, we introduce WebGen-Bench, a\nnovel benchmark designed to measure an LLM-based agent's ability to create\nmulti-file website codebases from scratch. It contains diverse instructions for\nwebsite generation, created through the combined efforts of human annotators\nand GPT-4o. These instructions span three major categories and thirteen minor\ncategories, encompassing nearly all important types of web applications. To\nassess the quality of the generated websites, we use GPT-4o to generate test\ncases targeting each functionality described in the instructions, and then\nmanually filter, adjust, and organize them to ensure accuracy, resulting in 647\ntest cases. Each test case specifies an operation to be performed on the\nwebsite and the expected result after the operation. To automate testing and\nimprove reproducibility, we employ a powerful web-navigation agent to execute\ntests on the generated websites and determine whether the observed responses\nalign with the expected results. We evaluate three high-performance code-agent\nframeworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and\nopen-source LLMs as engines. The best-performing combination, Bolt.diy powered\nby DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting\nthe challenging nature of our benchmark. Additionally, we construct\nWebGen-Instruct, a training set consisting of 6,667 website-generation\ninstructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories\ngenerated from a subset of this training set achieves an accuracy of 38.2\\%,\nsurpassing the performance of the best proprietary model.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86WebGen-Bench\uff0c\u4e00\u4e2a\u8861\u91cfLLM\u4ee3\u7406\u4ece\u96f6\u521b\u5efa\u591a\u6587\u4ef6\u7f51\u7ad9\u4ee3\u7801\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b\u591a\u6837\u5316\u7684\u7f51\u7ad9\u751f\u6210\u6307\u4ee4\u548c647\u4e2a\u6d4b\u8bd5\u7528\u4f8b\u3002\u8bc4\u4f30\u4e2d\uff0c\u6700\u4f73\u7ec4\u5408\u51c6\u786e\u7387\u4ec5\u4e3a27.8%\uff0c\u7a81\u663e\u4e86\u57fa\u51c6\u7684\u6311\u6218\u6027\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u8bad\u7ec3\u6570\u636e\u96c6WebGen-Instruct\uff0cQwen2.5-Coder-32B-Instruct\u7684\u51c6\u786e\u7387\u63d0\u5347\u81f338.2%\uff0c\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "motivation": "LLM\u4ee3\u7406\u5728\u590d\u6742\u4ee3\u7801\u5e93\u4e2d\u751f\u6210\u548c\u7ba1\u7406\u4ee3\u7801\u7684\u80fd\u529b\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u8861\u91cf\u5176\u4ece\u96f6\u521b\u5efa\u591a\u6587\u4ef6\u7f51\u7ad9\u80fd\u529b\u7684\u57fa\u51c6\u3002", "method": "\u63d0\u51faWebGen-Bench\u57fa\u51c6\uff0c\u5305\u542b\u591a\u6837\u5316\u7684\u7f51\u7ad9\u751f\u6210\u6307\u4ee4\u548c647\u4e2a\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5e76\u5229\u7528\u81ea\u52a8\u5316\u6d4b\u8bd5\u5de5\u5177\u8bc4\u4f30\u4e09\u79cd\u9ad8\u6027\u80fd\u4ee3\u7801\u4ee3\u7406\u6846\u67b6\u7684\u6027\u80fd\u3002", "result": "\u6700\u4f73\u7ec4\u5408\uff08Bolt.diy+DeepSeek-R1\uff09\u5728\u6d4b\u8bd5\u7528\u4f8b\u4e2d\u7684\u51c6\u786e\u7387\u4ec5\u4e3a27.8%\uff0c\u800c\u901a\u8fc7\u8bad\u7ec3\u6570\u636e\u96c6WebGen-Instruct\u8bad\u7ec3\u7684Qwen2.5-Coder-32B-Instruct\u51c6\u786e\u7387\u8fbe\u523038.2%\u3002", "conclusion": "WebGen-Bench\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86LLM\u4ee3\u7406\u5728\u7f51\u7ad9\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u901a\u8fc7\u9488\u5bf9\u6027\u8bad\u7ec3\u63d0\u5347\u6027\u80fd\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2505.02848", "pdf": "https://arxiv.org/pdf/2505.02848", "abs": "https://arxiv.org/abs/2505.02848", "authors": ["Kexin Ding", "Mu Zhou", "Akshay Chaudhari", "Shaoting Zhang", "Dimitris N. Metaxas"], "title": "Aligning Large Language Models with Healthcare Stakeholders: A Pathway to Trustworthy AI Integration", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "The wide exploration of large language models (LLMs) raises the awareness of\nalignment between healthcare stakeholder preferences and model outputs. This\nalignment becomes a crucial foundation to empower the healthcare workflow\neffectively, safely, and responsibly. Yet the varying behaviors of LLMs may not\nalways match with healthcare stakeholders' knowledge, demands, and values. To\nenable a human-AI alignment, healthcare stakeholders will need to perform\nessential roles in guiding and enhancing the performance of LLMs. Human\nprofessionals must participate in the entire life cycle of adopting LLM in\nhealthcare, including training data curation, model training, and inference. In\nthis review, we discuss the approaches, tools, and applications of alignments\nbetween healthcare stakeholders and LLMs. We demonstrate that LLMs can better\nfollow human values by properly enhancing healthcare knowledge integration,\ntask understanding, and human guidance. We provide outlooks on enhancing the\nalignment between humans and LLMs to build trustworthy real-world healthcare\napplications.", "AI": {"tldr": "\u603b\u7ed3: \u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5728\u533b\u7597\u5065\u5eb7\u9886\u57df\u5b9e\u73b0\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u4e0e\u5229\u76ca\u76f8\u5173\u8005\u9700\u6c42\u7684\u5bf9\u63a5\uff0c\u5f3a\u8c03\u4eba\u7c7b\u4e13\u4e1a\u4eba\u58eb\u5728\u6574\u4e2a\u751f\u547d\u5468\u671f\u4e2d\u7684\u53c2\u4e0e\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u5b9e\u7528\u6027\u4e0e\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u786e\u4fddLLM\u7684\u8f93\u51fa\u4e0e\u533b\u7597\u5065\u5eb7\u9886\u57df\u7684\u9700\u6c42\u3001\u77e5\u8bc6\u548c\u4ef7\u503c\u89c2\u5bf9\u9f50\uff0c\u4ee5\u5b89\u5168\u3001\u9ad8\u6548\u5730\u652f\u6301\u533b\u7597\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u533b\u7597\u5229\u76ca\u76f8\u5173\u8005\u5728LLM\u7684\u6574\u4e2a\u751f\u547d\u5468\u671f(\u5982\u6570\u636e\u6536\u96c6\u3001\u6a21\u578b\u8bad\u7ec3\u548c\u63a8\u7406)\u4e2d\u7684\u53c2\u4e0e\uff0c\u4ee5\u53ca\u533b\u7597\u77e5\u8bc6\u6574\u5408\u3001\u4efb\u52a1\u7406\u89e3\u548c\u4eba\u5de5\u5f15\u5bfc\u6280\u672f\u7684\u5e94\u7528\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u533b\u7597\u77e5\u8bc6\u6574\u5408\u548c\u4eba\u5de5\u5f15\u5bfc\uff0cLLM\u80fd\u66f4\u597d\u5730\u9075\u5faa\u4eba\u7c7b\u4ef7\u503c\u89c2\uff0c\u589e\u5f3a\u6a21\u578b\u4e0e\u533b\u7597\u9700\u6c42\u7684\u5bf9\u9f50\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u672a\u6765\u9700\u8981\u52a0\u5f3a\u4eba\u7c7b\u4e0eLLM\u4e4b\u95f4\u7684\u5bf9\u9f50\uff0c\u4ee5\u5f00\u53d1\u66f4\u503c\u5f97\u4fe1\u8d56\u7684\u5b9e\u9645\u533b\u7597\u5e94\u7528\u3002"}}
{"id": "2505.03281", "pdf": "https://arxiv.org/pdf/2505.03281", "abs": "https://arxiv.org/abs/2505.03281", "authors": ["Zhou Wu", "Junyi An", "Baile Xu", "Furao Shen", "Jian Zhao"], "title": "Physics-inspired Energy Transition Neural Network for Sequence Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recently, the superior performance of Transformers has made them a more\nrobust and scalable solution for sequence modeling than traditional recurrent\nneural networks (RNNs). However, the effectiveness of Transformer in capturing\nlong-term dependencies is primarily attributed to their comprehensive\npair-modeling process rather than inherent inductive biases toward sequence\nsemantics. In this study, we explore the capabilities of pure RNNs and reassess\ntheir long-term learning mechanisms. Inspired by the physics energy transition\nmodels that track energy changes over time, we propose a effective recurrent\nstructure called the``Physics-inspired Energy Transition Neural Network\"\n(PETNN). We demonstrate that PETNN's memory mechanism effectively stores\ninformation over long-term dependencies. Experimental results indicate that\nPETNN outperforms transformer-based methods across various sequence tasks.\nFurthermore, owing to its recurrent nature, PETNN exhibits significantly lower\ncomplexity. Our study presents an optimal foundational recurrent architecture\nand highlights the potential for developing effective recurrent neural networks\nin fields currently dominated by Transformer.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPETNN\u7684\u65b0\u578b\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\uff0c\u53d7\u7269\u7406\u80fd\u91cf\u8f6c\u6362\u6a21\u578b\u542f\u53d1\uff0c\u65e8\u5728\u89e3\u51b3Transformer\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\u3002\u5b9e\u9a8c\u8bc1\u660ePETNN\u5728\u591a\u79cd\u5e8f\u5217\u4efb\u52a1\u4e2d\u4f18\u4e8eTransformer\uff0c\u4e14\u590d\u6742\u5ea6\u66f4\u4f4e\u3002", "motivation": "\u5c3d\u7ba1Transformer\u5728\u5e8f\u5217\u5efa\u6a21\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u4f46\u5176\u957f\u4f9d\u8d56\u6355\u6349\u80fd\u529b\u4e3b\u8981\u6e90\u4e8e\u590d\u6742\u7684\u914d\u5bf9\u5efa\u6a21\u800c\u975e\u56fa\u6709\u7684\u5e8f\u5217\u8bed\u4e49\u504f\u7f6e\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u8005\u63a2\u7d22\u7eafRNN\u7684\u6f5c\u529b\uff0c\u5e76\u91cd\u65b0\u8bc4\u4f30\u5176\u957f\u65f6\u5b66\u4e60\u673a\u5236\u3002", "method": "\u53d7\u7269\u7406\u80fd\u91cf\u8f6c\u6362\u6a21\u578b\u7684\u542f\u53d1\uff0c\u63d0\u51faPETNN\uff0c\u901a\u8fc7\u6a21\u62df\u80fd\u91cf\u53d8\u5316\u6765\u8bbe\u8ba1\u5176\u8bb0\u5fc6\u673a\u5236\uff0c\u4ee5\u9ad8\u6548\u5b58\u50a8\u957f\u65f6\u4f9d\u8d56\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u663e\u793aPETNN\u5728\u591a\u79cd\u5e8f\u5217\u4efb\u52a1\u4e2d\u8d85\u8d8aTransformer\u65b9\u6cd5\uff0c\u540c\u65f6\u56e0\u5faa\u73af\u7279\u6027\u5177\u5907\u663e\u8457\u66f4\u4f4e\u7684\u590d\u6742\u5ea6\u3002", "conclusion": "PETNN\u4e3a\u4e00\u79cd\u9ad8\u6548\u7684\u5faa\u73af\u67b6\u6784\uff0c\u5c55\u793a\u4e86\u7eafRNN\u5728\u5f53\u524dTransformer\u4e3b\u5bfc\u9886\u57df\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.03739", "pdf": "https://arxiv.org/pdf/2505.03739", "abs": "https://arxiv.org/abs/2505.03739", "authors": ["Zuwei Long", "Yunhang Shen", "Chaoyou Fu", "Heting Gao", "Lijiang Li", "Peixian Chen", "Mengdan Zhang", "Hang Shao", "Jian Li", "Jinlong Peng", "Haoyu Cao", "Ke Li", "Rongrong Ji", "Xing Sun"], "title": "VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model", "categories": ["cs.CL", "cs.AI"], "comment": "Training and Inference Codes: https://github.com/VITA-MLLM/VITA-Audio", "summary": "With the growing requirement for natural human-computer interaction,\nspeech-based systems receive increasing attention as speech is one of the most\ncommon forms of daily communication. However, the existing speech models still\nexperience high latency when generating the first audio token during streaming,\nwhich poses a significant bottleneck for deployment. To address this issue, we\npropose VITA-Audio, an end-to-end large speech model with fast audio-text token\ngeneration. Specifically, we introduce a lightweight Multiple Cross-modal Token\nPrediction (MCTP) module that efficiently generates multiple audio tokens\nwithin a single model forward pass, which not only accelerates the inference\nbut also significantly reduces the latency for generating the first audio in\nstreaming scenarios. In addition, a four-stage progressive training strategy is\nexplored to achieve model acceleration with minimal loss of speech quality. To\nour knowledge, VITA-Audio is the first multi-modal large language model capable\nof generating audio output during the first forward pass, enabling real-time\nconversational capabilities with minimal latency. VITA-Audio is fully\nreproducible and is trained on open-source data only. Experimental results\ndemonstrate that our model achieves an inference speedup of 3~5x at the 7B\nparameter scale, but also significantly outperforms open-source models of\nsimilar model size on multiple benchmarks for automatic speech recognition\n(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.", "AI": {"tldr": "VITA-Audio \u662f\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u5927\u578b\u8bed\u97f3\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u4ee4\u724c\u9884\u6d4b\u6a21\u5757\u548c\u56db\u9636\u6bb5\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u97f3\u9891\u6d41\u5f0f\u4f20\u8f93\u4e2d\u7684\u5ef6\u8fdf\uff0c\u5e76\u52a0\u901f\u4e86\u63a8\u7406\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u6a21\u578b\u5728\u6d41\u5f0f\u4f20\u8f93\u65f6\u751f\u6210\u7b2c\u4e00\u4e2a\u97f3\u9891\u4ee4\u724c\u7684\u5ef6\u8fdf\u8f83\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f15\u5165\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u4ee4\u724c\u9884\u6d4b\u6a21\u5757\uff08MCTP\uff09\uff0c\u5728\u4e00\u6b21\u524d\u5411\u4f20\u9012\u4e2d\u751f\u6210\u591a\u4e2a\u97f3\u9891\u4ee4\u724c\uff0c\u5e76\u91c7\u7528\u56db\u9636\u6bb5\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u57287B\u53c2\u6570\u89c4\u6a21\u4e0b\uff0cVITA-Audio\u5b9e\u73b0\u4e863~5\u500d\u7684\u63a8\u7406\u52a0\u901f\uff0c\u5e76\u5728\u8bed\u97f3\u8bc6\u522b\u3001\u6587\u672c\u8f6c\u8bed\u97f3\u548c\u53e3\u8bed\u95ee\u7b54\u4efb\u52a1\u4e0a\u8d85\u8d8a\u540c\u7c7b\u5f00\u6e90\u6a21\u578b\u3002", "conclusion": "VITA-Audio \u662f\u9996\u4e2a\u80fd\u5728\u9996\u6b21\u524d\u5411\u4f20\u9012\u4e2d\u751f\u6210\u97f3\u9891\u8f93\u51fa\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6d41\u5f0f\u4f20\u8f93\u7684\u5ef6\u8fdf\u3002"}}
{"id": "2505.02849", "pdf": "https://arxiv.org/pdf/2505.02849", "abs": "https://arxiv.org/abs/2505.02849", "authors": ["Mohsen Balavar", "Wenli Yang", "David Herbert", "Soonja Yeom"], "title": "Enhancing tutoring systems by leveraging tailored promptings and domain knowledge with Large Language Models", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Recent advancements in artificial intelligence (AI) and machine learning have\nreignited interest in their impact on Computer-based Learning (CBL). AI-driven\ntools like ChatGPT and Intelligent Tutoring Systems (ITS) have enhanced\nlearning experiences through personalisation and flexibility. ITSs can adapt to\nindividual learning needs and provide customised feedback based on a student's\nperformance, cognitive state, and learning path. Despite these advances,\nchallenges remain in accommodating diverse learning styles and delivering\nreal-time, context-aware feedback. Our research aims to address these gaps by\nintegrating skill-aligned feedback via Retrieval Augmented Generation (RAG)\ninto prompt engineering for Large Language Models (LLMs) and developing an\napplication to enhance learning through personalised tutoring in a computer\nscience programming context. The pilot study evaluated a proposed system using\nthree quantitative metrics: readability score, response time, and feedback\ndepth, across three programming tasks of varying complexity. The system\nsuccessfully sorted simulated students into three skill-level categories and\nprovided context-aware feedback. This targeted approach demonstrated better\neffectiveness and adaptability compared to general methods.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u548c\u673a\u5668\u5b66\u4e60\u5728\u8ba1\u7b97\u673a\u8f85\u52a9\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u7ed3\u5408Retrieval Augmented Generation\uff08RAG\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5f00\u53d1\u4e2a\u6027\u5316\u8f85\u5bfc\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u591a\u6837\u5b66\u4e60\u98ce\u683c\u548c\u5b9e\u65f6\u53cd\u9988\u7684\u6311\u6218\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8ba1\u7b97\u673a\u8f85\u52a9\u5b66\u4e60\u4e2d\u591a\u6837\u5b66\u4e60\u98ce\u683c\u548c\u5b9e\u65f6\u53cd\u9988\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7AI\u6280\u672f\u63d0\u5347\u4e2a\u6027\u5316\u5b66\u4e60\u4f53\u9a8c\u3002", "method": "\u7814\u7a76\u91c7\u7528RAG\u7ed3\u5408LLMs\u7684\u63d0\u793a\u5de5\u7a0b\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e2a\u6027\u5316\u8f85\u5bfc\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u7f16\u7a0b\u4efb\u52a1\u7684\u5b9a\u91cf\u6307\u6807\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u7cfb\u7edf\u6210\u529f\u5c06\u6a21\u62df\u5b66\u751f\u6309\u6280\u80fd\u6c34\u5e73\u5206\u7c7b\uff0c\u5e76\u63d0\u4f9b\u4e0a\u4e0b\u6587\u611f\u77e5\u53cd\u9988\uff0c\u6bd4\u901a\u7528\u65b9\u6cd5\u66f4\u5177\u6548\u679c\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u7ed3\u5408RAG\u548cLLMs\u7684\u4e2a\u6027\u5316\u8f85\u5bfc\u7cfb\u7edf\u80fd\u6709\u6548\u63d0\u5347\u8ba1\u7b97\u673a\u8f85\u52a9\u5b66\u4e60\u7684\u6548\u679c\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2505.03323", "pdf": "https://arxiv.org/pdf/2505.03323", "abs": "https://arxiv.org/abs/2505.03323", "authors": ["Arthur Corr\u00eaa", "Alexandre Jesus", "Crist\u00f3v\u00e3o Silva", "Samuel Moniz"], "title": "Unraveling the Rainbow: can value-based methods schedule?", "categories": ["cs.LG"], "comment": null, "summary": "Recently, deep reinforcement learning has emerged as a promising approach for\nsolving complex combinatorial optimization problems. Broadly, deep\nreinforcement learning methods fall into two categories: policy-based and\nvalue-based. While value-based approaches have achieved notable success in\ndomains such as the Arcade Learning Environment, the combinatorial optimization\ncommunity has predominantly favored policy-based methods, often overlooking the\npotential of value-based algorithms. In this work, we conduct a comprehensive\nempirical evaluation of value-based algorithms, including the deep q-network\nand several of its advanced extensions, within the context of two complex\ncombinatorial problems: the job-shop and the flexible job-shop scheduling\nproblems, two fundamental challenges with multiple industrial applications. Our\nresults challenge the assumption that policy-based methods are inherently\nsuperior for combinatorial optimization. We show that several value-based\napproaches can match or even outperform the widely adopted proximal policy\noptimization algorithm, suggesting that value-based strategies deserve greater\nattention from the combinatorial optimization community. Our code is openly\navailable at: https://github.com/AJ-Correa/Unraveling-the-Rainbow.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\uff0c\u63ed\u793a\u5728\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\uff0c\u57fa\u4e8e\u4ef7\u503c\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u5982DQN\u53ca\u5176\u6269\u5c55\uff09\u53ef\u4ee5\u5ab2\u7f8e\u751a\u81f3\u4f18\u4e8e\u6d41\u884c\u7684\u57fa\u4e8e\u7b56\u7565\u7684\u65b9\u6cd5\uff08\u5982PPO\uff09\uff0c\u6311\u6218\u4e86\u540e\u8005\u5728\u9886\u57df\u5185\u7684\u4e3b\u5bfc\u5730\u4f4d\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u4ef7\u503c\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728Atari\u6e38\u620f\u7b49\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7ec4\u5408\u4f18\u5316\u9886\u57df\u4e3b\u8981\u91c7\u7528\u57fa\u4e8e\u7b56\u7565\u7684\u65b9\u6cd5\u3002\u4f5c\u8005\u8d28\u7591\u8fd9\u79cd\u504f\u597d\u662f\u5426\u5408\u7406\uff0c\u65e8\u5728\u8bc4\u4f30\u57fa\u4e8e\u4ef7\u503c\u65b9\u6cd5\u5728\u590d\u6742\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u5728\u4f5c\u4e1a\u8f66\u95f4\u8c03\u5ea6\u548c\u7075\u6d3b\u4f5c\u4e1a\u8f66\u95f4\u8c03\u5ea6\u4e24\u7c7b\u7ecf\u5178\u5de5\u4e1a\u95ee\u9898\u4e0a\uff0c\u7cfb\u7edf\u5bf9\u6bd4\u4e86\u6df1\u5ea6Q\u7f51\u7edc\uff08DQN\uff09\u53ca\u5176\u591a\u79cd\u6539\u8fdb\u7b97\u6cd5\u4e0e\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u7684\u8868\u73b0\uff0c\u5b9e\u9a8c\u8bbe\u8ba1\u4e25\u8c28\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u4ef7\u503c\u7684\u65b9\u6cd5\uff08\u5982Rainbow DQN\uff09\u5728\u4e24\u9879\u4efb\u52a1\u4e2d\u53ef\u8fbe\u5230\u4e0ePPO\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u6253\u7834\u2018\u57fa\u4e8e\u7b56\u7565\u65b9\u6cd5\u5fc5\u7136\u66f4\u4f18\u2019\u7684\u4f20\u7edf\u8ba4\u77e5\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u57fa\u4e8e\u4ef7\u503c\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u7ec4\u5408\u4f18\u5316\u9886\u57df\u5177\u6709\u88ab\u4f4e\u4f30\u7684\u6f5c\u529b\uff0c\u547c\u5401\u5b66\u754c\u91cd\u65b0\u5ba1\u89c6\u5176\u4ef7\u503c\u5e76\u6295\u5165\u66f4\u591a\u7814\u7a76\u5173\u6ce8\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u590d\u73b0\u3002"}}
{"id": "2505.03335", "pdf": "https://arxiv.org/pdf/2505.03335", "abs": "https://arxiv.org/abs/2505.03335", "authors": ["Andrew Zhao", "Yiran Wu", "Yang Yue", "Tong Wu", "Quentin Xu", "Yang Yue", "Matthieu Lin", "Shenzhi Wang", "Qingyun Wu", "Zilong Zheng", "Gao Huang"], "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in\nenhancing the reasoning capabilities of large language models by learning\ndirectly from outcome-based rewards. Recent RLVR works that operate under the\nzero setting avoid supervision in labeling the reasoning process, but still\ndepend on manually curated collections of questions and answers for training.\nThe scarcity of high-quality, human-produced examples raises concerns about the\nlong-term scalability of relying on human supervision, a challenge already\nevident in the domain of language model pretraining. Furthermore, in a\nhypothetical future where AI surpasses human intelligence, tasks provided by\nhumans may offer limited learning potential for a superintelligent system. To\naddress these concerns, we propose a new RLVR paradigm called Absolute Zero, in\nwhich a single model learns to propose tasks that maximize its own learning\nprogress and improves reasoning by solving them, without relying on any\nexternal data. Under this paradigm, we introduce the Absolute Zero Reasoner\n(AZR), a system that self-evolves its training curriculum and reasoning ability\nby using a code executor to both validate proposed code reasoning tasks and\nverify answers, serving as an unified source of verifiable reward to guide\nopen-ended yet grounded learning. Despite being trained entirely without\nexternal data, AZR achieves overall SOTA performance on coding and mathematical\nreasoning tasks, outperforming existing zero-setting models that rely on tens\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate\nthat AZR can be effectively applied across different model scales and is\ncompatible with various model classes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAbsolute Zero\u7684\u65b0RLVR\u8303\u5f0f\uff0c\u901a\u8fc7\u81ea\u6211\u751f\u6210\u4efb\u52a1\u5e76\u9a8c\u8bc1\u5956\u52b1\uff0c\u65e0\u9700\u5916\u90e8\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u5728\u6570\u5b66\u548c\u7f16\u7801\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff0c\u96be\u4ee5\u6269\u5c55\u4e14\u5728AI\u8d85\u8d8a\u4eba\u7c7b\u667a\u80fd\u65f6\u6f5c\u529b\u6709\u9650\u3002Absolute Zero\u901a\u8fc7\u81ea\u6211\u9a71\u52a8\u7684\u4efb\u52a1\u751f\u6210\u548c\u9a8c\u8bc1\uff0c\u89e3\u51b3\u4e86\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5f15\u5165Absolute Zero Reasoner\uff08AZR\uff09\uff0c\u5229\u7528\u4ee3\u7801\u6267\u884c\u5668\u81ea\u6211\u751f\u6210\u4efb\u52a1\u5e76\u9a8c\u8bc1\u7b54\u6848\uff0c\u5b9e\u73b0\u65e0\u5916\u90e8\u6570\u636e\u7684\u5f00\u653e\u5b66\u4e60\u3002", "result": "AZR\u5728\u6570\u5b66\u548c\u7f16\u7801\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f9d\u8d56\u5927\u91cf\u4eba\u5de5\u6570\u636e\u7684\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u9002\u7528\u4e8e\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u548c\u7c7b\u578b\u3002", "conclusion": "Absolute Zero\u4e3aRLVR\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u6570\u636e\u7684\u81ea\u6211\u9a71\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u5176\u5728\u8d85\u4eba\u7c7b\u667a\u80fd\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2505.03368", "pdf": "https://arxiv.org/pdf/2505.03368", "abs": "https://arxiv.org/abs/2505.03368", "authors": ["Stef De Sabbata", "Stefano Mizzaro", "Kevin Roitero"], "title": "Geospatial Mechanistic Interpretability of Large Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated unprecedented capabilities\nacross various natural language processing tasks. Their ability to process and\ngenerate viable text and code has made them ubiquitous in many fields, while\ntheir deployment as knowledge bases and \"reasoning\" tools remains an area of\nongoing research. In geography, a growing body of literature has been focusing\non evaluating LLMs' geographical knowledge and their ability to perform spatial\nreasoning. However, very little is still known about the internal functioning\nof these models, especially about how they process geographical information.\n  In this chapter, we establish a novel framework for the study of geospatial\nmechanistic interpretability - using spatial analysis to reverse engineer how\nLLMs handle geographical information. Our aim is to advance our understanding\nof the internal representations that these complex models generate while\nprocessing geographical information - what one might call \"how LLMs think about\ngeographic information\" if such phrasing was not an undue anthropomorphism.\n  We first outline the use of probing in revealing internal structures within\nLLMs. We then introduce the field of mechanistic interpretability, discussing\nthe superposition hypothesis and the role of sparse autoencoders in\ndisentangling polysemantic internal representations of LLMs into more\ninterpretable, monosemantic features. In our experiments, we use spatial\nautocorrelation to show how features obtained for placenames display spatial\npatterns related to their geographic location and can thus be interpreted\ngeospatially, providing insights into how these models process geographical\ninformation. We conclude by discussing how our framework can help shape the\nstudy and use of foundation models in geography.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5982\u4f55\u5904\u7406\u5730\u7406\u4fe1\u606f\uff0c\u901a\u8fc7\u7a7a\u95f4\u5206\u6790\u63ed\u793a\u5176\u5185\u90e8\u673a\u5236\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5904\u7406\u5730\u7406\u4fe1\u606f\u7684\u5185\u90e8\u673a\u5236\u5c1a\u4e0d\u660e\u786e\uff0c\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u4f5c\u8005\u7ed3\u5408\u63a2\u6d4b\u6280\u672f\u548c\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff08\u5982\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff09\uff0c\u901a\u8fc7\u7a7a\u95f4\u81ea\u76f8\u5173\u6027\u5206\u6790\u63ed\u793aLLMs\u5185\u90e8\u7684\u5730\u7406\u4fe1\u606f\u8868\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5730\u540d\u7279\u5f81\u7684\u7a7a\u95f4\u6a21\u5f0f\u4e0e\u5730\u7406\u4f4d\u7f6e\u76f8\u5173\uff0c\u63ed\u793a\u4e86LLMs\u5904\u7406\u5730\u7406\u4fe1\u606f\u7684\u673a\u5236\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5730\u7406\u5b66\u4e2d\u57fa\u7840\u6a21\u578b\u7684\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u4fc3\u8fdb\u4e86\u5bf9\u5176\u5185\u90e8\u5730\u7406\u4fe1\u606f\u5904\u7406\u673a\u5236\u7684\u7406\u89e3\u3002"}}
{"id": "2505.02931", "pdf": "https://arxiv.org/pdf/2505.02931", "abs": "https://arxiv.org/abs/2505.02931", "authors": ["Fernando Vallecillos Ruiz", "Max Hort", "Leon Moonen"], "title": "The Art of Repair: Optimizing Iterative Program Repair with Instruction-Tuned Models", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted for publication in the research track of the 29th\n  International Conference on Evaluation and Assessment in Software Engineering\n  (EASE), 17-20 June 2025, Istanbul, T\\\"urkiye", "summary": "Automatic program repair (APR) aims to reduce the manual efforts required to\nidentify and fix errors in source code. Before the rise of LLM-based agents, a\ncommon strategy was to increase the number of generated patches, sometimes to\nthe thousands, to achieve better repair results on benchmarks. More recently,\nself-iterative capabilities enabled LLMs to refine patches over multiple rounds\nguided by feedback. However, literature often focuses on many iterations and\ndisregards different numbers of outputs.\n  We investigate an APR pipeline that balances these two approaches, the\ngeneration of multiple outputs and multiple rounds of iteration, while imposing\na limit of 10 total patches per bug. We apply three SOTA instruction-tuned LLMs\n- DeepSeekCoder-Instruct, Codellama-Instruct, Llama3.1-Instruct - to the APR\ntask. We further fine-tune each model on an APR dataset with three sizes (1K,\n30K, 65K) and two techniques (Full Fine-Tuning and LoRA), allowing us to assess\ntheir repair capabilities on two APR benchmarks: HumanEval-Java and Defects4J.\n  Our results show that by using only a fraction (<1%) of the fine-tuning\ndataset, we can achieve improvements of up to 78% in the number of plausible\npatches generated, challenging prior studies that reported limited gains using\nFull Fine-Tuning. However, we find that exceeding certain thresholds leads to\ndiminishing outcomes, likely due to overfitting. Moreover, we show that base\nmodels greatly benefit from creating patches in an iterative fashion rather\nthan generating them all at once. In addition, the benefit of iterative\nstrategies becomes more pronounced in complex benchmarks. Even fine-tuned\nmodels, while benefiting less from iterations, still gain advantages,\nparticularly on complex benchmarks. The research underscores the need for\nbalanced APR strategies that combine multi-output generation and iterative\nrefinement.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5728\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\uff08APR\uff09\u4e2d\u5e73\u8861\u591a\u8f93\u51fa\u751f\u6210\u4e0e\u591a\u8f6e\u8fed\u4ee3\u4f18\u5316\uff0c\u4f7f\u7528\u5c11\u91cf\u5fae\u8c03\u6570\u636e\uff08<1%\uff09\u5373\u53ef\u663e\u8457\u63d0\u5347\u4fee\u590d\u6548\u679c\u3002", "motivation": "\u65e8\u5728\u51cf\u5c11\u624b\u52a8\u4fee\u590d\u4ee3\u7801\u9519\u8bef\u7684\u5de5\u4f5c\u91cf\uff0c\u5e76\u63a2\u7d22\u5728\u6709\u9650\u8865\u4e01\u6570\u91cf\uff0810\u4e2a/\u9519\u8bef\uff09\u4e0b\u5e73\u8861\u4e24\u79cd\u4e3b\u6d41APR\u7b56\u7565\uff08\u591a\u8f93\u51fa\u751f\u6210\u4e0e\u8fed\u4ee3\u4f18\u5316\uff09\u7684\u6548\u679c\u3002", "method": "\u91c7\u7528\u4e09\u79cd\u6307\u4ee4\u8c03\u4f18\u7684LLM\u6a21\u578b\uff08DeepSeekCoder-Instruct\u3001Codellama-Instruct\u3001Llama3.1-Instruct\uff09\uff0c\u5728\u4e09\u79cd\u6570\u636e\u96c6\u89c4\u6a21\uff081K\u300130K\u300165K\uff09\u548c\u4e24\u79cd\u5fae\u8c03\u6280\u672f\uff08\u5168\u5fae\u8c03\u4e0eLoRA\uff09\u4e0b\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5176\u5728HumanEval-Java\u548cDefects4J\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u4ec5\u7528\u5c11\u91cf\u5fae\u8c03\u6570\u636e\uff08<1%\uff09\u5373\u53ef\u5c06\u53ef\u884c\u8865\u4e01\u751f\u6210\u7387\u63d0\u534778%\uff1b\u8fed\u4ee3\u7b56\u7565\u5bf9\u57fa\u7840\u6a21\u578b\u6548\u679c\u663e\u8457\uff0c\u590d\u6742\u57fa\u51c6\u4e2d\u4f18\u52bf\u66f4\u660e\u663e\uff0c\u4f46\u8fc7\u5ea6\u5fae\u8c03\u4f1a\u56e0\u8fc7\u62df\u5408\u5bfc\u81f4\u6536\u76ca\u9012\u51cf\u3002", "conclusion": "\u9700\u7ed3\u5408\u591a\u8f93\u51fa\u751f\u6210\u4e0e\u8fed\u4ee3\u4f18\u5316\u7684\u5e73\u8861\u7b56\u7565\uff0c\u5fae\u8c03\u867d\u6709\u6548\u4f46\u9700\u8c28\u614e\uff0c\u8fed\u4ee3\u5bf9\u590d\u6742\u95ee\u9898\u5c24\u5176\u91cd\u8981\u3002"}}
{"id": "2505.02853", "pdf": "https://arxiv.org/pdf/2505.02853", "abs": "https://arxiv.org/abs/2505.02853", "authors": ["Francesco Balzan", "Pedro P. Santos", "Maurizio Gabbrielli", "Mahault Albarracin", "Manuel Lopes"], "title": "A Computational Model of Inclusive Pedagogy: From Understanding to Application", "categories": ["cs.CY", "cs.AI"], "comment": "This is a preprint version of a manuscript intended for submission to\n  the International Journal of Artificial Intelligence in Education (IJAIED)", "summary": "Human education transcends mere knowledge transfer, it relies on\nco-adaptation dynamics -- the mutual adjustment of teaching and learning\nstrategies between agents. Despite its centrality, computational models of\nco-adaptive teacher-student interactions (T-SI) remain underdeveloped. We argue\nthat this gap impedes Educational Science in testing and scaling contextual\ninsights across diverse settings, and limits the potential of Machine Learning\nsystems, which struggle to emulate and adaptively support human learning\nprocesses. To address this, we present a computational T-SI model that\nintegrates contextual insights on human education into a testable framework. We\nuse the model to evaluate diverse T-SI strategies in a realistic synthetic\nclassroom setting, simulating student groups with unequal access to sensory\ninformation. Results show that strategies incorporating co-adaptation\nprinciples (e.g., bidirectional agency) outperform unilateral approaches (i.e.,\nwhere only the teacher or the student is active), improving the learning\noutcomes for all learning types. Beyond the testing and scaling of\ncontext-dependent educational insights, our model enables hypothesis generation\nin controlled yet adaptable environments. This work bridges non-computational\ntheories of human education with scalable, inclusive AI in Education systems,\nproviding a foundation for equitable technologies that dynamically adapt to\nlearner needs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8ba1\u7b97\u6a21\u578b\uff0c\u6a21\u62df\u6559\u5e08-\u5b66\u751f\u4e92\u52a8\uff08T-SI\uff09\u4e2d\u7684\u5171\u9002\u5e94\u52a8\u6001\uff0c\u65e8\u5728\u586b\u8865\u6559\u80b2\u79d1\u5b66\u548c\u673a\u5668\u5b66\u4e60\u5728\u9002\u5e94\u6027\u5b66\u4e60\u652f\u6301\u65b9\u9762\u7684\u7814\u7a76\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u6559\u80b2\u79d1\u5b66\u7f3a\u4e4f\u8ba1\u7b97\u6a21\u578b\u6765\u6d4b\u8bd5\u548c\u6269\u5c55\u6559\u5e08-\u5b66\u751f\u4e92\u52a8\u7684\u60c5\u5883\u6d1e\u5bdf\uff0c\u540c\u65f6\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u96be\u4ee5\u6a21\u62df\u548c\u9002\u5e94\u6027\u652f\u6301\u4eba\u7c7b\u5b66\u4e60\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6574\u5408\u4eba\u7c7b\u6559\u80b2\u60c5\u5883\u6d1e\u5bdf\u7684T-SI\u8ba1\u7b97\u6a21\u578b\uff0c\u5e76\u5728\u5408\u6210\u8bfe\u5802\u73af\u5883\u4e2d\u8bc4\u4f30\u4e0d\u540cT-SI\u7b56\u7565\uff0c\u6a21\u62df\u4e0d\u540c\u611f\u5b98\u4fe1\u606f\u8bbf\u95ee\u80fd\u529b\u7684\u5b66\u751f\u7fa4\u4f53\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u91c7\u7528\u5171\u9002\u5e94\u539f\u5219\u7684\u7b56\u7565\uff08\u5982\u53cc\u5411\u4e3b\u52a8\u6027\uff09\u4f18\u4e8e\u5355\u8fb9\u7b56\u7565\uff08\u4ec5\u6559\u5e08\u6216\u5b66\u751f\u4e3b\u52a8\uff09\uff0c\u80fd\u63d0\u5347\u6240\u6709\u5b66\u4e60\u7c7b\u578b\u7684\u5b66\u4e60\u6548\u679c\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u6559\u80b2\u79d1\u5b66\u7684\u60c5\u5883\u6d1e\u5bdf\u63d0\u4f9b\u4e86\u53ef\u6d4b\u8bd5\u548c\u6269\u5c55\u7684\u5de5\u5177\uff0c\u540c\u65f6\u4e3a\u9002\u5e94\u6027\u6559\u80b2AI\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u5b9e\u73b0\u52a8\u6001\u9002\u5e94\u5b66\u4e60\u8005\u9700\u6c42\u7684\u516c\u5e73\u6280\u672f\u3002"}}
{"id": "2505.03373", "pdf": "https://arxiv.org/pdf/2505.03373", "abs": "https://arxiv.org/abs/2505.03373", "authors": ["Hanyu Hu", "Xiaoming Yuan"], "title": "SPAP: Structured Pruning via Alternating Optimization and Penalty Methods", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "The deployment of large language models (LLMs) is often constrained by their\nsubstantial computational and memory demands. While structured pruning presents\na viable approach by eliminating entire network components, existing methods\nsuffer from performance degradation, reliance on heuristic metrics, or\nexpensive finetuning. To address these challenges, we propose SPAP (Structured\nPruning via Alternating Optimization and Penalty Methods), a novel and\nefficient structured pruning framework for LLMs grounded in optimization\ntheory. SPAP formulates the pruning problem through a mixed-integer\noptimization model, employs a penalty method that effectively makes pruning\ndecisions to minimize pruning errors, and introduces an alternating\nminimization algorithm tailored to the splittable problem structure for\nefficient weight updates and performance recovery. Extensive experiments on\nOPT, LLaMA-3/3.1/3.2, and Qwen2.5 models demonstrate SPAP's superiority over\nstate-of-the-art methods, delivering linear inference speedups (1.29$\\times$ at\n30% sparsity) and proportional memory reductions. Our work offers a practical,\noptimization-driven solution for pruning LLMs while preserving model\nperformance.", "AI": {"tldr": "SPAP\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u7406\u8bba\u7684\u7ed3\u6784\u5316\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u548c\u60e9\u7f5a\u65b9\u6cd5\u6709\u6548\u51cf\u5c11LLMs\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\u5b58\u5728\u6027\u80fd\u4e0b\u964d\u3001\u4f9d\u8d56\u542f\u53d1\u5f0f\u6307\u6807\u6216\u5fae\u8c03\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u3001\u4f18\u5316\u9a71\u52a8\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "SPAP\u91c7\u7528\u6df7\u5408\u6574\u6570\u4f18\u5316\u6a21\u578b\uff0c\u7ed3\u5408\u60e9\u7f5a\u65b9\u6cd5\u51cf\u5c11\u526a\u679d\u8bef\u5dee\uff0c\u5e76\u8bbe\u8ba1\u4ea4\u66ff\u6700\u5c0f\u5316\u7b97\u6cd5\u4f18\u5316\u6743\u91cd\u66f4\u65b0\u548c\u6027\u80fd\u6062\u590d\u3002", "result": "\u5728\u591a\u4e2aLLM\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cSPAP\u572830%\u7a00\u758f\u5ea6\u4e0b\u5b9e\u73b01.29\u500d\u63a8\u7406\u52a0\u901f\u548c\u5185\u5b58\u7ebf\u6027\u51cf\u5c11\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SPAP\u4e3aLLM\u526a\u679d\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u4f18\u5316\u9a71\u52a8\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u4e14\u4e0d\u727a\u7272\u6027\u80fd\u3002"}}
{"id": "2505.03382", "pdf": "https://arxiv.org/pdf/2505.03382", "abs": "https://arxiv.org/abs/2505.03382", "authors": ["Matthias H\u00f6fler", "Francesco Regazzoni", "Stefano Pagani", "Elias Karabelas", "Christoph Augustin", "Gundolf Haase", "Gernot Plank", "Federica Caforio"], "title": "Physics-informed neural network estimation of active material properties in time-dependent cardiac biomechanical models", "categories": ["cs.LG", "cs.NA", "math.NA", "G.1.8; I.2.0; I.6.4; J.3"], "comment": null, "summary": "Active stress models in cardiac biomechanics account for the mechanical\ndeformation caused by muscle activity, thus providing a link between the\nelectrophysiological and mechanical properties of the tissue. The accurate\nassessment of active stress parameters is fundamental for a precise\nunderstanding of myocardial function but remains difficult to achieve in a\nclinical setting, especially when only displacement and strain data from\nmedical imaging modalities are available. This work investigates, through an\nin-silico study, the application of physics-informed neural networks (PINNs)\nfor inferring active contractility parameters in time-dependent cardiac\nbiomechanical models from these types of imaging data. In particular, by\nparametrising the sought state and parameter field with two neural networks,\nrespectively, and formulating an energy minimisation problem to search for the\noptimal network parameters, we are able to reconstruct in various settings\nactive stress fields in the presence of noise and with a high spatial\nresolution. To this end, we also advance the vanilla PINN learning algorithm\nwith the use of adaptive weighting schemes, ad-hoc regularisation strategies,\nFourier features, and suitable network architectures. In addition, we\nthoroughly analyse the influence of the loss weights in the reconstruction of\nactive stress parameters. Finally, we apply the method to the characterisation\nof tissue inhomogeneities and detection of fibrotic scars in myocardial tissue.\nThis approach opens a new pathway to significantly improve the diagnosis,\ntreatment planning, and management of heart conditions associated with cardiac\nfibrosis.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u4ece\u533b\u5b66\u5f71\u50cf\u6570\u636e\u4e2d\u63a8\u65ad\u5fc3\u810f\u751f\u7269\u529b\u5b66\u6a21\u578b\u4e2d\u7684\u4e3b\u52a8\u6536\u7f29\u53c2\u6570\uff0c\u901a\u8fc7\u4f18\u5316\u7f51\u7edc\u53c2\u6570\u5b9e\u73b0\u4e86\u9ad8\u7a7a\u95f4\u5206\u8fa8\u7387\u7684\u4e3b\u52a8\u5e94\u529b\u573a\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u4e34\u5e8a\u73af\u5883\u4e2d\u96be\u4ee5\u51c6\u786e\u8bc4\u4f30\u5fc3\u808c\u4e3b\u52a8\u5e94\u529b\u53c2\u6570\uff0c\u5c24\u5176\u662f\u4ec5\u4f9d\u9760\u533b\u5b66\u5f71\u50cf\u7684\u4f4d\u79fb\u548c\u5e94\u53d8\u6570\u636e\u65f6\u3002\u7814\u7a76\u65e8\u5728\u4ece\u8fd9\u4e9b\u6570\u636e\u4e2d\u63a8\u65ad\u53c2\u6570\uff0c\u4ece\u800c\u66f4\u597d\u5730\u7406\u89e3\u5fc3\u808c\u529f\u80fd\u3002", "method": "\u901a\u8fc7\u53c2\u6570\u5316\u72b6\u6001\u548c\u53c2\u6570\u573a\u4e3a\u4e24\u4e2a\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u6784\u5efa\u80fd\u91cf\u6700\u5c0f\u5316\u95ee\u9898\u4f18\u5316\u7f51\u7edc\u53c2\u6570\uff0c\u540c\u65f6\u7ed3\u5408\u81ea\u9002\u5e94\u6743\u91cd\u3001\u6b63\u5219\u5316\u7b56\u7565\u548c\u5085\u91cc\u53f6\u7279\u5f81\u7b49\u6539\u8fdb\u4f20\u7edfPINN\u7b97\u6cd5\u3002", "result": "\u5728\u5404\u79cd\u8bbe\u7f6e\u4e0b\u6210\u529f\u91cd\u5efa\u4e86\u5e26\u566a\u58f0\u7684\u4e3b\u52a8\u5e94\u529b\u573a\uff0c\u5e76\u5206\u6790\u4e86\u6743\u91cd\u5bf9\u53c2\u6570\u91cd\u5efa\u7684\u5f71\u54cd\uff0c\u8fdb\u4e00\u6b65\u5e94\u7528\u4e8e\u5fc3\u808c\u7ec4\u7ec7\u5f02\u8d28\u6027\u8868\u5f81\u548c\u7ea4\u7ef4\u5316\u7622\u75d5\u68c0\u6d4b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5fc3\u810f\u7ea4\u7ef4\u5316\u76f8\u5173\u75be\u75c5\u7684\u8bca\u65ad\u548c\u6cbb\u7597\u89c4\u5212\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2505.02856", "pdf": "https://arxiv.org/pdf/2505.02856", "abs": "https://arxiv.org/abs/2505.02856", "authors": ["Mahir Akgun", "Hadi Hosseini"], "title": "AI Education in a Mirror: Challenges Faced by Academic and Industry Experts", "categories": ["cs.CY", "cs.AI"], "comment": "To appear in AIED 2025", "summary": "As Artificial Intelligence (AI) technologies continue to evolve, the gap\nbetween academic AI education and real-world industry challenges remains an\nimportant area of investigation. This study provides preliminary insights into\nchallenges AI professionals encounter in both academia and industry, based on\nsemi-structured interviews with 14 AI experts - eight from industry and six\nfrom academia. We identify key challenges related to data quality and\navailability, model scalability, practical constraints, user behavior, and\nexplainability. While both groups experience data and model adaptation\ndifficulties, industry professionals more frequently highlight deployment\nconstraints, resource limitations, and external dependencies, whereas academics\nemphasize theoretical adaptation and standardization issues. These exploratory\nfindings suggest that AI curricula could better integrate real-world\ncomplexities, software engineering principles, and interdisciplinary learning,\nwhile recognizing the broader educational goals of building foundational and\nethical reasoning skills.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86AI\u6559\u80b2\u4e0e\u5b9e\u9645\u5de5\u4e1a\u6311\u6218\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u901a\u8fc7\u91c7\u8bbf14\u4f4d\u4e13\u5bb6\uff088\u4f4d\u6765\u81ea\u5de5\u4e1a\u754c\uff0c6\u4f4d\u6765\u81ea\u5b66\u672f\u754c\uff09\uff0c\u8bc6\u522b\u4e86\u6570\u636e\u8d28\u91cf\u3001\u6a21\u578b\u53ef\u6269\u5c55\u6027\u7b49\u5173\u952e\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdbAI\u8bfe\u7a0b\u7684\u5efa\u8bae\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5f25\u5408AI\u5b66\u672f\u6559\u80b2\u4e0e\u5de5\u4e1a\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ee5\u63d0\u5347\u6559\u80b2\u4e0e\u5b9e\u8df5\u7684\u76f8\u5173\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u91c7\u7528\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u6cd5\uff0c\u91c7\u8bbf\u4e8614\u4f4dAI\u4e13\u5bb6\uff088\u4f4d\u5de5\u4e1a\u754c\uff0c6\u4f4d\u5b66\u672f\u754c\uff09\uff0c\u5206\u6790\u4e86\u4ed6\u4eec\u5728\u6570\u636e\u3001\u6a21\u578b\u7b49\u65b9\u9762\u7684\u6311\u6218\u3002", "result": "\u5de5\u4e1a\u754c\u66f4\u5173\u6ce8\u90e8\u7f72\u9650\u5236\u548c\u8d44\u6e90\u95ee\u9898\uff0c\u5b66\u672f\u754c\u66f4\u5173\u6ce8\u7406\u8bba\u9002\u5e94\u548c\u6807\u51c6\u5316\u95ee\u9898\u3002\u4e24\u8005\u5747\u9762\u4e34\u6570\u636e\u548c\u6a21\u578b\u9002\u5e94\u7684\u56f0\u96be\u3002", "conclusion": "\u5efa\u8baeAI\u8bfe\u7a0b\u5e94\u66f4\u591a\u878d\u5165\u5b9e\u9645\u590d\u6742\u6027\u548c\u8de8\u5b66\u79d1\u5b66\u4e60\uff0c\u540c\u65f6\u6ce8\u91cd\u57fa\u7840\u4e0e\u4f26\u7406\u63a8\u7406\u80fd\u529b\u7684\u57f9\u517b\u3002"}}
{"id": "2505.03387", "pdf": "https://arxiv.org/pdf/2505.03387", "abs": "https://arxiv.org/abs/2505.03387", "authors": ["Diego Perazzolo", "Pietro Fanton", "Ilaria Barison", "Marny Fedrigo", "Annalisa Angelini", "Chiara Castellani", "Enrico Grisan"], "title": "Improving Omics-Based Classification: The Role of Feature Selection and Synthetic Data Generation", "categories": ["cs.LG", "I.2.1; I.5.2; I.5.4; I.2.8"], "comment": "Paper accepted at the 47th Annual International Conference IEEE EMBC\n  2025 (Engineering in Medicine and Biology Society), Copenhagen, Denmark", "summary": "Given the increasing complexity of omics datasets, a key challenge is not\nonly improving classification performance but also enhancing the transparency\nand reliability of model decisions. Effective model performance and feature\nselection are fundamental for explainability and reliability. In many cases,\nhigh dimensional omics datasets suffer from limited number of samples due to\nclinical constraints, patient conditions, phenotypes rarity and others\nconditions. Current omics based classification models often suffer from narrow\ninterpretability, making it difficult to discern meaningful insights where\ntrust and reproducibility are critical. This study presents a machine learning\nbased classification framework that integrates feature selection with data\naugmentation techniques to achieve high standard classification accuracy while\nensuring better interpretability. Using the publicly available dataset (E MTAB\n8026), we explore a bootstrap analysis in six binary classification scenarios\nto evaluate the proposed model's behaviour. We show that the proposed pipeline\nyields cross validated perfomance on small dataset that is conserved when the\ntrained classifier is applied to a larger test set. Our findings emphasize the\nfundamental balance between accuracy and feature selection, highlighting the\npositive effect of introducing synthetic data for better generalization, even\nin scenarios with very limited samples availability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7279\u5f81\u9009\u62e9\u4e0e\u6570\u636e\u589e\u5f3a\u7684\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u5c0f\u6837\u672c\u9ad8\u7ef4\u7ec4\u5b66\u6570\u636e\u7684\u5206\u7c7b\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u9762\u5bf9\u9ad8\u7ef4\u7ec4\u5b66\u6570\u636e\u6837\u672c\u91cf\u5c0f\u3001\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u6311\u6218\uff0c\u7814\u7a76\u65e8\u5728\u5e73\u8861\u5206\u7c7b\u51c6\u786e\u6027\u4e0e\u7279\u5f81\u9009\u62e9\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u96c6\u6210\u7279\u5f81\u9009\u62e9\u4e0e\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5e76\u5728\u516c\u5f00\u6570\u636e\u96c6\uff08E-MTAB 8026\uff09\u4e0a\u901a\u8fc7\u516d\u79cd\u4e8c\u5206\u7c7b\u573a\u666f\u8fdb\u884c\u81ea\u4e3e\u5206\u6790\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u5c0f\u6837\u672c\u6570\u636e\u4e0a\u5177\u6709\u7a33\u5b9a\u7684\u4ea4\u53c9\u9a8c\u8bc1\u6027\u80fd\uff0c\u4e14\u8bad\u7ec3\u540e\u7684\u5206\u7c7b\u5668\u5728\u66f4\u5927\u7684\u6d4b\u8bd5\u96c6\u4e0a\u4fdd\u6301\u6027\u80fd\u4e00\u81f4\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u51c6\u786e\u6027\u4e0e\u7279\u5f81\u9009\u62e9\u7684\u5e73\u8861\uff0c\u8bc1\u660e\u5408\u6210\u6570\u636e\u7684\u5f15\u5165\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u6781\u6709\u9650\u6837\u672c\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.03390", "pdf": "https://arxiv.org/pdf/2505.03390", "abs": "https://arxiv.org/abs/2505.03390", "authors": ["Zhengqin Yang", "Di Wu", "Jia Chen", "Xin Luo"], "title": "Concept Factorization via Self-Representation and Adaptive Graph Structure Learning", "categories": ["cs.LG"], "comment": null, "summary": "Concept Factorization (CF) models have attracted widespread attention due to\ntheir excellent performance in data clustering. In recent years, many variant\nmodels based on CF have achieved great success in clustering by taking into\naccount the internal geometric manifold structure of the dataset and using\ngraph regularization techniques. However, their clustering performance depends\ngreatly on the construction of the initial graph structure. In order to enable\nadaptive learning of the graph structure of the data, we propose a Concept\nFactorization Based on Self-Representation and Adaptive Graph Structure\nLearning (CFSRAG) Model. CFSRAG learns the affinity relationship between data\nthrough a self-representation method, and uses the learned affinity matrix to\nimplement dynamic graph regularization constraints, thereby ensuring dynamic\nlearning of the internal geometric structure of the data. Finally, we give the\nCFSRAG update rule and convergence analysis, and conduct comparative\nexperiments on four real datasets. The results show that our model outperforms\nother state-of-the-art models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u8868\u793a\u548c\u81ea\u9002\u5e94\u56fe\u7ed3\u6784\u5b66\u4e60\u7684\u6982\u5ff5\u5206\u89e3\u6a21\u578b\uff08CFSRAG\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u5b66\u4e60\u6570\u636e\u7684\u51e0\u4f55\u7ed3\u6784\u6765\u63d0\u5347\u805a\u7c7b\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u6982\u5ff5\u5206\u89e3\u6a21\u578b\u7684\u805a\u7c7b\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u521d\u59cb\u56fe\u7ed3\u6784\u6784\u5efa\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u9002\u5e94\u5b66\u4e60\u56fe\u7ed3\u6784\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "CFSRAG\u7ed3\u5408\u81ea\u8868\u793a\u65b9\u6cd5\u5b66\u4e60\u6570\u636e\u95f4\u7684\u76f8\u4f3c\u6027\u5173\u7cfb\uff0c\u5e76\u5229\u7528\u5b66\u5230\u7684\u76f8\u4f3c\u77e9\u9635\u5b9e\u73b0\u52a8\u6001\u56fe\u6b63\u5219\u5316\u7ea6\u675f\uff0c\u4ece\u800c\u52a8\u6001\u5b66\u4e60\u6570\u636e\u7684\u5185\u90e8\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCFSRAG\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002", "conclusion": "CFSRAG\u901a\u8fc7\u81ea\u9002\u5e94\u56fe\u7ed3\u6784\u5b66\u4e60\u6709\u6548\u63d0\u5347\u4e86\u805a\u7c7b\u6027\u80fd\uff0c\u4e3a\u6570\u636e\u6316\u6398\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.03273", "pdf": "https://arxiv.org/pdf/2505.03273", "abs": "https://arxiv.org/abs/2505.03273", "authors": ["Zhaoxi Mu", "Xinyu Yang", "Gang Wang"], "title": "SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Appears in IJCAI 2025", "summary": "While contemporary speech separation technologies adeptly process lengthy\nmixed audio waveforms, they are frequently challenged by the intricacies of\nreal-world environments, including noisy and reverberant settings, which can\nresult in artifacts or distortions in the separated speech. To overcome these\nlimitations, we introduce SepALM, a pioneering approach that employs audio\nlanguage models (ALMs) to rectify and re-synthesize speech within the text\ndomain following preliminary separation. SepALM comprises four core components:\na separator, a corrector, a synthesizer, and an aligner. By integrating an\nALM-based end-to-end error correction mechanism, we mitigate the risk of error\naccumulation and circumvent the optimization hurdles typically encountered in\nconventional methods that amalgamate automatic speech recognition (ASR) with\nlarge language models (LLMs). Additionally, we have developed Chain-of-Thought\n(CoT) prompting and knowledge distillation techniques to facilitate the\nreasoning and training processes of the ALM. Our experiments substantiate that\nSepALM not only elevates the precision of speech separation but also markedly\nbolsters adaptability in novel acoustic environments.", "AI": {"tldr": "SepALM\u662f\u4e00\u79cd\u5229\u7528\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff08ALM\uff09\u5728\u6587\u672c\u57df\u6821\u6b63\u548c\u91cd\u65b0\u5408\u6210\u8bed\u97f3\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff08\u5206\u79bb\u5668\u3001\u6821\u6b63\u5668\u3001\u5408\u6210\u5668\u548c\u5bf9\u9f50\u5668\uff09\u63d0\u5347\u8bed\u97f3\u5206\u79bb\u7684\u7cbe\u5ea6\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8bed\u97f3\u5206\u79bb\u6280\u672f\u5728\u5608\u6742\u548c\u6df7\u54cd\u73af\u5883\u4e2d\u6613\u4ea7\u751f\u4f2a\u5f71\u6216\u5931\u771f\u7684\u95ee\u9898\uff0c\u901a\u8fc7ALM\u5728\u6587\u672c\u57df\u7684\u6821\u6b63\u4f18\u5316\u5206\u79bb\u6548\u679c\u3002", "method": "\u63d0\u51faSepALM\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u79bb\u3001\u6821\u6b63\u3001\u5408\u6210\u548c\u5bf9\u9f50\u56db\u4e2a\u6a21\u5757\uff0c\u5e76\u5f15\u5165Chain-of-Thought\u63d0\u793a\u548c\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u4f18\u5316ALM\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSepALM\u663e\u8457\u63d0\u9ad8\u4e86\u8bed\u97f3\u5206\u79bb\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u65b0\u58f0\u5b66\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u9002\u5e94\u6027\u3002", "conclusion": "SepALM\u901a\u8fc7ALM\u7684\u7aef\u5230\u7aef\u9519\u8bef\u6821\u6b63\u673a\u5236\uff0c\u6709\u6548\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u8bed\u97f3\u5206\u79bb\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.02861", "pdf": "https://arxiv.org/pdf/2505.02861", "abs": "https://arxiv.org/abs/2505.02861", "authors": ["Kushagra Agrawal", "Nisharg Nargund"], "title": "Neural Orchestration for Multi-Agent Systems: A Deep Learning Framework for Optimal Agent Selection in Multi-Domain Task Environments", "categories": ["cs.MA", "cs.AI", "cs.NE"], "comment": null, "summary": "Multi-agent systems (MAS) are foundational in simulating complex real-world\nscenarios involving autonomous, interacting entities. However, traditional MAS\narchitectures often suffer from rigid coordination mechanisms and difficulty\nadapting to dynamic tasks. We propose MetaOrch, a neural orchestration\nframework for optimal agent selection in multi-domain task environments. Our\nsystem implements a supervised learning approach that models task context,\nagent histories, and expected response quality to select the most appropriate\nagent for each task. A novel fuzzy evaluation module scores agent responses\nalong completeness, relevance, and confidence dimensions, generating soft\nsupervision labels for training the orchestrator. Unlike previous methods that\nhard-code agent-task mappings, MetaOrch dynamically predicts the most suitable\nagent while estimating selection confidence. Experiments in simulated\nenvironments with heterogeneous agents demonstrate that our approach achieves\n86.3% selection accuracy, significantly outperforming baseline strategies\nincluding random selection and round-robin scheduling. The modular architecture\nemphasizes extensibility, allowing agents to be registered, updated, and\nqueried independently. Results suggest that neural orchestration offers a\npowerful approach to enhancing the autonomy, interpretability, and adaptability\nof multi-agent systems across diverse task domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMetaOrch\u7684\u795e\u7ecf\u534f\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u548c\u6a21\u7cca\u8bc4\u4f30\u6a21\u5757\u52a8\u6001\u9009\u62e9\u591a\u57df\u4efb\u52a1\u73af\u5883\u4e2d\u6700\u5408\u9002\u7684\u4ee3\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9009\u62e9\u51c6\u786e\u6027\u548c\u7cfb\u7edf\u9002\u5e94\u6027\u3002", "motivation": "\u4f20\u7edf\u591a\u4ee3\u7406\u7cfb\u7edf\uff08MAS\uff09\u67b6\u6784\u7684\u534f\u8c03\u673a\u5236\u8fc7\u4e8e\u50f5\u5316\uff0c\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u4efb\u52a1\u73af\u5883\uff0c\u56e0\u6b64\u9700\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "MetaOrch\u91c7\u7528\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5efa\u6a21\u4efb\u52a1\u4e0a\u4e0b\u6587\u3001\u4ee3\u7406\u5386\u53f2\u8bb0\u5f55\u548c\u9884\u671f\u54cd\u5e94\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u6a21\u7cca\u8bc4\u4f30\u6a21\u5757\u5bf9\u4ee3\u7406\u54cd\u5e94\u8fdb\u884c\u8bc4\u5206\uff0c\u52a8\u6001\u9884\u6d4b\u6700\u4f73\u4ee3\u7406\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u4e2d\uff0cMetaOrch\u5b9e\u73b0\u4e8686.3%\u7684\u9009\u62e9\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u968f\u673a\u9009\u62e9\u548c\u8f6e\u8be2\u8c03\u5ea6\u7b49\u57fa\u7ebf\u7b56\u7565\u3002", "conclusion": "\u795e\u7ecf\u534f\u8c03\u4e3a\u591a\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u81ea\u4e3b\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9002\u5e94\u6027\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7684\u4efb\u52a1\u9886\u57df\u3002"}}
{"id": "2505.03392", "pdf": "https://arxiv.org/pdf/2505.03392", "abs": "https://arxiv.org/abs/2505.03392", "authors": ["Saleh Zare Zade", "Yao Qiang", "Xiangyu Zhou", "Hui Zhu", "Mohammad Amin Roshani", "Prashant Khanduri", "Dongxiao Zhu"], "title": "Automatic Calibration for Membership Inference Attack on Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Membership Inference Attacks (MIAs) have recently been employed to determine\nwhether a specific text was part of the pre-training data of Large Language\nModels (LLMs). However, existing methods often misinfer non-members as members,\nleading to a high false positive rate, or depend on additional reference models\nfor probability calibration, which limits their practicality. To overcome these\nchallenges, we introduce a novel framework called Automatic Calibration\nMembership Inference Attack (ACMIA), which utilizes a tunable temperature to\ncalibrate output probabilities effectively. This approach is inspired by our\ntheoretical insights into maximum likelihood estimation during the pre-training\nof LLMs. We introduce ACMIA in three configurations designed to accommodate\ndifferent levels of model access and increase the probability gap between\nmembers and non-members, improving the reliability and robustness of membership\ninference. Extensive experiments on various open-source LLMs demonstrate that\nour proposed attack is highly effective, robust, and generalizable, surpassing\nstate-of-the-art baselines across three widely used benchmarks. Our code is\navailable at:\n\\href{https://github.com/Salehzz/ACMIA}{\\textcolor{blue}{Github}}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aACMIA\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u8c03\u6e29\u5ea6\u6821\u51c6\u8f93\u51fa\u6765\u6539\u5584\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6210\u5458\u63a8\u65ad\u653b\u51fb\uff08MIAs\uff09\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u6210\u5458\u63a8\u65ad\u653b\u51fb\u65b9\u6cd5\u5b58\u5728\u9ad8\u8bef\u62a5\u7387\u6216\u4f9d\u8d56\u989d\u5916\u53c2\u8003\u6a21\u578b\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "ACMIA\u5229\u7528\u53ef\u8c03\u6e29\u5ea6\u6821\u51c6\u8f93\u51fa\u6982\u7387\uff0c\u5e76\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u7406\u8bba\u6d1e\u5bdf\u8bbe\u8ba1\u4e09\u79cd\u914d\u7f6e\u4ee5\u9002\u5e94\u4e0d\u540c\u6a21\u578b\u8bbf\u95ee\u7ea7\u522b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cACMIA\u5728\u591a\u79cd\u5f00\u6e90LLMs\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u9ad8\u6548\u6027\u3001\u9c81\u68d2\u6027\u548c\u666e\u9002\u6027\u3002", "conclusion": "ACMIA\u663e\u8457\u63d0\u5347\u4e86\u6210\u5458\u63a8\u65ad\u7684\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3aLLMs\u7684\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2505.03393", "pdf": "https://arxiv.org/pdf/2505.03393", "abs": "https://arxiv.org/abs/2505.03393", "authors": ["Lena Stempfle", "Anton Matsson", "Newton Mwai", "Fredrik D. Johansson"], "title": "Prediction Models That Learn to Avoid Missing Values", "categories": ["cs.LG"], "comment": null, "summary": "Handling missing values at test time is challenging for machine learning\nmodels, especially when aiming for both high accuracy and interpretability.\nEstablished approaches often add bias through imputation or excessive model\ncomplexity via missingness indicators. Moreover, either method can obscure\ninterpretability, making it harder to understand how the model utilizes the\nobserved variables in predictions. We propose missingness-avoiding (MA) machine\nlearning, a general framework for training models to rarely require the values\nof missing (or imputed) features at test time. We create tailored MA learning\nalgorithms for decision trees, tree ensembles, and sparse linear models by\nincorporating classifier-specific regularization terms in their learning\nobjectives. The tree-based models leverage contextual missingness by reducing\nreliance on missing values based on the observed context. Experiments on\nreal-world datasets demonstrate that MA-DT, MA-LASSO, MA-RF, and MA-GBT\neffectively reduce the reliance on features with missing values while\nmaintaining predictive performance competitive with their unregularized\ncounterparts. This shows that our framework gives practitioners a powerful tool\nto maintain interpretability in predictions with test-time missing values.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7f3a\u5931\u907f\u514d\uff08MA\uff09\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5b9a\u6b63\u5219\u5316\u9879\u51cf\u5c11\u6a21\u578b\u5bf9\u7f3a\u5931\u503c\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u6d4b\u6027\u80fd\u548c\u9ad8\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u7f3a\u5931\u503c\u65f6\u5f80\u5f80\u5f15\u5165\u504f\u5dee\u6216\u590d\u6742\u6a21\u578b\u7ed3\u6784\uff0c\u727a\u7272\u53ef\u89e3\u91ca\u6027\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e2\u80fd\u51cf\u5c11\u5bf9\u7f3a\u5931\u503c\u7684\u4f9d\u8d56\uff0c\u53c8\u4e0d\u5f71\u54cd\u9884\u6d4b\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMA\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u7c7b\u5668\u7279\u5b9a\u7684\u6b63\u5219\u5316\u9879\u8bad\u7ec3\u6a21\u578b\uff08\u5982\u51b3\u7b56\u6811\u3001\u6811\u96c6\u6210\u3001\u7a00\u758f\u7ebf\u6027\u6a21\u578b\uff09\uff0c\u5229\u7528\u4e0a\u4e0b\u6587\u7f3a\u5931\u6027\u964d\u4f4e\u5bf9\u7f3a\u5931\u503c\u7684\u4f9d\u8d56\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMA-DT\u3001MA-LASSO\u3001MA-RF\u548cMA-GBT\u80fd\u6709\u6548\u51cf\u5c11\u5bf9\u7f3a\u5931\u503c\u7684\u4f9d\u8d56\uff0c\u4e14\u9884\u6d4b\u6027\u80fd\u4e0e\u539f\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "MA\u6846\u67b6\u4e3a\u5904\u7406\u6d4b\u8bd5\u65f6\u7f3a\u5931\u503c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.03414", "pdf": "https://arxiv.org/pdf/2505.03414", "abs": "https://arxiv.org/abs/2505.03414", "authors": ["Fangming Cui", "Yonggang Zhang", "Xuan Wang", "Xinmei Tian", "Jun Yu"], "title": "Enhancing Target-unspecific Tasks through a Features Matrix", "categories": ["cs.CV", "cs.CL"], "comment": "ICML 2025", "summary": "Recent developments in prompt learning of large vision-language models have\nsignificantly improved performance in target-specific tasks. However, these\nprompt optimizing methods often struggle to tackle the target-unspecific or\ngeneralizable tasks effectively. It may be attributed to the fact that\noverfitting training causes the model to forget its general knowledge having\nstrong promotion on target-unspecific tasks. To alleviate this issue, we\npropose a novel Features Matrix (FM) regularization approach designed to\nenhance these models on target-unspecific tasks. Our method extracts and\nleverages general knowledge, shaping a Features Matrix (FM). Specifically, the\nFM captures the semantics of diverse inputs from a deep and fine perspective,\npreserving essential general knowledge, which mitigates the risk of\noverfitting. Representative evaluations demonstrate that: 1) the FM is\ncompatible with existing frameworks as a generic and flexible module, and 2)\nthe FM significantly showcases its effectiveness in enhancing target-unspecific\ntasks, achieving state-of-the-art performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u7279\u5f81\u77e9\u9635\uff08FM\uff09\u6b63\u5219\u5316\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u5347\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u76ee\u6807\u975e\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u73b0\u6709\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u6613\u8fc7\u62df\u5408\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u517c\u5bb9\u6027\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u5728\u76ee\u6807\u975e\u7279\u5f02\u6027\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u8fc7\u62df\u5408\u5bfc\u81f4\u6a21\u578b\u9057\u5fd8\u901a\u7528\u77e5\u8bc6\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51faFM\u6b63\u5219\u5316\u65b9\u6cd5\u4ee5\u4fdd\u7559\u548c\u5229\u7528\u901a\u7528\u77e5\u8bc6\u3002", "method": "\u901a\u8fc7\u63d0\u53d6\u5e76\u5229\u7528\u901a\u7528\u77e5\u8bc6\u6784\u5efa\u7279\u5f81\u77e9\u9635\uff08FM\uff09\uff0c\u4ece\u6df1\u5ea6\u548c\u7ec6\u7c92\u5ea6\u89d2\u5ea6\u6355\u6349\u591a\u6837\u5316\u8f93\u5165\u7684\u8bed\u4e49\uff0c\u907f\u514d\u8fc7\u62df\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFM\u80fd\u517c\u5bb9\u73b0\u6709\u6846\u67b6\uff0c\u5e76\u663e\u8457\u63d0\u5347\u76ee\u6807\u975e\u7279\u5b9a\u4efb\u52a1\u8868\u73b0\uff0c\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "FM\u6b63\u5219\u5316\u662f\u4e00\u79cd\u901a\u7528\u4e14\u7075\u6d3b\u7684\u6a21\u5757\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u8fc7\u62df\u5408\u95ee\u9898\u5e76\u63d0\u5347\u76ee\u6807\u975e\u7279\u5f02\u6027\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2505.02863", "pdf": "https://arxiv.org/pdf/2505.02863", "abs": "https://arxiv.org/abs/2505.02863", "authors": ["Newnew Deng", "Edward Jiusi Liu", "Xiaoming Zhai"], "title": "Understanding University Students' Use of Generative AI: The Roles of Demographics and Personality Traits", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "The use of generative AI (GAI) among university students is rapidly\nincreasing, yet empirical research on students' GAI use and the factors\ninfluencing it remains limited. To address this gap, we surveyed 363\nundergraduate and graduate students in the United States, examining their GAI\nusage and how it relates to demographic variables and personality traits based\non the Big Five model (i.e., extraversion, agreeableness, conscientiousness,\nand emotional stability, and intellect/imagination). Our findings reveal: (a)\nStudents in higher academic years are more inclined to use GAI and prefer it\nover traditional resources. (b) Non-native English speakers use and adopt GAI\nmore readily than native speakers. (c) Compared to White, Asian students report\nhigher GAI usage, perceive greater academic benefits, and express a stronger\npreference for it. Similarly, Black students report a more positive impact of\nGAI on their academic performance. Personality traits also play a significant\nrole in shaping perceptions and usage of GAI. After controlling demographic\nfactors, we found that personality still significantly predicts GAI use and\nattitudes: (a) Students with higher conscientiousness use GAI less. (b)\nStudents who are higher in agreeableness perceive a less positive impact of GAI\non academic performance and express more ethical concerns about using it for\nacademic work. (c) Students with higher emotional stability report a more\npositive impact of GAI on learning and fewer concerns about its academic use.\n(d) Students with higher extraversion show a stronger preference for GAI over\ntraditional resources. (e) Students with higher intellect/imagination tend to\nprefer traditional resources. These insights highlight the need for\nuniversities to provide personalized guidance to ensure students use GAI\neffectively, ethically, and equitably in their academic pursuits.", "AI": {"tldr": "\u5927\u5b66\u751f\u4f7f\u7528\u751f\u6210\u5f0fAI\uff08GAI\uff09\u7684\u73b0\u8c61\u65e5\u76ca\u666e\u904d\uff0c\u4f46\u76f8\u5173\u5b9e\u8bc1\u7814\u7a76\u8f83\u5c11\u3002\u672c\u7814\u7a76\u8c03\u67e5\u4e86363\u540d\u7f8e\u56fd\u672c\u79d1\u751f\u548c\u7814\u7a76\u751f\u7684GAI\u4f7f\u7528\u60c5\u51b5\u53ca\u5176\u5f71\u54cd\u56e0\u7d20\uff08\u4eba\u53e3\u7edf\u8ba1\u5b66\u548c\u4eba\u683c\u7279\u8d28\uff09\u3002\u53d1\u73b0\u9ad8\u5e74\u7ea7\u3001\u975e\u82f1\u8bed\u6bcd\u8bed\u8005\u3001\u4e9a\u88d4\u548c\u9ed1\u4eba\u5bf9GAI\u63a5\u53d7\u5ea6\u66f4\u9ad8\uff0c\u4e14\u4eba\u683c\u7279\u8d28\uff08\u5982\u5c3d\u8d23\u6027\u3001\u5b9c\u4eba\u6027\u7b49\uff09\u663e\u8457\u5f71\u54cd\u4f7f\u7528\u6001\u5ea6\u3002\u547c\u5401\u9ad8\u6821\u63d0\u4f9b\u4e2a\u6027\u5316\u6307\u5bfc\u3002", "motivation": "\u586b\u8865\u5173\u4e8e\u5927\u5b66\u751fGAI\u4f7f\u7528\u53ca\u5176\u5f71\u54cd\u56e0\u7d20\u7684\u5b9e\u8bc1\u7814\u7a76\u7a7a\u767d\uff0c\u5e2e\u52a9\u9ad8\u6821\u66f4\u597d\u5730\u5f15\u5bfc\u5b66\u751f\u7684AI\u4f7f\u7528\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u5bf9363\u540d\u7f8e\u56fd\u672c\u79d1\u751f\u548c\u7814\u7a76\u751f\u8fdb\u884c\u95ee\u5377\u8c03\u67e5\uff0c\u5206\u6790\u5176GAI\u4f7f\u7528\u884c\u4e3a\u4e0e\u4eba\u53e3\u7edf\u8ba1\u5b66\u53d8\u91cf\u3001\u5927\u4e94\u4eba\u683c\u7279\u8d28\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u9ad8\u5e74\u7ea7\u3001\u975e\u82f1\u8bed\u6bcd\u8bed\u8005\u3001\u4e9a\u88d4\u548c\u9ed1\u4eba\u5bf9GAI\u63a5\u53d7\u5ea6\u66f4\u9ad8\uff1b\u4eba\u683c\u7279\u8d28\uff08\u5982\u5c3d\u8d23\u6027\u4f4e\u3001\u5b9c\u4eba\u6027\u4f4e\u6216\u5916\u5411\u6027\u9ad8\u7b49\uff09\u663e\u8457\u5f71\u54cdGAI\u7684\u4f7f\u7528\u6001\u5ea6\u548c\u504f\u597d\u3002", "conclusion": "\u9ad8\u6821\u9700\u6839\u636e\u5b66\u751f\u4e2a\u4f53\u5dee\u5f02\uff08\u5982\u4eba\u683c\u3001\u8bed\u8a00\u80cc\u666f\uff09\u63d0\u4f9b\u9488\u5bf9\u6027\u6307\u5bfc\uff0c\u4ee5\u786e\u4fddGAI\u5728\u5b66\u672f\u4e2d\u7684\u6709\u6548\u3001\u9053\u5fb7\u548c\u516c\u5e73\u4f7f\u7528\u3002"}}
{"id": "2505.03418", "pdf": "https://arxiv.org/pdf/2505.03418", "abs": "https://arxiv.org/abs/2505.03418", "authors": ["Da Zheng", "Lun Du", "Junwei Su", "Yuchen Tian", "Yuqi Zhu", "Jintian Zhang", "Lanning Wei", "Ningyu Zhang", "Huajun Chen"], "title": "Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey", "categories": ["cs.LG"], "comment": null, "summary": "Problem-solving has been a fundamental driver of human progress in numerous\ndomains. With advancements in artificial intelligence, Large Language Models\n(LLMs) have emerged as powerful tools capable of tackling complex problems\nacross diverse domains. Unlike traditional computational systems, LLMs combine\nraw computational power with an approximation of human reasoning, allowing them\nto generate solutions, make inferences, and even leverage external\ncomputational tools. However, applying LLMs to real-world problem-solving\npresents significant challenges, including multi-step reasoning, domain\nknowledge integration, and result verification. This survey explores the\ncapabilities and limitations of LLMs in complex problem-solving, examining\ntechniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation,\nand various LLM-based and tool-based verification techniques. Additionally, we\nhighlight domain-specific challenges in various domains, such as software\nengineering, mathematical reasoning and proving, data analysis and modeling,\nand scientific research. The paper further discusses the fundamental\nlimitations of the current LLM solutions and the future directions of LLM-based\ncomplex problems solving from the perspective of multi-step reasoning, domain\nknowledge integration and result verification.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u590d\u6742\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u80fd\u529b\u548c\u9650\u5236\uff0c\u63a2\u8ba8\u4e86\u94fe\u5f0f\u601d\u8003\u63a8\u7406\uff08CoT\uff09\u3001\u77e5\u8bc6\u589e\u5f3a\u53ca\u591a\u79cd\u9a8c\u8bc1\u6280\u672f\uff0c\u5e76\u8ba8\u8bba\u4e86\u5f53\u524dLLM\u89e3\u51b3\u65b9\u6848\u7684\u57fa\u672c\u5c40\u9650\u6027\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u7684\u8fdb\u6b65\uff0cLLMs\u5df2\u6210\u4e3a\u89e3\u51b3\u8de8\u9886\u57df\u590d\u6742\u95ee\u9898\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u4f46\u5176\u5728\u73b0\u5b9e\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u4ecd\u9762\u4e34\u591a\u6b65\u63a8\u7406\u3001\u9886\u57df\u77e5\u8bc6\u6574\u5408\u548c\u7ed3\u679c\u9a8c\u8bc1\u7b49\u6311\u6218\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5206\u6790\u94fe\u5f0f\u601d\u8003\u63a8\u7406\uff08CoT\uff09\u3001\u77e5\u8bc6\u589e\u5f3a\u53ca\u591a\u79cdLLM\u548c\u5de5\u5177\u9a8c\u8bc1\u6280\u672f\uff0c\u63a2\u8ba8\u4e86LLMs\u5728\u590d\u6742\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1LLMs\u5728\u591a\u6b65\u63a8\u7406\u548c\u77e5\u8bc6\u6574\u5408\u65b9\u9762\u53d6\u5f97\u4e86\u4e00\u5b9a\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u5e94\u5bf9\u5b9e\u9645\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u8f6f\u4ef6\u5de5\u7a0b\u3001\u6570\u5b66\u63a8\u7406\u53ca\u79d1\u5b66\u7814\u7a76\u7b49\u9886\u57df\u3002", "conclusion": "LLMs\u5728\u590d\u6742\u95ee\u9898\u89e3\u51b3\u9886\u57df\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u9700\u7a81\u7834\u591a\u6b65\u63a8\u7406\u3001\u9886\u57df\u77e5\u8bc6\u6574\u5408\u548c\u7ed3\u679c\u9a8c\u8bc1\u7b49\u6280\u672f\u74f6\u9888\uff0c\u672a\u6765\u5e94\u671d\u8fd9\u4e9b\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2505.03443", "pdf": "https://arxiv.org/pdf/2505.03443", "abs": "https://arxiv.org/abs/2505.03443", "authors": ["Valerio Bellandi"], "title": "Elevating Semantic Exploration: A Novel Approach Utilizing Distributed Repositories", "categories": ["cs.DC", "cs.AI", "cs.CL"], "comment": "This paper has been accepted at the 6th International Conference on\n  Recent Trends and Applications in Computer Science. It will appear in the\n  proceedings", "summary": "Centralized and distributed systems are two main approaches to organizing ICT\ninfrastructure, each with its pros and cons. Centralized systems concentrate\nresources in one location, making management easier but creating single points\nof failure. Distributed systems, on the other hand, spread resources across\nmultiple nodes, offering better scalability and fault tolerance, but requiring\nmore complex management. The choice between them depends on factors like\napplication needs, scalability, and data sensitivity. Centralized systems suit\napplications with limited scalability and centralized control, while\ndistributed systems excel in large-scale environments requiring high\navailability and performance. This paper explores a distributed document\nrepository system developed for the Italian Ministry of Justice, using edge\nrepositories to analyze textual data and metadata, enhancing semantic\nexploration capabilities.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u4f18\u7f3a\u70b9\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3a\u610f\u5927\u5229\u53f8\u6cd5\u90e8\u5f00\u53d1\u7684\u5206\u5e03\u5f0f\u6587\u6863\u5b58\u50a8\u7cfb\u7edf\uff0c\u5229\u7528\u8fb9\u7f18\u5b58\u50a8\u5e93\u5206\u6790\u6587\u672c\u6570\u636e\u548c\u5143\u6570\u636e\uff0c\u63d0\u5347\u8bed\u4e49\u63a2\u7d22\u80fd\u529b\u3002", "motivation": "\u63a2\u8ba8\u96c6\u4e2d\u5f0f\u4e0e\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u9002\u7528\u573a\u666f\uff0c\u9488\u5bf9\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u7684\u9ad8\u53ef\u7528\u6027\u548c\u6027\u80fd\u9700\u6c42\uff0c\u5f00\u53d1\u5206\u5e03\u5f0f\u6587\u6863\u5b58\u50a8\u7cfb\u7edf\u4ee5\u6ee1\u8db3\u8bed\u4e49\u63a2\u7d22\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u8fb9\u7f18\u5b58\u50a8\u5e93\u5206\u6790\u6587\u672c\u6570\u636e\u548c\u5143\u6570\u636e\uff0c\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u5206\u5e03\u5f0f\u6587\u6863\u5b58\u50a8\u7cfb\u7edf\u3002", "result": "\u7cfb\u7edf\u6210\u529f\u63d0\u5347\u4e86\u8bed\u4e49\u63a2\u7d22\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u7684\u9ad8\u53ef\u7528\u6027\u548c\u6027\u80fd\u9700\u6c42\u3002", "conclusion": "\u5206\u5e03\u5f0f\u7cfb\u7edf\u5728\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u9ad8\u53ef\u7528\u6027\u548c\u8bed\u4e49\u63a2\u7d22\u80fd\u529b\u7684\u573a\u666f\u4e0b\u3002"}}
{"id": "2505.03424", "pdf": "https://arxiv.org/pdf/2505.03424", "abs": "https://arxiv.org/abs/2505.03424", "authors": ["Kirill Lukyanov", "Mikhail Drobyshevskiy", "Georgii Sazonov", "Mikhail Soloviov", "Ilya Makarov"], "title": "Framework GNN-AID: Graph Neural Network Analysis Interpretation and Defense", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The growing need for Trusted AI (TAI) highlights the importance of\ninterpretability and robustness in machine learning models. However, many\nexisting tools overlook graph data and rarely combine these two aspects into a\nsingle solution. Graph Neural Networks (GNNs) have become a popular approach,\nachieving top results across various tasks. We introduce GNN-AID (Graph Neural\nNetwork Analysis, Interpretation, and Defense), an open-source framework\ndesigned for graph data to address this gap. Built as a Python library, GNN-AID\nsupports advanced trust methods and architectural layers, allowing users to\nanalyze graph datasets and GNN behavior using attacks, defenses, and\ninterpretability methods.\n  GNN-AID is built on PyTorch-Geometric, offering preloaded datasets, models,\nand support for any GNNs through customizable interfaces. It also includes a\nweb interface with tools for graph visualization and no-code features like an\ninteractive model builder, simplifying the exploration and analysis of GNNs.\nThe framework also supports MLOps techniques, ensuring reproducibility and\nresult versioning to track and revisit analyses efficiently.\n  GNN-AID is a flexible tool for developers and researchers. It helps\ndevelopers create, analyze, and customize graph models, while also providing\naccess to prebuilt datasets and models for quick experimentation. Researchers\ncan use the framework to explore advanced topics on the relationship between\ninterpretability and robustness, test defense strategies, and combine methods\nto protect against different types of attacks.\n  We also show how defenses against evasion and poisoning attacks can conflict\nwhen applied to graph data, highlighting the complex connections between\ndefense strategies.\n  GNN-AID is available at\n\\href{https://github.com/ispras/GNN-AID}{github.com/ispras/GNN-AID}", "AI": {"tldr": "GNN-AID is an open-source Python framework for analyzing and defending Graph Neural Networks (GNNs), integrating interpretability and robustness for graph data.", "motivation": "The growing need for Trusted AI (TAI) highlights gaps in existing tools that overlook graph data and fail to combine interpretability and robustness in a single solution.", "method": "GNN-AID is built on PyTorch-Geometric, offering preloaded datasets, models, customizable interfaces, and a web UI for visualization and no-code experimentation. It supports MLOps for reproducibility.", "result": "The framework provides tools for developers and researchers to analyze, defend, and interpret GNNs, while revealing conflicts between defense strategies against evasion and poisoning attacks.", "conclusion": "GNN-AID bridges the gap in Trusted AI for graph data, offering a flexible and efficient solution for GNN analysis, defense, and interpretability."}}
{"id": "2505.03501", "pdf": "https://arxiv.org/pdf/2505.03501", "abs": "https://arxiv.org/abs/2505.03501", "authors": ["Zihan Wang", "Hongwei Li", "Rui Zhang", "Wenbo Jiang", "Kangjie Chen", "Tianwei Zhang", "Qingchuan Zhao", "Guowen Xu"], "title": "BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "In this paper, we present a new form of backdoor attack against Large\nLanguage Models (LLMs): lingual-backdoor attacks. The key novelty of\nlingual-backdoor attacks is that the language itself serves as the trigger to\nhijack the infected LLMs to generate inflammatory speech. They enable the\nprecise targeting of a specific language-speaking group, exacerbating racial\ndiscrimination by malicious entities. We first implement a baseline\nlingual-backdoor attack, which is carried out by poisoning a set of training\ndata for specific downstream tasks through translation into the trigger\nlanguage. However, this baseline attack suffers from poor task generalization\nand is impractical in real-world settings. To address this challenge, we design\nBadLingual, a novel task-agnostic lingual-backdoor, capable of triggering any\ndownstream tasks within the chat LLMs, regardless of the specific questions of\nthese tasks. We design a new approach using PPL-constrained Greedy Coordinate\nGradient-based Search (PGCG) based adversarial training to expand the decision\nboundary of lingual-backdoor, thereby enhancing the generalization ability of\nlingual-backdoor across various tasks. We perform extensive experiments to\nvalidate the effectiveness of our proposed attacks. Specifically, the baseline\nattack achieves an ASR of over 90% on the specified tasks. However, its ASR\nreaches only 37.61% across six tasks in the task-agnostic scenario. In\ncontrast, BadLingual brings up to 37.35% improvement over the baseline. Our\nstudy sheds light on a new perspective of vulnerabilities in LLMs with\nmultilingual capabilities and is expected to promote future research on the\npotential defenses to enhance the LLMs' robustness", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u578b\u540e\u95e8\u653b\u51fb\u2014\u2014\u8bed\u8a00\u540e\u95e8\u653b\u51fb\uff0c\u5229\u7528\u8bed\u8a00\u672c\u8eab\u4f5c\u4e3a\u89e6\u53d1\u6761\u4ef6\uff0c\u8bf1\u5bfc\u6a21\u578b\u751f\u6210\u717d\u52a8\u6027\u8a00\u8bba\u3002\u901a\u8fc7\u8bbe\u8ba1\u4efb\u52a1\u65e0\u5173\u7684BadLingual\u653b\u51fb\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653b\u51fb\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u6f0f\u6d1e\u3002", "motivation": "\u968f\u7740\u5168\u7403\u4e92\u8054\u7f51\u7684\u53d1\u5c55\u548c\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u7684\u666e\u53ca\uff0c\u786e\u4fdd\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\u3002\u6076\u610f\u52bf\u529b\u53ef\u80fd\u5229\u7528\u6a21\u578b\u7684\u7279\u6027\u53d1\u8d77\u7279\u5b9a\u8bed\u8a00\u7fa4\u4f53\u7684\u5b9a\u5411\u653b\u51fb\uff0c\u8fd9\u52a0\u5267\u4e86\u79cd\u65cf\u6b67\u89c6\u7b49\u793e\u4f1a\u95ee\u9898\u3002\u4e3a\u4e86\u6df1\u5165\u4e86\u89e3\u8fd9\u4e9b\u5a01\u80c1\u5e76\u6fc0\u53d1\u9632\u5fa1\u7814\u7a76\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u65b0\u578b\u7684\u8bed\u8a00\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u8bba\u6587\u9996\u5148\u5b9e\u73b0\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7ffb\u8bd1\u89e6\u53d1\u7684\u57fa\u51c6\u8bed\u8a00\u540e\u95e8\u653b\u51fb\uff0c\u4f46\u7531\u4e8e\u5176\u6cdb\u5316\u6027\u80fd\u5dee\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002\u4e3a\u6b64\uff0c\u7814\u7a76\u8005\u8bbe\u8ba1\u4e86\u4efb\u52a1\u65e0\u5173\u7684\u653b\u51fb\u65b9\u6cd5BadLingual\uff0c\u5e76\u91c7\u7528PPL\u7ea6\u675f\u7684\u8d2a\u5a6a\u5750\u6807\u68af\u5ea6\u641c\u7d22\uff08PGCG\uff09\u8fdb\u884c\u5bf9\u6297\u8bad\u7ec3\uff0c\u4ee5\u6269\u5c55\u540e\u95e8\u7684\u51b3\u7b56\u8fb9\u754c\uff0c\u63d0\u5347\u5176\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u51c6\u653b\u51fb\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u653b\u51fb\u6210\u529f\u7387\uff08ASR\uff09\u8d85\u8fc790%\uff0c\u4f46\u5728\u4efb\u52a1\u65e0\u5173\u573a\u666f\u4e0b\u4ec5\u4e3a37.61%\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0cBadLingual\u5728\u4efb\u52a1\u65e0\u5173\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u6bd4\u57fa\u51c6\u653b\u51fb\u9ad837.35%\u7684\u6539\u8fdb\u6548\u679c\u3002", "conclusion": "\u8bed\u8a00\u540e\u95e8\u653b\u51fb\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u63ed\u793a\u4e86\u591a\u8bed\u8a00\u80fd\u529b\u7684\u6f5c\u5728\u6f0f\u6d1e\u3002\u672a\u6765\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u9632\u5fa1\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2505.03432", "pdf": "https://arxiv.org/pdf/2505.03432", "abs": "https://arxiv.org/abs/2505.03432", "authors": ["Stefano Bruno", "Sotirios Sabanis"], "title": "Wasserstein Convergence of Score-based Generative Models under Semiconvexity and Discontinuous Gradients", "categories": ["cs.LG", "math.OC", "math.PR", "stat.ML"], "comment": null, "summary": "Score-based Generative Models (SGMs) approximate a data distribution by\nperturbing it with Gaussian noise and subsequently denoising it via a learned\nreverse diffusion process. These models excel at modeling complex data\ndistributions and generating diverse samples, achieving state-of-the-art\nperformance across domains such as computer vision, audio generation,\nreinforcement learning, and computational biology. Despite their empirical\nsuccess, existing Wasserstein-2 convergence analysis typically assume strong\nregularity conditions-such as smoothness or strict log-concavity of the data\ndistribution-that are rarely satisfied in practice. In this work, we establish\nthe first non-asymptotic Wasserstein-2 convergence guarantees for SGMs\ntargeting semiconvex distributions with potentially discontinuous gradients.\nOur upper bounds are explicit and sharp in key parameters, achieving optimal\ndependence of $O(\\sqrt{d})$ on the data dimension $d$ and convergence rate of\norder one. The framework accommodates a wide class of practically relevant\ndistributions, including symmetric modified half-normal distributions, Gaussian\nmixtures, double-well potentials, and elastic net potentials. By leveraging\nsemiconvexity without requiring smoothness assumptions on the potential such as\ndifferentiability, our results substantially broaden the theoretical\nfoundations of SGMs, bridging the gap between empirical success and rigorous\nguarantees in non-smooth, complex data regimes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u534a\u51f8\u5206\u5e03\u7684\u57fa\u4e8e\u8bc4\u5206\u7684\u751f\u6210\u6a21\u578b\uff08SGMs\uff09\u7684\u975e\u6e10\u8fd1Wasserstein-2\u6536\u655b\u4fdd\u8bc1\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u5206\u6790\u4e2d\u5bf9\u6570\u636e\u5206\u5e03\u7684\u5f3a\u6b63\u5219\u6027\u5047\u8bbe\u3002", "motivation": "\u73b0\u6709Wasserstein-2\u6536\u655b\u5206\u6790\u901a\u5e38\u5047\u8bbe\u6570\u636e\u5206\u5e03\u5177\u6709\u5f3a\u6b63\u5219\u6027\uff08\u5982\u5e73\u6ed1\u6027\u6216\u4e25\u683c\u5bf9\u6570\u51f9\u6027\uff09\uff0c\u800c\u8fd9\u5728\u5b9e\u9645\u4e2d\u5f88\u5c11\u6ee1\u8db3\u3002\u672c\u6587\u65e8\u5728\u586b\u8865SGMs\u5728\u975e\u5e73\u6ed1\u3001\u590d\u6742\u6570\u636e\u573a\u666f\u4e0b\u7684\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5229\u7528\u534a\u51f8\u6027\uff08\u65e0\u9700\u6f5c\u5728\u51fd\u6570\u7684\u53ef\u5fae\u6027\u5047\u8bbe\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5305\u62ec\u5bf9\u79f0\u4fee\u6b63\u534a\u6b63\u6001\u5206\u5e03\u3001\u9ad8\u65af\u6df7\u5408\u3001\u53cc\u9631\u52bf\u7b49\u5e7f\u6cdb\u7684\u5b9e\u9645\u76f8\u5173\u5206\u5e03\u3002", "result": "\u5b9e\u73b0\u4e86\u5728\u6570\u636e\u7ef4\u5ea6$d$\u4e0a\u7684\u6700\u4f18\u4f9d\u8d56\u5173\u7cfb$O(\\sqrt{d})$\u548c\u9636\u6570\u4e00\u7684\u6536\u655b\u901f\u7387\uff0c\u663e\u8457\u6269\u5c55\u4e86SGMs\u7684\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u672c\u6587\u7684\u5de5\u4f5c\u6781\u5927\u5730\u62d3\u5c55\u4e86SGMs\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5f25\u5408\u4e86\u975e\u5e73\u6ed1\u3001\u590d\u6742\u6570\u636e\u573a\u666f\u4e0b\u5b9e\u8bc1\u6210\u529f\u4e0e\u4e25\u683c\u7406\u8bba\u4fdd\u8bc1\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2505.03676", "pdf": "https://arxiv.org/pdf/2505.03676", "abs": "https://arxiv.org/abs/2505.03676", "authors": ["Arthur Satouf", "Gabriel Ben Zenou", "Benjamin Piwowarski", "Habiboulaye Amadou Boubacar", "Pablo Piantanida"], "title": "Rational Retrieval Acts: Leveraging Pragmatic Reasoning to Improve Sparse Retrieval", "categories": ["cs.IR", "cs.CL", "68P20, 68T50", "H.3"], "comment": "6 pages - 2 figures - conference: accepted at SIGIR 2025", "summary": "Current sparse neural information retrieval (IR) methods, and to a lesser\nextent more traditional models such as BM25, do not take into account the\ndocument collection and the complex interplay between different term weights\nwhen representing a single document. In this paper, we show how the Rational\nSpeech Acts (RSA), a linguistics framework used to minimize the number of\nfeatures to be communicated when identifying an object in a set, can be adapted\nto the IR case -- and in particular to the high number of potential features\n(here, tokens). RSA dynamically modulates token-document interactions by\nconsidering the influence of other documents in the dataset, better contrasting\ndocument representations. Experiments show that incorporating RSA consistently\nimproves multiple sparse retrieval models and achieves state-of-the-art\nperformance on out-of-domain datasets from the BEIR benchmark.\nhttps://github.com/arthur-75/Rational-Retrieval-Acts", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7406\u6027\u8a00\u8bed\u884c\u4e3a\uff08RSA\uff09\u6846\u67b6\u7684\u7a00\u758f\u795e\u7ecf\u4fe1\u606f\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6587\u6863\u96c6\u5408\u4e2d\u8bcd\u9879\u6743\u91cd\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u8de8\u57df\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u5f53\u524d\u7a00\u758f\u795e\u7ecf\u4fe1\u606f\u68c0\u7d22\u65b9\u6cd5\u53ca\u4f20\u7edf\u6a21\u578b\u5982BM25\u672a\u80fd\u5145\u5206\u8003\u8651\u6587\u6863\u96c6\u5408\u4e2d\u8bcd\u9879\u6743\u91cd\u7684\u590d\u6742\u4ea4\u4e92\u4f5c\u7528\uff0c\u5bfc\u81f4\u6587\u6863\u8868\u793a\u4e0d\u591f\u51c6\u786e\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7RSA\u6846\u67b6\u4f18\u5316\u8fd9\u79cd\u4ea4\u4e92\uff0c\u63d0\u5347\u68c0\u7d22\u6548\u679c\u3002", "method": "\u65b9\u6cd5\u4e0a\uff0c\u8bba\u6587\u5c06RSA\u6846\u67b6\uff08\u4e00\u79cd\u8bed\u8a00\u5b66\u4e2d\u7528\u4e8e\u6700\u5c0f\u5316\u7279\u5f81\u6570\u91cf\u7684\u65b9\u6cd5\uff09\u9002\u914d\u5230\u4fe1\u606f\u68c0\u7d22\u573a\u666f\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u8282\u8bcd\u9879-\u6587\u6863\u7684\u4ea4\u4e92\u5173\u7cfb\uff0c\u8003\u8651\u6587\u6863\u96c6\u5408\u7684\u6574\u4f53\u5f71\u54cd\uff0c\u4ece\u800c\u66f4\u51c6\u786e\u5730\u8868\u793a\u6587\u6863\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f15\u5165RSA\u7684\u68c0\u7d22\u6a21\u578b\u5728\u591a\u9879\u7a00\u758f\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5728BEIR\u57fa\u51c6\u6d4b\u8bd5\u7684\u8de8\u57df\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u4f18\u7ed3\u679c\u3002", "conclusion": "\u8bba\u6587\u9a8c\u8bc1\u4e86RSA\u6846\u67b6\u5728\u7a00\u758f\u795e\u7ecf\u4fe1\u606f\u68c0\u7d22\u4e2d\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u52a8\u6001\u8003\u8651\u6587\u6863\u96c6\u5408\u7684\u5168\u5c40\u4fe1\u606f\u53ef\u4ee5\u663e\u8457\u6539\u5584\u68c0\u7d22\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.03490", "pdf": "https://arxiv.org/pdf/2505.03490", "abs": "https://arxiv.org/abs/2505.03490", "authors": ["Faiz Taleb", "Ivan Gazeau", "Maryline Laurent"], "title": "A new membership inference attack that spots memorization in generative and predictive models: Loss-Based with Reference Model algorithm (LBRM)", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Generative models can unintentionally memorize training data, posing\nsignificant privacy risks. This paper addresses the memorization phenomenon in\ntime series imputation models, introducing the Loss-Based with Reference Model\n(LBRM) algorithm. The LBRM method leverages a reference model to enhance the\naccuracy of membership inference attacks, distinguishing between training and\ntest data. Our contributions are twofold: first, we propose an innovative\nmethod to effectively extract and identify memorized training data,\nsignificantly improving detection accuracy. On average, without fine-tuning,\nthe AUROC improved by approximately 40\\%. With fine-tuning, the AUROC increased\nby approximately 60\\%. Second, we validate our approach through membership\ninference attacks on two types of architectures designed for time series\nimputation, demonstrating the robustness and versatility of the LBRM approach\nin different contexts. These results highlight the significant enhancement in\ndetection accuracy provided by the LBRM approach, addressing privacy risks in\ntime series imputation models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLBRM\u7b97\u6cd5\uff0c\u901a\u8fc7\u53c2\u8003\u6a21\u578b\u63d0\u9ad8\u65f6\u95f4\u5e8f\u5217\u63d2\u503c\u6a21\u578b\u4e2d\u8bad\u7ec3\u6570\u636e\u8bb0\u5fc6\u73b0\u8c61\u7684\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u6210\u5458\u63a8\u65ad\u653b\u51fb\u7684AUROC\u63d0\u534740%-60%\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u53ef\u80fd\u65e0\u610f\u4e2d\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\uff0c\u5e26\u6765\u9690\u79c1\u98ce\u9669\uff0c\u5c24\u5176\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217\u63d2\u503c\u6a21\u578b\u4e2d\u7684\u8bb0\u5fc6\u73b0\u8c61\u8fdb\u884c\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u53c2\u8003\u6a21\u578b\u7684LBRM\u7b97\u6cd5\uff0c\u901a\u8fc7\u6210\u5458\u63a8\u65ad\u653b\u51fb\u533a\u5206\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\uff0c\u4f18\u5316\u8bb0\u5fc6\u6570\u636e\u7684\u63d0\u53d6\u4e0e\u8bc6\u522b\u3002", "result": "\u672a\u7ecf\u5fae\u8c03\u65f6AUROC\u63d0\u5347\u7ea640%\uff0c\u5fae\u8c03\u540e\u63d0\u5347\u7ea660%\uff0c\u5e76\u5728\u4e24\u79cd\u65f6\u95f4\u5e8f\u5217\u63d2\u503c\u67b6\u6784\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "LBRM\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u6709\u6548\u5e94\u5bf9\u65f6\u95f4\u5e8f\u5217\u63d2\u503c\u6a21\u578b\u4e2d\u7684\u9690\u79c1\u98ce\u9669\u3002"}}
{"id": "2505.03509", "pdf": "https://arxiv.org/pdf/2505.03509", "abs": "https://arxiv.org/abs/2505.03509", "authors": ["Pablo G\u00f3mez", "David O'Ryan"], "title": "AnomalyMatch: Discovering Rare Objects of Interest with Semi-supervised and Active Learning", "categories": ["cs.LG", "astro-ph.IM"], "comment": "Journal submission in preparation", "summary": "Anomaly detection in large datasets is essential in fields such as astronomy\nand computer vision; however, supervised methods typically require extensive\nanomaly labelling, which is often impractical. We present AnomalyMatch, an\nanomaly detection framework combining the semi-supervised FixMatch algorithm\nusing EfficientNet classifiers with active learning. By treating anomaly\ndetection as a semi-supervised binary classification problem, we efficiently\nutilise limited labelled and abundant unlabelled images. We allow iterative\nmodel refinement in a user interface for expert verification of high-confidence\nanomalies and correction of false positives. Built for astronomical data,\nAnomalyMatch generalises readily to other domains facing similar data\nchallenges. Evaluations on the GalaxyMNIST astronomical dataset and the\nminiImageNet natural-image benchmark under severe class imbalance (1% anomalies\nfor miniImageNet) display strong performance: starting from five to ten\nlabelled anomalies and after three active learning cycles, we achieve an\naverage AUROC of 0.95 (miniImageNet) and 0.86 (GalaxyMNIST), with respective\nAUPRC of 0.77 and 0.71. After active learning cycles, anomalies are ranked with\n71% (miniImageNet) to 93% precision in the 1% of the highest-ranked images.\nAnomalyMatch is tailored for large-scale applications, efficiently processing\npredictions for 100 million images within three days on a single GPU.\nIntegrated into ESAs Datalabs platform, AnomalyMatch facilitates targeted\ndiscovery of scientifically valuable anomalies in vast astronomical datasets.\nOur results underscore the exceptional utility and scalability of this approach\nfor anomaly discovery, highlighting the value of specialised approaches for\ndomains characterised by severe label scarcity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAnomalyMatch\u7684\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u534a\u76d1\u7763FixMatch\u7b97\u6cd5\u4e0e\u4e3b\u52a8\u5b66\u4e60\uff0c\u9002\u7528\u4e8e\u6807\u7b7e\u7a00\u7f3a\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff08\u5982\u5929\u6587\u548c\u81ea\u7136\u56fe\u50cf\uff09\u3002\u8be5\u65b9\u6cd5\u5728GalaxyMNIST\u548cminiImageNet\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u901a\u8fc7\u5c11\u91cf\u521d\u59cb\u6807\u6ce8\u548c\u4e3b\u52a8\u5b66\u4e60\u5faa\u73af\u8fbe\u5230\u9ad8\u7cbe\u5ea6\uff08AUROC 0.95/0.86\uff09\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e2d\u5f02\u5e38\u68c0\u6d4b\u7684\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\uff0c\u4f20\u7edf\u76d1\u7763\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u4e0d\u73b0\u5b9e\u3002", "method": "\u7ed3\u5408\u534a\u76d1\u7763FixMatch\u7b97\u6cd5\uff08\u57fa\u4e8eEfficientNet\u5206\u7c7b\u5668\uff09\u4e0e\u4e3b\u52a8\u5b66\u4e60\uff0c\u901a\u8fc7\u4e13\u5bb6\u9a8c\u8bc1\u8fed\u4ee3\u4f18\u5316\u6a21\u578b\uff0c\u5c06\u5f02\u5e38\u68c0\u6d4b\u8f6c\u5316\u4e3a\u534a\u76d1\u7763\u4e8c\u5206\u7c7b\u95ee\u9898\u3002", "result": "\u5728miniImageNet\uff081%\u5f02\u5e38\uff09\u548cGalaxyMNIST\u4e0a\uff0c\u521d\u59cb5-10\u4e2a\u6807\u6ce8\u6837\u672c\u7ecf3\u8f6e\u4e3b\u52a8\u5b66\u4e60\u540e\uff0cAUROC\u5206\u522b\u8fbe0.95\u548c0.86\uff1b\u6700\u9ad8\u6392\u540d\u76841%\u56fe\u50cf\u4e2d\u5f02\u5e38\u8bc6\u522b\u7cbe\u5ea6\u8fbe71%-93%\u3002", "conclusion": "AnomalyMatch\u5728\u5927\u89c4\u6a21\u5e94\u7528\u4e2d\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\uff08\u5982\u5355GPU\u5904\u74061\u4ebf\u56fe\u50cf\u4ec5\u97003\u5929\uff09\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6807\u7b7e\u7a00\u7f3a\u9886\u57df\uff0c\u5df2\u5728\u6b27\u6d32\u822a\u5929\u5c40\u6570\u636e\u5e73\u53f0\u90e8\u7f72\u3002"}}
{"id": "2505.03519", "pdf": "https://arxiv.org/pdf/2505.03519", "abs": "https://arxiv.org/abs/2505.03519", "authors": ["Sy-Tuyen Ho", "Koh Jun Hao", "Ngoc-Bao Nguyen", "Alexander Binder", "Ngai-Man Cheung"], "title": "Uncovering the Limitations of Model Inversion Evaluation: Benchmarks and Connection to Type-I Adversarial Attacks", "categories": ["cs.LG"], "comment": "Our dataset and code are available in the Supp", "summary": "Model Inversion (MI) attacks aim to reconstruct information of private\ntraining data by exploiting access to machine learning models. The most common\nevaluation framework for MI attacks/defenses relies on an evaluation model that\nhas been utilized to assess progress across almost all MI attacks and defenses\nproposed in recent years. In this paper, for the first time, we present an\nin-depth study of MI evaluation. Firstly, we construct the first comprehensive\nhuman-annotated dataset of MI attack samples, based on 28 setups of different\nMI attacks, defenses, private and public datasets. Secondly, using our dataset,\nwe examine the accuracy of the MI evaluation framework and reveal that it\nsuffers from a significant number of false positives. These findings raise\nquestions about the previously reported success rates of SOTA MI attacks.\nThirdly, we analyze the causes of these false positives, design controlled\nexperiments, and discover the surprising effect of Type I adversarial features\non MI evaluation, as well as adversarial transferability, highlighting a\nrelationship between two previously distinct research areas. Our findings\nsuggest that the performance of SOTA MI attacks has been overestimated, with\nthe actual privacy leakage being significantly less than previously reported.\nIn conclusion, we highlight critical limitations in the widely used MI\nevaluation framework and present our methods to mitigate false positive rates.\nWe remark that prior research has shown that Type I adversarial attacks are\nvery challenging, with no existing solution. Therefore, we urge to consider\nhuman evaluation as a primary MI evaluation framework rather than merely a\nsupplement as in previous MI research. We also encourage further work on\ndeveloping more robust and reliable automatic evaluation frameworks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6df1\u5165\u7814\u7a76\u4e86\u6a21\u578b\u53cd\u8f6c\uff08MI\uff09\u653b\u51fb\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u53d1\u73b0\u5176\u5b58\u5728\u5927\u91cf\u5047\u9633\u6027\u95ee\u9898\uff0c\u5bfc\u81f4\u4e4b\u524d\u62a5\u544a\u7684MI\u653b\u51fb\u6210\u529f\u7387\u88ab\u9ad8\u4f30\u3002", "motivation": "\u73b0\u6709MI\u653b\u51fb/\u9632\u5fa1\u7684\u8bc4\u4f30\u6846\u67b6\u5b58\u5728\u5047\u9633\u6027\u95ee\u9898\uff0c\u53ef\u80fd\u5bfc\u81f4\u5bf9\u9690\u79c1\u6cc4\u6f0f\u7a0b\u5ea6\u7684\u8bef\u89e3\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u5206\u6790\u5176\u51c6\u786e\u6027\u548c\u5c40\u9650\u6027\u3002", "method": "\u6784\u5efa\u9996\u4e2a\u5168\u9762\u7684MI\u653b\u51fb\u6837\u672c\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5206\u6790\u8bc4\u4f30\u6846\u67b6\u7684\u5047\u9633\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u63ed\u793a\u7c7b\u578bI\u5bf9\u6297\u7279\u5f81\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u8bc4\u4f30\u6846\u67b6\u7684\u5047\u9633\u6027\u95ee\u9898\u663e\u8457\uff0c\u5b9e\u9645\u9690\u79c1\u6cc4\u6f0f\u7a0b\u5ea6\u4f4e\u4e8e\u4ee5\u5f80\u62a5\u544a\uff0c\u4e14\u7c7b\u578bI\u5bf9\u6297\u7279\u5f81\u4e0e\u5bf9\u6297\u53ef\u8f6c\u79fb\u6027\u5b58\u5728\u5173\u8054\u3002", "conclusion": "\u63d0\u51fa\u6539\u8fdb\u8bc4\u4f30\u6846\u67b6\u4ee5\u51cf\u5c11\u5047\u9633\u6027\uff0c\u5e76\u5efa\u8bae\u5c06\u4eba\u5de5\u8bc4\u4f30\u4f5c\u4e3a\u4e3b\u8981\u65b9\u6cd5\uff0c\u540c\u65f6\u547c\u5401\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u81ea\u52a8\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2505.03530", "pdf": "https://arxiv.org/pdf/2505.03530", "abs": "https://arxiv.org/abs/2505.03530", "authors": ["Dip Roy"], "title": "Causal Intervention Framework for Variational Auto Encoder Mechanistic Interpretability", "categories": ["cs.LG"], "comment": null, "summary": "Mechanistic interpretability of deep learning models has emerged as a crucial\nresearch direction for understanding the functioning of neural networks. While\nsignificant progress has been made in interpreting discriminative models like\ntransformers, understanding generative models such as Variational Autoencoders\n(VAEs) remains challenging. This paper introduces a comprehensive causal\nintervention framework for mechanistic interpretability of VAEs. We develop\ntechniques to identify and analyze \"circuit motifs\" in VAEs, examining how\nsemantic factors are encoded, processed, and disentangled through the network\nlayers. Our approach uses targeted interventions at different levels: input\nmanipulations, latent space perturbations, activation patching, and causal\nmediation analysis. We apply our framework to both synthetic datasets with\nknown causal relationships and standard disentanglement benchmarks. Results\nshow that our interventions can successfully isolate functional circuits, map\ncomputational graphs to causal graphs of semantic factors, and distinguish\nbetween polysemantic and monosemantic units. Furthermore, we introduce metrics\nfor causal effect strength, intervention specificity, and circuit modularity\nthat quantify the interpretability of VAE components. Experimental results\ndemonstrate clear differences between VAE variants, with FactorVAE achieving\nhigher disentanglement scores (0.084) and effect strengths (mean 4.59) compared\nto standard VAE (0.064, 3.99) and Beta-VAE (0.051, 3.43). Our framework\nadvances the mechanistic understanding of generative models and provides tools\nfor more transparent and controllable VAE architectures.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668(VAE)\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7684\u56e0\u679c\u5e72\u9884\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c42\u6b21\u7684\u5e72\u9884\u6280\u672f\u5206\u6790\u5176\u5185\u90e8\u7535\u8def\u529f\u80fd\uff0c\u5e76\u5f15\u5165\u91cf\u5316\u6307\u6807\u6bd4\u8f83\u4e0d\u540cVAE\u53d8\u4f53\u7684\u8868\u73b0\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u673a\u5236\u53ef\u89e3\u91ca\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u751f\u6210\u6a21\u578b\uff08\u5982VAE\uff09\u7684\u53ef\u89e3\u91ca\u6027\u4ecd\u5177\u6311\u6218\u6027\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3aVAEs\u63d0\u4f9b\u7cfb\u7edf\u6027\u5206\u6790\u5de5\u5177\u3002", "method": "\u91c7\u7528\u4e86\u56e0\u679c\u5e72\u9884\u6846\u67b6\uff0c\u5305\u62ec\u8f93\u5165\u64cd\u63a7\u3001\u6f5c\u7a7a\u95f4\u6270\u52a8\u3001\u6fc0\u6d3b\u8865\u4e01\u548c\u56e0\u679c\u4e2d\u4ecb\u5206\u6790\u7b49\u6280\u672f\uff0c\u7ed3\u5408\u5408\u6210\u6570\u636e\u96c6\u548c\u6807\u51c6\u89e3\u7ea0\u7f20\u57fa\u51c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u6709\u6548\u8bc6\u522b\u529f\u80fd\u7535\u8def\uff0cFactorVAE\u5728\u89e3\u7ea0\u7f20\u5206\u6570\uff080.084\uff09\u548c\u56e0\u679c\u6548\u5e94\u5f3a\u5ea6\uff08\u5747\u503c4.59\uff09\u4e0a\u4f18\u4e8e\u6807\u51c6VAE\u548cBeta-VAE\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u5347\u4e86\u751f\u6210\u6a21\u578b\u7684\u673a\u5236\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u8bbe\u8ba1\u548c\u4f18\u5316\u66f4\u900f\u660e\u3001\u53ef\u63a7\u7684VAE\u67b6\u6784\u63d0\u4f9b\u4e86\u5de5\u5177\u3002"}}
{"id": "2505.02886", "pdf": "https://arxiv.org/pdf/2505.02886", "abs": "https://arxiv.org/abs/2505.02886", "authors": ["David H. Silver"], "title": "Taskmaster Deconstructed: A Quantitative Look at Tension, Volatility, and Viewer Ratings", "categories": ["physics.soc-ph", "cs.AI", "62P25 (Applications to social sciences), 91B14 (Social choice) 62P25", "H.5.1; I.2.7"], "comment": "29 pages, includes 5 figures and 18 supplementary visualizations.\n  Submitted as a preprint. Code and data available at github dot com slash\n  silverdavi slash taskmaster-stats", "summary": "Taskmaster is a British television show that combines comedic performance\nwith a formal scoring system. Despite the appearance of structured competition,\nit remains unclear whether scoring dynamics contribute meaningfully to audience\nengagement. We conducted a statistical analysis of 162 episodes across 18\nseries, using fifteen episode-level metrics to quantify rank volatility, point\nspread, lead changes, and winner dominance. None of these metrics showed a\nsignificant association with IMDb ratings, even after controlling for series\neffects. Long-term trends suggest that average points have increased over time,\nwhile volatility has slightly declined and rank spread has remained stable.\nThese patterns indicate an attempt to enhance competitive visibility without\naltering the show's structural equilibrium. We also analyzed contestant rank\ntrajectories and identified five recurring archetypes describing performance\nstyles. These patterns suggest that viewer interest is shaped more by\ncontestant behavior than by game mechanics.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u82f1\u56fd\u7535\u89c6\u8282\u76ee\u300aTaskmaster\u300b\u7684\u8bc4\u5206\u52a8\u6001\u5bf9\u89c2\u4f17\u5438\u5f15\u529b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u8bc4\u5206\u6307\u6807\u4e0eIMDb\u8bc4\u5206\u65e0\u663e\u8457\u5173\u8054\uff0c\u89c2\u4f17\u5174\u8da3\u66f4\u591a\u53d7\u9009\u624b\u884c\u4e3a\u800c\u975e\u6e38\u620f\u673a\u5236\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u300aTaskmaster\u300b\u4e2d\u7ed3\u6784\u5316\u8bc4\u5206\u7cfb\u7edf\u662f\u5426\u771f\u6b63\u5f71\u54cd\u89c2\u4f17\u53c2\u4e0e\u5ea6\u3002", "method": "\u5bf918\u5b63162\u96c6\u7684\u5341\u4e94\u9879\u8bc4\u5206\u6307\u6807\uff08\u5982\u6392\u540d\u6ce2\u52a8\u3001\u5206\u6570\u5dee\u8ddd\u7b49\uff09\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\uff0c\u5e76\u5206\u6790\u9009\u624b\u7684\u6392\u540d\u8f68\u8ff9\u3002", "result": "\u8bc4\u5206\u52a8\u6001\u4e0eIMDb\u8bc4\u5206\u65e0\u663e\u8457\u5173\u8054\uff1b\u957f\u671f\u8d8b\u52bf\u663e\u793a\u5e73\u5747\u5206\u6570\u4e0a\u5347\uff0c\u6ce2\u52a8\u6027\u7565\u964d\uff0c\u6392\u540d\u5dee\u8ddd\u7a33\u5b9a\uff1b\u9009\u624b\u884c\u4e3a\u6a21\u5f0f\u66f4\u5f71\u54cd\u89c2\u4f17\u5174\u8da3\u3002", "conclusion": "\u8282\u76ee\u901a\u8fc7\u8c03\u6574\u7ade\u4e89\u53ef\u89c6\u6027\u4fdd\u6301\u7ed3\u6784\u5e73\u8861\uff0c\u89c2\u4f17\u5174\u8da3\u4e3b\u8981\u7531\u9009\u624b\u884c\u4e3a\u9a71\u52a8\uff0c\u800c\u975e\u8bc4\u5206\u673a\u5236\u3002"}}
{"id": "2505.03533", "pdf": "https://arxiv.org/pdf/2505.03533", "abs": "https://arxiv.org/abs/2505.03533", "authors": ["Jiacheng Wang", "Le Liang", "Hao Ye", "Chongtao Guo", "Shi Jin"], "title": "Small-Scale-Fading-Aware Resource Allocation in Wireless Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "Judicious resource allocation can effectively enhance federated learning (FL)\ntraining performance in wireless networks by addressing both system and\nstatistical heterogeneity. However, existing strategies typically rely on block\nfading assumptions, which overlooks rapid channel fluctuations within each\nround of FL gradient uploading, leading to a degradation in FL training\nperformance. Therefore, this paper proposes a small-scale-fading-aware resource\nallocation strategy using a multi-agent reinforcement learning (MARL)\nframework. Specifically, we establish a one-step convergence bound of the FL\nalgorithm and formulate the resource allocation problem as a decentralized\npartially observable Markov decision process (Dec-POMDP), which is subsequently\nsolved using the QMIX algorithm. In our framework, each client serves as an\nagent that dynamically determines spectrum and power allocations within each\ncoherence time slot, based on local observations and a reward derived from the\nconvergence analysis. The MARL setting reduces the dimensionality of the action\nspace and facilitates decentralized decision-making, enhancing the scalability\nand practicality of the solution. Experimental results demonstrate that our\nQMIX-based resource allocation strategy significantly outperforms baseline\nmethods across various degrees of statistical heterogeneity. Additionally,\nablation studies validate the critical importance of incorporating small-scale\nfading dynamics, highlighting its role in optimizing FL performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7684\u8d44\u6e90\u5206\u914d\u7b56\u7565\uff0c\u4ee5\u4f18\u5316\u65e0\u7ebf\u7f51\u7edc\u4e2d\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u9891\u8c31\u548c\u529f\u7387\u5206\u914d\u6765\u5e94\u5bf9\u5feb\u901f\u4fe1\u9053\u6ce2\u52a8\u3002", "motivation": "\u73b0\u6709\u7684\u8d44\u6e90\u5206\u914d\u7b56\u7565\u901a\u5e38\u5ffd\u7565FL\u68af\u5ea6\u4e0a\u4f20\u8fc7\u7a0b\u4e2d\u7684\u5feb\u901f\u4fe1\u9053\u6ce2\u52a8\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u52a8\u6001\u7684\u5206\u914d\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528QMIX\u7b97\u6cd5\u89e3\u51b3\u90e8\u5206\u53ef\u89c2\u6d4b\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08Dec-POMDP\uff09\u95ee\u9898\uff0c\u6bcf\u4e2a\u5ba2\u6237\u7aef\u4f5c\u4e3a\u667a\u80fd\u4f53\u52a8\u6001\u5206\u914d\u9891\u8c31\u548c\u529f\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7edf\u8ba1\u5f02\u6784\u6027\u4e0d\u540c\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u8003\u8651\u5c0f\u5c3a\u5ea6\u8870\u843d\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u7b56\u7565\u901a\u8fc7MARL\u6846\u67b6\u4f18\u5316\u8d44\u6e90\u5206\u914d\uff0c\u63d0\u5347\u4e86FL\u8bad\u7ec3\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.02887", "pdf": "https://arxiv.org/pdf/2505.02887", "abs": "https://arxiv.org/abs/2505.02887", "authors": ["Cheng Ge", "Han-Shen Tae", "Zhenqiang Zhang", "Lu Lu", "Zhijie Huang", "Yilin Wang", "Tao Jiang", "Wenqing Cai", "Shan Chang", "David J. Adams", "Rilei Yu"], "title": "CreoPep: A Universal Deep Learning Framework for Target-Specific Peptide Design and Optimization", "categories": ["q-bio.BM", "cs.AI", "cs.LG"], "comment": "16 pages, 6 figures", "summary": "Target-specific peptides, such as conotoxins, exhibit exceptional binding\naffinity and selectivity toward ion channels and receptors. However, their\ntherapeutic potential remains underutilized due to the limited diversity of\nnatural variants and the labor-intensive nature of traditional optimization\nstrategies. Here, we present CreoPep, a deep learning-based conditional\ngenerative framework that integrates masked language modeling with a\nprogressive masking scheme to design high-affinity peptide mutants while\nuncovering novel structural motifs. CreoPep employs an integrative augmentation\npipeline, combining FoldX-based energy screening with temperature-controlled\nmultinomial sampling, to generate structurally and functionally diverse\npeptides that retain key pharmacological properties. We validate this approach\nby designing conotoxin inhibitors targeting the $\\alpha$7 nicotinic\nacetylcholine receptor, achieving submicromolar potency in electrophysiological\nassays. Structural analysis reveals that CreoPep-generated variants engage in\nboth conserved and novel binding modes, including disulfide-deficient forms,\nthus expanding beyond conventional design paradigms. Overall, CreoPep offers a\nrobust and generalizable platform that bridges computational peptide design\nwith experimental validation, accelerating the discovery of next-generation\npeptide therapeutics.", "AI": {"tldr": "CreoPep\u662f\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6761\u4ef6\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u8bbe\u8ba1\u9ad8\u4eb2\u548c\u529b\u80bd\u53d8\u4f53\u5e76\u53d1\u73b0\u65b0\u7ed3\u6784\u57fa\u5e8f\uff0c\u901a\u8fc7\u6574\u5408\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u548c\u6e10\u8fdb\u63a9\u7801\u65b9\u6848\uff0c\u7ed3\u5408FoldX\u80fd\u91cf\u7b5b\u9009\u548c\u6e29\u5ea6\u63a7\u5236\u591a\u6001\u91c7\u6837\uff0c\u6210\u529f\u8bbe\u8ba1\u51fa\u9488\u5bf9\u03b17\u70df\u78b1\u578b\u4e59\u9170\u80c6\u78b1\u53d7\u4f53\u7684\u5f3a\u6548\u6291\u5236\u5242\uff0c\u5c55\u793a\u4e86\u5176\u5728\u80bd\u7597\u6cd5\u8bbe\u8ba1\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u5929\u7136\u80bd\u53d8\u4f53\u7684\u591a\u6837\u6027\u6709\u9650\u4e14\u4f20\u7edf\u4f18\u5316\u7b56\u7565\u8d39\u65f6\u8d39\u529b\uff0c\u9650\u5236\u4e86\u9776\u5411\u80bd\uff08\u5982\u828b\u87ba\u6bd2\u7d20\uff09\u5728\u6cbb\u7597\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u8ba1\u7b97\u65b9\u6cd5\u52a0\u901f\u80bd\u836f\u7269\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u6761\u4ef6\u751f\u6210\u6846\u67b6CreoPep\uff0c\u7ed3\u5408\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u548c\u6e10\u8fdb\u63a9\u7801\u65b9\u6848\uff0c\u901a\u8fc7FoldX\u80fd\u91cf\u7b5b\u9009\u548c\u6e29\u5ea6\u63a7\u5236\u591a\u6001\u91c7\u6837\u751f\u6210\u7ed3\u6784\u529f\u80fd\u591a\u6837\u7684\u80bd\u53d8\u4f53\u3002", "result": "\u6210\u529f\u8bbe\u8ba1\u51fa\u9488\u5bf9\u03b17\u70df\u78b1\u578b\u4e59\u9170\u80c6\u78b1\u53d7\u4f53\u7684\u4e9a\u5fae\u6469\u5c14\u7ea7\u5f3a\u6548\u6291\u5236\u5242\uff0c\u7ed3\u6784\u5206\u6790\u663e\u793a\u53d8\u4f53\u5177\u6709\u4fdd\u5b88\u548c\u65b0\u9896\u7684\u7ed3\u5408\u6a21\u5f0f\uff0c\u5305\u62ec\u65e0\u4e8c\u786b\u952e\u5f62\u5f0f\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u8bbe\u8ba1\u8303\u5f0f\u3002", "conclusion": "CreoPep\u4e3a\u8ba1\u7b97\u80bd\u8bbe\u8ba1\u4e0e\u5b9e\u9a8c\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u5f3a\u5927\u4e14\u901a\u7528\u6027\u5f3a\u7684\u5e73\u53f0\uff0c\u6709\u671b\u52a0\u901f\u4e0b\u4e00\u4ee3\u80bd\u7597\u6cd5\u7684\u53d1\u73b0\u3002"}}
{"id": "2505.03552", "pdf": "https://arxiv.org/pdf/2505.03552", "abs": "https://arxiv.org/abs/2505.03552", "authors": ["Linus Langenkamp", "Philip Hannebohm", "Bernhard Bachmann"], "title": "Efficient Training of Physics-enhanced Neural ODEs via Direct Collocation and Nonlinear Programming", "categories": ["cs.LG", "math.DS", "math.OC", "90C30, 68T05", "G.1.6; I.2.6"], "comment": "16 pages, 9 figures, submitted to 16th International Modelica & FMI\n  Conference", "summary": "We propose a novel approach for training Physics-enhanced Neural ODEs\n(PeNODEs) by expressing the training process as a dynamic optimization problem.\nThe full model, including neural components, is discretized using a high-order\nimplicit Runge-Kutta method with flipped Legendre-Gauss-Radau points, resulting\nin a large-scale nonlinear program (NLP) efficiently solved by state-of-the-art\nNLP solvers such as Ipopt. This formulation enables simultaneous optimization\nof network parameters and state trajectories, addressing key limitations of ODE\nsolver-based training in terms of stability, runtime, and accuracy. Extending\non a recent direct collocation-based method for Neural ODEs, we generalize to\nPeNODEs, incorporate physical constraints, and present a custom, parallelized,\nopen-source implementation. Benchmarks on a Quarter Vehicle Model and a\nVan-der-Pol oscillator demonstrate superior accuracy, speed, and generalization\nwith smaller networks compared to other training techniques. We also outline a\nplanned integration into OpenModelica to enable accessible training of Neural\nDAEs.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bad\u7ec3\u7269\u7406\u589e\u5f3a\u795e\u7ecfODE\uff08PeNODEs\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8bad\u7ec3\u8fc7\u7a0b\u8868\u8fbe\u4e3a\u52a8\u6001\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u7a33\u5b9a\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e3a\u4e86\u6539\u8fdb\u57fa\u4e8eODE\u6c42\u89e3\u5668\u7684\u8bad\u7ec3\u65b9\u6cd5\u5728\u7a33\u5b9a\u6027\u3001\u8fd0\u884c\u65f6\u95f4\u548c\u51c6\u786e\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u7ed3\u5408\u7269\u7406\u7ea6\u675f\u4ee5\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "method": "\u91c7\u7528\u9ad8\u9636\u9690\u5f0fRunge-Kutta\u65b9\u6cd5\u548c\u7ffb\u8f6cLegendre-Gauss-Radau\u70b9\u79bb\u6563\u5316\u6a21\u578b\uff0c\u751f\u6210\u5927\u89c4\u6a21\u975e\u7ebf\u6027\u7a0b\u5e8f\uff08NLP\uff09\uff0c\u5e76\u5229\u7528Ipopt\u7b49NLP\u6c42\u89e3\u5668\u9ad8\u6548\u6c42\u89e3\u3002", "result": "\u901a\u8fc7Quarter Vehicle Model\u548cVan-der-Pol\u632f\u8361\u5668\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u901f\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u4f18\u52bf\uff0c\u4e14\u53ef\u4ee5\u4f7f\u7528\u66f4\u5c0f\u7684\u7f51\u7edc\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7a33\u5b9a\u7684PeNODEs\u8bad\u7ec3\u65b9\u6cd5\uff0c\u672a\u6765\u8ba1\u5212\u6574\u5408\u5230OpenModelica\u4e2d\u4ee5\u652f\u6301\u795e\u7ecfDAE\u7684\u6613\u7528\u8bad\u7ec3\u3002"}}
{"id": "2505.03560", "pdf": "https://arxiv.org/pdf/2505.03560", "abs": "https://arxiv.org/abs/2505.03560", "authors": ["Simon Baeuerle", "Ian F. Mendonca", "Kristof Van Laerhoven", "Ralf Mikut", "Andreas Steimer"], "title": "Rapid AI-based generation of coverage paths for dispensing applications", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Coverage Path Planning of Thermal Interface Materials (TIM) plays a crucial\nrole in the design of power electronics and electronic control units. Up to\nnow, this is done manually by experts or by using optimization approaches with\na high computational effort. We propose a novel AI-based approach to generate\ndispense paths for TIM and similar dispensing applications. It is a drop-in\nreplacement for optimization-based approaches. An Artificial Neural Network\n(ANN) receives the target cooling area as input and directly outputs the\ndispense path. Our proposed setup does not require labels and we show its\nfeasibility on multiple target areas. The resulting dispense paths can be\ndirectly transferred to automated manufacturing equipment and do not exhibit\nair entrapments. The approach of using an ANN to predict process parameters for\na desired target state in real-time could potentially be transferred to other\nmanufacturing processes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u70ed\u754c\u9762\u6750\u6599\uff08TIM\uff09\u7684\u55b7\u6d82\u8def\u5f84\uff0c\u66ff\u4ee3\u4f20\u7edf\u624b\u52a8\u6216\u9ad8\u8ba1\u7b97\u91cf\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "motivation": "\u76ee\u524dTIM\u7684\u55b7\u6d82\u8def\u5f84\u89c4\u5212\u4f9d\u8d56\u4e13\u5bb6\u624b\u52a8\u6216\u8ba1\u7b97\u5bc6\u96c6\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u4e14\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u65e0\u6807\u7b7e\u8bad\u7ec3\u7684\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANN\uff09\uff0c\u76f4\u63a5\u6839\u636e\u76ee\u6807\u51b7\u5374\u533a\u57df\u751f\u6210\u55b7\u6d82\u8def\u5f84\u3002", "result": "\u751f\u6210\u7684\u8def\u5f84\u53ef\u76f4\u63a5\u7528\u4e8e\u81ea\u52a8\u5316\u5236\u9020\u8bbe\u5907\uff0c\u4e14\u907f\u514d\u4e86\u7a7a\u6c14\u5939\u5e26\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5728\u591a\u76ee\u6807\u533a\u57df\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u5b9e\u65f6\u9884\u6d4b\u5236\u9020\u5de5\u827a\u53c2\u6570\uff0c\u5e76\u53ef\u80fd\u63a8\u5e7f\u81f3\u5176\u4ed6\u5236\u9020\u6d41\u7a0b\u3002"}}
{"id": "2505.03561", "pdf": "https://arxiv.org/pdf/2505.03561", "abs": "https://arxiv.org/abs/2505.03561", "authors": ["Leo Maxime Brunswic", "Mateo Clemente", "Rui Heng Yang", "Adam Sigal", "Amir Rasouli", "Yinchuan Li"], "title": "Ergodic Generative Flows", "categories": ["cs.LG", "cs.AI", "math.DG", "math.DS", "37A25, 68T07, 68W20, 68Q87, 68T99"], "comment": "20 pages, 5 figures, 1 table, accepted at ICML 2025", "summary": "Generative Flow Networks (GFNs) were initially introduced on directed acyclic\ngraphs to sample from an unnormalized distribution density. Recent works have\nextended the theoretical framework for generative methods allowing more\nflexibility and enhancing application range. However, many challenges remain in\ntraining GFNs in continuous settings and for imitation learning (IL), including\nintractability of flow-matching loss, limited tests of non-acyclic training,\nand the need for a separate reward model in imitation learning. The present\nwork proposes a family of generative flows called Ergodic Generative Flows\n(EGFs) which are used to address the aforementioned issues. First, we leverage\nergodicity to build simple generative flows with finitely many globally defined\ntransformations (diffeomorphisms) with universality guarantees and tractable\nflow-matching loss (FM loss). Second, we introduce a new loss involving\ncross-entropy coupled to weak flow-matching control, coined KL-weakFM loss. It\nis designed for IL training without a separate reward model. We evaluate\nIL-EGFs on toy 2D tasks and real-world datasets from NASA on the sphere, using\nthe KL-weakFM loss. Additionally, we conduct toy 2D reinforcement learning\nexperiments with a target reward, using the FM loss.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7c7b\u79f0\u4e3a\u904d\u5386\u751f\u6210\u6d41(EGFs)\u7684\u751f\u6210\u6d41\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u751f\u6210\u6d41\u7f51\u7edc(GFNs)\u5728\u8fde\u7eed\u73af\u5883\u548c\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u6311\u6218\uff0c\u5982\u6d41\u5339\u914d\u635f\u5931\u7684\u96be\u5904\u7406\u6027\u548c\u5bf9\u5355\u72ec\u5956\u52b1\u6a21\u578b\u7684\u4f9d\u8d56\u3002", "motivation": "\u4f20\u7edfGFNs\u5728\u8fde\u7eed\u73af\u5883\u548c\u6a21\u4eff\u5b66\u4e60\u4e2d\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u5982\u6d41\u5339\u914d\u635f\u5931\u7684\u96be\u5904\u7406\u6027\u548c\u9700\u8981\u5355\u72ec\u5956\u52b1\u6a21\u578b\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7EGFs\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86EGFs\uff0c\u5229\u7528\u904d\u5386\u6027\u6784\u5efa\u7b80\u5355\u7684\u751f\u6210\u6d41\uff0c\u5177\u6709\u5168\u5c40\u5b9a\u4e49\u7684\u53d8\u6362\u548c\u53ef\u5904\u7406\u7684\u6d41\u5339\u914d\u635f\u5931\uff1b\u5e76\u8bbe\u8ba1\u4e86KL-weakFM\u635f\u5931\uff0c\u7528\u4e8e\u65e0\u9700\u5355\u72ec\u5956\u52b1\u6a21\u578b\u7684\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "\u57282D\u73a9\u5177\u4efb\u52a1\u548cNASA\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86IL-EGFs\uff0c\u4f7f\u7528KL-weakFM\u635f\u5931\uff1b\u5e76\u57282D\u5f3a\u5316\u5b66\u4e60\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86FM\u635f\u5931\u7684\u6709\u6548\u6027\u3002", "conclusion": "EGFs\u901a\u8fc7\u904d\u5386\u6027\u548c\u65b0\u8bbe\u8ba1\u7684\u635f\u5931\u51fd\u6570\uff0c\u6709\u6548\u89e3\u51b3\u4e86GFNs\u5728\u8fde\u7eed\u73af\u5883\u548c\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u95ee\u9898\uff0c\u5c55\u73b0\u4e86\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2505.03595", "pdf": "https://arxiv.org/pdf/2505.03595", "abs": "https://arxiv.org/abs/2505.03595", "authors": ["Sidharth S. Menon", "Ameya D. Jagtap"], "title": "Anant-Net: Breaking the Curse of Dimensionality with Scalable and Interpretable Neural Surrogate for High-Dimensional PDEs", "categories": ["cs.LG"], "comment": "27 pages, 13 figures", "summary": "High-dimensional partial differential equations (PDEs) arise in diverse\nscientific and engineering applications but remain computationally intractable\ndue to the curse of dimensionality. Traditional numerical methods struggle with\nthe exponential growth in computational complexity, particularly on hypercubic\ndomains, where the number of required collocation points increases rapidly with\ndimensionality. Here, we introduce Anant-Net, an efficient neural surrogate\nthat overcomes this challenge, enabling the solution of PDEs in high\ndimensions. Unlike hyperspheres, where the internal volume diminishes as\ndimensionality increases, hypercubes retain or expand their volume (for unit or\nlarger length), making high-dimensional computations significantly more\ndemanding. Anant-Net efficiently incorporates high-dimensional boundary\nconditions and minimizes the PDE residual at high-dimensional collocation\npoints. To enhance interpretability, we integrate Kolmogorov-Arnold networks\ninto the Anant-Net architecture. We benchmark Anant-Net's performance on\nseveral linear and nonlinear high-dimensional equations, including the Poisson,\nSine-Gordon, and Allen-Cahn equations, demonstrating high accuracy and\nrobustness across randomly sampled test points from high-dimensional space.\nImportantly, Anant-Net achieves these results with remarkable efficiency,\nsolving 300-dimensional problems on a single GPU within a few hours. We also\ncompare Anant-Net's results for accuracy and runtime with other\nstate-of-the-art methods. Our findings establish Anant-Net as an accurate,\ninterpretable, and scalable framework for efficiently solving high-dimensional\nPDEs.", "AI": {"tldr": "Anant-Net\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u795e\u7ecf\u4ee3\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u9ad8\u7ef4\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u7ef4\u6570\u707e\u96be\u95ee\u9898\uff0c\u5c24\u5176\u5728\u8d85\u7acb\u65b9\u4f53\u57df\u4e0a\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u5728\u9ad8\u7ef4PDE\u4e2d\u56e0\u8ba1\u7b97\u590d\u6742\u5ea6\u6307\u6570\u7ea7\u589e\u957f\u800c\u5931\u6548\uff0c\u5c24\u5176\u5728\u8d85\u7acb\u65b9\u4f53\u57df\u4e0a\uff0cAnant-Net\u65e8\u5728\u514b\u670d\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u901a\u8fc7\u795e\u7ecf\u4ee3\u7406Anant-Net\u9ad8\u6548\u5904\u7406\u9ad8\u7ef4\u8fb9\u754c\u6761\u4ef6\u5e76\u6700\u5c0f\u5316PDE\u6b8b\u5dee\uff0c\u540c\u65f6\u5f15\u5165Kolmogorov-Arnold\u7f51\u7edc\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728Poisson\u3001Sine-Gordon\u548cAllen-Cahn\u7b49\u65b9\u7a0b\u4e2d\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u5355GPU\u51e0\u5c0f\u65f6\u5185\u89e3\u51b3300\u7ef4\u95ee\u9898\u3002", "conclusion": "Anant-Net\u4e3a\u9ad8\u7ef4PDE\u63d0\u4f9b\u4e86\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02945", "pdf": "https://arxiv.org/pdf/2505.02945", "abs": "https://arxiv.org/abs/2505.02945", "authors": ["Egil Diau"], "title": "The Cognitive Foundations of Economic Exchange: A Modular Framework Grounded in Behavioral Evidence", "categories": ["cs.CY", "cs.AI", "cs.MA"], "comment": "This is a position paper. Theoretical framework is finalized; minor\n  language revisions are ongoing. Submitted for feedback and public discussion", "summary": "A key challenge in multi-agent AI is modeling social cooperation under\nrealistic behavioral constraints. Many foundational concepts in economics and\nethics such as \"trust\" or \"morality\" are often defined informally, without\noperational criteria or cognitive grounding, which limits their testability and\nimplementation in artificial agents. Drawing on converging empirical evidence\nfrom primate behavior, infant cognition, and economic anthropology, we propose\na conceptual framework composed of three cognitively minimal mechanisms:\nindividual recognition, reciprocal credence, and cost return sensitivity. This\nframework reframes trust as a graded cognitive expectation, providing a\nsimulateable basis for reciprocal exchange in artificial agents, and enabling\nthe bottom-up emergence of scalable cooperation and institutional dynamics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e09\u79cd\u8ba4\u77e5\u6700\u5c0f\u673a\u5236\uff08\u4e2a\u4f53\u8bc6\u522b\u3001\u4e92\u76f8\u4fe1\u4efb\u548c\u6210\u672c\u56de\u62a5\u654f\u611f\u6027\uff09\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u667a\u80fd\u4f53AI\u4e2d\u793e\u4f1a\u5408\u4f5c\u7684\u5efa\u6a21\u95ee\u9898\u3002", "motivation": "\u7ecf\u6d4e\u5b66\u548c\u4f26\u7406\u5b66\u4e2d\u8bb8\u591a\u57fa\u7840\u6982\u5ff5\uff08\u5982\u201c\u4fe1\u4efb\u201d\u6216\u201c\u9053\u5fb7\u201d\uff09\u901a\u5e38\u5b9a\u4e49\u6a21\u7cca\uff0c\u7f3a\u4e4f\u64cd\u4f5c\u6807\u51c6\u6216\u8ba4\u77e5\u57fa\u7840\uff0c\u9650\u5236\u4e86\u5176\u5728\u4eba\u5de5\u4ee3\u7406\u4e2d\u7684\u53ef\u6d4b\u8bd5\u6027\u548c\u5b9e\u73b0\u3002", "method": "\u901a\u8fc7\u6574\u5408\u7075\u957f\u7c7b\u884c\u4e3a\u3001\u5a74\u513f\u8ba4\u77e5\u548c\u7ecf\u6d4e\u4eba\u7c7b\u5b66\u7684\u5b9e\u8bc1\u8bc1\u636e\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u4e09\u79cd\u8ba4\u77e5\u6700\u5c0f\u673a\u5236\u7684\u6982\u5ff5\u6846\u67b6\u3002", "result": "\u8be5\u6846\u67b6\u5c06\u4fe1\u4efb\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e00\u79cd\u5206\u7ea7\u8ba4\u77e5\u9884\u671f\uff0c\u4e3a\u4eba\u5de5\u4ee3\u7406\u4e2d\u7684\u4e92\u60e0\u4ea4\u6362\u63d0\u4f9b\u4e86\u53ef\u6a21\u62df\u7684\u57fa\u7840\uff0c\u5e76\u5b9e\u73b0\u4e86\u81ea\u4e0b\u800c\u4e0a\u6d8c\u73b0\u7684\u53ef\u6269\u5c55\u5408\u4f5c\u548c\u5236\u5ea6\u52a8\u6001\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u591a\u667a\u80fd\u4f53AI\u4e2d\u7684\u793e\u4f1a\u5408\u4f5c\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u3001\u8ba4\u77e5\u57fa\u7840\u624e\u5b9e\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.03617", "pdf": "https://arxiv.org/pdf/2505.03617", "abs": "https://arxiv.org/abs/2505.03617", "authors": ["Thien Nhan Vo", "Thanh Xuan Truong"], "title": "Understand the Effect of Importance Weighting in Deep Learning on Dataset Shift", "categories": ["cs.LG"], "comment": null, "summary": "We evaluate the effectiveness of importance weighting in deep neural networks\nunder label shift and covariate shift. On synthetic 2D data (linearly separable\nand moon-shaped) using logistic regression and MLPs, we observe that weighting\nstrongly affects decision boundaries early in training but fades with prolonged\noptimization. On CIFAR-10 with various class imbalances, only L2 regularization\n(not dropout) helps preserve weighting effects. In a covariate-shift\nexperiment, importance weighting yields no significant performance gain,\nhighlighting challenges on complex data. Our results call into question the\npractical utility of importance weighting for real-world distribution shifts.", "AI": {"tldr": "\u7814\u7a76\u4e86\u91cd\u8981\u6027\u52a0\u6743\u5728\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u5bf9\u6807\u7b7e\u504f\u79fb\u548c\u534f\u53d8\u91cf\u504f\u79fb\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u5176\u6548\u679c\u968f\u8bad\u7ec3\u65f6\u95f4\u51cf\u5f31\uff0c\u590d\u6742\u6570\u636e\u4e2d\u6548\u679c\u6709\u9650\u3002", "motivation": "\u63a2\u7d22\u91cd\u8981\u6027\u52a0\u6743\u5728\u5b9e\u9645\u5206\u5e03\u504f\u79fb\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u5c40\u9650\u6027\u3002", "method": "\u5728\u5408\u62102D\u6570\u636e\u548cCIFAR-10\u4e0a\u6d4b\u8bd5\u903b\u8f91\u56de\u5f52\u548cMLP\uff0c\u7ed3\u5408L2\u6b63\u5219\u5316\u548cdropout\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u52a0\u6743\u5728\u65e9\u671f\u5f71\u54cd\u660e\u663e\u4f46\u540e\u671f\u51cf\u5f31\uff0c\u4ec5\u5728L2\u6b63\u5219\u5316\u4e0b\u8868\u73b0\u8f83\u597d\uff0c\u590d\u6742\u6570\u636e\u4e2d\u65e0\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u91cd\u8981\u6027\u52a0\u6743\u5728\u771f\u5b9e\u4e16\u754c\u5206\u5e03\u504f\u79fb\u4e2d\u7684\u5b9e\u7528\u6027\u5b58\u7591\u3002"}}
{"id": "2505.02966", "pdf": "https://arxiv.org/pdf/2505.02966", "abs": "https://arxiv.org/abs/2505.02966", "authors": ["Alexander Holmberg"], "title": "Generating Narrated Lecture Videos from Slides with Synchronized Highlights", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Turning static slides into engaging video lectures takes considerable time\nand effort, requiring presenters to record explanations and visually guide\ntheir audience through the material. We introduce an end-to-end system designed\nto automate this process entirely. Given a slide deck, this system synthesizes\na video lecture featuring AI-generated narration synchronized precisely with\ndynamic visual highlights. These highlights automatically draw attention to the\nspecific concept being discussed, much like an effective presenter would. The\ncore technical contribution is a novel highlight alignment module. This module\naccurately maps spoken phrases to locations on a given slide using diverse\nstrategies (e.g., Levenshtein distance, LLM-based semantic analysis) at\nselectable granularities (line or word level) and utilizes timestamp-providing\nText-to-Speech (TTS) for timing synchronization. We demonstrate the system's\neffectiveness through a technical evaluation using a manually annotated slide\ndataset with 1000 samples, finding that LLM-based alignment achieves high\nlocation accuracy (F1 > 92%), significantly outperforming simpler methods,\nespecially on complex, math-heavy content. Furthermore, the calculated\ngeneration cost averages under $1 per hour of video, offering potential savings\nof two orders of magnitude compared to conservative estimates of manual\nproduction costs. This combination of high accuracy and extremely low cost\npositions this approach as a practical and scalable tool for transforming\nstatic slides into effective, visually-guided video lectures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u5c06\u9759\u6001\u5e7b\u706f\u7247\u8f6c\u6362\u4e3a\u5e26\u6709AI\u751f\u6210\u8bb2\u89e3\u548c\u52a8\u6001\u89c6\u89c9\u91cd\u70b9\u7684\u89c6\u9891\u8bb2\u5ea7\uff0c\u6838\u5fc3\u662f\u65b0\u9896\u7684\u9ad8\u4eae\u5bf9\u9f50\u6a21\u5757\uff0c\u5229\u7528\u591a\u79cd\u7b56\u7565\u7cbe\u786e\u540c\u6b65\u8bb2\u89e3\u4e0e\u89c6\u89c9\u5143\u7d20\uff0c\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u4e14\u6210\u672c\u6781\u4f4e\u3002", "motivation": "\u5c06\u9759\u6001\u5e7b\u706f\u7247\u8f6c\u6362\u4e3a\u89c6\u9891\u8bb2\u5ea7\u901a\u5e38\u8017\u65f6\u8017\u529b\uff0c\u9700\u8981\u4eba\u5de5\u5f55\u5236\u8bb2\u89e3\u548c\u89c6\u89c9\u5f15\u5bfc\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u7cfb\u7edf\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u4f9b\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u7684\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u7ed3\u5408\u9ad8\u4eae\u5bf9\u9f50\u6a21\u5757\uff08\u4f7f\u7528Levenshtein\u8ddd\u79bb\u548cLLM\u8bed\u4e49\u5206\u6790\uff09\u548cTTS\u65f6\u95f4\u540c\u6b65\uff0c\u751f\u6210\u52a8\u6001\u9ad8\u4eae\u4e0e\u8bb2\u89e3\u540c\u6b65\u7684\u89c6\u9891\u3002", "result": "\u57281000\u4e2a\u6837\u672c\u7684\u6570\u636e\u96c6\u4e0a\uff0cLLM\u5bf9\u9f50\u7684\u51c6\u786e\u7387\uff08F1>92%\uff09\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5c24\u5176\u5bf9\u590d\u6742\u6570\u5b66\u5185\u5bb9\u66f4\u6709\u6548\uff0c\u4e14\u751f\u6210\u6210\u672c\u4f4e\u4e8e1\u7f8e\u5143/\u5c0f\u65f6\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4ee5\u9ad8\u7cbe\u5ea6\u548c\u6781\u4f4e\u6210\u672c\u5b9e\u73b0\u4e86\u5e7b\u706f\u7247\u5230\u89c6\u9891\u8bb2\u5ea7\u7684\u81ea\u52a8\u5316\u8f6c\u6362\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.03646", "pdf": "https://arxiv.org/pdf/2505.03646", "abs": "https://arxiv.org/abs/2505.03646", "authors": ["Chethan Krishnamurthy Ramanaik", "Arjun Roy", "Eirini Ntoutsi"], "title": "ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Despite the extensive use of deep autoencoders (AEs) in critical\napplications, their adversarial robustness remains relatively underexplored\ncompared to classification models. AE robustness is characterized by the\nLipschitz bounds of its components. Existing robustness evaluation frameworks\nbased on white-box attacks do not fully exploit the vulnerabilities of\nintermediate ill-conditioned layers in AEs. In the context of optimizing\nimperceptible norm-bounded additive perturbations to maximize output damage,\nexisting methods struggle to effectively propagate adversarial loss gradients\nthroughout the network, often converging to less effective perturbations. To\naddress this, we propose a novel layer-conditioning-based adversarial\noptimization objective that effectively guides the adversarial map toward\nregions of local Lipschitz bounds by enhancing loss gradient information\npropagation during attack optimization. We demonstrate through extensive\nexperiments on state-of-the-art AEs that our adversarial objective results in\nstronger attacks, outperforming existing methods in both universal and\nsample-specific scenarios. As a defense method against this attack, we\nintroduce an inference-time adversarially trained defense plugin that mitigates\nthe effects of adversarial examples.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c42\u6761\u4ef6\u5bf9\u6297\u4f18\u5316\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u589e\u5f3a\u81ea\u7f16\u7801\u5668\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u5c3d\u7ba1\u81ea\u7f16\u7801\u5668\u5728\u5173\u952e\u5e94\u7528\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u5bf9\u6297\u9c81\u68d2\u6027\u7814\u7a76\u76f8\u6bd4\u5206\u7c7b\u6a21\u578b\u4ecd\u663e\u4e0d\u8db3\u3002\u73b0\u6709\u57fa\u4e8e\u767d\u76d2\u653b\u51fb\u7684\u8bc4\u4f30\u6846\u67b6\u672a\u80fd\u5145\u5206\u6316\u6398\u81ea\u7f16\u7801\u5668\u4e2d\u6761\u4ef6\u4e0d\u4f73\u7684\u4e2d\u5c42\u8106\u5f31\u6027\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5c42\u6761\u4ef6\u5bf9\u6297\u4f18\u5316\u76ee\u6807\uff0c\u901a\u8fc7\u589e\u5f3a\u5bf9\u6297\u635f\u5931\u68af\u5ea6\u7684\u4fe1\u606f\u4f20\u64ad\uff0c\u6709\u6548\u5f15\u5bfc\u5bf9\u6297\u6620\u5c04\u63a5\u8fd1\u5c40\u90e8Lipschitz\u8fb9\u754c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u901a\u7528\u548c\u6837\u672c\u7279\u5b9a\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u3002\u8bba\u6587\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u63a8\u7406\u65f6\u5bf9\u6297\u8bad\u7ec3\u9632\u5fa1\u63d2\u4ef6\uff0c\u7528\u4e8e\u7f13\u89e3\u5bf9\u6297\u6837\u672c\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u81ea\u7f16\u7801\u5668\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u8bc4\u4f30\u6548\u679c\uff0c\u5e76\u4e3a\u9632\u5fa1\u5bf9\u6297\u653b\u51fb\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.03652", "pdf": "https://arxiv.org/pdf/2505.03652", "abs": "https://arxiv.org/abs/2505.03652", "authors": ["Yihang Wang", "Chris Chi", "Aaron R. Dinner"], "title": "Mitigating mode collapse in normalizing flows by annealing with an adaptive schedule: Application to parameter estimation", "categories": ["cs.LG", "physics.comp-ph", "physics.data-an", "q-bio.QM", "stat.ML"], "comment": "19 pages, 10 figures", "summary": "Normalizing flows (NFs) provide uncorrelated samples from complex\ndistributions, making them an appealing tool for parameter estimation. However,\nthe practical utility of NFs remains limited by their tendency to collapse to a\nsingle mode of a multimodal distribution. In this study, we show that annealing\nwith an adaptive schedule based on the effective sample size (ESS) can mitigate\nmode collapse. We demonstrate that our approach can converge the marginal\nlikelihood for a biochemical oscillator model fit to time-series data in\nten-fold less computation time than a widely used ensemble Markov chain Monte\nCarlo (MCMC) method. We show that the ESS can also be used to reduce variance\nby pruning the samples. We expect these developments to be of general use for\nsampling with NFs and discuss potential opportunities for further improvements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6709\u6548\u6837\u672c\u5927\u5c0f\uff08ESS\uff09\u7684\u81ea\u9002\u5e94\u9000\u706b\u8c03\u5ea6\u65b9\u6cd5\uff0c\u7528\u4e8e\u7f13\u89e3\u5f52\u4e00\u5316\u6d41\uff08NFs\uff09\u5728\u591a\u6a21\u6001\u5206\u5e03\u4e2d\u7684\u6a21\u5f0f\u584c\u9677\u95ee\u9898\uff0c\u5e76\u5728\u751f\u5316\u632f\u8361\u5668\u6a21\u578b\u4e2d\u5c55\u793a\u4e86\u5176\u9ad8\u6548\u6027\u3002", "motivation": "\u5f52\u4e00\u5316\u6d41\uff08NFs\uff09\u5728\u53c2\u6570\u4f30\u8ba1\u4e2d\u56e0\u80fd\u63d0\u4f9b\u590d\u6742\u5206\u5e03\u7684\u65e0\u5173\u8054\u6837\u672c\u800c\u5907\u53d7\u5173\u6ce8\uff0c\u4f46\u5176\u5728\u591a\u6a21\u6001\u5206\u5e03\u4e2d\u6613\u51fa\u73b0\u6a21\u5f0f\u584c\u9677\u7684\u95ee\u9898\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u57fa\u4e8e\u6709\u6548\u6837\u672c\u5927\u5c0f\uff08ESS\uff09\u7684\u81ea\u9002\u5e94\u9000\u706b\u8c03\u5ea6\uff0c\u7f13\u89e3\u6a21\u5f0f\u584c\u9677\u95ee\u9898\uff0c\u5e76\u8f85\u4ee5\u6837\u672c\u4fee\u526a\u4ee5\u51cf\u5c11\u65b9\u5dee\u3002", "result": "\u5728\u751f\u5316\u632f\u8361\u5668\u6a21\u578b\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6bd4\u5e7f\u6cdb\u4f7f\u7528\u7684\u96c6\u6210\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\uff08MCMC\uff09\u65b9\u6cd5\u5feb\u5341\u500d\u6536\u655b\u8fb9\u9645\u4f3c\u7136\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aNFs\u91c7\u6837\u63d0\u4f9b\u4e86\u901a\u7528\u6539\u8fdb\u65b9\u5411\uff0c\u5e76\u63a2\u8ba8\u4e86\u8fdb\u4e00\u6b65\u4f18\u5316\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2505.03018", "pdf": "https://arxiv.org/pdf/2505.03018", "abs": "https://arxiv.org/abs/2505.03018", "authors": ["Aurora Rofena", "Arianna Manchia", "Claudia Lucia Piccolo", "Bruno Beomonte Zobel", "Paolo Soda", "Valerio Guarrasi"], "title": "Lesion-Aware Generative Artificial Intelligence for Virtual Contrast-Enhanced Mammography in Breast Cancer", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Contrast-Enhanced Spectral Mammography (CESM) is a dual-energy mammographic\ntechnique that improves lesion visibility through the administration of an\niodinated contrast agent. It acquires both a low-energy image, comparable to\nstandard mammography, and a high-energy image, which are then combined to\nproduce a dual-energy subtracted image highlighting lesion contrast\nenhancement. While CESM offers superior diagnostic accuracy compared to\nstandard mammography, its use entails higher radiation exposure and potential\nside effects associated with the contrast medium. To address these limitations,\nwe propose Seg-CycleGAN, a generative deep learning framework for Virtual\nContrast Enhancement in CESM. The model synthesizes high-fidelity dual-energy\nsubtracted images from low-energy images, leveraging lesion segmentation maps\nto guide the generative process and improve lesion reconstruction. Building\nupon the standard CycleGAN architecture, Seg-CycleGAN introduces localized loss\nterms focused on lesion areas, enhancing the synthesis of diagnostically\nrelevant regions. Experiments on the CESM@UCBM dataset demonstrate that\nSeg-CycleGAN outperforms the baseline in terms of PSNR and SSIM, while\nmaintaining competitive MSE and VIF. Qualitative evaluations further confirm\nimproved lesion fidelity in the generated images. These results suggest that\nsegmentation-aware generative models offer a viable pathway toward\ncontrast-free CESM alternatives.", "AI": {"tldr": "Seg-CycleGAN\u662f\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4f4e\u80fd\u91cf\u56fe\u50cf\u751f\u6210\u9ad8\u4fdd\u771f\u7684\u53cc\u80fd\u91cf\u51cf\u5f71\u56fe\u50cf\uff0c\u51cf\u5c11\u8f90\u5c04\u548c\u5bf9\u6bd4\u5242\u526f\u4f5c\u7528\uff0c\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5bf9\u6bd4\u589e\u5f3a\u4e73\u817a\u6444\u5f71\uff08CESM\uff09\u4e2d\u9ad8\u8f90\u5c04\u548c\u5bf9\u6bd4\u5242\u526f\u4f5c\u7528\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u65e0\u5bf9\u6bd4\u5242\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51faSeg-CycleGAN\uff0c\u57fa\u4e8eCycleGAN\u67b6\u6784\uff0c\u5229\u7528\u75c5\u7076\u5206\u5272\u56fe\u6307\u5bfc\u751f\u6210\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u5c40\u90e8\u635f\u5931\u51fd\u6570\u4f18\u5316\u75c5\u7076\u91cd\u5efa\u3002", "result": "\u5728CESM@UCBM\u6570\u636e\u96c6\u4e0a\uff0cSeg-CycleGAN\u5728PSNR\u548cSSIM\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684MSE\u548cVIF\u3002\u5b9a\u6027\u8bc4\u4f30\u663e\u793a\u75c5\u7076\u4fdd\u771f\u5ea6\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u5206\u5272\u7684\u751f\u6210\u6a21\u578b\u4e3a\u65e0\u5bf9\u6bd4\u5242CESM\u63d0\u4f9b\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2505.03677", "pdf": "https://arxiv.org/pdf/2505.03677", "abs": "https://arxiv.org/abs/2505.03677", "authors": ["Emanuele Zappala", "Alice Giola", "Andreas Kramer", "Enrico Greco"], "title": "Neural Integral Operators for Inverse problems in Spectroscopy", "categories": ["cs.LG"], "comment": "13 pages. Codes available upon request", "summary": "Deep learning has shown high performance on spectroscopic inverse problems\nwhen sufficient data is available. However, it is often the case that data in\nspectroscopy is scarce, and this usually causes severe overfitting problems\nwith deep learning methods. Traditional machine learning methods are viable\nwhen datasets are smaller, but the accuracy and applicability of these methods\nis generally more limited.\n  We introduce a deep learning method for classification of molecular spectra\nbased on learning integral operators via integral equations of the first kind,\nwhich results in an algorithm that is less affected by overfitting issues on\nsmall datasets, compared to other deep learning models.\n  The problem formulation of the deep learning approach is based on inverse\nproblems, which have traditionally found important applications in\nspectroscopy. We perform experiments on real world data to showcase our\nalgorithm. It is seen that the model outperforms traditional machine learning\napproaches such as decision tree and support vector machine, and for small\ndatasets it outperforms other deep learning models. Therefore, our methodology\nleverages the power of deep learning, still maintaining the performance when\nthe available data is very limited, which is one of the main issues that deep\nlearning faces in spectroscopy, where datasets are often times of small size.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u79ef\u5206\u65b9\u7a0b\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u5c0f\u6570\u636e\u96c6\u4e0b\u7684\u5206\u5b50\u5149\u8c31\u5206\u7c7b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u5176\u4ed6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "motivation": "\u7531\u4e8e\u5149\u8c31\u6570\u636e\u901a\u5e38\u8f83\u5c11\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u800c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7cbe\u5ea6\u6709\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u8d8a\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7b2c\u4e00\u7c7b\u79ef\u5206\u65b9\u7a0b\u5b66\u4e60\u79ef\u5206\u7b97\u5b50\uff0c\u6784\u5efa\u4e86\u4e00\u79cd\u5bf9\u6297\u8fc7\u62df\u5408\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7ed3\u5408\u5149\u8c31\u9006\u95ee\u9898\u7684\u6570\u5b66\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u51b3\u7b56\u6811\u3001\u652f\u6301\u5411\u91cf\u673a\u548c\u5176\u4ed6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6df1\u5ea6\u5b66\u4e60\u7684\u5f3a\u5927\u6027\u80fd\u3002", "conclusion": "\u8be5\u6a21\u578b\u6210\u529f\u89e3\u51b3\u4e86\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5c0f\u6570\u636e\u96c6\u5149\u8c31\u5206\u6790\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u4f18\u8d8a\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.03712", "pdf": "https://arxiv.org/pdf/2505.03712", "abs": "https://arxiv.org/abs/2505.03712", "authors": ["Deming Sheng", "Ricardo Henao"], "title": "Learning Survival Distributions with the Asymmetric Laplace Distribution", "categories": ["cs.LG", "math.ST", "stat.TH"], "comment": "Accepted to ICML 2025", "summary": "Probabilistic survival analysis models seek to estimate the distribution of\nthe future occurrence (time) of an event given a set of covariates. In recent\nyears, these models have preferred nonparametric specifications that avoid\ndirectly estimating survival distributions via discretization. Specifically,\nthey estimate the probability of an individual event at fixed times or the time\nof an event at fixed probabilities (quantiles), using supervised learning.\nBorrowing ideas from the quantile regression literature, we propose a\nparametric survival analysis method based on the Asymmetric Laplace\nDistribution (ALD). This distribution allows for closed-form calculation of\npopular event summaries such as mean, median, mode, variation, and quantiles.\nThe model is optimized by maximum likelihood to learn, at the individual level,\nthe parameters (location, scale, and asymmetry) of the ALD distribution.\nExtensive results on synthetic and real-world data demonstrate that the\nproposed method outperforms parametric and nonparametric approaches in terms of\naccuracy, discrimination and calibration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u975e\u5bf9\u79f0\u62c9\u666e\u62c9\u65af\u5206\u5e03\uff08ALD\uff09\u7684\u53c2\u6570\u5316\u751f\u5b58\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5b66\u4e60\u4e2a\u4f53\u5c42\u9762\u7684\u5206\u5e03\u53c2\u6570\uff0c\u5728\u51c6\u786e\u6027\u548c\u6821\u51c6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u53c2\u6570\u548c\u975e\u53c2\u6570\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u975e\u53c2\u6570\u751f\u5b58\u5206\u6790\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u79bb\u6563\u5316\u95f4\u63a5\u4f30\u8ba1\u751f\u5b58\u5206\u5e03\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528ALD\u5206\u5e03\u7684\u7279\u6027\u76f4\u63a5\u63d0\u4f9b\u4e8b\u4ef6\u65f6\u95f4\u7684\u5173\u952e\u7edf\u8ba1\u91cf\uff08\u5982\u5747\u503c\u3001\u4e2d\u4f4d\u6570\u3001\u5206\u4f4d\u6570\u7b49\uff09\u3002", "method": "\u91c7\u7528ALD\u5206\u5e03\u8fdb\u884c\u53c2\u6570\u5316\u5efa\u6a21\uff0c\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5b66\u4e60\u4e2a\u4f53\u7684\u4f4d\u7f6e\u3001\u5c3a\u5ea6\u548c\u4e0d\u5bf9\u79f0\u6027\u53c2\u6570\uff0c\u4ece\u800c\u76f4\u63a5\u8ba1\u7b97\u4e8b\u4ef6\u7684\u7edf\u8ba1\u91cf\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u533a\u5206\u5ea6\u548c\u6821\u51c6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u53c2\u6570\u548c\u975e\u53c2\u6570\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8eALD\u7684\u53c2\u6570\u5316\u751f\u5b58\u5206\u6790\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u751f\u5b58\u5206\u6790\u5de5\u5177\uff0c\u5c24\u5176\u5728\u9700\u8981\u76f4\u63a5\u83b7\u53d6\u4e8b\u4ef6\u65f6\u95f4\u7edf\u8ba1\u91cf\u7684\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2505.03721", "pdf": "https://arxiv.org/pdf/2505.03721", "abs": "https://arxiv.org/abs/2505.03721", "authors": ["Dian Chen", "Zelin Wan", "Dong Sam Ha", "Jin-Hee Cho"], "title": "Sustainable Smart Farm Networks: Enhancing Resilience and Efficiency with Decision Theory-Guided Deep Reinforcement Learning", "categories": ["cs.LG", "cs.MA"], "comment": null, "summary": "Solar sensor-based monitoring systems have become a crucial agricultural\ninnovation, advancing farm management and animal welfare through integrating\nsensor technology, Internet-of-Things, and edge and cloud computing. However,\nthe resilience of these systems to cyber-attacks and their adaptability to\ndynamic and constrained energy supplies remain largely unexplored. To address\nthese challenges, we propose a sustainable smart farm network designed to\nmaintain high-quality animal monitoring under various cyber and adversarial\nthreats, as well as fluctuating energy conditions. Our approach utilizes deep\nreinforcement learning (DRL) to devise optimal policies that maximize both\nmonitoring effectiveness and energy efficiency. To overcome DRL's inherent\nchallenge of slow convergence, we integrate transfer learning (TL) and decision\ntheory (DT) to accelerate the learning process. By incorporating DT-guided\nstrategies, we optimize monitoring quality and energy sustainability,\nsignificantly reducing training time while achieving comparable performance\nrewards. Our experimental results prove that DT-guided DRL outperforms\nTL-enhanced DRL models, improving system performance and reducing training\nruntime by 47.5%.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6301\u7eed\u667a\u80fd\u519c\u573a\u7f51\u7edc\uff0c\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u3001\u8fc1\u79fb\u5b66\u4e60\uff08TL\uff09\u548c\u51b3\u7b56\u7406\u8bba\uff08DT\uff09\uff0c\u4ee5\u4f18\u5316\u52a8\u7269\u76d1\u63a7\u6548\u679c\u4e0e\u80fd\u6e90\u6548\u7387\uff0c\u540c\u65f6\u62b5\u5fa1\u7f51\u7edc\u653b\u51fb\u548c\u9002\u5e94\u52a8\u6001\u80fd\u6e90\u4f9b\u5e94\u3002\u5b9e\u9a8c\u8868\u660e\uff0cDT\u5f15\u5bfc\u7684DRL\u6a21\u578b\u6027\u80fd\u4f18\u4e8eTL\u589e\u5f3a\u7684DRL\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1147.5%\u3002", "motivation": "\u5f53\u524d\u592a\u9633\u80fd\u4f20\u611f\u5668\u76d1\u63a7\u7cfb\u7edf\u5728\u7f51\u7edc\u5b89\u5168\u548c\u52a8\u6001\u80fd\u6e90\u4f9b\u5e94\u65b9\u9762\u7684\u9002\u5e94\u6027\u7814\u7a76\u4e0d\u8db3\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u591f\u5e94\u5bf9\u7f51\u7edc\u653b\u51fb\u548c\u80fd\u6e90\u6ce2\u52a8\u7684\u667a\u80fd\u519c\u573a\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u8bbe\u8ba1\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\uff08TL\uff09\u548c\u51b3\u7b56\u7406\u8bba\uff08DT\uff09\u52a0\u901f\u5b66\u4e60\u8fc7\u7a0b\uff0c\u4ee5\u63d0\u5347\u76d1\u63a7\u8d28\u91cf\u4e0e\u80fd\u6e90\u53ef\u6301\u7eed\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cDT\u5f15\u5bfc\u7684DRL\u6a21\u578b\u5728\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\u4e0a\u4f18\u4e8eTL\u589e\u5f3a\u7684DRL\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1147.5%\uff0c\u4e14\u76d1\u63a7\u6548\u679c\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u667a\u80fd\u519c\u573a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6301\u7eed\u7684\u76d1\u63a7\u65b9\u6848\uff0c\u901a\u8fc7DRL\u4e0eDT\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7cfb\u7edf\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2505.03035", "pdf": "https://arxiv.org/pdf/2505.03035", "abs": "https://arxiv.org/abs/2505.03035", "authors": ["Mohammad Mohammadi", "Daniel Honerkamp", "Martin B\u00fcchner", "Matteo Cassinelli", "Tim Welschehold", "Fabien Despinoy", "Igor Gilitschenski", "Abhinav Valada"], "title": "MORE: Mobile Manipulation Rearrangement Through Grounded Language Reasoning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Autonomous long-horizon mobile manipulation encompasses a multitude of\nchallenges, including scene dynamics, unexplored areas, and error recovery.\nRecent works have leveraged foundation models for scene-level robotic reasoning\nand planning. However, the performance of these methods degrades when dealing\nwith a large number of objects and large-scale environments. To address these\nlimitations, we propose MORE, a novel approach for enhancing the capabilities\nof language models to solve zero-shot mobile manipulation planning for\nrearrangement tasks. MORE leverages scene graphs to represent environments,\nincorporates instance differentiation, and introduces an active filtering\nscheme that extracts task-relevant subgraphs of object and region instances.\nThese steps yield a bounded planning problem, effectively mitigating\nhallucinations and improving reliability. Additionally, we introduce several\nenhancements that enable planning across both indoor and outdoor environments.\nWe evaluate MORE on 81 diverse rearrangement tasks from the BEHAVIOR-1K\nbenchmark, where it becomes the first approach to successfully solve a\nsignificant share of the benchmark, outperforming recent foundation model-based\napproaches. Furthermore, we demonstrate the capabilities of our approach in\nseveral complex real-world tasks, mimicking everyday activities. We make the\ncode publicly available at https://more-model.cs.uni-freiburg.de.", "AI": {"tldr": "MORE\u662f\u4e00\u79cd\u65b0\u578b\u65b9\u6cd5\uff0c\u901a\u8fc7\u573a\u666f\u56fe\u8868\u793a\u73af\u5883\u548c\u5b9e\u4f8b\u533a\u5206\uff0c\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u89c4\u5212\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u89e3\u51b3\u7387\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u65b9\u6cd5\u5728\u5927\u91cf\u7269\u4f53\u548c\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u6b64\u63d0\u51faMORE\u6765\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "MORE\u5229\u7528\u573a\u666f\u56fe\u8868\u793a\u73af\u5883\uff0c\u7ed3\u5408\u5b9e\u4f8b\u533a\u5206\u548c\u4e3b\u52a8\u7b5b\u9009\u65b9\u6848\uff0c\u63d0\u53d6\u4efb\u52a1\u76f8\u5173\u7684\u5b50\u56fe\uff0c\u5b9e\u73b0\u6709\u754c\u89c4\u5212\u95ee\u9898\u3002", "result": "\u5728BEHAVIOR-1K\u57fa\u51c6\u6d4b\u8bd5\u768481\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u4e3a\u9996\u4e2a\u663e\u8457\u89e3\u51b3\u8be5\u57fa\u51c6\u7684\u65b9\u6cd5\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u590d\u6742\u73b0\u5b9e\u4efb\u52a1\u3002", "conclusion": "MORE\u901a\u8fc7\u573a\u666f\u56fe\u548c\u5b50\u56fe\u7b5b\u9009\u6709\u6548\u63d0\u5347\u89c4\u5212\u53ef\u9760\u6027\uff0c\u4e3a\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u9896\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02844", "pdf": "https://arxiv.org/pdf/2505.02844", "abs": "https://arxiv.org/abs/2505.02844", "authors": ["Zhikai Wang", "Yanyan Shen", "Zibin Zhang", "Kangyi Lin"], "title": "Feature Staleness Aware Incremental Learning for CTR Prediction", "categories": ["cs.IR", "cs.LG"], "comment": "The code can be found in https://github.com/cloudcatcher888/FeSAIL", "summary": "Click-through Rate (CTR) prediction in real-world recommender systems often\ndeals with billions of user interactions every day. To improve the training\nefficiency, it is common to update the CTR prediction model incrementally using\nthe new incremental data and a subset of historical data. However, the feature\nembeddings of a CTR prediction model often get stale when the corresponding\nfeatures do not appear in current incremental data. In the next period, the\nmodel would have a performance degradation on samples containing stale\nfeatures, which we call the feature staleness problem. To mitigate this\nproblem, we propose a Feature Staleness Aware Incremental Learning method for\nCTR prediction (FeSAIL) which adaptively replays samples containing stale\nfeatures. We first introduce a staleness aware sampling algorithm (SAS) to\nsample a fixed number of stale samples with high sampling efficiency. We then\nintroduce a staleness aware regularization mechanism (SAR) for a fine-grained\ncontrol of the feature embedding updating. We instantiate FeSAIL with a general\ndeep learning-based CTR prediction model and the experimental results\ndemonstrate FeSAIL outperforms various state-of-the-art methods on four\nbenchmark datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFeSAIL\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3CTR\u9884\u6d4b\u4e2d\u7684\u7279\u5f81\u9648\u65e7\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u56de\u653e\u9648\u65e7\u7279\u5f81\u6837\u672c\u548c\u6b63\u5219\u5316\u673a\u5236\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684CTR\u9884\u6d4b\u6bcf\u5929\u5904\u7406\u6570\u5341\u4ebf\u7528\u6237\u4ea4\u4e92\uff0c\u589e\u91cf\u66f4\u65b0\u6a21\u578b\u65f6\uff0c\u672a\u88ab\u65b0\u6570\u636e\u8986\u76d6\u7684\u7279\u5f81\u5d4c\u5165\u4f1a\u9648\u65e7\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "FeSAIL\u5305\u542b\u4e24\u90e8\u5206\uff1a(1) \u9ad8\u6548\u91c7\u6837\u9648\u65e7\u6837\u672c\u7684SAS\u7b97\u6cd5\uff1b(2) \u7cbe\u7ec6\u63a7\u5236\u7279\u5f81\u5d4c\u5165\u66f4\u65b0\u7684SAR\u6b63\u5219\u5316\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eFeSAIL\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FeSAIL\u901a\u8fc7\u52a8\u6001\u5904\u7406\u7279\u5f81\u9648\u65e7\u95ee\u9898\u663e\u8457\u63d0\u5347CTR\u9884\u6d4b\u6a21\u578b\u7684\u589e\u91cf\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2505.03077", "pdf": "https://arxiv.org/pdf/2505.03077", "abs": "https://arxiv.org/abs/2505.03077", "authors": ["Donghun Noh", "Deqian Kong", "Minglu Zhao", "Andrew Lizarraga", "Jianwen Xie", "Ying Nian Wu", "Dennis Hong"], "title": "Latent Adaptive Planner for Dynamic Manipulation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper presents Latent Adaptive Planner (LAP), a novel approach for\ndynamic nonprehensile manipulation tasks that formulates planning as latent\nspace inference, effectively learned from human demonstration videos. Our\nmethod addresses key challenges in visuomotor policy learning through a\nprincipled variational replanning framework that maintains temporal consistency\nwhile efficiently adapting to environmental changes. LAP employs Bayesian\nupdating in latent space to incrementally refine plans as new observations\nbecome available, striking an optimal balance between computational efficiency\nand real-time adaptability. We bridge the embodiment gap between humans and\nrobots through model-based proportional mapping that regenerates accurate\nkinematic-dynamic joint states and object positions from human demonstrations.\nExperimental evaluations across multiple complex manipulation benchmarks\ndemonstrate that LAP achieves state-of-the-art performance, outperforming\nexisting approaches in success rate, trajectory smoothness, and energy\nefficiency, particularly in dynamic adaptation scenarios. Our approach enables\nrobots to perform complex interactions with human-like adaptability while\nproviding an expandable framework applicable to diverse robotic platforms using\nthe same human demonstration videos.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLatent Adaptive Planner (LAP)\uff0c\u4e00\u79cd\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u63a8\u65ad\u4ece\u4eba\u7c7b\u793a\u8303\u89c6\u9891\u5b66\u4e60\u7684\u52a8\u6001\u975e\u6293\u53d6\u64cd\u4f5c\u89c4\u5212\u65b9\u6cd5\uff0c\u7ed3\u5408\u53d8\u5206\u91cd\u89c4\u5212\u548c\u8d1d\u53f6\u65af\u66f4\u65b0\uff0c\u5728\u590d\u6742\u573a\u666f\u4e2d\u5b9e\u73b0\u9ad8\u6548\u9002\u5e94\u4e0e\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u975e\u6293\u53d6\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u89c4\u5212\u95ee\u9898\uff0c\u7279\u522b\u662f\u901a\u8fc7\u4eba\u7c7b\u793a\u8303\u89c6\u9891\u5b66\u4e60\u89c6\u52a8\u7b56\u7565\u7684\u6311\u6218\uff0c\u5305\u62ec\u4fdd\u6301\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u9002\u5e94\u73af\u5883\u53d8\u5316\u3002", "method": "\u91c7\u7528\u6f5c\u5728\u7a7a\u95f4\u63a8\u65ad\u7684\u53d8\u5206\u91cd\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u66f4\u65b0\u9010\u6b65\u4f18\u5316\u8ba1\u5212\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u6a21\u578b\u7684\u6bd4\u4f8b\u6620\u5c04\u586b\u8865\u4eba\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u5177\u8eab\u9e3f\u6c9f\u3002", "result": "\u5728\u591a\u4e2a\u590d\u6742\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLAP\u5728\u6210\u529f\u7387\u3001\u8f68\u8ff9\u5e73\u6ed1\u6027\u548c\u80fd\u6548\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u52a8\u6001\u9002\u5e94\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "LAP\u4e3a\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u7c7b\u4eba\u7c7b\u9002\u5e94\u6027\u7684\u590d\u6742\u4ea4\u4e92\u80fd\u529b\uff0c\u5176\u53ef\u6269\u5c55\u6846\u67b6\u9002\u7528\u4e8e\u591a\u79cd\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u5e76\u57fa\u4e8e\u76f8\u540c\u7684\u4eba\u7c7b\u793a\u8303\u89c6\u9891\u3002"}}
{"id": "2505.03096", "pdf": "https://arxiv.org/pdf/2505.03096", "abs": "https://arxiv.org/abs/2505.03096", "authors": ["Joshua Owotogbe"], "title": "Assessing and Enhancing the Robustness of LLM-based Multi-Agent Systems Through Chaos Engineering", "categories": ["cs.MA", "cs.AI", "cs.SE"], "comment": null, "summary": "This study explores the application of chaos engineering to enhance the\nrobustness of Large Language Model-Based Multi-Agent Systems (LLM-MAS) in\nproduction-like environments under real-world conditions. LLM-MAS can\npotentially improve a wide range of tasks, from answering questions and\ngenerating content to automating customer support and improving decision-making\nprocesses. However, LLM-MAS in production or preproduction environments can be\nvulnerable to emergent errors or disruptions, such as hallucinations, agent\nfailures, and agent communication failures. This study proposes a chaos\nengineering framework to proactively identify such vulnerabilities in LLM-MAS,\nassess and build resilience against them, and ensure reliable performance in\ncritical applications.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u6c8c\u5de5\u7a0b\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u591a\u4ee3\u7406\u7cfb\u7edf\uff08LLM-MAS\uff09\u5728\u771f\u5b9e\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u5e94\u5bf9\u7a81\u53d1\u9519\u8bef\u548c\u4e2d\u65ad\u3002", "motivation": "LLM-MAS\u5728\u4efb\u52a1\u5904\u7406\u4e2d\u5177\u6709\u5e7f\u6cdb\u6f5c\u529b\uff0c\u4f46\u5728\u751f\u4ea7\u6216\u9884\u751f\u4ea7\u73af\u5883\u4e2d\u6613\u53d7\u7a81\u53d1\u9519\u8bef\uff08\u5982\u5e7b\u89c9\u3001\u4ee3\u7406\u5931\u8d25\u548c\u901a\u4fe1\u6545\u969c\uff09\u7684\u5f71\u54cd\uff0c\u9700\u8981\u589e\u5f3a\u5176\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u6c8c\u5de5\u7a0b\u6846\u67b6\uff0c\u7528\u4e8e\u4e3b\u52a8\u8bc6\u522bLLM-MAS\u7684\u8106\u5f31\u70b9\uff0c\u8bc4\u4f30\u5e76\u6784\u5efa\u5176\u6297\u5e72\u6270\u80fd\u529b\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u5e2e\u52a9LLM-MAS\u5728\u5173\u952e\u5e94\u7528\u4e2d\u5b9e\u73b0\u53ef\u9760\u6027\u80fd\u3002", "conclusion": "\u6df7\u6c8c\u5de5\u7a0b\u662f\u786e\u4fddLLM-MAS\u5728\u771f\u5b9e\u73af\u5883\u4e0b\u7a33\u5065\u8fd0\u884c\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2505.03105", "pdf": "https://arxiv.org/pdf/2505.03105", "abs": "https://arxiv.org/abs/2505.03105", "authors": ["Xule Lin"], "title": "Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI Knowledge Co-Creation", "categories": ["cs.HC", "cs.AI", "cs.CY", "H.5.3; I.2.11; K.4.3; H.1.2; I.2.4"], "comment": "62 pages (31 appendix pages for guidance), 2 figures", "summary": "Scientific knowledge creation is fundamentally transforming as humans and AI\nsystems evolve beyond tool-user relationships into co-evolutionary epistemic\npartnerships. When AlphaFold revolutionized protein structure prediction,\nresearchers described engaging with an epistemic partner that reshaped how they\nconceptualized fundamental relationships. This article introduces Cognitio\nEmergens (CE), a framework addressing critical limitations in existing models\nthat focus on static roles or narrow metrics while failing to capture how\nscientific understanding emerges through recursive human-AI interaction over\ntime. CE integrates three components addressing these limitations: Agency\nConfigurations describing how authority distributes between humans and AI\n(Directed, Contributory, Partnership), with partnerships dynamically\noscillating between configurations rather than following linear progression;\nEpistemic Dimensions capturing six specific capabilities emerging through\ncollaboration across Discovery, Integration, and Projection axes, creating\ndistinctive \"capability signatures\" that guide development; and Partnership\nDynamics identifying forces shaping how these relationships evolve,\nparticularly the risk of epistemic alienation where researchers lose\ninterpretive control over knowledge they formally endorse. Drawing from\nautopoiesis theory, social systems theory, and organizational modularity, CE\nreveals how knowledge co-creation emerges through continuous negotiation of\nroles, values, and organizational structures. By reconceptualizing human-AI\nscientific collaboration as fundamentally co-evolutionary, CE offers a balanced\nperspective that neither uncritically celebrates nor unnecessarily fears AI's\nevolving role, instead providing conceptual tools for cultivating partnerships\nthat maintain meaningful human participation while enabling transformative\nscientific breakthroughs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCognitio Emergens (CE)\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7684\u4eba\u7c7b\u4e0eAI\u5408\u4f5c\u6a21\u578b\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\uff0c\u5f3a\u8c03\u79d1\u5b66\u7406\u89e3\u901a\u8fc7\u9012\u5f52\u4e92\u52a8\u6d8c\u73b0\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u8fc7\u4e8e\u9759\u6001\u6216\u4fa7\u91cd\u72ed\u7a84\u6307\u6807\uff0c\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u4e0eAI\u5408\u4f5c\u4e2d\u79d1\u5b66\u7406\u89e3\u7684\u52a8\u6001\u6d8c\u73b0\u8fc7\u7a0b\u3002", "method": "CE\u6574\u5408\u4e86\u673a\u6784\u914d\u7f6e\uff08\u52a8\u6001\u5206\u914d\u4eba\u7c7b\u4e0eAI\u7684\u6743\u5a01\uff09\u3001\u8ba4\u77e5\u7ef4\u5ea6\uff08\u516d\u79cd\u5408\u4f5c\u80fd\u529b\uff09\u548c\u5408\u4f5c\u52a8\u6001\uff08\u5173\u7cfb\u6f14\u5316\u7684\u9a71\u52a8\u529b\uff09\u3002", "result": "CE\u63ed\u793a\u4e86\u77e5\u8bc6\u5171\u540c\u521b\u9020\u901a\u8fc7\u89d2\u8272\u3001\u4ef7\u503c\u548c\u7ed3\u6784\u7684\u6301\u7eed\u8c08\u5224\u6d8c\u73b0\uff0c\u4e3a\u5e73\u8861\u4eba\u7c7b\u53c2\u4e0e\u548cAI\u7a81\u7834\u63d0\u4f9b\u5de5\u5177\u3002", "conclusion": "CE\u91cd\u6784\u4e86\u4eba\u7c7b\u4e0eAI\u7684\u79d1\u5b66\u5408\u4f5c\uff0c\u65e2\u4e0d\u76f2\u76ee\u4e50\u89c2\u4e5f\u4e0d\u8fc7\u5ea6\u6050\u60e7AI\u89d2\u8272\uff0c\u4e3a\u57f9\u80b2\u6709\u610f\u4e49\u7684\u4eba\u7c7b\u53c2\u4e0e\u7684\u4f19\u4f34\u5173\u7cfb\u63d0\u4f9b\u6982\u5ff5\u5de5\u5177\u3002"}}
{"id": "2505.02963", "pdf": "https://arxiv.org/pdf/2505.02963", "abs": "https://arxiv.org/abs/2505.02963", "authors": ["Rohan Ghuge", "Sahil Singla", "Yifan Wang"], "title": "Single-Sample and Robust Online Resource Allocation", "categories": ["cs.DS", "cs.GT", "cs.LG"], "comment": "Full version of STOC 2025 paper", "summary": "Online Resource Allocation problem is a central problem in many areas of\nComputer Science, Operations Research, and Economics. In this problem, we\nsequentially receive $n$ stochastic requests for $m$ kinds of shared resources,\nwhere each request can be satisfied in multiple ways, consuming different\namounts of resources and generating different values. The goal is to achieve a\n$(1-\\epsilon)$-approximation to the hindsight optimum, where $\\epsilon>0$ is a\nsmall constant, assuming each resource has a large budget.\n  In this paper, we investigate the learnability and robustness of online\nresource allocation. Our primary contribution is a novel Exponential Pricing\nalgorithm with the following properties: 1. It requires only a \\emph{single\nsample} from each of the $n$ request distributions to achieve a\n$(1-\\epsilon)$-approximation for online resource allocation with large budgets.\nSuch an algorithm was previously unknown, even with access to polynomially many\nsamples, as prior work either assumed full distributional knowledge or was\nlimited to i.i.d.\\,or random-order arrivals. 2. It is robust to corruptions in\nthe outliers model and the value augmentation model. Specifically, it maintains\nits $(1 - \\epsilon)$-approximation guarantee under both these robustness\nmodels, resolving the open question posed in Argue, Gupta, Molinaro, and Singla\n(SODA'22). 3. It operates as a simple item-pricing algorithm that ensures\nincentive compatibility.\n  The intuition behind our Exponential Pricing algorithm is that the price of a\nresource should adjust exponentially as it is overused or underused. It differs\nfrom conventional approaches that use an online learning algorithm for item\npricing. This departure guarantees that the algorithm will never run out of any\nresource, but loses the usual no-regret properties of online learning\nalgorithms, necessitating a new analytical approach.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5728\u7ebf\u8d44\u6e90\u5206\u914d\u95ee\u9898\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6307\u6570\u5b9a\u4ef7\u7b97\u6cd5\uff0c\u4ec5\u9700\u5355\u4e00\u6837\u672c\u5373\u5b9e\u73b0\u8fd1\u4f3c\u6700\u4f18\u89e3\uff0c\u5e76\u5177\u5907\u6297\u5e72\u6270\u548c\u6fc0\u52b1\u517c\u5bb9\u7279\u6027\u3002", "motivation": "\u7814\u7a76\u5728\u7ebf\u8d44\u6e90\u5206\u914d\u95ee\u9898\u7684\u53ef\u5b66\u4e60\u6027\u548c\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5bf9\u5b8c\u6574\u5206\u5e03\u4fe1\u606f\u6216\u7279\u5b9a\u5230\u8fbe\u6a21\u5f0f\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u589e\u5f3a\u7b97\u6cd5\u7684\u6297\u5e72\u6270\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u6307\u6570\u5b9a\u4ef7\u7b97\u6cd5\uff0c\u901a\u8fc7\u8d44\u6e90\u4ef7\u683c\u7684\u52a8\u6001\u8c03\u6574\uff08\u6307\u6570\u53d8\u5316\uff09\u5b9e\u73b0\u8d44\u6e90\u5206\u914d\u7684\u4f18\u5316\uff0c\u907f\u514d\u8d44\u6e90\u8017\u5c3d\u95ee\u9898\u3002", "result": "\u7b97\u6cd5\u5728\u5355\u6837\u672c\u6761\u4ef6\u4e0b\u5b9e\u73b0$(1-\\epsilon)$-\u8fd1\u4f3c\u6700\u4f18\uff0c\u5e76\u5728\u5e72\u6270\u6a21\u578b\u4e0b\u4fdd\u6301\u6027\u80fd\uff0c\u540c\u65f6\u786e\u4fdd\u6fc0\u52b1\u517c\u5bb9\u6027\u3002", "conclusion": "\u6307\u6570\u5b9a\u4ef7\u7b97\u6cd5\u4e3a\u5728\u7ebf\u8d44\u6e90\u5206\u914d\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9884\u7b97\u5145\u8db3\u4e14\u6837\u672c\u6709\u9650\u7684\u573a\u666f\u3002"}}
{"id": "2505.03132", "pdf": "https://arxiv.org/pdf/2505.03132", "abs": "https://arxiv.org/abs/2505.03132", "authors": ["Xinyuan Yan", "Xiwei Xuan", "Jorge Piazentin Ono", "Jiajing Guo", "Vikram Mohanty", "Shekar Arvind Kumar", "Liang Gou", "Bei Wang", "Liu Ren"], "title": "VISLIX: An XAI Framework for Validating Vision Models with Slice Discovery and Analysis", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "Real-world machine learning models require rigorous evaluation before\ndeployment, especially in safety-critical domains like autonomous driving and\nsurveillance. The evaluation of machine learning models often focuses on data\nslices, which are subsets of the data that share a set of characteristics. Data\nslice finding automatically identifies conditions or data subgroups where\nmodels underperform, aiding developers in mitigating performance issues.\nDespite its popularity and effectiveness, data slicing for vision model\nvalidation faces several challenges. First, data slicing often needs additional\nimage metadata or visual concepts, and falls short in certain computer vision\ntasks, such as object detection. Second, understanding data slices is a\nlabor-intensive and mentally demanding process that heavily relies on the\nexpert's domain knowledge. Third, data slicing lacks a human-in-the-loop\nsolution that allows experts to form hypothesis and test them interactively. To\novercome these limitations and better support the machine learning operations\nlifecycle, we introduce VISLIX, a novel visual analytics framework that employs\nstate-of-the-art foundation models to help domain experts analyze slices in\ncomputer vision models. Our approach does not require image metadata or visual\nconcepts, automatically generates natural language insights, and allows users\nto test data slice hypothesis interactively. We evaluate VISLIX with an expert\nstudy and three use cases, that demonstrate the effectiveness of our tool in\nproviding comprehensive insights for validating object detection models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aVISLIX\u7684\u89c6\u89c9\u5206\u6790\u6846\u67b6\uff0c\u5229\u7528\u5148\u8fdb\u7684\u57fa\u7840\u6a21\u578b\u5e2e\u52a9\u4e13\u5bb6\u5206\u6790\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u4e2d\u7684\u6570\u636e\u5207\u7247\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6570\u636e\u5207\u7247\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\u548c\u76d1\u63a7\uff09\u9700\u8981\u4e25\u683c\u9a8c\u8bc1\uff0c\u4f46\u4f20\u7edf\u7684\u6570\u636e\u5207\u7247\u65b9\u6cd5\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u5b58\u5728\u8bf8\u591a\u6311\u6218\uff0c\u5982\u4f9d\u8d56\u989d\u5916\u5143\u6570\u636e\u3001\u4eba\u5de5\u9700\u6c42\u9ad8\u4e14\u7f3a\u4e4f\u4ea4\u4e92\u6027\u3002", "method": "\u5f15\u5165VISLIX\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u81ea\u52a8\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89c1\u89e3\uff0c\u5e76\u652f\u6301\u7528\u6237\u4ea4\u4e92\u5f0f\u6d4b\u8bd5\u6570\u636e\u5207\u7247\u5047\u8bbe\uff0c\u65e0\u9700\u56fe\u50cf\u5143\u6570\u636e\u6216\u89c6\u89c9\u6982\u5ff5\u3002", "result": "\u901a\u8fc7\u4e13\u5bb6\u7814\u7a76\u548c\u4e09\u4e2a\u7528\u4f8b\u8bc1\u660e\u4e86VISLIX\u5728\u9a8c\u8bc1\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u65f6\u7684\u6709\u6548\u6027\uff0c\u80fd\u63d0\u4f9b\u5168\u9762\u89c1\u89e3\u3002", "conclusion": "VISLIX\u514b\u670d\u4e86\u4f20\u7edf\u6570\u636e\u5207\u7247\u7684\u9650\u5236\uff0c\u63d0\u5347\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u9a8c\u8bc1\u7684\u6548\u7387\u548c\u53ef\u64cd\u4f5c\u6027\u3002"}}
{"id": "2505.02972", "pdf": "https://arxiv.org/pdf/2505.02972", "abs": "https://arxiv.org/abs/2505.02972", "authors": ["Aoran Chen", "Yang Feng"], "title": "GeoERM: Geometry-Aware Multi-Task Representation Learning on Riemannian Manifolds", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "Multi-Task Learning (MTL) seeks to boost statistical power and learning\nefficiency by discovering structure shared across related tasks.\nState-of-the-art MTL representation methods, however, usually treat the latent\nrepresentation matrix as a point in ordinary Euclidean space, ignoring its\noften non-Euclidean geometry, thus sacrificing robustness when tasks are\nheterogeneous or even adversarial. We propose GeoERM, a geometry-aware MTL\nframework that embeds the shared representation on its natural Riemannian\nmanifold and optimizes it via explicit manifold operations. Each training cycle\nperforms (i) a Riemannian gradient step that respects the intrinsic curvature\nof the search space, followed by (ii) an efficient polar retraction to remain\non the manifold, guaranteeing geometric fidelity at every iteration. The\nprocedure applies to a broad class of matrix-factorized MTL models and retains\nthe same per-iteration cost as Euclidean baselines. Across a set of synthetic\nexperiments with task heterogeneity and on a wearable-sensor\nactivity-recognition benchmark, GeoERM consistently improves estimation\naccuracy, reduces negative transfer, and remains stable under adversarial label\nnoise, outperforming leading MTL and single-task alternatives.", "AI": {"tldr": "GeoERM\u662f\u4e00\u79cd\u51e0\u4f55\u611f\u77e5\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u9ece\u66fc\u6d41\u5f62\u4e0a\u4f18\u5316\u5171\u4eab\u8868\u793a\uff0c\u63d0\u5347\u4e86\u4efb\u52a1\u5f02\u8d28\u6027\u6216\u5bf9\u6297\u6027\u73af\u5883\u4e0b\u7684\u7a33\u5065\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u5728\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u5904\u7406\u6f5c\u5728\u8868\u793a\u77e9\u9635\uff0c\u5ffd\u7565\u4e86\u5176\u975e\u6b27\u51e0\u4f55\u7279\u6027\uff0c\u5bfc\u81f4\u5728\u4efb\u52a1\u5f02\u8d28\u6216\u5bf9\u6297\u65f6\u6027\u80fd\u4e0b\u964d\u3002GeoERM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "GeoERM\u5c06\u5171\u4eab\u8868\u793a\u5d4c\u5165\u81ea\u7136\u7684\u9ece\u66fc\u6d41\u5f62\uff0c\u901a\u8fc7\u9ece\u66fc\u68af\u5ea6\u6b65\u548c\u6781\u5750\u6807\u56de\u7f29\u64cd\u4f5c\u4f18\u5316\uff0c\u786e\u4fdd\u51e0\u4f55\u4fdd\u771f\u5ea6\u3002", "result": "\u5728\u5408\u6210\u5b9e\u9a8c\u548c wearable-sensor \u6d3b\u52a8\u8bc6\u522b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGeoERM\u663e\u8457\u63d0\u9ad8\u4e86\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u51cf\u5c11\u4e86\u8d1f\u9762\u8fc1\u79fb\uff0c\u5e76\u5728\u5bf9\u6297\u6027\u6807\u7b7e\u566a\u58f0\u4e0b\u4fdd\u6301\u7a33\u5b9a\u3002", "conclusion": "GeoERM\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u4f18\u5316\uff0c\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u5c55\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u7a33\u5065\u6027\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.03149", "pdf": "https://arxiv.org/pdf/2505.03149", "abs": "https://arxiv.org/abs/2505.03149", "authors": ["Joseph William Kettelkamp", "Ludovica Romanin", "Sarv Priya", "Mathews Jacob"], "title": "Motion-compensated cardiac MRI using low-rank diffeomorphic flow (DMoCo)", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce an unsupervised motion-compensated image reconstruction\nalgorithm for free-breathing and ungated 3D cardiac magnetic resonance imaging\n(MRI). We express the image volume corresponding to each specific motion phase\nas the deformation of a single static image template. The main contribution of\nthe work is the low-rank model for the compact joint representation of the\nfamily of diffeomorphisms, parameterized by the motion phases. The\ndiffeomorphism at a specific motion phase is obtained by integrating a\nparametric velocity field along a path connecting the reference template phase\nto the motion phase. The velocity field at different phases is represented\nusing a low-rank model. The static template and the low-rank motion model\nparameters are learned directly from the k-space data in an unsupervised\nfashion. The more constrained motion model is observed to offer improved\nrecovery compared to current motion-resolved and motion-compensated algorithms\nfor free-breathing 3D cine MRI.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u76d1\u7763\u8fd0\u52a8\u8865\u507f\u56fe\u50cf\u91cd\u5efa\u7b97\u6cd5\u7528\u4e8e\u81ea\u7531\u547c\u5438\u548c\u95e8\u63a73D\u5fc3\u810fMRI\uff0c\u901a\u8fc7\u4f4e\u79e9\u6a21\u578b\u7d27\u51d1\u8868\u793a\u8fd0\u52a8\u53c2\u6570\u4ee5\u63d0\u5347\u91cd\u5efa\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u8fd0\u52a8\u5206\u8fa8\u548c\u8fd0\u52a8\u8865\u507f\u7b97\u6cd5\u5728\u81ea\u7531\u547c\u54383D\u5fc3\u810fMRI\u4e2d\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u66f4\u7d27\u51d1\u7684\u8fd0\u52a8\u8868\u793a\u65b9\u6cd5\u6765\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "method": "\u5229\u7528\u4f4e\u79e9\u6a21\u578b\u8868\u793a\u8fd0\u52a8\u76f8\u4f4d\u53c2\u6570\u5316\u540e\u7684\u901f\u5ea6\u573a\uff0c\u901a\u8fc7\u79ef\u5206\u8def\u5f84\u83b7\u5f97\u53d8\u5f62\u573a\uff0c\u76f4\u63a5\u4ecek\u7a7a\u95f4\u6570\u636e\u65e0\u76d1\u7763\u5b66\u4e60\u9759\u6001\u6a21\u677f\u4e0e\u8fd0\u52a8\u53c2\u6570\u3002", "result": "\u76f8\u8f83\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u51fa\u7684\u4f4e\u79e9\u8fd0\u52a8\u6a21\u578b\u5728\u81ea\u7531\u547c\u54383D\u5fc3\u810fMRI\u4e2d\u91cd\u5efa\u6548\u679c\u66f4\u4f18\u3002", "conclusion": "\u4f4e\u79e9\u8fd0\u52a8\u6a21\u578b\u4e3a\u65e0\u76d1\u7763\u5fc3\u810fMRI\u91cd\u5efa\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u8fd0\u52a8\u8865\u507f\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002"}}
{"id": "2505.02979", "pdf": "https://arxiv.org/pdf/2505.02979", "abs": "https://arxiv.org/abs/2505.02979", "authors": ["Ruiyue Huang", "Claire E. Heaney", "Maarten van Reeuwijk"], "title": "Parameter estimation for land-surface models using machine learning libraries", "categories": ["physics.ao-ph", "cs.LG"], "comment": "9 pages, 5 figures, 3 tables", "summary": "The Neural Networks for Partial Differential Equations (NN4PDEs) approach is\nused to determine the parameters of a simple land-surface model using PyTorch's\nbackpropagation engine. In order to test the inverse model, a synthetic dataset\nis created by running the model in forward mode with known parameter values to\ncreate soil temperature time series that can be used as observations for the\ninverse model. We show that it is not possible to obtain a reliable parameter\nestimation using a single observed soil temperature time series. Using\nmeasurements at two depths, reliable parameter estimates can be obtained\nalthough it is not possible to differentiate between latent and sensible heat\nfluxes. We apply the inverse model to urban flux tower data in Phoenix, United\nStates, and show that the thermal conductivity, volumetric heat capacity, and\nthe combined sensible-latent heat transfer coefficient can be reliably\nestimated using an observed value for the effective surface albedo. The\nresulting model accurately predicts the outgoing longwave radiation, conductive\nsoil fluxes and the combined sensible-latent heat fluxes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528NN4PDEs\u65b9\u6cd5\u548cPyTorch\u7684\u53cd\u5411\u4f20\u64ad\u5f15\u64ce\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u6d4b\u8bd5\u5730\u8868\u6a21\u578b\u7684\u53c2\u6570\u53cd\u6f14\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5355\u5c42\u571f\u58e4\u6e29\u5ea6\u6570\u636e\u65e0\u6cd5\u53ef\u9760\u4f30\u8ba1\u53c2\u6570\uff0c\u800c\u53cc\u5c42\u6570\u636e\u867d\u80fd\u4f30\u53c2\u4f46\u65e0\u6cd5\u533a\u5206\u611f\u70ed\u548c\u6f5c\u70ed\u901a\u91cf\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u7ed3\u5408\u6709\u6548\u53cd\u7167\u7387\u6570\u636e\u53ef\u53ef\u9760\u4f30\u7b97\u70ed\u5bfc\u7387\u3001\u70ed\u5bb9\u53ca\u611f\u70ed-\u6f5c\u70ed\u8054\u5408\u7cfb\u6570\uff0c\u6a21\u578b\u80fd\u51c6\u786e\u9884\u6d4b\u8f90\u5c04\u548c\u70ed\u901a\u91cf\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u548c\u53cd\u5411\u4f20\u64ad\u6280\u672f\u89e3\u51b3\u5730\u8868\u6a21\u578b\u53c2\u6570\u53cd\u6f14\u95ee\u9898\uff0c\u5c24\u5176\u662f\u56e0\u89c2\u6d4b\u6570\u636e\u6709\u9650\u5bfc\u81f4\u53c2\u6570\u4f30\u8ba1\u4e0d\u51c6\u786e\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528PyTorch\u53cd\u5411\u4f20\u64ad\u5f15\u64ce\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\uff08\u5df2\u77e5\u53c2\u6570\u6a21\u62df\u7684\u571f\u58e4\u6e29\u5ea6\u65f6\u95f4\u5e8f\u5217\uff09\u9a8c\u8bc1NN4PDEs\u65b9\u6cd5\u7684\u53cd\u6f14\u80fd\u529b\u3002", "result": "\u5355\u5c42\u571f\u58e4\u6e29\u5ea6\u6570\u636e\u65e0\u6cd5\u53ef\u9760\u4f30\u53c2\uff1b\u53cc\u5c42\u6570\u636e\u53ef\u4f30\u53c2\u4f46\u65e0\u6cd5\u533a\u5206\u611f\u70ed\u4e0e\u6f5c\u70ed\u901a\u91cf\u3002\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u7ed3\u5408\u6709\u6548\u53cd\u7167\u7387\u53ef\u51c6\u786e\u4f30\u7b97\u70ed\u5bfc\u7387\u3001\u70ed\u5bb9\u53ca\u611f\u70ed-\u6f5c\u70ed\u8054\u5408\u7cfb\u6570\uff0c\u6a21\u578b\u9884\u6d4b\u8f90\u5c04\u548c\u70ed\u901a\u91cf\u6548\u679c\u826f\u597d\u3002", "conclusion": "NN4PDEs\u65b9\u6cd5\u5728\u7ed3\u5408\u591a\u5c42\u89c2\u6d4b\u6570\u636e\u65f6\u80fd\u6709\u6548\u53cd\u6f14\u5730\u8868\u6a21\u578b\u53c2\u6570\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u89e3\u51b3\u611f\u70ed\u4e0e\u6f5c\u70ed\u901a\u91cf\u7684\u533a\u5206\u95ee\u9898\u3002\u5b9e\u9645\u5e94\u7528\u4e2d\u8054\u5408\u53cd\u7167\u7387\u6570\u636e\u53ef\u63d0\u5347\u6a21\u578b\u9884\u6d4b\u7cbe\u5ea6\u3002"}}
{"id": "2505.03154", "pdf": "https://arxiv.org/pdf/2505.03154", "abs": "https://arxiv.org/abs/2505.03154", "authors": ["Yuxuan Mu", "Hung Yu Ling", "Yi Shi", "Ismael Baira Ojeda", "Pengcheng Xi", "Chang Shu", "Fabio Zinno", "Xue Bin Peng"], "title": "StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "17 pages, 13 figures", "summary": "Motion capture (mocap) data often exhibits visually jarring artifacts due to\ninaccurate sensors and post-processing. Cleaning this corrupted data can\nrequire substantial manual effort from human experts, which can be a costly and\ntime-consuming process. Previous data-driven motion cleanup methods offer the\npromise of automating this cleanup process, but often require in-domain paired\ncorrupted-to-clean training data. Constructing such paired datasets requires\naccess to high-quality, relatively artifact-free motion clips, which often\nnecessitates laborious manual cleanup. In this work, we present StableMotion, a\nsimple yet effective method for training motion cleanup models directly from\nunpaired corrupted datasets that need cleanup. The core component of our method\nis the introduction of motion quality indicators, which can be easily annotated\nthrough manual labeling or heuristic algorithms and enable training of\nquality-aware motion generation models on raw motion data with mixed quality.\nAt test time, the model can be prompted to generate high-quality motions using\nthe quality indicators. Our method can be implemented through a simple\ndiffusion-based framework, leading to a unified motion generate-discriminate\nmodel, which can be used to both identify and fix corrupted frames. We\ndemonstrate that our proposed method is effective for training motion cleanup\nmodels on raw mocap data in production scenarios by applying StableMotion to\nSoccerMocap, a 245-hour soccer mocap dataset containing real-world motion\nartifacts. The trained model effectively corrects a wide range of motion\nartifacts, reducing motion pops and frozen frames by 68% and 81%, respectively.\nSee https://youtu.be/3Y7MMAH02B4 for more results.", "AI": {"tldr": "StableMotion\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u4ece\u975e\u914d\u5bf9\u7684\u635f\u574f\u52a8\u4f5c\u6570\u636e\u4e2d\u8bad\u7ec3\u52a8\u4f5c\u6e05\u7406\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u52a8\u4f5c\u8d28\u91cf\u6307\u793a\u5668\u5b9e\u73b0\u9ad8\u8d28\u91cf\u52a8\u4f5c\u751f\u6210\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u6e05\u7406\u7684\u8d1f\u62c5\u3002", "motivation": "\u52a8\u4f5c\u6355\u6349\u6570\u636e\u5e38\u56e0\u4f20\u611f\u5668\u4e0d\u51c6\u786e\u6216\u540e\u5904\u7406\u5bfc\u81f4\u89c6\u89c9\u4e0a\u7684\u4e0d\u81ea\u7136\uff0c\u4eba\u5de5\u6e05\u7406\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u3002\u73b0\u6709\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u9700\u914d\u5bf9\u6570\u636e\uff0c\u800c\u9ad8\u8d28\u91cf\u6570\u636e\u83b7\u53d6\u56f0\u96be\u3002", "method": "\u63d0\u51faStableMotion\uff0c\u5229\u7528\u53ef\u624b\u52a8\u6216\u542f\u53d1\u5f0f\u6807\u6ce8\u7684\u52a8\u4f5c\u8d28\u91cf\u6307\u793a\u5668\uff0c\u5728\u6269\u6563\u6a21\u578b\u6846\u67b6\u4e0b\u8bad\u7ec3\u8d28\u91cf\u611f\u77e5\u7684\u52a8\u4f5c\u751f\u6210\u6a21\u578b\uff0c\u7edf\u4e00\u751f\u6210\u4e0e\u5224\u522b\u529f\u80fd\u3002", "result": "\u5728SoccerMocap\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u6a21\u578b\u6709\u6548\u4fee\u590d\u591a\u79cd\u52a8\u4f5c\u7455\u75b5\uff0c\u52a8\u4f5c\u5361\u987f\u548c\u51bb\u7ed3\u5e27\u5206\u522b\u51cf\u5c1168%\u548c81%\u3002", "conclusion": "StableMotion\u4e3a\u52a8\u4f5c\u6570\u636e\u6e05\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u65e0\u9700\u914d\u5bf9\u6570\u636e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u5316\u6e05\u7406\u7684\u6548\u679c\u3002"}}
{"id": "2505.02987", "pdf": "https://arxiv.org/pdf/2505.02987", "abs": "https://arxiv.org/abs/2505.02987", "authors": ["Yifan Chen"], "title": "New affine invariant ensemble samplers and their dimensional scaling", "categories": ["stat.CO", "cs.LG", "math.ST", "stat.ML", "stat.TH"], "comment": "Any feedback welcome!", "summary": "We introduce some new affine invariant ensemble samplers that are easy to\nconstruct and improve upon existing widely used algorithms, especially for\nhigh-dimensional problems. Specifically, we propose a derivative-free ensemble\nside move sampler that performs favorably compared to popular samplers in the\n\\texttt{emcee} package. Additionally, we develop a class of derivative-based\nensemble Hamiltonian Monte Carlo (HMC) samplers with affine invariance, which\noutperform standard HMC without affine invariance when sampling highly skewed\ndistributions. We provide asymptotic scaling analysis for high-dimensional\nGaussian targets to further elucidate the properties of these affine invariant\nensemble samplers. In particular, with derivative information, the affine\ninvariant ensemble HMC can scale much better with dimension compared to\nderivative-free ensemble samplers.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4eff\u5c04\u4e0d\u53d8\u96c6\u6210\u91c7\u6837\u5668\uff0c\u6539\u8fdb\u4e86\u73b0\u6709\u7b97\u6cd5\uff0c\u5c24\u5176\u5728\u5904\u7406\u9ad8\u7ef4\u95ee\u9898\u65f6\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u91c7\u6837\u5668\uff08\u5982emcee\u5305\u4e2d\u7684\u7b97\u6cd5\uff09\u5728\u9ad8\u7ef4\u6216\u504f\u6001\u5206\u5e03\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u91c7\u6837\u65b9\u6cd5\u3002", "method": "1. \u63d0\u51fa\u65e0\u9700\u5bfc\u6570\u7684\u96c6\u6210\u4fa7\u79fb\u91c7\u6837\u5668\uff1b2. \u5f00\u53d1\u57fa\u4e8e\u5bfc\u6570\u7684\u4eff\u5c04\u4e0d\u53d8\u96c6\u6210HMC\u91c7\u6837\u5668\uff0c\u9002\u7528\u4e8e\u504f\u6001\u5206\u5e03\u3002", "result": "\u4eff\u5c04\u4e0d\u53d8\u96c6\u6210HMC\u91c7\u6837\u5668\u5728\u9ad8\u65af\u76ee\u6807\u5206\u5e03\u4e2d\u5c55\u73b0\u51fa\u4f18\u4e8e\u6807\u51c6HMC\u7684\u7ef4\u5ea6\u6269\u5c55\u6027\uff0c\u5c24\u5176\u5728\u5229\u7528\u5bfc\u6570\u4fe1\u606f\u65f6\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u65b0\u91c7\u6837\u5668\u901a\u8fc7\u4eff\u5c04\u4e0d\u53d8\u6027\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u7ef4\u548c\u504f\u6001\u5206\u5e03\u7684\u91c7\u6837\u6548\u7387\uff0c\u4e3a\u590d\u6742\u6982\u7387\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u4f18\u5de5\u5177\u3002"}}
{"id": "2505.03156", "pdf": "https://arxiv.org/pdf/2505.03156", "abs": "https://arxiv.org/abs/2505.03156", "authors": ["Claudio Mayrink Verdun", "Alex Oesterling", "Himabindu Lakkaraju", "Flavio P. Calmon"], "title": "Soft Best-of-n Sampling for Model Alignment", "categories": ["cs.IT", "cs.AI", "math.IT"], "comment": "Accepted for presentation at the 2025 IEEE International Symposium on\n  Information Theory (ISIT 2025)", "summary": "Best-of-$n$ (BoN) sampling is a practical approach for aligning language\nmodel outputs with human preferences without expensive fine-tuning. BoN\nsampling is performed by generating $n$ responses to a prompt and then\nselecting the sample that maximizes a reward function. BoN yields high reward\nvalues in practice at a distortion cost, as measured by the KL-divergence\nbetween the sampled and original distribution. This distortion is coarsely\ncontrolled by varying the number of samples: larger $n$ yields a higher reward\nat a higher distortion cost. We introduce Soft Best-of-$n$ sampling, a\ngeneralization of BoN that allows for smooth interpolation between the original\ndistribution and reward-maximizing distribution through a temperature parameter\n$\\lambda$. We establish theoretical guarantees showing that Soft Best-of-$n$\nsampling converges sharply to the optimal tilted distribution at a rate of\n$O(1/n)$ in KL and the expected (relative) reward. For sequences of discrete\noutputs, we analyze an additive reward model that reveals the fundamental\nlimitations of blockwise sampling.", "AI": {"tldr": "Soft Best-of-$n$\u91c7\u6837\u662f\u4e00\u79cd\u901a\u8fc7\u6e29\u5ea6\u53c2\u6570\u5e73\u6ed1\u63a7\u5236\u539f\u59cb\u5206\u5e03\u4e0e\u5956\u52b1\u6700\u5927\u5316\u5206\u5e03\u4e4b\u95f4\u8fc7\u6e21\u7684\u65b9\u6cd5\uff0c\u76f8\u6bd4BoN\u91c7\u6837\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "BoN\u91c7\u6837\u867d\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5bf9\u9f50\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\uff0c\u4f46\u5176\u901a\u8fc7\u6837\u672c\u6570\u91cf\u7c97\u7cd9\u63a7\u5236KL\u6563\u5ea6\uff0c\u5bfc\u81f4\u6548\u7387\u4e0d\u9ad8\u4e14\u7406\u8bba\u652f\u6301\u4e0d\u8db3\u3002", "method": "\u5f15\u5165\u6e29\u5ea6\u53c2\u6570$\u03bb$\uff0c\u63d0\u51faSoft Best-of-$n$\u91c7\u6837\u65b9\u6cd5\uff0c\u7406\u8bba\u4e0a\u8bc1\u660e\u5176\u4ee5$O(1/n)$\u7684KL\u548c\u671f\u671b\u5956\u52b1\u901f\u7387\u6536\u655b\u81f3\u6700\u4f18\u503e\u659c\u5206\u5e03\u3002", "result": "Soft Best-of-$n$\u5728\u79bb\u6563\u5e8f\u5217\u7684\u52a0\u6027\u5956\u52b1\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u6bd4BoN\u66f4\u4f4e\u7684\u91c7\u6837\u504f\u5dee\uff08$O(1/n)$\uff09\uff0c\u540c\u65f6\u7ef4\u6301\u9ad8\u5956\u52b1\u503c\u3002", "conclusion": "Soft Best-of-$n$\u901a\u8fc7\u6e29\u5ea6\u53c2\u6570\u5b9e\u73b0\u5e73\u6ed1\u8fc7\u6e21\uff0c\u7406\u8bba\u6536\u655b\u6027\u66f4\u4f18\uff0c\u4e3a\u65e0\u5fae\u8c03\u5bf9\u9f50\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.03034", "pdf": "https://arxiv.org/pdf/2505.03034", "abs": "https://arxiv.org/abs/2505.03034", "authors": ["Sweta Rai", "Douglas W. Nychka", "Soutir Bandyopadhyay"], "title": "Modeling Spatial Extremes using Non-Gaussian Spatial Autoregressive Models via Convolutional Neural Networks", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Data derived from remote sensing or numerical simulations often have a\nregular gridded structure and are large in volume, making it challenging to\nfind accurate spatial models that can fill in missing grid cells or simulate\nthe process effectively, especially in the presence of spatial heterogeneity\nand heavy-tailed marginal distributions. To overcome this issue, we present a\nspatial autoregressive modeling framework, which maps observations at a\nlocation and its neighbors to independent random variables. This is a highly\nflexible modeling approach and well-suited for non-Gaussian fields, providing\nsimpler interpretability. In particular, we consider the SAR model with\nGeneralized Extreme Value distribution innovations to combine the observation\nat a central grid location with its neighbors, capturing extreme spatial\nbehavior based on the heavy-tailed innovations. While these models are fast to\nsimulate by exploiting the sparsity of the key matrices in the computations,\nthe maximum likelihood estimation of the parameters is prohibitive due to the\nintractability of the likelihood, making optimization challenging. To overcome\nthis, we train a convolutional neural network on a large training set that\ncovers a useful parameter space, and then use the trained network for fast\nparameter estimation. Finally, we apply this model to analyze annual maximum\nprecipitation data from ERA-Interim-driven Weather Research and Forecasting\n(WRF) simulations, allowing us to explore its spatial extreme behavior across\nNorth America.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7a7a\u95f4\u81ea\u56de\u5f52\u5efa\u6a21\u6846\u67b6\uff0c\u7ed3\u5408\u5e7f\u4e49\u6781\u503c\u5206\u5e03\u521b\u65b0\u9879\uff0c\u7528\u4e8e\u5904\u7406\u975e\u9ad8\u65af\u573a\u548c\u6781\u7aef\u7a7a\u95f4\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u53c2\u6570\u4f30\u8ba1\uff0c\u5e94\u7528\u4e8e\u5317\u7f8e\u6700\u5927\u964d\u6c34\u6570\u636e\u5206\u6790\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u3001\u975e\u9ad8\u65af\u4e14\u7a7a\u95f4\u5f02\u8d28\u6027\u5f3a\u7684\u7f51\u683c\u6570\u636e\u65f6\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u586b\u8865\u7f3a\u5931\u7f51\u683c\u6216\u6a21\u62df\u6781\u7aef\u7a7a\u95f4\u884c\u4e3a\u65f6\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u7a7a\u95f4\u81ea\u56de\u5f52\u6a21\u578b\uff08SAR\uff09\u7ed3\u5408\u5e7f\u4e49\u6781\u503c\u5206\u5e03\u521b\u65b0\u9879\uff0c\u901a\u8fc7\u90bb\u57df\u89c2\u6d4b\u6620\u5c04\u5230\u72ec\u7acb\u968f\u673a\u53d8\u91cf\uff0c\u5e76\u5229\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5feb\u901f\u4f30\u8ba1\u53c2\u6570\u3002", "result": "\u8be5\u6846\u67b6\u5728\u6a21\u62df\u548c\u586b\u8865\u7f51\u683c\u6570\u636e\u65f6\u8868\u73b0\u7075\u6d3b\u9ad8\u6548\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6781\u7aef\u7a7a\u95f4\u884c\u4e3a\u7684\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5927\u5e45\u63d0\u5347\u4e86\u53c2\u6570\u4f30\u8ba1\u901f\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u975e\u9ad8\u65af\u573a\u548c\u6781\u7aef\u7a7a\u95f4\u884c\u4e3a\u7684\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u5c24\u5176\u5728\u5904\u7406\u5927\u89c4\u6a21\u7f51\u683c\u6570\u636e\u65f6\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2505.03173", "pdf": "https://arxiv.org/pdf/2505.03173", "abs": "https://arxiv.org/abs/2505.03173", "authors": ["Sameer Malik", "Moyuru Yamada", "Ayush Singh", "Dishank Aggarwal"], "title": "RAVU: Retrieval Augmented Video Understanding with Compositional Reasoning over Graph", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Comprehending long videos remains a significant challenge for Large\nMulti-modal Models (LMMs). Current LMMs struggle to process even minutes to\nhours videos due to their lack of explicit memory and retrieval mechanisms. To\naddress this limitation, we propose RAVU (Retrieval Augmented Video\nUnderstanding), a novel framework for video understanding enhanced by retrieval\nwith compositional reasoning over a spatio-temporal graph. We construct a graph\nrepresentation of the video, capturing both spatial and temporal relationships\nbetween entities. This graph serves as a long-term memory, allowing us to track\nobjects and their actions across time. To answer complex queries, we decompose\nthe queries into a sequence of reasoning steps and execute these steps on the\ngraph, retrieving relevant key information. Our approach enables more accurate\nunderstanding of long videos, particularly for queries that require multi-hop\nreasoning and tracking objects across frames. Our approach demonstrate superior\nperformances with limited retrieved frames (5-10) compared with other SOTA\nmethods and baselines on two major video QA datasets, NExT-QA and EgoSchema.", "AI": {"tldr": "RAVU\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u7d22\u589e\u5f3a\u7684\u957f\u89c6\u9891\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u7a7a\u56fe\u8bb0\u5fc6\u673a\u5236\u89e3\u51b3\u73b0\u6709LMM\u5904\u7406\u957f\u89c6\u9891\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709LMM\u56e0\u7f3a\u4e4f\u663e\u5f0f\u8bb0\u5fc6\u4e0e\u68c0\u7d22\u673a\u5236\uff0c\u96be\u4ee5\u5904\u7406\u5206\u949f\u81f3\u5c0f\u65f6\u7ea7\u957f\u89c6\u9891\uff0c\u5c24\u5176\u5728\u591a\u8df3\u63a8\u7406\u548c\u8de8\u5e27\u76ee\u6807\u8ffd\u8e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u6784\u5efa\u89c6\u9891\u7684\u65f6\u7a7a\u56fe\u8868\u793a\u4f5c\u4e3a\u957f\u671f\u8bb0\u5fc6\uff0c\u901a\u8fc7\u68c0\u7d22\u4e0e\u7ec4\u5408\u5f0f\u63a8\u7406\u5206\u89e3\u590d\u6742\u67e5\u8be2\uff0c\u5728\u56fe\u4e2d\u9010\u6b65\u6267\u884c\u63a8\u7406\u6b65\u9aa4\u3002", "result": "\u5728NExT-QA\u548cEgoSchema\u6570\u636e\u96c6\u4e0a\uff0c\u4ec5\u75285-10\u5e27\u68c0\u7d22\u5373\u53ef\u8d85\u8d8aSOTA\u65b9\u6cd5\u4e0e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "RAVU\u8bc1\u660e\u4e86\u65f6\u7a7a\u56fe\u4e0e\u68c0\u7d22\u589e\u5f3a\u673a\u5236\u5bf9\u957f\u89c6\u9891\u7406\u89e3\u7684\u6709\u6548\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.03176", "pdf": "https://arxiv.org/pdf/2505.03176", "abs": "https://arxiv.org/abs/2505.03176", "authors": ["Hafez Ghaemi", "Eilif Muller", "Shahab Bakhtiari"], "title": "seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Current self-supervised algorithms mostly rely on transformations such as\ndata augmentation and masking to learn visual representations. This is achieved\nby inducing invariance or equivariance with respect to these transformations\nafter encoding two views of an image. This dominant two-view paradigm can limit\nthe flexibility of learned representations for downstream adaptation by\ncreating performance trade-offs between invariance-related tasks such as image\nclassification and more fine-grained equivariance-related tasks. In this work,\nwe introduce \\emph{seq-JEPA}, a world modeling paradigm based on\njoint-embedding predictive architecture that leverages architectural inductive\nbiases to resolve this trade-off. Without requiring an additional equivariance\npredictor or loss term, seq-JEPA simultaneously learns two architecturally\nsegregated representations: one equivariant to the specified transformations\nand another invariant to them and suited for tasks such as classification. To\ndo so, our model processes a short sequence of different views (observations)\nof an input image. Each encoded view is concatenated with embeddings\ncorresponding to the relative transformation (action) producing the next\nobservation in the sequence. A transformer encoder outputs an aggregate\nrepresentation of this sequence, which is subsequently conditioned on the\naction leading to the next observation to predict its representation.\nEmpirically, seq-JEPA achieves strong performance on equivariant benchmarks and\nimage classification without sacrificing one for the other. Additionally, our\nframework excels at tasks that inherently require aggregating a sequence of\nobservations, such as path integration across actions and predictive learning\nacross eye movements.", "AI": {"tldr": "seq-JEPA\u901a\u8fc7\u5904\u7406\u56fe\u50cf\u591a\u89c6\u56fe\u5e8f\u5217\uff0c\u540c\u65f6\u5b66\u4e60\u4e0d\u53d8\u548c\u7b49\u53d8\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u53cc\u89c6\u56fe\u65b9\u6cd5\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u7075\u6d3b\u6027\u9650\u5236\uff0c\u5e73\u8861\u4e86\u5206\u7c7b\u4e0e\u7b49\u53d8\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u53cc\u89c6\u56fe\u81ea\u76d1\u7763\u5b66\u4e60\u56e0\u8fc7\u5ea6\u4f9d\u8d56\u6570\u636e\u589e\u5f3a\u548c\u63a9\u7801\uff0c\u5bfc\u81f4\u4e0d\u53d8\u6027\u548c\u7b49\u53d8\u6027\u4efb\u52a1\u6027\u80fd\u96be\u4ee5\u517c\u987e\uff0c\u9650\u5236\u4e86\u8868\u793a\u7075\u6d3b\u6027\u3002", "method": "seq-JEPA\u57fa\u4e8e\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\uff0c\u5904\u7406\u591a\u89c6\u56fe\u5e8f\u5217\u5e76\u878d\u5408\u76f8\u5bf9\u53d8\u6362\u4fe1\u606f\uff0c\u901a\u8fc7Transformer\u7f16\u7801\u5668\u751f\u6210\u805a\u5408\u8868\u793a\uff0c\u9884\u6d4b\u4e0b\u4e00\u89c6\u56fe\u8868\u5f81\u3002", "result": "seq-JEPA\u5728\u7b49\u53d8\u57fa\u51c6\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u64c5\u957f\u5e8f\u5217\u89c2\u6d4b\u805a\u5408\u4efb\u52a1\uff08\u5982\u8def\u5f84\u6574\u5408\u548c\u773c\u52a8\u9884\u6d4b\uff09\u3002", "conclusion": "seq-JEPA\u901a\u8fc7\u591a\u89c6\u56fe\u5e8f\u5217\u5efa\u6a21\u6709\u6548\u5e73\u8861\u4e86\u4e0d\u53d8\u6027\u4e0e\u7b49\u53d8\u6027\u9700\u6c42\uff0c\u4e3a\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u8868\u793a\u6846\u67b6\u3002"}}
{"id": "2505.03069", "pdf": "https://arxiv.org/pdf/2505.03069", "abs": "https://arxiv.org/abs/2505.03069", "authors": ["Yurui Zhang", "Ruigang Wang", "Ian R. Manchester"], "title": "Robustly Invertible Nonlinear Dynamics and the BiLipREN: Contracting Neural Models with Contracting Inverses", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": null, "summary": "We study the invertibility of nonlinear dynamical systems from the\nperspective of contraction and incremental stability analysis and propose a new\ninvertible recurrent neural model: the BiLipREN. In particular, we consider a\nnonlinear state space model to be robustly invertible if an inverse exists with\na state space realisation, and both the forward model and its inverse are\ncontracting, i.e. incrementally exponentially stable, and Lipschitz, i.e. have\nbounded incremental gain. This property of bi-Lipschitzness implies both\nrobustness in the sense of sensitivity to input perturbations, as well as\nrobust distinguishability of different inputs from their corresponding outputs,\ni.e. the inverse model robustly reconstructs the input sequence despite small\nperturbations to the initial conditions and measured output. Building on this\nfoundation, we propose a parameterization of neural dynamic models:\nbi-Lipschitz recurrent equilibrium networks (biLipREN), which are robustly\ninvertible by construction. Moreover, biLipRENs can be composed with orthogonal\nlinear systems to construct more general bi-Lipschitz dynamic models, e.g., a\nnonlinear analogue of minimum-phase/all-pass (inner/outer) factorization. We\nillustrate the utility of our proposed approach with numerical examples.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u7684\u53ef\u9006\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u9006\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u6a21\u578bBiLipREN\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u7a33\u5065\u6027\u548c\u6784\u9020\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u7684\u53ef\u9006\u6027\u53ca\u5176\u7a33\u5065\u6027\uff0c\u4ee5\u89e3\u51b3\u8f93\u5165\u6270\u52a8\u548c\u521d\u59cb\u6761\u4ef6\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u8f93\u5165\u91cd\u5efa\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u6536\u7f29\u548c\u589e\u91cf\u7a33\u5b9a\u6027\u5206\u6790\uff0c\u63d0\u51faBiLipREN\u6a21\u578b\uff0c\u786e\u4fdd\u6a21\u578b\u53ca\u5176\u9006\u90fd\u662f\u6536\u7f29\u4e14Lipschitz\u7684\uff0c\u4ece\u800c\u7a33\u5065\u5730\u91cd\u5efa\u8f93\u5165\u5e8f\u5217\u3002", "result": "BiLipREN\u6a21\u578b\u901a\u8fc7\u6784\u9020\u5b9e\u73b0\u7a33\u5065\u53ef\u9006\u6027\uff0c\u5e76\u80fd\u4e0e\u6b63\u4ea4\u7ebf\u6027\u7cfb\u7edf\u7ed3\u5408\u6784\u5efa\u66f4\u4e00\u822c\u7684bi-Lipschitz\u52a8\u6001\u6a21\u578b\u3002", "conclusion": "BiLipREN\u4e3a\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u7684\u7a33\u5065\u53ef\u9006\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u793a\u4f8b\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.03193", "pdf": "https://arxiv.org/pdf/2505.03193", "abs": "https://arxiv.org/abs/2505.03193", "authors": ["Wei Meng"], "title": "A study on audio synchronous steganography detection and distributed guide inference model based on sliding spectral features and intelligent inference drive", "categories": ["cs.SD", "cs.AI", "cs.CR", "eess.AS", "94A12 (Primary), 68T07, 42A38 (Secondary)", "H.3.3; I.5.4; I.2.6"], "comment": "This paper proposes a novel framework for detecting steganographic\n  content in short video audio streams using sliding spectral features and\n  distributed inference models, combining STFT analysis, entropy-based\n  synchronization, and deep learning-driven decoding strategies", "summary": "With the rise of short video platforms in global communication, embedding\nsteganographic data in audio synchronization streams has emerged as a new\ncovert communication method. To address the limitations of traditional\ntechniques in detecting synchronized steganography, this paper proposes a\ndetection and distributed guidance reconstruction model based on short video\n\"Yupan\" samples released by China's South Sea Fleet on TikTok. The method\nintegrates sliding spectrum feature extraction and intelligent inference\nmechanisms. A 25 ms sliding window with short-time Fourier transform (STFT) is\nused to extract the main frequency trajectory and construct the synchronization\nframe detection model (M1), identifying a frame flag \"FFFFFFFFFFFFFFFFFF80\".\nThe subsequent 32-byte payload is decoded by a structured model (M2) to infer\ndistributed guidance commands. Analysis reveals a low-entropy, repetitive byte\nsequence in the 36 to 45 second audio segment with highly concentrated spectral\nenergy, confirming the presence of synchronization frames. Although plaintext\nsemantics are not restored, the consistency in command field layout suggests\nfeatures of military communication protocols. The multi-segment splicing model\nfurther shows cross-video embedding and centralized decoding capabilities. The\nproposed framework validates the effectiveness of sliding spectral features for\nsynchronized steganography detection and builds an extensible inference model\nfor covert communication analysis and tactical guidance simulation on open\nplatforms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ed1\u52a8\u9891\u8c31\u7279\u5f81\u548c\u667a\u80fd\u63a8\u7406\u7684\u540c\u6b65\u9690\u5199\u68c0\u6d4b\u4e0e\u5206\u5e03\u5f0f\u5f15\u5bfc\u91cd\u5efa\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5f00\u653e\u5e73\u53f0\u9690\u853d\u901a\u4fe1\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u89e3\u51b3\u4f20\u7edf\u6280\u672f\u5728\u68c0\u6d4b\u540c\u6b65\u9690\u5199\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7814\u7a76\u57fa\u4e8e\u4e2d\u56fd\u5357\u6d77\u8230\u961f\u5728TikTok\u53d1\u5e03\u7684\u77ed\u89c6\u9891\u6837\u672c\uff0c\u63d0\u51fa\u65b0\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u91c7\u752825\u6beb\u79d2\u6ed1\u52a8\u7a97\u53e3\u548c\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362\uff08STFT\uff09\u63d0\u53d6\u4e3b\u9891\u8f68\u8ff9\uff0c\u6784\u5efa\u540c\u6b65\u5e27\u68c0\u6d4b\u6a21\u578b\uff08M1\uff09\u548c\u7ed3\u6784\u5316\u6a21\u578b\uff08M2\uff09\u89e3\u7801\u6709\u6548\u8f7d\u8377\u3002", "result": "\u5728\u97f3\u9891\u6bb5\uff0836-45\u79d2\uff09\u53d1\u73b0\u4f4e\u71b5\u91cd\u590d\u5b57\u8282\u5e8f\u5217\u548c\u9ad8\u96c6\u4e2d\u9891\u8c31\u80fd\u91cf\uff0c\u9a8c\u8bc1\u4e86\u540c\u6b65\u5e27\u7684\u5b58\u5728\u548c\u519b\u4e8b\u901a\u4fe1\u534f\u8bae\u7279\u5f81\u3002", "conclusion": "\u6ed1\u52a8\u9891\u8c31\u7279\u5f81\u5bf9\u540c\u6b65\u9690\u5199\u68c0\u6d4b\u6709\u6548\uff0c\u5efa\u7acb\u7684\u53ef\u6269\u5c55\u63a8\u7406\u6a21\u578b\u9002\u7528\u4e8e\u9690\u853d\u901a\u4fe1\u5206\u6790\u548c\u6218\u672f\u5f15\u5bfc\u6a21\u62df\u3002"}}
{"id": "2505.03196", "pdf": "https://arxiv.org/pdf/2505.03196", "abs": "https://arxiv.org/abs/2505.03196", "authors": ["Haoxiang Luo", "Gang Sun", "Yinqiu Liu", "Dusit Niyato", "Hongfang Yu", "Mohammed Atiquzzaman", "Schahram Dustdar"], "title": "A Trustworthy Multi-LLM Network: Challenges,Solutions, and A Use Case", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate strong potential across a variety of\ntasks in communications and networking due to their advanced reasoning\ncapabilities. However, because different LLMs have different model structures\nand are trained using distinct corpora and methods, they may offer varying\noptimization strategies for the same network issues. Moreover, the limitations\nof an individual LLM's training data, aggravated by the potential maliciousness\nof its hosting device, can result in responses with low confidence or even\nbias. To address these challenges, we propose a blockchain-enabled\ncollaborative framework that connects multiple LLMs into a Trustworthy\nMulti-LLM Network (MultiLLMN). This architecture enables the cooperative\nevaluation and selection of the most reliable and high-quality responses to\ncomplex network optimization problems. Specifically, we begin by reviewing\nrelated work and highlighting the limitations of existing LLMs in collaboration\nand trust, emphasizing the need for trustworthiness in LLM-based systems. We\nthen introduce the workflow and design of the proposed Trustworthy MultiLLMN\nframework. Given the severity of False Base Station (FBS) attacks in B5G and 6G\ncommunication systems and the difficulty of addressing such threats through\ntraditional modeling techniques, we present FBS defense as a case study to\nempirically validate the effectiveness of our approach. Finally, we outline\npromising future research directions in this emerging area.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u591aLLM\u534f\u4f5c\u6846\u67b6\uff08MultiLLMN\uff09\uff0c\u65e8\u5728\u901a\u8fc7\u8054\u5408\u8bc4\u4f30\u63d0\u5347\u590d\u6742\u7f51\u7edc\u4f18\u5316\u95ee\u9898\u7684\u54cd\u5e94\u53ef\u9760\u6027\u548c\u8d28\u91cf\uff0c\u5e76\u4ee5FBS\u653b\u51fb\u9632\u5fa1\u4e3a\u4f8b\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e0d\u540cLLM\u56e0\u7ed3\u6784\u548c\u8bad\u7ec3\u6570\u636e\u5dee\u5f02\u53ef\u80fd\u5bfc\u81f4\u4f18\u5316\u7b56\u7565\u4e0d\u4e00\u81f4\uff0c\u52a0\u4e4b\u5355\u4e00LLM\u7684\u6570\u636e\u5c40\u9650\u6027\u548c\u6f5c\u5728\u6076\u610f\u98ce\u9669\uff0c\u9700\u6784\u5efa\u53ef\u4fe1\u7684\u591aLLM\u534f\u4f5c\u7f51\u7edc\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u591aLLM\u534f\u4f5c\u6846\u67b6\uff0c\u8054\u5408\u8bc4\u4f30\u5e76\u7b5b\u9009\u6700\u4f18\u54cd\u5e94\uff1b\u4ee5FBS\u653b\u51fb\u9632\u5fa1\u4e3a\u4f8b\u8fdb\u884c\u5b9e\u8bc1\u3002", "result": "\u6846\u67b6\u6709\u6548\u63d0\u5347\u7f51\u7edc\u4f18\u5316\u95ee\u9898\u7684\u54cd\u5e94\u53ef\u9760\u6027\u548c\u8d28\u91cf\uff0cFBS\u6848\u4f8b\u9a8c\u8bc1\u5176\u53ef\u884c\u6027\u3002", "conclusion": "MultiLLMN\u4e3a\u89e3\u51b3LLM\u534f\u4f5c\u4e0e\u4fe1\u4efb\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u66f4\u591a\u7f51\u7edc\u573a\u666f\u3002"}}
{"id": "2505.03120", "pdf": "https://arxiv.org/pdf/2505.03120", "abs": "https://arxiv.org/abs/2505.03120", "authors": ["Abdul Mustafa", "Muhammad Talha Khan", "Muhammad Azmi Umer", "Zaki Masood", "Chuadhry Mujeeb Ahmed"], "title": "Adversarial Sample Generation for Anomaly Detection in Industrial Control Systems", "categories": ["cs.CR", "cs.LG"], "comment": "Accepted in the 1st Workshop on Modeling and Verification for Secure\n  and Performant Cyber-Physical Systems in conjunction with Cyber-Physical\n  Systems and Internet-of-Things Week, Irvine, USA, May 6-9, 2025", "summary": "Machine learning (ML)-based intrusion detection systems (IDS) are vulnerable\nto adversarial attacks. It is crucial for an IDS to learn to recognize\nadversarial examples before malicious entities exploit them. In this paper, we\ngenerated adversarial samples using the Jacobian Saliency Map Attack (JSMA). We\nvalidate the generalization and scalability of the adversarial samples to\ntackle a broad range of real attacks on Industrial Control Systems (ICS). We\nevaluated the impact by assessing multiple attacks generated using the proposed\nmethod. The model trained with adversarial samples detected attacks with 95%\naccuracy on real-world attack data not used during training. The study was\nconducted using an operational secure water treatment (SWaT) testbed.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u5bf9\u5bf9\u6297\u6837\u672c\u7684\u8106\u5f31\u6027\uff0c\u4f7f\u7528JSMA\u751f\u6210\u5bf9\u6297\u6837\u672c\uff0c\u5e76\u5728\u5de5\u4e1a\u63a7\u5236\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u5176\u6cdb\u5316\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002\u5bf9\u6297\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u5728\u771f\u5b9e\u653b\u51fb\u6570\u636e\u4e0a\u8fbe\u523095%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u5bf9\u6297\u653b\u51fb\u53ef\u80fd\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6f0f\u6d1e\uff0c\u5c24\u5176\u5728\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u4e2d\uff0c\u8bc6\u522b\u5bf9\u6297\u6837\u672c\u5bf9\u786e\u4fdd\u7cfb\u7edf\u5b89\u5168\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528Jacobian Saliency Map Attack (JSMA) \u751f\u6210\u5bf9\u6297\u6837\u672c\uff0c\u5e76\u5728\u5de5\u4e1a\u63a7\u5236\u7cfb\u7edf\uff08ICS\uff09\u7684\u5b9e\u9645\u653b\u51fb\u6570\u636e\u4e0a\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u5bf9\u6297\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u5728\u672a\u53c2\u4e0e\u8bad\u7ec3\u7684\u771f\u5b9e\u653b\u51fb\u6570\u636e\u4e0a\u8fbe\u523095%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "conclusion": "\u5bf9\u6297\u8bad\u7ec3\u80fd\u6709\u6548\u63d0\u5347\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u5bf9\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5de5\u4e1a\u63a7\u5236\u7cfb\u7edf\u7b49\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2505.03204", "pdf": "https://arxiv.org/pdf/2505.03204", "abs": "https://arxiv.org/abs/2505.03204", "authors": ["Liu Suxing", "Byungwon Min"], "title": "DCS-ST for Classification of Breast Cancer Histopathology Images with Limited Annotations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep learning methods have shown promise in classifying breast cancer\nhistopathology images, but their performance often declines with limited\nannotated data, a critical challenge in medical imaging due to the high cost\nand expertise required for annotations.", "AI": {"tldr": "\u6df1\u5ea6\u5b66\u4e60\u5728\u4e73\u817a\u764c\u7ec4\u7ec7\u75c5\u7406\u56fe\u50cf\u5206\u7c7b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u8fd9\u662f\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u91cd\u8981\u6311\u6218\u3002", "motivation": "\u7531\u4e8e\u533b\u5b66\u5f71\u50cf\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u83b7\u53d6\u5927\u91cf\u6807\u6ce8\u6570\u636e\u56f0\u96be\uff0c\u800c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u6570\u636e\u6709\u9650\u65f6\u6027\u80fd\u4e0d\u4f73\u3002", "method": "", "result": "", "conclusion": ""}}
{"id": "2505.03134", "pdf": "https://arxiv.org/pdf/2505.03134", "abs": "https://arxiv.org/abs/2505.03134", "authors": ["Sajjad Rezvani Boroujeni", "Hossein Abedi", "Tom Bush"], "title": "Enhancing Glass Defect Detection with Diffusion Models: Addressing Imbalanced Datasets in Manufacturing Quality Control", "categories": ["cs.CV", "cs.LG"], "comment": "12 pages, 7 figures, submitted to Computer and Decision Making An\n  International Journal (COMDEM)", "summary": "Visual defect detection in industrial glass manufacturing remains a critical\nchallenge due to the low frequency of defective products, leading to imbalanced\ndatasets that limit the performance of deep learning models and computer vision\nsystems. This paper presents a novel approach using Denoising Diffusion\nProbabilistic Models (DDPMs) to generate synthetic defective glass product\nimages for data augmentation, effectively addressing class imbalance issues in\nmanufacturing quality control and automated visual inspection. The methodology\nsignificantly enhances image classification performance of standard CNN\narchitectures (ResNet50V2, EfficientNetB0, and MobileNetV2) in detecting\nanomalies by increasing the minority class representation. Experimental results\ndemonstrate substantial improvements in key machine learning metrics,\nparticularly in recall for defective samples across all tested deep neural\nnetwork architectures while maintaining perfect precision. The most dramatic\nimprovement was observed in ResNet50V2's overall classification accuracy, which\nincreased from 78 percent to 93 percent when trained with the augmented data.\nThis work provides a scalable, cost-effective approach to enhancing automated\ndefect detection in glass manufacturing that can potentially be extended to\nother industrial quality assurance systems and industries with similar class\nimbalance challenges.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDenoising Diffusion Probabilistic Models (DDPMs)\u751f\u6210\u5408\u6210\u7f3a\u9677\u73bb\u7483\u4ea7\u54c1\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6570\u636e\u589e\u5f3a\uff0c\u89e3\u51b3\u5236\u9020\u4e1a\u8d28\u91cf\u63a7\u5236\u548c\u81ea\u52a8\u5316\u89c6\u89c9\u68c0\u6d4b\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002\u901a\u8fc7\u589e\u52a0\u5c11\u6570\u7c7b\u522b\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86CNN\u67b6\u6784\uff08ResNet50V2\u3001EfficientNetB0\u548cMobileNetV2\uff09\u5728\u7f3a\u9677\u68c0\u6d4b\u4e2d\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u53ec\u56de\u7387\u4e0a\u7684\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\u3002\u5728ResNet50V2\u4e0a\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u4ece78%\u63d0\u5347\u81f393%\u3002", "motivation": "\u5de5\u4e1a\u73bb\u7483\u5236\u9020\u4e2d\u89c6\u89c9\u7f3a\u9677\u68c0\u6d4b\u9762\u4e34\u7740\u7f3a\u9677\u4ea7\u54c1\u9891\u7387\u4f4e\u3001\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u7684\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u7684\u8868\u73b0\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u751f\u6210\u5408\u6210\u6570\u636e\u4ee5\u589e\u5f3a\u5c11\u6570\u7c7b\u522b\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Denoising Diffusion Probabilistic Models (DDPMs)\u751f\u6210\u5408\u6210\u7f3a\u9677\u73bb\u7483\u4ea7\u54c1\u56fe\u50cf\uff0c\u4f5c\u4e3a\u6570\u636e\u589e\u5f3a\u624b\u6bb5\u3002\u968f\u540e\u5229\u7528ResNet50V2\u3001EfficientNetB0\u548cMobileNetV2\u7b49CNN\u67b6\u6784\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u6d4b\u8bd5\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u7f3a\u9677\u6837\u672c\u7684\u53ec\u56de\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b8c\u7f8e\u7cbe\u5ea6\u3002ResNet50V2\u7684\u5206\u7c7b\u51c6\u786e\u7387\u4ece78%\u63d0\u5347\u81f393%\uff0c\u63d0\u5347\u6700\u4e3a\u663e\u8457\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u52a8\u5316\u7f3a\u9677\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e0d\u4ec5\u9002\u7528\u4e8e\u73bb\u7483\u5236\u9020\u4e1a\uff0c\u8fd8\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u9762\u4e34\u7c7b\u4f3c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u7684\u5de5\u4e1a\u8d28\u91cf\u4fdd\u8bc1\u7cfb\u7edf\u3002"}}
{"id": "2505.03214", "pdf": "https://arxiv.org/pdf/2505.03214", "abs": "https://arxiv.org/abs/2505.03214", "authors": ["Qiang Sun", "Sirui Li", "Tingting Bi", "Du Huynh", "Mark Reynolds", "Yuanyi Luo", "Wei Liu"], "title": "DocSpiral: A Platform for Integrated Assistive Document Annotation through Human-in-the-Spiral", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Acquiring structured data from domain-specific, image-based documents such as\nscanned reports is crucial for many downstream tasks but remains challenging\ndue to document variability. Many of these documents exist as images rather\nthan as machine-readable text, which requires human annotation to train\nautomated extraction systems. We present DocSpiral, the first\nHuman-in-the-Spiral assistive document annotation platform, designed to address\nthe challenge of extracting structured information from domain-specific,\nimage-based document collections. Our spiral design establishes an iterative\ncycle in which human annotations train models that progressively require less\nmanual intervention. DocSpiral integrates document format normalization,\ncomprehensive annotation interfaces, evaluation metrics dashboard, and API\nendpoints for the development of AI / ML models into a unified workflow.\nExperiments demonstrate that our framework reduces annotation time by at least\n41\\% while showing consistent performance gains across three iterations during\nmodel training. By making this annotation platform freely accessible, we aim to\nlower barriers to AI/ML models development in document processing, facilitating\nthe adoption of large language models in image-based, document-intensive fields\nsuch as geoscience and healthcare. The system is freely available at:\nhttps://app.ai4wa.com. The demonstration video is available:\nhttps://app.ai4wa.com/docs/docspiral/demo.", "AI": {"tldr": "DocSpiral\u662f\u4e00\u4e2a\u4eba\u7c7b\u53c2\u4e0e\u7684\u8f85\u52a9\u6587\u6863\u6ce8\u91ca\u5e73\u53f0\uff0c\u901a\u8fc7\u8fed\u4ee3\u5faa\u73af\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\uff0c\u63d0\u9ad8\u4ece\u56fe\u50cf\u6587\u6863\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u6570\u636e\u7684\u6548\u7387\uff0c\u5b9e\u9a8c\u663e\u793a\u6ce8\u91ca\u65f6\u95f4\u51cf\u5c11\u81f3\u5c1141%\u3002", "motivation": "\u7531\u4e8e\u9886\u57df\u7279\u5b9a\u7684\u56fe\u50cf\u6587\u6863\u5b58\u5728\u53d8\u5f02\u6027\u4e14\u591a\u4e3a\u975e\u673a\u5668\u53ef\u8bfb\u6587\u672c\uff0c\u9700\u8981\u4eba\u5de5\u6ce8\u91ca\u8bad\u7ec3\u81ea\u52a8\u5316\u63d0\u53d6\u7cfb\u7edf\uff0c\u5f00\u53d1DocSpiral\u65e8\u5728\u964d\u4f4e\u8fd9\u7c7b\u6587\u6863\u5904\u7406\u7684AI/ML\u6a21\u578b\u5f00\u53d1\u95e8\u69db\u3002", "method": "\u91c7\u7528\u87ba\u65cb\u8bbe\u8ba1\uff0c\u901a\u8fc7\u6587\u6863\u683c\u5f0f\u6807\u51c6\u5316\u3001\u7efc\u5408\u6ce8\u91ca\u754c\u9762\u3001\u8bc4\u4f30\u6307\u6807\u9762\u677f\u548cAPI\u7aef\u70b9\u7684\u96c6\u6210\u5de5\u4f5c\u6d41\uff0c\u5b9e\u73b0\u6e10\u8fdb\u5f0f\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5e73\u53f0\u5728\u6a21\u578b\u8bad\u7ec3\u7684\u4e09\u4e2a\u8fed\u4ee3\u4e2d\u8868\u73b0\u7a33\u5b9a\uff0c\u6ce8\u91ca\u65f6\u95f4\u51cf\u5c11\u81f3\u5c1141%\u3002", "conclusion": "\u514d\u8d39\u5f00\u653e\u7684DocSpiral\u5e73\u53f0\u6709\u52a9\u4e8e\u63a8\u52a8\u56fe\u50cf\u5bc6\u96c6\u578b\u9886\u57df\uff08\u5982\u5730\u5b66\u4e0e\u533b\u7597\uff09\u4e2d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e94\u7528\uff0c\u63d0\u5347\u6587\u6863\u5904\u7406\u6548\u7387\u3002"}}
{"id": "2505.03140", "pdf": "https://arxiv.org/pdf/2505.03140", "abs": "https://arxiv.org/abs/2505.03140", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "HMAE: Self-Supervised Few-Shot Learning for Quantum Spin Systems", "categories": ["quant-ph", "cs.LG"], "comment": null, "summary": "Quantum machine learning for spin and molecular systems faces critical\nchallenges of scarce labeled data and computationally expensive simulations. To\naddress these limitations, we introduce Hamiltonian-Masked Autoencoding (HMAE),\na novel self-supervised framework that pre-trains transformers on unlabeled\nquantum Hamiltonians, enabling efficient few-shot transfer learning. Unlike\nrandom masking approaches, HMAE employs a physics-informed strategy based on\nquantum information theory to selectively mask Hamiltonian terms based on their\nphysical significance. Experiments on 12,500 quantum Hamiltonians (60%\nreal-world, 40% synthetic) demonstrate that HMAE achieves 85.3% $\\pm$ 1.5%\naccuracy in phase classification and 0.15 $\\pm$ 0.02 eV MAE in ground state\nenergy prediction with merely 10 labeled examples - a statistically significant\nimprovement (p < 0.01) over classical graph neural networks (78.1% $\\pm$ 2.1%)\nand quantum neural networks (76.8% $\\pm$ 2.3%). Our method's primary advantage\nis exceptional sample efficiency - reducing required labeled examples by 3-5x\ncompared to baseline methods - though we emphasize that ground truth values for\nfine-tuning and evaluation still require exact diagonalization or tensor\nnetworks. We explicitly acknowledge that our current approach is limited to\nsmall quantum systems (specifically limited to 12 qubits during training, with\nlimited extension to 16-20 qubits in testing) and that, while promising within\nthis regime, this size restriction prevents immediate application to larger\nsystems of practical interest in materials science and quantum chemistry.", "AI": {"tldr": "HMAE\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u7406\u4fe1\u606f\u63a9\u7801\u9884\u8bad\u7ec3Transformer\uff0c\u663e\u8457\u63d0\u5347\u91cf\u5b50\u7cfb\u7edf\u7684\u5206\u7c7b\u548c\u9884\u6d4b\u6027\u80fd\uff0c\u4ec5\u9700\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u3002", "motivation": "\u89e3\u51b3\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4e2d\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u548c\u8ba1\u7b97\u6a21\u62df\u6602\u8d35\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faHamiltonian-Masked Autoencoding (HMAE)\uff0c\u57fa\u4e8e\u91cf\u5b50\u4fe1\u606f\u7406\u8bba\u9009\u62e9\u6027\u63a9\u7801\u54c8\u5bc6\u987f\u91cf\u9879\u3002", "result": "\u572812500\u4e2a\u91cf\u5b50\u54c8\u5bc6\u987f\u91cf\u4e0a\uff0cHMAE\u5b9e\u73b085.3%\u7684\u76f8\u5206\u7c7b\u51c6\u786e\u7387\u548c0.15 eV\u7684\u57fa\u6001\u80fd\u91cf\u9884\u6d4b\u8bef\u5dee\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "HMAE\u5177\u6709\u5353\u8d8a\u7684\u6837\u672c\u6548\u7387\uff0c\u4f46\u53d7\u9650\u4e8e\u5c0f\u89c4\u6a21\u91cf\u5b50\u7cfb\u7edf\uff0812-20\u91cf\u5b50\u6bd4\u7279\uff09\u3002"}}
{"id": "2505.03217", "pdf": "https://arxiv.org/pdf/2505.03217", "abs": "https://arxiv.org/abs/2505.03217", "authors": ["Xiaobo Jin", "JiaShu Tu"], "title": "Accelerating Evolution: Integrating PSO Principles into Real-Coded Genetic Algorithm Crossover", "categories": ["cs.NE", "cs.AI", "I.2.8; G.1.6"], "comment": "14 pages,2 figures,4 tables", "summary": "This study introduces an innovative crossover operator named Particle Swarm\nOptimization-inspired Crossover (PSOX), which is specifically developed for\nreal-coded genetic algorithms. Departing from conventional crossover approaches\nthat only exchange information between individuals within the same generation,\nPSOX uniquely incorporates guidance from both the current global best solution\nand historical optimal solutions across multiple generations. This novel\nmechanism enables the algorithm to maintain population diversity while\nsimultaneously accelerating convergence toward promising regions of the search\nspace. The effectiveness of PSOX is rigorously evaluated through comprehensive\nexperiments on 15 benchmark test functions with diverse characteristics,\nincluding unimodal, multimodal, and highly complex landscapes. Comparative\nanalysis against five state-of-the-art crossover operators reveals that PSOX\nconsistently delivers superior performance in terms of solution accuracy,\nalgorithmic stability, and convergence speed, especially when combined with an\nappropriate mutation strategy. Furthermore, the study provides an in-depth\ninvestigation of how different mutation rates influence PSOX's performance,\nyielding practical guidelines for parameter tuning when addressing optimization\nproblems with varying landscape properties.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aPSOX\u7684\u65b0\u578b\u4ea4\u53c9\u7b97\u5b50\uff0c\u7ed3\u5408\u5f53\u524d\u6700\u4f18\u548c\u5386\u53f2\u6700\u4f18\u4fe1\u606f\uff0c\u63d0\u5347\u9057\u4f20\u7b97\u6cd5\u5728\u591a\u6837\u6027\u4fdd\u6301\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u7684\u6027\u80fd\u3002\u572815\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u51fd\u6570\u4e0a\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edf\u4ea4\u53c9\u7b97\u5b50\u4ec5\u5728\u540c\u4ee3\u4e2a\u4f53\u95f4\u4ea4\u6362\u4fe1\u606f\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5386\u53f2\u6700\u4f18\u4fe1\u606f\u3002PSOX\u65e8\u5728\u7ed3\u5408\u5168\u5c40\u6700\u4f18\u548c\u5386\u53f2\u6700\u4f18\uff0c\u63d0\u5347\u7b97\u6cd5\u6027\u80fd\u3002", "method": "\u5f00\u53d1PSOX\u4ea4\u53c9\u7b97\u5b50\uff0c\u878d\u5408\u7c92\u5b50\u7fa4\u4f18\u5316\u601d\u60f3\uff0c\u5229\u7528\u5f53\u524d\u548c\u5386\u53f2\u6700\u4f18\u89e3\u6307\u5bfc\u641c\u7d22\uff0c\u5e76\u901a\u8fc715\u4e2a\u57fa\u51c6\u51fd\u6570\u5bf9\u6bd4\u5b9e\u9a8c\u9a8c\u8bc1\u6548\u679c\u3002", "result": "PSOX\u5728\u89e3\u8d28\u91cf\u3001\u7a33\u5b9a\u6027\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e5\u79cd\u5148\u8fdb\u4ea4\u53c9\u7b97\u5b50\uff0c\u5c24\u5176\u5728\u642d\u914d\u5408\u9002\u53d8\u5f02\u7b56\u7565\u65f6\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "PSOX\u4e3a\u5b9e\u6570\u7f16\u7801\u9057\u4f20\u7b97\u6cd5\u63d0\u4f9b\u9ad8\u6548\u4ea4\u53c9\u65b9\u6848\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u8d8a\u6027\uff0c\u5e76\u7ed9\u51fa\u53d8\u5f02\u7387\u8c03\u53c2\u7684\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2505.03146", "pdf": "https://arxiv.org/pdf/2505.03146", "abs": "https://arxiv.org/abs/2505.03146", "authors": ["Fei Han", "Pengming Guo", "Hao Chen", "Weikun Li", "Jingbo Ren", "Naijun Liu", "Ning Yang", "Dixia Fan"], "title": "Learn to Swim: Data-Driven LSTM Hydrodynamic Model for Quadruped Robot Gait Optimization", "categories": ["cs.RO", "cs.LG"], "comment": "This work has been accepted for publication in the IEEE International\n  Conference on Robotics and Automation (ICRA) 2025. The final version will be\n  available in IEEE Xplore (DOI to be assigned upon publication)", "summary": "This paper presents a Long Short-Term Memory network-based Fluid Experiment\nData-Driven model (FED-LSTM) for predicting unsteady, nonlinear hydrodynamic\nforces on the underwater quadruped robot we constructed. Trained on\nexperimental data from leg force and body drag tests conducted in both a\nrecirculating water tank and a towing tank, FED-LSTM outperforms traditional\nEmpirical Formulas (EF) commonly used for flow prediction over flat surfaces.\nThe model demonstrates superior accuracy and adaptability in capturing complex\nfluid dynamics, particularly in straight-line and turning-gait optimizations\nvia the NSGA-II algorithm. FED-LSTM reduces deflection errors during\nstraight-line swimming and improves turn times without increasing the turning\nradius. Hardware experiments further validate the model's precision and\nstability over EF. This approach provides a robust framework for enhancing the\nswimming performance of legged robots, laying the groundwork for future\nadvances in underwater robotic locomotion.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8eLSTM\u7f51\u7edc\u7684\u6d41\u4f53\u5b9e\u9a8c\u6570\u636e\u9a71\u52a8\u6a21\u578b\uff08FED-LSTM\uff09\uff0c\u7528\u4e8e\u9884\u6d4b\u6c34\u4e0b\u56db\u8db3\u673a\u5668\u4eba\u7684\u975e\u7a33\u6001\u975e\u7ebf\u6027\u6c34\u52a8\u529b\uff0c\u5e76\u5728\u5b9e\u9a8c\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u7ecf\u9a8c\u516c\u5f0f\u3002", "motivation": "\u4f20\u7edf\u7ecf\u9a8c\u516c\u5f0f\u96be\u4ee5\u51c6\u786e\u6355\u6349\u590d\u6742\u6d41\u4f53\u52a8\u529b\u5b66\uff0c\u5c24\u5176\u662f\u5728\u673a\u5668\u4eba\u76f4\u7ebf\u6e38\u52a8\u548c\u8f6c\u5411\u4f18\u5316\u4e2d\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6570\u636e\u8bad\u7ec3FED-LSTM\u6a21\u578b\uff0c\u7ed3\u5408NSGA-II\u7b97\u6cd5\u4f18\u5316\u76f4\u7ebf\u548c\u8f6c\u5411\u6b65\u6001\u3002", "result": "FED-LSTM\u5728\u76f4\u7ebf\u6e38\u52a8\u4e2d\u51cf\u5c11\u4e86\u504f\u5dee\uff0c\u5e76\u6539\u5584\u4e86\u8f6c\u5411\u65f6\u95f4\uff0c\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u7cbe\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u63d0\u5347\u6c34\u4e0b\u817f\u5f0f\u673a\u5668\u4eba\u7684\u6e38\u6cf3\u6027\u80fd\u63d0\u4f9b\u4e86\u53ef\u9760\u6846\u67b6\uff0c\u4e3a\u672a\u6765\u6c34\u4e0b\u673a\u5668\u4eba\u8fd0\u52a8\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.03242", "pdf": "https://arxiv.org/pdf/2505.03242", "abs": "https://arxiv.org/abs/2505.03242", "authors": ["Davide Talon", "Federico Girella", "Ziyue Liu", "Marco Cristani", "Yiming Wang"], "title": "Seeing the Abstract: Translating the Abstract Language for Vision Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to CVPR25. Project page:\n  https://davidetalon.github.io/fashionact-page/", "summary": "Natural language goes beyond dryly describing visual content. It contains\nrich abstract concepts to express feeling, creativity and properties that\ncannot be directly perceived. Yet, current research in Vision Language Models\n(VLMs) has not shed light on abstract-oriented language. Our research breaks\nnew ground by uncovering its wide presence and under-estimated value, with\nextensive analysis. Particularly, we focus our investigation on the fashion\ndomain, a highly-representative field with abstract expressions. By analyzing\nrecent large-scale multimodal fashion datasets, we find that abstract terms\nhave a dominant presence, rivaling the concrete ones, providing novel\ninformation, and being useful in the retrieval task. However, a critical\nchallenge emerges: current general-purpose or fashion-specific VLMs are\npre-trained with databases that lack sufficient abstract words in their text\ncorpora, thus hindering their ability to effectively represent\nabstract-oriented language. We propose a training-free and model-agnostic\nmethod, Abstract-to-Concrete Translator (ACT), to shift abstract\nrepresentations towards well-represented concrete ones in the VLM latent space,\nusing pre-trained models and existing multimodal databases. On the\ntext-to-image retrieval task, despite being training-free, ACT outperforms the\nfine-tuned VLMs in both same- and cross-dataset settings, exhibiting its\neffectiveness with a strong generalization capability. Moreover, the\nimprovement introduced by ACT is consistent with various VLMs, making it a\nplug-and-play solution.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e2d\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u62bd\u8c61\u5bfc\u5411\u8bed\u8a00\uff0c\u7279\u522b\u662f\u5728\u65f6\u5c1a\u9886\u57df\u3002\u7814\u7a76\u53d1\u73b0\u62bd\u8c61\u8bcd\u6c47\u5728\u8bb8\u591a\u65b9\u9762\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f46\u73b0\u6709VLMs\u56e0\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u7f3a\u4e4f\u8db3\u591f\u62bd\u8c61\u8bcd\u6c47\u800c\u8868\u73b0\u4e0d\u8db3\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5ACT\uff0c\u901a\u8fc7\u5c06\u62bd\u8c61\u8868\u793a\u6620\u5c04\u5230\u5df2\u5145\u5206\u8868\u793a\u7684\u5177\u8c61\u8bcd\u6c47\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u63ed\u793a\u5e76\u9a8c\u8bc1\u62bd\u8c61\u5bfc\u5411\u8bed\u8a00\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7f\u6cdb\u5b58\u5728\u53ca\u5176\u88ab\u4f4e\u4f30\u7684\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u65f6\u5c1a\u9886\u57df\u8fd9\u79cd\u5bcc\u542b\u62bd\u8c61\u8868\u8fbe\u7684\u573a\u666f\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u4e14\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\u2014\u2014\u62bd\u8c61\u5230\u5177\u8c61\u8f6c\u6362\u5668\uff08ACT\uff09\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u591a\u6a21\u6001\u6570\u636e\u5e93\uff0c\u5c06\u62bd\u8c61\u8bcd\u6c47\u6620\u5c04\u5230VLM\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5df2\u5145\u5206\u8868\u793a\u7684\u5177\u8c61\u8bcd\u6c47\u3002", "result": "\u5728\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\u4e2d\uff0cACT\u5728\u76f8\u540c\u6570\u636e\u96c6\u548c\u8de8\u6570\u636e\u96c6\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e\u5fae\u8c03\u7684VLMs\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u6539\u8fdb\u6548\u679c\u4e0e\u4e0d\u540cVLMs\u517c\u5bb9\u3002", "conclusion": "ACT\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u663e\u8457\u63d0\u5347VLMs\u5904\u7406\u62bd\u8c61\u8bed\u8a00\u7684\u80fd\u529b\uff0c\u5c24\u5176\u5728\u65f6\u5c1a\u9886\u57df\u7684\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.03159", "pdf": "https://arxiv.org/pdf/2505.03159", "abs": "https://arxiv.org/abs/2505.03159", "authors": ["Zaid Ghazal", "Ali Al-Bustami", "Khouloud Gaaloul", "Jaerock Kwon"], "title": "Systematic Evaluation of Initial States and Exploration-Exploitation Strategies in PID Auto-Tuning: A Framework-Driven Approach Applied on Mobile Robots", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "PID controllers are widely used in control systems because of their\nsimplicity and effectiveness. Although advanced optimization techniques such as\nBayesian Optimization and Differential Evolution have been applied to address\nthe challenges of automatic tuning of PID controllers, the influence of initial\nsystem states on convergence and the balance between exploration and\nexploitation remains underexplored. Moreover, experimenting the influence\ndirectly on real cyber-physical systems such as mobile robots is crucial for\nderiving realistic insights. In the present paper, a novel framework is\nintroduced to evaluate the impact of systematically varying these factors on\nthe PID auto-tuning processes that utilize Bayesian Optimization and\nDifferential Evolution. Testing was conducted on two distinct PID-controlled\nrobotic platforms, an omnidirectional robot and a differential drive mobile\nrobot, to assess the effects on convergence rate, settling time, rise time, and\novershoot percentage. As a result, the experimental outcomes yield evidence on\nthe effects of the systematic variations, thereby providing an empirical basis\nfor future research studies in the field.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86PID\u63a7\u5236\u5668\u81ea\u52a8\u8c03\u8c10\u8fc7\u7a0b\u4e2d\u521d\u59cb\u7cfb\u7edf\u72b6\u6001\u5bf9\u6536\u655b\u6027\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u63a2\u7d22\u4e0e\u5f00\u53d1\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u5728\u4e24\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u79fb\u52a8\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u63a2\u8ba8\u521d\u59cb\u7cfb\u7edf\u72b6\u6001\u548c\u63a2\u7d22\u4e0e\u5f00\u53d1\u7684\u5e73\u8861\u5bf9PID\u81ea\u52a8\u8c03\u8c10\u7684\u5f71\u54cd\uff0c\u4ee5\u4f18\u5316\u63a7\u5236\u6027\u80fd\u3002", "method": "\u5f15\u5165\u65b0\u6846\u67b6\u8bc4\u4f30\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u5dee\u5206\u8fdb\u5316\u5728PID\u81ea\u52a8\u8c03\u8c10\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u5728\u4e24\u79cd\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u63ed\u793a\u4e86\u7cfb\u7edf\u53d8\u5316\u5bf9\u6536\u655b\u901f\u5ea6\u3001\u7a33\u5b9a\u65f6\u95f4\u3001\u4e0a\u5347\u65f6\u95f4\u548c\u8d85\u8c03\u767e\u5206\u6bd4\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u4e3aPID\u63a7\u5236\u5668\u81ea\u52a8\u8c03\u8c10\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2505.03265", "pdf": "https://arxiv.org/pdf/2505.03265", "abs": "https://arxiv.org/abs/2505.03265", "authors": ["Abdelkarim El-Hajjami", "Camille Salinesi"], "title": "Synthline: A Product Line Approach for Synthetic Requirements Engineering Data Generation using Large Language Models", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "While modern Requirements Engineering (RE) heavily relies on natural language\nprocessing and Machine Learning (ML) techniques, their effectiveness is limited\nby the scarcity of high-quality datasets. This paper introduces Synthline, a\nProduct Line (PL) approach that leverages Large Language Models to\nsystematically generate synthetic RE data for classification-based use cases.\nThrough an empirical evaluation conducted in the context of using ML for the\nidentification of requirements specification defects, we investigated both the\ndiversity of the generated data and its utility for training downstream models.\nOur analysis reveals that while synthetic datasets exhibit less diversity than\nreal data, they are good enough to serve as viable training resources.\nMoreover, our evaluation shows that combining synthetic and real data leads to\nsubstantial performance improvements. Specifically, hybrid approaches achieve\nup to 85% improvement in precision and a 2x increase in recall compared to\nmodels trained exclusively on real data. These findings demonstrate the\npotential of PL-based synthetic data generation to address data scarcity in RE.\nWe make both our implementation and generated datasets publicly available to\nsupport reproducibility and advancement in the field.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Synthline\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5408\u6210\u9700\u6c42\u5de5\u7a0b\u6570\u636e\uff0c\u4ee5\u89e3\u51b3\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5408\u6210\u6570\u636e\u867d\u7136\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u4f46\u4e0e\u771f\u5b9e\u6570\u636e\u7ed3\u5408\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u7cbe\u786e\u5ea6\u63d0\u9ad885%\uff0c\u53ec\u56de\u7387\u7ffb\u500d\u3002", "motivation": "\u73b0\u4ee3\u9700\u6c42\u5de5\u7a0b\u4f9d\u8d56\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u4f46\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7a00\u7f3a\u9650\u5236\u4e86\u5176\u6548\u679c\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u751f\u6210\u5408\u6210\u6570\u636e\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faSynthline\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\u6027\u751f\u6210\u5408\u6210\u9700\u6c42\u5de5\u7a0b\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u751f\u6210\u6570\u636e\u7684\u591a\u6837\u6027\u548c\u5bf9\u4e0b\u6e38\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5408\u6210\u6570\u636e\u591a\u6837\u6027\u4e0d\u5982\u771f\u5b9e\u6570\u636e\uff0c\u4f46\u53ef\u4f5c\u4e3a\u6709\u6548\u8bad\u7ec3\u8d44\u6e90\u3002\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u7ed3\u5408\u4f7f\u6a21\u578b\u7cbe\u786e\u5ea6\u63d0\u534785%\uff0c\u53ec\u56de\u7387\u7ffb\u500d\u3002", "conclusion": "\u57fa\u4e8e\u4ea7\u54c1\u7ebf\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u80fd\u6709\u6548\u89e3\u51b3\u9700\u6c42\u5de5\u7a0b\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u7814\u7a76\u516c\u5f00\u4e86\u5b9e\u73b0\u548c\u751f\u6210\u6570\u636e\u96c6\u4ee5\u652f\u6301\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2505.03174", "pdf": "https://arxiv.org/pdf/2505.03174", "abs": "https://arxiv.org/abs/2505.03174", "authors": ["Guillermo Roque", "Erika Maquiling", "Jose Giovanni Tapia Lopez", "Ross Greer"], "title": "Automated Data Curation Using GPS & NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Instruction-Action (IA) data pairs are valuable for training robotic systems,\nespecially autonomous vehicles (AVs), but having humans manually annotate this\ndata is costly and time-inefficient. This paper explores the potential of using\nmobile application Global Positioning System (GPS) references and Natural\nLanguage Processing (NLP) to automatically generate large volumes of IA\ncommands and responses without having a human generate or retroactively tag the\ndata. In our pilot data collection, by driving to various destinations and\ncollecting voice instructions from GPS applications, we demonstrate a means to\ncollect and categorize the diverse sets of instructions, further accompanied by\nvideo data to form complete vision-language-action triads. We provide details\non our completely automated data collection prototype system, ADVLAT-Engine. We\ncharacterize collected GPS voice instructions into eight different\nclassifications, highlighting the breadth of commands and referentialities\navailable for curation from freely available mobile applications. Through\nresearch and exploration into the automation of IA data pairs using GPS\nreferences, the potential to increase the speed and volume at which\nhigh-quality IA datasets are created, while minimizing cost, can pave the way\nfor robust vision-language-action (VLA) models to serve tasks in\nvision-language navigation (VLN) and human-interactive autonomous systems.", "AI": {"tldr": "This paper explores using GPS and NLP to automatically generate instruction-action (IA) data pairs for training autonomous vehicles, reducing human annotation costs and improving efficiency.", "motivation": "Manual annotation of IA data for robotic systems is costly and inefficient, so the authors seek an automated solution using GPS and NLP.", "method": "Collect GPS voice instructions and video data to form vision-language-action triads, then categorize instructions into eight classes using their prototype ADVLAT-Engine.", "result": "Demonstrated successful automated collection and categorization of diverse IA pairs, highlighting scalability and cost reduction.", "conclusion": "Automated IA data generation via GPS/NLP has potential to accelerate high-quality dataset creation for vision-language-action models in autonomous systems."}}
{"id": "2505.03296", "pdf": "https://arxiv.org/pdf/2505.03296", "abs": "https://arxiv.org/abs/2505.03296", "authors": ["Jan Ole von Hartz", "Adrian R\u00f6fer", "Joschka Boedecker", "Abhinav Valada"], "title": "The Unreasonable Effectiveness of Discrete-Time Gaussian Process Mixtures for Robot Policy Learning", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Submitted for publication to IEEE Transaction on Robotics", "summary": "We present Mixture of Discrete-time Gaussian Processes (MiDiGap), a novel\napproach for flexible policy representation and imitation learning in robot\nmanipulation. MiDiGap enables learning from as few as five demonstrations using\nonly camera observations and generalizes across a wide range of challenging\ntasks. It excels at long-horizon behaviors such as making coffee, highly\nconstrained motions such as opening doors, dynamic actions such as scooping\nwith a spatula, and multimodal tasks such as hanging a mug. MiDiGap learns\nthese tasks on a CPU in less than a minute and scales linearly to large\ndatasets. We also develop a rich suite of tools for inference-time steering\nusing evidence such as collision signals and robot kinematic constraints. This\nsteering enables novel generalization capabilities, including obstacle\navoidance and cross-embodiment policy transfer. MiDiGap achieves\nstate-of-the-art performance on diverse few-shot manipulation benchmarks. On\nconstrained RLBench tasks, it improves policy success by 76 percentage points\nand reduces trajectory cost by 67%. On multimodal tasks, it improves policy\nsuccess by 48 percentage points and increases sample efficiency by a factor of\n20. In cross-embodiment transfer, it more than doubles policy success. We make\nthe code publicly available at https://midigap.cs.uni-freiburg.de.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMiDiGap\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7075\u6d3b\u7b56\u7565\u8868\u793a\u4e0e\u6a21\u4eff\u5b66\u4e60\uff0c\u80fd\u591f\u5728\u4ec5\u97005\u6b21\u6f14\u793a\u7684\u60c5\u51b5\u4e0b\u4ece\u76f8\u673a\u89c2\u6d4b\u4e2d\u5b66\u4e60\uff0c\u5e76\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u6837\u672c\u6548\u7387\u3001\u4efb\u52a1\u591a\u6837\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u6df7\u5408\u79bb\u6563\u65f6\u95f4\u9ad8\u65af\u8fc7\u7a0b\uff08MiDiGap\uff09\uff0c\u5229\u7528\u5c11\u91cf\u6f14\u793a\u548c\u76f8\u673a\u89c2\u6d4b\u8fdb\u884c\u5b66\u4e60\uff0c\u5e76\u5f00\u53d1\u4e86\u63a8\u7406\u65f6\u5f15\u5bfc\u5de5\u5177\u4ee5\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u5404\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5982RLBench\u4efb\u52a1\u6210\u529f\u7387\u63d0\u534776%\uff0c\u8f68\u8ff9\u6210\u672c\u964d\u4f4e67%\uff0c\u591a\u6a21\u6001\u4efb\u52a1\u6210\u529f\u7387\u63d0\u534748%\uff0c\u6837\u672c\u6548\u7387\u63d0\u9ad820\u500d\u3002", "conclusion": "MiDiGap\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.03177", "pdf": "https://arxiv.org/pdf/2505.03177", "abs": "https://arxiv.org/abs/2505.03177", "authors": ["Keilung Choy", "Wei Xie", "Keqi Wang"], "title": "A Symbolic and Statistical Learning Framework to Discover Bioprocessing Regulatory Mechanism: Cell Culture Example", "categories": ["stat.ML", "cs.LG", "q-bio.QM"], "comment": "11 pages, 2 figures", "summary": "Bioprocess mechanistic modeling is essential for advancing intelligent\ndigital twin representation of biomanufacturing, yet challenges persist due to\ncomplex intracellular regulation, stochastic system behavior, and limited\nexperimental data. This paper introduces a symbolic and statistical learning\nframework to identify key regulatory mechanisms and quantify model uncertainty.\nBioprocess dynamics is formulated with stochastic differential equations\ncharacterizing intrinsic process variability, with a predefined set of\ncandidate regulatory mechanisms constructed from biological knowledge. A\nBayesian learning approach is developed, which is based on a joint learning of\nkinetic parameters and regulatory structure through a formulation of the\nmixture model. To enhance computational efficiency, a Metropolis-adjusted\nLangevin algorithm with adjoint sensitivity analysis is developed for posterior\nexploration. Compared to state-of-the-art Bayesian inference approaches, the\nproposed framework achieves improved sample efficiency and robust model\nselection. An empirical study demonstrates its ability to recover missing\nregulatory mechanisms and improve model fidelity under data-limited conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7b26\u53f7\u4e0e\u7edf\u8ba1\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u751f\u7269\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u8c03\u63a7\u673a\u5236\u5e76\u91cf\u5316\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u5b66\u4e60\u548c\u9ad8\u6548\u7b97\u6cd5\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u4e0e\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u751f\u7269\u8fc7\u7a0b\u673a\u5236\u5efa\u6a21\u5bf9\u751f\u7269\u5236\u9020\u7684\u667a\u80fd\u6570\u5b57\u5b6a\u751f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u590d\u6742\u7684\u7ec6\u80de\u5185\u8c03\u63a7\u3001\u968f\u673a\u7cfb\u7edf\u884c\u4e3a\u548c\u5b9e\u9a8c\u6570\u636e\u6709\u9650\uff0c\u4ecd\u5b58\u5728\u6311\u6218\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u751f\u7269\u77e5\u8bc6\u7684\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u548c\u5019\u9009\u8c03\u63a7\u673a\u5236\u96c6\u5408\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u6df7\u5408\u6a21\u578b\u8054\u5408\u5b66\u4e60\u52a8\u529b\u5b66\u53c2\u6570\u548c\u8c03\u63a7\u7ed3\u6784\uff0c\u5e76\u5f00\u53d1\u4e86Metropolis-adjusted Langevin\u7b97\u6cd5\u548c\u4f34\u968f\u654f\u611f\u6027\u5206\u6790\u4ee5\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u4e0e\u5148\u8fdb\u7684\u8d1d\u53f6\u65af\u63a8\u65ad\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u5728\u6837\u672c\u6548\u7387\u548c\u6a21\u578b\u9009\u62e9\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u5b9e\u8bc1\u7814\u7a76\u663e\u793a\u5176\u80fd\u6062\u590d\u7f3a\u5931\u8c03\u63a7\u673a\u5236\u5e76\u5728\u6570\u636e\u6709\u9650\u6761\u4ef6\u4e0b\u63d0\u9ad8\u6a21\u578b\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6570\u636e\u7a00\u7f3a\u73af\u5883\u4e0b\u751f\u7269\u8fc7\u7a0b\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u6570\u5b57\u5b6a\u751f\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.03299", "pdf": "https://arxiv.org/pdf/2505.03299", "abs": "https://arxiv.org/abs/2505.03299", "authors": ["Pierre Adorni", "Minh-Tan Pham", "St\u00e9phane May", "S\u00e9bastien Lef\u00e8vre"], "title": "Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at the MORSE workshop of CVPR 2025", "summary": "Foundation models constitute a significant advancement in computer vision:\nafter a single, albeit costly, training phase, they can address a wide array of\ntasks. In the field of Earth observation, over 75 remote sensing vision\nfoundation models have been developed in the past four years. However, none has\nconsistently outperformed the others across all available downstream tasks. To\nfacilitate their comparison, we propose a cost-effective method for predicting\na model's performance on multiple downstream tasks without the need for\nfine-tuning on each one. This method is based on what we call \"capabilities\nencoding.\" The utility of this novel approach is twofold: we demonstrate its\npotential to simplify the selection of a foundation model for a given new task,\nand we employ it to offer a fresh perspective on the existing literature,\nsuggesting avenues for future research. Codes are available at\nhttps://github.com/pierreadorni/capabilities-encoding.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u2018\u80fd\u529b\u7f16\u7801\u2019\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u57fa\u7840\u6a21\u578b\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u65e0\u9700\u5fae\u8c03\uff0c\u4ee5\u7b80\u5316\u6a21\u578b\u9009\u62e9\u5e76\u63d0\u4f9b\u73b0\u6709\u6587\u732e\u7684\u65b0\u89c6\u89d2\u3002", "motivation": "\u5728\u9065\u611f\u9886\u57df\uff0c\u5df2\u5f00\u53d1\u4e86\u8d85\u8fc775\u79cd\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u4f46\u6ca1\u6709\u4e00\u79cd\u5728\u6240\u6709\u4e0b\u6e38\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002\u4e3a\u4e86\u7b80\u5316\u6a21\u578b\u9009\u62e9\u5e76\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u8fd9\u4e00\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u2018\u80fd\u529b\u7f16\u7801\u2019\u8fd9\u4e00\u6210\u672c\u6548\u76ca\u9ad8\u7684\u65b9\u6cd5\uff0c\u9884\u6d4b\u57fa\u7840\u6a21\u578b\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u65e0\u9700\u9010\u4e00\u5fae\u8c03\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u7b80\u5316\u57fa\u7840\u6a21\u578b\u7684\u9009\u62e9\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u9065\u611f\u9886\u57df\u7684\u57fa\u7840\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.03201", "pdf": "https://arxiv.org/pdf/2505.03201", "abs": "https://arxiv.org/abs/2505.03201", "authors": ["Kien Tran Duc Tuan", "Tam Nguyen Trong", "Son Nguyen Hoang", "Khoat Than", "Anh Nguyen Duc"], "title": "Weighted Average Gradients for Feature Attribution", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "In explainable AI, Integrated Gradients (IG) is a widely adopted technique\nfor assessing the significance of feature attributes of the input on model\noutputs by evaluating contributions from a baseline input to the current input.\nThe choice of the baseline input significantly influences the resulting\nexplanation. While the traditional Expected Gradients (EG) method assumes\nbaselines can be uniformly sampled and averaged with equal weights, this study\nargues that baselines should not be treated equivalently. We introduce Weighted\nAverage Gradients (WG), a novel approach that unsupervisedly evaluates baseline\nsuitability and incorporates a strategy for selecting effective baselines.\nTheoretical analysis demonstrates that WG satisfies essential explanation\nmethod criteria and offers greater stability than prior approaches.\nExperimental results further confirm that WG outperforms EG across diverse\nscenarios, achieving an improvement of 10-35\\% on main metrics. Moreover, by\nevaluating baselines, our method can filter a subset of effective baselines for\neach input to calculate explanations, maintaining high accuracy while reducing\ncomputational cost. The code is available at:\nhttps://github.com/Tamnt240904/weighted_baseline.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Weighted Average Gradients (WG)\u65b9\u6cd5\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u8bc4\u4f30\u57fa\u7ebf\u9002\u7528\u6027\u5e76\u9009\u62e9\u6709\u6548\u57fa\u7ebf\uff0c\u4f18\u4e8e\u4f20\u7edfExpected Gradients (EG)\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u534710-35%\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5728\u53ef\u89e3\u91caAI\u4e2d\uff0c\u57fa\u7ebf\u7684\u9009\u62e9\u5bf9Integrated Gradients (IG)\u6280\u672f\u7684\u89e3\u91ca\u7ed3\u679c\u5f71\u54cd\u5f88\u5927\u3002\u4f20\u7edfEG\u65b9\u6cd5\u5047\u8bbe\u57fa\u7ebf\u53ef\u5747\u5300\u91c7\u6837\u4e14\u6743\u91cd\u76f8\u540c\uff0c\u4f46\u5b9e\u9645\u57fa\u7ebf\u4e0d\u5e94\u7b49\u540c\u5bf9\u5f85\u3002", "method": "\u63d0\u51faWG\u65b9\u6cd5\uff0c\u8be5\u6280\u672f\u65e0\u76d1\u7763\u8bc4\u4f30\u57fa\u7ebf\u9002\u7528\u6027\u5e76\u9009\u62e9\u6709\u6548\u57fa\u7ebf\uff0c\u901a\u8fc7\u52a0\u6743\u5e73\u5747\u8ba1\u7b97\u8d21\u732e\uff0c\u6ee1\u8db3\u89e3\u91ca\u65b9\u6cd5\u7684\u7406\u8bba\u8981\u6c42\u4e14\u7a33\u5b9a\u6027\u66f4\u9ad8\u3002", "result": "\u5b9e\u9a8c\u8868\u660eWG\u5728\u591a\u79cd\u573a\u666f\u4e0b\u4f18\u4e8eEG\uff0c\u6027\u80fd\u63d0\u534710-35%\uff0c\u4e14\u80fd\u901a\u8fc7\u7b5b\u9009\u6709\u6548\u57fa\u7ebf\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\uff0c\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "WG\u662f\u4e00\u79cd\u66f4\u4f18\u7684\u57fa\u7ebf\u9009\u62e9\u548c\u52a0\u6743\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86IG\u7684\u89e3\u91ca\u6548\u679c\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.03303", "pdf": "https://arxiv.org/pdf/2505.03303", "abs": "https://arxiv.org/abs/2505.03303", "authors": ["Tasnim Shahriar"], "title": "Comparative Analysis of Lightweight Deep Learning Models for Memory-Constrained Devices", "categories": ["cs.CV", "cs.AI", "68-XX (Primary) 68Txx, 68T07 (Secondary)"], "comment": "22 pages, 10 figures, 4 tables, submitted to Springer - Pattern\n  Recognition and Image Analysis", "summary": "This paper presents a comprehensive evaluation of lightweight deep learning\nmodels for image classification, emphasizing their suitability for deployment\nin resource-constrained environments such as low-memory devices. Five\nstate-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,\nEfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse\ndatasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using\nfour key performance metrics: classification accuracy, inference time,\nfloating-point operations (FLOPs), and model size. Additionally, we investigate\nthe impact of hyperparameter tuning, data augmentation, and training paradigms\nby comparing pretrained models with scratch-trained counterparts, focusing on\nMobileNetV3 Small. Our findings reveal that transfer learning significantly\nenhances model accuracy and computational efficiency, particularly for complex\ndatasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest\naccuracy, while MobileNetV3 offers the best balance between accuracy and\nefficiency, and SqueezeNet excels in inference speed and compactness. This\nstudy highlights critical trade-offs between accuracy and efficiency, offering\nactionable insights for deploying lightweight models in real-world applications\nwhere computational resources are limited. By addressing these challenges, this\nresearch contributes to optimizing deep learning systems for edge computing and\nmobile platforms.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e94\u79cd\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08MobileNetV3 Small\u3001ResNet18\u3001SqueezeNet\u3001EfficientNetV2-S\u548cShuffleNetV2\uff09\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\uff0c\u53d1\u73b0EfficientNetV2\u7cbe\u5ea6\u6700\u9ad8\uff0cMobileNetV3\u5e73\u8861\u6027\u6700\u4f73\uff0cSqueezeNet\u901f\u5ea6\u6700\u5feb\u3002", "motivation": "\u63a2\u8ba8\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\uff08\u5982\u4f4e\u5185\u5b58\u8bbe\u5907\uff09\u4e2d\u7684\u9002\u7528\u6027\uff0c\u4e3a\u8fb9\u7f18\u8ba1\u7b97\u548c\u79fb\u52a8\u5e73\u53f0\u4f18\u5316\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u5728CIFAR-10\u3001CIFAR-100\u548cTiny ImageNet\u6570\u636e\u96c6\u4e0a\uff0c\u8bc4\u4f30\u4e94\u79cd\u6a21\u578b\u7684\u5206\u7c7b\u7cbe\u5ea6\u3001\u63a8\u7406\u65f6\u95f4\u3001FLOPs\u548c\u6a21\u578b\u5927\u5c0f\uff0c\u5e76\u5bf9\u6bd4\u9884\u8bad\u7ec3\u4e0e\u4ece\u5934\u8bad\u7ec3\u7684MobileNetV3 Small\u3002", "result": "\u8fc1\u79fb\u5b66\u4e60\u663e\u8457\u63d0\u5347\u6a21\u578b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u5c24\u5176\u5bf9\u590d\u6742\u6570\u636e\u96c6\uff1bEfficientNetV2\u7cbe\u5ea6\u6700\u9ad8\uff0cMobileNetV3\u5e73\u8861\u6027\u6700\u4f73\uff0cSqueezeNet\u901f\u5ea6\u548c\u7d27\u51d1\u6027\u6700\u4f18\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u7cbe\u5ea6\u4e0e\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2505.03223", "pdf": "https://arxiv.org/pdf/2505.03223", "abs": "https://arxiv.org/abs/2505.03223", "authors": ["Spencer Compton", "Chirag Pabbaraju", "Nikita Zhivotovskiy"], "title": "Lower Bounds for Greedy Teaching Set Constructions", "categories": ["stat.ML", "cs.DS", "cs.LG", "math.CO"], "comment": null, "summary": "A fundamental open problem in learning theory is to characterize the\nbest-case teaching dimension $\\operatorname{TS}_{\\min}$ of a concept class\n$\\mathcal{C}$ with finite VC dimension $d$. Resolving this problem will, in\nparticular, settle the conjectured upper bound on Recursive Teaching Dimension\nposed by [Simon and Zilles; COLT 2015]. Prior work used a natural greedy\nalgorithm to construct teaching sets recursively, thereby proving upper bounds\non $\\operatorname{TS}_{\\min}$, with the best known bound being $O(d^2)$ [Hu,\nWu, Li, and Wang; COLT 2017]. In each iteration, this greedy algorithm chooses\nto add to the teaching set the $k$ labeled points that restrict the concept\nclass the most. In this work, we prove lower bounds on the performance of this\ngreedy approach for small $k$. Specifically, we show that for $k = 1$, the\nalgorithm does not improve upon the halving-based bound of\n$O(\\log(|\\mathcal{C}|))$. Furthermore, for $k = 2$, we complement the upper\nbound of $O\\left(\\log(\\log(|\\mathcal{C}|))\\right)$ from [Moran, Shpilka,\nWigderson, and Yuhudayoff; FOCS 2015] with a matching lower bound. Most\nconsequentially, our lower bound extends up to $k \\le \\lceil c d \\rceil$ for\nsmall constant $c>0$: suggesting that studying higher-order interactions may be\nnecessary to resolve the conjecture that $\\operatorname{TS}_{\\min} = O(d)$.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u6709\u9650VC\u7ef4\u5ea6\u7684\u6982\u5ff5\u7c7b\u4e2d\uff0c\u8d2a\u5a6a\u7b97\u6cd5\u5728\u6784\u9020\u6559\u5b66\u96c6\u65f6\u7684\u6027\u80fd\u4e0b\u754c\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u4e8e\u5c0f\u7684k\u503c\uff08\u5982k=1\u548ck=2\uff09\uff0c\u7b97\u6cd5\u6027\u80fd\u6709\u9650\u4e14\u5b58\u5728\u5339\u914d\u4e0b\u754c\uff0c\u6697\u793a\u53ef\u80fd\u9700\u8981\u7814\u7a76\u9ad8\u9636\u4ea4\u4e92\u624d\u80fd\u89e3\u51b3TS_min=O(d)\u7684\u731c\u60f3\u3002", "motivation": "\u5b66\u4e60\u7406\u8bba\u4e2d\u7684\u4e00\u4e2a\u6838\u5fc3\u95ee\u9898\u662f\u786e\u5b9a\u6709\u9650VC\u7ef4\u5ea6d\u7684\u6982\u5ff5\u7c7bC\u7684\u6700\u4f73\u6559\u5b66\u7ef4\u5ea6TS_min\u3002\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u5c06\u6709\u52a9\u4e8e\u9a8c\u8bc1Simon\u548cZilles\u57282015\u5e74\u63d0\u51fa\u7684\u5173\u4e8e\u9012\u5f52\u6559\u5b66\u7ef4\u5ea6\u7684\u731c\u60f3\u3002\u6b64\u524d\u7684\u7814\u7a76\u901a\u8fc7\u8d2a\u5a6a\u7b97\u6cd5\u8bc1\u660e\u4e86TS_min\u7684\u4e0a\u754c\uff0c\u4f46\u5bf9\u5176\u6027\u80fd\u7684\u4e0b\u754c\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5206\u6790\u8d2a\u5a6a\u7b97\u6cd5\u5728\u5c0fk\u503c\uff08\u5982k=1\u548ck=2\uff09\u4e0b\u7684\u6027\u80fd\uff0c\u6784\u9020\u4e86\u5bf9\u5e94\u7684\u4e0b\u754c\u3002\u7279\u522b\u5730\uff0c\u5bf9\u4e8ek=1\uff0c\u7b97\u6cd5\u65e0\u6cd5\u4f18\u4e8e\u57fa\u4e8e\u5bf9\u5206\u6cd5\u7684O(log|C|)\u8fb9\u754c\uff1b\u5bf9\u4e8ek=2\uff0c\u7ed9\u51fa\u4e86\u4e0e\u5df2\u77e5\u4e0a\u754cO(log log|C|)\u5339\u914d\u7684\u4e0b\u754c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u5bf9\u4e8ek\u2264\u2308cd\u2309\uff08c\u4e3a\u5c0f\u5e38\u6570\uff09\uff0c\u8d2a\u5a6a\u7b97\u6cd5\u7684\u6027\u80fd\u5b58\u5728\u663e\u8457\u4e0b\u754c\uff0c\u8868\u660e\u73b0\u6709\u65b9\u6cd5\u53ef\u80fd\u4e0d\u8db3\u4ee5\u8fbe\u5230TS_min=O(d)\u7684\u76ee\u6807\uff0c\u9700\u8981\u66f4\u9ad8\u9636\u7684\u4ea4\u4e92\u5206\u6790\u3002", "conclusion": "\u672c\u6587\u63ed\u793a\u4e86\u8d2a\u5a6a\u7b97\u6cd5\u5728\u5c0fk\u503c\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u6307\u51fa\u89e3\u51b3TS_min=O(d)\u731c\u60f3\u53ef\u80fd\u9700\u8981\u66f4\u590d\u6742\u7684\u9ad8\u9636\u4ea4\u4e92\u7814\u7a76\u3002"}}
{"id": "2505.03314", "pdf": "https://arxiv.org/pdf/2505.03314", "abs": "https://arxiv.org/abs/2505.03314", "authors": ["Jincheng Zhang", "Gy\u00f6rgy Fazekas", "Charalampos Saitis"], "title": "Mamba-Diffusion Model with Learnable Wavelet for Controllable Symbolic Music Generation", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "The recent surge in the popularity of diffusion models for image synthesis\nhas attracted new attention to their potential for generation tasks in other\ndomains. However, their applications to symbolic music generation remain\nlargely under-explored because symbolic music is typically represented as\nsequences of discrete events and standard diffusion models are not well-suited\nfor discrete data. We represent symbolic music as image-like pianorolls,\nfacilitating the use of diffusion models for the generation of symbolic music.\nMoreover, this study introduces a novel diffusion model that incorporates our\nproposed Transformer-Mamba block and learnable wavelet transform.\nClassifier-free guidance is utilised to generate symbolic music with target\nchords. Our evaluation shows that our method achieves compelling results in\nterms of music quality and controllability, outperforming the strong baseline\nin pianoroll generation. Our code is available at\nhttps://github.com/jinchengzhanggg/proffusion.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u7b26\u53f7\u97f3\u4e50\u8868\u793a\u4e3a\u56fe\u50cf\u7c7b\u94a2\u7434\u5377\u5e18\u7684\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u7ed3\u5408Transformer-Mamba\u5757\u548c\u53ef\u5b66\u4e60\u5c0f\u6ce2\u53d8\u6362\u7684\u65b0\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u53ef\u63a7\u7684\u7b26\u53f7\u97f3\u4e50\uff0c\u6548\u679c\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u5408\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u7b26\u53f7\u97f3\u4e50\u751f\u6210\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u4e3b\u8981\u539f\u56e0\u5728\u4e8e\u7b26\u53f7\u97f3\u4e50\u7684\u79bb\u6563\u6027\u4f7f\u5f97\u6807\u51c6\u6269\u6563\u6a21\u578b\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u3002", "method": "\u5c06\u7b26\u53f7\u97f3\u4e50\u8868\u793a\u4e3a\u94a2\u7434\u5377\u5e18\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542bTransformer-Mamba\u5757\u548c\u53ef\u5b66\u4e60\u5c0f\u6ce2\u53d8\u6362\u7684\u6269\u6563\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u65e0\u5206\u7c7b\u5668\u6307\u5bfc\u751f\u6210\u5177\u6709\u76ee\u6807\u548c\u5f26\u7684\u97f3\u4e50\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u97f3\u4e50\u8d28\u91cf\u548c\u53ef\u63a7\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u94a2\u7434\u5377\u5e18\u751f\u6210\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u6269\u6563\u6a21\u578b\u5728\u7b26\u53f7\u97f3\u4e50\u751f\u6210\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u514b\u670d\u79bb\u6563\u6570\u636e\u5e26\u6765\u7684\u6311\u6218\u3002"}}
{"id": "2505.03254", "pdf": "https://arxiv.org/pdf/2505.03254", "abs": "https://arxiv.org/abs/2505.03254", "authors": ["Lukas Meiner", "Jens Mehnert", "Alexandru Paul Condurache"], "title": "PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Convolutional neural networks (CNNs) are crucial for computer vision tasks on\nresource-constrained devices. Quantization effectively compresses these models,\nreducing storage size and energy cost. However, in modern depthwise-separable\narchitectures, the computational cost is distributed unevenly across its\ncomponents, with pointwise operations being the most expensive. By applying a\ngeneral quantization scheme to this imbalanced cost distribution, existing\nquantization approaches fail to fully exploit potential efficiency gains. To\nthis end, we introduce PROM, a straightforward approach for quantizing modern\ndepthwise-separable convolutional networks by selectively using two distinct\nbit-widths. Specifically, pointwise convolutions are quantized to ternary\nweights, while the remaining modules use 8-bit weights, which is achieved\nthrough a simple quantization-aware training procedure. Additionally, by\nquantizing activations to 8-bit, our method transforms pointwise convolutions\nwith ternary weights into int8 additions, which enjoy broad support across\nhardware platforms and effectively eliminates the need for expensive\nmultiplications. Applying PROM to MobileNetV2 reduces the model's energy cost\nby more than an order of magnitude (23.9x) and its storage size by 2.7x\ncompared to the float16 baseline while retaining similar classification\nperformance on ImageNet. Our method advances the Pareto frontier for energy\nconsumption vs. top-1 accuracy for quantized convolutional models on ImageNet.\nPROM addresses the challenges of quantizing depthwise-separable convolutional\nnetworks to both ternary and 8-bit weights, offering a simple way to reduce\nenergy cost and storage size.", "AI": {"tldr": "PROM\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u7f51\u7edc\u7684\u9ad8\u6548\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u4f7f\u7528\u4e09\u5143\u548c8-bit\u6743\u91cd\uff0c\u964d\u4f4e\u80fd\u91cf\u6d88\u8017\u548c\u5b58\u50a8\u5927\u5c0f\u3002", "motivation": "\u9488\u5bf9\u73b0\u4ee3\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u7f51\u7edc\u4e2d\u8ba1\u7b97\u6210\u672c\u5206\u5e03\u4e0d\u5747\u7684\u95ee\u9898\uff0c\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u5229\u7528\u6548\u7387\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u53cc\u4f4d\u5bbd\u91cf\u5316\u7b56\u7565\uff0c\u5bf9\u70b9\u5377\u79ef\u4f7f\u7528\u4e09\u5143\u6743\u91cd\uff0c\u5176\u4ed6\u6a21\u5757\u4f7f\u75288-bit\u6743\u91cd\uff0c\u5e76\u901a\u8fc7\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u5b9e\u73b0\u3002", "result": "\u5728MobileNetV2\u4e0a\uff0c\u80fd\u91cf\u6d88\u8017\u964d\u4f4e23.9\u500d\uff0c\u5b58\u50a8\u5927\u5c0f\u51cf\u5c112.7\u500d\uff0c\u540c\u65f6\u4fdd\u6301ImageNet\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "PROM\u4e3a\u91cf\u5316\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u7f51\u7edc\u63d0\u4f9b\u4e86\u7b80\u5355\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u5316\u4e86\u80fd\u91cf\u4e0e\u7cbe\u5ea6\u7684\u6743\u8861\u3002"}}
{"id": "2505.03319", "pdf": "https://arxiv.org/pdf/2505.03319", "abs": "https://arxiv.org/abs/2505.03319", "authors": ["Manolis Mylonas", "Evlampios Apostolidis", "Vasileios Mezaris"], "title": "SD-VSum: A Method and Dataset for Script-Driven Video Summarization", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Under review", "summary": "In this work, we introduce the task of script-driven video summarization,\nwhich aims to produce a summary of the full-length video by selecting the parts\nthat are most relevant to a user-provided script outlining the visual content\nof the desired summary. Following, we extend a recently-introduced large-scale\ndataset for generic video summarization (VideoXum) by producing natural\nlanguage descriptions of the different human-annotated summaries that are\navailable per video. In this way we make it compatible with the introduced\ntask, since the available triplets of ``video, summary and summary\ndescription'' can be used for training a method that is able to produce\ndifferent summaries for a given video, driven by the provided script about the\ncontent of each summary. Finally, we develop a new network architecture for\nscript-driven video summarization (SD-VSum), that relies on the use of a\ncross-modal attention mechanism for aligning and fusing information from the\nvisual and text modalities. Our experimental evaluations demonstrate the\nadvanced performance of SD-VSum against state-of-the-art approaches for\nquery-driven and generic (unimodal and multimodal) summarization from the\nliterature, and document its capacity to produce video summaries that are\nadapted to each user's needs about their content.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u811a\u672c\u9a71\u52a8\u7684\u89c6\u9891\u6458\u8981\u4efb\u52a1\uff0c\u901a\u8fc7\u7528\u6237\u63d0\u4f9b\u7684\u811a\u672c\u9009\u62e9\u6700\u76f8\u5173\u7247\u6bb5\u751f\u6210\u6458\u8981\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u6570\u636e\u96c6\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u7f51\u7edc\u67b6\u6784SD-VSum\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u89c6\u9891\u6458\u8981\u65b9\u6cd5\u65e0\u6cd5\u6839\u636e\u7528\u6237\u9700\u6c42\u7075\u6d3b\u8c03\u6574\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u811a\u672c\u9a71\u52a8\u7684\u89c6\u9891\u6458\u8981\u4efb\u52a1\uff0c\u65e8\u5728\u901a\u8fc7\u7528\u6237\u63d0\u4f9b\u7684\u811a\u672c\u751f\u6210\u5b9a\u5236\u5316\u7684\u89c6\u9891\u6458\u8981\u3002", "method": "\u672c\u6587\u6269\u5c55\u4e86VideoXum\u6570\u636e\u96c6\uff0c\u589e\u52a0\u6458\u8981\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u5e76\u63d0\u51fa\u4e86SD-VSum\u7f51\u7edc\u67b6\u6784\uff0c\u5229\u7528\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u9f50\u548c\u878d\u5408\u89c6\u89c9\u4e0e\u6587\u672c\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSD-VSum\u5728\u811a\u672c\u9a71\u52a8\u7684\u89c6\u9891\u6458\u8981\u4efb\u52a1\u4e2d\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7684\u67e5\u8be2\u9a71\u52a8\u548c\u901a\u7528\u6458\u8981\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u7b26\u5408\u7528\u6237\u9700\u6c42\u7684\u5185\u5bb9\u3002", "conclusion": "SD-VSum\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u878d\u5408\u89c6\u89c9\u4e0e\u6587\u672c\u4fe1\u606f\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u6458\u8981\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.03327", "pdf": "https://arxiv.org/pdf/2505.03327", "abs": "https://arxiv.org/abs/2505.03327", "authors": ["Jos\u00e9-Luis Bueso-Bello", "Benjamin Chauvel", "Daniel Carcereri", "Philipp Posovszky", "Pietro Milillo", "Jennifer Ruiz", "Juan-Carlos Fern\u00e1ndez-Diaz", "Carolina Gonz\u00e1lez", "Michele Martone", "Ronny H\u00e4nsch", "Paola Rizzoli"], "title": "Very High-Resolution Forest Mapping with TanDEM-X InSAR Data and Self-Supervised Learning", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": "Preprint submitted to Remote Sensing of Environment", "summary": "Deep learning models have shown encouraging capabilities for mapping\naccurately forests at medium resolution with TanDEM-X interferometric SAR data.\nSuch models, as most of current state-of-the-art deep learning techniques in\nremote sensing, are trained in a fully-supervised way, which requires a large\namount of labeled data for training and validation. In this work, our aim is to\nexploit the high-resolution capabilities of the TanDEM-X mission to map forests\nat 6 m. The goal is to overcome the intrinsic limitations posed by\nmidresolution products, which affect, e.g., the detection of narrow roads\nwithin vegetated areas and the precise delineation of forested regions\ncontours. To cope with the lack of extended reliable reference datasets at such\na high resolution, we investigate self-supervised learning techniques for\nextracting highly informative representations from the input features, followed\nby a supervised training step with a significantly smaller number of reliable\nlabels. A 1 m resolution forest/non-forest reference map over Pennsylvania,\nUSA, allows for comparing different training approaches for the development of\nan effective forest mapping framework with limited labeled samples. We select\nthe best-performing approach over this test region and apply it in a real-case\nforest mapping scenario over the Amazon rainforest, where only very few labeled\ndata at high resolution are available. In this challenging scenario, the\nproposed self-supervised framework significantly enhances the classification\naccuracy with respect to fully-supervised methods, trained using the same\namount of labeled data, representing an extremely promising starting point for\nlarge-scale, very high-resolution forest mapping with TanDEM-X data.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u76d1\u7763\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u5229\u7528TanDEM-X\u5e72\u6d89SAR\u6570\u636e\u8fdb\u884c\u9ad8\u5206\u8fa8\u7387\uff086\u7c73\uff09\u68ee\u6797\u5236\u56fe\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u7f3a\u4e4f\u5927\u91cf\u6807\u6ce8\u6570\u636e\u65f6\u7684\u9650\u5236\uff0c\u5e76\u5728\u4e9a\u9a6c\u900a\u96e8\u6797\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u4e2d\u7b49\u5206\u8fa8\u7387\u68ee\u6797\u5236\u56fe\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u3002\u9ad8\u5206\u8fa8\u7387\u5236\u56fe\uff08\u59826\u7c73\uff09\u9762\u4e34\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u68c0\u6d4b\u72ed\u7a84\u9053\u8def\u548c\u7cbe\u786e\u5212\u5b9a\u68ee\u6797\u8fb9\u754c\u65f6\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u65e8\u5728\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u4ece\u8f93\u5165\u7279\u5f81\u4e2d\u63d0\u53d6\u9ad8\u4fe1\u606f\u91cf\u8868\u793a\uff0c\u7136\u540e\u4f7f\u7528\u5c11\u91cf\u53ef\u9760\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3\u3002\u901a\u8fc7\u7f8e\u56fd\u5bbe\u5915\u6cd5\u5c3c\u4e9a\u5dde\u76841\u7c73\u5206\u8fa8\u7387\u68ee\u6797/\u975e\u68ee\u6797\u53c2\u8003\u5730\u56fe\u9a8c\u8bc1\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6700\u7ec8\u5728\u4e9a\u9a6c\u900a\u96e8\u6797\u7684\u5b9e\u9645\u573a\u666f\u4e2d\u5e94\u7528\u6700\u4f73\u65b9\u6cd5\u3002", "result": "\u5728\u6807\u6ce8\u6570\u636e\u6781\u5c11\u7684\u4e9a\u9a6c\u900a\u96e8\u6797\u573a\u666f\u4e2d\uff0c\u63d0\u51fa\u7684\u81ea\u76d1\u7763\u6846\u67b6\u76f8\u6bd4\u5168\u76d1\u7763\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u7cbe\u5ea6\uff0c\u4e3a\u5927\u89c4\u6a21\u9ad8\u5206\u8fa8\u7387\u68ee\u6797\u5236\u56fe\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "conclusion": "\u81ea\u76d1\u7763\u5b66\u4e60\u80fd\u6709\u6548\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5728\u9ad8\u5206\u8fa8\u7387\u68ee\u6797\u5236\u56fe\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u5168\u76d1\u7763\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.03308", "pdf": "https://arxiv.org/pdf/2505.03308", "abs": "https://arxiv.org/abs/2505.03308", "authors": ["C\u00f4me Annicchiarico", "Fabien Lotte", "J\u00e9r\u00e9mie Mattout"], "title": "An Active Inference perspective on Neurofeedback Training", "categories": ["q-bio.NC", "cs.LG"], "comment": "Preprint, 43 pages, 14 figures", "summary": "Neurofeedback training (NFT) aims to teach self-regulation of brain activity\nthrough real-time feedback, but suffers from highly variable outcomes and\npoorly understood mechanisms, hampering its validation. To address these\nissues, we propose a formal computational model of the NFT closed loop. Using\nActive Inference, a Bayesian framework modelling perception, action, and\nlearning, we simulate agents interacting with an NFT environment. This enables\nus to test the impact of design choices (e.g., feedback quality, biomarker\nvalidity) and subject factors (e.g., prior beliefs) on training. Simulations\nshow that training effectiveness is sensitive to feedback noise or bias, and to\nprior beliefs (highlighting the importance of guiding instructions), but also\nreveal that perfect feedback is insufficient to guarantee high performance.\nThis approach provides a tool for assessing and predicting NFT variability,\ninterpret empirical data, and potentially develop personalized training\nprotocols.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e3b\u52a8\u63a8\u65ad\u7684\u8ba1\u7b97\u6a21\u578b\uff0c\u7528\u4e8e\u6a21\u62df\u795e\u7ecf\u53cd\u9988\u8bad\u7ec3\uff08NFT\uff09\u7684\u95ed\u73af\u8fc7\u7a0b\uff0c\u5206\u6790\u53cd\u9988\u8d28\u91cf\u548c\u5148\u9a8c\u4fe1\u5ff5\u7b49\u56e0\u7d20\u5bf9\u8bad\u7ec3\u6548\u679c\u7684\u5f71\u54cd\u3002", "motivation": "\u7531\u4e8e\u795e\u7ecf\u53cd\u9988\u8bad\u7ec3\u7684\u6548\u679c\u5dee\u5f02\u5927\u4e14\u673a\u5236\u4e0d\u660e\u786e\uff0c\u96be\u4ee5\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u8ba1\u7b97\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e3b\u52a8\u63a8\u65ad\uff08\u4e00\u79cd\u8d1d\u53f6\u65af\u6846\u67b6\uff09\u6a21\u62df\u4ee3\u7406\u4e0eNFT\u73af\u5883\u7684\u4ea4\u4e92\uff0c\u6d4b\u8bd5\u8bbe\u8ba1\u548c\u4e2a\u4f53\u56e0\u7d20\u5bf9\u8bad\u7ec3\u7684\u5f71\u54cd\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\u8bad\u7ec3\u6548\u679c\u5bf9\u53cd\u9988\u566a\u58f0\u3001\u504f\u5dee\u548c\u5148\u9a8c\u4fe1\u5ff5\u654f\u611f\uff0c\u4e14\u5b8c\u7f8e\u53cd\u9988\u65e0\u6cd5\u786e\u4fdd\u9ad8\u6027\u80fd\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u8bc4\u4f30NFT\u53d8\u5f02\u6027\u3001\u89e3\u91ca\u5b9e\u8bc1\u6570\u636e\u548c\u5f00\u53d1\u4e2a\u6027\u5316\u8bad\u7ec3\u534f\u8bae\u63d0\u4f9b\u4e86\u5de5\u5177\u3002"}}
{"id": "2505.03336", "pdf": "https://arxiv.org/pdf/2505.03336", "abs": "https://arxiv.org/abs/2505.03336", "authors": ["Hao Liao", "Wensheng Lu", "Jianxun Lian", "Mingqi Wu", "Shuo Wang", "Yong Zhang", "Yitian Huang", "Mingyang Zhou", "Xing Xie"], "title": "Avoid Recommending Out-of-Domain Items: Constrained Generative Recommendation with LLMs", "categories": ["cs.IR", "cs.AI"], "comment": "13 pages", "summary": "Large Language Models (LLMs) have shown promise for generative recommender\nsystems due to their transformative capabilities in user interaction. However,\nensuring they do not recommend out-of-domain (OOD) items remains a challenge.\nWe study two distinct methods to address this issue: RecLM-ret, a\nretrieval-based method, and RecLM-cgen, a constrained generation method. Both\nmethods integrate seamlessly with existing LLMs to ensure in-domain\nrecommendations. Comprehensive experiments on three recommendation datasets\ndemonstrate that RecLM-cgen consistently outperforms RecLM-ret and existing\nLLM-based recommender models in accuracy while eliminating OOD recommendations,\nmaking it the preferred method for adoption. Additionally, RecLM-cgen maintains\nstrong generalist capabilities and is a lightweight plug-and-play module for\neasy integration into LLMs, offering valuable practical benefits for the\ncommunity. Source code is available at https://github.com/microsoft/RecAI", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff08RecLM-ret\u548cRecLM-cgen\uff09\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684OOD\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660eRecLM-cgen\u5728\u51c6\u786e\u6027\u548c\u6d88\u9664OOD\u63a8\u8350\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u4e14\u6613\u4e8e\u96c6\u6210\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u5c55\u73b0\u6f5c\u529b\uff0c\u4f46OOD\u63a8\u8350\u95ee\u9898\u5c1a\u672a\u89e3\u51b3\u3002\u9700\u8981\u6709\u6548\u65b9\u6cd5\u786e\u4fdd\u63a8\u8350\u5185\u5bb9\u5728\u9886\u57df\u5185\u3002", "method": "\u7814\u7a76\u4e24\u79cd\u65b9\u6cd5\uff1a\u57fa\u4e8e\u68c0\u7d22\u7684RecLM-ret\u548c\u57fa\u4e8e\u7ea6\u675f\u751f\u6210\u7684RecLM-cgen\uff0c\u4e24\u8005\u5747\u53ef\u4e0e\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728\u4e09\u4e2a\u63a8\u8350\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cRecLM-cgen\u5728\u51c6\u786e\u6027\u548c\u6d88\u9664OOD\u63a8\u8350\u65b9\u9762\u4f18\u4e8eRecLM-ret\u53ca\u5176\u4ed6\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u8350\u65b9\u6cd5\u3002", "conclusion": "RecLM-cgen\u662f\u66f4\u4f18\u65b9\u6cd5\uff0c\u517c\u5177\u8f7b\u91cf\u5316\u548c\u6613\u96c6\u6210\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u5e26\u6765\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2505.03344", "pdf": "https://arxiv.org/pdf/2505.03344", "abs": "https://arxiv.org/abs/2505.03344", "authors": ["Keyu Chen", "Wenchao Sun", "Hao Cheng", "Sifa Zheng"], "title": "RIFT: Closed-Loop RL Fine-Tuning for Realistic and Controllable Traffic Simulation", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Achieving both realism and controllability in interactive closed-loop traffic\nsimulation remains a key challenge in autonomous driving. Data-driven\nsimulation methods reproduce realistic trajectories but suffer from covariate\nshift in closed-loop deployment, compounded by simplified dynamics models that\nfurther reduce reliability. Conversely, physics-based simulation methods\nenhance reliable and controllable closed-loop interactions but often lack\nexpert demonstrations, compromising realism. To address these challenges, we\nintroduce a dual-stage AV-centered simulation framework that conducts open-loop\nimitation learning pre-training in a data-driven simulator to capture\ntrajectory-level realism and multimodality, followed by closed-loop\nreinforcement learning fine-tuning in a physics-based simulator to enhance\ncontrollability and mitigate covariate shift. In the fine-tuning stage, we\npropose RIFT, a simple yet effective closed-loop RL fine-tuning strategy that\npreserves the trajectory-level multimodality through a GRPO-style\ngroup-relative advantage formulation, while enhancing controllability and\ntraining stability by replacing KL regularization with the dual-clip mechanism.\nExtensive experiments demonstrate that RIFT significantly improves the realism\nand controllability of generated traffic scenarios, providing a robust platform\nfor evaluating autonomous vehicle performance in diverse and interactive\nscenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u9636\u6bb5\u81ea\u52a8\u9a7e\u9a76\u4eff\u771f\u6846\u67b6\uff0c\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u548c\u7269\u7406\u4eff\u771f\uff0c\u63d0\u5347\u771f\u5b9e\u6027\u548c\u53ef\u63a7\u6027\uff0c\u5e76\u901a\u8fc7RIFT\u7b56\u7565\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u8fc7\u7a0b\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4eff\u771f\u4e2d\u771f\u5b9e\u6027\u4e0e\u53ef\u63a7\u6027\u96be\u4ee5\u517c\u987e\u7684\u95ee\u9898\uff0c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5b58\u5728\u534f\u53d8\u91cf\u504f\u79fb\uff0c\u7269\u7406\u4eff\u771f\u7f3a\u4e4f\u771f\u5b9e\u6027\u3002", "method": "\u91c7\u7528\u53cc\u9636\u6bb5\u6846\u67b6\uff1a\u6570\u636e\u9a71\u52a8\u4eff\u771f\u7684\u5f00\u73af\u6a21\u4eff\u5b66\u4e60\u9884\u8bad\u7ec3\uff0c\u7ed3\u5408\u7269\u7406\u4eff\u771f\u7684\u95ed\u73af\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff1b\u63d0\u51faRIFT\u7b56\u7565\uff0c\u4fdd\u7559\u591a\u6a21\u6001\u6027\u5e76\u589e\u5f3a\u53ef\u63a7\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRIFT\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u4ea4\u901a\u573a\u666f\u7684\u771f\u5b9e\u6027\u548c\u53ef\u63a7\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5f3a\u5927\u5e73\u53f0\u3002", "conclusion": "\u63d0\u51fa\u7684\u53cc\u9636\u6bb5\u6846\u67b6\u548cRIFT\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u4eff\u771f\u4e2d\u7684\u771f\u5b9e\u6027\u4e0e\u53ef\u63a7\u6027\u51b2\u7a81\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6a21\u62df\u73af\u5883\u3002"}}
{"id": "2505.03338", "pdf": "https://arxiv.org/pdf/2505.03338", "abs": "https://arxiv.org/abs/2505.03338", "authors": ["Lena Reissinger", "Yuanyuan Li", "Anna-Carolina Haensch", "Neeraj Sarna"], "title": "Safer Prompts: Reducing IP Risk in Visual Generative AI", "categories": ["math.NA", "cs.AI", "cs.NA"], "comment": null, "summary": "Visual Generative AI models have demonstrated remarkable capability in\ngenerating high-quality images from simple inputs like text prompts. However,\nbecause these models are trained on images from diverse sources, they risk\nmemorizing and reproducing specific content, raising concerns about\nintellectual property (IP) infringement. Recent advances in prompt engineering\noffer a cost-effective way to enhance generative AI performance. In this paper,\nwe evaluate the effectiveness of prompt engineering techniques in mitigating IP\ninfringement risks in image generation. Our findings show that Chain of Thought\nPrompting and Task Instruction Prompting significantly reduce the similarity\nbetween generated images and the training data of diffusion models, thereby\nlowering the risk of IP infringement.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u964d\u4f4e\u89c6\u89c9\u751f\u6210AI\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u77e5\u8bc6\u4ea7\u6743\u4fb5\u6743\u98ce\u9669\uff0c\u53d1\u73b0\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u548c\u4efb\u52a1\u6307\u4ee4\u63d0\u793a\u80fd\u6709\u6548\u51cf\u5c11\u751f\u6210\u56fe\u50cf\u4e0e\u8bad\u7ec3\u6570\u636e\u7684\u76f8\u4f3c\u6027\u3002", "motivation": "\u7531\u4e8e\u89c6\u89c9\u751f\u6210AI\u6a21\u578b\u5728\u591a\u6837\u5316\u7684\u56fe\u50cf\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5b58\u5728\u8bb0\u5fc6\u5e76\u91cd\u73b0\u7279\u5b9a\u5185\u5bb9\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u8fd9\u53ef\u80fd\u5f15\u53d1\u77e5\u8bc6\u4ea7\u6743\u4fb5\u6743\u95ee\u9898\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\uff08Chain of Thought Prompting\uff09\u548c\u4efb\u52a1\u6307\u4ee4\u63d0\u793a\uff08Task Instruction Prompting\uff09\u6280\u672f\u5bf9\u6269\u6563\u6a21\u578b\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8fd9\u4e24\u79cd\u63d0\u793a\u6280\u672f\u80fd\u663e\u8457\u964d\u4f4e\u751f\u6210\u56fe\u50cf\u4e0e\u8bad\u7ec3\u6570\u636e\u7684\u76f8\u4f3c\u6027\uff0c\u4ece\u800c\u51cf\u5c11\u77e5\u8bc6\u4ea7\u6743\u4fb5\u6743\u98ce\u9669\u3002", "conclusion": "\u63d0\u793a\u5de5\u7a0b\u662f\u964d\u4f4e\u89c6\u89c9\u751f\u6210AI\u6a21\u578b\u77e5\u8bc6\u4ea7\u6743\u4fb5\u6743\u98ce\u9669\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u548c\u4efb\u52a1\u6307\u4ee4\u63d0\u793a\u3002"}}
{"id": "2505.03385", "pdf": "https://arxiv.org/pdf/2505.03385", "abs": "https://arxiv.org/abs/2505.03385", "authors": ["Julia Bringewald"], "title": "Solar Flare Forecast: A Comparative Analysis of Machine Learning Algorithms for Solar Flare Class Prediction", "categories": ["astro-ph.SR", "astro-ph.IM", "cs.LG", "I.5.0"], "comment": null, "summary": "Solar flares are among the most powerful and dynamic events in the solar\nsystem, resulting from the sudden release of magnetic energy stored in the\nSun's atmosphere. These energetic bursts of electromagnetic radiation can\nrelease up to 10^32 erg of energy, impacting space weather and posing risks to\ntechnological infrastructure and therefore require accurate forecasting of\nsolar flare occurrences and intensities. This study evaluates the predictive\nperformance of three machine learning algorithms: Random Forest, k-Nearest\nNeighbors (KNN), and Extreme Gradient Boosting (XGBoost) for classifying solar\nflares into 4 categories (B, C, M, X). Using the dataset of 13 SHARP\nparameters, the effectiveness of the models was evaluated in binary and\nmulticlass classification tasks. The analysis utilized 8 principal components\n(PC), capturing 95% of data variance, and 100 PCs, capturing 97.5% of variance.\nOur approach uniquely combines binary and multiclass classification with\ndifferent levels of dimensionality reduction, an innovative methodology not\npreviously explored in the context of solar flare prediction. Employing a\n10-fold stratified cross-validation and grid search for hyperparameter tuning\nensured robust model evaluation. Our findings indicate that Random Forest and\nXGBoost consistently demonstrate strong performance across all metrics,\nbenefiting significantly from increased dimensionality. The insights of this\nstudy enhance future research by optimizing dimensionality reduction techniques\nand informing model selection for astrophysical tasks. By integrating this\nnewly acquired knowledge into future research, more accurate space weather\nforecasting systems can be developed, along with a deeper understanding of\nsolar physics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff08\u968f\u673a\u68ee\u6797\u3001KNN\u548cXGBoost\uff09\u5728\u592a\u9633\u8000\u6591\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u968f\u673a\u68ee\u6797\u548cXGBoost\u5728\u591a\u7ef4\u5ea6\u6570\u636e\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u592a\u9633\u8000\u6591\u5bf9\u7a7a\u95f4\u5929\u6c14\u548c\u57fa\u7840\u8bbe\u65bd\u6709\u91cd\u5927\u5f71\u54cd\uff0c\u9700\u8981\u51c6\u786e\u9884\u6d4b\u5176\u53d1\u751f\u548c\u5f3a\u5ea6\u3002", "method": "\u4f7f\u752813\u4e2aSHARP\u53c2\u6570\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u4e0d\u540c\u7ef4\u5ea6\u964d\u7ef4\uff088\u548c100\u4e2a\u4e3b\u6210\u5206\uff09\uff0c\u901a\u8fc710\u6298\u5206\u5c42\u4ea4\u53c9\u9a8c\u8bc1\u548c\u7f51\u683c\u641c\u7d22\u8c03\u53c2\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u4e8c\u5143\u548c\u591a\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u968f\u673a\u68ee\u6797\u548cXGBoost\u5728\u6240\u6709\u6307\u6807\u4e2d\u8868\u73b0\u7a33\u5b9a\uff0c\u5c24\u5176\u5728\u66f4\u9ad8\u7ef4\u5ea6\u6570\u636e\u4e0b\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u672a\u6765\u5929\u6587\u7269\u7406\u4efb\u52a1\u7684\u6a21\u578b\u9009\u62e9\u548c\u7ef4\u5ea6\u964d\u7ef4\u4f18\u5316\u63d0\u4f9b\u4e86\u53c2\u8003\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u51c6\u786e\u7684\u7a7a\u95f4\u5929\u6c14\u9884\u62a5\u7cfb\u7edf\u3002"}}
{"id": "2505.03397", "pdf": "https://arxiv.org/pdf/2505.03397", "abs": "https://arxiv.org/abs/2505.03397", "authors": ["Chris Wise", "Akram Youssry", "Alberto Peruzzo", "Jo Plested", "Matt Woolley"], "title": "Quantum Feature Space of a Qubit Coupled to an Arbitrary Bath", "categories": ["quant-ph", "cs.LG"], "comment": "19 pages, 3 figures, 4 tables", "summary": "Qubit control protocols have traditionally leveraged a characterisation of\nthe qubit-bath coupling via its power spectral density. Previous work proposed\nthe inference of noise operators that characterise the influence of a classical\nbath using a grey-box approach that combines deep neural networks with\nphysics-encoded layers. This overall structure is complex and poses challenges\nin scaling and real-time operations. Here, we show that no expensive neural\nnetworks are needed and that this noise operator description admits an\nefficient parameterisation. We refer to the resulting parameter space as the\n\\textit{quantum feature space} of the qubit dynamics resulting from the coupled\nbath. We show that the Euclidean distance defined over the quantum feature\nspace provides an effective method for classifying noise processes in the\npresence of a given set of controls. Using the quantum feature space as the\ninput space for a simple machine learning algorithm (random forest, in this\ncase), we demonstrate that it can effectively classify the stationarity and the\nbroad class of noise processes perturbing a qubit. Finally, we explore how\ncontrol pulse parameters map to the quantum feature space.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u53c2\u6570\u5316\u65b9\u6cd5\uff08\u91cf\u5b50\u7279\u5f81\u7a7a\u95f4\uff09\u6765\u66ff\u4ee3\u590d\u6742\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u7c7b\u548c\u8868\u5f81\u91cf\u5b50\u6bd4\u7279\u566a\u58f0\u8fc7\u7a0b\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u566a\u58f0\u5206\u7c7b\u548c\u63a7\u5236\u8109\u51b2\u53c2\u6570\u6620\u5c04\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e0e\u7269\u7406\u5b66\u7f16\u7801\u5c42\u7684\u7ed3\u5408\uff0c\u4e0d\u4ec5\u7ed3\u6784\u590d\u6742\uff0c\u8fd8\u96be\u4ee5\u6269\u5c55\u548c\u5b9e\u65f6\u64cd\u4f5c\u3002\u4f5c\u8005\u65e8\u5728\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u53c2\u6570\u5316\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u91cf\u5b50\u7279\u5f81\u7a7a\u95f4\u6765\u53c2\u6570\u5316\u566a\u58f0\u64cd\u4f5c\u7b26\uff0c\u5229\u7528\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u8fdb\u884c\u566a\u58f0\u8fc7\u7a0b\u5206\u7c7b\uff0c\u5e76\u91c7\u7528\u968f\u673a\u68ee\u6797\u7b49\u7b80\u5355\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u9a8c\u8bc1\u5176\u5206\u7c7b\u6548\u679c\u3002", "result": "\u91cf\u5b50\u7279\u5f81\u7a7a\u95f4\u80fd\u6709\u6548\u5206\u7c7b\u566a\u58f0\u7684\u5e73\u7a33\u6027\u548c\u7c7b\u578b\uff0c\u5e76\u80fd\u6620\u5c04\u63a7\u5236\u8109\u51b2\u53c2\u6570\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u91cf\u5b50\u7279\u5f81\u7a7a\u95f4\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7b80\u5316\u7684\u566a\u58f0\u8868\u5f81\u548c\u5206\u7c7b\u65b9\u6cd5\uff0c\u4f18\u4e8e\u4f20\u7edf\u590d\u6742\u7684\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u4e14\u6613\u4e8e\u6269\u5c55\u548c\u5b9e\u65f6\u64cd\u4f5c\u3002"}}
{"id": "2505.03380", "pdf": "https://arxiv.org/pdf/2505.03380", "abs": "https://arxiv.org/abs/2505.03380", "authors": ["Haonan Wang", "Jiaji Mao", "Lehan Wang", "Qixiang Zhang", "Marawan Elbatel", "Yi Qin", "Huijun Hu", "Baoxun Li", "Wenhui Deng", "Weifeng Qin", "Hongrui Li", "Jialin Liang", "Jun Shen", "Xiaomeng Li"], "title": "Reinforced Correlation Between Vision and Language for Precise Medical AI Assistant", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Medical AI assistants support doctors in disease diagnosis, medical image\nanalysis, and report generation. However, they still face significant\nchallenges in clinical use, including limited accuracy with multimodal content\nand insufficient validation in real-world settings. We propose RCMed, a\nfull-stack AI assistant that improves multimodal alignment in both input and\noutput, enabling precise anatomical delineation, accurate localization, and\nreliable diagnosis through hierarchical vision-language grounding. A\nself-reinforcing correlation mechanism allows visual features to inform\nlanguage context, while language semantics guide pixel-wise attention, forming\na closed loop that refines both modalities. This correlation is enhanced by a\ncolor region description strategy, translating anatomical structures into\nsemantically rich text to learn shape-location-text relationships across\nscales. Trained on 20 million image-mask-description triplets, RCMed achieves\nstate-of-the-art precision in contextualizing irregular lesions and subtle\nanatomical boundaries, excelling in 165 clinical tasks across 9 modalities. It\nachieved a 23.5% relative improvement in cell segmentation from microscopy\nimages over prior methods. RCMed's strong vision-language alignment enables\nexceptional generalization, with state-of-the-art performance in external\nvalidation across 20 clinically significant cancer types, including novel\ntasks. This work demonstrates how integrated multimodal models capture\nfine-grained patterns, enabling human-level interpretation in complex scenarios\nand advancing human-centric AI healthcare.", "AI": {"tldr": "RCMed\u662f\u4e00\u4e2a\u5168\u6808AI\u52a9\u624b\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u63d0\u5347\u591a\u6a21\u6001\u8f93\u5165\u7684\u51c6\u786e\u6027\uff0c\u5b9e\u73b0\u7cbe\u786e\u89e3\u5256\u5b9a\u4f4d\u548c\u53ef\u9760\u8bca\u65ad\u3002\u5b83\u5728165\u4e2a\u4e34\u5e8a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u5916\u90e8\u9a8c\u8bc1\u4e2d\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u533b\u7597AI\u52a9\u624b\u5728\u591a\u6a21\u6001\u5185\u5bb9\u51c6\u786e\u6027\u548c\u5b9e\u9645\u4e34\u5e8a\u9a8c\u8bc1\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cRCMed\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u591a\u6a21\u6001\u5bf9\u9f50\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u63a8\u52a8\u7cbe\u51c6\u533b\u7597\u7684\u53d1\u5c55\u3002", "method": "RCMed\u91c7\u7528\u81ea\u589e\u5f3a\u7684\u89c6\u89c9-\u8bed\u8a00\u5173\u8054\u673a\u5236\uff0c\u7ed3\u5408\u989c\u8272\u533a\u57df\u63cf\u8ff0\u7b56\u7565\uff0c\u8bad\u7ec3\u4e862000\u4e07\u56fe\u50cf-\u63a9\u7801-\u63cf\u8ff0\u4e09\u5143\u7ec4\uff0c\u5b9e\u73b0\u8de8\u5c3a\u5ea6\u7684\u5f62\u72b6-\u4f4d\u7f6e-\u6587\u672c\u5173\u7cfb\u5b66\u4e60\u3002", "result": "RCMed\u5728165\u4e2a\u4e34\u5e8a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7ec6\u80de\u5206\u5272\u7cbe\u5ea6\u76f8\u5bf9\u63d0\u534723.5%\uff0c\u5e76\u572820\u79cd\u764c\u75c7\u7c7b\u578b\u7684\u9a8c\u8bc1\u4e2d\u8fbe\u5230\u6700\u65b0\u6280\u672f\u6c34\u5e73\u3002", "conclusion": "RCMed\u5c55\u793a\u4e86\u96c6\u6210\u591a\u6a21\u6001\u6a21\u578b\u5728\u590d\u6742\u533b\u7597\u573a\u666f\u4e2d\u5982\u4f55\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u6a21\u5f0f\u6355\u6349\u548c\u4eba\u7c7b\u6c34\u5e73\u7684\u89e3\u91ca\u80fd\u529b\uff0c\u63a8\u52a8\u4e86\u4ee5\u4eba\u4e3a\u672c\u7684AI\u533b\u7597\u53d1\u5c55\u3002"}}
{"id": "2505.03401", "pdf": "https://arxiv.org/pdf/2505.03401", "abs": "https://arxiv.org/abs/2505.03401", "authors": ["Shanshan Song", "Hui Tang", "Honglong Yang", "Xiaomeng Li"], "title": "DDaTR: Dynamic Difference-aware Temporal Residual Network for Longitudinal Radiology Report Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Radiology Report Generation (RRG) automates the creation of radiology reports\nfrom medical imaging, enhancing the efficiency of the reporting process.\nLongitudinal Radiology Report Generation (LRRG) extends RRG by incorporating\nthe ability to compare current and prior exams, facilitating the tracking of\ntemporal changes in clinical findings. Existing LRRG approaches only extract\nfeatures from prior and current images using a visual pre-trained encoder,\nwhich are then concatenated to generate the final report. However, these\nmethods struggle to effectively capture both spatial and temporal correlations\nduring the feature extraction process. Consequently, the extracted features\ninadequately capture the information of difference across exams and thus\nunderrepresent the expected progressions, leading to sub-optimal performance in\nLRRG. To address this, we develop a novel dynamic difference-aware temporal\nresidual network (DDaTR). In DDaTR, we introduce two modules at each stage of\nthe visual encoder to capture multi-level spatial correlations. The Dynamic\nFeature Alignment Module (DFAM) is designed to align prior features across\nmodalities for the integrity of prior clinical information. Prompted by the\nenriched prior features, the dynamic difference-aware module (DDAM) captures\nfavorable difference information by identifying relationships across exams.\nFurthermore, our DDaTR employs the dynamic residual network to unidirectionally\ntransmit longitudinal information, effectively modelling temporal correlations.\nExtensive experiments demonstrated superior performance over existing methods\non three benchmarks, proving its efficacy in both RRG and LRRG tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u5dee\u5f02\u611f\u77e5\u65f6\u95f4\u6b8b\u5dee\u7f51\u7edc\uff08DDaTR\uff09\uff0c\u7528\u4e8e\u6539\u8fdb\u7eb5\u5411\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\uff08LRRG\uff09\uff0c\u901a\u8fc7\u6355\u6349\u7a7a\u95f4\u548c\u65f6\u95f4\u76f8\u5173\u6027\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LRRG\u65b9\u6cd5\u5728\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b\u4e2d\u96be\u4ee5\u6709\u6548\u6355\u6349\u7a7a\u95f4\u548c\u65f6\u95f4\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u5dee\u5f02\u4fe1\u606f\u6355\u83b7\u4e0d\u8db3\uff0c\u5f71\u54cd\u4e86\u62a5\u544a\u751f\u6210\u7684\u8d28\u91cf\u3002", "method": "DDaTR\u7f51\u7edc\u5f15\u5165\u4e86\u52a8\u6001\u7279\u5f81\u5bf9\u9f50\u6a21\u5757\uff08DFAM\uff09\u548c\u52a8\u6001\u5dee\u5f02\u611f\u77e5\u6a21\u5757\uff08DDAM\uff09\uff0c\u4ee5\u591a\u7ea7\u63d0\u53d6\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u6b8b\u5dee\u7f51\u7edc\u5efa\u6a21\u65f6\u95f4\u76f8\u5173\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDDaTR\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728RRG\u548cLRRG\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "DDaTR\u901a\u8fc7\u6539\u8fdb\u7279\u5f81\u63d0\u53d6\u548c\u5efa\u6a21\u65f6\u95f4\u76f8\u5173\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u7684\u6027\u80fd\u3002"}}
{"id": "2505.03442", "pdf": "https://arxiv.org/pdf/2505.03442", "abs": "https://arxiv.org/abs/2505.03442", "authors": ["Diep Luong", "Mikko Heikkinen", "Konstantinos Drossos", "Tuomas Virtanen"], "title": "Knowledge Distillation for Speech Denoising by Latent Representation Alignment with Cosine Distance", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": null, "summary": "Speech denoising is a generally adopted and impactful task, appearing in many\ncommon and everyday-life use cases. Although there are very powerful methods\npublished, most of those are too complex for deployment in everyday and\nlow-resources computational environments, like hand-held devices, intelligent\nglasses, hearing aids, etc. Knowledge distillation (KD) is a prominent way for\nalleviating this complexity mismatch and is based on the\ntransferring/distilling of knowledge from a pre-trained complex model, the\nteacher, to another less complex one, the student. Existing KD methods for\nspeech denoising are based on processes that potentially hamper the KD by\nbounding the learning of the student to the distribution, information ordering,\nand feature dimensionality learned by the teacher. In this paper, we present\nand assess a method that tries to treat this issue, by exploiting the\nwell-known denoising-autoencoder framework, the linear inverted bottlenecks,\nand the properties of the cosine similarity. We use a public dataset and\nconduct repeated experiments with different mismatching scenarios between the\nteacher and the student, reporting the mean and standard deviation of the\nmetrics of our method and another, state-of-the-art method that is used as a\nbaseline. Our results show that with the proposed method, the student can\nperform better and can also retain greater mismatching conditions compared to\nthe teacher.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u8bed\u97f3\u964d\u566a\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u6559\u5e08\u6a21\u578b\u7684\u7279\u5f81\u9650\u5236\uff0c\u63d0\u5347\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u9650\u5236\u4e86\u5b66\u751f\u6a21\u578b\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u5bfc\u81f4\u5728\u4f4e\u8d44\u6e90\u8bbe\u5907\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u7ed3\u5408\u964d\u566a\u81ea\u7f16\u7801\u5668\u6846\u67b6\u3001\u7ebf\u6027\u5012\u7f6e\u74f6\u9888\u7ed3\u6784\u548c\u4f59\u5f26\u76f8\u4f3c\u6027\u7279\u6027\uff0c\u4f18\u5316\u77e5\u8bc6\u84b8\u998f\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u4f7f\u5b66\u751f\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u4e14\u80fd\u9002\u5e94\u66f4\u5927\u7684\u5e08\u751f\u6a21\u578b\u4e0d\u5339\u914d\u60c5\u51b5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u5b66\u751f\u6a21\u578b\u7684\u8bed\u97f3\u964d\u566a\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u573a\u666f\u3002"}}
{"id": "2505.03470", "pdf": "https://arxiv.org/pdf/2505.03470", "abs": "https://arxiv.org/abs/2505.03470", "authors": ["Vibhas Vats", "Md. Alimoor Reza", "David Crandall", "Soon-heung Jung"], "title": "Blending 3D Geometry and Machine Learning for Multi-View Stereopsis", "categories": ["cs.CV", "cs.AI", "cs.CG", "cs.LG"], "comment": "A pre-print -- paper under-review. arXiv admin note: substantial text\n  overlap with arXiv:2310.19583", "summary": "Traditional multi-view stereo (MVS) methods primarily depend on photometric\nand geometric consistency constraints. In contrast, modern learning-based\nalgorithms often rely on the plane sweep algorithm to infer 3D geometry,\napplying explicit geometric consistency (GC) checks only as a post-processing\nstep, with no impact on the learning process itself. In this work, we introduce\nGC MVSNet plus plus, a novel approach that actively enforces geometric\nconsistency of reference view depth maps across multiple source views (multi\nview) and at various scales (multi scale) during the learning phase (see Fig.\n1). This integrated GC check significantly accelerates the learning process by\ndirectly penalizing geometrically inconsistent pixels, effectively halving the\nnumber of training iterations compared to other MVS methods. Furthermore, we\nintroduce a densely connected cost regularization network with two distinct\nblock designs simple and feature dense optimized to harness dense feature\nconnections for enhanced regularization. Extensive experiments demonstrate that\nour approach achieves a new state of the art on the DTU and BlendedMVS datasets\nand secures second place on the Tanks and Temples benchmark. To our knowledge,\nGC MVSNet plus plus is the first method to enforce multi-view, multi-scale\nsupervised geometric consistency during learning. Our code is available.", "AI": {"tldr": "GC MVSNet++\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u591a\u89c6\u56fe\u548c\u591a\u5c3a\u5ea6\u4e0b\u4e3b\u52a8\u5b9e\u65bd\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u52a0\u901f\u5b66\u4e60\u8fc7\u7a0b\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684MVS\u65b9\u6cd5\u4f9d\u8d56\u540e\u5904\u7406\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u68c0\u67e5\uff0c\u800c\u73b0\u4ee3\u5b66\u4e60\u65b9\u6cd5\u672a\u5728\u5b66\u4e60\u9636\u6bb5\u5145\u5206\u5229\u7528\u51e0\u4f55\u4e00\u81f4\u6027\u3002\u4f5c\u8005\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u591a\u89c6\u89d2\u3001\u591a\u5c3a\u5ea6\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u76d1\u7763\u6539\u8fdb\u5b66\u4e60\u6548\u7387\u548c\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u5728\u591a\u89c6\u56fe\u548c\u591a\u5c3a\u5ea6\u4e0b\u76f4\u63a5\u60e9\u7f5a\u51e0\u4f55\u4e0d\u4e00\u81f4\u50cf\u7d20\uff0c\u52a0\u901f\u5b66\u4e60\uff1b\u63d0\u51fa\u7a20\u5bc6\u8fde\u63a5\u7684\u4ee3\u4ef7\u6b63\u5219\u5316\u7f51\u7edc\uff0c\u5305\u542b\u4e24\u79cd\u5757\u8bbe\u8ba1\u4ee5\u589e\u5f3a\u7279\u5f81\u8fde\u63a5\u3002", "result": "\u5728DTU\u548cBlendedMVS\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u65b0SOTA\uff0c\u5728Tanks and Temples\u699c\u5355\u4f4d\u5c45\u7b2c\u4e8c\u3002", "conclusion": "GC MVSNet++\u9996\u6b21\u5728\u5b66\u4e60\u9636\u6bb5\u5b9e\u65bd\u591a\u89c6\u56fe\u3001\u591a\u5c3a\u5ea6\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u76d1\u7763\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u548c\u5b66\u4e60\u6548\u7387\u3002"}}
{"id": "2505.03426", "pdf": "https://arxiv.org/pdf/2505.03426", "abs": "https://arxiv.org/abs/2505.03426", "authors": ["Ziyu Li", "Yujian Hu", "Zhengyao Ding", "Yiheng Mao", "Haitao Li", "Fan Yi", "Hongkun Zhang", "Zhengxing Huang"], "title": "Phenotype-Guided Generative Model for High-Fidelity Cardiac MRI Synthesis: Advancing Pretraining and Clinical Applications", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Cardiac Magnetic Resonance (CMR) imaging is a vital non-invasive tool for\ndiagnosing heart diseases and evaluating cardiac health. However, the limited\navailability of large-scale, high-quality CMR datasets poses a major challenge\nto the effective application of artificial intelligence (AI) in this domain.\nEven the amount of unlabeled data and the health status it covers are difficult\nto meet the needs of model pretraining, which hinders the performance of AI\nmodels on downstream tasks. In this study, we present Cardiac Phenotype-Guided\nCMR Generation (CPGG), a novel approach for generating diverse CMR data that\ncovers a wide spectrum of cardiac health status. The CPGG framework consists of\ntwo stages: in the first stage, a generative model is trained using cardiac\nphenotypes derived from CMR data; in the second stage, a masked autoregressive\ndiffusion model, conditioned on these phenotypes, generates high-fidelity CMR\ncine sequences that capture both structural and functional features of the\nheart in a fine-grained manner. We synthesized a massive amount of CMR to\nexpand the pretraining data. Experimental results show that CPGG generates\nhigh-quality synthetic CMR data, significantly improving performance on various\ndownstream tasks, including diagnosis and cardiac phenotypes prediction. These\ngains are demonstrated across both public and private datasets, highlighting\nthe effectiveness of our approach. Code is availabel at\nhttps://anonymous.4open.science/r/CPGG.", "AI": {"tldr": "CPGG\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u591a\u6837\u5316\u5fc3\u810f\u78c1\u5171\u632f\uff08CMR\uff09\u6570\u636e\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u9ad8\u8d28\u91cfCMR\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4ee5\u652f\u6301AI\u6a21\u578b\u5728\u5fc3\u810f\u5065\u5eb7\u8bc4\u4f30\u4e2d\u7684\u6709\u6548\u5e94\u7528\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u9996\u5148\u751f\u6210\u6a21\u578b\u5b66\u4e60CMR\u6570\u636e\u7684\u5fc3\u810f\u8868\u578b\uff0c\u968f\u540e\u5229\u7528\u63a9\u7801\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u4fdd\u771fCMR\u5e8f\u5217\u3002", "result": "\u751f\u6210\u7684\u5408\u6210CMR\u6570\u636e\u8d28\u91cf\u9ad8\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bca\u65ad\u548c\u8868\u578b\u9884\u6d4b\u7b49\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "CPGG\u6846\u67b6\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u6570\u636e\uff0c\u6709\u6548\u89e3\u51b3\u4e86CMR\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u63a8\u52a8\u4e86AI\u5728\u5fc3\u810f\u5065\u5eb7\u9886\u57df\u7684\u5e94\u7528\u3002"}}
{"id": "2505.03480", "pdf": "https://arxiv.org/pdf/2505.03480", "abs": "https://arxiv.org/abs/2505.03480", "authors": ["Lilian Marey", "Charlotte Laclau", "Bruno Sguerra", "Tiphaine Viard", "Manuel Moussallam"], "title": "Modeling Musical Genre Trajectories through Pathlet Learning", "categories": ["cs.IR", "cs.LG", "cs.MM"], "comment": "Adjunct Proceedings of the 33rd ACM Conference on User Modeling,\n  Adaptation and Personalization (UMAP Adjunct '25)", "summary": "The increasing availability of user data on music streaming platforms opens\nup new possibilities for analyzing music consumption. However, understanding\nthe evolution of user preferences remains a complex challenge, particularly as\ntheir musical tastes change over time. This paper uses the dictionary learning\nparadigm to model user trajectories across different musical genres. We define\na new framework that captures recurring patterns in genre trajectories, called\npathlets, enabling the creation of comprehensible trajectory embeddings. We\nshow that pathlet learning reveals relevant listening patterns that can be\nanalyzed both qualitatively and quantitatively. This work improves our\nunderstanding of users' interactions with music and opens up avenues of\nresearch into user behavior and fostering diversity in recommender systems. A\ndataset of 2000 user histories tagged by genre over 17 months, supplied by\nDeezer (a leading music streaming company), is also released with the code.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b57\u5178\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u7528\u6237\u5728\u97f3\u4e50\u6d41\u5a92\u4f53\u5e73\u53f0\u4e0a\u7684\u97f3\u4e50\u7c7b\u578b\u8f68\u8ff9\uff0c\u63ed\u793a\u4e86\u7528\u6237\u542c\u6b4c\u504f\u597d\u7684\u6f14\u53d8\u6a21\u5f0f\u3002", "motivation": "\u968f\u7740\u97f3\u4e50\u6d41\u5a92\u4f53\u5e73\u53f0\u4e0a\u7528\u6237\u6570\u636e\u7684\u589e\u52a0\uff0c\u5206\u6790\u7528\u6237\u97f3\u4e50\u6d88\u8d39\u884c\u4e3a\u6210\u4e3a\u53ef\u80fd\uff0c\u4f46\u8ffd\u8e2a\u7528\u6237\u504f\u597d\u7684\u53d8\u5316\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u5b57\u5178\u5b66\u4e60\u8303\u5f0f\uff0c\u5b9a\u4e49\u4e86\u4e00\u79cd\u79f0\u4e3a`pathlets`\u7684\u6846\u67b6\uff0c\u6355\u6349\u97f3\u4e50\u7c7b\u578b\u8f68\u8ff9\u4e2d\u7684\u91cd\u590d\u6a21\u5f0f\uff0c\u751f\u6210\u53ef\u7406\u89e3\u7684\u8f68\u8ff9\u5d4c\u5165\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c`pathlets`\u80fd\u6709\u6548\u63ed\u793a\u7528\u6237\u542c\u6b4c\u6a21\u5f0f\uff0c\u5e76\u53ef\u5b9a\u6027\u5b9a\u91cf\u5206\u6790\uff1b\u540c\u65f6\u53d1\u5e03\u4e86Deezer\u63d0\u4f9b\u76842000\u540d\u7528\u623717\u4e2a\u6708\u7684\u97f3\u4e50\u7c7b\u578b\u6807\u7b7e\u6570\u636e\u96c6\u3002", "conclusion": "\u7814\u7a76\u589e\u8fdb\u4e86\u5bf9\u7528\u6237\u4e0e\u97f3\u4e50\u4e92\u52a8\u884c\u4e3a\u7684\u7406\u89e3\uff0c\u4e3a\u63a8\u8350\u7cfb\u7edf\u4f18\u5316\u53ca\u7528\u6237\u884c\u4e3a\u591a\u6837\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.03451", "pdf": "https://arxiv.org/pdf/2505.03451", "abs": "https://arxiv.org/abs/2505.03451", "authors": ["Fouad Trad", "Ali Chehab"], "title": "Detecting Quishing Attacks with Machine Learning Techniques Through QR Code Analysis", "categories": ["cs.CR", "cs.AI"], "comment": "Accepted in 8th International Conference on Optimization and Learning\n  (OLA2025)", "summary": "The rise of QR code based phishing (\"Quishing\") poses a growing cybersecurity\nthreat, as attackers increasingly exploit QR codes to bypass traditional\nphishing defenses. Existing detection methods predominantly focus on URL\nanalysis, which requires the extraction of the QR code payload, and may\ninadvertently expose users to malicious content. Moreover, QR codes can encode\nvarious types of data beyond URLs, such as Wi-Fi credentials and payment\ninformation, making URL-based detection insufficient for broader security\nconcerns. To address these gaps, we propose the first framework for quishing\ndetection that directly analyzes QR code structure and pixel patterns without\nextracting the embedded content. We generated a dataset of phishing and benign\nQR codes and we used it to train and evaluate multiple machine learning models,\nincluding Logistic Regression, Decision Trees, Random Forest, Naive Bayes,\nLightGBM, and XGBoost. Our best-performing model (XGBoost) achieves an AUC of\n0.9106, demonstrating the feasibility of QR-centric detection. Through feature\nimportance analysis, we identify key visual indicators of malicious intent and\nrefine our feature set by removing non-informative pixels, improving\nperformance to an AUC of 0.9133 with a reduced feature space. Our findings\nreveal that the structural features of QR code correlate strongly with phishing\nrisk. This work establishes a foundation for quishing mitigation and highlights\nthe potential of direct QR analysis as a critical layer in modern phishing\ndefenses.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u76f4\u63a5\u5206\u6790\u4e8c\u7ef4\u7801\u7ed3\u6784\u548c\u50cf\u7d20\u6a21\u5f0f\u7684quishing\u68c0\u6d4b\u6846\u67b6\uff0c\u65e0\u9700\u63d0\u53d6\u5d4c\u5165\u5185\u5bb9\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8AUC\u6027\u80fd\u3002", "motivation": "\u4e8c\u7ef4\u7801\u9493\u9c7c\u653b\u51fb\uff08Quishing\uff09\u65e5\u76ca\u4e25\u91cd\uff0c\u73b0\u6709\u57fa\u4e8eURL\u7684\u68c0\u6d4b\u65b9\u6cd5\u4e0d\u4ec5\u9700\u8981\u63d0\u53d6\u4e8c\u7ef4\u7801\u5185\u5bb9\uff0c\u8fd8\u53ef\u80fd\u66b4\u9732\u7528\u6237\u4e8e\u6076\u610f\u5185\u5bb9\uff0c\u4e14\u65e0\u6cd5\u8986\u76d6\u4e8c\u7ef4\u7801\u7f16\u7801\u7684\u5176\u4ed6\u7c7b\u578b\u6570\u636e\u3002", "method": "\u751f\u6210\u9493\u9c7c\u548c\u826f\u6027\u4e8c\u7ef4\u7801\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u5e76\u8bc4\u4f30\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982XGBoost\uff09\uff0c\u901a\u8fc7\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u4f18\u5316\u7279\u5f81\u96c6\u3002", "result": "\u6700\u4f73\u6a21\u578b\uff08XGBoost\uff09AUC\u8fbe0.9106\uff0c\u4f18\u5316\u7279\u5f81\u96c6\u540e\u63d0\u5347\u81f30.9133\uff0c\u53d1\u73b0\u4e8c\u7ef4\u7801\u7ed3\u6784\u7279\u5f81\u4e0e\u9493\u9c7c\u98ce\u9669\u5f3a\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\u76f4\u63a5\u5206\u6790\u4e8c\u7ef4\u7801\u7684\u53ef\u884c\u6027\uff0c\u4e3aquishing\u9632\u5fa1\u5960\u5b9a\u57fa\u7840\uff0c\u5e76\u51f8\u663e\u5176\u4f5c\u4e3a\u73b0\u4ee3\u9493\u9c7c\u9632\u5fa1\u5173\u952e\u5c42\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.03585", "pdf": "https://arxiv.org/pdf/2505.03585", "abs": "https://arxiv.org/abs/2505.03585", "authors": ["Charita Dellaporta", "Patrick O'Hara", "Theodoros Damoulas"], "title": "Decision Making under Model Misspecification: DRO with Robust Bayesian Ambiguity Sets", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Distributionally Robust Optimisation (DRO) protects risk-averse\ndecision-makers by considering the worst-case risk within an ambiguity set of\ndistributions based on the empirical distribution or a model. To further guard\nagainst finite, noisy data, model-based approaches admit Bayesian formulations\nthat propagate uncertainty from the posterior to the decision-making problem.\nHowever, when the model is misspecified, the decision maker must stretch the\nambiguity set to contain the data-generating process (DGP), leading to overly\nconservative decisions. We address this challenge by introducing DRO with\nRobust, to model misspecification, Bayesian Ambiguity Sets (DRO-RoBAS). These\nare Maximum Mean Discrepancy ambiguity sets centred at a robust posterior\npredictive distribution that incorporates beliefs about the DGP. We show that\nthe resulting optimisation problem obtains a dual formulation in the\nReproducing Kernel Hilbert Space and we give probabilistic guarantees on the\ntolerance level of the ambiguity set. Our method outperforms other Bayesian and\nempirical DRO approaches in out-of-sample performance on the Newsvendor and\nPortfolio problems with various cases of model misspecification.", "AI": {"tldr": "\u63d0\u51faDRO-RoBAS\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u7a33\u5065\u540e\u9a8c\u9884\u6d4b\u5206\u5e03\u89e3\u51b3\u6a21\u578b\u8bef\u8bbe\u95ee\u9898\uff0c\u5728\u6570\u636e\u566a\u58f0\u548c\u6709\u9650\u6837\u672c\u6761\u4ef6\u4e0b\u4f18\u4e8e\u4f20\u7edf\u8d1d\u53f6\u65af\u548c\u5b9e\u8bc1DRO\u65b9\u6cd5\u3002", "motivation": "\u9488\u5bf9\u6a21\u578b\u8bef\u8bbe\u5bfc\u81f4\u51b3\u7b56\u8fc7\u4e8e\u4fdd\u5b88\u7684\u95ee\u9898\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u65b9\u6cd5\u63d0\u51fa\u66f4\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165DRO-RoBAS\uff0c\u4f7f\u7528\u57fa\u4e8e\u6700\u5927\u5747\u503c\u5dee\u5f02\u7684\u6a21\u7cca\u96c6\uff0c\u4ee5\u7a33\u5065\u540e\u9a8c\u9884\u6d4b\u5206\u5e03\u4e3a\u4e2d\u5fc3\uff0c\u7ed3\u5408\u5bf9\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u7684\u4fe1\u5ff5\u3002", "result": "\u5728Newsvendor\u548cPortfolio\u95ee\u9898\u4e0a\uff0cDRO-RoBAS\u5728\u6837\u672c\u5916\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u8d1d\u53f6\u65af\u548c\u5b9e\u8bc1DRO\u65b9\u6cd5\u3002", "conclusion": "DRO-RoBAS\u6709\u6548\u5e94\u5bf9\u6a21\u578b\u8bef\u8bbe\uff0c\u63d0\u5347\u4e86\u51b3\u7b56\u7684\u7a33\u5065\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2505.03590", "pdf": "https://arxiv.org/pdf/2505.03590", "abs": "https://arxiv.org/abs/2505.03590", "authors": ["Julian P. Merkofer", "Dennis M. J. van de Sande", "Alex A. Bhogal", "Ruud J. G. van Sloun"], "title": "Physics-Informed Sylvester Normalizing Flows for Bayesian Inference in Magnetic Resonance Spectroscopy", "categories": ["stat.ML", "cs.LG", "eess.SP"], "comment": "Preprint submitted to IEEE MLSP 2025", "summary": "Magnetic resonance spectroscopy (MRS) is a non-invasive technique to measure\nthe metabolic composition of tissues, offering valuable insights into\nneurological disorders, tumor detection, and other metabolic dysfunctions.\nHowever, accurate metabolite quantification is hindered by challenges such as\nspectral overlap, low signal-to-noise ratio, and various artifacts. Traditional\nmethods like linear-combination modeling are susceptible to ambiguities and\ncommonly only provide a theoretical lower bound on estimation accuracy in the\nform of the Cram\\'er-Rao bound. This work introduces a Bayesian inference\nframework using Sylvester normalizing flows (SNFs) to approximate posterior\ndistributions over metabolite concentrations, enhancing quantification\nreliability. A physics-based decoder incorporates prior knowledge of MRS signal\nformation, ensuring realistic distribution representations. We validate the\nmethod on simulated 7T proton MRS data, demonstrating accurate metabolite\nquantification, well-calibrated uncertainties, and insights into parameter\ncorrelations and multi-modal distributions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSylvester\u5f52\u4e00\u5316\u6d41\uff08SNFs\uff09\u7684\u8d1d\u53f6\u65af\u63a8\u65ad\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u78c1\u5171\u632f\u6ce2\u8c31\uff08MRS\uff09\u4e2d\u4ee3\u8c22\u7269\u5b9a\u91cf\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5982\u7ebf\u6027\u7ec4\u5408\u6a21\u578b\u5728\u4ee3\u8c22\u7269\u5b9a\u91cf\u4e2d\u53d7\u9650\u4e8e\u8c31\u91cd\u53e0\u3001\u4f4e\u4fe1\u566a\u6bd4\u548c\u4f2a\u5f71\u7b49\u95ee\u9898\uff0c\u4e14\u4ec5\u63d0\u4f9b\u7406\u8bba\u7cbe\u5ea6\u4e0b\u9650\u3002", "method": "\u91c7\u7528Sylvester\u5f52\u4e00\u5316\u6d41\uff08SNFs\uff09\u8fd1\u4f3c\u4ee3\u8c22\u7269\u6d53\u5ea6\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u7269\u7406\u7684\u89e3\u7801\u5668\u5f15\u5165MRS\u4fe1\u53f7\u5f62\u6210\u7684\u5148\u9a8c\u77e5\u8bc6\u3002", "result": "\u5728\u6a21\u62df\u76847T\u8d28\u5b50MRS\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u3001\u4e0d\u786e\u5b9a\u5ea6\u6821\u51c6\u826f\u597d\uff0c\u5e76\u63ed\u793a\u4e86\u53c2\u6570\u76f8\u5173\u6027\u548c\u591a\u6a21\u6001\u5206\u5e03\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u8c22\u7269\u5b9a\u91cf\u7684\u53ef\u9760\u6027\uff0c\u4e3aMRS\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2505.03648", "pdf": "https://arxiv.org/pdf/2505.03648", "abs": "https://arxiv.org/abs/2505.03648", "authors": ["Vladimir Fanaskov", "Ivan Oseledets"], "title": "Binding threshold units with artificial oscillatory neurons", "categories": ["q-bio.NC", "cs.AI", "cs.LG"], "comment": null, "summary": "Artificial Kuramoto oscillatory neurons were recently introduced as an\nalternative to threshold units. Empirical evidence suggests that oscillatory\nunits outperform threshold units in several tasks including unsupervised object\ndiscovery and certain reasoning problems. The proposed coupling mechanism for\nthese oscillatory neurons is heterogeneous, combining a generalized Kuramoto\nequation with standard coupling methods used for threshold units. In this\nresearch note, we present a theoretical framework that clearly distinguishes\noscillatory neurons from threshold units and establishes a coupling mechanism\nbetween them. We argue that, from a biological standpoint, oscillatory and\nthreshold units realise distinct aspects of neural coding: roughly, threshold\nunits model intensity of neuron firing, while oscillatory units facilitate\ninformation exchange by frequency modulation. To derive interaction between\nthese two types of units, we constrain their dynamics by focusing on dynamical\nsystems that admit Lyapunov functions. For threshold units, this leads to\nHopfield associative memory model, and for oscillatory units it yields a\nspecific form of generalized Kuramoto model. The resulting dynamical systems\ncan be naturally coupled to form a Hopfield-Kuramoto associative memory model,\nwhich also admits a Lyapunov function. Various forms of coupling are possible.\nNotably, oscillatory neurons can be employed to implement a low-rank correction\nto the weight matrix of a Hopfield network. This correction can be viewed\neither as a form of Hebbian learning or as a popular LoRA method used for\nfine-tuning of large language models. We demonstrate the practical realization\nof this particular coupling through illustrative toy experiments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u632f\u8361\u795e\u7ecf\u5143\u4e0e\u9608\u503c\u5355\u533a\u5206\u5f00\uff0c\u5e76\u5efa\u7acb\u4e86\u5b83\u4eec\u4e4b\u95f4\u7684\u8026\u5408\u673a\u5236\u3002\u4ece\u751f\u7269\u89d2\u5ea6\u51fa\u53d1\uff0c\u632f\u8361\u795e\u7ecf\u5143\u548c\u9608\u503c\u5355\u5b9e\u73b0\u4e0d\u540c\u7684\u795e\u7ecf\u7f16\u7801\uff0c\u524d\u8005\u901a\u8fc7\u9891\u7387\u8c03\u5236\u4fc3\u8fdb\u4fe1\u606f\u4ea4\u6362\uff0c\u540e\u8005\u6a21\u62df\u795e\u7ecf\u5143\u653e\u7535\u5f3a\u5ea6\u3002\u7814\u7a76\u8868\u660e\u53ef\u4ee5\u901a\u8fc7Lyapunov\u7ea6\u675f\u5c06\u8fd9\u4e24\u79cd\u5355\u8026\u5408\u8d77\u6765\uff0c\u5f62\u6210Hopfield-Kuramoto\u8054\u60f3\u8bb0\u5fc6\u6a21\u578b\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u533a\u522b\u632f\u8361\u795e\u7ecf\u5143\u4e0e\u9608\u503c\u5355\u7684\u7f16\u7801\u673a\u5236\uff0c\u5e76\u63a2\u7d22\u5b83\u4eec\u4e4b\u95f4\u7684\u8026\u5408\uff0c\u4ee5\u7ed3\u5408\u53cc\u65b9\u4f18\u52bf\uff0c\u63d0\u9ad8\u4efb\u52a1\u8868\u73b0\u3002", "method": "\u901a\u8fc7Lyapunov\u51fd\u6570\u7ea6\u675f\u632f\u8361\u795e\u7ecf\u5143\uff08\u5e7f\u4e49Kuramoto\u6a21\u578b\uff09\u548c\u9608\u503c\u5355\uff08Hopfield\u8054\u60f3\u8bb0\u5fc6\u6a21\u578b\uff09\u7684\u52a8\u6001\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e24\u8005\u8026\u5408\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86Hopfield-Kuramoto\u8054\u60f3\u8bb0\u5fc6\u8026\u5408\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u632f\u8361\u795e\u7ecf\u5143\u53ef\u4fee\u6b63Hopfield\u7f51\u7edc\u7684\u6743\u91cd\u77e9\u9635\uff0c\u5e76\u5728\u73a9\u5177\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u8be5\u8026\u5408\u3002", "conclusion": "\u8026\u5408\u673a\u5236\u4e3a\u4e24\u7c7b\u795e\u7ecf\u5143\u534f\u540c\u5de5\u4f5c\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5728\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528\u9608\u503c\u5355\uff0c\u5e76\u5728\u751f\u7269\u795e\u7ecf\u7f16\u7801\u7814\u7a76\u548c\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2505.03649", "pdf": "https://arxiv.org/pdf/2505.03649", "abs": "https://arxiv.org/abs/2505.03649", "authors": ["Bernardo Marenco", "Paola Bermolen", "Marcelo Fiori", "Federico Larroca", "Gonzalo Mateos"], "title": "Weighted Random Dot Product Graphs", "categories": ["stat.ML", "cs.LG", "math.CO", "math.PR"], "comment": "30 pages, 12 figures, code to generate Figures 3 to 12 available at\n  https://github.com/bmarenco/wrdpg", "summary": "Modeling of intricate relational patterns % through the analysis structures\nof network data has become a cornerstone of contemporary statistical research\nand related data science fields. Networks, represented as graphs, offer a\nnatural framework for this analysis. This paper extends the Random Dot Product\nGraph (RDPG) model to accommodate weighted graphs, markedly broadening the\nmodel's scope to scenarios where edges exhibit heterogeneous weight\ndistributions. We propose a nonparametric weighted (W)RDPG model that assigns a\nsequence of latent positions to each node. Inner products of these nodal\nvectors specify the moments of their incident edge weights' distribution via\nmoment-generating functions. In this way, and unlike prior art, the WRDPG can\ndiscriminate between weight distributions that share the same mean but differ\nin other higher-order moments. We derive statistical guarantees for an\nestimator of the nodal's latent positions adapted from the workhorse adjacency\nspectral embedding, establishing its consistency and asymptotic normality. We\nalso contribute a generative framework that enables sampling of graphs that\nadhere to a (prescribed or data-fitted) WRDPG, facilitating, e.g., the analysis\nand testing of observed graph metrics using judicious reference distributions.\nThe paper is organized to formalize the model's definition, the estimation (or\nnodal embedding) process and its guarantees, as well as the methodologies for\ngenerating weighted graphs, all complemented by illustrative and reproducible\nexamples showcasing the WRDPG's effectiveness in various network analytic\napplications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6269\u5c55\u4e86\u968f\u673a\u70b9\u79ef\u56fe\uff08RDPG\uff09\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u53c2\u6570\u52a0\u6743\uff08W\uff09RDPG\u6a21\u578b\uff0c\u652f\u6301\u6743\u91cd\u5f02\u8d28\u5206\u5e03\u7684\u56fe\u5206\u6790\uff0c\u5e76\u63d0\u4f9b\u4e86\u8282\u70b9\u9690\u4f4d\u7f6e\u4f30\u8ba1\u7684\u7edf\u8ba1\u4fdd\u8bc1\u548c\u751f\u6210\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u7684\u7f51\u7edc\u6a21\u578b\u65e0\u6cd5\u5145\u5206\u6355\u6349\u6743\u91cd\u7684\u5f02\u8d28\u6027\u5206\u5e03\uff0c\u5c24\u5176\u662f\u5f53\u4e0d\u540c\u6743\u91cd\u5206\u5e03\u5177\u6709\u76f8\u540c\u7684\u5747\u503c\u4f46\u9ad8\u9636\u77e9\u4e0d\u540c\u65f6\u3002\u6269\u5c55RDPG\u6a21\u578b\u4ee5\u5904\u7406\u52a0\u6743\u56fe\u7684\u9700\u6c42\u4fc3\u6210\u4e86\u672c\u7814\u7a76\u7684\u52a8\u673a\u3002", "method": "\u901a\u8fc7\u4e3a\u975e\u53c2\u6570\u52a0\u6743RDPG\u6a21\u578b\u5206\u914d\u8282\u70b9\u9690\u4f4d\u7f6e\u5e8f\u5217\uff0c\u5229\u7528\u8282\u70b9\u5411\u91cf\u7684\u5185\u79ef\u5b9a\u4e49\u8fb9\u6743\u91cd\u5206\u5e03\u7684\u77e9\u751f\u6210\u51fd\u6570\uff0c\u4ece\u800c\u533a\u5206\u4e0d\u540c\u9ad8\u9636\u77e9\u7684\u6743\u91cd\u5206\u5e03\u3002", "result": "\u63d0\u51fa\u4e86WRDPG\u6a21\u578b\u53ca\u5176\u8282\u70b9\u9690\u4f4d\u7f6e\u4f30\u8ba1\u7684\u7edf\u8ba1\u4fdd\u8bc1\uff08\u4e00\u81f4\u6027\u548c\u6e10\u8fd1\u6b63\u6001\u6027\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u751f\u6210\u6846\u67b6\u4ee5\u652f\u6301\u52a0\u6743\u56fe\u7684\u91c7\u6837\u548c\u5206\u6790\u3002", "conclusion": "WRDPG\u6a21\u578b\u6709\u6548\u6269\u5c55\u4e86RDPG\u7684\u5e94\u7528\u8303\u56f4\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7f51\u7edc\u5206\u6790\u573a\u666f\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u6848\u4f8b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2505.03492", "pdf": "https://arxiv.org/pdf/2505.03492", "abs": "https://arxiv.org/abs/2505.03492", "authors": ["Xiaoan Liu"], "title": "Augmenting Human Cognition through Everyday AR", "categories": ["cs.HC", "cs.AI"], "comment": "3 pages, 4 figures. Position paper accepted to CHI'25 Workshop\n  'Everyday AR through AI-in-the-Loop'", "summary": "As spatial computing and multimodal LLMs mature, AR is tending to become an\nintuitive \"thinking tool,\" embedding semantic and context-aware intelligence\ndirectly into everyday environments. This paper explores how always-on AR can\nseamlessly bridge digital cognition and physical affordances, enabling\nproactive, context-sensitive interactions that enhance human task performance\nand understanding.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5e38\u5f00\u5f0fAR\u5982\u4f55\u65e0\u7f1d\u8fde\u63a5\u6570\u5b57\u8ba4\u77e5\u4e0e\u7269\u7406\u529f\u80fd\uff0c\u4ee5\u589e\u5f3a\u4eba\u7c7b\u4efb\u52a1\u8868\u73b0\u548c\u7406\u89e3\u3002", "motivation": "\u968f\u7740\u7a7a\u95f4\u8ba1\u7b97\u548c\u591a\u6a21\u6001LLM\u7684\u53d1\u5c55\uff0cAR\u9010\u6e10\u6210\u4e3a\u4e00\u79cd\u76f4\u89c2\u7684\u2018\u601d\u7ef4\u5de5\u5177\u2019\uff0c\u5c06\u8bed\u4e49\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u667a\u80fd\u5d4c\u5165\u65e5\u5e38\u73af\u5883\u4e2d\u3002", "method": "\u7814\u7a76\u63a2\u7d22\u4e86\u5e38\u5f00\u5f0fAR\u6280\u672f\uff0c\u901a\u8fc7\u8bed\u4e49\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u667a\u80fd\u5b9e\u73b0\u4e3b\u52a8\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "result": "\u901a\u8fc7AR\u6280\u672f\u7684\u5e94\u7528\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u4eba\u7c7b\u7684\u4efb\u52a1\u8868\u73b0\u548c\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "\u5e38\u5f00\u5f0fAR\u4f5c\u4e3a\u6865\u6881\uff0c\u80fd\u591f\u5c06\u6570\u5b57\u667a\u80fd\u4e0e\u7269\u7406\u4e16\u754c\u65e0\u7f1d\u7ed3\u5408\uff0c\u4e3a\u4eba\u673a\u4ea4\u4e92\u5e26\u6765\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2505.03670", "pdf": "https://arxiv.org/pdf/2505.03670", "abs": "https://arxiv.org/abs/2505.03670", "authors": ["Katy Craig", "Nicol\u00e1s Garc\u00eda Trillos", "\u0110or\u0111e Nikoli\u0107"], "title": "Vector valued optimal transport: from dynamic to static formulations", "categories": ["math.AP", "cs.LG", "math.MG"], "comment": null, "summary": "Motivated by applications in classification of vector valued measures and\nmultispecies PDE, we develop a theory that unifies existing notions of vector\nvalued optimal transport, from dynamic formulations (\\`a la Benamou-Brenier) to\nstatic formulations (\\`a la Kantorovich). In our framework, vector valued\nmeasures are modeled as probability measures on a product space $\\mathbb{R}^d\n\\times G$, where $G$ is a weighted graph over a finite set of nodes and the\ngraph geometry strongly influences the associated dynamic and static distances.\nWe obtain sharp inequalities relating four notions of vector valued optimal\ntransport and prove that the distances are mutually bi-H\\\"older equivalent. We\ndiscuss the theoretical and practical advantages of each metric and indicate\npotential applications in multispecies PDE and data analysis. In particular,\none of the static formulations discussed in the paper is amenable to\nlinearization, a technique that has been explored in recent years to accelerate\nthe computation of pairwise optimal transport distances.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u5411\u91cf\u503c\u6700\u4f18\u8fd0\u8f93\u7406\u8bba\u7684\u6846\u67b6\uff0c\u5c06\u52a8\u6001\uff08Benamou-Brenier\uff09\u548c\u9759\u6001\uff08Kantorovich\uff09\u65b9\u6cd5\u6574\u5408\uff0c\u8bc1\u660e\u56db\u79cd\u8ddd\u79bb\u7684Bi-H\u00f6lder\u7b49\u4ef7\u6027\uff0c\u5e76\u63a2\u8ba8\u5176\u5728\u591a\u7269\u79cdPDE\u548c\u6570\u636e\u5206\u6790\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u5411\u91cf\u503c\u5ea6\u91cf\u7684\u5206\u7c7b\u548c\u591a\u7269\u79cd\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u7684\u5e94\u7528\u9700\u6c42\uff0c\u65e8\u5728\u7edf\u4e00\u73b0\u6709\u7684\u5411\u91cf\u503c\u6700\u4f18\u8fd0\u8f93\u7406\u8bba\u3002", "method": "\u901a\u8fc7\u5c06\u5411\u91cf\u503c\u5ea6\u91cf\u5efa\u6a21\u4e3a\u79ef\u7a7a\u95f4\u4e0a\u7684\u6982\u7387\u6d4b\u5ea6\uff08\u5176\u4e2dG\u4e3a\u52a0\u6743\u56fe\uff09\uff0c\u7ed3\u5408\u56fe\u7684\u51e0\u4f55\u7279\u6027\uff0c\u63d0\u51fa\u52a8\u6001\u548c\u9759\u6001\u8ddd\u79bb\u7406\u8bba\uff0c\u5e76\u8bc1\u660e\u5176\u7b49\u4ef7\u6027\u3002", "result": "\u83b7\u5f97\u4e86\u56db\u79cd\u5411\u91cf\u503c\u6700\u4f18\u8fd0\u8f93\u8ddd\u79bb\u7684Bi-H\u00f6lder\u7b49\u4ef7\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u6bcf\u79cd\u5ea6\u91cf\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u7269\u79cdPDE\u548c\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\uff0c\u5c24\u5176\u662f\u5176\u4e2d\u4e00\u79cd\u9759\u6001\u516c\u5f0f\u53ef\u901a\u8fc7\u7ebf\u6027\u5316\u52a0\u901f\u8ba1\u7b97\u3002"}}
{"id": "2505.03510", "pdf": "https://arxiv.org/pdf/2505.03510", "abs": "https://arxiv.org/abs/2505.03510", "authors": ["Ludovico Iannello", "Luca Ciampi", "Gabriele Lagani", "Fabrizio Tonelli", "Eleonora Crocco", "Lucio Maria Calcagnile", "Angelo Di Garbo", "Federico Cremisi", "Giuseppe Amato"], "title": "From Neurons to Computation: Biological Reservoir Computing for Pattern Recognition", "categories": ["cs.NE", "cs.AI", "cs.CV"], "comment": null, "summary": "In this paper, we introduce a novel paradigm for reservoir computing (RC)\nthat leverages a pool of cultured biological neurons as the reservoir\nsubstrate, creating a biological reservoir computing (BRC). This system\noperates similarly to an echo state network (ESN), with the key distinction\nthat the neural activity is generated by a network of cultured neurons, rather\nthan being modeled by traditional artificial computational units. The neuronal\nactivity is recorded using a multi-electrode array (MEA), which enables\nhigh-throughput recording of neural signals. In our approach, inputs are\nintroduced into the network through a subset of the MEA electrodes, while the\nremaining electrodes capture the resulting neural activity. This generates a\nnonlinear mapping of the input data to a high-dimensional biological feature\nspace, where distinguishing between data becomes more efficient and\nstraightforward, allowing a simple linear classifier to perform pattern\nrecognition tasks effectively. To evaluate the performance of our proposed\nsystem, we present an experimental study that includes various input patterns,\nsuch as positional codes, bars with different orientations, and a digit\nrecognition task. The results demonstrate the feasibility of using biological\nneural networks to perform tasks traditionally handled by artificial neural\nnetworks, paving the way for further exploration of biologically-inspired\ncomputing systems, with potential applications in neuromorphic engineering and\nbio-hybrid computing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u57f9\u517b\u751f\u7269\u795e\u7ecf\u5143\u4f5c\u4e3a\u50a8\u5907\u6c60\u7684\u65b0\u578b\u50a8\u5907\u8ba1\u7b97\uff08BRC\uff09\u8303\u5f0f\uff0c\u4e0e\u4f20\u7edf\u4eba\u5de5\u8ba1\u7b97\u5355\u5143\u4e0d\u540c\uff0c\u5176\u795e\u7ecf\u7f51\u7edc\u6d3b\u52a8\u7531\u751f\u7269\u795e\u7ecf\u5143\u4ea7\u751f\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u5206\u7c7b\u5668\u9ad8\u6548\u5b9e\u73b0\u6a21\u5f0f\u8bc6\u522b\u3002", "motivation": "\u63a2\u7d22\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u5728\u8ba1\u7b97\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u4e3a\u751f\u7269\u542f\u53d1\u5f0f\u8ba1\u7b97\u7cfb\u7edf\u548c\u751f\u7269\u6df7\u5408\u8ba1\u7b97\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "method": "\u901a\u8fc7\u591a\u7535\u6781\u9635\u5217\uff08MEA\uff09\u8bb0\u5f55\u57f9\u517b\u795e\u7ecf\u5143\u7684\u7535\u6d3b\u52a8\uff0c\u8f93\u5165\u6570\u636e\u901a\u8fc7\u90e8\u5206\u7535\u6781\u5f15\u5165\uff0c\u5176\u4f59\u7535\u6781\u6355\u83b7\u795e\u7ecf\u6d3b\u52a8\uff0c\u5f62\u6210\u975e\u7ebf\u6027\u6620\u5c04\u5230\u9ad8\u7ef4\u751f\u7269\u7279\u5f81\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBRC\u80fd\u6709\u6548\u5904\u7406\u4f4d\u7f6e\u7f16\u7801\u3001\u4e0d\u540c\u65b9\u5411\u7684\u6761\u5f62\u56fe\u4ee5\u53ca\u6570\u5b57\u8bc6\u522b\u4efb\u52a1\uff0c\u9a8c\u8bc1\u4e86\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u66ff\u4ee3\u4f20\u7edf\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u884c\u6027\u3002", "conclusion": "BRC\u4e3a\u795e\u7ecf\u5f62\u6001\u5de5\u7a0b\u548c\u751f\u7269\u6df7\u5408\u8ba1\u7b97\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5c55\u73b0\u4e86\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u5728\u8ba1\u7b97\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.03522", "pdf": "https://arxiv.org/pdf/2505.03522", "abs": "https://arxiv.org/abs/2505.03522", "authors": ["Haotong Cheng", "Zhiqi Zhang", "Hao Li", "Xinshang Zhang"], "title": "Optimization of Module Transferability in Single Image Super-Resolution: Universality Assessment and Cycle Residual Blocks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep learning has substantially advanced the Single Image Super-Resolution\n(SISR). However, existing researches have predominantly focused on raw\nperformance gains, with little attention paid to quantifying the\ntransferability of architectural components. In this paper, we introduce the\nconcept of \"Universality\" and its associated definitions which extend the\ntraditional notion of \"Generalization\" to encompass the modules' ease of\ntransferability, thus revealing the relationships between module universality\nand model generalizability. Then we propose the Universality Assessment\nEquation (UAE), a metric for quantifying how readily a given module could be\ntransplanted across models. Guided by the UAE results of standard residual\nblocks and other plug-and-play modules, we further design two optimized\nmodules, Cycle Residual Block (CRB) and Depth-Wise Cycle Residual Block (DCRB).\nThrough comprehensive experiments on natural-scene benchmarks, remote-sensing\ndatasets, extreme-industrial imagery and on-device deployments, we demonstrate\nthat networks embedded with the proposed plug-and-play modules outperform\nseveral state-of-the-arts, reaching a PSNR enhancement of up to 0.83dB or\nenabling a 71.3% reduction in parameters with negligible loss in reconstruction\nfidelity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u6a21\u5757'\u666e\u9002\u6027'\u7684\u6982\u5ff5\u53ca\u5176\u8bc4\u4f30\u65b9\u7a0b(UAE)\uff0c\u7528\u4e8e\u91cf\u5316\u6a21\u5757\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e24\u79cd\u4f18\u5316\u6a21\u5757(CRB\u548cDCRB)\uff0c\u5b9e\u9a8c\u8868\u660e\u8fd9\u4e9b\u6a21\u5757\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u6027\u80fd\u63d0\u5347\uff0c\u800c\u5ffd\u7565\u4e86\u67b6\u6784\u7ec4\u4ef6\u7684\u8fc1\u79fb\u80fd\u529b\u91cf\u5316\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u5b9a\u4e49'\u666e\u9002\u6027'\u548c\u8bbe\u8ba1\u8bc4\u4f30\u6307\u6807\uff0c\u63ed\u793a\u6a21\u5757\u666e\u9002\u6027\u4e0e\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u5173\u7cfb\u3002", "method": "\u63d0\u51faUAE\u4f5c\u4e3a\u6a21\u5757\u8fc1\u79fb\u80fd\u529b\u7684\u91cf\u5316\u6307\u6807\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u4e86\u4e24\u79cd\u4f18\u5316\u6a21\u5757(CRB\u548cDCRB)\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u8bbe\u5907\u90e8\u7f72\u4e2d\uff0c\u5d4c\u5165\u8fd9\u4e9b\u6a21\u5757\u7684\u7f51\u7edc\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cPSNR\u63d0\u5347\u6700\u9ad8\u8fbe0.83dB\uff0c\u6216\u53c2\u6570\u51cf\u5c1171.3%\u4e14\u91cd\u5efa\u8d28\u91cf\u51e0\u4e4e\u65e0\u635f\u3002", "conclusion": "\u6a21\u5757\u7684\u666e\u9002\u6027\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0cUAE\u548c\u4f18\u5316\u6a21\u5757\u7684\u8bbe\u8ba1\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.03702", "pdf": "https://arxiv.org/pdf/2505.03702", "abs": "https://arxiv.org/abs/2505.03702", "authors": ["Srecharan Selvam", "Abhishesh Silwal", "George Kanter"], "title": "Self-Supervised Learning for Robotic Leaf Manipulation: A Hybrid Geometric-Neural Approach", "categories": ["cs.RO", "cs.CV", "cs.LG", "I.2.10"], "comment": "13 pages, 9 figures", "summary": "Automating leaf manipulation in agricultural settings faces significant\nchallenges, including the variability of plant morphologies and deformable\nleaves. We propose a novel hybrid geometric-neural approach for autonomous leaf\ngrasping that combines traditional computer vision with neural networks through\nself-supervised learning. Our method integrates YOLOv8 for instance\nsegmentation and RAFT-Stereo for 3D depth estimation to build rich leaf\nrepresentations, which feed into both a geometric feature scoring pipeline and\na neural refinement module (GraspPointCNN). The key innovation is our\nconfidence-weighted fusion mechanism that dynamically balances the contribution\nof each approach based on prediction certainty. Our self-supervised framework\nuses the geometric pipeline as an expert teacher to automatically generate\ntraining data. Experiments demonstrate that our approach achieves an 88.0%\nsuccess rate in controlled environments and 84.7% in real greenhouse\nconditions, significantly outperforming both purely geometric (75.3%) and\nneural (60.2%) methods. This work establishes a new paradigm for agricultural\nrobotics where domain expertise is seamlessly integrated with machine learning\ncapabilities, providing a foundation for fully automated crop monitoring\nsystems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u51e0\u4f55-\u795e\u7ecf\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u7ed3\u5408\u4f20\u7edf\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u795e\u7ecf\u7f51\u7edc\uff0c\u5b9e\u73b0\u4e8688.0%\u7684\u6293\u53d6\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u7eaf\u51e0\u4f55\uff0875.3%\uff09\u548c\u7eaf\u795e\u7ecf\uff0860.2%\uff09\u65b9\u6cd5\u3002", "motivation": "\u519c\u4e1a\u573a\u666f\u4e2d\u53f6\u7247\u64cd\u4f5c\u7684\u81ea\u52a8\u5316\u9762\u4e34\u690d\u7269\u5f62\u6001\u591a\u53d8\u548c\u53f6\u7247\u53ef\u53d8\u5f62\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u7ed3\u5408\u9886\u57df\u4e13\u5bb6\u77e5\u8bc6\u4e0e\u673a\u5668\u5b66\u4e60\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u91c7\u7528YOLOv8\u5b9e\u4f8b\u5206\u5272\u548cRAFT-Stereo 3D\u6df1\u5ea6\u4f30\u8ba1\u6784\u5efa\u53f6\u7247\u8868\u5f81\uff0c\u7ed3\u5408\u51e0\u4f55\u7279\u5f81\u8bc4\u5206\u7ba1\u9053\u548c\u795e\u7ecf\u7ec6\u5316\u6a21\u5757\uff08GraspPointCNN\uff09\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u52a0\u6743\u878d\u5408\u52a8\u6001\u5e73\u8861\u4e24\u8005\u8d21\u732e\uff0c\u5e76\u5229\u7528\u51e0\u4f55\u7ba1\u9053\u4f5c\u4e3a\u4e13\u5bb6\u6559\u5e08\u751f\u6210\u81ea\u76d1\u7763\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728\u53d7\u63a7\u73af\u5883\u548c\u771f\u5b9e\u6e29\u5ba4\u6761\u4ef6\u4e0b\uff0c\u5206\u522b\u8fbe\u523088.0%\u548c84.7%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u7eaf\u51e0\u4f55\uff0875.3%\uff09\u548c\u7eaf\u795e\u7ecf\u65b9\u6cd5\uff0860.2%\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u519c\u4e1a\u673a\u5668\u4eba\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u878d\u5408\u4e13\u5bb6\u77e5\u8bc6\u4e0e\u673a\u5668\u5b66\u4e60\u7684\u65b0\u8303\u5f0f\uff0c\u4e3a\u5168\u81ea\u52a8\u4f5c\u7269\u76d1\u6d4b\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.03557", "pdf": "https://arxiv.org/pdf/2505.03557", "abs": "https://arxiv.org/abs/2505.03557", "authors": ["Koray Ulusan", "Benjamin Kiefer"], "title": "Generating Synthetic Data via Augmentations for Improved Facial Resemblance in DreamBooth and InstantID", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to CVPR 2025 Workshop \"Synthetic Data for Computer Vision\n  Workshop\", https://syndata4cv.github.io/", "summary": "The personalization of Stable Diffusion for generating professional portraits\nfrom amateur photographs is a burgeoning area, with applications in various\ndownstream contexts. This paper investigates the impact of augmentations on\nimproving facial resemblance when using two prominent personalization\ntechniques: DreamBooth and InstantID. Through a series of experiments with\ndiverse subject datasets, we assessed the effectiveness of various augmentation\nstrategies on the generated headshots' fidelity to the original subject. We\nintroduce FaceDistance, a wrapper around FaceNet, to rank the generations based\non facial similarity, which aided in our assessment. Ultimately, this research\nprovides insights into the role of augmentations in enhancing facial\nresemblance in SDXL-generated portraits, informing strategies for their\neffective deployment in downstream applications.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5982\u4f55\u4f7f\u7528\u589e\u5f3a\u6280\u672f\u63d0\u5347Stable Diffusion\uff08SDXL\uff09\u751f\u6210\u8096\u50cf\u7684\u9762\u90e8\u76f8\u4f3c\u5ea6\uff0c\u91cd\u70b9\u5173\u6ce8DreamBooth\u548cInstantID\u4e24\u79cd\u4e2a\u6027\u5316\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7FaceDistance\u8bc4\u4f30\u751f\u6210\u6548\u679c\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5728\u4e1a\u4f59\u7167\u7247\u751f\u6210\u7684\u8096\u50cf\u4e2d\u63d0\u9ad8\u9762\u90e8\u76f8\u4f3c\u5ea6\uff0c\u4ee5\u6ee1\u8db3\u4e13\u4e1a\u8096\u50cf\u751f\u6210\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528DreamBooth\u548cInstantID\u4e24\u79cd\u4e2a\u6027\u5316\u6280\u672f\uff0c\u7ed3\u5408\u591a\u79cd\u589e\u5f3a\u7b56\u7565\uff0c\u5e76\u4f7f\u7528FaceDistance\uff08\u57fa\u4e8eFaceNet\u7684\u5c01\u88c5\uff09\u91cf\u5316\u8bc4\u4f30\u751f\u6210\u8096\u50cf\u7684\u9762\u90e8\u76f8\u4f3c\u5ea6\u3002", "result": "\u7814\u7a76\u8868\u660e\u589e\u5f3a\u6280\u672f\u80fd\u663e\u8457\u63d0\u5347\u751f\u6210\u8096\u50cf\u7684\u9762\u90e8\u76f8\u4f3c\u6027\uff0c\u4e3a\u4e0b\u6e38\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u7b56\u7565\u3002", "conclusion": "\u589e\u5f3a\u6280\u672f\u5bf9\u63d0\u5347SDXL\u751f\u6210\u8096\u50cf\u7684\u9762\u90e8\u76f8\u4f3c\u5ea6\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u9700\u6839\u636e\u5177\u4f53\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684\u589e\u5f3a\u7b56\u7565\u3002"}}
{"id": "2505.03703", "pdf": "https://arxiv.org/pdf/2505.03703", "abs": "https://arxiv.org/abs/2505.03703", "authors": ["Fran\u00e7ois Role", "S\u00e9bastien Meyer", "Victor Amblard"], "title": "Fill the Gap: Quantifying and Reducing the Modality Gap in Image-Text Representation Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Vision-language models (VLMs) allow to embed texts and images in a shared\nrepresentation space. However, it has been shown that these models are subject\nto a modality gap phenomenon meaning there exists a clear separation between\nthe embeddings from one modality and another in the embedding space. While this\nmisalignment is detrimental for downstream tasks such as multimodal retrieval,\nmultimodal clustering or zero-shot classification, etc. no generic and\npractical methods have so far been proposed to assess it precisely and even\nreduce it. We therefore propose novel measures and effective techniques\n(spectral- and optimal transport-based methods) to achieve this goal. Extensive\nexperiments conducted on several image-text datasets and models demonstrate\ntheir effectiveness and beneficial effects on downstream tasks. Our code is\navailable at the URL provided in the paper's abstract.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u548c\u6307\u6807\u6765\u8861\u91cf\u548c\u51cf\u5c11\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6a21\u6001\u95f4\u9699\u95ee\u9898\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6a21\u6001\u95f4\u9699\uff08modality gap\uff09\u95ee\u9898\u5bfc\u81f4\u8de8\u6a21\u6001\u8868\u793a\u4e0d\u5339\u914d\uff0c\u5f71\u54cd\u4e86\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u591a\u6a21\u6001\u68c0\u7d22\u3001\u805a\u7c7b\u548c\u96f6\u6837\u672c\u5206\u7c7b\uff09\u7684\u6027\u80fd\u3002\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u548c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u57fa\u4e8e\u8c31\u65b9\u6cd5\u548c\u6700\u4f18\u4f20\u8f93\u6280\u672f\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u6d4b\u91cf\u5e76\u51cf\u5c11\u6a21\u6001\u95f4\u9699\u3002", "result": "\u5728\u591a\u4e2a\u56fe\u50cf-\u6587\u672c\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u6a21\u6001\u95f4\u9699\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6a21\u6001\u95f4\u9699\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8bc4\u4f30\u548c\u4f18\u5316\u65b9\u6848\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.03704", "pdf": "https://arxiv.org/pdf/2505.03704", "abs": "https://arxiv.org/abs/2505.03704", "authors": ["Kiichi Obuchi", "Yuta Yahagi", "Kiyohiko Toyama", "Shukichi Tanaka", "Kota Matsui"], "title": "Multi-modal cascade feature transfer for polymer property prediction", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "In this paper, we propose a novel transfer learning approach called\nmulti-modal cascade model with feature transfer for polymer property\nprediction.Polymers are characterized by a composite of data in several\ndifferent formats, including molecular descriptors and additive information as\nwell as chemical structures. However, in conventional approaches, prediction\nmodels were often constructed using each type of data separately. Our model\nenables more accurate prediction of physical properties for polymers by\ncombining features extracted from the chemical structure by graph convolutional\nneural networks (GCN) with features such as molecular descriptors and additive\ninformation. The predictive performance of the proposed method is empirically\nevaluated using several polymer datasets. We report that the proposed method\nshows high predictive performance compared to the baseline conventional\napproach using a single feature.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u591a\u6a21\u6001\u7ea7\u8054\u6a21\u578b\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u63d0\u53d6\u7684\u5316\u5b66\u7ed3\u6784\u7279\u5f81\u4e0e\u5206\u5b50\u63cf\u8ff0\u7b26\u7b49\u6570\u636e\uff0c\u63d0\u5347\u805a\u5408\u7269\u7269\u7406\u6027\u8d28\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u5355\u72ec\u4f7f\u7528\u4e0d\u540c\u7c7b\u578b\u7684\u6570\u636e\u6784\u5efa\u9884\u6d4b\u6a21\u578b\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u805a\u5408\u7269\u591a\u6a21\u6001\u6570\u636e\u7684\u6f5c\u529b\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u6570\u636e\u7279\u5f81\uff0c\u63d0\u9ad8\u805a\u5408\u7269\u7269\u7406\u6027\u8d28\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u7ea7\u8054\u6a21\u578b\uff0c\u5229\u7528\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08GCN\uff09\u63d0\u53d6\u5316\u5b66\u7ed3\u6784\u7279\u5f81\uff0c\u5e76\u5c06\u5176\u4e0e\u5206\u5b50\u63cf\u8ff0\u7b26\u3001\u6dfb\u52a0\u5242\u4fe1\u606f\u7b49\u5176\u4ed6\u7279\u5f81\u7ed3\u5408\uff0c\u6784\u5efa\u7efc\u5408\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u805a\u5408\u7269\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4ec5\u4f7f\u7528\u5355\u4e00\u7279\u5f81\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u6709\u6548\u63d0\u5347\u4e86\u805a\u5408\u7269\u7269\u7406\u6027\u8d28\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u76f8\u6bd4\u4e8e\u4f20\u7edf\u5355\u7279\u5f81\u65b9\u6cd5\u7684\u4f18\u52bf\u3002"}}
{"id": "2505.03710", "pdf": "https://arxiv.org/pdf/2505.03710", "abs": "https://arxiv.org/abs/2505.03710", "authors": ["Kevin Tan", "Wei Fan", "Yuting Wei"], "title": "Actor-Critics Can Achieve Optimal Sample Efficiency", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": "Accepted to ICML 2025", "summary": "Actor-critic algorithms have become a cornerstone in reinforcement learning\n(RL), leveraging the strengths of both policy-based and value-based methods.\nDespite recent progress in understanding their statistical efficiency, no\nexisting work has successfully learned an $\\epsilon$-optimal policy with a\nsample complexity of $O(1/\\epsilon^2)$ trajectories with general function\napproximation when strategic exploration is necessary.\n  We address this open problem by introducing a novel actor-critic algorithm\nthat attains a sample-complexity of $O(dH^5 \\log|\\mathcal{A}|/\\epsilon^2 + d\nH^4 \\log|\\mathcal{F}|/ \\epsilon^2)$ trajectories, and accompanying $\\sqrt{T}$\nregret when the Bellman eluder dimension $d$ does not increase with $T$ at more\nthan a $\\log T$ rate.\n  Here, $\\mathcal{F}$ is the critic function class, $\\mathcal{A}$ is the action\nspace, and $H$ is the horizon in the finite horizon MDP setting. Our algorithm\nintegrates optimism, off-policy critic estimation targeting the optimal\nQ-function, and rare-switching policy resets.\n  We extend this to the setting of Hybrid RL, showing that initializing the\ncritic with offline data yields sample efficiency gains compared to purely\noffline or online RL. Further, utilizing access to offline data, we provide a\n\\textit{non-optimistic} provably efficient actor-critic algorithm that only\nadditionally requires $N_{\\text{off}} \\geq c_{\\text{off}}^*dH^4/\\epsilon^2$ in\nexchange for omitting optimism, where $c_{\\text{off}}^*$ is the single-policy\nconcentrability coefficient and $N_{\\text{off}}$ is the number of offline\nsamples. This addresses another open problem in the literature. We further\nprovide numerical experiments to support our theoretical findings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684actor-critic\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u6837\u672c\u590d\u6742\u5ea6\u96be\u4ee5\u8fbe\u5230O(1/\u03b5\u00b2)\u7684\u5f00\u653e\u95ee\u9898\uff0c\u540c\u65f6\u5728\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\uff08Hybrid RL\uff09\u4e2d\u5c55\u793a\u4e86\u79bb\u7ebf\u6570\u636e\u7684\u4f18\u52bf\u3002", "motivation": "\u5c3d\u7ba1actor-critic\u7b97\u6cd5\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5728\u9700\u8981\u6218\u7565\u63a2\u7d22\u7684\u901a\u7528\u51fd\u6570\u903c\u8fd1\u4e0b\u5b9e\u73b0O(1/\u03b5\u00b2)\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u65b0\u7b97\u6cd5\u7ed3\u5408\u4e86\u4e50\u89c2\u63a2\u7d22\u3001\u79bb\u7b56\u7565critic\u4f30\u8ba1\u548c\u4f4e\u9891\u7b56\u7565\u91cd\u7f6e\uff0c\u540c\u65f6\u5728\u6df7\u5408RL\u4e2d\u5229\u7528\u79bb\u7ebf\u6570\u636e\u63d0\u5347\u6548\u7387\u3002", "result": "\u7b97\u6cd5\u5b9e\u73b0\u4e86O(dH\u2075log|A|/\u03b5\u00b2 + dH\u2074log|F|/\u03b5\u00b2)\u7684\u6837\u672c\u590d\u6742\u5ea6\u548c\u221aT\u7684\u9057\u61be\uff0c\u79bb\u7ebf\u6570\u636e\u8fdb\u4e00\u6b65\u964d\u4f4e\u4e86\u6837\u672c\u9700\u6c42\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e0d\u4ec5\u586b\u8865\u4e86\u7406\u8bba\u7a7a\u767d\uff0c\u8fd8\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u4e3a\u6df7\u5408RL\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.03562", "pdf": "https://arxiv.org/pdf/2505.03562", "abs": "https://arxiv.org/abs/2505.03562", "authors": ["Jiwoo Jeong", "Kirok Kim", "Wooju Kim", "Nam-Joon Kim"], "title": "Real-Time Person Image Synthesis Using a Flow Matching Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Pose-Guided Person Image Synthesis (PGPIS) generates realistic person images\nconditioned on a target pose and a source image. This task plays a key role in\nvarious real-world applications, such as sign language video generation, AR/VR,\ngaming, and live streaming. In these scenarios, real-time PGPIS is critical for\nproviding immediate visual feedback and maintaining user immersion.However,\nachieving real-time performance remains a significant challenge due to the\ncomplexity of synthesizing high-fidelity images from diverse and dynamic human\nposes. Recent diffusion-based methods have shown impressive image quality in\nPGPIS, but their slow sampling speeds hinder deployment in time-sensitive\napplications. This latency is particularly problematic in tasks like generating\nsign language videos during live broadcasts, where rapid image updates are\nrequired. Therefore, developing a fast and reliable PGPIS model is a crucial\nstep toward enabling real-time interactive systems. To address this challenge,\nwe propose a generative model based on flow matching (FM). Our approach enables\nfaster, more stable, and more efficient training and sampling. Furthermore, the\nproposed model supports conditional generation and can operate in latent space,\nmaking it especially suitable for real-time PGPIS applications where both speed\nand quality are critical. We evaluate our proposed method, Real-Time Person\nImage Synthesis Using a Flow Matching Model (RPFM), on the widely used\nDeepFashion dataset for PGPIS tasks. Our results show that RPFM achieves\nnear-real-time sampling speeds while maintaining performance comparable to the\nstate-of-the-art models. Our methodology trades off a slight, acceptable\ndecrease in generated-image accuracy for over a twofold increase in generation\nspeed, thereby ensuring real-time performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\uff08FM\uff09\u7684\u751f\u6210\u6a21\u578bRPFM\uff0c\u7528\u4e8e\u5b9e\u65f6\u751f\u6210\u59ff\u6001\u5f15\u5bfc\u7684\u4eba\u7269\u56fe\u50cf\uff08PGPIS\uff09\uff0c\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u751f\u6210\u901f\u5ea6\u3002", "motivation": "\u5b9e\u65f6\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4eba\u7269\u56fe\u50cf\u5728\u5e94\u7528\uff08\u5982\u624b\u8bed\u89c6\u9891\u3001AR/VR\u7b49\uff09\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6269\u6563\u6a21\u578b\u901f\u5ea6\u6162\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u6027\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u751f\u6210\u6a21\u578b\uff0c\u652f\u6301\u9690\u7a7a\u95f4\u6761\u4ef6\u751f\u6210\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u7a33\u5b9a\u7684\u8bad\u7ec3\u4e0e\u91c7\u6837\u3002", "result": "\u5728DeepFashion\u6570\u636e\u96c6\u4e0a\uff0cRPFM\u5728\u751f\u6210\u901f\u5ea6\u4e0a\u63d0\u5347\u4e24\u500d\u4ee5\u4e0a\uff0c\u63a5\u8fd1\u5b9e\u65f6\uff0c\u540c\u65f6\u56fe\u50cf\u8d28\u91cf\u4e0e\u73b0\u6709\u6700\u4f73\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "RPFM\u901a\u8fc7\u8f7b\u5fae\u727a\u7272\u751f\u6210\u7cbe\u5ea6\u6362\u53d6\u4e86\u663e\u8457\u7684\u5b9e\u65f6\u6027\u63d0\u5347\uff0c\u9002\u5408\u901f\u5ea6\u4e0e\u8d28\u91cf\u5e76\u91cd\u7684\u5b9e\u65f6PGPIS\u5e94\u7528\u3002"}}
{"id": "2505.03717", "pdf": "https://arxiv.org/pdf/2505.03717", "abs": "https://arxiv.org/abs/2505.03717", "authors": ["Richard Y. Zhang"], "title": "Nonnegative Low-rank Matrix Recovery Can Have Spurious Local Minima", "categories": ["math.OC", "cs.LG", "stat.ML"], "comment": null, "summary": "The classical low-rank matrix recovery problem is well-known to exhibit\n\\emph{benign nonconvexity} under the restricted isometry property (RIP): local\noptimization is guaranteed to converge to the global optimum, where the ground\ntruth is recovered. We investigate whether benign nonconvexity continues to\nhold when the factor matrices are constrained to be elementwise nonnegative --\na common practical requirement. In the simple setting of a rank-1 nonnegative\nground truth, we confirm that benign nonconvexity holds in the fully-observed\ncase with RIP constant $\\delta=0$. Surprisingly, however, this property fails\nto extend to the partially-observed case with any arbitrarily small RIP\nconstant $\\delta\\to0^{+}$, irrespective of rank overparameterization. This\nfinding exposes a critical theoretical gap: the continuity argument widely used\nto explain the empirical robustness of low-rank matrix recovery fundamentally\nbreaks down once nonnegative constraints are imposed.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5e26\u975e\u8d1f\u7ea6\u675f\u7684\u4f4e\u79e9\u77e9\u9635\u6062\u590d\u95ee\u9898\uff0c\u53d1\u73b0\u5728\u5b8c\u5168\u89c2\u6d4b\u60c5\u51b5\u4e0b\u79e9\u4e3a1\u65f6\u4ecd\u5177\u826f\u6027\u975e\u51f8\u6027\uff0c\u4f46\u5728\u90e8\u5206\u89c2\u6d4b\u60c5\u51b5\u4e0b\u5373\u4f7fRIP\u5e38\u6570\u8d8b\u8fd1\u4e8e0\u65f6\u5931\u6548\uff0c\u63ed\u793a\u4e86\u7406\u8bba\u4e0a\u7684\u91cd\u8981\u5dee\u5f02\u3002", "motivation": "\u63a2\u8ba8\u4f4e\u79e9\u77e9\u9635\u6062\u590d\u5728\u5f15\u5165\u975e\u8d1f\u7ea6\u675f\u540e\u662f\u5426\u4ecd\u4fdd\u6301\u5176\u826f\u6027\u975e\u51f8\u7279\u6027\uff0c\u586b\u8865\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u5206\u6790\u79e9\u4e3a1\u7684\u975e\u8d1f\u56e0\u5b50\u77e9\u9635\u5728\u5b8c\u5168\u89c2\u6d4b\u548c\u90e8\u5206\u89c2\u6d4b\u60c5\u51b5\u4e0b\u7684\u884c\u4e3a\uff0c\u5bf9\u6bd4\u6709\u65e0RIP\u6761\u4ef6\u4e0b\u7684\u6536\u655b\u6027\u3002", "result": "\u5b8c\u5168\u89c2\u6d4b\u4e0b\uff08RIP\u5e38\u6570\u03b4=0\uff09\u4fdd\u6301\u826f\u6027\u975e\u51f8\u6027\uff0c\u4f46\u90e8\u5206\u89c2\u6d4b\u4e0b\uff08\u03b4\u21920\u207a\uff09\u5931\u6548\uff0c\u5373\u4f7f\u79e9\u8fc7\u53c2\u6570\u5316\u4e5f\u65e0\u6d4e\u4e8e\u4e8b\u3002", "conclusion": "\u975e\u8d1f\u7ea6\u675f\u4f1a\u7834\u574f\u4f4e\u79e9\u77e9\u9635\u6062\u590d\u7684\u8fde\u7eed\u6027\u8bba\u8bc1\u6846\u67b6\uff0c\u5bfc\u81f4\u90e8\u5206\u89c2\u6d4b\u573a\u666f\u4e0b\u826f\u6027\u975e\u51f8\u6027\u4e0d\u590d\u5b58\u5728\u3002"}}
{"id": "2505.03574", "pdf": "https://arxiv.org/pdf/2505.03574", "abs": "https://arxiv.org/abs/2505.03574", "authors": ["Sahana Chennabasappa", "Cyrus Nikolaidis", "Daniel Song", "David Molnar", "Stephanie Ding", "Shengye Wan", "Spencer Whitman", "Lauren Deason", "Nicholas Doucette", "Abraham Montilla", "Alekhya Gampa", "Beto de Paola", "Dominik Gabi", "James Crnkovich", "Jean-Christophe Testud", "Kat He", "Rashnil Chaturvedi", "Wu Zhou", "Joshua Saxe"], "title": "LlamaFirewall: An open source guardrail system for building secure AI agents", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have evolved from simple chatbots into\nautonomous agents capable of performing complex tasks such as editing\nproduction code, orchestrating workflows, and taking higher-stakes actions\nbased on untrusted inputs like webpages and emails. These capabilities\nintroduce new security risks that existing security measures, such as model\nfine-tuning or chatbot-focused guardrails, do not fully address. Given the\nhigher stakes and the absence of deterministic solutions to mitigate these\nrisks, there is a critical need for a real-time guardrail monitor to serve as a\nfinal layer of defense, and support system level, use case specific safety\npolicy definition and enforcement. We introduce LlamaFirewall, an open-source\nsecurity focused guardrail framework designed to serve as a final layer of\ndefense against security risks associated with AI Agents. Our framework\nmitigates risks such as prompt injection, agent misalignment, and insecure code\nrisks through three powerful guardrails: PromptGuard 2, a universal jailbreak\ndetector that demonstrates clear state of the art performance; Agent Alignment\nChecks, a chain-of-thought auditor that inspects agent reasoning for prompt\ninjection and goal misalignment, which, while still experimental, shows\nstronger efficacy at preventing indirect injections in general scenarios than\npreviously proposed approaches; and CodeShield, an online static analysis\nengine that is both fast and extensible, aimed at preventing the generation of\ninsecure or dangerous code by coding agents. Additionally, we include\neasy-to-use customizable scanners that make it possible for any developer who\ncan write a regular expression or an LLM prompt to quickly update an agent's\nsecurity guardrails.", "AI": {"tldr": "LlamaFirewall\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5b89\u5168\u62a4\u680f\u6846\u67b6\uff0c\u65e8\u5728\u4f5c\u4e3aAI\u4ee3\u7406\u76f8\u5173\u5b89\u5168\u98ce\u9669\u7684\u6700\u540e\u4e00\u5c42\u9632\u5fa1\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\uff0cAI\u4ee3\u7406\u53ef\u80fd\u6267\u884c\u9ad8\u98ce\u9669\u4efb\u52a1\uff0c\u800c\u73b0\u6709\u5b89\u5168\u63aa\u65bd\u65e0\u6cd5\u5b8c\u5168\u5e94\u5bf9\u8fd9\u4e9b\u65b0\u98ce\u9669\uff0c\u56e0\u6b64\u9700\u8981\u5b9e\u65f6\u62a4\u680f\u76d1\u63a7\u6765\u4fdd\u969c\u5b89\u5168\u3002", "method": "\u6846\u67b6\u901a\u8fc7\u4e09\u4e2a\u6838\u5fc3\u62a4\u680f\u6765\u5b9e\u73b0\u5b89\u5168\u9632\u62a4\uff1aPromptGuard 2\uff08\u901a\u7528\u7684\u8d8a\u72f1\u68c0\u6d4b\u5668\uff09\u3001Agent Alignment Checks\uff08\u601d\u7ef4\u94fe\u5ba1\u8ba1\u5de5\u5177\uff09\u548cCodeShield\uff08\u5728\u7ebf\u9759\u6001\u5206\u6790\u5f15\u64ce\uff09\u3002", "result": "LlamaFirewall\u5728\u9632\u6b62\u63d0\u793a\u6ce8\u5165\u3001\u4ee3\u7406\u4e0d\u5bf9\u9f50\u548c\u4e0d\u5b89\u5168\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662fPromptGuard 2\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "LlamaFirewall\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u7f13\u89e3AI\u4ee3\u7406\u7684\u5b89\u5168\u98ce\u9669\uff0c\u5e76\u652f\u6301\u5f00\u53d1\u8005\u81ea\u5b9a\u4e49\u5b89\u5168\u89c4\u5219\u3002"}}
{"id": "2505.03738", "pdf": "https://arxiv.org/pdf/2505.03738", "abs": "https://arxiv.org/abs/2505.03738", "authors": ["Jialong Li", "Xuxin Cheng", "Tianshu Huang", "Shiqi Yang", "Ri-Zhao Qiu", "Xiaolong Wang"], "title": "AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "website: https://amo-humanoid.github.io", "summary": "Humanoid robots derive much of their dexterity from hyper-dexterous\nwhole-body movements, enabling tasks that require a large operational\nworkspace: such as picking objects off the ground. However, achieving these\ncapabilities on real humanoids remains challenging due to their high degrees of\nfreedom (DoF) and nonlinear dynamics. We propose Adaptive Motion Optimization\n(AMO), a framework that integrates sim-to-real reinforcement learning (RL) with\ntrajectory optimization for real-time, adaptive whole-body control. To mitigate\ndistribution bias in motion imitation RL, we construct a hybrid AMO dataset and\ntrain a network capable of robust, on-demand adaptation to potentially O.O.D.\ncommands. We validate AMO in simulation and on a 29-DoF Unitree G1 humanoid\nrobot, demonstrating superior stability and an expanded workspace compared to\nstrong baselines. Finally, we show that AMO's consistent performance supports\nautonomous task execution via imitation learning, underscoring the system's\nversatility and robustness.", "AI": {"tldr": "\u6458\u8981\u4ecb\u7ecd\u4e86\u81ea\u9002\u5e94\u8fd0\u52a8\u4f18\u5316\uff08AMO\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4eff\u4eba\u673a\u5668\u4eba\u9ad8\u81ea\u7531\u5ea6\u548c\u975e\u7ebf\u6027\u52a8\u6001\u5e26\u6765\u7684\u63a7\u5236\u6311\u6218\uff0c\u901a\u8fc7\u7ed3\u5408\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5f3a\u5316\u5b66\u4e60\u4e0e\u8f68\u8ff9\u4f18\u5316\uff0c\u5b9e\u73b0\u5b9e\u65f6\u5168\u8eab\u63a7\u5236\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u5176\u7a33\u5b9a\u6027\u548c\u6269\u5c55\u7684\u5de5\u4f5c\u7a7a\u95f4\u3002", "motivation": "\u89e3\u51b3\u4eff\u4eba\u673a\u5668\u4eba\u5728\u9ad8\u81ea\u7531\u5ea6\u548c\u975e\u7ebf\u6027\u52a8\u6001\u4e0b\u7684\u5b9e\u65f6\u81ea\u9002\u5e94\u5168\u8eab\u63a7\u5236\u95ee\u9898\uff0c\u4ee5\u6269\u5c55\u5176\u5de5\u4f5c\u7a7a\u95f4\u5e76\u63d0\u5347\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faAMO\u6846\u67b6\uff0c\u7ed3\u5408\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e0e\u8f68\u8ff9\u4f18\u5316\uff0c\u6784\u5efa\u6df7\u5408\u6570\u636e\u96c6\u8bad\u7ec3\u7f51\u7edc\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u81ea\u9002\u5e94\u63a7\u5236\u3002", "result": "\u572829\u81ea\u7531\u5ea6\u7684Unitree G1\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0cAMO\u5c55\u73b0\u51fa\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u7684\u7a33\u5b9a\u6027\u548c\u5de5\u4f5c\u7a7a\u95f4\u6269\u5c55\u80fd\u529b\uff0c\u5e76\u652f\u6301\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u5b9e\u73b0\u81ea\u4e3b\u4efb\u52a1\u6267\u884c\u3002", "conclusion": "AMO\u6846\u67b6\u901a\u8fc7\u7ed3\u5408RL\u4e0e\u8f68\u8ff9\u4f18\u5316\uff0c\u4e0d\u4ec5\u89e3\u51b3\u4e86\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u7684\u63a7\u5236\u6311\u6218\uff0c\u8fd8\u5c55\u793a\u4e86\u5176\u591a\u529f\u80fd\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u4eff\u4eba\u673a\u5668\u4eba\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.03584", "pdf": "https://arxiv.org/pdf/2505.03584", "abs": "https://arxiv.org/abs/2505.03584", "authors": ["Lucas Anastasiou", "Anna De Liddo"], "title": "BCause: Human-AI collaboration to improve hybrid mapping and ideation in argumentation-grounded deliberation", "categories": ["cs.HC", "cs.AI", "cs.CY", "I.2"], "comment": "5 pages, 3 figures", "summary": "Public deliberation, as in open discussion of issues of public concern, often\nsuffers from scattered and shallow discourse, poor sensemaking, and a\ndisconnect from actionable policy outcomes. This paper introduces BCause, a\ndiscussion system leveraging generative AI and human-machine collaboration to\ntransform unstructured dialogue around public issues (such as urban living,\npolicy changes, and current socio-economic transformations) into structured,\nactionable democratic processes. We present three innovations: (i) importing\nand transforming unstructured transcripts into argumentative discussions, (ii)\ngeo-deliberated problem-sensing via a Telegram bot for local issue reporting,\nand (iii) smart reporting with customizable widgets (e.g., summaries, topic\nmodelling, policy recommendations, clustered arguments). The system's human-AI\npartnership preserves critical human participation to ensure ethical oversight,\ncontextual relevance, and creative synthesis.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86BCause\u8ba8\u8bba\u7cfb\u7edf\uff0c\u5229\u7528\u751f\u6210\u5f0fAI\u548c\u4eba\u673a\u534f\u4f5c\uff0c\u5c06\u516c\u5171\u8bae\u9898\u7684\u65e0\u5e8f\u5bf9\u8bdd\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u3001\u53ef\u64cd\u4f5c\u7684\u6c11\u4e3b\u8fdb\u7a0b\u3002", "motivation": "\u516c\u5171\u8ba8\u8bba\u5e38\u56e0\u4f4e\u6548\u3001\u80a4\u6d45\u3001\u7f3a\u4e4f\u884c\u52a8\u5bfc\u5411\u800c\u53d7\u9650\uff0cBCause\u65e8\u5728\u901a\u8fc7\u6280\u672f\u6539\u8fdb\u8fd9\u4e00\u73b0\u72b6\u3002", "method": "\u7cfb\u7edf\u901a\u8fc7\u4e09\u5927\u521b\u65b0\u5b9e\u73b0\u76ee\u6807\uff1a\u5c06\u65e0\u5e8f\u5bf9\u8bdd\u8f6c\u4e3a\u8bba\u8fa9\u8ba8\u8bba\u3001\u57fa\u4e8e\u5730\u7406\u7684Telegram\u673a\u5668\u4eba\u8bae\u9898\u62a5\u544a\u3001\u667a\u80fd\u62a5\u544a\u751f\u6210\u5de5\u5177\u3002", "result": "BCause\u7cfb\u7edf\u5b9e\u73b0\u4e86\u7ed3\u6784\u5316\u516c\u5171\u8ba8\u8bba\uff0c\u5e76\u4fdd\u6301\u4e86\u4eba\u7c7b\u53c2\u4e0e\u4ee5\u786e\u4fdd\u4f26\u7406\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u3002", "conclusion": "\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\uff0cBCause\u6210\u529f\u63d0\u5347\u4e86\u516c\u5171\u8ba8\u8bba\u7684\u8d28\u91cf\u548c\u884c\u52a8\u5bfc\u5411\u6027\u3002"}}
{"id": "2505.03586", "pdf": "https://arxiv.org/pdf/2505.03586", "abs": "https://arxiv.org/abs/2505.03586", "authors": ["Songchen Fu", "Siang Chen", "Shaojing Zhao", "Letian Bai", "Ta Li", "Yonghong Yan"], "title": "Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation", "categories": ["cs.MA", "cs.AI", "68T07 (Primary), 68T20, 68T42 (Secondary)", "I.2"], "comment": "The code will be open-sourced in the RDC-pymarl project under\n  https://github.com/linkjoker1006", "summary": "In real-world multi-agent systems (MASs), observation delays are ubiquitous,\npreventing agents from making decisions based on the environment's true state.\nAn individual agent's local observation often consists of multiple components\nfrom other agents or dynamic entities in the environment. These discrete\nobservation components with varying delay characteristics pose significant\nchallenges for multi-agent reinforcement learning (MARL). In this paper, we\nfirst formulate the decentralized stochastic individual delay partially\nobservable Markov decision process (DSID-POMDP) by extending the standard\nDec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL\ntraining framework for addressing stochastic individual delays, along with\nrecommended implementations for its constituent modules. We implement the\nDSID-POMDP's observation generation pattern using standard MARL benchmarks,\nincluding MPE and SMAC. Experiments demonstrate that baseline MARL methods\nsuffer severe performance degradation under fixed and unfixed delays. The\nRDC-enhanced approach mitigates this issue, remarkably achieving ideal\ndelay-free performance in certain delay scenarios while maintaining\ngeneralization capability. Our work provides a novel perspective on multi-agent\ndelayed observation problems and offers an effective solution framework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5904\u7406\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u89c2\u6d4b\u5ef6\u8fdf\u7684\u6846\u67b6RDC\uff0c\u901a\u8fc7\u6269\u5c55Dec-POMDP\u6a21\u578b\u5e76\u5f15\u5165RAINBOW\u65b9\u6cd5\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u5ef6\u8fdf\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u666e\u904d\u5b58\u5728\u7684\u89c2\u6d4b\u5ef6\u8fdf\u95ee\u9898\uff0c\u63d0\u5347\u667a\u80fd\u4f53\u5728\u5ef6\u8fdf\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u80fd\u529b\u3002", "method": "\u63d0\u51faDSID-POMDP\u6a21\u578b\u6269\u5c55Dec-POMDP\uff0c\u5e76\u8bbe\u8ba1RAINBOW Delay Compensation (RDC)\u8bad\u7ec3\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRDC\u80fd\u663e\u8457\u51cf\u8f7b\u5ef6\u8fdf\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u80fd\u8fbe\u5230\u65e0\u5ef6\u8fdf\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u591a\u667a\u80fd\u4f53\u5ef6\u8fdf\u89c2\u6d4b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u548c\u6709\u6548\u7684\u89e3\u51b3\u6846\u67b6\u3002"}}
{"id": "2505.03654", "pdf": "https://arxiv.org/pdf/2505.03654", "abs": "https://arxiv.org/abs/2505.03654", "authors": ["Yifan Xiang", "Zhenxi Zhang", "Bin Li", "Yixuan Weng", "Shoujun Zhou", "Yangfan He", "Keqin Li"], "title": "ReGraP-LLaVA: Reasoning enabled Graph-based Personalized Large Language and Vision Assistant", "categories": ["cs.CV", "cs.AI"], "comment": "Work in progress", "summary": "Recent advances in personalized MLLMs enable effective capture of\nuser-specific concepts, supporting both recognition of personalized concepts\nand contextual captioning. However, humans typically explore and reason over\nrelations among objects and individuals, transcending surface-level information\nto achieve more personalized and contextual understanding. To this end,\nexisting methods may face three main limitations: Their training data lacks\nmulti-object sets in which relations among objects are learnable. Building on\nthe limited training data, their models overlook the relations between\ndifferent personalized concepts and fail to reason over them. Their experiments\nmainly focus on a single personalized concept, where evaluations are limited to\nrecognition and captioning tasks. To address the limitations, we present a new\ndataset named ReGraP, consisting of 120 sets of personalized knowledge. Each\nset includes images, KGs, and CoT QA pairs derived from the KGs, enabling more\nstructured and sophisticated reasoning pathways. We propose ReGraP-LLaVA, an\nMLLM trained with the corresponding KGs and CoT QA pairs, where soft and hard\ngraph prompting methods are designed to align KGs within the model's semantic\nspace. We establish the ReGraP Benchmark, which contains diverse task types:\nmultiple-choice, fill-in-the-blank, True/False, and descriptive questions in\nboth open- and closed-ended settings. The proposed benchmark is designed to\nevaluate the relational reasoning and knowledge-connection capability of\npersonalized MLLMs. We conduct experiments on the proposed ReGraP-LLaVA and\nother competitive MLLMs. Results show that the proposed model not only learns\npersonalized knowledge but also performs relational reasoning in responses,\nachieving the SoTA performance compared with the competitive methods. All the\ncodes and datasets are released at: https://github.com/xyfyyds/ReGraP.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86ReGraP\u6570\u636e\u96c6\u548cReGraP-LLaVA\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u4e2a\u6027\u5316MLLM\u5728\u5173\u7cfb\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u901a\u8fc7\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u548c\u601d\u7ef4\u94fe\uff08CoT\uff09\u95ee\u7b54\u5bf9\uff0c\u6a21\u578b\u80fd\u591f\u66f4\u597d\u5730\u8fdb\u884c\u5173\u7cfb\u63a8\u7406\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u7c7b\u578b\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4e2a\u6027\u5316MLLM\u5728\u5173\u7cfb\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u8bad\u7ec3\u6570\u636e\u7f3a\u4e4f\u591a\u5bf9\u8c61\u96c6\u4e14\u6a21\u578b\u5ffd\u89c6\u4e2a\u6027\u5316\u6982\u5ff5\u95f4\u7684\u5173\u7cfb\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u65b0\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51faReGraP\u6570\u636e\u96c6\uff0c\u5305\u542b\u56fe\u50cf\u3001\u77e5\u8bc6\u56fe\u8c31\u548c\u601d\u7ef4\u94fe\u95ee\u7b54\u5bf9\uff0c\u5e76\u8bbe\u8ba1ReGraP-LLaVA\u6a21\u578b\uff0c\u91c7\u7528\u8f6f\u786c\u56fe\u63d0\u793a\u65b9\u6cd5\u5bf9\u9f50\u77e5\u8bc6\u56fe\u8c31\u4e0e\u6a21\u578b\u8bed\u4e49\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cReGraP-LLaVA\u4e0d\u4ec5\u5b66\u4e60\u4e2a\u6027\u5316\u77e5\u8bc6\uff0c\u8fd8\u80fd\u8fdb\u884c\u5173\u7cfb\u63a8\u7406\uff0c\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7ReGraP\u6570\u636e\u96c6\u548cReGraP-LLaVA\u6a21\u578b\u6210\u529f\u63d0\u5347\u4e86MLLM\u5728\u5173\u7cfb\u63a8\u7406\u548c\u77e5\u8bc6\u8fde\u63a5\u80fd\u529b\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2505.03655", "pdf": "https://arxiv.org/pdf/2505.03655", "abs": "https://arxiv.org/abs/2505.03655", "authors": ["Le Pan", "Yuanjiang Cao", "Chengkai Huang", "Wenjie Zhang", "Lina Yao"], "title": "Counterfactual Inference for Eliminating Sentiment Bias in Recommender Systems", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Recommender Systems (RSs) aim to provide personalized recommendations for\nusers. A newly discovered bias, known as sentiment bias, uncovers a common\nphenomenon within Review-based RSs (RRSs): the recommendation accuracy of users\nor items with negative reviews deteriorates compared with users or items with\npositive reviews. Critical users and niche items are disadvantaged by such\nunfair recommendations. We study this problem from the perspective of\ncounterfactual inference with two stages. At the model training stage, we build\na causal graph and model how sentiment influences the final rating score.\nDuring the inference stage, we decouple the direct and indirect effects to\nmitigate the impact of sentiment bias and remove the indirect effect using\ncounterfactual inference. We have conducted extensive experiments, and the\nresults validate that our model can achieve comparable performance on rating\nprediction for better recommendations and effective mitigation of sentiment\nbias. To the best of our knowledge, this is the first work to employ\ncounterfactual inference on sentiment bias mitigation in RSs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9488\u5bf9\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u60c5\u611f\u504f\u89c1\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u60c5\u611f\u5bf9\u8bc4\u5206\u7684\u5f71\u54cd\u5e76\u89e3\u8026\u76f4\u63a5\u4e0e\u95f4\u63a5\u6548\u5e94\uff0c\u6709\u6548\u51cf\u8f7b\u504f\u89c1\uff0c\u63d0\u5347\u63a8\u8350\u516c\u5e73\u6027\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\u57fa\u4e8e\u8bc4\u8bba\u7684\u63a8\u8350\u7cfb\u7edf\u5b58\u5728\u60c5\u611f\u504f\u89c1\uff0c\u8d1f\u9762\u8bc4\u8bba\u7528\u6237\u6216\u7269\u54c1\u7684\u63a8\u8350\u51c6\u786e\u6027\u8f83\u5dee\uff0c\u5bfc\u81f4\u5bf9\u6279\u8bc4\u7528\u6237\u548c\u5c0f\u4f17\u7269\u54c1\u4e0d\u516c\u5e73\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u4ece\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u89d2\u5ea6\u5c55\u5f00\u7814\u7a76\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u8bad\u7ec3\u9636\u6bb5\u6784\u5efa\u56e0\u679c\u56fe\u5efa\u6a21\u60c5\u611f\u5bf9\u8bc4\u5206\u7684\u5f71\u54cd\uff1b2) \u63a8\u7406\u9636\u6bb5\u901a\u8fc7\u53cd\u4e8b\u5b9e\u63a8\u7406\u89e3\u8026\u76f4\u63a5\u4e0e\u95f4\u63a5\u6548\u5e94\uff0c\u6d88\u9664\u60c5\u611f\u504f\u89c1\u7684\u95f4\u63a5\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8bc4\u5206\u9884\u6d4b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u6709\u6548\u7f13\u89e3\u60c5\u611f\u504f\u89c1\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5c06\u53cd\u4e8b\u5b9e\u63a8\u7406\u5e94\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u60c5\u611f\u504f\u89c1\u7f13\u89e3\uff0c\u4e3a\u63d0\u5347\u63a8\u8350\u516c\u5e73\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.03662", "pdf": "https://arxiv.org/pdf/2505.03662", "abs": "https://arxiv.org/abs/2505.03662", "authors": ["Xin Du", "Francesca M. Cozzi", "Rajesh Jena"], "title": "Revolutionizing Brain Tumor Imaging: Generating Synthetic 3D FA Maps from T1-Weighted MRI using CycleGAN Models", "categories": ["cs.CV", "cs.AI", "68U10"], "comment": "9 pages", "summary": "Fractional anisotropy (FA) and directionally encoded colour (DEC) maps are\nessential for evaluating white matter integrity and structural connectivity in\nneuroimaging. However, the spatial misalignment between FA maps and\ntractography atlases hinders their effective integration into predictive\nmodels. To address this issue, we propose a CycleGAN based approach for\ngenerating FA maps directly from T1-weighted MRI scans, representing the first\napplication of this technique to both healthy and tumour-affected tissues. Our\nmodel, trained on unpaired data, produces high fidelity maps, which have been\nrigorously evaluated using Structural Similarity Index (SSIM) and Peak\nSignal-to-Noise Ratio (PSNR), demonstrating particularly robust performance in\ntumour regions. Radiological assessments further underscore the model's\npotential to enhance clinical workflows by providing an AI-driven alternative\nthat reduces the necessity for additional scans.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8eCycleGAN\u7684\u65b9\u6cd5\uff0c\u76f4\u63a5\u4eceT1\u52a0\u6743MRI\u626b\u63cf\u751f\u6210FA\u56fe\uff0c\u63d0\u9ad8\u4e86\u767d\u8d28\u5b8c\u6574\u6027\u548c\u7ed3\u6784\u8fde\u901a\u6027\u8bc4\u4f30\u7684\u6548\u7387\uff0c\u5c24\u5176\u662f\u5728\u80bf\u7624\u533a\u57df\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "FA\u56fe\u4e0e\u7ea4\u7ef4\u675f\u8ffd\u8e2a\u56fe\u8c31\u7684\u7a7a\u95f4\u9519\u4f4d\u5f71\u54cd\u4e86\u5176\u4e0e\u9884\u6d4b\u6a21\u578b\u7684\u6709\u6548\u6574\u5408\uff0c\u9700\u4e00\u79cd\u76f4\u63a5\u4eceT1\u52a0\u6743MRI\u751f\u6210FA\u56fe\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528CycleGAN\u6a21\u578b\uff0c\u5229\u7528\u672a\u914d\u5bf9\u6570\u636e\u8bad\u7ec3\uff0c\u751f\u6210\u9ad8\u4fdd\u771fFA\u56fe\uff0c\u5e76\u901a\u8fc7SSIM\u548cPSNR\u8fdb\u884c\u4e25\u683c\u8bc4\u4f30\u3002", "result": "\u6a21\u578b\u5728\u80bf\u7624\u533a\u57df\u8868\u73b0\u5c24\u4e3a\u4f18\u5f02\uff0c\u653e\u5c04\u5b66\u8bc4\u4f30\u8868\u660e\u5176\u53ef\u51cf\u5c11\u989d\u5916\u626b\u63cf\u9700\u6c42\uff0c\u4f18\u5316\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aFA\u56fe\u7684\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cdAI\u9a71\u52a8\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5177\u6709\u663e\u8457\u7684\u4e34\u5e8a\u6f5c\u529b\u3002"}}
{"id": "2505.03694", "pdf": "https://arxiv.org/pdf/2505.03694", "abs": "https://arxiv.org/abs/2505.03694", "authors": ["Parv Kapoor", "Ian Higgins", "Nikhil Keetha", "Jay Patrikar", "Brady Moon", "Zelin Ye", "Yao He", "Ivan Cisneros", "Yaoyu Hu", "Changliu Liu", "Eunsuk Kang", "Sebastian Scherer"], "title": "Demonstrating ViSafe: Vision-enabled Safety for High-speed Detect and Avoid", "categories": ["cs.RO", "cs.AI"], "comment": "13 pages, RSS 2025 Demo track", "summary": "Assured safe-separation is essential for achieving seamless high-density\noperation of airborne vehicles in a shared airspace. To equip\nresource-constrained aerial systems with this safety-critical capability, we\npresent ViSafe, a high-speed vision-only airborne collision avoidance system.\nViSafe offers a full-stack solution to the Detect and Avoid (DAA) problem by\ntightly integrating a learning-based edge-AI framework with a custom\nmulti-camera hardware prototype designed under SWaP-C constraints. By\nleveraging perceptual input-focused control barrier functions (CBF) to design,\nencode, and enforce safety thresholds, ViSafe can provide provably safe runtime\nguarantees for self-separation in high-speed aerial operations. We evaluate\nViSafe's performance through an extensive test campaign involving both\nsimulated digital twins and real-world flight scenarios. By independently\nvarying agent types, closure rates, interaction geometries, and environmental\nconditions (e.g., weather and lighting), we demonstrate that ViSafe\nconsistently ensures self-separation across diverse scenarios. In\nfirst-of-its-kind real-world high-speed collision avoidance tests with closure\nrates reaching 144 km/h, ViSafe sets a new benchmark for vision-only autonomous\ncollision avoidance, establishing a new standard for safety in high-speed\naerial navigation.", "AI": {"tldr": "ViSafe\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u9ad8\u901f\u7a7a\u4e2d\u907f\u78b0\u7cfb\u7edf\uff0c\u7ed3\u5408\u5b66\u4e60\u578b\u8fb9\u7f18AI\u6846\u67b6\u548c\u591a\u6444\u50cf\u5934\u786c\u4ef6\u539f\u578b\uff0c\u901a\u8fc7\u611f\u77e5\u8f93\u5165\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u63d0\u4f9b\u5b89\u5168\u8fd0\u884c\u65f6\u4fdd\u8bc1\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u9ad8\u901f\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u9ad8\u5bc6\u5ea6\u7a7a\u4e2d\u4ea4\u901a\u64cd\u4f5c\u4e2d\u7684\u5b89\u5168\u4fdd\u969c\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u7a7a\u4e2d\u7cfb\u7edf\u4e2d\uff0c\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u9760\u7684\u907f\u78b0\u7cfb\u7edf\u662f\u5fc5\u8981\u7684\u3002", "method": "ViSafe\u96c6\u6210\u4e86\u5b66\u4e60\u578b\u8fb9\u7f18AI\u6846\u67b6\u548c\u5b9a\u5236\u591a\u6444\u50cf\u5934\u786c\u4ef6\u539f\u578b\uff0c\u5229\u7528\u611f\u77e5\u8f93\u5165\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08CBF\uff09\u8bbe\u8ba1\u548c\u5f3a\u5236\u6267\u884c\u5b89\u5168\u9608\u503c\u3002", "result": "ViSafe\u5728\u6a21\u62df\u548c\u771f\u5b9e\u98de\u884c\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6700\u9ad8\u907f\u78b0\u901f\u5ea6\u8fbe144 km/h\uff0c\u4e3a\u7eaf\u89c6\u89c9\u81ea\u4e3b\u907f\u78b0\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\u3002", "conclusion": "ViSafe\u4e3a\u9ad8\u901f\u7a7a\u4e2d\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u8bc1\u660e\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u5b89\u5168\u6027\u548c\u64cd\u4f5c\u5bc6\u5ea6\u3002"}}
{"id": "2505.03730", "pdf": "https://arxiv.org/pdf/2505.03730", "abs": "https://arxiv.org/abs/2505.03730", "authors": ["Shiyi Zhang", "Junhao Zhuang", "Zhaoyang Zhang", "Ying Shan", "Yansong Tang"], "title": "FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Accepted by Siggraph2025, Project Page:\n  https://shiyi-zh0408.github.io/projectpages/FlexiAct/", "summary": "Action customization involves generating videos where the subject performs\nactions dictated by input control signals. Current methods use pose-guided or\nglobal motion customization but are limited by strict constraints on spatial\nstructure, such as layout, skeleton, and viewpoint consistency, reducing\nadaptability across diverse subjects and scenarios. To overcome these\nlimitations, we propose FlexiAct, which transfers actions from a reference\nvideo to an arbitrary target image. Unlike existing methods, FlexiAct allows\nfor variations in layout, viewpoint, and skeletal structure between the subject\nof the reference video and the target image, while maintaining identity\nconsistency. Achieving this requires precise action control, spatial structure\nadaptation, and consistency preservation. To this end, we introduce RefAdapter,\na lightweight image-conditioned adapter that excels in spatial adaptation and\nconsistency preservation, surpassing existing methods in balancing appearance\nconsistency and structural flexibility. Additionally, based on our\nobservations, the denoising process exhibits varying levels of attention to\nmotion (low frequency) and appearance details (high frequency) at different\ntimesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike\nexisting methods that rely on separate spatial-temporal architectures, directly\nachieves action extraction during the denoising process. Experiments\ndemonstrate that our method effectively transfers actions to subjects with\ndiverse layouts, skeletons, and viewpoints. We release our code and model\nweights to support further research at\nhttps://shiyi-zh0408.github.io/projectpages/FlexiAct/", "AI": {"tldr": "FlexiAct\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7RefAdapter\u548cFAE\u6280\u672f\uff0c\u5b9e\u73b0\u4ece\u53c2\u8003\u89c6\u9891\u5230\u4efb\u610f\u76ee\u6807\u56fe\u50cf\u7684\u52a8\u4f5c\u8f6c\u79fb\uff0c\u6253\u7834\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7a7a\u95f4\u7ed3\u6784\u4e0a\u7684\u4e25\u683c\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u4f5c\u5b9a\u5236\u4e2d\u53d7\u9650\u4e8e\u7a7a\u95f4\u7ed3\u6784\u7684\u4e25\u683c\u7ea6\u675f\uff08\u5982\u5e03\u5c40\u3001\u9aa8\u67b6\u548c\u89c6\u89d2\u4e00\u81f4\u6027\uff09\uff0c\u96be\u4ee5\u9002\u5e94\u591a\u6837\u5316\u4e3b\u4f53\u548c\u573a\u666f\u3002FlexiAct\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u63d0\u5347\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "FlexiAct\u7ed3\u5408\u4e86RefAdapter\uff08\u8f7b\u91cf\u7ea7\u56fe\u50cf\u6761\u4ef6\u9002\u914d\u5668\uff09\u548cFAE\uff08\u9891\u7387\u611f\u77e5\u52a8\u4f5c\u63d0\u53d6\uff09\u6280\u672f\u3002RefAdapter\u4e13\u6ce8\u4e8e\u7a7a\u95f4\u9002\u5e94\u548c\u4e00\u81f4\u6027\u4fdd\u6301\uff0c\u800cFAE\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u63d0\u53d6\u52a8\u4f5c\uff0c\u907f\u514d\u4f20\u7edf\u65f6\u7a7a\u5206\u79bb\u67b6\u6784\u7684\u590d\u6742\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFlexiAct\u80fd\u6709\u6548\u5c06\u52a8\u4f5c\u8f6c\u79fb\u81f3\u5177\u6709\u4e0d\u540c\u5e03\u5c40\u3001\u9aa8\u67b6\u548c\u89c6\u89d2\u7684\u4e3b\u4f53\u4e0a\uff0c\u5728\u4fdd\u6301\u5916\u89c2\u4e00\u81f4\u6027\u7684\u540c\u65f6\u63d0\u4f9b\u66f4\u9ad8\u7684\u7ed3\u6784\u7075\u6d3b\u6027\u3002", "conclusion": "FlexiAct\u4e3a\u52a8\u4f5c\u5b9a\u5236\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u521b\u65b0\u7684RefAdapter\u548cFAE\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u591a\u6837\u5316\u573a\u666f\u4e0b\u7684\u9ad8\u8d28\u91cf\u52a8\u4f5c\u8f6c\u79fb\u3002"}}
