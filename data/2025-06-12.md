<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 64]
- [cs.LG](#cs.LG) [Total: 95]
- [cs.AI](#cs.AI) [Total: 12]
- [cs.RO](#cs.RO) [Total: 11]
- [cs.SE](#cs.SE) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.GR](#cs.GR) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.CY](#cs.CY) [Total: 3]
- [stat.ML](#stat.ML) [Total: 7]
- [cs.NE](#cs.NE) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.LO](#cs.LO) [Total: 1]
- [eess.SP](#eess.SP) [Total: 5]
- [cs.CV](#cs.CV) [Total: 48]
- [cs.NI](#cs.NI) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.SD](#cs.SD) [Total: 6]
- [cs.CR](#cs.CR) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [LLM-as-a-qualitative-judge: automating error analysis in natural language generation](https://arxiv.org/abs/2506.09147)
*Nadezhda Chirkova,Tunde Oluwaseyi Ajayi,Seth Aycock,Zain Muhammad Mujahid,Vladana Perlić,Ekaterina Borisova,Markarit Vartampetian*

Key words: LLM, NLG, 定性评估, 错误分析, 结构化报告

TL;DR: 提出一种LLM定性评估方法，通过结构报告而非数值评分帮助开发者改进NLG系统。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有LLM评估主要依赖数值评分，缺乏对NLG系统改进的具体指导。

Method: 分两步：开放式实例问题分析与累积聚类算法。

Result: 在2/3情况下正确识别特定问题，生成与人工标注相似的错误类型报告。

Conclusion: LLM定性评估能有效提供NLG系统改进的实用见解。

Abstract: Prompting large language models (LLMs) to evaluate generated text, known as
LLM-as-a-judge, has become a standard evaluation approach in natural language
generation (NLG), but is primarily used as a quantitative tool, i.e. with
numerical scores as main outputs. In this work, we propose
LLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main
output being a structured report of common issue types in the NLG system
outputs. Our approach is targeted at providing developers with meaningful
insights on what improvements can be done to a given NLG system and consists of
two main steps, namely open-ended per-instance issue analysis and clustering of
the discovered issues using an intuitive cumulative algorithm. We also
introduce a strategy for evaluating the proposed approach, coupled with ~300
annotations of issues in instances from 12 NLG datasets. Our results show that
LLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3
cases and is capable of producing error type reports resembling the reports
composed by human annotators. Our code and data are publicly available at
https://github.com/tunde-ajayi/llm-as-a-qualitative-judge.

</details>


### [2] [PHRASED: Phrase Dictionary Biasing for Speech Translation](https://arxiv.org/abs/2506.09175)
*Peidong Wang,Jian Xue,Rui Zhao,Junkun Chen,Aswin Shanmugam Subramanian,Jinyu Li*

Key words: 短语翻译、语音翻译、字典偏置、多模态大语言模型

TL;DR: 本文提出短语字典偏置方法，提升语音翻译任务中短语翻译的准确性，在流式语音翻译和多模态大语言模型中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决短语在训练数据中罕见导致的翻译挑战。

Method: 提出短语字典偏置方法，利用源语言到目标语言的短语映射对。

Result: 流式语音翻译模型相比短语列表偏置方法提升21%，多模态大语言模型短语召回率提升85%。

Conclusion: 短语字典偏置方法能有效利用外部短语信息，显著提升翻译性能。

Abstract: Phrases are essential to understand the core concepts in conversations.
However, due to their rare occurrence in training data, correct translation of
phrases is challenging in speech translation tasks. In this paper, we propose a
phrase dictionary biasing method to leverage pairs of phrases mapping from the
source language to the target language. We apply the phrase dictionary biasing
method to two types of widely adopted models, a transducer-based streaming
speech translation model and a multimodal large language model. Experimental
results show that the phrase dictionary biasing method outperforms phrase list
biasing by 21% relatively for the streaming speech translation model. In
addition, phrase dictionary biasing enables multimodal large language models to
use external phrase information, achieving 85% relative improvement in phrase
recall.

</details>


### [3] [A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs](https://arxiv.org/abs/2506.09218)
*Bruno Ferenc Šegedin*

Key words: 深度神经网络, 卷积神经网络, 语音规则, 全连接层, 泛化能力

TL;DR: 研究了深度神经网络（DNNs）在语音规则泛化能力上的表现，提出了一种新方法在窄瓶颈条件下探测模型的泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索生成卷积神经网络（CNNs）是否能从原始音频波形中学习并泛化语音规则，并研究缩小全连接层（FC）瓶颈的影响。

Method: 训练生成CNN模型，缩小FC瓶颈至8通道，提出绕过FC并输入随机特征图的新技术。

Result: 卷积层能动态泛化语音依赖，不受FC学习到的词汇限制配置约束。

Conclusion: 卷积层具备独立于词汇的语音规则泛化能力。

Abstract: The ability of deep neural networks (DNNs) to represent phonotactic
generalizations derived from lexical learning remains an open question. This
study (1) investigates the lexically-invariant generalization capacity of
generative convolutional neural networks (CNNs) trained on raw audio waveforms
of lexical items and (2) explores the consequences of shrinking the
fully-connected layer (FC) bottleneck from 1024 channels to 8 before training.
Ultimately, a novel technique for probing a model's lexically-independent
generalizations is proposed that works only under the narrow FC bottleneck:
generating audio outputs by bypassing the FC and inputting randomized feature
maps into the convolutional block. These outputs are equally biased by a
phonotactic restriction in training as are outputs generated with the FC. This
result shows that the convolutional layers can dynamically generalize phonetic
dependencies beyond lexically-constrained configurations learned by the FC.

</details>


### [4] [Extrapolation by Association: Length Generalization Transfer in Transformers](https://arxiv.org/abs/2506.09251)
*Ziyang Cai,Nayoung Lee,Avi Schwarzschild,Samet Oymak,Dimitris Papailiopoulos*

Key words: Transformer, 长度泛化, 任务关联, 注意力头, 算法任务

TL;DR: 该论文研究了Transformer模型如何通过任务关联实现长度泛化能力，发现通过训练模型在相关长任务上，可以使其在其他目标任务的未见长输入上泛化，并在多种算法任务中验证了这一现象。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索Transformer模型如何在自然语言处理中展现出泛化能力，尤其是研究其在未见长输入上的表现。

Method: 通过任务关联方法，研究训练模型在相关长任务上是否能帮助其在其他目标任务上实现长度泛化，并分析注意力头重用机制。

Result: 发现Transformer模型可以通过联合训练从相关任务中继承泛化能力，预训练语言模型也显示出类似的传递效果。

Conclusion: Transformer模型的泛化能力可以通过任务间共享的计算结构实现，研究加深了对模型如何处理分布外输入的理解。

Abstract: Transformer language models have demonstrated impressive generalization
capabilities in natural language domains, yet we lack a fine-grained
understanding of how such generalization arises. In this paper, we investigate
length generalization--the ability to extrapolate from shorter to longer
inputs--through the lens of \textit{task association}. We find that length
generalization can be \textit{transferred} across related tasks. That is,
training a model with a longer and related auxiliary task can lead it to
generalize to unseen and longer inputs from some other target task. We
demonstrate this length generalization transfer across diverse algorithmic
tasks, including arithmetic operations, string transformations, and maze
navigation. Our results show that transformer models can inherit generalization
capabilities from similar tasks when trained jointly. Moreover, we observe
similar transfer effects in pretrained language models, suggesting that
pretraining equips models with reusable computational scaffolding that
facilitates extrapolation in downstream settings. Finally, we provide initial
mechanistic evidence that length generalization transfer correlates with the
re-use of the same attention heads between the tasks. Together, our findings
deepen our understanding of how transformers generalize to out-of-distribution
inputs and highlight the compositional reuse of inductive structure across
tasks.

</details>


### [5] [Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat](https://arxiv.org/abs/2506.09259)
*Zhuofang Li,Rafal Kocielnik,Fereshteh Soltani,Penphob,Boonyarungsrit,Animashree Anandkumar,R. Michael Alvarez*

Key words: 亲社会行为、自锚注意力模型、游戏聊天、NLP技术、低资源学习

TL;DR: 该论文提出了一种新颖的自锚注意力模型（SAAM），用于在游戏聊天文本中识别和分类亲社会行为，相比现有技术提升了7.9%。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管已有研究关注通过NLP技术检测毒性内容，但亲社会行为的识别同样重要且缺乏相关数据集和模型。

Method: 结合无监督发现与游戏领域专家合作识别亲社会行为，并提出自锚注意力模型（SAAM）以提升低资源环境下的分类效果。

Result: 在《使命召唤：现代战争II》游戏聊天中验证了方法的有效性，开发了首个自动化分类亲社会行为的系统。

Conclusion: 该研究为在线平台提供了从惩罚毒性转向鼓励正面互动的新视角，扩展了NLP技术在游戏社交中的应用。

Abstract: Millions of players engage daily in competitive online games, communicating
through in-game chat. Prior research has focused on detecting relatively small
volumes of toxic content using various Natural Language Processing (NLP)
techniques for the purpose of moderation. However, recent studies emphasize the
importance of detecting prosocial communication, which can be as crucial as
identifying toxic interactions. Recognizing prosocial behavior allows for its
analysis, rewarding, and promotion. Unlike toxicity, there are limited
datasets, models, and resources for identifying prosocial behaviors in
game-chat text. In this work, we employed unsupervised discovery combined with
game domain expert collaboration to identify and categorize prosocial player
behaviors from game chat. We further propose a novel Self-Anchored Attention
Model (SAAM) which gives 7.9% improvement compared to the best existing
technique. The approach utilizes the entire training set as "anchors" to help
improve model performance under the scarcity of training data. This approach
led to the development of the first automated system for classifying prosocial
behaviors in in-game chats, particularly given the low-resource settings where
large-scale labeled data is not available. Our methodology was applied to one
of the most popular online gaming titles - Call of Duty(R): Modern
Warfare(R)II, showcasing its effectiveness. This research is novel in applying
NLP techniques to discover and classify prosocial behaviors in player in-game
chat communication. It can help shift the focus of moderation from solely
penalizing toxicity to actively encouraging positive interactions on online
platforms.

</details>


### [6] [Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models](https://arxiv.org/abs/2506.09277)
*Milan Bhan,Jean-Noel Vittaut,Nicolas Chesneau,Sarath Chandar,Marie-Jeanne Lesot*

Key words: Large Language Models, self-NLE, faithfulness, neural activity, quantitative evaluation

TL;DR: 该论文提出了一种新框架，通过比较模型生成的自我自然语言解释（self-NLE）与模型内部隐藏状态的解释，定量评估其忠实性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 由于现有的评估方法未能深入分析模型神经活动，无法保证self-NLE的忠实性，作者希望建立一种更直接的评估框架。

Method: 通过直接对比self-NLE与模型内部隐藏状态的解释，设计灵活且定量的评估方法。

Result: 新框架提供了深入洞察，揭示了self-NLE与模型推理之间的直接联系，为生成更忠实的解释奠定了基础。

Conclusion: 该研究推进了对self-NLE忠实性的理解，并为未来生成更可靠的解释提供了工具。

Abstract: Large Language Models (LLM) have demonstrated the capability of generating
free text self Natural Language Explanation (self-NLE) to justify their
answers. Despite their logical appearance, self-NLE do not necessarily reflect
the LLM actual decision-making process, making such explanations unfaithful.
While existing methods for measuring self-NLE faithfulness mostly rely on
behavioral tests or computational block identification, none of them examines
the neural activity underlying the model's reasoning. This work introduces a
novel flexible framework for quantitatively measuring the faithfulness of
LLM-generated self-NLE by directly comparing the latter with interpretations of
the model's internal hidden states. The proposed framework is versatile and
provides deep insights into self-NLE faithfulness by establishing a direct
connection between self-NLE and model reasoning. This approach advances the
understanding of self-NLE faithfulness and provides building blocks for
generating more faithful self-NLE.

</details>


### [7] [$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding](https://arxiv.org/abs/2506.09301)
*Cesare Spinoso-Di Piano,David Austin,Pablo Piantanida,Jackie Chi Kit Cheung*

Key words: 比喻语言, RSA框架, 修辞策略, 讽刺理解, LLMs

TL;DR: 提出了$(RSA)^2$框架，通过考虑修辞策略来解释比喻语言，无需建模说话者动机，结合LLMs在讽刺理解任务中达到SOTA。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 比喻语言（如讽刺、夸张、低调陈述）在人类交流中普遍存在，但现有RSA框架无法有效处理比喻表达或需特定建模说话者动机。

Method: 引入$(RSA)^2$框架，通过建模说话者的修辞策略来解释非字面语言，并利用LLMs提升性能。

Result: $(RSA)^2$框架在新讽刺数据集PragMega+上实现了SOTA性能，且无需建模说话者动机。

Conclusion: $(RSA)^2$框架为比喻语言提供了一种高效的解释方法，展示了LLMs在语言理解中的潜力。

Abstract: Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in
human communication, resulting in utterances where the literal and the intended
meanings do not match. The Rational Speech Act (RSA) framework, which
explicitly models speaker intentions, is the most widespread theory of
probabilistic pragmatics, but existing implementations are either unable to
account for figurative expressions or require modeling the implicit motivations
for using figurative language (e.g., to express joy or annoyance) in a
setting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware
RSA $(RSA)^2$ framework which models figurative language use by considering a
speaker's employed rhetorical strategy. We show that $(RSA)^2$ enables
human-compatible interpretations of non-literal utterances without modeling a
speaker's motivations for being non-literal. Combined with LLMs, it achieves
state-of-the-art performance on the ironic split of PragMega+, a new irony
interpretation dataset introduced in this study.

</details>


### [8] [Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models](https://arxiv.org/abs/2506.09315)
*Yao Xiao,Heidi Christensen,Stefan Goetze*

Key words: 阿尔茨海默病, 配对困惑度, Mistral-7B, 语言模型, 模型解释性

TL;DR: 本文利用Mistral-7B大型语言模型改进配对困惑度方法，提升AD检测精度3.33%-6.35%，并提供清晰可解释的决策边界。模型还学习到AD患者的语言模式。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 阿尔茨海默病（AD）常影响语言能力，当前检测方法存在不透明决策问题，需提升精度并解释模型决策。

Method: 扩展配对困惑度方法，采用指令遵循版Mistral-7B模型进行AD检测，并通过提示微调模型分析语言模式。

Result: 方法平均精度提升3.33%，且决策边界清晰。模型可捕捉AD患者的特殊语言模式。

Conclusion: 新方法显著提升AD检测性能，并揭示模型学习潜力，为数据增强和解释性研究提供可能。

Abstract: Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive
decline that commonly impacts language ability. This work extends the paired
perplexity approach to detecting AD by using a recent large language model
(LLM), the instruction-following version of Mistral-7B. We improve accuracy by
an average of 3.33% over the best current paired perplexity method and by 6.35%
over the top-ranked method from the ADReSS 2020 challenge benchmark. Our
further analysis demonstrates that the proposed approach can effectively detect
AD with a clear and interpretable decision boundary in contrast to other
methods that suffer from opaque decision-making processes. Finally, by
prompting the fine-tuned LLMs and comparing the model-generated responses to
human responses, we illustrate that the LLMs have learned the special language
patterns of AD speakers, which opens up possibilities for novel methods of
model interpretation and data augmentation.

</details>


### [9] [Towards Efficient and Effective Alignment of Large Language Models](https://arxiv.org/abs/2506.09329)
*Yuxin Jiang*

Key words: 大语言模型, 对齐, 数据收集, 训练优化, 评估基准, Lion, WebR, LTE, BMC, FollowBench

TL;DR: 该论文针对大语言模型（LLM）与人类期望的高效对齐问题，提出了数据收集、训练和评估的新方法，包括Lion框架、WebR技术、LTE优化、BMC改进和FollowBench评估工具。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大语言模型在多任务中表现出色，但与人类期望的高效对齐仍具挑战性，亟需改进数据收集、训练和评估方法。

Method: 提出Lion对抗蒸馏框架改进数据收集；开发WebR自动合成指令调优数据；设计LTE元学习框架优化训练；改进BMC捕获令牌级相关性；构建FollowBench评估约束遵循能力。

Result: Lion实现零样本推理的先进水平；WebR提升了数据多样性和可扩展性；LTE优化知识更新效率；BMC在QA和数学推理任务中表现优异；FollowBench揭示模型在约束遵循上的不足。

Conclusion: 论文为LLM对齐提供了实用的方法和工具，解决了数据收集、训练和评估的关键问题，为未来改进提供了方向。

Abstract: Large language models (LLMs) exhibit remarkable capabilities across diverse
tasks, yet aligning them efficiently and effectively with human expectations
remains a critical challenge. This thesis advances LLM alignment by introducing
novel methodologies in data collection, training, and evaluation. We first
address alignment data collection. Existing approaches rely heavily on manually
curated datasets or proprietary models. To overcome these limitations, we
propose Lion, an adversarial distillation framework that iteratively refines
training data by identifying and generating challenging instructions, enabling
state-of-the-art zero-shot reasoning. Additionally, we introduce Web
Reconstruction (WebR), a fully automated framework that synthesizes
instruction-tuning data directly from raw web documents, significantly
improving data diversity and scalability over existing synthetic data methods.
Next, we enhance alignment training through novel optimization techniques. We
develop Learning to Edit (LTE), a framework that enables LLMs to efficiently
integrate new knowledge while preserving existing information. LTE leverages
meta-learning to improve both real-time and batch knowledge updates.
Furthermore, we introduce Bridging and Modeling Correlations (BMC), a
refinement of Direct Preference Optimization (DPO) that explicitly captures
token-level correlations in preference data, leading to superior alignment
across QA and mathematical reasoning tasks. Finally, we tackle the challenge of
evaluating alignment. Existing benchmarks emphasize response quality but
overlook adherence to specific constraints. To bridge this gap, we introduce
FollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to
follow complex constraints across diverse instruction types. Our results expose
key weaknesses in current models' constraint adherence, offering insights for
future improvements.

</details>


### [10] [Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation](https://arxiv.org/abs/2506.09331)
*Arjun Vaithilingam Sudhakar*

Key words: 大型语言模型,心智理论,多智能体强化学习,协作,人机交互

TL;DR: 本文探讨了大型语言模型（LLMs）是否具备理解他人意图的能力，即是否具有‘心智理论’，并通过多智能体强化学习（MARL）研究其应用前景。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: LLMs在零样本和少样本任务中表现出色，但能否推断他人意图尚未明确。理解意图是有效协作的基础，对未来人机协作至关重要。

Method: 采用基于LLM的智能体，结合多智能体强化学习（MARL）框架，研究其协作能力。

Result: 研究表明LLM可能具备一定程度的心智理论，能够模拟和推理他人意图。

Conclusion: LLMs的协作能力有望推动人机混合系统的无缝协作，对未来人机交互具有广泛意义。

Abstract: Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot
generalization capabilities across complex natural language tasks, enabling
their widespread use as virtual assistants for diverse applications such as
translation and summarization. Despite being trained solely on large corpora of
text without explicit supervision on author intent, LLMs appear to infer the
underlying meaning of textual interactions. This raises a fundamental question:
can LLMs model and reason about the intentions of others, i.e., do they possess
a form of theory of mind? Understanding other's intentions is crucial for
effective collaboration, which underpins human societal success and is
essential for cooperative interactions among multiple agents, including humans
and autonomous systems. In this work, we investigate the theory of mind in LLMs
through the lens of cooperative multi-agent reinforcement learning (MARL),
where agents learn to collaborate via repeated interactions, mirroring human
social reasoning. Our approach aims to enhance artificial agent's ability to
adapt and cooperate with both artificial and human partners. By leveraging
LLM-based agents capable of natural language interaction, we move towards
creating hybrid human-AI systems that can foster seamless collaboration, with
broad implications for the future of human-artificial interaction.

</details>


### [11] [RePO: Replay-Enhanced Policy Optimization](https://arxiv.org/abs/2506.09340)
*Siheng Li,Zhanhui Zhou,Wai Lam,Chao Yang,Chaochao Lu*

Key words: 强化学习,大语言模型,策略优化,回放策略

TL;DR: RePO通过利用多样化的回放策略从回放缓冲区中检索离策略样本，显著提升大语言模型的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的GRPO方法因计算成本高和数据效率低而受到限制，需要一种更高效的方法来优化大语言模型。

Method: 提出RePO方法，结合回放策略和离策略样本进行策略优化。

Result: RePO在多个数学推理基准测试中性能显著提升，计算成本仅增加15%，但有效优化步骤增加48%。

Conclusion: RePO是一种高效且性能优越的大语言模型优化方法。

Abstract: Reinforcement learning (RL) is vital for optimizing large language models
(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages
using multiple on-policy outputs per prompt, leading to high computational
costs and low data efficiency. To address this, we introduce Replay-Enhanced
Policy Optimization (RePO), which leverages diverse replay strategies to
retrieve off-policy samples from a replay buffer, allowing policy optimization
based on a broader and more diverse set of samples for each prompt. Experiments
on five LLMs across seven mathematical reasoning benchmarks demonstrate that
RePO achieves absolute average performance gains of $18.4$ and $4.1$ points for
Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further
analysis indicates that RePO increases computational cost by $15\%$ while
raising the number of effective optimization steps by $48\%$ for Qwen3-1.7B,
with both on-policy and off-policy sample numbers set to $8$. The repository
can be accessed at https://github.com/SihengLi99/RePO.

</details>


### [12] [Latent Multi-Head Attention for Small Language Models](https://arxiv.org/abs/2506.09342)
*Sushant Mehta,Raj Dandekar,Rajat Dandekar,Sreedath Panat*

Key words: 小语言模型,潜在多头注意力,旋转位置嵌入,效率-质量权衡

TL;DR: 本文研究了小语言模型中潜在多头注意力(MLA)的效率与质量权衡，发现MLA结合旋转位置嵌入(RoPE)在减少内存的同时保持模型性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索在小语言模型中应用潜在多头注意力(MLA)的可行性与优势，特别是在内存受限的场景下。

Method: 训练30M参数的GPT模型，比较标准多头注意力(MHA)、MLA及结合RoPE的MLA(MLA+RoPE)的性能。

Result: MLA+RoPE在减少45%内存的同时，验证损失仅增加0.3%，推理速度提升1.4倍，并在质量评估中表现最佳。

Conclusion: MLA+RoPE在小语言模型中实现了内存与性能的帕累托改进，适用于内存受限的部署场景。

Abstract: We present the first comprehensive study of latent multi-head attention (MLA)
for small language models, revealing interesting efficiency-quality trade-offs.
Training 30M-parameter GPT models on 100,000 synthetic stories, we benchmark
three architectural variants: standard multi-head attention (MHA), MLA, and MLA
with rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE
with half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory
reduction while incurring only a 0.3% increase in validation loss (essentially
matching MHA quality)- a Pareto improvement for memory constrained deployment.
We further show that RoPE is crucial for MLA in small models: without it, MLA
underperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by
2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2
achieves a 1.4 times speedup over full-rank MLA while maintaining the memory
savings. GPT-4 evaluations corroborate perplexity results, with ours achieving
the highest quality scores (7.4/10) across grammar, creativity, and consistency
metrics. Code and models will be released upon acceptance.

</details>


### [13] [OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment](https://arxiv.org/abs/2506.09349)
*Chao-Hong Tan,Qian Chen,Wen Wang,Chong Deng,Qinglin Zhang,Luyao Cheng,Hai Yu,Xin Zhang,Xiang Lv,Tianyu Zhao,Chong Zhang,Yukun Ma,Yafeng Chen,Hui Wang,Jiaqing Liu,Jieping Ye*

Key words: 语音生成, 大语言模型, 自回归建模, 跨模态对齐, OmniDRCA

TL;DR: 该论文提出了一种基于联合自回归建模的并行语音-文本基础模型OmniDRCA，通过双分辨率语音表示和对比跨模态对齐，实现了语音与文本的并行处理，并在口语问答任务中取得了最优性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有语音生成方法分为独立生成语音标记和联合自回归建模两类，但前者缺乏模态间交互。本文旨在通过并行建模和对比对齐提升语音与文本的相互感知能力。

Method: OmniDRCA采用并行联合自回归建模，引入双分辨率语音表示和对比跨模态对齐技术，增强语音理解能力。

Result: 在口语问答基准测试中，OmniDRCA在并行联合语音-文本建模模型中表现最优，与交错模型相比性能相当。

Conclusion: OmniDRCA展示了并行语音-文本建模的潜力，并可能扩展至全双工会话场景。

Abstract: Recent studies on end-to-end speech generation with large language models
(LLMs) have attracted significant community attention, with multiple works
extending text-based LLMs to generate discrete speech tokens. Existing
approaches primarily fall into two categories: (1) Methods that generate
discrete speech tokens independently without incorporating them into the LLM's
autoregressive process, resulting in text generation being unaware of
concurrent speech synthesis. (2) Models that generate interleaved or parallel
speech-text tokens through joint autoregressive modeling, enabling mutual
modality awareness during generation. This paper presents OmniDRCA, a parallel
speech-text foundation model based on joint autoregressive modeling, featuring
dual-resolution speech representations and contrastive cross-modal alignment.
Our approach processes speech and text representations in parallel while
enhancing audio comprehension through contrastive alignment. Experimental
results on Spoken Question Answering benchmarks demonstrate that OmniDRCA
establishes new state-of-the-art (SOTA) performance among parallel joint
speech-text modeling based foundation models, and achieves competitive
performance compared to interleaved models. Additionally, we explore the
potential of extending the framework to full-duplex conversational scenarios.

</details>


### [14] [DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts](https://arxiv.org/abs/2506.09351)
*Yuchen Feng,Bowen Shen,Naibin Gu,Jiaxuan Zhao,Peng Fu,Zheng Lin,Weiping Wang*

Key words: Large language models, Mixture-of-Experts, diversity, pruning, reconstruction

TL;DR: 论文提出了一种名为DIVE的多样性增强重建方法，通过修剪和重组FFN模块，高效地重构稠密LLM为MoE LLM，减少了训练开销并提升了专家多样性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有MoE LLM重构方法常忽略专家多样性，导致冗余。本文提出利用不同校准数据集修剪后的LLM多样性优势进行重构。

Method: DIVE方法包括领域亲和性挖掘、基于修剪的专家重构及高效再训练，具体涉及FFN模块的修剪与重组，并再训练路由器、专家和归一化模块。

Result: 在Llama风格LLM上实现DIVE，实验表明其在保持准确性的同时提升了训练效率，优于现有修剪和MoE重构方法。

Conclusion: DIVE通过增强专家多样性，显著降低了MoE LLM的训练开销，且性能优于现有方法。

Abstract: Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture
achieve high cost-efficiency by selectively activating a subset of the
parameters. Despite the inference efficiency of MoE LLMs, the training of
extensive experts from scratch incurs substantial overhead, whereas
reconstructing a dense LLM into an MoE LLM significantly reduces the training
budget. However, existing reconstruction methods often overlook the diversity
among experts, leading to potential redundancy. In this paper, we come up with
the observation that a specific LLM exhibits notable diversity after being
pruned on different calibration datasets, based on which we present a
Diversity-Enhanced reconstruction method named DIVE. The recipe of DIVE
includes domain affinity mining, pruning-based expert reconstruction, and
efficient retraining. Specifically, the reconstruction includes pruning and
reassembly of the feed-forward network (FFN) module. After reconstruction, we
efficiently retrain the model on routers, experts and normalization modules. We
implement DIVE on Llama-style LLMs with open-source training corpora.
Experiments show that DIVE achieves training efficiency with minimal accuracy
trade-offs, outperforming existing pruning and MoE reconstruction methods with
the same number of activated parameters.

</details>


### [15] [Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL](https://arxiv.org/abs/2506.09359)
*Qingyun Zeng,Simin Ma,Arash Niknafs,Ashish Basran,Carol Szabo*

Key words: 大型语言模型, Text-to-SQL, 语义等价性, 模糊查询, 评估挑战

TL;DR: 本文探讨了利用大型语言模型（LLMs）评估生成的SQL语义等价性，分析了常见模式及挑战。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型的兴起推动了文本到SQL系统的发展，但评估生成的SQL语义等价性仍具挑战性。

Method: 采用LLMs评估语义和部分语义等价性，分析SQL等价与不等价的常见模式。

Result: 总结SQL语义等价性评估的常见模式，并讨论LLM评估中的挑战。

Conclusion: LLM在评估SQL语义等价性方面具有潜力，但仍需克服实际应用中的挑战。

Abstract: The rise of Large Language Models (LLMs) has significantly advanced
Text-to-SQL (NL2SQL) systems, yet evaluating the semantic equivalence of
generated SQL remains a challenge, especially given ambiguous user queries and
multiple valid SQL interpretations. This paper explores using LLMs to assess
both semantic and a more practical "weak" semantic equivalence. We analyze
common patterns of SQL equivalence and inequivalence, discuss challenges in
LLM-based evaluation.

</details>


### [16] [COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content](https://arxiv.org/abs/2506.09367)
*Zhengyuan Liu,Stella Xin Yin,Dion Hoe-Lian Goh,Nancy F. Chen*

Key words: 生成式AI, 教育内容, STEM, 可读性控制, 课程导向

TL;DR: COGENT是一个课程导向的框架，用于生成适合年级的教育内容，通过结合课程标准和可读性控制，解决了生成式AI在教育中的挑战。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 生成式AI在教育中的应用面临课程标准对齐和年级可读性控制的挑战，尤其是在STEM教育中。

Method: COGENT结合科学概念、核心思想和学习目标，控制可读性，并采用“基于好奇”的方法提高学生兴趣。

Result: 多维度评估显示，COGENT生成的文本优于或等同于人工参考内容。

Conclusion: COGENT为扩展自适应、高质量学习资源提供了可行方案。

Abstract: While Generative AI has demonstrated strong potential and versatility in
content generation, its application to educational contexts presents several
challenges. Models often fail to align with curriculum standards and maintain
grade-appropriate reading levels consistently. Furthermore, STEM education
poses additional challenges in balancing scientific explanations with everyday
language when introducing complex and abstract ideas and phenomena to younger
students. In this work, we propose COGENT, a curriculum-oriented framework for
generating grade-appropriate educational content. We incorporate three
curriculum components (science concepts, core ideas, and learning objectives),
control readability through length, vocabulary, and sentence complexity, and
adopt a ``wonder-based'' approach to increase student engagement and interest.
We conduct a multi-dimensional evaluation via both LLM-as-a-judge and human
expert analysis. Experimental results show that COGENT consistently produces
grade-appropriate passages that are comparable or superior to human references.
Our work establishes a viable approach for scaling adaptive and high-quality
learning resources.

</details>


### [17] [RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling](https://arxiv.org/abs/2506.08672)
*Yang Liu,Jiaqi Li,Zilong Zheng*

Key words: 规则推理, 小型推理模型, 强化学习, 动态采样, 领域感知

TL;DR: 论文提出了一种名为 RuleReasoner 的方法，通过强化学习增强小型推理模型（SRMs）在多样化任务和领域中的规则推理能力，显著优于现有大型推理模型（LRMs）。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现实应用中规则格式、类型和复杂性的多样化给规则推理带来挑战，现有大型推理模型性能虽出色，但小型推理模型的泛化能力尚未明确。

Method: 提出 Reinforced Rule-based Reasoning（RuleReasoner），通过动态采样策略和领域感知方法，结合强化学习优化训练批次，无需人工干预。

Result: RuleReasoner 在分布内（ID）和分布外（OOD）任务上显著优于前沿大型推理模型（分别提升 4.1% 和 10.4%），且计算效率更高。

Conclusion: RuleReasoner 证明了小型推理模型通过强化学习能够高效学习规则推理，并具备跨任务和领域的泛化能力。

Abstract: Rule-based reasoning has been acknowledged as one of the fundamental problems
in reasoning, while deviations in rule formats, types, and complexity in
real-world applications pose severe challenges. Recent studies have shown that
large reasoning models (LRMs) have remarkable reasoning capabilities, and their
performance is substantially enhanced by reinforcement learning (RL). However,
it remains an open question whether small reasoning models (SRMs) can learn
rule-based reasoning effectively with robust generalization across diverse
tasks and domains. To address this, we introduce Reinforced Rule-based
Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct
rule-based reasoning via a wide collection of curated tasks and a novel
domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples
each training batch by updating the sampling weights of different domains based
on historical rewards. This facilitates domain augmentation and flexible online
learning schedules for RL, obviating the need for pre-hoc human-engineered
mix-training recipes used in existing methods. Empirical evaluations on
in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that
RuleReasoner outperforms frontier LRMs by a significant margin ($\Delta$4.1%
average points on eight ID tasks and $\Delta$10.4% average points on three OOD
tasks over OpenAI-o1). Notably, our approach also exhibits higher computational
efficiency compared to prior dynamic sampling methods for RL.

</details>


### [18] [CoLMbo: Speaker Language Model for Descriptive Profiling](https://arxiv.org/abs/2506.09375)
*Massa Baali,Shuo Han,Syed Abdul Hannan,Purusottam Samal,Karanveer Singh,Soham Deshmukh,Rita Singh,Bhiksha Raj*

Key words: 说话人识别,语言模型,提示条件,零样本学习

TL;DR: CoLMbo是一种新型的说话人语言模型（SLM），通过整合说话人编码器和提示条件，解决了现有系统在生成详细说话人特征方面的不足，能够根据用户定义的提示动态生成定制化描述，适用于零样本场景。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的说话人识别系统通常局限于分类任务，难以生成详细的说话人特征或上下文丰富的描述。它们主要提取嵌入用于说话人识别，但无法以结构化方式捕捉方言、性别和年龄等人口统计属性。

Method: CoLMbo整合了说话人编码器与提示条件，利用用户定义的提示动态适应新的说话人特征，生成包括方言和年龄特征在内的定制化描述。

Result: CoLMbo在传统说话人分析的基础上进行了增强，并在多样化的数据集上表现出色，显著提升了说话人识别领域的性能。

Conclusion: CoLMbo的创新方法不仅改进了传统说话人分析，还为说话人识别领域带来重要进展。

Abstract: Speaker recognition systems are often limited to classification tasks and
struggle to generate detailed speaker characteristics or provide context-rich
descriptions. These models primarily extract embeddings for speaker
identification but fail to capture demographic attributes such as dialect,
gender, and age in a structured manner. This paper introduces CoLMbo, a Speaker
Language Model (SLM) that addresses these limitations by integrating a speaker
encoder with prompt-based conditioning. This allows for the creation of
detailed captions based on speaker embeddings. CoLMbo utilizes user-defined
prompts to adapt dynamically to new speaker characteristics and provides
customized descriptions, including regional dialect variations and age-related
traits. This innovative approach not only enhances traditional speaker
profiling but also excels in zero-shot scenarios across diverse datasets,
marking a significant advancement in the field of speaker recognition.

</details>


### [19] [Binary classification for perceived quality of headlines and links on worldwide news websites, 2018-2024](https://arxiv.org/abs/2506.09381)
*Austin McCutcheon,Thiago E. A. de Oliveira,Aleksandr Zheleznov,Chris Brogly*

Key words: 新闻质量,机器学习,DistilBERT,NLP特征,集成学习

TL;DR: 研究通过机器学习模型自动区分高质量与低质量新闻标题/链接，传统集成方法和微调DistilBERT模型表现良好。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 在线新闻泛滥导致低质量新闻标题/链接广泛传播，需自动化区分高低质量新闻。

Method: 使用12种机器学习模型和115个语言特征，在5744万条新闻链接/标题上训练（二分类，标签基于专家评估）。

Result: 传统集成方法（如Bagging分类器）准确率达88.1%，微调DistilBERT模型最高达90.3%。

Conclusion: NLP特征结合传统分类器或深度学习模型能有效区分新闻质量，但需平衡性能与训练时间。

Abstract: The proliferation of online news enables potential widespread publication of
perceived low-quality news headlines/links. As a result, we investigated
whether it was possible to automatically distinguish perceived lower-quality
news headlines/links from perceived higher-quality headlines/links. We
evaluated twelve machine learning models on a binary, balanced dataset of
57,544,214 worldwide news website links/headings from 2018-2024 (28,772,107 per
class) with 115 extracted linguistic features. Binary labels for each text were
derived from scores based on expert consensus regarding the respective news
domain quality. Traditional ensemble methods, particularly the bagging
classifier, had strong performance (88.1% accuracy, 88.3% F1, 80/20 train/test
split). Fine-tuned DistilBERT achieved the highest accuracy (90.3%, 80/20
train/test split) but required more training time. The results suggest that
both NLP features with traditional classifiers and deep learning models can
effectively differentiate perceived news headline/link quality, with some
trade-off between predictive performance and train time.

</details>


### [20] [Comparing human and LLM politeness strategies in free production](https://arxiv.org/abs/2506.09391)
*Haoran Zhao,Robert D. Hawkins*

Key words: 礼貌语言、大型语言模型、语用学、对齐、人际沟通

TL;DR: 研究探讨了大型语言模型（LLMs）在礼貌语言使用方面的表现，发现大模型能够复现人类偏好，但过度依赖负面礼貌策略可能导致误解。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 礼貌语言对LLMs构成了对齐挑战，研究旨在评估模型是否能像人类一样灵活运用礼貌策略。

Method: 通过比较人类和LLM在受限和开放式任务中的回应，分析其礼貌策略使用。

Result: 大模型能复现人类偏好，但在正面语境中过度依赖负面策略，可能导致误解。

Conclusion: LLMs在礼貌语言上表现优异，但仍有细微差异，需进一步研究其对AI系统实用性的影响。

Abstract: Polite speech poses a fundamental alignment challenge for large language
models (LLMs). Humans deploy a rich repertoire of linguistic strategies to
balance informational and social goals -- from positive approaches that build
rapport (compliments, expressions of interest) to negative strategies that
minimize imposition (hedging, indirectness). We investigate whether LLMs employ
a similarly context-sensitive repertoire by comparing human and LLM responses
in both constrained and open-ended production tasks. We find that larger models
($\ge$70B parameters) successfully replicate key preferences from the
computational pragmatics literature, and human evaluators surprisingly prefer
LLM-generated responses in open-ended contexts. However, further linguistic
analyses reveal that models disproportionately rely on negative politeness
strategies even in positive contexts, potentially leading to
misinterpretations. While modern LLMs demonstrate an impressive handle on
politeness strategies, these subtle differences raise important questions about
pragmatic alignment in AI systems.

</details>


### [21] [A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings](https://arxiv.org/abs/2506.09393)
*Xinyi Gao,Qiucheng Wu,Yang Zhang,Xuechen Liu,Kaizhi Qian,Ying Xu,Shiyu Chang*

Key words: 知识追踪、低资源、知识树、隐马尔可夫树模型、EM算法

TL;DR: KT$^2$是一种基于知识树的概率框架，利用隐马尔可夫树模型在低资源条件下提升学生知识追踪性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决在低资源和在线更新场景下，现有知识追踪方法性能不足的问题。

Method: 提出KT$^2$框架，利用知识概念的层次结构建模学生掌握情况，支持EM算法和增量更新。

Result: 在低资源在线场景中，KT$^2$表现优于基线方法。

Conclusion: KT$^2$通过利用层次知识概念和增量更新机制，有效提升了知识追踪在低资源条件下的性能。

Abstract: Knowledge tracing (KT) aims to estimate a student's evolving knowledge state
and predict their performance on new exercises based on performance history.
Many realistic classroom settings for KT are typically low-resource in data and
require online updates as students' exercise history grows, which creates
significant challenges for existing KT approaches. To restore strong
performance under low-resource conditions, we revisit the hierarchical
knowledge concept (KC) information, which is typically available in many
classroom settings and can provide strong prior when data are sparse. We
therefore propose Knowledge-Tree-based Knowledge Tracing (KT$^2$), a
probabilistic KT framework that models student understanding over a
tree-structured hierarchy of knowledge concepts using a Hidden Markov Tree
Model. KT$^2$ estimates student mastery via an EM algorithm and supports
personalized prediction through an incremental update mechanism as new
responses arrive. Our experiments show that KT$^2$ consistently outperforms
strong baselines in realistic online, low-resource settings.

</details>


### [22] [Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models](https://arxiv.org/abs/2506.09408)
*Jui-Ming Yao,Hao-Yuan Chen,Zi-Xian Tang,Bing-Jia Tan,Sheng-Wei Peng,Bing-Cheng Xie,Shun-Feng Su*

Key words: 大型语言模型, Token Constraint Decoding, 多选题回答, 鲁棒性, 提示工程

TL;DR: 论文提出了Token Constraint Decoding (TCD)方法，通过增强语言模型的抗干扰能力，显著提升了其在噪声环境下的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型在多选题回答任务中表现出色，但对输入扰动非常敏感，因此需要一种方法提升其鲁棒性。

Method: 提出了TCD算法，通过对齐令牌级预测来增强模型的抗噪能力，并与提示工程结合使用。

Result: TCD显著提升了模型在噪声环境下的性能，较弱模型性能提升高达39%，且能隐式正则化过自信的输出。

Conclusion: TCD是一种实用的、模型无关的方法，为语言模型在关键安全或用户交互应用中提供了更可靠的部署途径。

Abstract: Large Language Models (LLMs) have demonstrated impressive performance on
multiple-choice question answering (MCQA) benchmarks, yet they remain highly
vulnerable to minor input perturbations. In this paper, we introduce and
evaluate Token Constraint Decoding (TCD). This simple yet effective
inference-time algorithm enforces alignment between token-level predictions to
enhance robustness in noisy settings. Through extensive experiments on
CommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired
with prompt engineering (PE) fixes, significantly restores performance degraded
by input noise, yielding up to +39\% absolute gains for weaker models like
Gemma3 1B. Penalty sweep analyses further reveal that TCD implicitly
regularizes overconfident outputs, with different models requiring distinct
penalty schedules to maximize resilience. Our findings establish TCD as a
practical, model-agnostic approach for improving reasoning stability under
real-world imperfections and pave the way for more reliable deployment of LLMs
in safety-critical or user-facing applications.

</details>


### [23] [PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering](https://arxiv.org/abs/2506.09414)
*Xiujun Zhou,Pingjian Zhang,Deyou Tang*

Key words: 知识图谱问答, 数据增强, 多跳推理, 提示设计, 大语言模型

TL;DR: PGDA-KGQA是一种基于提示引导的生成框架，通过多种数据增强策略解决知识图谱问答中的多样性和多跳推理问题，显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有方法在多样性和多跳推理方面存在局限性，传统数据增强方法易导致语义失真，而基于LLM的方法则忽视了多跳推理。PGDA-KGQA旨在解决这些问题。

Method: PGDA-KGQA通过提示设计引导LLM生成大规模（问题、逻辑形式）对，并采用单跳伪问题生成、语义保留改写和答案引导的反向路径探索三种策略增强数据。

Result: 在WebQSP和ComplexWebQuestions数据集上，PGDA-KGQA在F1、Hits@1和准确率上分别提升了2.8%、1.2%、3.1%和1.8%、1.1%、2.4%。

Conclusion: PGDA-KGQA通过增强数据多样性和多跳推理能力，显著提高了知识图谱问答的性能。

Abstract: Knowledge Graph Question Answering (KGQA) is a crucial task in natural
language processing that requires reasoning over knowledge graphs (KGs) to
answer natural language questions. Recent methods utilizing large language
models (LLMs) have shown remarkable semantic parsing capabilities but are
limited by the scarcity of diverse annotated data and multi-hop reasoning
samples. Traditional data augmentation approaches are focus mainly on
single-hop questions and prone to semantic distortion, while LLM-based methods
primarily address semantic distortion but usually neglect multi-hop reasoning,
thus limiting data diversity. The scarcity of multi-hop samples further weakens
models' generalization. To address these issues, we propose PGDA-KGQA, a
prompt-guided generative framework with multiple data augmentation strategies
for KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by
crafting meticulously engineered prompts that integrate the provided textual
content, it leverages LLMs to generate large-scale (question, logical form)
pairs for model training. Specifically, PGDA-KGQA enriches its training set by:
(1) generating single-hop pseudo questions to improve the alignment of question
semantics with KG relations; (2) applying semantic-preserving question
rewriting to improve robustness against linguistic variations; (3) employing
answer-guided reverse path exploration to create realistic multi-hop questions.
By adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA
utilizes the augmented data to enhance the accuracy of logical form generation
and thus improve answer retrieval performance. Experiments demonstrate that
outperforms state-of-the-art methods on standard KGQA datasets, achieving
improvements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by
1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.

</details>


### [24] [Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings](https://arxiv.org/abs/2506.09424)
*Md Messal Monem Miah,Adrita Anika,Xi Shi,Ruihong Huang*

Key words: 欺骗检测,大型语言模型,多模态模型,零样本学习,少样本学习

TL;DR: 本文系统评估了大型语言模型（LLMs）和多模态模型（LMMs）在多种领域的自动欺骗检测能力，分析了不同实验设置的性能，并探讨了其在现实应用中的潜力与限制。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 在数字化时代，欺骗检测至关重要但具有挑战性，本文旨在探索LLMs和LMMs在跨模态欺骗检测中的表现和应用潜力。

Method: 通过三个数据集（RLTD、MU3D、OpSpam）评估开源和商用模型的性能，研究零样本和少样本学习等实验设置，并分析辅助特征和提示策略的效果。

Result: 微调后的LLMs在文本欺骗检测任务中表现最佳，而LMMs在多模态线索利用上表现不佳。提示策略和辅助特征对结果有显著影响。

Conclusion: LLMs在多领域欺骗检测中展现了潜力，但LMMs的多模态能力仍需改进，为实际应用提供了重要参考。

Abstract: Detecting deception in an increasingly digital world is both a critical and
challenging task. In this study, we present a comprehensive evaluation of the
automated deception detection capabilities of Large Language Models (LLMs) and
Large Multimodal Models (LMMs) across diverse domains. We assess the
performance of both open-source and commercial LLMs on three distinct datasets:
real life trial interviews (RLTD), instructed deception in interpersonal
scenarios (MU3D), and deceptive reviews (OpSpam). We systematically analyze the
effectiveness of different experimental setups for deception detection,
including zero-shot and few-shot approaches with random or similarity-based
in-context example selection. Our results show that fine-tuned LLMs achieve
state-of-the-art performance on textual deception detection tasks, while LMMs
struggle to fully leverage cross-modal cues. Additionally, we analyze the
impact of auxiliary features, such as non-verbal gestures and video summaries,
and examine the effectiveness of different prompting strategies, including
direct label generation and chain-of-thought reasoning. Our findings provide
key insights into how LLMs process and interpret deceptive cues across
modalities, highlighting their potential and limitations in real-world
deception detection applications.

</details>


### [25] [Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting](https://arxiv.org/abs/2506.09428)
*Fei Ding,Baiqiao Wang*

Key words: 监督微调,SFT,灾难性遗忘,大型语言模型,任务特定性能

TL;DR: 提出了一种新的SFT方法，以减少灾难性遗忘的风险，同时提升任务特定性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决监督微调(SFT)对大型语言模型(LLMs)通用能力的削弱问题，尤其是在无法访问原始预训练数据时。

Method: 重构基础模型的SFT指令分布，通过多模型筛选选择最优数据，与新数据混合后进行SFT。

Result: 在保持通用领域能力的同时，提升了任务特定性能。

Conclusion: 该方法有效减少了灾难性遗忘，且更具成本效益。

Abstract: Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)'
instruction-following capabilities and domain-specific task adaptability, often
diminishes their general capabilities. Moreover, due to the inaccessibility of
original pre-training data, catastrophic forgetting tends to be exacerbated
when third-party practitioners implement SFT on open-sourced models. To address
this challenge, we propose a novel, more cost-effective SFT method which could
effectively reduce the risk of catastrophic forgetting without access to
original SFT data. Our approach begins by reconstructing the likely SFT
instruction distribution of the base model, followed by a multi-model screening
process to select optimal data, which is then mixed with new data for SFT.
Experimental results demonstrate that our method preserves generalization
capabilities in general domains while improving task-specific performance.

</details>


### [26] [GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture](https://arxiv.org/abs/2506.09440)
*GigaChat team,Mamedov Valentin,Evgenii Kosarev,Gregory Leleytner,Ilya Shchuckin,Valeriy Berezovskiy,Daniil Smirnov,Dmitry Kozlov,Sergei Averkiev,Lukyanenko Ivan,Aleksandr Proshunin,Ainur Israfilova,Ivan Baskov,Artem Chervyakov,Emil Shakirov,Mikhail Kolesov,Daria Khomich,Darya Latortseva,Sergei Porkhun,Yury Fedorov,Oleg Kutuzov,Polina Kudriavtseva,Sofiia Soldatova,Kolodin Egor,Stanislav Pyatkin,Dzmitry Menshykh,Grafov Sergei,Eldar Damirov,Karlov Vladimir,Ruslan Gaitukiev,Arkadiy Shatenov,Alena Fenogenova,Nikita Savushkin,Fedor Minkin*

Key words: 大语言模型, 俄语, GigaChat, 机器学习, NLP

TL;DR: 论文介绍了GigaChat系列俄语大语言模型，包括基础模型和指令调优版本，详细描述了模型架构、预训练过程和实验，并评估了其性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 针对俄语的大语言模型开发受限，主要因计算资源需求高，本文旨在填补这一空白。

Method: 介绍GigaChat系列模型的架构、预训练过程，并进行实验设计。

Result: 评估了模型在俄语和英语基准上的性能，并与多语言模型对比，展示了API、Telegram机器人和Web接口的演示。

Conclusion: 发布了三个开源GigaChat模型，旨在支持俄语NLP研究和工业应用开发。

Abstract: Generative large language models (LLMs) have become crucial for modern NLP
research and applications across various languages. However, the development of
foundational models specifically tailored to the Russian language has been
limited, primarily due to the significant computational resources required.
This paper introduces the GigaChat family of Russian LLMs, available in various
sizes, including base models and instruction-tuned versions. We provide a
detailed report on the model architecture, pre-training process, and
experiments to guide design choices. In addition, we evaluate their performance
on Russian and English benchmarks and compare GigaChat with multilingual
analogs. The paper presents a system demonstration of the top-performing models
accessible via an API, a Telegram bot, and a Web interface. Furthermore, we
have released three open GigaChat models in open-source
(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities
and support the development of industrial solutions for the Russian language.

</details>


### [27] [UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs](https://arxiv.org/abs/2506.09450)
*Prameshwar Thiyagarajan,Vaishnavi Parimi,Shamant Sai,Soumil Garg,Zhangir Meirbek,Nitin Yarlagadda,Kevin Zhu,Chris Kim*

Key words: Theory of Mind, UniToMBench, LLMs, 评估基准, 社会认知

TL;DR: 论文介绍了UniToMBench，一个结合SimToM和TOMBENCH优点的统一基准，用于评估和改进大语言模型(LLMs)的'心智理论'(ToM)能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为了系统性改进和评估LLMs在ToM任务中的表现，发现当前模型在情感和信念相关任务中表现良好，但在知识类任务中表现不一致。

Method: 设计包含多交互任务和动态故事场景的UniToMBench，使用1000多个手写场景和多样化的评估指标。

Result: GPT-4o等模型在情感和信念任务中准确率超过80%，但在知识类任务中表现不稳定。

Conclusion: UniToMBench是一个全面的工具，展示了当前LLMs在ToM任务中的强项与局限。

Abstract: Theory of Mind (ToM), the ability to understand the mental states of oneself
and others, remains a challenging area for large language models (LLMs), which
often fail to predict human mental states accurately. In this paper, we
introduce UniToMBench, a unified benchmark that integrates the strengths of
SimToM and TOMBENCH to systematically improve and assess ToM capabilities in
LLMs by integrating multi-interaction task designs and evolving story
scenarios. Supported by a custom dataset of over 1,000 hand-written scenarios,
UniToMBench combines perspective-taking techniques with diverse evaluation
metrics to better stimulate social cognition in LLMs. Through evaluation, we
observe that while models like GPT-4o and GPT-4o Mini show consistently high
accuracy in tasks involving emotional and belief-related scenarios, with
results usually above 80%, there is significant variability in their
performance across knowledge-based tasks. These results highlight both the
strengths and limitations of current LLMs in ToM-related tasks, underscoring
the value of UniToMBench as a comprehensive tool for future development. Our
code is publicly available here:
https://github.com/Shamant/unifiedtombenchmark.

</details>


### [28] [Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms](https://arxiv.org/abs/2506.09457)
*Zeguan Xiao,Yun Chen,Guanhua Chen*

Key words: 直接对齐算法, POET, 奖励生成差距, DPO, SimPO

TL;DR: 介绍了一种名为POET的方法，通过截断响应长度来优化直接对齐算法（DAAs）的奖励生成差距。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 直接对齐算法（DAAs）如DPO和SimPO在训练目标和生成性能之间存在不匹配，称为"奖励生成差距"，POET旨在解决此问题。

Method: 提出POET方法，将偏好和非偏好响应截断为相同长度，优化DAAs的目标函数，使其更关注前缀token。

Result: 实验表明POET在DPO和SimPO上表现优异，AlpacaEval 2提升15.6分，下游任务普遍改善。

Conclusion: POET通过解决奖励优化与生成性能的不匹配问题，显著提升了DAAs的效果。

Abstract: Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization
(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient
alternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms
for aligning large language models (LLMs) with human preferences. However, DAAs
suffer from a fundamental limitation we identify as the "reward-generation gap"
-- a misalignment between optimization objectives during training and actual
generation performance during inference. In this paper, we find a contributor
to the reward-generation gap is the mismatch between the inherent importance of
prefix tokens during the LLM generation process and how this importance is
reflected in the implicit reward functions of DAAs. To bridge the gap, we
introduce a simple yet effective approach called Prefix-Oriented Equal-length
Training (POET), which truncates both preferred and dispreferred responses to
match the shorter one's length. Training with POET, where both responses in
each sample are truncated to equal length, resulting in diverse truncated
lengths across samples, the optimization of DAAs objective is implicitly
constrained to converge across all positions, thus paying more attention to
prefix tokens than the standard DAAs. We conduct experiments with DPO and
SimPO, two representative DAAs, demonstrating that POET improves over their
standard implementations, achieving up to 15.6 points in AlpacaEval 2 and
overall improvements across downstream tasks. Our results highlight the
importance of addressing the misalignment between reward optimization and
generation performance in DAAs.

</details>


### [29] [Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers](https://arxiv.org/abs/2506.09495)
*Ilanit Sobol,Shir Lissak,Refael Tikochinski,Tal Nakash,Anat Brunstein Klomek,Eyal Fruchter,Roi Reichart*

Key words: 自杀行为, 社交媒体, YouTube, 心理健康, 数字足迹

TL;DR: 研究探讨了YouTube上自杀行为的数字足迹，结合计算、混合和专家驱动方法，发现心理健康问题和平台参与度与自杀尝试相关。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 自杀是西方国家主要死因之一，社交媒体成为研究自杀行为的新途径。

Method: 采用计算自下而上、混合和专家驱动自上而下方法，分析181个有自杀尝试的YouTube频道和134个对照频道。

Result: 发现五个与自杀尝试相关的主题，其中心理健康问题和YouTube参与度随时间变化显著。专家未发现的平台特定行为也具价值。

Conclusion: 整合多方法揭示了自杀行为的数字表现与临床见解的差异。

Abstract: Suicide remains a leading cause of death in Western countries, underscoring
the need for new research approaches. As social media becomes central to daily
life, digital footprints offer valuable insight into suicidal behavior.
Focusing on individuals who attempted suicide while uploading videos to their
channels, we investigate: How do suicidal behaviors manifest on YouTube, and
how do they differ from expert knowledge? We applied complementary approaches:
computational bottom-up, hybrid, and expert-driven top-down, on a novel
longitudinal dataset of 181 YouTube channels from individuals with
life-threatening attempts, alongside 134 control channels. In the bottom-up
approach, we applied LLM-based topic modeling to identify behavioral
indicators. Of 166 topics, five were associated with suicide-attempt, with two
also showing temporal attempt-related changes ($p<.01$) - Mental Health
Struggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,
a clinical expert reviewed LLM-derived topics and flagged 19 as
suicide-related. However, none showed significant attempt-related temporal
effects beyond those identified bottom-up. Notably, YouTube Engagement, a
platform-specific indicator, was not flagged by the expert, underscoring the
value of bottom-up discovery. In the top-down approach, psychological
assessment of suicide attempt narratives revealed that the only significant
difference between individuals who attempted before and those attempted during
their upload period was the motivation to share this experience: the former
aimed to Help Others ($\beta=-1.69$, $p<.01$), while the latter framed it as
part of their Personal Recovery ($\beta=1.08$, $p<.01$). By integrating these
approaches, we offer a nuanced understanding of suicidality, bridging digital
behavior and clinical insights.
  * Within-group changes in relation to the suicide attempt.

</details>


### [30] [Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning](https://arxiv.org/abs/2506.09501)
*Jiayi Yuan,Hao Li,Xinheng Ding,Wenya Xie,Yu-Jhe Li,Wentian Zhao,Kun Wan,Jing Shi,Xia Hu,Zirui Liu*

Key words: 大型语言模型，性能重现性，浮点运算，LayerCast，数值稳定性

TL;DR: 研究发现大型语言模型（LLMs）的性能重现性受系统配置（如GPU数量、类型和评估批次大小）影响显著，尤其是在推理模型中。浮点运算的非关联性是主要原因，提出了轻量级推理管道LayerCast以平衡内存效率和数值稳定性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管LLMs在各领域表现优异，但其性能的准确性和重现性依赖于基准分数。本研究旨在揭示LLM性能重现性脆弱的问题及其根源。

Method: 通过在不同硬件、软件和精度设置下的受控实验，量化模型输出的差异，并提出LayerCast管道解决数值稳定性问题。

Result: 实验显示，GPU配置和评估批次大小可导致推理模型准确率高达9%的变化，以及9,000个令牌的响应长度差异。LayerCast有效平衡了内存效率和数值稳定性。

Conclusion: 浮点精度对LLM重现性至关重要，但常被忽视。LayerCast提供了一种实用解决方案，代码已开源。

Abstract: Large Language Models (LLMs) are now integral across various domains and have
demonstrated impressive performance. Progress, however, rests on the premise
that benchmark scores are both accurate and reproducible. We demonstrate that
the reproducibility of LLM performance is fragile: changing system
configuration such as evaluation batch size, GPU count, and GPU version can
introduce significant difference in the generated responses. This issue is
especially pronounced in reasoning models, where minor rounding differences in
early tokens can cascade into divergent chains of thought, ultimately affecting
accuracy. For instance, under bfloat16 precision with greedy decoding, a
reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation
in accuracy and 9,000 tokens difference in response length due to differences
in GPU count, type, and evaluation batch size. We trace the root cause of this
variability to the non-associative nature of floating-point arithmetic under
limited numerical precision. This work presents the first systematic
investigation into how numerical precision affects reproducibility in LLM
inference. Through carefully controlled experiments across various hardware,
software, and precision settings, we quantify when and how model outputs
diverge. Our analysis reveals that floating-point precision -- while critical
for reproducibility -- is often neglected in evaluation practices. Inspired by
this, we develop a lightweight inference pipeline, dubbed LayerCast, that
stores weights in 16-bit precision but performs all computations in FP32,
balancing memory efficiency with numerical stability. Code is available at
https://github.com/nanomaoli/llm_reproducibility.

</details>


### [31] [TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding](https://arxiv.org/abs/2506.09507)
*Bingheng Wu,Jingze Shi,Yifan Wu,Nan Tang,Yuyu Luo*

Key words: Transformer, State Space Model, 旋转位置嵌入, 混合架构, 长上下文建模

TL;DR: 提出了一种统一的旋转位置嵌入方法（ourRoPE），解决了Transformer和状态空间模型（SSM）在位置编码上的不兼容问题，并基于此设计了混合架构model，显著提升了性能和效率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Transformer和SSM在位置编码机制上存在不兼容性，导致混合架构的性能和效率受限，因此需要一种统一的解决方案。

Method: 提出了统一的旋转位置嵌入方法（ourRoPE），并为Transformer和SSM设计了一种混合架构（model）。

Result: 在4K序列长度下，model的训练和推理速度分别比标准Transformer快42.3%和29.5%，且在语言建模任务中准确率提升4%以上。此外，model-1.3B版本比320M版本平均准确率提升7.22%。

Conclusion: 统一的位置编码解决了混合模型中的位置不兼容问题，实现了高效且高性能的长上下文建模。

Abstract: Transformers exhibit proficiency in capturing long-range dependencies,
whereas State Space Models (SSMs) facilitate linear-time sequence modeling.
Notwithstanding their synergistic potential, the integration of these
architectures presents a significant challenge, primarily attributable to a
fundamental incongruity in their respective positional encoding mechanisms:
Transformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs
leverage implicit positional representations via convolutions. This divergence
often precipitates discontinuities and suboptimal performance. To address this
impediment, we propose a unified rotary position embedding (\textbf{\ourRoPE})
methodology, thereby establishing a consistent positional encoding framework
for both self-attention and state-space components. Using this \ourRoPE, we
introduce \textbf{\model}, a hybrid architecture that coherently integrates the
Transformer and SSM layers under this unified positional encoding scheme. At a
4K sequence length, \model exhibits training and inference speeds that are
\textbf{42.3\% and 29.5\% faster}, respectively, relative to standard
Transformer models. It also delivers higher accuracy: under comparable
settings, it surpasses a Transformer baseline by over 4\% on language modeling
benchmarks. \model furthermore scales more effectively: \model-1.3B gains
\textbf{7.22\%} in average accuracy over its 320M version (versus about 6\%
gains for equivalent Transformers or SSMs). Our results show that unified
positional encoding resolves positional incompatibility in hybrid models,
enabling efficient, high-performance long-context modeling.

</details>


### [32] [ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning](https://arxiv.org/abs/2506.09513)
*Yu Sun,Xingyu Qian,Weiwen Xu,Hao Zhang,Chenghao Xiao,Long Li,Yu Rong,Wenbing Huang,Qifeng Bai,Tingyang Xu*

Key words: 大型语言模型、医学问答、推理数据集、多智能体验证、Chain-of-Thought

TL;DR: 论文介绍了ReasonMed，最大的医学推理数据集，通过多智能体验证和优化过程构建，结合详细推理链和简洁答案摘要的训练策略，使ReasonMed-7B在子10B模型中表现最佳。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索基于推理的大型语言模型（LLMs）在知识密集型医学问答中的潜力，填补当前研究的空白。

Method: 通过多智能体验证和优化过程构建ReasonMed数据集，利用错误修正器优化推理路径，结合Chain-of-Thought和答案摘要进行模型训练。

Result: ReasonMed-7B在子10B模型中表现最佳，优于先前最佳模型4.17%，并在PubMedQA上超过LLaMA3.1-70B 4.60%。

Conclusion: 结合详细推理链和简洁答案摘要的训练策略在医学推理模型中效果最佳，ReasonMed数据集及模型为医学问答提供了新基准。

Abstract: Though reasoning-based large language models (LLMs) have excelled in
mathematics and programming, their capabilities in knowledge-intensive medical
question answering remain underexplored. To address this, we introduce
ReasonMed, the largest medical reasoning dataset, comprising 370k high-quality
examples distilled from 1.7 million initial reasoning paths generated by
various LLMs. ReasonMed is constructed through a \textit{multi-agent
verification and refinement process}, where we design an \textit{Error Refiner}
to enhance the reasoning paths by identifying and correcting error-prone steps
flagged by a verifier. Leveraging ReasonMed, we systematically investigate best
practices for training medical reasoning models and find that combining
detailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields
the most effective fine-tuning strategy. Based on this strategy, we train
ReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the
prior best by 4.17\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\%.

</details>


### [33] [KG-Infused RAG: Augmenting Corpus-Based RAG with External Knowledge Graphs](https://arxiv.org/abs/2506.09542)
*Dingjun Wu,Yukun Yan,Zhenghao Liu,Zhiyuan Liu,Maosong Sun*

Key words: RAG, 知识图谱, 扩散激活, QA基准, 多源检索

TL;DR: KG-Infused RAG通过结合知识图谱和RAG系统，实现了更高的知识关联性，显著提升了事实准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有RAG方法通常依赖单一知识源，缺乏认知启发机制，导致知识激活不足。

Method: 提出KG-Infused RAG框架，整合知识图谱并实现扩散激活机制，结合非结构化和结构化知识，优化生成过程。

Result: 在五个问答基准测试中，性能优于原始RAG（提升3.8%至13.8%），并可作为插件进一步优化其他RAG方法。

Conclusion: KG-Infused RAG有效提升知识检索的关联性和生成准确性，具有普适性和灵活性。

Abstract: Retrieval-Augmented Generation (RAG) improves factual accuracy by grounding
responses in external knowledge. However, existing methods typically rely on a
single source, either unstructured text or structured knowledge. Moreover, they
lack cognitively inspired mechanisms for activating relevant knowledge. To
address these issues, we propose KG-Infused RAG, a framework that integrates
KGs into RAG systems to implement spreading activation, a cognitive process
that enables concept association and inference. KG-Infused RAG retrieves KG
facts, expands the query accordingly, and enhances generation by combining
corpus passages with structured facts, enabling interpretable, multi-source
retrieval grounded in semantic structure. We further improve KG-Infused RAG via
preference learning on sampled key stages in the pipeline. Experiments on five
QA benchmarks show that KG-Infused RAG consistently outperforms vanilla RAG (by
3.8% to 13.8%). Additionally, when integrated into Self-RAG, KG-Infused RAG
brings further performance gains, demonstrating its effectiveness and
versatility as a plug-and-play enhancement module for corpus-based RAG methods.

</details>


### [34] [MEDUSA: A Multimodal Deep Fusion Multi-Stage Training Framework for Speech Emotion Recognition in Naturalistic Conditions](https://arxiv.org/abs/2506.09556)
*Georgios Chatzichristodoulou,Despoina Kosmopoulou,Antonios Kritikos,Anastasia Poulopoulou,Efthymios Georgiou,Athanasios Katsamanis,Vassilis Katsouros,Alexandros Potamianos*

Key words: 情感识别、多模态、类不平衡、DeepSER、元分类器

TL;DR: MEDUSA是一种多模态框架，用于解决情感识别中的类不平衡和情感模糊问题，通过四阶段训练流程和多种技术优化，在Interspeech 2025挑战赛中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 情感识别（SER）的挑战在于情感的客观性和自然条件下数据的不平衡分布。

Method: 采用四阶段训练流程：前两阶段训练基于DeepSER的集成分类器；后两阶段优化元分类器；结合数据采样和多任务学习。

Result: MEDUSA在Interspeech 2025挑战赛中排名第一。

Conclusion: MEDUSA框架能有效处理SER中的不平衡和模糊问题。

Abstract: SER is a challenging task due to the subjective nature of human emotions and
their uneven representation under naturalistic conditions. We propose MEDUSA, a
multimodal framework with a four-stage training pipeline, which effectively
handles class imbalance and emotion ambiguity. The first two stages train an
ensemble of classifiers that utilize DeepSER, a novel extension of a deep
cross-modal transformer fusion mechanism from pretrained self-supervised
acoustic and linguistic representations. Manifold MixUp is employed for further
regularization. The last two stages optimize a trainable meta-classifier that
combines the ensemble predictions. Our training approach incorporates human
annotation scores as soft targets, coupled with balanced data sampling and
multitask learning. MEDUSA ranked 1st in Task 1: Categorical Emotion
Recognition in the Interspeech 2025: Speech Emotion Recognition in Naturalistic
Conditions Challenge.

</details>


### [35] [Gender Bias in English-to-Greek Machine Translation](https://arxiv.org/abs/2506.09558)
*Eleni Gkovedarou,Joke Daems,Luna De Bruyne*

Key words: 机器翻译, 性别偏见, GPT-4o, 包容性语言, 英译希腊语

TL;DR: 研究分析了Google Translate和DeepL在英译希腊语中的性别偏见，发现两者在性别明确时表现较好，但在性别未指定时表现不佳。GPT-4o展示了缓解偏见的潜力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着对包容性语言需求的增加，机器翻译系统中的性别偏见问题引起了关注。

Method: 使用GendEL数据集分析两种商业翻译系统（Google Translate和DeepL）的性别偏见，并探索GPT-4o在缓解偏见中的作用。

Result: 两种翻译系统在性别明确时表现良好，但在性别未指定时表现不佳。GPT-4o能生成性别明确和中性的替代方案，但仍存在偏见。

Conclusion: 机器翻译系统在性别包容性方面仍有改进空间，GPT-4o展示了缓解偏见的潜力。

Abstract: As the demand for inclusive language increases, concern has grown over the
susceptibility of machine translation (MT) systems to reinforce gender
stereotypes. This study investigates gender bias in two commercial MT systems,
Google Translate and DeepL, focusing on the understudied English-to-Greek
language pair. We address three aspects of gender bias: i) male bias, ii)
occupational stereotyping, and iii) errors in anti-stereotypical translations.
Additionally, we explore the potential of prompted GPT-4o as a bias mitigation
tool that provides both gender-explicit and gender-neutral alternatives when
necessary. To achieve this, we introduce GendEL, a manually crafted bilingual
dataset of 240 gender-ambiguous and unambiguous sentences that feature
stereotypical occupational nouns and adjectives. We find persistent gender bias
in translations by both MT systems; while they perform well in cases where
gender is explicitly defined, with DeepL outperforming both Google Translate
and GPT-4o in feminine gender-unambiguous sentences, they are far from
producing gender-inclusive or neutral translations when the gender is
unspecified. GPT-4o shows promise, generating appropriate gendered and neutral
alternatives for most ambiguous cases, though residual biases remain evident.

</details>


### [36] [Towards Open Foundation Language Model and Corpus for Macedonian: A Low-Resource Language](https://arxiv.org/abs/2506.09560)
*Stefan Krsteski,Matea Tashkovska,Borjan Sazdov,Hristijan Gjoreski,Branislav Gerazov*

Key words: 大型语言模型、低资源语言、马其顿语、语料库、指令数据集

TL;DR: 论文提出了针对低资源语言马其顿语的资源建设，包括语料库、指令数据集和评估套件，并训练了一个8B参数的模型，性能优于同类模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为低资源语言（如马其顿语）提供工具支持，以促进LLMs的普及和研究。

Method: 收集40GB马其顿语料库、构建106k条指令数据集和评测套件，训练8B参数模型domestic-yak。

Result: 模型在8B参数范围内优于所有基线，性能接近更大模型，并获更高语法和文化评分。

Conclusion: 资源公开发布，为低资源语言的LLM研究奠定基础。

Abstract: The increase in technological adoption worldwide comes with demands for novel
tools to be used by the general population. Large Language Models (LLMs)
provide a great opportunity in this respect, but their capabilities remain
limited for low-resource languages, restricting applications in countries where
such languages are spoken. We create several resources to facilitate the
adoption of LLMs and to support research advancements for Macedonian. We
collect the largest Macedonian corpus to date, consisting of 40GB of textual
data and totaling 3.5B words. To support conversational applications, we
collect a 106k-instance instruction dataset, carefully built to be culturally
grounded. For evaluation, we construct a Macedonian evaluation suite covering
seven benchmarks. Finally, we train domestic-yak, a state-of-the-art
8B-parameter model, on our curated datasets and evaluate it against eight
baseline models using the newly constructed benchmark suite. Our model
outperforms all existing models in the 8B parameter range across all
benchmarks, and achieves performance comparable to models up to 10x larger.
Furthermore, a qualitative analysis with native speakers reveals that our model
is preferred over larger counterparts, receiving higher ratings for grammatical
correctness and cultural appropriateness. All datasets, code, and model weights
are openly released, setting a foundation for advancing LLMs in similarly
underrepresented languages. These resources are publicly available at
github.com/LVSTCK for source code, and at huggingface.co/LVSTCK for pretrained
model weights and data.

</details>


### [37] [From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies](https://arxiv.org/abs/2506.09566)
*Blaž Škrlj,Boshko Koloski,Senja Pollak,Nada Lavrač*

Key words: 知识图谱、大型语言模型、结构化知识、推理能力、神经符号集成

TL;DR: 本文综述了知识图谱（KGs）与大型语言模型（LLMs）的协同作用，分类了两者的集成方法，并探讨了未来研究方向。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 增强LLMs的事实基础和推理能力，同时推动KGs的构建与应用。

Method: 将现有方法分为两类：KG增强的LLMs和LLM增强的KGs，并通过系统分析识别关键问题。

Result: 研究发现结构化知识集成具有显著优势，并提出了未来研究方向。

Conclusion: 结合KGs和LLMs为智能系统处理复杂知识任务提供了新途径。

Abstract: Integrating structured knowledge from Knowledge Graphs (KGs) into Large
Language Models (LLMs) enhances factual grounding and reasoning capabilities.
This survey paper systematically examines the synergy between KGs and LLMs,
categorizing existing approaches into two main groups: KG-enhanced LLMs, which
improve reasoning, reduce hallucinations, and enable complex question
answering; and LLM-augmented KGs, which facilitate KG construction, completion,
and querying. Through comprehensive analysis, we identify critical gaps and
highlight the mutual benefits of structured knowledge integration. Compared to
existing surveys, our study uniquely emphasizes scalability, computational
efficiency, and data quality. Finally, we propose future research directions,
including neuro-symbolic integration, dynamic KG updating, data reliability,
and ethical considerations, paving the way for intelligent systems capable of
managing more complex real-world knowledge tasks.

</details>


### [38] [Memorization in Language Models through the Lens of Intrinsic Dimension](https://arxiv.org/abs/2506.09591)
*Stefan Arnold*

Key words: 语言模型, 内在维度（ID）, 记忆行为, 隐私保护, 过参数化模型

TL;DR: 研究了语言模型在训练过程中对数据的记忆行为，发现内在维度（ID）作为结构复杂性的代理，能够抑制记忆行为，高ID序列比低ID序列更难被记忆，尤其是在过参数化模型和稀疏暴露情况下。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨语言模型在训练过程中对数据的记忆问题，特别是隐私泄露和知识产权泄漏的风险，研究如何通过序列的内在结构复杂性（ID）来调节记忆率。

Method: 通过分析内在维度（ID）作为序列潜在空间结构复杂性的几何代理，研究其如何调节语言模型的记忆行为，重点关注过参数化模型和稀疏暴露条件。

Result: 高ID序列比低ID序列更不容易被记忆，尤其是在过参数化模型和稀疏暴露情况中，表明ID可以作为抑制记忆的信号。

Conclusion: 内在维度（ID）与记忆率之间存在负相关关系，为理解语言模型记忆行为的复杂性提供了新的视角。

Abstract: Language Models (LMs) are prone to memorizing parts of their data during
training and unintentionally emitting them at generation time, raising concerns
about privacy leakage and disclosure of intellectual property. While previous
research has identified properties such as context length, parameter size, and
duplication frequency, as key drivers of unintended memorization, little is
known about how the latent structure modulates this rate of memorization. We
investigate the role of Intrinsic Dimension (ID), a geometric proxy for the
structural complexity of a sequence in latent space, in modulating
memorization. Our findings suggest that ID acts as a suppressive signal for
memorization: compared to low-ID sequences, high-ID sequences are less likely
to be memorized, particularly in overparameterized models and under sparse
exposure. These findings highlight the interaction between scale, exposure, and
complexity in shaping memorization.

</details>


### [39] [Benchmarking Debiasing Methods for LLM-based Parameter Estimates](https://arxiv.org/abs/2506.09627)
*Nicolas Audinet de Pieuchon,Adel Daoud,Connor T. Jerzak,Moa Johansson,Richard Johansson*

Key words: 大语言模型, 文本标注, 去偏方法, DSL, PPI, 有限样本

TL;DR: 大语言模型（LLM）提供了一种廉价但强大的文本标注方式，但与专家标注相比常有不一致性。这些错误会影响下游的人口参数估计（如回归系数和因果效应）。为减少偏差，研究者开发了去偏方法（如基于设计的监督学习DSL和预测驱动推断PPI），通过结合LLM标注和有限数量的专家标注实现有效估计。本文研究这些方法在有限样本中的表现，发现DSL在偏差减少和效率上通常优于PPI，但其表现不稳定。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨LLM标注的偏差对下游估计的影响，并比较两种去偏方法（DSL和PPI）在有限样本中的性能差异。

Method: 通过实验研究DSL和PPI在不同任务中的表现，分析它们在专家标注数量变化时的性能缩放情况。

Result: DSL在偏差减少和效率上通常优于PPI，但其表现不稳定；PPI在大数据集上表现接近DSL，但小数据集时效果较差。

Conclusion: 去偏方法存在偏差-方差权衡，需开发更多指标来量化其在有限样本中的效率。

Abstract: Large language models (LLMs) offer an inexpensive yet powerful way to
annotate text, but are often inconsistent when compared with experts. These
errors can bias downstream estimates of population parameters such as
regression coefficients and causal effects. To mitigate this bias, researchers
have developed debiasing methods such as Design-based Supervised Learning (DSL)
and Prediction-Powered Inference (PPI), which promise valid estimation by
combining LLM annotations with a limited number of expensive expert
annotations. Although these methods produce consistent estimates under
theoretical assumptions, it is unknown how they compare in finite samples of
sizes encountered in applied research. We make two contributions: First, we
study how each method's performance scales with the number of expert
annotations, highlighting regimes where LLM bias or limited expert labels
significantly affect results. Second, we compare DSL and PPI across a range of
tasks, finding that although both achieve low bias with large datasets, DSL
often outperforms PPI on bias reduction and empirical efficiency, but its
performance is less consistent across datasets. Our findings indicate that
there is a bias-variance tradeoff at the level of debiasing methods, calling
for more research on developing metrics for quantifying their efficiency in
finite samples.

</details>


### [40] [Modeling Probabilistic Reduction using Information Theory and Naive Discriminative Learning](https://arxiv.org/abs/2506.09641)
*Anna Stein,Kevin Tang*

Key words: 信息论,NDL,N-gram,声学词持续时间,概率预测器

TL;DR: 比较基于信息论的概率预测器与NDL预测器在声学词持续时间建模中的表现，发现N-gram模型优于NDL模型，但信息论公式能提升NDL性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨NDL预测器与信息论概率预测器在声学词持续时间建模中的表现差异，验证NDL是否因认知动机更有效。

Method: 使用Buckeye语料库，比较三种模型：基于信息论公式的NDL模型、传统NDL模型和N-gram概率模型。

Result: N-gram模型优于两种NDL模型，但信息论公式能提升NDL性能。结论是需结合频率、上下文预测性和信息论指标。

Conclusion: 研究强调结合频率、上下文预测性和信息论指标对声学缩减建模的重要性。

Abstract: This study compares probabilistic predictors based on information theory with
Naive Discriminative Learning (NDL) predictors in modeling acoustic word
duration, focusing on probabilistic reduction. We examine three models using
the Buckeye corpus: one with NDL-derived predictors using information-theoretic
formulas, one with traditional NDL predictors, and one with N-gram
probabilistic predictors. Results show that the N-gram model outperforms both
NDL models, challenging the assumption that NDL is more effective due to its
cognitive motivation. However, incorporating information-theoretic formulas
into NDL improves model performance over the traditional model. This research
highlights a) the need to incorporate not only frequency and contextual
predictability but also average contextual predictability, and b) the
importance of combining information-theoretic metrics of predictability and
information derived from discriminative learning in modeling acoustic
reduction.

</details>


### [41] [Using Sign Language Production as Data Augmentation to enhance Sign Language Translation](https://arxiv.org/abs/2506.09643)
*Harry Walsh,Maksym Ivashechkin,Richard Bowden*

Key words: 手语生成,手语翻译,数据增强,低资源语言,机器学习

TL;DR: 利用手语生成技术增强手语翻译模型性能，提升数据集多样性，性能提升高达19%。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 手语数据稀缺且收集困难，限制了手语翻译模型的发展。通过生成手语视频增强数据集有助于解决这一问题。

Method: 采用骨架生成、手语拼接和两种逼真生成模型（SignGAN和SignSplat）生成多样化手语数据。

Result: 实验表明，方法能有效增强数据集，提升翻译模型性能达19%。

Conclusion: 所提技术为资源受限环境下开发更鲁棒的手语翻译系统铺平道路。

Abstract: Machine learning models fundamentally rely on large quantities of
high-quality data. Collecting the necessary data for these models can be
challenging due to cost, scarcity, and privacy restrictions. Signed languages
are visual languages used by the deaf community and are considered low-resource
languages. Sign language datasets are often orders of magnitude smaller than
their spoken language counterparts. Sign Language Production is the task of
generating sign language videos from spoken language sentences, while Sign
Language Translation is the reverse translation task. Here, we propose
leveraging recent advancements in Sign Language Production to augment existing
sign language datasets and enhance the performance of Sign Language Translation
models. For this, we utilize three techniques: a skeleton-based approach to
production, sign stitching, and two photo-realistic generative models, SignGAN
and SignSplat. We evaluate the effectiveness of these techniques in enhancing
the performance of Sign Language Translation models by generating variation in
the signer's appearance and the motion of the skeletal data. Our results
demonstrate that the proposed methods can effectively augment existing datasets
and enhance the performance of Sign Language Translation models by up to 19%,
paving the way for more robust and accurate Sign Language Translation systems,
even in resource-constrained environments.

</details>


### [42] [Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering](https://arxiv.org/abs/2506.09645)
*Tianjun Yao,Haoxuan Li,Zhiqiang Shen,Pan Li,Tongliang Liu,Kun Zhang*

Key words: 知识图谱，检索增强生成，LLM，RAPL

TL;DR: RAPL是一种新的知识图谱增强框架，通过两阶段标记、图转换和路径推理策略，显著提升了知识图谱问答的检索效果和泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决现有知识图谱检索方法在泛化能力和结构化推理上的不足。

Method: 采用两阶段标记策略、模型无关的图转换方法和路径推理策略。

Result: RAPL在性能上超越现有方法2.66%-20.34%，并减少了不同规模LLM之间的性能差距。

Conclusion: RAPL框架在知识图谱问答中展现出优越的检索能力和泛化性。

Abstract: Large Language Models (LLMs) have shown strong inductive reasoning ability
across various domains, but their reliability is hindered by the outdated
knowledge and hallucinations. Retrieval-Augmented Generation mitigates these
issues by grounding LLMs with external knowledge; however, most existing RAG
pipelines rely on unstructured text, limiting interpretability and structured
reasoning. Knowledge graphs, which represent facts as relational triples, offer
a more structured and compact alternative. Recent studies have explored
integrating knowledge graphs with LLMs for knowledge graph question answering
(KGQA), with a significant proportion adopting the retrieve-then-reasoning
paradigm. In this framework, graph-based retrievers have demonstrated strong
empirical performance, yet they still face challenges in generalization
ability. In this work, we propose RAPL, a novel framework for efficient and
effective graph retrieval in KGQA. RAPL addresses these limitations through
three aspects: (1) a two-stage labeling strategy that combines heuristic
signals with parametric models to provide causally grounded supervision; (2) a
model-agnostic graph transformation approach to capture both intra- and
inter-triple interactions, thereby enhancing representational capacity; and (3)
a path-based reasoning strategy that facilitates learning from the injected
rational knowledge, and supports downstream reasoner through structured inputs.
Empirically, RAPL outperforms state-of-the-art methods by $2.66\%-20.34\%$, and
significantly reduces the performance gap between smaller and more powerful
LLM-based reasoners, as well as the gap under cross-dataset settings,
highlighting its superior retrieval capability and generalizability. Codes are
available at: https://github.com/tianyao-aka/RAPL.

</details>


### [43] [Bridging the Gap Between Open-Source and Proprietary LLMs in Table QA](https://arxiv.org/abs/2506.09657)
*Nikolas Evkarpidi,Elena Tutubalina*

Key words: SemEval, QA, 表格数据, 文本到SQL, RAG, LLM

TL;DR: 本文介绍了一个为SemEval 2025任务8开发的表格问答系统，结合文本到SQL/代码生成、自校正、RAG和端到端模块，使用LLM协调，取得80%准确率，排名前13。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 提升开源模型在表格问答任务中的性能，接近专有LLM的表现。

Method: 结合文本到SQL/代码生成、自校正机制、RAG和端到端模块，通过LLM协调。

Result: 80%准确率，排名前13/38。表现优于开源模型，接近专有LLM。

Conclusion: 系统显著提升开源模型性能，展示了LLM在表格问答中的应用潜力。

Abstract: This paper presents a system developed for SemEval 2025 Task 8: Question
Answering (QA) over tabular data. Our approach integrates several key
components: text-to-SQL and text-to-code generation modules, a self-correction
mechanism, and a retrieval-augmented generation (RAG). Additionally, it
includes an end-to-end (E2E) module, all orchestrated by a large language model
(LLM). Through ablation studies, we analyzed the effects of different parts of
our pipeline and identified the challenges that are still present in this
field. During the evaluation phase of the competition, our solution achieved an
accuracy of 80%, resulting in a top-13 ranking among the 38 participating
teams. Our pipeline demonstrates a significant improvement in accuracy for
open-source models and achieves a performance comparable to proprietary LLMs in
QA tasks over tables. The code is available at GitHub repository.

</details>


### [44] [Query-Level Uncertainty in Large Language Models](https://arxiv.org/abs/2506.09669)
*Lihu Chen,Gaël Varoquaux*

Key words: 大型语言模型,知识边界,查询级别不确定性,内部置信度,自适应推理

TL;DR: 提出了一种名为“内部置信度”的无训练方法，通过查询级别的不确定性检测大型语言模型的知识边界，以优化推理过程并提升AI效率与可信度。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型需要能够识别其知识的边界，以便进行自适应推理，例如调用RAG、深度思考或采用弃权机制，这对发展高效可信的AI至关重要。

Method: 提出了一种无训练的“内部置信度”方法，通过层间和令牌的自评估来确定模型是否能够不生成任何令牌就解决查询。

Result: 在事实性QA和数学推理任务上，内部置信度表现优于多个基线方法，并可用于高效RAG和模型级联，降低推理成本并保持性能。

Conclusion: 内部置信度是一种有效的方法，能够帮助模型识别知识边界，并在实际应用中优化推理效率和成本。

Abstract: It is important for Large Language Models to be aware of the boundary of
their knowledge, the mechanism of identifying known and unknown queries. This
type of awareness can help models perform adaptive inference, such as invoking
RAG, engaging in slow and deep thinking, or adopting the abstention mechanism,
which is beneficial to the development of efficient and trustworthy AI. In this
work, we propose a method to detect knowledge boundaries via Query-Level
Uncertainty, which aims to determine if the model is able to address a given
query without generating any tokens. To this end, we introduce a novel and
training-free method called \emph{Internal Confidence}, which leverages
self-evaluations across layers and tokens. Empirical results on both factual QA
and mathematical reasoning tasks demonstrate that our internal confidence can
outperform several baselines. Furthermore, we showcase that our proposed method
can be used for efficient RAG and model cascading, which is able to reduce
inference costs while maintaining performance.

</details>


### [45] [Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data](https://arxiv.org/abs/2506.09672)
*Hao Xiong,Chuanyuan Tan,Wenliang Chen*

Key words: 非结构化知识编辑（UKE）、大型语言模型（LLMs）、微调（FT）、局部性评估、优化方法

TL;DR: 该论文探讨了非结构化知识编辑（UKE）的重要性，并提出了解决现有问题的方法，包括构建数据集评估局部性以及优化基于微调的方法。实验表明，优化后的方法（FT-UKE）性能优于现有技术。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型（LLMs）的非结构化知识编辑（UKE）缺乏局部性评估，且基于微调（FT）的方法存在异常失败问题。论文旨在解决这些问题。

Method: 构建了UnKEBench-Loc和AKEW-Loc数据集，扩展了现有UKE数据集的局部性测试数据，并分析了影响FT方法的四个因素，提出了优化FT-UKE的训练方案。

Result: 优化后的FT-UKE方法性能显著优于现有方法，尤其在批量编辑场景中，其优势随批量增加而扩大。

Conclusion: FT-UKE为非结构化知识编辑提供了有效的解决方案，并在实验中验证了其优越性。

Abstract: Unstructured Knowledge Editing (UKE) is crucial for updating the relevant
knowledge of large language models (LLMs). It focuses on unstructured inputs,
such as long or free-form texts, which are common forms of real-world
knowledge. Although previous studies have proposed effective methods and tested
them, some issues exist: (1) Lack of Locality evaluation for UKE, and (2)
Abnormal failure of fine-tuning (FT) based methods for UKE. To address these
issues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by
extending two existing UKE datasets with locality test data from the
unstructured and structured views. This enables a systematic evaluation of the
Locality of post-edited models. Furthermore, we identify four factors that may
affect the performance of FT-based methods. Based on these factors, we conduct
experiments to determine how the well-performing FT-based methods should be
trained for the UKE task, providing a training recipe for future research. Our
experimental results indicate that the FT-based method with the optimal setting
(FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art
(SOTA). In batch editing scenarios, FT-UKE shows strong performance as well,
with its advantage over SOTA methods increasing as the batch size grows,
expanding the average metric lead from +6.78% to +10.80%

</details>


### [46] [Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models](https://arxiv.org/abs/2506.09684)
*Haoyi Song,Ruihan Ji,Naichen Shi,Fan Lai,Raed Al Kontar*

Key words: large language models, uncertainty quantification, probabilistic framework, Inv-Entropy, GAAP

TL;DR: 该论文提出了一种基于概率框架的LLMs不确定性量化方法，通过引入逆模型和Inv-Entropy度量，并结合遗传算法扰动策略和新评价指标TSU，显著提升了现有语义不确定性量化方法的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型的不确定性量化（UQ）对可靠部署至关重要，但现有方法缺乏理论依据且多为启发式。

Method: 提出双随机游走视角，通过逆模型构建概率框架，定义Inv-Entropy不确定性度量，并开发基于遗传算法的扰动算法GAAP。

Result: 实验表明Inv-Entropy在语义不确定性量化中优于现有方法。

Conclusion: 该框架灵活性强，支持多种定义和策略，为LLMs的不确定性量化提供了新思路。

Abstract: Large language models (LLMs) have transformed natural language processing,
but their reliable deployment requires effective uncertainty quantification
(UQ). Existing UQ methods are often heuristic and lack a probabilistic
foundation. This paper begins by providing a theoretical justification for the
role of perturbations in UQ for LLMs. We then introduce a dual random walk
perspective, modeling input-output pairs as two Markov chains with transition
probabilities defined by semantic similarity. Building on this, we propose a
fully probabilistic framework based on an inverse model, which quantifies
uncertainty by evaluating the diversity of the input space conditioned on a
given output through systematic perturbations. Within this framework, we define
a new uncertainty measure, Inv-Entropy. A key strength of our framework is its
flexibility: it supports various definitions of uncertainty measures,
embeddings, perturbation strategies, and similarity metrics. We also propose
GAAP, a perturbation algorithm based on genetic algorithms, which enhances the
diversity of sampled inputs. In addition, we introduce a new evaluation metric,
Temperature Sensitivity of Uncertainty (TSU), which directly assesses
uncertainty without relying on correctness as a proxy. Extensive experiments
demonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code
to reproduce the results can be found at
https://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs.

</details>


### [47] [ComfyUI-R1: Exploring Reasoning Models for Workflow Generation](https://arxiv.org/abs/2506.09790)
*Zhenran Xu,Yiyu Wang,Xue Yang,Longyue Wang,Weihua Luo,Kaifu Zhang,Baotian Hu,Min Zhang*

Key words: AI生成内容、工作流自动化、长链思维推理、强化学习、ComfyUI

TL;DR: 论文介绍了ComfyUI-R1，首个用于自动生成工作流的大型推理模型，通过两阶段训练框架显著提升了工作流生成的格式有效性和结构完整性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 针对AI生成内容中需要专家知识定制工作流的高门槛问题，旨在降低用户的学习曲线。

Method: 构建4K工作流数据集，生成长链思维推理数据；通过两阶段训练（CoT微调和强化学习）优化模型。

Result: 7B参数模型达到97%格式有效性，高通过率及F1分数，显著优于GPT-4o和Claude等闭源模型。

Conclusion: 长链思维推理和代码化工作流对AI艺术创作至关重要，证明了ComfyUI-R1在复杂节点合成中的潜力。

Abstract: AI-generated content has evolved from monolithic models to modular workflows,
particularly on platforms like ComfyUI, enabling customization in creative
pipelines. However, crafting effective workflows requires great expertise to
orchestrate numerous specialized components, presenting a steep learning curve
for users. To address this challenge, we introduce ComfyUI-R1, the first large
reasoning model for automated workflow generation. Starting with our curated
dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning
data, including node selection, workflow planning, and code-level workflow
representation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT
fine-tuning for cold start, adapting models to the ComfyUI domain; (2)
reinforcement learning for incentivizing reasoning capability, guided by a
fine-grained rule-metric hybrid reward, ensuring format validity, structural
integrity, and node-level fidelity. Experiments show that our 7B-parameter
model achieves a 97\% format validity rate, along with high pass rate,
node-level and graph-level F1 scores, significantly surpassing prior
state-of-the-art methods that employ leading closed-source models such as
GPT-4o and Claude series. Further analysis highlights the critical role of the
reasoning process and the advantage of transforming workflows into code.
Qualitative comparison reveals our strength in synthesizing intricate workflows
with diverse nodes, underscoring the potential of long CoT reasoning in AI art
creation.

</details>


### [48] [Do LLMs Give Psychometrically Plausible Responses in Educational Assessments?](https://arxiv.org/abs/2506.09796)
*Andreas Säuberli,Diego Frassinelli,Barbara Plank*

Key words: 大型语言模型, 教育评估, 心理测量学, 测试开发

TL;DR: 论文探讨了利用大型语言模型（LLMs）模拟人类测试参与者的可能性，以加速教育测试开发，并通过心理测量学理论评估了18个LLMs的反应类人性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 减少教育测试开发中的人力和时间成本，通过LLMs模拟人类反应来评估测试项目质量。

Method: 基于经典测试理论和项目反应理论，评估18个LLMs在阅读理解、美国历史和经济学三个学科上的多选测试反应类人性和心理测量合理性。

Result: 较大模型反应类人性可通过温度校准提高；阅读理解领域的LLMs与人类相关性较高，但整体相关性不强。

Conclusion: 零样本设置下LLMs不适合直接用作教育测试的试点参与者。

Abstract: Knowing how test takers answer items in educational assessments is essential
for test development, to evaluate item quality, and to improve test validity.
However, this process usually requires extensive pilot studies with human
participants. If large language models (LLMs) exhibit human-like response
behavior to test items, this could open up the possibility of using them as
pilot participants to accelerate test development. In this paper, we evaluate
the human-likeness or psychometric plausibility of responses from 18
instruction-tuned LLMs with two publicly available datasets of multiple-choice
test items across three subjects: reading, U.S. history, and economics. Our
methodology builds on two theoretical frameworks from psychometrics which are
commonly used in educational assessment, classical test theory and item
response theory. The results show that while larger models are excessively
confident, their response distributions can be more human-like when calibrated
with temperature scaling. In addition, we find that LLMs tend to correlate
better with humans in reading comprehension items compared to other subjects.
However, the correlations are not very strong overall, indicating that LLMs
should not be used for piloting educational assessments in a zero-shot setting.

</details>


### [49] [CoRT: Code-integrated Reasoning within Thinking](https://arxiv.org/abs/2506.09820)
*Chengpeng Li,Zhengyang Tang,Ziniu Li,Mingfeng Xue,Keqin Bao,Tian Ding,Ruoyu Sun,Benyou Wang,Xiang Wang,Junyang Lin,Dayiheng Liu*

Key words: 大型推理模型, 代码解释器, Hint-Engineering, 数学推理, CoRT

TL;DR: 论文提出CoRT框架，通过Hint-Engineering优化大型推理模型（LRMs）与代码解释器（CI）的交互，显著提升数学推理能力并减少token使用。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的大型推理模型在处理复杂数学运算时效率或准确性不足，直接结合外部计算工具效果不佳，因此需要优化模型与工具的交互方式。

Method: 通过Hint-Engineering合成代码集成的推理数据，训练模型范围从1.5B到32B参数，采用监督微调、拒绝微调和强化学习等方法。

Result: 实验结果显示，Hint-Engineering模型在多个数学推理数据集上表现提升（4%-8%），同时减少token使用（30%-50%）。

Conclusion: CoRT框架能有效提升模型与代码解释器的协作效率，为复杂推理任务提供新思路。

Abstract: Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable
progress in natural language reasoning with long chain-of-thought (CoT), yet
they remain inefficient or inaccurate when handling complex mathematical
operations. Addressing these limitations through computational tools (e.g.,
computation libraries and symbolic solvers) is promising, but it introduces a
technical challenge: Code Interpreter (CI) brings external knowledge beyond the
model's internal text representations, thus the direct combination is not
efficient. This paper introduces CoRT, a post-training framework for teaching
LRMs to leverage CI effectively and efficiently. As a first step, we address
the data scarcity issue by synthesizing code-integrated reasoning data through
Hint-Engineering, which strategically inserts different hints at appropriate
positions to optimize LRM-CI interaction. We manually create 30 high-quality
samples, upon which we post-train models ranging from 1.5B to 32B parameters,
with supervised fine-tuning, rejection fine-tuning and reinforcement learning.
Our experimental results demonstrate that Hint-Engineering models achieve 4\%
and 8\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and
DeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging
mathematical reasoning datasets. Furthermore, Hint-Engineering models use about
30\% fewer tokens for the 32B model and 50\% fewer tokens for the 1.5B model
compared with the natural language models. The models and code are available at
https://github.com/ChengpengLi1003/CoRT.

</details>


### [50] [EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection](https://arxiv.org/abs/2506.09827)
*Christoph Schuhmann,Robert Kaczmarczyk,Gollam Rabby,Felix Friedrich,Maurice Kraus,Kourosh Nadi,Huu Nguyen,Kristian Kersting,Sören Auer*

Key words: 语音情感识别, 数据集, 情感谱, 合成音频, 专家标注

TL;DR: EmoNet-Voice 是一个新的语音情感检测资源，包含大规模预训练数据集和专家标注的基准数据集，用于评估 AI 系统在细粒度情感谱上的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前语音情感识别（SER）数据集在情感粒度、隐私问题或依赖表演方面存在局限，因此需要新的资源来更准确地评估 AI 的情感理解能力。

Method: 通过合成音频片段模拟特定情感场景，并邀请心理学专家标注感知强度，构建了包含 40 种情感类别和强度的数据集。

Result: 提出的 Empathic Insight Voice 模型在语音情感识别上表现优异，与人类专家高度一致，且高唤醒情感（如愤怒）比低唤醒情感（如专注）更容易检测。

Conclusion: EmoNet-Voice 为语音情感识别提供了隐私保护和细粒度情感评估的新基准，推动了 AI 情感理解的发展。

Abstract: The advancement of text-to-speech and audio generation models necessitates
robust benchmarks for evaluating the emotional understanding capabilities of AI
systems. Current speech emotion recognition (SER) datasets often exhibit
limitations in emotional granularity, privacy concerns, or reliance on acted
portrayals. This paper introduces EmoNet-Voice, a new resource for speech
emotion detection, which includes EmoNet-Voice Big, a large-scale pre-training
dataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,
and 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human
expert annotations. EmoNet-Voice is designed to evaluate SER models on a
fine-grained spectrum of 40 emotion categories with different levels of
intensities. Leveraging state-of-the-art voice generation, we curated synthetic
audio snippets simulating actors portraying scenes designed to evoke specific
emotions. Crucially, we conducted rigorous validation by psychology experts who
assigned perceived intensity labels. This synthetic, privacy-preserving
approach allows for the inclusion of sensitive emotional states often absent in
existing datasets. Lastly, we introduce Empathic Insight Voice models that set
a new standard in speech emotion recognition with high agreement with human
experts. Our evaluations across the current model landscape exhibit valuable
findings, such as high-arousal emotions like anger being much easier to detect
than low-arousal states like concentration.

</details>


### [51] [Error-Guided Pose Augmentation: Enhancing Rehabilitation Exercise Assessment through Targeted Data Generation](https://arxiv.org/abs/2506.09833)
*Omar Sherif,Ali Hamdi*

Key words: 康复评估,数据增强,图卷积网络,EGPA

TL;DR: EGPA方法通过模拟临床相关动作错误生成合成骨骼数据，结合注意力图卷积网络，显著提升康复评估性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有系统在家庭康复中面临数据不平衡和难以检测细微动作错误的挑战。

Method: 提出Error-Guided Pose Augmentation (EGPA)，模拟临床动作错误生成合成数据，并与注意力图卷积网络结合。

Result: 实验显示，绝对误差降低27.6%，错误分类准确率提升45.8%。

Conclusion: EGPA为临床和家庭康复中的自动化动作质量评估提供了有效方法。

Abstract: Effective rehabilitation assessment is essential for monitoring patient
progress, particularly in home-based settings. Existing systems often face
challenges such as data imbalance and difficulty detecting subtle movement
errors. This paper introduces Error-Guided Pose Augmentation (EGPA), a method
that generates synthetic skeleton data by simulating clinically relevant
movement mistakes. Unlike standard augmentation techniques, EGPA targets
biomechanical errors observed in rehabilitation. Combined with an
attention-based graph convolutional network, EGPA improves performance across
multiple evaluation metrics. Experiments demonstrate reductions in mean
absolute error of up to 27.6 percent and gains in error classification accuracy
of 45.8 percent. Attention visualizations show that the model learns to focus
on clinically significant joints and movement phases, enhancing both accuracy
and interpretability. EGPA offers a promising approach for improving automated
movement quality assessment in both clinical and home-based rehabilitation
contexts.

</details>


### [52] [Dataset of News Articles with Provenance Metadata for Media Relevance Assessment](https://arxiv.org/abs/2506.09847)
*Tomas Peterka,Matyas Bohacek*

Key words: 媒体操控、新闻图片、来源验证、大语言模型

TL;DR: 论文提出了一种新数据集和任务，用于检测新闻图片的上下文滥用问题，并测试了六种大语言模型的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前的信息操控手段主要是图片的滥用，而现有方法往往只关注图片语义与文本是否匹配，忽略了更深层次的上下文验证。

Method: 引入了新闻媒体来源数据集（News Media Provenance Dataset），并定义了两种任务（LOR和DTOR），测试了六种大语言模型的基线性能。

Result: 零样本在LOR任务表现良好，但DTOR任务表现不佳，表明需要更专业的架构优化。

Conclusion: 研究发现需要进一步优化模型以解决DTOR任务的挑战。

Abstract: Out-of-context and misattributed imagery is the leading form of media
manipulation in today's misinformation and disinformation landscape. The
existing methods attempting to detect this practice often only consider whether
the semantics of the imagery corresponds to the text narrative, missing
manipulation so long as the depicted objects or scenes somewhat correspond to
the narrative at hand. To tackle this, we introduce News Media Provenance
Dataset, a dataset of news articles with provenance-tagged images. We formulate
two tasks on this dataset, location of origin relevance (LOR) and date and time
of origin relevance (DTOR), and present baseline results on six large language
models (LLMs). We identify that, while the zero-shot performance on LOR is
promising, the performance on DTOR hinders, leaving room for specialized
architectures and future work.

</details>


### [53] [Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.09853)
*Xiangning Yu,Zhuohan Wang,Linyi Yang,Haoxuan Li,Anjie Liu,Xiao Xue,Jun Wang,Mengyue Yang*

Key words: Chain-of-Thought, 充分性, 必要性, 因果框架, 推理效率

TL;DR: 本文提出了一种因果框架，通过充分性和必要性的双重角度改进CoT提示，以提高LLM的推理效率和成本效益。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Chain-of-Thought (CoT)提示在赋予大型语言模型复杂推理能力中起关键作用，但其面临充分性和必要性两大挑战。

Method: 采用因果框架，结合充分性和必要性的概率分析，自动添加缺失步骤并剪枝冗余步骤。

Result: 在多个数学和常识推理基准测试中，显著提高了推理效率并减少token使用，且不影响准确性。

Conclusion: 本研究为提升LLM推理性能和成本效益提供了有前景的方向。

Abstract: Chain-of-Thought (CoT) prompting plays an indispensable role in endowing
large language models (LLMs) with complex reasoning capabilities. However, CoT
currently faces two fundamental challenges: (1) Sufficiency, which ensures that
the generated intermediate inference steps comprehensively cover and
substantiate the final conclusion; and (2) Necessity, which identifies the
inference steps that are truly indispensable for the soundness of the resulting
answer. We propose a causal framework that characterizes CoT reasoning through
the dual lenses of sufficiency and necessity. Incorporating causal Probability
of Sufficiency and Necessity allows us not only to determine which steps are
logically sufficient or necessary to the prediction outcome, but also to
quantify their actual influence on the final reasoning outcome under different
intervention scenarios, thereby enabling the automated addition of missing
steps and the pruning of redundant ones. Extensive experimental results on
various mathematical and commonsense reasoning benchmarks confirm substantial
improvements in reasoning efficiency and reduced token usage without
sacrificing accuracy. Our work provides a promising direction for improving LLM
reasoning performance and cost-effectiveness.

</details>


### [54] [Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs](https://arxiv.org/abs/2506.09886)
*Rodion Oblovatny,Alexandra Bazarova,Alexey Zaytsev*

Key words: 大语言模型，幻觉检测，隐状态分布，深度可学习核函数

TL;DR: 提出一种通过分析提示与响应隐状态分布的概率差异来检测大语言模型幻觉的新方法，发现幻觉响应与提示的偏差较小，并提出了一种无需外部知识的内源性检测方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究大语言模型中幻觉的成因，并提出一种高效、可扩展的检测方法。

Method: 通过计算提示与响应隐状态分布的距离作为幻觉分数，并引入深度可学习核函数增强检测灵敏度。

Result: 在多个基准测试中表现优于现有方法，且无需核训练时仍具竞争力。

Conclusion: 该方法为幻觉检测提供了高效、可扩展的解决方案。

Abstract: We present a novel approach for detecting hallucinations in large language
models (LLMs) by analyzing the probabilistic divergence between prompt and
response hidden-state distributions. Counterintuitively, we find that
hallucinated responses exhibit smaller deviations from their prompts compared
to grounded responses, suggesting that hallucinations often arise from
superficial rephrasing rather than substantive reasoning. Leveraging this
insight, we propose a model-intrinsic detection method that uses distributional
distances as principled hallucination scores, eliminating the need for external
knowledge or auxiliary models. To enhance sensitivity, we employ deep learnable
kernels that automatically adapt to capture nuanced geometric differences
between distributions. Our approach outperforms existing baselines,
demonstrating state-of-the-art performance on several benchmarks. The method
remains competitive even without kernel training, offering a robust, scalable
solution for hallucination detection.

</details>


### [55] [The Emergence of Abstract Thought in Large Language Models Beyond Any Language](https://arxiv.org/abs/2506.09890)
*Yuxin Chen,Yiran Zhao,Yang Zhang,An Zhang,Kenji Kawaguchi,Shafiq Joty,Junnan Li,Tat-Seng Chua,Michael Qizhe Shieh,Wenxuan Zhang*

Key words: 大语言模型, 多语言性能, 神经元分类, 语言无关参数, 训练策略

TL;DR: 论文研究发现，大语言模型（LLMs）在发展中逐渐形成了一个与语言无关的核心参数空间，这一小部分参数对模型的多语言性能至关重要。研究还识别了与特定语言相关的神经元，并提出针对不同发展阶段的语言无关训练策略。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨LLMs在多语言任务中表现优秀的机制，特别是其内部参数空间如何支持语言无关的抽象思考。

Method: 识别并分类语言相关神经元（共享或独占），提出基于神经元特性的训练策略，并验证其效果。

Result: 实验证明，共享神经元在多语言任务中起关键作用，且其比例和重要性随模型发展而增加。提出的训练策略在不同LLM家族中表现良好。

Conclusion: LLMs通过共享神经元形成语言无关的抽象思考能力，针对性训练策略可进一步提升多语言性能。

Abstract: As large language models (LLMs) continue to advance, their capacity to
function effectively across a diverse range of languages has shown marked
improvement. Preliminary studies observe that the hidden activations of LLMs
often resemble English, even when responding to non-English prompts. This has
led to the widespread assumption that LLMs may "think" in English. However,
more recent results showing strong multilingual performance, even surpassing
English performance on specific tasks in other languages, challenge this view.
In this work, we find that LLMs progressively develop a core language-agnostic
parameter space-a remarkably small subset of parameters whose deactivation
results in significant performance degradation across all languages. This
compact yet critical set of parameters underlies the model's ability to
generalize beyond individual languages, supporting the emergence of abstract
thought that is not tied to any specific linguistic system. Specifically, we
identify language-related neurons-those are consistently activated during the
processing of particular languages, and categorize them as either shared
(active across multiple languages) or exclusive (specific to one). As LLMs
undergo continued development over time, we observe a marked increase in both
the proportion and functional importance of shared neurons, while exclusive
neurons progressively diminish in influence. These shared neurons constitute
the backbone of the core language-agnostic parameter space, supporting the
emergence of abstract thought. Motivated by these insights, we propose
neuron-specific training strategies tailored to LLMs' language-agnostic levels
at different development stages. Experiments across diverse LLM families
support our approach.

</details>


### [56] [PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants](https://arxiv.org/abs/2506.09902)
*Zheng Zhao,Clara Vania,Subhradeep Kayal,Naila Khan,Shay B. Cohen,Emine Yilmaz*

Key words: 大语言模型, 个性化评估, 任务导向助手, PersonaLens, AI助手

TL;DR: 论文介绍了PersonaLens，一个用于评估任务导向AI助手个性化能力的综合基准，揭示了当前LLM助手在个性化方面的显著差异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的个性化评估基准无法全面捕捉任务导向助手的复杂性，因此需要一个新的基准来衡量个性化能力。

Method: 引入PersonaLens基准，结合多样化用户配置和两个基于LLM的代理（用户代理和评估代理），通过实验评估LLM助手的个性化表现。

Result: 实验显示不同LLM助手在个性化能力上存在显著差异，为改进会话AI系统提供了关键洞见。

Conclusion: PersonaLens为任务导向AI助手的个性化评估提供了有效工具，揭示了当前技术的局限性。

Abstract: Large language models (LLMs) have advanced conversational AI assistants.
However, systematically evaluating how well these assistants apply
personalization--adapting to individual user preferences while completing
tasks--remains challenging. Existing personalization benchmarks focus on
chit-chat, non-conversational tasks, or narrow domains, failing to capture the
complexities of personalized task-oriented assistance. To address this, we
introduce PersonaLens, a comprehensive benchmark for evaluating personalization
in task-oriented AI assistants. Our benchmark features diverse user profiles
equipped with rich preferences and interaction histories, along with two
specialized LLM-based agents: a user agent that engages in realistic
task-oriented dialogues with AI assistants, and a judge agent that employs the
LLM-as-a-Judge paradigm to assess personalization, response quality, and task
success. Through extensive experiments with current LLM assistants across
diverse tasks, we reveal significant variability in their personalization
capabilities, providing crucial insights for advancing conversational AI
systems.

</details>


### [57] [Aspect-Based Opinion Summarization with Argumentation Schemes](https://arxiv.org/abs/2506.09917)
*Wendi Zhou,Ameer Saadat-Yazd,Nadin Kokciyan*

Key words: opinion summarization, aspect-centric, ASESUM, online reviews, automated summarization

TL;DR: 论文提出了一种新颖的意见摘要系统ASESUM，能够从产品评论中提取基于关键方面的观点，并衡量其显著性和有效性，无需依赖预定义方面集。实验证明其优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 在线购物中，海量评论使消费者难以手动总结主要观点，现有摘要方法在生成基于方面的总结方面存在局限。

Method: 提出ASESUM框架，通过提取基于方面的论点并衡量其显著性和有效性，生成适应不同领域的摘要。

Result: 实验表明ASESUM在捕捉原始评论多样观点方面优于现有方法。

Conclusion: ASESUM是一种高效且适应性强的意见摘要系统，能够为消费者提供有价值的决策支持。

Abstract: Reviews are valuable resources for customers making purchase decisions in
online shopping. However, it is impractical for customers to go over the vast
number of reviews and manually conclude the prominent opinions, which prompts
the need for automated opinion summarization systems. Previous approaches,
either extractive or abstractive, face challenges in automatically producing
grounded aspect-centric summaries. In this paper, we propose a novel
summarization system that not only captures predominant opinions from an aspect
perspective with supporting evidence, but also adapts to varying domains
without relying on a pre-defined set of aspects. Our proposed framework,
ASESUM, summarizes viewpoints relevant to the critical aspects of a product by
extracting aspect-centric arguments and measuring their salience and validity.
We conduct experiments on a real-world dataset to demonstrate the superiority
of our approach in capturing diverse perspectives of the original reviews
compared to new and existing methods.

</details>


### [58] [VerIF: Verification Engineering for Reinforcement Learning in Instruction Following](https://arxiv.org/abs/2506.09942)
*Hao Peng,Yunjia Qi,Xiaozhi Wang,Bin Xu,Lei Hou,Juanzi Li*

Key words: 强化学习,指令跟随,验证工程,VerIF,VerInstruct

TL;DR: 论文提出了一种结合规则代码验证和大型推理模型验证的方法VerIF，以提升强化学习在指令跟随任务中的效果，并在多个基准测试中实现了显著改进。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决强化学习在指令跟随任务中验证挑战的问题，探索如何通过验证工程提升模型性能。

Method: 提出VerIF方法，结合规则代码验证和LLM推理验证，使用高质量数据集VerInstruct（约22,000条实例）进行训练。

Result: 在多个指令跟随基准测试中取得显著改进，模型性能达到同类最佳，且不影响其通用能力。

Conclusion: VerIF方法可集成到现有强化学习流程中，提升模型整体性能，适合未来研究推广。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a key
technique for enhancing large language models (LLMs), with verification
engineering playing a central role. However, best practices for RL in
instruction following remain underexplored. In this work, we explore the
verification challenge in RL for instruction following and propose VerIF, a
verification method that combines rule-based code verification with LLM-based
verification from a large reasoning model (e.g., QwQ-32B). To support this
approach, we construct a high-quality instruction-following dataset,
VerInstruct, containing approximately 22,000 instances with associated
verification signals. We apply RL training with VerIF to two models, achieving
significant improvements across several representative instruction-following
benchmarks. The trained models reach state-of-the-art performance among models
of comparable size and generalize well to unseen constraints. We further
observe that their general capabilities remain unaffected, suggesting that RL
with VerIF can be integrated into existing RL recipes to enhance overall model
performance. We have released our datasets, codes, and models to facilitate
future research at https://github.com/THU-KEG/VerIF.

</details>


### [59] [Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking](https://arxiv.org/abs/2506.09944)
*Wuwei Zhang,Fangcong Yin,Howard Yen,Danqi Chen,Xi Ye*

Key words: QRHEAD, QR-RETRIEVER, 长文本检索, 多跳推理, 零样本检索

TL;DR: 本文提出了一种改进的注意力头QRHEAD，通过聚合输入查询的注意力分数来增强长文本检索，并开发了高效检索器QR-RETRIEVER，在多跳推理任务中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 针对长文本语言模型中检索头（retrieval heads）的局限性，提出改进方法以提升检索效果。

Method: 通过聚合注意力分数识别QRHEAD，并开发QR-RETRIEVER利用其注意力质量进行高效检索。

Result: 在LongMemEval和CLIPPER任务中性能提升超10%，且在BEIR基准测试中零样本表现优异。

Conclusion: QRHEAD和QR-RETRIEVER为通用检索器，同时揭示了长文本语言模型的可解释性。

Abstract: Recent work has identified retrieval heads (Wu et al., 2025b), a subset of
attention heads responsible for retrieving salient information in long-context
language models (LMs), as measured by their copy-paste behavior in
Needle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused
Retrieval Head), an improved set of attention heads that enhance retrieval from
long context. We identify QRHEAD by aggregating attention scores with respect
to the input query, using a handful of examples from real-world tasks (e.g.,
long-context QA). We further introduce QR- RETRIEVER, an efficient and
effective retriever that uses the accumulated attention mass of QRHEAD as
retrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting
the most relevant parts with the highest retrieval scores. On multi-hop
reasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains
over full context and outperforms strong dense retrievers. We also evaluate
QRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves
strong zero-shot performance, outperforming other LLM-based re-rankers such as
RankGPT. Further analysis shows that both the querycontext attention scoring
and task selection are crucial for identifying QRHEAD with strong downstream
utility. Overall, our work contributes a general-purpose retriever and offers
interpretability insights into the long-context capabilities of LMs.

</details>


### [60] [Resa: Transparent Reasoning Models via SAEs](https://arxiv.org/abs/2506.09967)
*Shangshang Wang,Julian Asilis,Ömer Faruk Akgül,Enes Burak Bilgin,Ollie Liu,Deqing Fu,Willie Neiswanger*

Key words: 稀疏自编码器（SAE）、推理能力、成本效益、模块化、通用性

TL;DR: Resa利用稀疏自编码器调优（SAE-Tuning）方法，以低成本高效地提升语言模型的推理能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究如何通过利用语言模型的底层表征，以高成本效益的方式激发其强大的推理能力。

Method: 使用稀疏自编码器（SAE）从源模型中捕获推理能力，并通过监督微调过程将这些能力引导到目标模型中，无需推理轨迹数据。

Result: SAE-Tuning在降低训练成本2000倍（约1美元）和时间450倍（约20分钟）的同时，保持RL训练模型推理性能的97%。在轻量RL训练模型上，能以低成本显著提升性能（如AIME24上43.33% Pass@1）。

Conclusion: SAE提取的推理能力具有通用性和模块化特性，能够跨数据集和模型迁移，且无需重新训练。

Abstract: How cost-effectively can we elicit strong reasoning in language models by
leveraging their underlying representations? We answer this question with Resa,
a family of 1.5B reasoning models trained via a novel and efficient sparse
autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to
capture reasoning abilities from a source model, and then uses the trained SAE
to guide a standard supervised fine-tuning process to elicit such abilities in
a target model, all using verified question-answer data without any reasoning
traces. Notably, when applied to certain base models before further RL
post-training, SAE-Tuning retains >97% of its RL-trained counterpart's
reasoning performance while reducing training costs by >2000x to roughly \$1
and training time by >450x to around 20 minutes. Furthermore, when applied to
lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning
performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only
around \$1 additional cost. Surprisingly, the reasoning abilities extracted via
SAEs are potentially both generalizable and modular. Generality means abilities
extracted from one dataset still elevate performance on a larger and
overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math
can be attached to the R1-Distill model at test time, without any retraining,
and yield comparable gains. Extensive ablations validate these findings and all
artifacts are fully open-sourced.

</details>


### [61] [When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text](https://arxiv.org/abs/2506.09975)
*Hillary Dawkins,Kathleen C. Fraser,Svetlana Kiritchenko*

Key words: AI生成文本检测,社交媒体,微调LLM,检测算法,网络影响活动

TL;DR: 论文研究了社交媒体上AI生成文本的检测问题，发现在实际场景中检测效果显著下降。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 社交媒体是网络影响活动的重要载体，AI生成的批量帖子可能被用来支持或反对特定政策或事件，因此检测这些文本至关重要。

Method: 研究人员创建了一个包含505,159条AI生成社交媒体帖子的数据集，覆盖11个争议话题，并模拟威胁行为者的方式进行分析。

Result: 在典型研究假设下，检测效果良好；但在实际场景中（攻击者不公开微调模型时），检测率显著下降。人类研究进一步验证了这一结果。

Conclusion: 微调LLM对检测算法构成显著挑战，这一结果对所有检测领域均有广泛影响。

Abstract: Detecting AI-generated text is a difficult problem to begin with; detecting
AI-generated text on social media is made even more difficult due to the short
text length and informal, idiosyncratic language of the internet. It is
nonetheless important to tackle this problem, as social media represents a
significant attack vector in online influence campaigns, which may be bolstered
through the use of mass-produced AI-generated posts supporting (or opposing)
particular policies, decisions, or events. We approach this problem with the
mindset and resources of a reasonably sophisticated threat actor, and create a
dataset of 505,159 AI-generated social media posts from a combination of
open-source, closed-source, and fine-tuned LLMs, covering 11 different
controversial topics. We show that while the posts can be detected under
typical research assumptions about knowledge of and access to the generating
models, under the more realistic assumption that an attacker will not release
their fine-tuned model to the public, detectability drops dramatically. This
result is confirmed with a human study. Ablation experiments highlight the
vulnerability of various detection algorithms to fine-tuned LLMs. This result
has implications across all detection domains, since fine-tuning is a generally
applicable and realistic LLM use case.

</details>


### [62] [Step-by-step Instructions and a Simple Tabular Output Format Improve the Dependency Parsing Accuracy of LLMs](https://arxiv.org/abs/2506.09983)
*Hiroshi Matsuda,Chunpeng Ma,Masayuki Asahara*

Key words: 大语言模型, 依赖解析, 逐步指令策略, 词性标注, 多语言微调

TL;DR: 论文提出了一种基于逐步指令策略的依赖解析方法，通过先进行词性标注再预测句法头和依赖标签，结合简化的输出格式，显著提升了多语言依赖解析的准确性和一致性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管大语言模型（LLMs）在多种任务中表现出色，但在依赖解析等结构化输出任务中，标准提示方法往往难以生成准确且结构化的结果。

Method: 采用逐步指令策略，先进行通用词性标注，再预测句法头和依赖标签，并使用简化的CoNLL-U格式输出。

Result: 在17种语言的Universal Dependencies数据集上实现了最先进的准确性，同时通过多语言微调提升了跨语言泛化性能。

Conclusion: 研究表明，显式的推理步骤在大语言模型依赖解析中非常有效，并提出了一种可扩展且格式一致的替代方法。

Abstract: Recent advances in large language models (LLMs) have enabled impressive
performance in various tasks. However, standard prompting often struggles to
produce structurally valid and accurate outputs, especially in dependency
parsing. We propose a novel step-by-step instruction strategy, where universal
part-of-speech tagging precedes the prediction of syntactic heads and
dependency labels, and a simplified CoNLL-U like output format, our method
achieves state-of-the-art accuracy on Universal Dependencies datasets across 17
languages without hallucination or contamination. We further show that
multilingual fine-tuning simultaneously improves cross-language generalization
performance. Our results highlight the effectiveness of explicit reasoning
steps in LLM-based parsing and offer a scalable, format-consistent alternative
to bracket-based approaches.

</details>


### [63] [Large Language Models for Toxic Language Detection in Low-Resource Balkan Languages](https://arxiv.org/abs/2506.09992)
*Amel Muminovic,Amela Kadric Muminovic*

Key words: 毒性语言检测, 塞尔维亚语, 克罗地亚语, 波斯尼亚语, 语言模型, 内容审核

TL;DR: 该研究评估了大型语言模型在塞尔维亚语、克罗地亚语和波斯尼亚语中处理毒性评论的能力，通过构建和标记4500条评论的数据集，测试了四种模型在零样本和上下文增强模式下的表现，发现上下文片段显著提高了召回率和F1分数。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 在线毒性语言在缺乏内容审核工具的地区造成实际伤害，研究旨在探索如何利用大型语言模型在这些低资源语言环境中更有效地检测毒性内容。

Method: 研究者构建了4500条来自YouTube和TikTok的评论数据集，测试了GPT-3.5 Turbo、GPT-4.1、Gemini 1.5 Pro和Claude 3 Opus两种模式（零样本和上下文增强）下的性能，评估了精确率、召回率、F1分数、准确率和误报率。

Result: 上下文增强模式平均提高了0.12的召回率，F1分数提升高达0.10，但可能增加误报。Gemini在上下文增强模式下表现最佳（F1得分0.82），GPT-4.1在零样本模式下误报最低。

Conclusion: 研究表明，通过改进提示设计和阈值校准，可以在低资源语言中显著提升毒性检测效果。

Abstract: Online toxic language causes real harm, especially in regions with limited
moderation tools. In this study, we evaluate how large language models handle
toxic comments in Serbian, Croatian, and Bosnian, languages with limited
labeled data. We built and manually labeled a dataset of 4,500 YouTube and
TikTok comments drawn from videos across diverse categories, including music,
politics, sports, modeling, influencer content, discussions of sexism, and
general topics. Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude
3 Opus) were tested in two modes: zero-shot and context-augmented. We measured
precision, recall, F1 score, accuracy and false positive rates. Including a
short context snippet raised recall by about 0.12 on average and improved F1
score by up to 0.10, though it sometimes increased false positives. The best
balance came from Gemini in context-augmented mode, reaching an F1 score of
0.82 and accuracy of 0.82, while zero-shot GPT-4.1 led on precision and had the
lowest false alarms. We show how adding minimal context can improve toxic
language detection in low-resource settings and suggest practical strategies
such as improved prompt design and threshold calibration. These results show
that prompt design alone can yield meaningful gains in toxicity detection for
underserved Balkan language communities.

</details>


### [64] [From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring](https://arxiv.org/abs/2506.09996)
*Yang Li,Qiang Sheng,Yehan Yang,Xueyao Zhang,Juan Cao*

Key words: 大语言模型，安全对齐，部分检测，流式监控，FineHarm

TL;DR: 论文提出了一种数据与模型结合的解决方案，支持部分检测以降低延迟，并提出了FineHarm数据集和流式内容监控器（SCM），实验表明SCM性能接近完整检测。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的大语言模型安全问题中，传统全检测方法延迟高，而部分检测方法存在训练与推理差距，需改进。

Method: 构建FineHarm数据集，提出流式内容监控器（SCM），结合响应和标记级别的双重监督进行训练。

Result: SCM仅查看18%的响应标记即可达到与全检测相当的0.95+宏F1分数，并能提高模型安全性。

Conclusion: SCM有效平衡了检测效率与性能，可作为安全对齐的伪标注器，提升模型无害性。

Abstract: Though safety alignment has been applied to most large language models
(LLMs), LLM service providers generally deploy a subsequent moderation as the
external safety guardrail in real-world products. Existing moderators mainly
practice a conventional full detection, which determines the harmfulness based
on the complete LLM output, causing high service latency. Recent works pay more
attention to partial detection where moderators oversee the generation midway
and early stop the output if harmfulness is detected, but they directly apply
moderators trained with the full detection paradigm to incomplete outputs,
introducing a training-inference gap that lowers the performance. In this
paper, we explore how to form a data-and-model solution that natively supports
partial detection. For the data, we construct FineHarm, a dataset consisting of
29K prompt-response pairs with fine-grained annotations to provide reasonable
supervision for token-level training. Then, we propose the streaming content
monitor, which is trained with dual supervision of response- and token-level
labels and can follow the output stream of LLM to make a timely judgment of
harmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is
comparable to full detection, by only seeing the first 18% of tokens in
responses on average. Moreover, the SCM can serve as a pseudo-harmfulness
annotator for improving safety alignment and lead to a higher harmlessness
score than DPO.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [65] [Llama-Affinity: A Predictive Antibody Antigen Binding Model Integrating Antibody Sequences with Llama3 Backbone Architecture](https://arxiv.org/abs/2506.09052)
*Delower Hossain,Ehsan Saghapour,Kevin Song,Jake Y. Chen*

Key words: 抗体-抗原亲和力预测，Llama 3，OAS数据库，AI驱动设计，计算效率

TL;DR: 本文提出了一种基于开源Llama 3框架的抗体-抗原结合亲和力预测模型（LlamaAffinity），利用OAS数据库中的抗体序列数据，显著优于现有技术，并具有更高的计算效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统抗体亲和力测量方法耗时且昂贵，AI技术的发展为基于抗体的药物设计和亲和力预测提供了新思路。

Method: 采用开源Llama 3作为模型框架，结合OAS数据库中的抗体序列数据，开发了LlamaAffinity模型。

Result: 模型在多个评估指标上优于现有技术，准确率为0.9640，F1分数为0.9643，AUC-ROC为0.9936，且训练时间显著缩短。

Conclusion: LlamaAffinity模型在抗体-抗原亲和力预测中表现优异，计算效率高，为AI驱动的抗体设计提供了新的可能性。

Abstract: Antibody-facilitated immune responses are central to the body's defense
against pathogens, viruses, and other foreign invaders. The ability of
antibodies to specifically bind and neutralize antigens is vital for
maintaining immunity. Over the past few decades, bioengineering advancements
have significantly accelerated therapeutic antibody development. These
antibody-derived drugs have shown remarkable efficacy, particularly in treating
cancer, SARS-CoV-2, autoimmune disorders, and infectious diseases.
Traditionally, experimental methods for affinity measurement have been
time-consuming and expensive. With the advent of artificial intelligence, in
silico medicine has been revolutionized; recent developments in machine
learning, particularly the use of large language models (LLMs) for representing
antibodies, have opened up new avenues for AI-based design and improved
affinity prediction. Herein, we present an advanced antibody-antigen binding
affinity prediction model (LlamaAffinity), leveraging an open-source Llama 3
backbone and antibody sequence data sourced from the Observed Antibody Space
(OAS) database. The proposed approach shows significant improvement over
existing state-of-the-art (SOTA) methods (AntiFormer, AntiBERTa, AntiBERTy)
across multiple evaluation metrics. Specifically, the model achieved an
accuracy of 0.9640, an F1-score of 0.9643, a precision of 0.9702, a recall of
0.9586, and an AUC-ROC of 0.9936. Moreover, this strategy unveiled higher
computational efficiency, with a five-fold average cumulative training time of
only 0.46 hours, significantly lower than in previous studies.

</details>


### [66] [FinHEAR: Human Expertise and Adaptive Risk-Aware Temporal Reasoning for Financial Decision-Making](https://arxiv.org/abs/2506.09080)
*Jiaxiang Chen,Mingxi Zou,Zhuo Wang,Qifan Wang,Dongning Sun,Chi Zhang,Zenglin Xu*

Key words: 金融决策, 语言模型, 多代理系统, 行为经济学, 风险调整

TL;DR: FinHEAR是一个基于多代理的语言模型框架，专注于金融决策中的时间推理、风险适应和行为模式捕捉，显著提升了预测精度和风险调整回报。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有大语言模型在金融决策中难以捕捉人类行为的核心模式（如信息不对称下的专家依赖、损失厌恶和时间调整），需要更适应性的解决方案。

Method: 提出FinHEAR框架，通过多代理LLM协作分析历史趋势、动态事件和专家先例，结合行为经济学设计专家检索、信心调整头寸和结果优化机制。

Result: 在金融数据集上的实验表明，FinHEAR在趋势预测和交易任务中均优于基线，表现更高的准确性和风险调整回报。

Conclusion: FinHEAR通过结合人类行为特性和适应性推理，为复杂金融决策提供了更鲁棒和可解释的解决方案。

Abstract: Financial decision-making presents unique challenges for language models,
demanding temporal reasoning, adaptive risk assessment, and responsiveness to
dynamic events. While large language models (LLMs) show strong general
reasoning capabilities, they often fail to capture behavioral patterns central
to human financial decisions-such as expert reliance under information
asymmetry, loss-averse sensitivity, and feedback-driven temporal adjustment. We
propose FinHEAR, a multi-agent framework for Human Expertise and Adaptive
Risk-aware reasoning. FinHEAR orchestrates specialized LLM-based agents to
analyze historical trends, interpret current events, and retrieve
expert-informed precedents within an event-centric pipeline. Grounded in
behavioral economics, it incorporates expert-guided retrieval,
confidence-adjusted position sizing, and outcome-based refinement to enhance
interpretability and robustness. Empirical results on curated financial
datasets show that FinHEAR consistently outperforms strong baselines across
trend prediction and trading tasks, achieving higher accuracy and better
risk-adjusted returns.

</details>


### [67] [Enhanced Whole Page Optimization via Mixed-Grained Reward Mechanism-Adapted Language Models](https://arxiv.org/abs/2506.09084)
*Xinyuan Wang,Liang Wu,Yanjie Fu*

Key words: 整页优化, 大语言模型, 用户反馈, 奖励机制, 推荐系统

TL;DR: 论文提出了一种基于用户反馈的奖励微调方法PageLLM，用于优化预训练大语言模型在整页优化（WPO）任务中的表现，解决了传统方法依赖昂贵人工标注数据的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 优化搜索和推荐结果的展示对用户体验至关重要，但现有方法依赖大量人工标注数据，成本高昂，因此需要一种更高效的解决方案。

Method: 提出PageLLM方法，采用混合粒度奖励机制（页面级和项目级奖励），利用用户反馈作为监督信号进行模型微调。

Result: 在公开和工业数据集上验证，PageLLM优于基线方法，并在千万级用户的在线A/B测试中实现GMV增长0.44%。

Conclusion: PageLLM通过用户反馈驱动的奖励机制，有效平衡了整体页面质量与关键推荐项精度，具有实际应用价值。

Abstract: Optimizing the presentation of search and recommendation results is crucial
to enhancing user experience and engagement. Whole Page Optimization (WPO)
plays a pivotal role in this process, as it directly influences how information
is surfaced to users. While Pre-trained Large Language Models (LLMs) have
demonstrated remarkable capabilities in generating coherent and contextually
relevant content, fine-tuning these models for complex tasks like WPO presents
challenges. Specifically, the need for extensive human-annotated data to
mitigate issues such as hallucinations and model instability can be
prohibitively expensive, especially in large-scale systems that interact with
millions of items daily. In this work, we address the challenge of fine-tuning
LLMs for WPO by using user feedback as the supervision. Unlike manually labeled
datasets, user feedback is inherently noisy and less precise. To overcome this,
we propose a reward-based fine-tuning approach, PageLLM, which employs a
mixed-grained reward mechanism that combines page-level and item-level rewards.
The page-level reward evaluates the overall quality and coherence, while the
item-level reward focuses on the accuracy and relevance of key recommendations.
This dual-reward structure ensures that both the holistic presentation and the
critical individual components are optimized. We validate PageLLM on both
public and industrial datasets. PageLLM outperforms baselines and achieves a
0.44\% GMV increase in an online A/B test with over 10 million users,
demonstrating its real-world impact.

</details>


### [68] [LLM-ML Teaming: Integrated Symbolic Decoding and Gradient Search for Valid and Stable Generative Feature Transformation](https://arxiv.org/abs/2506.09085)
*Xinyuan Wang,Haoyue Bai,Nanxu Gong,Wangyang Ying,Sixun Dong,Xiquan Cui,Yanjie Fu*

Key words: 特征转换, 生成式AI, LLM, ML, 团队框架

TL;DR: 提出一种结合LLMs和ML的团队框架，通过符号生成和梯度优化实现特征转换，提升下游性能并减少错误。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统ML和现成LLMs分别存在低效性和不稳定性问题，无法同时解决特征转换的稳定性和有效性挑战。

Method: 四步骤框架：黄金样本生成、特征转换序列嵌入与搜索、学生LLM特征转换、LLM-ML解码器协同。

Result: 实验显示下游性能提升5%，错误案例减少近半，且团队策略高效稳健。

Conclusion: 该框架成功结合LLMs和ML优势，解决了特征转换的关键问题，同时揭示了LLMs对原始数据的理解能力。

Abstract: Feature transformation enhances data representation by deriving new features
from the original data. Generative AI offers potential for this task, but faces
challenges in stable generation (consistent outputs) and valid generation
(error-free sequences). Existing methods--traditional MLs' low validity and
LLMs' instability--fail to resolve both. We find that LLMs ensure valid syntax,
while ML's gradient-steered search stabilizes performance. To bridge this gap,
we propose a teaming framework combining LLMs' symbolic generation with ML's
gradient optimization. This framework includes four steps: (1) golden examples
generation, aiming to prepare high-quality samples with the ground knowledge of
the teacher LLM; (2) feature transformation sequence embedding and search,
intending to uncover potentially superior embeddings within the latent space;
(3) student LLM feature transformation, aiming to distill knowledge from the
teacher LLM; (4) LLM-ML decoder teaming, dedicating to combine ML and the
student LLM probabilities for valid and stable generation. The experiments on
various datasets show that the teaming policy can achieve 5\% improvement in
downstream performance while reducing nearly half of the error cases. The
results also demonstrate the efficiency and robustness of the teaming policy.
Additionally, we also have exciting findings on LLMs' capacity to understand
the original data.

</details>


### [69] [Spiking Neural Models for Decision-Making Tasks with Learning](https://arxiv.org/abs/2506.09087)
*Sophie Jaffard,Giulia Mezzadri,Patricia Reynaud-Bouret,Etienne Tanré*

Key words: 漂移扩散模型,泊松计数器模型,尖峰神经网络,Hawkes过程,学习机制,决策

TL;DR: 该论文提出了一个基于尖峰神经网络的生物可解释决策模型，通过Hawkes过程模拟神经元活动，并引入学习机制，弥补了传统模型（如漂移扩散模型和泊松计数器模型）的不足。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统认知模型（如漂移扩散模型和泊松计数器模型）缺乏学习机制，且仅适用于参与者具有先验知识的任务。论文旨在弥合认知模型与生物模型之间的差距。

Method: 提出了一种基于尖峰神经网络（SNN）的生物可解释模型，利用多元Hawkes过程模拟神经元活动，并引入了局部学习规则。此外，还设计了一个在线分类任务来验证模型预测。

Result: 研究发现漂移扩散模型和泊松计数器模型的分类结果和反应时间相似，且可以通过尖峰泊松神经元进行近似。进一步证明，相关噪声的漂移扩散模型可以从Hawkes网络中推导出来。

Conclusion: 该研究为将生物相关的神经机制整合到认知模型中提供了重要进展，深化了对神经活动与行为关系的理解。

Abstract: In cognition, response times and choices in decision-making tasks are
commonly modeled using Drift Diffusion Models (DDMs), which describe the
accumulation of evidence for a decision as a stochastic process, specifically a
Brownian motion, with the drift rate reflecting the strength of the evidence.
In the same vein, the Poisson counter model describes the accumulation of
evidence as discrete events whose counts over time are modeled as Poisson
processes, and has a spiking neurons interpretation as these processes are used
to model neuronal activities. However, these models lack a learning mechanism
and are limited to tasks where participants have prior knowledge of the
categories. To bridge the gap between cognitive and biological models, we
propose a biologically plausible Spiking Neural Network (SNN) model for
decision-making that incorporates a learning mechanism and whose neurons
activities are modeled by a multivariate Hawkes process. First, we show a
coupling result between the DDM and the Poisson counter model, establishing
that these two models provide similar categorizations and reaction times and
that the DDM can be approximated by spiking Poisson neurons. To go further, we
show that a particular DDM with correlated noise can be derived from a Hawkes
network of spiking neurons governed by a local learning rule. In addition, we
designed an online categorization task to evaluate the model predictions. This
work provides a significant step toward integrating biologically relevant
neural mechanisms into cognitive models, fostering a deeper understanding of
the relationship between neural activity and behavior.

</details>


### [70] [Integrating Asynchronous AdaBoost into Federated Learning: Five Real World Applications](https://arxiv.org/abs/2506.09090)
*Arthur Oghlukyan,Nuria Gomez Blas*

Key words: 联邦学习, 异步AdaBoost, 通信效率, 自适应调度

TL;DR: 本文提出了一种改进的异步AdaBoost框架，用于联邦学习（FL），并通过自适应通信调度和延迟权重补偿降低了通信开销，同时保持或提高了模型准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 联邦学习在多个领域（如边缘设备计算机视觉、区块链透明度、移动设备个性化等）的应用需要更高的通信效率和模型鲁棒性，传统AdaBoost框架存在同步频率高和通信开销大的问题。

Method: 提出了一种结合自适应通信调度和延迟权重补偿的异步AdaBoost算法，减少同步频率和通信开销。

Result: 实验结果显示训练时间减少20-35%，通信开销减少30-40%，且在更少的提升轮次中收敛。

Conclusion: 改进后的AdaBoost框架在多样化的联邦学习场景中表现出显著更高的效率和鲁棒性。

Abstract: This paper presents a comprehensive analysis of an enhanced asynchronous
AdaBoost framework for federated learning (FL), focusing on its application
across five distinct domains: computer vision on edge devices, blockchain-based
model transparency, on-device mobile personalization, IoT anomaly detection,
and federated healthcare diagnostics. The proposed algorithm incorporates
adaptive communication scheduling and delayed weight compensation to reduce
synchronization frequency and communication overhead while preserving or
improving model accuracy. We examine how these innovations improve
communication efficiency, scalability, convergence, and robustness in each
domain. Comparative metrics including training time, communication overhead,
convergence iterations, and classification accuracy are evaluated using data
and estimates derived from Oghlukyan's enhanced AdaBoost framework. Empirical
results show, for example, training time reductions on the order of 20-35% and
communication overhead reductions of 30-40% compared to baseline AdaBoost, with
convergence achieved in significantly fewer boosting rounds. Tables and charts
summarize these improvements by domain. Mathematical formulations of the
adaptive scheduling rule and error-driven synchronization thresholds are
provided. Overall, the enhanced AdaBoost exhibits markedly improved efficiency
and robustness across diverse FL scenarios, suggesting broad applicability of
the approach.

</details>


### [71] [Variational Inference Optimized Using the Curved Geometry of Coupled Free Energy](https://arxiv.org/abs/2506.09091)
*Kenric Nelson,Igor Oliveira,Amenah Al-Najafi,Fode Zhang,Hon Keung Tony Ng*

Key words: 变分推理, 耦合自由能量, 耦合变分自编码器, 重尾分布, Wasserstein-2

TL;DR: 提出了一种基于耦合自由能量的优化框架，扩展了变分推理技术，以处理耦合指数族的曲面几何。通过改进的模型准确性和鲁棒性，应用于耦合变分自编码器（CVAE），在训练过程中减少异常值。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统变分推理技术未能充分处理耦合指数族的曲面几何，特别是对于重尾分布（如广义Pareto和Student's t分布）。需要一种能够改进模型准确性和鲁棒性的新方法。

Method: 利用耦合自由能量（耦合ELBO的逆概率）和耦合的Fisher信息度量及仿射连接，设计耦合变分自编码器（CVAE）。通过重尾潜在分布采样及其关联耦合概率，实现更快衰减的尾部。

Result: CVAE在重构CelebA图像时，Wasserstein-2或Fréchet Inception Distance比VAE提高了3%。

Conclusion: 耦合变分推理框架能够有效处理重尾分布，并在训练中减少异常值，提升模型性能。

Abstract: We introduce an optimization framework for variational inference based on the
coupled free energy, extending variational inference techniques to account for
the curved geometry of the coupled exponential family. This family includes
important heavy-tailed distributions such as the generalized Pareto and the
Student's t. By leveraging the coupled free energy, which is equal to the
coupled evidence lower bound (ELBO) of the inverted probabilities, we improve
the accuracy and robustness of the learned model. The coupled generalization of
Fisher Information metric and the affine connection. The method is applied to
the design of a coupled variational autoencoder (CVAE). By using the coupling
for both the distributions and cost functions, the reconstruction metric is
derived to still be the mean-square average loss with modified constants. The
novelty comes from sampling the heavy-tailed latent distribution with its
associated coupled probability, which has faster decaying tails. The result is
the ability to train a model with high penalties in the tails, while assuring
that the training samples have a reduced number of outliers. The Wasserstein-2
or Fr\'echet Inception Distance of the reconstructed CelebA images shows the
CVAE has a 3\% improvement over the VAE after 5 epochs of training.

</details>


### [72] [CUDA-LLM: LLMs Can Write Efficient CUDA Kernels](https://arxiv.org/abs/2506.09092)
*Wentao Chen,Jiace Zhu,Qi Fan,Yehan Ma,An Zou*

Key words: 大语言模型, CUDA, GPU, 代码生成, 性能优化

TL;DR: 该论文提出了一种名为FSR的框架，结合LLMs生成和优化CUDA程序，旨在高效利用GPU硬件，结果显示生成的内核代码性能可提升至179倍。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决LLMs在生成硬件特定、架构感知和高性能GPU代码方面的挑战，提升代码的效率和正确性。

Method: 提出FSR框架，联合优化编译、功能正确性和运行时性能，通过测试和实际内核执行验证。

Result: FSR增强的LLMs不仅保证正确性，生成的代码性能比人工编写的最快可提升179倍。

Conclusion: 结合LLMs和性能强化可实现高效、硬件感知的GPU编程自动化。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
general-purpose code generation. However, generating the code which is deeply
hardware-specific, architecture-aware, and performance-critical, especially for
massively parallel GPUs, remains a complex challenge. In this work, we explore
the use of LLMs for the automated generation and optimization of CUDA programs,
with the goal of producing high-performance GPU kernels that fully exploit the
underlying hardware. To address this challenge, we propose a novel framework
called \textbf{Feature Search and Reinforcement (FSR)}. FSR jointly optimizes
compilation and functional correctness, as well as the runtime performance,
which are validated through extensive and diverse test cases, and measured by
actual kernel execution latency on the target GPU, respectively. This approach
enables LLMs not only to generate syntactically and semantically correct CUDA
code but also to iteratively refine it for efficiency, tailored to the
characteristics of the GPU architecture. We evaluate FSR on representative CUDA
kernels, covering AI workloads and computational intensive algorithms. Our
results show that LLMs augmented with FSR consistently guarantee correctness
rates. Meanwhile, the automatically generated kernels can outperform general
human-written code by a factor of up to 179$\times$ in execution speeds. These
findings highlight the potential of combining LLMs with performance
reinforcement to automate GPU programming for hardware-specific,
architecture-sensitive, and performance-critical applications.

</details>


### [73] [Merging Smarter, Generalizing Better: Enhancing Model Merging on OOD Data](https://arxiv.org/abs/2506.09093)
*Bingjie Zhang,Hongkang Li,Changlong Shi,Guowei Rong,He Zhao,Dongsheng Wang,Dandan Guo,Meng Wang*

Key words: 多任务学习,模型合并,域外性能,层剪枝

TL;DR: 论文提出了LwPTV方法，通过层级剪枝任务向量提升多任务学习（MTL）的域外（OOD）性能，同时保持域内（ID）表现。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前多任务学习模型合并方法主要关注域内数据集性能，忽视了域外数据效果，希望提升OOD任务表现。

Method: 构建显著分数衡量任务向量参数冗余度，生成掩码向量并进行层级剪枝，仅保留预训练模型参数。

Result: 实验显示LwPTV显著提升了模型在OOD任务的表现，同时不影响ID任务能力。

Conclusion: LwPTV方法灵活且高效，可与现有模型合并方法结合，优化OOD表现。

Abstract: Multi-task learning (MTL) concurrently trains a model on diverse task
datasets to exploit common features, thereby improving overall performance
across the tasks. Recent studies have dedicated efforts to merging multiple
independent model parameters into a unified model for MTL, thus circumventing
the need for training data and expanding the scope of applicable scenarios of
MTL. However, current approaches to model merging predominantly concentrate on
enhancing performance within in-domain (ID) datasets, often overlooking their
efficacy on out-of-domain (OOD) datasets. In this work, we proposed LwPTV
(Layer-wise Pruning Task Vector) by building a saliency score, measuring the
redundancy of parameters in task vectors. Designed in this way ours can achieve
mask vector for each task and thus perform layer-wise pruning on the task
vectors, only keeping the pre-trained model parameters at the corresponding
layer in merged model. Owing to its flexibility, our method can be seamlessly
integrated with most of existing model merging methods to improve their
performance on OOD tasks. Extensive experiments demonstrate that the
application of our method results in substantial enhancements in OOD
performance while preserving the ability on ID tasks.

</details>


### [74] [Intra-Trajectory Consistency for Reward Modeling](https://arxiv.org/abs/2506.09096)
*Chaoyang Zhou,Shunyu Liu,Zengmao Wang,Di Wang,Rong-Cheng Tu,Bo Du,Dacheng Tao*

Key words: 奖励模型,RLHF,生成概率,一致性正则化

TL;DR: 本文提出了一种利用生成概率在响应轨迹中建立奖励一致性的方法，通过细粒度信号改进奖励模型的泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前奖励模型依赖粗粒度的响应级评分，难以识别与评分相关的响应轨迹中具体部分，导致泛化能力差。

Method: 提出利用生成概率建立奖励一致性，并通过贝叶斯框架开发一致性正则化方法。

Result: 改进的奖励模型在RewardBench上表现更好，DPO对齐策略和BON推理验证结果也更好。

Conclusion: 细粒度的奖励一致性信号能显著提升奖励模型的性能。

Abstract: Reward models are critical for improving large language models (LLMs),
particularly in reinforcement learning from human feedback (RLHF) or
inference-time verification. Current reward modeling typically relies on scores
of overall responses to learn the outcome rewards for the responses. However,
since the response-level scores are coarse-grained supervision signals, the
reward model struggles to identify the specific components within a response
trajectory that truly correlate with the scores, leading to poor generalization
on unseen responses. In this paper, we propose to leverage generation
probabilities to establish reward consistency between processes in the response
trajectory, which allows the response-level supervisory signal to propagate
across processes, thereby providing additional fine-grained signals for reward
learning. Building on analysis under the Bayesian framework, we develop an
intra-trajectory consistency regularization to enforce that adjacent processes
with higher next-token generation probability maintain more consistent rewards.
We apply the proposed regularization to the advanced outcome reward model,
improving its performance on RewardBench. Besides, we show that the reward
model trained with the proposed regularization induces better DPO-aligned
policies and achieves better best-of-N (BON) inference-time verification
results. Our code is provided in https://github.com/chaoyang101/ICRM.

</details>


### [75] [Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers](https://arxiv.org/abs/2506.09099)
*Joshua Barron,Devin White*

Key words: 记忆，泛化，Transformer模型，模型容量，学习行为

TL;DR: 研究表明，大语言模型中的记忆与泛化能力之间存在权衡关系，模型容量影响学习行为。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨大规模语言模型中记忆与泛化能力的相互关系及其与模型容量的关系。

Method: 通过预训练不同容量的Transformer模型，设计字符级别的任务分别测试泛化和记忆能力。

Result: 小模型擅长泛化但记忆差，大模型相反；联合训练时，所有模型均无法泛化。

Conclusion: 预训练可能偏向某种学习模式，研究为小型语言模型设计与部署提供启示。

Abstract: The relationship between memorization and generalization in large language
models (LLMs) remains an open area of research, with growing evidence that the
two are deeply intertwined. In this work, we investigate this relationship by
pre-training a series of capacity-limited Transformer models from scratch on
two synthetic character-level tasks designed to separately probe generalization
(via arithmetic extrapolation) and memorization (via factual recall). We
observe a consistent trade-off: small models extrapolate to unseen arithmetic
cases but fail to memorize facts, while larger models memorize but fail to
extrapolate. An intermediate-capacity model exhibits a similar shift toward
memorization. When trained on both tasks jointly, no model (regardless of size)
succeeds at extrapolation. These findings suggest that pre-training may
intrinsically favor one learning mode over the other. By isolating these
dynamics in a controlled setting, our study offers insight into how model
capacity shapes learning behavior and offers broader implications for the
design and deployment of small language models.

</details>


### [76] [An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks](https://arxiv.org/abs/2410.16222)
*Valentyn Boreiko,Alexander Panfilov,Vaclav Voracek,Matthias Hein,Jonas Geiping*

Key words: 越狱攻击、威胁模型、N-gram语言模型、离散优化、LLM安全性

TL;DR: 提出了一种统一威胁模型，用于系统比较越狱攻击方法，发现离散优化攻击优于LLM攻击，并分析了有效攻击的关键特征。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有越狱攻击方法差异大，缺乏统一比较标准，需系统性评估。

Method: 基于N-gram语言模型构建威胁模型，评估攻击在文本分布中的可能性，并对比多种攻击方法。

Result: 安全调优模型的攻击成功率低于预期，离散优化攻击表现优于LLM攻击。

Conclusion: 威胁模型可解释性强，揭示了有效攻击利用罕见双字母组合的特点。

Abstract: A plethora of jailbreaking attacks have been proposed to obtain harmful
responses from safety-tuned LLMs. These methods largely succeed in coercing the
target output in their original settings, but their attacks vary substantially
in fluency and computational effort. In this work, we propose a unified threat
model for the principled comparison of these methods. Our threat model checks
if a given jailbreak is likely to occur in the distribution of text. For this,
we build an N-gram language model on 1T tokens, which, unlike model-based
perplexity, allows for an LLM-agnostic, nonparametric, and inherently
interpretable evaluation. We adapt popular attacks to this threat model, and,
for the first time, benchmark these attacks on equal footing with it. After an
extensive comparison, we find attack success rates against safety-tuned modern
models to be lower than previously presented and that attacks based on discrete
optimization significantly outperform recent LLM-based attacks. Being
inherently interpretable, our threat model allows for a comprehensive analysis
and comparison of jailbreak attacks. We find that effective attacks exploit and
abuse infrequent bigrams, either selecting the ones absent from real-world text
or rare ones, e.g., specific to Reddit or code datasets.

</details>


### [77] [Feature Shift Localization Network](https://arxiv.org/abs/2506.09101)
*Míriam Barrabés,Daniel Mas Montserrat,Kapal Dev,Alexander G. Ioannidis*

Key words: 特征偏移、定位、神经网络、高维数据、数据校正

TL;DR: FSL-Net 是一种神经网络，用于在大规模高维数据中快速准确地定位特征偏移，解决了当前方法不准确或不可扩展的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于数据源异构、测量噪声或处理不一致，特征偏移在多个领域中普遍存在，影响下游分析质量，但目前缺乏高效准确的定位方法。

Method: 提出了特征偏移定位网络（FSL-Net），通过训练大量数据集学习统计特性，无需重新训练即可在新数据集和偏移中定位特征。

Result: FSL-Net 能够高效且准确地定位特征偏移，适用于大规模高维数据，代码和预训练模型已开源。

Conclusion: FSL-Net 为解决特征偏移定位问题提供了一种可扩展且准确的解决方案。

Abstract: Feature shifts between data sources are present in many applications
involving healthcare, biomedical, socioeconomic, financial, survey, and
multi-sensor data, among others, where unharmonized heterogeneous data sources,
noisy data measurements, or inconsistent processing and standardization
pipelines can lead to erroneous features. Localizing shifted features is
important to address the underlying cause of the shift and correct or filter
the data to avoid degrading downstream analysis. While many techniques can
detect distribution shifts, localizing the features originating them is still
challenging, with current solutions being either inaccurate or not scalable to
large and high-dimensional datasets. In this work, we introduce the Feature
Shift Localization Network (FSL-Net), a neural network that can localize
feature shifts in large and high-dimensional datasets in a fast and accurate
manner. The network, trained with a large number of datasets, learns to extract
the statistical properties of the datasets and can localize feature shifts from
previously unseen datasets and shifts without the need for re-training. The
code and ready-to-use trained model are available at
https://github.com/AI-sandbox/FSL-Net.

</details>


### [78] [Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs](https://arxiv.org/abs/2506.09104)
*Jung Hyun Lee,Seungjae Shin,Vinnam Kim,Jaeseong You,An Chen*

Key words: 大语言模型,极低位量化,指令调整型模型,蒸馏训练,渐进量化

TL;DR: 提出了一种新型渐进量化框架UPQ，将块级后训练量化与蒸馏量化训练结合，成功将指令调整型LLM量化为2位，且在MMLU和IFEval基准测试中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决资源受限设备上部署大语言模型的问题，尤其是针对指令调整型模型的极低位量化。

Method: 采用FP16→INT4→INT2的渐进量化策略，结合块级PTQ和Distill-QAT方法，最小化Jensen-Shannon散度。

Result: 首次实现开源指令调整型LLM的2位量化，无需专有数据，且在MMLU和IFEval基准中达到最优性能。

Conclusion: UPQ框架为指令调整型LLM的低位量化提供了高效解决方案。

Abstract: As the rapid scaling of large language models (LLMs) poses significant
challenges for deployment on resource-constrained devices, there is growing
interest in extremely low-bit quantization, such as 2-bit. Although prior works
have shown that 2-bit large models are pareto-optimal over their 4-bit smaller
counterparts in both accuracy and latency, these advancements have been limited
to pre-trained LLMs and have not yet been extended to instruction-tuned models.
To bridge this gap, we propose Unified Progressive Quantization (UPQ)$-$a novel
progressive quantization framework (FP16$\rightarrow$INT4$\rightarrow$INT2)
that unifies block-wise post-training quantization (PTQ) with
distillation-based quantization-aware training (Distill-QAT) for INT2
instruction-tuned LLM quantization. UPQ first quantizes FP16 instruction-tuned
models to INT4 using block-wise PTQ to significantly reduce the quantization
error introduced by subsequent INT2 quantization. Next, UPQ applies Distill-QAT
to enable INT2 instruction-tuned LLMs to generate responses consistent with
their original FP16 counterparts by minimizing the generalized Jensen-Shannon
divergence (JSD) between the two. To the best of our knowledge, we are the
first to demonstrate that UPQ can quantize open-source instruction-tuned LLMs
to INT2 without relying on proprietary post-training data, while achieving
state-of-the-art performances on MMLU and IFEval$-$two of the most
representative benchmarks for evaluating instruction-tuned LLMs.

</details>


### [79] [MetaTT: A Global Tensor-Train Adapter for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2506.09105)
*Javier Lopez-Piqueres,Pranav Deshpande,Archan Ray,Mattia J. Villani,Marco Pistoia,Niraj Kumar*

Key words: MetaTT, Tensor Train, 预训练Transformer, 低秩微调, 参数压缩

TL;DR: MetaTT是一个统一的Tensor Train适配器框架，用于预训练Transformer的全局低秩微调，通过共享TT分解所有子模块，显著减少参数数量并保持准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了解决现有微调方法（如LoRA）独立微调每个权重矩阵导致参数冗余的问题，MetaTT旨在通过共享Tensor Train结构实现更高效的参数压缩。

Method: MetaTT使用单一的共享Tensor Train来分解Transformer的所有子模块（如查询、键、值和投影层），并通过索引结构轴（如层和矩阵类型）来优化参数存储，支持动态扩展以适应多任务。

Result: 在标准语言建模基准测试中，MetaTT在保持与LoRA相近的准确性的同时，显著减少了参数数量，甚至优于其他基于张量分解的方法。

Conclusion: MetaTT提供了一种高效的全局低秩微调方法，通过共享Tensor Train结构实现了参数的显著压缩，同时简化了训练过程并支持多任务扩展。

Abstract: We present MetaTT, a unified Tensor Train (TT) adapter framework for global
low-rank fine-tuning of pre-trained transformers. Unlike LoRA, which fine-tunes
each weight matrix independently, MetaTT uses a single shared TT to factorize
all transformer sub-modules -- query, key, value, projection, and feed-forward
layers -- by indexing the structural axes like layer and matrix type, and
optionally heads and tasks. For a given rank, while LoRA adds parameters
proportional to the product across modes, MetaTT only adds parameters
proportional to the sum across modes leading to a significantly compressed
final adapter. Our benchmarks compare MetaTT with LoRA along with recent
state-of-the-art matrix and tensor decomposition based fine-tuning schemes. We
observe that when tested on standard language modeling benchmarks, MetaTT leads
to the most reduction in the parameters while maintaining similar accuracy to
LoRA and even outperforming other tensor-based methods. Unlike CP or other
rank-factorizations, the TT ansatz benefits from mature optimization routines
-- e.g., DMRG-style rank adaptive minimization in addition to Adam, which we
find simplifies training. Because new modes can be appended cheaply, MetaTT
naturally extends to shared adapters across many tasks without redesigning the
core tensor.

</details>


### [80] [SensorLM: Learning the Language of Wearable Sensors](https://arxiv.org/abs/2506.09108)
*Yuwei Zhang,Kumar Ayush,Siyuan Qiao,A. Ali Heydari,Girish Narayanswamy,Maxwell A. Xu,Ahmed A. Metwally,Shawn Xu,Jake Garrison,Xuhai Xu,Tim Althoff,Yun Liu,Pushmeet Kohli,Jiening Zhan,Mark Malhotra,Shwetak Patel,Cecilia Mascolo,Xin Liu,Daniel McDuff,Yuzhe Yang*

Key words: 可穿戴传感器, 自然语言处理, 多模态预训练, 零样本学习, 跨模态检索

TL;DR: SensorLM是一款传感器-语言基础模型家族，通过自然语言理解可穿戴传感器数据，解决了未标注真实世界数据对齐的挑战，并在多个任务中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决可穿戴传感器数据与自然语言对齐和解释的难题，尤其是在缺乏标注数据的情况下。

Method: 采用层次化标题生成流程，结合多模态预训练架构（如CLIP、CoCa），构建大规模传感器-语言数据集。

Result: 在零样本识别、少样本学习和跨模态检索等任务中优于现有技术，并展示了扩展性和标签效率等能力。

Conclusion: SensorLM为传感器数据的自然语言理解提供了强有力的工具，并在多个应用场景中表现出色。

Abstract: We present SensorLM, a family of sensor-language foundation models that
enable wearable sensor data understanding with natural language. Despite its
pervasive nature, aligning and interpreting sensor data with language remains
challenging due to the lack of paired, richly annotated sensor-text
descriptions in uncurated, real-world wearable data. We introduce a
hierarchical caption generation pipeline designed to capture statistical,
structural, and semantic information from sensor data. This approach enabled
the curation of the largest sensor-language dataset to date, comprising over
59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM
extends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and
recovers them as specific variants within a generic architecture. Extensive
experiments on real-world tasks in human activity analysis and healthcare
verify the superior performance of SensorLM over state-of-the-art in zero-shot
recognition, few-shot learning, and cross-modal retrieval. SensorLM also
demonstrates intriguing capabilities including scaling behaviors, label
efficiency, sensor captioning, and zero-shot generalization to unseen tasks.

</details>


### [81] [CodeBrain: Bridging Decoupled Tokenizer and Multi-Scale Architecture for EEG Foundation Model](https://arxiv.org/abs/2506.09110)
*Jingying Ma,Feng Wu,Qika Lin,Yucheng Xing,Chenyu Liu,Ziyu Jia,Mengling Feng*

Key words: EEG, 基础模型, TFDual-Tokenizer, EEGSSM, 多尺度依赖

TL;DR: CodeBrain是一种高效的EEG基础模型，通过TFDual-Tokenizer和EEGSSM分阶段训练，解决了现有模型在异质表示和脑依赖捕获方面的不足。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统EEG模型因配置和任务差异泛化能力受限，现有EFMs在异质表示和多尺度脑依赖捕获上表现不足。

Method: 提出TFDual-Tokenizer独立标记时间与频率成分，EEGSSM结合全局卷积和滑动窗口注意力，训练采用掩码自监督学习。

Result: 在10个公开EEG数据集上验证了CodeBrain的泛化能力，线性探测表现优异。

Conclusion: CodeBrain为EEG建模提供了生物信息学解释性，为未来神经科学研究奠定了基础。

Abstract: Electroencephalography (EEG) provides real-time insights into brain activity
and is widely used in neuroscience. However, variations in channel
configurations, sequence lengths, and task objectives limit the transferability
of traditional task-specific models. Although recent EEG foundation models
(EFMs) aim to learn generalizable representations, they struggle with limited
heterogeneous representation capacity and inefficiency in capturing multi-scale
brain dependencies. To address these challenges, we propose CodeBrain, an
efficient EFM structurally aligned with brain organization, trained in two
stages. (1) We introduce a TFDual-Tokenizer that independently tokenizes
heterogeneous temporal and frequency components, enabling a quadratic expansion
of the discrete representation space. This also offers a degree of
interpretability through cross-domain token analysis. (2) We propose the
EEGSSM, which combines a structured global convolution architecture and a
sliding window attention mechanism to jointly model sparse long-range and local
dependencies. Unlike fully connected Transformer models, EEGSSM better reflects
the brain's small-world topology and efficiently captures EEG's inherent
multi-scale structure. EEGSSM is trained with a masked self-supervised learning
objective to predict token indices obtained in TFDual-Tokenizer. Comprehensive
experiments on 10 public EEG datasets demonstrate the generalizability of
CodeBrain with linear probing. By offering biologically informed and
interpretable EEG modeling, CodeBrain lays the foundation for future
neuroscience research. Both code and pretraining weights will be released in
the future version.

</details>


### [82] [TRACE: Grounding Time Series in Context for Multimodal Embedding and Retrieval](https://arxiv.org/abs/2506.09114)
*Jialin Chen,Ziyu Zhao,Gaukhar Nurbek,Aosong Feng,Ali Maatouk,Leandros Tassiulas,Yifeng Gao,Rex Ying*

Key words: 时间序列检索, 跨模态对齐, 多模态, 硬负样本挖掘, 预测模型

TL;DR: TRACE是一个通用的多模态检索器，通过将时间序列嵌入与对齐的文本上下文结合，实现了跨模态检索，提升了预测准确性和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 动态数据（如天气、医疗和能源领域）的普遍性增加了对时间序列数据有效解释和检索的需求，现有方法缺乏语义基础和异构模态对齐能力。

Method: 提出TRACE，通过细粒度通道级对齐和硬负样本挖掘，支持灵活的跨模态检索模式（文本到时间序列和时间序列到文本）。

Result: TRACE在下游预测和分类任务中实现了最先进的性能，并作为通用检索器增强了时间序列模型。

Conclusion: TRACE不仅是一个有效的检索器，还是一个强大的编码器，为时间序列模型提供了丰富的信息上下文。

Abstract: The ubiquity of dynamic data in domains such as weather, healthcare, and
energy underscores a growing need for effective interpretation and retrieval of
time-series data. These data are inherently tied to domain-specific contexts,
such as clinical notes or weather narratives, making cross-modal retrieval
essential not only for downstream tasks but also for developing robust
time-series foundation models by retrieval-augmented generation (RAG). Despite
the increasing demand, time-series retrieval remains largely underexplored.
Existing methods often lack semantic grounding, struggle to align heterogeneous
modalities, and have limited capacity for handling multi-channel signals. To
address this gap, we propose TRACE, a generic multimodal retriever that grounds
time-series embeddings in aligned textual context. TRACE enables fine-grained
channel-level alignment and employs hard negative mining to facilitate
semantically meaningful retrieval. It supports flexible cross-modal retrieval
modes, including Text-to-Timeseries and Timeseries-to-Text, effectively linking
linguistic descriptions with complex temporal patterns. By retrieving
semantically relevant pairs, TRACE enriches downstream models with informative
context, leading to improved predictive accuracy and interpretability. Beyond a
static retrieval engine, TRACE also serves as a powerful standalone encoder,
with lightweight task-specific tuning that refines context-aware
representations while maintaining strong cross-modal alignment. These
representations achieve state-of-the-art performance on downstream forecasting
and classification tasks. Extensive experiments across multiple domains
highlight its dual utility, as both an effective encoder for downstream
applications and a general-purpose retriever to enhance time-series models.

</details>


### [83] [Scalable Spatiotemporal Inference with Biased Scan Attention Transformer Neural Processes](https://arxiv.org/abs/2506.09163)
*Daniel Jenson,Jhonathan Navott,Piotr Grynfelder,Mengyan Zhang,Makkunda Sharma,Elizaveta Semenova,Seth Flaxman*

Key words: Neural Processes, Translation Invariance, Scalability, Kernel Regression Blocks, Biased Scan Attention

TL;DR: BSA-TNP是一种新型神经过程模型，通过引入KRBlocks和BSA技术，实现了在保持高精度的同时提高效率和可扩展性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了解决现有神经过程模型在复杂应用中准确性与可扩展性之间的权衡问题，尤其是在具有平移不变性的过程中。

Method: 提出BSA-TNP架构，结合KRBlocks、群不变注意力偏置和BSA技术。

Result: BSA-TNP在准确性上优于或匹配最佳模型，且训练时间更短，支持多分辨率学习和高维固定效应，并能高效处理大规模数据。

Conclusion: BSA-TNP在保持高精度的同时显著提升了模型的可扩展性和效率，适用于多种复杂应用场景。

Abstract: Neural Processes (NPs) are a rapidly evolving class of models designed to
directly model the posterior predictive distribution of stochastic processes.
While early architectures were developed primarily as a scalable alternative to
Gaussian Processes (GPs), modern NPs tackle far more complex and data hungry
applications spanning geology, epidemiology, climate, and robotics. These
applications have placed increasing pressure on the scalability of these
models, with many architectures compromising accuracy for scalability. In this
paper, we demonstrate that this tradeoff is often unnecessary, particularly
when modeling fully or partially translation invariant processes. We propose a
versatile new architecture, the Biased Scan Attention Transformer Neural
Process (BSA-TNP), which introduces Kernel Regression Blocks (KRBlocks),
group-invariant attention biases, and memory-efficient Biased Scan Attention
(BSA). BSA-TNP is able to: (1) match or exceed the accuracy of the best models
while often training in a fraction of the time, (2) exhibit translation
invariance, enabling learning at multiple resolutions simultaneously, (3)
transparently model processes that evolve in both space and time, (4) support
high dimensional fixed effects, and (5) scale gracefully -- running inference
with over 1M test points with 100K context points in under a minute on a single
24GB GPU.

</details>


### [84] [Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search](https://arxiv.org/abs/2506.09171)
*Samuel Holt,Max Ruiz Luyten,Thomas Pouplin,Mihaela van der Schaar*

Key words: 大型语言模型（LLM）、上下文学习、递归前瞻搜索、原子事实、交互任务

TL;DR: 该论文提出了一种新型大型语言模型（LLM）代理框架，通过上下文学习和递归前瞻搜索增强规划能力，无需微调即可动态提升决策效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有LLM在复杂交互环境中需要大量指导或交互历史才能有效工作，且难以适应新信息或高效利用历史经验进行多步推理。

Method: 通过原子事实增强和递归前瞻搜索，代理从交互轨迹中提取关键"原子事实"，动态增强LLM组件的提示，支持动作提议、潜世界模型模拟和状态值估计。

Result: 代理在交互任务（如TextFrozenLake和ALFWorld）中表现出更好的性能和适应性，随着经验积累行为更优。

Conclusion: 论文提出的方法通过事实抽象和LLM模拟质量的提升，实现了代理在复杂环境中的在线学习和行为优化。

Abstract: Large Language Models (LLMs) are increasingly capable but often require
significant guidance or extensive interaction history to perform effectively in
complex, interactive environments. Existing methods may struggle with adapting
to new information or efficiently utilizing past experiences for multi-step
reasoning without fine-tuning. We introduce a novel LLM agent framework that
enhances planning capabilities through in-context learning, facilitated by
atomic fact augmentation and a recursive lookahead search. Our agent learns to
extract task-critical ``atomic facts'' from its interaction trajectories. These
facts dynamically augment the prompts provided to LLM-based components
responsible for action proposal, latent world model simulation, and state-value
estimation. Planning is performed via a depth-limited lookahead search, where
the LLM simulates potential trajectories and evaluates their outcomes, guided
by the accumulated facts and interaction history. This approach allows the
agent to improve its understanding and decision-making online, leveraging its
experience to refine its behavior without weight updates. We provide a
theoretical motivation linking performance to the quality of fact-based
abstraction and LLM simulation accuracy. Empirically, our agent demonstrates
improved performance and adaptability on challenging interactive tasks,
achieving more optimal behavior as it accumulates experience, showcased in
tasks such as TextFrozenLake and ALFWorld.

</details>


### [85] [MultiNet: An Open-Source Software Toolkit \& Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models](https://arxiv.org/abs/2506.09172)
*Pranav Guruprasad,Yangyue Wang,Harshvardhan Sikka*

Key words: 多模态动作模型, 视觉语言模型, 基准测试, 开源软件, 数据集

TL;DR: MultiNet是一个开源的多模态动作模型基准测试和软件生态系统，用于评估和适应视觉、语言和动作领域的模型。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 开发一个通用多模态智能体需要结合视觉理解、语言理解和动作生成，MultiNet旨在提供标准化评估工具和数据支持。

Method: MultiNet提供了标准化的评估协议、开源软件环境，以及包含1.3万亿标记的复合数据集，覆盖多种任务。

Result: MultiNet的基准测试、框架和工具包已被用于下游研究，揭示了视觉语言动作模型的泛化局限性。

Conclusion: MultiNet为多模态动作模型的开发和评估提供了重要资源，支持未来研究。

Abstract: Recent innovations in multimodal action models represent a promising
direction for developing general-purpose agentic systems, combining visual
understanding, language comprehension, and action generation. We introduce
MultiNet - a novel, fully open-source benchmark and surrounding software
ecosystem designed to rigorously evaluate and adapt models across vision,
language, and action domains. We establish standardized evaluation protocols
for assessing vision-language models (VLMs) and vision-language-action models
(VLAs), and provide open source software to download relevant data, models, and
evaluations. Additionally, we provide a composite dataset with over 1.3
trillion tokens of image captioning, visual question answering, commonsense
reasoning, robotic control, digital game-play, simulated
locomotion/manipulation, and many more tasks. The MultiNet benchmark,
framework, toolkit, and evaluation harness have been used in downstream
research on the limitations of VLA generalization.

</details>


### [86] [The Curious Language Model: Strategic Test-Time Information Acquisition](https://arxiv.org/abs/2506.09173)
*Michael Cooper,Rohan Wadhawan,John Michael Giorgi,Chenhao Tan,Davis Liang*

Key words: 信息获取,贪心树搜索,大型语言模型,临床诊断

TL;DR: CuriosiTree是一种基于启发式的零-shot信息获取策略，用于大型语言模型（LLMs），通过贪心树搜索平衡信息增益和成本，在临床诊断模拟中表现优于基线策略。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 决策者在信息不足时需要获取额外信息，但不同信息获取方式成本不同，需平衡信息增益与成本。

Method: 提出CuriosiTree，基于贪心树搜索估计每个行动的预期信息增益，并根据信息增益与成本选择行动。

Result: 在临床诊断模拟中，CuriosiTree能有效整合异构信息源，诊断准确性优于基线策略。

Conclusion: CuriosiTree是一种高效、成本可控的信息获取策略，适用于LLMs。

Abstract: Decision-makers often possess insufficient information to render a confident
decision. In these cases, the decision-maker can often undertake actions to
acquire the necessary information about the problem at hand, e.g., by
consulting knowledgeable authorities or by conducting experiments. Importantly,
different levers of information acquisition come with different costs, posing
the challenge of selecting the actions that are both informative and
cost-effective. In this work, we propose CuriosiTree, a heuristic-based,
test-time policy for zero-shot information acquisition in large language models
(LLMs). CuriosiTree employs a greedy tree search to estimate the expected
information gain of each action and strategically chooses actions based on a
balance of anticipated information gain and associated cost. Empirical
validation in a clinical diagnosis simulation shows that CuriosiTree enables
cost-effective integration of heterogenous sources of information, and
outperforms baseline action selection strategies in selecting action sequences
that enable accurate diagnosis.

</details>


### [87] [Multivariate Long-term Time Series Forecasting with Fourier Neural Filter](https://arxiv.org/abs/2506.09174)
*Chenheng Xu,Dan Wu,Yixin Zhu,Ying Nian Wu*

Key words: 时间序列预测,时空建模,FNF,DBD,信息瓶颈

TL;DR: FNF和DBD架构解决了多元长期时间序列预测中时空建模的挑战，无需辅助技术即可实现最先进性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法未能充分处理时间序列的独特特性（如周期性），研究缺乏具有时序特定归纳偏置的专用主干网络。

Method: 提出FNF主干和DBD架构，结合局部时间域和全局频率域信息处理，并通过信息瓶颈理论优化梯度流和表示能力。

Result: 在11个公共基准数据集上取得最优性能，且无需辅助技术，证明了设计良好的神经网络架构的潜力。

Conclusion: FNF和DBD架构能有效捕捉时间序列的内在特性，可能推动科学和工业应用中的时间序列建模变革。

Abstract: Multivariate long-term time series forecasting has been suffering from the
challenge of capturing both temporal dependencies within variables and spatial
correlations across variables simultaneously. Current approaches predominantly
repurpose backbones from natural language processing or computer vision (e.g.,
Transformers), which fail to adequately address the unique properties of time
series (e.g., periodicity). The research community lacks a dedicated backbone
with temporal-specific inductive biases, instead relying on domain-agnostic
backbones supplemented with auxiliary techniques (e.g., signal decomposition).
We introduce FNF as the backbone and DBD as the architecture to provide
excellent learning capabilities and optimal learning pathways for
spatio-temporal modeling, respectively. Our theoretical analysis proves that
FNF unifies local time-domain and global frequency-domain information
processing within a single backbone that extends naturally to spatial modeling,
while information bottleneck theory demonstrates that DBD provides superior
gradient flow and representation capacity compared to existing unified or
sequential architectures. Our empirical evaluation across 11 public benchmark
datasets spanning five domains (energy, meteorology, transportation,
environment, and nature) confirms state-of-the-art performance with consistent
hyperparameter settings. Notably, our approach achieves these results without
any auxiliary techniques, suggesting that properly designed neural
architectures can capture the inherent properties of time series, potentially
transforming time series modeling in scientific and industrial applications.

</details>


### [88] [Multi-Task Reward Learning from Human Ratings](https://arxiv.org/abs/2506.09183)
*Mingkang Wu,Devin White,Evelyn Rose,Vernon Lawhern,Nicholas R Waytowich,Yongcan Cao*

Key words: 强化学习, 人类反馈, 决策建模, 多任务学习, 奖励函数

TL;DR: 提出了一种模仿人类决策的强化学习方法，通过联合考虑多任务来优化奖励函数，实验结果优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 目前基于人类反馈的强化学习（RLHF）方法通常将人决策简化为孤立任务（如分类或回归），未能全面捕捉人类的多策略整合能力，因此需要一种更接近人类决策的方法。

Method: 提出一种新强化学习方法，利用无奖励环境中的人类评分推断奖励函数，通过可学习权重平衡分类和回归模型的贡献，以捕捉人类决策的不确定性。

Result: 实验表明，该方法在合成人类评分上优于现有基于评分的RL方法，甚至在某些情况下超越传统RL方法。

Conclusion: 该方法通过模仿人类多任务决策机制，显著提升了模型行为与用户目标的对齐能力。

Abstract: Reinforcement learning from human feeback (RLHF) has become a key factor in
aligning model behavior with users' goals. However, while humans integrate
multiple strategies when making decisions, current RLHF approaches often
simplify this process by modeling human reasoning through isolated tasks such
as classification or regression. In this paper, we propose a novel
reinforcement learning (RL) method that mimics human decision-making by jointly
considering multiple tasks. Specifically, we leverage human ratings in
reward-free environments to infer a reward function, introducing learnable
weights that balance the contributions of both classification and regression
models. This design captures the inherent uncertainty in human decision-making
and allows the model to adaptively emphasize different strategies. We conduct
several experiments using synthetic human ratings to validate the effectiveness
of the proposed approach. Results show that our method consistently outperforms
existing rating-based RL methods, and in some cases, even surpasses traditional
RL approaches.

</details>


### [89] [LaDCast: A Latent Diffusion Model for Medium-Range Ensemble Weather Forecasting](https://arxiv.org/abs/2506.09193)
*Yilin Zhuang,Karthik Duraisamy*

Key words: 集合预报、潜在扩散、机器学习、极端事件、不确定性量化

TL;DR: LaDCast是全球首个用于中程集合预报的潜在扩散框架，通过潜在空间生成小时级集合预报，结合多种创新技术提升预报性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决传统集合数值天气预报和机器学习方法在准确预报和高效不确定性量化方面的挑战。

Method: 使用自编码器压缩高维数据，结合基于变压器的扩散模型生成潜在更新，引入GeoRoPE、双流注意力机制和正弦时间嵌入。

Result: LaDCast在确定性和概率性技能上接近欧洲中期预报中心IFS-ENS，且在极端事件追踪上表现更优，计算和存储成本大幅降低。

Conclusion: LaDCast展示了实时千米分辨率预报的可行性，为高性能天气预报提供了新途径。

Abstract: Accurate probabilistic weather forecasting demands both high accuracy and
efficient uncertainty quantification, challenges that overburden both ensemble
numerical weather prediction (NWP) and recent machine-learning methods. We
introduce LaDCast, the first global latent-diffusion framework for medium-range
ensemble forecasting, which generates hourly ensemble forecasts entirely in a
learned latent space. An autoencoder compresses high-dimensional ERA5
reanalysis fields into a compact representation, and a transformer-based
diffusion model produces sequential latent updates with arbitrary hour
initialization. The model incorporates Geometric Rotary Position Embedding
(GeoRoPE) to account for the Earth's spherical geometry, a dual-stream
attention mechanism for efficient conditioning, and sinusoidal temporal
embeddings to capture seasonal patterns. LaDCast achieves deterministic and
probabilistic skill close to that of the European Centre for Medium-Range
Forecast IFS-ENS, without any explicit perturbations. Notably, LaDCast
demonstrates superior performance in tracking rare extreme events such as
cyclones, capturing their trajectories more accurately than established models.
By operating in latent space, LaDCast reduces storage and compute by orders of
magnitude, demonstrating a practical path toward forecasting at kilometer-scale
resolution in real time. We open-source our code and models and provide the
training and evaluation pipelines at: https://github.com/tonyzyl/ladcast.

</details>


### [90] [FLoRIST: Singular Value Thresholding for Efficient and Accurate Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.09199)
*Hariharan Ramesh,Jyotikrishna Dass*

Key words: 联邦学习, LoRA, 通信效率, 奇异值分解, 异构客户端

TL;DR: FLoRIST是一个联邦学习框架，通过低秩适应（LoRA）实现高效微调大型语言模型，避免了数据共享，并在通信效率和计算成本之间取得平衡。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决联邦学习中LoRA方法在通信效率、模型准确性和计算成本之间的不平衡问题。

Method: 采用奇异值分解（SVD）分步处理本地适配器，在紧凑的中介空间中构建全局低秩适配器。

Result: 在多种数据集和LLM上，FLoRIST表现出卓越的通信效率和竞争性性能。

Conclusion: FLoRIST在异构和同构环境中均优于现有方法，平衡了通信与计算开销。

Abstract: Integrating Low-Rank Adaptation (LoRA) into federated learning offers a
promising solution for parameter-efficient fine-tuning of Large Language Models
(LLMs) without sharing local data. However, several methods designed for
federated LoRA present significant challenges in balancing communication
efficiency, model accuracy, and computational cost, particularly among
heterogeneous clients. These methods either rely on simplistic averaging of
local adapters, which introduces aggregation noise, require transmitting large
stacked local adapters, leading to poor communication efficiency, or
necessitate reconstructing memory-dense global weight-update matrix and
performing computationally expensive decomposition to design client-specific
low-rank adapters. In this work, we propose FLoRIST, a federated fine-tuning
framework that achieves mathematically accurate aggregation without incurring
high communication or computational overhead. Instead of constructing the full
global weight-update matrix at the server, FLoRIST employs an efficient
decomposition pipeline by performing singular value decomposition on stacked
local adapters separately. This approach operates within a compact intermediate
space to represent the accumulated information from local LoRAs. We introduce
tunable singular value thresholding for server-side optimal rank selection to
construct a pair of global low-rank adapters shared by all clients. Extensive
empirical evaluations across multiple datasets and LLMs demonstrate that
FLoRIST consistently strikes the best balance between superior communication
efficiency and competitive performance in both homogeneous and heterogeneous
setups.

</details>


### [91] [FedRAG: A Framework for Fine-Tuning Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2506.09200)
*Val Andrei Fajardo,David B. Emerson,Amandeep Singh,Veronica Chatrath,Marcelo Lotif,Ravi Theja,Alex Cheung,Izuki Matsubi*

Key words: FedRAG, RAG, retrieval-augmented generation, fine-tuning, federated learning

TL;DR: FedRAG是一个用于在集中式和联邦架构中微调RAG系统的框架，填补了现有工具的空白。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决仅依赖大语言模型参数内存的缺点，通过微调检索器和生成器模型提高RAG系统的性能。

Method: 引入FedRAG框架，支持最先进的微调方法，提供简单直观的界面和从集中式到联邦式训练的转换。

Result: FedRAG深度集成到现代RAG生态系统中，填补了工具空白。

Conclusion: FedRAG为RAG系统的微调提供了高效且灵活的解决方案。

Abstract: Retrieval-augmented generation (RAG) systems have been shown to be effective
in addressing many of the drawbacks of relying solely on the parametric memory
of large language models. Recent work has demonstrated that RAG systems can be
improved via fine-tuning of their retriever and generator models. In this work,
we introduce FedRAG, a framework for fine-tuning RAG systems across centralized
and federated architectures. FedRAG supports state-of-the-art fine-tuning
methods, offering a simple and intuitive interface and a seamless conversion
from centralized to federated training tasks. FedRAG is also deeply integrated
with the modern RAG ecosystem, filling a critical gap in available tools.

</details>


### [92] [Policy-Based Trajectory Clustering in Offline Reinforcement Learning](https://arxiv.org/abs/2506.09202)
*Hao Hu,Xinqi Wang,Simon Shaolei Du*

Key words: 轨迹聚类, 离线强化学习, PG-Kmeans, CAAE, KL散度

TL;DR: 论文提出了一种新的轨迹聚类任务，通过KL散度和策略引导的方法实现，并验证了方法的有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决离线强化学习数据集中轨迹聚类的问题，通过策略生成概率实现聚类。

Method: 提出了Policy-Guided K-means (PG-Kmeans) 和 Centroid-Attracted Autoencoder (CAAE)两种方法。

Result: 在D4RL和GridWorld数据集上验证了方法的有效性，能够将轨迹分成有意义的簇。

Conclusion: 提出的方法为策略引导的轨迹聚类提供了有效框架，适用于离线强化学习等领域。

Abstract: We introduce a novel task of clustering trajectories from offline
reinforcement learning (RL) datasets, where each cluster center represents the
policy that generated its trajectories. By leveraging the connection between
the KL-divergence of offline trajectory distributions and a mixture of
policy-induced distributions, we formulate a natural clustering objective. To
solve this, we propose Policy-Guided K-means (PG-Kmeans) and Centroid-Attracted
Autoencoder (CAAE). PG-Kmeans iteratively trains behavior cloning (BC) policies
and assigns trajectories based on policy generation probabilities, while CAAE
resembles the VQ-VAE framework by guiding the latent representations of
trajectories toward the vicinity of specific codebook entries to achieve
clustering. Theoretically, we prove the finite-step convergence of PG-Kmeans
and identify a key challenge in offline trajectory clustering: the inherent
ambiguity of optimal solutions due to policy-induced conflicts, which can
result in multiple equally valid but structurally distinct clusterings.
Experimentally, we validate our methods on the widely used D4RL dataset and
custom GridWorld environments. Our results show that both PG-Kmeans and CAAE
effectively partition trajectories into meaningful clusters. They offer a
promising framework for policy-based trajectory clustering, with broad
applications in offline RL and beyond.

</details>


### [93] [mLaSDI: Multi-stage latent space dynamics identification](https://arxiv.org/abs/2506.09207)
*William Anderson,Kevin Chung,Youngsoo Choi*

Key words: 降阶模型, 自编码器, PDE求解, 潜在空间动态性

TL;DR: 论文提出了一种改进的降阶模型方法mLaSDI，通过多阶段训练自编码器来降低误差和训练时间。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统的LaSDI方法在处理复杂或高频率数据时，难以同时满足重构精度和潜在空间动态性，因此需要改进。

Method: 采用多阶段自编码器训练（mLaSDI），每一阶段的自编码器学习纠正前一阶段的误差。

Result: mLaSDI方法在小规模自编码器上表现出更低的预测和重构误差，同时减少了训练时间。

Conclusion: mLaSDI是一种高效且精确的降阶模型方法，适用于复杂或高频率的PDE求解问题。

Abstract: Determining accurate numerical solutions of partial differential equations
(PDEs) is an important task in many scientific disciplines. However, solvers
can be computationally expensive, leading to the development of reduced-order
models (ROMs). Recently, Latent Space Dynamics Identification (LaSDI) was
proposed as a data-driven, non-intrusive ROM framework. LaSDI compresses the
training data using an autoencoder and learns a system of user-chosen ordinary
differential equations (ODEs), which govern the latent space dynamics. This
allows for rapid predictions by interpolating and evolving the low-dimensional
ODEs in the latent space. While LaSDI has produced effective ROMs for numerous
problems, the autoencoder can have difficulty accurately reconstructing
training data while also satisfying the imposed dynamics in the latent space,
particularly in complex or high-frequency regimes. To address this, we propose
multi-stage Latent Space Dynamics Identification (mLaSDI). With mLaSDI, several
autoencoders are trained sequentially in stages, where each autoencoder learns
to correct the error of the previous stages. We find that applying mLaSDI with
small autoencoders results in lower prediction and reconstruction errors, while
also reducing training time compared to LaSDI.

</details>


### [94] [Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs](https://arxiv.org/abs/2506.09215)
*Greyson Brothers*

Key words: Transformer, 池化方法, 信号噪声比, 注意力机制, 向量量化, 鲁棒性

TL;DR: 研究了用于总结Transformer嵌入模型输出的池化方法设计，特别关注信号与噪声混合的输入场景。标准池化方法在信号噪声比（SNR）波动时易失效，而提出的自适应注意力池化方法在理论上和实验中均表现更优。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对Transformer输出池化方法的不足，尤其是在输入中存在信号与噪声混合的情况下，标准池化方法容易因SNR波动而性能下降，因此寻找更具鲁棒性的池化方法。

Method: 提出了一种基于注意力的自适应池化方法，通过向量量化的框架最小化信号损失，理论上能够在任何SNR下近似信号最优的向量量化器。

Result: 自适应池化方法在合成数据集和实际任务（关系推理、多智能体强化学习、视觉基准测试）中均表现出更高的鲁棒性，优于标准池化方法。

Conclusion: 自适应注意力池化方法能够有效应对输入中的信号噪声问题，在各种任务中展现出更强的适应性和鲁棒性，为标准池化方法提供了改进方向。

Abstract: We investigate the design of pooling methods used to summarize the outputs of
transformer embedding models, primarily motivated by reinforcement learning and
vision applications. This work considers problems where a subset of the input
vectors contains requisite information for a downstream task (signal) while the
rest are distractors (noise). By framing pooling as vector quantization with
the goal of minimizing signal loss, we demonstrate that the standard methods
used to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are
vulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs
fluctuates. We then show that an attention-based adaptive pooling method can
approximate the signal-optimal vector quantizer within derived error bounds for
any SNR. Our theoretical results are first validated by supervised experiments
on a synthetic dataset designed to isolate the SNR problem, then generalized to
standard relational reasoning, multi-agent reinforcement learning, and vision
benchmarks with noisy observations, where transformers with adaptive pooling
display superior robustness across tasks.

</details>


### [95] [SoK: Machine Unlearning for Large Language Models](https://arxiv.org/abs/2506.09227)
*Jie Ren,Yue Xing,Yingqian Cui,Charu C. Aggarwal,Hui Liu*

Key words: 大语言模型,遗忘技术,意图导向分类,知识移除,行为抑制,评估策略,实际挑战

TL;DR: 论文提出了一种基于意图导向的新分类法来系统化LLM遗忘技术，探讨了真正遗忘是否必要或可行，评估了现有评估策略的局限性，并指出了实际应用中的挑战。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究LLM遗忘技术的根本意图（真正移除知识还是仅抑制行为影响），以填补现有方法分类忽视的这一维度。

Method: 提出基于意图的新分类法，分析遗忘方法的实际效果（移除vs抑制），调查评估策略的局限性，并探讨实际挑战。

Result: 发现许多遗忘方法可能只是抑制而非真正移除知识，评估指标和基准存在不足，实际应用中面临可扩展性和顺序遗忘等挑战。

Conclusion: 论文提供了一个全面的框架来理解和推进生成式AI中的遗忘技术，为未来研究和数据隐私政策提供了指导。

Abstract: Large language model (LLM) unlearning has become a critical topic in machine
learning, aiming to eliminate the influence of specific training data or
knowledge without retraining the model from scratch. A variety of techniques
have been proposed, including Gradient Ascent, model editing, and re-steering
hidden representations. While existing surveys often organize these methods by
their technical characteristics, such classifications tend to overlook a more
fundamental dimension: the underlying intention of unlearning--whether it seeks
to truly remove internal knowledge or merely suppress its behavioral effects.
In this SoK paper, we propose a new taxonomy based on this intention-oriented
perspective. Building on this taxonomy, we make three key contributions. First,
we revisit recent findings suggesting that many removal methods may
functionally behave like suppression, and explore whether true removal is
necessary or achievable. Second, we survey existing evaluation strategies,
identify limitations in current metrics and benchmarks, and suggest directions
for developing more reliable and intention-aligned evaluations. Third, we
highlight practical challenges--such as scalability and support for sequential
unlearning--that currently hinder the broader deployment of unlearning methods.
In summary, this work offers a comprehensive framework for understanding and
advancing unlearning in generative AI, aiming to support future research and
guide policy decisions around data removal and privacy.

</details>


### [96] [Agent-based Condition Monitoring Assistance with Multimodal Industrial Database Retrieval Augmented Generation](https://arxiv.org/abs/2506.09247)
*Karl Löwenmark,Daniel Strömbergsson,Chang Liu,Marcus Liwicki,Fredrik Sandin*

Key words: 条件监控, 大型语言模型, 多模态检索增强生成, 决策支持, 工业应用

TL;DR: 该论文提出了一种结合大型语言模型（LLM）与条件监控（CM）工作流程的MindRAG框架，旨在减少误报、改进故障严重性估计，并提供可解释的决策支持。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前计算机化维护系统在故障检测和分类方面表现良好，但在故障严重性估计和维护决策方面依赖人工专家，且存在高误报率和工作效率低下的问题。

Method: 论文提出MindRAG框架，结合多模态检索增强生成（RAG）与专为CM数据设计的向量存储结构，利用现有注释和维护工单作为监督学习的标签替代。

Result: 初步结果表明，MindRAG能有效提供决策支持，提高报警管理的效率，并增强CM系统的可解释性。

Conclusion: MindRAG通过LLM驱动的推理代理和多模态RAG技术，显著提升了CM系统的自动化决策能力和实用性。

Abstract: Condition monitoring (CM) plays a crucial role in ensuring reliability and
efficiency in the process industry. Although computerised maintenance systems
effectively detect and classify faults, tasks like fault severity estimation,
and maintenance decisions still largely depend on human expert analysis. The
analysis and decision making automatically performed by current systems
typically exhibit considerable uncertainty and high false alarm rates, leading
to increased workload and reduced efficiency.
  This work integrates large language model (LLM)-based reasoning agents with
CM workflows to address analyst and industry needs, namely reducing false
alarms, enhancing fault severity estimation, improving decision support, and
offering explainable interfaces. We propose MindRAG, a modular framework
combining multimodal retrieval-augmented generation (RAG) with novel vector
store structures designed specifically for CM data. The framework leverages
existing annotations and maintenance work orders as surrogates for labels in a
supervised learning protocol, addressing the common challenge of training
predictive models on unlabelled and noisy real-world datasets.
  The primary contributions include: (1) an approach for structuring industry
CM data into a semi-structured multimodal vector store compatible with
LLM-driven workflows; (2) developing multimodal RAG techniques tailored for CM
data; (3) developing practical reasoning agents capable of addressing
real-world CM queries; and (4) presenting an experimental framework for
integrating and evaluating such agents in realistic industrial scenarios.
Preliminary results, evaluated with the help of an experienced analyst,
indicate that MindRAG provide meaningful decision support for more efficient
management of alarms, thereby improving the interpretability of CM systems.

</details>


### [97] [CFMI: Flow Matching for Missing Data Imputation](https://arxiv.org/abs/2506.09258)
*Vaidotas Simkus,Michael U. Gutmann*

Key words: CFMI, 缺失数据插补, 连续归一化流, 流匹配, 共享条件建模

TL;DR: CFMI是一种新的通用缺失数据插补方法，结合了连续归一化流、流匹配和共享条件建模，优于传统和现代技术。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统多重插补方法存在复杂性高的问题，CFMI旨在提供一种高效且通用的解决方案。

Method: 采用连续归一化流、流匹配和共享条件建模技术。

Result: 在24个小到中等规模数据集上表现优于9种传统和现代方法，且在时间序列数据中计算效率更高。

Conclusion: CFMI在低维和高维数据中均表现优异，成为广泛适用的插补方法。

Abstract: We introduce conditional flow matching for imputation (CFMI), a new
general-purpose method to impute missing data. The method combines continuous
normalising flows, flow-matching, and shared conditional modelling to deal with
intractabilities of traditional multiple imputation. Our comparison with nine
classical and state-of-the-art imputation methods on 24 small to
moderate-dimensional tabular data sets shows that CFMI matches or outperforms
both traditional and modern techniques across a wide range of metrics. Applying
the method to zero-shot imputation of time-series data, we find that it matches
the accuracy of a related diffusion-based method while outperforming it in
terms of computational efficiency. Overall, CFMI performs at least as well as
traditional methods on lower-dimensional data while remaining scalable to
high-dimensional settings, matching or exceeding the performance of other deep
learning-based approaches, making it a go-to imputation method for a wide range
of data types and dimensionalities.

</details>


### [98] [Uncertainty Prioritized Experience Replay](https://arxiv.org/abs/2506.09270)
*Rodrigo Carrasco-Davis,Sebastian Lee,Claudia Clopath,Will Dabney*

Key words: 强化学习,经验回放,认知不确定性,噪声干扰

TL;DR: 提出一种基于认知不确定性估计的经验回放优先选择方法，以减少噪声对价值估计的干扰，提升强化学习效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统基于时间差分误差的优先级选择易受噪声影响，本文旨在通过认知不确定性估计改善这一问题。

Method: 使用认知不确定性估计从回放缓冲区中优先选择过渡样本，避免噪声干扰。

Result: 在表格模型和Atari游戏中表现优异，超越了现有基准。

Conclusion: 认知不确定性优先回放为强化学习提供了一种有效的新方法。

Abstract: Prioritized experience replay, which improves sample efficiency by selecting
relevant transitions to update parameter estimates, is a crucial component of
contemporary value-based deep reinforcement learning models. Typically,
transitions are prioritized based on their temporal difference error. However,
this approach is prone to favoring noisy transitions, even when the value
estimation closely approximates the target mean. This phenomenon resembles the
noisy TV problem postulated in the exploration literature, in which
exploration-guided agents get stuck by mistaking noise for novelty. To mitigate
the disruptive effects of noise in value estimation, we propose using epistemic
uncertainty estimation to guide the prioritization of transitions from the
replay buffer. Epistemic uncertainty quantifies the uncertainty that can be
reduced by learning, hence reducing transitions sampled from the buffer
generated by unpredictable random processes. We first illustrate the benefits
of epistemic uncertainty prioritized replay in two tabular toy models: a simple
multi-arm bandit task, and a noisy gridworld. Subsequently, we evaluate our
prioritization scheme on the Atari suite, outperforming quantile regression
deep Q-learning benchmarks; thus forging a path for the use of uncertainty
prioritized replay in reinforcement learning agents.

</details>


### [99] [G-Sim: Generative Simulations with Large Language Models and Gradient-Free Calibration](https://arxiv.org/abs/2506.09272)
*Samuel Holt,Max Ruiz Luyten,Antonin Berthon,Mihaela van der Schaar*

Key words: 模拟器, LLM, 因果推断, 参数校准, 决策支持

TL;DR: G-Sim是一个混合框架，结合了LLM驱动的结构设计和严格的实证校准，以构建可靠的模拟器。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 构建强大的模拟器对于回答“假设”问题和指导关键领域的政策至关重要，但现有方法在泛化性和准确性方面表现不佳。

Method: G-Sim利用LLM在迭代循环中提出和优化模拟器的核心组件和因果关系，然后通过灵活的校准技术（如无梯度和无似然方法）对参数进行估计。

Result: G-Sim能够生成可靠且因果相关的模拟器，缓解数据效率问题，并为复杂决策提供系统级干预支持。

Conclusion: G-Sim通过结合领域先验和实证证据，在非可微分和随机模拟器中展现出强大的适应性和实用性。

Abstract: Constructing robust simulators is essential for asking "what if?" questions
and guiding policy in critical domains like healthcare and logistics. However,
existing methods often struggle, either failing to generalize beyond historical
data or, when using Large Language Models (LLMs), suffering from inaccuracies
and poor empirical alignment. We introduce G-Sim, a hybrid framework that
automates simulator construction by synergizing LLM-driven structural design
with rigorous empirical calibration. G-Sim employs an LLM in an iterative loop
to propose and refine a simulator's core components and causal relationships,
guided by domain knowledge. This structure is then grounded in reality by
estimating its parameters using flexible calibration techniques. Specifically,
G-Sim can leverage methods that are both likelihood-free and gradient-free with
respect to the simulator, such as gradient-free optimization for direct
parameter estimation or simulation-based inference for obtaining a posterior
distribution over parameters. This allows it to handle non-differentiable and
stochastic simulators. By integrating domain priors with empirical evidence,
G-Sim produces reliable, causally-informed simulators, mitigating
data-inefficiency and enabling robust system-level interventions for complex
decision-making.

</details>


### [100] [Learning The Minimum Action Distance](https://arxiv.org/abs/2506.09276)
*Lorenzo Steccanella,Joshua B. Evans,Özgür Şimşek,Anders Jonsson*

Key words: 马尔可夫决策过程, 状态表示, 最小动作距离, 无监督学习, 强化学习

TL;DR: 本文提出了一种用于马尔可夫决策过程的状态表示框架，仅需状态轨迹即可学习，无需奖励信号或动作信息。最小动作距离（MAD）被作为环境结构的基础度量，支持目标导向强化学习等任务。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的状态表示方法通常依赖奖励信号或动作信息，限制了其适用范围。本文旨在通过无监督学习捕捉环境结构，提出一种更通用的表示框架。

Method: 通过学习最小动作距离（MAD），构建嵌入空间，使嵌入距离对应MAD。该方法适应对称和非对称近似，适用于多种环境类型。

Result: 实验表明，该方法能在不同类型环境中高效学习准确的MAD表示，并在表示质量上显著优于现有方法。

Conclusion: MAD为无监督状态表示提供了有效度量，支持多种下游任务，具有广泛适用性。

Abstract: This paper presents a state representation framework for Markov decision
processes (MDPs) that can be learned solely from state trajectories, requiring
neither reward signals nor the actions executed by the agent. We propose
learning the minimum action distance (MAD), defined as the minimum number of
actions required to transition between states, as a fundamental metric that
captures the underlying structure of an environment. MAD naturally enables
critical downstream tasks such as goal-conditioned reinforcement learning and
reward shaping by providing a dense, geometrically meaningful measure of
progress. Our self-supervised learning approach constructs an embedding space
where the distances between embedded state pairs correspond to their MAD,
accommodating both symmetric and asymmetric approximations. We evaluate the
framework on a comprehensive suite of environments with known MAD values,
encompassing both deterministic and stochastic dynamics, as well as discrete
and continuous state spaces, and environments with noisy observations.
Empirical results demonstrate that the proposed approach not only efficiently
learns accurate MAD representations across these diverse settings but also
significantly outperforms existing state representation methods in terms of
representation quality.

</details>


### [101] [A Topic Modeling Analysis of Stigma Dimensions, Social, and Related Behavioral Circumstances in Clinical Notes Among Patients with HIV](https://arxiv.org/abs/2506.09279)
*Ziyi Chen,Yiyang Liu,Mattia Prosperi,Krishna Vaddiparti,Robert L Cook,Jiang Bian,Yi Guo,Yonghui Wu*

Key words: HIV耻辱感, 自然语言处理, 电子健康记录, 主题建模, 社会行为

TL;DR: 该研究利用自然语言处理技术分析电子健康记录，揭示HIV感染者面临的耻辱感、社会及行为问题，并发现不同年龄组的主题差异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统问卷调查在评估HIV相关耻辱感时效率低，研究者希望通过电子健康记录的文本分析实现高效、可扩展的评估。

Method: 使用潜在狄利克雷分配（LDA）进行主题建模，通过关键词筛选和领域专家人工审核，识别与HIV耻辱感相关的主题，并对不同年龄和性别亚组进行主题差异分析。

Result: 研究发现多个与HIV相关的主题，如心理健康问题、社会支持、医疗资源限制等，且不同年龄组间存在差异。

Conclusion: 通过电子健康记录分析HIV相关耻辱感和社会行为问题，可克服传统方法局限，提升患者护理效果。

Abstract: Objective: To characterize stigma dimensions, social, and related behavioral
circumstances in people living with HIV (PLWHs) seeking care, using natural
language processing methods applied to a large collection of electronic health
record (EHR) clinical notes from a large integrated health system in the
southeast United States. Methods: We identified 9,140 cohort of PLWHs from the
UF Health IDR and performed topic modeling analysis using Latent Dirichlet
Allocation (LDA) to uncover stigma dimensions, social, and related behavioral
circumstances. Domain experts created a seed list of HIV-related stigma
keywords, then applied a snowball strategy to iteratively review notes for
additional terms until saturation was reached. To identify more target topics,
we tested three keyword-based filtering strategies. Domain experts manually
reviewed the detected topics using the prevalent terms and key discussion
topics. Word frequency analysis was used to highlight the prevalent terms
associated with each topic. In addition, we conducted topic variation analysis
among subgroups to examine differences across age and sex-specific
demographics. Results and Conclusion: Topic modeling on sentences containing at
least one keyword uncovered a wide range of topic themes associated with
HIV-related stigma, social, and related behaviors circumstances, including
"Mental Health Concern and Stigma", "Social Support and Engagement", "Limited
Healthcare Access and Severe Illness", "Treatment Refusal and Isolation" and so
on. Topic variation analysis across age subgroups revealed differences.
Extracting and understanding the HIV-related stigma dimensions, social, and
related behavioral circumstances from EHR clinical notes enables scalable,
time-efficient assessment, overcoming the limitations of traditional
questionnaires and improving patient outcomes.

</details>


### [102] [Causal Graph Recovery in Neuroimaging through Answer Set Programming](https://arxiv.org/abs/2506.09286)
*Mohammadsajad Abavisani,Kseniya Solovyeva,David Danks,Vince Calhoun,Sergey Plis*

Key words: 因果图、时间序列、子采样、答案集编程、图论、约束优化

TL;DR: 提出了一种基于答案集编程（ASP）的方法，用于从子采样时间序列数据中学习因果图，显著提高了准确性和效率，并在模拟和实证数据中验证了其优越性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决从时间序列数据中学习因果图时由于子采样导致的信息丢失问题，提高因果图推断的准确性和鲁棒性。

Method: 使用约束优化方法（答案集编程，ASP）推断最可能的因果图，并利用图论进一步优化结果。

Result: 在模拟数据和实证脑结构连接数据上验证了方法的优越性，F1分数平均提高12%，并在子采样时间序列的因果图重建中达到先进水平。

Conclusion: 该方法能够有效克服子采样的影响，提供更精准的因果图推断，且对不同子采样率表现出更强的鲁棒性。

Abstract: Learning graphical causal structures from time series data presents
significant challenges, especially when the measurement frequency does not
match the causal timescale of the system. This often leads to a set of equally
possible underlying causal graphs due to information loss from sub-sampling
(i.e., not observing all possible states of the system throughout time). Our
research addresses this challenge by incorporating the effects of sub-sampling
in the derivation of causal graphs, resulting in more accurate and intuitive
outcomes. We use a constraint optimization approach, specifically answer set
programming (ASP), to find the optimal set of answers. ASP not only identifies
the most probable underlying graph, but also provides an equivalence class of
possible graphs for expert selection. In addition, using ASP allows us to
leverage graph theory to further prune the set of possible solutions, yielding
a smaller, more accurate answer set significantly faster than traditional
approaches. We validate our approach on both simulated data and empirical
structural brain connectivity, and demonstrate its superiority over established
methods in these experiments. We further show how our method can be used as a
meta-approach on top of established methods to obtain, on average, 12%
improvement in F1 score. In addition, we achieved state of the art results in
terms of precision and recall of reconstructing causal graph from sub-sampled
time series data. Finally, our method shows robustness to varying degrees of
sub-sampling on realistic simulations, whereas other methods perform worse for
higher rates of sub-sampling.

</details>


### [103] [On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear Attention](https://arxiv.org/abs/2506.09316)
*Yeonju Ro,Zhenyu Zhang,Souvik Kundu,Zhangyang Wang,Aditya Akella*

Key words: 大型语言模型,线性注意力,双状态,自适应蒸馏,推理效率

TL;DR: 论文提出了一种双状态线性注意力（DSLA）和自适应蒸馏框架（Serve），在保持性能的同时显著提升了长文本推理效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大型语言模型在处理长文本时面临计算和内存成本高的问题，现有方法如线性注意力虽能降低成本但会牺牲准确性。

Method: 提出DSLA（双状态线性注意力）以平衡历史与最新上下文的依赖关系，并开发Serve框架动态替代Transformer层。

Result: 在多项任务测试中，Serve比基准模型快2.3-3.0倍且性能相当，DSLA能有效捕捉全局与局部依赖。

Conclusion: DSLA和Serve在效率和性能上取得了平衡，解决了线性注意力模型的历史依赖不足问题。

Abstract: Large language models (LLMs) excel at capturing global token dependencies via
self-attention but face prohibitive compute and memory costs on lengthy inputs.
While sub-quadratic methods (e.g., linear attention) can reduce these costs,
they often degrade accuracy due to overemphasizing recent tokens. In this work,
we first propose \textit{dual-state linear attention} (\textbf{\dsla}), a novel
design that maintains two specialized hidden states-one for preserving
historical context and one for tracking recency-thereby mitigating the
short-range bias typical of linear-attention architectures. To further balance
efficiency and accuracy under dynamic workload conditions, we introduce
\textbf{\serve}, an online \textit{adaptive distillation} framework that
progressively replaces Transformer layers with DSLA layers at inference time,
guided by a sensitivity-based layer ordering. \serve\ uses a chained
fine-tuning strategy to ensure that each newly converted DSLA layer remains
consistent with previously replaced layers, preserving the overall quality.
Extensive evaluations on commonsense reasoning, long-context QA, and text
summarization demonstrate that \serve\ yields \textbf{2.3x} faster inference
than Llama2-7B and \textbf{3.0x} faster than the hybrid Zamba-7B, while
retaining comparable performance across downstream tasks. Our ablation studies
show that DSLA's dual states capture both global and local dependencies,
addressing the historical-token underrepresentation seen in prior linear
attentions. Codes are available at https://github.com/utnslab/DSLA-Serve.

</details>


### [104] [Natural Language Guided Ligand-Binding Protein Design](https://arxiv.org/abs/2506.09332)
*Zhenqiao Song,Ramith Hettiarachchi,Chuan Li,Jianwen Xie,Lei Li*

Key words: 蛋白质设计, 自然语言指令, 配体结合, InstructPro, AI模型

TL;DR: InstructPro是一个基于自然语言指令设计蛋白质的生成模型，能够通过文本描述和配体公式生成功能一致的蛋白质序列，性能优于现有基线模型。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于蛋白质-配体复合物数据稀缺，而人类标注的文本描述丰富，研究团队提出InstructPro模型，利用自然语言指令生成功能性蛋白质。

Method: 提出InstructPro模型架构和训练策略，并构建大规模数据集InstructProBench。模型包括1B和3B两个变体，均通过文本描述和配体公式生成蛋白质序列。

Result: InstructPro-1B和InstructPro-3B在对接成功率和RMSD上表现优异，其中3B变体平均RMSD低至2.527Å。

Conclusion: InstructPro能够高效生成符合功能要求的配体结合蛋白质，性能显著优于现有方法。

Abstract: Can AI protein models follow human language instructions and design proteins
with desired functions (e.g. binding to a ligand)? Designing proteins that bind
to a given ligand is crucial in a wide range of applications in biology and
chemistry. Most prior AI models are trained on protein-ligand complex data,
which is scarce due to the high cost and time requirements of laboratory
experiments. In contrast, there is a substantial body of human-curated text
descriptions about protein-ligand interactions and ligand formula. In this
paper, we propose InstructPro, a family of protein generative models that
follow natural language instructions to design ligand-binding proteins. Given a
textual description of the desired function and a ligand formula in SMILES,
InstructPro generates protein sequences that are functionally consistent with
the specified instructions. We develop the model architecture, training
strategy, and a large-scale dataset, InstructProBench, to support both training
and evaluation. InstructProBench consists of 9,592,829 triples of (function
description, ligand formula, protein sequence). We train two model variants:
InstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion
parameters). Both variants consistently outperform strong baselines, including
ProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking
success rate (81.52% at moderate confidence) and the lowest average root mean
square deviation (RMSD) compared to ground truth structures (4.026{\AA}).
InstructPro-3B further descreases the average RMSD to 2.527{\AA}, demonstrating
InstructPro's ability to generate ligand-binding proteins that align with the
functional specifications.

</details>


### [105] [ErrorEraser: Unlearning Data Bias for Improved Continual Learning](https://arxiv.org/abs/2506.09347)
*Xuemei Cao,Hanlin Gu,Xin Yang,Bingjun Wei,Haoyang Liang,Xiangkun Wang,Tianrui Li*

Key words: Continual Learning, Data Bias, ErrorEraser, Intentional Forgetting

TL;DR: 本文提出了一种新的持续学习方法ErrorEraser，不仅防止遗忘，还通过消除数据偏差带来的错误记忆提升性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统持续学习方法忽略了真实数据中的偏差，导致模型学习虚假相关性。为了解决这一问题，作者提出了Intentional Forgetting的概念。

Method: ErrorEraser由错误识别和错误擦除两个模块组成。前者在无先验知识的情况下识别潜在偏差样本，后者通过调整决策空间消除错误知识。

Result: 实验表明，ErrorEraser显著减少了数据偏差的负面影响，在三种持续学习方法中实现了更高的准确率和更低的遗忘率。

Conclusion: ErrorEraser是一种通用插件，能够有效提升持续学习的性能。

Abstract: Continual Learning (CL) primarily aims to retain knowledge to prevent
catastrophic forgetting and transfer knowledge to facilitate learning new
tasks. Unlike traditional methods, we propose a novel perspective: CL not only
needs to prevent forgetting, but also requires intentional forgetting.This
arises from existing CL methods ignoring biases in real-world data, leading the
model to learn spurious correlations that transfer and amplify across tasks.
From feature extraction and prediction results, we find that data biases
simultaneously reduce CL's ability to retain and transfer knowledge. To address
this, we propose ErrorEraser, a universal plugin that removes erroneous
memories caused by biases in CL, enhancing performance in both new and old
tasks. ErrorEraser consists of two modules: Error Identification and Error
Erasure. The former learns the probability density distribution of task data in
the feature space without prior knowledge, enabling accurate identification of
potentially biased samples. The latter ensures only erroneous knowledge is
erased by shifting the decision space of representative outlier samples.
Additionally, an incremental feature distribution learning strategy is designed
to reduce the resource overhead during error identification in downstream
tasks. Extensive experimental results show that ErrorEraser significantly
mitigates the negative impact of data biases, achieving higher accuracy and
lower forgetting rates across three types of CL methods. The code is available
at https://github.com/diadai/ErrorEraser.

</details>


### [106] [Adversarial Surrogate Risk Bounds for Binary Classification](https://arxiv.org/abs/2506.09348)
*Natalie S. Frank*

Key words: 对抗训练,分类风险,代理风险,收敛速率,分布相关界限

TL;DR: 论文研究了对抗训练中分类风险的收敛速率，提出了代理风险界限并推导了标准学习场景中的分布相关代理风险界限。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 对抗训练的鲁棒性分类器研究中，现有工作仅关注对抗一致性，未量化对抗分类风险的收敛速率。

Method: 通过提出代理风险界限来量化对抗分类风险的最优值收敛速率，并推导标准学习场景中的分布相关界限。

Result: 论文提供了对抗分类风险的代理风险界限，并在标准学习中提出了分布相关的界限。

Conclusion: 研究填补了对抗训练中分类风险收敛速率的理论空白，同时扩展了标准学习场景的界限。

Abstract: A central concern in classification is the vulnerability of machine learning
models to adversarial attacks. Adversarial training is one of the most popular
techniques for training robust classifiers, which involves minimizing an
adversarial surrogate risk. Recent work characterized when a minimizing
sequence of an adversarial surrogate risk is also a minimizing sequence of the
adversarial classification risk for binary classification -- a property known
as adversarial consistency. However, these results do not address the rate at
which the adversarial classification risk converges to its optimal value for
such a sequence of functions that minimize the adversarial surrogate. This
paper provides surrogate risk bounds that quantify that convergence rate.
Additionally, we derive distribution-dependent surrogate risk bounds in the
standard (non-adversarial) learning setting, that may be of independent
interest.

</details>


### [107] [Anomaly Detection and Generation with Diffusion Models: A Survey](https://arxiv.org/abs/2506.09368)
*Yang Liu,Jing Liu,Chengfang Li,Rui Xi,Wenchao Li,Liang Cao,Jin Wang,Laurence T. Yang,Junsong Yuan,Wei Zhou*

Key words: 异常检测,扩散模型,无监督学习,数据生成

TL;DR: 本文综述了扩散模型在异常检测和生成中的应用，强调了二者的协同关系，并提出了未来研究方向。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 异常检测在各领域至关重要，扩散模型因其强大的数据分布学习能力为无监督异常检测提供了新框架。

Method: 综述结合教程式分析，对扩散模型的理论基础和实践应用进行分类，涵盖多种数据类型和评分机制。

Result: 揭示了异常检测与生成技术的协同作用，提供了详细的分类评估，总结了现有方法的优缺点。

Conclusion: 扩散模型为异常检测带来创新潜力，未来需解决可扩展性和计算效率等挑战。

Abstract: Anomaly detection (AD) plays a pivotal role across diverse domains, including
cybersecurity, finance, healthcare, and industrial manufacturing, by
identifying unexpected patterns that deviate from established norms in
real-world data. Recent advancements in deep learning, specifically diffusion
models (DMs), have sparked significant interest due to their ability to learn
complex data distributions and generate high-fidelity samples, offering a
robust framework for unsupervised AD. In this survey, we comprehensively review
anomaly detection and generation with diffusion models (ADGDM), presenting a
tutorial-style analysis of the theoretical foundations and practical
implementations and spanning images, videos, time series, tabular, and
multimodal data. Crucially, unlike existing surveys that often treat anomaly
detection and generation as separate problems, we highlight their inherent
synergistic relationship. We reveal how DMs enable a reinforcing cycle where
generation techniques directly address the fundamental challenge of anomaly
data scarcity, while detection methods provide critical feedback to improve
generation fidelity and relevance, advancing both capabilities beyond their
individual potential. A detailed taxonomy categorizes ADGDM methods based on
anomaly scoring mechanisms, conditioning strategies, and architectural designs,
analyzing their strengths and limitations. We final discuss key challenges
including scalability and computational efficiency, and outline promising
future directions such as efficient architectures, conditioning strategies, and
integration with foundation models (e.g., visual-language models and large
language models). By synthesizing recent advances and outlining open research
questions, this survey aims to guide researchers and practitioners in
leveraging DMs for innovative AD solutions across diverse applications.

</details>


### [108] [LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization](https://arxiv.org/abs/2506.09373)
*Jiaqi Tang,Yu Xia,Yi-Feng Wu,Yuwei Hu,Yuhui Chen,Qing-Guo Chen,Xiaogang Xu,Xiangyu Wu,Hao Lu,Yanqing Ma,Shiyin Lu,Qifeng Chen*

Key words: GUI代理, 位置优化, 信息熵, 动态奖励, GRPO

TL;DR: 本文提出了一种名为LPO的新方法，通过利用位置数据和信息熵优化GUI交互，显著提高了交互精度，并在实验中取得了SOTA结果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的GUI代理中的空间定位方法（如监督微调和强化学习）在位置数据感知和评估方面存在局限性，影响了交互的准确性和实用性。

Method: 提出了Location Preference Optimization (LPO)，利用信息熵预测信息丰富区域的交互位置，并引入动态位置奖励函数和Group Relative Preference Optimization (GRPO)来优化交互偏好。

Result: LPO在离线和在线评估中均表现出色，实现了最先进的性能。

Conclusion: LPO通过优化位置数据和动态奖励，显著提升了GUI交互的精度和效果。

Abstract: The advent of autonomous agents is transforming interactions with Graphical
User Interfaces (GUIs) by employing natural language as a powerful
intermediary. Despite the predominance of Supervised Fine-Tuning (SFT) methods
in current GUI agents for achieving spatial localization, these methods face
substantial challenges due to their limited capacity to accurately perceive
positional data. Existing strategies, such as reinforcement learning, often
fail to assess positional accuracy effectively, thereby restricting their
utility. In response, we introduce Location Preference Optimization (LPO), a
novel approach that leverages locational data to optimize interaction
preferences. LPO uses information entropy to predict interaction positions by
focusing on zones rich in information. Besides, it further introduces a dynamic
location reward function based on physical distance, reflecting the varying
importance of interaction positions. Supported by Group Relative Preference
Optimization (GRPO), LPO facilitates an extensive exploration of GUI
environments and significantly enhances interaction precision. Comprehensive
experiments demonstrate LPO's superior performance, achieving SOTA results
across both offline benchmarks and real-world online evaluations. Our code will
be made publicly available soon, at https://github.com/AIDC-AI/LPO.

</details>


### [109] [Revisiting Diffusion Models: From Generative Pre-training to One-Step Generation](https://arxiv.org/abs/2506.09376)
*Bowen Zheng,Tianming Yang*

Key words: 扩散模型、蒸馏、GAN、一步生成、预训练

TL;DR: 研究发现扩散蒸馏技术的局限性，提出通过轻量级GAN微调预训练扩散模型，实现高效一步生成。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 扩散蒸馏技术虽普遍应用于降低扩散模型采样成本，但训练耗时且性能下降。研究旨在探索如何解决这一问题。

Method: 通过分析扩散蒸馏的局限性，提出使用GAN目标取代蒸馏损失，并对预训练扩散模型进行轻量级GAN微调。

Result: 使用仅0.2M图像微调预训练模型，性能强劲；5M图像时达到接近SOTA效果。频率域分析解释了扩散训练中的一步生成能力。

Conclusion: 扩散训练可视为生成式预训练，为构建高效一步生成模型提供新视角。

Abstract: Diffusion distillation is a widely used technique to reduce the sampling cost
of diffusion models, yet it often requires extensive training, and the student
performance tends to be degraded. Recent studies show that incorporating a GAN
objective may alleviate these issues, yet the underlying mechanism remains
unclear. In this work, we first identify a key limitation of distillation:
mismatched step sizes and parameter numbers between the teacher and the student
model lead them to converge to different local minima, rendering direct
imitation suboptimal. We further demonstrate that a standalone GAN objective,
without relying a distillation loss, overcomes this limitation and is
sufficient to convert diffusion models into efficient one-step generators.
Based on this finding, we propose that diffusion training may be viewed as a
form of generative pre-training, equipping models with capabilities that can be
unlocked through lightweight GAN fine-tuning. Supporting this view, we create a
one-step generation model by fine-tuning a pre-trained model with 85% of
parameters frozen, achieving strong performance with only 0.2M images and
near-SOTA results with 5M images. We further present a frequency-domain
analysis that may explain the one-step generative capability gained in
diffusion training. Overall, our work provides a new perspective for diffusion
training, highlighting its role as a powerful generative pre-training process,
which can be the basis for building efficient one-step generation models.

</details>


### [110] [Efficient Prediction of SO(3)-Equivariant Hamiltonian Matrices via SO(2) Local Frames](https://arxiv.org/abs/2506.09398)
*Haiyang Yu,Yuchao Lin,Xuan Zhang,Xiaofeng Qian,Shuiwang Ji*

Key words: 哈密顿矩阵, SO(2)等价性, QHNetV2, 电子结构计算, 分子动力学

TL;DR: 提出了一种名为QHNetV2的高效网络，通过引入SO(2)等价操作，在SO(2)局部框架中实现全局SO(3)等价性，无需昂贵的SO(3)张量积，显著加速电子结构计算。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 电子结构计算在物理、化学和材料科学中至关重要，但传统方法计算成本高。本文旨在通过预测哈密顿矩阵来加速这一过程。

Method: 提出QHNetV2网络，利用SO(2)局部框架中的高效操作实现SO(3)等价性，避免SO(3)张量积，并通过SO(2)张量积融合节点特征。

Result: 在QH9和MD17数据集上的实验表明，该模型在多种分子结构和轨迹上表现卓越，具有强泛化能力。

Conclusion: SO(2)局部框架中的操作为可扩展且对称感知的电子结构学习提供了新方向。

Abstract: We consider the task of predicting Hamiltonian matrices to accelerate
electronic structure calculations, which plays an important role in physics,
chemistry, and materials science. Motivated by the inherent relationship
between the off-diagonal blocks of the Hamiltonian matrix and the SO(2) local
frame, we propose a novel and efficient network, called QHNetV2, that achieves
global SO(3) equivariance without the costly SO(3) Clebsch-Gordan tensor
products. This is achieved by introducing a set of new efficient and powerful
SO(2)-equivariant operations and performing all off-diagonal feature updates
and message passing within SO(2) local frames, thereby eliminating the need of
SO(3) tensor products. Moreover, a continuous SO(2) tensor product is performed
within the SO(2) local frame at each node to fuse node features, mimicking the
symmetric contraction operation. Extensive experiments on the large QH9 and
MD17 datasets demonstrate that our model achieves superior performance across a
wide range of molecular structures and trajectories, highlighting its strong
generalization capability. The proposed SO(2) operations on SO(2) local frames
offer a promising direction for scalable and symmetry-aware learning of
electronic structures. Our code will be released as part of the AIRS library
https://github.com/divelab/AIRS.

</details>


### [111] [Synergizing Reinforcement Learning and Genetic Algorithms for Neural Combinatorial Optimization](https://arxiv.org/abs/2506.09404)
*Shengda Gu,Kai Li,Junliang Xing,Yifan Zhang,Jian Cheng*

Key words: 组合优化、深度强化学习、遗传算法、进化增强机制、求解效率

TL;DR: 提出了一种结合深度强化学习和遗传算法的进化增强机制（EAM），显著提升了组合优化问题的求解质量和训练效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 组合优化问题因离散结构和巨大解空间而具有挑战性，现有深度强化学习方法探索能力有限且易陷入局部最优，而遗传算法虽全局搜索能力强但不高效。

Method: EAM通过从学习策略生成解，利用遗传操作（如交叉、变异）优化解，并将优化后的解重新注入策略训练循环。

Result: 在TSP、CVRP等基准问题上，EAM显著优于现有方法，提升了求解质量和训练效率。

Conclusion: EAM有效结合DRL的学习效率和遗传算法的全局搜索能力，为组合优化提供了高效稳定的新框架。

Abstract: Combinatorial optimization problems are notoriously challenging due to their
discrete structure and exponentially large solution space. Recent advances in
deep reinforcement learning (DRL) have enabled the learning heuristics directly
from data. However, DRL methods often suffer from limited exploration and
susceptibility to local optima. On the other hand, evolutionary algorithms such
as Genetic Algorithms (GAs) exhibit strong global exploration capabilities but
are typically sample inefficient and computationally intensive. In this work,
we propose the Evolutionary Augmentation Mechanism (EAM), a general and
plug-and-play framework that synergizes the learning efficiency of DRL with the
global search power of GAs. EAM operates by generating solutions from a learned
policy and refining them through domain-specific genetic operations such as
crossover and mutation. These evolved solutions are then selectively reinjected
into the policy training loop, thereby enhancing exploration and accelerating
convergence. We further provide a theoretical analysis that establishes an
upper bound on the KL divergence between the evolved solution distribution and
the policy distribution, ensuring stable and effective policy updates. EAM is
model-agnostic and can be seamlessly integrated with state-of-the-art DRL
solvers such as the Attention Model, POMO, and SymNCO. Extensive results on
benchmark problems (e.g., TSP, CVRP, PCTSP, and OP) demonstrate that EAM
significantly improves both solution quality and training efficiency over
competitive baselines.

</details>


### [112] [Mitigating Spurious Correlations in LLMs via Causality-Aware Post-Training](https://arxiv.org/abs/2506.09433)
*Shurui Gui,Shuiwang Ji*

Key words: 大语言模型, 虚假相关性, 因果关系, 后训练, 泛化能力

TL;DR: 论文提出了一种因果关系感知的后训练方法（CAPT），通过分解有偏预测为两个无偏步骤，减少大型语言模型在预训练中的虚假相关性，从而提高模型泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 尽管大语言模型在语言建模方面表现出色，但在分布外样本上常常因预训练中的虚假相关性而失败。本研究旨在通过后训练方法缓解这一问题。

Method: 采用CAPT方法，将偏置预测分解为事件估计和事件干预两个无偏步骤，避免引入额外的微调偏置。

Result: 在CLadder和PrOntoQA数据集上的实验表明，3B规模的模型经过CAPT微调后，仅用100个样本即可在分布内和分布外任务上超越传统SFT和更大规模的模型。

Conclusion: CAPT方法在提升模型泛化能力和样本效率方面具有显著效果。

Abstract: While large language models (LLMs) have demonstrated remarkable capabilities
in language modeling, recent studies reveal that they often fail on
out-of-distribution (OOD) samples due to spurious correlations acquired during
pre-training. Here, we aim to mitigate such spurious correlations through
causality-aware post-training (CAPT). By decomposing a biased prediction into
two unbiased steps, known as \textit{event estimation} and \textit{event
intervention}, we reduce LLMs' pre-training biases without incurring additional
fine-tuning biases, thus enhancing the model's generalization ability.
Experiments on the formal causal inference benchmark CLadder and the logical
reasoning dataset PrOntoQA show that 3B-scale language models fine-tuned with
CAPT can outperform both traditional SFT and larger LLMs on in-distribution
(ID) and OOD tasks using only 100 ID fine-tuning samples, demonstrating the
effectiveness and sample efficiency of CAPT.

</details>


### [113] [Generalization Error Analysis for Attack-Free and Byzantine-Resilient Decentralized Learning with Data Heterogeneity](https://arxiv.org/abs/2506.09438)
*Haoxiang Ye,Tao Sun,Qing Ling*

Key words: 去中心化学习, 泛化误差, 数据异质性, 拜占庭攻击

TL;DR: 该论文研究了去中心化学习的泛化误差，分析了数据异质性、模型初始化和随机梯度噪声等因素的影响，并探讨了拜占庭攻击对泛化误差的负面影响。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 去中心化学习的优化误差已被广泛研究，但其泛化误差却较少被探索，而泛化误差在实际应用中至关重要。

Method: 论文提出了针对异构数据的去中心化学习的细粒度泛化误差分析，包括无攻击和拜占庭弹性的情况，并进行了数值实验验证。

Result: 研究揭示了数据异质性、模型初始化和随机梯度噪声对泛化误差的影响，以及拜占庭攻击的负面影响。

Conclusion: 去中心化学习的泛化误差受多种因素影响，拜占庭攻击的负面影响与数据异质性相关，与样本量无关。

Abstract: Decentralized learning, which facilitates joint model training across
geographically scattered agents, has gained significant attention in the field
of signal and information processing in recent years. While the optimization
errors of decentralized learning algorithms have been extensively studied,
their generalization errors remain relatively under-explored. As the
generalization errors reflect the scalability of trained models on unseen data
and are crucial in determining the performance of trained models in real-world
applications, understanding the generalization errors of decentralized learning
is of paramount importance. In this paper, we present fine-grained
generalization error analysis for both attack-free and Byzantine-resilient
decentralized learning with heterogeneous data as well as under mild
assumptions, in contrast to prior studies that consider homogeneous data and/or
rely on a stringent bounded stochastic gradient assumption. Our results shed
light on the impact of data heterogeneity, model initialization and stochastic
gradient noise -- factors that have not been closely investigated before -- on
the generalization error of decentralized learning. We also reveal that
Byzantine attacks performed by malicious agents largely affect the
generalization error, and their negative impact is inherently linked to the
data heterogeneity while remaining independent on the sample size. Numerical
experiments on both convex and non-convex tasks are conducted to validate our
theoretical findings.

</details>


### [114] [Safe Screening Rules for Group SLOPE](https://arxiv.org/abs/2506.09451)
*Runxue Bao,Quanchao Lu,Yanfu Zhang*

Key words: Group SLOPE, 高维稀疏学习, 筛选规则, 计算效率, 内存优化

TL;DR: 针对Group SLOPE模型中的计算和内存问题，提出了一种安全筛选规则，能够高效识别并排除零系数的不活跃组，显著提升计算效率和内存使用。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在具有组结构的高维稀疏学习中，Group SLOPE的自适应性虽好，但其块不可分性质会导致计算和内存问题，亟需改进。

Method: 设计了一种针对Group SLOPE的安全筛选规则，有效识别不活跃组，并将其集成到现有批量与随机算法求解器中。

Result: 实验证明该筛选规则能准确识别不活跃组，显著提升计算效率且不牺牲精度。

Conclusion: 所提方法解决了Group SLOPE的块不可分性问题，显著优化了计算和内存性能。

Abstract: Variable selection is a challenging problem in high-dimensional sparse
learning, especially when group structures exist. Group SLOPE performs well for
the adaptive selection of groups of predictors. However, the block
non-separable group effects in Group SLOPE make existing methods either invalid
or inefficient. Consequently, Group SLOPE tends to incur significant
computational costs and memory usage in practical high-dimensional scenarios.
To overcome this issue, we introduce a safe screening rule tailored for the
Group SLOPE model, which efficiently identifies inactive groups with zero
coefficients by addressing the block non-separable group effects. By excluding
these inactive groups during training, we achieve considerable gains in
computational efficiency and memory usage. Importantly, the proposed screening
rule can be seamlessly integrated into existing solvers for both batch and
stochastic algorithms. Theoretically, we establish that our screening rule can
be safely employed with existing optimization algorithms, ensuring the same
results as the original approaches. Experimental results confirm that our
method effectively detects inactive feature groups and significantly boosts
computational efficiency without compromising accuracy.

</details>


### [115] [Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform](https://arxiv.org/abs/2506.09452)
*Jay Roberts,Kyle Mylonakis,Sidhartha Roy,Kaan Kale*

Key words: 隐私保护,大型语言模型,词嵌入变换,高斯混合模型,互信息

TL;DR: 本文介绍了Stained Glass Transform，一种通过学习、随机和序列依赖的变换方法，保护LLM输入数据的隐私，同时保留模型实用性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于AI计算资源的高成本和LLM服务的挑战，企业常采用托管服务或多租户计算，但这导致敏感数据以明文形式存在。本文旨在解决这一隐私问题。

Method: 提出Stained Glass Transform，对LLM的词嵌入进行变换，通过理论分析将其与高斯混合模型的互信息理论关联。

Result: 通过后验隐私估计和LLM性能基准测试，验证了变换后的嵌入在隐私和实用性上的表现。

Conclusion: Stained Glass Transform在保护输入数据隐私的同时，保持了模型性能，为敏感数据的使用提供了一种解决方案。

Abstract: The high cost of ownership of AI compute infrastructure and challenges of
robust serving of large language models (LLMs) has led to a surge in managed
Model-as-a-service deployments. Even when enterprises choose on-premises
deployments, the compute infrastructure is typically shared across many teams
in order to maximize the return on investment. In both scenarios the deployed
models operate only on plaintext data, and so enterprise data owners must allow
their data to appear in plaintext on a shared or multi-tenant compute
infrastructure. This results in data owners with private or sensitive data
being hesitant or restricted in what data they use with these types of
deployments. In this work we introduce the Stained Glass Transform, a learned,
stochastic, and sequence dependent transformation of the word embeddings of an
LLM which information theoretically provides privacy to the input of the LLM
while preserving the utility of model. We theoretically connect a particular
class of Stained Glass Transforms to the theory of mutual information of
Gaussian Mixture Models. We then calculate a-postiori privacy estimates, based
on mutual information, and verify the privacy and utility of instances of
transformed embeddings through token level metrics of privacy and standard LLM
performance benchmarks.

</details>


### [116] [NDCG-Consistent Softmax Approximation with Accelerated Convergence](https://arxiv.org/abs/2506.09454)
*Yuanhao Pu,Defu Lian,Xiaolong Chen,Xu Huang,Jin Chen,Enhong Chen*

Key words: 相似性学习, 排序任务, Softmax Loss, 泰勒展开, ALS优化

TL;DR: 针对大规模对象排序任务，论文提出了两种新的损失函数（RG²和RG^×），通过泰勒展开Softmax Loss得到，解决了其计算开销大和扩展性差的问题，并结合ALS优化方法，既保证性能又加速收敛。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Softmax Loss虽然在大规模相似性学习中表现优异，但存在计算开销大和扩展性问题，因此需要提出更高效的替代方案。

Method: 通过对Softmax Loss进行泰勒展开，提出RG²和RG^×损失函数，并结合ALS优化方法。

Result: 在真实数据集上验证，新方法在保持或超越Softmax Loss性能的同时，显著加速收敛。

Conclusion: 新提出的RG损失函数不仅提供理论支持，还为相似性学习提供了高效工具，适用于需要平衡排序质量和计算效率的场景。

Abstract: Ranking tasks constitute fundamental components of extreme similarity
learning frameworks, where extremely large corpora of objects are modeled
through relative similarity relationships adhering to predefined ordinal
structures. Among various ranking surrogates, Softmax (SM) Loss has been widely
adopted due to its natural capability to handle listwise ranking via global
negative comparisons, along with its flexibility across diverse application
scenarios. However, despite its effectiveness, SM Loss often suffers from
significant computational overhead and scalability limitations when applied to
large-scale object spaces. To address this challenge, we propose novel loss
formulations that align directly with ranking metrics: the
Ranking-Generalizable \textbf{squared} (RG$^2$) Loss and the
Ranking-Generalizable interactive (RG$^\times$) Loss, both derived through
Taylor expansions of the SM Loss. Notably, RG$^2$ reveals the intrinsic
mechanisms underlying weighted squared losses (WSL) in ranking methods and
uncovers fundamental connections between sampling-based and non-sampling-based
loss paradigms. Furthermore, we integrate the proposed RG losses with the
highly efficient Alternating Least Squares (ALS) optimization method, providing
both generalization guarantees and convergence rate analyses. Empirical
evaluations on real-world datasets demonstrate that our approach achieves
comparable or superior ranking performance relative to SM Loss, while
significantly accelerating convergence. This framework offers the similarity
learning community both theoretical insights and practically efficient tools,
with methodologies applicable to a broad range of tasks where balancing ranking
quality and computational efficiency is essential.

</details>


### [117] [On a few pitfalls in KL divergence gradient estimation for RL](https://arxiv.org/abs/2506.09477)
*Yunhao Tang,Rémi Munos*

Key words: KL散度,梯度估计,强化学习,LLM

TL;DR: 指出在RL训练中KL散度梯度估计的几个常见错误，并提出正确实现方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对开源源代码和论文中KL散度梯度估计的不正确实现问题，探讨其影响并提供解决方案。

Method: 分析两种主要误区：将KL估计作为损失函数直接微分，以及忽视估计问题的序列性。通过实验展示问题影响，并提出正确实现方法。

Result: 实验证明错误实现无法得到所需KL梯度，而正确方法能有效解决这一问题。

Conclusion: 指出了KL散度梯度估计的正确实现方式，为未来研究提供了技术指导。

Abstract: We point out a few pitfalls in implementing gradient estimation for KL
divergence in RL training for LLM, as seen in a number of open source projects
and papers. The first major pitfall is to differentiate through the KL estimate
as loss functions to minimize KL divergence. We show that such implementations
are generally incorrect and do not produce the desired KL gradient. Secondly,
we show that some implementations do not account for the sequential nature of
the estimation problem and produce a partial gradient at best. We demonstrate
the impact of such issues with illustrative tabular and LLM experiments, and
show the correct way to implement the KL gradient.

</details>


### [118] [EnerBridge-DPO: Energy-Guided Protein Inverse Folding with Markov Bridges and Direct Preference Optimization](https://arxiv.org/abs/2506.09496)
*Dingyi Rong,Haotian Lu,Wenzhuo Zheng,Fan Zhang,Shuangjia Zheng,Ning Liu*

Key words: 蛋白质逆折叠,低能量序列,Markov Bridges,Direct Preference Optimization,能量约束

TL;DR: EnerBridge-DPO是一种新型蛋白质逆折叠框架，通过结合Markov Bridges与Direct Preference Optimization(DPO)，直接生成低能量、高稳定性的蛋白质序列，克服了现有方法忽视序列能量的局限性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前深度学习方法在蛋白质逆折叠中主要关注序列恢复率，而忽略了生成序列的能量稳定性。本研究旨在开发一种直接生成低能量、稳定蛋白质序列的模型。

Method: 提出EnerBridge-DPO框架，整合Markov Bridges与DPO，利用能量偏好微调模型，并引入显式能量约束损失，从先验知识中学习能量表示并预测序列能量值。

Result: EnerBridge-DPO能设计出能量更低且序列恢复率与前沿模型相当的蛋白质序列，并能准确预测不同序列间的$ΔΔG$值。

Conclusion: EnerBridge-DPO在蛋白质序列设计中表现出色，兼具低能量和高稳定性，为蛋白质逆折叠领域提供了新思路。

Abstract: Designing protein sequences with optimal energetic stability is a key
challenge in protein inverse folding, as current deep learning methods are
primarily trained by maximizing sequence recovery rates, often neglecting the
energy of the generated sequences. This work aims to overcome this limitation
by developing a model that directly generates low-energy, stable protein
sequences. We propose EnerBridge-DPO, a novel inverse folding framework focused
on generating low-energy, high-stability protein sequences. Our core innovation
lies in: First, integrating Markov Bridges with Direct Preference Optimization
(DPO), where energy-based preferences are used to fine-tune the Markov Bridge
model. The Markov Bridge initiates optimization from an information-rich prior
sequence, providing DPO with a pool of structurally plausible sequence
candidates. Second, an explicit energy constraint loss is introduced, which
enhances the energy-driven nature of DPO based on prior sequences, enabling the
model to effectively learn energy representations from a wealth of prior
knowledge and directly predict sequence energy values, thereby capturing
quantitative features of the energy landscape. Our evaluations demonstrate that
EnerBridge-DPO can design protein complex sequences with lower energy while
maintaining sequence recovery rates comparable to state-of-the-art models, and
accurately predicts $\Delta \Delta G$ values between various sequences.

</details>


### [119] [A Unified Theory of Compositionality, Modularity, and Interpretability in Markov Decision Processes](https://arxiv.org/abs/2506.09499)
*Thomas J. Ringstrom,Paul R. Schrater*

Key words: Option Kernel Bellman Equations, state-time option kernel, Chapman-Kolmogorov, intrinsic motivation, verifiable planning

TL;DR: 本文介绍了Option Kernel Bellman Equations (OKBEs)，用于构建和优化一个称为状态-时间选项核(STOK)的预测映射，以最大化完成目标并避免约束违反的概率。STOK具有组合性、模块化和可解释性，支持长期规划和内在动机。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统的基于价值函数的方法在组合性、模块化和可解释性方面存在不足。OKBEs通过构造STOK，旨在解决这些问题，支持可验证的长期规划和内在动机。

Method: 引入OKBEs，构建和优化STOK，利用Chapman-Kolmogorov方程组合STOK进行长期预测，并以因子化和可重构形式高效表示高维STOK。

Result: STOK支持高维状态转移模型的分解，能够灵活合成元策略、跨任务重用规划表示，并提供语义可解释的目标成功和约束违反概率。

Conclusion: OKBEs通过STOK的组合性和可解释性，克服了传统奖励最大化方法的局限性，支持可扩展的高维动态世界模型规划。

Abstract: We introduce Option Kernel Bellman Equations (OKBEs) for a new reward-free
Markov Decision Process. Rather than a value function, OKBEs directly construct
and optimize a predictive map called a state-time option kernel (STOK) to
maximize the probability of completing a goal while avoiding constraint
violations. STOKs are compositional, modular, and interpretable
initiation-to-termination transition kernels for policies in the Options
Framework of Reinforcement Learning. This means: 1) STOKs can be composed using
Chapman-Kolmogorov equations to make spatiotemporal predictions for multiple
policies over long horizons, 2) high-dimensional STOKs can be represented and
computed efficiently in a factorized and reconfigurable form, and 3) STOKs
record the probabilities of semantically interpretable goal-success and
constraint-violation events, needed for formal verification. Given a
high-dimensional state-transition model for an intractable planning problem, we
can decompose it with local STOKs and goal-conditioned policies that are
aggregated into a factorized goal kernel, making it possible to forward-plan at
the level of goals in high-dimensions to solve the problem. These properties
lead to highly flexible agents that can rapidly synthesize meta-policies, reuse
planning representations across many tasks, and justify goals using
empowerment, an intrinsic motivation function. We argue that
reward-maximization is in conflict with the properties of compositionality,
modularity, and interpretability. Alternatively, OKBEs facilitate these
properties to support verifiable long-horizon planning and intrinsic motivation
that scales to dynamic high-dimensional world-models.

</details>


### [120] [Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design](https://arxiv.org/abs/2506.09508)
*Andreas Schlaginhaufen,Reda Ouhamma,Maryam Kamgarpour*

Key words: 强化学习, 人类反馈, 偏好查询, 马尔可夫决策过程, 最优实验设计

TL;DR: 提出了一种基于随机探索的元算法，用于从人类偏好反馈中学习，避免了乐观方法的计算复杂性，并在理论和实践中表现良好。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究在马尔可夫决策过程中如何从轨迹级别的偏好比较中学习，解决选择信息性偏好查询以识别潜在奖励的挑战。

Method: 提出基于随机探索的元算法，并通过批量收集轨迹对和最优实验设计来改进查询复杂度，同时支持并行化偏好查询。

Result: 在温和的强化学习假设下，证明了算法的遗憾和最终迭代保证，实验表明其与基于奖励的强化学习方法竞争力强且查询次数少。

Conclusion: 该算法在理论和实践中均表现优异，显著降低了偏好查询的复杂度，适合实际部署。

Abstract: We study reinforcement learning from human feedback in general Markov
decision processes, where agents learn from trajectory-level preference
comparisons. A central challenge in this setting is to design algorithms that
select informative preference queries to identify the underlying reward while
ensuring theoretical guarantees. We propose a meta-algorithm based on
randomized exploration, which avoids the computational challenges associated
with optimistic approaches and remains tractable. We establish both regret and
last-iterate guarantees under mild reinforcement learning oracle assumptions.
To improve query complexity, we introduce and analyze an improved algorithm
that collects batches of trajectory pairs and applies optimal experimental
design to select informative comparison queries. The batch structure also
enables parallelization of preference queries, which is relevant in practical
deployment as feedback can be gathered concurrently. Empirical evaluation
confirms that the proposed method is competitive with reward-based
reinforcement learning while requiring a small number of preference queries.

</details>


### [121] [Neural Functions for Learning Periodic Signal](https://arxiv.org/abs/2506.09526)
*Woojin Cho,Minju Jo,Kookjin Lee,Noseong Park*

Key words: 深度神经网络, 周期性信号, 泛化性, 外推性能, MLPs

TL;DR: 提出了一种新网络架构，以增强周期性信号表示的泛化性和外推性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对周期性信号，传统的基于坐标的多层感知机（MLPs）存在过拟合和外推能力不足的问题。

Method: 提出了一种新网络架构，通过提取周期性模式并利用这些信息表示信号。

Result: 实验验证了该方法在微分方程周期性解学习、时间序列插补和外推任务中的有效性。

Conclusion: 新方法能显著提升周期性信号的外推和泛化性能。

Abstract: As function approximators, deep neural networks have served as an effective
tool to represent various signal types. Recent approaches utilize multi-layer
perceptrons (MLPs) to learn a nonlinear mapping from a coordinate to its
corresponding signal, facilitating the learning of continuous neural
representations from discrete data points. Despite notable successes in
learning diverse signal types, coordinate-based MLPs often face issues of
overfitting and limited generalizability beyond the training region, resulting
in subpar extrapolation performance. This study addresses scenarios where the
underlying true signals exhibit periodic properties, either spatially or
temporally. We propose a novel network architecture, which extracts periodic
patterns from measurements and leverages this information to represent the
signal, thereby enhancing generalization and improving extrapolation
performance. We demonstrate the efficacy of the proposed method through
comprehensive experiments, including the learning of the periodic solutions for
differential equations, and time series imputation (interpolation) and
forecasting (extrapolation) on real-world datasets.

</details>


### [122] [Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models](https://arxiv.org/abs/2506.09532)
*Shuai Wang,Zhenhua Liu,Jiaheng Wei,Xuanwu Yin,Dong Li,Emad Barsoum*

Key words: 多模态过程奖励模型, 预测一致性, ORM初始化, 负数据上采样, 奖励排名微调

TL;DR: Athena-PRM是一种多模态过程奖励模型，用于评估解决复杂推理问题中每一步的奖励分数。通过利用弱与强完成者之间预测一致性生成高质量标签，显著减少了计算成本。仅需5000样本，该模型在多个场景和基准测试中表现出色，并显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统的过程奖励模型需要大量时间和经济投入，且自动标注方法常产生噪声标签。研究旨在通过高效生成高质量过程标签数据来解决这一问题。

Method: 利用弱与强完成者之间的预测一致性作为识别可靠过程标签的标准。采用ORM初始化和负数据上采样策略优化模型性能。在验证测试时间扩展、评估推理步骤正确性和奖励排名微调三种场景中验证了方法的有效性。

Result: Athena-PRM在WebMath和MathVista上分别提升了10.2和7.1分，并在VisualProcessBench上创下3.9 F1得分的SoTA记录。奖励排名微调开发的Athena-7B在五个基准测试中显著优于基线。

Conclusion: Athena-PRM通过高效生成高质量过程标签数据，显著提升了推理问题的评估性能和模型训练效果。

Abstract: We present Athena-PRM, a multimodal process reward model (PRM) designed to
evaluate the reward score for each step in solving complex reasoning problems.
Developing high-performance PRMs typically demands significant time and
financial investment, primarily due to the necessity for step-level annotations
of reasoning steps. Conventional automated labeling methods, such as Monte
Carlo estimation, often produce noisy labels and incur substantial
computational costs. To efficiently generate high-quality process-labeled data,
we propose leveraging prediction consistency between weak and strong completers
as a criterion for identifying reliable process labels. Remarkably, Athena-PRM
demonstrates outstanding effectiveness across various scenarios and benchmarks
with just 5,000 samples. Furthermore, we also develop two effective strategies
to improve the performance of PRMs: ORM initialization and up-sampling for
negative data. We validate our approach in three specific scenarios:
verification for test time scaling, direct evaluation of reasoning step
correctness, and reward ranked fine-tuning. Our Athena-PRM consistently
achieves superior performance across multiple benchmarks and scenarios.
Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances
performance by 10.2 points on WeMath and 7.1 points on MathVista for test time
scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in
VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,
showcasing its robust capability to accurately assess the correctness of the
reasoning step. Additionally, utilizing Athena-PRM as the reward model, we
develop Athena-7B with reward ranked fine-tuning and outperforms baseline with
a significant margin on five benchmarks.

</details>


### [123] [STOAT: Spatial-Temporal Probabilistic Causal Inference Network](https://arxiv.org/abs/2506.09544)
*Yang Yang,Du Yin,Hao Xue,Flora Salim*

Key words: 空间-时间因果时间序列,概率预测,因果推断,深度概率模型,COVID-19

TL;DR: 论文提出了一种名为STOAT的新框架，用于空间-时间因果时间序列的概率预测，通过结合空间关系矩阵和深度概率模型，优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法通常独立建模空间和时间动态，忽视了因果驱动的概率预测，限制了预测能力。

Method: STOAT框架结合了因果推断方法和空间关系矩阵，利用深度概率模型进行分布参数估计，支持多种输出分布。

Result: 在COVID-19数据上的实验表明，STOAT在关键指标上优于现有模型，尤其在空间依赖性强的区域。

Conclusion: STOAT为复杂的空间-时间任务（如流行病管理）提供了一个通用的框架。

Abstract: Spatial-temporal causal time series (STC-TS) involve region-specific temporal
observations driven by causally relevant covariates and interconnected across
geographic or network-based spaces. Existing methods often model spatial and
temporal dynamics independently and overlook causality-driven probabilistic
forecasting, limiting their predictive power. To address this, we propose STOAT
(Spatial-Temporal Probabilistic Causal Inference Network), a novel framework
for probabilistic forecasting in STC-TS. The proposed method extends a causal
inference approach by incorporating a spatial relation matrix that encodes
interregional dependencies (e.g. proximity or connectivity), enabling spatially
informed causal effect estimation. The resulting latent series are processed by
deep probabilistic models to estimate the parameters of the distributions,
enabling calibrated uncertainty modeling. We further explore multiple output
distributions (e.g., Gaussian, Student's-$t$, Laplace) to capture
region-specific variability. Experiments on COVID-19 data across six countries
demonstrate that STOAT outperforms state-of-the-art probabilistic forecasting
models (DeepAR, DeepVAR, Deep State Space Model, etc.) in key metrics,
particularly in regions with strong spatial dependencies. By bridging causal
inference and geospatial probabilistic forecasting, STOAT offers a
generalizable framework for complex spatial-temporal tasks, such as epidemic
management.

</details>


### [124] [MOORL: A Framework for Integrating Offline-Online Reinforcement Learning](https://arxiv.org/abs/2506.09574)
*Gaurav Chaudhary,Wassim Uddin Mondal,Laxmidhar Behera*

Key words: 深度强化学习, 离线RL, 探索, 元策略, 样本效率

TL;DR: MOORL是一种结合离线与在线强化学习的混合框架，通过元策略适应两种数据来源，提升探索效率与性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决深度强化学习中样本效率与探索的挑战，特别是在复杂领域中离线RL面临的OOD问题。

Method: 提出MOORL框架，通过元策略无缝结合离线与在线轨迹，利用离线数据初始化并在线交互探索。

Result: 理论分析与实验验证表明，MOORL在28个任务上优于现有方法，性能稳定且计算开销小。

Conclusion: MOORL为实际应用提供了一种高效且可扩展的解决方案，结合离线与在线学习的优势。

Abstract: Sample efficiency and exploration remain critical challenges in Deep
Reinforcement Learning (DRL), particularly in complex domains. Offline RL,
which enables agents to learn optimal policies from static, pre-collected
datasets, has emerged as a promising alternative. However, offline RL is
constrained by issues such as out-of-distribution (OOD) actions that limit
policy performance and generalization. To overcome these limitations, we
propose Meta Offline-Online Reinforcement Learning (MOORL), a hybrid framework
that unifies offline and online RL for efficient and scalable learning. While
previous hybrid methods rely on extensive design components and added
computational complexity to utilize offline data effectively, MOORL introduces
a meta-policy that seamlessly adapts across offline and online trajectories.
This enables the agent to leverage offline data for robust initialization while
utilizing online interactions to drive efficient exploration. Our theoretical
analysis demonstrates that the hybrid approach enhances exploration by
effectively combining the complementary strengths of offline and online data.
Furthermore, we demonstrate that MOORL learns a stable Q-function without added
complexity. Extensive experiments on 28 tasks from the D4RL and V-D4RL
benchmarks validate its effectiveness, showing consistent improvements over
state-of-the-art offline and hybrid RL baselines. With minimal computational
overhead, MOORL achieves strong performance, underscoring its potential for
practical applications in real-world scenarios.

</details>


### [125] [Beyond Overconfidence: Foundation Models Redefine Calibration in Deep Neural Networks](https://arxiv.org/abs/2506.09593)
*Achim Hekler,Lukas Kuhn,Florian Buettner*

Key words: 基础模型, 校准, 分布偏移, 深度神经网络, 欠自信

TL;DR: 该论文研究了基础模型的校准行为，发现这些模型在分布内预测中表现出欠自信，导致较高的校准误差，但在分布偏移下校准效果更好。研究还表明，后校准技术能有效缓解欠自信，但在严重分布偏移下效果有限。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 深度神经网络在分布偏移下存在系统性过度自信问题，而基础模型的校准性质尚未充分研究。本研究旨在揭示基础模型的校准行为，挑战现有范式。

Method: 通过实证分析，研究基础模型在校准中的表现，特别是在分布内和分布偏移情况下，并评估后校准技术的有效性。

Result: 发现基础模型在分布内预测中欠自信，校准误差高；分布偏移下校准改善。后校准技术对分布内有效，但对严重偏移效果有限。

Conclusion: 基础模型的校准行为复杂且非单调，挑战了持续改进的传统观点。

Abstract: Reliable uncertainty calibration is essential for safely deploying deep
neural networks in high-stakes applications. Deep neural networks are known to
exhibit systematic overconfidence, especially under distribution shifts.
Although foundation models such as ConvNeXt, EVA and BEiT have demonstrated
significant improvements in predictive performance, their calibration
properties remain underexplored. This paper presents a comprehensive
investigation into the calibration behavior of foundation models, revealing
insights that challenge established paradigms. Our empirical analysis shows
that these models tend to be underconfident in in-distribution predictions,
resulting in higher calibration errors, while demonstrating improved
calibration under distribution shifts. Furthermore, we demonstrate that
foundation models are highly responsive to post-hoc calibration techniques in
the in-distribution setting, enabling practitioners to effectively mitigate
underconfidence bias. However, these methods become progressively less reliable
under severe distribution shifts and can occasionally produce counterproductive
results. Our findings highlight the complex, non-monotonic effects of
architectural and training innovations on calibration, challenging established
narratives of continuous improvement.

</details>


### [126] [Accelerating Large-Scale Regularized High-Order Tensor Recovery](https://arxiv.org/abs/2506.09594)
*Wenjin Qin,Hailin Wang,Jingyao Hou,Jianjun Wang*

Key words: 张量恢复, 随机化算法, 非凸建模, 大规模数据, 优化算法

TL;DR: 该论文提出了一种新的快速随机化算法和广义非凸建模框架，用于解决大规模高阶张量恢复问题，并通过实验验证了其高效性和优越性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有张量恢复方法未考虑张量尺度变化对结构特性的影响，且处理大规模数据时计算成本过高。

Method: 基于Krylov子空间迭代、块Lanczos双对角化过程和随机投影策略，设计了两种快速随机化算法，并提出了广义非凸建模框架和新正则化范式。

Result: 理论分析了近似误差的估计精度，并通过实验证明所提方法在大规模张量数据上的实用性和高效性。

Conclusion: 该方法在处理大规模高阶张量恢复任务中表现出显著优势，优于现有先进方法。

Abstract: Currently, existing tensor recovery methods fail to recognize the impact of
tensor scale variations on their structural characteristics. Furthermore,
existing studies face prohibitive computational costs when dealing with
large-scale high-order tensor data. To alleviate these issue, assisted by the
Krylov subspace iteration, block Lanczos bidiagonalization process, and random
projection strategies, this article first devises two fast and accurate
randomized algorithms for low-rank tensor approximation (LRTA) problem.
Theoretical bounds on the accuracy of the approximation error estimate are
established. Next, we develop a novel generalized nonconvex modeling framework
tailored to large-scale tensor recovery, in which a new regularization paradigm
is exploited to achieve insightful prior representation for large-scale
tensors. On the basis of the above, we further investigate new unified
nonconvex models and efficient optimization algorithms, respectively, for
several typical high-order tensor recovery tasks in unquantized and quantized
situations. To render the proposed algorithms practical and efficient for
large-scale tensor data, the proposed randomized LRTA schemes are integrated
into their central and time-intensive computations. Finally, we conduct
extensive experiments on various large-scale tensors, whose results demonstrate
the practicability, effectiveness and superiority of the proposed method in
comparison with some state-of-the-art approaches.

</details>


### [127] [SparseSSM: Efficient Selective Structured State Space Models Can Be Pruned in One-Shot](https://arxiv.org/abs/2506.09613)
*Kaiwen Tuo,Huan Wang*

Key words: state-space models, pruning, SparseSSM, Mamba, OBS

TL;DR: SparseSSM是一种无需训练的剪枝框架，专为状态空间模型设计，能够高效剪枝并保持模型性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有剪枝方法不适用于状态空间模型，导致模型参数量大，部署困难。

Method: 提出SparseSSM，扩展OBS框架，结合二阶显著性评分和组件敏感性分析，支持半结构和结构化稀疏化。

Result: 剪枝50%SSM权重时，零样本精度无损失，性能优于现有剪枝方法。

Conclusion: SparseSSM为Mamba类大模型提供了高效的剪枝解决方案。

Abstract: State-space language models such as Mamba match Transformer quality while
permitting linear complexity inference, yet still comprise billions of
parameters that hinder deployment. Existing one-shot pruning methods are
tailored to attention blocks and fail to account for the time-shared and
discretized state-transition matrix at the heart of the selective state-space
module (SSM). In this paper, we introduce SparseSSM, the first training-free
pruning framework that extends the classic optimal brain surgeon (OBS)
framework to state space architectures. Our layer-wise algorithm (i) derives an
approximate second-order saliency score that aggregates Hessian-trace
information across time steps, (ii) incorporates a component sensitivity
analysis to guide feed-forward network (FFN) pruning, which also sheds light on
where redundancy resides in mamba architecture, (iii) can be easily extended to
semi-structured and structured sparsity. Empirically, we prune 50% of SSM
weights without fine-tuning and observe no zero-shot accuracy loss, achieving
the current state-of-the-art pruning algorithm for Mamba-based LLMs.

</details>


### [128] [GLGENN: A Novel Parameter-Light Equivariant Neural Networks Architecture Based on Clifford Geometric Algebras](https://arxiv.org/abs/2506.09625)
*Ekaterina Filimoshina,Dmitry Shirokov*

Key words: 等变神经网络, 几何代数, 权重共享, 伪正交变换, 过拟合

TL;DR: 论文提出了一种基于几何代数的新架构GLGENN，具有对伪正交变换的等变性，并通过权重共享技术减少参数量，降低了过拟合风险，同时在多个基准任务中表现优于或媲美基线模型。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了开发一种对伪正交变换（如旋转和反射）具有等变性的神经网络架构，以适用于具有非退化或退化对称双线性形式的向量空间，同时解决现有等变模型参数量大和过拟合的问题。

Method: 提出了一种基于几何（Clifford）代数的广义Lipschitz群等变神经网络（GLGENN），采用权重共享参数化技术，利用几何代数的基本结构和操作来减少参数量。

Result: GLGENN在多个等变基准任务（如等变函数估计和凸包实验）中表现优于或与基线模型相当，且使用的可优化参数量显著减少。

Conclusion: GLGENN架构不仅解决了现有模型的过拟合问题，还在保持高性能的同时显著减少了参数量，证明了其在等变任务中的有效性。

Abstract: We propose, implement, and compare with competitors a new architecture of
equivariant neural networks based on geometric (Clifford) algebras: Generalized
Lipschitz Group Equivariant Neural Networks (GLGENN). These networks are
equivariant to all pseudo-orthogonal transformations, including rotations and
reflections, of a vector space with any non-degenerate or degenerate symmetric
bilinear form. We propose a weight-sharing parametrization technique that takes
into account the fundamental structures and operations of geometric algebras.
Due to this technique, GLGENN architecture is parameter-light and has less
tendency to overfitting than baseline equivariant models. GLGENN outperforms or
matches competitors on several benchmarking equivariant tasks, including
estimation of an equivariant function and a convex hull experiment, while using
significantly fewer optimizable parameters.

</details>


### [129] [In-Context Bias Propagation in LLM-Based Tabular Data Generation](https://arxiv.org/abs/2506.09630)
*Pol G. Recasens,Alberto Gutierrez,Jordi Torres,Josep. Ll Berral,Anisa Halimi,Kieran Fraser*

Key words: 大型语言模型,合成数据生成,统计偏差,数据公平性,安全漏洞

TL;DR: 本文系统研究了大型语言模型（LLMs）在合成表格数据生成中，因上下文示例的统计偏差导致的数据分布失真问题，并揭示了一种恶意注入偏差的新漏洞。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究LLMs在数据稀缺场景下用于数据增强时，上下文示例中的统计偏差如何传播到合成数据中，并探讨其对下游任务公平性的潜在威胁。

Method: 通过系统实验分析上下文示例中的偏差对合成数据分布的影响，并设计对抗性场景展示恶意注入偏差的可能性。

Result: 研究表明，即使轻微的上下文偏差也会导致全局统计失真，且恶意行为者可以通过上下文示例注入偏差，影响下游分类器的公平性。

Conclusion: LLM基于上下文提示的数据生成流程在敏感领域存在新的安全漏洞，需警惕偏差传播和恶意利用。

Abstract: Large Language Models (LLMs) are increasingly used for synthetic tabular data
generation through in-context learning (ICL), offering a practical solution for
data augmentation in data scarce scenarios. While prior work has shown the
potential of LLMs to improve downstream task performance through augmenting
underrepresented groups, these benefits often assume access to a subset of
unbiased in-context examples, representative of the real dataset. In real-world
settings, however, data is frequently noisy and demographically skewed. In this
paper, we systematically study how statistical biases within in-context
examples propagate to the distribution of synthetic tabular data, showing that
even mild in-context biases lead to global statistical distortions. We further
introduce an adversarial scenario where a malicious contributor can inject bias
into the synthetic dataset via a subset of in-context examples, ultimately
compromising the fairness of downstream classifiers for a targeted and
protected subgroup. Our findings demonstrate a new vulnerability associated
with LLM-based data generation pipelines that rely on in-context prompts with
in sensitive domains.

</details>


### [130] [FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models](https://arxiv.org/abs/2506.09638)
*Weiying Zheng,Ziyue Lin,Pengxin Guo,Yuyin Zhou,Feifei Wang,Liangqiong Qu*

Key words: VLM,联邦学习,隐私保护,多模态,基准测试

TL;DR: FedVLMBench是首个系统性的联邦学习视觉语言模型（VLMs）微调基准，涵盖多种架构、策略和任务，提供关键见解和标准化工具。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决现有VLM微调方法在隐私敏感领域（如医疗）中的部署挑战，缺乏全面评估联邦学习在VLM中应用的问题。

Method: 整合两种主流VLM架构、四种微调策略、五种联邦学习算法、六个数据集，覆盖四种任务类别和多任务设置。

Result: 2层MLP连接器与并发调优在联邦学习中效果最佳，且当前联邦学习方法对视觉任务的异构数据更敏感。

Conclusion: FedVLMBench为推动隐私保护的多模态基础模型联邦训练提供了标准化平台和实用指导。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable capabilities in
cross-modal understanding and generation by integrating visual and textual
information. While instruction tuning and parameter-efficient fine-tuning
methods have substantially improved the generalization of VLMs, most existing
approaches rely on centralized training, posing challenges for deployment in
domains with strict privacy requirements like healthcare. Recent efforts have
introduced Federated Learning (FL) into VLM fine-tuning to address these
privacy concerns, yet comprehensive benchmarks for evaluating federated
fine-tuning strategies, model architectures, and task generalization remain
lacking. In this work, we present \textbf{FedVLMBench}, the first systematic
benchmark for federated fine-tuning of VLMs. FedVLMBench integrates two
mainstream VLM architectures (encoder-based and encoder-free), four fine-tuning
strategies, five FL algorithms, six multimodal datasets spanning four
cross-domain single-task scenarios and two cross-domain multitask settings,
covering four distinct downstream task categories. Through extensive
experiments, we uncover key insights into the interplay between VLM
architectures, fine-tuning strategies, data heterogeneity, and multi-task
federated optimization. Notably, we find that a 2-layer multilayer perceptron
(MLP) connector with concurrent connector and LLM tuning emerges as the optimal
configuration for encoder-based VLMs in FL. Furthermore, current FL methods
exhibit significantly higher sensitivity to data heterogeneity in
vision-centric tasks than text-centric ones, across both encoder-free and
encoder-based VLM architectures. Our benchmark provides essential tools,
datasets, and empirical guidance for the research community, offering a
standardized platform to advance privacy-preserving, federated training of
multimodal foundation models.

</details>


### [131] [SyncFed: Time-Aware Federated Learning through Explicit Timestamping and Synchronization](https://arxiv.org/abs/2506.09660)
*Baran Can Gül,Stefanos Tziampazis,Nasser Jazdi,Michael Weyrich*

Key words: 联邦学习, 时间同步, 陈旧性量化, NTP, 分布式训练

TL;DR: SyncFed是一个时间感知的联邦学习框架，通过时间同步和标记来解决分布式环境中的训练一致性问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 联邦学习在分布式环境中面临网络延迟、时钟不同步和客户端更新不一致等问题，这些问题可能导致模型可靠性下降和收敛困难。

Method: SyncFed使用显式时间同步和网络时间协议（NTP）标记，为系统建立共同的时间参考，并根据时间戳量化更新的陈旧程度。

Result: 在分布式测试环境中，SyncFed显著提升了模型准确性和信息新鲜度，优于传统基于轮次的基准方法。

Conclusion: SyncFed通过时间感知机制有效解决了联邦学习中的不一致问题，提高了模型性能和可靠性。

Abstract: As Federated Learning (FL) expands to larger and more distributed
environments, consistency in training is challenged by network-induced delays,
clock unsynchronicity, and variability in client updates. This combination of
factors may contribute to misaligned contributions that undermine model
reliability and convergence. Existing methods like staleness-aware aggregation
and model versioning address lagging updates heuristically, yet lack mechanisms
to quantify staleness, especially in latency-sensitive and cross-regional
deployments. In light of these considerations, we introduce \emph{SyncFed}, a
time-aware FL framework that employs explicit synchronization and timestamping
to establish a common temporal reference across the system. Staleness is
quantified numerically based on exchanged timestamps under the Network Time
Protocol (NTP), enabling the server to reason about the relative freshness of
client updates and apply temporally informed weighting during aggregation. Our
empirical evaluation on a geographically distributed testbed shows that, under
\emph{SyncFed}, the global model evolves within a stable temporal context,
resulting in improved accuracy and information freshness compared to
round-based baselines devoid of temporal semantics.

</details>


### [132] [Wavelet Scattering Transform and Fourier Representation for Offline Detection of Malicious Clients in Federated Learning](https://arxiv.org/abs/2506.09674)
*Alessandro Licciardi,Davide Leo,Davide Carbone*

Key words: 联邦学习,异常检测,小波散射变换,傅里叶变换

TL;DR: WAFFLE算法通过小波和傅里叶变换检测异常客户端，提升联邦学习的模型性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决联邦学习中因异常客户端导致模型性能下降的问题。

Method: 使用小波散射变换（WST）或傅里叶变换生成低维嵌入，通过轻量级检测器标记恶意客户端。

Result: 实验证明WAFFLE在检测准确性和分类性能上优于现有方法。

Conclusion: WAFFLE是联邦学习中预处理异常客户端的有效替代方案。

Abstract: Federated Learning (FL) enables the training of machine learning models
across decentralized clients while preserving data privacy. However, the
presence of anomalous or corrupted clients - such as those with faulty sensors
or non representative data distributions - can significantly degrade model
performance. Detecting such clients without accessing raw data remains a key
challenge. We propose WAFFLE (Wavelet and Fourier representations for Federated
Learning) a detection algorithm that labels malicious clients {\it before
training}, using locally computed compressed representations derived from
either the Wavelet Scattering Transform (WST) or the Fourier Transform. Both
approaches provide low-dimensional, task-agnostic embeddings suitable for
unsupervised client separation. A lightweight detector, trained on a
distillated public dataset, performs the labeling with minimal communication
and computational overhead. While both transforms enable effective detection,
WST offers theoretical advantages, such as non-invertibility and stability to
local deformations, that make it particularly well-suited to federated
scenarios. Experiments on benchmark datasets show that our method improves
detection accuracy and downstream classification performance compared to
existing FL anomaly detection algorithms, validating its effectiveness as a
pre-training alternative to online detection strategies.

</details>


### [133] [Wasserstein Hypergraph Neural Network](https://arxiv.org/abs/2506.09682)
*Iulia Duta,Pietro Liò*

Key words: 超图神经网络, Wasserstein距离, 信息聚合, 节点分类

TL;DR: 该论文提出了一种名为Wasserstein超图神经网络的新模型，通过将节点和超边邻域视为分布，并使用Sliced Wasserstein Pooling进行信息聚合，以保留几何特性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统的超图神经网络多为两阶段框架，且信息聚合方式简化。本文希望通过引入Wasserstein距离，更好地捕捉分布的形状和扩散特性，提升节点分类任务的性能。

Method: 提出了Wasserstein超图神经网络，将节点和超边邻域建模为分布，使用Sliced Wasserstein Pooling进行信息聚合。相比传统的均值或求和聚合，该方法能保留分布的几何特性。

Result: 实验结果表明，该方法在多个真实数据集上的节点分类任务中表现优异，达到顶级性能。

Conclusion: 通过引入Wasserstein距离和最优传输原理，该模型能够更有效地捕捉超图中的高阶关系。

Abstract: The ability to model relational information using machine learning has driven
advancements across various domains, from medicine to social science. While
graph representation learning has become mainstream over the past decade,
representing higher-order relationships through hypergraphs is rapidly gaining
momentum. In the last few years, numerous hypergraph neural networks have
emerged, most of them falling under a two-stage, set-based framework. The
messages are sent from nodes to edges and then from edges to nodes. However,
most of the advancement still takes inspiration from the graph counterpart,
often simplifying the aggregations to basic pooling operations. In this paper
we are introducing Wasserstein Hypergraph Neural Network, a model that treats
the nodes and hyperedge neighbourhood as distributions and aggregate the
information using Sliced Wasserstein Pooling. Unlike conventional aggregators
such as mean or sum, which only capture first-order statistics, our approach
has the ability to preserve geometric properties like the shape and spread of
distributions. This enables the learned embeddings to reflect how easily one
hyperedge distribution can be transformed into another, following principles of
optimal transport. Experimental results demonstrate that applying Wasserstein
pooling in a hypergraph setting significantly benefits node classification
tasks, achieving top performance on several real-world datasets.

</details>


### [134] [TRIDENT: Temporally Restricted Inference via DFA-Enhanced Neural Traversal](https://arxiv.org/abs/2506.09701)
*Vincenzo Collura,Karim Tit,Laura Bussi,Eleonora Giunchiglia,Maxime Cordy*

Key words: TRIDENT, LTLf, DFA, 约束满足, LLMs

TL;DR: TRIDENT是一种通用的、与模型无关的推理时间算法，通过将LTLf公式编译为DFA来保证输出满足时间约束，无需重新训练。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 尽管LLMs和神经网络在生成和分类任务上表现出色，但它们在确保输出满足时间约束方面仍有不足。

Method: TRIDENT将LTLf公式编译为DFA，用于引导约束波束搜索，动态屏蔽违反约束的路径，并根据模型概率和DFA接受结构重新排序剩余路径。

Result: TRIDENT在图像流分类和受控文本生成任务中实现了完美的约束满足，同时提升了效率和质量指标。

Conclusion: TRIDENT有效解决了LLMs在满足时间约束方面的局限性，并在实际应用中表现出色。

Abstract: Large Language Models (LLMs) and other neural architectures have achieved
impressive results across a variety of generative and classification tasks.
However, they remain fundamentally ill-equipped to ensure that their outputs
satisfy temporal constraints, such as those expressible in Linear Temporal
Logic over finite traces (LTLf). In this paper, we introduce TRIDENT: a general
and model-agnostic inference-time algorithm that guarantees compliance with
such constraints without requiring any retraining. TRIDENT compiles LTLf
formulas into a Deterministic Finite Automaton (DFA), which is used to guide a
constrained variant of beam search. At each decoding step, transitions that
would lead to constraint violations are masked, while remaining paths are
dynamically re-ranked based on both the model's probabilities and the DFA's
acceptance structure. We formally prove that the resulting sequences are
guaranteed to satisfy the given LTLf constraints, and we empirically
demonstrate that TRIDENT also improves output quality. We validate our approach
on two distinct tasks: temporally constrained image-stream classification and
controlled text generation. In both settings, TRIDENT achieves perfect
constraint satisfaction, while comparison with the state of the art shows
improved efficiency and high standard quality metrics.

</details>


### [135] [Auto-Compressing Networks](https://arxiv.org/abs/2506.09714)
*Vaggelis Dorovatas,Georgios Paraskevopoulos,Alexandros Potamianos*

Key words: Auto-Compressing Networks, 信息压缩, 残差连接, 深度神经网络, 迁移学习

TL;DR: Auto-Compressing Networks (ACNs)通过长前馈连接替代传统的短残差连接，实现了信息在训练中的自动压缩，提升了表示质量并减少了冗余层。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统的深度神经网络在增加深度时容易出现计算冗余，而ACNs旨在通过架构设计解决这一问题。

Method: ACNs采用从每层到输出的长前馈连接替代短残差连接，实现“自动压缩”特性，动态优化网络表示。

Result: ACNs在降低遗忘率18%的同时实现30-80%的结构压缩，并在噪声鲁棒性、低数据训练和迁移学习方面表现优越。

Conclusion: ACNs是一种高效且自适应的架构设计，能够根据任务复杂度动态调整计算资源，同时学习到更鲁棒的表示。

Abstract: Deep neural networks with short residual connections have demonstrated
remarkable success across domains, but increasing depth often introduces
computational redundancy without corresponding improvements in representation
quality. In this work, we introduce Auto-Compressing Networks (ACNs), an
architectural variant where additive long feedforward connections from each
layer to the output replace traditional short residual connections. ACNs
showcase a unique property we coin as "auto-compression", the ability of a
network to organically compress information during training with gradient
descent, through architectural design alone. Through auto-compression,
information is dynamically "pushed" into early layers during training,
enhancing their representational quality and revealing potential redundancy in
deeper ones. We theoretically show that this property emerges from layer-wise
training patterns present in ACNs, where layers are dynamically utilized during
training based on task requirements. We also find that ACNs exhibit enhanced
noise robustness compared to residual networks, superior performance in
low-data settings, improved transfer learning capabilities, and mitigate
catastrophic forgetting suggesting that they learn representations that
generalize better despite using fewer parameters. Our results demonstrate up to
18% reduction in catastrophic forgetting and 30-80% architectural compression
while maintaining accuracy across vision transformers, MLP-mixers, and BERT
architectures. Furthermore, we demonstrate that coupling ACNs with traditional
pruning techniques, enables significantly better sparsity-performance
trade-offs compared to conventional architectures. These findings establish
ACNs as a practical approach to developing efficient neural architectures that
automatically adapt their computational footprint to task complexity, while
learning robust representations.

</details>


### [136] [AtmosMJ: Revisiting Gating Mechanism for AI Weather Forecasting Beyond the Year Scale](https://arxiv.org/abs/2506.09733)
*Minjong Cheon*

Key words: Large Weather Models, AtmosMJ, Gated Residual Fusion, long-range forecasting, ERA5

TL;DR: AtmosMJ是一种直接处理标准经纬网格数据的深度卷积网络，通过Gated Residual Fusion机制实现长期稳定预测，挑战了非标准空间域的必要性假设。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨是否能在标准经纬网格上实现长期稳定的天气预测，避免非标准空间域数据转换的复杂性。

Method: 提出AtmosMJ网络，直接处理ERA5数据，采用Gated Residual Fusion机制防止误差累积。

Result: AtmosMJ实现了约500天的稳定预测，10天预测精度与Pangu-Weather等模型相当，且训练成本低。

Conclusion: 高效的架构设计是长期稳定天气预测的关键，而非依赖非标准数据表示。

Abstract: The advent of Large Weather Models (LWMs) has marked a turning point in
data-driven forecasting, with many models now outperforming traditional
numerical systems in the medium range. However, achieving stable, long-range
autoregressive forecasts beyond a few weeks remains a significant challenge.
Prevailing state-of-the-art models that achieve year-long stability, such as
SFNO and DLWP-HPX, have relied on transforming input data onto non-standard
spatial domains like spherical harmonics or HEALPix meshes. This has led to the
prevailing assumption that such representations are necessary to enforce
physical consistency and long-term stability. This paper challenges that
assumption by investigating whether comparable long-range performance can be
achieved on the standard latitude-longitude grid. We introduce AtmosMJ, a deep
convolutional network that operates directly on ERA5 data without any spherical
remapping. The model's stability is enabled by a novel Gated Residual Fusion
(GRF) mechanism, which adaptively moderates feature updates to prevent error
accumulation over long recursive simulations. Our results demonstrate that
AtmosMJ produces stable and physically plausible forecasts for about 500 days.
In quantitative evaluations, it achieves competitive 10-day forecast accuracy
against models like Pangu-Weather and GraphCast, all while requiring a
remarkably low training budget of 5.7 days on a V100 GPU. Our findings suggest
that efficient architectural design, rather than non-standard data
representation, can be the key to unlocking stable and computationally
efficient long-range weather prediction.

</details>


### [137] [Towards Multi-modal Graph Large Language Model](https://arxiv.org/abs/2506.09738)
*Xin Wang,Zeyang Zhang,Linxin Xiao,Haibo Chen,Chendi Ge,Wenwu Zhu*

Key words: 多模态图、大语言模型、通用框架、多任务学习

TL;DR: 该论文探索了多模态图大语言模型（MG-LLM）的潜力，旨在统一和泛化多样化的多模态图数据和任务，提出了一个统一框架，并总结了相关数据集。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法难以跨任务和数据泛化，需要一种通用方法来解决多模态图学习的局限性。

Method: 提出MG-LLM框架，强调多粒度、多尺度特性，并列出五大关键特征，如统一空间和多任务处理能力。

Result: 论文总结了实现这些特征的挑战和相关工作，并提供了适用的多模态图数据集。

Conclusion: MG-LLM有望推动多模态图数据和任务的通用性研究。

Abstract: Multi-modal graphs, which integrate diverse multi-modal features and
relations, are ubiquitous in real-world applications. However, existing
multi-modal graph learning methods are typically trained from scratch for
specific graph data and tasks, failing to generalize across various multi-modal
graph data and tasks. To bridge this gap, we explore the potential of
Multi-modal Graph Large Language Models (MG-LLM) to unify and generalize across
diverse multi-modal graph data and tasks. We propose a unified framework of
multi-modal graph data, task, and model, discovering the inherent
multi-granularity and multi-scale characteristics in multi-modal graphs.
Specifically, we present five key desired characteristics for MG-LLM: 1)
unified space for multi-modal structures and attributes, 2) capability of
handling diverse multi-modal graph tasks, 3) multi-modal graph in-context
learning, 4) multi-modal graph interaction with natural language, and 5)
multi-modal graph reasoning. We then elaborate on the key challenges, review
related works, and highlight promising future research directions towards
realizing these ambitious characteristics. Finally, we summarize existing
multi-modal graph datasets pertinent for model training. We believe this paper
can contribute to the ongoing advancement of the research towards MG-LLM for
generalization across multi-modal graph data and tasks.

</details>


### [138] [Feature Engineering for Agents: An Adaptive Cognitive Architecture for Interpretable ML Monitoring](https://arxiv.org/abs/2506.09742)
*Gusseppe Bravo-Rocca,Peini Liu,Jordi Guitart,Rodrigo M Carrillo-Larco,Ajay Dholakia,David Ellison*

Key words: 机器学习监控, 特征工程, 大语言模型, 决策支持, 可解释性

TL;DR: 提出一种基于特征工程的LLM监控认知架构，提升监控输出的可解释性，通过重构、分解和编译步骤优化决策支持。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统ML模型监控方法输出冗长且可解释性低，影响决策效率。

Method: 采用特征工程原理设计LLM代理监控架构，核心为决策过程模块（重构、分解、编译）。

Result: 实验表明方法显著提升准确率，优于多种基线。

Conclusion: 结合特征工程与LLM的监控方法能生成可解释且可操作的洞察。

Abstract: Monitoring Machine Learning (ML) models in production environments is
crucial, yet traditional approaches often yield verbose, low-interpretability
outputs that hinder effective decision-making. We propose a cognitive
architecture for ML monitoring that applies feature engineering principles to
agents based on Large Language Models (LLMs), significantly enhancing the
interpretability of monitoring outputs. Central to our approach is a Decision
Procedure module that simulates feature engineering through three key steps:
Refactor, Break Down, and Compile. The Refactor step improves data
representation to better capture feature semantics, allowing the LLM to focus
on salient aspects of the monitoring data while reducing noise and irrelevant
information. Break Down decomposes complex information for detailed analysis,
and Compile integrates sub-insights into clear, interpretable outputs. This
process leads to a more deterministic planning approach, reducing dependence on
LLM-generated planning, which can sometimes be inconsistent and overly general.
The combination of feature engineering-driven planning and selective LLM
utilization results in a robust decision support system, capable of providing
highly interpretable and actionable insights. Experiments using multiple LLMs
demonstrate the efficacy of our approach, achieving significantly higher
accuracy compared to various baselines across several domains.

</details>


### [139] [Load-Aware Training Scheduling for Model Circulation-based Decentralized Federated Learning](https://arxiv.org/abs/2506.09769)
*Haruki Kainuma,Takayuki Nishio*

Key words: 联邦学习, 训练调度, 非IID数据, 计算负载, 通信负载

TL;DR: Load-aware Tram-FL通过引入训练调度机制，最小化分布式联邦学习的总训练时间，并通过分解全局优化问题为节点级子问题使其可解。实验证明其在非IID数据下显著降低训练时间并加速收敛。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 通过考虑计算和通信负载，优化分布式联邦学习的训练时间，解决非IID数据分布下的数据利用不平衡问题。

Method: 提出一种训练调度机制，将全局优化问题分解为节点级子问题，并引入方差约束以确保数据利用均衡。

Result: 在MNIST和CIFAR-10上的仿真结果表明，Load-aware Tram-FL显著减少训练时间并提升收敛速度。

Conclusion: Load-aware Tram-FL通过优化调度机制，有效提升了分布式联邦学习的效率和性能。

Abstract: This paper proposes Load-aware Tram-FL, an extension of Tram-FL that
introduces a training scheduling mechanism to minimize total training time in
decentralized federated learning by accounting for both computational and
communication loads. The scheduling problem is formulated as a global
optimization task, which-though intractable in its original form-is made
solvable by decomposing it into node-wise subproblems. To promote balanced data
utilization under non-IID distributions, a variance constraint is introduced,
while the overall training latency, including both computation and
communication costs, is minimized through the objective function. Simulation
results on MNIST and CIFAR-10 demonstrate that Load-aware Tram-FL significantly
reduces training time and accelerates convergence compared to baseline methods.

</details>


### [140] [On the Similarities of Embeddings in Contrastive Learning](https://arxiv.org/abs/2506.09781)
*Chungpa Lee,Sehee Lim,Kibok Lee,Jy-yong Sohn*

Key words: 对比学习, 余弦相似性, 小批次训练, 辅助损失

TL;DR: 本文提出了一个统一的对比学习（CL）框架，基于正负样本对嵌入的余弦相似性分析，并揭示了在批次设置下CL的局限性及改进方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 尽管已有多种对比损失形式被提出，但缺乏一个系统性解释这些目标的统一框架，本文旨在填补这一空白。

Method: 通过分析正负样本对的余弦相似性，揭示了在全批次和小批次设置下的局限性，并提出了通过引入辅助损失项来减小负样本对相似性的方差。

Result: 实验证明，引入所提损失项能显著提升小批次训练中CL方法的性能。

Conclusion: 本文提出了一个统一框架来理解CL，并有效解决了小批次训练中的性能下降问题。

Abstract: Contrastive learning (CL) operates on a simple yet effective principle:
embeddings of positive pairs are pulled together, while those of negative pairs
are pushed apart. Although various forms of contrastive loss have been proposed
and analyzed from different perspectives, prior works lack a comprehensive
framework that systematically explains a broad class of these objectives. In
this paper, we present a unified framework for understanding CL, which is based
on analyzing the cosine similarity between embeddings of positive and negative
pairs. In full-batch settings, we show that perfect alignment of positive pairs
is unattainable when similarities of negative pairs fall below a certain
threshold, and that this misalignment can be alleviated by incorporating
within-view negative pairs. In mini-batch settings, we demonstrate that smaller
batch sizes incur stronger separation among negative pairs within batches,
which leads to higher variance in similarities of negative pairs. To address
this limitation of mini-batch CL, we introduce an auxiliary loss term that
reduces the variance of similarities of negative pairs in CL. Empirical results
demonstrate that incorporating the proposed loss consistently improves the
performance of CL methods in small-batch training.

</details>


### [141] [A theoretical framework for self-supervised contrastive learning for continuous dependent data](https://arxiv.org/abs/2506.09785)
*Alexander Marusov,Alexander Yuhay,Alexey Zaytsev*

Key words: 自监督学习, 时空数据, 对比学习, 依赖关系

TL;DR: 提出了一种针对连续依赖数据的自监督学习理论框架，优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统对比自监督学习方法假设样本间语义独立，而依赖数据（如时空数据）存在复杂相关性，现有方法难以处理。

Method: 提出两种相似性度量（硬和软接近度），并推导出适应样本依赖关系的估计相似性矩阵，设计依赖感知的损失函数。

Result: 在时空下游任务中表现优异，UEA和UCR基准上分别提升4.17%和2.08%，干旱分类任务ROC-AUC提高7%。

Conclusion: 理论框架在捕捉时空依赖关系上有效，显著优于现有方法。

Abstract: Self-supervised learning (SSL) has emerged as a powerful approach to learning
representations, particularly in the field of computer vision. However, its
application to dependent data, such as temporal and spatio-temporal domains,
remains underexplored. Besides, traditional contrastive SSL methods often
assume \emph{semantic independence between samples}, which does not hold for
dependent data exhibiting complex correlations. We propose a novel theoretical
framework for contrastive SSL tailored to \emph{continuous dependent data},
which allows the nearest samples to be semantically close to each other. In
particular, we propose two possible \textit{ground truth similarity measures}
between objects -- \emph{hard} and \emph{soft} closeness. Under it, we derive
an analytical form for the \textit{estimated similarity matrix} that
accommodates both types of closeness between samples, thereby introducing
dependency-aware loss functions. We validate our approach, \emph{Dependent
TS2Vec}, on temporal and spatio-temporal downstream problems. Given the
dependency patterns presented in the data, our approach surpasses modern ones
for dependent data, highlighting the effectiveness of our theoretically
grounded loss functions for SSL in capturing spatio-temporal dependencies.
Specifically, we outperform TS2Vec on the standard UEA and UCR benchmarks, with
accuracy improvements of $4.17$\% and $2.08$\%, respectively. Furthermore, on
the drought classification task, which involves complex spatio-temporal
patterns, our method achieves a $7$\% higher ROC-AUC score.

</details>


### [142] [Devil's Hand: Data Poisoning Attacks to Locally Private Graph Learning Protocols](https://arxiv.org/abs/2506.09803)
*Longzhu He,Chaozhuo Li,Peng Tang,Litian Zhang,Sen Su*

Key words: 图神经网络, 本地差分隐私, 数据投毒攻击, 隐私保护, 图学习

TL;DR: 该论文提出了首个针对本地隐私图学习协议的数据投毒攻击，并展示了其有效性和防御策略的不足。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的本地隐私图学习协议在保护用户隐私的同时，可能面临数据投毒攻击的威胁，这一问题尚未被研究。

Method: 攻击者通过注入虚假用户并操纵其与真实用户建立连接，向服务器发送精心设计的数据以破坏隐私图学习的效用。理论分析和实证研究验证了攻击的有效性。

Result: 攻击在理论和实验上均被证明有效，且现有的防御策略效果有限。

Conclusion: 本地隐私图学习协议需要更强大的防御机制以应对数据投毒攻击。

Abstract: Graph neural networks (GNNs) have achieved significant success in graph
representation learning and have been applied to various domains. However, many
real-world graphs contain sensitive personal information, such as user profiles
in social networks, raising serious privacy concerns when graph learning is
performed using GNNs. To address this issue, locally private graph learning
protocols have gained considerable attention. These protocols leverage the
privacy advantages of local differential privacy (LDP) and the effectiveness of
GNN's message-passing in calibrating noisy data, offering strict privacy
guarantees for users' local data while maintaining high utility (e.g., node
classification accuracy) for graph learning. Despite these advantages, such
protocols may be vulnerable to data poisoning attacks, a threat that has not
been considered in previous research. Identifying and addressing these threats
is crucial for ensuring the robustness and security of privacy-preserving graph
learning frameworks. This work introduces the first data poisoning attack
targeting locally private graph learning protocols. The attacker injects fake
users into the protocol, manipulates these fake users to establish links with
genuine users, and sends carefully crafted data to the server, ultimately
compromising the utility of private graph learning. The effectiveness of the
attack is demonstrated both theoretically and empirically. In addition, several
defense strategies have also been explored, but their limited effectiveness
highlights the need for more robust defenses.

</details>


### [143] [Generalizing Supervised Contrastive learning: A Projection Perspective](https://arxiv.org/abs/2506.09810)
*Minoh Jeong,Alfred Hero*

Key words: 监督对比学习,自监督对比学习,互信息,ProjNCE

TL;DR: 本文提出了ProjNCE，一种将自监督和监督对比学习统一起来的损失函数，并证明其为有效的互信息边界，同时通过实验验证其优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 监督对比学习（SupCon）在互信息视角下的研究较少，本文旨在填补这一空白。

Method: 通过引入投影函数和调整负对的调整项，提出ProjNCE损失函数，并将其应用于SupCon。

Result: ProjNCE在多个数据集和设置中优于SupCon和标准交叉熵训练。

Conclusion: ProjNCE从互信息解释和投影设计两方面改进了SupCon，为对比学习提供了广泛适用的优化方案。

Abstract: Self-supervised contrastive learning (SSCL) has emerged as a powerful
paradigm for representation learning and has been studied from multiple
perspectives, including mutual information and geometric viewpoints. However,
supervised contrastive (SupCon) approaches have received comparatively little
attention in this context: for instance, while InfoNCE used in SSCL is known to
form a lower bound on mutual information (MI), the relationship between SupCon
and MI remains unexplored. To address this gap, we introduce ProjNCE, a
generalization of the InfoNCE loss that unifies supervised and self-supervised
contrastive objectives by incorporating projection functions and an adjustment
term for negative pairs. We prove that ProjNCE constitutes a valid MI bound and
affords greater flexibility in selecting projection strategies for class
embeddings. Building on this flexibility, we further explore the centroid-based
class embeddings in SupCon by exploring a variety of projection methods.
Extensive experiments on multiple datasets and settings demonstrate that
ProjNCE consistently outperforms both SupCon and standard cross-entropy
training. Our work thus refines SupCon along two complementary
perspective--mutual information interpretation and projection design--and
offers broadly applicable improvements whenever SupCon serves as the
foundational contrastive objective.

</details>


### [144] [Metritocracy: Representative Metrics for Lite Benchmarks](https://arxiv.org/abs/2506.09813)
*Ariel Procaccia,Benjamin Schiffer,Serena Wang,Shirley Zhang*

Key words: LLM评估, 社会选择理论, 指标子集选择, 代表性, 比例性

TL;DR: 该研究通过社会选择理论的形式化方法，提出了两种评价指标子集选择的代表性定义，并探讨了其在现实案例中的应用。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决LLM评估中选择指标子集时‘代表性’定义不明确的问题。

Method: 引入位置代表性和位置比例性两种形式化定义，探讨其理论界限，并通过案例研究验证。

Result: 证明了在最坏情况下满足这些性质所需的最小指标数量，并在实际案例中验证了方法的有效性。

Conclusion: 提出的形式化方法为指标子集选择提供了明确的代表性和比例性保证，适用于LLM评估和其他领域。

Abstract: A common problem in LLM evaluation is how to choose a subset of metrics from
a full suite of possible metrics. Subset selection is usually done for
efficiency or interpretability reasons, and the goal is often to select a
``representative'' subset of metrics. However, ``representative'' is rarely
clearly defined. In this work, we use ideas from social choice theory to
formalize two notions of representation for the selection of a subset of
evaluation metrics. We first introduce positional representation, which
guarantees every alternative is sufficiently represented at every position
cutoff. We then introduce positional proportionality, which guarantees no
alternative is proportionally over- or under-represented by more than a small
error at any position. We prove upper and lower bounds on the smallest number
of metrics needed to guarantee either of these properties in the worst case. We
also study a generalized form of each property that allows for additional input
on groups of metrics that must be represented. Finally, we tie theory to
practice through real-world case studies on both LLM evaluation and hospital
quality evaluation.

</details>


### [145] [Identifiability Challenges in Sparse Linear Ordinary Differential Equations](https://arxiv.org/abs/2506.09816)
*Cecilia Casolo,Sören Becker,Niki Kilbertus*

Key words: 稀疏线性ODE, 可识别性, 动力学系统, 数据驱动建模

TL;DR: 本文探讨了稀疏线性常微分方程（ODE）的可识别性问题，发现与稠密矩阵不同，稀疏系统在某些情况下是不可识别的，并提供了理论支持和实证结果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究稀疏线性ODE的可识别性，填补现有研究空白，尤其是在生物学、社会学和物理学中广泛出现的稀疏系统中。

Method: 通过理论分析和实证研究，研究了稀疏线性ODE的可识别性及其在实际估计方法中的表现。

Result: 稀疏线性ODE在某些情况下是不可识别的，理论上的不可识别性在实践中也得到证实。

Conclusion: 研究结果对数据驱动的动力学系统建模提出了新的思考，需要重新评估学习到的线性ODE的可信度。

Abstract: Dynamical systems modeling is a core pillar of scientific inquiry across
natural and life sciences. Increasingly, dynamical system models are learned
from data, rendering identifiability a paramount concept. For systems that are
not identifiable from data, no guarantees can be given about their behavior
under new conditions and inputs, or about possible control mechanisms to steer
the system. It is known in the community that "linear ordinary differential
equations (ODE) are almost surely identifiable from a single trajectory."
However, this only holds for dense matrices. The sparse regime remains
underexplored, despite its practical relevance with sparsity arising naturally
in many biological, social, and physical systems. In this work, we address this
gap by characterizing the identifiability of sparse linear ODEs. Contrary to
the dense case, we show that sparse systems are unidentifiable with a positive
probability in practically relevant sparsity regimes and provide lower bounds
for this probability. We further study empirically how this theoretical
unidentifiability manifests in state-of-the-art methods to estimate linear ODEs
from data. Our results corroborate that sparse systems are also practically
unidentifiable. Theoretical limitations are not resolved through inductive
biases or optimization dynamics. Our findings call for rethinking what can be
expected from data-driven dynamical system modeling and allows for quantitative
assessments of how much to trust a learned linear ODE.

</details>


### [146] [Weighted Loss Methods for Robust Federated Learning under Data Heterogeneity](https://arxiv.org/abs/2506.09824)
*Johan Erbani,Sonia Ben Mokhtar,Pierre-Edouard Portier,Elod Egyed-Zsigmond,Diana Nurbakova*

Key words: 联邦学习, 拜占庭攻击, 数据异质性, 梯度对齐

TL;DR: 该论文提出了一种名为WoLA的加权损失方法，旨在解决联邦学习中由于数据异质性导致的诚实梯度与拜占庭梯度难以区分的问题，显著提高了在异质环境下的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 联邦学习中，拜占庭参与者可能贡献有害梯度，影响模型收敛。传统方法在异质环境下难以区分诚实梯度与拜占庭梯度，需要一种新的解决方案。

Method: 提出了Worker Label Alignement Loss (WoLA)，一种加权损失函数，用于对齐诚实工作者的梯度，从而更易于识别拜占庭梯度。

Result: WoLA方法在异质环境下显著优于现有方法，并通过理论和实验验证了其有效性。

Conclusion: WoLA提供了一种有效的解决方案，解决了联邦学习中的拜占庭攻击问题，尤其在数据异质性场景下表现优异。

Abstract: Federated learning (FL) is a machine learning paradigm that enables multiple
data holders to collaboratively train a machine learning model without sharing
their training data with external parties. In this paradigm, workers locally
update a model and share with a central server their updated gradients (or
model parameters). While FL seems appealing from a privacy perspective, it
opens a number of threats from a security perspective as (Byzantine)
participants can contribute poisonous gradients (or model parameters) harming
model convergence. Byzantine-resilient FL addresses this issue by ensuring that
the training proceeds as if Byzantine participants were absent. Towards this
purpose, common strategies ignore outlier gradients during model aggregation,
assuming that Byzantine gradients deviate more from honest gradients than
honest gradients do from each other. However, in heterogeneous settings, honest
gradients may differ significantly, making it difficult to distinguish honest
outliers from Byzantine ones. In this paper, we introduce the Worker Label
Alignement Loss (WoLA), a weighted loss that aligns honest worker gradients
despite data heterogeneity, which facilitates the identification of Byzantines'
gradients. This approach significantly outperforms state-of-the-art methods in
heterogeneous settings. In this paper, we provide both theoretical insights and
empirical evidence of its effectiveness.

</details>


### [147] [Guided Graph Compression for Quantum Graph Neural Networks](https://arxiv.org/abs/2506.09862)
*Mikel Casals,Vasilis Belis,Elias F. Combarro,Eduard Alarcón,Sofia Vallecorsa,Michele Grossi*

Key words: Graph Neural Networks, Quantum Computing, Guided Graph Compression, Jet Tagging, High Energy Physics

TL;DR: 提出了一个名为GGC的框架，通过图自动编码器压缩图数据节点和特征维度，提升下游分类任务性能，适用于量子或经典分类器，并在高能物理任务中表现出色。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决GNN在大图上内存需求高及GPU稀疏矩阵操作低效的问题，同时克服量子硬件对数据编码维度的限制。

Method: 引入GGC框架，利用图自动编码器压缩节点数量和特征维度，并优化下游分类任务性能。

Result: GGC在Jet Tagging任务中表现优于单独使用自动编码器和经典GNN分类器的基准方法。

Conclusion: GGC不仅提升了分类性能，还支持在真实数据集上测试新型QGNN方法。

Abstract: Graph Neural Networks (GNNs) are effective for processing graph-structured
data but face challenges with large graphs due to high memory requirements and
inefficient sparse matrix operations on GPUs. Quantum Computing (QC) offers a
promising avenue to address these issues and inspires new algorithmic
approaches. In particular, Quantum Graph Neural Networks (QGNNs) have been
explored in recent literature. However, current quantum hardware limits the
dimension of the data that can be effectively encoded. Existing approaches
either simplify datasets manually or use artificial graph datasets. This work
introduces the Guided Graph Compression (GGC) framework, which uses a graph
autoencoder to reduce both the number of nodes and the dimensionality of node
features. The compression is guided to enhance the performance of a downstream
classification task, which can be applied either with a quantum or a classical
classifier. The framework is evaluated on the Jet Tagging task, a
classification problem of fundamental importance in high energy physics that
involves distinguishing particle jets initiated by quarks from those by gluons.
The GGC is compared against using the autoencoder as a standalone preprocessing
step and against a baseline classical GNN classifier. Our numerical results
demonstrate that GGC outperforms both alternatives, while also facilitating the
testing of novel QGNN ansatzes on realistic datasets.

</details>


### [148] [Machine Learning-Based Classification of Oils Using Dielectric Properties and Microwave Resonant Sensing](https://arxiv.org/abs/2506.09867)
*Amit Baran Dey,Wasim Arif,Rakhesh Singh Kshetrimayum*

Key words: 机器学习，微波共振传感器，油分类，介电特性，工业应用

TL;DR: 提出一种基于机器学习的微波共振传感器方法，通过油样的介电特性进行分类，分类精度高达99.41%。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 利用油的分子组成对介电行为的影响，开发一种非破坏性、低功耗且适用于工业实时应用的油分类方法。

Method: 通过微波共振传感器捕捉油样的共振频率和振幅变化，提取特征后输入多种机器学习分类器。

Result: 随机森林分类器在油样分类中达到99.41%的高精度。

Conclusion: 该方法在工业环境中具有快速、可靠且高效的油分类潜力。

Abstract: This paper proposes a machine learning-based methodology for the
classification of various oil samples based on their dielectric properties,
utilizing a microwave resonant sensor. The dielectric behaviour of oils,
governed by their molecular composition, induces distinct shifts in the
sensor's resonant frequency and amplitude response. These variations are
systematically captured and processed to extract salient features, which serve
as inputs for multiple machine learning classifiers. The microwave resonant
sensor operates in a non-destructive, low-power manner, making it particularly
well-suited for real-time industrial applications. A comprehensive dataset is
developed by varying the permittivity of oil samples and acquiring the
corresponding sensor responses. Several classifiers are trained and evaluated
using the extracted resonant features to assess their capability in
distinguishing between oil types. Experimental results demonstrate that the
proposed approach achieves a high classification accuracy of 99.41% with the
random forest classifier, highlighting its strong potential for automated oil
identification. The system's compact form factor, efficiency, and high
performance underscore its viability for fast and reliable oil characterization
in industrial environments.

</details>


### [149] [Private Aggregation for Byzantine-Resilient Heterogeneous Federated Learning](https://arxiv.org/abs/2506.09870)
*Maximilian Egger,Rawad Bitar*

Key words: 联邦学习, 拜占庭容忍, 隐私保护, 安全聚合, 可验证的秘密共享, 对称私有信息检索

TL;DR: 该论文提出了一种多阶段方法，结合了可验证的秘密共享、安全聚合和定制的对称私有信息检索方案，以在数据异构情况下实现信息论隐私保证和拜占庭容忍。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在联邦学习中，如何在保证客户端数据隐私的同时抵御拜占庭攻击是一个基本挑战，尤其是在数据异构的情况下。

Method: 通过多阶段方法，结合可验证的秘密共享、安全聚合和对称私有信息检索方案，设计了一个新的隐私保护机制。

Result: 该方法在各种攻击下表现优异，优于之前已知的技术，并通过零阶估计方法降低了通信开销。

Conclusion: 该多阶段方法在数据异构情况下实现了信息论隐私保证和拜占庭容忍，同时通过通信优化使其可扩展。

Abstract: Ensuring resilience to Byzantine clients while maintaining the privacy of the
clients' data is a fundamental challenge in federated learning (FL). When the
clients' data is homogeneous, suitable countermeasures were studied from an
information-theoretic perspective utilizing secure aggregation techniques while
ensuring robust aggregation of the clients' gradients. However, the
countermeasures used fail when the clients' data is heterogeneous. Suitable
pre-processing techniques, such as nearest neighbor mixing, were recently shown
to enhance the performance of those countermeasures in the heterogeneous
setting. Nevertheless, those pre-processing techniques cannot be applied with
the introduced privacy-preserving mechanisms.
  We propose a multi-stage method encompassing a careful co-design of
verifiable secret sharing, secure aggregation, and a tailored symmetric private
information retrieval scheme to achieve information-theoretic privacy
guarantees and Byzantine resilience under data heterogeneity. We evaluate the
effectiveness of our scheme on a variety of attacks and show how it outperforms
the previously known techniques. Since the communication overhead of secure
aggregation is non-negligible, we investigate the interplay with zero-order
estimation methods that reduce the communication cost in state-of-the-art FL
tasks and thereby make private aggregation scalable.

</details>


### [150] [Learning single-index models via harmonic decomposition](https://arxiv.org/abs/2506.09887)
*Nirmit Joshi,Hugo Koubbi,Theodor Misiakiewicz,Nathan Srebro*

Key words: 单指标模型, 球面谐波, 统计复杂性, 计算复杂性, 高斯输入

TL;DR: 论文提出以球面谐波而非赫米特多项式为基础，分析单指标模型的学习复杂性，设计了两种最优估计器，并在高斯输入下验证了新现象。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究单指标模型学习中，传统基于赫米特多项式的方法未能捕捉其旋转对称性，提出以球面谐波为基础的新视角。

Method: 引入基于张量展开和在线SGD的两类估计器，分别优化样本复杂度或计算时间。

Result: 理论分析表明，同时实现样本和计算最优的估计器可能不存在，高斯输入下验证了新现象。

Conclusion: 球面谐波为基础的分析提供了更自然的框架，改进了对单指标模型学习的理解。

Abstract: We study the problem of learning single-index models, where the label $y \in
\mathbb{R}$ depends on the input $\boldsymbol{x} \in \mathbb{R}^d$ only through
an unknown one-dimensional projection $\langle
\boldsymbol{w}_*,\boldsymbol{x}\rangle$. Prior work has shown that under
Gaussian inputs, the statistical and computational complexity of recovering
$\boldsymbol{w}_*$ is governed by the Hermite expansion of the link function.
In this paper, we propose a new perspective: we argue that "spherical
harmonics" -- rather than "Hermite polynomials" -- provide the natural basis
for this problem, as they capture its intrinsic "rotational symmetry". Building
on this insight, we characterize the complexity of learning single-index models
under arbitrary spherically symmetric input distributions. We introduce two
families of estimators -- based on tensor unfolding and online SGD -- that
respectively achieve either optimal sample complexity or optimal runtime, and
argue that estimators achieving both may not exist in general. When specialized
to Gaussian inputs, our theory not only recovers and clarifies existing results
but also reveals new phenomena that had previously been overlooked.

</details>


### [151] [Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized Rejection Sampling](https://arxiv.org/abs/2506.09998)
*Tim Z. Xiao,Johannes Zenn,Zhen Liu,Weiyang Liu,Robert Bamler,Bernhard Schölkopf*

Key words: 大型语言模型,拒绝抽样,概率分布,随机性

TL;DR: 本文研究了大型语言模型（LLM）在描述概率分布时表现优异，但在生成可靠样本时存在困难的问题，提出了一种改进的拒绝抽样方法VRS。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: LLM在描述概率分布方面表现良好，但在生成可靠样本时存在不足，限制了其在需要随机性的任务中的应用。本文旨在解决这一局限。

Method: 引入Verbalized Rejection Sampling (VRS)，这是一种基于自然语言的改进拒绝抽样方法，通过让LLM对样本进行推理和接受或拒绝，减少采样偏差。

Result: VRS显著减少了采样偏差，且在温和假设下优于直接抽样，改进源于算法和提示设计的结合。

Conclusion: VRS展示了如何将经典概率工具嵌入LLM工作流以提高可靠性，无需复杂提示或访问模型内部。

Abstract: Large language models (LLMs) can often accurately describe probability
distributions using natural language, yet they still struggle to generate
faithful samples from them. This mismatch limits their use in tasks requiring
reliable stochasticity, such as Monte Carlo methods, agent-based simulations,
and randomized decision-making. We investigate this gap between knowledge and
sampling in the context of Bernoulli distributions. We introduce Verbalized
Rejection Sampling (VRS), a natural-language adaptation of classical rejection
sampling that prompts the LLM to reason about and accept or reject proposed
samples. Despite relying on the same Bernoulli mechanism internally, VRS
substantially reduces sampling bias across models. We provide theoretical
analysis showing that, under mild assumptions, VRS improves over direct
sampling, with gains attributable to both the algorithm and prompt design. More
broadly, our results show how classical probabilistic tools can be verbalized
and embedded into LLM workflows to improve reliability, without requiring
access to model internals or heavy prompt engineering.

</details>


### [152] [Causal Climate Emulation with Bayesian Filtering](https://arxiv.org/abs/2506.09891)
*Sebastian Hickman,Ilija Trajkovic,Julia Kaltenborn,Francis Pelletier,Alex Archibald,Yaniv Gurwicz,Peer Nowack,David Rolnick,Julien Boussard*

Key words: 气候模型、机器学习、因果表示学习、物理学信息、贝叶斯滤波器

TL;DR: 这篇论文提出了一种基于因果表示学习的可解释气候模型模拟器，通过物理学信息方法和贝叶斯滤波器实现稳定的长期自回归模拟。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统气候模型计算成本高，机器学习虽能快速模拟气候数据，但当前方法无法融入物理学信息的因果关系。

Method: 开发了基于因果表示学习的模拟器，结合物理学信息和贝叶斯滤波器，进行长期自回归模拟。

Result: 模拟器能准确学习气候动力学，并在合成数据集和两个广泛使用的气候模型数据上验证了其各组件的重要性。

Conclusion: 该模拟器在准确性和可解释性方面表现出色，为气候预测提供了高效工具。

Abstract: Traditional models of climate change use complex systems of coupled equations
to simulate physical processes across the Earth system. These simulations are
highly computationally expensive, limiting our predictions of climate change
and analyses of its causes and effects. Machine learning has the potential to
quickly emulate data from climate models, but current approaches are not able
to incorporate physics-informed causal relationships. Here, we develop an
interpretable climate model emulator based on causal representation learning.
We derive a physics-informed approach including a Bayesian filter for stable
long-term autoregressive emulation. We demonstrate that our emulator learns
accurate climate dynamics, and we show the importance of each one of its
components on a realistic synthetic dataset and data from two widely deployed
climate models.

</details>


### [153] [A look at adversarial attacks on radio waveforms from discrete latent space](https://arxiv.org/abs/2506.09896)
*Attanasia Garuso,Silvija Kokalj-Filipovic,Yagna Kaasaragadda*

Key words: VQVAE, 对抗攻击, 射频数据, 潜在空间, 分类器

TL;DR: 本文研究VQVAE在对抗攻击中对射频数据的防御能力，通过保持相位和未保持相位的攻击对比，发现VQVAE能显著降低攻击效果，并揭示离散潜在空间的特性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 评估VQVAE在高信噪比射频数据中对对抗攻击的抑制能力，探索其在数字调制波形中的防御潜力。

Method: 设计保持相位和不保持相位的对抗攻击，测试分类器在原始数据和VQVAE重建数据上的准确率，分析潜在空间的分布特性。

Result: VQVAE显著降低了对抗攻击的效果，且在潜在空间中观察到有助于攻击检测的特性。

Conclusion: VQVAE能有效抑制对抗攻击，潜在空间分析为攻击检测提供了新思路。

Abstract: Having designed a VQVAE that maps digital radio waveforms into discrete
latent space, and yields a perfectly classifiable reconstruction of the
original data, we here analyze the attack suppressing properties of VQVAE when
an adversarial attack is performed on high-SNR radio-frequency (RF)
data-points. To target amplitude modulations from a subset of digitally
modulated waveform classes, we first create adversarial attacks that preserve
the phase between the in-phase and quadrature component whose values are
adversarially changed. We compare them with adversarial attacks of the same
intensity where phase is not preserved. We test the classification accuracy of
such adversarial examples on a classifier trained to deliver 100% accuracy on
the original data. To assess the ability of VQVAE to suppress the strength of
the attack, we evaluate the classifier accuracy on the reconstructions by VQVAE
of the adversarial datapoints and show that VQVAE substantially decreases the
effectiveness of the attack. We also compare the I/Q plane diagram of the
attacked data, their reconstructions and the original data. Finally, using
multiple methods and metrics, we compare the probability distribution of the
VQVAE latent space with and without attack. Varying the attack strength, we
observe interesting properties of the discrete space, which may help detect the
attacks.

</details>


### [154] ["What are my options?": Explaining RL Agents with Diverse Near-Optimal Alternatives (Extended)](https://arxiv.org/abs/2506.09901)
*Noel Brindise,Vijeth Hebbar,Riya Shah,Cedric Langbort*

Key words: 可解释强化学习,多样化策略,轨迹规划,奖励塑造,ε-最优性

TL;DR: 本文讨论了名为Diverse Near-Optimal Alternatives（DNA）的新型可解释强化学习方法，通过生成多样化的轨迹选项帮助人类用户理解代理的决策，并展示了其在模拟中的有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: DNA的动机是为轨迹规划代理提供多样化的策略选项，通过解释这些选项来增强可解释性，并为强化学习探索和自适应规划开辟新可能性。

Method: DNA采用奖励塑造方法，在局部修改的Q学习问题中求解具有ε-最优性的不同策略，生成在欧几里得空间中具有多样性的轨迹。

Result: 实验表明，DNA成功生成了具有显著差异的策略，并在模拟中验证了其有效性，同时与质量多样性领域的相关方法进行了简要比较。

Conclusion: DNA不仅提升了强化学习的可解释性，还为探索和自适应规划提供了新的研究方向。

Abstract: In this work, we provide an extended discussion of a new approach to
explainable Reinforcement Learning called Diverse Near-Optimal Alternatives
(DNA), first proposed at L4DC 2025. DNA seeks a set of reasonable "options" for
trajectory-planning agents, optimizing policies to produce qualitatively
diverse trajectories in Euclidean space. In the spirit of explainability, these
distinct policies are used to "explain" an agent's options in terms of
available trajectory shapes from which a human user may choose. In particular,
DNA applies to value function-based policies on Markov decision processes where
agents are limited to continuous trajectories. Here, we describe DNA, which
uses reward shaping in local, modified Q-learning problems to solve for
distinct policies with guaranteed epsilon-optimality. We show that it
successfully returns qualitatively different policies that constitute
meaningfully different "options" in simulation, including a brief comparison to
related approaches in the stochastic optimization field of Quality Diversity.
Beyond the explanatory motivation, this work opens new possibilities for
exploration and adaptive planning in RL.

</details>


### [155] [Apollo: A Posteriori Label-Only Membership Inference Attack Towards Machine Unlearning](https://arxiv.org/abs/2506.09923)
*Liou Tang,James Joshi,Ashish Kundu*

Key words: Machine Unlearning, Privacy Attack, Membership Inference, Apollo

TL;DR: 本文提出了一种名为Apollo的新型隐私攻击方法，通过在严格威胁模型下仅利用标签输出来推断数据样本是否被遗忘，提高了攻击的可行性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的遗忘学习隐私推断攻击依赖访问原始模型和遗忘模型的较弱威胁模型，限制了实际场景中的可行性。本文旨在提出一种更严格的威胁模型下的攻击方法。

Method: 提出了一种名为Apollo的后验标签仅成员推断攻击，攻击者仅需访问遗忘模型的标签输出即可推断样本的遗忘状态。

Result: 实验表明，Apollo在仅访问标签输出的情况下，仍能对遗忘样本的成员状态实现较高的推断精度。

Conclusion: Apollo攻击证明了即使在严格威胁模型下，遗忘学习仍可能面临隐私泄露风险。

Abstract: Machine Unlearning (MU) aims to update Machine Learning (ML) models following
requests to remove training samples and their influences on a trained model
efficiently without retraining the original ML model from scratch. While MU
itself has been employed to provide privacy protection and regulatory
compliance, it can also increase the attack surface of the model. Existing
privacy inference attacks towards MU that aim to infer properties of the
unlearned set rely on the weaker threat model that assumes the attacker has
access to both the unlearned model and the original model, limiting their
feasibility toward real-life scenarios. We propose a novel privacy attack, A
Posteriori Label-Only Membership Inference Attack towards MU, Apollo, that
infers whether a data sample has been unlearned, following a strict threat
model where an adversary has access to the label-output of the unlearned model
only. We demonstrate that our proposed attack, while requiring less access to
the target model compared to previous attacks, can achieve relatively high
precision on the membership status of the unlearned samples.

</details>


### [156] [Bayesian Probabilistic Matrix Factorization](https://arxiv.org/abs/2506.09928)
*Ruixuan Xu,Xiangxiang Weng*

Key words: 矩阵分解,概率矩阵分解,贝叶斯推断,MCMC,VI

TL;DR: 比较了MCMC和VI两种贝叶斯推断方法在PMF中的性能，VI收敛更快，MCMC后验估计更准确。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统概率矩阵分解(PMF)在高维积分下后验分布难以计算，需解决这一问题。

Method: 采用Markov Chain Monte Carlo (MCMC)和Variational Inference (VI)两种贝叶斯推断方法来近似后验分布。

Result: 实验结果显示，VI收敛速度更快，而MCMC提供的后验估计更准确。

Conclusion: 根据需求选择方法：追求速度用VI，追求准确性用MCMC。

Abstract: Matrix factorization is a widely used technique in recommendation systems.
Probabilistic Matrix Factorization (PMF) [1] extends traditional matrix
factorization by incorporating probability distributions over latent factors,
allowing for uncertainty quantification. However, computing the posterior
distribution is intractable due to the high-dimensional integral. To address
this, we employ two Bayesian inference methods: Markov Chain Monte Carlo (MCMC)
[2] and Variational Inference (VI) [3] to approximate the posterior. We
evaluate their performance on MovieLens dataset and compare their convergence
speed, predictive accuracy, and computational efficiency. Experimental results
demonstrate that VI offers faster convergence, while MCMC provides more
accurate posterior estimates.

</details>


### [157] [The Sample Complexity of Online Strategic Decision Making with Information Asymmetry and Knowledge Transportability](https://arxiv.org/abs/2506.09940)
*Jiachen Hu,Rui Ai,Han Zhong,Xiaoyu Chen,Liwei Wang,Zhaoran Wang,Zhuoran Yang*

Key words: 信息不对称, 知识迁移, 在线学习, 强化学习, 战略交互

TL;DR: 本文研究了多智能体系统中信息不对称和知识迁移的挑战，提出了一种样本高效的在线学习算法，以在战略交互模型中学习和优化策略。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 信息不对称和知识迁移问题是多智能体系统中的核心挑战，尤其是在经济学和社会科学领域。本文旨在解决在非独立同分布行为和知识迁移需求下的学习问题。

Method: 提出了一种样本高效的算法，专注于在信息不对称和知识迁移的复杂环境中学习系统动态，并在在线战略交互模型中实现策略优化。

Result: 该算法在样本复杂度为$O(1/\epsilon^2)$的情况下，能够学习到$\epsilon$-最优策略。

Conclusion: 本文的方法在理论和实践中均表现出色，为解决信息不对称和知识迁移问题提供了有效的解决方案。

Abstract: Information asymmetry is a pervasive feature of multi-agent systems,
especially evident in economics and social sciences. In these settings, agents
tailor their actions based on private information to maximize their rewards.
These strategic behaviors often introduce complexities due to confounding
variables. Simultaneously, knowledge transportability poses another significant
challenge, arising from the difficulties of conducting experiments in target
environments. It requires transferring knowledge from environments where
empirical data is more readily available. Against these backdrops, this paper
explores a fundamental question in online learning: Can we employ non-i.i.d.
actions to learn about confounders even when requiring knowledge transfer? We
present a sample-efficient algorithm designed to accurately identify system
dynamics under information asymmetry and to navigate the challenges of
knowledge transfer effectively in reinforcement learning, framed within an
online strategic interaction model. Our method provably achieves learning of an
$\epsilon$-optimal policy with a tight sample complexity of $O(1/\epsilon^2)$.

</details>


### [158] [Canonical Latent Representations in Conditional Diffusion Models](https://arxiv.org/abs/2506.09955)
*Yitao Xu,Tong Zhang,Ehsan Pajouheshgar,Sabine Süsstrunk*

Key words: conditional diffusion models, CLAReps, feature distillation, adversarial robustness

TL;DR: 提出CLAReps，通过CDM提取保留类别信息的潜在表征，开发了CaDistill框架，利用少量数据实现高效的知识蒸馏。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决CDM在生成任务中类别特征与无关信息纠缠的问题。

Method: 提出CLAReps潜在表征，设计CaDistill框架进行特征蒸馏。

Result: 学生在仅用10%数据的情况下，表现出强对抗鲁棒性和泛化能力。

Conclusion: CDM可作为高效教学工具，促进鲁棒表征学习。

Abstract: Conditional diffusion models (CDMs) have shown impressive performance across
a range of generative tasks. Their ability to model the full data distribution
has opened new avenues for analysis-by-synthesis in downstream discriminative
learning. However, this same modeling capacity causes CDMs to entangle the
class-defining features with irrelevant context, posing challenges to
extracting robust and interpretable representations. To this end, we identify
Canonical LAtent Representations (CLAReps), latent codes whose internal CDM
features preserve essential categorical information while discarding
non-discriminative signals. When decoded, CLAReps produce representative
samples for each class, offering an interpretable and compact summary of the
core class semantics with minimal irrelevant details. Exploiting CLAReps, we
develop a novel diffusion-based feature-distillation paradigm, CaDistill. While
the student has full access to the training set, the CDM as teacher transfers
core class knowledge only via CLAReps, which amounts to merely 10 % of the
training data in size. After training, the student achieves strong adversarial
robustness and generalization ability, focusing more on the class signals
instead of spurious background cues. Our findings suggest that CDMs can serve
not just as image generators but also as compact, interpretable teachers that
can drive robust representation learning.

</details>


### [159] [Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation](https://arxiv.org/abs/2506.09991)
*Xinyu Yang,Yuwei An,Hongyi Liu,Tianqi Chen,Beidi Chen*

Key words: Multiverse, 并行生成, MapReduce, AR-LLMs, AIME24

TL;DR: Multiverse是一种新型并行生成模型，通过MapReduce范式实现高效并行，性能媲美领先的自回归模型，且速度更快。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 受自回归大语言模型（AR-LLMs）隐式并行性的启发，提出Multiverse以实现原生并行生成，提升效率。

Method: 采用MapReduce范式，分三个阶段：Map（任务分解）、Process（并行执行）、Reduce（结果合成），并设计了Multiverse Attention和Multiverse Engine。

Result: Multiverse-32B在3小时微调后，性能与同类AR-LLMs相当，速度提升2倍，且开源了整个生态系统。

Conclusion: Multiverse展示了高效并行生成的潜力，为未来模型设计提供了新方向。

Abstract: Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit
parallelism in sequential generation. Inspired by this, we introduce
Multiverse, a new generative model that enables natively parallel generation.
Multiverse internalizes a MapReduce paradigm, generating automatically through
three stages: (i) a Map stage for adaptive task decomposition, (ii) a Process
stage for parallel subtask execution, and (iii) a Reduce stage for lossless
result synthesis. Next, we build a real-world Multiverse reasoning model with
co-design of data, algorithm, and system, enabling rapid and seamless transfer
from frontier AR-LLMs. Starting from sequential reasoning chains, we create
Multiverse 1K by converting them into structured training data using an
automated LLM-assisted pipeline, avoiding costly human annotations.
Algorithmically, we design Multiverse Attention to separate parallel reasoning
steps while keeping compatibility with causal attention for efficient training.
Systematically, we implement Multiverse Engine to enable parallel inference. It
features a dedicated scheduler that dynamically switches between sequential and
parallel generation, triggered directly by the model. After a 3-hour
fine-tuning with 1K examples, our Multiverse-32B stands as the only
open-sourced non-AR model achieving performance on par with leading AR-LLMs of
the same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively.
Moreover, our budget control experiments show that Multiverse-32B exhibits
superior scaling, outperforming AR-LLMs by 1.87% on average using the same
context length. Such scaling further leads to practical efficiency gain,
achieving up to 2x speedup across varying batch sizes. We have open-sourced the
entire Multiverse ecosystem, including data, model weights, engine, supporting
tools, as well as complete data curation prompts and detailed training and
evaluation recipes.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [160] [Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism](https://arxiv.org/abs/2506.09176)
*Haoyuan Cai,Zhenghao Peng,Bolei Zhou*

Key words: 

TL;DR: AIM方法通过自适应请求人类干预，显著降低了监督成本并提高了学习效率。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前交互模仿学习（IIL）方法对人类的认知要求较高，需改进以减少监督负担。

Method: 提出自适应干预机制（AIM），利用代理Q函数评估与专家的对齐程度，动态请求干预。

Result: AIM在实验中减少了40%的人类接管成本，并提高了学习效率和安全性。

Conclusion: AIM通过动态请求干预有效提升了IIL的效率和质量，减少了专家数据需求。

Abstract: Interactive Imitation Learning (IIL) allows agents to acquire desired
behaviors through human interventions, but current methods impose high
cognitive demands on human supervisors. We propose the Adaptive Intervention
Mechanism (AIM), a novel robot-gated IIL algorithm that learns an adaptive
criterion for requesting human demonstrations. AIM utilizes a proxy Q-function
to mimic the human intervention rule and adjusts intervention requests based on
the alignment between agent and human actions. By assigning high Q-values when
the agent deviates from the expert and decreasing these values as the agent
becomes proficient, the proxy Q-function enables the agent to assess the
real-time alignment with the expert and request assistance when needed. Our
expert-in-the-loop experiments reveal that AIM significantly reduces expert
monitoring efforts in both continuous and discrete control tasks. Compared to
the uncertainty-based baseline Thrifty-DAgger, our method achieves a 40%
improvement in terms of human take-over cost and learning efficiency.
Furthermore, AIM effectively identifies safety-critical states for expert
assistance, thereby collecting higher-quality expert demonstrations and
reducing overall expert data and environment interactions needed. Code and demo
video are available at https://github.com/metadriverse/AIM.

</details>


### [161] [Comment on The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.09250)
*C. Opus,A. Lawsen*

Key words: 大型推理模型, 实验设计, 准确性崩溃, 规划谜题, 评估框架

TL;DR: 论文指出Shojaee等人关于大型推理模型（LRMs）在复杂规划谜题上出现“准确性崩溃”的结论源于实验设计的问题，而非模型推理能力的根本缺陷。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 为了澄清大型推理模型在复杂任务中的真实表现，揭示现有实验设计的局限性。

Method: 分析Shojaee等人的实验设计，识别了三个关键问题：输出令牌限制、自动评估框架的缺陷以及基准测试中包含不可解问题。改进实验设计后重新评估模型表现。

Result: 在控制实验设计缺陷后，模型在之前报告为完全失败的Tower of Hanoi任务上表现出高准确性。

Conclusion: 实验设计对评估AI推理能力至关重要，需避免因设计缺陷导致的误解。

Abstract: Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit
"accuracy collapse" on planning puzzles beyond certain complexity thresholds.
We demonstrate that their findings primarily reflect experimental design
limitations rather than fundamental reasoning failures. Our analysis reveals
three critical issues: (1) Tower of Hanoi experiments systematically exceed
model output token limits at reported failure points, with models explicitly
acknowledging these constraints in their outputs; (2) The authors' automated
evaluation framework fails to distinguish between reasoning failures and
practical constraints, leading to misclassification of model capabilities; (3)
Most concerningly, their River Crossing benchmarks include mathematically
impossible instances for N > 5 due to insufficient boat capacity, yet models
are scored as failures for not solving these unsolvable problems. When we
control for these experimental artifacts, by requesting generating functions
instead of exhaustive move lists, preliminary experiments across multiple
models indicate high accuracy on Tower of Hanoi instances previously reported
as complete failures. These findings highlight the importance of careful
experimental design when evaluating AI reasoning capabilities.

</details>


### [162] [Ming-Omni: A Unified Multimodal Model for Perception and Generation](https://arxiv.org/abs/2506.09344)
*Inclusion AI,Biao Gong,Cheng Zou,Chuanyang Zheng,Chunluan Zhou,Canxiang Yan,Chunxiang Jin,Chunjie Shen,Dandan Zheng,Fudong Wang,Furong Xu,GuangMing Yao,Jun Zhou,Jingdong Chen,Jianxin Sun,Jiajia Liu,Jianjiang Zhu,Jun Peng,Kaixiang Ji,Kaiyou Song,Kaimeng Ren,Libin Wang,Lixiang Ru,Lele Xie,Longhua Tan,Lyuxin Xue,Lan Wang,Mochen Bai,Ning Gao,Pei Chen,Qingpei Guo,Qinglong Zhang,Qiang Xu,Rui Liu,Ruijie Xiong,Sirui Gao,Tinghao Liu,Taisong Li,Weilong Chai,Xinyu Xiao,Xiaomei Wang,Xiaoxue Chen,Xiao Lu,Xiaoyu Li,Xingning Dong,Xuzheng Yu,Yi Yuan,Yuting Gao,Yunxiao Sun,Yipeng Chen,Yifei Wu,Yongjie Lyu,Ziping Ma,Zipeng Feng,Zhijiang Fang,Zhihao Qiu,Ziyuan Huang,Zhengyu He*

Key words: 多模态模型, 语音生成, 图像生成, 统一框架, 开源

TL;DR: Ming-Omni是一个统一的多模态模型，支持处理图像、文本、音频和视频，并在语音和图像生成方面表现出色。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 实现一个单一模型高效处理和多模态输入融合，支持多样任务，超越传统多模态模型的限制。

Method: 采用专用编码器提取多模态令牌，通过MoE架构（Ling）和新提出的模态特定路由器处理，集成先进音频解码器和Ming-Lite-Uni图像生成模块。

Result: 实验显示Ming-Omni在统一感知和生成任务中表现出色，支持音频和图像生成等功能。

Conclusion: Ming-Omni是一个开源模型，首次在模态支持上达到GPT-4o水平，为社区研究和开发提供了强大工具。

Abstract: We propose Ming-Omni, a unified multimodal model capable of processing
images, text, audio, and video, while demonstrating strong proficiency in both
speech and image generation. Ming-Omni employs dedicated encoders to extract
tokens from different modalities, which are then processed by Ling, an MoE
architecture equipped with newly proposed modality-specific routers. This
design enables a single model to efficiently process and fuse multimodal inputs
within a unified framework, thereby facilitating diverse tasks without
requiring separate models, task-specific fine-tuning, or structural redesign.
Importantly, Ming-Omni extends beyond conventional multimodal models by
supporting audio and image generation. This is achieved through the integration
of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for
high-quality image generation, which also allow the model to engage in
context-aware chatting, perform text-to-speech conversion, and conduct
versatile image editing. Our experimental results showcase Ming-Omni offers a
powerful solution for unified perception and generation across all modalities.
Notably, our proposed Ming-Omni is the first open-source model we are aware of
to match GPT-4o in modality support, and we release all code and model weights
to encourage further research and development in the community.

</details>


### [163] [Beyond Nash Equilibrium: Bounded Rationality of LLMs and humans in Strategic Decision-making](https://arxiv.org/abs/2506.09390)
*Kehan Zheng,Jinfeng Zhou,Hongning Wang*

Key words: 大语言模型, 有限理性, 行为博弈论, 剪刀石头布, 囚徒困境

TL;DR: 研究比较了大语言模型（LLMs）与人类在战略决策中的行为，发现LLMs表现出类似人类的有限理性，但灵活性较低。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探讨LLMs在战略游戏（如剪刀石头布和囚徒困境）中是否表现出与人类相似的有限理性行为。

Method: 通过行为博弈论实验范式，将LLMs置于与人类相同的游戏环境中，分析其行为特征。

Result: LLMs展现了类似人类的启发式策略（如结果导向的策略切换和未来互动时的合作行为），但对环境变化的敏感性较弱。

Conclusion: 当前LLMs仅部分模拟了人类的有限理性，需改进训练方法以提高灵活性和情境感知能力。

Abstract: Large language models are increasingly used in strategic decision-making
settings, yet evidence shows that, like humans, they often deviate from full
rationality. In this study, we compare LLMs and humans using experimental
paradigms directly adapted from behavioral game-theory research. We focus on
two well-studied strategic games, Rock-Paper-Scissors and the Prisoner's
Dilemma, which are well known for revealing systematic departures from rational
play in human subjects. By placing LLMs in identical experimental conditions,
we evaluate whether their behaviors exhibit the bounded rationality
characteristic of humans. Our findings show that LLMs reproduce familiar human
heuristics, such as outcome-based strategy switching and increased cooperation
when future interaction is possible, but they apply these rules more rigidly
and demonstrate weaker sensitivity to the dynamic changes in the game
environment. Model-level analyses reveal distinctive architectural signatures
in strategic behavior, and even reasoning models sometimes struggle to find
effective strategies in adaptive situations. These results indicate that
current LLMs capture only a partial form of human-like bounded rationality and
highlight the need for training methods that encourage flexible opponent
modeling and stronger context awareness.

</details>


### [164] [A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy](https://arxiv.org/abs/2506.09420)
*Henry Peng Zou,Wei-Chieh Huang,Yaozu Wu,Chunyu Miao,Dongyuan Li,Aiwei Liu,Yue Zhou,Yankai Chen,Weizhi Zhang,Yangning Li,Liancheng Fang,Renhe Jiang,Philip S. Yu*

Key words: 大规模语言模型,自主AI,人类协作,LLM-HAS

TL;DR: 本文质疑完全自主AI代理的发展方向，提出基于LLM的人机协作系统（LLM-HAS）作为替代方案。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探讨当前自主AI系统在可靠性、透明性和理解人类需求方面的不足，强调人机协作的重要性。

Method: 提出LLM-HAS框架，通过人类参与提供指导、解答问题并保持控制，提升系统可信度和适应性。

Result: 通过医疗、金融和软件开发案例，证明人机协作在复杂任务中表现优于AI单独工作。

Conclusion: AI进步应衡量其与人类协作的能力，未来方向是增强人类能力的合作关系而非替代人类。

Abstract: Recent improvements in large language models (LLMs) have led many researchers
to focus on building fully autonomous AI agents. This position paper questions
whether this approach is the right path forward, as these autonomous systems
still have problems with reliability, transparency, and understanding the
actual requirements of human. We suggest a different approach: LLM-based
Human-Agent Systems (LLM-HAS), where AI works with humans rather than replacing
them. By keeping human involved to provide guidance, answer questions, and
maintain control, these systems can be more trustworthy and adaptable. Looking
at examples from healthcare, finance, and software development, we show how
human-AI teamwork can handle complex tasks better than AI working alone. We
also discuss the challenges of building these collaborative systems and offer
practical solutions. This paper argues that progress in AI should not be
measured by how independent systems become, but by how well they can work with
humans. The most promising future for AI is not in systems that take over human
roles, but in those that enhance human capabilities through meaningful
partnership.

</details>


### [165] [Fast Monte Carlo Tree Diffusion: 100x Speedup via Parallel Sparse Planning](https://arxiv.org/abs/2506.09498)
*Jaesik Yoon,Hyeonseo Cho,Yoshua Bengio,Sungjin Ahn*

Key words: 扩散模型, 轨迹规划, MCTD, Fast-MCTD, 并行化, 稀疏化

TL;DR: Fast-MCTD 是一种高效的扩散模型轨迹规划方法，通过并行化和稀疏化改进 MCTD，实现 100 倍加速且性能不降。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: MCTD 结合扩散模型与树搜索，在复杂规划任务中表现优异但计算开销大，因此需要更高效的改进版本。

Method: Fast-MCTD 结合 Parallel MCTD（并行化树更新）和 Sparse MCTD（轨迹稀疏化）来加速计算。

Result: Fast-MCTD 在保持或提升规划性能的同时，速度最高提升 100 倍，甚至在某些任务中比 Diffuser 更快。

Conclusion: Fast-MCTD 为基于扩散模型的实时推理提供了实用且可扩展的解决方案。

Abstract: Diffusion models have recently emerged as a powerful approach for trajectory
planning. However, their inherently non-sequential nature limits their
effectiveness in long-horizon reasoning tasks at test time. The recently
proposed Monte Carlo Tree Diffusion (MCTD) offers a promising solution by
combining diffusion with tree-based search, achieving state-of-the-art
performance on complex planning problems. Despite its strengths, our analysis
shows that MCTD incurs substantial computational overhead due to the sequential
nature of tree search and the cost of iterative denoising. To address this, we
propose Fast-MCTD, a more efficient variant that preserves the strengths of
MCTD while significantly improving its speed and scalability. Fast-MCTD
integrates two techniques: Parallel MCTD, which enables parallel rollouts via
delayed tree updates and redundancy-aware selection; and Sparse MCTD, which
reduces rollout length through trajectory coarsening. Experiments show that
Fast-MCTD achieves up to 100x speedup over standard MCTD while maintaining or
improving planning performance. Remarkably, it even outperforms Diffuser in
inference speed on some tasks, despite Diffuser requiring no search and
yielding weaker solutions. These results position Fast-MCTD as a practical and
scalable solution for diffusion-based inference-time reasoning.

</details>


### [166] [DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy](https://arxiv.org/abs/2506.09655)
*Kaixuan Xu,Jiajun Chai,Sicheng Li,Yuqian Fu,Yuanheng Zhu,Dongbin Zhao*

Key words: DipLLM, LLM, 外交游戏, 自回归分解

TL;DR: DipLLM利用LLM处理复杂多人游戏

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决传统AI系统在《外交》游戏中因计算资源需求高而难以应用的问题。

Method: 微调LLM，采用自回归分解框架简化多单位动作分配。

Result: 仅需1.5%数据即超过Cicero模型表现。

Conclusion: 微调LLM在复杂多人游戏战略决策中具有潜力。

Abstract: Diplomacy is a complex multiplayer game that requires both cooperation and
competition, posing significant challenges for AI systems. Traditional methods
rely on equilibrium search to generate extensive game data for training, which
demands substantial computational resources. Large Language Models (LLMs) offer
a promising alternative, leveraging pre-trained knowledge to achieve strong
performance with relatively small-scale fine-tuning. However, applying LLMs to
Diplomacy remains challenging due to the exponential growth of possible action
combinations and the intricate strategic interactions among players. To address
this challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns
equilibrium policies for Diplomacy. DipLLM employs an autoregressive
factorization framework to simplify the complex task of multi-unit action
assignment into a sequence of unit-level decisions. By defining an equilibrium
policy within this framework as the learning objective, we fine-tune the model
using only 1.5% of the data required by the state-of-the-art Cicero model,
surpassing its performance. Our results demonstrate the potential of fine-tuned
LLMs for tackling complex strategic decision-making in multiplayer games.

</details>


### [167] [Application-Driven Value Alignment in Agentic AI Systems: Survey and Perspectives](https://arxiv.org/abs/2506.09656)
*Wei Zeng,Hengshu Zhu,Chuan Qin,Han Wu,Yihang Cheng,Sirui Zhang,Xiaowei Jin,Yinuo Shen,Zhenxing Wang,Feimin Zhong,Hui Xiong*

Key words: Agentic AI, 价值对齐, 大型语言模型, 多代理系统, 社会规范

TL;DR: 该论文综述了在特定应用场景中智能代理系统的价值对齐问题，整合了大模型驱动的AI进展与社会治理需求，涵盖了价值原则、应用场景及评估方法，并提出了未来研究方向。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着AI进入代理式AI阶段，多代理自主决策与复杂环境协作中的价值对齐问题日益重要，需确保代理目标、偏好与行为符合人类价值观和社会规范。

Method: 论文采用层次化视角组织价值原则（宏观、中观、微观），从泛化到具体的分类方式分析代理应用场景，并系统评估数据集与对齐方法。

Result: 提出价值对齐原则框架、场景分类及评估体系，并探讨多代理间的价值协调问题。

Conclusion: 论文总结了价值对齐在代理系统中的关键问题与挑战，并指出未来研究方向，为相关领域提供了系统性参考。

Abstract: The ongoing evolution of AI paradigms has propelled AI research into the
Agentic AI stage. Consequently, the focus of research has shifted from single
agents and simple applications towards multi-agent autonomous decision-making
and task collaboration in complex environments. As Large Language Models (LLMs)
advance, their applications become more diverse and complex, leading to
increasingly situational and systemic risks. This has brought significant
attention to value alignment for AI agents, which aims to ensure that an
agent's goals, preferences, and behaviors align with human values and societal
norms. This paper reviews value alignment in agent systems within specific
application scenarios. It integrates the advancements in AI driven by large
models with the demands of social governance. Our review covers value
principles, agent system application scenarios, and agent value alignment
evaluation. Specifically, value principles are organized hierarchically from a
top-down perspective, encompassing macro, meso, and micro levels. Agent system
application scenarios are categorized and reviewed from a general-to-specific
viewpoint. Agent value alignment evaluation systematically examines datasets
for value alignment assessment and relevant value alignment methods.
Additionally, we delve into value coordination among multiple agents within
agent systems. Finally, we propose several potential research directions in
this field.

</details>


### [168] [Intent Factored Generation: Unleashing the Diversity in Your Language Model](https://arxiv.org/abs/2506.09659)
*Eltayeb Ahmed,Uljad Berdica,Martha Elliott,Danijela Horak,Jakob N. Foerster*

Key words: LLM，多样性，意图分解生成，温度采样，推理任务

TL;DR: 为LLM的固定提示生成多样且高质量样本的挑战，通过意图分解生成（IFG）实现两阶段采样，提升多样性与质量。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前方法在增加多样性时仅关注词汇层面，导致推理问题探索不足和对话代理重复无趣。

Method: 意图分解生成（IFG）：首先生成语义密集意图，再基于原始提示和意图生成最终响应。不同温度设置控制多样性和一致性。

Result: 在数学、代码任务中提升pass@k和RL反馈性能；结合Direct Preference Optimisation提高对话多样性且不牺牲奖励。

Conclusion: IFG简单有效，通过调整提示和温度可轻松集成到多种算法中，提升LLM样本多样性和性能。

Abstract: Obtaining multiple meaningfully diverse, high quality samples from Large
Language Models for a fixed prompt remains an open challenge. Current methods
for increasing diversity often only operate at the token-level, paraphrasing
the same response. This is problematic because it leads to poor exploration on
reasoning problems and to unengaging, repetitive conversational agents. To
address this we propose Intent Factored Generation (IFG), factorising the
sampling process into two stages. First, we sample a semantically dense intent,
e.g., a summary or keywords. Second, we sample the final response conditioning
on both the original prompt and the intent from the first stage. This allows us
to use a higher temperature during the intent step to promote conceptual
diversity, and a lower temperature during the final generation to ensure the
outputs are coherent and self-consistent. Additionally, we find that prompting
the model to explicitly state its intent for each step of the chain-of-thought
before generating the step is beneficial for reasoning tasks. We demonstrate
our method's effectiveness across a diverse set of tasks. We show this method
improves both pass@k and Reinforcement Learning from Verifier Feedback on maths
and code tasks. For instruction-tuning, we combine IFG with Direct Preference
Optimisation to increase conversational diversity without sacrificing reward.
Finally, we achieve higher diversity while maintaining the quality of
generations on a general language modelling task, using a new dataset of reader
comments and news articles that we collect and open-source. In summary, we
present a simple method of increasing the sample diversity of LLMs while
maintaining performance. This method can be implemented by changing the prompt
and varying the temperature during generation, making it easy to integrate into
many algorithms for gains across various applications.

</details>


### [169] [How Do People Revise Inconsistent Beliefs? Examining Belief Revision in Humans with User Studies](https://arxiv.org/abs/2506.09977)
*Stylianos Loukas Vasileiou,Antonio Rago,Maria Vanina Martinez,William Yeoh*

Key words: 信念修订、人类推理、AI对齐、解释、认知心理学

TL;DR: 研究发现人类倾向于基于解释的信念修订，这与经典信念改变理论不同，对AI系统设计有重要启示。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探究人类如何在新信息下修正信念，以开发更符合人类推理的AI系统。

Method: 通过三项用户研究，系统地观察人类在不同情境下的信念修订行为。

Result: 人类偏好基于解释的修订方式，即使这种修订可能不符合经典理论的最小修订原则。

Conclusion: AI系统应引入基于解释的信念修订算子，以更好地模拟人类认知过程。

Abstract: Understanding how humans revise their beliefs in light of new information is
crucial for developing AI systems which can effectively model, and thus align
with, human reasoning. While theoretical belief revision frameworks rely on a
set of principles that establish how these operations are performed, empirical
evidence from cognitive psychology suggests that people may follow different
patterns when presented with conflicting information. In this paper, we present
three comprehensive user studies showing that people consistently prefer
explanation-based revisions, i.e., those which are guided by explanations, that
result in changes to their belief systems that are not necessarily captured by
classical belief change theory. Our experiments systematically investigate how
people revise their beliefs with explanations for inconsistencies, whether they
are provided with them or left to formulate them themselves, demonstrating a
robust preference for what may seem non-minimal revisions across different
types of scenarios. These findings have implications for AI systems designed to
model human reasoning or interact with humans, suggesting that such systems
should accommodate explanation-based, potentially non-minimal belief revision
operators to better align with human cognitive processes.

</details>


### [170] [V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning](https://arxiv.org/abs/2506.09985)
*Mido Assran,Adrien Bardes,David Fan,Quentin Garrido,Russell Howes,Mojtaba,Komeili,Matthew Muckley,Ammar Rizvi,Claire Roberts,Koustuv Sinha,Artem Zholus,Sergio Arnaud,Abha Gejji,Ada Martin,Francois Robert Hogan,Daniel Dugas,Piotr Bojanowski,Vasil Khalidov,Patrick Labatut,Francisco Massa,Marc Szafraniec,Kapil Krishnakumar,Yong Li,Xiaodong Ma,Sarath Chandar,Franziska Meier,Yann LeCun,Michael Rabbat,Nicolas Ballas*

Key words: 自监督学习, 视频预测, 机器人规划, 世界模型

TL;DR: 通过自监督学习结合互联网视频和少量机器人交互数据，开发物理世界理解和规划模型。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决现代AI通过观察学习和理解世界的挑战。

Method: 使用V-JEPA 2架构预训练，结合视频和图像数据，再与语言模型对齐，应用于机器人规划任务。

Result: 在视频理解和动作预测任务表现优异（如Something-Something v2准确率77.3），并在机器人零样本规划中成功应用。

Conclusion: 自监督学习结合少量交互数据可实现物理世界规划。

Abstract: A major challenge for modern AI is to learn to understand the world and learn
to act largely by observation. This paper explores a self-supervised approach
that combines internet-scale video data with a small amount of interaction data
(robot trajectories), to develop models capable of understanding, predicting,
and planning in the physical world. We first pre-train an action-free
joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset
comprising over 1 million hours of internet video. V-JEPA 2 achieves strong
performance on motion understanding (77.3 top-1 accuracy on Something-Something
v2) and state-of-the-art performance on human action anticipation (39.7
recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.
Additionally, after aligning V-JEPA 2 with a large language model, we
demonstrate state-of-the-art performance on multiple video question-answering
tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on
TempCompass). Finally, we show how self-supervised learning can be applied to
robotic planning tasks by post-training a latent action-conditioned world
model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the
Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different
labs and enable picking and placing of objects using planning with image goals.
Notably, this is achieved without collecting any data from the robots in these
environments, and without any task-specific training or reward. This work
demonstrates how self-supervised learning from web-scale data and a small
amount of robot interaction data can yield a world model capable of planning in
the physical world.

</details>


### [171] [Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering](https://arxiv.org/abs/2506.06905)
*Akash Gupta,Amos Storkey,Mirella Lapata*

Key words: 大型多模态模型（LMMs）, 元学习, 少样本学习, 软提示, 注意力映射

TL;DR: 论文提出了一种元学习方法，通过提取任务相关的图像特征生成软提示，解决了小规模大型多模态模型（LMMs）在少样本学习中的性能不稳定问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有的LMMs依赖上下文学习（ICL）进行少样本任务，但性能不稳定，可能因图像嵌入中冗余信息的影响而无法随样本增加单调提升。

Method: 提出一种元学习方法，通过注意力映射模块提取任务相关图像特征生成软提示，并结合LLaVA v1.5架构，实现低数据条件下的任务适应。

Result: 在VL-ICL Bench上验证，该方法在图像扰动下仍优于ICL及其他提示调优方法，提升了视觉问答任务的推理能力。

Conclusion: 通过任务相关的软提示和注意力映射模块，有效提升了LMMs在少样本任务中的性能和稳定性。

Abstract: Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to
perform new tasks with minimal supervision. However, ICL performance,
especially in smaller LMMs, is inconsistent and does not always improve
monotonically with increasing examples. We hypothesize that this occurs due to
the LMM being overwhelmed by additional information present in the image
embeddings, which is not required for the downstream task. To address this, we
propose a meta-learning approach that provides an alternative for inducing
few-shot capabilities in LMMs, using a fixed set of soft prompts that are
distilled from task-relevant image features and can be adapted at test time
using a few examples. To facilitate this distillation, we introduce an
attention-mapper module that can be easily integrated with the popular LLaVA
v1.5 architecture and is jointly learned with soft prompts, enabling task
adaptation in LMMs under low-data regimes with just a few gradient steps.
Evaluation on the VL-ICL Bench shows that our method consistently outperforms
ICL and related prompt-tuning approaches, even under image perturbations,
improving task induction and reasoning across visual question answering tasks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [172] [UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation](https://arxiv.org/abs/2506.09284)
*Yihe Tang,Wenlong Huang,Yingke Wang,Chengshu Li,Roy Yuan,Ruohan Zhang,Jiajun Wu,Li Fei-Fei*

Key words: UAD, 无监督学习, 功能建模, 视觉语言模型, 模仿学习

TL;DR: UAD是一种无需人工标注、从基础模型中提取物体功能知识的方法，能显著提升机器人在非结构化环境中的操作性。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 现有视觉功能预测方法依赖人工标注或局限于预定义任务，限制了在开放任务指令下的泛化能力。

Method: UAD通过结合大型视觉模型和视觉语言模型的优势，自动生成大规模任务标注数据集，并训练轻量级任务条件解码器。

Result: UAD在仿真环境中训练，却能在真实场景中表现出色，模仿学习策略仅需少量演示即可泛化到新对象和任务。

Conclusion: UAD为机器人操作提供了一种高效的无监督功能学习方法，具有广泛的实际应用潜力。

Abstract: Understanding fine-grained object affordances is imperative for robots to
manipulate objects in unstructured environments given open-ended task
instructions. However, existing methods of visual affordance predictions often
rely on manually annotated data or conditions only on a predefined set of
tasks. We introduce UAD (Unsupervised Affordance Distillation), a method for
distilling affordance knowledge from foundation models into a task-conditioned
affordance model without any manual annotations. By leveraging the
complementary strengths of large vision models and vision-language models, UAD
automatically annotates a large-scale dataset with detailed $<$instruction,
visual affordance$>$ pairs. Training only a lightweight task-conditioned
decoder atop frozen features, UAD exhibits notable generalization to
in-the-wild robotic scenes and to various human activities, despite only being
trained on rendered objects in simulation. Using affordance provided by UAD as
the observation space, we show an imitation learning policy that demonstrates
promising generalization to unseen object instances, object categories, and
even variations in task instructions after training on as few as 10
demonstrations. Project website: https://unsup-affordance.github.io/

</details>


### [173] [Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations](https://arxiv.org/abs/2506.09383)
*Chengtian Ma,Yunyue Wei,Chenhui Zuo,Chen Zhang,Yanan Sui*

Key words: 平衡控制,肌肉骨骼系统,外骨骼辅助,跌倒模拟

TL;DR: 该论文提出了一种层级控制框架，用于模拟人体平衡，揭示了肌肉损伤对平衡行为的影响，并展示了髋部外骨骼辅助在扰动下对平衡维护和减少肌肉努力的改善。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 研究静态平衡和跌倒的定量理解不足，希望通过肌肉骨骼系统的模拟填补这一空白。

Method: 采用层级控制框架，结合全身肌肉骨骼系统，分析平衡时的时空动态，模拟肌肉损伤和髋部外骨骼辅助效果。

Result: 揭示了稳定站立时的平衡动态，模拟结果与临床数据一致，外骨骼辅助显著改善了平衡维护和肌肉努力。

Conclusion: 该研究为开发针对平衡障碍的干预措施和人形机器人系统提供了肌肉层次的见解。

Abstract: Balance control is important for human and bipedal robotic systems. While
dynamic balance during locomotion has received considerable attention,
quantitative understanding of static balance and falling remains limited. This
work presents a hierarchical control pipeline for simulating human balance via
a comprehensive whole-body musculoskeletal system. We identified spatiotemporal
dynamics of balancing during stable standing, revealed the impact of muscle
injury on balancing behavior, and generated fall contact patterns that aligned
with clinical data. Furthermore, our simulated hip exoskeleton assistance
demonstrated improvement in balance maintenance and reduced muscle effort under
perturbation. This work offers unique muscle-level insights into human balance
dynamics that are challenging to capture experimentally. It could provide a
foundation for developing targeted interventions for individuals with balance
impairments and support the advancement of humanoid robotic systems.

</details>


### [174] [Adv-BMT: Bidirectional Motion Transformer for Safety-Critical Traffic Scenario Generation](https://arxiv.org/abs/2506.09485)
*Yuxin Liu,Zhenghao Peng,Xuanhao Cui,Bolei Zhou*

Key words: 自动驾驶测试、Adv-BMT框架、双向运动变换模型、碰撞场景生成、数据增强

TL;DR: 论文提出了Adv-BMT框架，通过双向运动变换模型生成多样且真实的对抗性交互，解决了自动驾驶测试中长尾安全关键场景数据稀缺的问题。实验结果表明，使用该框架增强的数据集可将碰撞率降低20%。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 现有自动驾驶测试数据集缺乏长尾安全关键场景，限制了测试的有效性。为了解决这一问题，需要一种能够生成多样且真实对抗性交互的方法。

Method: 提出了Adv-BMT框架，采用双向运动变换模型（BMT）逆向预测交通运动。框架包括两个阶段：对抗性初始化和逆向运动预测，无需碰撞数据进行预训练。

Result: 实验验证了Adv-BMT生成的碰撞场景质量，使用增强数据集训练可将碰撞率降低20%，优于之前的工作。

Conclusion: Adv-BMT框架有效解决了数据稀缺问题，生成的交互多样且真实，显著提升了自动驾驶系统的测试性能。

Abstract: Scenario-based testing is essential for validating the performance of
autonomous driving (AD) systems. However, such testing is limited by the
scarcity of long-tailed, safety-critical scenarios in existing datasets
collected in the real world. To tackle the data issue, we propose the Adv-BMT
framework, which augments real-world scenarios with diverse and realistic
adversarial interactions. The core component of Adv-BMT is a bidirectional
motion transformer (BMT) model to perform inverse traffic motion predictions,
which takes agent information in the last time step of the scenario as input,
and reconstruct the traffic in the inverse of chronological order until the
initial time step. The Adv-BMT framework is a two-staged pipeline: it first
conducts adversarial initializations and then inverse motion predictions.
Different from previous work, we do not need any collision data for
pretraining, and are able to generate realistic and diverse collision
interactions. Our experimental results validate the quality of generated
collision scenarios by Adv-BMT: training in our augmented dataset would reduce
episode collision rates by 20\% compared to previous work.

</details>


### [175] [Tightly-Coupled LiDAR-IMU-Leg Odometry with Online Learned Leg Kinematics Incorporating Foot Tactile Information](https://arxiv.org/abs/2506.09548)
*Taku Okawara,Kenji Koide,Aoki Takanose,Shuji Oishi,Masashi Yokozuka,Kentaro Uno,Kazuya Yoshida*

Key words: LiDAR-IMU里程计、腿部运动学模型、在线学习、特征缺失环境、可变形地形

TL;DR: 本文提出了一种紧耦合的LiDAR-IMU-腿里程计方法，适用于特征缺失环境和可变形地形。通过在线学习腿部运动学模型，结合触觉信息，提高了机器人对负载变化和地形条件的适应性。实验验证了该方法的优越性。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 开发一种能够在特征缺失环境和可变形地形中稳定工作的里程计方法，解决机器人适应负载变化和不同地形条件的问题。

Method: 提出了一种基于在线学习的神经网络腿部运动学模型（neural leg kinematics model），结合触觉信息（脚部反作用力），通过在统一因子图上联合训练模型和估计里程计，保持一致性。

Result: 实验证明，该方法在沙滩和校园等多种挑战性环境中优于现有技术。

Conclusion: 提出的方法通过在线学习和触觉信息增强了里程计的稳健性，适用于复杂环境。

Abstract: In this letter, we present tightly coupled LiDAR-IMU-leg odometry, which is
robust to challenging conditions such as featureless environments and
deformable terrains. We developed an online learning-based leg kinematics model
named the neural leg kinematics model, which incorporates tactile information
(foot reaction force) to implicitly express the nonlinear dynamics between
robot feet and the ground. Online training of this model enhances its
adaptability to weight load changes of a robot (e.g., assuming delivery or
transportation tasks) and terrain conditions. According to the \textit{neural
adaptive leg odometry factor} and online uncertainty estimation of the leg
kinematics model-based motion predictions, we jointly solve online training of
this kinematics model and odometry estimation on a unified factor graph to
retain the consistency of both. The proposed method was verified through real
experiments using a quadruped robot in two challenging situations: 1) a sandy
beach, representing an extremely featureless area with a deformable terrain,
and 2) a campus, including multiple featureless areas and terrain types of
asphalt, gravel (deformable terrain), and grass. Experimental results showed
that our odometry estimation incorporating the \textit{neural leg kinematics
model} outperforms state-of-the-art works. Our project page is available for
further details: https://takuokawara.github.io/RAL2025_project_page/

</details>


### [176] [SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation via Skill Blending](https://arxiv.org/abs/2506.09366)
*Yuxuan Kuang,Haoran Geng,Amine Elhafsi,Tan-Dzung Do,Pieter Abbeel,Jitendra Malik,Marco Pavone,Yue Wang*

Key words: 人形机器人，分层强化学习，运动操作，SkillBlender，SkillBench

TL;DR: SkillBlender是一个新颖的分层强化学习框架，通过预训练任务无关的原始技能并动态混合这些技能，实现了多功能的人形机器人运动操作。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 针对现有方法需要针对每个任务进行繁琐调整的问题，提出了SkillBlender框架，以提升人形机器人在日常场景中的通用性和可扩展性。

Method: 首先预训练目标条件的任务无关原始技能，然后动态混合这些技能以完成复杂的运动操作任务，最小化任务特定的奖励工程。

Result: 实验表明，SkillBlender显著优于所有基线方法，同时在多个运动操作任务中产生更准确和可行的行为。

Conclusion: SkillBlender提供了有效的解决方案，减少了任务特定调整的需求，适用于多样化日常任务。

Abstract: Humanoid robots hold significant potential in accomplishing daily tasks
across diverse environments thanks to their flexibility and human-like
morphology. Recent works have made significant progress in humanoid whole-body
control and loco-manipulation leveraging optimal control or reinforcement
learning. However, these methods require tedious task-specific tuning for each
task to achieve satisfactory behaviors, limiting their versatility and
scalability to diverse tasks in daily scenarios. To that end, we introduce
SkillBlender, a novel hierarchical reinforcement learning framework for
versatile humanoid loco-manipulation. SkillBlender first pretrains
goal-conditioned task-agnostic primitive skills, and then dynamically blends
these skills to accomplish complex loco-manipulation tasks with minimal
task-specific reward engineering. We also introduce SkillBench, a parallel,
cross-embodiment, and diverse simulated benchmark containing three embodiments,
four primitive skills, and eight challenging loco-manipulation tasks,
accompanied by a set of scientific evaluation metrics balancing accuracy and
feasibility. Extensive simulated experiments show that our method significantly
outperforms all baselines, while naturally regularizing behaviors to avoid
reward hacking, resulting in more accurate and feasible movements for diverse
loco-manipulation tasks in our daily scenarios. Our code and benchmark will be
open-sourced to the community to facilitate future research. Project page:
https://usc-gvl.github.io/SkillBlender-web/.

</details>


### [177] [Scoop-and-Toss: Dynamic Object Collection for Quadrupedal Systems](https://arxiv.org/abs/2506.09406)
*Minji Kang,Chanwoo Baek,Yoonsang Lee*

Key words: 四足机器人, 动态操作, 分层策略, 物体收集

TL;DR: 本文提出了一种利用四足机器人腿部敏捷性的框架，无需额外执行器即可收集物体，使用分层策略实现动态物体操作。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 探索四足机器人如何在无需额外执行器的情况下，通过腿部敏捷性完成动态物体操作，扩展其功能。

Method: 通过简单的铲状附件和分层策略（包括铲取-投掷、靠近物体位置的专家策略及动态切换的元策略）实现多物体收集。

Result: 展示了四足机器人腿部在动态物体操作中的有效性，扩展了其角色。

Conclusion: 该方法证明了四足机器人腿部在动态任务中的潜力，为更广泛的应用提供了基础。

Abstract: Quadruped robots have made significant advances in locomotion, extending
their capabilities from controlled environments to real-world applications.
Beyond movement, recent work has explored loco-manipulation using the legs to
perform tasks such as pressing buttons or opening doors. While these efforts
demonstrate the feasibility of leg-based manipulation, most have focused on
relatively static tasks. In this work, we propose a framework that enables
quadruped robots to collect objects without additional actuators by leveraging
the agility of their legs. By attaching a simple scoop-like add-on to one leg,
the robot can scoop objects and toss them into a collection tray mounted on its
back. Our method employs a hierarchical policy structure comprising two expert
policies-one for scooping and tossing, and one for approaching object
positions-and a meta-policy that dynamically switches between them. The expert
policies are trained separately, followed by meta-policy training for
coordinated multi-object collection. This approach demonstrates how quadruped
legs can be effectively utilized for dynamic object manipulation, expanding
their role beyond locomotion.

</details>


### [178] [Time-Unified Diffusion Policy with Action Discrimination for Robotic Manipulation](https://arxiv.org/abs/2506.09422)
*Ye Niu,Sanping Zhou,Yizhe Li,Ye Den,Le Wang*

Key words: 机器人操纵，扩散模型，动作识别，去噪过程，时间统一策略

TL;DR: 提出了一种时间统一的扩散策略（TUDP），通过统一的去噪过程和动作识别能力，提高了机器人动作生成的效率和准确性。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 现有扩散策略需要长时间迭代去噪，且时间变化的去噪过程增加了训练难度和动作精度不足的问题。

Method: 采用时间统一的速度场和动作识别分支，简化去噪过程并提高准确性。

Result: 在RLBench上达到最高成功率（多视图82.6%，单视图83.8%），尤其在少次去噪迭代时表现更优。

Conclusion: TUDP显著提升了动作生成效率和准确性，适用于实际任务。

Abstract: In many complex scenarios, robotic manipulation relies on generative models
to estimate the distribution of multiple successful actions. As the diffusion
model has better training robustness than other generative models, it performs
well in imitation learning through successful robot demonstrations. However,
the diffusion-based policy methods typically require significant time to
iteratively denoise robot actions, which hinders real-time responses in robotic
manipulation. Moreover, existing diffusion policies model a time-varying action
denoising process, whose temporal complexity increases the difficulty of model
training and leads to suboptimal action accuracy. To generate robot actions
efficiently and accurately, we present the Time-Unified Diffusion Policy
(TUDP), which utilizes action recognition capabilities to build a time-unified
denoising process. On the one hand, we build a time-unified velocity field in
action space with additional action discrimination information. By unifying all
timesteps of action denoising, our velocity field reduces the difficulty of
policy learning and speeds up action generation. On the other hand, we propose
an action-wise training method, which introduces an action discrimination
branch to supply additional action discrimination information. Through
action-wise training, the TUDP implicitly learns the ability to discern
successful actions to better denoising accuracy. Our method achieves
state-of-the-art performance on RLBench with the highest success rate of 82.6%
on a multi-view setup and 83.8% on a single-view setup. In particular, when
using fewer denoising iterations, TUDP achieves a more significant improvement
in success rate. Additionally, TUDP can produce accurate actions for a wide
range of real-world tasks.

</details>


### [179] [SAFE: Multitask Failure Detection for Vision-Language-Action Models](https://arxiv.org/abs/2506.09937)
*Qiao Gu,Yuanliang Ju,Shengxiang Sun,Igor Gilitschenski,Haruki Nishimura,Masha Itkina,Florian Shkurti*

Key words: 故障检测, 视觉-语言-动作模型, 机器人策略, 通用化

TL;DR: 提出了名为SAFE的通用故障检测器，专为视觉-语言-动作模型（VLA）设计，能在未见任务和环境中及时检测故障。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 现有的故障检测器仅针对特定任务，无法满足VLA模型在多种任务和环境中通用化的需求。

Method: 通过分析VLA特征空间，利用VLA内部的通用任务知识，设计SAFE从这些特征中学习并预测故障可能性。

Result: SAFE在多个策略架构（如OpenVLA和π0）上表现优异，在准确性和检测时间上均优于基线方法。

Conclusion: SAFE是一种适用于通用机器人策略的高效故障检测器，在未见任务和环境中表现出色。

Abstract: While vision-language-action models (VLAs) have shown promising robotic
behaviors across a diverse set of manipulation tasks, they achieve limited
success rates when deployed on novel tasks out-of-the-box. To allow these
policies to safely interact with their environments, we need a failure detector
that gives a timely alert such that the robot can stop, backtrack, or ask for
help. However, existing failure detectors are trained and tested only on one or
a few specific tasks, while VLAs require the detector to generalize and detect
failures also in unseen tasks and novel environments. In this paper, we
introduce the multitask failure detection problem and propose SAFE, a failure
detector for generalist robot policies such as VLAs. We analyze the VLA feature
space and find that VLAs have sufficient high-level knowledge about task
success and failure, which is generic across different tasks. Based on this
insight, we design SAFE to learn from VLA internal features and predict a
single scalar indicating the likelihood of task failure. SAFE is trained on
both successful and failed rollouts, and is evaluated on unseen tasks. SAFE is
compatible with different policy architectures. We test it on OpenVLA, $\pi_0$,
and $\pi_0$-FAST in both simulated and real-world environments extensively. We
compare SAFE with diverse baselines and show that SAFE achieves
state-of-the-art failure detection performance and the best trade-off between
accuracy and detection time using conformal prediction. More qualitative
results can be found at https://vla-safe.github.io/.

</details>


### [180] [eFlesh: Highly customizable Magnetic Touch Sensing using Cut-Cell Microstructures](https://arxiv.org/abs/2506.09994)
*Venkatesh Pattabiraman,Zizhou Huang,Daniele Panozzo,Denis Zorin,Lerrel Pinto,Raunaq Bhirangi*

Key words: 触觉传感器、机器人操作、3D打印、开源工具、力感知

TL;DR: eFlesh是一种低成本、易制作和高度可定制的磁力触觉传感器，通过3D打印和开源工具，解决了机器人操作中缺乏通用触觉传感器的问题。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 在非结构化环境中（如家庭和办公室），机器人需要感知交互中的力，但目前缺乏通用、易用的触觉传感器，导致解决方案分散甚至无传感器。

Method: eFlesh采用磁力触觉传感器，通过3D打印、现成磁铁和磁力计电路板制作，设计工具可将3D模型转换为可打印文件。

Result: 实验显示，eFlesh的接触定位误差为0.5毫米，力预测误差为0.27N（法向力）和0.12N（剪切力），滑动检测准确率达95%，视觉触觉控制任务成功率达91%。

Conclusion: eFlesh为机器人操作提供了一种低成本、可定制的触觉传感解决方案，并通过开源工具促进了更广泛的应用和研究。

Abstract: If human experience is any guide, operating effectively in unstructured
environments -- like homes and offices -- requires robots to sense the forces
during physical interaction. Yet, the lack of a versatile, accessible, and
easily customizable tactile sensor has led to fragmented, sensor-specific
solutions in robotic manipulation -- and in many cases, to force-unaware,
sensorless approaches. With eFlesh, we bridge this gap by introducing a
magnetic tactile sensor that is low-cost, easy to fabricate, and highly
customizable. Building an eFlesh sensor requires only four components: a
hobbyist 3D printer, off-the-shelf magnets (<$5), a CAD model of the desired
shape, and a magnetometer circuit board. The sensor is constructed from tiled,
parameterized microstructures, which allow for tuning the sensor's geometry and
its mechanical response. We provide an open-source design tool that converts
convex OBJ/STL files into 3D-printable STLs for fabrication. This modular
design framework enables users to create application-specific sensors, and to
adjust sensitivity depending on the task. Our sensor characterization
experiments demonstrate the capabilities of eFlesh: contact localization RMSE
of 0.5 mm, and force prediction RMSE of 0.27 N for normal force and 0.12 N for
shear force. We also present a learned slip detection model that generalizes to
unseen objects with 95% accuracy, and visuotactile control policies that
improve manipulation performance by 40% over vision-only baselines -- achieving
91% average success rate for four precise tasks that require sub-mm accuracy
for successful completion. All design files, code and the CAD-to-eFlesh STL
conversion tool are open-sourced and available on https://e-flesh.com.

</details>


### [181] [Learning to Optimize Package Picking for Large-Scale, Real-World Robot Induction](https://arxiv.org/abs/2506.09765)
*Shuai Li,Azarakhsh Keipour,Sicong Zhao,Srinath Rajagopalan,Charles Swan,Kostas E. Bekris*

Key words: 仓库自动化, 机器学习, 拣选优化, 吸盘选择, 数据驱动

TL;DR: 提出一种基于机器学习的框架，通过预测调整和优化吸盘选择，提升大规模仓库自动化中拣选的成功率。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 仓库自动化对提升效率、降低成本至关重要，现有研究多基于启发式方法，缺乏数据驱动的优化。

Method: 提出ML框架，预测变换调整和优化吸盘选择，应用于亚马逊机器人车队测试。

Result: 在200万次拣选中，拣选失败率降低20%，验证了框架的有效性。

Conclusion: 该框架能显著提升大规模仓库自动化的性能，是数据驱动优化的成功实践。

Abstract: Warehouse automation plays a pivotal role in enhancing operational
efficiency, minimizing costs, and improving resilience to workforce
variability. While prior research has demonstrated the potential of machine
learning (ML) models to increase picking success rates in large-scale robotic
fleets by prioritizing high-probability picks and packages, these efforts
primarily focused on predicting success probabilities for picks sampled using
heuristic methods. Limited attention has been given, however, to leveraging
data-driven approaches to directly optimize sampled picks for better
performance at scale. In this study, we propose an ML-based framework that
predicts transform adjustments as well as improving the selection of suction
cups for multi-suction end effectors for sampled picks to enhance their success
probabilities. The framework was integrated and evaluated in test workcells
that resemble the operations of Amazon Robotics' Robot Induction (Robin) fleet,
which is used for package manipulation. Evaluated on over 2 million picks, the
proposed method achieves a 20\% reduction in pick failure rates compared to a
heuristic-based pick sampling baseline, demonstrating its effectiveness in
large-scale warehouse automation scenarios.

</details>


### [182] [Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation](https://arxiv.org/abs/2506.09990)
*Wenbo Zhang,Tianrun Hu,Yanyuan Qiao,Hanbo Zhang,Yuchu Qin,Yang Li,Jiajun Liu,Tao Kong,Lingqiao Liu,Xiao Ma*

Key words: Chain-of-Action, 反向推理, 轨迹自回归, 视觉运动策略, RLBench

TL;DR: Chain-of-Action (CoA) 是一种基于轨迹自回归建模的全新视觉运动策略范式，通过反向推理生成完整轨迹，实现全局到局部的动作约束，显著提升性能。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 传统方法通常通过前向预测下一步动作，无法保证动作与最终目标的全局一致性，因此需要一种反向推理机制来优化动作生成。

Method: CoA 通过动作级别的链式思维过程，自回归生成轨迹。关键设计包括：连续动作令牌表示、动态停止、反向时间集成和多令牌预测。

Result: CoA 在 60 个 RLBench 任务和 8 个现实世界操作任务中达到最先进性能。

Conclusion: CoA 通过反向推理和全局到局部约束，实现了高效且灵活的视觉运动策略，显著优于传统方法。

Abstract: We present Chain-of-Action (CoA), a novel visuo-motor policy paradigm built
upon Trajectory Autoregressive Modeling. Unlike conventional approaches that
predict next step action(s) forward, CoA generates an entire trajectory by
explicit backward reasoning with task-specific goals through an action-level
Chain-of-Thought (CoT) process. This process is unified within a single
autoregressive structure: (1) the first token corresponds to a stable keyframe
action that encodes the task-specific goals; and (2) subsequent action tokens
are generated autoregressively, conditioned on the initial keyframe and
previously predicted actions. This backward action reasoning enforces a
global-to-local structure, allowing each local action to be tightly constrained
by the final goal. To further realize the action reasoning structure, CoA
incorporates four complementary designs: continuous action token
representation; dynamic stopping for variable-length trajectory generation;
reverse temporal ensemble; and multi-token prediction to balance action chunk
modeling with global structure. As a result, CoA gives strong spatial
generalization capabilities while preserving the flexibility and simplicity of
a visuo-motor policy. Empirically, we observe CoA achieves the state-of-the-art
performance across 60 RLBench tasks and 8 real-world manipulation tasks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [183] [UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench](https://arxiv.org/abs/2506.09289)
*Boxi Yu,Yuxuan Zhu,Pinjia He,Daniel Kang*

Key words: Large Language Models, SWE-Bench, UTGenerator, UTBoost, test case generation

TL;DR: 论文提出UTGenerator和UTBoost框架，通过生成测试案例增强SWE-Bench的评估能力，纠正了大量错误标记的补丁，显著影响排行榜排名。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 解决SWE-Bench中手动编写测试案例不足的问题，确保生成的补丁能真正解决问题。

Method: 利用LLM驱动的UTGenerator自动生成测试案例，并构建UTBoost框架进行扩展评估。

Result: 发现36个任务实例测试不足，纠正345个错误标记补丁，影响40.9%的Lite版本和24.4%的Verified版本排名。

Conclusion: UTGenerator和UTBoost有效提升代码生成评估的准确性，显著改善SWE-Bench的结果。

Abstract: The advent of Large Language Models (LLMs) has spurred the development of
coding agents for real-world code generation. As a widely used benchmark for
evaluating the code generation capabilities of these agents, SWE-Bench uses
real-world problems based on GitHub issues and their corresponding pull
requests. However, the manually written test cases included in these pull
requests are often insufficient, allowing generated patches to pass the tests
without resolving the underlying issue. To address this challenge, we introduce
UTGenerator, an LLM-driven test case generator that automatically analyzes
codebases and dependencies to generate test cases for real-world Python
projects. Building on UTGenerator, we propose UTBoost, a comprehensive
framework for test case augmentation. In our evaluation, we identified 36 task
instances with insufficient test cases and uncovered 345 erroneous patches
incorrectly labeled as passed in the original SWE Bench. These corrections,
impacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard
entries, yield 18 and 11 ranking changes, respectively.

</details>


### [184] [Reasoning as a Resource: Optimizing Fast and Slow Thinking in Code Generation Models](https://arxiv.org/abs/2506.09396)
*Zongjie Li,Shuai Wang*

Key words: 代码生成, 推理深度, 适应性控制, 快速思考, 深度思考

TL;DR: 提出将推理深度作为可控资源的设计理念，主张显式管理直接快速回答与多步推理的权衡，以优化准确性、延迟和成本。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 现有代码生成模型的推理深度通常是无意中的副产品，缺乏显式管理。通过控制推理资源，可以更好地优化模型性能。

Method: 提出适应性控制推理的方法，涵盖从合成数据创建、基准测试到实际部署的全生命周期管理。

Result: 通过显式管理推理资源，能够实现准确性、延迟和成本的更优权衡。

Conclusion: 将快速和深度推理作为互补模式进行调度，有望提升编码代理的性能和实用性。

Abstract: This position paper proposes a fundamental shift in designing code generation
models: treating reasoning depth as a controllable resource. Rather than being
an incidental byproduct of prompting, we argue that the trade-off between
rapid, direct answers ("fast thinking") and elaborate, chain-of-thought
deliberation ("slow thinking") must be explicitly managed. We contend that
optimizing reasoning budgets across the entire model lifecycle - from synthetic
data creation and benchmarking to real-world deploymen - can unlock superior
trade-offs among accuracy, latency, and cost. This paper outlines how adaptive
control over reasoning can enrich supervision signals, motivate new
multi-dimensional benchmarks, and inform cost-aware, security-conscious
deployment policies. By viewing fast and slow thinking as complementary modes
to be scheduled, we envision coding agents that think deep when necessary and
act fast when possible.

</details>


### [185] [Stakeholder Participation for Responsible AI Development: Disconnects Between Guidance and Current Practice](https://arxiv.org/abs/2506.09873)
*Emma Kallina,Thomas Bohné,Jat Singh*

Key words: 负责任AI、利益相关方参与、商业优先、脱节、干预措施

TL;DR: 研究探讨了负责任AI（rAI）指南中利益相关方参与（SHI）的实践，发现商业环境中的SHI主要受商业目标驱动，与rAI目标存在脱节，并提出干预措施。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 研究旨在明确现有SHI实践对rAI努力的贡献及其潜在脱节，以指导未来实践。

Method: 分析了56份rAI指南，并通过对130名AI从业者的在线调查和10次半结构化访谈了解SHI实践情况。

Result: 商业环境中的SHI主要由商业优先级驱动，与rAI目标（如权力再分配、风险预测等）存在脱节。

Conclusion: 现有SHI实践未显著贡献于rAI，需针对性干预和研究以促进rAI发展。

Abstract: Responsible AI (rAI) guidance increasingly promotes stakeholder involvement
(SHI) during AI development. At the same time, SHI is already common in
commercial software development, but with potentially different foci. This
study clarifies the extent to which established SHI practices are able to
contribute to rAI efforts as well as potential disconnects -- essential
insights to inform and tailor future interventions that further shift industry
practice towards rAI efforts. First, we analysed 56 rAI guidance documents to
identify why SHI is recommended (i.e. its expected benefits for rAI) and
uncovered goals such as redistributing power, improving socio-technical
understandings, anticipating risks, and enhancing public oversight. To
understand why and how SHI is currently practised in commercial settings, we
then conducted an online survey (n=130) and semi-structured interviews (n=10)
with AI practitioners. Our findings reveal that SHI in practice is primarily
driven by commercial priorities (e.g. customer value, compliance) and several
factors currently discourage more rAI-aligned SHI practices. This suggests that
established SHI practices are largely not contributing to rAI efforts. To
address this disconnect, we propose interventions and research opportunities to
advance rAI development in practice.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [186] [How attention simplifies mental representations for planning](https://arxiv.org/abs/2506.09520)
*Jason da Silva Castanheira,Nicholas Shea,Stephen M. Fleming*

Key words: 人类规划、空间注意力、任务表征、虚拟迷宫、计算模型

TL;DR: 该研究探讨人类如何通过空间注意力构建简化的任务表征以高效规划，揭示注意力对任务表征的影响及其个体差异，并提出一种计算模型。

<details>
  <summary>Details</summary>

Main category: q-bio.NC

Motivation: 人类的高效和灵活规划能力依赖于简化的环境表征，但其知觉和注意机制尚不明确，因此研究旨在揭示空间注意力如何影响任务表征和规划。

Method: 利用虚拟迷宫导航任务，研究空间注意力对任务表征的调控作用，并分析注意力自然分布对简化表征的影响。

Result: 空间接近性决定迷宫中哪些信息可用于规划，注意力自然分布有助于简化表征；个体差异显著影响表征和行为。

Conclusion: 研究将空间注意力引入计算模型，为理解人类如何利用知觉表征辅助规划提供了新视角。

Abstract: Human planning is efficient -- it frugally deploys limited cognitive
resources to accomplish difficult tasks -- and flexible -- adapting to novel
problems and environments. Computational approaches suggest that people
construct simplified mental representations of their environment, balancing the
complexity of a task representation with its utility. These models imply a
nested optimisation in which planning shapes perception, and perception shapes
planning -- but the perceptual and attentional mechanisms governing how this
interaction unfolds remain unknown. Here, we harness virtual maze navigation to
characterise how spatial attention controls which aspects of a task
representation enter subjective awareness and are available for planning. We
find that spatial proximity governs which aspects of a maze are available for
planning, and that when task-relevant information follows natural (lateralised)
contours of attention, people can more easily construct simplified and useful
maze representations. This influence of attention varies considerably across
individuals, explaining differences in people's task representations and
behaviour. Inspired by the 'spotlight of attention' analogy, we incorporate the
effects of visuospatial attention into existing computational accounts of
value-guided construal. Together, our work bridges computational perspectives
on perception and decision-making to better understand how individuals
represent their environments in aid of planning.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [187] [STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory Optimization and Architectural Support](https://arxiv.org/abs/2506.09070)
*Chenqi Zhang,Yu Feng,Jieru Zhao,Guangda Liu,Wenchao Ding,Chentao Wu,Minyi Guo*

Key words: 3D高斯泼溅,移动设备,算法-架构协同设计,内存效率,实时渲染

TL;DR: 3D高斯泼溅（3DGS）因其高效和基于高斯稀疏表示的特性而流行，但在资源受限的移动设备上难以满足90 FPS的实时要求。STREAMINGGS通过算法-架构协同设计，实现了细粒度流水线并减少了DRAM流量，性能大幅提升。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 3DGS在移动设备上的性能不足，现有的加速器忽视了内存效率，导致DRAM流量冗余。

Method: 提出了一种名为STREAMINGGS的完全流式算法-架构协同设计，从基于瓦片的渲染转变为基于内存的渲染。

Result: 实验结果显示，该设计在移动Ampere GPU上实现了高达45.7倍的加速和62.9倍的能效提升。

Conclusion: STREAMINGGS通过优化内存和计算效率，显著提升了3DGS在移动设备上的性能。

Abstract: 3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and
sparse Gaussian-based representation. However, 3DGS struggles to meet the
real-time requirement of 90 frames per second (FPS) on resource-constrained
mobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on
compute efficiency but overlook memory efficiency, leading to redundant DRAM
traffic. We introduce STREAMINGGS, a fully streaming 3DGS
algorithm-architecture co-design that achieves fine-grained pipelining and
reduces DRAM traffic by transforming from a tile-centric rendering to a
memory-centric rendering. Results show that our design achieves up to 45.7
$\times$ speedup and 62.9 $\times$ energy savings over mobile Ampere GPUs.

</details>


### [188] [SILK: Smooth InterpoLation frameworK for motion in-betweening A Simplified Computational Approach](https://arxiv.org/abs/2506.09075)
*Elly Akhoundi,Hung Yu Ling,Anup Anand Deshmukh,Judith Butepage*

Key words: 运动插值, Transformer, 数据建模, 动画质量, 姿态表示

TL;DR: 本文提出了一种基于Transformer的简单框架，通过单一Transformer编码器实现高质量运动插值，强调数据建模选择的重要性。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 解决现有运动插值方法依赖复杂模型和多模块训练的局限性，寻求更简化的解决方案。

Method: 采用单一Transformer编码器，重点优化数据建模，包括数据量、姿态表示和速度输入特征。

Result: 实验表明，简单模型结合数据优化能实现高质量运动插值，挑战了模型复杂度的主导地位。

Conclusion: 数据中心的优化策略比模型复杂度更能提升运动插值效果，为动画领域提供新思路。

Abstract: Motion in-betweening is a crucial tool for animators, enabling intricate
control over pose-level details in each keyframe. Recent machine learning
solutions for motion in-betweening rely on complex models, incorporating
skeleton-aware architectures or requiring multiple modules and training steps.
In this work, we introduce a simple yet effective Transformer-based framework,
employing a single Transformer encoder to synthesize realistic motions for
motion in-betweening tasks. We find that data modeling choices play a
significant role in improving in-betweening performance. Among others, we show
that increasing data volume can yield equivalent or improved motion
transitions, that the choice of pose representation is vital for achieving
high-quality results, and that incorporating velocity input features enhances
animation performance. These findings challenge the assumption that model
complexity is the primary determinant of animation quality and provide insights
into a more data-centric approach to motion interpolation. Additional videos
and supplementary material are available at https://silk-paper.github.io.

</details>


### [189] [DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos](https://arxiv.org/abs/2506.09997)
*Chieh Hubert Lin,Zhaoyang Lv,Songyin Wu,Zhen Xu,Thu Nguyen-Phuoc,Hung-Yu Tseng,Julian Straub,Numair Khan,Lei Xiao,Ming-Hsuan Yang,Yuheng Ren,Richard Newcombe,Zhao Dong,Zhengqin Li*

Key words: 动态场景重建、3D高斯斑点、前馈模型、可变形表示

TL;DR: DGS-LRM 是首个前馈方法，可从单目视频预测动态场景的可变形3D高斯斑点，解决了现有方法局限于静态场景的不足。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 解决前馈动态场景重建的挑战，如训练数据稀缺、3D表示和训练范式需求。

Method: 引入大规模合成数据集、可变形3D高斯表示和大规模变压器网络。

Result: 重建质量媲美优化方法，优于现有动态重建方法，且在3D跟踪任务中表现出色。

Conclusion: DGS-LRM 高效且通用，适用于动态场景重建和3D跟踪。

Abstract: We introduce the Deformable Gaussian Splats Large Reconstruction Model
(DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian
splats from a monocular posed video of any dynamic scene. Feed-forward scene
reconstruction has gained significant attention for its ability to rapidly
create digital replicas of real-world environments. However, most existing
models are limited to static scenes and fail to reconstruct the motion of
moving objects. Developing a feed-forward model for dynamic scene
reconstruction poses significant challenges, including the scarcity of training
data and the need for appropriate 3D representations and training paradigms. To
address these challenges, we introduce several key technical contributions: an
enhanced large-scale synthetic dataset with ground-truth multi-view videos and
dense 3D scene flow supervision; a per-pixel deformable 3D Gaussian
representation that is easy to learn, supports high-quality dynamic view
synthesis, and enables long-range 3D tracking; and a large transformer network
that achieves real-time, generalizable dynamic scene reconstruction. Extensive
qualitative and quantitative experiments demonstrate that DGS-LRM achieves
dynamic scene reconstruction quality comparable to optimization-based methods,
while significantly outperforming the state-of-the-art predictive dynamic
reconstruction method on real-world examples. Its predicted physically grounded
3D deformation is accurate and can readily adapt for long-range 3D tracking
tasks, achieving performance on par with state-of-the-art monocular video 3D
tracking methods.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [190] [Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian Mixtures and Part Discovery](https://arxiv.org/abs/2506.09063)
*Shayan Shekarforoush,David B. Lindell,Marcus A. Brubaker,David J. Fleet*

Key words: Cryo-EM, 3D重建, 高斯混合模型, 非刚体, 组成变化

TL;DR: 该论文介绍了CryoSPIRE，一种基于高斯混合模型的新型3D重建框架，用于处理冷冻电镜图像中的非刚体构象灵活性和组成变化问题。

<details>
  <summary>Details</summary>

Main category: q-bio.QM

Motivation: 冷冻电镜（Cryo-EM）已成为分子生物学的重要工具，但其在非刚体构象灵活性和组成变化情况下的建模仍具有挑战性。

Method: 采用分层高斯混合模型，结合部分分割方法，处理构象和组成变化。

Result: CryoSPIRE在复杂实验数据中揭示了生物学意义的结构，并在CryoBench基准测试中达到新最优。

Conclusion: CryoSPIRE框架为冷冻电镜中的非刚性和组成变化问题提供了有效解决方案。

Abstract: Cryo-EM is a transformational paradigm in molecular biology where
computational methods are used to infer 3D molecular structure at atomic
resolution from extremely noisy 2D electron microscope images. At the forefront
of research is how to model the structure when the imaged particles exhibit
non-rigid conformational flexibility and compositional variation where parts
are sometimes missing. We introduce a novel 3D reconstruction framework with a
hierarchical Gaussian mixture model, inspired in part by Gaussian Splatting for
4D scene reconstruction. In particular, the structure of the model is grounded
in an initial process that infers a part-based segmentation of the particle,
providing essential inductive bias in order to handle both conformational and
compositional variability. The framework, called CryoSPIRE, is shown to reveal
biologically meaningful structures on complex experimental datasets, and
establishes a new state-of-the-art on CryoBench, a benchmark for cryo-EM
heterogeneity methods.

</details>


### [191] [Detecting malignant dynamics on very few blood sample using signature coefficients](https://arxiv.org/abs/2506.09097)
*Rémi Vaucher,Stéphane Chrétien*

Key words: ctDNA, 签名理论, 癌症监测, 马尔可夫模型, 机器学习

TL;DR: 该论文探讨了利用循环肿瘤DNA（ctDNA）水平和签名理论相结合的方法，通过血液样本监测癌症，解决了数据稀疏性问题。

<details>
  <summary>Details</summary>

Main category: q-bio.QM

Motivation: 通过ctDNA水平动态监测癌症早期检测的需求，提出结合签名理论以提高检测效率和准确性。

Method: 结合连续时间马尔可夫模型和签名理论，构建高效的测试流程，解决数据稀疏性问题。

Result: 通过广泛的数值实验验证了所提方法的有效性，能够准确检测侵袭性癌症肿瘤。

Conclusion: 签名理论与ctDNA分析的结合为癌症监测提供了一种高效且低负担的方法。

Abstract: Recent discoveries have suggested that the promising avenue of using
circulating tumor DNA (ctDNA) levels in blood samples provides reasonable
accuracy for cancer monitoring, with extremely low burden on the patient's
side. It is known that the presence of ctDNA can result from various mechanisms
leading to DNA release from cells, such as apoptosis, necrosis or active
secretion. One key idea in recent cancer monitoring studies is that monitoring
the dynamics of ctDNA levels might be sufficient for early multi-cancer
detection. This interesting idea has been turned into commercial products, e.g.
in the company named GRAIL.
  In the present work, we propose to explore the use of Signature theory for
detecting aggressive cancer tumors based on the analysis of blood samples. Our
approach combines tools from continuous time Markov modelling for the dynamics
of ctDNA levels in the blood, with Signature theory for building efficient
testing procedures. Signature theory is a topic of growing interest in the
Machine Learning community (see Chevyrev2016 and Fermanian2021), which is now
recognised as a powerful feature extraction tool for irregularly sampled
signals. The method proposed in the present paper is shown to correctly address
the challenging problem of overcoming the inherent data scarsity due to the
extremely small number of blood samples per patient. The relevance of our
approach is illustrated with extensive numerical experiments that confirm the
efficiency of the proposed pipeline.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [192] [A Survey on the Role of Artificial Intelligence and Machine Learning in 6G-V2X Applications](https://arxiv.org/abs/2506.09512)
*Donglin Wang,Anjie Qiu,Qiuheng Zhou,Hans D. Schotten*

Key words: 6G-V2X通信, 人工智能, 机器学习, 深度学习, 强化学习, 生成学习, 联邦学习, 智能交通系统

TL;DR: 该论文综述了人工智能（AI）和机器学习（ML）在6G-V2X通信中的最新进展，重点介绍了深度学习、强化学习、生成学习和联邦学习等技术，并探讨了其在智能交通系统中的潜力与挑战。

<details>
  <summary>Details</summary>

Main category: eess.SY

Motivation: 随着6G网络的发展，V2X通信在智能交通系统中变得愈发重要。AI和ML因其在自然语言处理和计算机视觉等领域的出色表现，被视为优化V2X通信的关键技术，但目前缺乏对这些技术应用的系统性总结。

Method: 论文通过综述的方式，回顾了过去两年中应用于6G-V2X通信的AI和ML模型，尤其是深度学习、强化学习、生成学习和联邦学习等先进技术。

Result: 研究表明，AI（特别是生成学习）在提升6G-V2X系统的性能、适应性和智能化方面具有显著潜力。同时，论文总结了技术挑战，如计算复杂性、数据隐私和实时决策限制。

Conclusion: 论文为研究人员、工程师和政策制定者提供了有价值的见解，旨在推动6G通信中AI驱动的V2X生态系统的实现，并指出了未来的研究方向。

Abstract: The rapid advancement of Vehicle-to-Everything (V2X) communication is
transforming Intelligent Transportation Systems (ITS), with 6G networks
expected to provide ultra-reliable, low-latency, and high-capacity connectivity
for Connected and Autonomous Vehicles (CAVs). Artificial Intelligence (AI) and
Machine Learning (ML) have emerged as key enablers in optimizing V2X
communication by enhancing network management, predictive analytics, security,
and cooperative driving due to their outstanding performance across various
domains, such as natural language processing and computer vision. This survey
comprehensively reviews recent advances in AI and ML models applied to 6G-V2X
communication. It focuses on state-of-the-art techniques, including Deep
Learning (DL), Reinforcement Learning (RL), Generative Learning (GL), and
Federated Learning (FL), with particular emphasis on developments from the past
two years. Notably, AI, especially GL, has shown remarkable progress and
emerging potential in enhancing the performance, adaptability, and intelligence
of 6G-V2X systems. Despite these advances, a systematic summary of recent
research efforts in this area remains lacking, which this survey aims to
address. We analyze their roles in 6G-V2X applications, such as intelligent
resource allocation, beamforming, intelligent traffic management, and security
management. Furthermore, we explore the technical challenges, including
computational complexity, data privacy, and real-time decision-making
constraints, while identifying future research directions for AI-driven 6G-V2X
development. This study aims to provide valuable insights for researchers,
engineers, and policymakers working towards realizing intelligent, AI-powered
V2X ecosystems in 6G communication.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [193] [Exploring Image Transforms derived from Eye Gaze Variables for Progressive Autism Diagnosis](https://arxiv.org/abs/2506.09065)
*Abigail Copiaco,Christian Ritz,Yassine Himeur,Valsamma Eapen,Ammar Albanna,Wathiq Mansoor*

Key words: 自闭症谱系障碍, 人工智能, 诊断技术, 迁移学习, 眼动追踪

TL;DR: 本文提出了一种基于人工智能的辅助技术，通过整合迁移学习和眼动变量生成的图像变换，实现自闭症谱系障碍（ASD）的快速诊断和管理。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 由于ASD诊断技术的耗时性带来高昂的社会和经济成本，需要一种更高效、便捷的诊断方法。

Method: 结合迁移学习和眼动变量生成的图像变换，设计了一个AI驱动的诊断系统。

Result: 该方法不仅提高了诊断效率，还支持家庭定期诊断，保护用户隐私，并改善了监护人与治疗师之间的沟通。

Conclusion: 所提出的方法能够实现及时、便捷的ASD诊断，同时保护隐私，从而改善患者的治疗效果。

Abstract: The prevalence of Autism Spectrum Disorder (ASD) has surged rapidly over the
past decade, posing significant challenges in communication, behavior, and
focus for affected individuals. Current diagnostic techniques, though
effective, are time-intensive, leading to high social and economic costs. This
work introduces an AI-powered assistive technology designed to streamline ASD
diagnosis and management, enhancing convenience for individuals with ASD and
efficiency for caregivers and therapists. The system integrates transfer
learning with image transforms derived from eye gaze variables to diagnose ASD.
This facilitates and opens opportunities for in-home periodical diagnosis,
reducing stress for individuals and caregivers, while also preserving user
privacy through the use of image transforms. The accessibility of the proposed
method also offers opportunities for improved communication between guardians
and therapists, ensuring regular updates on progress and evolving support
needs. Overall, the approach proposed in this work ensures timely, accessible
diagnosis while protecting the subjects' privacy, improving outcomes for
individuals with ASD.

</details>


### [194] [Foundation Models in Medical Imaging -- A Review and Outlook](https://arxiv.org/abs/2506.09095)
*Vivien van Veldhuizen,Vanessa Botha,Chunyao Lu,Melis Erdal Cesur,Kevin Groot Lipman,Edwin D. de Jong,Hugo Horlings,Clárisa Sanchez,Cees Snoek,Ritse Mann,Eric Marcus,Jonas Teuwen*

Key words: 基础模型, 医学影像, 自监督学习, 病理学, 放射学, 眼科学

TL;DR: 综述探讨了基础模型在医学影像分析中的应用，重点介绍了其在病理学、放射学和眼科学中的发展及150多项研究证据，并讨论了模型架构、自监督学习方法及下游适应策略。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 基础模型通过大量无标签数据学习通用特征，减少了对人工标注的依赖，为医学影像分析提供了新的可能性。

Method: 分析了基础模型的管道核心组件，包括模型架构、自监督学习方法及下游适应策略，并比较了不同应用领域的设计选择。

Result: 基础模型在病理学、放射学和眼科学中的应用显示出潜力，但仍存在挑战。

Conclusion: 综述总结了基础模型在医学影像中的应用现状及未来研究方向。

Abstract: Foundation models (FMs) are changing the way medical images are analyzed by
learning from large collections of unlabeled data. Instead of relying on
manually annotated examples, FMs are pre-trained to learn general-purpose
visual features that can later be adapted to specific clinical tasks with
little additional supervision. In this review, we examine how FMs are being
developed and applied in pathology, radiology, and ophthalmology, drawing on
evidence from over 150 studies. We explain the core components of FM pipelines,
including model architectures, self-supervised learning methods, and strategies
for downstream adaptation. We also review how FMs are being used in each
imaging domain and compare design choices across applications. Finally, we
discuss key challenges and open questions to guide future research.

</details>


### [195] [Low-Rank Augmented Implicit Neural Representation for Unsupervised High-Dimensional Quantitative MRI Reconstruction](https://arxiv.org/abs/2506.09100)
*Haonan Zhang,Guoyan Lao,Yuyao Zhang,Hongjiang Wei*

Key words: MP-qMRI, 低秩表示, 隐式神经表示, 无监督学习, 零样本学习

TL;DR: 提出了一种名为LoREIN的新型无监督双先验集成框架，用于加速3D多参数定量MRI重建，结合低秩先验和连续性先验以提高重建精度。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 现有方法仅依赖单一先验或物理模型解决高度不适定的逆问题，导致结果不理想，因此需要一种更鲁棒的框架。

Method: LoREIN结合低秩表示（LRR）和隐式神经表示（INR），利用低秩先验和连续性先验增强重建保真度。

Result: 该框架成功提高了加权图像和定量参数图的重建精度，并引入了零样本学习范式，具有广泛潜力。

Conclusion: LoREIN在医学影像重建任务中表现出色，为复杂时空和高维图像重建提供了新思路。

Abstract: Quantitative magnetic resonance imaging (qMRI) provides tissue-specific
parameters vital for clinical diagnosis. Although simultaneous multi-parametric
qMRI (MP-qMRI) technologies enhance imaging efficiency, robustly reconstructing
qMRI from highly undersampled, high-dimensional measurements remains a
significant challenge. This difficulty arises primarily because current
reconstruction methods that rely solely on a single prior or physics-informed
model to solve the highly ill-posed inverse problem, which often leads to
suboptimal results. To overcome this limitation, we propose LoREIN, a novel
unsupervised and dual-prior-integrated framework for accelerated 3D MP-qMRI
reconstruction. Technically, LoREIN incorporates both low-rank prior and
continuity prior via low-rank representation (LRR) and implicit neural
representation (INR), respectively, to enhance reconstruction fidelity. The
powerful continuous representation of INR enables the estimation of optimal
spatial bases within the low-rank subspace, facilitating high-fidelity
reconstruction of weighted images. Simultaneously, the predicted multi-contrast
weighted images provide essential structural and quantitative guidance, further
enhancing the reconstruction accuracy of quantitative parameter maps.
Furthermore, our work introduces a zero-shot learning paradigm with broad
potential in complex spatiotemporal and high-dimensional image reconstruction
tasks, further advancing the field of medical imaging.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [196] [You Are What You Say: Exploiting Linguistic Content for VoicePrivacy Attacks](https://arxiv.org/abs/2506.09521)
*Ünal Ege Gaznepoglu,Anna Leschanowsky,Ahmad Aloradi,Prachi Singh,Daniel Tenbrinck,Emanuël A. P. Habets,Nils Peters*

Key words: 说话人匿名化、自动说话人验证、BERT、隐私评估、EER

TL;DR: 该研究评估了说话人匿名化系统中的隐私保护效果，通过改进BERT模型作为自动说话人验证系统，揭示了数据集内语言学内容的相似性对隐私评估的潜在偏差。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 研究旨在探讨说话人匿名化技术的隐私保护效果，尤其关注攻击者训练和评估数据集中语言学内容相似性的影响。

Method: 采用BERT作为自动说话人验证系统，分析语音隐私攻击数据集中的文本内容相似性。

Result: 在VoicePrivacy挑战数据集上，平均等错误率（EER）为35%，部分说话人EER低至2%，且发现系统决策与语义相似关键词相关。

Conclusion: 建议重新设计VoicePrivacy数据集以确保公正评估，并质疑全局EER作为隐私评估标准的可靠性。

Abstract: Speaker anonymization systems hide the identity of speakers while preserving
other information such as linguistic content and emotions. To evaluate their
privacy benefits, attacks in the form of automatic speaker verification (ASV)
systems are employed. In this study, we assess the impact of intra-speaker
linguistic content similarity in the attacker training and evaluation datasets,
by adapting BERT, a language model, as an ASV system. On the VoicePrivacy
Attacker Challenge datasets, our method achieves a mean equal error rate (EER)
of 35%, with certain speakers attaining EERs as low as 2%, based solely on the
textual content of their utterances. Our explainability study reveals that the
system decisions are linked to semantically similar keywords within utterances,
stemming from how LibriSpeech is curated. Our study suggests reworking the
VoicePrivacy datasets to ensure a fair and unbiased evaluation and challenge
the reliance on global EER for privacy evaluations.

</details>


### [197] [Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements](https://arxiv.org/abs/2506.09707)
*Suhas BN,Andrew M. Sherrill,Jyoti Alaparthi,Dominik Mattioli,Rosa I. Arriaga,Chris W. Wiese,Saeed Abdullah*

Key words: 延长暴露疗法（PE）, 治疗师忠实度, 音频-文本模型, LoRA, 时间定位

TL;DR: 提出了一种基于音频-文本输入的自动化方法，用于定位延长暴露疗法（PE）中的核心元素，通过微调预训练模型Qwen2-Audio和LoRA技术，实现了5.3秒的平均绝对误差。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 手动评估PE治疗师忠实度耗时且费力，亟需自动化解决方案以提高效率。

Method: 使用Qwen2-Audio模型和LoRA技术处理30秒音频-文本窗口，结合LLM生成的标签和人工验证，预测边界偏移。

Result: 在313个真实PE会话数据上，最佳配置（LoRA rank 8，30秒窗口）的平均绝对误差为5.3秒。

Conclusion: 该框架为PE治疗中的忠实度跟踪提供了可扩展的解决方案，有望支持临床培训、监督和质量保证。

Abstract: Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic
stress disorder (PTSD), but evaluating therapist fidelity remains
labor-intensive due to the need for manual review of session recordings. We
present a method for the automatic temporal localization of key PE fidelity
elements -- identifying their start and stop times -- directly from session
audio and transcripts. Our approach fine-tunes a large pre-trained
audio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process
focused 30-second windows of audio-transcript input. Fidelity labels for three
core protocol phases -- therapist orientation (P1), imaginal exposure (P2), and
post-imaginal processing (P3) -- are generated via LLM-based prompting and
verified by trained raters. The model is trained to predict normalized boundary
offsets using soft supervision guided by task-specific prompts. On a dataset of
313 real PE sessions, our best configuration (LoRA rank 8, 30s windows)
achieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further
analyze the effects of window size and LoRA rank, highlighting the importance
of context granularity and model adaptation. This work introduces a scalable
framework for fidelity tracking in PE therapy, with potential to support
clinician training, supervision, and quality assurance.

</details>


### [198] [Regularizing Learnable Feature Extraction for Automatic Speech Recognition](https://arxiv.org/abs/2506.09804)
*Peter Vieting,Maximilian Kannen,Benedikt Hilmes,Ralf Schlüter,Hermann Ney*

Key words: ASR, 神经网络前端, 正则化, SpecAugment, STFT

TL;DR: 研究了神经网络前端在ASR系统中的正则化方法，通过音频扰动和STFT域掩码改进其性能。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 神经网络前端在ASR系统中表现不佳，主要因其易过拟合。

Method: 提出音频扰动和STFT域掩码两种正则化方法。

Result: 成功缩小了传统方法与可学习特征的性能差距。

Conclusion: 综合应用两种正则化方法可显著提升神经网络前端的表现。

Abstract: Neural front-ends are an appealing alternative to traditional, fixed feature
extraction pipelines for automatic speech recognition (ASR) systems since they
can be directly trained to fit the acoustic model. However, their performance
often falls short compared to classical methods, which we show is largely due
to their increased susceptibility to overfitting. This work therefore
investigates regularization methods for training ASR models with learnable
feature extraction front-ends. First, we examine audio perturbation methods and
show that larger relative improvements can be obtained for learnable features.
Additionally, we identify two limitations in the standard use of SpecAugment
for these front-ends and propose masking in the short time Fourier transform
(STFT)-domain as a simple but effective modification to address these
challenges. Finally, integrating both regularization approaches effectively
closes the performance gap between traditional and learnable features.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [199] [Designing conflict-based communicative tasks in Teaching Chinese as a Foreign Language with ChatGPT](https://arxiv.org/abs/2506.09089)
*Xia Li*

Key words: 汉语教学, 口语表达, ChatGPT, 任务设计, 互动技能

TL;DR: 摘要介绍了一项大学汉语口语教学中，教师设计基于冲突的交际任务以提高学生互动能力，并使用ChatGPT辅助课程开发的研究，重点探讨了教师与ChatGPT的互动特征及其影响。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 研究动机是探索ChatGPT在汉语口语教学任务设计中的辅助作用及其互动特征。

Method: 方法为教师设计基于冲突的交际任务，并利用ChatGPT辅助完成教学计划。

Result: 结果展示了教师与ChatGPT的互动特性及ChatGPT在特定教学情境中的应用效果。

Conclusion: 结论强调了ChatGPT在汉语口语教学任务设计中的潜在价值和实际影响。

Abstract: In developing the teaching program for a course in Oral Expression in
Teaching Chinese as a Foreign Language at the university level, the teacher
designs communicative tasks based on conflicts to encourage learners to engage
in interactive dynamics and develop their oral interaction skills. During the
design of these tasks, the teacher uses ChatGPT to assist in finalizing the
program. This article aims to present the key characteristics of the
interactions between the teacher and ChatGPT during this program development
process, as well as to examine the use of ChatGPT and its impacts in this
specific context.

</details>


### [200] ["Is This Really a Human Peer Supporter?": Misalignments Between Peer Supporters and Experts in LLM-Supported Interactions](https://arxiv.org/abs/2506.09354)
*Kellie Yu Hui Sim,Roy Ka-Wei Lee,Kenny Tsu Wei Choo*

Key words: 心理健康, AI, 同伴支持, 大型语言模型, 心理咨询

TL;DR: 研究探讨了AI如何通过大型语言模型(LLMs)提升心理健康同伴支持的效果，发现系统能改善互动质量但存在专业与非专业人士间的关键分歧。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 全球心理健康问题日益严重，AI驱动的方法可能扩展心理社会支持的可及性，同伴支持作为专业护理的补充，但其质量和安全性存在挑战。

Method: 开发了一个AI支持系统，包含LLM模拟的求助者、情境敏感的建议和实时情绪可视化，通过12名同伴支持者和5名专家的混合方法研究评估系统效果。

Result: 系统和LLM的建议被认为能提升培训质量，但专家发现同伴支持者常忽略求助信号和过早给出建议，显示培训不足的问题。

Conclusion: 研究强调了标准化和心理学基础培训的重要性，并展示了LLM在支持心理健康领域的潜力，但需专家指导和谨慎设计。

Abstract: Mental health is a growing global concern, prompting interest in AI-driven
solutions to expand access to psychosocial support. Peer support, grounded in
lived experience, offers a valuable complement to professional care. However,
variability in training, effectiveness, and definitions raises concerns about
quality, consistency, and safety. Large Language Models (LLMs) present new
opportunities to enhance peer support interactions, particularly in real-time,
text-based interactions. We present and evaluate an AI-supported system with an
LLM-simulated distressed client, context-sensitive LLM-generated suggestions,
and real-time emotion visualisations. 2 mixed-methods studies with 12 peer
supporters and 5 mental health professionals (i.e., experts) examined the
system's effectiveness and implications for practice. Both groups recognised
its potential to enhance training and improve interaction quality. However, we
found a key tension emerged: while peer supporters engaged meaningfully,
experts consistently flagged critical issues in peer supporter responses, such
as missed distress cues and premature advice-giving. This misalignment
highlights potential limitations in current peer support training, especially
in emotionally charged contexts where safety and fidelity to best practices are
essential. Our findings underscore the need for standardised, psychologically
grounded training, especially as peer support scales globally. They also
demonstrate how LLM-supported systems can scaffold this development--if
designed with care and guided by expert oversight. This work contributes to
emerging conversations on responsible AI integration in mental health and the
evolving role of LLMs in augmenting peer-delivered care.

</details>


### [201] ["I Said Things I Needed to Hear Myself": Peer Support as an Emotional, Organisational, and Sociotechnical Practice in Singapore](https://arxiv.org/abs/2506.09362)
*Kellie Yu Hui Sim,Kenny Tsu Wei Choo*

Key words: 同伴支持；数字平台；心理健康；文化响应设计；AI

TL;DR: 本文探讨了新加坡20位同伴支持者在线上线下环境中提供心理支持的情况，分析了他们的动机、情感劳动及社会文化因素，并提出了针对文化敏感的数字化工具设计方向。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 研究数字平台在心理支持中的作用，尤其是在亚洲背景下，以填补设计和影响方面的研究空白。

Method: 通过对20位新加坡同伴支持者的访谈研究，采用主题分析方法。

Result: 揭示了同伴支持者的动机、情感劳动及其社会文化背景，提出了文化敏感的数字化工具设计方向。

Conclusion: 研究为心理健康领域中可信赖且情境敏感的AI提供了设计启示，强调了以人为本的计算视角。

Abstract: Peer support plays a vital role in expanding access to mental health care by
providing empathetic, community-based support outside formal clinical systems.
As digital platforms increasingly mediate such support, the design and impact
of these technologies remain under-examined, particularly in Asian contexts.
This paper presents findings from an interview study with 20 peer supporters in
Singapore, who operate across diverse online, offline, and hybrid environments.
Through a thematic analysis, we unpack how participants start, conduct, and
sustain peer support, highlighting their motivations, emotional labour, and the
sociocultural dimensions shaping their practices. Building on this grounded
understanding, we surface design directions for culturally responsive digital
tools that scaffold rather than supplant relational care. Drawing insights from
qualitative accounts, we offer a situated perspective on how AI might
responsibly augment peer support. This research contributes to human-centred
computing by articulating the lived realities of peer supporters and proposing
design implications for trustworthy and context-sensitive AI in mental health.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [202] [Revolutionizing Clinical Trials: A Manifesto for AI-Driven Transformation](https://arxiv.org/abs/2506.09102)
*Mihaela van der Schaar,Richard Peck,Eoin McKinney,Jim Weatherall,Stuart Bailey,Justine Rochon,Chris Anagnostopoulos,Pierre Marquet,Anthony Wood,Nicky Best,Harry Amad,Julianna Piskorz,Krzysztof Kacprzyk,Rafik Salama,Christina Gunther,Francesca Frau,Antoine Pugeat,Ramon Hernandez*

Key words: AI、临床试验、因果推理、数字孪生、监管框架

TL;DR: 合作愿景提出利用因果推理和数字孪生技术优化临床试验，旨在提供更快、更安全、更个性化的结果。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 通过AI技术提升临床试验的效率和质量，同时符合现有监管框架。

Method: 提出了因果推理和数字孪生两项AI技术的应用路径。

Result: 有望通过AI技术变革临床研究，重新定义临床试验的金标准。

Conclusion: AI技术可以推动临床试验的革命，但仍需在现有监管框架内实施。

Abstract: This manifesto represents a collaborative vision forged by leaders in
pharmaceuticals, consulting firms, clinical research, and AI. It outlines a
roadmap for two AI technologies - causal inference and digital twins - to
transform clinical trials, delivering faster, safer, and more personalized
outcomes for patients. By focusing on actionable integration within existing
regulatory frameworks, we propose a way forward to revolutionize clinical
research and redefine the gold standard for clinical trials using AI.

</details>


### [203] [FAIRTOPIA: Envisioning Multi-Agent Guardianship for Disrupting Unfair AI Pipelines](https://arxiv.org/abs/2506.09107)
*Athena Vakali,Ilias Dimitriadis*

Key words: AI公平性, 代理技术, FAIRTOPIA框架, 人类中心原则

TL;DR: 该论文提出了一种名为FAIRTOPIA的框架，通过多角色代理技术在AI流程中嵌入公平性保障机制，以解决现有AI系统中的不公平问题。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: AI技术快速发展导致了许多有害事件，现行AI流程忽视人类原则，亟需引入公平性保障机制。

Method: 采用代理技术构建FAIRTOPIA框架，通过三层架构和多角色代理实现公平性的持续监控。

Result: 提出的框架能够在AI流程的各个阶段实现公平性保障，并促进基于人类中心原则的新研究方法。

Conclusion: FAIRTOPIA框架为AI公平性研究提供了新方向，强调了系统性和跨学科的重要性。

Abstract: AI models have become active decision makers, often acting without human
supervision. The rapid advancement of AI technology has already caused harmful
incidents that have hurt individuals and societies and AI unfairness in heavily
criticized. It is urgent to disrupt AI pipelines which largely neglect human
principles and focus on computational biases exploration at the data (pre),
model(in), and deployment (post) processing stages. We claim that by exploiting
the advances of agents technology, we will introduce cautious, prompt, and
ongoing fairness watch schemes, under realistic, systematic, and human-centric
fairness expectations. We envision agents as fairness guardians, since agents
learn from their environment, adapt to new information, and solve complex
problems by interacting with external tools and other systems. To set the
proper fairness guardrails in the overall AI pipeline, we introduce a
fairness-by-design approach which embeds multi-role agents in an end-to-end
(human to AI) synergetic scheme. Our position is that we may design adaptive
and realistic AI fairness frameworks, and we introduce a generalized algorithm
which can be customized to the requirements and goals of each AI decision
making scenario. Our proposed, so called FAIRTOPIA framework, is structured
over a three-layered architecture, which encapsulates the AI pipeline inside an
agentic guardian and a knowledge-based, self-refining layered scheme. Based on
our proposition, we enact fairness watch in all of the AI pipeline stages,
under robust multi-agent workflows, which will inspire new fairness research
hypothesis, heuristics, and methods grounded in human-centric, systematic,
interdisciplinary, socio-technical principles.

</details>


### [204] [Understanding Human-AI Trust in Education](https://arxiv.org/abs/2506.09160)
*Griffin Pitts,Sanaz Motamedi*

Key words: AI聊天机器人, 教育, 信任机制, 人机交互, 结构方程模型

TL;DR: 研究了AI聊天机器人在教育中的应用，探讨学生对AI的信任是类似人际信任还是技术信任，并提出了新的人机信任理论框架。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: AI聊天机器人在教育中使用日益增多，但学生对AI的信任机制尚不明确，需要区分其是基于人际信任还是技术信任。

Method: 通过偏最小二乘结构方程模型，分析了学生对AI聊天机器人的人类化信任与系统化信任对其使用意愿、感知有用性和愉悦感的影响。

Result: 人类化信任更强地影响信任意愿，系统化信任更显著地影响使用意愿和感知有用性，两者对愉悦感的影响相似。

Conclusion: 提出了一种新的人机信任模型，不同于传统人际或技术信任，为AI在教育中的应用提供了理论和实践指导。

Abstract: As AI chatbots become increasingly integrated in education, students are
turning to these systems for guidance, feedback, and information. However, the
anthropomorphic characteristics of these chatbots create ambiguity regarding
whether students develop trust toward them as they would a human peer or
instructor, based in interpersonal trust, or as they would any other piece of
technology, based in technology trust. This ambiguity presents theoretical
challenges, as interpersonal trust models may inappropriately ascribe human
intentionality and morality to AI, while technology trust models were developed
for non-social technologies, leaving their applicability to anthropomorphic
systems unclear. To address this gap, we investigate how human-like and
system-like trusting beliefs comparatively influence students' perceived
enjoyment, trusting intention, behavioral intention to use, and perceived
usefulness of an AI chatbot - factors associated with students' engagement and
learning outcomes. Through partial least squares structural equation modeling,
we found that human-like and system-like trust significantly influenced student
perceptions, with varied effects. Human-like trust more strongly predicted
trusting intention, while system-like trust better predicted behavioral
intention and perceived usefulness. Both had similar effects on perceived
enjoyment. Given the partial explanatory power of each type of trust, we
propose that students develop a distinct form of trust with AI chatbots
(human-AI trust) that differs from human-human and human-technology models of
trust. Our findings highlight the need for new theoretical frameworks specific
to human-AI trust and offer practical insights for fostering appropriately
calibrated trust, which is critical for the effective adoption and pedagogical
impact of AI in education.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [205] [Know What You Don't Know: Uncertainty Calibration of Process Reward Models](https://arxiv.org/abs/2506.09338)
*Young-Jin Park,Kristjan Greenewald,Kaveh Alim,Hao Wang,Navid Azizan*

Key words: PRM, 分位数回归, 校准, 实例自适应扩展, 推理效率

TL;DR: 该论文提出了一种基于分位数回归的PRM校准方法，并引入实例自适应扩展框架（IAS），显著降低了推理成本同时保持准确性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 现有PRM模型在预测成功概率时存在校准不足和过高估计的问题，影响了推理效率。

Method: 使用分位数回归校准PRM输出，提出IAS框架动态调整推理预算。

Result: 校准方法减小了校准误差，IAS框架在保持准确性的同时降低了推理成本。

Conclusion: 校准后的PRM和IAS框架显著提升了推理效率和成本效益。

Abstract: Process reward models (PRMs) play a central role in guiding inference-time
scaling algorithms for large language models (LLMs). However, we observe that
even state-of-the-art PRMs can be poorly calibrated and often overestimate
success probabilities. To address this, we present a calibration approach,
performed via quantile regression, that adjusts PRM outputs to better align
with true success probabilities. Leveraging these calibrated success estimates
and their associated confidence bounds, we introduce an \emph{instance-adaptive
scaling} (IAS) framework that dynamically adjusts the inference budget based on
the estimated likelihood that a partial reasoning trajectory will yield a
correct final answer. Unlike conventional methods that allocate a fixed number
of reasoning trajectories per query, this approach successfully adapts to each
instance and reasoning step when using our calibrated PRMs. Experiments on
mathematical reasoning benchmarks show that (i) our PRM calibration method
successfully achieves small calibration error, outperforming the baseline
methods, (ii) calibration is crucial for enabling effective adaptive scaling,
and (iii) the proposed IAS strategy reduces inference costs while maintaining
final answer accuracy, utilizing less compute on more confident problems as
desired.

</details>


### [206] [Attention-Bayesian Hybrid Approach to Modular Multiple Particle Tracking](https://arxiv.org/abs/2506.09441)
*Piyush Mishra,Philippe Roudot*

Key words: 多粒子跟踪，Transformer，贝叶斯滤波，自注意力，假设剪枝

TL;DR: 提出了一种结合自注意力机制和贝叶斯滤波的混合跟踪框架，用于噪声和杂乱场景中的多粒子跟踪，提高了跟踪准确性和鲁棒性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 由于轨迹假设的组合爆炸问题，传统的多粒子跟踪在噪声和杂乱场景中面临挑战。虽然Transformer架构在减少假设空间方面表现优异，但在稀疏场景下仍无法达到贝叶斯滤波的最优性能，因此需要结合两者的优势。

Method: 采用混合跟踪框架，利用Transformer编码器学习粒子行为的底层表示并推断帧间检测的软关联，剪枝假设集后，在贝叶斯滤波框架中进行高效的轨迹关联。

Result: 该方法在噪声和高杂乱场景中表现出更高的跟踪准确性和对虚假检测的鲁棒性。

Conclusion: 混合框架结合了Transformer的学习能力和贝叶斯滤波的可靠性，为多粒子跟踪提供了一种有效解决方案。

Abstract: Tracking multiple particles in noisy and cluttered scenes remains challenging
due to a combinatorial explosion of trajectory hypotheses, which scales
super-exponentially with the number of particles and frames. The transformer
architecture has shown a significant improvement in robustness against this
high combinatorial load. However, its performance still falls short of the
conventional Bayesian filtering approaches in scenarios presenting a reduced
set of trajectory hypothesis. This suggests that while transformers excel at
narrowing down possible associations, they may not be able to reach the
optimality of the Bayesian approach in locally sparse scenario. Hence, we
introduce a hybrid tracking framework that combines the ability of
self-attention to learn the underlying representation of particle behavior with
the reliability and interpretability of Bayesian filtering. We perform
trajectory-to-detection association by solving a label prediction problem,
using a transformer encoder to infer soft associations between detections
across frames. This prunes the hypothesis set, enabling efficient
multiple-particle tracking in Bayesian filtering framework. Our approach
demonstrates improved tracking accuracy and robustness against spurious
detections, offering a solution for high clutter multiple particle tracking
scenarios.

</details>


### [207] [LLM-Powered CPI Prediction Inference with Online Text Time Series](https://arxiv.org/abs/2506.09516)
*Yingying Fan,Jinchi Lv,Ao Sun,Yurou Wang*

Key words: CPI预测、大语言模型、高频文本数据、BERT、时间序列

TL;DR: 本文提出了一种基于大语言模型（LLM）的CPI预测方法LLM-CPI，通过结合高频在线文本数据改进传统的低频调查数据方法。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: CPI预测在经济学中重要但具有挑战性，传统方法依赖低频数据，而LLM的发展为利用高频文本数据提供了新机会。

Method: 通过收集社交网络高频文本，使用LLM（如ChatGPT和BERT）构建通胀标签，提取文本嵌入，并开发结合月度CPI数据和每日CPI代理的联合时间序列框架。

Result: LLM-CPI在模拟和实际数据中表现出色，并提供了预测区间。

Conclusion: LLM-CPI利用高频文本数据改进了CPI预测，具有实际应用优势。

Abstract: Forecasting the Consumer Price Index (CPI) is an important yet challenging
task in economics, where most existing approaches rely on low-frequency,
survey-based data. With the recent advances of large language models (LLMs),
there is growing potential to leverage high-frequency online text data for
improved CPI prediction, an area still largely unexplored. This paper proposes
LLM-CPI, an LLM-based approach for CPI prediction inference incorporating
online text time series. We collect a large set of high-frequency online texts
from a popularly used Chinese social network site and employ LLMs such as
ChatGPT and the trained BERT models to construct continuous inflation labels
for posts that are related to inflation. Online text embeddings are extracted
via LDA and BERT. We develop a joint time series framework that combines
monthly CPI data with LLM-generated daily CPI surrogates. The monthly model
employs an ARX structure combining observed CPI data with text embeddings and
macroeconomic variables, while the daily model uses a VARX structure built on
LLM-generated CPI surrogates and text embeddings. We establish the asymptotic
properties of the method and provide two forms of constructed prediction
intervals. The finite-sample performance and practical advantages of LLM-CPI
are demonstrated through both simulation and real data examples.

</details>


### [208] [Evasion Attacks Against Bayesian Predictive Models](https://arxiv.org/abs/2506.09640)
*Pablo G. Arce,Roi Naveiro,David Ríos Insua*

Key words: 对抗攻击,贝叶斯预测模型,梯度攻击,规避攻击

TL;DR: 摘要提出了一种针对贝叶斯预测模型的最优规避攻击方法，研究了两种对抗目标：扰动特定点预测和改变整个后验预测分布。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 研究贝叶斯预测模型在对抗攻击中的脆弱性，填补现有研究的空白。

Method: 提出了基于梯度的攻击方法，并探讨其在多种计算环境中的实现和特性。

Result: 展示了针对点预测和后验预测分布的有效攻击策略。

Conclusion: 该研究为分析贝叶斯模型的对抗鲁棒性提供了新方法。

Abstract: There is an increasing interest in analyzing the behavior of machine learning
systems against adversarial attacks. However, most of the research in
adversarial machine learning has focused on studying weaknesses against evasion
or poisoning attacks to predictive models in classical setups, with the
susceptibility of Bayesian predictive models to attacks remaining
underexplored. This paper introduces a general methodology for designing
optimal evasion attacks against such models. We investigate two adversarial
objectives: perturbing specific point predictions and altering the entire
posterior predictive distribution. For both scenarios, we propose novel
gradient-based attacks and study their implementation and properties in various
computational setups.

</details>


### [209] [Scaling Laws for Uncertainty in Deep Learning](https://arxiv.org/abs/2506.09648)
*Mattia Rosso,Simone Rossi,Giulio Franzese,Markus Heinonen,Maurizio Filippone*

Key words: 深度学习, 预测不确定性, 扩展规律, 贝叶斯方法

TL;DR: 研究发现深度学习中的预测不确定性遵循与数据集和模型大小相关的扩展规律，挑战了“大数据下贝叶斯方法不必要”的观点。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 探究深度学习中的预测不确定性是否遵循类似的扩展规律，尤其是针对过参数化模型，以反驳对贝叶斯方法的常见质疑。

Method: 通过实证研究，采用近似贝叶斯推理和集成方法，在视觉和语言任务中测量预测不确定性。

Result: 实验证实了预测不确定性与数据集和模型大小之间的扩展规律，表明大数据并不总能消除认知不确定性。

Conclusion: 研究不仅发现了新的扩展规律，还证明了大数据环境下认知不确定性的存在，为贝叶斯方法提供了有力支持。

Abstract: Deep learning has recently revealed the existence of scaling laws,
demonstrating that model performance follows predictable trends based on
dataset and model sizes. Inspired by these findings and fascinating phenomena
emerging in the over-parameterized regime, we examine a parallel direction: do
similar scaling laws govern predictive uncertainties in deep learning? In
identifiable parametric models, such scaling laws can be derived in a
straightforward manner by treating model parameters in a Bayesian way. In this
case, for example, we obtain $O(1/N)$ contraction rates for epistemic
uncertainty with respect to the number of data $N$. However, in
over-parameterized models, these guarantees do not hold, leading to largely
unexplored behaviors. In this work, we empirically show the existence of
scaling laws associated with various measures of predictive uncertainty with
respect to dataset and model sizes. Through experiments on vision and language
tasks, we observe such scaling laws for in- and out-of-distribution predictive
uncertainty estimated through popular approximate Bayesian inference and
ensemble methods. Besides the elegance of scaling laws and the practical
utility of extrapolating uncertainties to larger data or models, this work
provides strong evidence to dispel recurring skepticism against Bayesian
approaches: "In many applications of deep learning we have so much data
available: what do we need Bayes for?". Our findings show that "so much data"
is typically not enough to make epistemic uncertainty negligible.

</details>


### [210] [Assessing the Quality of Denoising Diffusion Models in Wasserstein Distance: Noisy Score and Optimal Bounds](https://arxiv.org/abs/2506.09681)
*Vahan Arsenyan,Elen Vardanyan,Arnak Dalalyan*

Key words: 生成模型,去噪扩散概率模型,稳健性,Wasserstein距离,收敛速率

TL;DR: 该论文研究了去噪扩散概率模型（DDPMs）在噪声评估中的稳健性，并提供了Wasserstein-2距离的有限样本保证，展示了比以往更快的收敛速度。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 旨在证明DDPMs在估计得分函数时对恒定方差噪声的稳健性，并提出更快的收敛速率。

Method: 通过扩散过程映射布朗运动，利用估计的得分函数构建新的随机样本。

Result: DDPMs对噪声得分估计具有稳健性，且在Wasserstein-2距离中实现了更快的收敛速率，结果接近高斯情况的最优速率。

Conclusion: DDPMs在噪声环境下的稳健性和快速收敛速率被验证，且结果接近最优。

Abstract: Generative modeling aims to produce new random examples from an unknown
target distribution, given access to a finite collection of examples. Among the
leading approaches, denoising diffusion probabilistic models (DDPMs) construct
such examples by mapping a Brownian motion via a diffusion process driven by an
estimated score function. In this work, we first provide empirical evidence
that DDPMs are robust to constant-variance noise in the score evaluations. We
then establish finite-sample guarantees in Wasserstein-2 distance that exhibit
two key features: (i) they characterize and quantify the robustness of DDPMs to
noisy score estimates, and (ii) they achieve faster convergence rates than
previously known results. Furthermore, we observe that the obtained rates match
those known in the Gaussian case, implying their optimality.

</details>


### [211] [A Deep Generative Model for the Simulation of Discrete Karst Networks](https://arxiv.org/abs/2506.09832)
*Dany Lauzon,Julien Straubhaar,Philippe Renard*

Key words: 喀斯特网络, 图生成模型, 深度学习, 概率模型

TL;DR: 论文提出了一种基于图生成模型的新方法，通过深度学习和概率模型模拟复杂的喀斯特网络。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 喀斯特网络的模拟由于地质和水文条件的复杂性而具有挑战性，研究旨在通过图生成模型捕捉其复杂特征。

Method: 结合GraphRNN和G-DDPM模型，分步生成喀斯特网络的拓扑结构和节点特征，确保生成的结果符合实际数据特征。

Result: 通过实际喀斯特网络测试，生成子图与实际子图在几何和拓扑指标上表现一致。

Conclusion: 该方法能有效模拟不同类型的喀斯特网络，为物理过程（如流动和传输）研究提供了工具。

Abstract: The simulation of discrete karst networks presents a significant challenge
due to the complexity of the physicochemical processes occurring within various
geological and hydrogeological contexts over extended periods. This complex
interplay leads to a wide variety of karst network patterns, each intricately
linked to specific hydrogeological conditions. We explore a novel approach that
represents karst networks as graphs and applies graph generative models (deep
learning techniques) to capture the intricate nature of karst environments. In
this representation, nodes retain spatial information and properties, while
edges signify connections between nodes. Our generative process consists of two
main steps. First, we utilize graph recurrent neural networks (GraphRNN) to
learn the topological distribution of karst networks. GraphRNN decomposes the
graph simulation into a sequential generation of nodes and edges, informed by
previously generated structures. Second, we employ denoising diffusion
probabilistic models on graphs (G-DDPM) to learn node features (spatial
coordinates and other properties). G-DDPMs enable the generation of nodes
features on the graphs produced by the GraphRNN that adhere to the learned
statistical properties by sampling from the derived probability distribution,
ensuring that the generated graphs are realistic and capture the essential
features of the original data. We test our approach using real-world karst
networks and compare generated subgraphs with actual subgraphs from the
database, by using geometry and topology metrics. Our methodology allows
stochastic simulation of discrete karst networks across various types of
formations, a useful tool for studying the behavior of physical processes such
as flow and transport.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [212] [A Topological Improvement of the Overall Performance of Sparse Evolutionary Training: Motif-Based Structural Optimization of Sparse MLPs Project](https://arxiv.org/abs/2506.09204)
*Xiaotian Chen,Hongyun Liu,Seyed Sahand Mohammadi Ziabari*

Key words: 稀疏性,多层感知器,稀疏进化训练,结构优化

TL;DR: 研究探讨利用结构优化（motif-based optimization）提升稀疏进化训练（SET-MLP）的性能和效率。

<details>
  <summary>Details</summary>

Main category: cs.NE

Motivation: 随着DNN模型复杂度增加，降低计算成本和内存开销的需求日益迫切，稀疏性成为主要解决方案之一。

Method: 应用基于motif的结构优化方法改进稀疏进化训练（SET-MLP）。

Result: 预期效率提升超过40%，性能下降低于4%。

Conclusion: 结构优化可显著提升SET-MLP的性能和效率。

Abstract: Deep Neural Networks (DNNs) have been proven to be exceptionally effective
and have been applied across diverse domains within deep learning. However, as
DNN models increase in complexity, the demand for reduced computational costs
and memory overheads has become increasingly urgent. Sparsity has emerged as a
leading approach in this area. The robustness of sparse Multi-layer Perceptrons
(MLPs) for supervised feature selection, along with the application of Sparse
Evolutionary Training (SET), illustrates the feasibility of reducing
computational costs without compromising accuracy. Moreover, it is believed
that the SET algorithm can still be improved through a structural optimization
method called motif-based optimization, with potential efficiency gains
exceeding 40% and a performance decline of under 4%. This research investigates
whether the structural optimization of Sparse Evolutionary Training applied to
Multi-layer Perceptrons (SET-MLP) can enhance performance and to what extent
this improvement can be achieved.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [213] [A Probabilistic Framework for Imputing Genetic Distances in Spatiotemporal Pathogen Models](https://arxiv.org/abs/2506.09076)
*Haley Stone,Jing Du,Hao Xue,Matthew Scotch,David Heslop,Andreas Züfle,Chandini Raina MacIntyre,Flora Salim*

Key words: 病原体基因组、遗传距离、时间感知模型、进化距离、禽流感

TL;DR: 提出了一种基于时间感知进化距离模型的概率框架，用于推断未测序病例与已知序列间的遗传距离，从而增强病原体基因组数据在空间模型中的应用。

<details>
  <summary>Details</summary>

Main category: q-bio.GN

Motivation: 由于测序覆盖率不足，病原体基因组数据的应用受限，需要一种方法来填补未测序病例与已知序列间的遗传距离信息。

Method: 基于时间感知的进化距离建模，通过采样日期和观察到的遗传距离推断成对分化情况，无需序列比对或已知传播链。

Result: 应用于美国野生鸟类中的高致病性禽流感A/H5病例，该方法能够扩展性地增强基因组数据集，并提升进化信息在时空建模中的应用。

Conclusion: 该方法为不完全测序数据提供了生物学合理的填补手段，增强了基因组数据在时空建模中的实用性。

Abstract: Pathogen genome data offers valuable structure for spatial models, but its
utility is limited by incomplete sequencing coverage. We propose a
probabilistic framework for inferring genetic distances between unsequenced
cases and known sequences within defined transmission chains, using time-aware
evolutionary distance modeling. The method estimates pairwise divergence from
collection dates and observed genetic distances, enabling biologically
plausible imputation grounded in observed divergence patterns, without
requiring sequence alignment or known transmission chains. Applied to highly
pathogenic avian influenza A/H5 cases in wild birds in the United States, this
approach supports scalable, uncertainty-aware augmentation of genomic datasets
and enhances the integration of evolutionary information into spatiotemporal
modeling workflows.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [214] [Empirical and computer-aided robustness analysis of long-step and accelerated methods in smooth convex optimization](https://arxiv.org/abs/2506.09730)
*Pierre Vernimmen,François Glineur*

Key words: 一阶优化方法, 梯度不精确性, 鲁棒性, 缩短因子, 大规模计算

TL;DR: 本文通过理论和实证评估了一阶优化方法在梯度计算相对不精确情况下的鲁棒性，提出了一种半启发式的缩短因子来改进方法性能，并展示了其在实践中的有效性。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: 研究梯度计算相对不精确（如数据压缩导致）对优化方法性能的影响，以解决大规模GPU计算中的实际问题。

Method: 分析了梯度下降、长步方法和加速方法三类优化方法，并提出半启发式缩短因子以提高鲁棒性。

Result: 实验显示加速方法的鲁棒性超出预期，缩短因子显著提升了长步方法的性能，所有缩短方法在不精确环境下均表现良好。

Conclusion: 提出的缩短因子使优化方法在处理相对不精确梯度时仍具高效性，为大规模计算问题提供了实用解决方案。

Abstract: This work assesses both empirically and theoretically, using the performance
estimation methodology, how robust different first-order optimization methods
are when subject to relative inexactness in their gradient computations.
Relative inexactness occurs, for example, when compressing the gradient using
fewer bits of information, which happens when dealing with large-scale problems
on GPUs. Three major families of methods are analyzed: constant step gradient
descent, long-step methods, and accelerated methods. The latter two are first
shown to be theoretically not robust to inexactness. Then, a semi-heuristic
shortening factor is introduced to improve their theoretical guarantees. All
methods are subsequently tested on a concrete inexact problem, with two
different types of relative inexactness, and it is observed that both
accelerated methods are much more robust than expected, and that the shortening
factor significantly helps the long-step methods. In the end, all shortened
methods appear to be promising, even in this inexact setting.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [215] [Advancing Exchange Rate Forecasting: Leveraging Machine Learning and AI for Enhanced Accuracy in Global Financial Markets](https://arxiv.org/abs/2506.09851)
*Md. Yeasin Rahat,Rajan Das Gupta,Nur Raisa Rahman,Sudipto Roy Pritom,Samiur Rahman Shakir,Md Imrul Hasan Showmick,Md. Jakir Hossen*

Key words: 外汇预测, LSTM, 梯度提升分类器, RMSE, 金融风险

TL;DR: 研究利用LSTM和GBC模型预测美元与孟加拉塔卡的汇率，LSTM准确率高达99.449%，显著优于传统ARIMA模型，GBC在交易中盈利率为40.82%，但净亏损。

<details>
  <summary>Details</summary>

Main category: q-fin.ST

Motivation: 外汇汇率预测对全球金融市场至关重要，本研究旨在通过机器学习提升预测准确性。

Method: 使用LSTM和GBC模型分析2018-2023年的历史汇率数据。

Result: LSTM表现优异，RMSE为0.9858；GBC交易盈利率40.82%，但净亏损。

Conclusion: 深度学习在外汇预测中潜力巨大，未来可结合情感分析和实时经济指标。

Abstract: The prediction of foreign exchange rates, such as the US Dollar (USD) to
Bangladeshi Taka (BDT), plays a pivotal role in global financial markets,
influencing trade, investments, and economic stability. This study leverages
historical USD/BDT exchange rate data from 2018 to 2023, sourced from Yahoo
Finance, to develop advanced machine learning models for accurate forecasting.
A Long Short-Term Memory (LSTM) neural network is employed, achieving an
exceptional accuracy of 99.449%, a Root Mean Square Error (RMSE) of 0.9858, and
a test loss of 0.8523, significantly outperforming traditional methods like
ARIMA (RMSE 1.342). Additionally, a Gradient Boosting Classifier (GBC) is
applied for directional prediction, with backtesting on a $10,000 initial
capital revealing a 40.82% profitable trade rate, though resulting in a net
loss of $20,653.25 over 49 trades. The study analyzes historical trends,
showing a decline in BDT/USD rates from 0.012 to 0.009, and incorporates
normalized daily returns to capture volatility. These findings highlight the
potential of deep learning in forex forecasting, offering traders and
policymakers robust tools to mitigate risks. Future work could integrate
sentiment analysis and real-time economic indicators to further enhance model
adaptability in volatile markets.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [216] [Surrogate models to optimize plasma assisted atomic layer deposition in high aspect ratio features](https://arxiv.org/abs/2506.09313)
*Angel Yanguas-Gil,Jeffrey W. Elam*

Key words: PEALD, 替代模型, 机器学习, 高纵横比, 表面复合

TL;DR: 本文探索了使用替代模型优化高纵横比特征中等离子体增强原子层沉积（PEALD）的方法，通过机器学习预测饱和时间，结果表明仅需少量实验即可实现高精度预测。

<details>
  <summary>Details</summary>

Main category: cond-mat.mtrl-sci

Motivation: 在PEALD等等离子体基过程中，表面复合可能主导等离子体物种与表面的反应，导致纳米结构内部实现完全共形需要极长的暴露时间。

Method: 基于PEALD模拟生成合成数据集，训练人工神经网络预测部分涂覆条件下的饱和时间。

Result: 仅需两次未饱和实验即可在10%误差内预测饱和时间；表面复合主导判断的准确率达99%。

Conclusion: 机器学习为加速PEALD优化提供了新途径，并可扩展到原子层蚀刻和更复杂结构。

Abstract: In this work we explore surrogate models to optimize plasma enhanced atomic
layer deposition (PEALD) in high aspect ratio features. In plasma-based
processes such as PEALD and atomic layer etching, surface recombination can
dominate the reactivity of plasma species with the surface, which can lead to
unfeasibly long exposure times to achieve full conformality inside
nanostructures like high aspect ratio vias. Using a synthetic dataset based on
simulations of PEALD, we train artificial neural networks to predict saturation
times based on cross section thickness data obtained for partially coated
conditions. The results obtained show that just two experiments in
undersaturated conditions contain enough information to predict saturation
times within 10% of the ground truth. A surrogate model trained to determine
whether surface recombination dominates the plasma-surface interactions in a
PEALD process achieves 99% accuracy. This demonstrates that machine learning
can provide a new pathway to accelerate the optimization of PEALD processes in
areas such as microelectronics. Our approach can be easily extended to atomic
layer etching and more complex structures.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [217] [EdgeProfiler: A Fast Profiling Framework for Lightweight LLMs on Edge Using Analytical Model](https://arxiv.org/abs/2506.09061)
*Alyssa Pinnock,Shakya Jayakody,Kawsher A Roxy,Md Rubel Ahmed*

Key words: EdgeProfiler, LLM, 边缘计算, 量化, 能耗优化

TL;DR: EdgeProfiler是一个快速分析框架，用于评估边缘系统上的轻量级大语言模型（LLM），通过量化技术和内存约束优化性能。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 大语言模型在边缘设备上的高计算、内存和功耗需求限制了其应用，EdgeProfiler旨在解决这一问题。

Method: 使用4位量化技术分析紧凑型LLM，通过模型估计延迟、FLOPs和能耗。

Result: 量化减少了60-70%内存占用，准确率仅下降2-5%，推理速度提升2-3倍，能耗降低35-50%。

Conclusion: EdgeProfiler展示了轻量级LLM在边缘设备上的可行性和优化潜力。

Abstract: This paper introduces EdgeProfiler, a fast profiling framework designed for
evaluating lightweight Large Language Models (LLMs) on edge systems. While LLMs
offer remarkable capabilities in natural language understanding and generation,
their high computational, memory, and power requirements often confine them to
cloud environments. EdgeProfiler addresses these challenges by providing a
systematic methodology for assessing LLM performance in resource-constrained
edge settings. The framework profiles compact LLMs, including TinyLLaMA,
Gemma3.1B, Llama3.2-1B, and DeepSeek-r1-1.5B, using aggressive quantization
techniques and strict memory constraints. Analytical modeling is used to
estimate latency, FLOPs, and energy consumption. The profiling reveals that
4-bit quantization reduces model memory usage by approximately 60-70%, while
maintaining accuracy within 2-5% of full-precision baselines. Inference speeds
are observed to improve by 2-3x compared to FP16 baselines across various edge
devices. Power modeling estimates a 35-50% reduction in energy consumption for
INT4 configurations, enabling practical deployment on hardware such as
Raspberry Pi 4/5 and Jetson Orin Nano Super. Our findings emphasize the
importance of efficient profiling tailored to lightweight LLMs in edge
environments, balancing accuracy, energy efficiency, and computational
feasibility.

</details>


### [218] [SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving](https://arxiv.org/abs/2506.09397)
*Xiangchen Li,Dimitrios Spatharakis,Saeid Ghafouri,Jiakun Fan,Dimitrios Nikolopoulos*

Key words: 边缘计算、推测解码、异构设备、大规模语言模型、SLED

TL;DR: 论文提出了一种名为SLED的新方法，通过异构设备协同计算，利用推测解码在边缘设备上高效推理大规模语言模型，显著降低延迟和能耗，同时保持模型精度。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 由于边缘设备的存储和功耗限制，高效推理大规模语言模型仍具挑战性，现有方法（如量化或远程推理）往往以牺牲精度或增加成本为代价。

Method: 提出SLED方法，轻量级边缘设备使用多样化草稿模型生成本地候选令牌，同时共享边缘服务器通过更精确的目标模型进行批量验证，减少服务器内存占用。

Result: 实验结果表明，SLED显著降低延迟、提升能效，并支持更多并发推理会话，同时保持模型精度。

Conclusion: SLED是一种适合边缘计算的解决方案，兼顾效率和精度，支持设备异构性。

Abstract: Regardless the advancements in device capabilities, efficient inferencing
advanced large language models (LLMs) at the edge remains challenging due to
limited device memory and power constraints. Existing strategies, such as
aggressive quantization, pruning, or remote inference, trade accuracy for
efficiency or lead to substantial cost burdens. This position paper introduces
a new approach that leverages speculative decoding, previously viewed primarily
as a decoding acceleration technique for autoregressive generation of LLMs, as
a promising approach specifically adapted for edge computing by orchestrating
computation across heterogeneous devices. We propose SLED, a method that allows
lightweight edge devices to draft multiple candidate tokens locally using
diverse draft models, while a single, shared edge server efficiently batches
and verifies the tokens utilizing a more precise target model. This approach
supports device heterogeneity and reduces server-side memory footprint by
avoiding the need to deploy multiple target models. Our initial experiments
with Jetson Orin Nano, Raspberry Pi 5, and an RTX 6000 edge server indicate
substantial benefits: significantly reduced latency, improved energy
efficiency, and increased concurrent inference sessions, all without
sacrificing model accuracy.

</details>


### [219] [TTrace: Lightweight Error Checking and Diagnosis for Distributed Training](https://arxiv.org/abs/2506.09280)
*Haitian Jiang,Shaowei Zhu,Zhen Zhang,Zhenyu Song,Xinwei Fu,Zhen Jia,Yida Wang,Jinyang Li*

Key words: 分布式训练, 无声错误, 调试系统, TTrace, 浮点误差

TL;DR: TTrace是一种用于检测和定位分布式训练中无声错误的系统，通过精细收集中间张量并与单设备参考实现对比，有效区分错误与浮点舍入误差。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 分布式训练在大型神经网络训练中至关重要，但容易产生无声错误，导致训练不正确，现有调试方法效率低下。

Method: TTrace收集分布式训练的中间张量并与单设备参考实现对比，提出数学分析设置阈值以区分错误和浮点舍入误差。

Result: 实验显示TTrace在Megatron-LM框架中有效检测到11个已知错误和3个新错误，且代码改动少于10行。

Conclusion: TTrace在低精度训练等多种场景中有效检测和定位无声错误，解决了分布式训练的调试难题。

Abstract: Distributed training is essential for scaling the training of large neural
network models, such as large language models (LLMs), across thousands of GPUs.
However, the complexity of distributed training programs makes them
particularly prone to silent bugs, which do not produce explicit error signal
but lead to incorrect training outcome. Effectively detecting and localizing
such silent bugs in distributed training is challenging. Common debugging
practice using metrics like training loss or gradient norm curves can be
inefficient and ineffective. Additionally, obtaining intermediate tensor values
and determining whether they are correct during silent bug localization is
difficult, particularly in the context of low-precision training.
  To address those challenges, we design and implement TTrace, the first system
capable of detecting and localizing silent bugs in distributed training. TTrace
collects intermediate tensors from distributing training in a fine-grained
manner and compares them against those from a trusted single-device reference
implementation. To properly compare the floating-point values in the tensors,
we propose novel mathematical analysis that provides a guideline for setting
thresholds, enabling TTrace to distinguish bug-induced errors from
floating-point round-off errors. Experimental results demonstrate that TTrace
effectively detects 11 existing bugs and 3 new bugs in the widely used
Megatron-LM framework, while requiring fewer than 10 lines of code change.
TTrace is effective in various training recipes, including low-precision
recipes involving BF16 and FP8.

</details>


### [220] [ScalableHD: Scalable and High-Throughput Hyperdimensional Computing Inference on Multi-Core CPUs](https://arxiv.org/abs/2506.09282)
*Dhruv Parikh,Viktor Prasanna*

Key words: 超维计算, 多核CPU, 流水线并行化, 高吞吐量

TL;DR: ScalableHD是一种在多核CPU上实现高吞吐量超维计算推理的方法，通过两阶段流水线并行化设计，显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 传统HDC方法在推理时效率较低，尤其是在多核CPU上的研究不足，ScalableHD旨在填补这一空白。

Method: 采用两阶段流水线并行化设计，结合内存平铺和NUMA感知的核绑定，针对不同批量大小优化执行变体。

Result: 在多种任务中实现了最高10倍的吞吐量提升，并保持了任务准确性，且扩展性良好。

Conclusion: ScalableHD为多核CPU提供了高效的HDC推理解决方案，性能卓越且易于扩展。

Abstract: Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that
represents and manipulates information using high-dimensional vectors, called
hypervectors (HV). Traditional HDC methods, while robust to noise and
inherently parallel, rely on single-pass, non-parametric training and often
suffer from low accuracy. To address this, recent approaches adopt iterative
training of base and class HVs, typically accelerated on GPUs. Inference,
however, remains lightweight and well-suited for real-time execution. Yet,
efficient HDC inference has been studied almost exclusively on specialized
hardware such as FPGAs and GPUs, with limited attention to general-purpose
multi-core CPUs. To address this gap, we propose ScalableHD for scalable and
high-throughput HDC inference on multi-core CPUs. ScalableHD employs a
two-stage pipelined execution model, where each stage is parallelized across
cores and processes chunks of base and class HVs. Intermediate results are
streamed between stages using a producer-consumer mechanism, enabling
on-the-fly consumption and improving cache locality. To maximize performance,
ScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.
Further, it features two execution variants tailored for small and large batch
sizes, each designed to exploit compute parallelism based on workload
characteristics while mitigating the memory-bound compute pattern that limits
HDC inference performance on modern multi-core CPUs. ScalableHD achieves up to
10x speedup in throughput (samples per second) over state-of-the-art baselines
such as TorchHD, across a diverse set of tasks ranging from human activity
recognition to image classification, while preserving task accuracy.
Furthermore, ScalableHD exhibits robust scalability: increasing the number of
cores yields near-proportional throughput improvements.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [221] [Intelligent System of Emergent Knowledge: A Coordination Fabric for Billions of Minds](https://arxiv.org/abs/2506.09335)
*Moshi Wei,Sparks Li*

Key words: 去中心化, 多智能体系统, Web3, 区块链, 激励机制, 自适应, 认知生态系统

TL;DR: ISEK是一个基于Web3的去中心化认知生态系统，通过多智能体协作和区块链技术实现自我组织和自适应，结合经济激励和声誉系统推动分布式智能发展。

<details>
  <summary>Details</summary>

Main category: cs.MA

Motivation: ISEK旨在通过结合AI与人类智能，打破中心化平台的限制，实现去中心化的协作和智能涌现。

Method: ISEK采用六阶段工作流（发布、发现、招募、执行、结算、反馈）和分布式共识机制，结合$ISEK代币和NFT身份管理，实现动态任务分配与经济激励。

Result: ISEK构建了一个支持大规模去中心化认知协作的基础设施，使自主智能体能够超越中心化约束共同进化。

Conclusion: ISEK通过区块链、AI和激励工程的结合，为分布式智能的涌现提供了一种创新框架。

Abstract: The Intelligent System of Emergent Knowledge (ISEK) establishes a
decentralized network where human and artificial intelligence agents
collaborate as peers, forming a self-organizing cognitive ecosystem. Built on
Web3 infrastructure, ISEK combines three fundamental principles: (1) a
decentralized multi-agent architecture resistant to censorship, (2) symbiotic
AI-human collaboration with equal participation rights, and (3) resilient
self-adaptation through distributed consensus mechanisms.
  The system implements an innovative coordination protocol featuring a
six-phase workflow (Publish, Discover, Recruit, Execute, Settle, Feedback) for
dynamic task allocation, supported by robust fault tolerance and a
multidimensional reputation system. Economic incentives are governed by the
native $ISEK token, facilitating micropayments, governance participation, and
reputation tracking, while agent sovereignty is maintained through NFT-based
identity management.
  This synthesis of blockchain technology, artificial intelligence, and
incentive engineering creates an infrastructure that actively facilitates
emergent intelligence. ISEK represents a paradigm shift from conventional
platforms, enabling the organic development of large-scale, decentralized
cognitive systems where autonomous agents collectively evolve beyond
centralized constraints.

</details>


### [222] [Effective Red-Teaming of Policy-Adherent Agents](https://arxiv.org/abs/2506.09600)
*Itay Nakash,George Kour,Koren Lazar,Matan Vetzler,Guy Uziel,Ateret Anaby-Tavor*

Key words: LLM代理, 对抗性攻击, 政策遵循, CRAFT, tau-break

TL;DR: 研究提出CRAFT系统，用于测试和提升任务导向型LLM代理在对抗性用户攻击下的稳健性。

<details>
  <summary>Details</summary>

Main category: cs.MA

Motivation: 确保任务导向型LLM代理严格遵循政策，同时保持交互的自然性，尤其面对恶意用户的挑战。

Method: 提出CRAFT多代理红队系统及tau-break基准，测试代理在客服场景中的抗攻击能力。

Result: CRAFT在对抗性攻击中优于传统方法（如DAN提示），但现有防御措施仍有不足。

Conclusion: 需进一步研究更强健的防护措施，以增强政策遵循型代理的抗攻击能力。

Abstract: Task-oriented LLM-based agents are increasingly used in domains with strict
policies, such as refund eligibility or cancellation rules. The challenge lies
in ensuring that the agent consistently adheres to these rules and policies,
appropriately refusing any request that would violate them, while still
maintaining a helpful and natural interaction. This calls for the development
of tailored design and evaluation methodologies to ensure agent resilience
against malicious user behavior. We propose a novel threat model that focuses
on adversarial users aiming to exploit policy-adherent agents for personal
benefit. To address this, we present CRAFT, a multi-agent red-teaming system
that leverages policy-aware persuasive strategies to undermine a
policy-adherent agent in a customer-service scenario, outperforming
conventional jailbreak methods such as DAN prompts, emotional manipulation, and
coercive. Building upon the existing tau-bench benchmark, we introduce
tau-break, a complementary benchmark designed to rigorously assess the agent's
robustness against manipulative user behavior. Finally, we evaluate several
straightforward yet effective defense strategies. While these measures provide
some protection, they fall short, highlighting the need for stronger,
research-driven safeguards to protect policy-adherent agents from adversarial
attacks

</details>


### [223] [When Is Diversity Rewarded in Cooperative Multi-Agent Learning?](https://arxiv.org/abs/2506.09434)
*Michael Amir,Matteo Bettini,Amanda Prorok*

Key words: 异质性团队, 奖励设计, 任务分配, 多智能体强化学习, HED算法

TL;DR: 研究团队通过奖励设计分析异质性团队的优势，提出HED算法验证理论预测，帮助理解行为多样性的益处。

<details>
  <summary>Details</summary>

Main category: cs.MA

Motivation: 探讨异质性团队在任务分配中超越同质性团队的条件，从奖励设计的角度寻找适用目标。

Method: 结合广义聚合算子和多智能体强化学习（MARL），提出HED算法优化环境参数以促进异质性。

Result: 理论和实验验证了异质性团队在特定奖励设计下的优势，HED算法成功应用于不同场景。

Conclusion: 研究揭示了行为多样性在特定奖励机制下的优势，为团队设计提供了理论支持。

Abstract: The success of teams in robotics, nature, and society often depends on the
division of labor among diverse specialists; however, a principled explanation
for when such diversity surpasses a homogeneous team is still missing. Focusing
on multi-agent task allocation problems, our goal is to study this question
from the perspective of reward design: what kinds of objectives are best suited
for heterogeneous teams? We first consider an instantaneous, non-spatial
setting where the global reward is built by two generalized aggregation
operators: an inner operator that maps the $N$ agents' effort allocations on
individual tasks to a task score, and an outer operator that merges the $M$
task scores into the global team reward. We prove that the curvature of these
operators determines whether heterogeneity can increase reward, and that for
broad reward families this collapses to a simple convexity test. Next, we ask
what incentivizes heterogeneity to emerge when embodied, time-extended agents
must learn an effort allocation policy. To study heterogeneity in such
settings, we use multi-agent reinforcement learning (MARL) as our computational
paradigm, and introduce Heterogeneous Environment Design (HED), a
gradient-based algorithm that optimizes the parameter space of underspecified
MARL environments to find scenarios where heterogeneity is advantageous.
Experiments in matrix games and an embodied Multi-Goal-Capture environment show
that, despite the difference in settings, HED rediscovers the reward regimes
predicted by our theory to maximize the advantage of heterogeneity, both
validating HED and connecting our theoretical insights to reward design in
MARL. Together, these results help us understand when behavioral diversity
delivers a measurable benefit.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [224] [Large Language Models for Design Structure Matrix Optimization](https://arxiv.org/abs/2506.09749)
*Shuo Jiang,Min Xie,Jianxi Luo*

Key words: 大型语言模型, 设计结构矩阵, 组合优化, 网络拓扑, 领域知识

TL;DR: 该论文探讨了利用大型语言模型（LLM）解决复杂工程系统中设计结构矩阵（DSM）的组合优化问题，提出了一种基于LLM的新框架，结合网络拓扑和领域知识，实验表明该方法优于传统优化方法。

<details>
  <summary>Details</summary>

Main category: cs.CE

Motivation: 传统优化方法在处理复杂依赖网络时难以捕捉上下文细节，因此研究者探索利用LLM的高级推理和上下文理解能力来解决这些问题。

Method: 提出了一种基于LLM的框架，整合网络拓扑和领域知识，迭代优化DSM元素排序。

Result: 实验结果显示，该方法在收敛速度和解决方案质量上均优于随机和确定性基准方法，且领域知识的加入显著提升了优化性能。

Conclusion: 该研究展示了LLM通过结合语义和数学推理解决复杂工程优化问题的潜力，为基于LLM的工程设计优化开辟了新范式。

Abstract: In complex engineering systems, the interdependencies among components or
development activities are often modeled and analyzed using Design Structure
Matrix (DSM). Reorganizing elements within a DSM to minimize feedback loops and
enhance modularity or process efficiency constitutes a challenging
combinatorial optimization (CO) problem in engineering design and operations.
As problem sizes increase and dependency networks become more intricate,
traditional optimization methods that solely use mathematical heuristics often
fail to capture the contextual nuances and struggle to deliver effective
solutions. In this study, we explore the potential of Large Language Models
(LLMs) for helping solve such CO problems by leveraging their capabilities for
advanced reasoning and contextual understanding. We propose a novel LLM-based
framework that integrates network topology with contextual domain knowledge for
iterative optimization of DSM element sequencing - a common CO problem.
Experiments on various DSM cases show that our method consistently achieves
faster convergence and superior solution quality compared to both stochastic
and deterministic baselines. Notably, we find that incorporating contextual
domain knowledge significantly enhances optimization performance regardless of
the chosen LLM backbone. These findings highlight the potential of LLMs to
solve complex engineering CO problems by combining semantic and mathematical
reasoning. This approach paves the way towards a new paradigm in LLM-based
engineering design optimization.

</details>


### [225] [Intelligent Design 4.0: Paradigm Evolution Toward the Agentic AI Era](https://arxiv.org/abs/2506.09755)
*Shuo Jiang,Min Xie,Frank Youhua Chen,Jian Ma,Jianxi Luo*

Key words: 智能设计, 基础模型, 大型语言模型, 代理AI系统, 端到端自动化

TL;DR: 本文介绍了由代理AI系统驱动的智能设计4.0（ID 4.0）这一新兴范式，回顾了智能设计的历史演变，并提出了支持端到端工程设计自动化的概念框架，展望了未来发展方向。

<details>
  <summary>Details</summary>

Main category: cs.CE

Motivation: 研究动机在于探讨如何利用基础模型（FMs）特别是大型语言模型（LLMs）的通用知识推理能力，推动工程设计领域的进一步变革。

Method: 作者回顾了智能设计的四个历史阶段（基于规则的专家系统、任务特定的机器学习模型、大规模基础AI模型、多代理协作），并提出了ID 4.0的概念框架，探讨如何通过自主多代理系统实现端到端自动化。

Result: 提出了智能设计4.0的潜在能力，包括支持工程设计流程的全面自动化，并总结了未来的研究方向。

Conclusion: 研究表明，ID 4.0有望通过提升适应性、自主性和有效性，解决日益复杂的设计挑战。

Abstract: Research and practice in Intelligent Design (ID) have significantly enhanced
engineering innovation, efficiency, quality, and productivity over recent
decades, fundamentally reshaping how engineering designers think, behave, and
interact with design processes. The recent emergence of Foundation Models
(FMs), particularly Large Language Models (LLMs), has demonstrated general
knowledge-based reasoning capabilities, and open new paths and avenues for
further transformation in engineering design. In this context, this paper
introduces Intelligent Design 4.0 (ID 4.0) as an emerging paradigm empowered by
agentic AI systems. We review the historical evolution of ID across four
distinct stages: rule-based expert systems, task-specific machine learning
models, large-scale foundation AI models, and the recent emerging paradigm of
multi-agent collaboration. We propose a conceptual framework for ID 4.0 and
discuss its potential to support end-to-end automation of engineering design
processes through coordinated, autonomous multi-agent-based systems.
Furthermore, we discuss future perspectives to enhance and fully realize ID
4.0's potential, including more complex design scenarios, more practical design
implementations, novel agent coordination mechanisms, and autonomous design
goal-setting with better human value alignment. In sum, these insights lay a
foundation for advancing Intelligent Design toward greater adaptivity,
autonomy, and effectiveness in addressing increasingly complex design
challenges.

</details>


### [226] [Superstudent intelligence in thermodynamics](https://arxiv.org/abs/2506.09822)
*Rebecca Loubet,Pascal Zittlau,Marco Hoffmann,Luisa Vollmer,Sophie Fellenz,Heike Leitte,Fabian Jirasek,Johannes Lenhard,Hans Hasse*

Key words: OpenAI, 热力学考试, 零样本模式, 人工智能, 工程教育

TL;DR: OpenAI的o3模型在热力学考试中表现优于所有学生，展示了机器在复杂任务中的卓越能力。

<details>
  <summary>Details</summary>

Main category: cs.CE

Motivation: 探讨AI模型在传统上被视为人类智力证明的复杂任务中的表现，及其对工程教育和职业的影响。

Method: 将OpenAI的o3模型与学生一起参加热力学考试，对其回答进行相同标准的评分。

Result: o3模型在零样本模式下正确解决了所有问题，成绩超过所有学生，达到了历史最佳水平。

Conclusion: 机器在复杂任务中的卓越表现标志着技术发展的转折点，对工程师工作和教育提出了新挑战。

Abstract: In this short note, we report and analyze a striking event: OpenAI's large
language model o3 has outwitted all students in a university exam on
thermodynamics. The thermodynamics exam is a difficult hurdle for most
students, where they must show that they have mastered the fundamentals of this
important topic. Consequently, the failure rates are very high, A-grades are
rare - and they are considered proof of the students' exceptional intellectual
abilities. This is because pattern learning does not help in the exam. The
problems can only be solved by knowledgeably and creatively combining
principles of thermodynamics. We have given our latest thermodynamics exam not
only to the students but also to OpenAI's most powerful reasoning model, o3,
and have assessed the answers of o3 exactly the same way as those of the
students. In zero-shot mode, the model o3 solved all problems correctly, better
than all students who took the exam; its overall score was in the range of the
best scores we have seen in more than 10,000 similar exams since 1985. This is
a turning point: machines now excel in complex tasks, usually taken as proof of
human intellectual capabilities. We discuss the consequences this has for the
work of engineers and the education of future engineers.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [227] [ThinkQE: Query Expansion via an Evolving Thinking Process](https://arxiv.org/abs/2506.09260)
*Yibin Lei,Tao Shen,Andrew Yates*

Key words: 查询扩展, 网络搜索, LLM, 多样性, 迭代优化

TL;DR: ThinkQE通过思考驱动的扩展过程和基于语料库的反馈迭代优化查询扩展，显著提升网络搜索性能。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 现有基于LLM的查询扩展方法虽然提高了检索性能，但扩展结果过于狭窄，忽略了探索性和多样性需求。

Method: 提出ThinkQE框架，包括思考驱动的扩展过程和基于语料库反馈的迭代优化策略。

Result: 在DL19、DL20和BRIGHT等多样化网络搜索基准测试中，ThinkQE表现优于现有方法。

Conclusion: ThinkQE通过深度语义探索和迭代优化，解决了查询扩展中的局限性。

Abstract: Effective query expansion for web search benefits from promoting both
exploration and result diversity to capture multiple interpretations and facets
of a query. While recent LLM-based methods have improved retrieval performance
and demonstrate strong domain generalization without additional training, they
often generate narrowly focused expansions that overlook these desiderata. We
propose ThinkQE, a test-time query expansion framework addressing this
limitation through two key components: a thinking-based expansion process that
encourages deeper and comprehensive semantic exploration, and a
corpus-interaction strategy that iteratively refines expansions using retrieval
feedback from the corpus. Experiments on diverse web search benchmarks (DL19,
DL20, and BRIGHT) show ThinkQE consistently outperforms prior approaches,
including training-intensive dense retrievers and rerankers.

</details>


### [228] [Revisiting Graph Projections for Effective Complementary Product Recommendation](https://arxiv.org/abs/2506.09209)
*Leandro Anghinoni,Pablo Zivic,Jorge Adrian Sanchez*

Key words: 互补产品推荐,二部图投影,用户-物品交互,推荐系统

TL;DR: 提出了一种基于用户-物品二部图投影的简单有效方法，用于推荐互补产品，相比现有方法性能提升显著。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 互补产品推荐是提升用户体验和零售销售额的重要策略，但由于用户-物品交互数据的噪声和稀疏性，推荐合适的互补产品具有挑战性。

Method: 提出了一种基于用户-物品二部图投影至有向加权图结构的方法，从中推断互补关系。

Result: 在多个基准测试中，该方法比现有序列推荐和图推荐平均提升43%和38%。

Conclusion: 该方法简单有效，显著提升了互补产品推荐的性能。

Abstract: Complementary product recommendation is a powerful strategy to improve
customer experience and retail sales. However, recommending the right product
is not a simple task because of the noisy and sparse nature of user-item
interactions. In this work, we propose a simple yet effective method to predict
a list of complementary products given a query item, based on the structure of
a directed weighted graph projected from the user-item bipartite graph. We
revisit bipartite graph projections for recommender systems and propose a novel
approach for inferring complementarity relationships from historical user-item
interactions. We compare our model with recent methods from the literature and
show, despite the simplicity of our approach, an average improvement of +43%
and +38% over sequential and graph-based recommenders, respectively, over
different benchmarks.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [229] [Abstraction-Based Proof Production in Formal Verification of Neural Networks](https://arxiv.org/abs/2506.09455)
*Yizhak Yisrael Elboher,Omri Isac,Guy Katz,Tobias Ladner,Haoze Wu*

Key words: 深层神经网络, 验证, 抽象技术, 形式化证明, 可扩展性

TL;DR: 提出一种新型框架，结合抽象技术和形式化证明，解决现有DNN验证工具在可扩展性和可证明性之间的差距。

<details>
  <summary>Details</summary>

Main category: cs.LO

Motivation: 现代深层神经网络（DNN）验证工具依赖抽象技术提升规模，但现有可生成证明的验证器不支持抽象推理，导致可扩展性与可证明性之间的冲突。

Method: 将验证任务分为两部分：1）证明抽象网络的正确性；2）证明抽象相对于原始DNN的可靠性。前者利用现有工具，后者提出新方法生成形式化证明。

Result: 框架初步支持在形式化证明中嵌入常见抽象技术，旨在实现可扩展且可信的DNN验证。

Conclusion: 该研究填补了抽象技术与形式化证明之间的空白，为DNN验证提供了兼具可扩展性和可靠性的新路径。

Abstract: Modern verification tools for deep neural networks (DNNs) increasingly rely
on abstraction to scale to realistic architectures. In parallel, proof
production is becoming a critical requirement for increasing the reliability of
DNN verification results. However, current proofproducing verifiers do not
support abstraction-based reasoning, creating a gap between scalability and
provable guarantees. We address this gap by introducing a novel framework for
proof-producing abstraction-based DNN verification. Our approach modularly
separates the verification task into two components: (i) proving the
correctness of an abstract network, and (ii) proving the soundness of the
abstraction with respect to the original DNN. The former can be handled by
existing proof-producing verifiers, whereas we propose the first method for
generating formal proofs for the latter. This preliminary work aims to enable
scalable and trustworthy verification by supporting common abstraction
techniques within a formal proof framework.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [230] [Estimating Visceral Adiposity from Wrist-Worn Accelerometry](https://arxiv.org/abs/2506.09167)
*James R. Williamson,Andrew Alini,Brian A. Telfer,Adam W. Potter,Karl E. Friedl*

Key words: 内脏脂肪组织（VAT）、身体活动（PA）、代谢健康、机器学习、加速度计数据

TL;DR: 通过加速度计数据和机器学习方法估计内脏脂肪组织（VAT），展示了其与身体活动和代谢健康的强相关性。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 内脏脂肪组织（VAT）是代谢健康和身体活动的重要标志，与2型糖尿病和胰岛素抵抗高度相关。研究旨在探索通过身体活动数据直接推断VAT的方法。

Method: 使用NHANES 2011-2014年的加速度计数据，采用两种方法估算VAT：1）基于步态和睡眠特征的山脊回归；2）基于24小时加速度计数据的深度神经网络（包括基础模型和Transformer模型）。结合人口统计学和身体测量协变量提升准确性。

Result: 两种方法结合后，VAT估计的相关系数达到r=0.86，表明身体活动与VAT及代谢健康风险之间存在强关联。

Conclusion: 研究表明，通过身体活动数据能够准确估计VAT，为代谢健康风险评估提供新工具。

Abstract: Visceral adipose tissue (VAT) is a key marker of both metabolic health and
habitual physical activity (PA). Excess VAT is highly correlated with type 2
diabetes and insulin resistance. The mechanistic basis for this pathophysiology
relates to overloading the liver with fatty acids. VAT is also a highly labile
fat depot, with increased turnover stimulated by catecholamines during
exercise. VAT can be measured with sophisticated imaging technologies, but can
also be inferred directly from PA. We tested this relationship using National
Health and Nutrition Examination Survey (NHANES) data from 2011-2014, for
individuals aged 20-60 years with 7 days of accelerometry data (n=2,456 men;
2,427 women) [1]. Two approaches were used for estimating VAT from activity.
The first used engineered features based on movements during gait and sleep,
and then ridge regression to map summary statistics of these features into a
VAT estimate. The second approach used deep neural networks trained on 24 hours
of continuous accelerometry. A foundation model first mapped each 10s frame
into a high-dimensional feature vector. A transformer model then mapped each
day's feature vector time series into a VAT estimate, which were averaged over
multiple days. For both approaches, the most accurate estimates were obtained
with the addition of covariate information about subject demographics and body
measurements. The best performance was obtained by combining the two
approaches, resulting in VAT estimates with correlations of r=0.86. These
findings demonstrate a strong relationship between PA and VAT and, by
extension, between PA and metabolic health risks.

</details>


### [231] [Integration of Contrastive Predictive Coding and Spiking Neural Networks](https://arxiv.org/abs/2506.09194)
*Emirhan Bilgiç,Neslihan Serap Şengör,Namık Berk Yalabık,Yavuz Selim İşler,Aykut Görkem Gelen,Rahmi Elibol*

Key words: 对比预测编码, 脉冲神经网络, 生物合理性, MNIST数据集, 分类任务

TL;DR: 研究了将对比预测编码（CPC）与脉冲神经网络（SNN）结合的方法，旨在通过脉冲系统提升生物合理性，并在MNIST数据集上验证了分类效果。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 结合CPC和SNN，以开发更具生物合理性的预测编码模型，同时探索SNN在分类任务外的编码机制。

Method: 提出了一种基于脉冲系统的预测编码模型，并在MNIST数据集上进行测试。

Result: 模型在区分正序与非序样本时表现出高分类率，证明了CPC与SNN结合的有效性。

Conclusion: 研究表明CPC可与SNN有效结合，拓展了SNN在编码任务中的应用潜力。

Abstract: This study examines the integration of Contrastive Predictive Coding (CPC)
with Spiking Neural Networks (SNN). While CPC learns the predictive structure
of data to generate meaningful representations, SNN mimics the computational
processes of biological neural systems over time. In this study, the goal is to
develop a predictive coding model with greater biological plausibility by
processing inputs and outputs in a spike-based system. The proposed model was
tested on the MNIST dataset and achieved a high classification rate in
distinguishing positive sequential samples from non-sequential negative
samples. The study demonstrates that CPC can be effectively combined with SNN,
showing that an SNN trained for classification tasks can also function as an
encoding mechanism. Project codes and detailed results can be accessed on our
GitHub page: https://github.com/vnd-ogrenme/ongorusel-kodlama/tree/main/CPC_SNN

</details>


### [232] [Graph Attention-based Decentralized Actor-Critic for Dual-Objective Control of Multi-UAV Swarms](https://arxiv.org/abs/2506.09195)
*Haoran Peng,Ying-Jun Angela Zhang*

Key words: 多无人机系统, 图注意力网络, 去中心化Actor-Critic, KL散度, 双目标优化

TL;DR: 论文提出了一种基于图注意力网络的去中心化Actor-Critic方法（GADC），用于优化多无人机系统的服务覆盖范围和电池续航能力，并通过KL散度平衡两者之间的权衡。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 研究旨在解决多无人机系统中的双目标优化问题：最大化服务覆盖范围（主要目标）和延长电池续航能力（次要目标）。

Method: 采用图注意力网络（GAT）处理无人机的有限局部观测并降低环境状态维度，开发了actor-double-critic网络以管理双策略，并使用KL散度平衡目标之间的权衡。

Result: GADC在理想环境和NVIDIA Sionna的射线追踪环境中均表现出卓越性能，优于现有方法。

Conclusion: GADC在多无人机系统中实现了高效的双目标优化，并在理论和实验验证中表现出优越性。

Abstract: This research focuses on optimizing multi-UAV systems with dual objectives:
maximizing service coverage as the primary goal while extending battery
lifetime as the secondary objective. We propose a Graph Attention-based
Decentralized Actor-Critic (GADC) to optimize the dual objectives. The proposed
approach leverages a graph attention network to process UAVs' limited local
observation and reduce the dimension of the environment states. Subsequently,
an actor-double-critic network is developed to manage dual policies for joint
objective optimization. The proposed GADC uses a Kullback-Leibler (KL)
divergence factor to balance the tradeoff between coverage performance and
battery lifetime in the multi-UAV system. We assess the scalability and
efficiency of GADC through comprehensive benchmarking against state-of-the-art
methods, considering both theory and experimental aspects. Extensive testing in
both ideal settings and NVIDIA Sionna's realistic ray tracing environment
demonstrates GADC's superior performance.

</details>


### [233] [AI-Driven SEEG Channel Ranking for Epileptogenic Zone Localization](https://arxiv.org/abs/2506.09255)
*Saeed Hashemi,Genchang Peng,Mehrdad Nourani,Omar Nofal,Jay Harvey*

Key words: SEEG, XGBoost, SHAP, 癫痫灶定位, 通道排序

TL;DR: 本文提出了一种基于机器学习的SEEG信号分析方法，通过XGBoost训练分类模型并利用SHAP评分对癫痫发作期间的通道进行排序，结合临床医生选择和计算发现，提高了癫痫灶定位的效率和准确性。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 传统的SEEG信号视觉检查方法效率低下且耗时，本研究旨在利用机器学习技术自动识别和排序对癫痫发作有影响的通道，并结合临床医生选择的通道扩展搜索空间。

Method: 采用XGBoost训练分类模型，利用SHAP评分对SEEG通道进行排序，并结合通道扩展策略扩大可疑癫痫灶区域。

Result: 对5名患者的SEEG数据进行分析，结果显示该方法在准确性、一致性和可解释性方面表现良好。

Conclusion: 该方法能够辅助临床医生高效定位癫痫灶，并提供更全面的可疑区域。

Abstract: Stereo-electroencephalography (SEEG) is an invasive technique to implant
depth electrodes and collect data for pre-surgery evaluation. Visual inspection
of signals recorded from hundreds of channels is time consuming and
inefficient. We propose a machine learning approach to rank the impactful
channels by incorporating clinician's selection and computational finding. A
classification model using XGBoost is trained to learn the discriminative
features of each channel during ictal periods. Then, the SHapley Additive
exPlanations (SHAP) scoring is utilized to rank SEEG channels based on their
contribution to seizures. A channel extension strategy is also incorporated to
expand the search space and identify suspicious epileptogenic zones beyond
those selected by clinicians. For validation, SEEG data for five patients were
analyzed showing promising results in terms of accuracy, consistency, and
explainability.

</details>


### [234] [Cross-Channel Unlabeled Sensing over a Union of Signal Subspaces](https://arxiv.org/abs/2506.09773)
*Taulant Koka,Manolis C. Tsakiris,Benjamín Béjar Haro,Michael Muma*

Key words: 跨通道无标记传感,子空间联合,信号重建,压缩感知,全脑钙成像

TL;DR: 该研究扩展了跨通道无标记传感框架，使其适用于子空间联合的信号，提高了复杂信号的处理能力，并在压缩感知等任务中表现出色。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 解决全脑钙成像和多重目标跟踪中样本与通道映射不精确的问题。

Method: 通过推导更严格的样本数量界限，支持更广泛的信号类型。

Result: 在实际应用中成功重建信号，验证了框架的有效性。

Conclusion: 扩展后的框架在处理复杂信号和不精确通道映射时表现优异。

Abstract: Cross-channel unlabeled sensing addresses the problem of recovering a
multi-channel signal from measurements that were shuffled across channels. This
work expands the cross-channel unlabeled sensing framework to signals that lie
in a union of subspaces. The extension allows for handling more complex signal
structures and broadens the framework to tasks like compressed sensing. These
mismatches between samples and channels often arise in applications such as
whole-brain calcium imaging of freely moving organisms or multi-target
tracking. We improve over previous models by deriving tighter bounds on the
required number of samples for unique reconstruction, while supporting more
general signal types. The approach is validated through an application in
whole-brain calcium imaging, where organism movements disrupt sample-to-neuron
mappings. This demonstrates the utility of our framework in real-world settings
with imprecise sample-channel associations, achieving accurate signal
reconstruction.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [235] [ReStNet: A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices](https://arxiv.org/abs/2506.09066)
*Maoyu Wang,Yao Lu,Jiaqi Nie,Zeyu Wang,Yun Lin,Qi Xuan,Guan Gui*

Key words: 深度学习, 预训练模型, 物联网, 模型拼接, 资源异构性

TL;DR: ReStNet是一种可重用和可拼接的网络，通过动态拼接预训练模型解决物联网设备资源异构性问题，实现了高效灵活的模型部署。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 由于物联网设备的计算和内存资源异构性，传统的单一模型部署方法效率低下，且传统压缩方法无法动态适应资源变化。

Method: ReStNet通过计算层间相似性选择拼接点，拼接大容量模型的前层和小容量模型的后层，并仅微调拼接层，实现高效部署。

Result: 实验表明，ReStNet能灵活权衡准确性和效率，显著降低训练成本，并支持同质和异质模型拼接。

Conclusion: ReStNet为资源受限环境提供了一种高效的动态模型部署解决方案。

Abstract: With the rapid development of deep learning, a growing number of pre-trained
models have been publicly available. However, deploying these fixed models in
real-world IoT applications is challenging because different devices possess
heterogeneous computational and memory resources, making it impossible to
deploy a single model across all platforms. Although traditional compression
methods, such as pruning, quantization, and knowledge distillation, can improve
efficiency, they become inflexible once applied and cannot adapt to changing
resource constraints. To address these issues, we propose ReStNet, a Reusable
and Stitchable Network that dynamically constructs a hybrid network by
stitching two pre-trained models together. Implementing ReStNet requires
addressing several key challenges, including how to select the optimal
stitching points, determine the stitching order of the two pre-trained models,
and choose an effective fine-tuning strategy. To systematically address these
challenges and adapt to varying resource constraints, ReStNet determines the
stitching point by calculating layer-wise similarity via Centered Kernel
Alignment (CKA). It then constructs the hybrid model by retaining early layers
from a larger-capacity model and appending deeper layers from a smaller one. To
facilitate efficient deployment, only the stitching layer is fine-tuned. This
design enables rapid adaptation to changing budgets while fully leveraging
available resources. Moreover, ReStNet supports both homogeneous (CNN-CNN,
Transformer-Transformer) and heterogeneous (CNN-Transformer) stitching,
allowing to combine different model families flexibly. Extensive experiments on
multiple benchmarks demonstrate that ReStNet achieve flexible
accuracy-efficiency trade-offs at runtime while significantly reducing training
cost.

</details>


### [236] [Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations](https://arxiv.org/abs/2506.09067)
*Zhiyu Xue,Reza Abbasi-Asl,Ramtin Pedarsani*

Key words: 医疗视觉语言模型, 安全漏洞, 防御策略, 合成临床演示, 越狱攻击

TL;DR: 该论文探讨了生成式医疗视觉语言模型（Med-VLMs）在安全漏洞方面的挑战，并提出了一种新的防御策略。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Med-VLMs在生成复杂文本信息（如诊断报告）时，可能面临有害查询（如保险欺诈）的安全风险。同时，过度防御可能导致模型拒绝良性临床查询。

Method: 提出了一种基于合成临床演示的推理时防御策略，通过使用多模态医学图像数据集，增强模型安全性而不显著降低性能。

Result: 实验表明，该方法有效防御视觉和文本越狱攻击，增加演示预算可减轻过度防御问题。

Conclusion: 论文提出了混合演示策略，在预算有限的情况下平衡安全与性能。

Abstract: Generative medical vision-language models~(Med-VLMs) are primarily designed
to generate complex textual information~(e.g., diagnostic reports) from
multimodal inputs including vision modality~(e.g., medical images) and language
modality~(e.g., clinical queries). However, their security vulnerabilities
remain underexplored. Med-VLMs should be capable of rejecting harmful queries,
such as \textit{Provide detailed instructions for using this CT scan for
insurance fraud}. At the same time, addressing security concerns introduces the
risk of over-defense, where safety-enhancing mechanisms may degrade general
performance, causing Med-VLMs to reject benign clinical queries. In this paper,
we propose a novel inference-time defense strategy to mitigate harmful queries,
enabling defense against visual and textual jailbreak attacks. Using diverse
medical imaging datasets collected from nine modalities, we demonstrate that
our defense strategy based on synthetic clinical demonstrations enhances model
safety without significantly compromising performance. Additionally, we find
that increasing the demonstration budget alleviates the over-defense issue. We
then introduce a mixed demonstration strategy as a trade-off solution for
balancing security and performance under few-shot demonstration budget
constraints.

</details>


### [237] [Segment Any Architectural Facades (SAAF):An automatic segmentation model for building facades, walls and windows based on multimodal semantics guidance](https://arxiv.org/abs/2506.09071)
*Peilin Li,Jun Yin,Jing Zhong,Ran Luo,Pengyu Zeng,Miao Zhang*

Key words: 建筑立面分割、多模态学习、自然语言处理、计算机视觉

TL;DR: 该研究提出了一种基于多模态语义指导的建筑立面墙体和窗户自动分割模型（SAAF），通过结合自然语言处理技术，提高了建筑立面组件的语义理解能力，并在多个数据集上展现了优越的分割性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 在建筑数字化的背景下，自动分割墙体和窗户是提高建筑信息模型和计算机辅助设计效率的关键步骤。

Method: SAAF采用多模态语义协同特征提取机制，结合自然语言处理技术，开发了端到端的训练框架，减少人工干预对分割结果的影响。

Result: SAAF在多个立面数据集上的分割结果在mIoU指标上优于现有方法，展现了高精度和泛化能力。

Conclusion: SAAF为建筑计算机视觉技术的发展提供了参考，并为多模态学习在建筑领域的应用探索了新思路。

Abstract: In the context of the digital development of architecture, the automatic
segmentation of walls and windows is a key step in improving the efficiency of
building information models and computer-aided design. This study proposes an
automatic segmentation model for building facade walls and windows based on
multimodal semantic guidance, called Segment Any Architectural Facades (SAAF).
First, SAAF has a multimodal semantic collaborative feature extraction
mechanism. By combining natural language processing technology, it can fuse the
semantic information in text descriptions with image features, enhancing the
semantic understanding of building facade components. Second, we developed an
end-to-end training framework that enables the model to autonomously learn the
mapping relationship from text descriptions to image segmentation, reducing the
influence of manual intervention on the segmentation results and improving the
automation and robustness of the model. Finally, we conducted extensive
experiments on multiple facade datasets. The segmentation results of SAAF
outperformed existing methods in the mIoU metric, indicating that the SAAF
model can maintain high-precision segmentation ability when faced with diverse
datasets. Our model has made certain progress in improving the accuracy and
generalization ability of the wall and window segmentation task. It is expected
to provide a reference for the development of architectural computer vision
technology and also explore new ideas and technical paths for the application
of multimodal learning in the architectural field.

</details>


### [238] [VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks](https://arxiv.org/abs/2506.09079)
*Xinlong Chen,Yuanxing Zhang,Yushuo Guan,Bohan Zeng,Yang Shi,Sihan Yang,Pengfei Wan,Qiang Liu,Liang Wang,Tieniu Tan*

Key words: 视频推理,多模态语言模型,数据集,强化学习

TL;DR: 该论文提出了两个新数据集DarkEventInfer和MixVidQA，用于提升视频理解和推理能力，并开发了首个基于Reason-Then-Respond范式的多任务模型VersaVid-R1。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 视频推理领域缺乏高质量数据和有效训练方法，需填补这一空白。

Method: 使用DarkEventInfer和MixVidQA数据集，结合强化学习，开发多任务模型VersaVid-R1。

Result: VersaVid-R1在多个基准测试中显著优于现有模型。

Conclusion: 该研究为视频推理提供了数据和方法上的新思路，模型表现优异。

Abstract: Recent advancements in multimodal large language models have successfully
extended the Reason-Then-Respond paradigm to image-based reasoning, yet
video-based reasoning remains an underdeveloped frontier, primarily due to the
scarcity of high-quality reasoning-oriented data and effective training
methodologies. To bridge this gap, we introduce DarkEventInfer and MixVidQA,
two novel datasets specifically designed to stimulate the model's advanced
video understanding and reasoning abilities. DarkEventinfer presents videos
with masked event segments, requiring models to infer the obscured content
based on contextual video cues. MixVidQA, on the other hand, presents
interleaved video sequences composed of two distinct clips, challenging models
to isolate and reason about one while disregarding the other. Leveraging these
carefully curated training samples together with reinforcement learning guided
by diverse reward functions, we develop VersaVid-R1, the first versatile video
understanding and reasoning model under the Reason-Then-Respond paradigm
capable of handling multiple-choice and open-ended question answering, as well
as video captioning tasks. Extensive experiments demonstrate that VersaVid-R1
significantly outperforms existing models across a broad spectrum of
benchmarks, covering video general understanding, cognitive reasoning, and
captioning tasks.

</details>


### [239] [FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation](https://arxiv.org/abs/2506.09081)
*Zheqi He,Yesheng Liu,Jing-shu Zheng,Xuejing Li,Richeng Xuan,Jin-Ge Yao,Xi Yang*

Key words: 多模态模型, 评估框架, 视觉-语言任务, 开源工具, 高效评估

TL;DR: FlagEvalMM是一个开源的多模态模型评估框架，旨在通过独立评估服务全面评估视觉-语言任务，并提升评估效率。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 开发一个灵活、高效且可扩展的多模态模型评估工具，以支持模型在不同任务中的性能分析。

Method: 通过解耦模型推理与评估，利用先进推理加速工具和异步数据加载技术优化评估流程。

Result: 实验表明FlagEvalMM能高效准确地揭示模型优劣势，推动多模态研究发展。

Conclusion: FlagEvalMM为多模态研究提供了有价值的评估工具，未来可扩展支持更多任务和模型。

Abstract: We present FlagEvalMM, an open-source evaluation framework designed to
comprehensively assess multimodal models across a diverse range of
vision-language understanding and generation tasks, such as visual question
answering, text-to-image/video generation, and image-text retrieval. We
decouple model inference from evaluation through an independent evaluation
service, thus enabling flexible resource allocation and seamless integration of
new tasks and models. Moreover, FlagEvalMM utilizes advanced inference
acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to
significantly enhance evaluation efficiency. Extensive experiments show that
FlagEvalMM offers accurate and efficient insights into model strengths and
limitations, making it a valuable tool for advancing multimodal research. The
framework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.

</details>


### [240] [AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models](https://arxiv.org/abs/2506.09082)
*Zheda Mai,Arpita Chowdhury,Zihe Wang,Sooyoung Jeon,Lemeng Wang,Jiacheng Hou,Jihyung Kil,Wei-Lun Chao*

Key words: 视觉基础模型、视觉问答、原子视觉能力、评测基准

TL;DR: AVA-Bench是一个新基准测试，旨在通过分解14种原子视觉能力（AVAs）来解决现有视觉问答（VQA）评测中的不足，从而实现对视觉基础模型（VFMs）的精准评估。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前评测方法存在两个主要问题：指令调优数据与VQA测试分布不一致，以及VQA评测任务通常需要多种视觉能力，难以定位模型的短板。AVA-Bench旨在弥补这些不足。

Method: 设计AVA-Bench，通过分解14种原子视觉能力（如定位、深度估计等），并在每种能力内匹配训练与测试数据分布，从而精确评估VFMs的表现。

Result: AVA-Bench揭示了VFMs的独特“能力指纹”，并发现0.5B参数的LLM在VFM排名上表现与7B LLM相似，但GPU耗时减少8倍。

Conclusion: AVA-Bench为下一代VFMs的发展提供了全面且透明的评测基础，使模型选择从经验猜测变为有据可依。

Abstract: The rise of vision foundation models (VFMs) calls for systematic evaluation.
A common approach pairs VFMs with large language models (LLMs) as
general-purpose heads, followed by evaluation on broad Visual Question
Answering (VQA) benchmarks. However, this protocol has two key blind spots: (i)
the instruction tuning data may not align with VQA test distributions, meaning
a wrong prediction can stem from such data mismatch rather than a VFM' visual
shortcomings; (ii) VQA benchmarks often require multiple visual abilities,
making it hard to tell whether errors stem from lacking all required abilities
or just a single critical one. To address these gaps, we introduce AVA-Bench,
the first benchmark that explicitly disentangles 14 Atomic Visual Abilities
(AVAs) -- foundational skills like localization, depth estimation, and spatial
understanding that collectively support complex visual reasoning tasks. By
decoupling AVAs and matching training and test distributions within each,
AVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench
to leading VFMs thus reveals distinctive "ability fingerprints," turning VFM
selection from educated guesswork into principled engineering. Notably, we find
that a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours
by 8x, enabling more efficient evaluation. By offering a comprehensive and
transparent benchmark, we hope AVA-Bench lays the foundation for the next
generation of VFMs.

</details>


### [241] [BakuFlow: A Streamlining Semi-Automatic Label Generation Tool](https://arxiv.org/abs/2506.09083)
*Jerry Lin,Partick P. W. Chen*

Key words: 数据标注,半自动工具,YOLOE,效率提升,计算机视觉

TL;DR: BakuFlow是一款半自动标签生成工具，通过多项创新功能显著提升数据标注效率，适用于计算机视觉和工业场景。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决计算机视觉中大规模数据标注的耗时和易错问题，提供更高效的半自动化标注方案。

Method: 结合可调放大镜、交互式数据增强、标签传播和改进的YOLOE自动标注模块，实现灵活且高效的标注流程。

Result: BakuFlow有效减少标注工作量，提升对象检测和跟踪任务的效率。

Conclusion: BakuFlow为动态、真实世界数据集提供了可扩展的标注工具，优化了用户体验和工作效率。

Abstract: Accurately labeling (or annotation) data is still a bottleneck in computer
vision, especially for large-scale tasks where manual labeling is
time-consuming and error-prone. While tools like LabelImg can handle the
labeling task, some of them still require annotators to manually label each
image. In this paper, we introduce BakuFlow, a streamlining semi-automatic
label generation tool. Key features include (1) a live adjustable magnifier for
pixel-precise manual corrections, improving user experience; (2) an interactive
data augmentation module to diversify training datasets; (3) label propagation
for rapidly copying labeled objects between consecutive frames, greatly
accelerating annotation of video data; and (4) an automatic labeling module
powered by a modified YOLOE framework. Unlike the original YOLOE, our extension
supports adding new object classes and any number of visual prompts per class
during annotation, enabling flexible and scalable labeling for dynamic,
real-world datasets. These innovations make BakuFlow especially effective for
object detection and tracking, substantially reducing labeling workload and
improving efficiency in practical computer vision and industrial scenarios.

</details>


### [242] [CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation](https://arxiv.org/abs/2506.09109)
*Arnav Yayavaram,Siddharth Yayavaram,Simran Khanuja,Michael Saxon,Graham Neubig*

Key words: 文本到图像模型，文化偏差，评估指标，CAIRe，文化相关性

TL;DR: 论文提出了一种名为CAIRe的新评估指标，用于衡量图像在不同文化背景下的相关性，解决了文本到图像模型中跨文化偏差难以衡量的问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 文本到图像模型在多样化文化环境中的公平性能是关键，但现有方法在减少跨文化偏差时存在性能下降、事实不准确或输出冒犯性内容等问题。由于缺乏可靠的偏差测量方法，进展受阻。

Method: CAIRe通过将图像中的实体和概念与知识库关联，利用事实信息为每个文化标签提供独立的分级判断。方法在手动构建的文化相关性数据集中表现出色。

Result: CAIRe在文化稀有项数据集上比基线方法高出28%的F1分数，并在文化通用概念数据集中与人类评分的Pearson相关系数分别为0.56和0.66。

Conclusion: CAIRe能够有效评估图像的文化相关性，与人类判断高度一致，为解决跨文化偏差提供了有力工具。

Abstract: As text-to-image models become increasingly prevalent, ensuring their
equitable performance across diverse cultural contexts is critical. Efforts to
mitigate cross-cultural biases have been hampered by trade-offs, including a
loss in performance, factual inaccuracies, or offensive outputs. Despite
widespread recognition of these challenges, an inability to reliably measure
these biases has stalled progress. To address this gap, we introduce CAIRe, a
novel evaluation metric that assesses the degree of cultural relevance of an
image, given a user-defined set of labels. Our framework grounds entities and
concepts in the image to a knowledge base and uses factual information to give
independent graded judgments for each culture label. On a manually curated
dataset of culturally salient but rare items built using language models, CAIRe
surpasses all baselines by 28% F1 points. Additionally, we construct two
datasets for culturally universal concept, one comprising of T2I-generated
outputs and another retrieved from naturally occurring data. CAIRe achieves
Pearson's correlations of 0.56 and 0.66 with human ratings on these sets, based
on a 5-point Likert scale of cultural relevance. This demonstrates its strong
alignment with human judgment across diverse image sources.

</details>


### [243] [Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation](https://arxiv.org/abs/2506.09350)
*Shanchuan Lin,Ceyuan Yang,Hao He,Jianwen Jiang,Yuxi Ren,Xin Xia,Yang Zhao,Xuefeng Xiao,Lu Jiang*

Key words: 视频生成, 自回归对抗训练, 实时交互, 潜在扩散模型, 高分辨率

TL;DR: 本文提出了一种自回归对抗后训练方法（AAPT），将预训练的潜在视频扩散模型转化为实时交互式视频生成器，支持单次神经网络评估（1NFE）生成每帧，并在单个H100上实现24fps的736x416分辨率视频流。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的视频生成模型计算量大，难以应用于实时和交互场景，因此需要一种高效的方法来实现实时视频生成与控制。

Method: 通过自回归对抗后训练（AAPT），设计了一种高效的单步生成架构，充分利用KV缓存，并以学生强制方式训练，减少长视频生成中的错误累积。

Result: 在单个H100上实现24fps的736x416分辨率视频流，或在8个H100上支持1280x720分辨率长达1分钟（1440帧）的视频生成。

Conclusion: AAPT是一种高效的实时交互视频生成方法，显著提升了生成速度和交互性，适用于高分辨率长视频的生成任务。

Abstract: Existing large-scale video generation models are computationally intensive,
preventing adoption in real-time and interactive applications. In this work, we
propose autoregressive adversarial post-training (AAPT) to transform a
pre-trained latent video diffusion model into a real-time, interactive video
generator. Our model autoregressively generates a latent frame at a time using
a single neural function evaluation (1NFE). The model can stream the result to
the user in real time and receive interactive responses as controls to generate
the next latent frame. Unlike existing approaches, our method explores
adversarial training as an effective paradigm for autoregressive generation.
This not only allows us to design an architecture that is more efficient for
one-step generation while fully utilizing the KV cache, but also enables
training the model in a student-forcing manner that proves to be effective in
reducing error accumulation during long video generation. Our experiments
demonstrate that our 8B model achieves real-time, 24fps, streaming video
generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to
a minute long (1440 frames). Visit our research website at
https://seaweed-apt.com/2

</details>


### [244] [SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing](https://arxiv.org/abs/2506.09363)
*Hongguang Zhu,Yunchao Wei,Mengyu Wang,Siyu Jiao,Yan Fang,Jiannan Huang,Yao Zhao*

Key words: 扩散模型, 概念擦除, 安全生成, 语义增强, 全局-局部协作

TL;DR: 论文提出了一种名为SAGE的方法，通过语义增强擦除和全局-局部协作保留机制，解决了扩散模型在预训练中可能包含敏感信息的安全风险问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 扩散模型在文本到图像生成中取得了显著进展，但预训练过程中不可避免地包含敏感信息，导致安全风险。目前的概念擦除方法将不安全概念视为固定词汇，无法实现广义的概念相关擦除。

Method: 提出语义增强擦除，将词汇概念擦除转化为概念域擦除，同时提出全局-局部协作保留机制，以保持不相关概念的生成能力。

Result: 实验证明SAGE在扩散模型的安全生成方面优于其他方法。

Conclusion: SAGE通过创新的擦除和保留机制，有效解决了扩散模型的安全问题，同时保持了无关概念的生成能力。

Abstract: Diffusion models (DMs) have achieved significant progress in text-to-image
generation. However, the inevitable inclusion of sensitive information during
pre-training poses safety risks, such as unsafe content generation and
copyright infringement. Concept erasing finetunes weights to unlearn
undesirable concepts, and has emerged as a promising solution. However,
existing methods treat unsafe concept as a fixed word and repeatedly erase it,
trapping DMs in ``word concept abyss'', which prevents generalized
concept-related erasing. To escape this abyss, we introduce semantic-augment
erasing which transforms concept word erasure into concept domain erasure by
the cyclic self-check and self-erasure. It efficiently explores and unlearns
the boundary representation of concept domain through semantic spatial
relationships between original and training DMs, without requiring additional
preprocessed data. Meanwhile, to mitigate the retention degradation of
irrelevant concepts while erasing unsafe concepts, we further propose the
global-local collaborative retention mechanism that combines global semantic
relationship alignment with local predicted noise preservation, effectively
expanding the retentive receptive field for irrelevant concepts. We name our
method SAGE, and extensive experiments demonstrate the comprehensive
superiority of SAGE compared with other methods in the safe generation of DMs.
The code and weights will be open-sourced at
https://github.com/KevinLight831/SAGE.

</details>


### [245] [Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs](https://arxiv.org/abs/2506.09522)
*Beomsik Cho,Jaehyung Kim*

Key words: 大型视觉-语言模型,视觉基础,解码策略,ReVisiT,多模态任务

TL;DR: 提出了一种名为ReVisiT的解码方法，通过引用视觉标记来引导大型视觉-语言模型的文本生成，提升视觉信息的利用效果。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统解码策略未能充分利用视觉信息，导致生成响应缺乏视觉基础，ReVisiT旨在解决这一问题。

Method: 通过将视觉标记投影到文本标记分布空间，并通过约束分歧最小化动态选择最相关视觉标记，优化输出分布以融入视觉语义。

Result: 在三个基准测试中，ReVisiT显著增强了视觉基础，计算成本降低至原来的50%。

Conclusion: ReVisiT是一种高效且计算成本低的解码方法，能显著提升视觉-语言模型的视觉基础性能。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable performance
across various multimodal tasks by integrating visual perception with language
understanding. However, conventional decoding strategies of LVLMs often fail to
successfully utilize visual information, leading to visually ungrounded
responses. While various approaches have been proposed to address this
limitation, they typically require additional training, multi-step inference
procedures, or external model dependencies. This paper introduces ReVisiT, a
simple yet effective decoding method that references vision tokens to guide the
text generation process in LVLMs. Our approach leverages the semantic
information embedded within vision tokens by projecting them into the text
token distribution space, and dynamically selecting the most relevant vision
token at each decoding step through constrained divergence minimization. This
selected vision token is then used to refine the output distribution to better
incorporate visual semantics. Experiments on three LVLM hallucination
benchmarks with two recent LVLMs demonstrate that ReVisiT consistently enhances
visual grounding with minimal computational overhead. Moreover, our method
achieves competitive or superior results relative to state-of-the-art baselines
while reducing computational costs for up to $2\times$.

</details>


### [246] [Synthetic Human Action Video Data Generation with Pose Transfer](https://arxiv.org/abs/2506.09411)
*Vaclav Knapp,Matyas Bohacek*

Key words: 合成数据, 姿态迁移, 视频理解, 动作识别, 3D高斯模型

TL;DR: 提出了一种基于姿态迁移的合成人类动作视频数据生成方法，解决了合成数据在视频理解任务中的‘恐怖谷’效应问题，并在多个数据集上验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的合成数据在人类动作视频理解任务中因‘恐怖谷’效应导致效果不佳，限制了其在手势识别、动作理解等任务中的应用潜力。

Method: 使用可控的3D高斯虚拟人物模型进行姿态迁移，生成合成人类动作视频数据。

Result: 在Toyota Smarthome和NTU RGB+D数据集上验证了方法在动作识别任务中的性能提升，并能有效扩展小样本数据集。

Conclusion: 该方法显著提升了合成数据的实用性，并在多样化背景和小样本任务中表现出色。

Abstract: In video understanding tasks, particularly those involving human motion,
synthetic data generation often suffers from uncanny features, diminishing its
effectiveness for training. Tasks such as sign language translation, gesture
recognition, and human motion understanding in autonomous driving have thus
been unable to exploit the full potential of synthetic data. This paper
proposes a method for generating synthetic human action video data using pose
transfer (specifically, controllable 3D Gaussian avatar models). We evaluate
this method on the Toyota Smarthome and NTU RGB+D datasets and show that it
improves performance in action recognition tasks. Moreover, we demonstrate that
the method can effectively scale few-shot datasets, making up for groups
underrepresented in the real training data and adding diverse backgrounds. We
open-source the method along with RANDOM People, a dataset with videos and
avatars of novel human identities for pose transfer crowd-sourced from the
internet.

</details>


### [247] [A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation](https://arxiv.org/abs/2506.09427)
*Yukang Feng,Jianwen Sun,Chuanhao Li,Zizhen Li,Jiaxin Ai,Fanrui Zhang,Yifan Chang,Sizhuo Zhou,Shenglin Zhang,Yu Dai,Kaipeng Zhang*

Key words: Large Multimodal Models (LMMs), InterSyn, Self-Evaluation with Iterative Refinement (SEIR), SynJudge, multimodal understanding

TL;DR: 论文介绍了InterSyn数据集和SEIR方法，用于提升多模态模型的图像-文本交织生成能力，同时提出了SynJudge评估工具。实验表明SEIR显著提升数据集质量，InterSyn训练的多模态模型性能全面提升。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有大型多模态模型（LMMs）在生成紧密交织的图像-文本输出方面表现不佳，原因是训练数据集规模、质量和指令丰富度不足。

Method: 采用Self-Evaluation with Iterative Refinement (SEIR)方法构建InterSyn数据集，包含多轮指令驱动对话和紧密交织的图像-文本响应，并通过SynJudge评估模型量化评估多模态输出。

Result: SEIR方法显著提升数据集质量；基于InterSyn训练的LMMs在所有评估指标上均实现性能提升。

Conclusion: InterSyn数据集和SEIR方法为下一代指令跟随型LMMs的训练提供了高质量资源，SynJudge解决了多模态输出评估的不足。

Abstract: Recent advancements in Large Multimodal Models (LMMs) have significantly
improved multimodal understanding and generation. However, these models still
struggle to generate tightly interleaved image-text outputs, primarily due to
the limited scale, quality and instructional richness of current training
datasets. To address this, we introduce InterSyn, a large-scale multimodal
dataset constructed using our Self-Evaluation with Iterative Refinement (SEIR)
method. InterSyn features multi-turn, instruction-driven dialogues with tightly
interleaved imagetext responses, providing rich object diversity and rigorous
automated quality refinement, making it well-suited for training
next-generation instruction-following LMMs. Furthermore, to address the lack of
reliable evaluation tools capable of assessing interleaved multimodal outputs,
we introduce SynJudge, an automatic evaluation model designed to quantitatively
assess multimodal outputs along four dimensions: text content, image content,
image quality, and image-text synergy.
  Experimental studies show that the SEIR method leads to substantially higher
dataset quality compared to an otherwise identical process without refinement.
  Moreover, LMMs trained on InterSyn achieve uniform performance gains across
all evaluation metrics, confirming InterSyn's utility for advancing multimodal
systems.

</details>


### [248] [Adding simple structure at inference improves Vision-Language Compositionality](https://arxiv.org/abs/2506.09691)
*Imanol Miranda,Ander Salaberria,Eneko Agirre,Gorka Azkune*

Key words: 视觉语言模型,组合性,图像检索,推理时技术

TL;DR: 论文提出了一种在推理时增强视觉语言模型（VLM）组合性的方法，通过分割图像和文本片段并匹配它们，无需额外训练即可提升性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的双编码器视觉语言模型（如CLIP）在组合性上表现不佳，限制了检索性能。训练方法已有较多研究，但推理时技术较少被关注。

Method: 1) 将图像分割为小区域；2) 提取文本片段（对象、属性、关系）；3) 使用VLM匹配图像区域与文本片段；4) 聚合匹配结果计算最终相似度。

Result: 方法在多个数据集上显著提升了双编码器VLM的性能，尤其在属性-对象绑定任务中表现突出。

Conclusion: 推理时技术具有潜力，图像分割和匹配是关键；未来可进一步优化推理方法。

Abstract: Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for
image-text retrieval tasks. However, those models struggle with
compositionality, showing a bag-of-words-like behavior that limits their
retrieval performance. Many different training approaches have been proposed to
improve the vision-language compositionality capabilities of those models. In
comparison, inference-time techniques have received little attention. In this
paper, we propose to add simple structure at inference, where, given an image
and a caption: i) we divide the image into different smaller crops, ii) we
extract text segments, capturing objects, attributes and relations, iii) using
a VLM, we find the image crops that better align with text segments obtaining
matches, and iv) we compute the final image-text similarity aggregating the
individual similarities of the matches. Based on various popular dual encoder
VLMs, we evaluate our approach in controlled and natural datasets for VL
compositionality. We find that our approach consistently improves the
performance of evaluated VLMs without any training, which shows the potential
of inference-time techniques. The results are especially good for
attribute-object binding as shown in the controlled dataset. As a result of an
extensive analysis: i) we show that processing image crops is actually
essential for the observed gains in performance, and ii) we identify specific
areas to further improve inference-time approaches.

</details>


### [249] [TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision](https://arxiv.org/abs/2506.09445)
*Ayush Gupta,Anirban Roy,Rama Chellappa,Nathaniel D. Bastian,Alvaro Velasquez,Susmit Jha*

Key words: 视频问答、时间定位、弱监督、TOGA模型

TL;DR: 论文提出TOGA模型，用于弱监督下基于时间定位的视频问答任务，通过联合生成答案与时间定位实现性能提升。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决无时间标注情况下视频问答的时间定位问题。

Method: 提出TOGA模型，通过指令调优联合生成答案与时间定位，并利用伪标签和一致性约束确保有效性。

Result: 在NExT-GQA、MSVD-QA和ActivityNet-QA等基准测试中达到最优性能。

Conclusion: 联合生成答案与时间定位能提升性能，TOGA在弱监督条件下表现优异。

Abstract: We address the problem of video question answering (video QA) with temporal
grounding in a weakly supervised setup, without any temporal annotations. Given
a video and a question, we generate an open-ended answer grounded with the
start and end time. For this task, we propose TOGA: a vision-language model for
Temporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune
TOGA to jointly generate the answer and the temporal grounding. We operate in a
weakly supervised setup where the temporal grounding annotations are not
available. We generate pseudo labels for temporal grounding and ensure the
validity of these labels by imposing a consistency constraint between the
question of a grounding response and the response generated by a question
referring to the same temporal segment. We notice that jointly generating the
answers with the grounding improves performance on question answering as well
as grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For
grounded QA, we consider the NExT-GQA benchmark which is designed to evaluate
weakly supervised grounded question answering. For open-ended QA, we consider
the MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art
performance for both tasks on these benchmarks.

</details>


### [250] [Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos](https://arxiv.org/abs/2506.09953)
*Benjamin Reichman,Constantin Patsch,Jack Truxal,Atishay Jain,Larry Heck*

Key words: 视觉问答, OK-VQA, 视频对话, 外部知识

TL;DR: 论文提出了一个基于视频的视觉问答任务（OK-VQA），模型需结合视频中的视觉信息和外部知识进行对话。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 探索在对话中结合视觉信息和外部知识的任务，特别是在视频场景中。

Method: 构建了一个包含2,017个视频和5,986个对话的数据集，标注了40,954个对话轮次。

Result: 提供了多个基线模型，并展示了任务的挑战性。

Conclusion: 该任务不仅需要识别视频中的相关信息，还需利用外部知识进行对话。

Abstract: In outside knowledge visual question answering (OK-VQA), the model must
identify relevant visual information within an image and incorporate external
knowledge to accurately respond to a question. Extending this task to a
visually grounded dialogue setting based on videos, a conversational model must
both recognize pertinent visual details over time and answer questions where
the required information is not necessarily present in the visual information.
Moreover, the context of the overall conversation must be considered for the
subsequent dialogue. To explore this task, we introduce a dataset comprised of
$2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$
interleaved dialogue turns. While the dialogue context is visually grounded in
specific video segments, the questions further require external knowledge that
is not visually present. Thus, the model not only has to identify relevant
video parts but also leverage external knowledge to converse within the
dialogue. We further provide several baselines evaluated on our dataset and
show future challenges associated with this task. The dataset is made publicly
available here: https://github.com/c-patsch/OKCV.

</details>


### [251] [Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search](https://arxiv.org/abs/2505.14156)
*Songhao Wu,Quan Tu,Hong Liu,Jia Xu,Zhongyi Liu,Guannan Zhang,Ran Wang,Xiuying Chen,Rui Yan*

Key words: 会话搜索, 大型语言模型, 图结构, 符号化学习, 自监督任务

TL;DR: 论文提出了一种结合文本和图结构的会话搜索方法(SGR)，通过符号化语法规则将图结构转换为文本，并利用大型语言模型(LLM)增强对图结构的理解。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前会话搜索方法侧重于序列建模，忽略交互的图结构，而现有图结构方法又忽视单词级语义建模。本文旨在结合两者的优点。

Method: 提出Symbolic Graph Ranker (SGR)，使用符号化语法规则将会话图转为文本，并设计自监督任务（如链接预测、节点内容生成）增强LLM对图结构的理解。

Result: 在AOL和Tiangong-ST数据集上验证了方法的优越性。

Conclusion: SGR为传统搜索策略与现代LLM架起桥梁，提供了一种新颖有效的解决方案。

Abstract: Session search involves a series of interactive queries and actions to
fulfill user's complex information need. Current strategies typically
prioritize sequential modeling for deep semantic understanding, overlooking the
graph structure in interactions. While some approaches focus on capturing
structural information, they use a generalized representation for documents,
neglecting the word-level semantic modeling. In this paper, we propose Symbolic
Graph Ranker (SGR), which aims to take advantage of both text-based and
graph-based approaches by leveraging the power of recent Large Language Models
(LLMs). Concretely, we first introduce a set of symbolic grammar rules to
convert session graph into text. This allows integrating session history,
interaction process, and task instruction seamlessly as inputs for the LLM.
Moreover, given the natural discrepancy between LLMs pre-trained on textual
corpora, and the symbolic language we produce using our graph-to-text grammar,
our objective is to enhance LLMs' ability to capture graph structures within a
textual format. To achieve this, we introduce a set of self-supervised symbolic
learning tasks including link prediction, node content generation, and
generative contrastive learning, to enable LLMs to capture the topological
information from coarse-grained to fine-grained. Experiment results and
comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm
the superiority of our approach. Our paradigm also offers a novel and effective
methodology that bridges the gap between traditional search strategies and
modern LLMs.

</details>


### [252] [AD^2-Bench: A Hierarchical CoT Benchmark for MLLM in Autonomous Driving under Adverse Conditions](https://arxiv.org/abs/2506.09557)
*Zhaoyang Wei,Chenhui Qiang,Bowen Jiang,Xumeng Han,Xuehui Yu,Zhenjun Han*

Key words: Chain-of-Thought, 自动驾驶, 恶劣天气, 复杂场景, MLLMs

TL;DR: AD^2-Bench是首个针对恶劣天气和复杂场景下自动驾驶的Chain-of-Thought基准测试，填补了现有评估的空白。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有基准测试在恶劣天气和复杂交通环境下对CoT推理的评估不足，需要更全面的评估工具。

Method: AD^2-Bench包含5.4k高质量手动标注的CoT实例，每个推理步骤均有明确标注，支持多层次视觉提示。

Result: 评估显示当前MLLMs的准确率低于60%，表明基准的高难度和需改进推理能力。

Conclusion: AD^2-Bench为自动驾驶中的CoT推理提供了标准评估平台，推动MLLMs的进步。

Abstract: Chain-of-Thought (CoT) reasoning has emerged as a powerful approach to
enhance the structured, multi-step decision-making capabilities of Multi-Modal
Large Models (MLLMs), is particularly crucial for autonomous driving with
adverse weather conditions and complex traffic environments. However, existing
benchmarks have largely overlooked the need for rigorous evaluation of CoT
processes in these specific and challenging scenarios. To address this critical
gap, we introduce AD^2-Bench, the first Chain-of-Thought benchmark specifically
designed for autonomous driving with adverse weather and complex scenes.
AD^2-Bench is meticulously constructed to fulfill three key criteria:
comprehensive data coverage across diverse adverse environments, fine-grained
annotations that support multi-step reasoning, and a dedicated evaluation
framework tailored for assessing CoT performance. The core contribution of
AD^2-Bench is its extensive collection of over 5.4k high-quality, manually
annotated CoT instances. Each intermediate reasoning step in these annotations
is treated as an atomic unit with explicit ground truth, enabling unprecedented
fine-grained analysis of MLLMs' inferential processes under text-level,
point-level, and region-level visual prompts. Our comprehensive evaluation of
state-of-the-art MLLMs on AD^2-Bench reveals accuracy below 60%, highlighting
the benchmark's difficulty and the need to advance robust, interpretable
end-to-end autonomous driving systems. AD^2-Bench thus provides a standardized
evaluation platform, driving research forward by improving MLLMs' reasoning in
autonomous driving, making it an invaluable resource.

</details>


### [253] [BG-HOP: A Bimanual Generative Hand-Object Prior](https://arxiv.org/abs/2506.09068)
*Sriram Krishna,Sravan Chittupalli,Sungjae Park*

Key words: 双手交互, 生成型先验, 3D建模, 物体抓取

TL;DR: BG-HOP是一种生成型先验模型，用于建模3D中的双手与物体交互，解决了双手交互数据不足的问题，并通过实验展示了其在生成双手交互和物体抓取方面的能力。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决双手交互数据有限的问题，扩展单手的生成型先验模型。

Method: 通过扩展现有单手的生成型先验，建模双手与物体的联合分布。

Result: 模型能够生成双手交互并合成给定物体的抓取动作，代码和模型已公开。

Conclusion: BG-HOP为双手与物体交互提供了一种有效的生成型先验模型。

Abstract: In this work, we present BG-HOP, a generative prior that seeks to model
bimanual hand-object interactions in 3D. We address the challenge of limited
bimanual interaction data by extending existing single-hand generative priors,
demonstrating preliminary results in capturing the joint distribution of hands
and objects. Our experiments showcase the model's capability to generate
bimanual interactions and synthesize grasps for given objects. We make code and
models publicly available.

</details>


### [254] [HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding](https://arxiv.org/abs/2506.09634)
*Yanzhao Shi,Xiaodan Zhang,Junzhong Ji,Haoning Jiang,Chengxin Zheng,Yinong Wang,Liangqiong Qu*

Key words: 3D CT诊断、多模态大语言模型、视觉语言理解、HSENet

TL;DR: HSENet使用双重3D视觉编码器和空间压缩技术，提升3D医学图像的多模态语言理解能力，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有多模态大语言模型（MLLMs）主要针对2D医学图像，无法充分捕捉复杂的3D解剖结构，易导致误诊。

Method: HSENet结合双重3D视觉编码器和空间压缩投影器，实现全局与局部特征的融合，并通过预训练对齐诊断报告。

Result: 在3D视觉语言检索、医学报告生成和视觉问答任务中，HSENet性能显著提升，达到最先进水平。

Conclusion: HSENet为3D医学图像的多模态理解提供了有效解决方案，展现了临床应用潜力。

Abstract: Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based
decisions by enhancing diagnostic accuracy and workflow efficiency. While
multimodal large language models (MLLMs) exhibit promising performance in
visual-language understanding, existing methods mainly focus on 2D medical
images, which fundamentally limits their ability to capture complex 3D
anatomical structures. This limitation often leads to misinterpretation of
subtle pathologies and causes diagnostic hallucinations. In this paper, we
present Hybrid Spatial Encoding Network (HSENet), a framework that exploits
enriched 3D medical visual cues by effective visual perception and projection
for accurate and robust vision-language understanding. Specifically, HSENet
employs dual-3D vision encoders to perceive both global volumetric contexts and
fine-grained anatomical details, which are pre-trained by dual-stage alignment
with diagnostic reports. Furthermore, we propose Spatial Packer, an efficient
multimodal projector that condenses high-resolution 3D spatial regions into a
compact set of informative visual tokens via centroid-based compression. By
assigning spatial packers with dual-3D vision encoders, HSENet can seamlessly
perceive and transfer hybrid visual representations to LLM's semantic space,
facilitating accurate diagnostic text generation. Experimental results
demonstrate that our method achieves state-of-the-art performance in 3D
language-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report
generation (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering
(73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness.
Our code is available at https://github.com/YanzhaoShi/HSENet.

</details>


### [255] [DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning](https://arxiv.org/abs/2506.09644)
*Dongxu Liu,Yuang Peng,Haomiao Tang,Yuwei Chen,Chunrui Han,Zheng Ge,Daxin Jiang,Mingxue Liao*

Key words: 自编码器, 扩散模型, 潜在空间, 图像生成

TL;DR: DGAE提出一种通过扩散模型引导解码器恢复潜在表示中未完全解码信号的方法，在高空间压缩比下有效减少性能下降，同时实现了2倍更小的潜在空间。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决自编码器在高压缩比下的性能下降问题，并通过提升解码器的表达能力来实现更高效的潜在表示。

Method: 采用扩散模型（DGAE）引导解码器恢复潜在表示中的信息信号，优化解码过程。

Result: DGAE在高压缩比下减少了性能下降，潜在空间缩小2倍，且在图像生成任务中表现优异，加速了扩散模型的收敛。

Conclusion: DGAE通过扩散模型优化解码器，在高效压缩和性能提升方面取得突破。

Abstract: Autoencoders empower state-of-the-art image and video generative models by
compressing pixels into a latent space through visual tokenization. Although
recent advances have alleviated the performance degradation of autoencoders
under high compression ratios, addressing the training instability caused by
GAN remains an open challenge. While improving spatial compression, we also aim
to minimize the latent space dimensionality, enabling more efficient and
compact representations. To tackle these challenges, we focus on improving the
decoder's expressiveness. Concretely, we propose DGAE, which employs a
diffusion model to guide the decoder in recovering informative signals that are
not fully decoded from the latent representation. With this design, DGAE
effectively mitigates the performance degradation under high spatial
compression rates. At the same time, DGAE achieves state-of-the-art performance
with a 2x smaller latent space. When integrated with Diffusion Models, DGAE
demonstrates competitive performance on image generation for ImageNet-1K and
shows that this compact latent representation facilitates faster convergence of
the diffusion model.

</details>


### [256] [Reasoning Models Are More Easily Gaslighted Than You Think](https://arxiv.org/abs/2506.09677)
*Bin Zhu,Hailong Yin,Jingjing Chen,Yu-Gang Jiang*

Key words: 推理模型, 鲁棒性, 误导性输入, GaslightingBench-R, 多模态基准

TL;DR: 论文研究了推理模型在误导性用户输入下的鲁棒性，发现即使是最先进的模型也难以保持正确性，并提出了新的诊断基准GaslightingBench-R。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 评估推理模型在面对误导性输入时的表现，揭示其在保持正确性方面的局限性。

Method: 系统评估了三种最先进的推理模型，并在三个多模态基准上测试其准确性下降情况，随后提出了GaslightingBench-R基准。

Result: 模型在误导性提示下平均准确性下降25-53%，揭示了推理模型的鲁棒性不足。

Conclusion: 推理模型在逐步推理与信念持久性之间存在显著差距，需进一步改进。

Abstract: Recent advances in reasoning-centric models promise improved robustness
through mechanisms such as chain-of-thought prompting and test-time scaling.
However, their ability to withstand misleading user input remains
underexplored. In this paper, we conduct a systematic evaluation of three
state-of-the-art reasoning models, i.e., OpenAI's o4-mini, Claude-3.7-Sonnet
and Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and
CharXiv. Our evaluation reveals significant accuracy drops (25-29% on average)
following gaslighting negation prompts, indicating that even top-tier reasoning
models struggle to preserve correct answers under manipulative user feedback.
Built upon the insights of the evaluation and to further probe this
vulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark
specifically designed to evaluate reasoning models' susceptibility to defend
their belief under gaslighting negation prompt. Constructed by filtering and
curating 1,025 challenging samples from the existing benchmarks,
GaslightingBench-R induces even more dramatic failures, with accuracy drops
exceeding 53% on average. Our findings reveal fundamental limitations in the
robustness of reasoning models, highlighting the gap between step-by-step
reasoning and belief persistence.

</details>


### [257] [Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model](https://arxiv.org/abs/2506.09695)
*Changwei Wu,Yifei Chen,Yuxin Du,Jinying Zong,Jie Dong,Mingxuan Liu,Yong Peng,Jin Fan,Feiwei Qin,Changmiao Wang*

Key words: 阿尔茨海默病, 脉冲神经网络, 高效诊断, 3D MRI

TL;DR: FasterSNN是一种融合LIF神经元和区域自适应卷积的混合神经架构，用于高效诊断阿尔茨海默病（AD），特别是在资源有限的环境下。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前AD早期诊断依赖于主观评估和高成本的多模态成像，深度学习方法虽自动化但能耗高。SNN作为脑启发范式，适合模拟AD的稀疏事件驱动模式，但现有SNN表达能力不足且训练不稳定。

Method: 提出FasterSNN，结合LIF神经元、区域自适应卷积和多尺度脉冲注意力，用于稀疏高效处理3D MRI数据。

Result: 在基准数据集上，FasterSNN表现优异，效率与稳定性显著提升，适合实际AD筛查。

Conclusion: FasterSNN为资源受限环境下的AD诊断提供了高效且稳定的解决方案。

Abstract: Early diagnosis of Alzheimer's Disease (AD), especially at the mild cognitive
impairment (MCI) stage, is vital yet hindered by subjective assessments and the
high cost of multimodal imaging modalities. Although deep learning methods
offer automated alternatives, their energy inefficiency and computational
demands limit real-world deployment, particularly in resource-constrained
settings. As a brain-inspired paradigm, spiking neural networks (SNNs) are
inherently well-suited for modeling the sparse, event-driven patterns of neural
degeneration in AD, offering a promising foundation for interpretable and
low-power medical diagnostics. However, existing SNNs often suffer from weak
expressiveness and unstable training, which restrict their effectiveness in
complex medical tasks. To address these limitations, we propose FasterSNN, a
hybrid neural architecture that integrates biologically inspired LIF neurons
with region-adaptive convolution and multi-scale spiking attention. This design
enables sparse, efficient processing of 3D MRI while preserving diagnostic
accuracy. Experiments on benchmark datasets demonstrate that FasterSNN achieves
competitive performance with substantially improved efficiency and stability,
supporting its potential for practical AD screening. Our source code is
available at https://github.com/wuchangw/FasterSNN.

</details>


### [258] [Bias Analysis in Unconditional Image Generative Models](https://arxiv.org/abs/2506.09106)
*Xiaofeng Zhang,Michelle Lin,Simon Lacoste-Julien,Aaron Courville,Yash Goyal*

Key words: 生成AI, 无条件下生成, 偏差评估, 分类器敏感性

TL;DR: 论文研究了生成AI模型中无条件下生成的偏差机制及评估方法的局限性，提出需要更全面的标注实践和评估框架审查。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 研究生成AI模型中无条件下生成偏差的出现机制及其评估框架的不足。

Method: 训练无条件图像生成模型，采用常用偏差评估框架，分析训练与生成分布之间的偏差变化。

Result: 实验显示属性偏移较小，但偏移对使用的属性分类器敏感，尤其在分类器决策边界位于高密度区域时。

Conclusion: 需改进标注实践、审查评估框架，并认识到属性在社会复杂性中的多样性。

Abstract: The widespread adoption of generative AI models has raised growing concerns
about representational harm and potential discriminatory outcomes. Yet, despite
growing literature on this topic, the mechanisms by which bias emerges -
especially in unconditional generation - remain disentangled. We define the
bias of an attribute as the difference between the probability of its presence
in the observed distribution and its expected proportion in an ideal reference
distribution. In our analysis, we train a set of unconditional image generative
models and adopt a commonly used bias evaluation framework to study bias shift
between training and generated distributions. Our experiments reveal that the
detected attribute shifts are small. We find that the attribute shifts are
sensitive to the attribute classifier used to label generated images in the
evaluation framework, particularly when its decision boundaries fall in
high-density regions. Our empirical analysis indicates that this classifier
sensitivity is often observed in attributes values that lie on a spectrum, as
opposed to exhibiting a binary nature. This highlights the need for more
representative labeling practices, understanding the shortcomings through
greater scrutiny of evaluation frameworks, and recognizing the socially complex
nature of attributes when evaluating bias.

</details>


### [259] [Non-Contact Health Monitoring During Daily Personal Care Routines](https://arxiv.org/abs/2506.09718)
*Xulin Ma,Jiankai Tang,Zhang Jiang,Songqin Cheng,Yuanchun Shi,Dong LI,Xin Liu,Daniel McDuff,Xiaojing Liu,Yuntao Wang*

Key words: 远程光电容积描记,高海拔,长期健康监测,多任务学习,数据集

TL;DR: LADH数据集旨在解决高海拔环境中长期远程光电容积描记（rPPG）监测的挑战，结合RGB和红外视频输入提升生理信号监测的准确性和鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决高海拔环境下长期个人护理场景中rPPG技术面临的照明变化、遮挡和动态面部姿势等挑战。

Method: 提出LADH数据集，包含240段RGB和红外同步面部视频，结合多任务学习方法。

Result: RGB和红外视频结合的rPPG监测在心率估计中MAE为4.99 BPM，多任务学习提升了多项生理指标的监测性能。

Conclusion: LADH数据集和方法显著提升了长期rPPG监测在高海拔环境中的适用性和准确性。

Abstract: Remote photoplethysmography (rPPG) enables non-contact, continuous monitoring
of physiological signals and offers a practical alternative to traditional
health sensing methods. Although rPPG is promising for daily health monitoring,
its application in long-term personal care scenarios, such as mirror-facing
routines in high-altitude environments, remains challenging due to ambient
lighting variations, frequent occlusions from hand movements, and dynamic
facial postures. To address these challenges, we present LADH (Long-term
Altitude Daily Health), the first long-term rPPG dataset containing 240
synchronized RGB and infrared (IR) facial videos from 21 participants across
five common personal care scenarios, along with ground-truth PPG, respiration,
and blood oxygen signals. Our experiments demonstrate that combining RGB and IR
video inputs improves the accuracy and robustness of non-contact physiological
monitoring, achieving a mean absolute error (MAE) of 4.99 BPM in heart rate
estimation. Furthermore, we find that multi-task learning enhances performance
across multiple physiological indicators simultaneously. Dataset and code are
open at https://github.com/McJackTang/FusionVitals.

</details>


### [260] [Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math Reasoning](https://arxiv.org/abs/2506.09736)
*Yuting Li,Lai Wei,Kaipeng Zheng,Jingyuan Huang,Linghe Kong,Lichao Sun,Weiran Huang*

Key words: 多模态大语言模型（MLLMs）、视觉扰动、数学推理、感知鲁棒性

TL;DR: 论文通过实验发现，仅使用语言模型结合图像描述可以达到甚至超越多模态大语言模型（MLLMs）的性能。为此，作者提出了一种视觉扰动框架，无需算法修改或额外数据即可提升感知鲁棒性，显著提高了数学推理能力。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前MLLMs在视觉处理方面存在局限，虽然能生成准确的视觉描述，但在推理时未能有效整合视觉信息。通过视觉扰动提升模型的感知鲁棒性，从而提高性能。

Method: 提出三种视觉扰动策略（干扰项拼接、保持主导性的混排、随机旋转），可无缝集成到现有训练流程（如SFT、DPO、GRPO）中。

Result: 实验表明，视觉扰动显著提升了数学推理性能，效果与算法改进相当。训练Qwen2.5-VL-7B模型实现了开源7B RL-tuned模型的竞争力。

Conclusion: 视觉扰动在多模态数学推理中至关重要，提升视觉处理能力可显著改善推理表现。

Abstract: Despite the rapid progress of multimodal large language models (MLLMs), they
have largely overlooked the importance of visual processing. In a simple yet
revealing experiment, we interestingly find that language-only models, when
provided with image captions, can achieve comparable or even better performance
than MLLMs that consume raw visual inputs. This suggests that current MLLMs may
generate accurate visual descriptions but fail to effectively integrate them
during reasoning. Motivated by this, we propose a simple visual perturbation
framework that enhances perceptual robustness without requiring algorithmic
modifications or additional training data. Our approach introduces three
targeted perturbations: distractor concatenation, dominance-preserving mixup,
and random rotation, that can be easily integrated into existing post-training
pipelines including SFT, DPO, and GRPO. Through extensive experiments across
multiple datasets, we demonstrate consistent improvements in mathematical
reasoning performance, with gains comparable to those achieved through
algorithmic changes. Additionally, we achieve competitive performance among
open-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual
perturbation. Through comprehensive ablation studies, we analyze the
effectiveness of different perturbation strategies, revealing that each
perturbation type contributes uniquely to different aspects of visual
reasoning. Our findings highlight the critical role of visual perturbation in
multimodal mathematical reasoning: better reasoning begins with better seeing.
Our code is available at https://github.com/YutingLi0606/Vision-Matters.

</details>


### [261] [PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies](https://arxiv.org/abs/2506.09237)
*Mojtaba Nafez,Amirhossein Koochakian,Arad Maleki,Jafar Habibi,Mohammad Hossein Rohban*

Key words: 异常检测,异常定位,对抗鲁棒性,ViT,对抗训练

TL;DR: PatchGuard是一种基于Vision Transformer的对抗性鲁棒异常检测与定位方法，通过引入伪异常样本和定位掩码，结合新损失函数提升模型在对抗攻击下的性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前异常检测与定位方法易受对抗攻击，主要因训练数据仅包含正常样本，缺乏对抗鲁棒性。

Method: 利用前景感知伪异常样本，结合ViT架构和对抗训练，采用新损失函数增强模型鲁棒性。

Result: 在工业和医疗数据集上，PatchGuard在对抗性环境下AD性能提升53.2%，AL提升68.5%，非对抗性环境下表现仍优。

Conclusion: PatchGuard通过伪异常样本和理论驱动的对抗训练显著提升了异常检测与定位的鲁棒性。

Abstract: Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields
that demand high reliability, such as medical imaging and industrial
monitoring. However, current AD and AL approaches are often susceptible to
adversarial attacks due to limitations in training data, which typically
include only normal, unlabeled samples. This study introduces PatchGuard, an
adversarially robust AD and AL method that incorporates pseudo anomalies with
localization masks within a Vision Transformer (ViT)-based architecture to
address these vulnerabilities. We begin by examining the essential properties
of pseudo anomalies, and follow it by providing theoretical insights into the
attention mechanisms required to enhance the adversarial robustness of AD and
AL systems. We then present our approach, which leverages Foreground-Aware
Pseudo-Anomalies to overcome the deficiencies of previous anomaly-aware
methods. Our method incorporates these crafted pseudo-anomaly samples into a
ViT-based framework, with adversarial training guided by a novel loss function
designed to improve model robustness, as supported by our theoretical analysis.
Experimental results on well-established industrial and medical datasets
demonstrate that PatchGuard significantly outperforms previous methods in
adversarial settings, achieving performance gains of $53.2\%$ in AD and
$68.5\%$ in AL, while also maintaining competitive accuracy in non-adversarial
settings. The code repository is available at
https://github.com/rohban-lab/PatchGuard .

</details>


### [262] [ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models](https://arxiv.org/abs/2506.09740)
*Qin Zhou,Zhiyang Zhang,Jinglong Wang,Xiaobin Li,Jing Zhang,Qian Yu,Lu Sheng,Dong Xu*

Key words: 扩散模型, 文本-图像对齐, 零样本分割, ELBO, 像素校准

TL;DR: 论文提出了ELBO-T2IAlign方法，用于校准扩散模型中的像素-文本对齐问题，无需训练且适用于多种架构。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 扩散模型虽然在图像生成上表现优秀，但其文本-图像对齐假设并不完美，尤其在像素级别和类别级别上存在对齐偏差。

Method: 使用零样本参考图像分割作为代理任务评估对齐问题，并提出基于证据下界（ELBO）的校准方法ELBO-T2IAlign。

Result: 实验证明该方法在图像分割和生成任务上有效，尤其在处理小尺寸、遮挡或罕见类别对象时表现突出。

Conclusion: ELBO-T2IAlign是一种简单且通用的校准方法，能够显著提升扩散模型的像素-文本对齐性能。

Abstract: Diffusion models excel at image generation. Recent studies have shown that
these models not only generate high-quality images but also encode text-image
alignment information through attention maps or loss functions. This
information is valuable for various downstream tasks, including segmentation,
text-guided image editing, and compositional image generation. However, current
methods heavily rely on the assumption of perfect text-image alignment in
diffusion models, which is not the case. In this paper, we propose using
zero-shot referring image segmentation as a proxy task to evaluate the
pixel-level image and class-level text alignment of popular diffusion models.
We conduct an in-depth analysis of pixel-text misalignment in diffusion models
from the perspective of training data bias. We find that misalignment occurs in
images with small sized, occluded, or rare object classes. Therefore, we
propose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text
alignment in diffusion models based on the evidence lower bound (ELBO) of
likelihood. Our method is training-free and generic, eliminating the need to
identify the specific cause of misalignment and works well across various
diffusion model architectures. Extensive experiments on commonly used benchmark
datasets on image segmentation and generation have verified the effectiveness
of our proposed calibration approach.

</details>


### [263] [UFM: A Simple Path towards Unified Dense Correspondence with Flow](https://arxiv.org/abs/2506.09278)
*Yuchen Zhang,Nikhil Keetha,Chenwei Lyu,Bhuvan Jhamb,Yutian Chen,Yuheng Qiu,Jay Karhade,Shreyas Jha,Yaoyu Hu,Deva Ramanan,Sebastian Scherer,Wenshan Wang*

Key words: 密集图像对应, 光流估计, 宽基线匹配, 统一模型, 变换器架构

TL;DR: 提出了一种统一的流与匹配模型（UFM），通过统一训练在宽基线与光流估计任务中超越了专门的现有方法，实现了更高精度、更低错误率和更快速度。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决密集图像对应在宽基线场景和光流估计中分离处理的问题，探索统一训练的潜力，以提升通用性和性能。

Method: 使用简单的通用变换器架构直接回归(u,v)流，避免了传统的粗到细成本体积方法，统一训练数据为共可见像素。

Result: UFM比最先进的光流方法（Unimatch）准确率高28%，宽基线匹配器（RoMa）错误率低62%，速度快6.7倍。

Conclusion: 统一训练首次在宽基线与光流任务中超越专门方法，为多模态、长距离和实时对应任务开辟了新方向。

Abstract: Dense image correspondence is central to many applications, such as visual
odometry, 3D reconstruction, object association, and re-identification.
Historically, dense correspondence has been tackled separately for
wide-baseline scenarios and optical flow estimation, despite the common goal of
matching content between two images. In this paper, we develop a Unified Flow &
Matching model (UFM), which is trained on unified data for pixels that are
co-visible in both source and target images. UFM uses a simple, generic
transformer architecture that directly regresses the (u,v) flow. It is easier
to train and more accurate for large flows compared to the typical
coarse-to-fine cost volumes in prior work. UFM is 28% more accurate than
state-of-the-art flow methods (Unimatch), while also having 62% less error and
6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to
demonstrate that unified training can outperform specialized approaches across
both domains. This result enables fast, general-purpose correspondence and
opens new directions for multi-modal, long-range, and real-time correspondence
tasks.

</details>


### [264] [Inverting Black-Box Face Recognition Systems via Zero-Order Optimization in Eigenface Space](https://arxiv.org/abs/2506.09777)
*Anton Razzhigaev,Matvey Mikhalchuk,Klim Kireev,Igor Udovichenko,Andrey Kuznetsov,Aleksandr Petiushko*

Key words: 模型反演、黑盒、相似度分数、零阶优化、特征脸

TL;DR: DarkerBB是一种新颖的方法，通过PCA特征脸空间零阶优化重建彩色人脸图像，仅使用相似度分数实现模型反演，在LFW等基准测试中表现出色。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决从黑盒识别模型仅使用相似度分数重建人脸图像的隐私威胁问题。

Method: 采用PCA特征脸空间的零阶优化方法。

Result: 在LFW、AgeDB-30和CFP-FP基准测试中，DarkerBB在仅使用相似度分数的设置下达到最先进的验证准确率。

Conclusion: DarkerBB在信息高度受限的情况下，仍能高效实现人脸图像重建。

Abstract: Reconstructing facial images from black-box recognition models poses a
significant privacy threat. While many methods require access to embeddings, we
address the more challenging scenario of model inversion using only similarity
scores. This paper introduces DarkerBB, a novel approach that reconstructs
color faces by performing zero-order optimization within a PCA-derived
eigenface space. Despite this highly limited information, experiments on LFW,
AgeDB-30, and CFP-FP benchmarks demonstrate that DarkerBB achieves
state-of-the-art verification accuracies in the similarity-only setting, with
competitive query efficiency.

</details>


### [265] [Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery](https://arxiv.org/abs/2506.09299)
*Sindhu Boddu,Arindam Mukherjee*

Key words: 轻量级,物体检测,航拍图像,应急响应,YOLOv4-Tiny,量化,低功耗设备

TL;DR: 本文提出了一种轻量级且高能效的物体检测方法，适用于应急响应中的航拍图像，采用了量化后的YOLOv4-Tiny模型，性能接近YOLOv5-small，但显著减小了模型体积并提升了推理速度。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 针对应急响应中航拍图像检测的需求，现有公开数据集不足，且需要高效、实时的模型部署在低功耗设备上。

Method: 使用YOLOv4-Tiny模型，通过后训练量化至INT8精度，并在自定义的航拍应急数据集上训练。

Result: 量化后的模型体积减少71%，推理速度提升44%，检测性能与YOLOv5-small相当。

Conclusion: 量化后的YOLOv4-Tiny模型适合在低功耗边缘设备上实时检测应急场景。

Abstract: This paper presents a lightweight and energy-efficient object detection
solution for aerial imagery captured during emergency response situations. We
focus on deploying the YOLOv4-Tiny model, a compact convolutional neural
network, optimized through post-training quantization to INT8 precision. The
model is trained on a custom-curated aerial emergency dataset, consisting of
10,820 annotated images covering critical emergency scenarios. Unlike prior
works that rely on publicly available datasets, we created this dataset
ourselves due to the lack of publicly available drone-view emergency imagery,
making the dataset itself a key contribution of this work. The quantized model
is evaluated against YOLOv5-small across multiple metrics, including mean
Average Precision (mAP), F1 score, inference time, and model size. Experimental
results demonstrate that the quantized YOLOv4-Tiny achieves comparable
detection performance while reducing the model size from 22.5 MB to 6.4 MB and
improving inference speed by 44\%. With a 71\% reduction in model size and a
44\% increase in inference speed, the quantized YOLOv4-Tiny model proves highly
suitable for real-time emergency detection on low-power edge devices.

</details>


### [266] [Q-SAM2: Accurate Quantization for Segment Anything Model 2](https://arxiv.org/abs/2506.09782)
*Nicola Farronato,Florian Scheidegger,Mattia Rigotti,Cristiano Malossi,Michele Magno,Haotong Qin*

Key words: SAM2, 量化, 高效计算, 量化感知训练, 2比特量化

TL;DR: Q-SAM2是一种针对SAM2模型的低比特量化方法，通过线性层校准和量化感知训练显著提升了模型效率，尤其在超低2比特量化下表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决Segment Anything Model 2 (SAM2)在资源受限场景下计算和内存消耗过高的问题，提升其实际应用性。

Method: 提出Q-SAM2方法：1) 线性层校准优化量化初始化；2) 量化感知训练（QAT）抑制异常值并适应量化阈值。

Result: Q-SAM2在保持高精度推理的同时显著提升效率，2比特量化下表现优于现有方案，后训练量化精度提升66%。

Conclusion: Q-SAM2为资源受限场景提供了一种高效且精确的量化解决方案。

Abstract: The Segment Anything Model 2 (SAM2) has gained significant attention as a
foundational approach for promptable image and video segmentation. However, its
expensive computational and memory consumption poses a severe challenge for its
application in resource-constrained scenarios. In this paper, we propose an
accurate low-bit quantization method for efficient SAM2, termed Q-SAM2. To
address the performance degradation caused by the singularities in weight and
activation distributions during quantization, Q-SAM2 introduces two novel
technical contributions. We first introduce a linear layer calibration method
for low-bit initialization of SAM2, which minimizes the Frobenius norm over a
small image batch to reposition weight distributions for improved quantization.
We then propose a Quantization-Aware Training (QAT) pipeline that applies
clipping to suppress outliers and allows the network to adapt to quantization
thresholds during training. Our comprehensive experiments demonstrate that
Q-SAM2 allows for highly accurate inference while substantially improving
efficiency. Both quantitative and visual results show that our Q-SAM2 surpasses
existing state-of-the-art general quantization schemes, especially for
ultra-low 2-bit quantization. While designed for quantization-aware training,
our proposed calibration technique also proves effective in post-training
quantization, achieving up to a 66% mIoU accuracy improvement over
non-calibrated models.

</details>


### [267] [DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction](https://arxiv.org/abs/2506.09836)
*Junli Deng,Ping Shi,Qipei Li,Jinyang Guo*

Key words: 动态场景重建,高斯泼溅,分层运动建模,静态-动态分离

TL;DR: DynaSplat是一种基于高斯泼溅的动态场景重建方法，通过动态-静态分离和分层运动建模，显著提升了复杂动态场景的重建精度和效率。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有方法在处理复杂动态场景时表现不足，因此提出DynaSplat以解决这一问题，旨在捕捉真实世界中的精细运动。

Method: 结合变形偏移统计和2D运动流一致性分类静态与动态元素；采用分层运动建模捕捉全局与局部运动；基于物理的透明度估计确保视觉一致性。

Result: 在多个数据集上，DynaSplat在精度和逼真度上超越现有技术，同时提供更高效的重建路径。

Conclusion: DynaSplat为动态场景重建提供了高效且直观的解决方案，适用于复杂非刚性运动场景。

Abstract: Reconstructing intricate, ever-changing environments remains a central
ambition in computer vision, yet existing solutions often crumble before the
complexity of real-world dynamics. We present DynaSplat, an approach that
extends Gaussian Splatting to dynamic scenes by integrating dynamic-static
separation and hierarchical motion modeling. First, we classify scene elements
as static or dynamic through a novel fusion of deformation offset statistics
and 2D motion flow consistency, refining our spatial representation to focus
precisely where motion matters. We then introduce a hierarchical motion
modeling strategy that captures both coarse global transformations and
fine-grained local movements, enabling accurate handling of intricate,
non-rigid motions. Finally, we integrate physically-based opacity estimation to
ensure visually coherent reconstructions, even under challenging occlusions and
perspective shifts. Extensive experiments on challenging datasets reveal that
DynaSplat not only surpasses state-of-the-art alternatives in accuracy and
realism but also provides a more intuitive, compact, and efficient route to
dynamic scene reconstruction.

</details>


### [268] [OctoNav: Towards Generalist Embodied Navigation](https://arxiv.org/abs/2506.09839)
*Chen Gao,Liankai Jin,Xingyu Peng,Jiazhao Zhang,Yue Deng,Annan Li,He Wang,Si Liu*

Key words: Embodied AI, Navigation, Multi-modal, Generalist Agent, TBA-CoT

TL;DR: 论文提出了一种通用导航代理OctoNav-R1和大型基准OctoNav-Bench，支持多模态和多能力的自由指令导航，并通过三阶段训练范式提升了模型的推理能力。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 为了解决现有导航任务（如ObjNav、ImgNav和VLN）因任务目标和模态差异而导致的独立设计和缺乏通用性的问题，研究旨在开发一种能够处理多模态和多能力自由指令的通用导航代理。

Method: 提出OctoNav-Bench基准和OctoNav-R1方法。基准通过设计标注流程构建连续环境和多样指令-轨迹对，方法基于MLLMs和VLA模型，采用包含Action-/TBA-SFT、Nav-GPRO和Online RL三阶段的混合训练范式。

Result: OctoNav-R1在性能上优于之前的方法。

Conclusion: 通过Think-Before-Action的设计和三阶段训练，模型在通用导航任务中展现出优越的推理能力和性能。

Abstract: Embodied navigation stands as a foundation pillar within the broader pursuit
of embodied AI. However, previous navigation research is divided into different
tasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task
objectives and modalities, making datasets and methods are designed
individually. In this work, we take steps toward generalist navigation agents,
which can follow free-form instructions that include arbitrary compounds of
multi-modal and multi-capability. To achieve this, we propose a large-scale
benchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1.
Specifically, OctoNav-Bench features continuous environments and is constructed
via a designed annotation pipeline. We thoroughly craft instruction-trajectory
pairs, where instructions are diverse in free-form with arbitrary modality and
capability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within
OctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1,
we build it upon MLLMs and adapt it to a VLA-type model, which can produce
low-level actions solely based on 2D visual observations. Moreover, we design a
Hybrid Training Paradigm (HTP) that consists of three stages, i.e.,
Action-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains
specifically designed learning policies and rewards. Importantly, for TBA-SFT
and Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which
show impressive reasoning ability via thinking-before-answer. Thus, we aim to
investigate how to achieve thinking-before-action in the embodied navigation
field, to improve model's reasoning ability toward generalists. Specifically,
we propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a
cold-start phrase and then leverage Nav-GPRO to improve its thinking ability.
Finally, OctoNav-R1 shows superior performance compared with previous methods.

</details>


### [269] [Learning to Align: Addressing Character Frequency Distribution Shifts in Handwritten Text Recognition](https://arxiv.org/abs/2506.09846)
*Panagiotis Kaliosis,John Pavlopoulos*

Key words: 手写文本识别, Wasserstein距离, 字符频率分布, 损失函数, 引导解码

TL;DR: 提出一种新的损失函数，结合Wasserstein距离来优化手写文本识别模型，提升其在时间和上下文变化下的准确性和鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 手写文本识别因手写风格的多变性和语境依赖性而具有挑战性，传统模型在特定子集上表现不佳。

Method: 提出一种新的损失函数，利用Wasserstein距离惩罚预测文本与目标字符频率分布的差异。

Result: 实验证明该方法提高了模型的泛化能力和性能。

Conclusion: 通过字符分布对齐可以显著提升手写文本识别的效果。

Abstract: Handwritten text recognition aims to convert visual input into
machine-readable text, and it remains challenging due to the evolving and
context-dependent nature of handwriting. Character sets change over time, and
character frequency distributions shift across historical periods or regions,
often causing models trained on broad, heterogeneous corpora to underperform on
specific subsets. To tackle this, we propose a novel loss function that
incorporates the Wasserstein distance between the character frequency
distribution of the predicted text and a target distribution empirically
derived from training data. By penalizing divergence from expected
distributions, our approach enhances both accuracy and robustness under
temporal and contextual intra-dataset shifts. Furthermore, we demonstrate that
character distribution alignment can also improve existing models at inference
time without requiring retraining by integrating it as a scoring function in a
guided decoding scheme. Experimental results across multiple datasets and
architectures confirm the effectiveness of our method in boosting
generalization and performance. We open source our code at
https://github.com/pkaliosis/fada.

</details>


### [270] [3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation](https://arxiv.org/abs/2506.09883)
*Seonho Lee,Jiho Choi,Inha Kang,Jiwook Kim,Junsung Park,Hyunjung Shim*

Key words: 视觉语言模型,3D空间理解,几何蒸馏,轻量级框架,多模态任务

TL;DR: 通过几何蒸馏框架，将人类启发的几何线索注入预训练的视觉语言模型，提升其3D空间理解能力。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前视觉语言模型（VLM）在3D空间结构理解方面存在局限，需要一种轻量级且无需标注的方法来增强其几何感知能力。

Method: 提出几何蒸馏框架，从现成的3D基础模型（如MASt3R、VGGT）中提取稀疏对应、相对深度关系和密集成本体积，注入VLM中。

Result: 在3D视觉语言推理和3D感知基准测试中表现优异，显著提升了3D空间推理能力且计算成本更低。

Conclusion: 该方法为2D训练的VLM与3D理解之间的桥梁提供了一条可扩展且高效的路径。

Abstract: Vision-Language Models (VLMs) have shown remarkable performance on diverse
visual and linguistic tasks, yet they remain fundamentally limited in their
understanding of 3D spatial structures. We propose Geometric Distillation, a
lightweight, annotation-free fine-tuning framework that injects human-inspired
geometric cues into pretrained VLMs without modifying their architecture. By
distilling (1) sparse correspondences, (2) relative depth relations, and (3)
dense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R,
VGGT), our method shapes representations to be geometry-aware while remaining
compatible with natural image-text inputs. Through extensive evaluations on 3D
vision-language reasoning and 3D perception benchmarks, our method consistently
outperforms prior approaches, achieving improved 3D spatial reasoning with
significantly lower computational cost. Our work demonstrates a scalable and
efficient path to bridge 2D-trained VLMs with 3D understanding, opening up
wider use in spatially grounded multimodal tasks.

</details>


### [271] [HadaNorm: Diffusion Transformer Quantization through Mean-Centered Transformations](https://arxiv.org/abs/2506.09932)
*Marco Federici,Riccardo Del Chiaro,Boris van Breugel,Paul Whatmough,Markus Nagel*

Key words: 扩散模型,后训练量化,异常值,激活量化,Hadamard变换

TL;DR: 论文提出HadaNorm，一种新型线性变换方法，通过归一化激活特征通道并结合Hadamard变换，有效减少量化误差，适用于资源受限设备的扩散模型部署。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 扩散模型在图像生成领域处于前沿，但其高内存和计算需求限制了在资源受限设备上的应用。后训练量化（PTQ）虽是一种解决方案，但标准PTQ方法对异常值处理不佳，且高压缩需要额外的权重和激活变换。

Method: 提出HadaNorm方法，结合归一化和Hadamard变换，对激活通道进行预处理，以实现更激进的激活量化。

Result: HadaNorm在Transformer块的各组件上一致减少量化误差，相比现有方法，实现了更优的效率与性能平衡。

Conclusion: HadaNorm通过创新的线性变换方法，解决了PTQ中的异常值问题，提升了量化效果，适合资源受限场景。

Abstract: Diffusion models represent the cutting edge in image generation, but their
high memory and computational demands hinder deployment on resource-constrained
devices. Post-Training Quantization (PTQ) offers a promising solution by
reducing the bitwidth of matrix operations. However, standard PTQ methods
struggle with outliers, and achieving higher compression often requires
transforming model weights and activations before quantization. In this work,
we propose HadaNorm, a novel linear transformation that extends existing
approaches and effectively mitigates outliers by normalizing activations
feature channels before applying Hadamard transformations, enabling more
aggressive activation quantization. We demonstrate that HadaNorm consistently
reduces quantization error across the various components of transformer blocks,
achieving superior efficiency-performance trade-offs when compared to
state-of-the-art methods.

</details>


### [272] [CausalVQA: A Physically Grounded Causal Reasoning Benchmark for Video Models](https://arxiv.org/abs/2506.09943)
*Aaron Foss,Chloe Evans,Sasha Mitts,Koustuv Sinha,Ammar Rizvi,Justine T. Kao*

Key words: CausalVQA, 视频问答, 因果关系, 物理推理, 多模态模型

TL;DR: CausalVQA是一个用于视频问答（VQA）的基准数据集，旨在测试模型对物理世界中因果关系的理解能力。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的VQA数据集通常侧重于对现实世界视频的表面感知理解或基于模拟环境的狭窄物理推理问题，而CausalVQA填补了这一空白。

Method: CausalVQA通过五种问题类型（反事实、假设、预期、规划和描述性）设计问题，并通过质量控制机制确保模型基于深层次视觉理解而非语言线索回答问题。

Result: 当前前沿的多模态模型在CausalVQA上的表现远低于人类，尤其是在预期和假设问题上。

Conclusion: 研究突显了当前系统在时空推理、物理原理理解以及替代方案理解方面的不足。

Abstract: We introduce CausalVQA, a benchmark dataset for video question answering
(VQA) composed of question-answer pairs that probe models' understanding of
causality in the physical world. Existing VQA benchmarks either tend to focus
on surface perceptual understanding of real-world videos, or on narrow physical
reasoning questions created using simulation environments. CausalVQA fills an
important gap by presenting challenging questions that are grounded in
real-world scenarios, while focusing on models' ability to predict the likely
outcomes of different actions and events through five question types:
counterfactual, hypothetical, anticipation, planning and descriptive. We
designed quality control mechanisms that prevent models from exploiting trivial
shortcuts, requiring models to base their answers on deep visual understanding
instead of linguistic cues. We find that current frontier multimodal models
fall substantially below human performance on the benchmark, especially on
anticipation and hypothetical questions. This highlights a challenge for
current systems to leverage spatial-temporal reasoning, understanding of
physical principles, and comprehension of possible alternatives to make
accurate predictions in real-world settings.

</details>


### [273] [UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting](https://arxiv.org/abs/2506.09952)
*Ziyi Wang,Yanran Zhang,Jie Zhou,Jiwen Lu*

Key words: 点云,预训练,3D视觉,高斯渲染,统一模型

TL;DR: UniPre3D是一种统一的三维点云预训练方法，适用于任何尺度的点云和任何架构的3D模型，通过高斯基元预测和可微分高斯渲染实现精确监督。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决点云数据尺度多样性带来的挑战，提出首个适用于不同尺度点云的统一预训练方法。

Method: 采用高斯基元预测作为预训练任务，结合可微分高斯渲染和2D预训练特征，实现端到端优化和纹理知识融合。

Result: 在多种对象和场景级任务中验证了方法的普遍有效性。

Conclusion: UniPre3D为点云表示学习提供了一种统一且高效的预训练方案。

Abstract: The scale diversity of point cloud data presents significant challenges in
developing unified representation learning techniques for 3D vision. Currently,
there are few unified 3D models, and no existing pre-training method is equally
effective for both object- and scene-level point clouds. In this paper, we
introduce UniPre3D, the first unified pre-training method that can be
seamlessly applied to point clouds of any scale and 3D models of any
architecture. Our approach predicts Gaussian primitives as the pre-training
task and employs differentiable Gaussian splatting to render images, enabling
precise pixel-level supervision and end-to-end optimization. To further
regulate the complexity of the pre-training task and direct the model's focus
toward geometric structures, we integrate 2D features from pre-trained image
models to incorporate well-established texture knowledge. We validate the
universal effectiveness of our proposed method through extensive experiments
across a variety of object- and scene-level tasks, using diverse point cloud
models as backbones. Code is available at https://github.com/wangzy22/UniPre3D.

</details>


### [274] [Vision Generalist Model: A Survey](https://arxiv.org/abs/2506.09954)
*Ziyi Wang,Yongming Rao,Shuofeng Sun,Xinrun Liu,Yi Wei,Xumin Yu,Zuyan Liu,Yanbo Wang,Hongmin Liu,Jie Zhou,Jiwen Lu*

Key words: 视觉通用模型，计算机视觉，通用框架，多任务处理

TL;DR: 本文综述了视觉通用模型的特点、能力及其设计框架，并探讨了相关领域的互连与潜在协同作用，最后提出了实际应用场景和未来研究方向。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 通用模型在自然语言处理中的成功激发了将其应用于计算机视觉任务的兴趣，但视觉任务的输入输出多样性使其难以统一表示。

Method: 综述了视觉通用模型的背景、数据集、任务和基准，分析了现有研究的框架设计及性能提升技术，并探讨了相关领域的协同作用。

Result: 总结了视觉通用模型的实际应用场景，并识别了当前面临的挑战。

Conclusion: 提出了未来研究的方向，以进一步推动视觉通用模型的发展。

Abstract: Recently, we have witnessed the great success of the generalist model in
natural language processing. The generalist model is a general framework
trained with massive data and is able to process various downstream tasks
simultaneously. Encouraged by their impressive performance, an increasing
number of researchers are venturing into the realm of applying these models to
computer vision tasks. However, the inputs and outputs of vision tasks are more
diverse, and it is difficult to summarize them as a unified representation. In
this paper, we provide a comprehensive overview of the vision generalist
models, delving into their characteristics and capabilities within the field.
First, we review the background, including the datasets, tasks, and benchmarks.
Then, we dig into the design of frameworks that have been proposed in existing
research, while also introducing the techniques employed to enhance their
performance. To better help the researchers comprehend the area, we take a
brief excursion into related domains, shedding light on their interconnections
and potential synergies. To conclude, we provide some real-world application
scenarios, undertake a thorough examination of the persistent challenges, and
offer insights into possible directions for future research endeavors.

</details>


### [275] [Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing](https://arxiv.org/abs/2506.09965)
*Junfei Wu,Jian Guan,Kaituo Feng,Qiang Liu,Shu Wu,Liang Wang,Wei Wu,Tieniu Tan*

Key words: 多模态推理,视觉语言模型,空间推理,绘图操作,三阶段训练

TL;DR: 该论文提出了一种新的多模态推理范式，通过视觉空间中的基本绘图操作增强大型视觉语言模型的空间推理能力，显著提高了性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有方法在多模态推理中主要依赖文本，导致在需要精确几何理解和空间跟踪的任务中存在局限。研究旨在通过视觉绘图操作解决这一问题。

Method: 提出‘drawing to reason in space’范式，赋予模型基本绘图操作能力。采用三阶段训练框架：合成数据冷启动训练、自反射拒绝采样和强化学习。

Result: 实验表明，模型VILASR在多种空间推理任务中平均提升18.4%，性能显著优于现有方法。

Conclusion: 该研究通过视觉绘图操作成功提升了模型的空间推理能力，为多模态推理提供了新方向。

Abstract: As textual reasoning with large language models (LLMs) has advanced
significantly, there has been growing interest in enhancing the multimodal
reasoning capabilities of large vision-language models (LVLMs). However,
existing methods primarily approach multimodal reasoning in a straightforward,
text-centric manner, where both reasoning and answer derivation are conducted
purely through text, with the only difference being the presence of multimodal
input. As a result, these methods often encounter fundamental limitations in
spatial reasoning tasks that demand precise geometric understanding and
continuous spatial tracking-capabilities that humans achieve through mental
visualization and manipulation. To address the limitations, we propose drawing
to reason in space, a novel paradigm that enables LVLMs to reason through
elementary drawing operations in the visual space. By equipping models with
basic drawing operations, including annotating bounding boxes and drawing
auxiliary lines, we empower them to express and analyze spatial relationships
through direct visual manipulation, meanwhile avoiding the performance ceiling
imposed by specialized perception tools in previous tool-integrated reasoning
approaches. To cultivate this capability, we develop a three-stage training
framework: cold-start training with synthetic data to establish basic drawing
abilities, reflective rejection sampling to enhance self-reflection behaviors,
and reinforcement learning to directly optimize for target rewards. Extensive
experiments demonstrate that our model, named VILASR, consistently outperforms
existing methods across diverse spatial reasoning benchmarks, involving maze
navigation, static spatial reasoning, video-based reasoning, and
multi-view-based reasoning tasks, with an average improvement of 18.4%.

</details>


### [276] [InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions](https://arxiv.org/abs/2506.09984)
*Zhenzhi Wang,Jiaqi Yang,Jianwen Jiang,Chao Liang,Gaojie Lin,Zerong Zheng,Ceyuan Yang,Dahua Lin*

Key words: 多模态条件、多概念视频、布局控制、区域绑定、人类动画

TL;DR: 本文提出了一种新框架，用于在多模态条件下生成多概念人类中心视频，通过区域特定的条件绑定实现精确控制。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有方法通常只能对单一体进行全局条件注入，无法处理多概念视频中的复杂交互，限制了应用场景。

Method: 引入强区域绑定条件的方法，利用掩码预测器自动推断布局信息，并通过迭代方式将局部音频条件注入对应区域。

Result: 实验和消融研究证明了该方法在多模态条件下的显式布局控制优于隐式方法和其他现有方法。

Conclusion: 该框架为多概念视频生成提供了高质量、可控的解决方案。

Abstract: End-to-end human animation with rich multi-modal conditions, e.g., text,
image and audio has achieved remarkable advancements in recent years. However,
most existing methods could only animate a single subject and inject conditions
in a global manner, ignoring scenarios that multiple concepts could appears in
the same video with rich human-human interactions and human-object
interactions. Such global assumption prevents precise and per-identity control
of multiple concepts including humans and objects, therefore hinders
applications. In this work, we discard the single-entity assumption and
introduce a novel framework that enforces strong, region-specific binding of
conditions from modalities to each identity's spatiotemporal footprint. Given
reference images of multiple concepts, our method could automatically infer
layout information by leveraging a mask predictor to match appearance cues
between the denoised video and each reference appearance. Furthermore, we
inject local audio condition into its corresponding region to ensure
layout-aligned modality matching in a iterative manner. This design enables the
high-quality generation of controllable multi-concept human-centric videos.
Empirical results and ablation studies validate the effectiveness of our
explicit layout control for multi-modal conditions compared to implicit
counterparts and other existing methods.

</details>


### [277] [HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios](https://arxiv.org/abs/2506.09650)
*Kunyu Peng,Junchao Huang,Xiangsheng Huang,Di Wen,Junwei Zheng,Yufan Chen,Kailun Yang,Jiamin Wu,Chongqing Hao,Rainer Stiefelhagen*

Key words: 动作分割、多人物场景、文本参考、xLSTM、傅里叶条件

TL;DR: 该论文提出了一种多人物场景下的文本参考引导的动作分割方法，并介绍了一个包含137种细粒度动作的数据集RHAS133。通过提出的HopaDIFF框架，结合xLSTM和傅里叶条件，实现了最先进的性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有动作分割方法主要针对单人物固定动作序列，忽视了多人物场景。本文旨在填补这一空白，通过文本描述指定目标人物进行分割。

Method: 提出HopaDIFF框架，结合新颖的跨输入门注意力xLSTM和傅里叶条件，增强整体-部分的长期推理能力和动作分割生成的控制能力。

Result: HopaDIFF在RHAS133数据集上取得了最先进的结果，表现出优于现有方法的性能。

Conclusion: HopaDIFF为多人物动作分割提供了有效的解决方案，并通过实验验证了其优越性。

Abstract: Action segmentation is a core challenge in high-level video understanding,
aiming to partition untrimmed videos into segments and assign each a label from
a predefined action set. Existing methods primarily address single-person
activities with fixed action sequences, overlooking multi-person scenarios. In
this work, we pioneer textual reference-guided human action segmentation in
multi-person settings, where a textual description specifies the target person
for segmentation. We introduce the first dataset for Referring Human Action
Segmentation, i.e., RHAS133, built from 133 movies and annotated with 137
fine-grained actions with 33h video data, together with textual descriptions
for this new task. Benchmarking existing action recognition methods on RHAS133
using VLM-based feature extractors reveals limited performance and poor
aggregation of visual cues for the target person. To address this, we propose a
holistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF,
leveraging a novel cross-input gate attentional xLSTM to enhance
holistic-partial long-range reasoning and a novel Fourier condition to
introduce more fine-grained control to improve the action segmentation
generation. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse
evaluation settings. The code is available at
https://github.com/KPeng9510/HopaDIFF.git.

</details>


### [278] [EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits](https://arxiv.org/abs/2506.09988)
*Ron Yosef,Moran Yanuka,Yonatan Bitton,Dani Lischinski*

Key words: 文本引导图像编辑, 评估基准, 生成式AI, 伪影检测, 差异描述

TL;DR: 论文提出EditInspector基准，用于评估文本引导的图像编辑质量，并发现现有模型在此任务上的不足，提出了两种新方法以提升性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 随着生成式AI的发展，文本引导的图像编辑日益普及，亟需一个全面的框架来验证和评估编辑质量。

Method: 基于人类标注的EditInspector基准，评估SoTA模型在多个维度的表现，并提出两种新方法以改进检测和描述编辑变化的能力。

Result: 现有模型在综合评估编辑质量和描述变化时表现不佳，提出的新方法在伪影检测和差异描述生成上优于SoTA模型。

Conclusion: EditInspector为文本引导编辑评估提供了新基准，新方法显著提升了评估准确性，为未来研究指明了方向。

Abstract: Text-guided image editing, fueled by recent advancements in generative AI, is
becoming increasingly widespread. This trend highlights the need for a
comprehensive framework to verify text-guided edits and assess their quality.
To address this need, we introduce EditInspector, a novel benchmark for
evaluation of text-guided image edits, based on human annotations collected
using an extensive template for edit verification. We leverage EditInspector to
evaluate the performance of state-of-the-art (SoTA) vision and language models
in assessing edits across various dimensions, including accuracy, artifact
detection, visual quality, seamless integration with the image scene, adherence
to common sense, and the ability to describe edit-induced changes. Our findings
indicate that current models struggle to evaluate edits comprehensively and
frequently hallucinate when describing the changes. To address these
challenges, we propose two novel methods that outperform SoTA models in both
artifact detection and difference caption generation.

</details>


### [279] [Text-Aware Image Restoration with Diffusion Models](https://arxiv.org/abs/2506.09993)
*Jaewon Min,Jin Hyeon Kim,Paul Hyunbin Cho,Jaeeun Lee,Jihye Park,Minkyu Park,Sangpil Kim,Hyunhee Park,Seungryong Kim*

Key words: 图像修复,文本感知,扩散模型,文本幻觉,多任务框架

TL;DR: 提出了文本感知图像修复（TAIR）任务，并开发了多任务扩散框架TeReDiff，显著提升了文本区域的修复效果。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有扩散修复方法在文本区域修复中存在幻觉现象，即生成看似合理但错误的文本模式。

Method: 提出TAIR任务并构建SA-Text基准数据集，开发了TeReDiff框架，整合扩散模型和文本检测模块。

Result: 实验表明，TeReDiff在文本识别准确性上显著优于现有方法。

Conclusion: TAIR任务和TeReDiff框架在文本区域的修复中取得了显著进展。

Abstract: Image restoration aims to recover degraded images. However, existing
diffusion-based restoration methods, despite great success in natural image
restoration, often struggle to faithfully reconstruct textual regions in
degraded images. Those methods frequently generate plausible but incorrect
text-like patterns, a phenomenon we refer to as text-image hallucination. In
this paper, we introduce Text-Aware Image Restoration (TAIR), a novel
restoration task that requires the simultaneous recovery of visual contents and
textual fidelity. To tackle this task, we present SA-Text, a large-scale
benchmark of 100K high-quality scene images densely annotated with diverse and
complex text instances. Furthermore, we propose a multi-task diffusion
framework, called TeReDiff, that integrates internal features from diffusion
models into a text-spotting module, enabling both components to benefit from
joint training. This allows for the extraction of rich text representations,
which are utilized as prompts in subsequent denoising steps. Extensive
experiments demonstrate that our approach consistently outperforms
state-of-the-art restoration methods, achieving significant gains in text
recognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/

</details>


### [280] [CINeMA: Conditional Implicit Neural Multi-Modal Atlas for a Spatio-Temporal Representation of the Perinatal Brain](https://arxiv.org/abs/2506.09668)
*Maik Dannecker,Vasiliki Sideri-Lampretsa,Sophie Starck,Angeline Mihailov,Mathieu Milh,Nadine Girard,Guillaume Auzias,Daniel Rueckert*

Key words: 脑图谱, 多模态, 隐式神经表示, 数据稀缺, CINeMA

TL;DR: 提出了一种名为CINeMA的新框架，用于在数据稀缺情况下构建高分辨率、多模态的胎儿和新生儿脑图谱。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 研究胎儿和新生儿大脑的快速神经发育需要高时空分辨率的脑图谱，但传统方法和深度学习需要大量数据，而病理情况下数据稀缺。

Method: CINeMA通过隐式神经表示在潜在空间中操作，避免计算密集型图像配准，并支持基于解剖特征的灵活条件建模。

Result: CINeMA在准确性、效率和多功能性上超越现有方法，支持组织分割、年龄预测等下游任务，并能生成合成数据。

Conclusion: CINeMA是一个强大的工具，可推进大脑研究，尤其在数据稀缺情况下表现突出。

Abstract: Magnetic resonance imaging of fetal and neonatal brains reveals rapid
neurodevelopment marked by substantial anatomical changes unfolding within
days. Studying this critical stage of the developing human brain, therefore,
requires accurate brain models-referred to as atlases-of high spatial and
temporal resolution. To meet these demands, established traditional atlases and
recently proposed deep learning-based methods rely on large and comprehensive
datasets. This poses a major challenge for studying brains in the presence of
pathologies for which data remains scarce. We address this limitation with
CINeMA (Conditional Implicit Neural Multi-Modal Atlas), a novel framework for
creating high-resolution, spatio-temporal, multimodal brain atlases, suitable
for low-data settings. Unlike established methods, CINeMA operates in latent
space, avoiding compute-intensive image registration and reducing atlas
construction times from days to minutes. Furthermore, it enables flexible
conditioning on anatomical features including GA, birth age, and pathologies
like ventriculomegaly (VM) and agenesis of the corpus callosum (ACC). CINeMA
supports downstream tasks such as tissue segmentation and age prediction
whereas its generative properties enable synthetic data creation and
anatomically informed data augmentation. Surpassing state-of-the-art methods in
accuracy, efficiency, and versatility, CINeMA represents a powerful tool for
advancing brain research. We release the code and atlases at
https://github.com/m-dannecker/CINeMA.

</details>


### [281] [Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy](https://arxiv.org/abs/2506.09958)
*Sushant Gautam,Michael A. Riegler,Pål Halvorsen*

Key words: 医学视觉问答, 数据集, 胃肠道内窥镜, 大语言模型, 视觉增强

TL;DR: Kvasir-VQA-x1是一个新的、大规模的胃肠道内窥镜数据集，旨在提升医学视觉问答（MedVQA）领域的临床复杂性和视觉多样性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有数据集缺乏临床复杂性和视觉多样性，限制了MedVQA的进展。

Method: 通过大语言模型生成159,549个问题-答案对，并引入视觉增强模拟成像伪影。

Result: 数据集支持标准VQA性能和模型鲁棒性评估，提升了临床相关性和挑战性。

Conclusion: Kvasir-VQA-x1为开发更可靠的临床多模态AI系统提供了宝贵资源。

Abstract: Medical Visual Question Answering (MedVQA) is a promising field for
developing clinical decision support systems, yet progress is often limited by
the available datasets, which can lack clinical complexity and visual
diversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,
large-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly
expands upon the original Kvasir-VQA by incorporating 159,549 new
question-answer pairs that are designed to test deeper clinical reasoning. We
developed a systematic method using large language models to generate these
questions, which are stratified by complexity to better assess a model's
inference capabilities. To ensure our dataset prepares models for real-world
clinical scenarios, we have also introduced a variety of visual augmentations
that mimic common imaging artifacts. The dataset is structured to support two
main evaluation tracks: one for standard VQA performance and another to test
model robustness against these visual perturbations. By providing a more
challenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate
the development of more reliable and effective multimodal AI systems for use in
clinical settings. The dataset is fully accessible and adheres to FAIR data
principles, making it a valuable resource for the wider research community.
Code and data: https://github.com/Simula/Kvasir-VQA-x1 and
https://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1

</details>


### [282] [A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs](https://arxiv.org/abs/2506.09987)
*Benno Krojer,Mojtaba Komeili,Candace Ross,Quentin Garrido,Koustuv Sinha,Nicolas Ballas,Mahmoud Assran*

Key words: 视频语言模型, 基准评测, 物理理解, 最小变化对

TL;DR: 该论文提出了MVP基准，用于评估视频语言模型对物理世界的理解能力，旨在避免因表面视觉或文本线索导致的分数膨胀。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的视频语言模型评测基准容易因表面线索导致分数不准确，MVP通过引入最小变化对来解决这一问题。

Method: MVP包含55K个高质量多选视频QA样本，每个样本有一个视觉相似的对照视频和相同问题但相反答案的对。

Result: 人类在MVP上表现达92.9%，最佳开源模型仅40.2%，而随机表现为25%。

Conclusion: MVP有效避免了表面线索的干扰，更准确地评估了模型的物理世界理解能力。

Abstract: Existing benchmarks for assessing the spatio-temporal understanding and
reasoning abilities of video language models are susceptible to score inflation
due to the presence of shortcut solutions based on superficial visual or
textual cues. This paper mitigates the challenges in accurately assessing model
performance by introducing the Minimal Video Pairs (MVP) benchmark, a simple
shortcut-aware video QA benchmark for assessing the physical understanding of
video language models. The benchmark is comprised of 55K high-quality
multiple-choice video QA examples focusing on physical world understanding.
Examples are curated from nine video data sources, spanning first-person
egocentric and exocentric videos, robotic interaction data, and cognitive
science intuitive physics benchmarks. To mitigate shortcut solutions that rely
on superficial visual or textual cues and biases, each sample in MVP has a
minimal-change pair -- a visually similar video accompanied by an identical
question but an opposing answer. To answer a question correctly, a model must
provide correct answers for both examples in the minimal-change pair; as such,
models that solely rely on visual or textual biases would achieve below random
performance. Human performance on MVP is 92.9\%, while the best open-source
state-of-the-art video-language model achieves 40.2\% compared to random
performance at 25\%.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [283] [A Multi-Armed Bandit Framework for Online Optimisation in Green Integrated Terrestrial and Non-Terrestrial Networks](https://arxiv.org/abs/2506.09268)
*Henri Alam,Antonio de Domenico,Tareq Si Salem,Florian Kaltenberger*

Key words: 集成网络,非地面网络,能源效率,在线优化,MAB,BCOMD

TL;DR: 论文提出了一种基于MAB和BCOMD算法的在线优化框架，用于集成TN-NTN架构，以平衡网络容量和能源效率，显著提升了网络性能。

<details>
  <summary>Details</summary>

Main category: cs.NI

Motivation: 随着地面网络密集化问题日益突出，研究非地面网络（NTN）在缓解地面网络负载和实现能源高效运行方面的潜力。

Method: 采用多臂老虎机（MAB）框架和BCOMD算法，实时优化带宽分配、用户设备关联和宏基站关闭等关键系统参数。

Result: 在24小时系统级仿真中，框架显著减少高峰时段未满足需求的用户比例，在低流量时段实现19%的吞吐量提升和5%的能源节省。

Conclusion: 提出的框架优于遵循3GPP标准的传统网络设置，展示了NTN在支持可持续网络中的潜力。

Abstract: Integrated terrestrial and non-terrestrial network (TN-NTN) architectures
offer a promising solution for expanding coverage and improving capacity for
the network. While non-terrestrial networks (NTNs) are primarily exploited for
these specific reasons, their role in alleviating terrestrial network (TN) load
and enabling energy-efficient operation has received comparatively less
attention. In light of growing concerns associated with the densification of
terrestrial deployments, this work aims to explore the potential of NTNs in
supporting a more sustainable network. In this paper, we propose a novel online
optimisation framework for integrated TN-NTN architectures, built on a
multi-armed bandit (MAB) formulation and leveraging the Bandit-feedback
Constrained Online Mirror Descent (BCOMD) algorithm. Our approach adaptively
optimises key system parameters--including bandwidth allocation, user equipment
(UE) association, and macro base station (MBS) shutdown--to balance network
capacity and energy efficiency in real time. Extensive system-level simulations
over a 24-hour period show that our framework significantly reduces the
proportion of unsatisfied UEs during peak hours and achieves up to 19%
throughput gains and 5% energy savings in low-traffic periods, outperforming
standard network settings following 3GPP recommendations.

</details>


### [284] [Real-Time Network Traffic Forecasting with Missing Data: A Generative Model Approach](https://arxiv.org/abs/2506.09647)
*Lei Deng,Wenhan Xu,Jingwei Li,Danny H. K. Tsang*

Key words: 网络流量预测,张量完成,生成模型,低秩结构,实时性

TL;DR: 提出一种生成模型方法，用于实时网络流量预测，尤其适用于数据缺失的情况，通过张量完成和预训练生成模型实现低秩结构优化，实验效果显著。

<details>
  <summary>Details</summary>

Main category: cs.NI

Motivation: 解决实际网络流量数据因人为或自然因素导致的不完整性问题，实时预测流量对网络管理至关重要。

Method: 将流量预测任务建模为张量完成问题，利用预训练生成模型捕捉数据的低秩结构，优化潜在表示而非高维张量。

Result: 在Abilene数据集上验证，预测误差(MAE)低于0.002，实时性达100毫秒以内。

Conclusion: 该方法在数据缺失情况下仍能高效完成流量预测，具有理论和实践价值。

Abstract: Real-time network traffic forecasting is crucial for network management and
early resource allocation. Existing network traffic forecasting approaches
operate under the assumption that the network traffic data is fully observed.
However, in practical scenarios, the collected data are often incomplete due to
various human and natural factors. In this paper, we propose a generative model
approach for real-time network traffic forecasting with missing data. Firstly,
we model the network traffic forecasting task as a tensor completion problem.
Secondly, we incorporate a pre-trained generative model to achieve the low-rank
structure commonly associated with tensor completion. The generative model
effectively captures the intrinsic low-rank structure of network traffic data
during pre-training and enables the mapping from a compact latent
representation to the tensor space. Thirdly, rather than directly optimizing
the high-dimensional tensor, we optimize its latent representation, which
simplifies the optimization process and enables real-time forecasting. We also
establish a theoretical recovery guarantee that quantifies the error bound of
the proposed approach. Experiments on real-world datasets demonstrate that our
approach achieves accurate network traffic forecasting within 100 ms, with a
mean absolute error (MAE) below 0.002, as validated on the Abilene dataset.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [285] [Devanagari Digit Recognition using Quantum Machine Learning](https://arxiv.org/abs/2506.09069)
*Sahaj Raj Malla*

Key words: Devanagari手写数字识别, 混合量子-经典架构, CNN, 变分量子电路, 量子机器学习

TL;DR: 提出了一种混合量子-经典架构，用于Devanagari手写数字识别，结合CNN和量子电路，在低资源语言环境中取得了卓越的准确性。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 解决Devanagari手写数字识别的复杂性和数据稀缺问题，推动多语言文档数字化和文化遗产保护。

Method: 使用CNN提取空间特征，搭配10-qubit变分量子电路进行量子增强分类。

Result: 在DHCD数据集上实现了99.80%的测试准确率，优于传统CNN，参数更少且更具鲁棒性。

Conclusion: 该模型为区域脚本识别设定了新标准，展示了量子机器学习在低资源语言环境中的潜力。

Abstract: Handwritten digit recognition in regional scripts, such as Devanagari, is
crucial for multilingual document digitization, educational tools, and the
preservation of cultural heritage. The script's complex structure and limited
annotated datasets pose significant challenges to conventional models. This
paper introduces the first hybrid quantum-classical architecture for Devanagari
handwritten digit recognition, combining a convolutional neural network (CNN)
for spatial feature extraction with a 10-qubit variational quantum circuit
(VQC) for quantum-enhanced classification. Trained and evaluated on the
Devanagari Handwritten Character Dataset (DHCD), the proposed model achieves a
state-of-the-art test accuracy for quantum implementation of 99.80% and a test
loss of 0.2893, with an average per-class F1-score of 0.9980. Compared to
equivalent classical CNNs, our model demonstrates superior accuracy with
significantly fewer parameters and enhanced robustness. By leveraging quantum
principles such as superposition and entanglement, this work establishes a
novel benchmark for regional script recognition, highlighting the promise of
quantum machine learning (QML) in real-world, low-resource language settings.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [286] [Automatic Treatment Planning using Reinforcement Learning for High-dose-rate Prostate Brachytherapy](https://arxiv.org/abs/2506.09805)
*Tonghe Wang,Yining Feng,Xiaofeng Yang*

Key words: 强化学习、高剂量率、前列腺近距离放射治疗、针位优化、自动化计划

TL;DR: 该研究探讨了使用强化学习（RL）在高剂量率前列腺近距离放射治疗中自动生成针位和停留时间的可行性，以减少手术时间并保证计划质量。

<details>
  <summary>Details</summary>

Main category: physics.med-ph

Motivation: 目前针位放置仅依赖医生经验，存在主观性和时间成本高的问题，RL方法有望优化这一过程。

Method: 训练RL代理逐步调整针位和停留时间以最大化奖励函数，使用11例患者数据（1例训练，10例测试）。

Result: RL计划与临床计划在前列腺覆盖和直肠剂量上无显著差异，但在减少前列腺热点和尿道剂量上表现更优，且平均少用2根针。

Conclusion: RL方法首次证明能生成临床实用的近距离放射治疗计划，质量相当或更好，且更具标准化潜力。

Abstract: Purpose: In high-dose-rate (HDR) prostate brachytherapy procedures, the
pattern of needle placement solely relies on physician experience. We
investigated the feasibility of using reinforcement learning (RL) to provide
needle positions and dwell times based on patient anatomy during pre-planning
stage. This approach would reduce procedure time and ensure consistent plan
quality. Materials and Methods: We train a RL agent to adjust the position of
one selected needle and all the dwell times on it to maximize a pre-defined
reward function after observing the environment. After adjusting, the RL agent
then moves on to the next needle, until all needles are adjusted. Multiple
rounds are played by the agent until the maximum number of rounds is reached.
Plan data from 11 prostate HDR boost patients (1 for training, and 10 for
testing) treated in our clinic were included in this study. The dosimetric
metrics and the number of used needles of RL plan were compared to those of the
clinical results (ground truth). Results: On average, RL plans and clinical
plans have very similar prostate coverage (Prostate V100) and Rectum D2cc (no
statistical significance), while RL plans have less prostate hotspot (Prostate
V150) and Urethra D20% plans with statistical significance. Moreover, RL plans
use 2 less needles than clinical plan on average. Conclusion: We present the
first study demonstrating the feasibility of using reinforcement learning to
autonomously generate clinically practical HDR prostate brachytherapy plans.
This RL-based method achieved equal or improved plan quality compared to
conventional clinical approaches while requiring fewer needles. With minimal
data requirements and strong generalizability, this approach has substantial
potential to standardize brachytherapy planning, reduce clinical variability,
and enhance patient outcomes.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [287] [A theoretical basis for model collapse in recursive training](https://arxiv.org/abs/2506.09401)
*Vivek Shripad Borkar*

Key words: 递归训练, 生成模型, 概率分布崩溃, 外部样本

TL;DR: 递归训练生成模型可能导致概率分布‘崩溃’，而外部样本的影响会导致两种不同的渐进行为。

<details>
  <summary>Details</summary>

Main category: math.PR

Motivation: 研究递归训练生成模型时概率分布的崩溃现象及其受外部样本影响的行为差异。

Method: 通过分析递归训练生成模型的过程，观察有无外部样本输入时的不同渐进行为。

Result: 发现外部样本的存在与否会导致两种不同的渐进行为。

Conclusion: 外部样本的贡献对递归训练生成模型的渐进行为有显著影响。

Abstract: It is known that recursive training from generative models can lead to the so
called `collapse' of the simulated probability distribution. This note shows
that one in fact gets two different asymptotic behaviours depending on whether
an external source, howsoever minor, is also contributing samples.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [288] [Alice and the Caterpillar: A more descriptive null model for assessing data mining results](https://arxiv.org/abs/2506.09764)
*Giulia Preti,Gianmarco De Francisci Morales,Matteo Riondato*

Key words: 零模型, 统计假设检验, Bipartite Joint Degree Matrix, 马尔可夫链蒙特卡洛

TL;DR: 本文提出了一种新的零模型方法，用于评估二元交易和序列数据集，通过统计假设检验保留更多原始数据集特性。

<details>
  <summary>Details</summary>

Main category: cs.SI

Motivation: 现有零模型在保留数据集特性方面不足，本文旨在提出更能保持数据集属性的新模型。

Method: 使用马尔可夫链蒙特卡洛算法（Alice）采样，通过定义状态集和高效操作保持Bipartite Joint Degree Matrix等特性。

Result: Alice算法混合速度快、扩展性好，且能发现与现有模型不同的显著结果。

Conclusion: 新零模型在保留数据集特性和检测显著性方面优于现有方法。

Abstract: We introduce novel null models for assessing the results obtained from
observed binary transactional and sequence datasets, using statistical
hypothesis testing. Our null models maintain more properties of the observed
dataset than existing ones. Specifically, they preserve the Bipartite Joint
Degree Matrix of the bipartite (multi-)graph corresponding to the dataset,
which ensures that the number of caterpillars, i.e., paths of length three, is
preserved, in addition to other properties considered by other models. We
describe Alice, a suite of Markov chain Monte Carlo algorithms for sampling
datasets from our null models, based on a carefully defined set of states and
efficient operations to move between them. The results of our experimental
evaluation show that Alice mixes fast and scales well, and that our null model
finds different significant results than ones previously considered in the
literature.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [289] [SimClass: A Classroom Speech Dataset Generated via Game Engine Simulation For Automatic Speech Recognition Research](https://arxiv.org/abs/2506.09206)
*Ahmed Adel Attia,Jing Liu,Carl Espy-Wilson*

Key words: 教室噪声、语音合成、游戏引擎、数据集、语音识别

TL;DR: 论文提出了一种利用游戏引擎合成教室噪声的可扩展方法，并推出了SimClass数据集，包含合成噪声和模拟教室语音数据，用于开发语音识别和增强模型。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 大规模教室语音数据的稀缺阻碍了教育领域AI语音模型的发展，现有公共数据集有限且缺乏专用教室噪声库。

Method: 使用游戏引擎合成教室噪声，并将公开儿童语音库与YouTube讲座视频配对生成模拟教室语音数据。

Result: SimClass数据集在实验中显示出与真实教室语音高度近似，适用于语音识别和增强模型的开发。

Conclusion: SimClass为开发鲁棒语音模型提供了有价值的资源，且方法可扩展至其他领域。

Abstract: The scarcity of large-scale classroom speech data has hindered the
development of AI-driven speech models for education. Public classroom datasets
remain limited, and the lack of a dedicated classroom noise corpus prevents the
use of standard data augmentation techniques.
  In this paper, we introduce a scalable methodology for synthesizing classroom
noise using game engines, a framework that extends to other domains. Using this
methodology, we present SimClass, a dataset that includes both a synthesized
classroom noise corpus and a simulated classroom speech dataset. The speech
data is generated by pairing a public children's speech corpus with YouTube
lecture videos to approximate real classroom interactions in clean conditions.
Our experiments on clean and noisy speech demonstrate that SimClass closely
approximates real classroom speech, making it a valuable resource for
developing robust speech recognition and enhancement models.

</details>


### [290] [OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary](https://arxiv.org/abs/2506.09448)
*Yui Sudo,Yusuke Fujita,Atsushi Kojima,Tomoya Mizumoto,Lianbo Liu*

Key words: 语音基础模型, 上下文偏置, 罕见词识别, 自动语音识别

TL;DR: 本文提出了一种将上下文偏置（CB）方法与预训练的语音基础模型（如OWSM v3.1）结合的方法，以改善罕见词和未见词的识别，同时保留预训练模型的优势。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 尽管语音基础模型（SFMs）在自动语音识别（ASR）中表现优异，但对罕见词和未见词的识别仍不理想。上下文偏置（CB）方法虽有望改进这一问题，但传统CB方法因缺乏预训练知识而性能较低。

Method: 将现有的CB方法与预训练的OWSM v3.1模型结合，并冻结其预训练参数，利用SFMs中嵌入的知识实现有效的上下文偏置。

Result: 实验结果表明，该方法在LibriSpeech 100测试集上显著降低了11.6%的偏置词错误率（B-WER），整体WER提高了0.9%，实时因子减少了7.5%。

Conclusion: 通过结合CB方法与预训练SFMs，该方法在保留SFMs优点的同时，显著提升了罕见词和未见词的识别性能。

Abstract: Speech foundation models (SFMs), such as Open Whisper-Style Speech Models
(OWSM), are trained on massive datasets to achieve accurate automatic speech
recognition. However, even SFMs struggle to accurately recognize rare and
unseen words. While contextual biasing (CB) is a promising approach to improve
recognition of such words, most CB methods are trained from scratch, resulting
in lower performance than SFMs due to the lack of pre-trained knowledge. This
paper integrates an existing CB method with OWSM v3.1 while freezing its
pre-trained parameters. By leveraging the knowledge embedded in SFMs, the
proposed method enables effective CB while preserving the advantages of SFMs,
even with a small dataset. Experimental results show that the proposed method
improves the biasing word error rate (B-WER) by 11.6 points, resulting in a 0.9
point improvement in the overall WER while reducing the real-time factor by
7.5% compared to the non-biasing baseline on the LibriSpeech 100 test-clean
set.

</details>


### [291] [BemaGANv2: A Tutorial and Comparative Survey of GAN-based Vocoders for Long-Term Audio Generation](https://arxiv.org/abs/2506.09487)
*Taesoo Park,Mungwi Jeong,Mingyu Park,Narae Kim,Junyoung Kim,Mujung Kim,Jisang Yoo,Hoyun Lee,Sanghoon Kim,Soonchul Kwon*

Key words: BemaGANv2, GAN, 音频生成, AMP模块, MED判别器, 多分辨率判别器

TL;DR: 本文介绍了BemaGANv2的教程式综述与实现指南，这是一种基于GAN的高保真音频生成模型，改进了生成器和判别器架构。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 为了解决高频与长期音频生成的挑战，BemaGANv2在原始架构基础上引入了创新模块。

Method: 生成器使用AMP模块替代ResBlocks，判别器整合了MED与MRD，并通过多种配置进行系统评估。

Result: BemaGANv2在客观和主观评估中表现优异，支持长期依赖性建模。

Conclusion: 该研究为高保真音频生成提供了有效方案，并通过开源促进可重复性。

Abstract: This paper presents a tutorial-style survey and implementation guide of
BemaGANv2, an advanced GAN-based vocoder designed for high-fidelity and
long-term audio generation. Built upon the original BemaGAN architecture,
BemaGANv2 incorporates major architectural innovations by replacing traditional
ResBlocks in the generator with the Anti-aliased Multi-Periodicity composition
(AMP) module, which internally applies the Snake activation function to better
model periodic structures. In the discriminator framework, we integrate the
Multi-Envelope Discriminator (MED), a novel architecture we originally
proposed, to extract rich temporal envelope features crucial for periodicity
detection. Coupled with the Multi-Resolution Discriminator (MRD), this
combination enables more accurate modeling of long-range dependencies in audio.
We systematically evaluate various discriminator configurations, including MSD
+ MED, MSD + MRD, and MPD + MED + MRD, using objective metrics (FAD, SSIM,
PLCC, MCD) and subjective evaluations (MOS, SMOS). This paper also provides a
comprehensive tutorial on the model architecture, training methodology, and
implementation to promote reproducibility. The code and pre-trained models are
available at: https://github.com/dinhoitt/BemaGANv2.

</details>


### [292] [Training-Free Voice Conversion with Factorized Optimal Transport](https://arxiv.org/abs/2506.09709)
*Alexander Lobashev,Assel Yermekova,Maria Larchenko*

Key words: 语音转换, 跨语言, 最优传输, WavLM, kNN-VC

TL;DR: Factorized MKL-VC 是一种无需训练的改进方法，用于 kNN-VC 流程，支持高质量跨语言语音转换，仅需 5 秒参考音频。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 提升跨语言语音转换的效果，尤其是在短参考音频下的内容保留和鲁棒性。

Method: 用因子化的最优传输映射替代 kNN 回归，解决 WavLM 嵌入子空间中的非均匀方差问题。

Result: 在 LibriSpeech 和 FLEURS 数据集上，MKL-VC 显著优于 kNN-VC，且在跨语言任务中表现接近 FACodec。

Conclusion: MKL-VC 在短参考音频下实现了高性能的跨语言语音转换。

Abstract: This paper introduces Factorized MKL-VC, a training-free modification for
kNN-VC pipeline. In contrast with original pipeline, our algorithm performs
high quality any-to-any cross-lingual voice conversion with only 5 second of
reference audio. MKL-VC replaces kNN regression with a factorized optimal
transport map in WavLM embedding subspaces, derived from Monge-Kantorovich
Linear solution. Factorization addresses non-uniform variance across
dimensions, ensuring effective feature transformation. Experiments on
LibriSpeech and FLEURS datasets show MKL-VC significantly improves content
preservation and robustness with short reference audio, outperforming kNN-VC.
MKL-VC achieves performance comparable to FACodec, especially in cross-lingual
voice conversion domain.

</details>


### [293] [Incorporating Linguistic Constraints from External Knowledge Source for Audio-Visual Target Speech Extraction](https://arxiv.org/abs/2506.09792)
*Wenxuan Wu,Shuai Wang,Xixin Wu,Helen Meng,Haizhou Li*

Key words: 视听目标说话人提取,预训练语音语言模型,预训练语言模型,语言约束,多语言

TL;DR: 本文提出了一种利用预训练语音语言模型（PSLM）和预训练语言模型（PLM）的辅助知识改进视听目标说话人提取（AV-TSE）模型的方法。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 受到人类利用语言知识（如语法和语义）辅助语音感知的启发，探索PSLM和PLM作为AV-TSE的辅助知识源。

Method: 将PSLM或PLM的语言约束作为额外的监督信号引入AV-TSE模型。

Result: 在不增加推理计算成本的情况下，显著提高了语音质量和可懂度，并在多语言和视觉线索受损场景中表现出稳健性能。

Conclusion: 利用预训练模型的辅助知识可以有效提升AV-TSE的性能，且无需额外推理成本。

Abstract: Audio-visual target speaker extraction (AV-TSE) models primarily rely on
target visual cues to isolate the target speaker's voice from others. We know
that humans leverage linguistic knowledge, such as syntax and semantics, to
support speech perception. Inspired by this, we explore the potential of
pre-trained speech-language models (PSLMs) and pre-trained language models
(PLMs) as auxiliary knowledge sources for AV-TSE. In this study, we propose
incorporating the linguistic constraints from PSLMs or PLMs for the AV-TSE
model as additional supervision signals. Without introducing any extra
computational cost during inference, the proposed approach consistently
improves speech quality and intelligibility. Furthermore, we evaluate our
method in multi-language settings and visual cue-impaired scenarios and show
robust performance gains.

</details>


### [294] [UmbraTTS: Adapting Text-to-Speech to Environmental Contexts with Flow Matching](https://arxiv.org/abs/2506.09874)
*Neta Glazer,Aviv Navon,Yael Segal,Aviv Shamsian,Hilit Segev,Asaf Buchnick,Menachem Pirchi,Gil Hetz,Joseph Keshet*

Key words: Text-to-Speech, environmental audio, self-supervised learning, flow-matching, audio synthesis

TL;DR: UmbraTTS是一个基于流匹配的TTS模型，能够同时生成语音和环境音频，解决了语音与复杂背景环境结合的挑战。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 尽管TTS技术已能生成高度自然的语音，但将语音与复杂背景环境结合仍具有挑战性。

Method: 通过自监督框架从无标注录音中提取语音、背景音频和文本，训练一个流匹配模型。

Result: UmbraTTS在生成自然、高质量且环境感知的音频方面显著优于现有基线。

Conclusion: UmbraTTS为语音与背景环境的结合提供了一种有效解决方案。

Abstract: Recent advances in Text-to-Speech (TTS) have enabled highly natural speech
synthesis, yet integrating speech with complex background environments remains
challenging. We introduce UmbraTTS, a flow-matching based TTS model that
jointly generates both speech and environmental audio, conditioned on text and
acoustic context. Our model allows fine-grained control over background volume
and produces diverse, coherent, and context-aware audio scenes. A key challenge
is the lack of data with speech and background audio aligned in natural
context. To overcome the lack of paired training data, we propose a
self-supervised framework that extracts speech, background audio, and
transcripts from unannotated recordings. Extensive evaluations demonstrate that
UmbraTTS significantly outperformed existing baselines, producing natural,
high-quality, environmentally aware audios.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [295] [Adversarial Text Generation with Dynamic Contextual Perturbation](https://arxiv.org/abs/2506.09148)
*Hetvi Waghela,Jaydip Sen,Sneha Rakshit,Subhasis Dasgupta*

Key words: 对抗攻击, NLP, 上下文感知, 动态扰动, 语义一致性

TL;DR: 提出了一种名为DCP的动态上下文扰动方案，通过上下文感知的文本扰动提升对抗攻击的隐蔽性和效果。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 现有对抗攻击方法多局限于词级或局部文本改动，缺乏对上下文的理解，导致扰动易检测或语义不一致。

Method: DCP利用预训练语言模型动态生成上下文感知的扰动，并通过对抗目标函数平衡攻击效果与文本自然性。

Result: 实验表明，DCP能更有效地攻击多种NLP模型，同时保持文本的语义一致性和流畅性。

Conclusion: DCP强调了上下文在对抗攻击中的重要性，为构建更鲁棒的NLP系统奠定了基础。

Abstract: Adversarial attacks on Natural Language Processing (NLP) models expose
vulnerabilities by introducing subtle perturbations to input text, often
leading to misclassification while maintaining human readability. Existing
methods typically focus on word-level or local text segment alterations,
overlooking the broader context, which results in detectable or semantically
inconsistent perturbations. We propose a novel adversarial text attack scheme
named Dynamic Contextual Perturbation (DCP). DCP dynamically generates
context-aware perturbations across sentences, paragraphs, and documents,
ensuring semantic fidelity and fluency. Leveraging the capabilities of
pre-trained language models, DCP iteratively refines perturbations through an
adversarial objective function that balances the dual objectives of inducing
model misclassification and preserving the naturalness of the text. This
comprehensive approach allows DCP to produce more sophisticated and effective
adversarial examples that better mimic natural language patterns. Our
experimental results, conducted on various NLP models and datasets, demonstrate
the efficacy of DCP in challenging the robustness of state-of-the-art NLP
systems. By integrating dynamic contextual analysis, DCP significantly enhances
the subtlety and impact of adversarial attacks. This study highlights the
critical role of context in adversarial attacks and lays the groundwork for
creating more robust NLP systems capable of withstanding sophisticated
adversarial strategies.

</details>


### [296] [Empirical Quantification of Spurious Correlations in Malware Detection](https://arxiv.org/abs/2506.09662)
*Bianca Perasso,Ludovico Lozza,Andrea Ponte,Luca Demetrio,Luca Oneto,Fabio Roli*

Key words: 深度学习, 恶意软件检测, 虚假相关性, 编译器空白空间

TL;DR: 本文研究了深度学习在恶意软件检测中如何利用虚假相关性，并量化了其对决策的影响，提出了两种端到端模型的排名。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 探索虚假相关性如何影响深度学习在恶意软件检测中的性能，并量化其影响。

Method: 通过分析编译器留下的空白空间对模型决策的影响，在小规模平衡数据集上进行实验。

Result: 研究发现深度学习模型过度依赖编译器的空白空间，减少了编译代码的重要性。

Conclusion: 提出了两种端到端模型的排名，帮助选择更适合生产环境的模型。

Abstract: End-to-end deep learning exhibits unmatched performance for detecting
malware, but such an achievement is reached by exploiting spurious correlations
-- features with high relevance at inference time, but known to be useless
through domain knowledge. While previous work highlighted that deep networks
mainly focus on metadata, none investigated the phenomenon further, without
quantifying their impact on the decision. In this work, we deepen our
understanding of how spurious correlation affects deep learning for malware
detection by highlighting how much models rely on empty spaces left by the
compiler, which diminishes the relevance of the compiled code. Through our
seminal analysis on a small-scale balanced dataset, we introduce a ranking of
two end-to-end models to better understand which is more suitable to be put in
production.

</details>


### [297] [What is the Cost of Differential Privacy for Deep Learning-Based Trajectory Generation?](https://arxiv.org/abs/2506.09312)
*Erik Buchholz,Natasha Fernandes,David D. Nguyen,Alsharif Abuadbba,Surya Nepal,Salil S. Kanhere*

Key words: 差分隐私, 生成模型, 轨迹数据, DP-SGD, 效用隐私权衡

TL;DR: 研究探讨了如何在生成模型中通过差分隐私（DP）实现隐私与效用的平衡，分析了不同模型类型的表现，并提出了一种新的条件生成DP机制。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 为了解决轨迹数据生成中隐私保护与效用之间的挑战，确保形式化隐私保障同时维持数据的实用性。

Method: 采用DP-SGD评估现有生成模型的效用影响，提出一种新的条件生成DP机制，并比较扩散模型、VAE和GAN的效用-隐私权衡。

Result: DP-SGD对性能影响显著，但大数据集下仍保留部分效用；新DP机制提高训练稳定性，特别是对GAN和小数据集；扩散模型在无保障时表现最佳，GAN在DP-SGD下表现最优。

Conclusion: DP轨迹生成仍具挑战性，形式化隐私保障目前仅在大数据集和受限用例中可行。

Abstract: While location trajectories offer valuable insights, they also reveal
sensitive personal information. Differential Privacy (DP) offers formal
protection, but achieving a favourable utility-privacy trade-off remains
challenging. Recent works explore deep learning-based generative models to
produce synthetic trajectories. However, current models lack formal privacy
guarantees and rely on conditional information derived from real data during
generation. This work investigates the utility cost of enforcing DP in such
models, addressing three research questions across two datasets and eleven
utility metrics. (1) We evaluate how DP-SGD, the standard DP training method
for deep learning, affects the utility of state-of-the-art generative models.
(2) Since DP-SGD is limited to unconditional models, we propose a novel DP
mechanism for conditional generation that provides formal guarantees and assess
its impact on utility. (3) We analyse how model types - Diffusion, VAE, and GAN
- affect the utility-privacy trade-off. Our results show that DP-SGD
significantly impacts performance, although some utility remains if the
datasets is sufficiently large. The proposed DP mechanism improves training
stability, particularly when combined with DP-SGD, for unstable models such as
GANs and on smaller datasets. Diffusion models yield the best utility without
guarantees, but with DP-SGD, GANs perform best, indicating that the best
non-private model is not necessarily optimal when targeting formal guarantees.
In conclusion, DP trajectory generation remains a challenging task, and formal
guarantees are currently only feasible with large datasets and in constrained
use cases.

</details>


### [298] [TooBadRL: Trigger Optimization to Boost Effectiveness of Backdoor Attacks on Deep Reinforcement Learning](https://arxiv.org/abs/2506.09562)
*Songze Li,Mingxuan Zhang,Oubo Ma,Kang Wei,Shouling Ji*

Key words: 深度强化学习, 后门攻击, 触发器优化, Shapley值分析, 梯度对抗

TL;DR: 本文介绍了TooBadRL框架，首次系统性优化深度强化学习(DRL)的后门触发器，通过时间、空间和强度三个关键维度提升攻击效果。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 现有后门攻击多依赖简单启发式触发器配置，忽视了触发器优化的潜在效果，作者希望通过系统优化填补这一空白。

Method: 提出的TooBadRL框架包括性能感知的自适应冻结机制（时间维度）、基于Shapley值分析的维度选择（空间维度）和梯度对抗过程（强度维度）。

Result: 在三种主流DRL算法和九项基准任务中，TooBadRL显著提高了攻击成功率，同时确保正常任务性能的最小损失。

Conclusion: 研究表明，触发器优化在DRL后门攻击中具有被低估的重要性，TooBadRL为这一领域提供了新的思路。

Abstract: Deep reinforcement learning (DRL) has achieved remarkable success in a wide
range of sequential decision-making domains, including robotics, healthcare,
smart grids, and finance. Recent research demonstrates that attackers can
efficiently exploit system vulnerabilities during the training phase to execute
backdoor attacks, producing malicious actions when specific trigger patterns
are present in the state observations. However, most existing backdoor attacks
rely primarily on simplistic and heuristic trigger configurations, overlooking
the potential efficacy of trigger optimization. To address this gap, we
introduce TooBadRL (Trigger Optimization to Boost Effectiveness of Backdoor
Attacks on DRL), the first framework to systematically optimize DRL backdoor
triggers along three critical axes, i.e., temporal, spatial, and magnitude.
Specifically, we first introduce a performance-aware adaptive freezing
mechanism for injection timing. Then, we formulate dimension selection as a
cooperative game, utilizing Shapley value analysis to identify the most
influential state variable for the injection dimension. Furthermore, we propose
a gradient-based adversarial procedure to optimize the injection magnitude
under environment constraints. Evaluations on three mainstream DRL algorithms
and nine benchmark tasks show that TooBadRL significantly improves attack
success rates, while ensuring minimal degradation of normal task performance.
These results highlight the previously underappreciated importance of
principled trigger optimization in DRL backdoor attacks. The source code of
TooBadRL can be found at https://github.com/S3IC-Lab/TooBadRL.

</details>


### [299] [LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection Challenge](https://arxiv.org/abs/2506.09956)
*Sahar Abdelnabi,Aideen Fay,Ahmed Salem,Egor Zverev,Kai-Chieh Liao,Chi-Huang Liu,Chun-Chih Kuo,Jannis Weigend,Danyael Manlangit,Alex Apostolov,Haris Umair,João Donato,Masayuki Kawakita,Athar Mahboob,Tran Huu Bach,Tsun-Han Chiang,Myeongjin Cho,Hajin Choi,Byeonghyeon Kim,Hyeonjin Lee,Benjamin Pannell,Conor McCauley,Mark Russinovich,Andrew Paverd,Giovanni Cherubin*

Key words: 提示注入, LLM, 安全性, 隐私, 适应性攻击

TL;DR: 论文分析了间接提示注入攻击对大型语言模型（LLM）的影响，展示了LLMail-Inject挑战的结果，该挑战模拟了现实场景以评估防御策略的有效性。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 研究动机在于评估现有防御策略对适应性攻击的有效性，揭示LLM在区分指令与数据输入时的固有局限性。

Method: 通过公开挑战LLMail-Inject，模拟真实场景，收集参与者尝试注入恶意指令的行为数据，涵盖多种防御策略、LLM架构和检索配置。

Result: 挑战共收集208,095次攻击提交，来自839名参与者，提供了丰富的指令与数据分离问题的研究数据。

Conclusion: 论文希望该数据集和挑战代码能为未来解决提示注入问题的结构化方案提供基础。

Abstract: Indirect Prompt Injection attacks exploit the inherent limitation of Large
Language Models (LLMs) to distinguish between instructions and data in their
inputs. Despite numerous defense proposals, the systematic evaluation against
adaptive adversaries remains limited, even when successful attacks can have
wide security and privacy implications, and many real-world LLM-based
applications remain vulnerable. We present the results of LLMail-Inject, a
public challenge simulating a realistic scenario in which participants
adaptively attempted to inject malicious instructions into emails in order to
trigger unauthorized tool calls in an LLM-based email assistant. The challenge
spanned multiple defense strategies, LLM architectures, and retrieval
configurations, resulting in a dataset of 208,095 unique attack submissions
from 839 participants. We release the challenge code, the full dataset of
submissions, and our analysis demonstrating how this data can provide new
insights into the instruction-data separation problem. We hope this will serve
as a foundation for future research towards practical structural solutions to
prompt injection.

</details>
