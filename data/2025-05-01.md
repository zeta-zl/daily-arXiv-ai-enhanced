<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 48]
- [cs.LG](#cs.LG) [Total: 44]
- [cs.AI](#cs.AI) [Total: 14]
- [stat.ML](#stat.ML) [Total: 5]
- [q-bio.CB](#q-bio.CB) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [eess.SY](#eess.SY) [Total: 2]
- [econ.GN](#econ.GN) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.CV](#cs.CV) [Total: 22]
- [math.NA](#math.NA) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [eess.IV](#eess.IV) [Total: 2]
- [physics.data-an](#physics.data-an) [Total: 1]
- [cs.RO](#cs.RO) [Total: 9]
- [cs.CR](#cs.CR) [Total: 26]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.CY](#cs.CY) [Total: 7]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.GR](#cs.GR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Waking Up an AI: A Quantitative Framework for Prompt-Induced Phase Transition in Large Language Models](https://arxiv.org/abs/2504.21012)
*Makoto Sato*

Main category: cs.CL

TL;DR: 该论文提出了一种定量分析大型语言模型（LLM）认知行为的框架，通过诱导性提示（TIP）和量化性提示（TQP）来测量其行为变化，发现当前LLM尚未能复现人类直觉中的概念融合过程。


<details>
  <summary>Details</summary>
Motivation: 研究人类直觉思维的底层机制，通过对比人类与LLM的认知行为差异，以揭示人工与人类思维的关键区别。

Method: 采用TIP触发LLM行为突变，并通过TQP量化评估行为变化，对比分析了语义融合与非融合提示对LLM的影响。

Result: 实验显示，LLM对语义融合提示的响应与人类不同，未能体现人类直觉中的概念融合能力。

Conclusion: 当前LLM尚未完全模拟人类直觉的认知过程，但该方法为未来研究提供了可复现的量化工具。

Abstract: What underlies intuitive human thinking? One approach to this question is to
compare the cognitive dynamics of humans and large language models (LLMs).
However, such a comparison requires a method to quantitatively analyze AI
cognitive behavior under controlled conditions. While anecdotal observations
suggest that certain prompts can dramatically change LLM behavior, these
observations have remained largely qualitative. Here, we propose a two-part
framework to investigate this phenomenon: a Transition-Inducing Prompt (TIP)
that triggers a rapid shift in LLM responsiveness, and a Transition Quantifying
Prompt (TQP) that evaluates this change using a separate LLM. Through
controlled experiments, we examined how LLMs react to prompts embedding two
semantically distant concepts (e.g., mathematical aperiodicity and traditional
crafts)--either fused together or presented separately--by changing their
linguistic quality and affective tone. Whereas humans tend to experience
heightened engagement when such concepts are meaningfully blended producing a
novel concept--a form of conceptual fusion--current LLMs showed no significant
difference in responsiveness between semantically fused and non-fused prompts.
This suggests that LLMs may not yet replicate the conceptual integration
processes seen in human intuition. Our method enables fine-grained,
reproducible measurement of cognitive responsiveness, and may help illuminate
key differences in how intuition and conceptual leaps emerge in artificial
versus human minds.

</details>


### [2] [Analyzing Feedback Mechanisms in AI-Generated MCQs: Insights into Readability, Lexical Properties, and Levels of Challenge](https://arxiv.org/abs/2504.21013)
*Antoun Yaacoub,Zainab Assaghir,Lionel Prevost,Jérôme Da-Rugna*

Main category: cs.CL

TL;DR: 该研究分析了Google Gemini 1.5-flash文本模型生成的计算机科学选择题反馈的语言特征，包括可读性、词汇丰富度等，揭示了反馈语气与题目难度之间的动态关系。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补关于AI生成反馈的语言特征（如可读性、词汇丰富度）在不同难度和语气下的适应性的知识空白，以推动更个性化的教育反馈机制。

Method: 基于1,200多道选择题数据集，分析不同难度（易、中、难）和语气（支持性、中性、挑战性）的反馈。采用RoBERTa多任务学习模型预测语言属性。

Result: 模型在可读性（MAE=2.0）和词汇丰富度（MAE=0.03）上表现良好，发现反馈语气与题目难度存在显著交互作用，表明AI反馈能动态适应教育场景。

Conclusion: 研究为开发更有效的个性化AI反馈提供了依据，同时强调了设计中伦理考量的重要性。

Abstract: Artificial Intelligence (AI)-generated feedback in educational settings has
garnered considerable attention due to its potential to enhance learning
outcomes. However, a comprehensive understanding of the linguistic
characteristics of AI-generated feedback, including readability, lexical
richness, and adaptability across varying challenge levels, remains limited.
This study delves into the linguistic and structural attributes of feedback
generated by Google's Gemini 1.5-flash text model for computer science
multiple-choice questions (MCQs). A dataset of over 1,200 MCQs was analyzed,
considering three difficulty levels (easy, medium, hard) and three feedback
tones (supportive, neutral, challenging). Key linguistic metrics, such as
length, readability scores (Flesch-Kincaid Grade Level), vocabulary richness,
and lexical density, were computed and examined. A fine-tuned RoBERTa-based
multi-task learning (MTL) model was trained to predict these linguistic
properties, achieving a Mean Absolute Error (MAE) of 2.0 for readability and
0.03 for vocabulary richness. The findings reveal significant interaction
effects between feedback tone and question difficulty, demonstrating the
dynamic adaptation of AI-generated feedback within diverse educational
contexts. These insights contribute to the development of more personalized and
effective AI-driven feedback mechanisms, highlighting the potential for
improved learning outcomes while underscoring the importance of ethical
considerations in their design and deployment.

</details>


### [3] [Nested Named-Entity Recognition on Vietnamese COVID-19: Dataset and Experiments](https://arxiv.org/abs/2504.21016)
*Ngoc C. Lê,Hai-Chung Nguyen-Phung,Thu-Huong Pham Thi,Hue Vu,Phuong-Thao Nguyen Thi,Thu-Thuy Tran,Hong-Nhung Le Thi,Thuy-Duong Nguyen-Thi,Thanh-Huy Nguyen*

Main category: cs.CL

TL;DR: 研究提出了一种基于命名实体识别（NER）的系统，用于辅助越南的COVID-19疫情追踪与防控，并构建了一个手动标注的越南语嵌套命名实体识别数据集。


<details>
  <summary>Details</summary>
Motivation: 越南通过手工方式进行疫情接触者追踪、定位和隔离，效率低下且工作量大。研究旨在通过自动化技术提升效率。

Method: 采用命名实体识别（NER）技术，定义新实体类型，并构建手动标注的越南语嵌套命名实体数据集。

Result: 提出了一个针对越南语的NER系统，并创建了专用数据集，支持COVID-19防控中的实体识别任务。

Conclusion: 通过NER技术和专用数据集，研究为越南的疫情追踪提供了自动化解决方案，有望提高防控效率。

Abstract: The COVID-19 pandemic caused great losses worldwide, efforts are taken place
to prevent but many countries have failed. In Vietnam, the traceability,
localization, and quarantine of people who contact with patients contribute to
effective disease prevention. However, this is done by hand, and take a lot of
work. In this research, we describe a named-entity recognition (NER) study that
assists in the prevention of COVID-19 pandemic in Vietnam. We also present our
manually annotated COVID-19 dataset with nested named entity recognition task
for Vietnamese which be defined new entity types using for our system.

</details>


### [4] [ViQA-COVID: COVID-19 Machine Reading Comprehension Dataset for Vietnamese](https://arxiv.org/abs/2504.21017)
*Hai-Chung Nguyen-Phung,Ngoc C. Lê,Van-Chien Nguyen,Hang Thi Nguyen,Thuy Phuong Thi Nguyen*

Main category: cs.CL

TL;DR: 该论文介绍了首个针对越南语的COVID-19多跨度提取机器阅读理解（MRC）数据集ViQA-COVID，旨在支持疾病预防和促进越南语及多语言MRC研究。


<details>
  <summary>Details</summary>
Motivation: COVID-19全球大流行对经济和社会造成严重影响，尤其是Omicron变种迅速传播导致资源过载，亟需AI技术如MRC来辅助疾病预防和支持研究。

Method: 研究团队创建了越南语首个多跨度提取MRC数据集ViQA-COVID，用于构建模型和系统以支持COVID-19防控。

Result: ViQA-COVID不仅是首个越南语COVID-19 MRC数据集，也是越南语首个多跨度提取MRC数据集，有望推动越南语和多语言MRC研究发展。

Conclusion: ViQA-COVID数据集的创建填补了越南语MRC研究的空白，并将为COVID-19防控和多语言MRC技术进步提供有力支持。

Abstract: After two years of appearance, COVID-19 has negatively affected people and
normal life around the world. As in May 2022, there are more than 522 million
cases and six million deaths worldwide (including nearly ten million cases and
over forty-three thousand deaths in Vietnam). Economy and society are both
severely affected. The variant of COVID-19, Omicron, has broken disease
prevention measures of countries and rapidly increased number of infections.
Resources overloading in treatment and epidemics prevention is happening all
over the world. It can be seen that, application of artificial intelligence
(AI) to support people at this time is extremely necessary. There have been
many studies applying AI to prevent COVID-19 which are extremely useful, and
studies on machine reading comprehension (MRC) are also in it. Realizing that,
we created the first MRC dataset about COVID-19 for Vietnamese: ViQA-COVID and
can be used to build models and systems, contributing to disease prevention.
Besides, ViQA-COVID is also the first multi-span extraction MRC dataset for
Vietnamese, we hope that it can contribute to promoting MRC studies in
Vietnamese and multilingual.

</details>


### [5] [HYPEROFA: Expanding LLM Vocabulary to New Languages via Hypernetwork-Based Embedding Initialization](https://arxiv.org/abs/2504.21018)
*Enes Özeren,Yihong Liu,Hinrich Schütze*

Main category: cs.CL

TL;DR: HYPEROFA是一种基于超网络的方法，用于更灵活地初始化目标语言词汇嵌入，解决了传统方法OFA在表达上的局限性，并在实验表现上超越了随机初始化和OFA。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练语言模型在中低资源语言上表现不佳，主要因为预训练中接触这些语言的有限。OFA虽提出了一种有效的子词初始化方法，但其表达受限，因此需要更灵活的初始化策略。

Method: HYPEROFA通过超网络将多语言词向量空间映射到预训练模型的词嵌入空间，为目标语言生成灵活的嵌入作为预训练的起点。

Result: 实验表明，HYPEROFA在持续预训练收敛和下游任务表现上优于随机初始化，且与OFA持平或更好。

Conclusion: HYPEROFA通过超网络实现了更具适应性的词嵌入初始化，为中低资源语言的预训练提供了更优解决方案。

Abstract: Many pre-trained language models (PLMs) exhibit suboptimal performance on
mid- and low-resource languages, largely due to limited exposure to these
languages during pre-training. A common strategy to address this is to
introduce new tokens specific to the target languages, initialize their
embeddings, and apply continual pre-training on target-language data. Among
such methods, OFA (Liu et al., 2024a) proposes a similarity-based subword
embedding initialization heuristic that is both effective and efficient.
However, OFA restricts target-language token embeddings to be convex
combinations of a fixed number of source-language embeddings, which may limit
expressiveness. To overcome this limitation, we propose HYPEROFA, a
hypernetwork-based approach for more adaptive token embedding initialization.
The hypernetwork is trained to map from an external multilingual word vector
space to the PLMs token embedding space using source-language tokens. Once
trained, it can generate flexible embeddings for target-language tokens,
serving as a good starting point for continual pretraining. Experiments
demonstrate that HYPEROFA consistently outperforms random initialization
baseline and matches or exceeds the performance of OFA in both continual
pre-training convergence and downstream task performance. We make the code
publicly available.

</details>


### [6] [Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations](https://arxiv.org/abs/2504.21019)
*Yinghan Zhou,Juan Wen,Wanli Peng,Yiming Xue,Ziwei Zhang,Zhengxian Wu*

Main category: cs.CL

TL;DR: 论文提出了一种新的AI生成文本检测方法（DP-Net），通过强化学习引入动态扰动，同时提升了模型的泛化能力和鲁棒性。实验证明其在跨域场景和对抗攻击下表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普及，AI生成文本（AIGT）的滥用风险增加。现有方法往往只关注泛化性或鲁棒性，而缺乏同时解决两者的统一机制。

Method: 提出DP-Net方法，通过强化学习的动态扰动机制，结合精心设计的奖励和动作，实现泛化性和鲁棒性的统一优化。

Result: 实验显示，DP-Net在三种跨域场景中显著优于现有方法，并在两种文本对抗攻击下表现最佳。

Conclusion: DP-Net通过动态扰动机制有效兼顾了泛化性和鲁棒性，为AIGT检测提供了新思路，代码已开源。

Abstract: The growing popularity of large language models has raised concerns regarding
the potential to misuse AI-generated text (AIGT). It becomes increasingly
critical to establish an excellent AIGT detection method with high
generalization and robustness. However, existing methods either focus on model
generalization or concentrate on robustness. The unified mechanism, to
simultaneously address the challenges of generalization and robustness, is less
explored. In this paper, we argue that robustness can be view as a specific
form of domain shift, and empirically reveal an intrinsic mechanism for model
generalization of AIGT detection task. Then, we proposed a novel AIGT detection
method (DP-Net) via dynamic perturbations introduced by a reinforcement
learning with elaborated reward and action. Experimentally, extensive results
show that the proposed DP-Net significantly outperforms some state-of-the-art
AIGT detection methods for generalization capacity in three cross-domain
scenarios. Meanwhile, the DP-Net achieves best robustness under two text
adversarial attacks. The code is publicly available at
https://github.com/CAU-ISS-Lab/AIGT-Detection-Evade-Detection/tree/main/DP-Net.

</details>


### [7] [Context-Enhanced Contrastive Search for Improved LLM Text Generation](https://arxiv.org/abs/2504.21020)
*Jaydip Sen,Rohit Pandey,Hetvi Waghela*

Main category: cs.CL

TL;DR: 论文提出了一种改进的Contrastive Search算法CECS，通过动态上下文重要性加权、多级对比搜索和自适应温度控制优化文本生成质量，在BLEU、ROUGE等指标上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有解码方法在生成长文本时易出现重复或不连贯问题，因此需要一种能平衡流畅性、多样性和相关性的新方法。

Method: 提出Context-Enhanced Contrastive Search (CECS)，结合动态上下文权重、多级对比搜索和自适应温度控制。

Result: 实验显示CECS在文本连贯性和相关性上显著优于现有技术，BLEU、ROUGE等指标提升明显。

Conclusion: CECS在文本生成任务中表现出色，适用于法律文件起草、客服聊天机器人等实际场景。

Abstract: Recently, Large Language Models (LLMs) have demonstrated remarkable
advancements in Natural Language Processing (NLP). However, generating
high-quality text that balances coherence, diversity, and relevance remains
challenging. Traditional decoding methods, such as bean search and top-k
sampling, often struggle with either repetitive or incoherent outputs,
particularly in tasks that require long-form text generation. To address these
limitations, the paper proposes a novel enhancement of the well-known
Contrastive Search algorithm, Context-Enhanced Contrastive Search (CECS) with
contextual calibration. The proposed scheme introduces several novelties
including dynamic contextual importance weighting, multi-level Contrastive
Search, and adaptive temperature control, to optimize the balance between
fluency, creativity, and precision. The performance of CECS is evaluated using
several standard metrics such as BLEU, ROUGE, and semantic similarity.
Experimental results demonstrate significant improvements in both coherence and
relevance of the generated texts by CECS outperforming the existing Contrastive
Search techniques. The proposed algorithm has several potential applications in
the real world including legal document drafting, customer service chatbots,
and content marketing.

</details>


### [8] [ConformalNL2LTL: Translating Natural Language Instructions into Temporal Logic Formulas with Conformal Correctness Guarantees](https://arxiv.org/abs/2504.21022)
*Jun Wang,David Smith Sundarsingh,Jyotirmoy V. Deshmukh,Yiannis Kantaros*

Main category: cs.CL

TL;DR: 本文提出了一种新方法ConformalNL2LTL，用于将自然语言指令转化为线性时序逻辑（LTL）公式，并通过符合性预测技术确保翻译准确性，同时最小化求助率。


<details>
  <summary>Details</summary>
Motivation: 现有的自然语言到LTL公式的翻译方法缺乏正确性保证，增加了人工干预需求。为此，作者旨在开发一种具有用户定义翻译成功率的新方法。

Method: ConformalNL2LTL通过迭代解决一系列开放词汇问答（QA）问题生成LTL公式，并结合符合性预测（CP）技术量化LLM生成答案的不确定性，从而在自信不足时主动求助。

Result: 理论和实验结果表明，该方法能在满足用户指定翻译准确度的同时，有效降低求助率。

Conclusion: ConformalNL2LTL为自然语言到LTL的翻译提供了一种高可靠性且高效的解决方案，减少了人工干预需求。

Abstract: Linear Temporal Logic (LTL) has become a prevalent specification language for
robotic tasks. To mitigate the significant manual effort and expertise required
to define LTL-encoded tasks, several methods have been proposed for translating
Natural Language (NL) instructions into LTL formulas, which, however, lack
correctness guarantees. To address this, we introduce a new NL-to-LTL
translation method, called ConformalNL2LTL, that can achieve user-defined
translation success rates over unseen NL commands. Our method constructs LTL
formulas iteratively by addressing a sequence of open-vocabulary
Question-Answering (QA) problems with LLMs. To enable uncertainty-aware
translation, we leverage conformal prediction (CP), a distribution-free
uncertainty quantification tool for black-box models. CP enables our method to
assess the uncertainty in LLM-generated answers, allowing it to proceed with
translation when sufficiently confident and request help otherwise. We provide
both theoretical and empirical results demonstrating that ConformalNL2LTL
achieves user-specified translation accuracy while minimizing help rates.

</details>


### [9] [Param$Δ$ for Direct Weight Mixing: Post-Train Large Language Model at Zero Cost](https://arxiv.org/abs/2504.21023)
*Sheng Cao,Mingrui Wu,Karthik Prasad,Yuandong Tian,Zechun Liu*

Main category: cs.CL

TL;DR: 本文提出了一种名为$Param\Delta$的新方法，通过直接应用已训练模型的权重差异来更新基础模型，从而无需额外训练即可实现后训练能力，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 后训练阶段需要大量高质量数据和计算资源，且容易过拟合。本文旨在解决这一效率问题，通过知识迁移实现零成本模型更新。

Method: 通过计算后训练模型与基础模型的权重差异，并将该差异应用于更新后的基础模型，实现零训练的后训练能力迁移。

Result: 在多个模型（如LLama3、Qwen等）上的实验表明，$Param\Delta$方法能达到传统后训练95%的性能，且无额外计算开销。

Conclusion: $Param\Delta$为开源社区提供了一种高效利用模型权重的框架，加速了模型迭代周期。

Abstract: The post-training phase of large language models is essential for enhancing
capabilities such as instruction-following, reasoning, and alignment with human
preferences. However, it demands extensive high-quality data and poses risks
like overfitting, alongside significant computational costs due to repeated
post-training and evaluation after each base model update. This paper
introduces $Param\Delta$, a novel method that streamlines post-training by
transferring knowledge from an existing post-trained model to a newly updated
base model with ZERO additional training. By computing the difference between
post-trained model weights ($\Theta_\text{post}$) and base model weights
($\Theta_\text{base}$), and adding this to the updated base model
($\Theta'_\text{base}$), we define $Param\Delta$ Model as:
$\Theta_{\text{Param}\Delta} = \Theta_\text{post} - \Theta_\text{base} +
\Theta'_\text{base}$. This approach surprisingly equips the new base model with
post-trained capabilities, achieving performance comparable to direct
post-training. We did analysis on LLama3, Llama3.1, Qwen, and
DeepSeek-distilled models. Results indicate $Param\Delta$ Model effectively
replicates traditional post-training. For example, the $Param\Delta$ Model
obtained from 70B Llama3-inst, Llama3-base, Llama3.1-base models attains
approximately 95\% of Llama3.1-inst model's performance on average.
$Param\Delta$ brings a new perspective on how to fully leverage models in the
open-weight community, where checkpoints for base and instruct models are
readily available and frequently updated, by providing a cost-free framework to
accelerate the iterative cycle of model development.

</details>


### [10] [WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model](https://arxiv.org/abs/2504.21024)
*Tianqing Fang,Hongming Zhang,Zhisong Zhang,Kaixin Ma,Wenhao Yu,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: 该论文提出了一种结合世界模型的新框架，通过增强探索和利用预训练知识来解决自主代理在自我改进过程中的性能停滞问题，实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是当前自主代理在自我改进过程中因环境探索不足和预训练知识利用不充分而导致的性能停滞问题。

Method: 论文提出了一种引入共进化世界模型的新框架，该模型能预测网络环境中的下一个观察结果，并作为虚拟网络服务器和自我想象引擎，提升代理的性能。

Result: 实验结果显示，在多个真实网络环境中，该方法比现有自进化代理有10%的性能提升，证明了其效果和泛化能力。

Conclusion: 论文结论指出，将世界模型整合到自主代理框架中对于实现持续的适应性至关重要。

Abstract: Agent self-improvement, where the backbone Large Language Model (LLM) of the
agent are trained on trajectories sampled autonomously based on their own
policies, has emerged as a promising approach for enhancing performance. Recent
advancements, particularly in web environments, face a critical limitation:
their performance will reach a stagnation point during autonomous learning
cycles, hindering further improvement. We argue that this stems from limited
exploration of the web environment and insufficient exploitation of pre-trained
web knowledge in LLMs. To improve the performance of self-improvement, we
propose a novel framework that introduces a co-evolving World Model LLM. This
world model predicts the next observation based on the current observation and
action within the web environment. Leveraging LLMs' pretrained knowledge of
abundant web content, the World Model serves dual roles: (1) as a virtual web
server generating self-instructed training data to continuously refine the
agent's policy, and (2) as an imagination engine during inference, enabling
look-ahead simulation to guide action selection for the agent LLM. Experiments
in real-world web environments (Mind2Web-Live, WebVoyager, and GAIA-web) show a
10% performance gain over existing self-evolving agents, demonstrating the
efficacy and generalizability of our approach, without using any distillation
from more powerful close-sourced models. Our work establishes the necessity of
integrating world models into autonomous agent frameworks to unlock sustained
adaptability.

</details>


### [11] [Durghotona GPT: A Web Scraping and Large Language Model Based Framework to Generate Road Accident Dataset Automatically in Bangladesh](https://arxiv.org/abs/2504.21025)
*MD Thamed Bin Zaman Chowdhury,Moazzem Hossain,Md. Ridwanul Islam*

Main category: cs.CL

TL;DR: 论文提出了一个名为'Durghotona GPT'的新框架，结合网络爬虫和大语言模型（LLMs），从孟加拉国的三大报纸自动生成详细的交通事故数据集，以解决手动数据收集的延迟、错误和沟通问题。评估显示，开源模型Llama-3的性能与GPT-4相当，准确率达89%，是一种经济高效的替代方案。


<details>
  <summary>Details</summary>
Motivation: 交通事故导致巨大的经济损失和社会问题，准确及时的数据对预测和缓解事故至关重要。传统手动数据收集方法存在局限，因此需要自动化解决方案。

Method: 框架从Prothom Alo、Dhaka Tribune和The Daily Star三大报纸爬取事故新闻，并利用GPT-4、GPT-3.5和Llama-3处理数据，提取、分类并生成数据集。

Result: Llama-3表现与GPT-4相当，准确率为89%。生成的数据库显著提升了数据的质量和可用性，支持交通安全分析和城市规划等应用。

Conclusion: 该框架是手动数据收集的有效替代方案，未来将扩展数据收集方法并优化模型以进一步提高精度和适用性。

Abstract: Road accidents pose significant concerns globally. They lead to large
financial losses, injuries, disabilities, and societal challenges. Accurate and
timely accident data is essential for predicting and mitigating these events.
This paper presents a novel framework named 'Durghotona GPT' that integrates
web scraping and Large Language Models (LLMs) to automate the generation of
comprehensive accident datasets from prominent national dailies in Bangladesh.
The authors collected accident reports from three major newspapers: Prothom
Alo, Dhaka Tribune, and The Daily Star. The collected news was then processed
using the newest available LLMs: GPT-4, GPT-3.5, and Llama-3. The framework
efficiently extracts relevant information, categorizes reports, and compiles
detailed datasets. Thus, this framework overcomes limitations of manual data
collection methods such as delays, errors, and communication gaps. The authors'
evaluation demonstrates that Llama-3, an open-source model, performs comparably
to GPT-4. It achieved 89% accuracy in the authors' evaluation. Therefore, it
can be considered a cost-effective alternative for similar tasks. The results
suggest that the framework developed by the authors can drastically enhance the
quality and availability of accident data. As a result, it can support critical
applications in traffic safety analysis, urban planning, and public health. The
authors also developed an interface for 'Durghotona GPT' for ease of use as
part of this paper. Future work will focus on expanding data collection methods
and refining LLMs to further increase dataset accuracy and applicability.

</details>


### [12] [Creating and Evaluating Code-Mixed Nepali-English and Telugu-English Datasets for Abusive Language Detection Using Traditional and Deep Learning Models](https://arxiv.org/abs/2504.21026)
*Manish Pandey,Nageshwar Prasad Yadav,Mokshada Adduru,Sawan Rai*

Main category: cs.CL

TL;DR: 论文提出了一种针对泰卢固-英语和尼泊尔-英语混合文本的滥用语言检测方法，通过构建新型标注数据集并评估多种模型性能，填补了低资源语言研究的空白。


<details>
  <summary>Details</summary>
Motivation: 由于多语言用户在社交媒体上的增加，混合文本中的滥用语言检测面临挑战。现有研究主要集中在高资源语言（如英语和印地语），而低资源语言（如泰卢固语和尼泊尔语）的研究不足。

Method: 研究构建了一个包含2000条泰卢固-英语和500条尼泊尔-英语混合注释的数据集，并对其进行了严格的预处理。随后，评估了多种机器学习、深度学习和大语言模型的性能，包括逻辑回归、随机森林、SVM、神经网络、LSTM、CNN等，并通过10折交叉验证和t检验优化性能。

Result: 研究结果为混合文本中的滥用语言检测提供了关键见解，并对不同计算方法进行了比较分析。论文为泰卢固-英语和尼泊尔-英语混合文本的滥用语言检测建立了基准。

Conclusion: 该研究通过构建数据集和评估模型，推动了低资源语言NLP的发展，为多语言社交媒体环境中的内容审核提供了更鲁棒的策略。

Abstract: With the growing presence of multilingual users on social media, detecting
abusive language in code-mixed text has become increasingly challenging.
Code-mixed communication, where users seamlessly switch between English and
their native languages, poses difficulties for traditional abuse detection
models, as offensive content may be context-dependent or obscured by linguistic
blending. While abusive language detection has been extensively explored for
high-resource languages like English and Hindi, low-resource languages such as
Telugu and Nepali remain underrepresented, leaving gaps in effective
moderation. In this study, we introduce a novel, manually annotated dataset of
2 thousand Telugu-English and 5 Nepali-English code-mixed comments, categorized
as abusive and non-abusive, collected from various social media platforms. The
dataset undergoes rigorous preprocessing before being evaluated across multiple
Machine Learning (ML), Deep Learning (DL), and Large Language Models (LLMs). We
experimented with models including Logistic Regression, Random Forest, Support
Vector Machines (SVM), Neural Networks (NN), LSTM, CNN, and LLMs, optimizing
their performance through hyperparameter tuning, and evaluate it using 10-fold
cross-validation and statistical significance testing (t-test). Our findings
provide key insights into the challenges of detecting abusive language in
code-mixed settings and offer a comparative analysis of computational
approaches. This study contributes to advancing NLP for low-resource languages
by establishing benchmarks for abusive language detection in Telugu-English and
Nepali-English code-mixed text. The dataset and insights can aid in the
development of more robust moderation strategies for multilingual social media
environments.

</details>


### [13] [UrbanPlanBench: A Comprehensive Urban Planning Benchmark for Evaluating Large Language Models](https://arxiv.org/abs/2504.21027)
*Yu Zheng,Longyi Liu,Yuming Lin,Jie Feng,Guozhen Zhang,Depeng Jin,Yong Li*

Main category: cs.CL

TL;DR: 本研究通过UrbanPlanBench评估大语言模型（LLM）在城市规划中的表现，发现其知识获取不平衡，尤其在法规理解上落后。同时发布最大的监督微调数据集UrbanPlanText，虽提升了模型表现，但在专业术语和推理任务上仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs如何辅助依赖专业知识和经验的城市规划领域，填补其在城市规划中应用的研究空白。

Method: 开发UrbanPlanBench评估标准，涵盖规划基本原则、专业知识和法规；构建UrbanPlanText数据集（30,000+指令对）用于模型微调。

Result: LLMs表现不均衡，70%在法规理解上不及格；微调后模型记忆和知识理解能力提升，但专业术语和推理仍不足。

Conclusion: 开源工具集促进LLMs与城市规划的结合，需进一步优化模型的专业能力以实现人机协作。

Abstract: The advent of Large Language Models (LLMs) holds promise for revolutionizing
various fields traditionally dominated by human expertise. Urban planning, a
professional discipline that fundamentally shapes our daily surroundings, is
one such field heavily relying on multifaceted domain knowledge and experience
of human experts. The extent to which LLMs can assist human practitioners in
urban planning remains largely unexplored. In this paper, we introduce a
comprehensive benchmark, UrbanPlanBench, tailored to evaluate the efficacy of
LLMs in urban planning, which encompasses fundamental principles, professional
knowledge, and management and regulations, aligning closely with the
qualifications expected of human planners. Through extensive evaluation, we
reveal a significant imbalance in the acquisition of planning knowledge among
LLMs, with even the most proficient models falling short of meeting
professional standards. For instance, we observe that 70% of LLMs achieve
subpar performance in understanding planning regulations compared to other
aspects. Besides the benchmark, we present the largest-ever supervised
fine-tuning (SFT) dataset, UrbanPlanText, comprising over 30,000 instruction
pairs sourced from urban planning exams and textbooks. Our findings demonstrate
that fine-tuned models exhibit enhanced performance in memorization tests and
comprehension of urban planning knowledge, while there exists significant room
for improvement, particularly in tasks requiring domain-specific terminology
and reasoning. By making our benchmark, dataset, and associated evaluation and
fine-tuning toolsets publicly available at
https://github.com/tsinghua-fib-lab/PlanBench, we aim to catalyze the
integration of LLMs into practical urban planning, fostering a symbiotic
collaboration between human expertise and machine intelligence.

</details>


### [14] [Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG Evaluation Prompts](https://arxiv.org/abs/2504.21117)
*Hanhua Hong,Chenghao Xiao,Yang Wang,Yiqi Liu,Wenge Rong,Chenghua Lin*

Main category: cs.CL

TL;DR: 本文提出了一种逆学习方法，用于自动生成高效的模型特定评估提示，解决了基于LLM评估的提示设计敏感性问题，提高了评估效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于人类评估存在不一致性、缺乏标准化和人口统计偏见等问题，且LLM评估对提示设计高度敏感，作者希望通过自动生成评估提示来提高评估效率和鲁棒性。

Method: 采用逆学习方法，学习从模型输出反向映射到输入指令，生成高效、模型特定的评估提示，仅需单个评估样本，无需人工提示工程。

Result: 该方法实现了高效且鲁棒的自动提示生成，显著提高了LLM评估的效率和一致性。

Conclusion: 本文提出的方法为LLM评估提供了一种更高效、鲁棒的新方向，减少了人工干预的需求。

Abstract: Evaluating natural language generation (NLG) systems is challenging due to
the diversity of valid outputs. While human evaluation is the gold standard, it
suffers from inconsistencies, lack of standardisation, and demographic biases,
limiting reproducibility. LLM-based evaluation offers a scalable alternative
but is highly sensitive to prompt design, where small variations can lead to
significant discrepancies. In this work, we propose an inversion learning
method that learns effective reverse mappings from model outputs back to their
input instructions, enabling the automatic generation of highly effective,
model-specific evaluation prompts. Our method requires only a single evaluation
sample and eliminates the need for time-consuming manual prompt engineering,
thereby improving both efficiency and robustness. Our work contributes toward a
new direction for more robust and efficient LLM-based evaluation.

</details>


### [15] [LLM Enhancer: Merged Approach using Vector Embedding for Reducing Large Language Model Hallucinations with External Knowledge](https://arxiv.org/abs/2504.21132)
*Naheed Rayhan,Md. Ashrafuzzaman*

Main category: cs.CL

TL;DR: 论文提出LLM ENHANCER系统，通过整合多个在线资源（如Google、Wikipedia）提升LLMs生成信息的准确性，减少幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs（如ChatGPT）在实际关键场景中的应用受限于生成信息不准确且缺乏外部知识整合能力，作者希望通过结合多源在线数据提升其表现。

Method: 系统采用开源LLMs，并行获取多源数据（如Google、Wikipedia），利用向量嵌入筛选最相关信息，并通过自定义代理工具优化信息流。

Result: LLM ENHANCER系统有效减少了LLMs的幻觉问题，同时保持了回答的自然性和准确性。

Conclusion: 通过整合外部知识源，LLM ENHANCER系统显著提升了LLMs在真实场景中的可靠性和实用性。

Abstract: Large Language Models (LLMs), such as ChatGPT, have demonstrated the
capability to generate human like, natural responses across a range of tasks,
including task oriented dialogue and question answering. However, their
application in real world, critical scenarios is often hindered by a tendency
to produce inaccurate information and a limited ability to leverage external
knowledge sources. This paper introduces the LLM ENHANCER system, designed to
integrate multiple online sources such as Google, Wikipedia, and DuckDuckGo to
enhance data accuracy. The LLMs employed within this system are open source.
The data acquisition process for the LLM ENHANCER system operates in parallel,
utilizing custom agent tools to manage the flow of information. Vector
embeddings are used to identify the most pertinent information, which is
subsequently supplied to the LLM for user interaction. The LLM ENHANCER system
mitigates hallucinations in chat based LLMs while preserving response
naturalness and accuracy.

</details>


### [16] [Detecting Manipulated Contents Using Knowledge-Grounded Inference](https://arxiv.org/abs/2504.21165)
*Mark Huasong Meng,Ruizhe Wang,Meng Xu,Chuan Yan,Guangdong Bai*

Main category: cs.CL

TL;DR: 本文提出了Manicod工具，用于检测零日操纵内容，通过检索增强生成（RAG）将上下文信息向量化，并利用大语言模型（LLM）进行推理，实现了在检测虚假新闻上的高性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有虚假新闻检测方法依赖训练时获得的内在知识或手动整理的上下文，无法有效识别零日操纵内容。本文旨在填补这一空白，通过实时上下文信息提升检测能力。

Method: 提出Manicod工具，首先从主流搜索引擎获取输入声明的上下文信息，然后通过RAG将其向量化，供LLM进行推理，生成决策（“真实”或“操纵”）及相关解释。

Result: 在包含4270条虚假新闻的数据集上，Manicod的F1分数达到0.856，比现有方法在事实核查和声明验证上的性能提升了1.9倍。

Conclusion: Manicod成功解决了零日操纵内容检测的挑战，展现了基于上下文增强的LLM方法在虚假新闻检测中的潜力。

Abstract: The detection of manipulated content, a prevalent form of fake news, has been
widely studied in recent years. While existing solutions have been proven
effective in fact-checking and analyzing fake news based on historical events,
the reliance on either intrinsic knowledge obtained during training or manually
curated context hinders them from tackling zero-day manipulated content, which
can only be recognized with real-time contextual information. In this work, we
propose Manicod, a tool designed for detecting zero-day manipulated content.
Manicod first sources contextual information about the input claim from
mainstream search engines, and subsequently vectorizes the context for the
large language model (LLM) through retrieval-augmented generation (RAG). The
LLM-based inference can produce a "truthful" or "manipulated" decision and
offer a textual explanation for the decision. To validate the effectiveness of
Manicod, we also propose a dataset comprising 4270 pieces of manipulated fake
news derived from 2500 recent real-world news headlines. Manicod achieves an
overall F1 score of 0.856 on this dataset and outperforms existing methods by
up to 1.9x in F1 score on their benchmarks on fact-checking and claim
verification.

</details>


### [17] [Small or Large? Zero-Shot or Finetuned? Guiding Language Model Choice for Specialized Applications in Healthcare](https://arxiv.org/abs/2504.21191)
*Lovedeep Gondara,Jonathan Simkin,Graham Sayle,Shebnum Devji,Gregory Arbour,Raymond Ng*

Main category: cs.CL

TL;DR: 研究比较了语言模型调优与零样本使用的效果，探讨了领域相关预训练模型与通用模型的差异，并评估了小型语言模型（SLMs）与大型语言模型（LLMs）在特定任务中的表现。结果显示，调优后的SLMs表现优于零样本LLM，尤其在复杂任务中。


<details>
  <summary>Details</summary>
Motivation: 探索在专业领域中如何选择语言模型，包括调优必要性、预训练模型的领域适应性，以及SLMs与LLMs的实际效能对比。

Method: 使用英国哥伦比亚癌症登记处的病理报告数据，评估三种不同难度的分类任务，比较调优与零样本的SLMs以及零样本LLM的表现。

Result: 调优显著提升SLMs表现，尤其在复杂任务中；领域相关的预训练模型表现更优；调优后SLMs超过零样本LLM性能。

Conclusion: 在专业领域任务中，SLMs通过调优可超越零样本LLM，且资源效率更高，证明了其在LLM时代仍具重要价值。

Abstract: This study aims to guide language model selection by investigating: 1) the
necessity of finetuning versus zero-shot usage, 2) the benefits of
domain-adjacent versus generic pretrained models, 3) the value of further
domain-specific pretraining, and 4) the continued relevance of Small Language
Models (SLMs) compared to Large Language Models (LLMs) for specific tasks.
Using electronic pathology reports from the British Columbia Cancer Registry
(BCCR), three classification scenarios with varying difficulty and data size
are evaluated. Models include various SLMs and an LLM. SLMs are evaluated both
zero-shot and finetuned; the LLM is evaluated zero-shot only. Finetuning
significantly improved SLM performance across all scenarios compared to their
zero-shot results. The zero-shot LLM outperformed zero-shot SLMs but was
consistently outperformed by finetuned SLMs. Domain-adjacent SLMs generally
performed better than the generic SLM after finetuning, especially on harder
tasks. Further domain-specific pretraining yielded modest gains on easier tasks
but significant improvements on the complex, data-scarce task. The results
highlight the critical role of finetuning for SLMs in specialized domains,
enabling them to surpass zero-shot LLM performance on targeted classification
tasks. Pretraining on domain-adjacent or domain-specific data provides further
advantages, particularly for complex problems or limited finetuning data. While
LLMs offer strong zero-shot capabilities, their performance on these specific
tasks did not match that of appropriately finetuned SLMs. In the era of LLMs,
SLMs remain relevant and effective, offering a potentially superior
performance-resource trade-off compared to LLMs.

</details>


### [18] [Automatic Legal Writing Evaluation of LLMs](https://arxiv.org/abs/2504.21202)
*Ramon Pires,Roseval Malaquias Junior,Rodrigo Nogueira*

Main category: cs.CL

TL;DR: 该论文提出了一个名为oab-bench的基准测试，用于评估大语言模型在巴西律师考试中的表现，并探讨了这些模型作为自动评分者的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于法律写作评估的复杂性和缺乏公共测试数据集，作者开发了oab-bench，以满足对公开、更新频繁且包含全面评估指南的领域特定任务测试数据的需求。

Method: 作者从巴西律师考试中选取了105个问题，涵盖七个法律领域，并结合详细的评估指南和参考材料，测试了四种大语言模型的性能，并使用前沿模型作为自动评分者。

Result: Claude-3.5 Sonnet在测试中表现最佳，平均得分为7.93（满分10），并通过了所有21项考试。前沿模型（如OpenAI的o1）在评分时与人类评委分数的相关性较高。

Conclusion: 尽管法律写作评估具有主观性，但大语言模型表现出作为可靠自动评分者的潜力，oab-bench及其相关资源已公开供进一步研究使用。

Abstract: Despite the recent advances in Large Language Models, benchmarks for
evaluating legal writing remain scarce due to the inherent complexity of
assessing open-ended responses in this domain. One of the key challenges in
evaluating language models on domain-specific tasks is finding test datasets
that are public, frequently updated, and contain comprehensive evaluation
guidelines. The Brazilian Bar Examination meets these requirements. We
introduce oab-bench, a benchmark comprising 105 questions across seven areas of
law from recent editions of the exam. The benchmark includes comprehensive
evaluation guidelines and reference materials used by human examiners to ensure
consistent grading. We evaluate the performance of four LLMs on oab-bench,
finding that Claude-3.5 Sonnet achieves the best results with an average score
of 7.93 out of 10, passing all 21 exams. We also investigated whether LLMs can
serve as reliable automated judges for evaluating legal writing. Our
experiments show that frontier models like OpenAI's o1 achieve a strong
correlation with human scores when evaluating approved exams, suggesting their
potential as reliable automated evaluators despite the inherently subjective
nature of legal writing assessment. The source code and the benchmark --
containing questions, evaluation guidelines, model-generated responses, and
their respective automated evaluations -- are publicly available.

</details>


### [19] [Pretraining Large Brain Language Model for Active BCI: Silent Speech](https://arxiv.org/abs/2504.21214)
*Jinzhao Zhou,Zehong Cao,Yiqun Duan,Connor Barkley,Daniel Leong,Xiaowei Jiang,Quoc-Toan Nguyen,Ziyi Zhao,Thomas Do,Yu-Cheng Chang,Sheng-Fu Liang,Chin-teng Lin*

Main category: cs.CL

TL;DR: 论文提出了一种名为LBLM的大型脑语言模型，利用自监督预训练（FSTP方法）解码静默语音，显著提升了BCI系统的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的BCI系统在自然性和灵活性上有限，需要更先进的静默语音解码技术以改善通信。

Method: 采用FSTP预训练方法，通过自回归建模从EEG信号中学习时域和频域表示，并微调至词级和语义级分类任务。

Result: LBLM在跨会话设置中，语义级分类准确率47.0%，词级分类39.6%，分别比基线方法高5.4%和7.3%。

Conclusion: 该研究为BCI静默语音解码提供了新方法，并贡献了一个新的EEG数据集。

Abstract: This paper explores silent speech decoding in active brain-computer interface
(BCI) systems, which offer more natural and flexible communication than
traditional BCI applications. We collected a new silent speech dataset of over
120 hours of electroencephalogram (EEG) recordings from 12 subjects, capturing
24 commonly used English words for language model pretraining and decoding.
Following the recent success of pretraining large models with self-supervised
paradigms to enhance EEG classification performance, we propose Large Brain
Language Model (LBLM) pretrained to decode silent speech for active BCI. To
pretrain LBLM, we propose Future Spectro-Temporal Prediction (FSTP) pretraining
paradigm to learn effective representations from unlabeled EEG data. Unlike
existing EEG pretraining methods that mainly follow a masked-reconstruction
paradigm, our proposed FSTP method employs autoregressive modeling in temporal
and frequency domains to capture both temporal and spectral dependencies from
EEG signals. After pretraining, we finetune our LBLM on downstream tasks,
including word-level and semantic-level classification. Extensive experiments
demonstrate significant performance gains of the LBLM over fully-supervised and
pretrained baseline models. For instance, in the difficult cross-session
setting, our model achieves 47.0\% accuracy on semantic-level classification
and 39.6\% in word-level classification, outperforming baseline methods by
5.4\% and 7.3\%, respectively. Our research advances silent speech decoding in
active BCI systems, offering an innovative solution for EEG language model
pretraining and a new dataset for fundamental research.

</details>


### [20] [Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math](https://arxiv.org/abs/2504.21233)
*Haoran Xu,Baolin Peng,Hany Awadalla,Dongdong Chen,Yen-Chun Chen,Mei Gao,Young Jin Kim,Yunsheng Li,Liliang Ren,Yelong Shen,Shuohang Wang,Weijian Xu,Jianfeng Gao,Weizhu Chen*

Main category: cs.CL

TL;DR: 该论文提出了一种系统化的训练方法，用于提升小语言模型（SLMs）的推理能力，通过四步训练食谱，使得3.8B参数的小模型在数学推理任务上超越更大的模型。


<details>
  <summary>Details</summary>
Motivation: 研究发现虽然大语言模型（LLMs）能通过思维链（CoT）显著提升推理能力，但小语言模型（SLMs）由于模型容量有限，推理能力提升较难。本文旨在通过系统化的训练方法解决这一问题。

Method: 论文提出了一种四步训练方法：（1）多样化的长思维链数据的大规模中间训练，（2）高质量长思维链数据的监督微调，（3）利用精选偏好数据集的Rollout DPO，（4）带可验证奖励的强化学习（RL）。

Result: 在Phi-4-Mini（3.8B参数模型）上应用该方法后，生成的Phi-4-Mini-Reasoning在数学推理任务上的表现超越更大模型，如Math-500上分别比DeepSeek-R1-Distill-Qwen-7B和DeepSeek-R1-Distill-Llama-8B高出3.2和7.7分。

Conclusion: 研究表明，精心设计的高质量思维链数据训练食谱可有效解锁小模型的强大推理能力，即使在资源受限的情况下。

Abstract: Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities
in Large Language Models (LLMs) by training them to explicitly generate
intermediate reasoning steps. While LLMs readily benefit from such techniques,
improving reasoning in Small Language Models (SLMs) remains challenging due to
their limited model capacity. Recent work by Deepseek-R1 demonstrates that
distillation from LLM-generated synthetic data can substantially improve the
reasoning ability of SLM. However, the detailed modeling recipe is not
disclosed. In this work, we present a systematic training recipe for SLMs that
consists of four steps: (1) large-scale mid-training on diverse distilled
long-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3)
Rollout DPO leveraging a carefully curated preference dataset, and (4)
Reinforcement Learning (RL) with Verifiable Reward. We apply our method on
Phi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning
model exceeds, on math reasoning tasks, much larger reasoning models, e.g.,
outperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and
DeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate
that a carefully designed training recipe, with large-scale high-quality CoT
data, is effective to unlock strong reasoning capabilities even in
resource-constrained small models.

</details>


### [21] [Memorization and Knowledge Injection in Gated LLMs](https://arxiv.org/abs/2504.21239)
*Xu Pan,Ely Hahami,Zechen Zhang,Haim Sompolinsky*

Main category: cs.CL

TL;DR: 该论文提出了MEGa框架，通过将记忆直接嵌入LLM的权重中，实现连续学习，解决了LLM难以整合新知识的问题，并在实验中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）难以连续添加记忆和整合新知识，与人类持续学习能力形成对比。研究旨在通过模仿人脑互补记忆系统，提升LLM的持续学习能力。

Method: 提出MEGa框架，将事件记忆嵌入LLM的权重中，采用低秩权重和门控机制，通过查询与记忆嵌入匹配激活相关记忆权重。

Result: 在虚构角色和维基百科事件数据集上，MEGa在缓解灾难性遗忘方面表现优于基线方法。

Conclusion: MEGa通过权重嵌入和门控机制，有效实现了LLM的连续学习，为LLM的知识更新提供了新思路。

Abstract: Large Language Models (LLMs) currently struggle to sequentially add new
memories and integrate new knowledge. These limitations contrast with the human
ability to continuously learn from new experiences and acquire knowledge
throughout life. Most existing approaches add memories either through large
context windows or external memory buffers (e.g., Retrieval-Augmented
Generation), and studies on knowledge injection rarely test scenarios
resembling everyday life events. In this work, we introduce a continual
learning framework, Memory Embedded in Gated LLMs (MEGa), which injects event
memories directly into the weights of LLMs. Each memory is stored in a
dedicated set of gated low-rank weights. During inference, a gating mechanism
activates relevant memory weights by matching query embeddings to stored memory
embeddings. This enables the model to both recall entire memories and answer
related questions. On two datasets - fictional characters and Wikipedia events
- MEGa outperforms baseline approaches in mitigating catastrophic forgetting.
Our model draws inspiration from the complementary memory system of the human
brain.

</details>


### [22] [Talk Before You Retrieve: Agent-Led Discussions for Better RAG in Medical QA](https://arxiv.org/abs/2504.21252)
*Xuanzhao Dong,Wenhui Zhu,Hao Wang,Xiwen Chen,Peijie Qiu,Rui Yin,Yi Su,Yalin Wang*

Main category: cs.CL

TL;DR: Discuss-RAG enhances medical QA by using agent-based reasoning and improved retrieval, achieving significant accuracy gains on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Current medical RAG systems struggle with irrelevant retrievals and lack human-like reasoning, highlighting the need for better modeling and corpus quality.

Method: Introduces a summarizer agent for multi-turn brainstorming and a decision-making agent to filter retrieved content, improving relevance and accuracy.

Result: Discuss-RAG outperforms MedRAG, boosting accuracy by up to 16.67% on BioASQ and 12.20% on PubMedQA.

Conclusion: Agent-based reasoning and enhanced retrieval significantly improve medical QA performance, demonstrating the potential of collaborative RAG frameworks.

Abstract: Medical question answering (QA) is a reasoning-intensive task that remains
challenging for large language models (LLMs) due to hallucinations and outdated
domain knowledge. Retrieval-Augmented Generation (RAG) provides a promising
post-training solution by leveraging external knowledge. However, existing
medical RAG systems suffer from two key limitations: (1) a lack of modeling for
human-like reasoning behaviors during information retrieval, and (2) reliance
on suboptimal medical corpora, which often results in the retrieval of
irrelevant or noisy snippets. To overcome these challenges, we propose
Discuss-RAG, a plug-and-play module designed to enhance the medical QA RAG
system through collaborative agent-based reasoning. Our method introduces a
summarizer agent that orchestrates a team of medical experts to emulate
multi-turn brainstorming, thereby improving the relevance of retrieved content.
Additionally, a decision-making agent evaluates the retrieved snippets before
their final integration. Experimental results on four benchmark medical QA
datasets show that Discuss-RAG consistently outperforms MedRAG, especially
significantly improving answer accuracy by up to 16.67% on BioASQ and 12.20% on
PubMedQA. The code is available at: https://github.com/LLM-VLM-GSL/Discuss-RAG.

</details>


### [23] [BiasGuard: A Reasoning-enhanced Bias Detection Tool For Large Language Models](https://arxiv.org/abs/2504.21299)
*Zhiting Fan,Ruizhe Chen,Zuozhu Liu*

Main category: cs.CL

TL;DR: 文章介绍了一个新型偏倚检测工具BiasGuard，通过两阶段方法提高LLM生成内容的公平性判断准确性并减少误判。


<details>
  <summary>Details</summary>
Motivation: 为了改进现有公平性分类器和LLM评判工具的局限性，如难以理解意图和缺乏公平标准。

Method: 采用两阶段方法: 第一阶段基于公平规范显式推理初始化模型，第二阶段通过强化学习增强推理和判断能力。

Result: 在五个数据集上的实验表明，BiasGuard优于现有工具，提升了准确性并减少了过公平误判。

Conclusion: 证明了推理增强决策的重要性以及两阶段优化管道的有效性。

Abstract: Identifying bias in LLM-generated content is a crucial prerequisite for
ensuring fairness in LLMs. Existing methods, such as fairness classifiers and
LLM-based judges, face limitations related to difficulties in understanding
underlying intentions and the lack of criteria for fairness judgment. In this
paper, we introduce BiasGuard, a novel bias detection tool that explicitly
analyzes inputs and reasons through fairness specifications to provide accurate
judgments. BiasGuard is implemented through a two-stage approach: the first
stage initializes the model to explicitly reason based on fairness
specifications, while the second stage leverages reinforcement learning to
enhance its reasoning and judgment capabilities. Our experiments, conducted
across five datasets, demonstrate that BiasGuard outperforms existing tools,
improving accuracy and reducing over-fairness misjudgments. We also highlight
the importance of reasoning-enhanced decision-making and provide evidence for
the effectiveness of our two-stage optimization pipeline.

</details>


### [24] [Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges](https://arxiv.org/abs/2504.21303)
*Xiao Xiao,Yu Su,Sijing Zhang,Zhang Chen,Yadong Chen,Tian Liu*

Main category: cs.CL

TL;DR: 该研究提出了一种基于贝叶斯方法的语言模型评估框架，通过概率推理整合先验知识，解决了传统确定性指标在小样本情况下的局限性，并在实验中验证了其优势。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法依赖确定性指标，无法充分捕捉语言模型的概率性输出特征，尤其是在小样本情况下表现不佳。因此，研究希望通过贝叶斯方法提升评估的鲁棒性和实用性。

Method: 将模型能力视为潜在变量，通过精心设计的查询集获取判别性响应，并将模型排名问题形式化为贝叶斯假设检验。

Result: 实验表明，该方法在GPT系列模型上表现优于传统方法，即使样本量减少也能保持统计鲁棒性，并提供模型超越特定基线的概率性结论。

Conclusion: 该研究通过贝叶斯推理与实践约束的结合，推动了语言模型评估方法的发展，为实际部署场景提供了更可靠的评估工具。

Abstract: Large language models (LLMs) exhibit probabilistic output characteristics,
yet conventional evaluation frameworks rely on deterministic scalar metrics.
This study introduces a Bayesian approach for LLM capability assessment that
integrates prior knowledge through probabilistic inference, addressing
limitations under limited-sample regimes. By treating model capabilities as
latent variables and leveraging a curated query set to induce discriminative
responses, we formalize model ranking as a Bayesian hypothesis testing problem
over mutually exclusive capability intervals. Experimental evaluations with
GPT-series models demonstrate that the proposed method achieves superior
discrimination compared to conventional evaluation methods. Results indicate
that even with reduced sample sizes, the approach maintains statistical
robustness while providing actionable insights, such as probabilistic
statements about a model's likelihood of surpassing specific baselines. This
work advances LLM evaluation methodologies by bridging Bayesian inference with
practical constraints in real-world deployment scenarios.

</details>


### [25] [Does the Prompt-based Large Language Model Recognize Students' Demographics and Introduce Bias in Essay Scoring?](https://arxiv.org/abs/2504.21330)
*Kaixun Yang,Mladen Raković,Dragan Gašević,Guanliang Chen*

Main category: cs.CL

TL;DR: 论文研究了基于提示的大型语言模型（LLM）在自动作文评分（AES）中的偏见问题，发现模型能通过作文推断学生背景（如母语），且对非母语者的评分偏见更明显。


<details>
  <summary>Details</summary>
Motivation: 探讨基于提示的LLM在AES中是否延续或放大了传统微调方法中的偏见，尤其是针对弱势群体的偏见。

Method: 使用包含2.5万篇议论文的公开数据集，设计提示让GPT-4o推断学生背景（如性别、母语），并评估评分的公平性，通过多元回归分析模型预测能力对评分结果的影响。

Result: 研究表明：(i) LLM能部分推断学生背景（尤其是母语）；(ii) 当LLM正确预测母语背景时，评分偏见更显著；(iii) 对非母语者的评分误差在其身份被正确识别时更高。

Conclusion: 提示式LLM在AES中存在基于学生背景的评分偏见，需进一步研究以缓解这一问题。

Abstract: Large Language Models (LLMs) are widely used in Automated Essay Scoring (AES)
due to their ability to capture semantic meaning. Traditional fine-tuning
approaches required technical expertise, limiting accessibility for educators
with limited technical backgrounds. However, prompt-based tools like ChatGPT
have made AES more accessible, enabling educators to obtain machine-generated
scores using natural-language prompts (i.e., the prompt-based paradigm).
Despite advancements, prior studies have shown bias in fine-tuned LLMs,
particularly against disadvantaged groups. It remains unclear whether such
biases persist or are amplified in the prompt-based paradigm with cutting-edge
tools. Since such biases are believed to stem from the demographic information
embedded in pre-trained models (i.e., the ability of LLMs' text embeddings to
predict demographic attributes), this study explores the relationship between
the model's predictive power of students' demographic attributes based on their
written works and its predictive bias in the scoring task in the prompt-based
paradigm. Using a publicly available dataset of over 25,000 students'
argumentative essays, we designed prompts to elicit demographic inferences
(i.e., gender, first-language background) from GPT-4o and assessed fairness in
automated scoring. Then we conducted multivariate regression analysis to
explore the impact of the model's ability to predict demographics on its
scoring outcomes. Our findings revealed that (i) prompt-based LLMs can somewhat
infer students' demographics, particularly their first-language backgrounds,
from their essays; (ii) scoring biases are more pronounced when the LLM
correctly predicts students' first-language background than when it does not;
and (iii) scoring error for non-native English speakers increases when the LLM
correctly identifies them as non-native.

</details>


### [26] [Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction](https://arxiv.org/abs/2504.21372)
*Máté Gedeon*

Main category: cs.CL

TL;DR: 论文提出了一种模块化的语音事件提取框架SpeechEE，结合高性能ASR和基于语义检索增强的LLM提示，显著提升了事件触发和参数分类的性能。


<details>
  <summary>Details</summary>
Motivation: 语音事件提取任务位于ASR和NLP交叉领域，传统方法面临挑战。本文旨在通过模块化流水线和检索增强的LLM提示，提升性能并保持可解释性。

Method: 采用混合过滤机制（规则、BERT、LLM）分类语音片段，再通过动态语义检索增强的少样本LLM提示提取事件触发词和参数。

Result: 实验显示o1-mini模型表现最佳，触发词分类F1达63.3%，参数分类F1达27.8%，超越现有基准。

Conclusion: 流水线方法结合检索增强的LLM可匹敌端到端系统，同时保持模块化和可解释性，为未来结合文本与声学特征的混合模型提供了方向。

Abstract: Speech Event Extraction (SpeechEE) is a challenging task that lies at the
intersection of Automatic Speech Recognition (ASR) and Natural Language
Processing (NLP), requiring the identification of structured event information
from spoken language. In this work, we present a modular, pipeline-based
SpeechEE framework that integrates high-performance ASR with semantic
search-enhanced prompting of Large Language Models (LLMs). Our system first
classifies speech segments likely to contain events using a hybrid filtering
mechanism including rule-based, BERT-based, and LLM-based models. It then
employs few-shot LLM prompting, dynamically enriched via semantic similarity
retrieval, to identify event triggers and extract corresponding arguments. We
evaluate the pipeline using multiple LLMs (Llama3-8B, GPT-4o-mini, and o1-mini)
highlighting significant performance gains with o1-mini, which achieves 63.3%
F1 on trigger classification and 27.8% F1 on argument classification,
outperforming prior benchmarks. Our results demonstrate that pipeline
approaches, when empowered by retrieval-augmented LLMs, can rival or exceed
end-to-end systems while maintaining interpretability and modularity. This work
provides practical insights into LLM-driven event extraction and opens pathways
for future hybrid models combining textual and acoustic features.

</details>


### [27] [The Distribution of Dependency Distance and Hierarchical Distance in Contemporary Written Japanese and Its Influencing Factors](https://arxiv.org/abs/2504.21421)
*Linxuan Wang,Shuiyuan Yu*

Main category: cs.CL

TL;DR: 该研究探讨了日语中依赖距离（DD）和层次距离（HD）的关系，发现谓词的配价是调节两者权衡关系的核心因素，并影响其概率分布和均值差异。


<details>
  <summary>Details</summary>
Motivation: 日语中依赖距离和层次距离的关系尚未完全明确，尤其是在句子长度变化和谓词配价的影响下。研究旨在揭示这一关系的认知和语言机制。

Method: 通过分析日语平衡语料库，固定句子长度比较DD和HD的概率分布，并计算MDD和MHD随句子长度的变化及相关性。

Result: 谓词配价是MDD和MHD权衡关系的关键因素，其影响在HD的分布上比DD更显著，导致MDD均值低于MHD。

Conclusion: 日语母语者通过谓词配价调节线性和层次复杂度，配价阈值决定了MDD和MHD的相对大小，认知负荷外，配价还直接影响了DD和HD的分布差异。

Abstract: To explore the relationship between dependency distance (DD) and hierarchical
distance (HD) in Japanese, we compared the probability distributions of DD and
HD with and without sentence length fixed, and analyzed the changes in mean
dependency distance (MDD) and mean hierarchical distance (MHD) as sentence
length increases, along with their correlation coefficient based on the
Balanced Corpus of Contemporary Written Japanese. It was found that the valency
of the predicates is the underlying factor behind the trade-off relation
between MDD and MHD in Japanese. Native speakers of Japanese regulate the
linear complexity and hierarchical complexity through the valency of the
predicates, and the relative sizes of MDD and MHD depend on whether the
threshold of valency has been reached. Apart from the cognitive load, the
valency of the predicates also affects the probability distributions of DD and
HD. The effect of the valency of the predicates on the distribution of HD is
greater than on that of DD, which leads to differences in their probability
distributions and causes the mean of MDD to be lower than that of MHD.

</details>


### [28] [RWKV-X: A Linear Complexity Hybrid Language Model](https://arxiv.org/abs/2504.21463)
*Haowen Hou,Zhiyi Huang,Kaifeng Tan,Rongchang Lu,Fei Richard Yu*

Main category: cs.CL

TL;DR: 论文介绍了RWKV-X，一种结合RWKV短程建模效率和稀疏注意力机制的混合架构，实现了线性和常数时间复杂度的训练与推理，并在64K标记序列上表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决传统混合方法因全注意力层导致的二次复杂度问题，提出更高效的模型以支持长上下文建模。

Method: 结合RWKV的短程建模能力和稀疏注意力机制，优化训练和推理的时间复杂度。

Result: 在64K标记序列上达到近乎完美的准确性，长上下文任务表现优于RWKV-7，且短上下文任务性能稳定。

Conclusion: RWKV-X是高效且可扩展的通用语言建模框架，支持百万标记序列解码，并公开了代码和模型。

Abstract: In this paper, we introduce \textbf{RWKV-X}, a novel hybrid architecture that
combines the efficiency of RWKV for short-range modeling with a sparse
attention mechanism designed to capture long-range context. Unlike previous
hybrid approaches that rely on full attention layers and retain quadratic
complexity, RWKV-X achieves linear-time complexity in training and
constant-time complexity in inference decoding. We demonstrate that RWKV-X,
when continually pretrained on 64K-token sequences, achieves near-perfect
accuracy on the 64K passkey retrieval benchmark. It consistently outperforms
prior RWKV-7 models on long-context benchmarks, while maintaining strong
performance on short-context tasks. These results highlight RWKV-X as a
scalable and efficient backbone for general-purpose language modeling, capable
of decoding sequences up to 1 million tokens with stable speed and memory
usage. To facilitate further research and analysis, we have made the
checkpoints and the associated code publicly accessible at:
https://github.com/howard-hou/RWKV-X.

</details>


### [29] [Homa at SemEval-2025 Task 5: Aligning Librarian Records with OntoAligner for Subject Tagging](https://arxiv.org/abs/2504.21474)
*Hadi Bayrami Asl Tekanlou,Jafar Razmara,Mahsa Sanaei,Mostafa Rahgouy,Hamed Babaei Giglou*

Main category: cs.CL

TL;DR: 简短概述：Homa系统通过OntoAligner工具包和RAG技术，将主题标记任务转化为对齐问题，评估其在多语言记录中的效果。


<details>
  <summary>Details</summary>
Motivation: 动机：解决TIBKAT技术记录中自动分配GND分类主题标签的挑战，提升数字图书馆的主题标记效率。

Method: 方法：使用OntoAligner工具包结合RAG技术，通过语义相似性匹配记录与GND分类。

Result: 结果：展示了该方法在多语言记录中的优势与局限，验证了对齐技术在主题标记中的潜力。

Conclusion: 结论：对齐技术为数字图书馆主题标记提供了有效改进方向。

Abstract: This paper presents our system, Homa, for SemEval-2025 Task 5: Subject
Tagging, which focuses on automatically assigning subject labels to technical
records from TIBKAT using the Gemeinsame Normdatei (GND) taxonomy. We leverage
OntoAligner, a modular ontology alignment toolkit, to address this task by
integrating retrieval-augmented generation (RAG) techniques. Our approach
formulates the subject tagging problem as an alignment task, where records are
matched to GND categories based on semantic similarity. We evaluate
OntoAligner's adaptability for subject indexing and analyze its effectiveness
in handling multilingual records. Experimental results demonstrate the
strengths and limitations of this method, highlighting the potential of
alignment techniques for improving subject tagging in digital libraries.

</details>


### [30] [Advancing Arabic Reverse Dictionary Systems: A Transformer-Based Approach with Dataset Construction Guidelines](https://arxiv.org/abs/2504.21475)
*Serry Sibaee,Samar Ahmed,Abdullah Al Harbi,Omer Nacar,Adel Ammar,Yasser Habashi,Wadii Boulila*

Main category: cs.CL

TL;DR: 该研究开发了一种基于变压器的阿拉伯语反向词典系统，通过半编码器神经网络架构和几何递减层实现最先进性能，为阿拉伯语自然语言处理填补了重要空白。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语自然语言处理中反向词典任务的缺失问题，提供高效的工具支持语言学习、学术写作和专业交流。

Method: 采用基于变压器的半编码器神经网络架构，结合阿拉伯语特定预训练模型（如ARBERTv2）和综合性数据集构建，提出了八项定义构建标准。

Result: ARBERTv2模型在反向词典任务中表现最佳（排名得分0.0644），并开发了可扩展的Python库（RDTL）和数据集质量分析工具。

Conclusion: 该研究不仅提升了阿拉伯语反向词典的实践能力，还通过理论抽象和工具开发为阿拉伯语计算语言学作出了重要贡献。

Abstract: This study addresses the critical gap in Arabic natural language processing
by developing an effective Arabic Reverse Dictionary (RD) system that enables
users to find words based on their descriptions or meanings. We present a novel
transformer-based approach with a semi-encoder neural network architecture
featuring geometrically decreasing layers that achieves state-of-the-art
results for Arabic RD tasks. Our methodology incorporates a comprehensive
dataset construction process and establishes formal quality standards for
Arabic lexicographic definitions. Experiments with various pre-trained models
demonstrate that Arabic-specific models significantly outperform general
multilingual embeddings, with ARBERTv2 achieving the best ranking score
(0.0644). Additionally, we provide a formal abstraction of the reverse
dictionary task that enhances theoretical understanding and develop a modular,
extensible Python library (RDTL) with configurable training pipelines. Our
analysis of dataset quality reveals important insights for improving Arabic
definition construction, leading to eight specific standards for building
high-quality reverse dictionary resources. This work contributes significantly
to Arabic computational linguistics and provides valuable tools for language
learning, academic writing, and professional communication in Arabic.

</details>


### [31] [Improving Informally Romanized Language Identification](https://arxiv.org/abs/2504.21540)
*Adrian Benton,Alexander Gutkin,Christo Kirov,Brian Roark*

Main category: cs.CL

TL;DR: 通过改进训练数据合成方法，该研究提升了拉丁化文本的语言识别准确率，尤其在涵盖自然拼写变体的情况下表现更优。


<details>
  <summary>Details</summary>
Motivation: 拉丁化文本中存在拼写高变异性，导致语言识别困难（如印地语和乌尔都语混淆）。研究旨在提高此类情况下的语言识别准确率。

Method: 通过合成包含自然拼写变体的训练样本，并与现有自然样本结合训练线性分类器。

Result: 在20种印度语言的评估集上，测试F1从74.7%提升至88.2%，创下新最优性能。

Conclusion: 合成数据结合自然样本可显著提升拉丁化文本的语言识别效果，且简单线性分类器即可实现高性能。

Abstract: The Latin script is often used to informally write languages with non-Latin
native scripts. In many cases (e.g., most languages in India), there is no
conventional spelling of words in the Latin script, hence there will be high
spelling variability in written text. Such romanization renders languages that
are normally easily distinguished based on script highly confusable, such as
Hindi and Urdu. In this work, we increase language identification (LID)
accuracy for romanized text by improving the methods used to synthesize
training sets. We find that training on synthetic samples which incorporate
natural spelling variation yields higher LID system accuracy than including
available naturally occurring examples in the training set, or even training
higher capacity models. We demonstrate new state-of-the-art LID performance on
romanized text from 20 Indic languages in the Bhasha-Abhijnaanam evaluation set
(Madhani et al., 2023a), improving test F1 from the reported 74.7% (using a
pretrained neural model) to 85.4% using a linear classifier trained solely on
synthetic data and 88.2% when also training on available harvested text.

</details>


### [32] [TartuNLP at SemEval-2025 Task 5: Subject Tagging as Two-Stage Information Retrieval](https://arxiv.org/abs/2504.21547)
*Aleksei Dorkin,Kairit Sirts*

Main category: cs.CL

TL;DR: 本文提出了一种两阶段信息检索系统，用于为图书馆记录分配主题标签。该系统结合了双编码器和交叉编码器，显著提高了召回率。


<details>
  <summary>Details</summary>
Motivation: 旨在帮助图书馆员为图书馆记录分配主题标签，通过文档内容从大型主题分类中检索相关标签。

Method: 将任务构建为信息检索问题，采用两阶段系统：第一阶段使用双编码器进行粗粒度候选标签提取，第二阶段使用交叉编码器进行细粒度重排序。

Result: 该方法显著提高了召回率，且在定性评估中表现竞争性。

Conclusion: 两阶段信息检索系统在主题标签分配任务中有效且优于单阶段方法。

Abstract: We present our submission to the Task 5 of SemEval-2025 that aims to aid
librarians in assigning subject tags to the library records by producing a list
of likely relevant tags for a given document. We frame the task as an
information retrieval problem, where the document content is used to retrieve
subject tags from a large subject taxonomy. We leverage two types of encoder
models to build a two-stage information retrieval system -- a bi-encoder for
coarse-grained candidate extraction at the first stage, and a cross-encoder for
fine-grained re-ranking at the second stage. This approach proved effective,
demonstrating significant improvements in recall compared to single-stage
methods and showing competitive results according to qualitative evaluation.

</details>


### [33] [Precision Where It Matters: A Novel Spike Aware Mixed-Precision Quantization Strategy for LLaMA-based Language Models](https://arxiv.org/abs/2504.21553)
*Lucas Maisonnave,Cyril Moineau,Olivier Bichler,Fabrice Rastello*

Main category: cs.CL

TL;DR: 该论文提出了一种针对LLaMA架构的新型混合精度量化方法，专注于处理激活异常值的特定投影层，通过在这些层使用更高精度（FP16或FP8），其余部分使用低比特量化，显著提升了LLM的性能和部署效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在任务中表现出色，但其规模导致部署和推理的挑战。本研究的动机是通过量化技术优化LLM，尤其是在LLaMA架构中处理激活异常值的问题，以提高模型效率和可部署性。

Method: 提出了一种混合精度量化方法，针对LLaMA架构的特定投影层（存在激活峰值）使用更高精度（FP16或FP8），其余部分则量化为更低比特。

Result: 在LLaMA2、LLaMA3和Mistral模型上的实验表明，该方法在困惑度和零样本准确率上优于通用量化技术，尤其是在8比特张量量化中表现突出。

Conclusion: 该研究强调了结合模型特异性设计量化策略的重要性，为LLM的高效部署提供了新方向，尤其适合资源受限的环境。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various natural language processing tasks. However, their size presents
significant challenges for deployment and inference. This paper investigates
the quantization of LLMs, focusing on the LLaMA architecture and its
derivatives. We challenge existing assumptions about activation outliers in
LLMs and propose a novel mixed-precision quantization approach tailored for
LLaMA-like models. Our method leverages the observation that activation spikes
in LLaMA architectures are predominantly concentrated in specific projection
layers. By applying higher precision (FP16 or FP8) to these layers while
quantizing the rest of the model to lower bit-widths, we achieve superior
performance compared to existing quantization techniques. Experimental results
on LLaMA2, LLaMA3, and Mistral models demonstrate significant improvements in
perplexity and zero-shot accuracy, particularly for 8-bit per-tensor
quantization. Our approach outperforms general-purpose methods designed to
handle outliers across all architecture types, highlighting the benefits of
architecture-specific quantization strategies. This research contributes to the
ongoing efforts to make LLMs more efficient and deployable, potentially
enabling their use in resource-constrained environments. Our findings emphasize
the importance of considering model-specific characteristics in developing
effective quantization pipelines for state-of-the-art language models by
identifying and targeting a small number of projections that concentrate
activation spikes.

</details>


### [34] [DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for Automated Subject Indexing](https://arxiv.org/abs/2504.21589)
*Lisa Kluge,Maximilian Kähler*

Main category: cs.CL

TL;DR: 论文介绍了为SemEval-2025 Task 5开发的系统，通过少样本提示LLMs生成关键词，结合后处理步骤在技术图书馆开放目录中实现自动化主题标注。系统在量化排名中第四，但在专家质评中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 旨在利用LLMs技术自动化处理国家技术图书馆开放目录的主题标注任务，减少人工标注成本并提升效率。

Method: 采用少样本提示多种LLMs生成关键词，后经词汇映射、集成投票和相关性排序等后处理步骤优化结果。

Result: 系统在全部主题赛道的量化排名中位列第四，但在专家定性评估中表现最优。

Conclusion: 结合LLMs与后处理的混合方法在自动化主题标注中展现出质效优势，未来可进一步优化模型选择和集成策略。

Abstract: This paper presents our system developed for the SemEval-2025 Task 5:
LLMs4Subjects: LLM-based Automated Subject Tagging for a National Technical
Library's Open-Access Catalog. Our system relies on prompting a selection of
LLMs with varying examples of intellectually annotated records and asking the
LLMs to similarly suggest keywords for new records. This few-shot prompting
technique is combined with a series of post-processing steps that map the
generated keywords to the target vocabulary, aggregate the resulting subject
terms to an ensemble vote and, finally, rank them as to their relevance to the
record. Our system is fourth in the quantitative ranking in the all-subjects
track, but achieves the best result in the qualitative ranking conducted by
subject indexing experts.

</details>


### [35] [Robust Misinformation Detection by Visiting Potential Commonsense Conflict](https://arxiv.org/abs/2504.21604)
*Bing Wang,Ximing Li,Changchun Li,Bingrui Zhao,Bo Fu,Renchu Guan,Shengsheng Wang*

Main category: cs.CL

TL;DR: 论文提出了一种名为MD-PCC的新型插拔式增强方法，通过利用常识冲突检测网络谣言，并在新数据集CoMis上验证其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 互联网技术的发展导致虚假信息泛滥，自动检测此类信息的Misinformation Detection (MD)成为研究热点。本研究受先前研究启发，发现虚假文章更可能存在常识冲突，因此提出利用常识冲突增强MD任务。

Method: 提出MD-PCC方法：通过构建文章中的常识表达，捕捉潜在常识冲突（利用COMET工具对比提取的三元组与黄金标准），并将其作为数据增强。同时收集新数据集CoMis（所有虚假文章均基于常识冲突）。

Result: 在4个公共基准数据集和CoMis上测试，MD-PCC与多种现有MD主干模型结合后均表现出色，一致优于基线方法。

Conclusion: MD-PCC通过常识冲突增强显著提升了虚假信息检测性能，验证了常识冲突在MD任务中的有效性，同时新数据集CoMis为未来研究提供了资源。

Abstract: The development of Internet technology has led to an increased prevalence of
misinformation, causing severe negative effects across diverse domains. To
mitigate this challenge, Misinformation Detection (MD), aiming to detect online
misinformation automatically, emerges as a rapidly growing research topic in
the community. In this paper, we propose a novel plug-and-play augmentation
method for the MD task, namely Misinformation Detection with Potential
Commonsense Conflict (MD-PCC). We take inspiration from the prior studies
indicating that fake articles are more likely to involve commonsense conflict.
Accordingly, we construct commonsense expressions for articles, serving to
express potential commonsense conflicts inferred by the difference between
extracted commonsense triplet and golden ones inferred by the well-established
commonsense reasoning tool COMET. These expressions are then specified for each
article as augmentation. Any specific MD methods can be then trained on those
commonsense-augmented articles. Besides, we also collect a novel
commonsense-oriented dataset named CoMis, whose all fake articles are caused by
commonsense conflict. We integrate MD-PCC with various existing MD backbones
and compare them across both 4 public benchmark datasets and CoMis. Empirical
results demonstrate that MD-PCC can consistently outperform the existing MD
baselines.

</details>


### [36] [RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations](https://arxiv.org/abs/2504.21605)
*Jonas Gwozdz,Andreas Both*

Main category: cs.CL

TL;DR: 提出一个基于RDF的框架，评估多语言LLM在知识冲突中的表现，涵盖四种上下文条件，并在德英双语中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以系统评估LLM在信息冲突时的可靠性，需一种结构化方案来量化知识泄漏、错误检测和多语言一致性。

Method: 设计RDF框架，在完全/不完全/冲突/无上下文的四种条件下收集模型响应，通过火灾安全案例进行实验验证。

Result: 框架能全面分析知识泄漏（模型偏好训练数据）和语言差异，28问测试表明其词汇足以覆盖所有评估维度。

Conclusion: 该框架为LLM知识冲突评估提供了可扩展方法，揭示了上下文优先级和语言特性的关键影响。

Abstract: Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet
systematically assessing their reliability with conflicting information remains
difficult. We propose an RDF-based framework to assess multilingual LLM
quality, focusing on knowledge conflicts. Our approach captures model responses
across four distinct context conditions (complete, incomplete, conflicting, and
no-context information) in German and English. This structured representation
enables the comprehensive analysis of knowledge leakage-where models favor
training data over provided context-error detection, and multilingual
consistency. We demonstrate the framework through a fire safety domain
experiment, revealing critical patterns in context prioritization and
language-specific performance, and demonstrating that our vocabulary was
sufficient to express every assessment facet encountered in the 28-question
study.

</details>


### [37] [Meeseeks: An Iterative Benchmark Evaluating LLMs Multi-Turn Instruction-Following Ability](https://arxiv.org/abs/2504.21625)
*Jiaming Wang*

Main category: cs.CL

TL;DR: Meeseeks是一个新的基准测试，通过迭代反馈模拟真实的人-LLM互动，评估LLMs在实际应用中的指令跟随能力。


<details>
  <summary>Details</summary>
Motivation: 现有的指令跟随基准多为单轮或不允许自我修正，无法反映真实的用户端使用模式。Meeseeks旨在填补这一空白，提供更全面的评估。

Method: Meeseeks设计了一个迭代反馈过程，允许模型根据特定需求失败进行自我修正，并包含38个能力标签的评估系统，覆盖三个维度。

Result: 通过严格评估，Meeseeks为LLMs在实际应用中的指令跟随能力提供了有价值的洞察。

Conclusion: Meeseeks提供了一个更贴近实际应用的指令跟随能力评估基准，有助于LLMs作为可靠代理的发展。

Abstract: The ability to follow instructions accurately is fundamental for Large
Language Models (LLMs) to serve as reliable agents in real-world applications.
While existing instruction-following benchmarks are either single-turn or
introduce new requirements in each turn without allowing self-correction,
Meeseeks simulates realistic human-LLM interactions through an iterative
feedback process. This design enables models to self-correct based on specific
requirement failures, better reflecting real-world user-end usage patterns. The
benchmark implements a comprehensive evaluation system with 38 capability tags
organized across three dimensions: Intent Recognition, Granular Content
Validation, and Output Structure Validation. Through rigorous evaluation across
LLMs, Meeseeks provides valuable insights into LLMs' instruction-following
capabilities in practical applications.

</details>


### [38] [Sadeed: Advancing Arabic Diacritization Through Small Language Model](https://arxiv.org/abs/2504.21635)
*Zeina Aldallal,Sara Chrouf,Khalil Hennara,Mohamed Motaism Hamed,Muhammad Hreden,Safwan AlModhayan*

Main category: cs.CL

TL;DR: 论文提出了Sadeed，一种基于微调解码器语言模型的新方法，用于阿拉伯语文本标注，并引入了新基准SadeedDiac-25以改进评估。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语标注的挑战及其在自然语言处理中的重要性，当前方法的不足促使开发更高效和公平的解决方案。

Method: 采用微调的Kuwain 1.5B语言模型，构建高质量的标注数据集，并建立严格的数据清理和规范化流程。

Result: Sadeed在有限计算资源下表现优异，优于传统模型，并提出了更公平的评估基准。

Conclusion: Sadeed和新基准为阿拉伯语的NLP应用提供了更坚实的基础。

Abstract: Arabic text diacritization remains a persistent challenge in natural language
processing due to the language's morphological richness. In this paper, we
introduce Sadeed, a novel approach based on a fine-tuned decoder-only language
model adapted from Kuwain 1.5B Hennara et al. [2025], a compact model
originally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully
curated, high-quality diacritized datasets, constructed through a rigorous
data-cleaning and normalization pipeline. Despite utilizing modest
computational resources, Sadeed achieves competitive results compared to
proprietary large language models and outperforms traditional models trained on
similar domains. Additionally, we highlight key limitations in current
benchmarking practices for Arabic diacritization. To address these issues, we
introduce SadeedDiac-25, a new benchmark designed to enable fairer and more
comprehensive evaluation across diverse text genres and complexity levels.
Together, Sadeed and SadeedDiac-25 provide a robust foundation for advancing
Arabic NLP applications, including machine translation, text-to-speech, and
language learning tools.

</details>


### [39] [20min-XD: A Comparable Corpus of Swiss News Articles](https://arxiv.org/abs/2504.21677)
*Michelle Wastl,Jannis Vamvas,Selena Calleri,Rico Sennrich*

Main category: cs.CL

TL;DR: 20min-XD是一个法语-德语的新闻语料库，包含约1.5万篇文章对，时间跨度为2015至2024年，适用于NLP和语言学研究。


<details>
  <summary>Details</summary>
Motivation: 构建一个跨语言、文档级别的新闻语料库，以支持NLP应用和语言学分析。

Method: 利用语义相似度自动对齐文章对，并详细描述数据收集和对齐方法。

Result: 语料库涵盖了从近似翻译到松散相关文章的广泛跨语言相似性。

Conclusion: 20min-XD数据集公开提供文档和句子对齐版本及相关代码，为研究提供资源。

Abstract: We present 20min-XD (20 Minuten cross-lingual document-level), a
French-German, document-level comparable corpus of news articles, sourced from
the Swiss online news outlet 20 Minuten/20 minutes. Our dataset comprises
around 15,000 article pairs spanning 2015 to 2024, automatically aligned based
on semantic similarity. We detail the data collection process and alignment
methodology. Furthermore, we provide a qualitative and quantitative analysis of
the corpus. The resulting dataset exhibits a broad spectrum of cross-lingual
similarity, ranging from near-translations to loosely related articles, making
it valuable for various NLP applications and broad linguistically motivated
studies. We publicly release the dataset in document- and sentence-aligned
versions and code for the described experiments.

</details>


### [40] [Investigating the Effect of Parallel Data in the Cross-Lingual Transfer for Vision-Language Encoders](https://arxiv.org/abs/2504.21681)
*Andrei-Alexandru Manea,Jindřich Libovický*

Main category: cs.CL

TL;DR: 该论文研究了在预训练视觉-语言模型中，通过平行数据迁移已训练编码器的方法，重点关注平行数据的领域和语言数量对多语言任务的影响，结果表明机器翻译的任务数据效果最佳，但真实平行数据在某些语言中表现更好，且多语言训练对大多数语言有益。


<details>
  <summary>Details</summary>
Motivation: 当前预训练的视觉-语言模型及其下游任务数据多为英文，多语言任务通常依赖跨语言迁移。本研究探讨了另一种方法：利用平行数据迁移已训练编码器，以探索平行数据的领域和语言数量对性能的影响。

Method: 研究通过迁移已训练编码器，并分析不同领域（如机器翻译的任务数据或真实平行数据）和语言数量的平行数据对多语言视觉-语言任务的影响。

Result: 机器翻译的任务数据平均效果最好，但某些语言中真实平行数据表现更优。此外，多语言训练对大多数语言有益。

Conclusion: 平行数据的领域和语言数量对多语言视觉-语言任务的性能有显著影响，机器翻译数据通常有效，但真实数据在某些情况下更优，同时多语言训练能提升模型表现。

Abstract: Most pre-trained Vision-Language (VL) models and training data for the
downstream tasks are only available in English. Therefore, multilingual VL
tasks are solved using cross-lingual transfer: fine-tune a multilingual
pre-trained model or transfer the text encoder using parallel data. We study
the alternative approach: transferring an already trained encoder using
parallel data. We investigate the effect of parallel data: domain and the
number of languages, which were out of focus in previous work. Our results show
that even machine-translated task data are the best on average, caption-like
authentic parallel data outperformed it in some languages. Further, we show
that most languages benefit from multilingual training.

</details>


### [41] [Enhancing Health Mention Classification Performance: A Study on Advancements in Parameter Efficient Tuning](https://arxiv.org/abs/2504.21685)
*Reem Abdel-Salam,Mary Adewunmi*

Main category: cs.CL

TL;DR: 该论文提出了一种结合词性标注信息（POS）和高效微调技术（PEFT）的方法，以优化社交媒体中健康提及分类（HMC）的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 由于社交媒体中的健康提及常包含比喻语言和描述性术语，而非明确反映个人疾病，HMC面临巨大挑战。论文旨在通过改进的生物医学自然语言处理方法解决这一问题。

Method: 采用词性标注信息（POS）和高效微调技术（PEFT），并在三种常用数据集（RHDM、PHM、Illness）上进行实验。

Result: 结合POS和PEFT技术在F1分数上显著优于现有方法，同时使用更小的模型和高效的训练。

Conclusion: 该方法为社交媒体中健康提及的分类提供了一种高效且准确的解决方案，优化了模型大小和训练效率。

Abstract: Health Mention Classification (HMC) plays a critical role in leveraging
social media posts for real-time tracking and public health monitoring.
Nevertheless, the process of HMC presents significant challenges due to its
intricate nature, primarily stemming from the contextual aspects of health
mentions, such as figurative language and descriptive terminology, rather than
explicitly reflecting a personal ailment. To address this problem, we argue
that clearer mentions can be achieved through conventional fine-tuning with
enhanced parameters of biomedical natural language methods (NLP). In this
study, we explore different techniques such as the utilisation of
part-of-speech (POS) tagger information, improving on PEFT techniques, and
different combinations thereof. Extensive experiments are conducted on three
widely used datasets: RHDM, PHM, and Illness. The results incorporated POS
tagger information, and leveraging PEFT techniques significantly improves
performance in terms of F1-score compared to state-of-the-art methods across
all three datasets by utilising smaller models and efficient training.
Furthermore, the findings highlight the effectiveness of incorporating POS
tagger information and leveraging PEFT techniques for HMC. In conclusion, the
proposed methodology presents a potentially effective approach to accurately
classifying health mentions in social media posts while optimising the model
size and training efficiency.

</details>


### [42] [Investigating Literary Motifs in Ancient and Medieval Novels with Large Language Models](https://arxiv.org/abs/2504.21742)
*Emelie Hallenberg*

Main category: cs.CL

TL;DR: 该研究使用微调的大语言模型分析希腊爱情小说中的文学母题，发现部分母题贯穿始终，另一些则频率波动，表明趋势或外部影响。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过大语言模型明确希腊爱情小说中共同的文学母题及其差异，为文学分析提供量化与质性数据。

Method: 应用微调的大语言模型对文本进行母题提取与分析。

Result: 结果显示部分母题持续存在，另一些频率变化，反映趋势或外部影响。

Conclusion: 该方法有效提取文学母题，支持定量与定性分析。

Abstract: The Greek fictional narratives often termed love novels or romances, ranging
from the first century CE to the middle of the 15th century, have long been
considered as similar in many ways, not least in the use of particular literary
motifs. By applying the use of fine-tuned large language models, this study
aims to investigate which motifs exactly that the texts in this corpus have in
common, and in which ways they differ from each other. The results show that
while some motifs persist throughout the corpus, others fluctuate in frequency,
indicating certain trends or external influences. Conclusively, the method
proves to adequately extract literary motifs according to a set definition,
providing data for both quantitative and qualitative analyses.

</details>


### [43] [Improving Retrieval-Augmented Neural Machine Translation with Monolingual Data](https://arxiv.org/abs/2504.21747)
*Maxime Bouthors,Josep Crego,François Yvon*

Main category: cs.CL

TL;DR: 改进跨语言检索系统，利用单语目标语料提升检索增强神经机器翻译的性能，表现优于传统基于翻译记忆的方法。


<details>
  <summary>Details</summary>
Motivation: 利用目标单语语料资源改进检索增强神经机器翻译（RANMT），因为传统方法主要依赖双语语料（如翻译记忆），而目标单语语料在许多场景下更易获取。

Method: 设计结合句子级和词级匹配目标的跨语言检索系统，并在两种RANMT架构中进行实验。

Result: 在可控和真实场景下，新方法性能超越基于翻译记忆的基准模型和通用跨语言检索器。

Conclusion: 通过跨语言目标提升检索增强翻译性能，尤其在目标单语资源远多于平行数据时效果显著。

Abstract: Conventional retrieval-augmented neural machine translation (RANMT) systems
leverage bilingual corpora, e.g., translation memories (TMs). Yet, in many
settings, in-domain monolingual target-side corpora are often available. This
work explores ways to take advantage of such resources by retrieving relevant
segments directly in the target language, based on a source-side query. For
this, we design improved cross-lingual retrieval systems, trained with both
sentence level and word-level matching objectives. In our experiments with two
RANMT architectures, we first demonstrate the benefits of such cross-lingual
objectives in a controlled setting, obtaining translation performances that
surpass standard TM-based models. We then showcase our method on a real-world
set-up, where the target monolingual resources far exceed the amount of
parallel data and observe large improvements of our new techniques, which
outperform both the baseline setting, and general-purpose cross-lingual
retrievers.

</details>


### [44] [MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness](https://arxiv.org/abs/2504.21773)
*Junsheng Huang,Zhitao He,Sandeep Polisetty,Qingyun Wang,May Fung*

Main category: cs.CL

TL;DR: 论文提出MAC-Tuning方法，通过分步学习答案预测和置信度估计，显著提升大语言模型在多问题设置下的自信度估计性能，平均精度提高25%。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在生成内容时存在幻觉问题，且现有研究多关注单一问题设置下的置信度估计。在多问题设置下，模型对自身知识边界的认知仍需探索，因此提出新方法以填补这一空白。

Method: 引入MAC-Tuning方法，在指令数据微调过程中分步学习答案预测和置信度估计，避免两者相互干扰。

Result: 实验表明，MAC-Tuning在平均精度上优于基线方法25%。

Conclusion: MAC-Tuning有效提升模型在多问题设置下的置信度估计能力，为解决幻觉问题提供了新思路。

Abstract: With the widespread application of large language models (LLMs), the issue of
generating non-existing facts, known as hallucination, has garnered increasing
attention. Previous research in enhancing LLM confidence estimation mainly
focuses on the single problem setting. However, LLM awareness of its internal
parameterized knowledge boundary under the more challenging multi-problem
setting, which requires answering multiple problems accurately simultaneously,
remains underexplored. To bridge this gap, we introduce a novel method,
Multiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates
the learning of answer prediction and confidence estimation during fine-tuning
on instruction data. Extensive experiments demonstrate that our method
outperforms baselines by up to 25% in average precision.

</details>


### [45] [WebThinker: Empowering Large Reasoning Models with Deep Research Capability](https://arxiv.org/abs/2504.21776)
*Xiaoxi Li,Jiajie Jin,Guanting Dong,Hongjin Qian,Yutao Zhu,Yongkang Wu,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.CL

TL;DR: WebThinker是一种深度研究代理，通过动态网络搜索和自主思考-搜索-草拟策略，增强大型推理模型（LRMs）在复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LRMs依赖静态内部知识，在复杂、知识密集型任务中表现受限，无法生成需要综合网络信息的全面研究报告。

Method: 结合深度网络探索模块和自主策略，实时交替进行推理、信息收集与报告撰写，并通过强化学习优化训练。

Result: 在多个复杂推理基准和科学报告生成任务中显著超越现有方法及专有系统，提升了LRMs的可靠性和适用性。

Conclusion: WebThinker为更强大、通用的深度研究系统提供了可能，代码已开源。

Abstract: Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate
impressive long-horizon reasoning capabilities. However, their reliance on
static internal knowledge limits their performance on complex,
knowledge-intensive tasks and hinders their ability to produce comprehensive
research reports requiring synthesis of diverse web information. To address
this, we propose \textbf{WebThinker}, a deep research agent that empowers LRMs
to autonomously search the web, navigate web pages, and draft research reports
during the reasoning process. WebThinker integrates a \textbf{Deep Web
Explorer} module, enabling LRMs to dynamically search, navigate, and extract
information from the web when encountering knowledge gaps. It also employs an
\textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to
seamlessly interleave reasoning, information gathering, and report writing in
real time. To further enhance research tool utilization, we introduce an
\textbf{RL-based training strategy} via iterative online Direct Preference
Optimization (DPO). Extensive experiments on complex reasoning benchmarks
(GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive)
demonstrate that WebThinker significantly outperforms existing methods and
strong proprietary systems. Our approach enhances LRM reliability and
applicability in complex scenarios, paving the way for more capable and
versatile deep research systems. The code is available at
https://github.com/RUC-NLPIR/WebThinker.

</details>


### [46] [How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues](https://arxiv.org/abs/2504.21800)
*Suhas BN,Dominik Mattioli,Saeed Abdullah,Rosa I. Arriaga,Chris W. Wiese,Andrew M. Sherrill*

Main category: cs.CL

TL;DR: 该论文探讨了在PTSD治疗中使用合成的Prolonged Exposure (PE)对话数据作为临床模型训练的替代方案，并通过多维度比较真实与合成对话，发现合成数据在结构上接近真实数据，但在临床关键指标（如痛苦监测）上存在不足。


<details>
  <summary>Details</summary>
Motivation: 驱动这项研究的因素包括医疗数据隐私问题、真实世界数据获取的限制以及数据标注的高成本，而合成数据被视为一种潜在的解决方案。

Method: 研究通过系统比较真实与合成的PE治疗对话，使用语言学、结构化和协议特定指标（如对话轮换模式和治疗忠实度），并引入了基于语言学分析和语义建模的PE特定指标。

Result: 结果显示，合成数据在结构特征（如说话者切换比例）上与真实数据接近，但在关键的临床忠实度指标（如痛苦监测）上表现不足。

Conclusion: 论文指出合成数据在缓解数据稀缺和保护隐私方面具有潜力，但也揭示了其在捕捉治疗交互细微动态上的局限性，建议开发超越表面流畅度的忠实度感知指标以改进评估框架。

Abstract: The growing adoption of synthetic data in healthcare is driven by privacy
concerns, limited access to real-world data, and the high cost of annotation.
This work explores the use of synthetic Prolonged Exposure (PE) therapeutic
conversations for Post-Traumatic Stress Disorder (PTSD) as a scalable
alternative for training and evaluating clinical models. We systematically
compare real and synthetic dialogues using linguistic, structural, and
protocol-specific metrics, including turn-taking patterns and treatment
fidelity. We also introduce and evaluate PE-specific metrics derived from
linguistic analysis and semantic modeling, offering a novel framework for
assessing clinical fidelity beyond surface fluency. Our findings show that
although synthetic data holds promise for mitigating data scarcity and
protecting patient privacy, it can struggle to capture the subtle dynamics of
therapeutic interactions. In our dataset, synthetic dialogues match structural
features of real-world dialogues (e.g., speaker switch ratio: 0.98 vs. 0.99),
however, synthetic interactions do not adequately reflect key fidelity markers
(e.g., distress monitoring). We highlight gaps in existing evaluation
frameworks and advocate for fidelity-aware metrics that go beyond surface
fluency to uncover clinically significant failures. Our findings clarify where
synthetic data can effectively complement real-world datasets -- and where
critical limitations remain.

</details>


### [47] [DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition](https://arxiv.org/abs/2504.21801)
*Z. Z. Ren,Zhihong Shao,Junxiao Song,Huajian Xin,Haocheng Wang,Wanjia Zhao,Liyue Zhang,Zhe Fu,Qihao Zhu,Dejian Yang,Z. F. Wu,Zhibin Gou,Shirong Ma,Hongxuan Tang,Yuxuan Liu,Wenjun Gao,Daya Guo,Chong Ruan*

Main category: cs.CL

TL;DR: 摘要介绍了DeepSeek-Prover-V2，一个专为Lean 4设计的开源大语言模型，通过递归定理证明管道初始化数据，结合非正式和正式数学推理，实现了在神经定理证明中的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 旨在整合非正式和正式的数学推理，提升大语言模型在定理证明中的能力，缩小形式与非形式数学推理之间的差距。

Method: 利用DeepSeek-V3分解复杂问题为子目标，合成解决子目标的链式推理过程，结合强化学习的冷启动训练过程。

Result: DeepSeek-Prover-V2-671B在MiniF2F-test上达到88.9%通过率，解决了PutnamBench中的49个问题，并在AIME竞赛的15个问题中解决了6个。

Conclusion: 模型展示了在形式化定理证明中的高效能力，并缩小了形式与非形式数学推理之间的差距。

Abstract: We introduce DeepSeek-Prover-V2, an open-source large language model designed
for formal theorem proving in Lean 4, with initialization data collected
through a recursive theorem proving pipeline powered by DeepSeek-V3. The
cold-start training procedure begins by prompting DeepSeek-V3 to decompose
complex problems into a series of subgoals. The proofs of resolved subgoals are
synthesized into a chain-of-thought process, combined with DeepSeek-V3's
step-by-step reasoning, to create an initial cold start for reinforcement
learning. This process enables us to integrate both informal and formal
mathematical reasoning into a unified model. The resulting model,
DeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural
theorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49
out of 658 problems from PutnamBench. In addition to standard benchmarks, we
introduce ProverBench, a collection of 325 formalized problems, to enrich our
evaluation, including 15 selected problems from the recent AIME competitions
(years 24-25). Further evaluation on these 15 AIME problems shows that the
model successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of
these problems using majority voting, highlighting that the gap between formal
and informal mathematical reasoning in large language models is substantially
narrowing.

</details>


### [48] [TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments](https://arxiv.org/abs/2504.21851)
*Sichang Tu,Abigail Powers,Stephen Doogan,Jinho D. Choi*

Main category: cs.CL

TL;DR: 该论文提出了一个名为TRUST的框架，利用大语言模型（LLM）开发对话系统，用于标准化诊断访谈和评估，尤其是创伤后应激障碍（PTSD），并通过专家评估验证其效果。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏用于标准化诊断访谈和评估的对话系统，该研究旨在填补这一空白，提高心理健康服务的可及性。

Method: 研究引入TRUST框架，包含协作式LLM模块，使用专门设计的对话行为模式和基于真实访谈记录的患者模拟方法，避免了耗时昂贵的手动测试。

Result: 专家评估表明，TRUST的表现与现实临床访谈相当，系统达到普通临床医生的水平，但在沟通风格和反应适切性方面仍有改进空间。

Conclusion: TRUST展示了增强心理健康服务可及性的潜力，未来可在系统优化上继续探索。

Abstract: Objectives: While Large Language Models (LLMs) have been widely used to
assist clinicians and support patients, no existing work has explored dialogue
systems for standard diagnostic interviews and assessments. This study aims to
bridge the gap in mental healthcare accessibility by developing an LLM-powered
dialogue system that replicates clinician behavior. Materials and Methods: We
introduce TRUST, a framework of cooperative LLM modules capable of conducting
formal diagnostic interviews and assessments for Post-Traumatic Stress Disorder
(PTSD). To guide the generation of appropriate clinical responses, we propose a
Dialogue Acts schema specifically designed for clinical interviews.
Additionally, we develop a patient simulation approach based on real-life
interview transcripts to replace time-consuming and costly manual testing by
clinicians. Results: A comprehensive set of evaluation metrics is designed to
assess the dialogue system from both the agent and patient simulation
perspectives. Expert evaluations by conversation and clinical specialists show
that TRUST performs comparably to real-life clinical interviews. Discussion:
Our system performs at the level of average clinicians, with room for future
enhancements in communication styles and response appropriateness. Conclusions:
Our TRUST framework shows its potential to facilitate mental healthcare
availability.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [49] [Model Connectomes: A Generational Approach to Data-Efficient Language Models](https://arxiv.org/abs/2504.21047)
*Klemen Kotar,Greta Tuckute*

Main category: cs.LG

TL;DR: 该论文提出一种结合进化（外循环）和学习（内循环）的框架，模拟生物神经网络的特性。实验显示，继承进化连接的模型在语言任务中表现更优，尤其在数据稀缺时。


<details>
  <summary>Details</summary>
Motivation: 生物神经网络通过代际进化和个体学习共同塑造，而传统人工神经网络仅依赖单次训练。本文旨在通过引入进化维度（模型连接组），缩小两者差距。

Method: 设计双循环框架：外循环模拟进化生成模型连接组，内循环用1亿token的小规模语料训练继承连接组的模型，并与对照组对比。

Result: 继承连接组的模型在NLP任务、人类行为及脑数据对齐上表现更优，证明其在低数据场景下可作为高效学习先验。

Conclusion: 模型连接组能有效模拟生物演化对学习的促进作用，为人工神经网络设计提供了新思路。

Abstract: Biological neural networks are shaped both by evolution across generations
and by individual learning within an organism's lifetime, whereas standard
artificial neural networks undergo a single, large training procedure without
inherited constraints. In this preliminary work, we propose a framework that
incorporates this crucial generational dimension - an "outer loop" of evolution
that shapes the "inner loop" of learning - so that artificial networks better
mirror the effects of evolution and individual learning in biological
organisms. Focusing on language, we train a model that inherits a "model
connectome" from the outer evolution loop before exposing it to a
developmental-scale corpus of 100M tokens. Compared with two closely matched
control models, we show that the connectome model performs better or on par on
natural language processing tasks as well as alignment to human behavior and
brain data. These findings suggest that a model connectome serves as an
efficient prior for learning in low-data regimes - narrowing the gap between
single-generation artificial models and biologically evolved neural networks.

</details>


### [50] [Multimodal Large Language Models for Medicine: A Comprehensive Survey](https://arxiv.org/abs/2504.21051)
*Jiarui Ye,Hao Tang*

Main category: cs.LG

TL;DR: 本文综述了多模态大语言模型（MLLMs）在医疗领域的应用，包括医疗报告、诊断和治疗，总结了六大主流数据模式及评估基准，并探讨了面临的挑战和解决方案。


<details>
  <summary>Details</summary>
Motivation: 探索MLLMs在医疗领域的潜力，以应对复杂的多模态任务并推动该领域的研究与应用。

Method: 基于对330篇近期论文的综合分析，总结了MLLMs的背景、工作原理及三大应用方向，并提供了具体实例和数据模式分析。

Result: MLLMs在医疗领域展现出显著能力，但也面临数据、评估等挑战，文章提出了相应解决方案。

Conclusion: MLLMs在医疗领域具有广阔前景，但需进一步解决技术和应用中的问题。

Abstract: MLLMs have recently become a focal point in the field of artificial
intelligence research. Building on the strong capabilities of LLMs, MLLMs are
adept at addressing complex multi-modal tasks. With the release of GPT-4, MLLMs
have gained substantial attention from different domains. Researchers have
begun to explore the potential of MLLMs in the medical and healthcare domain.
In this paper, we first introduce the background and fundamental concepts
related to LLMs and MLLMs, while emphasizing the working principles of MLLMs.
Subsequently, we summarize three main directions of application within
healthcare: medical reporting, medical diagnosis, and medical treatment. Our
findings are based on a comprehensive review of 330 recent papers in this area.
We illustrate the remarkable capabilities of MLLMs in these domains by
providing specific examples. For data, we present six mainstream modes of data
along with their corresponding evaluation benchmarks. At the end of the survey,
we discuss the challenges faced by MLLMs in the medical and healthcare domain
and propose feasible methods to mitigate or overcome these issues.

</details>


### [51] [NeuRel-Attack: Neuron Relearning for Safety Disalignment in Large Language Models](https://arxiv.org/abs/2504.21053)
*Yi Zhou,Wenpeng Xing,Dezhang Kong,Changting Lin,Meng Han*

Main category: cs.LG

TL;DR: 本文提出了一种通过识别和修改负责安全约束的神经元来诱导大型语言模型（LLM）安全对齐失效的新方法，实验证明该方法能有效移除安全约束，揭示了当前对齐技术的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM安全对齐技术通过调节神经元激活来抑制有害内容，作者希望通过逆向操作揭示其脆弱性，从而推动更鲁棒的防御方法。

Method: 提出三步法：神经元激活分析（检测区分有害/无害输入的神经元）、相似性神经元识别（定位负责安全对齐的神经元）、神经元重学习（微调这些神经元以恢复生成受限内容的能力）。

Result: 实验表明，该方法能以最小微调有效移除安全约束，证实了现有对齐技术的潜在漏洞。

Conclusion: 该研究强调需开发更鲁棒的防御机制以对抗针对LLM的对抗性微调攻击。

Abstract: Safety alignment in large language models (LLMs) is achieved through
fine-tuning mechanisms that regulate neuron activations to suppress harmful
content. In this work, we propose a novel approach to induce disalignment by
identifying and modifying the neurons responsible for safety constraints. Our
method consists of three key steps: Neuron Activation Analysis, where we
examine activation patterns in response to harmful and harmless prompts to
detect neurons that are critical for distinguishing between harmful and
harmless inputs; Similarity-Based Neuron Identification, which systematically
locates the neurons responsible for safe alignment; and Neuron Relearning for
Safety Removal, where we fine-tune these selected neurons to restore the
model's ability to generate previously restricted responses. Experimental
results demonstrate that our method effectively removes safety constraints with
minimal fine-tuning, highlighting a critical vulnerability in current alignment
techniques. Our findings underscore the need for robust defenses against
adversarial fine-tuning attacks on LLMs.

</details>


### [52] [Modeling and Performance Analysis for Semantic Communications Based on Empirical Results](https://arxiv.org/abs/2504.21055)
*Shuai Ma,Bin Shen,Chuanhui Zhang,Youlong Wu,Hang Li,Shiyin Li,Guangming Shi,Naofal Al-Dhahir*

Main category: cs.LG

TL;DR: 论文提出了Alpha-Beta-Gamma (ABG)公式，用于建模语义通信中端到端性能指标与信噪比的关系，并基于此设计了自适应功率控制方案。


<details>
  <summary>Details</summary>
Motivation: 深度学习语义编解码器的黑盒特性使得语义通信性能分析困难，需一种可建模的方法。

Method: 提出ABG公式建模端到端指标与SNR关系，并基于此设计功率分配方案以提高能效和QoS。

Result: ABG公式能准确拟合常用深度学习网络的性能，并验证了功率分配方案的有效性。

Conclusion: ABG公式为语义通信提供了理论性能分析工具，功率分配方案优化了系统能效和用户体验。

Abstract: Due to the black-box characteristics of deep learning based semantic encoders
and decoders, finding a tractable method for the performance analysis of
semantic communications is a challenging problem. In this paper, we propose an
Alpha-Beta-Gamma (ABG) formula to model the relationship between the end-to-end
measurement and SNR, which can be applied for both image reconstruction tasks
and inference tasks. Specifically, for image reconstruction tasks, the proposed
ABG formula can well fit the commonly used DL networks, such as SCUNet, and
Vision Transformer, for semantic encoding with the multi scale-structural
similarity index measure (MS-SSIM) measurement. Furthermore, we find that the
upper bound of the MS-SSIM depends on the number of quantized output bits of
semantic encoders, and we also propose a closed-form expression to fit the
relationship between the MS-SSIM and quantized output bits. To the best of our
knowledge, this is the first theoretical expression between end-to-end
performance metrics and SNR for semantic communications. Based on the proposed
ABG formula, we investigate an adaptive power control scheme for semantic
communications over random fading channels, which can effectively guarantee
quality of service (QoS) for semantic communications, and then design the
optimal power allocation scheme to maximize the energy efficiency of the
semantic communication system. Furthermore, by exploiting the bisection
algorithm, we develop the power allocation scheme to maximize the minimum QoS
of multiple users for OFDMA downlink semantic communication Extensive
simulations verify the effectiveness and superiority of the proposed ABG
formula and power allocation schemes.

</details>


### [53] [A Hamiltonian Higher-Order Elasticity Framework for Dynamic Diagnostics(2HOED)](https://arxiv.org/abs/2504.21062)
*Ngueuleweu Tiwang Gildas*

Main category: cs.LG

TL;DR: 2HOED框架结合机器学习、区块链和因果推断，通过哈密顿动力学建模复杂系统的能量解剖，揭示线性模型无法捕捉的弹性、韧性和政策敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有技术（如机器学习、区块链）虽能检测模式或保证信任，但无法全面解析复杂系统的动态能量结构，2HOED旨在填补这一空白。

Method: 基于经典力学扩展的哈密顿动力学，用位置、速度、加速度等弹性项建模系统能量，适用经济、气候等多领域，结合小波谱、相空间吸引子等工具分析多阶段政策杠杆。

Result: 2HOED提供跨学科的弹性诊断能力，揭示韧性、临界点和非线性反馈，为决策者提供动态能量地图以预测危机和设计适应性政策。

Conclusion: 2HOED兼具便携性、可解释性和计算轻量化，将数据流转化为动态能量地图，融合AI预测能力与物理解释清晰度，为复杂系统分析提供新范式。

Abstract: Machine learning detects patterns, block chain guarantees trust and
immutability, and modern causal inference identifies directional linkages, yet
none alone exposes the full energetic anatomy of complex systems; the
Hamiltonian Higher Order Elasticity Dynamics(2HOED) framework bridges these
gaps. Grounded in classical mechanics but extended to Economics order
elasticity terms, 2HOED represents economic, social, and physical systems as
energy-based Hamiltonians whose position, velocity, acceleration, and jerk of
elasticity jointly determine systemic power, Inertia, policy sensitivity, and
marginal responses. Because the formalism is scaling free and coordinate
agnostic, it transfers seamlessly from financial markets to climate science,
from supply chain logistics to epidemiology, thus any discipline in which
adaptation and shocks coexist. By embedding standard econometric variables
inside a Hamiltonian, 2HOED enriches conventional economic analysis with
rigorous diagnostics of resilience, tipping points, and feedback loops,
revealing failure modes invisible to linear models. Wavelet spectra, phase
space attractors, and topological persistence diagrams derived from 2HOED
expose multistage policy leverage that machine learning detects only
empirically and block chain secures only after the fact. For economists,
physicians and other scientists, the method opens a new causal energetic
channel linking biological or mechanical elasticity to macro level outcomes.
Portable, interpretable, and computationally light, 2HOED turns data streams
into dynamical energy maps, empowering decision makers to anticipate crises,
design adaptive policies, and engineer robust systems delivering the predictive
punch of AI with the explanatory clarity of physics.

</details>


### [54] [Token-Level Prompt Mixture with Parameter-Free Routing for Federated Domain Generalization](https://arxiv.org/abs/2504.21063)
*Shuai Gong,Chaoran Cui,Xiaolin Dong,Xiushan Nie,Lei Zhu,Xiaojun Chang*

Main category: cs.LG

TL;DR: TRIP提出了一种基于Token级提示混合的无参数路由框架（FedDG），通过将不同Token分配给特定专家并合成实例特定提示，解决了现有方法中的专家分配粗糙和通信成本高的问题，实现了最佳泛化效果且每轮仅需传输1K参数。


<details>
  <summary>Details</summary>
Motivation: 联邦域泛化（FedDG）需要从异构数据中学习全局可泛化模型，而现有单全局提示方法会导致个性化样本性能下降，且现有MoE方法存在专家分配粗糙和通信成本高的问题。

Method: TRIP采用Token级提示混合和无参数路由机制，将多个提示视为专家，通过Token聚类和最优传输分配专家，合成实例特定提示，并利用VLM的零样本泛化能力进行无偏学习。

Result: 在四个基准测试中，TRIP实现了最优的泛化性能，每轮通信仅需1K参数。

Conclusion: TRIP通过Token级专家分配和高效路由机制，显著提升了FedDG的性能和通信效率。

Abstract: Federated domain generalization (FedDG) aims to learn a globally
generalizable model from decentralized clients with heterogeneous data while
preserving privacy. Recent studies have introduced prompt learning to adapt
vision-language models (VLMs) in FedDG by learning a single global prompt.
However, such a one-prompt-fits-all learning paradigm typically leads to
performance degradation on personalized samples. Although the mixture of
experts (MoE) offers a promising solution for specialization, existing
MoE-based methods suffer from coarse image-level expert assignment and high
communication costs from parameterized routers. To address these limitations,
we propose TRIP, a Token-level prompt mixture with parameter-free routing
framework for FedDG, which treats multiple prompts as distinct experts. Unlike
existing image-level routing designs, TRIP assigns different tokens within an
image to specific experts. To ensure communication efficiency, TRIP
incorporates a parameter-free routing mechanism based on token clustering and
optimal transport. The instance-specific prompt is then synthesized by
aggregating experts, weighted by the number of tokens assigned to each.
Additionally, TRIP develops an unbiased learning strategy for prompt experts,
leveraging the VLM's zero-shot generalization capability. Extensive experiments
across four benchmarks demonstrate that TRIP achieves optimal generalization
results, with communication of only 1K parameters per round. Our code is
available at https://github.com/GongShuai8210/TRIP.

</details>


### [55] [Frequency Feature Fusion Graph Network For Depression Diagnosis Via fNIRS](https://arxiv.org/abs/2504.21064)
*Chengkai Yang,Xingping Dong,Xiaofen Zong*

Main category: cs.LG

TL;DR: 该论文提出了一种基于离散傅里叶变换（DFT）的新型抑郁症诊断生物标志物，并结合定制的时序图卷积网络（TGCN）架构，显著提升了诊断性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络（GNN）的抑郁症诊断模型因缺乏可靠的时序生物标志物而效果受限，亟需一种能有效捕捉脑通道功能连接的时空特征的方法。

Method: 采用离散傅里叶变换提取时序生物标志物，设计定制化的时序图卷积网络（TGCN），并在包含1,086名受试者的大规模数据集上训练，进一步通过倾向得分匹配（PSM）优化数据。

Result: 新生物标志物显著提升了脑通道时序特征的表示能力，在真实数据集和PSM数据集的F1分数上均取得改进，并通过SHAP验证了模型的可解释性。

Conclusion: 该研究为抑郁症诊断工具的改进提供了有效路径，兼具性能提升和临床实用性。

Abstract: Data-driven approaches for depression diagnosis have emerged as a significant
research focus in neuromedicine, driven by the development of relevant
datasets. Recently, graph neural network (GNN)-based models have gained
widespread adoption due to their ability to capture brain channel functional
connectivity from both spatial and temporal perspectives. However, their
effectiveness is hindered by the absence of a robust temporal biomarker. In
this paper, we introduce a novel and effective biomarker for depression
diagnosis by leveraging the discrete Fourier transform (DFT) and propose a
customized graph network architecture based on Temporal Graph Convolutional
Network (TGCN). Our model was trained on a dataset comprising 1,086 subjects,
which is over 10 times larger than previous datasets in the field of depression
diagnosis. Furthermore, to align with medical requirements, we performed
propensity score matching (PSM) to create a refined subset, referred to as the
PSM dataset. Experimental results demonstrate that incorporating our newly
designed biomarker enhances the representation of temporal characteristics in
brain channels, leading to improved F1 scores in both the real-world dataset
and the PSM dataset. This advancement has the potential to contribute to the
development of more effective depression diagnostic tools. In addition, we used
SHapley Additive exPlaination (SHAP) to validate the interpretability of our
model, ensuring its practical applicability in medical settings.

</details>


### [56] [A 3D pocket-aware and affinity-guided diffusion model for lead optimization](https://arxiv.org/abs/2504.21065)
*Anjie Qiao,Junjie Xie,Weifeng Huang,Hao Zhang,Jiahua Rao,Shuangjia Zheng,Yuedong Yang,Zhen Wang,Guo-Bo Li,Jinping Lei*

Main category: cs.LG

TL;DR: Diffleop，一种3D口袋感知和亲和力引导的扩散模型，用于优化分子结合亲和力，性能优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习3D生成模型在分子优化中难以充分考量蛋白质靶点的结合亲和力，导致优化效果有限。

Method: 提出Diffleop模型，显式结合蛋白质-配体结合亲和力知识，通过去噪采样生成高亲和力分子。

Result: Diffleop在多项指标，尤其是结合亲和力方面显著优于基准模型。

Conclusion: Diffleop为药物发现中的分子优化提供了高效且亲和力导向的新方法。

Abstract: Molecular optimization, aimed at improving binding affinity or other
molecular properties, is a crucial task in drug discovery that often relies on
the expertise of medicinal chemists. Recently, deep learning-based 3D
generative models showed promise in enhancing the efficiency of molecular
optimization. However, these models often struggle to adequately consider
binding affinities with protein targets during lead optimization. Herein, we
propose a 3D pocket-aware and affinity-guided diffusion model, named Diffleop,
to optimize molecules with enhanced binding affinity. The model explicitly
incorporates the knowledge of protein-ligand binding affinity to guide the
denoising sampling for molecule generation with high affinity. The
comprehensive evaluations indicated that Diffleop outperforms baseline models
across multiple metrics, especially in terms of binding affinity.

</details>


### [57] [A Brief Review for Compression and Transfer Learning Techniques in DeepFake Detection](https://arxiv.org/abs/2504.21066)
*Andreas Karathanasis,John Violos,Ioannis Kompatsiaris,Symeon Papadopoulos*

Main category: cs.LG

TL;DR: 论文研究了如何在边缘设备上通过压缩技术和迁移学习方法高效部署深度伪造检测模型，尽管在高压缩率下性能保持稳定，但在面对训练集中未出现的DeepFake模型时存在领域泛化问题。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备因计算和内存资源有限难以部署深度伪造检测模型的问题，同时保护数据隐私。

Method: 采用剪枝、知识蒸馏、量化、微调和适配器技术，并在Synthbuster、RAISE和ForenSynths数据集上评估效果。

Result: 在相同DeepFake模型生成的数据下，即使压缩率高达90%，性能仍能保持；但在未训练过的模型生成数据时出现领域泛化问题。

Conclusion: 压缩和迁移学习可有效部署模型，但需进一步解决领域泛化问题以提高泛化能力。

Abstract: Training and deploying deepfake detection models on edge devices offers the
advantage of maintaining data privacy and confidentiality by processing it
close to its source. However, this approach is constrained by the limited
computational and memory resources available at the edge. To address this
challenge, we explore compression techniques to reduce computational demands
and inference time, alongside transfer learning methods to minimize training
overhead. Using the Synthbuster, RAISE, and ForenSynths datasets, we evaluate
the effectiveness of pruning, knowledge distillation (KD), quantization,
fine-tuning, and adapter-based techniques. Our experimental results demonstrate
that both compression and transfer learning can be effectively achieved, even
with a high compression level of 90%, remaining at the same performance level
when the training and validation data originate from the same DeepFake model.
However, when the testing dataset is generated by DeepFake models not present
in the training set, a domain generalization issue becomes evident.

</details>


### [58] [R^2VFL: A Robust Random Vector Functional Link Network with Huber-Weighted Framework](https://arxiv.org/abs/2504.21069)
*Anuradha Kumari,Mushir Akhtar,P. N. Suganthan,M. Tanveer*

Main category: cs.LG

TL;DR: 本文提出了R2VFL模型，通过Huber加权函数和类别概率机制增强RVFL神经网络的鲁棒性，减少噪声和异常值的影响，并在47个UCI数据集和EEG信号分类中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统的RVFL神经网络在处理噪声和异常值时效果不佳，假设所有数据样本贡献相同。为了增强模型的鲁棒性和适应性，本文提出了R2VFL框架。

Method: R2VFL结合了Huber加权函数和类别概率机制，减少异常值和噪声的影响。通过两种计算类别中心的方法（简单平均和中位数），提出了R2VFL-A和R2VFL-M两种变体。

Result: 在47个UCI数据集上进行的广泛评估和统计测试表明，R2VFL模型表现优异，尤其在EEG信号分类任务中展现出实际应用价值。

Conclusion: R2VFL框架显著提升了RVFL神经网络对噪声和异常值的鲁棒性，扩展了其在真实场景中的应用潜力，特别是在医疗领域。

Abstract: The random vector functional link (RVFL) neural network has shown significant
potential in overcoming the constraints of traditional artificial neural
networks, such as excessive computation time and suboptimal solutions. However,
RVFL faces challenges when dealing with noise and outliers, as it assumes all
data samples contribute equally. To address this issue, we propose a novel
robust framework, R2VFL, RVFL with Huber weighting function and class
probability, which enhances the model's robustness and adaptability by
effectively mitigating the impact of noise and outliers in the training data.
The Huber weighting function reduces the influence of outliers, while the class
probability mechanism assigns less weight to noisy data points, resulting in a
more resilient model. We explore two distinct approaches for calculating class
centers within the R2VFL framework: the simple average of all data points in
each class and the median of each feature, the later providing a robust
alternative by minimizing the effect of extreme values. These approaches give
rise to two novel variants of the model-R2VFL-A and R2VFL-M. We extensively
evaluate the proposed models on 47 UCI datasets, encompassing both binary and
multiclass datasets, and conduct rigorous statistical testing, which confirms
the superiority of the proposed models. Notably, the models also demonstrate
exceptional performance in classifying EEG signals, highlighting their
practical applicability in real-world biomedical domain.

</details>


### [59] [A Survey on Parameter-Efficient Fine-Tuning for Foundation Models in Federated Learning](https://arxiv.org/abs/2504.21099)
*Jieming Bian,Yuanzhe Peng,Lei Wang,Yin Huang,Jie Xu*

Main category: cs.LG

TL;DR: 该论文综述了参数高效微调（PEFT）与联邦学习（FL）的结合，将其分为三类方法，并分析了它们在联邦学习环境中的应用与挑战。


<details>
  <summary>Details</summary>
Motivation: 大规模基础模型的微调在计算资源上成本高昂，而联邦学习在隐私敏感场景下具有优势，因此研究如何高效地在联邦学习中应用PEFT方法具有重要意义。

Method: 将现有PEFT方法分为三类：附加型PEFT（新增可训练参数）、选择型PEFT（仅微调部分参数）和重构型PEFT（通过架构变换实现高效更新），并分析它们在联邦学习中的适应性。

Result: 论文分析了各类PEFT方法如何应对联邦学习中的数据异质性、通信效率、计算限制和隐私问题，并总结了它们在自然语言处理和计算机视觉任务中的应用。

Conclusion: 未来研究方向包括扩展至更大基础模型、联邦PEFT方法的理论分析，以及资源受限环境下的可持续方案。

Abstract: Foundation models have revolutionized artificial intelligence by providing
robust, versatile architectures pre-trained on large-scale datasets. However,
adapting these massive models to specific downstream tasks requires
fine-tuning, which can be prohibitively expensive in computational resources.
Parameter-Efficient Fine-Tuning (PEFT) methods address this challenge by
selectively updating only a small subset of parameters. Meanwhile, Federated
Learning (FL) enables collaborative model training across distributed clients
without sharing raw data, making it ideal for privacy-sensitive applications.
This survey provides a comprehensive review of the integration of PEFT
techniques within federated learning environments. We systematically categorize
existing approaches into three main groups: Additive PEFT (which introduces new
trainable parameters), Selective PEFT (which fine-tunes only subsets of
existing parameters), and Reparameterized PEFT (which transforms model
architectures to enable efficient updates). For each category, we analyze how
these methods address the unique challenges of federated settings, including
data heterogeneity, communication efficiency, computational constraints, and
privacy concerns. We further organize the literature based on application
domains, covering both natural language processing and computer vision tasks.
Finally, we discuss promising research directions, including scaling to larger
foundation models, theoretical analysis of federated PEFT methods, and
sustainable approaches for resource-constrained environments.

</details>


### [60] [SMOGAN: Synthetic Minority Oversampling with GAN Refinement for Imbalanced Regression](https://arxiv.org/abs/2504.21152)
*Shayan Alahyari,Mike Domaratzki*

Main category: cs.LG

TL;DR: SMOGAN是一个两阶段过采样框架，用于解决不平衡回归问题，通过生成和过滤合成样本，提高模型在稀疏区域的性能。


<details>
  <summary>Details</summary>
Motivation: 现有不平衡回归方法基于线性假设生成的合成样本无法准确反映复杂的数据分布，SMOGAN旨在解决这一问题。

Method: SMOGAN包含两个阶段：首先生成初始合成样本，然后用Distribution-aware GAN（DistGAN）过滤样本，使其更接近真实分布。

Result: 在23个不平衡数据集上的实验表明，SMOGAN显著优于未使用DistGAN过滤的基准方法。

Conclusion: SMOGAN通过两阶段方法有效改善不平衡回归任务中稀疏区域的预测性能。

Abstract: Imbalanced regression refers to prediction tasks where the target variable is
skewed. This skewness hinders machine learning models, especially neural
networks, which concentrate on dense regions and therefore perform poorly on
underrepresented (minority) samples. Despite the importance of this problem,
only a few methods have been proposed for imbalanced regression. Many of the
available solutions for imbalanced regression adapt techniques from the class
imbalance domain, such as linear interpolation and the addition of Gaussian
noise, to create synthetic data in sparse regions. However, in many cases, the
underlying distribution of the data is complex and non-linear. Consequently,
these approaches generate synthetic samples that do not accurately represent
the true feature-target relationship. To overcome these limitations, we propose
SMOGAN, a two-step oversampling framework for imbalanced regression. In Stage
1, an existing oversampler generates initial synthetic samples in sparse target
regions. In Stage 2, we introduce DistGAN, a distribution-aware GAN that serves
as SMOGAN's filtering layer and refines these samples via adversarial loss
augmented with a Maximum Mean Discrepancy objective, aligning them with the
true joint feature-target distribution. Extensive experiments on 23 imbalanced
datasets show that SMOGAN consistently outperforms the default oversampling
method without the DistGAN filtering layer.

</details>


### [61] [Efficient LLMs with AMP: Attention Heads and MLP Pruning](https://arxiv.org/abs/2504.21174)
*Leandro Giusti Mugnaini,Bruno Lopes Yamamoto,Lucas Lauton de Alcantara,Victor Zacarias,Edson Bollis,Lucas Pellicer,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: AMP是一种新型结构化剪枝方法，通过去除LLMs中不太关键的MHA和MLP结构，高效压缩模型，提升推理速度且不影响任务性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）参数量庞大，计算成本高且推理速度慢，限制了其在资源受限环境中的部署。

Method: AMP通过将输入数据投影到权重来评估结构重要性，剪枝MHA和MLP中的次要结构。

Result: AMP在常识推理任务上超越现有技术1.49个百分点，实现30%剪枝率且零样本任务性能影响最小。

Conclusion: AMP灵活高效，适用于多种LLMs家族，适合在资源受限环境中部署。

Abstract: Deep learning drives a new wave in computing systems and triggers the
automation of increasingly complex problems. In particular, Large Language
Models (LLMs) have significantly advanced cognitive tasks, often matching or
even surpassing human-level performance. However, their extensive parameters
result in high computational costs and slow inference, posing challenges for
deployment in resource-limited settings. Among the strategies to overcome the
aforementioned challenges, pruning emerges as a successful mechanism since it
reduces model size while maintaining predictive ability. In this paper, we
introduce AMP: Attention Heads and MLP Pruning, a novel structured pruning
method that efficiently compresses LLMs by removing less critical structures
within Multi-Head Attention (MHA) and Multilayer Perceptron (MLP). By
projecting the input data onto weights, AMP assesses structural importance and
overcomes the limitations of existing techniques, which often fall short in
flexibility or efficiency. In particular, AMP surpasses the current
state-of-the-art on commonsense reasoning tasks by up to 1.49 percentage
points, achieving a 30% pruning ratio with minimal impact on zero-shot task
performance. Moreover, AMP also improves inference speeds, making it
well-suited for deployment in resource-constrained environments. We confirm the
flexibility of AMP on different families of LLMs, including LLaMA and Phi.

</details>


### [62] [GLIP-OOD: Zero-Shot Graph OOD Detection with Foundation Model](https://arxiv.org/abs/2504.21186)
*Haoyan Xu,Zhengtao Yao,Xuzhi Zhang,Ziyi Wang,Langzhou He,Yushun Dong,Philip S. Yu,Mengyuan Li,Yue Zhao*

Main category: cs.LG

TL;DR: 该论文首次探索了图结构化数据的零样本OOD检测，利用图基础模型(GFM)和LLMs生成的伪OOD标签，实现了无需节点监督的先进性能。


<details>
  <summary>Details</summary>
Motivation: 在动态开放环境中，OOD检测对机器学习系统的安全性和可靠性至关重要。尽管视觉和文本领域的零样本OOD检测已取得进展，但图数据的复杂性导致其相关研究滞后。

Method: 使用图基础模型(GFM)进行零样本OOD检测；引入GLIP-OOD框架，通过LLMs生成伪OOD标签以捕捉ID/OOD的语义边界，无需节点级监督。

Result: 该方法在四个文本属性图数据集上实现了当前最佳性能，且无需任何标注数据。

Conclusion: 研究首次实现了图数据的完全零样本OOD检测，为开放图学习场景提供了实用解决方案。

Abstract: Out-of-distribution (OOD) detection is critical for ensuring the safety and
reliability of machine learning systems, particularly in dynamic and open-world
environments. In the vision and text domains, zero-shot OOD detection - which
requires no training on in-distribution (ID) data - has made significant
progress through the use of large-scale pretrained models such as
vision-language models (VLMs) and large language models (LLMs). However,
zero-shot OOD detection in graph-structured data remains largely unexplored,
primarily due to the challenges posed by complex relational structures and the
absence of powerful, large-scale pretrained models for graphs. In this work, we
take the first step toward enabling zero-shot graph OOD detection by leveraging
a graph foundation model (GFM). We show that, when provided only with class
label names, the GFM can perform OOD detection without any node-level
supervision - outperforming existing supervised methods across multiple
datasets. To address the more practical setting where OOD label names are
unavailable, we introduce GLIP-OOD, a novel framework that employs LLMs to
generate semantically informative pseudo-OOD labels from unlabeled data. These
labels enable the GFM to capture nuanced semantic boundaries between ID and OOD
classes and perform fine-grained OOD detection - without requiring any labeled
nodes. Our approach is the first to enable node-level graph OOD detection in a
fully zero-shot setting, and achieves state-of-the-art performance on four
benchmark text-attributed graph datasets.

</details>


### [63] [LIFT: LLM-Based Pragma Insertion for HLS via GNN Supervised Fine-Tuning](https://arxiv.org/abs/2504.21187)
*Neha Prakriya,Zijian Ding,Yizhou Sun,Jason Cong*

Main category: cs.LG

TL;DR: LIFT是一种基于大型语言模型（LLM）的HLS编码助手，通过结合GNN增强的结构和语义理解，自动生成性能关键pragma，显著提升FPGA设计性能。


<details>
  <summary>Details</summary>
Motivation: 解决HLS工具中需要专家手动优化pragma的挑战，提升FPGA编程的效率和性能。

Method: 通过LLM与GNN的紧密集成和监督训练，结合序列建模与代码结构分析，自动生成pragma。

Result: LIFT性能平均提升3.52x（AutoDSE）、2.16x（HARP）和66x（GPT-4o）。

Conclusion: LIFT为FPGA高性能HLS设计提供了自动化解决方案，显著减少人工优化需求。

Abstract: FPGAs are increasingly adopted in datacenter environments for their
reconfigurability and energy efficiency. High-Level Synthesis (HLS) tools have
eased FPGA programming by raising the abstraction level from RTL to untimed
C/C++, yet attaining high performance still demands expert knowledge and
iterative manual insertion of optimization pragmas to modify the
microarchitecture. To address this challenge, we propose LIFT, a large language
model (LLM)-based coding assistant for HLS that automatically generates
performance-critical pragmas given a C/C++ design. We fine-tune the LLM by
tightly integrating and supervising the training process with a graph neural
network (GNN), combining the sequential modeling capabilities of LLMs with the
structural and semantic understanding of GNNs necessary for reasoning over code
and its control/data dependencies. On average, LIFT produces designs that
improve performance by 3.52x and 2.16x than prior state-of the art AutoDSE and
HARP respectively, and 66x than GPT-4o.

</details>


### [64] [Artificial Intelligence for Personalized Prediction of Alzheimer's Disease Progression: A Survey of Methods, Data Challenges, and Future Directions](https://arxiv.org/abs/2504.21189)
*Gulsah Hancerliogullari Koksalmis,Bulent Soykan,Laura J. Brattain,Hsin-Hsiung Huang*

Main category: cs.LG

TL;DR: 本文综述了人工智能（AI）在个性化阿尔茨海默病（AD）进展预测中的应用，包括状态空间模型、深度学习和图神经网络等方法，并探讨了数据挑战及应对策略，如合成数据生成。


<details>
  <summary>Details</summary>
Motivation: AD进展的个体差异性使得准确预测和个性化护理变得复杂，AI能通过分析多模态、纵向数据来应对这一挑战。

Method: 综述了状态空间模型、RNN、GNN及数字孪生等AI方法，并探讨了VAEs和GANs在数据增强中的应用。

Result: 总结了现有方法的优势和局限性，强调了多模态整合趋势，并指出了模型可解释性和泛化性的需求。

Conclusion: 提出了未来研究方向，如混合模型、因果推断和联邦学习，并呼吁关注临床整合及伦理问题，旨在推动AD个性化预测工具的临床实用性。

Abstract: Alzheimer's Disease (AD) is marked by significant inter-individual
variability in its progression, complicating accurate prognosis and
personalized care planning. This heterogeneity underscores the critical need
for predictive models capable of forecasting patient-specific disease
trajectories. Artificial Intelligence (AI) offers powerful tools to address
this challenge by analyzing complex, multi-modal, and longitudinal patient
data. This paper provides a comprehensive survey of AI methodologies applied to
personalized AD progression prediction. We review key approaches including
state-space models for capturing temporal dynamics, deep learning techniques
like Recurrent Neural Networks for sequence modeling, Graph Neural Networks
(GNNs) for leveraging network structures, and the emerging concept of AI-driven
digital twins for individualized simulation. Recognizing that data limitations
often impede progress, we examine common challenges such as high
dimensionality, missing data, and dataset imbalance. We further discuss
AI-driven mitigation strategies, with a specific focus on synthetic data
generation using Variational Autoencoders (VAEs) and Generative Adversarial
Networks (GANs) to augment and balance datasets. The survey synthesizes the
strengths and limitations of current approaches, emphasizing the trend towards
multimodal integration and the persistent need for model interpretability and
generalizability. Finally, we identify critical open challenges, including
robust external validation, clinical integration, and ethical considerations,
and outline promising future research directions such as hybrid models, causal
inference, and federated learning. This review aims to consolidate current
knowledge and guide future efforts in developing clinically relevant AI tools
for personalized AD prognostication.

</details>


### [65] [TT-LoRA MoE: Unifying Parameter-Efficient Fine-Tuning and Sparse Mixture-of-Experts](https://arxiv.org/abs/2504.21190)
*Pradip Kunwar,Minh N. Vu,Maanak Gupta,Mahmoud Abdelsalam,Manish Bhattarai*

Main category: cs.LG

TL;DR: 提出了一种名为TT-LoRA MoE的新型计算框架，通过结合参数高效微调（PEFT）和稀疏MoE路由，解决了大规模模型部署中的扩展性问题。该方法比传统MoE方法更高效，仅需少量参数即可在多任务中实现优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统MoE方法在专家数量增加时计算开销显著，限制了大规模模型的部署。TT-LoRA MoE旨在通过分解训练阶段和优化专家选择，提升计算效率和灵活性。

Method: 分两阶段训练：首先独立训练轻量级张量化的低秩适配器（TT-LoRA专家），随后冻结这些适配器并单独训练稀疏MoE路由器，实现动态专家选择。

Result: 在保持低秩适配器内存效率的同时，该方法仅需极少参数（如2%的LoRA参数），在多任务上表现优于AdapterFusion。

Conclusion: TT-LoRA MoE通过结构化解耦显著提升了计算效率和灵活性，为大规模多任务推理部署提供了实用且可扩展的解决方案。

Abstract: We propose Tensor-Trained Low-Rank Adaptation Mixture of Experts (TT-LoRA
MoE), a novel computational framework integrating Parameter-Efficient
Fine-Tuning (PEFT) with sparse MoE routing to address scalability challenges in
large model deployments. Unlike traditional MoE approaches, which face
substantial computational overhead as expert counts grow, TT-LoRA MoE
decomposes training into two distinct, optimized stages. First, we
independently train lightweight, tensorized low-rank adapters (TT-LoRA
experts), each specialized for specific tasks. Subsequently, these expert
adapters remain frozen, eliminating inter-task interference and catastrophic
forgetting in multi-task setting. A sparse MoE router, trained separately,
dynamically leverages base model representations to select exactly one
specialized adapter per input at inference time, automating expert selection
without explicit task specification. Comprehensive experiments confirm our
architecture retains the memory efficiency of low-rank adapters, seamlessly
scales to large expert pools, and achieves robust task-level optimization. This
structured decoupling significantly enhances computational efficiency and
flexibility: uses only 2% of LoRA, 0.3% of Adapters and 0.03% of AdapterFusion
parameters and outperforms AdapterFusion by 4 value in multi-tasking, enabling
practical and scalable multi-task inference deployments.

</details>


### [66] [Graph Synthetic Out-of-Distribution Exposure with Large Language Models](https://arxiv.org/abs/2504.21198)
*Haoyan Xu,Zhengtao Yao,Ziyi Wang,Zhan Cheng,Xiyang Hu,Mengyuan Li,Yue Zhao*

Main category: cs.LG

TL;DR: 论文提出了一种利用大型语言模型（LLM）进行图OOD检测的新框架GOE-LLM，无需真实OOD样本即可提升检测性能。通过零样本标注和LLM生成伪OOD节点，框架在无真实OOD数据的情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 图数据的OOD检测对模型鲁棒性至关重要，但现有方法依赖真实OOD样本，而获取成本高且不实际。GOE-LLM创新地利用LLM生成伪OOD数据，解决了这一限制。

Method: 1. 通过零样本LLM标注从未标注图中识别伪OOD节点；2. 利用LLM生成语义丰富的合成OOD节点。这些伪OOD节点用于训练ID分类器，增强OOD感知能力。

Result: GOE-LLM在多个基准数据集上显著优于不采用OOD暴露的现有方法，性能接近依赖真实OOD数据的方法。

Conclusion: GOE-LLM为无需真实OOD数据的图OOD检测提供了一种高效解决方案，展示了LLM在该领域的潜力。

Abstract: Out-of-distribution (OOD) detection in graphs is critical for ensuring model
robustness in open-world and safety-sensitive applications. Existing approaches
to graph OOD detection typically involve training an in-distribution (ID)
classifier using only ID data, followed by the application of post-hoc OOD
scoring techniques. Although OOD exposure - introducing auxiliary OOD samples
during training - has proven to be an effective strategy for enhancing
detection performance, current methods in the graph domain generally assume
access to a set of real OOD nodes. This assumption, however, is often
impractical due to the difficulty and cost of acquiring representative OOD
samples. In this paper, we introduce GOE-LLM, a novel framework that leverages
Large Language Models (LLMs) for OOD exposure in graph OOD detection without
requiring real OOD nodes. GOE-LLM introduces two pipelines: (1) identifying
pseudo-OOD nodes from the initially unlabeled graph using zero-shot LLM
annotations, and (2) generating semantically informative synthetic OOD nodes
via LLM-prompted text generation. These pseudo-OOD nodes are then used to
regularize the training of the ID classifier for improved OOD awareness. We
evaluate our approach across multiple benchmark datasets, showing that GOE-LLM
significantly outperforms state-of-the-art graph OOD detection methods that do
not use OOD exposure and achieves comparable performance to those relying on
real OOD data.

</details>


### [67] [FedHERO: A Federated Learning Approach for Node Classification Task on Heterophilic Graphs](https://arxiv.org/abs/2504.21206)
*Zihan Chen,Xingbo Fu,Yushun Dong,Jundong Li,Cong Shen*

Main category: cs.LG

TL;DR: FedHERO是一种联邦图学习框架，旨在解决异构图数据导致的本地模型知识冲突问题，通过双通道GNN和结构学习器提升全局模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统FGL方法假设所有客户端的图数据是同质的，但实际中节点邻域分布可能不同（异质性），导致模型聚合时性能下降。FedHERO旨在解决这一问题。

Method: 提出FedHERO框架，采用双通道GNN和结构学习器，从本地图中识别并学习通用结构知识，适用于不同节点邻域分布模式的图数据。

Result: 实验表明，FedHERO在异构图数据上显著优于现有方法，提升了个体客户端模型性能，并实现了全局模型的稳定聚合。

Conclusion: FedHERO为处理异质性图数据的联邦学习设立了新标准，通过结构知识共享有效解决了模型冲突问题。

Abstract: Federated Graph Learning (FGL) empowers clients to collaboratively train
Graph neural networks (GNNs) in a distributed manner while preserving data
privacy. However, FGL methods usually require that the graph data owned by all
clients is homophilic to ensure similar neighbor distribution patterns of
nodes. Such an assumption ensures that the learned knowledge is consistent
across the local models from all clients. Therefore, these local models can be
properly aggregated as a global model without undermining the overall
performance. Nevertheless, when the neighbor distribution patterns of nodes
vary across different clients (e.g., when clients hold graphs with different
levels of heterophily), their local models may gain different and even conflict
knowledge from their node-level predictive tasks. Consequently, aggregating
these local models usually leads to catastrophic performance deterioration on
the global model. To address this challenge, we propose FedHERO, an FGL
framework designed to harness and share insights from heterophilic graphs
effectively. At the heart of FedHERO is a dual-channel GNN equipped with a
structure learner, engineered to discern the structural knowledge encoded in
the local graphs. With this specialized component, FedHERO enables the local
model for each client to identify and learn patterns that are universally
applicable across graphs with different patterns of node neighbor
distributions. FedHERO not only enhances the performance of individual client
models by leveraging both local and shared structural insights but also sets a
new precedent in this field to effectively handle graph data with various node
neighbor distribution patterns. We conduct extensive experiments to validate
the superior performance of FedHERO against existing alternatives.

</details>


### [68] [A Cost-Effective LLM-based Approach to Identify Wildlife Trafficking in Online Marketplaces](https://arxiv.org/abs/2504.21211)
*Juliana Barbosa,Ulhas Gondhali,Gohar Petrossian,Kinshuk Sharma,Sunandan Chakraborty,Jennifer Jacquet,Juliana Freire*

Main category: cs.LG

TL;DR: 该论文提出了一种利用大型语言模型(LLMs)生成伪标签的低成本方法，用于高效训练分类模型，以识别在线野生动物买卖活动，最终实现了95%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 在线平台的兴起使野生动物买卖活动更加隐蔽且难以追踪。传统的人工标注方法成本高且效率低，需要自动化解决方案来提高检测效能。

Method: 利用LLMs为少量数据生成伪标签，再通过这些标签训练专门的分类模型，降低标注成本并提升模型效果。

Result: 该方法训练的模型F1分数达到95%，性能优于直接使用LLMs，且成本更低。

Conclusion: 该方法为野生动物买卖活动的分析提供了高效、低成本的解决方案，推动了该领域数据科学的发展。

Abstract: Wildlife trafficking remains a critical global issue, significantly impacting
biodiversity, ecological stability, and public health. Despite efforts to
combat this illicit trade, the rise of e-commerce platforms has made it easier
to sell wildlife products, putting new pressure on wild populations of
endangered and threatened species. The use of these platforms also opens a new
opportunity: as criminals sell wildlife products online, they leave digital
traces of their activity that can provide insights into trafficking activities
as well as how they can be disrupted. The challenge lies in finding these
traces. Online marketplaces publish ads for a plethora of products, and
identifying ads for wildlife-related products is like finding a needle in a
haystack. Learning classifiers can automate ad identification, but creating
them requires costly, time-consuming data labeling that hinders support for
diverse ads and research questions. This paper addresses a critical challenge
in the data science pipeline for wildlife trafficking analytics: generating
quality labeled data for classifiers that select relevant data. While large
language models (LLMs) can directly label advertisements, doing so at scale is
prohibitively expensive. We propose a cost-effective strategy that leverages
LLMs to generate pseudo labels for a small sample of the data and uses these
labels to create specialized classification models. Our novel method
automatically gathers diverse and representative samples to be labeled while
minimizing the labeling costs. Our experimental evaluation shows that our
classifiers achieve up to 95% F1 score, outperforming LLMs at a lower cost. We
present real use cases that demonstrate the effectiveness of our approach in
enabling analyses of different aspects of wildlife trafficking.

</details>


### [69] [ABG-NAS: Adaptive Bayesian Genetic Neural Architecture Search for Graph Representation Learning](https://arxiv.org/abs/2504.21254)
*Sixuan Wang,Jiao Yin,Jinli Cao,MingJian Tang,Hua Wang,Yanchun Zhang*

Main category: cs.LG

TL;DR: ABG-NAS提出了一个自动化图神经网络架构搜索框架，旨在高效学习图表示。通过结合全面的架构搜索空间、自适应遗传优化策略和贝叶斯调优模块，该方法在多数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络（GNN）架构难以适应多样且复杂的图结构，限制了其表示学习的通用性和鲁棒性，因此提出了ABG-NAS来解决这一问题。

Method: ABG-NAS包含三个核心组件：综合架构搜索空间（CASS）、自适应遗传优化策略（AGOS）和贝叶斯引导调优模块（BGTM），通过动态平衡搜索与优化提升性能。

Result: 在Cora、PubMed等基准数据集上，ABG-NAS显著优于手工设计的GNN和现有神经架构搜索方法。

Conclusion: ABG-NAS通过自动化和优化的方法，为复杂图结构提供了可扩展且自适应的解决方案，推动了图表示学习的发展。

Abstract: Effective and efficient graph representation learning is essential for
enabling critical downstream tasks, such as node classification, link
prediction, and subgraph search. However, existing graph neural network (GNN)
architectures often struggle to adapt to diverse and complex graph structures,
limiting their ability to provide robust and generalizable representations. To
address this challenge, we propose ABG-NAS, a novel framework for automated
graph neural network architecture search tailored for efficient graph
representation learning. ABG-NAS encompasses three key components: a
Comprehensive Architecture Search Space (CASS), an Adaptive Genetic
Optimization Strategy (AGOS), and a Bayesian-Guided Tuning Module (BGTM). CASS
systematically explores diverse propagation (P) and transformation (T)
operations, enabling the discovery of GNN architectures capable of capturing
intricate graph characteristics. AGOS dynamically balances exploration and
exploitation, ensuring search efficiency and preserving solution diversity.
BGTM further optimizes hyperparameters periodically, enhancing the scalability
and robustness of the resulting architectures. Empirical evaluations on
benchmark datasets (Cora, PubMed, Citeseer, and CoraFull) demonstrate that
ABG-NAS consistently outperforms both manually designed GNNs and
state-of-the-art neural architecture search (NAS) methods. These results
highlight the potential of ABG-NAS to advance graph representation learning by
providing scalable and adaptive solutions for diverse graph structures. Our
code is publicly available at https://github.com/sserranw/ABG-NAS.

</details>


### [70] [Multi-Domain Causal Discovery in Bijective Causal Models](https://arxiv.org/abs/2504.21261)
*Kasra Jalaldoust,Saber Salehkaleybar,Negar Kiyavash*

Main category: cs.LG

TL;DR: 该论文提出了一种多域设置下的因果发现方法，利用双射生成机制（BGM）放宽了对因果函数的假设，并通过统计测试和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在因果发现的多域设置中，作者希望放宽对因果函数的假设，同时保持因果图的识别性。通过假设因果函数在多个域中保持不变，而外生噪声的分布可以变化，提出了BGM作为一种通用的因果生成机制。

Method: 作者引入了双射生成机制（BGM），该机制确保了外生噪声与内生变量之间的函数关系是双射且可微的。BGM概括了多种现有模型，如加性噪声模型、LiNGAM、后非线性模型和位置尺度噪声模型。此外，开发了一种统计测试来识别目标变量的父节点集。

Result: 实验表明，基于BGM的方法在各种合成和真实数据集上都能有效识别因果结构，且适用性广。

Conclusion: BGM提供了一个通用的因果发现框架，在多域设置下能放宽函数假设，同时保持因果结构的识别性。

Abstract: We consider the problem of causal discovery (a.k.a., causal structure
learning) in a multi-domain setting. We assume that the causal functions are
invariant across the domains, while the distribution of the exogenous noise may
vary. Under causal sufficiency (i.e., no confounders exist), we show that the
causal diagram can be discovered under less restrictive functional assumptions
compared to previous work. What enables causal discovery in this setting is
bijective generation mechanisms (BGM), which ensures that the functional
relation between the exogenous noise $E$ and the endogenous variable $Y$ is
bijective and differentiable in both directions at every level of the cause
variable $X = x$. BGM generalizes a variety of models including additive noise
model, LiNGAM, post-nonlinear model, and location-scale noise model. Further,
we derive a statistical test to find the parents set of the target variable.
Experiments on various synthetic and real-world datasets validate our
theoretical findings.

</details>


### [71] [Orthogonal Factor-Based Biclustering Algorithm (BCBOF) for High-Dimensional Data and Its Application in Stock Trend Prediction](https://arxiv.org/abs/2504.21289)
*Yan Huang,Da-Qing Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于正交因子的双聚类算法BCBOF，用于解决高维数据处理中的稀疏性和局部结构破坏问题，并在股票技术指标组合和趋势预测中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统双聚类算法在处理高维数据时面临两个主要问题：高维空间中的距离集中现象导致数据稀疏性，以及线性降维方法破坏局部结构模式。为此，作者提出了BCBOF算法以解决这些问题。

Method: BCBOF算法首先在高维数据向量空间中构建正交因子，然后在正交子空间中以原始数据的坐标作为聚类目标进行聚类，最终得到双聚类结果。该方法通过先降维再聚类的方式缓解了高维数据稀疏性问题。此外，还将双聚类结果转化为模糊规则，并与止损和止盈规则结合，构建了股票价格趋势预测的模糊推理系统。

Result: 通过多种评估指标的比较，BCBOF算法在性能上优于其他双聚类方法。虚拟交易实验显示，基于该算法的交易策略能为投资者带来更高收益。

Conclusion: BCBOF算法有效解决了高维数据双聚类的核心问题，并在股票预测中展现了实用价值，验证了其优越性和应用潜力。

Abstract: Biclustering is an effective technique in data mining and pattern
recognition. Biclustering algorithms based on traditional clustering face two
fundamental limitations when processing high-dimensional data: (1) The distance
concentration phenomenon in high-dimensional spaces leads to data sparsity,
rendering similarity measures ineffective; (2) Mainstream linear dimensionality
reduction methods disrupt critical local structural patterns. To apply
biclustering to high-dimensional datasets, we propose an orthogonal
factor-based biclustering algorithm (BCBOF). First, we constructed orthogonal
factors in the vector space of the high-dimensional dataset. Then, we performed
clustering using the coordinates of the original data in the orthogonal
subspace as clustering targets. Finally, we obtained biclustering results of
the original dataset. Since dimensionality reduction was applied before
clustering, the proposed algorithm effectively mitigated the data sparsity
problem caused by high dimensionality. Additionally, we applied this
biclustering algorithm to stock technical indicator combinations and stock
price trend prediction. Biclustering results were transformed into fuzzy rules,
and we incorporated profit-preserving and stop-loss rules into the rule set,
ultimately forming a fuzzy inference system for stock price trend predictions
and trading signals. To evaluate the performance of BCBOF, we compared it with
existing biclustering methods using multiple evaluation metrics. The results
showed that our algorithm outperformed other biclustering techniques. To
validate the effectiveness of the fuzzy inference system, we conducted virtual
trading experiments using historical data from 10 A-share stocks. The
experimental results showed that the generated trading strategies yielded
higher returns for investors.

</details>


### [72] [Fairness in Graph Learning Augmented with Machine Learning: A Survey](https://arxiv.org/abs/2504.21296)
*Renqiang Luo,Ziqi Xu,Xikun Zhang,Qing Qing,Huafei Huang,Enyan Dai,Zhe Wang,Bo Yang*

Main category: cs.LG

TL;DR: 本文探讨了图学习结合机器学习（GL-ML）中的公平性挑战，分析了其复杂性与应对方法，并提出了四种关键技术来提升公平性，为该领域未来研究奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于GL-ML虽然在多个领域取得成功，但其复杂性可能引发公平性问题，尤其是在高风险应用中，可能导致歧视性结果。

Method: 本文系统性地分析了GL-ML的公平性挑战，并探讨了四种关键技术以提升公平性。

Result: 研究发现，图学习与机器学习的结合既增强了性能，又带来了公平性上的复杂性。

Conclusion: 结论指出，这一研究为GL-ML公平性领域的未来发展提供了坚实的理论基础。

Abstract: Augmenting specialised machine learning techniques into traditional graph
learning models has achieved notable success across various domains, including
federated graph learning, dynamic graph learning, and graph transformers.
However, the intricate mechanisms of these specialised techniques introduce
significant challenges in maintaining model fairness, potentially resulting in
discriminatory outcomes in high-stakes applications such as recommendation
systems, disaster response, criminal justice, and loan approval. This paper
systematically examines the unique fairness challenges posed by Graph Learning
augmented with Machine Learning (GL-ML). It highlights the complex interplay
between graph learning mechanisms and machine learning techniques, emphasising
how the augmentation of machine learning both enhances and complicates
fairness. Additionally, we explore four critical techniques frequently employed
to improve fairness in GL-ML methods. By thoroughly investigating the root
causes and broader implications of fairness challenges in this rapidly evolving
field, this work establishes a robust foundation for future research and
innovation in GL-ML fairness.

</details>


### [73] [Unsupervised Feature Transformation via In-context Generation, Generator-critic LLM Agents, and Duet-play Teaming](https://arxiv.org/abs/2504.21304)
*Nanxu Gong,Xinyuan Wang,Wangyang Ying,Haoyue Bai,Sixun Dong,Haifeng Chen,Yanjie Fu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于LLM智能体的生成器-批评者框架，通过无监督学习高效转换高维特征空间，解决了现有方法在特征组合导航和监督依赖上的不足，并在实验中表现优于监督基线。


<details>
  <summary>Details</summary>
Motivation: 在高维数据（如材料性能筛选）中，标签获取成本高且耗时长，现有特征转换方法在无监督设置下效率不足，因此需要一种无需监督的高效特征转换框架。

Method: 采用了生成器-批评者双智能体框架，分三步实现：(1)批评者诊断数据并生成建议，(2)生成器根据建议生成标记化特征转换，(3)通过迭代反馈优化结果。还可替换批评者为人类专家实现人机协作。

Result: 实验表明，该框架在特征转换效率、鲁棒性和实际适用性上优于监督基线方法。

Conclusion: 该生成器-批评者框架为无监督特征转换提供了一种高效、可扩展的解决方案，尤其在标签稀缺的高维领域具有广泛应用潜力。

Abstract: Feature transformation involves generating a new set of features from the
original dataset to enhance the data's utility. In certain domains like
material performance screening, dimensionality is large and collecting labels
is expensive and lengthy. It highly necessitates transforming feature spaces
efficiently and without supervision to enhance data readiness and AI utility.
However, existing methods fall short in efficient navigation of a vast space of
feature combinations, and are mostly designed for supervised settings. To fill
this gap, our unique perspective is to leverage a generator-critic duet-play
teaming framework using LLM agents and in-context learning to derive
pseudo-supervision from unsupervised data. The framework consists of three
interconnected steps: (1) Critic agent diagnoses data to generate actionable
advice, (2) Generator agent produces tokenized feature transformations guided
by the critic's advice, and (3) Iterative refinement ensures continuous
improvement through feedback between agents. The generator-critic framework can
be generalized to human-agent collaborative generation, by replacing the critic
agent with human experts. Extensive experiments demonstrate that the proposed
framework outperforms even supervised baselines in feature transformation
efficiency, robustness, and practical applicability across diverse datasets.

</details>


### [74] [Capturing Conditional Dependence via Auto-regressive Diffusion Models](https://arxiv.org/abs/2504.21314)
*Xunpeng Huang,Yujin Han,Difan Zou,Yian Ma,Tong Zhang*

Main category: cs.LG

TL;DR: 本文研究了自回归（AR）扩散模型在捕捉数据条件依赖结构上的有效性，并与普通扩散模型对比。理论分析显示AR扩散模型采样误差更小，推理时间仅略增，实证结果证明了其在有明确条件依赖结构时的优势。


<details>
  <summary>Details</summary>
Motivation: 普通扩散模型难以捕捉现实世界中的高级关系（如物理定律或物体稳定存在），因为其无法充分建模数据的条件依赖结构。本文旨在通过AR扩散模型解决这一问题。

Method: 提出并理论分析了AR扩散模型，推导其在最弱数据假设下的采样误差，并通过实证比较其与普通扩散模型在条件依赖结构捕捉上的差异。

Result: AR扩散模型在数据存在条件依赖时表现显著优于普通扩散模型（如DDPM），但在无依赖结构时表现相当。其推理时间仅适度增加，仍适合大规模应用。

Conclusion: AR扩散模型能有效强化条件依赖结构的捕捉，理论误差更小且实用性强，但其优势高度依赖数据中的条件依赖关系。

Abstract: Diffusion models have demonstrated appealing performance in both image and
video generation. However, many works discover that they struggle to capture
important, high-level relationships that are present in the real world. For
example, they fail to learn physical laws from data, and even fail to
understand that the objects in the world exist in a stable fashion. This is due
to the fact that important conditional dependence structures are not adequately
captured in the vanilla diffusion models. In this work, we initiate an in-depth
study on strengthening the diffusion model to capture the conditional
dependence structures in the data. In particular, we examine the efficacy of
the auto-regressive (AR) diffusion models for such purpose and develop the
first theoretical results on the sampling error of AR diffusion models under
(possibly) the mildest data assumption. Our theoretical findings indicate that,
compared with typical diffusion models, the AR variant produces samples with a
reduced gap in approximating the data conditional distribution. On the other
hand, the overall inference time of the AR-diffusion models is only moderately
larger than that for the vanilla diffusion models, making them still practical
for large scale applications. We also provide empirical results showing that
when there is clear conditional dependence structure in the data, the AR
diffusion models captures such structure, whereas vanilla DDPM fails to do so.
On the other hand, when there is no obvious conditional dependence across
patches of the data, AR diffusion does not outperform DDPM.

</details>


### [75] [Q-function Decomposition with Intervention Semantics with Factored Action Spaces](https://arxiv.org/abs/2504.21326)
*Junkyu Lee,Tian Gao,Elliot Nelson,Miao Liu,Debarun Bhattacharjya,Songtao Lu*

Main category: cs.LG

TL;DR: 论文提出了一种基于因果统计的动作分解强化学习方法，通过投影Q函数避免组合动作空间的高复杂度，提高了样本效率和在线/离线环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 面对离散组合动作空间的高复杂度问题，现有的线性分解方法存在局限性，需探索更高效的无偏估计方法。

Method: 利用因果统计中的无混杂因子设定，在动作空间的低维投影子空间定义Q函数，提出动作分解强化学习框架，并与免模型算法结合。

Result: 在模型强化学习设定中降低样本复杂度，并在在线连续控制任务和离线脓毒症治疗环境中优于基线方法。

Conclusion: 通过因果驱动的动作分解，显著提升了复杂动作空间下的学习效率，具有实际应用潜力。

Abstract: Many practical reinforcement learning environments have a discrete factored
action space that induces a large combinatorial set of actions, thereby posing
significant challenges. Existing approaches leverage the regular structure of
the action space and resort to a linear decomposition of Q-functions, which
avoids enumerating all combinations of factored actions. In this paper, we
consider Q-functions defined over a lower dimensional projected subspace of the
original action space, and study the condition for the unbiasedness of
decomposed Q-functions using causal effect estimation from the no unobserved
confounder setting in causal statistics. This leads to a general scheme which
we call action decomposed reinforcement learning that uses the projected
Q-functions to approximate the Q-function in standard model-free reinforcement
learning algorithms. The proposed approach is shown to improve sample
complexity in a model-based reinforcement learning setting. We demonstrate
improvements in sample efficiency compared to state-of-the-art baselines in
online continuous control environments and a real-world offline sepsis
treatment environment.

</details>


### [76] [A Generalized Meta Federated Learning Framework with Theoretical Convergence Guarantees](https://arxiv.org/abs/2504.21327)
*Mohammad Vahid Jamali,Hamid Saber,Jung Hyun Bae*

Main category: cs.LG

TL;DR: Meta federated learning (FL) 提出了一种个性化FL框架，通过最小化代理在任意次数微调后的损失，提高模型个性化效果，并在实验中表现出优于传统方法的准确性和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统meta FL方法在高度异构数据分布下代理需要多次微调的问题，需要一种更灵活的方法来优化代理在多次微调后的局部模型表现。

Method: 提出了一种广义的meta FL框架，最小化代理在任意ν次微调后的平均损失，并设计了一种改进的FedAvg算法，进行了理论收敛分析。

Result: 实验表明，所提方法在真实数据集上具有更高的准确性和更快的收敛速度。

Conclusion: 该框架有效解决了异构数据下的个性化问题，为meta FL提供了更灵活和高效的解决方案。

Abstract: Meta federated learning (FL) is a personalized variant of FL, where multiple
agents collaborate on training an initial shared model without exchanging raw
data samples. The initial model should be trained in a way that current or new
agents can easily adapt it to their local datasets after one or a few
fine-tuning steps, thus improving the model personalization. Conventional meta
FL approaches minimize the average loss of agents on the local models obtained
after one step of fine-tuning. In practice, agents may need to apply several
fine-tuning steps to adapt the global model to their local data, especially
under highly heterogeneous data distributions across agents. To this end, we
present a generalized framework for the meta FL by minimizing the average loss
of agents on their local model after any arbitrary number $\nu$ of fine-tuning
steps. For this generalized framework, we present a variant of the well-known
federated averaging (FedAvg) algorithm and conduct a comprehensive theoretical
convergence analysis to characterize the convergence speed as well as behavior
of the meta loss functions in both the exact and approximated cases. Our
experiments on real-world datasets demonstrate superior accuracy and faster
convergence for the proposed scheme compared to conventional approaches.

</details>


### [77] [Multi-level datasets training method in Physics-Informed Neural Networks](https://arxiv.org/abs/2504.21328)
*Yao-Hsuan Tsai,Hsiao-Tung Juan,Pao-Hsiung Chiu,Chao-An Lin*

Main category: cs.LG

TL;DR: 本文提出了一种改进物理信息神经网络（PINNs）的新方法，通过多网格训练策略处理高频问题，显著提高了计算精度，减少了调参需求，并在高雷诺数流体问题中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决物理信息神经网络（PINNs）在处理高频或刚性偏微分方程时的精度和收敛性问题，减少计算成本和调参复杂度。

Method: 采用多网格训练策略，通过不同分辨率的训练样本逐步消除误差，避免复杂的网络结构调整和超参数优化。

Result: 在1D高频ODE和2D对流-扩散方程中验证后，应用于经典Lid-driven腔流问题，预测精度提升了30%至60%，并成功扩展到高雷诺数（Re=5000）。

Conclusion: 该方法简化了PINNs的训练流程，显著提升了高频问题的求解能力，为复杂物理问题的模拟提供了高效工具。

Abstract: Physics-Informed Neural Networks have emerged as a promising methodology for
solving PDEs, gaining significant attention in computer science and various
physics-related fields. Despite being demonstrated the ability to incorporate
the physics of laws for versatile applications, PINNs still struggle with the
challenging problems which are stiff to be solved and/or have high-frequency
components in the solutions, resulting in accuracy and convergence issues. It
may not only increase computational costs, but also lead to accuracy loss or
solution divergence. In this study, an alternative approach is proposed to
mitigate the above-mentioned problems. Inspired by the multi-grid method in CFD
community, the underlying idea of the current approach is to efficiently remove
different frequency errors via training with different levels of training
samples, resulting in a simpler way to improve the training accuracy without
spending time in fine-tuning of neural network structures, loss weights as well
as hyperparameters. To demonstrate the efficacy of current approach, we first
investigate canonical 1D ODE with high-frequency component and 2D
convection-diffusion equation with V-cycle training strategy. Finally, the
current method is employed for the classical benchmark problem of steady
Lid-driven cavity flows at different Reynolds numbers, to investigate the
applicability and efficacy for the problem involved multiple modes of high and
low frequency. By virtue of various training sequence modes, improvement
through predictions lead to 30% to 60% accuracy improvement. We also
investigate the synergies between current method and transfer learning
techniques for more challenging problems (i.e., higher Re). From the present
results, it also revealed that the current framework can produce good
predictions even for the case of Re=5000, demonstrating the ability to solve
complex high-frequency PDEs.

</details>


### [78] [Generative QoE Modeling: A Lightweight Approach for Telecom Networks](https://arxiv.org/abs/2504.21353)
*Vinti Nayar,Kanica Sachdev,Brejesh Lall*

Main category: cs.LG

TL;DR: 该论文提出了一种轻量级的生成建模框架（VQ-HMM），通过向量量化（VQ）预处理和隐马尔可夫模型（HMM），在资源受限的环境下高效预测用户体验（QoE），并兼顾计算效率和预测准确性。


<details>
  <summary>Details</summary>
Motivation: 当前QoE预测主要依赖深度学习模型，但其计算复杂度高，难以在资源受限或延迟敏感的场景中应用。因此，需要一种轻量级且高效的替代方案。

Method: 采用向量量化（VQ）技术将连续网络特征离散化为分类符号，再结合隐马尔可夫模型（HMM）进行时序建模，形成VQ-HMM流程。

Result: 在公开时间序列数据集上的实验表明，该方法能够有效捕捉动态QoE模式，并在资源有限或延迟关键的环境中表现优异。

Conclusion: VQ-HMM框架为复杂深度学习提供了一种可扩展的轻量级替代方案，特别适用于计算资源有限或延迟敏感的场景。

Abstract: Quality of Experience (QoE) prediction plays a crucial role in optimizing
resource management and enhancing user satisfaction across both
telecommunication and OTT services. While recent advances predominantly rely on
deep learning models, this study introduces a lightweight generative modeling
framework that balances computational efficiency, interpretability, and
predictive accuracy. By validating the use of Vector Quantization (VQ) as a
preprocessing technique, continuous network features are effectively
transformed into discrete categorical symbols, enabling integration with a
Hidden Markov Model (HMM) for temporal sequence modeling. This VQ-HMM pipeline
enhances the model's capacity to capture dynamic QoE patterns while supporting
probabilistic inference on new and unseen data. Experimental results on
publicly available time-series datasets incorporating both objective indicators
and subjective QoE scores demonstrate the viability of this approach in
real-time and resource-constrained environments, where inference latency is
also critical. The framework offers a scalable alternative to complex deep
learning methods, particularly in scenarios with limited computational
resources or where latency constraints are critical.

</details>


### [79] [A comparative study of deep learning and ensemble learning to extend the horizon of traffic forecasting](https://arxiv.org/abs/2504.21358)
*Xiao Zheng,Saeed Asadi Bagloee,Majid Sarvi*

Main category: cs.LG

TL;DR: 该论文探讨了机器学习（ML）和深度学习（DL）方法在长期（30天）交通流量预测中的表现，提出时间嵌入（time embedding）对周期性建模的重要性，并比较了XGBoost、RNN和Transformer等方法的性能。


<details>
  <summary>Details</summary>
Motivation: 交通流量预测对智能交通系统至关重要，但现有研究多关注短期预测，长期预测仍具挑战性。本文旨在评估ML和DL方法在长期预测中的表现，并提供改进方向。

Method: 通过实验比较XGBoost、基于RNN的方法和Transformer方法，并引入时间嵌入增强周期性和事件因素的理解。

Result: 结果显示，随着预测时间跨度延长，周期性建模比时序依赖捕捉更关键；时间嵌入显著提升RNN性能；XGBoost在仅基于时间特征时表现与DL方法相当。

Conclusion: 研究为长期交通预测提供了重要见解，强调了周期性建模和时间嵌入的作用，并建议未来研究进一步优化AI的学习能力。

Abstract: Traffic forecasting is vital for Intelligent Transportation Systems, for
which Machine Learning (ML) methods have been extensively explored to develop
data-driven Artificial Intelligence (AI) solutions. Recent research focuses on
modelling spatial-temporal correlations for short-term traffic prediction,
leaving the favourable long-term forecasting a challenging and open issue. This
paper presents a comparative study on large-scale real-world signalized
arterials and freeway traffic flow datasets, aiming to evaluate promising ML
methods in the context of large forecasting horizons up to 30 days. Focusing on
modelling capacity for temporal dynamics, we develop one ensemble ML method,
eXtreme Gradient Boosting (XGBoost), and a range of Deep Learning (DL) methods,
including Recurrent Neural Network (RNN)-based methods and the state-of-the-art
Transformer-based method. Time embedding is leveraged to enhance their
understanding of seasonality and event factors. Experimental results highlight
that while the attention mechanism/Transformer framework is effective for
capturing long-range dependencies in sequential data, as the forecasting
horizon extends, the key to effective traffic forecasting gradually shifts from
temporal dependency capturing to periodicity modelling. Time embedding is
particularly effective in this context, helping naive RNN outperform Informer
by 31.1% for 30-day-ahead forecasting. Meanwhile, as an efficient and robust
model, XGBoost, while learning solely from time features, performs
competitively with DL methods. Moreover, we investigate the impacts of various
factors like input sequence length, holiday traffic, data granularity, and
training data size. The findings offer valuable insights and serve as a
reference for future long-term traffic forecasting research and the improvement
of AI's corresponding learning capabilities.

</details>


### [80] [Synergy-CLIP: Extending CLIP with Multi-modal Integration for Robust Representation Learning](https://arxiv.org/abs/2504.21375)
*Sangyeon Cho,Jangyeong Jeon,Mingi Kim,Junyeong Kim*

Main category: cs.LG

TL;DR: Synergy-CLIP 扩展了 CLIP 架构，通过整合视觉、文本和音频模态来增强多模态表征学习，并引入了平衡的三模态数据集 VGG-sound+，在多种下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前多模态学习方法主要局限于双模态（如图像-文本），且缺乏平衡的大规模数据集，限制了多模态数据的充分利用。

Method: 提出 Synergy-CLIP 框架，通过扩展 CLIP 架构，实现对视觉、文本和音频三模态的均衡表征学习，并构建了平衡数据集 VGG-sound+。

Result: Synergy-CLIP 在零样本分类等任务中表现优于基线方法，并能通过缺失模态重建任务提取模态间的协同性。

Conclusion: 研究为多模态表征学习提供了新框架和数据集，推动了多模态领域的发展并开辟了新研究方向。

Abstract: Multi-modal representation learning has become a pivotal area in artificial
intelligence, enabling the integration of diverse modalities such as vision,
text, and audio to solve complex problems. However, existing approaches
predominantly focus on bimodal interactions, such as image-text pairs, which
limits their ability to fully exploit the richness of multi-modal data.
Furthermore, the integration of modalities in equal-scale environments remains
underexplored due to the challenges of constructing large-scale, balanced
datasets. In this study, we propose Synergy-CLIP, a novel framework that
extends the contrastive language-image pre-training (CLIP) architecture to
enhance multi-modal representation learning by integrating visual, textual, and
audio modalities. Unlike existing methods that focus on adapting individual
modalities to vanilla-CLIP, Synergy-CLIP aligns and captures latent information
across three modalities equally. To address the high cost of constructing
large-scale multi-modal datasets, we introduce VGG-sound+, a triple-modal
dataset designed to provide equal-scale representation of visual, textual, and
audio data. Synergy-CLIP is validated on various downstream tasks, including
zero-shot classification, where it outperforms existing baselines.
Additionally, we introduce a missing modality reconstruction task,
demonstrating Synergy-CLIP's ability to extract synergy among modalities in
realistic application scenarios. These contributions provide a robust
foundation for advancing multi-modal representation learning and exploring new
research directions.

</details>


### [81] [Sparse-to-Sparse Training of Diffusion Models](https://arxiv.org/abs/2504.21380)
*Inês Cardoso Oliveira,Decebal Constantin Mocanu,Luis A. Leiva*

Main category: cs.LG

TL;DR: 本文提出了一种稀疏到稀疏的训练范式，用于扩散模型（DMs），旨在提高训练和推理效率，实验表明稀疏DMs在减少参数和计算量的同时，性能与密集模型相当甚至更优。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然在图像合成等任务中表现出色，但其计算资源需求高。以往研究多关注推理效率，本文首次提出稀疏到稀疏训练范式，旨在优化训练和推理效率。

Method: 研究了无条件生成任务，采用三种方法（Static-DM、RigL-DM和MagRan-DM）在六个数据集上训练稀疏扩散模型（Latent Diffusion和ChiroDiff）。

Result: 稀疏DMs在减少可训练参数和FLOPs的同时，性能与密集模型相当或更优，并确定了稀疏到稀疏训练的安全有效值。

Conclusion: 稀疏到稀疏训练是一种有效的扩散模型优化方法，能够在不牺牲性能的前提下显著提升效率。

Abstract: Diffusion models (DMs) are a powerful type of generative models that have
achieved state-of-the-art results in various image synthesis tasks and have
shown potential in other domains, such as natural language processing and
temporal data modeling. Despite their stable training dynamics and ability to
produce diverse high-quality samples, DMs are notorious for requiring
significant computational resources, both in the training and inference stages.
Previous work has focused mostly on increasing the efficiency of model
inference. This paper introduces, for the first time, the paradigm of
sparse-to-sparse training to DMs, with the aim of improving both training and
inference efficiency. We focus on unconditional generation and train sparse DMs
from scratch (Latent Diffusion and ChiroDiff) on six datasets using three
different methods (Static-DM, RigL-DM, and MagRan-DM) to study the effect of
sparsity in model performance. Our experiments show that sparse DMs are able to
match and often outperform their Dense counterparts, while substantially
reducing the number of trainable parameters and FLOPs. We also identify safe
and effective values to perform sparse-to-sparse training of DMs.

</details>


### [82] [FAST-Q: Fast-track Exploration with Adversarially Balanced State Representations for Counterfactual Action Estimation in Offline Reinforcement Learning](https://arxiv.org/abs/2504.21383)
*Pulkit Agrawal,Rukma Talwadker,Aditya Pareek,Tridib Mukherjee*

Main category: cs.LG

TL;DR: FAST-Q提出了一种新的离线强化学习方法，通过梯度反转学习和Q值分解策略解决状态空间稀疏和策略偏差问题，显著提升游戏推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 针对离线强化学习中函数近似误差和状态空间稀疏性导致的Q值高估问题，尤其是在高风险的在线游戏推荐中，现有方法难以处理玩家心理和平台波动性带来的挑战。

Method: FAST-Q采用梯度反转学习构建平衡状态表示，支持离线反事实探索，并提出Q值分解策略以实现多目标优化。

Result: 实验表明，FAST-Q在玩家收益、生命周期价值、推荐参与度等方面均优于现有方法，并显著降低了推荐成本。

Conclusion: FAST-Q通过新方法有效解决了离线强化学习中的关键问题，为高波动性环境下的推荐系统提供了可行的解决方案。

Abstract: Recent advancements in state-of-the-art (SOTA) offline reinforcement learning
(RL) have primarily focused on addressing function approximation errors, which
contribute to the overestimation of Q-values for out-of-distribution actions, a
challenge that static datasets exacerbate. However, high stakes applications
such as recommendation systems in online gaming, introduce further complexities
due to player's psychology (intent) driven by gameplay experiences and the
inherent volatility on the platform. These factors create highly sparse,
partially overlapping state spaces across policies, further influenced by the
experiment path selection logic which biases state spaces towards specific
policies. Current SOTA methods constrain learning from such offline data by
clipping known counterfactual actions as out-of-distribution due to poor
generalization across unobserved states. Further aggravating conservative
Q-learning and necessitating more online exploration. FAST-Q introduces a novel
approach that (1) leverages Gradient Reversal Learning to construct balanced
state representations, regularizing the policy-specific bias between the
player's state and action thereby enabling counterfactual estimation; (2)
supports offline counterfactual exploration in parallel with static data
exploitation; and (3) proposes a Q-value decomposition strategy for
multi-objective optimization, facilitating explainable recommendations over
short and long-term objectives. These innovations demonstrate superiority of
FAST-Q over prior SOTA approaches and demonstrates at least 0.15 percent
increase in player returns, 2 percent improvement in lifetime value (LTV), 0.4
percent enhancement in the recommendation driven engagement, 2 percent
improvement in the player's platform dwell time and an impressive 10 percent
reduction in the costs associated with the recommendation, on our volatile
gaming platform.

</details>


### [83] [Enhanced Semi-Supervised Stamping Process Monitoring with Physically-Informed Feature Extraction](https://arxiv.org/abs/2504.21389)
*Jianyu Zhang,Jianshe Feng,Yizhang Zhu,Fanyu Qi*

Main category: cs.LG

TL;DR: 该研究提出了一种新颖的半监督冲压过程异常监控框架，结合加速度计信号和物理信息，有效检测异常，减少批量缺陷风险并提高生产效率。


<details>
  <summary>Details</summary>
Motivation: 针对冲压过程中频繁出现的异常问题，研究旨在通过半监督方法实时监控异常，解决样本分布不均的挑战，提升生产良率。

Method: 采用混合特征提取算法结合数据驱动与物理机制，并建立半监督异常检测模型，仅使用正常样本构建基准模型，通过新提出的偏差分数量化异常程度。

Result: 验证了特征提取方法的有效性，并在实际冲压车间数据中展示了所提框架的优越性能，显著提升了异常监控效果。

Conclusion: 该框架成功实现了冲压过程的实时异常监控，为减少缺陷风险和提高生产质量提供了有效解决方案。

Abstract: In tackling frequent anomalies in stamping processes, this study introduces a
novel semi-supervised in-process anomaly monitoring framework, utilizing
accelerometer signals and physics information, to capture the process anomaly
effectively. The proposed framework facilitates the construction of a
monitoring model with imbalanced sample distribution, which enables in-process
condition monitoring in real-time to prevent batch anomalies, which helps to
reduce batch defects risk and enhance production yield. Firstly, to effectively
capture key features from raw data containing redundant information, a hybrid
feature extraction algorithm is proposed to utilize data-driven methods and
physical mechanisms simultaneously. Secondly, to address the challenge brought
by imbalanced sample distribution, a semi-supervised anomaly detection model is
established, which merely employs normal samples to build a golden baseline
model, and a novel deviation score is proposed to quantify the anomaly level of
each online stamping stroke. The effectiveness of the proposed feature
extraction method is validated with various classification algorithms. A
real-world in-process dataset from stamping manufacturing workshop is employed
to illustrate the superiority of proposed semi-supervised framework with
enhance performance for process anomaly monitoring.

</details>


### [84] [MPEC: Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based Classifiers](https://arxiv.org/abs/2504.21427)
*Shermin Shahbazi,Mohammad-Reza Nasiri,Majid Ramezani*

Main category: cs.LG

TL;DR: MPEC是一种保留EEG信号流形结构的分类方法，通过结合协方差矩阵和RBF核的特征工程以及基于黎曼流形的改进K均值聚类，显著提高了分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有EEG信号分类方法未能充分考虑其非欧几里得流形结构，导致性能不佳，需要一种能保留流形信息的新方法。

Method: MPEC采用两步创新：1) 结合协方差矩阵和RBF核的特征工程；2) 在黎曼流形空间中使用改进的K均值聚类，并通过集成多个聚类分类器提升效果。

Result: 在BCI Competition IV dataset 2a上验证，MPEC表现出显著优于传统方法的分类性能。

Conclusion: MPEC通过保留EEG信号的流形几何结构，实现了更准确的分类，为BCI和神经假体应用提供了更优解决方案。

Abstract: Accurate classification of EEG signals is crucial for brain-computer
interfaces (BCIs) and neuroprosthetic applications, yet many existing methods
fail to account for the non-Euclidean, manifold structure of EEG data,
resulting in suboptimal performance. Preserving this manifold information is
essential to capture the true geometry of EEG signals, but traditional
classification techniques largely overlook this need. To this end, we propose
MPEC (Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based
Classifiers), that introduces two key innovations: (1) a feature engineering
phase that combines covariance matrices and Radial Basis Function (RBF) kernels
to capture both linear and non-linear relationships among EEG channels, and (2)
a clustering phase that employs a modified K-means algorithm tailored for the
Riemannian manifold space, ensuring local geometric sensitivity. Ensembling
multiple clustering-based classifiers, MPEC achieves superior results,
validated by significant improvements on the BCI Competition IV dataset 2a.

</details>


### [85] [Whispers of Data: Unveiling Label Distributions in Federated Learning Through Virtual Client Simulation](https://arxiv.org/abs/2504.21436)
*Zhixuan Ma,Haichang Gao,Junxiang Huang,Ping Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新颖且稳定的标签分布推断攻击方法，适用于多种场景，尤其在差分隐私防御机制下仍保持有效。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽能保护数据隐私，但仍易受推断攻击，尤其是标签推断攻击。现有方法对受害者客户端设置敏感且在防御策略下效果不佳。

Method: 通过估计受害者客户端的数据集大小，构建虚拟客户端，量化标签的时间泛化性，并利用其变化训练推断模型，预测标签分布比例。

Result: 在多个数据集上验证了方法的优越性，尤其在差分隐私防御下仍有效。

Conclusion: 该方法在多种场景下稳定且高效，具有实际应用潜力。

Abstract: Federated Learning enables collaborative training of a global model across
multiple geographically dispersed clients without the need for data sharing.
However, it is susceptible to inference attacks, particularly label inference
attacks.
  Existing studies on label distribution inference exhibits sensitive to the
specific settings of the victim client and typically underperforms under
defensive strategies. In this study, we propose a novel label distribution
inference attack that is stable and adaptable to various scenarios.
Specifically, we estimate the size of the victim client's dataset and construct
several virtual clients tailored to the victim client. We then quantify the
temporal generalization of each class label for the virtual clients and utilize
the variation in temporal generalization to train an inference model that
predicts the label distribution proportions of the victim client.
  We validate our approach on multiple datasets, including MNIST,
Fashion-MNIST, FER2013, and AG-News. The results demonstrate the superiority of
our method compared to state-of-the-art techniques. Furthermore, our attack
remains effective even under differential privacy defense mechanisms,
underscoring its potential for real-world applications.

</details>


### [86] [xEEGNet: Towards Explainable AI in EEG Dementia Classification](https://arxiv.org/abs/2504.21457)
*Andrea Zanola,Louis Fabrice Tshimanga,Federico Del Pup,Marco Baiesi,Manfredo Atzori*

Main category: cs.LG

TL;DR: xEEGNet是一种新型、紧凑且可解释的神经网络，用于EEG数据分析，适用于痴呆症分类等神经疾病。


<details>
  <summary>Details</summary>
Motivation: 开发一个既能保持高性能又能解释临床相关性的紧凑模型，以克服深度学习中的“黑盒”问题和过拟合。

Method: 从ShallowNet逐步改进，分析其结构并简化参数，使用嵌套交叉验证评估性能，并结合临床视角解释模型权重。

Result: xEEGNet仅使用168个参数，性能与ShallowNet相当，同时减少了过拟合和性能波动，并展示了EEG数据中的关键频谱特征和拓扑分布。

Conclusion: 小型架构在EEG病理分类中可以与大型深度学习模型竞争，xEEGNet展示了紧凑模型在高性能与可解释性之间的平衡。

Abstract: This work presents xEEGNet, a novel, compact, and explainable neural network
for EEG data analysis. It is fully interpretable and reduces overfitting
through major parameter reduction. As an applicative use case, we focused on
classifying common dementia conditions, Alzheimer's and frontotemporal
dementia, versus controls. xEEGNet is broadly applicable to other neurological
conditions involving spectral alterations. We initially used ShallowNet, a
simple and popular model from the EEGNet-family. Its structure was analyzed and
gradually modified to move from a "black box" to a more transparent model,
without compromising performance. The learned kernels and weights were examined
from a clinical standpoint to assess medical relevance. Model variants,
including ShallowNet and the final xEEGNet, were evaluated using robust
Nested-Leave-N-Subjects-Out cross-validation for unbiased performance
estimates. Variability across data splits was explained using embedded EEG
representations, grouped by class and set, with pairwise separability to
quantify group distinction. Overfitting was assessed through
training-validation loss correlation and training speed. xEEGNet uses only 168
parameters, 200 times fewer than ShallowNet, yet retains interpretability,
resists overfitting, achieves comparable median performance (-1.5%), and
reduces variability across splits. This variability is explained by embedded
EEG representations: higher accuracy correlates with greater separation between
test set controls and Alzheimer's cases, without significant influence from
training data. xEEGNet's ability to filter specific EEG bands, learn
band-specific topographies, and use relevant spectral features demonstrates its
interpretability. While large deep learning models are often prioritized for
performance, this study shows smaller architectures like xEEGNet can be equally
effective in EEG pathology classification.

</details>


### [87] [Deep Learning Optimization Using Self-Adaptive Weighted Auxiliary Variables](https://arxiv.org/abs/2504.21501)
*Yaru Liu,Yiqi Gu,Michael K. Ng*

Main category: cs.LG

TL;DR: 提出了一种新的优化框架，通过引入辅助变量和自适应权重来解决深度学习中梯度下降的低效问题。


<details>
  <summary>Details</summary>
Motivation: 针对深度学习中损失函数的高度非凸性和梯度消失问题，提出优化框架以提高训练效率。

Method: 引入辅助变量分离神经网络层，设计自适应权重保持新损失函数与原损失函数的一致性。

Result: 数值实验验证了新方法的有效性和鲁棒性，性能优于传统梯度下降。

Conclusion: 该方法为深度学习中的优化问题提供了新的思路和实用工具。

Abstract: In this paper, we develop a new optimization framework for the least squares
learning problem via fully connected neural networks or physics-informed neural
networks. The gradient descent sometimes behaves inefficiently in deep learning
because of the high non-convexity of loss functions and the vanishing gradient
issue. Our idea is to introduce auxiliary variables to separate the layers of
the deep neural networks and reformulate the loss functions for ease of
optimization. We design the self-adaptive weights to preserve the consistency
between the reformulated loss and the original mean squared loss, which
guarantees that optimizing the new loss helps optimize the original problem.
Numerical experiments are presented to verify the consistency and show the
effectiveness and robustness of our models over gradient descent.

</details>


### [88] [Towards proactive self-adaptive AI for non-stationary environments with dataset shifts](https://arxiv.org/abs/2504.21565)
*David Fernández Narro,Pablo Ferri,Juan M. García-Gómez,Carlos Sáez*

Main category: cs.LG

TL;DR: 提出了一种自适应的AI方法（pro-adaptive），通过建模AI参数的时间轨迹来预测短期参数值，提升了非平稳环境下的性能。


<details>
  <summary>Details</summary>
Motivation: AI模型在非平稳环境中（尤其是医疗场景）性能容易下降，且缺乏及时更新的标注数据。

Method: 使用多项式样条基和功能数据分析框架建模参数时间轨迹，并用逻辑回归模型验证了方法对多种数据偏移的有效性。

Result: 在模拟和真实COVID-19数据集上验证了该方法优于基线模型，且无需更新训练数据。

Conclusion: 为动态环境中的自适应AI研究奠定了基础，且兼容数据保护要求。

Abstract: Artificial Intelligence (AI) models deployed in production frequently face
challenges in maintaining their performance in non-stationary environments.
This issue is particularly noticeable in medical settings, where temporal
dataset shifts often occur. These shifts arise when the distributions of
training data differ from those of the data encountered during deployment over
time. Further, new labeled data to continuously retrain AI is not typically
available in a timely manner due to data access limitations. To address these
challenges, we propose a proactive self-adaptive AI approach, or pro-adaptive,
where we model the temporal trajectory of AI parameters, allowing us to
short-term forecast parameter values. To this end, we use polynomial spline
bases, within an extensible Functional Data Analysis framework. We validate our
methodology with a logistic regression model addressing prior probability
shift, covariate shift, and concept shift. This validation is conducted on both
a controlled simulated dataset and a publicly available real-world COVID-19
dataset from Mexico, with various shifts occurring between 2020 and 2024. Our
results indicate that this approach enhances the performance of AI against
shifts compared to baseline stable models trained at different time distances
from the present, without requiring updated training data. This work lays the
foundation for pro-adaptive AI research against dynamic, non-stationary
environments, being compatible with data protection, in resilient AI production
environments for health.

</details>


### [89] [On Advancements of the Forward-Forward Algorithm](https://arxiv.org/abs/2504.21662)
*Mauricio Ortiz Torres,Markus Lange,Arne P. Raulf*

Main category: cs.LG

TL;DR: 论文提出改进的Forward-Forward算法，通过结合卷积通道分组、学习率调度和独立块结构等技术，在CIFAR10数据集上实现了20%测试错误率的降低，并推出轻量级模型以适配低性能硬件。


<details>
  <summary>Details</summary>
Motivation: 旨在解决Forward-Forward算法在处理复杂任务（如CIFAR10数据集）时的性能不足问题，同时保持算法的灵活性和低内存占用，并为低容量硬件提供可行方案。

Method: 采用卷积通道分组、动态学习率调度及独立块结构训练方法，并设计轻量级模型以减少参数数量。

Result: 测试错误率降低20%，轻量模型在测试错误率(21±6)%范围内，参数数量介于164,706至754,386之间。

Conclusion: 改进的算法显著提升了性能，轻量模型为低硬件资源场景提供了解决方案，为未来神经网络验证与验证研究奠定基础。

Abstract: The Forward-Forward algorithm has evolved in machine learning research,
tackling more complex tasks that mimic real-life applications. In the last
years, it has been improved by several techniques to perform better than its
original version, handling a challenging dataset like CIFAR10 without losing
its flexibility and low memory usage. We have shown in our results that
improvements are achieved through a combination of convolutional channel
grouping, learning rate schedules, and independent block structures during
training that lead to a 20\% decrease in test error percentage. Additionally,
to approach further implementations on low-capacity hardware projects we have
presented a series of lighter models that achieve low test error percentages
within (21$\pm$6)\% and number of trainable parameters between 164,706 and
754,386. This serving also as a basis for our future study on complete
verification and validation of these kinds of neural networks.

</details>


### [90] [Recursive KL Divergence Optimization: A Dynamic Framework for Representation Learning](https://arxiv.org/abs/2504.21707)
*Anthony D Martin*

Main category: cs.LG

TL;DR: 该论文提出了一种称为递归KL散度优化（RKDO）的动态形式化方法，将表示学习重新定义为KL散度在数据邻域上的演化。相比静态方法，RKDO在损失值和计算资源上均有显著优势。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如I-Con）通过固定邻域条件分布的KL散度统一学习范式，但忽略了学习过程中的递归结构。本文旨在通过动态形式化更好地捕捉这一结构。

Method: 提出RKDO框架，将表示学习建模为KL散度在数据邻域上的递归演化，涵盖对比学习、聚类和降维等静态方法。

Result: 实验显示RKDO在三个数据集上损失值降低约30%，计算资源节省60%-80%，表明其优化效率显著优于静态方法。

Conclusion: RKDO的递归更新机制为表示学习提供了更高效的优化路径，尤其适用于资源受限的场景。

Abstract: We propose a generalization of modern representation learning objectives by
reframing them as recursive divergence alignment processes over localized
conditional distributions While recent frameworks like Information Contrastive
Learning I-Con unify multiple learning paradigms through KL divergence between
fixed neighborhood conditionals we argue this view underplays a crucial
recursive structure inherent in the learning process. We introduce Recursive KL
Divergence Optimization RKDO a dynamic formalism where representation learning
is framed as the evolution of KL divergences across data neighborhoods. This
formulation captures contrastive clustering and dimensionality reduction
methods as static slices while offering a new path to model stability and local
adaptation. Our experiments demonstrate that RKDO offers dual efficiency
advantages approximately 30 percent lower loss values compared to static
approaches across three different datasets and 60 to 80 percent reduction in
computational resources needed to achieve comparable results. This suggests
that RKDOs recursive updating mechanism provides a fundamentally more efficient
optimization landscape for representation learning with significant
implications for resource constrained applications.

</details>


### [91] [Learning Heterogeneous Performance-Fairness Trade-offs in Federated Learning](https://arxiv.org/abs/2504.21775)
*Rongguang Ye,Ming Tang*

Main category: cs.LG

TL;DR: 论文提出HetPFL方法，通过自适应偏好采样和超网络融合解决联邦学习中性能和公平性权衡的异质性问题，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法在联邦学习中使用超网络处理性能与公平性权衡时，忽略客户端本地Pareto前沿的异质性，且未考虑本地与全局Pareto前沿的泛化差距。

Method: HetPFL包含两个模块：PSA（自适应确定客户端最优偏好采样分布）和PHF（偏好感知超网络融合，确保全局Pareto性能）。

Result: 理论证明HetPFL在较弱假设下线性收敛；实验表明其在四个数据集上显著优于七个基线方法。

Conclusion: HetPFL有效解决了异质性和泛化问题，提升了本地和全局Pareto前沿的学习质量。

Abstract: Recent methods leverage a hypernet to handle the performance-fairness
trade-offs in federated learning. This hypernet maps the clients' preferences
between model performance and fairness to preference-specifc models on the
trade-off curve, known as local Pareto front. However, existing methods
typically adopt a uniform preference sampling distribution to train the
hypernet across clients, neglecting the inherent heterogeneity of their local
Pareto fronts. Meanwhile, from the perspective of generalization, they do not
consider the gap between local and global Pareto fronts on the global dataset.
To address these limitations, we propose HetPFL to effectively learn both local
and global Pareto fronts. HetPFL comprises Preference Sampling Adaptation (PSA)
and Preference-aware Hypernet Fusion (PHF). PSA adaptively determines the
optimal preference sampling distribution for each client to accommodate
heterogeneous local Pareto fronts. While PHF performs preference-aware fusion
of clients' hypernets to ensure the performance of the global Pareto front. We
prove that HetPFL converges linearly with respect to the number of rounds,
under weaker assumptions than existing methods. Extensive experiments on four
datasets show that HetPFL significantly outperforms seven baselines in terms of
the quality of learned local and global Pareto fronts.

</details>


### [92] [Stable Trajectory Clustering: An Efficient Split and Merge Algorithm](https://arxiv.org/abs/2504.21808)
*Atieh Rahmani,Mansoor Davoodi,Justin M. Calabrese*

Main category: cs.LG

TL;DR: 该论文基于DBSCAN线段聚类提出了全轨迹聚类和子轨迹聚类算法，通过线段的分裂与合并事件处理移动对象轨迹，并引入稳定轨迹聚类算法以减少临时异常的影响，从而提高聚类稳定性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决现有轨迹聚类算法因处理临时异常而分裂轨迹导致聚类模式不稳定的问题，改进聚类结果的可靠性和解释性。

Method: 论文通过DBSCAN线段聚类（包括分裂与合并事件）设计全轨迹和子轨迹聚类算法，并利用滑动窗口模型识别相似子轨迹；提出稳定轨迹聚类算法，采用平均绝对偏差选择性忽略瞬态偏差。

Result: 实验验证了所提算法在真实轨迹数据集上的有效性和对参数变化的敏感性，显示稳定轨迹聚类算法能显著提升聚类稳定性。

Conclusion: 研究表明，选择性忽略瞬态偏差可保持聚类完整性，同时增强其稳定性和可解释性，为轨迹分析提供了更可靠的工具。

Abstract: Clustering algorithms group data points by characteristics to identify
patterns. Over the past two decades, researchers have extended these methods to
analyze trajectories of humans, animals, and vehicles, studying their behavior
and movement across applications. This paper presents whole-trajectory
clustering and sub-trajectory clustering algorithms based on DBSCAN line
segment clustering, which encompasses two key events: split and merge of line
segments. The events are employed by object movement history and the average
Euclidean distance between line segments. In this framework, whole-trajectory
clustering considers entire entities' trajectories, whereas sub-trajectory
clustering employs a sliding window model to identify similar sub-trajectories.
Many existing trajectory clustering algorithms respond to temporary anomalies
in data by splitting trajectories, which often obscures otherwise consistent
clustering patterns and leads to less reliable insights. We introduce the
stable trajectory clustering algorithm, which leverages the mean absolute
deviation concept to demonstrate that selective omission of transient
deviations not only preserves the integrity of clusters but also improves their
stability and interpretability. We run all proposed algorithms on real
trajectory datasets to illustrate their effectiveness and sensitivity to
parameter variations.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [93] [A Formalism for Optimal Search with Dynamic Heuristics](https://arxiv.org/abs/2504.21131)
*Remo Christen,Florian Pommerening,Clemens Büchner,Malte Helmert*

Main category: cs.AI

TL;DR: 该论文探讨了动态启发式在搜索算法中的应用，通过形式化动态启发式概念并在通用算法框架中实现，证明了其最优性，并将现有经典规划方法视为特例。


<details>
  <summary>Details</summary>
Motivation: 传统启发式搜索仅依赖状态，而动态启发式则积累搜索历史信息。现有方法在$	ext{A}^*$类算法中使用动态启发式时，往往忽略其复杂性，直接引用经典$	ext{A}^*$的最优性结果。本文旨在填补这一研究空白。

Method: 论文形式化了动态启发式的概念，并将其融入通用算法框架。通过一个具体实例化模型（$	ext{A}^*$与动态启发式结合），证明了其一般最优性结果。

Result: 研究展示了动态启发式在框架中的最优性，并指出经典规划中的现有方法可作为该实例化的特例，从而直接应用其最优性结论。

Conclusion: 本文通过形式化和实例化动态启发式，为搜索算法提供了更通用的最优性保证，扩展了经典规划方法的理论基础。

Abstract: While most heuristics studied in heuristic search depend only on the state,
some accumulate information during search and thus also depend on the search
history. Various existing approaches use such dynamic heuristics in
$\mathrm{A}^*$-like algorithms and appeal to classic results for $\mathrm{A}^*$
to show optimality. However, doing so ignores the complexities of searching
with a mutable heuristic. In this paper we formalize the idea of dynamic
heuristics and use them in a generic algorithm framework. We study a particular
instantiation that models $\mathrm{A}^*$ with dynamic heuristics and show
general optimality results. Finally we show how existing approaches from
classical planning can be viewed as special cases of this instantiation, making
it possible to directly apply our optimality results.

</details>


### [94] [AffectEval: A Modular and Customizable Framework for Affective Computing](https://arxiv.org/abs/2504.21184)
*Emily Zhou,Khushboo Khatri,Yixue Zhao,Bhaskar Krishnamachari*

Main category: cs.AI

TL;DR: AffectEval是一个模块化、可定制的情感计算框架，旨在减少开发情感计算管道时的手动工作和重复劳动，实验验证其减少90%的编程工作量。


<details>
  <summary>Details</summary>
Motivation: 当前情感计算领域缺乏支持多模态、多领域情感识别应用的软件框架，导致不同应用管道开发时需重复工作，效率低下。

Method: 提出AffectEval框架，通过模块化和定制化设计，减少手动操作和重复工作。通过复现先前实验验证其有效性。

Result: 实验显示，AffectEval可减少90%的原始代码量，显著降低编程工作量。

Conclusion: AffectEval有效解决了情感计算管道开发中的效率和通用性问题，为领域提供了实用工具。

Abstract: The field of affective computing focuses on recognizing, interpreting, and
responding to human emotions, and has broad applications across education,
child development, and human health and wellness. However, developing affective
computing pipelines remains labor-intensive due to the lack of software
frameworks that support multimodal, multi-domain emotion recognition
applications. This often results in redundant effort when building pipelines
for different applications. While recent frameworks attempt to address these
challenges, they remain limited in reducing manual effort and ensuring
cross-domain generalizability. We introduce AffectEval, a modular and
customizable framework to facilitate the development of affective computing
pipelines while reducing the manual effort and duplicate work involved in
developing such pipelines. We validate AffectEval by replicating prior
affective computing experiments, and we demonstrate that our framework reduces
programming effort by up to 90%, as measured by the reduction in raw lines of
code.

</details>


### [95] [Theoretical Foundations for Semantic Cognition in Artificial Intelligence](https://arxiv.org/abs/2504.21218)
*Sebastian Dumbrava*

Main category: cs.AI

TL;DR: 本文提出了一种基于结构化语义状态的模块化认知架构，旨在构建能够自我调节、反思和目标导向思维的AI代理。通过整合哲学、认知科学和神经科学的理论，定义了一种动态的信念状态模型和生成性结构（如Null Tower），适用于符号系统和神经网络实现。


<details>
  <summary>Details</summary>
Motivation: 动机是构建一个能够模拟人类信念形成和调节过程的AI架构，以实现更结构化、可解释的推理和记忆能力。

Method: 方法包括定义动态信念状态为语言表达的集合，提出“认知真空”作为信念空间的概念起点，并设计可递归生成的Null Tower结构。架构旨在兼容符号和神经网络系统。

Result: 结果展示了一个分层框架，支持自我调节的认知代理，能够在结构化语义空间中实现信念的同化、抽象和反思。

Conclusion: 结论表明该架构为构建具有高阶认知能力的AI代理提供了理论基础，适用于多种AI系统的实现。

Abstract: This monograph presents a modular cognitive architecture for artificial
intelligence grounded in the formal modeling of belief as structured semantic
state. Belief states are defined as dynamic ensembles of linguistic expressions
embedded within a navigable manifold, where operators enable assimilation,
abstraction, nullification, memory, and introspection. Drawing from philosophy,
cognitive science, and neuroscience, we develop a layered framework that
enables self-regulating epistemic agents capable of reflective, goal-directed
thought. At the core of this framework is the epistemic vacuum: a class of
semantically inert cognitive states that serves as the conceptual origin of
belief space. From this foundation, the Null Tower arises as a generative
structure recursively built through internal representational capacities. The
theoretical constructs are designed to be implementable in both symbolic and
neural systems, including large language models, hybrid agents, and adaptive
memory architectures. This work offers a foundational substrate for
constructing agents that reason, remember, and regulate their beliefs in
structured, interpretable ways.

</details>


### [96] [Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2504.21277)
*Guanghao Zhou,Panjia Qiu,Cen Chen,Jie Wang,Zheming Yang,Jian Xu,Minghui Qiu*

Main category: cs.AI

TL;DR: 这篇论文综述了强化学习在多模态大语言模型中的推理能力整合，分析了两种主要的强化学习范式，并讨论了其在实际应用中的挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决多模态大语言模型在跨模态推理中的挑战，通过强化学习优化推理轨迹和对齐多模态信息。

Method: 方法包括系统回顾基于强化学习的推理方法，涵盖关键算法设计、奖励机制创新和实际应用，特别关注值无关和基于值的方法。

Result: 结果部分概述了基准数据集、评估协议和现有局限性，并提出了解决稀疏奖励、低效跨模态推理和实际部署约束的未来方向。

Conclusion: 结论是提供了全面的结构化指南，旨在推动多模态时代的强化学习推理研究。

Abstract: The integration of reinforcement learning (RL) into the reasoning
capabilities of Multimodal Large Language Models (MLLMs) has rapidly emerged as
a transformative research direction. While MLLMs significantly extend Large
Language Models (LLMs) to handle diverse modalities such as vision, audio, and
video, enabling robust reasoning across multimodal inputs remains a major
challenge. This survey systematically reviews recent advances in RL-based
reasoning for MLLMs, covering key algorithmic designs, reward mechanism
innovations, and practical applications. We highlight two main RL
paradigms--value-free and value-based methods--and analyze how RL enhances
reasoning abilities by optimizing reasoning trajectories and aligning
multimodal information. Furthermore, we provide an extensive overview of
benchmark datasets, evaluation protocols, and existing limitations, and propose
future research directions to address current bottlenecks such as sparse
rewards, inefficient cross-modal reasoning, and real-world deployment
constraints. Our goal is to offer a comprehensive and structured guide to
researchers interested in advancing RL-based reasoning in the multimodal era.

</details>


### [97] [Phi-4-reasoning Technical Report](https://arxiv.org/abs/2504.21318)
*Marah Abdin,Sahaj Agarwal,Ahmed Awadallah,Vidhisha Balachandran,Harkirat Behl,Lingjiao Chen,Gustavo de Rosa,Suriya Gunasekar,Mojan Javaheripi,Neel Joshi,Piero Kauffmann,Yash Lara,Caio César Teodoro Mendes,Arindam Mitra,Besmira Nushi,Dimitris Papailiopoulos,Olli Saarikivi,Shital Shah,Vaishnavi Shrivastava,Vibhav Vineet,Yue Wu,Safoora Yousefi,Guoqing Zheng*

Main category: cs.AI

TL;DR: 介绍了Phi-4-reasoning，一个140亿参数的推理模型，通过监督微调Phi-4和改进的强化学习变体Phi-4-reasoning-plus，在多领域推理任务中表现优于更大的开放权重模型。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过精心策划的训练数据和强化学习提升推理模型的性能，特别是在复杂推理任务上的表现。

Method: 使用监督微调（SFT）和结果驱动的强化学习（RL）训练Phi-4-reasoning及增强版Phi-4-reasoning-plus。

Result: 模型在数学推理、科学推理、编码等多个任务中显著优于大型开放权重模型，并接近DeepSeek-R1完整模型的性能。

Conclusion: 证明了精心设计的数据和混合训练方法对提升推理模型性能的有效性，同时指出评估推理模型性能的改进方向。

Abstract: We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that
achieves strong performance on complex reasoning tasks. Trained via supervised
fine-tuning of Phi-4 on carefully curated set of "teachable" prompts-selected
for the right level of complexity and diversity-and reasoning demonstrations
generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains
that effectively leverage inference-time compute. We further develop
Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based
reinforcement learning that offers higher performance by generating longer
reasoning traces. Across a wide range of reasoning tasks, both models
outperform significantly larger open-weight models such as
DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full
DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and
scientific reasoning, coding, algorithmic problem solving, planning, and
spatial understanding. Interestingly, we observe a non-trivial transfer of
improvements to general-purpose benchmarks as well. In this report, we provide
insights into our training data, our training methodologies, and our
evaluations. We show that the benefit of careful data curation for supervised
fine-tuning (SFT) extends to reasoning language models, and can be further
amplified by reinforcement learning (RL). Finally, our evaluation points to
opportunities for improving how we assess the performance and robustness of
reasoning models.

</details>


### [98] [IRL Dittos: Embodied Multimodal AI Agent Interactions in Open Spaces](https://arxiv.org/abs/2504.21347)
*Seonghee Lee,Denae Ford,John Tang,Sasa Junuzovic,Asta Roseway,Ed Cutrell,Kori Inkpen*

Main category: cs.AI

TL;DR: IRL Ditto是一种AI驱动的实体代理，用于在共享办公空间中代表远程同事，促进实时互动。研究通过四天实验发现其增强社交关系的能力取决于用户与远程同事原有的关系基础。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过IRL Ditto这一实体代理技术在共享办公空间中模拟远程同事的物理存在，从而影响同事间的互动与关系。

Method: 进行为期四天的研究，评估IRL Ditto在不同社交熟悉度下模拟存在感及促进有意义互动的能力。

Result: IRL Ditto增强社交关系的效果与用户和远程同事原有的关系基础密切相关。

Conclusion: 研究揭示了实体代理在提升分布式团队工作动态中的作用和局限性。

Abstract: We introduce the In Real Life (IRL) Ditto, an AI-driven embodied agent
designed to represent remote colleagues in shared office spaces, creating
opportunities for real-time exchanges even in their absence. IRL Ditto offers a
unique hybrid experience by allowing in-person colleagues to encounter a
digital version of their remote teammates, initiating greetings, updates, or
small talk as they might in person. Our research question examines: How can the
IRL Ditto influence interactions and relationships among colleagues in a shared
office space? Through a four-day study, we assessed IRL Ditto's ability to
strengthen social ties by simulating presence and enabling meaningful
interactions across different levels of social familiarity. We find that
enhancing social relationships depended deeply on the foundation of the
relationship participants had with the source of the IRL Ditto. This study
provides insights into the role of embodied agents in enriching workplace
dynamics for distributed teams.

</details>


### [99] [ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning](https://arxiv.org/abs/2504.21370)
*Jingyang Yi,Jiazheng Wang*

Main category: cs.AI

TL;DR: 论文提出了一种名为ShorterBetter的强化学习方法，帮助推理模型自动找到最优的思考链长度，从而在保持准确性的同时大幅缩短输出长度。


<details>
  <summary>Details</summary>
Motivation: 现有的推理模型（如OpenAI o3和DeepSeek-R1）通过扩展的思考链（CoT）提示在处理复杂任务时表现优异，但过长的推理痕迹会导致模型效率低下。研究人员希望找到一种方法，让模型能够在不依赖人工干预的情况下，动态确定最优的思考链长度。

Method: 论文提出了ShorterBetter方法，通过为每个问题采样多个输出，将其中最短的正确答案定义为样本最优长度（SOL），并利用强化学习引导模型动态调整推理长度。

Result: 在DeepSeek-Distill-Qwen-1.5B模型上的实验结果显示，ShorterBetter能够将输出长度缩短高达80%，同时保持推理任务的准确性。

Conclusion: 研究表明，过长的推理痕迹通常反映了推理方向的迷失，因此扩展的思考链具有高度可压缩性。ShorterBetter方法不仅提高了推理效率，还为优化模型推理提供了新思路。

Abstract: Reasoning models such as OpenAI o3 and DeepSeek-R1 have demonstrated strong
performance on reasoning-intensive tasks through extended Chain-of-Thought
(CoT) prompting. While longer reasoning traces can facilitate a more thorough
exploration of solution paths for complex problems, researchers have observed
that these models often "overthink", leading to inefficient inference. In this
paper, we introduce ShorterBetter, a simple yet effective reinforcement
learning methed that enables reasoning language models to discover their own
optimal CoT lengths without human intervention. By sampling multiple outputs
per problem and defining the Sample Optimal Length (SOL) as the shortest
correct response among all the outputs, our method dynamically guides the model
toward optimal inference lengths. Applied to the DeepSeek-Distill-Qwen-1.5B
model, ShorterBetter achieves up to an 80% reduction in output length on both
in-domain and out-of-domain reasoning tasks while maintaining accuracy. Our
analysis shows that overly long reasoning traces often reflect loss of
reasoning direction, and thus suggests that the extended CoT produced by
reasoning models is highly compressible.

</details>


### [100] [NGENT: Next-Generation AI Agents Must Integrate Multi-Domain Abilities to Achieve Artificial General Intelligence](https://arxiv.org/abs/2504.21433)
*Zhicong Li,Hangyu Mao,Jiangjin Yin,Mingzhe Xing,Zhiwei Xu,Yuanxing Zhang,Yang Xiao*

Main category: cs.AI

TL;DR: 该论文主张下一代AI代理（NGENT）需整合跨领域能力以推进人工通用智能（AGI）。尽管现有AI在机器人、角色扮演等专业任务中表现优异，但领域局限明显。未来AI应融合文本、视觉、机器人等多领域技术，构建统一框架。这种整合对实现人类智能的多样性和适应性至关重要。


<details>
  <summary>Details</summary>
Motivation: 当前AI虽在单一领域表现突出，但缺乏跨领域能力，限制了其向AGI的发展。论文探讨了整合多领域技术的必要性，以应对用户对跨功能AI的需求，并推动AGI的实现。

Method: 提出通过统一框架整合文本、视觉、机器人、强化学习、情感智能等AI子领域的技术，具体路径包括技术融合与需求驱动。

Result: 整合跨领域技术不仅可行，且能显著提升AI的适应性和多功能性，为AGI奠定基础。

Conclusion: 开发跨领域AI代理是通往AGI的关键步骤，需多领域技术协同与用户需求共同推动。

Abstract: This paper argues that the next generation of AI agent (NGENT) should
integrate across-domain abilities to advance toward Artificial General
Intelligence (AGI). Although current AI agents are effective in specialized
tasks such as robotics, role-playing, and tool-using, they remain confined to
narrow domains. We propose that future AI agents should synthesize the
strengths of these specialized systems into a unified framework capable of
operating across text, vision, robotics, reinforcement learning, emotional
intelligence, and beyond. This integration is not only feasible but also
essential for achieving the versatility and adaptability that characterize
human intelligence. The convergence of technologies across AI domains, coupled
with increasing user demand for cross-domain capabilities, suggests that such
integration is within reach. Ultimately, the development of these versatile
agents is a critical step toward realizing AGI. This paper explores the
rationale for this shift, potential pathways for achieving it.

</details>


### [101] [A Study on Group Decision Making Problem Based on Fuzzy Reasoning and Bayesian Networks](https://arxiv.org/abs/2504.21568)
*Shui-jin Rong,Wei Guo,Da-qing Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种结合模糊推理和贝叶斯网络的群决策系统，用于解决多目标属性的群决策问题，通过模糊规则库和动态优化的贝叶斯网络提高决策准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 针对多目标属性的群决策问题，传统方法难以处理定量挑战（如尺度差异和专家语言变量），因此需要一种更有效的方法。

Method: 构建模糊规则库解决定量挑战，设计分层贝叶斯网络并通过最大似然估计动态优化条件概率表，建模多维指标的非线性相关性。

Result: 在学生评价案例中，分类准确率达86.0%，F1值比传统方法提高53.4%；在多个实际场景中的计算实验验证了方法的性能和鲁棒性。

Conclusion: 该方法在规则构建和排序一致性上表现优异，适用于多样化场景，具有较高的可靠性和实用性。

Abstract: Aiming at the group decision - making problem with multi - objective
attributes, this study proposes a group decision - making system that
integrates fuzzy inference and Bayesian network. A fuzzy rule base is
constructed by combining threshold values, membership functions, expert
experience, and domain knowledge to address quantitative challenges such as
scale differences and expert linguistic variables. A hierarchical Bayesian
network is designed, featuring a directed acyclic graph with nodes selected by
experts, and maximum likelihood estimation is used to dynamically optimize the
conditional probability table, modeling the nonlinear correlations among
multidimensional indices for posterior probability aggregation. In a
comprehensive student evaluation case, this method is compared with the
traditional weighted scoring approach. The results indicate that the proposed
method demonstrates effectiveness in both rule criterion construction and
ranking consistency, with a classification accuracy of 86.0% and an F1 value
improvement of 53.4% over the traditional method. Additionally, computational
experiments on real - world datasets across various group decision scenarios
assess the method's performance and robustness, providing evidence of its
reliability in diverse contexts.

</details>


### [102] [Designing Control Barrier Function via Probabilistic Enumeration for Safe Reinforcement Learning Navigation](https://arxiv.org/abs/2504.21643)
*Luca Marzari,Francesco Trotti,Enrico Marchesini,Alessandro Farinelli*

Main category: cs.AI

TL;DR: 利用神经网路验证技术和分层控制框架设计安全强化学习导航策略。通过概率枚举识别不安全操作区域，构建基于控制屏障函数的控制层。实验验证了该框架在模拟和实际机器人任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了实现机器人在动态和不确定的现实环境中安全自主导航，研究提出了结合神经网路验证和分层控制的方法，以确保导航策略的安全性。

Method: 采用概率枚举识别不安全区域，构建控制屏障函数（CBFs）和控制层，并应用于任意策略。实验包括模拟和实际机器人任务。

Result: 实验结果显示，该框架能够修正不安全行为，同时保持高效导航。在标准移动机器人基准和动态水生环境监测任务中表现良好。

Conclusion: 基于分层验证的系统在复杂场景中展现出实现安全鲁棒导航行为的潜力。

Abstract: Achieving safe autonomous navigation systems is critical for deploying robots
in dynamic and uncertain real-world environments. In this paper, we propose a
hierarchical control framework leveraging neural network verification
techniques to design control barrier functions (CBFs) and policy correction
mechanisms that ensure safe reinforcement learning navigation policies. Our
approach relies on probabilistic enumeration to identify unsafe regions of
operation, which are then used to construct a safe CBF-based control layer
applicable to arbitrary policies. We validate our framework both in simulation
and on a real robot, using a standard mobile robot benchmark and a highly
dynamic aquatic environmental monitoring task. These experiments demonstrate
the ability of the proposed solution to correct unsafe actions while preserving
efficient navigation behavior. Our results show the promise of developing
hierarchical verification-based systems to enable safe and robust navigation
behaviors in complex scenarios.

</details>


### [103] [AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization](https://arxiv.org/abs/2504.21659)
*Haotian Luo,Haiying He,Yibo Wang,Jinluan Yang,Rui Liu,Naiqiang Tan,Xiaochun Cao,Dacheng Tao,Li Shen*

Main category: cs.AI

TL;DR: 提出自适应高效推理框架，通过混合长短CoT模型和双层偏好训练，优化推理效率，在五大数据集上平均推理长度减少50%。


<details>
  <summary>Details</summary>
Motivation: 现有长推理模型在复杂任务上表现优异但效率低下，且不同问题对推理深度的需求各异，需自适应策略优化推理效率。

Method: 两阶段框架：1) 合并长短CoT模型实现多样推理风格；2) 双层偏好训练（组级选择风格，实例级偏好简洁正确推理）。

Result: 显著降低推理成本（推理长度平均减少50%），同时保持性能，在五类数学数据集上验证有效性。

Conclusion: 自适应策略能高效优化大语言模型推理，未来可进一步探索

Abstract: Recently, long-thought reasoning models achieve strong performance on complex
reasoning tasks, but often incur substantial inference overhead, making
efficiency a critical concern. Our empirical analysis reveals that the benefit
of using Long-CoT varies across problems: while some problems require elaborate
reasoning, others show no improvement, or even degraded accuracy. This
motivates adaptive reasoning strategies that tailor reasoning depth to the
input. However, prior work primarily reduces redundancy within long reasoning
paths, limiting exploration of more efficient strategies beyond the Long-CoT
paradigm. To address this, we propose a novel two-stage framework for adaptive
and efficient reasoning. First, we construct a hybrid reasoning model by
merging long and short CoT models to enable diverse reasoning styles. Second,
we apply bi-level preference training to guide the model to select suitable
reasoning styles (group-level), and prefer concise and correct reasoning within
each style group (instance-level). Experiments demonstrate that our method
significantly reduces inference costs compared to other baseline approaches,
while maintaining performance. Notably, on five mathematical datasets, the
average length of reasoning is reduced by more than 50%, highlighting the
potential of adaptive strategies to optimize reasoning efficiency in large
language models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1

</details>


### [104] [Extension-ranking Semantics for Abstract Argumentation Preprint](https://arxiv.org/abs/2504.21683)
*Kenneth Skiba,Tjitze Rienstra,Matthias Thimm,Jesse Heyninck,Gabriele Kern-Isberner*

Main category: cs.AI

TL;DR: 论文提出了一个基于论证可接受性的框架，用于对抽象论证中的论点集合进行排序，扩展了Dung的扩展语义，并提出了一系列评价标准。


<details>
  <summary>Details</summary>
Motivation: 为了解决抽象论证中如何衡量论点集合的可接受性程度，提供一个更灵活的排序框架，弥补传统扩展语义的不足。

Method: 通过扩展Dung的扩展语义为扩展排序语义，引入基础关系和评价原则，并结合现有文献中的排序方法，形成一系列扩展排序语义。

Result: 论文提出了一族扩展排序语义，并通过评价原则验证了其合理性，同时展示了如何将文献中的方法应用于扩展排序语义。

Conclusion: 扩展排序语义为抽象论证中的集合排序提供了通用框架，且能满足良好行为的要求，为后续研究奠定了基础。

Abstract: In this paper, we present a general framework for ranking sets of arguments
in abstract argumentation based on their plausibility of acceptance. We present
a generalisation of Dung's extension semantics as extension-ranking semantics,
which induce a preorder over the power set of all arguments, allowing us to
state that one set is "closer" to being acceptable than another. To evaluate
the extension-ranking semantics, we introduce a number of principles that a
well-behaved extension-ranking semantics should satisfy. We consider several
simple base relations, each of which models a single central aspect of
argumentative reasoning. The combination of these base relations provides us
with a family of extension-ranking semantics. We also adapt a number of
approaches from the literature for ranking extensions to be usable in the
context of extension-ranking semantics, and evaluate their behaviour.

</details>


### [105] [Automatic Mapping of AutomationML Files to Ontologies for Graph Queries and Validation](https://arxiv.org/abs/2504.21694)
*Tom Westermann,Malte Ramonat,Johannes Hujer,Felix Gehlhoff,Alexander Fay*

Main category: cs.AI

TL;DR: 论文提出了一种将AutomationML转换为RDF三元组的方法，并构建了AutomationML的最新本体，使其更易于集成到工业知识图谱中，从而实现了更强大的查询和验证功能。


<details>
  <summary>Details</summary>
Motivation: AutomationML作为一种开放数据交换格式在自动化领域广泛应用，但由于其扩展语义限制了常见XML工具的适用性，如查询和数据验证。论文旨在解决这一问题。

Method: 提供了AutomationML标准的最新本体，并设计了一种声明性映射方法，将AutomationML模型自动转换为RDF三元组。

Result: 研究表明，将AutomationML转换为OWL后，可实现更强大的查询和验证功能，这在未经转换的情况下是不可能的。

Conclusion: 该研究为AutomationML信息与工业知识图谱的集成提供了便利，拓展了其在自动化领域的应用潜力。

Abstract: AutomationML has seen widespread adoption as an open data exchange format in
the automation domain. It is an open and vendor neutral standard based on the
extensible markup language XML. However, AutomationML extends XML with
additional semantics, that limit the applicability of common XML-tools for
applications like querying or data validation. This article provides
practitioners with 1) an up-to-date ontology of the concepts in the
AutomationML-standard, as well as 2) a declarative mapping to automatically
transform any AutomationML model into RDF triples. Together, these artifacts
allow practitioners an easy integration of AutomationML information into
industrial knowledge graphs. A study on examples from the automation domain
concludes that transforming AutomationML to OWL opens up new powerful ways for
querying and validation that are impossible without transformation.

</details>


### [106] [Is Intermediate Fusion All You Need for UAV-based Collaborative Perception?](https://arxiv.org/abs/2504.21774)
*Jiuwu Hao,Liguo Sun,Yuting Wan,Yueyang Wu,Ti Xiang,Haolin Song,Pin Lv*

Main category: cs.AI

TL;DR: 该论文提出了一种基于晚期中间融合（LIF）的通信高效协同感知框架，通过交换紧凑的检测结果并在特征表示层面进行融合，以减少无人机（UAV）协同感知中的通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有无人机协同感知方法忽视了无人机视角的独特性，导致通信开销过大，因此需要一种更高效的通信机制。

Method: 采用晚期中间融合（LIF）框架，结合视觉引导的位置嵌入（VPE）和基于框的虚拟增强特征（BoBEV），并引入不确定性驱动的通信机制选择高质量共享区域。

Result: 实验表明，LIF框架在最小通信带宽下实现了优越性能，验证了其有效性和实用性。

Conclusion: LIF框架为无人机协同感知提供了一种高效且实用的解决方案，显著降低了通信开销。

Abstract: Collaborative perception enhances environmental awareness through inter-agent
communication and is regarded as a promising solution to intelligent
transportation systems. However, existing collaborative methods for Unmanned
Aerial Vehicles (UAVs) overlook the unique characteristics of the UAV
perspective, resulting in substantial communication overhead. To address this
issue, we propose a novel communication-efficient collaborative perception
framework based on late-intermediate fusion, dubbed LIF. The core concept is to
exchange informative and compact detection results and shift the fusion stage
to the feature representation level. In particular, we leverage vision-guided
positional embedding (VPE) and box-based virtual augmented feature (BoBEV) to
effectively integrate complementary information from various agents.
Additionally, we innovatively introduce an uncertainty-driven communication
mechanism that uses uncertainty evaluation to select high-quality and reliable
shared areas. Experimental results demonstrate that our LIF achieves superior
performance with minimal communication bandwidth, proving its effectiveness and
practicality. Code and models are available at https://github.com/uestchjw/LIF.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [107] [Generate-then-Verify: Reconstructing Data from Limited Published Statistics](https://arxiv.org/abs/2504.21199)
*Terrance Liu,Eileen Xiao,Pratiksha Thaker,Adam Smith,Zhiwei Steven Wu*

Main category: stat.ML

TL;DR: 本文研究了从聚合统计中重建表格数据的问题，重点关注当多个数据集可能与发布的统计信息匹配时(即无法完美重建完整数据)，如何部分重建数据的某些行或列并确保其正确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中在统计信息足够丰富时可以完整重建数据集的情况，而本文关注统计信息较稀疏时，仍能通过部分重建获取隐私数据的挑战。

Method: 提出了一种新颖的整数规划方法，首先生成一组声明，然后验证这些声明是否在所有与发布统计一致的数据集中成立。

Result: 在美国十年期人口普查的住房微观数据上验证，表明即使在发布信息较稀疏时，隐私泄露风险仍存在。

Conclusion: 即使在统计信息不足以完整重建数据时，部分重建仍可能导致隐私泄露，需进一步研究防御措施。

Abstract: We study the problem of reconstructing tabular data from aggregate
statistics, in which the attacker aims to identify interesting claims about the
sensitive data that can be verified with 100% certainty given the aggregates.
Successful attempts in prior work have conducted studies in settings where the
set of published statistics is rich enough that entire datasets can be
reconstructed with certainty. In our work, we instead focus on the regime where
many possible datasets match the published statistics, making it impossible to
reconstruct the entire private dataset perfectly (i.e., when approaches in
prior work fail). We propose the problem of partial data reconstruction, in
which the goal of the adversary is to instead output a $\textit{subset}$ of
rows and/or columns that are $\textit{guaranteed to be correct}$. We introduce
a novel integer programming approach that first $\textbf{generates}$ a set of
claims and then $\textbf{verifies}$ whether each claim holds for all possible
datasets consistent with the published aggregates. We evaluate our approach on
the housing-level microdata from the U.S. Decennial Census release,
demonstrating that privacy violations can still persist even when information
published about such data is relatively sparse.

</details>


### [108] [Kernel Density Machines](https://arxiv.org/abs/2504.21419)
*Damir Filipovic,Paul Schneider*

Main category: stat.ML

TL;DR: KDM是一种新型的密度比估计器，适用于广泛概率测度，无需连续性假设，并通过低秩近似提高计算效率。论文提供了理论保证和实证结果。


<details>
  <summary>Details</summary>
Motivation: 为解决传统密度比估计方法在计算效率和适用范围上的局限性，提出了KDM。

Method: 在再生核希尔伯特空间框架下，结合低秩近似实现高效计算，适用于大样本场景。

Result: 理论保证包括渐近一致性和有限样本误差界，实证结果验证了其精确性。

Conclusion: KDM为密度比估计提供了高效、通用的解决方案，具有理论和实践价值。

Abstract: We introduce kernel density machines (KDM), a novel density ratio estimator
in a reproducing kernel Hilbert space setting. KDM applies to general
probability measures on countably generated measurable spaces without
restrictive assumptions on continuity, or the existence of a Lebesgue density.
For computational efficiency, we incorporate a low-rank approximation with
precisely controlled error that grants scalability to large-sample settings. We
provide rigorous theoretical guarantees, including asymptotic consistency, a
functional central limit theorem, and finite-sample error bounds, establishing
a strong foundation for practical use. Empirical results based on simulated and
real data demonstrate the efficacy and precision of KDM.

</details>


### [109] [Wasserstein-Aitchison GAN for angular measures of multivariate extremes](https://arxiv.org/abs/2504.21438)
*Stéphane Lhaut,Holger Rootzén,Johan Segers*

Main category: stat.ML

TL;DR: 论文提出了一种名为WA-GAN的新方法，用于模拟多维极端事件的概率估计，结合了极值分析与非参数GAN建模，在捕获极端数据依赖结构和生成新极端数据方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 为经济有效地缓解多维极端风险（如极端降雨、股价大幅波动、交通系统崩溃），需要估计未来此类风险发生的概率。传统方法难以准确模拟高维极端事件的依赖结构，因此需要一种新方法。

Method: 论文提出WA-GAN方法：首先将观测数据转换为单位帕累托尺度，假设其分布具有正则变化性；然后结合极值分析（处理边缘分布的尾部）与非参数GAN建模（处理角分布），通过Aitchison坐标转换和Wasserstein GAN生成新数据；最后逆向转换回原始数据尺度。

Result: 在模拟的50维逻辑模型和30维金融数据集中，WA-GAN在捕获极端事件依赖结构和生成新极端数据方面优于现有方法。

Conclusion: WA-GAN为多维极端风险的概率估计提供了一种高效且准确的新工具，具有良好的应用前景。

Abstract: Economically responsible mitigation of multivariate extreme risks -- extreme
rainfall in a large area, huge variations of many stock prices, widespread
breakdowns in transportation systems -- requires estimates of the probabilities
that such risks will materialize in the future. This paper develops a new
method, Wasserstein--Aitchison Generative Adversarial Networks (WA-GAN), which
provides simulated values of future $d$-dimensional multivariate extreme events
and which hence can be used to give estimates of such probabilities. The main
hypothesis is that, after transforming the observations to the unit-Pareto
scale, their distribution is regularly varying in the sense that the
distributions of their radial and angular components (with respect to the
$L_1$-norm) converge and become asymptotically independent as the radius gets
large. The method is a combination of standard extreme value analysis modeling
of the tails of the marginal distributions with nonparametric GAN modeling of
the angular distribution. For the latter, the angular values are transformed to
Aitchison coordinates in a full $(d-1)$-dimensional linear space, and a
Wasserstein GAN is trained on these coordinates and used to generate new
values. A reverse transformation is then applied to these values and gives
simulated values on the original data scale. The method shows good performance
compared to other existing methods in the literature, both in terms of
capturing the dependence structure of the extremes in the data, as well as in
generating accurate new extremes of the data distribution. The comparison is
performed on simulated multivariate extremes from a logistic model in
dimensions up to 50 and on a 30-dimensional financial data set.

</details>


### [110] [A comparison of generative deep learning methods for multivariate angular simulation](https://arxiv.org/abs/2504.21505)
*Jakob Benjamin Wessel,Callum J. R. Murphy-Barltrop,Emma S. Simpson*

Main category: stat.ML

TL;DR: 论文探讨了多种深度学习方法（如生成对抗网络、标准化流和流匹配）在高维角度变量模拟中的应用，并对比了传统von Mises-Fisher混合模型，展示了这些方法在真实气象海洋数据集中的适用性。


<details>
  <summary>Details</summary>
Motivation: 随着多元极值分析中几何和角度-径向框架的发展，高维角度变量的可靠模拟变得日益重要。传统方法在低维表现尚可，但高维下缺乏灵活性和扩展性，而深度学习因其灵活性有望解决这一问题。

Method: 研究比较了多种深度学习方法（生成对抗网络、标准化流、流匹配）与传统的von Mises-Fisher混合模型，并通过多项指标评估其性能。

Result: 深度学习方法能够捕捉复杂数据结构，在模拟角度变量上表现优于传统方法，特别是在气象海洋数据等真实场景中。

Conclusion: 深度学习为高维角度变量模拟提供了更灵活和可扩展的解决方案，证实了其在复杂数据建模中的潜力。

Abstract: With the recent development of new geometric and angular-radial frameworks
for multivariate extremes, reliably simulating from angular variables in
moderate-to-high dimensions is of increasing importance. Empirical approaches
have the benefit of simplicity, and work reasonably well in low dimensions, but
as the number of variables increases, they can lack the required flexibility
and scalability. Classical parametric models for angular variables, such as the
von Mises-Fisher (vMF) distribution, provide an alternative. Exploiting
mixtures of vMF distributions increases their flexibility, but there are cases
where even this is not sufficient to capture the intricate features that can
arise in data. Owing to their flexibility, generative deep learning methods are
able to capture complex data structures; they therefore have the potential to
be useful in the simulation of angular variables. In this paper, we explore a
range of deep learning approaches for this task, including generative
adversarial networks, normalizing flows and flow matching. We assess their
performance via a range of metrics and make comparisons to the more classical
approach of using a mixture of vMF distributions. The methods are also applied
to a metocean data set, demonstrating their applicability to real-world,
complex data structures.

</details>


### [111] [Balancing Interpretability and Flexibility in Modeling Diagnostic Trajectories with an Embedded Neural Hawkes Process Model](https://arxiv.org/abs/2504.21795)
*Yuankang Zhao,Matthew Engelhard*

Main category: stat.ML

TL;DR: 论文提出了一种基于事件嵌入空间的灵活影响核的霍克斯过程（HP）新方法，该方法通过神经网络实现，既保留了传统HP的可解释性，又具有神经网络的高灵活性，并在医疗数据中验证了其性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的霍克斯过程通过参数化影响函数捕捉事件的自我强化动态，而神经网络HP虽灵活但牺牲了可解释性。医疗领域对可解释性需求高，因此需要一种既能保持性能又能提供解释的方法。

Method: 提出了一种新HP方法，通过在事件嵌入空间中定义灵活的影响核（用神经网络实现），结合Transformer编码层进一步上下文化事件嵌入，实现了灵活性与可解释性的权衡。

Result: 实验表明，该方法能准确恢复模拟中的影响函数，在MIMIC-IV手术数据集上表现优异，且在儿童诊断数据集中获得了临床可解释性，无需Transformer层即可有效捕捉事件动态。

Conclusion: 灵活的影响核方法在保持性能的同时提供了可解释性，适用于电子健康记录等需要高解释性的领域，表明不必牺牲性能即可实现可解释性。

Abstract: The Hawkes process (HP) is commonly used to model event sequences with
self-reinforcing dynamics, including electronic health records (EHRs).
Traditional HPs capture self-reinforcement via parametric impact functions that
can be inspected to understand how each event modulates the intensity of
others. Neural network-based HPs offer greater flexibility, resulting in
improved fit and prediction performance, but at the cost of interpretability,
which is often critical in healthcare. In this work, we aim to understand and
improve upon this tradeoff. We propose a novel HP formulation in which impact
functions are modeled by defining a flexible impact kernel, instantiated as a
neural network, in event embedding space, which allows us to model large-scale
event sequences with many event types. This approach is more flexible than
traditional HPs yet more interpretable than other neural network approaches,
and allows us to explicitly trade flexibility for interpretability by adding
transformer encoder layers to further contextualize the event embeddings.
Results show that our method accurately recovers impact functions in
simulations, achieves competitive performance on MIMIC-IV procedure dataset,
and gains clinically meaningful interpretation on XX-EHR with children
diagnosis dataset even without transformer layers. This suggests that our
flexible impact kernel is often sufficient to capture self-reinforcing dynamics
in EHRs and other data effectively, implying that interpretability can be
maintained without loss of performance.

</details>


<div id='q-bio.CB'></div>

# q-bio.CB [[Back]](#toc)

### [112] [Glucagon and insulin production in pancreatic cells modeled using Petri nets and Boolean networks](https://arxiv.org/abs/2504.21578)
*Kamila Barylska,Frank Delaplace,Anna Gogolińska,Ewa Pańkowska*

Main category: q-bio.CB

TL;DR: 论文提出了基于Petri网的健康和糖尿病患者体内葡萄糖调节模型，重点关注胰岛素和胰高血糖素的分泌机制及其相互作用。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解葡萄糖调节的复杂过程，尤其是胰岛素和胰高血糖素在糖尿病中的作用。

Method: 使用Petri网建模胰岛素和胰高血糖素的分泌过程，并分析其动态行为；还将模型转换为布尔网络。

Result: 成功建立了胰岛素和胰高血糖素分泌的Petri网模型，并分析了其在健康和糖尿病状态下的动态变化。

Conclusion: 这些模型为理解糖尿病的发病机制提供了基础，并展示了Petri网在复杂生物过程建模中的潜力。

Abstract: Diabetes is a civilization chronic disease characterized by a constant
elevated concentration of glucose in the blood. Many processes are involved in
the glucose regulation, and their interactions are very complex. To better
understand those processes we set ourselves a goal to create a Petri net model
of the glucose regulation in the whole body. So far we have managed to create a
model of glycolysis and synthesis of glucose in the liver, and the general
overview models of the glucose regulation in a healthy and diabetic person. In
this paper we introduce Petri nets models of insulin secretion in beta cell of
the pancreas, and glucagon in the pancreas alpha cells. Those two hormones have
mutually opposite effects: insulin preventing hyperglycemia, and glucagon
preventing hypoglycemia. Understanding the mechanisms of insulin and glucagon
secretion constitutes the basis for understanding diabetes. We also present a
model in which both processes occur together, depending on the blood glucose
level. The dynamics of each model is analysed. Additionally, we transform the
overall insulin and glucagon secretion system to a Boolean network, following
standard transformation rules.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [113] [Don't Retrieve, Generate: Prompting LLMs for Synthetic Training Data in Dense Retrieval](https://arxiv.org/abs/2504.21015)
*Aarush Sinha*

Main category: cs.IR

TL;DR: 提出一种基于LLM生成查询和困难负例的端到端流程，替代传统的依赖于整个语料库的BM25或交叉编码器方法，性能相当但更高效。


<details>
  <summary>Details</summary>
Motivation: 传统方法如BM25或交叉编码器需要访问整个语料库且计算成本高，因此探索一种仅依赖LLM生成查询和负例的简化流程。

Method: 使用LLM直接从段落生成查询，并仅基于查询文本生成困难负例，无需访问语料库。

Result: 在多个BEIR基准数据集上，该方法在nDCG@10等指标上与BM25和交叉编码器方法性能相当。

Conclusion: LLM生成负例的方法无需复杂语料库挖掘，即可实现与传统方法相同的效果，提供更高效的训练途径。

Abstract: Training effective dense retrieval models often relies on hard negative (HN)
examples mined from the document corpus via methods like BM25 or cross-encoders
(CE), processes that can be computationally demanding and require full corpus
access. This paper introduces a different approach, an end-to-end pipeline
where a Large Language Model (LLM) first generates a query from a passage, and
then generates a hard negative example using \emph{only} that query text. This
corpus-free negative generation contrasts with standard mining techniques. We
evaluated this \textsc{LLM Query $\rightarrow$ LLM HN} approach against
traditional \textsc{LLM Query $\rightarrow$ BM25 HN} and \textsc{LLM Query
$\rightarrow$ CE HN} pipelines using E5-Base and GTE-Base models on several
BEIR benchmark datasets. Our results show the proposed all-LLM pipeline
achieves performance identical to both the BM25 and the computationally
intensive CE baselines across nDCG@10, Precision@10, and Recall@100 metrics.
This demonstrates that our corpus-free negative generation method matches the
effectiveness of complex, corpus-dependent mining techniques, offering a
potentially simpler and more efficient pathway for training high-performance
retrievers without sacrificing results. We make the dataset including the
queries and the hard-negatives for all three methods publicly available
https://huggingface.co/collections/chungimungi/arxiv-hard-negatives-68027bbc601ff6cc8eb1f449.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [114] [DGFNet: End-to-End Audio-Visual Source Separation Based on Dynamic Gating Fusion](https://arxiv.org/abs/2504.21366)
*Yinfeng Yu,Shiyu Sun*

Main category: cs.SD

TL;DR: 论文提出了一种基于门控机制的动态融合方法，解决了音频-视觉源分离任务中模态特征融合问题，同时引入音频注意力模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在音频和视觉特征融合时存在信息丢失或交互不足的问题，作者希望通过动态调整融合程度和增强音频表达能力来改进。

Method: 采用门控机制动态调整模态融合程度，并引入音频注意力模块增强音频特征表达。

Result: 在两个基准数据集上取得显著性能提升。

Conclusion: 该动态融合方法有效解决了模态融合问题，提升了音频-视觉源分离任务的性能。

Abstract: Current Audio-Visual Source Separation methods primarily adopt two design
strategies. The first strategy involves fusing audio and visual features at the
bottleneck layer of the encoder, followed by processing the fused features
through the decoder. However, when there is a significant disparity between the
two modalities, this approach may lead to the loss of critical information. The
second strategy avoids direct fusion and instead relies on the decoder to
handle the interaction between audio and visual features. Nonetheless, if the
encoder fails to integrate information across modalities adequately, the
decoder may be unable to effectively capture the complex relationships between
them. To address these issues, this paper proposes a dynamic fusion method
based on a gating mechanism that dynamically adjusts the modality fusion
degree. This approach mitigates the limitations of solely relying on the
decoder and facilitates efficient collaboration between audio and visual
features. Additionally, an audio attention module is introduced to enhance the
expressive capacity of audio features, thereby further improving model
performance. Experimental results demonstrate that our method achieves
significant performance improvements on two benchmark datasets, validating its
effectiveness and advantages in Audio-Visual Source Separation tasks.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [115] [Estimation of discrete distributions in relative entropy, and the deviations of the missing mass](https://arxiv.org/abs/2504.21787)
*Jaouad Mourtada*

Main category: math.ST

TL;DR: 论文研究了从独立同分布样本中估计有限字母表上的分布问题，以相对熵衡量精度。分析了经典Laplace估计器的性能并证明其在置信度无关估计器中的最优性，提出了一种简单置信度依赖平滑技术实现了极小极大最优高概率风险。还研究了适应稀疏分布情况的估计方法，并提出了一种数据依赖平滑估计器。


<details>
  <summary>Details</summary>
Motivation: 研究从有限字母表的样本中估计分布的高概率性能保证，特别是在样本量小于字母表大小时的稀疏适应性问题。

Method: 使用Laplace估计器分析上界和下界，提出置信度依赖平滑技术，并设计数据依赖平滑估计器以适应稀疏分布。

Result: 证明了Laplace估计器在置信度无关情况下的最优性，得到了极小极大最优风险，并在稀疏场景下建立了高概率风险界限。

Conclusion: 研究揭示了最优非渐近风险与理想渐近风险间的对数因子差异，并提出适应稀疏分布的有效估计方法。

Abstract: We study the problem of estimating a distribution over a finite alphabet from
an i.i.d. sample, with accuracy measured in relative entropy (Kullback-Leibler
divergence). While optimal expected risk bounds are known, high-probability
guarantees remain less well-understood. First, we analyze the classical Laplace
(add-$1$) estimator, obtaining matching upper and lower bounds on its
performance and showing its optimality among confidence-independent estimators.
We then characterize the minimax-optimal high-probability risk achievable by
any estimator, which is attained via a simple confidence-dependent smoothing
technique. Interestingly, the optimal non-asymptotic risk contains an
additional logarithmic factor over the ideal asymptotic risk. Next, motivated
by scenarios where the alphabet exceeds the sample size, we investigate methods
that adapt to the sparsity of the distribution at hand. We introduce an
estimator using data-dependent smoothing, for which we establish a
high-probability risk bound depending on two effective sparsity parameters. As
part of the analysis, we also derive a sharp high-probability upper bound on
the missing mass.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [116] [QAOA Parameter Transferability for Maximum Independent Set using Graph Attention Networks](https://arxiv.org/abs/2504.21135)
*Hanjing Xu,Xiaoyuan Liu,Alex Pothen,Ilya Safro*

Main category: quant-ph

TL;DR: 本文提出了一种基于图注意力网络（GAT）的量子近似优化算法（QAOA）参数迁移方案，用于解决最大独立集（MIS）问题，并结合分布式资源感知算法（HyDRA-MIS）在大规模图上取得与经典算法KaMIS竞争的结果。


<details>
  <summary>Details</summary>
Motivation: 量子计算中的QAOA需要优化变分参数，但非线性非凸优化过程复杂。本文旨在通过GAT实现参数迁移，提升QAOA在大规模图上的性能。

Method: 使用GAT迁移12和14顶点图的优化参数至大规模图，并设计HyDRA-MIS算法将问题分解为适合NISQ设备的子问题。

Result: 在数千顶点图上，结合GAT和HyDRA-MIS的方法与经典算法KaMIS表现相当。

Conclusion: GAT参数迁移和HyDRA-MIS的结合为QAOA在大规模问题上的应用提供了有效途径。

Abstract: The quantum approximate optimization algorithm (QAOA) is one of the promising
variational approaches of quantum computing to solve combinatorial optimization
problems. In QAOA, variational parameters need to be optimized by solving a
series of nonlinear, nonconvex optimization programs. In this work, we propose
a QAOA parameter transfer scheme using Graph Attention Networks (GAT) to solve
Maximum Independent Set (MIS) problems. We prepare optimized parameters for
graphs of 12 and 14 vertices and use GATs to transfer their parameters to
larger graphs. Additionally, we design a hybrid distributed resource-aware
algorithm for MIS (HyDRA-MIS), which decomposes large problems into smaller
ones that can fit onto noisy intermediate-scale quantum (NISQ) computers. We
integrate our GAT-based parameter transfer approach to HyDRA-MIS and
demonstrate competitive results compared to KaMIS, a state-of-the-art classical
MIS solver, on graphs with several thousands vertices.

</details>


### [117] [Efficient Quantum-Safe Homomorphic Encryption for Quantum Computer Programs](https://arxiv.org/abs/2504.21235)
*Ben Goertzel*

Main category: quant-ph

TL;DR: 该论文提出了一种基于格的量子程序同态加密方案，通过MLWE格和BNSF掩码实现量子安全的同态计算，并解决了实际应用中的隐私和效率问题。


<details>
  <summary>Details</summary>
Motivation: 旨在将经典同态加密扩展到量子领域，同时确保方案对量子对手的安全性，并解决实际量子计算中的隐私和效率问题。

Method: 使用MLWE格替代复合阶群，引入BNSF掩码隐藏振幅，并提出qIND-CPA安全模型和四混合归约证明安全性。

Result: 方案在100量子比特、深度10^3的Teleportation证明中仅需10毫秒，公钥仅32字节，展示了实际可行性。

Conclusion: 该方案表明完全同态的、基于知识库的量子推理可与近期量子云和标准后量子安全假设兼容。

Abstract: We present a lattice-based scheme for homomorphic evaluation of quantum
programs and proofs that remains secure against quantum adversaries. Classical
homomorphic encryption is lifted to the quantum setting by replacing
composite-order groups with Module Learning-With-Errors (MLWE) lattices and by
generalizing polynomial functors to bounded natural super functors (BNSFs). A
secret depolarizing BNSF mask hides amplitudes, while each quantum state is
stored as an MLWE ciphertext pair. We formalize security with the qIND-CPA game
that allows coherent access to the encryption oracle and give a four-hybrid
reduction to decisional MLWE.
  The design also covers practical issues usually left open. A typed QC-bridge
keeps classical bits produced by measurements encrypted yet still usable as
controls, with weak-measurement semantics for expectation-value workloads.
Encrypted Pauli twirls add circuit privacy. If a fixed knowledge base is
needed, its axioms are shipped as MLWE "capsules"; the evaluator can use them
but cannot read them. A rho-calculus driver schedules encrypted tasks across
several QPUs and records an auditable trace on an RChain-style ledger.
  Performance analysis shows that the extra lattice arithmetic fits inside
today's QPU idle windows: a 100-qubit, depth-10^3 teleportation-based proof
runs in about 10 ms, the public key (seed only) is 32 bytes, and even a
CCA-level key stays below 300 kB. A photonic Dirac-3 prototype that executes
homomorphic teleportation plus knowledge-base-relative amplitude checks appears
feasible with current hardware. These results indicate that fully homomorphic,
knowledge-base-aware quantum reasoning is compatible with near-term quantum
clouds and standard post-quantum security assumptions.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [118] [Data-driven operator learning for energy-efficient building control](https://arxiv.org/abs/2504.21243)
*Yuexin Bian,Yuanyuan Shi*

Main category: eess.SY

TL;DR: 该论文提出了一种结合CFD高精度与机器学习高效性的数据驱动框架，用于优化建筑通风控制，实现了显著的节能效果并保障了室内空气质量。


<details>
  <summary>Details</summary>
Motivation: 解决传统CFD模拟计算成本高、难以实时应用于建筑管理系统的问题，同时兼顾能效与空气质量。

Method: 通过训练神经算子变换器学习CFD数据中的控制动作与气流分布映射，构建基于梯度的优化控制框架。

Result: 实验表明，相比最大风量控制、基于规则的控制及其他数据驱动方法，该方法节能效果显著且能保持安全空气质量。

Conclusion: 该方法兼具实用性与可扩展性，为安全高效的建筑管理提供了可行方案。

Abstract: Energy-efficient ventilation control plays a vital role in reducing building
energy consumption while ensuring occupant health and comfort. While
Computational Fluid Dynamics (CFD) simulations offer high-fidelity modeling of
airflow for building HVAC design, their high computational cost makes them
impractical for practical adoption in real-time building management system. In
this work, we present a data-driven framework that combines the physical
accuracy of CFD with the computational efficiency of machine learning to enable
energy-efficient building ventilation control. Our method jointly optimizes
airflow supply rates and vent angles to reduce energy use and adhere to air
quality constraints. We train a neural operator transformer to learn the
mapping from building control actions to airflow field distributions using
high-resolution CFD data. This learned operator enables a gradient-based
control framework capable of optimal decision-making. Experimental results
demonstrate that our approach achieves substantial energy savings compared to
maximum airflow rate control, rule-based control, and data-driven control based
on regional average CO2 predictions, while consistently maintaining safe indoor
air quality. These results highlight the practicality and scalability of our
method for enabling safe and energy-efficient building management.

</details>


### [119] [Power Flow Approximations for Multiphase Distribution Networks using Gaussian Processes](https://arxiv.org/abs/2504.21260)
*Daniel Glover,Parikshit Pareek,Deepjyoti Deka,Anamika Dubey*

Main category: eess.SY

TL;DR: 该论文提出了一种基于高斯过程（GPs）的数据驱动方法，用于近似多相电力流模型，通过映射净负载注入到节点电压。实验显示GP模型在少量训练数据下能可靠预测非线性电力流解，并在训练效率和测试性能上优于深度神经网络（DNN）。


<details>
  <summary>Details</summary>
Motivation: 基于模型的学习方法在电力分配网络中因其数据效率和鲁棒性优于无模型方法。然而，有效模型学习需要底层电力流模型的学习型近似器。本研究旨在通过引入高斯过程（GPs）提升模型学习效果，减少训练数据需求。

Method: 采用高斯过程（GPs）来近似多相电力流模型，将净负载注入映射到节点电压。在IEEE 123-bus和8500节点分配测试馈线上进行仿真，并与深度神经网络（DNN）方法进行对比。

Result: GP模型在少量训练数据下能可靠预测非线性电力流解。与DNN相比，GP模型减少了85%的训练样本量（训练时间提升92.8%），同时平均绝对误差相对降低了99.9%。

Conclusion: 基于高斯过程（GPs）的数据驱动方法在电力流模型近似中表现出高效性和准确性，显著优于深度神经网络（DNN），尤其在数据效率和训练时间上具有显著优势。

Abstract: Learning-based approaches are increasingly leveraged to manage and coordinate
the operation of grid-edge resources in active power distribution networks.
Among these, model-based techniques stand out for their superior data
efficiency and robustness compared to model-free methods. However, effective
model learning requires a learning-based approximator for the underlying power
flow model. This study extends existing work by introducing a data-driven power
flow method based on Gaussian Processes (GPs) to approximate the multiphase
power flow model, by mapping net load injections to nodal voltages. Simulation
results using the IEEE 123-bus and 8500-node distribution test feeders
demonstrate that the trained GP model can reliably predict the nonlinear power
flow solutions with minimal training data. We also conduct a comparative
analysis of the training efficiency and testing performance of the proposed
GP-based power flow approximator against a deep neural network-based
approximator, highlighting the advantages of our data-efficient approach.
Results over realistic operating conditions show that despite an 85% reduction
in the training sample size (corresponding to a 92.8% improvement in training
time), GP models produce a 99.9% relative reduction in mean absolute error
compared to the baselines of deep neural networks.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [120] [Who Gets the Callback? Generative AI and Gender Bias](https://arxiv.org/abs/2504.21400)
*Sugat Chaturvedi,Rochana Chaturvedi*

Main category: econ.GN

TL;DR: 论文审核了多个开源大语言模型在招聘中的性别偏见，发现多数模型倾向男性，尤其高薪职位，且推荐结果与传统性别刻板印象高度一致。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI（尤其是大语言模型）在招聘中的性别偏见，揭示其对劳动力市场公平性和多样性的潜在影响。

Method: 使用33万条真实招聘广告数据，通过模型推荐男女候选人面试，并分析职业分类、语言特征及模拟招聘者个性。

Result: 模型普遍偏向男性（尤其高薪岗位），职业性别隔离现象明显；低宜人性人格的模拟招聘者能减少刻板印象。

Conclusion: AI招聘可能加剧劳动力市场偏见，需关注模型公平性及多样性影响。

Abstract: Generative artificial intelligence (AI), particularly large language models
(LLMs), is being rapidly deployed in recruitment and for candidate
shortlisting. We audit several mid-sized open-source LLMs for gender bias using
a dataset of 332,044 real-world online job postings. For each posting, we
prompt the model to recommend whether an equally qualified male or female
candidate should receive an interview callback. We find that most models tend
to favor men, especially for higher-wage roles. Mapping job descriptions to the
Standard Occupational Classification system, we find lower callback rates for
women in male-dominated occupations and higher rates in female-associated ones,
indicating occupational segregation. A comprehensive analysis of linguistic
features in job ads reveals strong alignment of model recommendations with
traditional gender stereotypes. To examine the role of recruiter identity, we
steer model behavior by infusing Big Five personality traits and simulating the
perspectives of historical figures. We find that less agreeable personas reduce
stereotyping, consistent with an agreeableness bias in LLMs. Our findings
highlight how AI-driven hiring may perpetuate biases in the labor market and
have implications for fairness and diversity within firms.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [121] [Generalised Label-free Artefact Cleaning for Real-time Medical Pulsatile Time Series](https://arxiv.org/abs/2504.21209)
*Xuhang Chen,Ihsane Olakorede,Stefan Yu Bögli,Wenhao Xu,Erta Beqiri,Xuemeng Li,Chenyu Tang,Zeyu Gao,Shuo Gao,Ari Ercole,Peter Smielewski*

Main category: eess.SP

TL;DR: GenClean是一个无需标签的实时伪影清洗框架，适用于多种医学脉动信号，证明了其在患者间和疾病间的泛化能力，并通过实际集成验证了实用性。


<details>
  <summary>Details</summary>
Motivation: 医学时间序列中的伪影影响临床决策，现有方法多依赖监督学习且忽略患者层面的分布偏移，需要一种通用的无标签解决方案。

Method: 提出GenClean框架，利用18万份动脉血压样本训练，并扩展到PPG信号，测试了其在患者内和跨患者、跨疾病场景的泛化能力。

Result: 实验显示模型在分布偏移下表现稳健，并在MIMIC-III数据库中验证了跨疾病有效性，成功集成到临床软件ICM+中。

Conclusion: GenClean为高分辨率医学时间序列分析提供了可靠的无标签解决方案，支持精准医疗发展。

Abstract: Artefacts compromise clinical decision-making in the use of medical time
series. Pulsatile waveforms offer probabilities for accurate artefact
detection, yet most approaches rely on supervised manners and overlook
patient-level distribution shifts. To address these issues, we introduce a
generalised label-free framework, GenClean, for real-time artefact cleaning and
leverage an in-house dataset of 180,000 ten-second arterial blood pressure
(ABP) samples for training. We first investigate patient-level generalisation,
demonstrating robust performances under both intra- and inter-patient
distribution shifts. We further validate its effectiveness through challenging
cross-disease cohort experiments on the MIMIC-III database. Additionally, we
extend our method to photoplethysmography (PPG), highlighting its applicability
to diverse medical pulsatile signals. Finally, its integration into ICM+, a
clinical research monitoring software, confirms the real-time feasibility of
our framework, emphasising its practical utility in continuous physiological
monitoring. This work provides a foundational step toward precision medicine in
improving the reliability of high-resolution medical time series analysis

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [122] [SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding](https://arxiv.org/abs/2504.21435)
*Chenkai Zhang,Yiming Lei,Zeming Liu,Haitao Leng,ShaoGuo Liu,Tingting Gao,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: 本文提出了SeriesBench，一个专注于评估多模态大语言模型（MLLMs）对叙事驱动系列视频理解能力的基准，并提出了PC-DCoT框架以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估独立视频的视觉元素，忽视了现实视频中常见的复杂连续叙事。SeriesBench旨在填补这一空白。

Method: 通过精选105个叙事系列，设计28种任务，结合长跨度叙事标注方法和全信息转换技术，提出PC-DCoT框架增强模型叙事推理能力。

Result: 现有MLLMs在SeriesBench上表现不佳，而PC-DCoT显著提升了模型性能。

Conclusion: SeriesBench和PC-DCoT强调了提升模型叙事理解能力的必要性，为MLLMs的未来发展提供了方向。

Abstract: With the rapid development of Multi-modal Large Language Models (MLLMs), an
increasing number of benchmarks have been established to evaluate the video
understanding capabilities of these models. However, these benchmarks focus on
\textbf{standalone} videos and mainly assess ``visual elements'' like human
actions and object states. In reality, contemporary videos often encompass
complex and continuous narratives, typically presented as a \textbf{series}. To
address this challenge, we propose \textbf{SeriesBench}, a benchmark consisting
of 105 carefully curated narrative-driven series, covering 28 specialized tasks
that require deep narrative understanding. Specifically, we first select a
diverse set of drama series spanning various genres. Then, we introduce a novel
long-span narrative annotation method, combined with a full-information
transformation approach to convert manual annotations into diverse task
formats. To further enhance model capacity for detailed analysis of plot
structures and character relationships within series, we propose a novel
narrative reasoning framework, \textbf{PC-DCoT}. Extensive results on
\textbf{SeriesBench} indicate that existing MLLMs still face significant
challenges in understanding narrative-driven series, while \textbf{PC-DCoT}
enables these MLLMs to achieve performance improvements. Overall, our
\textbf{SeriesBench} and \textbf{PC-DCoT} highlight the critical necessity of
advancing model capabilities to understand narrative-driven series, guiding the
future development of MLLMs. SeriesBench is publicly available at
https://github.com/zackhxn/SeriesBench-CVPR2025.

</details>


### [123] [Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models](https://arxiv.org/abs/2504.21559)
*Sangmin Woo,Kang Zhou,Yun Zhou,Shuai Wang,Sheng Guan,Haibo Ding,Lin Lee Cheong*

Main category: cs.CV

TL;DR: 通过视觉提示工程框架BBVPE，动态选择最优视觉提示以减少大型视觉语言模型中的物体幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLM）常出现物体幻觉，影响其可靠性，而现有视觉提示方法效果不一，需一种无需模型内部信息的解决方案。

Method: 提出黑盒视觉提示工程（BBVPE），通过候选视觉提示池和路由模型动态选择最有效的视觉提示，适用于开源和专有LVLM。

Result: 在POPE和CHAIR基准测试中，BBVPE显著减少了物体幻觉。

Conclusion: BBVPE是一种无需模型内部访问的实用方法，能有效提升LVLM的可靠性。

Abstract: Large Vision Language Models (LVLMs) often suffer from object hallucination,
which undermines their reliability. Surprisingly, we find that simple
object-based visual prompting -- overlaying visual cues (e.g., bounding box,
circle) on images -- can significantly mitigate such hallucination; however,
different visual prompts (VPs) vary in effectiveness. To address this, we
propose Black-Box Visual Prompt Engineering (BBVPE), a framework to identify
optimal VPs that enhance LVLM responses without needing access to model
internals. Our approach employs a pool of candidate VPs and trains a router
model to dynamically select the most effective VP for a given input image. This
black-box approach is model-agnostic, making it applicable to both open-source
and proprietary LVLMs. Evaluations on benchmarks such as POPE and CHAIR
demonstrate that BBVPE effectively reduces object hallucination.

</details>


### [124] [Emotion Recognition in Contemporary Dance Performances Using Laban Movement Analysis](https://arxiv.org/abs/2504.21154)
*Muhammad Turab,Philippe Colantoni,Damien Muselet,Alain Tremeau*

Main category: cs.CV

TL;DR: 该论文提出了一种新的当代舞蹈情感识别框架，通过改进Laban动作分析（LMA）特征描述符并引入新的鲁棒描述符，结合可解释机器学习方法，实现了96.85%的最高准确率。


<details>
  <summary>Details</summary>
Motivation: 改进现有舞蹈情感识别技术，结合定量和定性特征，提升识别准确率，以应用于表演分析、舞蹈训练和人机交互等领域。

Method: 从专业舞者的3D关键点数据中提取特征，结合改进的LMA和新型描述符，使用随机森林和支持向量机等分类器进行训练，并利用可解释机器学习方法分析特征重要性。

Result: 最高准确率达到96.85%，显著提升了当代舞蹈情感识别的性能。

Conclusion: 该框架不仅提高了情感识别的准确性，还通过特征解释增强了模型的可理解性，为舞蹈相关应用提供了实用工具。

Abstract: This paper presents a novel framework for emotion recognition in contemporary
dance by improving existing Laban Movement Analysis (LMA) feature descriptors
and introducing robust, novel descriptors that capture both quantitative and
qualitative aspects of the movement. Our approach extracts expressive
characteristics from 3D keypoints data of professional dancers performing
contemporary dance under various emotional states, and trains multiple
classifiers, including Random Forests and Support Vector Machines.
Additionally, we provide in-depth explanation of features and their impact on
model predictions using explainable machine learning methods. Overall, our
study improves emotion recognition in contemporary dance and offers promising
applications in performance analysis, dance training, and human--computer
interaction, with a highest accuracy of 96.85\%.

</details>


### [125] [Dance Style Recognition Using Laban Movement Analysis](https://arxiv.org/abs/2504.21166)
*Muhammad Turab,Philippe Colantoni,Damien Muselet,Alain Tremeau*

Main category: cs.CV

TL;DR: 该研究通过结合3D姿态估计、人体网格重建和动态滑动窗口方法，改进了舞蹈风格识别的LMA特征提取，显著提升了分类准确率至99.18%。


<details>
  <summary>Details</summary>
Motivation: 现有舞蹈风格识别方法缺乏对时间上下文和动态动作过渡的捕捉，限制了LMA特征的应用效果。

Method: 提出新流程：结合3D姿态估计、人体网格重建和地板感知建模提取LMA特征，并采用滑动窗口方法增加时间上下文。

Result: 分类准确率达99.18%，证明时间上下文的加入显著提升了识别性能。

Conclusion: 提出的方法通过增强LMA特征的时间维度，有效提升了复杂舞蹈动作的识别效果。

Abstract: The growing interest in automated movement analysis has presented new
challenges in recognition of complex human activities including dance. This
study focuses on dance style recognition using features extracted using Laban
Movement Analysis. Previous studies for dance style recognition often focus on
cross-frame movement analysis, which limits the ability to capture temporal
context and dynamic transitions between movements. This gap highlights the need
for a method that can add temporal context to LMA features. For this, we
introduce a novel pipeline which combines 3D pose estimation, 3D human mesh
reconstruction, and floor aware body modeling to effectively extract LMA
features. To address the temporal limitation, we propose a sliding window
approach that captures movement evolution across time in features. These
features are then used to train various machine learning methods for
classification, and their explainability explainable AI methods to evaluate the
contribution of each feature to classification performance. Our proposed method
achieves a highest classification accuracy of 99.18\% which shows that the
addition of temporal context significantly improves dance style recognition
performance.

</details>


### [126] [Geolocating Earth Imagery from ISS: Integrating Machine Learning with Astronaut Photography for Enhanced Geographic Mapping](https://arxiv.org/abs/2504.21194)
*Vedika Srivastava,Hemant Kumar Singh,Jaisal Singh*

Main category: cs.CV

TL;DR: 论文提出了一种通过机器学习算法从国际空间站（ISS）拍摄的图像中自动定位地球位置的新方法，使用了三种不同的图像处理流程（神经网络、SIFT和GPT-4），并在140多张ISS图像上进行了验证，结果显示各方法在不同方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管ISS的坐标精确，但宇航员拍摄的照片中具体地球位置经常未被识别。这项研究旨在填补这一空白，提升从太空图像中自动定位地球位置的准确性和效率。

Method: 研究采用了三种图像处理流程：基于神经网络的方法、基于SIFT的方法和GPT-4模型。每种方法针对高分辨率ISS图像进行优化，识别自然和人造地理特征。

Result: 神经网络方法在地理特征匹配上成功率较高，SIFT方法在放大图像处理上表现优异，GPT-4模型则能提供丰富的地理描述和位置预测。

Conclusion: 该研究通过提升太空图像的自动定位能力，为遥感和地球观测领域做出了贡献，有助于环境监测和全球测绘工作。

Abstract: This paper presents a novel approach to geolocating images captured from the
International Space Station (ISS) using advanced machine learning algorithms.
Despite having precise ISS coordinates, the specific Earth locations depicted
in astronaut-taken photographs often remain unidentified. Our research
addresses this gap by employing three distinct image processing pipelines: a
Neural Network based approach, a SIFT based method, and GPT-4 model. Each
pipeline is tailored to process high-resolution ISS imagery, identifying both
natural and man-made geographical features. Through extensive evaluation on a
diverse dataset of over 140 ISS images, our methods demonstrate significant
promise in automated geolocation with varied levels of success. The NN approach
showed a high success rate in accurately matching geographical features, while
the SIFT pipeline excelled in processing zoomed-in images. GPT-4 model provided
enriched geographical descriptions alongside location predictions. This
research contributes to the fields of remote sensing and Earth observation by
enhancing the accuracy and efficiency of geolocating space-based imagery,
thereby aiding environmental monitoring and global mapping efforts.

</details>


### [127] [Legilimens: Performant Video Analytics on the System-on-Chip Edge](https://arxiv.org/abs/2504.21136)
*Murali Ramanujam,Yinwei Dai,Kyle Jamieson,Ravi Netravali*

Main category: cs.CV

TL;DR: Legilimens是一个针对移动边缘设备SoC GPU的持续学习系统，通过高效数据选择和部分模型更新，显著降低重训练成本并提升准确性。


<details>
  <summary>Details</summary>
Motivation: 移动边缘设备（如无人机和车载摄像头）资源有限且计算能力较弱，但拥有丰富统一内存。现有系统依赖传统边缘服务器的闲置计算资源进行模型重训练，无法直接适用于这些设备。

Method: 利用视觉场景在模型嵌入中的重叠性，通过设备内存中的基础模型实现轻量化场景适配，并提出高效计算技术：1) 选择高价值样本；2) 部分更新基础模型；3) 在重训练和实时推理间分时共享计算资源。

Result: 在多样化工作负载中，Legilimens将重训练成本降低2.8-10倍，同时实现18-45%的准确性提升。

Conclusion: Legilimens有效解决了移动边缘设备资源受限下的持续学习问题，为高精度视频分析提供了一种轻量化解决方案。

Abstract: Continually retraining models has emerged as a primary technique to enable
high-accuracy video analytics on edge devices. Yet, existing systems employ
such adaptation by relying on the spare compute resources that traditional
(memory-constrained) edge servers afford. In contrast, mobile edge devices such
as drones and dashcams offer a fundamentally different resource profile:
weak(er) compute with abundant unified memory pools. We present Legilimens, a
continuous learning system for the mobile edge's System-on-Chip GPUs. Our
driving insight is that visually distinct scenes that require retraining
exhibit substantial overlap in model embeddings; if captured into a base model
on device memory, specializing to each new scene can become lightweight,
requiring very few samples. To practically realize this approach, Legilimens
presents new, compute-efficient techniques to (1) select high-utility data
samples for retraining specialized models, (2) update the base model without
complete retraining, and (3) time-share compute resources between retraining
and live inference for maximal accuracy. Across diverse workloads, Legilimens
lowers retraining costs by 2.8-10x compared to existing systems, resulting in
18-45% higher accuracies.

</details>


### [128] [MemeBLIP2: A novel lightweight multimodal system to detect harmful memes](https://arxiv.org/abs/2504.21226)
*Jiaqi Liu,Ran Tong,Aowei Shen,Shuzheng Li,Changlin Yang,Lisha Xu*

Main category: cs.CV

TL;DR: MemeBLIP2是一种轻量级多模态系统，能有效结合图像和文本特征来检测有害模因。


<details>
  <summary>Details</summary>
Motivation: 模因常结合视觉和简短文本传递幽默或观点，但其中可能包含有害信息如仇恨言论。

Method: 采用BLIP-2作为核心视觉语言模型，添加模块以对齐和融合图像与文本特征。

Result: 在PrideMM数据集上测试显示，MemeBLIP2能捕捉多模态中的细微线索，提升有害内容检测效果。

Conclusion: MemeBLIP2通过多模态特征融合，有效识别包含讽刺或文化特定内容的有害模因。

Abstract: Memes often merge visuals with brief text to share humor or opinions, yet
some memes contain harmful messages such as hate speech. In this paper, we
introduces MemeBLIP2, a light weight multimodal system that detects harmful
memes by combining image and text features effectively. We build on previous
studies by adding modules that align image and text representations into a
shared space and fuse them for better classification. Using BLIP-2 as the core
vision-language model, our system is evaluated on the PrideMM datasets. The
results show that MemeBLIP2 can capture subtle cues in both modalities, even in
cases with ironic or culturally specific content, thereby improving the
detection of harmful material.

</details>


### [129] [T2ID-CAS: Diffusion Model and Class Aware Sampling to Mitigate Class Imbalance in Neck Ultrasound Anatomical Landmark Detection](https://arxiv.org/abs/2504.21231)
*Manikanta Varaganti,Amulya Vankayalapati,Nour Awad,Gregory R. Dion,Laura J. Brattain*

Main category: cs.CV

TL;DR: 论文提出了一种结合文本到图像潜扩散模型和类感知采样的混合方法T2ID-CAS，用于解决颈部超声图像中类别不平衡问题，显著提升了目标检测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 颈部超声在气道管理中具有重要作用，但数据集中关键结构（如气管环和声带）的类别不平衡问题严重影响了目标检测模型的性能，因此需要一种有效的方法来改善这一问题。

Method: 提出了一种名为T2ID-CAS的混合方法，结合了文本到图像的潜扩散模型和类感知采样，生成高质量的合成样本以增强少数类的表示。

Result: 实验结果显示，使用YOLOv9进行颈部超声解剖标志检测时，T2ID-CAS的平均精确率达到88.2，显著高于基线方法的66。

Conclusion: T2ID-CAS是一种计算效率高且可扩展的解决方案，能够有效缓解AI辅助超声引导干预中的类别不平衡问题，展示了其在临床应用中的潜力。

Abstract: Neck ultrasound (US) plays a vital role in airway management by providing
non-invasive, real-time imaging that enables rapid and precise interventions.
Deep learning-based anatomical landmark detection in neck US can further
facilitate procedural efficiency. However, class imbalance within datasets,
where key structures like tracheal rings and vocal folds are underrepresented,
presents significant challenges for object detection models. To address this,
we propose T2ID-CAS, a hybrid approach that combines a text-to-image latent
diffusion model with class-aware sampling to generate high-quality synthetic
samples for underrepresented classes. This approach, rarely explored in the
ultrasound domain, improves the representation of minority classes.
Experimental results using YOLOv9 for anatomical landmark detection in neck US
demonstrated that T2ID-CAS achieved a mean Average Precision of 88.2,
significantly surpassing the baseline of 66. This highlights its potential as a
computationally efficient and scalable solution for mitigating class imbalance
in AI-assisted ultrasound-guided interventions.

</details>


### [130] [Embracing Collaboration Over Competition: Condensing Multiple Prompts for Visual In-Context Learning](https://arxiv.org/abs/2504.21263)
*Jinpeng Wang,Tianci Luo,Yaohua Zha,Yan Feng,Ruisheng Luo,Bin Chen,Tao Dai,Long Chen,Yaowei Wang,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Condenser的轻量级外部插件，通过整合多个候选提示的细粒度上下文信息，解决了视觉上下文学习（VICL）中提示选择的难题，提高了任务表现的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前视觉上下文学习（VICL）中，提示选择方法通常假设存在单一的“理想”提示，但实践中可能存在多个合适的提示。然而，单独使用时这些提示往往效果不佳，导致上下文信息丢失。

Method: 论文提出了“提示浓缩”（prompt condensation）方法，设计了Condenser插件，通过轻量级的外部模块整合多个提示的有效上下文信息，并与主干网络端到端优化。

Result: 实验表明，Condenser在多个基准任务上优于现有方法，具有更高的上下文压缩能力、对更多提示的扩展性，以及比集成方法更高的计算效率。

Conclusion: Condenser为VICL提供了一个高效且可扩展的解决方案，能够整合多个提示的上下文信息，提升任务表现。

Abstract: Visual In-Context Learning (VICL) enables adaptively solving vision tasks by
leveraging pixel demonstrations, mimicking human-like task completion through
analogy. Prompt selection is critical in VICL, but current methods assume the
existence of a single "ideal" prompt in a pool of candidates, which in practice
may not hold true. Multiple suitable prompts may exist, but individually they
often fall short, leading to difficulties in selection and the exclusion of
useful context. To address this, we propose a new perspective: prompt
condensation. Rather than relying on a single prompt, candidate prompts
collaborate to efficiently integrate informative contexts without sacrificing
resolution. We devise Condenser, a lightweight external plugin that compresses
relevant fine-grained context across multiple prompts. Optimized end-to-end
with the backbone, Condenser ensures accurate integration of contextual cues.
Experiments demonstrate Condenser outperforms state-of-the-arts across
benchmark tasks, showing superior context compression, scalability with more
prompts, and enhanced computational efficiency compared to ensemble methods,
positioning it as a highly competitive solution for VICL. Code is open-sourced
at https://github.com/gimpong/CVPR25-Condenser.

</details>


### [131] [Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Early Lung Cancer Detection](https://arxiv.org/abs/2504.21344)
*Luoting Zhuang,Seyed Mohammad Hossein Tabatabaei,Ramin Salehi-Rad,Linh M. Tran,Denise R. Aberle,Ashley E. Prosper,William Hsu*

Main category: cs.CV

TL;DR: 该研究通过整合放射科医生的语义特征，提升了肺癌预测模型的临床相关性和解释性，并在外部数据集上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型依赖手动标注、解释性差且对成像变化敏感，难以应用于临床实践。因此，本研究希望通过整合放射科医生的语义特征，构建更稳健、可解释的肺癌预测模型。

Method: 研究采用预训练的CLIP模型，通过参数高效微调方法对齐影像和语义特征，并预测一年内肺癌诊断结果。使用了包括NLST、LIDC等多个数据集进行验证。

Result: 模型在一年肺癌诊断中的AUROC为0.90，AUPRC为0.78，优于基线模型。还能预测结节边缘、质地等语义特征（AUROC 0.81-0.84），提供解释性输出。

Conclusion: 该方法不仅准确区分良恶性结节，还提供可解释的结果，帮助临床医生理解模型预测依据，并避免模型学习捷径，具有跨临床场景的泛化能力。

Abstract: Objective: A number of machine learning models have utilized semantic
features, deep features, or both to assess lung nodule malignancy. However,
their reliance on manual annotation during inference, limited interpretability,
and sensitivity to imaging variations hinder their application in real-world
clinical settings. Thus, this research aims to integrate semantic features
derived from radiologists' assessments of nodules, allowing the model to learn
clinically relevant, robust, and explainable features for predicting lung
cancer. Methods: We obtained 938 low-dose CT scans from the National Lung
Screening Trial with 1,246 nodules and semantic features. The Lung Image
Database Consortium dataset contains 1,018 CT scans, with 2,625 lesions
annotated for nodule characteristics. Three external datasets were obtained
from UCLA Health, the LUNGx Challenge, and the Duke Lung Cancer Screening. We
finetuned a pretrained Contrastive Language-Image Pretraining model with a
parameter-efficient fine-tuning approach to align imaging and semantic features
and predict the one-year lung cancer diagnosis. Results: We evaluated the
performance of the one-year diagnosis of lung cancer with AUROC and AUPRC and
compared it to three state-of-the-art models. Our model demonstrated an AUROC
of 0.90 and AUPRC of 0.78, outperforming baseline state-of-the-art models on
external datasets. Using CLIP, we also obtained predictions on semantic
features, such as nodule margin (AUROC: 0.81), nodule consistency (0.81), and
pleural attachment (0.84), that can be used to explain model predictions.
Conclusion: Our approach accurately classifies lung nodules as benign or
malignant, providing explainable outputs, aiding clinicians in comprehending
the underlying meaning of model predictions. This approach also prevents the
model from learning shortcuts and generalizes across clinical settings.

</details>


### [132] [Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing](https://arxiv.org/abs/2504.21356)
*Hong Zhang,Zhongjie Duan,Xingjun Wang,Yingda Chen,Yuze Zhao,Yu Zhang*

Main category: cs.CV

TL;DR: Nexus-Gen提出了一种统一的多模态大语言模型，通过双阶段对齐训练融合语言推理和图像生成能力，解决了现有开源模型中性能不足的问题，并开源了所有资源以促进领域发展。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有开源统一多模态大语言模型（MLLMs）与领域特定架构之间的性能差距，研究团队开发了Nexus-Gen，旨在结合语言推理和图像生成能力，提升多模态任务的综合表现。

Method: 采用双阶段对齐训练：1. 自回归LLM学习根据多模态输入预测图像嵌入；2. 视觉解码器训练从这些嵌入中重建高保真图像。此外，引入预填充自回归策略以避免连续嵌入空间中的误差积累问题。

Result: Nexus-Gen实现了对图像理解、生成和编辑任务的综合处理能力，并通过开源模型、数据集和代码推动了领域的进一步发展。

Conclusion: Nexus-Gen通过创新的双阶段对齐和预填充自回归策略，成功弥补了现有统一模型的性能差距，为多模态任务提供了高效的解决方案，并推动开放研究生态。

Abstract: Unified multimodal large language models (MLLMs) aim to integrate multimodal
understanding and generation abilities through a single framework. Despite
their versatility, existing open-source unified models exhibit performance gaps
against domain-specific architectures. To bridge this gap, we present
Nexus-Gen, a unified model that synergizes the language reasoning capabilities
of LLMs with the image synthesis power of diffusion models. To align the
embedding space of the LLM and diffusion model, we conduct a dual-phase
alignment training process. (1) The autoregressive LLM learns to predict image
embeddings conditioned on multimodal inputs, while (2) the vision decoder is
trained to reconstruct high-fidelity images from these embeddings. During
training the LLM, we identified a critical discrepancy between the
autoregressive paradigm's training and inference phases, where error
accumulation in continuous embedding space severely degrades generation
quality. To avoid this issue, we introduce a prefilled autoregression strategy
that prefills input sequence with position-embedded special tokens instead of
continuous embeddings. Through dual-phase training, Nexus-Gen has developed the
integrated capability to comprehensively address the image understanding,
generation and editing tasks. All models, datasets, and codes are published at
https://github.com/modelscope/Nexus-Gen.git to facilitate further advancements
across the field.

</details>


### [133] [Towards Improved Cervical Cancer Screening: Vision Transformer-Based Classification and Interpretability](https://arxiv.org/abs/2504.21340)
*Khoa Tuan Nguyen,Ho-min Park,Gaeun Oh,Joris Vankerschaver,Wesley De Neve*

Main category: cs.CV

TL;DR: 本文提出了一种基于EVA-02变压器模型的宫颈细胞图像分类新方法，通过四步流程实现了优于基线模型的性能，并提供了模型决策的可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 旨在提高宫颈癌筛查中细胞图像分类的准确性和可解释性。

Method: 采用四步流程：微调EVA-02模型、特征提取、通过多种机器学习模型选择重要特征，以及训练带可选损失加权的新人工神经网络。

Result: 最佳模型的F1-score达到0.85227，优于基线EVA-02模型（0.84878），并通过Kernel SHAP分析提供了关键特征的解释。

Conclusion: 该方法在宫颈细胞图像分类中表现出色且兼具可解释性，代码已开源。

Abstract: We propose a novel approach to cervical cell image classification for
cervical cancer screening using the EVA-02 transformer model. We developed a
four-step pipeline: fine-tuning EVA-02, feature extraction, selecting important
features through multiple machine learning models, and training a new
artificial neural network with optional loss weighting for improved
generalization. With this design, our best model achieved an F1-score of
0.85227, outperforming the baseline EVA-02 model (0.84878). We also utilized
Kernel SHAP analysis and identified key features correlating with cell
morphology and staining characteristics, providing interpretable insights into
the decision-making process of the fine-tuned model. Our code is available at
https://github.com/Khoa-NT/isbi2025_ps3c.

</details>


### [134] [Revisiting Diffusion Autoencoder Training for Image Reconstruction Quality](https://arxiv.org/abs/2504.21368)
*Pramook Khungurn,Sukit Seripanitkarn,Phonphrm Thawatdamrongkit,Supasorn Suwajanakorn*

Main category: cs.CV

TL;DR: 论文提出了一种改进扩散自编码器（DAE）的训练方法，通过分两个阶段训练模型，第一阶段强制编码器和解码器在高噪声水平下学习结构信息，第二阶段在低噪声区域训练以优化细节，从而生成更高质量的图像。


<details>
  <summary>Details</summary>
Motivation: 传统的扩散自编码器使用线性噪声调度，大部分采样步骤集中在高噪声水平，导致生成的图像模糊且细节不足。作者认为，潜在编码已包含结构信息，因此可以通过调整噪声调度来优化细节生成。

Method: 方法分两阶段：1）在高噪声水平下训练DAE作为普通自编码器，强制学习结构信息；2）采用低噪声调度，集中优化细节生成。

Result: 改进后的方法生成的图像在高层结构和低层细节上均表现出色，同时保留了潜在编码的有用特性。

Conclusion: 通过调整噪声调度并分阶段训练，DAE能够生成更高质量的图像，证明了潜在编码在结构信息中的作用及噪声调度优化的重要性。

Abstract: Diffusion autoencoders (DAEs) are typically formulated as a noise prediction
model and trained with a linear-$\beta$ noise schedule that spends much of its
sampling steps at high noise levels. Because high noise levels are associated
with recovering large-scale image structures and low noise levels with
recovering details, this configuration can result in low-quality and blurry
images. However, it should be possible to improve details while spending fewer
steps recovering structures because the latent code should already contain
structural information. Based on this insight, we propose a new DAE training
method that improves the quality of reconstructed images. We divide training
into two phases. In the first phase, the DAE is trained as a vanilla
autoencoder by always setting the noise level to the highest, forcing the
encoder and decoder to populate the latent code with structural information. In
the second phase, we incorporate a noise schedule that spends more time in the
low-noise region, allowing the DAE to learn how to perfect the details. Our
method results in images that have accurate high-level structures and low-level
details while still preserving useful properties of the latent codes.

</details>


### [135] [ClassWise-CRF: Category-Specific Fusion for Enhanced Semantic Segmentation of Remote Sensing Imagery](https://arxiv.org/abs/2504.21491)
*Qinfeng Zhu,Yunxi Jiang,Lei Fan*

Main category: cs.CV

TL;DR: ClassWise-CRF是一种结果级类别特定融合架构，通过两阶段（专家网络选择和自适应加权融合）优化语义分割性能，利用CRF进一步提升空间一致性和边界精度。实验表明其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决语义分割中不同网络在不同类别表现不均的问题，提出了类别特定融合架构，旨在动态优化网络权重，提升整体性能。

Method: 采用两阶段方法：1）贪心算法选择特定类别表现好的专家网络；2）基于CRF的动态加权融合策略，结合验证集的分割指标（如IoU）进行优化。

Result: 在LoveDA和Vaihingen数据集上，mIoU显著提升（最高1.00%和0.91%），验证了方法的有效性和泛化能力。

Conclusion: ClassWise-CRF通过类别特定优化和CRF后处理，显著提升遥感图像语义分割性能，具有实用性和推广价值。

Abstract: We propose a result-level category-specific fusion architecture called
ClassWise-CRF. This architecture employs a two-stage process: first, it selects
expert networks that perform well in specific categories from a pool of
candidate networks using a greedy algorithm; second, it integrates the
segmentation predictions of these selected networks by adaptively weighting
their contributions based on their segmentation performance in each category.
Inspired by Conditional Random Field (CRF), the ClassWise-CRF architecture
treats the segmentation predictions from multiple networks as confidence vector
fields. It leverages segmentation metrics (such as Intersection over Union)
from the validation set as priors and employs an exponential weighting strategy
to fuse the category-specific confidence scores predicted by each network. This
fusion method dynamically adjusts the weights of each network for different
categories, achieving category-specific optimization. Building on this, the
architecture further optimizes the fused results using unary and pairwise
potentials in CRF to ensure spatial consistency and boundary accuracy. To
validate the effectiveness of ClassWise-CRF, we conducted experiments on two
remote sensing datasets, LoveDA and Vaihingen, using eight classic and advanced
semantic segmentation networks. The results show that the ClassWise-CRF
architecture significantly improves segmentation performance: on the LoveDA
dataset, the mean Intersection over Union (mIoU) metric increased by 1.00% on
the validation set and by 0.68% on the test set; on the Vaihingen dataset, the
mIoU improved by 0.87% on the validation set and by 0.91% on the test set.
These results fully demonstrate the effectiveness and generality of the
ClassWise-CRF architecture in semantic segmentation of remote sensing images.
The full code is available at https://github.com/zhuqinfeng1999/ClassWise-CRF.

</details>


### [136] [Rethinking Visual Layer Selection in Multimodal LLMs](https://arxiv.org/abs/2504.21447)
*Haoran Chen,Junyan Lin,Xinhao Chen,Yue Fan,Xin Jin,Hui Su,Jianfeng Dong,Jinlan Fu,Xiaoyu Shen*

Main category: cs.CV

TL;DR: 该论文提出了一种基于层间表征相似性的方法，将CLIP-ViT的层分为浅层、中层和深层，并分析了它们对多模态大语言模型性能的影响。实验发现不同任务需要不同层的特征，并提出一种轻量级跨层融合方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLMs）通常基于经验选择视觉特征层，缺乏系统性分析。作者希望通过分析CLIP-ViT不同层的表征行为，为视觉层选择提供理论基础。

Method: 提出了一种层间表征相似性方法，将CLIP-ViT层分为浅层、中层和深层，并构建了从1.4B到7B参数的LLaVA风格模型，在10个数据集和4种任务上进行实验。

Result: 实验结果表明：1）深层对OCR任务至关重要；2）浅层和中层在计数、定位等推理任务中表现更优；3）跨层轻量级融合在9/10数据集中优于单层或专用融合方法。

Conclusion: 这项研究首次系统性分析了MLLMs中视觉层选择问题，为未来视觉表征学习的深入研究奠定了基础。

Abstract: Multimodal large language models (MLLMs) have achieved impressive performance
across a wide range of tasks, typically using CLIP-ViT as their visual encoder
due to its strong text-image alignment capabilities. While prior studies
suggest that different CLIP-ViT layers capture different types of information,
with shallower layers focusing on fine visual details and deeper layers
aligning more closely with textual semantics, most MLLMs still select visual
features based on empirical heuristics rather than systematic analysis. In this
work, we propose a Layer-wise Representation Similarity approach to group
CLIP-ViT layers with similar behaviors into {shallow, middle, and deep}
categories and assess their impact on MLLM performance. Building on this
foundation, we revisit the visual layer selection problem in MLLMs at scale,
training LLaVA-style models ranging from 1.4B to 7B parameters. Through
extensive experiments across 10 datasets and 4 tasks, we find that: (1) deep
layers are essential for OCR tasks; (2) shallow and middle layers substantially
outperform deep layers on reasoning tasks involving counting, positioning, and
object localization; (3) a lightweight fusion of features across shallow,
middle, and deep layers consistently outperforms specialized fusion baselines
and single-layer selections, achieving gains on 9 out of 10 datasets. Our work
offers the first principled study of visual layer selection in MLLMs, laying
the groundwork for deeper investigations into visual representation learning
for MLLMs.

</details>


### [137] [Anomaly-Driven Approach for Enhanced Prostate Cancer Segmentation](https://arxiv.org/abs/2504.21789)
*Alessia Hu,Regina Beets-Tan,Lishan Cai,Eduardo Pooch*

Main category: cs.CV

TL;DR: 该研究通过将异常检测整合到分割框架中，提出了一种改进的深度学习方法（adU-Net），用于自动化识别临床显著的前列腺癌，并展示了其在性能上的提升。


<details>
  <summary>Details</summary>
Motivation: MRI在前列腺癌识别中至关重要，但自动化方法面临数据不平衡、肿瘤大小不一和标注数据不足等挑战。

Method: 研究引入adU-Net，利用从双参数MRI序列衍生的异常图（通过Fixed-Point GAN生成）来指导分割模型，并通过AUROC和AP均值评估性能。

Result: 在外部测试集上，adU-Net的平均得分为0.618，优于基线nnU-Net模型（0.605），ADC异常图表现最佳。

Conclusion: 整合异常检测到分割框架中提升了模型泛化能力和性能，为自动化前列腺癌识别提供了新方向。

Abstract: Magnetic Resonance Imaging (MRI) plays an important role in identifying
clinically significant prostate cancer (csPCa), yet automated methods face
challenges such as data imbalance, variable tumor sizes, and a lack of
annotated data. This study introduces Anomaly-Driven U-Net (adU-Net), which
incorporates anomaly maps derived from biparametric MRI sequences into a deep
learning-based segmentation framework to improve csPCa identification. We
conduct a comparative analysis of anomaly detection methods and evaluate the
integration of anomaly maps into the segmentation pipeline. Anomaly maps,
generated using Fixed-Point GAN reconstruction, highlight deviations from
normal prostate tissue, guiding the segmentation model to potential cancerous
regions. We compare the performance by using the average score, computed as the
mean of the AUROC and Average Precision (AP). On the external test set, adU-Net
achieves the best average score of 0.618, outperforming the baseline nnU-Net
model (0.605). The results demonstrate that incorporating anomaly detection
into segmentation improves generalization and performance, particularly with
ADC-based anomaly maps, offering a promising direction for automated csPCa
identification.

</details>


### [138] [GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal Diffusion Transformers](https://arxiv.org/abs/2504.21476)
*Xinyu Li,Qi Yao,Yuanda Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为GarmentDiffusion的新型生成模型，能够从多模态输入（文本、图像和不完整的缝纫图案）高效生成厘米级精度的矢量化3D缝纫图案，比现有方法快100倍。


<details>
  <summary>Details</summary>
Motivation: 现有缝纫图案生成方法因依赖单一输入模态或生成效率低下而受限，为解决这一问题，作者提出了GarmentDiffusion。

Method: 通过将3D缝纫图案参数编码为紧凑的边缘令牌表示，并利用扩散变换器同时沿时间轴去噪所有边缘令牌，实现高效的图案生成。

Result: 在DressCodeData和GarmentCodeData数据集上达到新的最先进结果，生成速度比SewingGPT快100倍。

Conclusion: GarmentDiffusion在缝纫图案生成任务中表现出色，通过多模态输入和高效生成方法显著提升了性能。

Abstract: Garment sewing patterns are fundamental design elements that bridge the gap
between design concepts and practical manufacturing. The generative modeling of
sewing patterns is crucial for creating diversified garments. However, existing
approaches are limited either by reliance on a single input modality or by
suboptimal generation efficiency. In this work, we present
\textbf{\textit{GarmentDiffusion}}, a new generative model capable of producing
centimeter-precise, vectorized 3D sewing patterns from multimodal inputs (text,
image, and incomplete sewing pattern). Our method efficiently encodes 3D sewing
pattern parameters into compact edge token representations, achieving a
sequence length that is $\textbf{10}\times$ shorter than that of the
autoregressive SewingGPT in DressCode. By employing a diffusion transformer, we
simultaneously denoise all edge tokens along the temporal axis, while
maintaining a constant number of denoising steps regardless of dataset-specific
edge and panel statistics. With all combination of designs of our model, the
sewing pattern generation speed is accelerated by $\textbf{100}\times$ compared
to SewingGPT. We achieve new state-of-the-art results on DressCodeData, as well
as on the largest sewing pattern dataset, namely GarmentCodeData. The project
website is available at https://shenfu-research.github.io/Garment-Diffusion/.

</details>


### [139] [eNCApsulate: NCA for Precision Diagnosis on Capsule Endoscopes](https://arxiv.org/abs/2504.21562)
*Henry John Krumb,Anirban Mukhopadhyay*

Main category: cs.CV

TL;DR: 该论文提出了一种基于神经细胞自动机（NCA）的轻量级方法，用于无线胶囊内窥镜的出血分割和深度估计，并通过蒸馏大模型和硬件优化实现了在微型设备上的高效运行。


<details>
  <summary>Details</summary>
Motivation: 无线胶囊内窥镜生成的海量视频数据需要长时间检查，且胶囊定位是挑战。传统深度学习方法计算量大，难以在胶囊内直接运行。

Method: 使用NCA架构进行出血分割和深度估计，通过蒸馏大模型的输出作为伪真值训练NCA，并将其部署到ESP32微型控制器上。

Result: NCA在精度（Dice）上优于其他便携分割模型，参数存储需求减少100倍以上，ESP32-S3上的推理速度提升3倍以上。

Conclusion: 通过算法优化和蒸馏，首次实现了胶囊内可靠的出血分割和深度估计，为结合视觉里程计的精准胶囊定位奠定了基础。

Abstract: Wireless Capsule Endoscopy is a non-invasive imaging method for the entire
gastrointestinal tract, and is a pain-free alternative to traditional
endoscopy. It generates extensive video data that requires significant review
time, and localizing the capsule after ingestion is a challenge. Techniques
like bleeding detection and depth estimation can help with localization of
pathologies, but deep learning models are typically too large to run directly
on the capsule. Neural Cellular Automata (NCA) for bleeding segmentation and
depth estimation are trained on capsule endoscopic images. For monocular depth
estimation, we distill a large foundation model into the lean NCA architecture,
by treating the outputs of the foundation model as pseudo ground truth. We then
port the trained NCA to the ESP32 microcontroller, enabling efficient image
processing on hardware as small as a camera capsule. NCA are more accurate
(Dice) than other portable segmentation models, while requiring more than 100x
fewer parameters stored in memory than other small-scale models. The visual
results of NCA depth estimation look convincing, and in some cases beat the
realism and detail of the pseudo ground truth. Runtime optimizations on the
ESP32-S3 accelerate the average inference speed significantly, by more than
factor 3. With several algorithmic adjustments and distillation, it is possible
to eNCApsulate NCA models into microcontrollers that fit into wireless capsule
endoscopes. This is the first work that enables reliable bleeding segmentation
and depth estimation on a miniaturized device, paving the way for precise
diagnosis combined with visual odometry as a means of precise localization of
the capsule -- on the capsule.

</details>


### [140] [Enhancing Self-Supervised Fine-Grained Video Object Tracking with Dynamic Memory Prediction](https://arxiv.org/abs/2504.21692)
*Zihan Zhou,Changrui Dai,Aibo Song,Xiaolin Fang*

Main category: cs.CV

TL;DR: 提出了一种动态内存预测（DMP）框架，通过多参考帧直接增强帧重建，核心是动态选择帧的内存引擎和双向目标预测网络，实验显示其在目标分割和关键点跟踪任务中优于现有自监督技术。


<details>
  <summary>Details</summary>
Motivation: 现有帧重建方法效率虽高，但忽视了多参考帧的直接参与价值，尤其在复杂场景（如遮挡或快速运动）中。

Method: 引入动态内存预测框架，包括动态选择参考帧的内存引擎和双向目标预测网络。

Result: 在目标分割和关键点跟踪任务上优于当前最先进的自我监督技术。

Conclusion: DMP框架通过创新性地利用多参考帧，有效提升了复杂场景下的帧重建和跟踪准确性。

Abstract: Successful video analysis relies on accurate recognition of pixels across
frames, and frame reconstruction methods based on video correspondence learning
are popular due to their efficiency. Existing frame reconstruction methods,
while efficient, neglect the value of direct involvement of multiple reference
frames for reconstruction and decision-making aspects, especially in complex
situations such as occlusion or fast movement. In this paper, we introduce a
Dynamic Memory Prediction (DMP) framework that innovatively utilizes multiple
reference frames to concisely and directly enhance frame reconstruction. Its
core component is a Reference Frame Memory Engine that dynamically selects
frames based on object pixel features to improve tracking accuracy. In
addition, a Bidirectional Target Prediction Network is built to utilize
multiple reference frames to improve the robustness of the model. Through
experiments, our algorithm outperforms the state-of-the-art self-supervised
techniques on two fine-grained video object tracking tasks: object segmentation
and keypoint tracking.

</details>


### [141] [Vision Transformers in Precision Agriculture: A Comprehensive Survey](https://arxiv.org/abs/2504.21706)
*Saber Mehdipour,Seyed Abolghasem Mirroshandel,Seyed Amirhossein Tabatabaei*

Main category: cs.CV

TL;DR: 该论文是一篇关于视觉变换器（ViTs）在精准农业中应用的综述，包括疾病检测、分类、分割等任务，并对比了ViTs与传统CNN方法的优缺点。


<details>
  <summary>Details</summary>
Motivation: 传统植物病害检测方法（如人工检查或传统机器学习）在扩展性和准确性上有局限，而ViTs因其在视觉任务中的长程依赖处理能力和扩展性优势，成为有前景的替代方案。

Method: 综述了ViTs的基础架构及其从NLP到计算机视觉的转换，讨论了传统CNN的归纳偏差问题以及ViTs如何缓解这些问题，并汇总了相关方法论、数据集和性能指标。

Result: 通过比较CNN和ViTs及混合模型，本文总结了ViTs在农业任务中的优势和技术挑战（如数据需求、计算成本和模型可解释性），并提出了解决方案。

Conclusion: ViTs有望革新精准农业，但仍需解决技术和实践挑战。本文为研究人员和实践者提供了ViTs应用前景的全面指导。

Abstract: Detecting plant diseases is a crucial aspect of modern agriculture - it plays
a key role in maintaining crop health and increasing overall yield. Traditional
approaches, though still valuable, often rely on manual inspection or
conventional machine learning techniques, both of which face limitations in
scalability and accuracy. Recently, Vision Transformers (ViTs) have emerged as
a promising alternative, offering benefits such as improved handling of
long-range dependencies and better scalability for visual tasks. This survey
explores the application of ViTs in precision agriculture, covering tasks from
classification to detection and segmentation. We begin by introducing the
foundational architecture of ViTs and discuss their transition from Natural
Language Processing (NLP) to computer vision. The discussion includes the
concept of inductive bias in traditional models like Convolutional Neural
Networks (CNNs), and how ViTs mitigate these biases. We provide a comprehensive
review of recent literature, focusing on key methodologies, datasets, and
performance metrics. The survey also includes a comparative analysis of CNNs
and ViTs, with a look at hybrid models and performance enhancements. Technical
challenges - such as data requirements, computational demands, and model
interpretability - are addressed alongside potential solutions. Finally, we
outline potential research directions and technological advancements that could
further support the integration of ViTs in real-world agricultural settings.
Our goal with this study is to offer practitioners and researchers a deeper
understanding of how ViTs are poised to transform smart and precision
agriculture.

</details>


### [142] [Early Exit and Multi Stage Knowledge Distillation in VLMs for Video Summarization](https://arxiv.org/abs/2504.21831)
*Anas Anwarul Haq Khan,Utkarsh Verma,Prateek Chanda,Ganesh Ramakrishnan*

Main category: cs.CV

TL;DR: DEEVISum是一种轻量高效的视觉语言模型，用于视频摘要，结合多模态提示和多阶段知识蒸馏（MSKD）及早期退出（EE）技术，平衡性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在视频摘要任务中计算成本高，DEEVISum旨在通过轻量化设计和高效率技术解决这一问题。

Method: 采用MSKD和EE技术，结合文本和音频信号的多模态提示，优化模型性能与推理速度。

Result: 在TVSum数据集上，最佳模型F1得分为61.1，推理时间减少21%，性能接近更大模型。

Conclusion: DEEVISum在保持高效的同时实现了接近高端模型的性能，推动了轻量级视觉语言模型的研究。

Abstract: We introduce DEEVISum (Distilled Early Exit Vision language model for
Summarization), a lightweight, efficient, and scalable vision language model
designed for segment wise video summarization. Leveraging multi modal prompts
that combine textual and audio derived signals, DEEVISum incorporates Multi
Stage Knowledge Distillation (MSKD) and Early Exit (EE) to strike a balance
between performance and efficiency. MSKD offers a 1.33% absolute F1 improvement
over baseline distillation (0.5%), while EE reduces inference time by
approximately 21% with a 1.3 point drop in F1. Evaluated on the TVSum dataset,
our best model PaLI Gemma2 3B + MSKD achieves an F1 score of 61.1, competing
the performance of significantly larger models, all while maintaining a lower
computational footprint. We publicly release our code and processed dataset to
support further research.

</details>


### [143] [Active Light Modulation to Counter Manipulation of Speech Visual Content](https://arxiv.org/abs/2504.21846)
*Hadleigh Schwartz,Xiaofeng Yan,Charles J. Carver,Xia Zhou*

Main category: cs.CV

TL;DR: Spotlight是一种低开销、非侵入式的系统，通过不可见光动态签名保护直播演讲视频免遭身份和面部动作的篡改，检测篡改视频的AUC≥0.99，真阳性率100%。


<details>
  <summary>Details</summary>
Motivation: 高影响力演讲视频易受伪造攻击，现有数字域检测方法有限，需在物理层面嵌入独特签名以确保视频完整性。

Method: 利用局部敏感哈希生成紧凑特征，通过光调制将加密签名嵌入视频，验证时提取签名并与内容匹配。

Result: 实验显示AUC≥0.99，真阳性率100%，对录制条件、后处理及对抗攻击高度鲁棒。

Conclusion: Spotlight以物理签名有效防伪，适用于实时视频保护，性能优越且抗干扰能力强。

Abstract: High-profile speech videos are prime targets for falsification, owing to
their accessibility and influence. This work proposes Spotlight, a low-overhead
and unobtrusive system for protecting live speech videos from visual
falsification of speaker identity and lip and facial motion. Unlike predominant
falsification detection methods operating in the digital domain, Spotlight
creates dynamic physical signatures at the event site and embeds them into all
video recordings via imperceptible modulated light. These physical signatures
encode semantically-meaningful features unique to the speech event, including
the speaker's identity and facial motion, and are cryptographically-secured to
prevent spoofing. The signatures can be extracted from any video downstream and
validated against the portrayed speech content to check its integrity. Key
elements of Spotlight include (1) a framework for generating extremely compact
(i.e., 150-bit), pose-invariant speech video features, based on
locality-sensitive hashing; and (2) an optical modulation scheme that embeds
>200 bps into video while remaining imperceptible both in video and live.
Prototype experiments on extensive video datasets show Spotlight achieves AUCs
$\geq$ 0.99 and an overall true positive rate of 100% in detecting falsified
videos. Further, Spotlight is highly robust across recording conditions, video
post-processing techniques, and white-box adversarial attacks on its video
feature extraction methodologies.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [144] [Low-rank computation of the posterior mean in Multi-Output Gaussian Processes](https://arxiv.org/abs/2504.21527)
*Sebastian Esche,Martin Stoll*

Main category: math.NA

TL;DR: 本文提出了低秩方法来高效计算多输出高斯过程的后验均值，利用空间和时间的可分离性，通过Kronecker乘积分解协方差矩阵，并结合LRPCG和KPIK方法求解Stein方程。在实际街网数据上验证方法，并提出了加权平均协方差矩阵以提升效率。


<details>
  <summary>Details</summary>
Motivation: 多输出高斯过程（MOGP）在机器学习中应用广泛，但计算后验均值的复杂度高，尤其是在大规模数据下。本文旨在通过低秩方法和结构化协方差函数提高计算效率。

Method: 假设时空可分离性，将协方差矩阵分解为Kronecker乘积，结合噪声项后通过LRPCG和KPIK方法求解Stein方程，并测试了街网数据和加权平均协方差矩阵。

Result: 提出的方法在实际街网数据上表现良好，加权平均协方差矩阵在特定假设下能提升收敛效率。

Conclusion: 低秩方法和结构化协方差函数显著提升了MOGP后验均值的计算效率，为大规模数据处理提供了可行方案。

Abstract: Gaussian processes (GP) are a versatile tool in machine learning and
computational science. We here consider the case of multi-output Gaussian
processes (MOGP) and present low-rank approaches for efficiently computing the
posterior mean of a MOGP. Starting from low-rank spatio-temporal data we
consider a structured covariance function, assuming separability across space
and time. This separability, in turn, gives a decomposition of the covariance
matrix into a Kronecker product of individual covariance matrices.
Incorporating the typical noise term to the model then requires the solution of
a large-scale Stein equation for computing the posterior mean. For this, we
propose efficient low-rank methods based on a combination of a LRPCG method
with the Sylvester equation solver KPIK adjusted for solving Stein equations.
We test the developed method on real world street network graphs by using graph
filters as covariance matrices. Moreover, we propose a degree-weighted average
covariance matrix, which can be employed under specific assumptions to achieve
more efficient convergence.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [145] [Passive Measurement of Autonomic Arousal in Real-World Settings](https://arxiv.org/abs/2504.21242)
*Samy Abdel-Ghaffar,Isaac Galatzer-Levy,Conor Heneghan,Xin Liu,Sarah Kernasovskiy,Brennan Garrett,Andrew Barakat,Daniel McDuff*

Main category: cs.HC

TL;DR: 摘要介绍了一种名为Fitbit Body Response Algorithm的新方法，用于通过手腕传感器远程持续测量自主神经系统（ANS）激活，验证了其在现实环境中的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于ANS在压力下的激活对健康有负面影响，而现有方法多为实验室环境设计，缺乏现实环境验证，因此需要一种能在真实场景中有效测量ANS的方法。

Method: 研究通过两项实验验证算法：Trier Social Stress Test（45人）和生态瞬时评估（EMA，87人），结合多种传感器信号预测压力感知。

Result: 使用所有传感器信号时，模型预测压力感知的准确率达0.85，优于仅使用部分信号的模型。

Conclusion: 算法在现实环境中表现良好，解决了传统实验室无法覆盖的传感挑战，为远程ANS监测提供了可行方案。

Abstract: The autonomic nervous system (ANS) is activated during stress, which can have
negative effects on cardiovascular health, sleep, the immune system, and mental
health. While there are ways to quantify ANS activity in laboratories, there is
a paucity of methods that have been validated in real-world contexts. We
present the Fitbit Body Response Algorithm, an approach to continuous remote
measurement of ANS activation through widely available remote wrist-based
sensors. The design was validated via two experiments, a Trier Social Stress
Test (n = 45) and ecological momentary assessments (EMA) of perceived stress
(n=87), providing both controlled and ecologically valid test data. Model
performance predicting perceived stress when using all available sensor
modalities was consistent with expectations (accuracy=0.85) and outperformed
models with access to only a subset of the signals. We discuss and address
challenges to sensing that arise in real world settings that do not present in
conventional lab environments.

</details>


### [146] [Adaptive 3D UI Placement in Mixed Reality Using Deep Reinforcement Learning](https://arxiv.org/abs/2504.21731)
*Feiyu Lu,Mengyu Chen,Hsiang Hsu,Pranav Deshpande,Cheng Yao Wang,Blair MacIntyre*

Main category: cs.HC

TL;DR: 论文探讨了如何利用强化学习（RL）在混合现实（MR）中动态优化3D内容放置，以适应用户姿态和环境变化，初步结果显示了RL的潜力，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 混合现实中虚拟内容的动态放置一直是一个挑战性问题，传统优化方法存在局限，因此研究者探索如何利用强化学习实现更智能的内容布局。

Method: 研究采用强化学习方法，通过用户姿态和周围环境的实时感知，动态调整3D内容的放置位置，以最大化用户奖励。

Result: 初步实验结果证明，强化学习能够有效优化MR中的内容放置，提升用户体验。

Conclusion: 强化学习在MR内容放置中展现出潜力，未来可进一步研究个性化与优化的UI布局。

Abstract: Mixed Reality (MR) could assist users' tasks by continuously integrating
virtual content with their view of the physical environment. However, where and
how to place these content to best support the users has been a challenging
problem due to the dynamic nature of MR experiences. In contrast to prior work
that investigates optimization-based methods, we are exploring how
reinforcement learning (RL) could assist with continuous 3D content placement
that is aware of users' poses and their surrounding environments. Through an
initial exploration and preliminary evaluation, our results demonstrate the
potential of RL to position content that maximizes the reward for users on the
go. We further identify future directions for research that could harness the
power of RL for personalized and optimized UI and content placement in MR.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [147] [Light Weight CNN for classification of Brain Tumors from MRI Images](https://arxiv.org/abs/2504.21188)
*Natnael Alemayehu*

Main category: eess.IV

TL;DR: 该论文提出了一种基于卷积神经网络（CNN）的轻量级深度学习模型，用于对脑部肿瘤MRI图像进行多分类（四类），通过预处理和超参数优化，模型准确率达98.78%，具备临床辅助诊断潜力。


<details>
  <summary>Details</summary>
Motivation: 目标是构建一个高精度、低复杂度的深度学习模型，以自动分类脑肿瘤类型，辅助临床早期诊断。

Method: 采用CNN架构，结合图像预处理（归一化、数据增强、裁剪去背景），并使用Keras Tuner进行超参数优化，通过5折交叉验证评估模型。

Result: 模型在四分类任务中达到98.78%的准确率，验证了其有效性。

Conclusion: 所提方法为脑肿瘤早期诊断提供了低复杂度且高效的解决方案，具有临床应用前景。

Abstract: This study presents a convolutional neural network (CNN)-based approach for
the multi-class classification of brain tumors using magnetic resonance imaging
(MRI) scans. We utilize a publicly available dataset containing MRI images
categorized into four classes: glioma, meningioma, pituitary tumor, and no
tumor. Our primary objective is to build a light weight deep learning model
that can automatically classify brain tumor types with high accuracy. To
achieve this goal, we incorporate image preprocessing steps, including
normalization, data augmentation, and a cropping technique designed to reduce
background noise and emphasize relevant regions. The CNN architecture is
optimized through hyperparameter tuning using Keras Tuner, enabling systematic
exploration of network parameters. To ensure reliable evaluation, we apply
5-fold cross-validation, where each hyperparameter configuration is evaluated
across multiple data splits to mitigate overfitting. Experimental results
demonstrate that the proposed model achieves a classification accuracy of
98.78%, indicating its potential as a diagnostic aid in clinical settings. The
proposed method offers a low-complexity yet effective solution for assisting in
early brain tumor diagnosis.

</details>


### [148] [Gradient Attention Map Based Verification of Deep Convolutional Neural Networks with Application to X-ray Image Datasets](https://arxiv.org/abs/2504.21227)
*Omid Halimi Milani,Amanda Nikho,Lauren Mills,Marouane Tliba,Ahmet Enis Cetin,Mohammed H. Elnagar*

Main category: eess.IV

TL;DR: 论文提出了一种综合验证框架，用于评估深度学习模型在医学影像（如正畸和骨骼成熟度评估）中的适用性，通过梯度注意力图和早期卷积特征图分析，以及引入垃圾类别来拒绝非分布输入，以提高模型的可靠性和安全性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学影像中有巨大潜力，但模型在面对与训练集不同的数据时可能产生不可靠的预测，从而影响患者护理。因此，需要一种方法来验证模型的适用性。

Method: 1. 使用梯度注意力图（GAM）分析注意力模式（基于Grad-CAM），并通过多种相似性度量（如IoU、Dice、SSIM等）进行比较。2. 扩展到早期卷积特征图以捕捉结构错位。3. 在分类模型中引入垃圾类别以明确拒绝非分布输入。

Result: 实验结果表明，这些方法的组合能有效识别不合适的模型和输入，从而提升深度学习在医学影像中部署的安全性和可靠性。

Conclusion: 提出的综合验证框架为深度学习模型在医学影像中的可靠性提供了多层面的保障，有助于临床实践中的安全应用。

Abstract: Deep learning models have great potential in medical imaging, including
orthodontics and skeletal maturity assessment. However, applying a model to
data different from its training set can lead to unreliable predictions that
may impact patient care. To address this, we propose a comprehensive
verification framework that evaluates model suitability through multiple
complementary strategies. First, we introduce a Gradient Attention Map
(GAM)-based approach that analyzes attention patterns using Grad-CAM and
compares them via similarity metrics such as IoU, Dice Similarity, SSIM, Cosine
Similarity, Pearson Correlation, KL Divergence, and Wasserstein Distance.
Second, we extend verification to early convolutional feature maps, capturing
structural mis-alignments missed by attention alone. Finally, we incorporate an
additional garbage class into the classification model to explicitly reject
out-of-distribution inputs. Experimental results demonstrate that these
combined methods effectively identify unsuitable models and inputs, promoting
safer and more reliable deployment of deep learning in medical imaging.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [149] [Scalable Multi-Task Learning for Particle Collision Event Reconstruction with Heterogeneous Graph Neural Networks](https://arxiv.org/abs/2504.21844)
*William Sutcliffe,Marta Calvi,Simone Capelli,Jonas Eschle,Julián García Pardiñas,Abhijit Mathad,Azusa Uzuki,Nicola Serra*

Main category: physics.data-an

TL;DR: 论文提出了一种基于异构图神经网络（HGNN）的新方法，用于解决大型强子对撞机数据处理中的挑战，包括粒子顶点关联和图剪枝，显著提高了美强子重建性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型强子对撞机（LHC）的亮度提升，粒子碰撞事件的重建与分析面临更大挑战，如更高的背景噪声和粒子顶点误关联，因此需要开发更全面且可扩展的重建方法。

Method: 采用了一种新型异构图神经网络（HGNN）架构，通过独特的粒子碰撞关系表示和集成的图剪枝层来提升可扩展性，训练时使用了多任务范式，并在模拟LHCb实验的环境中进行。

Result: 实验表明，HGNN显著提升了美强子重建性能，同时在一个框架内完成了粒子顶点关联和图剪枝，且推理时间随事件复杂度的增加而优化。

Conclusion: 提出的HGNN框架有效解决了高亮度环境下粒子碰撞数据处理的挑战，为未来高能物理实验提供了可扩展的重建方法。

Abstract: The growing luminosity frontier at the Large Hadron Collider is challenging
the reconstruction and analysis of particle collision events. Increased
particle multiplicities are straining latency and storage requirements at the
data acquisition stage, while new complications are emerging, including higher
background levels and more frequent particle vertex misassociations. This in
turn necessitates the development of more holistic and scalable reconstruction
methods that take advantage of recent advances in machine learning. We propose
a novel Heterogeneous Graph Neural Network (HGNN) architecture featuring unique
representations for diverse particle collision relationships and integrated
graph pruning layers for scalability. Trained with a multi-task paradigm in an
environment mimicking the LHCb experiment, this HGNN significantly improves
beauty hadron reconstruction performance. Notably, it concurrently performs
particle vertex association and graph pruning within a single framework. We
quantify reconstruction and pruning performance, demonstrate enhanced inference
time scaling with event complexity, and mitigate potential performance loss
using a weighted message passing scheme.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [150] [Automated Generation of Precedence Graphs in Digital Value Chains for Automotive Production](https://arxiv.org/abs/2504.19835)
*Cornelius Hake,Christian Friedrich*

Main category: cs.RO

TL;DR: 提出了一种新型优先级图设计与自动化调度算法，优化汽车制造中的数字价值链，显著减少生产站点数量和空闲时间，提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 研究旨在优化汽车制造中电子控制单元的数字价值链流程，解决传统方法效率低、成本高的问题。

Method: 采用混合整数线性规划技术设计自动化调度算法，结合优先级图优化任务并行化与流程安排。

Result: 算法减少了50%的准备时间，将优先级图生成时间缩短至2分钟，显著提高了生产效率和资源利用率。

Conclusion: 自动化调度算法在效率、功能和适应性上远超传统手动方法，适用于灵活配置和高响应需求的生产环境。

Abstract: This study examines the digital value chain in automotive manufacturing,
focusing on the identification, software flashing, customization, and
commissioning of electronic control units in vehicle networks. A novel
precedence graph design is proposed to optimize this process chain using an
automated scheduling algorithm that employs mixed integer linear programming
techniques. The results show significant improvements in key metrics. The
algorithm reduces the number of production stations equipped with expensive
hardware and software to execute digital value chain processes, while
increasing capacity utilization through efficient scheduling and reduced idle
time. Task parallelization is optimized, resulting in streamlined workflows and
increased throughput. Compared to the traditional method, the automated
approach has reduced preparation time by 50% and reduced scheduling activities,
as it now takes two minutes to create the precedence graph. The flexibility of
the algorithm's constraints allows for vehicle-specific configurations while
maintaining high responsiveness, eliminating backup stations and facilitating
the integration of new topologies. Automated scheduling significantly
outperforms manual methods in efficiency, functionality, and adaptability.

</details>


### [151] [LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics](https://arxiv.org/abs/2504.21716)
*Marc Glocker,Peter Hönig,Matthias Hirschmanner,Markus Vincze*

Main category: cs.RO

TL;DR: 该论文提出了一种基于LLM驱动的代理编排架构的机器人系统，用于家庭物品的自主管理。系统通过内存增强的任务规划和三种专用代理（路由代理、任务规划代理和知识库代理）实现高级用户命令的执行，并利用RAG技术提升长期物品追踪能力，实验结果显示了高效的任务规划准确性。


<details>
  <summary>Details</summary>
Motivation: 为了解决家庭环境中机器人执行复杂任务的挑战，如长期物品追踪和动态任务规划，作者提出了一种结合LLM和专用代理的系统架构，旨在提升机器人的自主性和场景理解能力。

Method: 通过LLM驱动的代理编排架构（包括路由代理、任务规划代理和知识库代理）实现任务分解与执行，利用RAG增强记忆检索能力，并结合Grounded SAM与LLaMa3.2-Vision实现语义场景理解。

Result: 在三种家庭场景的评估中，系统展现出高效的任务规划准确性和记忆召回能力。Qwen2.5在专用代理中表现最佳，而LLaMA3.1在路由任务中表现优异。

Conclusion: 该研究证明，结合LLM和专用代理的系统架构能有效提升家庭环境中机器人的任务执行能力，尤其是通过RAG技术增强长期物品追踪的可行性。代码已开源。

Abstract: We present an embodied robotic system with an LLM-driven agent-orchestration
architecture for autonomous household object management. The system integrates
memory-augmented task planning, enabling robots to execute high-level user
commands while tracking past actions. It employs three specialized agents: a
routing agent, a task planning agent, and a knowledge base agent, each powered
by task-specific LLMs. By leveraging in-context learning, our system avoids the
need for explicit model training. RAG enables the system to retrieve context
from past interactions, enhancing long-term object tracking. A combination of
Grounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating
semantic scene understanding for task planning. Evaluation across three
household scenarios demonstrates high task planning accuracy and an improvement
in memory recall due to RAG. Specifically, Qwen2.5 yields best performance for
specialized agents, while LLaMA3.1 excels in routing tasks. The source code is
available at: https://github.com/marc1198/chat-hsr.

</details>


### [152] [UAV Marketplace Simulation Tool for BVLOS Operations](https://arxiv.org/abs/2504.21428)
*Kıvanç Şerefoğlu,Önder Gürcan,Reyhan Aydoğan*

Main category: cs.RO

TL;DR: 该论文介绍了一款用于评估多无人机（UAV）BVLOS任务中团队形成的模拟工具，支持动态对抗环境下协作策略的测试与比较。


<details>
  <summary>Details</summary>
Motivation: 研究旨在为多无人机在BVLOS任务中的团队协作提供可控的测试环境，尤其是应对对抗性干扰。

Method: 开发了一个模拟工具，支持配置任务参数和对抗行为，记录仿真日志与性能指标以便统计分析。

Result: 工具能有效测试和比较不同团队形成策略，支持在动态对抗条件下优化无人机协作。

Conclusion: 该工具为多无人机任务中的协作策略研究与实际应用提供了一个灵活且实用的测试平台。

Abstract: We present a simulation tool for evaluating team formation in autonomous
multi-UAV (Unmanned Aerial Vehicle) missions that operate Beyond Visual Line of
Sight (BVLOS). The tool models UAV collaboration and mission execution in
dynamic and adversarial conditions, where Byzantine UAVs attempt to disrupt
operations. Our tool allows researchers to integrate and compare various team
formation strategies in a controlled environment with configurable mission
parameters and adversarial behaviors. The log of each simulation run is stored
in a structured way along with performance metrics so that statistical analysis
could be done straightforwardly. The tool is versatile for testing and
improving UAV coordination strategies in real-world applications.

</details>


### [153] [Real Time Semantic Segmentation of High Resolution Automotive LiDAR Scans](https://arxiv.org/abs/2504.21602)
*Hannes Reichert,Benjamin Serfling,Elijah Schüssler,Kerim Turacan,Konrad Doll,Bernhard Sick*

Main category: cs.RO

TL;DR: 该论文提出了一种针对现代高分辨率LiDAR传感器的语义分割框架，解决了准确性和实时处理的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前的研究多基于过时、低分辨率的LiDAR传感器，难以满足实时性和高精度的需求。

Method: 提出了一种利用表面法线作为强输入特征的语义分割方法，并发布了一个128层LiDAR采集的新数据集。

Result: 该方法在精度和实时性上表现优异，并提供了ROS2实现和公开的数据集与代码。

Conclusion: 该研究填补了前沿研究与实际汽车应用之间的差距，推动了自动驾驶技术的发展。

Abstract: In recent studies, numerous previous works emphasize the importance of
semantic segmentation of LiDAR data as a critical component to the development
of driver-assistance systems and autonomous vehicles. However, many
state-of-the-art methods are tested on outdated, lower-resolution LiDAR sensors
and struggle with real-time constraints. This study introduces a novel semantic
segmentation framework tailored for modern high-resolution LiDAR sensors that
addresses both accuracy and real-time processing demands. We propose a novel
LiDAR dataset collected by a cutting-edge automotive 128 layer LiDAR in urban
traffic scenes. Furthermore, we propose a semantic segmentation method
utilizing surface normals as strong input features. Our approach is bridging
the gap between cutting-edge research and practical automotive applications.
Additionaly, we provide a Robot Operating System (ROS2) implementation that we
operate on our research vehicle. Our dataset and code are publicly available:
https://github.com/kav-institute/SemanticLiDAR.

</details>


### [154] [SimPRIVE: a Simulation framework for Physical Robot Interaction with Virtual Environments](https://arxiv.org/abs/2504.21454)
*Federico Nesti,Gianluca D'Amico,Mauro Marinoni,Giorgio Buttazzo*

Main category: cs.RO

TL;DR: 该论文介绍了SimPRIVE，一个用于物理机器人与虚拟环境交互的仿真框架，通过虚实结合的方式测试复杂算法，降低实际测试的成本和风险。


<details>
  <summary>Details</summary>
Motivation: 为了解决神经网络和强化学习代理在物理系统中的不可预测行为问题，同时利用高仿真模拟器降低实际测试的成本和风险。

Method: 提出了SimPRIVE框架，支持ROS 2的移动机器人在虚幻引擎5构建的虚拟环境中操作数字孪生体，并可自定义场景。

Result: 验证了强化学习代理在虚拟办公室环境中的避障能力，物理机器人在受限空间内无碰撞导航。

Conclusion: SimPRIVE框架为算法测试提供了一种高效、低成本且安全的解决方案。

Abstract: The use of machine learning in cyber-physical systems has attracted the
interest of both industry and academia. However, no general solution has yet
been found against the unpredictable behavior of neural networks and
reinforcement learning agents. Nevertheless, the improvements of
photo-realistic simulators have paved the way towards extensive testing of
complex algorithms in different virtual scenarios, which would be expensive and
dangerous to implement in the real world.
  This paper presents SimPRIVE, a simulation framework for physical robot
interaction with virtual environments, which operates as a vehicle-in-the-loop
platform, rendering a virtual world while operating the vehicle in the real
world.
  Using SimPRIVE, any physical mobile robot running on ROS 2 can easily be
configured to move its digital twin in a virtual world built with the Unreal
Engine 5 graphic engine, which can be populated with objects, people, or other
vehicles with programmable behavior.
  SimPRIVE has been designed to accommodate custom or pre-built virtual worlds
while being light-weight to contain execution times and allow fast rendering.
Its main advantage lies in the possibility of testing complex algorithms on the
full software and hardware stack while minimizing the risks and costs of a test
campaign. The framework has been validated by testing a reinforcement learning
agent trained for obstacle avoidance on an AgileX Scout Mini rover that
navigates a virtual office environment where everyday objects and people are
placed as obstacles. The physical rover moves with no collision in an indoor
limited space, thanks to a LiDAR-based heuristic.

</details>


### [155] [Multi-Goal Dexterous Hand Manipulation using Probabilistic Model-based Reinforcement Learning](https://arxiv.org/abs/2504.21585)
*Yingzhuo Jiang,Wenjun Huang,Rongdun Lin,Chenyang Miao,Tianfu Sun,Yunduan Cui*

Main category: cs.RO

TL;DR: 本文提出了一种基于目标条件概率模型预测控制（GC-PMPC）的方法，用于多目标灵巧手操纵任务，通过概率神经网络集成描述高维动态，并引入异步MPC策略满足实时控制需求，在仿真和真实平台上均表现出高效学习与控制性能。


<details>
  <summary>Details</summary>
Motivation: 灵巧手的高维动态和实时控制需求使得多目标操纵任务具有挑战性，本文旨在通过模型增强的强化学习方法提升其学习效率和控制性能。

Method: 设计概率神经网络集成描述灵巧手动态，结合异步模型预测控制（MPC）策略，开发了GC-PMPC框架。

Result: 在四种模拟Shadow Hand场景中，GC-PMPC显著优于现有方法，并在真实DexHand 021平台上仅用80分钟实现立方体骰子操纵到三个目标位姿。

Conclusion: GC-PMPC展示了在多目标灵巧手操纵任务中的高效学习能力和出色控制性能，为低成本灵巧手平台提供了可行解决方案。

Abstract: This paper tackles the challenge of learning multi-goal dexterous hand
manipulation tasks using model-based Reinforcement Learning. We propose
Goal-Conditioned Probabilistic Model Predictive Control (GC-PMPC) by designing
probabilistic neural network ensembles to describe the high-dimensional
dexterous hand dynamics and introducing an asynchronous MPC policy to meet the
control frequency requirements in real-world dexterous hand systems. Extensive
evaluations on four simulated Shadow Hand manipulation scenarios with randomly
generated goals demonstrate GC-PMPC's superior performance over
state-of-the-art baselines. It successfully drives a cable-driven Dexterous
hand, DexHand 021 with 12 Active DOFs and 5 tactile sensors, to learn
manipulating a cubic die to three goal poses within approximately 80 minutes of
interactions, demonstrating exceptional learning efficiency and control
performance on a cost-effective dexterous hand platform.

</details>


### [156] [One Net to Rule Them All: Domain Randomization in Quadcopter Racing Across Different Platforms](https://arxiv.org/abs/2504.21586)
*Robin Ferede,Till Blaha,Erin Lucassen,Christophe De Wagter,Guido C. H. E. de Croon*

Main category: cs.RO

TL;DR: 该论文提出了一种通过领域随机化训练的神经网络控制器，能够通用地控制不同类型的竞速无人机，并在实际测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在高速竞速无人机领域，开发一个适用于不同平台的通用控制器具有挑战性。

Method: 使用领域随机化训练单一神经网络，直接根据当前状态计算电机指令，并在两种不同尺寸的无人机上验证其鲁棒性。

Result: 通用控制器在适应性上表现优异，尽管速度略低于专用控制器，但无随机化的方法无法实现仿真到现实的迁移。

Conclusion: 领域随机化为通用控制器提供了潜力，尽管在速度和鲁棒性之间存在权衡。

Abstract: In high-speed quadcopter racing, finding a single controller that works well
across different platforms remains challenging. This work presents the first
neural network controller for drone racing that generalizes across physically
distinct quadcopters. We demonstrate that a single network, trained with domain
randomization, can robustly control various types of quadcopters. The network
relies solely on the current state to directly compute motor commands. The
effectiveness of this generalized controller is validated through real-world
tests on two substantially different crafts (3-inch and 5-inch race
quadcopters). We further compare the performance of this generalized controller
with controllers specifically trained for the 3-inch and 5-inch drone, using
their identified model parameters with varying levels of domain randomization
(0%, 10%, 20%, 30%). While the generalized controller shows slightly slower
speeds compared to the fine-tuned models, it excels in adaptability across
different platforms. Our results show that no randomization fails sim-to-real
transfer while increasing randomization improves robustness but reduces speed.
Despite this trade-off, our findings highlight the potential of domain
randomization for generalizing controllers, paving the way for universal AI
controllers that can adapt to any platform.

</details>


### [157] [Leveraging Pre-trained Large Language Models with Refined Prompting for Online Task and Motion Planning](https://arxiv.org/abs/2504.21596)
*Huihui Guo,Huilong Pi,Yunchuan Qin,Zhuo Tang,Kenli Li*

Main category: cs.RO

TL;DR: 论文提出了一个基于大语言模型（LLM）的闭环任务规划与执行系统LLM-PAS，通过将部分约束检查从规划阶段转移到执行阶段，提升了任务执行的鲁棒性和应对异常的能力，并引入了First Look Prompting（FLP）方法优化目标生成。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的快速发展，智能机器人需要在复杂任务中稳定执行并应对异常。传统方法在任务规划和执行阶段的分离导致对异常处理的不足，因此需要一个更集成、鲁棒的系统。

Method: LLM-PAS结合了任务规划与执行，利用LLM进行长时程任务规划和异常处理，通过FLP方法优化PDDL目标生成，并在执行阶段动态检查约束。

Result: 实验表明，LLM-PAS在异常条件下表现出更高的有效性和鲁棒性，FLP方法显著提升了目标生成的效率。

Conclusion: LLM-PAS通过闭环规划和执行机制，结合LLM的推理能力，显著提升了智能机器人在复杂任务中的表现，为未来系统设计提供了新思路。

Abstract: With the rapid advancement of artificial intelligence, there is an increasing
demand for intelligent robots capable of assisting humans in daily tasks and
performing complex operations. Such robots not only require task planning
capabilities but must also execute tasks with stability and robustness. In this
paper, we present a closed-loop task planning and acting system, LLM-PAS, which
is assisted by a pre-trained Large Language Model (LLM). While LLM-PAS plans
long-horizon tasks in a manner similar to traditional task and motion planners,
it also emphasizes the execution phase of the task. By transferring part of the
constraint-checking process from the planning phase to the execution phase,
LLM-PAS enables exploration of the constraint space and delivers more accurate
feedback on environmental anomalies during execution. The reasoning
capabilities of the LLM allow it to handle anomalies that cannot be addressed
by the robust executor. To further enhance the system's ability to assist the
planner during replanning, we propose the First Look Prompting (FLP) method,
which induces LLM to generate effective PDDL goals. Through comparative
prompting experiments and systematic experiments, we demonstrate the
effectiveness and robustness of LLM-PAS in handling anomalous conditions during
task execution.

</details>


### [158] [Self-Supervised Monocular Visual Drone Model Identification through Improved Occlusion Handling](https://arxiv.org/abs/2504.21695)
*Stavrow A. Bahnam,Christophe De Wagter,Guido C. H. E. de Croon*

Main category: cs.RO

TL;DR: 论文提出了一种自监督学习方案，仅使用单目视频和无人机控制器数据，训练基于神经网络的无人机模型，解决了高动态与遮挡条件下的运动估计难题，并在实际飞行中验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 在GPS缺失环境下，传统视觉方法因高速飞行和遮挡导致性能下降，而现有无人机模型依赖外部运动捕捉系统的监督学习，限制了其扩展性。本文旨在通过自监督学习，解决这些问题。

Method: 首先训练自监督相对位姿估计模型（教师模型），再将其用于指导无人机模型（学生模型）的学习，并改进了遮挡处理方法以提高训练效果。

Result: 改进后的方法平均降低15%的里程计误差，且无人机模型在高速下比教师模型更精确。将其集成到VIO系统（ROVIO）后，在激进飞行轨迹中表现优异。

Conclusion: 自监督学习方案缩小了实验室环境与真实应用间的差距，结合视觉与无人机模型的方案能提升高速飞行和状态估计的适用性。

Abstract: Ego-motion estimation is vital for drones when flying in GPS-denied
environments. Vision-based methods struggle when flight speed increases and
close-by objects lead to difficult visual conditions with considerable motion
blur and large occlusions. To tackle this, vision is typically complemented by
state estimation filters that combine a drone model with inertial measurements.
However, these drone models are currently learned in a supervised manner with
ground-truth data from external motion capture systems, limiting scalability to
different environments and drones. In this work, we propose a self-supervised
learning scheme to train a neural-network-based drone model using only onboard
monocular video and flight controller data (IMU and motor feedback). We achieve
this by first training a self-supervised relative pose estimation model, which
then serves as a teacher for the drone model. To allow this to work at high
speed close to obstacles, we propose an improved occlusion handling method for
training self-supervised pose estimation models. Due to this method, the root
mean squared error of resulting odometry estimates is reduced by an average of
15%. Moreover, the student neural drone model can be successfully obtained from
the onboard data. It even becomes more accurate at higher speeds compared to
its teacher, the self-supervised vision-based model. We demonstrate the value
of the neural drone model by integrating it into a traditional filter-based VIO
system (ROVIO), resulting in superior odometry accuracy on aggressive 3D racing
trajectories near obstacles. Self-supervised learning of ego-motion estimation
represents a significant step toward bridging the gap between flying in
controlled, expensive lab environments and real-world drone applications. The
fusion of vision and drone models will enable higher-speed flight and improve
state estimation, on any drone in any environment.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [159] [Research on CNN-BiLSTM Network Traffic Anomaly Detection Model Based on MindSpore](https://arxiv.org/abs/2504.21008)
*Qiuyan Xiang,Shuang Wu,Dongze Wu,Yuxin Liu,Zhenkai Qin*

Main category: cs.CR

TL;DR: 论文提出了一种结合CNN和BiLSTM的网络流量异常检测模型，在NF-BoT-IoT数据集上表现优异，各项指标达99%。


<details>
  <summary>Details</summary>
Motivation: 随着IoT和IIoT的普及，网络架构复杂化和流量激增使传统安全机制难以应对高频、多样且隐蔽的网络攻击，因此需要新的检测方法。

Method: 采用卷积神经网络（CNN）和双向长短期记忆网络（BiLSTM）结合的模型，并在MindSpore框架上实现。

Result: 在NF-BoT-IoT数据集上的实验显示，该模型在准确率、精确率、召回率和F1分数上均达到99%。

Conclusion: 提出的模型在网络入侵检测任务中表现出色且鲁棒性强。

Abstract: With the widespread adoption of the Internet of Things (IoT) and Industrial
IoT (IIoT) technologies, network architectures have become increasingly
complex, and the volume of traffic has grown substantially. This evolution
poses significant challenges to traditional security mechanisms, particularly
in detecting high-frequency, diverse, and highly covert network attacks. To
address these challenges, this study proposes a novel network traffic anomaly
detection model that integrates a Convolutional Neural Network (CNN) with a
Bidirectional Long Short-Term Memory (BiLSTM) network, implemented on the
MindSpore framework. Comprehensive experiments were conducted using the
NF-BoT-IoT dataset. The results demonstrate that the proposed model achieves
99% across accuracy, precision, recall, and F1-score, indicating its strong
performance and robustness in network intrusion detection tasks.

</details>


### [160] [Semantic-Aware Contrastive Fine-Tuning: Boosting Multimodal Malware Classification with Discriminative Embeddings](https://arxiv.org/abs/2504.21028)
*Ivan Montoya Sanchez,Shaswata Mitra,Aritran Piplai,Sudip Mittal*

Main category: cs.CR

TL;DR: 论文提出了一种基于对比微调（CFT）的方法，通过针对性选择难负样本来优化LLM嵌入，提升恶意软件家族分类的准确性，在few-shot场景下显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前恶意软件变种快速演变，但现有LLM生成的描述存在语义嵌入重叠和与二进制行为特征不匹配的问题，导致分类效果受限。

Method: 采用对比微调（CFT），基于余弦相似度选择难负样本（高相似度和中等级别样本），优化LLM嵌入，并结合MAML框架构建多模态分类器。

Result: 在CIC-AndMal-2020和BODMAS数据集上，仅用20个样本达到63.15%分类准确率，优于基线11-21%，且相似性选择比随机采样提升10-23%。

Conclusion: 该方法通过细粒度语义区分提升了恶意软件分类效果，并为LLM适应网络安全任务提供了可扩展框架。

Abstract: The rapid evolution of malware variants requires robust classification
methods to enhance cybersecurity. While Large Language Models (LLMs) offer
potential for generating malware descriptions to aid family classification,
their utility is limited by semantic embedding overlaps and misalignment with
binary behavioral features. We propose a contrastive fine-tuning (CFT) method
that refines LLM embeddings via targeted selection of hard negative samples
based on cosine similarity, enabling LLMs to distinguish between closely
related malware families. Our approach combines high-similarity negatives to
enhance discriminative power and mid-tier negatives to increase embedding
diversity, optimizing both precision and generalization. Evaluated on the
CIC-AndMal-2020 and BODMAS datasets, our refined embeddings are integrated into
a multimodal classifier within a Model-Agnostic Meta-Learning (MAML) framework
on a few-shot setting. Experiments demonstrate significant improvements: our
method achieves 63.15% classification accuracy with as few as 20 samples on
CIC-AndMal-2020, outperforming baselines by 11--21 percentage points and
surpassing prior negative sampling strategies. Ablation studies confirm the
superiority of similarity-based selection over random sampling, with gains of
10-23%. Additionally, fine-tuned LLMs generate attribute-aware descriptions
that generalize to unseen variants, bridging textual and binary feature gaps.
This work advances malware classification by enabling nuanced semantic
distinctions and provides a scalable framework for adapting LLMs to
cybersecurity challenges.

</details>


### [161] [PICO: Secure Transformers via Robust Prompt Isolation and Cybersecurity Oversight](https://arxiv.org/abs/2504.21029)
*Ben Goertzel,Paulos Yibelo*

Main category: cs.CR

TL;DR: 论文提出了PICO框架，通过双通道结构和安全专家代理，有效防御提示注入攻击，确保响应安全可靠。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer架构易受提示注入攻击影响，需要一种更安全可靠的方法来分离和处理系统指令与用户输入。

Method: 采用双通道结构隔离系统指令与用户输入，结合安全专家代理和网络安全知识图谱（CKG），并通过可控门控机制融合结果。

Result: PICO框架在理论上和实际案例中（如策略傀儡攻击）均表现出色，既能从头训练也支持高效微调。

Conclusion: PICO框架为对抗提示注入攻击提供了高效解决方案，同时兼顾了安全性和灵活性。

Abstract: We propose a robust transformer architecture designed to prevent prompt
injection attacks and ensure secure, reliable response generation. Our PICO
(Prompt Isolation and Cybersecurity Oversight) framework structurally separates
trusted system instructions from untrusted user inputs through dual channels
that are processed independently and merged only by a controlled, gated fusion
mechanism. In addition, we integrate a specialized Security Expert Agent within
a Mixture-of-Experts (MoE) framework and incorporate a Cybersecurity Knowledge
Graph (CKG) to supply domain-specific reasoning. Our training design further
ensures that the system prompt branch remains immutable while the rest of the
network learns to handle adversarial inputs safely. This PICO framework is
presented via a general mathematical formulation, then elaborated in terms of
the specifics of transformer architecture, and fleshed out via hypothetical
case studies including Policy Puppetry attacks. While the most effective
implementation may involve training transformers in a PICO-based way from
scratch, we also present a cost-effective fine-tuning approach.

</details>


### [162] [SAGA: A Security Architecture for Governing AI Agentic Systems](https://arxiv.org/abs/2504.21034)
*Georgios Syros,Anshuman Suri,Cristina Nita-Rotaru,Alina Oprea*

Main category: cs.CR

TL;DR: SAGA框架为用户提供了对其自主代理生命周期的全面控制，通过中央注册和密码学机制实现精细的访问控制，平衡安全与性能。


<details>
  <summary>Details</summary>
Motivation: 现有代理系统设计多为理论性，缺乏用户控制的代理管理，SAGA填补了这一空白，确保代理在敏感环境中的安全部署。

Method: 采用中央提供商（Provider）注册代理，结合密码学机制生成访问控制令牌，实现精细化的代理间通信管理。

Result: 在不同地理位置和多LLM环境下测试，SAGA展现了低性能开销且不影响任务效用，适用于敏感环境。

Conclusion: SAGA框架为自主代理的安全可信部署提供了有效解决方案，推动了该技术在敏感环境中的负责任应用。

Abstract: Large Language Model (LLM)-based agents increasingly interact, collaborate,
and delegate tasks to one another autonomously with minimal human interaction.
Industry guidelines for agentic system governance emphasize the need for users
to maintain comprehensive control over their agents, mitigating potential
damage from malicious agents. Several proposed agentic system designs address
agent identity, authorization, and delegation, but remain purely theoretical,
without concrete implementation and evaluation. Most importantly, they do not
provide user-controlled agent management. To address this gap, we propose SAGA,
a Security Architecture for Governing Agentic systems, that offers user
oversight over their agents' lifecycle. In our design, users register their
agents with a central entity, the Provider, that maintains agents contact
information, user-defined access control policies, and helps agents enforce
these policies on inter-agent communication. We introduce a cryptographic
mechanism for deriving access control tokens, that offers fine-grained control
over an agent's interaction with other agents, balancing security and
performance consideration. We evaluate SAGA on several agentic tasks, using
agents in different geolocations, and multiple on-device and cloud LLMs,
demonstrating minimal performance overhead with no impact on underlying task
utility in a wide range of conditions. Our architecture enables secure and
trustworthy deployment of autonomous agents, accelerating the responsible
adoption of this technology in sensitive environments.

</details>


### [163] [Can Differentially Private Fine-tuning LLMs Protect Against Privacy Attacks?](https://arxiv.org/abs/2504.21036)
*Hao Du,Shang Liu,Yang Cao*

Main category: cs.CR

TL;DR: 本研究探讨了在大型语言模型（LLM）微调中应用差分隐私（DP）的隐私效果，分析了不同微调方法和隐私预算下的隐私风险与模型效用的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着LLM微调成为适应特定任务的关键策略，其带来的隐私问题日益凸显。虽然DP提供了理论上的隐私保障，但其在实际LLM微调中的效果尚不明确。

Method: 通过数据提取和成员推断攻击，系统地评估了不同微调方法和隐私预算下的DP效果。

Result: 研究发现DP能显著降低隐私风险，但对模型效用的影响因微调方法而异；某些方法因效用损失严重而不适合DP。

Conclusion: 结果为隐私敏感的LLM部署提供了实用指导，并为未来优化隐私-效用权衡的研究奠定了基础。

Abstract: Fine-tuning large language models (LLMs) has become an essential strategy for
adapting them to specialized tasks; however, this process introduces
significant privacy challenges, as sensitive training data may be inadvertently
memorized and exposed. Although differential privacy (DP) offers strong
theoretical guarantees against such leakage, its empirical privacy
effectiveness on LLMs remains unclear, especially under different fine-tuning
methods. In this paper, we systematically investigate the impact of DP across
fine-tuning methods and privacy budgets, using both data extraction and
membership inference attacks to assess empirical privacy risks. Our main
findings are as follows: (1) Differential privacy reduces model utility, but
its impact varies significantly across different fine-tuning methods. (2)
Without DP, the privacy risks of models fine-tuned with different approaches
differ considerably. (3) When DP is applied, even a relatively high privacy
budget can substantially lower privacy risk. (4) The privacy-utility trade-off
under DP training differs greatly among fine-tuning methods, with some methods
being unsuitable for DP due to severe utility degradation. Our results provide
practical guidance for privacy-conscious deployment of LLMs and pave the way
for future research on optimizing the privacy-utility trade-off in fine-tuning
methodologies.

</details>


### [164] [Security Bug Report Prediction Within and Across Projects: A Comparative Study of BERT and Random Forest](https://arxiv.org/abs/2504.21037)
*Farnaz Soltaniani,Mohammad Ghafari,Mohammed Sayagh*

Main category: cs.CR

TL;DR: 摘要比较了BERT和随机森林（RF）在安全缺陷报告（SBR）预测中的表现，发现RF在项目内预测表现更好，而BERT在跨项目预测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 早期发现安全缺陷报告（SBRs）对预防漏洞和确保系统可靠性至关重要，现有机器学习模型的预测性能还有提升空间。

Method: 研究中全面比较了BERT和随机森林（RF）在SBR预测中的表现，包括项目内和跨项目预测。

Result: RF在项目内预测中表现更好（G-measure高34%），而BERT在跨项目预测中表现优异（G-measure达62%），且混合训练数据时BERT表现最佳（66% G-measure）。

Conclusion: 研究表明不同场景下BERT和RF各有优势，BERT在跨项目预测中表现突出，RF则在项目内预测中更优。

Abstract: Early detection of security bug reports (SBRs) is crucial for preventing
vulnerabilities and ensuring system reliability. While machine learning models
have been developed for SBR prediction, their predictive performance still has
room for improvement. In this study, we conduct a comprehensive comparison
between BERT and Random Forest (RF), a competitive baseline for predicting
SBRs. The results show that RF outperforms BERT with a 34% higher average
G-measure for within-project predictions. Adding only SBRs from various
projects improves both models' average performance. However, including both
security and nonsecurity bug reports significantly reduces RF's average
performance to 46%, while boosts BERT to its best average performance of 66%,
surpassing RF. In cross-project SBR prediction, BERT achieves a remarkable 62%
G-measure, which is substantially higher than RF.

</details>


### [165] [Prefill-Based Jailbreak: A Novel Approach of Bypassing LLM Safety Boundary](https://arxiv.org/abs/2504.21038)
*Yakai Li,Jiekang Hu,Weiduan Sang,Luping Ma,Jing Xie,Weijuan Zhang,Aimin Yu,Shijie Zhao,Qingjia Huang,Qihang Zhou*

Main category: cs.CR

TL;DR: 本文提出了一种利用LLMs预填充特性的新型越狱攻击方法，包括静态预填充（SP）和优化预填充（OP），实验证明其攻击成功率显著高于基线方法。


<details>
  <summary>Details</summary>
Motivation: 研究越狱方法的动机在于暴露LLMs的系统性漏洞，为开发者提供安全增强的指导。

Method: 提出两种攻击变体：SP使用通用预填充文本，OP通过迭代优化预填充文本来最大化攻击成功率。

Result: 在AdvBench基准测试中，OP方法在某些模型上攻击成功率高达99.82%。

Conclusion: 该研究强调了需要加强内容验证机制以防止预填充特性的对抗性利用，相关代码和数据已公开。

Abstract: Large Language Models (LLMs) are designed to generate helpful and safe
content. However, adversarial attacks, commonly referred to as jailbreak, can
bypass their safety protocols, prompting LLMs to generate harmful content or
reveal sensitive data. Consequently, investigating jailbreak methodologies is
crucial for exposing systemic vulnerabilities within LLMs, ultimately guiding
the continuous implementation of security enhancements by developers. In this
paper, we introduce a novel jailbreak attack method that leverages the
prefilling feature of LLMs, a feature designed to enhance model output
constraints. Unlike traditional jailbreak methods, the proposed attack
circumvents LLMs' safety mechanisms by directly manipulating the probability
distribution of subsequent tokens, thereby exerting control over the model's
output. We propose two attack variants: Static Prefilling (SP), which employs a
universal prefill text, and Optimized Prefilling (OP), which iteratively
optimizes the prefill text to maximize the attack success rate. Experiments on
six state-of-the-art LLMs using the AdvBench benchmark validate the
effectiveness of our method and demonstrate its capability to substantially
enhance attack success rates when combined with existing jailbreak approaches.
The OP method achieved attack success rates of up to 99.82% on certain models,
significantly outperforming baseline methods. This work introduces a new
jailbreak attack method in LLMs, emphasizing the need for robust content
validation mechanisms to mitigate the adversarial exploitation of prefilling
features. All code and data used in this paper are publicly available.

</details>


### [166] [Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report](https://arxiv.org/abs/2504.21039)
*Paul Kassianik,Baturay Saglam,Alexander Chen,Blaine Nelson,Anu Vellore,Massimo Aufiero,Fraser Burch,Dhruv Kedia,Avi Zohary,Sajana Weerawardhena,Aman Priyanshu,Adam Swanda,Amy Chang,Hyrum Anderson,Kojin Oshiba,Omar Santos,Yaron Singer,Amin Karbasi*

Main category: cs.CR

TL;DR: 摘要介绍了Foundation-Sec-8B，一个基于Llama 3.1架构的网络安全专用大语言模型，通过在精选的网络安全语料库上进行持续预训练，解决了该领域数据稀缺和知识表示复杂的挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在多个领域取得了显著成功，但由于网络安全领域的数据稀缺和知识复杂性，其在该领域的应用仍有限。本文旨在填补这一空白。

Method: 基于Llama 3.1架构，通过持续预训练精选的网络安全语料库开发了Foundation-Sec-8B模型。

Result: Foundation-Sec-8B在网络安全任务上表现优异，可匹敌Llama 3.1-70B和GPT-4o-mini等更大规模的模型。

Conclusion: 通过公开发布Foundation-Sec-8B，希望能加速人工智能驱动的网络安全工具在公共和私营领域的应用进程。

Abstract: As transformer-based large language models (LLMs) increasingly permeate
society, they have revolutionized domains such as software engineering,
creative writing, and digital arts. However, their adoption in cybersecurity
remains limited due to challenges like scarcity of specialized training data
and complexity of representing cybersecurity-specific knowledge. To address
these gaps, we present Foundation-Sec-8B, a cybersecurity-focused LLM built on
the Llama 3.1 architecture and enhanced through continued pretraining on a
carefully curated cybersecurity corpus. We evaluate Foundation-Sec-8B across
both established and new cybersecurity benchmarks, showing that it matches
Llama 3.1-70B and GPT-4o-mini in certain cybersecurity-specific tasks. By
releasing our model to the public, we aim to accelerate progress and adoption
of AI-driven tools in both public and private cybersecurity contexts.

</details>


### [167] [What's Pulling the Strings? Evaluating Integrity and Attribution in AI Training and Inference through Concept Shift](https://arxiv.org/abs/2504.21042)
*Jiamin Chang,Haoyang Li,Hammond Pearce,Ruoxi Sun,Bo Li,Minhui Xue*

Main category: cs.CR

TL;DR: ConceptLens是一个通用框架，利用预训练多模态模型分析概念漂移，评估AI系统中的完整性威胁、隐私风险和社会学偏见，并提供可操作见解以增强AI信任度。


<details>
  <summary>Details</summary>
Motivation: 随着AI的普及，其可信度问题（如完整性、隐私、鲁棒性和偏见）日益突出。研究旨在通过ConceptLens框架识别和归因这些威胁，提升AI系统的安全性和可信性。

Method: 提出ConceptLens框架，利用预训练多模态模型分析探测样本中的概念漂移，检测数据投毒攻击、隐私风险及社会学偏见，并识别模型对关键概念的过度依赖。

Result: ConceptLens在检测普通数据投毒攻击和偏见注入漏洞方面表现优异，能识别高风险样本中的隐私问题，并揭示生成内容中的社会学偏见。

Conclusion: ConceptLens揭示了安全数据可能被意外利用的风险，为增强AI系统可信度提供了具体方案，有助于加速AI的采用和创新。

Abstract: The growing adoption of artificial intelligence (AI) has amplified concerns
about trustworthiness, including integrity, privacy, robustness, and bias. To
assess and attribute these threats, we propose ConceptLens, a generic framework
that leverages pre-trained multimodal models to identify the root causes of
integrity threats by analyzing Concept Shift in probing samples. ConceptLens
demonstrates strong detection performance for vanilla data poisoning attacks
and uncovers vulnerabilities to bias injection, such as the generation of
covert advertisements through malicious concept shifts. It identifies privacy
risks in unaltered but high-risk samples, filters them before training, and
provides insights into model weaknesses arising from incomplete or imbalanced
training data. Additionally, at the model level, it attributes concepts that
the target model is overly dependent on, identifies misleading concepts, and
explains how disrupting key concepts negatively impacts the model. Furthermore,
it uncovers sociological biases in generative content, revealing disparities
across sociological contexts. Strikingly, ConceptLens reveals how safe training
and inference data can be unintentionally and easily exploited, potentially
undermining safety alignment. Our study informs actionable insights to breed
trust in AI systems, thereby speeding adoption and driving greater innovation.

</details>


### [168] [CodeBC: A More Secure Large Language Model for Smart Contract Code Generation in Blockchain](https://arxiv.org/abs/2504.21043)
*Lingxiang wang,Hainan Zhang,Qinnan Zhang,Ziwei Wang,Hongwei Zheng,Jin Dong,Zhiming Zheng*

Main category: cs.CR

TL;DR: CodeBC是一种专门为区块链智能合约设计的代码生成模型，通过三阶段微调方法，无需依赖成对的漏洞标注数据，而是利用漏洞和安全标签来提升模型的安全意识，实验证明其在生成安全代码方面优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在生成代码时缺乏对安全漏洞的理解，尤其在区块链智能合约等高安全性任务中表现不足。标注数据稀缺（如Solidity语言）进一步加剧了这一挑战。

Method: 基于CodeLlama的三阶段微调方法，利用漏洞和安全标签（而非成对标注数据）训练模型区分漏洞代码与安全代码，推理时通过安全标签生成安全代码。

Result: 实验表明，CodeBC在BLEU、CodeBLEU和编译通过率上优于基线模型，同时显著降低漏洞率。

Conclusion: CodeBC的三阶段微调策略高效且成本低，为生成安全的智能合约代码提供了可行方案。

Abstract: Large language models (LLMs) excel at generating code from natural language
instructions, yet they often lack an understanding of security vulnerabilities.
This limitation makes it difficult for LLMs to avoid security risks in
generated code, particularly in high-security programming tasks such as smart
contract development for blockchain. Researchers have attempted to enhance the
vulnerability awareness of these models by training them to differentiate
between vulnerable and fixed code snippets. However, this approach relies
heavily on manually labeled vulnerability data, which is only available for
popular languages like Python and C++. For low-resource languages like
Solidity, used in smart contracts, large-scale annotated datasets are scarce
and difficult to obtain. To address this challenge, we introduce CodeBC, a code
generation model specifically designed for generating secure smart contracts in
blockchain. CodeBC employs a three-stage fine-tuning approach based on
CodeLlama, distinguishing itself from previous methods by not relying on
pairwise vulnerability location annotations. Instead, it leverages
vulnerability and security tags to teach the model the differences between
vulnerable and secure code. During the inference phase, the model leverages
security tags to generate secure and robust code. Experimental results
demonstrate that CodeBC outperforms baseline models in terms of BLEU, CodeBLEU,
and compilation pass rates, while significantly reducing vulnerability rates.
These findings validate the effectiveness and cost-efficiency of our
three-stage fine-tuning strategy, making CodeBC a promising solution for
generating secure smart contract code.

</details>


### [169] [AGATE: Stealthy Black-box Watermarking for Multimodal Model Copyright Protection](https://arxiv.org/abs/2504.21044)
*Jianbo Gao,Keke Gai,Jing Yu,Liehuang Zhu,Qi Wu*

Main category: cs.CR

TL;DR: 论文提出了AGATE框架，用于解决多模态模型版权保护中的隐蔽性和鲁棒性问题，通过对抗性触发生成和后变换模块改进现有方法。


<details>
  <summary>Details</summary>
Motivation: 大规模AI模型成为攻击目标，现有版权保护方法易受恶意检测和伪造，需要更隐蔽和鲁棒的解决方案。

Method: 提出AGATE框架，包括对抗性触发生成、后变换模块和两阶段水印验证。

Result: 在五个数据集的多模态图文检索和图像分类任务中优于现有方法，并验证了对抗攻击下的鲁棒性。

Conclusion: AGATE显著提升了多模态模型版权保护的隐蔽性和鲁棒性，适用于实际场景。

Abstract: Recent advancement in large-scale Artificial Intelligence (AI) models
offering multimodal services have become foundational in AI systems, making
them prime targets for model theft. Existing methods select Out-of-Distribution
(OoD) data as backdoor watermarks and retrain the original model for copyright
protection. However, existing methods are susceptible to malicious detection
and forgery by adversaries, resulting in watermark evasion. In this work, we
propose Model-\underline{ag}nostic Black-box Backdoor W\underline{ate}rmarking
Framework (AGATE) to address stealthiness and robustness challenges in
multimodal model copyright protection. Specifically, we propose an adversarial
trigger generation method to generate stealthy adversarial triggers from
ordinary dataset, providing visual fidelity while inducing semantic shifts. To
alleviate the issue of anomaly detection among model outputs, we propose a
post-transform module to correct the model output by narrowing the distance
between adversarial trigger image embedding and text embedding. Subsequently, a
two-phase watermark verification is proposed to judge whether the current model
infringes by comparing the two results with and without the transform module.
Consequently, we consistently outperform state-of-the-art methods across five
datasets in the downstream tasks of multimodal image-text retrieval and image
classification. Additionally, we validated the robustness of AGATE under two
adversarial attack scenarios.

</details>


### [170] [Leveraging LLM to Strengthen ML-Based Cross-Site Scripting Detection](https://arxiv.org/abs/2504.21045)
*Dennis Miczek,Divyesh Gabbireddy,Suman Saha*

Main category: cs.CR

TL;DR: 该研究针对混淆XSS攻击，提出了一种利用大型语言模型（LLM）自动生成复杂混淆XSS样本的方法，显著提升了机器学习模型的检测能力。


<details>
  <summary>Details</summary>
Motivation: XSS攻击是网络安全的重大威胁，尤其是混淆XSS攻击更难检测。现有方法在混淆样本上的性能较差，需通过更复杂的训练数据提升模型效果。

Method: 使用LLM自动生成复杂混淆XSS样本，扩充训练数据集，并训练随机森林模型进行检测。

Result: 模型在混淆样本上的准确率从81.9%提升至99.5%，且LLM生成的样本复杂度比现有工具高28.1%。

Conclusion: 通过LLM生成复杂混淆样本，显著提高了XSS检测模型的性能，适用于实际网络安全应用。

Abstract: According to the Open Web Application Security Project (OWASP), Cross-Site
Scripting (XSS) is a critical security vulnerability. Despite decades of
research, XSS remains among the top 10 security vulnerabilities. Researchers
have proposed various techniques to protect systems from XSS attacks, with
machine learning (ML) being one of the most widely used methods. An ML model is
trained on a dataset to identify potential XSS threats, making its
effectiveness highly dependent on the size and diversity of the training data.
A variation of XSS is obfuscated XSS, where attackers apply obfuscation
techniques to alter the code's structure, making it challenging for security
systems to detect its malicious intent. Our study's random forest model was
trained on traditional (non-obfuscated) XSS data achieved 99.8% accuracy.
However, when tested against obfuscated XSS samples, accuracy dropped to 81.9%,
underscoring the importance of training ML models with obfuscated data to
improve their effectiveness in detecting XSS attacks. A significant challenge
is to generate highly complex obfuscated code despite the availability of
several public tools. These tools can only produce obfuscation up to certain
levels of complexity.
  In our proposed system, we fine-tune a Large Language Model (LLM) to generate
complex obfuscated XSS payloads automatically. By transforming original XSS
samples into diverse obfuscated variants, we create challenging training data
for ML model evaluation. Our approach achieved a 99.5% accuracy rate with the
obfuscated dataset. We also found that the obfuscated samples generated by the
LLMs were 28.1% more complex than those created by other tools, significantly
improving the model's ability to handle advanced XSS attacks and making it more
effective for real-world application security.

</details>


### [171] [Phishing URL Detection using Bi-LSTM](https://arxiv.org/abs/2504.21049)
*Sneha Baskota*

Main category: cs.CR

TL;DR: 论文提出了一种基于Bi-LSTM的深度学习模型，用于分类URL，准确率达到97%，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前网络钓鱼攻击日益严重，传统检测系统存在高误报率且难以应对多样化的攻击类型，因此需要更高效的解决方案。

Method: 使用双向长短期记忆网络（Bi-LSTM）模型，通过分析URL的序列数据并捕捉上下文信息，对URL进行分类（良性、钓鱼、篡改、恶意软件）。

Result: 在包含65万多条URL的数据集上，模型达到了97%的准确率，显著优于传统技术。

Conclusion: 该Bi-LSTM模型能有效提升钓鱼攻击检测的准确性，为解决网络安全问题提供了新思路。

Abstract: Phishing attacks threaten online users, often leading to data breaches,
financial losses, and identity theft. Traditional phishing detection systems
struggle with high false positive rates and are usually limited by the types of
attacks they can identify. This paper proposes a deep learning-based approach
using a Bidirectional Long Short-Term Memory (Bi-LSTM) network to classify URLs
into four categories: benign, phishing, defacement, and malware. The model
leverages sequential URL data and captures contextual information, improving
the accuracy of phishing detection. Experimental results on a dataset
comprising over 650,000 URLs demonstrate the model's effectiveness, achieving
97% accuracy and significant improvements over traditional techniques.

</details>


### [172] [SFIBA: Spatial-based Full-target Invisible Backdoor Attacks](https://arxiv.org/abs/2504.21052)
*Yangxu Yin,Honglong Chen,Yudong Gao,Peng Sun,Zhishuai Li,Weifeng Liu*

Main category: cs.CR

TL;DR: 该论文提出了一种名为SFIBA的空间全目标隐形后门攻击方法，通过限制特定空间区域和形态的触发器和基于频率域的触发器注入技术，解决了现有多目标后门攻击在触发特异性和隐蔽性上的不足。


<details>
  <summary>Details</summary>
Motivation: 多目标后门攻击在深度神经网络中存在显著安全威胁，但现有方法在触发特异性和隐蔽性上表现不足。SFIBA旨在解决这些问题，提升攻击的有效性和隐蔽性。

Method: SFIBA通过将不同类别的触发器限制在特定局部空间区域和形态，并结合快速傅里叶变换、离散小波变换和奇异值分解等技术，在频率域中注入触发器，以保证特异性和隐蔽性。

Result: 实验表明，SFIBA在多个数据集和模型上表现出优秀的攻击性能和隐蔽性，同时保持了模型在良性样本上的性能，并能绕过现有后门防御。

Conclusion: SFIBA是一种有效的多目标隐形后门攻击方法，解决了现有方法的局限性，具有较高的实用性和隐蔽性。

Abstract: Multi-target backdoor attacks pose significant security threats to deep
neural networks, as they can preset multiple target classes through a single
backdoor injection. This allows attackers to control the model to misclassify
poisoned samples with triggers into any desired target class during inference,
exhibiting superior attack performance compared with conventional backdoor
attacks. However, existing multi-target backdoor attacks fail to guarantee
trigger specificity and stealthiness in black-box settings, resulting in two
main issues. First, they are unable to simultaneously target all classes when
only training data can be manipulated, limiting their effectiveness in
realistic attack scenarios. Second, the triggers often lack visual
imperceptibility, making poisoned samples easy to detect. To address these
problems, we propose a Spatial-based Full-target Invisible Backdoor Attack,
called SFIBA. It restricts triggers for different classes to specific local
spatial regions and morphologies in the pixel space to ensure specificity,
while employing a frequency-domain-based trigger injection method to guarantee
stealthiness. Specifically, for injection of each trigger, we first apply fast
fourier transform to obtain the amplitude spectrum of clean samples in local
spatial regions. Then, we employ discrete wavelet transform to extract the
features from the amplitude spectrum and use singular value decomposition to
integrate the trigger. Subsequently, we selectively filter parts of the trigger
in pixel space to implement trigger morphology constraints and adjust injection
coefficients based on visual effects. We conduct experiments on multiple
datasets and models. The results demonstrate that SFIBA can achieve excellent
attack performance and stealthiness, while preserving the model's performance
on benign samples, and can also bypass existing backdoor defenses.

</details>


### [173] [FFCBA: Feature-based Full-target Clean-label Backdoor Attacks](https://arxiv.org/abs/2504.21054)
*Yangxu Yin,Honglong Chen,Yudong Gao,Peng Sun,Liantao Wu,Zhe Li,Weifeng Liu*

Main category: cs.CR

TL;DR: 该论文提出了一种特征驱动的全目标干净标签后门攻击方法（FFCBA），包含两种范式（FSBA和FMBA），以解决现有多目标攻击的高污染率和易检测性问题，实验证明其攻击效果出色且防御鲁棒性强。


<details>
  <summary>Details</summary>
Motivation: 现有多目标后门攻击多为脏标签范式，污染率高且易被检测，而干净标签攻击则难以实现稳定且高效的多目标攻击。为此，论文旨在设计更隐蔽且高效的多目标干净标签攻击方法。

Method: FFCBA包含两种范式：FSBA（利用类条件自编码器生成对齐原始类别特征的触发噪声）和FMBA（通过两阶段训练生成具有强目标类别特征的触发噪声），分别侧重高效攻击和跨模型攻击能力。

Result: 实验表明，FFCBA在多个数据集和模型上表现优异，攻击成功率高，且能抵御现有先进后门防御方法。

Conclusion: FFCBA通过特征驱动设计，实现了隐蔽、高效且鲁棒的多目标干净标签后门攻击，弥补了现有方法的不足。

Abstract: Backdoor attacks pose a significant threat to deep neural networks, as
backdoored models would misclassify poisoned samples with specific triggers
into target classes while maintaining normal performance on clean samples.
Among these, multi-target backdoor attacks can simultaneously target multiple
classes. However, existing multi-target backdoor attacks all follow the
dirty-label paradigm, where poisoned samples are mislabeled, and most of them
require an extremely high poisoning rate. This makes them easily detectable by
manual inspection. In contrast, clean-label attacks are more stealthy, as they
avoid modifying the labels of poisoned samples. However, they generally
struggle to achieve stable and satisfactory attack performance and often fail
to scale effectively to multi-target attacks. To address this issue, we propose
the Feature-based Full-target Clean-label Backdoor Attacks (FFCBA) which
consists of two paradigms: Feature-Spanning Backdoor Attacks (FSBA) and
Feature-Migrating Backdoor Attacks (FMBA). FSBA leverages class-conditional
autoencoders to generate noise triggers that align perturbed in-class samples
with the original category's features, ensuring the effectiveness, intra-class
consistency, inter-class specificity and natural-feature correlation of
triggers. While FSBA supports swift and efficient attacks, its cross-model
attack capability is relatively weak. FMBA employs a two-stage
class-conditional autoencoder training process that alternates between using
out-of-class samples and in-class samples. This allows FMBA to generate
triggers with strong target-class features, making it highly effective for
cross-model attacks. We conduct experiments on multiple datasets and models,
the results show that FFCBA achieves outstanding attack performance and
maintains desirable robustness against the state-of-the-art backdoor defenses.

</details>


### [174] [A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond Surface-level Privacy Leakage](https://arxiv.org/abs/2504.21035)
*Rui Xin,Niloofar Mireshghallah,Shuyue Stella Li,Michael Duan,Hyunwoo Kim,Yejin Choi,Yulia Tsvetkov,Sewoong Oh,Pang Wei Koh*

Main category: cs.CR

TL;DR: 该研究挑战了传统敏感文本数据的隐私保护方法，提出了一种新框架评估重识别攻击，发现现有方法（如Azure的商业PII移除工具）在真实数据集上效果不佳，差分隐私保护虽然能缓解风险但会降低数据实用性，指出当前方法仅提供‘虚假的隐私保护感’。


<details>
  <summary>Details</summary>
Motivation: 当前敏感文本数据的隐私保护方法（如移除PII或生成合成数据）通常仅评估显式标识符的泄漏，而忽视可能导致重识别的细微文本标记。研究者旨在揭露这种‘隐私保护的错觉’，量化数据发布后的个体隐私风险。

Method: 研究者提出一个新框架，通过评估重识别攻击来量化隐私风险，并展示辅助信息（如日常社交活动）如何用于从已处理数据中推断敏感属性。他们还测试了Azure的PII移除工具效果，并探讨了差分隐私的适用性。

Result: 研究发现，Azure的PII移除工具在MedQA数据集中未能保护74%的信息。虽然差分隐私能部分降低风险，但会显著减少数据在下游任务中的实用性。

Conclusion: 现有隐私保护技术仅提供‘虚假的隐私保护感’，未来需开发更鲁棒的方法以防止语义级信息泄漏。

Abstract: Sanitizing sensitive text data typically involves removing personally
identifiable information (PII) or generating synthetic data under the
assumption that these methods adequately protect privacy; however, their
effectiveness is often only assessed by measuring the leakage of explicit
identifiers but ignoring nuanced textual markers that can lead to
re-identification. We challenge the above illusion of privacy by proposing a
new framework that evaluates re-identification attacks to quantify individual
privacy risks upon data release. Our approach shows that seemingly innocuous
auxiliary information -- such as routine social activities -- can be used to
infer sensitive attributes like age or substance use history from sanitized
data. For instance, we demonstrate that Azure's commercial PII removal tool
fails to protect 74\% of information in the MedQA dataset. Although
differential privacy mitigates these risks to some extent, it significantly
reduces the utility of the sanitized text for downstream tasks. Our findings
indicate that current sanitization techniques offer a \textit{false sense of
privacy}, highlighting the need for more robust methods that protect against
semantic-level information leakage.

</details>


### [175] [Erased but Not Forgotten: How Backdoors Compromise Concept Erasure](https://arxiv.org/abs/2504.21072)
*Jonas Henry Grebe,Tobias Braun,Marcus Rohrbach,Anna Rohrbach*

Main category: cs.CR

TL;DR: 该论文提出了一种新威胁模型ToxE，展示了现有的机器学习遗忘技术如何被后门攻击绕过，揭示当前内容擦除方法的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着文本到图像扩散模型的广泛应用，其可能生成不良或有害内容的风险引起关注。现有遗忘技术试图通过微调擦除不良概念，但存在被攻击绕过的风险。

Method: 论文引入了ToxE威胁模型，通过后门攻击（如文本编码器或交叉注意力层操作）链接触发词与不良内容。并提出了深度介入评分攻击（DISA），优化U-Net以提升攻击持久性。

Result: 实验表明，在名人身份擦除任务中，DISA攻击成功率最高达82%；在不良内容擦除中，攻击可导致身体暴露部分增加9倍，DISA平均提升2.9倍。

Conclusion: 研究揭示了当前遗忘策略的严重安全缺陷，强调需开发更鲁棒的防御机制以应对此类攻击。

Abstract: The expansion of large-scale text-to-image diffusion models has raised
growing concerns about their potential to generate undesirable or harmful
content, ranging from fabricated depictions of public figures to sexually
explicit images. To mitigate these risks, prior work has devised machine
unlearning techniques that attempt to erase unwanted concepts through
fine-tuning. However, in this paper, we introduce a new threat model, Toxic
Erasure (ToxE), and demonstrate how recent unlearning algorithms, including
those explicitly designed for robustness, can be circumvented through targeted
backdoor attacks. The threat is realized by establishing a link between a
trigger and the undesired content. Subsequent unlearning attempts fail to erase
this link, allowing adversaries to produce harmful content. We instantiate ToxE
via two established backdoor attacks: one targeting the text encoder and
another manipulating the cross-attention layers. Further, we introduce Deep
Intervention Score-based Attack (DISA), a novel, deeper backdoor attack that
optimizes the entire U-Net using a score-based objective, improving the
attack's persistence across different erasure methods. We evaluate five recent
concept erasure methods against our threat model. For celebrity identity
erasure, our deep attack circumvents erasure with up to 82% success, averaging
57% across all erasure methods. For explicit content erasure, ToxE attacks can
elicit up to 9 times more exposed body parts, with DISA yielding an average
increase by a factor of 2.9. These results highlight a critical security gap in
current unlearning strategies.

</details>


### [176] [SecRepoBench: Benchmarking LLMs for Secure Code Generation in Real-World Repositories](https://arxiv.org/abs/2504.21205)
*Connor Dilgren,Purva Chiniya,Luke Griffith,Yu Ding,Yizheng Chen*

Main category: cs.CR

TL;DR: SecRepoBench是一个用于评估LLMs在现实代码库中生成安全代码能力的基准测试，包含318个任务，覆盖15种CWE。测试发现，现有LLMs在生成安全代码方面表现不佳，且传统提示工程技术在代码库级别效果有限。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注独立程序生成，而现实中代码库级别的安全代码生成更为复杂且重要，因此需要一个新的基准测试来评估LLMs在这一领域的表现。

Method: 构建SecRepoBench，包含318个C/C++代码库任务，评估19种先进LLMs，并测试提示工程技术和代理技术的效果。

Result: LLMs在生成安全代码方面表现不佳，传统提示工程技术效果有限，SecRepoBench是目前最难的代码安全生成基准测试。

Conclusion: 研究揭示了LLMs在现实代码库中生成安全代码的挑战，并为未来改进提供了方向。

Abstract: This paper introduces SecRepoBench, a benchmark to evaluate LLMs on secure
code generation in real-world repositories. SecRepoBench has 318 code
generation tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 19
state-of-the-art LLMs using our benchmark and find that the models struggle
with generating correct and secure code. In addition, the performance of LLMs
to generate self-contained programs as measured by prior benchmarks do not
translate to comparative performance at generating secure and correct code at
the repository level in SecRepoBench. We show that the state-of-the-art prompt
engineering techniques become less effective when applied to the repository
level secure code generation problem. We conduct extensive experiments,
including an agentic technique to generate secure code, to demonstrate that our
benchmark is currently the most difficult secure coding benchmark, compared to
previous state-of-the-art benchmarks. Finally, our comprehensive analysis
provides insights into potential directions for enhancing the ability of LLMs
to generate correct and secure code in real-world repositories.

</details>


### [177] [Federated One-Shot Learning with Data Privacy and Objective-Hiding](https://arxiv.org/abs/2504.21182)
*Maximilian Egger,Rüdiger Urbanke,Rawad Bitar*

Main category: cs.CR

TL;DR: 该论文提出了一种新颖的联邦学习方法，同时保护客户端数据和联邦目标的隐私，利用知识蒸馏和私有信息检索技术，通过分阶段协议实现信息理论隐私保障。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习研究多关注客户端数据隐私，而对联邦目标隐私的关注不足。本文旨在同时解决这两个问题，填补研究空白。

Method: 采用三阶段协议：阶段0（客户端本地计算）、阶段1（客户端共享结果）、阶段2（联邦者安全获取目标），结合秘密共享的多方计算和图基私有信息检索方案。

Result: 该方法在适配场景下优于现有工具，展示了更强的隐私保障能力。

Conclusion: 通过创新的协议设计，论文成功实现了联邦学习中数据与目标的双向隐私保护，为复杂函数计算提供了可行方案。

Abstract: Privacy in federated learning is crucial, encompassing two key aspects:
safeguarding the privacy of clients' data and maintaining the privacy of the
federator's objective from the clients. While the first aspect has been
extensively studied, the second has received much less attention.
  We present a novel approach that addresses both concerns simultaneously,
drawing inspiration from techniques in knowledge distillation and private
information retrieval to provide strong information-theoretic privacy
guarantees.
  Traditional private function computation methods could be used here; however,
they are typically limited to linear or polynomial functions. To overcome these
constraints, our approach unfolds in three stages. In stage 0, clients perform
the necessary computations locally. In stage 1, these results are shared among
the clients, and in stage 2, the federator retrieves its desired objective
without compromising the privacy of the clients' data. The crux of the method
is a carefully designed protocol that combines secret-sharing-based multi-party
computation and a graph-based private information retrieval scheme. We show
that our method outperforms existing tools from the literature when properly
adapted to this setting.

</details>


### [178] [CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks](https://arxiv.org/abs/2504.21228)
*Rui Wang,Junda Wu,Yu Xia,Tong Yu,Ruiyi Zhang,Ryan Rossi,Lina Yao,Julian McAuley*

Main category: cs.CR

TL;DR: 本文提出CachePrune方法，通过识别并修剪KV缓存中的任务触发神经元，防御LLMs的间接提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: LLMs易受间接提示注入攻击，因其无法区分指令与数据。本文旨在开发更安全的AI系统。

Method: 采用CachePrune，利用特征归因和DPO目标损失函数识别并修剪任务触发神经元。

Result: 实验显示，CachePrune显著降低攻击成功率且不影响响应质量。

Conclusion: CachePrune有效提升LLMs对间接提示注入攻击的防御能力，无需额外调用或修改提示格式。

Abstract: Large Language Models (LLMs) are identified as being susceptible to indirect
prompt injection attack, where the model undesirably deviates from
user-provided instructions by executing tasks injected in the prompt context.
This vulnerability stems from LLMs' inability to distinguish between data and
instructions within a prompt. In this paper, we propose CachePrune that defends
against this attack by identifying and pruning task-triggering neurons from the
KV cache of the input prompt context. By pruning such neurons, we encourage the
LLM to treat the text spans of input prompt context as only pure data, instead
of any indicator of instruction following. These neurons are identified via
feature attribution with a loss function induced from an upperbound of the
Direct Preference Optimization (DPO) objective. We show that such a loss
function enables effective feature attribution with only a few samples. We
further improve on the quality of feature attribution, by exploiting an
observed triggering effect in instruction following. Our approach does not
impose any formatting on the original prompt or introduce extra test-time LLM
calls. Experiments show that CachePrune significantly reduces attack success
rates without compromising the response quality. Note: This paper aims to
defend against indirect prompt injection attacks, with the goal of developing
more secure and robust AI systems.

</details>


### [179] [How to Backdoor the Knowledge Distillation](https://arxiv.org/abs/2504.21323)
*Chen Wu,Qian Ma,Prasenjit Mitra,Sencun Zhu*

Main category: cs.CR

TL;DR: 该论文挑战了知识蒸馏过程安全的传统假设，提出了一种新攻击方法，通过中毒蒸馏数据集来隐秘地破坏学生模型，同时保持教师模型的完整性。


<details>
  <summary>Details</summary>
Motivation: 传统认为知识蒸馏是安全的，因为它使用干净的教师模型输出。本文质疑这一假设，研究如何利用蒸馏过程的漏洞进行攻击。

Method: 提出了一种新颖的攻击方法，通过在蒸馏数据集中嵌入对抗样本和后门触发器，隐秘地感染学生模型。

Result: 通过多种数据集和攻击设置的实验，证明了该方法的鲁棒性、隐秘性和有效性。

Conclusion: 研究揭示了知识蒸馏过程中未识别的漏洞，为未来研究如何防御此类后门攻击提供了方向。

Abstract: Knowledge distillation has become a cornerstone in modern machine learning
systems, celebrated for its ability to transfer knowledge from a large, complex
teacher model to a more efficient student model. Traditionally, this process is
regarded as secure, assuming the teacher model is clean. This belief stems from
conventional backdoor attacks relying on poisoned training data with backdoor
triggers and attacker-chosen labels, which are not involved in the distillation
process. Instead, knowledge distillation uses the outputs of a clean teacher
model to guide the student model, inherently preventing recognition or response
to backdoor triggers as intended by an attacker. In this paper, we challenge
this assumption by introducing a novel attack methodology that strategically
poisons the distillation dataset with adversarial examples embedded with
backdoor triggers. This technique allows for the stealthy compromise of the
student model while maintaining the integrity of the teacher model. Our
innovative approach represents the first successful exploitation of
vulnerabilities within the knowledge distillation process using clean teacher
models. Through extensive experiments conducted across various datasets and
attack settings, we demonstrate the robustness, stealthiness, and effectiveness
of our method. Our findings reveal previously unrecognized vulnerabilities and
pave the way for future research aimed at securing knowledge distillation
processes against backdoor attacks.

</details>


### [180] [Optimizing Mouse Dynamics for User Authentication by Machine Learning: Addressing Data Sufficiency, Accuracy-Practicality Trade-off, and Model Performance Challenges](https://arxiv.org/abs/2504.21415)
*Yi Wang,Chengyv Wu,Yang Liao,Maowei You*

Main category: cs.CR

TL;DR: 本文提出了一种基于鼠标动态的用户认证方法，通过统计模型优化数据量，并设计新框架提升认证准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统用户认证方法在可用性、成本和安全性上存在局限，鼠标动态认证提供了一种低成本、非侵入且适应性强的解决方案。

Method: 使用高斯核密度估计和KL散度确定训练数据量，结合ApEn优化行为表示，并设计LT-AMouse框架整合1D-ResNet和GRU。

Result: 在Balabit和DFL数据集上，数据规模显著降低（DFL减少10倍），盲攻击防御AUC分别达到98.52%和94.65%。

Conclusion: 该方法在减少数据需求和提升认证性能上优于现有技术，为鼠标动态认证提供了实用解决方案。

Abstract: User authentication is essential to ensure secure access to computer systems,
yet traditional methods face limitations in usability, cost, and security.
Mouse dynamics authentication, based on the analysis of users' natural
interaction behaviors with mouse devices, offers a cost-effective,
non-intrusive, and adaptable solution. However, challenges remain in
determining the optimal data volume, balancing accuracy and practicality, and
effectively capturing temporal behavioral patterns. In this study, we propose a
statistical method using Gaussian kernel density estimate (KDE) and
Kullback-Leibler (KL) divergence to estimate the sufficient data volume for
training authentication models. We introduce the Mouse Authentication Unit
(MAU), leveraging Approximate Entropy (ApEn) to optimize segment length for
efficient and accurate behavioral representation. Furthermore, we design the
Local-Time Mouse Authentication (LT-AMouse) framework, integrating 1D-ResNet
for local feature extraction and GRU for modeling long-term temporal
dependencies. Taking the Balabit and DFL datasets as examples, we significantly
reduced the data scale, particularly by a factor of 10 for the DFL dataset,
greatly alleviating the training burden. Additionally, we determined the
optimal input recognition unit length for the user authentication system on
different datasets based on the slope of Approximate Entropy. Training with
imbalanced samples, our model achieved a successful defense AUC 98.52% for
blind attack on the DFL dataset and 94.65% on the Balabit dataset, surpassing
the current sota performance.

</details>


### [181] [Traceback of Poisoning Attacks to Retrieval-Augmented Generation](https://arxiv.org/abs/2504.21668)
*Baolei Zhang,Haoran Xin,Minghong Fang,Zhuqing Liu,Biao Yi,Tong Li,Zheli Liu*

Main category: cs.CR

TL;DR: 论文提出RAGForensics，首个用于RAG系统的溯源工具，通过迭代检索和LLM辅助检测，有效识别知识库中的中毒文本。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统易受中毒攻击，现有防御方法多关注推理时缓解，但效果不足。

Method: RAGForensics通过迭代检索知识库子集，并利用定制提示引导LLM检测潜在中毒文本。

Result: 在多个数据集上验证，RAGForensics能有效对抗最先进的中毒攻击。

Conclusion: RAGForensics为RAG系统安全提供了实用且前景广阔的防御机制。

Abstract: Large language models (LLMs) integrated with retrieval-augmented generation
(RAG) systems improve accuracy by leveraging external knowledge sources.
However, recent research has revealed RAG's susceptibility to poisoning
attacks, where the attacker injects poisoned texts into the knowledge database,
leading to attacker-desired responses. Existing defenses, which predominantly
focus on inference-time mitigation, have proven insufficient against
sophisticated attacks. In this paper, we introduce RAGForensics, the first
traceback system for RAG, designed to identify poisoned texts within the
knowledge database that are responsible for the attacks. RAGForensics operates
iteratively, first retrieving a subset of texts from the database and then
utilizing a specially crafted prompt to guide an LLM in detecting potential
poisoning texts. Empirical evaluations across multiple datasets demonstrate the
effectiveness of RAGForensics against state-of-the-art poisoning attacks. This
work pioneers the traceback of poisoned texts in RAG systems, providing a
practical and promising defense mechanism to enhance their security.

</details>


### [182] [XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs](https://arxiv.org/abs/2504.21700)
*Marco Arazzi,Vignesh Kumar Kembu,Antonino Nocera,Vinod P*

Main category: cs.CR

TL;DR: LLMs的安全威胁阻碍其关键领域应用，现有审查机制易受越狱攻击。本文提出基于可解释AI的分析方法XBreaking，利用独特对齐模式通过定向噪音注入攻击LLMs，效果显著。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs审查机制并设计针对性越狱攻击，以提高对其安全约束的理解并揭示潜在漏洞。

Method: 提出XBreaking方法，通过对比分析审查与未审查模型行为，识别可被利用的对齐模式，并设计定向噪音注入攻击。

Result: 实验验证了XBreaking的有效性与性能，同时深入解析了审查机制的行为特点。

Conclusion: XBreaking成功突破LLMs安全约束，为审查机制改进及防御策略设计提供了重要参考。

Abstract: Large Language Models are fundamental actors in the modern IT landscape
dominated by AI solutions. However, security threats associated with them might
prevent their reliable adoption in critical application scenarios such as
government organizations and medical institutions. For this reason, commercial
LLMs typically undergo a sophisticated censoring mechanism to eliminate any
harmful output they could possibly produce. In response to this, LLM
Jailbreaking is a significant threat to such protections, and many previous
approaches have already demonstrated its effectiveness across diverse domains.
Existing jailbreak proposals mostly adopt a generate-and-test strategy to craft
malicious input. To improve the comprehension of censoring mechanisms and
design a targeted jailbreak attack, we propose an Explainable-AI solution that
comparatively analyzes the behavior of censored and uncensored models to derive
unique exploitable alignment patterns. Then, we propose XBreaking, a novel
jailbreak attack that exploits these unique patterns to break the security
constraints of LLMs by targeted noise injection. Our thorough experimental
campaign returns important insights about the censoring mechanisms and
demonstrates the effectiveness and performance of our attack.

</details>


### [183] [Cert-SSB: Toward Certified Sample-Specific Backdoor Defense](https://arxiv.org/abs/2504.21730)
*Ting Qiao,Yingjia Wang,Xing Liu,Sixing Wu,Jianbing Li,Yiming Li*

Main category: cs.CR

TL;DR: 该论文提出了一种基于随机平滑的样本特定认证后门防御方法Cert-SSB，通过优化每个样本的噪声大小并动态调整认证区域，显著提高了认证防御性能。


<details>
  <summary>Details</summary>
Motivation: 现有的随机平滑防御方法假设所有样本与决策边界距离相同，这在实践中往往不成立，导致防御性能不佳。为应对这一局限，需开发一种样本特定的防御方法。

Method: Cert-SSB通过随机梯度上升优化每个样本的噪声大小，并将其应用于多个中毒训练集以重新训练多个平滑模型，最后聚合这些模型的预测结果生成最终鲁棒预测。

Result: 在多个基准数据集上的实验表明，Cert-SSB显著提升了认证防御性能，成功应对了更高级的后门攻击。

Conclusion: Cert-SSB通过样本特定的噪声优化和动态认证区域调整，克服了现有方法的局限性，为后门防御提供了更可靠的解决方案。

Abstract: Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an
attacker manipulates a small portion of the training data to implant hidden
backdoors into the model. The compromised model behaves normally on clean
samples but misclassifies backdoored samples into the attacker-specified target
class, posing a significant threat to real-world DNN applications. Currently,
several empirical defense methods have been proposed to mitigate backdoor
attacks, but they are often bypassed by more advanced backdoor techniques. In
contrast, certified defenses based on randomized smoothing have shown promise
by adding random noise to training and testing samples to counteract backdoor
attacks. In this paper, we reveal that existing randomized smoothing defenses
implicitly assume that all samples are equidistant from the decision boundary.
However, it may not hold in practice, leading to suboptimal certification
performance. To address this issue, we propose a sample-specific certified
backdoor defense method, termed Cert-SSB. Cert-SSB first employs stochastic
gradient ascent to optimize the noise magnitude for each sample, ensuring a
sample-specific noise level that is then applied to multiple poisoned training
sets to retrain several smoothed models. After that, Cert-SSB aggregates the
predictions of multiple smoothed models to generate the final robust
prediction. In particular, in this case, existing certification methods become
inapplicable since the optimized noise varies across different samples. To
conquer this challenge, we introduce a storage-update-based certification
method, which dynamically adjusts each sample's certification region to improve
certification performance. We conduct extensive experiments on multiple
benchmark datasets, demonstrating the effectiveness of our proposed method. Our
code is available at https://github.com/NcepuQiaoTing/Cert-SSB.

</details>


### [184] [A Comprehensive Study of Exploitable Patterns in Smart Contracts: From Vulnerability to Defense](https://arxiv.org/abs/2504.21480)
*Yuchen Ding,Hongli Peng,Xiaoqi Li*

Main category: cs.CR

TL;DR: 这篇论文分析了以太坊智能合约（尤其是Solidity编写的合约）的安全性风险，重点关注重入和整数溢出漏洞，研究了其机制、攻击场景及防御措施。


<details>
  <summary>Details</summary>
Motivation: 随着智能合约功能日趋复杂，安全问题日益突出，尤其是漏洞可能导致重大经济损失，因此研究以太坊智能合约的安全性风险及其应对措施至关重要。

Method: 论文通过分析漏洞机制、复现攻击场景以及评估防御措施，重点研究了重入和整数溢出两类常见且严重的漏洞。

Result: 研究发现这些漏洞的潜在危害极大，并验证了现有防御措施的有效性。

Conclusion: 论文为智能合约开发者提供了针对重入和整数溢出漏洞的防护指导，有助于提升智能合约的整体安全性。

Abstract: With the rapid advancement of blockchain technology, smart contracts have
enabled the implementation of increasingly complex functionalities. However,
ensuring the security of smart contracts remains a persistent challenge across
the stages of development, compilation, and execution. Vulnerabilities within
smart contracts not only undermine the security of individual applications but
also pose significant risks to the broader blockchain ecosystem, as
demonstrated by the growing frequency of attacks since 2016, resulting in
substantial financial losses. This paper provides a comprehensive analysis of
key security risks in Ethereum smart contracts, specifically those written in
Solidity and executed on the Ethereum Virtual Machine (EVM). We focus on two
prevalent and critical vulnerability types (reentrancy and integer overflow) by
examining their underlying mechanisms, replicating attack scenarios, and
assessing effective countermeasures.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [185] [Galvatron: An Automatic Distributed System for Efficient Foundation Model Training](https://arxiv.org/abs/2504.21411)
*Xinyi Liu,Yujie Wang,Shenhan Zhu,Fangcheng Fu,Qingshuo Liu,Guangming Lin,Bin Cui*

Main category: cs.DC

TL;DR: Galvatron是一个分布式系统，用于高效训练大规模基础模型，通过自动选择最优并行策略，整合多种并行技术和重计算，显著提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 训练大规模基础模型面临并行策略选择和优化的复杂性，Galvatron旨在自动化这一过程，简化分布式训练的复杂性并提升效率。

Method: 系统架构包括硬件和模型分析的性能分析器、基于决策树和动态规划的搜索引擎优化策略，以及高效执行策略的运行时模块。

Result: 在不同集群上的测试表明，Galvatron的吞吐量优于现有框架。

Conclusion: Galvatron通过开源系统、友好接口和完善文档，使复杂的分布式训练变得更易用和高效。

Abstract: Galvatron is a distributed system for efficiently training large-scale
Foundation Models. It overcomes the complexities of selecting optimal
parallelism strategies by automatically identifying the most efficient hybrid
strategy, incorporating data, tensor, pipeline, sharded data, and sequence
parallelism, along with recomputation. The system's architecture includes a
profiler for hardware and model analysis, a search engine for strategy
optimization using decision trees and dynamic programming, and a runtime for
executing these strategies efficiently. Benchmarking on various clusters
demonstrates Galvatron's superior throughput compared to existing frameworks.
This open-source system offers user-friendly interfaces and comprehensive
documentation, making complex distributed training accessible and efficient.
The source code of Galvatron is available at
https://github.com/PKU-DAIR/Hetu-Galvatron.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [186] [On the Potential of Large Language Models to Solve Semantics-Aware Process Mining Tasks](https://arxiv.org/abs/2504.21074)
*Adrian Rebmann,Fabian David Schmidt,Goran Glavaš,Han van der Aa*

Main category: cs.DB

TL;DR: 论文探讨了大型语言模型（LLMs）在语义感知过程挖掘任务中的能力，通过上下文学习和监督微调评估其表现，结果显示微调后的LLMs表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在语义感知过程挖掘任务中的潜力，填补现有研究多关注默认状态下LLMs能力的空白。

Method: 通过上下文学习和监督微调方法，定义了五个需要语义理解的过程挖掘任务，并进行广泛的基准数据集测试。

Result: 实验显示，默认状态或简单上下文示例下LLMs表现不佳，但经过微调后，其对多种过程和行业的任务表现出色。

Conclusion: LLMs在语义感知过程挖掘任务中具有潜力，尤其是通过微调可显著提升其在复杂任务中的表现。

Abstract: Large language models (LLMs) have shown to be valuable tools for tackling
process mining tasks. Existing studies report on their capability to support
various data-driven process analyses and even, to some extent, that they are
able to reason about how processes work. This reasoning ability suggests that
there is potential for LLMs to tackle semantics-aware process mining tasks,
which are tasks that rely on an understanding of the meaning of activities and
their relationships. Examples of these include process discovery, where the
meaning of activities can indicate their dependency, whereas in anomaly
detection the meaning can be used to recognize process behavior that is
abnormal. In this paper, we systematically explore the capabilities of LLMs for
such tasks. Unlike prior work, which largely evaluates LLMs in their default
state, we investigate their utility through both in-context learning and
supervised fine-tuning. Concretely, we define five process mining tasks
requiring semantic understanding and provide extensive benchmarking datasets
for evaluation. Our experiments reveal that while LLMs struggle with
challenging process mining tasks when used out of the box or with minimal
in-context examples, they achieve strong performance when fine-tuned for these
tasks across a broad range of process types and industries.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [187] [Turning Up the Heat: Assessing 2-m Temperature Forecast Errors in AI Weather Prediction Models During Heat Waves](https://arxiv.org/abs/2504.21195)
*Kelsey E. Ennis,Elizabeth A. Barnes,Marybeth C. Arcodia,Martin A. Fernandez,Eric D. Maloney*

Main category: physics.ao-ph

TL;DR: 该论文探讨了人工智能天气预测模型（AIWP）在极端高温预测中的表现，与传统数值天气预测模型（NWP）相比，Google GraphCast在多数情况下表现更优，但模型在预测中普遍存在冷偏差。


<details>
  <summary>Details</summary>
Motivation: 极端高温是美国最致命的天气相关灾害，且频率和强度不断增加。传统NWP模型在中长期和次季节至季节（S2S）预测中表现不佳，而AIWP模型的性能尚不明确，因此需要研究其在极端高温预测中的表现。

Method: 研究对比了两种AIWP模型（Google GraphCast和Pangu-Weather）和一种传统NWP模型（NOAA UFS GEFS），分析了60次极端高温事件在四个季节和四个美国地区的预测表现，时间跨度达20天。

Result: 结果显示，AIWP和NWP模型在高温事件发生前5到10天均表现出区域性冷偏差。GraphCast在多数情况下表现最优，超越了UFS GEFS和Pangu-Weather。除冬季的Pangu-Weather在高温前表现暖偏差外，其他模型和季节均存在冷偏差。

Conclusion: 研究表明，AIWP模型可能在中长期和S2S极端高温预测中具有实用价值，尽管存在偏差，但表现优于传统模型。

Abstract: Extreme heat is the deadliest weather-related hazard in the United States.
Furthermore, it is increasing in intensity, frequency, and duration, making
skillful forecasts vital to protecting life and property. Traditional numerical
weather prediction (NWP) models struggle with extreme heat for medium-range and
subseasonal-to-seasonal (S2S) timescales. Meanwhile, artificial
intelligence-based weather prediction (AIWP) models are progressing rapidly.
However, it is largely unknown how well AIWP models forecast extremes,
especially for medium-range and S2S timescales. This study investigates 2-m
temperature forecasts for 60 heat waves across the four boreal seasons and over
four CONUS regions at lead times up to 20 days, using two AIWP models (Google
GraphCast and Pangu-Weather) and one traditional NWP model (NOAA United
Forecast System Global Ensemble Forecast System (UFS GEFS)). First, case study
analyses show that both AIWP models and the UFS GEFS exhibit consistent cold
biases on regional scales in the 5-10 days of lead time before heat wave onset.
GraphCast is the more skillful AIWP model, outperforming UFS GEFS and
Pangu-Weather in most locations. Next, the two AIWP models are isolated and
analyzed across all heat waves and seasons, with events split among the model's
testing (2018-2023) and training (1979-2017) periods. There are cold biases
before and during the heat waves in both models and all seasons, except
Pangu-Weather in winter, which exhibits a mean warm bias before heat wave
onset. Overall, results offer encouragement that AIWP models may be useful for
medium-range and S2S predictability of extreme heat.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [188] [Redundancy Analysis and Mitigation for Machine Learning-Based Process Monitoring of Additive Manufacturing](https://arxiv.org/abs/2504.21317)
*Jiarui Xie,Yaoyao Fiona Zhao*

Main category: cs.CE

TL;DR: 论文提出了一种多级冗余缓解（MLRM）框架，用于解决基于机器学习的增材制造过程监控系统中的冗余问题，显著降低了延迟、错误率和存储需求。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对冗余的统一定义和系统性评估与缓解方法，冗余问题导致设备成本增加、模型性能下降和高计算需求。

Method: 定义了样本级、特征级和模型级冗余，并提出包含数据注册、降维、跨模态知识迁移和模型剪枝等方法的MLRM框架。

Result: 在DED缺陷检测案例中，框架实现了延迟降低91%、错误率下降47%和存储需求减少99.4%，同时还降低了传感器成本和能耗。

Conclusion: 通过定义冗余并引入结构化缓解框架，该研究为生产环境中基于机器学习的监控系统提供了高效、轻量且可扩展的解决方案。

Abstract: The deployment of machine learning (ML)-based process monitoring systems has
significantly advanced additive manufacturing (AM) by enabling real-time defect
detection, quality assessment, and process optimization. However, redundancy is
a critical yet often overlooked challenge in the deployment and operation of
ML-based AM process monitoring systems. Excessive redundancy leads to increased
equipment costs, compromised model performance, and high computational
requirements, posing barriers to industrial adoption. However, existing
research lacks a unified definition of redundancy and a systematic framework
for its evaluation and mitigation. This paper defines redundancy in ML-based AM
process monitoring and categorizes it into sample-level, feature-level, and
model-level redundancy. A comprehensive multi-level redundancy mitigation
(MLRM) framework is proposed, incorporating advanced methods such as data
registration, downscaling, cross-modality knowledge transfer, and model pruning
to systematically reduce redundancy while improving model performance. The
framework is validated through an ML-based in-situ defect detection case study
for directed energy deposition (DED), demonstrating a 91% reduction in latency,
a 47% decrease in error rate, and a 99.4% reduction in storage requirements.
Additionally, the proposed approach lowers sensor costs and energy consumption,
enabling a lightweight, cost-effective, and scalable monitoring system. By
defining redundancy and introducing a structured mitigation framework, this
study establishes redundancy analysis and mitigation as a key enabler of
efficient ML-based process monitoring in production environments.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [189] [A Memetic Algorithm based on Variational Autoencoder for Black-Box Discrete Optimization with Epistasis among Parameters](https://arxiv.org/abs/2504.21338)
*Aoi Kato,Kenta Kojima,Masahiro Nomura,Isao Ono*

Main category: cs.NE

TL;DR: 基于VAE的采样与局部搜索相结合的新算法在解决高维BB-DO问题中表现优异。


<details>
  <summary>Details</summary>
Motivation: 针对BB-DO问题中参数间相互作用的挑战，结合VAE和局部搜索的优势，提出高效且计算成本可控的解决方案。

Method: 提出了一种结合VAE采样和局部搜索的混合算法，融合了VAE-EDA和局部搜索方法的优点。

Result: 在NK景观实验中，新算法优于现有的VAE-EDA方法和领先的P3、DSMGA-II等算法。

Conclusion: 该方法有效解决了高维BB-DO问题中的参数相互作用问题，计算效率高，性能显著。

Abstract: Black-box discrete optimization (BB-DO) problems arise in many real-world
applications, such as neural architecture search and mathematical model
estimation. A key challenge in BB-DO is epistasis among parameters where
multiple variables must be modified simultaneously to effectively improve the
objective function. Estimation of Distribution Algorithms (EDAs) provide a
powerful framework for tackling BB-DO problems. In particular, an EDA
leveraging a Variational Autoencoder (VAE) has demonstrated strong performance
on relatively low-dimensional problems with epistasis while reducing
computational cost. Meanwhile, evolutionary algorithms such as DSMGA-II and P3,
which integrate bit-flip-based local search with linkage learning, have shown
excellent performance on high-dimensional problems. In this study, we propose a
new memetic algorithm that combines VAE-based sampling with local search. The
proposed method inherits the strengths of both VAE-based EDAs and local
search-based approaches: it effectively handles high-dimensional problems with
epistasis among parameters without incurring excessive computational overhead.
Experiments on NK landscapes -- a challenging benchmark for BB-DO involving
epistasis among parameters -- demonstrate that our method outperforms
state-of-the-art VAE-based EDA methods, as well as leading approaches such as
P3 and DSMGA-II.

</details>


### [190] [Meta knowledge assisted Evolutionary Neural Architecture Search](https://arxiv.org/abs/2504.21545)
*Yangyang Li,Guanlong Liu,Ronghua Shang,Licheng Jiao*

Main category: cs.NE

TL;DR: 该论文提出了一种基于进化计算的高效神经架构搜索方法，通过创新的元学习框架解决高计算成本和固定学习率问题，实验证明其性能优越且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 现有基于进化计算的神经架构搜索方法因高计算成本和固定学习率导致信息损失，需要更高效且灵活的方法。

Method: 1. 使用元学习率方案预训练获得适合的学习率安排；2. 设计自适应代理模型筛选潜在架构；3. 提出周期性变异算子增加种群多样性。

Result: 在CIFAR-10、CIFAR-100和ImageNet1K数据集上，该方法性能与现有最优方法相当，但计算成本更低且更鲁棒。

Conclusion: 该方法通过元学习和自适应策略显著提升了神经架构搜索的效率和性能，为自动设计神经网络提供了新思路。

Abstract: Evolutionary computation (EC)-based neural architecture search (NAS) has
achieved remarkable performance in the automatic design of neural
architectures. However, the high computational cost associated with evaluating
searched architectures poses a challenge for these methods, and a fixed form of
learning rate (LR) schedule means greater information loss on diverse searched
architectures. This paper introduces an efficient EC-based NAS method to solve
these problems via an innovative meta-learning framework. Specifically, a
meta-learning-rate (Meta-LR) scheme is used through pretraining to obtain a
suitable LR schedule, which guides the training process with lower information
loss when evaluating each individual. An adaptive surrogate model is designed
through an adaptive threshold to select the potential architectures in a few
epochs and then evaluate the potential architectures with complete epochs.
Additionally, a periodic mutation operator is proposed to increase the
diversity of the population, which enhances the generalizability and
robustness. Experiments on CIFAR-10, CIFAR-100, and ImageNet1K datasets
demonstrate that the proposed method achieves high performance comparable to
that of many state-of-the-art peer methods, with lower computational cost and
greater robustness.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [191] [Participatory AI, Public Sector AI, Differential Privacy, Conversational Interfaces, Explainable AI, Citizen Engagement in AI](https://arxiv.org/abs/2504.21297)
*Wenjun Yang,Eyhab Al-Masri*

Main category: cs.IT

TL;DR: 论文介绍了一种对话式界面系统，用于公共领域差分隐私AI系统的参与式设计，通过TOPSIS多标准决策、实时MAE可视化和动态隐私预算调节，平衡隐私保护与民主问责。


<details>
  <summary>Details</summary>
Motivation: 解决如何在公共领域的AI系统中平衡数学隐私保证与民主问责的挑战，以增强公众参与和合规性。

Method: 提出三点贡献：(1)基于TOPSIS的自适应$ε选择协议，(2)可解释的噪声注入框架，含MAE可视化和GPT-4分析，(3)动态调节隐私预算的合规机制。

Result: 成果表明对话式界面能提升公众参与，确保隐私保护AI既数学严谨又民主问责。

Conclusion: 该研究推动了参与式AI实践，为公共治理中的隐私保护AI提供了新方法。

Abstract: This paper introduces a conversational interface system that enables
participatory design of differentially private AI systems in public sector
applications. Addressing the challenge of balancing mathematical privacy
guarantees with democratic accountability, we propose three key contributions:
(1) an adaptive $\epsilon$-selection protocol leveraging TOPSIS multi-criteria
decision analysis to align citizen preferences with differential privacy (DP)
parameters, (2) an explainable noise-injection framework featuring real-time
Mean Absolute Error (MAE) visualizations and GPT-4-powered impact analysis, and
(3) an integrated legal-compliance mechanism that dynamically modulates privacy
budgets based on evolving regulatory constraints. Our results advance
participatory AI practices by demonstrating how conversational interfaces can
enhance public engagement in algorithmic privacy mechanisms, ensuring that
privacy-preserving AI in public sector governance remains both mathematically
robust and democratically accountable.

</details>


### [192] [Sionna RT: Technical Report](https://arxiv.org/abs/2504.21719)
*Fayçal Aït Aoudia,Jakob Hoydis,Merlin Nimier-David,Sebastian Cammerer,Alexander Keller*

Main category: cs.IT

TL;DR: Sionna是一个开源、GPU加速的库，其0.14版本引入了可微分的光线追踪技术，用于模拟无线电波传播。Sionna 1.0对光线追踪器进行了全面升级，提升了速度、内存效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 为了解决无线电波传播模拟中的梯度计算问题，并提升光线追踪的性能和效率。

Method: 使用可微分的光线追踪技术，结合SBR（发射和反弹射线）和图像方法计算信道冲激响应（CIR），并通过哈希机制消除重复路径；无线电地图则采用纯SBR方法。

Result: Sionna RT能够高效计算CIR和无线电地图的梯度，同时显著提升了光线追踪的速度和内存效率。

Conclusion: Sionna RT为无线电波传播模拟提供了高效、可扩展的解决方案，但仍存在一些局限性需要进一步研究。

Abstract: Sionna is an open-source, GPU-accelerated library that, as of version 0.14,
incorporates a ray tracer for simulating radio wave propagation. A unique
feature of Sionna RT is differentiability, enabling the calculation of
gradients for the channel impulse responses (CIRs), radio maps, and other
related metrics with respect to system and environmental parameters, such as
material properties, antenna patterns, and array geometries. The release of
Sionna 1.0 provides a complete overhaul of the ray tracer, significantly
improving its speed, memory efficiency, and extensibility. This document
details the algorithms employed by Sionna RT to simulate radio wave propagation
efficiently, while also addressing their current limitations. Given that the
computation of CIRs and radio maps requires distinct algorithms, these are
detailed in separate sections. For CIRs, Sionna RT integrates shooting and
bouncing of rays (SBR) with the image method and uses a hashing-based mechanism
to efficiently eliminate duplicate paths. Radio maps are computed using a
purely SBR-based approach.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [193] [Selecting the Right LLM for eGov Explanations](https://arxiv.org/abs/2504.21032)
*Lior Limonad,Fabiana Fournier,Hadar Mulian,George Manias,Spiros Borotis,Danai Kyrkou*

Main category: cs.CY

TL;DR: 论文提出了一个系统方法，通过调整现有量表评估不同大语言模型生成解释的质量，并以税务返还为例验证其适用性，同时探索自动化评估的可能性。


<details>
  <summary>Details</summary>
Motivation: 电子政务服务中的解释质量对增强用户信任和提升服务使用率至关重要。如何选择合适的大语言模型生成高质量解释成为难题，需要一个系统性评估方法。

Method: 调整既有量表用于分析不同大语言模型生成解释的质量，通过税务返还案例进行实证研究，收集128名用户反馈，并尝试用预测技术自动化评估过程。

Result: 研究展示了量表在评估LLM生成解释质量中的实用性，并为选择最适合的模型提供了方法基础。

Conclusion: 该工作为电子政务服务提供商选择LLM生成解释提供了实用工具，同时探索了评估过程的自动化潜力。

Abstract: The perceived quality of the explanations accompanying e-government services
is key to gaining trust in these institutions, consequently amplifying further
usage of these services. Recent advances in generative AI, and concretely in
Large Language Models (LLMs) allow the automation of such content
articulations, eliciting explanations' interpretability and fidelity, and more
generally, adapting content to various audiences. However, selecting the right
LLM type for this has become a non-trivial task for e-government service
providers. In this work, we adapted a previously developed scale to assist with
this selection, providing a systematic approach for the comparative analysis of
the perceived quality of explanations generated by various LLMs. We further
demonstrated its applicability through the tax-return process, using it as an
exemplar use case that could benefit from employing an LLM to generate
explanations about tax refund decisions. This was attained through a user study
with 128 survey respondents who were asked to rate different versions of
LLM-generated explanations about tax refund decisions, providing a
methodological basis for selecting the most appropriate LLM. Recognizing the
practical challenges of conducting such a survey, we also began exploring the
automation of this process by attempting to replicate human feedback using a
selection of cutting-edge predictive techniques.

</details>


### [194] [AI Supply Chains: An Emerging Ecosystem of AI Actors, Products, and Services](https://arxiv.org/abs/2504.20185)
*Aspen Hopkins,Sarah H. Cen,Andrew Ilyas,Isabella Struckman,Luis Videgaray,Aleksander Mądry*

Main category: cs.CY

TL;DR: 该研究首次对AI供应链及其影响进行了形式化研究，通过两个案例表明AI开发和监管因供应链的存在而复杂化，并呼吁进一步研究其社会、经济、监管和技术影响。


<details>
  <summary>Details</summary>
Motivation: AI供应链的广泛应用及其复杂性尚缺乏深入研究，研究者希望填补这一空白。

Method: 通过历史视角分析AI供应链的发展，并将其建模为有向图，再通过两个案例研究（信息传递和上游设计选择的影响）进行理论和实证分析。

Result: 研究发现AI供应链中的信息传递不完善可能导致误解，且上游设计选择对下游AI产品有显著影响。

Conclusion: AI供应链的研究具有重要社会和经济意义，未来需进一步探索其多维度影响。

Abstract: The widespread adoption of AI in recent years has led to the emergence of AI
supply chains: complex networks of AI actors contributing models, datasets, and
more to the development of AI products and services. AI supply chains have many
implications yet are poorly understood. In this work, we take a first step
toward a formal study of AI supply chains and their implications, providing two
illustrative case studies indicating that both AI development and regulation
are complicated in the presence of supply chains. We begin by presenting a
brief historical perspective on AI supply chains, discussing how their rise
reflects a longstanding shift towards specialization and outsourcing that
signals the healthy growth of the AI industry. We then model AI supply chains
as directed graphs and demonstrate the power of this abstraction by connecting
examples of AI issues to graph properties. Finally, we examine two case studies
in detail, providing theoretical and empirical results in both. In the first,
we show that information passing (specifically, of explanations) along the AI
supply chains is imperfect, which can result in misunderstandings that have
real-world implications. In the second, we show that upstream design choices
(e.g., by base model providers) have downstream consequences (e.g., on AI
products fine-tuned on the base model). Together, our findings motivate further
study of AI supply chains and their increasingly salient social, economic,
regulatory, and technical implications.

</details>


### [195] [LSTM+Geo with xgBoost Filtering: A Novel Approach for Race and Ethnicity Imputation with Reduced Bias](https://arxiv.org/abs/2504.21259)
*S. Chalavadi,A. Pastor,T. Leitch*

Main category: cs.CY

TL;DR: LSTM+Geo，一种结合地理位置信息改善LSTM的方法，在种族和民族分类任务中优于传统方法，减少了非白人误分类为白人的情况，并可作为高级系统的模块进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 准确分类种族和民族对分析不平等和制定政策至关重要。传统方法如BISG存在系统性分类偏差，因此需要改进。

Method: LSTM+Geo，即通过加入人口普查区域的地理信息增强LSTM模型，并在大型选民数据集上进行验证。

Result: LSTM+Geo准确率达88.7%，显著优于LSTM（86.4%）、BISG（82.9%）和BIFSG（86.8%），且非白人误分类比例更低。与XGBoost结合可进一步提升性能。

Conclusion: LSTM+Geo是一种性能优越且偏见更少的模型，适合单独使用或与其他方法结合。

Abstract: Accurate imputation of race and ethnicity (R&E) is crucial for analyzing
disparities and informing policy. Methods like Bayesian Improved Surname
Geocoding (BISG) are widely used but exhibit limitations, including systematic
misclassification biases linked to socioeconomic status. This paper introduces
LSTM+Geo, a novel approach enhancing Long Short-Term Memory (LSTM) networks
with census tract geolocation information. Using a large voter dataset, we
demonstrate that LSTM+Geo (88.7% accuracy) significantly outperforms standalone
LSTM (86.4%) and Bayesian methods like BISG (82.9%) and BIFSG (86.8%) in
accuracy and F1-score on a held-out validation set. LSTM+Geo reduces the rate
at which non-White individuals are misclassified as White (White FPR 19.3%)
compared to name-only LSTMs (White FPR 24.6%). While sophisticated ensemble
methods incorporating XGBoost achieve the highest overall accuracy (up to
89.4%) and lowest White FPR (17.8%), LSTM+Geo offers strong standalone
performance with improved bias characteristics compared to baseline models.
Integrating LSTM+Geo into an XGBoost ensemble further boosts accuracy,
highlighting its utility as both a standalone model and a component for
advanced systems. We give a caution at the end regarding the appropriate use of
these methods.

</details>


### [196] [Quantitative Auditing of AI Fairness with Differentially Private Synthetic Data](https://arxiv.org/abs/2504.21634)
*Chih-Cheng Rex Yuan,Bow-Yaw Wang*

Main category: cs.CY

TL;DR: 论文提出了一种利用差分隐私合成数据审核AI系统公平性的框架，解决了传统审核中的安全和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 传统AI公平性审核使用真实数据会引发安全和隐私风险，需要一种既能保护隐私又能有效评估公平性的方法。

Method: 通过差分隐私机制生成合成数据，保留原始数据的统计特性，同时比较合成与真实数据的公平性指标。

Result: 实验表明，合成数据能有效保留真实数据的公平性属性，同时保护敏感信息。

Conclusion: 该框架在关键和敏感领域中既能实现公平性审核，又能保障隐私安全。

Abstract: Fairness auditing of AI systems can identify and quantify biases. However,
traditional auditing using real-world data raises security and privacy
concerns. It exposes auditors to security risks as they become custodians of
sensitive information and targets for cyberattacks. Privacy risks arise even
without direct breaches, as data analyses can inadvertently expose confidential
information. To address these, we propose a framework that leverages
differentially private synthetic data to audit the fairness of AI systems. By
applying privacy-preserving mechanisms, it generates synthetic data that
mirrors the statistical properties of the original dataset while ensuring
privacy. This method balances the goal of rigorous fairness auditing and the
need for strong privacy protections. Through experiments on real datasets like
Adult, COMPAS, and Diabetes, we compare fairness metrics of synthetic and real
data. By analyzing the alignment and discrepancies between these metrics, we
assess the capacity of synthetic data to preserve the fairness properties of
real data. Our results demonstrate the framework's ability to enable meaningful
fairness evaluations while safeguarding sensitive information, proving its
applicability across critical and sensitive domains.

</details>


### [197] [TRIED: Truly Innovative and Effective Detection Benchmark, developed by WITNESS](https://arxiv.org/abs/2504.21489)
*Shirin Anlen,Zuzanna Wojciak*

Main category: cs.CY

TL;DR: 报告提出了TRIED基准，用于评估AI检测工具在现实世界中的效果和创新能力，强调工具需适应多元化语境以应对生成式AI和虚假合成媒体的威胁。


<details>
  <summary>Details</summary>
Motivation: 生成式AI和虚假合成媒体的兴起对全球信息生态系统构成威胁，现有AI检测工具在现实场景中表现不足，急需改进。

Method: 通过前线经验、虚假AI案例和全球调研，WITNESS开发了TRIED基准，评估工具的创新性和实际影响。

Result: 报告为开发者、政策制定者等提供了实用指导，强调工具需透明、可问责且以用户为中心，并融入社会技术考量。

Conclusion: 采用TRIED基准可推动创新、增强公众信任和AI素养，提升全球信息可信度。

Abstract: The rise of generative AI and deceptive synthetic media threatens the global
information ecosystem, especially across the Global Majority. This report from
WITNESS highlights the limitations of current AI detection tools, which often
underperform in real-world scenarios due to challenges related to
explainability, fairness, accessibility, and contextual relevance. In response,
WITNESS introduces the Truly Innovative and Effective AI Detection (TRIED)
Benchmark, a new framework for evaluating detection tools based on their
real-world impact and capacity for innovation. Drawing on frontline
experiences, deceptive AI cases, and global consultations, the report outlines
how detection tools must evolve to become truly innovative and relevant by
meeting diverse linguistic, cultural, and technological contexts. It offers
practical guidance for developers, policymakers, and standards bodies to design
accountable, transparent, and user-centered detection solutions, and
incorporate sociotechnical considerations into future AI standards, procedures
and evaluation frameworks. By adopting the TRIED Benchmark, stakeholders can
drive innovation, safeguard public trust, strengthen AI literacy, and
contribute to a more resilient global information credibility.

</details>


### [198] [Characterizing AI Agents for Alignment and Governance](https://arxiv.org/abs/2504.21848)
*Atoosa Kasirzadeh,Iason Gabriel*

Main category: cs.CY

TL;DR: 论文提出AI代理的四个核心维度（自主性、效能、目标复杂性和通用性），并构建'代理档案'以帮助治理不同类别的AI代理。


<details>
  <summary>Details</summary>
Motivation: 理解AI代理的核心属性及其与治理问题的关系，以开发更有效的治理机制。

Method: 提出四个维度的AI代理分类框架，并构建不同代理的'代理档案'。

Result: 框架揭示了不同类别AI代理的治理挑战，为开发者和政策制定者提供了指导。

Conclusion: 该框架有助于设计更符合社会集体目标的AI治理方案。

Abstract: The creation of effective governance mechanisms for AI agents requires a
deeper understanding of their core properties and how these properties relate
to questions surrounding the deployment and operation of agents in the world.
This paper provides a characterization of AI agents that focuses on four
dimensions: autonomy, efficacy, goal complexity, and generality. We propose
different gradations for each dimension, and argue that each dimension raises
unique questions about the design, operation, and governance of these systems.
Moreover, we draw upon this framework to construct "agentic profiles" for
different kinds of AI agents. These profiles help to illuminate cross-cutting
technical and non-technical governance challenges posed by different classes of
AI agents, ranging from narrow task-specific assistants to highly autonomous
general-purpose systems. By mapping out key axes of variation and continuity,
this framework provides developers, policymakers, and members of the public
with the opportunity to develop governance approaches that better align with
collective societal goals.

</details>


### [199] [Public Opinion and The Rise of Digital Minds: Perceived Risk, Trust, and Regulation Support](https://arxiv.org/abs/2504.21849)
*Justin B. Bullock,Janet V. T. Pauketat,Hsini Huang,Yi-Fan Wang,Jacy Reese Anthis*

Main category: cs.CY

TL;DR: 研究探讨公众对AI监管的偏好，发现风险感知和政府信任度高的人更支持监管，而信任AI公司和技术的人则倾向于反对限制。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的社会风险增加，研究公众信任和风险感知如何影响AI监管偏好，为政策制定提供依据。

Method: 使用2023年AIMS全国代表性调查数据，分析公众对政府、AI公司及技术的信任度与监管支持的关系。

Result: 公众普遍支持AI监管，风险感知和政府信任是支持监管的关键因素，而AI公司和技术信任者则反对限制。

Conclusion: 政策制定需平衡公众风险担忧与机构信任，研究为AI治理提供了实证基础，并呼吁进一步探索信任、风险与监管策略。

Abstract: Governance institutions must respond to societal risks, including those posed
by generative AI. This study empirically examines how public trust in
institutions and AI technologies, along with perceived risks, shape preferences
for AI regulation. Using the nationally representative 2023 Artificial
Intelligence, Morality, and Sentience (AIMS) survey, we assess trust in
government, AI companies, and AI technologies, as well as public support for
regulatory measures such as slowing AI development or outright bans on advanced
AI. Our findings reveal broad public support for AI regulation, with risk
perception playing a significant role in shaping policy preferences.
Individuals with higher trust in government favor regulation, while those with
greater trust in AI companies and AI technologies are less inclined to support
restrictions. Trust in government and perceived risks significantly predict
preferences for both soft (e.g., slowing development) and strong (e.g., banning
AI systems) regulatory interventions. These results highlight the importance of
public opinion in AI governance. As AI capabilities advance, effective
regulation will require balancing public concerns about risks with trust in
institutions. This study provides a foundational empirical baseline for
policymakers navigating AI governance and underscores the need for further
research into public trust, risk perception, and regulatory strategies in the
evolving AI landscape.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [200] [Solving Copyright Infringement on Short Video Platforms: Novel Datasets and an Audio Restoration Deep Learning Pipeline](https://arxiv.org/abs/2504.21772)
*Minwoo Oh,Minsu Park,Eunil Park*

Main category: cs.MM

TL;DR: 提出的新流程结合音乐源分离和跨模态视频-音乐检索，有效分离短视频中的背景音乐和原声，修复音频完整性。


<details>
  <summary>Details</summary>
Motivation: 短视频平台存在版权问题，用户嵌入背景音乐以掩盖原声逃避检测，需解决方案。

Method: 使用音乐源分离和跨模态视频-音乐检索技术构建流程。

Result: 实验证明，流程能高精度移除背景音乐并修复原声，维护内容完整性。

Conclusion: 该方案为短视频平台提供了道德且可扩展的版权问题解决方案。

Abstract: Short video platforms like YouTube Shorts and TikTok face significant
copyright compliance challenges, as infringers frequently embed arbitrary
background music (BGM) to obscure original soundtracks (OST) and evade content
originality detection. To tackle this issue, we propose a novel pipeline that
integrates Music Source Separation (MSS) and cross-modal video-music retrieval
(CMVMR). Our approach effectively separates arbitrary BGM from the original
OST, enabling the restoration of authentic video audio tracks. To support this
work, we introduce two domain-specific datasets: OASD-20K for audio separation
and OSVAR-160 for pipeline evaluation. OASD-20K contains 20,000 audio clips
featuring mixed BGM and OST pairs, while OSVAR160 is a unique benchmark dataset
comprising 1,121 video and mixed-audio pairs, specifically designed for short
video restoration tasks. Experimental results demonstrate that our pipeline not
only removes arbitrary BGM with high accuracy but also restores OSTs, ensuring
content integrity. This approach provides an ethical and scalable solution to
copyright challenges in user-generated content on short video platforms.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [201] [Advancing Multi-Agent Systems Through Model Context Protocol: Architecture, Implementation, and Applications](https://arxiv.org/abs/2504.21030)
*Naveen Krishnan*

Main category: cs.MA

TL;DR: 该论文提出了一种名为模型上下文协议（MCP）的框架，通过标准化上下文共享和协调机制，解决了多智能体系统在上下文管理、协调效率和可扩展性方面的挑战，并在多个应用领域展示了性能提升。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统面临上下文管理、协调效率和可扩展性的根本挑战，亟需一种统一的方法来提升其协作能力和实用性。

Method: 开发了MCP框架，包括统一的理论基础、高级上下文管理技术和可扩展的协调模式，并通过案例研究验证其有效性。

Result: 在知识管理、协作研究和分布式问题解决等领域的案例研究中，MCP框架展现出相比传统方法的显著性能提升。

Conclusion: MCP框架为构建更具协作性和上下文感知能力的人工智能系统提供了基础，并指出了未来的研究方向和应用潜力。

Abstract: Multi-agent systems represent a significant advancement in artificial
intelligence, enabling complex problem-solving through coordinated specialized
agents. However, these systems face fundamental challenges in context
management, coordination efficiency, and scalable operation. This paper
introduces a comprehensive framework for advancing multi-agent systems through
Model Context Protocol (MCP), addressing these challenges through standardized
context sharing and coordination mechanisms. We extend previous work on AI
agent architectures by developing a unified theoretical foundation, advanced
context management techniques, and scalable coordination patterns. Through
detailed implementation case studies across enterprise knowledge management,
collaborative research, and distributed problem-solving domains, we demonstrate
significant performance improvements compared to traditional approaches. Our
evaluation methodology provides a systematic assessment framework with
benchmark tasks and datasets specifically designed for multi-agent systems. We
identify current limitations, emerging research opportunities, and potential
transformative applications across industries. This work contributes to the
evolution of more capable, collaborative, and context-aware artificial
intelligence systems that can effectively address complex real-world
challenges.

</details>


### [202] [Multi-Agent Reinforcement Learning for Resources Allocation Optimization: A Survey](https://arxiv.org/abs/2504.21048)
*Mohamad A. Hady,Siyi Hu,Mahardhika Pratama,Jimmy Cao,Ryszard Kowalczyk*

Main category: cs.MA

TL;DR: 本文综述了多智能体强化学习（MARL）在资源分配优化（RAO）中的应用，总结了核心概念、分类和结构化分类法，旨在帮助研究者和实践者利用MARL的潜力推动资源分配解决方案的发展。


<details>
  <summary>Details</summary>
Motivation: 资源分配优化在动态和去中心化环境中的挑战日益增多，MARL因其在分布式决策和复杂环境交互中的优势，成为解决RAO问题的有力工具。本文旨在通过综述现有研究，推动RAO领域的进一步发展。

Method: 本文采用文献综述的方法，系统回顾了近年来的MARL算法在RAO中的应用，并构建了核心概念、分类和结构化分类法。

Result: 综述提供了MARL在RAO中的全面视角，总结了当前研究现状，并指出了主要挑战和未来方向。

Conclusion: MARL在RAO中展现出巨大潜力，但仍面临一些挑战。本文为研究者和实践者提供了宝贵的参考，以推动该领域的未来发展。

Abstract: Multi-Agent Reinforcement Learning (MARL) has become a powerful framework for
numerous real-world applications, modeling distributed decision-making and
learning from interactions with complex environments. Resource Allocation
Optimization (RAO) benefits significantly from MARL's ability to tackle dynamic
and decentralized contexts. MARL-based approaches are increasingly applied to
RAO challenges across sectors playing pivotal roles to Industry 4.0
developments. This survey provides a comprehensive review of recent MARL
algorithms for RAO, encompassing core concepts, classifications, and a
structured taxonomy. By outlining the current research landscape and
identifying primary challenges and future directions, this survey aims to
support researchers and practitioners in leveraging MARL's potential to advance
resource allocation solutions.

</details>


### [203] [MF-LLM: Simulating Collective Decision Dynamics via a Mean-Field Large Language Model Framework](https://arxiv.org/abs/2504.21582)
*Qirui Mi,Mengyue Yang,Xiangning Yu,Zhiyu Zhao,Cheng Deng,Bo An,Haifeng Zhang,Xu Chen,Jun Wang*

Main category: cs.MA

TL;DR: 提出了MF-LLM框架，通过结合微观决策与宏观群体的反馈循环模拟集体决策，并使用IB-Tune方法优化模型匹配真实数据，显著提升了社会模拟的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在社会模拟中与现实数据存在偏差，需要一种更准确模拟集体决策动态的方法。

Method: MF-LLM框架通过策略模型和平均场模型交替更新微观与宏观行为，并使用基于信息瓶颈原则的IB-Tune方法优化模型。

Result: MF-LLM在真实数据集上相比基线降低了47%的KL散度，并能准确预测趋势与规划干预，在多个领域和模型上具有泛化性。

Conclusion: MF-LLM为高保真社会模拟提供了可扩展的基础，显著提升了模拟的准确性与实用性。

Abstract: Simulating collective decision-making involves more than aggregating
individual behaviors; it arises from dynamic interactions among individuals.
While large language models (LLMs) show promise for social simulation, existing
approaches often exhibit deviations from real-world data. To address this gap,
we propose the Mean-Field LLM (MF-LLM) framework, which explicitly models the
feedback loop between micro-level decisions and macro-level population. MF-LLM
alternates between two models: a policy model that generates individual actions
based on personal states and group-level information, and a mean field model
that updates the population distribution from the latest individual decisions.
Together, they produce rollouts that simulate the evolving trajectories of
collective decision-making. To better match real-world data, we introduce
IB-Tune, a fine-tuning method for LLMs grounded in the information bottleneck
principle, which maximizes the relevance of population distributions to future
actions while minimizing redundancy with historical data. We evaluate MF-LLM on
a real-world social dataset, where it reduces KL divergence to human population
distributions by 47 percent over non-mean-field baselines, and enables accurate
trend forecasting and intervention planning. It generalizes across seven
domains and four LLM backbones, providing a scalable foundation for
high-fidelity social simulation.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [204] [Evaluation and Verification of Physics-Informed Neural Models of the Grad-Shafranov Equation](https://arxiv.org/abs/2504.21155)
*Fauzan Nazranda Rizqa,Matthew Hole,Charles Gretton*

Main category: physics.plasm-ph

TL;DR: 摘要介绍了使用物理信息神经网络（PINN）和傅里叶神经算子（FNO）建模Grad-Shafranov方程（GSE）以解决磁约束聚变反应堆的MHD平衡问题，并首次研究了这些网络的验证工作流程。


<details>
  <summary>Details</summary>
Motivation: 动机是解决轴对称托卡马克反应堆中GSE建模的局限性，特别是现有研究未考虑网络对多种边界条件的泛化能力。

Method: 方法包括评估一种将边界点作为输入的PINN架构，并与FNO模型在准确性和推理速度上进行比较，最后使用Marabou工具进行网络验证。

Result: 结果显示PINN模型在性能和准确性上优于FNO，尽管PyTorch和Marabou评估之间存在一些差异，但仍验证了实用工作流程。

Conclusion: 结论指出这是首次对这类网络进行验证研究，证明了验证工作流程的实用性。

Abstract: Our contributions are motivated by fusion reactors that rely on maintaining
magnetohydrodynamic (MHD) equilibrium, where the balance between plasma
pressure and confining magnetic fields is required for stable operation. In
axisymmetric tokamak reactors in particular, and under the assumption of
toroidal symmetry, this equilibrium can be mathematically modelled using the
Grad-Shafranov Equation (GSE). Recent works have demonstrated the potential of
using Physics-Informed Neural Networks (PINNs) to model the GSE. Existing
studies did not examine realistic scenarios in which a single network
generalizes to a variety of boundary conditions. Addressing that limitation, we
evaluate a PINN architecture that incorporates boundary points as network
inputs. Additionally, we compare PINN model accuracy and inference speeds with
a Fourier Neural Operator (FNO) model. Finding the PINN model to be the most
performant, and accurate in our setting, we use the network verification tool
Marabou to perform a range of verification tasks. Although we find some
discrepancies between evaluations of the networks natively in PyTorch, compared
to via Marabou, we are able to demonstrate useful and practical verification
workflows. Our study is the first investigation of verification of such
networks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [205] [CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code Generation](https://arxiv.org/abs/2504.21751)
*Sizhe Wang,Zhengren Wang,Dongsheng Ma,Yongan Yu,Rui Ling,Zhiyu Li,Feiyu Xiong,Wentao Zhang*

Main category: cs.SE

TL;DR: 论文介绍了CodeFlowBench，首个为评估大语言模型在多轮迭代代码复用中的能力而设计的基准测试，包含5258个问题，并展示了模型在多轮模式下的表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 现实开发需要代码可读、可扩展和可测试，需通过模块化组件和迭代复用实现，但现有大语言模型在此场景下的能力尚不明确，需系统评估。

Method: 通过自动化流程从Codeforces分解问题为函数级子任务，并设计多轮复用评估框架，对比单轮和多轮模式下的模型表现。

Result: 模型在多轮模式下的表现显著下降（如o1-mini从单轮的37.8%降至20.8%），且难以解决结构复杂的问题。

Conclusion: CodeFlowBench为多轮代码生成任务提供了新基准和见解，揭示当前模型的局限性，指导未来改进方向。

Abstract: Real world development demands code that is readable, extensible, and
testable by organizing the implementation into modular components and
iteratively reuse pre-implemented code. We term this iterative, multi-turn
process codeflow and introduce CodeFlowBench, the first benchmark designed for
comprehensively evaluating LLMs' ability to perform codeflow, namely to
implement new functionality by reusing existing functions over multiple turns.
CodeFlowBench comprises 5258 problems drawn from Codeforces and is continuously
updated via an automated pipeline that decomposes each problem into a series of
function-level subproblems based on its dependency tree and each subproblem is
paired with unit tests. We further propose a novel evaluation framework with
tasks and metrics tailored to multi-turn code reuse to assess model
performance. In experiments across various LLMs under both multi-turn and
single-turn patterns. We observe models' poor performance on CodeFlowBench,
with a substantial performance drop in the iterative codeflow scenario. For
instance, o1-mini achieves a pass@1 of 20.8% in multi-turn pattern versus 37.8%
in single-turn pattern. Further analysis shows that different models excel at
different dependency depths, yet all struggle to correctly solve structurally
complex problems, highlighting challenges for current LLMs to serve as code
generation tools when performing codeflow. Overall, CodeFlowBench offers a
comprehensive benchmark and new insights into LLM capabilities for multi-turn,
iterative code generation, guiding future advances in code generation tasks.

</details>


### [206] [SWE-smith: Scaling Data for Software Engineering Agents](https://arxiv.org/abs/2504.21798)
*John Yang,Kilian Leret,Carlos E. Jimenez,Alexander Wettig,Kabir Khandpur,Yanzhe Zhang,Binyuan Hui,Ofir Press,Ludwig Schmidt,Diyi Yang*

Main category: cs.SE

TL;DR: 论文提出了SWE-smith，一个自动化生成大规模软件工程训练数据的流程，解决了现有数据集小且构建复杂的问题，通过该方法创建的数据集规模远超前人工作。


<details>
  <summary>Details</summary>
Motivation: 现有软件工程训练数据集规模小且构建复杂，限制了语言模型在软件工程领域的应用和发展。

Method: 利用SWE-smith流程自动构建执行环境并从Python代码库中生成大量任务实例。

Result: 生成了包含50k实例的数据集，训练出的SWE-agent-LM-32B模型在SWE-bench基准测试中达到40.2% Pass@1的解决率。

Conclusion: SWE-smith为自动化软件工程研究提供了高效的数据生成工具，并开源了相关资源以促进领域发展。

Abstract: Despite recent progress in Language Models (LMs) for software engineering,
collecting training data remains a significant pain point. Existing datasets
are small, with at most 1,000s of training instances from 11 or fewer GitHub
repositories. The procedures to curate such datasets are often complex,
necessitating hundreds of hours of human labor; companion execution
environments also take up several terabytes of storage, severely limiting their
scalability and usability. To address this pain point, we introduce SWE-smith,
a novel pipeline for generating software engineering training data at scale.
Given any Python codebase, SWE-smith constructs a corresponding execution
environment, then automatically synthesizes 100s to 1,000s of task instances
that break existing test(s) in the codebase. Using SWE-smith, we create a
dataset of 50k instances sourced from 128 GitHub repositories, an order of
magnitude larger than all previous works. We train SWE-agent-LM-32B, achieving
40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art
among open source models. We open source SWE-smith (collection procedure, task
instances, trajectories, models) to lower the barrier of entry for research in
LM systems for automated software engineering. All assets available at
https://swesmith.com.

</details>


### [207] [Assessing LLM code generation quality through path planning tasks](https://arxiv.org/abs/2504.21276)
*Wanyi Chen,Meng-Wen Su,Mary L. Cummings*

Main category: cs.SE

TL;DR: LLM生成的代码在安全关键路径规划应用中存在严重风险，需严格测试。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成代码的普及，需要评估其风险，尤其是安全关键应用（如路径规划）中现有基准测试的不足。

Method: 评估6个LLM生成三种路径规划算法的代码，并在三种不同难度的地图上进行测试。

Result: LLM生成的代码在路径规划中表现出严重隐患。

Conclusion: 安全关键应用中不应直接使用未经严格测试的LLM生成代码。

Abstract: As LLM-generated code grows in popularity, more evaluation is needed to
assess the risks of using such tools, especially for safety-critical
applications such as path planning. Existing coding benchmarks are insufficient
as they do not reflect the context and complexity of safety-critical
applications. To this end, we assessed six LLMs' abilities to generate the code
for three different path-planning algorithms and tested them on three maps of
various difficulties. Our results suggest that LLM-generated code presents
serious hazards for path planning applications and should not be applied in
safety-critical contexts without rigorous testing.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [208] [Transcending Dimensions using Generative AI: Real-Time 3D Model Generation in Augmented Reality](https://arxiv.org/abs/2504.21033)
*Majid Behravan,Maryam Haghani,Denis Gracanin*

Main category: cs.GR

TL;DR: 论文提出了一种结合生成式AI和增强现实的系统，简化3D建模过程，用户无需专业技能即可在AR环境中实时生成与交互3D模型。


<details>
  <summary>Details</summary>
Motivation: 传统3D建模需要专业技术和软件，耗时且门槛高。研究旨在通过AI和AR技术降低门槛，使非专业用户也能轻松创建3D模型。

Method: 使用Shap-E等先进AI模型，结合Mask R-CNN等物体检测方法，解决从2D图像生成3D模型、复杂背景处理及实时交互等挑战。

Result: 35名参与者的系统可用性评分（SUS）为69.64，其中高频使用AR/VR技术的用户评分显著更高（80.71）。

Conclusion: 该系统在游戏、教育和AR电商等领域有应用潜力，为非专业用户提供了直观的3D建模工具。

Abstract: Traditional 3D modeling requires technical expertise, specialized software,
and time-intensive processes, making it inaccessible for many users. Our
research aims to lower these barriers by combining generative AI and augmented
reality (AR) into a cohesive system that allows users to easily generate,
manipulate, and interact with 3D models in real time, directly within AR
environments. Utilizing cutting-edge AI models like Shap-E, we address the
complex challenges of transforming 2D images into 3D representations in AR
environments. Key challenges such as object isolation, handling intricate
backgrounds, and achieving seamless user interaction are tackled through
advanced object detection methods, such as Mask R-CNN. Evaluation results from
35 participants reveal an overall System Usability Scale (SUS) score of 69.64,
with participants who engaged with AR/VR technologies more frequently rating
the system significantly higher, at 80.71. This research is particularly
relevant for applications in gaming, education, and AR-based e-commerce,
offering intuitive, model creation for users without specialized skills.

</details>
