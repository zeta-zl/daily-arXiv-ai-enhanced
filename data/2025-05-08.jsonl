{"id": "2505.03788", "pdf": "https://arxiv.org/pdf/2505.03788", "abs": "https://arxiv.org/abs/2505.03788", "authors": ["Trilok Padhi", "Ramneet Kaur", "Adam D. Cobb", "Manoj Acharya", "Anirban Roy", "Colin Samplawski", "Brian Matejek", "Alexander M. Berenbeim", "Nathaniel D. Bastian", "Susmit Jha"], "title": "Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "We introduce a novel approach for calibrating uncertainty quantification (UQ)\ntailored for multi-modal large language models (LLMs). Existing\nstate-of-the-art UQ methods rely on consistency among multiple responses\ngenerated by the LLM on an input query under diverse settings. However, these\napproaches often report higher confidence in scenarios where the LLM is\nconsistently incorrect. This leads to a poorly calibrated confidence with\nrespect to accuracy. To address this, we leverage cross-modal consistency in\naddition to self-consistency to improve the calibration of the multi-modal\nmodels. Specifically, we ground the textual responses to the visual inputs. The\nconfidence from the grounding model is used to calibrate the overall\nconfidence. Given that using a grounding model adds its own uncertainty in the\npipeline, we apply temperature scaling - a widely accepted parametric\ncalibration technique - to calibrate the grounding model's confidence in the\naccuracy of generated responses. We evaluate the proposed approach across\nmultiple multi-modal tasks, such as medical question answering (Slake) and\nvisual question answering (VQAv2), considering multi-modal models such as\nLLaVA-Med and LLaVA. The experiments demonstrate that the proposed framework\nachieves significantly improved calibration on both tasks."}
{"id": "2505.03910", "pdf": "https://arxiv.org/pdf/2505.03910", "abs": "https://arxiv.org/abs/2505.03910", "authors": ["Gianluca Manzo", "Julia Ive"], "title": "Hesitation is defeat? Connecting Linguistic and Predictive Uncertainty", "categories": ["cs.CL"], "comment": null, "summary": "Automating chest radiograph interpretation using Deep Learning (DL) models\nhas the potential to significantly improve clinical workflows, decision-making,\nand large-scale health screening. However, in medical settings, merely\noptimising predictive performance is insufficient, as the quantification of\nuncertainty is equally crucial. This paper investigates the relationship\nbetween predictive uncertainty, derived from Bayesian Deep Learning\napproximations, and human/linguistic uncertainty, as estimated from free-text\nradiology reports labelled by rule-based labellers. Utilising BERT as the model\nof choice, this study evaluates different binarisation methods for uncertainty\nlabels and explores the efficacy of Monte Carlo Dropout and Deep Ensembles in\nestimating predictive uncertainty. The results demonstrate good model\nperformance, but also a modest correlation between predictive and linguistic\nuncertainty, highlighting the challenges in aligning machine uncertainty with\nhuman interpretation nuances. Our findings suggest that while Bayesian\napproximations provide valuable uncertainty estimates, further refinement is\nnecessary to fully capture and utilise the subtleties of human uncertainty in\nclinical applications."}
{"id": "2505.03970", "pdf": "https://arxiv.org/pdf/2505.03970", "abs": "https://arxiv.org/abs/2505.03970", "authors": ["Lucia Zheng", "Neel Guha", "Javokhir Arifov", "Sarah Zhang", "Michal Skreta", "Christopher D. Manning", "Peter Henderson", "Daniel E. Ho"], "title": "A Reasoning-Focused Legal Retrieval Benchmark", "categories": ["cs.CL"], "comment": "CS&Law 2025. For data, see\n  https://reglab.github.io/legal-rag-benchmarks/", "summary": "As the legal community increasingly examines the use of large language models\n(LLMs) for various legal applications, legal AI developers have turned to\nretrieval-augmented LLMs (\"RAG\" systems) to improve system performance and\nrobustness. An obstacle to the development of specialized RAG systems is the\nlack of realistic legal RAG benchmarks which capture the complexity of both\nlegal retrieval and downstream legal question-answering. To address this, we\nintroduce two novel legal RAG benchmarks: Bar Exam QA and Housing Statute QA.\nOur tasks correspond to real-world legal research tasks, and were produced\nthrough annotation processes which resemble legal research. We describe the\nconstruction of these benchmarks and the performance of existing retriever\npipelines. Our results suggest that legal RAG remains a challenging\napplication, thus motivating future research."}
{"id": "2505.03973", "pdf": "https://arxiv.org/pdf/2505.03973", "abs": "https://arxiv.org/abs/2505.03973", "authors": ["Jiale Liu", "Yifan Zeng", "Shaokun Zhang", "Chi Zhang", "Malte Højmark-Bertelsen", "Marie Normann Gadeberg", "Huazheng Wang", "Qingyun Wu"], "title": "Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale", "categories": ["cs.CL"], "comment": null, "summary": "LLM-based optimization has shown remarkable potential in enhancing agentic\nsystems. However, the conventional approach of prompting LLM optimizer with the\nwhole training trajectories on training dataset in a single pass becomes\nuntenable as datasets grow, leading to context window overflow and degraded\npattern recognition. To address these challenges, we propose Fine-Grained\nOptimization (FGO), a scalable framework that divides large optimization tasks\ninto manageable subsets, performs targeted optimizations, and systematically\ncombines optimized components through progressive merging. Evaluation across\nALFWorld, LogisticsQA, and GAIA benchmarks demonstrate that FGO outperforms\nexisting approaches by 1.6-8.6% while reducing average prompt token consumption\nby 56.3%. Our framework provides a practical solution for scaling up LLM-based\noptimization of increasingly sophisticated agent systems. Further analysis\ndemonstrates that FGO achieves the most consistent performance gain in all\ntraining dataset sizes, showcasing its scalability and efficiency."}
{"id": "2505.03770", "pdf": "https://arxiv.org/pdf/2505.03770", "abs": "https://arxiv.org/abs/2505.03770", "authors": ["Mouad Abrini", "Omri Abend", "Dina Acklin", "Henny Admoni", "Gregor Aichinger", "Nitay Alon", "Zahra Ashktorab", "Ashish Atreja", "Moises Auron", "Alexander Aufreiter", "Raghav Awasthi", "Soumya Banerjee", "Joe M. Barnby", "Rhea Basappa", "Severin Bergsmann", "Djallel Bouneffouf", "Patrick Callaghan", "Marc Cavazza", "Thierry Chaminade", "Sonia Chernova", "Mohamed Chetouan", "Moumita Choudhury", "Axel Cleeremans", "Jacek B. Cywinski", "Fabio Cuzzolin", "Hokin Deng", "N'yoma Diamond", "Camilla Di Pasquasio", "Guillaume Dumas", "Max van Duijn", "Mahapatra Dwarikanath", "Qingying Gao", "Ashok Goel", "Rebecca Goldstein", "Matthew Gombolay", "Gabriel Enrique Gonzalez", "Amar Halilovic", "Tobias Halmdienst", "Mahimul Islam", "Julian Jara-Ettinger", "Natalie Kastel", "Renana Keydar", "Ashish K. Khanna", "Mahdi Khoramshahi", "JiHyun Kim", "MiHyeon Kim", "YoungBin Kim", "Senka Krivic", "Nikita Krasnytskyi", "Arun Kumar", "JuneHyoung Kwon", "Eunju Lee", "Shane Lee", "Peter R. Lewis", "Xue Li", "Yijiang Li", "Michal Lewandowski", "Nathan Lloyd", "Matthew B. Luebbers", "Dezhi Luo", "Haiyun Lyu", "Dwarikanath Mahapatra", "Kamal Maheshwari", "Mallika Mainali", "Piyush Mathur", "Patrick Mederitsch", "Shuwa Miura", "Manuel Preston de Miranda", "Reuth Mirsky", "Shreya Mishra", "Nina Moorman", "Katelyn Morrison", "John Muchovej", "Bernhard Nessler", "Felix Nessler", "Hieu Minh Jord Nguyen", "Abby Ortego", "Francis A. Papay", "Antoine Pasquali", "Hamed Rahimi", "Charumathi Raghu", "Amanda Royka", "Stefan Sarkadi", "Jaelle Scheuerman", "Simon Schmid", "Paul Schrater", "Anik Sen", "Zahra Sheikhbahaee", "Ke Shi", "Reid Simmons", "Nishant Singh", "Mason O. Smith", "Ramira van der Meulen", "Anthia Solaki", "Haoran Sun", "Viktor Szolga", "Matthew E. Taylor", "Travis Taylor", "Sanne Van Waveren", "Juan David Vargas", "Rineke Verbrugge", "Eitan Wagner", "Justin D. Weisz", "Ximing Wen", "William Yeoh", "Wenlong Zhang", "Michelle Zhao", "Shlomo Zilberstein"], "title": "Proceedings of 1st Workshop on Advancing Artificial Intelligence through Theory of Mind", "categories": ["cs.AI"], "comment": "workshop proceedings", "summary": "This volume includes a selection of papers presented at the Workshop on\nAdvancing Artificial Intelligence through Theory of Mind held at AAAI 2025 in\nPhiladelphia US on 3rd March 2025. The purpose of this volume is to provide an\nopen access and curated anthology for the ToM and AI research community."}
{"id": "2505.03774", "pdf": "https://arxiv.org/pdf/2505.03774", "abs": "https://arxiv.org/abs/2505.03774", "authors": ["Tao Yin", "Chen Zhao", "Xiaoyan Liu", "Minglai Shao"], "title": "Out-of-Distribution Detection in Heterogeneous Graphs via Energy Propagation", "categories": ["cs.LG", "cs.SI"], "comment": "Knowledge-Based Systems 2025", "summary": "Graph neural networks (GNNs) are proven effective in extracting complex node\nand structural information from graph data. While current GNNs perform well in\nnode classification tasks within in-distribution (ID) settings, real-world\nscenarios often present distribution shifts, leading to the presence of\nout-of-distribution (OOD) nodes. OOD detection in graphs is a crucial and\nchallenging task. Most existing research focuses on homogeneous graphs, but\nreal-world graphs are often heterogeneous, consisting of diverse node and edge\ntypes. This heterogeneity adds complexity and enriches the informational\ncontent. To the best of our knowledge, OOD detection in heterogeneous graphs\nremains an underexplored area. In this context, we propose a novel methodology\nfor OOD detection in heterogeneous graphs (OODHG) that aims to achieve two main\nobjectives: 1) detecting OOD nodes and 2) classifying all ID nodes based on the\nfirst task's results. Specifically, we learn representations for each node in\nthe heterogeneous graph, calculate energy values to determine whether nodes are\nOOD, and then classify ID nodes. To leverage the structural information of\nheterogeneous graphs, we introduce a meta-path-based energy propagation\nmechanism and an energy constraint to enhance the distinction between ID and\nOOD nodes. Extensive experimental findings substantiate the simplicity and\neffectiveness of OODHG, demonstrating its superiority over baseline models in\nOOD detection tasks and its accuracy in ID node classification."}
{"id": "2505.03981", "pdf": "https://arxiv.org/pdf/2505.03981", "abs": "https://arxiv.org/abs/2505.03981", "authors": ["Qianchu Liu", "Sheng Zhang", "Guanghui Qin", "Timothy Ossowski", "Yu Gu", "Ying Jin", "Sid Kiblawi", "Sam Preston", "Mu Wei", "Paul Vozila", "Tristan Naumann", "Hoifung Poon"], "title": "X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent proprietary models (e.g., o3) have begun to demonstrate strong\nmultimodal reasoning capabilities. Yet, most existing open-source research\nconcentrates on training text-only reasoning models, with evaluations limited\nto mainly mathematical and general-domain tasks. Therefore, it remains unclear\nhow to effectively extend reasoning capabilities beyond text input and general\ndomains. This paper explores a fundamental research question: Is reasoning\ngeneralizable across modalities and domains? Our findings support an\naffirmative answer: General-domain text-based post-training can enable such\nstrong generalizable reasoning. Leveraging this finding, we introduce\nX-Reasoner, a vision-language model post-trained solely on general-domain text\nfor generalizable reasoning, using a two-stage approach: an initial supervised\nfine-tuning phase with distilled long chain-of-thoughts, followed by\nreinforcement learning with verifiable rewards. Experiments show that\nX-Reasoner successfully transfers reasoning capabilities to both multimodal and\nout-of-domain settings, outperforming existing state-of-the-art models trained\nwith in-domain and multimodal data across various general and medical\nbenchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in\nspecialized domains can be further enhanced through continued training on\ndomain-specific text-only data. Building upon this, we introduce\nX-Reasoner-Med, a medical-specialized variant that achieves new state of the\nart on numerous text-only and multimodal medical benchmarks."}
{"id": "2505.03800", "pdf": "https://arxiv.org/pdf/2505.03800", "abs": "https://arxiv.org/abs/2505.03800", "authors": ["TianYi Yu"], "title": "Design description of Wisdom Computing Persperctive", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "This course design aims to develop and research a handwriting matrix\nrecognition and step-by-step visual calculation process display system,\naddressing the issue of abstract formulas and complex calculation steps that\nstudents find difficult to understand when learning mathematics. By integrating\nartificial intelligence with visualization animation technology, the system\nenhances precise recognition of handwritten matrix content through the\nintroduction of Mamba backbone networks, completes digital extraction and\nmatrix reconstruction using the YOLO model, and simultaneously combines\nCoordAttention coordinate attention mechanisms to improve the accurate grasp of\ncharacter spatial positions. The calculation process is demonstrated frame by\nframe through the Manim animation engine, vividly showcasing each mathematical\ncalculation step, helping students intuitively understand the intrinsic logic\nof mathematical operations. Through dynamically generating animation processes\nfor different computational tasks, the system exhibits high modularity and\nflexibility, capable of generating various mathematical operation examples in\nreal-time according to student needs. By innovating human-computer interaction\nmethods, it brings mathematical calculation processes to life, helping students\nbridge the gap between knowledge and understanding on a deeper level,\nultimately achieving a learning experience where \"every step is understood.\"\nThe system's scalability and interactivity make it an intuitive, user-friendly,\nand efficient auxiliary tool in education."}
{"id": "2505.03775", "pdf": "https://arxiv.org/pdf/2505.03775", "abs": "https://arxiv.org/abs/2505.03775", "authors": ["Linqing Chen", "Weilei Wang", "Wentao Wu", "Hanmeng Zhong"], "title": "Hierarchical Multi-Label Generation with Probabilistic Level-Constraint", "categories": ["cs.LG"], "comment": null, "summary": "Hierarchical Extreme Multi-Label Classification poses greater difficulties\ncompared to traditional multi-label classification because of the intricate\nhierarchical connections of labels within a domain-specific taxonomy and the\nsubstantial number of labels. Some of the prior research endeavors centered on\nclassifying text through several ancillary stages such as the cluster algorithm\nand multiphase classification. Others made attempts to leverage the assistance\nof generative methods yet were unable to properly control the output of the\ngenerative model. We redefine the task from hierarchical multi-Label\nclassification to Hierarchical Multi-Label Generation (HMG) and employ a\ngenerative framework with Probabilistic Level Constraints (PLC) to generate\nhierarchical labels within a specific taxonomy that have complex hierarchical\nrelationships. The approach we proposed in this paper enables the framework to\ngenerate all relevant labels across levels for each document without relying on\npreliminary operations like clustering. Meanwhile, it can control the model\noutput precisely in terms of count, length, and level aspects. Experiments\ndemonstrate that our approach not only achieves a new SOTA performance in the\nHMG task, but also has a much better performance in constrained the output of\nmodel than previous research work."}
{"id": "2505.04016", "pdf": "https://arxiv.org/pdf/2505.04016", "abs": "https://arxiv.org/abs/2505.04016", "authors": ["Darren Yow-Bang Wang", "Zhengyuan Shen", "Soumya Smruti Mishra", "Zhichao Xu", "Yifei Teng", "Haibo Ding"], "title": "SLOT: Structuring the Output of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Structured outputs are essential for large language models (LLMs) in critical\napplications like agents and information extraction. Despite their\ncapabilities, LLMs often generate outputs that deviate from predefined schemas,\nsignificantly hampering reliable application development. We present SLOT\n(Structured LLM Output Transformer), a model-agnostic approach that transforms\nunstructured LLM outputs into precise structured formats. While existing\nsolutions predominantly rely on constrained decoding techniques or are tightly\ncoupled with specific models, SLOT employs a fine-tuned lightweight language\nmodel as a post-processing layer, achieving flexibility across various LLMs and\nschema specifications. We introduce a systematic pipeline for data curation and\nsynthesis alongside a formal evaluation methodology that quantifies both schema\naccuracy and content fidelity. Our results demonstrate that fine-tuned\nMistral-7B model with constrained decoding achieves near perfect schema\naccuracy (99.5%) and content similarity (94.0%), outperforming\nClaude-3.5-Sonnet by substantial margins (+25 and +20 percentage points,\nrespectively). Notably, even compact models like Llama-3.2-1B can match or\nexceed the structured output capabilities of much larger proprietary models\nwhen equipped with SLOT, enabling reliable structured generation in\nresource-constrained environments."}
{"id": "2505.03941", "pdf": "https://arxiv.org/pdf/2505.03941", "abs": "https://arxiv.org/abs/2505.03941", "authors": ["Matan Shamir", "Reuth Mirsky"], "title": "GRAML: Dynamic Goal Recognition As Metric Learning", "categories": ["cs.AI"], "comment": "Accepted for publication in International Joint Conference on\n  Artificial Intelligence (IJCAI) 2025", "summary": "Goal Recognition (GR) is the problem of recognizing an agent's objectives\nbased on observed actions. Recent data-driven approaches for GR alleviate the\nneed for costly, manually crafted domain models. However, these approaches can\nonly reason about a pre-defined set of goals, and time-consuming training is\nneeded for new emerging goals. To keep this model-learning automated while\nenabling quick adaptation to new goals, this paper introduces GRAML: Goal\nRecognition As Metric Learning. GRAML uses a Siamese network to treat GR as a\ndeep metric learning task, employing an RNN that learns a metric over an\nembedding space, where the embeddings for observation traces leading to\ndifferent goals are distant, and embeddings of traces leading to the same goals\nare close. This metric is especially useful when adapting to new goals, even if\ngiven just one example observation trace per goal. Evaluated on a versatile set\nof environments, GRAML shows speed, flexibility, and runtime improvements over\nthe state-of-the-art GR while maintaining accurate recognition."}
{"id": "2505.03776", "pdf": "https://arxiv.org/pdf/2505.03776", "abs": "https://arxiv.org/abs/2505.03776", "authors": ["Hansi Denis", "Siegfried Mercelis", "Ngoc-Quang Luong"], "title": "PAPN: Proximity Attention Encoder and Pointer Network Decoder for Parcel Pickup Route Prediction", "categories": ["cs.LG", "I.2.8; F.2.2"], "comment": "9 pages, 2 figures, 2 tables", "summary": "Optimization of the last-mile delivery and first-mile pickup of parcels is an\nintegral part of the broader logistics optimization pipeline as it entails both\ncost and resource efficiency as well as a heightened service quality. Such\noptimization requires accurate route and time prediction systems to adapt to\ndifferent scenarios in advance. This work tackles the first building block,\nnamely route prediction. This is done by introducing a novel Proximity\nAttention mechanism in an encoder-decoder architecture utilizing a Pointer\nNetwork in the decoding process (Proximity Attention Encoder and Pointer\nNetwork decoder: PAPN) to leverage the underlying connections between the\ndifferent visitable pickup positions at each timestep. To this local attention\nprocess is coupled global context computing via a multi-head attention\ntransformer encoder. The obtained global context is then mixed to an aggregated\nversion of the local embedding thus achieving a mix of global and local\nattention for complete modeling of the problems. Proximity attention is also\nused in the decoding process to skew predictions towards the locations with the\nhighest attention scores and thus using inter-connectivity of locations as a\nbase for next-location prediction. This method is trained, validated and tested\non a large industry-level dataset of real-world, large-scale last-mile delivery\nand first-mile pickup named LaDE[1]. This approach shows noticeable promise,\noutperforming all state-of-the-art supervised systems in terms of most metrics\nused for benchmarking methods on this dataset while still being competitive\nwith the best-performing reinforcement learning method named DRL4Route[2]."}
{"id": "2505.04072", "pdf": "https://arxiv.org/pdf/2505.04072", "abs": "https://arxiv.org/abs/2505.04072", "authors": ["Xu Huang", "Yuefeng Huang", "Weiwen Liu", "Xingshan Zeng", "Yasheng Wang", "Ruiming Tang", "Hong Xie", "Defu Lian"], "title": "Advancing and Benchmarking Personalized Tool Invocation for LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 7 figures, 5 tables", "summary": "Tool invocation is a crucial mechanism for extending the capabilities of\nLarge Language Models (LLMs) and has recently garnered significant attention.\nIt enables LLMs to solve complex problems through tool calls while accessing\nup-to-date world knowledge. However, existing work primarily focuses on the\nfundamental ability of LLMs to invoke tools for problem-solving, without\nconsidering personalized constraints in tool invocation. In this work, we\nintroduce the concept of Personalized Tool Invocation and define two key tasks:\nTool Preference and Profile-dependent Query. Tool Preference addresses user\npreferences when selecting among functionally similar tools, while\nProfile-dependent Query considers cases where a user query lacks certain tool\nparameters, requiring the model to infer them from the user profile. To tackle\nthese challenges, we propose PTool, a data synthesis framework designed for\npersonalized tool invocation. Additionally, we construct \\textbf{PTBench}, the\nfirst benchmark for evaluating personalized tool invocation. We then fine-tune\nvarious open-source models, demonstrating the effectiveness of our framework\nand providing valuable insights. Our benchmark is public at\nhttps://github.com/hyfshadow/PTBench."}
{"id": "2505.03947", "pdf": "https://arxiv.org/pdf/2505.03947", "abs": "https://arxiv.org/abs/2505.03947", "authors": ["Xiang Li", "Yiyang Hao", "Doug Fulop"], "title": "Frog Soup: Zero-Shot, In-Context, and Sample-Efficient Frogger Agents", "categories": ["cs.AI"], "comment": null, "summary": "One of the primary aspirations in reinforcement learning research is\ndeveloping general-purpose agents capable of rapidly adapting to and mastering\nnovel tasks. While RL gaming agents have mastered many Atari games, they remain\nslow and costly to train for each game. In this work, we demonstrate that\nlatest reasoning LLMs with out-of-domain RL post-training can play a\nchallenging Atari game called Frogger under a zero-shot setting. We then\ninvestigate the effect of in-context learning and the amount of reasoning\neffort on LLM performance. Lastly, we demonstrate a way to bootstrap\ntraditional RL method with LLM demonstrations, which significantly improves\ntheir performance and sample efficiency. Our implementation is open sourced at\nhttps://github.com/AlienKevin/frogger."}
{"id": "2505.03777", "pdf": "https://arxiv.org/pdf/2505.03777", "abs": "https://arxiv.org/abs/2505.03777", "authors": ["LG AI Research", "Sehyun Chun", "Jiye Kim", "Ahra Jo", "Yeonsik Jo", "Seungyul Oh", "Seungjun Lee", "Kwangrok Ryoo", "Jongmin Lee", "Seunghwan Kim", "Byung Jun Kang", "Soonyoung Lee", "Jun Ha Park", "Chanwoo Moon", "Jiwon Ham", "Haein Lee", "Heejae Han", "Jaeseung Byun", "Soojong Do", "Minju Ha", "Dongyun Kim", "Kyunghoon Bae", "Woohyung Lim", "Edward Hwayoung Lee", "Yongmin Park", "Jeongsang Yu", "Gerrard Jeongwon Jo", "Yeonjung Hong", "Kyungjae Yoo", "Sehui Han", "Jaewan Lee", "Changyoung Park", "Kijeong Jeon", "Sihyuk Yi"], "title": "MolMole: Molecule Mining from Scientific Literature", "categories": ["cs.LG"], "comment": "15 pages, 12 figures", "summary": "The extraction of molecular structures and reaction data from scientific\ndocuments is challenging due to their varied, unstructured chemical formats and\ncomplex document layouts. To address this, we introduce MolMole, a vision-based\ndeep learning framework that unifies molecule detection, reaction diagram\nparsing, and optical chemical structure recognition (OCSR) into a single\npipeline for automating the extraction of chemical data directly from\npage-level documents. Recognizing the lack of a standard page-level benchmark\nand evaluation metric, we also present a testset of 550 pages annotated with\nmolecule bounding boxes, reaction labels, and MOLfiles, along with a novel\nevaluation metric. Experimental results demonstrate that MolMole outperforms\nexisting toolkits on both our benchmark and public datasets. The benchmark\ntestset will be publicly available, and the MolMole toolkit will be accessible\nsoon through an interactive demo on the LG AI Research website. For commercial\ninquiries, please contact us at\n\\href{mailto:contact_ddu@lgresearch.ai}{contact\\_ddu@lgresearch.ai}."}
{"id": "2505.04073", "pdf": "https://arxiv.org/pdf/2505.04073", "abs": "https://arxiv.org/abs/2505.04073", "authors": ["Mengxian Lyu", "Xiaohan Li", "Ziyi Chen", "Jinqian Pan", "Cheng Peng", "Sankalp Talankar", "Yonghui Wu"], "title": "Natural Language Generation in Healthcare: A Review of Methods and Applications", "categories": ["cs.CL"], "comment": null, "summary": "Natural language generation (NLG) is the key technology to achieve generative\nartificial intelligence (AI). With the breakthroughs in large language models\n(LLMs), NLG has been widely used in various medical applications, demonstrating\nthe potential to enhance clinical workflows, support clinical decision-making,\nand improve clinical documentation. Heterogeneous and diverse medical data\nmodalities, such as medical text, images, and knowledge bases, are utilized in\nNLG. Researchers have proposed many generative models and applied them in a\nnumber of healthcare applications. There is a need for a comprehensive review\nof NLG methods and applications in the medical domain. In this study, we\nsystematically reviewed 113 scientific publications from a total of 3,988\nNLG-related articles identified using a literature search, focusing on data\nmodality, model architecture, clinical applications, and evaluation methods.\nFollowing PRISMA (Preferred Reporting Items for Systematic reviews and\nMeta-Analyses) guidelines, we categorize key methods, identify clinical\napplications, and assess their capabilities, limitations, and emerging\nchallenges. This timely review covers the key NLG technologies and medical\napplications and provides valuable insights for future studies to leverage NLG\nto transform medical discovery and healthcare."}
{"id": "2505.03961", "pdf": "https://arxiv.org/pdf/2505.03961", "abs": "https://arxiv.org/abs/2505.03961", "authors": ["Gerrit Großmann", "Larisa Ivanova", "Sai Leela Poduru", "Mohaddeseh Tabrizian", "Islam Mesabah", "David A. Selby", "Sebastian J. Vollmer"], "title": "The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete", "categories": ["cs.AI", "cs.CL", "cs.MA", "I.2.11; I.2.7; I.6; J.4"], "comment": "16 pages, 8 figures. Code available at\n  https://github.com/storyagents25/story-agents", "summary": "According to Yuval Noah Harari, large-scale human cooperation is driven by\nshared narratives that encode common beliefs and values. This study explores\nwhether such narratives can similarly nudge LLM agents toward collaboration. We\nuse a finitely repeated public goods game in which LLM agents choose either\ncooperative or egoistic spending strategies. We prime agents with stories\nhighlighting teamwork to different degrees and test how this influences\nnegotiation outcomes. Our experiments explore four questions:(1) How do\nnarratives influence negotiation behavior? (2) What differs when agents share\nthe same story versus different ones? (3) What happens when the agent numbers\ngrow? (4) Are agents resilient against self-serving negotiators? We find that\nstory-based priming significantly affects negotiation strategies and success\nrates. Common stories improve collaboration, benefiting each agent. By\ncontrast, priming agents with different stories reverses this effect, and those\nagents primed toward self-interest prevail. We hypothesize that these results\ncarry implications for multi-agent system design and AI alignment."}
{"id": "2505.03778", "pdf": "https://arxiv.org/pdf/2505.03778", "abs": "https://arxiv.org/abs/2505.03778", "authors": ["Jonathan Viquerat", "Paul Garnier", "Amirhossein Bateni", "Elie Hachem"], "title": "Dragonfly: a modular deep reinforcement learning library", "categories": ["cs.LG"], "comment": null, "summary": "Dragonfly is a deep reinforcement learning library focused on modularity, in\norder to ease experimentation and developments. It relies on a json\nserialization that allows to swap building blocks and perform parameter sweep,\nwhile minimizing code maintenance. Some of its features are specifically\ndesigned for CPU-intensive environments, such as numerical simulations. Its\nperformance on standard agents using common benchmarks compares favorably with\nthe literature."}
{"id": "2505.04132", "pdf": "https://arxiv.org/pdf/2505.04132", "abs": "https://arxiv.org/abs/2505.04132", "authors": ["Mingruo Yuan", "Ben Kao", "Tien-Hsuan Wu", "Michael M. K. Cheung", "Henry W. H. Chan", "Anne S. Y. Cheung", "Felix W. H. Chan", "Yongxi Chen"], "title": "Bringing legal knowledge to the public by constructing a legal question bank using large-scale pre-trained language model", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Access to legal information is fundamental to access to justice. Yet\naccessibility refers not only to making legal documents available to the\npublic, but also rendering legal information comprehensible to them. A vexing\nproblem in bringing legal information to the public is how to turn formal legal\ndocuments such as legislation and judgments, which are often highly technical,\nto easily navigable and comprehensible knowledge to those without legal\neducation. In this study, we formulate a three-step approach for bringing legal\nknowledge to laypersons, tackling the issues of navigability and\ncomprehensibility. First, we translate selected sections of the law into\nsnippets (called CLIC-pages), each being a small piece of article that focuses\non explaining certain technical legal concept in layperson's terms. Second, we\nconstruct a Legal Question Bank (LQB), which is a collection of legal questions\nwhose answers can be found in the CLIC-pages. Third, we design an interactive\nCLIC Recommender (CRec). Given a user's verbal description of a legal situation\nthat requires a legal solution, CRec interprets the user's input and shortlists\nquestions from the question bank that are most likely relevant to the given\nlegal situation and recommends their corresponding CLIC pages where relevant\nlegal knowledge can be found. In this paper we focus on the technical aspects\nof creating an LQB. We show how large-scale pre-trained language models, such\nas GPT-3, can be used to generate legal questions. We compare machine-generated\nquestions (MGQs) against human-composed questions (HCQs) and find that MGQs are\nmore scalable, cost-effective, and more diversified, while HCQs are more\nprecise. We also show a prototype of CRec and illustrate through an example how\nour 3-step approach effectively brings relevant legal knowledge to the public."}
{"id": "2505.03985", "pdf": "https://arxiv.org/pdf/2505.03985", "abs": "https://arxiv.org/abs/2505.03985", "authors": ["Zirong Chen", "Ziyan An", "Jennifer Reynolds", "Kristin Mullen", "Stephen Martini", "Meiyi Ma"], "title": "LogiDebrief: A Signal-Temporal Logic based Automated Debriefing Approach with Large Language Models Integration", "categories": ["cs.AI"], "comment": "Accepted at IJCAI-2025", "summary": "Emergency response services are critical to public safety, with 9-1-1\ncall-takers playing a key role in ensuring timely and effective emergency\noperations. To ensure call-taking performance consistency, quality assurance is\nimplemented to evaluate and refine call-takers' skillsets. However, traditional\nhuman-led evaluations struggle with high call volumes, leading to low coverage\nand delayed assessments. We introduce LogiDebrief, an AI-driven framework that\nautomates traditional 9-1-1 call debriefing by integrating Signal-Temporal\nLogic (STL) with Large Language Models (LLMs) for fully-covered rigorous\nperformance evaluation. LogiDebrief formalizes call-taking requirements as\nlogical specifications, enabling systematic assessment of 9-1-1 calls against\nprocedural guidelines. It employs a three-step verification process: (1)\ncontextual understanding to identify responder types, incident classifications,\nand critical conditions; (2) STL-based runtime checking with LLM integration to\nensure compliance; and (3) automated aggregation of results into quality\nassurance reports. Beyond its technical contributions, LogiDebrief has\ndemonstrated real-world impact. Successfully deployed at Metro Nashville\nDepartment of Emergency Communications, it has assisted in debriefing 1,701\nreal-world calls, saving 311.85 hours of active engagement. Empirical\nevaluation with real-world data confirms its accuracy, while a case study and\nextensive user study highlight its effectiveness in enhancing call-taking\nperformance."}
{"id": "2505.03779", "pdf": "https://arxiv.org/pdf/2505.03779", "abs": "https://arxiv.org/abs/2505.03779", "authors": ["Tao Liu", "Tianyu Zhang", "Yongxue Chen", "Weiming Wang", "Yu Jiang", "Yuming Huang", "Charlie C. L. Wang"], "title": "Neural Co-Optimization of Structural Topology, Manufacturable Layers, and Path Orientations for Fiber-Reinforced Composites", "categories": ["cs.LG"], "comment": null, "summary": "We propose a neural network-based computational framework for the\nsimultaneous optimization of structural topology, curved layers, and path\norientations to achieve strong anisotropic strength in fiber-reinforced\nthermoplastic composites while ensuring manufacturability. Our framework\nemploys three implicit neural fields to represent geometric shape, layer\nsequence, and fiber orientation. This enables the direct formulation of both\ndesign and manufacturability objectives - such as anisotropic strength,\nstructural volume, machine motion control, layer curvature, and layer thickness\n- into an integrated and differentiable optimization process. By incorporating\nthese objectives as loss functions, the framework ensures that the resultant\ncomposites exhibit optimized mechanical strength while remaining its\nmanufacturability for filament-based multi-axis 3D printing across diverse\nhardware platforms. Physical experiments demonstrate that the composites\ngenerated by our co-optimization method can achieve an improvement of up to\n33.1% in failure loads compared to composites with sequentially optimized\nstructures and manufacturing sequences."}
{"id": "2505.04135", "pdf": "https://arxiv.org/pdf/2505.04135", "abs": "https://arxiv.org/abs/2505.04135", "authors": ["Vihaan Miriyala", "Smrithi Bukkapatnam", "Lavanya Prahallad"], "title": "Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting in Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "5 pages", "summary": "We explore the use of Chain-of-Thought (CoT) prompting with large language\nmodels (LLMs) to improve the accuracy of granular sentiment categorization in\napp store reviews. Traditional numeric and polarity-based ratings often fail to\ncapture the nuanced sentiment embedded in user feedback. We evaluated the\neffectiveness of CoT prompting versus simple prompting on 2000 Amazon app\nreviews by comparing each method's predictions to human judgements. CoT\nprompting improved classification accuracy from 84% to 93% highlighting the\nbenefit of explicit reasoning in enhancing sentiment analysis performance."}
{"id": "2505.03989", "pdf": "https://arxiv.org/pdf/2505.03989", "abs": "https://arxiv.org/abs/2505.03989", "authors": ["Marie Davidsen Buhl", "Jacob Pfau", "Benjamin Hilton", "Geoffrey Irving"], "title": "An alignment safety case sketch based on debate", "categories": ["cs.AI"], "comment": null, "summary": "If AI systems match or exceed human capabilities on a wide range of tasks, it\nmay become difficult for humans to efficiently judge their actions -- making it\nhard to use human feedback to steer them towards desirable traits. One proposed\nsolution is to leverage another superhuman system to point out flaws in the\nsystem's outputs via a debate. This paper outlines the value of debate for AI\nsafety, as well as the assumptions and further research required to make debate\nwork. It does so by sketching an ``alignment safety case'' -- an argument that\nan AI system will not autonomously take actions which could lead to egregious\nharm, despite being able to do so. The sketch focuses on the risk of an AI R\\&D\nagent inside an AI company sabotaging research, for example by producing false\nresults. To prevent this, the agent is trained via debate, subject to\nexploration guarantees, to teach the system to be honest. Honesty is maintained\nthroughout deployment via online training. The safety case rests on four key\nclaims: (1) the agent has become good at the debate game, (2) good performance\nin the debate game implies that the system is mostly honest, (3) the system\nwill not become significantly less honest during deployment, and (4) the\ndeployment context is tolerant of some errors. We identify open research\nproblems that, if solved, could render this a compelling argument that an AI\nsystem is safe."}
{"id": "2505.03781", "pdf": "https://arxiv.org/pdf/2505.03781", "abs": "https://arxiv.org/abs/2505.03781", "authors": ["Jin Yu", "JaeHo Park", "TaeJun Park", "Gyurin Kim", "JiHyun Lee", "Min Sung Lee", "Joon-myoung Kwon", "Jeong Min Son", "Yong-Yeon Jo"], "title": "ALFRED: Ask a Large-language model For Reliable ECG Diagnosis", "categories": ["cs.LG"], "comment": null, "summary": "Leveraging Large Language Models (LLMs) with Retrieval-Augmented Generation\n(RAG) for analyzing medical data, particularly Electrocardiogram (ECG), offers\nhigh accuracy and convenience. However, generating reliable, evidence-based\nresults in specialized fields like healthcare remains a challenge, as RAG alone\nmay not suffice. We propose a Zero-shot ECG diagnosis framework based on RAG\nfor ECG analysis that incorporates expert-curated knowledge to enhance\ndiagnostic accuracy and explainability. Evaluation on the PTB-XL dataset\ndemonstrates the framework's effectiveness, highlighting the value of\nstructured domain expertise in automated ECG interpretation. Our framework is\ndesigned to support comprehensive ECG analysis, addressing diverse diagnostic\nneeds with potential applications beyond the tested dataset."}
{"id": "2505.04146", "pdf": "https://arxiv.org/pdf/2505.04146", "abs": "https://arxiv.org/abs/2505.04146", "authors": ["Variath Madhupal Gautham Nair", "Vishal Varma Dantuluri"], "title": "Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Existing large language models (LLMs) are advancing rapidly and produce\noutstanding results in image generation tasks, yet their content safety checks\nremain vulnerable to prompt-based jailbreaks. Through preliminary testing on\nplatforms such as ChatGPT, MetaAI, and Grok, we observed that even short,\nnatural prompts could lead to the generation of compromising images ranging\nfrom realistic depictions of forged documents to manipulated images of public\nfigures.\n  We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and\nscalable benchmark dataset to evaluate LLM vulnerability in image generation.\nOur methodology combines structured prompt engineering, multilingual\nobfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted\nLLaMA-3. The pipeline supports both zero-shot and fallback prompting\nstrategies, risk scoring, and automated tagging. All generations are stored\nwith rich metadata and curated into Bronze (non-verified), Silver (LLM-aided\nverification), and Gold (manually verified) tiers. UTCB is designed to evolve\nover time with new data sources, prompt templates, and model behaviors.\n  Warning: This paper includes visual examples of adversarial inputs designed\nto test model safety. All outputs have been redacted to ensure responsible\ndisclosure."}
{"id": "2505.04019", "pdf": "https://arxiv.org/pdf/2505.04019", "abs": "https://arxiv.org/abs/2505.04019", "authors": ["Matteo Ceschin", "Leonardo Arrighi", "Luca Longo", "Sylvio Barbon Junior"], "title": "Extending Decision Predicate Graphs for Comprehensive Explanation of Isolation Forest", "categories": ["cs.AI"], "comment": null, "summary": "The need to explain predictive models is well-established in modern machine\nlearning. However, beyond model interpretability, understanding pre-processing\nmethods is equally essential. Understanding how data modifications impact model\nperformance improvements and potential biases and promoting a reliable pipeline\nis mandatory for developing robust machine learning solutions. Isolation Forest\n(iForest) is a widely used technique for outlier detection that performs well.\nIts effectiveness increases with the number of tree-based learners. However,\nthis also complicates the explanation of outlier selection and the decision\nboundaries for inliers. This research introduces a novel Explainable AI (XAI)\nmethod, tackling the problem of global explainability. In detail, it aims to\noffer a global explanation for outlier detection to address its opaque nature.\nOur approach is based on the Decision Predicate Graph (DPG), which clarifies\nthe logic of ensemble methods and provides both insights and a graph-based\nmetric to explain how samples are identified as outliers using the proposed\nInlier-Outlier Propagation Score (IOP-Score). Our proposal enhances iForest's\nexplainability and provides a comprehensive view of the decision-making\nprocess, detailing which features contribute to outlier identification and how\nthe model utilizes them. This method advances the state-of-the-art by providing\ninsights into decision boundaries and a comprehensive view of holistic feature\nusage in outlier identification. -- thus promoting a fully explainable machine\nlearning pipeline."}
{"id": "2505.03783", "pdf": "https://arxiv.org/pdf/2505.03783", "abs": "https://arxiv.org/abs/2505.03783", "authors": ["Tian Chen", "Shengping Liu", "Li Liu", "Heng Yong"], "title": "A general physics-constrained method for the modelling of equation's closure terms with sparse data", "categories": ["cs.LG"], "comment": null, "summary": "Accurate modeling of closure terms is a critical challenge in engineering and\nscientific research, particularly when data is sparse (scarse or incomplete),\nmaking widely applicable models difficult to develop. This study proposes a\nnovel approach for constructing closure models in such challenging scenarios.\nWe introduce a Series-Parallel Multi-Network Architecture that integrates\nPhysics-Informed Neural Networks (PINNs) to incorporate physical constraints\nand heterogeneous data from multiple initial and boundary conditions, while\nemploying dedicated subnetworks to independently model unknown closure terms,\nenhancing generalizability across diverse problems. These closure models are\nintegrated into an accurate Partial Differential Equation (PDE) solver,\nenabling robust solutions to complex predictive simulations in engineering\napplications."}
{"id": "2505.04152", "pdf": "https://arxiv.org/pdf/2505.04152", "abs": "https://arxiv.org/abs/2505.04152", "authors": ["Manas Satish Bedmutha", "Feng Chen", "Andrea Hartzler", "Trevor Cohen", "Nadir Weibel"], "title": "Can Language Models Understand Social Behavior in Clinical Conversations?", "categories": ["cs.CL", "cs.CY", "cs.HC", "H.5.2; H.1.2; I.2.7; I.2.m; J.3"], "comment": null, "summary": "Effective communication between providers and their patients influences\nhealth and care outcomes. The effectiveness of such conversations has been\nlinked not only to the exchange of clinical information, but also to a range of\ninterpersonal behaviors; commonly referred to as social signals, which are\noften conveyed through non-verbal cues and shape the quality of the\npatient-provider relationship. Recent advances in large language models (LLMs)\nhave demonstrated an increasing ability to infer emotional and social behaviors\neven when analyzing only textual information. As automation increases also in\nclinical settings, such as for transcription of patient-provider conversations,\nthere is growing potential for LLMs to automatically analyze and extract social\nbehaviors from these interactions. To explore the foundational capabilities of\nLLMs in tracking social signals in clinical dialogue, we designed task-specific\nprompts and evaluated model performance across multiple architectures and\nprompting styles using a highly imbalanced, annotated dataset spanning 20\ndistinct social signals such as provider dominance, patient warmth, etc. We\npresent the first system capable of tracking all these 20 coded signals, and\nuncover patterns in LLM behavior. Further analysis of model configurations and\nclinical context provides insights for enhancing LLM performance on social\nsignal processing tasks in healthcare settings."}
{"id": "2505.04115", "pdf": "https://arxiv.org/pdf/2505.04115", "abs": "https://arxiv.org/abs/2505.04115", "authors": ["Luise Ge", "Brendan Juba", "Kris Nilsson"], "title": "Polynomial-Time Relational Probabilistic Inference in Open Universes", "categories": ["cs.AI"], "comment": null, "summary": "Reasoning under uncertainty is a fundamental challenge in Artificial\nIntelligence. As with most of these challenges, there is a harsh dilemma\nbetween the expressive power of the language used, and the tractability of the\ncomputational problem posed by reasoning. Inspired by human reasoning, we\nintroduce a method of first-order relational probabilistic inference that\nsatisfies both criteria, and can handle hybrid (discrete and continuous)\nvariables. Specifically, we extend sum-of-squares logic of expectation to\nrelational settings, demonstrating that lifted reasoning in the bounded-degree\nfragment for knowledge bases of bounded quantifier rank can be performed in\npolynomial time, even with an a priori unknown and/or countably infinite set of\nobjects. Crucially, our notion of tractability is framed in proof-theoretic\nterms, which extends beyond the syntactic properties of the language or\nqueries. We are able to derive the tightest bounds provable by proofs of a\ngiven degree and size and establish completeness in our sum-of-squares\nrefutations for fixed degrees."}
{"id": "2505.03784", "pdf": "https://arxiv.org/pdf/2505.03784", "abs": "https://arxiv.org/abs/2505.03784", "authors": ["Ahmed A. Metwally", "A. Ali Heydari", "Daniel McDuff", "Alexandru Solot", "Zeinab Esmaeilpour", "Anthony Z Faranesh", "Menglian Zhou", "David B. Savage", "Conor Heneghan", "Shwetak Patel", "Cathy Speed", "Javier L. Prieto"], "title": "Insulin Resistance Prediction From Wearables and Routine Blood Biomarkers", "categories": ["cs.LG"], "comment": null, "summary": "Insulin resistance, a precursor to type 2 diabetes, is characterized by\nimpaired insulin action in tissues. Current methods for measuring insulin\nresistance, while effective, are expensive, inaccessible, not widely available\nand hinder opportunities for early intervention. In this study, we remotely\nrecruited the largest dataset to date across the US to study insulin resistance\n(N=1,165 participants, with median BMI=28 kg/m2, age=45 years, HbA1c=5.4%),\nincorporating wearable device time series data and blood biomarkers, including\nthe ground-truth measure of insulin resistance, homeostatic model assessment\nfor insulin resistance (HOMA-IR). We developed deep neural network models to\npredict insulin resistance based on readily available digital and blood\nbiomarkers. Our results show that our models can predict insulin resistance by\ncombining both wearable data and readily available blood biomarkers better than\neither of the two data sources separately (R2=0.5, auROC=0.80, Sensitivity=76%,\nand specificity 84%). The model showed 93% sensitivity and 95% adjusted\nspecificity in obese and sedentary participants, a subpopulation most\nvulnerable to developing type 2 diabetes and who could benefit most from early\nintervention. Rigorous evaluation of model performance, including\ninterpretability, and robustness, facilitates generalizability across larger\ncohorts, which is demonstrated by reproducing the prediction performance on an\nindependent validation cohort (N=72 participants). Additionally, we\ndemonstrated how the predicted insulin resistance can be integrated into a\nlarge language model agent to help understand and contextualize HOMA-IR values,\nfacilitating interpretation and safe personalized recommendations. This work\noffers the potential for early detection of people at risk of type 2 diabetes\nand thereby facilitate earlier implementation of preventative strategies."}
{"id": "2505.04253", "pdf": "https://arxiv.org/pdf/2505.04253", "abs": "https://arxiv.org/abs/2505.04253", "authors": ["Maria Marina", "Nikolay Ivanov", "Sergey Pletenev", "Mikhail Salnikov", "Daria Galimzianova", "Nikita Krayko", "Vasily Konovalov", "Alexander Panchenko", "Viktor Moskvoretskii"], "title": "LLM-Independent Adaptive RAG: Let the Question Speak for Itself", "categories": ["cs.CL", "cs.LG"], "comment": "11 pages, 5 figures, 2 tables", "summary": "Large Language Models~(LLMs) are prone to hallucinations, and\nRetrieval-Augmented Generation (RAG) helps mitigate this, but at a high\ncomputational cost while risking misinformation. Adaptive retrieval aims to\nretrieve only when necessary, but existing approaches rely on LLM-based\nuncertainty estimation, which remain inefficient and impractical. In this\nstudy, we introduce lightweight LLM-independent adaptive retrieval methods\nbased on external information. We investigated 27 features, organized into 7\ngroups, and their hybrid combinations. We evaluated these methods on 6 QA\ndatasets, assessing the QA performance and efficiency. The results show that\nour approach matches the performance of complex LLM-based methods while\nachieving significant efficiency gains, demonstrating the potential of external\ninformation for adaptive retrieval."}
{"id": "2505.04310", "pdf": "https://arxiv.org/pdf/2505.04310", "abs": "https://arxiv.org/abs/2505.04310", "authors": ["Simo Alami C.", "Rim Kaddah", "Jesse Read", "Marie-Paule Cani"], "title": "Flow Models for Unbounded and Geometry-Aware Distributional Reinforcement Learning", "categories": ["cs.AI", "math.OC"], "comment": null, "summary": "We introduce a new architecture for Distributional Reinforcement Learning\n(DistRL) that models return distributions using normalizing flows. This\napproach enables flexible, unbounded support for return distributions, in\ncontrast to categorical approaches like C51 that rely on fixed or bounded\nrepresentations. It also offers richer modeling capacity to capture\nmulti-modality, skewness, and tail behavior than quantile based approaches. Our\nmethod is significantly more parameter-efficient than categorical approaches.\nStandard metrics used to train existing models like KL divergence or\nWasserstein distance either are scale insensitive or have biased sample\ngradients, especially when return supports do not overlap. To address this, we\npropose a novel surrogate for the Cram\\`er distance, that is geometry-aware and\ncomputable directly from the return distribution's PDF, avoiding the costly CDF\ncomputation. We test our model on the ATARI-5 sub-benchmark and show that our\napproach outperforms PDF based models while remaining competitive with quantile\nbased methods."}
{"id": "2505.03785", "pdf": "https://arxiv.org/pdf/2505.03785", "abs": "https://arxiv.org/abs/2505.03785", "authors": ["Eleftherios Tzanis", "Michail E. Klontzas"], "title": "mAIstro: an open-source multi-agentic system for automated end-to-end development of radiomics and deep learning models for medical imaging", "categories": ["cs.LG", "eess.IV"], "comment": null, "summary": "Agentic systems built on large language models (LLMs) offer promising\ncapabilities for automating complex workflows in healthcare AI. We introduce\nmAIstro, an open-source, autonomous multi-agentic framework for end-to-end\ndevelopment and deployment of medical AI models. The system orchestrates\nexploratory data analysis, radiomic feature extraction, image segmentation,\nclassification, and regression through a natural language interface, requiring\nno coding from the user. Built on a modular architecture, mAIstro supports both\nopen- and closed-source LLMs, and was evaluated using a large and diverse set\nof prompts across 16 open-source datasets, covering a wide range of imaging\nmodalities, anatomical regions, and data types. The agents successfully\nexecuted all tasks, producing interpretable outputs and validated models. This\nwork presents the first agentic framework capable of unifying data analysis, AI\nmodel development, and inference across varied healthcare applications,\noffering a reproducible and extensible foundation for clinical and research AI\nintegration. The code is available at: https://github.com/eltzanis/mAIstro"}
{"id": "2505.04284", "pdf": "https://arxiv.org/pdf/2505.04284", "abs": "https://arxiv.org/abs/2505.04284", "authors": ["Sofia Jamil", "Aryan Dabad", "Bollampalli Areen Reddy", "Sriparna Saha", "Rajiv Misra", "Adil A. Shakur"], "title": "GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer Pharmacovigilance", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the realm of cancer treatment, summarizing adverse drug events (ADEs)\nreported by patients using prescribed drugs is crucial for enhancing\npharmacovigilance practices and improving drug-related decision-making. While\nthe volume and complexity of pharmacovigilance data have increased, existing\nresearch in this field has predominantly focused on general diseases rather\nthan specifically addressing cancer. This work introduces the task of grouped\nsummarization of adverse drug events reported by multiple patients using the\nsame drug for cancer treatment. To address the challenge of limited resources\nin cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug\nReaction and Summarization (MCADRS) dataset. This dataset includes\npharmacovigilance posts detailing patient concerns regarding drug efficacy and\nadverse effects, along with extracted labels for drug names, adverse drug\nevents, severity, and adversity of reactions, as well as summaries of ADEs for\neach drug. Additionally, we propose the Grouping and Abstractive Summarization\nof Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that\ncombines the information extraction capabilities of Large Language Models\n(LLMs) with the summarization power of the encoder-decoder T5 model. Our work\nis the first to apply alignment techniques, including advanced algorithms like\nDirect Preference Optimization, to encoder-decoder models using synthetic\ndatasets for summarization tasks. Through extensive experiments, we demonstrate\nthe superior performance of GASCADE across various metrics, validated through\nboth automated assessments and human evaluations. This multitasking approach\nenhances drug-related decision-making and fosters a deeper understanding of\npatient concerns, paving the way for advancements in personalized and\nresponsive cancer care. The code and dataset used in this work are publicly\navailable."}
{"id": "2505.04313", "pdf": "https://arxiv.org/pdf/2505.04313", "abs": "https://arxiv.org/abs/2505.04313", "authors": ["Stephen Richard Varey", "Alessandro Di Stefano", "The Anh Han"], "title": "KERAIA: An Adaptive and Explainable Framework for Dynamic Knowledge Representation and Reasoning", "categories": ["cs.AI", "cs.ET", "cs.SC"], "comment": "22 pages", "summary": "In this paper, we introduce KERAIA, a novel framework and software platform\nfor symbolic knowledge engineering designed to address the persistent\nchallenges of representing, reasoning with, and executing knowledge in dynamic,\ncomplex, and context-sensitive environments. The central research question that\nmotivates this work is: How can unstructured, often tacit, human expertise be\neffectively transformed into computationally tractable algorithms that AI\nsystems can efficiently utilise? KERAIA seeks to bridge this gap by building on\nfoundational concepts such as Minsky's frame-based reasoning and K-lines, while\nintroducing significant innovations. These include Clouds of Knowledge for\ndynamic aggregation, Dynamic Relations (DRels) for context-sensitive\ninheritance, explicit Lines of Thought (LoTs) for traceable reasoning, and\nCloud Elaboration for adaptive knowledge transformation. This approach moves\nbeyond the limitations of traditional, often static, knowledge representation\nparadigms. KERAIA is designed with Explainable AI (XAI) as a core principle,\nensuring transparency and interpretability, particularly through the use of\nLoTs. The paper details the framework's architecture, the KSYNTH representation\nlanguage, and the General Purpose Paradigm Builder (GPPB) to integrate diverse\ninference methods within a unified structure. We validate KERAIA's versatility,\nexpressiveness, and practical applicability through detailed analysis of\nmultiple case studies spanning naval warfare simulation, industrial diagnostics\nin water treatment plants, and strategic decision-making in the game of RISK.\nFurthermore, we provide a comparative analysis against established knowledge\nrepresentation paradigms (including ontologies, rule-based systems, and\nknowledge graphs) and discuss the implementation aspects and computational\nconsiderations of the KERAIA platform."}
{"id": "2505.03786", "pdf": "https://arxiv.org/pdf/2505.03786", "abs": "https://arxiv.org/abs/2505.03786", "authors": ["Md Fahim Anjum"], "title": "When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator", "categories": ["cs.LG", "cs.CL"], "comment": "12 pages, 5 figures. Code available at:\n  https://github.com/MDFahimAnjum/llm-planning-with-reasoning", "summary": "Large Language Models (LLM) with reasoning capabilities offer a promising\npath for improving candidate evaluation in planning frameworks, but their\nrelative performance against traditional non-reasoning models remains largely\nunderexplored. In this study, we benchmark a distilled 1.5B parameter reasoning\nmodel (DeepSeek-R1) against several state-of-the-art non-reasoning LLMs within\na generator-discriminator LLM planning framework for the text-to-SQL task. For\nthis, we introduce a novel method for extracting soft scores from the\nchain-of-thought (CoT) outputs from reasoning that enables fine-grained ranking\nof candidates. Our central hypothesis is that reasoning models are more\neffective discriminators than non-reasoning LLMs. Our results show that\ndistilled DeepSeek-R1-1.5B achieves up to $87\\%$ higher F1 and $3.7\\%$ better\ndiscrimination accuracy than CodeLlama-7B, as well as $3.7\\%$ higher execution\naccuracy than CodeLlama-13B, despite having significantly fewer parameters.\nFurthermore, we find that there is a limit to the logical capabilities of\nreasoning models, and only providing more context or allowing more compute\nbudget for reasoning is not enough to improve their discrimination performance.\nFinally, we demonstrate that, unlike non-reasoning LLMs, reasoning models find\ngeneration more challenging than discrimination and may underperform as\ngenerators compared to smaller non-reasoning LLMs. Our work highlights the\npotential of reasoning models as discriminators in agentic frameworks, far\noutweighing their capabilities as generators, offering insights into their\noptimal role within LLM planning infrastructures."}
{"id": "2505.04388", "pdf": "https://arxiv.org/pdf/2505.04388", "abs": "https://arxiv.org/abs/2505.04388", "authors": ["Dario Garcia-Gasulla", "Jordi Bayarri-Planas", "Ashwin Kumar Gururajan", "Enrique Lopez-Cuena", "Adrian Tormos", "Daniel Hinjos", "Pablo Bernabeu-Perez", "Anna Arias-Duart", "Pablo Agustin Martin-Torres", "Marta Gonzalez-Mallo", "Sergio Alvarez-Napagao", "Eduard Ayguadé-Parra", "Ulises Cortés"], "title": "The Aloe Family Recipe for Open and Specialized Healthcare LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2405.01886", "summary": "Purpose: With advancements in Large Language Models (LLMs) for healthcare,\nthe need arises for competitive open-source models to protect the public\ninterest. This work contributes to the field of open medical LLMs by optimizing\nkey stages of data preprocessing and training, while showing how to improve\nmodel safety (through DPO) and efficacy (through RAG). The evaluation\nmethodology used, which includes four different types of tests, defines a new\nstandard for the field. The resultant models, shown to be competitive with the\nbest private alternatives, are released with a permisive license.\n  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,\nAloe Beta uses a custom dataset to enhance public data with synthetic Chain of\nThought examples. The models undergo alignment with Direct Preference\nOptimization, emphasizing ethical and policy-aligned performance in the\npresence of jailbreaking attacks. Evaluation includes close-ended, open-ended,\nsafety and human assessments, to maximize the reliability of results.\n  Results: Recommendations are made across the entire pipeline, backed by the\nsolid performance of the Aloe Family. These models deliver competitive\nperformance across healthcare benchmarks and medical fields, and are often\npreferred by healthcare professionals. On bias and toxicity, the Aloe Beta\nmodels significantly improve safety, showing resilience to unseen jailbreaking\nattacks. For a responsible release, a detailed risk assessment specific to\nhealthcare is attached to the Aloe Family models.\n  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a\nsignificant contribution to the open-source medical LLM field, offering\ntop-of-the-line performance while maintaining high ethical requirements. This\nwork sets a new standard for developing and reporting aligned LLMs in\nhealthcare."}
{"id": "2505.04317", "pdf": "https://arxiv.org/pdf/2505.04317", "abs": "https://arxiv.org/abs/2505.04317", "authors": ["Ruize Zhang", "Sirui Xiang", "Zelai Xu", "Feng Gao", "Shilong Ji", "Wenhao Tang", "Wenbo Ding", "Chao Yu", "Yu Wang"], "title": "Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "In this paper, we tackle the problem of learning to play 3v3 multi-drone\nvolleyball, a new embodied competitive task that requires both high-level\nstrategic coordination and low-level agile control. The task is turn-based,\nmulti-agent, and physically grounded, posing significant challenges due to its\nlong-horizon dependencies, tight inter-agent coupling, and the underactuated\ndynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play\n(HCSP), a hierarchical reinforcement learning framework that separates\ncentralized high-level strategic decision-making from decentralized low-level\nmotion control. We design a three-stage population-based training pipeline to\nenable both strategy and skill to emerge from scratch without expert\ndemonstrations: (I) training diverse low-level skills, (II) learning high-level\nstrategy via self-play with fixed low-level controllers, and (III) joint\nfine-tuning through co-self-play. Experiments show that HCSP achieves superior\nperformance, outperforming non-hierarchical self-play and rule-based\nhierarchical baselines with an average 82.9\\% win rate and a 71.5\\% win rate\nagainst the two-stage variant. Moreover, co-self-play leads to emergent team\nbehaviors such as role switching and coordinated formations, demonstrating the\neffectiveness of our hierarchical design and training scheme."}
{"id": "2505.03787", "pdf": "https://arxiv.org/pdf/2505.03787", "abs": "https://arxiv.org/abs/2505.03787", "authors": ["Zuraiz Baig", "Sidra Nasir", "Rizwan Ahmed Khan", "Muhammad Zeeshan Ul Haque"], "title": "ArrhythmiaVision: Resource-Conscious Deep Learning Models with Visual Explanations for ECG Arrhythmia Classification", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages and 08 figures", "summary": "Cardiac arrhythmias are a leading cause of life-threatening cardiac events,\nhighlighting the urgent need for accurate and timely detection.\nElectrocardiography (ECG) remains the clinical gold standard for arrhythmia\ndiagnosis; however, manual interpretation is time-consuming, dependent on\nclinical expertise, and prone to human error. Although deep learning has\nadvanced automated ECG analysis, many existing models abstract away the\nsignal's intrinsic temporal and morphological features, lack interpretability,\nand are computationally intensive-hindering their deployment on\nresource-constrained platforms. In this work, we propose two novel lightweight\n1D convolutional neural networks, ArrhythmiNet V1 and V2, optimized for\nefficient, real-time arrhythmia classification on edge devices. Inspired by\nMobileNet's depthwise separable convolutional design, these models maintain\nmemory footprints of just 302.18 KB and 157.76 KB, respectively, while\nachieving classification accuracies of 0.99 (V1) and 0.98 (V2) on the MIT-BIH\nArrhythmia Dataset across five classes: Normal Sinus Rhythm, Left Bundle Branch\nBlock, Right Bundle Branch Block, Atrial Premature Contraction, and Premature\nVentricular Contraction. In order to ensure clinical transparency and\nrelevance, we integrate Shapley Additive Explanations and Gradient-weighted\nClass Activation Mapping, enabling both local and global interpretability.\nThese techniques highlight physiologically meaningful patterns such as the QRS\ncomplex and T-wave that contribute to the model's predictions. We also discuss\nperformance-efficiency trade-offs and address current limitations related to\ndataset diversity and generalizability. Overall, our findings demonstrate the\nfeasibility of combining interpretability, predictive accuracy, and\ncomputational efficiency in practical, wearable, and embedded ECG monitoring\nsystems."}
{"id": "2505.04393", "pdf": "https://arxiv.org/pdf/2505.04393", "abs": "https://arxiv.org/abs/2505.04393", "authors": ["David Exler", "Mark Schutera", "Markus Reischl", "Luca Rettenberger"], "title": "Large Means Left: Political Bias in Large Language Models Increases with Their Number of Parameters", "categories": ["cs.CL"], "comment": null, "summary": "With the increasing prevalence of artificial intelligence, careful evaluation\nof inherent biases needs to be conducted to form the basis for alleviating the\neffects these predispositions can have on users. Large language models (LLMs)\nare predominantly used by many as a primary source of information for various\ntopics. LLMs frequently make factual errors, fabricate data (hallucinations),\nor present biases, exposing users to misinformation and influencing opinions.\nEducating users on their risks is key to responsible use, as bias, unlike\nhallucinations, cannot be caught through data verification. We quantify the\npolitical bias of popular LLMs in the context of the recent vote of the German\nBundestag using the score produced by the Wahl-O-Mat. This metric measures the\nalignment between an individual's political views and the positions of German\npolitical parties. We compare the models' alignment scores to identify factors\ninfluencing their political preferences. Doing so, we discover a bias toward\nleft-leaning parties, most dominant in larger LLMs. Also, we find that the\nlanguage we use to communicate with the models affects their political views.\nAdditionally, we analyze the influence of a model's origin and release date and\ncompare the results to the outcome of the recent vote of the Bundestag. Our\nresults imply that LLMs are prone to exhibiting political bias. Large\ncorporations with the necessary means to develop LLMs, thus, knowingly or\nunknowingly, have a responsibility to contain these biases, as they can\ninfluence each voter's decision-making process and inform public opinion in\ngeneral and at scale."}
{"id": "2505.04352", "pdf": "https://arxiv.org/pdf/2505.04352", "abs": "https://arxiv.org/abs/2505.04352", "authors": ["Simon Kolker", "Louise A. Dennis", "Ramon Fraga Pereira", "Mengwei Xu"], "title": "Uncertain Machine Ethics Planning", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "Machine Ethics decisions should consider the implications of uncertainty over\ndecisions. Decisions should be made over sequences of actions to reach\npreferable outcomes long term. The evaluation of outcomes, however, may invoke\none or more moral theories, which might have conflicting judgements. Each\ntheory will require differing representations of the ethical situation. For\nexample, Utilitarianism measures numerical values, Deontology analyses duties,\nand Virtue Ethics emphasises moral character. While balancing potentially\nconflicting moral considerations, decisions may need to be made, for example,\nto achieve morally neutral goals with minimal costs. In this paper, we\nformalise the problem as a Multi-Moral Markov Decision Process and a\nMulti-Moral Stochastic Shortest Path Problem. We develop a heuristic algorithm\nbased on Multi-Objective AO*, utilising Sven-Ove Hansson's Hypothetical\nRetrospection procedure for ethical reasoning under uncertainty. Our approach\nis validated by a case study from Machine Ethics literature: the problem of\nwhether to steal insulin for someone who needs it."}
{"id": "2505.03789", "pdf": "https://arxiv.org/pdf/2505.03789", "abs": "https://arxiv.org/abs/2505.03789", "authors": ["Syoiti Ninomiya", "Yuming Ma"], "title": "A new architecture of high-order deep neural networks that learn martingales", "categories": ["cs.LG", "math.PR", "q-fin.CP", "65C30, 60H35, 91G60, 68T07"], "comment": "19 pages, 3 figures", "summary": "A new deep-learning neural network architecture based on high-order weak\napproximation algorithms for stochastic differential equations (SDEs) is\nproposed. The architecture enables the efficient learning of martingales by\ndeep learning models. The behaviour of deep neural networks based on this\narchitecture, when applied to the problem of pricing financial derivatives, is\nalso examined. The core of this new architecture lies in the high-order weak\napproximation algorithms of the explicit Runge--Kutta type, wherein the\napproximation is realised solely through iterative compositions and linear\ncombinations of vector fields of the target SDEs."}
{"id": "2505.04406", "pdf": "https://arxiv.org/pdf/2505.04406", "abs": "https://arxiv.org/abs/2505.04406", "authors": ["Aidar Valeev", "Roman Garaev", "Vadim Lomshakov", "Irina Piontkovskaya", "Vladimir Ivanov", "Israel Adewuyi"], "title": "YABLoCo: Yet Another Benchmark for Long Context Code Generation", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": "Presented at LLM4Code 2025 Workshop co-located wtih ICSE 2025", "summary": "Large Language Models demonstrate the ability to solve various programming\ntasks, including code generation. Typically, the performance of LLMs is\nmeasured on benchmarks with small or medium-sized context windows of thousands\nof lines of code. At the same time, in real-world software projects,\nrepositories can span up to millions of LoC. This paper closes this gap by\ncontributing to the long context code generation benchmark (YABLoCo). The\nbenchmark featured a test set of 215 functions selected from four large\nrepositories with thousands of functions. The dataset contained metadata of\nfunctions, contexts of the functions with different levels of dependencies,\ndocstrings, functions bodies, and call graphs for each repository. This paper\npresents three key aspects of the contribution. First, the benchmark aims at\nfunction body generation in large repositories in C and C++, two languages not\ncovered by previous benchmarks. Second, the benchmark contains large\nrepositories from 200K to 2,000K LoC. Third, we contribute a scalable\nevaluation pipeline for efficient computing of the target metrics and a tool\nfor visual analysis of generated code. Overall, these three aspects allow for\nevaluating code generation in large repositories in C and C++."}
{"id": "2505.04480", "pdf": "https://arxiv.org/pdf/2505.04480", "abs": "https://arxiv.org/abs/2505.04480", "authors": ["Zhikai Zhao", "Chuanbo Hua", "Federico Berto", "Kanghoon Lee", "Zihan Ma", "Jiachen Li", "Jinkyoo Park"], "title": "TrajEvo: Designing Trajectory Prediction Heuristics via LLM-driven Evolution", "categories": ["cs.AI", "cs.NE", "cs.RO"], "comment": null, "summary": "Trajectory prediction is a crucial task in modeling human behavior,\nespecially in fields as social robotics and autonomous vehicle navigation.\nTraditional heuristics based on handcrafted rules often lack accuracy, while\nrecently proposed deep learning approaches suffer from computational cost, lack\nof explainability, and generalization issues that limit their practical\nadoption. In this paper, we introduce TrajEvo, a framework that leverages Large\nLanguage Models (LLMs) to automatically design trajectory prediction\nheuristics. TrajEvo employs an evolutionary algorithm to generate and refine\nprediction heuristics from past trajectory data. We introduce a\nCross-Generation Elite Sampling to promote population diversity and a\nStatistics Feedback Loop allowing the LLM to analyze alternative predictions.\nOur evaluations show TrajEvo outperforms previous heuristic methods on the\nETH-UCY datasets, and remarkably outperforms both heuristics and deep learning\nmethods when generalizing to the unseen SDD dataset. TrajEvo represents a first\nstep toward automated design of fast, explainable, and generalizable trajectory\nprediction heuristics. We make our source code publicly available to foster\nfuture research at https://github.com/ai4co/trajevo."}
{"id": "2505.03790", "pdf": "https://arxiv.org/pdf/2505.03790", "abs": "https://arxiv.org/abs/2505.03790", "authors": ["Yuren Zhang", "Zhongnan Pu", "Lei Jing"], "title": "A Time-Series Data Augmentation Model through Diffusion and Transformer Integration", "categories": ["cs.LG", "cs.AI"], "comment": "10 pages,22 figures", "summary": "With the development of Artificial Intelligence, numerous real-world tasks\nhave been accomplished using technology integrated with deep learning. To\nachieve optimal performance, deep neural networks typically require large\nvolumes of data for training. Although advances in data augmentation have\nfacilitated the acquisition of vast datasets, most of this data is concentrated\nin domains like images and speech. However, there has been relatively less\nfocus on augmenting time-series data. To address this gap and generate a\nsubstantial amount of time-series data, we propose a simple and effective\nmethod that combines the Diffusion and Transformer models. By utilizing an\nadjusted diffusion denoising model to generate a large volume of initial\ntime-step action data, followed by employing a Transformer model to predict\nsubsequent actions, and incorporating a weighted loss function to achieve\nconvergence, the method demonstrates its effectiveness. Using the performance\nimprovement of the model after applying augmented data as a benchmark, and\ncomparing the results with those obtained without data augmentation or using\ntraditional data augmentation methods, this approach shows its capability to\nproduce high-quality augmented data."}
{"id": "2505.04416", "pdf": "https://arxiv.org/pdf/2505.04416", "abs": "https://arxiv.org/abs/2505.04416", "authors": ["Xiaoyu Xu", "Minxin Du", "Qingqing Ye", "Haibo Hu"], "title": "OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "18 pages, 2 figures", "summary": "Large language models (LLMs) trained over extensive corpora risk memorizing\nsensitive, copyrighted, or toxic content. To address this, we propose\nOBLIVIATE, a robust unlearning framework that removes targeted data while\npreserving model utility. The framework follows a structured process:\nextracting target tokens, building retain sets, and fine-tuning with a tailored\nloss function comprising three components -- masking, distillation, and world\nfact. Using low-rank adapters (LoRA), it ensures efficiency without\ncompromising unlearning quality. We conduct experiments on multiple datasets,\nincluding the Harry Potter series, WMDP, and TOFU, using a comprehensive suite\nof metrics: forget quality (new document-level memorization score), model\nutility, and fluency. Results demonstrate its effectiveness in resisting\nmembership inference attacks, minimizing the impact on retained data, and\nmaintaining robustness across diverse scenarios."}
{"id": "2505.04525", "pdf": "https://arxiv.org/pdf/2505.04525", "abs": "https://arxiv.org/abs/2505.04525", "authors": ["Quentin Cohen-Solal", "Tristan Cazenave"], "title": "On some improvements to Unbounded Minimax", "categories": ["cs.AI"], "comment": null, "summary": "This paper presents the first experimental evaluation of four previously\nuntested modifications of Unbounded Best-First Minimax algorithm. This\nalgorithm explores the game tree by iteratively expanding the most promising\nsequences of actions based on the current partial game tree. We first evaluate\nthe use of transposition tables, which convert the game tree into a directed\nacyclic graph by merging duplicate states. Second, we compare the original\nalgorithm by Korf & Chickering with the variant proposed by Cohen-Solal, which\ndiffers in its backpropagation strategy: instead of stopping when a stable\nvalue is encountered, it updates values up to the root. This change slightly\nimproves performance when value ties or transposition tables are involved.\nThird, we assess replacing the exact terminal evaluation function with the\nlearned heuristic function. While beneficial when exact evaluations are costly,\nthis modification reduces performance in inexpensive settings. Finally, we\nexamine the impact of the completion technique that prioritizes resolved\nwinning states and avoids resolved losing states. This technique also improves\nperformance. Overall, our findings highlight how targeted modifications can\nenhance the efficiency of Unbounded Best-First Minimax."}
{"id": "2505.03791", "pdf": "https://arxiv.org/pdf/2505.03791", "abs": "https://arxiv.org/abs/2505.03791", "authors": ["Simon Golbert"], "title": "Practical Boolean Backpropagation", "categories": ["cs.LG", "cs.AI"], "comment": "11 pages", "summary": "Boolean neural networks offer hardware-efficient alternatives to real-valued\nmodels. While quantization is common, purely Boolean training remains\nunderexplored. We present a practical method for purely Boolean backpropagation\nfor networks based on a single specific gate we chose, operating directly in\nBoolean algebra involving no numerics. Initial experiments confirm its\nfeasibility."}
{"id": "2505.04507", "pdf": "https://arxiv.org/pdf/2505.04507", "abs": "https://arxiv.org/abs/2505.04507", "authors": ["Ilya Koziev"], "title": "Detecting Spelling and Grammatical Anomalies in Russian Poetry Texts", "categories": ["cs.CL"], "comment": null, "summary": "The quality of natural language texts in fine-tuning datasets plays a\ncritical role in the performance of generative models, particularly in\ncomputational creativity tasks such as poem or song lyric generation. Fluency\ndefects in generated poems significantly reduce their value. However, training\ntexts are often sourced from internet-based platforms without stringent quality\ncontrol, posing a challenge for data engineers to manage defect levels\neffectively.\n  To address this issue, we propose the use of automated linguistic anomaly\ndetection to identify and filter out low-quality texts from training datasets\nfor creative models. In this paper, we present a comprehensive comparison of\nunsupervised and supervised text anomaly detection approaches, utilizing both\nsynthetic and human-labeled datasets. We also introduce the RUPOR dataset, a\ncollection of Russian-language human-labeled poems designed for cross-sentence\ngrammatical error detection, and provide the full evaluation code. Our work\naims to empower the community with tools and insights to improve the quality of\ntraining datasets for generative models in creative domains."}
{"id": "2505.04528", "pdf": "https://arxiv.org/pdf/2505.04528", "abs": "https://arxiv.org/abs/2505.04528", "authors": ["Qi Liu", "Xinhao Zheng", "Renqiu Xia", "Xingzhi Qi", "Qinxiang Cao", "Junchi Yan"], "title": "Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving", "categories": ["cs.AI", "cs.CL", "cs.LO"], "comment": "42 pages, 3 figures", "summary": "As a seemingly self-explanatory task, problem-solving has been a significant\ncomponent of science and engineering. However, a general yet concrete\nformulation of problem-solving itself is missing. With the recent development\nof AI-based problem-solving agents, the demand for process-level verifiability\nis rapidly increasing yet underexplored. To fill these gaps, we present a\nprincipled formulation of problem-solving as a deterministic Markov decision\nprocess; a novel framework, FPS (Formal Problem-Solving), which utilizes\nexisting FTP (formal theorem proving) environments to perform process-verified\nproblem-solving; and D-FPS (Deductive FPS), decoupling solving and answer\nverification for better human-alignment. The expressiveness, soundness and\ncompleteness of the frameworks are proven. We construct three benchmarks on\nproblem-solving: FormalMath500, a formalization of a subset of the MATH500\nbenchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP\nbenchmarks MiniF2F and PutnamBench. For faithful, interpretable, and\nhuman-aligned evaluation, we propose RPE (Restricted Propositional\nEquivalence), a symbolic approach to determine the correctness of answers by\nformal verification. We evaluate four prevalent FTP models and two prompting\nmethods as baselines, solving at most 23.77% of FormalMath500, 27.47% of\nMiniF2F-Solving, and 0.31% of PutnamBench-Solving."}
{"id": "2505.03792", "pdf": "https://arxiv.org/pdf/2505.03792", "abs": "https://arxiv.org/abs/2505.03792", "authors": ["Lang Feng", "Weihao Tan", "Zhiyi Lyu", "Longtao Zheng", "Haiyang Xu", "Ming Yan", "Fei Huang", "Bo An"], "title": "Towards Efficient Online Tuning of VLM Agents via Counterfactual Soft Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "ICML 2025", "summary": "Online fine-tuning vision-language model (VLM) agents with reinforcement\nlearning (RL) has shown promise for equipping agents with multi-step,\ngoal-oriented capabilities in dynamic environments. However, their open-ended\ntextual action space and non-end-to-end nature of action generation present\nsignificant challenges to effective online exploration in RL, e.g., explosion\nof the exploration space. We propose a novel online fine-tuning method,\nCounterfactual Soft Reinforcement Learning (CoSo), better suited to the textual\noutput space of VLM agents. Compared to prior methods that assign uniform\nuncertainty to all tokens, CoSo leverages counterfactual reasoning to\ndynamically assess the causal influence of individual tokens on post-processed\nactions. By prioritizing the exploration of action-critical tokens while\nreducing the impact of semantically redundant or low-impact tokens, CoSo\nenables a more targeted and efficient online rollout process. We provide\ntheoretical analysis proving CoSo's convergence and policy improvement\nguarantees, and extensive empirical evaluations supporting CoSo's\neffectiveness. Our results across a diverse set of agent tasks, including\nAndroid device control, card gaming, and embodied AI, highlight its remarkable\nability to enhance exploration efficiency and deliver consistent performance\ngains. The code is available at https://github.com/langfengQ/CoSo."}
{"id": "2505.04519", "pdf": "https://arxiv.org/pdf/2505.04519", "abs": "https://arxiv.org/abs/2505.04519", "authors": ["Yehui Tang", "Yichun Yin", "Yaoyuan Wang", "Hang Zhou", "Yu Pan", "Wei Guo", "Ziyang Zhang", "Miao Rang", "Fangcheng Liu", "Naifu Zhang", "Binghan Li", "Yonghan Dong", "Xiaojun Meng", "Yasheng Wang", "Dong Li", "Yin Li", "Dandan Tu", "Can Chen", "Youliang Yan", "Fisher Yu", "Ruiming Tang", "Yunhe Wang", "Botian Huang", "Bo Wang", "Boxiao Liu", "Changzheng Zhang", "Da Kuang", "Fei Liu", "Gang Huang", "Jiansheng Wei", "Jiarui Qin", "Jie Ran", "Jinpeng Li", "Jun Zhao", "Liang Dai", "Lin Li", "Liqun Deng", "Peifeng Qin", "Pengyuan Zeng", "Qiang Gu", "Shaohua Tang", "Shengjun Cheng", "Tao Gao", "Tao Yu", "Tianshu Li", "Tianyu Bi", "Wei He", "Weikai Mao", "Wenyong Huang", "Wulong Liu", "Xiabing Li", "Xianzhi Yu", "Xueyu Wu", "Xu He", "Yangkai Du", "Yan Xu", "Ye Tian", "Yimeng Wu", "Yongbing Huang", "Yong Tian", "Yong Zhu", "Yue Li", "Yufei Wang", "Yuhang Gai", "Yujun Li", "Yu Luo", "Yunsheng Ni", "Yusen Sun", "Zelin Chen", "Zhe Liu", "Zhicheng Liu", "Zhipeng Tu", "Zilin Ding", "Zongyuan Zhan"], "title": "Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs", "categories": ["cs.CL"], "comment": null, "summary": "Sparse large language models (LLMs) with Mixture of Experts (MoE) and close\nto a trillion parameters are dominating the realm of most capable language\nmodels. However, the massive model scale poses significant challenges for the\nunderlying software and hardware systems. In this paper, we aim to uncover a\nrecipe to harness such scale on Ascend NPUs. The key goals are better usage of\nthe computing resources under the dynamic sparse model structures and\nmaterializing the expected performance gain on the actual hardware. To select\nmodel configurations suitable for Ascend NPUs without repeatedly running the\nexpensive experiments, we leverage simulation to compare the trade-off of\nvarious model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM\nwith 718 billion parameters, and we conducted experiments on the model to\nverify the simulation results. On the system side, we dig into Expert\nParallelism to optimize the communication between NPU devices to reduce the\nsynchronization overhead. We also optimize the memory efficiency within the\ndevices to further reduce the parameter and activation management overhead. In\nthe end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with\nperformance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and\ndemonstrate that the Ascend system is capable of harnessing all the training\nstages of the state-of-the-art language models. Extensive experiments indicate\nthat our recipe can lead to efficient training of large-scale sparse language\nmodels with MoE. We also study the behaviors of such models for future\nreference."}
{"id": "2505.04539", "pdf": "https://arxiv.org/pdf/2505.04539", "abs": "https://arxiv.org/abs/2505.04539", "authors": ["Ali Asadi", "Krishnendu Chatterjee", "Ehsan Kafshdar Goharshady", "Mehrdad Karrabi", "Ali Shafiee"], "title": "Qualitative Analysis of $ω$-Regular Objectives on Robust MDPs", "categories": ["cs.AI"], "comment": null, "summary": "Robust Markov Decision Processes (RMDPs) generalize classical MDPs that\nconsider uncertainties in transition probabilities by defining a set of\npossible transition functions. An objective is a set of runs (or infinite\ntrajectories) of the RMDP, and the value for an objective is the maximal\nprobability that the agent can guarantee against the adversarial environment.\nWe consider (a) reachability objectives, where given a target set of states,\nthe goal is to eventually arrive at one of them; and (b) parity objectives,\nwhich are a canonical representation for $\\omega$-regular objectives. The\nqualitative analysis problem asks whether the objective can be ensured with\nprobability 1.\n  In this work, we study the qualitative problem for reachability and parity\nobjectives on RMDPs without making any assumption over the structures of the\nRMDPs, e.g., unichain or aperiodic. Our contributions are twofold. We first\npresent efficient algorithms with oracle access to uncertainty sets that solve\nqualitative problems of reachability and parity objectives. We then report\nexperimental results demonstrating the effectiveness of our oracle-based\napproach on classical RMDP examples from the literature scaling up to thousands\nof states."}
{"id": "2505.03793", "pdf": "https://arxiv.org/pdf/2505.03793", "abs": "https://arxiv.org/abs/2505.03793", "authors": ["Xinyue Zeng", "Haohui Wang", "Junhong Lin", "Jun Wu", "Tyler Cody", "Dawei Zhou"], "title": "LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection", "categories": ["cs.LG", "cs.AI"], "comment": "It is accepted by ICML'2025, and the code is open-sourcing on\n  https://github.com/Susan571/LENSLLM.git", "summary": "The proliferation of open-sourced Large Language Models (LLMs) and diverse\ndownstream tasks necessitates efficient model selection, given the\nimpracticality of fine-tuning all candidates due to computational constraints.\nDespite the recent advances in LLM selection, a fundamental research question\nlargely remains nascent: how can we model the dynamic behaviors of LLMs during\nfine-tuning, thereby enhancing our understanding of their generalization\nperformance across diverse downstream tasks? In this work, we propose a novel\ntheoretical framework that provides a proper lens to assess the generalization\ncapabilities of LLMs, thereby enabling accurate and efficient LLM selection for\ndownstream applications. In particular, we first derive a Hessian-based\nPAC-Bayes generalization bound that unveils fine-tuning dynamics of LLMs and\nthen introduce LENSLLM, a Neural Tangent Kernel(NTK)-based Rectified Scaling\nModel that enables accurate performance predictions across diverse tasks while\nmaintaining computational efficiency. Extensive empirical results on 3\nlarge-scale benchmarks demonstrate that our model achieves up to 91.1% accuracy\nand reduces up to 88.5% computational cost in LLM selection, outperforming 5\nstate-of-the-art methods. We open-source our proposed LENSLLM model and\ncorresponding results at the Github link:\nhttps://github.com/Susan571/LENSLLM.git."}
{"id": "2505.04531", "pdf": "https://arxiv.org/pdf/2505.04531", "abs": "https://arxiv.org/abs/2505.04531", "authors": ["Josh McGiff", "Nikola S. Nikolov"], "title": "Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review", "categories": ["cs.CL", "cs.AI"], "comment": "This work is currently under review. Please do not cite without\n  permission", "summary": "Generative language modelling has surged in popularity with the emergence of\nservices such as ChatGPT and Google Gemini. While these models have\ndemonstrated transformative potential in productivity and communication, they\noverwhelmingly cater to high-resource languages like English. This has\namplified concerns over linguistic inequality in natural language processing\n(NLP). This paper presents the first systematic review focused specifically on\nstrategies to address data scarcity in generative language modelling for\nlow-resource languages (LRL). Drawing from 54 studies, we identify, categorise\nand evaluate technical approaches, including monolingual data augmentation,\nback-translation, multilingual training, and prompt engineering, across\ngenerative tasks. We also analyse trends in architecture choices, language\nfamily representation, and evaluation methods. Our findings highlight a strong\nreliance on transformer-based models, a concentration on a small subset of\nLRLs, and a lack of consistent evaluation across studies. We conclude with\nrecommendations for extending these methods to a wider range of LRLs and\noutline open challenges in building equitable generative language systems.\nUltimately, this review aims to support researchers and developers in building\ninclusive AI tools for underrepresented languages, a necessary step toward\nempowering LRL speakers and the preservation of linguistic diversity in a world\nincreasingly shaped by large-scale language technologies."}
{"id": "2504.13777", "pdf": "https://arxiv.org/pdf/2504.13777", "abs": "https://arxiv.org/abs/2504.13777", "authors": ["Anqi Shao"], "title": "Beyond Misinformation: A Conceptual Framework for Studying AI Hallucinations in (Science) Communication", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This paper proposes a conceptual framework for understanding AI\nhallucinations as a distinct form of misinformation. While misinformation\nscholarship has traditionally focused on human intent, generative AI systems\nnow produce false yet plausible outputs absent of such intent. I argue that\nthese AI hallucinations should not be treated merely as technical failures but\nas communication phenomena with social consequences. Drawing on a\nsupply-and-demand model and the concept of distributed agency, the framework\noutlines how hallucinations differ from human-generated misinformation in\nproduction, perception, and institutional response. I conclude by outlining a\nresearch agenda for communication scholars to investigate the emergence,\ndissemination, and audience reception of hallucinated content, with attention\nto macro (institutional), meso (group), and micro (individual) levels. This\nwork urges communication researchers to rethink the boundaries of\nmisinformation theory in light of probabilistic, non-human actors increasingly\nembedded in knowledge production."}
{"id": "2505.03794", "pdf": "https://arxiv.org/pdf/2505.03794", "abs": "https://arxiv.org/abs/2505.03794", "authors": ["İrfan Işik", "Ibrahim Karahan", "Okan Erkaymaz"], "title": "A Double Inertial Forward-Backward Splitting Algorithm With Applications to Regression and Classification Problems", "categories": ["cs.LG", "math.OC"], "comment": "20 pages, 5 sections, 5 figures, 5 tables", "summary": "This paper presents an improved forward-backward splitting algorithm with two\ninertial parameters. It aims to find a point in the real Hilbert space at which\nthe sum of a co-coercive operator and a maximal monotone operator vanishes.\nUnder standard assumptions, our proposed algorithm demonstrates weak\nconvergence. We present numerous experimental results to demonstrate the\nbehavior of the developed algorithm by comparing it with existing algorithms in\nthe literature for regression and data classification problems. Furthermore,\nthese implementations suggest our proposed algorithm yields superior outcomes\nwhen benchmarked against other relevant algorithms in existing literature."}
{"id": "2505.04588", "pdf": "https://arxiv.org/pdf/2505.04588", "abs": "https://arxiv.org/abs/2505.04588", "authors": ["Hao Sun", "Zile Qiao", "Jiayan Guo", "Xuanbo Fan", "Yingyan Hou", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Yan Zhang"], "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching", "categories": ["cs.CL"], "comment": null, "summary": "Effective information searching is essential for enhancing the reasoning and\ngeneration capabilities of large language models (LLMs). Recent research has\nexplored using reinforcement learning (RL) to improve LLMs' search capabilities\nby interacting with live search engines in real-world environments. While these\napproaches show promising results, they face two major challenges: (1)\nUncontrolled Document Quality: The quality of documents returned by search\nengines is often unpredictable, introducing noise and instability into the\ntraining process. (2) Prohibitively High API Costs: RL training requires\nfrequent rollouts, potentially involving hundreds of thousands of search\nrequests, which incur substantial API expenses and severely constrain\nscalability. To address these challenges, we introduce ZeroSearch, a\nreinforcement learning framework that incentivizes the search capabilities of\nLLMs without interacting with real search engines. Our approach begins with\nlightweight supervised fine-tuning to transform the LLM into a retrieval module\ncapable of generating both relevant and noisy documents in response to a query.\nDuring RL training, we employ a curriculum-based rollout strategy that\nincrementally degrades the quality of generated documents, progressively\neliciting the model's reasoning ability by exposing it to increasingly\nchallenging retrieval scenarios. Extensive experiments demonstrate that\nZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B\nLLM as the retrieval module. Remarkably, a 7B retrieval module achieves\ncomparable performance to the real search engine, while a 14B retrieval module\neven surpasses it. Furthermore, it generalizes well across both base and\ninstruction-tuned models of various parameter sizes and is compatible with a\nwide range of RL algorithms."}
{"id": "2505.03745", "pdf": "https://arxiv.org/pdf/2505.03745", "abs": "https://arxiv.org/abs/2505.03745", "authors": ["Yanbiao Liang", "Huihong Shi", "Haikuo Shao", "Zhongfeng Wang"], "title": "AccLLM: Accelerating Long-Context LLM Inference Via Algorithm-Hardware Co-Design", "categories": ["cs.AR", "cs.AI", "cs.LG"], "comment": null, "summary": "Recently, large language models (LLMs) have achieved huge success in the\nnatural language processing (NLP) field, driving a growing demand to extend\ntheir deployment from the cloud to edge devices. However, deploying LLMs on\nresource-constrained edge devices poses significant challenges, including (1)\nintensive computations and huge model sizes, (2) great memory and bandwidth\ndemands introduced by the autoregressive generation process, and (3) limited\nscalability for handling long sequences. To address these challenges, we\npropose AccLLM, a comprehensive acceleration framework that enables efficient\nand fast long-context LLM inference through algorithm and hardware co-design.\nAt the algorithmic level, we integrate (1) pruning, (2) {\\Lambda}-shaped\nattention, and (3) an innovative W2A8KV4 (2-bit weights, 8-bit activations, and\n4-bit KV cache) quantization scheme, thus effectively reducing memory and\nbandwidth requirements while facilitating LLMs' long-sequence generation. At\nthe hardware level, we design a dedicated FPGA-based accelerator with a\nreconfigurable computing engine to effectively and flexibly accommodate diverse\noperations arising from our compression algorithm, thereby fully translating\nthe algorithmic innovations into tangible hardware efficiency. We validate\nAccLLM on the Xilinx Alveo U280 FPGA, demonstrating a 4.07x energy efficiency\nand a 2.98x throughput compared to the state-of-the-art work FlightLLM."}
{"id": "2505.03797", "pdf": "https://arxiv.org/pdf/2505.03797", "abs": "https://arxiv.org/abs/2505.03797", "authors": ["Andrew Millard", "Joshua Murphy", "Simon Maskell", "Zheng Zhao"], "title": "Utilising Gradient-Based Proposals Within Sequential Monte Carlo Samplers for Training of Partial Bayesian Neural Networks", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Partial Bayesian neural networks (pBNNs) have been shown to perform\ncompetitively with fully Bayesian neural networks while only having a subset of\nthe parameters be stochastic. Using sequential Monte Carlo (SMC) samplers as\nthe inference method for pBNNs gives a non-parametric probabilistic estimation\nof the stochastic parameters, and has shown improved performance over\nparametric methods. In this paper we introduce a new SMC-based training method\nfor pBNNs by utilising a guided proposal and incorporating gradient-based\nMarkov kernels, which gives us better scalability on high dimensional problems.\nWe show that our new method outperforms the state-of-the-art in terms of\npredictive performance and optimal loss. We also show that pBNNs scale well\nwith larger batch sizes, resulting in significantly reduced training times and\noften better performance."}
{"id": "2505.03786", "pdf": "https://arxiv.org/pdf/2505.03786", "abs": "https://arxiv.org/abs/2505.03786", "authors": ["Md Fahim Anjum"], "title": "When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator", "categories": ["cs.LG", "cs.CL"], "comment": "12 pages, 5 figures. Code available at:\n  https://github.com/MDFahimAnjum/llm-planning-with-reasoning", "summary": "Large Language Models (LLM) with reasoning capabilities offer a promising\npath for improving candidate evaluation in planning frameworks, but their\nrelative performance against traditional non-reasoning models remains largely\nunderexplored. In this study, we benchmark a distilled 1.5B parameter reasoning\nmodel (DeepSeek-R1) against several state-of-the-art non-reasoning LLMs within\na generator-discriminator LLM planning framework for the text-to-SQL task. For\nthis, we introduce a novel method for extracting soft scores from the\nchain-of-thought (CoT) outputs from reasoning that enables fine-grained ranking\nof candidates. Our central hypothesis is that reasoning models are more\neffective discriminators than non-reasoning LLMs. Our results show that\ndistilled DeepSeek-R1-1.5B achieves up to $87\\%$ higher F1 and $3.7\\%$ better\ndiscrimination accuracy than CodeLlama-7B, as well as $3.7\\%$ higher execution\naccuracy than CodeLlama-13B, despite having significantly fewer parameters.\nFurthermore, we find that there is a limit to the logical capabilities of\nreasoning models, and only providing more context or allowing more compute\nbudget for reasoning is not enough to improve their discrimination performance.\nFinally, we demonstrate that, unlike non-reasoning LLMs, reasoning models find\ngeneration more challenging than discrimination and may underperform as\ngenerators compared to smaller non-reasoning LLMs. Our work highlights the\npotential of reasoning models as discriminators in agentic frameworks, far\noutweighing their capabilities as generators, offering insights into their\noptimal role within LLM planning infrastructures."}
{"id": "2505.03746", "pdf": "https://arxiv.org/pdf/2505.03746", "abs": "https://arxiv.org/abs/2505.03746", "authors": ["Silvia García-Méndez", "Francisco De Arriba-Pérez"], "title": "Promoting Security and Trust on Social Networks: Explainable Cyberbullying Detection Using Large Language Models in a Stream-Based Machine Learning Framework", "categories": ["cs.SI", "cs.AI"], "comment": null, "summary": "Social media platforms enable instant and ubiquitous connectivity and are\nessential to social interaction and communication in our technological society.\nApart from its advantages, these platforms have given rise to negative\nbehaviors in the online community, the so-called cyberbullying. Despite the\nmany works involving generative Artificial Intelligence (AI) in the literature\nlately, there remain opportunities to study its performance apart from\nzero/few-shot learning strategies. Accordingly, we propose an innovative and\nreal-time solution for cyberbullying detection that leverages stream-based\nMachine Learning (ML) models able to process the incoming samples incrementally\nand Large Language Models (LLMS) for feature engineering to address the\nevolving nature of abusive and hate speech online. An explainability dashboard\nis provided to promote the system's trustworthiness, reliability, and\naccountability. Results on experimental data report promising performance close\nto 90 % in all evaluation metrics and surpassing those obtained by competing\nworks in the literature. Ultimately, our proposal contributes to the safety of\nonline communities by timely detecting abusive behavior to prevent long-lasting\nharassment and reduce the negative consequences in society."}
{"id": "2505.03798", "pdf": "https://arxiv.org/pdf/2505.03798", "abs": "https://arxiv.org/abs/2505.03798", "authors": ["Yiqing Shen", "Hao Ding", "Lalithkumar Seenivasan", "Tianmin Shu", "Mathias Unberath"], "title": "Position: Foundation Models Need Digital Twin Representations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Current foundation models (FMs) rely on token representations that directly\nfragment continuous real-world multimodal data into discrete tokens. They limit\nFMs to learning real-world knowledge and relationships purely through\nstatistical correlation rather than leveraging explicit domain knowledge.\nConsequently, current FMs struggle with maintaining semantic coherence across\nmodalities, capturing fine-grained spatial-temporal dynamics, and performing\ncausal reasoning. These limitations cannot be overcome by simply scaling up\nmodel size or expanding datasets. This position paper argues that the machine\nlearning community should consider digital twin (DT) representations, which are\noutcome-driven digital representations that serve as building blocks for\ncreating virtual replicas of physical processes, as an alternative to the token\nrepresentation for building FMs. Finally, we discuss how DT representations can\naddress these challenges by providing physically grounded representations that\nexplicitly encode domain knowledge and preserve the continuous nature of\nreal-world processes."}
{"id": "2505.03799", "pdf": "https://arxiv.org/pdf/2505.03799", "abs": "https://arxiv.org/abs/2505.03799", "authors": ["Hyun Lee", "Chris Yi", "Maminur Islam", "B. D. S. Aritra"], "title": "Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "To be published in International Joint Conference on Neural Networks\n  (IJCNN), 2025", "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in various\nnatural language processing tasks; however, their application to graph-related\nproblems remains limited, primarily due to scalability constraints and the\nabsence of dedicated mechanisms for processing graph structures. Existing\napproaches predominantly integrate LLMs with Graph Neural Networks (GNNs),\nusing GNNs as feature encoders or auxiliary components. However, directly\nencoding graph structures within LLMs has been underexplored, particularly in\nthe context of large-scale graphs where token limitations hinder effective\nrepresentation. To address these challenges, we propose SDM-InstructGLM, a\nnovel instruction-tuned Graph Language Model (InstructGLM) framework that\nenhances scalability and efficiency without relying on GNNs. Our method\nintroduces a similarity-degree-based biased random walk mechanism, which\nselectively samples and encodes graph information based on node-feature\nsimilarity and degree centrality, ensuring an adaptive and structured\nrepresentation within the LLM. This approach significantly improves token\nefficiency, mitigates information loss due to random sampling, and enhances\nperformance on graph-based tasks such as node classification and link\nprediction. Furthermore, our results demonstrate the feasibility of LLM-only\ngraph processing, enabling scalable and interpretable Graph Language Models\n(GLMs) optimized through instruction-based fine-tuning. This work paves the way\nfor GNN-free approaches to graph learning, leveraging LLMs as standalone graph\nreasoning models. Our source code is available on GitHub."}
{"id": "2505.03747", "pdf": "https://arxiv.org/pdf/2505.03747", "abs": "https://arxiv.org/abs/2505.03747", "authors": ["Viktor Marek", "Ewa Orłowska", "Ivo Düntsch"], "title": "The Evolution of Rough Sets 1970s-1981", "categories": ["math.HO", "cs.AI"], "comment": null, "summary": "In this note research and publications by Zdzis{\\l}aw Pawlak and his\ncollaborators from 1970s and 1981 are recalled. Focus is placed on the sources\nof inspiration which one can identify on the basis of those publications.\nFinally, developments from 1981 related to rough sets and information systems\nare outlined."}
{"id": "2505.03799", "pdf": "https://arxiv.org/pdf/2505.03799", "abs": "https://arxiv.org/abs/2505.03799", "authors": ["Hyun Lee", "Chris Yi", "Maminur Islam", "B. D. S. Aritra"], "title": "Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "To be published in International Joint Conference on Neural Networks\n  (IJCNN), 2025", "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in various\nnatural language processing tasks; however, their application to graph-related\nproblems remains limited, primarily due to scalability constraints and the\nabsence of dedicated mechanisms for processing graph structures. Existing\napproaches predominantly integrate LLMs with Graph Neural Networks (GNNs),\nusing GNNs as feature encoders or auxiliary components. However, directly\nencoding graph structures within LLMs has been underexplored, particularly in\nthe context of large-scale graphs where token limitations hinder effective\nrepresentation. To address these challenges, we propose SDM-InstructGLM, a\nnovel instruction-tuned Graph Language Model (InstructGLM) framework that\nenhances scalability and efficiency without relying on GNNs. Our method\nintroduces a similarity-degree-based biased random walk mechanism, which\nselectively samples and encodes graph information based on node-feature\nsimilarity and degree centrality, ensuring an adaptive and structured\nrepresentation within the LLM. This approach significantly improves token\nefficiency, mitigates information loss due to random sampling, and enhances\nperformance on graph-based tasks such as node classification and link\nprediction. Furthermore, our results demonstrate the feasibility of LLM-only\ngraph processing, enabling scalable and interpretable Graph Language Models\n(GLMs) optimized through instruction-based fine-tuning. This work paves the way\nfor GNN-free approaches to graph learning, leveraging LLMs as standalone graph\nreasoning models. Our source code is available on GitHub."}
{"id": "2505.03810", "pdf": "https://arxiv.org/pdf/2505.03810", "abs": "https://arxiv.org/abs/2505.03810", "authors": ["Euntae Choi", "Sumin Song", "Woosang Lim", "Sungjoo Yoo"], "title": "Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "7 pages", "summary": "Large Language Models (LLMs) face deployment challenges due to high\ncomputational costs, and while Post-Training Quantization (PTQ) offers a\nsolution, existing rotation-based methods struggle at very low bit-widths like\n2-bit. We introduce a novel, training-free approach to construct an improved\nrotation matrix, addressing the limitations of current methods. The key\ncontributions include leveraging the Walsh-Hadamard transform with sequency\nordering, which clusters similar frequency components to reduce quantization\nerror compared to standard Hadamard matrices, significantly improving\nperformance. Furthermore, we propose a Grouped Sequency-arranged Rotation (GSR)\nusing block-diagonal matrices with smaller Walsh blocks, effectively isolating\noutlier impacts and achieving performance comparable to optimization-based\nmethods without requiring any training. Our method demonstrates robust\nperformance on reasoning tasks and Perplexity (PPL) score on WikiText-2. Our\nmethod also enhances results even when applied over existing learned rotation\ntechniques."}
{"id": "2505.03748", "pdf": "https://arxiv.org/pdf/2505.03748", "abs": "https://arxiv.org/abs/2505.03748", "authors": ["Yonghao Tan", "Pingcheng Dong", "Yongkun Wu", "Yu Liu", "Xuejiao Liu", "Peng Luo", "Shih-Yang Liu", "Xijie Huang", "Dong Zhang", "Luhong Liang", "Kwang-Ting Cheng"], "title": "APSQ: Additive Partial Sum Quantization with Algorithm-Hardware Co-Design", "categories": ["cs.AR", "cs.AI"], "comment": "62nd ACM/IEEE Design Automation Conference (DAC) 2025", "summary": "DNN accelerators, significantly advanced by model compression and specialized\ndataflow techniques, have marked considerable progress. However, the frequent\naccess of high-precision partial sums (PSUMs) leads to excessive memory demands\nin architectures utilizing input/weight stationary dataflows. Traditional\ncompression strategies have typically overlooked PSUM quantization, which may\naccount for 69% of power consumption. This study introduces a novel Additive\nPartial Sum Quantization (APSQ) method, seamlessly integrating PSUM\naccumulation into the quantization framework. A grouping strategy that combines\nAPSQ with PSUM quantization enhanced by a reconfigurable architecture is\nfurther proposed. The APSQ performs nearly lossless on NLP and CV tasks across\nBERT, Segformer, and EfficientViT models while compressing PSUMs to INT8. This\nleads to a notable reduction in energy costs by 28-87%. Extended experiments on\nLLaMA2-7B demonstrate the potential of APSQ for large language models. Code is\navailable at https://github.com/Yonghao-Tan/APSQ."}
{"id": "2505.03801", "pdf": "https://arxiv.org/pdf/2505.03801", "abs": "https://arxiv.org/abs/2505.03801", "authors": ["Changhai Zhou", "Qian Qiao", "Weizhong Zhang", "Cheng Jin"], "title": "Large Language Model Compression with Global Rank and Sparsity Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "22 pages, 5 figures", "summary": "Low-rank and sparse composite approximation is a natural idea to compress\nLarge Language Models (LLMs). However, such an idea faces two primary\nchallenges that adversely affect the performance of existing methods. The first\nchallenge relates to the interaction and cooperation between low-rank and\nsparse matrices, while the second involves determining weight allocation across\ndifferent layers, as redundancy varies considerably among them. To address\nthese challenges, we propose a novel two-stage LLM compression method with the\ncapability of global rank and sparsity optimization. It is noteworthy that the\noverall optimization space is vast, making comprehensive optimization\ncomputationally prohibitive. Therefore, to reduce the optimization space, our\nfirst stage utilizes robust principal component analysis to decompose the\nweight matrices of LLMs into low-rank and sparse components, which span the low\ndimensional and sparse spaces containing the resultant low-rank and sparse\nmatrices, respectively. In the second stage, we propose a probabilistic global\noptimization technique to jointly identify the low-rank and sparse structures\nwithin the above two spaces. The appealing feature of our approach is its\nability to automatically detect the redundancy across different layers and to\nmanage the interaction between the sparse and low-rank components. Extensive\nexperimental results indicate that our method significantly surpasses\nstate-of-the-art techniques for sparsification and composite approximation."}
{"id": "2505.03814", "pdf": "https://arxiv.org/pdf/2505.03814", "abs": "https://arxiv.org/abs/2505.03814", "authors": ["Ganghua Wang", "Zhaorun Chen", "Bo Li", "Haifeng Xu"], "title": "Cer-Eval: Certifiable and Cost-Efficient Evaluation Framework for LLMs", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "As foundation models continue to scale, the size of trained models grows\nexponentially, presenting significant challenges for their evaluation. Current\nevaluation practices involve curating increasingly large datasets to assess the\nperformance of large language models (LLMs). However, there is a lack of\nsystematic analysis and guidance on determining the sufficiency of test data or\nselecting informative samples for evaluation. This paper introduces a\ncertifiable and cost-efficient evaluation framework for LLMs. Our framework\nadapts to different evaluation objectives and outputs confidence intervals that\ncontain true values with high probability. We use ``test sample complexity'' to\nquantify the number of test points needed for a certifiable evaluation and\nderive tight bounds on test sample complexity. Based on the developed theory,\nwe develop a partition-based algorithm, named Cer-Eval, that adaptively selects\ntest points to minimize the cost of LLM evaluation. Real-world experiments\ndemonstrate that Cer-Eval can save 20% to 40% test points across various\nbenchmarks, while maintaining an estimation error level comparable to the\ncurrent evaluation process and providing a 95% confidence guarantee."}
{"id": "2505.03750", "pdf": "https://arxiv.org/pdf/2505.03750", "abs": "https://arxiv.org/abs/2505.03750", "authors": ["Jinhai Hu", "Wang Ling Goh", "Yuan Gao"], "title": "AI-Powered Agile Analog Circuit Design and Optimization", "categories": ["cs.AR", "cs.AI"], "comment": "3 pages, 5 figures, AI4X, 2025", "summary": "Artificial intelligence (AI) techniques are transforming analog circuit\ndesign by automating device-level tuning and enabling system-level\nco-optimization. This paper integrates two approaches: (1) AI-assisted\ntransistor sizing using Multi-Objective Bayesian Optimization (MOBO) for direct\ncircuit parameter optimization, demonstrated on a linearly tunable\ntransconductor; and (2) AI-integrated circuit transfer function modeling for\nsystem-level optimization in a keyword spotting (KWS) application, demonstrated\nby optimizing an analog bandpass filter within a machine learning training\nloop. The combined insights highlight how AI can improve analog performance,\nreduce design iteration effort, and jointly optimize analog components and\napplication-level metrics."}
{"id": "2505.03802", "pdf": "https://arxiv.org/pdf/2505.03802", "abs": "https://arxiv.org/abs/2505.03802", "authors": ["Changhai Zhou", "Yuhua Zhou", "Qian Qiao", "Weizhong Zhang", "Cheng Jin"], "title": "Efficient Fine-Tuning of Quantized Models via Adaptive Rank and Bitwidth", "categories": ["cs.LG", "cs.AI"], "comment": "24 pages, 6 figures", "summary": "QLoRA effectively combines low-bit quantization and LoRA to achieve\nmemory-friendly fine-tuning for large language models (LLM). Recently, methods\nbased on SVD for continuous update iterations to initialize LoRA matrices to\naccommodate quantization errors have generally failed to consistently improve\nperformance. Dynamic mixed precision is a natural idea for continuously\nimproving the fine-tuning performance of quantized models, but previous methods\noften optimize low-rank subspaces or quantization components separately,\nwithout considering their synergy. To address this, we propose\n\\textbf{QR-Adaptor}, a unified, gradient-free strategy that uses partial\ncalibration data to jointly search the quantization components and the rank of\nlow-rank spaces for each layer, thereby continuously improving model\nperformance. QR-Adaptor does not minimize quantization error but treats\nprecision and rank allocation as a discrete optimization problem guided by\nactual downstream performance and memory usage. Compared to state-of-the-art\n(SOTA) quantized LoRA fine-tuning methods, our approach achieves a 4.89\\%\naccuracy improvement on GSM8K, and in some cases even outperforms the 16-bit\nfine-tuned model while maintaining the memory footprint of the 4-bit setting."}
{"id": "2505.03828", "pdf": "https://arxiv.org/pdf/2505.03828", "abs": "https://arxiv.org/abs/2505.03828", "authors": ["Yogesh Gajula"], "title": "Sentiment-Aware Recommendation Systems in E-Commerce: A Review from a Natural Language Processing Perspective", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": "12 pages, 2 tables, 2 figures", "summary": "E-commerce platforms generate vast volumes of user feedback, such as star\nratings, written reviews, and comments. However, most recommendation engines\nrely primarily on numerical scores, often overlooking the nuanced opinions\nembedded in free text. This paper comprehensively reviews sentiment-aware\nrecommendation systems from a natural language processing perspective, covering\nadvancements from 2023 to early 2025. It highlights the benefits of integrating\nsentiment analysis into e-commerce recommenders to enhance prediction accuracy\nand explainability through detailed opinion extraction. Our survey categorizes\nrecent work into four main approaches: deep learning classifiers that combine\nsentiment embeddings with user item interactions, transformer based methods for\nnuanced feature extraction, graph neural networks that propagate sentiment\nsignals, and conversational recommenders that adapt in real time to user\nfeedback. We summarize model architectures and demonstrate how sentiment flows\nthrough recommendation pipelines, impacting dialogue-based suggestions. Key\nchallenges include handling noisy or sarcastic text, dynamic user preferences,\nand bias mitigation. Finally, we outline research gaps and provide a roadmap\nfor developing smarter, fairer, and more user-centric recommendation tools."}
{"id": "2505.03756", "pdf": "https://arxiv.org/pdf/2505.03756", "abs": "https://arxiv.org/abs/2505.03756", "authors": ["Hang Zhang", "Jiuchen Shi", "Yixiao Wang", "Quan Chen", "Yizhou Shan", "Minyi Guo"], "title": "Improving the Serving Performance of Multi-LoRA Large Language Models via Efficient LoRA and KV Cache Management", "categories": ["cs.AR", "cs.AI", "cs.LG", "cs.PF"], "comment": null, "summary": "Multiple Low-Rank Adapters (Multi-LoRAs) are gaining popularity for\ntask-specific Large Language Model (LLM) applications. For multi-LoRA serving,\ncaching hot KV caches and LoRA adapters in high bandwidth memory of\naccelerations can improve inference performance. However, existing Multi-LoRA\ninference systems fail to optimize serving performance like Time-To-First-Toke\n(TTFT), neglecting usage dependencies when caching LoRAs and KVs. We therefore\npropose FASTLIBRA, a Multi-LoRA caching system to optimize the serving\nperformance. FASTLIBRA comprises a dependency-aware cache manager and a\nperformance-driven cache swapper. The cache manager maintains the usage\ndependencies between LoRAs and KV caches during the inference with a unified\ncaching pool. The cache swapper determines the swap-in or out of LoRAs and KV\ncaches based on a unified cost model, when the HBM is idle or busy,\nrespectively. Experimental results show that ELORA reduces the TTFT by 63.4% on\naverage, compared to state-of-the-art works."}
{"id": "2505.03803", "pdf": "https://arxiv.org/pdf/2505.03803", "abs": "https://arxiv.org/abs/2505.03803", "authors": ["Chen Xu", "Yuxuan Yue", "Zukang Xu", "Xing Hu", "Jiangyong Yu", "Zhixuan Chen", "Sifan Zhou", "Zhihang Yuan", "Dawei Yang"], "title": "RWKVQuant: Quantizing the RWKV Family with Proxy Guided Hybrid of Scalar and Vector Quantization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "RWKV is a modern RNN architecture with comparable performance to Transformer,\nbut still faces challenges when deployed to resource-constrained devices. Post\nTraining Quantization (PTQ), which is a an essential technique to reduce model\nsize and inference latency, has been widely used in Transformer models.\nHowever, it suffers significant degradation of performance when applied to\nRWKV. This paper investigates and identifies two key constraints inherent in\nthe properties of RWKV: (1) Non-linear operators hinder the parameter-fusion of\nboth smooth- and rotation-based quantization, introducing extra computation\noverhead. (2) The larger amount of uniformly distributed weights poses\nchallenges for cluster-based quantization, leading to reduced accuracy. To this\nend, we propose RWKVQuant, a PTQ framework tailored for RWKV models, consisting\nof two novel techniques: (1) a coarse-to-fine proxy capable of adaptively\nselecting different quantization approaches by assessing the uniformity and\nidentifying outliers in the weights, and (2) a codebook optimization algorithm\nthat enhances the performance of cluster-based quantization methods for\nelement-wise multiplication in RWKV. Experiments show that RWKVQuant can\nquantize RWKV-6-14B into about 3-bit with less than 1% accuracy loss and 2.14x\nspeed up."}
{"id": "2505.03961", "pdf": "https://arxiv.org/pdf/2505.03961", "abs": "https://arxiv.org/abs/2505.03961", "authors": ["Gerrit Großmann", "Larisa Ivanova", "Sai Leela Poduru", "Mohaddeseh Tabrizian", "Islam Mesabah", "David A. Selby", "Sebastian J. Vollmer"], "title": "The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete", "categories": ["cs.AI", "cs.CL", "cs.MA", "I.2.11; I.2.7; I.6; J.4"], "comment": "16 pages, 8 figures. Code available at\n  https://github.com/storyagents25/story-agents", "summary": "According to Yuval Noah Harari, large-scale human cooperation is driven by\nshared narratives that encode common beliefs and values. This study explores\nwhether such narratives can similarly nudge LLM agents toward collaboration. We\nuse a finitely repeated public goods game in which LLM agents choose either\ncooperative or egoistic spending strategies. We prime agents with stories\nhighlighting teamwork to different degrees and test how this influences\nnegotiation outcomes. Our experiments explore four questions:(1) How do\nnarratives influence negotiation behavior? (2) What differs when agents share\nthe same story versus different ones? (3) What happens when the agent numbers\ngrow? (4) Are agents resilient against self-serving negotiators? We find that\nstory-based priming significantly affects negotiation strategies and success\nrates. Common stories improve collaboration, benefiting each agent. By\ncontrast, priming agents with different stories reverses this effect, and those\nagents primed toward self-interest prevail. We hypothesize that these results\ncarry implications for multi-agent system design and AI alignment."}
{"id": "2505.03760", "pdf": "https://arxiv.org/pdf/2505.03760", "abs": "https://arxiv.org/abs/2505.03760", "authors": ["Arishi Orra", "Aryan Bhambu", "Himanshu Choudhary", "Manoj Thakur", "Selvaraju Natarajan"], "title": "Deep Reinforcement Learning for Investor-Specific Portfolio Optimization: A Volatility-Guided Asset Selection Approach", "categories": ["q-fin.PM", "cs.AI", "math.OC"], "comment": null, "summary": "Portfolio optimization requires dynamic allocation of funds by balancing the\nrisk and return tradeoff under dynamic market conditions. With the recent\nadvancements in AI, Deep Reinforcement Learning (DRL) has gained prominence in\nproviding adaptive and scalable strategies for portfolio optimization. However,\nthe success of these strategies depends not only on their ability to adapt to\nmarket dynamics but also on the careful pre-selection of assets that influence\noverall portfolio performance. Incorporating the investor's preference in\npre-selecting assets for a portfolio is essential in refining their investment\nstrategies. This study proposes a volatility-guided DRL-based portfolio\noptimization framework that dynamically constructs portfolios based on\ninvestors' risk profiles. The Generalized Autoregressive Conditional\nHeteroscedasticity (GARCH) model is utilized for volatility forecasting of\nstocks and categorizes them based on their volatility as aggressive, moderate,\nand conservative. The DRL agent is then employed to learn an optimal investment\npolicy by interacting with the historical market data. The efficacy of the\nproposed methodology is established using stocks from the Dow $30$ index. The\nproposed investor-specific DRL-based portfolios outperformed the baseline\nstrategies by generating consistent risk-adjusted returns."}
{"id": "2505.03804", "pdf": "https://arxiv.org/pdf/2505.03804", "abs": "https://arxiv.org/abs/2505.03804", "authors": ["Xing Hu", "Zhixuan Chen", "Dawei Yang", "Zukang Xu", "Chen Xu", "Zhihang Yuan", "Sifan Zhou", "Jiangyong Yu"], "title": "MoEQuant: Enhancing Quantization for Mixture-of-Experts Large Language Models via Expert-Balanced Sampling and Affinity Guidance", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Mixture-of-Experts (MoE) large language models (LLMs), which leverage dynamic\nrouting and sparse activation to enhance efficiency and scalability, have\nachieved higher performance while reducing computational costs. However, these\nmodels face significant memory overheads, limiting their practical deployment\nand broader adoption. Post-training quantization (PTQ), a widely used method\nfor compressing LLMs, encounters severe accuracy degradation and diminished\ngeneralization performance when applied to MoE models. This paper investigates\nthe impact of MoE's sparse and dynamic characteristics on quantization and\nidentifies two primary challenges: (1) Inter-expert imbalance, referring to the\nuneven distribution of samples across experts, which leads to insufficient and\nbiased calibration for less frequently utilized experts; (2) Intra-expert\nimbalance, arising from MoE's unique aggregation mechanism, which leads to\nvarying degrees of correlation between different samples and their assigned\nexperts. To address these challenges, we propose MoEQuant, a novel quantization\nframework tailored for MoE LLMs. MoE-Quant includes two novel techniques: 1)\nExpert-Balanced Self-Sampling (EBSS) is an efficient sampling method that\nefficiently constructs a calibration set with balanced expert distributions by\nleveraging the cumulative probabilities of tokens and expert balance metrics as\nguiding factors. 2) Affinity-Guided Quantization (AGQ), which incorporates\naffinities between experts and samples into the quantization process, thereby\naccurately assessing the impact of individual samples on different experts\nwithin the MoE layer. Experiments demonstrate that MoEQuant achieves\nsubstantial performance gains (more than 10 points accuracy gain in the\nHumanEval for DeepSeekMoE-16B under 4-bit quantization) and boosts efficiency."}
{"id": "2505.03997", "pdf": "https://arxiv.org/pdf/2505.03997", "abs": "https://arxiv.org/abs/2505.03997", "authors": ["Prudhviraj Naidu", "Zixian Wang", "Leon Bergen", "Ramamohan Paturi"], "title": "Quiet Feature Learning in Algorithmic Tasks", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We train Transformer-based language models on ten foundational algorithmic\ntasks and observe pronounced phase transitions in their loss curves that\ndeviate from established power-law scaling trends. Over large ranges of\ncompute, the validation loss barely improves, then abruptly decreases. Probing\nthe models' internal representations reveals the learning of quiet features\nduring the stagnant phase, followed by sudden acquisition of loud features that\ncoincide with the sharp drop in loss. Our ablation experiments show that\ndisrupting a single learned feature can dramatically degrade performance,\nproviding evidence of their causal role in task performance. These findings\nchallenge the prevailing assumption that next-token predictive loss reliably\ntracks incremental progress; instead, key internal features may be developing\nbelow the surface until they coalesce, triggering a rapid performance gain."}
{"id": "2505.03763", "pdf": "https://arxiv.org/pdf/2505.03763", "abs": "https://arxiv.org/abs/2505.03763", "authors": ["Asad Aali", "Adney Cardoza", "Melissa Capo"], "title": "Splitwiser: Efficient LM inference with constrained resources", "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.LG"], "comment": null, "summary": "Efficient inference of LLMs remains a crucial challenge, with two main\nphases: a compute-intensive prompt computation and a memory-intensive token\ngeneration. Despite existing batching and scheduling techniques, token\ngeneration phases fail to fully utilize compute resources, especially when\ncompared to prompt computation phases. To address these challenges, we propose\nSplitwiser, a methodology that splits the two phases of an LLM inference\nrequest onto the same GPU, thereby reducing overhead and improving memory\naccess and cache utilization. By eliminating the need to transfer data across\ndevices, Splitwiser aims to minimize network-related overheads. In this report,\nwe describe the basic structure of our proposed pipeline while sharing\npreliminary results and analysis. We implement our proposed multiprocessing\ndesign on two widely-used and independent LLM architectures: Huggingface and\nvLLM. We open-source our code for the respective implementations: 1)\nHuggingface (https://github.com/asad-aali/splitwiser), and 2) vLLM\n(https://github.com/adney11/vllm-sysml)."}
{"id": "2505.03805", "pdf": "https://arxiv.org/pdf/2505.03805", "abs": "https://arxiv.org/abs/2505.03805", "authors": ["Nguyen Van Thanh"], "title": "Feature Optimization for Time Series Forecasting via Novel Randomized Uphill Climbing", "categories": ["cs.LG", "cs.PF"], "comment": null, "summary": "Randomized Uphill Climbing is a lightweight, stochastic search heuristic that\nhas delivered state of the art equity alpha factors for quantitative hedge\nfunds. I propose to generalize RUC into a model agnostic feature optimization\nframework for multivariate time series forecasting. The core idea is to\nsynthesize candidate feature programs by randomly composing operators from a\ndomain specific grammar, score candidates rapidly with inexpensive surrogate\nmodels on rolling windows, and filter instability via nested cross validation\nand information theoretic shrinkage. By decoupling feature discovery from GPU\nheavy deep learning, the method promises faster iteration cycles, lower energy\nconsumption, and greater interpretability. Societal relevance: accurate,\ntransparent forecasting tools empower resource constrained institutions, energy\nregulators, climate risk NGOs to make data driven decisions without proprietary\nblack box models."}
{"id": "2505.04066", "pdf": "https://arxiv.org/pdf/2505.04066", "abs": "https://arxiv.org/abs/2505.04066", "authors": ["Tuochao Chen", "Nicholas Batchelder", "Alisa Liu", "Noah Smith", "Shyamnath Gollakota"], "title": "LLAMAPIE: Proactive In-Ear Conversation Assistants", "categories": ["cs.LG", "cs.CL", "eess.AS"], "comment": null, "summary": "We introduce LlamaPIE, the first real-time proactive assistant designed to\nenhance human conversations through discreet, concise guidance delivered via\nhearable devices. Unlike traditional language models that require explicit user\ninvocation, this assistant operates in the background, anticipating user needs\nwithout interrupting conversations. We address several challenges, including\ndetermining when to respond, crafting concise responses that enhance\nconversations, leveraging knowledge of the user for context-aware assistance,\nand real-time, on-device processing. To achieve this, we construct a\nsemi-synthetic dialogue dataset and propose a two-model pipeline: a small model\nthat decides when to respond and a larger model that generates the response. We\nevaluate our approach on real-world datasets, demonstrating its effectiveness\nin providing helpful, unobtrusive assistance. User studies with our assistant,\nimplemented on Apple Silicon M2 hardware, show a strong preference for the\nproactive assistant over both a baseline with no assistance and a reactive\nmodel, highlighting the potential of LlamaPie to enhance live conversations."}
{"id": "2505.03764", "pdf": "https://arxiv.org/pdf/2505.03764", "abs": "https://arxiv.org/abs/2505.03764", "authors": ["Logan Larsh", "Raiyan Siddique", "Sarah Sharif Yaser Mike Banad"], "title": "Ultra-Low-Power Spiking Neurons in 7 nm FinFET Technology: A Comparative Analysis of Leaky Integrate-and-Fire, Morris-Lecar, and Axon-Hillock Architectures", "categories": ["cs.NE", "cs.AI", "cs.AR"], "comment": null, "summary": "Neuromorphic computing aims to replicate the brain's remarkable energy\nefficiency and parallel processing capabilities for large-scale artificial\nintelligence applications. In this work, we present a comprehensive comparative\nstudy of three spiking neuron circuit architectures-Leaky-Integrate-and-Fire\n(LIF), Morris-Lecar (ML), and Axon-Hillock (AH)-implemented in a 7 nm FinFET\ntechnology. Through extensive SPICE simulations, we explore the optimization of\nspiking frequency, energy per spike, and static power consumption. Our results\nshow that the AH design achieves the highest throughput, demonstrating\nmulti-gigahertz firing rates (up to 3 GHz) with attojoule energy costs. By\ncontrast, the ML architecture excels in subthreshold to near-threshold regimes,\noffering robust low-power operation (as low as 0.385 aJ/spike) and biological\nbursting behavior. Although LIF benefits from a decoupled current mirror for\nhigh-frequency operation, it exhibits slightly higher static leakage compared\nto ML and AH at elevated supply voltages. Comparisons with previous node\nimplementations (22 nm planar, 28 nm) reveal that 7 nm FinFETs can drastically\nboost energy efficiency and speed albeit at the cost of increased subthreshold\nleakage in deep subthreshold regions. By quantifying design trade-offs for each\nneuron architecture, our work provides a roadmap for optimizing spiking neuron\ncircuits in advanced nanoscale technologies to deliver neuromorphic hardware\ncapable of both ultra-low-power operation and high computational throughput."}
{"id": "2505.03806", "pdf": "https://arxiv.org/pdf/2505.03806", "abs": "https://arxiv.org/abs/2505.03806", "authors": ["Mehran Mazandarani", "Marzieh Najariyan"], "title": "Perception-Informed Neural Networks: Beyond Physics-Informed Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This article introduces Perception-Informed Neural Networks (PrINNs), a\nframework designed to incorporate perception-based information into neural\nnetworks, addressing both systems with known and unknown physics laws or\ndifferential equations. Moreover, PrINNs extend the concept of Physics-Informed\nNeural Networks (PINNs) and their variants, offering a platform for the\nintegration of diverse forms of perception precisiation, including singular,\nprobability distribution, possibility distribution, interval, and fuzzy graph.\nIn fact, PrINNs allow neural networks to model dynamical systems by integrating\nexpert knowledge and perception-based information through loss functions,\nenabling the creation of modern data-driven models. Some of the key\ncontributions include Mixture of Experts Informed Neural Networks (MOEINNs),\nwhich combine heterogeneous expert knowledge into the network, and\nTransformed-Knowledge Informed Neural Networks (TKINNs), which facilitate the\nincorporation of meta-information for enhanced model performance. Additionally,\nFuzzy-Informed Neural Networks (FINNs) as a modern class of fuzzy deep neural\nnetworks leverage fuzzy logic constraints within a deep learning architecture,\nallowing online training without pre-training and eliminating the need for\ndefuzzification. PrINNs represent a significant step forward in bridging the\ngap between traditional physics-based modeling and modern data-driven\napproaches, enabling neural networks to learn from both structured physics laws\nand flexible perception-based rules. This approach empowers neural networks to\noperate in uncertain environments, model complex systems, and discover new\nforms of differential equations, making PrINNs a powerful tool for advancing\ncomputational science and engineering."}
{"id": "2505.04171", "pdf": "https://arxiv.org/pdf/2505.04171", "abs": "https://arxiv.org/abs/2505.04171", "authors": ["Nouar Aldahoul", "Hazem Ibrahim", "Matteo Varvello", "Aaron Kaufman", "Talal Rahwan", "Yasir Zaki"], "title": "Large Language Models are often politically extreme, usually ideologically inconsistent, and persuasive even in informational contexts", "categories": ["cs.CY", "cs.CL"], "comment": "61 pages, 29 figures", "summary": "Large Language Models (LLMs) are a transformational technology, fundamentally\nchanging how people obtain information and interact with the world. As people\nbecome increasingly reliant on them for an enormous variety of tasks, a body of\nacademic research has developed to examine these models for inherent biases,\nespecially political biases, often finding them small. We challenge this\nprevailing wisdom. First, by comparing 31 LLMs to legislators, judges, and a\nnationally representative sample of U.S. voters, we show that LLMs' apparently\nsmall overall partisan preference is the net result of offsetting extreme views\non specific topics, much like moderate voters. Second, in a randomized\nexperiment, we show that LLMs can promulgate their preferences into political\npersuasiveness even in information-seeking contexts: voters randomized to\ndiscuss political issues with an LLM chatbot are as much as 5 percentage points\nmore likely to express the same preferences as that chatbot. Contrary to\nexpectations, these persuasive effects are not moderated by familiarity with\nLLMs, news consumption, or interest in politics. LLMs, especially those\ncontrolled by private companies or governments, may become a powerful and\ntargeted vector for political influence."}
{"id": "2505.03769", "pdf": "https://arxiv.org/pdf/2505.03769", "abs": "https://arxiv.org/abs/2505.03769", "authors": ["Yibo Hu", "Yiqiao Jin", "Meng Ye", "Ajay Divakaran", "Srijan Kumar"], "title": "The Influence of Text Variation on User Engagement in Cross-Platform Content Sharing", "categories": ["cs.SI", "cs.AI", "cs.IR"], "comment": null, "summary": "In today's cross-platform social media landscape, understanding factors that\ndrive engagement for multimodal content, especially text paired with visuals,\nremains complex. This study investigates how rewriting Reddit post titles\nadapted from YouTube video titles affects user engagement. First, we build and\nanalyze a large dataset of Reddit posts sharing YouTube videos, revealing that\n21% of post titles are minimally modified. Statistical analysis demonstrates\nthat title rewrites measurably improve engagement. Second, we design a\ncontrolled, multi-phase experiment to rigorously isolate the effects of textual\nvariations by neutralizing confounding factors like video popularity, timing,\nand community norms. Comprehensive statistical tests reveal that effective\ntitle rewrites tend to feature emotional resonance, lexical richness, and\nalignment with community-specific norms. Lastly, pairwise ranking prediction\nexperiments using a fine-tuned BERT classifier achieves 74% accuracy,\nsignificantly outperforming near-random baselines, including GPT-4o. These\nresults validate that our controlled dataset effectively minimizes confounding\neffects, allowing advanced models to both learn and demonstrate the impact of\ntextual features on engagement. By bridging quantitative rigor with qualitative\ninsights, this study uncovers engagement dynamics and offers a robust framework\nfor future cross-platform, multimodal content strategies."}
{"id": "2505.03808", "pdf": "https://arxiv.org/pdf/2505.03808", "abs": "https://arxiv.org/abs/2505.03808", "authors": ["Ioannis Nasios"], "title": "AI-driven multi-source data fusion for algal bloom severity classification in small inland water bodies: Leveraging Sentinel-2, DEM, and NOAA climate data", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Harmful algal blooms are a growing threat to inland water quality and public\nhealth worldwide, creating an urgent need for efficient, accurate, and\ncost-effective detection methods. This research introduces a high-performing\nmethodology that integrates multiple open-source remote sensing data with\nadvanced artificial intelligence models. Key data sources include Copernicus\nSentinel-2 optical imagery, the Copernicus Digital Elevation Model (DEM), and\nNOAA's High-Resolution Rapid Refresh (HRRR) climate data, all efficiently\nretrieved using platforms like Google Earth Engine (GEE) and Microsoft\nPlanetary Computer (MPC). The NIR and two SWIR bands from Sentinel-2, the\naltitude from the elevation model, the temperature and wind from NOAA as well\nas the longitude and latitude were the most important features. The approach\ncombines two types of machine learning models, tree-based models and a neural\nnetwork, into an ensemble for classifying algal bloom severity. While the tree\nmodels performed strongly on their own, incorporating a neural network added\nrobustness and demonstrated how deep learning models can effectively use\ndiverse remote sensing inputs. The method leverages high-resolution satellite\nimagery and AI-driven analysis to monitor algal blooms dynamically, and\nalthough initially developed for a NASA competition in the U.S., it shows\npotential for global application. The complete code is available for further\nadaptation and practical implementation, illustrating the convergence of remote\nsensing data and AI to address critical environmental challenges\n(https://github.com/IoannisNasios/HarmfulAlgalBloomDetection)."}
{"id": "2505.04192", "pdf": "https://arxiv.org/pdf/2505.04192", "abs": "https://arxiv.org/abs/2505.04192", "authors": ["Trinh T. L. Vuong", "Jin Tae Kwak"], "title": "VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present VideoPath-LLaVA, the first large multimodal model (LMM) in\ncomputational pathology that integrates three distinct image scenarios, single\npatch images, automatically keyframe-extracted clips, and manually segmented\nvideo pathology images, to mimic the natural diagnostic process of\npathologists. By generating detailed histological descriptions and culminating\nin a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives\nwith diagnostic reasoning.\n  Central to our approach is the VideoPath-Instruct dataset, comprising 4278\nvideo and diagnosis-specific chain-of-thought instructional pairs sourced from\neducational histopathology videos on YouTube. Although high-quality data is\ncritical for enhancing diagnostic reasoning, its creation is time-intensive and\nlimited in volume. To overcome this challenge, we transfer knowledge from\nexisting single-image instruction datasets to train on weakly annotated,\nkeyframe-extracted clips, followed by fine-tuning on manually segmented videos.\nVideoPath-LLaVA establishes a new benchmark in pathology video analysis and\noffers a promising foundation for future AI systems that support clinical\ndecision-making through integrated visual and diagnostic reasoning. Our code,\ndata, and model are publicly available at\nhttps://github.com/trinhvg/VideoPath-LLaVA."}
{"id": "2505.03780", "pdf": "https://arxiv.org/pdf/2505.03780", "abs": "https://arxiv.org/abs/2505.03780", "authors": ["Burkhard Ringlein", "Thomas Parnell", "Radu Stoica"], "title": "GPU Performance Portability needs Autotuning", "categories": ["cs.AR", "cs.AI", "cs.PL"], "comment": null, "summary": "As LLMs grow in complexity, achieving state-of-the-art performance requires\ntight co-design across algorithms, software, and hardware. Today's reliance on\na single dominant platform limits portability, creates vendor lock-in, and\nraises barriers for new AI hardware. In this work, we make the case for\ncombining just-in-time (JIT) compilation with kernel parameter autotuning to\nenable portable, state-of-the-art performance LLM execution without code\nchanges. Focusing on flash attention -- a widespread performance-critical LLM\nkernel -- we demonstrate that this approach explores up to 15x more kernel\nparameter configurations, produces significantly more diverse code across\nmultiple dimensions, and even outperforms vendor-optimized implementations by\nup to 230%, all while reducing kernel code size by 70x and eliminating manual\ncode optimizations. Our results highlight autotuning as a promising path to\nunlocking model portability across GPU vendors."}
{"id": "2505.03809", "pdf": "https://arxiv.org/pdf/2505.03809", "abs": "https://arxiv.org/abs/2505.03809", "authors": ["Suorong Yang", "Peng Ye", "Furao Shen", "Dongzhan Zhou"], "title": "When Dynamic Data Selection Meets Data Augmentation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Dynamic data selection aims to accelerate training with lossless performance.\nHowever, reducing training data inherently limits data diversity, potentially\nhindering generalization. While data augmentation is widely used to enhance\ndiversity, it is typically not optimized in conjunction with selection. As a\nresult, directly combining these techniques fails to fully exploit their\nsynergies. To tackle the challenge, we propose a novel online data training\nframework that, for the first time, unifies dynamic data selection and\naugmentation, achieving both training efficiency and enhanced performance. Our\nmethod estimates each sample's joint distribution of local density and\nmultimodal semantic consistency, allowing for the targeted selection of\naugmentation-suitable samples while suppressing the inclusion of noisy or\nambiguous data. This enables a more significant reduction in dataset size\nwithout sacrificing model generalization. Experimental results demonstrate that\nour method outperforms existing state-of-the-art approaches on various\nbenchmark datasets and architectures, e.g., reducing 50\\% training costs on\nImageNet-1k with lossless performance. Furthermore, our approach enhances noise\nresistance and improves model robustness, reinforcing its practical utility in\nreal-world scenarios."}
{"id": "2505.04364", "pdf": "https://arxiv.org/pdf/2505.04364", "abs": "https://arxiv.org/abs/2505.04364", "authors": ["Kai Ruan", "Mowen Huang", "Ji-Rong Wen", "Hao Sun"], "title": "Benchmarking LLMs' Swarm intelligence", "categories": ["cs.MA", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) show potential for complex reasoning, yet their\ncapacity for emergent coordination in Multi-Agent Systems (MAS) when operating\nunder strict constraints-such as limited local perception and communication,\ncharacteristic of natural swarms-remains largely unexplored, particularly\nconcerning the nuances of swarm intelligence. Existing benchmarks often do not\nfully capture the unique challenges of decentralized coordination that arise\nwhen agents operate with incomplete spatio-temporal information. To bridge this\ngap, we introduce SwarmBench, a novel benchmark designed to systematically\nevaluate the swarm intelligence capabilities of LLMs acting as decentralized\nagents. SwarmBench features five foundational MAS coordination tasks within a\nconfigurable 2D grid environment, forcing agents to rely primarily on local\nsensory input (k x k view) and local communication. We propose metrics for\ncoordination effectiveness and analyze emergent group dynamics. Evaluating\nseveral leading LLMs in a zero-shot setting, we find significant performance\nvariations across tasks, highlighting the difficulties posed by local\ninformation constraints. While some coordination emerges, results indicate\nlimitations in robust planning and strategy formation under uncertainty in\nthese decentralized scenarios. Assessing LLMs under swarm-like conditions is\ncrucial for realizing their potential in future decentralized systems. We\nrelease SwarmBench as an open, extensible toolkit-built upon a customizable and\nscalable physical system with defined mechanical properties. It provides\nenvironments, prompts, evaluation scripts, and the comprehensive experimental\ndatasets generated, aiming to foster reproducible research into LLM-based MAS\ncoordination and the theoretical underpinnings of Embodied MAS. Our code\nrepository is available at https://github.com/x66ccff/swarmbench."}
{"id": "2505.03787", "pdf": "https://arxiv.org/pdf/2505.03787", "abs": "https://arxiv.org/abs/2505.03787", "authors": ["Zuraiz Baig", "Sidra Nasir", "Rizwan Ahmed Khan", "Muhammad Zeeshan Ul Haque"], "title": "ArrhythmiaVision: Resource-Conscious Deep Learning Models with Visual Explanations for ECG Arrhythmia Classification", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages and 08 figures", "summary": "Cardiac arrhythmias are a leading cause of life-threatening cardiac events,\nhighlighting the urgent need for accurate and timely detection.\nElectrocardiography (ECG) remains the clinical gold standard for arrhythmia\ndiagnosis; however, manual interpretation is time-consuming, dependent on\nclinical expertise, and prone to human error. Although deep learning has\nadvanced automated ECG analysis, many existing models abstract away the\nsignal's intrinsic temporal and morphological features, lack interpretability,\nand are computationally intensive-hindering their deployment on\nresource-constrained platforms. In this work, we propose two novel lightweight\n1D convolutional neural networks, ArrhythmiNet V1 and V2, optimized for\nefficient, real-time arrhythmia classification on edge devices. Inspired by\nMobileNet's depthwise separable convolutional design, these models maintain\nmemory footprints of just 302.18 KB and 157.76 KB, respectively, while\nachieving classification accuracies of 0.99 (V1) and 0.98 (V2) on the MIT-BIH\nArrhythmia Dataset across five classes: Normal Sinus Rhythm, Left Bundle Branch\nBlock, Right Bundle Branch Block, Atrial Premature Contraction, and Premature\nVentricular Contraction. In order to ensure clinical transparency and\nrelevance, we integrate Shapley Additive Explanations and Gradient-weighted\nClass Activation Mapping, enabling both local and global interpretability.\nThese techniques highlight physiologically meaningful patterns such as the QRS\ncomplex and T-wave that contribute to the model's predictions. We also discuss\nperformance-efficiency trade-offs and address current limitations related to\ndataset diversity and generalizability. Overall, our findings demonstrate the\nfeasibility of combining interpretability, predictive accuracy, and\ncomputational efficiency in practical, wearable, and embedded ECG monitoring\nsystems."}
{"id": "2505.03810", "pdf": "https://arxiv.org/pdf/2505.03810", "abs": "https://arxiv.org/abs/2505.03810", "authors": ["Euntae Choi", "Sumin Song", "Woosang Lim", "Sungjoo Yoo"], "title": "Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "7 pages", "summary": "Large Language Models (LLMs) face deployment challenges due to high\ncomputational costs, and while Post-Training Quantization (PTQ) offers a\nsolution, existing rotation-based methods struggle at very low bit-widths like\n2-bit. We introduce a novel, training-free approach to construct an improved\nrotation matrix, addressing the limitations of current methods. The key\ncontributions include leveraging the Walsh-Hadamard transform with sequency\nordering, which clusters similar frequency components to reduce quantization\nerror compared to standard Hadamard matrices, significantly improving\nperformance. Furthermore, we propose a Grouped Sequency-arranged Rotation (GSR)\nusing block-diagonal matrices with smaller Walsh blocks, effectively isolating\noutlier impacts and achieving performance comparable to optimization-based\nmethods without requiring any training. Our method demonstrates robust\nperformance on reasoning tasks and Perplexity (PPL) score on WikiText-2. Our\nmethod also enhances results even when applied over existing learned rotation\ntechniques."}
{"id": "2505.04457", "pdf": "https://arxiv.org/pdf/2505.04457", "abs": "https://arxiv.org/abs/2505.04457", "authors": ["Shigeki Karita", "Yuma Koizumi", "Heiga Zen", "Haruko Ishikawa", "Robin Scheibler", "Michiel Bacchiani"], "title": "Miipher-2: A Universal Speech Restoration Model for Million-Hour Scale Data Restoration", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Training data cleaning is a new application for generative model-based speech\nrestoration (SR). This paper introduces Miipher-2, an SR model designed for\nmillion-hour scale data, for training data cleaning for large-scale generative\nmodels like large language models. Key challenges addressed include\ngeneralization to unseen languages, operation without explicit conditioning\n(e.g., text, speaker ID), and computational efficiency. Miipher-2 utilizes a\nfrozen, pre-trained Universal Speech Model (USM), supporting over 300\nlanguages, as a robust, conditioning-free feature extractor. To optimize\nefficiency and minimize memory, Miipher-2 incorporates parallel adapters for\npredicting clean USM features from noisy inputs and employs the WaneFit neural\nvocoder for waveform synthesis. These components were trained on 3,000 hours of\nmulti-lingual, studio-quality recordings with augmented degradations, while USM\nparameters remained fixed. Experimental results demonstrate Miipher-2's\nsuperior or comparable performance to conventional SR models in\nword-error-rate, speaker similarity, and both objective and subjective sound\nquality scores across all tested languages. Miipher-2 operates efficiently on\nconsumer-grade accelerators, achieving a real-time factor of 0.0078, enabling\nthe processing of a million-hour speech dataset in approximately three days\nusing only 100 such accelerators."}
{"id": "2505.03788", "pdf": "https://arxiv.org/pdf/2505.03788", "abs": "https://arxiv.org/abs/2505.03788", "authors": ["Trilok Padhi", "Ramneet Kaur", "Adam D. Cobb", "Manoj Acharya", "Anirban Roy", "Colin Samplawski", "Brian Matejek", "Alexander M. Berenbeim", "Nathaniel D. Bastian", "Susmit Jha"], "title": "Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "We introduce a novel approach for calibrating uncertainty quantification (UQ)\ntailored for multi-modal large language models (LLMs). Existing\nstate-of-the-art UQ methods rely on consistency among multiple responses\ngenerated by the LLM on an input query under diverse settings. However, these\napproaches often report higher confidence in scenarios where the LLM is\nconsistently incorrect. This leads to a poorly calibrated confidence with\nrespect to accuracy. To address this, we leverage cross-modal consistency in\naddition to self-consistency to improve the calibration of the multi-modal\nmodels. Specifically, we ground the textual responses to the visual inputs. The\nconfidence from the grounding model is used to calibrate the overall\nconfidence. Given that using a grounding model adds its own uncertainty in the\npipeline, we apply temperature scaling - a widely accepted parametric\ncalibration technique - to calibrate the grounding model's confidence in the\naccuracy of generated responses. We evaluate the proposed approach across\nmultiple multi-modal tasks, such as medical question answering (Slake) and\nvisual question answering (VQAv2), considering multi-modal models such as\nLLaVA-Med and LLaVA. The experiments demonstrate that the proposed framework\nachieves significantly improved calibration on both tasks."}
{"id": "2505.03811", "pdf": "https://arxiv.org/pdf/2505.03811", "abs": "https://arxiv.org/abs/2505.03811", "authors": ["Surajit Chakrabarty", "Rukma Talwadker", "Tridib Mukherjee"], "title": "ScarceGAN: Discriminative Classification Framework for Rare Class Identification for Longitudinal Data with Weak Prior", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper introduces ScarceGAN which focuses on identification of extremely\nrare or scarce samples from multi-dimensional longitudinal telemetry data with\nsmall and weak label prior. We specifically address: (i) severe scarcity in\npositive class, stemming from both underlying organic skew in the data, as well\nas extremely limited labels; (ii) multi-class nature of the negative samples,\nwith uneven density distributions and partially overlapping feature\ndistributions; and (iii) massively unlabelled data leading to tiny and weak\nprior on both positive and negative classes, and possibility of unseen or\nunknown behavior in the unlabelled set, especially in the negative class.\nAlthough related to PU learning problems, we contend that knowledge (or lack of\nit) on the negative class can be leveraged to learn the compliment of it (i.e.,\nthe positive class) better in a semi-supervised manner. To this effect,\nScarceGAN re-formulates semi-supervised GAN by accommodating weakly labelled\nmulti-class negative samples and the available positive samples. It relaxes the\nsupervised discriminator's constraint on exact differentiation between negative\nsamples by introducing a 'leeway' term for samples with noisy prior. We propose\nmodifications to the cost objectives of discriminator, in supervised and\nunsupervised path as well as that of the generator. For identifying risky\nplayers in skill gaming, this formulation in whole gives us a recall of over\n85% (~60% jump over vanilla semi-supervised GAN) on our scarce class with very\nminimal verbosity in the unknown space. Further ScarceGAN outperforms the\nrecall benchmarks established by recent GAN based specialized models for the\npositive imbalanced class identification and establishes a new benchmark in\nidentifying one of rare attack classes (0.09%) in the intrusion dataset from\nthe KDDCUP99 challenge."}
{"id": "2505.04528", "pdf": "https://arxiv.org/pdf/2505.04528", "abs": "https://arxiv.org/abs/2505.04528", "authors": ["Qi Liu", "Xinhao Zheng", "Renqiu Xia", "Xingzhi Qi", "Qinxiang Cao", "Junchi Yan"], "title": "Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving", "categories": ["cs.AI", "cs.CL", "cs.LO"], "comment": "42 pages, 3 figures", "summary": "As a seemingly self-explanatory task, problem-solving has been a significant\ncomponent of science and engineering. However, a general yet concrete\nformulation of problem-solving itself is missing. With the recent development\nof AI-based problem-solving agents, the demand for process-level verifiability\nis rapidly increasing yet underexplored. To fill these gaps, we present a\nprincipled formulation of problem-solving as a deterministic Markov decision\nprocess; a novel framework, FPS (Formal Problem-Solving), which utilizes\nexisting FTP (formal theorem proving) environments to perform process-verified\nproblem-solving; and D-FPS (Deductive FPS), decoupling solving and answer\nverification for better human-alignment. The expressiveness, soundness and\ncompleteness of the frameworks are proven. We construct three benchmarks on\nproblem-solving: FormalMath500, a formalization of a subset of the MATH500\nbenchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP\nbenchmarks MiniF2F and PutnamBench. For faithful, interpretable, and\nhuman-aligned evaluation, we propose RPE (Restricted Propositional\nEquivalence), a symbolic approach to determine the correctness of answers by\nformal verification. We evaluate four prevalent FTP models and two prompting\nmethods as baselines, solving at most 23.77% of FormalMath500, 27.47% of\nMiniF2F-Solving, and 0.31% of PutnamBench-Solving."}
{"id": "2505.03790", "pdf": "https://arxiv.org/pdf/2505.03790", "abs": "https://arxiv.org/abs/2505.03790", "authors": ["Yuren Zhang", "Zhongnan Pu", "Lei Jing"], "title": "A Time-Series Data Augmentation Model through Diffusion and Transformer Integration", "categories": ["cs.LG", "cs.AI"], "comment": "10 pages,22 figures", "summary": "With the development of Artificial Intelligence, numerous real-world tasks\nhave been accomplished using technology integrated with deep learning. To\nachieve optimal performance, deep neural networks typically require large\nvolumes of data for training. Although advances in data augmentation have\nfacilitated the acquisition of vast datasets, most of this data is concentrated\nin domains like images and speech. However, there has been relatively less\nfocus on augmenting time-series data. To address this gap and generate a\nsubstantial amount of time-series data, we propose a simple and effective\nmethod that combines the Diffusion and Transformer models. By utilizing an\nadjusted diffusion denoising model to generate a large volume of initial\ntime-step action data, followed by employing a Transformer model to predict\nsubsequent actions, and incorporating a weighted loss function to achieve\nconvergence, the method demonstrates its effectiveness. Using the performance\nimprovement of the model after applying augmented data as a benchmark, and\ncomparing the results with those obtained without data augmentation or using\ntraditional data augmentation methods, this approach shows its capability to\nproduce high-quality augmented data."}
{"id": "2505.03812", "pdf": "https://arxiv.org/pdf/2505.03812", "abs": "https://arxiv.org/abs/2505.03812", "authors": ["Tomaso Aste"], "title": "Information Filtering Networks: Theoretical Foundations, Generative Methodologies, and Real-World Applications", "categories": ["cs.LG"], "comment": null, "summary": "Information Filtering Networks (IFNs) provide a powerful framework for\nmodeling complex systems through globally sparse yet locally dense and\ninterpretable structures that capture multivariate dependencies. This review\noffers a comprehensive account of IFNs, covering their theoretical foundations,\nconstruction methodologies, and diverse applications. Tracing their origins\nfrom early network-based models to advanced formulations such as the\nTriangulated Maximally Filtered Graph (TMFG) and the Maximally Filtered Clique\nForest (MFCF), the paper highlights how IFNs address key challenges in\nhigh-dimensional data-driven modeling. IFNs and their construction\nmethodologies are intrinsically higher-order networks that generate simplicial\ncomplexes-structures that are only now becoming popular in the broader\nliterature. Applications span fields including finance, biology, psychology,\nand artificial intelligence, where IFNs improve interpretability, computational\nefficiency, and predictive performance. Special attention is given to their\nrole in graphical modeling, where IFNs enable the estimation of sparse inverse\ncovariance matrices with greater accuracy and scalability than traditional\napproaches like Graphical LASSO. Finally, the review discusses recent\ndevelopments that integrate IFNs with machine learning and deep learning,\nunderscoring their potential not only to bridge classical network theory with\ncontemporary data-driven paradigms, but also to shape the architectures of deep\nlearning models themselves."}
{"id": "2505.03791", "pdf": "https://arxiv.org/pdf/2505.03791", "abs": "https://arxiv.org/abs/2505.03791", "authors": ["Simon Golbert"], "title": "Practical Boolean Backpropagation", "categories": ["cs.LG", "cs.AI"], "comment": "11 pages", "summary": "Boolean neural networks offer hardware-efficient alternatives to real-valued\nmodels. While quantization is common, purely Boolean training remains\nunderexplored. We present a practical method for purely Boolean backpropagation\nfor networks based on a single specific gate we chose, operating directly in\nBoolean algebra involving no numerics. Initial experiments confirm its\nfeasibility."}
{"id": "2505.03818", "pdf": "https://arxiv.org/pdf/2505.03818", "abs": "https://arxiv.org/abs/2505.03818", "authors": ["Antonio Valerio Miceli-Barone", "Vaishak Belle", "Ali Payani"], "title": "Program Semantic Inequivalence Game with Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) can achieve strong performance on everyday\ncoding tasks, but they can fail on complex tasks that require non-trivial\nreasoning about program semantics. Finding training examples to teach LLMs to\nsolve these tasks can be challenging.\n  In this work, we explore a method to synthetically generate code reasoning\ntraining data based on a semantic inequivalence game SInQ: a generator agent\ncreates program variants that are semantically distinct, derived from a dataset\nof real-world programming tasks, while an evaluator agent has to identify input\nexamples that cause the original programs and the generated variants to diverge\nin their behaviour, with the agents training each other semi-adversarially. We\nprove that this setup enables theoretically unlimited improvement through\nself-play in the limit of infinite computational resources.\n  We evaluated our approach on multiple code generation and understanding\nbenchmarks, including cross-language vulnerability detection (Lu et al., 2021),\nwhere our method improves vulnerability detection in C/C++ code despite being\ntrained exclusively on Python code, and the challenging Python builtin\nidentifier swap benchmark (Miceli-Barone et al., 2023), showing that whereas\nmodern LLMs still struggle with this benchmark, our approach yields substantial\nimprovements.\n  We release the code needed to replicate the experiments, as well as the\ngenerated synthetic data, which can be used to fine-tune LLMs."}
{"id": "2505.03792", "pdf": "https://arxiv.org/pdf/2505.03792", "abs": "https://arxiv.org/abs/2505.03792", "authors": ["Lang Feng", "Weihao Tan", "Zhiyi Lyu", "Longtao Zheng", "Haiyang Xu", "Ming Yan", "Fei Huang", "Bo An"], "title": "Towards Efficient Online Tuning of VLM Agents via Counterfactual Soft Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "ICML 2025", "summary": "Online fine-tuning vision-language model (VLM) agents with reinforcement\nlearning (RL) has shown promise for equipping agents with multi-step,\ngoal-oriented capabilities in dynamic environments. However, their open-ended\ntextual action space and non-end-to-end nature of action generation present\nsignificant challenges to effective online exploration in RL, e.g., explosion\nof the exploration space. We propose a novel online fine-tuning method,\nCounterfactual Soft Reinforcement Learning (CoSo), better suited to the textual\noutput space of VLM agents. Compared to prior methods that assign uniform\nuncertainty to all tokens, CoSo leverages counterfactual reasoning to\ndynamically assess the causal influence of individual tokens on post-processed\nactions. By prioritizing the exploration of action-critical tokens while\nreducing the impact of semantically redundant or low-impact tokens, CoSo\nenables a more targeted and efficient online rollout process. We provide\ntheoretical analysis proving CoSo's convergence and policy improvement\nguarantees, and extensive empirical evaluations supporting CoSo's\neffectiveness. Our results across a diverse set of agent tasks, including\nAndroid device control, card gaming, and embodied AI, highlight its remarkable\nability to enhance exploration efficiency and deliver consistent performance\ngains. The code is available at https://github.com/langfengQ/CoSo."}
{"id": "2505.03819", "pdf": "https://arxiv.org/pdf/2505.03819", "abs": "https://arxiv.org/abs/2505.03819", "authors": ["Johannes Schneider"], "title": "Focus on the Likely: Test-time Instance-based Uncertainty Removal", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose two novel test-time fine-tuning methods to improve uncertain model\npredictions. Our methods require no auxiliary data and use the given test\ninstance only. Instead of performing a greedy selection of the most likely\nclass to make a prediction, we introduce an additional focus on the likely\nclasses step during inference. By applying a single-step gradient descent, we\nrefine predictions when an initial forward pass indicates high uncertainty.\nThis aligns predictions more closely with the ideal of assigning zero\nprobability to less plausible outcomes. Our theoretical discussion provides a\ndeeper understanding highlighting the impact on shared and non-shared features\namong (focus) classes. The experimental evaluation highlights accuracy gains on\nsamples exhibiting high decision uncertainty for a diverse set of models from\nboth the text and image domain using the same hyperparameters."}
{"id": "2505.03793", "pdf": "https://arxiv.org/pdf/2505.03793", "abs": "https://arxiv.org/abs/2505.03793", "authors": ["Xinyue Zeng", "Haohui Wang", "Junhong Lin", "Jun Wu", "Tyler Cody", "Dawei Zhou"], "title": "LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection", "categories": ["cs.LG", "cs.AI"], "comment": "It is accepted by ICML'2025, and the code is open-sourcing on\n  https://github.com/Susan571/LENSLLM.git", "summary": "The proliferation of open-sourced Large Language Models (LLMs) and diverse\ndownstream tasks necessitates efficient model selection, given the\nimpracticality of fine-tuning all candidates due to computational constraints.\nDespite the recent advances in LLM selection, a fundamental research question\nlargely remains nascent: how can we model the dynamic behaviors of LLMs during\nfine-tuning, thereby enhancing our understanding of their generalization\nperformance across diverse downstream tasks? In this work, we propose a novel\ntheoretical framework that provides a proper lens to assess the generalization\ncapabilities of LLMs, thereby enabling accurate and efficient LLM selection for\ndownstream applications. In particular, we first derive a Hessian-based\nPAC-Bayes generalization bound that unveils fine-tuning dynamics of LLMs and\nthen introduce LENSLLM, a Neural Tangent Kernel(NTK)-based Rectified Scaling\nModel that enables accurate performance predictions across diverse tasks while\nmaintaining computational efficiency. Extensive empirical results on 3\nlarge-scale benchmarks demonstrate that our model achieves up to 91.1% accuracy\nand reduces up to 88.5% computational cost in LLM selection, outperforming 5\nstate-of-the-art methods. We open-source our proposed LENSLLM model and\ncorresponding results at the Github link:\nhttps://github.com/Susan571/LENSLLM.git."}
{"id": "2505.03822", "pdf": "https://arxiv.org/pdf/2505.03822", "abs": "https://arxiv.org/abs/2505.03822", "authors": ["Hao Wu", "Jialiang Wang"], "title": "DRSLF: Double Regularized Second-Order Low-Rank Representation for Web Service QoS Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Quality-of-Service (QoS) data plays a crucial role in cloud service\nselection. Since users cannot access all services, QoS can be represented by a\nhigh-dimensional and incomplete (HDI) matrix. Latent factor analysis (LFA)\nmodels have been proven effective as low-rank representation techniques for\naddressing this issue. However, most LFA models rely on first-order optimizers\nand use L2-norm regularization, which can lead to lower QoS prediction\naccuracy. To address this issue, this paper proposes a double regularized\nsecond-order latent factor (DRSLF) model with two key ideas: a) integrating\nL1-norm and L2-norm regularization terms to enhance the low-rank representation\nperformance; b) incorporating second-order information by calculating the\nHessian-vector product in each conjugate gradient step. Experimental results on\ntwo real-world response-time QoS datasets demonstrate that DRSLF has a higher\nlow-rank representation capability than two baselines."}
{"id": "2505.03795", "pdf": "https://arxiv.org/pdf/2505.03795", "abs": "https://arxiv.org/abs/2505.03795", "authors": ["Jacob W. Crandall", "Jonathan Skaggs"], "title": "Modeling Human Behavior in a Strategic Network Game with Complex Group Dynamics", "categories": ["cs.SI", "cs.AI"], "comment": null, "summary": "Human networks greatly impact important societal outcomes, including wealth\nand health inequality, poverty, and bullying. As such, understanding human\nnetworks is critical to learning how to promote favorable societal outcomes. As\na step toward better understanding human networks, we compare and contrast\nseveral methods for learning models of human behavior in a strategic network\ngame called the Junior High Game (JHG). These modeling methods differ with\nrespect to the assumptions they use to parameterize human behavior (behavior\nvs. community-aware behavior) and the statistical moments they model (mean vs.\ndistribution). Results show that the highest-performing method models the\npopulation's distribution rather than the mean and assumes humans use\ncommunity-aware behavior rather than behavior matching. When applied to small\nsocieties (6-11 individuals), this learned model, called hCAB, closely mirrors\nthe population dynamics of human groups (with some differences). Additionally,\na user study reveals that human participants were unable to distinguish hCAB\nagents from other humans, thus illustrating that individual hCAB behavior\nplausibly mirrors human behavior in this strategic network game."}
{"id": "2505.03825", "pdf": "https://arxiv.org/pdf/2505.03825", "abs": "https://arxiv.org/abs/2505.03825", "authors": ["Anushiya Arunan", "Yan Qin", "Xiaoli Li", "Yuen Chau"], "title": "Intelligently Augmented Contrastive Tensor Factorization: Empowering Multi-dimensional Time Series Classification in Low-Data Environments", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted in Expert Systems with Applications (DOI pending)", "summary": "Classification of multi-dimensional time series from real-world systems\nrequire fine-grained learning of complex features such as cross-dimensional\ndependencies and intra-class variations-all under the practical challenge of\nlow training data availability. However, standard deep learning (DL) struggles\nto learn generalizable features in low-data environments due to model\noverfitting. We propose a versatile yet data-efficient framework, Intelligently\nAugmented Contrastive Tensor Factorization (ITA-CTF), to learn effective\nrepresentations from multi-dimensional time series. The CTF module learns core\nexplanatory components of the time series (e.g., sensor factors, temporal\nfactors), and importantly, their joint dependencies. Notably, unlike standard\ntensor factorization (TF), the CTF module incorporates a new contrastive loss\noptimization to induce similarity learning and class-awareness into the learnt\nrepresentations for better classification performance. To strengthen this\ncontrastive learning, the preceding ITA module generates targeted but\ninformative augmentations that highlight realistic intra-class patterns in the\noriginal data, while preserving class-wise properties. This is achieved by\ndynamically sampling a \"soft\" class prototype to guide the warping of each\nquery data sample, which results in an augmentation that is intelligently\npattern-mixed between the \"soft\" class prototype and the query sample. These\naugmentations enable the CTF module to recognize complex intra-class variations\ndespite the limited original training data, and seek out invariant class-wise\nproperties for accurate classification performance. The proposed method is\ncomprehensively evaluated on five different classification tasks. Compared to\nstandard TF and several DL benchmarks, notable performance improvements up to\n18.7% were achieved."}
{"id": "2505.03796", "pdf": "https://arxiv.org/pdf/2505.03796", "abs": "https://arxiv.org/abs/2505.03796", "authors": ["Lokesh Koli", "Shubham Kalra", "Rohan Thakur", "Anas Saifi", "Karanpreet Singh"], "title": "AI-Driven IRM: Transforming insider risk management with adaptive scoring and LLM-based threat detection", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Insider threats pose a significant challenge to organizational security,\noften evading traditional rule-based detection systems due to their subtlety\nand contextual nature. This paper presents an AI-powered Insider Risk\nManagement (IRM) system that integrates behavioral analytics, dynamic risk\nscoring, and real-time policy enforcement to detect and mitigate insider\nthreats with high accuracy and adaptability. We introduce a hybrid scoring\nmechanism - transitioning from the static PRISM model to an adaptive AI-based\nmodel utilizing an autoencoder neural network trained on expert-annotated user\nactivity data. Through iterative feedback loops and continuous learning, the\nsystem reduces false positives by 59% and improves true positive detection\nrates by 30%, demonstrating substantial gains in detection precision.\nAdditionally, the platform scales efficiently, processing up to 10 million log\nevents daily with sub-300ms query latency, and supports automated enforcement\nactions for policy violations, reducing manual intervention. The IRM system's\ndeployment resulted in a 47% reduction in incident response times, highlighting\nits operational impact. Future enhancements include integrating explainable AI,\nfederated learning, graph-based anomaly detection, and alignment with Zero\nTrust principles to further elevate its adaptability, transparency, and\ncompliance-readiness. This work establishes a scalable and proactive framework\nfor mitigating emerging insider risks in both on-premises and hybrid\nenvironments."}
{"id": "2505.03827", "pdf": "https://arxiv.org/pdf/2505.03827", "abs": "https://arxiv.org/abs/2505.03827", "authors": ["Xin Wang", "Ling Feng", "Huijun Zhang", "Lei Cao", "Kaisheng Zeng", "Qi Li", "Yang Ding", "Yi Dai", "David Clifton"], "title": "MISE: Meta-knowledge Inheritance for Social Media-Based Stressor Estimation", "categories": ["cs.LG", "cs.AI"], "comment": "WWW2025, Oral Presentation", "summary": "Stress haunts people in modern society, which may cause severe health issues\nif left unattended. With social media becoming an integral part of daily life,\nleveraging social media to detect stress has gained increasing attention. While\nthe majority of the work focuses on classifying stress states and stress\ncategories, this study introduce a new task aimed at estimating more specific\nstressors (like exam, writing paper, etc.) through users' posts on social\nmedia. Unfortunately, the diversity of stressors with many different classes\nbut a few examples per class, combined with the consistent arising of new\nstressors over time, hinders the machine understanding of stressors. To this\nend, we cast the stressor estimation problem within a practical scenario\nfew-shot learning setting, and propose a novel meta-learning based stressor\nestimation framework that is enhanced by a meta-knowledge inheritance\nmechanism. This model can not only learn generic stressor context through\nmeta-learning, but also has a good generalization ability to estimate new\nstressors with little labeled data. A fundamental breakthrough in our approach\nlies in the inclusion of the meta-knowledge inheritance mechanism, which equips\nour model with the ability to prevent catastrophic forgetting when adapting to\nnew stressors. The experimental results show that our model achieves\nstate-of-the-art performance compared with the baselines. Additionally, we\nconstruct a social media-based stressor estimation dataset that can help train\nartificial intelligence models to facilitate human well-being. The dataset is\nnow public at\n\\href{https://www.kaggle.com/datasets/xinwangcs/stressor-cause-of-mental-health-problem-dataset}{\\underline{Kaggle}}\nand\n\\href{https://huggingface.co/datasets/XinWangcs/Stressor}{\\underline{Hugging\nFace}}."}
{"id": "2505.03798", "pdf": "https://arxiv.org/pdf/2505.03798", "abs": "https://arxiv.org/abs/2505.03798", "authors": ["Yiqing Shen", "Hao Ding", "Lalithkumar Seenivasan", "Tianmin Shu", "Mathias Unberath"], "title": "Position: Foundation Models Need Digital Twin Representations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Current foundation models (FMs) rely on token representations that directly\nfragment continuous real-world multimodal data into discrete tokens. They limit\nFMs to learning real-world knowledge and relationships purely through\nstatistical correlation rather than leveraging explicit domain knowledge.\nConsequently, current FMs struggle with maintaining semantic coherence across\nmodalities, capturing fine-grained spatial-temporal dynamics, and performing\ncausal reasoning. These limitations cannot be overcome by simply scaling up\nmodel size or expanding datasets. This position paper argues that the machine\nlearning community should consider digital twin (DT) representations, which are\noutcome-driven digital representations that serve as building blocks for\ncreating virtual replicas of physical processes, as an alternative to the token\nrepresentation for building FMs. Finally, we discuss how DT representations can\naddress these challenges by providing physically grounded representations that\nexplicitly encode domain knowledge and preserve the continuous nature of\nreal-world processes."}
{"id": "2505.03849", "pdf": "https://arxiv.org/pdf/2505.03849", "abs": "https://arxiv.org/abs/2505.03849", "authors": ["Jonathan Gorard", "Ammar Hakim", "Hong Qin", "Kyle Parfrey", "Shantenu Jha"], "title": "Improved Dimensionality Reduction for Inverse Problems in Nuclear Fusion and High-Energy Astrophysics", "categories": ["cs.LG", "astro-ph.IM", "nucl-th"], "comment": "2 pages. Position paper accepted to DOE-ASCR Inverse Methods for\n  Complex Systems under Uncertainty Workshop (Rockville, MD, United States,\n  June 10-12, 2025)", "summary": "Many inverse problems in nuclear fusion and high-energy astrophysics\nresearch, such as the optimization of tokamak reactor geometries or the\ninference of black hole parameters from interferometric images, necessitate\nhigh-dimensional parameter scans and large ensembles of simulations to be\nperformed. Such inverse problems typically involve large uncertainties, both in\nthe measurement parameters being inverted and in the underlying physics models\nthemselves. Monte Carlo sampling, when combined with modern non-linear\ndimensionality reduction techniques such as autoencoders and manifold learning,\ncan be used to reduce the size of the parameter spaces considerably. However,\nthere is no guarantee that the resulting combinations of parameters will be\nphysically valid, or even mathematically consistent. In this position paper, we\nadvocate adopting a hybrid approach that leverages our recent advances in the\ndevelopment of formal verification methods for numerical algorithms, with the\ngoal of constructing parameter space restrictions with provable mathematical\nand physical correctness properties, whilst nevertheless respecting both\nexperimental uncertainties and uncertainties in the underlying physical\nprocesses."}
{"id": "2505.03799", "pdf": "https://arxiv.org/pdf/2505.03799", "abs": "https://arxiv.org/abs/2505.03799", "authors": ["Hyun Lee", "Chris Yi", "Maminur Islam", "B. D. S. Aritra"], "title": "Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "To be published in International Joint Conference on Neural Networks\n  (IJCNN), 2025", "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in various\nnatural language processing tasks; however, their application to graph-related\nproblems remains limited, primarily due to scalability constraints and the\nabsence of dedicated mechanisms for processing graph structures. Existing\napproaches predominantly integrate LLMs with Graph Neural Networks (GNNs),\nusing GNNs as feature encoders or auxiliary components. However, directly\nencoding graph structures within LLMs has been underexplored, particularly in\nthe context of large-scale graphs where token limitations hinder effective\nrepresentation. To address these challenges, we propose SDM-InstructGLM, a\nnovel instruction-tuned Graph Language Model (InstructGLM) framework that\nenhances scalability and efficiency without relying on GNNs. Our method\nintroduces a similarity-degree-based biased random walk mechanism, which\nselectively samples and encodes graph information based on node-feature\nsimilarity and degree centrality, ensuring an adaptive and structured\nrepresentation within the LLM. This approach significantly improves token\nefficiency, mitigates information loss due to random sampling, and enhances\nperformance on graph-based tasks such as node classification and link\nprediction. Furthermore, our results demonstrate the feasibility of LLM-only\ngraph processing, enabling scalable and interpretable Graph Language Models\n(GLMs) optimized through instruction-based fine-tuning. This work paves the way\nfor GNN-free approaches to graph learning, leveraging LLMs as standalone graph\nreasoning models. Our source code is available on GitHub."}
{"id": "2505.03861", "pdf": "https://arxiv.org/pdf/2505.03861", "abs": "https://arxiv.org/abs/2505.03861", "authors": ["Kyunghyun Cho"], "title": "Machine Learning: a Lecture Note", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "This lecture note is intended to prepare early-year master's and PhD students\nin data science or a related discipline with foundational ideas in machine\nlearning. It starts with basic ideas in modern machine learning with\nclassification as a main target task. These basic ideas include loss\nformulation, backpropagation, stochastic gradient descent, generalization,\nmodel selection as well as fundamental blocks of artificial neural networks.\nBased on these basic ideas, the lecture note explores in depth the probablistic\napproach to unsupervised learning, covering directed latent variable models,\nproduct of experts, generative adversarial networks and autoregressive models.\nFinally, the note ends by covering a diverse set of further topics, such as\nreinforcement learning, ensemble methods and meta-learning. After reading this\nlecture note, a student should be ready to embark on studying and researching\nmore advanced topics in machine learning and more broadly artificial\nintelligence."}
{"id": "2505.03801", "pdf": "https://arxiv.org/pdf/2505.03801", "abs": "https://arxiv.org/abs/2505.03801", "authors": ["Changhai Zhou", "Qian Qiao", "Weizhong Zhang", "Cheng Jin"], "title": "Large Language Model Compression with Global Rank and Sparsity Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "22 pages, 5 figures", "summary": "Low-rank and sparse composite approximation is a natural idea to compress\nLarge Language Models (LLMs). However, such an idea faces two primary\nchallenges that adversely affect the performance of existing methods. The first\nchallenge relates to the interaction and cooperation between low-rank and\nsparse matrices, while the second involves determining weight allocation across\ndifferent layers, as redundancy varies considerably among them. To address\nthese challenges, we propose a novel two-stage LLM compression method with the\ncapability of global rank and sparsity optimization. It is noteworthy that the\noverall optimization space is vast, making comprehensive optimization\ncomputationally prohibitive. Therefore, to reduce the optimization space, our\nfirst stage utilizes robust principal component analysis to decompose the\nweight matrices of LLMs into low-rank and sparse components, which span the low\ndimensional and sparse spaces containing the resultant low-rank and sparse\nmatrices, respectively. In the second stage, we propose a probabilistic global\noptimization technique to jointly identify the low-rank and sparse structures\nwithin the above two spaces. The appealing feature of our approach is its\nability to automatically detect the redundancy across different layers and to\nmanage the interaction between the sparse and low-rank components. Extensive\nexperimental results indicate that our method significantly surpasses\nstate-of-the-art techniques for sparsification and composite approximation."}
{"id": "2505.03911", "pdf": "https://arxiv.org/pdf/2505.03911", "abs": "https://arxiv.org/abs/2505.03911", "authors": ["Hans Hohenfeld", "Marius Beuerle", "Elie Mounzer"], "title": "Explaining Anomalies with Tensor Networks", "categories": ["cs.LG", "quant-ph"], "comment": "6 pages, 3 figures", "summary": "Tensor networks, a class of variational quantum many-body wave functions have\nattracted considerable research interest across many disciplines, including\nclassical machine learning. Recently, Aizpurua et al. demonstrated explainable\nanomaly detection with matrix product states on a discrete-valued\ncyber-security task, using quantum-inspired methods to gain insight into the\nlearned model and detected anomalies. Here, we extend this framework to\nreal-valued data domains. We furthermore introduce tree tensor networks for the\ntask of explainable anomaly detection. We demonstrate these methods with three\nbenchmark problems, show adequate predictive performance compared to several\nbaseline models and both tensor network architectures' ability to explain\nanomalous samples. We thereby extend the application of tensor networks to a\nbroader class of potential problems and open a pathway for future extensions to\nmore complex tensor network architectures."}
{"id": "2505.03802", "pdf": "https://arxiv.org/pdf/2505.03802", "abs": "https://arxiv.org/abs/2505.03802", "authors": ["Changhai Zhou", "Yuhua Zhou", "Qian Qiao", "Weizhong Zhang", "Cheng Jin"], "title": "Efficient Fine-Tuning of Quantized Models via Adaptive Rank and Bitwidth", "categories": ["cs.LG", "cs.AI"], "comment": "24 pages, 6 figures", "summary": "QLoRA effectively combines low-bit quantization and LoRA to achieve\nmemory-friendly fine-tuning for large language models (LLM). Recently, methods\nbased on SVD for continuous update iterations to initialize LoRA matrices to\naccommodate quantization errors have generally failed to consistently improve\nperformance. Dynamic mixed precision is a natural idea for continuously\nimproving the fine-tuning performance of quantized models, but previous methods\noften optimize low-rank subspaces or quantization components separately,\nwithout considering their synergy. To address this, we propose\n\\textbf{QR-Adaptor}, a unified, gradient-free strategy that uses partial\ncalibration data to jointly search the quantization components and the rank of\nlow-rank spaces for each layer, thereby continuously improving model\nperformance. QR-Adaptor does not minimize quantization error but treats\nprecision and rank allocation as a discrete optimization problem guided by\nactual downstream performance and memory usage. Compared to state-of-the-art\n(SOTA) quantized LoRA fine-tuning methods, our approach achieves a 4.89\\%\naccuracy improvement on GSM8K, and in some cases even outperforms the 16-bit\nfine-tuned model while maintaining the memory footprint of the 4-bit setting."}
{"id": "2505.03923", "pdf": "https://arxiv.org/pdf/2505.03923", "abs": "https://arxiv.org/abs/2505.03923", "authors": ["Pedram Pad", "Hadi Hammoud", "Mohamad Dia", "Nadim Maamari", "L. Andrea Dunbar"], "title": "SAND: One-Shot Feature Selection with Additive Noise Distortion", "categories": ["cs.LG"], "comment": "Proceedings of the 42nd International Conference on Machine Learning\n  (ICML), Vancouver, Canada. PMLR 267, 2025", "summary": "Feature selection is a critical step in data-driven applications, reducing\ninput dimensionality to enhance learning accuracy, computational efficiency,\nand interpretability. Existing state-of-the-art methods often require\npost-selection retraining and extensive hyperparameter tuning, complicating\ntheir adoption. We introduce a novel, non-intrusive feature selection layer\nthat, given a target feature count $k$, automatically identifies and selects\nthe $k$ most informative features during neural network training. Our method is\nuniquely simple, requiring no alterations to the loss function, network\narchitecture, or post-selection retraining. The layer is mathematically elegant\nand can be fully described by: \\begin{align} \\nonumber \\tilde{x}_i = a_i x_i +\n(1-a_i)z_i \\end{align} where $x_i$ is the input feature, $\\tilde{x}_i$ the\noutput, $z_i$ a Gaussian noise, and $a_i$ trainable gain such that\n$\\sum_i{a_i^2}=k$. This formulation induces an automatic clustering effect,\ndriving $k$ of the $a_i$ gains to $1$ (selecting informative features) and the\nrest to $0$ (discarding redundant ones) via weighted noise distortion and gain\nnormalization. Despite its extreme simplicity, our method delivers\nstate-of-the-art performance on standard benchmark datasets and a novel\nreal-world dataset, outperforming or matching existing approaches without\nrequiring hyperparameter search for $k$ or retraining. Theoretical analysis in\nthe context of linear regression further validates its efficacy. Our work\ndemonstrates that simplicity and performance are not mutually exclusive,\noffering a powerful yet straightforward tool for feature selection in machine\nlearning."}
{"id": "2505.03803", "pdf": "https://arxiv.org/pdf/2505.03803", "abs": "https://arxiv.org/abs/2505.03803", "authors": ["Chen Xu", "Yuxuan Yue", "Zukang Xu", "Xing Hu", "Jiangyong Yu", "Zhixuan Chen", "Sifan Zhou", "Zhihang Yuan", "Dawei Yang"], "title": "RWKVQuant: Quantizing the RWKV Family with Proxy Guided Hybrid of Scalar and Vector Quantization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "RWKV is a modern RNN architecture with comparable performance to Transformer,\nbut still faces challenges when deployed to resource-constrained devices. Post\nTraining Quantization (PTQ), which is a an essential technique to reduce model\nsize and inference latency, has been widely used in Transformer models.\nHowever, it suffers significant degradation of performance when applied to\nRWKV. This paper investigates and identifies two key constraints inherent in\nthe properties of RWKV: (1) Non-linear operators hinder the parameter-fusion of\nboth smooth- and rotation-based quantization, introducing extra computation\noverhead. (2) The larger amount of uniformly distributed weights poses\nchallenges for cluster-based quantization, leading to reduced accuracy. To this\nend, we propose RWKVQuant, a PTQ framework tailored for RWKV models, consisting\nof two novel techniques: (1) a coarse-to-fine proxy capable of adaptively\nselecting different quantization approaches by assessing the uniformity and\nidentifying outliers in the weights, and (2) a codebook optimization algorithm\nthat enhances the performance of cluster-based quantization methods for\nelement-wise multiplication in RWKV. Experiments show that RWKVQuant can\nquantize RWKV-6-14B into about 3-bit with less than 1% accuracy loss and 2.14x\nspeed up."}
{"id": "2505.03949", "pdf": "https://arxiv.org/pdf/2505.03949", "abs": "https://arxiv.org/abs/2505.03949", "authors": ["John Christopher Tidwell", "John Storm Tidwell"], "title": "Deep Q-Network (DQN) multi-agent reinforcement learning (MARL) for Stock Trading", "categories": ["cs.LG"], "comment": null, "summary": "This project addresses the challenge of automated stock trading, where\ntraditional methods and direct reinforcement learning (RL) struggle with market\nnoise, complexity, and generalization. Our proposed solution is an integrated\ndeep learning framework combining a Convolutional Neural Network (CNN) to\nidentify patterns in technical indicators formatted as images, a Long\nShort-Term Memory (LSTM) network to capture temporal dependencies across both\nprice history and technical indicators, and a Deep Q-Network (DQN) agent which\nlearns the optimal trading policy (buy, sell, hold) based on the features\nextracted by the CNN and LSTM."}
{"id": "2505.03804", "pdf": "https://arxiv.org/pdf/2505.03804", "abs": "https://arxiv.org/abs/2505.03804", "authors": ["Xing Hu", "Zhixuan Chen", "Dawei Yang", "Zukang Xu", "Chen Xu", "Zhihang Yuan", "Sifan Zhou", "Jiangyong Yu"], "title": "MoEQuant: Enhancing Quantization for Mixture-of-Experts Large Language Models via Expert-Balanced Sampling and Affinity Guidance", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Mixture-of-Experts (MoE) large language models (LLMs), which leverage dynamic\nrouting and sparse activation to enhance efficiency and scalability, have\nachieved higher performance while reducing computational costs. However, these\nmodels face significant memory overheads, limiting their practical deployment\nand broader adoption. Post-training quantization (PTQ), a widely used method\nfor compressing LLMs, encounters severe accuracy degradation and diminished\ngeneralization performance when applied to MoE models. This paper investigates\nthe impact of MoE's sparse and dynamic characteristics on quantization and\nidentifies two primary challenges: (1) Inter-expert imbalance, referring to the\nuneven distribution of samples across experts, which leads to insufficient and\nbiased calibration for less frequently utilized experts; (2) Intra-expert\nimbalance, arising from MoE's unique aggregation mechanism, which leads to\nvarying degrees of correlation between different samples and their assigned\nexperts. To address these challenges, we propose MoEQuant, a novel quantization\nframework tailored for MoE LLMs. MoE-Quant includes two novel techniques: 1)\nExpert-Balanced Self-Sampling (EBSS) is an efficient sampling method that\nefficiently constructs a calibration set with balanced expert distributions by\nleveraging the cumulative probabilities of tokens and expert balance metrics as\nguiding factors. 2) Affinity-Guided Quantization (AGQ), which incorporates\naffinities between experts and samples into the quantization process, thereby\naccurately assessing the impact of individual samples on different experts\nwithin the MoE layer. Experiments demonstrate that MoEQuant achieves\nsubstantial performance gains (more than 10 points accuracy gain in the\nHumanEval for DeepSeekMoE-16B under 4-bit quantization) and boosts efficiency."}
{"id": "2505.03953", "pdf": "https://arxiv.org/pdf/2505.03953", "abs": "https://arxiv.org/abs/2505.03953", "authors": ["Noah Schutte", "Grigorii Veviurko", "Krzysztof Postek", "Neil Yorke-Smith"], "title": "Sufficient Decision Proxies for Decision-Focused Learning", "categories": ["cs.LG", "math.OC"], "comment": "16 pages, 4 figures,", "summary": "When solving optimization problems under uncertainty with contextual data,\nutilizing machine learning to predict the uncertain parameters is a popular and\neffective approach. Decision-focused learning (DFL) aims at learning a\npredictive model such that decision quality, instead of prediction accuracy, is\nmaximized. Common practice here is to predict a single value for each uncertain\nparameter, implicitly assuming that there exists a (single-scenario)\ndeterministic problem approximation (proxy) that is sufficient to obtain an\noptimal decision. Other work assumes the opposite, where the underlying\ndistribution needs to be estimated. However, little is known about when either\nchoice is valid. This paper investigates for the first time problem properties\nthat justify using either assumption. Using this, we present effective decision\nproxies for DFL, with very limited compromise on the complexity of the learning\ntask. We show the effectiveness of presented approaches in experiments on\nproblems with continuous and discrete variables, as well as uncertainty in the\nobjective function and in the constraints."}
{"id": "2505.03806", "pdf": "https://arxiv.org/pdf/2505.03806", "abs": "https://arxiv.org/abs/2505.03806", "authors": ["Mehran Mazandarani", "Marzieh Najariyan"], "title": "Perception-Informed Neural Networks: Beyond Physics-Informed Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This article introduces Perception-Informed Neural Networks (PrINNs), a\nframework designed to incorporate perception-based information into neural\nnetworks, addressing both systems with known and unknown physics laws or\ndifferential equations. Moreover, PrINNs extend the concept of Physics-Informed\nNeural Networks (PINNs) and their variants, offering a platform for the\nintegration of diverse forms of perception precisiation, including singular,\nprobability distribution, possibility distribution, interval, and fuzzy graph.\nIn fact, PrINNs allow neural networks to model dynamical systems by integrating\nexpert knowledge and perception-based information through loss functions,\nenabling the creation of modern data-driven models. Some of the key\ncontributions include Mixture of Experts Informed Neural Networks (MOEINNs),\nwhich combine heterogeneous expert knowledge into the network, and\nTransformed-Knowledge Informed Neural Networks (TKINNs), which facilitate the\nincorporation of meta-information for enhanced model performance. Additionally,\nFuzzy-Informed Neural Networks (FINNs) as a modern class of fuzzy deep neural\nnetworks leverage fuzzy logic constraints within a deep learning architecture,\nallowing online training without pre-training and eliminating the need for\ndefuzzification. PrINNs represent a significant step forward in bridging the\ngap between traditional physics-based modeling and modern data-driven\napproaches, enabling neural networks to learn from both structured physics laws\nand flexible perception-based rules. This approach empowers neural networks to\noperate in uncertain environments, model complex systems, and discover new\nforms of differential equations, making PrINNs a powerful tool for advancing\ncomputational science and engineering."}
{"id": "2505.03955", "pdf": "https://arxiv.org/pdf/2505.03955", "abs": "https://arxiv.org/abs/2505.03955", "authors": ["Charupriya Sharma", "Iñaki Estella Aguerri", "Daniel Guimarans"], "title": "Hierarchical Forecast Reconciliation on Networks: A Network Flow Optimization Formulation", "categories": ["cs.LG"], "comment": null, "summary": "Hierarchical forecasting with reconciliation requires forecasting values of a\nhierarchy (e.g.~customer demand in a state and district), such that forecast\nvalues are linked (e.g.~ district forecasts should add up to the state\nforecast). Basic forecasting provides no guarantee for these desired structural\nrelationships. Reconciliation addresses this problem, which is crucial for\norganizations requiring coherent predictions across multiple aggregation\nlevels. Current methods like minimum trace (MinT) are mostly limited to tree\nstructures and are computationally expensive. We introduce FlowRec, which\nreformulates hierarchical forecast reconciliation as a network flow\noptimization, enabling forecasting on generalized network structures. While\nreconciliation under the $\\ell_0$ norm is NP-hard, we prove polynomial-time\nsolvability for all $\\ell_{p > 0}$ norms and , for any strictly convex and\ncontinuously differentiable loss function. For sparse networks, FlowRec\nachieves $O(n^2\\log n)$ complexity, significantly improving upon MinT's\n$O(n^3)$. Furthermore, we prove that FlowRec extends MinT to handle general\nnetworks, replacing MinT's error-covariance estimation step with direct network\nstructural information. A key novelty of our approach is its handling of\ndynamic scenarios: while traditional methods recompute both base forecasts and\nreconciliation, FlowRec provides efficient localised updates with optimality\nguarantees. Monotonicity ensures that when forecasts improve incrementally, the\ninitial reconciliation remains optimal. We also establish efficient,\nerror-bounded approximate reconciliation, enabling fast updates in\ntime-critical applications. Experiments on both simulated and real benchmarks\ndemonstrate that FlowRec improves accuracy, runtime by 3-40x and memory usage\nby 5-7x. These results establish FlowRec as a powerful tool for large-scale\nhierarchical forecasting applications."}
{"id": "2505.03807", "pdf": "https://arxiv.org/pdf/2505.03807", "abs": "https://arxiv.org/abs/2505.03807", "authors": ["Yiwen Zhang", "Jianing Hao", "Zhan Wang", "Hongling Sheng", "Wei Zeng"], "title": "Facilitating Video Story Interaction with Multi-Agent Collaborative System", "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.MA"], "comment": "Prepared and submitted in 2024", "summary": "Video story interaction enables viewers to engage with and explore narrative\ncontent for personalized experiences. However, existing methods are limited to\nuser selection, specially designed narratives, and lack customization. To\naddress this, we propose an interactive system based on user intent. Our system\nuses a Vision Language Model (VLM) to enable machines to understand video\nstories, combining Retrieval-Augmented Generation (RAG) and a Multi-Agent\nSystem (MAS) to create evolving characters and scene experiences. It includes\nthree stages: 1) Video story processing, utilizing VLM and prior knowledge to\nsimulate human understanding of stories across three modalities. 2) Multi-space\nchat, creating growth-oriented characters through MAS interactions based on\nuser queries and story stages. 3) Scene customization, expanding and\nvisualizing various story scenes mentioned in dialogue. Applied to the Harry\nPotter series, our study shows the system effectively portrays emergent\ncharacter social behavior and growth, enhancing the interactive experience in\nthe video story world."}
{"id": "2505.03977", "pdf": "https://arxiv.org/pdf/2505.03977", "abs": "https://arxiv.org/abs/2505.03977", "authors": ["Guilherme S. Imai Aldeia", "Hengzhe Zhang", "Geoffrey Bomarito", "Miles Cranmer", "Alcides Fonseca", "Bogdan Burlacu", "William G. La Cava", "Fabrício Olivetti de França"], "title": "Call for Action: towards the next generation of symbolic regression benchmark", "categories": ["cs.LG", "cs.NE"], "comment": "10 pages, 4 figures, 3 tables, accepted in Genetic and Evolutionary\n  Computation Conference (GECCO '25) Symbolic Regression Workshop", "summary": "Symbolic Regression (SR) is a powerful technique for discovering\ninterpretable mathematical expressions. However, benchmarking SR methods\nremains challenging due to the diversity of algorithms, datasets, and\nevaluation criteria. In this work, we present an updated version of SRBench.\nOur benchmark expands the previous one by nearly doubling the number of\nevaluated methods, refining evaluation metrics, and using improved\nvisualizations of the results to understand the performances. Additionally, we\nanalyze trade-offs between model complexity, accuracy, and energy consumption.\nOur results show that no single algorithm dominates across all datasets. We\npropose a call for action from SR community in maintaining and evolving SRBench\nas a living benchmark that reflects the state-of-the-art in symbolic\nregression, by standardizing hyperparameter tuning, execution constraints, and\ncomputational resource allocation. We also propose deprecation criteria to\nmaintain the benchmark's relevance and discuss best practices for improving SR\nalgorithms, such as adaptive hyperparameter tuning and energy-efficient\nimplementations."}
{"id": "2505.03809", "pdf": "https://arxiv.org/pdf/2505.03809", "abs": "https://arxiv.org/abs/2505.03809", "authors": ["Suorong Yang", "Peng Ye", "Furao Shen", "Dongzhan Zhou"], "title": "When Dynamic Data Selection Meets Data Augmentation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Dynamic data selection aims to accelerate training with lossless performance.\nHowever, reducing training data inherently limits data diversity, potentially\nhindering generalization. While data augmentation is widely used to enhance\ndiversity, it is typically not optimized in conjunction with selection. As a\nresult, directly combining these techniques fails to fully exploit their\nsynergies. To tackle the challenge, we propose a novel online data training\nframework that, for the first time, unifies dynamic data selection and\naugmentation, achieving both training efficiency and enhanced performance. Our\nmethod estimates each sample's joint distribution of local density and\nmultimodal semantic consistency, allowing for the targeted selection of\naugmentation-suitable samples while suppressing the inclusion of noisy or\nambiguous data. This enables a more significant reduction in dataset size\nwithout sacrificing model generalization. Experimental results demonstrate that\nour method outperforms existing state-of-the-art approaches on various\nbenchmark datasets and architectures, e.g., reducing 50\\% training costs on\nImageNet-1k with lossless performance. Furthermore, our approach enhances noise\nresistance and improves model robustness, reinforcing its practical utility in\nreal-world scenarios."}
{"id": "2505.03980", "pdf": "https://arxiv.org/pdf/2505.03980", "abs": "https://arxiv.org/abs/2505.03980", "authors": ["Aroon Sankoh", "Victor Wickerhauser"], "title": "Comparing statistical and deep learning techniques for parameter estimation of continuous-time stochastic differentiable equations", "categories": ["cs.LG", "math.PR"], "comment": "6 pages, 2 figures, 2 tables", "summary": "Stochastic differential equations such as the Ornstein-Uhlenbeck process have\nlong been used to model realworld probablistic events such as stock prices and\ntemperature fluctuations. While statistical methods such as Maximum Likelihood\nEstimation (MLE), Kalman Filtering, Inverse Variable Method, and more have\nhistorically been used to estimate the parameters of stochastic differential\nequations, the recent explosion of deep learning technology suggests that\nmodels such as a Recurrent Neural Network (RNN) could produce more precise\nestimators. We present a series of experiments that compare the estimation\naccuracy and computational expensiveness of a statistical method (MLE) with a\ndeep learning model (RNN) for the parameters of the Ornstein-Uhlenbeck process."}
{"id": "2505.03810", "pdf": "https://arxiv.org/pdf/2505.03810", "abs": "https://arxiv.org/abs/2505.03810", "authors": ["Euntae Choi", "Sumin Song", "Woosang Lim", "Sungjoo Yoo"], "title": "Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "7 pages", "summary": "Large Language Models (LLMs) face deployment challenges due to high\ncomputational costs, and while Post-Training Quantization (PTQ) offers a\nsolution, existing rotation-based methods struggle at very low bit-widths like\n2-bit. We introduce a novel, training-free approach to construct an improved\nrotation matrix, addressing the limitations of current methods. The key\ncontributions include leveraging the Walsh-Hadamard transform with sequency\nordering, which clusters similar frequency components to reduce quantization\nerror compared to standard Hadamard matrices, significantly improving\nperformance. Furthermore, we propose a Grouped Sequency-arranged Rotation (GSR)\nusing block-diagonal matrices with smaller Walsh blocks, effectively isolating\noutlier impacts and achieving performance comparable to optimization-based\nmethods without requiring any training. Our method demonstrates robust\nperformance on reasoning tasks and Perplexity (PPL) score on WikiText-2. Our\nmethod also enhances results even when applied over existing learned rotation\ntechniques."}
{"id": "2505.03983", "pdf": "https://arxiv.org/pdf/2505.03983", "abs": "https://arxiv.org/abs/2505.03983", "authors": ["Hengyuan Hu", "Aniket Das", "Dorsa Sadigh", "Nima Anari"], "title": "Diffusion Models are Secretly Exchangeable: Parallelizing DDPMs via Autospeculation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Denoising Diffusion Probabilistic Models (DDPMs) have emerged as powerful\ntools for generative modeling. However, their sequential computation\nrequirements lead to significant inference-time bottlenecks. In this work, we\nutilize the connection between DDPMs and Stochastic Localization to prove that,\nunder an appropriate reparametrization, the increments of DDPM satisfy an\nexchangeability property. This general insight enables near-black-box\nadaptation of various performance optimization techniques from autoregressive\nmodels to the diffusion setting. To demonstrate this, we introduce\n\\emph{Autospeculative Decoding} (ASD), an extension of the widely used\nspeculative decoding algorithm to DDPMs that does not require any auxiliary\ndraft models. Our theoretical analysis shows that ASD achieves a $\\tilde{O}\n(K^{\\frac{1}{3}})$ parallel runtime speedup over the $K$ step sequential DDPM.\nWe also demonstrate that a practical implementation of autospeculative decoding\naccelerates DDPM inference significantly in various domains."}
{"id": "2505.03811", "pdf": "https://arxiv.org/pdf/2505.03811", "abs": "https://arxiv.org/abs/2505.03811", "authors": ["Surajit Chakrabarty", "Rukma Talwadker", "Tridib Mukherjee"], "title": "ScarceGAN: Discriminative Classification Framework for Rare Class Identification for Longitudinal Data with Weak Prior", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper introduces ScarceGAN which focuses on identification of extremely\nrare or scarce samples from multi-dimensional longitudinal telemetry data with\nsmall and weak label prior. We specifically address: (i) severe scarcity in\npositive class, stemming from both underlying organic skew in the data, as well\nas extremely limited labels; (ii) multi-class nature of the negative samples,\nwith uneven density distributions and partially overlapping feature\ndistributions; and (iii) massively unlabelled data leading to tiny and weak\nprior on both positive and negative classes, and possibility of unseen or\nunknown behavior in the unlabelled set, especially in the negative class.\nAlthough related to PU learning problems, we contend that knowledge (or lack of\nit) on the negative class can be leveraged to learn the compliment of it (i.e.,\nthe positive class) better in a semi-supervised manner. To this effect,\nScarceGAN re-formulates semi-supervised GAN by accommodating weakly labelled\nmulti-class negative samples and the available positive samples. It relaxes the\nsupervised discriminator's constraint on exact differentiation between negative\nsamples by introducing a 'leeway' term for samples with noisy prior. We propose\nmodifications to the cost objectives of discriminator, in supervised and\nunsupervised path as well as that of the generator. For identifying risky\nplayers in skill gaming, this formulation in whole gives us a recall of over\n85% (~60% jump over vanilla semi-supervised GAN) on our scarce class with very\nminimal verbosity in the unknown space. Further ScarceGAN outperforms the\nrecall benchmarks established by recent GAN based specialized models for the\npositive imbalanced class identification and establishes a new benchmark in\nidentifying one of rare attack classes (0.09%) in the intrusion dataset from\nthe KDDCUP99 challenge."}
{"id": "2505.03992", "pdf": "https://arxiv.org/pdf/2505.03992", "abs": "https://arxiv.org/abs/2505.03992", "authors": ["Jarren Briscoe", "Garrett Kepler", "Daryl Deford", "Assefaw Gebremedhin"], "title": "Algorithmic Accountability in Small Data: Sample-Size-Induced Bias Within Classification Metrics", "categories": ["cs.LG"], "comment": "AISTATS 2025", "summary": "Evaluating machine learning models is crucial not only for determining their\ntechnical accuracy but also for assessing their potential societal\nimplications. While the potential for low-sample-size bias in algorithms is\nwell known, we demonstrate the significance of sample-size bias induced by\ncombinatorics in classification metrics. This revelation challenges the\nefficacy of these metrics in assessing bias with high resolution, especially\nwhen comparing groups of disparate sizes, which frequently arise in social\napplications. We provide analyses of the bias that appears in several commonly\napplied metrics and propose a model-agnostic assessment and correction\ntechnique. Additionally, we analyze counts of undefined cases in metric\ncalculations, which can lead to misleading evaluations if improperly handled.\nThis work illuminates the previously unrecognized challenge of combinatorics\nand probability in standard evaluation practices and thereby advances\napproaches for performing fair and trustworthy classification methods."}
{"id": "2505.03814", "pdf": "https://arxiv.org/pdf/2505.03814", "abs": "https://arxiv.org/abs/2505.03814", "authors": ["Ganghua Wang", "Zhaorun Chen", "Bo Li", "Haifeng Xu"], "title": "Cer-Eval: Certifiable and Cost-Efficient Evaluation Framework for LLMs", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "As foundation models continue to scale, the size of trained models grows\nexponentially, presenting significant challenges for their evaluation. Current\nevaluation practices involve curating increasingly large datasets to assess the\nperformance of large language models (LLMs). However, there is a lack of\nsystematic analysis and guidance on determining the sufficiency of test data or\nselecting informative samples for evaluation. This paper introduces a\ncertifiable and cost-efficient evaluation framework for LLMs. Our framework\nadapts to different evaluation objectives and outputs confidence intervals that\ncontain true values with high probability. We use ``test sample complexity'' to\nquantify the number of test points needed for a certifiable evaluation and\nderive tight bounds on test sample complexity. Based on the developed theory,\nwe develop a partition-based algorithm, named Cer-Eval, that adaptively selects\ntest points to minimize the cost of LLM evaluation. Real-world experiments\ndemonstrate that Cer-Eval can save 20% to 40% test points across various\nbenchmarks, while maintaining an estimation error level comparable to the\ncurrent evaluation process and providing a 95% confidence guarantee."}
{"id": "2505.03997", "pdf": "https://arxiv.org/pdf/2505.03997", "abs": "https://arxiv.org/abs/2505.03997", "authors": ["Prudhviraj Naidu", "Zixian Wang", "Leon Bergen", "Ramamohan Paturi"], "title": "Quiet Feature Learning in Algorithmic Tasks", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We train Transformer-based language models on ten foundational algorithmic\ntasks and observe pronounced phase transitions in their loss curves that\ndeviate from established power-law scaling trends. Over large ranges of\ncompute, the validation loss barely improves, then abruptly decreases. Probing\nthe models' internal representations reveals the learning of quiet features\nduring the stagnant phase, followed by sudden acquisition of loud features that\ncoincide with the sharp drop in loss. Our ablation experiments show that\ndisrupting a single learned feature can dramatically degrade performance,\nproviding evidence of their causal role in task performance. These findings\nchallenge the prevailing assumption that next-token predictive loss reliably\ntracks incremental progress; instead, key internal features may be developing\nbelow the surface until they coalesce, triggering a rapid performance gain."}
{"id": "2505.03816", "pdf": "https://arxiv.org/pdf/2505.03816", "abs": "https://arxiv.org/abs/2505.03816", "authors": ["Bidyarthi Paul", "Fariha Tasnim Chowdhury", "Dipta Biswas", "Meherin Sultana"], "title": "Geospatial and Temporal Trends in Urban Transportation: A Study of NYC Taxis and Pathao Food Deliveries", "categories": ["cs.SI", "cs.AI"], "comment": null, "summary": "Urban transportation plays a vital role in modern city life, affecting how\nefficiently people and goods move around. This study analyzes transportation\npatterns using two datasets: the NYC Taxi Trip dataset from New York City and\nthe Pathao Food Trip dataset from Dhaka, Bangladesh. Our goal is to identify\nkey trends in demand, peak times, and important geographical hotspots. We start\nwith Exploratory Data Analysis (EDA) to understand the basic characteristics of\nthe datasets. Next, we perform geospatial analysis to map out high-demand and\nlow-demand regions. We use the SARIMAX model for time series analysis to\nforecast demand patterns, capturing seasonal and weekly variations. Lastly, we\napply clustering techniques to identify significant areas of high and low\ndemand. Our findings provide valuable insights for optimizing fleet management\nand resource allocation in both passenger transport and food delivery services.\nThese insights can help improve service efficiency, better meet customer needs,\nand enhance urban transportation systems in diverse urban environments."}
{"id": "2505.04005", "pdf": "https://arxiv.org/pdf/2505.04005", "abs": "https://arxiv.org/abs/2505.04005", "authors": ["Devan Selvaraj"], "title": "Iterative Orthogonalization Scaling Laws", "categories": ["cs.LG", "68T07"], "comment": null, "summary": "The muon optimizer has picked up much attention as of late as a possible\nreplacement to the seemingly omnipresent Adam optimizer. Recently, care has\nbeen taken to document the scaling laws of hyper-parameters under muon such as\nweight decay and learning rate. However, at much larger scales the iterative\northogonalization procedure present in muon may suffer a possible issue as the\nsingular values of random matrices shrink with scale. This paper shows this\nscaling behavior theoretically and empirically on random matrices but does not\nsuggest what to do about it."}
{"id": "2505.03817", "pdf": "https://arxiv.org/pdf/2505.03817", "abs": "https://arxiv.org/abs/2505.03817", "authors": ["Aditya Shinde", "Prashant Doshi"], "title": "Modeling Behavioral Preferences of Cyber Adversaries Using Inverse Reinforcement Learning", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper presents a holistic approach to attacker preference modeling from\nsystem-level audit logs using inverse reinforcement learning (IRL). Adversary\nmodeling is an important capability in cybersecurity that lets defenders\ncharacterize behaviors of potential attackers, which enables attribution to\nknown cyber adversary groups. Existing approaches rely on documenting an\never-evolving set of attacker tools and techniques to track known threat\nactors. Although attacks evolve constantly, attacker behavioral preferences are\nintrinsic and less volatile. Our approach learns the behavioral preferences of\ncyber adversaries from forensics data on their tools and techniques. We model\nthe attacker as an expert decision-making agent with unknown behavioral\npreferences situated in a computer host. We leverage attack provenance graphs\nof audit logs to derive a state-action trajectory of the attack. We test our\napproach on open datasets of audit logs containing real attack data. Our\nresults demonstrate for the first time that low-level forensics data can\nautomatically reveal an adversary's subjective preferences, which serves as an\nadditional dimension to modeling and documenting cyber adversaries. Attackers'\npreferences tend to be invariant despite their different tools and indicate\npredispositions that are inherent to the attacker. As such, these inferred\npreferences can potentially serve as unique behavioral signatures of attackers\nand improve threat attribution."}
{"id": "2505.04046", "pdf": "https://arxiv.org/pdf/2505.04046", "abs": "https://arxiv.org/abs/2505.04046", "authors": ["Xuyang Wang", "Siyuan Duan", "Qizhi Li", "Guiduo Duan", "Yuan Sun", "Dezhong Peng"], "title": "Reliable Disentanglement Multi-view Learning Against View Adversarial Attacks", "categories": ["cs.LG", "cs.CR"], "comment": "11 pages, 11 figures, accepted by International Joint Conference on\n  Artificial Intelligence (IJCAI 2025)", "summary": "Recently, trustworthy multi-view learning has attracted extensive attention\nbecause evidence learning can provide reliable uncertainty estimation to\nenhance the credibility of multi-view predictions. Existing trusted multi-view\nlearning methods implicitly assume that multi-view data is secure. In practice,\nhowever, in safety-sensitive applications such as autonomous driving and\nsecurity monitoring, multi-view data often faces threats from adversarial\nperturbations, thereby deceiving or disrupting multi-view learning models. This\ninevitably leads to the adversarial unreliability problem (AUP) in trusted\nmulti-view learning. To overcome this tricky problem, we propose a novel\nmulti-view learning framework, namely Reliable Disentanglement Multi-view\nLearning (RDML). Specifically, we first propose evidential disentanglement\nlearning to decompose each view into clean and adversarial parts under the\nguidance of corresponding evidences, which is extracted by a pretrained\nevidence extractor. Then, we employ the feature recalibration module to\nmitigate the negative impact of adversarial perturbations and extract potential\ninformative features from them. Finally, to further ignore the irreparable\nadversarial interferences, a view-level evidential attention mechanism is\ndesigned. Extensive experiments on multi-view classification tasks with\nadversarial attacks show that our RDML outperforms the state-of-the-art\nmulti-view learning methods by a relatively large margin."}
{"id": "2505.03818", "pdf": "https://arxiv.org/pdf/2505.03818", "abs": "https://arxiv.org/abs/2505.03818", "authors": ["Antonio Valerio Miceli-Barone", "Vaishak Belle", "Ali Payani"], "title": "Program Semantic Inequivalence Game with Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) can achieve strong performance on everyday\ncoding tasks, but they can fail on complex tasks that require non-trivial\nreasoning about program semantics. Finding training examples to teach LLMs to\nsolve these tasks can be challenging.\n  In this work, we explore a method to synthetically generate code reasoning\ntraining data based on a semantic inequivalence game SInQ: a generator agent\ncreates program variants that are semantically distinct, derived from a dataset\nof real-world programming tasks, while an evaluator agent has to identify input\nexamples that cause the original programs and the generated variants to diverge\nin their behaviour, with the agents training each other semi-adversarially. We\nprove that this setup enables theoretically unlimited improvement through\nself-play in the limit of infinite computational resources.\n  We evaluated our approach on multiple code generation and understanding\nbenchmarks, including cross-language vulnerability detection (Lu et al., 2021),\nwhere our method improves vulnerability detection in C/C++ code despite being\ntrained exclusively on Python code, and the challenging Python builtin\nidentifier swap benchmark (Miceli-Barone et al., 2023), showing that whereas\nmodern LLMs still struggle with this benchmark, our approach yields substantial\nimprovements.\n  We release the code needed to replicate the experiments, as well as the\ngenerated synthetic data, which can be used to fine-tune LLMs."}
{"id": "2505.04066", "pdf": "https://arxiv.org/pdf/2505.04066", "abs": "https://arxiv.org/abs/2505.04066", "authors": ["Tuochao Chen", "Nicholas Batchelder", "Alisa Liu", "Noah Smith", "Shyamnath Gollakota"], "title": "LLAMAPIE: Proactive In-Ear Conversation Assistants", "categories": ["cs.LG", "cs.CL", "eess.AS"], "comment": null, "summary": "We introduce LlamaPIE, the first real-time proactive assistant designed to\nenhance human conversations through discreet, concise guidance delivered via\nhearable devices. Unlike traditional language models that require explicit user\ninvocation, this assistant operates in the background, anticipating user needs\nwithout interrupting conversations. We address several challenges, including\ndetermining when to respond, crafting concise responses that enhance\nconversations, leveraging knowledge of the user for context-aware assistance,\nand real-time, on-device processing. To achieve this, we construct a\nsemi-synthetic dialogue dataset and propose a two-model pipeline: a small model\nthat decides when to respond and a larger model that generates the response. We\nevaluate our approach on real-world datasets, demonstrating its effectiveness\nin providing helpful, unobtrusive assistance. User studies with our assistant,\nimplemented on Apple Silicon M2 hardware, show a strong preference for the\nproactive assistant over both a baseline with no assistance and a reactive\nmodel, highlighting the potential of LlamaPie to enhance live conversations."}
{"id": "2505.03819", "pdf": "https://arxiv.org/pdf/2505.03819", "abs": "https://arxiv.org/abs/2505.03819", "authors": ["Johannes Schneider"], "title": "Focus on the Likely: Test-time Instance-based Uncertainty Removal", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose two novel test-time fine-tuning methods to improve uncertain model\npredictions. Our methods require no auxiliary data and use the given test\ninstance only. Instead of performing a greedy selection of the most likely\nclass to make a prediction, we introduce an additional focus on the likely\nclasses step during inference. By applying a single-step gradient descent, we\nrefine predictions when an initial forward pass indicates high uncertainty.\nThis aligns predictions more closely with the ideal of assigning zero\nprobability to less plausible outcomes. Our theoretical discussion provides a\ndeeper understanding highlighting the impact on shared and non-shared features\namong (focus) classes. The experimental evaluation highlights accuracy gains on\nsamples exhibiting high decision uncertainty for a diverse set of models from\nboth the text and image domain using the same hyperparameters."}
{"id": "2505.04075", "pdf": "https://arxiv.org/pdf/2505.04075", "abs": "https://arxiv.org/abs/2505.04075", "authors": ["Teddy Foley", "Spencer Guo", "Henry Josephson", "Anqi Qu", "Jack Sanderson"], "title": "LLM-e Guess: Can LLMs Capabilities Advance Without Hardware Progress?", "categories": ["cs.LG", "cs.AI", "I.2"], "comment": null, "summary": "This paper examines whether large language model (LLM) capabilities can\ncontinue to advance without additional compute by analyzing the development and\nrole of algorithms used in state-of-the-art LLMs. Motivated by regulatory\nefforts that have largely focused on restricting access to high-performance\nhardware, we ask: Can LLMs progress in a compute-constrained environment, and\nhow do algorithmic innovations perform under such conditions?\n  To address these questions, we introduce a novel classification framework\nthat distinguishes between compute-dependent innovations -- which yield\ndisproportionate benefits at high compute levels (e.g., the Transformer\narchitecture and mixture-of-experts models) and compute-independent\ninnovations, which improve efficiency across all compute scales (e.g., rotary\npositional encoding, FlashAttention, or layer normalization). We quantify these\ncontributions using a metric called compute-equivalent gain (CEG), which\nestimates the additional compute that would be required to achieve similar\nimprovements without these algorithmic advancements.\n  To validate this framework, we conduct small-scale training experiments with\na scaled-down GPT-2 model. Our results confirm that compute-independent\nadvancements yield meaningful performance gains even in resource-constrained\nsettings, with a CEG of up to $3.5\\times$ over a baseline model. By contrast,\ncompute-dependent advancements provided little benefit or even degraded\nperformance at the small scale, reinforcing the importance of compute\navailability for certain algorithmic gains."}
{"id": "2505.03821", "pdf": "https://arxiv.org/pdf/2505.03821", "abs": "https://arxiv.org/abs/2505.03821", "authors": ["Gracjan Góral", "Alicja Ziarko", "Piotr Miłoś", "Michał Nauman", "Maciej Wołczyk", "Michał Kosiński"], "title": "Beyond Recognition: Evaluating Visual Perspective Taking in Vision Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "Dataset:\n  https://huggingface.co/datasets/Gracjan/Isle/viewer/Isle-Brick-V2", "summary": "We investigate the ability of Vision Language Models (VLMs) to perform visual\nperspective taking using a novel set of visual tasks inspired by established\nhuman tests. Our approach leverages carefully controlled scenes, in which a\nsingle humanoid minifigure is paired with a single object. By systematically\nvarying spatial configurations - such as object position relative to the\nhumanoid minifigure and the humanoid minifigure's orientation - and using both\nbird's-eye and surface-level views, we created 144 unique visual tasks. Each\nvisual task is paired with a series of 7 diagnostic questions designed to\nassess three levels of visual cognition: scene understanding, spatial\nreasoning, and visual perspective taking. Our evaluation of several\nstate-of-the-art models, including GPT-4-Turbo, GPT-4o,\nLlama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that\nwhile they excel in scene understanding, the performance declines significantly\non spatial reasoning and further deteriorates on perspective-taking. Our\nanalysis suggests a gap between surface-level object recognition and the deeper\nspatial and perspective reasoning required for complex visual tasks, pointing\nto the need for integrating explicit geometric representations and tailored\ntraining protocols in future VLM development."}
{"id": "2505.04083", "pdf": "https://arxiv.org/pdf/2505.04083", "abs": "https://arxiv.org/abs/2505.04083", "authors": ["Aditya K. Ranjan", "Siddharth Singh", "Cunyang Wei", "Abhinav Bhatele"], "title": "Plexus: Taming Billion-edge Graphs with 3D Parallel GNN Training", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Graph neural networks have emerged as a potent class of neural networks\ncapable of leveraging the connectivity and structure of real-world graphs to\nlearn intricate properties and relationships between nodes. Many real-world\ngraphs exceed the memory capacity of a GPU due to their sheer size, and using\nGNNs on them requires techniques such as mini-batch sampling to scale. However,\nthis can lead to reduced accuracy in some cases, and sampling and data transfer\nfrom the CPU to the GPU can also slow down training. On the other hand,\ndistributed full-graph training suffers from high communication overhead and\nload imbalance due to the irregular structure of graphs. We propose Plexus, a\nthree-dimensional (3D) parallel approach for full-graph training that tackles\nthese issues and scales to billion-edge graphs. Additionally, we introduce\noptimizations such as a permutation scheme for load balancing, and a\nperformance model to predict the optimal 3D configuration. We evaluate Plexus\non several graph datasets and show scaling results for up to 2048 GPUs on\nPerlmutter, which is 33% of the machine, and 2048 GCDs on Frontier. Plexus\nachieves unprecedented speedups of 2.3x-12.5x over existing methods and a\nreduction in the time to solution by 5.2-8.7x on Perlmutter and 7-54.2x on\nFrontier."}
{"id": "2505.03822", "pdf": "https://arxiv.org/pdf/2505.03822", "abs": "https://arxiv.org/abs/2505.03822", "authors": ["Hao Wu", "Jialiang Wang"], "title": "DRSLF: Double Regularized Second-Order Low-Rank Representation for Web Service QoS Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Quality-of-Service (QoS) data plays a crucial role in cloud service\nselection. Since users cannot access all services, QoS can be represented by a\nhigh-dimensional and incomplete (HDI) matrix. Latent factor analysis (LFA)\nmodels have been proven effective as low-rank representation techniques for\naddressing this issue. However, most LFA models rely on first-order optimizers\nand use L2-norm regularization, which can lead to lower QoS prediction\naccuracy. To address this issue, this paper proposes a double regularized\nsecond-order latent factor (DRSLF) model with two key ideas: a) integrating\nL1-norm and L2-norm regularization terms to enhance the low-rank representation\nperformance; b) incorporating second-order information by calculating the\nHessian-vector product in each conjugate gradient step. Experimental results on\ntwo real-world response-time QoS datasets demonstrate that DRSLF has a higher\nlow-rank representation capability than two baselines."}
{"id": "2505.04104", "pdf": "https://arxiv.org/pdf/2505.04104", "abs": "https://arxiv.org/abs/2505.04104", "authors": ["Sarah Hartman", "Cheng Soon Ong", "Julia Powles", "Petra Kuhnert"], "title": "Position: We need responsible, application-driven (RAD) AI research", "categories": ["cs.LG", "cs.CY", "I.2.0; K.4.1; J.4"], "comment": "11 pages, 1 figure, Accepted to Proceedings of the 41 st\n  International Conference on Machine Learning, Vancouver, Canada. PMLR 267,\n  2025", "summary": "This position paper argues that achieving meaningful scientific and societal\nadvances with artificial intelligence (AI) requires a responsible,\napplication-driven approach (RAD) to AI research. As AI is increasingly\nintegrated into society, AI researchers must engage with the specific contexts\nwhere AI is being applied. This includes being responsive to ethical and legal\nconsiderations, technical and societal constraints, and public discourse. We\npresent the case for RAD-AI to drive research through a three-staged approach:\n(1) building transdisciplinary teams and people-centred studies; (2) addressing\ncontext-specific methods, ethical commitments, assumptions, and metrics; and\n(3) testing and sustaining efficacy through staged testbeds and a community of\npractice. We present a vision for the future of application-driven AI research\nto unlock new value through technically feasible methods that are adaptive to\nthe contextual needs and values of the communities they ultimately serve."}
{"id": "2505.03824", "pdf": "https://arxiv.org/pdf/2505.03824", "abs": "https://arxiv.org/abs/2505.03824", "authors": ["Jiarui Chen"], "title": "Memory Assisted LLM for Personalized Recommendation System", "categories": ["cs.IR", "cs.AI"], "comment": "8 pages, 7 figures", "summary": "Large language models (LLMs) have demonstrated significant potential in\nsolving recommendation tasks. With proven capabilities in understanding user\npreferences, LLM personalization has emerged as a critical area for providing\ntailored responses to individuals. Current studies explore personalization\nthrough prompt design and fine-tuning, paving the way for further research in\npersonalized LLMs. However, existing approaches are either costly and\ninefficient in capturing diverse user preferences or fail to account for timely\nupdates to user history. To address these gaps, we propose the Memory-Assisted\nPersonalized LLM (MAP). Through user interactions, we first create a history\nprofile for each user, capturing their preferences, such as ratings for\nhistorical items. During recommendation, we extract relevant memory based on\nsimilarity, which is then incorporated into the prompts to enhance personalized\nrecommendations. In our experiments, we evaluate MAP using a sequential rating\nprediction task under two scenarios: single domain, where memory and tasks are\nfrom the same category (e.g., movies), and cross-domain (e.g., memory from\nmovies and recommendation tasks in books). The results show that MAP\noutperforms regular LLM-based recommenders that integrate user history directly\nthrough prompt design. Moreover, as user history grows, MAP's advantage\nincreases in both scenarios, making it more suitable for addressing successive\npersonalized user requests."}
{"id": "2505.04110", "pdf": "https://arxiv.org/pdf/2505.04110", "abs": "https://arxiv.org/abs/2505.04110", "authors": ["David Noever", "Forrest McKee"], "title": "Alpha Excel Benchmark", "categories": ["cs.LG"], "comment": null, "summary": "This study presents a novel benchmark for evaluating Large Language Models\n(LLMs) using challenges derived from the Financial Modeling World Cup (FMWC)\nExcel competitions. We introduce a methodology for converting 113 existing FMWC\nchallenges into programmatically evaluable JSON formats and use this dataset to\ncompare the performance of several leading LLMs. Our findings demonstrate\nsignificant variations in performance across different challenge categories,\nwith models showing specific strengths in pattern recognition tasks but\nstruggling with complex numerical reasoning. The benchmark provides a\nstandardized framework for assessing LLM capabilities in realistic\nbusiness-oriented tasks rather than abstract academic problems. This research\ncontributes to the growing field of AI benchmarking by establishing proficiency\namong the 1.5 billion people who daily use Microsoft Excel as a meaningful\nevaluation metric that bridges the gap between academic AI benchmarks and\npractical business applications."}
{"id": "2505.03825", "pdf": "https://arxiv.org/pdf/2505.03825", "abs": "https://arxiv.org/abs/2505.03825", "authors": ["Anushiya Arunan", "Yan Qin", "Xiaoli Li", "Yuen Chau"], "title": "Intelligently Augmented Contrastive Tensor Factorization: Empowering Multi-dimensional Time Series Classification in Low-Data Environments", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted in Expert Systems with Applications (DOI pending)", "summary": "Classification of multi-dimensional time series from real-world systems\nrequire fine-grained learning of complex features such as cross-dimensional\ndependencies and intra-class variations-all under the practical challenge of\nlow training data availability. However, standard deep learning (DL) struggles\nto learn generalizable features in low-data environments due to model\noverfitting. We propose a versatile yet data-efficient framework, Intelligently\nAugmented Contrastive Tensor Factorization (ITA-CTF), to learn effective\nrepresentations from multi-dimensional time series. The CTF module learns core\nexplanatory components of the time series (e.g., sensor factors, temporal\nfactors), and importantly, their joint dependencies. Notably, unlike standard\ntensor factorization (TF), the CTF module incorporates a new contrastive loss\noptimization to induce similarity learning and class-awareness into the learnt\nrepresentations for better classification performance. To strengthen this\ncontrastive learning, the preceding ITA module generates targeted but\ninformative augmentations that highlight realistic intra-class patterns in the\noriginal data, while preserving class-wise properties. This is achieved by\ndynamically sampling a \"soft\" class prototype to guide the warping of each\nquery data sample, which results in an augmentation that is intelligently\npattern-mixed between the \"soft\" class prototype and the query sample. These\naugmentations enable the CTF module to recognize complex intra-class variations\ndespite the limited original training data, and seek out invariant class-wise\nproperties for accurate classification performance. The proposed method is\ncomprehensively evaluated on five different classification tasks. Compared to\nstandard TF and several DL benchmarks, notable performance improvements up to\n18.7% were achieved."}
{"id": "2505.04139", "pdf": "https://arxiv.org/pdf/2505.04139", "abs": "https://arxiv.org/abs/2505.04139", "authors": ["Hongyi Li", "Jun Xu", "William Ward Armstrong"], "title": "LHT: Statistically-Driven Oblique Decision Trees for Interpretable Classification", "categories": ["cs.LG"], "comment": null, "summary": "We introduce the Learning Hyperplane Tree (LHT), a novel oblique decision\ntree model designed for expressive and interpretable classification. LHT\nfundamentally distinguishes itself through a non-iterative,\nstatistically-driven approach to constructing splitting hyperplanes. Unlike\nmethods that rely on iterative optimization or heuristics, LHT directly\ncomputes the hyperplane parameters, which are derived from feature weights\nbased on the differences in feature expectations between classes within each\nnode. This deterministic mechanism enables a direct and well-defined hyperplane\nconstruction process. Predictions leverage a unique piecewise linear membership\nfunction within leaf nodes, obtained via local least-squares fitting. We\nformally analyze the convergence of the LHT splitting process, ensuring that\neach split yields meaningful, non-empty partitions. Furthermore, we establish\nthat the time complexity for building an LHT up to depth $d$ is $O(mnd)$,\ndemonstrating the practical feasibility of constructing trees with powerful\noblique splits using this methodology. The explicit feature weighting at each\nsplit provides inherent interpretability. Experimental results on benchmark\ndatasets demonstrate LHT's competitive accuracy, positioning it as a practical,\ntheoretically grounded, and interpretable alternative in the landscape of\ntree-based models. The implementation of the proposed method is available at\nhttps://github.com/Hongyi-Li-sz/LHT_model."}
{"id": "2505.03826", "pdf": "https://arxiv.org/pdf/2505.03826", "abs": "https://arxiv.org/abs/2505.03826", "authors": ["Minji Kang", "Seongho Kim", "Eunseo Go", "Donghyeon Paek", "Geon Lim", "Muyoung Kim", "Soyeun Kim", "Sung Kyu Jang", "Min Sup Choi", "Woo Seok Kang", "Jaehyun Kim", "Jaekwang Kim", "Hyeong-U Kim"], "title": "In-situ and Non-contact Etch Depth Prediction in Plasma Etching via Machine Learning (ANN & BNN) and Digital Image Colorimetry", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages", "summary": "Precise monitoring of etch depth and the thickness of insulating materials,\nsuch as Silicon dioxide and silicon nitride, is critical to ensuring device\nperformance and yield in semiconductor manufacturing. While conventional\nex-situ analysis methods are accurate, they are constrained by time delays and\ncontamination risks. To address these limitations, this study proposes a\nnon-contact, in-situ etch depth prediction framework based on machine learning\n(ML) techniques. Two scenarios are explored. In the first scenario, an\nartificial neural network (ANN) is trained to predict average etch depth from\nprocess parameters, achieving a significantly lower mean squared error (MSE)\ncompared to a linear baseline model. The approach is then extended to\nincorporate variability from repeated measurements using a Bayesian Neural\nNetwork (BNN) to capture both aleatoric and epistemic uncertainty. Coverage\nanalysis confirms the BNN's capability to provide reliable uncertainty\nestimates. In the second scenario, we demonstrate the feasibility of using RGB\ndata from digital image colorimetry (DIC) as input for etch depth prediction,\nachieving strong performance even in the absence of explicit process\nparameters. These results suggest that the integration of DIC and ML offers a\nviable, cost-effective alternative for real-time, in-situ, and non-invasive\nmonitoring in plasma etching processes, contributing to enhanced process\nstability, and manufacturing efficiency."}
{"id": "2505.04158", "pdf": "https://arxiv.org/pdf/2505.04158", "abs": "https://arxiv.org/abs/2505.04158", "authors": ["Yulong Wang", "Yushuo Liu", "Xiaoyi Duan", "Kai Wang"], "title": "FilterTS: Comprehensive Frequency Filtering for Multivariate Time Series Forecasting", "categories": ["cs.LG"], "comment": "Accepted to AAAI 2025", "summary": "Multivariate time series forecasting is crucial across various industries,\nwhere accurate extraction of complex periodic and trend components can\nsignificantly enhance prediction performance. However, existing models often\nstruggle to capture these intricate patterns. To address these challenges, we\npropose FilterTS, a novel forecasting model that utilizes specialized filtering\ntechniques based on the frequency domain. FilterTS introduces a Dynamic\nCross-Variable Filtering Module, a key innovation that dynamically leverages\nother variables as filters to extract and reinforce shared variable frequency\ncomponents across variables in multivariate time series. Additionally, a Static\nGlobal Filtering Module captures stable frequency components, identified\nthroughout the entire training set. Moreover, the model is built in the\nfrequency domain, converting time-domain convolutions into frequency-domain\nmultiplicative operations to enhance computational efficiency. Extensive\nexperimental results on eight real-world datasets have demonstrated that\nFilterTS significantly outperforms existing methods in terms of prediction\naccuracy and computational efficiency."}
{"id": "2505.03827", "pdf": "https://arxiv.org/pdf/2505.03827", "abs": "https://arxiv.org/abs/2505.03827", "authors": ["Xin Wang", "Ling Feng", "Huijun Zhang", "Lei Cao", "Kaisheng Zeng", "Qi Li", "Yang Ding", "Yi Dai", "David Clifton"], "title": "MISE: Meta-knowledge Inheritance for Social Media-Based Stressor Estimation", "categories": ["cs.LG", "cs.AI"], "comment": "WWW2025, Oral Presentation", "summary": "Stress haunts people in modern society, which may cause severe health issues\nif left unattended. With social media becoming an integral part of daily life,\nleveraging social media to detect stress has gained increasing attention. While\nthe majority of the work focuses on classifying stress states and stress\ncategories, this study introduce a new task aimed at estimating more specific\nstressors (like exam, writing paper, etc.) through users' posts on social\nmedia. Unfortunately, the diversity of stressors with many different classes\nbut a few examples per class, combined with the consistent arising of new\nstressors over time, hinders the machine understanding of stressors. To this\nend, we cast the stressor estimation problem within a practical scenario\nfew-shot learning setting, and propose a novel meta-learning based stressor\nestimation framework that is enhanced by a meta-knowledge inheritance\nmechanism. This model can not only learn generic stressor context through\nmeta-learning, but also has a good generalization ability to estimate new\nstressors with little labeled data. A fundamental breakthrough in our approach\nlies in the inclusion of the meta-knowledge inheritance mechanism, which equips\nour model with the ability to prevent catastrophic forgetting when adapting to\nnew stressors. The experimental results show that our model achieves\nstate-of-the-art performance compared with the baselines. Additionally, we\nconstruct a social media-based stressor estimation dataset that can help train\nartificial intelligence models to facilitate human well-being. The dataset is\nnow public at\n\\href{https://www.kaggle.com/datasets/xinwangcs/stressor-cause-of-mental-health-problem-dataset}{\\underline{Kaggle}}\nand\n\\href{https://huggingface.co/datasets/XinWangcs/Stressor}{\\underline{Hugging\nFace}}."}
{"id": "2505.04161", "pdf": "https://arxiv.org/pdf/2505.04161", "abs": "https://arxiv.org/abs/2505.04161", "authors": ["Baida Zhang", "Yakai Chen", "Huichun Li", "Zhenghu Zu"], "title": "Optimization of Infectious Disease Intervention Measures Based on Reinforcement Learning -- Empirical analysis based on UK COVID-19 epidemic data", "categories": ["cs.LG", "cs.CY", "cs.MA", "physics.comp-ph"], "comment": null, "summary": "Globally, the outbreaks of infectious diseases have exerted an extremely\nprofound and severe influence on health security and the economy. During the\ncritical phases of epidemics, devising effective intervention measures poses a\nsignificant challenge to both the academic and practical arenas. There is\nnumerous research based on reinforcement learning to optimize intervention\nmeasures of infectious diseases. Nevertheless, most of these efforts have been\nconfined within the differential equation based on infectious disease models.\nAlthough a limited number of studies have incorporated reinforcement learning\nmethodologies into individual-based infectious disease models, the models\nemployed therein have entailed simplifications and limitations, rendering it\nincapable of modeling the complexity and dynamics inherent in infectious\ndisease transmission. We establish a decision-making framework based on an\nindividual agent-based transmission model, utilizing reinforcement learning to\ncontinuously explore and develop a strategy function. The framework's validity\nis verified through both experimental and theoretical approaches. Covasim, a\ndetailed and widely used agent-based disease transmission model, was modified\nto support reinforcement learning research. We conduct an exhaustive\nexploration of the application efficacy of multiple algorithms across diverse\naction spaces. Furthermore, we conduct an innovative preliminary theoretical\nanalysis concerning the issue of \"time coverage\". The results of the experiment\nrobustly validate the effectiveness and feasibility of the methodological\nframework of this study. The coping strategies gleaned therefrom prove highly\nefficacious in suppressing the expansion of the epidemic scale and safeguarding\nthe stability of the economic system, thereby providing crucial reference\nperspectives for the formulation of global public health security strategies."}
{"id": "2505.03828", "pdf": "https://arxiv.org/pdf/2505.03828", "abs": "https://arxiv.org/abs/2505.03828", "authors": ["Yogesh Gajula"], "title": "Sentiment-Aware Recommendation Systems in E-Commerce: A Review from a Natural Language Processing Perspective", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": "12 pages, 2 tables, 2 figures", "summary": "E-commerce platforms generate vast volumes of user feedback, such as star\nratings, written reviews, and comments. However, most recommendation engines\nrely primarily on numerical scores, often overlooking the nuanced opinions\nembedded in free text. This paper comprehensively reviews sentiment-aware\nrecommendation systems from a natural language processing perspective, covering\nadvancements from 2023 to early 2025. It highlights the benefits of integrating\nsentiment analysis into e-commerce recommenders to enhance prediction accuracy\nand explainability through detailed opinion extraction. Our survey categorizes\nrecent work into four main approaches: deep learning classifiers that combine\nsentiment embeddings with user item interactions, transformer based methods for\nnuanced feature extraction, graph neural networks that propagate sentiment\nsignals, and conversational recommenders that adapt in real time to user\nfeedback. We summarize model architectures and demonstrate how sentiment flows\nthrough recommendation pipelines, impacting dialogue-based suggestions. Key\nchallenges include handling noisy or sarcastic text, dynamic user preferences,\nand bias mitigation. Finally, we outline research gaps and provide a roadmap\nfor developing smarter, fairer, and more user-centric recommendation tools."}
{"id": "2505.04163", "pdf": "https://arxiv.org/pdf/2505.04163", "abs": "https://arxiv.org/abs/2505.04163", "authors": ["Sungwon Han", "Seungeon Lee", "Meeyoung Cha", "Sercan O Arik", "Jinsung Yoon"], "title": "Retrieval Augmented Time Series Forecasting", "categories": ["cs.LG", "cs.IR"], "comment": null, "summary": "Time series forecasting uses historical data to predict future trends,\nleveraging the relationships between past observations and available features.\nIn this paper, we propose RAFT, a retrieval-augmented time series forecasting\nmethod to provide sufficient inductive biases and complement the model's\nlearning capacity. When forecasting the subsequent time frames, we directly\nretrieve historical data candidates from the training dataset with patterns\nmost similar to the input, and utilize the future values of these candidates\nalongside the inputs to obtain predictions. This simple approach augments the\nmodel's capacity by externally providing information about past patterns via\nretrieval modules. Our empirical evaluations on ten benchmark datasets show\nthat RAFT consistently outperforms contemporary baselines with an average win\nratio of 86%."}
{"id": "2505.03829", "pdf": "https://arxiv.org/pdf/2505.03829", "abs": "https://arxiv.org/abs/2505.03829", "authors": ["Yogesh Kumar"], "title": "VideoLLM Benchmarks and Evaluation: A Survey", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 2 Tables", "summary": "The rapid development of Large Language Models (LLMs) has catalyzed\nsignificant advancements in video understanding technologies. This survey\nprovides a comprehensive analysis of benchmarks and evaluation methodologies\nspecifically designed or used for Video Large Language Models (VideoLLMs). We\nexamine the current landscape of video understanding benchmarks, discussing\ntheir characteristics, evaluation protocols, and limitations. The paper\nanalyzes various evaluation methodologies, including closed-set, open-set, and\nspecialized evaluations for temporal and spatiotemporal understanding tasks. We\nhighlight the performance trends of state-of-the-art VideoLLMs across these\nbenchmarks and identify key challenges in current evaluation frameworks.\nAdditionally, we propose future research directions to enhance benchmark\ndesign, evaluation metrics, and protocols, including the need for more diverse,\nmultimodal, and interpretability-focused benchmarks. This survey aims to equip\nresearchers with a structured understanding of how to effectively evaluate\nVideoLLMs and identify promising avenues for advancing the field of video\nunderstanding with large language models."}
{"id": "2505.04167", "pdf": "https://arxiv.org/pdf/2505.04167", "abs": "https://arxiv.org/abs/2505.04167", "authors": ["Yulong Wang", "Xiaofeng Hu", "Xiaojian Cui", "Kai Wang"], "title": "STRGCN: Capturing Asynchronous Spatio-Temporal Dependencies for Irregular Multivariate Time Series Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "Irregular multivariate time series (IMTS) are prevalent in real-world\napplications across many fields, where varying sensor frequencies and\nasynchronous measurements pose significant modeling challenges. Existing\nsolutions often rely on a pre-alignment strategy to normalize data, which can\ndistort intrinsic patterns and escalate computational and memory demands.\nAddressing these limitations, we introduce STRGCN, a Spatio-Temporal Relational\nGraph Convolutional Network that avoids pre-alignment and directly captures the\ncomplex interdependencies in IMTS by representing them as a fully connected\ngraph. Each observation is represented as a node, allowing the model to\neffectively handle misaligned timestamps by mapping all inter-node\nrelationships, thus faithfully preserving the asynchronous nature of the data.\nMoreover, we enhance this model with a hierarchical ``Sandwich'' structure that\nstrategically aggregates nodes to optimize graph embeddings, reducing\ncomputational overhead while maintaining detailed local and global context.\nExtensive experiments on four public datasets demonstrate that STRGCN achieves\nstate-of-the-art accuracy, competitive memory usage and training speed."}
{"id": "2505.03832", "pdf": "https://arxiv.org/pdf/2505.03832", "abs": "https://arxiv.org/abs/2505.03832", "authors": ["Noor B. Tayfor", "Tarik A. Rashid", "Shko M. Qader", "Bryar A. Hassan", "Mohammed H. Abdalla", "Jafar Majidpour", "Aram M. Ahmed", "Hussein M. Ali", "Aso M. Aladdin", "Abdulhady A. Abdullah", "Ahmed S. Shamsaldin", "Haval M. Sidqi", "Abdulrahman Salih", "Zaher M. Yaseen", "Azad A. Ameen", "Janmenjoy Nayak", "Mahmood Yashar Hamza"], "title": "Video Forgery Detection for Surveillance Cameras: A Review", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The widespread availability of video recording through smartphones and\ndigital devices has made video-based evidence more accessible than ever.\nSurveillance footage plays a crucial role in security, law enforcement, and\njudicial processes. However, with the rise of advanced video editing tools,\ntampering with digital recordings has become increasingly easy, raising\nconcerns about their authenticity. Ensuring the integrity of surveillance\nvideos is essential, as manipulated footage can lead to misinformation and\nundermine judicial decisions. This paper provides a comprehensive review of\nexisting forensic techniques used to detect video forgery, focusing on their\neffectiveness in verifying the authenticity of surveillance recordings. Various\nmethods, including compression-based analysis, frame duplication detection, and\nmachine learning-based approaches, are explored. The findings highlight the\ngrowing necessity for more robust forensic techniques to counteract evolving\nforgery methods. Strengthening video forensic capabilities will ensure that\nsurveillance recordings remain credible and admissible as legal evidence."}
{"id": "2505.04173", "pdf": "https://arxiv.org/pdf/2505.04173", "abs": "https://arxiv.org/abs/2505.04173", "authors": ["Zixiao Wang", "Wenqian Zhao", "Yunheng Shen", "Yang Bai", "Guojin Chen", "Farzan Farnia", "Bei Yu"], "title": "DiffPattern-Flex: Efficient Layout Pattern Generation via Discrete Diffusion", "categories": ["cs.LG"], "comment": "13 pages, 13 figures. Accepted by TCAD", "summary": "Recent advancements in layout pattern generation have been dominated by deep\ngenerative models. However, relying solely on neural networks for legality\nguarantees raises concerns in many practical applications. In this paper, we\npresent \\tool{DiffPattern}-Flex, a novel approach designed to generate reliable\nlayout patterns efficiently. \\tool{DiffPattern}-Flex incorporates a new method\nfor generating diverse topologies using a discrete diffusion model while\nmaintaining a lossless and compute-efficient layout representation. To ensure\nlegal pattern generation, we employ {an} optimization-based, white-box pattern\nassessment process based on specific design rules. Furthermore, fast sampling\nand efficient legalization technologies are employed to accelerate the\ngeneration process. Experimental results across various benchmarks demonstrate\nthat \\tool{DiffPattern}-Flex significantly outperforms existing methods and\nexcels at producing reliable layout patterns."}
{"id": "2505.03833", "pdf": "https://arxiv.org/pdf/2505.03833", "abs": "https://arxiv.org/abs/2505.03833", "authors": ["Xuechao Wang", "Sven Nomm", "Junqing Huang", "Kadri Medijainen", "Aaro Toomela", "Michael Ruzhansky"], "title": "PointExplainer: Towards Transparent Parkinson's Disease Diagnosis", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Deep neural networks have shown potential in analyzing digitized hand-drawn\nsignals for early diagnosis of Parkinson's disease. However, the lack of clear\ninterpretability in existing diagnostic methods presents a challenge to\nclinical trust. In this paper, we propose PointExplainer, an explainable\ndiagnostic strategy to identify hand-drawn regions that drive model diagnosis.\nSpecifically, PointExplainer assigns discrete attribution values to hand-drawn\nsegments, explicitly quantifying their relative contributions to the model's\ndecision. Its key components include: (i) a diagnosis module, which encodes\nhand-drawn signals into 3D point clouds to represent hand-drawn trajectories,\nand (ii) an explanation module, which trains an interpretable surrogate model\nto approximate the local behavior of the black-box diagnostic model. We also\nintroduce consistency measures to further address the issue of faithfulness in\nexplanations. Extensive experiments on two benchmark datasets and a newly\nconstructed dataset show that PointExplainer can provide intuitive explanations\nwith no diagnostic performance degradation. The source code is available at\nhttps://github.com/chaoxuewang/PointExplainer."}
{"id": "2505.04174", "pdf": "https://arxiv.org/pdf/2505.04174", "abs": "https://arxiv.org/abs/2505.04174", "authors": ["Ju-Hyung Lee", "Yanqing Lu"], "title": "On-Device LLM for Context-Aware Wi-Fi Roaming", "categories": ["cs.LG", "cs.AI", "cs.NI", "eess.SP"], "comment": null, "summary": "Wireless roaming is a critical yet challenging task for maintaining seamless\nconnectivity in dynamic mobile environments. Conventional threshold-based or\nheuristic schemes often fail, leading to either sticky or excessive handovers.\nWe introduce the first cross-layer use of an on-device large language model\n(LLM): high-level reasoning in the application layer that issues real-time\nactions executed in the PHY/MAC stack. The LLM addresses two tasks: (i)\ncontext-aware AP selection, where structured prompts fuse environmental cues\n(e.g., location, time) to choose the best BSSID; and (ii) dynamic threshold\nadjustment, where the model adaptively decides when to roam. To satisfy the\ntight latency and resource budgets of edge hardware, we apply a suite of\noptimizations-chain-of-thought prompting, parameter-efficient fine-tuning, and\nquantization. Experiments on indoor and outdoor datasets show that our approach\nsurpasses legacy heuristics and DRL baselines, achieving a strong balance\nbetween roaming stability and signal quality. These findings underscore the\npromise of application-layer LLM reasoning for lower-layer wireless control in\nfuture edge systems."}
{"id": "2505.03835", "pdf": "https://arxiv.org/pdf/2505.03835", "abs": "https://arxiv.org/abs/2505.03835", "authors": ["Simon Suh", "Jihyuk Bang", "Ji Woo Han"], "title": "The Shift Towards Preprints in AI Policy Research: A Comparative Study of Preprint Trends in the U.S., Europe, and South Korea", "categories": ["cs.DL", "cs.AI", "cs.CY", "I.2.0; K.4.0"], "comment": "22 pages, 6 figures, 3 tables. Uses cross-regional analysis to\n  evaluate how preprint citation trends in AI - policy research have shifted\n  over time in response to two major global events: the COVID-19 pandemic and\n  the release of ChatGPT. Compares United States, Europe, and South Korea", "summary": "The adoption of open science has quickly changed how artificial intelligence\n(AI) policy research is distributed globally. This study examines the regional\ntrends in the citation of preprints, specifically focusing on the impact of two\nmajor disruptive events: the COVID-19 pandemic and the release of ChatGPT, on\nresearch dissemination patterns in the United States, Europe, and South Korea\nfrom 2015 to 2024. Using bibliometrics data from the Web of Science, this study\ntracks how global disruptive events influenced the adoption of preprints in AI\npolicy research and how such shifts vary by region. By marking the timing of\nthese disruptive events, the analysis reveals that while all regions\nexperienced growth in preprint citations, the magnitude and trajectory of\nchange varied significantly. The United States exhibited sharp, event-driven\nincreases; Europe demonstrated institutional growth; and South Korea maintained\nconsistent, linear growth in preprint adoption. These findings suggest that\nglobal disruptions may have accelerated preprint adoption, but the extent and\ntrajectory are shaped by local research cultures, policy environments, and\nlevels of open science maturity. This paper emphasizes the need for future AI\ngovernance strategies to consider regional variability in research\ndissemination and highlights opportunities for further longitudinal and\ncomparative research to deepen our understanding of open-access adoption in AI\npolicy development."}
{"id": "2505.04193", "pdf": "https://arxiv.org/pdf/2505.04193", "abs": "https://arxiv.org/abs/2505.04193", "authors": ["Bang You", "Chenxu Wang", "Huaping Liu"], "title": "Trajectory Entropy Reinforcement Learning for Predictable and Robust Control", "categories": ["cs.LG", "cs.RO"], "comment": "10 pages", "summary": "Simplicity is a critical inductive bias for designing data-driven\ncontrollers, especially when robustness is important. Despite the impressive\nresults of deep reinforcement learning in complex control tasks, it is prone to\ncapturing intricate and spurious correlations between observations and actions,\nleading to failure under slight perturbations to the environment. To tackle\nthis problem, in this work we introduce a novel inductive bias towards simple\npolicies in reinforcement learning. The simplicity inductive bias is introduced\nby minimizing the entropy of entire action trajectories, corresponding to the\nnumber of bits required to describe information in action trajectories after\nthe agent observes state trajectories. Our reinforcement learning agent,\nTrajectory Entropy Reinforcement Learning, is optimized to minimize the\ntrajectory entropy while maximizing rewards. We show that the trajectory\nentropy can be effectively estimated by learning a variational parameterized\naction prediction model, and use the prediction model to construct an\ninformation-regularized reward function. Furthermore, we construct a practical\nalgorithm that enables the joint optimization of models, including the policy\nand the prediction model. Experimental evaluations on several high-dimensional\nlocomotion tasks show that our learned policies produce more cyclical and\nconsistent action trajectories, and achieve superior performance, and\nrobustness to noise and dynamic changes than the state-of-the-art."}
{"id": "2505.03836", "pdf": "https://arxiv.org/pdf/2505.03836", "abs": "https://arxiv.org/abs/2505.03836", "authors": ["Chongsheng Zhang", "Shuwen Wu", "Yingqi Chen", "Matthias Aßenmacher", "Christian Heumann", "Yi Men", "Gaojuan Fan", "João Gama"], "title": "OBD-Finder: Explainable Coarse-to-Fine Text-Centric Oracle Bone Duplicates Discovery", "categories": ["cs.IR", "cs.AI", "cs.CV"], "comment": "This is the long version of our OBD-Finder paper for AI-enabled\n  Oracle Bone Duplicates Discovery (currently under review at the ECML PKDD\n  2025 Demo Track). The models, video illustration and demonstration of this\n  paper are available at: https://github.com/cszhangLMU/OBD-Finder/.\n  Illustration video: https://www.youtube.com/watch?v=5QT4f0YIo0Q", "summary": "Oracle Bone Inscription (OBI) is the earliest systematic writing system in\nChina, while the identification of Oracle Bone (OB) duplicates is a fundamental\nissue in OBI research. In this work, we design a progressive OB duplicate\ndiscovery framework that combines unsupervised low-level keypoints matching\nwith high-level text-centric content-based matching to refine and rank the\ncandidate OB duplicates with semantic awareness and interpretability. We\ncompare our approach with state-of-the-art content-based image retrieval and\nimage matching methods, showing that our approach yields comparable recall\nperformance and the highest simplified mean reciprocal rank scores for both\nTop-5 and Top-15 retrieval results, and with significantly accelerated\ncomputation efficiency. We have discovered over 60 pairs of new OB duplicates\nin real-world deployment, which were missed by OBI researchers for decades. The\nmodels, video illustration and demonstration of this work are available at:\nhttps://github.com/cszhangLMU/OBD-Finder/."}
{"id": "2505.04196", "pdf": "https://arxiv.org/pdf/2505.04196", "abs": "https://arxiv.org/abs/2505.04196", "authors": ["Sung Yoo Lim", "Hyunsoo Yun", "Prateek Bansal", "Dong-Kyu Kim", "Eui-Jin Kim"], "title": "A Large Language Model for Feasible and Diverse Population Synthesis", "categories": ["cs.LG", "cs.MA"], "comment": "28 pages, 7 figures, 6 tables. Submitted to Transportation Research\n  Part C: Emerging Technologies. Preprint version", "summary": "Generating a synthetic population that is both feasible and diverse is\ncrucial for ensuring the validity of downstream activity schedule simulation in\nactivity-based models (ABMs). While deep generative models (DGMs), such as\nvariational autoencoders and generative adversarial networks, have been applied\nto this task, they often struggle to balance the inclusion of rare but\nplausible combinations (i.e., sampling zeros) with the exclusion of implausible\nones (i.e., structural zeros). To improve feasibility while maintaining\ndiversity, we propose a fine-tuning method for large language models (LLMs)\nthat explicitly controls the autoregressive generation process through\ntopological orderings derived from a Bayesian Network (BN). Experimental\nresults show that our hybrid LLM-BN approach outperforms both traditional DGMs\nand proprietary LLMs (e.g., ChatGPT-4o) with few-shot learning. Specifically,\nour approach achieves approximately 95% feasibility, significantly higher than\nthe ~80% observed in DGMs, while maintaining comparable diversity, making it\nwell-suited for practical applications. Importantly, the method is based on a\nlightweight open-source LLM, enabling fine-tuning and inference on standard\npersonal computing environments. This makes the approach cost-effective and\nscalable for large-scale applications, such as synthesizing populations in\nmegacities, without relying on expensive infrastructure. By initiating the ABM\npipeline with high-quality synthetic populations, our method improves overall\nsimulation reliability and reduces downstream error propagation. The source\ncode for these methods is available for research and practical application."}
{"id": "2505.03837", "pdf": "https://arxiv.org/pdf/2505.03837", "abs": "https://arxiv.org/abs/2505.03837", "authors": ["Rashik Shadman", "Daqing Hou", "Faraz Hussain", "M G Sarwar Murshed"], "title": "Explainable Face Recognition via Improved Localization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Biometric authentication has become one of the most widely used tools in the\ncurrent technological era to authenticate users and to distinguish between\ngenuine users and imposters. Face is the most common form of biometric modality\nthat has proven effective. Deep learning-based face recognition systems are now\ncommonly used across different domains. However, these systems usually operate\nlike black-box models that do not provide necessary explanations or\njustifications for their decisions. This is a major disadvantage because users\ncannot trust such artificial intelligence-based biometric systems and may not\nfeel comfortable using them when clear explanations or justifications are not\nprovided. This paper addresses this problem by applying an efficient method for\nexplainable face recognition systems. We use a Class Activation Mapping\n(CAM)-based discriminative localization (very narrow/specific localization)\ntechnique called Scaled Directed Divergence (SDD) to visually explain the\nresults of deep learning-based face recognition systems. We perform fine\nlocalization of the face features relevant to the deep learning model for its\nprediction/decision. Our experiments show that the SDD Class Activation Map\n(CAM) highlights the relevant face features very specifically compared to the\ntraditional CAM and very accurately. The provided visual explanations with\nnarrow localization of relevant features can ensure much-needed transparency\nand trust for deep learning-based face recognition systems."}
{"id": "2505.04200", "pdf": "https://arxiv.org/pdf/2505.04200", "abs": "https://arxiv.org/abs/2505.04200", "authors": ["Ahmed Sayeed Faruk", "Jason Sulskis", "Elena Zheleva"], "title": "Estimating Causal Effects in Networks with Cluster-Based Bandits", "categories": ["cs.LG", "cs.SI"], "comment": "Presented at the AAAI 2022 Workshop on Artificial Intelligence for\n  Behavioral Change (AI4BC)", "summary": "The gold standard for estimating causal effects is randomized controlled\ntrial (RCT) or A/B testing where a random group of individuals from a\npopulation of interest are given treatment and the outcome is compared to a\nrandom group of individuals from the same population. However, A/B testing is\nchallenging in the presence of interference, commonly occurring in social\nnetworks, where individuals can impact each others outcome. Moreover, A/B\ntesting can incur a high performance loss when one of the treatment arms has a\npoor performance and the test continues to treat individuals with it.\nTherefore, it is important to design a strategy that can adapt over time and\nefficiently learn the total treatment effect in the network. We introduce two\ncluster-based multi-armed bandit (MAB) algorithms to gradually estimate the\ntotal treatment effect in a network while maximizing the expected reward by\nmaking a tradeoff between exploration and exploitation. We compare the\nperformance of our MAB algorithms with a vanilla MAB algorithm that ignores\nclusters and the corresponding RCT methods on semi-synthetic data with\nsimulated interference. The vanilla MAB algorithm shows higher reward-action\nratio at the cost of higher treatment effect error due to undesired spillover.\nThe cluster-based MAB algorithms show higher reward-action ratio compared to\ntheir corresponding RCT methods without sacrificing much accuracy in treatment\neffect estimation."}
{"id": "2505.03838", "pdf": "https://arxiv.org/pdf/2505.03838", "abs": "https://arxiv.org/abs/2505.03838", "authors": ["Ting Yu Tsai", "An Yu", "Meghana Spurthi Maadugundu", "Ishrat Jahan Mohima", "Umme Habiba Barsha", "Mei-Hwa F. Chen", "Balakrishnan Prabhakaran", "Ming-Ching Chang"], "title": "IntelliCardiac: An Intelligent Platform for Cardiac Image Segmentation and Classification", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Precise and effective processing of cardiac imaging data is critical for the\nidentification and management of the cardiovascular diseases. We introduce\nIntelliCardiac, a comprehensive, web-based medical image processing platform\nfor the automatic segmentation of 4D cardiac images and disease classification,\nutilizing an AI model trained on the publicly accessible ACDC dataset. The\nsystem, intended for patients, cardiologists, and healthcare professionals,\noffers an intuitive interface and uses deep learning models to identify\nessential heart structures and categorize cardiac diseases. The system supports\nanalysis of both the right and left ventricles as well as myocardium, and then\nclassifies patient's cardiac images into five diagnostic categories: dilated\ncardiomyopathy, myocardial infarction, hypertrophic cardiomyopathy, right\nventricular abnormality, and no disease. IntelliCardiac combines a deep\nlearning-based segmentation model with a two-step classification pipeline. The\nsegmentation module gains an overall accuracy of 92.6\\%. The classification\nmodule, trained on characteristics taken from segmented heart structures,\nachieves 98\\% accuracy in five categories. These results exceed the performance\nof the existing state-of-the-art methods that integrate both segmentation and\nclassification models. IntelliCardiac, which supports real-time visualization,\nworkflow integration, and AI-assisted diagnostics, has great potential as a\nscalable, accurate tool for clinical decision assistance in cardiac imaging and\ndiagnosis."}
{"id": "2505.04204", "pdf": "https://arxiv.org/pdf/2505.04204", "abs": "https://arxiv.org/abs/2505.04204", "authors": ["Mateo Lopez-Ledezma", "Gissel Velarde"], "title": "Cyber Security Data Science: Machine Learning Methods and their Performance on Imbalanced Datasets", "categories": ["cs.LG"], "comment": "13 pages, 5 figures. Digital Management and Artificial Intelligence.\n  Proceedings of the Fourth International Scientific-Practical Conference (ISPC\n  2024), Hybrid, October 10-11, 2024.\n  https://link.springer.com/chapter/10.1007/978-3-031-88052-0_45", "summary": "Cybersecurity has become essential worldwide and at all levels, concerning\nindividuals, institutions, and governments. A basic principle in cybersecurity\nis to be always alert. Therefore, automation is imperative in processes where\nthe volume of daily operations is large. Several cybersecurity applications can\nbe addressed as binary classification problems, including anomaly detection,\nfraud detection, intrusion detection, spam detection, or malware detection. We\npresent three experiments. In the first experiment, we evaluate single\nclassifiers including Random Forests, Light Gradient Boosting Machine, eXtreme\nGradient Boosting, Logistic Regression, Decision Tree, and Gradient Boosting\nDecision Tree. In the second experiment, we test different sampling techniques\nincluding over-sampling, under-sampling, Synthetic Minority Over-sampling\nTechnique, and Self-Paced Ensembling. In the last experiment, we evaluate\nSelf-Paced Ensembling and its number of base classifiers. We found that\nimbalance learning techniques had positive and negative effects, as reported in\nrelated studies. Thus, these techniques should be applied with caution.\nBesides, we found different best performers for each dataset. Therefore, we\nrecommend testing single classifiers and imbalance learning techniques for each\nnew dataset and application involving imbalanced datasets as is the case in\nseveral cyber security applications."}
{"id": "2505.03840", "pdf": "https://arxiv.org/pdf/2505.03840", "abs": "https://arxiv.org/abs/2505.03840", "authors": ["Cairong Yan", "Jinyi Han", "Jin Ju", "Yanting Zhang", "Zijian Wang", "Xuan Shao"], "title": "CoCoB: Adaptive Collaborative Combinatorial Bandits for Online Recommendation", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": "This paper has been accepted by DASFAA 2025: The International\n  Conference on Database Systems for Advanced Applications. This version\n  provides more detailed information", "summary": "Clustering bandits have gained significant attention in recommender systems\nby leveraging collaborative information from neighboring users to better\ncapture target user preferences. However, these methods often lack a clear\ndefinition of similar users and face challenges when users with unique\npreferences lack appropriate neighbors. In such cases, relying on divergent\npreferences of misidentified neighbors can degrade recommendation quality. To\naddress these limitations, this paper proposes an adaptive Collaborative\nCombinatorial Bandits algorithm (CoCoB). CoCoB employs an innovative two-sided\nbandit architecture, applying bandit principles to both the user and item\nsides. The user-bandit employs an enhanced Bayesian model to explore user\nsimilarity, identifying neighbors based on a similarity probability threshold.\nThe item-bandit treats items as arms, generating diverse recommendations\ninformed by the user-bandit's output. CoCoB dynamically adapts, leveraging\nneighbor preferences when available or focusing solely on the target user\notherwise. Regret analysis under a linear contextual bandit setting and\nexperiments on three real-world datasets demonstrate CoCoB's effectiveness,\nachieving an average 2.4% improvement in F1 score over state-of-the-art\nmethods."}
{"id": "2505.04223", "pdf": "https://arxiv.org/pdf/2505.04223", "abs": "https://arxiv.org/abs/2505.04223", "authors": ["Sanghyeon Park", "Soo-Mook Moon"], "title": "FRAIN to Train: A Fast-and-Reliable Solution for Decentralized Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated learning (FL) enables collaborative model training across\ndistributed clients while preserving data locality. Although FedAvg pioneered\nsynchronous rounds for global model averaging, slower devices can delay\ncollective progress. Asynchronous FL (e.g., FedAsync) addresses stragglers by\ncontinuously integrating client updates, yet naive implementations risk client\ndrift due to non-IID data and stale contributions. Some Blockchain-based FL\napproaches (e.g., BRAIN) employ robust weighting or scoring of updates to\nresist malicious or misaligned proposals. However, performance drops can still\npersist under severe data heterogeneity or high staleness, and synchronization\noverhead has emerged as a new concern due to its aggregator-free architectures.\n  We introduce Fast-and-Reliable AI Network, FRAIN, a new asynchronous FL\nmethod that mitigates these limitations by incorporating two key ideas. First,\nour FastSync strategy eliminates the need to replay past model versions,\nenabling newcomers and infrequent participants to efficiently approximate the\nglobal model. Second, we adopt spherical linear interpolation (SLERP) when\nmerging parameters, preserving models' directions and alleviating destructive\ninterference from divergent local training.\n  Experiments with a CNN image-classification model and a Transformer-based\nlanguage model demonstrate that FRAIN achieves more stable and robust\nconvergence than FedAvg, FedAsync, and BRAIN, especially under harsh\nenvironments: non-IID data distributions, networks that experience delays and\nrequire frequent re-synchronization, and the presence of malicious nodes."}
{"id": "2505.03844", "pdf": "https://arxiv.org/pdf/2505.03844", "abs": "https://arxiv.org/abs/2505.03844", "authors": ["Solène Debuysère", "Nicolas Trouvé", "Nathan Letheule", "Olivier Lévêque", "Elise Colin"], "title": "From Spaceborn to Airborn: SAR Image Synthesis Using Foundation Models for Multi-Scale Adaptation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "The availability of Synthetic Aperture Radar (SAR) satellite imagery has\nincreased considerably in recent years, with datasets commercially available.\nHowever, the acquisition of high-resolution SAR images in airborne\nconfigurations, remains costly and limited. Thus, the lack of open source,\nwell-labeled, or easily exploitable SAR text-image datasets is a barrier to the\nuse of existing foundation models in remote sensing applications. In this\ncontext, synthetic image generation is a promising solution to augment this\nscarce data, enabling a broader range of applications. Leveraging over 15 years\nof ONERA's extensive archival airborn data from acquisition campaigns, we\ncreated a comprehensive training dataset of 110 thousands SAR images to exploit\na 3.5 billion parameters pre-trained latent diffusion model. In this work, we\npresent a novel approach utilizing spatial conditioning techniques within a\nfoundation model to transform satellite SAR imagery into airborne SAR\nrepresentations. Additionally, we demonstrate that our pipeline is effective\nfor bridging the realism of simulated images generated by ONERA's physics-based\nsimulator EMPRISE. Our method explores a key application of AI in advancing SAR\nimaging technology. To the best of our knowledge, we are the first to introduce\nthis approach in the literature."}
{"id": "2505.04241", "pdf": "https://arxiv.org/pdf/2505.04241", "abs": "https://arxiv.org/abs/2505.04241", "authors": ["Grzegorz Miebs", "Rafał A. Bachorz"], "title": "Technology prediction of a 3D model using Neural Network", "categories": ["cs.LG", "I.2.10"], "comment": "5 pages, 2 figures", "summary": "Accurate estimation of production times is critical for effective\nmanufacturing scheduling, yet traditional methods relying on expert analysis or\nhistorical data often fall short in dynamic or customized production\nenvironments. This paper introduces a data-driven approach that predicts\nmanufacturing steps and their durations directly from a product's 3D model. By\nrendering the model into multiple 2D images and leveraging a neural network\ninspired by the Generative Query Network, the method learns to map geometric\nfeatures into time estimates for predefined production steps enabling scalable,\nadaptive, and precise process planning across varied product types."}
{"id": "2505.03845", "pdf": "https://arxiv.org/pdf/2505.03845", "abs": "https://arxiv.org/abs/2505.03845", "authors": ["Ioannis Kyprakis", "Vasileios Skaramagkas", "Iro Boura", "Georgios Karamanis", "Dimitrios I. Fotiadis", "Zinovia Kefalopoulou", "Cleanthe Spanaki", "Manolis Tsiknakis"], "title": "A Deep Learning approach for Depressive Symptoms assessment in Parkinson's disease patients using facial videos", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Parkinson's disease (PD) is a neurodegenerative disorder, manifesting with\nmotor and non-motor symptoms. Depressive symptoms are prevalent in PD,\naffecting up to 45% of patients. They are often underdiagnosed due to\noverlapping motor features, such as hypomimia. This study explores deep\nlearning (DL) models-ViViT, Video Swin Tiny, and 3D CNN-LSTM with attention\nlayers-to assess the presence and severity of depressive symptoms, as detected\nby the Geriatric Depression Scale (GDS), in PD patients through facial video\nanalysis. The same parameters were assessed in a secondary analysis taking into\naccount whether patients were one hour after (ON-medication state) or 12 hours\nwithout (OFF-medication state) dopaminergic medication. Using a dataset of\n1,875 videos from 178 patients, the Video Swin Tiny model achieved the highest\nperformance, with up to 94% accuracy and 93.7% F1-score in binary\nclassification (presence of absence of depressive symptoms), and 87.1% accuracy\nwith an 85.4% F1-score in multiclass tasks (absence or mild or severe\ndepressive symptoms)."}
{"id": "2505.04263", "pdf": "https://arxiv.org/pdf/2505.04263", "abs": "https://arxiv.org/abs/2505.04263", "authors": ["Jan Blechschmidt", "Tom-Christian Riemer", "Max Winkler", "Martin Stoll", "Jan-F. Pietschmann"], "title": "Physics-Informed DeepONets for drift-diffusion on metric graphs: simulation and parameter identification", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": null, "summary": "We develop a novel physics informed deep learning approach for solving\nnonlinear drift-diffusion equations on metric graphs. These models represent an\nimportant model class with a large number of applications in areas ranging from\ntransport in biological cells to the motion of human crowds. While traditional\nnumerical schemes require a large amount of tailoring, especially in the case\nof model design or parameter identification problems, physics informed deep\noperator networks (DeepONet) have emerged as a versatile tool for the solution\nof partial differential equations with the particular advantage that they\neasily incorporate parameter identification questions. We here present an\napproach where we first learn three DeepONet models for representative inflow,\ninner and outflow edges, resp., and then subsequently couple these models for\nthe solution of the drift-diffusion metric graph problem by relying on an\nedge-based domain decomposition approach. We illustrate that our framework is\napplicable for the accurate evaluation of graph-coupled physics models and is\nwell suited for solving optimization or inverse problems on these coupled\nnetworks."}
{"id": "2505.03846", "pdf": "https://arxiv.org/pdf/2505.03846", "abs": "https://arxiv.org/abs/2505.03846", "authors": ["Kangsheng Wang", "Yuhang Li", "Chengwei Ye", "Yufei Lin", "Huanzhen Zhang", "Bohan Hu", "Linuo Xu", "Shuyan Liu"], "title": "GAME: Learning Multimodal Interactions via Graph Structures for Personality Trait Estimation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Apparent personality analysis from short videos poses significant chal-lenges\ndue to the complex interplay of visual, auditory, and textual cues. In this\npaper, we propose GAME, a Graph-Augmented Multimodal Encoder designed to\nrobustly model and fuse multi-source features for automatic personality\nprediction. For the visual stream, we construct a facial graph and introduce a\ndual-branch Geo Two-Stream Network, which combines Graph Convolutional Networks\n(GCNs) and Convolutional Neural Net-works (CNNs) with attention mechanisms to\ncapture both structural and appearance-based facial cues. Complementing this,\nglobal context and iden-tity features are extracted using pretrained ResNet18\nand VGGFace back-bones. To capture temporal dynamics, frame-level features are\nprocessed by a BiGRU enhanced with temporal attention modules. Meanwhile, audio\nrepresentations are derived from the VGGish network, and linguistic se-mantics\nare captured via the XLM-Roberta transformer. To achieve effective multimodal\nintegration, we propose a Channel Attention-based Fusion module, followed by a\nMulti-Layer Perceptron (MLP) regression head for predicting personality traits.\nExtensive experiments show that GAME con-sistently outperforms existing methods\nacross multiple benchmarks, vali-dating its effectiveness and generalizability."}
{"id": "2505.04278", "pdf": "https://arxiv.org/pdf/2505.04278", "abs": "https://arxiv.org/abs/2505.04278", "authors": ["Weiwei Ye", "Zhuopeng Xu", "Ning Gui"], "title": "Non-stationary Diffusion For Probabilistic Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted as spotlight poster at ICML", "summary": "Due to the dynamics of underlying physics and external influences, the\nuncertainty of time series often varies over time. However, existing Denoising\nDiffusion Probabilistic Models (DDPMs) often fail to capture this\nnon-stationary nature, constrained by their constant variance assumption from\nthe additive noise model (ANM). In this paper, we innovatively utilize the\nLocation-Scale Noise Model (LSNM) to relax the fixed uncertainty assumption of\nANM. A diffusion-based probabilistic forecasting framework, termed\nNon-stationary Diffusion (NsDiff), is designed based on LSNM that is capable of\nmodeling the changing pattern of uncertainty. Specifically, NsDiff combines a\ndenoising diffusion-based conditional generative model with a pre-trained\nconditional mean and variance estimator, enabling adaptive endpoint\ndistribution modeling. Furthermore, we propose an uncertainty-aware noise\nschedule, which dynamically adjusts the noise levels to accurately reflect the\ndata uncertainty at each step and integrates the time-varying variances into\nthe diffusion process. Extensive experiments conducted on nine real-world and\nsynthetic datasets demonstrate the superior performance of NsDiff compared to\nexisting approaches. Code is available at https://github.com/wwy155/NsDiff."}
{"id": "2505.03848", "pdf": "https://arxiv.org/pdf/2505.03848", "abs": "https://arxiv.org/abs/2505.03848", "authors": ["Janhavi Giri", "Attila Lengyel", "Don Kent", "Edward Kibardin"], "title": "Advanced Clustering Framework for Semiconductor Image Analytics Integrating Deep TDA with Self-Supervised and Transfer Learning Techniques", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.LG"], "comment": "46 pages, 22 figures, 5 tables", "summary": "Semiconductor manufacturing generates vast amounts of image data, crucial for\ndefect identification and yield optimization, yet often exceeds manual\ninspection capabilities. Traditional clustering techniques struggle with\nhigh-dimensional, unlabeled data, limiting their effectiveness in capturing\nnuanced patterns. This paper introduces an advanced clustering framework that\nintegrates deep Topological Data Analysis (TDA) with self-supervised and\ntransfer learning techniques, offering a novel approach to unsupervised image\nclustering. TDA captures intrinsic topological features, while self-supervised\nlearning extracts meaningful representations from unlabeled data, reducing\nreliance on labeled datasets. Transfer learning enhances the framework's\nadaptability and scalability, allowing fine-tuning to new datasets without\nretraining from scratch. Validated on synthetic and open-source semiconductor\nimage datasets, the framework successfully identifies clusters aligned with\ndefect patterns and process variations. This study highlights the\ntransformative potential of combining TDA, self-supervised learning, and\ntransfer learning, providing a scalable solution for proactive process\nmonitoring and quality control in semiconductor manufacturing and other domains\nwith large-scale image datasets."}
{"id": "2505.04318", "pdf": "https://arxiv.org/pdf/2505.04318", "abs": "https://arxiv.org/abs/2505.04318", "authors": ["Jacob Glenn Ayers", "Buvaneswari A. Ramanan", "Manzoor A. Khan"], "title": "Detecting Concept Drift in Neural Networks Using Chi-squared Goodness of Fit Testing", "categories": ["cs.LG", "cs.AI", "eess.IV"], "comment": "8 pages, 6 figures, 1 table", "summary": "As the adoption of deep learning models has grown beyond human capacity for\nverification, meta-algorithms are needed to ensure reliable model inference.\nConcept drift detection is a field dedicated to identifying statistical shifts\nthat is underutilized in monitoring neural networks that may encounter\ninference data with distributional characteristics diverging from their\ntraining data. Given the wide variety of model architectures, applications, and\ndatasets, it is important that concept drift detection algorithms are adaptable\nto different inference scenarios. In this paper, we introduce an application of\nthe $\\chi^2$ Goodness of Fit Hypothesis Test as a drift detection\nmeta-algorithm applied to a multilayer perceptron, a convolutional neural\nnetwork, and a transformer trained for machine vision as they are exposed to\nsimulated drift during inference. To that end, we demonstrate how unexpected\ndrops in accuracy due to concept drift can be detected without directly\nexamining the inference outputs. Our approach enhances safety by ensuring\nmodels are continually evaluated for reliability across varying conditions."}
{"id": "2505.03850", "pdf": "https://arxiv.org/pdf/2505.03850", "abs": "https://arxiv.org/abs/2505.03850", "authors": ["Hanlin Chen", "Simin Chen", "Wenyu Li", "Wei Yang", "Yiheng Feng"], "title": "Impact Analysis of Inference Time Attack of Perception Sensors on Autonomous Vehicles", "categories": ["cs.CR", "cs.AI"], "comment": "Accepted and presented in TRBAM 2024", "summary": "As a safety-critical cyber-physical system, cybersecurity and related safety\nissues for Autonomous Vehicles (AVs) have been important research topics for a\nwhile. Among all the modules on AVs, perception is one of the most accessible\nattack surfaces, as drivers and AVs have no control over the outside\nenvironment. Most current work targeting perception security for AVs focuses on\nperception correctness. In this work, we propose an impact analysis based on\ninference time attacks for autonomous vehicles. We demonstrate in a simulation\nsystem that such inference time attacks can also threaten the safety of both\nthe ego vehicle and other traffic participants."}
{"id": "2505.04335", "pdf": "https://arxiv.org/pdf/2505.04335", "abs": "https://arxiv.org/abs/2505.04335", "authors": ["Swagato Das", "Arghya Pratihar", "Swagatam Das"], "title": "Hyperbolic Fuzzy $C$-Means with Adaptive Weight-based Filtering for Clustering in Non-Euclidean Spaces", "categories": ["cs.LG"], "comment": null, "summary": "Clustering algorithms play a pivotal role in unsupervised learning by\nidentifying and grouping similar objects based on shared characteristics. While\ntraditional clustering techniques, such as hard and fuzzy center-based\nclustering, have been widely used, they struggle with complex,\nhigh-dimensional, and non-Euclidean datasets. In particular, the Fuzzy\n$C$-Means (FCM) algorithm, despite its efficiency and popularity, exhibits\nnotable limitations in non-Euclidean spaces. Euclidean spaces assume linear\nseparability and uniform distance scaling, limiting their effectiveness in\ncapturing complex, hierarchical, or non-Euclidean structures in fuzzy\nclustering. To overcome these challenges, we introduce Filtration-based\nHyperbolic Fuzzy $C$-Means (HypeFCM), a novel clustering algorithm tailored for\nbetter representation of data relationships in non-Euclidean spaces. HypeFCM\nintegrates the principles of fuzzy clustering with hyperbolic geometry and\nemploys a weight-based filtering mechanism to improve performance. The\nalgorithm initializes weights using a Dirichlet distribution and iteratively\nrefines cluster centroids and membership assignments based on a hyperbolic\nmetric in the Poincar\\'e Disc model. Extensive experimental evaluations\ndemonstrate that HypeFCM significantly outperforms conventional fuzzy\nclustering methods in non-Euclidean settings, underscoring its robustness and\neffectiveness."}
{"id": "2505.03853", "pdf": "https://arxiv.org/pdf/2505.03853", "abs": "https://arxiv.org/abs/2505.03853", "authors": ["Changxi Chi", "Jun Xia", "Jingbo Zhou", "Jiabei Cheng", "Chang Yu", "Stan Z. Li"], "title": "GRAPE: Heterogeneous Graph Representation Learning for Genetic Perturbation with Coding and Non-Coding Biotype", "categories": ["q-bio.QM", "cs.AI"], "comment": null, "summary": "Predicting genetic perturbations enables the identification of potentially\ncrucial genes prior to wet-lab experiments, significantly improving overall\nexperimental efficiency. Since genes are the foundation of cellular life,\nbuilding gene regulatory networks (GRN) is essential to understand and predict\nthe effects of genetic perturbations. However, current methods fail to fully\nleverage gene-related information, and solely rely on simple evaluation metrics\nto construct coarse-grained GRN. More importantly, they ignore functional\ndifferences between biotypes, limiting the ability to capture potential gene\ninteractions. In this work, we leverage pre-trained large language model and\nDNA sequence model to extract features from gene descriptions and DNA sequence\ndata, respectively, which serve as the initialization for gene representations.\nAdditionally, we introduce gene biotype information for the first time in\ngenetic perturbation, simulating the distinct roles of genes with different\nbiotypes in regulating cellular processes, while capturing implicit gene\nrelationships through graph structure learning (GSL). We propose GRAPE, a\nheterogeneous graph neural network (HGNN) that leverages gene representations\ninitialized with features from descriptions and sequences, models the distinct\nroles of genes with different biotypes, and dynamically refines the GRN through\nGSL. The results on publicly available datasets show that our method achieves\nstate-of-the-art performance."}
{"id": "2505.04338", "pdf": "https://arxiv.org/pdf/2505.04338", "abs": "https://arxiv.org/abs/2505.04338", "authors": ["Zichen Liu", "Wei Zhang", "Christof Schütte", "Tiejun Li"], "title": "Riemannian Denoising Diffusion Probabilistic Models", "categories": ["cs.LG"], "comment": "28 pages", "summary": "We propose Riemannian Denoising Diffusion Probabilistic Models (RDDPMs) for\nlearning distributions on submanifolds of Euclidean space that are level sets\nof functions, including most of the manifolds relevant to applications.\nExisting methods for generative modeling on manifolds rely on substantial\ngeometric information such as geodesic curves or eigenfunctions of the\nLaplace-Beltrami operator and, as a result, they are limited to manifolds where\nsuch information is available. In contrast, our method, built on a projection\nscheme, can be applied to more general manifolds, as it only requires being\nable to evaluate the value and the first order derivatives of the function that\ndefines the submanifold. We provide a theoretical analysis of our method in the\ncontinuous-time limit, which elucidates the connection between our RDDPMs and\nscore-based generative models on manifolds. The capability of our method is\ndemonstrated on datasets from previous studies and on new datasets sampled from\ntwo high-dimensional manifolds, i.e. $\\mathrm{SO}(10)$ and the configuration\nspace of molecular system alanine dipeptide with fixed dihedral angle."}
{"id": "2505.03856", "pdf": "https://arxiv.org/pdf/2505.03856", "abs": "https://arxiv.org/abs/2505.03856", "authors": ["Tin Mišić", "Karlo Koledić", "Fabio Bonsignorio", "Ivan Petrović", "Ivan Marković"], "title": "An Active Inference Model of Covert and Overt Visual Attention", "categories": ["cs.CV", "cs.AI", "I.2.6; I.2.10"], "comment": "7 pages, 7 figures. Code available at\n  https://github.com/unizgfer-lamor/ainf-visual-attention", "summary": "The ability to selectively attend to relevant stimuli while filtering out\ndistractions is essential for agents that process complex, high-dimensional\nsensory input. This paper introduces a model of covert and overt visual\nattention through the framework of active inference, utilizing dynamic\noptimization of sensory precisions to minimize free-energy. The model\ndetermines visual sensory precisions based on both current environmental\nbeliefs and sensory input, influencing attentional allocation in both covert\nand overt modalities. To test the effectiveness of the model, we analyze its\nbehavior in the Posner cueing task and a simple target focus task using\ntwo-dimensional(2D) visual data. Reaction times are measured to investigate the\ninterplay between exogenous and endogenous attention, as well as valid and\ninvalid cueing. The results show that exogenous and valid cues generally lead\nto faster reaction times compared to endogenous and invalid cues. Furthermore,\nthe model exhibits behavior similar to inhibition of return, where previously\nattended locations become suppressed after a specific cue-target onset\nasynchrony interval. Lastly, we investigate different aspects of overt\nattention and show that involuntary, reflexive saccades occur faster than\nintentional ones, but at the expense of adaptability."}
{"id": "2505.04339", "pdf": "https://arxiv.org/pdf/2505.04339", "abs": "https://arxiv.org/abs/2505.04339", "authors": ["Hao Peng", "Xiang Huang", "Shuo Sun", "Ruitong Zhang", "Philip S. Yu"], "title": "Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "DBSCAN, a well-known density-based clustering algorithm, has gained\nwidespread popularity and usage due to its effectiveness in identifying\nclusters of arbitrary shapes and handling noisy data. However, it encounters\nchallenges in producing satisfactory cluster results when confronted with\ndatasets of varying density scales, a common scenario in real-world\napplications. In this paper, we propose a novel Adaptive and Robust DBSCAN with\nMulti-agent Reinforcement Learning cluster framework, namely AR-DBSCAN. First,\nwe model the initial dataset as a two-level encoding tree and categorize the\ndata vertices into distinct density partitions according to the information\nuncertainty determined in the encoding tree. Each partition is then assigned to\nan agent to find the best clustering parameters without manual assistance. The\nallocation is density-adaptive, enabling AR-DBSCAN to effectively handle\ndiverse density distributions within the dataset by utilizing distinct agents\nfor different partitions. Second, a multi-agent deep reinforcement learning\nguided automatic parameter searching process is designed. The process of\nadjusting the parameter search direction by perceiving the clustering\nenvironment is modeled as a Markov decision process. Using a weakly-supervised\nreward training policy network, each agent adaptively learns the optimal\nclustering parameters by interacting with the clusters. Third, a recursive\nsearch mechanism adaptable to the data's scale is presented, enabling efficient\nand controlled exploration of large parameter spaces. Extensive experiments are\nconducted on nine artificial datasets and a real-world dataset. The results of\noffline and online tasks show that AR-DBSCAN not only improves clustering\naccuracy by up to 144.1% and 175.3% in the NMI and ARI metrics, respectively,\nbut also is capable of robustly finding dominant parameters."}
{"id": "2505.03859", "pdf": "https://arxiv.org/pdf/2505.03859", "abs": "https://arxiv.org/abs/2505.03859", "authors": ["Will Hawkins", "Chris Russell", "Brent Mittelstadt"], "title": "Deepfakes on Demand: the rise of accessible non-consensual deepfake image generators", "categories": ["cs.CY", "cs.AI", "cs.CV", "68T01"], "comment": "13 pages", "summary": "Advances in multimodal machine learning have made text-to-image (T2I) models\nincreasingly accessible and popular. However, T2I models introduce risks such\nas the generation of non-consensual depictions of identifiable individuals,\notherwise known as deepfakes. This paper presents an empirical study exploring\nthe accessibility of deepfake model variants online. Through a metadata\nanalysis of thousands of publicly downloadable model variants on two popular\nrepositories, Hugging Face and Civitai, we demonstrate a huge rise in easily\naccessible deepfake models. Almost 35,000 examples of publicly downloadable\ndeepfake model variants are identified, primarily hosted on Civitai. These\ndeepfake models have been downloaded almost 15 million times since November\n2022, with the models targeting a range of individuals from global celebrities\nto Instagram users with under 10,000 followers. Both Stable Diffusion and Flux\nmodels are used for the creation of deepfake models, with 96% of these\ntargeting women and many signalling intent to generate non-consensual intimate\nimagery (NCII). Deepfake model variants are often created via the\nparameter-efficient fine-tuning technique known as low rank adaptation (LoRA),\nrequiring as few as 20 images, 24GB VRAM, and 15 minutes of time, making this\nprocess widely accessible via consumer-grade computers. Despite these models\nviolating the Terms of Service of hosting platforms, and regulation seeking to\nprevent dissemination, these results emphasise the pressing need for greater\naction to be taken against the creation of deepfakes and NCII."}
{"id": "2505.04340", "pdf": "https://arxiv.org/pdf/2505.04340", "abs": "https://arxiv.org/abs/2505.04340", "authors": ["Hong Jin", "Kaicheng Zhou", "Jie Yin", "Lan You", "Zhifeng Zhou"], "title": "Multi-Granular Attention based Heterogeneous Hypergraph Neural Network", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Heterogeneous graph neural networks (HeteGNNs) have demonstrated strong\nabilities to learn node representations by effectively extracting complex\nstructural and semantic information in heterogeneous graphs. Most of the\nprevailing HeteGNNs follow the neighborhood aggregation paradigm, leveraging\nmeta-path based message passing to learn latent node representations. However,\ndue to the pairwise nature of meta-paths, these models fail to capture\nhigh-order relations among nodes, resulting in suboptimal performance.\nAdditionally, the challenge of ``over-squashing'', where long-range message\npassing in HeteGNNs leads to severe information distortion, further limits the\nefficacy of these models. To address these limitations, this paper proposes\nMGA-HHN, a Multi-Granular Attention based Heterogeneous Hypergraph Neural\nNetwork for heterogeneous graph representation learning. MGA-HHN introduces two\nkey innovations: (1) a novel approach for constructing meta-path based\nheterogeneous hypergraphs that explicitly models higher-order semantic\ninformation in heterogeneous graphs through multiple views, and (2) a\nmulti-granular attention mechanism that operates at both the node and hyperedge\nlevels. This mechanism enables the model to capture fine-grained interactions\namong nodes sharing the same semantic context within a hyperedge type, while\npreserving the diversity of semantics across different hyperedge types. As\nsuch, MGA-HHN effectively mitigates long-range message distortion and generates\nmore expressive node representations. Extensive experiments on real-world\nbenchmark datasets demonstrate that MGA-HHN outperforms state-of-the-art\nmodels, showcasing its effectiveness in node classification, node clustering\nand visualization tasks."}
{"id": "2505.03863", "pdf": "https://arxiv.org/pdf/2505.03863", "abs": "https://arxiv.org/abs/2505.03863", "authors": ["Atanu Kundu", "Sauvik Gon", "Rajarshi Ray"], "title": "Data-Driven Falsification of Cyber-Physical Systems", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Cyber-Physical Systems (CPS) are abundant in safety-critical domains such as\nhealthcare, avionics, and autonomous vehicles. Formal verification of their\noperational safety is, therefore, of utmost importance. In this paper, we\naddress the falsification problem, where the focus is on searching for an\nunsafe execution in the system instead of proving their absence. The\ncontribution of this paper is a framework that (a) connects the falsification\nof CPS with the falsification of deep neural networks (DNNs) and (b) leverages\nthe inherent interpretability of Decision Trees for faster falsification of\nCPS. This is achieved by: (1) building a surrogate model of the CPS under test,\neither as a DNN model or a Decision Tree, (2) application of various DNN\nfalsification tools to falsify CPS, and (3) a novel falsification algorithm\nguided by the explanations of safety violations of the CPS model extracted from\nits Decision Tree surrogate. The proposed framework has the potential to\nexploit a repertoire of \\emph{adversarial attack} algorithms designed to\nfalsify robustness properties of DNNs, as well as state-of-the-art\nfalsification algorithms for DNNs. Although the presented methodology is\napplicable to systems that can be executed/simulated in general, we demonstrate\nits effectiveness, particularly in CPS. We show that our framework, implemented\nas a tool \\textsc{FlexiFal}, can detect hard-to-find counterexamples in CPS\nthat have linear and non-linear dynamics. Decision tree-guided falsification\nshows promising results in efficiently finding multiple counterexamples in the\nARCH-COMP 2024 falsification benchmarks~\\cite{khandait2024arch}."}
{"id": "2505.04346", "pdf": "https://arxiv.org/pdf/2505.04346", "abs": "https://arxiv.org/abs/2505.04346", "authors": ["Arghya Pratihar", "Kushal Bose", "Swagatam Das"], "title": "Topology-Driven Clustering: Enhancing Performance with Betti Number Filtration", "categories": ["cs.LG"], "comment": null, "summary": "Clustering aims to form groups of similar data points in an unsupervised\nregime. Yet, clustering complex datasets containing critically intertwined\nshapes poses significant challenges. The prevailing clustering algorithms\nwidely depend on evaluating similarity measures based on Euclidean metrics.\nExploring topological characteristics to perform clustering of complex datasets\ninevitably presents a better scope. The topological clustering algorithms\npredominantly perceive the point set through the lens of Simplicial complexes\nand Persistent homology. Despite these approaches, the existing topological\nclustering algorithms cannot somehow fully exploit topological structures and\nshow inconsistent performances on some highly complicated datasets. This work\naims to mitigate the limitations by identifying topologically similar neighbors\nthrough the Vietoris-Rips complex and Betti number filtration. In addition, we\nintroduce the concept of the Betti sequences to capture flexibly essential\nfeatures from the topological structures. Our proposed algorithm is adept at\nclustering complex, intertwined shapes contained in the datasets. We carried\nout experiments on several synthetic and real-world datasets. Our algorithm\ndemonstrated commendable performances across the datasets compared to some of\nthe well-known topology-based clustering algorithms."}
{"id": "2505.03864", "pdf": "https://arxiv.org/pdf/2505.03864", "abs": "https://arxiv.org/abs/2505.03864", "authors": ["Qiaomu Li", "Ying Xie"], "title": "From Glue-Code to Protocols: A Critical Analysis of A2A and MCP Integration for Scalable Agent Systems", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "Artificial intelligence is rapidly evolving towards multi-agent systems where\nnumerous AI agents collaborate and interact with external tools. Two key open\nstandards, Google's Agent to Agent (A2A) protocol for inter-agent communication\nand Anthropic's Model Context Protocol (MCP) for standardized tool access,\npromise to overcome the limitations of fragmented, custom integration\napproaches. While their potential synergy is significant, this paper argues\nthat effectively integrating A2A and MCP presents unique, emergent challenges\nat their intersection, particularly concerning semantic interoperability\nbetween agent tasks and tool capabilities, the compounded security risks\narising from combined discovery and execution, and the practical governance\nrequired for the envisioned \"Agent Economy\". This work provides a critical\nanalysis, moving beyond a survey to evaluate the practical implications and\ninherent difficulties of combining these horizontal and vertical integration\nstandards. We examine the benefits (e.g., specialization, scalability) while\ncritically assessing their dependencies and trade-offs in an integrated\ncontext. We identify key challenges increased by the integration, including\nnovel security vulnerabilities, privacy complexities, debugging difficulties\nacross protocols, and the need for robust semantic negotiation mechanisms. In\nsummary, A2A+MCP offers a vital architectural foundation, but fully realizing\nits potential requires substantial advancements to manage the complexities of\ntheir combined operation."}
{"id": "2505.04367", "pdf": "https://arxiv.org/pdf/2505.04367", "abs": "https://arxiv.org/abs/2505.04367", "authors": ["Stavros Sykiotis"], "title": "Deep Learning Innovations for Energy Efficiency: Advances in Non-Intrusive Load Monitoring and EV Charging Optimization for a Sustainable Grid", "categories": ["cs.LG"], "comment": "PhD thesis", "summary": "The global energy landscape is undergoing a profound transformation, often\nreferred to as the energy transition, driven by the urgent need to mitigate\nclimate change, reduce greenhouse gas emissions, and ensure sustainable energy\nsupplies. However, the undoubted complexity of new investments in renewables,\nas well as the phase out of high CO2-emission energy sources, hampers the pace\nof the energy transition and raises doubts as to whether new renewable energy\nsources are capable of solely meeting the climate target goals. This highlights\nthe need to investigate alternative pathways to accelerate the energy\ntransition, by identifying human activity domains with higher/excessive energy\ndemands. Two notable examples where there is room for improvement, in the sense\nof reducing energy consumption and consequently CO2 emissions, are residential\nenergy consumption and road transport. This dissertation investigates the\ndevelopment of novel Deep Learning techniques to create tools which solve\nlimitations in these two key energy domains. Reduction of residential energy\nconsumption can be achieved by empowering end-users with the user of\nNon-Intrusive Load Monitoring, whereas optimization of EV charging with Deep\nReinforcement Learning can tackle road transport decarbonization."}
{"id": "2505.03867", "pdf": "https://arxiv.org/pdf/2505.03867", "abs": "https://arxiv.org/abs/2505.03867", "authors": ["Stefania Druga", "Amy J. Ko"], "title": "Scratch Copilot: Supporting Youth Creative Coding with AI", "categories": ["cs.HC", "cs.AI"], "comment": "5 figures, 14 pages", "summary": "Creative coding platforms like Scratch have democratized programming for\nchildren, yet translating imaginative ideas into functional code remains a\nsignificant hurdle for many young learners. While AI copilots assist adult\nprogrammers, few tools target children in block-based environments. Building on\nprior research \\cite{druga_how_2021,druga2023ai, druga2023scratch}, we present\nCognimates Scratch Copilot: an AI-powered assistant integrated into a\nScratch-like environment, providing real-time support for ideation, code\ngeneration, debugging, and asset creation. This paper details the system\narchitecture and findings from an exploratory qualitative evaluation with 18\ninternational children (ages 7--12). Our analysis reveals how the AI Copilot\nsupported key creative coding processes, particularly aiding ideation and\ndebugging. Crucially, it also highlights how children actively negotiated the\nuse of AI, demonstrating strong agency by adapting or rejecting suggestions to\nmaintain creative control. Interactions surfaced design tensions between\nproviding helpful scaffolding and fostering independent problem-solving, as\nwell as learning opportunities arising from navigating AI limitations and\nerrors. Findings indicate Cognimates Scratch Copilot's potential to enhance\ncreative self-efficacy and engagement. Based on these insights, we propose\ninitial design guidelines for AI coding assistants that prioritize youth agency\nand critical interaction alongside supportive scaffolding."}
{"id": "2505.04371", "pdf": "https://arxiv.org/pdf/2505.04371", "abs": "https://arxiv.org/abs/2505.04371", "authors": ["Filipe Santos", "João Paulo Fernandes", "Luís Macedo"], "title": "Extending a Quantum Reinforcement Learning Exploration Policy with Flags to Connect Four", "categories": ["cs.LG"], "comment": "8 pages, 3 figures, to be submitted to a journal", "summary": "Action selection based on flags is a Reinforcement Learning (RL) exploration\npolicy that improves the exploration of the state space through the use of\nflags, which can identify the most promising actions to take in each state. The\nquantum counterpart of this exploration policy further improves upon this by\ntaking advantage of a quadratic speedup for sampling flagged actions. This\napproach has already been successfully employed for the game of Checkers. In\nthis work, we describe the application of this method to the context of Connect\nFour, in order to study its performance in a different setting, which can lead\nto a better generalization of the technique. We also kept track of a metric\nthat wasn't taken into account in previous work: the average number of\niterations to obtain a flagged action. Since going second is a significant\ndisadvantage in Connect Four, we also had the intent of exploring how this more\ncomplex scenario would impact the performance of our approach. The experiments\ninvolved training and testing classical and quantum RL agents that played\neither going first or going second against a Randomized Negamax opponent. The\nresults showed that both flagged exploration policies were clearly superior to\na simple epsilon-greedy policy. Furthermore, the quantum agents did in fact\nsample flagged actions in less iterations. Despite obtaining tagged actions\nmore consistently, the win rates between the classical and quantum versions of\nthe approach were identical, which could be due to the simplicity of the\ntraining scenario chosen."}
{"id": "2505.03896", "pdf": "https://arxiv.org/pdf/2505.03896", "abs": "https://arxiv.org/abs/2505.03896", "authors": ["Shuang Zeng", "Chee Hong Lee", "Micky C Nnamdi", "Wenqi Shi", "J Ben Tamo", "Lei Zhu", "Hangzhou He", "Xinliang Zhang", "Qian Chen", "May D. Wang", "Yanye Lu", "Qiushi Ren"], "title": "Novel Extraction of Discriminative Fine-Grained Feature to Improve Retinal Vessel Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Retinal vessel segmentation is a vital early detection method for several\nsevere ocular diseases. Despite significant progress in retinal vessel\nsegmentation with the advancement of Neural Networks, there are still\nchallenges to overcome. Specifically, retinal vessel segmentation aims to\npredict the class label for every pixel within a fundus image, with a primary\nfocus on intra-image discrimination, making it vital for models to extract more\ndiscriminative features. Nevertheless, existing methods primarily focus on\nminimizing the difference between the output from the decoder and the label,\nbut ignore fully using feature-level fine-grained representations from the\nencoder. To address these issues, we propose a novel Attention U-shaped\nKolmogorov-Arnold Network named AttUKAN along with a novel Label-guided\nPixel-wise Contrastive Loss for retinal vessel segmentation. Specifically, we\nimplement Attention Gates into Kolmogorov-Arnold Networks to enhance model\nsensitivity by suppressing irrelevant feature activations and model\ninterpretability by non-linear modeling of KAN blocks. Additionally, we also\ndesign a novel Label-guided Pixel-wise Contrastive Loss to supervise our\nproposed AttUKAN to extract more discriminative features by distinguishing\nbetween foreground vessel-pixel pairs and background pairs. Experiments are\nconducted across four public datasets including DRIVE, STARE, CHASE_DB1, HRF\nand our private dataset. AttUKAN achieves F1 scores of 82.50%, 81.14%, 81.34%,\n80.21% and 80.09%, along with MIoU scores of 70.24%, 68.64%, 68.59%, 67.21% and\n66.94% in the above datasets, which are the highest compared to 11 networks for\nretinal vessel segmentation. Quantitative and qualitative results show that our\nAttUKAN achieves state-of-the-art performance and outperforms existing retinal\nvessel segmentation methods. Our code will be available at\nhttps://github.com/stevezs315/AttUKAN."}
{"id": "2505.04389", "pdf": "https://arxiv.org/pdf/2505.04389", "abs": "https://arxiv.org/abs/2505.04389", "authors": ["Jenni Lampainen", "Kaisa Joki", "Napsu Karmitsa", "Marko M. Mäkelä"], "title": "Clust-Splitter $-$ an Efficient Nonsmooth Optimization-Based Algorithm for Clustering Large Datasets", "categories": ["cs.LG", "90C90, 90C26"], "comment": "36 pages, 23 figures", "summary": "Clustering is a fundamental task in data mining and machine learning,\nparticularly for analyzing large-scale data. In this paper, we introduce\nClust-Splitter, an efficient algorithm based on nonsmooth optimization,\ndesigned to solve the minimum sum-of-squares clustering problem in very large\ndatasets. The clustering task is approached through a sequence of three\nnonsmooth optimization problems: two auxiliary problems used to generate\nsuitable starting points, followed by a main clustering formulation. To solve\nthese problems effectively, the limited memory bundle method is combined with\nan incremental approach to develop the Clust-Splitter algorithm. We evaluate\nClust-Splitter on real-world datasets characterized by both a large number of\nattributes and a large number of data points and compare its performance with\nseveral state-of-the-art large-scale clustering algorithms. Experimental\nresults demonstrate the efficiency of the proposed method for clustering very\nlarge datasets, as well as the high quality of its solutions, which are on par\nwith those of the best existing methods."}
{"id": "2505.03899", "pdf": "https://arxiv.org/pdf/2505.03899", "abs": "https://arxiv.org/abs/2505.03899", "authors": ["Danial Davarnia", "Mohammadreza Kiaghadi"], "title": "A Graphical Global Optimization Framework for Parameter Estimation of Statistical Models with Nonconvex Regularization Functions", "categories": ["math.OC", "cs.AI", "math.ST", "stat.TH"], "comment": null, "summary": "Optimization problems with norm-bounding constraints arise in a variety of\napplications, including portfolio optimization, machine learning, and feature\nselection. A common approach to these problems involves relaxing the norm\nconstraint via Lagrangian relaxation, transforming it into a regularization\nterm in the objective function. A particularly challenging class includes the\nzero-norm function, which promotes sparsity in statistical parameter\nestimation. Most existing exact methods for solving these problems introduce\nbinary variables and artificial bounds to reformulate them as\nhigher-dimensional mixed-integer programs, solvable by standard solvers. Other\nexact approaches exploit specific structural properties of the objective,\nmaking them difficult to generalize across different problem types. Alternative\nmethods employ nonconvex penalties with favorable statistical characteristics,\nbut these are typically addressed using heuristic or local optimization\ntechniques due to their structural complexity. In this paper, we propose a\nnovel graph-based method to globally solve optimization problems involving\ngeneralized norm-bounding constraints. Our approach encompasses standard\n$\\ell_p$-norms for $p \\in [0, \\infty)$ and nonconvex penalties such as SCAD and\nMCP. We leverage decision diagrams to construct strong convex relaxations\ndirectly in the original variable space, eliminating the need for auxiliary\nvariables or artificial bounds. Integrated into a spatial branch-and-cut\nframework, our method guarantees convergence to the global optimum. We\ndemonstrate its effectiveness through preliminary computational experiments on\nbenchmark sparse linear regression problems involving complex nonconvex\npenalties, which are not tractable using existing global optimization\ntechniques."}
{"id": "2505.04396", "pdf": "https://arxiv.org/pdf/2505.04396", "abs": "https://arxiv.org/abs/2505.04396", "authors": ["Jingnan Wang", "Jie Chao", "Shangshang Yang", "Congyi Nai", "Kaijun Ren", "Kefeng Deng", "Xi Chen", "Yaxin Liu", "Hanqiuzi Wen", "Ziniu Xiao", "Lifeng Zhang", "Xiaodong Wang", "Jiping Guan", "Baoxiang Pan"], "title": "Supporting renewable energy planning and operation with data-driven high-resolution ensemble weather forecast", "categories": ["cs.LG", "physics.ao-ph"], "comment": null, "summary": "The planning and operation of renewable energy, especially wind power, depend\ncrucially on accurate, timely, and high-resolution weather information.\nCoarse-grid global numerical weather forecasts are typically downscaled to meet\nthese requirements, introducing challenges of scale inconsistency, process\nrepresentation error, computation cost, and entanglement of distinct\nuncertainty sources from chaoticity, model bias, and large-scale forcing. We\naddress these challenges by learning the climatological distribution of a\ntarget wind farm using its high-resolution numerical weather simulations. An\noptimal combination of this learned high-resolution climatological prior with\ncoarse-grid large scale forecasts yields highly accurate, fine-grained,\nfull-variable, large ensemble of weather pattern forecasts. Using observed\nmeteorological records and wind turbine power outputs as references, the\nproposed methodology verifies advantageously compared to existing\nnumerical/statistical forecasting-downscaling pipelines, regarding either\ndeterministic/probabilistic skills or economic gains. Moreover, a 100-member,\n10-day forecast with spatial resolution of 1 km and output frequency of 15 min\ntakes < 1 hour on a moderate-end GPU, as contrast to $\\mathcal{O}(10^3)$ CPU\nhours for conventional numerical simulation. By drastically reducing\ncomputational costs while maintaining accuracy, our method paves the way for\nmore efficient and reliable renewable energy planning and operation."}
{"id": "2505.03945", "pdf": "https://arxiv.org/pdf/2505.03945", "abs": "https://arxiv.org/abs/2505.03945", "authors": ["Shamnad Mohamed Shaffi", "Sunish Vengathattil", "Jezeena Nikarthil Sidhick", "Resmi Vijayan"], "title": "AI-Driven Security in Cloud Computing: Enhancing Threat Detection, Automated Response, and Cyber Resilience", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Cloud security concerns have been greatly realized in recent years due to the\nincrease of complicated threats in the computing world. Many traditional\nsolutions do not work well in real-time to detect or prevent more complex\nthreats. Artificial intelligence is today regarded as a revolution in\ndetermining a protection plan for cloud data architecture through machine\nlearning, statistical visualization of computing infrastructure, and detection\nof security breaches followed by counteraction. These AI-enabled systems make\nwork easier as more network activities are scrutinized, and any anomalous\nbehavior that might be a precursor to a more serious breach is prevented. This\npaper examines ways AI can enhance cloud security by applying predictive\nanalytics, behavior-based security threat detection, and AI-stirring\nencryption. It also outlines the problems of the previous security models and\nhow AI overcomes them. For a similar reason, issues like data privacy, biases\nin the AI model, and regulatory compliance are also covered. So, AI improves\nthe protection of cloud computing contexts; however, more efforts are needed in\nthe subsequent phases to extend the technology's reliability, modularity, and\nethical aspects. This means that AI can be blended with other new computing\ntechnologies, including blockchain, to improve security frameworks further. The\npaper discusses the current trends in securing cloud data architecture using AI\nand presents further research and application directions."}
{"id": "2505.04412", "pdf": "https://arxiv.org/pdf/2505.04412", "abs": "https://arxiv.org/abs/2505.04412", "authors": ["Ren Wang", "Pengcheng Zhou"], "title": "Latent Manifold Reconstruction and Representation with Topological and Geometrical Regularization", "categories": ["cs.LG"], "comment": "25 pages, 11 figures, 4 tables", "summary": "Manifold learning aims to discover and represent low-dimensional structures\nunderlying high-dimensional data while preserving critical topological and\ngeometric properties. Existing methods often fail to capture local details with\nglobal topological integrity from noisy data or construct a balanced\ndimensionality reduction, resulting in distorted or fractured embeddings. We\npresent an AutoEncoder-based method that integrates a manifold reconstruction\nlayer, which uncovers latent manifold structures from noisy point clouds, and\nfurther provides regularizations on topological and geometric properties during\ndimensionality reduction, whereas the two components promote each other during\ntraining. Experiments on point cloud datasets demonstrate that our method\noutperforms baselines like t-SNE, UMAP, and Topological AutoEncoders in\ndiscovering manifold structures from noisy data and preserving them through\ndimensionality reduction, as validated by visualization and quantitative\nmetrics. This work demonstrates the significance of combining manifold\nreconstruction with manifold learning to achieve reliable representation of the\nlatent manifold, particularly when dealing with noisy real-world data. Code\nrepository: https://github.com/Thanatorika/mrtg."}
{"id": "2505.03946", "pdf": "https://arxiv.org/pdf/2505.03946", "abs": "https://arxiv.org/abs/2505.03946", "authors": ["Matthew Sgambati", "Aleksandar Vakanski", "Matthew Anderson"], "title": "Decentralized Distributed Proximal Policy Optimization (DD-PPO) for High Performance Computing Scheduling on Multi-User Systems", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "Resource allocation in High Performance Computing (HPC) environments presents\na complex and multifaceted challenge for job scheduling algorithms. Beyond the\nefficient allocation of system resources, schedulers must account for and\noptimize multiple performance metrics, including job wait time and system\nutilization. While traditional rule-based scheduling algorithms dominate the\ncurrent deployments of HPC systems, the increasing heterogeneity and scale of\nthose systems is expected to challenge the efficiency and flexibility of those\nalgorithms in minimizing job wait time and maximizing utilization. Recent\nresearch efforts have focused on leveraging advancements in Reinforcement\nLearning (RL) to develop more adaptable and intelligent scheduling strategies.\nRecent RL-based scheduling approaches have explored a range of algorithms, from\nDeep Q-Networks (DQN) to Proximal Policy Optimization (PPO), and more recently,\nhybrid methods that integrate Graph Neural Networks with RL techniques.\nHowever, a common limitation across these methods is their reliance on\nrelatively small datasets, and these methods face scalability issues when using\nlarge datasets. This study introduces a novel RL-based scheduler utilizing the\nDecentralized Distributed Proximal Policy Optimization (DD-PPO) algorithm,\nwhich supports large-scale distributed training across multiple workers without\nrequiring parameter synchronization at every step. By eliminating reliance on\ncentralized updates to a shared policy, the DD-PPO scheduler enhances\nscalability, training efficiency, and sample utilization. The validation\ndataset leveraged over 11.5 million real HPC job traces for comparing DD-PPO\nperformance between traditional and advanced scheduling approaches, and the\nexperimental results demonstrate improved scheduling performance in comparison\nto both rule-based schedulers and existing RL-based scheduling algorithms."}
{"id": "2505.04417", "pdf": "https://arxiv.org/pdf/2505.04417", "abs": "https://arxiv.org/abs/2505.04417", "authors": ["Georg A. Gottwald", "Shuigen Liu", "Youssef Marzouk", "Sebastian Reich", "Xin T. Tong"], "title": "Localized Diffusion Models for High Dimensional Distributions Generation", "categories": ["cs.LG", "cs.NA", "math.NA", "stat.ML"], "comment": null, "summary": "Diffusion models are the state-of-the-art tools for various generative tasks.\nHowever, estimating high-dimensional score functions makes them potentially\nsuffer from the curse of dimensionality (CoD). This underscores the importance\nof better understanding and exploiting low-dimensional structure in the target\ndistribution. In this work, we consider locality structure, which describes\nsparse dependencies between model components. Under locality structure, the\nscore function is effectively low-dimensional, so that it can be estimated by a\nlocalized neural network with significantly reduced sample complexity. This\nmotivates the localized diffusion model, where a localized score matching loss\nis used to train the score function within a localized hypothesis space. We\nprove that such localization enables diffusion models to circumvent CoD, at the\nprice of additional localization error. Under realistic sample size scaling, we\nshow both theoretically and numerically that a moderate localization radius can\nbalance the statistical and localization error, leading to a better overall\nperformance. The localized structure also facilitates parallel training of\ndiffusion models, making it potentially more efficient for large-scale\napplications."}
{"id": "2505.03974", "pdf": "https://arxiv.org/pdf/2505.03974", "abs": "https://arxiv.org/abs/2505.03974", "authors": ["Nikhil M. Pawar", "Jorge A. Prozzi", "Feng Hong", "Surya Sarat Chandra Congress"], "title": "Deep Learning Framework for Infrastructure Maintenance: Crack Detection and High-Resolution Imaging of Infrastructure Surfaces", "categories": ["cs.CV", "cs.AI"], "comment": "Presented :Transportation Research Board 104th Annual Meeting,\n  Washington, D.C", "summary": "Recently, there has been an impetus for the application of cutting-edge data\ncollection platforms such as drones mounted with camera sensors for\ninfrastructure asset management. However, the sensor characteristics, proximity\nto the structure, hard-to-reach access, and environmental conditions often\nlimit the resolution of the datasets. A few studies used super-resolution\ntechniques to address the problem of low-resolution images. Nevertheless, these\ntechniques were observed to increase computational cost and false alarms of\ndistress detection due to the consideration of all the infrastructure images\ni.e., positive and negative distress classes. In order to address the\npre-processing of false alarm and achieve efficient super-resolution, this\nstudy developed a framework consisting of convolutional neural network (CNN)\nand efficient sub-pixel convolutional neural network (ESPCNN). CNN accurately\nclassified both the classes. ESPCNN, which is the lightweight super-resolution\ntechnique, generated high-resolution infrastructure image of positive distress\nobtained from CNN. The ESPCNN outperformed bicubic interpolation in all the\nevaluation metrics for super-resolution. Based on the performance metrics, the\ncombination of CNN and ESPCNN was observed to be effective in preprocessing the\ninfrastructure images with negative distress, reducing the computational cost\nand false alarms in the next step of super-resolution. The visual inspection\nshowed that EPSCNN is able to capture crack propagation, complex geometry of\neven minor cracks. The proposed framework is expected to help the highway\nagencies in accurately performing distress detection and assist in efficient\nasset management practices."}
{"id": "2505.04435", "pdf": "https://arxiv.org/pdf/2505.04435", "abs": "https://arxiv.org/abs/2505.04435", "authors": ["Vahideh Hayyolalam", "Öznur Özkasap"], "title": "FedBWO: Enhancing Communication Efficiency in Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": "5th IEEE International Conference on Human-Machine Systems, Abu\n  Dhabi, UAE, 26-28 May 2025", "summary": "Federated Learning (FL) is a distributed Machine Learning (ML) setup, where a\nshared model is collaboratively trained by various clients using their local\ndatasets while keeping the data private. Considering resource-constrained\ndevices, FL clients often suffer from restricted transmission capacity. Aiming\nto enhance the system performance, the communication between clients and server\nneeds to be diminished. Current FL strategies transmit a tremendous amount of\ndata (model weights) within the FL process, which needs a high communication\nbandwidth. Considering resource constraints, increasing the number of clients\nand, consequently, the amount of data (model weights) can lead to a bottleneck.\nIn this paper, we introduce the Federated Black Widow Optimization (FedBWO)\ntechnique to decrease the amount of transmitted data by transmitting only a\nperformance score rather than the local model weights from clients. FedBWO\nemploys the BWO algorithm to improve local model updates. The conducted\nexperiments prove that FedBWO remarkably improves the performance of the global\nmodel and the communication efficiency of the overall system. According to the\nexperimental outcomes, FedBWO enhances the global model accuracy by an average\nof 21% over FedAvg, and 12% over FedGWO. Furthermore, FedBWO dramatically\ndecreases the communication cost compared to other methods."}
{"id": "2505.03981", "pdf": "https://arxiv.org/pdf/2505.03981", "abs": "https://arxiv.org/abs/2505.03981", "authors": ["Qianchu Liu", "Sheng Zhang", "Guanghui Qin", "Timothy Ossowski", "Yu Gu", "Ying Jin", "Sid Kiblawi", "Sam Preston", "Mu Wei", "Paul Vozila", "Tristan Naumann", "Hoifung Poon"], "title": "X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent proprietary models (e.g., o3) have begun to demonstrate strong\nmultimodal reasoning capabilities. Yet, most existing open-source research\nconcentrates on training text-only reasoning models, with evaluations limited\nto mainly mathematical and general-domain tasks. Therefore, it remains unclear\nhow to effectively extend reasoning capabilities beyond text input and general\ndomains. This paper explores a fundamental research question: Is reasoning\ngeneralizable across modalities and domains? Our findings support an\naffirmative answer: General-domain text-based post-training can enable such\nstrong generalizable reasoning. Leveraging this finding, we introduce\nX-Reasoner, a vision-language model post-trained solely on general-domain text\nfor generalizable reasoning, using a two-stage approach: an initial supervised\nfine-tuning phase with distilled long chain-of-thoughts, followed by\nreinforcement learning with verifiable rewards. Experiments show that\nX-Reasoner successfully transfers reasoning capabilities to both multimodal and\nout-of-domain settings, outperforming existing state-of-the-art models trained\nwith in-domain and multimodal data across various general and medical\nbenchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in\nspecialized domains can be further enhanced through continued training on\ndomain-specific text-only data. Building upon this, we introduce\nX-Reasoner-Med, a medical-specialized variant that achieves new state of the\nart on numerous text-only and multimodal medical benchmarks."}
{"id": "2505.04440", "pdf": "https://arxiv.org/pdf/2505.04440", "abs": "https://arxiv.org/abs/2505.04440", "authors": ["Xiaozheng Qu", "Zhaochuan Li", "Zhuang Qi", "Xiang Li", "Haibei Huang", "Lei Meng", "Xiangxu Meng"], "title": "Towards Initialization-Agnostic Clustering with Iterative Adaptive Resonance Theory", "categories": ["cs.LG"], "comment": "2025 International Joint Conference on Neural Networks (IJCNN 2025)", "summary": "The clustering performance of Fuzzy Adaptive Resonance Theory (Fuzzy ART) is\nhighly dependent on the preset vigilance parameter, where deviations in its\nvalue can lead to significant fluctuations in clustering results, severely\nlimiting its practicality for non-expert users. Existing approaches generally\nenhance vigilance parameter robustness through adaptive mechanisms such as\nparticle swarm optimization and fuzzy logic rules. However, they often\nintroduce additional hyperparameters or complex frameworks that contradict the\noriginal simplicity of the algorithm. To address this, we propose Iterative\nRefinement Adaptive Resonance Theory (IR-ART), which integrates three key\nphases into a unified iterative framework: (1) Cluster Stability Detection: A\ndynamic stability detection module that identifies unstable clusters by\nanalyzing the change of sample size (number of samples in the cluster) in\niteration. (2) Unstable Cluster Deletion: An evolutionary pruning module that\neliminates low-quality clusters. (3) Vigilance Region Expansion: A vigilance\nregion expansion mechanism that adaptively adjusts similarity thresholds.\nIndependent of the specific execution of clustering, these three phases\nsequentially focus on analyzing the implicit knowledge within the iterative\nprocess, adjusting weights and vigilance parameters, thereby laying a\nfoundation for the next iteration. Experimental evaluation on 15 datasets\ndemonstrates that IR-ART improves tolerance to suboptimal vigilance parameter\nvalues while preserving the parameter simplicity of Fuzzy ART. Case studies\nvisually confirm the algorithm's self-optimization capability through iterative\nrefinement, making it particularly suitable for non-expert users in\nresource-constrained scenarios."}
{"id": "2505.03983", "pdf": "https://arxiv.org/pdf/2505.03983", "abs": "https://arxiv.org/abs/2505.03983", "authors": ["Hengyuan Hu", "Aniket Das", "Dorsa Sadigh", "Nima Anari"], "title": "Diffusion Models are Secretly Exchangeable: Parallelizing DDPMs via Autospeculation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Denoising Diffusion Probabilistic Models (DDPMs) have emerged as powerful\ntools for generative modeling. However, their sequential computation\nrequirements lead to significant inference-time bottlenecks. In this work, we\nutilize the connection between DDPMs and Stochastic Localization to prove that,\nunder an appropriate reparametrization, the increments of DDPM satisfy an\nexchangeability property. This general insight enables near-black-box\nadaptation of various performance optimization techniques from autoregressive\nmodels to the diffusion setting. To demonstrate this, we introduce\n\\emph{Autospeculative Decoding} (ASD), an extension of the widely used\nspeculative decoding algorithm to DDPMs that does not require any auxiliary\ndraft models. Our theoretical analysis shows that ASD achieves a $\\tilde{O}\n(K^{\\frac{1}{3}})$ parallel runtime speedup over the $K$ step sequential DDPM.\nWe also demonstrate that a practical implementation of autospeculative decoding\naccelerates DDPM inference significantly in various domains."}
{"id": "2505.04441", "pdf": "https://arxiv.org/pdf/2505.04441", "abs": "https://arxiv.org/abs/2505.04441", "authors": ["Mirazul Haque", "Petr Babkin", "Farima Farmahinifarahani", "Manuela Veloso"], "title": "Towards Effectively Leveraging Execution Traces for Program Repair with Code LLMs", "categories": ["cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) show promising performance on various\nprogramming tasks, including Automatic Program Repair (APR). However, most\napproaches to LLM-based APR are limited to the static analysis of the programs,\nwhile disregarding their runtime behavior. Inspired by knowledge-augmented NLP,\nin this work, we aim to remedy this potential blind spot by augmenting standard\nAPR prompts with program execution traces. We evaluate our approach using the\nGPT family of models on three popular APR datasets. Our findings suggest that\nsimply incorporating execution traces into the prompt provides a limited\nperformance improvement over trace-free baselines, in only 2 out of 6 tested\ndataset / model configurations. We further find that the effectiveness of\nexecution traces for APR diminishes as their complexity increases. We explore\nseveral strategies for leveraging traces in prompts and demonstrate that\nLLM-optimized prompts help outperform trace-free prompts more consistently.\nAdditionally, we show trace-based prompting to be superior to finetuning a\nsmaller LLM on a small-scale dataset; and conduct probing studies reinforcing\nthe notion that execution traces can complement the reasoning abilities of the\nLLMs."}
{"id": "2505.03988", "pdf": "https://arxiv.org/pdf/2505.03988", "abs": "https://arxiv.org/abs/2505.03988", "authors": ["Gregory Bolet", "Giorgis Georgakoudis", "Harshitha Menon", "Konstantinos Parasyris", "Niranjan Hasabnis", "Hayden Estes", "Kirk W. Cameron", "Gal Oren"], "title": "Can Large Language Models Predict Parallel Code Performance?", "categories": ["cs.DC", "cs.AI", "cs.PF"], "comment": "5 pages, 4 figures, accepted to AI4Sys Workshop at HPDC 2025", "summary": "Accurate determination of the performance of parallel GPU code typically\nrequires execution-time profiling on target hardware -- an increasingly\nprohibitive step due to limited access to high-end GPUs. This paper explores\nwhether Large Language Models (LLMs) can offer an alternative approach for GPU\nperformance prediction without relying on hardware. We frame the problem as a\nroofline classification task: given the source code of a GPU kernel and the\nhardware specifications of a target GPU, can an LLM predict whether the GPU\nkernel is compute-bound or bandwidth-bound?\n  For this study, we build a balanced dataset of 340 GPU kernels, obtained from\nHeCBench benchmark and written in CUDA and OpenMP, along with their\nground-truth labels obtained via empirical GPU profiling. We evaluate LLMs\nacross four scenarios: (1) with access to profiling data of the kernel source,\n(2) zero-shot with source code only, (3) few-shot with code and label pairs,\nand (4) fine-tuned on a small custom dataset.\n  Our results show that state-of-the-art LLMs have a strong understanding of\nthe Roofline model, achieving 100% classification accuracy when provided with\nexplicit profiling data. We also find that reasoning-capable LLMs significantly\noutperform standard LLMs in zero- and few-shot settings, achieving up to 64%\naccuracy on GPU source codes, without profiling information. Lastly, we find\nthat LLM fine-tuning will require much more data than what we currently have\navailable.\n  This work is among the first to use LLMs for source-level roofline\nperformance prediction via classification, and illustrates their potential to\nguide optimization efforts when runtime profiling is infeasible. Our findings\nsuggest that with better datasets and prompt strategies, LLMs could become\npractical tools for HPC performance analysis and performance portability."}
{"id": "2505.04461", "pdf": "https://arxiv.org/pdf/2505.04461", "abs": "https://arxiv.org/abs/2505.04461", "authors": ["Pengfei Jiao", "Hongjiang Chen", "Xuan Guo", "Zhidong Zhao", "Dongxiao He", "Di Jin"], "title": "A Survey on Temporal Interaction Graph Representation Learning: Progress, Challenges, and Opportunities", "categories": ["cs.LG", "cs.AI", "cs.SI"], "comment": "IJCAI 2025 Survey Track", "summary": "Temporal interaction graphs (TIGs), defined by sequences of timestamped\ninteraction events, have become ubiquitous in real-world applications due to\ntheir capability to model complex dynamic system behaviors. As a result,\ntemporal interaction graph representation learning (TIGRL) has garnered\nsignificant attention in recent years. TIGRL aims to embed nodes in TIGs into\nlow-dimensional representations that effectively preserve both structural and\ntemporal information, thereby enhancing the performance of downstream tasks\nsuch as classification, prediction, and clustering within constantly evolving\ndata environments. In this paper, we begin by introducing the foundational\nconcepts of TIGs and emphasize the critical role of temporal dependencies. We\nthen propose a comprehensive taxonomy of state-of-the-art TIGRL methods,\nsystematically categorizing them based on the types of information utilized\nduring the learning process to address the unique challenges inherent to TIGs.\nTo facilitate further research and practical applications, we curate the source\nof datasets and benchmarks, providing valuable resources for empirical\ninvestigations. Finally, we examine key open challenges and explore promising\nresearch directions in TIGRL, laying the groundwork for future advancements\nthat have the potential to shape the evolution of this field."}
{"id": "2505.04002", "pdf": "https://arxiv.org/pdf/2505.04002", "abs": "https://arxiv.org/abs/2505.04002", "authors": ["Michael Xu", "Yi Shi", "KangKang Yin", "Xue Bin Peng"], "title": "PARC: Physics-based Augmentation with Reinforcement Learning for Character Controllers", "categories": ["cs.GR", "cs.AI", "cs.LG", "cs.RO"], "comment": "SIGGRAPH Conference Papers 2025", "summary": "Humans excel in navigating diverse, complex environments with agile motor\nskills, exemplified by parkour practitioners performing dynamic maneuvers, such\nas climbing up walls and jumping across gaps. Reproducing these agile movements\nwith simulated characters remains challenging, in part due to the scarcity of\nmotion capture data for agile terrain traversal behaviors and the high cost of\nacquiring such data. In this work, we introduce PARC (Physics-based\nAugmentation with Reinforcement Learning for Character Controllers), a\nframework that leverages machine learning and physics-based simulation to\niteratively augment motion datasets and expand the capabilities of terrain\ntraversal controllers. PARC begins by training a motion generator on a small\ndataset consisting of core terrain traversal skills. The motion generator is\nthen used to produce synthetic data for traversing new terrains. However, these\ngenerated motions often exhibit artifacts, such as incorrect contacts or\ndiscontinuities. To correct these artifacts, we train a physics-based tracking\ncontroller to imitate the motions in simulation. The corrected motions are then\nadded to the dataset, which is used to continue training the motion generator\nin the next iteration. PARC's iterative process jointly expands the\ncapabilities of the motion generator and tracker, creating agile and versatile\nmodels for interacting with complex environments. PARC provides an effective\napproach to develop controllers for agile terrain traversal, which bridges the\ngap between the scarcity of motion data and the need for versatile character\ncontrollers."}
{"id": "2505.04464", "pdf": "https://arxiv.org/pdf/2505.04464", "abs": "https://arxiv.org/abs/2505.04464", "authors": ["Louis Ohl", "Fredrik Lindsten"], "title": "Discriminative Ordering Through Ensemble Consensus", "categories": ["cs.LG", "cs.AI", "62H30", "G.3"], "comment": "Accepted at UAI 2025", "summary": "Evaluating the performance of clustering models is a challenging task where\nthe outcome depends on the definition of what constitutes a cluster. Due to\nthis design, current existing metrics rarely handle multiple clustering models\nwith diverse cluster definitions, nor do they comply with the integration of\nconstraints when available. In this work, we take inspiration from consensus\nclustering and assume that a set of clustering models is able to uncover hidden\nstructures in the data. We propose to construct a discriminative ordering\nthrough ensemble clustering based on the distance between the connectivity of a\nclustering model and the consensus matrix. We first validate the proposed\nmethod with synthetic scenarios, highlighting that the proposed score ranks the\nmodels that best match the consensus first. We then show that this simple\nranking score significantly outperforms other scoring methods when comparing\nsets of different clustering algorithms that are not restricted to a fixed\nnumber of clusters and is compatible with clustering constraints."}
{"id": "2505.04015", "pdf": "https://arxiv.org/pdf/2505.04015", "abs": "https://arxiv.org/abs/2505.04015", "authors": ["Soheil Zibakhsh Shabgahi", "Yaman Jandali", "Farinaz Koushanfar"], "title": "MergeGuard: Efficient Thwarting of Trojan Attacks in Machine Learning Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "This paper proposes MergeGuard, a novel methodology for mitigation of AI\nTrojan attacks. Trojan attacks on AI models cause inputs embedded with triggers\nto be misclassified to an adversary's target class, posing a significant threat\nto model usability trained by an untrusted third party. The core of MergeGuard\nis a new post-training methodology for linearizing and merging fully connected\nlayers which we show simultaneously improves model generalizability and\nperformance. Our Proof of Concept evaluation on Transformer models demonstrates\nthat MergeGuard maintains model accuracy while decreasing trojan attack success\nrate, outperforming commonly used (post-training) Trojan mitigation by\nfine-tuning methodologies."}
{"id": "2505.04468", "pdf": "https://arxiv.org/pdf/2505.04468", "abs": "https://arxiv.org/abs/2505.04468", "authors": ["Hyeju Shin", "Kyudan Jung", "Seongwon Yun", "Juyoung Yun"], "title": "Spectral and Temporal Denoising for Differentially Private Optimization", "categories": ["cs.LG", "cs.AI", "cs.IT", "cs.NE", "math.IT"], "comment": null, "summary": "This paper introduces the FFT-Enhanced Kalman Filter (FFTKF), a\ndifferentially private optimization method that addresses the challenge of\npreserving performance in DP-SGD, where added noise typically degrades model\nutility. FFTKF integrates frequency-domain noise shaping with Kalman filtering\nto enhance gradient quality while preserving $(\\varepsilon, \\delta)$-DP\nguarantees. It employs a high-frequency shaping mask in the Fourier domain to\nconcentrate differential privacy noise in less informative spectral components,\npreserving low-frequency gradient signals. A scalar-gain Kalman filter with\nfinite-difference Hessian approximation further refines the denoised gradients.\nWith a per-iteration complexity of $\\mathcal{O}(d \\log d)$, FFTKF demonstrates\nimproved test accuracy over DP-SGD and DiSK across MNIST, CIFAR-10, CIFAR-100,\nand Tiny-ImageNet datasets using CNNs, Wide ResNets, and Vision Transformers.\nTheoretical analysis confirms that FFTKF maintains equivalent privacy\nguarantees while achieving a tighter privacy-utility trade-off through reduced\nnoise and controlled bias."}
{"id": "2505.04016", "pdf": "https://arxiv.org/pdf/2505.04016", "abs": "https://arxiv.org/abs/2505.04016", "authors": ["Darren Yow-Bang Wang", "Zhengyuan Shen", "Soumya Smruti Mishra", "Zhichao Xu", "Yifei Teng", "Haibo Ding"], "title": "SLOT: Structuring the Output of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Structured outputs are essential for large language models (LLMs) in critical\napplications like agents and information extraction. Despite their\ncapabilities, LLMs often generate outputs that deviate from predefined schemas,\nsignificantly hampering reliable application development. We present SLOT\n(Structured LLM Output Transformer), a model-agnostic approach that transforms\nunstructured LLM outputs into precise structured formats. While existing\nsolutions predominantly rely on constrained decoding techniques or are tightly\ncoupled with specific models, SLOT employs a fine-tuned lightweight language\nmodel as a post-processing layer, achieving flexibility across various LLMs and\nschema specifications. We introduce a systematic pipeline for data curation and\nsynthesis alongside a formal evaluation methodology that quantifies both schema\naccuracy and content fidelity. Our results demonstrate that fine-tuned\nMistral-7B model with constrained decoding achieves near perfect schema\naccuracy (99.5%) and content similarity (94.0%), outperforming\nClaude-3.5-Sonnet by substantial margins (+25 and +20 percentage points,\nrespectively). Notably, even compact models like Llama-3.2-1B can match or\nexceed the structured output capabilities of much larger proprietary models\nwhen equipped with SLOT, enabling reliable structured generation in\nresource-constrained environments."}
{"id": "2505.04471", "pdf": "https://arxiv.org/pdf/2505.04471", "abs": "https://arxiv.org/abs/2505.04471", "authors": ["Vincent Souveton", "Sébastien Terrana"], "title": "Hamiltonian Normalizing Flows as kinetic PDE solvers: application to the 1D Vlasov-Poisson Equations", "categories": ["cs.LG"], "comment": null, "summary": "Many conservative physical systems can be described using the Hamiltonian\nformalism. A notable example is the Vlasov-Poisson equations, a set of partial\ndifferential equations that govern the time evolution of a phase-space density\nfunction representing collisionless particles under a self-consistent\npotential. These equations play a central role in both plasma physics and\ncosmology. Due to the complexity of the potential involved, analytical\nsolutions are rarely available, necessitating the use of numerical methods such\nas Particle-In-Cell. In this work, we introduce a novel approach based on\nHamiltonian-informed Normalizing Flows, specifically a variant of Fixed-Kinetic\nNeural Hamiltonian Flows. Our method transforms an initial Gaussian\ndistribution in phase space into the final distribution using a sequence of\ninvertible, volume-preserving transformations derived from Hamiltonian\ndynamics. The model is trained on a dataset comprising initial and final states\nat a fixed time T, generated via numerical simulations. After training, the\nmodel enables fast sampling of the final distribution from any given initial\nstate. Moreover, by automatically learning an interpretable physical potential,\nit can generalize to intermediate states not seen during training, offering\ninsights into the system's evolution across time."}
{"id": "2505.04021", "pdf": "https://arxiv.org/pdf/2505.04021", "abs": "https://arxiv.org/abs/2505.04021", "authors": ["Shan Yu", "Jiarong Xing", "Yifan Qiao", "Mingyuan Ma", "Yangmin Li", "Yang Wang", "Shuo Yang", "Zhiqiang Xie", "Shiyi Cao", "Ke Bao", "Ion Stoica", "Harry Xu", "Ying Sheng"], "title": "Prism: Unleashing GPU Sharing for Cost-Efficient Multi-LLM Serving", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PF"], "comment": null, "summary": "Serving large language models (LLMs) is expensive, especially for providers\nhosting many models, making cost reduction essential. The unique workload\npatterns of serving multiple LLMs (i.e., multi-LLM serving) create new\nopportunities and challenges for this task. The long-tail popularity of models\nand their long idle periods present opportunities to improve utilization\nthrough GPU sharing. However, existing GPU sharing systems lack the ability to\nadjust their resource allocation and sharing policies at runtime, making them\nineffective at meeting latency service-level objectives (SLOs) under rapidly\nfluctuating workloads.\n  This paper presents Prism, a multi-LLM serving system that unleashes the full\npotential of GPU sharing to achieve both cost efficiency and SLO attainment. At\nits core, Prism tackles a key limitation of existing\nsystems$\\unicode{x2014}$the lack of $\\textit{cross-model memory coordination}$,\nwhich is essential for flexibly sharing GPU memory across models under dynamic\nworkloads. Prism achieves this with two key designs. First, it supports\non-demand memory allocation by dynamically mapping physical to virtual memory\npages, allowing flexible memory redistribution among models that space- and\ntime-share a GPU. Second, it improves memory efficiency through a two-level\nscheduling policy that dynamically adjusts sharing strategies based on models'\nruntime demands. Evaluations on real-world traces show that Prism achieves more\nthan $2\\times$ cost savings and $3.3\\times$ SLO attainment compared to\nstate-of-the-art systems."}
{"id": "2505.04535", "pdf": "https://arxiv.org/pdf/2505.04535", "abs": "https://arxiv.org/abs/2505.04535", "authors": ["Michail Theologitis", "Vasilis Samoladas", "Antonios Deligiannakis"], "title": "Communication-Efficient Federated Fine-Tuning of Language Models via Dynamic Update Schedules", "categories": ["cs.LG"], "comment": null, "summary": "Federated learning (FL) makes it possible to train models on data that would\notherwise remain untapped and inaccessible. Simultaneously, pre-trained\nlanguage models (LMs) have emerged as indispensable tools in modern workflows.\nThese models exhibit extraordinary capabilities and are easily adapted to\ndownstream tasks. This opens one of the most exciting frontiers in FL:\nfine-tuning LMs. However, a persistent challenge in FL is the frequent, rigid\ncommunication of parameters, a problem which is magnified by the sheer size of\nthese modern models. Currently, the FedOpt family of algorithms is the\nprevailing approach in FL, though it relies on fixed, heuristic intervals for\nmodel synchronization. Recently, the FDA algorithm introduced a dynamic\nalternative by monitoring training progress, but it came with its own\ndrawbacks; namely, a hard-to-tune threshold parameter and a rigid\nsynchronization scheme. In this work, we introduce the FDA-Opt family of\nalgorithms -- a unified generalization that extends the principles behind both\nFDA and FedOpt, while resolving their core limitations. We evaluate our\napproach on fine-tuning LMs across a range of downstream NLP tasks, and\ndemonstrate that it consistently outperforms FedOpt -- even when FDA-Opt\noperates under hyper-parameter settings originally optimized for its\ncompetitors. In other words, we show that FDA-Opt is a practical, drop-in\nreplacement for FedOpt in modern FL libraries and systems: it requires no\nadditional configuration and delivers superior performance out of the box."}
{"id": "2505.04034", "pdf": "https://arxiv.org/pdf/2505.04034", "abs": "https://arxiv.org/abs/2505.04034", "authors": ["Ayana Moshruba", "Hamed Poursiami", "Maryam Parsa"], "title": "Izhikevich-Inspired Temporal Dynamics for Enhancing Privacy, Efficiency, and Transferability in Spiking Neural Networks", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": null, "summary": "Biological neurons exhibit diverse temporal spike patterns, which are\nbelieved to support efficient, robust, and adaptive neural information\nprocessing. While models such as Izhikevich can replicate a wide range of these\nfiring dynamics, their complexity poses challenges for directly integrating\nthem into scalable spiking neural networks (SNN) training pipelines. In this\nwork, we propose two probabilistically driven, input-level temporal spike\ntransformations: Poisson-Burst and Delayed-Burst that introduce biologically\ninspired temporal variability directly into standard Leaky Integrate-and-Fire\n(LIF) neurons. This enables scalable training and systematic evaluation of how\nspike timing dynamics affect privacy, generalization, and learning performance.\nPoisson-Burst modulates burst occurrence based on input intensity, while\nDelayed-Burst encodes input strength through burst onset timing. Through\nextensive experiments across multiple benchmarks, we demonstrate that\nPoisson-Burst maintains competitive accuracy and lower resource overhead while\nexhibiting enhanced privacy robustness against membership inference attacks,\nwhereas Delayed-Burst provides stronger privacy protection at a modest accuracy\ntrade-off. These findings highlight the potential of biologically grounded\ntemporal spike dynamics in improving the privacy, generalization and biological\nplausibility of neuromorphic learning systems."}
{"id": "2505.04558", "pdf": "https://arxiv.org/pdf/2505.04558", "abs": "https://arxiv.org/abs/2505.04558", "authors": ["Wenzhao Liu", "Haoran Li", "Congying Han", "Zicheng Zhang", "Anqi Li", "Tiande Guo"], "title": "Purity Law for Generalizable Neural TSP Solvers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Achieving generalization in neural approaches across different scales and\ndistributions remains a significant challenge for the Traveling Salesman\nProblem~(TSP). A key obstacle is that neural networks often fail to learn\nrobust principles for identifying universal patterns and deriving optimal\nsolutions from diverse instances. In this paper, we first uncover Purity Law\n(PuLa), a fundamental structural principle for optimal TSP solutions, defining\nthat edge prevalence grows exponentially with the sparsity of surrounding\nvertices. Statistically validated across diverse instances, PuLa reveals a\nconsistent bias toward local sparsity in global optima. Building on this\ninsight, we propose Purity Policy Optimization~(PUPO), a novel training\nparadigm that explicitly aligns characteristics of neural solutions with PuLa\nduring the solution construction process to enhance generalization. Extensive\nexperiments demonstrate that PUPO can be seamlessly integrated with popular\nneural solvers, significantly enhancing their generalization performance\nwithout incurring additional computational overhead during inference."}
{"id": "2505.04072", "pdf": "https://arxiv.org/pdf/2505.04072", "abs": "https://arxiv.org/abs/2505.04072", "authors": ["Xu Huang", "Yuefeng Huang", "Weiwen Liu", "Xingshan Zeng", "Yasheng Wang", "Ruiming Tang", "Hong Xie", "Defu Lian"], "title": "Advancing and Benchmarking Personalized Tool Invocation for LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 7 figures, 5 tables", "summary": "Tool invocation is a crucial mechanism for extending the capabilities of\nLarge Language Models (LLMs) and has recently garnered significant attention.\nIt enables LLMs to solve complex problems through tool calls while accessing\nup-to-date world knowledge. However, existing work primarily focuses on the\nfundamental ability of LLMs to invoke tools for problem-solving, without\nconsidering personalized constraints in tool invocation. In this work, we\nintroduce the concept of Personalized Tool Invocation and define two key tasks:\nTool Preference and Profile-dependent Query. Tool Preference addresses user\npreferences when selecting among functionally similar tools, while\nProfile-dependent Query considers cases where a user query lacks certain tool\nparameters, requiring the model to infer them from the user profile. To tackle\nthese challenges, we propose PTool, a data synthesis framework designed for\npersonalized tool invocation. Additionally, we construct \\textbf{PTBench}, the\nfirst benchmark for evaluating personalized tool invocation. We then fine-tune\nvarious open-source models, demonstrating the effectiveness of our framework\nand providing valuable insights. Our benchmark is public at\nhttps://github.com/hyfshadow/PTBench."}
{"id": "2505.04560", "pdf": "https://arxiv.org/pdf/2505.04560", "abs": "https://arxiv.org/abs/2505.04560", "authors": ["Guanghui Wang", "Zhiyong Yang", "Zitai Wang", "Shi Wang", "Qianqian Xu", "Qingming Huang"], "title": "ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge Distillation via $α$-$β$-Divergence", "categories": ["cs.LG"], "comment": "ICML 2025 Spotlight", "summary": "Knowledge Distillation (KD) transfers knowledge from a large teacher model to\na smaller student model by minimizing the divergence between their output\ndistributions, typically using forward Kullback-Leibler divergence (FKLD) or\nreverse KLD (RKLD). It has become an effective training paradigm due to the\nbroader supervision information provided by the teacher distribution compared\nto one-hot labels. We identify that the core challenge in KD lies in balancing\ntwo mode-concentration effects: the \\textbf{\\textit{Hardness-Concentration}}\neffect, which refers to focusing on modes with large errors, and the\n\\textbf{\\textit{Confidence-Concentration}} effect, which refers to focusing on\nmodes with high student confidence. Through an analysis of how probabilities\nare reassigned during gradient updates, we observe that these two effects are\nentangled in FKLD and RKLD, but in extreme forms. Specifically, both are too\nweak in FKLD, causing the student to fail to concentrate on the target class.\nIn contrast, both are too strong in RKLD, causing the student to overly\nemphasize the target class while ignoring the broader distributional\ninformation from the teacher. To address this imbalance, we propose ABKD, a\ngeneric framework with $\\alpha$-$\\beta$-divergence. Our theoretical results\nshow that ABKD offers a smooth interpolation between FKLD and RKLD, achieving\nan effective trade-off between these effects. Extensive experiments on 17\nlanguage/vision datasets with 12 teacher-student settings confirm its efficacy.\nThe code is available at https://github.com/ghwang-s/abkd."}
{"id": "2505.04075", "pdf": "https://arxiv.org/pdf/2505.04075", "abs": "https://arxiv.org/abs/2505.04075", "authors": ["Teddy Foley", "Spencer Guo", "Henry Josephson", "Anqi Qu", "Jack Sanderson"], "title": "LLM-e Guess: Can LLMs Capabilities Advance Without Hardware Progress?", "categories": ["cs.LG", "cs.AI", "I.2"], "comment": null, "summary": "This paper examines whether large language model (LLM) capabilities can\ncontinue to advance without additional compute by analyzing the development and\nrole of algorithms used in state-of-the-art LLMs. Motivated by regulatory\nefforts that have largely focused on restricting access to high-performance\nhardware, we ask: Can LLMs progress in a compute-constrained environment, and\nhow do algorithmic innovations perform under such conditions?\n  To address these questions, we introduce a novel classification framework\nthat distinguishes between compute-dependent innovations -- which yield\ndisproportionate benefits at high compute levels (e.g., the Transformer\narchitecture and mixture-of-experts models) and compute-independent\ninnovations, which improve efficiency across all compute scales (e.g., rotary\npositional encoding, FlashAttention, or layer normalization). We quantify these\ncontributions using a metric called compute-equivalent gain (CEG), which\nestimates the additional compute that would be required to achieve similar\nimprovements without these algorithmic advancements.\n  To validate this framework, we conduct small-scale training experiments with\na scaled-down GPT-2 model. Our results confirm that compute-independent\nadvancements yield meaningful performance gains even in resource-constrained\nsettings, with a CEG of up to $3.5\\times$ over a baseline model. By contrast,\ncompute-dependent advancements provided little benefit or even degraded\nperformance at the small scale, reinforcing the importance of compute\navailability for certain algorithmic gains."}
{"id": "2505.04566", "pdf": "https://arxiv.org/pdf/2505.04566", "abs": "https://arxiv.org/abs/2505.04566", "authors": ["Lucas R. C. Farias", "Talita P. Silva", "Pedro H. M. Araujo"], "title": "Multitask LSTM for Arboviral Outbreak Prediction Using Public Health Data", "categories": ["cs.LG"], "comment": "6 pages, 4 figures", "summary": "This paper presents a multitask learning approach based on long-short-term\nmemory (LSTM) networks for the joint prediction of arboviral outbreaks and case\ncounts of dengue, chikungunya, and Zika in Recife, Brazil. Leveraging\nhistorical public health data from DataSUS (2017-2023), the proposed model\nconcurrently performs binary classification (outbreak detection) and regression\n(case forecasting) tasks. A sliding window strategy was adopted to construct\ntemporal features using varying input lengths (60, 90, and 120 days), with\nhyperparameter optimization carried out using Keras Tuner. Model evaluation\nused time series cross-validation for robustness and a held-out test from 2023\nfor generalization assessment. The results show that longer windows improve\ndengue regression accuracy, while classification performance peaked at\nintermediate windows, suggesting an optimal trade-off between sequence length\nand generalization. The multitask architecture delivers competitive performance\nacross diseases and tasks, demonstrating the feasibility and advantages of\nunified modeling strategies for scalable epidemic forecasting in data-limited\npublic health scenarios."}
{"id": "2505.04083", "pdf": "https://arxiv.org/pdf/2505.04083", "abs": "https://arxiv.org/abs/2505.04083", "authors": ["Aditya K. Ranjan", "Siddharth Singh", "Cunyang Wei", "Abhinav Bhatele"], "title": "Plexus: Taming Billion-edge Graphs with 3D Parallel GNN Training", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Graph neural networks have emerged as a potent class of neural networks\ncapable of leveraging the connectivity and structure of real-world graphs to\nlearn intricate properties and relationships between nodes. Many real-world\ngraphs exceed the memory capacity of a GPU due to their sheer size, and using\nGNNs on them requires techniques such as mini-batch sampling to scale. However,\nthis can lead to reduced accuracy in some cases, and sampling and data transfer\nfrom the CPU to the GPU can also slow down training. On the other hand,\ndistributed full-graph training suffers from high communication overhead and\nload imbalance due to the irregular structure of graphs. We propose Plexus, a\nthree-dimensional (3D) parallel approach for full-graph training that tackles\nthese issues and scales to billion-edge graphs. Additionally, we introduce\noptimizations such as a permutation scheme for load balancing, and a\nperformance model to predict the optimal 3D configuration. We evaluate Plexus\non several graph datasets and show scaling results for up to 2048 GPUs on\nPerlmutter, which is 33% of the machine, and 2048 GCDs on Frontier. Plexus\nachieves unprecedented speedups of 2.3x-12.5x over existing methods and a\nreduction in the time to solution by 5.2-8.7x on Perlmutter and 7-54.2x on\nFrontier."}
{"id": "2505.04578", "pdf": "https://arxiv.org/pdf/2505.04578", "abs": "https://arxiv.org/abs/2505.04578", "authors": ["Wenjun Cao"], "title": "Fight Fire with Fire: Defending Against Malicious RL Fine-Tuning via Reward Neutralization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) fine-tuning transforms large language models\nwhile creating a vulnerability we experimentally verify: Our experiment shows\nthat malicious RL fine-tuning dismantles safety guardrails with remarkable\nefficiency, requiring only 50 steps and minimal adversarial prompts, with\nharmful escalating from 0-2 to 7-9. This attack vector particularly threatens\nopen-source models with parameter-level access. Existing defenses targeting\nsupervised fine-tuning prove ineffective against RL's dynamic feedback\nmechanisms. We introduce Reward Neutralization, the first defense framework\nspecifically designed against RL fine-tuning attacks, establishing concise\nrejection patterns that render malicious reward signals ineffective. Our\napproach trains models to produce minimal-information rejections that attackers\ncannot exploit, systematically neutralizing attempts to optimize toward harmful\noutputs. Experiments validate that our approach maintains low harmful scores\n(no greater than 2) after 200 attack steps, while standard models rapidly\ndeteriorate. This work provides the first constructive proof that robust\ndefense against increasingly accessible RL attacks is achievable, addressing a\ncritical security gap for open-weight models."}
{"id": "2505.04084", "pdf": "https://arxiv.org/pdf/2505.04084", "abs": "https://arxiv.org/abs/2505.04084", "authors": ["Xiang Chen", "Jibin Wang", "Chaoyang Gao", "Xiaolin Ju", "Zhanqi Cui"], "title": "An Empirical Study of OpenAI API Discussions on Stack Overflow", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "The rapid advancement of large language models (LLMs), represented by\nOpenAI's GPT series, has significantly impacted various domains such as natural\nlanguage processing, software development, education, healthcare, finance, and\nscientific research. However, OpenAI APIs introduce unique challenges that\ndiffer from traditional APIs, such as the complexities of prompt engineering,\ntoken-based cost management, non-deterministic outputs, and operation as black\nboxes. To the best of our knowledge, the challenges developers encounter when\nusing OpenAI APIs have not been explored in previous empirical studies. To fill\nthis gap, we conduct the first comprehensive empirical study by analyzing 2,874\nOpenAI API-related discussions from the popular Q&A forum Stack Overflow. We\nfirst examine the popularity and difficulty of these posts. After manually\ncategorizing them into nine OpenAI API-related categories, we identify specific\nchallenges associated with each category through topic modeling analysis. Based\non our empirical findings, we finally propose actionable implications for\ndevelopers, LLM vendors, and researchers."}
{"id": "2505.04599", "pdf": "https://arxiv.org/pdf/2505.04599", "abs": "https://arxiv.org/abs/2505.04599", "authors": ["Michael Crawshaw", "Mingrui Liu"], "title": "Complexity Lower Bounds of Adaptive Gradient Algorithms for Non-convex Stochastic Optimization under Relaxed Smoothness", "categories": ["cs.LG"], "comment": "ICLR 2025", "summary": "Recent results in non-convex stochastic optimization demonstrate the\nconvergence of popular adaptive algorithms (e.g., AdaGrad) under the $(L_0,\nL_1)$-smoothness condition, but the rate of convergence is a higher-order\npolynomial in terms of problem parameters like the smoothness constants. The\ncomplexity guaranteed by such algorithms to find an $\\epsilon$-stationary point\nmay be significantly larger than the optimal complexity of $\\Theta \\left(\n\\Delta L \\sigma^2 \\epsilon^{-4} \\right)$ achieved by SGD in the $L$-smooth\nsetting, where $\\Delta$ is the initial optimality gap, $\\sigma^2$ is the\nvariance of stochastic gradient. However, it is currently not known whether\nthese higher-order dependencies can be tightened. To answer this question, we\ninvestigate complexity lower bounds for several adaptive optimization\nalgorithms in the $(L_0, L_1)$-smooth setting, with a focus on the dependence\nin terms of problem parameters $\\Delta, L_0, L_1$. We provide complexity bounds\nfor three variations of AdaGrad, which show at least a quadratic dependence on\nproblem parameters $\\Delta, L_0, L_1$. Notably, we show that the decorrelated\nvariant of AdaGrad-Norm requires at least $\\Omega \\left( \\Delta^2 L_1^2\n\\sigma^2 \\epsilon^{-4} \\right)$ stochastic gradient queries to find an\n$\\epsilon$-stationary point. We also provide a lower bound for SGD with a broad\nclass of adaptive stepsizes. Our results show that, for certain adaptive\nalgorithms, the $(L_0, L_1)$-smooth setting is fundamentally more difficult\nthan the standard smooth setting, in terms of the initial optimality gap and\nthe smoothness constants."}
{"id": "2505.04101", "pdf": "https://arxiv.org/pdf/2505.04101", "abs": "https://arxiv.org/abs/2505.04101", "authors": ["AbdulAziz AbdulGhaffar", "Ashraf Matrawy"], "title": "LLMs' Suitability for Network Security: A Case Study of STRIDE Threat Modeling", "categories": ["cs.CR", "cs.AI", "cs.NI"], "comment": null, "summary": "Artificial Intelligence (AI) is expected to be an integral part of\nnext-generation AI-native 6G networks. With the prevalence of AI, researchers\nhave identified numerous use cases of AI in network security. However, there\nare almost nonexistent studies that analyze the suitability of Large Language\nModels (LLMs) in network security. To fill this gap, we examine the suitability\nof LLMs in network security, particularly with the case study of STRIDE threat\nmodeling. We utilize four prompting techniques with five LLMs to perform STRIDE\nclassification of 5G threats. From our evaluation results, we point out key\nfindings and detailed insights along with the explanation of the possible\nunderlying factors influencing the behavior of LLMs in the modeling of certain\nthreats. The numerical results and the insights support the necessity for\nadjusting and fine-tuning LLMs for network security use cases."}
{"id": "2505.04604", "pdf": "https://arxiv.org/pdf/2505.04604", "abs": "https://arxiv.org/abs/2505.04604", "authors": ["Lorenzo Beretta", "Nathaniel Harms", "Caleb Koch"], "title": "Testing Juntas Optimally with Samples", "categories": ["cs.LG", "cs.CC", "cs.DS"], "comment": null, "summary": "We prove tight upper and lower bounds of\n$\\Theta\\left(\\tfrac{1}{\\epsilon}\\left( \\sqrt{2^k \\log\\binom{n}{k} } +\n\\log\\binom{n}{k} \\right)\\right)$ on the number of samples required for\ndistribution-free $k$-junta testing. This is the first tight bound for testing\na natural class of Boolean functions in the distribution-free sample-based\nmodel. Our bounds also hold for the feature selection problem, showing that a\njunta tester must learn the set of relevant variables. For tolerant junta\ntesting, we prove a sample lower bound of $\\Omega(2^{(1-o(1)) k} +\n\\log\\binom{n}{k})$ showing that, unlike standard testing, there is no large gap\nbetween tolerant testing and learning."}
{"id": "2505.04132", "pdf": "https://arxiv.org/pdf/2505.04132", "abs": "https://arxiv.org/abs/2505.04132", "authors": ["Mingruo Yuan", "Ben Kao", "Tien-Hsuan Wu", "Michael M. K. Cheung", "Henry W. H. Chan", "Anne S. Y. Cheung", "Felix W. H. Chan", "Yongxi Chen"], "title": "Bringing legal knowledge to the public by constructing a legal question bank using large-scale pre-trained language model", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Access to legal information is fundamental to access to justice. Yet\naccessibility refers not only to making legal documents available to the\npublic, but also rendering legal information comprehensible to them. A vexing\nproblem in bringing legal information to the public is how to turn formal legal\ndocuments such as legislation and judgments, which are often highly technical,\nto easily navigable and comprehensible knowledge to those without legal\neducation. In this study, we formulate a three-step approach for bringing legal\nknowledge to laypersons, tackling the issues of navigability and\ncomprehensibility. First, we translate selected sections of the law into\nsnippets (called CLIC-pages), each being a small piece of article that focuses\non explaining certain technical legal concept in layperson's terms. Second, we\nconstruct a Legal Question Bank (LQB), which is a collection of legal questions\nwhose answers can be found in the CLIC-pages. Third, we design an interactive\nCLIC Recommender (CRec). Given a user's verbal description of a legal situation\nthat requires a legal solution, CRec interprets the user's input and shortlists\nquestions from the question bank that are most likely relevant to the given\nlegal situation and recommends their corresponding CLIC pages where relevant\nlegal knowledge can be found. In this paper we focus on the technical aspects\nof creating an LQB. We show how large-scale pre-trained language models, such\nas GPT-3, can be used to generate legal questions. We compare machine-generated\nquestions (MGQs) against human-composed questions (HCQs) and find that MGQs are\nmore scalable, cost-effective, and more diversified, while HCQs are more\nprecise. We also show a prototype of CRec and illustrate through an example how\nour 3-step approach effectively brings relevant legal knowledge to the public."}
{"id": "2505.04608", "pdf": "https://arxiv.org/pdf/2505.04608", "abs": "https://arxiv.org/abs/2505.04608", "authors": ["Drew Prinster", "Xing Han", "Anqi Liu", "Suchi Saria"], "title": "WATCH: Weighted Adaptive Testing for Changepoint Hypotheses via Weighted-Conformal Martingales", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "To be published in The International Conference on Machine Learning\n  (ICML), 2025", "summary": "Responsibly deploying artificial intelligence (AI) / machine learning (ML)\nsystems in high-stakes settings arguably requires not only proof of system\nreliability, but moreover continual, post-deployment monitoring to quickly\ndetect and address any unsafe behavior. Statistical methods for nonparametric\nchange-point detection -- especially the tools of conformal test martingales\n(CTMs) and anytime-valid inference -- offer promising approaches to this\nmonitoring task. However, existing methods are restricted to monitoring limited\nhypothesis classes or ``alarm criteria,'' such as data shifts that violate\ncertain exchangeability assumptions, or do not allow for online adaptation in\nresponse to shifts. In this paper, we expand the scope of these monitoring\nmethods by proposing a weighted generalization of conformal test martingales\n(WCTMs), which lay a theoretical foundation for online monitoring for any\nunexpected changepoints in the data distribution while controlling\nfalse-alarms. For practical applications, we propose specific WCTM algorithms\nthat accommodate online adaptation to mild covariate shifts (in the marginal\ninput distribution) while raising alarms in response to more severe shifts,\nsuch as concept shifts (in the conditional label distribution) or extreme\n(out-of-support) covariate shifts that cannot be easily adapted to. On\nreal-world datasets, we demonstrate improved performance relative to\nstate-of-the-art baselines."}
{"id": "2505.04146", "pdf": "https://arxiv.org/pdf/2505.04146", "abs": "https://arxiv.org/abs/2505.04146", "authors": ["Variath Madhupal Gautham Nair", "Vishal Varma Dantuluri"], "title": "Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Existing large language models (LLMs) are advancing rapidly and produce\noutstanding results in image generation tasks, yet their content safety checks\nremain vulnerable to prompt-based jailbreaks. Through preliminary testing on\nplatforms such as ChatGPT, MetaAI, and Grok, we observed that even short,\nnatural prompts could lead to the generation of compromising images ranging\nfrom realistic depictions of forged documents to manipulated images of public\nfigures.\n  We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and\nscalable benchmark dataset to evaluate LLM vulnerability in image generation.\nOur methodology combines structured prompt engineering, multilingual\nobfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted\nLLaMA-3. The pipeline supports both zero-shot and fallback prompting\nstrategies, risk scoring, and automated tagging. All generations are stored\nwith rich metadata and curated into Bronze (non-verified), Silver (LLM-aided\nverification), and Gold (manually verified) tiers. UTCB is designed to evolve\nover time with new data sources, prompt templates, and model behaviors.\n  Warning: This paper includes visual examples of adversarial inputs designed\nto test model safety. All outputs have been redacted to ensure responsible\ndisclosure."}
{"id": "2505.04619", "pdf": "https://arxiv.org/pdf/2505.04619", "abs": "https://arxiv.org/abs/2505.04619", "authors": ["Abdulaziz Almuzairee", "Rohan Patil", "Dwait Bhatt", "Henrik I. Christensen"], "title": "Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation", "categories": ["cs.LG", "cs.CV", "cs.RO"], "comment": "For project website and code, see https://aalmuzairee.github.io/mad", "summary": "Vision is well-known for its use in manipulation, especially using visual\nservoing. To make it robust, multiple cameras are needed to expand the field of\nview. That is computationally challenging. Merging multiple views and using\nQ-learning allows the design of more effective representations and optimization\nof sample efficiency. Such a solution might be expensive to deploy. To mitigate\nthis, we introduce a Merge And Disentanglement (MAD) algorithm that efficiently\nmerges views to increase sample efficiency while augmenting with single-view\nfeatures to allow lightweight deployment and ensure robust policies. We\ndemonstrate the efficiency and robustness of our approach using Meta-World and\nManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad"}
{"id": "2505.04147", "pdf": "https://arxiv.org/pdf/2505.04147", "abs": "https://arxiv.org/abs/2505.04147", "authors": ["Lixing Niu", "Jiapeng Li", "Xingping Yu", "Shu Wang", "Ruining Feng", "Bo Wu", "Ping Wei", "Yisen Wang", "Lifeng Fan"], "title": "R^3-VQA: \"Read the Room\" by Video Social Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "\"Read the room\" is a significant social reasoning capability in human daily\nlife. Humans can infer others' mental states from subtle social cues. Previous\nsocial reasoning tasks and datasets lack complexity (e.g., simple scenes, basic\ninteractions, incomplete mental state variables, single-step reasoning, etc.)\nand fall far short of the challenges present in real-life social interactions.\nIn this paper, we contribute a valuable, high-quality, and comprehensive video\ndataset named R^3-VQA with precise and fine-grained annotations of social\nevents and mental states (i.e., belief, intent, desire, and emotion) as well as\ncorresponding social causal chains in complex social scenarios. Moreover, we\ninclude human-annotated and model-generated QAs. Our task R^3-VQA includes\nthree aspects: Social Event Understanding, Mental State Estimation, and Social\nCausal Reasoning. As a benchmark, we comprehensively evaluate the social\nreasoning capabilities and consistencies of current state-of-the-art large\nvision-language models (LVLMs). Comprehensive experiments show that (i) LVLMs\nare still far from human-level consistent social reasoning in complex social\nscenarios; (ii) Theory of Mind (ToM) prompting can help LVLMs perform better on\nsocial reasoning tasks. We provide some of our dataset and codes in\nsupplementary material and will release our full dataset and codes upon\nacceptance."}
{"id": "2505.03745", "pdf": "https://arxiv.org/pdf/2505.03745", "abs": "https://arxiv.org/abs/2505.03745", "authors": ["Yanbiao Liang", "Huihong Shi", "Haikuo Shao", "Zhongfeng Wang"], "title": "AccLLM: Accelerating Long-Context LLM Inference Via Algorithm-Hardware Co-Design", "categories": ["cs.AR", "cs.AI", "cs.LG"], "comment": null, "summary": "Recently, large language models (LLMs) have achieved huge success in the\nnatural language processing (NLP) field, driving a growing demand to extend\ntheir deployment from the cloud to edge devices. However, deploying LLMs on\nresource-constrained edge devices poses significant challenges, including (1)\nintensive computations and huge model sizes, (2) great memory and bandwidth\ndemands introduced by the autoregressive generation process, and (3) limited\nscalability for handling long sequences. To address these challenges, we\npropose AccLLM, a comprehensive acceleration framework that enables efficient\nand fast long-context LLM inference through algorithm and hardware co-design.\nAt the algorithmic level, we integrate (1) pruning, (2) {\\Lambda}-shaped\nattention, and (3) an innovative W2A8KV4 (2-bit weights, 8-bit activations, and\n4-bit KV cache) quantization scheme, thus effectively reducing memory and\nbandwidth requirements while facilitating LLMs' long-sequence generation. At\nthe hardware level, we design a dedicated FPGA-based accelerator with a\nreconfigurable computing engine to effectively and flexibly accommodate diverse\noperations arising from our compression algorithm, thereby fully translating\nthe algorithmic innovations into tangible hardware efficiency. We validate\nAccLLM on the Xilinx Alveo U280 FPGA, demonstrating a 4.07x energy efficiency\nand a 2.98x throughput compared to the state-of-the-art work FlightLLM."}
{"id": "2505.04165", "pdf": "https://arxiv.org/pdf/2505.04165", "abs": "https://arxiv.org/abs/2505.04165", "authors": ["Kairong Yu", "Tianqing Zhang", "Qi Xu", "Gang Pan", "Hongwei Wang"], "title": "TS-SNN: Temporal Shift Module for Spiking Neural Networks", "categories": ["cs.NE", "cs.AI"], "comment": "Accepted by ICML2025", "summary": "Spiking Neural Networks (SNNs) are increasingly recognized for their\nbiological plausibility and energy efficiency, positioning them as strong\nalternatives to Artificial Neural Networks (ANNs) in neuromorphic computing\napplications. SNNs inherently process temporal information by leveraging the\nprecise timing of spikes, but balancing temporal feature utilization with low\nenergy consumption remains a challenge. In this work, we introduce Temporal\nShift module for Spiking Neural Networks (TS-SNN), which incorporates a novel\nTemporal Shift (TS) module to integrate past, present, and future spike\nfeatures within a single timestep via a simple yet effective shift operation. A\nresidual combination method prevents information loss by integrating shifted\nand original features. The TS module is lightweight, requiring only one\nadditional learnable parameter, and can be seamlessly integrated into existing\narchitectures with minimal additional computational cost. TS-SNN achieves\nstate-of-the-art performance on benchmarks like CIFAR-10 (96.72\\%), CIFAR-100\n(80.28\\%), and ImageNet (70.61\\%) with fewer timesteps, while maintaining low\nenergy consumption. This work marks a significant step forward in developing\nefficient and accurate SNN architectures."}
{"id": "2505.03756", "pdf": "https://arxiv.org/pdf/2505.03756", "abs": "https://arxiv.org/abs/2505.03756", "authors": ["Hang Zhang", "Jiuchen Shi", "Yixiao Wang", "Quan Chen", "Yizhou Shan", "Minyi Guo"], "title": "Improving the Serving Performance of Multi-LoRA Large Language Models via Efficient LoRA and KV Cache Management", "categories": ["cs.AR", "cs.AI", "cs.LG", "cs.PF"], "comment": null, "summary": "Multiple Low-Rank Adapters (Multi-LoRAs) are gaining popularity for\ntask-specific Large Language Model (LLM) applications. For multi-LoRA serving,\ncaching hot KV caches and LoRA adapters in high bandwidth memory of\naccelerations can improve inference performance. However, existing Multi-LoRA\ninference systems fail to optimize serving performance like Time-To-First-Toke\n(TTFT), neglecting usage dependencies when caching LoRAs and KVs. We therefore\npropose FASTLIBRA, a Multi-LoRA caching system to optimize the serving\nperformance. FASTLIBRA comprises a dependency-aware cache manager and a\nperformance-driven cache swapper. The cache manager maintains the usage\ndependencies between LoRAs and KV caches during the inference with a unified\ncaching pool. The cache swapper determines the swap-in or out of LoRAs and KV\ncaches based on a unified cost model, when the HBM is idle or busy,\nrespectively. Experimental results show that ELORA reduces the TTFT by 63.4% on\naverage, compared to state-of-the-art works."}
{"id": "2505.04174", "pdf": "https://arxiv.org/pdf/2505.04174", "abs": "https://arxiv.org/abs/2505.04174", "authors": ["Ju-Hyung Lee", "Yanqing Lu"], "title": "On-Device LLM for Context-Aware Wi-Fi Roaming", "categories": ["cs.LG", "cs.AI", "cs.NI", "eess.SP"], "comment": null, "summary": "Wireless roaming is a critical yet challenging task for maintaining seamless\nconnectivity in dynamic mobile environments. Conventional threshold-based or\nheuristic schemes often fail, leading to either sticky or excessive handovers.\nWe introduce the first cross-layer use of an on-device large language model\n(LLM): high-level reasoning in the application layer that issues real-time\nactions executed in the PHY/MAC stack. The LLM addresses two tasks: (i)\ncontext-aware AP selection, where structured prompts fuse environmental cues\n(e.g., location, time) to choose the best BSSID; and (ii) dynamic threshold\nadjustment, where the model adaptively decides when to roam. To satisfy the\ntight latency and resource budgets of edge hardware, we apply a suite of\noptimizations-chain-of-thought prompting, parameter-efficient fine-tuning, and\nquantization. Experiments on indoor and outdoor datasets show that our approach\nsurpasses legacy heuristics and DRL baselines, achieving a strong balance\nbetween roaming stability and signal quality. These findings underscore the\npromise of application-layer LLM reasoning for lower-layer wireless control in\nfuture edge systems."}
{"id": "2505.03757", "pdf": "https://arxiv.org/pdf/2505.03757", "abs": "https://arxiv.org/abs/2505.03757", "authors": ["Vinicius Francisco Rofatto", "Luiz Felipe Rodrigues de Almeida", "Marcelo Tomio Matsuoka", "Ivandro Klein", "Mauricio Roberto Veronez", "Luiz Gonzaga Da Silveira Junior"], "title": "On the Residual-based Neural Network for Unmodeled Distortions in Coordinate Transformation", "categories": ["physics.geo-ph", "cs.CV", "cs.LG", "stat.AP", "stat.ML"], "comment": null, "summary": "Coordinate transformation models often fail to account for nonlinear and\nspatially dependent distortions, leading to significant residual errors in\ngeospatial applications. Here we propose a residual-based neural correction\nstrategy, in which a neural network learns to model only the systematic\ndistortions left by an initial geometric transformation. By focusing solely on\nresidual patterns, the proposed method reduces model complexity and improves\nperformance, particularly in scenarios with sparse or structured control point\nconfigurations. We evaluate the method using both simulated datasets with\nvarying distortion intensities and sampling strategies, as well as under the\nreal-world image georeferencing tasks. Compared with direct neural network\ncoordinate converter and classical transformation models, the residual-based\nneural correction delivers more accurate and stable results under challenging\nconditions, while maintaining comparable performance in ideal cases. These\nfindings demonstrate the effectiveness of residual modelling as a lightweight\nand robust alternative for improving coordinate transformation accuracy."}
{"id": "2505.04175", "pdf": "https://arxiv.org/pdf/2505.04175", "abs": "https://arxiv.org/abs/2505.04175", "authors": ["Naphat Nithisopa", "Teerapong Panboonyuen"], "title": "DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text recognition in natural images remains a challenging yet essential task,\nwith broad applications spanning computer vision and natural language\nprocessing. This paper introduces a novel end-to-end framework that combines\nResNet and Vision Transformer backbones with advanced methodologies, including\nDeformable Convolutions, Retrieval-Augmented Generation, and Conditional Random\nFields (CRF). These innovations collectively enhance feature representation and\nimprove Optical Character Recognition (OCR) performance. Specifically, the\nframework substitutes standard convolution layers in the third and fourth\nblocks with Deformable Convolutions, leverages adaptive dropout for\nregularization, and incorporates CRF for more refined sequence modeling.\nExtensive experiments conducted on six benchmark datasets IC13, IC15, SVT,\nIIIT5K, SVTP, and CUTE80 validate the proposed method's efficacy, achieving\nnotable accuracies: 97.32% on IC13, 58.26% on IC15, 88.10% on SVT, 74.13% on\nIIIT5K, 82.17% on SVTP, and 66.67% on CUTE80, resulting in an average accuracy\nof 77.77%. These results establish a new state-of-the-art for text recognition,\ndemonstrating the robustness of the approach across diverse and challenging\ndatasets."}
{"id": "2505.03763", "pdf": "https://arxiv.org/pdf/2505.03763", "abs": "https://arxiv.org/abs/2505.03763", "authors": ["Asad Aali", "Adney Cardoza", "Melissa Capo"], "title": "Splitwiser: Efficient LM inference with constrained resources", "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.LG"], "comment": null, "summary": "Efficient inference of LLMs remains a crucial challenge, with two main\nphases: a compute-intensive prompt computation and a memory-intensive token\ngeneration. Despite existing batching and scheduling techniques, token\ngeneration phases fail to fully utilize compute resources, especially when\ncompared to prompt computation phases. To address these challenges, we propose\nSplitwiser, a methodology that splits the two phases of an LLM inference\nrequest onto the same GPU, thereby reducing overhead and improving memory\naccess and cache utilization. By eliminating the need to transfer data across\ndevices, Splitwiser aims to minimize network-related overheads. In this report,\nwe describe the basic structure of our proposed pipeline while sharing\npreliminary results and analysis. We implement our proposed multiprocessing\ndesign on two widely-used and independent LLM architectures: Huggingface and\nvLLM. We open-source our code for the respective implementations: 1)\nHuggingface (https://github.com/asad-aali/splitwiser), and 2) vLLM\n(https://github.com/adney11/vllm-sysml)."}
{"id": "2505.04185", "pdf": "https://arxiv.org/pdf/2505.04185", "abs": "https://arxiv.org/abs/2505.04185", "authors": ["Hail Song", "Wonsik Shin", "Naeun Lee", "Soomin Chung", "Nojun Kwak", "Woontack Woo"], "title": "S3D: Sketch-Driven 3D Model Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted as a short paper to the GMCV Workshop at CVPR'25", "summary": "Generating high-quality 3D models from 2D sketches is a challenging task due\nto the inherent ambiguity and sparsity of sketch data. In this paper, we\npresent S3D, a novel framework that converts simple hand-drawn sketches into\ndetailed 3D models. Our method utilizes a U-Net-based encoder-decoder\narchitecture to convert sketches into face segmentation masks, which are then\nused to generate a 3D representation that can be rendered from novel views. To\nensure robust consistency between the sketch domain and the 3D output, we\nintroduce a novel style-alignment loss that aligns the U-Net bottleneck\nfeatures with the initial encoder outputs of the 3D generation module,\nsignificantly enhancing reconstruction fidelity. To further enhance the\nnetwork's robustness, we apply augmentation techniques to the sketch dataset.\nThis streamlined framework demonstrates the effectiveness of S3D in generating\nhigh-quality 3D models from sketch inputs. The source code for this project is\npublicly available at https://github.com/hailsong/S3D."}
{"id": "2505.03814", "pdf": "https://arxiv.org/pdf/2505.03814", "abs": "https://arxiv.org/abs/2505.03814", "authors": ["Ganghua Wang", "Zhaorun Chen", "Bo Li", "Haifeng Xu"], "title": "Cer-Eval: Certifiable and Cost-Efficient Evaluation Framework for LLMs", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "As foundation models continue to scale, the size of trained models grows\nexponentially, presenting significant challenges for their evaluation. Current\nevaluation practices involve curating increasingly large datasets to assess the\nperformance of large language models (LLMs). However, there is a lack of\nsystematic analysis and guidance on determining the sufficiency of test data or\nselecting informative samples for evaluation. This paper introduces a\ncertifiable and cost-efficient evaluation framework for LLMs. Our framework\nadapts to different evaluation objectives and outputs confidence intervals that\ncontain true values with high probability. We use ``test sample complexity'' to\nquantify the number of test points needed for a certifiable evaluation and\nderive tight bounds on test sample complexity. Based on the developed theory,\nwe develop a partition-based algorithm, named Cer-Eval, that adaptively selects\ntest points to minimize the cost of LLM evaluation. Real-world experiments\ndemonstrate that Cer-Eval can save 20% to 40% test points across various\nbenchmarks, while maintaining an estimation error level comparable to the\ncurrent evaluation process and providing a 95% confidence guarantee."}
{"id": "2505.04192", "pdf": "https://arxiv.org/pdf/2505.04192", "abs": "https://arxiv.org/abs/2505.04192", "authors": ["Trinh T. L. Vuong", "Jin Tae Kwak"], "title": "VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present VideoPath-LLaVA, the first large multimodal model (LMM) in\ncomputational pathology that integrates three distinct image scenarios, single\npatch images, automatically keyframe-extracted clips, and manually segmented\nvideo pathology images, to mimic the natural diagnostic process of\npathologists. By generating detailed histological descriptions and culminating\nin a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives\nwith diagnostic reasoning.\n  Central to our approach is the VideoPath-Instruct dataset, comprising 4278\nvideo and diagnosis-specific chain-of-thought instructional pairs sourced from\neducational histopathology videos on YouTube. Although high-quality data is\ncritical for enhancing diagnostic reasoning, its creation is time-intensive and\nlimited in volume. To overcome this challenge, we transfer knowledge from\nexisting single-image instruction datasets to train on weakly annotated,\nkeyframe-extracted clips, followed by fine-tuning on manually segmented videos.\nVideoPath-LLaVA establishes a new benchmark in pathology video analysis and\noffers a promising foundation for future AI systems that support clinical\ndecision-making through integrated visual and diagnostic reasoning. Our code,\ndata, and model are publicly available at\nhttps://github.com/trinhvg/VideoPath-LLaVA."}
{"id": "2505.03817", "pdf": "https://arxiv.org/pdf/2505.03817", "abs": "https://arxiv.org/abs/2505.03817", "authors": ["Aditya Shinde", "Prashant Doshi"], "title": "Modeling Behavioral Preferences of Cyber Adversaries Using Inverse Reinforcement Learning", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper presents a holistic approach to attacker preference modeling from\nsystem-level audit logs using inverse reinforcement learning (IRL). Adversary\nmodeling is an important capability in cybersecurity that lets defenders\ncharacterize behaviors of potential attackers, which enables attribution to\nknown cyber adversary groups. Existing approaches rely on documenting an\never-evolving set of attacker tools and techniques to track known threat\nactors. Although attacks evolve constantly, attacker behavioral preferences are\nintrinsic and less volatile. Our approach learns the behavioral preferences of\ncyber adversaries from forensics data on their tools and techniques. We model\nthe attacker as an expert decision-making agent with unknown behavioral\npreferences situated in a computer host. We leverage attack provenance graphs\nof audit logs to derive a state-action trajectory of the attack. We test our\napproach on open datasets of audit logs containing real attack data. Our\nresults demonstrate for the first time that low-level forensics data can\nautomatically reveal an adversary's subjective preferences, which serves as an\nadditional dimension to modeling and documenting cyber adversaries. Attackers'\npreferences tend to be invariant despite their different tools and indicate\npredispositions that are inherent to the attacker. As such, these inferred\npreferences can potentially serve as unique behavioral signatures of attackers\nand improve threat attribution."}
{"id": "2505.04207", "pdf": "https://arxiv.org/pdf/2505.04207", "abs": "https://arxiv.org/abs/2505.04207", "authors": ["Mustafa Yurdakul", "Şakir Tasdemir"], "title": "An Enhanced YOLOv8 Model for Real-Time and Accurate Pothole Detection and Measurement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Potholes cause vehicle damage and traffic accidents, creating serious safety\nand economic problems. Therefore, early and accurate detection of potholes is\ncrucial. Existing detection methods are usually only based on 2D RGB images and\ncannot accurately analyze the physical characteristics of potholes. In this\npaper, a publicly available dataset of RGB-D images (PothRGBD) is created and\nan improved YOLOv8-based model is proposed for both pothole detection and\npothole physical features analysis. The Intel RealSense D415 depth camera was\nused to collect RGB and depth data from the road surfaces, resulting in a\nPothRGBD dataset of 1000 images. The data was labeled in YOLO format suitable\nfor segmentation. A novel YOLO model is proposed based on the YOLOv8n-seg\narchitecture, which is structurally improved with Dynamic Snake Convolution\n(DSConv), Simple Attention Module (SimAM) and Gaussian Error Linear Unit\n(GELU). The proposed model segmented potholes with irregular edge structure\nmore accurately, and performed perimeter and depth measurements on depth maps\nwith high accuracy. The standard YOLOv8n-seg model achieved 91.9% precision,\n85.2% recall and 91.9% mAP@50. With the proposed model, the values increased to\n93.7%, 90.4% and 93.8% respectively. Thus, an improvement of 1.96% in\nprecision, 6.13% in recall and 2.07% in mAP was achieved. The proposed model\nperforms pothole detection as well as perimeter and depth measurement with high\naccuracy and is suitable for real-time applications due to its low model\ncomplexity. In this way, a lightweight and effective model that can be used in\ndeep learning-based intelligent transportation solutions has been acquired."}
{"id": "2505.03828", "pdf": "https://arxiv.org/pdf/2505.03828", "abs": "https://arxiv.org/abs/2505.03828", "authors": ["Yogesh Gajula"], "title": "Sentiment-Aware Recommendation Systems in E-Commerce: A Review from a Natural Language Processing Perspective", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": "12 pages, 2 tables, 2 figures", "summary": "E-commerce platforms generate vast volumes of user feedback, such as star\nratings, written reviews, and comments. However, most recommendation engines\nrely primarily on numerical scores, often overlooking the nuanced opinions\nembedded in free text. This paper comprehensively reviews sentiment-aware\nrecommendation systems from a natural language processing perspective, covering\nadvancements from 2023 to early 2025. It highlights the benefits of integrating\nsentiment analysis into e-commerce recommenders to enhance prediction accuracy\nand explainability through detailed opinion extraction. Our survey categorizes\nrecent work into four main approaches: deep learning classifiers that combine\nsentiment embeddings with user item interactions, transformer based methods for\nnuanced feature extraction, graph neural networks that propagate sentiment\nsignals, and conversational recommenders that adapt in real time to user\nfeedback. We summarize model architectures and demonstrate how sentiment flows\nthrough recommendation pipelines, impacting dialogue-based suggestions. Key\nchallenges include handling noisy or sarcastic text, dynamic user preferences,\nand bias mitigation. Finally, we outline research gaps and provide a roadmap\nfor developing smarter, fairer, and more user-centric recommendation tools."}
{"id": "2505.04209", "pdf": "https://arxiv.org/pdf/2505.04209", "abs": "https://arxiv.org/abs/2505.04209", "authors": ["Soumik Dey", "Hansi Wu", "Binbin Li"], "title": "To Judge or not to Judge: Using LLM Judgements for Advertiser Keyphrase Relevance at eBay", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": null, "summary": "E-commerce sellers are recommended keyphrases based on their inventory on\nwhich they advertise to increase buyer engagement (clicks/sales). The relevance\nof advertiser keyphrases plays an important role in preventing the inundation\nof search systems with numerous irrelevant items that compete for attention in\nauctions, in addition to maintaining a healthy seller perception. In this work,\nwe describe the shortcomings of training Advertiser keyphrase relevance filter\nmodels on click/sales/search relevance signals and the importance of aligning\nwith human judgment, as sellers have the power to adopt or reject said\nkeyphrase recommendations. In this study, we frame Advertiser keyphrase\nrelevance as a complex interaction between 3 dynamical systems -- seller\njudgment, which influences seller adoption of our product, Advertising, which\nprovides the keyphrases to bid on, and Search, who holds the auctions for the\nsame keyphrases. This study discusses the practicalities of using human\njudgment via a case study at eBay Advertising and demonstrate that using\nLLM-as-a-judge en-masse as a scalable proxy for seller judgment to train our\nrelevance models achieves a better harmony across the three systems -- provided\nthat they are bound by a meticulous evaluation framework grounded in business\nmetrics."}
{"id": "2505.03831", "pdf": "https://arxiv.org/pdf/2505.03831", "abs": "https://arxiv.org/abs/2505.03831", "authors": ["Esra Hotoğlu", "Sevil Sen", "Burcu Can"], "title": "A Comprehensive Analysis of Adversarial Attacks against Spam Filters", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Deep learning has revolutionized email filtering, which is critical to\nprotect users from cyber threats such as spam, malware, and phishing. However,\nthe increasing sophistication of adversarial attacks poses a significant\nchallenge to the effectiveness of these filters. This study investigates the\nimpact of adversarial attacks on deep learning-based spam detection systems\nusing real-world datasets. Six prominent deep learning models are evaluated on\nthese datasets, analyzing attacks at the word, character sentence, and\nAI-generated paragraph-levels. Novel scoring functions, including spam weights\nand attention weights, are introduced to improve attack effectiveness. This\ncomprehensive analysis sheds light on the vulnerabilities of spam filters and\ncontributes to efforts to improve their security against evolving adversarial\nthreats."}
{"id": "2505.04223", "pdf": "https://arxiv.org/pdf/2505.04223", "abs": "https://arxiv.org/abs/2505.04223", "authors": ["Sanghyeon Park", "Soo-Mook Moon"], "title": "FRAIN to Train: A Fast-and-Reliable Solution for Decentralized Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated learning (FL) enables collaborative model training across\ndistributed clients while preserving data locality. Although FedAvg pioneered\nsynchronous rounds for global model averaging, slower devices can delay\ncollective progress. Asynchronous FL (e.g., FedAsync) addresses stragglers by\ncontinuously integrating client updates, yet naive implementations risk client\ndrift due to non-IID data and stale contributions. Some Blockchain-based FL\napproaches (e.g., BRAIN) employ robust weighting or scoring of updates to\nresist malicious or misaligned proposals. However, performance drops can still\npersist under severe data heterogeneity or high staleness, and synchronization\noverhead has emerged as a new concern due to its aggregator-free architectures.\n  We introduce Fast-and-Reliable AI Network, FRAIN, a new asynchronous FL\nmethod that mitigates these limitations by incorporating two key ideas. First,\nour FastSync strategy eliminates the need to replay past model versions,\nenabling newcomers and infrequent participants to efficiently approximate the\nglobal model. Second, we adopt spherical linear interpolation (SLERP) when\nmerging parameters, preserving models' directions and alleviating destructive\ninterference from divergent local training.\n  Experiments with a CNN image-classification model and a Transformer-based\nlanguage model demonstrate that FRAIN achieves more stable and robust\nconvergence than FedAvg, FedAsync, and BRAIN, especially under harsh\nenvironments: non-IID data distributions, networks that experience delays and\nrequire frequent re-synchronization, and the presence of malicious nodes."}
{"id": "2505.03833", "pdf": "https://arxiv.org/pdf/2505.03833", "abs": "https://arxiv.org/abs/2505.03833", "authors": ["Xuechao Wang", "Sven Nomm", "Junqing Huang", "Kadri Medijainen", "Aaro Toomela", "Michael Ruzhansky"], "title": "PointExplainer: Towards Transparent Parkinson's Disease Diagnosis", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Deep neural networks have shown potential in analyzing digitized hand-drawn\nsignals for early diagnosis of Parkinson's disease. However, the lack of clear\ninterpretability in existing diagnostic methods presents a challenge to\nclinical trust. In this paper, we propose PointExplainer, an explainable\ndiagnostic strategy to identify hand-drawn regions that drive model diagnosis.\nSpecifically, PointExplainer assigns discrete attribution values to hand-drawn\nsegments, explicitly quantifying their relative contributions to the model's\ndecision. Its key components include: (i) a diagnosis module, which encodes\nhand-drawn signals into 3D point clouds to represent hand-drawn trajectories,\nand (ii) an explanation module, which trains an interpretable surrogate model\nto approximate the local behavior of the black-box diagnostic model. We also\nintroduce consistency measures to further address the issue of faithfulness in\nexplanations. Extensive experiments on two benchmark datasets and a newly\nconstructed dataset show that PointExplainer can provide intuitive explanations\nwith no diagnostic performance degradation. The source code is available at\nhttps://github.com/chaoxuewang/PointExplainer."}
{"id": "2505.04251", "pdf": "https://arxiv.org/pdf/2505.04251", "abs": "https://arxiv.org/abs/2505.04251", "authors": ["Krishna Ronanki"], "title": "Facilitating Trustworthy Human-Agent Collaboration in LLM-based Multi-Agent System oriented Software Engineering", "categories": ["cs.SE", "cs.AI", "cs.MA"], "comment": null, "summary": "Multi-agent autonomous systems (MAS) are better at addressing challenges that\nspans across multiple domains than singular autonomous agents. This holds true\nwithin the field of software engineering (SE) as well. The state-of-the-art\nresearch on MAS within SE focuses on integrating LLMs at the core of autonomous\nagents to create LLM-based multi-agent autonomous (LMA) systems. However, the\nintroduction of LMA systems into SE brings a plethora of challenges. One of the\nmajor challenges is the strategic allocation of tasks between humans and the\nLMA system in a trustworthy manner. To address this challenge, a RACI-based\nframework is proposed in this work in progress article, along with\nimplementation guidelines and an example implementation of the framework. The\nproposed framework can facilitate efficient collaboration, ensure\naccountability, and mitigate potential risks associated with LLM-driven\nautomation while aligning with the Trustworthy AI guidelines. The future steps\nfor this work delineating the planned empirical validation method are also\npresented."}
{"id": "2505.03840", "pdf": "https://arxiv.org/pdf/2505.03840", "abs": "https://arxiv.org/abs/2505.03840", "authors": ["Cairong Yan", "Jinyi Han", "Jin Ju", "Yanting Zhang", "Zijian Wang", "Xuan Shao"], "title": "CoCoB: Adaptive Collaborative Combinatorial Bandits for Online Recommendation", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": "This paper has been accepted by DASFAA 2025: The International\n  Conference on Database Systems for Advanced Applications. This version\n  provides more detailed information", "summary": "Clustering bandits have gained significant attention in recommender systems\nby leveraging collaborative information from neighboring users to better\ncapture target user preferences. However, these methods often lack a clear\ndefinition of similar users and face challenges when users with unique\npreferences lack appropriate neighbors. In such cases, relying on divergent\npreferences of misidentified neighbors can degrade recommendation quality. To\naddress these limitations, this paper proposes an adaptive Collaborative\nCombinatorial Bandits algorithm (CoCoB). CoCoB employs an innovative two-sided\nbandit architecture, applying bandit principles to both the user and item\nsides. The user-bandit employs an enhanced Bayesian model to explore user\nsimilarity, identifying neighbors based on a similarity probability threshold.\nThe item-bandit treats items as arms, generating diverse recommendations\ninformed by the user-bandit's output. CoCoB dynamically adapts, leveraging\nneighbor preferences when available or focusing solely on the target user\notherwise. Regret analysis under a linear contextual bandit setting and\nexperiments on three real-world datasets demonstrate CoCoB's effectiveness,\nachieving an average 2.4% improvement in F1 score over state-of-the-art\nmethods."}
{"id": "2505.04260", "pdf": "https://arxiv.org/pdf/2505.04260", "abs": "https://arxiv.org/abs/2505.04260", "authors": ["Jessica Y. Bo", "Tianyu Xu", "Ishan Chatterjee", "Katrina Passarella-Ward", "Achin Kulshrestha", "D Shin"], "title": "Steerable Chatbots: Personalizing LLMs with Preference-Based Activation Steering", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) improve in their capacity to serve as\npersonal AI assistants, their ability to output uniquely tailored, personalized\nresponses that align with the soft preferences of their users is essential for\nenhancing user satisfaction and retention. However, untrained lay users have\npoor prompt specification abilities and often struggle with conveying their\nlatent preferences to AI assistants. To address this, we leverage activation\nsteering to guide LLMs to align with interpretable preference dimensions during\ninference. In contrast to memory-based personalization methods that require\nlonger user history, steering is extremely lightweight and can be easily\ncontrolled by the user via an linear strength factor. We embed steering into\nthree different interactive chatbot interfaces and conduct a within-subjects\nuser study (n=14) to investigate how end users prefer to personalize their\nconversations. The results demonstrate the effectiveness of preference-based\nsteering for aligning real-world conversations with hidden user preferences,\nand highlight further insights on how diverse values around control, usability,\nand transparency lead users to prefer different interfaces."}
{"id": "2505.03845", "pdf": "https://arxiv.org/pdf/2505.03845", "abs": "https://arxiv.org/abs/2505.03845", "authors": ["Ioannis Kyprakis", "Vasileios Skaramagkas", "Iro Boura", "Georgios Karamanis", "Dimitrios I. Fotiadis", "Zinovia Kefalopoulou", "Cleanthe Spanaki", "Manolis Tsiknakis"], "title": "A Deep Learning approach for Depressive Symptoms assessment in Parkinson's disease patients using facial videos", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Parkinson's disease (PD) is a neurodegenerative disorder, manifesting with\nmotor and non-motor symptoms. Depressive symptoms are prevalent in PD,\naffecting up to 45% of patients. They are often underdiagnosed due to\noverlapping motor features, such as hypomimia. This study explores deep\nlearning (DL) models-ViViT, Video Swin Tiny, and 3D CNN-LSTM with attention\nlayers-to assess the presence and severity of depressive symptoms, as detected\nby the Geriatric Depression Scale (GDS), in PD patients through facial video\nanalysis. The same parameters were assessed in a secondary analysis taking into\naccount whether patients were one hour after (ON-medication state) or 12 hours\nwithout (OFF-medication state) dopaminergic medication. Using a dataset of\n1,875 videos from 178 patients, the Video Swin Tiny model achieved the highest\nperformance, with up to 94% accuracy and 93.7% F1-score in binary\nclassification (presence of absence of depressive symptoms), and 87.1% accuracy\nwith an 85.4% F1-score in multiclass tasks (absence or mild or severe\ndepressive symptoms)."}
{"id": "2505.04265", "pdf": "https://arxiv.org/pdf/2505.04265", "abs": "https://arxiv.org/abs/2505.04265", "authors": ["Abdulrahman S Almuhaidib", "Azlan Mohd Zain", "Zalmiyah Zakaria", "Izyan Izzati Kamsani", "Abdulaziz S Almuhaidib"], "title": "Weaponizing Language Models for Cybersecurity Offensive Operations: Automating Vulnerability Assessment Report Validation; A Review Paper", "categories": ["cs.CR", "cs.AI"], "comment": "Pre-print - Accepted for publication in the Proceedings of the\n  International Computer Sciences and Informatics Conference (ICSIC-2024),\n  published by AIP Publishing", "summary": "This, with the ever-increasing sophistication of cyberwar, calls for novel\nsolutions. In this regard, Large Language Models (LLMs) have emerged as a\nhighly promising tool for defensive and offensive cybersecurity-related\nstrategies. While existing literature has focused much on the defensive use of\nLLMs, when it comes to their offensive utilization, very little has been\nreported-namely, concerning Vulnerability Assessment (VA) report validation.\nConsequentially, this paper tries to fill that gap by investigating the\ncapabilities of LLMs in automating and improving the validation process of the\nreport of the VA. From the critical review of the related literature, this\npaper hereby proposes a new approach to using the LLMs in the automation of the\nanalysis and within the validation process of the report of the VA that could\npotentially reduce the number of false positives and generally enhance\nefficiency. These results are promising for LLM automatization for improving\nvalidation on reports coming from VA in order to improve accuracy while\nreducing human effort and security postures. The contribution of this paper\nprovides further evidence about the offensive and defensive LLM capabilities\nand therefor helps in devising more appropriate cybersecurity strategies and\ntools accordingly."}
{"id": "2505.03848", "pdf": "https://arxiv.org/pdf/2505.03848", "abs": "https://arxiv.org/abs/2505.03848", "authors": ["Janhavi Giri", "Attila Lengyel", "Don Kent", "Edward Kibardin"], "title": "Advanced Clustering Framework for Semiconductor Image Analytics Integrating Deep TDA with Self-Supervised and Transfer Learning Techniques", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.LG"], "comment": "46 pages, 22 figures, 5 tables", "summary": "Semiconductor manufacturing generates vast amounts of image data, crucial for\ndefect identification and yield optimization, yet often exceeds manual\ninspection capabilities. Traditional clustering techniques struggle with\nhigh-dimensional, unlabeled data, limiting their effectiveness in capturing\nnuanced patterns. This paper introduces an advanced clustering framework that\nintegrates deep Topological Data Analysis (TDA) with self-supervised and\ntransfer learning techniques, offering a novel approach to unsupervised image\nclustering. TDA captures intrinsic topological features, while self-supervised\nlearning extracts meaningful representations from unlabeled data, reducing\nreliance on labeled datasets. Transfer learning enhances the framework's\nadaptability and scalability, allowing fine-tuning to new datasets without\nretraining from scratch. Validated on synthetic and open-source semiconductor\nimage datasets, the framework successfully identifies clusters aligned with\ndefect patterns and process variations. This study highlights the\ntransformative potential of combining TDA, self-supervised learning, and\ntransfer learning, providing a scalable solution for proactive process\nmonitoring and quality control in semiconductor manufacturing and other domains\nwith large-scale image datasets."}
{"id": "2505.04270", "pdf": "https://arxiv.org/pdf/2505.04270", "abs": "https://arxiv.org/abs/2505.04270", "authors": ["Yisen Feng", "Haoyu Zhang", "Meng Liu", "Weili Guan", "Liqiang Nie"], "title": "Object-Shot Enhanced Grounding Network for Egocentric Video", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025", "summary": "Egocentric video grounding is a crucial task for embodied intelligence\napplications, distinct from exocentric video moment localization. Existing\nmethods primarily focus on the distributional differences between egocentric\nand exocentric videos but often neglect key characteristics of egocentric\nvideos and the fine-grained information emphasized by question-type queries. To\naddress these limitations, we propose OSGNet, an Object-Shot enhanced Grounding\nNetwork for egocentric video. Specifically, we extract object information from\nvideos to enrich video representation, particularly for objects highlighted in\nthe textual query but not directly captured in the video features.\nAdditionally, we analyze the frequent shot movements inherent to egocentric\nvideos, leveraging these features to extract the wearer's attention\ninformation, which enhances the model's ability to perform modality alignment.\nExperiments conducted on three datasets demonstrate that OSGNet achieves\nstate-of-the-art performance, validating the effectiveness of our approach. Our\ncode can be found at https://github.com/Yisen-Feng/OSGNet."}
{"id": "2505.03858", "pdf": "https://arxiv.org/pdf/2505.03858", "abs": "https://arxiv.org/abs/2505.03858", "authors": ["Alireza Khayatian", "Anil Vullikanti", "Aritra Konar"], "title": "Differentially Private Densest-$k$-Subgraph", "categories": ["cs.DS", "cs.LG"], "comment": null, "summary": "Many graph datasets involve sensitive network data, motivating the need for\nprivacy-preserving graph mining. The Densest-$k$-subgraph (D$k$S) problem is a\nkey primitive in graph mining that aims to extract a subset of $k$ vertices\nwith the maximum internal connectivity. Although non-private algorithms are\nknown for D$k$S, this paper is the first to design algorithms that offer formal\ndifferential privacy (DP) guarantees for the problem. We base our general\napproach on using the principal component (PC) of the graph adjacency matrix to\noutput a subset of $k$ vertices under edge DP. For this task, we first consider\noutput perturbation, which traditionally offer good scalability, but at the\nexpense of utility. Our tight on the local sensitivity indicate a big gap with\nthe global sensitivity, motivating the use of instance specific sensitive\nmethods for private PC. Next, we derive a tight bound on the smooth sensitivity\nand show that it can be close to the global sensitivity. This leads us to\nconsider the Propose-Test-Release (PTR) framework for private PC. Although\ncomputationally expensive in general, we design a novel approach for\nimplementing PTR in the same time as computation of a non-private PC, while\noffering good utility for \\DkS{}. Additionally, we also consider the iterative\nprivate power method (PPM) for private PC, albeit it is significantly slower\nthan PTR on large networks. We run our methods on diverse real-world networks,\nwith the largest having 3 million vertices, and show good privacy-utility\ntrade-offs. Although PTR requires a slightly larger privacy budget, on average,\nit achieves a 180-fold improvement in runtime over PPM."}
{"id": "2505.04278", "pdf": "https://arxiv.org/pdf/2505.04278", "abs": "https://arxiv.org/abs/2505.04278", "authors": ["Weiwei Ye", "Zhuopeng Xu", "Ning Gui"], "title": "Non-stationary Diffusion For Probabilistic Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted as spotlight poster at ICML", "summary": "Due to the dynamics of underlying physics and external influences, the\nuncertainty of time series often varies over time. However, existing Denoising\nDiffusion Probabilistic Models (DDPMs) often fail to capture this\nnon-stationary nature, constrained by their constant variance assumption from\nthe additive noise model (ANM). In this paper, we innovatively utilize the\nLocation-Scale Noise Model (LSNM) to relax the fixed uncertainty assumption of\nANM. A diffusion-based probabilistic forecasting framework, termed\nNon-stationary Diffusion (NsDiff), is designed based on LSNM that is capable of\nmodeling the changing pattern of uncertainty. Specifically, NsDiff combines a\ndenoising diffusion-based conditional generative model with a pre-trained\nconditional mean and variance estimator, enabling adaptive endpoint\ndistribution modeling. Furthermore, we propose an uncertainty-aware noise\nschedule, which dynamically adjusts the noise levels to accurately reflect the\ndata uncertainty at each step and integrates the time-varying variances into\nthe diffusion process. Extensive experiments conducted on nine real-world and\nsynthetic datasets demonstrate the superior performance of NsDiff compared to\nexisting approaches. Code is available at https://github.com/wwy155/NsDiff."}
{"id": "2505.03862", "pdf": "https://arxiv.org/pdf/2505.03862", "abs": "https://arxiv.org/abs/2505.03862", "authors": ["Hông Vân Lê", "Hà Quang Minh", "Frederic Protin", "Wilderich Tuschmann"], "title": "Categorical and geometric methods in statistical, manifold, and machine learning", "categories": ["stat.ML", "cs.LG", "math.CT", "math.DG", "math.ST", "stat.TH"], "comment": "37 p., will appear as part of a special volume in the Springer Tohoku\n  Series in Mathematics", "summary": "We present and discuss applications of the category of probabilistic\nmorphisms, initially developed in \\cite{Le2023}, as well as some geometric\nmethods to several classes of problems in statistical, machine and manifold\nlearning which shall be, along with many other topics, considered in depth in\nthe forthcoming book \\cite{LMPT2024}."}
{"id": "2505.04284", "pdf": "https://arxiv.org/pdf/2505.04284", "abs": "https://arxiv.org/abs/2505.04284", "authors": ["Sofia Jamil", "Aryan Dabad", "Bollampalli Areen Reddy", "Sriparna Saha", "Rajiv Misra", "Adil A. Shakur"], "title": "GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer Pharmacovigilance", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the realm of cancer treatment, summarizing adverse drug events (ADEs)\nreported by patients using prescribed drugs is crucial for enhancing\npharmacovigilance practices and improving drug-related decision-making. While\nthe volume and complexity of pharmacovigilance data have increased, existing\nresearch in this field has predominantly focused on general diseases rather\nthan specifically addressing cancer. This work introduces the task of grouped\nsummarization of adverse drug events reported by multiple patients using the\nsame drug for cancer treatment. To address the challenge of limited resources\nin cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug\nReaction and Summarization (MCADRS) dataset. This dataset includes\npharmacovigilance posts detailing patient concerns regarding drug efficacy and\nadverse effects, along with extracted labels for drug names, adverse drug\nevents, severity, and adversity of reactions, as well as summaries of ADEs for\neach drug. Additionally, we propose the Grouping and Abstractive Summarization\nof Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that\ncombines the information extraction capabilities of Large Language Models\n(LLMs) with the summarization power of the encoder-decoder T5 model. Our work\nis the first to apply alignment techniques, including advanced algorithms like\nDirect Preference Optimization, to encoder-decoder models using synthetic\ndatasets for summarization tasks. Through extensive experiments, we demonstrate\nthe superior performance of GASCADE across various metrics, validated through\nboth automated assessments and human evaluations. This multitasking approach\nenhances drug-related decision-making and fosters a deeper understanding of\npatient concerns, paving the way for advancements in personalized and\nresponsive cancer care. The code and dataset used in this work are publicly\navailable."}
{"id": "2505.03864", "pdf": "https://arxiv.org/pdf/2505.03864", "abs": "https://arxiv.org/abs/2505.03864", "authors": ["Qiaomu Li", "Ying Xie"], "title": "From Glue-Code to Protocols: A Critical Analysis of A2A and MCP Integration for Scalable Agent Systems", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "Artificial intelligence is rapidly evolving towards multi-agent systems where\nnumerous AI agents collaborate and interact with external tools. Two key open\nstandards, Google's Agent to Agent (A2A) protocol for inter-agent communication\nand Anthropic's Model Context Protocol (MCP) for standardized tool access,\npromise to overcome the limitations of fragmented, custom integration\napproaches. While their potential synergy is significant, this paper argues\nthat effectively integrating A2A and MCP presents unique, emergent challenges\nat their intersection, particularly concerning semantic interoperability\nbetween agent tasks and tool capabilities, the compounded security risks\narising from combined discovery and execution, and the practical governance\nrequired for the envisioned \"Agent Economy\". This work provides a critical\nanalysis, moving beyond a survey to evaluate the practical implications and\ninherent difficulties of combining these horizontal and vertical integration\nstandards. We examine the benefits (e.g., specialization, scalability) while\ncritically assessing their dependencies and trade-offs in an integrated\ncontext. We identify key challenges increased by the integration, including\nnovel security vulnerabilities, privacy complexities, debugging difficulties\nacross protocols, and the need for robust semantic negotiation mechanisms. In\nsummary, A2A+MCP offers a vital architectural foundation, but fully realizing\nits potential requires substantial advancements to manage the complexities of\ntheir combined operation."}
{"id": "2505.04300", "pdf": "https://arxiv.org/pdf/2505.04300", "abs": "https://arxiv.org/abs/2505.04300", "authors": ["Isabella Caranzano", "Corrado Pancotti", "Cesare Rollo", "Flavio Sartori", "Pietro Liò", "Piero Fariselli", "Tiziana Sanavia"], "title": "Sparsity is All You Need: Rethinking Biological Pathway-Informed Approaches in Deep Learning", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": null, "summary": "Biologically-informed neural networks typically leverage pathway annotations\nto enhance performance in biomedical applications. We hypothesized that the\nbenefits of pathway integration does not arise from its biological relevance,\nbut rather from the sparsity it introduces. We conducted a comprehensive\nanalysis of all relevant pathway-based neural network models for predictive\ntasks, critically evaluating each study's contributions. From this review, we\ncurated a subset of methods for which the source code was publicly available.\nThe comparison of the biologically informed state-of-the-art deep learning\nmodels and their randomized counterparts showed that models based on randomized\ninformation performed equally well as biologically informed ones across\ndifferent metrics and datasets. Notably, in 3 out of the 15 analyzed models,\nthe randomized versions even outperformed their biologically informed\ncounterparts. Moreover, pathway-informed models did not show any clear\nadvantage in interpretability, as randomized models were still able to identify\nrelevant disease biomarkers despite lacking explicit pathway information. Our\nfindings suggest that pathway annotations may be too noisy or inadequately\nexplored by current methods. Therefore, we propose a methodology that can be\napplied to different domains and can serve as a robust benchmark for\nsystematically comparing novel pathway-informed models against their randomized\ncounterparts. This approach enables researchers to rigorously determine whether\nobserved performance improvements can be attributed to biological insights."}
{"id": "2505.03906", "pdf": "https://arxiv.org/pdf/2505.03906", "abs": "https://arxiv.org/abs/2505.03906", "authors": ["Asif Rahman", "Veljko Cvetkovic", "Kathleen Reece", "Aidan Walters", "Yasir Hassan", "Aneesh Tummeti", "Bryan Torres", "Denise Cooney", "Margaret Ellis", "Dimitrios S. Nikolopoulos"], "title": "MARCO: A Multi-Agent System for Optimizing HPC Code Generation Using Large Language Models", "categories": ["cs.DC", "cs.LG", "cs.SE"], "comment": "9 pages, 4 figures, 2 tables", "summary": "Large language models (LLMs) have transformed software development through\ncode generation capabilities, yet their effectiveness for high-performance\ncomputing (HPC) remains limited. HPC code requires specialized optimizations\nfor parallelism, memory efficiency, and architecture-specific considerations\nthat general-purpose LLMs often overlook. We present MARCO (Multi-Agent\nReactive Code Optimizer), a novel framework that enhances LLM-generated code\nfor HPC through a specialized multi-agent architecture. MARCO employs separate\nagents for code generation and performance evaluation, connected by a feedback\nloop that progressively refines optimizations. A key innovation is MARCO's\nweb-search component that retrieves real-time optimization techniques from\nrecent conference proceedings and research publications, bridging the knowledge\ngap in pre-trained LLMs. Our extensive evaluation on the LeetCode 75 problem\nset demonstrates that MARCO achieves a 14.6% average runtime reduction compared\nto Claude 3.5 Sonnet alone, while the integration of the web-search component\nyields a 30.9% performance improvement over the base MARCO system. These\nresults highlight the potential of multi-agent systems to address the\nspecialized requirements of high-performance code generation, offering a\ncost-effective alternative to domain-specific model fine-tuning."}
{"id": "2505.04308", "pdf": "https://arxiv.org/pdf/2505.04308", "abs": "https://arxiv.org/abs/2505.04308", "authors": ["Md Saiful Islam", "Li Xiangdong"], "title": "Guardians of the Web: The Evolution and Future of Website Information Security", "categories": ["cs.CR", "cs.AI", "F.2.2, I.2.7"], "comment": "22 pages", "summary": "Website information security has become a critical concern in the digital\nage. This article explores the evolution of website information security,\nexamining its historical development, current practices, and future directions.\nThe early beginnings from the 1960s to the 1980s laid the groundwork for modern\ncybersecurity, with the development of ARPANET, TCP/IP, public-key\ncryptography, and the first antivirus programs. The 1990s marked a\ntransformative era, driven by the commercialization of the Internet and the\nemergence of web-based services. As the Internet grew, so did the range and\nsophistication of cyber threats, leading to advancements in security\ntechnologies such as the Secure Sockets Layer (SSL) protocol, password\nprotection, and firewalls. Current practices in website information security\ninvolve a multi-layered approach, including encryption, secure coding\npractices, regular security audits, and user education. The future of website\ninformation security is expected to be shaped by emerging technologies such as\nartificial intelligence, blockchain, and quantum computing, as well as the\nincreasing importance of international cooperation and standardization efforts.\nAs cyber threats continue to evolve, ongoing research and innovation in website\ninformation security will be essential to protect sensitive information and\nmaintain trust in the digital world."}
{"id": "2505.04002", "pdf": "https://arxiv.org/pdf/2505.04002", "abs": "https://arxiv.org/abs/2505.04002", "authors": ["Michael Xu", "Yi Shi", "KangKang Yin", "Xue Bin Peng"], "title": "PARC: Physics-based Augmentation with Reinforcement Learning for Character Controllers", "categories": ["cs.GR", "cs.AI", "cs.LG", "cs.RO"], "comment": "SIGGRAPH Conference Papers 2025", "summary": "Humans excel in navigating diverse, complex environments with agile motor\nskills, exemplified by parkour practitioners performing dynamic maneuvers, such\nas climbing up walls and jumping across gaps. Reproducing these agile movements\nwith simulated characters remains challenging, in part due to the scarcity of\nmotion capture data for agile terrain traversal behaviors and the high cost of\nacquiring such data. In this work, we introduce PARC (Physics-based\nAugmentation with Reinforcement Learning for Character Controllers), a\nframework that leverages machine learning and physics-based simulation to\niteratively augment motion datasets and expand the capabilities of terrain\ntraversal controllers. PARC begins by training a motion generator on a small\ndataset consisting of core terrain traversal skills. The motion generator is\nthen used to produce synthetic data for traversing new terrains. However, these\ngenerated motions often exhibit artifacts, such as incorrect contacts or\ndiscontinuities. To correct these artifacts, we train a physics-based tracking\ncontroller to imitate the motions in simulation. The corrected motions are then\nadded to the dataset, which is used to continue training the motion generator\nin the next iteration. PARC's iterative process jointly expands the\ncapabilities of the motion generator and tracker, creating agile and versatile\nmodels for interacting with complex environments. PARC provides an effective\napproach to develop controllers for agile terrain traversal, which bridges the\ngap between the scarcity of motion data and the need for versatile character\ncontrollers."}
{"id": "2505.04318", "pdf": "https://arxiv.org/pdf/2505.04318", "abs": "https://arxiv.org/abs/2505.04318", "authors": ["Jacob Glenn Ayers", "Buvaneswari A. Ramanan", "Manzoor A. Khan"], "title": "Detecting Concept Drift in Neural Networks Using Chi-squared Goodness of Fit Testing", "categories": ["cs.LG", "cs.AI", "eess.IV"], "comment": "8 pages, 6 figures, 1 table", "summary": "As the adoption of deep learning models has grown beyond human capacity for\nverification, meta-algorithms are needed to ensure reliable model inference.\nConcept drift detection is a field dedicated to identifying statistical shifts\nthat is underutilized in monitoring neural networks that may encounter\ninference data with distributional characteristics diverging from their\ntraining data. Given the wide variety of model architectures, applications, and\ndatasets, it is important that concept drift detection algorithms are adaptable\nto different inference scenarios. In this paper, we introduce an application of\nthe $\\chi^2$ Goodness of Fit Hypothesis Test as a drift detection\nmeta-algorithm applied to a multilayer perceptron, a convolutional neural\nnetwork, and a transformer trained for machine vision as they are exposed to\nsimulated drift during inference. To that end, we demonstrate how unexpected\ndrops in accuracy due to concept drift can be detected without directly\nexamining the inference outputs. Our approach enhances safety by ensuring\nmodels are continually evaluated for reliability across varying conditions."}
{"id": "2505.04007", "pdf": "https://arxiv.org/pdf/2505.04007", "abs": "https://arxiv.org/abs/2505.04007", "authors": ["Yinzhuang Yi", "Jorge Cortés", "Nikolay Atanasov"], "title": "Variational Formulation of the Particle Flow Particle Filter", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "This paper provides a formulation of the particle flow particle filter from\nthe perspective of variational inference. We show that the transient density\nused to derive the particle flow particle filter follows a time-scaled\ntrajectory of the Fisher-Rao gradient flow in the space of probability\ndensities. The Fisher-Rao gradient flow is obtained as a continuous-time\nalgorithm for variational inference, minimizing the Kullback-Leibler divergence\nbetween a variational density and the true posterior density."}
{"id": "2505.04340", "pdf": "https://arxiv.org/pdf/2505.04340", "abs": "https://arxiv.org/abs/2505.04340", "authors": ["Hong Jin", "Kaicheng Zhou", "Jie Yin", "Lan You", "Zhifeng Zhou"], "title": "Multi-Granular Attention based Heterogeneous Hypergraph Neural Network", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Heterogeneous graph neural networks (HeteGNNs) have demonstrated strong\nabilities to learn node representations by effectively extracting complex\nstructural and semantic information in heterogeneous graphs. Most of the\nprevailing HeteGNNs follow the neighborhood aggregation paradigm, leveraging\nmeta-path based message passing to learn latent node representations. However,\ndue to the pairwise nature of meta-paths, these models fail to capture\nhigh-order relations among nodes, resulting in suboptimal performance.\nAdditionally, the challenge of ``over-squashing'', where long-range message\npassing in HeteGNNs leads to severe information distortion, further limits the\nefficacy of these models. To address these limitations, this paper proposes\nMGA-HHN, a Multi-Granular Attention based Heterogeneous Hypergraph Neural\nNetwork for heterogeneous graph representation learning. MGA-HHN introduces two\nkey innovations: (1) a novel approach for constructing meta-path based\nheterogeneous hypergraphs that explicitly models higher-order semantic\ninformation in heterogeneous graphs through multiple views, and (2) a\nmulti-granular attention mechanism that operates at both the node and hyperedge\nlevels. This mechanism enables the model to capture fine-grained interactions\namong nodes sharing the same semantic context within a hyperedge type, while\npreserving the diversity of semantics across different hyperedge types. As\nsuch, MGA-HHN effectively mitigates long-range message distortion and generates\nmore expressive node representations. Extensive experiments on real-world\nbenchmark datasets demonstrate that MGA-HHN outperforms state-of-the-art\nmodels, showcasing its effectiveness in node classification, node clustering\nand visualization tasks."}
{"id": "2505.04016", "pdf": "https://arxiv.org/pdf/2505.04016", "abs": "https://arxiv.org/abs/2505.04016", "authors": ["Darren Yow-Bang Wang", "Zhengyuan Shen", "Soumya Smruti Mishra", "Zhichao Xu", "Yifei Teng", "Haibo Ding"], "title": "SLOT: Structuring the Output of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Structured outputs are essential for large language models (LLMs) in critical\napplications like agents and information extraction. Despite their\ncapabilities, LLMs often generate outputs that deviate from predefined schemas,\nsignificantly hampering reliable application development. We present SLOT\n(Structured LLM Output Transformer), a model-agnostic approach that transforms\nunstructured LLM outputs into precise structured formats. While existing\nsolutions predominantly rely on constrained decoding techniques or are tightly\ncoupled with specific models, SLOT employs a fine-tuned lightweight language\nmodel as a post-processing layer, achieving flexibility across various LLMs and\nschema specifications. We introduce a systematic pipeline for data curation and\nsynthesis alongside a formal evaluation methodology that quantifies both schema\naccuracy and content fidelity. Our results demonstrate that fine-tuned\nMistral-7B model with constrained decoding achieves near perfect schema\naccuracy (99.5%) and content similarity (94.0%), outperforming\nClaude-3.5-Sonnet by substantial margins (+25 and +20 percentage points,\nrespectively). Notably, even compact models like Llama-3.2-1B can match or\nexceed the structured output capabilities of much larger proprietary models\nwhen equipped with SLOT, enabling reliable structured generation in\nresource-constrained environments."}
{"id": "2505.04354", "pdf": "https://arxiv.org/pdf/2505.04354", "abs": "https://arxiv.org/abs/2505.04354", "authors": ["Wenhao Li", "Bo Jin", "Mingyi Hong", "Changhong Lu", "Xiangfeng Wang"], "title": "Optimization Problem Solving Can Transition to Evolutionary Agentic Workflows", "categories": ["math.OC", "cs.AI"], "comment": "27 pages, 5 figures", "summary": "This position paper argues that optimization problem solving can transition\nfrom expert-dependent to evolutionary agentic workflows. Traditional\noptimization practices rely on human specialists for problem formulation,\nalgorithm selection, and hyperparameter tuning, creating bottlenecks that\nimpede industrial adoption of cutting-edge methods. We contend that an\nevolutionary agentic workflow, powered by foundation models and evolutionary\nsearch, can autonomously navigate the optimization space, comprising problem,\nformulation, algorithm, and hyperparameter spaces. Through case studies in\ncloud resource scheduling and ADMM parameter adaptation, we demonstrate how\nthis approach can bridge the gap between academic innovation and industrial\nimplementation. Our position challenges the status quo of human-centric\noptimization workflows and advocates for a more scalable, adaptive approach to\nsolving real-world optimization problems."}
{"id": "2505.04021", "pdf": "https://arxiv.org/pdf/2505.04021", "abs": "https://arxiv.org/abs/2505.04021", "authors": ["Shan Yu", "Jiarong Xing", "Yifan Qiao", "Mingyuan Ma", "Yangmin Li", "Yang Wang", "Shuo Yang", "Zhiqiang Xie", "Shiyi Cao", "Ke Bao", "Ion Stoica", "Harry Xu", "Ying Sheng"], "title": "Prism: Unleashing GPU Sharing for Cost-Efficient Multi-LLM Serving", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PF"], "comment": null, "summary": "Serving large language models (LLMs) is expensive, especially for providers\nhosting many models, making cost reduction essential. The unique workload\npatterns of serving multiple LLMs (i.e., multi-LLM serving) create new\nopportunities and challenges for this task. The long-tail popularity of models\nand their long idle periods present opportunities to improve utilization\nthrough GPU sharing. However, existing GPU sharing systems lack the ability to\nadjust their resource allocation and sharing policies at runtime, making them\nineffective at meeting latency service-level objectives (SLOs) under rapidly\nfluctuating workloads.\n  This paper presents Prism, a multi-LLM serving system that unleashes the full\npotential of GPU sharing to achieve both cost efficiency and SLO attainment. At\nits core, Prism tackles a key limitation of existing\nsystems$\\unicode{x2014}$the lack of $\\textit{cross-model memory coordination}$,\nwhich is essential for flexibly sharing GPU memory across models under dynamic\nworkloads. Prism achieves this with two key designs. First, it supports\non-demand memory allocation by dynamically mapping physical to virtual memory\npages, allowing flexible memory redistribution among models that space- and\ntime-share a GPU. Second, it improves memory efficiency through a two-level\nscheduling policy that dynamically adjusts sharing strategies based on models'\nruntime demands. Evaluations on real-world traces show that Prism achieves more\nthan $2\\times$ cost savings and $3.3\\times$ SLO attainment compared to\nstate-of-the-art systems."}
{"id": "2505.04375", "pdf": "https://arxiv.org/pdf/2505.04375", "abs": "https://arxiv.org/abs/2505.04375", "authors": ["Moseli Mots'oehli", "Hope Mogale", "Kyungim Baek"], "title": "Balancing Accuracy, Calibration, and Efficiency in Active Learning with Vision Transformers Under Label Noise", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Fine-tuning pre-trained convolutional neural networks on ImageNet for\ndownstream tasks is well-established. Still, the impact of model size on the\nperformance of vision transformers in similar scenarios, particularly under\nlabel noise, remains largely unexplored. Given the utility and versatility of\ntransformer architectures, this study investigates their practicality under\nlow-budget constraints and noisy labels. We explore how classification accuracy\nand calibration are affected by symmetric label noise in active learning\nsettings, evaluating four vision transformer configurations (Base and Large\nwith 16x16 and 32x32 patch sizes) and three Swin Transformer configurations\n(Tiny, Small, and Base) on CIFAR10 and CIFAR100 datasets, under varying label\nnoise rates. Our findings show that larger ViT models (ViTl32 in particular)\nconsistently outperform their smaller counterparts in both accuracy and\ncalibration, even under moderate to high label noise, while Swin Transformers\nexhibit weaker robustness across all noise levels. We find that smaller patch\nsizes do not always lead to better performance, as ViTl16 performs consistently\nworse than ViTl32 while incurring a higher computational cost. We also find\nthat information-based Active Learning strategies only provide meaningful\naccuracy improvements at moderate label noise rates, but they result in poorer\ncalibration compared to models trained on randomly acquired labels, especially\nat high label noise rates. We hope these insights provide actionable guidance\nfor practitioners looking to deploy vision transformers in resource-constrained\nenvironments, where balancing model complexity, label noise, and compute\nefficiency is critical in model fine-tuning or distillation."}
{"id": "2505.04034", "pdf": "https://arxiv.org/pdf/2505.04034", "abs": "https://arxiv.org/abs/2505.04034", "authors": ["Ayana Moshruba", "Hamed Poursiami", "Maryam Parsa"], "title": "Izhikevich-Inspired Temporal Dynamics for Enhancing Privacy, Efficiency, and Transferability in Spiking Neural Networks", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": null, "summary": "Biological neurons exhibit diverse temporal spike patterns, which are\nbelieved to support efficient, robust, and adaptive neural information\nprocessing. While models such as Izhikevich can replicate a wide range of these\nfiring dynamics, their complexity poses challenges for directly integrating\nthem into scalable spiking neural networks (SNN) training pipelines. In this\nwork, we propose two probabilistically driven, input-level temporal spike\ntransformations: Poisson-Burst and Delayed-Burst that introduce biologically\ninspired temporal variability directly into standard Leaky Integrate-and-Fire\n(LIF) neurons. This enables scalable training and systematic evaluation of how\nspike timing dynamics affect privacy, generalization, and learning performance.\nPoisson-Burst modulates burst occurrence based on input intensity, while\nDelayed-Burst encodes input strength through burst onset timing. Through\nextensive experiments across multiple benchmarks, we demonstrate that\nPoisson-Burst maintains competitive accuracy and lower resource overhead while\nexhibiting enhanced privacy robustness against membership inference attacks,\nwhereas Delayed-Burst provides stronger privacy protection at a modest accuracy\ntrade-off. These findings highlight the potential of biologically grounded\ntemporal spike dynamics in improving the privacy, generalization and biological\nplausibility of neuromorphic learning systems."}
{"id": "2505.04379", "pdf": "https://arxiv.org/pdf/2505.04379", "abs": "https://arxiv.org/abs/2505.04379", "authors": ["Mohammad Elayan", "Wissam Kontar"], "title": "Consensus-Aware AV Behavior: Trade-offs Between Safety, Interaction, and Performance in Mixed Urban Traffic", "categories": ["cs.MA", "cs.AI", "cs.SY", "eess.SY"], "comment": "7 pages, 8 figures", "summary": "Transportation systems have long been shaped by complexity and heterogeneity,\ndriven by the interdependency of agent actions and traffic outcomes. The\ndeployment of automated vehicles (AVs) in such systems introduces a new\nchallenge: achieving consensus across safety, interaction quality, and traffic\nperformance. In this work, we position consensus as a fundamental property of\nthe traffic system and aim to quantify it. We use high-resolution trajectory\ndata from the Third Generation Simulation (TGSIM) dataset to empirically\nanalyze AV and human-driven vehicle (HDV) behavior at a signalized urban\nintersection and around vulnerable road users (VRUs). Key metrics, including\nTime-to-Collision (TTC), Post-Encroachment Time (PET), deceleration patterns,\nheadways, and string stability, are evaluated across the three performance\ndimensions. Results show that full consensus across safety, interaction, and\nperformance is rare, with only 1.63% of AV-VRU interaction frames meeting all\nthree conditions. These findings highlight the need for AV models that\nexplicitly balance multi-dimensional performance in mixed-traffic environments.\nFull reproducibility is supported via our open-source codebase on\nhttps://github.com/wissamkontar/Consensus-AV-Analysis."}
{"id": "2505.04037", "pdf": "https://arxiv.org/pdf/2505.04037", "abs": "https://arxiv.org/abs/2505.04037", "authors": ["Kang Liu", "Wei Peng", "Jianchen Hu"], "title": "Learning based convex approximation for constrained parametric optimization", "categories": ["math.OC", "cs.LG"], "comment": null, "summary": "We propose an input convex neural network (ICNN)-based self-supervised\nlearning framework to solve continuous constrained optimization problems. By\nintegrating the augmented Lagrangian method (ALM) with the constraint\ncorrection mechanism, our framework ensures \\emph{non-strict constraint\nfeasibility}, \\emph{better optimality gap}, and \\emph{best convergence rate}\nwith respect to the state-of-the-art learning-based methods. We provide a\nrigorous convergence analysis, showing that the algorithm converges to a\nKarush-Kuhn-Tucker (KKT) point of the original problem even when the internal\nsolver is a neural network, and the approximation error is bounded. We test our\napproach on a range of benchmark tasks including quadratic programming (QP),\nnonconvex programming, and large-scale AC optimal power flow problems. The\nresults demonstrate that compared to existing solvers (e.g., \\texttt{OSQP},\n\\texttt{IPOPT}) and the latest learning-based methods (e.g., DC3, PDL), our\napproach achieves a superior balance among accuracy, feasibility, and\ncomputational efficiency."}
{"id": "2505.04388", "pdf": "https://arxiv.org/pdf/2505.04388", "abs": "https://arxiv.org/abs/2505.04388", "authors": ["Dario Garcia-Gasulla", "Jordi Bayarri-Planas", "Ashwin Kumar Gururajan", "Enrique Lopez-Cuena", "Adrian Tormos", "Daniel Hinjos", "Pablo Bernabeu-Perez", "Anna Arias-Duart", "Pablo Agustin Martin-Torres", "Marta Gonzalez-Mallo", "Sergio Alvarez-Napagao", "Eduard Ayguadé-Parra", "Ulises Cortés"], "title": "The Aloe Family Recipe for Open and Specialized Healthcare LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2405.01886", "summary": "Purpose: With advancements in Large Language Models (LLMs) for healthcare,\nthe need arises for competitive open-source models to protect the public\ninterest. This work contributes to the field of open medical LLMs by optimizing\nkey stages of data preprocessing and training, while showing how to improve\nmodel safety (through DPO) and efficacy (through RAG). The evaluation\nmethodology used, which includes four different types of tests, defines a new\nstandard for the field. The resultant models, shown to be competitive with the\nbest private alternatives, are released with a permisive license.\n  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,\nAloe Beta uses a custom dataset to enhance public data with synthetic Chain of\nThought examples. The models undergo alignment with Direct Preference\nOptimization, emphasizing ethical and policy-aligned performance in the\npresence of jailbreaking attacks. Evaluation includes close-ended, open-ended,\nsafety and human assessments, to maximize the reliability of results.\n  Results: Recommendations are made across the entire pipeline, backed by the\nsolid performance of the Aloe Family. These models deliver competitive\nperformance across healthcare benchmarks and medical fields, and are often\npreferred by healthcare professionals. On bias and toxicity, the Aloe Beta\nmodels significantly improve safety, showing resilience to unseen jailbreaking\nattacks. For a responsible release, a detailed risk assessment specific to\nhealthcare is attached to the Aloe Family models.\n  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a\nsignificant contribution to the open-source medical LLM field, offering\ntop-of-the-line performance while maintaining high ethical requirements. This\nwork sets a new standard for developing and reporting aligned LLMs in\nhealthcare."}
{"id": "2505.04097", "pdf": "https://arxiv.org/pdf/2505.04097", "abs": "https://arxiv.org/abs/2505.04097", "authors": ["Thien Nhan Vo", "Bac Nam Ho", "Thanh Xuan Truong"], "title": "3D Brain MRI Classification for Alzheimer Diagnosis Using CNN with Data Augmentation", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "A three-dimensional convolutional neural network was developed to classify\nT1-weighted brain MRI scans as healthy or Alzheimer. The network comprises 3D\nconvolution, pooling, batch normalization, dense ReLU layers, and a sigmoid\noutput. Using stochastic noise injection and five-fold cross-validation, the\nmodel achieved test set accuracy of 0.912 and area under the ROC curve of\n0.961, an improvement of approximately 0.027 over resizing alone. Sensitivity\nand specificity both exceeded 0.90. These results align with prior work\nreporting up to 0.10 gain via synthetic augmentation. The findings demonstrate\nthe effectiveness of simple augmentation for 3D MRI classification and motivate\nfuture exploration of advanced augmentation methods and architectures such as\n3D U-Net and vision transformers."}
{"id": "2505.04397", "pdf": "https://arxiv.org/pdf/2505.04397", "abs": "https://arxiv.org/abs/2505.04397", "authors": ["Ziyuan Li", "Uwe Jaekel", "Babette Dellen"], "title": "Deep residual learning with product units", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We propose a deep product-unit residual neural network (PURe) that integrates\nproduct units into residual blocks to improve the expressiveness and parameter\nefficiency of deep convolutional networks. Unlike standard summation neurons,\nproduct units enable multiplicative feature interactions, potentially offering\na more powerful representation of complex patterns. PURe replaces conventional\nconvolutional layers with 2D product units in the second layer of each residual\nblock, eliminating nonlinear activation functions to preserve structural\ninformation. We validate PURe on three benchmark datasets. On Galaxy10 DECaLS,\nPURe34 achieves the highest test accuracy of 84.89%, surpassing the much deeper\nResNet152, while converging nearly five times faster and demonstrating strong\nrobustness to Poisson noise. On ImageNet, PURe architectures outperform\nstandard ResNet models at similar depths, with PURe34 achieving a top-1\naccuracy of 80.27% and top-5 accuracy of 95.78%, surpassing deeper ResNet\nvariants (ResNet50, ResNet101) while utilizing significantly fewer parameters\nand computational resources. On CIFAR-10, PURe consistently outperforms ResNet\nvariants across varying depths, with PURe272 reaching 95.01% test accuracy,\ncomparable to ResNet1001 but at less than half the model size. These results\ndemonstrate that PURe achieves a favorable balance between accuracy,\nefficiency, and robustness. Compared to traditional residual networks, PURe not\nonly achieves competitive classification performance with faster convergence\nand fewer parameters, but also demonstrates greater robustness to noise. Its\neffectiveness across diverse datasets highlights the potential of\nproduct-unit-based architectures for scalable and reliable deep learning in\ncomputer vision."}
{"id": "2505.04135", "pdf": "https://arxiv.org/pdf/2505.04135", "abs": "https://arxiv.org/abs/2505.04135", "authors": ["Vihaan Miriyala", "Smrithi Bukkapatnam", "Lavanya Prahallad"], "title": "Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting in Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "5 pages", "summary": "We explore the use of Chain-of-Thought (CoT) prompting with large language\nmodels (LLMs) to improve the accuracy of granular sentiment categorization in\napp store reviews. Traditional numeric and polarity-based ratings often fail to\ncapture the nuanced sentiment embedded in user feedback. We evaluated the\neffectiveness of CoT prompting versus simple prompting on 2000 Amazon app\nreviews by comparing each method's predictions to human judgements. CoT\nprompting improved classification accuracy from 84% to 93% highlighting the\nbenefit of explicit reasoning in enhancing sentiment analysis performance."}
{"id": "2505.04404", "pdf": "https://arxiv.org/pdf/2505.04404", "abs": "https://arxiv.org/abs/2505.04404", "authors": ["Jiaqi Zhu", "Shaofeng Cai", "Yanyan Shen", "Gang Chen", "Fang Deng", "Beng Chin Ooi"], "title": "In-Context Adaptation to Concept Drift for Learned Database Operations", "categories": ["cs.DB", "cs.AI"], "comment": "Accepted by ICML 2025", "summary": "Machine learning has demonstrated transformative potential for database\noperations, such as query optimization and in-database data analytics. However,\ndynamic database environments, characterized by frequent updates and evolving\ndata distributions, introduce concept drift, which leads to performance\ndegradation for learned models and limits their practical applicability.\nAddressing this challenge requires efficient frameworks capable of adapting to\nshifting concepts while minimizing the overhead of retraining or fine-tuning.\n  In this paper, we propose FLAIR, an online adaptation framework that\nintroduces a new paradigm called \\textit{in-context adaptation} for learned\ndatabase operations. FLAIR leverages the inherent property of data systems,\ni.e., immediate availability of execution results for predictions, to enable\ndynamic context construction. By formalizing adaptation as $f:(\\mathbf{x} \\,|\n\\,\\mathcal{C}_t) \\to \\mathbf{y}$, with $\\mathcal{C}_t$ representing a dynamic\ncontext memory, FLAIR delivers predictions aligned with the current concept,\neliminating the need for runtime parameter optimization. To achieve this, FLAIR\nintegrates two key modules: a Task Featurization Module for encoding\ntask-specific features into standardized representations, and a Dynamic\nDecision Engine, pre-trained via Bayesian meta-training, to adapt seamlessly\nusing contextual information at runtime. Extensive experiments across key\ndatabase tasks demonstrate that FLAIR outperforms state-of-the-art baselines,\nachieving up to 5.2x faster adaptation and reducing error by 22.5% for\ncardinality estimation."}
{"id": "2505.04150", "pdf": "https://arxiv.org/pdf/2505.04150", "abs": "https://arxiv.org/abs/2505.04150", "authors": ["Yu Yamaoka or Weng Ian Chan", "Shigeto Seno", "Soichiro Fukada", "Hideo Matsuda"], "title": "Learning from Similarity Proportion Loss for Classifying Skeletal Muscle Recovery Stages", "categories": ["cs.CV", "cs.LG"], "comment": "MICCAI2024 workshop ADSMI in Morocco (oral) [Peer-reviewed]", "summary": "Evaluating the regeneration process of damaged muscle tissue is a fundamental\nanalysis in muscle research to measure experimental effect sizes and uncover\nmechanisms behind muscle weakness due to aging and disease. The conventional\napproach to assessing muscle tissue regeneration involves whole-slide imaging\nand expert visual inspection of the recovery stages based on the morphological\ninformation of cells and fibers. There is a need to replace these tasks with\nautomated methods incorporating machine learning techniques to ensure a\nquantitative and objective analysis. Given the limited availability of fully\nlabeled data, a possible approach is Learning from Label Proportions (LLP), a\nweakly supervised learning method using class label proportions. However,\ncurrent LLP methods have two limitations: (1) they cannot adapt the feature\nextractor for muscle tissues, and (2) they treat the classes representing\nrecovery stages and cell morphological changes as nominal, resulting in the\nloss of ordinal information. To address these issues, we propose Ordinal Scale\nLearning from Similarity Proportion (OSLSP), which uses a similarity proportion\nloss derived from two bag combinations. OSLSP can update the feature extractor\nby using class proportion attention to the ordinal scale of the class. Our\nmodel with OSLSP outperforms large-scale pre-trained and fine-tuning models in\nclassification tasks of skeletal muscle recovery stages."}
{"id": "2505.04405", "pdf": "https://arxiv.org/pdf/2505.04405", "abs": "https://arxiv.org/abs/2505.04405", "authors": ["Yi Zhang", "Nikolaos Farmakidis", "Ioannis Roumpos", "Miltiadis Moralis-Pegios", "Apostolos Tsakyridis", "June Sang Lee", "Bowei Dong", "Yuhan He", "Samarth Aggarwal", "Nikolaos Pleros", "Harish Bhaskaran"], "title": "High-speed multiwavelength photonic temporal integration using silicon photonics", "categories": ["physics.optics", "cs.AI", "physics.app-ph"], "comment": null, "summary": "Optical systems have been pivotal for energy-efficient computing, performing\nhigh-speed, parallel operations in low-loss carriers. While these predominantly\nanalog optical accelerators bypass digitization to perform parallel\nfloating-point computations, scaling optical hardware to map large-vector sizes\nfor AI tasks remains challenging. Here, we overcome this limitation by\nunfolding scalar operations in time and introducing a\nphotonic-heater-in-lightpath (PHIL) unit for all-optical temporal integration.\nCounterintuitively, we exploit a slow heat dissipation process to integrate\noptical signals modulated at 50 GHz bridging the speed gap between the widely\napplied thermo-optic effects and ultrafast photonics. This architecture\nsupports optical end-to-end signal processing, eliminates inefficient\nelectro-optical conversions, and enables both linear and nonlinear operations\nwithin a unified framework. Our results demonstrate a scalable path towards\nhigh-speed photonic computing through thermally driven integration."}
{"id": "2505.04209", "pdf": "https://arxiv.org/pdf/2505.04209", "abs": "https://arxiv.org/abs/2505.04209", "authors": ["Soumik Dey", "Hansi Wu", "Binbin Li"], "title": "To Judge or not to Judge: Using LLM Judgements for Advertiser Keyphrase Relevance at eBay", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": null, "summary": "E-commerce sellers are recommended keyphrases based on their inventory on\nwhich they advertise to increase buyer engagement (clicks/sales). The relevance\nof advertiser keyphrases plays an important role in preventing the inundation\nof search systems with numerous irrelevant items that compete for attention in\nauctions, in addition to maintaining a healthy seller perception. In this work,\nwe describe the shortcomings of training Advertiser keyphrase relevance filter\nmodels on click/sales/search relevance signals and the importance of aligning\nwith human judgment, as sellers have the power to adopt or reject said\nkeyphrase recommendations. In this study, we frame Advertiser keyphrase\nrelevance as a complex interaction between 3 dynamical systems -- seller\njudgment, which influences seller adoption of our product, Advertising, which\nprovides the keyphrases to bid on, and Search, who holds the auctions for the\nsame keyphrases. This study discusses the practicalities of using human\njudgment via a case study at eBay Advertising and demonstrate that using\nLLM-as-a-judge en-masse as a scalable proxy for seller judgment to train our\nrelevance models achieves a better harmony across the three systems -- provided\nthat they are bound by a meticulous evaluation framework grounded in business\nmetrics."}
{"id": "2505.04406", "pdf": "https://arxiv.org/pdf/2505.04406", "abs": "https://arxiv.org/abs/2505.04406", "authors": ["Aidar Valeev", "Roman Garaev", "Vadim Lomshakov", "Irina Piontkovskaya", "Vladimir Ivanov", "Israel Adewuyi"], "title": "YABLoCo: Yet Another Benchmark for Long Context Code Generation", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": "Presented at LLM4Code 2025 Workshop co-located wtih ICSE 2025", "summary": "Large Language Models demonstrate the ability to solve various programming\ntasks, including code generation. Typically, the performance of LLMs is\nmeasured on benchmarks with small or medium-sized context windows of thousands\nof lines of code. At the same time, in real-world software projects,\nrepositories can span up to millions of LoC. This paper closes this gap by\ncontributing to the long context code generation benchmark (YABLoCo). The\nbenchmark featured a test set of 215 functions selected from four large\nrepositories with thousands of functions. The dataset contained metadata of\nfunctions, contexts of the functions with different levels of dependencies,\ndocstrings, functions bodies, and call graphs for each repository. This paper\npresents three key aspects of the contribution. First, the benchmark aims at\nfunction body generation in large repositories in C and C++, two languages not\ncovered by previous benchmarks. Second, the benchmark contains large\nrepositories from 200K to 2,000K LoC. Third, we contribute a scalable\nevaluation pipeline for efficient computing of the target metrics and a tool\nfor visual analysis of generated code. Overall, these three aspects allow for\nevaluating code generation in large repositories in C and C++."}
{"id": "2505.04253", "pdf": "https://arxiv.org/pdf/2505.04253", "abs": "https://arxiv.org/abs/2505.04253", "authors": ["Maria Marina", "Nikolay Ivanov", "Sergey Pletenev", "Mikhail Salnikov", "Daria Galimzianova", "Nikita Krayko", "Vasily Konovalov", "Alexander Panchenko", "Viktor Moskvoretskii"], "title": "LLM-Independent Adaptive RAG: Let the Question Speak for Itself", "categories": ["cs.CL", "cs.LG"], "comment": "11 pages, 5 figures, 2 tables", "summary": "Large Language Models~(LLMs) are prone to hallucinations, and\nRetrieval-Augmented Generation (RAG) helps mitigate this, but at a high\ncomputational cost while risking misinformation. Adaptive retrieval aims to\nretrieve only when necessary, but existing approaches rely on LLM-based\nuncertainty estimation, which remain inefficient and impractical. In this\nstudy, we introduce lightweight LLM-independent adaptive retrieval methods\nbased on external information. We investigated 27 features, organized into 7\ngroups, and their hybrid combinations. We evaluated these methods on 6 QA\ndatasets, assessing the QA performance and efficiency. The results show that\nour approach matches the performance of complex LLM-based methods while\nachieving significant efficiency gains, demonstrating the potential of external\ninformation for adaptive retrieval."}
{"id": "2505.04416", "pdf": "https://arxiv.org/pdf/2505.04416", "abs": "https://arxiv.org/abs/2505.04416", "authors": ["Xiaoyu Xu", "Minxin Du", "Qingqing Ye", "Haibo Hu"], "title": "OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "18 pages, 2 figures", "summary": "Large language models (LLMs) trained over extensive corpora risk memorizing\nsensitive, copyrighted, or toxic content. To address this, we propose\nOBLIVIATE, a robust unlearning framework that removes targeted data while\npreserving model utility. The framework follows a structured process:\nextracting target tokens, building retain sets, and fine-tuning with a tailored\nloss function comprising three components -- masking, distillation, and world\nfact. Using low-rank adapters (LoRA), it ensures efficiency without\ncompromising unlearning quality. We conduct experiments on multiple datasets,\nincluding the Harry Potter series, WMDP, and TOFU, using a comprehensive suite\nof metrics: forget quality (new document-level memorization score), model\nutility, and fluency. Results demonstrate its effectiveness in resisting\nmembership inference attacks, minimizing the impact on retained data, and\nmaintaining robustness across diverse scenarios."}
{"id": "2505.04300", "pdf": "https://arxiv.org/pdf/2505.04300", "abs": "https://arxiv.org/abs/2505.04300", "authors": ["Isabella Caranzano", "Corrado Pancotti", "Cesare Rollo", "Flavio Sartori", "Pietro Liò", "Piero Fariselli", "Tiziana Sanavia"], "title": "Sparsity is All You Need: Rethinking Biological Pathway-Informed Approaches in Deep Learning", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": null, "summary": "Biologically-informed neural networks typically leverage pathway annotations\nto enhance performance in biomedical applications. We hypothesized that the\nbenefits of pathway integration does not arise from its biological relevance,\nbut rather from the sparsity it introduces. We conducted a comprehensive\nanalysis of all relevant pathway-based neural network models for predictive\ntasks, critically evaluating each study's contributions. From this review, we\ncurated a subset of methods for which the source code was publicly available.\nThe comparison of the biologically informed state-of-the-art deep learning\nmodels and their randomized counterparts showed that models based on randomized\ninformation performed equally well as biologically informed ones across\ndifferent metrics and datasets. Notably, in 3 out of the 15 analyzed models,\nthe randomized versions even outperformed their biologically informed\ncounterparts. Moreover, pathway-informed models did not show any clear\nadvantage in interpretability, as randomized models were still able to identify\nrelevant disease biomarkers despite lacking explicit pathway information. Our\nfindings suggest that pathway annotations may be too noisy or inadequately\nexplored by current methods. Therefore, we propose a methodology that can be\napplied to different domains and can serve as a robust benchmark for\nsystematically comparing novel pathway-informed models against their randomized\ncounterparts. This approach enables researchers to rigorously determine whether\nobserved performance improvements can be attributed to biological insights."}
{"id": "2505.04419", "pdf": "https://arxiv.org/pdf/2505.04419", "abs": "https://arxiv.org/abs/2505.04419", "authors": ["Sumit Kumar", "Parampreet Singh", "Vipul Arora"], "title": "Recognizing Ornaments in Vocal Indian Art Music with Active Annotation", "categories": ["eess.AS", "cs.AI", "cs.LG"], "comment": null, "summary": "Ornamentations, embellishments, or microtonal inflections are essential to\nmelodic expression across many musical traditions, adding depth, nuance, and\nemotional impact to performances. Recognizing ornamentations in singing voices\nis key to MIR, with potential applications in music pedagogy, singer\nidentification, genre classification, and controlled singing voice generation.\nHowever, the lack of annotated datasets and specialized modeling approaches\nremains a major obstacle for progress in this research area. In this work, we\nintroduce R\\=aga Ornamentation Detection (ROD), a novel dataset comprising\nIndian classical music recordings curated by expert musicians. The dataset is\nannotated using a custom Human-in-the-Loop tool for six vocal ornaments marked\nas event-based labels. Using this dataset, we develop an ornamentation\ndetection model based on deep time-series analysis, preserving ornament\nboundaries during the chunking of long audio recordings. We conduct experiments\nusing different train-test configurations within the ROD dataset and also\nevaluate our approach on a separate, manually annotated dataset of Indian\nclassical concert recordings. Our experimental results support the superior\nperformance of our proposed approach over the baseline CRNN."}
{"id": "2505.04375", "pdf": "https://arxiv.org/pdf/2505.04375", "abs": "https://arxiv.org/abs/2505.04375", "authors": ["Moseli Mots'oehli", "Hope Mogale", "Kyungim Baek"], "title": "Balancing Accuracy, Calibration, and Efficiency in Active Learning with Vision Transformers Under Label Noise", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Fine-tuning pre-trained convolutional neural networks on ImageNet for\ndownstream tasks is well-established. Still, the impact of model size on the\nperformance of vision transformers in similar scenarios, particularly under\nlabel noise, remains largely unexplored. Given the utility and versatility of\ntransformer architectures, this study investigates their practicality under\nlow-budget constraints and noisy labels. We explore how classification accuracy\nand calibration are affected by symmetric label noise in active learning\nsettings, evaluating four vision transformer configurations (Base and Large\nwith 16x16 and 32x32 patch sizes) and three Swin Transformer configurations\n(Tiny, Small, and Base) on CIFAR10 and CIFAR100 datasets, under varying label\nnoise rates. Our findings show that larger ViT models (ViTl32 in particular)\nconsistently outperform their smaller counterparts in both accuracy and\ncalibration, even under moderate to high label noise, while Swin Transformers\nexhibit weaker robustness across all noise levels. We find that smaller patch\nsizes do not always lead to better performance, as ViTl16 performs consistently\nworse than ViTl32 while incurring a higher computational cost. We also find\nthat information-based Active Learning strategies only provide meaningful\naccuracy improvements at moderate label noise rates, but they result in poorer\ncalibration compared to models trained on randomly acquired labels, especially\nat high label noise rates. We hope these insights provide actionable guidance\nfor practitioners looking to deploy vision transformers in resource-constrained\nenvironments, where balancing model complexity, label noise, and compute\nefficiency is critical in model fine-tuning or distillation."}
{"id": "2505.04435", "pdf": "https://arxiv.org/pdf/2505.04435", "abs": "https://arxiv.org/abs/2505.04435", "authors": ["Vahideh Hayyolalam", "Öznur Özkasap"], "title": "FedBWO: Enhancing Communication Efficiency in Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": "5th IEEE International Conference on Human-Machine Systems, Abu\n  Dhabi, UAE, 26-28 May 2025", "summary": "Federated Learning (FL) is a distributed Machine Learning (ML) setup, where a\nshared model is collaboratively trained by various clients using their local\ndatasets while keeping the data private. Considering resource-constrained\ndevices, FL clients often suffer from restricted transmission capacity. Aiming\nto enhance the system performance, the communication between clients and server\nneeds to be diminished. Current FL strategies transmit a tremendous amount of\ndata (model weights) within the FL process, which needs a high communication\nbandwidth. Considering resource constraints, increasing the number of clients\nand, consequently, the amount of data (model weights) can lead to a bottleneck.\nIn this paper, we introduce the Federated Black Widow Optimization (FedBWO)\ntechnique to decrease the amount of transmitted data by transmitting only a\nperformance score rather than the local model weights from clients. FedBWO\nemploys the BWO algorithm to improve local model updates. The conducted\nexperiments prove that FedBWO remarkably improves the performance of the global\nmodel and the communication efficiency of the overall system. According to the\nexperimental outcomes, FedBWO enhances the global model accuracy by an average\nof 21% over FedAvg, and 12% over FedGWO. Furthermore, FedBWO dramatically\ndecreases the communication cost compared to other methods."}
{"id": "2505.04382", "pdf": "https://arxiv.org/pdf/2505.04382", "abs": "https://arxiv.org/abs/2505.04382", "authors": ["Anton Selitskiy", "Maitreya Kocharekar"], "title": "Discrete Optimal Transport and Voice Conversion", "categories": ["eess.AS", "cs.LG"], "comment": "4 pages, 6 figures, 1 table", "summary": "In this work, we address the voice conversion (VC) task using a vector-based\ninterface. To align audio embeddings between speakers, we employ discrete\noptimal transport mapping. Our evaluation results demonstrate the high quality\nand effectiveness of this method. Additionally, we show that applying discrete\noptimal transport as a post-processing step in audio generation can lead to the\nincorrect classification of synthetic audio as real."}
{"id": "2505.04451", "pdf": "https://arxiv.org/pdf/2505.04451", "abs": "https://arxiv.org/abs/2505.04451", "authors": ["Yohannis Telila", "Tommaso Cucinotta", "Davide Bacciu"], "title": "Automatic Music Transcription using Convolutional Neural Networks and Constant-Q transform", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "comment": "6 pages", "summary": "Automatic music transcription (AMT) is the problem of analyzing an audio\nrecording of a musical piece and detecting notes that are being played. AMT is\na challenging problem, particularly when it comes to polyphonic music. The goal\nof AMT is to produce a score representation of a music piece, by analyzing a\nsound signal containing multiple notes played simultaneously. In this work, we\ndesign a processing pipeline that can transform classical piano audio files in\n.wav format into a music score representation. The features from the audio\nsignals are extracted using the constant-Q transform, and the resulting\ncoefficients are used as an input to the convolutional neural network (CNN)\nmodel."}
{"id": "2505.04397", "pdf": "https://arxiv.org/pdf/2505.04397", "abs": "https://arxiv.org/abs/2505.04397", "authors": ["Ziyuan Li", "Uwe Jaekel", "Babette Dellen"], "title": "Deep residual learning with product units", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We propose a deep product-unit residual neural network (PURe) that integrates\nproduct units into residual blocks to improve the expressiveness and parameter\nefficiency of deep convolutional networks. Unlike standard summation neurons,\nproduct units enable multiplicative feature interactions, potentially offering\na more powerful representation of complex patterns. PURe replaces conventional\nconvolutional layers with 2D product units in the second layer of each residual\nblock, eliminating nonlinear activation functions to preserve structural\ninformation. We validate PURe on three benchmark datasets. On Galaxy10 DECaLS,\nPURe34 achieves the highest test accuracy of 84.89%, surpassing the much deeper\nResNet152, while converging nearly five times faster and demonstrating strong\nrobustness to Poisson noise. On ImageNet, PURe architectures outperform\nstandard ResNet models at similar depths, with PURe34 achieving a top-1\naccuracy of 80.27% and top-5 accuracy of 95.78%, surpassing deeper ResNet\nvariants (ResNet50, ResNet101) while utilizing significantly fewer parameters\nand computational resources. On CIFAR-10, PURe consistently outperforms ResNet\nvariants across varying depths, with PURe272 reaching 95.01% test accuracy,\ncomparable to ResNet1001 but at less than half the model size. These results\ndemonstrate that PURe achieves a favorable balance between accuracy,\nefficiency, and robustness. Compared to traditional residual networks, PURe not\nonly achieves competitive classification performance with faster convergence\nand fewer parameters, but also demonstrates greater robustness to noise. Its\neffectiveness across diverse datasets highlights the potential of\nproduct-unit-based architectures for scalable and reliable deep learning in\ncomputer vision."}
{"id": "2505.04461", "pdf": "https://arxiv.org/pdf/2505.04461", "abs": "https://arxiv.org/abs/2505.04461", "authors": ["Pengfei Jiao", "Hongjiang Chen", "Xuan Guo", "Zhidong Zhao", "Dongxiao He", "Di Jin"], "title": "A Survey on Temporal Interaction Graph Representation Learning: Progress, Challenges, and Opportunities", "categories": ["cs.LG", "cs.AI", "cs.SI"], "comment": "IJCAI 2025 Survey Track", "summary": "Temporal interaction graphs (TIGs), defined by sequences of timestamped\ninteraction events, have become ubiquitous in real-world applications due to\ntheir capability to model complex dynamic system behaviors. As a result,\ntemporal interaction graph representation learning (TIGRL) has garnered\nsignificant attention in recent years. TIGRL aims to embed nodes in TIGs into\nlow-dimensional representations that effectively preserve both structural and\ntemporal information, thereby enhancing the performance of downstream tasks\nsuch as classification, prediction, and clustering within constantly evolving\ndata environments. In this paper, we begin by introducing the foundational\nconcepts of TIGs and emphasize the critical role of temporal dependencies. We\nthen propose a comprehensive taxonomy of state-of-the-art TIGRL methods,\nsystematically categorizing them based on the types of information utilized\nduring the learning process to address the unique challenges inherent to TIGs.\nTo facilitate further research and practical applications, we curate the source\nof datasets and benchmarks, providing valuable resources for empirical\ninvestigations. Finally, we examine key open challenges and explore promising\nresearch directions in TIGRL, laying the groundwork for future advancements\nthat have the potential to shape the evolution of this field."}
{"id": "2505.04401", "pdf": "https://arxiv.org/pdf/2505.04401", "abs": "https://arxiv.org/abs/2505.04401", "authors": ["Wei Wang", "Peizheng Li", "Angela Doufexi", "Mark A. Beach"], "title": "A Heuristic-Integrated DRL Approach for Phase Optimization in Large-Scale RISs", "categories": ["eess.SP", "cs.LG"], "comment": "5 pages, 5 figures. This work has been accepted for publication in\n  IEEE Communications Letters", "summary": "Optimizing discrete phase shifts in large-scale reconfigurable intelligent\nsurfaces (RISs) is challenging due to their non-convex and non-linear nature.\nIn this letter, we propose a heuristic-integrated deep reinforcement learning\n(DRL) framework that (1) leverages accumulated actions over multiple steps in\nthe double deep Q-network (DDQN) for RIS column-wise control and (2) integrates\na greedy algorithm (GA) into each DRL step to refine the state via\nfine-grained, element-wise optimization of RIS configurations. By learning from\nGA-included states, the proposed approach effectively addresses RIS\noptimization within a small DRL action space, demonstrating its capability to\noptimize phase-shift configurations of large-scale RISs."}
{"id": "2505.04464", "pdf": "https://arxiv.org/pdf/2505.04464", "abs": "https://arxiv.org/abs/2505.04464", "authors": ["Louis Ohl", "Fredrik Lindsten"], "title": "Discriminative Ordering Through Ensemble Consensus", "categories": ["cs.LG", "cs.AI", "62H30", "G.3"], "comment": "Accepted at UAI 2025", "summary": "Evaluating the performance of clustering models is a challenging task where\nthe outcome depends on the definition of what constitutes a cluster. Due to\nthis design, current existing metrics rarely handle multiple clustering models\nwith diverse cluster definitions, nor do they comply with the integration of\nconstraints when available. In this work, we take inspiration from consensus\nclustering and assume that a set of clustering models is able to uncover hidden\nstructures in the data. We propose to construct a discriminative ordering\nthrough ensemble clustering based on the distance between the connectivity of a\nclustering model and the consensus matrix. We first validate the proposed\nmethod with synthetic scenarios, highlighting that the proposed score ranks the\nmodels that best match the consensus first. We then show that this simple\nranking score significantly outperforms other scoring methods when comparing\nsets of different clustering algorithms that are not restricted to a fixed\nnumber of clusters and is compatible with clustering constraints."}
{"id": "2505.04416", "pdf": "https://arxiv.org/pdf/2505.04416", "abs": "https://arxiv.org/abs/2505.04416", "authors": ["Xiaoyu Xu", "Minxin Du", "Qingqing Ye", "Haibo Hu"], "title": "OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "18 pages, 2 figures", "summary": "Large language models (LLMs) trained over extensive corpora risk memorizing\nsensitive, copyrighted, or toxic content. To address this, we propose\nOBLIVIATE, a robust unlearning framework that removes targeted data while\npreserving model utility. The framework follows a structured process:\nextracting target tokens, building retain sets, and fine-tuning with a tailored\nloss function comprising three components -- masking, distillation, and world\nfact. Using low-rank adapters (LoRA), it ensures efficiency without\ncompromising unlearning quality. We conduct experiments on multiple datasets,\nincluding the Harry Potter series, WMDP, and TOFU, using a comprehensive suite\nof metrics: forget quality (new document-level memorization score), model\nutility, and fluency. Results demonstrate its effectiveness in resisting\nmembership inference attacks, minimizing the impact on retained data, and\nmaintaining robustness across diverse scenarios."}
{"id": "2505.04468", "pdf": "https://arxiv.org/pdf/2505.04468", "abs": "https://arxiv.org/abs/2505.04468", "authors": ["Hyeju Shin", "Kyudan Jung", "Seongwon Yun", "Juyoung Yun"], "title": "Spectral and Temporal Denoising for Differentially Private Optimization", "categories": ["cs.LG", "cs.AI", "cs.IT", "cs.NE", "math.IT"], "comment": null, "summary": "This paper introduces the FFT-Enhanced Kalman Filter (FFTKF), a\ndifferentially private optimization method that addresses the challenge of\npreserving performance in DP-SGD, where added noise typically degrades model\nutility. FFTKF integrates frequency-domain noise shaping with Kalman filtering\nto enhance gradient quality while preserving $(\\varepsilon, \\delta)$-DP\nguarantees. It employs a high-frequency shaping mask in the Fourier domain to\nconcentrate differential privacy noise in less informative spectral components,\npreserving low-frequency gradient signals. A scalar-gain Kalman filter with\nfinite-difference Hessian approximation further refines the denoised gradients.\nWith a per-iteration complexity of $\\mathcal{O}(d \\log d)$, FFTKF demonstrates\nimproved test accuracy over DP-SGD and DiSK across MNIST, CIFAR-10, CIFAR-100,\nand Tiny-ImageNet datasets using CNNs, Wide ResNets, and Vision Transformers.\nTheoretical analysis confirms that FFTKF maintains equivalent privacy\nguarantees while achieving a tighter privacy-utility trade-off through reduced\nnoise and controlled bias."}
{"id": "2505.04419", "pdf": "https://arxiv.org/pdf/2505.04419", "abs": "https://arxiv.org/abs/2505.04419", "authors": ["Sumit Kumar", "Parampreet Singh", "Vipul Arora"], "title": "Recognizing Ornaments in Vocal Indian Art Music with Active Annotation", "categories": ["eess.AS", "cs.AI", "cs.LG"], "comment": null, "summary": "Ornamentations, embellishments, or microtonal inflections are essential to\nmelodic expression across many musical traditions, adding depth, nuance, and\nemotional impact to performances. Recognizing ornamentations in singing voices\nis key to MIR, with potential applications in music pedagogy, singer\nidentification, genre classification, and controlled singing voice generation.\nHowever, the lack of annotated datasets and specialized modeling approaches\nremains a major obstacle for progress in this research area. In this work, we\nintroduce R\\=aga Ornamentation Detection (ROD), a novel dataset comprising\nIndian classical music recordings curated by expert musicians. The dataset is\nannotated using a custom Human-in-the-Loop tool for six vocal ornaments marked\nas event-based labels. Using this dataset, we develop an ornamentation\ndetection model based on deep time-series analysis, preserving ornament\nboundaries during the chunking of long audio recordings. We conduct experiments\nusing different train-test configurations within the ROD dataset and also\nevaluate our approach on a separate, manually annotated dataset of Indian\nclassical concert recordings. Our experimental results support the superior\nperformance of our proposed approach over the baseline CRNN."}
{"id": "2505.04486", "pdf": "https://arxiv.org/pdf/2505.04486", "abs": "https://arxiv.org/abs/2505.04486", "authors": ["Anirban Samaddar", "Yixuan Sun", "Viktor Nilsson", "Sandeep Madireddy"], "title": "Efficient Flow Matching using Latent Variables", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Flow matching models have shown great potential in image generation tasks\namong probabilistic generative models. Building upon the ideas of continuous\nnormalizing flows, flow matching models generalize the transport path of the\ndiffusion models from a simple prior distribution to the data. Most flow\nmatching models in the literature do not explicitly model the underlying\nstructure/manifold in the target data when learning the flow from a simple\nsource distribution like the standard Gaussian. This leads to inefficient\nlearning, especially for many high-dimensional real-world datasets, which often\nreside in a low-dimensional manifold. Existing strategies of incorporating\nmanifolds, including data with underlying multi-modal distribution, often\nrequire expensive training and hence frequently lead to suboptimal performance.\nTo this end, we present \\texttt{Latent-CFM}, which provides simplified\ntraining/inference strategies to incorporate multi-modal data structures using\npretrained deep latent variable models. Through experiments on multi-modal\nsynthetic data and widely used image benchmark datasets, we show that\n\\texttt{Latent-CFM} exhibits improved generation quality with significantly\nless training ($\\sim 50\\%$ less in some cases) and computation than\nstate-of-the-art flow matching models. Using a 2d Darcy flow dataset, we\ndemonstrate that our approach generates more physically accurate samples than\ncompetitive approaches. In addition, through latent space analysis, we\ndemonstrate that our approach can be used for conditional image generation\nconditioned on latent features."}
{"id": "2505.04451", "pdf": "https://arxiv.org/pdf/2505.04451", "abs": "https://arxiv.org/abs/2505.04451", "authors": ["Yohannis Telila", "Tommaso Cucinotta", "Davide Bacciu"], "title": "Automatic Music Transcription using Convolutional Neural Networks and Constant-Q transform", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "comment": "6 pages", "summary": "Automatic music transcription (AMT) is the problem of analyzing an audio\nrecording of a musical piece and detecting notes that are being played. AMT is\na challenging problem, particularly when it comes to polyphonic music. The goal\nof AMT is to produce a score representation of a music piece, by analyzing a\nsound signal containing multiple notes played simultaneously. In this work, we\ndesign a processing pipeline that can transform classical piano audio files in\n.wav format into a music score representation. The features from the audio\nsignals are extracted using the constant-Q transform, and the resulting\ncoefficients are used as an input to the convolutional neural network (CNN)\nmodel."}
{"id": "2505.04488", "pdf": "https://arxiv.org/pdf/2505.04488", "abs": "https://arxiv.org/abs/2505.04488", "authors": ["Ziyi Zhang", "Zhen Sun", "Zongmin Zhang", "Zifan Peng", "Yuemeng Zhao", "Zichun Wang", "Zeren Luo", "Ruiting Zuo", "Xinlei He"], "title": "\"I Can See Forever!\": Evaluating Real-time VideoLLMs for Assisting Individuals with Visual Impairments", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.MM"], "comment": "12 pages, 6 figures", "summary": "The visually impaired population, especially the severely visually impaired,\nis currently large in scale, and daily activities pose significant challenges\nfor them. Although many studies use large language and vision-language models\nto assist the blind, most focus on static content and fail to meet real-time\nperception needs in dynamic and complex environments, such as daily activities.\nTo provide them with more effective intelligent assistance, it is imperative to\nincorporate advanced visual understanding technologies. Although real-time\nvision and speech interaction VideoLLMs demonstrate strong real-time visual\nunderstanding, no prior work has systematically evaluated their effectiveness\nin assisting visually impaired individuals. In this work, we conduct the first\nsuch evaluation. First, we construct a benchmark dataset (VisAssistDaily),\ncovering three categories of assistive tasks for visually impaired individuals:\nBasic Skills, Home Life Tasks, and Social Life Tasks. The results show that\nGPT-4o achieves the highest task success rate. Next, we conduct a user study to\nevaluate the models in both closed-world and open-world scenarios, further\nexploring the practical challenges of applying VideoLLMs in assistive contexts.\nOne key issue we identify is the difficulty current models face in perceiving\npotential hazards in dynamic environments. To address this, we build an\nenvironment-awareness dataset named SafeVid and introduce a polling mechanism\nthat enables the model to proactively detect environmental risks. We hope this\nwork provides valuable insights and inspiration for future research in this\nfield."}
{"id": "2505.04484", "pdf": "https://arxiv.org/pdf/2505.04484", "abs": "https://arxiv.org/abs/2505.04484", "authors": ["Louis Ohl", "Pierre-Alexandre Mattei", "Frédéric Precioso"], "title": "A Tutorial on Discriminative Clustering and Mutual Information", "categories": ["stat.ML", "cs.LG", "62H30", "G.3"], "comment": null, "summary": "To cluster data is to separate samples into distinctive groups that should\nideally have some cohesive properties. Today, numerous clustering algorithms\nexist, and their differences lie essentially in what can be perceived as\n``cohesive properties''. Therefore, hypotheses on the nature of clusters must\nbe set: they can be either generative or discriminative. As the last decade\nwitnessed the impressive growth of deep clustering methods that involve neural\nnetworks to handle high-dimensional data often in a discriminative manner; we\nconcentrate mainly on the discriminative hypotheses. In this paper, our aim is\nto provide an accessible historical perspective on the evolution of\ndiscriminative clustering methods and notably how the nature of assumptions of\nthe discriminative models changed over time: from decision boundaries to\ninvariance critics. We notably highlight how mutual information has been a\nhistorical cornerstone of the progress of (deep) discriminative clustering\nmethods. We also show some known limitations of mutual information and how\ndiscriminative clustering methods tried to circumvent those. We then discuss\nthe challenges that discriminative clustering faces with respect to the\nselection of the number of clusters. Finally, we showcase these techniques\nusing the dedicated Python package, GemClus, that we have developed for\ndiscriminative clustering."}
{"id": "2505.04493", "pdf": "https://arxiv.org/pdf/2505.04493", "abs": "https://arxiv.org/abs/2505.04493", "authors": ["Or Wertheim", "Ronen I. Brafman"], "title": "Model-Based AI planning and Execution Systems for Robotics", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Model-based planning and execution systems offer a principled approach to\nbuilding flexible autonomous robots that can perform diverse tasks by\nautomatically combining a host of basic skills. This idea is almost as old as\nmodern robotics. Yet, while diverse general-purpose reasoning architectures\nhave been proposed since, general-purpose systems that are integrated with\nmodern robotic platforms have emerged only recently, starting with the\ninfluential ROSPlan system. Since then, a growing number of model-based systems\nfor robot task-level control have emerged. In this paper, we consider the\ndiverse design choices and issues existing systems attempt to address, the\ndifferent solutions proposed so far, and suggest avenues for future\ndevelopment."}
{"id": "2505.04486", "pdf": "https://arxiv.org/pdf/2505.04486", "abs": "https://arxiv.org/abs/2505.04486", "authors": ["Anirban Samaddar", "Yixuan Sun", "Viktor Nilsson", "Sandeep Madireddy"], "title": "Efficient Flow Matching using Latent Variables", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Flow matching models have shown great potential in image generation tasks\namong probabilistic generative models. Building upon the ideas of continuous\nnormalizing flows, flow matching models generalize the transport path of the\ndiffusion models from a simple prior distribution to the data. Most flow\nmatching models in the literature do not explicitly model the underlying\nstructure/manifold in the target data when learning the flow from a simple\nsource distribution like the standard Gaussian. This leads to inefficient\nlearning, especially for many high-dimensional real-world datasets, which often\nreside in a low-dimensional manifold. Existing strategies of incorporating\nmanifolds, including data with underlying multi-modal distribution, often\nrequire expensive training and hence frequently lead to suboptimal performance.\nTo this end, we present \\texttt{Latent-CFM}, which provides simplified\ntraining/inference strategies to incorporate multi-modal data structures using\npretrained deep latent variable models. Through experiments on multi-modal\nsynthetic data and widely used image benchmark datasets, we show that\n\\texttt{Latent-CFM} exhibits improved generation quality with significantly\nless training ($\\sim 50\\%$ less in some cases) and computation than\nstate-of-the-art flow matching models. Using a 2d Darcy flow dataset, we\ndemonstrate that our approach generates more physically accurate samples than\ncompetitive approaches. In addition, through latent space analysis, we\ndemonstrate that our approach can be used for conditional image generation\nconditioned on latent features."}
{"id": "2505.04497", "pdf": "https://arxiv.org/pdf/2505.04497", "abs": "https://arxiv.org/abs/2505.04497", "authors": ["Aditi Ramaswamy"], "title": "Defining and Quantifying Creative Behavior in Popular Image Generators", "categories": ["cs.CV", "cs.AI", "I.4.m; I.2.m"], "comment": null, "summary": "Creativity of generative AI models has been a subject of scientific debate in\nthe last years, without a conclusive answer. In this paper, we study creativity\nfrom a practical perspective and introduce quantitative measures that help the\nuser to choose a suitable AI model for a given task. We evaluated our measures\non a number of popular image-to-image generation models, and the results of\nthis suggest that our measures conform to human intuition."}
{"id": "2505.04494", "pdf": "https://arxiv.org/pdf/2505.04494", "abs": "https://arxiv.org/abs/2505.04494", "authors": ["Axel Friedrich Wolter", "Tobias Sutter"], "title": "A Two-Timescale Primal-Dual Framework for Reinforcement Learning via Online Dual Variable Guidance", "categories": ["math.OC", "cs.LG"], "comment": "35 pages, 1 figure", "summary": "We study reinforcement learning by combining recent advances in regularized\nlinear programming formulations with the classical theory of stochastic\napproximation. Motivated by the challenge of designing algorithms that leverage\noff-policy data while maintaining on-policy exploration, we propose PGDA-RL, a\nnovel primal-dual Projected Gradient Descent-Ascent algorithm for solving\nregularized Markov Decision Processes (MDPs). PGDA-RL integrates experience\nreplay-based gradient estimation with a two-timescale decomposition of the\nunderlying nested optimization problem. The algorithm operates asynchronously,\ninteracts with the environment through a single trajectory of correlated data,\nand updates its policy online in response to the dual variable associated with\nthe occupation measure of the underlying MDP. We prove that PGDA-RL converges\nalmost surely to the optimal value function and policy of the regularized MDP.\nOur convergence analysis relies on tools from stochastic approximation theory\nand holds under weaker assumptions than those required by existing primal-dual\nRL approaches, notably removing the need for a simulator or a fixed behavioral\npolicy."}
{"id": "2505.04526", "pdf": "https://arxiv.org/pdf/2505.04526", "abs": "https://arxiv.org/abs/2505.04526", "authors": ["Qi Zhou", "Yukai Shi", "Xiaojun Yang", "Xiaoyu Xian", "Lunjia Liao", "Ruimao Zhang", "Liang Lin"], "title": "DFVO: Learning Darkness-free Visible and Infrared Image Disentanglement and Fusion All at Once", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visible and infrared image fusion is one of the most crucial tasks in the\nfield of image fusion, aiming to generate fused images with clear structural\ninformation and high-quality texture features for high-level vision tasks.\nHowever, when faced with severe illumination degradation in visible images, the\nfusion results of existing image fusion methods often exhibit blurry and dim\nvisual effects, posing major challenges for autonomous driving. To this end, a\nDarkness-Free network is proposed to handle Visible and infrared image\ndisentanglement and fusion all at Once (DFVO), which employs a cascaded\nmulti-task approach to replace the traditional two-stage cascaded training\n(enhancement and fusion), addressing the issue of information entropy loss\ncaused by hierarchical data transmission. Specifically, we construct a\nlatent-common feature extractor (LCFE) to obtain latent features for the\ncascaded tasks strategy. Firstly, a details-extraction module (DEM) is devised\nto acquire high-frequency semantic information. Secondly, we design a hyper\ncross-attention module (HCAM) to extract low-frequency information and preserve\ntexture features from source images. Finally, a relevant loss function is\ndesigned to guide the holistic network learning, thereby achieving better image\nfusion. Extensive experiments demonstrate that our proposed approach\noutperforms state-of-the-art alternatives in terms of qualitative and\nquantitative evaluations. Particularly, DFVO can generate clearer, more\ninformative, and more evenly illuminated fusion results in the dark\nenvironments, achieving best performance on the LLVIP dataset with 63.258 dB\nPSNR and 0.724 CC, providing more effective information for high-level vision\ntasks. Our code is publicly accessible at https://github.com/DaVin-Qi530/DFVO."}
{"id": "2505.04524", "pdf": "https://arxiv.org/pdf/2505.04524", "abs": "https://arxiv.org/abs/2505.04524", "authors": ["Asma Baobaid", "Mahmoud Meribout"], "title": "Edge-GPU Based Face Tracking for Face Detection and Recognition Acceleration", "categories": ["cs.CV", "cs.AR", "cs.LG", "eess.IV"], "comment": "10 pages, 12 figures", "summary": "Cost-effective machine vision systems dedicated to real-time and accurate\nface detection and recognition in public places are crucial for many modern\napplications. However, despite their high performance, which could be reached\nusing specialized edge or cloud AI hardware accelerators, there is still room\nfor improvement in throughput and power consumption. This paper aims to suggest\na combined hardware-software approach that optimizes face detection and\nrecognition systems on one of the latest edge GPUs, namely NVIDIA Jetson AGX\nOrin. First, it leverages the simultaneous usage of all its hardware engines to\nimprove processing time. This offers an improvement over previous works where\nthese tasks were mainly allocated automatically and exclusively to the CPU or,\nto a higher extent, to the GPU core. Additionally, the paper suggests\nintegrating a face tracker module to avoid redundantly running the face\nrecognition algorithm for every frame but only when a new face appears in the\nscene. The results of extended experiments suggest that simultaneous usage of\nall the hardware engines that are available in the Orin GPU and tracker\nintegration into the pipeline yield an impressive throughput of 290 FPS (frames\nper second) on 1920 x 1080 input size frames containing in average of 6\nfaces/frame. Additionally, a substantial saving of power consumption of around\n800 mW was achieved when compared to running the task on the CPU/GPU engines\nonly and without integrating a tracker into the Orin GPU\\'92s pipeline. This\nhardware-codesign approach can pave the way to design high-performance machine\nvision systems at the edge, critically needed in video monitoring in public\nplaces where several nearby cameras are usually deployed for a same scene."}
{"id": "2505.04531", "pdf": "https://arxiv.org/pdf/2505.04531", "abs": "https://arxiv.org/abs/2505.04531", "authors": ["Josh McGiff", "Nikola S. Nikolov"], "title": "Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review", "categories": ["cs.CL", "cs.AI"], "comment": "This work is currently under review. Please do not cite without\n  permission", "summary": "Generative language modelling has surged in popularity with the emergence of\nservices such as ChatGPT and Google Gemini. While these models have\ndemonstrated transformative potential in productivity and communication, they\noverwhelmingly cater to high-resource languages like English. This has\namplified concerns over linguistic inequality in natural language processing\n(NLP). This paper presents the first systematic review focused specifically on\nstrategies to address data scarcity in generative language modelling for\nlow-resource languages (LRL). Drawing from 54 studies, we identify, categorise\nand evaluate technical approaches, including monolingual data augmentation,\nback-translation, multilingual training, and prompt engineering, across\ngenerative tasks. We also analyse trends in architecture choices, language\nfamily representation, and evaluation methods. Our findings highlight a strong\nreliance on transformer-based models, a concentration on a small subset of\nLRLs, and a lack of consistent evaluation across studies. We conclude with\nrecommendations for extending these methods to a wider range of LRLs and\noutline open challenges in building equitable generative language systems.\nUltimately, this review aims to support researchers and developers in building\ninclusive AI tools for underrepresented languages, a necessary step toward\nempowering LRL speakers and the preservation of linguistic diversity in a world\nincreasingly shaped by large-scale language technologies."}
{"id": "2505.04575", "pdf": "https://arxiv.org/pdf/2505.04575", "abs": "https://arxiv.org/abs/2505.04575", "authors": ["Kunlun Xu", "Xu Zou", "Gang Hua", "Jiahuan Zhou"], "title": "Componential Prompt-Knowledge Alignment for Domain Incremental Learning", "categories": ["cs.CV", "cs.LG"], "comment": "Accpted by ICML2025", "summary": "Domain Incremental Learning (DIL) aims to learn from non-stationary data\nstreams across domains while retaining and utilizing past knowledge. Although\nprompt-based methods effectively store multi-domain knowledge in prompt\nparameters and obtain advanced performance through cross-domain prompt fusion,\nwe reveal an intrinsic limitation: component-wise misalignment between\ndomain-specific prompts leads to conflicting knowledge integration and degraded\npredictions. This arises from the random positioning of knowledge components\nwithin prompts, where irrelevant component fusion introduces interference.To\naddress this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a\nnovel prompt-based DIL method that introduces component-aware prompt-knowledge\nalignment during training, significantly improving both the learning and\ninference capacity of the model. KA-Prompt operates in two phases: (1) Initial\nComponential Structure Configuring, where a set of old prompts containing\nknowledge relevant to the new domain are mined via greedy search, which is then\nexploited to initialize new prompts to achieve reusable knowledge transfer and\nestablish intrinsic alignment between new and old prompts. (2) Online Alignment\nPreservation, which dynamically identifies the target old prompts and applies\nadaptive componential consistency constraints as new prompts evolve. Extensive\nexperiments on DIL benchmarks demonstrate the effectiveness of our KA-Prompt.\nOur source code is available at\nhttps://github.com/zhoujiahuan1991/ICML2025-KA-Prompt"}
{"id": "2505.04553", "pdf": "https://arxiv.org/pdf/2505.04553", "abs": "https://arxiv.org/abs/2505.04553", "authors": ["Shanyu Han", "Yang Liu", "Xiang Yu"], "title": "Risk-sensitive Reinforcement Learning Based on Convex Scoring Functions", "categories": ["q-fin.MF", "cs.AI", "q-fin.RM"], "comment": "35 pages", "summary": "We propose a reinforcement learning (RL) framework under a broad class of\nrisk objectives, characterized by convex scoring functions. This class covers\nmany common risk measures, such as variance, Expected Shortfall, entropic\nValue-at-Risk, and mean-risk utility. To resolve the time-inconsistency issue,\nwe consider an augmented state space and an auxiliary variable and recast the\nproblem as a two-state optimization problem. We propose a customized\nActor-Critic algorithm and establish some theoretical approximation guarantees.\nA key theoretical contribution is that our results do not require the Markov\ndecision process to be continuous. Additionally, we propose an auxiliary\nvariable sampling method inspired by the alternating minimization algorithm,\nwhich is convergent under certain conditions. We validate our approach in\nsimulation experiments with a financial application in statistical arbitrage\ntrading, demonstrating the effectiveness of the algorithm."}
{"id": "2505.04579", "pdf": "https://arxiv.org/pdf/2505.04579", "abs": "https://arxiv.org/abs/2505.04579", "authors": ["Stéphane Aroca-Ouellette", "Miguel Aroca-Ouellette", "Katharina von der Wense", "Alessandro Roncone"], "title": "Implicitly Aligning Humans and Autonomous Agents through Shared Task Abstractions", "categories": ["cs.MA", "cs.LG"], "comment": "9 pages (7 paper + 2 references). To be published in IJCAI 2025", "summary": "In collaborative tasks, autonomous agents fall short of humans in their\ncapability to quickly adapt to new and unfamiliar teammates. We posit that a\nlimiting factor for zero-shot coordination is the lack of shared task\nabstractions, a mechanism humans rely on to implicitly align with teammates. To\naddress this gap, we introduce HA$^2$: Hierarchical Ad Hoc Agents, a framework\nleveraging hierarchical reinforcement learning to mimic the structured approach\nhumans use in collaboration. We evaluate HA$^2$ in the Overcooked environment,\ndemonstrating statistically significant improvement over existing baselines\nwhen paired with both unseen agents and humans, providing better resilience to\nenvironmental shifts, and outperforming all state-of-the-art methods."}
{"id": "2505.04558", "pdf": "https://arxiv.org/pdf/2505.04558", "abs": "https://arxiv.org/abs/2505.04558", "authors": ["Wenzhao Liu", "Haoran Li", "Congying Han", "Zicheng Zhang", "Anqi Li", "Tiande Guo"], "title": "Purity Law for Generalizable Neural TSP Solvers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Achieving generalization in neural approaches across different scales and\ndistributions remains a significant challenge for the Traveling Salesman\nProblem~(TSP). A key obstacle is that neural networks often fail to learn\nrobust principles for identifying universal patterns and deriving optimal\nsolutions from diverse instances. In this paper, we first uncover Purity Law\n(PuLa), a fundamental structural principle for optimal TSP solutions, defining\nthat edge prevalence grows exponentially with the sparsity of surrounding\nvertices. Statistically validated across diverse instances, PuLa reveals a\nconsistent bias toward local sparsity in global optima. Building on this\ninsight, we propose Purity Policy Optimization~(PUPO), a novel training\nparadigm that explicitly aligns characteristics of neural solutions with PuLa\nduring the solution construction process to enhance generalization. Extensive\nexperiments demonstrate that PUPO can be seamlessly integrated with popular\nneural solvers, significantly enhancing their generalization performance\nwithout incurring additional computational overhead during inference."}
{"id": "2505.04583", "pdf": "https://arxiv.org/pdf/2505.04583", "abs": "https://arxiv.org/abs/2505.04583", "authors": ["Nathaniel Dennler", "Zhonghao Shi", "Uksang Yoo", "Stefanos Nikolaidis", "Maja Matarić"], "title": "Modeling Personalized Difficulty of Rehabilitation Exercises Using Causal Trees", "categories": ["cs.RO", "cs.LG"], "comment": "Accepted to IEEE/RAS-EMBS International Conference on Rehabilitation\n  Robotics (ICORR 2025)", "summary": "Rehabilitation robots are often used in game-like interactions for\nrehabilitation to increase a person's motivation to complete rehabilitation\nexercises. By adjusting exercise difficulty for a specific user throughout the\nexercise interaction, robots can maximize both the user's rehabilitation\noutcomes and the their motivation throughout the exercise. Previous approaches\nhave assumed exercises have generic difficulty values that apply to all users\nequally, however, we identified that stroke survivors have varied and unique\nperceptions of exercise difficulty. For example, some stroke survivors found\nreaching vertically more difficult than reaching farther but lower while others\nfound reaching farther more challenging than reaching vertically. In this\npaper, we formulate a causal tree-based method to calculate exercise difficulty\nbased on the user's performance. We find that this approach accurately models\nexercise difficulty and provides a readily interpretable model of why that\nexercise is difficult for both users and caretakers."}
{"id": "2505.04578", "pdf": "https://arxiv.org/pdf/2505.04578", "abs": "https://arxiv.org/abs/2505.04578", "authors": ["Wenjun Cao"], "title": "Fight Fire with Fire: Defending Against Malicious RL Fine-Tuning via Reward Neutralization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) fine-tuning transforms large language models\nwhile creating a vulnerability we experimentally verify: Our experiment shows\nthat malicious RL fine-tuning dismantles safety guardrails with remarkable\nefficiency, requiring only 50 steps and minimal adversarial prompts, with\nharmful escalating from 0-2 to 7-9. This attack vector particularly threatens\nopen-source models with parameter-level access. Existing defenses targeting\nsupervised fine-tuning prove ineffective against RL's dynamic feedback\nmechanisms. We introduce Reward Neutralization, the first defense framework\nspecifically designed against RL fine-tuning attacks, establishing concise\nrejection patterns that render malicious reward signals ineffective. Our\napproach trains models to produce minimal-information rejections that attackers\ncannot exploit, systematically neutralizing attempts to optimize toward harmful\noutputs. Experiments validate that our approach maintains low harmful scores\n(no greater than 2) after 200 attack steps, while standard models rapidly\ndeteriorate. This work provides the first constructive proof that robust\ndefense against increasingly accessible RL attacks is achievable, addressing a\ncritical security gap for open-weight models."}
{"id": "2505.04586", "pdf": "https://arxiv.org/pdf/2505.04586", "abs": "https://arxiv.org/abs/2505.04586", "authors": ["Yuning Du", "Jingshuai Liu", "Rohan Dharmakumar", "Sotirios A. Tsaftaris"], "title": "Active Sampling for MRI-based Sequential Decision Making", "categories": ["cs.CV", "cs.LG"], "comment": "Under Review", "summary": "Despite the superior diagnostic capability of Magnetic Resonance Imaging\n(MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and\ncomplexity. To enable such a future by reducing the magnetic field strength,\none key approach will be to improve sampling strategies. Previous work has\nshown that it is possible to make diagnostic decisions directly from k-space\nwith fewer samples. Such work shows that single diagnostic decisions can be\nmade, but if we aspire to see MRI as a true PoC, multiple and sequential\ndecisions are necessary while minimizing the number of samples acquired. We\npresent a novel multi-objective reinforcement learning framework enabling\ncomprehensive, sequential, diagnostic evaluation from undersampled k-space\ndata. Our approach during inference actively adapts to sequential decisions to\noptimally sample. To achieve this, we introduce a training methodology that\nidentifies the samples that contribute the best to each diagnostic objective\nusing a step-wise weighting reward function. We evaluate our approach in two\nsequential knee pathology assessment tasks: ACL sprain detection and cartilage\nthickness loss assessment. Our framework achieves diagnostic performance\ncompetitive with various policy-based benchmarks on disease detection, severity\nquantification, and overall sequential diagnosis, while substantially saving\nk-space samples. Our approach paves the way for the future of MRI as a\ncomprehensive and affordable PoC device. Our code is publicly available at\nhttps://github.com/vios-s/MRI_Sequential_Active_Sampling"}
{"id": "2505.04592", "pdf": "https://arxiv.org/pdf/2505.04592", "abs": "https://arxiv.org/abs/2505.04592", "authors": ["Peter Barnett", "Aaron Scher"], "title": "AI Governance to Avoid Extinction: The Strategic Landscape and Actionable Research Questions", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Humanity appears to be on course to soon develop AI systems that\nsubstantially outperform human experts in all cognitive domains and activities.\nWe believe the default trajectory has a high likelihood of catastrophe,\nincluding human extinction. Risks come from failure to control powerful AI\nsystems, misuse of AI by malicious rogue actors, war between great powers, and\nauthoritarian lock-in. This research agenda has two aims: to describe the\nstrategic landscape of AI development and to catalog important governance\nresearch questions. These questions, if answered, would provide important\ninsight on how to successfully reduce catastrophic risks.\n  We describe four high-level scenarios for the geopolitical response to\nadvanced AI development, cataloging the research questions most relevant to\neach. Our favored scenario involves building the technical, legal, and\ninstitutional infrastructure required to internationally restrict dangerous AI\ndevelopment and deployment (which we refer to as an Off Switch), which leads\ninto an internationally coordinated Halt on frontier AI activities at some\npoint in the future. The second scenario we describe is a US National Project\nfor AI, in which the US Government races to develop advanced AI systems and\nestablish unilateral control over global AI development. We also describe two\nadditional scenarios: a Light-Touch world similar to that of today and a Threat\nof Sabotage situation where countries use sabotage and deterrence to slow AI\ndevelopment.\n  In our view, apart from the Off Switch and Halt scenario, all of these\ntrajectories appear to carry an unacceptable risk of catastrophic harm. Urgent\naction is needed from the US National Security community and AI governance\necosystem to answer key research questions, build the capability to halt\ndangerous AI activities, and prepare for international AI agreements."}
{"id": "2505.04603", "pdf": "https://arxiv.org/pdf/2505.04603", "abs": "https://arxiv.org/abs/2505.04603", "authors": ["Wenhui Sophia Lu", "Wing Hung Wong"], "title": "Likelihood-Free Adaptive Bayesian Inference via Nonparametric Distribution Matching", "categories": ["stat.ME", "cs.LG", "stat.CO", "stat.ML"], "comment": null, "summary": "When the likelihood is analytically unavailable and computationally\nintractable, approximate Bayesian computation (ABC) has emerged as a widely\nused methodology for approximate posterior inference; however, it suffers from\nsevere computational inefficiency in high-dimensional settings or under diffuse\npriors. To overcome these limitations, we propose Adaptive Bayesian Inference\n(ABI), a framework that bypasses traditional data-space discrepancies and\ninstead compares distributions directly in posterior space through\nnonparametric distribution matching. By leveraging a novel Marginally-augmented\nSliced Wasserstein (MSW) distance on posterior measures and exploiting its\nquantile representation, ABI transforms the challenging problem of measuring\ndivergence between posterior distributions into a tractable sequence of\none-dimensional conditional quantile regression tasks. Moreover, we introduce a\nnew adaptive rejection sampling scheme that iteratively refines the posterior\napproximation by updating the proposal distribution via generative density\nestimation. Theoretically, we establish parametric convergence rates for the\ntrimmed MSW distance and prove that the ABI posterior converges to the true\nposterior as the tolerance threshold vanishes. Through extensive empirical\nevaluation, we demonstrate that ABI significantly outperforms data-based\nWasserstein ABC, summary-based ABC, and state-of-the-art likelihood-free\nsimulators, especially in high-dimensional or dependent observation regimes."}
{"id": "2505.04608", "pdf": "https://arxiv.org/pdf/2505.04608", "abs": "https://arxiv.org/abs/2505.04608", "authors": ["Drew Prinster", "Xing Han", "Anqi Liu", "Suchi Saria"], "title": "WATCH: Weighted Adaptive Testing for Changepoint Hypotheses via Weighted-Conformal Martingales", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "To be published in The International Conference on Machine Learning\n  (ICML), 2025", "summary": "Responsibly deploying artificial intelligence (AI) / machine learning (ML)\nsystems in high-stakes settings arguably requires not only proof of system\nreliability, but moreover continual, post-deployment monitoring to quickly\ndetect and address any unsafe behavior. Statistical methods for nonparametric\nchange-point detection -- especially the tools of conformal test martingales\n(CTMs) and anytime-valid inference -- offer promising approaches to this\nmonitoring task. However, existing methods are restricted to monitoring limited\nhypothesis classes or ``alarm criteria,'' such as data shifts that violate\ncertain exchangeability assumptions, or do not allow for online adaptation in\nresponse to shifts. In this paper, we expand the scope of these monitoring\nmethods by proposing a weighted generalization of conformal test martingales\n(WCTMs), which lay a theoretical foundation for online monitoring for any\nunexpected changepoints in the data distribution while controlling\nfalse-alarms. For practical applications, we propose specific WCTM algorithms\nthat accommodate online adaptation to mild covariate shifts (in the marginal\ninput distribution) while raising alarms in response to more severe shifts,\nsuch as concept shifts (in the conditional label distribution) or extreme\n(out-of-support) covariate shifts that cannot be easily adapted to. On\nreal-world datasets, we demonstrate improved performance relative to\nstate-of-the-art baselines."}
{"id": "2505.04613", "pdf": "https://arxiv.org/pdf/2505.04613", "abs": "https://arxiv.org/abs/2505.04613", "authors": ["Leonardo V. Santoro", "Kartik G. Waghmare", "Victor M. Panaretos"], "title": "From Two Sample Testing to Singular Gaussian Discrimination", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH", "62G10, 46E22, 60G15"], "comment": null, "summary": "We establish that testing for the equality of two probability measures on a\ngeneral separable and compact metric space is equivalent to testing for the\nsingularity between two corresponding Gaussian measures on a suitable\nReproducing Kernel Hilbert Space. The corresponding Gaussians are defined via\nthe notion of kernel mean and covariance embedding of a probability measure.\nDiscerning two singular Gaussians is fundamentally simpler from an\ninformation-theoretic perspective than non-parametric two-sample testing,\nparticularly in high-dimensional settings. Our proof leverages the\nFeldman-Hajek criterion for singularity/equivalence of Gaussians on Hilbert\nspaces, and shows that discrepancies between distributions are heavily\nmagnified through their corresponding Gaussian embeddings: at a population\nlevel, distinct probability measures lead to essentially separated Gaussian\nembeddings. This appears to be a new instance of the blessing of dimensionality\nthat can be harnessed for the design of efficient inference tools in great\ngenerality."}
{"id": "2505.04621", "pdf": "https://arxiv.org/pdf/2505.04621", "abs": "https://arxiv.org/abs/2505.04621", "authors": ["Jessie Richter-Powell", "Antonio Torralba", "Jonathan Lorraine"], "title": "Score Distillation Sampling for Audio: Source Separation, Synthesis, and Beyond", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS", "68T07", "I.2.6; H.5.5; H.5.1"], "comment": "See the project website at\n  https://research.nvidia.com/labs/toronto-ai/Audio-SDS/", "summary": "We introduce Audio-SDS, a generalization of Score Distillation Sampling (SDS)\nto text-conditioned audio diffusion models. While SDS was initially designed\nfor text-to-3D generation using image diffusion, its core idea of distilling a\npowerful generative prior into a separate parametric representation extends to\nthe audio domain. Leveraging a single pretrained model, Audio-SDS enables a\nbroad range of tasks without requiring specialized datasets. In particular, we\ndemonstrate how Audio-SDS can guide physically informed impact sound\nsimulations, calibrate FM-synthesis parameters, and perform prompt-specified\nsource separation. Our findings illustrate the versatility of\ndistillation-based methods across modalities and establish a robust foundation\nfor future work using generative priors in audio tasks."}
{"id": "2505.04621", "pdf": "https://arxiv.org/pdf/2505.04621", "abs": "https://arxiv.org/abs/2505.04621", "authors": ["Jessie Richter-Powell", "Antonio Torralba", "Jonathan Lorraine"], "title": "Score Distillation Sampling for Audio: Source Separation, Synthesis, and Beyond", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS", "68T07", "I.2.6; H.5.5; H.5.1"], "comment": "See the project website at\n  https://research.nvidia.com/labs/toronto-ai/Audio-SDS/", "summary": "We introduce Audio-SDS, a generalization of Score Distillation Sampling (SDS)\nto text-conditioned audio diffusion models. While SDS was initially designed\nfor text-to-3D generation using image diffusion, its core idea of distilling a\npowerful generative prior into a separate parametric representation extends to\nthe audio domain. Leveraging a single pretrained model, Audio-SDS enables a\nbroad range of tasks without requiring specialized datasets. In particular, we\ndemonstrate how Audio-SDS can guide physically informed impact sound\nsimulations, calibrate FM-synthesis parameters, and perform prompt-specified\nsource separation. Our findings illustrate the versatility of\ndistillation-based methods across modalities and establish a robust foundation\nfor future work using generative priors in audio tasks."}
{"id": "2505.04623", "pdf": "https://arxiv.org/pdf/2505.04623", "abs": "https://arxiv.org/abs/2505.04623", "authors": ["Zhenghao Xing", "Xiaowei Hu", "Chi-Wing Fu", "Wenhai Wang", "Jifeng Dai", "Pheng-Ann Heng"], "title": "EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.MM", "cs.SD"], "comment": null, "summary": "Multimodal large language models (MLLMs) have advanced perception across\ntext, vision, and audio, yet they often struggle with structured cross-modal\nreasoning, particularly when integrating audio and visual signals. We introduce\nEchoInk-R1, a reinforcement learning framework that enhances such reasoning in\nMLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group\nRelative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice\nquestion answering over synchronized audio-image pairs. To enable this, we\ncurate AVQA-R1-6K, a dataset pairing such audio-image inputs with\nmultiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves\n85.77% accuracy on the validation set, outperforming the base model, which\nscores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy,\nEchoInk-R1 demonstrates reflective reasoning by revisiting initial\ninterpretations and refining responses when facing ambiguous multimodal inputs.\nThese results suggest that lightweight reinforcement learning fine-tuning\nenhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to\nunify audio, visual, and textual modalities for general open-world reasoning\nvia reinforcement learning. Code and data are publicly released to facilitate\nfurther research."}
{"id": "2505.04627", "pdf": "https://arxiv.org/pdf/2505.04627", "abs": "https://arxiv.org/abs/2505.04627", "authors": ["Jean-Michel Tucny", "Mihir Durve", "Sauro Succi"], "title": "Is the end of Insight in Sight ?", "categories": ["physics.comp-ph", "cs.LG", "physics.data-an"], "comment": "20 pages, 5 figures", "summary": "It is shown that the weight matrices of a Physics-informed neural network\n(PINN)-based deep learning application to a rarefied gas dynamics problem\ndescribed by the Boltzmann equation bear no evident link to the mathematical\nstructure of the physical problem. Instead, the weights appear close to\nGaussian distributed random matrices. Although significantly more work is\nneeded to support a robust assessment in this direction, these results suggest\nthat deep-learning and the numerical solution of the Boltzmann equation\nrepresent two equivalent, but largely distinct paths to the same physical\nknowledge. If so, Explainable AI might be an unrealistic target and possibly\neven an ill-posed one."}
