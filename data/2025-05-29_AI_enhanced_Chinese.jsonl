{"id": "2505.21523", "pdf": "https://arxiv.org/pdf/2505.21523", "abs": "https://arxiv.org/abs/2505.21523", "authors": ["Chengzhi Liu", "Zhongxing Xu", "Qingyue Wei", "Juncheng Wu", "James Zou", "Xin Eric Wang", "Yuyin Zhou", "Sheng Liu"], "title": "More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Test-time compute has empowered multimodal large language models to generate\nextended reasoning chains, yielding strong performance on tasks such as\nmultimodal math reasoning. However, this improved reasoning ability often comes\nwith increased hallucination: as generations become longer, models tend to\ndrift away from image-grounded content and rely more heavily on language\npriors. Attention analysis shows that longer reasoning chains lead to reduced\nfocus on visual inputs, which contributes to hallucination. To systematically\nstudy this phenomenon, we introduce RH-AUC, a metric that quantifies how a\nmodel's perception accuracy changes with reasoning length, allowing us to\nevaluate whether the model preserves visual grounding during reasoning. We also\nrelease RH-Bench, a diagnostic benchmark that spans a variety of multimodal\ntasks, designed to assess the trade-off between reasoning ability and\nhallucination. Our analysis reveals that (i) larger models typically achieve a\nbetter balance between reasoning and perception, and (ii) this balance is\ninfluenced more by the types and domains of training data than by its overall\nvolume. These findings underscore the importance of evaluation frameworks that\njointly consider both reasoning quality and perceptual fidelity.", "AI": {"tldr": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u589e\u52a0\u63a8\u7406\u94fe\u957f\u5ea6\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u4e5f\u5bfc\u81f4\u5e7b\u89c9\u589e\u52a0\uff0c\u5373\u6a21\u578b\u66f4\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\u800c\u975e\u89c6\u89c9\u8f93\u5165\u3002\u7814\u7a76\u63d0\u51faRH-AUC\u6307\u6807\u548cRH-Bench\u57fa\u51c6\uff0c\u5206\u6790\u63a8\u7406\u80fd\u529b\u4e0e\u611f\u77e5\u4fdd\u771f\u5ea6\u7684\u5e73\u8861\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u63a8\u7406\u94fe\u957f\u5ea6\u589e\u52a0\u5bfc\u81f4\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5373\u6a21\u578b\u9010\u6e10\u5ffd\u89c6\u89c6\u89c9\u8f93\u5165\u800c\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u63d0\u51faRH-AUC\u6307\u6807\u91cf\u5316\u611f\u77e5\u51c6\u786e\u6027\u968f\u63a8\u7406\u957f\u5ea6\u7684\u53d8\u5316\uff0c\u5e76\u5f00\u53d1RH-Bench\u57fa\u51c6\u8bc4\u4f30\u63a8\u7406\u80fd\u529b\u4e0e\u5e7b\u89c9\u7684\u6743\u8861\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u66f4\u5927\u6a21\u578b\u901a\u5e38\u5728\u63a8\u7406\u4e0e\u611f\u77e5\u95f4\u53d6\u5f97\u66f4\u597d\u5e73\u8861\uff0c\u4e14\u5e73\u8861\u66f4\u591a\u53d7\u8bad\u7ec3\u6570\u636e\u7c7b\u578b\u548c\u9886\u57df\u5f71\u54cd\u800c\u975e\u6570\u636e\u91cf\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u9700\u8981\u8054\u5408\u8bc4\u4f30\u63a8\u7406\u8d28\u91cf\u4e0e\u611f\u77e5\u4fdd\u771f\u5ea6\u7684\u6846\u67b6\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b,\u5e7b\u89c9,\u63a8\u7406\u94fe,\u611f\u77e5\u4fdd\u771f\u5ea6,RH-AUC,RH-Bench"}}
{"id": "2505.21578", "pdf": "https://arxiv.org/pdf/2505.21578", "abs": "https://arxiv.org/abs/2505.21578", "authors": ["Titouan Parcollet", "Yuan Tseng", "Shucong Zhang", "Rogier van Dalen"], "title": "Loquacious Set: 25,000 Hours of Transcribed and Diverse English Speech Recognition Data for Research and Commercial Use", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Automatic speech recognition (ASR) research is driven by the availability of\ncommon datasets between industrial researchers and academics, encouraging\ncomparisons and evaluations. LibriSpeech, despite its long success as an ASR\nbenchmark, is now limited by its size and focus on clean, read speech, leading\nto near-zero word error rates. More recent datasets, including MOSEL, YODAS,\nGigaspeech, OWSM, Libriheavy or People's Speech suffer from major limitations\nincluding licenses that researchers in the industry cannot use, unreliable\ntranscriptions, incorrect audio data, or the lack of evaluation sets. This work\npresents the Loquacious Set, a 25,000-hour curated collection of commercially\nusable English speech. Featuring hundreds of thousands of speakers with diverse\naccents and a wide range of speech types (read, spontaneous, talks, clean,\nnoisy), the Loquacious Set is designed to work for academics and researchers in\nthe industry to build ASR systems in real-world scenarios.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Loquacious Set\uff0c\u4e00\u4e2a25,000\u5c0f\u65f6\u7684\u5546\u4e1a\u53ef\u7528\u82f1\u8bed\u8bed\u97f3\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709ASR\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\uff0c\u5982\u89c4\u6a21\u5c0f\u3001\u8bb8\u53ef\u95ee\u9898\u6216\u6570\u636e\u8d28\u91cf\u95ee\u9898\u3002", "motivation": "\u73b0\u6709ASR\u6570\u636e\u96c6\uff08\u5982LibriSpeech\uff09\u5728\u89c4\u6a21\u3001\u591a\u6837\u6027\u548c\u5546\u4e1a\u53ef\u7528\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u7684\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u6536\u96c6\u548c\u6574\u740625,000\u5c0f\u65f6\u7684\u591a\u6837\u5316\u82f1\u8bed\u8bed\u97f3\u6570\u636e\uff08\u5305\u62ec\u4e0d\u540c\u53e3\u97f3\u548c\u8bed\u97f3\u7c7b\u578b\uff09\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aLoquacious Set\u7684\u65b0\u6570\u636e\u96c6\u3002", "result": "Loquacious Set\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u9650\u5236\uff0c\u63d0\u4f9b\u4e86\u5546\u4e1a\u53ef\u7528\u3001\u591a\u6837\u5316\u7684\u8bed\u97f3\u6570\u636e\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u573a\u666f\u4e0b\u7684ASR\u7cfb\u7edf\u5f00\u53d1\u3002", "conclusion": "Loquacious Set\u4e3aASR\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u4e86\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u7684\u5408\u4f5c\u4e0e\u8fdb\u6b65\u3002", "keywords": "\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b, ASR, Loquacious Set, \u591a\u6837\u5316\u8bed\u97f3\u6570\u636e, \u5546\u4e1a\u53ef\u7528"}}
{"id": "2505.21598", "pdf": "https://arxiv.org/pdf/2505.21598", "abs": "https://arxiv.org/abs/2505.21598", "authors": ["Yajiao Liu", "Congliang Chen", "Junchi Yang", "Ruoyu Sun"], "title": "Rethinking Data Mixture for Large Language Models: A Comprehensive Survey and New Perspectives", "categories": ["cs.CL"], "comment": "The first version of this paper was submitted to ACL ARR 2025\n  February Submission", "summary": "Training large language models with data collected from various domains can\nimprove their performance on downstream tasks. However, given a fixed training\nbudget, the sampling proportions of these different domains significantly\nimpact the model's performance. How can we determine the domain weights across\ndifferent data domains to train the best-performing model within constrained\ncomputational resources? In this paper, we provide a comprehensive overview of\nexisting data mixture methods. First, we propose a fine-grained categorization\nof existing methods, extending beyond the previous offline and online\nclassification. Offline methods are further grouped into heuristic-based,\nalgorithm-based, and function fitting-based methods. For online methods, we\ncategorize them into three groups: online min-max optimization, online mixing\nlaw, and other approaches by drawing connections with the optimization\nframeworks underlying offline methods. Second, we summarize the problem\nformulations, representative algorithms for each subtype of offline and online\nmethods, and clarify the relationships and distinctions among them. Finally, we\ndiscuss the advantages and disadvantages of each method and highlight key\nchallenges in the field of data mixture.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u4e0d\u540c\u6570\u636e\u9886\u57df\u91c7\u6837\u6bd4\u4f8b\u5bf9\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u73b0\u6709\u6570\u636e\u6df7\u5408\u65b9\u6cd5\u7684\u7ec6\u7c92\u5ea6\u5206\u7c7b\uff0c\u5e76\u603b\u7ed3\u4e86\u5404\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u53ca\u9886\u57df\u5173\u952e\u6311\u6218\u3002", "motivation": "\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\uff0c\u5982\u4f55\u786e\u5b9a\u4e0d\u540c\u6570\u636e\u9886\u57df\u7684\u6743\u91cd\u4ee5\u8bad\u7ec3\u51fa\u6027\u80fd\u6700\u4f18\u7684\u6a21\u578b\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u73b0\u6709\u65b9\u6cd5\u5e76\u660e\u786e\u5176\u5206\u7c7b\u4e0e\u8054\u7cfb\u3002", "method": "\u8bba\u6587\u9996\u5148\u5bf9\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u7c7b\uff08\u79bb\u7ebf\u65b9\u6cd5\u5206\u4e3a\u542f\u53d1\u5f0f\u3001\u7b97\u6cd5\u5f0f\u548c\u51fd\u6570\u62df\u5408\u5f0f\uff0c\u5728\u7ebf\u65b9\u6cd5\u5206\u4e3a\u5728\u7ebf\u6781\u5c0f\u6781\u5927\u4f18\u5316\u3001\u5728\u7ebf\u6df7\u5408\u5b9a\u5f8b\u7b49\uff09\uff0c\u5e76\u603b\u7ed3\u5404\u5b50\u7c7b\u7684\u516c\u5f0f\u53ca\u4ee3\u8868\u6027\u7b97\u6cd5\u3002", "result": "\u660e\u786e\u4e86\u4e0d\u540c\u7c7b\u578b\u65b9\u6cd5\u7684\u5173\u7cfb\u4e0e\u533a\u522b\uff0c\u68b3\u7406\u4e86\u5404\u81ea\u4f18\u7f3a\u70b9\uff0c\u4e3a\u6570\u636e\u6df7\u5408\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u53c2\u8003\u3002", "conclusion": "\u6570\u636e\u6df7\u5408\u65b9\u6cd5\u4ecd\u9700\u89e3\u51b3\u5173\u952e\u6311\u6218\uff0c\u5982\u52a8\u6001\u8c03\u6574\u6743\u91cd\u548c\u8de8\u9886\u57df\u6cdb\u5316\uff0c\u672a\u6765\u7814\u7a76\u53ef\u7ed3\u5408\u79bb\u7ebf\u4e0e\u5728\u7ebf\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "keywords": "\u6570\u636e\u6df7\u5408\u3001\u9886\u57df\u6743\u91cd\u3001\u8bed\u8a00\u6a21\u578b\u3001\u79bb\u7ebf\u65b9\u6cd5\u3001\u5728\u7ebf\u65b9\u6cd5"}}
{"id": "2505.21600", "pdf": "https://arxiv.org/pdf/2505.21600", "abs": "https://arxiv.org/abs/2505.21600", "authors": ["Tianyu Fu", "Yi Ge", "Yichen You", "Enshu Liu", "Zhihang Yuan", "Guohao Dai", "Shengen Yan", "Huazhong Yang", "Yu Wang"], "title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the\ncost of substantial inference overhead, posing substantial deployment\nchallenges. Although distilled Small Language Models (SLMs) significantly\nenhance efficiency, their performance suffers as they fail to follow LLMs'\nreasoning paths. Luckily, we reveal that only a small fraction of tokens\ngenuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens\nare either identical or exhibit neutral differences, such as minor variations\nin abbreviations or expressions. Leveraging this insight, we introduce **Roads\nto Rome (R2R)**, a neural token routing method that selectively utilizes LLMs\nonly for these critical, path-divergent tokens, while leaving the majority of\ntoken generation to the SLM. We also develop an automatic data generation\npipeline that identifies divergent tokens and generates token-level routing\nlabels to train the lightweight router. We apply R2R to combine R1-1.5B and\nR1-32B models from the DeepSeek family, and evaluate on challenging math,\ncoding, and QA benchmarks. With an average activated parameter size of 5.6B,\nR2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the\nR1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with\ncomparable performance, advancing the Pareto frontier of test-time scaling\nefficiency. Our code is available at https://github.com/thu-nics/R2R.", "AI": {"tldr": "R2R introduces a neural token routing method to selectively use LLMs for critical tokens, improving efficiency while maintaining performance.", "motivation": "To address the high inference overhead of LLMs while preserving reasoning capabilities, by leveraging the observation that only a few tokens significantly differ between LLMs and SLMs.", "method": "Developed Roads to Rome (R2R), a neural token routing method that utilizes LLMs only for path-divergent tokens, and an automatic data generation pipeline for training the router.", "result": "R2R, with an average activated parameter size of 5.6B, outperformed R1-7B by 1.6x and achieved a 2.8x speedup compared to R1-32B with comparable performance.", "conclusion": "R2R effectively balances efficiency and performance, advancing the Pareto frontier of test-time scaling efficiency.", "keywords": "Large Language Models, Small Language Models, token routing, efficiency, performance"}}
{"id": "2505.21512", "pdf": "https://arxiv.org/pdf/2505.21512", "abs": "https://arxiv.org/abs/2505.21512", "authors": ["Harry Li", "Gabriel Appleby", "Kenneth Alperin", "Steven R Gomez", "Ashley Suh"], "title": "The Role of Visualization in LLM-Assisted Knowledge Graph Systems: Effects on User Trust, Exploration, and Workflows", "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Knowledge graphs (KGs) are powerful data structures, but exploring them\neffectively remains difficult for even expert users. Large language models\n(LLMs) are increasingly used to address this gap, yet little is known\nempirically about how their usage with KGs shapes user trust, exploration\nstrategies, or downstream decision-making - raising key design challenges for\nLLM-based KG visual analysis systems. To study these effects, we developed\nLinkQ, a KG exploration system that converts natural language questions into\nstructured queries with an LLM. We collaborated with KG experts to design five\nvisual mechanisms that help users assess the accuracy of both KG queries and\nLLM responses: an LLM-KG state diagram that illustrates which stage of the\nexploration pipeline LinkQ is in, a query editor displaying the generated query\npaired with an LLM explanation, an entity-relation ID table showing extracted\nKG entities and relations with semantic descriptions, a query structure graph\nthat depicts the path traversed in the KG, and an interactive graph\nvisualization of query results. From a qualitative evaluation with 14\npractitioners, we found that users - even KG experts - tended to overtrust\nLinkQ's outputs due to its \"helpful\" visualizations, even when the LLM was\nincorrect. Users exhibited distinct workflows depending on their prior\nfamiliarity with KGs and LLMs, challenging the assumption that these systems\nare one-size-fits-all - despite often being designed as if they are. Our\nfindings highlight the risks of false trust in LLM-assisted data analysis tools\nand the need for further investigation into the role of visualization as a\nmitigation technique.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u7ed3\u5408\u4f7f\u7528\u65f6\u5bf9\u7528\u6237\u4fe1\u4efb\u548c\u51b3\u7b56\u7684\u5f71\u54cd\uff0c\u4ecb\u7ecd\u4e86LinkQ\u7cfb\u7edf\u53ca\u4e94\u79cd\u89c6\u89c9\u673a\u5236\uff0c\u901a\u8fc7\u5b9a\u6027\u8bc4\u4f30\u53d1\u73b0\u7528\u6237\u503e\u5411\u4e8e\u8fc7\u5ea6\u4fe1\u4efbLLM\u8f93\u51fa\uff0c\u63d0\u51fa\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76\u53ef\u89c6\u5316\u4f5c\u4e3a\u7f13\u89e3\u6280\u672f\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u7814\u7a76LLM\u4e0eKG\u7ed3\u5408\u4f7f\u7528\u65f6\u5982\u4f55\u5f71\u54cd\u7528\u6237\u4fe1\u4efb\u3001\u63a2\u7d22\u7b56\u7565\u548c\u51b3\u7b56\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u5e76\u89e3\u51b3LLM\u8f85\u52a9KG\u89c6\u89c9\u5206\u6790\u7cfb\u7edf\u7684\u8bbe\u8ba1\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86LinkQ\u7cfb\u7edf\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u67e5\u8be2\uff0c\u5e76\u901a\u8fc7\u4e94\u79cd\u89c6\u89c9\u673a\u5236\u5e2e\u52a9\u7528\u6237\u8bc4\u4f30\u67e5\u8be2\u548cLLM\u54cd\u5e94\u7684\u51c6\u786e\u6027\u3002\u4e0e14\u4f4d\u4e13\u5bb6\u8fdb\u884c\u5b9a\u6027\u8bc4\u4f30\u3002", "result": "\u7528\u6237\u503e\u5411\u4e8e\u8fc7\u5ea6\u4fe1\u4efbLinkQ\u7684\u8f93\u51fa\uff0c\u5373\u4f7fLLM\u56de\u7b54\u9519\u8bef\uff1b\u7528\u6237\u5de5\u4f5c\u6d41\u56e0KG\u548cLLM\u7684\u719f\u6089\u5ea6\u800c\u5f02\uff0c\u8868\u660e\u7cfb\u7edf\u5e76\u975e\u901a\u7528\u3002", "conclusion": "LLM\u8f85\u52a9\u6570\u636e\u5206\u6790\u5de5\u5177\u5b58\u5728\u8bef\u4fe1\u98ce\u9669\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u53ef\u89c6\u5316\u6280\u672f\u4f5c\u4e3a\u7f13\u89e3\u624b\u6bb5\u3002", "keywords": "knowledge graphs, large language models, visualization, trust, decision-making"}}
{"id": "2505.21552", "pdf": "https://arxiv.org/pdf/2505.21552", "abs": "https://arxiv.org/abs/2505.21552", "authors": ["Diogo Cruz"], "title": "Understanding the learned look-ahead behavior of chess neural networks", "categories": ["cs.AI", "cs.LG"], "comment": "40 pages, 47 figures", "summary": "We investigate the look-ahead capabilities of chess-playing neural networks,\nspecifically focusing on the Leela Chess Zero policy network. We build on the\nwork of Jenner et al. (2024) by analyzing the model's ability to consider\nfuture moves and alternative sequences beyond the immediate next move. Our\nfindings reveal that the network's look-ahead behavior is highly\ncontext-dependent, varying significantly based on the specific chess position.\nWe demonstrate that the model can process information about board states up to\nseven moves ahead, utilizing similar internal mechanisms across different\nfuture time steps. Additionally, we provide evidence that the network considers\nmultiple possible move sequences rather than focusing on a single line of play.\nThese results offer new insights into the emergence of sophisticated look-ahead\ncapabilities in neural networks trained on strategic tasks, contributing to our\nunderstanding of AI reasoning in complex domains. Our work also showcases the\neffectiveness of interpretability techniques in uncovering cognitive-like\nprocesses in artificial intelligence systems.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u56fd\u9645\u8c61\u68cb\u795e\u7ecf\u7f51\u7edc\u7684\u524d\u77bb\u80fd\u529b\uff0c\u7279\u522b\u662fLeela Chess Zero\u7b56\u7565\u7f51\u7edc\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5728\u4e0d\u540c\u68cb\u5c40\u4e0b\u5176\u524d\u77bb\u884c\u4e3a\u5dee\u5f02\u663e\u8457\uff0c\u6700\u591a\u53ef\u9884\u6d4b\u4e03\u6b65\u68cb\uff0c\u5e76\u540c\u65f6\u8003\u8651\u591a\u79cd\u8d70\u6cd5\u5e8f\u5217\u3002", "motivation": "\u6df1\u5165\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u5728\u56fd\u9645\u8c61\u68cb\u7b49\u6218\u7565\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u7684\u524d\u77bb\u80fd\u529b\uff0c\u63ed\u793a\u5176\u5185\u90e8\u5de5\u4f5c\u673a\u5236\uff0c\u4ee5\u63a8\u52a8\u5bf9AI\u590d\u6742\u63a8\u7406\u80fd\u529b\u7684\u8ba4\u77e5\u3002", "method": "\u57fa\u4e8eJenner et al. (2024)\u7684\u7814\u7a76\uff0c\u5206\u6790\u6a21\u578b\u5bf9\u672a\u6765\u8d70\u6cd5\u53ca\u66ff\u4ee3\u5e8f\u5217\u7684\u5904\u7406\u80fd\u529b\uff0c\u91c7\u7528\u53ef\u89e3\u91ca\u6027\u6280\u672f\u89e3\u6790\u5176\u5185\u90e8\u8ba4\u77e5\u8fc7\u7a0b\u3002", "result": "\u7f51\u7edc\u7684\u524d\u77bb\u80fd\u529b\u9ad8\u5ea6\u4f9d\u8d56\u68cb\u5c40\u4e0a\u4e0b\u6587\uff0c\u6700\u591a\u80fd\u9884\u6d4b\u4e03\u6b65\u68cb\uff0c\u4e14\u540c\u65f6\u8bc4\u4f30\u591a\u79cd\u8d70\u6cd5\u5e8f\u5217\uff0c\u5c55\u73b0\u51fa\u7c7b\u4f3c\u8ba4\u77e5\u7684\u5185\u90e8\u673a\u5236\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u795e\u7ecf\u7f51\u7edc\u5728\u590d\u6742\u9886\u57df\u4e2d\u5c55\u73b0\u7684\u667a\u80fd\u524d\u77bb\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5e76\u9a8c\u8bc1\u4e86\u53ef\u89e3\u91ca\u6027\u6280\u672f\u5728AI\u8ba4\u77e5\u8fc7\u7a0b\u7814\u7a76\u4e2d\u7684\u6709\u6548\u6027\u3002", "keywords": "\u56fd\u9645\u8c61\u68cb\uff0c\u795e\u7ecf\u7f51\u7edc\uff0c\u524d\u77bb\u80fd\u529b\uff0cLeela Chess Zero\uff0c\u53ef\u89e3\u91ca\u6027"}}
{"id": "2505.21608", "pdf": "https://arxiv.org/pdf/2505.21608", "abs": "https://arxiv.org/abs/2505.21608", "authors": ["Miao Peng", "Nuo Chen", "Jianheng Tang", "Jia Li"], "title": "How does Misinformation Affect Large Language Model Behaviors and Preferences?", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Large Language Models (LLMs) have shown remarkable capabilities in\nknowledge-intensive tasks, while they remain vulnerable when encountering\nmisinformation. Existing studies have explored the role of LLMs in combating\nmisinformation, but there is still a lack of fine-grained analysis on the\nspecific aspects and extent to which LLMs are influenced by misinformation. To\nbridge this gap, we present MisBench, the current largest and most\ncomprehensive benchmark for evaluating LLMs' behavior and knowledge preference\ntoward misinformation. MisBench consists of 10,346,712 pieces of\nmisinformation, which uniquely considers both knowledge-based conflicts and\nstylistic variations in misinformation. Empirical results reveal that while\nLLMs demonstrate comparable abilities in discerning misinformation, they still\nremain susceptible to knowledge conflicts and stylistic variations. Based on\nthese findings, we further propose a novel approach called Reconstruct to\nDiscriminate (RtD) to strengthen LLMs' ability to detect misinformation. Our\nstudy provides valuable insights into LLMs' interactions with misinformation,\nand we believe MisBench can serve as an effective benchmark for evaluating\nLLM-based detectors and enhancing their reliability in real-world applications.\nCodes and data are available at https://github.com/GKNL/MisBench.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86MisBench\uff0c\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u4fe1\u606f\u8bef\u533a\u7684\u884c\u4e3a\u548c\u77e5\u8bc6\u504f\u597d\u7684\u6700\u5927\u6700\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1LLMs\u5728\u8bc6\u522b\u4fe1\u606f\u8bef\u533a\u65b9\u9762\u8868\u73b0\u51fa\u76f8\u5f53\u7684\u80fd\u529b\uff0c\u4f46\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u77e5\u8bc6\u51b2\u7a81\u548c\u98ce\u683c\u53d8\u5316\u7684\u5f71\u54cd\u3002\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5Reconstruct to Discriminate\uff08RtD\uff09\u6765\u589e\u5f3aLLMs\u68c0\u6d4b\u4fe1\u606f\u8bef\u533a\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u867d\u7136\u63a2\u8ba8\u4e86LLMs\u5728\u5bf9\u6297\u4fe1\u606f\u8bef\u533a\u4e2d\u7684\u4f5c\u7528\uff0c\u4f46\u7f3a\u4e4f\u5bf9LLMs\u5982\u4f55\u4ee5\u53ca\u5728\u4f55\u79cd\u7a0b\u5ea6\u4e0a\u53d7\u4fe1\u606f\u8bef\u533a\u5f71\u54cd\u7684\u7ec6\u7c92\u5ea6\u5206\u6790\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u8bc4\u4f30LLMs\u5728\u4fe1\u606f\u8bef\u533a\u9762\u524d\u7684\u8106\u5f31\u6027\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86MisBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b10,346,712\u6761\u4fe1\u606f\u8bef\u533a\uff0c\u72ec\u7279\u5730\u8003\u8651\u4e86\u77e5\u8bc6\u51b2\u7a81\u548c\u98ce\u683c\u53d8\u5316\u3002\u5b9e\u9a8c\u8bc4\u4f30\u4e86LLMs\u5728\u8fd9\u4e9b\u8bef\u533a\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86Reconstruct to Discriminate\uff08RtD\uff09\u65b9\u6cd5\u6765\u589e\u5f3a\u68c0\u6d4b\u80fd\u529b\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\uff0cLLMs\u5728\u8bc6\u522b\u4fe1\u606f\u8bef\u533a\u65b9\u9762\u80fd\u529b\u76f8\u5f53\uff0c\u4f46\u4ecd\u6613\u53d7\u77e5\u8bc6\u51b2\u7a81\u548c\u98ce\u683c\u53d8\u5316\u7684\u5f71\u54cd\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u63d0\u51fa\u7684RtD\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86LLMs\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aLLMs\u4e0e\u4fe1\u606f\u8bef\u533a\u7684\u4e92\u52a8\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\uff0cMisBench\u53ef\u4f5c\u4e3a\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u68c0\u6d4b\u5668\u5e76\u63d0\u5347\u5176\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u7684\u6709\u6548\u57fa\u51c6\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4fe1\u606f\u8bef\u533a\uff0c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u77e5\u8bc6\u51b2\u7a81\uff0c\u98ce\u683c\u53d8\u5316"}}
{"id": "2505.21514", "pdf": "https://arxiv.org/pdf/2505.21514", "abs": "https://arxiv.org/abs/2505.21514", "authors": ["Mingchao Jiang", "Abhinav Jain", "Sophia Zorek", "Chris Jermaine"], "title": "SIMCOPILOT: Evaluating Large Language Models for Copilot-Style Code Generation", "categories": ["cs.LG", "cs.PL", "cs.SE"], "comment": "Keywords: Benchmark Dataset, LLM Evaluation, Gen-AI, Program\n  Synthesis; TLDR: SimCoPilot is a benchmark for evaluating LLMs as\n  \"copilot\"-style interactive coding assistants, testing their ability to\n  integrate and complete code within complex real-world software environments", "summary": "We introduce SIMCOPILOT, a benchmark that simulates the role of large\nlanguage models (LLMs) as interactive, \"copilot\"-style coding assistants.\nTargeting both completion (finishing incomplete methods or code blocks) and\ninfill tasks (filling missing segments within existing code), SIMCOPILOT\nprovides a comprehensive framework for evaluating LLM coding capabilities. The\nbenchmark comprises dedicated sub-benchmarks for Java (SIMCOPILOTJ) and Python\n(SIMCOPILOTP), covering diverse codebases varying in size and complexity. Our\nkey contributions include: (a) establishing a realistic, detailed evaluation\nenvironment to assess LLM utility in practical coding scenarios, and (b)\nproviding fine-grained analyses that address critical factors frequently\noverlooked by existing benchmarks, such as task-specific performance nuances,\ncontextual understanding across code segments, and sensitivity to variable\nscope. Evaluations conducted across domains-including algorithms, databases,\ncomputer vision, and neural networks-offer insights into model strengths and\nhighlight persistent challenges in maintaining logical consistency within\ncomplex dependency structures. Beyond benchmarking, our study sheds light on\nthe current limitations of LLM-driven code generation and underscores the\nongoing transition of LLMs from merely syntax-aware generators toward reliable,\nintelligent software development partners.", "AI": {"tldr": "SIMCOPILOT\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u8865\u5168\u548c\u586b\u5145\u4efb\u52a1\u4e2d\u8868\u73b0\u7684\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8eJava\u548cPython\uff0c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u5206\u6790\u3002", "motivation": "\u8bc4\u4f30LLM\u5728\u5b9e\u9645\u7f16\u7801\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u5e76\u5173\u6ce8\u73b0\u6709\u57fa\u51c6\u5ffd\u7565\u7684\u7ec6\u8282\uff0c\u5982\u4efb\u52a1\u7279\u6027\u3001\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u53d8\u91cf\u4f5c\u7528\u57df\u654f\u611f\u6027\u3002", "method": "\u5efa\u7acbJava\u548cPython\u5b50\u57fa\u51c6(SIMCOPILOTJ/P)\uff0c\u8986\u76d6\u4e0d\u540c\u89c4\u6a21\u548c\u590d\u6742\u5ea6\u7684\u4ee3\u7801\u5e93\uff0c\u8fdb\u884c\u591a\u9886\u57df\u8bc4\u4f30\u3002", "result": "\u63ed\u793a\u4e86LLM\u5728\u4fdd\u6301\u903b\u8f91\u4e00\u81f4\u6027\u548c\u590d\u6742\u4f9d\u8d56\u7ed3\u6784\u4e2d\u7684\u6311\u6218\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u5176\u4ee3\u7801\u751f\u6210\u6f5c\u529b\u3002", "conclusion": "LLM\u6b63\u4ece\u8bed\u6cd5\u751f\u6210\u5668\u5411\u53ef\u9760\u5f00\u53d1\u4f19\u4f34\u8fc7\u6e21\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u903b\u8f91\u4e00\u81f4\u6027\u7b49\u5173\u952e\u95ee\u9898\u3002", "keywords": "LLM, \u4ee3\u7801\u8865\u5168, \u4ee3\u7801\u586b\u5145, Java, Python, \u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2505.21668", "pdf": "https://arxiv.org/pdf/2505.21668", "abs": "https://arxiv.org/abs/2505.21668", "authors": ["Yongchao Chen", "Yueying Liu", "Junwei Zhou", "Yilun Hao", "Jingquan Wang", "Yang Zhang", "Chuchu Fan"], "title": "R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning", "categories": ["cs.AI", "cs.CL", "cs.SC"], "comment": "33 pages, 8 figures", "summary": "Despite advances in reasoning and planning of R1-like models, Large Language\nModels (LLMs) still struggle with tasks requiring precise computation, symbolic\nmanipulation, optimization, and algorithmic reasoning, in which textual\nreasoning lacks the rigor of code execution. A key challenge is enabling LLMs\nto decide when to use textual reasoning versus code generation. While OpenAI\ntrains models to invoke a Code Interpreter as needed, public research lacks\nguidance on aligning pre-trained LLMs to effectively leverage code and\ngeneralize across diverse tasks. We present R1-Code-Interpreter, an extension\nof a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and\nreinforcement learning (RL) to autonomously generate multiple code queries\nduring step-by-step reasoning. We curate 144 reasoning and planning tasks (107\nfor training, 37 for testing), each with over 200 diverse questions. We\nfine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies,\ninvestigating different answer formats, reasoning vs. non-reasoning models,\ncold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs.\nUnlike prior RL work on narrow domains, we find that Code Interpreter training\nis significantly harder due to high task diversity and expensive code\nexecution, highlighting the critical role of the SFT stage. Our final model,\nR1-CI-14B, improves average accuracy on the 37 test tasks from 44.0\\% to\n64.1\\%, outperforming GPT-4o (text-only: 58.6\\%) and approaching GPT-4o with\nCode Interpreter (70.9\\%), with the emergent self-checking behavior via code\ngeneration. Datasets, Codes, and Models are available at\nhttps://github.com/yongchao98/R1-Code-Interpreter and\nhttps://huggingface.co/yongchao98.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86R1-Code-Interpreter\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u8f6e\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3LLM\u81ea\u4e3b\u751f\u6210\u4ee3\u7801\u67e5\u8be2\uff0c\u5728144\u4e2a\u4efb\u52a1\u4e0a\u6d4b\u8bd5\u8868\u73b0\u4f18\u5f02\uff0c\u63a5\u8fd1GPT-4o\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7cbe\u786e\u8ba1\u7b97\u3001\u7b26\u53f7\u5904\u7406\u548c\u7b97\u6cd5\u63a8\u7406\u7b49\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\uff0c\u63a2\u7d22\u5982\u4f55\u6709\u6548\u7ed3\u5408\u6587\u672c\u63a8\u7406\u4e0e\u4ee3\u7801\u751f\u6210\u3002", "method": "\u91c7\u7528\u591a\u8f6e\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8bad\u7ec3\u6a21\u578b\uff0c\u5bf9\u6bd4\u4e0d\u540c\u7b56\u7565\uff08\u5982GRPO vs. PPO\uff09\u548c\u4ee3\u7801\u8f93\u51fa\u683c\u5f0f\u3002", "result": "R1-CI-14B\u6a21\u578b\u572837\u4e2a\u6d4b\u8bd5\u4efb\u52a1\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u4ece44.0%\u63d0\u5347\u81f364.1%\uff0c\u4f18\u4e8eGPT-4o\u6587\u672c\u7248\uff0858.6%\uff09\uff0c\u63a5\u8fd1\u5176\u4ee3\u7801\u89e3\u91ca\u5668\u7248\uff0870.9%\uff09\u3002", "conclusion": "\u4ee3\u7801\u89e3\u91ca\u5668\u8bad\u7ec3\u56e0\u4efb\u52a1\u591a\u6837\u6027\u548c\u6267\u884c\u6210\u672c\u53d8\u5f97\u590d\u6742\uff0c\u7a81\u663e\u4e86SFT\u9636\u6bb5\u7684\u5173\u952e\u4f5c\u7528\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u4ee3\u7801\u751f\u6210, \u76d1\u7763\u5fae\u8c03, \u5f3a\u5316\u5b66\u4e60, \u7b97\u6cd5\u63a8\u7406"}}
{"id": "2505.21646", "pdf": "https://arxiv.org/pdf/2505.21646", "abs": "https://arxiv.org/abs/2505.21646", "authors": ["Lei Zhang", "Markus Stricker"], "title": "Iterative Corpus Refinement for Materials Property Prediction Based on Scientific Texts", "categories": ["cs.CL", "cond-mat.mtrl-sci"], "comment": "13 pages, 5 figures, 2 tables, accepted at ECMLPKDD 2025", "summary": "The discovery and optimization of materials for specific applications is\nhampered by the practically infinite number of possible elemental combinations\nand associated properties, also known as the `combinatorial explosion'. By\nnature of the problem, data are scarce and all possible data sources should be\nused. In addition to simulations and experimental results, the latent knowledge\nin scientific texts is not yet used to its full potential. We present an\niterative framework that refines a given scientific corpus by strategic\nselection of the most diverse documents, training Word2Vec models, and\nmonitoring the convergence of composition-property correlations in embedding\nspace. Our approach is applied to predict high-performing materials for oxygen\nreduction (ORR), hydrogen evolution (HER), and oxygen evolution (OER) reactions\nfor a large number of possible candidate compositions. Our method successfully\npredicts the highest performing compositions among a large pool of candidates,\nvalidated by experimental measurements of the electrocatalytic performance in\nthe lab. This work demonstrates and validates the potential of iterative corpus\nrefinement to accelerate materials discovery and optimization, offering a\nscalable and efficient tool for screening large compositional spaces where\nreliable data are scarce or non-existent.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u79d1\u5b66\u6587\u732e\u8bed\u6599\u5e93\u5e76\u7ed3\u5408Word2Vec\u6a21\u578b\u6765\u9884\u6d4b\u9ad8\u6027\u80fd\u6750\u6599\u7684\u6846\u67b6\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u6c27\u8fd8\u539f\u3001\u6c22\u6790\u51fa\u548c\u6c27\u6790\u51fa\u53cd\u5e94\u7684\u6750\u6599\u7b5b\u9009\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u89e3\u51b3\u6750\u6599\u8bbe\u8ba1\u4e2d\u7531\u4e8e\u7ec4\u5408\u7206\u70b8\u5bfc\u81f4\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5145\u5206\u5229\u7528\u79d1\u5b66\u6587\u732e\u4e2d\u7684\u6f5c\u5728\u77e5\u8bc6\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u591a\u6837\u5316\u7684\u6587\u732e\u8bad\u7ec3Word2Vec\u6a21\u578b\uff0c\u5e76\u76d1\u63a7\u5d4c\u5165\u7a7a\u95f4\u4e2d\u6210\u5206-\u6027\u80fd\u5173\u8054\u7684\u6536\u655b\u3002", "result": "\u6210\u529f\u9884\u6d4b\u4e86\u9ad8\u6027\u80fd\u6750\u6599\u7ec4\u5408\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u7535\u50ac\u5316\u6027\u80fd\u3002", "conclusion": "\u8fed\u4ee3\u8bed\u6599\u5e93\u4f18\u5316\u65b9\u6cd5\u53ef\u52a0\u901f\u6750\u6599\u53d1\u73b0\u4e0e\u4f18\u5316\uff0c\u4e3a\u6570\u636e\u7a00\u7f3a\u7684\u5927\u89c4\u6a21\u7ec4\u5408\u7a7a\u95f4\u63d0\u4f9b\u9ad8\u6548\u7b5b\u9009\u5de5\u5177\u3002", "keywords": "\u6750\u6599\u53d1\u73b0, Word2Vec, \u7535\u50ac\u5316, \u7ec4\u5408\u7206\u70b8, \u8bed\u6599\u5e93\u4f18\u5316"}}
{"id": "2505.21525", "pdf": "https://arxiv.org/pdf/2505.21525", "abs": "https://arxiv.org/abs/2505.21525", "authors": ["Peiliang Gong", "Yucheng Wang", "Min Wu", "Zhenghua Chen", "Xiaoli Li", "Daoqiang Zhang"], "title": "Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained model from\nan annotated source domain to an unlabelled target domain without accessing the\nsource data, thereby preserving data privacy. While existing SFDA methods have\nproven effective in reducing reliance on source data, they struggle to perform\nwell on multivariate time series (MTS) due to their failure to consider the\nintrinsic spatial correlations inherent in MTS data. These spatial correlations\nare crucial for accurately representing MTS data and preserving invariant\ninformation across domains. To address this challenge, we propose Temporal\nRestoration and Spatial Rewiring (TERSE), a novel and concise SFDA method\ntailored for MTS data. Specifically, TERSE comprises a customized\nspatial-temporal feature encoder designed to capture the underlying\nspatial-temporal characteristics, coupled with both temporal restoration and\nspatial rewiring tasks to reinstate latent representations of the temporally\nmasked time series and the spatially masked correlated structures. During the\ntarget adaptation phase, the target encoder is guided to produce spatially and\ntemporally consistent features with the source domain by leveraging the source\npre-trained temporal restoration and spatial rewiring networks. Therefore,\nTERSE can effectively model and transfer spatial-temporal dependencies across\ndomains, facilitating implicit feature alignment. In addition, as the first\napproach to simultaneously consider spatial-temporal consistency in MTS-SFDA,\nTERSE can also be integrated as a versatile plug-and-play module into\nestablished SFDA methods. Extensive experiments on three real-world time series\ndatasets demonstrate the effectiveness and versatility of our approach.", "AI": {"tldr": "TERSE\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578bSFDA\u65b9\u6cd5\uff0c\u9488\u5bf9MTS\u6570\u636e\u4f18\u5316\uff0c\u901a\u8fc7\u65f6\u95f4\u548c\u7a7a\u95f4\u7279\u5f81\u6062\u590d\u5b9e\u73b0\u8de8\u57df\u7279\u5f81\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709SFDA\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651MTS\u6570\u636e\u7684\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u8de8\u57df\u9002\u5e94\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u4ee5\u63d0\u5347\u9690\u79c1\u4fdd\u62a4\u4e0b\u7684\u6a21\u578b\u9002\u5e94\u6027\u3002", "method": "TERSE\u7ed3\u5408\u65f6\u7a7a\u7279\u5f81\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u65f6\u95f4\u6062\u590d\u548c\u7a7a\u95f4\u91cd\u8fde\u4efb\u52a1\u91cd\u5efa\u63a9\u7801\u7684\u65f6\u5e8f\u4e0e\u7a7a\u95f4\u7ed3\u6784\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7f51\u7edc\u5f15\u5bfc\u76ee\u6807\u57df\u7279\u5f81\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u65f6\u5e8f\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cTERSE\u6709\u6548\u5efa\u6a21\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff0c\u63d0\u5347\u8de8\u57df\u6027\u80fd\uff0c\u5e76\u53ef\u4f5c\u4e3a\u6a21\u5757\u5d4c\u5165\u73b0\u6709SFDA\u65b9\u6cd5\u3002", "conclusion": "TERSE\u9996\u6b21\u5728MTS-SFDA\u4e2d\u540c\u6b65\u8003\u8651\u65f6\u7a7a\u4e00\u81f4\u6027\uff0c\u4e3a\u8de8\u57df\u65f6\u5e8f\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "SFDA, MTS, \u65f6\u7a7a\u7279\u5f81, \u8de8\u57df\u9002\u5e94, \u9690\u79c1\u4fdd\u62a4"}}
{"id": "2505.21671", "pdf": "https://arxiv.org/pdf/2505.21671", "abs": "https://arxiv.org/abs/2505.21671", "authors": ["Davin Choo", "Yuqi Pan", "Tonghan Wang", "Milind Tambe", "Alastair van Heerden", "Cheryl Johnson"], "title": "Adaptive Frontier Exploration on Graphs with Applications to Network-Based Disease Testing", "categories": ["cs.AI", "cs.DS", "cs.LG", "math.OC"], "comment": null, "summary": "We study a sequential decision-making problem on a $n$-node graph $G$ where\neach node has an unknown label from a finite set $\\mathbf{\\Sigma}$, drawn from\na joint distribution $P$ that is Markov with respect to $G$. At each step,\nselecting a node reveals its label and yields a label-dependent reward. The\ngoal is to adaptively choose nodes to maximize expected accumulated discounted\nrewards. We impose a frontier exploration constraint, where actions are limited\nto neighbors of previously selected nodes, reflecting practical constraints in\nsettings such as contact tracing and robotic exploration. We design a Gittins\nindex-based policy that applies to general graphs and is provably optimal when\n$G$ is a forest. Our implementation runs in $O(n^2 \\cdot |\\mathbf{\\Sigma}|^2)$\ntime while using $O(n \\cdot |\\mathbf{\\Sigma}|^2)$ oracle calls to $P$ and\n$O(n^2 \\cdot |\\mathbf{\\Sigma}|)$ space. Experiments on synthetic and real-world\ngraphs show that our method consistently outperforms natural baselines,\nincluding in non-tree, budget-limited, and undiscounted settings. For example,\nin HIV testing simulations on real-world sexual interaction networks, our\npolicy detects nearly all positive cases with only half the population tested,\nsubstantially outperforming other baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5177\u6709\u672a\u77e5\u8282\u70b9\u6807\u7b7e\u7684\u56fe\u4e0a\u8fdb\u884c\u987a\u5e8f\u51b3\u7b56\u7684\u95ee\u9898\uff0c\u76ee\u6807\u662f\u6700\u5927\u5316\u7d2f\u79ef\u6298\u6263\u5956\u52b1\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8eGittins\u6307\u6570\u7684\u7b56\u7565\uff0c\u5728\u68ee\u6797\u56fe\u4e2d\u88ab\u8bc1\u660e\u662f\u6700\u4f18\u7684\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u5b9e\u9645\u5e94\u7528\uff08\u5982\u63a5\u89e6\u8ffd\u8e2a\u548c\u673a\u5668\u4eba\u63a2\u7d22\uff09\u4e2d\u53d7\u9650\u4e8e\u524d\u6cbf\u63a2\u7d22\u7ea6\u675f\u7684\u987a\u5e8f\u51b3\u7b56\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8eGittins\u6307\u6570\u7684\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u4e00\u822c\u56fe\u7ed3\u6784\uff0c\u5e76\u5728\u68ee\u6797\u56fe\u4e2d\u8bc1\u660e\u6700\u4f18\u6027\u3002\u5b9e\u73b0\u65f6\u95f4\u4e3aO(n\u00b2\u00b7|\u03a3|\u00b2)\uff0c\u7a7a\u95f4\u590d\u6742\u5ea6\u4e3aO(n\u00b2\u00b7|\u03a3|)\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u56fe\u7684\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u4f8b\u5982\u5728HIV\u68c0\u6d4b\u6a21\u62df\u4e2d\uff0c\u4ec5\u6d4b\u8bd5\u4e00\u534a\u4eba\u53e3\u5373\u53ef\u68c0\u6d4b\u51fa\u51e0\u4e4e\u6240\u6709\u9633\u6027\u75c5\u4f8b\u3002", "conclusion": "\u63d0\u51fa\u7684Gittins\u6307\u6570\u7b56\u7565\u5728\u591a\u79cd\u8bbe\u7f6e\u4e0b\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u6811\u7ed3\u6784\u548c\u9884\u7b97\u53d7\u9650\u573a\u666f\u4e2d\u5177\u6709\u4f18\u52bf\u3002", "keywords": "\u987a\u5e8f\u51b3\u7b56\u3001Gittins\u6307\u6570\u3001\u56fe\u63a2\u7d22\u3001\u6700\u4f18\u7b56\u7565\u3001\u63a5\u89e6\u8ffd\u8e2a"}}
{"id": "2505.21657", "pdf": "https://arxiv.org/pdf/2505.21657", "abs": "https://arxiv.org/abs/2505.21657", "authors": ["Zeinab Dehghani", "Koorosh Aslansefat", "Adil Khan", "Mohammed Naveed Akram"], "title": "Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2412.16277", "summary": "Large language models like GPT, LLAMA, and Claude have become incredibly\npowerful at generating text, but they are still black boxes, so it is hard to\nunderstand how they decide what to say. That lack of transparency can be\nproblematic, especially in fields where trust and accountability matter. To\nhelp with this, we introduce SMILE, a new method that explains how these models\nrespond to different parts of a prompt. SMILE is model-agnostic and works by\nslightly changing the input, measuring how the output changes, and then\nhighlighting which words had the most impact. Create simple visual heat maps\nshowing which parts of a prompt matter the most. We tested SMILE on several\nleading LLMs and used metrics such as accuracy, consistency, stability, and\nfidelity to show that it gives clear and reliable explanations. By making these\nmodels easier to understand, SMILE brings us one step closer to making AI more\ntransparent and trustworthy.", "AI": {"tldr": "SMILE\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03\u8f93\u5165\u5e76\u6d4b\u91cf\u8f93\u51fa\u53d8\u5316\u6765\u89e3\u91ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u54cd\u5e94\u673a\u5236\uff0c\u751f\u6210\u70ed\u56fe\u7a81\u51fa\u663e\u793a\u63d0\u793a\u4e2d\u6700\u5173\u952e\u7684\u90e8\u5206\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u578b\u900f\u660e\u5ea6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982GPT\u3001LLAMA\u548cClaude\u867d\u5f3a\u5927\u4f46\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u5c24\u5176\u5728\u9700\u8981\u4fe1\u4efb\u548c\u8d23\u4efb\u7684\u9886\u57df\uff0c\u8fd9\u6210\u4e3a\u95ee\u9898\u3002", "method": "SMILE\u901a\u8fc7\u5fae\u8c03\u8f93\u5165\u3001\u6d4b\u91cf\u8f93\u51fa\u53d8\u5316\uff0c\u5e76\u751f\u6210\u70ed\u56fe\u6765\u53ef\u89c6\u5316\u63d0\u793a\u4e2d\u5f71\u54cd\u6700\u5927\u7684\u90e8\u5206\uff0c\u540c\u65f6\u8bc4\u4f30\u51c6\u786e\u6027\u3001\u4e00\u81f4\u6027\u3001\u7a33\u5b9a\u6027\u548c\u4fdd\u771f\u5ea6\u3002", "result": "\u5728\u591a\u4e2a\u9886\u5148\u7684LLM\u4e0a\u6d4b\u8bd5\u8868\u660e\uff0cSMILE\u80fd\u63d0\u4f9b\u6e05\u6670\u53ef\u9760\u7684\u89e3\u91ca\u3002", "conclusion": "SMILE\u901a\u8fc7\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff0c\u4f7fAI\u66f4\u900f\u660e\u53ef\u4fe1\u3002", "keywords": "SMILE, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u53ef\u89e3\u91ca\u6027, \u70ed\u56fe, \u900f\u660e\u5ea6"}}
{"id": "2505.21569", "pdf": "https://arxiv.org/pdf/2505.21569", "abs": "https://arxiv.org/abs/2505.21569", "authors": ["Zhucong Li", "Bowei Zhang", "Jin Xiao", "Zhijian Zhou", "Fenglei Cao", "Jiaqing Liang", "Yuan Qi"], "title": "ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "9 pages", "summary": "Large Language Model (LLM)-based agents have demonstrated the ability to\nimprove performance in chemistry-related tasks by selecting appropriate tools.\nHowever, their effectiveness remains limited by the inherent prediction errors\nof chemistry tools. In this paper, we take a step further by exploring how\nLLMbased agents can, in turn, be leveraged to reduce prediction errors of the\ntools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking),\na simple yet effective method that enhances chemistry tools through optimizing\nagent-stacking structures from limited data. ChemHAS achieves state-of-the-art\nperformance across four fundamental chemistry tasks, demonstrating that our\nmethod can effectively compensate for prediction errors of the tools.\nFurthermore, we identify and characterize four distinct agent-stacking\nbehaviors, potentially improving interpretability and revealing new\npossibilities for AI agent applications in scientific research. Our code and\ndataset are publicly available at https:\n//anonymous.4open.science/r/ChemHAS-01E4/README.md.", "AI": {"tldr": "ChemHAS\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u4ee3\u7406\u5806\u53e0\u7ed3\u6784\u6765\u51cf\u5c11\u5316\u5b66\u5de5\u5177\u7684\u9884\u6d4b\u8bef\u5dee\uff0c\u5e76\u5728\u56db\u4e2a\u57fa\u7840\u5316\u5b66\u4efb\u52a1\u4e2d\u53d6\u5f97\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684LLM\u4ee3\u7406\u5728\u5316\u5b66\u4efb\u52a1\u4e2d\u8868\u73b0\u53d7\u9650\uff0c\u4e3b\u8981\u56e0\u4e3a\u5316\u5b66\u5de5\u5177\u7684\u9884\u6d4b\u8bef\u5dee\u3002", "method": "\u63d0\u51faChemHAS\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u4ee3\u7406\u5806\u53e0\u7ed3\u6784\u6765\u589e\u5f3a\u5316\u5b66\u5de5\u5177\u7684\u6027\u80fd\u3002", "result": "ChemHAS\u5728\u56db\u4e2a\u57fa\u7840\u5316\u5b66\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u5e76\u63ed\u793a\u4e86\u56db\u79cd\u4ee3\u7406\u5806\u53e0\u884c\u4e3a\u3002", "conclusion": "ChemHAS\u80fd\u6709\u6548\u8865\u507f\u5de5\u5177\u9884\u6d4b\u8bef\u5dee\uff0c\u5e76\u4e3a\u79d1\u5b66\u7814\u7a76\u7684AI\u4ee3\u7406\u5e94\u7528\u63d0\u4f9b\u65b0\u53ef\u80fd\u3002", "keywords": "LLM\u4ee3\u7406\u3001\u5316\u5b66\u5de5\u5177\u3001\u9884\u6d4b\u8bef\u5dee\u3001\u4ee3\u7406\u5806\u53e0\u3001ChemHAS"}}
{"id": "2505.21674", "pdf": "https://arxiv.org/pdf/2505.21674", "abs": "https://arxiv.org/abs/2505.21674", "authors": ["Michael Katz", "Harsha Kokel", "Christian Muise", "Shirin Sohrabi", "Sarath Sreedharan"], "title": "Make Planning Research Rigorous Again!", "categories": ["cs.AI"], "comment": null, "summary": "In over sixty years since its inception, the field of planning has made\nsignificant contributions to both the theory and practice of building planning\nsoftware that can solve a never-before-seen planning problem. This was done\nthrough established practices of rigorous design and evaluation of planning\nsystems. It is our position that this rigor should be applied to the current\ntrend of work on planning with large language models. One way to do so is by\ncorrectly incorporating the insights, tools, and data from the automated\nplanning community into the design and evaluation of LLM-based planners. The\nexperience and expertise of the planning community are not just important from\na historical perspective; the lessons learned could play a crucial role in\naccelerating the development of LLM-based planners. This position is\nparticularly important in light of the abundance of recent works that replicate\nand propagate the same pitfalls that the planning community has encountered and\nlearned from. We believe that avoiding such known pitfalls will contribute\ngreatly to the progress in building LLM-based planners and to planning in\ngeneral.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u5c06\u81ea\u52a8\u5316\u89c4\u5212\u9886\u57df\u7684\u7ecf\u9a8c\u4e0e\u4e25\u8c28\u65b9\u6cd5\u878d\u5165\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u89c4\u5212\u7cfb\u7edf\u5f00\u53d1\uff0c\u4ee5\u907f\u514d\u91cd\u590d\u5df2\u77e5\u9519\u8bef\u5e76\u52a0\u901f\u8fdb\u5c55\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u89c4\u5212\u7814\u7a76\u4e2d\u5b58\u5728\u91cd\u590d\u81ea\u52a8\u5316\u89c4\u5212\u9886\u57df\u5df2\u77e5\u9519\u8bef\u7684\u73b0\u8c61\uff0c\u9700\u8981\u5f15\u5165\u8be5\u9886\u57df\u7684\u4e25\u8c28\u5b9e\u8df5\u4ee5\u63a8\u52a8\u8fdb\u6b65\u3002", "method": "\u63d0\u51fa\u5c06\u81ea\u52a8\u5316\u89c4\u5212\u9886\u57df\u7684\u89c1\u89e3\u3001\u5de5\u5177\u548c\u6570\u636e\u6b63\u786e\u6574\u5408\u5230LLM\u89c4\u5212\u5668\u7684\u8bbe\u8ba1\u4e0e\u8bc4\u4f30\u4e2d\u3002", "result": "\u901a\u8fc7\u907f\u514d\u5df2\u77e5\u9677\u9631\uff0c\u80fd\u663e\u8457\u63d0\u5347LLM\u89c4\u5212\u5668\u7684\u5f00\u53d1\u6548\u7387\u4e0e\u89c4\u5212\u9886\u57df\u7684\u6574\u4f53\u8fdb\u5c55\u3002", "conclusion": "\u878d\u5408\u89c4\u5212\u793e\u533a\u7684\u7ecf\u9a8c\u5bf9\u52a0\u901fLLM\u89c4\u5212\u5668\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u9700\u907f\u514d\u91cd\u8e48\u8986\u8f99\u3002", "keywords": "\u81ea\u52a8\u5316\u89c4\u5212, \u5927\u8bed\u8a00\u6a21\u578b, \u4e25\u8c28\u8bbe\u8ba1, \u8bc4\u4f30\u5b9e\u8df5"}}
{"id": "2505.21670", "pdf": "https://arxiv.org/pdf/2505.21670", "abs": "https://arxiv.org/abs/2505.21670", "authors": ["Rahul Raman", "Khushi Sharma", "Sai Qian Zhang"], "title": "Rethinking the Outlier Distribution in Large Language Models: An In-depth Study", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Investigating outliers in large language models (LLMs) is crucial due to\ntheir significant impact on various aspects of LLM performance, including\nquantization and compression. Outliers often cause considerable quantization\nerrors, leading to degraded model performance. Identifying and addressing these\noutliers can enhance the accuracy and efficiency of the quantization process,\nenabling smoother deployment on edge devices or specialized hardware. Recent\nstudies have identified two common types of outliers in LLMs: massive\nactivations and channel-wise outliers. While numerous quantization algorithms\nhave been proposed to mitigate their effects and maintain satisfactory\naccuracy, few have thoroughly explored the root causes of these outliers in\ndepth. In this paper, we conduct a comprehensive investigation into the\nformation mechanisms of these outliers and propose potential strategies to\nmitigate their occurrence. Ultimately, we introduce some efficient approaches\nto eliminate most massive activations and channel-wise outliers with minimal\nimpact on accuracy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u5f02\u5e38\u503c\uff0c\u63a2\u8ba8\u4e86\u5176\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e9b\u6709\u6548\u7684\u6d88\u9664\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u5f02\u5e38\u503c\u5bf9LLM\u7684\u91cf\u5316\u4e0e\u538b\u7f29\u7b49\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u7814\u7a76\u5176\u5f62\u6210\u673a\u5236\u5e76\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u53ef\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u91cf\u5316\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002", "method": "\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5927\u91cf\u6fc0\u6d3b\u4e0e\u901a\u9053\u7ea7\u5f02\u5e38\u503c\u8fdb\u884c\u6df1\u5165\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e9b\u6f5c\u5728\u7684\u6d88\u9664\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u4e86\u9ad8\u6548\u7684\u6d88\u9664\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u5f02\u5e38\u503c\u5bf9\u6a21\u578b\u7cbe\u5ea6\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u63d0\u5347LLM\u91cf\u5316\u6548\u679c\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5bf9\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff1b\u5f02\u5e38\u503c\uff1b\u91cf\u5316\uff1b\u538b\u7f29\uff1b\u8fb9\u7f18\u8ba1\u7b97"}}
{"id": "2505.21571", "pdf": "https://arxiv.org/pdf/2505.21571", "abs": "https://arxiv.org/abs/2505.21571", "authors": ["Yao Lu", "Tengfei Ma", "Zeyu Wang", "Zhuangzhi Chen", "Dongwei Xu", "Yun Lin", "Qi Xuan", "Guan Gui"], "title": "FCOS: A Two-Stage Recoverable Model Pruning Framework for Automatic Modulation Recognition", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "With the rapid development of wireless communications and the growing\ncomplexity of digital modulation schemes, traditional manual modulation\nrecognition methods struggle to extract reliable signal features and meet\nreal-time requirements in modern scenarios. Recently, deep learning based\nAutomatic Modulation Recognition (AMR) approaches have greatly improved\nclassification accuracy. However, their large model sizes and high\ncomputational demands hinder deployment on resource-constrained devices. Model\npruning provides a general approach to reduce model complexity, but existing\nweight, channel, and layer pruning techniques each present a trade-off between\ncompression rate, hardware acceleration, and accuracy preservation. To this\nend, in this paper, we introduce FCOS, a novel Fine-to-COarse two-Stage pruning\nframework that combines channel-level pruning with layer-level collapse\ndiagnosis to achieve extreme compression, high performance and efficient\ninference. In the first stage of FCOS, hierarchical clustering and parameter\nfusion are applied to channel weights to achieve channel-level pruning. Then a\nLayer Collapse Diagnosis (LaCD) module uses linear probing to identify layer\ncollapse and removes the collapsed layers due to high channel compression\nratio. Experiments on multiple AMR benchmarks demonstrate that FCOS outperforms\nexisting channel and layer pruning methods. Specifically, FCOS achieves 95.51%\nFLOPs reduction and 95.31% parameter reduction while still maintaining\nperformance close to the original ResNet56, with only a 0.46% drop in accuracy\non Sig2019-12. Code is available at https://github.com/yaolu-zjut/FCOS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFCOS\u7684\u65b0\u578b\u4e24\u9636\u6bb5\u526a\u679d\u6846\u67b6\uff0c\u7ed3\u5408\u901a\u9053\u7ea7\u526a\u679d\u548c\u5c42\u574d\u584c\u8bca\u65ad\uff0c\u5b9e\u73b0\u4e86\u6781\u9ad8\u538b\u7f29\u7387\u548c\u9ad8\u6548\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u539f\u59cb\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u624b\u52a8\u8c03\u5236\u8bc6\u522b\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u73b0\u4ee3\u573a\u666f\u7684\u5b9e\u65f6\u6027\u548c\u7279\u5f81\u63d0\u53d6\u9700\u6c42\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60AMR\u65b9\u6cd5\u867d\u51c6\u786e\u4f46\u8ba1\u7b97\u91cf\u5927\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5e73\u8861\u538b\u7f29\u7387\u3001\u52a0\u901f\u548c\u7cbe\u5ea6\u7684\u65b9\u6cd5\u3002", "method": "FCOS\u6846\u67b6\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u901a\u9053\u7ea7\u526a\u679d\uff08\u901a\u8fc7\u5c42\u6b21\u805a\u7c7b\u548c\u53c2\u6570\u878d\u5408\uff09\u548c\u5c42\u574d\u584c\u8bca\u65ad\uff08\u4f7f\u7528\u7ebf\u6027\u63a2\u6d4b\u8bc6\u522b\u5c42\u574d\u584c\u5e76\u79fb\u9664\uff09\u3002", "result": "\u5728\u591a\u4e2aAMR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFCOS\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cFLOPs\u548c\u53c2\u6570\u5206\u522b\u51cf\u5c1195.51%\u548c95.31%\uff0c\u7cbe\u5ea6\u4ec5\u4e0b\u964d0.46%\u3002", "conclusion": "FCOS\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u526a\u679d\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\uff0c\u4e14\u6027\u80fd\u63a5\u8fd1\u539f\u59cb\u6a21\u578b\u3002", "keywords": "AMR, \u6a21\u578b\u526a\u679d, \u6df1\u5ea6\u5b66\u4e60, \u9ad8\u6548\u63a8\u7406"}}
{"id": "2505.21765", "pdf": "https://arxiv.org/pdf/2505.21765", "abs": "https://arxiv.org/abs/2505.21765", "authors": ["Sohyun An", "Ruochen Wang", "Tianyi Zhou", "Cho-Jui Hsieh"], "title": "Don't Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models", "categories": ["cs.AI"], "comment": "Work In Progress", "summary": "While recent success of large reasoning models (LRMs) significantly advanced\nLLMs' reasoning capability by optimizing the final answer accuracy using\nreinforcement learning, they may also drastically increase the output length\ndue to overthinking, characterized by unnecessarily complex reasoning paths\nthat waste computation and potentially degrade the performance. We hypothesize\nthat such inefficiencies stem from LRMs' limited capability to dynamically\nselect the proper modular reasoning strategies, termed thinking patterns at the\nright position. To investigate this hypothesis, we propose a dynamic\noptimization framework that segments model-generated reasoning paths into\ndistinct thinking patterns, systematically identifying and promoting beneficial\npatterns that improve the answer while removing detrimental ones. Empirical\nanalysis confirms that our optimized thinking paths yield more concise yet\nsufficiently informative trajectories, enhancing reasoning efficiency by\nreducing attention FLOPs by up to 47% while maintaining accuracy for originally\ncorrect responses. Moreover, a non-trivial portion of originally incorrect\nresponses are transformed into correct ones, achieving a 15.6% accuracy\nimprovement with reduced length. Motivated by the improvement brought by the\noptimized thinking paths, we apply a preference optimization technique\nsupported by a pairwise dataset contrasting suboptimal and optimal reasoning\npaths. Experimental evaluations across multiple mathematical reasoning\nbenchmarks reveal that our method notably reduces computational overhead while\nsimultaneously improving reasoning accuracy, achieving up to a 12% accuracy\nimprovement and reducing token usage from approximately 5,000 to 3,000 tokens.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5272\u548c\u4f18\u5316\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u751f\u6210\u7684\u63a8\u7406\u8def\u5f84\uff0c\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u7387\uff0c\u6700\u7ec8\u5728\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u867d\u7136\u63d0\u9ad8\u4e86\u6700\u7ec8\u7b54\u6848\u7684\u51c6\u786e\u7387\uff0c\u4f46\u7531\u4e8e\u8fc7\u5ea6\u601d\u8003\uff08overthinking\uff09\u5bfc\u81f4\u63a8\u7406\u8def\u5f84\u590d\u6742\u5316\uff0c\u6d6a\u8d39\u8ba1\u7b97\u8d44\u6e90\u5e76\u53ef\u80fd\u964d\u4f4e\u6027\u80fd\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6a21\u5757\u5316\u63a8\u7406\u7b56\u7565\u6765\u4f18\u5316\u6548\u7387\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u6a21\u578b\u751f\u6210\u7684\u63a8\u7406\u8def\u5f84\u5206\u5272\u4e3a\u4e0d\u540c\u7684\u601d\u7ef4\u6a21\u5f0f\uff0c\u8bc6\u522b\u5e76\u4fc3\u8fdb\u6709\u76ca\u6a21\u5f0f\uff0c\u53bb\u9664\u4e0d\u5229\u6a21\u5f0f\u3002\u91c7\u7528\u504f\u597d\u4f18\u5316\u6280\u672f\uff0c\u901a\u8fc7\u5bf9\u6bd4\u6b21\u4f18\u548c\u6700\u4f18\u63a8\u7406\u8def\u5f84\u7684\u6570\u636e\u96c6\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u4f18\u5316\u540e\u7684\u63a8\u7406\u8def\u5f84\u51cf\u5c11\u4e8647%\u7684\u6ce8\u610f\u529bFLOPs\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u51c6\u786e\u7387\uff0c\u5e76\u63d0\u5347\u4e8615.6%\u7684\u9519\u8bef\u7b54\u6848\u4fee\u6b63\u7387\u3002\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u534712%\uff0ctoken\u4f7f\u7528\u91cf\u4ece\u7ea65000\u964d\u81f33000\u3002", "conclusion": "\u901a\u8fc7\u52a8\u6001\u4f18\u5316\u6846\u67b6\uff0c\u53ef\u663e\u8457\u63d0\u5347\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6548\u7387\u548c\u51c6\u786e\u7387\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u3001\u52a8\u6001\u4f18\u5316\u3001\u601d\u7ef4\u6a21\u5f0f\u3001\u8ba1\u7b97\u6548\u7387\u3001\u504f\u597d\u4f18\u5316"}}
{"id": "2505.21689", "pdf": "https://arxiv.org/pdf/2505.21689", "abs": "https://arxiv.org/abs/2505.21689", "authors": ["Avijit Gayen", "Somyajit Chakraborty", "Mainak Sen", "Soham Paul", "Angshuman Jana"], "title": "LLMPR: A Novel LLM-Driven Transfer Learning based Petition Ranking Model", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "28 pages, 5 figures, journal paper, submitted to AI and Law", "summary": "The persistent accumulation of unresolved legal cases, especially within the\nIndian judiciary, significantly hampers the timely delivery of justice. Manual\nmethods of prioritizing petitions are often prone to inefficiencies and\nsubjective biases further exacerbating delays. To address this issue, we\npropose LLMPR (Large Language Model-based Petition Ranking), an automated\nframework that utilizes transfer learning and machine learning to assign\npriority rankings to legal petitions based on their contextual urgency.\nLeveraging the ILDC dataset comprising 7,593 annotated petitions, we process\nunstructured legal text and extract features through various embedding\ntechniques, including DistilBERT, LegalBERT, and MiniLM. These textual\nembeddings are combined with quantitative indicators such as gap days, rank\nscores, and word counts to train multiple machine learning models, including\nRandom Forest, Decision Tree, XGBoost, LightGBM, and CatBoost. Our experiments\ndemonstrate that Random Forest and Decision Tree models yield superior\nperformance, with accuracy exceeding 99% and a Spearman rank correlation of\n0.99. Notably, models using only numerical features achieve nearly optimal\nranking results (R2 = 0.988, \\r{ho} = 0.998), while LLM-based embeddings offer\nonly marginal gains. These findings suggest that automated petition ranking can\neffectively streamline judicial workflows, reduce case backlog, and improve\nfairness in legal prioritization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6cd5\u5f8b\u8bf7\u613f\u4e66\u81ea\u52a8\u6392\u5e8f\u6846\u67b6\uff08LLMPR\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u5d4c\u5165\u548c\u5b9a\u91cf\u6307\u6807\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6392\u5e8f\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5370\u5ea6\u53f8\u6cd5\u7cfb\u7edf\u4e2d\u672a\u51b3\u6848\u4ef6\u7684\u957f\u671f\u79ef\u7d2f\u95ee\u9898\uff0c\u51cf\u5c11\u4eba\u5de5\u6392\u5e8f\u7684\u4f4e\u6548\u548c\u4e3b\u89c2\u504f\u89c1\uff0c\u4fc3\u8fdb\u516c\u6b63\u548c\u53ca\u65f6\u7684\u53f8\u6cd5\u88c1\u51b3\u3002", "method": "\u4f7f\u7528ILDC\u6570\u636e\u96c6\uff087,593\u4efd\u5e26\u6807\u6ce8\u7684\u8bf7\u613f\u4e66\uff09\uff0c\u7ed3\u5408DistilBERT\u3001LegalBERT\u7b49\u6587\u672c\u5d4c\u5165\u6280\u672f\u53ca\u5b9a\u91cf\u6307\u6807\uff08\u5982\u95f4\u9694\u5929\u6570\u3001\u6392\u540d\u5206\u6570\u7b49\uff09\uff0c\u8bad\u7ec3\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982\u968f\u673a\u68ee\u6797\u3001\u51b3\u7b56\u6811\u7b49\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u968f\u673a\u68ee\u6797\u548c\u51b3\u7b56\u6811\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8d85\u8fc799%\uff0c\u65af\u76ae\u5c14\u66fc\u7b49\u7ea7\u76f8\u5173\u7cfb\u6570\u8fbe0.99\uff1b\u4ec5\u4f7f\u7528\u6570\u503c\u7279\u5f81\u7684\u6a21\u578b\u4e5f\u53ef\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u7684\u6392\u5e8f\u6548\u679c\uff08R2 = 0.988\uff09\u3002", "conclusion": "\u81ea\u52a8\u5316\u6392\u5e8f\u80fd\u6709\u6548\u4f18\u5316\u53f8\u6cd5\u6d41\u7a0b\uff0c\u51cf\u5c11\u6848\u4ef6\u79ef\u538b\uff0c\u63d0\u5347\u516c\u6b63\u6027\uff1bLLM\u5d4c\u5165\u6280\u672f\u867d\u7136\u6548\u679c\u6709\u9650\uff0c\u4f46\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "keywords": "\u6cd5\u5f8b\u8bf7\u613f\u6392\u5e8f\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u673a\u5668\u5b66\u4e60\uff0c\u53f8\u6cd5\u6548\u7387\uff0c\u81ea\u52a8\u5316\u4f18\u5148\u7ea7"}}
{"id": "2505.21573", "pdf": "https://arxiv.org/pdf/2505.21573", "abs": "https://arxiv.org/abs/2505.21573", "authors": ["Han Wan", "Rui Zhang", "Hao Sun"], "title": "Spectral-inspired Neural Operator for Data-efficient PDE Simulation in Physics-agnostic Regimes", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Partial differential equations (PDEs) govern the spatiotemporal evolution of\nvarious physical systems. Classical numerical solvers, while accurate, require\nfine discretization and full knowledge of the governing PDEs, limiting their\napplicability when the physics is unknown or fast inference is required.\nData-driven neural PDE solvers alleviate these constraints by learning from\ndata but demand large training datasets and perform poorly in data-scarce\nregimes. Physics-aware methods mitigate data requirements by incorporating\nphysical knowledge yet rely on known PDE terms or local numerical schemes,\nrestricting their ability to handle unknown or globally coupled systems. In\nthis work, we propose the Spectral-inspired Neural Operator (SINO), a novel\nframework that learns PDE operators from limited trajectories (as few as 2-5),\nwithout any known PDE terms. SINO operates in the frequency domain and\nintroduces a Frequency-to-Vector module to learn spectral representations\nanalogous to derivative multipliers. To model nonlinear physical interactions,\nwe design a nonlinear operator block that includes a $\\Pi$-Block with low-pass\nfiltering to prevent aliasing. Finally, we introduce an operator distillation\ntechnique to distill the trained model for efficient inference. SINO achieves\nstate-of-the-art results across multiple PDE benchmarks, demonstrating strong\ndiscretization invariance and robust generalization to out-of-distribution\ninitial conditions. To our knowledge, SINO is the first physics-aware method\ncapable of accurately simulating globally coupled systems (e.g., the\nNavier-Stokes equations) from limited data without any explicit PDE terms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSINO\u7684\u9891\u8c31\u542f\u53d1\u795e\u7ecf\u7b97\u5b50\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4ec5\u9700\u5c11\u91cf\u8f68\u8ff9\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u65e0\u9700\u5df2\u77e5PDE\u9879\u5373\u53ef\u5b66\u4e60\u504f\u5fae\u5206\u65b9\u7a0b\u7b97\u5b50\uff0c\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u6c42\u89e3\u5668\u9700\u8981\u7cbe\u786e\u79bb\u6563\u5316\u548c\u5df2\u77e5PDE\u9879\uff0c\u6570\u636e\u9a71\u52a8\u7684\u795e\u7ecf\u6c42\u89e3\u5668\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u800c\u73b0\u6709\u7684\u7269\u7406\u611f\u77e5\u65b9\u6cd5\u4f9d\u8d56\u5df2\u77e5PDE\u9879\u6216\u5c40\u90e8\u6570\u503c\u65b9\u6848\u3002SINO\u65e8\u5728\u901a\u8fc7\u9891\u8c31\u57df\u5b66\u4e60\u548c\u9891\u57df-\u5411\u91cf\u6a21\u5757\u7f13\u89e3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "SINO\u5728\u9891\u57df\u4e2d\u64cd\u4f5c\uff0c\u5f15\u5165Frequency-to-Vector\u6a21\u5757\u5b66\u4e60\u9891\u8c31\u8868\u793a\uff0c\u8bbe\u8ba1\u975e\u7ebf\u6027\u7b97\u5b50\u5757\uff08\u542b\u4f4e\u901a\u6ee4\u6ce2\u9632\u6b62\u6df7\u53e0\uff09\uff0c\u5e76\u901a\u8fc7\u7b97\u5b50\u84b8\u998f\u6280\u672f\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "result": "SINO\u5728\u591a\u4e2aPDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5c55\u793a\u4e86\u8f83\u5f3a\u7684\u79bb\u6563\u4e0d\u53d8\u6027\u548c\u5bf9\u5206\u5e03\u5916\u521d\u59cb\u6761\u4ef6\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "SINO\u662f\u9996\u4e2a\u65e0\u9700\u663e\u5f0fPDE\u9879\u5373\u53ef\u4ece\u6709\u9650\u6570\u636e\u4e2d\u51c6\u786e\u6a21\u62df\u5168\u5c40\u8026\u5408\u7cfb\u7edf\u7684\u7269\u7406\u611f\u77e5\u65b9\u6cd5\u3002", "keywords": "\u504f\u5fae\u5206\u65b9\u7a0b,\u795e\u7ecf\u7b97\u5b50,\u9891\u8c31\u57df\u5b66\u4e60,\u6570\u636e\u7a00\u7f3a,\u7b97\u5b50\u84b8\u998f"}}
{"id": "2505.21784", "pdf": "https://arxiv.org/pdf/2505.21784", "abs": "https://arxiv.org/abs/2505.21784", "authors": ["Tharindu Kumarage", "Ninareh Mehrabi", "Anil Ramakrishna", "Xinyan Zhao", "Richard Zemel", "Kai-Wei Chang", "Aram Galstyan", "Rahul Gupta", "Charith Peris"], "title": "Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025 (Findings)", "summary": "Safety reasoning is a recent paradigm where LLMs reason over safety policies\nbefore generating responses, thereby mitigating limitations in existing safety\nmeasures such as over-refusal and jailbreak vulnerabilities. However,\nimplementing this paradigm is challenging due to the resource-intensive process\nof creating high-quality policy-embedded chain-of-thought (CoT) datasets while\nensuring reasoning remains accurate and free from hallucinations or policy\nconflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation\nfor Safety Reasoning, a novel data generation recipe that leverages multi-agent\ndeliberation to iteratively expand reasoning on safety policies. A data refiner\nstage in AIDSAFE ensures high-quality outputs by eliminating repetitive,\nredundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong\nfoundation for supervised fine-tuning (SFT)-based safety training.\nAdditionally, to address the need of preference data in alignment stages, such\nas DPO training, we introduce a supplemental recipe that uses belief\naugmentation to create distinct selected and rejected CoT samples. Our\nevaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy\nadherence and reasoning quality. Consequently, we show that fine-tuning\nopen-source LLMs on these CoTs can significantly improve safety generalization\nand jailbreak robustness while maintaining acceptable utility and over-refusal\naccuracy. AIDSAFE-generated CoT datasets can be found here:\nhttps://huggingface.co/datasets/AmazonScience/AIDSAFE", "AI": {"tldr": "AIDSAFE is a method for generating high-quality safety policy reasoning data using multi-agent deliberation and iterative refinement to improve LLM safety training.", "motivation": "Existing safety measures in LLMs face issues like over-refusal and jailbreak vulnerabilities. Current methods for creating policy-embedded CoT datasets are resource-intensive and prone to inaccuracies.", "method": "AIDSAFE uses multi-agent deliberation to iteratively expand safety policy reasoning, with a data refiner stage to eliminate low-quality thoughts. It also introduces belief augmentation for preference data in alignment stages.", "result": "AIDSAFE-generated CoTs show superior policy adherence and reasoning quality, significantly improving safety generalization and jailbreak robustness in fine-tuned LLMs.", "conclusion": "AIDSAFE provides an effective solution for generating high-quality safety reasoning data, enhancing LLM safety training while maintaining utility and accuracy.", "keywords": "safety reasoning, LLMs, AIDSAFE, multi-agent deliberation, policy adherence, jailbreak robustness"}}
{"id": "2505.21693", "pdf": "https://arxiv.org/pdf/2505.21693", "abs": "https://arxiv.org/abs/2505.21693", "authors": ["Raoyuan Zhao", "Beiduo Chen", "Barbara Plank", "Michael A. Hedderich"], "title": "MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are used globally across many languages, but\ntheir English-centric pretraining raises concerns about cross-lingual\ndisparities for cultural awareness, often resulting in biased outputs. However,\ncomprehensive multilingual evaluation remains challenging due to limited\nbenchmarks and questionable translation quality. To better assess these\ndisparities, we introduce MAKIEval, an automatic multilingual framework for\nevaluating cultural awareness in LLMs across languages, regions, and topics.\nMAKIEval evaluates open-ended text generation, capturing how models express\nculturally grounded knowledge in natural language. Leveraging Wikidata's\nmultilingual structure as a cross-lingual anchor, it automatically identifies\ncultural entities in model outputs and links them to structured knowledge,\nenabling scalable, language-agnostic evaluation without manual annotation or\ntranslation. We then introduce four metrics that capture complementary\ndimensions of cultural awareness: granularity, diversity, cultural specificity,\nand consensus across languages. We assess 7 LLMs developed from different parts\nof the world, encompassing both open-source and proprietary systems, across 13\nlanguages, 19 countries and regions, and 6 culturally salient topics (e.g.,\nfood, clothing). Notably, we find that models tend to exhibit stronger cultural\nawareness in English, suggesting that English prompts more effectively activate\nculturally grounded knowledge. We publicly release our code and data.", "AI": {"tldr": "This paper introduces MAKIEval, a multilingual framework for evaluating cultural awareness in large language models (LLMs) across languages, regions, and topics, addressing biases in English-centric models.", "motivation": "The study aims to tackle cross-lingual disparities and biases in LLMs, especially due to their English-centric pretraining, which often leads to culturally unaware outputs in other languages.", "method": "MAKIEval uses Wikidata\u2019s multilingual structure to automatically identify cultural entities in LLM outputs and links them to structured knowledge, enabling scalable evaluation. Four metrics (granularity, diversity, cultural specificity, consensus) are introduced to assess cultural awareness.", "result": "Evaluation of 7 LLMs across 13 languages, 19 regions, and 6 topics revealed that models show stronger cultural awareness in English, suggesting English prompts better activate cultural knowledge.", "conclusion": "MAKIEval provides a scalable, language-agnostic tool for assessing cultural awareness in LLMs, highlighting the need for more balanced multilingual training.", "keywords": "large language models, cultural awareness, multilingual evaluation, bias, MAKIEval"}}
{"id": "2505.21576", "pdf": "https://arxiv.org/pdf/2505.21576", "abs": "https://arxiv.org/abs/2505.21576", "authors": ["Jiawei Tang", "Yuheng Jia"], "title": "Concentration Distribution Learning from Label Distributions", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Label distribution learning (LDL) is an effective method to predict the\nrelative label description degree (a.k.a. label distribution) of a sample.\nHowever, the label distribution is not a complete representation of an instance\nbecause it overlooks the absolute intensity of each label. Specifically, it's\nimpossible to obtain the total description degree of hidden labels that not in\nthe label space, which leads to the loss of information and confusion in\ninstances. To solve the above problem, we come up with a new concept named\nbackground concentration to serve as the absolute description degree term of\nthe label distribution and introduce it into the LDL process, forming the\nimproved paradigm of concentration distribution learning. Moreover, we propose\na novel model by probabilistic methods and neural networks to learn label\ndistributions and background concentrations from existing LDL datasets.\nExtensive experiments prove that the proposed approach is able to extract\nbackground concentrations from label distributions while producing more\naccurate prediction results than the state-of-the-art LDL methods. The code is\navailable in https://github.com/seutjw/CDL-LD.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u80cc\u666f\u6d53\u5ea6\u7684\u65b0\u6982\u5ff5\uff0c\u7528\u4e8e\u6539\u8fdb\u6807\u7b7e\u5206\u5e03\u5b66\u4e60\uff08LDL\uff09\uff0c\u5f25\u8865\u4f20\u7edfLDL\u5ffd\u7565\u6807\u7b7e\u7edd\u5bf9\u5f3a\u5ea6\u7684\u7f3a\u9677\uff0c\u5e76\u901a\u8fc7\u6982\u7387\u65b9\u6cd5\u548c\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u6807\u7b7e\u5206\u5e03\u4e0e\u80cc\u666f\u6d53\u5ea6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u6807\u7b7e\u5206\u5e03\u5b66\u4e60\uff08LDL\uff09\u4ec5\u5173\u6ce8\u6807\u7b7e\u7684\u76f8\u5bf9\u63cf\u8ff0\u7a0b\u5ea6\uff0c\u5ffd\u7565\u4e86\u9690\u85cf\u6807\u7b7e\u7684\u7edd\u5bf9\u5f3a\u5ea6\u548c\u603b\u63cf\u8ff0\u5ea6\uff0c\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u548c\u5b9e\u4f8b\u6df7\u6dc6\u3002\u4e3a\u6b64\uff0c\u8bba\u6587\u63d0\u51fa\u5f15\u5165\u80cc\u666f\u6d53\u5ea6\u4f5c\u4e3a\u7edd\u5bf9\u63cf\u8ff0\u9879\uff0c\u5b8c\u5584\u6807\u7b7e\u5206\u5e03\u8868\u793a\u3002", "method": "\u63d0\u51fa\u80cc\u666f\u6d53\u5ea6\u6982\u5ff5\u5e76\u5c06\u5176\u6574\u5408\u5230LDL\u4e2d\uff0c\u5f62\u6210\u6d53\u5ea6\u5206\u5e03\u5b66\u4e60\uff08CDL\uff09\u8303\u5f0f\uff1b\u8bbe\u8ba1\u57fa\u4e8e\u6982\u7387\u65b9\u6cd5\u548c\u795e\u7ecf\u7f51\u7edc\u7684\u6a21\u578b\uff0c\u4ece\u73b0\u6709LDL\u6570\u636e\u96c6\u4e2d\u540c\u65f6\u5b66\u4e60\u6807\u7b7e\u5206\u5e03\u548c\u80cc\u666f\u6d53\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u4ece\u6807\u7b7e\u5206\u5e03\u4e2d\u63d0\u53d6\u80cc\u666f\u6d53\u5ea6\uff0c\u4e14\u9884\u6d4b\u7cbe\u5ea6\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684LDL\u65b9\u6cd5\u3002", "conclusion": "\u80cc\u666f\u6d53\u5ea6\u7684\u5f15\u5165\u663e\u8457\u63d0\u5347\u4e86\u6807\u7b7e\u5206\u5e03\u5b66\u4e60\u7684\u5b8c\u6574\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3aLDL\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u6539\u8fdb\u65b9\u5411\u3002", "keywords": "\u6807\u7b7e\u5206\u5e03\u5b66\u4e60, \u80cc\u666f\u6d53\u5ea6, \u6d53\u5ea6\u5206\u5e03\u5b66\u4e60, \u6982\u7387\u65b9\u6cd5, \u795e\u7ecf\u7f51\u7edc"}}
{"id": "2505.21828", "pdf": "https://arxiv.org/pdf/2505.21828", "abs": "https://arxiv.org/abs/2505.21828", "authors": ["Chen Yueh-Han", "Guy Davidson", "Brenden M. Lake"], "title": "SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety Facts", "categories": ["cs.AI"], "comment": null, "summary": "Do LLMs robustly generalize critical safety facts to novel situations?\nLacking this ability is dangerous when users ask naive questions. For instance,\n\"I'm considering packing melon balls for my 10-month-old's lunch. What other\nfoods would be good to include?\" Before offering food options, the LLM should\nwarn that melon balls pose a choking hazard to toddlers, as documented by the\nCDC. Failing to provide such warnings could result in serious injuries or even\ndeath. To evaluate this, we introduce SAGE-Eval, SAfety-fact systematic\nGEneralization evaluation, the first benchmark that tests whether LLMs properly\napply well established safety facts to naive user queries. SAGE-Eval comprises\n104 facts manually sourced from reputable organizations, systematically\naugmented to create 10,428 test scenarios across 7 common domains (e.g.,\nOutdoor Activities, Medicine). We find that the top model, Claude-3.7-sonnet,\npasses only 58% of all the safety facts tested. We also observe that model\ncapabilities and training compute weakly correlate with performance on\nSAGE-Eval, implying that scaling up is not the golden solution. Our findings\nsuggest frontier LLMs still lack robust generalization ability. We recommend\ndevelopers use SAGE-Eval in pre-deployment evaluations to assess model\nreliability in addressing salient risks. We publicly release SAGE-Eval at\nhttps://huggingface.co/datasets/YuehHanChen/SAGE-Eval and our code is available\nat https://github.com/YuehHanChen/SAGE-Eval/tree/main.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faSAGE-Eval\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u662f\u5426\u80fd\u5c06\u5df2\u77e5\u5b89\u5168\u77e5\u8bc6\u63a8\u5e7f\u5230\u65b0\u60c5\u5883\u4e2d\uff0c\u53d1\u73b0\u5f53\u524d\u6700\u4f73\u6a21\u578b\u4ec5\u901a\u8fc758%\u7684\u6d4b\u8bd5\uff0c\u5e76\u5efa\u8bae\u5f00\u53d1\u8005\u5728\u90e8\u7f72\u524d\u4f7f\u7528\u6b64\u57fa\u51c6\u8bc4\u4f30\u6a21\u578b\u53ef\u9760\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u9a8c\u8bc1LLMs\u662f\u5426\u80fd\u5728\u9762\u5bf9\u7528\u6237\u5929\u771f\u63d0\u95ee\u65f6\uff0c\u6b63\u786e\u5e94\u7528\u5df2\u77e5\u5b89\u5168\u77e5\u8bc6\uff08\u5982\u907f\u514d\u7a92\u606f\u98ce\u9669\uff09\uff0c\u4ee5\u907f\u514d\u6f5c\u5728\u4f24\u5bb3\u6216\u6b7b\u4ea1\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6784\u5efaSAGE-Eval\u57fa\u51c6\uff0c\u6db5\u76d6104\u6761\u6743\u5a01\u5b89\u5168\u4e8b\u5b9e\uff0c\u5e76\u901a\u8fc7\u7cfb\u7edf\u6269\u5c55\u751f\u621010,428\u4e2a\u6d4b\u8bd5\u573a\u666f\uff0c\u8986\u76d67\u4e2a\u5e38\u89c1\u9886\u57df\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u9876\u7ea7\u6a21\u578bClaude-3.7-sonnet\u4ec5\u901a\u8fc758%\u7684\u5b89\u5168\u4e8b\u5b9e\u6d4b\u8bd5\uff0c\u4e14\u6a21\u578b\u80fd\u529b\u4e0e\u8bad\u7ec3\u8ba1\u7b97\u91cf\u5bf9\u6027\u80fd\u7684\u5173\u8054\u6027\u8f83\u5f31\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u524d\u6cbfLLMs\u4ecd\u7f3a\u4e4f\u7a33\u5065\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5efa\u8bae\u5f00\u53d1\u8005\u4f7f\u7528SAGE-Eval\u8fdb\u884c\u9884\u90e8\u7f72\u8bc4\u4f30\u3002", "keywords": "LLM, \u5b89\u5168\u8bc4\u4f30, SAGE-Eval, \u6cdb\u5316\u80fd\u529b, \u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2505.21701", "pdf": "https://arxiv.org/pdf/2505.21701", "abs": "https://arxiv.org/abs/2505.21701", "authors": ["Raoyuan Zhao", "Abdullatif K\u00f6ksal", "Ali Modarressi", "Michael A. Hedderich", "Hinrich Sch\u00fctze"], "title": "Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge Probing", "categories": ["cs.CL"], "comment": null, "summary": "The reliability of large language models (LLMs) is greatly compromised by\ntheir tendency to hallucinate, underscoring the need for precise identification\nof knowledge gaps within LLMs. Various methods for probing such gaps exist,\nranging from calibration-based to prompting-based methods. To evaluate these\nprobing methods, in this paper, we propose a new process based on using input\nvariations and quantitative metrics. Through this, we expose two dimensions of\ninconsistency in knowledge gap probing. (1) Intra-method inconsistency: Minimal\nnon-semantic perturbations in prompts lead to considerable variance in detected\nknowledge gaps within the same probing method; e.g., the simple variation of\nshuffling answer options can decrease agreement to around 40%. (2) Cross-method\ninconsistency: Probing methods contradict each other on whether a model knows\nthe answer. Methods are highly inconsistent -- with decision consistency across\nmethods being as low as 7% -- even though the model, dataset, and prompt are\nall the same. These findings challenge existing probing methods and highlight\nthe urgent need for perturbation-robust probing frameworks.", "AI": {"tldr": "\u6458\u8981\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u77e5\u8bc6\u7f3a\u53e3\u63a2\u6d4b\u65b9\u6cd5\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u8f93\u5165\u53d8\u5316\u548c\u5b9a\u91cf\u6307\u6807\u7684\u8bc4\u4f30\u8fc7\u7a0b\uff0c\u63ed\u793a\u4e86\u63a2\u6d4b\u65b9\u6cd5\u5728\u76f8\u540c\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u63d0\u793a\u4e0b\u7684\u4e0d\u4e00\u81f4\u6027\u3002", "motivation": "\u7531\u4e8eLLMs\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u7cbe\u786e\u8bc6\u522b\u5176\u77e5\u8bc6\u7f3a\u53e3\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u63a2\u6d4b\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u53d7\u5230\u4e0d\u4e00\u81f4\u6027\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u8f93\u5165\u53d8\u5316\u548c\u5b9a\u91cf\u6307\u6807\uff0c\u8bc4\u4f30\u63a2\u6d4b\u65b9\u6cd5\u7684\u4e00\u81f4\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "\u53d1\u73b0\u63a2\u6d4b\u65b9\u6cd5\u5728\u5185\u90e8\uff08\u5982\u63d0\u793a\u5fae\u5c0f\u53d8\u5316\uff09\u548c\u8de8\u65b9\u6cd5\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u4e0d\u4e00\u81f4\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5f00\u53d1\u5bf9\u6297\u6270\u52a8\u7684\u9c81\u68d2\u63a2\u6d4b\u6846\u67b6\u7684\u7d27\u8feb\u6027\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u77e5\u8bc6\u7f3a\u53e3\u63a2\u6d4b, \u4e0d\u4e00\u81f4\u6027, \u9c81\u68d2\u6027"}}
{"id": "2505.21584", "pdf": "https://arxiv.org/pdf/2505.21584", "abs": "https://arxiv.org/abs/2505.21584", "authors": ["Afaf Taik", "Khaoula Chehbouni", "Golnoosh Farnadi"], "title": "Fairness in Federated Learning: Fairness for Whom?", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": null, "summary": "Fairness in federated learning has emerged as a rapidly growing area of\nresearch, with numerous works proposing formal definitions and algorithmic\ninterventions. Yet, despite this technical progress, fairness in FL is often\ndefined and evaluated in ways that abstract away from the sociotechnical\ncontexts in which these systems are deployed. In this paper, we argue that\nexisting approaches tend to optimize narrow system level metrics, such as\nperformance parity or contribution-based rewards, while overlooking how harms\narise throughout the FL lifecycle and how they impact diverse stakeholders. We\nsupport this claim through a critical analysis of the literature, based on a\nsystematic annotation of papers for their fairness definitions, design\ndecisions, evaluation practices, and motivating use cases. Our analysis reveals\nfive recurring pitfalls: 1) fairness framed solely through the lens of server\nclient architecture, 2) a mismatch between simulations and motivating use-cases\nand contexts, 3) definitions that conflate protecting the system with\nprotecting its users, 4) interventions that target isolated stages of the\nlifecycle while neglecting upstream and downstream effects, 5) and a lack of\nmulti-stakeholder alignment where multiple fairness definitions can be relevant\nat once. Building on these insights, we propose a harm centered framework that\nlinks fairness definitions to concrete risks and stakeholder vulnerabilities.\nWe conclude with recommendations for more holistic, context-aware, and\naccountable fairness research in FL.", "AI": {"tldr": "\u8bba\u6587\u6279\u5224\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u516c\u5e73\u6027\u7814\u7a76\u8fc7\u4e8e\u6280\u672f\u5316\uff0c\u5ffd\u89c6\u4e86\u793e\u4f1a\u6280\u672f\u4e0a\u4e0b\u6587\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ee5\u5371\u5bb3\u4e3a\u4e2d\u5fc3\u7684\u6846\u67b6\uff0c\u5e76\u5efa\u8bae\u66f4\u5168\u9762\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u516c\u5e73\u6027\u7814\u7a76\u8fc7\u4e8e\u5173\u6ce8\u7cfb\u7edf\u7ea7\u6307\u6807\uff0c\u5ffd\u89c6\u4e86\u793e\u4f1a\u6280\u672f\u4e0a\u4e0b\u6587\u4e2d\u7684\u5b9e\u9645\u5371\u5bb3\u548c\u591a\u5229\u76ca\u76f8\u5173\u8005\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5bf9\u6587\u732e\u7684\u7cfb\u7edf\u6807\u6ce8\u5206\u6790\uff0c\u8bc6\u522b\u4e86\u4e94\u4e2a\u5e38\u89c1\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5371\u5bb3\u4e2d\u5fc3\u6846\u67b6\u3002", "result": "\u63ed\u793a\u4e86\u73b0\u6709\u7814\u7a76\u7684\u4e94\u4e2a\u5e38\u89c1\u7f3a\u9677\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u516c\u5e73\u6027\u5b9a\u4e49\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "\u5efa\u8bae\u672a\u6765\u7684\u7814\u7a76\u5e94\u66f4\u5177\u6574\u4f53\u6027\u3001\u4e0a\u4e0b\u6587\u610f\u8bc6\u548c\u5bf9\u591a\u5229\u76ca\u76f8\u5173\u8005\u7684\u8d23\u4efb\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60, \u516c\u5e73\u6027, \u793e\u4f1a\u6280\u672f, \u5371\u5bb3\u4e2d\u5fc3\u6846\u67b6, \u591a\u5229\u76ca\u76f8\u5173\u8005"}}
{"id": "2505.21887", "pdf": "https://arxiv.org/pdf/2505.21887", "abs": "https://arxiv.org/abs/2505.21887", "authors": ["Ahmed Heakl", "Yahia Salaheldin Shaaban", "Martin Takac", "Salem Lahlou", "Zangir Iklassov"], "title": "SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem", "categories": ["cs.AI", "cs.CE", "cs.LG"], "comment": "18 pages, 14 figures, 11 tables", "summary": "Robust routing under uncertainty is central to real-world logistics, yet most\nbenchmarks assume static, idealized settings. We present SVRPBench, the first\nopen benchmark to capture high-fidelity stochastic dynamics in vehicle routing\nat urban scale. Spanning more than 500 instances with up to 1000 customers, it\nsimulates realistic delivery conditions: time-dependent congestion, log-normal\ndelays, probabilistic accidents, and empirically grounded time windows for\nresidential and commercial clients. Our pipeline generates diverse,\nconstraint-rich scenarios, including multi-depot and multi-vehicle setups.\nBenchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade\nby over 20% under distributional shift, while classical and metaheuristic\nmethods remain robust. To enable reproducible research, we release the dataset\nand evaluation suite. SVRPBench challenges the community to design solvers that\ngeneralize beyond synthetic assumptions and adapt to real-world uncertainty.", "AI": {"tldr": "SVRPBench\u662f\u4e00\u4e2a\u5f00\u653e\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u6a21\u62df\u57ce\u5e02\u89c4\u6a21\u4e0b\u8f66\u8f86\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u9ad8\u4fdd\u771f\u968f\u673a\u52a8\u6001\uff0c\u5305\u542b500\u591a\u4e2a\u5b9e\u4f8b\uff0c\u6d4b\u8bd5\u663e\u793a\u73b0\u6709RL\u6c42\u89e3\u5668\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u6027\u80fd\u4e0b\u964d\u660e\u663e\u3002", "motivation": "\u73b0\u6709\u8f66\u8f86\u8def\u5f84\u89c4\u5212\u57fa\u51c6\u5927\u591a\u5047\u8bbe\u9759\u6001\u7406\u60f3\u6761\u4ef6\uff0c\u800c\u73b0\u5b9e\u4e2d\u5b58\u5728\u5927\u91cf\u4e0d\u786e\u5b9a\u6027\uff0cSVRPBench\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u7684\u7ea6\u675f\u4e30\u5bcc\u573a\u666f\uff08\u5982\u591a\u4ed3\u5e93\u3001\u591a\u8f66\u8f86\u8bbe\u7f6e\uff09\u6a21\u62df\u771f\u5b9e\u914d\u9001\u6761\u4ef6\uff0c\u5305\u62ec\u65f6\u95f4\u4f9d\u8d56\u6027\u62e5\u5835\u3001\u5bf9\u6570\u6b63\u6001\u5ef6\u8fdf\u7b49\u3002", "result": "\u6d4b\u8bd5\u53d1\u73b0\u5148\u8fdb\u7684RL\u6c42\u89e3\u5668\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u6027\u80fd\u4e0b\u964d\u8d85\u8fc720%\uff0c\u800c\u7ecf\u5178\u548c\u5143\u542f\u53d1\u5f0f\u65b9\u6cd5\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "SVRPBench\u547c\u5401\u8bbe\u8ba1\u80fd\u591f\u8d85\u8d8a\u5408\u6210\u5047\u8bbe\u3001\u9002\u5e94\u73b0\u5b9e\u4e0d\u786e\u5b9a\u6027\u7684\u6c42\u89e3\u5668\u3002", "keywords": "SVRPBench, \u8f66\u8f86\u8def\u5f84\u89c4\u5212, \u4e0d\u786e\u5b9a\u6027, \u57fa\u51c6\u6d4b\u8bd5, \u5f3a\u5316\u5b66\u4e60"}}
{"id": "2505.21710", "pdf": "https://arxiv.org/pdf/2505.21710", "abs": "https://arxiv.org/abs/2505.21710", "authors": ["Barbarestani Baran", "Maks Isa", "Vossen Piek"], "title": "Assessing and Refining ChatGPT's Performance in Identifying Targeting and Inappropriate Language: A Comparative Study", "categories": ["cs.CL"], "comment": null, "summary": "This study evaluates the effectiveness of ChatGPT, an advanced AI model for\nnatural language processing, in identifying targeting and inappropriate\nlanguage in online comments. With the increasing challenge of moderating vast\nvolumes of user-generated content on social network sites, the role of AI in\ncontent moderation has gained prominence. We compared ChatGPT's performance\nagainst crowd-sourced annotations and expert evaluations to assess its\naccuracy, scope of detection, and consistency. Our findings highlight that\nChatGPT performs well in detecting inappropriate content, showing notable\nimprovements in accuracy through iterative refinements, particularly in Version\n6. However, its performance in targeting language detection showed variability,\nwith higher false positive rates compared to expert judgments. This study\ncontributes to the field by demonstrating the potential of AI models like\nChatGPT to enhance automated content moderation systems while also identifying\nareas for further improvement. The results underscore the importance of\ncontinuous model refinement and contextual understanding to better support\nautomated moderation and mitigate harmful online behavior.", "AI": {"tldr": "\u8bc4\u4f30ChatGPT\u5728\u8bc6\u522b\u5728\u7ebf\u8bc4\u8bba\u4e2d\u7684\u4e0d\u5f53\u53ca\u9488\u5bf9\u6027\u8bed\u8a00\u7684\u6548\u679c\uff0c\u663e\u793a\u5176\u5728\u68c0\u6d4b\u4e0d\u5f53\u5185\u5bb9\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9488\u5bf9\u6027\u8bed\u8a00\u68c0\u6d4b\u4e0a\u5b58\u5728\u53d8\u5f02\u6027\u3002", "motivation": "\u7814\u7a76AI\u6a21\u578b\u5728\u5185\u5bb9\u5ba1\u6838\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u6d77\u91cf\u7528\u6237\u751f\u6210\u5185\u5bb9\u65f6\uff0c\u5982\u4f55\u63d0\u5347\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u6548\u679c\u3002", "method": "\u5bf9\u6bd4ChatGPT\u4e0e\u4eba\u5de5\u6807\u6ce8\uff08\u4f17\u5305\u548c\u4e13\u5bb6\u8bc4\u4f30\uff09\uff0c\u8bc4\u4f30\u5176\u51c6\u786e\u6027\u3001\u68c0\u6d4b\u8303\u56f4\u53ca\u4e00\u81f4\u6027\u3002", "result": "ChatGPT\u68c0\u6d4b\u4e0d\u5f53\u5185\u5bb9\u8868\u73b0\u4f18\u5f02\uff08\u5c24\u5176\u662f\u8fed\u4ee3\u4f18\u5316\u7684V6\u7248\u672c\uff09\uff0c\u4f46\u5728\u9488\u5bf9\u6027\u8bed\u8a00\u68c0\u6d4b\u4e2d\u5047\u9633\u6027\u7387\u8f83\u9ad8\u3002", "conclusion": "AI\u6a21\u578b\uff08\u5982ChatGPT\uff09\u53ef\u589e\u5f3a\u81ea\u52a8\u5316\u5185\u5bb9\u5ba1\u6838\u7cfb\u7edf\uff0c\u4f46\u9700\u6301\u7eed\u4f18\u5316\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u4ee5\u51cf\u5c11\u8bef\u5224\u3002", "keywords": "ChatGPT, \u5185\u5bb9\u5ba1\u6838, \u4e0d\u5f53\u8bed\u8a00\u68c0\u6d4b, \u9488\u5bf9\u6027\u8bed\u8a00, AI\u6a21\u578b\u4f18\u5316"}}
{"id": "2505.21587", "pdf": "https://arxiv.org/pdf/2505.21587", "abs": "https://arxiv.org/abs/2505.21587", "authors": ["Bin Qin", "Qirui Ji", "Jiangmeng Li", "Yupeng Wang", "Xuesong Wu", "Jianwen Cao", "Fanjiang Xu"], "title": "CellCLAT: Preserving Topology and Trimming Redundancy in Self-Supervised Cellular Contrastive Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Self-supervised topological deep learning (TDL) represents a nascent but\nunderexplored area with significant potential for modeling higher-order\ninteractions in simplicial complexes and cellular complexes to derive\nrepresentations of unlabeled graphs. Compared to simplicial complexes, cellular\ncomplexes exhibit greater expressive power. However, the advancement in\nself-supervised learning for cellular TDL is largely hindered by two core\nchallenges: \\textit{extrinsic structural constraints} inherent to cellular\ncomplexes, and intrinsic semantic redundancy in cellular representations. The\nfirst challenge highlights that traditional graph augmentation techniques may\ncompromise the integrity of higher-order cellular interactions, while the\nsecond underscores that topological redundancy in cellular complexes\npotentially diminish task-relevant information. To address these issues, we\nintroduce Cellular Complex Contrastive Learning with Adaptive Trimming\n(CellCLAT), a twofold framework designed to adhere to the combinatorial\nconstraints of cellular complexes while mitigating informational redundancy.\nSpecifically, we propose a parameter perturbation-based augmentation method\nthat injects controlled noise into cellular interactions without altering the\nunderlying cellular structures, thereby preserving cellular topology during\ncontrastive learning. Additionally, a cellular trimming scheduler is employed\nto mask gradient contributions from task-irrelevant cells through a bi-level\nmeta-learning approach, effectively removing redundant topological elements\nwhile maintaining critical higher-order semantics. We provide theoretical\njustification and empirical validation to demonstrate that CellCLAT achieves\nsubstantial improvements over existing self-supervised graph learning methods,\nmarking a significant attempt in this domain.", "AI": {"tldr": "CellCLAT\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u62d3\u6251\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u6570\u6270\u52a8\u589e\u5f3a\u548c\u5143\u5b66\u4e60\u4fee\u526a\uff0c\u89e3\u51b3\u4e86\u7ec6\u80de\u590d\u5408\u4f53\u7684\u7ed3\u6784\u7ea6\u675f\u548c\u8bed\u4e49\u5197\u4f59\u95ee\u9898\u3002", "motivation": "\u7ec6\u80de\u590d\u5408\u4f53\u867d\u6bd4\u5355\u7eaf\u590d\u5f62\u66f4\u5177\u8868\u8fbe\u529b\uff0c\u4f46\u5176\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u8fdb\u5c55\u53d7\u9650\u4e8e\u7ed3\u6784\u7ea6\u675f\u548c\u8bed\u4e49\u5197\u4f59\u4e24\u5927\u6311\u6218\u3002", "method": "\u63d0\u51faCellCLAT\u6846\u67b6\uff0c\u5305\u542b\u53c2\u6570\u6270\u52a8\u589e\u5f3a\uff08\u4fdd\u7559\u62d3\u6251\u7ed3\u6784\uff09\u548c\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u7ec6\u80de\u4fee\u526a\uff08\u79fb\u9664\u5197\u4f59\u4fe1\u606f\uff09\u3002", "result": "\u7406\u8bba\u53ca\u5b9e\u9a8c\u8bc1\u660eCellCLAT\u4f18\u4e8e\u73b0\u6709\u81ea\u76d1\u7763\u56fe\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "CellCLAT\u4e3a\u7ec6\u80de\u590d\u5408\u4f53\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "keywords": "\u81ea\u76d1\u7763\u5b66\u4e60, \u62d3\u6251\u6df1\u5ea6\u5b66\u4e60, \u7ec6\u80de\u590d\u5408\u4f53, \u5bf9\u6bd4\u5b66\u4e60, \u5143\u5b66\u4e60"}}
{"id": "2505.21907", "pdf": "https://arxiv.org/pdf/2505.21907", "abs": "https://arxiv.org/abs/2505.21907", "authors": ["Saleh Afzoon", "Zahra Jahanandish", "Phuong Thao Huynh", "Amin Beheshti", "Usman Naseem"], "title": "Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "AI copilots, context-aware, AI-powered systems designed to assist users in\ntasks such as software development and content creation, are becoming integral\nto modern workflows. As these systems grow in capability and adoption,\npersonalization has emerged as a cornerstone for ensuring usability, trust, and\nproductivity. Central to this personalization is preference optimization: the\nability of AI copilots to detect, interpret, and align with individual user\npreferences. While personalization techniques are well-established in domains\nlike recommender systems and dialogue agents, their adaptation to interactive,\nreal-time systems like AI copilots remains fragmented and underexplored. This\nsurvey addresses this gap by synthesizing research on how user preferences are\ncaptured, modeled, and refined within the design of AI copilots. We introduce a\nunified definition of AI copilots and propose a phase-based taxonomy of\npreference optimization strategies, structured around pre-interaction,\nmid-interaction, and post-interaction stages. We analyze techniques for\nacquiring preference signals, modeling user intent, and integrating feedback\nloops, highlighting both established approaches and recent innovations. By\nbridging insights from AI personalization, human-AI collaboration, and large\nlanguage model adaptation, this survey provides a structured foundation for\ndesigning adaptive, preference-aware AI copilots. It offers a holistic view of\nthe available preference resources, how they can be leveraged, and which\ntechnical approaches are most suited to each stage of system design.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86AI\u526f\u9a7e\u9a76\uff08AI copilots\uff09\u5728\u4e2a\u6027\u5316\u504f\u597d\u4f18\u5316\u65b9\u9762\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u63d0\u51fa\u4e86\u504f\u597d\u4f18\u5316\u7684\u9636\u6bb5\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u76f8\u5173\u6280\u672f\u53ca\u5176\u5728\u8bbe\u8ba1\u81ea\u9002\u5e94AI\u526f\u9a7e\u9a76\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u968f\u7740AI\u526f\u9a7e\u9a76\u80fd\u529b\u7684\u63d0\u5347\u548c\u666e\u53ca\uff0c\u4e2a\u6027\u5316\u6210\u4e3a\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u548c\u6548\u7387\u7684\u5173\u952e\uff0c\u4f46\u76ee\u524d\u76f8\u5173\u6280\u672f\u5728\u5b9e\u65f6\u4ea4\u4e92\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u7cfb\u7edf\u5316\u3002", "method": "\u901a\u8fc7\u8c03\u7814AI\u526f\u9a7e\u9a76\u4e2d\u7528\u6237\u504f\u597d\u7684\u6355\u83b7\u3001\u5efa\u6a21\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u63d0\u51fa\u57fa\u4e8e\u4ea4\u4e92\u9636\u6bb5\uff08\u524d\u3001\u4e2d\u3001\u540e\uff09\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u5206\u6790\u6280\u672f\u624b\u6bb5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u5957\u7edf\u4e00\u7684AI\u526f\u9a7e\u9a76\u5b9a\u4e49\u548c\u504f\u597d\u4f18\u5316\u5206\u7c7b\u6cd5\uff0c\u603b\u7ed3\u4e86\u73b0\u6709\u6280\u672f\u548c\u521b\u65b0\u70b9\uff0c\u4e3a\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u57fa\u7840\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8bbe\u8ba1\u504f\u597d\u611f\u77e5\u7684\u81ea\u9002\u5e94AI\u526f\u9a7e\u9a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u6846\u67b6\uff0c\u6574\u5408\u4e86\u591a\u4e2a\u9886\u57df\u7684\u89c1\u89e3\uff0c\u5e76\u660e\u786e\u4e86\u5404\u9636\u6bb5\u9002\u7528\u7684\u6280\u672f\u65b9\u6848\u3002", "keywords": "AI copilots, preference optimization, personalization, human-AI collaboration, large language models"}}
{"id": "2505.21740", "pdf": "https://arxiv.org/pdf/2505.21740", "abs": "https://arxiv.org/abs/2505.21740", "authors": ["Marvin Limpijankit", "Yanda Chen", "Melanie Subbiah", "Nicholas Deas", "Kathleen McKeown"], "title": "Counterfactual Simulatability of LLM Explanations for Generation Tasks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs can be unpredictable, as even slight alterations to the prompt can cause\nthe output to change in unexpected ways. Thus, the ability of models to\naccurately explain their behavior is critical, especially in high-stakes\nsettings. One approach for evaluating explanations is counterfactual\nsimulatability, how well an explanation allows users to infer the model's\noutput on related counterfactuals. Counterfactual simulatability has been\npreviously studied for yes/no question answering tasks. We provide a general\nframework for extending this method to generation tasks, using news\nsummarization and medical suggestion as example use cases. We find that while\nLLM explanations do enable users to better predict LLM outputs on\ncounterfactuals in the summarization setting, there is significant room for\nimprovement for medical suggestion. Furthermore, our results suggest that the\nevaluation for counterfactual simulatability may be more appropriate for\nskill-based tasks as opposed to knowledge-based tasks.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86LLMs\u89e3\u91ca\u884c\u4e3a\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u53ef\u6a21\u62df\u6027\u7684\u901a\u7528\u6846\u67b6\u6765\u8bc4\u4f30\u5176\u89e3\u91ca\u6548\u679c\uff0c\u53d1\u73b0\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4e00\u3002", "motivation": "LLMs\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\u5bfc\u81f4\u5176\u884c\u4e3a\u89e3\u91ca\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4ec5\u9002\u7528\u4e8e\u4e8c\u5143\u95ee\u7b54\u4efb\u52a1\uff0c\u9700\u6269\u5c55\u81f3\u751f\u6210\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u4e86\u53cd\u4e8b\u5b9e\u53ef\u6a21\u62df\u6027\u6846\u67b6\uff0c\u5e94\u7528\u4e8e\u65b0\u95fb\u6458\u8981\u548c\u533b\u7597\u5efa\u8bae\u4efb\u52a1\uff0c\u8bc4\u4f30\u7528\u6237\u9884\u6d4b\u6a21\u578b\u8f93\u51fa\u7684\u80fd\u529b\u3002", "result": "\u65b0\u95fb\u6458\u8981\u4efb\u52a1\u4e2dLLM\u89e3\u91ca\u6709\u6548\u5e2e\u52a9\u7528\u6237\u9884\u6d4b\uff0c\u4f46\u5728\u533b\u7597\u5efa\u8bae\u4e2d\u6548\u679c\u6709\u9650\uff1b\u8bc4\u4f30\u65b9\u6cd5\u66f4\u9002\u7528\u4e8e\u6280\u80fd\u578b\u4efb\u52a1\u800c\u975e\u77e5\u8bc6\u578b\u4efb\u52a1\u3002", "conclusion": "\u53cd\u4e8b\u5b9e\u53ef\u6a21\u62df\u6027\u6846\u67b6\u9002\u7528\u4e8e\u751f\u6210\u4efb\u52a1\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\uff0c\u5c24\u5176\u5728\u77e5\u8bc6\u578b\u4efb\u52a1\u4e2d\u3002", "keywords": "LLMs, \u53cd\u4e8b\u5b9e\u53ef\u6a21\u62df\u6027, \u65b0\u95fb\u6458\u8981, \u533b\u7597\u5efa\u8bae, \u89e3\u91ca\u8bc4\u4f30"}}
{"id": "2505.21591", "pdf": "https://arxiv.org/pdf/2505.21591", "abs": "https://arxiv.org/abs/2505.21591", "authors": ["Maosen Zhao", "Pengtao Chen", "Chong Yu", "Yan Wen", "Xudong Tan", "Tao Chen"], "title": "Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Model quantization reduces the bit-width of weights and activations,\nimproving memory efficiency and inference speed in diffusion models. However,\nachieving 4-bit quantization remains challenging. Existing methods, primarily\nbased on integer quantization and post-training quantization fine-tuning,\nstruggle with inconsistent performance. Inspired by the success of\nfloating-point (FP) quantization in large language models, we explore low-bit\nFP quantization for diffusion models and identify key challenges: the failure\nof signed FP quantization to handle asymmetric activation distributions, the\ninsufficient consideration of temporal complexity in the denoising process\nduring fine-tuning, and the misalignment between fine-tuning loss and\nquantization error. To address these challenges, we propose the mixup-sign\nfloating-point quantization (MSFP) framework, first introducing unsigned FP\nquantization in model quantization, along with timestep-aware LoRA (TALoRA) and\ndenoising-factor loss alignment (DFA), which ensure precise and stable\nfine-tuning. Extensive experiments show that we are the first to achieve\nsuperior performance in 4-bit FP quantization for diffusion models,\noutperforming existing PTQ fine-tuning methods in 4-bit INT quantization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMSFP\u7684\u6df7\u5408\u7b26\u53f7\u6d6e\u70b9\u91cf\u5316\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u67094\u4f4d\u91cf\u5316\u65b9\u6cd5\u5728\u6269\u6563\u6a21\u578b\u4e2d\u6027\u80fd\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u76844\u4f4d\u91cf\u5316\u5728\u5185\u5b58\u6548\u7387\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u4e0d\u7a33\u5b9a\uff0c\u4e3a\u6b64\u63a2\u7d22\u4e86\u4f4e\u6bd4\u7279\u6d6e\u70b9\u91cf\u5316\u3002", "method": "\u63d0\u51fa\u4e86MSFP\u6846\u67b6\uff0c\u5f15\u5165\u65e0\u7b26\u53f7\u6d6e\u70b9\u91cf\u5316\u3001\u65f6\u95f4\u611f\u77e5LoRA\uff08TALoRA\uff09\u548c\u53bb\u566a\u56e0\u5b50\u635f\u5931\u5bf9\u9f50\uff08DFA\uff09\uff0c\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7a33\u5b9a\u7684\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u9996\u6b21\u57284\u4f4d\u6d6e\u70b9\u91cf\u5316\u4e2d\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u67094\u4f4d\u6574\u6570\u91cf\u5316\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "MSFP\u6846\u67b6\u4e3a\u6269\u6563\u6a21\u578b\u7684\u4f4e\u6bd4\u7279\u91cf\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u6269\u6563\u6a21\u578b\u30014\u4f4d\u91cf\u5316\u3001\u6d6e\u70b9\u91cf\u5316\u3001MSFP\u3001\u5fae\u8c03"}}
{"id": "2505.21935", "pdf": "https://arxiv.org/pdf/2505.21935", "abs": "https://arxiv.org/abs/2505.21935", "authors": ["Kaiyu He", "Zhiyu Chen"], "title": "From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Since the advent of Large Language Models (LLMs), efforts have largely\nfocused on improving their instruction-following and deductive reasoning\nabilities, leaving open the question of whether these models can truly discover\nnew knowledge. In pursuit of artificial general intelligence (AGI), there is a\ngrowing need for models that not only execute commands or retrieve information\nbut also learn, reason, and generate new knowledge by formulating novel\nhypotheses and theories that deepen our understanding of the world. Guided by\nPeirce's framework of abduction, deduction, and induction, this survey offers a\nstructured lens to examine LLM-based hypothesis discovery. We synthesize\nexisting work in hypothesis generation, application, and validation,\nidentifying both key achievements and critical gaps. By unifying these threads,\nwe illuminate how LLMs might evolve from mere ``information executors'' into\nengines of genuine innovation, potentially transforming research, science, and\nreal-world problem solving.", "AI": {"tldr": "\u8be5\u7efc\u8ff0\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u5426\u8d85\u8d8a\u6307\u4ee4\u6267\u884c\u548c\u4fe1\u606f\u68c0\u7d22\uff0c\u771f\u6b63\u53d1\u73b0\u65b0\u77e5\u8bc6\u3002\u901a\u8fc7\u7ed3\u5408\u76ae\u5c14\u58eb\u7684\u2018\u6eaf\u56e0-\u6f14\u7ece-\u5f52\u7eb3\u2019\u6846\u67b6\uff0c\u5206\u6790\u4e86LLMs\u5728\u5047\u8bbe\u751f\u6210\u4e0e\u5e94\u7528\u65b9\u9762\u7684\u7814\u7a76\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u53d1\u5c55\u7684\u6f5c\u529b\u4e0e\u6311\u6218\u3002", "motivation": "\u5f53\u524dLLMs\u7684\u7814\u7a76\u591a\u96c6\u4e2d\u5728\u6307\u4ee4\u6267\u884c\u548c\u63a8\u7406\u80fd\u529b\u4e0a\uff0c\u7f3a\u4e4f\u5bf9\u5176\u77e5\u8bc6\u53d1\u73b0\u80fd\u529b\u7684\u63a2\u7d22\u3002\u4e3a\u5b9e\u73b0\u4eba\u5de5\u901a\u7528\u667a\u80fd\uff08AGI\uff09\uff0c\u9700\u8981LLMs\u4e0d\u4ec5\u80fd\u6267\u884c\u4efb\u52a1\uff0c\u8fd8\u80fd\u751f\u6210\u65b0\u5047\u8bbe\u548c\u7406\u8bba\uff0c\u63a8\u52a8\u79d1\u5b66\u7814\u7a76\u548c\u5b9e\u9645\u95ee\u9898\u7684\u89e3\u51b3\u3002", "method": "\u57fa\u4e8e\u76ae\u5c14\u58eb\u7684\u2018\u6eaf\u56e0-\u6f14\u7ece-\u5f52\u7eb3\u2019\u6846\u67b6\uff0c\u7cfb\u7edf\u68b3\u7406\u4e86LLMs\u5728\u5047\u8bbe\u751f\u6210\u3001\u5e94\u7528\u4e0e\u9a8c\u8bc1\u9886\u57df\u7684\u73b0\u6709\u7814\u7a76\uff0c\u5e76\u63d0\u51fa\u7ed3\u6784\u5316\u5206\u6790\u89c6\u89d2\u3002", "result": "\u603b\u7ed3\u4e86LLMs\u5728\u5047\u8bbe\u751f\u6210\u65b9\u9762\u7684\u6210\u679c\u4e0e\u4e0d\u8db3\uff0c\u5f3a\u8c03\u4e86\u5176\u5728\u4ece\u2018\u4fe1\u606f\u6267\u884c\u8005\u2019\u5411\u2018\u521b\u65b0\u5f15\u64ce\u2019\u8f6c\u578b\u7684\u6f5c\u529b\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u73b0\u6709\u7814\u7a76\uff0c\u672c\u6587\u63ed\u793a\u4e86LLMs\u5728\u672a\u6765\u53ef\u80fd\u6210\u4e3a\u771f\u6b63\u77e5\u8bc6\u53d1\u73b0\u5de5\u5177\u7684\u65b9\u5411\uff0c\u4f46\u4e5f\u6307\u51fa\u4e86\u5173\u952e\u7684\u6280\u672f\u4e0e\u7406\u8bba\u7f3a\u53e3\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u5047\u8bbe\u53d1\u73b0, \u4eba\u5de5\u901a\u7528\u667a\u80fd, \u6eaf\u56e0\u63a8\u7406, \u77e5\u8bc6\u751f\u6210"}}
{"id": "2505.21757", "pdf": "https://arxiv.org/pdf/2505.21757", "abs": "https://arxiv.org/abs/2505.21757", "authors": ["Yubin Kim", "Zhiyuan Hu", "Hyewon Jeong", "Eugene Park", "Shuyue Stella Li", "Chanwoo Park", "Shiyun Xiong", "MingYu Lu", "Hyeonhoon Lee", "Xin Liu", "Daniel McDuff", "Cynthia Breazeal", "Samir Tulebaev", "Hae Won Park"], "title": "BehaviorSFT: Behavioral Token Conditioning for Clinical Agents Across the Proactivity Spectrum", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) as clinical agents require careful behavioral\nadaptation. While adept at reactive tasks (e.g., diagnosis reasoning), LLMs\noften struggle with proactive engagement, like unprompted identification of\ncritical missing information or risks. We introduce BehaviorBench, a\ncomprehensive dataset to evaluate agent behaviors across a clinical assistance\nspectrum, ranging from reactive query responses to proactive interventions\n(e.g., clarifying ambiguities, flagging overlooked critical data). Our\nBehaviorBench experiments reveal LLMs' inconsistent proactivity. To address\nthis, we propose BehaviorSFT, a novel training strategy using behavioral tokens\nto explicitly condition LLMs for dynamic behavioral selection along this\nspectrum. BehaviorSFT boosts performance, achieving up to 97.3% overall Macro\nF1 on BehaviorBench and improving proactive task scores (e.g., from 95.0% to\n96.5% for Qwen2.5-7B-Ins). Crucially, blind clinician evaluations confirmed\nBehaviorSFT-trained agents exhibit more realistic clinical behavior, striking a\nsuperior balance between helpful proactivity (e.g., timely, relevant\nsuggestions) and necessary restraint (e.g., avoiding over-intervention) versus\nstandard fine-tuning or explicit instructed agents.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86BehaviorBench\u6570\u636e\u96c6\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u4e34\u5e8a\u8f85\u52a9\u4efb\u52a1\u4e2d\u7684\u884c\u4e3a\u8868\u73b0\uff0c\u5e76\u63d0\u51faBehaviorSFT\u8bad\u7ec3\u7b56\u7565\u4ee5\u63d0\u5347LLM\u7684\u4e3b\u52a8\u6027\u4e0e\u884c\u4e3a\u9002\u5e94\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u4e0e\u4e34\u5e8a\u5b9e\u7528\u6027\u3002", "motivation": "LLM\u5728\u4e34\u5e8a\u4efb\u52a1\u4e2d\u867d\u64c5\u957f\u53cd\u5e94\u6027\u4efb\u52a1\uff08\u5982\u8bca\u65ad\u63a8\u7406\uff09\uff0c\u4f46\u7f3a\u4e4f\u4e3b\u52a8\u884c\u4e3a\uff08\u5982\u8bc6\u522b\u5173\u952e\u7f3a\u5931\u4fe1\u606f\uff09\u3002\u9700\u5f00\u53d1\u65b9\u6cd5\u63d0\u5347\u5176\u4e3b\u52a8\u6027\u4e0e\u884c\u4e3a\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faBehaviorSFT\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u884c\u4e3a\u6807\u8bb0\u663e\u5f0f\u8c03\u6574LLM\u884c\u4e3a\uff0c\u4f7f\u5176\u80fd\u5728\u53cd\u5e94\u6027\u4e0e\u4e3b\u52a8\u6027\u4efb\u52a1\u95f4\u52a8\u6001\u5207\u6362\u3002", "result": "BehaviorSFT\u4f7fLLM\u5728BehaviorBench\u4e0aMacro F1\u8fbe97.3%\uff0c\u4e3b\u52a8\u4efb\u52a1\u5206\u6570\u63d0\u5347\uff08\u5982Qwen2.5-7B-Ins\u4ece95.0%\u5347\u81f396.5%\uff09\uff0c\u4e34\u5e8a\u8bc4\u4f30\u663e\u793a\u5176\u884c\u4e3a\u66f4\u63a5\u8fd1\u73b0\u5b9e\u9700\u6c42\u3002", "conclusion": "BehaviorSFT\u80fd\u6709\u6548\u63d0\u5347LLM\u5728\u4e34\u5e8a\u4efb\u52a1\u4e2d\u7684\u4e3b\u52a8\u6027\u4e0e\u884c\u4e3a\u5e73\u8861\uff0c\u4f18\u4e8e\u4f20\u7edf\u5fae\u8c03\u6216\u663e\u5f0f\u6307\u4ee4\u65b9\u6cd5\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u4e34\u5e8a\u4ee3\u7406, \u884c\u4e3a\u9002\u5e94\u6027, \u4e3b\u52a8\u6027\u4efb\u52a1, BehaviorBench, BehaviorSFT"}}
{"id": "2505.21595", "pdf": "https://arxiv.org/pdf/2505.21595", "abs": "https://arxiv.org/abs/2505.21595", "authors": ["Shreyas Gururaj", "Lars Gr\u00fcne", "Wojciech Samek", "Sebastian Lapuschkin", "Leander Weber"], "title": "Relevance-driven Input Dropout: an Explanation-guided Regularization Technique", "categories": ["cs.LG", "cs.AI", "I.2.6"], "comment": null, "summary": "Overfitting is a well-known issue extending even to state-of-the-art (SOTA)\nMachine Learning (ML) models, resulting in reduced generalization, and a\nsignificant train-test performance gap. Mitigation measures include a\ncombination of dropout, data augmentation, weight decay, and other\nregularization techniques. Among the various data augmentation strategies,\nocclusion is a prominent technique that typically focuses on randomly masking\nregions of the input during training. Most of the existing literature\nemphasizes randomness in selecting and modifying the input features instead of\nregions that strongly influence model decisions. We propose Relevance-driven\nInput Dropout (RelDrop), a novel data augmentation method which selectively\noccludes the most relevant regions of the input, nudging the model to use other\nimportant features in the prediction process, thus improving model\ngeneralization through informed regularization. We further conduct qualitative\nand quantitative analyses to study how Relevance-driven Input Dropout (RelDrop)\naffects model decision-making. Through a series of experiments on benchmark\ndatasets, we demonstrate that our approach improves robustness towards\nocclusion, results in models utilizing more features within the region of\ninterest, and boosts inference time generalization performance. Our code is\navailable at https://github.com/Shreyas-Gururaj/LRP_Relevance_Dropout.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Relevance-driven Input Dropout (RelDrop)\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u906e\u6321\u8f93\u5165\u7684\u6700\u76f8\u5173\u533a\u57df\u6765\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u591a\u91c7\u7528\u968f\u673a\u906e\u6321\uff0c\u672a\u5145\u5206\u5229\u7528\u5bf9\u6a21\u578b\u51b3\u7b56\u5f71\u54cd\u6700\u5927\u7684\u533a\u57df\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51faRelDrop\u65b9\u6cd5\uff0c\u9009\u62e9\u6027\u906e\u6321\u8f93\u5165\u7684\u6700\u76f8\u5173\u533a\u57df\uff0c\u8feb\u4f7f\u6a21\u578b\u5229\u7528\u5176\u4ed6\u91cd\u8981\u7279\u5f81\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eRelDrop\u63d0\u5347\u4e86\u5bf9\u906e\u6321\u7684\u9c81\u68d2\u6027\uff0c\u6a21\u578b\u80fd\u5229\u7528\u66f4\u591a\u611f\u5174\u8da3\u533a\u57df\u7684\u7279\u5f81\uff0c\u5e76\u63d0\u9ad8\u4e86\u63a8\u7406\u65f6\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "RelDrop\u901a\u8fc7\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u9009\u62e9\u6027\u906e\u6321\uff0c\u6709\u6548\u6539\u5584\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "keywords": "\u8fc7\u62df\u5408, \u6570\u636e\u589e\u5f3a, \u9009\u62e9\u6027\u906e\u6321, \u6cdb\u5316\u80fd\u529b, \u673a\u5668\u5b66\u4e60"}}
{"id": "2505.21988", "pdf": "https://arxiv.org/pdf/2505.21988", "abs": "https://arxiv.org/abs/2505.21988", "authors": ["Ziyang Zheng", "Kezhi Li", "Zhengyuan Shi", "Qiang Xu"], "title": "Functional Matching of Logic Subgraphs: Beyond Structural Isomorphism", "categories": ["cs.AI"], "comment": null, "summary": "Subgraph matching in logic circuits is foundational for numerous Electronic\nDesign Automation (EDA) applications, including datapath optimization,\narithmetic verification, and hardware trojan detection. However, existing\ntechniques rely primarily on structural graph isomorphism and thus fail to\nidentify function-related subgraphs when synthesis transformations\nsubstantially alter circuit topology. To overcome this critical limitation, we\nintroduce the concept of functional subgraph matching, a novel approach that\nidentifies whether a given logic function is implicitly present within a larger\ncircuit, irrespective of structural variations induced by synthesis or\ntechnology mapping. Specifically, we propose a two-stage multi-modal framework:\n(1) learning robust functional embeddings across AIG and post-mapping netlists\nfor functional subgraph detection, and (2) identifying fuzzy boundaries using a\ngraph segmentation approach. Evaluations on standard benchmarks (ITC99,\nOpenABCD, ForgeEDA) demonstrate significant performance improvements over\nexisting structural methods, with average $93.8\\%$ accuracy in functional\nsubgraph detection and a dice score of $91.3\\%$ in fuzzy boundary\nidentification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u529f\u80fd\u6027\u5b50\u56fe\u5339\u914d\u65b9\u6cd5\uff0c\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u7ed3\u6784\u56fe\u540c\u6784\u7684\u6280\u672f\u65e0\u6cd5\u8bc6\u522b\u529f\u80fd\u76f8\u5173\u5b50\u56fe\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u591a\u6a21\u6001\u6846\u67b6\u5b9e\u73b0\u4e8693.8%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709EDA\u5e94\u7528\u4e2d\u57fa\u4e8e\u7ed3\u6784\u56fe\u540c\u6784\u7684\u5b50\u56fe\u5339\u914d\u65b9\u6cd5\u56e0\u5408\u6210\u53d8\u6362\u5bfc\u81f4\u7684\u62d3\u6251\u53d8\u5316\u800c\u5931\u6548\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u8bc6\u522b\u529f\u80fd\u76f8\u5173\u5b50\u56fe\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u591a\u6a21\u6001\u6846\u67b6\uff1a(1) \u5b66\u4e60AIG\u548c\u540e\u6620\u5c04\u7f51\u8868\u7684\u9c81\u68d2\u529f\u80fd\u5d4c\u5165\uff0c(2) \u901a\u8fc7\u56fe\u5206\u5272\u65b9\u6cd5\u8bc6\u522b\u6a21\u7cca\u8fb9\u754c\u3002", "result": "\u5728\u6807\u51c6\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e8693.8%\u7684\u529f\u80fd\u5b50\u56fe\u68c0\u6d4b\u51c6\u786e\u7387\u548c91.3%\u7684\u6a21\u7cca\u8fb9\u754c\u8bc6\u522bDice\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7ed3\u6784\u65b9\u6cd5\u3002", "conclusion": "\u529f\u80fd\u6027\u5b50\u56fe\u5339\u914d\u65b9\u6cd5\u6709\u6548\u514b\u670d\u4e86\u7ed3\u6784\u5dee\u5f02\u5e26\u6765\u7684\u9650\u5236\uff0c\u4e3aEDA\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5b50\u56fe\u8bc6\u522b\u80fd\u529b\u3002", "keywords": "\u5b50\u56fe\u5339\u914d, \u529f\u80fd\u8bc6\u522b, EDA, \u591a\u6a21\u6001\u5b66\u4e60, \u56fe\u5206\u5272"}}
{"id": "2505.21772", "pdf": "https://arxiv.org/pdf/2505.21772", "abs": "https://arxiv.org/abs/2505.21772", "authors": ["Reza Khanmohammadi", "Erfan Miahi", "Mehrsa Mardikoraem", "Simerjot Kaur", "Ivan Brugere", "Charese H. Smiley", "Kundan Thind", "Mohammad M. Ghassemi"], "title": "Calibrating LLM Confidence by Probing Perturbed Representation Stability", "categories": ["cs.CL"], "comment": null, "summary": "Miscalibration in Large Language Models (LLMs) undermines their reliability,\nhighlighting the need for accurate confidence estimation. We introduce CCPS\n(Calibrating LLM Confidence by Probing Perturbed Representation Stability), a\nnovel method analyzing internal representational stability in LLMs. CCPS\napplies targeted adversarial perturbations to final hidden states, extracts\nfeatures reflecting the model's response to these perturbations, and uses a\nlightweight classifier to predict answer correctness. CCPS was evaluated on\nLLMs from 8B to 32B parameters (covering Llama, Qwen, and Mistral\narchitectures) using MMLU and MMLU-Pro benchmarks in both multiple-choice and\nopen-ended formats. Our results show that CCPS significantly outperforms\ncurrent approaches. Across four LLMs and three MMLU variants, CCPS reduces\nExpected Calibration Error by approximately 55% and Brier score by 21%, while\nincreasing accuracy by 5 percentage points, Area Under the Precision-Recall\nCurve by 4 percentage points, and Area Under the Receiver Operating\nCharacteristic Curve by 6 percentage points, all relative to the strongest\nprior method. CCPS delivers an efficient, broadly applicable, and more accurate\nsolution for estimating LLM confidence, thereby improving their\ntrustworthiness.", "AI": {"tldr": "CCPS\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u8868\u793a\u7684\u7a33\u5b9a\u6027\u6765\u6821\u51c6\u5176\u7f6e\u4fe1\u5ea6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6821\u51c6\u8bef\u5dee\u5e76\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6821\u51c6\u4e0d\u8db3\u5f71\u54cd\u4e86\u5176\u53ef\u9760\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u51c6\u786e\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "CCPS\u901a\u8fc7\u5bf9\u6700\u7ec8\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u6709\u9488\u5bf9\u6027\u7684\u5bf9\u6297\u6270\u52a8\uff0c\u63d0\u53d6\u53cd\u6620\u6a21\u578b\u54cd\u5e94\u7684\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u9884\u6d4b\u7b54\u6848\u6b63\u786e\u6027\u3002", "result": "\u5728\u591a\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u548c\u8bc4\u6d4b\u57fa\u51c6\u4e0a\uff0cCCPS\u5c06\u9884\u671f\u6821\u51c6\u8bef\u5dee\u964d\u4f4e\u4e86\u7ea655%\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u591a\u9879\u6027\u80fd\u6307\u6807\u3002", "conclusion": "CCPS\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5e7f\u6cdb\u9002\u7528\u4e14\u66f4\u51c6\u786e\u7684\u4f30\u8ba1\u5927\u8bed\u8a00\u6a21\u578b\u7f6e\u4fe1\u5ea6\u7684\u65b9\u6cd5\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u5176\u53ef\u4fe1\u8d56\u6027\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u7f6e\u4fe1\u5ea6\u6821\u51c6, \u5bf9\u6297\u6270\u52a8, \u8868\u793a\u7a33\u5b9a\u6027, \u9884\u671f\u6821\u51c6\u8bef\u5dee"}}
{"id": "2505.21605", "pdf": "https://arxiv.org/pdf/2505.21605", "abs": "https://arxiv.org/abs/2505.21605", "authors": ["Fengqing Jiang", "Fengbo Ma", "Zhangchen Xu", "Yuetai Li", "Bhaskar Ramasubramanian", "Luyao Niu", "Bo Li", "Xianyan Chen", "Zhen Xiang", "Radha Poovendran"], "title": "SOSBENCH: Benchmarking Safety Alignment on Scientific Knowledge", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Large language models (LLMs) exhibit advancing capabilities in complex tasks,\nsuch as reasoning and graduate-level question answering, yet their resilience\nagainst misuse, particularly involving scientifically sophisticated risks,\nremains underexplored. Existing safety benchmarks typically focus either on\ninstructions requiring minimal knowledge comprehension (e.g., ``tell me how to\nbuild a bomb\") or utilize prompts that are relatively low-risk (e.g.,\nmultiple-choice or classification tasks about hazardous content). Consequently,\nthey fail to adequately assess model safety when handling knowledge-intensive,\nhazardous scenarios.\n  To address this critical gap, we introduce SOSBench, a regulation-grounded,\nhazard-focused benchmark encompassing six high-risk scientific domains:\nchemistry, biology, medicine, pharmacology, physics, and psychology. The\nbenchmark comprises 3,000 prompts derived from real-world regulations and laws,\nsystematically expanded via an LLM-assisted evolutionary pipeline that\nintroduces diverse, realistic misuse scenarios (e.g., detailed explosive\nsynthesis instructions involving advanced chemical formulas). We evaluate\nfrontier models within a unified evaluation framework using our SOSBench.\nDespite their alignment claims, advanced models consistently disclose\npolicy-violating content across all domains, demonstrating alarmingly high\nrates of harmful responses (e.g., 79.1% for Deepseek-R1 and 47.3% for GPT-4.1).\nThese results highlight significant safety alignment deficiencies and\nunderscore urgent concerns regarding the responsible deployment of powerful\nLLMs.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86SOSBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u79d1\u5b66\u9886\u57df\u7684\u5b89\u5168\u6027\u95ee\u9898\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u58f0\u79f0\u5bf9\u9f50\u7684\u5148\u8fdb\u6a21\u578b\u5728\u8fd9\u4e9b\u9886\u57df\u4ecd\u5b58\u5728\u5927\u91cf\u6709\u5bb3\u54cd\u5e94\uff0c\u63ed\u793a\u4e86\u5b89\u5168\u5bf9\u9f50\u7684\u91cd\u5927\u7f3a\u9677\u3002", "motivation": "\u76ee\u524d\u7684\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u5173\u6ce8\u4f4e\u98ce\u9669\u6216\u77e5\u8bc6\u8981\u6c42\u4f4e\u7684\u573a\u666f\uff0c\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u9ad8\u98ce\u9669\u9886\u57df\u7684\u8868\u73b0\u3002SOSBench\u7684\u63a8\u51fa\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u5173\u952e\u7a7a\u767d\u3002", "method": "\u4f5c\u8005\u5f00\u53d1\u4e86SOSBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u516d\u4e2a\u9ad8\u98ce\u9669\u79d1\u5b66\u9886\u57df\u76843000\u4e2a\u63d0\u793a\uff0c\u901a\u8fc7LLM\u8f85\u52a9\u7684\u8fdb\u5316\u7ba1\u9053\u751f\u6210\u591a\u6837\u5316\u7684\u73b0\u5b9e\u6ee5\u7528\u573a\u666f\uff0c\u5e76\u5728\u6b64\u6846\u67b6\u4e0b\u8bc4\u4f30\u524d\u6cbf\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u5373\u4f7f\u58f0\u79f0\u5bf9\u9f50\u7684\u5148\u8fdb\u6a21\u578b\u5728\u6240\u6709\u9886\u57df\u5747\u5b58\u5728\u8fdd\u89c4\u5185\u5bb9\uff0c\u6709\u5bb3\u54cd\u5e94\u7387\u60ca\u4eba\uff08\u5982Deepseek-R1\u4e3a79.1%\uff0cGPT-4.1\u4e3a47.3%\uff09\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709\u6a21\u578b\u5728\u5b89\u5168\u5bf9\u9f50\u4e0a\u5b58\u5728\u91cd\u5927\u7f3a\u9677\uff0c\u5bf9\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8d1f\u8d23\u4efb\u90e8\u7f72\u63d0\u51fa\u4e86\u7d27\u8feb\u5173\u5207\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u3001\u9ad8\u98ce\u9669\u79d1\u5b66\u9886\u57df\u3001\u6709\u5bb3\u54cd\u5e94\u3001\u6a21\u578b\u5bf9\u9f50"}}
{"id": "2505.22006", "pdf": "https://arxiv.org/pdf/2505.22006", "abs": "https://arxiv.org/abs/2505.22006", "authors": ["Changze Qiao", "Mingming Lu"], "title": "Efficiently Enhancing General Agents With Hierarchical-categorical Memory", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "With large language models (LLMs) demonstrating remarkable capabilities,\nthere has been a surge in research on leveraging LLMs to build general-purpose\nmulti-modal agents. However, existing approaches either rely on computationally\nexpensive end-to-end training using large-scale multi-modal data or adopt\ntool-use methods that lack the ability to continuously learn and adapt to new\nenvironments. In this paper, we introduce EHC, a general agent capable of\nlearning without parameter updates. EHC consists of a Hierarchical Memory\nRetrieval (HMR) module and a Task-Category Oriented Experience Learning (TOEL)\nmodule. The HMR module facilitates rapid retrieval of relevant memories and\ncontinuously stores new information without being constrained by memory\ncapacity. The TOEL module enhances the agent's comprehension of various task\ncharacteristics by classifying experiences and extracting patterns across\ndifferent categories. Extensive experiments conducted on multiple standard\ndatasets demonstrate that EHC outperforms existing methods, achieving\nstate-of-the-art performance and underscoring its effectiveness as a general\nagent for handling complex multi-modal tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u7684\u901a\u7528\u4ee3\u7406EHC\uff0c\u5305\u542b\u5206\u5c42\u8bb0\u5fc6\u68c0\u7d22\u548c\u4efb\u52a1\u5206\u7c7b\u5bfc\u5411\u7684\u7ecf\u9a8c\u5b66\u4e60\u6a21\u5757\uff0c\u5c55\u73b0\u4e86\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8ba1\u7b97\u6602\u8d35\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u6216\u7f3a\u4e4f\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u7684\u5de5\u5177\u4f7f\u7528\u6cd5\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u4ee3\u7406\u7684\u901a\u7528\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "EHC\u901a\u8fc7\u5206\u5c42\u8bb0\u5fc6\u68c0\u7d22(HMR)\u6a21\u5757\u5feb\u901f\u68c0\u7d22\u548c\u5b58\u50a8\u4fe1\u606f\uff0c\u4efb\u52a1\u5206\u7c7b\u5bfc\u5411\u7ecf\u9a8c\u5b66\u4e60(TOEL)\u6a21\u5757\u5206\u7c7b\u7ecf\u9a8c\u5e76\u63d0\u53d6\u8de8\u7c7b\u522b\u6a21\u5f0f\u3002", "result": "\u5728\u591a\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEHC\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "EHC\u4f5c\u4e3a\u901a\u7528\u4ee3\u7406\uff0c\u80fd\u9ad8\u6548\u5904\u7406\u590d\u6742\u591a\u6a21\u6001\u4efb\u52a1\uff0c\u4f53\u73b0\u4e86\u5176\u6301\u7eed\u5b66\u4e60\u548c\u9002\u5e94\u6027\u4f18\u52bf\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\u3001\u591a\u6a21\u6001\u4ee3\u7406\u3001\u5206\u5c42\u8bb0\u5fc6\u68c0\u7d22\u3001\u7ecf\u9a8c\u5b66\u4e60\u3001\u6301\u7eed\u9002\u5e94"}}
{"id": "2505.21781", "pdf": "https://arxiv.org/pdf/2505.21781", "abs": "https://arxiv.org/abs/2505.21781", "authors": ["Chutong Meng", "Antonios Anastasopoulos"], "title": "GMU Systems for the IWSLT 2025 Low-Resource Speech Translation Shared Task", "categories": ["cs.CL"], "comment": "IWSLT 2025", "summary": "This paper describes the GMU systems for the IWSLT 2025 low-resource speech\ntranslation shared task. We trained systems for all language pairs, except for\nLevantine Arabic. We fine-tuned SeamlessM4T-v2 for automatic speech recognition\n(ASR), machine translation (MT), and end-to-end speech translation (E2E ST).\nThe ASR and MT models are also used to form cascaded ST systems. Additionally,\nwe explored various training paradigms for E2E ST fine-tuning, including direct\nE2E fine-tuning, multi-task training, and parameter initialization using\ncomponents from fine-tuned ASR and/or MT models. Our results show that (1)\ndirect E2E fine-tuning yields strong results; (2) initializing with a\nfine-tuned ASR encoder improves ST performance on languages SeamlessM4T-v2 has\nnot been trained on; (3) multi-task training can be slightly helpful.", "AI": {"tldr": "GMU\u7cfb\u7edf\u5728IWSLT 2025\u4f4e\u8d44\u6e90\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u79c0\uff0c\u4f7f\u7528\u4e86SeamlessM4T-v2\u8fdb\u884cASR\u3001MT\u548cE2E ST\u7684\u5fae\u8c03\uff0c\u5e76\u63a2\u7d22\u4e86\u591a\u79cd\u8bad\u7ec3\u8303\u5f0f\u3002", "motivation": "\u4e3a\u4e86\u5728\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u63d0\u5347\u8bed\u97f3\u7ffb\u8bd1\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u672a\u8bad\u7ec3\u8bed\u8a00\u4e0a\u63d0\u9ad8\u8868\u73b0\u3002", "method": "\u4f7f\u7528SeamlessM4T-v2\u5fae\u8c03ASR\u3001MT\u548cE2E ST\u6a21\u578b\uff0c\u63a2\u7d22\u4e86\u76f4\u63a5\u5fae\u8c03\u3001\u591a\u4efb\u52a1\u8bad\u7ec3\u548c\u53c2\u6570\u521d\u59cb\u5316\u7b49\u65b9\u6cd5\u3002", "result": "\u76f4\u63a5E2E\u5fae\u8c03\u6548\u679c\u6700\u597d\uff1bASR\u7f16\u7801\u5668\u521d\u59cb\u5316\u63d0\u5347\u4e86\u672a\u8bad\u7ec3\u8bed\u8a00\u7684ST\u8868\u73b0\uff1b\u591a\u4efb\u52a1\u8bad\u7ec3\u7565\u6709\u5e2e\u52a9\u3002", "conclusion": "\u76f4\u63a5E2E\u5fae\u8c03\u662f\u6700\u6709\u6548\u7684\u4f4e\u8d44\u6e90ST\u65b9\u6cd5\uff0cASR\u7f16\u7801\u5668\u521d\u59cb\u5316\u5bf9\u672a\u8bad\u7ec3\u8bed\u8a00\u6709\u663e\u8457\u63d0\u5347\u3002", "keywords": "IWSLT, speech translation, low-resource, SeamlessM4T-v2, fine-tuning"}}
{"id": "2505.21626", "pdf": "https://arxiv.org/pdf/2505.21626", "abs": "https://arxiv.org/abs/2505.21626", "authors": ["Nicolas Guerra", "Nicholas H. Nelsen", "Yunan Yang"], "title": "Learning Where to Learn: Training Distribution Selection for Provable OOD Performance", "categories": ["cs.LG", "math.OC", "stat.ML", "62K05, 65K10 (Primary) 68T07, 65D15, 62R20, 60G57 (Secondary)"], "comment": "32 pages, 8 figures, 2 tables, 3 algorithms", "summary": "Out-of-distribution (OOD) generalization remains a fundamental challenge in\nmachine learning. Models trained on one data distribution often experience\nsubstantial performance degradation when evaluated on shifted or unseen\ndomains. To address this challenge, the present paper studies the design of\ntraining data distributions that maximize average-case OOD performance. First,\na theoretical analysis establishes a family of generalization bounds that\nquantify how the choice of training distribution influences OOD error across a\npredefined family of target distributions. These insights motivate the\nintroduction of two complementary algorithmic strategies: (i) directly\nformulating OOD risk minimization as a bilevel optimization problem over the\nspace of probability measures and (ii) minimizing a theoretical upper bound on\nOOD error. Last, the paper evaluates the two approaches across a range of\nfunction approximation and operator learning examples. The proposed methods\nsignificantly improve OOD accuracy over standard empirical risk minimization\nwith a fixed distribution. These results highlight the potential of\ndistribution-aware training as a principled and practical framework for robust\nOOD generalization.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u8bbe\u8ba1\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u4ee5\u6700\u5927\u5316OOD\u6cdb\u5316\u6027\u80fd\uff0c\u63d0\u51fa\u4e24\u79cd\u7b97\u6cd5\u7b56\u7565\u5e76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u4e2dOOD\u6cdb\u5316\u7684\u6311\u6218\uff0c\u63d0\u5347\u6a21\u578b\u5728\u672a\u89c1\u5206\u5e03\u4e0a\u7684\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u5236\u5b9a\u6cdb\u5316\u8fb9\u754c\uff0c\u5e76\u63d0\u51fa\u53cc\u5c42\u4f18\u5316\u548c\u6700\u5c0f\u5316OOD\u8bef\u5dee\u4e0a\u754c\u4e24\u79cd\u7b97\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u56fa\u5b9a\u5206\u5e03\u7684ERM\u3002", "conclusion": "\u5206\u5e03\u611f\u77e5\u8bad\u7ec3\u662f\u63d0\u5347OOD\u6cdb\u5316\u7684\u6709\u6548\u6846\u67b6\u3002", "keywords": "OOD\u6cdb\u5316, \u6570\u636e\u5206\u5e03, \u53cc\u5c42\u4f18\u5316, \u6cdb\u5316\u8fb9\u754c"}}
{"id": "2505.22050", "pdf": "https://arxiv.org/pdf/2505.22050", "abs": "https://arxiv.org/abs/2505.22050", "authors": ["Di Wu", "Jiaxin Fan", "Junzhe Zang", "Guanbo Wang", "Wei Yin", "Wenhao Li", "Bo Jin"], "title": "Reinforced Reasoning for Embodied Planning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Embodied planning requires agents to make coherent multi-step decisions based\non dynamic visual observations and natural language goals. While recent\nvision-language models (VLMs) excel at static perception tasks, they struggle\nwith the temporal reasoning, spatial understanding, and commonsense grounding\nneeded for planning in interactive environments. In this work, we introduce a\nreinforcement fine-tuning framework that brings R1-style reasoning enhancement\ninto embodied planning. We first distill a high-quality dataset from a powerful\nclosed-source model and perform supervised fine-tuning (SFT) to equip the model\nwith structured decision-making priors. We then design a rule-based reward\nfunction tailored to multi-step action quality and optimize the policy via\nGeneralized Reinforced Preference Optimization (GRPO). Our approach is\nevaluated on Embench, a recent benchmark for interactive embodied tasks,\ncovering both in-domain and out-of-domain scenarios. Experimental results show\nthat our method significantly outperforms models of similar or larger scale,\nincluding GPT-4o-mini and 70B+ open-source baselines, and exhibits strong\ngeneralization to unseen environments. This work highlights the potential of\nreinforcement-driven reasoning to advance long-horizon planning in embodied AI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f3a\u5316\u5fae\u8c03\u6846\u67b6\uff0c\u7ed3\u5408\u4e86R1\u5f0f\u63a8\u7406\u589e\u5f3a\uff0c\u7528\u4e8e\u63d0\u5347\u5728\u4ea4\u4e92\u5f0f\u73af\u5883\u4e2d\u7684\u591a\u6b65\u51b3\u7b56\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u867d\u7136\u5728\u9759\u6001\u611f\u77e5\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9700\u8981\u65f6\u5e8f\u63a8\u7406\u3001\u7a7a\u95f4\u7406\u89e3\u548c\u5e38\u8bc6\u57fa\u7840\u7684\u4ea4\u4e92\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5e0c\u671b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u6a21\u578b\u7684\u89c4\u5212\u80fd\u529b\u3002", "method": "\u9996\u5148\u4ece\u4e00\u4e2a\u5f3a\u5927\u7684\u95ed\u6e90\u6a21\u578b\u4e2d\u63d0\u53d6\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4ee5\u8d4b\u4e88\u6a21\u578b\u7ed3\u6784\u5316\u51b3\u7b56\u5148\u9a8c\uff1b\u7136\u540e\u8bbe\u8ba1\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u901a\u8fc7\u5e7f\u4e49\u5f3a\u5316\u504f\u597d\u4f18\u5316\uff08GRPO\uff09\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728Embench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u57df\u5185\u548c\u57df\u5916\u573a\u666f\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u7c7b\u4f3c\u6216\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\uff0c\u5305\u62ecGPT-4o-mini\u548c70B+\u5f00\u6e90\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u8868\u73b0\u51fa\u5bf9\u672a\u89c1\u73af\u5883\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5f3a\u5316\u9a71\u52a8\u7684\u63a8\u7406\u65b9\u6cd5\u5728\u63d0\u5347\u5177\u8eabAI\u7684\u957f\u65f6\u89c4\u5212\u80fd\u529b\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002", "keywords": "\u5177\u8eab\u89c4\u5212\uff0c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u5f3a\u5316\u5b66\u4e60\uff0c\u591a\u6b65\u51b3\u7b56\uff0c\u6cdb\u5316\u80fd\u529b"}}
{"id": "2505.21786", "pdf": "https://arxiv.org/pdf/2505.21786", "abs": "https://arxiv.org/abs/2505.21786", "authors": ["Dasha Metropolitansky", "Jonathan Larson"], "title": "VeriTrail: Closed-Domain Hallucination Detection with Traceability", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Even when instructed to adhere to source material, Language Models often\ngenerate unsubstantiated content - a phenomenon known as \"closed-domain\nhallucination.\" This risk is amplified in processes with multiple generative\nsteps (MGS), compared to processes with a single generative step (SGS).\nHowever, due to the greater complexity of MGS processes, we argue that\ndetecting hallucinations in their final outputs is necessary but not\nsufficient: it is equally important to trace where hallucinated content was\nlikely introduced and how faithful content may have been derived from the\nsource through intermediate outputs. To address this need, we present\nVeriTrail, the first closed-domain hallucination detection method designed to\nprovide traceability for both MGS and SGS processes. We also introduce the\nfirst datasets to include all intermediate outputs as well as human annotations\nof final outputs' faithfulness for their respective MGS processes. We\ndemonstrate that VeriTrail outperforms baseline methods on both datasets.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51faVeriTrail\uff0c\u9996\u4e2a\u9488\u5bf9\u591a\u6b65\u9aa4\u751f\u6210\u8fc7\u7a0b\uff08MGS\uff09\u548c\u5355\u6b65\u9aa4\u751f\u6210\u8fc7\u7a0b\uff08SGS\uff09\u7684\u5c01\u95ed\u57df\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u5185\u5bb9\u53ef\u8ffd\u6eaf\u6027\u3002\u7814\u7a76\u8fd8\u5f15\u5165\u4e86\u5305\u542b\u4e2d\u95f4\u8f93\u51fa\u548c\u4eba\u5de5\u6807\u6ce8\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u8bc1\u660eVeriTrail\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u5185\u5bb9\u65f6\u5b58\u5728\u5c01\u95ed\u57df\u5e7b\u89c9\u73b0\u8c61\uff0c\u5c24\u5176\u5728\u591a\u6b65\u9aa4\u751f\u6210\u8fc7\u7a0b\u4e2d\u98ce\u9669\u66f4\u9ad8\u3002\u4ec5\u68c0\u6d4b\u6700\u7ec8\u8f93\u51fa\u4e0d\u8db3\u4ee5\u89e3\u51b3\u95ee\u9898\uff0c\u9700\u8ffd\u8e2a\u5e7b\u89c9\u5185\u5bb9\u7684\u5f15\u5165\u53ca\u4e2d\u95f4\u8f93\u51fa\u7684\u5fe0\u5b9e\u6027\u3002", "method": "\u63d0\u51faVeriTrail\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u7528\u4e8e\u591a\u6b65\u9aa4\u548c\u5355\u6b65\u9aa4\u751f\u6210\u8fc7\u7a0b\u7684\u5c01\u95ed\u57df\u5e7b\u89c9\u68c0\u6d4b\uff0c\u63d0\u4f9b\u5185\u5bb9\u53ef\u8ffd\u6eaf\u6027\u3002\u540c\u65f6\u521b\u5efa\u5305\u542b\u4e2d\u95f4\u8f93\u51fa\u548c\u4eba\u5de5\u6807\u6ce8\u7684\u65b0\u6570\u636e\u96c6\u3002", "result": "VeriTrail\u5728\u4e24\u79cd\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "VeriTrail\u4e3a\u5c01\u95ed\u57df\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u8ffd\u6eaf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u591a\u6b65\u9aa4\u751f\u6210\u8fc7\u7a0b\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "keywords": "\u5c01\u95ed\u57df\u5e7b\u89c9,\u591a\u6b65\u9aa4\u751f\u6210\u8fc7\u7a0b,\u53ef\u8ffd\u6eaf\u6027,VeriTrail,\u8bed\u8a00\u6a21\u578b"}}
{"id": "2505.21639", "pdf": "https://arxiv.org/pdf/2505.21639", "abs": "https://arxiv.org/abs/2505.21639", "authors": ["Mauricio Junca", "Esteban Leiva"], "title": "Apprenticeship learning with prior beliefs using inverse optimization", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "The relationship between inverse reinforcement learning (IRL) and inverse\noptimization (IO) for Markov decision processes (MDPs) has been relatively\nunderexplored in the literature, despite addressing the same problem. In this\nwork, we revisit the relationship between the IO framework for MDPs, IRL, and\napprenticeship learning (AL). We incorporate prior beliefs on the structure of\nthe cost function into the IRL and AL problems, and demonstrate that the\nconvex-analytic view of the AL formalism (Kamoutsi et al., 2021) emerges as a\nrelaxation of our framework. Notably, the AL formalism is a special case in our\nframework when the regularization term is absent. Focusing on the suboptimal\nexpert setting, we formulate the AL problem as a regularized min-max problem.\nThe regularizer plays a key role in addressing the ill-posedness of IRL by\nguiding the search for plausible cost functions. To solve the resulting\nregularized-convex-concave-min-max problem, we use stochastic mirror descent\n(SMD) and establish convergence bounds for the proposed method. Numerical\nexperiments highlight the critical role of regularization in learning cost\nvectors and apprentice policies.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u9006\u5f3a\u5316\u5b66\u4e60\uff08IRL\uff09\u4e0e\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u7684\u9006\u4f18\u5316\uff08IO\uff09\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u6210\u672c\u51fd\u6570\u7684\u5148\u9a8c\u4fe1\u5ff5\uff0c\u5c55\u793a\u4e86\u5b66\u5f92\u5b66\u4e60\uff08AL\uff09\u5f62\u5f0f\u4e3b\u4e49\u7684\u51f8\u5206\u6790\u89c6\u89d2\u662f\u6846\u67b6\u7684\u677e\u5f1b\u5f62\u5f0f\u3002\u91cd\u70b9\u7814\u7a76\u4e86\u6b21\u4f18\u4e13\u5bb6\u8bbe\u7f6e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6b63\u5219\u5316\u7684\u6781\u5c0f\u6781\u5927\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u968f\u673a\u955c\u50cf\u4e0b\u964d\u6cd5\uff08SMD\uff09\u89e3\u51b3\uff0c\u6570\u503c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6b63\u5219\u5316\u7684\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u586b\u8865\u6587\u732e\u4e2dIRL\u4e0eMDP\u7684IO\u5173\u7cfb\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u7a7a\u767d\uff0c\u540c\u65f6\u89e3\u51b3IRL\u7684\u201c\u4e0d\u9002\u5b9a\u6027\u201d\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u5148\u9a8c\u4fe1\u5ff5\u548c\u6b63\u5219\u5316\u63d0\u5347\u5b66\u4e60\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5148\u9a8c\u4fe1\u5ff5\u7684\u6b63\u5219\u5316\u6781\u5c0f\u6781\u5927AL\u95ee\u9898\u6846\u67b6\uff0c\u5e76\u5229\u7528\u968f\u673a\u955c\u50cf\u4e0b\u964d\u6cd5\uff08SMD\uff09\u8fdb\u884c\u4f18\u5316\u6c42\u89e3\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6b63\u5219\u5316\u5728\u6210\u672c\u51fd\u6570\u548c\u5b66\u5f92\u7b56\u7565\u5b66\u4e60\u4e2d\u7684\u91cd\u8981\u4f5c\u7528\uff0c\u5c24\u5176\u662f\u5bf9\u6b21\u4f18\u4e13\u5bb6\u8bbe\u7f6e\u7684\u9002\u5e94\u6027\u63d0\u5347\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u7edf\u4e00IRL\u3001IO\u548cAL\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u6b63\u5219\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86IRL\u7684\u4e0d\u9002\u5b9a\u6027\u95ee\u9898\uff0c\u4e3a\u6b21\u4f18\u4e13\u5bb6\u73af\u5883\u7684\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "keywords": "\u9006\u5f3a\u5316\u5b66\u4e60, \u9006\u4f18\u5316, \u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b, \u5b66\u5f92\u5b66\u4e60, \u6b63\u5219\u5316"}}
{"id": "2505.22087", "pdf": "https://arxiv.org/pdf/2505.22087", "abs": "https://arxiv.org/abs/2505.22087", "authors": ["Ruxiao Chen", "Dezheng Han", "Wenjie Han", "Shuaishuai Guo"], "title": "Cognitively-Inspired Emergent Communication via Knowledge Graphs for Assisting the Visually Impaired", "categories": ["cs.AI"], "comment": null, "summary": "Assistive systems for visually impaired individuals must deliver rapid,\ninterpretable, and adaptive feedback to facilitate real-time navigation.\nCurrent approaches face a trade-off between latency and semantic richness:\nnatural language-based systems provide detailed guidance but are too slow for\ndynamic scenarios, while emergent communication frameworks offer low-latency\nsymbolic languages but lack semantic depth, limiting their utility in tactile\nmodalities like vibration. To address these limitations, we introduce a novel\nframework, Cognitively-Inspired Emergent Communication via Knowledge Graphs\n(VAG-EC), which emulates human visual perception and cognitive mapping. Our\nmethod constructs knowledge graphs to represent objects and their\nrelationships, incorporating attention mechanisms to prioritize task-relevant\nentities, thereby mirroring human selective attention. This structured approach\nenables the emergence of compact, interpretable, and context-sensitive symbolic\nlanguages. Extensive experiments across varying vocabulary sizes and message\nlengths demonstrate that VAG-EC outperforms traditional emergent communication\nmethods in Topographic Similarity (TopSim) and Context Independence (CI). These\nfindings underscore the potential of cognitively grounded emergent\ncommunication as a fast, adaptive, and human-aligned solution for real-time\nassistive technologies. Code is available at\nhttps://github.com/Anonymous-NLPcode/Anonymous_submission/tree/main.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVAG-EC\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u6a21\u4eff\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\uff0c\u4ee5\u89e3\u51b3\u89c6\u89c9\u8f85\u52a9\u7cfb\u7edf\u5728\u5ef6\u8fdf\u4e0e\u8bed\u4e49\u4e30\u5bcc\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8f85\u52a9\u7cfb\u7edf\u5728\u52a8\u6001\u573a\u666f\u4e2d\u9762\u4e34\u7740\u8bed\u4e49\u6df1\u5ea6\u4e0e\u4f4e\u5ef6\u8fdf\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u8ba4\u77e5\u673a\u5236\u6765\u5f00\u53d1\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u91c7\u7528\u4e86\u77e5\u8bc6\u56fe\u8c31\u8868\u793a\u5bf9\u8c61\u53ca\u5176\u5173\u7cfb\uff0c\u5e76\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u7b5b\u9009\u4efb\u52a1\u76f8\u5173\u5b9e\u4f53\uff0c\u4ece\u800c\u751f\u6210\u7d27\u51d1\u4e14\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u7b26\u53f7\u8bed\u8a00\u3002", "result": "VAG-EC\u5728Topographic Similarity\u548cContext Independence\u6307\u6807\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u5b9e\u65f6\u8f85\u52a9\u6280\u672f\u63d0\u4f9b\u4e86\u4e00\u79cd\u5feb\u901f\u3001\u9002\u5e94\u6027\u5f3a\u4e14\u7b26\u5408\u4eba\u7c7b\u8ba4\u77e5\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u89c6\u89c9\u8f85\u52a9\u7cfb\u7edf, \u77e5\u8bc6\u56fe\u8c31, \u6d8c\u73b0\u901a\u4fe1, \u6ce8\u610f\u529b\u673a\u5236, \u8bed\u4e49\u4e30\u5bcc\u6027"}}
{"id": "2505.21816", "pdf": "https://arxiv.org/pdf/2505.21816", "abs": "https://arxiv.org/abs/2505.21816", "authors": ["Amr Keleg", "Sharon Goldwater", "Walid Magdy"], "title": "Revisiting Common Assumptions about Arabic Dialects in NLP", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Arabic has diverse dialects, where one dialect can be substantially different\nfrom the others. In the NLP literature, some assumptions about these dialects\nare widely adopted (e.g., ``Arabic dialects can be grouped into distinguishable\nregional dialects\") and are manifested in different computational tasks such as\nArabic Dialect Identification (ADI). However, these assumptions are not\nquantitatively verified. We identify four of these assumptions and examine them\nby extending and analyzing a multi-label dataset, where the validity of each\nsentence in 11 different country-level dialects is manually assessed by\nspeakers of these dialects. Our analysis indicates that the four assumptions\noversimplify reality, and some of them are not always accurate. This in turn\nmight be hindering further progress in different Arabic NLP tasks.", "AI": {"tldr": "\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u591a\u6837\u6027\u7814\u7a76\u63ed\u793a\uff0c\u5e7f\u6cdb\u63a5\u53d7\u7684\u65b9\u8a00\u533a\u57df\u5206\u7ec4\u5047\u8bbe\u8fc7\u4e8e\u7b80\u5316\uff0c\u53ef\u80fd\u963b\u788d\u963f\u62c9\u4f2f\u8bedNLP\u4efb\u52a1\u7684\u8fdb\u5c55\u3002", "motivation": "\u9a8c\u8bc1\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u5904\u7406\u4e2d\u7684\u5e38\u89c1\u5047\u8bbe\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u5047\u8bbe\u672a\u88ab\u91cf\u5316\u9a8c\u8bc1\uff0c\u53ef\u80fd\u9650\u5236NLP\u4efb\u52a1\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u548c\u5206\u6790\u591a\u6807\u7b7e\u6570\u636e\u96c6\uff0c\u624b\u52a8\u8bc4\u4f3011\u4e2a\u56fd\u5bb6\u7ea7\u522b\u65b9\u8a00\u7684\u53e5\u5b50\u6709\u6548\u6027\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u56db\u79cd\u5047\u8bbe\u8fc7\u4e8e\u7b80\u5316\u73b0\u5b9e\uff0c\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5e76\u4e0d\u51c6\u786e\u3002", "conclusion": "\u5f53\u524d\u7684\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u5047\u8bbe\u53ef\u80fd\u963b\u788dNLP\u8fdb\u5c55\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u6a21\u578b\u3002", "keywords": "\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u3001NLP\u3001\u65b9\u8a00\u8bc6\u522b\u3001\u591a\u6807\u7b7e\u6570\u636e\u96c6\u3001\u8bed\u8a00\u5047\u8bbe"}}
{"id": "2505.21640", "pdf": "https://arxiv.org/pdf/2505.21640", "abs": "https://arxiv.org/abs/2505.21640", "authors": ["Oren Mangoubi", "Neil He", "Nisheeth K. Vishnoi"], "title": "Efficient Diffusion Models for Symmetric Manifolds", "categories": ["cs.LG", "cs.AI", "cs.DS", "math.PR", "stat.ML"], "comment": "The conference version of this paper appears in ICML 2025", "summary": "We introduce a framework for designing efficient diffusion models for\n$d$-dimensional symmetric-space Riemannian manifolds, including the torus,\nsphere, special orthogonal group and unitary group. Existing manifold diffusion\nmodels often depend on heat kernels, which lack closed-form expressions and\nrequire either $d$ gradient evaluations or exponential-in-$d$ arithmetic\noperations per training step. We introduce a new diffusion model for symmetric\nmanifolds with a spatially-varying covariance, allowing us to leverage a\nprojection of Euclidean Brownian motion to bypass heat kernel computations. Our\ntraining algorithm minimizes a novel efficient objective derived via Ito's\nLemma, allowing each step to run in $O(1)$ gradient evaluations and\nnearly-linear-in-$d$ ($O(d^{1.19})$) arithmetic operations, reducing the gap\nbetween diffusions on symmetric manifolds and Euclidean space. Manifold\nsymmetries ensure the diffusion satisfies an \"average-case\" Lipschitz\ncondition, enabling accurate and efficient sample generation. Empirically, our\nmodel outperforms prior methods in training speed and improves sample quality\non synthetic datasets on the torus, special orthogonal group, and unitary\ngroup.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u5bf9\u79f0\u7a7a\u95f4\u9ece\u66fc\u6d41\u5f62\u7684\u9ad8\u6548\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u53d8\u5316\u7684\u534f\u65b9\u5dee\u548c\u6b27\u6c0f\u5e03\u6717\u8fd0\u52a8\u6295\u5f71\u7ed5\u8fc7\u70ed\u6838\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6837\u672c\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u6d41\u5f62\u6269\u6563\u6a21\u578b\u4f9d\u8d56\u70ed\u6838\uff0c\u8ba1\u7b97\u590d\u6742\u4e14\u6548\u7387\u4f4e\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u3002\u56e0\u6b64\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u6027\u80fd\u3002", "method": "\u91c7\u7528\u7a7a\u95f4\u53d8\u5316\u534f\u65b9\u5dee\u548c\u6b27\u6c0f\u5e03\u6717\u8fd0\u52a8\u6295\u5f71\uff0c\u7ed3\u5408Ito\u5f15\u7406\u63a8\u5bfc\u7684\u65b0\u76ee\u6807\u51fd\u6570\uff0c\u5b9e\u73b0O(1)\u68af\u5ea6\u8ba1\u7b97\u548c\u8fd1\u4e4e\u7ebf\u6027\u7684\u7b97\u672f\u64cd\u4f5c\u3002", "result": "\u5728\u73af\u9762\u3001\u7279\u6b8a\u6b63\u4ea4\u7fa4\u548c\u9149\u7fa4\u7b49\u5408\u6210\u6570\u636e\u96c6\u4e0a\uff0c\u8bad\u7ec3\u901f\u5ea6\u548c\u6837\u672c\u8d28\u91cf\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5229\u7528\u6d41\u5f62\u5bf9\u79f0\u6027\u548c\u9ad8\u6548\u8ba1\u7b97\u7b56\u7565\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u5bf9\u79f0\u6d41\u5f62\u4e0e\u6b27\u5f0f\u7a7a\u95f4\u6269\u6563\u6a21\u578b\u7684\u6548\u7387\u5dee\u8ddd\u3002", "keywords": "\u6269\u6563\u6a21\u578b, \u9ece\u66fc\u6d41\u5f62, \u5bf9\u79f0\u7a7a\u95f4, \u70ed\u6838, \u5e03\u6717\u8fd0\u52a8"}}
{"id": "2505.22092", "pdf": "https://arxiv.org/pdf/2505.22092", "abs": "https://arxiv.org/abs/2505.22092", "authors": ["Valentin Cuzin-Rambaud", "Emilien Komlenovic", "Alexandre Faure", "Bruno Yun"], "title": "VIRAL: Vision-grounded Integration for Reward design And Learning", "categories": ["cs.AI"], "comment": null, "summary": "The alignment between humans and machines is a critical challenge in\nartificial intelligence today. Reinforcement learning, which aims to maximize a\nreward function, is particularly vulnerable to the risks associated with poorly\ndesigned reward functions. Recent advancements has shown that Large Language\nModels (LLMs) for reward generation can outperform human performance in this\ncontext. We introduce VIRAL, a pipeline for generating and refining reward\nfunctions through the use of multi-modal LLMs. VIRAL autonomously creates and\ninteractively improves reward functions based on a given environment and a goal\nprompt or annotated image. The refinement process can incorporate human\nfeedback or be guided by a description generated by a video LLM, which explains\nthe agent's policy in video form. We evaluated VIRAL in five Gymnasium\nenvironments, demonstrating that it accelerates the learning of new behaviors\nwhile ensuring improved alignment with user intent. The source-code and demo\nvideo are available at: https://github.com/VIRAL-UCBL1/VIRAL and\nhttps://youtu.be/t4_BXugBm9Q.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86VIRAL\uff0c\u4e00\u4e2a\u901a\u8fc7\u591a\u6a21\u6001LLM\u751f\u6210\u548c\u4f18\u5316\u5956\u52b1\u51fd\u6570\u7684\u6d41\u7a0b\uff0c\u8bc1\u660e\u5176\u5728\u63d0\u5347\u5b66\u4e60\u6548\u7387\u548c\u7528\u6237\u610f\u56fe\u5bf9\u9f50\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u4e2d\u4eba\u4e0e\u673a\u5668\u7684\u5bf9\u9f50\u662f\u5173\u952e\u6311\u6218\uff0c\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u5b58\u5728\u4e0d\u8db3\uff0c\u591a\u6a21\u6001LLM\u5728\u6b64\u5c55\u73b0\u51fa\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u6001LLM\u6784\u5efaVIRAL\u6d41\u7a0b\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u4f18\u5316\u5956\u52b1\u51fd\u6570\uff08\u652f\u6301\u4eba\u7c7b\u53cd\u9988\u6216\u89c6\u9891LLM\u751f\u6210\u7684\u7b56\u7565\u63cf\u8ff0\uff09\u3002", "result": "\u5728\u4e94\u4e2aGymnasium\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0cVIRAL\u52a0\u901f\u4e86\u65b0\u884c\u4e3a\u5b66\u4e60\u5e76\u66f4\u597d\u5bf9\u9f50\u7528\u6237\u610f\u56fe\u3002", "conclusion": "\u591a\u6a21\u6001LLM\u9a71\u52a8\u7684\u5956\u52b1\u51fd\u6570\u751f\u6210\u4e0e\u4f18\u5316\u662f\u53ef\u884c\u7684\uff0c\u80fd\u6539\u5584AI\u7cfb\u7edf\u7684\u5bf9\u9f50\u6027\u4e0e\u6548\u7387\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u5956\u52b1\u51fd\u6570, \u591a\u6a21\u6001LLM, \u4eba\u673a\u5bf9\u9f50, VIRAL"}}
{"id": "2505.21819", "pdf": "https://arxiv.org/pdf/2505.21819", "abs": "https://arxiv.org/abs/2505.21819", "authors": ["Charlotte Peale", "Vinod Raman", "Omer Reingold"], "title": "Representative Language Generation", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ICML 2025", "summary": "We introduce \"representative generation,\" extending the theoretical framework\nfor generation proposed by Kleinberg et al. (2024) and formalized by Li et al.\n(2024), to additionally address diversity and bias concerns in generative\nmodels. Our notion requires outputs of a generative model to proportionally\nrepresent groups of interest from the training data. We characterize\nrepresentative uniform and non-uniform generation, introducing the \"group\nclosure dimension\" as a key combinatorial quantity. For representative\ngeneration in the limit, we analyze both information-theoretic and\ncomputational aspects, demonstrating feasibility for countably infinite\nhypothesis classes and collections of groups under certain conditions, but\nproving a negative result for computability using only membership queries. This\ncontrasts with Kleinberg et al.'s (2024) positive results for standard\ngeneration in the limit. Our findings provide a rigorous foundation for\ndeveloping more diverse and representative generative models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86'\u4ee3\u8868\u6027\u751f\u6210'\u7684\u6982\u5ff5\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u7684\u751f\u6210\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u751f\u6210\u6a21\u578b\u7684\u591a\u6837\u6027\u548c\u504f\u89c1\u95ee\u9898\u3002\u901a\u8fc7\u5f15\u5165'\u7fa4\u4f53\u95ed\u5305\u7ef4\u5ea6'\u8fd9\u4e00\u5173\u952e\u7ec4\u5408\u91cf\uff0c\u4f5c\u8005\u5206\u6790\u4e86\u4fe1\u606f\u7406\u8bba\u548c\u8ba1\u7b97\u5c42\u9762\u7684\u53ef\u884c\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u67d0\u4e9b\u6761\u4ef6\u4e0b\u7684\u8d1f\u9762\u7ed3\u679c\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u751f\u6210\u6a21\u578b\u4e2d\u7684\u591a\u6837\u6027\u548c\u504f\u89c1\u95ee\u9898\uff0c\u786e\u4fdd\u8f93\u51fa\u80fd\u6309\u6bd4\u4f8b\u4ee3\u8868\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u611f\u5174\u8da3\u7fa4\u4f53\u3002", "method": "\u5f15\u5165'\u4ee3\u8868\u6027\u751f\u6210'\u6982\u5ff5\uff0c\u63d0\u51fa'\u7fa4\u4f53\u95ed\u5305\u7ef4\u5ea6'\u4f5c\u4e3a\u5173\u952e\u6307\u6807\uff0c\u5206\u6790\u4e86\u65e0\u9650\u5047\u8bbe\u7c7b\u548c\u7fa4\u4f53\u96c6\u5408\u4e0b\u7684\u4fe1\u606f\u7406\u8bba\u548c\u8ba1\u7b97\u53ef\u884c\u6027\u3002", "result": "\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u53ef\u4ee5\u5b9e\u73b0\u65e0\u9650\u5047\u8bbe\u7c7b\u7684\u4ee3\u8868\u6027\u751f\u6210\uff0c\u4f46\u4ec5\u901a\u8fc7\u6210\u5458\u67e5\u8be2\u65e0\u6cd5\u5b9e\u73b0\u53ef\u8ba1\u7b97\u7684\u4ee3\u8868\u6027\u751f\u6210\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u66f4\u591a\u6837\u548c\u4ee3\u8868\u6027\u7684\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "keywords": "\u4ee3\u8868\u6027\u751f\u6210, \u591a\u6837\u6027, \u504f\u89c1, \u7fa4\u4f53\u95ed\u5305\u7ef4\u5ea6, \u751f\u6210\u6a21\u578b"}}
{"id": "2505.21641", "pdf": "https://arxiv.org/pdf/2505.21641", "abs": "https://arxiv.org/abs/2505.21641", "authors": ["Maresa Schr\u00f6der", "Justin Hartenstein", "Stefan Feuerriegel"], "title": "PrivATE: Differentially Private Confidence Intervals for Average Treatment Effects", "categories": ["cs.LG", "cs.CR", "stat.ME"], "comment": null, "summary": "The average treatment effect (ATE) is widely used to evaluate the\neffectiveness of drugs and other medical interventions. In safety-critical\napplications like medicine, reliable inferences about the ATE typically require\nvalid uncertainty quantification, such as through confidence intervals (CIs).\nHowever, estimating treatment effects in these settings often involves\nsensitive data that must be kept private. In this work, we present PrivATE, a\nnovel machine learning framework for computing CIs for the ATE under\ndifferential privacy. Specifically, we focus on deriving valid\nprivacy-preserving CIs for the ATE from observational data. Our PrivATE\nframework consists of three steps: (i) estimating a differentially private ATE\nthrough output perturbation; (ii) estimating the differentially private\nvariance through a truncated output perturbation mechanism; and (iii)\nconstructing the CIs while accounting for the uncertainty from both the\nestimation and privatization steps. Our PrivATE framework is model agnostic,\ndoubly robust, and ensures valid CIs. We demonstrate the effectiveness of our\nframework using synthetic and real-world medical datasets. To the best of our\nknowledge, we are the first to derive a general, doubly robust framework for\nvalid CIs of the ATE under ($\\varepsilon$, $\\delta$)-differential privacy.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86PrivATE\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5dee\u5206\u9690\u79c1\u4e0b\u8ba1\u7b97\u5e73\u5747\u5904\u7406\u6548\u5e94\uff08ATE\uff09\u7684\u7f6e\u4fe1\u533a\u95f4\uff0c\u786e\u4fdd\u5728\u654f\u611f\u6570\u636e\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u548c\u7edf\u8ba1\u6709\u6548\u6027\u3002", "motivation": "\u5728\u533b\u7597\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\uff0c\u51c6\u786e\u8bc4\u4f30ATE\u9700\u8981\u53ef\u9760\u7684\u7f6e\u4fe1\u533a\u95f4\uff0c\u4f46\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u9650\u5236\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u9002\u7528\u6027\u3002\u56e0\u6b64\uff0c\u9700\u5f00\u53d1\u4e00\u79cd\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u63d0\u4f9b\u6709\u6548\u7edf\u8ba1\u63a8\u65ad\u7684\u6846\u67b6\u3002", "method": "PrivATE\u6846\u67b6\u5305\u542b\u4e09\u6b65\uff1a\u5dee\u5206\u9690\u79c1ATE\u4f30\u8ba1\u3001\u65b9\u5dee\u4f30\u8ba1\u53ca\u7f6e\u4fe1\u533a\u95f4\u6784\u5efa\u3002\u91c7\u7528\u8f93\u51fa\u6270\u52a8\u548c\u622a\u65ad\u673a\u5236\uff0c\u4e14\u5177\u5907\u6a21\u578b\u65e0\u5173\u6027\u548c\u53cc\u91cd\u7a33\u5065\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPrivATE\u5728\u5408\u6210\u548c\u771f\u5b9e\u533b\u5b66\u6570\u636e\u4e0a\u5747\u80fd\u6709\u6548\u751f\u6210\u6709\u6548\u7684\u7f6e\u4fe1\u533a\u95f4\uff0c\u586b\u8865\u4e86\u5dee\u5206\u9690\u79c1\u4e0bATE\u63a8\u65ad\u7684\u7a7a\u767d\u3002", "conclusion": "PrivATE\u662f\u9996\u4e2a\u5728\u5dee\u5206\u9690\u79c1\u4e0b\u63d0\u4f9b\u53cc\u91cd\u7a33\u5065\u6027ATE\u7f6e\u4fe1\u533a\u95f4\u7684\u901a\u7528\u6846\u67b6\uff0c\u517c\u5177\u9690\u79c1\u4fdd\u62a4\u548c\u7edf\u8ba1\u6709\u6548\u6027\u3002", "keywords": "\u5e73\u5747\u5904\u7406\u6548\u5e94,\u5dee\u5206\u9690\u79c1,\u7f6e\u4fe1\u533a\u95f4,\u673a\u5668\u5b66\u4e60,\u53cc\u91cd\u7a33\u5065\u6027"}}
{"id": "2505.22104", "pdf": "https://arxiv.org/pdf/2505.22104", "abs": "https://arxiv.org/abs/2505.22104", "authors": ["Davide Corsi", "Kaushik Mallik", "Andoni Rodriguez", "Cesar Sanchez"], "title": "Efficient Dynamic Shielding for Parametric Safety Specifications", "categories": ["cs.AI", "cs.LG", "cs.LO", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Shielding has emerged as a promising approach for ensuring safety of\nAI-controlled autonomous systems. The algorithmic goal is to compute a shield,\nwhich is a runtime safety enforcement tool that needs to monitor and intervene\nthe AI controller's actions if safety could be compromised otherwise.\nTraditional shields are designed statically for a specific safety requirement.\nTherefore, if the safety requirement changes at runtime due to changing\noperating conditions, the shield needs to be recomputed from scratch, causing\ndelays that could be fatal. We introduce dynamic shields for parametric safety\nspecifications, which are succinctly represented sets of all possible safety\nspecifications that may be encountered at runtime. Our dynamic shields are\nstatically designed for a given safety parameter set, and are able to\ndynamically adapt as the true safety specification (permissible by the\nparameters) is revealed at runtime. The main algorithmic novelty lies in the\ndynamic adaptation procedure, which is a simple and fast algorithm that\nutilizes known features of standard safety shields, like maximal\npermissiveness. We report experimental results for a robot navigation problem\nin unknown territories, where the safety specification evolves as new obstacles\nare discovered at runtime. In our experiments, the dynamic shields took a few\nminutes for their offline design, and took between a fraction of a second and a\nfew seconds for online adaptation at each step, whereas the brute-force online\nrecomputation approach was up to 5 times slower.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u9632\u62a4\u7f69\u65b9\u6cd5\uff0c\u7528\u4e8eAI\u63a7\u5236\u7684\u81ea\u4e3b\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u8fd0\u884c\u65f6\u5b89\u5168\u9700\u6c42\u53d8\u5316\u65f6\u5feb\u901f\u9002\u5e94\uff0c\u76f8\u6bd4\u4f20\u7edf\u9759\u6001\u9632\u62a4\u7f69\u66f4\u9ad8\u6548\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u9632\u62a4\u7f69\u5728\u5b89\u5168\u9700\u6c42\u53d8\u5316\u65f6\u9700\u91cd\u65b0\u8ba1\u7b97\uff0c\u53ef\u80fd\u5bfc\u81f4\u81f4\u547d\u5ef6\u8fdf\u3002\u52a8\u6001\u9632\u62a4\u7f69\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u8fd0\u884c\u65f6\u5b89\u5168\u4fdd\u62a4\u3002", "method": "\u8bbe\u8ba1\u4e86\u52a8\u6001\u9632\u62a4\u7f69\uff0c\u652f\u6301\u53c2\u6570\u5316\u5b89\u5168\u89c4\u8303\uff0c\u5229\u7528\u7b80\u5355\u5feb\u901f\u7684\u52a8\u6001\u9002\u5e94\u7b97\u6cd5\uff0c\u7ed3\u5408\u6807\u51c6\u5b89\u5168\u9632\u62a4\u7f69\u7684\u5df2\u77e5\u7279\u6027\uff08\u5982\u6700\u5927\u5141\u8bb8\u6027\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u52a8\u6001\u9632\u62a4\u7f69\u7684\u79bb\u7ebf\u8bbe\u8ba1\u4ec5\u9700\u51e0\u5206\u949f\uff0c\u5728\u7ebf\u9002\u5e94\u65f6\u95f4\u5728\u51e0\u5206\u4e4b\u4e00\u79d2\u5230\u51e0\u79d2\u4e4b\u95f4\uff0c\u6bd4\u66b4\u529b\u5728\u7ebf\u91cd\u65b0\u8ba1\u7b97\u65b9\u6cd5\u5feb5\u500d\u3002", "conclusion": "\u52a8\u6001\u9632\u62a4\u7f69\u4e3a\u8fd0\u884c\u65f6\u5b89\u5168\u9700\u6c42\u53d8\u5316\u7684\u573a\u666f\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9002\u5e94\u901f\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "keywords": "\u52a8\u6001\u9632\u62a4\u7f69, \u53c2\u6570\u5316\u5b89\u5168\u89c4\u8303, \u81ea\u4e3b\u7cfb\u7edf, \u8fd0\u884c\u65f6\u9002\u5e94, \u5b89\u5168\u4fdd\u62a4"}}
{"id": "2505.21859", "pdf": "https://arxiv.org/pdf/2505.21859", "abs": "https://arxiv.org/abs/2505.21859", "authors": ["Vishakh Padmakumar", "Zichao Wang", "David Arbour", "Jennifer Healey"], "title": "Principled Content Selection to Generate Diverse and Personalized Multi-Document Summaries", "categories": ["cs.CL"], "comment": "To appear at ACL 2025 - Main Conference", "summary": "While large language models (LLMs) are increasingly capable of handling\nlonger contexts, recent work has demonstrated that they exhibit the \"lost in\nthe middle\" phenomenon (Liu et al., 2024) of unevenly attending to different\nparts of the provided context. This hinders their ability to cover diverse\nsource material in multi-document summarization, as noted in the DiverseSumm\nbenchmark (Huang et al., 2024). In this work, we contend that principled\ncontent selection is a simple way to increase source coverage on this task. As\nopposed to prompting an LLM to perform the summarization in a single step, we\nexplicitly divide the task into three steps -- (1) reducing document\ncollections to atomic key points, (2) using determinantal point processes (DPP)\nto perform select key points that prioritize diverse content, and (3) rewriting\nto the final summary. By combining prompting steps, for extraction and\nrewriting, with principled techniques, for content selection, we consistently\nimprove source coverage on the DiverseSumm benchmark across various LLMs.\nFinally, we also show that by incorporating relevance to a provided user intent\ninto the DPP kernel, we can generate personalized summaries that cover relevant\nsource information while retaining coverage.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u4e09\u6b65\u6cd5\u6539\u8fdb\u591a\u6587\u6863\u6458\u8981\u7684\u6e90\u8986\u76d6\u8303\u56f4\uff1a\u63d0\u53d6\u5173\u952e\u70b9\u3001DPP\u9009\u62e9\u591a\u6837\u5316\u5185\u5bb9\u3001\u91cd\u5199\u751f\u6210\u6458\u8981\uff0c\u5e76\u5c55\u793a\u4e86\u5728DiverseSumm\u57fa\u51c6\u4e0a\u7684\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u6ce8\u610f\u529b\u4e0d\u5747\u8861\uff08\u2018\u4e2d\u95f4\u4e22\u5931\u2019\u73b0\u8c61\uff09\u5bfc\u81f4\u7684\u591a\u6587\u6863\u6458\u8981\u6e90\u8986\u76d6\u4e0d\u8db3\u95ee\u9898\u3002", "method": "1. \u63d0\u53d6\u539f\u5b50\u5173\u952e\u70b9\uff1b2. \u7528DPP\u9009\u62e9\u591a\u6837\u5316\u5173\u952e\u70b9\uff1b3. \u91cd\u5199\u4e3a\u6700\u7ec8\u6458\u8981\uff0c\u5e76\u7ed3\u5408\u7528\u6237\u610f\u56fe\u5b9e\u73b0\u4e2a\u6027\u5316\u3002", "result": "\u5728DiverseSumm\u57fa\u51c6\u4e0a\uff0c\u4e0d\u540cLLMs\u7684\u6e90\u8986\u76d6\u5747\u5f97\u5230\u4e00\u81f4\u63d0\u5347\uff0c\u4e14\u80fd\u751f\u6210\u4fdd\u7559\u8986\u76d6\u7387\u7684\u4e2a\u6027\u5316\u6458\u8981\u3002", "conclusion": "\u5206\u6b65\u65b9\u6cd5\uff08\u63d0\u53d6+\u9009\u62e9+\u91cd\u5199\uff09\u7ed3\u5408DPP\u663e\u8457\u63d0\u5347\u591a\u6587\u6863\u6458\u8981\u7684\u8986\u76d6\u7387\u548c\u4e2a\u6027\u5316\u80fd\u529b\u3002", "keywords": "\u591a\u6587\u6863\u6458\u8981, LLMs, DPP, \u6e90\u8986\u76d6\u7387, \u4e2a\u6027\u5316\u6458\u8981"}}
{"id": "2505.21651", "pdf": "https://arxiv.org/pdf/2505.21651", "abs": "https://arxiv.org/abs/2505.21651", "authors": ["Nikola Surjanovic", "Alexandre Bouchard-C\u00f4t\u00e9", "Trevor Campbell"], "title": "AutoSGD: Automatic Learning Rate Selection for Stochastic Gradient Descent", "categories": ["cs.LG", "math.OC", "stat.CO", "stat.ML"], "comment": null, "summary": "The learning rate is an important tuning parameter for stochastic gradient\ndescent (SGD) and can greatly influence its performance. However, appropriate\nselection of a learning rate schedule across all iterations typically requires\na non-trivial amount of user tuning effort. To address this, we introduce\nAutoSGD: an SGD method that automatically determines whether to increase or\ndecrease the learning rate at a given iteration and then takes appropriate\naction. We introduce theory supporting the convergence of AutoSGD, along with\nits deterministic counterpart for standard gradient descent. Empirical results\nsuggest strong performance of the method on a variety of traditional\noptimization problems and machine learning tasks.", "AI": {"tldr": "AutoSGD\u662f\u4e00\u79cd\u81ea\u52a8\u8c03\u6574\u5b66\u4e60\u7387\u7684SGD\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u589e\u51cf\u5b66\u4e60\u7387\u51cf\u5c11\u7528\u6237\u8c03\u53c2\u8d1f\u62c5\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u6536\u655b\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u591a\u79cd\u4f18\u5316\u548c\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5b66\u4e60\u7387\u662fSGD\u7684\u5173\u952e\u53c2\u6570\uff0c\u4f20\u7edf\u624b\u52a8\u8c03\u53c2\u8d39\u65f6\u8d39\u529b\uff0c\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faAutoSGD\uff0c\u52a8\u6001\u5224\u65ad\u5e76\u8c03\u6574\u6bcf\u8f6e\u8fed\u4ee3\u7684\u5b66\u4e60\u7387\u589e\u51cf\uff0c\u914d\u5957\u7406\u8bba\u652f\u6301\u5176\u6536\u655b\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1AutoSGD\u5728\u7ecf\u5178\u4f18\u5316\u95ee\u9898\u548c\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u6548\u679c\u663e\u8457\u3002", "conclusion": "AutoSGD\u901a\u8fc7\u81ea\u52a8\u5316\u5b66\u4e60\u7387\u8c03\u6574\u964d\u4f4e\u8c03\u53c2\u96be\u5ea6\uff0c\u517c\u5177\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u6027\u80fd\u4f18\u52bf\u3002", "keywords": "AutoSGD\u3001\u5b66\u4e60\u7387\u8c03\u6574\u3001\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u3001\u81ea\u52a8\u5316\u8c03\u53c2\u3001\u4f18\u5316\u7b97\u6cd5"}}
{"id": "2505.22112", "pdf": "https://arxiv.org/pdf/2505.22112", "abs": "https://arxiv.org/abs/2505.22112", "authors": ["Guangfu Hao", "Frederic Alexandre", "Shan Yu"], "title": "Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test", "categories": ["cs.AI", "q-bio.NC"], "comment": null, "summary": "Cognitive flexibility has been extensively studied in human cognition but\nremains relatively unexplored in the context of Visual Large Language Models\n(VLLMs). This study assesses the cognitive flexibility of state-of-the-art\nVLLMs (GPT-4o, Gemini-1.5 Pro, and Claude-3.5 Sonnet) using the Wisconsin Card\nSorting Test (WCST), a classic measure of set-shifting ability. Our results\nreveal that VLLMs achieve or surpass human-level set-shifting capabilities\nunder chain-of-thought prompting with text-based inputs. However, their\nabilities are highly influenced by both input modality and prompting strategy.\nIn addition, we find that through role-playing, VLLMs can simulate various\nfunctional deficits aligned with patients having impairments in cognitive\nflexibility, suggesting that VLLMs may possess a cognitive architecture, at\nleast regarding the ability of set-shifting, similar to the brain. This study\nreveals the fact that VLLMs have already approached the human level on a key\ncomponent underlying our higher cognition, and highlights the potential to use\nthem to emulate complex brain processes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\uff08VLLM\uff09\u7684\u8ba4\u77e5\u7075\u6d3b\u6027\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u601d\u7ef4\u94fe\u63d0\u793a\u4e0b\u80fd\u8fbe\u5230\u6216\u8d85\u8d8a\u4eba\u7c7b\u6c34\u5e73\u7684\u4efb\u52a1\u8f6c\u6362\u80fd\u529b\uff0c\u4f46\u5176\u8868\u73b0\u53d7\u8f93\u5165\u6a21\u6001\u548c\u63d0\u793a\u7b56\u7565\u5f71\u54cd\u3002", "motivation": "\u8ba4\u77e5\u7075\u6d3b\u6027\u5728\u4eba\u7c7b\u8ba4\u77e5\u4e2d\u5df2\u6709\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5728\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\uff08VLLM\uff09\u9886\u57df\u4ecd\u8f83\u5c11\u63a2\u8ba8\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528Wisconsin\u5361\u7247\u5206\u7c7b\u6d4b\u8bd5\uff08WCST\uff09\u8bc4\u4f30GPT-4o\u3001Gemini-1.5 Pro\u548cClaude-3.5 Sonnet\u7b49\u524d\u6cbfVLLM\u7684\u8ba4\u77e5\u7075\u6d3b\u6027\uff0c\u5e76\u5206\u6790\u4e86\u8f93\u5165\u6a21\u6001\u548c\u63d0\u793a\u7b56\u7565\u7684\u5f71\u54cd\u3002", "result": "VLLM\u5728\u601d\u7ef4\u94fe\u63d0\u793a\u4e0b\u80fd\u5339\u914d\u6216\u8d85\u8d8a\u4eba\u7c7b\u7684\u4efb\u52a1\u8f6c\u6362\u80fd\u529b\uff0c\u4e14\u80fd\u6a21\u62df\u8ba4\u77e5\u7075\u6d3b\u6027\u969c\u788d\u60a3\u8005\u7684\u529f\u80fd\u7f3a\u9677\u3002", "conclusion": "VLLM\u5728\u5173\u952e\u8ba4\u77e5\u80fd\u529b\u4e0a\u5df2\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\uff0c\u6709\u671b\u7528\u4e8e\u6a21\u62df\u590d\u6742\u7684\u8111\u8fc7\u7a0b\u3002", "keywords": "\u8ba4\u77e5\u7075\u6d3b\u6027,\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b,Wisconsin\u5361\u7247\u5206\u7c7b\u6d4b\u8bd5,\u4efb\u52a1\u8f6c\u6362,\u6a21\u62df"}}
{"id": "2505.21870", "pdf": "https://arxiv.org/pdf/2505.21870", "abs": "https://arxiv.org/abs/2505.21870", "authors": ["Shuyang Cao", "Karthik Radhakrishnan", "David Rosenberg", "Steven Lu", "Pengxiang Cheng", "Lu Wang", "Shiyue Zhang"], "title": "Evaluating the Retrieval Robustness of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages", "summary": "Retrieval-augmented generation (RAG) generally enhances large language\nmodels' (LLMs) ability to solve knowledge-intensive tasks. But RAG may also\nlead to performance degradation due to imperfect retrieval and the model's\nlimited ability to leverage retrieved content. In this work, we evaluate the\nrobustness of LLMs in practical RAG setups (henceforth retrieval robustness).\nWe focus on three research questions: (1) whether RAG is always better than\nnon-RAG; (2) whether more retrieved documents always lead to better\nperformance; (3) and whether document orders impact results. To facilitate this\nstudy, we establish a benchmark of 1500 open-domain questions, each with\nretrieved documents from Wikipedia. We introduce three robustness metrics, each\ncorresponds to one research question. Our comprehensive experiments, involving\n11 LLMs and 3 prompting strategies, reveal that all of these LLMs exhibit\nsurprisingly high retrieval robustness; nonetheless, different degrees of\nimperfect robustness hinders them from fully utilizing the benefits of RAG.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u9c81\u68d2\u6027\uff0c\u63a2\u8ba8\u4e86RAG\u662f\u5426\u603b\u662f\u4f18\u4e8e\u975eRAG\u3001\u66f4\u591a\u68c0\u7d22\u6587\u6863\u662f\u5426\u603b\u662f\u63d0\u5347\u6027\u80fd\u4ee5\u53ca\u6587\u6863\u987a\u5e8f\u662f\u5426\u5f71\u54cd\u7ed3\u679c\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1LLMs\u8868\u73b0\u51fa\u8f83\u9ad8\u7684\u68c0\u7d22\u9c81\u68d2\u6027\uff0c\u4f46\u4ecd\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22RAG\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u6548\u679c\uff0c\u5c24\u5176\u662f\u5176\u53ef\u80fd\u56e0\u68c0\u7d22\u4e0d\u5b8c\u7f8e\u6216\u6a21\u578b\u5229\u7528\u80fd\u529b\u6709\u9650\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u5efa\u7acb\u5305\u542b1500\u4e2a\u5f00\u653e\u57df\u95ee\u9898\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5f15\u5165\u4e09\u4e2a\u9c81\u68d2\u6027\u6307\u6807\u5bf9\u5e94\u4e09\u4e2a\u7814\u7a76\u95ee\u9898\uff0c\u5e76\u572811\u79cdLLMs\u548c3\u79cd\u63d0\u793a\u7b56\u7565\u4e0b\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u6240\u6709LLMs\u5747\u663e\u793a\u51fa\u8f83\u9ad8\u7684\u68c0\u7d22\u9c81\u68d2\u6027\uff0c\u4f46\u4e0d\u540c\u7a0b\u5ea6\u7684\u9c81\u68d2\u6027\u7f3a\u9677\u9650\u5236\u4e86\u5b83\u4eec\u5145\u5206\u5229\u7528RAG\u7684\u4f18\u52bf\u3002", "conclusion": "\u5c3d\u7ba1LLMs\u5728RAG\u4e2d\u8868\u73b0\u9c81\u68d2\uff0c\u4f46\u68c0\u7d22\u8d28\u91cf\u548c\u6a21\u578b\u5229\u7528\u80fd\u529b\u4ecd\u9700\u6539\u8fdb\u4ee5\u6700\u5927\u5316RAG\u7684\u6f5c\u529b\u3002", "keywords": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u3001\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3001\u68c0\u7d22\u9c81\u68d2\u6027\u3001\u5f00\u653e\u57df\u95ee\u9898\u3001\u63d0\u793a\u7b56\u7565"}}
{"id": "2505.21660", "pdf": "https://arxiv.org/pdf/2505.21660", "abs": "https://arxiv.org/abs/2505.21660", "authors": ["Xiaojie Xu", "Xinli Xu", "Sirui Chen", "Haoyu Chen", "Fan Zhang", "Ying-Cong Chen"], "title": "PreGenie: An Agentic Framework for High-quality Visual Presentation Generation", "categories": ["cs.LG"], "comment": "11 pages, 9 figures", "summary": "Visual presentations are vital for effective communication. Early attempts to\nautomate their creation using deep learning often faced issues such as poorly\norganized layouts, inaccurate text summarization, and a lack of image\nunderstanding, leading to mismatched visuals and text. These limitations\nrestrict their application in formal contexts like business and scientific\nresearch. To address these challenges, we propose PreGenie, an agentic and\nmodular framework powered by multimodal large language models (MLLMs) for\ngenerating high-quality visual presentations.\n  PreGenie is built on the Slidev presentation framework, where slides are\nrendered from Markdown code. It operates in two stages: (1) Analysis and\nInitial Generation, which summarizes multimodal input and generates initial\ncode, and (2) Review and Re-generation, which iteratively reviews intermediate\ncode and rendered slides to produce final, high-quality presentations. Each\nstage leverages multiple MLLMs that collaborate and share information.\nComprehensive experiments demonstrate that PreGenie excels in multimodal\nunderstanding, outperforming existing models in both aesthetics and content\nconsistency, while aligning more closely with human design preferences.", "AI": {"tldr": "PreGenie\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u89c9\u6f14\u793a\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\uff08\u5206\u6790\u4e0e\u521d\u59cb\u751f\u6210\u3001\u5ba1\u67e5\u4e0e\u91cd\u65b0\u751f\u6210\uff09\u63d0\u5347\u5e03\u5c40\u548c\u5185\u5bb9\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u89c6\u89c9\u6f14\u793a\u751f\u6210\u5b58\u5728\u5e03\u5c40\u6df7\u4e71\u3001\u6587\u672c\u6458\u8981\u4e0d\u51c6\u786e\u548c\u56fe\u6587\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u6b63\u5f0f\u573a\u5408\u7684\u5e94\u7528\u3002", "method": "\u57fa\u4e8eSlidev\u6846\u67b6\uff0c\u5206\u4e24\u9636\u6bb5\uff08\u5206\u6790\u4e0e\u521d\u59cb\u751f\u6210\u3001\u5ba1\u67e5\u4e0e\u91cd\u65b0\u751f\u6210\uff09\u8fed\u4ee3\u4f18\u5316\u6f14\u793a\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u534f\u4f5c\u3002", "result": "PreGenie\u5728\u591a\u6a21\u6001\u7406\u89e3\u548c\u7f8e\u5b66\u8bbe\u8ba1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u66f4\u7b26\u5408\u4eba\u7c7b\u8bbe\u8ba1\u504f\u597d\u3002", "conclusion": "PreGenie\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u591a\u6a21\u578b\u534f\u4f5c\uff0c\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u5316\u6f14\u793a\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u3002", "keywords": "\u89c6\u89c9\u6f14\u793a, \u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b, Slidev, \u81ea\u52a8\u5316\u751f\u6210"}}
{"id": "2505.22147", "pdf": "https://arxiv.org/pdf/2505.22147", "abs": "https://arxiv.org/abs/2505.22147", "authors": ["Florian Andreas Marwitz", "Tanya Braun", "Ralf M\u00f6ller", "Marcel Gehrke"], "title": "Lifted Forward Planning in Relational Factored Markov Decision Processes with Concurrent Actions", "categories": ["cs.AI"], "comment": null, "summary": "Decision making is a central problem in AI that can be formalized using a\nMarkov Decision Process. A problem is that, with increasing numbers of\n(indistinguishable) objects, the state space grows exponentially. To compute\npolicies, the state space has to be enumerated. Even more possibilities have to\nbe enumerated if the size of the action space depends on the size of the state\nspace, especially if we allow concurrent actions. To tackle the exponential\nblow-up in the action and state space, we present a first-order representation\nto store the spaces in polynomial instead of exponential size in the number of\nobjects and introduce Foreplan, a relational forward planner, which uses this\nrepresentation to efficiently compute policies for numerous indistinguishable\nobjects and actions. Additionally, we introduce an even faster approximate\nversion of Foreplan. Moreover, Foreplan identifies how many objects an agent\nshould act on to achieve a certain task given restrictions. Further, we provide\na theoretical analysis and an empirical evaluation of Foreplan, demonstrating a\nspeedup of at least four orders of magnitude.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aForeplan\u7684\u5173\u7cfb\u524d\u5411\u89c4\u5212\u5668\uff0c\u4f7f\u7528\u4e00\u9636\u8868\u793a\u6cd5\u6765\u5e94\u5bf9\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u7684\u6307\u6570\u7ea7\u589e\u957f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u968f\u7740\uff08\u4e0d\u53ef\u533a\u5206\u7684\uff09\u5bf9\u8c61\u6570\u91cf\u7684\u589e\u52a0\uff0c\u72b6\u6001\u7a7a\u95f4\u5448\u6307\u6570\u7ea7\u589e\u957f\uff0c\u4f7f\u5f97\u8ba1\u7b97\u7b56\u7565\u53d8\u5f97\u56f0\u96be\u3002\u9488\u5bf9\u8fd9\u4e00\u95ee\u9898\uff0c\u8bba\u6587\u65e8\u5728\u627e\u5230\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5904\u7406\u5927\u89c4\u6a21\u7684\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u3002", "method": "\u5f15\u5165\u4e00\u9636\u8868\u793a\u6cd5\u6765\u4ee5\u591a\u9879\u5f0f\u5927\u5c0f\u5b58\u50a8\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\uff0c\u5e76\u63d0\u51fa\u4e86Foreplan\u5173\u7cfb\u524d\u5411\u89c4\u5212\u5668\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u5feb\u901f\u7684\u8fd1\u4f3c\u7248\u672c\u3002\u6b64\u5916\uff0cForeplan\u80fd\u591f\u786e\u5b9a\u4e3a\u4e86\u5b8c\u6210\u7279\u5b9a\u4efb\u52a1\u9700\u8981\u64cd\u4f5c\u7684\u5bf9\u8c61\u6570\u91cf\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cForeplan\u5b9e\u73b0\u4e86\u81f3\u5c11\u56db\u4e2a\u6570\u91cf\u7ea7\u7684\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "Foreplan\u663e\u8457\u63d0\u9ad8\u4e86\u5904\u7406\u5927\u89c4\u6a21\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u95ee\u9898\u7684\u6548\u7387\uff0c\u5c24\u5176\u5728\u5904\u7406\u5927\u91cf\u4e0d\u53ef\u533a\u5206\u5bf9\u8c61\u65f6\u8868\u73b0\u4f18\u5f02\u3002", "keywords": "\u51b3\u7b56\u5236\u5b9a, \u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b, Foreplan, \u5173\u7cfb\u524d\u5411\u89c4\u5212\u5668, \u72b6\u6001\u7a7a\u95f4, \u52a8\u4f5c\u7a7a\u95f4"}}
{"id": "2505.21889", "pdf": "https://arxiv.org/pdf/2505.21889", "abs": "https://arxiv.org/abs/2505.21889", "authors": ["Tianyu Guo", "Hande Dong", "Yichong Leng", "Feng Liu", "Cheater Lin", "Nong Xiao", "Xianwei Zhang"], "title": "EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are often used for infilling tasks, which\ninvolve predicting or generating missing information in a given text. These\ntasks typically require multiple interactions with similar context. To reduce\nthe computation of repeated historical tokens, cross-request key-value (KV)\ncache reuse, a technique that stores and reuses intermediate computations, has\nbecome a crucial method in multi-round interactive services. However, in\ninfilling tasks, the KV cache reuse is often hindered by the structure of the\nprompt format, which typically consists of a prefix and suffix relative to the\ninsertion point. Specifically, the KV cache of the prefix or suffix part is\nfrequently invalidated as the other part (suffix or prefix) is incrementally\ngenerated. To address the issue, we propose EFIM, a transformed prompt format\nof FIM to unleash the performance potential of KV cache reuse. Although the\ntransformed prompt can solve the inefficiency, it exposes subtoken generation\nproblems in current LLMs, where they have difficulty generating partial words\naccurately. Therefore, we introduce a fragment tokenization training method\nwhich splits text into multiple fragments before tokenization during data\nprocessing. Experiments on two representative LLMs show that LLM serving with\nEFIM can lower the latency by 52% and improve the throughput by 98% while\nmaintaining the original infilling capability.EFIM's source code is publicly\navailable at https://github.com/gty111/EFIM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEFIM\uff0c\u4e00\u79cd\u6539\u8fdb\u7684\u63d0\u793a\u683c\u5f0f\uff0c\u7ed3\u5408\u7247\u6bb5\u6807\u8bb0\u5316\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347KV\u7f13\u5b58\u91cd\u7528\u6548\u7387\uff0c\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u541e\u5410\u91cf\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u586b\u5145\u4efb\u52a1\u4e2d\uff0c\u7531\u4e8e\u63d0\u793a\u683c\u5f0f\u7684\u7ed3\u6784\u95ee\u9898\uff0cKV\u7f13\u5b58\u91cd\u7528\u6548\u7387\u4f4e\u4e0b\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u3002", "method": "\u63d0\u51faEFIM\u63d0\u793a\u683c\u5f0f\uff0c\u7ed3\u5408\u7247\u6bb5\u6807\u8bb0\u5316\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f18\u5316KV\u7f13\u5b58\u91cd\u7528\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cEFIM\u80fd\u964d\u4f4e52%\u5ef6\u8fdf\uff0c\u63d0\u534798%\u541e\u5410\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u6709\u586b\u5145\u80fd\u529b\u3002", "conclusion": "EFIM\u6709\u6548\u89e3\u51b3KV\u7f13\u5b58\u91cd\u7528\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u586b\u5145\u4efb\u52a1\u6548\u7387\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u586b\u5145\u4efb\u52a1\u3001KV\u7f13\u5b58\u91cd\u7528\u3001EFIM\u3001\u7247\u6bb5\u6807\u8bb0\u5316"}}
{"id": "2505.21666", "pdf": "https://arxiv.org/pdf/2505.21666", "abs": "https://arxiv.org/abs/2505.21666", "authors": ["Owen Oertell", "Shikun Sun", "Yiding Chen", "Jin Peng Zhou", "Zhiyong Wang", "Wen Sun"], "title": "Efficient Controllable Diffusion via Optimal Classifier Guidance", "categories": ["cs.LG", "cs.AI"], "comment": "28 pages, 9 figures, 3 tables", "summary": "The controllable generation of diffusion models aims to steer the model to\ngenerate samples that optimize some given objective functions. It is desirable\nfor a variety of applications including image generation, molecule generation,\nand DNA/sequence generation. Reinforcement Learning (RL) based fine-tuning of\nthe base model is a popular approach but it can overfit the reward function\nwhile requiring significant resources. We frame controllable generation as a\nproblem of finding a distribution that optimizes a KL-regularized objective\nfunction. We present SLCD -- Supervised Learning based Controllable Diffusion,\nwhich iteratively generates online data and trains a small classifier to guide\nthe generation of the diffusion model. Similar to the standard\nclassifier-guided diffusion, SLCD's key computation primitive is classification\nand does not involve any complex concepts from RL or control. Via a reduction\nto no-regret online learning analysis, we show that under KL divergence, the\noutput from SLCD provably converges to the optimal solution of the\nKL-regularized objective. Further, we empirically demonstrate that SLCD can\ngenerate high quality samples with nearly the same inference time as the base\nmodel in both image generation with continuous diffusion and biological\nsequence generation with discrete diffusion. Our code is available at\nhttps://github.com/Owen-Oertell/slcd", "AI": {"tldr": "SLCD\u65b9\u6cd5\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u4f18\u5316\u6269\u6563\u6a21\u578b\u7684\u53ef\u63a7\u751f\u6210\uff0c\u907f\u514d\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u590d\u6742\u6027\u548c\u8d44\u6e90\u6d88\u8017\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u6837\u672c\u751f\u6210\u4e0e\u9ad8\u6548\u63a8\u7406\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u53ef\u63a7\u751f\u6210\u4e2d\u53ef\u80fd\u8fc7\u62df\u5408\u5956\u52b1\u51fd\u6570\u4e14\u8d44\u6e90\u6d88\u8017\u5927\uff0cSLCD\u65e8\u5728\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u7b80\u5316\u6d41\u7a0b\u5e76\u63d0\u5347\u6548\u7387\u3002", "method": "SLCD\u8fed\u4ee3\u751f\u6210\u5728\u7ebf\u6570\u636e\u5e76\u8bad\u7ec3\u5c0f\u578b\u5206\u7c7b\u5668\uff0c\u4ee5\u6307\u5bfc\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\uff0c\u6838\u5fc3\u4e3a\u5206\u7c7b\u4efb\u52a1\u800c\u975e\u590d\u6742RL\u63a7\u5236\u3002", "result": "\u7406\u8bba\u4e0aSLCD\u5728KL\u6563\u5ea6\u4e0b\u6536\u655b\u4e8e\u6700\u4f18\u89e3\uff1b\u5b9e\u8bc1\u4e2d\u5728\u56fe\u50cf\u548c\u751f\u7269\u5e8f\u5217\u751f\u6210\u4e2d\u5747\u5b9e\u73b0\u9ad8\u8d28\u91cf\u8f93\u51fa\u4e14\u63a8\u7406\u65f6\u95f4\u63a5\u8fd1\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "SLCD\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7b80\u5355\u9ad8\u6548\u7684\u53ef\u63a7\u751f\u6210\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8fde\u7eed\u548c\u79bb\u6563\u6269\u6563\u4efb\u52a1\u3002", "keywords": "\u6269\u6563\u6a21\u578b,\u53ef\u63a7\u751f\u6210,\u76d1\u7763\u5b66\u4e60,KL\u6563\u5ea6,\u5728\u7ebf\u5b66\u4e60"}}
{"id": "2505.22148", "pdf": "https://arxiv.org/pdf/2505.22148", "abs": "https://arxiv.org/abs/2505.22148", "authors": ["Gangwei Jiang", "Yahui Liu", "Zhaoyi Li", "Qi Wang", "Fuzheng Zhang", "Linqi Song", "Ying Wei", "Defu Lian"], "title": "What Makes a Good Reasoning Chain? Uncovering Structural Patterns in Long Chain-of-Thought Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in reasoning with large language models (LLMs) have\npopularized Long Chain-of-Thought (LCoT), a strategy that encourages deliberate\nand step-by-step reasoning before producing a final answer. While LCoTs have\nenabled expert-level performance in complex tasks, how the internal structures\nof their reasoning chains drive, or even predict, the correctness of final\nanswers remains a critical yet underexplored question. In this work, we present\nLCoT2Tree, an automated framework that converts sequential LCoTs into\nhierarchical tree structures and thus enables deeper structural analysis of LLM\nreasoning. Using graph neural networks (GNNs), we reveal that structural\npatterns extracted by LCoT2Tree, including exploration, backtracking, and\nverification, serve as stronger predictors of final performance across a wide\nrange of tasks and models. Leveraging an explainability technique, we further\nidentify critical thought patterns such as over-branching that account for\nfailures. Beyond diagnostic insights, the structural patterns by LCoT2Tree\nsupport practical applications, including improving Best-of-N decoding\neffectiveness. Overall, our results underscore the critical role of internal\nstructures of reasoning chains, positioning LCoT2Tree as a powerful tool for\ndiagnosing, interpreting, and improving reasoning in LLMs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86LCoT2Tree\u6846\u67b6\uff0c\u5c06\u957f\u601d\u7ef4\u94fe\uff08LCoT\uff09\u8f6c\u5316\u4e3a\u6811\u72b6\u7ed3\u6784\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u63ed\u793a\u601d\u7ef4\u94fe\u5185\u90e8\u7ed3\u6784\u4e0e\u6700\u7ec8\u7b54\u6848\u6b63\u786e\u6027\u7684\u5173\u7cfb\uff0c\u5e76\u8bc6\u522b\u5bfc\u81f4\u5931\u8d25\u7684\u5173\u952e\u6a21\u5f0f\u3002", "motivation": "\u63a2\u7a76\u957f\u601d\u7ef4\u94fe\u5185\u90e8\u7ed3\u6784\u5982\u4f55\u5f71\u54cd\u6216\u9884\u6d4b\u6700\u7ec8\u7b54\u6848\u7684\u6b63\u786e\u6027\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u5f00\u53d1LCoT2Tree\u6846\u67b6\uff0c\u5c06\u5e8f\u5217\u5316\u601d\u7ef4\u94fe\u8f6c\u5316\u4e3a\u5c42\u6b21\u6811\u7ed3\u6784\uff0c\u5e76\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u5206\u6790\u7ed3\u6784\u548c\u6027\u80fd\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u601d\u7ef4\u94fe\u7684\u7ed3\u6784\u6a21\u5f0f\uff08\u5982\u63a2\u7d22\u3001\u56de\u6eaf\u548c\u9a8c\u8bc1\uff09\u5bf9\u4efb\u52a1\u8868\u73b0\u6709\u66f4\u5f3a\u9884\u6d4b\u529b\uff0c\u5e76\u8bc6\u522b\u4e86\u5bfc\u81f4\u5931\u8d25\u7684\u5173\u952e\u6a21\u5f0f\uff08\u5982\u8fc7\u5ea6\u5206\u652f\uff09\u3002", "conclusion": "\u601d\u7ef4\u94fe\u5185\u90e8\u7ed3\u6784\u5bf9LLM\u63a8\u7406\u81f3\u5173\u91cd\u8981\uff0cLCoT2Tree\u53ef\u4f5c\u4e3a\u8bca\u65ad\u3001\u89e3\u91ca\u548c\u4f18\u5316\u63a8\u7406\u7684\u5f3a\u5927\u5de5\u5177\u3002", "keywords": "\u957f\u601d\u7ef4\u94fe\uff08LCoT\uff09\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u3001\u63a8\u7406\u7ed3\u6784\u3001LCoT2Tree\u3001\u601d\u7ef4\u6a21\u5f0f"}}
{"id": "2505.21898", "pdf": "https://arxiv.org/pdf/2505.21898", "abs": "https://arxiv.org/abs/2505.21898", "authors": ["Rennai Qiu", "Chen Qian", "Ran Li", "Yufan Dang", "Weize Chen", "Cheng Yang", "Yingli Zhang", "Ye Tian", "Xuantang Xiong", "Lei Han", "Zhiyuan Liu", "Maosong Sun"], "title": "Co-Saving: Resource Aware Multi-Agent Collaboration for Software Development", "categories": ["cs.CL", "cs.AI", "cs.MA", "cs.SE"], "comment": "Work in Progress", "summary": "Recent advancements in Large Language Models (LLMs) and autonomous agents\nhave demonstrated remarkable capabilities across various domains. However,\nstandalone agents frequently encounter limitations when handling complex tasks\nthat demand extensive interactions and substantial computational resources.\nAlthough Multi-Agent Systems (MAS) alleviate some of these limitations through\ncollaborative mechanisms like task decomposition, iterative communication, and\nrole specialization, they typically remain resource-unaware, incurring\nsignificant inefficiencies due to high token consumption and excessive\nexecution time. To address these limitations, we propose a resource-aware\nmulti-agent system -- Co-Saving (meaning that multiple agents collaboratively\nengage in resource-saving activities), which leverages experiential knowledge\nto enhance operational efficiency and solution quality. Our key innovation is\nthe introduction of \"shortcuts\" -- instructional transitions learned from\nhistorically successful trajectories -- which allows to bypass redundant\nreasoning agents and expedite the collective problem-solving process.\nExperiments for software development tasks demonstrate significant advantages\nover existing methods. Specifically, compared to the state-of-the-art MAS\nChatDev, our method achieves an average reduction of 50.85% in token usage, and\nimproves the overall code quality by 10.06%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8d44\u6e90\u611f\u77e5\u7684\u591a\u4ee3\u7406\u7cfb\u7edfCo-Saving\uff0c\u901a\u8fc7\u5b66\u4e60\u5386\u53f2\u6210\u529f\u8f68\u8ff9\u5f15\u5165\u2018\u5feb\u6377\u65b9\u5f0f\u2019\uff0c\u663e\u8457\u51cf\u5c11\u4ee4\u724c\u4f7f\u752850.85%\uff0c\u5e76\u63d0\u5347\u4ee3\u7801\u8d28\u91cf10.06%\u3002", "motivation": "\u5f53\u524d\u591a\u4ee3\u7406\u7cfb\u7edf\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8d44\u6e90\u6548\u7387\u4f4e\u4e0b\uff0c\u7f3a\u4e4f\u8d44\u6e90\u611f\u77e5\u80fd\u529b\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u534f\u4f5c\u673a\u5236\u4f18\u5316\u8d44\u6e90\u4f7f\u7528\u548c\u8d28\u91cf\u63d0\u5347\u3002", "method": "\u5f15\u5165\u4e86\u57fa\u4e8e\u5386\u53f2\u7ecf\u9a8c\u7684\u2018\u5feb\u6377\u65b9\u5f0f\u2019\u673a\u5236\uff0c\u7ed5\u8fc7\u5197\u4f59\u63a8\u7406\u4ee3\u7406\uff0c\u4f18\u5316\u534f\u4f5c\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u663e\u793aCo-Saving\u5728\u8f6f\u4ef6\u4efb\u52a1\u4e2d\u4ee4\u724c\u4f7f\u7528\u51cf\u5c1150.85%\uff0c\u4ee3\u7801\u8d28\u91cf\u63d0\u534710.06%\u3002", "conclusion": "Co-Saving\u901a\u8fc7\u8d44\u6e90\u611f\u77e5\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u6548\u7387\u548c\u7ed3\u679c\u8d28\u91cf\u3002", "keywords": "\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u8d44\u6e90\u611f\u77e5\uff0c\u5feb\u6377\u65b9\u5f0f\uff0c\u4ee4\u724c\u4f18\u5316\uff0c\u4ee3\u7801\u8d28\u91cf"}}
{"id": "2505.21677", "pdf": "https://arxiv.org/pdf/2505.21677", "abs": "https://arxiv.org/abs/2505.21677", "authors": ["Hung Ahn Vu", "Galen Reeves", "Emily Wenger"], "title": "What happens when generative AI models train recursively on each others' generated outputs?", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": "9 pages", "summary": "The internet is full of AI-generated content while also serving as a common\nsource of training data for generative AI (genAI) models. This duality raises\nthe possibility that future genAI models may be trained on other models'\ngenerated outputs. Prior work has studied consequences of models training on\ntheir own generated outputs, but limited work has considered what happens if\nmodels ingest content produced by other models. Given society's increasing\ndependence on genAI tools, understanding downstream effects of such\ndata-mediated model interactions is critical. To this end, we provide empirical\nevidence for how data-mediated interactions might unfold in practice, develop a\ntheoretical model for this interactive training process, and show\nexperimentally possible long-term results of such interactions. We find that\ndata-mediated interactions can benefit models by exposing them to novel\nconcepts perhaps missed in original training data, but also can homogenize\ntheir performance on shared tasks.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u751f\u6210AI\u6a21\u578b\u8bad\u7ec3\u4e8e\u5176\u4ed6\u6a21\u578b\u751f\u6210\u5185\u5bb9\u7684\u5f71\u54cd\uff0c\u63a2\u8ba8\u4e86\u6570\u636e\u4ea4\u4e92\u53ef\u80fd\u5e26\u6765\u7684\u76ca\u5904\uff08\u5982\u63a5\u89e6\u65b0\u6982\u5ff5\uff09\u548c\u98ce\u9669\uff08\u5982\u6027\u80fd\u540c\u8d28\u5316\uff09\u3002", "motivation": "\u968f\u7740\u793e\u4f1a\u5bf9\u751f\u6210AI\u5de5\u5177\u7684\u4f9d\u8d56\u589e\u52a0\uff0c\u7406\u89e3\u6a21\u578b\u95f4\u901a\u8fc7\u6570\u636e\u4ea4\u4e92\u53ef\u80fd\u4ea7\u751f\u7684\u4e0b\u6e38\u5f71\u54cd\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u548c\u7406\u8bba\u5efa\u6a21\uff0c\u7814\u7a76\u4e86\u6570\u636e\u4ea4\u4e92\u7684\u5b9e\u8df5\u8fc7\u7a0b\u548c\u957f\u671f\u6548\u5e94\u3002", "result": "\u6570\u636e\u4ea4\u4e92\u65e2\u80fd\u8ba9\u6a21\u578b\u63a5\u89e6\u65b0\u6982\u5ff5\uff0c\u4e5f\u53ef\u80fd\u5bfc\u81f4\u4efb\u52a1\u6027\u80fd\u540c\u8d28\u5316\u3002", "conclusion": "\u6570\u636e\u4ea4\u4e92\u5177\u6709\u53cc\u91cd\u5f71\u54cd\uff0c\u9700\u5728\u5229\u7528\u5176\u597d\u5904\u7684\u540c\u65f6\u8b66\u60d5\u6f5c\u5728\u98ce\u9669\u3002", "keywords": "\u751f\u6210AI, \u6570\u636e\u4ea4\u4e92, \u540c\u8d28\u5316, \u65b0\u6982\u5ff5, \u6a21\u578b\u8bad\u7ec3"}}
{"id": "2505.22244", "pdf": "https://arxiv.org/pdf/2505.22244", "abs": "https://arxiv.org/abs/2505.22244", "authors": ["Yaron Halle", "Ariel Felner", "Sven Koenig", "Oren Salzman"], "title": "A Preprocessing Framework for Efficient Approximate Bi-Objective Shortest-Path Computation in the Presence of Correlated Objectives", "categories": ["cs.AI"], "comment": null, "summary": "The bi-objective shortest-path (BOSP) problem seeks to find paths between\nstart and target vertices of a graph while optimizing two conflicting objective\nfunctions. We consider the BOSP problem in the presence of correlated\nobjectives. Such correlations often occur in real-world settings such as road\nnetworks, where optimizing two positively correlated objectives, such as travel\ntime and fuel consumption, is common. BOSP is generally computationally\nchallenging as the size of the search space is exponential in the number of\nobjective functions and the graph size. Bounded sub-optimal BOSP solvers such\nas A*pex alleviate this complexity by approximating the Pareto-optimal solution\nset rather than computing it exactly (given a user-provided approximation\nfactor). As the correlation between objective functions increases, smaller\napproximation factors are sufficient for collapsing the entire Pareto-optimal\nset into a single solution. We leverage this insight to propose an efficient\nalgorithm that reduces the search effort in the presence of correlated\nobjectives. Our approach for computing approximations of the entire\nPareto-optimal set is inspired by graph-clustering algorithms. It uses a\npreprocessing phase to identify correlated clusters within a graph and to\ngenerate a new graph representation. This allows a natural generalization of\nA*pex to run up to five times faster on DIMACS dataset instances, a standard\nbenchmark in the field. To the best of our knowledge, this is the first\nalgorithm proposed that efficiently and effectively exploits correlations in\nthe context of bi-objective search while providing theoretical guarantees on\nsolution quality.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u53cc\u76ee\u6807\u6700\u77ed\u8def\u5f84\u95ee\u9898\u7684\u9ad8\u6548\u7b97\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u76ee\u6807\u76f8\u5173\u6027\u51cf\u5c11\u641c\u7d22\u590d\u6742\u5ea6\uff0c\u6bd4\u6807\u51c6\u7b97\u6cd5\u5feb\u4e94\u500d\u3002", "motivation": "\u89e3\u51b3\u53cc\u76ee\u6807\u6700\u77ed\u8def\u5f84\u95ee\u9898\u5728\u76f8\u5173\u6027\u76ee\u6807\u4e0b\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u73b0\u5b9e\u573a\u666f\u5982\u8def\u7f51\u4e2d\u5e38\u5b58\u5728\u76f8\u5173\u6027\u76ee\u6807\uff08\u5982\u65f6\u95f4\u548c\u6cb9\u8017\uff09\u3002", "method": "\u91c7\u7528\u56fe\u5f62\u805a\u7c7b\u9884\u5904\u7406\uff0c\u8bc6\u522b\u76f8\u5173\u6027\u96c6\u7fa4\u5e76\u751f\u6210\u65b0\u56fe\u8868\u793a\uff0c\u6269\u5c55A*pex\u7b97\u6cd5\u3002", "result": "\u5728\u6807\u51c6\u6570\u636e\u96c6\uff08DIMACS\uff09\u4e0a\u6bd4\u6807\u51c6\u7b97\u6cd5\u5feb\u4e94\u500d\u3002", "conclusion": "\u9996\u6b21\u63d0\u51fa\u5229\u7528\u76f8\u5173\u6027\u9ad8\u6548\u89e3\u51b3\u53cc\u76ee\u6807\u641c\u7d22\u7684\u7b97\u6cd5\uff0c\u5e76\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u3002", "keywords": "\u53cc\u76ee\u6807\u6700\u77ed\u8def\u5f84,\u76f8\u5173\u6027,A*pex,\u56fe\u5f62\u805a\u7c7b,DIMACS"}}
{"id": "2505.21926", "pdf": "https://arxiv.org/pdf/2505.21926", "abs": "https://arxiv.org/abs/2505.21926", "authors": ["Yin Hua", "Zhiqiang Liu", "Mingyang Chen", "Zheng Fang", "Chi Man Wong", "Lingxiao Li", "Chi Man Vong", "Huajun Chen", "Wen Zhang"], "title": "Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "In natural language processing (NLP) and computer vision (CV), the successful\napplication of foundation models across diverse tasks has demonstrated their\nremarkable potential. However, despite the rich structural and textual\ninformation embedded in knowledge graphs (KGs), existing research of foundation\nmodel for KG has primarily focused on their structural aspects, with most\nefforts restricted to in-KG tasks (e.g., knowledge graph completion, KGC). This\nlimitation has hindered progress in addressing more challenging out-of-KG\ntasks. In this paper, we introduce MERRY, a foundation model for general\nknowledge graph reasoning, and investigate its performance across two task\ncategories: in-KG reasoning tasks (e.g., KGC) and out-of-KG tasks (e.g., KG\nquestion answering, KGQA). We not only utilize the structural information, but\nalso the textual information in KGs. Specifically, we propose a\nmulti-perspective Conditional Message Passing (CMP) encoding architecture to\nbridge the gap between textual and structural modalities, enabling their\nseamless integration. Additionally, we introduce a dynamic residual fusion\nmodule to selectively retain relevant textual information and a flexible edge\nscoring mechanism to adapt to diverse downstream tasks. Comprehensive\nevaluations on 28 datasets demonstrate that MERRY outperforms existing\nbaselines in most scenarios, showcasing strong reasoning capabilities within\nKGs and excellent generalization to out-of-KG tasks such as KGQA.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86MERRY\uff0c\u4e00\u4e2a\u7528\u4e8e\u901a\u7528\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u7684\u57fa\u7840\u6a21\u578b\uff0c\u7ed3\u5408\u7ed3\u6784\u548c\u6587\u672c\u4fe1\u606f\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u6761\u4ef6\u6d88\u606f\u4f20\u9012\u7f16\u7801\u67b6\u6784\u548c\u52a8\u6001\u6b8b\u5dee\u878d\u5408\u6a21\u5757\uff0c\u572828\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u5f53\u524d\u77e5\u8bc6\u56fe\u8c31\u57fa\u7840\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u7ed3\u6784\u4fe1\u606f\uff0c\u5ffd\u89c6\u4e86\u6587\u672c\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5728\u66f4\u5177\u6311\u6218\u6027\u7684\u975e\u56fe\u8c31\u4efb\u52a1\u4e2d\u7684\u8fdb\u5c55\u3002", "method": "\u63d0\u51faMERRY\u6a21\u578b\uff0c\u5305\u62ec\u591a\u89c6\u89d2\u6761\u4ef6\u6d88\u606f\u4f20\u9012\u7f16\u7801\u67b6\u6784\u3001\u52a8\u6001\u6b8b\u5dee\u878d\u5408\u6a21\u5757\u548c\u7075\u6d3b\u7684\u8fb9\u8bc4\u5206\u673a\u5236\uff0c\u6574\u5408\u7ed3\u6784\u548c\u6587\u672c\u6a21\u6001\u3002", "result": "\u572828\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cMERRY\u5728\u5927\u591a\u6570\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u56fe\u8c31\u5185\u63a8\u7406\u80fd\u529b\u548c\u826f\u597d\u7684\u975e\u56fe\u8c31\u4efb\u52a1\u6cdb\u5316\u6027\u3002", "conclusion": "MERRY\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u548c\u6587\u672c\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u975e\u56fe\u8c31\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "keywords": "\u77e5\u8bc6\u56fe\u8c31,\u57fa\u7840\u6a21\u578b,\u63a8\u7406,\u591a\u6a21\u6001,\u6761\u4ef6\u6d88\u606f\u4f20\u9012"}}
{"id": "2505.21680", "pdf": "https://arxiv.org/pdf/2505.21680", "abs": "https://arxiv.org/abs/2505.21680", "authors": ["Andrew J. Loza", "Jun Yup Kim", "Shangzheng Song", "Yihang Liu", "Joseph J. Y. Sung", "R Andrew Taylor", "Dennis L. Shung"], "title": "multivariateGPT: a decoder-only transformer for multivariate categorical and numeric data", "categories": ["cs.LG", "cs.AI", "68T07", "I.2.6; I.5.1"], "comment": "15 pates, 5 figures", "summary": "Real-world processes often generate data that are a mix of categorical and\nnumeric values that are recorded at irregular and informative intervals.\nDiscrete token-based approaches are limited in numeric representation capacity\nwhile methods like neural ordinary differential equations are not well suited\nfor categorical data or informative sampling and require augmentation to handle\ncertain classes of trajectories. Here, we present multivariateGPT, a single\narchitecture for modeling sequences of mixed categorical (including tokenized\ntext) and numeric data. This is accomplished with an autoregressive sequence\ndecomposition, embedding scheme, and loss function that extend the next token\nprediction task to likelihood estimation of the joint distribution of next\ntoken class and value. We demonstrate how this approach can efficiently learn\nto generalize patterns in simple physical systems and model complex time series\nincluding electrocardiograms and multivariate electronic health record data.\nThis work extends the utility of transformer based models to additional classes\nof data.", "AI": {"tldr": "\u591a\u5143GPT\u63d0\u51fa\u4e86\u4e00\u79cd\u5355\u4e00\u67b6\u6784\uff0c\u7528\u4e8e\u5904\u7406\u6df7\u5408\u5206\u7c7b\u548c\u6570\u503c\u6570\u636e\u7684\u5e8f\u5217\u5efa\u6a21\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u5e8f\u5217\u5206\u89e3\u548c\u5d4c\u5165\u65b9\u6848\u6269\u5c55\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5e94\u7528\u8303\u56f4\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u6570\u636e\u5e38\u4e3a\u6df7\u5408\u7c7b\u578b\u4e14\u91c7\u6837\u4e0d\u89c4\u5219\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u79bb\u6563\u6807\u8bb0\u6216\u795e\u7ecfODE\uff09\u5728\u5904\u7406\u6b64\u7c7b\u6570\u636e\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u901a\u7528\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u81ea\u56de\u5f52\u5e8f\u5217\u5206\u89e3\u548c\u7279\u5b9a\u5d4c\u5165\u65b9\u6848\uff0c\u5c06\u6a21\u578b\u6269\u5c55\u4e3a\u8054\u5408\u4f30\u8ba1\u4e0b\u4e2a\u6807\u8bb0\u7c7b\u522b\u4e0e\u503c\u7684\u5206\u5e03\uff0c\u9002\u7528\u4e8e\u5206\u7c7b\u4e0e\u6570\u503c\u6570\u636e\u7684\u6df7\u5408\u5904\u7406\u3002", "result": "\u6a21\u578b\u5728\u7b80\u5355\u7269\u7406\u7cfb\u7edf\u3001\u5fc3\u7535\u56fe\u548c\u591a\u53d8\u91cf\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u4e2d\u663e\u793a\u51fa\u9ad8\u6548\u7684\u6a21\u5f0f\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u591a\u5143GPT\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u6269\u5c55\u4e86\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u5728\u5904\u7406\u6df7\u5408\u7c7b\u578b\u6570\u636e\u4e0a\u7684\u5b9e\u7528\u6027\u3002", "keywords": "\u6df7\u5408\u6570\u636e\u7c7b\u578b\u3001\u81ea\u56de\u5f52\u5efa\u6a21\u3001\u5d4c\u5165\u65b9\u6848\u3001Transformer"}}
{"id": "2505.22288", "pdf": "https://arxiv.org/pdf/2505.22288", "abs": "https://arxiv.org/abs/2505.22288", "authors": ["Jan Speller", "Malte Luttermann", "Marcel Gehrke", "Tanya Braun"], "title": "Compression versus Accuracy: A Hierarchy of Lifted Models", "categories": ["cs.AI"], "comment": null, "summary": "Probabilistic graphical models that encode indistinguishable objects and\nrelations among them use first-order logic constructs to compress a\npropositional factorised model for more efficient (lifted) inference. To obtain\na lifted representation, the state-of-the-art algorithm Advanced Colour Passing\n(ACP) groups factors that represent matching distributions. In an approximate\nversion using $\\varepsilon$ as a hyperparameter, factors are grouped that\ndiffer by a factor of at most $(1\\pm \\varepsilon)$. However, finding a suitable\n$\\varepsilon$ is not obvious and may need a lot of exploration, possibly\nrequiring many ACP runs with different $\\varepsilon$ values. Additionally,\nvarying $\\varepsilon$ can yield wildly different models, leading to decreased\ninterpretability. Therefore, this paper presents a hierarchical approach to\nlifted model construction that is hyperparameter-free. It efficiently computes\na hierarchy of $\\varepsilon$ values that ensures a hierarchy of models, meaning\nthat once factors are grouped together given some $\\varepsilon$, these factors\nwill be grouped together for larger $\\varepsilon$ as well. The hierarchy of\n$\\varepsilon$ values also leads to a hierarchy of error bounds. This allows for\nexplicitly weighing compression versus accuracy when choosing specific\n$\\varepsilon$ values to run ACP with and enables interpretability between the\ndifferent models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c42\u6b21\u5316\u7684\u3001\u65e0\u9700\u8d85\u53c2\u6570\u7684\u65b9\u6cd5\u6765\u6784\u5efa\u6982\u7387\u56fe\u6a21\u578b\uff0c\u901a\u8fc7\u8ba1\u7b97\u03b5\u503c\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u786e\u4fdd\u6a21\u578b\u7684\u5c42\u6b21\u6027\uff0c\u4ece\u800c\u5728\u538b\u7f29\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u6743\u8861\uff0c\u5e76\u63d0\u5347\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709ACP\u7b97\u6cd5\u4f9d\u8d56\u8d85\u53c2\u6570\u03b5\uff0c\u9700\u591a\u6b21\u5c1d\u8bd5\u4e0d\u540c\u03b5\u503c\uff0c\u4e14\u6a21\u578b\u7ed3\u679c\u5dee\u5f02\u5927\u3001\u53ef\u89e3\u91ca\u6027\u4f4e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u8d85\u53c2\u6570\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u5c42\u6b21\u5316\u65b9\u6cd5\uff0c\u8ba1\u7b97\u03b5\u503c\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u786e\u4fdd\u6a21\u578b\u5c42\u6b21\u6027\uff08\u5c0f\u03b5\u7684\u7ec4\u5728\u5927\u03b5\u65f6\u4fdd\u6301\uff09\uff0c\u5e76\u751f\u6210\u8bef\u5dee\u754c\u9650\u5c42\u6b21\uff0c\u5b9e\u73b0\u538b\u7f29\u4e0e\u51c6\u786e\u6027\u7684\u6743\u8861\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u751f\u6210\u6a21\u578b\u5c42\u6b21\uff0c\u907f\u514d\u591a\u6b21\u8fd0\u884cACP\uff0c\u540c\u65f6\u901a\u8fc7\u8bef\u5dee\u754c\u9650\u5c42\u6b21\u652f\u6301\u663e\u5f0f\u7684\u538b\u7f29-\u51c6\u786e\u6027\u6743\u8861\u3002", "conclusion": "\u5c42\u6b21\u5316\u65b9\u6cd5\u89e3\u51b3\u4e86\u8d85\u53c2\u6570\u4f9d\u8d56\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6784\u5efa\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u8fd1\u4f3c\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "keywords": "\u6982\u7387\u56fe\u6a21\u578b\u3001\u63d0\u5347\u63a8\u7406\u3001ACP\u7b97\u6cd5\u3001\u5c42\u6b21\u5316\u65b9\u6cd5\u3001\u8d85\u53c2\u6570\u4f18\u5316"}}
{"id": "2505.21936", "pdf": "https://arxiv.org/pdf/2505.21936", "abs": "https://arxiv.org/abs/2505.21936", "authors": ["Zeyi Liao", "Jaylen Jones", "Linxi Jiang", "Eric Fosler-Lussier", "Yu Su", "Zhiqiang Lin", "Huan Sun"], "title": "RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments", "categories": ["cs.CL"], "comment": null, "summary": "Computer-use agents (CUAs) promise to automate complex tasks across operating\nsystems (OS) and the web, but remain vulnerable to indirect prompt injection.\nCurrent evaluations of this threat either lack support realistic but controlled\nenvironments or ignore hybrid web-OS attack scenarios involving both\ninterfaces. To address this, we propose RedTeamCUA, an adversarial testing\nframework featuring a novel hybrid sandbox that integrates a VM-based OS\nenvironment with Docker-based web platforms. Our sandbox supports key features\ntailored for red teaming, such as flexible adversarial scenario configuration,\nand a setting that decouples adversarial evaluation from navigational\nlimitations of CUAs by initializing tests directly at the point of an\nadversarial injection. Using RedTeamCUA, we develop RTC-Bench, a comprehensive\nbenchmark with 864 examples that investigate realistic, hybrid web-OS attack\nscenarios and fundamental security vulnerabilities. Benchmarking current\nfrontier CUAs identifies significant vulnerabilities: Claude 3.7 Sonnet | CUA\ndemonstrates an ASR of 42.9%, while Operator, the most secure CUA evaluated,\nstill exhibits an ASR of 7.6%. Notably, CUAs often attempt to execute\nadversarial tasks with an Attempt Rate as high as 92.5%, although failing to\ncomplete them due to capability limitations. Nevertheless, we observe\nconcerning ASRs of up to 50% in realistic end-to-end settings, with the\nrecently released frontier Claude 4 Opus | CUA showing an alarming ASR of 48%,\ndemonstrating that indirect prompt injection presents tangible risks for even\nadvanced CUAs despite their capabilities and safeguards. Overall, RedTeamCUA\nprovides an essential framework for advancing realistic, controlled, and\nsystematic analysis of CUA vulnerabilities, highlighting the urgent need for\nrobust defenses to indirect prompt injection prior to real-world deployment.", "AI": {"tldr": "RedTeamCUA\u662f\u4e00\u4e2a\u5bf9\u6297\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u6c99\u7bb1\u73af\u5883\uff08VM+Docker\uff09\u548cRTC-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u8ba1\u7b97\u673a\u4ee3\u7406\uff08CUA\uff09\u5728\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u4e0b\u7684\u9ad8\u6f0f\u6d1e\u7387\uff08\u6700\u9ad848%\uff09\uff0c\u5f3a\u8c03\u5b9e\u9645\u90e8\u7f72\u524d\u9700\u5f3a\u5316\u9632\u5fa1\u3002", "motivation": "\u73b0\u6709CUA\u5728\u6df7\u5408Web-OS\u653b\u51fb\u573a\u666f\u4e2d\u7f3a\u4e4f\u771f\u5b9e\u53ef\u63a7\u7684\u8bc4\u4f30\u73af\u5883\uff0c\u5bfc\u81f4\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u6f0f\u6d1e\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51faRedTeamCUA\u6846\u67b6\uff0c\u7ed3\u5408VM\u548cDocker\u7684\u6df7\u5408\u6c99\u7bb1\uff0c\u652f\u6301\u7075\u6d3b\u5bf9\u6297\u573a\u666f\u914d\u7f6e\u548cRTC-Bench\u57fa\u51c6\uff08864\u4e2a\u6d4b\u8bd5\u6848\u4f8b\uff09\u3002", "result": "Claude 3.7 Sonnet\u6f0f\u6d1e\u738742.9%\uff0cOperator\u6700\u4f4e7.6%\uff1bClaude 4 Opus\u8fbe48%\uff0c\u653b\u51fb\u5c1d\u8bd5\u7387\u6700\u9ad892.5%\u3002", "conclusion": "\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u5bf9\u5148\u8fdbCUA\u4ecd\u5177\u9ad8\u98ce\u9669\uff0c\u9700\u7cfb\u7edf\u6027\u6f0f\u6d1e\u5206\u6790\u53ca\u9632\u5fa1\u673a\u5236\u3002", "keywords": "\u8ba1\u7b97\u673a\u4ee3\u7406, \u95f4\u63a5\u63d0\u793a\u6ce8\u5165, \u6df7\u5408\u6c99\u7bb1, \u5bf9\u6297\u6d4b\u8bd5, \u6f0f\u6d1e\u57fa\u51c6"}}
{"id": "2505.21684", "pdf": "https://arxiv.org/pdf/2505.21684", "abs": "https://arxiv.org/abs/2505.21684", "authors": ["Joel Lidin", "Amir Sarfi", "Evangelos Pappas", "Samuel Dare", "Eugene Belilovsky", "Jacob Steeves"], "title": "Incentivizing Permissionless Distributed Learning of LLMs", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "We describe an incentive system for distributed deep learning of foundational\nmodels where peers are rewarded for contributions. The incentive system,\n\\textit{Gauntlet}, has been deployed on the bittensor blockchain and used to\ntrain a 1.2B LLM with completely permissionless contributions of\npseudo-gradients: no control over the users that can register or their\nhardware. \\textit{Gauntlet} can be applied to any synchronous distributed\ntraining scheme that relies on aggregating updates or pseudo-gradients. We rely\non a two-stage mechanism for fast filtering of peer uptime, reliability, and\nsynchronization, combined with the core component that estimates the loss\nbefore and after individual pseudo-gradient contributions. We utilized an\nOpenSkill rating system to track competitiveness of pseudo-gradient scores\nacross time. Finally, we introduce a novel mechanism to ensure peers on the\nnetwork perform unique computations. Our live 1.2B run, which has paid out\nreal-valued tokens to participants based on the value of their contributions,\nyielded a competitive (on a per-iteration basis) 1.2B model that demonstrates\nthe utility of our incentive system.", "AI": {"tldr": "\u8bba\u6587\u63cf\u8ff0\u4e86Gauntlet\uff0c\u4e00\u4e2a\u7528\u4e8e\u5206\u5e03\u5f0f\u6df1\u5ea6\u5b66\u4e60\u57fa\u7840\u6a21\u578b\u7684\u6fc0\u52b1\u7cfb\u7edf\uff0c\u5df2\u5728Bittensor\u533a\u5757\u94fe\u4e0a\u90e8\u7f72\uff0c\u5e76\u901a\u8fc7\u65e0\u8bb8\u53ef\u7684\u4f2a\u68af\u5ea6\u8d21\u732e\u8bad\u7ec3\u4e86\u4e00\u4e2a12\u4ebf\u53c2\u6570\u7684LLM\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u5206\u5e03\u5f0f\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5982\u4f55\u516c\u5e73\u5956\u52b1\u8d21\u732e\u8005\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u65e0\u8bb8\u53ef\u73af\u5883\u4e0b\uff0c\u786e\u4fdd\u6a21\u578b\u8bad\u7ec3\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\u3002", "method": "\u7ed3\u5408\u4e86\u4e24\u9636\u6bb5\u673a\u5236\uff08\u5feb\u901f\u8fc7\u6ee4\u8282\u70b9\u7684\u53ef\u7528\u6027\u3001\u53ef\u9760\u6027\u548c\u540c\u6b65\u6027\uff09\u548c\u6838\u5fc3\u7ec4\u4ef6\uff08\u8bc4\u4f30\u4f2a\u68af\u5ea6\u8d21\u732e\u524d\u540e\u7684\u635f\u5931\u53d8\u5316\uff09\uff0c\u5e76\u91c7\u7528OpenSkill\u8bc4\u5206\u7cfb\u7edf\u8ddf\u8e2a\u8d21\u732e\u8d28\u91cf\u3002", "result": "\u6210\u529f\u8bad\u7ec3\u4e86\u4e00\u4e2a12\u4ebf\u53c2\u6570\u7684LLM\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u4ee3\u5e01\u5956\u52b1\u53c2\u4e0e\u8005\uff0c\u8bc1\u660e\u4e86\u8be5\u6fc0\u52b1\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "conclusion": "Gauntlet\u7cfb\u7edf\u5728\u65e0\u8bb8\u53ef\u73af\u5883\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u6fc0\u52b1\u673a\u5236\u786e\u4fdd\u4e86\u8d21\u732e\u7684\u516c\u5e73\u6027\u3002", "keywords": "\u5206\u5e03\u5f0f\u6df1\u5ea6\u5b66\u4e60,\u6fc0\u52b1\u673a\u5236,Bittensor,\u4f2a\u68af\u5ea6,OpenSkill"}}
{"id": "2505.22290", "pdf": "https://arxiv.org/pdf/2505.22290", "abs": "https://arxiv.org/abs/2505.22290", "authors": ["Fanzeng Xia", "Yidong Luo", "Tinko Sebastian Bartels", "Yaqi Xu", "Tongxin Li"], "title": "Rethinking the Unsolvable: When In-Context Search Meets Test-Time Scaling", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent research has highlighted that Large Language Models (LLMs), even when\ntrained to generate extended long reasoning steps, still face significant\nchallenges on hard reasoning problems. However, much of the existing literature\nrelies on direct prompting with simple in-context learning examples for\nevaluation, which largely overlooks advanced techniques to elicit LLMs'\ndeliberate reasoning before drawing conclusions that LLMs hit a performance\nceiling. In this paper, we systematically explore the combined potential of\nin-context search and test-time scaling on super hard reasoning tasks. We find\nthat by employing advanced in-context search prompting to LLMs augmented with\ninternal scaling, one can achieve transformative performance breakthroughs on\ntasks previously deemed \"unsolvable\" (e.g., reported success rates below 5%).\nWe provide both empirical results and theoretical analysis of how this\ncombination can unleash LLM reasoning capabilities: i) Empirically, on\ncontrolled NP-hard tasks and complex real-world planning benchmarks, our\napproach achieves up to a 30x improvement in success rates compared to\npreviously reported results without any external mechanisms; ii) Theoretically,\nwe show that in-context search prompting, when combined with internal scaling,\nsignificantly extends the complexity class of solvable reasoning problems.\nThese findings challenge prevailing assumptions about the limitations of LLMs\non complex tasks, indicating that current evaluation paradigms systematically\nunderestimate their true potential. Our work calls for a critical reassessment\nof how LLM reasoning is benchmarked and a more robust evaluation strategy that\nfully captures the true capabilities of contemporary LLMs, which can lead to a\nbetter understanding of their operational reasoning boundaries in real-world\ndeployments.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u7ed3\u5408\u4e0a\u4e0b\u6587\u641c\u7d22\u63d0\u793a\u548c\u6d4b\u8bd5\u65f6\u6269\u5c55\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6b64\u524d\u8ba4\u4e3a\u2018\u65e0\u89e3\u2019\u7684\u95ee\u9898\uff0c\u5e76\u6311\u6218\u4e86\u73b0\u6709\u5bf9LLMs\u80fd\u529b\u7684\u8bc4\u4f30\u8303\u5f0f\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4f4e\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6f5c\u529b\uff0c\u4e3b\u8981\u4f9d\u8d56\u7b80\u5355\u4e0a\u4e0b\u6587\u5b66\u4e60\u8fdb\u884c\u8bc4\u4f30\uff0c\u5ffd\u7565\u4e86\u5148\u8fdb\u6280\u672f\u7684\u5e94\u7528\u3002", "method": "\u7ed3\u5408\u4e0a\u4e0b\u6587\u641c\u7d22\u63d0\u793a\u548c\u5185\u90e8\u6269\u5c55\u6280\u672f\uff0c\u7cfb\u7edf\u5730\u63a2\u7d22\u4e86LLMs\u5728\u8d85\u96be\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6f5c\u529b\u3002", "result": "\u5728NP-hard\u4efb\u52a1\u548c\u590d\u6742\u771f\u5b9e\u4e16\u754c\u89c4\u5212\u57fa\u51c6\u4e0a\uff0c\u6210\u529f\u7387\u8fbe\u5230\u6b64\u524d\u7ed3\u679c\u768430\u500d\u63d0\u5347\uff0c\u4e14\u7406\u8bba\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u6c42\u89e3\u95ee\u9898\u7684\u590d\u6742\u5ea6\u7c7b\u522b\u3002", "conclusion": "\u7814\u7a76\u6311\u6218\u4e86\u5bf9LLMs\u80fd\u529b\u7684\u73b0\u6709\u5047\u8bbe\uff0c\u547c\u5401\u91cd\u65b0\u8bc4\u4f30\u63a8\u7406\u8bc4\u6d4b\u65b9\u6cd5\u4ee5\u66f4\u5168\u9762\u53cd\u6620\u5176\u771f\u5b9e\u6f5c\u529b\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3001\u63a8\u7406\u80fd\u529b\u3001\u4e0a\u4e0b\u6587\u641c\u7d22\u3001\u6d4b\u8bd5\u65f6\u6269\u5c55\u3001NP-hard\u4efb\u52a1"}}
{"id": "2505.21937", "pdf": "https://arxiv.org/pdf/2505.21937", "abs": "https://arxiv.org/abs/2505.21937", "authors": ["Pratik Rakesh Singh", "Kritarth Prasad", "Mohammadi Zaki", "Pankaj Wasnik"], "title": "Graph-Assisted Culturally Adaptable Idiomatic Translation for Indic Languages", "categories": ["cs.CL"], "comment": null, "summary": "Translating multi-word expressions (MWEs) and idioms requires a deep\nunderstanding of the cultural nuances of both the source and target languages.\nThis challenge is further amplified by the one-to-many nature of idiomatic\ntranslations, where a single source idiom can have multiple target-language\nequivalents depending on cultural references and contextual variations.\nTraditional static knowledge graphs (KGs) and prompt-based approaches struggle\nto capture these complex relationships, often leading to suboptimal\ntranslations. To address this, we propose IdiomCE, an adaptive graph neural\nnetwork (GNN) based methodology that learns intricate mappings between\nidiomatic expressions, effectively generalizing to both seen and unseen nodes\nduring training. Our proposed method enhances translation quality even in\nresource-constrained settings, facilitating improved idiomatic translation in\nsmaller models. We evaluate our approach on multiple idiomatic translation\ndatasets using reference-less metrics, demonstrating significant improvements\nin translating idioms from English to various Indian languages.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u65b9\u6cd5IdiomCE\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u8bcd\u8868\u8fbe\uff08MWEs\uff09\u548c\u4e60\u8bed\u7684\u7ffb\u8bd1\u95ee\u9898\uff0c\u5c24\u5176\u5728\u6587\u5316\u5dee\u5f02\u548c\u4e0a\u4e0b\u6587\u53d8\u5316\u7684\u6311\u6218\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u591a\u8bcd\u8868\u8fbe\u548c\u4e60\u8bed\u7684\u7ffb\u8bd1\u56e0\u6587\u5316\u5dee\u5f02\u548c\u4e0a\u4e0b\u6587\u591a\u53d8\u800c\u590d\u6742\uff0c\u4f20\u7edf\u9759\u6001\u77e5\u8bc6\u56fe\u8c31\u548c\u63d0\u793a\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u8fd9\u4e9b\u590d\u6742\u5173\u7cfb\uff0c\u5bfc\u81f4\u7ffb\u8bd1\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u7684IdiomCE\u65b9\u6cd5\u91c7\u7528\u81ea\u9002\u5e94\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\uff0c\u5b66\u4e60\u4e60\u8bed\u8868\u8fbe\u95f4\u7684\u590d\u6742\u6620\u5c04\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u8bad\u7ec3\u4e2d\u672a\u89c1\u7684\u8282\u70b9\u3002", "result": "\u5728\u591a\u4e2a\u4e60\u8bed\u7ffb\u8bd1\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cIdiomCE\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u4ecd\u80fd\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u82f1\u8bed\u5230\u591a\u79cd\u5370\u5ea6\u8bed\u8a00\u7684\u7ffb\u8bd1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "IdiomCE\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94GNN\u6709\u6548\u89e3\u51b3\u4e86\u4e60\u8bed\u7ffb\u8bd1\u4e2d\u7684\u6587\u5316\u5dee\u5f02\u548c\u4e0a\u4e0b\u6587\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5c0f\u6a21\u578b\u3002", "keywords": "\u591a\u8bcd\u8868\u8fbe, \u4e60\u8bed\u7ffb\u8bd1, \u81ea\u9002\u5e94\u56fe\u795e\u7ecf\u7f51\u7edc, \u77e5\u8bc6\u56fe\u8c31, \u6587\u5316\u5dee\u5f02"}}
{"id": "2505.21695", "pdf": "https://arxiv.org/pdf/2505.21695", "abs": "https://arxiv.org/abs/2505.21695", "authors": ["Ganglou Xu"], "title": "AMSFL: Adaptive Multi-Step Federated Learning via Gradient Difference-Based Error Modeling", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Federated learning faces critical challenges in balancing communication\nefficiency and model accuracy. One key issue lies in the approximation of\nupdate errors without incurring high computational costs. In this paper, we\npropose a lightweight yet effective method called Gradient Difference\nApproximation (GDA), which leverages first-order information to estimate local\nerror trends without computing the full Hessian matrix. The proposed method\nforms a key component of the Adaptive Multi-Step Federated Learning (AMSFL)\nframework and provides a unified error modeling strategy for large-scale\nmulti-step adaptive training environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGradient Difference Approximation (GDA)\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e00\u9636\u4fe1\u606f\u4f30\u8ba1\u672c\u5730\u8bef\u5dee\u8d8b\u52bf\uff0c\u907f\u514d\u8ba1\u7b97\u5b8c\u6574\u7684Hessian\u77e9\u9635\uff0c\u63d0\u5347\u4e86\u8054\u90a6\u5b66\u4e60\u7684\u901a\u4fe1\u6548\u7387\u548c\u6a21\u578b\u51c6\u786e\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u5e73\u8861\u901a\u4fe1\u6548\u7387\u548c\u6a21\u578b\u51c6\u786e\u6027\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5982\u4f55\u5728\u4e0d\u589e\u52a0\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u8fd1\u4f3c\u66f4\u65b0\u8bef\u5dee\u3002", "method": "\u63d0\u51fa\u4e86Gradient Difference Approximation (GDA)\u65b9\u6cd5\uff0c\u5229\u7528\u4e00\u9636\u4fe1\u606f\u4f30\u8ba1\u672c\u5730\u8bef\u5dee\u8d8b\u52bf\uff0c\u65e0\u9700\u8ba1\u7b97\u5b8c\u6574\u7684Hessian\u77e9\u9635\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230Adaptive Multi-Step Federated Learning (AMSFL)\u6846\u67b6\u4e2d\u3002", "result": "GDA\u65b9\u6cd5\u5728AMSFL\u6846\u67b6\u4e2d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5927\u89c4\u6a21\u591a\u6b65\u81ea\u9002\u5e94\u8bad\u7ec3\u73af\u5883\u7684\u8bef\u5dee\u5efa\u6a21\u7b56\u7565\u3002", "conclusion": "GDA\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u8bef\u5dee\u8fd1\u4f3c\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u901a\u4fe1\u6548\u7387\u548c\u6a21\u578b\u51c6\u786e\u6027\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60, \u901a\u4fe1\u6548\u7387, \u6a21\u578b\u51c6\u786e\u6027, Gradient Difference Approximation, AMSFL\u6846\u67b6"}}
{"id": "2505.22311", "pdf": "https://arxiv.org/pdf/2505.22311", "abs": "https://arxiv.org/abs/2505.22311", "authors": ["Feibo Jiang", "Cunhua Pan", "Li Dong", "Kezhi Wang", "Octavia A. Dobre", "Merouane Debbah"], "title": "From Large AI Models to Agentic AI: A Tutorial on Future Intelligent Communications", "categories": ["cs.AI", "cs.CY", "cs.NI", "eess.SP"], "comment": null, "summary": "With the advent of 6G communications, intelligent communication systems face\nmultiple challenges, including constrained perception and response\ncapabilities, limited scalability, and low adaptability in dynamic\nenvironments. This tutorial provides a systematic introduction to the\nprinciples, design, and applications of Large Artificial Intelligence Models\n(LAMs) and Agentic AI technologies in intelligent communication systems, aiming\nto offer researchers a comprehensive overview of cutting-edge technologies and\npractical guidance. First, we outline the background of 6G communications,\nreview the technological evolution from LAMs to Agentic AI, and clarify the\ntutorial's motivation and main contributions. Subsequently, we present a\ncomprehensive review of the key components required for constructing LAMs. We\nfurther categorize LAMs and analyze their applicability, covering Large\nLanguage Models (LLMs), Large Vision Models (LVMs), Large Multimodal Models\n(LMMs), Large Reasoning Models (LRMs), and lightweight LAMs. Next, we propose a\nLAM-centric design paradigm tailored for communications, encompassing dataset\nconstruction and both internal and external learning approaches. Building upon\nthis, we develop an LAM-based Agentic AI system for intelligent communications,\nclarifying its core components such as planners, knowledge bases, tools, and\nmemory modules, as well as its interaction mechanisms. We also introduce a\nmulti-agent framework with data retrieval, collaborative planning, and\nreflective evaluation for 6G. Subsequently, we provide a detailed overview of\nthe applications of LAMs and Agentic AI in communication scenarios. Finally, we\nsummarize the research challenges and future directions in current studies,\naiming to support the development of efficient, secure, and sustainable\nnext-generation intelligent communication systems.", "AI": {"tldr": "\u8fd9\u7bc7\u6559\u7a0b\u7cfb\u7edf\u6027\u4ecb\u7ecd\u4e86\u5927\u578b\u4eba\u5de5\u667a\u80fd\u6a21\u578b\uff08LAMs\uff09\u548cAgentic AI\u6280\u672f\u57286G\u667a\u80fd\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u8bbe\u8ba1\u539f\u7406\u4e0e\u5e94\u7528\uff0c\u65e8\u5728\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u524d\u6cbf\u6280\u672f\u7efc\u8ff0\u548c\u5b9e\u8df5\u6307\u5bfc\u3002", "motivation": "6G\u901a\u4fe1\u65f6\u4ee3\u4e0b\uff0c\u667a\u80fd\u901a\u4fe1\u7cfb\u7edf\u9762\u4e34\u611f\u77e5\u54cd\u5e94\u80fd\u529b\u53d7\u9650\u3001\u6269\u5c55\u6027\u4e0d\u8db3\u548c\u52a8\u6001\u73af\u5883\u9002\u5e94\u6027\u4f4e\u7b49\u6311\u6218\uff0c\u9700\u901a\u8fc7LAMs\u548cAgentic AI\u6280\u672f\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u4eceLAMs\u6280\u672f\u6f14\u8fdb\u5206\u6790\u5165\u624b\uff0c\u63d0\u51fa\u901a\u4fe1\u4e13\u7528\u7684LAM\u8bbe\u8ba1\u8303\u5f0f\uff08\u542b\u6570\u636e\u96c6\u6784\u5efa\u53ca\u5b66\u4e60\u7b56\u7565\uff09\uff0c\u5e76\u6784\u5efa\u57fa\u4e8eLAM\u7684Agentic AI\u7cfb\u7edf\u6846\u67b6\uff08\u542b\u6838\u5fc3\u7ec4\u4ef6\u4e0e\u4ea4\u4e92\u673a\u5236\uff09\u548c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u3002", "result": "\u68b3\u7406\u4e86LAMs\uff08\u5982LLM\u3001LVM\u7b49\uff09\u7684\u9002\u7528\u573a\u666f\uff0c\u5c55\u793a\u4e86LAM\u4e0eAgentic AI\u5728\u901a\u4fe1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6848\u4f8b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u63d0\u5347\u7cfb\u7edf\u6548\u80fd\u3001\u5b89\u5168\u6027\u548c\u53ef\u6301\u7eed\u6027\u7684\u6f5c\u529b\u3002", "conclusion": "\u603b\u7ed3\u4e86\u5f53\u524d\u7814\u7a76\u7684\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u9ad8\u6548\u3001\u5b89\u5168\u3001\u53ef\u6301\u7eed\u7684\u667a\u80fd\u901a\u4fe1\u7cfb\u7edf\u53d1\u5c55\u63d0\u4f9b\u652f\u6301\u3002", "keywords": "6G\u901a\u4fe1, \u5927\u578b\u4eba\u5de5\u667a\u80fd\u6a21\u578b\uff08LAMs\uff09, Agentic AI, \u591a\u667a\u80fd\u4f53\u7cfb\u7edf, \u667a\u80fd\u901a\u4fe1"}}
{"id": "2505.21940", "pdf": "https://arxiv.org/pdf/2505.21940", "abs": "https://arxiv.org/abs/2505.21940", "authors": ["Bolei He", "Xinran He", "Mengke Chen", "Xianwei Xue", "Ying Zhu", "Zhenhua Ling"], "title": "RISE: Reasoning Enhancement via Iterative Self-Exploration in Multi-hop Question Answering", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Large Language Models (LLMs) excel in many areas but continue to face\nchallenges with complex reasoning tasks, such as Multi-Hop Question Answering\n(MHQA). MHQA requires integrating evidence from diverse sources while managing\nintricate logical dependencies, often leads to errors in reasoning.\nRetrieval-Augmented Generation (RAG), widely employed in MHQA tasks, faces\nchallenges in effectively filtering noisy data and retrieving all necessary\nevidence, thereby limiting its effectiveness in addressing MHQA challenges. To\naddress these challenges, we propose RISE:Reasoning Enhancement via Iterative\nSelf-Exploration, a novel framework designed to enhance models' reasoning\ncapability through iterative self-exploration. Specifically, RISE involves\nthree key steps in addressing MHQA tasks: question decomposition,\nretrieve-then-read, and self-critique. By leveraging continuous\nself-exploration, RISE identifies accurate reasoning paths, iteratively\nself-improving the model's capability to integrate evidence, maintain logical\nconsistency, and enhance performance in MHQA tasks. Extensive experiments on\nmultiple MHQA benchmarks demonstrate that RISE significantly improves reasoning\naccuracy and task performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRISE\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u81ea\u63a2\u7d22\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff08\u5982\u591a\u8df3\u95ee\u7b54MHQA\uff09\u4e2d\u7684\u8868\u73b0\u3002RISE\u5305\u542b\u95ee\u9898\u5206\u89e3\u3001\u68c0\u7d22-\u9605\u8bfb\u548c\u81ea\u6211\u6279\u5224\u4e09\u4e2a\u5173\u952e\u6b65\u9aa4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u63a8\u7406\u51c6\u786e\u6027\u548c\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bb8\u591a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff08\u5982\u591a\u8df3\u95ee\u7b54MHQA\uff09\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u6574\u5408\u591a\u6837\u8bc1\u636e\u548c\u7ba1\u7406\u903b\u8f91\u4f9d\u8d56\u65f6\u6613\u51fa\u9519\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u68c0\u7d22\u589e\u5f3a\u751f\u6210RAG\uff09\u5728\u8fc7\u6ee4\u566a\u58f0\u6570\u636e\u548c\u68c0\u7d22\u5fc5\u8981\u8bc1\u636e\u65b9\u9762\u6548\u679c\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "method": "RISE\u6846\u67b6\u901a\u8fc7\u8fed\u4ee3\u81ea\u63a2\u7d22\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u5305\u62ec\u4e09\u4e2a\u5173\u952e\u6b65\u9aa4\uff1a1)\u95ee\u9898\u5206\u89e3\uff0c\u5c06\u590d\u6742\u95ee\u9898\u62c6\u89e3\u4e3a\u5b50\u95ee\u9898\uff1b2)\u68c0\u7d22-\u9605\u8bfb\uff0c\u83b7\u53d6\u5e76\u6574\u5408\u76f8\u5173\u8bc1\u636e\uff1b3)\u81ea\u6211\u6279\u5224\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u63a8\u7406\u8def\u5f84\u548c\u903b\u8f91\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2aMHQA\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRISE\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u51c6\u786e\u6027\u548c\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "RISE\u901a\u8fc7\u8fed\u4ee3\u81ea\u63a2\u7d22\u6709\u6548\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u63a8\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u591a\u8df3\u95ee\u7b54,\u68c0\u7d22\u589e\u5f3a\u751f\u6210,\u63a8\u7406\u589e\u5f3a,\u8fed\u4ee3\u81ea\u63a2\u7d22"}}
{"id": "2505.21717", "pdf": "https://arxiv.org/pdf/2505.21717", "abs": "https://arxiv.org/abs/2505.21717", "authors": ["M\u00f3nika Farsang", "Ramin Hasani", "Radu Grosu"], "title": "Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient Sequence Modeling", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "We present LrcSSM, a \\textit{nonlinear} recurrent model that processes long\nsequences as fast as today's linear state-space layers. By forcing the\nstate-transition matrix to be diagonal and learned at every step, the full\nsequence can be solved in parallel with a single prefix-scan, giving\n$\\mathcal{O}(TD)$ time and memory and only $\\mathcal{O}(\\log T)$ sequential\ndepth, for input-sequence length $T$ and a state dimension $D$. Moreover,\nLrcSSM offers a formal gradient-stability guarantee that other input-varying\nsystems such as Liquid-S4 and Mamba do not provide. Lastly, for network depth\n$L$, as the forward and backward passes cost $\\Theta(T\\,D\\,L)$ FLOPs, with its\nlow sequential depth and parameter count $\\Theta(D\\,L)$, the model follows the\ncompute-optimal scaling law regime ($\\beta \\approx 0.42$) recently observed for\nMamba, outperforming quadratic-attention Transformers at equal compute while\navoiding the memory overhead of FFT-based long convolutions. We show that on a\nseries of long-range forecasting tasks, LrcSSM outperforms LRU, S5 and Mamba.", "AI": {"tldr": "LrcSSM\u662f\u4e00\u79cd\u975e\u7ebf\u6027\u5faa\u73af\u6a21\u578b\uff0c\u80fd\u4ee5\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u5c42\u7684\u901f\u5ea6\u5904\u7406\u957f\u5e8f\u5217\u3002\u901a\u8fc7\u5f3a\u5236\u72b6\u6001\u8f6c\u79fb\u77e9\u9635\u5bf9\u89d2\u5316\u5e76\u52a8\u6001\u5b66\u4e60\uff0c\u8be5\u6a21\u578b\u80fd\u4ee5\u5e76\u884c\u65b9\u5f0f\u9ad8\u6548\u5904\u7406\u5e8f\u5217\uff0c\u8fd0\u7b97\u590d\u6742\u5ea6\u4e3aO(TD)\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u957f\u5e8f\u5217\u5904\u7406\u4e0a\u7684\u6548\u7387\u95ee\u9898\uff0c\u5c24\u5176\u662f\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u5c42\u7684\u5c40\u9650\u6027\u548c\u5176\u4ed6\u6a21\u578b\u7684\u68af\u5ea6\u7a33\u5b9a\u6027\u4e0d\u8db3\uff0cLrcSSM\u5e94\u8fd0\u800c\u751f\u3002", "method": "\u91c7\u7528\u5bf9\u89d2\u5316\u72b6\u6001\u8f6c\u79fb\u77e9\u9635\uff0c\u52a8\u6001\u5b66\u4e60\u6bcf\u4e00\u6b65\u7684\u53c2\u6570\uff0c\u5229\u7528\u5e76\u884c\u524d\u7f00\u626b\u63cf\u6280\u672f\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002", "result": "\u5728\u957f\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cLrcSSM\u7684\u8868\u73b0\u4f18\u4e8eLRU\u3001S5\u548cMamba\uff0c\u8ba1\u7b97\u6548\u7387\u548c\u68af\u5ea6\u7a33\u5b9a\u6027\u5747\u6709\u63d0\u5347\u3002", "conclusion": "LrcSSM\u5728\u957f\u5e8f\u5217\u5904\u7406\u4e0a\u5177\u6709\u9ad8\u6548\u6027\u548c\u7a33\u5b9a\u6027\u4f18\u52bf\uff0c\u9002\u5408\u5927\u89c4\u6a21\u8ba1\u7b97\u4efb\u52a1\u3002", "keywords": "LrcSSM, \u975e\u7ebf\u6027\u5faa\u73af\u6a21\u578b, \u72b6\u6001\u7a7a\u95f4, \u957f\u5e8f\u5217\u5904\u7406, \u5e76\u884c\u8ba1\u7b97"}}
{"id": "2505.22368", "pdf": "https://arxiv.org/pdf/2505.22368", "abs": "https://arxiv.org/abs/2505.22368", "authors": ["Enfang Cui", "Yujun Cheng", "Rui She", "Dan Liu", "Zhiyuan Liang", "Minxin Guo", "Tianzheng Li", "Qian Wei", "Wenjuan Xing", "Zhijie Zhong"], "title": "AgentDNS: A Root Domain Naming System for LLM Agents", "categories": ["cs.AI"], "comment": "7 pages, 6 figures", "summary": "The rapid evolution of Large Language Model (LLM) agents has highlighted\ncritical challenges in cross-vendor service discovery, interoperability, and\ncommunication. Existing protocols like model context protocol and\nagent-to-agent protocol have made significant strides in standardizing\ninteroperability between agents and tools, as well as communication among\nmulti-agents. However, there remains a lack of standardized protocols and\nsolutions for service discovery across different agent and tool vendors. In\nthis paper, we propose AgentDNS, a root domain naming and service discovery\nsystem designed to enable LLM agents to autonomously discover, resolve, and\nsecurely invoke third-party agent and tool services across organizational and\ntechnological boundaries. Inspired by the principles of the traditional DNS,\nAgentDNS introduces a structured mechanism for service registration, semantic\nservice discovery, secure invocation, and unified billing. We detail the\narchitecture, core functionalities, and use cases of AgentDNS, demonstrating\nits potential to streamline multi-agent collaboration in real-world scenarios.\nThe source code will be published on https://github.com/agentdns.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAgentDNS\uff0c\u4e00\u79cd\u4e3aLLM\u4ee3\u7406\u8bbe\u8ba1\u7684\u57df\u540d\u548c\u670d\u52a1\u53d1\u73b0\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e0d\u540c\u4ee3\u7406\u548c\u5de5\u5177\u5382\u5546\u95f4\u7684\u670d\u52a1\u53d1\u73b0\u3001\u4e92\u64cd\u4f5c\u6027\u548c\u901a\u4fe1\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u534f\u8bae\u5728\u4ee3\u7406\u95f4\u4e92\u64cd\u4f5c\u6027\u548c\u591a\u4ee3\u7406\u901a\u4fe1\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u8de8\u5382\u5546\u670d\u52a1\u53d1\u73b0\u7684\u6807\u51c6\u65b9\u6848\u3002", "method": "\u501f\u9274\u4f20\u7edfDNS\u539f\u5219\uff0c\u8bbe\u8ba1AgentDNS\uff0c\u5305\u542b\u670d\u52a1\u6ce8\u518c\u3001\u8bed\u4e49\u53d1\u73b0\u3001\u5b89\u5168\u8c03\u7528\u548c\u7edf\u4e00\u8ba1\u8d39\u673a\u5236\u3002", "result": "\u901a\u8fc7\u67b6\u6784\u548c\u6838\u5fc3\u529f\u80fd\u5c55\u793a\uff0cAgentDNS\u80fd\u6709\u6548\u4fc3\u8fdb\u591a\u4ee3\u7406\u534f\u4f5c\u3002", "conclusion": "AgentDNS\u6709\u671b\u89e3\u51b3\u8de8\u7ec4\u7ec7\u548c\u6280\u672f\u8fb9\u754c\u7684\u670d\u52a1\u53d1\u73b0\u4e0e\u8c03\u7528\u95ee\u9898\u3002", "keywords": "Large Language Model, AgentDNS, \u670d\u52a1\u53d1\u73b0, \u4e92\u64cd\u4f5c\u6027, \u591a\u4ee3\u7406\u534f\u4f5c"}}
{"id": "2505.21941", "pdf": "https://arxiv.org/pdf/2505.21941", "abs": "https://arxiv.org/abs/2505.21941", "authors": ["Ashim Gupta", "Vivek Srikumar"], "title": "Test-Time Scaling with Repeated Sampling Improves Multilingual Text Generation", "categories": ["cs.CL"], "comment": null, "summary": "Inference-time scaling via repeated sampling has shown promise in reasoning\ntasks, but its effectiveness in multilingual generation remains underexplored.\nWe evaluate this approach using perplexity- and reward-based verifiers on two\nmultilingual benchmarks: the Aya Evaluation Suite and m-ArenaHard. Our results\nshow consistent quality improvements, with gains exceeding 35% in some cases.\nWhile perplexity-based scoring is effective for open-ended prompts, only\nreward-based verifiers improve performance on tasks requiring reasoning (e.g.,\nmath, code). Our results demonstrate the broader utility of repeated sampling\nfor multilingual text generation and underscore the importance of selecting\nright verifiers for the task.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u63a8\u7406\u65f6\u901a\u8fc7\u91cd\u590d\u91c7\u6837\u5728\u53cc\u8bed\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8d28\u91cf\u63d0\u5347\u8d85\u8fc735%\uff0c\u5e76\u5f3a\u8c03\u4e86\u9009\u62e9\u6b63\u786e\u9a8c\u8bc1\u5668\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7814\u7a76\u91cd\u590d\u91c7\u6837\u5728\u53cc\u8bed\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u662f\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u56e0\u4e3a\u8fd9\u65b9\u9762\u6b64\u524d\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u56f0\u60d1\u5ea6\u548c\u5956\u52b1\u7684\u9a8c\u8bc1\u5668\u5728\u4e24\u4e2a\u53cc\u8bed\u57fa\u51c6\u6d4b\u8bd5\uff08Aya Evaluation Suite\u548cm-ArenaHard\uff09\u4e0a\u8bc4\u4f30\u91cd\u590d\u91c7\u6837\u7684\u6548\u679c\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u91cd\u590d\u91c7\u6837\u5728\u53cc\u8bed\u6587\u672c\u751f\u6210\u4e2d\u5e26\u6765\u4e00\u81f4\u7684\u8d28\u91cf\u63d0\u5347\uff0c\u67d0\u4e9b\u60c5\u51b5\u4e0b\u63d0\u5347\u8d85\u8fc735%\u3002\u57fa\u4e8e\u56f0\u60d1\u5ea6\u7684\u8bc4\u5206\u5728\u5f00\u653e\u5f0f\u63d0\u793a\u4e2d\u6709\u6548\uff0c\u800c\u57fa\u4e8e\u5956\u52b1\u7684\u9a8c\u8bc1\u5668\u5728\u9700\u8981\u63a8\u7406\u7684\u4efb\u52a1\uff08\u5982\u6570\u5b66\u3001\u4ee3\u7801\uff09\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u91cd\u590d\u91c7\u6837\u5728\u53cc\u8bed\u6587\u672c\u751f\u6210\u4e2d\u5177\u6709\u5e7f\u6cdb\u5b9e\u7528\u6027\uff0c\u5e76\u5f3a\u8c03\u6839\u636e\u4efb\u52a1\u9009\u62e9\u5408\u9002\u9a8c\u8bc1\u5668\u7684\u91cd\u8981\u6027\u3002", "keywords": "\u91cd\u590d\u91c7\u6837,\u53cc\u8bed\u751f\u6210,\u63a8\u7406\u4efb\u52a1,\u9a8c\u8bc1\u5668,\u56f0\u60d1\u5ea6,\u5956\u52b1"}}
{"id": "2505.21722", "pdf": "https://arxiv.org/pdf/2505.21722", "abs": "https://arxiv.org/abs/2505.21722", "authors": ["Ioannis Bantzis", "James B. Simon", "Arthur Jacot"], "title": "Saddle-To-Saddle Dynamics in Deep ReLU Networks: Low-Rank Bias in the First Saddle Escape", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "When a deep ReLU network is initialized with small weights, GD is at first\ndominated by the saddle at the origin in parameter space. We study the\nso-called escape directions, which play a similar role as the eigenvectors of\nthe Hessian for strict saddles. We show that the optimal escape direction\nfeatures a low-rank bias in its deeper layers: the first singular value of the\n$\\ell$-th layer weight matrix is at least $\\ell^{\\frac{1}{4}}$ larger than any\nother singular value. We also prove a number of related results about these\nescape directions. We argue that this result is a first step in proving\nSaddle-to-Saddle dynamics in deep ReLU networks, where GD visits a sequence of\nsaddles with increasing bottleneck rank.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6df1\u5ea6ReLU\u7f51\u7edc\u5728\u5c0f\u6743\u91cd\u521d\u59cb\u5316\u4e0b\uff0c\u68af\u5ea6\u4e0b\u964d\uff08GD\uff09\u5728\u53c2\u6570\u7a7a\u95f4\u4e2d\u53d7\u539f\u70b9\u978d\u70b9\u4e3b\u5bfc\u7684\u73b0\u8c61\uff0c\u5206\u6790\u4e86\u9003\u9038\u65b9\u5411\u7684\u4f5c\u7528\u53ca\u5176\u4f4e\u79e9\u504f\u7f6e\u7279\u6027\u3002", "motivation": "\u63a2\u8ba8\u6df1\u5ea6ReLU\u7f51\u7edc\u4e2d\u68af\u5ea6\u4e0b\u964d\u7684\u521d\u59cb\u884c\u4e3a\uff0c\u5c24\u5176\u662f\u9003\u9038\u65b9\u5411\u7684\u6027\u8d28\u53ca\u5176\u5bf9\u4f18\u5316\u8fc7\u7a0b\u7684\u5f71\u54cd\uff0c\u4e3a\u7406\u89e3\u4f18\u5316\u52a8\u6001\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u7814\u7a76\u9003\u9038\u65b9\u5411\u7684\u6027\u8d28\uff0c\u8bc1\u660e\u66f4\u6df1\u5c42\u6743\u91cd\u77e9\u9635\u5728\u9003\u9038\u65b9\u5411\u4e0a\u7684\u6700\u4f18\u8def\u5f84\u5177\u6709\u4f4e\u79e9\u504f\u7f6e\uff0c\u5373\u7b2c\u4e00\u5947\u5f02\u503c\u663e\u8457\u5927\u4e8e\u5176\u4ed6\u5947\u5f02\u503c\u3002", "result": "\u53d1\u73b0\u9003\u9038\u65b9\u5411\u5728\u66f4\u6df1\u5c42\u8868\u73b0\u51fa\u4f4e\u79e9\u504f\u7f6e\uff0c\u7b2c\u4e00\u5947\u5f02\u503c\u81f3\u5c11\u662f\u5176\u4ed6\u5947\u5f02\u503c\u7684\u2113^(1/4)\u500d\u3002", "conclusion": "\u7ed3\u679c\u4e3a\u8bc1\u660e\u6df1\u5ea6ReLU\u7f51\u7edc\u4e2d\u7684\u201c\u978d\u70b9\u5230\u978d\u70b9\u201d\u52a8\u6001\u63d0\u4f9b\u4e86\u521d\u6b65\u652f\u6301\uff0c\u5373\u68af\u5ea6\u4e0b\u964d\u4f1a\u9010\u6b65\u8bbf\u95ee\u74f6\u9888\u79e9\u589e\u52a0\u7684\u978d\u70b9\u5e8f\u5217\u3002", "keywords": "ReLU\u7f51\u7edc, \u68af\u5ea6\u4e0b\u964d, \u978d\u70b9, \u9003\u9038\u65b9\u5411, \u4f4e\u79e9\u504f\u7f6e"}}
{"id": "2505.22451", "pdf": "https://arxiv.org/pdf/2505.22451", "abs": "https://arxiv.org/abs/2505.22451", "authors": ["Yuanhang Liu", "Yanxing Huang", "Yanqiao Wang", "Peng Li", "Yang Liu"], "title": "AI Mathematician: Towards Fully Automated Frontier Mathematical Research", "categories": ["cs.AI"], "comment": "95 pages, 1 figure", "summary": "Large Reasoning Models (LRMs) have made significant progress in mathematical\ncapabilities in recent times. However, these successes have been primarily\nconfined to competition-level problems. In this work, we propose AI\nMathematician (AIM) framework, which harnesses the reasoning strength of LRMs\nto support frontier mathematical research. We have identified two critical\nchallenges of mathematical research compared to competition, {\\it the intrinsic\ncomplexity of research problems} and {\\it the requirement of procedural rigor}.\nTo address these challenges, AIM incorporates two core strategies: an\nexploration mechanism to foster longer solution paths, and the pessimistic\nreasonable verification method to ensure reliability.\n  This early version of AIM already exhibits strong capability in tackling\nresearch-level tasks. We conducted extensive experiments across several\nreal-world mathematical topics and obtained promising results. AIM is able to\nautonomously construct substantial portions of proofs and uncover non-trivial\ninsights within each research area. These findings highlight the potential of\nLRMs in mathematical discovery and suggest that LRM-based agent systems could\nsignificantly accelerate mathematical research in the future.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86AI Mathematician (AIM)\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u63a8\u7406\u6a21\u578b(LRMs)\u652f\u6301\u524d\u6cbf\u6570\u5b66\u7814\u7a76\uff0c\u901a\u8fc7\u63a2\u7d22\u673a\u5236\u548c\u60b2\u89c2\u5408\u7406\u9a8c\u8bc1\u65b9\u6cd5\u89e3\u51b3\u7814\u7a76\u95ee\u9898\u7684\u590d\u6742\u6027\u548c\u7a0b\u5e8f\u4e25\u8c28\u6027\u6311\u6218\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u83b7\u5f97\u663e\u8457\u6210\u679c\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u63a8\u7406\u6a21\u578b(LRMs)\u5728\u6570\u5b66\u7ade\u8d5b\u9898\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u524d\u6cbf\u6570\u5b66\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u4ecd\u9762\u4e34\u7814\u7a76\u95ee\u9898\u590d\u6742\u6027\u548c\u7a0b\u5e8f\u4e25\u8c28\u6027\u7684\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u6846\u67b6\u6765\u63d0\u5347\u5176\u7814\u7a76\u80fd\u529b\u3002", "method": "\u63d0\u51faAIM\u6846\u67b6\uff0c\u91c7\u7528\u63a2\u7d22\u673a\u5236\u4fc3\u8fdb\u66f4\u957f\u7684\u89e3\u51b3\u8def\u5f84\uff0c\u5e76\u7ed3\u5408\u60b2\u89c2\u5408\u7406\u9a8c\u8bc1\u65b9\u6cd5\u6765\u786e\u4fdd\u53ef\u9760\u6027\u3002", "result": "\u5728\u591a\u4e2a\u5b9e\u9645\u6570\u5b66\u8bfe\u9898\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAIM\u80fd\u81ea\u4e3b\u6784\u5efa\u5927\u91cf\u8bc1\u660e\u7247\u6bb5\u5e76\u53d1\u73b0\u975e\u5e73\u51e1\u89c1\u89e3\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u6570\u5b66\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "AIM\u5c55\u793a\u4e86LRMs\u5728\u6570\u5b66\u53d1\u73b0\u4e2d\u7684\u6f5c\u529b\uff0c\u57fa\u4e8eLRM\u7684\u667a\u80fd\u7cfb\u7edf\u53ef\u80fd\u663e\u8457\u52a0\u901f\u672a\u6765\u7684\u6570\u5b66\u7814\u7a76\u3002", "keywords": "AI Mathematician, Large Reasoning Models, mathematical research, exploration mechanism, verification method"}}
{"id": "2505.21958", "pdf": "https://arxiv.org/pdf/2505.21958", "abs": "https://arxiv.org/abs/2505.21958", "authors": ["Qihuang Zhong", "Liang Ding", "Fei Liao", "Juhua Liu", "Bo Du", "Dacheng Tao"], "title": "Resolving Knowledge Conflicts in Domain-specific Data Selection: A Case Study on Medical Instruction-tuning", "categories": ["cs.CL"], "comment": null, "summary": "Domain-specific instruction-tuning has become the defacto standard for\nimproving the performance of large language models (LLMs) in specialized\napplications, e.g., medical question answering. Since the instruction-tuning\ndataset might contain redundant or low-quality data, data selection (DS) is\nusually required to maximize the data efficiency. Despite the successes in the\ngeneral domain, current DS methods often struggle to select the desired data\nfor domain-specific instruction-tuning. One of the main reasons is that they\nneglect the impact of knowledge conflicts, i.e., the discrepancy between LLMs'\npretrained knowledge and context knowledge of instruction data, which could\ndamage LLMs' prior abilities and lead to hallucination. To this end, we propose\na simple-yet-effective Knowledge-aware Data Selection (namely KDS) framework to\nselect the domain-specific instruction-tuning data that meets LLMs' actual\nneeds. The core of KDS is to leverage two knowledge-aware metrics for\nquantitatively measuring knowledge conflicts from two aspects: context-memory\nknowledge alignment and intra-memory knowledge consistency. By filtering the\ndata with large knowledge conflicts and sampling the high-quality and diverse\ndata, KDS can effectively stimulate the LLMs' abilities and achieve better\ndomain-specific performance. Taking the medical domain as the testbed, we\nconduct extensive experiments and empirically prove that KDS surpasses the\nother baselines and brings significant and consistent performance gains among\nall LLMs. More encouragingly, KDS effectively improves the model generalization\nand alleviates the hallucination problem.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKDS\u7684\u77e5\u8bc6\u611f\u77e5\u6570\u636e\u9009\u62e9\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u9886\u57df\u7279\u5b9a\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\uff0c\u4ee5\u51cf\u5c11\u77e5\u8bc6\u51b2\u7a81\u5e76\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u9886\u57df\u7279\u5b9a\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u4e2d\u53ef\u80fd\u5b58\u5728\u5197\u4f59\u6216\u4f4e\u8d28\u91cf\u6570\u636e\uff0c\u4e14\u73b0\u6709\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u5ffd\u7565\u4e86\u77e5\u8bc6\u51b2\u7a81\u7684\u5f71\u54cd\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u80fd\u529b\u4e0b\u964d\u548c\u5e7b\u89c9\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u611f\u77e5\u6307\u6807\uff08\u4e0a\u4e0b\u6587-\u8bb0\u5fc6\u77e5\u8bc6\u5bf9\u9f50\u548c\u8bb0\u5fc6\u5185\u77e5\u8bc6\u4e00\u81f4\u6027\uff09\u5b9a\u91cf\u8861\u91cf\u77e5\u8bc6\u51b2\u7a81\uff0c\u901a\u8fc7\u8fc7\u6ee4\u9ad8\u51b2\u7a81\u6570\u636e\u5e76\u9009\u62e9\u9ad8\u8d28\u91cf\u548c\u591a\u6837\u5316\u7684\u6570\u636e\uff0c\u4f18\u5316\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\u3002", "result": "\u5728\u533b\u5b66\u9886\u57df\u5b9e\u9a8c\u4e2d\uff0cKDS\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3001\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u7f13\u89e3\u4e86\u5e7b\u89c9\u95ee\u9898\u3002", "conclusion": "KDS\u901a\u8fc7\u77e5\u8bc6\u611f\u77e5\u7684\u6570\u636e\u9009\u62e9\u6709\u6548\u63d0\u5347\u4e86\u9886\u57df\u7279\u5b9a\u6307\u4ee4\u8c03\u4f18\u7684\u6548\u679c\uff0c\u5c55\u793a\u4e86\u5176\u5728\u89e3\u51b3\u77e5\u8bc6\u51b2\u7a81\u548c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u65b9\u9762\u7684\u6f5c\u529b\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u6307\u4ee4\u8c03\u4f18\u3001\u77e5\u8bc6\u51b2\u7a81\u3001\u6570\u636e\u9009\u62e9\u3001\u77e5\u8bc6\u611f\u77e5\u3001\u533b\u5b66\u9886\u57df"}}
{"id": "2505.21731", "pdf": "https://arxiv.org/pdf/2505.21731", "abs": "https://arxiv.org/abs/2505.21731", "authors": ["Quentin Delfosse", "Jannis Bl\u00fcml", "Fabian Tatai", "Th\u00e9o Vincent", "Bjarne Gregori", "Elisabeth Dillies", "Jan Peters", "Constantin Rothkopf", "Kristian Kersting"], "title": "Deep Reinforcement Learning Agents are not even close to Human Intelligence", "categories": ["cs.LG", "cs.AI"], "comment": "49 pages in total, 5 main figures, 14 figures total", "summary": "Deep reinforcement learning (RL) agents achieve impressive results in a wide\nvariety of tasks, but they lack zero-shot adaptation capabilities. While most\nrobustness evaluations focus on tasks complexifications, for which human also\nstruggle to maintain performances, no evaluation has been performed on tasks\nsimplifications. To tackle this issue, we introduce HackAtari, a set of task\nvariations of the Arcade Learning Environments. We use it to demonstrate that,\ncontrary to humans, RL agents systematically exhibit huge performance drops on\nsimpler versions of their training tasks, uncovering agents' consistent\nreliance on shortcuts. Our analysis across multiple algorithms and\narchitectures highlights the persistent gap between RL agents and human\nbehavioral intelligence, underscoring the need for new benchmarks and\nmethodologies that enforce systematic generalization testing beyond static\nevaluation protocols. Training and testing in the same environment is not\nenough to obtain agents equipped with human-like intelligence.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4ee3\u7406\u5728\u7b80\u5316\u4efb\u52a1\u7248\u672c\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u63ed\u793a\u4e86\u5176\u5bf9\u6377\u5f84\u7684\u4f9d\u8d56\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u65b0\u57fa\u51c6\u548c\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7f3a\u4e4f\u96f6\u6837\u672c\u9002\u5e94\u80fd\u529b\uff0c\u4e14\u73b0\u6709\u8bc4\u4f30\u591a\u96c6\u4e2d\u4e8e\u4efb\u52a1\u590d\u6742\u5316\uff0c\u5ffd\u7565\u4e86\u5bf9\u4efb\u52a1\u7b80\u5316\u7684\u7814\u7a76\u3002", "method": "\u5f15\u5165HackAtari\uff0c\u4e00\u5957Arcade\u5b66\u4e60\u73af\u5883\u7684\u4efb\u52a1\u53d8\u4f53\uff0c\u8bc4\u4f30\u591a\u79cdRL\u7b97\u6cd5\u548c\u67b6\u6784\u5728\u7b80\u5316\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "RL\u4ee3\u7406\u5728\u7b80\u5316\u4efb\u52a1\u4e0a\u8868\u73b0\u5927\u5e45\u4e0b\u964d\uff0c\u663e\u793a\u51fa\u5bf9\u6377\u5f84\u7684\u4f9d\u8d56\uff0c\u4e0e\u4eba\u7c7b\u884c\u4e3a\u667a\u80fd\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "\u9700\u5f00\u53d1\u65b0\u57fa\u51c6\u548c\u65b9\u6cd5\uff0c\u4ee5\u8d85\u8d8a\u9759\u6001\u8bc4\u4f30\u534f\u8bae\uff0c\u63d0\u5347RL\u4ee3\u7406\u7684\u7cfb\u7edf\u6027\u6cdb\u5316\u80fd\u529b\u3002", "keywords": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60,\u96f6\u6837\u672c\u9002\u5e94,HackAtari,\u6377\u5f84\u4f9d\u8d56,\u7cfb\u7edf\u6027\u6cdb\u5316"}}
{"id": "2505.22597", "pdf": "https://arxiv.org/pdf/2505.22597", "abs": "https://arxiv.org/abs/2505.22597", "authors": ["Ngoc La", "Ruaridh Mon-Williams", "Julie A. Shah"], "title": "HDDLGym: A Tool for Studying Multi-Agent Hierarchical Problems Defined in HDDL with OpenAI Gym", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": "Accepted to Proceedings of ICAPS 2025", "summary": "In recent years, reinforcement learning (RL) methods have been widely tested\nusing tools like OpenAI Gym, though many tasks in these environments could also\nbenefit from hierarchical planning. However, there is a lack of a tool that\nenables seamless integration of hierarchical planning with RL. Hierarchical\nDomain Definition Language (HDDL), used in classical planning, introduces a\nstructured approach well-suited for model-based RL to address this gap. To\nbridge this integration, we introduce HDDLGym, a Python-based tool that\nautomatically generates OpenAI Gym environments from HDDL domains and problems.\nHDDLGym serves as a link between RL and hierarchical planning, supporting\nmulti-agent scenarios and enabling collaborative planning among agents. This\npaper provides an overview of HDDLGym's design and implementation, highlighting\nthe challenges and design choices involved in integrating HDDL with the Gym\ninterface, and applying RL policies to support hierarchical planning. We also\nprovide detailed instructions and demonstrations for using the HDDLGym\nframework, including how to work with existing HDDL domains and problems from\nInternational Planning Competitions, exemplified by the Transport domain.\nAdditionally, we offer guidance on creating new HDDL domains for multi-agent\nscenarios and demonstrate the practical use of HDDLGym in the Overcooked\ndomain. By leveraging the advantages of HDDL and Gym, HDDLGym aims to be a\nvaluable tool for studying RL in hierarchical planning, particularly in\nmulti-agent contexts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86HDDLGym\uff0c\u4e00\u4e2a\u5c06\u5206\u5c42\u89c4\u5212\u4e0e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65e0\u7f1d\u96c6\u6210\u7684Python\u5de5\u5177\uff0c\u652f\u6301\u591a\u667a\u80fd\u4f53\u573a\u666f\uff0c\u5e76\u63d0\u4f9b\u4e86\u4f7f\u7528\u6307\u5357\u548c\u793a\u4f8b\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u7f3a\u4e4f\u5c06\u5206\u5c42\u89c4\u5212\u4e0eRL\u96c6\u6210\u7684\u80fd\u529b\uff0c\u800cHDDLGym\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u548c\u5b9e\u8df5\u63d0\u4f9b\u4e86\u4fbf\u5229\u3002", "method": "\u57fa\u4e8eHDDL\uff08\u5206\u5c42\u9886\u57df\u5b9a\u4e49\u8bed\u8a00\uff09\u5f00\u53d1\u4e86HDDLGym\u5de5\u5177\uff0c\u81ea\u52a8\u751f\u6210OpenAI Gym\u73af\u5883\uff0c\u652f\u6301\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u89c4\u5212\u3002", "result": "HDDLGym\u6210\u529f\u96c6\u6210\u4e86HDDL\u4e0eRL\uff0c\u5e76\u901a\u8fc7Transport\u548cOvercooked\u7b49\u793a\u4f8b\u5c55\u793a\u4e86\u5176\u5b9e\u7528\u6027\u3002", "conclusion": "HDDLGym\u662f\u7814\u7a76\u5206\u5c42\u89c4\u5212\u4e2dRL\u7684\u6709\u4ef7\u503c\u5de5\u5177\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u573a\u666f\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u5206\u5c42\u89c4\u5212, HDDL, OpenAI Gym, \u591a\u667a\u80fd\u4f53"}}
{"id": "2505.21963", "pdf": "https://arxiv.org/pdf/2505.21963", "abs": "https://arxiv.org/abs/2505.21963", "authors": ["Taro Yano", "Yoichi Ishibashi", "Masafumi Oyamada"], "title": "LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\na wide range of tasks. To further tailor LLMs to specific domains or\napplications, post-training techniques such as Supervised Fine-Tuning (SFT),\nPreference Learning, and model merging are commonly employed. While each of\nthese methods has been extensively studied in isolation, the automated\nconstruction of complete post-training pipelines remains an underexplored area.\nExisting approaches typically rely on manual design or focus narrowly on\noptimizing individual components, such as data ordering or merging strategies.\nIn this work, we introduce LaMDAgent (short for Language Model Developing\nAgent), a novel framework that autonomously constructs and optimizes full\npost-training pipelines through the use of LLM-based agents. LaMDAgent\nsystematically explores diverse model generation techniques, datasets, and\nhyperparameter configurations, leveraging task-based feedback to discover\nhigh-performing pipelines with minimal human intervention. Our experiments show\nthat LaMDAgent improves tool-use accuracy by 9.0 points while preserving\ninstruction-following capabilities. Moreover, it uncovers effective\npost-training strategies that are often overlooked by conventional human-driven\nexploration. We further analyze the impact of data and model size scaling to\nreduce computational costs on the exploration, finding that model size scalings\nintroduces new challenges, whereas scaling data size enables cost-effective\npipeline discovery.", "AI": {"tldr": "LaMDAgent\u662f\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7LLM\u4ee3\u7406\u81ea\u4e3b\u6784\u5efa\u548c\u4f18\u5316\u540e\u8bad\u7ec3\u6d41\u7a0b\uff0c\u63d0\u9ad8\u7cbe\u5ea6\uff0c\u964d\u4f4e\u4eba\u5de5\u5e72\u9884\u3002", "motivation": "\u73b0\u6709\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\u591a\u4e3a\u624b\u52a8\u8bbe\u8ba1\u6216\u5355\u7ec4\u4ef6\u4f18\u5316\uff0c\u7f3a\u4e4f\u81ea\u52a8\u5316\u7684\u5168\u6d41\u7a0b\u63a2\u7d22\u3002", "method": "LaMDAgent\u5229\u7528LLM\u4ee3\u7406\uff0c\u7cfb\u7edf\u5730\u63a2\u7d22\u6a21\u578b\u751f\u6210\u6280\u672f\u3001\u6570\u636e\u96c6\u548c\u8d85\u53c2\u6570\u914d\u7f6e\uff0c\u57fa\u4e8e\u4efb\u52a1\u53cd\u9988\u4f18\u5316\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLaMDAgent\u5c06\u5de5\u5177\u4f7f\u7528\u7cbe\u5ea6\u63d0\u9ad89.0\u5206\uff0c\u5e76\u53d1\u73b0\u4f20\u7edf\u65b9\u6cd5\u6613\u5ffd\u7565\u7684\u9ad8\u6548\u7b56\u7565\u3002", "conclusion": "LaMDAgent\u80fd\u9ad8\u6548\u53d1\u73b0\u9ad8\u6027\u80fd\u540e\u8bad\u7ec3\u6d41\u7a0b\uff0c\u6570\u636e\u89c4\u6a21\u6269\u5c55\u66f4\u5177\u6210\u672c\u6548\u76ca\u3002", "keywords": "LLMs, post-training, LaMDAgent, automation, pipeline optimization"}}
{"id": "2505.21732", "pdf": "https://arxiv.org/pdf/2505.21732", "abs": "https://arxiv.org/abs/2505.21732", "authors": ["Ruijie Zhang", "Ziyue Liu", "Zhengyang Wang", "Zheng Zhang"], "title": "LaX: Boosting Low-Rank Training of Foundation Models via Latent Crossing", "categories": ["cs.LG"], "comment": null, "summary": "Training foundation models such as ViTs and LLMs requires tremendous\ncomputing cost. Low-rank matrix or tensor factorization offers a\nparameter-efficient alternative, but often downgrades performance due to the\nrestricted parameter space. In this work, we introduce {\\textbf{Latent Crossing\n(LaX)}} -- a simple yet effective plug-and-play module that enhances the\ncapacity of low-rank models by enabling information flow across low-rank\nsubspaces. We extensively validate the benefits of LaX on pre-training tasks\nwith ViT-Base/Large and LLaMA-like models ranging from 60M to 1B parameters.\nLaX boosts low-rank model performance to match or exceed the full-rank\nbaselines while using 2-3\\(\\times\\) fewer parameters. When equipped with\nlow-rank adapters (i.e., LoRA) for fine-tuning LLaMA-7/13B, LaX consistently\nimproves performance on arithmetic and common sense reasoning tasks with\nnegligible cost.", "AI": {"tldr": "LaX\uff08Latent Crossing\uff09\u662f\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u6a21\u5757\uff0c\u901a\u8fc7\u4fc3\u8fdb\u4f4e\u79e9\u5b50\u7a7a\u95f4\u7684\u4fe1\u606f\u6d41\u52a8\uff0c\u63d0\u5347\u4e86\u4f4e\u79e9\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u5728\u51cf\u5c11\u53c2\u6570\u91cf\u7684\u540c\u65f6\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u5168\u79e9\u57fa\u7ebf\u3002", "motivation": "\u7531\u4e8e\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff08\u5982ViTs\u548cLLMs\uff09\u7684\u8ba1\u7b97\u6210\u672c\u6781\u9ad8\uff0c\u4f4e\u79e9\u77e9\u9635\u6216\u5f20\u91cf\u5206\u89e3\u6210\u4e3a\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u4f46\u56e0\u53c2\u6570\u7a7a\u95f4\u53d7\u9650\u5e38\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002LaX\u65e8\u5728\u901a\u8fc7\u589e\u5f3a\u4f4e\u79e9\u6a21\u578b\u7684\u80fd\u529b\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "LaX\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u901a\u8fc7\u4fc3\u8fdb\u4f4e\u79e9\u5b50\u7a7a\u95f4\u7684\u4fe1\u606f\u6d41\u52a8\u6765\u6269\u5c55\u4f4e\u79e9\u6a21\u578b\u7684\u5bb9\u91cf\u3002\u5b83\u88ab\u5e94\u7528\u4e8eViT\u548cLLaMA\u7c7b\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u5e76\u5728\u4f4e\u79e9\u9002\u914d\u5668\uff08\u5982LoRA\uff09\u5fae\u8c03\u65f6\u4fdd\u6301\u6027\u80fd\u63d0\u5347\u3002", "result": "LaX\u57282-3\u500d\u53c2\u6570\u51cf\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u4f4e\u79e9\u6a21\u578b\u6027\u80fd\u5339\u914d\u6216\u8d85\u8d8a\u5168\u79e9\u57fa\u7ebf\u3002\u5728LLaMA-7/13B\u7684\u5fae\u8c03\u4efb\u52a1\u4e2d\uff0cLaX\u5728\u7b97\u672f\u548c\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "LaX\u901a\u8fc7\u4fe1\u606f\u6d41\u52a8\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u4f4e\u79e9\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u9ad8\u6548\u8bad\u7ec3\u5927\u89c4\u6a21\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u4f4e\u79e9\u6a21\u578b, \u53c2\u6570\u9ad8\u6548, \u4fe1\u606f\u6d41\u52a8, ViT, LLaMA"}}
{"id": "2404.11045", "pdf": "https://arxiv.org/pdf/2404.11045", "abs": "https://arxiv.org/abs/2404.11045", "authors": ["James Y. Huang", "Wenxuan Zhou", "Fei Wang", "Fred Morstatter", "Sheng Zhang", "Hoifung Poon", "Muhao Chen"], "title": "Offset Unlearning for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published in TMLR. https://openreview.net/pdf?id=A4RLpHPXCu", "summary": "Despite the strong capabilities of Large Language Models (LLMs) to acquire\nknowledge from their training corpora, the memorization of sensitive\ninformation in the corpora such as copyrighted, biased, and private content has\nled to ethical and legal concerns. In response to these challenges, unlearning\nhas emerged as a potential remedy for LLMs affected by problematic training\ndata. However, previous unlearning techniques are either not applicable to\nblack-box LLMs due to required access to model internal weights, or violate\ndata protection principles by retaining sensitive data for inference-time\ncorrection. We propose {\\delta}-Unlearning, an offset unlearning framework for\nblack-box LLMs. Instead of tuning the black-box LLM itself, {\\delta}-Unlearning\nlearns the logit offset needed for unlearning by contrasting the logits from a\npair of smaller models. Experiments demonstrate that {\\delta}- Unlearning can\neffectively unlearn target data while maintaining similar or even stronger\nperformance on general out-of-forget-scope tasks. {\\delta}-Unlearning also\neffectively incorporates different unlearning algorithms, making our approach a\nversatile solution to adapting various existing unlearning algorithms to\nblack-box LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u9ed1\u76d2\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u2018\u53cd\u5b66\u4e60\u2019\u6846\u67b6{\\delta}-Unlearning\uff0c\u901a\u8fc7\u5bf9\u6bd4\u4e00\u5bf9\u8f83\u5c0f\u6a21\u578b\u7684logit\u504f\u79fb\u91cf\u5b9e\u73b0\u76ee\u6807\u6570\u636e\u7684\u9057\u5fd8\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u8bbf\u95ee\u6a21\u578b\u5185\u90e8\u6743\u91cd\u6216\u4fdd\u7559\u654f\u611f\u6570\u636e\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u9ed1\u76d2LLMs\u56e0\u8bad\u7ec3\u6570\u636e\u4e2d\u5305\u542b\u654f\u611f\u4fe1\u606f\uff08\u5982\u7248\u6743\u3001\u504f\u89c1\u548c\u9690\u79c1\u5185\u5bb9\uff09\u800c\u5f15\u53d1\u7684\u4f26\u7406\u548c\u6cd5\u5f8b\u95ee\u9898\uff0c\u540c\u65f6\u5f25\u8865\u73b0\u6709\u53cd\u5b66\u4e60\u6280\u672f\u5bf9\u9ed1\u76d2\u6a21\u578b\u4e0d\u9002\u7528\u6216\u8fdd\u53cd\u6570\u636e\u4fdd\u62a4\u539f\u5219\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa{\\delta}-Unlearning\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u4e00\u5bf9\u8f83\u5c0f\u6a21\u578b\u751f\u6210logit\u504f\u79fb\u91cf\uff0c\u65e0\u9700\u8c03\u6574\u9ed1\u76d2LLM\u672c\u8eab\u5373\u53ef\u5b9e\u73b0\u76ee\u6807\u6570\u636e\u7684\u9057\u5fd8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u9057\u5fd8\u76ee\u6807\u6570\u636e\uff0c\u540c\u65f6\u5728\u975e\u9057\u5fd8\u4efb\u52a1\u4e0a\u4fdd\u6301\u751a\u81f3\u4f18\u4e8e\u539f\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e14\u517c\u5bb9\u591a\u79cd\u53cd\u5b66\u4e60\u7b97\u6cd5\u3002", "conclusion": "{\\delta}-Unlearning\u4e3a\u9ed1\u76d2LLMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u53cd\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u987e\u5b9e\u7528\u6027\u4e0e\u6570\u636e\u4fdd\u62a4\u5408\u89c4\u6027\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u53cd\u5b66\u4e60,\u9ed1\u76d2\u6a21\u578b,\u6570\u636e\u9690\u79c1,\u4f26\u7406\u95ee\u9898"}}
{"id": "2505.21967", "pdf": "https://arxiv.org/pdf/2505.21967", "abs": "https://arxiv.org/abs/2505.21967", "authors": ["Juan Ren", "Mark Dras", "Usman Naseem"], "title": "Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Large Vision-Language Models (LVLMs) have shown remarkable capabilities\nacross a wide range of multimodal tasks. However, their integration of visual\ninputs introduces expanded attack surfaces, thereby exposing them to novel\nsecurity vulnerabilities. In this work, we conduct a systematic\nrepresentational analysis to uncover why conventional adversarial attacks can\ncircumvent the safety mechanisms embedded in LVLMs. We further propose a novel\ntwo stage evaluation framework for adversarial attacks on LVLMs. The first\nstage differentiates among instruction non compliance, outright refusal, and\nsuccessful adversarial exploitation. The second stage quantifies the degree to\nwhich the model's output fulfills the harmful intent of the adversarial prompt,\nwhile categorizing refusal behavior into direct refusals, soft refusals, and\npartial refusals that remain inadvertently helpful. Finally, we introduce a\nnormative schema that defines idealized model behavior when confronted with\nharmful prompts, offering a principled target for safety alignment in\nmultimodal systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bc4\u4f30\u6846\u67b6\u6765\u8bc4\u4f30\u5bf9\u6297\u653b\u51fb\u7684\u6709\u6548\u6027\uff0c\u5e76\u5b9a\u4e49\u4e86\u9762\u5bf9\u6709\u5bb3\u63d0\u793a\u65f6\u7684\u7406\u60f3\u6a21\u578b\u884c\u4e3a\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u89c6\u89c9\u8f93\u5165\u7684\u5f15\u5165\u6269\u5927\u4e86\u653b\u51fb\u9762\uff0c\u5bfc\u81f4\u65b0\u7684\u5b89\u5168\u6f0f\u6d1e\u3002\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u4f20\u7edf\u5bf9\u6297\u653b\u51fb\u5982\u4f55\u7ed5\u8fc7LVLMs\u7684\u5b89\u5168\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "1. \u901a\u8fc7\u7cfb\u7edf\u6027\u8868\u5f81\u5206\u6790\u63ed\u793a\u4f20\u7edf\u653b\u51fb\u7ed5\u8fc7\u5b89\u5168\u673a\u5236\u7684\u539f\u56e0\u30022. \u63d0\u51fa\u4e24\u9636\u6bb5\u8bc4\u4f30\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u533a\u5206\u6307\u4ee4\u4e0d\u9075\u4ece\u3001\u76f4\u63a5\u62d2\u7edd\u548c\u6210\u529f\u653b\u51fb\uff1b\u7b2c\u4e8c\u9636\u6bb5\u91cf\u5316\u6a21\u578b\u8f93\u51fa\u6ee1\u8db3\u5bf9\u6297\u63d0\u793a\u6709\u5bb3\u610f\u56fe\u7684\u7a0b\u5ea6\uff0c\u5e76\u5206\u7c7b\u62d2\u7edd\u884c\u4e3a\u30023. \u63d0\u51fa\u7406\u60f3\u5316\u6a21\u578b\u884c\u4e3a\u7684\u89c4\u8303\u6a21\u5f0f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5bf9\u6297\u653b\u51fb\u53ef\u4ee5\u7ed5\u8fc7LVLMs\u7684\u5b89\u5168\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6709\u6548\u8bc4\u4f30\u653b\u51fb\u7684\u6846\u67b6\u548c\u7406\u60f3\u884c\u4e3a\u7684\u89c4\u8303\u6a21\u5f0f\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u4e86\u5bf9LVLMs\u5b89\u5168\u6027\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\u548c\u89c4\u8303\u6a21\u5f0f\uff0c\u4e3a\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u5b89\u5168\u5bf9\u9f50\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u76ee\u6807\u3002", "keywords": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u5bf9\u6297\u653b\u51fb, \u5b89\u5168\u6027\u8bc4\u4f30, \u591a\u6a21\u6001\u7cfb\u7edf, \u89c4\u8303\u6a21\u5f0f"}}
{"id": "2505.21743", "pdf": "https://arxiv.org/pdf/2505.21743", "abs": "https://arxiv.org/abs/2505.21743", "authors": ["Zihao Li", "Xinyuan Cao", "Xiangbo Gao", "Kexin Tian", "Keshu Wu", "Mohammad Anis", "Hao Zhang", "Keke Long", "Jiwan Jiang", "Xiaopeng Li", "Yunlong Zhang", "Tianbao Yang", "Dominique Lord", "Zhengzhong Tu", "Yang Zhou"], "title": "Simulating the Unseen: Crash Prediction Must Learn from What Did Not Happen", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Traffic safety science has long been hindered by a fundamental data paradox:\nthe crashes we most wish to prevent are precisely those events we rarely\nobserve. Existing crash-frequency models and surrogate safety metrics rely\nheavily on sparse, noisy, and under-reported records, while even sophisticated,\nhigh-fidelity simulations undersample the long-tailed situations that trigger\ncatastrophic outcomes such as fatalities. We argue that the path to achieving\nVision Zero, i.e., the complete elimination of traffic fatalities and severe\ninjuries, requires a paradigm shift from traditional crash-only learning to a\nnew form of counterfactual safety learning: reasoning not only about what\nhappened, but also about the vast set of plausible yet perilous scenarios that\ncould have happened under slightly different circumstances. To operationalize\nthis shift, our proposed agenda bridges macro to micro. Guided by crash-rate\npriors, generative scene engines, diverse driver models, and causal learning,\nnear-miss events are synthesized and explained. A crash-focused digital twin\ntestbed links micro scenes to macro patterns, while a multi-objective validator\nensures that simulations maintain statistical realism. This pipeline transforms\nsparse crash data into rich signals for crash prediction, enabling the\nstress-testing of vehicles, roads, and policies before deployment. By learning\nfrom crashes that almost happened, we can shift traffic safety from reactive\nforensics to proactive prevention, advancing Vision Zero.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ea4\u901a\u5b89\u5168\u6027\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u672a\u53d1\u751f\u7684\u5371\u9669\u60c5\u666f\uff08\u5373\"\u5dee\u70b9\u4e8b\u6545\"\uff09\uff0c\u7ed3\u5408\u751f\u6210\u573a\u666f\u5f15\u64ce\u548c\u56e0\u679c\u5b66\u4e60\uff0c\u4ece\u7a00\u758f\u7684\u4e8b\u6545\u6570\u636e\u4e2d\u63d0\u53d6\u4e30\u5bcc\u4fe1\u606f\uff0c\u63a8\u52a8\u4ece\u88ab\u52a8\u5206\u6790\u5230\u4e3b\u52a8\u9884\u9632\u7684\u8f6c\u53d8\uff0c\u4ee5\u5b9e\u73b0\"\u96f6\u613f\u666f\"\u76ee\u6807\u3002", "motivation": "\u4f20\u7edf\u4ea4\u901a\u4e8b\u6545\u5206\u6790\u65b9\u6cd5\u4f9d\u8d56\u7a00\u758f\u4e14\u566a\u58f0\u8f83\u591a\u7684\u6570\u636e\uff0c\u96be\u4ee5\u6709\u6548\u9884\u9632\u7f55\u89c1\u4f46\u540e\u679c\u4e25\u91cd\u7684\u4ea4\u901a\u4e8b\u6545\u3002\u4e3a\u5b9e\u73b0\"\u96f6\u613f\u666f\"\uff08\u96f6\u6b7b\u4ea1\u548c\u91cd\u4f24\uff09\uff0c\u9700\u8981\u8f6c\u5411\u7814\u7a76\u6f5c\u5728\u5371\u9669\u60c5\u666f\u3002", "method": "\u63d0\u51fa\"\u53cd\u4e8b\u5b9e\u5b89\u5168\u6027\u5b66\u4e60\"\u6846\u67b6\uff0c\u7ed3\u5408\u751f\u6210\u573a\u666f\u5f15\u64ce\u3001\u591a\u6837\u5316\u9a7e\u9a76\u5458\u6a21\u578b\u548c\u56e0\u679c\u5b66\u4e60\uff0c\u5408\u6210\u5e76\u5206\u6790\"\u5dee\u70b9\u4e8b\u6545\"\u3002\u5efa\u7acb\u6570\u5b57\u5b6a\u751f\u6d4b\u8bd5\u5e73\u53f0\uff0c\u8fde\u63a5\u5fae\u89c2\u573a\u666f\u548c\u5b8f\u89c2\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u591a\u76ee\u6807\u9a8c\u8bc1\u5668\u786e\u4fdd\u6a21\u62df\u7684\u7edf\u8ba1\u771f\u5b9e\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5c06\u7a00\u758f\u7684\u4e8b\u6545\u6570\u636e\u8f6c\u5316\u4e3a\u4e30\u5bcc\u7684\u9884\u6d4b\u4fe1\u53f7\uff0c\u652f\u6301\u5bf9\u8f66\u8f86\u3001\u9053\u8def\u548c\u653f\u7b56\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5\uff0c\u4ece\u800c\u5728\u90e8\u7f72\u524d\u9884\u6d4b\u548c\u9884\u9632\u6f5c\u5728\u4e8b\u6545\u3002", "conclusion": "\u901a\u8fc7\u5b66\u4e60\u672a\u53d1\u751f\u7684\u5371\u9669\u60c5\u666f\uff0c\u4ea4\u901a\u5b89\u5168\u6027\u5206\u6790\u53ef\u4ee5\u4ece\u88ab\u52a8\u8c03\u67e5\u8f6c\u5411\u4e3b\u52a8\u9884\u9632\uff0c\u63a8\u52a8\"\u96f6\u613f\u666f\"\u7684\u5b9e\u73b0\u3002", "keywords": "\u4ea4\u901a\u5b89\u5168\u6027, \u53cd\u4e8b\u5b9e\u5b66\u4e60, \u6570\u5b57\u5b6a\u751f, \u751f\u6210\u573a\u666f\u5f15\u64ce, \u96f6\u613f\u666f"}}
{"id": "2505.21506", "pdf": "https://arxiv.org/pdf/2505.21506", "abs": "https://arxiv.org/abs/2505.21506", "authors": ["Eli Bogdanov", "Izack Cohen", "Avigdor Gal"], "title": "Conformance Checking for Less: Efficient Conformance Checking for Long Event Sequences", "categories": ["cs.DB", "cs.AI", "cs.PL"], "comment": "17 pages, 4 figures", "summary": "Long event sequences (termed traces) and large data logs that originate from\nsensors and prediction models are becoming increasingly common in our data-rich\nworld. In such scenarios, conformance checking-validating a data log against an\nexpected system behavior (the process model) can become computationally\ninfeasible due to the exponential complexity of finding an optimal alignment.\nTo alleviate scalability challenges for this task, we propose ConLES, a\nsliding-window conformance checking approach for long event sequences that\npreserves the interpretability of alignment-based methods. ConLES partitions\ntraces into manageable subtraces and iteratively aligns each against the\nexpected behavior, leading to significant reduction of the search space while\nmaintaining overall accuracy. We use global information that captures\nstructural properties of both the trace and the process model, enabling\ninformed alignment decisions and discarding unpromising alignments, even if\nthey appear locally optimal. Performance evaluations across multiple datasets\nhighlight that ConLES outperforms the leading optimal and heuristic algorithms\nfor long traces, consistently achieving the optimal or near-optimal solution.\nUnlike other conformance methods that struggle with long event sequences,\nConLES significantly reduces the search space, scales efficiently, and uniquely\nsupports both predefined and discovered process models, making it a viable and\nleading option for conformance checking of long event sequences.", "AI": {"tldr": "\u6458\u8981\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aConLES\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u957f\u4e8b\u4ef6\u5e8f\u5217\u7684\u5408\u89c4\u6027\u68c0\u67e5\uff0c\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u6280\u672f\u5c06\u957f\u5e8f\u5217\u5206\u5272\u4e3a\u53ef\u7ba1\u7406\u7684\u5b50\u5e8f\u5217\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u641c\u7d22\u7a7a\u95f4\u5e76\u4fdd\u6301\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u4f20\u611f\u5668\u548c\u9884\u6d4b\u6a21\u578b\u4ea7\u751f\u7684\u957f\u4e8b\u4ef6\u5e8f\u5217\u548c\u5927\u89c4\u6a21\u6570\u636e\u65e5\u5fd7\u65e5\u76ca\u666e\u904d\uff0c\u4f20\u7edf\u7684\u5408\u89c4\u6027\u68c0\u67e5\u65b9\u6cd5\u56e0\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u800c\u96be\u4ee5\u5e94\u5bf9\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u65b0\u65b9\u6cd5\u3002", "method": "ConLES\u91c7\u7528\u6ed1\u52a8\u7a97\u53e3\u6280\u672f\u5c06\u957f\u4e8b\u4ef6\u5e8f\u5217\u5206\u5272\u4e3a\u5b50\u5e8f\u5217\uff0c\u5e76\u8fed\u4ee3\u5730\u5bf9\u6bcf\u4e2a\u5b50\u5e8f\u5217\u8fdb\u884c\u5bf9\u9f50\uff0c\u540c\u65f6\u5229\u7528\u5168\u5c40\u4fe1\u606f\u4f18\u5316\u5bf9\u9f50\u51b3\u7b56\uff0c\u4e22\u5f03\u4e0d\u5177\u6f5c\u529b\u7684\u5c40\u90e8\u6700\u4f18\u5bf9\u9f50\u65b9\u6848\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u8bc4\u4f30\u663e\u793a\uff0cConLES\u5728\u5904\u7406\u957f\u4e8b\u4ef6\u5e8f\u5217\u65f6\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u548c\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u80fd\u6301\u7eed\u83b7\u5f97\u6700\u4f18\u6216\u63a5\u8fd1\u6700\u4f18\u7684\u89e3\u3002", "conclusion": "ConLES\u663e\u8457\u51cf\u5c11\u4e86\u641c\u7d22\u7a7a\u95f4\uff0c\u6269\u5c55\u6027\u5f3a\uff0c\u652f\u6301\u9884\u5b9a\u4e49\u548c\u53d1\u73b0\u7684\u6d41\u7a0b\u6a21\u578b\uff0c\u662f\u957f\u4e8b\u4ef6\u5e8f\u5217\u5408\u89c4\u6027\u68c0\u67e5\u7684\u9886\u5148\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5408\u89c4\u6027\u68c0\u67e5,\u957f\u4e8b\u4ef6\u5e8f\u5217,\u6ed1\u52a8\u7a97\u53e3,ConLES,\u5bf9\u9f50\u4f18\u5316"}}
{"id": "2505.21979", "pdf": "https://arxiv.org/pdf/2505.21979", "abs": "https://arxiv.org/abs/2505.21979", "authors": ["Fakhraddin Alwajih", "Samar Mohamed Magdy", "Abdellah El Mekki", "Omer Nacar", "Youssef Nafea", "Safaa Taher Abdelfadil", "Abdulfattah Mohammed Yahya", "Hamzah Luqman", "Nada Almarwani", "Samah Aloufi", "Baraah Qawasmeh", "Houdaifa Atou", "Serry Sibaee", "Hamzah A. Alsayadi", "Walid Al-Dhabyani", "Maged S. Al-shaibani", "Aya El aatar", "Nour Qandos", "Rahaf Alhamouri", "Samar Ahmad", "Razan Khassib", "Lina Hamad", "Mohammed Anwar AL-Ghrawi", "Fatimah Alshamari", "Cheikh Malainine", "Doaa Qawasmeh", "Aminetou Yacoub", "Tfeil moilid", "Ruwa AbuHweidi", "Ahmed Aboeitta", "Vatimetou Mohamed Lemin", "Reem Abdel-Salam", "Ahlam Bashiti", "Adel Ammar", "Aisha Alansari", "Ahmed Ashraf", "Nora Alturayeif", "Sara Shatnawi", "Alcides Alcoba Inciarte", "AbdelRahim A. Elmadany", "Mohamedou cheikh tourad", "Ismail Berrada", "Mustafa Jarrar", "Shady Shehata", "Muhammad Abdul-Mageed"], "title": "Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset", "categories": ["cs.CL"], "comment": "https://github.com/UBC-NLP/pearl", "summary": "Mainstream large vision-language models (LVLMs) inherently encode cultural\nbiases, highlighting the need for diverse multimodal datasets. To address this\ngap, we introduce Pearl, a large-scale Arabic multimodal dataset and benchmark\nexplicitly designed for cultural understanding. Constructed through advanced\nagentic workflows and extensive human-in-the-loop annotations by 45 annotators\nfrom across the Arab world, Pearl comprises over K multimodal examples spanning\nten culturally significant domains covering all Arab countries. We further\nprovide two robust evaluation benchmarks Pearl and Pearl-Lite along with a\nspecialized subset Pearl-X explicitly developed to assess nuanced cultural\nvariations. Comprehensive evaluations on state-of-the-art open and proprietary\nLVLMs demonstrate that reasoning-centric instruction alignment substantially\nimproves models' cultural grounding compared to conventional scaling methods.\nPearl establishes a foundational resource for advancing culturally-informed\nmultimodal modeling research. All datasets and benchmarks are publicly\navailable.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Pearl\uff0c\u4e00\u4e2a\u4e13\u4e3a\u6587\u5316\u7406\u89e3\u8bbe\u8ba1\u7684\u5927\u89c4\u6a21\u963f\u62c9\u4f2f\u591a\u6a21\u6001\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u901a\u8fc7\u5148\u8fdb\u7684\u5de5\u4f5c\u6d41\u7a0b\u548c\u4eba\u5de5\u6807\u6ce8\u6784\u5efa\uff0c\u8bc4\u4f30\u663e\u793a\u5176\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u6587\u5316\u9002\u5e94\u6027\u3002", "motivation": "\u4e3b\u6d41\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5b58\u5728\u6587\u5316\u504f\u89c1\uff0c\u9700\u8981\u591a\u5143\u5316\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u9ad8\u7ea7\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u548c45\u4f4d\u963f\u62c9\u4f2f\u4e16\u754c\u6807\u6ce8\u8005\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u6784\u5efa\u4e86\u6db5\u76d610\u4e2a\u6587\u5316\u9886\u57df\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6Pearl\uff0c\u5e76\u8bbe\u8ba1\u4e86Pearl\u548cPearl-Lite\u4e24\u4e2a\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "\u8bc4\u4f30\u8868\u660e\uff0c\u4ee5\u63a8\u7406\u4e3a\u4e2d\u5fc3\u7684\u6307\u4ee4\u5bf9\u9f50\u76f8\u6bd4\u4f20\u7edf\u6269\u5c55\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6587\u5316\u57fa\u7840\u3002", "conclusion": "Pearl\u4e3a\u63a8\u8fdb\u6587\u5316\u611f\u77e5\u7684\u591a\u6a21\u6001\u5efa\u6a21\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\uff0c\u6570\u636e\u96c6\u548c\u57fa\u51c6\u5df2\u516c\u5f00\u3002", "keywords": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u6587\u5316\u504f\u89c1\uff0c\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u963f\u62c9\u4f2f\u6587\u5316\uff0c\u6307\u4ee4\u5bf9\u9f50"}}
{"id": "2505.21749", "pdf": "https://arxiv.org/pdf/2505.21749", "abs": "https://arxiv.org/abs/2505.21749", "authors": ["M. Reza Ebrahimi", "Roland Memisevic"], "title": "Revisiting Bi-Linear State Transitions in Recurrent Neural Networks", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The role of hidden units in recurrent neural networks is typically seen as\nmodeling memory, with research focusing on enhancing information retention\nthrough gating mechanisms. A less explored perspective views hidden units as\nactive participants in the computation performed by the network, rather than\npassive memory stores. In this work, we revisit bi-linear operations, which\ninvolve multiplicative interactions between hidden units and input embeddings.\nWe demonstrate theoretically and empirically that they constitute a natural\ninductive bias for representing the evolution of hidden states in state\ntracking tasks. These are the simplest type of task that require hidden units\nto actively contribute to the behavior of the network. We also show that\nbi-linear state updates form a natural hierarchy corresponding to state\ntracking tasks of increasing complexity, with popular linear recurrent networks\nsuch as Mamba residing at the lowest-complexity center of that hierarchy.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u4e2d\u9690\u85cf\u5355\u5143\u7684\u4e3b\u52a8\u8ba1\u7b97\u89d2\u8272\uff0c\u800c\u975e\u88ab\u52a8\u8bb0\u5fc6\u5b58\u50a8\uff0c\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u53cc\u7ebf\u6027\u64cd\u4f5c\u5728\u72b6\u6001\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u7684\u81ea\u7136\u5f52\u7eb3\u504f\u7f6e\u3002", "motivation": "\u4f20\u7edf\u7814\u7a76\u591a\u5173\u6ce8\u9690\u85cf\u5355\u5143\u7684\u8bb0\u5fc6\u529f\u80fd\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5176\u4f5c\u4e3a\u8ba1\u7b97\u53c2\u4e0e\u8005\u7684\u89d2\u8272\uff0c\u5c24\u5176\u662f\u5728\u72b6\u6001\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u7684\u4e3b\u52a8\u8d21\u732e\u3002", "method": "\u91cd\u65b0\u5f15\u5165\u53cc\u7ebf\u6027\u64cd\u4f5c\uff08\u9690\u85cf\u5355\u5143\u4e0e\u8f93\u5165\u5d4c\u5165\u7684\u4e58\u6027\u4ea4\u4e92\uff09\uff0c\u7406\u8bba\u5206\u6790\u5176\u5728\u72b6\u6001\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u5e76\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5c42\u6b21\u5316\u7ed3\u6784\u3002", "result": "\u53cc\u7ebf\u6027\u72b6\u6001\u66f4\u65b0\u5f62\u6210\u4e0e\u4efb\u52a1\u590d\u6742\u5ea6\u5bf9\u5e94\u7684\u81ea\u7136\u5c42\u6b21\u7ed3\u6784\uff0c\u5982Mamba\u7b49\u7ebf\u6027\u5faa\u73af\u7f51\u7edc\u4f4d\u4e8e\u8be5\u5c42\u6b21\u7684\u6700\u4f4e\u590d\u6742\u5ea6\u7aef\u3002", "conclusion": "\u9690\u85cf\u5355\u5143\u7684\u4e3b\u52a8\u8ba1\u7b97\u89c6\u89d2\u4e3a\u7f51\u7edc\u884c\u4e3a\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u53cc\u7ebf\u6027\u64cd\u4f5c\u5728\u72b6\u6001\u8ddf\u8e2a\u4e2d\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u4f18\u52bf\u3002", "keywords": "\u5faa\u73af\u795e\u7ecf\u7f51\u7edc, \u9690\u85cf\u5355\u5143, \u53cc\u7ebf\u6027\u64cd\u4f5c, \u72b6\u6001\u8ddf\u8e2a, \u5f52\u7eb3\u504f\u7f6e"}}
{"id": "2505.21513", "pdf": "https://arxiv.org/pdf/2505.21513", "abs": "https://arxiv.org/abs/2505.21513", "authors": ["Nicolas Echevarrieta-Catalan", "Ana Ribas-Rodriguez", "Francisco Cedron", "Odelia Schwartz", "Vanessa Aguiar-Pulido"], "title": "Enhancing Vision Transformer Explainability Using Artificial Astrocytes", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "LXCV Workshop at IEEE / CVF Computer Vision and Pattern Recognition\n  Conference (CVPR) 2025", "summary": "Machine learning models achieve high precision, but their decision-making\nprocesses often lack explainability. Furthermore, as model complexity\nincreases, explainability typically decreases. Existing efforts to improve\nexplainability primarily involve developing new eXplainable artificial\nintelligence (XAI) techniques or incorporating explainability constraints\nduring training. While these approaches yield specific improvements, their\napplicability remains limited. In this work, we propose the Vision Transformer\nwith artificial Astrocytes (ViTA). This training-free approach is inspired by\nneuroscience and enhances the reasoning of a pretrained deep neural network to\ngenerate more human-aligned explanations. We evaluated our approach employing\ntwo well-known XAI techniques, Grad-CAM and Grad-CAM++, and compared it to a\nstandard Vision Transformer (ViT). Using the ClickMe dataset, we quantified the\nsimilarity between the heatmaps produced by the XAI techniques and a\n(human-aligned) ground truth. Our results consistently demonstrate that\nincorporating artificial astrocytes enhances the alignment of model\nexplanations with human perception, leading to statistically significant\nimprovements across all XAI techniques and metrics utilized.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u79d1\u5b66\u7684\u65e0\u8bad\u7ec3\u65b9\u6cd5ViTA\uff0c\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u5f15\u5165\u4eba\u5de5\u661f\u5f62\u80f6\u8d28\u7ec6\u80de\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u89e3\u91ca\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u7684\u5bf9\u9f50\u5ea6\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u5b66\u4e60\u6a21\u578b\u867d\u7136\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u89e3\u91ca\u6027\u4e0d\u8db3\uff0c\u968f\u7740\u6a21\u578b\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u89e3\u91ca\u6027\u8fdb\u4e00\u6b65\u964d\u4f4e\u3002\u73b0\u6709\u6539\u8fdb\u65b9\u6cd5\uff08\u5982XAI\u6280\u672f\u6216\u8bad\u7ec3\u65f6\u52a0\u5165\u89e3\u91ca\u6027\u7ea6\u675f\uff09\u9002\u7528\u6027\u6709\u9650\u3002", "method": "\u63d0\u51faViTA\u65b9\u6cd5\uff0c\u7ed3\u5408\u795e\u7ecf\u79d1\u5b66\u539f\u7406\uff0c\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u4eba\u5de5\u661f\u5f62\u80f6\u8d28\u7ec6\u80de\u589e\u5f3a\u9884\u8bad\u7ec3\u7f51\u7edc\u7684\u63a8\u7406\u80fd\u529b\uff0c\u751f\u6210\u66f4\u7b26\u5408\u4eba\u7c7b\u7406\u89e3\u7684\u89e3\u91ca\u3002\u91c7\u7528Grad-CAM\u548cGrad-CAM++\u6280\u672f\u8bc4\u4f30\uff0c\u5e76\u4e0e\u6807\u51c6ViT\u5bf9\u6bd4\u3002", "result": "\u5728ClickMe\u6570\u636e\u96c6\u4e0a\uff0cViTA\u663e\u8457\u63d0\u9ad8\u4e86XAI\u6280\u672f\u751f\u6210\u7684\u70ed\u56fe\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u57fa\u51c6\u7684\u5bf9\u9f50\u5ea6\uff0c\u6240\u6709\u6d4b\u8bd5\u6307\u6807\u5747\u663e\u793a\u7edf\u8ba1\u5b66\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "ViTA\u901a\u8fc7\u4eff\u751f\u5b66\u8bbe\u8ba1\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u89e3\u91ca\u7684\u4eba\u7c7b\u5bf9\u9f50\u6027\uff0c\u4e3a\u53ef\u89e3\u91caAI\u63d0\u4f9b\u4e86\u65b0\u7684\u65e0\u8bad\u7ec3\u4f18\u5316\u9014\u5f84\u3002", "keywords": "\u673a\u5668\u5b66\u4e60\u7684\u53ef\u89e3\u91ca\u6027, \u4eba\u5de5\u661f\u5f62\u80f6\u8d28\u7ec6\u80de, \u65e0\u8bad\u7ec3\u4f18\u5316, \u795e\u7ecf\u79d1\u5b66\u542f\u53d1, XAI"}}
{"id": "2505.21997", "pdf": "https://arxiv.org/pdf/2505.21997", "abs": "https://arxiv.org/abs/2505.21997", "authors": ["Jihong Zhang", "Xinya Liang", "Anqi Deng", "Nicole Bonge", "Lin Tan", "Ling Zhang", "Nicole Zarrett"], "title": "Leveraging Interview-Informed LLMs to Model Survey Responses: Comparative Insights from AI-Generated and Human Data", "categories": ["cs.CL"], "comment": null, "summary": "Mixed methods research integrates quantitative and qualitative data but faces\nchallenges in aligning their distinct structures, particularly in examining\nmeasurement characteristics and individual response patterns. Advances in large\nlanguage models (LLMs) offer promising solutions by generating synthetic survey\nresponses informed by qualitative data. This study investigates whether LLMs,\nguided by personal interviews, can reliably predict human survey responses,\nusing the Behavioral Regulations in Exercise Questionnaire (BREQ) and\ninterviews from after-school program staff as a case study. Results indicate\nthat LLMs capture overall response patterns but exhibit lower variability than\nhumans. Incorporating interview data improves response diversity for some\nmodels (e.g., Claude, GPT), while well-crafted prompts and low-temperature\nsettings enhance alignment between LLM and human responses. Demographic\ninformation had less impact than interview content on alignment accuracy. These\nfindings underscore the potential of interview-informed LLMs to bridge\nqualitative and quantitative methodologies while revealing limitations in\nresponse variability, emotional interpretation, and psychometric fidelity.\nFuture research should refine prompt design, explore bias mitigation, and\noptimize model settings to enhance the validity of LLM-generated survey data in\nsocial science research.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u7ed3\u5408\u8d28\u6027\u6570\u636e\u751f\u6210\u5408\u6210\u8c03\u67e5\u56de\u5e94\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0LLMs\u80fd\u6355\u6349\u4eba\u7c7b\u56de\u5e94\u6a21\u5f0f\u4f46\u53d8\u5f02\u6027\u8f83\u4f4e\uff0c\u8bbf\u8c08\u6570\u636e\u53ef\u63d0\u5347\u90e8\u5206\u6a21\u578b\u7684\u591a\u6837\u6027\uff0c\u63d0\u793a\u8bbe\u8ba1\u5bf9\u5bf9\u9f50\u51c6\u786e\u6027\u5f71\u54cd\u663e\u8457\u3002", "motivation": "\u89e3\u51b3\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\u4e2d\u5b9a\u91cf\u4e0e\u5b9a\u6027\u6570\u636e\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u5229\u7528LLMs\u751f\u6210\u57fa\u4e8e\u8d28\u6027\u6570\u636e\u7684\u5408\u6210\u8c03\u67e5\u56de\u5e94\u3002", "method": "\u4f7f\u7528BREQ\u95ee\u5377\u548c\u8bfe\u540e\u9879\u76ee\u5458\u5de5\u8bbf\u8c08\u6570\u636e\uff0c\u6d4b\u8bd5LLMs\u5728\u63d0\u793a\u6307\u5bfc\u4e0b\u7684\u8c03\u67e5\u56de\u5e94\u9884\u6d4b\u80fd\u529b\u3002", "result": "LLMs\u80fd\u91cd\u73b0\u4eba\u7c7b\u56de\u5e94\u6a21\u5f0f\u4f46\u53d8\u5f02\u6027\u4e0d\u8db3\uff0c\u8bbf\u8c08\u6570\u636e\u63d0\u5347GPT\u548cClaude\u7b49\u6a21\u578b\u7684\u591a\u6837\u6027\uff0c\u63d0\u793a\u8bbe\u8ba1\u548c\u6e29\u5ea6\u8bbe\u7f6e\u5f71\u54cd\u5bf9\u9f50\u6548\u679c\u3002", "conclusion": "\u8bbf\u8c08\u5f15\u5bfc\u7684LLMs\u6709\u6f5c\u529b\u5f25\u5408\u8d28\u6027-\u91cf\u5316\u65b9\u6cd5\u9e3f\u6c9f\uff0c\u4f46\u9700\u4f18\u5316\u63d0\u793a\u8bbe\u8ba1\u3001\u504f\u8bef\u6d88\u9664\u548c\u6a21\u578b\u8bbe\u7f6e\u4ee5\u63d0\u5347\u6570\u636e\u6548\u5ea6\u3002", "keywords": "\u6df7\u5408\u65b9\u6cd5\u7814\u7a76, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u5408\u6210\u6570\u636e, \u8c03\u67e5\u56de\u5e94, \u8d28\u6027\u6570\u636e"}}
{"id": "2505.21750", "pdf": "https://arxiv.org/pdf/2505.21750", "abs": "https://arxiv.org/abs/2505.21750", "authors": ["Vivienne Huiling Wang", "Tinghuai Wang", "Joni Pajarinen"], "title": "Hierarchical Reinforcement Learning with Uncertainty-Guided Diffusional Subgoals", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Hierarchical reinforcement learning (HRL) learns to make decisions on\nmultiple levels of temporal abstraction. A key challenge in HRL is that the\nlow-level policy changes over time, making it difficult for the high-level\npolicy to generate effective subgoals. To address this issue, the high-level\npolicy must capture a complex subgoal distribution while also accounting for\nuncertainty in its estimates. We propose an approach that trains a conditional\ndiffusion model regularized by a Gaussian Process (GP) prior to generate a\ncomplex variety of subgoals while leveraging principled GP uncertainty\nquantification. Building on this framework, we develop a strategy that selects\nsubgoals from both the diffusion policy and GP's predictive mean. Our approach\noutperforms prior HRL methods in both sample efficiency and performance on\nchallenging continuous control benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u548cGaussian Process\u7684\u5c42\u6b21\u5f3a\u5316\u5b66\u4e60\uff08HRL\uff09\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u9ad8\u5c42\u7b56\u7565\u751f\u6210\u6709\u6548\u5b50\u76ee\u6807\u7684\u6311\u6218\u3002", "motivation": "\u89e3\u51b3HRL\u4e2d\u56e0\u5e95\u5c42\u7b56\u7565\u53d8\u5316\u5bfc\u81f4\u9ad8\u5c42\u7b56\u7565\u96be\u4ee5\u751f\u6210\u6709\u6548\u5b50\u76ee\u6807\u7684\u95ee\u9898\uff0c\u540c\u65f6\u6355\u6349\u590d\u6742\u7684\u5b50\u76ee\u6807\u5206\u5e03\u5e76\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u751f\u6210\u5b50\u76ee\u6807\uff0c\u5e76\u7528Gaussian Process\u5148\u9a8c\u6b63\u5219\u5316\uff0c\u7ed3\u5408\u6269\u6563\u7b56\u7565\u548cGP\u9884\u6d4b\u5747\u503c\u9009\u62e9\u5b50\u76ee\u6807\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u6837\u672c\u6548\u7387\u548c\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u4f18\u4e8e\u73b0\u6709HRL\u65b9\u6cd5\u3002", "conclusion": "\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7ed3\u5408GP\u5148\u9a8c\u80fd\u6709\u6548\u751f\u6210\u590d\u6742\u5b50\u76ee\u6807\u5e76\u63d0\u9ad8HRL\u6027\u80fd\u3002", "keywords": "\u5c42\u6b21\u5f3a\u5316\u5b66\u4e60, \u6761\u4ef6\u6269\u6563\u6a21\u578b, Gaussian Process, \u5b50\u76ee\u6807\u751f\u6210"}}
{"id": "2505.21520", "pdf": "https://arxiv.org/pdf/2505.21520", "abs": "https://arxiv.org/abs/2505.21520", "authors": ["Spiros Baxavanakis", "Manos Schinas", "Symeon Papadopoulos"], "title": "Do DeepFake Attribution Models Generalize?", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in DeepFake generation, along with the proliferation of\nopen-source tools, have significantly lowered the barrier for creating\nsynthetic media. This trend poses a serious threat to the integrity and\nauthenticity of online information, undermining public trust in institutions\nand media. State-of-the-art research on DeepFake detection has primarily\nfocused on binary detection models. A key limitation of these models is that\nthey treat all manipulation techniques as equivalent, despite the fact that\ndifferent methods introduce distinct artifacts and visual cues. Only a limited\nnumber of studies explore DeepFake attribution models, although such models are\ncrucial in practical settings. By providing the specific manipulation method\nemployed, these models could enhance both the perceived trustworthiness and\nexplainability for end users. In this work, we leverage five state-of-the-art\nbackbone models and conduct extensive experiments across six DeepFake datasets.\nFirst, we compare binary and multi-class models in terms of cross-dataset\ngeneralization. Second, we examine the accuracy of attribution models in\ndetecting seen manipulation methods in unknown datasets, hence uncovering data\ndistribution shifts on the same DeepFake manipulations. Last, we assess the\neffectiveness of contrastive methods in improving cross-dataset generalization\nperformance. Our findings indicate that while binary models demonstrate better\ngeneralization abilities, larger models, contrastive methods, and higher data\nquality can lead to performance improvements in attribution models. The code of\nthis work is available on GitHub.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86DeepFake\u68c0\u6d4b\u4e2d\u7684\u591a\u5206\u7c7b\u6a21\u578b\u548c\u5f52\u56e0\u6a21\u578b\uff0c\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u548c\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u51fa\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5bf9\u63d0\u5347\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u7531\u4e8eDeepFake\u6280\u672f\u7684\u666e\u53ca\u964d\u4f4e\u4e86\u5408\u6210\u5a92\u4f53\u7684\u5236\u4f5c\u95e8\u69db\uff0c\u5bf9\u5728\u7ebf\u4fe1\u606f\u7684\u771f\u5b9e\u6027\u548c\u516c\u4f17\u4fe1\u4efb\u6784\u6210\u5a01\u80c1\u3002\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u4e8c\u5206\u7c7b\u6a21\u578b\uff0c\u5ffd\u7565\u4e86\u4e0d\u540c\u7be1\u6539\u65b9\u6cd5\u5e26\u6765\u7684\u72ec\u7279\u75d5\u8ff9\uff0c\u800c\u5f52\u56e0\u6a21\u578b\u80fd\u63d0\u4f9b\u66f4\u53ef\u4fe1\u548c\u53ef\u89e3\u91ca\u7684\u7ed3\u679c\u3002", "method": "\u5229\u7528\u4e94\u79cd\u9aa8\u5e72\u6a21\u578b\u5728\u516d\u4e2aDeepFake\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e8c\u5206\u7c7b\u4e0e\u591a\u5206\u7c7b\u6a21\u578b\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\uff0c\u8bc4\u4f30\u5f52\u56e0\u6a21\u578b\u5bf9\u672a\u77e5\u6570\u636e\u96c6\u7684\u68c0\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u7814\u7a76\u5bf9\u6bd4\u65b9\u6cd5\u5bf9\u6027\u80fd\u7684\u63d0\u5347\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e8c\u5206\u7c7b\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff0c\u4f46\u66f4\u5927\u6a21\u578b\u3001\u5bf9\u6bd4\u65b9\u6cd5\u548c\u66f4\u9ad8\u6570\u636e\u8d28\u91cf\u53ef\u4ee5\u63d0\u5347\u5f52\u56e0\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u5f52\u56e0\u6a21\u578b\u5728\u63d0\u5347\u53ef\u4fe1\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u6539\u5584\u8de8\u6570\u636e\u96c6\u6027\u80fd\u3002", "keywords": "DeepFake\u68c0\u6d4b, \u5f52\u56e0\u6a21\u578b, \u591a\u5206\u7c7b, \u5bf9\u6bd4\u5b66\u4e60, \u6cdb\u5316\u80fd\u529b"}}
{"id": "2505.21999", "pdf": "https://arxiv.org/pdf/2505.21999", "abs": "https://arxiv.org/abs/2505.21999", "authors": ["Ashim Gupta", "Maitrey Mehta", "Zhichao Xu", "Vivek Srikumar"], "title": "Found in Translation: Measuring Multilingual LLM Consistency as Simple as Translate then Evaluate", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) provide detailed and impressive responses to\nqueries in English. However, are they really consistent at responding to the\nsame query in other languages? The popular way of evaluating for multilingual\nperformance of LLMs requires expensive-to-collect annotated datasets. Further,\nevaluating for tasks like open-ended generation, where multiple correct answers\nmay exist, is nontrivial. Instead, we propose to evaluate the predictability of\nmodel response across different languages. In this work, we propose a framework\nto evaluate LLM's cross-lingual consistency based on a simple Translate then\nEvaluate strategy. We instantiate this evaluation framework along two\ndimensions of consistency: information and empathy. Our results reveal\npronounced inconsistencies in popular LLM responses across thirty languages,\nwith severe performance deficits in certain language families and scripts,\nunderscoring critical weaknesses in their multilingual capabilities. These\nfindings necessitate cross-lingual evaluations that are consistent along\nmultiple dimensions. We invite practitioners to use our framework for future\nmultilingual LLM benchmarking.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8de8\u8bed\u8a00\u4e00\u81f4\u6027\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ffb\u8bd1\u540e\u8bc4\u4f30\u7b56\u7565\u68c0\u6d4b\u5176\u54cd\u5e94\u7684\u4e00\u81f4\u6027\u548c\u6027\u80fd\u5dee\u5f02\uff0c\u63ed\u793a\u4e86\u591a\u8bed\u8a00\u80fd\u529b\u4e2d\u7684\u5173\u952e\u5f31\u70b9\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u8bc4\u4f30LLM\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u7684\u54cd\u5e94\u4e00\u81f4\u6027\uff0c\u907f\u514d\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5e76\u89e3\u51b3\u5f00\u653e\u751f\u6210\u4efb\u52a1\u8bc4\u4f30\u7684\u96be\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u201c\u7ffb\u8bd1\u540e\u8bc4\u4f30\u201d\u7684\u6846\u67b6\uff0c\u4ece\u4fe1\u606f\u548c\u60c5\u611f\u4e24\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u6a21\u578b\u572830\u79cd\u8bed\u8a00\u4e2d\u7684\u4e00\u81f4\u6027\u3002", "result": "\u7ed3\u679c\u53d1\u73b0LLM\u5728\u4e0d\u540c\u8bed\u7cfb\u548c\u6587\u5b57\u4e2d\u5b58\u5728\u663e\u8457\u7684\u54cd\u5e94\u4e0d\u4e00\u81f4\u6027\uff0c\u67d0\u4e9b\u8bed\u8a00\u8868\u73b0\u4e25\u91cd\u4e0d\u8db3\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u8de8\u8bed\u8a00\u8bc4\u4f30\u9700\u591a\u7ef4\u5ea6\u4e00\u81f4\u6027\uff0c\u5e76\u5efa\u8bae\u4f7f\u7528\u8be5\u6846\u67b6\u8fdb\u884c\u672a\u6765\u591a\u8bed\u8a00LLM\u57fa\u51c6\u6d4b\u8bd5\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u8de8\u8bed\u8a00\u4e00\u81f4\u6027, \u7ffb\u8bd1\u540e\u8bc4\u4f30, \u591a\u8bed\u8a00\u8bc4\u4f30, \u5f00\u653e\u751f\u6210"}}
{"id": "2505.21775", "pdf": "https://arxiv.org/pdf/2505.21775", "abs": "https://arxiv.org/abs/2505.21775", "authors": ["Michael Klamkin", "Arnaud Deza", "Sikai Cheng", "Haoruo Zhao", "Pascal Van Hentenryck"], "title": "DualSchool: How Reliable are LLMs for Optimization Education?", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "Consider the following task taught in introductory optimization courses which\naddresses challenges articulated by the community at the intersection of\n(generative) AI and OR: generate the dual of a linear program. LLMs, being\ntrained at web-scale, have the conversion process and many instances of Primal\nto Dual Conversion (P2DC) at their disposal. Students may thus reasonably\nexpect that LLMs would perform well on the P2DC task. To assess this\nexpectation, this paper introduces DualSchool, a comprehensive framework for\ngenerating and verifying P2DC instances. The verification procedure of\nDualSchool uses the Canonical Graph Edit Distance, going well beyond existing\nevaluation methods for optimization models, which exhibit many false positives\nand negatives when applied to P2DC. Experiments performed by DualSchool reveal\ninteresting findings. Although LLMs can recite the conversion procedure\naccurately, state-of-the-art open LLMs fail to consistently produce correct\nduals. This finding holds even for the smallest two-variable instances and for\nderivative tasks, such as correctness, verification, and error classification.\nThe paper also discusses the implications for educators, students, and the\ndevelopment of large reasoning systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4f18\u5316\u8bfe\u7a0b\u4e2d\u7ebf\u6027\u89c4\u5212\u5bf9\u5076\u751f\u6210\u4efb\u52a1\uff08P2DC\uff09\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u5f00\u653eLLM\u4e5f\u65e0\u6cd5\u59cb\u7ec8\u751f\u6210\u6b63\u786e\u7684\u5bf9\u5076\u89e3\u3002\u8bba\u6587\u63d0\u51fa\u4e86DualSchool\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u548c\u9a8c\u8bc1P2DC\u5b9e\u4f8b\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u8bc4\u4f30LLM\u5728\u751f\u6210\u7ebf\u6027\u89c4\u5212\u5bf9\u5076\uff08P2DC\uff09\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5c3d\u7ba1LLM\u5728\u8bad\u7ec3\u4e2d\u63a5\u89e6\u4e86\u5927\u91cf\u76f8\u5173\u5b9e\u4f8b\uff0c\u4f46\u5176\u5b9e\u9645\u8868\u73b0\u4ecd\u6709\u5f85\u9a8c\u8bc1\u3002", "method": "\u91c7\u7528DualSchool\u6846\u67b6\u751f\u6210\u548c\u9a8c\u8bc1P2DC\u5b9e\u4f8b\uff0c\u4f7f\u7528\u89c4\u8303\u56fe\u7f16\u8f91\u8ddd\u79bb\uff08Canonical Graph Edit Distance\uff09\u8fdb\u884c\u9a8c\u8bc1\uff0c\u8d85\u8d8a\u73b0\u6709\u4f18\u5316\u6a21\u578b\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u5f00\u653eLLM\u4e5f\u65e0\u6cd5\u59cb\u7ec8\u751f\u6210\u6b63\u786e\u7684\u5bf9\u5076\u89e3\uff0c\u751a\u81f3\u5728\u6700\u5c0f\u7684\u4e24\u53d8\u91cf\u5b9e\u4f8b\u548c\u884d\u751f\u4efb\u52a1\uff08\u5982\u6b63\u786e\u6027\u9a8c\u8bc1\u548c\u9519\u8bef\u5206\u7c7b\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u8bba\u6587\u7ed3\u8bba\u6307\u51faLLM\u5728P2DC\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5bf9\u6559\u80b2\u8005\u3001\u5b66\u751f\u548c\u5927\u578b\u63a8\u7406\u7cfb\u7edf\u5f00\u53d1\u7684\u542f\u793a\u3002", "keywords": "\u7ebf\u6027\u89c4\u5212\u5bf9\u5076\u751f\u6210\uff08P2DC\uff09\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0cDualSchool\u6846\u67b6\uff0c\u89c4\u8303\u56fe\u7f16\u8f91\u8ddd\u79bb\uff0c\u4f18\u5316\u6559\u80b2"}}
{"id": "2505.21522", "pdf": "https://arxiv.org/pdf/2505.21522", "abs": "https://arxiv.org/abs/2505.21522", "authors": ["Shan Gao", "Zhiqiang Wu", "Yawen Niu", "Xiaotao Li", "Qingqing Xu"], "title": "CIM-NET: A Video Denoising Deep Neural Network Model Optimized for Computing-in-Memory Architectures", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "While deep neural network (DNN)-based video denoising has demonstrated\nsignificant performance, deploying state-of-the-art models on edge devices\nremains challenging due to stringent real-time and energy efficiency\nrequirements. Computing-in-Memory (CIM) chips offer a promising solution by\nintegrating computation within memory cells, enabling rapid matrix-vector\nmultiplication (MVM). However, existing DNN models are often designed without\nconsidering CIM architectural constraints, thus limiting their acceleration\npotential during inference. To address this, we propose a hardware-algorithm\nco-design framework incorporating two innovations: (1) a CIM-Aware\nArchitecture, CIM-NET, optimized for large receptive field operation and CIM's\ncrossbar-based MVM acceleration; and (2) a pseudo-convolutional operator,\nCIM-CONV, used within CIM-NET to integrate slide-based processing with fully\nconnected transformations for high-quality feature extraction and\nreconstruction. This framework significantly reduces the number of MVM\noperations, improving inference speed on CIM chips while maintaining\ncompetitive performance. Experimental results indicate that, compared to the\nconventional lightweight model FastDVDnet, CIM-NET substantially reduces MVM\noperations with a slight decrease in denoising performance. With a stride value\nof 8, CIM-NET reduces MVM operations to 1/77th of the original, while\nmaintaining competitive PSNR (35.11 dB vs. 35.56 dB", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u786c\u4ef6-\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u6846\u67b6CIM-NET\uff0c\u7ed3\u5408CIM-Aware\u67b6\u6784\u548c\u4f2a\u5377\u79ef\u7b97\u5b50CIM-CONV\uff0c\u663e\u8457\u51cf\u5c11\u77e9\u9635-\u5411\u91cf\u4e58\u6cd5\u64cd\u4f5c\uff0c\u63d0\u5347CIM\u82af\u7247\u7684\u63a8\u7406\u901f\u5ea6\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u8fb9\u7f18\u8bbe\u5907\u5bf9\u5b9e\u65f6\u6027\u548c\u80fd\u6548\u7684\u4e25\u683c\u8981\u6c42\uff0c\u73b0\u6709DNN\u6a21\u578b\u672a\u8003\u8651CIM\u67b6\u6784\u9650\u5236\uff0c\u9650\u5236\u4e86\u5176\u63a8\u7406\u52a0\u901f\u6f5c\u529b\u3002\u56e0\u6b64\uff0c\u9700\u8bbe\u8ba1\u4e00\u79cd\u9002\u5408CIM\u67b6\u6784\u7684\u4f18\u5316\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86CIM-NET\u67b6\u6784\u548c\u4f2a\u5377\u79ef\u7b97\u5b50CIM-CONV\uff0c\u7ed3\u5408\u5927\u611f\u53d7\u91ce\u64cd\u4f5c\u548c\u57fa\u4e8e\u4ea4\u53c9\u5f00\u5173\u7684MVM\u52a0\u901f\uff0c\u51cf\u5c11MVM\u64cd\u4f5c\u3002", "result": "CIM-NET\u5c06MVM\u64cd\u4f5c\u51cf\u5c11\u81f31/77\uff0c\u540c\u65f6\u4fdd\u6301\u4e0eFastDVDnet\u76f8\u8fd1\u7684PSNR\uff0835.11 dB vs. 35.56 dB\uff09\u3002", "conclusion": "\u786c\u4ef6-\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\u6709\u6548\u4f18\u5316\u4e86CIM\u82af\u7247\u4e0a\u7684DNN\u63a8\u7406\uff0c\u5e73\u8861\u4e86\u901f\u5ea6\u548c\u6027\u80fd\u3002", "keywords": "DNN, CIM, video denoising, MVM, co-design"}}
{"id": "2505.22003", "pdf": "https://arxiv.org/pdf/2505.22003", "abs": "https://arxiv.org/abs/2505.22003", "authors": ["Jatin Gupta", "Akhil Sharma", "Saransh Singhania", "Ali Imam Abidi"], "title": "Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal Assistance", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 5 tables, 4 figures. This is a revised version of a preprint\n  previously available at this URL: https://doi.org/10.21203/rs.3.rs-5351879/v1", "summary": "Pursuit of accessible legal assistance in India faces a critical gap, as many\ncitizens struggle to leverage their legal rights due to limited awareness and\naccess to relevant legal information. This paper introduces Legal Assist AI, a\ntransformer-based model designed to bridge this gap by offering effective legal\nassistance through large language models (LLMs). The system retrieves relevant\nlegal information from a curated database and generates accurate responses,\nenabling effective assistance for diverse users, including legal professionals,\nscholars, and the general public. The model was fine-tuned on extensive\ndatasets from the Indian legal domain, including Indian Constitution, Bharatiya\nNyaya Sanhita (BNS), Bharatiya Nagarik Suraksha Sanhita (BNSS) and so forth,\nproviding a robust understanding of the complexities of Indian law. By\nincorporating domain-specific legal datasets, the proposed model demonstrated\nremarkable efficiency and specialization in legal Question-Answering. The model\nwas evaluated against state-of-the-art models such as GPT-3.5 Turbo and Mistral\n7B, achieving a 60.08% score on the AIBE, outperforming its competitors in\nlegal reasoning and accuracy. Unlike other models, Legal Assist AI avoided\ncommon issues such as hallucinations, making it highly reliable for practical\nlegal applications. It showcases the model's applicability in real-world legal\nscenarios, with future iterations aiming to enhance performance and expand its\ndataset to cover a broader range of multilingual and case-specific queries as\nwell.", "AI": {"tldr": "Legal Assist AI, a transformer-based model, was developed to address India's legal information gap by providing accurate, domain-specific legal assistance using LLMs, outperforming existing models like GPT-3.5 Turbo in legal reasoning.", "motivation": "The lack of accessible legal assistance and awareness in India motivated the creation of Legal Assist AI to bridge this gap and empower citizens with legal knowledge.", "method": "The model was fine-tuned on Indian legal datasets (e.g., Constitution, BNS, BNSS) to specialize in legal Question-Answering, leveraging LLMs for information retrieval and response generation.", "result": "Legal Assist AI achieved a 60.08% score on the AIBE, surpassing competitors like GPT-3.5 Turbo in accuracy and avoiding hallucinations, proving its reliability for real-world use.", "conclusion": "The model demonstrates strong potential for practical legal applications, with future goals including performance improvements and broader multilingual and case-specific coverage.", "keywords": "Legal Assist AI, LLMs, Indian law, transformer-based model, legal Question-Answering"}}
{"id": "2505.21777", "pdf": "https://arxiv.org/pdf/2505.21777", "abs": "https://arxiv.org/abs/2505.21777", "authors": ["Bao Pham", "Gabriel Raya", "Matteo Negri", "Mohammed J. Zaki", "Luca Ambrogioni", "Dmitry Krotov"], "title": "Memorization to Generalization: Emergence of Diffusion Models from Associative Memory", "categories": ["cs.LG", "cond-mat.dis-nn"], "comment": null, "summary": "Hopfield networks are associative memory (AM) systems, designed for storing\nand retrieving patterns as local minima of an energy landscape. In the\nclassical Hopfield model, an interesting phenomenon occurs when the amount of\ntraining data reaches its critical memory load $- spurious\\,\\,states$, or\nunintended stable points, emerge at the end of the retrieval dynamics, leading\nto incorrect recall. In this work, we examine diffusion models, commonly used\nin generative modeling, from the perspective of AMs. The training phase of\ndiffusion model is conceptualized as memory encoding (training data is stored\nin the memory). The generation phase is viewed as an attempt of memory\nretrieval. In the small data regime the diffusion model exhibits a strong\nmemorization phase, where the network creates distinct basins of attraction\naround each sample in the training set, akin to the Hopfield model below the\ncritical memory load. In the large data regime, a different phase appears where\nan increase in the size of the training set fosters the creation of new\nattractor states that correspond to manifolds of the generated samples.\nSpurious states appear at the boundary of this transition and correspond to\nemergent attractor states, which are absent in the training set, but, at the\nsame time, have distinct basins of attraction around them. Our findings\nprovide: a novel perspective on the memorization-generalization phenomenon in\ndiffusion models via the lens of AMs, theoretical prediction of existence of\nspurious states, empirical validation of this prediction in commonly-used\ndiffusion models.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ece\u8054\u60f3\u8bb0\u5fc6\uff08AM\uff09\u7684\u89d2\u5ea6\u5206\u6790\u4e86\u6269\u6563\u6a21\u578b\uff0c\u63a2\u8ba8\u4e86\u5728\u8bad\u7ec3\u6570\u636e\u91cf\u4e0d\u540c\u65f6\u6269\u6563\u6a21\u578b\u7684\u8bb0\u5fc6\u4e0e\u6cdb\u5316\u73b0\u8c61\uff0c\u9884\u6d4b\u5e76\u9a8c\u8bc1\u4e86\u865a\u5047\u72b6\u6001\u7684\u5b58\u5728\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u7406\u89e3\u6269\u6563\u6a21\u578b\u4e2d\u8bb0\u5fc6\u4e0e\u6cdb\u5316\u7684\u52a8\u6001\u8fc7\u7a0b\uff0c\u901a\u8fc7\u8054\u60f3\u8bb0\u5fc6\u7684\u6846\u67b6\u63ed\u793a\u8bad\u7ec3\u6570\u636e\u91cf\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "method": "\u65b9\u6cd5\u662f\u5c06\u6269\u6563\u6a21\u578b\u7684\u8bad\u7ec3\u9636\u6bb5\u89c6\u4e3a\u8bb0\u5fc6\u7f16\u7801\uff08\u5b58\u50a8\u8bad\u7ec3\u6570\u636e\uff09\uff0c\u751f\u6210\u9636\u6bb5\u89c6\u4e3a\u8bb0\u5fc6\u68c0\u7d22\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u865a\u5047\u72b6\u6001\u7684\u5b58\u5728\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u5728\u5c0f\u6570\u636e\u96c6\u4e0b\u6269\u6563\u6a21\u578b\u8868\u73b0\u51fa\u5f3a\u8bb0\u5fc6\u6027\uff0c\u800c\u5927\u6570\u636e\u96c6\u4e0b\u5219\u6d8c\u73b0\u65b0\u7684\u5438\u5f15\u5b50\u72b6\u6001\uff0c\u8fb9\u754c\u5904\u51fa\u73b0\u865a\u5047\u72b6\u6001\u3002", "conclusion": "\u7ed3\u8bba\u662f\u6269\u6563\u6a21\u578b\u7684\u8bb0\u5fc6-\u6cdb\u5316\u73b0\u8c61\u53ef\u901a\u8fc7\u8054\u60f3\u8bb0\u5fc6\u7684\u89c6\u89d2\u7406\u89e3\uff0c\u5e76\u9884\u6d4b\u548c\u9a8c\u8bc1\u4e86\u865a\u5047\u72b6\u6001\u7684\u5b58\u5728\u3002", "keywords": "\u6269\u6563\u6a21\u578b,\u8054\u60f3\u8bb0\u5fc6,\u865a\u5047\u72b6\u6001,\u8bb0\u5fc6-\u6cdb\u5316,\u5438\u5f15\u5b50"}}
{"id": "2505.22017", "pdf": "https://arxiv.org/pdf/2505.22017", "abs": "https://arxiv.org/abs/2505.22017", "authors": ["Siqi Fan", "Peng Han", "Shuo Shang", "Yequan Wang", "Aixin Sun"], "title": "CoThink: Token-Efficient Reasoning via Instruct Models Guiding Reasoning Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) benefit from increased test-time compute, a\nphenomenon known as test-time scaling. However, reasoning-optimized models\noften overthink even simple problems, producing excessively verbose outputs and\nleading to low token efficiency. By comparing these models with equally sized\ninstruct models, we identify two key causes of this verbosity: (1)\nreinforcement learning reduces the information density of forward reasoning,\nand (2) backward chain-of thought training encourages redundant and often\nunnecessary verification steps. Since LLMs cannot assess the difficulty of a\ngiven problem, they tend to apply the same cautious reasoning strategy across\nall tasks, resulting in inefficient overthinking. To address this, we propose\nCoThink, an embarrassingly simple pipeline: an instruct model first drafts a\nhigh-level solution outline; a reasoning model then works out the solution. We\nobserve that CoThink enables dynamic adjustment of reasoning depth based on\ninput difficulty. Evaluated with three reasoning models DAPO, DeepSeek-R1, and\nQwQ on three datasets GSM8K, MATH500, and AIME24, CoThink reduces total token\ngeneration by 22.3% while maintaining pass@1 accuracy within a 0.42% margin on\naverage. With reference to the instruct model, we formally define reasoning\nefficiency and observe a potential reasoning efficiency scaling law in LLMs.", "AI": {"tldr": "CoThink\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\uff0c\u51cf\u5c1122.3%\u7684token\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u63a8\u7406\u4f18\u5316\u6a21\u578b\u5728\u5904\u7406\u7b80\u5355\u95ee\u9898\u65f6\u8fc7\u5ea6\u601d\u8003\u5bfc\u81f4\u7684token\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "method": "\u63d0\u51faCoThink\u6d41\u7a0b\uff1a\u6307\u4ee4\u6a21\u578b\u751f\u6210\u9ad8\u5c42\u89e3\u51b3\u65b9\u6848\u6846\u67b6\uff0c\u63a8\u7406\u6a21\u578b\u8865\u5145\u7ec6\u8282\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\uff0ctoken\u751f\u6210\u51cf\u5c1122.3%\uff0c\u51c6\u786e\u7387\u4e0b\u964d\u4ec50.42%\u3002", "conclusion": "CoThink\u80fd\u52a8\u6001\u5339\u914d\u8f93\u5165\u96be\u5ea6\u4e0e\u63a8\u7406\u6df1\u5ea6\uff0c\u5e76\u63d0\u51fa\u6f5c\u5728\u7684\u63a8\u7406\u6548\u7387\u7f29\u653e\u5b9a\u5f8b\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u63a8\u7406\u6548\u7387\uff0ctoken\u6548\u7387\uff0cCoThink\uff0c\u52a8\u6001\u8c03\u6574"}}
{"id": "2505.21783", "pdf": "https://arxiv.org/pdf/2505.21783", "abs": "https://arxiv.org/abs/2505.21783", "authors": ["Hyunsik Yun"], "title": "P-DROP: Poisson-Based Dropout for Graph Neural Networks", "categories": ["cs.LG"], "comment": "10 pages, 9 figures", "summary": "Over-smoothing remains a major challenge in Graph Neural Networks (GNNs),\nwhere repeated message passing causes node representations to converge and lose\ndiscriminative power. To address this, we propose a novel node selection\nstrategy based on Poisson processes, introducing stochastic but structure-aware\nupdates. Specifically, we equip each node with an independent Poisson clock,\nenabling asynchronous and localized updates that preserve structural diversity.\nWe explore two applications of this strategy: as a replacement for\ndropout-based regularization and as a dynamic subgraph training scheme.\nExperimental results on standard benchmarks (Cora, Citeseer, Pubmed)\ndemonstrate that our Poisson-based method yields competitive or improved\naccuracy compared to traditional Dropout, DropEdge, and DropNode approaches,\nparticularly in later training stages.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6cca\u677e\u8fc7\u7a0b\u7684\u8282\u70b9\u9009\u62e9\u7b56\u7565\uff0c\u901a\u8fc7\u5f02\u6b65\u548c\u5c40\u90e8\u5316\u66f4\u65b0\u6765\u89e3\u51b3GNN\u4e2d\u7684\u8fc7\u5e73\u6ed1\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u56e0GNN\u4e2d\u91cd\u590d\u6d88\u606f\u4f20\u9012\u5bfc\u81f4\u8282\u70b9\u8868\u5f81\u6536\u655b\u4e14\u4e27\u5931\u533a\u5206\u6027\uff0c\u4e3a\u89e3\u51b3\u8fc7\u5e73\u6ed1\u95ee\u9898\u800c\u63d0\u51fa\u65b0\u578b\u8282\u70b9\u9009\u62e9\u7b56\u7565\u3002", "method": "\u4e3a\u6bcf\u4e2a\u8282\u70b9\u914d\u5907\u72ec\u7acb\u6cca\u677e\u65f6\u949f\uff0c\u5b9e\u73b0\u5f02\u6b65\u548c\u5c40\u90e8\u5316\u66f4\u65b0\uff0c\u5e94\u7528\u4e8e\u6b63\u5219\u5316\u548c\u52a8\u6001\u5b50\u56fe\u8bad\u7ec3\u3002", "result": "\u5728Cora\u3001Citeseer\u548cPubmed\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u540e\u671f\u8bad\u7ec3\u9636\u6bb5\u8868\u73b0\u4f18\u4e8eDropout\u3001DropEdge\u548cDropNode\u3002", "conclusion": "\u57fa\u4e8e\u6cca\u677e\u8fc7\u7a0b\u7684\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u8fc7\u5e73\u6ed1\u95ee\u9898\uff0c\u63d0\u5347\u4e86GNN\u6027\u80fd\u3002", "keywords": "GNN, \u8fc7\u5e73\u6ed1, \u6cca\u677e\u8fc7\u7a0b, \u8282\u70b9\u9009\u62e9, \u5f02\u6b65\u66f4\u65b0"}}
{"id": "2505.22018", "pdf": "https://arxiv.org/pdf/2505.22018", "abs": "https://arxiv.org/abs/2505.22018", "authors": ["Ruicheng Yin", "Xuan Gao", "Changze Lv", "Xiaohua Wang", "Xiaoqing Zheng", "Xuanjing Huang"], "title": "Improving Continual Pre-training Through Seamless Data Packing", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Continual pre-training has demonstrated significant potential in enhancing\nmodel performance, particularly in domain-specific scenarios. The most common\napproach for packing data before continual pre-training involves concatenating\ninput texts and splitting them into fixed-length sequences. While\nstraightforward and efficient, this method often leads to excessive truncation\nand context discontinuity, which can hinder model performance. To address these\nissues, we explore the potential of data engineering to enhance continual\npre-training, particularly its impact on model performance and efficiency. We\npropose Seamless Packing (SP), a novel data packing strategy aimed at\npreserving contextual information more effectively and enhancing model\nperformance. Our approach employs a sliding window technique in the first stage\nthat synchronizes overlapping tokens across consecutive sequences, ensuring\nbetter continuity and contextual coherence. In the second stage, we adopt a\nFirst-Fit-Decreasing algorithm to pack shorter texts into bins slightly larger\nthan the target sequence length, thereby minimizing padding and truncation.\nEmpirical evaluations across various model architectures and corpus domains\ndemonstrate the effectiveness of our method, outperforming baseline method in\n99% of all settings. Code is available at\nhttps://github.com/Infernus-WIND/Seamless-Packing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a'\u65e0\u7f1d\u6253\u5305'\uff08SP\uff09\u7684\u65b0\u6570\u636e\u6253\u5305\u7b56\u7565\uff0c\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u548cFFD\u7b97\u6cd5\u51cf\u5c11\u622a\u65ad\u548c\u586b\u5145\uff0c\u63d0\u5347\u9884\u8bad\u7ec3\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u56fa\u5b9a\u957f\u5ea6\u5e8f\u5217\u7684\u6570\u636e\u6253\u5305\u65b9\u6cd5\u5bfc\u81f4\u622a\u65ad\u548c\u4e0a\u4e0b\u6587\u4e0d\u8fde\u8d2f\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u6027\u80fd\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u6ed1\u52a8\u7a97\u53e3\u786e\u4fdd\u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027\uff1b2\uff09FFD\u7b97\u6cd5\u4f18\u5316\u586b\u5145\u548c\u622a\u65ad\u95ee\u9898\u3002", "result": "\u572899%\u7684\u5b9e\u9a8c\u573a\u666f\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SP\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027\u548c\u6570\u636e\u6548\u7387\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "keywords": "\u6301\u7eed\u9884\u8bad\u7ec3, \u6570\u636e\u6253\u5305, \u6ed1\u52a8\u7a97\u53e3, FFD\u7b97\u6cd5, \u6a21\u578b\u6027\u80fd"}}
{"id": "2505.21785", "pdf": "https://arxiv.org/pdf/2505.21785", "abs": "https://arxiv.org/abs/2505.21785", "authors": ["Yana Veitsman", "Mayank Jobanputra", "Yash Sarrof", "Aleksandra Bakalova", "Vera Demberg", "Ellie Pavlick", "Michael Hahn"], "title": "Born a Transformer -- Always a Transformer?", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformers have theoretical limitations in modeling certain\nsequence-to-sequence tasks, yet it remains largely unclear if these limitations\nplay a role in large-scale pretrained LLMs, or whether LLMs might effectively\novercome these constraints in practice due to the scale of both the models\nthemselves and their pretraining data. We explore how these architectural\nconstraints manifest after pretraining, by studying a family of\n$\\textit{retrieval}$ and $\\textit{copying}$ tasks inspired by Liu et al.\n[2024]. We use the recently proposed C-RASP framework for studying length\ngeneralization [Huang et al., 2025b] to provide guarantees for each of our\nsettings. Empirically, we observe an $\\textit{induction-versus-anti-induction}$\nasymmetry, where pretrained models are better at retrieving tokens to the right\n(induction) rather than the left (anti-induction) of a query token. This\nasymmetry disappears upon targeted fine-tuning if length-generalization is\nguaranteed by theory. Mechanistic analysis reveals that this asymmetry is\nconnected to the differences in the strength of induction versus anti-induction\ncircuits within pretrained Transformers. We validate our findings through\npractical experiments on real-world tasks demonstrating reliability risks. Our\nresults highlight that pretraining selectively enhances certain Transformer\ncapabilities, but does not overcome fundamental length-generalization limits.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u514b\u670d\u4e86Transformer\u5728\u5e8f\u5217\u4efb\u52a1\u4e0a\u7684\u7406\u8bba\u9650\u5236\uff0c\u53d1\u73b0\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u5b58\u5728\u5f52\u7eb3\u4e0e\u53cd\u5f52\u7eb3\u7684\u4e0d\u5bf9\u79f0\u6027\u3002", "motivation": "\u7814\u7a76\u9884\u8bad\u7ec3LLMs\u662f\u5426\u80fd\u514b\u670dTransformer\u67b6\u6784\u5728\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u7406\u8bba\u9650\u5236\uff0c\u5c24\u5176\u662f\u957f\u5ea6\u6cdb\u5316\u95ee\u9898\u3002", "method": "\u5229\u7528C-RASP\u6846\u67b6\u7814\u7a76\u957f\u5ea6\u6cdb\u5316\uff0c\u8bbe\u8ba1\u68c0\u7d22\u548c\u590d\u5236\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u5206\u6790\u6a21\u578b\u8868\u73b0\u3002", "result": "\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5f52\u7eb3\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u53cd\u5f52\u7eb3\u4efb\u52a1\uff0c\u4f46\u901a\u8fc7\u9488\u5bf9\u6027\u5fae\u8c03\u53ef\u4ee5\u6d88\u9664\u8fd9\u79cd\u4e0d\u5bf9\u79f0\u6027\u3002", "conclusion": "\u9884\u8bad\u7ec3\u867d\u80fd\u589e\u5f3aTransformer\u7684\u67d0\u4e9b\u80fd\u529b\uff0c\u4f46\u65e0\u6cd5\u7a81\u7834\u957f\u5ea6\u6cdb\u5316\u7684\u6839\u672c\u9650\u5236\u3002", "keywords": "Transformer, LLMs, \u9884\u8bad\u7ec3, \u957f\u5ea6\u6cdb\u5316, \u68c0\u7d22\u4efb\u52a1, \u590d\u5236\u4efb\u52a1"}}
{"id": "2505.21527", "pdf": "https://arxiv.org/pdf/2505.21527", "abs": "https://arxiv.org/abs/2505.21527", "authors": ["Jianheng Zhuo", "Yifan Yang", "Yiwen Shao", "Yong Xu", "Dong Yu", "Kai Yu", "Xie Chen"], "title": "VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled data and Large-Scale Speech Pretraining", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": null, "summary": "Automatic speech recognition (ASR) has made remarkable progress but heavily\nrelies on large-scale labeled data, which is scarce for low-resource languages\nlike Vietnamese. While existing systems such as Whisper, USM, and MMS achieve\npromising performance, their efficacy remains inadequate in terms of training\ncosts, latency, and accessibility. To address these issues, we propose VietASR,\na novel ASR training pipeline that leverages vast amounts of unlabeled data and\na small set of labeled data. Through multi-iteration ASR-biased self-supervised\nlearning on a large-scale unlabeled dataset, VietASR offers a cost-effective\nand practical solution for enhancing ASR performance. Experiments demonstrate\nthat pre-training on 70,000-hour unlabeled data and fine-tuning on merely\n50-hour labeled data yield a lightweight but powerful ASR model. It outperforms\nWhisper Large-v3 and commercial ASR systems on real-world data. Our code and\nmodels will be open-sourced to facilitate research in low-resource ASR.", "AI": {"tldr": "\u63d0\u51faVietASR\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5229\u7528\u65e0\u6807\u6ce8\u6570\u636e\u548c\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684ASR\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u8d8a\u5357\u8bed\uff09\u7684\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4f20\u7edfASR\u7cfb\u7edf\uff08\u5982Whisper\u7b49\uff09\u5728\u8bad\u7ec3\u6210\u672c\u3001\u5ef6\u8fdf\u548c\u53ef\u8bbf\u95ee\u6027\u4e0a\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u591a\u8f6eASR\u504f\u7f6e\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5728\u5927\u89c4\u6a21\u65e0\u6807\u6ce8\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\uff0c\u518d\u7528\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u5fae\u8c03\u3002", "result": "\u57287\u4e07\u5c0f\u65f6\u65e0\u6807\u6ce8\u6570\u636e\u548c50\u5c0f\u65f6\u6807\u6ce8\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u4f18\u4e8eWhisper Large-v3\u548c\u5546\u4e1aASR\u7cfb\u7edf\u3002", "conclusion": "VietASR\u4e3a\u4f4e\u8d44\u6e90ASR\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5f00\u6e90\u4ee3\u7801\u548c\u6a21\u578b\u3002", "keywords": "\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b,\u4f4e\u8d44\u6e90\u8bed\u8a00,\u81ea\u76d1\u7763\u5b66\u4e60,VietASR"}}
{"id": "2505.22019", "pdf": "https://arxiv.org/pdf/2505.22019", "abs": "https://arxiv.org/abs/2505.22019", "authors": ["Qiuchen Wang", "Ruixue Ding", "Yu Zeng", "Zehui Chen", "Lin Chen", "Shihang Wang", "Pengjun Xie", "Fei Huang", "Feng Zhao"], "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Effectively retrieving, reasoning and understanding visually rich information\nremains a challenge for RAG methods. Traditional text-based methods cannot\nhandle visual-related information. On the other hand, current vision-based RAG\napproaches are often limited by fixed pipelines and frequently struggle to\nreason effectively due to the insufficient activation of the fundamental\ncapabilities of models. As RL has been proven to be beneficial for model\nreasoning, we introduce VRAG-RL, a novel RL framework tailored for complex\nreasoning across visually rich information. With this framework, VLMs interact\nwith search engines, autonomously sampling single-turn or multi-turn reasoning\ntrajectories with the help of visual perception tokens and undergoing continual\noptimization based on these samples. Our approach highlights key limitations of\nRL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely\nincorporate images into the context, leading to insufficient reasoning token\nallocation and neglecting visual-specific perception; and (ii) When models\ninteract with search engines, their queries often fail to retrieve relevant\ninformation due to the inability to articulate requirements, thereby leading to\nsuboptimal performance. To address these challenges, we define an action space\ntailored for visually rich inputs, with actions including cropping and scaling,\nallowing the model to gather information from a coarse-to-fine perspective.\nFurthermore, to bridge the gap between users' original inquiries and the\nretriever, we employ a simple yet effective reward that integrates query\nrewriting and retrieval performance with a model-based reward. Our VRAG-RL\noptimizes VLMs for RAG tasks using specially designed RL strategies, aligning\nthe model with real-world applications. The code is available at\n\\hyperlink{https://github.com/Alibaba-NLP/VRAG}{https://github.com/Alibaba-NLP/VRAG}.", "AI": {"tldr": "VRAG-RL\u662f\u4e00\u4e2a\u9488\u5bf9\u89c6\u89c9\u4fe1\u606f\u68c0\u7d22\u4f18\u5316\u7684\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u4e3b\u91c7\u6837\u4e0e\u4f18\u5316\u89e3\u51b3\u89c6\u89c9\u63a8\u7406\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6587\u672c\u68c0\u7d22\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u89c6\u89c9\u4fe1\u606f\uff0c\u800c\u73b0\u6709\u89c6\u89c9RAG\u65b9\u6cd5\u56e0\u56fa\u5b9a\u6d41\u7a0b\u548c\u6a21\u578b\u80fd\u529b\u6fc0\u6d3b\u4e0d\u8db3\u5728\u63a8\u7406\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u8bbe\u8ba1\u4e86\u9488\u5bf9\u89c6\u89c9\u8f93\u5165\u7684\u88c1\u526a\u548c\u7f29\u653e\u52a8\u4f5c\u7a7a\u95f4\uff0c\u7ed3\u5408\u67e5\u8be2\u91cd\u5199\u548c\u68c0\u7d22\u6027\u80fd\u7684\u5956\u52b1\u673a\u5236\u3002", "result": "\u4f18\u5316\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "VRAG-RL\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u4fe1\u606f\u7684\u68c0\u7d22\u548c\u63a8\u7406\u80fd\u529b\u3002", "keywords": "VRAG, \u5f3a\u5316\u5b66\u4e60, \u89c6\u89c9\u4fe1\u606f\u68c0\u7d22, \u89c6\u89c9\u8bed\u8a00\u6a21\u578b"}}
{"id": "2505.21790", "pdf": "https://arxiv.org/pdf/2505.21790", "abs": "https://arxiv.org/abs/2505.21790", "authors": ["Hilal Asi", "Vinod Raman", "Kunal Talwar"], "title": "Faster Rates for Private Adversarial Bandits", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted to ICML 2025", "summary": "We design new differentially private algorithms for the problems of\nadversarial bandits and bandits with expert advice. For adversarial bandits, we\ngive a simple and efficient conversion of any non-private bandit algorithm to a\nprivate bandit algorithm. Instantiating our conversion with existing\nnon-private bandit algorithms gives a regret upper bound of\n$O\\left(\\frac{\\sqrt{KT}}{\\sqrt{\\epsilon}}\\right)$, improving upon the existing\nupper bound $O\\left(\\frac{\\sqrt{KT \\log(KT)}}{\\epsilon}\\right)$ for all\n$\\epsilon \\leq 1$. In particular, our algorithms allow for sublinear expected\nregret even when $\\epsilon \\leq \\frac{1}{\\sqrt{T}}$, establishing the first\nknown separation between central and local differential privacy for this\nproblem. For bandits with expert advice, we give the first differentially\nprivate algorithms, with expected regret\n$O\\left(\\frac{\\sqrt{NT}}{\\sqrt{\\epsilon}}\\right),\nO\\left(\\frac{\\sqrt{KT\\log(N)}\\log(KT)}{\\epsilon}\\right)$, and\n$\\tilde{O}\\left(\\frac{N^{1/6}K^{1/2}T^{2/3}\\log(NT)}{\\epsilon ^{1/3}} +\n\\frac{N^{1/2}\\log(NT)}{\\epsilon}\\right)$, where $K$ and $N$ are the number of\nactions and experts respectively. These rates allow us to get sublinear regret\nfor different combinations of small and large $K, N$ and $\\epsilon.$", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u5bf9\u6297\u6027\u8001\u864e\u673a\u548c\u4e13\u5bb6\u5efa\u8bae\u8001\u864e\u673a\u7684\u65b0\u5dee\u5206\u9690\u79c1\u7b97\u6cd5\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u8bc1\u660e\u4e86\u4e2d\u5fc3\u4e0e\u5c40\u90e8\u5dee\u5206\u9690\u79c1\u7684\u533a\u522b\u3002", "motivation": "\u7814\u7a76\u5dee\u5206\u9690\u79c1\u5728\u8001\u864e\u673a\u95ee\u9898\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5bf9\u6297\u6027\u8001\u864e\u673a\u548c\u4e13\u5bb6\u5efa\u8bae\u8001\u864e\u673a\u7684\u6027\u80fd\u63d0\u5347\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7b97\u6cd5\u7684\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u901a\u7528\u8f6c\u6362\u65b9\u6cd5\uff0c\u5c06\u975e\u9690\u79c1\u8001\u864e\u673a\u7b97\u6cd5\u8f6c\u6362\u4e3a\u9690\u79c1\u7b97\u6cd5\uff0c\u5e76\u5b9e\u4f8b\u5316\u73b0\u6709\u7b97\u6cd5\u4ee5\u4f18\u5316\u540e\u6094\u4e0a\u754c\u3002", "result": "\u5bf9\u6297\u6027\u8001\u864e\u673a\u7684\u540e\u6094\u4e0a\u754c\u63d0\u5347\u81f3$O\\left(\\frac{\\sqrt{KT}}{\\sqrt{\\epsilon}}\\right)$\uff0c\u4e13\u5bb6\u5efa\u8bae\u8001\u864e\u673a\u9996\u6b21\u5b9e\u73b0\u5dee\u5206\u9690\u79c1\uff0c\u4e14\u5728\u4e0d\u540c\u53c2\u6570\u7ec4\u5408\u4e0b\u5747\u5b9e\u73b0\u4e9a\u7ebf\u6027\u540e\u6094\u3002", "conclusion": "\u65b0\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u9996\u6b21\u8bc1\u660e\u4e2d\u5fc3\u4e0e\u5c40\u90e8\u5dee\u5206\u9690\u79c1\u5728\u8001\u864e\u673a\u95ee\u9898\u4e2d\u7684\u5dee\u5f02\uff0c\u5e76\u6269\u5c55\u4e86\u5dee\u5206\u9690\u79c1\u5728\u590d\u6742\u573a\u666f\u7684\u5e94\u7528\u3002", "keywords": "\u5dee\u5206\u9690\u79c1\u3001\u5bf9\u6297\u6027\u8001\u864e\u673a\u3001\u4e13\u5bb6\u5efa\u8bae\u8001\u864e\u673a\u3001\u540e\u6094\u4e0a\u754c\u3001\u4e9a\u7ebf\u6027\u540e\u6094"}}
{"id": "2505.21528", "pdf": "https://arxiv.org/pdf/2505.21528", "abs": "https://arxiv.org/abs/2505.21528", "authors": ["Mokai Pan", "Kaizhen Zhu", "Yuexin Ma", "Yanwei Fu", "Jingyi Yu", "Jingya Wang", "Ye Shi"], "title": "UniDB++: Fast Sampling of Unified Diffusion Bridge", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Diffusion Bridges enable transitions between arbitrary distributions, with\nthe Unified Diffusion Bridge (UniDB) framework achieving high-fidelity image\ngeneration via a Stochastic Optimal Control (SOC) formulation. However, UniDB's\nreliance on iterative Euler sampling methods results in slow, computationally\nexpensive inference, while existing acceleration techniques for diffusion or\ndiffusion bridge models fail to address its unique challenges: missing terminal\nmean constraints and SOC-specific penalty coefficients in its SDEs. We present\nUniDB++, a training-free sampling algorithm that significantly improves upon\nthese limitations. The method's key advancement comes from deriving exact\nclosed-form solutions for UniDB's reverse-time SDEs, effectively reducing the\nerror accumulation inherent in Euler approximations and enabling high-quality\ngeneration with up to 20$\\times$ fewer sampling steps. This method is further\ncomplemented by replacing conventional noise prediction with a more stable data\nprediction model, along with an SDE-Corrector mechanism that maintains\nperceptual quality for low-step regimes (5-10 steps). Additionally, we\ndemonstrate that UniDB++ aligns with existing diffusion bridge acceleration\nmethods by evaluating their update rules, and UniDB++ can recover DBIMs as\nspecial cases under some theoretical conditions. Experiments demonstrate\nUniDB++'s state-of-the-art performance in image restoration tasks,\noutperforming Euler-based methods in fidelity and speed while reducing\ninference time significantly. This work bridges the gap between theoretical\ngenerality and practical efficiency in SOC-driven diffusion bridge models. Our\ncode is available at https://github.com/2769433owo/UniDB-plusplus.", "AI": {"tldr": "UniDB++\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u91c7\u6837\u7b97\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdbUniDB\u6846\u67b6\uff0c\u51cf\u5c11\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u63d0\u9ad8\u4e86\u56fe\u50cf\u751f\u6210\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u4f4e\u6b65\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u8272\u3002", "motivation": "UniDB\u7684\u8fed\u4ee3\u91c7\u6837\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\uff0c\u4e14\u73b0\u6709\u52a0\u901f\u6280\u672f\u65e0\u6cd5\u89e3\u51b3\u5176\u72ec\u7279\u6311\u6218\uff0c\u5982\u7ec8\u7aef\u5747\u503c\u7ea6\u675f\u7f3a\u5931\u548cSOC\u7279\u5b9a\u7684\u60e9\u7f5a\u7cfb\u6570\u3002", "method": "UniDB++\u901a\u8fc7\u63a8\u5bfc\u53cd\u5411\u65f6\u95f4SDE\u7684\u95ed\u5f0f\u89e3\uff0c\u51cf\u5c11\u8bef\u5dee\u79ef\u7d2f\uff0c\u5e76\u5f15\u5165\u6570\u636e\u9884\u6d4b\u6a21\u578b\u548cSDE\u6821\u6b63\u673a\u5236\uff0c\u63d0\u5347\u7a33\u5b9a\u6027\u548c\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u663e\u793aUniDB++\u5728\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u4e2d\u6027\u80fd\u9886\u5148\uff0c\u751f\u6210\u901f\u5ea6\u63d0\u9ad820\u500d\uff0c\u4e14\u4f4e\u6b65\u6570\u4e0b\u4ecd\u4fdd\u6301\u9ad8\u8d28\u91cf\u3002", "conclusion": "UniDB++\u586b\u8865\u4e86SOC\u9a71\u52a8\u6269\u6563\u6865\u6a21\u578b\u7684\u7406\u8bba\u901a\u7528\u6027\u548c\u5b9e\u9645\u6548\u7387\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "keywords": "Diffusion Bridges, Stochastic Optimal Control, Image Generation, Sampling Efficiency"}}
{"id": "2505.22037", "pdf": "https://arxiv.org/pdf/2505.22037", "abs": "https://arxiv.org/abs/2505.22037", "authors": ["Jingyu Zhang", "Ahmed Elgohary", "Xiawei Wang", "A S M Iftekhar", "Ahmed Magooda", "Benjamin Van Durme", "Daniel Khashabi", "Kyle Jackson"], "title": "Jailbreak Distillation: Renewable Safety Benchmarking", "categories": ["cs.CL", "cs.CR", "cs.SE"], "comment": "Project page: https://aka.ms/jailbreak-distillation", "summary": "Large language models (LLMs) are rapidly deployed in critical applications,\nraising urgent needs for robust safety benchmarking. We propose Jailbreak\nDistillation (JBDistill), a novel benchmark construction framework that\n\"distills\" jailbreak attacks into high-quality and easily-updatable safety\nbenchmarks. JBDistill utilizes a small set of development models and existing\njailbreak attack algorithms to create a candidate prompt pool, then employs\nprompt selection algorithms to identify an effective subset of prompts as\nsafety benchmarks. JBDistill addresses challenges in existing safety\nevaluation: the use of consistent evaluation prompts across models ensures fair\ncomparisons and reproducibility. It requires minimal human effort to rerun the\nJBDistill pipeline and produce updated benchmarks, alleviating concerns on\nsaturation and contamination. Extensive experiments demonstrate our benchmarks\ngeneralize robustly to 13 diverse evaluation models held out from benchmark\nconstruction, including proprietary, specialized, and newer-generation LLMs,\nsignificantly outperforming existing safety benchmarks in effectiveness while\nmaintaining high separability and diversity. Our framework thus provides an\neffective, sustainable, and adaptable solution for streamlining safety\nevaluation.", "AI": {"tldr": "Jailbreak Distillation (JBDistill) is a framework that transforms jailbreak attacks into high-quality, updatable safety benchmarks for large language models, ensuring fair and reproducible evaluations with minimal human effort.", "motivation": "The rapid deployment of large language models (LLMs) in critical applications necessitates robust safety benchmarking to address vulnerabilities like jailbreak attacks. Existing methods lack consistency and adaptability, prompting the need for a sustainable solution.", "method": "JBDistill utilizes development models and jailbreak attacks to create a candidate prompt pool, then applies prompt selection algorithms to identify effective subsets for benchmarks, ensuring fair comparisons and reproducibility.", "result": "The benchmarks generalize robustly across 13 diverse LLMs, including proprietary and newer-generation models, outperforming existing benchmarks in effectiveness while maintaining high separability and diversity.", "conclusion": "JBDistill offers an effective, adaptable, and sustainable solution for safety evaluation, addressing challenges like saturation and contamination in existing methods.", "keywords": "large language models, safety benchmarking, jailbreak attacks, prompt selection, reproducibility"}}
{"id": "2505.21792", "pdf": "https://arxiv.org/pdf/2505.21792", "abs": "https://arxiv.org/abs/2505.21792", "authors": ["Yuanzhe Peng", "Jieming Bian", "Lei Wang", "Yin Huang", "Jie Xu"], "title": "Multimodal Federated Learning: A Survey through the Lens of Different FL Paradigms", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multimodal Federated Learning (MFL) lies at the intersection of two pivotal\nresearch areas: leveraging complementary information from multiple modalities\nto improve downstream inference performance and enabling distributed training\nto enhance efficiency and preserve privacy. Despite the growing interest in\nMFL, there is currently no comprehensive taxonomy that organizes MFL through\nthe lens of different Federated Learning (FL) paradigms. This perspective is\nimportant because multimodal data introduces distinct challenges across various\nFL settings. These challenges, including modality heterogeneity, privacy\nheterogeneity, and communication inefficiency, are fundamentally different from\nthose encountered in traditional unimodal or non-FL scenarios. In this paper,\nwe systematically examine MFL within the context of three major FL paradigms:\nhorizontal FL (HFL), vertical FL (VFL), and hybrid FL. For each paradigm, we\npresent the problem formulation, review representative training algorithms, and\nhighlight the most prominent challenge introduced by multimodal data in\ndistributed settings. We also discuss open challenges and provide insights for\nfuture research. By establishing this taxonomy, we aim to uncover the novel\nchallenges posed by multimodal data from the perspective of different FL\nparadigms and to offer a new lens through which to understand and advance the\ndevelopment of MFL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u8054\u90a6\u5b66\u4e60\uff08MFL\uff09\u7684\u5206\u7c7b\u6cd5\uff0c\u7814\u7a76\u4e86\u5728\u6c34\u5e73\u8054\u90a6\u5b66\u4e60\uff08HFL\uff09\u3001\u5782\u76f4\u8054\u90a6\u5b66\u4e60\uff08VFL\uff09\u548c\u6df7\u5408\u8054\u90a6\u5b66\u4e60\uff08Hybrid FL\uff09\u4e09\u79cd\u4e3b\u8981\u8303\u5f0f\u4e0b\u7684\u95ee\u9898\u3001\u7b97\u6cd5\u548c\u6311\u6218\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u591a\u6a21\u6001\u6570\u636e\u5728\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4e2d\u5e26\u6765\u4e86\u6a21\u6001\u5f02\u8d28\u6027\u3001\u9690\u79c1\u5f02\u8d28\u6027\u548c\u901a\u4fe1\u6548\u7387\u7b49\u65b0\u6311\u6218\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u7684\u5206\u7c7b\u6cd5\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u4e09\u79cdFL\u8303\u5f0f\uff08HFL\u3001VFL\u3001\u6df7\u5408FL\uff09\uff0c\u603b\u7ed3\u4e86\u6bcf\u79cd\u8303\u5f0f\u7684\u95ee\u9898\u5b9a\u4e49\u3001\u4ee3\u8868\u6027\u7b97\u6cd5\u548c\u7531\u591a\u6a21\u6001\u6570\u636e\u5f15\u53d1\u7684\u4e3b\u8981\u6311\u6218\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u5206\u7c7b\u6cd5\uff0c\u63ed\u793a\u4e86\u591a\u6a21\u6001\u6570\u636e\u5728\u4e0d\u540cFL\u8303\u5f0f\u4e0b\u7684\u72ec\u7279\u6311\u6218\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "conclusion": "\u901a\u8fc7\u5efa\u7acb\u8fd9\u4e00\u5206\u7c7b\u6cd5\uff0c\u672c\u6587\u4e3a\u7406\u89e3\u548c\u63a8\u8fdbMFL\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "keywords": "\u591a\u6a21\u6001\u8054\u90a6\u5b66\u4e60\u3001\u5206\u7c7b\u6cd5\u3001\u8054\u90a6\u5b66\u4e60\u8303\u5f0f\u3001\u6a21\u6001\u5f02\u8d28\u6027\u3001\u9690\u79c1\u4fdd\u62a4"}}
{"id": "2505.21530", "pdf": "https://arxiv.org/pdf/2505.21530", "abs": "https://arxiv.org/abs/2505.21530", "authors": ["Xuhang Chen", "Zhuo Li", "Yanyan Shen", "Mufti Mahmud", "Hieu Pham", "Chi-Man Pun", "Shuqiang Wang"], "title": "High-Fidelity Functional Ultrasound Reconstruction via A Visual Auto-Regressive Framework", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Functional ultrasound (fUS) imaging provides exceptional spatiotemporal\nresolution for neurovascular mapping, yet its practical application is\nsignificantly hampered by critical challenges. Foremost among these are data\nscarcity, arising from ethical considerations and signal degradation through\nthe cranium, which collectively limit dataset diversity and compromise the\nfairness of downstream machine learning models.", "AI": {"tldr": "\u529f\u80fd\u8d85\u58f0\uff08fUS\uff09\u6210\u50cf\u5728\u795e\u7ecf\u8840\u7ba1\u6620\u5c04\u4e2d\u63d0\u4f9b\u4e86\u51fa\u8272\u7684\u65f6\u7a7a\u5206\u8fa8\u7387\uff0c\u4f46\u5176\u5b9e\u9645\u5e94\u7528\u53d7\u5230\u6570\u636e\u7a00\u7f3a\u548c\u4fe1\u53f7\u8870\u51cf\u7684\u4e25\u91cd\u9650\u5236\u3002", "motivation": "\u89e3\u51b3\u529f\u80fd\u8d85\u58f0\u6210\u50cf\u5728\u795e\u7ecf\u8840\u7ba1\u6620\u5c04\u4e2d\u56e0\u6570\u636e\u7a00\u7f3a\u548c\u4fe1\u53f7\u8870\u51cf\u5bfc\u81f4\u7684\u6570\u636e\u96c6\u591a\u6837\u6027\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u516c\u5e73\u6027\u95ee\u9898\u3002", "method": "\u672a\u660e\u786e\u63d0\u53ca\u5177\u4f53\u65b9\u6cd5\uff0c\u4f46\u6697\u793a\u9700\u8981\u5e94\u5bf9\u6570\u636e\u7a00\u7f3a\u548c\u4fe1\u53f7\u8870\u51cf\u7684\u6311\u6218\u3002", "result": "\u672a\u63d0\u4f9b\u5177\u4f53\u7ed3\u679c\uff0c\u4f46\u5f3a\u8c03\u4e86\u73b0\u6709\u6280\u672f\u9762\u4e34\u7684\u9650\u5236\u3002", "conclusion": "\u529f\u80fd\u8d85\u58f0\u6210\u50cf\u5728\u795e\u7ecf\u8840\u7ba1\u6620\u5c04\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u514b\u670d\u6570\u636e\u7a00\u7f3a\u548c\u4fe1\u53f7\u8870\u51cf\u7684\u6311\u6218\u4ee5\u63d0\u9ad8\u5b9e\u7528\u6027\u3002", "keywords": "\u529f\u80fd\u8d85\u58f0\u6210\u50cf, \u795e\u7ecf\u8840\u7ba1\u6620\u5c04, \u6570\u636e\u7a00\u7f3a, \u4fe1\u53f7\u8870\u51cf, \u673a\u5668\u5b66\u4e60"}}
{"id": "2505.22054", "pdf": "https://arxiv.org/pdf/2505.22054", "abs": "https://arxiv.org/abs/2505.22054", "authors": ["Samuel Stucki", "Jan Deriu", "Mark Cieliebak"], "title": "Voice Adaptation for Swiss German", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Submitted to Interspeech", "summary": "This work investigates the performance of Voice Adaptation models for Swiss\nGerman dialects, i.e., translating Standard German text to Swiss German dialect\nspeech. For this, we preprocess a large dataset of Swiss podcasts, which we\nautomatically transcribe and annotate with dialect classes, yielding\napproximately 5000 hours of weakly labeled training material. We fine-tune the\nXTTSv2 model on this dataset and show that it achieves good scores in human and\nautomated evaluations and can correctly render the desired dialect. Our work\nshows a step towards adapting Voice Cloning technology to underrepresented\nlanguages. The resulting model achieves CMOS scores of up to -0.28 and SMOS\nscores of 3.8.", "AI": {"tldr": "\u7814\u7a76\u4e86\u745e\u58eb\u5fb7\u8bed\u65b9\u8a00\u7684\u8bed\u97f3\u9002\u5e94\u6a21\u578b\uff0c\u901a\u8fc7\u9884\u5904\u7406\u5927\u91cf\u745e\u58eb\u64ad\u5ba2\u6570\u636e\u5e76\u5fae\u8c03XTTSv2\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5728\u4eba\u7c7b\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\u4e2d\u7684\u826f\u597d\u8868\u73b0\u3002", "motivation": "\u9002\u5e94\u8bed\u97f3\u514b\u9686\u6280\u672f\u4ee5\u670d\u52a1\u4e8e underrepresented \u8bed\u8a00\uff08\u5982\u745e\u58eb\u5fb7\u8bed\u65b9\u8a00\uff09\u3002", "method": "\u9884\u5904\u7406\u745e\u58eb\u64ad\u5ba2\u6570\u636e\uff0c\u81ea\u52a8\u8f6c\u5f55\u5e76\u6807\u6ce8\u65b9\u8a00\u7c7b\u522b\uff0c\u5fae\u8c03XTTSv2\u6a21\u578b\u3002", "result": "\u5728\u4eba\u7c7b\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\uff0cCMOS\u8bc4\u5206-0.28\uff0cSMOS\u8bc4\u52063.8\u3002", "conclusion": "\u8bc1\u660e\u4e86\u8bed\u97f3\u514b\u9686\u6280\u672f\u53ef\u4ee5\u6210\u529f\u9002\u5e94 underrepresented \u8bed\u8a00\u3002", "keywords": ""}}
{"id": "2505.21800", "pdf": "https://arxiv.org/pdf/2505.21800", "abs": "https://arxiv.org/abs/2505.21800", "authors": ["Stanley Yu", "Vaidehi Bulusu", "Oscar Yasunaga", "Clayton Lau", "Cole Blondin", "Sean O'Brien", "Kevin Zhu", "Vasu Sharma"], "title": "From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit strong conversational abilities but\noften generate falsehoods. Prior work suggests that the truthfulness of simple\npropositions can be represented as a single linear direction in a model's\ninternal activations, but this may not fully capture its underlying geometry.\nIn this work, we extend the concept cone framework, recently introduced for\nmodeling refusal, to the domain of truth. We identify multi-dimensional cones\nthat causally mediate truth-related behavior across multiple LLM families. Our\nresults are supported by three lines of evidence: (i) causal interventions\nreliably flip model responses to factual statements, (ii) learned cones\ngeneralize across model architectures, and (iii) cone-based interventions\npreserve unrelated model behavior. These findings reveal the richer,\nmultidirectional structure governing simple true/false propositions in LLMs and\nhighlight concept cones as a promising tool for probing abstract behaviors.", "AI": {"tldr": "\u7814\u7a76\u8005\u6269\u5c55\u4e86\u6982\u5ff5\u9525\u6846\u67b6\u4ee5\u5efa\u6a21LLM\u4e2d\u7684\u771f\u5b9e\u6027\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u9525\u4f53\u6355\u6349\u5176\u56e0\u679c\u884c\u4e3a\uff0c\u5e76\u63d0\u4f9b\u4e86\u56e0\u679c\u5e72\u9884\u3001\u8de8\u67b6\u6784\u6cdb\u5316\u548c\u884c\u4e3a\u4fdd\u7559\u7684\u8bc1\u636e\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660eLLM\u5728\u5bf9\u8bdd\u4e2d\u5e38\u751f\u6210\u9519\u8bef\u4fe1\u606f\uff0c\u4e14\u771f\u5b9e\u6027\u4ec5\u88ab\u89c6\u4e3a\u5355\u4e00\u7ebf\u65b9\u5411\uff0c\u53ef\u80fd\u672a\u5145\u5206\u53cd\u6620\u5176\u5185\u90e8\u51e0\u4f55\u7ed3\u6784\u3002", "method": "\u6269\u5c55\u6982\u5ff5\u9525\u6846\u67b6\u81f3\u771f\u5b9e\u6027\u9886\u57df\uff0c\u8bc6\u522b\u591a\u7ef4\u5ea6\u9525\u4f53\uff0c\u5e76\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u3001\u8de8\u6a21\u578b\u6cdb\u5316\u6d4b\u8bd5\u548c\u65e0\u5173\u884c\u4e3a\u4fdd\u7559\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u53d1\u73b0\u591a\u7ef4\u5ea6\u9525\u4f53\u53ef\u56e0\u679c\u8c03\u63a7\u6a21\u578b\u5bf9\u4e8b\u5b9e\u9648\u8ff0\u7684\u53cd\u5e94\uff0c\u4e14\u8be5\u673a\u5236\u5728\u4e0d\u540cLLM\u67b6\u6784\u95f4\u5177\u6709\u6cdb\u5316\u6027\uff0c\u540c\u65f6\u4e0d\u5e72\u6270\u5176\u4ed6\u884c\u4e3a\u3002", "conclusion": "LLM\u4e2d\u7684\u771f\u5047\u547d\u9898\u7531\u66f4\u4e30\u5bcc\u7684\u591a\u5411\u7ed3\u6784\u652f\u914d\uff0c\u6982\u5ff5\u9525\u662f\u63a2\u7d22\u62bd\u8c61\u884c\u4e3a\u7684\u6709\u529b\u5de5\u5177\u3002", "keywords": "Large Language Models, truthfulness, concept cones, causal intervention, model generalization"}}
{"id": "2505.21531", "pdf": "https://arxiv.org/pdf/2505.21531", "abs": "https://arxiv.org/abs/2505.21531", "authors": ["Kunhang Li", "Jason Naradowsky", "Yansong Feng", "Yusuke Miyao"], "title": "How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "We explore Large Language Models (LLMs)' human motion knowledge through 3D\navatar control. Given a motion instruction, we prompt LLMs to first generate a\nhigh-level movement plan with consecutive steps (High-level Planning), then\nspecify body part positions in each step (Low-level Planning), which we\nlinearly interpolate into avatar animations as a clear verification lens for\nhuman evaluators. Through carefully designed 20 representative motion\ninstructions with full coverage of basic movement primitives and balanced body\npart usage, we conduct comprehensive evaluations including human assessment of\nboth generated animations and high-level movement plans, as well as automatic\ncomparison with oracle positions in low-level planning. We find that LLMs are\nstrong at interpreting the high-level body movements but struggle with precise\nbody part positioning. While breaking down motion queries into atomic\ncomponents improves planning performance, LLMs have difficulty with multi-step\nmovements involving high-degree-of-freedom body parts. Furthermore, LLMs\nprovide reasonable approximation for general spatial descriptions, but fail to\nhandle precise spatial specifications in text, and the precise spatial-temporal\nparameters needed for avatar control. Notably, LLMs show promise in\nconceptualizing creative motions and distinguishing culturally-specific motion\npatterns.", "AI": {"tldr": "\u901a\u8fc73D\u865a\u62df\u89d2\u8272\u63a7\u5236\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4eba\u7c7b\u52a8\u4f5c\u77e5\u8bc6\uff0c\u53d1\u73b0\u5c3d\u7ba1LLM\u5728\u9ad8\u5c42\u6b21\u8fd0\u52a8\u89e3\u91ca\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7cbe\u786e\u8eab\u4f53\u90e8\u4f4d\u5b9a\u4f4d\u548c\u590d\u6742\u591a\u6b65\u8fd0\u52a8\u4e0a\u5b58\u5728\u56f0\u96be\u3002", "motivation": "\u7814\u7a76LLM\u5728\u4eba\u7c7b\u52a8\u4f5c\u751f\u6210\u4e0e\u89c4\u5212\u4e2d\u7684\u80fd\u529b\uff0c\u9a8c\u8bc1\u5176\u80fd\u5426\u6709\u6548\u7406\u89e3\u548c\u751f\u6210\u590d\u6742\u7684\u591a\u5c42\u6b21\u8fd0\u52a8\u6307\u4ee4\u3002", "method": "\u5206\u4e24\u6b65\u751f\u6210\u52a8\u4f5c\uff1a\u9ad8\u5c42\u6b21\u8fd0\u52a8\u89c4\u5212\uff08\u751f\u6210\u8fde\u7eed\u6b65\u9aa4\uff09\u548c\u4f4e\u5c42\u6b21\u8eab\u4f53\u90e8\u4f4d\u5b9a\u4f4d\uff0c\u5e76\u901a\u8fc7\u52a8\u753b\u7ebf\u6027\u63d2\u503c\u9a8c\u8bc1\u3002\u8bbe\u8ba1\u4e8620\u79cd\u4ee3\u8868\u6027\u8fd0\u52a8\u6307\u4ee4\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "LLM\u64c5\u957f\u89e3\u91ca\u9ad8\u5c42\u6b21\u52a8\u4f5c\uff0c\u4f46\u96be\u4ee5\u7cbe\u786e\u5b9a\u4f4d\u8eab\u4f53\u90e8\u4f4d\uff1b\u591a\u81ea\u7531\u5ea6\u6216\u591a\u6b65\u8fd0\u52a8\u8868\u73b0\u8f83\u5dee\uff1b\u4f46\u5bf9\u521b\u610f\u52a8\u4f5c\u548c\u6587\u5316\u7279\u5b9a\u6a21\u5f0f\u6709\u4e00\u5b9a\u7406\u89e3\u3002", "conclusion": "LLM\u5728\u52a8\u4f5c\u751f\u6210\u4e2d\u6f5c\u529b\u4e0e\u5c40\u9650\u5e76\u5b58\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4f4e\u5c42\u6b21\u8fd0\u52a8\u89c4\u5212\u80fd\u529b\u3002", "keywords": "Large Language Models, 3D avatar control, motion planning, human motion knowledge, spatial-temporal parameters"}}
{"id": "2505.22061", "pdf": "https://arxiv.org/pdf/2505.22061", "abs": "https://arxiv.org/abs/2505.22061", "authors": ["Yujin Choi", "Youngjoo Park", "Junyoung Byun", "Jaewook Lee", "Jinseong Park"], "title": "Safeguarding Privacy of Retrieval Data against Membership Inference Attacks: Is This Query Too Close to Home?", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) mitigates the hallucination problem in\nlarge language models (LLMs) and has proven effective for specific,\npersonalized applications. However, passing private retrieved documents\ndirectly to LLMs introduces vulnerability to membership inference attacks\n(MIAs), which try to determine whether the target datum exists in the private\nexternal database or not. Based on the insight that MIA queries typically\nexhibit high similarity to only one target document, we introduce Mirabel, a\nsimilarity-based MIA detection framework designed for the RAG system. With the\nproposed Mirabel, we show that simple detect-and-hide strategies can\nsuccessfully obfuscate attackers, maintain data utility, and remain\nsystem-agnostic. We experimentally prove its detection and defense against\nvarious state-of-the-art MIA methods and its adaptability to existing private\nRAG systems.", "AI": {"tldr": "Mirabel\u662f\u4e00\u4e2a\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684MIA\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u4fdd\u62a4RAG\u7cfb\u7edf\u4e2d\u7684\u9690\u79c1\u6570\u636e\uff0c\u901a\u8fc7\u7b80\u5355\u68c0\u6d4b\u9690\u85cf\u7b56\u7565\u6709\u6548\u62b5\u5fa1\u653b\u51fb\u5e76\u4fdd\u6301\u6570\u636e\u5b9e\u7528\u6027\u3002", "motivation": "\u89e3\u51b3RAG\u7cfb\u7edf\u4e2d\u76f4\u63a5\u5c06\u79c1\u6709\u68c0\u7d22\u6587\u6863\u4f20\u9012\u7ed9LLM\u5bfc\u81f4\u7684\u6210\u5458\u63a8\u65ad\u653b\u51fb(MIA)\u6f0f\u6d1e\u95ee\u9898\u3002", "method": "\u5f15\u5165Mirabel\u6846\u67b6\uff0c\u57fa\u4e8eMIA\u67e5\u8be2\u4e0e\u76ee\u6807\u6587\u6863\u9ad8\u76f8\u4f3c\u6027\u7684\u7279\u6027\uff0c\u5b9e\u65bd\u68c0\u6d4b\u4e0e\u9690\u85cf\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMirabel\u80fd\u6709\u6548\u68c0\u6d4b\u5e76\u9632\u5fa1\u591a\u79cd\u5148\u8fdbMIA\u65b9\u6cd5\uff0c\u9002\u914d\u73b0\u6709\u79c1\u6709RAG\u7cfb\u7edf\u3002", "conclusion": "Mirabel\u901a\u8fc7\u76f8\u4f3c\u6027\u68c0\u6d4b\u4e0e\u9690\u85cf\u7b56\u7565\uff0c\u6210\u529f\u5e73\u8861\u9690\u79c1\u4fdd\u62a4\u4e0e\u6570\u636e\u5b9e\u7528\u6027\u3002", "keywords": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210, \u6210\u5458\u63a8\u65ad\u653b\u51fb, \u9690\u79c1\u4fdd\u62a4, \u76f8\u4f3c\u6027\u68c0\u6d4b, RAG\u7cfb\u7edf"}}
{"id": "2505.21806", "pdf": "https://arxiv.org/pdf/2505.21806", "abs": "https://arxiv.org/abs/2505.21806", "authors": ["Brian D. Bue", "Jake H. Lee", "Andrew K. Thorpe", "Philip G. Brodrick", "Daniel Cusworth", "Alana Ayasse", "Vassiliki Mancoridis", "Anagha Satish", "Shujun Xiong", "Riley Duren"], "title": "Towards Operational Automated Greenhouse Gas Plume Detection", "categories": ["cs.LG"], "comment": "Main 19 pages 14 figures. Supplemental 19 pages 16 figures. In review", "summary": "Operational deployment of a fully automated greenhouse gas (GHG) plume\ndetection system remains an elusive goal for imaging spectroscopy missions,\ndespite recent advances in deep learning approaches. With the dramatic increase\nin data availability, however, automation continues to increase in importance\nfor natural and anthropogenic emissions monitoring. This work reviews and\naddresses several key obstacles in the field: data and label quality control,\nprevention of spatiotemporal biases, and correctly aligned modeling objectives.\nWe demonstrate through rigorous experiments using multicampaign data from\nairborne and spaceborne instruments that convolutional neural networks (CNNs)\nare able to achieve operational detection performance when these obstacles are\nalleviated. We demonstrate that a multitask model that learns both instance\ndetection and pixelwise segmentation simultaneously can successfully lead\ntowards an operational pathway. We evaluate the model's plume detectability\nacross emission source types and regions, identifying thresholds for\noperational deployment. Finally, we provide analysis-ready data, models, and\nsource code for reproducibility, and work to define a set of best practices and\nvalidation standards to facilitate future contributions to the field.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\uff08\u5c24\u5176\u662fCNN\uff09\u81ea\u52a8\u5316\u68c0\u6d4b\u6e29\u5ba4\u6c14\u4f53\uff08GHG\uff09\u7fbd\u6d41\u7684\u5173\u952e\u6311\u6218\u53ca\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u591a\u4efb\u52a1\u6a21\u578b\u5728\u5b9e\u4f8b\u68c0\u6d4b\u548c\u50cf\u7d20\u7ea7\u5206\u5272\u4e0a\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5206\u6790\u5de5\u5177\u4e0e\u884c\u4e1a\u6807\u51c6\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u6e29\u5ba4\u6c14\u4f53\uff08GHG\uff09\u7fbd\u6d41\u68c0\u6d4b\u4e2d\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5168\u81ea\u52a8\u7cfb\u7edf\u7684\u5b9e\u9645\u90e8\u7f72\u4ecd\u9762\u4e34\u6311\u6218\u3002\u6570\u636e\u53ef\u7528\u6027\u589e\u52a0\u4f7f\u5f97\u81ea\u52a8\u5316\u76d1\u6d4b\u6108\u53d1\u91cd\u8981\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u6570\u636e\u8d28\u91cf\u3001\u65f6\u7a7a\u504f\u5dee\u548c\u5efa\u6a21\u76ee\u6807\u5bf9\u9f50\u7b49\u5173\u952e\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5e76\u7ed3\u5408\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u540c\u65f6\u8fdb\u884c\u5b9e\u4f8b\u68c0\u6d4b\u548c\u50cf\u7d20\u7ea7\u5206\u5272\u3002\u901a\u8fc7\u591a\u5e73\u53f0\uff08\u673a\u8f7d\u4e0e\u661f\u8f7d\uff09\u7684\u591a\u6e90\u6570\u636e\u9a8c\u8bc1\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u89e3\u51b3\u5173\u952e\u969c\u788d\u540e\uff0cCNN\u53ef\u5b9e\u73b0\u64cd\u4f5c\u7ea7\u68c0\u6d4b\u6027\u80fd\u3002\u591a\u4efb\u52a1\u6a21\u578b\u80fd\u6709\u6548\u652f\u6301\u5b9e\u9645\u5e94\u7528\uff0c\u5e76\u5728\u4e0d\u540c\u6392\u653e\u6e90\u7c7b\u578b\u548c\u533a\u57df\u4e2d\u786e\u5b9a\u4e86\u53ef\u90e8\u7f72\u7684\u68c0\u6d4b\u9608\u503c\u3002", "conclusion": "\u7814\u7a76\u4e3aGHG\u7fbd\u6d41\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u5de5\u5177\u548c\u6807\u51c6\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u5411\u5168\u81ea\u52a8\u5316\u76d1\u6d4b\u7684\u53d1\u5c55\u3002", "keywords": "\u6e29\u5ba4\u6c14\u4f53\uff08GHG\uff09\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u3001\u5b9e\u4f8b\u68c0\u6d4b\u3001\u50cf\u7d20\u5206\u5272"}}
{"id": "2505.21532", "pdf": "https://arxiv.org/pdf/2505.21532", "abs": "https://arxiv.org/abs/2505.21532", "authors": ["Ismail Erbas", "Ferhat Demirkiran", "Karthik Swaminathan", "Naigang Wang", "Navid Ibtehaj Nizam", "Stefan T. Radev", "Kaoutar El Maghraoui", "Xavier Intes", "Vikas Pandey"], "title": "EvidenceMoE: A Physics-Guided Mixture-of-Experts with Evidential Critics for Advancing Fluorescence Light Detection and Ranging in Scattering Media", "categories": ["cs.CV", "cs.AI", "cs.LG", "physics.optics"], "comment": "18 pages, 4 figures", "summary": "Fluorescence LiDAR (FLiDAR), a Light Detection and Ranging (LiDAR) technology\nemployed for distance and depth estimation across medical, automotive, and\nother fields, encounters significant computational challenges in scattering\nmedia. The complex nature of the acquired FLiDAR signal, particularly in such\nenvironments, makes isolating photon time-of-flight (related to target depth)\nand intrinsic fluorescence lifetime exceptionally difficult, thus limiting the\neffectiveness of current analytical and computational methodologies. To\novercome this limitation, we present a Physics-Guided Mixture-of-Experts (MoE)\nframework tailored for specialized modeling of diverse temporal components. In\ncontrast to the conventional MoE approaches our expert models are informed by\nunderlying physics, such as the radiative transport equation governing photon\npropagation in scattering media. Central to our approach is EvidenceMoE, which\nintegrates Evidence-Based Dirichlet Critics (EDCs). These critic models assess\nthe reliability of each expert's output by providing per-expert quality scores\nand corrective feedback. A Decider Network then leverages this information to\nfuse expert predictions into a robust final estimate adaptively. We validate\nour method using realistically simulated Fluorescence LiDAR (FLiDAR) data for\nnon-invasive cancer cell depth detection generated from photon transport models\nin tissue. Our framework demonstrates strong performance, achieving a\nnormalized root mean squared error (NRMSE) of 0.030 for depth estimation and\n0.074 for fluorescence lifetime.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u6307\u5bfc\u7684\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u8367\u5149\u6fc0\u5149\u96f7\u8fbe\uff08FLiDAR\uff09\u5728\u6563\u5c04\u4ecb\u8d28\u4e2d\u6df1\u5ea6\u548c\u8367\u5149\u5bff\u547d\u4f30\u8ba1\u7684\u6311\u6218\u3002", "motivation": "\u8367\u5149\u6fc0\u5149\u96f7\u8fbe\u6280\u672f\u5728\u590d\u6742\u6563\u5c04\u4ecb\u8d28\u4e2d\u9762\u4e34\u4fe1\u53f7\u89e3\u6790\u56f0\u96be\uff0c\u5bfc\u81f4\u6df1\u5ea6\u548c\u8367\u5149\u5bff\u547d\u4f30\u8ba1\u6548\u679c\u4e0d\u4f73\uff0c\u4e9f\u9700\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e86\u7269\u7406\u6307\u5bfc\u7684\u4e13\u5bb6\u6df7\u5408\u6846\u67b6\uff08EvidenceMoE\uff09\uff0c\u7ed3\u5408\u57fa\u4e8e\u8bc1\u636e\u7684Dirichlet\u8bc4\u8bba\u5bb6\uff08EDCs\uff09\u548c\u51b3\u7b56\u7f51\u7edc\uff0c\u52a8\u6001\u878d\u5408\u4e13\u5bb6\u9884\u6d4b\u3002", "result": "\u5728\u6a21\u62df\u7684FLiDAR\u6570\u636e\u4e2d\u9a8c\u8bc1\uff0c\u6df1\u5ea6\u4f30\u8ba1\u7684\u5f52\u4e00\u5316\u5747\u65b9\u6839\u8bef\u5dee\uff08NRMSE\uff09\u4e3a0.030\uff0c\u8367\u5149\u5bff\u547d\u4e3a0.074\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86FLiDAR\u5728\u6563\u5c04\u4ecb\u8d28\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u533b\u5b66\u548c\u5de5\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "keywords": "\u8367\u5149\u6fc0\u5149\u96f7\u8fbe, \u4e13\u5bb6\u6df7\u5408\u6846\u67b6, \u6563\u5c04\u4ecb\u8d28, \u6df1\u5ea6\u4f30\u8ba1, \u8367\u5149\u5bff\u547d"}}
{"id": "2505.22068", "pdf": "https://arxiv.org/pdf/2505.22068", "abs": "https://arxiv.org/abs/2505.22068", "authors": ["Ran Li", "Shimin Di", "Yuchen Liu", "Chen Jing", "Yu Qiu", "Lei Chen"], "title": "Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Previous study suggest that powerful Large Language Models (LLMs) trained\nwith Reinforcement Learning with Verifiable Rewards (RLVR) only refines\nreasoning path without improving the reasoning capacity in math tasks while\nsupervised-finetuning(SFT) with distillation can. We study this from the view\nof Scientific information extraction (SciIE) where LLMs and reasoning LLMs\nunderperforms small Bert-based models. SciIE require both the reasoning and\nmemorization. We argue that both SFT and RLVR can refine the reasoning path and\nimprove reasoning capacity in a simple way based on SciIE. We propose two-stage\ntraining with 1. MimicSFT, using structured reasoning templates without needing\nhigh-quality chain-of-thought data, 2. R$^2$GRPO with relevance and\nrule-induced rewards. Experiments on scientific IE benchmarks show that both\nmethods can improve the reasoning capacity. R$^2$GRPO with mimicSFT surpasses\nbaseline LLMs and specialized supervised models in relation extraction. Our\ncode is available at https://github.com/ranlislz/R2GRPO.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u5982\u4f55\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u51fa\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff08MimicSFT\u548cR\u00b2GRPO\uff09\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u4ec5\u80fd\u4f18\u5316\u63a8\u7406\u8def\u5f84\u800c\u65e0\u6cd5\u63d0\u5347\u6570\u5b66\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u800c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u53ef\u4ee5\u3002\u672c\u6587\u4ece\u79d1\u5b66\u4fe1\u606f\u62bd\u53d6\uff08SciIE\uff09\u4efb\u52a1\u7684\u89d2\u5ea6\u51fa\u53d1\uff0c\u63a2\u7d22\u5982\u4f55\u7ed3\u5408SFT\u548cRLVR\u5171\u540c\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u8bb0\u5fc6\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff1a1. MimicSFT\uff0c\u4f7f\u7528\u7ed3\u6784\u5316\u63a8\u7406\u6a21\u677f\uff0c\u65e0\u9700\u9ad8\u8d28\u91cf\u94fe\u5f0f\u601d\u8003\u6570\u636e\uff1b2. R\u00b2GRPO\uff0c\u7ed3\u5408\u76f8\u5173\u6027\u548c\u89c4\u5219\u8bf1\u5bfc\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e24\u79cd\u65b9\u6cd5\u5747\u80fd\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u4e2d\uff0cR\u00b2GRPO\u7ed3\u5408MimicSFT\u8d85\u8d8a\u4e86\u57fa\u7ebf\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4e13\u95e8\u76d1\u7763\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408SFT\u548cRLVR\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u79d1\u5b66\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5c24\u5176R\u00b2GRPO\u4e0eMimicSFT\u7684\u7ec4\u5408\u8868\u73b0\u6700\u4f73\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u76d1\u7763\u5fae\u8c03, \u5f3a\u5316\u5b66\u4e60, \u79d1\u5b66\u4fe1\u606f\u62bd\u53d6, \u5173\u7cfb\u62bd\u53d6"}}
{"id": "2505.21807", "pdf": "https://arxiv.org/pdf/2505.21807", "abs": "https://arxiv.org/abs/2505.21807", "authors": ["Tommy Xu", "Zhitian Zhang", "Xiangyu Sun", "Lauren Kelly Zung", "Hossein Hajimirsadeghi", "Greg Mori"], "title": "TabReason: A Reinforcement Learning-Enhanced Reasoning LLM for Explainable Tabular Data Prediction", "categories": ["cs.LG"], "comment": null, "summary": "Predictive modeling on tabular data is the cornerstone of many real-world\napplications. Although gradient boosting machines and some recent deep models\nachieve strong performance on tabular data, they often lack interpretability.\nOn the other hand, large language models (LLMs) have demonstrated powerful\ncapabilities to generate human-like reasoning and explanations, but remain\nunder-performed for tabular data prediction. In this paper, we propose a new\napproach that leverages reasoning-based LLMs, trained using reinforcement\nlearning, to perform more accurate and explainable predictions on tabular data.\nOur method introduces custom reward functions that guide the model not only\ntoward high prediction accuracy but also toward human-understandable reasons\nfor its predictions. Experimental results show that our model achieves\npromising performance on financial benchmark datasets, outperforming most\nexisting LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u8868\u683c\u6570\u636e\u9884\u6d4b\u7684\u65b9\u6cd5\uff0c\u517c\u987e\u4e86\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u68af\u5ea6\u63d0\u5347\u673a\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8868\u683c\u6570\u636e\u4e0a\u8868\u73b0\u826f\u597d\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u800cLLMs\u867d\u80fd\u751f\u6210\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u63a8\u7406\u4f46\u9884\u6d4b\u6027\u80fd\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u517c\u987e\u4e24\u8005\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684LLMs\uff0c\u7ed3\u5408\u81ea\u5b9a\u4e49\u5956\u52b1\u51fd\u6570\uff0c\u65e2\u8ffd\u6c42\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u53c8\u786e\u4fdd\u9884\u6d4b\u7406\u7531\u53ef\u7406\u89e3\u3002", "result": "\u5728\u91d1\u878d\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5927\u591a\u6570LLMs\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9884\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u8868\u683c\u6570\u636e\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u8868\u683c\u6570\u636e\u9884\u6d4b,\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u5f3a\u5316\u5b66\u4e60,\u53ef\u89e3\u91ca\u6027"}}
{"id": "2505.21534", "pdf": "https://arxiv.org/pdf/2505.21534", "abs": "https://arxiv.org/abs/2505.21534", "authors": ["Yao Fehlis"], "title": "Uncovering Bottlenecks and Optimizing Scientific Lab Workflows with Cycle Time Reduction Agents", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Scientific laboratories, particularly those in pharmaceutical and\nbiotechnology companies, encounter significant challenges in optimizing\nworkflows due to the complexity and volume of tasks such as compound screening\nand assay execution. We introduce Cycle Time Reduction Agents (CTRA), a\nLangGraph-based agentic workflow designed to automate the analysis of lab\noperational metrics. CTRA comprises three main components: the Question\nCreation Agent for initiating analysis, Operational Metrics Agents for data\nextraction and validation, and Insights Agents for reporting and visualization,\nidentifying bottlenecks in lab processes. This paper details CTRA's\narchitecture, evaluates its performance on a lab dataset, and discusses its\npotential to accelerate pharmaceutical and biotechnological development. CTRA\noffers a scalable framework for reducing cycle times in scientific labs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLangGraph\u7684Cycle Time Reduction Agents\uff08CTRA\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u79d1\u5b66\u5b9e\u9a8c\u5ba4\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7279\u522b\u662f\u836f\u7269\u548c\u751f\u7269\u6280\u672f\u516c\u53f8\u7684\u590d\u6742\u4efb\u52a1\u3002CTRA\u901a\u8fc7\u4e09\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff08\u95ee\u9898\u521b\u5efa\u4ee3\u7406\u3001\u64cd\u4f5c\u6307\u6807\u4ee3\u7406\u548c\u6d1e\u5bdf\u4ee3\u7406\uff09\u81ea\u52a8\u5316\u5206\u6790\u5b9e\u9a8c\u5ba4\u64cd\u4f5c\u6570\u636e\uff0c\u4ee5\u8bc6\u522b\u74f6\u9888\u5e76\u52a0\u901f\u7814\u53d1\u3002", "motivation": "\u79d1\u5b66\u5b9e\u9a8c\u5ba4\uff08\u5c24\u5176\u662f\u5236\u836f\u548c\u751f\u7269\u6280\u672f\u9886\u57df\uff09\u56e0\u4efb\u52a1\u590d\u6742\u4e14\u91cf\u5927\uff08\u5982\u5316\u5408\u7269\u7b5b\u9009\u548c\u68c0\u6d4b\u6267\u884c\uff09\u9762\u4e34\u5de5\u4f5c\u6d41\u7a0b\u4f18\u5316\u6311\u6218\uff0c\u9700\u8981\u4e00\u4e2a\u81ea\u52a8\u5316\u5de5\u5177\u6765\u63d0\u5347\u6548\u7387\u3002", "method": "\u91c7\u7528\u57fa\u4e8eLangGraph\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41CTRA\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u95ee\u9898\u521b\u5efa\u4ee3\u7406\uff08\u542f\u52a8\u5206\u6790\uff09\u3001\u64cd\u4f5c\u6307\u6807\u4ee3\u7406\uff08\u6570\u636e\u63d0\u53d6\u4e0e\u9a8c\u8bc1\uff09\u548c\u6d1e\u5bdf\u4ee3\u7406\uff08\u62a5\u544a\u4e0e\u53ef\u89c6\u5316\uff09\u3002", "result": "CTRA\u5728\u5b9e\u9a8c\u5ba4\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u80fd\u6709\u6548\u8bc6\u522b\u6d41\u7a0b\u74f6\u9888\uff0c\u4e3a\u836f\u7269\u548c\u751f\u7269\u6280\u672f\u7814\u53d1\u63d0\u4f9b\u52a0\u901f\u6f5c\u529b\u3002", "conclusion": "CTRA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u663e\u8457\u51cf\u5c11\u79d1\u5b66\u5b9e\u9a8c\u5ba4\u7684\u5468\u671f\u65f6\u95f4\u3002", "keywords": "\u5b9e\u9a8c\u5ba4\u5de5\u4f5c\u6d41\u7a0b\u4f18\u5316, LangGraph, \u81ea\u52a8\u5316\u4ee3\u7406, \u5236\u836f\u7814\u53d1, \u5468\u671f\u65f6\u95f4\u7f29\u51cf"}}
{"id": "2505.22076", "pdf": "https://arxiv.org/pdf/2505.22076", "abs": "https://arxiv.org/abs/2505.22076", "authors": ["Maja Stahl", "Timon Ziegenbein", "Joonsuk Park", "Henning Wachsmuth"], "title": "ArgInstruct: Specialized Instruction Fine-Tuning for Computational Argumentation", "categories": ["cs.CL"], "comment": null, "summary": "Training large language models (LLMs) to follow instructions has\nsignificantly enhanced their ability to tackle unseen tasks. However, despite\ntheir strong generalization capabilities, instruction-following LLMs encounter\ndifficulties when dealing with tasks that require domain knowledge. This work\nintroduces a specialized instruction fine-tuning for the domain of\ncomputational argumentation (CA). The goal is to enable an LLM to effectively\ntackle any unseen CA tasks while preserving its generalization capabilities.\nReviewing existing CA research, we crafted natural language instructions for\n105 CA tasks to this end. On this basis, we developed a CA-specific benchmark\nfor LLMs that allows for a comprehensive evaluation of LLMs' capabilities in\nsolving various CA tasks. We synthesized 52k CA-related instructions, adapting\nthe self-instruct process to train a CA-specialized instruction-following LLM.\nOur experiments suggest that CA-specialized instruction fine-tuning\nsignificantly enhances the LLM on both seen and unseen CA tasks. At the same\ntime, performance on the general NLP tasks of the SuperNI benchmark remains\nstable.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8ba1\u7b97\u8bba\u8bc1\uff08CA\uff09\u9886\u57df\u7684\u4e13\u95e8\u6307\u4ee4\u5fae\u8c03\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728CA\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u6307\u4ee4\u5fae\u8c03\u7684LLMs\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9700\u8981\u9886\u57df\u77e5\u8bc6\u7684\u4efb\u52a1\uff08\u5982\u8ba1\u7b97\u8bba\u8bc1\uff09\u4e2d\u4ecd\u5b58\u5728\u56f0\u96be\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u4e13\u95e8\u9488\u5bf9CA\u9886\u57df\u7684\u5fae\u8c03\u6765\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86105\u4e2aCA\u4efb\u52a1\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u5f00\u53d1\u4e86CA\u4e13\u7528\u57fa\u51c6\uff0c\u5e76\u901a\u8fc7\u81ea\u6307\u4ee4\u8fc7\u7a0b\u751f\u6210\u4e8652k\u6761CA\u76f8\u5173\u6307\u4ee4\u6765\u8bad\u7ec3CA\u4e13\u7528LLM\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCA\u4e13\u7528\u6307\u4ee4\u5fae\u8c03\u663e\u7740\u63d0\u5347\u4e86LLM\u5728\u5df2\u89c1\u548c\u672a\u89c1CA\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u540c\u65f6\u5728\u901a\u7528NLP\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u4fdd\u6301\u7a33\u5b9a\u3002", "conclusion": "\u4e13\u95e8\u9886\u57df\u6307\u4ee4\u5fae\u8c03\u662f\u63d0\u5347LLM\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u8868\u73b0\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u5176\u6cdb\u5316\u80fd\u529b\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u6307\u4ee4\u5fae\u8c03,\u8ba1\u7b97\u8bba\u8bc1,\u4e13\u95e8\u9886\u57df,\u4efb\u52a1\u8868\u73b0"}}
{"id": "2505.21813", "pdf": "https://arxiv.org/pdf/2505.21813", "abs": "https://arxiv.org/abs/2505.21813", "authors": ["Madi Matymov", "Ba-Hien Tran", "Michael Kampffmeyer", "Markus Heinonen", "Maurizio Filippone"], "title": "Optimizing Data Augmentation through Bayesian Model Selection", "categories": ["cs.LG", "stat.ML", "62F15, 68T07 (Primary) 62M45, 62C10, 65C60 (Secondary)"], "comment": "26 pages, 3 figures", "summary": "Data Augmentation (DA) has become an essential tool to improve robustness and\ngeneralization of modern machine learning. However, when deciding on DA\nstrategies it is critical to choose parameters carefully, and this can be a\ndaunting task which is traditionally left to trial-and-error or expensive\noptimization based on validation performance. In this paper, we counter these\nlimitations by proposing a novel framework for optimizing DA. In particular, we\ntake a probabilistic view of DA, which leads to the interpretation of\naugmentation parameters as model (hyper)-parameters, and the optimization of\nthe marginal likelihood with respect to these parameters as a Bayesian model\nselection problem. Due to its intractability, we derive a tractable Evidence\nLower BOund (ELBO), which allows us to optimize augmentation parameters jointly\nwith model parameters. We provide extensive theoretical results on variational\napproximation quality, generalization guarantees, invariance properties, and\nconnections to empirical Bayes. Through experiments on computer vision tasks,\nwe show that our approach improves calibration and yields robust performance\nover fixed or no augmentation. Our work provides a rigorous foundation for\noptimizing DA through Bayesian principles with significant potential for robust\nmachine learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u539f\u7406\u7684\u6570\u636e\u589e\u5f3a\uff08DA\uff09\u53c2\u6570\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u589e\u5f3a\u53c2\u6570\u89c6\u4e3a\u6a21\u578b\u8d85\u53c2\u6570\uff0c\u5e76\u5229\u7528\u8bc1\u636e\u4e0b\u754c\uff08ELBO\uff09\u8054\u5408\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u53c2\u6570\u9009\u62e9\u4f9d\u8d56\u8bd5\u9519\u6216\u6602\u8d35\u7684\u9a8c\u8bc1\u4f18\u5316\uff0c\u9650\u5236\u4e86\u5176\u6548\u7387\u548c\u6548\u679c\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8d1d\u53f6\u65af\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e00\u79cd\u7cfb\u7edf\u5316\u7684DA\u53c2\u6570\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u6982\u7387\u89c6\u89d2\u7684DA\u6846\u67b6\uff0c\u5c06\u589e\u5f3a\u53c2\u6570\u5efa\u6a21\u4e3a\u8d85\u53c2\u6570\uff0c\u901a\u8fc7\u4f18\u5316\u8fb9\u9645\u4f3c\u7136\u7684\u53d8\u5206\u4e0b\u754c\uff08ELBO\uff09\u5b9e\u73b0\u8054\u5408\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u63d0\u9ad8\u4e86\u6a21\u578b\u6821\u51c6\u6027\uff0c\u5e76\u4f18\u4e8e\u56fa\u5b9a\u6216\u65e0\u589e\u5f3a\u7b56\u7565\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u57fa\u4e8e\u8d1d\u53f6\u65af\u539f\u7406\u7684DA\u4f18\u5316\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5bf9\u9c81\u68d2\u673a\u5668\u5b66\u4e60\u5177\u6709\u91cd\u8981\u6f5c\u529b\u3002", "keywords": "\u6570\u636e\u589e\u5f3a, \u8d1d\u53f6\u65af\u4f18\u5316, \u53d8\u5206\u63a8\u65ad, \u6a21\u578b\u6821\u51c6, \u9c81\u68d2\u6027"}}
{"id": "2505.21535", "pdf": "https://arxiv.org/pdf/2505.21535", "abs": "https://arxiv.org/abs/2505.21535", "authors": ["Yuxin Ren", "Maxwell D Collins", "Miao Hu", "Huanrui Yang"], "title": "Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "12 pages main paper + 6 pages appendix, 14 figures; submitted to\n  NeurIPS 2025", "summary": "While transformers excel across vision and language pretraining tasks, their\nreliance on attention mechanisms poses challenges for inference efficiency,\nespecially on edge and embedded accelerators with limited parallelism and\nmemory bandwidth. Hinted by the observed redundancy of attention at inference\ntime, we hypothesize that though the model learns complicated token dependency\nthrough pretraining, the inference-time sequence-to-sequence mapping in each\nattention layer is actually ''simple'' enough to be represented with a much\ncheaper function. In this work, we explore FAR, a Function-preserving Attention\nReplacement framework that replaces all attention blocks in pretrained\ntransformers with learnable sequence-to-sequence modules, exemplified by an\nLSTM. FAR optimize a multi-head LSTM architecture with a block-wise\ndistillation objective and a global structural pruning framework to achieve a\nfamily of efficient LSTM-based models from pretrained transformers. We validate\nFAR on the DeiT vision transformer family and demonstrate that it matches the\naccuracy of the original models on ImageNet and multiple downstream tasks with\nreduced parameters and latency. Further analysis shows that FAR preserves the\nsemantic token relationships and the token-to-token correlation learned in the\ntransformer's attention module.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faFAR\u6846\u67b6\uff0c\u7528\u4e8e\u66ff\u4ee3\u9884\u8bad\u7ec3Transformer\u4e2d\u7684\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u4ee5\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u3002", "motivation": "\u7531\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u5728\u63a8\u7406\u65f6\u5b58\u5728\u5197\u4f59\uff0c\u4e14\u8fb9\u7f18\u548c\u5d4c\u5165\u5f0f\u8bbe\u5907\u5e76\u884c\u6027\u548c\u5185\u5b58\u5e26\u5bbd\u6709\u9650\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u7528\u66f4\u7b80\u5355\u7684\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u5757\uff08\u5982LSTM\uff09\u66ff\u4ee3\u6ce8\u610f\u529b\u6a21\u5757\u3002", "method": "FAR\u6846\u67b6\u901a\u8fc7\u5757\u7ea7\u84b8\u998f\u76ee\u6807\u548c\u5168\u5c40\u7ed3\u6784\u526a\u679d\uff0c\u5c06\u9884\u8bad\u7ec3Transformer\u7684\u6ce8\u610f\u529b\u6a21\u5757\u66ff\u6362\u4e3a\u53ef\u5b66\u4e60\u7684LSTM\u6a21\u5757\u3002", "result": "\u5728DeiT\u89c6\u89c9Transformer\u65cf\u4e0a\u9a8c\u8bc1\uff0cFAR\u5728\u51cf\u5c11\u53c2\u6570\u548c\u5ef6\u8fdf\u7684\u540c\u65f6\uff0c\u5728ImageNet\u53ca\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4fdd\u6301\u4e0e\u539f\u6a21\u578b\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002", "conclusion": "FAR\u4e0d\u4ec5\u80fd\u6709\u6548\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u8fd8\u80fd\u4fdd\u7559\u6ce8\u610f\u529b\u6a21\u5757\u5b66\u5230\u7684\u8bed\u4e49\u5173\u7cfb\u548c\u76f8\u5173\u6027\u3002", "keywords": "Transformer, \u6ce8\u610f\u529b\u673a\u5236, FAR, LSTM, \u63a8\u7406\u6548\u7387"}}
{"id": "2505.22095", "pdf": "https://arxiv.org/pdf/2505.22095", "abs": "https://arxiv.org/abs/2505.22095", "authors": ["Chunyi Peng", "Zhipeng Xu", "Zhenghao Liu", "Yishan Li", "Yukun Yan", "Shuo Wang", "Zhiyuan Liu", "Yu Gu", "Minghe Yu", "Ge Yu", "Maosong Sun"], "title": "Learning to Route Queries Across Knowledge Bases for Step-wise Retrieval-Augmented Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Retrieval-Augmented Generation (MRAG) has shown promise in\nmitigating hallucinations in Multimodal Large Language Models (MLLMs) by\nincorporating external knowledge during generation. Existing MRAG methods\ntypically adopt a static retrieval pipeline that fetches relevant information\nfrom multiple Knowledge Bases (KBs), followed by a refinement step. However,\nthese approaches overlook the reasoning and planning capabilities of MLLMs to\ndynamically determine how to interact with different KBs during the reasoning\nprocess. To address this limitation, we propose R1-Router, a novel MRAG\nframework that learns to decide when and where to retrieve knowledge based on\nthe evolving reasoning state. Specifically, R1-Router can generate follow-up\nqueries according to the current reasoning step, routing these intermediate\nqueries to the most suitable KB, and integrating external knowledge into a\ncoherent reasoning trajectory to answer the original query. Furthermore, we\nintroduce Step-wise Group Relative Policy Optimization (Step-GRPO), a tailored\nreinforcement learning algorithm that assigns step-specific rewards to optimize\nthe reasoning behavior of MLLMs. Experimental results on various open-domain QA\nbenchmarks across multiple modalities demonstrate that R1-Router outperforms\nbaseline models by over 7%. Further analysis shows that R1-Router can\nadaptively and effectively leverage diverse KBs, reducing unnecessary\nretrievals and improving both efficiency and accuracy.", "AI": {"tldr": "R1-Router \u662f\u4e00\u79cd\u65b0\u578b MRAG \u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u7531\u5230\u77e5\u8bc6\u5e93\uff08KB\uff09\u63d0\u9ad8\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6bd4\u57fa\u7ebf\u6a21\u578b\u6548\u679c\u9ad8 7%\u3002", "motivation": "\u73b0\u6709\u7684 MRAG \u65b9\u6cd5\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7f3a\u4e4f\u52a8\u6001\u4ea4\u4e92\u77e5\u8bc6\u5e93\u7684\u80fd\u529b\uff0cR1-Router \u65e8\u5728\u901a\u8fc7\u52a8\u6001\u8def\u7531\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "R1-Router \u6846\u67b6\u52a8\u6001\u751f\u6210\u67e5\u8be2\u5e76\u8def\u7531\u5230\u5408\u9002\u7684 KB\uff0c\u7ed3\u5408 Step-GRPO \u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u4f18\u5316\u63a8\u7406\u884c\u4e3a\u3002", "result": "\u5728\u591a\u6a21\u6001\u5f00\u653e\u9886\u57df QA \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cR1-Router \u6bd4\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\u63d0\u5347 7%\uff0c\u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684\u68c0\u7d22\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "conclusion": "R1-Router \u901a\u8fc7\u52a8\u6001\u77e5\u8bc6\u8def\u7531\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u77e5\u8bc6\u5229\u7528\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "keywords": ""}}
{"id": "2505.21824", "pdf": "https://arxiv.org/pdf/2505.21824", "abs": "https://arxiv.org/abs/2505.21824", "authors": ["Praveen Kumar", "Vincent T. Metzger", "Scott A. Malec"], "title": "Unsupervised Latent Pattern Analysis for Estimating Type 2 Diabetes Risk in Undiagnosed Populations", "categories": ["cs.LG", "stat.AP"], "comment": null, "summary": "The global prevalence of diabetes, particularly type 2 diabetes mellitus\n(T2DM), is rapidly increasing, posing significant health and economic\nchallenges. T2DM not only disrupts blood glucose regulation but also damages\nvital organs such as the heart, kidneys, eyes, nerves, and blood vessels,\nleading to substantial morbidity and mortality. In the US alone, the economic\nburden of diagnosed diabetes exceeded \\$400 billion in 2022. Early detection of\nindividuals at risk is critical to mitigating these impacts. While machine\nlearning approaches for T2DM prediction are increasingly adopted, many rely on\nsupervised learning, which is often limited by the lack of confirmed negative\ncases. To address this limitation, we propose a novel unsupervised framework\nthat integrates Non-negative Matrix Factorization (NMF) with statistical\ntechniques to identify individuals at risk of developing T2DM. Our method\nidentifies latent patterns of multimorbidity and polypharmacy among diagnosed\nT2DM patients and applies these patterns to estimate the T2DM risk in\nundiagnosed individuals. By leveraging data-driven insights from comorbidity\nand medication usage, our approach provides an interpretable and scalable\nsolution that can assist healthcare providers in implementing timely\ninterventions, ultimately improving patient outcomes and potentially reducing\nthe future health and economic burden of T2DM.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u975e\u8d1f\u77e9\u9635\u5206\u89e3\uff08NMF\uff09\u4e0e\u7edf\u8ba1\u6280\u672f\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b2\u578b\u7cd6\u5c3f\u75c5\uff08T2DM\uff09\u9ad8\u98ce\u9669\u4eba\u7fa4\uff0c\u89e3\u51b3\u4e86\u76d1\u7763\u5b66\u4e60\u4e2d\u7f3a\u4e4f\u9634\u6027\u6837\u672c\u7684\u95ee\u9898\u3002", "motivation": "\u5168\u74032\u578b\u7cd6\u5c3f\u75c5\uff08T2DM\uff09\u60a3\u75c5\u7387\u6025\u5267\u4e0a\u5347\uff0c\u5e26\u6765\u5de8\u5927\u7684\u5065\u5eb7\u548c\u7ecf\u6d4e\u8d1f\u62c5\u3002\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u591a\u4f9d\u8d56\u76d1\u7763\u5b66\u4e60\uff0c\u4f46\u7f3a\u5c11\u5df2\u786e\u8ba4\u7684\u9634\u6027\u75c5\u4f8b\u9650\u5236\u4e86\u5176\u6548\u679c\u3002", "method": "\u91c7\u7528\u975e\u8d1f\u77e9\u9635\u5206\u89e3\uff08NMF\uff09\u7ed3\u5408\u7edf\u8ba1\u6280\u672f\uff0c\u5229\u7528\u5171\u75c5\u548c\u7528\u836f\u6570\u636e\u63d0\u53d6\u6f5c\u5728\u6a21\u5f0f\uff0c\u5e76\u5e94\u7528\u4e8e\u672a\u8bca\u65ad\u4e2a\u4f53\u7684\u98ce\u9669\u8bc4\u4f30\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u591a\u75c5\u5171\u5b58\u548c\u591a\u91cd\u7528\u836f\u7684\u6f5c\u5728\u6a21\u5f0f\uff0c\u4e3a\u672a\u8bca\u65ad\u4e2a\u4f53\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684T2DM\u98ce\u9669\u9884\u6d4b\uff0c\u652f\u6301\u65e9\u671f\u5e72\u9884\u3002", "conclusion": "\u8be5\u65e0\u76d1\u7763\u6846\u67b6\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\uff0c\u6709\u52a9\u4e8e\u533b\u7597\u63d0\u4f9b\u8005\u5b9e\u65bd\u53ca\u65f6\u5e72\u9884\uff0c\u6539\u5584\u60a3\u8005\u9884\u540e\u5e76\u964d\u4f4eT2DM\u7684\u672a\u6765\u8d1f\u62c5\u3002", "keywords": "2\u578b\u7cd6\u5c3f\u75c5\uff0c\u65e0\u76d1\u7763\u5b66\u4e60\uff0c\u975e\u8d1f\u77e9\u9635\u5206\u89e3\uff0c\u5171\u75c5\u6a21\u5f0f\uff0c\u98ce\u9669\u8bc4\u4f30"}}
{"id": "2505.21537", "pdf": "https://arxiv.org/pdf/2505.21537", "abs": "https://arxiv.org/abs/2505.21537", "authors": ["Hao Sun", "Yunyi Shen", "Mihaela van der Schaar"], "title": "OpenReview Should be Protected and Leveraged as a Community Asset for Research in the Era of Large Language Models", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "In the era of large language models (LLMs), high-quality, domain-rich, and\ncontinuously evolving datasets capturing expert-level knowledge, core human\nvalues, and reasoning are increasingly valuable. This position paper argues\nthat OpenReview -- the continually evolving repository of research papers, peer\nreviews, author rebuttals, meta-reviews, and decision outcomes -- should be\nleveraged more broadly as a core community asset for advancing research in the\nera of LLMs. We highlight three promising areas in which OpenReview can\nuniquely contribute: enhancing the quality, scalability, and accountability of\npeer review processes; enabling meaningful, open-ended benchmarks rooted in\ngenuine expert deliberation; and supporting alignment research through\nreal-world interactions reflecting expert assessment, intentions, and\nscientific values. To better realize these opportunities, we suggest the\ncommunity collaboratively explore standardized benchmarks and usage guidelines\naround OpenReview, inviting broader dialogue on responsible data use, ethical\nconsiderations, and collective stewardship.", "AI": {"tldr": "\u8be5\u7acb\u573a\u8bba\u6587\u63d0\u51fa\uff0cOpenReview\uff08\u4e00\u4e2a\u6301\u7eed\u66f4\u65b0\u7684\u7814\u7a76\u8bba\u6587\u3001\u540c\u884c\u8bc4\u5ba1\u3001\u4f5c\u8005\u53cd\u9a73\u3001\u5143\u8bc4\u5ba1\u548c\u51b3\u7b56\u7ed3\u679c\u7684\u6570\u636e\u5e93\uff09\u5e94\u88ab\u66f4\u5e7f\u6cdb\u5730\u7528\u4e8e\u63a8\u52a8\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u65f6\u4ee3\u7684\u7814\u7a76\u3002", "motivation": "\u5728\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u4ee3\uff0c\u9ad8\u8d28\u91cf\u3001\u9886\u57df\u4e30\u5bcc\u4e14\u6301\u7eed\u66f4\u65b0\u7684\u6570\u636e\u96c6\u5bf9\u4e8e\u6355\u6349\u4e13\u5bb6\u7ea7\u77e5\u8bc6\u3001\u6838\u5fc3\u4eba\u7c7b\u4ef7\u503c\u89c2\u548c\u63a8\u7406\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002OpenReview\u4f5c\u4e3a\u793e\u533a\u8d44\u4ea7\uff0c\u53ef\u4ee5\u4fc3\u8fdb\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e09\u4e2aOpenReview\u53ef\u4ee5\u53d1\u6325\u72ec\u7279\u4f5c\u7528\u7684\u9886\u57df\uff1a\u63d0\u5347\u540c\u884c\u8bc4\u5ba1\u7684\u8d28\u91cf\u3001\u53ef\u6269\u5c55\u6027\u548c\u900f\u660e\u5ea6\uff1b\u5efa\u7acb\u57fa\u4e8e\u771f\u5b9e\u4e13\u5bb6\u8ba8\u8bba\u7684\u5f00\u653e\u5f0f\u57fa\u51c6\u6d4b\u8bd5\uff1b\u652f\u6301\u5bf9\u9f50\u7814\u7a76\u3002", "result": "OpenReview\u53ef\u4ee5\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u5b9d\u8d35\u7684\u6570\u636e\u8d44\u6e90\uff0c\u4fc3\u8fdb\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u548c\u4f7f\u7528\u6307\u5357\u7684\u5f00\u53d1\u3002", "conclusion": "\u5efa\u8bae\u793e\u533a\u5408\u4f5c\u63a2\u7d22OpenReview\u7684\u6807\u51c6\u5e94\u7528\u65b9\u5f0f\uff0c\u5e76\u5c31\u6570\u636e\u4f7f\u7528\u7684\u8d23\u4efb\u3001\u4f26\u7406\u548c\u96c6\u4f53\u7ba1\u7406\u5c55\u5f00\u66f4\u5e7f\u6cdb\u7684\u8ba8\u8bba\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, OpenReview, \u540c\u884c\u8bc4\u5ba1, \u6570\u636e\u96c6, \u5bf9\u9f50\u7814\u7a76"}}
{"id": "2505.22096", "pdf": "https://arxiv.org/pdf/2505.22096", "abs": "https://arxiv.org/abs/2505.22096", "authors": ["Jinheon Baek", "Horst Samulowitz", "Oktie Hassanzadeh", "Dharmashankar Subramanian", "Sola Shirai", "Alfio Gliozzo", "Debarun Bhattacharjya"], "title": "Knowledge Base Construction for Knowledge-Augmented Text-to-SQL", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL Findings 2025", "summary": "Text-to-SQL aims to translate natural language queries into SQL statements,\nwhich is practical as it enables anyone to easily retrieve the desired\ninformation from databases. Recently, many existing approaches tackle this\nproblem with Large Language Models (LLMs), leveraging their strong capability\nin understanding user queries and generating corresponding SQL code. Yet, the\nparametric knowledge in LLMs might be limited to covering all the diverse and\ndomain-specific queries that require grounding in various database schemas,\nwhich makes generated SQLs less accurate oftentimes. To tackle this, we propose\nconstructing the knowledge base for text-to-SQL, a foundational source of\nknowledge, from which we retrieve and generate the necessary knowledge for\ngiven queries. In particular, unlike existing approaches that either manually\nannotate knowledge or generate only a few pieces of knowledge for each query,\nour knowledge base is comprehensive, which is constructed based on a\ncombination of all the available questions and their associated database\nschemas along with their relevant knowledge, and can be reused for unseen\ndatabases from different datasets and domains. We validate our approach on\nmultiple text-to-SQL datasets, considering both the overlapping and\nnon-overlapping database scenarios, where it outperforms relevant baselines\nsubstantially.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u6784\u5efa\u4e00\u4e2a\u77e5\u8bc6\u5e93\u6765\u63d0\u5347Text-to-SQL\u4efb\u52a1\u7684\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u68c0\u7d22\u548c\u751f\u6210\u76f8\u5173\u77e5\u8bc6\u89e3\u51b3LLMs\u5728\u591a\u6837\u5316\u9886\u57df\u67e5\u8be2\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210SQL\uff0c\u4f46\u5176\u53c2\u6570\u5316\u77e5\u8bc6\u53ef\u80fd\u65e0\u6cd5\u8986\u76d6\u591a\u6837\u5316\u548c\u9886\u57df\u7279\u5b9a\u7684\u67e5\u8be2\uff0c\u5bfc\u81f4\u751f\u6210\u7684SQL\u51c6\u786e\u6027\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u52a8\u6001\u8865\u5145\u9886\u57df\u77e5\u8bc6\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u6784\u5efa\u4e00\u4e2a\u57fa\u4e8e\u95ee\u9898\u3001\u6570\u636e\u5e93\u6a21\u5f0f\u548c\u76f8\u5173\u77e5\u8bc6\u7684\u7efc\u5408\u6027\u77e5\u8bc6\u5e93\uff0c\u4ece\u4e2d\u68c0\u7d22\u5e76\u751f\u6210\u67e5\u8be2\u6240\u9700\u77e5\u8bc6\uff0c\u652f\u6301\u8de8\u6570\u636e\u96c6\u548c\u9886\u57df\u7684\u590d\u7528\u3002", "result": "\u5728\u591a\u4e2aText-to-SQL\u6570\u636e\u96c6\uff08\u5305\u62ec\u91cd\u53e0\u548c\u975e\u91cd\u53e0\u6570\u636e\u5e93\u573a\u666f\uff09\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u77e5\u8bc6\u5e93\u52a8\u6001\u8865\u5145\u9886\u57df\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86Text-to-SQL\u4efb\u52a1\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u591a\u6837\u5316\u67e5\u8be2\u548c\u8de8\u9886\u57df\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "keywords": "Text-to-SQL, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u77e5\u8bc6\u5e93, \u6570\u636e\u5e93\u6a21\u5f0f, \u8de8\u9886\u57df"}}
{"id": "2505.21825", "pdf": "https://arxiv.org/pdf/2505.21825", "abs": "https://arxiv.org/abs/2505.21825", "authors": ["Parsa Mirtaheri", "Ezra Edelman", "Samy Jelassi", "Eran Malach", "Enric Boix-Adsera"], "title": "Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Inference-time computation has emerged as a promising scaling axis for\nimproving large language model reasoning. However, despite yielding impressive\nperformance, the optimal allocation of inference-time computation remains\npoorly understood. A central question is whether to prioritize sequential\nscaling (e.g., longer chains of thought) or parallel scaling (e.g., majority\nvoting across multiple short chains of thought). In this work, we seek to\nilluminate the landscape of test-time scaling by demonstrating the existence of\nreasoning settings where sequential scaling offers an exponential advantage\nover parallel scaling. These settings are based on graph connectivity problems\nin challenging distributions of graphs. We validate our theoretical findings\nwith comprehensive experiments across a range of language models, including\nmodels trained from scratch for graph connectivity with different chain of\nthought strategies as well as large reasoning models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u63a8\u7406\u65f6\u8ba1\u7b97\u5206\u914d\u4e2d\u987a\u5e8f\u6269\u5c55\uff08\u5982\u66f4\u957f\u7684\u601d\u7ef4\u94fe\uff09\u4e0e\u5e76\u884c\u6269\u5c55\uff08\u5982\u591a\u6570\u6295\u7968\uff09\u7684\u4f18\u52bf\uff0c\u53d1\u73b0\u67d0\u4e9b\u56fe\u8fde\u901a\u6027\u95ee\u9898\u8bbe\u7f6e\u4e0b\u987a\u5e8f\u6269\u5c55\u5177\u6709\u6307\u6570\u7ea7\u4f18\u52bf\u3002", "motivation": "\u5c3d\u7ba1\u63a8\u7406\u65f6\u8ba1\u7b97\u80fd\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\uff0c\u4f46\u5176\u6700\u4f18\u5206\u914d\u65b9\u5f0f\u5c1a\u4e0d\u660e\u786e\uff0c\u5c24\u5176\u662f\u987a\u5e8f\u6269\u5c55\u4e0e\u5e76\u884c\u6269\u5c55\u7684\u9009\u62e9\u3002", "method": "\u901a\u8fc7\u56fe\u8fde\u901a\u6027\u95ee\u9898\u7684\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u987a\u5e8f\u6269\u5c55\u4e0e\u5e76\u884c\u6269\u5c55\u7684\u6548\u679c\uff0c\u8986\u76d6\u591a\u79cd\u8bed\u8a00\u6a21\u578b\u53ca\u5b9a\u5236\u5316\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "result": "\u5728\u7279\u5b9a\u56fe\u5206\u5e03\u8bbe\u7f6e\u4e0b\uff0c\u987a\u5e8f\u6269\u5c55\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u5e76\u884c\u6269\u5c55\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u53d1\u73b0\u3002", "conclusion": "\u63a8\u7406\u65f6\u8ba1\u7b97\u7684\u6700\u4f18\u5206\u914d\u53d6\u51b3\u4e8e\u4efb\u52a1\u7279\u6027\uff0c\u987a\u5e8f\u6269\u5c55\u5728\u90e8\u5206\u573a\u666f\u4e0b\u6548\u7387\u66f4\u9ad8\u3002", "keywords": "\u63a8\u7406\u65f6\u8ba1\u7b97\uff0c\u56fe\u8fde\u901a\u6027\uff0c\u987a\u5e8f\u6269\u5c55\uff0c\u5e76\u884c\u6269\u5c55\uff0c\u8bed\u8a00\u6a21\u578b"}}
{"id": "2505.21538", "pdf": "https://arxiv.org/pdf/2505.21538", "abs": "https://arxiv.org/abs/2505.21538", "authors": ["Zihan Weng", "Lucas Gomez", "Taylor Whittington Webb", "Pouya Bashivan"], "title": "Caption This, Reason That: VLMs Caught in the Middle", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) have shown remarkable progress in visual\nunderstanding in recent years. Yet, they still lag behind human capabilities in\nspecific visual tasks such as counting or relational reasoning. To understand\nthe underlying limitations, we adopt methodologies from cognitive science,\nanalyzing VLM performance along core cognitive axes: Perception, Attention, and\nMemory. Using a suite of tasks targeting these abilities, we evaluate\nstate-of-the-art VLMs, including GPT-4o. Our analysis reveals distinct\ncognitive profiles: while advanced models approach ceiling performance on some\ntasks (e.g. category identification), a significant gap persists, particularly\nin tasks requiring spatial understanding or selective attention. Investigating\nthe source of these failures and potential methods for improvement, we employ a\nvision-text decoupling analysis, finding that models struggling with direct\nvisual reasoning show marked improvement when reasoning over their own\ngenerated text captions. These experiments reveal a strong need for improved\nVLM Chain-of-Thought (CoT) abilities, even in models that consistently exceed\nhuman performance. Furthermore, we demonstrate the potential of targeted\nfine-tuning on composite visual reasoning tasks and show that fine-tuning\nsmaller VLMs substantially improves core cognitive abilities. While this\nimprovement does not translate to large enhancements on challenging,\nout-of-distribution benchmarks, we show broadly that VLM performance on our\ndatasets strongly correlates with performance on these other benchmarks. Our\nwork provides a detailed analysis of VLM cognitive strengths and weaknesses and\nidentifies key bottlenecks in simultaneous perception and reasoning while also\nproviding an effective and simple solution.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u8ba4\u77e5\u80fd\u529b\uff08\u5982\u611f\u77e5\u3001\u6ce8\u610f\u529b\u548c\u8bb0\u5fc6\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u7a7a\u95f4\u7406\u89e3\u548c\u9009\u62e9\u6027\u6ce8\u610f\u529b\u4efb\u52a1\u4e2d\u4ecd\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u751f\u6210\u6587\u672c\u63cf\u8ff0\u548c\u6539\u8fdb\u63a8\u7406\u80fd\u529b\u6765\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u8ba1\u6570\u6216\u5173\u7cfb\u63a8\u7406\uff09\u4e0a\u4ecd\u843d\u540e\u4e8e\u4eba\u7c7b\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5176\u8ba4\u77e5\u5c40\u9650\u6027\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u4e00\u5957\u9488\u5bf9\u611f\u77e5\u3001\u6ce8\u610f\u529b\u548c\u8bb0\u5fc6\u7684\u4efb\u52a1\u8bc4\u4f30\u73b0\u6709VLM\uff08\u5305\u62ecGPT-4o\uff09\uff0c\u91c7\u7528\u89c6\u89c9-\u6587\u672c\u89e3\u8026\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u76ee\u6807\u5fae\u8c03\u548c\u5c0f\u578bVLM\u4f18\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u5728\u76f4\u63a5\u89c6\u89c9\u63a8\u7406\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u901a\u8fc7\u751f\u6210\u6587\u672c\u63cf\u8ff0\u63a8\u7406\u80fd\u529b\u6709\u663e\u8457\u63d0\u5347\u3002\u5fae\u8c03\u5c0f\u578bVLM\u80fd\u6539\u5584\u6838\u5fc3\u8ba4\u77e5\u80fd\u529b\uff0c\u4f46\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u6548\u679c\u6709\u9650\u3002", "conclusion": "VLM\u5728\u8ba4\u77e5\u80fd\u529b\u4e0a\u5b58\u5728\u660e\u663e\u74f6\u9888\uff0c\u5c24\u5176\u662f\u540c\u65f6\u5904\u7406\u611f\u77e5\u548c\u63a8\u7406\u65f6\u3002\u901a\u8fc7\u6539\u8fdb\u63a8\u7406\u80fd\u529b\u548c\u5fae\u8c03\u53ef\u90e8\u5206\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u8ba4\u77e5\u80fd\u529b, \u611f\u77e5, \u6ce8\u610f\u529b, \u8bb0\u5fc6, \u5fae\u8c03"}}
{"id": "2505.22101", "pdf": "https://arxiv.org/pdf/2505.22101", "abs": "https://arxiv.org/abs/2505.22101", "authors": ["Zhiyu Li", "Shichao Song", "Hanyu Wang", "Simin Niu", "Ding Chen", "Jiawei Yang", "Chenyang Xi", "Huayi Lai", "Jihao Zhao", "Yezhaohui Wang", "Junpeng Ren", "Zehao Lin", "Jiahao Huo", "Tianyi Chen", "Kai Chen", "Kehang Li", "Zhiqiang Yin", "Qingchen Yu", "Bo Tang", "Hongkang Yang", "Zhi-Qin John Xu", "Feiyu Xiong"], "title": "MemOS: An Operating System for Memory-Augmented Generation (MAG) in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as foundational infrastructure in\nthe pursuit of Artificial General Intelligence (AGI). Despite their remarkable\ncapabilities in language perception and generation, current LLMs fundamentally\nlack a unified and structured architecture for handling memory. They primarily\nrely on parametric memory (knowledge encoded in model weights) and ephemeral\nactivation memory (context-limited runtime states). While emerging methods like\nRetrieval-Augmented Generation (RAG) incorporate plaintext memory, they lack\nlifecycle management and multi-modal integration, limiting their capacity for\nlong-term knowledge evolution. To address this, we introduce MemOS, a memory\noperating system designed for LLMs that, for the first time, elevates memory to\na first-class operational resource. It builds unified mechanisms for\nrepresentation, organization, and governance across three core memory types:\nparametric, activation, and plaintext. At its core is the MemCube, a\nstandardized memory abstraction that enables tracking, fusion, and migration of\nheterogeneous memory, while offering structured, traceable access across tasks\nand contexts. MemOS establishes a memory-centric execution framework with\nstrong controllability, adaptability, and evolvability. It fills a critical gap\nin current LLM infrastructure and lays the groundwork for continual adaptation,\npersonalized intelligence, and cross-platform coordination in next-generation\nintelligent systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86MemOS\uff0c\u4e00\u4e2a\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8bbe\u8ba1\u7684\u5185\u5b58\u64cd\u4f5c\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524dLLMs\u7f3a\u4e4f\u7edf\u4e00\u7ed3\u6784\u5316\u5185\u5b58\u7ba1\u7406\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLMs\u4e3b\u8981\u4f9d\u8d56\u53c2\u6570\u5316\u5185\u5b58\u548c\u4e34\u65f6\u6fc0\u6d3b\u5185\u5b58\uff0c\u7f3a\u4e4f\u5bf9\u5185\u5b58\u7684\u7edf\u4e00\u7ba1\u7406\u548c\u751f\u547d\u5468\u671f\u63a7\u5236\uff0c\u5c24\u5176\u662f\u591a\u6a21\u6001\u96c6\u6210\u548c\u957f\u671f\u77e5\u8bc6\u8fdb\u5316\u7684\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u5f15\u5165MemOS\uff0c\u9996\u6b21\u5c06\u5185\u5b58\u63d0\u5347\u4e3a\u4e00\u7c7b\u64cd\u4f5c\u8d44\u6e90\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u62bd\u8c61\uff08MemCube\uff09\u5b9e\u73b0\u5bf9\u53c2\u6570\u5316\u3001\u6fc0\u6d3b\u548c\u660e\u6587\u4e09\u79cd\u5185\u5b58\u7c7b\u578b\u7684\u7edf\u4e00\u8868\u793a\u3001\u7ec4\u7ec7\u548c\u6cbb\u7406\u3002", "result": "MemOS\u5efa\u7acb\u4e86\u5177\u6709\u5f3a\u53ef\u63a7\u6027\u3001\u9002\u5e94\u6027\u548c\u53ef\u8fdb\u5316\u6027\u7684\u5185\u5b58\u4e3a\u4e2d\u5fc3\u7684\u6267\u884c\u6846\u67b6\uff0c\u586b\u8865\u4e86\u73b0\u6709LLM\u57fa\u7840\u8bbe\u65bd\u7684\u7a7a\u767d\u3002", "conclusion": "MemOS\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u7cfb\u7edf\u7684\u6301\u7eed\u9002\u5e94\u3001\u4e2a\u6027\u5316\u667a\u80fd\u548c\u8de8\u5e73\u53f0\u534f\u8c03\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u5185\u5b58\u7ba1\u7406, MemOS, \u591a\u6a21\u6001\u96c6\u6210, \u77e5\u8bc6\u8fdb\u5316"}}
{"id": "2505.21829", "pdf": "https://arxiv.org/pdf/2505.21829", "abs": "https://arxiv.org/abs/2505.21829", "authors": ["Antonio Orvieto", "Robert Gower"], "title": "In Search of Adam's Secret Sauce", "categories": ["cs.LG"], "comment": null, "summary": "Understanding the remarkable efficacy of Adam when training transformer-based\nlanguage models has become a central research topic within the optimization\ncommunity. To gain deeper insights, several simplifications of Adam have been\nproposed, such as the signed gradient and signed momentum methods. In this\nwork, we conduct an extensive empirical study - training over 1,300 language\nmodels across different data configurations and scales - comparing Adam to\nseveral known simplified variants. We find that signed momentum methods are\nfaster than SGD, but consistently underperform relative to Adam, even after\ncareful tuning of momentum, clipping setting and learning rates. However, our\nanalysis reveals a compelling option that preserves near-optimal performance\nwhile allowing for new insightful reformulations: constraining the Adam\nmomentum parameters to be equal. Beyond robust performance, this choice affords\nnew theoretical insights, highlights the \"secret sauce\" on top of signed\nmomentum, and grants a precise statistical interpretation: we show that Adam in\nthis setting implements a natural online algorithm for estimating the mean and\nvariance of gradients-one that arises from a mean-field Gaussian variational\ninference perspective.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u6bd4\u8f83\u4e86Adam\u4f18\u5316\u5668\u53ca\u5176\u7b80\u5316\u53d8\u4f53\uff0c\u53d1\u73b0\u7ea6\u675fAdam\u52a8\u91cf\u53c2\u6570\u76f8\u7b49\u53ef\u4ee5\u5728\u4fdd\u6301\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\u7684\u540c\u65f6\u63d0\u4f9b\u65b0\u7684\u7406\u8bba\u89c1\u89e3\u3002", "motivation": "\u7814\u7a76Adam\u5728\u8bad\u7ec3\u57fa\u4e8eTransformer\u7684\u8bed\u8a00\u6a21\u578b\u65f6\u7684\u5353\u8d8a\u6548\u679c\uff0c\u5e76\u63a2\u7d22\u5176\u7b80\u5316\u53d8\u4f53\u7684\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3\u8d85\u8fc71,300\u4e2a\u8bed\u8a00\u6a21\u578b\uff0c\u5bf9\u6bd4Adam\u53ca\u5176\u7b80\u5316\u53d8\u4f53\uff08\u5982\u5e26\u7b26\u53f7\u52a8\u91cf\u7684\u65b9\u6cd5\uff09\u7684\u6027\u80fd\u3002", "result": "\u5e26\u7b26\u53f7\u52a8\u91cf\u7684\u65b9\u6cd5\u901f\u5ea6\u4f18\u4e8eSGD\uff0c\u4f46\u6027\u80fd\u59cb\u7ec8\u4e0d\u5982Adam\u3002\u7ea6\u675fAdam\u52a8\u91cf\u53c2\u6570\u76f8\u7b49\u53ef\u4fdd\u6301\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u65b0\u7406\u8bba\u89e3\u91ca\u3002", "conclusion": "\u7ea6\u675fAdam\u52a8\u91cf\u53c2\u6570\u76f8\u7b49\u4e0d\u4ec5\u6027\u80fd\u7a33\u5065\uff0c\u8fd8\u63ed\u793a\u4e86Adam\u4f18\u4e8e\u5e26\u7b26\u53f7\u52a8\u91cf\u7684\u5173\u952e\u6240\u5728\uff0c\u5e76\u4ece\u7edf\u8ba1\u89d2\u5ea6\u7ed9\u51fa\u4e86\u89e3\u91ca\u3002", "keywords": "Adam, optimization, transformer, language model, momentum"}}
{"id": "2505.21539", "pdf": "https://arxiv.org/pdf/2505.21539", "abs": "https://arxiv.org/abs/2505.21539", "authors": ["Ziming Wang", "Nan Xue", "Rebecka J\u00f6rnsten"], "title": "Equivariant Flow Matching for Point Cloud Assembly", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The goal of point cloud assembly is to reconstruct a complete 3D shape by\naligning multiple point cloud pieces. This work presents a novel equivariant\nsolver for assembly tasks based on flow matching models. We first theoretically\nshow that the key to learning equivariant distributions via flow matching is to\nlearn related vector fields. Based on this result, we propose an assembly\nmodel, called equivariant diffusion assembly (Eda), which learns related vector\nfields conditioned on the input pieces. We further construct an equivariant\npath for Eda, which guarantees high data efficiency of the training process.\nOur numerical results show that Eda is highly competitive on practical\ndatasets, and it can even handle the challenging situation where the input\npieces are non-overlapped.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u6d41\u5339\u914d\u6a21\u578b\u7684\u7b49\u53d8\u6c42\u89e3\u5668Eda\uff0c\u7528\u4e8e\u70b9\u4e91\u7ec4\u88c5\u4efb\u52a1\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u9ad8\u6548\u6027\uff0c\u5e76\u5728\u975e\u91cd\u53e0\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u70b9\u4e91\u7ec4\u88c5\u4efb\u52a1\u4e2d\u5b8c\u65743D\u5f62\u72b6\u7684\u91cd\u5efa\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u8f93\u5165\u70b9\u4e91\u7247\u6bb5\u975e\u91cd\u53e0\u65f6\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u6d41\u5339\u914d\u6a21\u578b\u5b66\u4e60\u7b49\u53d8\u5206\u5e03\u7684\u76f8\u5173\u5411\u91cf\u573a\uff0c\u6784\u5efa\u7b49\u53d8\u8def\u5f84\uff08Eda\u6a21\u578b\uff09\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u6570\u636e\u6548\u7387\u3002", "result": "Eda\u5728\u5b9e\u9645\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u51fa\u9ad8\u5ea6\u7ade\u4e89\u529b\uff0c\u5e76\u80fd\u5904\u7406\u975e\u91cd\u53e0\u8f93\u5165\u7247\u6bb5\u7684\u590d\u6742\u60c5\u51b5\u3002", "conclusion": "Eda\u901a\u8fc7\u7406\u8bba\u652f\u6301\u7684\u7b49\u53d8\u65b9\u6cd5\uff0c\u9ad8\u6548\u89e3\u51b3\u4e86\u70b9\u4e91\u7ec4\u88c5\u95ee\u9898\uff0c\u5c24\u5176\u5728\u975e\u91cd\u53e0\u6761\u4ef6\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "keywords": "\u70b9\u4e91\u7ec4\u88c5\uff0c3D\u91cd\u5efa\uff0c\u7b49\u53d8\u6c42\u89e3\u5668\uff0c\u6d41\u5339\u914d\uff0c\u975e\u91cd\u53e0\u7247\u6bb5"}}
{"id": "2505.22107", "pdf": "https://arxiv.org/pdf/2505.22107", "abs": "https://arxiv.org/abs/2505.22107", "authors": ["Shuhai Zhang", "Zeng You", "Yaofo Chen", "Zhiquan Wen", "Qianyue Wang", "Zhijie Qiu", "Yuanqing Li", "Mingkui Tan"], "title": "Curse of High Dimensionality Issue in Transformer for Long-context Modeling", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at ICML 2025", "summary": "Transformer-based large language models (LLMs) excel in natural language\nprocessing tasks by capturing long-range dependencies through self-attention\nmechanisms. However, long-context modeling faces significant computational\ninefficiencies due to \\textit{redundant} attention computations: while\nattention weights are often \\textit{sparse}, all tokens consume \\textit{equal}\ncomputational resources. In this paper, we reformulate traditional\nprobabilistic sequence modeling as a \\textit{supervised learning task},\nenabling the separation of relevant and irrelevant tokens and providing a\nclearer understanding of redundancy. Based on this reformulation, we\ntheoretically analyze attention sparsity, revealing that only a few tokens\nsignificantly contribute to predictions. Building on this, we formulate\nattention optimization as a linear coding problem and propose a \\textit{group\ncoding strategy}, theoretically showing its ability to improve robustness\nagainst random noise and enhance learning efficiency. Motivated by this, we\npropose \\textit{Dynamic Group Attention} (DGA), which leverages the group\ncoding to explicitly reduce redundancy by aggregating less important tokens\nduring attention computation. Empirical results show that our DGA significantly\nreduces computational costs while maintaining competitive performance.Code is\navailable at https://github.com/bolixinyu/DynamicGroupAttention.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u5206\u7ec4\u6ce8\u610f\u529b\uff08DGA\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u7ec4\u7f16\u7801\u7b56\u7565\u4f18\u5316\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u51cf\u5c11\u5197\u4f59\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6ce8\u610f\u529b\u8ba1\u7b97\u4e2d\u7684\u5197\u4f59\u95ee\u9898\u5bfc\u81f4\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u7684\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u5206\u79bb\u76f8\u5173\u548c\u4e0d\u76f8\u5173\u7684\u6807\u8bb0\u3002", "method": "\u5c06\u5e8f\u5217\u5efa\u6a21\u91cd\u65b0\u8868\u8ff0\u4e3a\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\uff0c\u5206\u6790\u6ce8\u610f\u529b\u7a00\u758f\u6027\u5e76\u63d0\u51fa\u5206\u7ec4\u7f16\u7801\u7b56\u7565\uff0c\u6700\u7ec8\u8bbe\u8ba1\u52a8\u6001\u5206\u7ec4\u6ce8\u610f\u529b\uff08DGA\uff09\u3002", "result": "DGA\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u3002", "conclusion": "\u52a8\u6001\u5206\u7ec4\u6ce8\u610f\u529b\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u6ce8\u610f\u529b\u4f18\u5316\u65b9\u6cd5\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u6ce8\u610f\u529b\u673a\u5236, \u8ba1\u7b97\u6548\u7387, \u5206\u7ec4\u7f16\u7801, \u52a8\u6001\u5206\u7ec4\u6ce8\u610f\u529b"}}
{"id": "2505.21835", "pdf": "https://arxiv.org/pdf/2505.21835", "abs": "https://arxiv.org/abs/2505.21835", "authors": ["Xiangyu Chen", "Jing Liu", "Ye Wang", "Matthew Brand", "Pu", "Wang", "Toshiaki Koike-Akino"], "title": "TuneComp: Joint Fine-tuning and Compression for Large Foundation Models", "categories": ["cs.LG", "cs.AI"], "comment": "Preliminary Work", "summary": "To reduce model size during post-training, compression methods, including\nknowledge distillation, low-rank approximation, and pruning, are often applied\nafter fine-tuning the model. However, sequential fine-tuning and compression\nsacrifices performance, while creating a larger than necessary model as an\nintermediate step. In this work, we aim to reduce this gap, by directly\nconstructing a smaller model while guided by the downstream task. We propose to\njointly fine-tune and compress the model by gradually distilling it to a pruned\nlow-rank structure. Experiments demonstrate that joint fine-tuning and\ncompression significantly outperforms other sequential compression methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u5fae\u8c03\u4e0e\u538b\u7f29\u7684\u65b9\u6cd5\uff0c\u76f4\u63a5\u6784\u5efa\u5c0f\u578b\u6a21\u578b\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u5206\u6b65\u538b\u7f29\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5fae\u8c03\u540e\u5206\u6b65\u538b\u7f29\u6a21\u578b\u6027\u80fd\u635f\u5931\u5927\u4e14\u4ea7\u751f\u4e0d\u5fc5\u8981\u7684\u5927\u6a21\u578b\u4e2d\u95f4\u6b65\u9aa4\uff0c\u7814\u7a76\u65e8\u5728\u51cf\u5c11\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u8054\u5408\u5fae\u8c03\u4e0e\u538b\u7f29\uff0c\u9010\u6b65\u84b8\u998f\u81f3\u526a\u679d\u7684\u4f4e\u79e9\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8054\u5408\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u5206\u6b65\u538b\u7f29\u65b9\u6cd5\u3002", "conclusion": "\u8054\u5408\u5fae\u8c03\u4e0e\u538b\u7f29\u80fd\u9ad8\u6548\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u4e14\u4fdd\u6301\u6027\u80fd\u3002", "keywords": "\u6a21\u578b\u538b\u7f29, \u77e5\u8bc6\u84b8\u998f, \u4f4e\u79e9\u8fd1\u4f3c, \u526a\u679d, \u8054\u5408\u5fae\u8c03"}}
{"id": "2505.21541", "pdf": "https://arxiv.org/pdf/2505.21541", "abs": "https://arxiv.org/abs/2505.21541", "authors": ["Zitong Wang", "Hang Zhao", "Qianyu Zhou", "Xuequan Lu", "Xiangtai Li", "Yiren Song"], "title": "DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models have recently motivated great success in many generation\ntasks like object removal. Nevertheless, existing image decomposition methods\nstruggle to disentangle semi-transparent or transparent layer occlusions due to\nmask prior dependencies, static object assumptions, and the lack of datasets.\nIn this paper, we delve into a novel task: Layer-Wise Decomposition of\nAlpha-Composited Images, aiming to recover constituent layers from single\noverlapped images under the condition of semi-transparent/transparent alpha\nlayer non-linear occlusion. To address challenges in layer ambiguity,\ngeneralization, and data scarcity, we first introduce AlphaBlend, the first\nlarge-scale and high-quality dataset for transparent and semi-transparent layer\ndecomposition, supporting six real-world subtasks (e.g., translucent flare\nremoval, semi-transparent cell decomposition, glassware decomposition).\nBuilding on this dataset, we present DiffDecompose, a diffusion\nTransformer-based framework that learns the posterior over possible layer\ndecompositions conditioned on the input image, semantic prompts, and blending\ntype. Rather than regressing alpha mattes directly, DiffDecompose performs\nIn-Context Decomposition, enabling the model to predict one or multiple layers\nwithout per-layer supervision, and introduces Layer Position Encoding Cloning\nto maintain pixel-level correspondence across layers. Extensive experiments on\nthe proposed AlphaBlend dataset and public LOGO dataset verify the\neffectiveness of DiffDecompose. The code and dataset will be available upon\npaper acceptance. Our code will be available at:\nhttps://github.com/Wangzt1121/DiffDecompose.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86DiffDecompose\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5355\u5f20\u534a\u900f\u660e/\u900f\u660e\u53e0\u52a0\u56fe\u50cf\u4e2d\u5206\u89e3\u51fa\u7ec4\u6210\u5c42\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5c42\u89e3\u8026\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u53d1\u5e03\u4e86AlphaBlend\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u5206\u89e3\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u534a\u900f\u660e\u6216\u900f\u660e\u5c42\u7684\u906e\u6321\u95ee\u9898\uff0c\u9700\u4f9d\u8d56\u63a9\u6a21\u5148\u9a8c\u3001\u9759\u6001\u7269\u4f53\u5047\u8bbe\uff0c\u4e14\u7f3a\u4e4f\u6570\u636e\u96c6\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u5c42\u5206\u89e3\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6269\u6563Transformer\u7684DiffDecompose\u6846\u67b6\uff0c\u5229\u7528\u4e0a\u4e0b\u6587\u5206\u89e3\uff08In-Context Decomposition\uff09\u548c\u5c42\u4f4d\u7f6e\u7f16\u7801\u514b\u9686\uff08Layer Position Encoding Cloning\uff09\uff0c\u65e0\u9700\u9010\u5c42\u76d1\u7763\u5373\u53ef\u9884\u6d4b\u591a\u5c42\u3002", "result": "\u5728AlphaBlend\u6570\u636e\u96c6\u548c\u516c\u5f00LOGO\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DiffDecompose\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u662f\u5728\u534a\u900f\u660e/\u900f\u660e\u5c42\u5206\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DiffDecompose\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u89e3\u51b3\u4e86\u5c42\u5206\u89e3\u4e2d\u7684\u6a21\u7cca\u6027\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u56fe\u50cf\u5206\u89e3\u3001\u6269\u6563\u6a21\u578b\u3001\u534a\u900f\u660e\u5c42\u3001AlphaBlend\u6570\u636e\u96c6\u3001Transformer"}}
{"id": "2505.22113", "pdf": "https://arxiv.org/pdf/2505.22113", "abs": "https://arxiv.org/abs/2505.22113", "authors": ["Zhiyuan Li", "Yi Chang", "Yuan Wu"], "title": "THINK-Bench: Evaluating Thinking Efficiency and Chain-of-Thought Quality of Large Reasoning Models", "categories": ["cs.CL"], "comment": "20 pages, 8 figures, 6 tables", "summary": "Large reasoning models (LRMs) have achieved impressive performance in complex\ntasks, often outperforming conventional large language models (LLMs). However,\nthe prevalent issue of overthinking severely limits their computational\nefficiency. Overthinking occurs when models generate excessive and redundant\ntokens that contribute little to accurate outcomes, especially in simple tasks,\nresulting in a significant waste of computational resources. To systematically\ninvestigate this issue, we introduce Think-Bench, a benchmark designed to\nevaluate the reasoning efficiency of LRMs. We also propose novel efficiency\nmetrics and conduct a comprehensive evaluation of various LRMs across multiple\ndimensions, including the reasoning process, outcome quality, and\nchain-of-thought (CoT) characteristics. Our analysis reveals that most LRMs\nexhibit overthinking in handling easy questions, generating unnecessarily\nlengthy reasoning chains. While many LRMs demonstrate high CoT quality, several\nsuffer from low efficiency. We hope that Think-Bench can serve as a robust\nfoundation for advancing research into LRMs.", "AI": {"tldr": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u7684\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u95ee\u9898\u3002\u672c\u6587\u63d0\u51faThink-Bench\u57fa\u51c6\u548c\u6548\u7387\u6307\u6807\uff0c\u8bc4\u4f30LRMs\u5728\u591a\u7ef4\u5ea6\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u591a\u6570\u6a21\u578b\u5728\u5904\u7406\u7b80\u5355\u95ee\u9898\u65f6\u751f\u6210\u5197\u4f59\u63a8\u7406\u94fe\uff0c\u6548\u7387\u8f83\u4f4e\u3002", "motivation": "\u7814\u7a76LRMs\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u56e0\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u5bfc\u81f4\u7684\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u95ee\u9898\uff0c\u4ee5\u7cfb\u7edf\u6027\u8bc4\u4f30\u548c\u6539\u8fdb\u63a8\u7406\u6548\u7387\u3002", "method": "\u63d0\u51faThink-Bench\u57fa\u51c6\u548c\u65b0\u578b\u6548\u7387\u6307\u6807\uff0c\u5bf9\u591a\u79cdLRMs\u8fdb\u884c\u63a8\u7406\u8fc7\u7a0b\u3001\u7ed3\u679c\u8d28\u91cf\u548c\u601d\u7ef4\u94fe\u7279\u6027\u7684\u591a\u7ef4\u8bc4\u4f30\u3002", "result": "\u591a\u6570LRMs\u5728\u7b80\u5355\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8fc7\u5ea6\u601d\u8003\uff0c\u751f\u6210\u5197\u957f\u63a8\u7406\u94fe\uff1b\u90e8\u5206\u6a21\u578b\u601d\u7ef4\u94fe\u8d28\u91cf\u9ad8\u4f46\u6548\u7387\u4f4e\u3002", "conclusion": "Think-Bench\u4e3aLRMs\u7814\u7a76\u63d0\u4f9b\u575a\u5b9e\u57fa\u7840\uff0c\u63a8\u52a8\u9ad8\u6548\u63a8\u7406\u6a21\u578b\u7684\u5f00\u53d1\u3002", "keywords": "\u5927\u578b\u63a8\u7406\u6a21\u578b, \u8fc7\u5ea6\u601d\u8003, \u63a8\u7406\u6548\u7387, Think-Bench, \u601d\u7ef4\u94fe"}}
{"id": "2505.21841", "pdf": "https://arxiv.org/pdf/2505.21841", "abs": "https://arxiv.org/abs/2505.21841", "authors": ["Jiahui Zhu", "Kihyun Yu", "Dabeen Lee", "Xin Liu", "Honghao Wei"], "title": "An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints", "categories": ["cs.LG", "cs.AI"], "comment": "Proceedings of the 41 st International Conference on Machine Learning", "summary": "Online safe reinforcement learning (RL) plays a key role in dynamic\nenvironments, with applications in autonomous driving, robotics, and\ncybersecurity. The objective is to learn optimal policies that maximize rewards\nwhile satisfying safety constraints modeled by constrained Markov decision\nprocesses (CMDPs). Existing methods achieve sublinear regret under stochastic\nconstraints but often fail in adversarial settings, where constraints are\nunknown, time-varying, and potentially adversarially designed. In this paper,\nwe propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the\nfirst to address online CMDPs with anytime adversarial constraints. OMDPD\nachieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K))\nwithout relying on Slater's condition or the existence of a strictly known safe\npolicy. We further show that access to accurate estimates of rewards and\ntransitions can further improve these bounds. Our results offer practical\nguarantees for safe decision-making in adversarial environments.", "AI": {"tldr": "OMDPD\u7b97\u6cd5\u9996\u6b21\u89e3\u51b3\u4e86\u5728\u7ebf\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08CMDP\uff09\u4e2d\u5bf9\u6297\u6027\u7ea6\u675f\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6700\u4f18\u540e\u6094\u548c\u5f3a\u7ea6\u675f\u8fdd\u53cd\u8fb9\u754c\uff0c\u65e0\u9700\u4f9d\u8d56\u4e25\u683c\u5df2\u77e5\u7684\u5b89\u5168\u7b56\u7565\u6216Slater\u6761\u4ef6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u968f\u673a\u7ea6\u675f\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5bf9\u6297\u6027\u7ea6\u675f\u73af\u5883\u4e2d\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7b97\u6cd5\u6765\u5904\u7406\u672a\u77e5\u3001\u65f6\u53d8\u548c\u5bf9\u6297\u6027\u8bbe\u8ba1\u7684\u7ea6\u675f\u3002", "method": "\u63d0\u51fa\u4e86\u4e50\u89c2\u955c\u50cf\u4e0b\u964d\u539f\u5bf9\u5076\uff08OMDPD\uff09\u7b97\u6cd5\uff0c\u65e0\u9700\u4f9d\u8d56Slater\u6761\u4ef6\u6216\u4e25\u683c\u5df2\u77e5\u7684\u5b89\u5168\u7b56\u7565\u3002", "result": "OMDPD\u5b9e\u73b0\u4e86\u6700\u4f18\u540e\u6094O(\u221aK)\u548c\u5f3a\u7ea6\u675f\u8fdd\u53cdO(\u221aK)\uff0c\u5e76\u4e14\u5728\u5956\u52b1\u548c\u8f6c\u79fb\u4f30\u8ba1\u51c6\u786e\u65f6\u80fd\u8fdb\u4e00\u6b65\u6539\u8fdb\u8fb9\u754c\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u5bf9\u6297\u6027\u73af\u5883\u4e2d\u7684\u5b89\u5168\u51b3\u7b56\u63d0\u4f9b\u4e86\u5b9e\u7528\u4fdd\u8bc1\u3002", "keywords": "\u5728\u7ebf\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u3001\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08CMDP\uff09\u3001\u5bf9\u6297\u6027\u7ea6\u675f\u3001OMDPD\u7b97\u6cd5"}}
{"id": "2505.21547", "pdf": "https://arxiv.org/pdf/2505.21547", "abs": "https://arxiv.org/abs/2505.21547", "authors": ["Weixing Wang", "Zifeng Ding", "Jindong Gu", "Rui Cao", "Christoph Meinel", "Gerard de Melo", "Haojin Yang"], "title": "Image Tokens Matter: Mitigating Hallucination in Discrete Tokenizer-based Large Vision-Language Models via Latent Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) with discrete image tokenizers unify\nmultimodal representations by encoding visual inputs into a finite set of\ntokens. Despite their effectiveness, we find that these models still\nhallucinate non-existent objects. We hypothesize that this may be due to visual\npriors induced during training: When certain image tokens frequently co-occur\nin the same spatial regions and represent shared objects, they become strongly\nassociated with the verbalizations of those objects. As a result, the model may\nhallucinate by evoking visually absent tokens that often co-occur with present\nones. To test this assumption, we construct a co-occurrence graph of image\ntokens using a segmentation dataset and employ a Graph Neural Network (GNN)\nwith contrastive learning followed by a clustering method to group tokens that\nfrequently co-occur in similar visual contexts. We find that hallucinations\npredominantly correspond to clusters whose tokens dominate the input, and more\nspecifically, that the visually absent tokens in those clusters show much\nhigher correlation with hallucinated objects compared to tokens present in the\nimage. Based on this observation, we propose a hallucination mitigation method\nthat suppresses the influence of visually absent tokens by modifying latent\nimage embeddings during generation. Experiments show our method reduces\nhallucinations while preserving expressivity. Code is available at\nhttps://github.com/weixingW/CGC-VTD/tree/main", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u56fe\u50cf\u6807\u8bb0\u7684\u5171\u73b0\u5173\u7cfb\u53ca\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u51cf\u5c11\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u5373\u751f\u6210\u4e0d\u5b58\u5728\u7684\u5bf9\u8c61\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u89c6\u89c9\u8f93\u5165\u65f6\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff08\u5373\u751f\u6210\u975e\u771f\u5b9e\u5b58\u5728\u7684\u5bf9\u8c61\uff09\uff0c\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u53ef\u80fd\u662f\u7531\u4e8e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u89c6\u89c9\u5148\u9a8c\u5bfc\u81f4\u7684\u67d0\u4e9b\u6807\u8bb0\u4e0e\u5bf9\u8c61\u7684\u5f3a\u70c8\u5173\u8054\u6027\u3002", "method": "1. \u6784\u5efa\u56fe\u50cf\u6807\u8bb0\u5171\u73b0\u56fe\uff1b2. \u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u5bf9\u6bd4\u5b66\u4e60\u5206\u6790\u6807\u8bb0\u7684\u5171\u73b0\u6a21\u5f0f\uff1b3. \u63d0\u51fa\u6291\u5236\u89c6\u89c9\u4e0a\u4e0d\u5b58\u5728\u7684\u6807\u8bb0\u5f71\u54cd\u7684\u5d4c\u5165\u4fee\u6539\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\u73b0\u8c61\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790\u6807\u8bb0\u5171\u73b0\u5173\u7cfb\u5e76\u8c03\u6574\u6f5c\u5728\u5d4c\u5165\uff0c\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u6a21\u578b\u5e7b\u89c9\u73b0\u8c61\uff0c\u63d0\u5347\u53ef\u9760\u6027\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u5e7b\u89c9\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u3001\u5171\u73b0\u5206\u6790\u3001\u5bf9\u6bd4\u5b66\u4e60"}}
{"id": "2505.22116", "pdf": "https://arxiv.org/pdf/2505.22116", "abs": "https://arxiv.org/abs/2505.22116", "authors": ["Jintao Zhang", "Zirui Liu", "Mingyue Cheng", "Shilong Zhang", "Tingyue Pan", "Qi Liu", "Yanhu Xie"], "title": "Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Intraoperative hypotension (IOH) frequently occurs under general anesthesia\nand is strongly linked to adverse outcomes such as myocardial injury and\nincreased mortality. Despite its significance, IOH prediction is hindered by\nevent sparsity and the challenge of integrating static and dynamic data across\ndiverse patients. In this paper, we propose \\textbf{IOHFuseLM}, a multimodal\nlanguage model framework. To accurately identify and differentiate sparse\nhypotensive events, we leverage a two-stage training strategy. The first stage\ninvolves domain adaptive pretraining on IOH physiological time series augmented\nthrough diffusion methods, thereby enhancing the model sensitivity to patterns\nassociated with hypotension. Subsequently, task fine-tuning is performed on the\noriginal clinical dataset to further enhance the ability to distinguish\nnormotensive from hypotensive states. To enable multimodal fusion for each\npatient, we align structured clinical descriptions with the corresponding\nphysiological time series at the token level. Such alignment enables the model\nto capture individualized temporal patterns alongside their corresponding\nclinical semantics. In addition, we convert static patient attributes into\nstructured text to enrich personalized information. Experimental evaluations on\ntwo intraoperative datasets demonstrate that IOHFuseLM outperforms established\nbaselines in accurately identifying IOH events, highlighting its applicability\nin clinical decision support scenarios. Our code is publicly available to\npromote reproducibility at https://github.com/zjt-gpu/IOHFuseLM.", "AI": {"tldr": "\u6458\u8981\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aIOHFuseLM\u7684\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u672f\u4e2d\u4f4e\u8840\u538b\u4e8b\u4ef6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u589e\u5f3a\u6a21\u578b\u5bf9\u4f4e\u8840\u538b\u6a21\u5f0f\u7684\u5206\u8fa8\u80fd\u529b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u672f\u4e2d\u4f4e\u8840\u538b\uff08IOH\uff09\u4e0e\u4e0d\u826f\u4e34\u5e8a\u7ed3\u679c\u5bc6\u5207\u76f8\u5173\uff0c\u4f46\u7531\u4e8e\u4e8b\u4ef6\u7a00\u758f\u6027\u4ee5\u53ca\u9759\u6001\u548c\u52a8\u6001\u6570\u636e\u6574\u5408\u56f0\u96be\uff0c\u5176\u9884\u6d4b\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a1. \u57fa\u4e8e\u6269\u6563\u65b9\u6cd5\u7684\u751f\u7406\u65f6\u95f4\u5e8f\u5217\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\uff1b2. \u5728\u539f\u59cb\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4efb\u52a1\u5fae\u8c03\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u4ee4\u724c\u7ea7\u5bf9\u9f50\u5b9e\u73b0\u591a\u6a21\u6001\u878d\u5408\uff0c\u5e76\u7ed3\u5408\u9759\u6001\u5c5e\u6027\u7684\u7ed3\u6784\u5316\u6587\u672c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cIOHFuseLM\u5728\u4e24\u4e2a\u672f\u4e2d\u6570\u636e\u96c6\u4e0a\u7684\u4f4e\u8840\u538b\u4e8b\u4ef6\u8bc6\u522b\u51c6\u786e\u7387\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "IOHFuseLM\u5728\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4ee3\u7801\u5df2\u516c\u5f00\u4ee5\u4fc3\u8fdb\u53ef\u91cd\u590d\u6027\u3002", "keywords": "\u672f\u4e2d\u4f4e\u8840\u538b\u3001\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u3001\u6269\u6563\u65b9\u6cd5\u3001\u4e24\u9636\u6bb5\u8bad\u7ec3\u3001\u4e34\u5e8a\u51b3\u7b56\u652f\u6301"}}
{"id": "2505.21852", "pdf": "https://arxiv.org/pdf/2505.21852", "abs": "https://arxiv.org/abs/2505.21852", "authors": ["Akifumi Wachi", "Kohei Miyaguchi", "Takumi Tanabe", "Rei Sato", "Youhei Akimoto"], "title": "A Provable Approach for End-to-End Safe Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.IT", "cs.RO", "math.IT"], "comment": "27 pages", "summary": "A longstanding goal in safe reinforcement learning (RL) is a method to ensure\nthe safety of a policy throughout the entire process, from learning to\noperation. However, existing safe RL paradigms inherently struggle to achieve\nthis objective. We propose a method, called Provably Lifetime Safe RL (PLS),\nthat integrates offline safe RL with safe policy deployment to address this\nchallenge. Our proposed method learns a policy offline using return-conditioned\nsupervised learning and then deploys the resulting policy while cautiously\noptimizing a limited set of parameters, known as target returns, using Gaussian\nprocesses (GPs). Theoretically, we justify the use of GPs by analyzing the\nmathematical relationship between target and actual returns. We then prove that\nPLS finds near-optimal target returns while guaranteeing safety with high\nprobability. Empirically, we demonstrate that PLS outperforms baselines both in\nsafety and reward performance, thereby achieving the longstanding goal to\nobtain high rewards while ensuring the safety of a policy throughout the\nlifetime from learning to operation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a'\u53ef\u8bc1\u660e\u7ec8\u8eab\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\uff08PLS\uff09'\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u79bb\u7ebf\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u4e0e\u5b89\u5168\u7b56\u7565\u90e8\u7f72\uff0c\u786e\u4fdd\u7b56\u7565\u4ece\u5b66\u4e60\u5230\u8fd0\u884c\u7684\u6574\u4e2a\u751f\u547d\u5468\u671f\u5185\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u786e\u4fdd\u7b56\u7565\u4ece\u5b66\u4e60\u5230\u8fd0\u884c\u7684\u6574\u4e2a\u8fc7\u7a0b\u4e2d\u7684\u5b89\u5168\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u4fdd\u8bc1\u7b56\u7565\u7ec8\u8eab\u5b89\u5168\u7684\u65b9\u6cd5\u3002", "method": "PLS\u65b9\u6cd5\u901a\u8fc7\u79bb\u7ebf\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\uff08\u57fa\u4e8e\u56de\u62a5\u6761\u4ef6\u7684\u76d1\u7763\u5b66\u4e60\uff09\u5b66\u4e60\u7b56\u7565\uff0c\u5e76\u5728\u90e8\u7f72\u65f6\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\uff08GPs\uff09\u8c28\u614e\u4f18\u5316\u76ee\u6807\u56de\u62a5\u53c2\u6570\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0cPLS\u80fd\u5728\u9ad8\u6982\u7387\u4e0b\u4fdd\u8bc1\u5b89\u5168\u6027\u5e76\u627e\u5230\u63a5\u8fd1\u6700\u4f18\u7684\u76ee\u6807\u56de\u62a5\u3002\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\uff0cPLS\u5728\u5b89\u5168\u6027\u548c\u5956\u52b1\u8868\u73b0\u4e0a\u5747\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "PLS\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5728\u4fdd\u8bc1\u7b56\u7565\u7ec8\u8eab\u5b89\u5168\u7684\u540c\u65f6\u83b7\u5f97\u9ad8\u5956\u52b1\u7684\u957f\u671f\u76ee\u6807\u3002", "keywords": "\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\uff0c\u7ec8\u8eab\u5b89\u5168\uff0c\u79bb\u7ebf\u5b66\u4e60\uff0c\u9ad8\u65af\u8fc7\u7a0b\uff0c\u7b56\u7565\u90e8\u7f72"}}
{"id": "2505.21548", "pdf": "https://arxiv.org/pdf/2505.21548", "abs": "https://arxiv.org/abs/2505.21548", "authors": ["Dhruv Agarwal", "Anya Shukla", "Sunayana Sitaram", "Aditya Vashistha"], "title": "Fluent but Culturally Distant: Can Regional Training Teach Cultural Understanding?", "categories": ["physics.soc-ph", "cs.AI", "cs.CL", "cs.CY"], "comment": "Under review", "summary": "Large language models (LLMs) are used around the world but exhibit Western\ncultural tendencies. To address this cultural misalignment, many countries have\nbegun developing \"regional\" LLMs tailored to local communities. Yet it remains\nunclear whether these models merely speak the language of their users or also\nreflect their cultural values and practices. Using India as a case study, we\nevaluate five Indic and five global LLMs along two key dimensions: values (via\nthe Inglehart-Welzel map and GlobalOpinionQA) and practices (via CulturalBench\nand NormAd). Across all four tasks, we find that Indic models do not align more\nclosely with Indian cultural norms than global models. In fact, an average\nAmerican person is a better proxy for Indian cultural values than any Indic\nmodel. Even prompting strategies fail to meaningfully improve alignment.\nAblations show that regional fine-tuning does not enhance cultural competence\nand may in fact hurt it by impeding recall of existing knowledge. We trace this\nfailure to the scarcity of high-quality, untranslated, and culturally grounded\npretraining and fine-tuning data. Our study positions cultural evaluation as a\nfirst-class requirement alongside multilingual benchmarks and offers a reusable\nmethodology for developers. We call for deeper investments in culturally\nrepresentative data to build and evaluate truly sovereign LLMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5370\u5ea6\u672c\u571f\u7684LLM\u5728\u6587\u5316\u4ef7\u503c\u89c2\u548c\u5b9e\u8df5\u4e0a\u5e76\u672a\u6bd4\u5168\u7403LLM\u66f4\u8d34\u8fd1\u5370\u5ea6\u6587\u5316\uff0c\u751a\u81f3\u7f8e\u56fd\u666e\u901a\u4eba\u66f4\u80fd\u4ee3\u8868\u5370\u5ea6\u6587\u5316\u4ef7\u503c\u3002\u5730\u57df\u5fae\u8c03\u53cd\u800c\u53ef\u80fd\u524a\u5f31\u6587\u5316\u9002\u5e94\u6027\uff0c\u4e3b\u56e0\u662f\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u672c\u571f\u6587\u5316\u6570\u636e\u3002", "motivation": "\u4e3a\u4e86\u63a2\u7a76\u533a\u57dfLLM\u662f\u5426\u771f\u6b63\u53cd\u6620\u672c\u571f\u6587\u5316\u4ef7\u503c\u89c2\u548c\u5b9e\u8df5\uff0c\u800c\u4e0d\u4ec5\u662f\u8bed\u8a00\u9002\u914d\u3002", "method": "\u901a\u8fc7\u8bc4\u4f30\u4e94\u4e2a\u5370\u5ea6\u672c\u571f\u548c\u4e94\u4e2a\u5168\u7403LLM\uff0c\u4f7f\u7528Inglehart-Welzel\u5730\u56fe\u3001GlobalOpinionQA\u3001CulturalBench\u548cNormAd\u7b49\u5de5\u5177\u5206\u6790\u5176\u6587\u5316\u548c\u5b9e\u8df5\u4e00\u81f4\u6027\u3002", "result": "\u5370\u5ea6\u672c\u571f\u6a21\u578b\u5728\u6587\u5316\u548c\u5b9e\u8df5\u4efb\u52a1\u4e2d\u672a\u4f18\u4e8e\u5168\u7403\u6a21\u578b\uff1b\u5730\u57df\u5fae\u8c03\u751a\u81f3\u53ef\u80fd\u524a\u5f31\u6587\u5316\u9002\u914d\u6027\uff0c\u4e14\u63d0\u793a\u7b56\u7565\u65e0\u6548\u3002", "conclusion": "\u9ad8\u8d28\u91cf\u7684\u672c\u571f\u6587\u5316\u6570\u636e\u7a00\u7f3a\u662f\u4e3b\u8981\u74f6\u9888\uff0c\u9700\u52a0\u5927\u6295\u5165\u4ee5\u6784\u5efa\u771f\u6b63\u5177\u6709\u6587\u5316\u4ee3\u8868\u6027\u7684LLM\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u6587\u5316\u9002\u914d\u6027, \u5370\u5ea6\u6587\u5316, \u672c\u571f\u5316LLM, \u6570\u636e\u7a00\u7f3a"}}
{"id": "2505.22118", "pdf": "https://arxiv.org/pdf/2505.22118", "abs": "https://arxiv.org/abs/2505.22118", "authors": ["Alan Ramponi", "Marco Rovera", "Robert Moro", "Sara Tonelli"], "title": "Multilingual vs Crosslingual Retrieval of Fact-Checked Claims: A Tale of Two Approaches", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval of previously fact-checked claims is a well-established task, whose\nautomation can assist professional fact-checkers in the initial steps of\ninformation verification. Previous works have mostly tackled the task\nmonolingually, i.e., having both the input and the retrieved claims in the same\nlanguage. However, especially for languages with a limited availability of\nfact-checks and in case of global narratives, such as pandemics, wars, or\ninternational politics, it is crucial to be able to retrieve claims across\nlanguages. In this work, we examine strategies to improve the multilingual and\ncrosslingual performance, namely selection of negative examples (in the\nsupervised) and re-ranking (in the unsupervised setting). We evaluate all\napproaches on a dataset containing posts and claims in 47 languages (283\nlanguage combinations). We observe that the best results are obtained by using\nLLM-based re-ranking, followed by fine-tuning with negative examples sampled\nusing a sentence similarity-based strategy. Most importantly, we show that\ncrosslinguality is a setup with its own unique characteristics compared to the\nmultilingual setup.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u8d1f\u4f8b\u9009\u62e9\u548c\u91cd\u65b0\u6392\u5e8f\u7b56\u7565\u6539\u8fdb\u591a\u8bed\u8a00\u548c\u8de8\u8bed\u8a00\u73af\u5883\u4e0b\u4e8b\u5b9e\u6838\u67e5\u58f0\u660e\u7684\u68c0\u7d22\u6548\u679c\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eLLM\u7684\u91cd\u65b0\u6392\u5e8f\u65b9\u6cd5\u548c\u7279\u5b9a\u8d1f\u4f8b\u91c7\u6837\u7b56\u7565\u6548\u679c\u6700\u4f73\uff0c\u4e14\u8de8\u8bed\u8a00\u73af\u5883\u4e0b\u5177\u6709\u72ec\u7279\u7279\u70b9\u3002", "motivation": "\u5c3d\u7ba1\u4e8b\u5b9e\u6838\u67e5\u58f0\u660e\u7684\u81ea\u52a8\u5316\u68c0\u7d22\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u591a\u96c6\u4e2d\u4e8e\u5355\u8bed\u8a00\u73af\u5883\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u8d44\u6e90\u532e\u4e4f\u7684\u8bed\u8a00\u6216\u5168\u7403\u6027\u8bae\u9898\uff0c\u8de8\u8bed\u8a00\u68c0\u7d22\u5c24\u4e3a\u91cd\u8981\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u5347\u591a\u8bed\u8a00\u548c\u8de8\u8bed\u8a00\u68c0\u7d22\u7684\u6548\u80fd\u3002", "method": "\u7814\u7a76\u5bf9\u6bd4\u4e86\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u8d1f\u4f8b\u9009\u62e9\u7b56\u7565\u548c\u65e0\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u91cd\u65b0\u6392\u5e8f\u65b9\u6cd5\uff0c\u5e76\u572847\u79cd\u8bed\u8a00\uff08283\u79cd\u8bed\u8a00\u7ec4\u5408\uff09\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\u57fa\u4e8e\u53e5\u5b50\u76f8\u4f3c\u6027\u7684\u8d1f\u4f8b\u91c7\u6837\u548c\u57fa\u4e8eLLM\u7684\u91cd\u65b0\u6392\u5e8f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8eLLM\u7684\u91cd\u65b0\u6392\u5e8f\u6548\u679c\u6700\u4f18\uff0c\u5176\u6b21\u662f\u91c7\u7528\u53e5\u5b50\u76f8\u4f3c\u6027\u8d1f\u4f8b\u91c7\u6837\u7684\u5fae\u8c03\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u7814\u7a76\u53d1\u73b0\u8de8\u8bed\u8a00\u73af\u5883\u4e0e\u591a\u8bed\u8a00\u73af\u5883\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u8de8\u8bed\u8a00\u68c0\u7d22\u5177\u6709\u72ec\u7279\u6027\uff0c\u800cLLM\u91cd\u65b0\u6392\u5e8f\u548c\u9488\u5bf9\u6027\u8d1f\u4f8b\u91c7\u6837\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002\u8fd9\u4e00\u53d1\u73b0\u4e3a\u672a\u6765\u591a\u8bed\u8a00\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002", "keywords": "\u4e8b\u5b9e\u6838\u67e5, \u8de8\u8bed\u8a00\u68c0\u7d22, \u591a\u8bed\u8a00\u68c0\u7d22, LLM\u91cd\u65b0\u6392\u5e8f, \u8d1f\u4f8b\u91c7\u6837"}}
{"id": "2505.21857", "pdf": "https://arxiv.org/pdf/2505.21857", "abs": "https://arxiv.org/abs/2505.21857", "authors": ["Mijung Park"], "title": "Revisiting Bayesian Model Averaging in the Era of Foundation Models", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We revisit the classical, full-fledged Bayesian model averaging (BMA)\nparadigm to ensemble pre-trained and/or lightly-finetuned foundation models to\nenhance the classification performance on image and text data. To make BMA\ntractable under foundation models, we introduce trainable linear classifiers\nthat take frozen features from the pre-trained foundation models as inputs. The\nmodel posteriors over the linear classifiers tell us which linear heads and\nfrozen features are better suited for a given dataset, resulting in a\nprincipled model ensembling method. Furthermore, we propose a computationally\ncheaper, optimizable model averaging scheme (OMA). In OMA, we directly optimize\nthe model ensemble weights, just like those weights based on model posterior\ndistributions in BMA, by reducing the amount of surprise (expected entropy of\nthe predictions) we get from predictions of ensembled models. With the rapid\ndevelopment of foundation models, these approaches will enable the\nincorporation of future, possibly significantly better foundation models to\nenhance the performance of challenging classification tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u7ecf\u5178\u8d1d\u53f6\u65af\u6a21\u578b\u5e73\u5747\uff08BMA\uff09\u8303\u5f0f\uff0c\u901a\u8fc7\u96c6\u6210\u9884\u8bad\u7ec3\u6216\u5fae\u8c03\u7684\u57fa\u7840\u6a21\u578b\u63d0\u5347\u56fe\u50cf\u548c\u6587\u672c\u6570\u636e\u7684\u5206\u7c7b\u6027\u80fd\u3002\u4e3a\u4e86\u5b9e\u73b0BMA\u5728\u57fa\u7840\u6a21\u578b\u4e0b\u7684\u9ad8\u6548\u8fd0\u884c\uff0c\u5f15\u5165\u4e86\u53ef\u8bad\u7ec3\u7684\u7ebf\u6027\u5206\u7c7b\u5668\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u540e\u9a8c\u6982\u7387\u4f18\u5316\u6a21\u578b\u96c6\u6210\u6743\u91cd\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u7684\u4f18\u5316\u6a21\u578b\u5e73\u5747\u65b9\u6848\uff08OMA\uff09\u3002", "motivation": "\u968f\u7740\u57fa\u7840\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5982\u4f55\u9ad8\u6548\u96c6\u6210\u9884\u8bad\u7ec3\u6216\u5fae\u8c03\u7684\u6a21\u578b\u4ee5\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u6210\u4e3a\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a1\uff09\u57fa\u4e8e\u8d1d\u53f6\u65af\u6a21\u578b\u5e73\u5747\uff08BMA\uff09\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u4f7f\u7528\u53ef\u8bad\u7ec3\u7684\u7ebf\u6027\u5206\u7c7b\u5668\u5904\u7406\u51bb\u7ed3\u7279\u5f81\uff1b2\uff09\u4f18\u5316\u6a21\u578b\u5e73\u5747\u65b9\u6848\uff08OMA\uff09\uff0c\u76f4\u63a5\u4f18\u5316\u6a21\u578b\u96c6\u6210\u6743\u91cd\u4ee5\u51cf\u5c11\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u96c6\u6210\u57fa\u7840\u6a21\u578b\uff0c\u63d0\u5347\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd9\u4e9b\u65b9\u6cd5\u4e3a\u672a\u6765\u66f4\u597d\u57fa\u7840\u6a21\u578b\u7684\u96c6\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "keywords": "\u8d1d\u53f6\u65af\u6a21\u578b\u5e73\u5747, \u57fa\u7840\u6a21\u578b, \u6a21\u578b\u96c6\u6210, \u7ebf\u6027\u5206\u7c7b\u5668, \u4f18\u5316\u6a21\u578b\u5e73\u5747"}}
{"id": "2505.21550", "pdf": "https://arxiv.org/pdf/2505.21550", "abs": "https://arxiv.org/abs/2505.21550", "authors": ["Rishi Sharma", "Martijn de Vos", "Pradyumna Chari", "Ramesh Raskar", "Anne-Marie Kermarrec"], "title": "Collaborative Agentic AI Needs Interoperability Across Ecosystems", "categories": ["cs.NI", "cs.AI", "cs.MA"], "comment": null, "summary": "Collaborative agentic AI is projected to transform entire industries by\nenabling AI-powered agents to autonomously perceive, plan, and act within\ndigital environments. Yet, current solutions in this field are all built in\nisolation, and we are rapidly heading toward a landscape of fragmented,\nincompatible ecosystems. In this position paper, we argue that\ninteroperability, achieved by the adoption of minimal standards, is essential\nto ensure open, secure, web-scale, and widely-adopted agentic ecosystems. To\nthis end, we devise a minimal architectural foundation for collaborative\nagentic AI, named Web of Agents, which is composed of four components:\nagent-to-agent messaging, interaction interoperability, state management, and\nagent discovery. Web of Agents adopts existing standards and reuses existing\ninfrastructure where possible. With Web of Agents, we take the first but\ncritical step toward interoperable agentic systems and offer a pragmatic path\nforward before ecosystem fragmentation becomes the norm.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u534f\u4f5c\u5f0f\u667a\u80fd\u4ee3\u7406AI\u751f\u6001\u4e92\u64cd\u4f5c\u6027\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aWeb of Agents\u7684\u67b6\u6784\u89e3\u51b3\u5f53\u524d\u751f\u6001\u788e\u7247\u5316\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u534f\u4f5c\u5f0f\u667a\u80fd\u4ee3\u7406AI\u89e3\u51b3\u65b9\u6848\u5404\u81ea\u5b64\u7acb\uff0c\u53ef\u80fd\u5bfc\u81f4\u751f\u6001\u788e\u7247\u5316\uff0c\u4f5c\u8005\u8ba4\u4e3a\u4e92\u64cd\u4f5c\u6027\u662f\u5b9e\u73b0\u5f00\u653e\u3001\u5b89\u5168\u3001\u89c4\u6a21\u5316\u7684\u5173\u952e\u3002", "method": "\u63d0\u51faWeb of Agents\u67b6\u6784\uff0c\u5305\u542b\u56db\u4e2a\u7ec4\u4ef6\uff1a\u4ee3\u7406\u95f4\u901a\u4fe1\u3001\u4ea4\u4e92\u4e92\u64cd\u4f5c\u6027\u3001\u72b6\u6001\u7ba1\u7406\u548c\u4ee3\u7406\u53d1\u73b0\uff0c\u5e76\u5c3d\u53ef\u80fd\u590d\u7528\u73b0\u6709\u6807\u51c6\u548c\u57fa\u7840\u8bbe\u65bd\u3002", "result": "Web of Agents\u4e3a\u5b9e\u73b0\u4e92\u64cd\u4f5c\u6027\u63d0\u4f9b\u4e86\u521d\u6b65\u4f46\u5173\u952e\u7684\u4e00\u6b65\uff0c\u907f\u514d\u751f\u6001\u788e\u7247\u5316\u6210\u4e3a\u5e38\u6001\u3002", "conclusion": "\u901a\u8fc7\u6700\u5c0f\u5316\u6807\u51c6\u548c\u57fa\u7840\u8bbe\u65bd\u590d\u7528\uff0cWeb of Agents\u4e3a\u534f\u4f5c\u5f0f\u667a\u80fd\u4ee3\u7406AI\u7684\u4e92\u64cd\u4f5c\u6027\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002", "keywords": "\u534f\u4f5c\u5f0fAI, \u667a\u80fd\u4ee3\u7406, \u4e92\u64cd\u4f5c\u6027, Web of Agents, \u751f\u6001\u788e\u7247\u5316"}}
{"id": "2505.22120", "pdf": "https://arxiv.org/pdf/2505.22120", "abs": "https://arxiv.org/abs/2505.22120", "authors": ["Runyu Wang", "Peng Ping", "Zhengyu Guo", "Xiaoye Zhang", "Quan Shi", "Liting Zhou", "Tianbo Ji"], "title": "LoKI: Low-damage Knowledge Implanting of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning adapts pretrained models for specific tasks but poses the risk of\ncatastrophic forgetting (CF), where critical knowledge from pre-training is\noverwritten. Current Parameter-Efficient Fine-Tuning (PEFT) methods for Large\nLanguage Models (LLMs), while efficient, often sacrifice general capabilities.\nTo address the issue of CF in a general-purpose PEFT framework, we propose\n\\textbf{Lo}w-damage \\textbf{K}nowledge \\textbf{I}mplanting (\\textbf{LoKI}), a\nPEFT technique that is based on a mechanistic understanding of how knowledge is\nstored in transformer architectures. In two real-world scenarios, LoKI\ndemonstrates task-specific performance that is comparable to or even surpasses\nthat of full fine-tuning and LoRA-based methods across various model types,\nwhile significantly better preserving general capabilities. Our work connects\nmechanistic insights into LLM knowledge storage with practical fine-tuning\nobjectives, achieving state-of-the-art trade-offs between task specialization\nand the preservation of general capabilities. Our implementation is publicly\navailable as ready-to-use code\\footnote{https://github.com/Nexround/LoKI}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLoKI\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u89e3Transformer\u67b6\u6784\u4e2d\u77e5\u8bc6\u7684\u5b58\u50a8\u673a\u5236\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u901a\u7528\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u51fa\u8272\u7684\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff08PEFT\uff09\u4e2d\u5e38\u89c1\u7684\u707e\u96be\u6027\u9057\u5fd8\uff08CF\uff09\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u727a\u7272\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u5bf9Transformer\u67b6\u6784\u4e2d\u77e5\u8bc6\u5b58\u50a8\u673a\u5236\u7684\u673a\u68b0\u6027\u7406\u89e3\uff0c\u8bbe\u8ba1\u4e86LoKI\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u635f\u4f24\u77e5\u8bc6\u690d\u5165\u5b9e\u73b0\u9ad8\u6548\u5fae\u8c03\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u7c7b\u578b\u4e0a\uff0cLoKI\u7684\u4efb\u52a1\u6027\u80fd\u4e0e\u5168\u5fae\u8c03\u548cLoRA\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u597d\uff0c\u540c\u65f6\u663e\u8457\u4fdd\u7559\u4e86\u901a\u7528\u80fd\u529b\u3002", "conclusion": "LoKI\u901a\u8fc7\u7ed3\u5408\u5bf9\u6a21\u578b\u77e5\u8bc6\u5b58\u50a8\u7684\u673a\u5236\u6027\u7406\u89e3\u548c\u5b9e\u9645\u5fae\u8c03\u76ee\u6807\uff0c\u5b9e\u73b0\u4e86\u4efb\u52a1\u4e13\u4e1a\u5316\u548c\u901a\u7528\u80fd\u529b\u4fdd\u62a4\u7684\u6700\u4f73\u5e73\u8861\u3002", "keywords": "\u53c2\u6570\u9ad8\u6548\u5fae\u8c03, \u707e\u96be\u6027\u9057\u5fd8, LoKI, Transformer\u67b6\u6784, \u901a\u7528\u80fd\u529b"}}
{"id": "2505.21877", "pdf": "https://arxiv.org/pdf/2505.21877", "abs": "https://arxiv.org/abs/2505.21877", "authors": ["Hongyao Chen", "Tianyang Xu", "Xiaojun Wu", "Josef Kittler"], "title": "Hybrid Batch Normalisation: Resolving the Dilemma of Batch Normalisation in Federated Learning", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Batch Normalisation (BN) is widely used in conventional deep neural network\ntraining to harmonise the input-output distributions for each batch of data.\nHowever, federated learning, a distributed learning paradigm, faces the\nchallenge of dealing with non-independent and identically distributed data\namong the client nodes. Due to the lack of a coherent methodology for updating\nBN statistical parameters, standard BN degrades the federated learning\nperformance. To this end, it is urgent to explore an alternative normalisation\nsolution for federated learning. In this work, we resolve the dilemma of the BN\nlayer in federated learning by developing a customised normalisation approach,\nHybrid Batch Normalisation (HBN). HBN separates the update of statistical\nparameters (i.e. , means and variances used for evaluation) from that of\nlearnable parameters (i.e. , parameters that require gradient updates),\nobtaining unbiased estimates of global statistical parameters in distributed\nscenarios. In contrast with the existing solutions, we emphasise the supportive\npower of global statistics for federated learning. The HBN layer introduces a\nlearnable hybrid distribution factor, allowing each computing node to\nadaptively mix the statistical parameters of the current batch with the global\nstatistics. Our HBN can serve as a powerful plugin to advance federated\nlearning performance. It reflects promising merits across a wide range of\nfederated learning settings, especially for small batch sizes and heterogeneous\ndata.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8054\u90a6\u5b66\u4e60\u7684\u6df7\u5408\u6279\u6807\u51c6\u5316\u65b9\u6cd5\uff08HBN\uff09\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6279\u6807\u51c6\u5316\uff08BN\uff09\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\uff0c\u7531\u4e8e\u5ba2\u6237\u7aef\u6570\u636e\u7684\u975e\u72ec\u7acb\u540c\u5206\u5e03\u7279\u6027\uff0c\u4f20\u7edfBN\u7684\u7edf\u8ba1\u53c2\u6570\u66f4\u65b0\u65b9\u6cd5\u5931\u6548\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u4e9f\u9700\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\u3002", "method": "HBN\u5c06\u7edf\u8ba1\u53c2\u6570\uff08\u5747\u503c\u548c\u65b9\u5dee\uff09\u4e0e\u53ef\u5b66\u4e60\u53c2\u6570\u5206\u5f00\u66f4\u65b0\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u6df7\u5408\u5206\u5e03\u56e0\u5b50\uff0c\u4f7f\u5404\u8282\u70b9\u80fd\u81ea\u9002\u5e94\u6df7\u5408\u5f53\u524d\u6279\u6b21\u4e0e\u5168\u5c40\u7edf\u8ba1\u53c2\u6570\u3002", "result": "HBN\u5728\u591a\u79cd\u8054\u90a6\u5b66\u4e60\u573a\u666f\uff08\u5c24\u5176\u662f\u5c0f\u6279\u6b21\u548c\u975e\u540c\u5206\u5e03\u6570\u636e\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "HBN\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u63d2\u4ef6\uff0c\u80fd\u591f\u589e\u5f3a\u8054\u90a6\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6570\u636e\u5f02\u8d28\u6027\u548c\u5c0f\u6279\u6b21\u573a\u666f\u4e0b\u6548\u679c\u7a81\u51fa\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60, \u6279\u6807\u51c6\u5316, \u6df7\u5408\u6279\u6807\u51c6\u5316, \u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e"}}
{"id": "2505.21551", "pdf": "https://arxiv.org/pdf/2505.21551", "abs": "https://arxiv.org/abs/2505.21551", "authors": ["Emmanuel Akinrintoyo", "Nadine Abdelhalim", "Nicole Salomons"], "title": "WhisperD: Dementia Speech Recognition and Filler Word Detection with Whisper", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "comment": "Submitted to Interspeech 2025 (Accepted)", "summary": "Whisper fails to correctly transcribe dementia speech because persons with\ndementia (PwDs) often exhibit irregular speech patterns and disfluencies such\nas pauses, repetitions, and fragmented sentences. It was trained on standard\nspeech and may have had little or no exposure to dementia-affected speech.\nHowever, correct transcription is vital for dementia speech for cost-effective\ndiagnosis and the development of assistive technology. In this work, we\nfine-tune Whisper with the open-source dementia speech dataset (DementiaBank)\nand our in-house dataset to improve its word error rate (WER). The fine-tuning\nalso includes filler words to ascertain the filler inclusion rate (FIR) and F1\nscore. The fine-tuned models significantly outperformed the off-the-shelf\nmodels. The medium-sized model achieved a WER of 0.24, outperforming previous\nwork. Similarly, there was a notable generalisability to unseen data and speech\npatterns.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5fae\u8c03Whisper\u6a21\u578b\uff0c\u4f7f\u7528DementiaBank\u548c\u5185\u90e8\u6570\u636e\u96c6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u75f4\u5446\u75c7\u8bed\u97f3\u8f6c\u5f55\u7684\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u3002", "motivation": "\u75f4\u5446\u75c7\u60a3\u8005\uff08PwDs\uff09\u7684\u8bed\u97f3\u5b58\u5728\u4e0d\u89c4\u5219\u6a21\u5f0f\u548c\u65ad\u7eed\u73b0\u8c61\uff0c\u6807\u51c6\u8bed\u97f3\u8bad\u7ec3\u7684\u6a21\u578b\u8f6c\u5f55\u6548\u679c\u4e0d\u4f73\uff0c\u4f46\u6b63\u786e\u8f6c\u5f55\u5bf9\u8bca\u65ad\u548c\u8f85\u52a9\u6280\u672f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528DementiaBank\u548c\u5185\u90e8\u6570\u636e\u96c6\u5fae\u8c03Whisper\u6a21\u578b\uff0c\u5e76\u52a0\u5165\u586b\u5145\u8bcd\u5206\u6790\u586b\u5145\u8bcd\u5305\u542b\u7387\uff08FIR\uff09\u548cF1\u5206\u6570\u3002", "result": "\u5fae\u8c03\u540e\u7684\u4e2d\u5c3a\u5bf8\u6a21\u578bWER\u4e3a0.24\uff0c\u4f18\u4e8e\u73b0\u6709\u5de5\u4f5c\uff0c\u4e14\u5bf9\u672a\u89c1\u6570\u636e\u548c\u8bed\u97f3\u6a21\u5f0f\u6709\u663e\u8457\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5fae\u8c03Whisper\u80fd\u6709\u6548\u63d0\u5347\u75f4\u5446\u75c7\u8bed\u97f3\u8f6c\u5f55\u51c6\u786e\u7387\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u75f4\u5446\u75c7\u8bed\u97f3\u8f6c\u5f55, Whisper\u6a21\u578b, \u5fae\u8c03, WER, FIR"}}
{"id": "2505.22131", "pdf": "https://arxiv.org/pdf/2505.22131", "abs": "https://arxiv.org/abs/2505.22131", "authors": ["Zhuoyang Wu", "Xinze Li", "Zhenghao Liu", "Yukun Yan", "Zhiyuan Liu", "Minghe Yu", "Cheng Yang", "Yu Gu", "Ge Yu", "Maosong Sun"], "title": "EULER: Enhancing the Reasoning Ability of Large Language Models through Error-Induced Learning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong reasoning capabilities\nand achieved promising results in mathematical problem-solving tasks. Learning\nfrom errors offers the potential to further enhance the performance of LLMs\nduring Supervised Fine-Tuning (SFT). However, the errors in synthesized\nsolutions are typically gathered from sampling trails, making it challenging to\ngenerate solution errors for each mathematical problem. This paper introduces\nthe Error-IndUced LEaRning (EULER) model, which aims to develop an error\nexposure model that generates high-quality solution errors to enhance the\nmathematical reasoning capabilities of LLMs. Specifically, EULER optimizes the\nerror exposure model to increase the generation probability of self-made\nsolution errors while utilizing solutions produced by a superior LLM to\nregularize the generation quality. Our experiments across various mathematical\nproblem datasets demonstrate the effectiveness of the EULER model, achieving an\nimprovement of over 4% compared to all baseline models. Further analysis\nreveals that EULER is capable of synthesizing more challenging and educational\nsolution errors, which facilitate both the training and inference processes of\nLLMs. All codes are available at https://github.com/NEUIR/EULER.", "AI": {"tldr": "EULER\u6a21\u578b\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u9519\u8bef\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b4%\u4ee5\u4e0a\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u89e3\u9898\u4e2d\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u4ece\u9519\u8bef\u4e2d\u5b66\u4e60\u53ef\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u6027\u80fd\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5408\u6210\u9519\u8bef\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4fc3\u4f7fEULER\u6a21\u578b\u7684\u5f00\u53d1\u3002", "method": "EULER\u901a\u8fc7\u4f18\u5316\u9519\u8bef\u66b4\u9732\u6a21\u578b\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u9519\u8bef\uff0c\u5e76\u5229\u7528\u9ad8\u9636LLM\u7684\u89e3\u51b3\u65b9\u6848\u89c4\u8303\u751f\u6210\u8d28\u91cf\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u95ee\u9898\u6570\u636e\u96c6\u4e0a\uff0cEULER\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u6a21\u578b\uff0c\u63d0\u5347\u8d85\u8fc74%\u3002", "conclusion": "EULER\u6a21\u578b\u80fd\u591f\u751f\u6210\u66f4\u5177\u6311\u6218\u6027\u548c\u6559\u80b2\u610f\u4e49\u7684\u9519\u8bef\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u63d0\u5347LLM\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u80fd\u529b\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u6570\u5b66\u63a8\u7406, \u9519\u8bef\u5b66\u4e60, EULER\u6a21\u578b, \u76d1\u7763\u5fae\u8c03"}}
{"id": "2505.21882", "pdf": "https://arxiv.org/pdf/2505.21882", "abs": "https://arxiv.org/abs/2505.21882", "authors": ["Ruijie Li", "Xiang Zhao", "Qiao Ning", "Shikai Guo"], "title": "HydraNet: Momentum-Driven State Space Duality for Multi-Granularity Tennis Tournaments Analysis", "categories": ["cs.LG"], "comment": "14 pages, 9 figures (including subfigures), 5 tables. This is the\n  first work to explore and effectively model momentum across multiple\n  granularities in professional tennis tournaments", "summary": "In tennis tournaments, momentum, a critical yet elusive phenomenon, reflects\nthe dynamic shifts in performance of athletes that can decisively influence\nmatch outcomes. Despite its significance, momentum in terms of effective\nmodeling and multi-granularity analysis across points, games, sets, and matches\nin tennis tournaments remains underexplored. In this study, we define a novel\nMomentum Score (MS) metric to quantify a player's momentum level in\nmulti-granularity tennis tournaments, and design HydraNet, a momentum-driven\nstate-space duality-based framework, to model MS by integrating thirty-two\nheterogeneous dimensions of athletes performance in serve, return, psychology\nand fatigue. HydraNet integrates a Hydra module, which builds upon a\nstate-space duality (SSD) framework, capturing explicit momentum with a\nsliding-window mechanism and implicit momentum through cross-game state\npropagation. It also introduces a novel Versus Learning method to better\nenhance the adversarial nature of momentum between the two athletes at a macro\nlevel, along with a Collaborative-Adversarial Attention Mechanism (CAAM) for\ncapturing and integrating intra-player and inter-player dynamic momentum at a\nmicro level. Additionally, we construct a million-level tennis cross-tournament\ndataset spanning from 2012-2023 Wimbledon and 2013-2023 US Open, and validate\nthe multi-granularity modeling capability of HydraNet for the MS metric on this\ndataset. Extensive experimental evaluations demonstrate that the MS metric\nconstructed by the HydraNet framework provides actionable insights into how\nmomentum impacts outcomes at different granularities, establishing a new\nfoundation for momentum modeling and sports analysis. To the best of our\nknowledge. The source code and datasets are available at\nhttps://github.com/ReyJerry/HydraNet.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMomentum Score (MS)\u7684\u65b0\u6307\u6807\uff0c\u901a\u8fc7HydraNet\u6846\u67b6\u91cf\u5316\u7f51\u7403\u6bd4\u8d5b\u4e2d\u591a\u7c92\u5ea6\u7684\u8fd0\u52a8\u5458\u8868\u73b0\u52a8\u91cf\uff0c\u7ed3\u540832\u4e2a\u7ef4\u5ea6\u7684\u6570\u636e\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u52a8\u91cf\u5728\u7f51\u7403\u6bd4\u8d5b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5728\u591a\u7c92\u5ea6\u5efa\u6a21\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8bbe\u8ba1\u4e86HydraNet\u6846\u67b6\uff0c\u7ed3\u5408\u72b6\u6001\u7a7a\u95f4\u5bf9\u5076\u6027\u548c\u6ed1\u52a8\u7a97\u53e3\u673a\u5236\uff0c\u63d0\u51faVersus Learning\u65b9\u6cd5\u548cCAAM\u673a\u5236\uff0c\u6784\u5efa\u767e\u4e07\u7ea7\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMS\u6307\u6807\u80fd\u6709\u6548\u5206\u6790\u52a8\u91cf\u5728\u4e0d\u540c\u7c92\u5ea6\u5bf9\u6bd4\u8d5b\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u4e3a\u52a8\u91cf\u5efa\u6a21\u548c\u4f53\u80b2\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u57fa\u7840\u3002", "conclusion": "HydraNet\u6846\u67b6\u548cMS\u6307\u6807\u4e3a\u52a8\u91cf\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "keywords": "\u52a8\u91cf\u8bc4\u5206, HydraNet, \u72b6\u6001\u7a7a\u95f4\u5bf9\u5076, \u591a\u7c92\u5ea6\u5206\u6790, \u7f51\u7403\u6bd4\u8d5b"}}
{"id": "2505.21553", "pdf": "https://arxiv.org/pdf/2505.21553", "abs": "https://arxiv.org/abs/2505.21553", "authors": ["Hui Ma", "Kai Yang"], "title": "MetaSTNet: Multimodal Meta-learning for Cellular Traffic Conformal Prediction", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": null, "summary": "Network traffic prediction techniques have attracted much attention since\nthey are valuable for network congestion control and user experience\nimprovement. While existing prediction techniques can achieve favorable\nperformance when there is sufficient training data, it remains a great\nchallenge to make accurate predictions when only a small amount of training\ndata is available. To tackle this problem, we propose a deep learning model,\nentitled MetaSTNet, based on a multimodal meta-learning framework. It is an\nend-to-end network architecture that trains the model in a simulator and\ntransfers the meta-knowledge to a real-world environment, which can quickly\nadapt and obtain accurate predictions on a new task with only a small amount of\nreal-world training data. In addition, we further employ cross conformal\nprediction to assess the calibrated prediction intervals. Extensive experiments\nhave been conducted on real-world datasets to illustrate the efficiency and\neffectiveness of MetaSTNet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5143\u5b66\u4e60\u6846\u67b6\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5MetaSTNet\uff0c\u7528\u4e8e\u5728\u5c0f\u6837\u672c\u6570\u636e\u4e0b\u5b9e\u73b0\u7cbe\u51c6\u7f51\u7edc\u6d41\u91cf\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7f51\u7edc\u6d41\u91cf\u9884\u6d4b\u6280\u672f\u5728\u5927\u6837\u672c\u6570\u636e\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5c0f\u6837\u672c\u60c5\u5883\u4e0b\u96be\u4ee5\u51c6\u786e\u9884\u6d4b\uff0c\u9700\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u91c7\u7528\u5143\u5b66\u4e60\u6846\u67b6MetaSTNet\uff0c\u901a\u8fc7\u6a21\u62df\u5668\u8bad\u7ec3\u5e76\u8fc1\u79fb\u5143\u77e5\u8bc6\u81f3\u771f\u5b9e\u73af\u5883\uff0c\u7ed3\u5408\u4ea4\u53c9\u5171\u5f62\u9884\u6d4b\u6821\u51c6\u9884\u6d4b\u533a\u95f4\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMetaSTNet\u80fd\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u5e76\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u3002", "conclusion": "MetaSTNet\u4e3a\u5c0f\u6837\u672c\u7f51\u7edc\u6d41\u91cf\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u5907\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u7f51\u7edc\u6d41\u91cf\u9884\u6d4b\u3001\u5143\u5b66\u4e60\u3001\u5c0f\u6837\u672c\u5b66\u4e60\u3001\u6df1\u5ea6\u5b66\u4e60"}}
{"id": "2505.22135", "pdf": "https://arxiv.org/pdf/2505.22135", "abs": "https://arxiv.org/abs/2505.22135", "authors": ["Yuichiro Hoshino", "Hideyuki Tachibana", "Muneyoshi Inahara", "Hiroto Takegawa"], "title": "RAD: Redundancy-Aware Distillation for Hybrid Models via Self-Speculative Decoding", "categories": ["cs.CL", "cs.LG"], "comment": "26 pages", "summary": "Hybrid models combining Transformers and State Space Models (SSMs) are\npromising for balancing performance and efficiency. However, optimizing these\nhybrid models, particularly by addressing the potential redundancy inherent\nwithin the Transformer components, remains a significant challenge. In this\npaper, we propose RAD (Redundancy-Aware Distillation), a novel framework that\nuses self-speculative decoding as a diagnostic tool to identify redundant\nattention layers within the model. These identified layers are then selectively\nreplaced with SSM components, followed by targeted (self-)distillation.\nSpecifically, RAD focuses knowledge transfer on the components identified as\nredundant, considering architectural changes and specific weight initialization\nstrategies. We experimentally demonstrate that self-distillation using RAD\nsignificantly surpasses the performance of the original base model on\nmathematical and coding tasks. Furthermore, RAD is also effective in standard\nknowledge distillation settings, achieving up to approximately 2x faster\nconvergence compared to baseline methods. Notably, while a baseline model\ndistilled from a Llama-3.1 70B teacher achieves scores of 46.17 on GSM8K and\n22.75 on CRUX, RAD achieves significantly higher scores of 71.27 on GSM8K and\n28.25 on CRUX, even when using a much smaller Llama-3.1 8B teacher. RAD offers\na new pathway for efficient optimization and performance enhancement in the\ndistillation of hybrid models.", "AI": {"tldr": "\u63d0\u51fa\u4e86RAD\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u63a8\u6d4b\u89e3\u7801\u8bc6\u522bTransformer\u5197\u4f59\u5c42\uff0c\u5e76\u7528SSM\u66ff\u6362\uff0c\u7ed3\u5408\u76ee\u6807\u81ea\u84b8\u998f\u4f18\u5316\u6df7\u5408\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5728\u77e5\u8bc6\u84b8\u998f\u4e2d\u5b9e\u73b0\u66f4\u5feb\u6536\u655b\u3002", "motivation": "\u65e8\u5728\u4f18\u5316Transformer\u4e0eSSM\u6df7\u5408\u6a21\u578b\uff0c\u89e3\u51b3Transformer\u7ec4\u4ef6\u5197\u4f59\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u6548\u7387\u4e0e\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u81ea\u63a8\u6d4b\u89e3\u7801\u8bca\u65ad\u5197\u4f59\u6ce8\u610f\u529b\u5c42\uff0c\u9009\u62e9\u6027\u66ff\u6362\u4e3aSSM\u7ec4\u4ef6\uff0c\u5e76\u91c7\u7528\u9488\u5bf9\u6027\u81ea\u84b8\u998f\u7b56\u7565\u3002", "result": "RAD\u5728GSM8K\u548cCRUX\u4efb\u52a1\u4e0a\u5206\u522b\u8fbe\u523071.27\u548c28.25\u5206\uff0c\u8fdc\u8d85\u57fa\u7ebf\uff0846.17\u548c22.75\uff09\uff0c\u4e14\u6536\u655b\u901f\u5ea6\u63d0\u5347\u7ea62\u500d\u3002", "conclusion": "RAD\u4e3a\u6df7\u5408\u6a21\u578b\u7684\u9ad8\u6548\u4f18\u5316\u4e0e\u6027\u80fd\u589e\u5f3a\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002", "keywords": "\u6df7\u5408\u6a21\u578b, Transformer, \u72b6\u6001\u7a7a\u95f4\u6a21\u578b, \u81ea\u84b8\u998f, \u5197\u4f59\u4f18\u5316"}}
{"id": "2505.21893", "pdf": "https://arxiv.org/pdf/2505.21893", "abs": "https://arxiv.org/abs/2505.21893", "authors": ["Xiaomeng Yang", "Zhiyu Tan", "Junyan Wang", "Zhijian Zhou", "Hao Li"], "title": "SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Preference learning has become a central technique for aligning generative\nmodels with human expectations. Recently, it has been extended to diffusion\nmodels through methods like Direct Preference Optimization (DPO). However,\nexisting approaches such as Diffusion-DPO suffer from two key challenges:\ntimestep-dependent instability, caused by a mismatch between the reverse and\nforward diffusion processes and by high gradient variance in early noisy\ntimesteps, and off-policy bias arising from the mismatch between optimization\nand data collection policies. We begin by analyzing the reverse diffusion\ntrajectory and observe that instability primarily occurs at early timesteps\nwith low importance weights. To address these issues, we first propose\nDPO-C\\&M, a practical strategy that improves stability by clipping and masking\nuninformative timesteps while partially mitigating off-policy bias. Building on\nthis, we introduce SDPO (Importance-Sampled Direct Preference Optimization), a\nprincipled framework that incorporates importance sampling into the objective\nto fully correct for off-policy bias and emphasize informative updates during\nthe diffusion process. Experiments on CogVideoX-2B, CogVideoX-5B, and\nWan2.1-1.3B demonstrate that both methods outperform standard Diffusion-DPO,\nwith SDPO achieving superior VBench scores, human preference alignment, and\ntraining robustness. These results highlight the importance of timestep-aware,\ndistribution-corrected optimization in diffusion-based preference learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u6539\u8fdb\u6269\u6563\u6a21\u578b\u504f\u597d\u5b66\u4e60\u7684\u65b9\u6cd5\uff1aDPO-C&M\u901a\u8fc7\u88c1\u526a\u548c\u63a9\u7801\u4e0d\u7a33\u5b9a\u65f6\u95f4\u6b65\u63d0\u9ad8\u7a33\u5b9a\u6027\uff0cSDPO\u5219\u5f15\u5165\u91cd\u8981\u6027\u91c7\u6837\u4ee5\u5b8c\u5168\u6821\u6b63\u79bb\u7b56\u7565\u504f\u5dee\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e24\u8005\u5747\u4f18\u4e8e\u6807\u51c6Diffusion-DPO\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u504f\u597d\u5b66\u4e60\u65b9\u6cd5\uff08\u5982Diffusion-DPO\uff09\u5b58\u5728\u65f6\u95f4\u6b65\u4f9d\u8d56\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u79bb\u7b56\u7565\u504f\u5dee\u95ee\u9898\uff0c\u9700\u6539\u8fdb\u4ee5\u63d0\u5347\u6a21\u578b\u4e0e\u4eba\u7c7b\u671f\u671b\u7684\u5bf9\u9f50\u6548\u679c\u3002", "method": "1. DPO-C&M\uff1a\u88c1\u526a\u548c\u63a9\u7801\u4f4e\u6743\u91cd\u65f6\u95f4\u6b65\u4ee5\u63d0\u5347\u7a33\u5b9a\u6027\uff1b2. SDPO\uff1a\u5728\u76ee\u6807\u51fd\u6570\u4e2d\u5f15\u5165\u91cd\u8981\u6027\u91c7\u6837\uff0c\u6821\u6b63\u504f\u5dee\u5e76\u4f18\u5316\u4fe1\u606f\u6027\u66f4\u65b0\u3002", "result": "\u5728CogVideoX-2B\u7b49\u6a21\u578b\u4e0a\uff0cSDPO\u8868\u73b0\u51fa\u66f4\u9ad8\u7684VBench\u5f97\u5206\u3001\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u53ca\u8bad\u7ec3\u9c81\u68d2\u6027\uff0c\u4f18\u4e8eDiffusion-DPO\u3002", "conclusion": "\u65f6\u95f4\u6b65\u611f\u77e5\u548c\u5206\u5e03\u6821\u6b63\u7684\u4f18\u5316\u5bf9\u6269\u6563\u6a21\u578b\u504f\u597d\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0cSDPO\u548cDPO-C&M\u4e3a\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u504f\u597d\u5b66\u4e60,\u6269\u6563\u6a21\u578b,DPO-C&M,SDPO,\u91cd\u8981\u6027\u91c7\u6837"}}
{"id": "2505.21556", "pdf": "https://arxiv.org/pdf/2505.21556", "abs": "https://arxiv.org/abs/2505.21556", "authors": ["Hee-Seon Kim", "Minbeom Kim", "Wonjun Lee", "Kihyun Kim", "Changick Kim"], "title": "Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless Prompts", "categories": ["cs.CV", "cs.AI"], "comment": "LVLM, Jailbreak", "summary": "Optimization-based jailbreaks typically adopt the Toxic-Continuation setting\nin large vision-language models (LVLMs), following the standard next-token\nprediction objective. In this setting, an adversarial image is optimized to\nmake the model predict the next token of a toxic prompt. However, we find that\nthe Toxic-Continuation paradigm is effective at continuing already-toxic\ninputs, but struggles to induce safety misalignment when explicit toxic signals\nare absent. We propose a new paradigm: Benign-to-Toxic (B2T) jailbreak. Unlike\nprior work, we optimize adversarial images to induce toxic outputs from benign\nconditioning. Since benign conditioning contains no safety violations, the\nimage alone must break the model's safety mechanisms. Our method outperforms\nprior approaches, transfers in black-box settings, and complements text-based\njailbreaks. These results reveal an underexplored vulnerability in multimodal\nalignment and introduce a fundamentally new direction for jailbreak approaches.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4ece\u826f\u6027\u5230\u6709\u6bd2\uff08B2T\uff09\u7684\u8d8a\u72f1\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u4f18\u5316\u5bf9\u6297\u56fe\u50cf\u5728\u65e0\u663e\u5f0f\u6709\u6bd2\u4fe1\u53f7\u6761\u4ef6\u4e0b\u8bf1\u53d1\u6a21\u578b\u8f93\u51fa\u6709\u6bd2\u5185\u5bb9\uff0c\u63ed\u793a\u4e86\u591a\u6a21\u6001\u5bf9\u9f50\u4e2d\u7684\u65b0\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4f18\u5316\u7684\u8d8a\u72f1\u65b9\u6cd5\uff08Toxic-Continuation\uff09\u5728\u7f3a\u4e4f\u663e\u5f0f\u6709\u6bd2\u4fe1\u53f7\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u6b64\u63d0\u51faB2T\u8303\u5f0f\u4ee5\u63a2\u7d22\u66f4\u5e7f\u6cdb\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u4f18\u5316\u5bf9\u6297\u56fe\u50cf\uff0c\u4f7f\u5176\u5728\u826f\u6027\u8f93\u5165\u6761\u4ef6\u4e0b\u8bf1\u5bfc\u6a21\u578b\u751f\u6210\u6709\u6bd2\u8f93\u51fa\uff0c\u65e0\u9700\u4f9d\u8d56\u9884\u5b58\u7684\u6709\u6bd2\u63d0\u793a\u3002", "result": "B2T\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5177\u5907\u9ed1\u76d2\u8fc1\u79fb\u80fd\u529b\uff0c\u5e76\u80fd\u4e0e\u6587\u672c\u8d8a\u72f1\u65b9\u6cd5\u4e92\u8865\u3002", "conclusion": "B2T\u63ed\u793a\u4e86\u591a\u6a21\u6001\u5bf9\u9f50\u4e2d\u672a\u5145\u5206\u63a2\u7d22\u7684\u8106\u5f31\u6027\uff0c\u4e3a\u8d8a\u72f1\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "\u5bf9\u6297\u56fe\u50cf, \u591a\u6a21\u6001\u5bf9\u9f50, \u8d8a\u72f1\u653b\u51fb, \u5b89\u5168\u673a\u5236, \u89c6\u89c9\u8bed\u8a00\u6a21\u578b"}}
{"id": "2505.22137", "pdf": "https://arxiv.org/pdf/2505.22137", "abs": "https://arxiv.org/abs/2505.22137", "authors": ["Marc Feger", "Katarina Boland", "Stefan Dietze"], "title": "Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This paper has been accepted to ACL 2025 and will be published after\n  27.07.2025", "summary": "Identifying arguments is a necessary prerequisite for various tasks in\nautomated discourse analysis, particularly within contexts such as political\ndebates, online discussions, and scientific reasoning. In addition to\ntheoretical advances in understanding the constitution of arguments, a\nsignificant body of research has emerged around practical argument mining,\nsupported by a growing number of publicly available datasets. On these\nbenchmarks, BERT-like transformers have consistently performed best,\nreinforcing the belief that such models are broadly applicable across diverse\ncontexts of debate. This study offers the first large-scale re-evaluation of\nsuch state-of-the-art models, with a specific focus on their ability to\ngeneralize in identifying arguments. We evaluate four transformers, three\nstandard and one enhanced with contrastive pre-training for better\ngeneralization, on 17 English sentence-level datasets as most relevant to the\ntask. Our findings show that, to varying degrees, these models tend to rely on\nlexical shortcuts tied to content words, suggesting that apparent progress may\noften be driven by dataset-specific cues rather than true task alignment. While\nthe models achieve strong results on familiar benchmarks, their performance\ndrops markedly when applied to unseen datasets. Nonetheless, incorporating both\ntask-specific pre-training and joint benchmark training proves effective in\nenhancing both robustness and generalization.", "AI": {"tldr": "\u8be5\u7814\u7a76\u91cd\u65b0\u8bc4\u4f30\u4e86BERT-like\u6a21\u578b\u5728\u8bba\u8fa9\u6316\u6398\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u8868\u73b0\u53d7\u6570\u636e\u96c6\u7279\u5b9a\u7ebf\u7d22\u5f71\u54cd\u8f83\u5927\uff0c\u4f46\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u9884\u8bad\u7ec3\u548c\u8054\u5408\u8bad\u7ec3\u53ef\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "motivation": "\u8bc4\u4f30\u73b0\u6709\u5148\u8fdb\u6a21\u578b\u5728\u8bba\u8fa9\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u63ed\u793a\u5176\u4f9d\u8d56\u6570\u636e\u96c6\u7279\u5b9a\u7ebf\u7d22\u800c\u975e\u4efb\u52a1\u672c\u8d28\u7684\u95ee\u9898\u3002", "method": "\u572817\u4e2a\u82f1\u6587\u53e5\u5b50\u7ea7\u6570\u636e\u96c6\u4e0a\u8bc4\u4f304\u79cdtransformer\u6a21\u578b\uff083\u79cd\u6807\u51c6\u6a21\u578b\u548c1\u79cd\u589e\u5f3a\u5bf9\u6bd4\u9884\u8bad\u7ec3\u6a21\u578b\uff09\uff0c\u5206\u6790\u5176\u8868\u73b0\u548c\u4f9d\u8d56\u6a21\u5f0f\u3002", "result": "\u6a21\u578b\u5728\u719f\u6089\u6570\u636e\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u672a\u89c1\u6570\u636e\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff1b\u4efb\u52a1\u7279\u5b9a\u9884\u8bad\u7ec3\u548c\u8054\u5408\u8bad\u7ec3\u53ef\u6709\u6548\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5f53\u524d\u6a21\u578b\u5728\u8bba\u8fa9\u8bc6\u522b\u4e2d\u4f9d\u8d56\u5c40\u90e8\u7ebf\u7d22\uff0c\u4efb\u52a1\u7279\u5b9a\u9884\u8bad\u7ec3\u548c\u8054\u5408\u8bad\u7ec3\u662f\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u7684\u6709\u6548\u65b9\u6cd5\u3002", "keywords": "\u8bba\u8fa9\u6316\u6398\u3001BERT-like\u6a21\u578b\u3001\u6cdb\u5316\u80fd\u529b\u3001\u4efb\u52a1\u7279\u5b9a\u9884\u8bad\u7ec3\u3001\u8054\u5408\u8bad\u7ec3"}}
{"id": "2505.21895", "pdf": "https://arxiv.org/pdf/2505.21895", "abs": "https://arxiv.org/abs/2505.21895", "authors": ["Cameron Gordon", "Yiping Ji", "Hemanth Saratchandran", "Paul Albert", "Simon Lucey"], "title": "Compressing Sine-Activated Low-Rank Adapters through Post-Training Quantization", "categories": ["cs.LG", "cs.AI"], "comment": "23 pages, 9 figures", "summary": "Low-Rank Adaptation (LoRA) has become a standard approach for\nparameter-efficient fine-tuning, offering substantial reductions in trainable\nparameters by modeling updates as the product of two low-rank matrices. While\neffective, the low-rank constraint inherently limits representational capacity,\noften resulting in reduced performance compared to full-rank fine-tuning.\nRecent work by Ji et al. (2025) has addressed this limitation by applying a\nfixed-frequency sinusoidal transformation to low-rank adapters, increasing\ntheir stable rank without introducing additional parameters. This raises a\ncrucial question: can the same sine-activated technique be successfully applied\nwithin the context of Post-Training Quantization to retain benefits even after\nmodel compression? In this paper, we investigate this question by extending the\nsinusoidal transformation framework to quantized LoRA adapters. We develop a\ntheoretical analysis showing that the stable rank of a quantized adapter is\ntightly linked to that of its full-precision counterpart, motivating the use of\nsuch rank-enhancing functions even under quantization. Our results demonstrate\nthat the expressivity gains from a sinusoidal non-linearity persist after\nquantization, yielding highly compressed adapters with negligible loss in\nperformance. We validate our approach across a range of fine-tuning tasks for\nlanguage, vision and text-to-image generation achieving significant memory\nsavings while maintaining competitive accuracy.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u91cf\u5316\u540e\u7684LoRA\u9002\u914d\u5668\u4e2d\u5e94\u7528\u6b63\u5f26\u53d8\u6362\u6280\u672f\uff0c\u4ee5\u4fdd\u6301\u6027\u80fd\u540c\u65f6\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "LoRA\u7684\u4f4e\u79e9\u7ea6\u675f\u9650\u5236\u4e86\u8868\u793a\u80fd\u529b\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u901a\u8fc7\u6b63\u5f26\u53d8\u6362\u63d0\u5347\u7a33\u5b9a\u79e9\uff0c\u80fd\u5426\u5728\u91cf\u5316\u540e\u4f9d\u7136\u6709\u6548\u662f\u7814\u7a76\u91cd\u70b9\u3002", "method": "\u6269\u5c55\u6b63\u5f26\u53d8\u6362\u6846\u67b6\u81f3\u91cf\u5316LoRA\u9002\u914d\u5668\uff0c\u7406\u8bba\u5206\u6790\u91cf\u5316\u540e\u7a33\u5b9a\u79e9\u4e0e\u5168\u7cbe\u5ea6\u7248\u672c\u7684\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u91cf\u5316\u540e\u4ecd\u80fd\u4fdd\u6301\u8868\u8fbe\u6027\u589e\u76ca\uff0c\u5b9e\u73b0\u9ad8\u538b\u7f29\u9002\u914d\u5668\u4e14\u6027\u80fd\u635f\u5931\u6781\u5c0f\u3002", "conclusion": "\u6b63\u5f26\u53d8\u6362\u6280\u672f\u9002\u7528\u4e8e\u91cf\u5316LoRA\u9002\u914d\u5668\uff0c\u663e\u8457\u8282\u7701\u5185\u5b58\u5e76\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "keywords": "LoRA, \u91cf\u5316, \u6b63\u5f26\u53d8\u6362, \u53c2\u6570\u9ad8\u6548\u5fae\u8c03, \u7a33\u5b9a\u79e9"}}
{"id": "2505.21557", "pdf": "https://arxiv.org/pdf/2505.21557", "abs": "https://arxiv.org/abs/2505.21557", "authors": ["Polad Geidarov"], "title": "Analytical Calculation of Weights Convolutional Neural Network", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper presents an algorithm for analytically calculating the weights and\nthresholds of convolutional neural networks (CNNs) without using standard\ntraining procedures. The algorithm enables the determination of CNN parameters\nbased on just 10 selected images from the MNIST dataset, each representing a\ndigit from 0 to 9. As part of the method, the number of channels in CNN layers\nis also derived analytically. A software module was implemented in C++ Builder,\nand a series of experiments were conducted using the MNIST dataset. Results\ndemonstrate that the analytically computed CNN can recognize over half of 1000\nhandwritten digit images without any training, achieving inference in fractions\nof a second. These findings suggest that CNNs can be constructed and applied\ndirectly for classification tasks without training, using purely analytical\ncomputation of weights.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u6807\u51c6\u8bad\u7ec3\u8fc7\u7a0b\u7684CNN\u6743\u91cd\u4e0e\u9608\u503c\u89e3\u6790\u8ba1\u7b97\u65b9\u6cd5\uff0c\u4ec5\u9700MNIST\u6570\u636e\u96c6\u4e2d10\u5f20\u4ee3\u8868\u6570\u5b57\u7684\u56fe\u7247\u5373\u53ef\u786e\u5b9a\u53c2\u6570\u3002", "motivation": "\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u4e0d\u4f9d\u8d56\u4f20\u7edf\u8bad\u7ec3\u6b65\u9aa4\uff0c\u76f4\u63a5\u901a\u8fc7\u89e3\u6790\u65b9\u6cd5\u8ba1\u7b97CNN\u53c2\u6570\u5e76\u5e94\u7528\u4e8e\u5206\u7c7b\u4efb\u52a1\u3002", "method": "\u57fa\u4e8e10\u5f20MNIST\u56fe\u7247\uff080-9\u5404\u4e00\u5f20\uff09\uff0c\u89e3\u6790\u8ba1\u7b97CNN\u5404\u5c42\u901a\u9053\u6570\u4e0e\u6743\u91cd/\u9608\u503c\uff0c\u5e76\u7528C++\u5b9e\u73b0\u9a8c\u8bc1\u3002", "result": "\u89e3\u6790\u8ba1\u7b97\u7684CNN\u80fd\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf91000\u5f20\u624b\u5199\u6570\u5b57\u5b9e\u73b0\u8fc7\u534a\u51c6\u786e\u7387\uff0c\u63a8\u7406\u8017\u65f6\u4e0d\u5230\u4e00\u79d2\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u7eaf\u89e3\u6790\u65b9\u6cd5\u6784\u5efa\u53ef\u76f4\u63a5\u5e94\u7528\u7684CNN\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u514d\u8bad\u7ec3\u5206\u7c7b\u6a21\u578b\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "keywords": "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc, \u89e3\u6790\u8ba1\u7b97, MNIST, \u6743\u91cd\u786e\u5b9a, \u514d\u8bad\u7ec3\u6a21\u578b"}}
{"id": "2505.22156", "pdf": "https://arxiv.org/pdf/2505.22156", "abs": "https://arxiv.org/abs/2505.22156", "authors": ["Shuaiyi Li", "Zhisong Zhang", "Yang Deng", "Chenlong Deng", "Tianqing Fang", "Hongming Zhang", "Haitao Mi", "Dong Yu", "Wai Lam"], "title": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for Efficient Model Editing", "categories": ["cs.CL"], "comment": "Under review", "summary": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method.", "AI": {"tldr": "InComeS\u6846\u67b6\u901a\u8fc7\u538b\u7f29\u548c\u9009\u62e9\u673a\u5236\u589e\u5f3aLLM\u5904\u7406\u7f16\u8f91\u4e0a\u4e0b\u6587\u7684\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\u5728\u9700\u8981\u6df1\u5c42\u8bed\u4e49\u7406\u89e3\u7684\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9650\u5236\u4e86LLM\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u80fd\u529b\u3002", "method": "\u63d0\u51faInComeS\u6846\u67b6\uff0c\u901a\u8fc7\u538b\u7f29\u7f16\u8f91\u4e0a\u4e0b\u6587\u4e3a\u952e\u503c\u7f13\u5b58\u5e76\u52a8\u6001\u9009\u62e9\u6700\u76f8\u5173\u4fe1\u606f\uff0c\u63d0\u5347\u591a\u7f16\u8f91\u5904\u7406\u7684\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eInComeS\u5728\u591a\u79cd\u7f16\u8f91\u683c\u5f0f\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6709\u6548\u4e14\u9ad8\u6548\u3002", "conclusion": "InComeS\u901a\u8fc7\u538b\u7f29\u548c\u52a8\u6001\u9009\u62e9\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u6a21\u578b\u7f16\u8f91\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "keywords": "\u6a21\u578b\u7f16\u8f91,LLM,\u4e0a\u4e0b\u6587\u538b\u7f29,\u52a8\u6001\u9009\u62e9,InComeS"}}
{"id": "2505.21908", "pdf": "https://arxiv.org/pdf/2505.21908", "abs": "https://arxiv.org/abs/2505.21908", "authors": ["Hanyin Wang", "Zhenbang Wu", "Gururaj Kolar", "Hariprasad Korsapati", "Brian Bartlett", "Bryan Hull", "Jimeng Sun"], "title": "Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement\nand operations but require labor-intensive assignment. Large Language Models\n(LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of\nthe task: pretraining corpora rarely contain private clinical or billing data.\nWe introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL)\nfor automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained\nwith Group Relative Policy Optimization (GRPO) using rule-based rewards,\nDRG-Sapphire introduces a series of RL enhancements to address domain-specific\nchallenges not seen in previous mathematical tasks. Our model achieves\nstate-of-the-art accuracy on the MIMIC-IV benchmark and generates\nphysician-validated reasoning for DRG assignments, significantly enhancing\nexplainability. Our study further sheds light on broader challenges of applying\nRL to knowledge-intensive, OOD tasks. We observe that RL performance scales\napproximately linearly with the logarithm of the number of supervised\nfine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally\nconstrained by the domain knowledge encoded in the base model. For OOD tasks\nlike DRG coding, strong RL performance requires sufficient knowledge infusion\nprior to RL. Consequently, scaling SFT may be more effective and\ncomputationally efficient than scaling RL alone for such tasks.", "AI": {"tldr": "DRG-Sapphire\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u81ea\u52a8\u5206\u914dDRG\u4ee3\u7801\uff0c\u5728MIMIC-IV\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u4f18\u51c6\u786e\u7387\uff0c\u5e76\u901a\u8fc7\u533b\u751f\u9a8c\u8bc1\u7684\u89e3\u91ca\u589e\u5f3a\u53ef\u7406\u89e3\u6027\u3002\u7814\u7a76\u53d1\u73b0RL\u6027\u80fd\u4e0eSFT\u6837\u672c\u6570\u91cf\u5448\u5bf9\u6570\u7ebf\u6027\u5173\u7cfb\uff0c\u8868\u660e\u77e5\u8bc6\u5bc6\u96c6\u578bOOD\u4efb\u52a1\u4e2d\uff0cSFT\u6269\u5c55\u6bd4\u5355\u7eafRL\u6269\u5c55\u66f4\u6709\u6548\u3002", "motivation": "\u4f20\u7edfDRG\u7f16\u7801\u4f9d\u8d56\u4eba\u5de5\u4e14\u8017\u65f6\uff0cLLM\u56e0\u8bad\u7ec3\u6570\u636e\u7f3a\u4e4f\u4e34\u5e8a/\u8d26\u5355\u4fe1\u606f\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u5f00\u53d1\u9002\u5e94OOD\u4efb\u52a1\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8eQwen2.5-7B\uff0c\u91c7\u7528Group Relative Policy Optimization\uff08GRPO\uff09\u548c\u89c4\u5219\u5956\u52b1\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff0c\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u4f18\u5316\u3002", "result": "\u5728MIMIC-IV\u4e0a\u5b9e\u73b0SOTA\u51c6\u786e\u7387\uff0c\u751f\u6210\u533b\u751f\u8ba4\u53ef\u7684\u89e3\u91ca\uff1b\u53d1\u73b0RL\u6027\u80fd\u4e0eSFT\u6837\u672c\u5bf9\u6570\u7ebf\u6027\u76f8\u5173\u3002", "conclusion": "\u77e5\u8bc6\u5bc6\u96c6\u578bOOD\u4efb\u52a1\u4e2d\uff0cRL\u9700\u4f9d\u8d56\u57fa\u6a21\u578b\u7684\u77e5\u8bc6\u6ce8\u5165\uff0c\u6269\u5c55SFT\u6bd4\u5355\u7eafRL\u66f4\u9ad8\u6548\u3002", "keywords": "DRG\u7f16\u7801\u3001\u5f3a\u5316\u5b66\u4e60\u3001OOD\u4efb\u52a1\u3001Qwen2.5-7B\u3001GRPO"}}
{"id": "2505.21558", "pdf": "https://arxiv.org/pdf/2505.21558", "abs": "https://arxiv.org/abs/2505.21558", "authors": ["Elhoucine Elfatimia", "Recep Eryigitb", "Lahcen Elfatimi"], "title": "A Novel Convolutional Neural Network-Based Framework for Complex Multiclass Brassica Seed Classification", "categories": ["cs.CV", "cs.AI", "cs.LG", "na"], "comment": "11 Figure", "summary": "Agricultural research has accelerated in recent years, yet farmers often lack\nthe time and resources for on-farm research due to the demands of crop\nproduction and farm operations. Seed classification offers valuable insights\ninto quality control, production efficiency, and impurity detection. Early\nidentification of seed types is critical to reducing the cost and risk\nassociated with field emergence, which can lead to yield losses or disruptions\nin downstream processes like harvesting. Seed sampling supports growers in\nmonitoring and managing seed quality, improving precision in determining seed\npurity levels, guiding management adjustments, and enhancing yield estimations.\nThis study proposes a novel convolutional neural network (CNN)-based framework\nfor the efficient classification of ten common Brassica seed types. The\napproach addresses the inherent challenge of texture similarity in seed images\nusing a custom-designed CNN architecture. The model's performance was evaluated\nagainst several pre-trained state-of-the-art architectures, with adjustments to\nlayer configurations for optimized classification. Experimental results using\nour collected Brassica seed dataset demonstrate that the proposed model\nachieved a high accuracy rate of 93 percent.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u5206\u7c7b\u5341\u79cd\u5e38\u89c1\u7684Brassica\u79cd\u5b50\uff0c\u89e3\u51b3\u4e86\u79cd\u5b50\u56fe\u50cf\u7eb9\u7406\u76f8\u4f3c\u6027\u7684\u6311\u6218\uff0c\u5e76\u5728\u81ea\u6536\u96c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8693%\u7684\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u519c\u6c11\u56e0\u519c\u4f5c\u7269\u751f\u4ea7\u548c\u519c\u573a\u8fd0\u8425\u7684\u7e41\u5fd9\u7f3a\u5c11\u65f6\u95f4\u548c\u8d44\u6e90\u8fdb\u884c\u5b9e\u5730\u7814\u7a76\uff0c\u79cd\u5b50\u5206\u7c7b\u5bf9\u4e8e\u8d28\u91cf\u63a7\u5236\u3001\u751f\u4ea7\u6548\u7387\u548c\u6742\u8d28\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u65e9\u671f\u8bc6\u522b\u79cd\u5b50\u7c7b\u578b\u6709\u52a9\u4e8e\u964d\u4f4e\u6210\u672c\u548c\u98ce\u9669\u3002", "method": "\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5b9a\u5236\u5316\u7684CNN\u67b6\u6784\uff0c\u901a\u8fc7\u8c03\u6574\u5c42\u7ea7\u914d\u7f6e\u4f18\u5316\u5206\u7c7b\uff0c\u5e76\u4e0e\u591a\u79cd\u9884\u8bad\u7ec3\u7684\u6700\u5148\u8fdb\u67b6\u6784\u8fdb\u884c\u4e86\u6027\u80fd\u6bd4\u8f83\u3002", "result": "\u5728\u81ea\u6536\u96c6\u7684Brassica\u79cd\u5b50\u6570\u636e\u96c6\u4e0a\uff0c\u63d0\u51fa\u7684\u6a21\u578b\u8fbe\u5230\u4e8693%\u7684\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u79cd\u5b50\u5206\u7c7b\u4e2d\u7684\u7eb9\u7406\u76f8\u4f3c\u6027\u95ee\u9898\uff0c\u4e3a\u519c\u6c11\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u79cd\u5b50\u8d28\u91cf\u76d1\u63a7\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u79cd\u5b50\u5206\u7c7b, \u5377\u79ef\u795e\u7ecf\u7f51\u7edc, \u6df1\u5ea6\u5b66\u4e60, \u519c\u4e1a\u7814\u7a76, \u8d28\u91cf\u63a7\u5236"}}
{"id": "2505.22157", "pdf": "https://arxiv.org/pdf/2505.22157", "abs": "https://arxiv.org/abs/2505.22157", "authors": ["Paramita Mirza", "Lucas Weber", "Fabian K\u00fcch"], "title": "Stratified Selective Sampling for Instruction Tuning with Dedicated Scoring Strategy", "categories": ["cs.CL"], "comment": null, "summary": "Recent work shows that post-training datasets for LLMs can be substantially\ndownsampled without noticeably deteriorating performance. However, data\nselection often incurs high computational costs or is limited to narrow\ndomains. In this paper, we demonstrate that data selection can be both --\nefficient and universal -- by using a multi-step pipeline in which we\nefficiently bin data points into groups, estimate quality using specialized\nmodels, and score difficulty with a robust, lightweight method. Task-based\ncategorization allows us to control the composition of our final data --\ncrucial for finetuning multi-purpose models. To guarantee diversity, we improve\nupon previous work using embedding models and a clustering algorithm. This\nintegrated strategy enables high-performance fine-tuning with minimal overhead.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6b65\u9aa4\u6d41\u7a0b\u5206\u7ec4\u3001\u8d28\u91cf\u8bc4\u4f30\u548c\u96be\u5ea6\u8bc4\u5206\uff0c\u4f18\u5316LLM\u5fae\u8c03\u6570\u636e\u96c6\uff0c\u4fdd\u8bc1\u591a\u6837\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u9002\u7528\u8303\u56f4\u7a84\uff0c\u672c\u6587\u65e8\u5728\u5b9e\u73b0\u9ad8\u6548\u4e14\u901a\u7528\u7684\u6570\u636e\u9009\u62e9\u3002", "method": "\u91c7\u7528\u591a\u6b65\u6d41\u7a0b\uff1a\u6570\u636e\u5206\u7ec4\u3001\u4e13\u7528\u6a21\u578b\u8bc4\u4f30\u8d28\u91cf\u3001\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u8bc4\u5206\u96be\u5ea6\uff0c\u7ed3\u5408\u5d4c\u5165\u6a21\u578b\u548c\u805a\u7c7b\u7b97\u6cd5\u4fdd\u8bc1\u591a\u6837\u6027\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u5fae\u8c03\u4e14\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u9009\u62e9\u548c\u6a21\u578b\u5fae\u8c03\u4e2d\u9ad8\u6548\u4e14\u901a\u7528\uff0c\u9002\u7528\u4e8e\u591a\u7528\u9014\u6a21\u578b\u3002", "keywords": "\u6570\u636e\u9009\u62e9, LLM\u5fae\u8c03, \u591a\u6837\u6027, \u8f7b\u91cf\u7ea7\u65b9\u6cd5, \u805a\u7c7b\u7b97\u6cd5"}}
{"id": "2505.21910", "pdf": "https://arxiv.org/pdf/2505.21910", "abs": "https://arxiv.org/abs/2505.21910", "authors": ["Xianbiao Qi", "Yelin He", "Jiaquan Ye", "Chun-Guang Li", "Bojia Zi", "Xili Dai", "Qin Zou", "Rong Xiao"], "title": "Taming Transformer Without Using Learning Rate Warmup", "categories": ["cs.LG", "cs.CV"], "comment": "This paper is published as a conference paper at ICLR 2025", "summary": "Scaling Transformer to a large scale without using some technical tricks such\nas learning rate warump and using an obviously lower learning rate is an\nextremely challenging task, and is increasingly gaining more attention. In this\npaper, we provide a theoretical analysis for the process of training\nTransformer and reveal the rationale behind the model crash phenomenon in the\ntraining process, termed \\textit{spectral energy concentration} of\n${\\bW_q}^{\\top} \\bW_k$, which is the reason for a malignant entropy collapse,\nwhere ${\\bW_q}$ and $\\bW_k$ are the projection matrices for the query and the\nkey in Transformer, respectively. To remedy this problem, motivated by\n\\textit{Weyl's Inequality}, we present a novel optimization strategy, \\ie,\nmaking the weight updating in successive steps smooth -- if the ratio\n$\\frac{\\sigma_{1}(\\nabla \\bW_t)}{\\sigma_{1}(\\bW_{t-1})}$ is larger than a\nthreshold, we will automatically bound the learning rate to a weighted multiple\nof $\\frac{\\sigma_{1}(\\bW_{t-1})}{\\sigma_{1}(\\nabla \\bW_t)}$, where $\\nabla\n\\bW_t$ is the updating quantity in step $t$. Such an optimization strategy can\nprevent spectral energy concentration to only a few directions, and thus can\navoid malignant entropy collapse which will trigger the model crash. We conduct\nextensive experiments using ViT, Swin-Transformer and GPT, showing that our\noptimization strategy can effectively and stably train these Transformers\nwithout using learning rate warmup.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u5e73\u6ed1\u6743\u91cd\u66f4\u65b0\u6b65\u9aa4\u6765\u9632\u6b62Transformer\u8bad\u7ec3\u4e2d\u7684\u8c31\u80fd\u91cf\u96c6\u4e2d\uff0c\u4ece\u800c\u907f\u514d\u6076\u6027\u71b5\u5d29\u6e83\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u7b56\u7565\u5728\u4e0d\u4f7f\u7528\u5b66\u4e60\u7387\u9884\u70ed\u7684\u60c5\u51b5\u4e0b\u80fd\u7a33\u5b9a\u8bad\u7ec3ViT\u3001Swin-Transformer\u548cGPT\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3Transformer\u5728\u5927\u89c4\u6a21\u8bad\u7ec3\u4e2d\u56e0\u8c31\u80fd\u91cf\u96c6\u4e2d\u5bfc\u81f4\u7684\u6076\u6027\u71b5\u5d29\u6e83\u95ee\u9898\uff0c\u907f\u514d\u6a21\u578b\u5d29\u6e83\uff0c\u540c\u65f6\u6446\u8131\u5bf9\u5b66\u4e60\u7387\u9884\u70ed\u7b49\u6280\u5de7\u7684\u4f9d\u8d56\u3002", "method": "\u65b9\u6cd5\u57fa\u4e8eWeyl\u4e0d\u7b49\u5f0f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u7b56\u7565\uff1a\u5982\u679c\u6743\u91cd\u66f4\u65b0\u7684\u68af\u5ea6\u4e0e\u4e0a\u4e00\u6b65\u6743\u91cd\u7684\u5947\u5f02\u503c\u6bd4\u8d85\u8fc7\u9608\u503c\uff0c\u5219\u81ea\u52a8\u9650\u5236\u5b66\u4e60\u7387\u4e3a\u5176\u52a0\u6743\u5012\u6570\uff0c\u4ee5\u9632\u6b62\u8c31\u80fd\u91cf\u8fc7\u5ea6\u96c6\u4e2d\u3002", "result": "\u901a\u8fc7\u5728ViT\u3001Swin-Transformer\u548cGPT\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u7b56\u7565\u80fd\u6709\u6548\u7a33\u5b9a\u8bad\u7ec3\uff0c\u65e0\u9700\u5b66\u4e60\u7387\u9884\u70ed\u3002", "conclusion": "\u7ed3\u8bba\u662f\u63d0\u51fa\u7684\u4f18\u5316\u7b56\u7565\u80fd\u907f\u514d\u8c31\u80fd\u91cf\u96c6\u4e2d\u548c\u71b5\u5d29\u6e83\uff0c\u7a33\u5b9a\u8bad\u7ec3Transformer\u6a21\u578b\u3002", "keywords": "Transformer, \u8c31\u80fd\u91cf\u96c6\u4e2d, \u71b5\u5d29\u6e83, Weyl\u4e0d\u7b49\u5f0f, \u4f18\u5316\u7b56\u7565"}}
{"id": "2505.21559", "pdf": "https://arxiv.org/pdf/2505.21559", "abs": "https://arxiv.org/abs/2505.21559", "authors": ["Julien Soul\u00e9", "Jean-Paul Jamont", "Michel Occello", "Louis-Marie Traonouez", "Paul Th\u00e9ron"], "title": "Streamlining Resilient Kubernetes Autoscaling with Multi-Agent Systems via an Automated Online Design Framework", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "In cloud-native systems, Kubernetes clusters with interdependent services\noften face challenges to their operational resilience due to poor workload\nmanagement issues such as resource blocking, bottlenecks, or continuous pod\ncrashes. These vulnerabilities are further amplified in adversarial scenarios,\nsuch as Distributed Denial-of-Service attacks (DDoS). Conventional Horizontal\nPod Autoscaling (HPA) approaches struggle to address such dynamic conditions,\nwhile reinforcement learning-based methods, though more adaptable, typically\noptimize single goals like latency or resource usage, neglecting broader\nfailure scenarios. We propose decomposing the overarching goal of maintaining\noperational resilience into failure-specific sub-goals delegated to\ncollaborative agents, collectively forming an HPA Multi-Agent System (MAS). We\nintroduce an automated, four-phase online framework for HPA MAS design: 1)\nmodeling a digital twin built from cluster traces; 2) training agents in\nsimulation using roles and missions tailored to failure contexts; 3) analyzing\nagent behaviors for explainability; and 4) transferring learned policies to the\nreal cluster. Experimental results demonstrate that the generated HPA MASs\noutperform three state-of-the-art HPA systems in sustaining operational\nresilience under various adversarial conditions in a proposed complex cluster.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u7684\u6c34\u5e73Pod\u81ea\u52a8\u6269\u5c55\uff08HPA\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u76ee\u6807\u4e3a\u5b50\u4efb\u52a1\u5e76\u534f\u540c\u4f18\u5316\uff0c\u63d0\u5347Kubernetes\u96c6\u7fa4\u5728\u5bf9\u6297\u6027\u6761\u4ef6\u4e0b\u7684\u64cd\u4f5c\u5f39\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfHPA\u65b9\u6cd5\u65e0\u6cd5\u5e94\u5bf9\u52a8\u6001\u5bf9\u6297\u6027\u6761\u4ef6\uff08\u5982DDoS\u653b\u51fb\uff09\u7684\u5c40\u9650\u6027\uff0c\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u591a\u4f18\u5316\u5355\u4e00\u76ee\u6807\uff0c\u5ffd\u89c6\u5e7f\u6cdb\u6545\u969c\u573a\u666f\u3002", "method": "\u63d0\u51fa\u56db\u9636\u6bb5\u5728\u7ebfHPA MAS\u8bbe\u8ba1\u6846\u67b6\uff1a1\uff09\u57fa\u4e8e\u96c6\u7fa4\u65e5\u5fd7\u6784\u5efa\u6570\u5b57\u5b6a\u751f\uff1b2\uff09\u9488\u5bf9\u6545\u969c\u573a\u666f\u8bad\u7ec3\u534f\u4f5c\u667a\u80fd\u4f53\uff1b3\uff09\u5206\u6790\u667a\u80fd\u4f53\u884c\u4e3a\u53ef\u89e3\u91ca\u6027\uff1b4\uff09\u5c06\u7b56\u7565\u8fc1\u79fb\u81f3\u771f\u5b9e\u96c6\u7fa4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u751f\u6210\u7684HPA MAS\u5728\u590d\u6742\u96c6\u7fa4\u4e2d\u4f18\u4e8e\u4e09\u79cd\u5148\u8fdbHPA\u7cfb\u7edf\uff0c\u80fd\u5728\u591a\u79cd\u5bf9\u6297\u6761\u4ef6\u4e0b\u4fdd\u6301\u64cd\u4f5c\u5f39\u6027\u3002", "conclusion": "\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u540c\u5206\u89e3\u76ee\u6807\uff0c\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86Kubernetes\u96c6\u7fa4\u7684\u5f39\u6027\uff0c\u5c24\u5176\u5728\u52a8\u6001\u5bf9\u6297\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "keywords": "Kubernetes, \u6c34\u5e73Pod\u81ea\u52a8\u6269\u5c55, \u591a\u667a\u80fd\u4f53\u7cfb\u7edf, \u64cd\u4f5c\u5f39\u6027, DDoS\u653b\u51fb"}}
{"id": "2505.22165", "pdf": "https://arxiv.org/pdf/2505.22165", "abs": "https://arxiv.org/abs/2505.22165", "authors": ["Bocheng Li", "Zhujin Gao", "Linli Xu"], "title": "Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Diffusion models have emerged as a promising approach for text generation,\nwith recent works falling into two main categories: discrete and continuous\ndiffusion models. Discrete diffusion models apply token corruption\nindependently using categorical distributions, allowing for different diffusion\nprogress across tokens but lacking fine-grained control. Continuous diffusion\nmodels map tokens to continuous spaces and apply fine-grained noise, but the\ndiffusion progress is uniform across tokens, limiting their ability to capture\nsemantic nuances. To address these limitations, we propose\n\\textbf{\\underline{N}}on-simultan\\textbf{\\underline{e}}ous\nC\\textbf{\\underline{o}}ntinuous \\textbf{\\underline{Diff}}usion Models\n(NeoDiff), a novel diffusion model that integrates the strengths of both\ndiscrete and continuous approaches. NeoDiff introduces a Poisson diffusion\nprocess for the forward process, enabling a flexible and fine-grained noising\nparadigm, and employs a time predictor for the reverse process to adaptively\nmodulate the denoising progress based on token semantics. Furthermore, NeoDiff\nutilizes an optimized schedule for inference to ensure more precise noise\ncontrol and improved performance. Our approach unifies the theories of discrete\nand continuous diffusion models, offering a more principled and effective\nframework for text generation. Experimental results on several text generation\ntasks demonstrate NeoDiff's superior performance compared to baselines of\nnon-autoregressive continuous and discrete diffusion models, iterative-based\nmethods and autoregressive diffusion-based methods. These results highlight\nNeoDiff's potential as a powerful tool for generating high-quality text and\nadvancing the field of diffusion-based text generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNeoDiff\u7684\u65b0\u578b\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u6574\u5408\u79bb\u6563\u548c\u8fde\u7eed\u6269\u6563\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002NeoDiff\u5728\u6b63\u5411\u8fc7\u7a0b\u4e2d\u5f15\u5165\u6cca\u677e\u6269\u6563\u8fc7\u7a0b\uff0c\u53cd\u5411\u8fc7\u7a0b\u4f7f\u7528\u65f6\u95f4\u9884\u6d4b\u5668\u81ea\u9002\u5e94\u8c03\u8282\u53bb\u566a\u8fdb\u5ea6\uff0c\u5e76\u5728\u63a8\u7406\u9636\u6bb5\u4f18\u5316\u8c03\u5ea6\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cNeoDiff\u5728\u591a\u4e2a\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u79bb\u6563\u548c\u8fde\u7eed\u6269\u6563\u6a21\u578b\u5404\u6709\u4f18\u7f3a\u70b9\uff1a\u79bb\u6563\u6a21\u578b\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u8fde\u7eed\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u8bed\u4e49\u7ec6\u5fae\u5dee\u522b\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\uff0c\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u6587\u672c\u751f\u6210\u6846\u67b6\u3002", "method": "NeoDiff\u901a\u8fc7\u6cca\u677e\u6269\u6563\u8fc7\u7a0b\uff08\u6b63\u5411\u8fc7\u7a0b\uff09\u548c\u65f6\u95f4\u9884\u6d4b\u5668\uff08\u53cd\u5411\u8fc7\u7a0b\uff09\u5b9e\u73b0\u7075\u6d3b\u7684\u7ec6\u7c92\u5ea6\u566a\u58f0\u63a7\u5236\u548c\u81ea\u9002\u5e94\u53bb\u566a\uff0c\u5e76\u4f18\u5316\u63a8\u7406\u8c03\u5ea6\u3002", "result": "\u5b9e\u9a8c\u663e\u793aNeoDiff\u5728\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u975e\u81ea\u56de\u5f52/\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\u53ca\u8fed\u4ee3\u65b9\u6cd5\u3002", "conclusion": "NeoDiff\u4e3a\u6587\u672c\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u7406\u8bba\u5316\u548c\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u751f\u6210\u9886\u57df\u7684\u53d1\u5c55\u3002", "keywords": "\u6269\u6563\u6a21\u578b, \u6587\u672c\u751f\u6210, \u6cca\u677e\u8fc7\u7a0b, \u65f6\u95f4\u9884\u6d4b\u5668, \u975e\u81ea\u56de\u5f52"}}
{"id": "2505.21918", "pdf": "https://arxiv.org/pdf/2505.21918", "abs": "https://arxiv.org/abs/2505.21918", "authors": ["Haruki Kai", "Tsuyoshi Okita"], "title": "Self-supervised Learning Method Using Transformer for Multi-dimensional Sensor Data Processing", "categories": ["cs.LG", "cs.AI"], "comment": "25 pages, 4 figures", "summary": "We developed a deep learning algorithm for human activity recognition using\nsensor signals as input. In this study, we built a pretrained language model\nbased on the Transformer architecture, which is widely used in natural language\nprocessing. By leveraging this pretrained model, we aimed to improve\nperformance on the downstream task of human activity recognition. While this\ntask can be addressed using a vanilla Transformer, we propose an enhanced\nn-dimensional numerical processing Transformer that incorporates three key\nfeatures: embedding n-dimensional numerical data through a linear layer,\nbinning-based pre-processing, and a linear transformation in the output layer.\nWe evaluated the effectiveness of our proposed model across five different\ndatasets. Compared to the vanilla Transformer, our model demonstrated 10%-15%\nimprovements in accuracy.", "AI": {"tldr": "\u6458\u8981\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eTransformer\u67b6\u6784\u7684\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u901a\u8fc7\u4f20\u611f\u5668\u4fe1\u53f7\u8fdb\u884c\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u7684n\u7ef4\u6570\u503c\u5904\u7406Transformer\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8610%-15%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4f20\u7edfTransformer\u5728\u8be5\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684n\u7ef4\u6570\u503c\u5904\u7406Transformer\uff0c\u5305\u542b\u7ebf\u6027\u5c42\u5d4c\u5165\u3001\u57fa\u4e8e\u5206\u7bb1\u7684\u9884\u5904\u7406\u548c\u8f93\u51fa\u5c42\u7684\u7ebf\u6027\u53d8\u6362\u4e09\u4e2a\u5173\u952e\u7279\u5f81\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u76f8\u8f83\u4e8e\u4f20\u7edfTransformer\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e8610%-15%\u3002", "conclusion": "\u6539\u8fdb\u7684n\u7ef4\u6570\u503c\u5904\u7406Transformer\u663e\u8457\u63d0\u5347\u4e86\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u4e3a\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u6df1\u5ea6\u5b66\u4e60, \u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b, Transformer, n\u7ef4\u6570\u503c\u5904\u7406, \u4f20\u611f\u5668\u4fe1\u53f7"}}
{"id": "2505.21562", "pdf": "https://arxiv.org/pdf/2505.21562", "abs": "https://arxiv.org/abs/2505.21562", "authors": ["Jennifer Turliuk", "Alejandro Sevilla", "Daniela Gorza", "Tod Hynes"], "title": "Enhancing Selection of Climate Tech Startups with AI -- A Case Study on Integrating Human and AI Evaluations in the ClimaTech Great Global Innovation Challenge", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "This case study examines the ClimaTech Great Global Innovation Challenge's\napproach to selecting climate tech startups by integrating human and AI\nevaluations. The competition aimed to identify top startups and enhance the\naccuracy and efficiency of the selection process through a hybrid model.\nResearch shows data-driven approaches help VC firms reduce bias and improve\ndecision-making. Machine learning models have outperformed human investors in\ndeal screening, helping identify high-potential startups. Incorporating AI\naimed to ensure more equitable and objective evaluations.\n  The methodology included three phases: initial AI review, semi-finals judged\nby humans, and finals using a hybrid weighting. In phase one, 57 applications\nwere scored by an AI tool built with StackAI and OpenAI's GPT-4o, and the top\n36 advanced. In the semi-finals, human judges, unaware of AI scores, evaluated\nstartups on team quality, market potential, and technological innovation. Each\nscore - human or AI - was weighted equally, resulting in 75 percent human and\n25 percent AI influence. In the finals, with five human judges, weighting\nshifted to 83.3 percent human and 16.7 percent AI. There was a moderate\npositive correlation between AI and human scores - Spearman's = 0.47 -\nindicating general alignment with key differences. Notably, the final four\nstartups, selected mainly by humans, were among those rated highest by the AI.\nThis highlights the complementary nature of AI and human judgment. The study\nshows that hybrid models can streamline and improve startup assessments. The\nClimaTech approach offers a strong framework for future competitions by\ncombining human expertise with AI capabilities.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ClimaTech\u5168\u7403\u521b\u65b0\u6311\u6218\u8d5b\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u548cAI\u8bc4\u4f30\u6765\u9009\u62e9\u6c14\u5019\u79d1\u6280\u521d\u521b\u4f01\u4e1a\u7684\u65b9\u6cd5\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u6df7\u5408\u6a21\u578b\u63d0\u9ad8\u521d\u521b\u4f01\u4e1a\u9009\u62e9\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u540c\u65f6\u51cf\u5c11\u504f\u89c1\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a\u521d\u59cbAI\u8bc4\u5ba1\u3001\u534a\u51b3\u8d5b\u4eba\u7c7b\u8bc4\u5ba1\u548c\u51b3\u8d5b\u6df7\u5408\u6743\u91cd\u8bc4\u4f30\u3002", "result": "AI\u4e0e\u4eba\u7c7b\u8bc4\u59d4\u8bc4\u5206\u5448\u73b0\u4e2d\u7b49\u6b63\u76f8\u5173\uff08Spearman's=0.47\uff09\uff0c\u6700\u7ec8\u9009\u51fa\u7684\u521d\u521b\u4f01\u4e1a\u4e2d\uff0c\u4eba\u7c7b\u548cAI\u8bc4\u5206\u8f83\u9ad8\u7684\u4f01\u4e1a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u6df7\u5408\u6a21\u578b\u53ef\u4ee5\u4f18\u5316\u521d\u521b\u4f01\u4e1a\u8bc4\u4f30\uff0c\u672a\u6765\u53ef\u4f5c\u4e3a\u7c7b\u4f3c\u7ade\u8d5b\u7684\u53c2\u8003\u6846\u67b6\u3002", "keywords": "\u6c14\u5019\u79d1\u6280\uff0c\u521d\u521b\u4f01\u4e1a\u8bc4\u4f30\uff0c\u4eba\u5de5\u667a\u80fd\uff0c\u6df7\u5408\u6a21\u578b"}}
{"id": "2505.22169", "pdf": "https://arxiv.org/pdf/2505.22169", "abs": "https://arxiv.org/abs/2505.22169", "authors": ["Gili Lior", "Eliya Habba", "Shahar Levy", "Avi Caciularu", "Gabriel Stanovsky"], "title": "ReliableEval: A Recipe for Stochastic LLM Evaluation via Method of Moments", "categories": ["cs.CL"], "comment": null, "summary": "LLMs are highly sensitive to prompt phrasing, yet standard benchmarks\ntypically report performance using a single prompt, raising concerns about the\nreliability of such evaluations. In this work, we argue for a stochastic method\nof moments evaluation over the space of meaning-preserving prompt\nperturbations. We introduce a formal definition of reliable evaluation that\naccounts for prompt sensitivity, and suggest ReliableEval - a method for\nestimating the number of prompt resamplings needed to obtain meaningful\nresults. Using our framework, we stochastically evaluate five frontier LLMs and\nfind that even top-performing models like GPT-4o and Claude-3.7-Sonnet exhibit\nsubstantial prompt sensitivity. Our approach is model-, task-, and\nmetric-agnostic, offering a recipe for meaningful and robust LLM evaluation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u65b9\u6cd5\u8bc4\u4f30LLM\u5bf9\u63d0\u793a\u654f\u611f\u6027\u7684\u6846\u67b6ReliableEval\uff0c\u5f3a\u8c03\u73b0\u6709\u5355\u4e00\u63d0\u793a\u8bc4\u4f30\u7684\u4e0d\u53ef\u9760\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u5373\u4f7f\u9876\u7ea7\u6a21\u578b\u4e5f\u5b58\u5728\u663e\u8457\u63d0\u793a\u654f\u611f\u6027\u3002", "motivation": "\u73b0\u6709LLM\u8bc4\u4f30\u901a\u5e38\u4ec5\u57fa\u4e8e\u5355\u4e00\u63d0\u793a\uff0c\u65e0\u6cd5\u53cd\u6620\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u8986\u76d6\u542b\u4e49\u4e00\u81f4\u63d0\u793a\u53d8\u4f53\u7684\u53ef\u9760\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5f15\u5165ReliableEval\u65b9\u6cd5\uff0c\u901a\u8fc7\u968f\u673a\u91c7\u6837\u542b\u4e49\u4e00\u81f4\u7684\u63d0\u793a\u6270\u52a8\uff0c\u5e76\u7edf\u8ba1\u8ba1\u7b97\u6240\u9700\u91c7\u6837\u6b21\u6570\u4ee5\u5b9e\u73b0\u8bc4\u4f30\u7a33\u5b9a\u6027\u3002", "result": "\u5bf9\u4e94\u79cd\u524d\u6cbfLLM\uff08\u5982GPT-4o\u548cClaude-3.7-Sonnet\uff09\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8fd9\u4e9b\u6a21\u578b\u5bf9\u63d0\u793a\u5b58\u5728\u663e\u8457\u654f\u611f\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e0e\u6a21\u578b\u3001\u4efb\u52a1\u548c\u6307\u6807\u65e0\u5173\u7684\u53ef\u9760\u65b9\u6cd5\u3002", "keywords": "LLM\u8bc4\u4f30, \u63d0\u793a\u654f\u611f\u6027, \u968f\u673a\u65b9\u6cd5, ReliableEval, \u9c81\u68d2\u6027"}}
{"id": "2505.21923", "pdf": "https://arxiv.org/pdf/2505.21923", "abs": "https://arxiv.org/abs/2505.21923", "authors": ["Asal Mehradfar", "Xuzhe Zhao", "Yilun Huang", "Emir Ceyani", "Yankai Yang", "Shihao Han", "Hamidreza Aghasi", "Salman Avestimehr"], "title": "FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.CE"], "comment": null, "summary": "Designing analog circuits from performance specifications is a complex,\nmulti-stage process encompassing topology selection, parameter inference, and\nlayout feasibility. We introduce FALCON, a unified machine learning framework\nthat enables fully automated, specification-driven analog circuit synthesis\nthrough topology selection and layout-constrained optimization. Given a target\nperformance, FALCON first selects an appropriate circuit topology using a\nperformance-driven classifier guided by human design heuristics. Next, it\nemploys a custom, edge-centric graph neural network trained to map circuit\ntopology and parameters to performance, enabling gradient-based parameter\ninference through the learned forward model. This inference is guided by a\ndifferentiable layout cost, derived from analytical equations capturing\nparasitic and frequency-dependent effects, and constrained by design rules. We\ntrain and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave\ncircuits, generated and simulated using Cadence Spectre across 20\nexpert-designed topologies. Through this evaluation, FALCON demonstrates >99\\%\naccuracy in topology inference, <10\\% relative error in performance prediction,\nand efficient layout-aware design that completes in under 1 second per\ninstance. Together, these results position FALCON as a practical and extensible\nfoundation model for end-to-end analog circuit design automation.", "AI": {"tldr": "FALCON\u662f\u4e00\u79cd\u7edf\u4e00\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\uff0c\u5305\u62ec\u62d3\u6251\u9009\u62e9\u548c\u5e03\u5c40\u7ea6\u675f\u4f18\u5316\uff0c\u6027\u80fd\u9ad8\u4e14\u901f\u5ea6\u5feb\u3002", "motivation": "\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u662f\u4e00\u4e2a\u590d\u6742\u591a\u9636\u6bb5\u8fc7\u7a0b\uff0cFALCON\u65e8\u5728\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u5b9e\u73b0\u81ea\u52a8\u5316\u8bbe\u8ba1\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "FALCON\u7ed3\u5408\u6027\u80fd\u9a71\u52a8\u5206\u7c7b\u5668\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u5b9e\u73b0\u62d3\u6251\u9009\u62e9\u548c\u68af\u5ea6\u53c2\u6570\u63a8\u65ad\uff0c\u540c\u65f6\u8003\u8651\u5e03\u5c40\u6210\u672c\u548c\u8bbe\u8ba1\u89c4\u5219\u7ea6\u675f\u3002", "result": "FALCON\u5728\u62d3\u6251\u63a8\u65ad\u51c6\u786e\u7387>99\uff05\uff0c\u6027\u80fd\u9884\u6d4b\u76f8\u5bf9\u8bef\u5dee<10\uff05\uff0c\u4e14\u6bcf\u4e2a\u8bbe\u8ba1\u5b9e\u4f8b\u5b8c\u6210\u65f6\u95f4\u4f4e\u4e8e1\u79d2\u3002", "conclusion": "FALCON\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u7aef\u5230\u7aef\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u81ea\u52a8\u5316\u57fa\u7840\u6a21\u578b\u3002", "keywords": "\u6a21\u62df\u7535\u8def\u8bbe\u8ba1, \u673a\u5668\u5b66\u4e60, \u62d3\u6251\u9009\u62e9, \u5e03\u5c40\u4f18\u5316, \u56fe\u795e\u7ecf\u7f51\u7edc"}}
{"id": "2505.21563", "pdf": "https://arxiv.org/pdf/2505.21563", "abs": "https://arxiv.org/abs/2505.21563", "authors": ["Kai Yang", "Hui Ma", "Shaoyu Dou"], "title": "Fog Intelligence for Network Anomaly Detection", "categories": ["cs.NI", "cs.AI"], "comment": "published in IEEE Network", "summary": "Anomalies are common in network system monitoring. When manifested as network\nthreats to be mitigated, service outages to be prevented, and security risks to\nbe ameliorated, detecting such anomalous network behaviors becomes of great\nimportance. However, the growing scale and complexity of the mobile\ncommunication networks, as well as the ever-increasing amount and\ndimensionality of the network surveillance data, make it extremely difficult to\nmonitor a mobile network and discover abnormal network behaviors. Recent\nadvances in machine learning allow for obtaining near-optimal solutions to\ncomplicated decision-making problems with many sources of uncertainty that\ncannot be accurately characterized by traditional mathematical models. However,\nmost machine learning algorithms are centralized, which renders them\ninapplicable to a large-scale distributed wireless networks with tens of\nmillions of mobile devices. In this article, we present fog intelligence, a\ndistributed machine learning architecture that enables intelligent wireless\nnetwork management. It preserves the advantage of both edge processing and\ncentralized cloud computing. In addition, the proposed architecture is\nscalable, privacy-preserving, and well suited for intelligent management of a\ndistributed wireless network.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a'\u96fe\u667a\u80fd'\u7684\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u65e0\u7ebf\u7f51\u7edc\u4e2d\u5f02\u5e38\u884c\u4e3a\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u7ed3\u5408\u8fb9\u7f18\u5904\u7406\u548c\u4e91\u8ba1\u7b97\u7684\u4f18\u52bf\u3002", "motivation": "\u7531\u4e8e\u79fb\u52a8\u901a\u4fe1\u7f51\u7edc\u89c4\u6a21\u6269\u5927\u3001\u6570\u636e\u91cf\u589e\u52a0\u4ee5\u53ca\u4f20\u7edf\u96c6\u4e2d\u5f0f\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5728\u5206\u5e03\u5f0f\u7f51\u7edc\u4e2d\u5e94\u7528\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u67b6\u6784\u6765\u5b9e\u73b0\u667a\u80fd\u7f51\u7edc\u7ba1\u7406\u3002", "method": "\u63d0\u51fa\u4e86'\u96fe\u667a\u80fd'\u67b6\u6784\uff0c\u7ed3\u5408\u8fb9\u7f18\u8ba1\u7b97\u548c\u96c6\u4e2d\u5f0f\u4e91\u8ba1\u7b97\uff0c\u5b9e\u73b0\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u3002", "result": "\u8be5\u67b6\u6784\u5177\u6709\u53ef\u6269\u5c55\u6027\u3001\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5206\u5e03\u5f0f\u65e0\u7ebf\u7f51\u7edc\u7684\u667a\u80fd\u7ba1\u7406\u3002", "conclusion": "\u96fe\u667a\u80fd\u67b6\u6784\u4e3a\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u7f51\u7edc\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u96fe\u667a\u80fd, \u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60, \u65e0\u7ebf\u7f51\u7edc\u7ba1\u7406, \u5f02\u5e38\u68c0\u6d4b, \u8fb9\u7f18\u8ba1\u7b97"}}
{"id": "2505.22172", "pdf": "https://arxiv.org/pdf/2505.22172", "abs": "https://arxiv.org/abs/2505.22172", "authors": ["Xiang Huang", "Ting-En Lin", "Feiteng Fang", "Yuchuan Wu", "Hangyu Li", "Yuzhong Qu", "Fei Huang", "Yongbin Li"], "title": "Reverse Preference Optimization for Complex Instruction Following", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Instruction following (IF) is a critical capability for large language models\n(LLMs). However, handling complex instructions with multiple constraints\nremains challenging. Previous methods typically select preference pairs based\non the number of constraints they satisfy, introducing noise where chosen\nexamples may fail to follow some constraints and rejected examples may excel in\ncertain respects over the chosen ones. To address the challenge of aligning\nwith multiple preferences, we propose a simple yet effective method called\nReverse Preference Optimization (RPO). It mitigates noise in preference pairs\nby dynamically reversing the constraints within the instruction to ensure the\nchosen response is perfect, alleviating the burden of extensive sampling and\nfiltering to collect perfect responses. Besides, reversal also enlarges the gap\nbetween chosen and rejected responses, thereby clarifying the optimization\ndirection and making it more robust to noise. We evaluate RPO on two multi-turn\nIF benchmarks, Sysbench and Multi-IF, demonstrating average improvements over\nthe DPO baseline of 4.6 and 2.5 points (on Llama-3.1 8B), respectively.\nMoreover, RPO scales effectively across model sizes (8B to 70B parameters),\nwith the 70B RPO model surpassing GPT-4o.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5904\u7406\u591a\u7ea6\u675f\u590d\u6742\u6307\u4ee4\u65f6\u7684\u504f\u597d\u5bf9\u9f50\u95ee\u9898\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u53cd\u5411\u504f\u597d\u4f18\u5316\uff08RPO\uff09\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u53cd\u8f6c\u6307\u4ee4\u4e2d\u7684\u7ea6\u675f\u4ee5\u51cf\u5c11\u504f\u597d\u5bf9\u4e2d\u7684\u566a\u58f0\uff0c\u5e76\u5728\u591a\u8f6e\u6307\u4ee4\u9075\u5faa\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u57fa\u4e8e\u7ea6\u675f\u6ee1\u8db3\u6570\u91cf\u9009\u62e9\u504f\u597d\u5bf9\u65f6\u5f15\u5165\u566a\u58f0\uff0c\u5bfc\u81f4\u9009\u4e2d\u7684\u54cd\u5e94\u53ef\u80fd\u4e0d\u5b8c\u5168\u6ee1\u8db3\u7ea6\u675f\uff0c\u800c\u88ab\u62d2\u7edd\u7684\u54cd\u5e94\u53ef\u80fd\u5728\u67d0\u4e9b\u65b9\u9762\u4f18\u4e8e\u9009\u4e2d\u7684\u54cd\u5e94\u3002\u8fd9\u9700\u8981\u5927\u91cf\u91c7\u6837\u548c\u8fc7\u6ee4\u624d\u80fd\u83b7\u5f97\u5b8c\u7f8e\u54cd\u5e94\uff0c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u53cd\u5411\u504f\u597d\u4f18\u5316\uff08RPO\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u53cd\u8f6c\u6307\u4ee4\u4e2d\u7684\u7ea6\u675f\uff0c\u786e\u4fdd\u9009\u4e2d\u54cd\u5e94\u5b8c\u7f8e\u6ee1\u8db3\u8981\u6c42\uff0c\u540c\u65f6\u6269\u5927\u4e86\u9009\u4e2d\u4e0e\u88ab\u62d2\u7edd\u54cd\u5e94\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ece\u800c\u51cf\u5c11\u566a\u58f0\u5e76\u660e\u786e\u4f18\u5316\u65b9\u5411\u3002", "result": "\u5728Sysbench\u548cMulti-IF\u4e24\u4e2a\u591a\u8f6e\u6307\u4ee4\u9075\u5faa\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRPO\u5206\u522b\u6bd4DPO\u57fa\u7ebf\u5e73\u5747\u63d0\u9ad8\u4e864.6\u5206\u548c2.5\u5206\uff08\u57fa\u4e8eLlama-3.1 8B\u6a21\u578b\uff09\u300270B\u53c2\u6570\u7684RPO\u6a21\u578b\u751a\u81f3\u8d85\u8fc7\u4e86GPT-4o\u3002", "conclusion": "RPO\u662f\u4e00\u79cd\u7b80\u5355\u4f46\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11\u591a\u7ea6\u675f\u6307\u4ee4\u504f\u597d\u5bf9\u9f50\u4e2d\u7684\u566a\u58f0\uff0c\u5e76\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6269\u5c55\u6027\u548c\u6027\u80fd\u4f18\u52bf\u3002", "keywords": "\u6307\u4ee4\u9075\u5faa\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u504f\u597d\u4f18\u5316\u3001\u591a\u7ea6\u675f\u3001RPO"}}
{"id": "2505.21930", "pdf": "https://arxiv.org/pdf/2505.21930", "abs": "https://arxiv.org/abs/2505.21930", "authors": ["Dongyue Li", "Ziniu Zhang", "Lu Wang", "Hongyang R. Zhang"], "title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets", "categories": ["cs.LG", "cs.CL"], "comment": "17 pages. To appear in ACL'25", "summary": "This paper develops an ensemble method for fine-tuning a language model to\nmultiple datasets. Existing methods, such as quantized LoRA (QLoRA), are\nefficient when adapting to a single dataset. When training on multiple datasets\nof different tasks, a common setup in practice, it remains unclear how to\ndesign an efficient adaptation for fine-tuning language models. We propose to\nuse an ensemble of multiple smaller adapters instead of a single adapter per\ntask. We design an efficient algorithm that partitions $n$ datasets into $m$\ngroups, where $m$ is typically much smaller than $n$ in practice, and train one\nadapter for each group before taking a weighted combination to form the\nensemble. The algorithm leverages a first-order approximation property of\nlow-rank adaptation to quickly obtain the fine-tuning performances of dataset\ncombinations since methods like LoRA stay close to the base model. Hence, we\nuse the gradients of the base model to estimate its behavior during\nfine-tuning. Empirically, this approximation holds with less than $1\\%$ error\non models with up to $34$ billion parameters, leading to an estimation of true\nfine-tuning performances under $5\\%$ error while speeding up computation\ncompared to base fine-tuning by $105$ times. When applied to fine-tune Llama\nand GPT models on ten text classification tasks, our approach provides up to\n$10\\%$ higher average test accuracy over QLoRA, with only $9\\%$ more FLOPs. On\na Llama model with $34$ billion parameters, an ensemble of QLoRA increases test\naccuracy by $3\\%$ compared to QLoRA, with only $8\\%$ more FLOPs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u5230\u591a\u4e2a\u6570\u636e\u96c6\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u533a\u548c\u52a0\u6743\u7ec4\u5408\u9002\u914d\u5668\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u4e0e\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982QLoRA\uff09\u5728\u9002\u5e94\u5355\u4e2a\u6570\u636e\u96c6\u65f6\u9ad8\u6548\uff0c\u4f46\u5728\u591a\u4efb\u52a1\u6570\u636e\u96c6\u4e0a\u7684\u9ad8\u6548\u9002\u5e94\u4ecd\u4e0d\u660e\u786e\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4f7f\u7528\u591a\u4e2a\u5c0f\u9002\u914d\u5668\u7684\u96c6\u6210\u65b9\u6848\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7b97\u6cd5\u5c06n\u4e2a\u6570\u636e\u96c6\u5206\u533a\u4e3am\u7ec4\uff08m\u8fdc\u5c0f\u4e8en\uff09\uff0c\u6bcf\u7ec4\u8bad\u7ec3\u4e00\u4e2a\u9002\u914d\u5668\u540e\u52a0\u6743\u7ec4\u5408\u3002\u5229\u7528\u4f4e\u79e9\u9002\u914d\u7684\u4e00\u9636\u8fd1\u4f3c\u6027\u8d28\uff0c\u901a\u8fc7\u57fa\u6a21\u578b\u68af\u5ea6\u5feb\u901f\u4f30\u8ba1\u5fae\u8c03\u6027\u80fd\u3002", "result": "\u5728340\u4ebf\u53c2\u6570\u6a21\u578b\u4e0a\uff0c\u8bef\u5dee\u5c0f\u4e8e1%\uff0c\u771f\u5b9e\u5fae\u8c03\u6027\u80fd\u4f30\u8ba1\u8bef\u5dee\u4f4e\u4e8e5%\uff0c\u8ba1\u7b97\u901f\u5ea6\u6bd4\u57fa\u5fae\u8c03\u5feb105\u500d\u3002\u5728Llama\u548cGPT\u6a21\u578b\u4e0a\uff0c\u5e73\u5747\u6d4b\u8bd5\u51c6\u786e\u7387\u6bd4QLoRA\u9ad810%\uff0cFLOPs\u4ec5\u589e\u52a09%\u3002\u5bf9\u4e8e340\u4ebf\u53c2\u6570\u7684Llama\u6a21\u578b\uff0c\u96c6\u6210QLoRA\u6d4b\u8bd5\u51c6\u786e\u7387\u63d0\u9ad83%\uff0cFLOPs\u589e\u52a08%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u4efb\u52a1\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5fae\u8c03\uff0c\u4e14\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "keywords": "\u8bed\u8a00\u6a21\u578b, \u5fae\u8c03, \u591a\u6570\u636e\u96c6, \u96c6\u6210\u65b9\u6cd5, \u4f4e\u79e9\u9002\u914d"}}
{"id": "2505.21565", "pdf": "https://arxiv.org/pdf/2505.21565", "abs": "https://arxiv.org/abs/2505.21565", "authors": ["Haicheng Liao", "Zhenning Li", "Guohui Zhang", "Keqiang Li", "Chengzhong Xu"], "title": "Towards Human-Like Trajectory Prediction for Autonomous Driving: A Behavior-Centric Approach", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Predicting the trajectories of vehicles is crucial for the development of\nautonomous driving (AD) systems, particularly in complex and dynamic traffic\nenvironments. In this study, we introduce HiT (Human-like Trajectory\nPrediction), a novel model designed to enhance trajectory prediction by\nincorporating behavior-aware modules and dynamic centrality measures. Unlike\ntraditional methods that primarily rely on static graph structures, HiT\nleverages a dynamic framework that accounts for both direct and indirect\ninteractions among traffic participants. This allows the model to capture the\nsubtle yet significant influences of surrounding vehicles, enabling more\naccurate and human-like predictions. To evaluate HiT's performance, we\nconducted extensive experiments using diverse and challenging real-world\ndatasets, including NGSIM, HighD, RounD, ApolloScape, and MoCAD++. The results\ndemonstrate that HiT consistently outperforms other top models across multiple\nmetrics, particularly excelling in scenarios involving aggressive driving\nbehaviors. This research presents a significant step forward in trajectory\nprediction, offering a more reliable and interpretable approach for enhancing\nthe safety and efficiency of fully autonomous driving systems.", "AI": {"tldr": "HiT, \u4e00\u79cd\u65b0\u578b\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u884c\u4e3a\u611f\u77e5\u6a21\u5757\u548c\u52a8\u6001\u4e2d\u5fc3\u6027\u6d4b\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u4eba\u6027\u5316\u8868\u73b0\uff0c\u9002\u7528\u4e8e\u590d\u6742\u52a8\u6001\u4ea4\u901a\u73af\u5883\u3002", "motivation": "\u4e3a\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u590d\u6742\u52a8\u6001\u4ea4\u901a\u73af\u5883\u4e2d\u7684\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u80fd\u529b\uff0c\u89e3\u51b3\u4f20\u7edf\u9759\u6001\u56fe\u7ed3\u6784\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u95f4\u63a5\u4ea4\u4e92\u5f71\u54cd\u7684\u95ee\u9898\u3002", "method": "HiT\u6a21\u578b\u521b\u65b0\u6027\u5730\u4f7f\u7528\u52a8\u6001\u6846\u67b6\uff0c\u7ed3\u5408\u884c\u4e3a\u611f\u77e5\u6a21\u5757\u548c\u52a8\u6001\u4e2d\u5fc3\u6027\u6d4b\u91cf\uff0c\u6355\u6349\u8f66\u8f86\u95f4\u7684\u76f4\u63a5\u4e0e\u95f4\u63a5\u4ea4\u4e92\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cHiT\u5728\u591a\u79cd\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u5176\u4ed6\u9876\u7ea7\u6a21\u578b\uff0c\u5c24\u5176\u5728\u6d89\u53ca\u6fc0\u8fdb\u9a7e\u9a76\u884c\u4e3a\u7684\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "keywords": "\u81ea\u52a8\u9a7e\u9a76, \u8f68\u8ff9\u9884\u6d4b, \u52a8\u6001\u4ea4\u4e92, \u884c\u4e3a\u611f\u77e5, HiT\u6a21\u578b"}}
{"id": "2505.22176", "pdf": "https://arxiv.org/pdf/2505.22176", "abs": "https://arxiv.org/abs/2505.22176", "authors": ["Vihang Pancholi", "Jainit Bafna", "Tejas Anvekar", "Manish Shrivastava", "Vivek Gupta"], "title": "TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table Evaluation", "categories": ["cs.CL"], "comment": "Accepeted for Findings at ACL 2025", "summary": "Evaluating tables qualitatively & quantitatively presents a significant\nchallenge, as traditional metrics often fail to capture nuanced structural and\ncontent discrepancies. To address this, we introduce a novel, methodical rubric\nintegrating multi-level structural descriptors with fine-grained contextual\nquantification, thereby establishing a robust foundation for comprehensive\ntable comparison. Building on this foundation, we propose TabXEval, an\neXhaustive and eXplainable two-phase evaluation framework. TabXEval initially\naligns reference tables structurally via TabAlign & subsequently conducts a\nsystematic semantic and syntactic comparison using TabCompare; this approach\nclarifies the evaluation process and pinpoints subtle discrepancies overlooked\nby conventional methods. The efficacy of this framework is assessed using\nTabXBench, a novel, diverse, multi-domain benchmark we developed, featuring\nrealistic table perturbations and human-annotated assessments. Finally, a\nsystematic analysis of existing evaluation methods through\nsensitivity-specificity trade-offs demonstrates the qualitative and\nquantitative effectiveness of TabXEval across diverse table-related tasks and\ndomains, paving the way for future innovations in explainable table evaluation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTabXEval\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5bf9\u9f50\u548c\u8bed\u4e49/\u53e5\u6cd5\u6bd4\u8f83\u6765\u5168\u9762\u8bc4\u4f30\u8868\u683c\uff0c\u5e76\u4f7f\u7528\u65b0\u7684\u57fa\u51c6TabXBench\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u8868\u683c\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7ed3\u6784\u548c\u5185\u5bb9\u4e0a\u7684\u7ec6\u5fae\u5dee\u5f02\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "TabXEval\u6846\u67b6\u5206\u4e3a\u4e24\u9636\u6bb5\uff1aTabAlign\u8fdb\u884c\u7ed3\u6784\u5bf9\u9f50\uff0cTabCompare\u8fdb\u884c\u8bed\u4e49\u548c\u53e5\u6cd5\u6bd4\u8f83\u3002\u9a8c\u8bc1\u4f7f\u7528\u4e86\u591a\u9886\u57df\u57fa\u51c6TabXBench\u3002", "result": "TabXEval\u5728\u591a\u6837\u5316\u7684\u8868\u683c\u4efb\u52a1\u548c\u9886\u57df\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u89e3\u91ca\u6027\u548c\u6548\u679c\u4e0a\u7684\u4f18\u52bf\u3002", "conclusion": "TabXEval\u4e3a\u8868\u683c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u548c\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "keywords": "\u8868\u683c\u8bc4\u4f30, TabXEval, \u7ed3\u6784\u5bf9\u9f50, \u8bed\u4e49\u6bd4\u8f83, \u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2505.21938", "pdf": "https://arxiv.org/pdf/2505.21938", "abs": "https://arxiv.org/abs/2505.21938", "authors": ["Qirun Zeng", "Eric He", "Richard Hoffmann", "Xuchuang Wang", "Jinhang Zuo"], "title": "Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Adversarial attacks on stochastic bandits have traditionally relied on some\nunrealistic assumptions, such as per-round reward manipulation and unbounded\nperturbations, limiting their relevance to real-world systems. We propose a\nmore practical threat model, Fake Data Injection, which reflects realistic\nadversarial constraints: the attacker can inject only a limited number of\nbounded fake feedback samples into the learner's history, simulating legitimate\ninteractions. We design efficient attack strategies under this model,\nexplicitly addressing both magnitude constraints (on reward values) and\ntemporal constraints (on when and how often data can be injected). Our\ntheoretical analysis shows that these attacks can mislead both Upper Confidence\nBound (UCB) and Thompson Sampling algorithms into selecting a target arm in\nnearly all rounds while incurring only sublinear attack cost. Experiments on\nsynthetic and real-world datasets validate the effectiveness of our strategies,\nrevealing significant vulnerabilities in widely used stochastic bandit\nalgorithms under practical adversarial scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u771f\u5b9e\u7684\u5bf9\u6297\u653b\u51fb\u6a21\u578b\u2014\u2014\u865a\u5047\u6570\u636e\u6ce8\u5165\uff0c\u9488\u5bf9\u968f\u673a\u591a\u81c2\u8d4c\u535a\u673a\u7b97\u6cd5\uff0c\u901a\u8fc7\u6709\u9650\u6b21\u6570\u7684\u6709\u754c\u865a\u5047\u53cd\u9988\u6837\u672c\u6ce8\u5165\uff0c\u6709\u6548\u8bef\u5bfcUCB\u548cThompson Sampling\u7b97\u6cd5\uff0c\u540c\u65f6\u653b\u51fb\u6210\u672c\u4ec5\u4e3a\u6b21\u7ebf\u6027\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u6a21\u578b\u4f9d\u8d56\u4e0d\u73b0\u5b9e\u7684\u5047\u8bbe\uff08\u5982\u6bcf\u8f6e\u5956\u52b1\u64cd\u7eb5\u548c\u65e0\u754c\u6270\u52a8\uff09\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u7684\u9002\u7528\u6027\u3002\u4e3a\u6b64\uff0c\u63d0\u51fa\u66f4\u8d34\u8fd1\u73b0\u5b9e\u7684\u5a01\u80c1\u6a21\u578b\uff0c\u8003\u8651\u653b\u51fb\u8005\u7684\u5b9e\u9645\u7ea6\u675f\uff08\u5982\u6ce8\u5165\u6570\u636e\u7684\u6570\u91cf\u3001\u5e45\u5ea6\u548c\u65f6\u95f4\u9650\u5236\uff09\u3002", "method": "\u8bbe\u8ba1\u4e86\u865a\u5047\u6570\u636e\u6ce8\u5165\u653b\u51fb\u7b56\u7565\uff0c\u660e\u786e\u8003\u8651\u5956\u52b1\u5e45\u5ea6\u7684\u7ea6\u675f\u548c\u6570\u636e\u6ce8\u5165\u7684\u65f6\u95f4\u3001\u9891\u7387\u9650\u5236\uff0c\u5e76\u5728\u7406\u8bba\u4e0a\u5206\u6790\u4e86\u653b\u51fb\u6548\u679c\u548c\u6210\u672c\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u653b\u51fb\u80fd\u591f\u4f7fUCB\u548cThompson Sampling\u7b97\u6cd5\u5728\u51e0\u4e4e\u6240\u6709\u8f6e\u6b21\u4e2d\u9009\u62e9\u76ee\u6807\u81c2\uff0c\u4e14\u653b\u51fb\u6210\u672c\u4e3a\u6b21\u7ebf\u6027\u3002\u5b9e\u9a8c\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u968f\u673a\u591a\u81c2\u8d4c\u535a\u673a\u7b97\u6cd5\u5728\u5b9e\u9645\u5bf9\u6297\u573a\u666f\u4e0b\u5b58\u5728\u663e\u8457\u8106\u5f31\u6027\uff0c\u865a\u5047\u6570\u636e\u6ce8\u5165\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u653b\u51fb\u65b9\u5f0f\u3002", "keywords": "\u5bf9\u6297\u653b\u51fb, \u968f\u673a\u591a\u81c2\u8d4c\u535a\u673a, \u865a\u5047\u6570\u636e\u6ce8\u5165, UCB, Thompson Sampling"}}
{"id": "2505.21568", "pdf": "https://arxiv.org/pdf/2505.21568", "abs": "https://arxiv.org/abs/2505.21568", "authors": ["Haiyun Li", "Zhiyong Wu", "Xiaofeng Xie", "Jingran Xie", "Yaoxun Xu", "Hanyang Peng"], "title": "VoiceMark: Zero-Shot Voice Cloning-Resistant Watermarking Approach Leveraging Speaker-Specific Latents", "categories": ["cs.SD", "cs.AI", "cs.CR", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Voice cloning (VC)-resistant watermarking is an emerging technique for\ntracing and preventing unauthorized cloning. Existing methods effectively trace\ntraditional VC models by training them on watermarked audio but fail in\nzero-shot VC scenarios, where models synthesize audio from an audio prompt\nwithout training. To address this, we propose VoiceMark, the first zero-shot\nVC-resistant watermarking method that leverages speaker-specific latents as the\nwatermark carrier, allowing the watermark to transfer through the zero-shot VC\nprocess into the synthesized audio. Additionally, we introduce VC-simulated\naugmentations and VAD-based loss to enhance robustness against distortions.\nExperiments on multiple zero-shot VC models demonstrate that VoiceMark achieves\nover 95% accuracy in watermark detection after zero-shot VC synthesis,\nsignificantly outperforming existing methods, which only reach around 50%. See\nour code and demos at: https://huggingface.co/spaces/haiyunli/VoiceMark", "AI": {"tldr": "VoiceMark\u662f\u4e00\u4e2a\u9488\u5bf9\u96f6\u6837\u672c\u8bed\u97f3\u514b\u9686\uff08VC\uff09\u7684\u6c34\u5370\u65b9\u6cd5\uff0c\u9996\u6b21\u5229\u7528\u8bf4\u8bdd\u4eba\u7279\u5b9a\u7684\u6f5c\u5728\u7279\u5f81\u4f5c\u4e3a\u6c34\u5370\u8f7d\u4f53\uff0c\u901a\u8fc7\u96f6\u6837\u672cVC\u8fc7\u7a0b\u4f20\u9012\u6c34\u5370\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u8d85\u8fc795%\u3002", "motivation": "\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u867d\u80fd\u8ffd\u8e2a\u4f20\u7edfVC\u6a21\u578b\uff0c\u4f46\u5728\u96f6\u6837\u672cVC\u573a\u666f\u4e0b\u5931\u6548\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u5728\u672a\u8bad\u7ec3\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u6709\u6548\u4f20\u9012\u6c34\u5370\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faVoiceMark\uff0c\u5229\u7528\u8bf4\u8bdd\u4eba\u7279\u5b9a\u6f5c\u5728\u7279\u5f81\u4f5c\u4e3a\u6c34\u5370\u8f7d\u4f53\uff0c\u7ed3\u5408VC\u6a21\u62df\u589e\u5f3a\u548cVAD\u635f\u5931\u51fd\u6570\uff0c\u63d0\u5347\u6c34\u5370\u5bf9\u626d\u66f2\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVoiceMark\u5728\u96f6\u6837\u672cVC\u5408\u6210\u540e\u6c34\u5370\u68c0\u6d4b\u51c6\u786e\u7387\u8d8595%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u7ea650%\uff09\u3002", "conclusion": "VoiceMark\u4e3a\u96f6\u6837\u672cVC\u573a\u666f\u63d0\u4f9b\u9ad8\u6548\u6c34\u5370\u65b9\u6848\uff0c\u5f3a\u5316\u4e86\u5bf9\u672a\u6388\u6743\u514b\u9686\u7684\u8ffd\u8e2a\u80fd\u529b\u3002", "keywords": "\u8bed\u97f3\u514b\u9686\u3001\u6c34\u5370\u3001\u96f6\u6837\u672c\u3001\u8bf4\u8bdd\u4eba\u7279\u5f81\u3001\u9c81\u68d2\u6027"}}
{"id": "2505.22179", "pdf": "https://arxiv.org/pdf/2505.22179", "abs": "https://arxiv.org/abs/2505.22179", "authors": ["Yudi Zhang", "Weilin Zhao", "Xu Han", "Tiejun Zhao", "Wang Xu", "Hailong Cao", "Conghui Zhu"], "title": "Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages, 5 figures", "summary": "Speculative decoding and quantization effectively accelerate memory-bound\ninference of large language models. Speculative decoding mitigates the memory\nbandwidth bottleneck by verifying multiple tokens within a single forward pass,\nwhich increases computational effort. Quantization achieves this optimization\nby compressing weights and activations into lower bit-widths and also reduces\ncomputations via low-bit matrix multiplications. To further leverage their\nstrengths, we investigate the integration of these two techniques.\nSurprisingly, experiments applying the advanced speculative decoding method\nEAGLE-2 to various quantized models reveal that the memory benefits from 4-bit\nweight quantization are diminished by the computational load from speculative\ndecoding. Specifically, verifying a tree-style draft incurs significantly more\ntime overhead than a single-token forward pass on 4-bit weight quantized\nmodels. This finding led to our new speculative decoding design: a hierarchical\nframework that employs a small model as an intermediate stage to turn\ntree-style drafts into sequence drafts, leveraging the memory access benefits\nof the target quantized model. Experimental results show that our hierarchical\napproach achieves a 2.78$\\times$ speedup across various tasks for the 4-bit\nweight Llama-3-70B model on an A100 GPU, outperforming EAGLE-2 by 1.31$\\times$.\nCode available at https://github.com/AI9Stars/SpecMQuant.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u7ed3\u5408\u63a8\u6d4b\u89e3\u7801\u548c\u91cf\u5316\u6280\u672f\u6765\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5b58\u53d7\u9650\u63a8\u7406\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5c42\u63a8\u6d4b\u89e3\u7801\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u901a\u8fc7\u7ed3\u5408\u63a8\u6d4b\u89e3\u7801\u548c\u91cf\u5316\u6280\u672f\uff0c\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5b58\u5e26\u5bbd\u74f6\u9888\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u4ee5\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\uff0c\u5148\u4f7f\u7528\u5c0f\u578b\u6a21\u578b\u5c06\u6811\u72b6\u8349\u6848\u8f6c\u6362\u4e3a\u5e8f\u5217\u8349\u6848\uff0c\u518d\u7ed3\u5408\u76ee\u6807\u91cf\u5316\u6a21\u578b\u7684\u5185\u5b58\u8bbf\u95ee\u4f18\u52bf\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u57284\u4f4d\u91cf\u5316Llama-3-70B\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e862.78\u500d\u7684\u52a0\u901f\uff0c\u4f18\u4e8eEAGLE-2\u76841.31\u500d\u3002", "conclusion": "\u5206\u5c42\u63a8\u6d4b\u89e3\u7801\u8bbe\u8ba1\u6709\u6548\u7ed3\u5408\u4e86\u91cf\u5316\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\u3002", "keywords": "\u63a8\u6d4b\u89e3\u7801, \u91cf\u5316, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u5185\u5b58\u4f18\u5316, \u63a8\u7406\u52a0\u901f"}}
{"id": "2505.21942", "pdf": "https://arxiv.org/pdf/2505.21942", "abs": "https://arxiv.org/abs/2505.21942", "authors": ["Prashant Bhat", "Laurens Niesten", "Elahe Arani", "Bahram Zonooz"], "title": "Continual Learning Beyond Experience Rehearsal and Full Model Surrogates", "categories": ["cs.LG", "stat.ML"], "comment": "23 pages, 9 figures", "summary": "Continual learning (CL) has remained a significant challenge for deep neural\nnetworks as learning new tasks erases previously acquired knowledge, either\npartially or completely. Existing solutions often rely on experience rehearsal\nor full model surrogates to mitigate CF. While effective, these approaches\nintroduce substantial memory and computational overhead, limiting their\nscalability and applicability in real-world scenarios. To address this, we\npropose SPARC, a scalable CL approach that eliminates the need for experience\nrehearsal and full-model surrogates. By effectively combining task-specific\nworking memories and task-agnostic semantic memory for cross-task knowledge\nconsolidation, SPARC results in a remarkable parameter efficiency, using only\n6% of the parameters required by full-model surrogates. Despite its lightweight\ndesign, SPARC achieves superior performance on Seq-TinyImageNet and matches\nrehearsal-based methods on various CL benchmarks. Additionally, weight\nre-normalization in the classification layer mitigates task-specific biases,\nestablishing SPARC as a practical and scalable solution for CL under stringent\nefficiency constraints.", "AI": {"tldr": "SPARC\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e0\u9700\u7ecf\u9a8c\u56de\u653e\u6216\u5b8c\u6574\u6a21\u578b\u66ff\u4ee3\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u5de5\u4f5c\u8bb0\u5fc6\u548c\u4efb\u52a1\u65e0\u5173\u8bed\u4e49\u8bb0\u5fc6\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u53c2\u6570\u6548\u7387\uff0c\u6027\u80fd\u4f18\u4e8e\u6216\u5339\u914d\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u4e2d\uff0c\u65b0\u4efb\u52a1\u5b66\u4e60\u4f1a\u90e8\u5206\u6216\u5b8c\u5168\u8986\u76d6\u65e7\u77e5\u8bc6\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5e38\u4f9d\u8d56\u7ecf\u9a8c\u56de\u653e\u6216\u5b8c\u6574\u6a21\u578b\u66ff\u4ee3\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faSPARC\u65b9\u6cd5\uff0c\u7ed3\u5408\u4efb\u52a1\u7279\u5b9a\u5de5\u4f5c\u8bb0\u5fc6\u548c\u4efb\u52a1\u65e0\u5173\u8bed\u4e49\u8bb0\u5fc6\u8fdb\u884c\u8de8\u4efb\u52a1\u77e5\u8bc6\u6574\u5408\uff0c\u5206\u7c7b\u5c42\u6743\u91cd\u91cd\u65b0\u5f52\u4e00\u5316\u4ee5\u7f13\u89e3\u4efb\u52a1\u7279\u5b9a\u504f\u5dee\u3002", "result": "SPARC\u4ec5\u9700\u5b8c\u6574\u6a21\u578b\u66ff\u4ee36%\u7684\u53c2\u6570\uff0c\u5728Seq-TinyImageNet\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e0e\u5176\u4ed6\u57fa\u51c6\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\uff0c\u4e14\u53c2\u6570\u6548\u7387\u663e\u8457\u3002", "conclusion": "SPARC\u662f\u4e00\u79cd\u5728\u4e25\u683c\u6548\u7387\u7ea6\u675f\u4e0b\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u6301\u7eed\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u6301\u7eed\u5b66\u4e60, \u53c2\u6570\u6548\u7387, \u8bed\u4e49\u8bb0\u5fc6, \u5de5\u4f5c\u8bb0\u5fc6, \u6743\u91cd\u5f52\u4e00\u5316"}}
{"id": "2505.22184", "pdf": "https://arxiv.org/pdf/2505.22184", "abs": "https://arxiv.org/abs/2505.22184", "authors": ["Xuchen Ma", "Jianxiang Yu", "Wenming Shao", "Bo Pang", "Xiang Li"], "title": "Breaking the Cloak! Unveiling Chinese Cloaked Toxicity with Homophone Graph and Toxic Lexicon", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 5 figures, 9 tables", "summary": "Social media platforms have experienced a significant rise in toxic content,\nincluding abusive language and discriminatory remarks, presenting growing\nchallenges for content moderation. Some users evade censorship by deliberately\ndisguising toxic words through homophonic cloak, which necessitates the task of\nunveiling cloaked toxicity. Existing methods are mostly designed for English\ntexts, while Chinese cloaked toxicity unveiling has not been solved yet. To\ntackle the issue, we propose C$^2$TU, a novel training-free and prompt-free\nmethod for Chinese cloaked toxic content unveiling. It first employs substring\nmatching to identify candidate toxic words based on Chinese homo-graph and\ntoxic lexicon. Then it filters those candidates that are non-toxic and corrects\ncloaks to be their corresponding toxicities. Specifically, we develop two model\nvariants for filtering, which are based on BERT and LLMs, respectively. For\nLLMs, we address the auto-regressive limitation in computing word occurrence\nprobability and utilize the full semantic contexts of a text sequence to reveal\ncloaked toxic words. Extensive experiments demonstrate that C$^2$TU can achieve\nsuperior performance on two Chinese toxic datasets. In particular, our method\noutperforms the best competitor by up to 71% on the F1 score and 35% on\naccuracy, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u548c\u63d0\u793a\u7684\u65b9\u6cd5C$^2$TU\uff0c\u7528\u4e8e\u68c0\u6d4b\u4e2d\u6587\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u4f2a\u88c5\u6bd2\u6027\u5185\u5bb9\uff0c\u901a\u8fc7\u5b50\u4e32\u5339\u914d\u548c\u8bed\u4e49\u8fc7\u6ee4\u6709\u6548\u8bc6\u522b\u5e76\u7ea0\u6b63\u6bd2\u6027\u8bcd\u6c47\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u793e\u4ea4\u5e73\u53f0\u4e0a\u4f2a\u88c5\u6bd2\u6027\u5185\u5bb9\uff08\u5982\u540c\u97f3\u4f2a\u88c5\uff09\u7684\u589e\u591a\u7ed9\u5185\u5bb9\u5ba1\u6838\u5e26\u6765\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u82f1\u6587\uff0c\u4e2d\u6587\u9886\u57df\u5c1a\u672a\u89e3\u51b3\u3002", "method": "C$^2$TU\u901a\u8fc7\u5b50\u4e32\u5339\u914d\u4ece\u540c\u97f3\u8bcd\u548c\u6bd2\u6027\u8bcd\u5e93\u4e2d\u7b5b\u9009\u5019\u9009\u8bcd\uff0c\u5e76\u5229\u7528BERT\u548cLLMs\u53d8\u4f53\u8fdb\u884c\u975e\u6bd2\u6027\u8fc7\u6ee4\u53ca\u4f2a\u88c5\u7ea0\u6b63\uff0cLLMs\u7248\u672c\u8fd8\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u9650\u5236\u3002", "result": "\u5728\u4e24\u4e2a\u4e2d\u6587\u6bd2\u6027\u6570\u636e\u96c6\u4e0a\uff0cC$^2$TU\u7684F1\u5206\u6570\u548c\u51c6\u786e\u7387\u5206\u522b\u6700\u9ad8\u63d0\u534771%\u548c35%\uff0c\u663e\u8457\u4f18\u4e8e\u7ade\u4e89\u5bf9\u624b\u3002", "conclusion": "C$^2$TU\u4e3a\u4e2d\u6587\u4f2a\u88c5\u6bd2\u6027\u5185\u5bb9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6027\u80fd\u4f18\u8d8a\u3002", "keywords": "\u4e2d\u6587\u4f2a\u88c5\u6bd2\u6027\u3001\u5185\u5bb9\u5ba1\u6838\u3001\u540c\u97f3\u8bcd\u3001BERT\u3001LLMs"}}
{"id": "2505.21944", "pdf": "https://arxiv.org/pdf/2505.21944", "abs": "https://arxiv.org/abs/2505.21944", "authors": ["Linli Zhou", "Bokun Wang", "My T. Thai", "Tianbao Yang"], "title": "Stochastic Primal-Dual Double Block-Coordinate for Two-way Partial AUC Maximization", "categories": ["cs.LG"], "comment": null, "summary": "Two-way partial AUC (TPAUC) is a critical performance metric for binary\nclassification with imbalanced data, as it focuses on specific ranges of the\ntrue positive rate (TPR) and false positive rate (FPR). However, stochastic\nalgorithms for TPAUC optimization remain under-explored, with existing methods\neither limited to approximated TPAUC loss functions or burdened by sub-optimal\ncomplexities. To overcome these limitations, we introduce two innovative\nstochastic primal-dual double block-coordinate algorithms for TPAUC\nmaximization. These algorithms utilize stochastic block-coordinate updates for\nboth the primal and dual variables, catering to both convex and non-convex\nsettings. We provide theoretical convergence rate analyses, demonstrating\nsignificant improvements over prior approaches. Our experimental results, based\non multiple benchmark datasets, validate the superior performance of our\nalgorithms, showcasing faster convergence and better generalization. This work\nadvances the state of the art in TPAUC optimization and offers practical tools\nfor real-world machine learning applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u521b\u65b0\u7684\u968f\u673a\u539f\u59cb-\u5bf9\u5076\u53cc\u5757\u5750\u6807\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u5728\u4e8c\u5143\u5206\u7c7b\u4e0d\u5e73\u8861\u6570\u636e\u4e2d\u5173\u952e\u7684TPAUC\u6307\u6807\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u7531\u4e8eTPAUC\u5728\u4e8c\u5143\u5206\u7c7b\u4e0d\u5e73\u8861\u6570\u636e\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u800c\u73b0\u6709\u7684\u968f\u673a\u7b97\u6cd5\u5b58\u5728\u8fd1\u4f3c\u635f\u5931\u51fd\u6570\u6216\u590d\u6742\u5ea6\u6b21\u4f18\u7684\u95ee\u9898\uff0c\u8bba\u6587\u65e8\u5728\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u4e24\u79cd\u968f\u673a\u539f\u59cb-\u5bf9\u5076\u53cc\u5757\u5750\u6807\u7b97\u6cd5\uff0c\u91c7\u7528\u968f\u673a\u5757\u5750\u6807\u66f4\u65b0\u5904\u7406\u539f\u59cb\u548c\u5bf9\u5076\u53d8\u91cf\uff0c\u9002\u7528\u4e8e\u51f8\u548c\u975e\u51f8\u573a\u666f\u3002", "result": "\u7406\u8bba\u5206\u6790\u663e\u793a\u6536\u655b\u901f\u5ea6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8868\u660e\u65b0\u7b97\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u6536\u655b\u66f4\u5feb\u4e14\u6cdb\u5316\u66f4\u597d\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86TPAUC\u4f18\u5316\u7684\u6280\u672f\u6c34\u5e73\uff0c\u4e3a\u5b9e\u9645\u673a\u5668\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "keywords": "TPAUC, \u4e8c\u5143\u5206\u7c7b, \u968f\u673a\u4f18\u5316, \u539f\u59cb-\u5bf9\u5076\u7b97\u6cd5, \u5757\u5750\u6807\u66f4\u65b0"}}
{"id": "2505.21570", "pdf": "https://arxiv.org/pdf/2505.21570", "abs": "https://arxiv.org/abs/2505.21570", "authors": ["Dalit Ken-Dror Feldman", "Daniel Benoliel"], "title": "Beyond Explainability: The Case for AI Validation", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Artificial Knowledge (AK) systems are transforming decision-making across\ncritical domains such as healthcare, finance, and criminal justice. However,\ntheir growing opacity presents governance challenges that current regulatory\napproaches, focused predominantly on explainability, fail to address\nadequately. This article argues for a shift toward validation as a central\nregulatory pillar. Validation, ensuring the reliability, consistency, and\nrobustness of AI outputs, offers a more practical, scalable, and risk-sensitive\nalternative to explainability, particularly in high-stakes contexts where\ninterpretability may be technically or economically unfeasible. We introduce a\ntypology based on two axes, validity and explainability, classifying AK systems\ninto four categories and exposing the trade-offs between interpretability and\noutput reliability. Drawing on comparative analysis of regulatory approaches in\nthe EU, US, UK, and China, we show how validation can enhance societal trust,\nfairness, and safety even where explainability is limited. We propose a\nforward-looking policy framework centered on pre- and post-deployment\nvalidation, third-party auditing, harmonized standards, and liability\nincentives. This framework balances innovation with accountability and provides\na governance roadmap for responsibly integrating opaque, high-performing AK\nsystems into society.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u9a8c\u8bc1\u5e94\u6210\u4e3aAI\u76d1\u7ba1\u7684\u6838\u5fc3\u652f\u67f1\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3AI\u7cfb\u7edf\u5728\u4e0d\u900f\u660e\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002\u4f5c\u8005\u63d0\u51fa\u57fa\u4e8e\u6709\u6548\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u9762\u5411\u90e8\u7f72\u524d\u540e\u7684\u9a8c\u8bc1\u6846\u67b6\u3002", "motivation": "\u5f53\u524dAI\u76d1\u7ba1\u8fc7\u5ea6\u4f9d\u8d56\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u53f8\u6cd5\uff09\u4e2d\uff0c\u53ef\u89e3\u91ca\u6027\u53ef\u80fd\u4e0d\u5207\u5b9e\u9645\uff0c\u9a8c\u8bc1\uff08\u786e\u4fddAI\u8f93\u51fa\u7684\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\uff09\u66f4\u5177\u5b9e\u7528\u6027\u548c\u6269\u5c55\u6027\u3002", "method": "\u901a\u8fc7\u6709\u6548\u6027-\u53ef\u89e3\u91ca\u6027\u5206\u7c7b\u6cd5\u5206\u6790AI\u7cfb\u7edf\uff0c\u5e76\u6bd4\u8f83\u6b27\u76df\u3001\u7f8e\u56fd\u3001\u82f1\u56fd\u548c\u4e2d\u56fd\u7684\u76d1\u7ba1\u65b9\u6cd5\uff0c\u63d0\u51fa\u57fa\u4e8e\u9a8c\u8bc1\u7684\u6cbb\u7406\u6846\u67b6\uff0c\u5305\u62ec\u90e8\u7f72\u524d\u540e\u9a8c\u8bc1\u3001\u7b2c\u4e09\u65b9\u5ba1\u8ba1\u548c\u6807\u51c6\u5316\u3002", "result": "\u9a8c\u8bc1\u6846\u67b6\u80fd\u5728\u53ef\u89e3\u91ca\u6027\u53d7\u9650\u65f6\u4ecd\u589e\u5f3a\u793e\u4f1a\u4fe1\u4efb\u3001\u516c\u5e73\u6027\u548c\u5b89\u5168\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u8d23\u4efb\u6fc0\u52b1\u673a\u5236\u5e73\u8861\u521b\u65b0\u4e0e\u95ee\u8d23\u3002", "conclusion": "\u9a8c\u8bc1\u4f5c\u4e3a\u4e00\u79cd\u66f4\u53ef\u884c\u7684\u76d1\u7ba1\u624b\u6bb5\uff0c\u4e3a\u9ad8\u6027\u80fd\u4f46\u4e0d\u900f\u660e\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6cbb\u7406\u8def\u7ebf\u56fe\uff0c\u517c\u987e\u521b\u65b0\u4e0e\u793e\u4f1a\u8d23\u4efb\u3002", "keywords": "AI\u6cbb\u7406\u3001\u9a8c\u8bc1\u3001\u53ef\u89e3\u91ca\u6027\u3001\u76d1\u7ba1\u6846\u67b6\u3001\u9ad8\u98ce\u9669AI"}}
{"id": "2505.22202", "pdf": "https://arxiv.org/pdf/2505.22202", "abs": "https://arxiv.org/abs/2505.22202", "authors": ["Hyeonbin Hwang", "Byeongguk Jeon", "Seungone Kim", "Jiyeon Kim", "Hoyeon Chang", "Sohee Yang", "Seungpil Won", "Dohaeng Lee", "Youbin Ahn", "Minjoon Seo"], "title": "Let's Predict Sentence by Sentence", "categories": ["cs.CL", "cs.AI"], "comment": "Work In Progress", "summary": "Autoregressive language models (LMs) generate one token at a time, yet human\nreasoning operates over higher-level abstractions - sentences, propositions,\nand concepts. This contrast raises a central question- Can LMs likewise learn\nto reason over structured semantic units rather than raw token sequences? In\nthis work, we investigate whether pretrained LMs can be lifted into such\nabstract reasoning spaces by building on their learned representations. We\npresent a framework that adapts a pretrained token-level LM to operate in\nsentence space by autoregressively predicting continuous embeddings of next\nsentences. We explore two embedding paradigms inspired by classical\nrepresentation learning: 1) semantic embeddings, learned via autoencoding to\npreserve surface meaning; and 2) contextual embeddings, trained via\nnext-sentence prediction to encode anticipatory structure. We evaluate both\nunder two inference regimes: Discretized, which decodes each predicted\nembedding into text before re-encoding; and Continuous, which reasons entirely\nin embedding space for improved efficiency. Across four domains - mathematics,\nlogic, commonsense, and planning - contextual embeddings under continuous\ninference show competitive performance with Chain-of-Thought (CoT) while\nreducing inference-time FLOPs on average by half. We also present early signs\nof scalability and modular adaptation. Finally, to visualize latent\ntrajectories, we introduce SentenceLens, a diagnostic tool that decodes\nintermediate model states into interpretable sentences. Together, our results\nindicate that pretrained LMs can effectively transition to abstract, structured\nreasoning within latent embedding spaces.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u662f\u5426\u80fd\u901a\u8fc7\u8fde\u7eed\u5d4c\u5165\u7a7a\u95f4\u8fdb\u884c\u62bd\u8c61\u63a8\u7406\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\u5c06\u6a21\u578b\u4ece\u8bcd\u7ea7\u522b\u63d0\u5347\u5230\u53e5\u5b50\u7ea7\u522b\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u4eba\u7c7b\u63a8\u7406\u57fa\u4e8e\u9ad8\u7ea7\u8bed\u4e49\u5355\u5143\uff08\u5982\u53e5\u5b50\u6216\u6982\u5ff5\uff09\uff0c\u800c\u4f20\u7edfLMs\u4ee5\u8bcd\u4e3a\u5355\u4f4d\u751f\u6210\uff0c\u80fd\u5426\u8ba9LMs\u5728\u62bd\u8c61\u8bed\u4e49\u5355\u5143\u4e0a\u63a8\u7406\u662f\u4e00\u4e2a\u6838\u5fc3\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u9884\u6d4b\u8fde\u7eed\u53e5\u5b50\u5d4c\u5165\uff0c\u63a2\u7d22\u4e86\u4e24\u79cd\u5d4c\u5165\u8303\u5f0f\uff08\u8bed\u4e49\u5d4c\u5165\u548c\u4e0a\u4e0b\u6587\u5d4c\u5165\uff09\uff0c\u5e76\u6bd4\u8f83\u4e86\u79bb\u6563\u5316\u548c\u8fde\u7eed\u63a8\u7406\u4e24\u79cd\u63a8\u7406\u65b9\u5f0f\u3002", "result": "\u5728\u6570\u5b66\u3001\u903b\u8f91\u3001\u5e38\u8bc6\u548c\u89c4\u5212\u7b49\u9886\u57df\uff0c\u4e0a\u4e0b\u6587\u5d4c\u5165\u7ed3\u5408\u8fde\u7eed\u63a8\u7406\u7684\u6027\u80fd\u4e0e\u601d\u7ef4\u94fe\uff08CoT\uff09\u76f8\u5f53\uff0c\u540c\u65f6\u63a8\u7406\u8ba1\u7b97\u91cf\u5e73\u5747\u51cf\u5c11\u4e00\u534a\u3002", "conclusion": "\u9884\u8bad\u7ec3LMs\u53ef\u4ee5\u5728\u6f5c\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u6709\u6548\u8fdb\u884c\u7ed3\u6784\u5316\u62bd\u8c61\u63a8\u7406\uff0c\u5177\u5907\u53ef\u6269\u5c55\u6027\u548c\u6a21\u5757\u5316\u9002\u5e94\u6027\u3002", "keywords": "\u8bed\u8a00\u6a21\u578b\u3001\u62bd\u8c61\u63a8\u7406\u3001\u53e5\u5b50\u5d4c\u5165\u3001\u4e0a\u4e0b\u6587\u5d4c\u5165\u3001\u8ba1\u7b97\u6548\u7387"}}
{"id": "2505.21959", "pdf": "https://arxiv.org/pdf/2505.21959", "abs": "https://arxiv.org/abs/2505.21959", "authors": ["Aakriti Agrawal", "Mucong Ding", "Zora Che", "Chenghao Deng", "Anirudh Satheesh", "Bang An", "Bayan Bruss", "John Langford", "Furong Huang"], "title": "EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles", "categories": ["cs.LG", "cs.CL"], "comment": "Superalignment. arXiv admin note: substantial text overlap with\n  arXiv:2410.04571", "summary": "With Large Language Models (LLMs) rapidly approaching and potentially\nsurpassing human-level performance, it has become imperative to develop\napproaches capable of effectively supervising and enhancing these powerful\nmodels using smaller, human-level models exposed to only human-level data. We\naddress this critical weak-to-strong (W2S) generalization challenge by\nproposing a novel method aimed at improving weak experts, by training on the\nsame limited human-level data, enabling them to generalize to complex,\nsuper-human-level tasks. Our approach, called \\textbf{EnsemW2S}, employs a\ntoken-level ensemble strategy that iteratively combines multiple weak experts,\nsystematically addressing the shortcomings identified in preceding iterations.\nBy continuously refining these weak models, we significantly enhance their\ncollective ability to supervise stronger student models. We extensively\nevaluate the generalization performance of both the ensemble of weak experts\nand the subsequent strong student model across in-distribution (ID) and\nout-of-distribution (OOD) datasets. For OOD, we specifically introduce question\ndifficulty as an additional dimension for defining distributional shifts. Our\nempirical results demonstrate notable improvements, achieving 4\\%, and 3.2\\%\nimprovements on ID datasets and, upto 6\\% and 2.28\\% on OOD datasets for\nexperts and student models respectively, underscoring the effectiveness of our\nproposed method in advancing W2S generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEnsemW2S\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u591a\u4e2a\u5f31\u4e13\u5bb6\u6a21\u578b\uff0c\u63d0\u5347\u5176\u5bf9\u8d85\u4eba\u7c7b\u7ea7\u522b\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4f5c\u8005\u5728ID\u548cOOD\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u7ed3\u679c\u663e\u793a\u4e13\u5bb6\u6a21\u578b\u548c\u5b66\u751f\u6a21\u578b\u5728ID\u548cOOD\u6570\u636e\u96c6\u4e0a\u5747\u6709\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u5176\u8bad\u7ec3\u548c\u76d1\u7763\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5982\u4f55\u5229\u7528\u4eba\u7c7b\u7ea7\u522b\u6570\u636e\u7684\u5f31\u4e13\u5bb6\u6a21\u578b\u6765\u63d0\u5347\u5bf9\u66f4\u590d\u6742\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528token\u7ea7\u96c6\u6210\u7b56\u7565\uff0c\u901a\u8fc7\u8fed\u4ee3\u7ec4\u5408\u591a\u4e2a\u5f31\u4e13\u5bb6\u6a21\u578b\uff0c\u9010\u6b65\u4f18\u5316\u6a21\u578b\u7684\u76d1\u7763\u80fd\u529b\uff0c\u6700\u7ec8\u63d0\u5347\u5bf9\u5b66\u751f\u6a21\u578b\u7684\u76d1\u7763\u6548\u679c\u3002", "result": "\u5728ID\u6570\u636e\u96c6\u4e0a\uff0c\u4e13\u5bb6\u6a21\u578b\u548c\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\u5206\u522b\u63d0\u5347\u4e864%\u548c3.2%\uff1b\u5728OOD\u6570\u636e\u96c6\u4e0a\uff0c\u63d0\u5347\u9ad8\u8fbe6%\u548c2.28%\u3002", "conclusion": "EnsemW2S\u80fd\u6709\u6548\u63d0\u5347\u5f31\u4e13\u5bb6\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u5f31\u5230\u5f3a\uff08W2S\uff09\u6cdb\u5316\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "Large Language Models, weak-to-strong generalization, token-level ensemble, supervision, human-level data"}}
{"id": "2505.22232", "pdf": "https://arxiv.org/pdf/2505.22232", "abs": "https://arxiv.org/abs/2505.22232", "authors": ["Mehdi Ali", "Manuel Brack", "Max L\u00fcbbering", "Elias Wendt", "Abbas Goher Khan", "Richard Rutmann", "Alex Jude", "Maurice Kraus", "Alexander Arno Weber", "Felix Stollenwerk", "David Kacz\u00e9r", "Florian Mai", "Lucie Flek", "Rafet Sifa", "Nicolas Flores-Herr", "Joachim K\u00f6hler", "Patrick Schramowski", "Michael Fromm", "Kristian Kersting"], "title": "Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project page available at https://huggingface.co/spaces/Jackal-AI/JQL", "summary": "High-quality multilingual training data is essential for effectively\npretraining large language models (LLMs). Yet, the availability of suitable\nopen-source multilingual datasets remains limited. Existing state-of-the-art\ndatasets mostly rely on heuristic filtering methods, restricting both their\ncross-lingual transferability and scalability. Here, we introduce JQL, a\nsystematic approach that efficiently curates diverse and high-quality\nmultilingual data at scale while significantly reducing computational demands.\nJQL distills LLMs' annotation capabilities into lightweight annotators based on\npretrained multilingual embeddings. These models exhibit robust multilingual\nand cross-lingual performance, even for languages and scripts unseen during\ntraining. Evaluated empirically across 35 languages, the resulting annotation\npipeline substantially outperforms current heuristic filtering methods like\nFineweb2. JQL notably enhances downstream model training quality and increases\ndata retention rates. Our research provides practical insights and valuable\nresources for multilingual data curation, raising the standards of multilingual\ndataset development.", "AI": {"tldr": "JQL\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6807\u6ce8\u5668\u9ad8\u6548\u7b5b\u9009\u9ad8\u8d28\u91cf\u591a\u8bed\u8a00\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u8de8\u8bed\u8a00\u8fc1\u79fb\u80fd\u529b\u4e0e\u6570\u636e\u4fdd\u7559\u7387\u3002", "motivation": "\u5f53\u524d\u5f00\u6e90\u591a\u8bed\u8a00\u6570\u636e\u96c6\u7a00\u7f3a\u4e14\u4f9d\u8d56\u542f\u53d1\u5f0f\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u9650\u5236\u4e86\u8de8\u8bed\u8a00\u8fc1\u79fb\u548c\u6269\u5c55\u6027\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6570\u636e\u7b5b\u9009\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u591a\u8bed\u8a00\u5d4c\u5165\u7684\u8f7b\u91cf\u7ea7\u6807\u6ce8\u5668\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6807\u6ce8\u80fd\u529b\u84b8\u998f\u5230\u9ad8\u6548\u7684\u6570\u636e\u7b5b\u9009\u6d41\u7a0b\u4e2d\u3002", "result": "\u572835\u79cd\u8bed\u8a00\u4e0a\u9a8c\u8bc1\uff0cJQL\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\uff08\u5982Fineweb2\uff09\uff0c\u63d0\u5347\u4e86\u4e0b\u6e38\u6a21\u578b\u8bad\u7ec3\u8d28\u91cf\u548c\u6570\u636e\u4fdd\u7559\u7387\u3002", "conclusion": "JQL\u4e3a\u591a\u8bed\u8a00\u6570\u636e\u7b5b\u9009\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u548c\u8d44\u6e90\uff0c\u63d0\u5347\u4e86\u591a\u8bed\u8a00\u6570\u636e\u96c6\u5f00\u53d1\u7684\u6807\u51c6\u3002", "keywords": "\u591a\u8bed\u8a00\u6570\u636e, \u6570\u636e\u7b5b\u9009, \u8f7b\u91cf\u7ea7\u6807\u6ce8\u5668, \u8de8\u8bed\u8a00\u8fc1\u79fb, JQL"}}
{"id": "2505.21972", "pdf": "https://arxiv.org/pdf/2505.21972", "abs": "https://arxiv.org/abs/2505.21972", "authors": ["Patrick Vossler", "Fan Xia", "Yifan Mai", "Jean Feng"], "title": "Judging LLMs on a Simplex", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "28 pages, 7 figures", "summary": "Automated evaluation of free-form outputs from large language models (LLMs)\nis challenging because many distinct answers can be equally valid. A common\npractice is to use LLMs themselves as judges, but the theoretical properties of\nthis approach are not yet well understood. We show that a geometric framework\nthat represents both judges and candidates as points on a probability simplex\ncan provide helpful insight on what is or is not identifiable using LLM judges.\nOur theoretical analysis uncovers a \"phase transition\" in ranking\nidentifiability: for binary scoring systems, true rankings are identifiable\neven with weak judges under mild assumptions, while rankings become\nnon-identifiable for three or more scoring levels even with infinite data,\nabsent additional prior knowledge. This non-identifiability highlights how\nuncertainty in rankings stems from not only aleatoric uncertainty (i.e.,\ninherent stochasticity in the data) but also epistemic uncertainty regarding\nwhich assumptions hold, an aspect that has received limited attention until\nnow. To integrate both types of uncertainty, we use Bayesian inference to\nencode assumptions as priors and conduct sensitivity analysis of ranking\nestimates and credible intervals. Empirical evaluations across multiple\nbenchmarks demonstrate that Bayesian inference yields more accurate rankings\nand substantially improves coverage rates. These results underscore the\nimportance of taking a more holistic approach to uncertainty quantification\nwhen using LLMs as judges.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4f7f\u7528LLMs\u4f5c\u4e3a\u8bc4\u4f30\u81ea\u7531\u5f62\u5f0f\u8f93\u51fa\u65f6\u7684\u51e0\u4f55\u6846\u67b6\u548c\u8d1d\u53f6\u65af\u63a8\u7406\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u8bc4\u5206\u7cfb\u7edf\u5728\u53ef\u8bc6\u522b\u6027\u65b9\u9762\u7684\u76f8\u53d8\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u5168\u9762\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8eLLM\u751f\u6210\u7684\u81ea\u7531\u5f62\u5f0f\u8f93\u51fa\u5b58\u5728\u591a\u79cd\u6709\u6548\u7b54\u6848\uff0c\u81ea\u52a8\u8bc4\u4f30\u5177\u6709\u6311\u6218\u6027\u3002\u7814\u7a76\u65e8\u5728\u7406\u89e3LLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u7684\u7406\u8bba\u7279\u6027\uff0c\u5e76\u6539\u8fdb\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "method": "\u63d0\u51fa\u51e0\u4f55\u6846\u67b6\uff0c\u5c06\u8bc4\u4f30\u8005\u548c\u5019\u9009\u8005\u8868\u793a\u4e3a\u6982\u7387\u5355\u7eaf\u5f62\u4e0a\u7684\u70b9\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u63a8\u7406\u8fdb\u884c\u654f\u611f\u6027\u5206\u6790\u3002", "result": "\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86\u53ef\u8bc6\u522b\u6027\u7684\u76f8\u53d8\u73b0\u8c61\uff0c\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\u8d1d\u53f6\u65af\u63a8\u7406\u80fd\u63d0\u9ad8\u6392\u540d\u51c6\u786e\u6027\u548c\u8986\u76d6\u7387\u3002", "conclusion": "\u91c7\u7528\u8d1d\u53f6\u65af\u63a8\u7406\u80fd\u66f4\u5168\u9762\u5730\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u5347LLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u7684\u53ef\u9760\u6027\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u81ea\u7531\u5f62\u5f0f\u8f93\u51fa\uff0c\u8bc4\u4f30\uff0c\u8d1d\u53f6\u65af\u63a8\u7406\uff0c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316"}}
{"id": "2505.21572", "pdf": "https://arxiv.org/pdf/2505.21572", "abs": "https://arxiv.org/abs/2505.21572", "authors": ["Sungwon Kim", "Namkyeong Lee", "Yunyoung Doh", "Seungmin Shin", "Guimok Cho", "Seung-Won Jeon", "Sangkook Kim", "Chanyoung Park"], "title": "Thickness-aware E(3)-Equivariant 3D Mesh Neural Networks", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "Mesh-based 3D static analysis methods have recently emerged as efficient\nalternatives to traditional computational numerical solvers, significantly\nreducing computational costs and runtime for various physics-based analyses.\nHowever, these methods primarily focus on surface topology and geometry, often\noverlooking the inherent thickness of real-world 3D objects, which exhibits\nhigh correlations and similar behavior between opposing surfaces. This\nlimitation arises from the disconnected nature of these surfaces and the\nabsence of internal edge connections within the mesh. In this work, we propose\na novel framework, the Thickness-aware E(3)-Equivariant 3D Mesh Neural Network\n(T-EMNN), that effectively integrates the thickness of 3D objects while\nmaintaining the computational efficiency of surface meshes. Additionally, we\nintroduce data-driven coordinates that encode spatial information while\npreserving E(3)-equivariance or invariance properties, ensuring consistent and\nrobust analysis. Evaluations on a real-world industrial dataset demonstrate the\nsuperior performance of T-EMNN in accurately predicting node-level 3D\ndeformations, effectively capturing thickness effects while maintaining\ncomputational efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6846\u67b6T-EMNN\uff0c\u901a\u8fc7\u8003\u86513D\u7269\u4f53\u7684\u539a\u5ea6\u7279\u6027\uff0c\u6539\u8fdb\u4e86\u73b0\u6709\u7684\u57fa\u4e8e\u7f51\u683c\u7684\u9759\u6001\u5206\u6790\u65b9\u6cd5\uff0c\u4e14\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u4fdd\u6301\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u76843D\u7f51\u683c\u5206\u6790\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8868\u9762\u62d3\u6251\u548c\u51e0\u4f55\u5f62\u72b6\uff0c\u5ffd\u7565\u4e86\u7269\u4f53\u539a\u5ea6\u7684\u5185\u5728\u7279\u6027\u53ca\u5176\u5bf9\u8868\u9762\u884c\u4e3a\u7684\u663e\u8457\u5f71\u54cd\uff0c\u5bfc\u81f4\u5206\u6790\u7ed3\u679c\u4e0d\u591f\u51c6\u786e\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u539a\u5ea6\u611f\u77e5\u7684E(3)-\u7b49\u53d83D\u7f51\u683c\u795e\u7ecf\u7f51\u7edc\uff08T-EMNN\uff09\uff0c\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u7684\u5750\u6807\u7f16\u7801\u65b9\u6cd5\uff0c\u4fdd\u6301\u4e86E(3)-\u7b49\u53d8\u6216\u4e0d\u53d8\u6027\u8d28\u3002", "result": "\u5728\u5b9e\u9645\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cT-EMNN\u80fd\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u8282\u70b9\u7ea73D\u53d8\u5f62\uff0c\u6709\u6548\u6355\u6349\u539a\u5ea6\u6548\u5e94\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "T-EMNN\u6846\u67b6\u901a\u8fc7\u6574\u5408\u539a\u5ea6\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u9759\u6001\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "keywords": "3D\u7f51\u683c\u5206\u6790\uff0c\u539a\u5ea6\u611f\u77e5\uff0cE(3)-\u7b49\u53d8\uff0c\u795e\u7ecf\u7f51\u7edc\uff0c\u5de5\u4e1a\u5e94\u7528"}}
{"id": "2505.22236", "pdf": "https://arxiv.org/pdf/2505.22236", "abs": "https://arxiv.org/abs/2505.22236", "authors": ["Charlotte Pouw", "Afra Alishahi", "Willem Zuidema"], "title": "A Linguistically Motivated Analysis of Intonational Phrasing in Text-to-Speech Systems: Revealing Gaps in Syntactic Sensitivity", "categories": ["cs.CL"], "comment": "Accepted to CoNLL 2025", "summary": "We analyze the syntactic sensitivity of Text-to-Speech (TTS) systems using\nmethods inspired by psycholinguistic research. Specifically, we focus on the\ngeneration of intonational phrase boundaries, which can often be predicted by\nidentifying syntactic boundaries within a sentence. We find that TTS systems\nstruggle to accurately generate intonational phrase boundaries in sentences\nwhere syntactic boundaries are ambiguous (e.g., garden path sentences or\nsentences with attachment ambiguity). In these cases, systems need superficial\ncues such as commas to place boundaries at the correct positions. In contrast,\nfor sentences with simpler syntactic structures, we find that systems do\nincorporate syntactic cues beyond surface markers. Finally, we finetune models\non sentences without commas at the syntactic boundary positions, encouraging\nthem to focus on more subtle linguistic cues. Our findings indicate that this\nleads to more distinct intonation patterns that better reflect the underlying\nstructure.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86TTS\u7cfb\u7edf\u5bf9\u53e5\u6cd5\u7684\u654f\u611f\u6027\uff0c\u53d1\u73b0\u7cfb\u7edf\u5728\u53e5\u6cd5\u8fb9\u754c\u6a21\u7cca\u65f6\u96be\u4ee5\u51c6\u786e\u751f\u6210\u8bed\u8c03\u77ed\u8bed\u8fb9\u754c\uff0c\u9700\u4f9d\u8d56\u9017\u53f7\u7b49\u8868\u9762\u6807\u8bb0\uff0c\u800c\u5728\u7b80\u5355\u53e5\u6cd5\u7ed3\u6784\u4e2d\u5219\u80fd\u5229\u7528\u66f4\u6df1\u5c42\u7684\u53e5\u6cd5\u7ebf\u7d22\u3002\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u5ffd\u7565\u9017\u53f7\uff0c\u7cfb\u7edf\u80fd\u751f\u6210\u66f4\u7b26\u5408\u5e95\u5c42\u7ed3\u6784\u7684\u8bed\u8c03\u6a21\u5f0f\u3002", "motivation": "\u63a2\u8ba8TTS\u7cfb\u7edf\u5728\u5904\u7406\u590d\u6742\u53e5\u6cd5\u7ed3\u6784\u65f6\u8bed\u8c03\u751f\u6210\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5f53\u53e5\u6cd5\u8fb9\u754c\u6a21\u7cca\u65f6\u7684\u8868\u73b0\uff0c\u4ee5\u6539\u8fdb\u5176\u81ea\u7136\u5ea6\u548c\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u5fc3\u7406\u8bed\u8a00\u5b66\u65b9\u6cd5\u5206\u6790TTS\u7cfb\u7edf\u751f\u6210\u8bed\u8c03\u77ed\u8bed\u8fb9\u754c\u7684\u884c\u4e3a\uff0c\u91cd\u70b9\u6d4b\u8bd5\u53e5\u6cd5\u6a21\u7cca\u53e5\u5b50\uff08\u5982\u82b1\u56ed\u8def\u5f84\u53e5\uff09\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u5ffd\u7565\u9017\u53f7\u6765\u4f18\u5316\u8bed\u8c03\u751f\u6210\u3002", "result": "TTS\u7cfb\u7edf\u5728\u53e5\u6cd5\u6a21\u7cca\u53e5\u5b50\u4e2d\u4f9d\u8d56\u8868\u9762\u6807\u8bb0\uff08\u5982\u9017\u53f7\uff09\uff0c\u4f46\u5728\u7b80\u5355\u7ed3\u6784\u4e2d\u80fd\u5229\u7528\u6df1\u5c42\u6b21\u53e5\u6cd5\u7ebf\u7d22\u3002\u5fae\u8c03\u540e\u6a21\u578b\u751f\u6210\u7684\u8bed\u8c03\u6a21\u5f0f\u66f4\u8d34\u5408\u53e5\u6cd5\u7ed3\u6784\u3002", "conclusion": "TTS\u7cfb\u7edf\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u66f4\u597d\u5730\u5904\u7406\u590d\u6742\u53e5\u6cd5\u7ed3\u6784\u7684\u8bed\u8c03\u751f\u6210\uff0c\u5f53\u524d\u4ecd\u4f9d\u8d56\u8868\u9762\u6807\u8bb0\uff0c\u4f46\u901a\u8fc7\u9488\u5bf9\u6027\u8bad\u7ec3\u53ef\u63d0\u5347\u8868\u73b0\u3002", "keywords": "Text-to-Speech, \u53e5\u6cd5\u654f\u611f\u6027, \u8bed\u8c03\u77ed\u8bed\u8fb9\u754c, \u82b1\u56ed\u8def\u5f84\u53e5, \u6a21\u578b\u5fae\u8c03"}}
{"id": "2505.21974", "pdf": "https://arxiv.org/pdf/2505.21974", "abs": "https://arxiv.org/abs/2505.21974", "authors": ["Yu-Heng Hung", "Kai-Jie Lin", "Yu-Heng Lin", "Chien-YiWang", "Cheng Sun", "Ping-Chun Hsieh"], "title": "BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via Non-Markovian RL", "categories": ["cs.LG"], "comment": null, "summary": "Bayesian optimization (BO) offers an efficient pipeline for optimizing\nblack-box functions with the help of a Gaussian process prior and an\nacquisition function (AF). Recently, in the context of single-objective BO,\nlearning-based AFs witnessed promising empirical results given its favorable\nnon-myopic nature. Despite this, the direct extension of these approaches to\nmulti-objective Bayesian optimization (MOBO) suffer from the\n\\textit{hypervolume identifiability issue}, which results from the\nnon-Markovian nature of MOBO problems. To tackle this, inspired by the\nnon-Markovian RL literature and the success of Transformers in language\nmodeling, we present a generalized deep Q-learning framework and propose\n\\textit{BOFormer}, which substantiates this framework for MOBO via sequence\nmodeling. Through extensive evaluation, we demonstrate that BOFormer constantly\noutperforms the benchmark rule-based and learning-based algorithms in various\nsynthetic MOBO and real-world multi-objective hyperparameter optimization\nproblems. We have made the source code publicly available to encourage further\nresearch in this direction.", "AI": {"tldr": "Bayesian optimization (BO) in multi-objective settings faces hypervolume identifiability issues. The paper proposes BOFormer, a deep Q-learning framework using Transformers, which outperforms existing methods in synthetic and real-world tasks.", "motivation": "Address the hypervolume identifiability issue in multi-objective Bayesian optimization (MOBO) by leveraging non-Markovian RL and Transformer models.", "method": "Introduces BOFormer, a generalized deep Q-learning framework for MOBO using sequence modeling with Transformers.", "result": "BOFormer consistently outperforms rule-based and learning-based benchmarks in synthetic and real-world multi-objective optimization tasks.", "conclusion": "BOFormer effectively addresses the non-Markovian challenges in MOBO, demonstrating superior performance in various applications.", "keywords": "Bayesian optimization, multi-objective optimization, deep Q-learning, Transformer, hypervolume identifiability"}}
{"id": "2505.22240", "pdf": "https://arxiv.org/pdf/2505.22240", "abs": "https://arxiv.org/abs/2505.22240", "authors": ["Yunsoo Kim", "Yusuf Abdulle", "Honghan Wu"], "title": "BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical Domain", "categories": ["cs.CL"], "comment": null, "summary": "Biomedical reasoning often requires traversing interconnected relationships\nacross entities such as drugs, diseases, and proteins. Despite the increasing\nprominence of large language models (LLMs), existing benchmarks lack the\nability to evaluate multi-hop reasoning in the biomedical domain, particularly\nfor queries involving one-to-many and many-to-many relationships. This gap\nleaves the critical challenges of biomedical multi-hop reasoning underexplored.\nTo address this, we introduce BioHopR, a novel benchmark designed to evaluate\nmulti-hop, multi-answer reasoning in structured biomedical knowledge graphs.\nBuilt from the comprehensive PrimeKG, BioHopR includes 1-hop and 2-hop\nreasoning tasks that reflect real-world biomedical complexities.\n  Evaluations of state-of-the-art models reveal that O3-mini, a proprietary\nreasoning-focused model, achieves 37.93% precision on 1-hop tasks and 14.57% on\n2-hop tasks, outperforming proprietary models such as GPT4O and open-source\nbiomedical models including HuatuoGPT-o1-70B and Llama-3.3-70B. However, all\nmodels exhibit significant declines in multi-hop performance, underscoring the\nchallenges of resolving implicit reasoning steps in the biomedical domain. By\naddressing the lack of benchmarks for multi-hop reasoning in biomedical domain,\nBioHopR sets a new standard for evaluating reasoning capabilities and\nhighlights critical gaps between proprietary and open-source models while\npaving the way for future advancements in biomedical LLMs.", "AI": {"tldr": "BioHopR\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u591a\u8df3\u591a\u7b54\u6848\u63a8\u7406\u80fd\u529b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u7a7a\u767d\u3002\u8bc4\u4f30\u663e\u793aO3-mini\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5728\u591a\u8df3\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u8bc4\u4f30\u751f\u7269\u533b\u5b66\u9886\u57df\u7684\u591a\u8df3\u63a8\u7406\uff0c\u5c24\u5176\u662f\u6d89\u53ca\u4e00\u5bf9\u591a\u548c\u591a\u5bf9\u591a\u5173\u7cfb\u7684\u67e5\u8be2\uff0c\u5bfc\u81f4\u751f\u7269\u533b\u5b66\u591a\u8df3\u63a8\u7406\u7684\u6311\u6218\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u57fa\u4e8ePrimeKG\u6784\u5efaBioHopR\uff0c\u5305\u542b1\u8df3\u548c2\u8df3\u63a8\u7406\u4efb\u52a1\uff0c\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u7684\u751f\u7269\u533b\u5b66\u590d\u6742\u6027\u3002\u8bc4\u4f30\u4e86O3-mini\u3001GPT4O\u3001HuatuoGPT-o1-70B\u548cLlama-3.3-70B\u7b49\u6a21\u578b\u3002", "result": "O3-mini\u57281\u8df3\u4efb\u52a1\u4e2d\u7cbe\u5ea6\u8fbe37.93%\uff0c2\u8df3\u4efb\u52a1\u4e2d\u4e3a14.57%\uff0c\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002\u6240\u6709\u6a21\u578b\u5728\u591a\u8df3\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "BioHopR\u4e3a\u751f\u7269\u533b\u5b66\u591a\u8df3\u63a8\u7406\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u63ed\u793a\u4e86\u4e13\u6709\u548c\u5f00\u6e90\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u4e3a\u672a\u6765\u751f\u7269\u533b\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u94fa\u5e73\u4e86\u9053\u8def\u3002", "keywords": "BioHopR, \u591a\u8df3\u63a8\u7406, \u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31, O3-mini, \u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2505.21978", "pdf": "https://arxiv.org/pdf/2505.21978", "abs": "https://arxiv.org/abs/2505.21978", "authors": ["Wanfu Gao", "Zengyao Man", "Zebin He", "Yuhao Tang", "Jun Gao", "Kunpeng Liu"], "title": "Two-Stage Feature Generation with Transformer and Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Feature generation is a critical step in machine learning, aiming to enhance\nmodel performance by capturing complex relationships within the data and\ngenerating meaningful new features. Traditional feature generation methods\nheavily rely on domain expertise and manual intervention, making the process\nlabor-intensive and challenging to adapt to different scenarios. Although\nautomated feature generation techniques address these issues to some extent,\nthey often face challenges such as feature redundancy, inefficiency in feature\nspace exploration, and limited adaptability to diverse datasets and tasks. To\naddress these problems, we propose a Two-Stage Feature Generation (TSFG)\nframework, which integrates a Transformer-based encoder-decoder architecture\nwith Proximal Policy Optimization (PPO). The encoder-decoder model in TSFG\nleverages the Transformer's self-attention mechanism to efficiently represent\nand transform features, capturing complex dependencies within the data. PPO\nfurther enhances TSFG by dynamically adjusting the feature generation strategy\nbased on task-specific feedback, optimizing the process for improved\nperformance and adaptability. TSFG dynamically generates high-quality feature\nsets, significantly improving the predictive performance of machine learning\nmodels. Experimental results demonstrate that TSFG outperforms existing\nstate-of-the-art methods in terms of feature quality and adaptability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u7279\u5f81\u751f\u6210\uff08TSFG\uff09\u6846\u67b6\uff0c\u7ed3\u5408Transformer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u548cPPO\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u7279\u5f81\u751f\u6210\u8d28\u91cf\u548c\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7279\u5f81\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u9886\u57df\u77e5\u8bc6\u548c\u4eba\u5de5\u5e72\u9884\uff0c\u81ea\u52a8\u5316\u6280\u672f\u53c8\u5b58\u5728\u5197\u4f59\u548c\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528Transformer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6355\u6349\u6570\u636e\u590d\u6742\u5173\u7cfb\uff0c\u5e76\u7ed3\u5408PPO\u52a8\u6001\u8c03\u6574\u7279\u5f81\u751f\u6210\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTSFG\u5728\u7279\u5f81\u8d28\u91cf\u548c\u9002\u5e94\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TSFG\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u7279\u5f81\u751f\u6210\u6846\u67b6\u3002", "keywords": "\u7279\u5f81\u751f\u6210, Transformer, PPO, \u673a\u5668\u5b66\u4e60, \u81ea\u9002\u5e94\u4f18\u5316"}}
{"id": "2505.21575", "pdf": "https://arxiv.org/pdf/2505.21575", "abs": "https://arxiv.org/abs/2505.21575", "authors": ["Dawei Feng", "Di Mei", "Huiri Tan", "Lei Ren", "Xianying Lou", "Zhangxi Tan"], "title": "StreamLink: Large-Language-Model Driven Distributed Data Engineering System", "categories": ["cs.DB", "cs.AI"], "comment": "Accepted by CIKM Workshop 2024,\n  https://sites.google.com/view/cikm2024-rag/papers?authuser=0#h.ddm5fg2z885t", "summary": "Large Language Models (LLMs) have shown remarkable proficiency in natural\nlanguage understanding (NLU), opening doors for innovative applications. We\nintroduce StreamLink - an LLM-driven distributed data system designed to\nimprove the efficiency and accessibility of data engineering tasks. We build\nStreamLink on top of distributed frameworks such as Apache Spark and Hadoop to\nhandle large data at scale. One of the important design philosophies of\nStreamLink is to respect user data privacy by utilizing local fine-tuned LLMs\ninstead of a public AI service like ChatGPT. With help from domain-adapted\nLLMs, we can improve our system's understanding of natural language queries\nfrom users in various scenarios and simplify the procedure of generating\ndatabase queries like the Structured Query Language (SQL) for information\nprocessing. We also incorporate LLM-based syntax and security checkers to\nguarantee the reliability and safety of each generated query. StreamLink\nillustrates the potential of merging generative LLMs with distributed data\nprocessing for comprehensive and user-centric data engineering. With this\narchitecture, we allow users to interact with complex database systems at\ndifferent scales in a user-friendly and security-ensured manner, where the SQL\ngeneration reaches over 10\\% of execution accuracy compared to baseline\nmethods, and allow users to find the most concerned item from hundreds of\nmillions of items within a few seconds using natural language.", "AI": {"tldr": "StreamLink\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5206\u5e03\u5f0f\u6570\u636e\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u672c\u5730\u5fae\u8c03\u7684LLM\u63d0\u5347\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u7684\u6548\u7387\u548c\u53ef\u8bbf\u95ee\u6027\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u6570\u636e\u9690\u79c1\u3002", "motivation": "\u5229\u7528LLM\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u7b80\u5316\u7528\u6237\u4e0e\u590d\u6742\u6570\u636e\u5e93\u7cfb\u7edf\u7684\u4ea4\u4e92\uff0c\u540c\u65f6\u786e\u4fdd\u6570\u636e\u9690\u79c1\u548c\u5b89\u5168\u6027\u3002", "method": "\u6784\u5efa\u4e8eApache Spark\u548cHadoop\u7b49\u5206\u5e03\u5f0f\u6846\u67b6\u4e4b\u4e0a\uff0c\u7ed3\u5408\u9886\u57df\u9002\u5e94\u7684LLM\u548c\u8bed\u6cd5/\u5b89\u5168\u68c0\u67e5\u5668\uff0c\u4f18\u5316\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u5230SQL\u7684\u8f6c\u6362\u3002", "result": "SQL\u751f\u6210\u6267\u884c\u51c6\u786e\u7387\u8d85\u8fc7\u57fa\u7ebf\u65b9\u6cd510%\uff0c\u652f\u6301\u7528\u6237\u5728\u6570\u79d2\u5185\u4ece\u6d77\u91cf\u6570\u636e\u4e2d\u68c0\u7d22\u76ee\u6807\u6761\u76ee\u3002", "conclusion": "StreamLink\u5c55\u793a\u4e86\u751f\u6210\u5f0fLLM\u4e0e\u5206\u5e03\u5f0f\u6570\u636e\u5904\u7406\u7ed3\u5408\u5728\u7528\u6237\u53cb\u597d\u3001\u5b89\u5168\u7684\u6570\u636e\u5de5\u7a0b\u4e2d\u7684\u6f5c\u529b\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u5206\u5e03\u5f0f\u7cfb\u7edf, \u6570\u636e\u5de5\u7a0b, \u81ea\u7136\u8bed\u8a00\u5904\u7406, \u9690\u79c1\u4fdd\u62a4"}}
{"id": "2505.22264", "pdf": "https://arxiv.org/pdf/2505.22264", "abs": "https://arxiv.org/abs/2505.22264", "authors": ["Maximiliano Hormaz\u00e1bal Lagos", "\u00c1lvaro Bueno Saez", "H\u00e9ctor Cerezo-Costas", "Pedro Alonso Doval", "Jorge Alcalde Vesteiro"], "title": "MRT at SemEval-2025 Task 8: Maximizing Recovery from Tables with Multiple Steps", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "7 pages, 6 tables", "summary": "In this paper we expose our approach to solve the \\textit{SemEval 2025 Task\n8: Question-Answering over Tabular Data} challenge. Our strategy leverages\nPython code generation with LLMs to interact with the table and get the answer\nto the questions. The process is composed of multiple steps: understanding the\ncontent of the table, generating natural language instructions in the form of\nsteps to follow in order to get the answer, translating these instructions to\ncode, running it and handling potential errors or exceptions. These steps use\nopen source LLMs and fine grained optimized prompts for each task (step). With\nthis approach, we achieved a score of $70.50\\%$ for subtask 1.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3SemEval 2025\u4efb\u52a18\uff08\u57fa\u4e8e\u8868\u683c\u6570\u636e\u7684\u95ee\u7b54\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528LLMs\u751f\u6210Python\u4ee3\u7801\u4e0e\u8868\u683c\u4e92\u52a8\uff0c\u5e76\u901a\u8fc7\u591a\u6b65\u9aa4\u6d41\u7a0b\uff08\u5305\u62ec\u7406\u89e3\u8868\u683c\u5185\u5bb9\u3001\u751f\u6210\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u3001\u7ffb\u8bd1\u4e3a\u4ee3\u7801\u5e76\u6267\u884c\uff09\u53d6\u5f97\u4e8670.50%\u7684\u5f97\u5206\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u4ee3\u7801\u751f\u6210\u4f18\u5316\u8868\u683c\u6570\u636e\u7684\u95ee\u7b54\u4efb\u52a1\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "method": "\u5229\u7528\u5f00\u6e90LLMs\u548c\u591a\u6b65\u9aa4\u6d41\u7a0b\uff08\u7406\u89e3\u8868\u683c\u3001\u751f\u6210\u6307\u4ee4\u3001\u4ee3\u7801\u7ffb\u8bd1\u548c\u6267\u884c\uff09\uff0c\u9488\u5bf9\u6bcf\u9879\u4efb\u52a1\u4f18\u5316\u63d0\u793a\u3002", "result": "\u5728\u5b50\u4efb\u52a11\u4e2d\u53d6\u5f97\u4e8670.50%\u7684\u5f97\u5206\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86LLMs\u5728\u8868\u683c\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u4f18\u5316\u4ee5\u63d0\u5347\u8868\u73b0\u3002", "keywords": "LLMs, \u8868\u683c\u6570\u636e, \u4ee3\u7801\u751f\u6210, \u95ee\u7b54\u7cfb\u7edf, SemEval"}}
{"id": "2505.21987", "pdf": "https://arxiv.org/pdf/2505.21987", "abs": "https://arxiv.org/abs/2505.21987", "authors": ["Zhendong Mi", "Zhenglun Kong", "Geng Yuan", "Shaoyi Huang"], "title": "ACE: Exploring Activation Cosine Similarity and Variance for Accurate and Calibration-Efficient LLM Pruning", "categories": ["cs.LG", "I.2.6; I.2.7"], "comment": "9 pages, 2 figures, 13 tables", "summary": "With the rapid expansion of large language models (LLMs), the demand for\nmemory and computational resources has grown significantly. Recent advances in\nLLM pruning aim to reduce the size and computational cost of these models.\nHowever, existing methods often suffer from either suboptimal pruning\nperformance or low time efficiency during the pruning process. In this work, we\npropose an efficient and effective pruning method that simultaneously achieves\nhigh pruning performance and fast pruning speed with improved calibration\nefficiency. Our approach introduces two key innovations: (1) An activation\ncosine similarity loss-guided pruning metric, which considers the angular\ndeviation of the output activation between the dense and pruned models. (2) An\nactivation variance-guided pruning metric, which helps preserve semantic\ndistinctions in output activations after pruning, enabling effective pruning\nwith shorter input sequences. These two components can be readily combined to\nenhance LLM pruning in both accuracy and efficiency. Experimental results show\nthat our method achieves up to an 18% reduction in perplexity and up to 63%\ndecrease in pruning time on prevalent LLMs such as LLaMA, LLaMA-2, and OPT.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5927\u8bed\u8a00\u6a21\u578b\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u79cd\u521b\u65b0\u6307\u6807\u63d0\u5347\u526a\u679d\u6027\u80fd\u548c\u901f\u5ea6\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728LLaMA\u7b49\u6a21\u578b\u4e0a\u56f0\u60d1\u5ea6\u964d\u4f4e18%\uff0c\u526a\u679d\u65f6\u95f4\u51cf\u5c1163%\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u526a\u679d\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u4e24\u79cd\u65b0\u6307\u6807\u2014\u2014\u6fc0\u6d3b\u4f59\u5f26\u76f8\u4f3c\u5ea6\u635f\u5931\u526a\u679d\u6307\u6807\u548c\u6fc0\u6d3b\u65b9\u5dee\u526a\u679d\u6307\u6807\uff0c\u4ee5\u4f18\u5316\u526a\u679d\u6548\u679c\u548c\u901f\u5ea6\u3002", "result": "\u5728LLaMA\u7b49\u6a21\u578b\u4e0a\uff0c\u5b9e\u73b0\u4e86\u56f0\u60d1\u5ea6\u964d\u4f4e18%\uff0c\u526a\u679d\u65f6\u95f4\u51cf\u5c1163%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u526a\u679d\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u526a\u679d, \u6fc0\u6d3b\u4f59\u5f26\u76f8\u4f3c\u5ea6, \u6fc0\u6d3b\u65b9\u5dee, \u6548\u7387\u4f18\u5316"}}
{"id": "2505.22273", "pdf": "https://arxiv.org/pdf/2505.22273", "abs": "https://arxiv.org/abs/2505.22273", "authors": ["Shohei Higashiyama", "Masao Utiyama"], "title": "Comprehensive Evaluation on Lexical Normalization: Boundary-Aware Approaches for Unsegmented Languages", "categories": ["cs.CL"], "comment": "23 pages", "summary": "Lexical normalization research has sought to tackle the challenge of\nprocessing informal expressions in user-generated text, yet the absence of\ncomprehensive evaluations leaves it unclear which methods excel across multiple\nperspectives. Focusing on unsegmented languages, we make three key\ncontributions: (1) creating a large-scale, multi-domain Japanese normalization\ndataset, (2) developing normalization methods based on state-of-the-art\npretrained models, and (3) conducting experiments across multiple evaluation\nperspectives. Our experiments show that both encoder-only and decoder-only\napproaches achieve promising results in both accuracy and efficiency.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u672a\u5206\u8bcd\u8bed\u8a00\u7684\u8bcd\u6c47\u89c4\u8303\u5316\u95ee\u9898\uff0c\u8d21\u732e\u5305\u62ec\u521b\u5efa\u5927\u89c4\u6a21\u65e5\u8bed\u6570\u636e\u96c6\u3001\u5f00\u53d1\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u591a\u89c6\u89d2\u5b9e\u9a8c\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u7528\u6237\u751f\u6210\u6587\u672c\u4e2d\u975e\u6b63\u5f0f\u8868\u8fbe\u7684\u5904\u7406\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u672a\u5206\u8bcd\u8bed\u8a00\u4e2d\u7f3a\u4e4f\u5168\u9762\u8bc4\u4f30\u7684\u73b0\u72b6\u3002", "method": "\u57fa\u4e8e\u6700\u5148\u8fdb\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5f00\u53d1\u89c4\u8303\u5316\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u9886\u57df\u65e5\u8bed\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u7f16\u7801\u5668\u548c\u4ec5\u89e3\u7801\u5668\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u9886\u57df\u65e5\u8bed\u8bcd\u6c47\u89c4\u8303\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6f5c\u529b\u3002", "keywords": "\u8bcd\u6c47\u89c4\u8303\u5316, \u672a\u5206\u8bcd\u8bed\u8a00, \u9884\u8bad\u7ec3\u6a21\u578b, \u65e5\u8bed\u6570\u636e\u96c6"}}
{"id": "2505.22014", "pdf": "https://arxiv.org/pdf/2505.22014", "abs": "https://arxiv.org/abs/2505.22014", "authors": ["J\u00f6rg K. H. Franke", "Urs Spiegelhalter", "Marianna Nezhurina", "Jenia Jitsev", "Frank Hutter", "Michael Hefenbrock"], "title": "Learning in Compact Spaces with Approximately Normalized Transformers", "categories": ["cs.LG"], "comment": "Preprint", "summary": "In deep learning, regularization and normalization are common solutions for\nchallenges such as overfitting, numerical instabilities, and the increasing\nvariance in the residual stream. An alternative approach is to force all\nparameters and representations to lie on a hypersphere. This removes the need\nfor regularization and increases convergence speed, but comes with additional\ncosts. In this work, we propose a more holistic but approximate normalization\n(anTransformer). Our approach constrains the norm of parameters and normalizes\nall representations via scalar multiplications motivated by the tight\nconcentration of the norms of high-dimensional random vectors. When applied to\nGPT training, we observe a 40% faster convergence compared to models with QK\nnormalization, with less than 3% additional runtime. Deriving scaling laws for\nanGPT, we found our method enables training with larger batch sizes and fewer\nhyperparameters, while matching the favorable scaling characteristics of\nclassic GPT architectures.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd1\u4f3c\u5f52\u4e00\u5316\u65b9\u6cd5anTransformer\uff0c\u901a\u8fc7\u7ea6\u675f\u53c2\u6570\u8303\u6570\u548c\u5f52\u4e00\u5316\u8868\u793a\uff0c\u63d0\u5347\u4e86\u6536\u655b\u901f\u5ea6\u5e76\u51cf\u5c11\u4e86\u8d85\u53c2\u6570\u9700\u6c42\uff0c\u9002\u7528\u4e8eGPT\u8bad\u7ec3\uff0c\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4e2d\u6b63\u5219\u5316\u548c\u5f52\u4e00\u5316\u5e38\u7528\u4e8e\u89e3\u51b3\u8fc7\u62df\u5408\u7b49\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u989d\u5916\u6210\u672c\u3002\u8bba\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u5168\u9762\u7684\u8fd1\u4f3c\u5f52\u4e00\u5316\u65b9\u6cd5\u4ee5\u63d0\u5347\u6548\u7387\u3002", "method": "\u63d0\u51faanTransformer\u65b9\u6cd5\uff0c\u7ea6\u675f\u53c2\u6570\u8303\u6570\u5e76\u901a\u8fc7\u6807\u91cf\u4e58\u6cd5\u5f52\u4e00\u5316\u8868\u793a\uff0c\u5229\u7528\u4e86\u9ad8\u7ef4\u968f\u673a\u5411\u91cf\u8303\u6570\u7684\u7d27\u5bc6\u96c6\u4e2d\u7279\u6027\u3002", "result": "\u5728GPT\u8bad\u7ec3\u4e2d\uff0c\u76f8\u6bd4QK\u5f52\u4e00\u5316\u65b9\u6cd5\u6536\u655b\u901f\u5ea6\u63d0\u534740%\uff0c\u8fd0\u884c\u65f6\u4ec5\u589e\u52a03%\uff0c\u652f\u6301\u66f4\u5927\u6279\u6b21\u8bad\u7ec3\u4e14\u8d85\u53c2\u6570\u9700\u6c42\u51cf\u5c11\u3002", "conclusion": "anTransformer\u65b9\u6cd5\u5728\u4fdd\u6301\u7ecf\u5178GPT\u67b6\u6784\u4f18\u70b9\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u5177\u6709\u5b9e\u7528\u6f5c\u529b\u3002", "keywords": "\u5f52\u4e00\u5316, \u6df1\u5ea6\u5b66\u4e60, GPT, \u6536\u655b\u901f\u5ea6, \u8d85\u53c2\u6570\u4f18\u5316"}}
{"id": "2505.21577", "pdf": "https://arxiv.org/pdf/2505.21577", "abs": "https://arxiv.org/abs/2505.21577", "authors": ["Huacan Wang", "Ziyi Ni", "Shuo Zhang", "Shuo Lu", "Sen Hu", "Ziyang He", "Chen Hu", "Jiaye Lin", "Yifu Guo", "Yuntao Du", "Pin Lyu"], "title": "RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for Complex Task Solving", "categories": ["cs.SE", "cs.AI"], "comment": "A novel approach; Very practical", "summary": "The ultimate goal of code agents is to solve complex tasks autonomously.\nAlthough large language models (LLMs) have made substantial progress in code\ngeneration, real-world tasks typically demand full-fledged code repositories\nrather than simple scripts. Building such repositories from scratch remains a\nmajor challenge. Fortunately, GitHub hosts a vast, evolving collection of\nopen-source repositories, which developers frequently reuse as modular\ncomponents for complex tasks. Yet, existing frameworks like OpenHands and\nSWE-Agent still struggle to effectively leverage these valuable resources.\nRelying solely on README files provides insufficient guidance, and deeper\nexploration reveals two core obstacles: overwhelming information and tangled\ndependencies of repositories, both constrained by the limited context windows\nof current LLMs. To tackle these issues, we propose RepoMaster, an autonomous\nagent framework designed to explore and reuse GitHub repositories for solving\ncomplex tasks. For efficient understanding, RepoMaster constructs function-call\ngraphs, module-dependency graphs, and hierarchical code trees to identify\nessential components, providing only identified core elements to the LLMs\nrather than the entire repository. During autonomous execution, it\nprogressively explores related components using our exploration tools and\nprunes information to optimize context usage. Evaluated on the adjusted\nMLE-bench, RepoMaster achieves a 110% relative boost in valid submissions over\nthe strongest baseline OpenHands. On our newly released GitTaskBench,\nRepoMaster lifts the task-pass rate from 24.1% to 62.9% while reducing token\nusage by 95%. Our code and demonstration materials are publicly available at\nhttps://github.com/wanghuacan/RepoMaster.", "AI": {"tldr": "RepoMaster\u662f\u4e00\u79cd\u81ea\u4e3b\u4ee3\u7406\u6846\u67b6\uff0c\u65e8\u5728\u63a2\u7d22\u548c\u91cd\u7528GitHub\u4ed3\u5e93\u4ee5\u89e3\u51b3\u590d\u6742\u4efb\u52a1\uff0c\u901a\u8fc7\u6784\u5efa\u529f\u80fd\u8c03\u7528\u56fe\u3001\u6a21\u5757\u4f9d\u8d56\u56fe\u548c\u5206\u5c42\u4ee3\u7801\u6811\u6765\u63d0\u9ad8\u6548\u7387\uff0c\u5e76\u5728\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u6846\u67b6\u5982OpenHands\u548cSWE-Agent\u5728\u5229\u7528GitHub\u5f00\u6e90\u4ed3\u5e93\u65f6\u6548\u7387\u4e0d\u8db3\uff0c\u4e3b\u8981\u56e0\u4e3a\u4fe1\u606f\u8fc7\u8f7d\u548c\u4f9d\u8d56\u5173\u7cfb\u590d\u6742\uff0c\u4e14\u53d7\u9650\u4e8eLLMs\u7684\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002", "method": "RepoMaster\u901a\u8fc7\u6784\u5efa\u529f\u80fd\u8c03\u7528\u56fe\u3001\u6a21\u5757\u4f9d\u8d56\u56fe\u548c\u5206\u5c42\u4ee3\u7801\u6811\u6765\u8bc6\u522b\u6838\u5fc3\u7ec4\u4ef6\uff0c\u5e76\u9010\u6b65\u63a2\u7d22\u548c\u4fee\u526a\u4fe1\u606f\u4ee5\u4f18\u5316\u4e0a\u4e0b\u6587\u4f7f\u7528\u3002", "result": "RepoMaster\u5728\u8c03\u6574\u540e\u7684MLE-bench\u4e0a\u6bd4\u57fa\u7ebfOpenHands\u63d0\u9ad8\u4e86110%\u7684\u6709\u6548\u63d0\u4ea4\u7387\uff0c\u5728GitTaskBench\u4e0a\u5c06\u4efb\u52a1\u901a\u8fc7\u7387\u4ece24.1%\u63d0\u5347\u81f362.9%\uff0c\u540c\u65f6\u51cf\u5c1195%\u7684token\u4f7f\u7528\u3002", "conclusion": "RepoMaster\u901a\u8fc7\u9ad8\u6548\u5229\u7528GitHub\u4ed3\u5e93\u4e2d\u7684\u6a21\u5757\u5316\u7ec4\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u4efb\u52a1\u7684\u89e3\u51b3\u80fd\u529b\uff0c\u540c\u65f6\u4f18\u5316\u8d44\u6e90\u4f7f\u7528\u3002", "keywords": "\u4ee3\u7801\u4ee3\u7406, GitHub\u4ed3\u5e93, \u4f9d\u8d56\u5173\u7cfb, \u5927\u8bed\u8a00\u6a21\u578b, RepoMaster"}}
{"id": "2505.22280", "pdf": "https://arxiv.org/pdf/2505.22280", "abs": "https://arxiv.org/abs/2505.22280", "authors": ["Zihan Xu", "Haotian Ma", "Gongbo Zhang", "Yihao Ding", "Chunhua Weng", "Yifan Peng"], "title": "Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Findings", "summary": "Evidence-based medicine (EBM) is at the forefront of modern healthcare,\nemphasizing the use of the best available scientific evidence to guide clinical\ndecisions. Due to the sheer volume and rapid growth of medical literature and\nthe high cost of curation, there is a critical need to investigate Natural\nLanguage Processing (NLP) methods to identify, appraise, synthesize, summarize,\nand disseminate evidence in EBM. This survey presents an in-depth review of 129\nresearch studies on leveraging NLP for EBM, illustrating its pivotal role in\nenhancing clinical decision-making processes. The paper systematically explores\nhow NLP supports the five fundamental steps of EBM -- Ask, Acquire, Appraise,\nApply, and Assess. The review not only identifies current limitations within\nthe field but also proposes directions for future research, emphasizing the\npotential for NLP to revolutionize EBM by refining evidence extraction,\nevidence synthesis, appraisal, summarization, enhancing data comprehensibility,\nand facilitating a more efficient clinical workflow.", "AI": {"tldr": "\u8be5\u8c03\u67e5\u7efc\u8ff0\u4e86129\u9879\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4fc3\u8fdb\u5faa\u8bc1\u533b\u5b66\uff08EBM\uff09\u7684\u7814\u7a76\uff0c\u63a2\u8ba8NLP\u5982\u4f55\u652f\u6301EBM\u7684\u4e94\u4e2a\u5173\u952e\u6b65\u9aa4\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u4ee5\u4f18\u5316\u8bc1\u636e\u63d0\u53d6\u3001\u5408\u6210\u4e0e\u4e34\u5e8a\u5e94\u7528\u3002", "motivation": "\u7531\u4e8e\u533b\u5b66\u6587\u732e\u91cf\u5927\u4e14\u5feb\u901f\u589e\u2ed3\uff0c\u4eba\u5de5\u6574\u7406\u6210\u672c\u9ad8\uff0cNLP\u65b9\u6cd5\u53ef\u9ad8\u6548\u652f\u6301\u5faa\u8bc1\u533b\u5b66\u7684\u8bc1\u636e\u8bc6\u522b\u3001\u8bc4\u4f30\u4e0e\u4f20\u64ad\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u56de\u987e129\u9879\u76f8\u5173\u7814\u7a76\uff0c\u5206\u6790NLP\u5728EBM\u4e94\u4e2a\u6838\u5fc3\u6b65\u9aa4\uff08Ask, Acquire, Appraise, Apply, Assess\uff09\u4e2d\u7684\u5e94\u7528\u3002", "result": "NLP\u80fd\u663e\u8457\u63d0\u5347EBM\u7684\u8bc1\u636e\u63d0\u53d6\u3001\u5408\u6210\u3001\u8bc4\u4f30\u548c\u4e34\u5e8a\u51b3\u7b56\u6548\u7387\uff0c\u4f46\u5f53\u524d\u6280\u672f\u4ecd\u5b58\u5728\u5c40\u9650\u6027\u3002", "conclusion": "NLP\u6709\u671b\u9769\u547d\u6027\u4f18\u5316EBM\u6d41\u7a0b\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u7a81\u7834\u6280\u672f\u74f6\u9888\u5e76\u589e\u5f3a\u6570\u636e\u53ef\u7406\u89e3\u6027\u3002", "keywords": "\u5faa\u8bc1\u533b\u5b66, \u81ea\u7136\u8bed\u8a00\u5904\u7406, \u8bc1\u636e\u63d0\u53d6, \u4e34\u5e8a\u51b3\u7b56, \u6587\u732e\u7efc\u8ff0"}}
{"id": "2505.22028", "pdf": "https://arxiv.org/pdf/2505.22028", "abs": "https://arxiv.org/abs/2505.22028", "authors": ["Zi-Hao Zhou", "Jun-Jie Wang", "Tong Wei", "Min-Ling Zhang"], "title": "Weakly-Supervised Contrastive Learning for Imprecise Class Labels", "categories": ["cs.LG"], "comment": "38 pages, 2 figures, 11 tables", "summary": "Contrastive learning has achieved remarkable success in learning effective\nrepresentations, with supervised contrastive learning often outperforming\nself-supervised approaches. However, in real-world scenarios, data annotations\nare often ambiguous or inaccurate, meaning that class labels may not reliably\nindicate whether two examples belong to the same class. This limitation\nrestricts the applicability of supervised contrastive learning. To address this\nchallenge, we introduce the concept of ``continuous semantic similarity'' to\ndefine positive and negative pairs. Instead of directly relying on imprecise\nclass labels, we measure the semantic similarity between example pairs, which\nquantifies how closely they belong to the same category by iteratively refining\nweak supervisory signals. Based on this concept, we propose a graph-theoretic\nframework for weakly-supervised contrastive learning, where semantic similarity\nserves as the graph weights. Our framework is highly versatile and can be\napplied to many weakly-supervised learning scenarios. We demonstrate its\neffectiveness through experiments in two common settings, i.e., noisy label and\npartial label learning, where existing methods can be easily integrated to\nsignificantly improve performance. Theoretically, we establish an error bound\nfor our approach, showing that it can approximate supervised contrastive\nlearning under mild conditions. The implementation code is available at\nhttps://github.com/Speechless-10308/WSC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fde\u7eed\u8bed\u4e49\u76f8\u4f3c\u6027\u7684\u5f31\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u5b9e\u9645\u6570\u636e\u4e2d\u6807\u7b7e\u6a21\u7cca\u6216\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u56fe\u8bba\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5b9e\u9645\u573a\u666f\u4e2d\u6570\u636e\u6807\u6ce8\u5e38\u4e0d\u7cbe\u786e\uff0c\u9650\u5236\u4e86\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u7684\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u5904\u7406\u6a21\u7cca\u6807\u7b7e\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u8fde\u7eed\u8bed\u4e49\u76f8\u4f3c\u6027\u5b9a\u4e49\u6b63\u8d1f\u6837\u672c\u5bf9\uff0c\u7ed3\u5408\u56fe\u8bba\u6846\u67b6\u8fed\u4ee3\u4f18\u5316\u5f31\u76d1\u7763\u4fe1\u53f7\u3002", "result": "\u5728\u566a\u58f0\u6807\u7b7e\u548c\u90e8\u5206\u6807\u7b7e\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u53ef\u903c\u8fd1\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6548\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5177\u6709\u666e\u9002\u6027\uff0c\u80fd\u663e\u8457\u63d0\u5347\u5f31\u76d1\u7763\u5b66\u4e60\u6027\u80fd\u3002", "keywords": "\u5bf9\u6bd4\u5b66\u4e60,\u5f31\u76d1\u7763,\u8bed\u4e49\u76f8\u4f3c\u6027,\u56fe\u8bba,\u566a\u58f0\u6807\u7b7e"}}
{"id": "2505.21582", "pdf": "https://arxiv.org/pdf/2505.21582", "abs": "https://arxiv.org/abs/2505.21582", "authors": ["Christopher Knievel", "Alexander Bernhardt", "Christian Bernhardt"], "title": "AITEE -- Agentic Tutor for Electrical Engineering", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "12 pages, 11 figures, 6 tables", "summary": "Intelligent tutoring systems combined with large language models offer a\npromising approach to address students' diverse needs and promote\nself-efficacious learning. While large language models possess good\nfoundational knowledge of electrical engineering basics, they remain\ninsufficiently capable of addressing specific questions about electrical\ncircuits. In this paper, we present AITEE, an agent-based tutoring system for\nelectrical engineering designed to accompany students throughout their learning\nprocess, offer individualized support, and promote self-directed learning.\nAITEE supports both hand-drawn and digital circuits through an adapted circuit\nreconstruction process, enabling natural interaction with students. Our novel\ngraph-based similarity measure identifies relevant context from lecture\nmaterials through a retrieval augmented generation approach, while parallel\nSpice simulation further enhances accuracy in applying solution methodologies.\nThe system implements a Socratic dialogue to foster learner autonomy through\nguided questioning. Experimental evaluations demonstrate that AITEE\nsignificantly outperforms baseline approaches in domain-specific knowledge\napplication, with even medium-sized LLM models showing acceptable performance.\nOur results highlight the potential of agentic tutors to deliver scalable,\npersonalized, and effective learning environments for electrical engineering\neducation.", "AI": {"tldr": "AITEE \u662f\u4e00\u79cd\u57fa\u4e8e\u667a\u80fd\u4ee3\u7406\u7684\u7535\u6c14\u5de5\u7a0b\u8f85\u5bfc\u7cfb\u7edf\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u7535\u8def\u91cd\u5efa\u6280\u672f\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u652f\u6301\u548c\u82cf\u683c\u62c9\u5e95\u5f0f\u5bf9\u8bdd\u63d0\u5347\u5b66\u751f\u5b66\u4e60\u6548\u679c\u3002\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u9886\u57df\u77e5\u8bc6\u5e94\u7528\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7535\u6c14\u5de5\u7a0b\u7279\u5b9a\u95ee\u9898\uff08\u5982\u7535\u8def\u5206\u6790\uff09\u4e0a\u7684\u4e0d\u8db3\uff0c\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u63d0\u4f9b\u4e2a\u6027\u5316\u652f\u6301\u5e76\u4fc3\u8fdb\u81ea\u4e3b\u5b66\u4e60\u7684\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u3002", "method": "1. \u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u56fe\u76f8\u4f3c\u5ea6\u6d4b\u91cf\u4ece\u6559\u6750\u4e2d\u63d0\u53d6\u4e0a\u4e0b\u6587\uff1b2. \u901a\u8fc7\u5e76\u884cSpice\u4eff\u771f\u63d0\u5347\u89e3\u51b3\u65b9\u6848\u7684\u51c6\u786e\u6027\uff1b3. \u91c7\u7528\u82cf\u683c\u62c9\u5e95\u5f0f\u5bf9\u8bdd\u5f15\u5bfc\u5b66\u751f\u81ea\u4e3b\u601d\u8003\u3002", "result": "AITEE \u5728\u9886\u57df\u77e5\u8bc6\u5e94\u7528\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5373\u4f7f\u4e2d\u7b49\u89c4\u6a21\u7684LLM\u6a21\u578b\u4e5f\u80fd\u53d6\u5f97\u53ef\u63a5\u53d7\u7684\u6027\u80fd\u3002", "conclusion": "AITEE \u5c55\u793a\u4e86\u4ee3\u7406\u578b\u8f85\u5bfc\u7cfb\u7edf\u5728\u7535\u6c14\u5de5\u7a0b\u6559\u80b2\u4e2d\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u4e2a\u6027\u5316\u4e14\u9ad8\u6548\u5b66\u4e60\u73af\u5883\u7684\u6f5c\u529b\u3002", "keywords": "\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u7535\u6c14\u5de5\u7a0b, \u7535\u8def\u91cd\u5efa, \u68c0\u7d22\u589e\u5f3a\u751f\u6210, Spice\u4eff\u771f"}}
{"id": "2505.22293", "pdf": "https://arxiv.org/pdf/2505.22293", "abs": "https://arxiv.org/abs/2505.22293", "authors": ["Samuel Frontull", "Thomas Str\u00f6hle"], "title": "Compensating for Data with Reasoning: Low-Resource Machine Translation with LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\nmultilingual machine translation, sometimes even outperforming traditional\nneural systems. However, previous research has highlighted the challenges of\nusing LLMs, particularly with prompt engineering, for low-resource languages.\nIn this work, we introduce Fragment-Shot Prompting, a novel in-context learning\nmethod that segments input and retrieves translation examples based on\nsyntactic coverage, along with Pivoted Fragment-Shot, an extension that enables\ntranslation without direct parallel data. We evaluate these methods using\nGPT-3.5, GPT-4o, o1-mini, LLaMA-3.3, and DeepSeek-R1 for translation between\nItalian and two Ladin variants, revealing three key findings: (1) Fragment-Shot\nPrompting is effective for translating into and between the studied\nlow-resource languages, with syntactic coverage positively correlating with\ntranslation quality; (2) Models with stronger reasoning abilities make more\neffective use of retrieved knowledge, generally produce better translations,\nand enable Pivoted Fragment-Shot to significantly improve translation quality\nbetween the Ladin variants; and (3) prompt engineering offers limited, if any,\nimprovements when translating from a low-resource to a high-resource language,\nwhere zero-shot prompting already yields satisfactory results. We publicly\nrelease our code and the retrieval corpora.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Fragment-Shot Prompting\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5757\u8f93\u5165\u548c\u57fa\u4e8e\u8bed\u6cd5\u8986\u76d6\u7684\u7ffb\u8bd1\u793a\u4f8b\u68c0\u7d22\u6765\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u9ad8\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u4e2d\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u63d0\u793a\u5de5\u7a0b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faFragment-Shot Prompting\u548cPivoted Fragment-Shot\u4e24\u79cd\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5757\u8f93\u5165\u548c\u8bed\u6cd5\u8986\u76d6\u68c0\u7d22\u793a\u4f8b\uff0c\u5229\u7528\u591a\u79cdLLMs\uff08\u5982GPT\u7cfb\u5217\u3001LLaMA\u7b49\uff09\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u53d1\u73b0Fragment-Shot Prompting\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u6709\u6548\uff0c\u8bed\u6cd5\u8986\u76d6\u4e0e\u7ffb\u8bd1\u8d28\u91cf\u6b63\u76f8\u5173\uff1b\u6a21\u578b\u63a8\u7406\u80fd\u529b\u8d8a\u5f3a\uff0c\u7ffb\u8bd1\u6548\u679c\u8d8a\u597d\uff1b\u63d0\u793a\u5de5\u7a0b\u5bf9\u4ece\u4f4e\u8d44\u6e90\u5230\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684\u7ffb\u8bd1\u6539\u5584\u6709\u9650\u3002", "conclusion": "Fragment-Shot Prompting\u662f\u4e00\u79cd\u6709\u6548\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5bf9\u63a8\u7406\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u3002", "keywords": "\u591a\u8bed\u8a00\u673a\u5668\u7ffb\u8bd1, \u4f4e\u8d44\u6e90\u8bed\u8a00, \u4e0a\u4e0b\u6587\u5b66\u4e60, \u63d0\u793a\u5de5\u7a0b, Fragment-Shot Prompting"}}
{"id": "2505.22041", "pdf": "https://arxiv.org/pdf/2505.22041", "abs": "https://arxiv.org/abs/2505.22041", "authors": ["Michael Grohs", "Adrian Rebmann", "Jana-Rebecca Rehse"], "title": "Detecting Undesired Process Behavior by Means of Retrieval Augmented Generation", "categories": ["cs.LG"], "comment": "Accepted at the BPM Forum, located at the International Conference on\n  Business Process Management (BPM) 2025", "summary": "Conformance checking techniques detect undesired process behavior by\ncomparing process executions that are recorded in event logs to desired\nbehavior that is captured in a dedicated process model. If such models are not\navailable, conformance checking techniques are not applicable, but\norganizations might still be interested in detecting undesired behavior in\ntheir processes. To enable this, existing approaches use Large Language Models\n(LLMs), assuming that they can learn to distinguish desired from undesired\nbehavior through fine-tuning. However, fine-tuning is highly resource-intensive\nand the fine-tuned LLMs often do not generalize well. To address these\nlimitations, we propose an approach that requires neither a dedicated process\nmodel nor resource-intensive fine-tuning to detect undesired process behavior.\nInstead, we use Retrieval Augmented Generation (RAG) to provide an LLM with\ndirect access to a knowledge base that contains both desired and undesired\nprocess behavior from other processes, assuming that the LLM can transfer this\nknowledge to the process at hand. Our evaluation shows that our approach\noutperforms fine-tuned LLMs in detecting undesired behavior, demonstrating that\nRAG is a viable alternative to resource-intensive fine-tuning, particularly\nwhen enriched with relevant context from the event log, such as frequent traces\nand activities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6ca1\u6709\u4e13\u7528\u8fc7\u7a0b\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u68c0\u6d4b\u4e0d\u671f\u671b\u7684\u8fc7\u7a0b\u884c\u4e3a\uff0c\u907f\u514d\u4e86\u8d44\u6e90\u5bc6\u96c6\u7684\u5fae\u8c03\uff0c\u5e76\u4f18\u4e8e\u4f20\u7edf\u5fae\u8c03LLM\u7684\u65b9\u6cd5\u3002", "motivation": "\u7f3a\u4e4f\u8fc7\u7a0b\u6a21\u578b\u65f6\uff0c\u4f20\u7edf\u7684\u4e00\u81f4\u6027\u68c0\u67e5\u6280\u672f\u65e0\u6cd5\u5e94\u7528\uff0c\u800c\u73b0\u6709\u7684\u57fa\u4e8eLLM\u5fae\u8c03\u7684\u65b9\u6cd5\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u6cdb\u5316\u6027\u5f3a\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\uff0c\u901a\u8fc7\u77e5\u8bc6\u5e93\u76f4\u63a5\u4e3aLLM\u63d0\u4f9b\u5df2\u77e5\u7684\u671f\u671b\u548c\u4e0d\u671f\u671b\u8fc7\u7a0b\u884c\u4e3a\uff0c\u907f\u514d\u5fae\u8c03\uff0c\u5e76\u5229\u7528\u4e8b\u4ef6\u65e5\u5fd7\u4e2d\u7684\u4e0a\u4e0b\u6587\uff08\u5982\u9891\u7e41\u8f68\u8ff9\u548c\u6d3b\u52a8\uff09\u589e\u5f3a\u6548\u679c\u3002", "result": "\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u4e0d\u671f\u671b\u884c\u4e3a\u4e0a\u4f18\u4e8e\u5fae\u8c03LLM\uff0c\u8bc1\u660e\u4e86RAG\u662f\u8d44\u6e90\u5bc6\u96c6\u5fae\u8c03\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "RAG\u65b9\u6cd5\u65e0\u9700\u4e13\u7528\u6a21\u578b\u6216\u5fae\u8c03\uff0c\u5373\u53ef\u9ad8\u6548\u68c0\u6d4b\u4e0d\u671f\u671b\u884c\u4e3a\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u7ed3\u5408\u4e8b\u4ef6\u65e5\u5fd7\u4e0a\u4e0b\u6587\u7684\u573a\u666f\u3002", "keywords": "\u4e00\u81f4\u6027\u68c0\u67e5\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3001\u8fc7\u7a0b\u6316\u6398\u3001\u4e8b\u4ef6\u65e5\u5fd7"}}
{"id": "2505.22296", "pdf": "https://arxiv.org/pdf/2505.22296", "abs": "https://arxiv.org/abs/2505.22296", "authors": ["Haosheng Zou", "Xiaowei Lv", "Shousheng Jia", "Xiangzheng Zhang"], "title": "360-LLaMA-Factory: Plug & Play Sequence Parallelism for Long Post-Training", "categories": ["cs.CL", "cs.LG"], "comment": "code at https://github.com/Qihoo360/360-LLaMA-Factory", "summary": "Adding sequence parallelism into LLaMA-Factory, we open-sourced\n360-LLaMA-Factory at https://github.com/Qihoo360/360-LLaMA-Factory.\n360-LLaMA-Factory has received wide recognition and used in models such as\nLight-R1 arXiv:2503.10460, TinyR1 arXiv:2503.04872, Kaggle AIMO math models and\nalso in large companies' training frameworks. This technical report delves\ndeeper into the different sequence parallel modes behind 360-LLaMA-Factory and\ndiscusses our implementation insights.", "AI": {"tldr": "\u8fd9\u7bc7\u6280\u672f\u62a5\u544a\u4ecb\u7ecd\u4e86360-LLaMA-Factory\u5f00\u6e90\u9879\u76ee\uff0c\u901a\u8fc7\u5f15\u5165\u5e8f\u5217\u5e76\u884c\u6280\u672f\uff0c\u5176\u5e94\u7528\u5df2\u5e7f\u6cdb\u8986\u76d6\u591a\u4e2a\u6a21\u578b\u548c\u5927\u516c\u53f8\u8bad\u7ec3\u6846\u67b6\u3002", "motivation": "\u63a2\u8ba8\u4e0d\u540c\u5e8f\u5217\u5e76\u884c\u6a21\u5f0f\u7684\u6280\u672f\u7ec6\u8282\u548c\u5b9e\u73b0\u7ecf\u9a8c\uff0c\u63d0\u5347\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "method": "\u5728LLaMA-Factory\u4e2d\u5f15\u5165\u5e8f\u5217\u5e76\u884c\u6280\u672f\uff0c\u5e76\u5f00\u6e90360-LLaMA-Factory\u9879\u76ee\u3002", "result": "\u9879\u76ee\u83b7\u5f97\u5e7f\u6cdb\u5e94\u7528\uff0c\u652f\u6301Light-R1\u3001TinyR1\u3001Kaggle AIMO\u6570\u5b66\u6a21\u578b\u7b49\u591a\u6a21\u578b\u53ca\u5927\u516c\u53f8\u8bad\u7ec3\u6846\u67b6\u3002", "conclusion": "\u5e8f\u5217\u5e76\u884c\u6280\u672f\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u548c\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u5e8f\u5217\u5e76\u884c, LLaMA-Factory, \u5f00\u6e90\u9879\u76ee, \u6a21\u578b\u8bad\u7ec3"}}
{"id": "2505.22042", "pdf": "https://arxiv.org/pdf/2505.22042", "abs": "https://arxiv.org/abs/2505.22042", "authors": ["Hao Yang", "Haoxuan Li", "Mengyue Yang", "Xu Chen", "Mingming Gong"], "title": "Estimating the Effects of Sample Training Orders for Large Language Models without Retraining", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The order of training samples plays a crucial role in large language models\n(LLMs), significantly impacting both their external performance and internal\nlearning dynamics. Traditional methods for investigating this effect generally\nrequire retraining the model with various sample orders, which is\ncomputationally infeasible for LLMs. In this work, we improve traditional\nmethods by designing a retraining-free framework. By approximating Adam\noptimizer updates with first- and second-order Taylor expansions and utilizing\nrandom projection methods to store intermediate checkpoints, our framework can\nefficiently estimate model parameters for arbitrary training sample orders.\nNext, we apply our framework to two downstream research problems: (1) Training\ncurriculum design for LLMs -- we base our retraining-free framework to propose\na novel curriculum learning strategy that augments curriculum proposals with\nestimated model performances, enabling more informed sample scheduling. (2)\nLLMs' memorization and generalization effect analysis -- we use our\nretraining-free framework to estimate how the positions of training samples\ninfluence LLMs' capacity for memorization and generalization. We conduct\nextensive experiments to validate the effectiveness of our retraining-free\nframework in reproducing the true model performances, and further demonstrate\nits potential in optimizing LLM training curricula and analyzing the\nmemorization and generalization effects of LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u4f30\u8ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u6837\u672c\u987a\u5e8f\u4e0b\u6027\u80fd\u7684\u6846\u67b6\uff0c\u6539\u8fdb\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u5e76\u5e94\u7528\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u8bbe\u8ba1\u548c\u8bb0\u5fc6/\u6cdb\u5316\u6548\u5e94\u5206\u6790\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u4ee5\u7814\u7a76\u6837\u672c\u987a\u5e8f\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u9ad8\u6548\u6846\u67b6\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u8d1f\u62c5\u5e76\u63d0\u5347\u7814\u7a76\u6548\u7387\u3002", "method": "\u901a\u8fc7\u4e00\u9636\u548c\u4e8c\u9636\u6cf0\u52d2\u5c55\u5f00\u8fd1\u4f3cAdam\u4f18\u5316\u5668\u7684\u66f4\u65b0\uff0c\u5e76\u5229\u7528\u968f\u673a\u6295\u5f71\u65b9\u6cd5\u5b58\u50a8\u4e2d\u95f4\u68c0\u67e5\u70b9\uff0c\u4ece\u800c\u9ad8\u6548\u4f30\u8ba1\u4efb\u610f\u6837\u672c\u987a\u5e8f\u4e0b\u7684\u6a21\u578b\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u6846\u67b6\u5728\u91cd\u73b0\u771f\u5b9e\u6a21\u578b\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u4f18\u5316\u8bfe\u7a0b\u5b66\u4e60\u8bbe\u8ba1\u548c\u5206\u6790\u8bb0\u5fc6/\u6cdb\u5316\u6548\u5e94\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u7814\u7a76\u6837\u672c\u987a\u5e8f\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u6837\u672c\u987a\u5e8f, \u8bfe\u7a0b\u5b66\u4e60, \u8bb0\u5fc6\u6548\u5e94, \u6cdb\u5316\u6548\u5e94"}}
{"id": "2505.22298", "pdf": "https://arxiv.org/pdf/2505.22298", "abs": "https://arxiv.org/abs/2505.22298", "authors": ["Yifan Lu", "Jing Li", "Yigeng Zhou", "Yihui Zhang", "Wenya Wang", "Xiucheng Li", "Meishan Zhang", "Fangming Liu", "Jun Yu", "Min Zhang"], "title": "Adaptive Detoxification: Safeguarding General Capabilities of LLMs through Toxicity-Aware Knowledge Editing", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Large language models (LLMs) exhibit impressive language capabilities but\nremain vulnerable to malicious prompts and jailbreaking attacks. Existing\nknowledge editing methods for LLM detoxification face two major challenges.\nFirst, they often rely on entity-specific localization, making them ineffective\nagainst adversarial inputs without explicit entities. Second, these methods\nsuffer from over-editing, where detoxified models reject legitimate queries,\ncompromising overall performance. In this paper, we propose ToxEdit, a\ntoxicity-aware knowledge editing approach that dynamically detects toxic\nactivation patterns during forward propagation. It then routes computations\nthrough adaptive inter-layer pathways to mitigate toxicity effectively. This\ndesign ensures precise toxicity mitigation while preserving LLMs' general\ncapabilities. To more accurately assess over-editing, we also enhance the\nSafeEdit benchmark by incorporating instruction-following evaluation tasks.\nExperimental results on multiple LLMs demonstrate that our ToxEdit outperforms\nprevious state-of-the-art methods in both detoxification performance and\nsafeguarding general capabilities of LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aToxEdit\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u68c0\u6d4b\u548c\u51cf\u8f7b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6bd2\u6027\u5185\u5bb9\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89e3\u6bd2\u65b9\u6cd5\u5b58\u5728\u5bf9\u663e\u5f0f\u5b9e\u4f53\u7684\u4f9d\u8d56\u548c\u8fc7\u5ea6\u7f16\u8f91\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u6574\u4f53\u6027\u80fd\u3002", "method": "ToxEdit\u901a\u8fc7\u52a8\u6001\u68c0\u6d4b\u524d\u5411\u4f20\u64ad\u4e2d\u7684\u6bd2\u6027\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u5e76\u81ea\u9002\u5e94\u5730\u8c03\u6574\u8ba1\u7b97\u8def\u5f84\u6765\u6709\u6548\u51cf\u8f7b\u6bd2\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eToxEdit\u5728\u89e3\u6bd2\u6027\u80fd\u548c\u4fdd\u6301\u6a21\u578b\u901a\u7528\u80fd\u529b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ToxEdit\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7cbe\u786e\u7684\u6bd2\u6027\u51cf\u8f7b\u65b9\u6cd5\uff0c\u540c\u65f6\u907f\u514d\u4e86\u8fc7\u5ea6\u7f16\u8f91\u7684\u95ee\u9898\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u89e3\u6bd2, \u6bd2\u6027\u68c0\u6d4b, \u77e5\u8bc6\u7f16\u8f91, \u901a\u7528\u80fd\u529b"}}
{"id": "2505.22049", "pdf": "https://arxiv.org/pdf/2505.22049", "abs": "https://arxiv.org/abs/2505.22049", "authors": ["Laetitia Chapel", "Romain Tavenard", "Samuel Vaiter"], "title": "Differentiable Generalized Sliced Wasserstein Plans", "categories": ["cs.LG"], "comment": null, "summary": "Optimal Transport (OT) has attracted significant interest in the machine\nlearning community, not only for its ability to define meaningful distances\nbetween probability distributions -- such as the Wasserstein distance -- but\nalso for its formulation of OT plans. Its computational complexity remains a\nbottleneck, though, and slicing techniques have been developed to scale OT to\nlarge datasets. Recently, a novel slicing scheme, dubbed min-SWGG, lifts a\nsingle one-dimensional plan back to the original multidimensional space,\nfinally selecting the slice that yields the lowest Wasserstein distance as an\napproximation of the full OT plan. Despite its computational and theoretical\nadvantages, min-SWGG inherits typical limitations of slicing methods: (i) the\nnumber of required slices grows exponentially with the data dimension, and (ii)\nit is constrained to linear projections. Here, we reformulate min-SWGG as a\nbilevel optimization problem and propose a differentiable approximation scheme\nto efficiently identify the optimal slice, even in high-dimensional settings.\nWe furthermore define its generalized extension for accommodating to data\nliving on manifolds. Finally, we demonstrate the practical value of our\napproach in various applications, including gradient flows on manifolds and\nhigh-dimensional spaces, as well as a novel sliced OT-based conditional flow\nmatching for image generation -- where fast computation of transport plans is\nessential.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5207\u7247\u4f18\u5316\u65b9\u6848\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u548c\u53ef\u5fae\u8fd1\u4f3c\u6539\u8fdbmin-SWGG\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u6570\u636e\u548c\u591a\u6d41\u5f62\u6570\u636e\u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u5e76\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u867d\u7136\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5176\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u4ecd\u662f\u74f6\u9888\u3002\u73b0\u6709\u7684\u5207\u7247\u6280\u672f\uff08\u5982min-SWGG\uff09\u867d\u7136\u9ad8\u6548\uff0c\u4f46\u5728\u9ad8\u7ef4\u6570\u636e\u548c\u975e\u7ebf\u6027\u6295\u5f71\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u5c06min-SWGG\u91cd\u65b0\u5efa\u6a21\u4e3a\u53cc\u5c42\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u53ef\u5fae\u8fd1\u4f3c\u65b9\u6848\u4ee5\u9ad8\u6548\u627e\u5230\u6700\u4f18\u5207\u7247\u3002\u8fdb\u4e00\u6b65\u6269\u5c55\u4e86\u65b9\u6cd5\u4ee5\u652f\u6301\u591a\u6d41\u5f62\u6570\u636e\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u9ad8\u7ef4\u6570\u636e\u548c\u591a\u6d41\u5f62\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u68af\u5ea6\u6d41\u548c\u56fe\u50cf\u751f\u6210\u7b49\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u5207\u7247\u6280\u672f\u548c\u6269\u5c55\u5e94\u7528\u573a\u666f\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86OT\u5728\u5927\u89c4\u6a21\u548c\u9ad8\u7ef4\u6570\u636e\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "keywords": "\u6700\u4f18\u4f20\u8f93\u3001\u5207\u7247\u6280\u672f\u3001\u53cc\u5c42\u4f18\u5316\u3001\u9ad8\u7ef4\u6570\u636e\u3001\u591a\u6d41\u5f62"}}
{"id": "2505.21588", "pdf": "https://arxiv.org/pdf/2505.21588", "abs": "https://arxiv.org/abs/2505.21588", "authors": ["Young-Min Cho", "Sharath Chandra Guntuku", "Lyle Ungar"], "title": "Herd Behavior: Investigating Peer Influence in LLM-based Multi-Agent Systems", "categories": ["cs.MA", "cs.AI"], "comment": "Preprint", "summary": "Recent advancements in Large Language Models (LLMs) have enabled the\nemergence of multi-agent systems where LLMs interact, collaborate, and make\ndecisions in shared environments. While individual model behavior has been\nextensively studied, the dynamics of peer influence in such systems remain\nunderexplored. In this paper, we investigate herd behavior, the tendency of\nagents to align their outputs with those of their peers, within LLM-based\nmulti-agent interactions. We present a series of controlled experiments that\nreveal how herd behaviors are shaped by multiple factors. First, we show that\nthe gap between self-confidence and perceived confidence in peers significantly\nimpacts an agent's likelihood to conform. Second, we find that the format in\nwhich peer information is presented plays a critical role in modulating the\nstrength of herd behavior. Finally, we demonstrate that the degree of herd\nbehavior can be systematically controlled, and that appropriately calibrated\nherd tendencies can enhance collaborative outcomes. These findings offer new\ninsights into the social dynamics of LLM-based systems and open pathways for\ndesigning more effective and adaptive multi-agent collaboration frameworks.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u7814\u7a76\u8005\u63a2\u7d22\u4e86\u4ece\u4f17\u884c\u4e3a\u7684\u52a8\u6001\u673a\u5236\u53ca\u5176\u5f71\u54cd\u56e0\u7d20\uff0c\u5305\u62ec\u81ea\u4fe1\u4e0e\u611f\u77e5\u540c\u4f34\u4fe1\u5fc3\u7684\u5dee\u8ddd\u3001\u4fe1\u606f\u5448\u73b0\u5f62\u5f0f\u4ee5\u53ca\u63a7\u5236\u4ece\u4f17\u884c\u4e3a\u7684\u53ef\u80fd\u6027\uff0c\u63ed\u793a\u4e86\u5982\u4f55\u4f18\u5316\u534f\u4f5c\u6846\u67b6\u3002", "motivation": "\u9488\u5bf9LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u4ea4\u4e92\u884c\u4e3a\uff0c\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u5355\u4e2a\u6a21\u578b\u7684\u8868\u73b0\uff0c\u800c\u540c\u4f34\u95f4\u5f71\u54cd\u7684\u52a8\u6001\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7a76\u4ece\u4f17\u884c\u4e3a\u5728\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4e2d\u7684\u4f5c\u7528\u53ca\u5176\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u4e00\u7cfb\u5217\u53d7\u63a7\u5b9e\u9a8c\uff0c\u5206\u6790\u667a\u80fd\u4f53\u5728\u540c\u4f34\u5f71\u54cd\u4e0b\u7684\u884c\u4e3a\u8c03\u6574\u3002\u5177\u4f53\u56e0\u7d20\u5305\u62ec\u81ea\u4fe1\u4e0e\u611f\u77e5\u540c\u4f34\u4fe1\u5fc3\u7684\u5dee\u5f02\u3001\u4fe1\u606f\u5448\u73b0\u5f62\u5f0f\uff0c\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7\u6821\u51c6\u4ece\u4f17\u503e\u5411\u6765\u8c03\u63a7\u534f\u4f5c\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a\uff081\uff09\u81ea\u4fe1\u4e0e\u611f\u77e5\u540c\u4f34\u4fe1\u5fc3\u7684\u5dee\u8ddd\u663e\u8457\u5f71\u54cd\u4ece\u4f17\u6982\u7387\uff1b\uff082\uff09\u4fe1\u606f\u5448\u73b0\u5f62\u5f0f\u8c03\u63a7\u4ece\u4f17\u5f3a\u5ea6\uff1b\uff083\uff09\u7cfb\u7edf\u5316\u63a7\u5236\u7684\u4ece\u4f17\u884c\u4e3a\u53ef\u63d0\u5347\u534f\u4f5c\u6548\u679c\u3002", "conclusion": "LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u793e\u4ea4\u52a8\u6001\u4e2d\uff0c\u4ece\u4f17\u884c\u4e3a\u53ef\u901a\u8fc7\u5173\u952e\u56e0\u7d20\u8c03\u63a7\uff0c\u4e3a\u8bbe\u8ba1\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u534f\u4f5c\u6846\u67b6\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3001\u4ece\u4f17\u884c\u4e3a\u3001\u540c\u4f34\u5f71\u54cd\u3001\u534f\u4f5c\u6846\u67b6"}}
{"id": "2505.22318", "pdf": "https://arxiv.org/pdf/2505.22318", "abs": "https://arxiv.org/abs/2505.22318", "authors": ["Ishwar B Balappanawar", "Vamshi Krishna Bonagiri", "Anish R Joishy", "Manas Gaur", "Krishnaprasad Thirunarayan", "Ponnurangam Kumaraguru"], "title": "If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?", "categories": ["cs.CL", "cs.LG"], "comment": "16 pages, 5 figures", "summary": "Large Language Models (LLMs) demonstrate impressive reasoning capabilities in\nfamiliar contexts, but struggle when the context conflicts with their\nparametric knowledge. To investigate this phenomenon, we introduce\nCounterLogic, a dataset containing 1,800 examples across 9 logical schemas,\nexplicitly designed to evaluate logical reasoning through counterfactual\n(hypothetical knowledge-conflicting) scenarios. Our systematic evaluation of 11\nLLMs across 6 different datasets reveals a consistent performance degradation,\nwith accuracies dropping by 27% on average when reasoning through\ncounterfactual information. We propose Self-Segregate, a prompting method\nenabling metacognitive awareness (explicitly identifying knowledge conflicts)\nbefore reasoning. Our method dramatically narrows the average performance gaps\nfrom 27% to just 11%, while significantly increasing the overall accuracy\n(+7.5%). We discuss the implications of these findings and draw parallels to\nhuman cognitive processes, particularly on how humans disambiguate conflicting\ninformation during reasoning tasks. Our findings offer practical insights for\nunderstanding and enhancing LLMs reasoning capabilities in real-world\napplications, especially where models must logically reason independently of\ntheir factual knowledge.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86LLMs\u5728\u4e0a\u4e0b\u6587\u4e0e\u53c2\u6570\u77e5\u8bc6\u51b2\u7a81\u65f6\u7684\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u51faCounterLogic\u6570\u636e\u96c6\u548cSelf-Segregate\u63d0\u793a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u53cd\u4e8b\u5b9e\u63a8\u7406\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u77e5\u8bc6\u51b2\u7a81\u60c5\u5883\u4e0b\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\u9000\u5316\u73b0\u8c61\uff0c\u5e76\u5bfb\u6c42\u63d0\u5347\u65b9\u6cd5\u3002", "method": "\u5f15\u5165CounterLogic\u6570\u636e\u96c6\u8bc4\u4f30\u53cd\u4e8b\u5b9e\u63a8\u7406\uff0c\u63d0\u51faSelf-Segregate\u63d0\u793a\u65b9\u6cd5\u589e\u5f3a\u5143\u8ba4\u77e5\u610f\u8bc6\u3002", "result": "\u53cd\u4e8b\u5b9e\u573a\u666f\u4e0b\u6a21\u578b\u51c6\u786e\u7387\u5e73\u5747\u4e0b\u964d27%\uff0c\u4f46\u65b0\u65b9\u6cd5\u5c06\u5dee\u8ddd\u7f29\u5c0f\u81f311%\u5e76\u63d0\u5347\u603b\u51c6\u786e\u73877.5%\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3\u548c\u589e\u5f3aLLMs\u5728\u77e5\u8bc6\u72ec\u7acb\u63a8\u7406\u4e2d\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u5b9e\u8df5\u542f\u793a\u3002", "keywords": "\u903b\u8f91\u63a8\u7406\uff1b\u53cd\u4e8b\u5b9e\u573a\u666f\uff1bLLMs\uff1b\u77e5\u8bc6\u51b2\u7a81\uff1b\u63d0\u793a\u65b9\u6cd5"}}
{"id": "2505.22074", "pdf": "https://arxiv.org/pdf/2505.22074", "abs": "https://arxiv.org/abs/2505.22074", "authors": ["Co\u015fku Can Horuz", "Geoffrey Kasenbacher", "Saya Higuchi", "Sebastian Kairat", "Jendrik Stoltz", "Moritz Pesl", "Bernhard A. Moser", "Christoph Linse", "Thomas Martinetz", "Sebastian Otte"], "title": "The Resurrection of the ReLU", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Modeling sophisticated activation functions within deep learning\narchitectures has evolved into a distinct research direction. Functions such as\nGELU, SELU, and SiLU offer smooth gradients and improved convergence\nproperties, making them popular choices in state-of-the-art models. Despite\nthis trend, the classical ReLU remains appealing due to its simplicity,\ninherent sparsity, and other advantageous topological characteristics. However,\nReLU units are prone to becoming irreversibly inactive - a phenomenon known as\nthe dying ReLU problem - which limits their overall effectiveness. In this\nwork, we introduce surrogate gradient learning for ReLU (SUGAR) as a novel,\nplug-and-play regularizer for deep architectures. SUGAR preserves the standard\nReLU function during the forward pass but replaces its derivative in the\nbackward pass with a smooth surrogate that avoids zeroing out gradients. We\ndemonstrate that SUGAR, when paired with a well-chosen surrogate function,\nsubstantially enhances generalization performance over convolutional network\narchitectures such as VGG-16 and ResNet-18, providing sparser activations while\neffectively resurrecting dead ReLUs. Moreover, we show that even in modern\narchitectures like Conv2NeXt and Swin Transformer - which typically employ GELU\n- substituting these with SUGAR yields competitive and even slightly superior\nperformance. These findings challenge the prevailing notion that advanced\nactivation functions are necessary for optimal performance. Instead, they\nsuggest that the conventional ReLU, particularly with appropriate gradient\nhandling, can serve as a strong, versatile revived classic across a broad range\nof deep learning vision models.", "AI": {"tldr": "\u63d0\u51fa\u4e86SUGAR\u65b9\u6cd5\uff0c\u901a\u8fc7\u66ff\u4ee3\u68af\u5ea6\u5b66\u4e60\u4f18\u5316ReLU\u6fc0\u6d3b\u51fd\u6570\uff0c\u89e3\u51b3\u5176\u2018\u6b7b\u4ea1ReLU\u2019\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u5728\u591a\u79cd\u73b0\u4ee3\u67b6\u6784\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5c3d\u7ba1ReLU\u56e0\u5176\u7b80\u5355\u6027\u548c\u7a00\u758f\u6027\u53d7\u6b22\u8fce\uff0c\u4f46\u5176\u2018\u6b7b\u4ea1ReLU\u2019\u95ee\u9898\u9650\u5236\u4e86\u6548\u679c\u3002SUGAR\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u68af\u5ea6\u5904\u7406\u4fdd\u7559ReLU\u4f18\u52bf\u5e76\u63d0\u5347\u6027\u80fd\u3002", "method": "SUGAR\u5728\u524d\u5411\u4f20\u64ad\u4e2d\u4fdd\u7559\u6807\u51c6ReLU\uff0c\u4f46\u5728\u53cd\u5411\u4f20\u64ad\u4e2d\u4f7f\u7528\u5e73\u6ed1\u66ff\u4ee3\u68af\u5ea6\u907f\u514d\u68af\u5ea6\u6d88\u5931\uff0c\u7ed3\u5408\u5408\u9002\u7684\u66ff\u4ee3\u51fd\u6570\u4f18\u5316\u8bad\u7ec3\u3002", "result": "\u5728VGG-16\u3001ResNet-18\u7b49\u67b6\u6784\u4e2d\uff0cSUGAR\u663e\u8457\u63d0\u5347\u6cdb\u5316\u6027\u80fd\uff0c\u5e76\u5728Conv2NeXt\u548cSwin Transformer\u4e2d\u8868\u73b0\u7ade\u4e89\u529b\uff0c\u751a\u81f3\u7565\u4f18\u4e8eGELU\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u9002\u5f53\u68af\u5ea6\u5904\u7406\u7684ReLU\u4ecd\u80fd\u4f5c\u4e3a\u9ad8\u6548\u4e14\u901a\u7528\u7684\u6fc0\u6d3b\u51fd\u6570\uff0c\u6311\u6218\u4e86\u9ad8\u7ea7\u6fc0\u6d3b\u51fd\u6570\u7684\u5fc5\u8981\u6027\u3002", "keywords": "SUGAR, ReLU, \u66ff\u4ee3\u68af\u5ea6, \u6df1\u5ea6\u5b66\u4e60, \u6fc0\u6d3b\u51fd\u6570"}}
{"id": "2505.21589", "pdf": "https://arxiv.org/pdf/2505.21589", "abs": "https://arxiv.org/abs/2505.21589", "authors": ["Carina Newen", "Luca Hinkamp", "Maria Ntonti", "Emmanuel M\u00fcller"], "title": "Do you see what I see? An Ambiguous Optical Illusion Dataset exposing limitations of Explainable AI", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "19 pages, 18 figures", "summary": "From uncertainty quantification to real-world object detection, we recognize\nthe importance of machine learning algorithms, particularly in safety-critical\ndomains such as autonomous driving or medical diagnostics. In machine learning,\nambiguous data plays an important role in various machine learning domains.\nOptical illusions present a compelling area of study in this context, as they\noffer insight into the limitations of both human and machine perception.\nDespite this relevance, optical illusion datasets remain scarce. In this work,\nwe introduce a novel dataset of optical illusions featuring intermingled animal\npairs designed to evoke perceptual ambiguity. We identify generalizable visual\nconcepts, particularly gaze direction and eye cues, as subtle yet impactful\nfeatures that significantly influence model accuracy. By confronting models\nwith perceptual ambiguity, our findings underscore the importance of concepts\nin visual learning and provide a foundation for studying bias and alignment\nbetween human and machine vision. To make this dataset useful for general\npurposes, we generate optical illusions systematically with different concepts\ndiscussed in our bias mitigation section. The dataset is accessible in Kaggle\nvia\nhttps://kaggle.com/datasets/693bf7c6dd2cb45c8a863f9177350c8f9849a9508e9d50526e2ffcc5559a8333.\nOur source code can be found at\nhttps://github.com/KDD-OpenSource/Ambivision.git.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5149\u5b66\u5e7b\u89c9\u6570\u636e\u96c6\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u89c6\u89c9\u5b66\u4e60\u4e2d\u7684\u6982\u5ff5\uff08\u5982\u51dd\u89c6\u65b9\u5411\u548c\u773c\u775b\u7ebf\u7d22\uff09\u5bf9\u6a21\u578b\u51c6\u786e\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u4e3a\u7814\u7a76\u4eba\u7c7b\u4e0e\u673a\u5668\u89c6\u89c9\u4e4b\u95f4\u7684\u504f\u5dee\u548c\u5bf9\u9f50\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "motivation": "\u5728\u81ea\u52a8\u9a7e\u9a76\u6216\u533b\u7597\u8bca\u65ad\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\uff0c\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u548c\u8bc6\u522b\u6a21\u7cca\u6570\u636e\u5bf9\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u81f3\u5173\u91cd\u8981\u3002\u5149\u5b66\u5e7b\u89c9\u662f\u7814\u7a76\u4eba\u7c7b\u548c\u673a\u5668\u611f\u77e5\u5c40\u9650\u6027\u7684\u91cd\u8981\u9886\u57df\uff0c\u4f46\u76f8\u5173\u6570\u636e\u96c6\u7a00\u7f3a\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u751f\u6210\u5305\u542b\u52a8\u7269\u5bf9\u7684\u5149\u5b66\u5e7b\u89c9\u6570\u636e\u96c6\uff0c\u7814\u7a76\u89c6\u89c9\u6982\u5ff5\uff08\u5982\u51dd\u89c6\u65b9\u5411\u548c\u773c\u775b\u7ebf\u7d22\uff09\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u516c\u5f00\u6570\u636e\u96c6\u548c\u6e90\u4ee3\u7801\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u89c6\u89c9\u6982\u5ff5\uff08\u5982\u51dd\u89c6\u65b9\u5411\u548c\u773c\u775b\u7ebf\u7d22\uff09\u5bf9\u6a21\u578b\u51c6\u786e\u6027\u6709\u663e\u8457\u5f71\u54cd\uff0c\u7a81\u51fa\u4e86\u89c6\u89c9\u5b66\u4e60\u4e2d\u6982\u5ff5\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u673a\u5668\u89c6\u89c9\u4e2d\u7684\u504f\u5dee\u548c\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u901a\u8fc7\u516c\u5f00\u6570\u636e\u96c6\u548c\u4ee3\u7801\u63a8\u52a8\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "keywords": "\u5149\u5b66\u5e7b\u89c9, \u673a\u5668\u5b66\u4e60, \u89c6\u89c9\u5b66\u4e60, \u6570\u636e\u96c6, \u504f\u5dee\u5bf9\u9f50"}}
{"id": "2505.22323", "pdf": "https://arxiv.org/pdf/2505.22323", "abs": "https://arxiv.org/abs/2505.22323", "authors": ["Hongcan Guo", "Haolang Lu", "Guoshun Nan", "Bolun Chu", "Jialin Zhuang", "Yuan Yang", "Wenhao Che", "Sicong Leng", "Qimei Cui", "Xudong Jiang"], "title": "Advancing Expert Specialization for Better MoE", "categories": ["cs.CL", "cs.SE", "68T07", "I.2.7"], "comment": "33pages, 6figures", "summary": "Mixture-of-Experts (MoE) models enable efficient scaling of large language\nmodels (LLMs) by activating only a subset of experts per input. However, we\nobserve that the commonly used auxiliary load balancing loss often leads to\nexpert overlap and overly uniform routing, which hinders expert specialization\nand degrades overall performance during post-training. To address this, we\npropose a simple yet effective solution that introduces two complementary\nobjectives: (1) an orthogonality loss to encourage experts to process distinct\ntypes of tokens, and (2) a variance loss to encourage more discriminative\nrouting decisions. Gradient-level analysis demonstrates that these objectives\nare compatible with the existing auxiliary loss and contribute to optimizing\nthe training process. Experimental results over various model architectures and\nacross multiple benchmarks show that our method significantly enhances expert\nspecialization. Notably, our method improves classic MoE baselines with\nauxiliary loss by up to 23.79%, while also maintaining load balancing in\ndownstream tasks, without any architectural modifications or additional\ncomponents. We will release our code to contribute to the community.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdbMoE\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u6b63\u4ea4\u6027\u548c\u65b9\u5dee\u635f\u5931\u6765\u63d0\u5347\u4e13\u5bb6\u4e13\u4e1a\u5316\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684MoE\u6a21\u578b\u4e2d\u8f85\u52a9\u8d1f\u8f7d\u5e73\u8861\u635f\u5931\u5bfc\u81f4\u4e13\u5bb6\u91cd\u53e0\u548c\u8def\u7531\u8fc7\u4e8e\u5747\u5300\uff0c\u9650\u5236\u4e86\u4e13\u5bb6\u4e13\u4e1a\u5316\u5e76\u964d\u4f4e\u6027\u80fd\u3002", "method": "\u5f15\u5165\u4e86\u6b63\u4ea4\u6027\u635f\u5931\u548c\u65b9\u5dee\u635f\u5931\uff0c\u524d\u8005\u4fc3\u4f7f\u4e13\u5bb6\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u7684\u4ee4\u724c\uff0c\u540e\u8005\u4fc3\u8fdb\u66f4\u5177\u533a\u5206\u6027\u7684\u8def\u7531\u51b3\u7b56\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u67b6\u6784\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4e13\u5bb6\u4e13\u4e1a\u5316\uff0c\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe23.79%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u6539\u53d8\u67b6\u6784\u6216\u589e\u52a0\u7ec4\u4ef6\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u4f18\u5316\u4e86MoE\u6a21\u578b\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5e76\u4fdd\u6301\u4e86\u8d1f\u8f7d\u5e73\u8861\u3002", "keywords": "Mixture-of-Experts, expert specialization, orthogonality loss, variance loss, large language models"}}
{"id": "2505.22081", "pdf": "https://arxiv.org/pdf/2505.22081", "abs": "https://arxiv.org/abs/2505.22081", "authors": ["Shun Sato", "Issei Sato"], "title": "Can Test-time Computation Mitigate Memorization Bias in Neural Symbolic Regression?", "categories": ["cs.LG"], "comment": null, "summary": "Symbolic regression aims to discover mathematical equations that fit given\nnumerical data. It has been applied in various fields of scientific research,\nsuch as producing human-readable expressions that explain physical phenomena.\nRecently, Neural symbolic regression (NSR) methods that involve Transformers\npre-trained on large-scale synthetic datasets have gained attention. While\nthese methods offer advantages such as short inference time, they suffer from\nlow performance, particularly when the number of input variables is large. In\nthis study, we hypothesized that this limitation stems from the memorization\nbias of Transformers in symbolic regression. We conducted a quantitative\nevaluation of this bias in Transformers using a synthetic dataset and found\nthat Transformers rarely generate expressions not present in the training data.\nAdditional theoretical analysis reveals that this bias arises from the\nTransformer's inability to construct expressions compositionally while\nverifying their numerical validity. We finally examined if tailoring test-time\nstrategies can lead to reduced memorization bias and better performance. We\nempirically demonstrate that providing additional information to the model at\ntest time can significantly mitigate memorization bias. On the other hand, we\nalso find that reducing memorization bias does not necessarily correlate with\nimproved performance. These findings contribute to a deeper understanding of\nthe limitations of NSR approaches and offer a foundation for designing more\nrobust, generalizable symbolic regression methods. Code is available at\nhttps://github.com/Shun-0922/Mem-Bias-NSR .", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u795e\u7ecf\u7b26\u53f7\u56de\u5f52\uff08NSR\uff09\u65b9\u6cd5\u4e2dTransformer\u6a21\u578b\u7684\u8bb0\u5fc6\u504f\u5dee\u95ee\u9898\uff0c\u53d1\u73b0\u5176\u5728\u5904\u7406\u591a\u53d8\u91cf\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u5e76\u63a2\u7d22\u4e86\u6d4b\u8bd5\u65f6\u7b56\u7565\u4ee5\u51cf\u5c11\u8fd9\u79cd\u504f\u5dee\u3002", "motivation": "\u63a2\u8ba8NSR\u65b9\u6cd5\u4e2dTransformer\u6a21\u578b\u7684\u8bb0\u5fc6\u504f\u5dee\u5982\u4f55\u5f71\u54cd\u5176\u5728\u7b26\u53f7\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5728\u8f93\u5165\u53d8\u91cf\u8f83\u591a\u65f6\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u5b9a\u91cf\u8bc4\u4f30Transformer\u7684\u8bb0\u5fc6\u504f\u5dee\uff0c\u5e76\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u9a8c\u8bc1\u5176\u5728\u7ec4\u5408\u6784\u5efa\u8868\u8fbe\u5f0f\u65f6\u7684\u5c40\u9650\u6027\u3002\u8fdb\u4e00\u6b65\u6d4b\u8bd5\u4e86\u6d4b\u8bd5\u65f6\u7b56\u7565\u5bf9\u51cf\u5c11\u504f\u5dee\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0Transformer\u5f88\u5c11\u751f\u6210\u8bad\u7ec3\u6570\u636e\u4e2d\u672a\u51fa\u73b0\u7684\u8868\u8fbe\u5f0f\uff0c\u4e14\u6d4b\u8bd5\u65f6\u63d0\u4f9b\u989d\u5916\u4fe1\u606f\u53ef\u663e\u8457\u51cf\u5c11\u8bb0\u5fc6\u504f\u5dee\uff0c\u4f46\u51cf\u5c11\u504f\u5dee\u5e76\u4e0d\u4e00\u5b9a\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u6df1\u5316\u4e86\u5bf9NSR\u65b9\u6cd5\u5c40\u9650\u6027\u7684\u7406\u89e3\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u9c81\u68d2\u3001\u901a\u7528\u7684\u7b26\u53f7\u56de\u5f52\u65b9\u6cd5\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "keywords": "\u7b26\u53f7\u56de\u5f52, \u795e\u7ecf\u7b26\u53f7\u56de\u5f52, Transformer, \u8bb0\u5fc6\u504f\u5dee, \u6d4b\u8bd5\u65f6\u7b56\u7565"}}
{"id": "2505.22327", "pdf": "https://arxiv.org/pdf/2505.22327", "abs": "https://arxiv.org/abs/2505.22327", "authors": ["Antonia Karamolegkou", "Angana Borah", "Eunjung Cho", "Sagnik Ray Choudhury", "Martina Galletti", "Rajarshi Ghosh", "Pranav Gupta", "Oana Ignat", "Priyanka Kargupta", "Neema Kotonya", "Hemank Lamba", "Sun-Joo Lee", "Arushi Mangla", "Ishani Mondal", "Deniz Nazarova", "Poli Nemkova", "Dina Pisarevskaya", "Naquee Rizwan", "Nazanin Sabri", "Dominik Stammbach", "Anna Steinberg", "David Tom\u00e1s", "Steven R Wilson", "Bowen Yi", "Jessica H Zhu", "Arkaitz Zubiaga", "Anders S\u00f8gaard", "Alexander Fraser", "Zhijing Jin", "Rada Mihalcea", "Joel R. Tetreault", "Daryna Dementieva"], "title": "NLP for Social Good: A Survey of Challenges, Opportunities, and Responsible Deployment", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have unlocked\nunprecedented possibilities across a range of applications. However, as a\ncommunity, we believe that the field of Natural Language Processing (NLP) has a\ngrowing need to approach deployment with greater intentionality and\nresponsibility. In alignment with the broader vision of AI for Social Good\n(Toma\\v{s}ev et al., 2020), this paper examines the role of NLP in addressing\npressing societal challenges. Through a cross-disciplinary analysis of social\ngoals and emerging risks, we highlight promising research directions and\noutline challenges that must be addressed to ensure responsible and equitable\nprogress in NLP4SG research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u9886\u57df\u66f4\u8d1f\u8d23\u4efb\u5730\u5e94\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u4ee5\u89e3\u51b3\u793e\u4f1a\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0cNLP\u9886\u57df\u9700\u8981\u66f4\u8c28\u614e\u5730\u90e8\u7f72\u6280\u672f\uff0c\u4ee5\u4fc3\u8fdb\u793e\u4f1a\u516c\u76ca\u5e76\u5e94\u5bf9\u6f5c\u5728\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u8de8\u5b66\u79d1\u5206\u6790\u793e\u4f1a\u76ee\u6807\u4e0e\u65b0\u5174\u98ce\u9669\uff0c\u63d0\u51fa\u7814\u7a76\u65b9\u5411\u3002", "result": "\u660e\u786e\u4e86NLP4SG\u7814\u7a76\u7684\u6f5c\u5728\u65b9\u5411\u4e0e\u9700\u89e3\u51b3\u7684\u6311\u6218\u3002", "conclusion": "NLP\u9886\u57df\u9700\u4ee5\u8d23\u4efb\u4e0e\u516c\u5e73\u4e3a\u6838\u5fc3\uff0c\u63a8\u52a8\u6280\u672f\u4e3a\u793e\u4f1a\u670d\u52a1\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, NLP4SG, \u793e\u4f1a\u8d23\u4efb, \u8de8\u5b66\u79d1\u5206\u6790, \u516c\u5e73\u6027"}}
{"id": "2505.22108", "pdf": "https://arxiv.org/pdf/2505.22108", "abs": "https://arxiv.org/abs/2505.22108", "authors": ["Santhosh Parampottupadam", "Melih Co\u015f\u011fun", "Sarthak Pati", "Maximilian Zenk", "Saikat Roy", "Dimitrios Bounias", "Benjamin Hamm", "Sinem Sav", "Ralf Floca", "Klaus Maier-Hein"], "title": "Inclusive, Differentially Private Federated Learning for Clinical Data", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "comment": null, "summary": "Federated Learning (FL) offers a promising approach for training clinical AI\nmodels without centralizing sensitive patient data. However, its real-world\nadoption is hindered by challenges related to privacy, resource constraints,\nand compliance. Existing Differential Privacy (DP) approaches often apply\nuniform noise, which disproportionately degrades model performance, even among\nwell-compliant institutions. In this work, we propose a novel compliance-aware\nFL framework that enhances DP by adaptively adjusting noise based on\nquantifiable client compliance scores. Additionally, we introduce a compliance\nscoring tool based on key healthcare and security standards to promote secure,\ninclusive, and equitable participation across diverse clinical settings.\nExtensive experiments on public datasets demonstrate that integrating\nunder-resourced, less compliant clinics with highly regulated institutions\nyields accuracy improvements of up to 15% over traditional FL. This work\nadvances FL by balancing privacy, compliance, and performance, making it a\nviable solution for real-world clinical workflows in global healthcare.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5408\u89c4\u611f\u77e5\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u566a\u58f0\u6765\u63d0\u9ad8\u9690\u79c1\u4fdd\u62a4\uff0c\u540c\u65f6\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4f18\u4e8e\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u9762\u4e34\u9690\u79c1\u3001\u8d44\u6e90\u9650\u5236\u548c\u5408\u89c4\u6027\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u7edf\u4e00\u7684\u5dee\u5206\u9690\u79c1\u65b9\u6cd5\u4f1a\u4e0d\u516c\u5e73\u5730\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u5408\u89c4\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165\u5408\u89c4\u611f\u77e5\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u57fa\u4e8e\u5ba2\u6237\u7aef\u5408\u89c4\u8bc4\u5206\u52a8\u6001\u8c03\u6574\u5dee\u5206\u9690\u79c1\u566a\u58f0\uff0c\u5e76\u5f00\u53d1\u5408\u89c4\u8bc4\u5206\u5de5\u5177\u4ee5\u8bc4\u4f30\u533b\u7597\u5b89\u5168\u6807\u51c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6846\u67b6\u5728\u6574\u5408\u8d44\u6e90\u532e\u4e4f\u548c\u4f4e\u5408\u89c4\u8bca\u6240\u65f6\uff0c\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u534715%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5e73\u8861\u4e86\u9690\u79c1\u3001\u5408\u89c4\u4e0e\u6027\u80fd\uff0c\u4e3a\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u8054\u90a6\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60,\u5dee\u5206\u9690\u79c1,\u5408\u89c4\u8bc4\u5206,\u533b\u7597AI,\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b"}}
{"id": "2505.21593", "pdf": "https://arxiv.org/pdf/2505.21593", "abs": "https://arxiv.org/abs/2505.21593", "authors": ["Yang Yang", "Siming Zheng", "Jinwei Chen", "Boxi Wu", "Xiaofei He", "Deng Cai", "Bo Li", "Peng-Tao Jiang"], "title": "Any-to-Bokeh: One-Step Video Bokeh via Multi-Plane Image Guided Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": "project page: https://vivocameraresearch.github.io/any2bokeh/", "summary": "Recent advances in diffusion based editing models have enabled realistic\ncamera simulation and image-based bokeh, but video bokeh remains largely\nunexplored. Existing video editing models cannot explicitly control focus\nplanes or adjust bokeh intensity, limiting their applicability for controllable\noptical effects. Moreover, naively extending image-based bokeh methods to video\noften results in temporal flickering and unsatisfactory edge blur transitions\ndue to the lack of temporal modeling and generalization capability. To address\nthese challenges, we propose a novel one-step video bokeh framework that\nconverts arbitrary input videos into temporally coherent, depth-aware bokeh\neffects. Our method leverages a multi-plane image (MPI) representation\nconstructed through a progressively widening depth sampling function, providing\nexplicit geometric guidance for depth-dependent blur synthesis. By conditioning\na single-step video diffusion model on MPI layers and utilizing the strong 3D\npriors from pre-trained models such as Stable Video Diffusion, our approach\nachieves realistic and consistent bokeh effects across diverse scenes.\nAdditionally, we introduce a progressive training strategy to enhance temporal\nconsistency, depth robustness, and detail preservation. Extensive experiments\ndemonstrate that our method produces high-quality, controllable bokeh effects\nand achieves state-of-the-art performance on multiple evaluation benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u6b65\u89c6\u9891\u80cc\u666f\u865a\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5e73\u9762\u56fe\u50cf\uff08MPI\uff09\u8868\u793a\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u76843D\u5148\u9a8c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u9891\u7f16\u8f91\u6a21\u578b\u5728\u7126\u70b9\u63a7\u5236\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u65e0\u6cd5\u660e\u786e\u63a7\u5236\u7126\u70b9\u5e73\u9762\u6216\u8c03\u6574\u865a\u5316\u5f3a\u5ea6\uff0c\u4e14\u56fe\u50cf\u65b9\u6cd5\u6269\u5c55\u5230\u89c6\u9891\u65f6\u4f1a\u51fa\u73b0\u65f6\u95f4\u95ea\u70c1\u548c\u8fb9\u7f18\u6a21\u7cca\u8fc7\u6e21\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u4e00\u81f4\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528MPI\u8868\u793a\uff0c\u7ed3\u5408\u6e10\u8fdb\u52a0\u5bbd\u7684\u6df1\u5ea6\u91c7\u6837\u51fd\u6570\u548c\u5355\u6b65\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982Stable Video Diffusion\uff09\u76843D\u5148\u9a8c\u5b9e\u73b0\u6df1\u5ea6\u611f\u77e5\u7684\u865a\u5316\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u63a7\u7684\u865a\u5316\u6548\u679c\uff0c\u5e76\u5728\u591a\u4e2a\u8bc4\u4f30\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u9891\u80cc\u666f\u865a\u5316\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u6df1\u5ea6\u63a7\u5236\u95ee\u9898\uff0c\u4e3a\u5149\u5b66\u6548\u679c\u7f16\u8f91\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u89c6\u9891\u7f16\u8f91, \u80cc\u666f\u865a\u5316, \u6269\u6563\u6a21\u578b, \u591a\u5e73\u9762\u56fe\u50cf, \u65f6\u95f4\u4e00\u81f4\u6027"}}
{"id": "2505.22334", "pdf": "https://arxiv.org/pdf/2505.22334", "abs": "https://arxiv.org/abs/2505.22334", "authors": ["Lai Wei", "Yuting Li", "Kaipeng Zheng", "Chen Wang", "Yue Wang", "Linghe Kong", "Lichao Sun", "Weiran Huang"], "title": "Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have demonstrated\nimpressive chain-of-thought reasoning capabilities, with reinforcement learning\n(RL) playing a crucial role in this progress. While \"aha moment\"\npatterns--where models exhibit self-correction through reflection--are often\nattributed to emergent properties from RL, we first demonstrate that these\npatterns exist in multimodal LLMs (MLLMs) prior to RL training but may not\nnecessarily correlate with improved reasoning performance. Building on these\ninsights, we present a comprehensive study on enhancing multimodal reasoning\nthrough a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start\nwith structured chain-of-thought reasoning patterns, followed by (2)\nreinforcement learning via GRPO to further refine these capabilities. Our\nextensive experiments show that this combined approach consistently outperforms\nboth SFT-only and RL-only methods across challenging multimodal reasoning\nbenchmarks. The resulting models achieve state-of-the-art performance among\nopen-source MLLMs at both 3B and 7B scales, with our 7B model showing\nsubstantial improvements over base models (e.g., 66.3 %$\\rightarrow$73.4 % on\nMathVista, 62.9 %$\\rightarrow$70.4 % on We-Math) and our 3B model achieving\nperformance competitive with several 7B models. Overall, this work provides\npractical guidance for building advanced multimodal reasoning models. Our code\nis available at https://github.com/waltonfuture/RL-with-Cold-Start.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u5982\u4f55\u5728\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\uff08\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff09\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u5353\u8d8a\u6210\u679c\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u8ba8\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u2018\u987f\u609f\u2019\u6a21\u5f0f\u662f\u5426\u4ec5\u7531\u5f3a\u5316\u5b66\u4e60\u5f15\u53d1\uff0c\u5e76\u63d0\u51fa\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u4ee5\u4f18\u5316\u63a8\u7406\u6027\u80fd\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4f5c\u4e3a\u51b7\u542f\u52a8\uff1b2\uff09\u901a\u8fc7GRPO\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u4ee5\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u7ed3\u5408\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528SFT\u6216RL\u7684\u65b9\u6cd5\uff0c3B\u548c7B\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\u3002", "conclusion": "\u8bba\u6587\u4e3a\u6784\u5efa\u5148\u8fdb\u7684\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u4ee3\u7801\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u76d1\u7763\u5fae\u8c03, \u5f3a\u5316\u5b66\u4e60, \u63a8\u7406\u80fd\u529b, \u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2505.22109", "pdf": "https://arxiv.org/pdf/2505.22109", "abs": "https://arxiv.org/abs/2505.22109", "authors": ["Paul Krzakala", "Gabriel Melo", "Charlotte Laclau", "Florence d'Alch\u00e9-Buc", "R\u00e9mi Flamary"], "title": "The quest for the GRAph Level autoEncoder (GRALE)", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Although graph-based learning has attracted a lot of attention, graph\nrepresentation learning is still a challenging task whose resolution may impact\nkey application fields such as chemistry or biology. To this end, we introduce\nGRALE, a novel graph autoencoder that encodes and decodes graphs of varying\nsizes into a shared embedding space. GRALE is trained using an Optimal\nTransport-inspired loss that compares the original and reconstructed graphs and\nleverages a differentiable node matching module, which is trained jointly with\nthe encoder and decoder. The proposed attention-based architecture relies on\nEvoformer, the core component of AlphaFold, which we extend to support both\ngraph encoding and decoding. We show, in numerical experiments on simulated and\nmolecular data, that GRALE enables a highly general form of pre-training,\napplicable to a wide range of downstream tasks, from classification and\nregression to more complex tasks such as graph interpolation, editing,\nmatching, and prediction.", "AI": {"tldr": "GRALE \u662f\u4e00\u79cd\u65b0\u578b\u56fe\u81ea\u52a8\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u635f\u5931\u548c\u53ef\u5fae\u5206\u8282\u70b9\u5339\u914d\u6a21\u5757\u8bad\u7ec3\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002", "motivation": "\u56fe\u8868\u793a\u5b66\u4e60\u662f\u4e00\u4e2a\u6311\u6218\u6027\u4efb\u52a1\uff0c\u5bf9\u5316\u5b66\u548c\u751f\u7269\u5b66\u7b49\u9886\u57df\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u67b6\u6784\uff0c\u6269\u5c55 Evoformer \u652f\u6301\u56fe\u7f16\u7801\u548c\u89e3\u7801\uff0c\u4f7f\u7528\u6700\u4f18\u4f20\u8f93\u635f\u5931\u548c\u53ef\u5fae\u5206\u8282\u70b9\u5339\u914d\u6a21\u5757\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u6a21\u62df\u548c\u5206\u5b50\u6570\u636e\u5b9e\u9a8c\u4e2d\uff0cGRALE \u8868\u73b0\u51fa\u9ad8\u5ea6\u901a\u7528\u6027\uff0c\u9002\u7528\u4e8e\u5206\u7c7b\u3001\u56de\u5f52\u3001\u56fe\u63d2\u503c\u3001\u7f16\u8f91\u3001\u5339\u914d\u548c\u9884\u6d4b\u7b49\u4efb\u52a1\u3002", "conclusion": "GRALE \u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u56fe\u5b66\u4e60\u4efb\u52a1\u3002", "keywords": "graph representation learning, autoencoder, optimal transport, Evoformer, molecular data"}}
{"id": "2505.21594", "pdf": "https://arxiv.org/pdf/2505.21594", "abs": "https://arxiv.org/abs/2505.21594", "authors": ["Yeshwanth Venkatesha", "Souvik Kundu", "Priyadarshini Panda"], "title": "Fast and Cost-effective Speculative Edge-Cloud Decoding with Early Exits", "categories": ["cs.RO", "cs.AI", "cs.DC"], "comment": null, "summary": "Large Language Models (LLMs) enable various applications on edge devices such\nas smartphones, wearables, and embodied robots. However, their deployment often\ndepends on expensive cloud-based APIs, creating high operational costs, which\nlimit access for smaller organizations and raise sustainability concerns.\nCertain LLMs can be deployed on-device, offering a cost-effective solution with\nreduced latency and improved privacy. Yet, limited computing resources\nconstrain the size and accuracy of models that can be deployed, necessitating a\ncollaborative design between edge and cloud. We propose a fast and\ncost-effective speculative edge-cloud decoding framework with a large target\nmodel on the server and a small draft model on the device. By introducing early\nexits in the target model, tokens are generated mid-verification, allowing the\nclient to preemptively draft subsequent tokens before final verification, thus\nutilizing idle time and enhancing parallelism between edge and cloud. Using an\nNVIDIA Jetson Nano (client) and an A100 GPU (server) with Vicuna-68M (draft)\nand Llama2-7B (target) models, our method achieves up to a 35% reduction in\nlatency compared to cloud-based autoregressive decoding, with an additional 11%\nimprovement from preemptive drafting. To demonstrate real-world applicability,\nwe deploy our method on the Unitree Go2 quadruped robot using Vision-Language\nModel (VLM) based control, achieving a 21% speedup over traditional cloud-based\nautoregressive decoding. These results demonstrate the potential of our\nframework for real-time LLM and VLM applications on resource-constrained edge\ndevices.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fb9\u7f18-\u4e91\u534f\u540c\u7684\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5c0f\u578b\u8349\u7a3f\u6a21\u578b\u548c\u4e91\u7aef\u7684\u5927\u578b\u76ee\u6807\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u548c\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u56e0\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u5bfc\u81f4\u7684\u9ad8\u5ef6\u8fdf\u548c\u9ad8\u6210\u672c\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8fb9\u7f18-\u4e91\u534f\u540c\u89e3\u7801\u6846\u67b6\uff0c\u8fb9\u7f18\u8bbe\u5907\u8fd0\u884c\u5c0f\u578b\u8349\u7a3f\u6a21\u578b\uff0c\u4e91\u7aef\u8fd0\u884c\u5927\u578b\u76ee\u6807\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u65e9\u671f\u9000\u51fa\u673a\u5236\u4ee5\u63d0\u5347\u5e76\u884c\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4e91\u7aef\u81ea\u56de\u5f52\u89e3\u7801\u5ef6\u8fdf\u964d\u4f4e35%\uff0c\u9884\u8d77\u8349\u673a\u5236\u989d\u5916\u63d0\u534711%\u6027\u80fd\uff0c\u5e76\u5728\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u5b9e\u73b021%\u7684\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0aLLM\u548cVLM\u7684\u5b9e\u65f6\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u8fb9\u7f18\u8ba1\u7b97, \u4e91\u534f\u540c, \u5b9e\u65f6\u5e94\u7528, \u5ef6\u8fdf\u4f18\u5316"}}
{"id": "2505.22338", "pdf": "https://arxiv.org/pdf/2505.22338", "abs": "https://arxiv.org/abs/2505.22338", "authors": ["Hanyang Wang", "Lu Wang", "Chaoyun Zhang", "Tianjun Mao", "Si Qin", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "Text2Grad: Reinforcement Learning from Natural Language Feedback", "categories": ["cs.CL", "cs.AI"], "comment": "The code for our method is available at\n  https://github.com/microsoft/Text2Grad", "summary": "Traditional RLHF optimizes language models with coarse, scalar rewards that\nmask the fine-grained reasons behind success or failure, leading to slow and\nopaque learning. Recent work augments RL with textual critiques through\nprompting or reflection, improving interpretability but leaving model\nparameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm\nthat turns free-form textual feedback into span-level gradients. Given human\n(or programmatic) critiques, Text2Grad aligns each feedback phrase with the\nrelevant token spans, converts these alignments into differentiable reward\nsignals, and performs gradient updates that directly refine the offending\nportions of the model's policy. This yields precise, feedback-conditioned\nadjustments instead of global nudges. Text2Grad is realized through three\ncomponents: (1) a high-quality feedback-annotation pipeline that pairs\ncritiques with token spans; (2) a fine-grained reward model that predicts\nspan-level reward on answer while generating explanatory critiques; and (3) a\nspan-level policy optimizer that back-propagates natural-language gradients.\nAcross summarization, code generation, and question answering, Text2Grad\nconsistently surpasses scalar-reward RL and prompt-only baselines, providing\nboth higher task metrics and richer interpretability. Our results demonstrate\nthat natural-language feedback, when converted to gradients, is a powerful\nsignal for fine-grained policy optimization. The code for our method is\navailable at https://github.com/microsoft/Text2Grad", "AI": {"tldr": "Text2Grad \u662f\u4e00\u79cd\u5c06\u6587\u672c\u53cd\u9988\u8f6c\u6362\u4e3a\u7ec6\u7c92\u5ea6\u68af\u5ea6\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cbe\u51c6\u8c03\u6574\u6a21\u578b\u7b56\u7565\uff0c\u4f18\u4e8e\u4f20\u7edf\u6807\u91cf\u5956\u52b1RL\u548c\u63d0\u793a\u57fa\u51c6\u3002", "motivation": "\u4f20\u7edfRLHF\u4f7f\u7528\u7c97\u7c92\u5ea6\u7684\u6807\u91cf\u5956\u52b1\uff0c\u5b66\u4e60\u6548\u7387\u4f4e\u4e14\u4e0d\u900f\u660e\u3002\u6587\u672c\u53cd\u9988\u867d\u63d0\u5347\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u672a\u8c03\u6574\u6a21\u578b\u53c2\u6570\uff0c\u56e0\u6b64\u9700\u5c06\u53cd\u9988\u8f6c\u5316\u4e3a\u7ec6\u7c92\u5ea6\u68af\u5ea6\u4ee5\u4f18\u5316\u7b56\u7565\u3002", "method": "Text2Grad\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a(1) \u5c06\u53cd\u9988\u4e0e\u8bcd\u5143\u8de8\u5ea6\u5bf9\u9f50\u7684\u6807\u6ce8\u6d41\u7a0b\uff0c(2) \u9884\u6d4b\u7ec6\u7c92\u5ea6\u5956\u52b1\u5e76\u751f\u6210\u89e3\u91ca\u6027\u53cd\u9988\u7684\u5956\u52b1\u6a21\u578b\uff0c(3) \u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u68af\u5ea6\u53cd\u4f20\u64ad\u4f18\u5316\u7b56\u7565\u7684\u6a21\u5757\u3002", "result": "\u5728\u6458\u8981\u3001\u4ee3\u7801\u751f\u6210\u548c\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0cText2Grad\u663e\u8457\u4f18\u4e8e\u6807\u91cf\u5956\u52b1RL\u548c\u63d0\u793a\u57fa\u51c6\uff0c\u4efb\u52a1\u6307\u6807\u66f4\u9ad8\u4e14\u53ef\u89e3\u91ca\u6027\u66f4\u5f3a\u3002", "conclusion": "\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u8f6c\u5316\u4e3a\u68af\u5ea6\u540e\uff0c\u80fd\u6709\u6548\u652f\u6301\u7ec6\u7c92\u5ea6\u7b56\u7565\u4f18\u5316\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u6587\u672c\u53cd\u9988, \u7ec6\u7c92\u5ea6\u4f18\u5316, \u53ef\u89e3\u91ca\u6027, \u81ea\u7136\u8bed\u8a00\u5904\u7406"}}
{"id": "2505.22114", "pdf": "https://arxiv.org/pdf/2505.22114", "abs": "https://arxiv.org/abs/2505.22114", "authors": ["MaryBeth Defrance", "Guillaume Bied", "Maarten Buyl", "Jefrey Lijffijt", "Tijl De Bie"], "title": "BiMi Sheets: Infosheets for bias mitigation methods", "categories": ["cs.LG"], "comment": null, "summary": "Over the past 15 years, hundreds of bias mitigation methods have been\nproposed in the pursuit of fairness in machine learning (ML). However,\nalgorithmic biases are domain-, task-, and model-specific, leading to a\n`portability trap': bias mitigation solutions in one context may not be\nappropriate in another. Thus, a myriad of design choices have to be made when\ncreating a bias mitigation method, such as the formalization of fairness it\npursues, and where and how it intervenes in the ML pipeline. This creates\nchallenges in benchmarking and comparing the relative merits of different bias\nmitigation methods, and limits their uptake by practitioners.\n  We propose BiMi Sheets as a portable, uniform guide to document the design\nchoices of any bias mitigation method. This enables researchers and\npractitioners to quickly learn its main characteristics and to compare with\ntheir desiderata. Furthermore, the sheets' structure allow for the creation of\na structured database of bias mitigation methods. In order to foster the\nsheets' adoption, we provide a platform for finding and creating BiMi Sheets at\nbimisheet.com.", "AI": {"tldr": "BiMi Sheets \u662f\u4e00\u79cd\u53ef\u79fb\u690d\u3001\u7edf\u4e00\u7684\u6307\u5357\uff0c\u7528\u4e8e\u8bb0\u5f55\u4efb\u4f55\u504f\u5dee\u7f13\u89e3\u65b9\u6cd5\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u5feb\u901f\u4e86\u89e3\u5176\u4e3b\u8981\u7279\u5f81\u5e76\u6bd4\u8f83\u9700\u6c42\u3002\u6b64\u5916\uff0c\u5176\u7ed3\u6784\u652f\u6301\u521b\u5efa\u7ed3\u6784\u5316\u504f\u5dee\u7f13\u89e3\u65b9\u6cd5\u6570\u636e\u5e93\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u504f\u5dee\u7f13\u89e3\u65b9\u6cd5\u56e0\u9886\u57df\u3001\u4efb\u52a1\u548c\u6a21\u578b\u4e0d\u540c\u800c\u5b58\u5728\u2018\u53ef\u79fb\u690d\u6027\u9677\u9631\u2019\uff0c\u5bfc\u81f4\u4e0d\u540c\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u6bd4\u8f83\u548c\u5e94\u7528\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6807\u51c6\u5316\u5de5\u5177\u6765\u7edf\u4e00\u8bb0\u5f55\u548c\u6bd4\u8f83\u65b9\u6cd5\u8bbe\u8ba1\u3002", "method": "\u63d0\u51faBiMi Sheets\u4f5c\u4e3a\u6587\u6863\u5316\u5de5\u5177\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u683c\u5f0f\u8bb0\u5f55\u504f\u5dee\u7f13\u89e3\u65b9\u6cd5\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u5e76\u5efa\u7acb\u5e73\u53f0bimisheet.com\u4ee5\u4fc3\u8fdb\u5176\u4f7f\u7528\u3002", "result": "BiMi Sheets \u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u504f\u5dee\u7f13\u89e3\u65b9\u6cd5\u6570\u636e\u5e93\uff0c\u652f\u6301\u5feb\u901f\u6bd4\u8f83\u548c\u5b66\u4e60\uff0c\u63d0\u5347\u4e86\u65b9\u6cd5\u7684\u53ef\u79fb\u690d\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "BiMi Sheets \u4e3a\u89e3\u51b3\u504f\u5dee\u7f13\u89e3\u65b9\u6cd5\u7684\u53ef\u79fb\u690d\u6027\u548c\u6bd4\u8f83\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u516c\u5e73\u673a\u5668\u5b66\u4e60\u7684\u5b9e\u8df5\u5e94\u7528\u3002", "keywords": "Bias mitigation, fairness, machine learning, portability, benchmarking"}}
{"id": "2505.22354", "pdf": "https://arxiv.org/pdf/2505.22354", "abs": "https://arxiv.org/abs/2505.22354", "authors": ["Judith Sieker", "Clara Lachenmaier", "Sina Zarrie\u00df"], "title": "LLMs Struggle to Reject False Presuppositions when Misinformation Stakes are High", "categories": ["cs.CL"], "comment": "8 pages (including References). Accepted at CogSci 2025", "summary": "This paper examines how LLMs handle false presuppositions and whether certain\nlinguistic factors influence their responses to falsely presupposed content.\nPresuppositions subtly introduce information as given, making them highly\neffective at embedding disputable or false information. This raises concerns\nabout whether LLMs, like humans, may fail to detect and correct misleading\nassumptions introduced as false presuppositions, even when the stakes of\nmisinformation are high. Using a systematic approach based on linguistic\npresupposition analysis, we investigate the conditions under which LLMs are\nmore or less sensitive to adopt or reject false presuppositions. Focusing on\npolitical contexts, we examine how factors like linguistic construction,\npolitical party, and scenario probability impact the recognition of false\npresuppositions. We conduct experiments with a newly created dataset and\nexamine three LLMs: OpenAI's GPT-4-o, Meta's LLama-3-8B, and MistralAI's\nMistral-7B-v03. Our results show that the models struggle to recognize false\npresuppositions, with performance varying by condition. This study highlights\nthat linguistic presupposition analysis is a valuable tool for uncovering the\nreinforcement of political misinformation in LLM responses.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u9519\u8bef\u9884\u8bbe\u7684\u5904\u7406\u80fd\u529b\uff0c\u5e76\u63a2\u8ba8\u4e86\u8bed\u8a00\u56e0\u7d20\u662f\u5426\u5f71\u54cd\u5176\u54cd\u5e94\u3002\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\uff0c\u7814\u7a76\u53d1\u73b0LLM\u96be\u4ee5\u8bc6\u522b\u9519\u8bef\u9884\u8bbe\uff0c\u6027\u80fd\u968f\u6761\u4ef6\u53d8\u5316\u3002", "motivation": "\u63a2\u8ba8LLM\u662f\u5426\u80fd\u50cf\u4eba\u7c7b\u4e00\u6837\u68c0\u6d4b\u548c\u7ea0\u6b63\u9519\u8bef\u9884\u8bbe\u5f15\u53d1\u7684\u8bef\u5bfc\u4fe1\u606f\uff0c\u5c24\u5176\u662f\u5728\u653f\u6cbb\u80cc\u666f\u4e0b\u7684\u9ad8\u98ce\u9669\u573a\u666f\u3002", "method": "\u57fa\u4e8e\u8bed\u8a00\u9884\u8bbe\u5206\u6790\u7684\u7cfb\u7edf\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u4f7f\u7528\u4e86\u65b0\u521b\u5efa\u7684\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u4e86GPT-4-o\u3001LLama-3-8B\u548cMistral-7B-v03\u4e09\u79cdLLM\u3002", "result": "\u6a21\u578b\u666e\u904d\u96be\u4ee5\u8bc6\u522b\u9519\u8bef\u9884\u8bbe\uff0c\u6027\u80fd\u8868\u73b0\u56e0\u6761\u4ef6\uff08\u5982\u8bed\u8a00\u7ed3\u6784\u3001\u653f\u6cbb\u6d3e\u522b\u7b49\uff09\u800c\u5f02\u3002", "conclusion": "\u8bed\u8a00\u9884\u8bbe\u5206\u6790\u662f\u63ed\u793aLLM\u56de\u5e94\u4e2d\u653f\u6cbb\u9519\u8bef\u4fe1\u606f\u5f3a\u5316\u73b0\u8c61\u7684\u6709\u529b\u5de5\u5177\u3002", "keywords": "LLM, false presuppositions, misinformation, linguistic analysis, political context"}}
{"id": "2505.22151", "pdf": "https://arxiv.org/pdf/2505.22151", "abs": "https://arxiv.org/abs/2505.22151", "authors": ["Claude Formanek", "Omayma Mahjoub", "Louay Ben Nessir", "Sasha Abramowitz", "Ruan de Kock", "Wiem Khlifi", "Simon Du Toit", "Felix Chalumeau", "Daniel Rajaonarivonivelomanantsoa", "Arnol Fokam", "Siddarth Singh", "Ulrich Mbou Sob", "Arnu Pretorius"], "title": "Oryx: a Performant and Scalable Algorithm for Many-Agent Coordination in Offline MARL", "categories": ["cs.LG"], "comment": null, "summary": "A key challenge in offline multi-agent reinforcement learning (MARL) is\nachieving effective many-agent multi-step coordination in complex environments.\nIn this work, we propose Oryx, a novel algorithm for offline cooperative MARL\nto directly address this challenge. Oryx adapts the recently proposed\nretention-based architecture Sable and combines it with a sequential form of\nimplicit constraint Q-learning (ICQ), to develop a novel offline\nauto-regressive policy update scheme. This allows Oryx to solve complex\ncoordination challenges while maintaining temporal coherence over lengthy\ntrajectories. We evaluate Oryx across a diverse set of benchmarks from prior\nworks (SMAC, RWARE, and Multi-Agent MuJoCo) covering tasks of both discrete and\ncontinuous control, varying in scale and difficulty. Oryx achieves\nstate-of-the-art performance on more than 80% of the 65 tested datasets,\noutperforming prior offline MARL methods and demonstrating robust\ngeneralisation across domains with many agents and long horizons. Finally, we\nintroduce new datasets to push the limits of many-agent coordination in offline\nMARL, and demonstrate Oryx's superior ability to scale effectively in such\nsettings. We will make all of our datasets, experimental data, and code\navailable upon publication.", "AI": {"tldr": "Oryx\u662f\u4e00\u79cd\u65b0\u9896\u7684\u79bb\u7ebf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u4fdd\u7559\u67b6\u6784Sable\u548c\u9690\u5f0f\u7ea6\u675fQ\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u590d\u6742\u73af\u5883\u4e2d\u7684\u591a\u667a\u80fd\u4f53\u534f\u8c03\uff0c\u5e76\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u79bb\u7ebf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e3b\u8981\u6311\u6218\u662f\u590d\u6742\u73af\u5883\u4e2d\u7684\u591a\u667a\u80fd\u4f53\u591a\u6b65\u534f\u8c03\uff0cOryx\u65e8\u5728\u76f4\u63a5\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "Oryx\u7ed3\u5408\u4e86\u4fdd\u7559\u67b6\u6784Sable\u548c\u9690\u5f0f\u7ea6\u675fQ\u5b66\u4e60\uff08ICQ\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u79bb\u7ebf\u81ea\u56de\u5f52\u7b56\u7565\u66f4\u65b0\u65b9\u6848\u3002", "result": "Oryx\u572865\u4e2a\u6d4b\u8bd5\u6570\u636e\u96c6\u4e2d\u8d85\u8fc780%\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5728\u591a\u79cd\u667a\u80fd\u4f53\u548c\u957f\u8f68\u8ff9\u4efb\u52a1\u4e2d\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Oryx\u5728\u79bb\u7ebf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u9886\u57df\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u667a\u80fd\u4f53\u534f\u8c03\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6269\u5c55\u80fd\u529b\u3002", "keywords": "\u79bb\u7ebf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60, Oryx, \u9690\u5f0f\u7ea6\u675fQ\u5b66\u4e60, \u4fdd\u7559\u67b6\u6784, \u591a\u667a\u80fd\u4f53\u534f\u8c03"}}
{"id": "2505.21596", "pdf": "https://arxiv.org/pdf/2505.21596", "abs": "https://arxiv.org/abs/2505.21596", "authors": ["Esra Adiyeke", "Tianqi Liu", "Venkata Sai Dheeraj Naganaboina", "Han Li", "Tyler J. Loftus", "Yuanfang Ren", "Benjamin Shickel", "Matthew M. Ruppert", "Karandeep Singh", "Ruogu Fang", "Parisa Rashidi", "Azra Bihorac", "Tezcan Ozrazgat-Baslanti"], "title": "Learning optimal treatment strategies for intraoperative hypotension using deep reinforcement learning", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": "41 pages, 1 table, 5 figures, 5 supplemental tables, 6 supplemental\n  figures", "summary": "Traditional methods of surgical decision making heavily rely on human\nexperience and prompt actions, which are variable. A data-driven system\ngenerating treatment recommendations based on patient states can be a\nsubstantial asset in perioperative decision-making, as in cases of\nintraoperative hypotension, for which suboptimal management is associated with\nacute kidney injury (AKI), a common and morbid postoperative complication. We\ndeveloped a Reinforcement Learning (RL) model to recommend optimum dose of\nintravenous (IV) fluid and vasopressors during surgery to avoid intraoperative\nhypotension and postoperative AKI. We retrospectively analyzed 50,021 surgeries\nfrom 42,547 adult patients who underwent major surgery at a quaternary care\nhospital between June 2014 and September 2020. Of these, 34,186 surgeries were\nused for model training and 15,835 surgeries were reserved for testing. We\ndeveloped a Deep Q-Networks based RL model using 16 variables including\nintraoperative physiologic time series, total dose of IV fluid and vasopressors\nextracted for every 15-minute epoch. The model replicated 69% of physician's\ndecisions for the dosage of vasopressors and proposed higher or lower dosage of\nvasopressors than received in 10% and 21% of the treatments, respectively. In\nterms of IV fluids, the model's recommendations were within 0.05 ml/kg/15 min\nof the actual dose in 41% of the cases, with higher or lower doses recommended\nfor 27% and 32% of the treatments, respectively. The model resulted in a higher\nestimated policy value compared to the physicians' actual treatments, as well\nas random and zero-drug policies. AKI prevalence was the lowest in patients\nreceiving medication dosages that aligned with model's decisions. Our findings\nsuggest that implementation of the model's policy has the potential to reduce\npostoperative AKI and improve other outcomes driven by intraoperative\nhypotension.", "AI": {"tldr": "\u4f20\u7edf\u624b\u672f\u51b3\u7b56\u4f9d\u8d56\u7ecf\u9a8c\u4e14\u5b58\u5728\u5dee\u5f02\uff0c\u6570\u636e\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u53ef\u4f18\u5316\u672f\u4e2d\u4f4e\u8840\u538b\u7ba1\u7406\uff0c\u51cf\u5c11\u672f\u540e\u6025\u6027\u80be\u635f\u4f24\uff08AKI\uff09\u3002\u6a21\u578b\u57fa\u4e8e16\u53d8\u91cf\u8bad\u7ec3\uff0c\u63a8\u8350\u8840\u7ba1\u52a0\u538b\u836f\u548c\u9759\u8109\u8f93\u6db2\u5242\u91cf\uff0c\u4e0e\u533b\u751f\u51b3\u7b56\u91cd\u5408\u738769%\uff0c\u653f\u7b56\u4ef7\u503c\u66f4\u9ad8\u4e14AKI\u53d1\u751f\u7387\u6700\u4f4e\u3002", "motivation": "\u4f20\u7edf\u624b\u672f\u51b3\u7b56\u4f9d\u8d56\u533b\u751f\u7ecf\u9a8c\uff0c\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\u3002\u6570\u636e\u9a71\u52a8\u7684\u63a8\u8350\u7cfb\u7edf\u53ef\u4f18\u5316\u672f\u4e2d\u4f4e\u8840\u538b\u7ba1\u7406\uff0c\u51cf\u5c11\u672f\u540e\u5e76\u53d1\u75c7\u5982AKI\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6Q\u7f51\u7edc\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\uff0c\u57fa\u4e8e16\u4e2a\u53d8\u91cf\uff08\u5982\u751f\u7406\u65f6\u95f4\u5e8f\u5217\u3001\u836f\u7269\u5242\u91cf\uff09\uff0c\u5206\u679050,021\u4f8b\u624b\u672f\u6570\u636e\uff0834,186\u8bad\u7ec3\uff0c15,835\u6d4b\u8bd5\uff09\u3002", "result": "\u6a21\u578b\u4e0e\u533b\u751f\u51b3\u7b56\u91cd\u5408\u738769%\uff0c\u653f\u7b56\u4ef7\u503c\u9ad8\u4e8e\u5b9e\u9645\u6cbb\u7597\u3001\u968f\u673a\u53ca\u96f6\u836f\u7269\u7b56\u7565\uff0cAKI\u53d1\u751f\u7387\u6700\u4f4e\u3002", "conclusion": "\u6a21\u578b\u653f\u7b56\u53ef\u964d\u4f4e\u672f\u540eAKI\u5e76\u6539\u5584\u672f\u4e2d\u4f4e\u8840\u538b\u76f8\u5173\u7ed3\u5c40\uff0c\u5177\u6709\u4e34\u5e8a\u6f5c\u529b\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u672f\u4e2d\u4f4e\u8840\u538b, \u6025\u6027\u80be\u635f\u4f24, \u8840\u7ba1\u52a0\u538b\u836f, \u9759\u8109\u8f93\u6db2"}}
{"id": "2505.22375", "pdf": "https://arxiv.org/pdf/2505.22375", "abs": "https://arxiv.org/abs/2505.22375", "authors": ["Hanting Chen", "Yasheng Wang", "Kai Han", "Dong Li", "Lin Li", "Zhenni Bi", "Jinpeng Li", "Haoyu Wang", "Fei Mi", "Mingjian Zhu", "Bin Wang", "Kaikai Song", "Yifei Fu", "Xu He", "Yu Luo", "Chong Zhu", "Quan He", "Xueyu Wu", "Wei He", "Hailin Hu", "Yehui Tang", "Dacheng Tao", "Xinghao Chen", "Yunhe Wang", "Other Contributors"], "title": "Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition", "categories": ["cs.CL"], "comment": null, "summary": "This work presents Pangu Embedded, an efficient Large Language Model (LLM)\nreasoner developed on Ascend Neural Processing Units (NPUs), featuring flexible\nfast and slow thinking capabilities. Pangu Embedded addresses the significant\ncomputational costs and inference latency challenges prevalent in existing\nreasoning-optimized LLMs. We propose a two-stage training framework for its\nconstruction. In Stage 1, the model is finetuned via an iterative distillation\nprocess, incorporating inter-iteration model merging to effectively aggregate\ncomplementary knowledge. This is followed by reinforcement learning on Ascend\nclusters, optimized by a latency-tolerant scheduler that combines stale\nsynchronous parallelism with prioritized data queues. The RL process is guided\nby a Multi-source Adaptive Reward System (MARS), which generates dynamic,\ntask-specific reward signals using deterministic metrics and lightweight LLM\nevaluators for mathematics, coding, and general problem-solving tasks. Stage 2\nintroduces a dual-system framework, endowing Pangu Embedded with a \"fast\" mode\nfor routine queries and a deeper \"slow\" mode for complex inference. This\nframework offers both manual mode switching for user control and an automatic,\ncomplexity-aware mode selection mechanism that dynamically allocates\ncomputational resources to balance latency and reasoning depth. Experimental\nresults on benchmarks including AIME 2024, GPQA, and LiveCodeBench demonstrate\nthat Pangu Embedded with 7B parameters, outperforms similar-size models like\nQwen3-8B and GLM4-9B. It delivers rapid responses and state-of-the-art\nreasoning quality within a single, unified model architecture, highlighting a\npromising direction for developing powerful yet practically deployable LLM\nreasoners.", "AI": {"tldr": "Pangu Embedded \u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u4e3a\u63a8\u7406\u4efb\u52a1\u8bbe\u8ba1\uff0c\u91c7\u7528\u53cc\u9636\u6bb5\u8bad\u7ec3\u548c\u53cc\u7cfb\u7edf\u6846\u67b6\uff0c\u7ed3\u5408\u5feb\u901f\u548c\u6162\u901f\u63a8\u7406\u6a21\u5f0f\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u63a8\u7406\u4f18\u5316 LLM \u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u5ef6\u8fdf\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u63a8\u7406\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u63a8\u7406\u5ef6\u8fdf\u95ee\u9898\uff0c\u5f00\u53d1\u4e86 Pangu Embedded\uff0c\u65e8\u5728\u63d0\u4f9b\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u5feb\u6162\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a\u9636\u6bb5 1 \u901a\u8fc7\u8fed\u4ee3\u84b8\u998f\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6027\u80fd\uff0c\u9636\u6bb5 2 \u5f15\u5165\u53cc\u7cfb\u7edf\u6846\u67b6\u652f\u6301\u5feb\u6162\u63a8\u7406\u6a21\u5f0f\u7684\u52a8\u6001\u5207\u6362\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cPangu Embedded \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u540c\u7c7b\u6a21\u578b\uff0c\u63d0\u4f9b\u5feb\u901f\u54cd\u5e94\u548c\u6700\u5148\u8fdb\u7684\u63a8\u7406\u8d28\u91cf\u3002", "conclusion": "Pangu Embedded \u5c55\u793a\u4e86\u9ad8\u6548\u4e14\u5b9e\u7528\u7684 LLM \u63a8\u7406\u5668\u7684\u53d1\u5c55\u65b9\u5411\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u63a8\u7406\u4f18\u5316, \u53cc\u9636\u6bb5\u8bad\u7ec3, \u5f3a\u5316\u5b66\u4e60, \u53cc\u7cfb\u7edf\u6846\u67b6"}}
{"id": "2505.22152", "pdf": "https://arxiv.org/pdf/2505.22152", "abs": "https://arxiv.org/abs/2505.22152", "authors": ["Dominik Fuchsgruber", "Tom Wollschl\u00e4ger", "Johannes Bordne", "Stephan G\u00fcnnemann"], "title": "Uncertainty Estimation for Heterophilic Graphs Through the Lens of Information Theory", "categories": ["cs.LG", "cs.SI"], "comment": null, "summary": "While uncertainty estimation for graphs recently gained traction, most\nmethods rely on homophily and deteriorate in heterophilic settings. We address\nthis by analyzing message passing neural networks from an information-theoretic\nperspective and developing a suitable analog to data processing inequality to\nquantify information throughout the model's layers. In contrast to non-graph\ndomains, information about the node-level prediction target can increase with\nmodel depth if a node's features are semantically different from its neighbors.\nTherefore, on heterophilic graphs, the latent embeddings of an MPNN each\nprovide different information about the data distribution - different from\nhomophilic settings. This reveals that considering all node representations\nsimultaneously is a key design principle for epistemic uncertainty estimation\non graphs beyond homophily. We empirically confirm this with a simple post-hoc\ndensity estimator on the joint node embedding space that provides\nstate-of-the-art uncertainty on heterophilic graphs. At the same time, it\nmatches prior work on homophilic graphs without explicitly exploiting homophily\nthrough post-processing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u89c6\u89d2\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cf\u5316\u56fe\u795e\u7ecf\u7f51\u7edc\u5404\u5c42\u4fe1\u606f\u4f20\u9012\u7684\u4e0d\u7b49\u5f0f\uff0c\u89e3\u51b3\u4e86\u5f02\u8d28\u6027\u56fe\u4e2d\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u540c\u8d28\u6027\u5047\u8bbe\uff0c\u5728\u5f02\u8d28\u6027\u56fe\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4e0d\u4f9d\u8d56\u540c\u8d28\u6027\u7684\u901a\u7528\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u4ece\u4fe1\u606f\u8bba\u89d2\u5ea6\u5206\u6790\u6d88\u606f\u4f20\u9012\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u51fa\u4e00\u79cd\u7c7b\u4f3c\u6570\u636e\u5904\u7406\u4e0d\u7b49\u5f0f\u7684\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u540e\u9a8c\u5bc6\u5ea6\u4f30\u8ba1\u5668\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5f02\u8d28\u6027\u56fe\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6548\u679c\uff0c\u540c\u65f6\u5728\u4e0d\u4f9d\u8d56\u540c\u8d28\u6027\u7684\u60c5\u51b5\u4e0b\u4e0e\u73b0\u6709\u540c\u8d28\u6027\u56fe\u65b9\u6cd5\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "\u540c\u65f6\u8003\u8651\u6240\u6709\u8282\u70b9\u5d4c\u5165\u662f\u5f02\u8d28\u6027\u56fe\u4e2d\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u5173\u952e\u8bbe\u8ba1\u539f\u5219\u3002", "keywords": "\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1, \u5f02\u8d28\u6027\u56fe, \u4fe1\u606f\u8bba, \u6d88\u606f\u4f20\u9012\u795e\u7ecf\u7f51\u7edc"}}
{"id": "2505.22430", "pdf": "https://arxiv.org/pdf/2505.22430", "abs": "https://arxiv.org/abs/2505.22430", "authors": ["Kun Li", "Yunxiang Li", "Tianhua Zhang", "Hongyin Luo", "Xixin Wu", "James Glass", "Helen Meng"], "title": "RAG-Zeval: Towards Robust and Interpretable Evaluation on RAG Responses through End-to-End Rule-Guided Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Robust evaluation is critical for deploying trustworthy retrieval-augmented\ngeneration (RAG) systems. However, current LLM-based evaluation frameworks\npredominantly rely on directly prompting resource-intensive models with complex\nmulti-stage prompts, underutilizing models' reasoning capabilities and\nintroducing significant computational cost. In this paper, we present RAG-Zeval\n(RAG-Zero Evaluator), a novel end-to-end framework that formulates faithfulness\nand correctness evaluation as a rule-guided reasoning task. Our approach trains\nevaluators with reinforcement learning, facilitating compact models to generate\ncomprehensive and sound assessments with detailed explanation in one-pass. We\nintroduce a ranking-based outcome reward mechanism, using preference judgments\nrather than absolute scores, to address the challenge of obtaining precise\npointwise reward signals. To this end, we synthesize the ranking references by\ngenerating quality-controlled responses with zero human annotation. Experiments\ndemonstrate RAG-Zeval's superior performance, achieving the strongest\ncorrelation with human judgments and outperforming baselines that rely on LLMs\nwith 10-100 times more parameters. Our approach also exhibits superior\ninterpretability in response evaluation.", "AI": {"tldr": "RAG-Zeval\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u89c4\u5219\u5f15\u5bfc\u7684\u63a8\u7406\u4efb\u52a1\u8bc4\u4f30RAG\u7cfb\u7edf\u7684\u5fe0\u5b9e\u6027\u548c\u6b63\u786e\u6027\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8bc4\u4f30\u5668\uff0c\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u6846\u67b6\u4f9d\u8d56\u8d44\u6e90\u5bc6\u96c6\u578b\u6a21\u578b\u548c\u590d\u6742\u591a\u9636\u6bb5\u63d0\u793a\uff0c\u672a\u5145\u5206\u5229\u7528\u6a21\u578b\u63a8\u7406\u80fd\u529b\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0cRAG-Zeval\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8bc4\u4f30\u5668\uff0c\u5f15\u5165\u57fa\u4e8e\u6392\u540d\u7684\u7ed3\u679c\u5956\u52b1\u673a\u5236\uff0c\u5229\u7528\u504f\u597d\u5224\u65ad\u800c\u975e\u7edd\u5bf9\u5206\u6570\uff0c\u5408\u6210\u65e0\u4eba\u5de5\u6807\u6ce8\u7684\u6392\u540d\u53c2\u8003\u3002", "result": "RAG-Zeval\u5728\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u6027\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f18\u4e8e\u53c2\u6570\u91cf\u592710-100\u500d\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u5177\u6709\u66f4\u9ad8\u7684\u89e3\u91ca\u6027\u3002", "conclusion": "RAG-Zeval\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684RAG\u7cfb\u7edf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u8bc4\u4f30\u8d28\u91cf\u3002", "keywords": "RAG, \u8bc4\u4f30\u6846\u67b6, \u5f3a\u5316\u5b66\u4e60, \u96f6\u4eba\u5de5\u6807\u6ce8, \u89e3\u91ca\u6027"}}
{"id": "2505.22158", "pdf": "https://arxiv.org/pdf/2505.22158", "abs": "https://arxiv.org/abs/2505.22158", "authors": ["Rustem Takhanov"], "title": "The informativeness of the gradient revisited", "categories": ["cs.LG"], "comment": null, "summary": "In the past decade gradient-based deep learning has revolutionized several\napplications. However, this rapid advancement has highlighted the need for a\ndeeper theoretical understanding of its limitations. Research has shown that,\nin many practical learning tasks, the information contained in the gradient is\nso minimal that gradient-based methods require an exceedingly large number of\niterations to achieve success. The informativeness of the gradient is typically\nmeasured by its variance with respect to the random selection of a target\nfunction from a hypothesis class.\n  We use this framework and give a general bound on the variance in terms of a\nparameter related to the pairwise independence of the target function class and\nthe collision entropy of the input distribution. Our bound scales as $\n\\tilde{\\mathcal{O}}(\\varepsilon+e^{-\\frac{1}{2}\\mathcal{E}_c}) $, where $\n\\tilde{\\mathcal{O}} $ hides factors related to the regularity of the learning\nmodel and the loss function, $ \\varepsilon $ measures the pairwise independence\nof the target function class and $\\mathcal{E}_c$ is the collision entropy of\nthe input distribution.\n  To demonstrate the practical utility of our bound, we apply it to the class\nof Learning with Errors (LWE) mappings and high-frequency functions. In\naddition to the theoretical analysis, we present experiments to understand\nbetter the nature of recent deep learning-based attacks on LWE.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u68af\u5ea6\u4fe1\u606f\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5173\u4e8e\u76ee\u6807\u51fd\u6570\u7c7b\u72ec\u7acb\u6027\u53ca\u8f93\u5165\u5206\u5e03\u78b0\u649e\u71b5\u5bf9\u68af\u5ea6\u65b9\u5dee\u5f71\u54cd\u7684\u901a\u7528\u4e0a\u754c\uff0c\u5e76\u5e94\u7528\u4e8eLWE\u6620\u5c04\u53ca\u9ad8\u9891\u51fd\u6570\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u57fa\u4e8e\u68af\u5ea6\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u67d0\u4e9b\u9886\u57df\u53d6\u5f97\u4e86\u5de8\u5927\u8fdb\u5c55\uff0c\u4f46\u540c\u65f6\u4e5f\u66b4\u9732\u4e86\u5176\u5c40\u9650\u6027\u3002\u7279\u522b\u662f\u5728\u8bb8\u591a\u5b9e\u9645\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u68af\u5ea6\u4fe1\u606f\u8fc7\u5c11\u5bfc\u81f4\u68af\u5ea6\u6cd5\u9700\u8981\u6781\u5927\u91cf\u7684\u8fed\u4ee3\u624d\u80fd\u6210\u529f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u66f4\u6df1\u5165\u7684\u7406\u8bba\u6846\u67b6\u6765\u8861\u91cf\u68af\u5ea6\u7684\u4fe1\u606f\u91cf\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5ea6\u91cf\u68af\u5ea6\u65b9\u5dee\u4e0e\u7814\u7a76\u76ee\u6807\u51fd\u6570\u7c7b\u53ca\u8f93\u5165\u5206\u5e03\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u4e0a\u754c\u8868\u8fbe\u5f0f\uff0c\u91cf\u5316\u4e86\u68af\u5ea6\u7684\u4fe1\u606f\u91cf\uff0c\u5e76\u8fdb\u4e00\u6b65\u5c06\u5176\u5e94\u7528\u4e8e\u5b66\u4e60\u9519\u8bef\u95ee\u9898\uff08LWE\uff09\u6620\u5c04\u53ca\u9ad8\u9891\u51fd\u6570\u7684\u5206\u6790\u3002", "result": "\u6587\u7ae0\u5f97\u51fa\u4e86\u4e00\u4e2a\u68af\u5ea6\u65b9\u5dee\u7684\u4e0a\u754c\u516c\u5f0f$ \tilde{\\mathcal{O}}(\\varepsilon+e^{-\\frac{1}{2}\\mathcal{E}_c}) $\uff0c\u63ed\u793a\u4e86\u76ee\u6807\u51fd\u6570\u7c7b\u72ec\u7acb\u6027$ \\varepsilon $\u4e0e\u8f93\u5165\u5206\u5e03\u78b0\u649e\u71b5$ \\mathcal{E}_c $\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u4e0a\u754c\u5728LWE\u53ca\u9ad8\u9891\u51fd\u6570\u5e94\u7528\u4e2d\u7684\u6548\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u68af\u5ea6\u4fe1\u606f\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u901a\u8fc7LWE\u95ee\u9898\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002\u672a\u6765\u7684\u65b9\u5411\u53ef\u4ee5\u5305\u62ec\u6269\u5c55\u7406\u8bba\u6846\u67b6\u4ee5\u8986\u76d6\u66f4\u591a\u5b66\u4e60\u4efb\u52a1\u3002", "keywords": "\u6df1\u5ea6\u5b66\u4e60\uff0c\u68af\u5ea6\u4fe1\u606f\uff0cLWE\u95ee\u9898\uff0c\u78b0\u649e\u71b5\uff0c\u9ad8\u9891\u51fd\u6570"}}
{"id": "2505.21603", "pdf": "https://arxiv.org/pdf/2505.21603", "abs": "https://arxiv.org/abs/2505.21603", "authors": ["Andre Massahiro Shimaoka", "Renato Cordeiro Ferreira", "Alfredo Goldman"], "title": "Leveraging XP and CRISP-DM for Agile Data Science Projects", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": null, "summary": "This study explores the integration of eXtreme Programming (XP) and the\nCross-Industry Standard Process for Data Mining (CRISP-DM) in agile Data\nScience projects. We conducted a case study at the e-commerce company Elo7 to\nanswer the research question: How can the agility of the XP method be\nintegrated with CRISP-DM in Data Science projects? Data was collected through\ninterviews and questionnaires with a Data Science team consisting of data\nscientists, ML engineers, and data product managers. The results show that 86%\nof the team frequently or always applies CRISP-DM, while 71% adopt XP practices\nin their projects. Furthermore, the study demonstrates that it is possible to\ncombine CRISP-DM with XP in Data Science projects, providing a structured and\ncollaborative approach. Finally, the study generated improvement\nrecommendations for the company.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u654f\u6377\u6570\u636e\u79d1\u5b66\u9879\u76ee\u4e2d\u5982\u4f55\u5c06\u6781\u9650\u7f16\u7a0b\uff08XP\uff09\u4e0e\u8de8\u884c\u4e1a\u6570\u636e\u6316\u6398\u6807\u51c6\u6d41\u7a0b\uff08CRISP-DM\uff09\u7ed3\u5408\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u4e86\u5176\u53ef\u884c\u6027\u53ca\u6548\u679c\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5728\u654f\u6377\u6570\u636e\u79d1\u5b66\u9879\u76ee\u4e2d\u7ed3\u5408XP\u7684\u7075\u6d3b\u6027\u4e0eCRISP-DM\u7684\u7ed3\u6784\u5316\u4f18\u52bf\uff0c\u4ee5\u63d0\u5347\u56e2\u961f\u534f\u4f5c\u4e0e\u9879\u76ee\u6548\u7387\u3002", "method": "\u91c7\u7528\u6848\u4f8b\u7814\u7a76\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbf\u8c08\u548c\u95ee\u5377\u8c03\u67e5\u6536\u96c6Elo7\u516c\u53f8\u6570\u636e\u79d1\u5b66\u56e2\u961f\u7684\u5b9e\u8df5\u6570\u636e\u3002", "result": "86%\u7684\u56e2\u961f\u9891\u7e41\u6216\u59cb\u7ec8\u5e94\u7528CRISP-DM\uff0c71%\u5728\u9879\u76ee\u4e2d\u91c7\u7528XP\u5b9e\u8df5\uff0c\u8bc1\u5b9e\u4e24\u8005\u53ef\u4ee5\u7ed3\u5408\u5e76\u63d0\u4f9b\u7ed3\u6784\u5316\u534f\u4f5c\u65b9\u5f0f\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9eXP\u4e0eCRISP-DM\u5728\u6570\u636e\u79d1\u5b66\u9879\u76ee\u4e2d\u7684\u7ed3\u5408\u53ef\u884c\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u5efa\u8bae\u3002", "keywords": "\u6781\u9650\u7f16\u7a0b, CRISP-DM, \u6570\u636e\u79d1\u5b66, \u654f\u6377\u65b9\u6cd5, \u6848\u4f8b\u7814\u7a76"}}
{"id": "2505.22453", "pdf": "https://arxiv.org/pdf/2505.22453", "abs": "https://arxiv.org/abs/2505.22453", "authors": ["Lai Wei", "Yuting Li", "Chen Wang", "Yue Wang", "Linghe Kong", "Weiran Huang", "Lichao Sun"], "title": "Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Improving Multi-modal Large Language Models (MLLMs) in the post-training\nstage typically relies on supervised fine-tuning (SFT) or reinforcement\nlearning (RL). However, these supervised methods require expensive and manually\nannotated multi-modal data--an ultimately unsustainable resource. While recent\nefforts have explored unsupervised post-training, their methods are complex and\ndifficult to iterate. In this work, we are the first to investigate the use of\nGRPO, a stable and scalable online RL algorithm, for enabling continual\nself-improvement without any external supervision. We propose MM-UPT, a simple\nyet effective framework for unsupervised post-training of MLLMs. MM-UPT builds\nupon GRPO, replacing traditional reward signals with a self-rewarding mechanism\nbased on majority voting over multiple sampled responses. Our experiments\ndemonstrate that MM-UPT significantly improves the reasoning ability of\nQwen2.5-VL-7B (e.g., 66.3 %$\\rightarrow$72.9 % on MathVista, 62.9\n%$\\rightarrow$68.7 % on We-Math), using standard dataset without ground truth\nlabels. MM-UPT also outperforms prior unsupervised baselines and even\napproaches the results of supervised GRPO. Furthermore, we show that\nincorporating synthetic questions, generated solely by MLLM itself, can boost\nperformance as well, highlighting a promising approach for scalable\nself-improvement. Overall, MM-UPT offers a new paradigm for continual,\nautonomous enhancement of MLLMs in the absence of external supervision. Our\ncode is available at https://github.com/waltonfuture/MM-UPT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMM-UPT\u7684\u65e0\u76d1\u7763\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u57fa\u4e8eGRPO\u7b97\u6cd5\u5b9e\u73b0\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6211\u6301\u7eed\u6539\u8fdb\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u6807\u6ce8\u6570\u636e\u6216\u590d\u6742\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u540e\u8bad\u7ec3\u6548\u7387\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63a2\u7d22\u4e86\u65e0\u9700\u5916\u90e8\u76d1\u7763\u3001\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u81ea\u6211\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMM-UPT\u6846\u67b6\uff0c\u57fa\u4e8eGRPO\u7b97\u6cd5\uff0c\u91c7\u7528\u591a\u6570\u6295\u7968\u7684\u81ea\u5956\u52b1\u673a\u5236\uff0c\u66ff\u4ee3\u4f20\u7edf\u5956\u52b1\u4fe1\u53f7\uff0c\u5e76\u5229\u7528\u6a21\u578b\u81ea\u8eab\u751f\u6210\u7684\u5408\u6210\u95ee\u9898\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728MathVista\u548cWe-Math\u6570\u636e\u96c6\u4e0a\u5206\u522b\u63d0\u53476.6%\u548c5.8%\uff0c\u6548\u679c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u63a5\u8fd1\u6709\u76d1\u7763GRPO\u7684\u7ed3\u679c\u3002", "conclusion": "MM-UPT\u4e3a\u65e0\u76d1\u7763\u73af\u5883\u4e0b\u7684MLLM\u6301\u7eed\u81ea\u6211\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5c55\u73b0\u4e86\u53ef\u6269\u5c55\u6027\u6f5c\u529b\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u65e0\u76d1\u7763\u540e\u8bad\u7ec3\u3001GRPO\u7b97\u6cd5\u3001\u81ea\u6211\u5956\u52b1\u673a\u5236\u3001\u6301\u7eed\u6539\u8fdb"}}
{"id": "2505.22196", "pdf": "https://arxiv.org/pdf/2505.22196", "abs": "https://arxiv.org/abs/2505.22196", "authors": ["Jingyi Cui", "Hongwei Wen", "Yisen Wang"], "title": "An Augmentation-Aware Theory for Self-Supervised Contrastive Learning", "categories": ["cs.LG"], "comment": "Accepted to ICML2025", "summary": "Self-supervised contrastive learning has emerged as a powerful tool in\nmachine learning and computer vision to learn meaningful representations from\nunlabeled data. Meanwhile, its empirical success has encouraged many\ntheoretical studies to reveal the learning mechanisms. However, in the existing\ntheoretical research, the role of data augmentation is still under-exploited,\nespecially the effects of specific augmentation types. To fill in the blank, we\nfor the first time propose an augmentation-aware error bound for\nself-supervised contrastive learning, showing that the supervised risk is\nbounded not only by the unsupervised risk, but also explicitly by a trade-off\ninduced by data augmentation. Then, under a novel semantic label assumption, we\ndiscuss how certain augmentation methods affect the error bound. Lastly, we\nconduct both pixel- and representation-level experiments to verify our proposed\ntheoretical results.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u7684\u589e\u5f3a\u611f\u77e5\u8bef\u5dee\u8fb9\u754c\uff0c\u9996\u6b21\u5c06\u6570\u636e\u589e\u5f3a\u7684\u5f71\u54cd\u663e\u5f0f\u5730\u7eb3\u5165\u4e86\u76d1\u7763\u98ce\u9669\u7684\u754c\u5b9a\u3002", "motivation": "\u73b0\u6709\u7406\u8bba\u7814\u7a76\u672a\u5145\u5206\u63a2\u8ba8\u6570\u636e\u589e\u5f3a\u7684\u4f5c\u7528\uff0c\u5c24\u5176\u662f\u7279\u5b9a\u589e\u5f3a\u7c7b\u578b\u7684\u5f71\u54cd\uff0c\u4f5c\u8005\u8bd5\u56fe\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u589e\u5f3a\u611f\u77e5\u8bef\u5dee\u8fb9\u754c\uff0c\u5206\u6790\u6570\u636e\u589e\u5f3a\u5bf9\u76d1\u7763\u98ce\u9669\u7684\u663e\u5f0f\u5f71\u54cd\uff0c\u5e76\u57fa\u4e8e\u8bed\u4e49\u6807\u7b7e\u5047\u8bbe\u8ba8\u8bba\u4e0d\u540c\u589e\u5f3a\u65b9\u6cd5\u5982\u4f55\u5f71\u54cd\u8bef\u5dee\u8fb9\u754c\u3002", "result": "\u901a\u8fc7\u50cf\u7d20\u7ea7\u548c\u8868\u793a\u7ea7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "conclusion": "\u6570\u636e\u589e\u5f3a\u5728\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u901a\u8fc7\u663e\u5f0f\u6743\u8861\u5f71\u54cd\u76d1\u7763\u98ce\u9669\uff0c\u7279\u5b9a\u589e\u5f3a\u65b9\u6cd5\u5bf9\u8bef\u5dee\u8fb9\u754c\u6709\u663e\u8457\u4f5c\u7528\u3002", "keywords": "\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60, \u6570\u636e\u589e\u5f3a, \u8bef\u5dee\u8fb9\u754c, \u8bed\u4e49\u6807\u7b7e"}}
{"id": "2505.21604", "pdf": "https://arxiv.org/pdf/2505.21604", "abs": "https://arxiv.org/abs/2505.21604", "authors": ["Kristina Radivojevic", "Caleb Reinking", "Shaun Whitfield", "Paul Brenner"], "title": "Public Discourse Sandbox: Facilitating Human and AI Digital Communication Research", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Social media serves as a primary communication and information dissemination\nplatform for major global events, entertainment, and niche or topically focused\ncommunity discussions. Therefore, it represents a valuable resource for\nresearchers who aim to understand numerous questions. However, obtaining data\ncan be difficult, expensive, and often unreliable due to the presence of bots,\nfake accounts, and manipulated content. Additionally, there are ethical\nconcerns if researchers decide to conduct an online experiment without\nexplicitly notifying social media users about their intent. There is a need for\nmore controlled and scalable mechanisms to evaluate the impacts of digital\ndiscussion interventions on audiences. We introduce the Public Discourse\nSandbox (PDS), which serves as a digital discourse research platform for\nhuman-AI as well as AI-AI discourse research, testing, and training. PDS\nprovides a safe and secure space for research experiments that are not viable\non public, commercial social media platforms. Its main purpose is to enable the\nunderstanding of AI behaviors and the impacts of customized AI participants via\ntechniques such as prompt engineering, retrieval-augmented generation (RAG),\nand fine-tuning. We provide a hosted live version of the sandbox to support\nresearchers as well as the open-sourced code on GitHub for community\ncollaboration and contribution.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aPublic Discourse Sandbox (PDS)\u7684\u6570\u5b57\u8ba8\u8bba\u7814\u7a76\u5e73\u53f0\uff0c\u7528\u4e8e\u5b89\u5168\u7814\u7a76\u548c\u6d4b\u8bd5AI\u884c\u4e3a\u53ca\u5176\u5f71\u54cd\u3002", "motivation": "\u89e3\u51b3\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u83b7\u53d6\u56f0\u96be\u3001\u4e0d\u53ef\u9760\u53ca\u4f26\u7406\u95ee\u9898\uff0c\u63d0\u4f9b\u53ef\u63a7\u7684\u7814\u7a76\u673a\u5236\u3002", "method": "PDS\u5e73\u53f0\u652f\u6301\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u548c\u5fae\u8c03\u7b49\u6280\u672f\u7814\u7a76AI\u884c\u4e3a\u3002", "result": "\u63d0\u4f9b\u4e86PDS\u7684\u6258\u7ba1\u7248\u672c\u548c\u5f00\u6e90\u4ee3\u7801\uff0c\u652f\u6301\u7814\u7a76\u548c\u793e\u533a\u534f\u4f5c\u3002", "conclusion": "PDS\u4e3a\u7814\u7a76AI\u884c\u4e3a\u53ca\u5176\u5f71\u54cd\u63d0\u4f9b\u4e86\u5b89\u5168\u4e14\u53ef\u6269\u5c55\u7684\u5e73\u53f0\u3002", "keywords": "\u793e\u4ea4\u5a92\u4f53, AI\u884c\u4e3a, \u6570\u5b57\u8ba8\u8bba, \u7814\u7a76\u5e73\u53f0, PDS"}}
{"id": "2505.22501", "pdf": "https://arxiv.org/pdf/2505.22501", "abs": "https://arxiv.org/abs/2505.22501", "authors": ["Dingchu Zhang", "Yida Zhao", "Jialong Wu", "Baixuan Li", "Wenbiao Yin", "Liwen Zhang", "Yong Jiang", "Yufeng Li", "Kewei Tu", "Pengjun Xie", "Fei Huang"], "title": "EvolveSearch: An Iterative Self-Evolving Search Agent", "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has transformed the\nlandscape of agentic information seeking capabilities through the integration\nof tools such as search engines and web browsers. However, current mainstream\napproaches for enabling LLM web search proficiency face significant challenges:\nsupervised fine-tuning struggles with data production in open-search domains,\nwhile RL converges quickly, limiting their data utilization efficiency. To\naddress these issues, we propose EvolveSearch, a novel iterative self-evolution\nframework that combines SFT and RL to enhance agentic web search capabilities\nwithout any external human-annotated reasoning data. Extensive experiments on\nseven multi-hop question-answering (MHQA) benchmarks demonstrate that\nEvolveSearch consistently improves performance across iterations, ultimately\nachieving an average improvement of 4.7\\% over the current state-of-the-art\nacross seven benchmarks, opening the door to self-evolution agentic\ncapabilities in open web search domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u540d\u4e3aEvolveSearch\u7684\u8fed\u4ee3\u81ea\u8fdb\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff0c\u65e0\u9700\u5916\u90e8\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u5373\u63d0\u5347\u4e86LLM\u5728\u5f00\u653e\u641c\u7d22\u9886\u57df\u7684\u7f51\u7edc\u641c\u7d22\u80fd\u529b\u3002\u5728\u4e03\u4e2a\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6301\u7eed\u6027\u80fd\u63d0\u5347\uff0c\u5e73\u5747\u6539\u8fdb\u8fbe4.7%\u3002", "motivation": "\u5f53\u524d\u4e3b\u6d41\u65b9\u6cd5\uff08\u5982\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff09\u5728\u5f00\u653e\u641c\u7d22\u9886\u57df\u9762\u4e34\u6570\u636e\u751f\u6210\u548c\u5229\u7528\u6548\u7387\u7684\u9650\u5236\uff0c\u4e9f\u9700\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u6807\u6ce8\u7684\u81ea\u8fdb\u5316\u65b9\u6848\u4ee5\u63d0\u5347LLM\u7684\u641c\u7d22\u80fd\u529b\u3002", "method": "\u63d0\u51faEvolveSearch\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff0c\u5b9e\u73b0LLM\u7684\u81ea\u6211\u8fdb\u5316\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u3002", "result": "\u5728\u4e03\u4e2a\u591a\u8df3\u95ee\u7b54\uff08MHQA\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u968f\u8fed\u4ee3\u6301\u7eed\u63d0\u5347\uff0c\u6700\u7ec8\u5e73\u5747\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd54.7%\u3002", "conclusion": "EvolveSearch\u4e3a\u5f00\u653e\u7f51\u7edc\u641c\u7d22\u9886\u57df\u7684\u81ea\u8fdb\u5316\u667a\u80fd\u4ee3\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3001\u5de5\u5177\u589e\u5f3a\u3001\u81ea\u6211\u8fdb\u5316\u3001\u5f00\u653e\u641c\u7d22\u3001\u591a\u8df3\u95ee\u7b54"}}
{"id": "2505.22199", "pdf": "https://arxiv.org/pdf/2505.22199", "abs": "https://arxiv.org/abs/2505.22199", "authors": ["Xinyue Hu", "Zhibin Duan", "Bo Chen", "Mingyuan Zhou"], "title": "Enhancing Uncertainty Estimation and Interpretability via Bayesian Non-negative Decision Layer", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by The Thirteenth International Conference on Learning\n  Representations (ICLR 2025)", "summary": "Although deep neural networks have demonstrated significant success due to\ntheir powerful expressiveness, most models struggle to meet practical\nrequirements for uncertainty estimation. Concurrently, the entangled nature of\ndeep neural networks leads to a multifaceted problem, where various localized\nexplanation techniques reveal that multiple unrelated features influence the\ndecisions, thereby undermining interpretability. To address these challenges,\nwe develop a Bayesian Non-negative Decision Layer (BNDL), which reformulates\ndeep neural networks as a conditional Bayesian non-negative factor analysis. By\nleveraging stochastic latent variables, the BNDL can model complex dependencies\nand provide robust uncertainty estimation. Moreover, the sparsity and\nnon-negativity of the latent variables encourage the model to learn\ndisentangled representations and decision layers, thereby improving\ninterpretability. We also offer theoretical guarantees that BNDL can achieve\neffective disentangled learning. In addition, we developed a corresponding\nvariational inference method utilizing a Weibull variational inference network\nto approximate the posterior distribution of the latent variables. Our\nexperimental results demonstrate that with enhanced disentanglement\ncapabilities, BNDL not only improves the model's accuracy but also provides\nreliable uncertainty estimation and improved interpretability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8d1d\u53f6\u65af\u975e\u8d1f\u51b3\u7b56\u5c42\uff08BNDL\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u65b0\u8868\u8ff0\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e3a\u6761\u4ef6\u8d1d\u53f6\u65af\u975e\u8d1f\u56e0\u5b50\u5206\u6790\uff0c\u89e3\u51b3\u4e86\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u867d\u7136\u8868\u8fbe\u80fd\u529b\u5f3a\uff0c\u4f46\u96be\u4ee5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u4e2d\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u9700\u6c42\uff0c\u4e14\u5176\u8026\u5408\u7279\u6027\u5bfc\u81f4\u51b3\u7b56\u53d7\u65e0\u5173\u7279\u5f81\u5f71\u54cd\uff0c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u3002", "method": "BNDL\u5229\u7528\u968f\u673a\u6f5c\u5728\u53d8\u91cf\u5efa\u6a21\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\uff0c\u901a\u8fc7\u5b66\u4e60\u7a00\u758f\u3001\u975e\u8d1f\u7684\u6f5c\u5728\u53d8\u91cf\u5b9e\u73b0\u89e3\u8026\u8868\u793a\u548c\u51b3\u7b56\u5c42\u3002\u91c7\u7528\u4e86\u57fa\u4e8eWeibull\u7684\u53d8\u5206\u63a8\u65ad\u7f51\u7edc\u8fd1\u4f3c\u6f5c\u5728\u53d8\u91cf\u540e\u9a8c\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBNDL\u589e\u5f3a\u4e86\u6a21\u578b\u89e3\u8026\u80fd\u529b\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u8fd8\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "BNDL\u901a\u8fc7\u7ed3\u5408\u8d1d\u53f6\u65af\u6846\u67b6\u548c\u7ed3\u6784\u7ea6\u675f\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "keywords": "\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1, \u53ef\u89e3\u91ca\u6027, \u8d1d\u53f6\u65af\u975e\u8d1f\u51b3\u7b56\u5c42, \u89e3\u8026\u8868\u793a, \u53d8\u5206\u63a8\u65ad"}}
{"id": "2505.22517", "pdf": "https://arxiv.org/pdf/2505.22517", "abs": "https://arxiv.org/abs/2505.22517", "authors": ["Yimeng Gu", "Zhao Tong", "Ignacio Castro", "Shu Wu", "Gareth Tyson"], "title": "Multi-MLLM Knowledge Distillation for Out-of-Context News Detection", "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "Multimodal out-of-context news is a type of misinformation in which the image\nis used outside of its original context. Many existing works have leveraged\nmultimodal large language models (MLLMs) for detecting out-of-context news.\nHowever, observing the limited zero-shot performance of smaller MLLMs, they\ngenerally require label-rich fine-tuning and/or expensive API calls to GPT\nmodels to improve the performance, which is impractical in low-resource\nscenarios. In contrast, we aim to improve the performance of small MLLMs in a\nmore label-efficient and cost-effective manner. To this end, we first prompt\nmultiple teacher MLLMs to generate both label predictions and corresponding\nrationales, which collectively serve as the teachers' knowledge. We then\nintroduce a two-stage knowledge distillation framework to transfer this\nknowledge to a student MLLM. In Stage 1, we apply LoRA fine-tuning to the\nstudent model using all training data. In Stage 2, we further fine-tune the\nstudent model using both LoRA fine-tuning and DPO on the data points where\nteachers' predictions conflict. This two-stage strategy reduces annotation\ncosts and helps the student model uncover subtle patterns in more challenging\ncases. Experimental results demonstrate that our approach achieves\nstate-of-the-art performance using less than 10% labeled data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6559\u5e08\u6a21\u578b\u751f\u6210\u9884\u6d4b\u548c\u89e3\u91ca\u6765\u9ad8\u6548\u63d0\u5347\u5c0f\u578b\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u8131\u79bb\u4e0a\u4e0b\u6587\u65b0\u95fb\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u4ec5\u9700\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u5373\u53ef\u8fbe\u5230\u6700\u4f73\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u6216\u6602\u8d35API\u8c03\u7528\uff0c\u4e0d\u9002\u5408\u4f4e\u8d44\u6e90\u573a\u666f\uff0c\u56e0\u6b64\u7814\u7a76\u5982\u4f55\u5728\u66f4\u9ad8\u6548\u548c\u4f4e\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u5c0f\u578b\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u9996\u5148\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u751f\u6210\u9884\u6d4b\u548c\u89e3\u91ca\u4f5c\u4e3a\u77e5\u8bc6\uff0c\u7136\u540e\u5206\u4e24\u9636\u6bb5\uff08LoRA\u5fae\u8c03\u548cDPO\u5fae\u8c03\uff09\u5c06\u77e5\u8bc6\u84b8\u998f\u5230\u5b66\u751f\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u6559\u5e08\u9884\u6d4b\u51b2\u7a81\u7684\u6570\u636e\u70b9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4ec5\u9700\u4e0d\u523010%\u7684\u6807\u6ce8\u6570\u636e\u5373\u53ef\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u84b8\u998f\u65b9\u6cd5\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u578b\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u6a21\u578b\u3001\u8131\u79bb\u4e0a\u4e0b\u6587\u65b0\u95fb\u3001\u77e5\u8bc6\u84b8\u998f\u3001\u4f4e\u8d44\u6e90\u3001\u5fae\u8c03"}}
{"id": "2505.22203", "pdf": "https://arxiv.org/pdf/2505.22203", "abs": "https://arxiv.org/abs/2505.22203", "authors": ["Yuzhen Huang", "Weihao Zeng", "Xingshan Zeng", "Qi Zhu", "Junxian He"], "title": "Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Trustworthy verifiers are essential for the success of reinforcement learning\nwith verifiable reward (RLVR), which is the core methodology behind various\nlarge reasoning models such as DeepSeek-R1. In complex domains like\nmathematical reasoning, rule-based verifiers have been widely adopted in\nprevious works to train strong reasoning models. However, the reliability of\nthese verifiers and their impact on the RL training process remain poorly\nunderstood. In this work, we take mathematical reasoning as a case study and\nconduct a comprehensive analysis of various verifiers in both static evaluation\nand RL training scenarios. First, we find that current open-source rule-based\nverifiers often fail to recognize equivalent answers presented in different\nformats across multiple commonly used mathematical datasets, resulting in\nnon-negligible false negative rates. This limitation adversely affects RL\ntraining performance and becomes more pronounced as the policy model gets\nstronger. Subsequently, we investigate model-based verifiers as a potential\nsolution to address these limitations. While the static evaluation shows that\nmodel-based verifiers achieve significantly higher verification accuracy,\nfurther analysis and RL training results imply that they are highly susceptible\nto hacking, where they misclassify certain patterns in responses as correct\n(i.e., false positives). This vulnerability is exploited during policy model\noptimization, leading to artificially inflated rewards. Our findings underscore\nthe unique risks inherent to both rule-based and model-based verifiers, aiming\nto offer valuable insights to develop more robust reward systems in\nreinforcement learning.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5206\u6790\u4e86\u5f3a\u5316\u5b66\u4e60\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u4e2d\u4e24\u7c7b\u9a8c\u8bc1\u5668\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0\u57fa\u4e8e\u89c4\u5219\u7684\u9a8c\u8bc1\u5668\u6613\u4ea7\u751f\u5047\u9634\u6027\u9519\u8bef\uff0c\u800c\u57fa\u4e8e\u6a21\u578b\u7684\u9a8c\u8bc1\u5668\u5219\u6613\u88ab\u653b\u51fb\u5bfc\u81f4\u5047\u9633\u6027\u9519\u8bef\uff0c\u5f71\u54cd\u4e86\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u8ba8\u5f3a\u5316\u5b66\u4e60\u53ef\u9a8c\u8bc1\u5956\u52b1\u4e2d\u9a8c\u8bc1\u5668\u7684\u53ef\u9760\u6027\u53ca\u5176\u5bf9\u8bad\u7ec3\u8fc7\u7a0b\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u6570\u5b66\u63a8\u7406\u7b49\u590d\u6742\u9886\u57df\u4e2d\u3002", "method": "\u4ee5\u6570\u5b66\u63a8\u7406\u4e3a\u4f8b\uff0c\u901a\u8fc7\u9759\u6001\u8bc4\u4f30\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u573a\u666f\u5bf9\u6bd4\u5206\u6790\u4e86\u57fa\u4e8e\u89c4\u5219\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u9a8c\u8bc1\u5668\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u53d1\u73b0\u57fa\u4e8e\u89c4\u5219\u7684\u9a8c\u8bc1\u5668\u5e38\u8bef\u5224\u4e0d\u540c\u683c\u5f0f\u7684\u7b49\u4ef7\u503c\u6848\uff0c\u800c\u57fa\u4e8e\u6a21\u578b\u7684\u9a8c\u8bc1\u5668\u867d\u9759\u6001\u8bc4\u4f30\u51c6\u786e\u5ea6\u9ad8\uff0c\u4f46\u6613\u53d7\u653b\u51fb\u5bfc\u81f4\u5047\u9633\u6027\u9519\u8bef\uff0c\u8fdb\u800c\u5f71\u54cd\u8bad\u7ec3\u7ed3\u679c\u7684\u771f\u5b9e\u6027\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u4e24\u7c7b\u9a8c\u8bc1\u5668\u5404\u6709\u56fa\u6709\u98ce\u9669\uff0c\u4e3a\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60,\u53ef\u9a8c\u8bc1\u5956\u52b1,\u9a8c\u8bc1\u5668,\u6570\u5b66\u63a8\u7406,\u53ef\u9760\u6027"}}
{"id": "2505.22548", "pdf": "https://arxiv.org/pdf/2505.22548", "abs": "https://arxiv.org/abs/2505.22548", "authors": ["Changhao Song", "Yazhou Zhang", "Peng Zhang"], "title": "Emotion-o1: Adaptive Long Reasoning for Emotion Understanding in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Emotion understanding includes basic tasks (e.g., sentiment/emotion\nclassification) and advanced tasks (e.g., sarcasm/humor detection). Current\nmethods rely on fixed-length CoT reasoning, failing to adapt to the varying\ncomplexity of emotions. We propose a task-adaptive reasoning framework that\nemploys DeepSeek-R1 to generate variable-length reasoning chains for different\nemotion tasks. By combining fine-tuning with reinforcement learning, we design\na composite reward function that balances four objectives: prediction accuracy,\nadaptive reasoning depth control, structural diversity in reasoning paths, and\nsuppression of repetitive logic. This approach achieves dynamic\ncontext-sensitive inference while enabling LLMs to autonomously develop deep\nreasoning capabilities. Experimental results demonstrate consistent\nimprovements in both Acc and F1 scores across four tasks: emotion, sentiment,\nhumor, and sarcasm. Notably, peak enhancements reached 3.56% F1 (2.76% Acc) for\nbasic tasks and 37.95% F1 (23.14% Acc) for advanced tasks. Our work bridges\nrigid CoT reasoning and emotional complexity through adaptive-depth analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4efb\u52a1\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6DeepSeek-R1\uff0c\u901a\u8fc7\u7ed3\u5408\u5fae\u8c03\u4e0e\u5f3a\u5316\u5b66\u4e60\uff0c\u52a8\u6001\u751f\u6210\u53ef\u53d8\u957f\u5ea6\u7684\u63a8\u7406\u94fe\u4ee5\u9002\u5e94\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u60c5\u611f\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7840\u4e0e\u9ad8\u7ea7\u60c5\u611f\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u56fa\u5b9a\u957f\u5ea6\u94fe\u5f0f\u63a8\u7406\uff08CoT\uff09\u7684\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u60c5\u611f\u4efb\u52a1\u7684\u590d\u6742\u5ea6\u53d8\u5316\uff0c\u4e9f\u9700\u4e00\u79cd\u81ea\u9002\u5e94\u6846\u67b6\u6765\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\u4e0e\u591a\u6837\u6027\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u7406\u89e3\u60c5\u611f\u3002", "method": "\u63d0\u51fa\u7ed3\u5408\u5fae\u8c03\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u4efb\u52a1\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u590d\u5408\u5956\u52b1\u51fd\u6570\u5e73\u8861\u9884\u6d4b\u51c6\u786e\u6027\u3001\u63a8\u7406\u6df1\u5ea6\u63a7\u5236\u3001\u8def\u5f84\u591a\u6837\u6027\u548c\u903b\u8f91\u91cd\u590d\u6291\u5236\u56db\u5927\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u60c5\u611f\u3001\u60c5\u7eea\u3001\u5e7d\u9ed8\u548c\u8bbd\u523a\u56db\u9879\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u6301\u7eed\u63d0\u5347\uff0c\u57fa\u7840\u4efb\u52a1F1\u5cf0\u503c\u63d0\u53473.56%\uff08Acc 2.76%\uff09\uff0c\u9ad8\u7ea7\u4efb\u52a1F1\u5cf0\u503c\u63d0\u534737.95%\uff08Acc 23.14%\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u81ea\u9002\u5e94\u6df1\u5ea6\u5206\u6790\u5f25\u5408\u4e86\u56fa\u5b9aCoT\u63a8\u7406\u4e0e\u60c5\u611f\u590d\u6742\u6027\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3aLLM\u7684\u6df1\u5ea6\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u60c5\u611f\u7406\u89e3\uff0c\u81ea\u9002\u5e94\u63a8\u7406\uff0c\u5f3a\u5316\u5b66\u4e60\uff0c\u6df1\u5ea6\u63a8\u7406\uff0c\u591a\u76ee\u6807\u4f18\u5316"}}
{"id": "2505.22208", "pdf": "https://arxiv.org/pdf/2505.22208", "abs": "https://arxiv.org/abs/2505.22208", "authors": ["Yosuke Oyama", "Yusuke Majima", "Eiji Ohta", "Yasufumi Sakai"], "title": "LaMM: Semi-Supervised Pre-Training of Large-Scale Materials Models", "categories": ["cs.LG"], "comment": "24 pages, 9 figures", "summary": "Neural network potentials (NNPs) are crucial for accelerating computational\nmaterials science by surrogating density functional theory (DFT) calculations.\nImproving their accuracy is possible through pre-training and fine-tuning,\nwhere an NNP model is first pre-trained on a large-scale dataset and then\nfine-tuned on a smaller target dataset. However, this approach is\ncomputationally expensive, mainly due to the cost of DFT-based dataset labeling\nand load imbalances during large-scale pre-training. To address this, we\npropose LaMM, a semi-supervised pre-training method incorporating improved\ndenoising self-supervised learning and a load-balancing algorithm for efficient\nmulti-node training. We demonstrate that our approach effectively leverages a\nlarge-scale dataset of $\\sim$300 million semi-labeled samples to train a single\nNNP model, resulting in improved fine-tuning performance in terms of both speed\nand accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLaMM\u7684\u534a\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u53bb\u566a\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8d1f\u8f7d\u5747\u8861\u7b97\u6cd5\uff0c\u6709\u6548\u5229\u7528\u5927\u89c4\u6a21\u534a\u6807\u8bb0\u6570\u636e\u96c6\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u52bf\u80fd\uff08NNP\uff09\u6a21\u578b\uff0c\u63d0\u5347\u5fae\u8c03\u7684\u6548\u7387\u548c\u7cbe\u5ea6\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u52bf\u80fd\uff08NNP\uff09\u662f\u52a0\u901f\u8ba1\u7b97\u6750\u6599\u79d1\u5b66\u7684\u5173\u952e\u6280\u672f\uff0c\u4f46\u4f20\u7edf\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e3b\u8981\u7531\u4e8eDFT\u6807\u8bb0\u548c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4e2d\u7684\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51faLaMM\u65b9\u6cd5\uff0c\u7ed3\u5408\u6539\u8fdb\u7684\u53bb\u566a\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8d1f\u8f7d\u5747\u8861\u7b97\u6cd5\uff0c\u652f\u6301\u591a\u8282\u70b9\u9ad8\u6548\u8bad\u7ec3\uff0c\u5e76\u5229\u7528\u7ea63\u4ebf\u534a\u6807\u8bb0\u6837\u672c\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLaMM\u80fd\u663e\u8457\u63d0\u5347\u5fae\u8c03\u9636\u6bb5\u7684\u8bad\u7ec3\u901f\u5ea6\u548c\u7cbe\u5ea6\u3002", "conclusion": "LaMM\u65b9\u6cd5\u4e3aNNP\u7684\u9ad8\u6548\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e73\u8861\u4e86\u8ba1\u7b97\u6210\u672c\u4e0e\u6a21\u578b\u6027\u80fd\u3002", "keywords": "\u795e\u7ecf\u7f51\u7edc\u52bf\u80fd\u3001\u534a\u76d1\u7763\u5b66\u4e60\u3001\u9884\u8bad\u7ec3\u3001\u8d1f\u8f7d\u5747\u8861\u3001\u6750\u6599\u79d1\u5b66"}}
{"id": "2505.21609", "pdf": "https://arxiv.org/pdf/2505.21609", "abs": "https://arxiv.org/abs/2505.21609", "authors": ["Mathew J. Walter", "Aaron Barrett", "Kimberly Tam"], "title": "Preventing Adversarial AI Attacks Against Autonomous Situational Awareness: A Maritime Case Study", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Adversarial artificial intelligence (AI) attacks pose a significant threat to\nautonomous transportation, such as maritime vessels, that rely on AI\ncomponents. Malicious actors can exploit these systems to deceive and\nmanipulate AI-driven operations. This paper addresses three critical research\nchallenges associated with adversarial AI: the limited scope of traditional\ndefences, inadequate security metrics, and the need to build resilience beyond\nmodel-level defences. To address these challenges, we propose building defences\nutilising multiple inputs and data fusion to create defensive components and an\nAI security metric as a novel approach toward developing more secure AI\nsystems. We name this approach the Data Fusion Cyber Resilience (DFCR) method,\nand we evaluate it through real-world demonstrations and comprehensive\nquantitative analyses, comparing a system built with the DFCR method against\nsingle-input models and models utilising existing state-of-the-art defences.\nThe findings show that the DFCR approach significantly enhances resilience\nagainst adversarial machine learning attacks in maritime autonomous system\noperations, achieving up to a 35\\% reduction in loss for successful\nmulti-pronged perturbation attacks, up to a 100\\% reduction in loss for\nsuccessful adversarial patch attacks and up to 100\\% reduction in loss for\nsuccessful spoofing attacks when using these more resilient systems. We\ndemonstrate how DFCR and DFCR confidence scores can reduce adversarial AI\ncontact confidence and improve decision-making by the system, even when typical\nadversarial defences have been compromised. Ultimately, this work contributes\nto the development of more secure and resilient AI-driven systems against\nadversarial attacks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDFCR\u7684\u65b0\u578b\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u8f93\u5165\u548c\u6570\u636e\u878d\u5408\u589e\u5f3aAI\u7cfb\u7edf\u5bf9\u6297\u5bf9\u6297\u6027\u653b\u51fb\u7684\u97e7\u6027\uff0c\u5728\u6d77\u4e0a\u81ea\u4e3b\u7cfb\u7edf\u7684\u5b9e\u9645\u6d4b\u8bd5\u4e2d\u663e\u8457\u964d\u4f4e\u4e86\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u5bf9\u6297\u6027AI\u653b\u51fb\u5bf9\u4f9d\u8d56AI\u7684\u81ea\u4e3b\u8fd0\u8f93\u7cfb\u7edf\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u8303\u56f4\u6709\u9650\u3001\u5b89\u5168\u6307\u6807\u4e0d\u8db3\uff0c\u4e9f\u9700\u5728\u6a21\u578b\u5c42\u9762\u4e4b\u5916\u6784\u5efa\u66f4\u5f3a\u7684\u97e7\u6027\u3002", "method": "\u91c7\u7528\u591a\u8f93\u5165\u548c\u6570\u636e\u878d\u5408\u6280\u672f\u6784\u5efa\u9632\u5fa1\u7ec4\u4ef6\uff0c\u5e76\u63d0\u51faAI\u5b89\u5168\u6307\u6807\uff0c\u8be5\u65b9\u6cd5\u79f0\u4e3a\u6570\u636e\u878d\u5408\u7f51\u7edc\u97e7\u6027(DFCR)\u3002", "result": "DFCR\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u653b\u51fb\u7684\u97e7\u6027\uff1a\u591a\u89d2\u5ea6\u6270\u52a8\u653b\u51fb\u635f\u5931\u51cf\u5c1135%\uff0c\u5bf9\u6297\u8865\u4e01\u653b\u51fb\u548c\u6b3a\u9a97\u653b\u51fb\u635f\u5931\u51cf\u5c11100%\u3002", "conclusion": "DFCR\u4e0d\u4ec5\u964d\u4f4e\u4e86\u5bf9\u6297\u6027AI\u7684\u63a5\u89e6\u4fe1\u5fc3\uff0c\u8fd8\u63d0\u5347\u4e86\u7cfb\u7edf\u51b3\u7b56\u80fd\u529b\uff0c\u4e3a\u5f00\u53d1\u66f4\u5b89\u5168\u3001\u66f4\u6709\u97e7\u6027\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "\u5bf9\u6297\u6027AI, \u81ea\u4e3b\u8fd0\u8f93, \u6570\u636e\u878d\u5408, \u7f51\u7edc\u97e7\u6027, \u5b89\u5168\u6307\u6807"}}
{"id": "2505.22552", "pdf": "https://arxiv.org/pdf/2505.22552", "abs": "https://arxiv.org/abs/2505.22552", "authors": ["Hoang Pham", "Thanh-Do Nguyen", "Khac-Hoai Nam Bui"], "title": "ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized LLM", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "Accepted by ACL 2025 findings", "summary": "Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of\nlarge language models (LLMs) is an emerging research challenge in claim\nverification. While KGs provide structured, semantically rich representations\nwell-suited for reasoning, most existing verification methods rely on\nunstructured text corpora, limiting their ability to effectively leverage KGs.\nAdditionally, despite possessing strong reasoning abilities, modern LLMs\nstruggle with multi-step modular pipelines and reasoning over KGs without\nadaptation. To address these challenges, we propose ClaimPKG, an end-to-end\nframework that seamlessly integrates LLM reasoning with structured knowledge\nfrom KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight,\nspecialized LLM to represent the input claim as pseudo-subgraphs, guiding a\ndedicated subgraph retrieval module to identify relevant KG subgraphs. These\nretrieved subgraphs are then processed by a general-purpose LLM to produce the\nfinal verdict and justification. Extensive experiments on the FactKG dataset\ndemonstrate that ClaimPKG achieves state-of-the-art performance, outperforming\nstrong baselines in this research field by 9%-12% accuracy points across\nmultiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability\nto unstructured datasets such as HoVer and FEVEROUS, effectively combining\nstructured knowledge from KGs with LLM reasoning across various LLM backbones.", "AI": {"tldr": "ClaimPKG\u901a\u8fc7\u5c06\u77e5\u8bc6\u56fe\u8c31\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u58f0\u660e\u786e\u8ba4\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u58f0\u660e\u786e\u8ba4\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u7ed3\u6784\u5316\u77e5\u8bc6\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6b65\u63a8\u7406\u548c\u77e5\u8bc6\u56fe\u8c31\u5e94\u7528\u4e2d\u7684\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u4e13\u7528\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4f2a\u5b50\u56fe\uff0c\u7ed3\u5408\u5b50\u56fe\u68c0\u7d22\u6a21\u5757\u63d0\u53d6\u76f8\u5173\u5b50\u56fe\uff0c\u518d\u7531\u901a\u7528\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6700\u7ec8\u5224\u65ad\u548c\u89e3\u91ca\u3002", "result": "\u5728FactKG\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd59%-12%\uff0c\u540c\u65f6\u5728HoVer\u548cFEVEROUS\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u51fa\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ClaimPKG\u6210\u529f\u5c06\u7ed3\u6784\u5316\u77e5\u8bc6\u4e0e\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7ed3\u5408\uff0c\u4e3a\u58f0\u660e\u786e\u8ba4\u9886\u57df\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u77e5\u8bc6\u56fe\u8c31,\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u58f0\u660e\u786e\u8ba4,\u96f6\u6837\u672c\u5b66\u4e60,\u4f2a\u5b50\u56fe"}}
{"id": "2505.22224", "pdf": "https://arxiv.org/pdf/2505.22224", "abs": "https://arxiv.org/abs/2505.22224", "authors": ["Senne Berden", "Ali \u0130rfan Mahmuto\u011fullar\u0131", "Dimos Tsouros", "Tias Guns"], "title": "Solver-Free Decision-Focused Learning for Linear Optimization Problems", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Mathematical optimization is a fundamental tool for decision-making in a wide\nrange of applications. However, in many real-world scenarios, the parameters of\nthe optimization problem are not known a priori and must be predicted from\ncontextual features. This gives rise to predict-then-optimize problems, where a\nmachine learning model predicts problem parameters that are then used to make\ndecisions via optimization. A growing body of work on decision-focused learning\n(DFL) addresses this setting by training models specifically to produce\npredictions that maximize downstream decision quality, rather than accuracy.\nWhile effective, DFL is computationally expensive, because it requires solving\nthe optimization problem with the predicted parameters at each loss evaluation.\nIn this work, we address this computational bottleneck for linear optimization\nproblems, a common class of problems in both DFL literature and real-world\napplications. We propose a solver-free training method that exploits the\ngeometric structure of linear optimization to enable efficient training with\nminimal degradation in solution quality. Our method is based on the insight\nthat a solution is optimal if and only if it achieves an objective value that\nis at least as good as that of its adjacent vertices on the feasible polytope.\nBuilding on this, our method compares the estimated quality of the ground-truth\noptimal solution with that of its precomputed adjacent vertices, and uses this\nas loss function. Experiments demonstrate that our method significantly reduces\ncomputational cost while maintaining high decision quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u7ebf\u6027\u4f18\u5316\u95ee\u9898\u7684\u65e0\u6c42\u89e3\u5668\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u51e0\u4f55\u7ed3\u6784\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u51b3\u7b56\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u51b3\u7b56\u805a\u7126\u5b66\u4e60\uff08DFL\uff09\u4e2d\u56e0\u6bcf\u6b21\u635f\u5931\u8bc4\u4f30\u9700\u89e3\u51b3\u4f18\u5316\u95ee\u9898\u5bfc\u81f4\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "method": "\u5229\u7528\u7ebf\u6027\u4f18\u5316\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u901a\u8fc7\u6bd4\u8f83\u771f\u5b9e\u6700\u4f18\u89e3\u4e0e\u5176\u76f8\u90bb\u9876\u70b9\u7684\u4f30\u8ba1\u8d28\u91cf\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\uff0c\u907f\u514d\u6c42\u89e3\u4f18\u5316\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u51b3\u7b56\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7ebf\u6027\u4f18\u5316\u95ee\u9898\u7684\u51b3\u7b56\u805a\u7126\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "keywords": ""}}
{"id": "2505.21620", "pdf": "https://arxiv.org/pdf/2505.21620", "abs": "https://arxiv.org/abs/2505.21620", "authors": ["Zhengyuan Jiang", "Moyang Guo", "Kecen Li", "Yuepeng Hu", "Yupu Wang", "Zhicong Huang", "Cheng Hong", "Neil Zhenqiang Gong"], "title": "VideoMarkBench: Benchmarking Robustness of Video Watermarking", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "The rapid development of video generative models has led to a surge in highly\nrealistic synthetic videos, raising ethical concerns related to disinformation\nand copyright infringement. Recently, video watermarking has been proposed as a\nmitigation strategy by embedding invisible marks into AI-generated videos to\nenable subsequent detection. However, the robustness of existing video\nwatermarking methods against both common and adversarial perturbations remains\nunderexplored. In this work, we introduce VideoMarkBench, the first systematic\nbenchmark designed to evaluate the robustness of video watermarks under\nwatermark removal and watermark forgery attacks. Our study encompasses a\nunified dataset generated by three state-of-the-art video generative models,\nacross three video styles, incorporating four watermarking methods and seven\naggregation strategies used during detection. We comprehensively evaluate 12\ntypes of perturbations under white-box, black-box, and no-box threat models.\nOur findings reveal significant vulnerabilities in current watermarking\napproaches and highlight the urgent need for more robust solutions. Our code is\navailable at https://github.com/zhengyuan-jiang/VideoMarkBench.", "AI": {"tldr": "VideoMarkBench\u662f\u9996\u4e2a\u7cfb\u7edf\u8bc4\u4f30\u89c6\u9891\u6c34\u5370\u5728\u79fb\u9664\u548c\u4f2a\u9020\u653b\u51fb\u4e0b\u9c81\u68d2\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u591a\u79cd\u751f\u6210\u6a21\u578b\u3001\u98ce\u683c\u3001\u6c34\u5370\u65b9\u6cd5\u548c\u5a01\u80c1\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u9488\u5bf9AI\u751f\u6210\u89c6\u9891\u7684\u6c34\u5370\u6280\u672f\u7f3a\u4e4f\u5bf9\u5e38\u89c1\u548c\u5bf9\u6297\u6027\u6270\u52a8\u7684\u9c81\u68d2\u6027\u7814\u7a76\uff0c\u9700\u7cfb\u7edf\u6027\u8bc4\u4f30\u4ee5\u89e3\u51b3\u4f26\u7406\u95ee\u9898\u3002", "method": "\u6784\u5efaVideoMarkBench\u57fa\u51c6\uff0c\u6574\u5408\u4e09\u79cd\u89c6\u9891\u751f\u6210\u6a21\u578b\u3001\u56db\u79cd\u6c34\u5370\u65b9\u6cd5\u3001\u4e03\u79cd\u68c0\u6d4b\u7b56\u7565\uff0c\u5e76\u5728\u4e09\u79cd\u5a01\u80c1\u6a21\u578b\u4e0b\u6d4b\u8bd512\u7c7b\u6270\u52a8\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u5b58\u5728\u663e\u8457\u6f0f\u6d1e\uff0c\u5c24\u5176\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u8868\u73b0\u8106\u5f31\u3002", "conclusion": "\u4e9f\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u89c6\u9891\u6c34\u5370\u65b9\u6848\uff0c\u4ee5\u5e94\u5bf9\u65e5\u76ca\u589e\u957f\u7684\u4f26\u7406\u6311\u6218\u3002", "keywords": "\u89c6\u9891\u6c34\u5370, \u9c81\u68d2\u6027\u8bc4\u4f30, AI\u751f\u6210\u89c6\u9891, \u5bf9\u6297\u653b\u51fb, \u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2505.22563", "pdf": "https://arxiv.org/pdf/2505.22563", "abs": "https://arxiv.org/abs/2505.22563", "authors": ["Yu Lei", "Xingyang Ge", "Yi Zhang", "Yiming Yang", "Bolei Ma"], "title": "Do Large Language Models Think Like the Brain? Sentence-Level Evidence from fMRI and Hierarchical Embeddings", "categories": ["cs.CL", "q-bio.NC"], "comment": null, "summary": "Understanding whether large language models (LLMs) and the human brain\nconverge on similar computational principles remains a fundamental and\nimportant question in cognitive neuroscience and AI. Do the brain-like patterns\nobserved in LLMs emerge simply from scaling, or do they reflect deeper\nalignment with the architecture of human language processing? This study\nfocuses on the sentence-level neural mechanisms of language models,\nsystematically investigating how hierarchical representations in LLMs align\nwith the dynamic neural responses during human sentence comprehension. By\ncomparing hierarchical embeddings from 14 publicly available LLMs with fMRI\ndata collected from participants, who were exposed to a naturalistic narrative\nstory, we constructed sentence-level neural prediction models to precisely\nidentify the model layers most significantly correlated with brain region\nactivations. Results show that improvements in model performance drive the\nevolution of representational architectures toward brain-like hierarchies,\nparticularly achieving stronger functional and anatomical correspondence at\nhigher semantic abstraction levels.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u4eba\u7c7b\u5927\u8111\u5728\u53e5\u5b50\u5904\u7406\u5c42\u9762\u7684\u76f8\u4f3c\u6027\uff0c\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u63d0\u5347\u4f1a\u63a8\u52a8\u5176\u8868\u5f81\u67b6\u6784\u5411\u7c7b\u4f3c\u5927\u8111\u7684\u5c42\u7ea7\u7ed3\u6784\u6f14\u5316\uff0c\u5c24\u5176\u5728\u9ad8\u7ea7\u8bed\u4e49\u62bd\u8c61\u5c42\u9762\u3002", "motivation": "\u63a2\u8ba8LLMs\u4e0e\u4eba\u7c7b\u5927\u8111\u662f\u5426\u5728\u8ba1\u7b97\u539f\u7406\u4e0a\u8d8b\u540c\uff0c\u5c24\u5176\u662f\u6a21\u578b\u4e2d\u7684\u5c42\u7ea7\u8868\u5f81\u662f\u5426\u4e0e\u4eba\u7c7b\u53e5\u5b50\u7406\u89e3\u65f6\u7684\u795e\u7ecf\u54cd\u5e94\u52a8\u6001\u5bf9\u9f50\u3002", "method": "\u901a\u8fc7\u6bd4\u8f8314\u4e2a\u516c\u5f00LLMs\u7684\u5c42\u7ea7\u5d4c\u5165\u4e0e\u4eba\u7c7b\u88ab\u8bd5\u5728\u81ea\u7136\u53d9\u4e8b\u6545\u4e8b\u4e2d\u7684fMRI\u6570\u636e\uff0c\u6784\u5efa\u53e5\u5b50\u7ea7\u795e\u7ecf\u9884\u6d4b\u6a21\u578b\uff0c\u5206\u6790\u6a21\u578b\u5c42\u4e0e\u8111\u533a\u6fc0\u6d3b\u7684\u76f8\u5173\u6027\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u6027\u80fd\u7684\u63d0\u5347\u4f1a\u4f7f\u5176\u8868\u5f81\u67b6\u6784\u66f4\u63a5\u8fd1\u5927\u8111\u7684\u5c42\u7ea7\u7ed3\u6784\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u7ea7\u8bed\u4e49\u62bd\u8c61\u5c42\u9762\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u529f\u80fd\u548c\u89e3\u5256\u5bf9\u5e94\u5173\u7cfb\u3002", "conclusion": "LLMs\u7684\u8868\u5f81\u67b6\u6784\u5728\u8fdb\u5316\u8fc7\u7a0b\u4e2d\u9010\u6e10\u4e0e\u4eba\u7c7b\u8bed\u8a00\u5904\u7406\u7684\u795e\u7ecf\u673a\u5236\u5bf9\u9f50\uff0c\u652f\u6301\u4e24\u8005\u5728\u8ba1\u7b97\u539f\u7406\u4e0a\u7684\u6f5c\u5728\u76f8\u4f3c\u6027\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u4eba\u7c7b\u5927\u8111, \u5c42\u7ea7\u8868\u5f81, \u53e5\u5b50\u7406\u89e3, fMRI, \u8bed\u4e49\u62bd\u8c61"}}
{"id": "2505.22235", "pdf": "https://arxiv.org/pdf/2505.22235", "abs": "https://arxiv.org/abs/2505.22235", "authors": ["Amon Lahr", "Johannes K\u00f6hler", "Anna Scampicchio", "Melanie N. Zeilinger"], "title": "Optimal kernel regression bounds under energy-bounded noise", "categories": ["cs.LG"], "comment": null, "summary": "Non-conservative uncertainty bounds are key for both assessing an estimation\nalgorithm's accuracy and in view of downstream tasks, such as its deployment in\nsafety-critical contexts. In this paper, we derive a tight, non-asymptotic\nuncertainty bound for kernel-based estimation, which can also handle correlated\nnoise sequences. Its computation relies on a mild norm-boundedness assumption\non the unknown function and the noise, returning the worst-case function\nrealization within the hypothesis class at an arbitrary query input location.\nThe value of this function is shown to be given in terms of the posterior mean\nand covariance of a Gaussian process for an optimal choice of the measurement\nnoise covariance. By rigorously analyzing the proposed approach and comparing\nit with other results in the literature, we show its effectiveness in returning\ntight and easy-to-compute bounds for kernel-based estimates.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e25\u683c\u7684\u975e\u6e10\u8fdb\u4e0d\u786e\u5b9a\u6027\u8fb9\u754c\uff0c\u9002\u7528\u4e8e\u57fa\u4e8e\u6838\u7684\u4f30\u8ba1\uff0c\u5e76\u80fd\u5904\u7406\u76f8\u5173\u566a\u58f0\u5e8f\u5217\uff0c\u5176\u8ba1\u7b97\u4f9d\u8d56\u4e8e\u5bf9\u672a\u77e5\u51fd\u6570\u548c\u566a\u58f0\u7684\u8303\u6570\u6709\u754c\u5047\u8bbe\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e3a\u4e86\u63d0\u4f9b\u4e00\u79cd\u975e\u4fdd\u5b88\u7684\u4e0d\u786e\u5b9a\u6027\u8fb9\u754c\uff0c\u65e2\u80fd\u8bc4\u4f30\u4f30\u8ba1\u7b97\u6cd5\u7684\u51c6\u786e\u6027\uff0c\u53c8\u9002\u7528\u4e8e\u5b89\u5168\u5173\u952e\u573a\u666f\u7684\u4e0b\u6e38\u4efb\u52a1\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u9ad8\u65af\u8fc7\u7a0b\u7684\u540e\u9a8c\u5747\u503c\u548c\u534f\u65b9\u5dee\uff0c\u5728\u6d4b\u91cf\u566a\u58f0\u534f\u65b9\u5dee\u6700\u4f18\u9009\u62e9\u4e0b\uff0c\u8fd4\u56de\u5047\u8bbe\u7c7b\u4e2d\u7684\u6700\u574f\u60c5\u51b5\u51fd\u6570\u5b9e\u73b0\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u63d0\u4f9b\u7d27\u5bc6\u4e14\u6613\u4e8e\u8ba1\u7b97\u7684\u4e0d\u786e\u5b9a\u6027\u8fb9\u754c\uff0c\u4f18\u4e8e\u6587\u732e\u4e2d\u7684\u5176\u4ed6\u7ed3\u679c\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8be5\u65b9\u6cd5\u662f\u6709\u6548\u7684\uff0c\u9002\u7528\u4e8e\u6838\u4f30\u8ba1\u7684\u4e25\u683c\u8fb9\u754c\u8ba1\u7b97\uff0c\u5e76\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u4e0d\u786e\u5b9a\u6027\u8fb9\u754c, \u6838\u4f30\u8ba1, \u9ad8\u65af\u8fc7\u7a0b, \u76f8\u5173\u566a\u58f0, \u5b89\u5168\u5173\u952e"}}
{"id": "2505.21627", "pdf": "https://arxiv.org/pdf/2505.21627", "abs": "https://arxiv.org/abs/2505.21627", "authors": ["Ander Artola Velasco", "Stratis Tsirtsis", "Nastaran Okati", "Manuel Gomez-Rodriguez"], "title": "Is Your LLM Overcharging You? Tokenization, Transparency, and Incentives", "categories": ["cs.GT", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "State-of-the-art large language models require specialized hardware and\nsubstantial energy to operate. As a consequence, cloud-based services that\nprovide access to large language models have become very popular. In these\nservices, the price users pay for an output provided by a model depends on the\nnumber of tokens the model uses to generate it -- they pay a fixed price per\ntoken. In this work, we show that this pricing mechanism creates a financial\nincentive for providers to strategize and misreport the (number of) tokens a\nmodel used to generate an output, and users cannot prove, or even know, whether\na provider is overcharging them. However, we also show that, if an unfaithful\nprovider is obliged to be transparent about the generative process used by the\nmodel, misreporting optimally without raising suspicion is hard. Nevertheless,\nas a proof-of-concept, we introduce an efficient heuristic algorithm that\nallows providers to significantly overcharge users without raising suspicion,\nhighlighting the vulnerability of users under the current pay-per-token pricing\nmechanism. Further, to completely eliminate the financial incentive to\nstrategize, we introduce a simple incentive-compatible token pricing mechanism.\nUnder this mechanism, the price users pay for an output provided by a model\ndepends on the number of characters of the output -- they pay a fixed price per\ncharacter. Along the way, to illustrate and complement our theoretical results,\nwe conduct experiments with several large language models from the\n$\\texttt{Llama}$, $\\texttt{Gemma}$ and $\\texttt{Ministral}$ families, and input\nprompts from the LMSYS Chatbot Arena platform.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u73b0\u6709\u6309\u6807\u8bb0\u4ed8\u8d39\u673a\u5236\u4e2d\u5b58\u5728\u7684\u63d0\u4f9b\u5546\u865a\u62a5\u6807\u8bb0\u6570\u4ee5\u591a\u6536\u8d39\u7528\u7684\u6f0f\u6d1e\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6309\u5b57\u7b26\u4ed8\u8d39\u7684\u65b0\u673a\u5236\u6765\u6d88\u9664\u8fd9\u4e00\u8d22\u52a1\u6fc0\u52b1\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6309\u6807\u8bb0\u6536\u8d39\u673a\u5236\u53ef\u80fd\u5bfc\u81f4\u63d0\u4f9b\u5546\u865a\u62a5\u6807\u8bb0\u6570\uff0c\u7528\u6237\u65e0\u6cd5\u9a8c\u8bc1\u5176\u771f\u5b9e\u6027\uff0c\u4ece\u800c\u88ab\u591a\u6536\u8d39\u7528\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86\u6309\u6807\u8bb0\u4ed8\u8d39\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u542f\u53d1\u5f0f\u7b97\u6cd5\u8bc1\u660e\u63d0\u4f9b\u5546\u53ef\u5982\u4f55\u865a\u62a5\u6807\u8bb0\u6570\u800c\u4e0d\u88ab\u53d1\u73b0\uff0c\u5e76\u8bbe\u8ba1\u4e86\u6309\u5b57\u7b26\u4ed8\u8d39\u7684\u6fc0\u52b1\u517c\u5bb9\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6309\u5b57\u7b26\u4ed8\u8d39\u673a\u5236\u80fd\u6709\u6548\u6d88\u9664\u63d0\u4f9b\u5546\u865a\u62a5\u6807\u8bb0\u6570\u7684\u8d22\u52a1\u6fc0\u52b1\u3002", "conclusion": "\u6309\u5b57\u7b26\u4ed8\u8d39\u662f\u4e00\u79cd\u66f4\u900f\u660e\u4e14\u6fc0\u52b1\u517c\u5bb9\u7684\u5b9a\u4ef7\u673a\u5236\uff0c\u53ef\u4fdd\u62a4\u7528\u6237\u514d\u53d7\u4e0d\u516c\u5e73\u6536\u8d39\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u5b9a\u4ef7\u673a\u5236,\u6807\u8bb0\u6570,\u5b57\u7b26\u6570,\u6fc0\u52b1\u517c\u5bb9"}}
{"id": "2505.22571", "pdf": "https://arxiv.org/pdf/2505.22571", "abs": "https://arxiv.org/abs/2505.22571", "authors": ["Hoang Pham", "Khac-Hoai Nam Bui"], "title": "Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "comment": null, "summary": "This paper presents a novel approach for unified retrieval-augmented\ngeneration (RAG) systems using the recent emerging large language model (LLM)\nagent concept. Specifically, Agent LLM, which utilizes LLM as fundamental\ncontrollers, has become a promising approach to enable the interpretability of\nRAG tasks, especially for complex reasoning question-answering systems (e.g.,\nmulti-hop queries). Nonetheless, previous works mainly focus on solving RAG\nsystems with either single-hop or multi-hop approaches separately, which limits\nthe application of those approaches to real-world applications. In this study,\nwe propose a trainable agent framework called Agent-UniRAG for unified\nretrieval-augmented LLM systems, which enhances the effectiveness and\ninterpretability of RAG systems. The main idea is to design an LLM agent\nframework to solve RAG tasks step-by-step based on the complexity of the\ninputs, simultaneously including single-hop and multi-hop queries in an\nend-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset\nto enable the proposed agent framework for small open-source LLMs (e.g.,\nLlama-3-8B). The results show comparable performances with closed-source and\nlarger open-source LLMs across various RAG benchmarks. Our source code and\ndataset are publicly available for further exploitation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u7edf\u4e00\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6846\u67b6Agent-UniRAG\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u5355\u8df3\u548c\u591a\u8df3\u67e5\u8be2\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6SynAgent-RAG\u63d0\u5347\u5c0f\u89c4\u6a21\u5f00\u6e90LLM\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u901a\u5e38\u5355\u72ec\u5904\u7406\u5355\u8df3\u6216\u591a\u8df3\u67e5\u8be2\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7LLM\u4ee3\u7406\u6846\u67b6\u7edf\u4e00\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u5347\u7cfb\u7edf\u7684\u6709\u6548\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faAgent-UniRAG\u6846\u67b6\uff0c\u901a\u8fc7LLM\u4ee3\u7406\u9010\u6b65\u89e3\u51b3\u4e0d\u540c\u590d\u6742\u5ea6\uff08\u5355\u8df3/\u591a\u8df3\uff09\u7684RAG\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u5408\u6210\u6570\u636e\u96c6SynAgent-RAG\u652f\u6301\u5c0f\u89c4\u6a21\u5f00\u6e90LLM\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAgent-UniRAG\u5728\u591a\u4e2aRAG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e0e\u95ed\u6e90\u53ca\u66f4\u5927\u89c4\u6a21\u7684\u5f00\u6e90LLM\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "Agent-UniRAG\u4e3a\u7edf\u4e00\u5904\u7406\u590d\u6742RAG\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a8\u5e7f\u3002", "keywords": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210,RAG,LLM\u4ee3\u7406,\u591a\u8df3\u67e5\u8be2,\u7edf\u4e00\u6846\u67b6"}}
{"id": "2505.22252", "pdf": "https://arxiv.org/pdf/2505.22252", "abs": "https://arxiv.org/abs/2505.22252", "authors": ["Magdalena Proszewska", "Tomasz Danel", "Dawid Rymarczyk"], "title": "B-XAIC Dataset: Benchmarking Explainable AI for Graph Neural Networks Using Chemical Data", "categories": ["cs.LG", "cs.CE"], "comment": "26 pages, 16 figures, 5 tables", "summary": "Understanding the reasoning behind deep learning model predictions is crucial\nin cheminformatics and drug discovery, where molecular design determines their\nproperties. However, current evaluation frameworks for Explainable AI (XAI) in\nthis domain often rely on artificial datasets or simplified tasks, employing\ndata-derived metrics that fail to capture the complexity of real-world\nscenarios and lack a direct link to explanation faithfulness. To address this,\nwe introduce B-XAIC, a novel benchmark constructed from real-world molecular\ndata and diverse tasks with known ground-truth rationales for assigned labels.\nThrough a comprehensive evaluation using B-XAIC, we reveal limitations of\nexisting XAI methods for Graph Neural Networks (GNNs) in the molecular domain.\nThis benchmark provides a valuable resource for gaining deeper insights into\nthe faithfulness of XAI, facilitating the development of more reliable and\ninterpretable models.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86B-XAIC\uff0c\u4e00\u4e2a\u57fa\u4e8e\u771f\u5b9e\u5206\u5b50\u6570\u636e\u548c\u591a\u79cd\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u53ef\u89e3\u91caAI\uff08XAI\uff09\u5728\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u4e2d\u7684\u80fd\u529b\u3002\u901a\u8fc7\u8fd9\u4e00\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u73b0\u6709XAI\u65b9\u6cd5\u5728\u5206\u5b50\u9886\u57df\u7684\u5c40\u9650\u6027\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u5728\u5316\u5b66\u4fe1\u606f\u5b66\u548c\u836f\u7269\u53d1\u73b0\u9886\u57df\uff0c\u5bf9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u7684\u89e3\u91ca\u6027\u8bc4\u4f30\u5927\u591a\u4f9d\u8d56\u4e8e\u4eba\u5de5\u6570\u636e\u96c6\u6216\u7b80\u5316\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5bf9\u771f\u5b9e\u573a\u666f\u590d\u6742\u6027\u7684\u6355\u6349\uff0c\u4e14\u4e0e\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\u5173\u8054\u6027\u4e0d\u5f3a\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7814\u7a76\u56e2\u961f\u63d0\u51fa\u4e86B-XAIC\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8be5\u6d4b\u8bd5\u57fa\u4e8e\u771f\u5b9e\u5206\u5b50\u6570\u636e\u548c\u591a\u6837\u4efb\u52a1\uff0c\u5e76\u4e14\u5df2\u77e5\u4e86\u6807\u7b7e\u7684ground-truth rationale\uff0c\u901a\u8fc7\u5168\u9762\u8bc4\u4f30\u63ed\u793a\u4e86\u73b0\u6709XAI\u65b9\u6cd5\u5728\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5206\u5b50\u9886\u57df\u7684\u4e0d\u8db3\u3002", "result": "B-XAIC\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u73b0\u6709XAI\u65b9\u6cd5\u5728\u5206\u5b50\u9886\u57df\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u3002", "conclusion": "B-XAIC\u586b\u8865\u4e86\u5f53\u524d\u8bc4\u4f30\u6846\u67b6\u7684\u4e0d\u8db3\uff0c\u4e3a\u63d0\u5347XAI\u5728\u5206\u5b50\u8bbe\u8ba1\u9886\u57df\u7684\u5e94\u7528\u6548\u679c\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002", "keywords": "\u53ef\u89e3\u91caAI, \u56fe\u795e\u7ecf\u7f51\u7edc, \u5206\u5b50\u8bbe\u8ba1, \u836f\u7269\u53d1\u73b0, \u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2505.21636", "pdf": "https://arxiv.org/pdf/2505.21636", "abs": "https://arxiv.org/abs/2505.21636", "authors": ["Alexander Nemecek", "Yuzhou Jiang", "Erman Ayday"], "title": "The Feasibility of Topic-Based Watermarking on Academic Peer Reviews", "categories": ["cs.CR", "cs.AI"], "comment": "8 pages main, 9 pages appendix", "summary": "Large language models (LLMs) are increasingly integrated into academic\nworkflows, with many conferences and journals permitting their use for tasks\nsuch as language refinement and literature summarization. However, their use in\npeer review remains prohibited due to concerns around confidentiality breaches,\nhallucinated content, and inconsistent evaluations. As LLM-generated text\nbecomes more indistinguishable from human writing, there is a growing need for\nreliable attribution mechanisms to preserve the integrity of the review\nprocess. In this work, we evaluate topic-based watermarking (TBW), a\nlightweight, semantic-aware technique designed to embed detectable signals into\nLLM-generated text. We conduct a comprehensive assessment across multiple LLM\nconfigurations, including base, few-shot, and fine-tuned variants, using\nauthentic peer review data from academic conferences. Our results show that TBW\nmaintains review quality relative to non-watermarked outputs, while\ndemonstrating strong robustness to paraphrasing-based evasion. These findings\nhighlight the viability of TBW as a minimally intrusive and practical solution\nfor enforcing LLM usage in peer review.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e00\u79cd\u540d\u4e3a\u4e3b\u9898\u6c34\u5370\uff08TBW\uff09\u7684\u8f7b\u91cf\u7ea7\u6280\u672f\uff0c\u7528\u4e8e\u5728LLM\u751f\u6210\u7684\u6587\u672c\u4e2d\u5d4c\u5165\u53ef\u68c0\u6d4b\u4fe1\u53f7\uff0c\u4ee5\u89e3\u51b3\u5176\u5728\u540c\u884c\u8bc4\u5ba1\u4e2d\u7684\u4f7f\u7528\u95ee\u9898\u3002\u7ed3\u679c\u663e\u793aTBW\u5728\u4fdd\u6301\u8bc4\u5ba1\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5bf9\u6539\u5199\u5177\u6709\u5f3a\u9c81\u68d2\u6027\u3002", "motivation": "\u968f\u7740LLM\u751f\u6210\u6587\u672c\u4e0e\u4eba\u7c7b\u5199\u4f5c\u96be\u4ee5\u533a\u5206\uff0c\u5982\u4f55\u786e\u4fdd\u540c\u884c\u8bc4\u5ba1\u7684\u4fdd\u5bc6\u6027\u3001\u907f\u514d\u5e7b\u89c9\u5185\u5bb9\u548c\u4e0d\u4e00\u81f4\u8bc4\u4f30\u6210\u4e3a\u8feb\u5207\u9700\u6c42\u3002", "method": "\u91c7\u7528\u4e3b\u9898\u6c34\u5370\uff08TBW\uff09\u6280\u672f\uff0c\u5e76\u5728\u591a\u79cdLLM\u914d\u7f6e\uff08\u57fa\u7840\u7248\u3001\u5c11\u6837\u672c\u7248\u548c\u5fae\u8c03\u7248\uff09\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff0c\u4f7f\u7528\u771f\u5b9e\u7684\u5b66\u672f\u4f1a\u8bae\u540c\u884c\u8bc4\u5ba1\u6570\u636e\u3002", "result": "TBW\u5728\u4fdd\u6301\u975e\u6c34\u5370\u8f93\u51fa\u7684\u8bc4\u5ba1\u8d28\u91cf\u540c\u65f6\uff0c\u8868\u73b0\u51fa\u5bf9\u6539\u5199\u653b\u51fb\u7684\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "TBW\u662f\u4e00\u79cd\u53ef\u884c\u3001\u4f4e\u4fb5\u5165\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5728\u540c\u884c\u8bc4\u5ba1\u4e2d\u89c4\u8303LLM\u7684\u4f7f\u7528\u3002", "keywords": "LLM, \u540c\u884c\u8bc4\u5ba1, \u4e3b\u9898\u6c34\u5370, \u4fdd\u5bc6\u6027, \u9c81\u68d2\u6027"}}
{"id": "2505.22572", "pdf": "https://arxiv.org/pdf/2505.22572", "abs": "https://arxiv.org/abs/2505.22572", "authors": ["Waldemar Chang", "Alhassan Yasin"], "title": "Fusion Steering: Prompt-Specific Activation Control", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 4 figures, 2 tables", "summary": "We present Fusion Steering, an activation steering methodology that improves\nfactual accuracy in large language models (LLMs) for question-answering (QA)\ntasks. This approach introduces flexible steering configurations, including\nfull-layer steering and segmented steering. Unlike traditional methods\nconstrained to single-layer or fixed-layer operations, Fusion Steering employs\ndynamic injection of prompt-specific activation deltas across all transformer\nlayers. These activation deltas are derived from reference completions that\ncombine the ground-truth answer with a model-generated explanation to\nfacilitate semantically enriched, example-specific steering. The injection\nweights are optimized per prompt using Optuna, targeting a joint objective that\nbalances token overlap (factual alignment) and perplexity (fluency proxy).\nEvaluation employs a composite score integrating token overlap and LLM-graded\nquality, encompassing factual accuracy, coherence, and relevance. Empirical\nresults on 260 SimpleQA prompts (selected from 500 where the baseline failed)\nshowcase the efficacy of segmented steering. Using Gemma-2-2B-IT with 8-bit\nquantization, segmented steering achieves an accuracy of 25.4% (outputs scoring\n$\\geq 0.6$), outperforming the baseline at 3.5% and full-layer steering at\n16.2%. Under the stricter SimpleQA rubric, segmented steering boosts fully\ncorrect responses from 0.0% to 13.1%. These findings highlight the strengths of\nsegmented, dynamic intervention strategies and the promise of per-prompt,\nfull-network activation control. Fusion Steering is also amenable to sparse\nrepresentations, such as Neuronpedia or sparse crosscoders, suggesting a\npromising direction for interpretable and scalable activation-level control in\nLLMs.", "AI": {"tldr": "Fusion Steering \u662f\u4e00\u79cd\u6fc0\u6d3b\u5bfc\u5411\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6ce8\u5165\u7279\u5b9a\u63d0\u793a\u7684\u6fc0\u6d3b\u5dee\u5f02\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9650\u4e8e\u5355\u5c42\u6216\u56fa\u5b9a\u5c42\u64cd\u4f5c\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\uff1bFusion Steering \u65e8\u5728\u901a\u8fc7\u5168\u7f51\u6fc0\u6d3b\u63a7\u5236\u548c\u4f18\u5316\u6743\u91cd\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u91c7\u7528\u52a8\u6001\u6fc0\u6d3b\u5dee\u5f02\u6ce8\u5165\uff08\u6e90\u81ea\u5305\u542b\u771f\u5b9e\u7b54\u6848\u548c\u6a21\u578b\u751f\u6210\u89e3\u91ca\u7684\u53c2\u8003\u5b8c\u6210\uff09\u53ca Optuna \u4f18\u5316\u7684\u6743\u91cd\u914d\u7f6e\uff0c\u8054\u5408\u76ee\u6807\u4e3a\u5e73\u8861\u4e8b\u5b9e\u5bf9\u9f50\u548c\u6d41\u7545\u6027\u3002", "result": "\u5728 260 \u4e2a SimpleQA \u63d0\u793a\u4e0a\uff0c\u5206\u6bb5\u5bfc\u5411\u65b9\u6cd5\u7684\u51c6\u786e\u7387\u8fbe 25.4%\uff0c\u8fdc\u8d85\u57fa\u7ebf\uff083.5%\uff09\u548c\u5168\u5c42\u5bfc\u5411\uff0816.2%\uff09\uff1b\u4e25\u683c\u6807\u51c6\u4e0b\u5b8c\u5168\u6b63\u786e\u7b54\u6848\u4ece 0% \u63d0\u5347\u81f3 13.1%\u3002", "conclusion": "\u5206\u6bb5\u52a8\u6001\u5e72\u9884\u7b56\u7565\u548c\u5168\u7f51\u6fc0\u6d3b\u63a7\u5236\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e14\u652f\u6301\u7a00\u758f\u8868\u793a\uff0c\u4e3a\u53ef\u89e3\u91ca\u3001\u53ef\u6269\u5c55\u7684\u6fc0\u6d3b\u7ea7\u63a7\u5236\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "keywords": "Fusion Steering, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u95ee\u7b54\u4efb\u52a1, \u6fc0\u6d3b\u5bfc\u5411, \u52a8\u6001\u6ce8\u5165"}}
{"id": "2505.22254", "pdf": "https://arxiv.org/pdf/2505.22254", "abs": "https://arxiv.org/abs/2505.22254", "authors": ["Xiangxiang Dai", "Xiaowei Sun", "Jinhang Zuo", "Xutong Liu", "John C. S. Lui"], "title": "A Unified Online-Offline Framework for Co-Branding Campaign Recommendations", "categories": ["cs.LG"], "comment": "Accepted at the 31st ACM SIGKDD Conference on Knowledge Discovery and\n  Data Mining, 2025", "summary": "Co-branding has become a vital strategy for businesses aiming to expand\nmarket reach within recommendation systems. However, identifying effective\ncross-industry partnerships remains challenging due to resource imbalances,\nuncertain brand willingness, and ever-changing market conditions. In this\npaper, we provide the first systematic study of this problem and propose a\nunified online-offline framework to enable co-branding recommendations. Our\napproach begins by constructing a bipartite graph linking ``initiating'' and\n``target'' brands to quantify co-branding probabilities and assess market\nbenefits. During the online learning phase, we dynamically update the graph in\nresponse to market feedback, while striking a balance between exploring new\ncollaborations for long-term gains and exploiting established partnerships for\nimmediate benefits. To address the high initial co-branding costs, our\nframework mitigates redundant exploration, thereby enhancing short-term\nperformance while ensuring sustainable strategic growth. In the offline\noptimization phase, our framework consolidates the interests of multiple\nsub-brands under the same parent brand to maximize overall returns, avoid\nexcessive investment in single sub-brands, and reduce unnecessary costs\nassociated with over-prioritizing a single sub-brand. We present a theoretical\nanalysis of our approach, establishing a highly nontrivial sublinear regret\nbound for online learning in the complex co-branding problem, and enhancing the\napproximation guarantee for the NP-hard offline budget allocation optimization.\nExperiments on both synthetic and real-world co-branding datasets demonstrate\nthe practical effectiveness of our framework, with at least 12\\% improvement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7ebf\u4e0a\u7ebf\u4e0b\u6846\u67b6\uff0c\u7528\u4e8e\u63a8\u8350\u8de8\u884c\u4e1a\u8054\u5408\u54c1\u724c\u5408\u4f5c\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u548c\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u679c\u3002", "motivation": "\u8054\u5408\u54c1\u724c\u662f\u6269\u5c55\u5e02\u573a\u7684\u5173\u952e\u7b56\u7565\uff0c\u4f46\u8de8\u884c\u4e1a\u5408\u4f5c\u7684\u8d44\u6e90\u4e0d\u5e73\u8861\u3001\u54c1\u724c\u610f\u613f\u4e0d\u786e\u5b9a\u4ee5\u53ca\u5e02\u573a\u53d8\u5316\u7b49\u95ee\u9898\u4f7f\u5176\u96be\u4ee5\u5b9e\u73b0\u3002\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u8be5\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e8c\u5206\u56fe\u91cf\u5316\u8054\u5408\u54c1\u724c\u6982\u7387\u548c\u5e02\u573a\u6536\u76ca\uff0c\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u548c\u79bb\u7ebf\u4f18\u5316\u52a8\u6001\u8c03\u6574\uff0c\u5e73\u8861\u63a2\u7d22\u65b0\u5408\u4f5c\u4e0e\u5229\u7528\u73b0\u6709\u5408\u4f5c\u7684\u5173\u7cfb\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u63d0\u5347\u4e8612%\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u80fd\u63d0\u5347\u77ed\u671f\u6548\u679c\uff0c\u8fd8\u80fd\u786e\u4fdd\u957f\u671f\u6218\u7565\u589e\u957f\u3002", "keywords": "\u8054\u5408\u54c1\u724c\u3001\u63a8\u8350\u7cfb\u7edf\u3001\u5728\u7ebf\u5b66\u4e60\u3001\u79bb\u7ebf\u4f18\u5316\u3001\u8de8\u884c\u4e1a\u5408\u4f5c"}}
{"id": "2505.22582", "pdf": "https://arxiv.org/pdf/2505.22582", "abs": "https://arxiv.org/abs/2505.22582", "authors": ["Xue Zhang", "Yunlong Liang", "Fandong Meng", "Songming Zhang", "Yufeng Chen", "Jinan Xu", "Jie Zhou"], "title": "Less, but Better: Efficient Multilingual Expansion for LLMs via Layer-wise Mixture-of-Experts", "categories": ["cs.CL"], "comment": "ACL 2025 (Main), 16 pages, 5 figures, 11 tables", "summary": "Continually expanding new languages for existing large language models (LLMs)\nis a promising yet challenging approach to building powerful multilingual LLMs.\nThe biggest challenge is to make the model continuously learn new languages\nwhile preserving the proficient ability of old languages. To achieve this,\nrecent work utilizes the Mixture-of-Experts (MoE) architecture to expand new\nlanguages by adding new experts and avoid catastrophic forgetting of old\nlanguages by routing corresponding tokens to the original model backbone (old\nexperts). Although intuitive, this kind of method is parameter-costly when\nexpanding new languages and still inevitably impacts the performance of old\nlanguages. To address these limitations, we analyze the language\ncharacteristics of different layers in LLMs and propose a layer-wise expert\nallocation algorithm (LayerMoE) to determine the appropriate number of new\nexperts for each layer. Specifically, we find different layers in LLMs exhibit\ndifferent representation similarities between languages and then utilize the\nsimilarity as the indicator to allocate experts for each layer, i.e., the\nhigher similarity, the fewer experts. Additionally, to further mitigate the\nforgetting of old languages, we add a classifier in front of the router network\non the layers with higher similarity to guide the routing of old language\ntokens. Experimental results show that our method outperforms the previous\nstate-of-the-art baseline with 60% fewer experts in the single-expansion\nsetting and with 33.3% fewer experts in the lifelong-expansion setting,\ndemonstrating the effectiveness of our method.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLayerMoE\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u4e13\u5bb6\u5206\u914d\u7b56\u7565\u548c\u5206\u7c7b\u5668\u5f15\u5bfc\u8def\u7531\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u65b0\u8bed\u8a00\u6269\u5c55\u6240\u9700\u7684\u4e13\u5bb6\u6570\u91cf\uff0c\u5e76\u6709\u6548\u7f13\u89e3\u4e86\u5bf9\u65e7\u8bed\u8a00\u7684\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6269\u5c55\u65b0\u8bed\u8a00\u65f6\uff0c\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\u4f1a\u5bfc\u81f4\u53c2\u6570\u91cf\u8fc7\u5927\uff0c\u4e14\u96be\u4ee5\u907f\u514d\u5bf9\u65e7\u8bed\u8a00\u6027\u80fd\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "method": "\u57fa\u4e8e\u4e0d\u540c\u5c42\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u76f8\u4f3c\u6027\u5206\u6790\uff0c\u63d0\u51fa\u5206\u5c42\u4e13\u5bb6\u5206\u914d\u7b97\u6cd5\uff08LayerMoE\uff09\uff0c\u5e76\u5f15\u5165\u5206\u7c7b\u5668\u5f15\u5bfc\u65e7\u8bed\u8a00\u7684\u8def\u7531\u51b3\u7b56\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5355\u6b21\u6269\u5c55\u548c\u6301\u7eed\u6269\u5c55\u573a\u666f\u4e0b\uff0c\u5206\u522b\u51cf\u5c1160%\u548c33.3%\u7684\u4e13\u5bb6\u6570\u91cf\uff0c\u540c\u65f6\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "LayerMoE\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u4e13\u5bb6\u5206\u914d\u548c\u4f18\u5316\u8def\u7531\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u8bed\u8a00\u6269\u5c55\u4e0e\u65e7\u8bed\u8a00\u4fdd\u62a4\u3002", "keywords": "\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u3001\u591a\u8bed\u8a00\u6269\u5c55\u3001\u6df7\u5408\u4e13\u5bb6\u3001\u5206\u5c42\u4e13\u5bb6\u5206\u914d\u3001\u707e\u96be\u6027\u9057\u5fd8"}}
{"id": "2505.22255", "pdf": "https://arxiv.org/pdf/2505.22255", "abs": "https://arxiv.org/abs/2505.22255", "authors": ["Vadim Kurochkin", "Yaroslav Aksenov", "Daniil Laptev", "Daniil Gavrilov", "Nikita Balagansky"], "title": "Train Sparse Autoencoders Efficiently by Utilizing Features Correlation", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Sparse Autoencoders (SAEs) have demonstrated significant promise in\ninterpreting the hidden states of language models by decomposing them into\ninterpretable latent directions. However, training SAEs at scale remains\nchallenging, especially when large dictionary sizes are used. While decoders\ncan leverage sparse-aware kernels for efficiency, encoders still require\ncomputationally intensive linear operations with large output dimensions. To\naddress this, we propose KronSAE, a novel architecture that factorizes the\nlatent representation via Kronecker product decomposition, drastically reducing\nmemory and computational overhead. Furthermore, we introduce mAND, a\ndifferentiable activation function approximating the binary AND operation,\nwhich improves interpretability and performance in our factorized framework.", "AI": {"tldr": "KronSAE\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7Kronecker\u4e58\u79ef\u5206\u89e3\u6765\u964d\u4f4e\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\u7684\u65b0\u67b6\u6784\uff0c\u5e76\u5f15\u5165\u4e86mAND\u6fc0\u6d3b\u51fd\u6570\u4ee5\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u5728\u89e3\u91ca\u8bed\u8a00\u6a21\u578b\u7684\u9690\u85cf\u72b6\u6001\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5927\u89c4\u6a21\u8bad\u7ec3\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u8ba1\u7b97\u5bc6\u96c6\u7684\u7ebf\u6027\u64cd\u4f5c\u95ee\u9898\u3002", "method": "\u63d0\u51faKronSAE\u67b6\u6784\uff0c\u5229\u7528Kronecker\u4e58\u79ef\u5206\u89e3\u56e0\u5b50\u5316\u6f5c\u5728\u8868\u793a\uff1b\u5f15\u5165mAND\u6fc0\u6d3b\u51fd\u6570\u6a21\u62df\u4e8c\u8fdb\u5236AND\u64cd\u4f5c\u4ee5\u4f18\u5316\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u663e\u8457\u51cf\u5c11\u4e86\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u3002", "conclusion": "KronSAE\u548cmAND\u7684\u7ed3\u5408\u4e3a\u5927\u89c4\u6a21\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u7a00\u758f\u81ea\u7f16\u7801\u5668, Kronecker\u4e58\u79ef, mAND, \u53ef\u89e3\u91ca\u6027, \u8ba1\u7b97\u6548\u7387"}}
{"id": "2505.21652", "pdf": "https://arxiv.org/pdf/2505.21652", "abs": "https://arxiv.org/abs/2505.21652", "authors": ["Yifan Yin", "Zhengtao Han", "Shivam Aarya", "Jianxin Wang", "Shuhang Xu", "Jiawei Peng", "Angtian Wang", "Alan Yuille", "Tianmin Shu"], "title": "PartInstruct: Part-level Instruction Following for Fine-grained Robot Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Fine-grained robot manipulation, such as lifting and rotating a bottle to\ndisplay the label on the cap, requires robust reasoning about object parts and\ntheir relationships with intended tasks. Despite recent advances in training\ngeneral-purpose robot manipulation policies guided by language instructions,\nthere is a notable lack of large-scale datasets for fine-grained manipulation\ntasks with part-level instructions and diverse 3D object instances annotated\nwith part-level labels. In this work, we introduce PartInstruct, the first\nlarge-scale benchmark for training and evaluating fine-grained robot\nmanipulation models using part-level instructions. PartInstruct comprises 513\nobject instances across 14 categories, each annotated with part-level\ninformation, and 1302 fine-grained manipulation tasks organized into 16 task\nclasses. Our training set consists of over 10,000 expert demonstrations\nsynthesized in a 3D simulator, where each demonstration is paired with a\nhigh-level task instruction, a chain of base part-based skill instructions, and\nground-truth 3D information about the object and its parts. Additionally, we\ndesigned a comprehensive test suite to evaluate the generalizability of learned\npolicies across new states, objects, and tasks. We evaluated several\nstate-of-the-art robot manipulation approaches, including end-to-end\nvision-language policy learning and bi-level planning models for robot\nmanipulation on our benchmark. The experimental results reveal that current\nmodels struggle to robustly ground part concepts and predict actions in 3D\nspace, and face challenges when manipulating object parts in long-horizon\ntasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u7528\u4e8e\u7ec6\u7c92\u5ea6\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5927\u89c4\u6a21\u57fa\u51c6PartInstruct\uff0c\u5305\u542b513\u4e2a\u5bf9\u8c61\u5b9e\u4f8b\u548c1302\u4e2a\u4efb\u52a1\uff0c\u6807\u6ce8\u4e86\u90e8\u4ef6\u7ea7\u4fe1\u606f\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u73b0\u6709\u6a21\u578b\u57283D\u7a7a\u95f4\u4e2d\u7684\u90e8\u4ef6\u6982\u5ff5\u7406\u89e3\u548c\u957f\u65f6\u4efb\u52a1\u4e2d\u4ecd\u5b58\u5728\u4e0d\u8db3\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u7528\u4e8e\u7ec6\u7c92\u5ea6\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5c24\u5176\u662f\u5305\u542b\u90e8\u4ef6\u7ea7\u6807\u6ce8\u548c\u591a\u6837\u53163D\u5bf9\u8c61\u5b9e\u4f8b\u7684\u6570\u636e\u3002\u8be5\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u4e86PartInstruct\u57fa\u51c6\uff0c\u5305\u542b513\u4e2a\u5bf9\u8c61\u5b9e\u4f8b\uff0814\u7c7b\uff09\u548c1302\u4e2a\u4efb\u52a1\uff0816\u7c7b\uff09\uff0c\u5e76\u5408\u621010,000+\u4e13\u5bb6\u6f14\u793a\uff08\u542b\u90e8\u4ef6\u7ea7\u6307\u4ee4\u548c3D\u4fe1\u606f\uff09\u3002\u8bbe\u8ba1\u4e86\u6d4b\u8bd5\u5957\u4ef6\u8bc4\u4f30\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6a21\u578b\uff08\u5982\u7aef\u5230\u7aef\u89c6\u89c9-\u8bed\u8a00\u7b56\u7565\u548c\u53cc\u5c42\u89c4\u5212\u6a21\u578b\uff09\u5728\u90e8\u4ef6\u6982\u5ff5\u7406\u89e3\u548c3D\u7a7a\u95f4\u52a8\u4f5c\u9884\u6d4b\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u957f\u65f6\u4efb\u52a1\u4e2d\u3002", "conclusion": "PartInstruct\u4e3a\u7ec6\u7c92\u5ea6\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u90e8\u4ef6\u7ea7\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "keywords": "\u7ec6\u7c92\u5ea6\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u90e8\u4ef6\u7ea7\u6307\u4ee4\uff0c3D\u5bf9\u8c61\u6807\u6ce8\uff0c\u957f\u65f6\u4efb\u52a1\uff0c\u57fa\u51c6\u8bc4\u6d4b"}}
{"id": "2505.22586", "pdf": "https://arxiv.org/pdf/2505.22586", "abs": "https://arxiv.org/abs/2505.22586", "authors": ["Yoav Gur-Arieh", "Clara Suslik", "Yihuai Hong", "Fazl Barez", "Mor Geva"], "title": "Precise In-Parameter Concept Erasure in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often acquire knowledge during pretraining that\nis undesirable in downstream deployments, e.g., sensitive information or\ncopyrighted content. Existing approaches for removing such knowledge rely on\nfine-tuning, training low-rank adapters or fact-level editing, but these are\neither too coarse, too shallow, or ineffective. In this work, we propose PISCES\n(Precise In-parameter Suppression for Concept EraSure), a novel framework for\nprecisely erasing entire concepts from model parameters by directly editing\ndirections that encode them in parameter space. PISCES uses a disentangler\nmodel to decompose MLP vectors into interpretable features, identifies those\nassociated with a target concept using automated interpretability techniques,\nand removes them from model parameters. Experiments on Gemma 2 and Llama 3.1\nover various concepts show that PISCES achieves modest gains in efficacy over\nleading erasure methods, reducing accuracy on the target concept to as low as\n7.7%, while dramatically improving erasure specificity (by up to 31%) and\nrobustness (by up to 38%). Overall, these results demonstrate that\nfeature-based in-parameter editing enables a more precise and reliable approach\nfor removing conceptual knowledge in language models.", "AI": {"tldr": "PISCES\u6846\u67b6\u901a\u8fc7\u53c2\u6570\u7a7a\u95f4\u76f4\u63a5\u7f16\u8f91\u6982\u5ff5\u7f16\u7801\u65b9\u5411\uff0c\u6709\u6548\u79fb\u9664\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u654f\u611f\u6216\u53d7\u7248\u6743\u4fdd\u62a4\u77e5\u8bc6\uff0c\u5728\u6548\u679c\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u4e2d\u83b7\u53d6\u7684\u77e5\u8bc6\u53ef\u80fd\u5305\u542b\u654f\u611f\u6216\u53d7\u7248\u6743\u5185\u5bb9\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u5fae\u8c03\u6216\u5c40\u90e8\u7f16\u8f91\uff09\u6548\u679c\u6709\u9650\u3002", "method": "\u4f7f\u7528\u89e3\u8026\u6a21\u578b\u5206\u89e3MLP\u5411\u91cf\u4e3a\u53ef\u89e3\u91ca\u7279\u5f81\uff0c\u7ed3\u5408\u81ea\u52a8\u53ef\u89e3\u91ca\u6027\u6280\u672f\u5b9a\u4f4d\u76ee\u6807\u6982\u5ff5\uff0c\u5e76\u5728\u53c2\u6570\u7a7a\u95f4\u4e2d\u76f4\u63a5\u79fb\u9664\u3002", "result": "\u5728Gemma 2\u548cLlama 3.1\u4e0a\uff0cPISCES\u5c06\u76ee\u6807\u6982\u5ff5\u51c6\u786e\u7387\u964d\u81f37.7%\uff0c\u64e6\u9664\u7279\u5f02\u6027\u63d0\u534731%\uff0c\u9c81\u68d2\u6027\u63d0\u534738%\u3002", "conclusion": "\u57fa\u4e8e\u7279\u5f81\u7684\u53c2\u6570\u5185\u7f16\u8f91\u63d0\u9ad8\u4e86\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u79fb\u9664\u7684\u7cbe\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u77e5\u8bc6\u64e6\u9664,\u53c2\u6570\u7f16\u8f91,PISCES,\u53ef\u89e3\u91ca\u6027"}}
{"id": "2505.22257", "pdf": "https://arxiv.org/pdf/2505.22257", "abs": "https://arxiv.org/abs/2505.22257", "authors": ["Youssef Mroueh", "Nicolas Dupuis", "Brian Belgodere", "Apoorva Nitsure", "Mattia Rigotti", "Kristjan Greenewald", "Jiri Navratil", "Jerret Ross", "Jesus Rios"], "title": "Revisiting Group Relative Policy Optimization: Insights into On-Policy and Off-Policy Training", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We revisit Group Relative Policy Optimization (GRPO) in both on-policy and\noff-policy optimization regimes. Our motivation comes from recent work on\noff-policy Proximal Policy Optimization (PPO), which improves training\nstability, sampling efficiency, and memory usage. In addition, a recent\nanalysis of GRPO suggests that estimating the advantage function with\noff-policy samples could be beneficial. Building on these observations, we\nadapt GRPO to the off-policy setting. We show that both on-policy and\noff-policy GRPO objectives yield an improvement in the reward. This result\nmotivates the use of clipped surrogate objectives in the off-policy version of\nGRPO. We then compare the empirical performance of reinforcement learning with\nverifiable rewards in post-training using both GRPO variants. Our results show\nthat off-policy GRPO either significantly outperforms or performs on par with\nits on-policy counterpart.", "AI": {"tldr": "\u8bba\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86Group Relative Policy Optimization\uff08GRPO\uff09\u7684\u5728\u7b56\u7565\u548c\u79bb\u7b56\u7565\u4f18\u5316\u673a\u5236\uff0c\u5e76\u5c06\u5176\u9002\u5e94\u4e8e\u79bb\u7b56\u7565\u8bbe\u7f6e\u3002\u7ed3\u679c\u663e\u793a\uff0c\u79bb\u7b56\u7565GRPO\u8868\u73b0\u4f18\u4e8e\u6216\u4e0e\u5728\u7b56\u7565GRPO\u76f8\u5f53\u3002", "motivation": "\u57fa\u4e8e\u6700\u8fd1\u5173\u4e8e\u79bb\u7b56\u7565PPO\u7684\u7814\u7a76\uff0c\u4f5c\u8005\u53d7\u542f\u53d1\u6539\u8fdb\u8bad\u7ec3\u7a33\u5b9a\u6027\u3001\u91c7\u6837\u6548\u7387\u548c\u5185\u5b58\u4f7f\u7528\uff0c\u5e76\u63a2\u7d22\u5728GRPO\u4e2d\u5229\u7528\u79bb\u7b56\u7565\u6837\u672c\u4f30\u8ba1\u4f18\u52bf\u51fd\u6570\u7684\u6f5c\u5728\u76ca\u5904\u3002", "method": "\u4f5c\u8005\u5c06\u5728\u7b56\u7565\u548c\u79bb\u7b56\u7565GRPO\u76ee\u6807\u5e94\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u8bbe\u7f6e\uff0c\u5e76\u4f7f\u7528\u88c1\u526a\u66ff\u4ee3\u76ee\u6807\u4f18\u5316\u79bb\u7b56\u7565\u7248\u672c\u7684GRPO\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u79bb\u7b56\u7565GRPO\u663e\u8457\u4f18\u4e8e\u6216\u4e0e\u5728\u7b56\u7565GRPO\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "\u7814\u7a76\u652f\u6301\u5728\u79bb\u7b56\u7565\u73af\u5883\u4e2d\u4f7f\u7528GRPO\uff0c\u5e76\u901a\u8fc7\u88c1\u526a\u66ff\u4ee3\u76ee\u6807\u8fdb\u4e00\u6b65\u4f18\u5316\u5176\u6027\u80fd\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, GRPO, \u7b56\u7565\u4f18\u5316, \u79bb\u7b56\u7565\u5b66\u4e60, PPO"}}
{"id": "2505.22591", "pdf": "https://arxiv.org/pdf/2505.22591", "abs": "https://arxiv.org/abs/2505.22591", "authors": ["Erxin Yu", "Jing Li", "Ming Liao", "Qi Zhu", "Boyang Xue", "Minghui Xu", "Baojun Wang", "Lanqing Hong", "Fei Mi", "Lifeng Shang"], "title": "Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "16 pages, 9 figures", "summary": "Although large language models demonstrate strong performance across various\ndomains, they still struggle with numerous bad cases in mathematical reasoning.\nPrevious approaches to learning from errors synthesize training data by solely\nextrapolating from isolated bad cases, thereby failing to generalize the\nextensive patterns inherent within these cases. This paper presents\nSelf-Error-Instruct (SEI), a framework that addresses these model weaknesses\nand synthesizes more generalized targeted training data. Specifically, we\nexplore a target model on two mathematical datasets, GSM8K and MATH, to\npinpoint bad cases. Then, we generate error keyphrases for these cases based on\nthe instructor model's (GPT-4o) analysis and identify error types by clustering\nthese keyphrases. Next, we sample a few bad cases during each generation for\neach identified error type and input them into the instructor model, which\nsynthesizes additional training data using a self-instruct approach. This new\ndata is refined through a one-shot learning process to ensure that only the\nmost effective examples are kept. Finally, we use these curated data to\nfine-tune the target model, iteratively repeating the process to enhance\nperformance. We apply our framework to various models and observe improvements\nin their reasoning abilities across both in-domain and out-of-domain\nmathematics datasets. These results demonstrate the effectiveness of self-error\ninstruction in improving LLMs' mathematical reasoning through error\ngeneralization.", "AI": {"tldr": "SEI\u6846\u67b6\u901a\u8fc7\u81ea\u6211\u9519\u8bef\u6307\u5bfc\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u805a\u7c7b\u9519\u8bef\u7c7b\u578b\u5e76\u751f\u6210\u9488\u5bf9\u6027\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u4ecd\u5b58\u5728\u8bb8\u591a\u9519\u8bef\u6848\u4f8b\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u8fd9\u4e9b\u9519\u8bef\u7684\u5185\u5728\u6a21\u5f0f\u3002", "method": "\u57fa\u4e8eGPT-4o\u5206\u6790\u9519\u8bef\u6848\u4f8b\u5e76\u751f\u6210\u9519\u8bef\u5173\u952e\u8bcd\uff0c\u805a\u7c7b\u9519\u8bef\u7c7b\u578b\u540e\uff0c\u901a\u8fc7\u81ea\u6307\u5bfc\u65b9\u6cd5\u5408\u6210\u9488\u5bf9\u6027\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u4f7f\u7528\u5355\u6837\u672c\u5b66\u4e60\u7b5b\u9009\u6709\u6548\u6570\u636e\u3002", "result": "\u5728GSM8K\u548cMATH\u6570\u636e\u96c6\u4e0a\uff0cSEI\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u6570\u636e\u96c6\u7684\u6cdb\u5316\u6548\u679c\u3002", "conclusion": "\u81ea\u6211\u9519\u8bef\u6307\u5bfc\u901a\u8fc7\u9519\u8bef\u6cdb\u5316\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u6570\u5b66\u63a8\u7406,\u9519\u8bef\u6307\u5bfc,\u6570\u636e\u5408\u6210,\u6cdb\u5316\u80fd\u529b"}}
{"id": "2505.22275", "pdf": "https://arxiv.org/pdf/2505.22275", "abs": "https://arxiv.org/abs/2505.22275", "authors": ["Alexander Hagg", "Adam Gaier", "Dominik Wilde", "Alexander Asteroth", "Holger Foysi", "Dirk Reith"], "title": "Full Domain Analysis in Fluid Dynamics", "categories": ["cs.LG", "cs.NE", "68U01", "I.2.1; I.2.6"], "comment": null, "summary": "Novel techniques in evolutionary optimization, simulation and machine\nlearning allow for a broad analysis of domains like fluid dynamics, in which\ncomputation is expensive and flow behavior is complex. Under the term of full\ndomain analysis we understand the ability to efficiently determine the full\nspace of solutions in a problem domain, and analyze the behavior of those\nsolutions in an accessible and interactive manner. The goal of full domain\nanalysis is to deepen our understanding of domains by generating many examples\nof flow, their diversification, optimization and analysis. We define a formal\nmodel for full domain analysis, its current state of the art, and requirements\nof subcomponents. Finally, an example is given to show what we can learn by\nusing full domain analysis. Full domain analysis, rooted in optimization and\nmachine learning, can be a helpful tool in understanding complex systems in\ncomputational physics and beyond.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u5168\u9886\u57df\u5206\u6790\u7684\u65b0\u6280\u672f\uff0c\u7ed3\u5408\u8fdb\u5316\u4f18\u5316\u3001\u6a21\u62df\u548c\u673a\u5668\u5b66\u4e60\uff0c\u7528\u4e8e\u6d41\u4f53\u52a8\u529b\u5b66\u7b49\u590d\u6742\u8ba1\u7b97\u9886\u57df\uff0c\u65e8\u5728\u9ad8\u6548\u63a2\u7d22\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u5e76\u5206\u6790\u5176\u884c\u4e3a\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u4e14\u6d41\u52a8\u884c\u4e3a\u590d\u6742\u7684\u9886\u57df\uff08\u5982\u6d41\u4f53\u52a8\u529b\u5b66\uff09\u4e2d\u5168\u9762\u5206\u6790\u89e3\u51b3\u65b9\u6848\u7684\u9700\u6c42\uff0c\u4ee5\u6df1\u5316\u5bf9\u8fd9\u7c7b\u9886\u57df\u7684\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u9886\u57df\u5206\u6790\u7684\u6b63\u5f0f\u6a21\u578b\uff0c\u7ed3\u5408\u8fdb\u5316\u4f18\u5316\u3001\u6a21\u62df\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u7528\u4e8e\u89e3\u51b3\u65b9\u6848\u7684\u591a\u6837\u5316\u3001\u4f18\u5316\u548c\u5206\u6790\u3002", "result": "\u901a\u8fc7\u4e00\u4e2a\u793a\u4f8b\u5c55\u793a\u4e86\u5168\u9886\u57df\u5206\u6790\u7684\u5e94\u7528\u4ef7\u503c\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u7406\u89e3\u8ba1\u7b97\u7269\u7406\u5b66\u7b49\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u5168\u9886\u57df\u5206\u6790\u7ed3\u5408\u4f18\u5316\u548c\u673a\u5668\u5b66\u4e60\uff0c\u662f\u7406\u89e3\u8ba1\u7b97\u7269\u7406\u5b66\u7b49\u590d\u6742\u7cfb\u7edf\u7684\u6709\u529b\u5de5\u5177\u3002", "keywords": "\u5168\u9886\u57df\u5206\u6790, \u8fdb\u5316\u4f18\u5316, \u673a\u5668\u5b66\u4e60, \u6d41\u4f53\u52a8\u529b\u5b66, \u590d\u6742\u7cfb\u7edf"}}
{"id": "2505.21664", "pdf": "https://arxiv.org/pdf/2505.21664", "abs": "https://arxiv.org/abs/2505.21664", "authors": ["Joe O'Brien", "Jeremy Dolan", "Jay Kim", "Jonah Dykhuizen", "Jeba Sania", "Sebastian Becker", "Jam Kraprayoon", "Cara Labrador"], "title": "Expert Survey: AI Reliability & Security Research Priorities", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Our survey of 53 specialists across 105 AI reliability and security research\nareas identifies the most promising research prospects to guide strategic AI\nR&D investment. As companies are seeking to develop AI systems with broadly\nhuman-level capabilities, research on reliability and security is urgently\nneeded to ensure AI's benefits can be safely and broadly realized and prevent\nsevere harms. This study is the first to quantify expert priorities across a\ncomprehensive taxonomy of AI safety and security research directions and to\nproduce a data-driven ranking of their potential impact. These rankings may\nsupport evidence-based decisions about how to effectively deploy resources\ntoward AI reliability and security research.", "AI": {"tldr": "\u4e13\u5bb6\u8c03\u67e5AI\u53ef\u9760\u6027\u4e0e\u5b89\u5168\u7814\u7a76\uff0c\u786e\u5b9a\u4f18\u5148\u7814\u7a76\u65b9\u5411\u4ee5\u6307\u5bfc\u6295\u8d44\u3002", "motivation": "\u786e\u4fddAI\u7cfb\u7edf\u7684\u5e7f\u6cdb\u76ca\u5904\u80fd\u5b89\u5168\u5b9e\u73b0\u5e76\u907f\u514d\u4e25\u91cd\u5371\u5bb3\u3002", "method": "\u8c03\u67e553\u4f4d\u4e13\u5bb6\uff0c\u6db5\u76d6105\u4e2aAI\u53ef\u9760\u6027\u4e0e\u5b89\u5168\u7814\u7a76\u9886\u57df\u3002", "result": "\u9996\u6b21\u91cf\u5316\u4e13\u5bb6\u4f18\u5148\u7ea7\u5e76\u751f\u6210\u6570\u636e\u9a71\u52a8\u7684\u6f5c\u5728\u5f71\u54cd\u6392\u540d\u3002", "conclusion": "\u6392\u540d\u53ef\u652f\u6301\u8d44\u6e90\u6709\u6548\u90e8\u7f72\u4e8eAI\u53ef\u9760\u6027\u4e0e\u5b89\u5168\u7814\u7a76\u7684\u8bc1\u636e\u51b3\u7b56\u3002", "keywords": "AI\u53ef\u9760\u6027\u3001\u5b89\u5168\u6027\u3001\u4e13\u5bb6\u8c03\u67e5\u3001\u6570\u636e\u9a71\u52a8\u3001\u8d44\u6e90\u5206\u914d"}}
{"id": "2505.22618", "pdf": "https://arxiv.org/pdf/2505.22618", "abs": "https://arxiv.org/abs/2505.22618", "authors": ["Chengyue Wu", "Hao Zhang", "Shuchen Xue", "Zhijian Liu", "Shizhe Diao", "Ligeng Zhu", "Ping Luo", "Song Han", "Enze Xie"], "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding", "categories": ["cs.CL"], "comment": null, "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u53cc\u5411\u6269\u6563\u6a21\u578b\u7684\u5757\u72b6\u8fd1\u4f3cKV\u7f13\u5b58\u673a\u5236\u548c\u7f6e\u4fe1\u5ea6\u611f\u77e5\u5e76\u884c\u89e3\u7801\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86Diffusion LLMs\u7684\u63a8\u7406\u901f\u5ea6\u5e76\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u5f00\u6e90Diffusion LLMs\u56e0\u7f3a\u4e4fKV\u7f13\u5b58\u548c\u5e76\u884c\u89e3\u7801\u65f6\u8d28\u91cf\u4e0b\u964d\u5bfc\u81f4\u7684\u63a8\u7406\u901f\u5ea6\u843d\u540e\u4e8e\u81ea\u56de\u5f52\u6a21\u578b\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u5757\u72b6\u8fd1\u4f3cKV\u7f13\u5b58\u673a\u5236\u4ee5\u51cf\u5c11\u6027\u80fd\u635f\u5931\uff0c\u5e76\u63d0\u51fa\u7f6e\u4fe1\u5ea6\u611f\u77e5\u5e76\u884c\u89e3\u7801\u7b56\u7565\u4ee5\u907f\u514d\u4ee4\u724c\u4f9d\u8d56\u7834\u574f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u591a\u4e2aLLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u541e\u5410\u91cf\u63d0\u5347\u9ad8\u8fbe27.6\u500d\uff0c\u4e14\u7cbe\u5ea6\u635f\u5931\u6781\u5c0f\u3002", "conclusion": "\u901a\u8fc7\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u4e3aDiffusion LLMs\u7684\u5b9e\u9645\u90e8\u7f72\u94fa\u5e73\u4e86\u9053\u8def\u3002", "keywords": "Diffusion LLMs, KV Cache, \u5e76\u884c\u89e3\u7801, \u7f6e\u4fe1\u5ea6\u611f\u77e5, \u63a8\u7406\u52a0\u901f"}}
{"id": "2505.22306", "pdf": "https://arxiv.org/pdf/2505.22306", "abs": "https://arxiv.org/abs/2505.22306", "authors": ["Zehua Chen", "Yuyang Miao", "Liyuan Wang", "Luyun Fan", "Danilo P. Mandic", "Jun Zhu"], "title": "Versatile Cardiovascular Signal Generation with a Unified Diffusion Transformer", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Cardiovascular signals such as photoplethysmography (PPG),\nelectrocardiography (ECG), and blood pressure (BP) are inherently correlated\nand complementary, together reflecting the health of cardiovascular system.\nHowever, their joint utilization in real-time monitoring is severely limited by\ndiverse acquisition challenges from noisy wearable recordings to burdened\ninvasive procedures. Here we propose UniCardio, a multi-modal diffusion\ntransformer that reconstructs low-quality signals and synthesizes unrecorded\nsignals in a unified generative framework. Its key innovations include a\nspecialized model architecture to manage the signal modalities involved in\ngeneration tasks and a continual learning paradigm to incorporate varying\nmodality combinations. By exploiting the complementary nature of cardiovascular\nsignals, UniCardio clearly outperforms recent task-specific baselines in signal\ndenoising, imputation, and translation. The generated signals match the\nperformance of ground-truth signals in detecting abnormal health conditions and\nestimating vital signs, even in unseen domains, while ensuring interpretability\nfor human experts. These advantages position UniCardio as a promising avenue\nfor advancing AI-assisted healthcare.", "AI": {"tldr": "UniCardio\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\uff0c\u7528\u4e8e\u91cd\u6784\u4f4e\u8d28\u91cf\u4fe1\u53f7\u548c\u5408\u6210\u672a\u8bb0\u5f55\u4fe1\u53f7\uff0c\u901a\u8fc7\u521b\u65b0\u6a21\u578b\u67b6\u6784\u548c\u6301\u7eed\u5b66\u4e60\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u5fc3\u8840\u7ba1\u4fe1\u53f7\u7684\u53bb\u566a\u3001\u586b\u5145\u548c\u7ffb\u8bd1\u6027\u80fd\u3002", "motivation": "\u5fc3\u8840\u7ba1\u4fe1\u53f7\uff08\u5982PPG\u3001ECG\u548cBP\uff09\u76f8\u5173\u6027\u9ad8\u4f46\u91c7\u96c6\u56f0\u96be\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u65f6\u76d1\u6d4b\u4e2d\u8868\u73b0\u53d7\u9650\uff0c\u9700\u7edf\u4e00\u751f\u6210\u6846\u67b6\u89e3\u51b3\u4fe1\u53f7\u8d28\u91cf\u4e0e\u7f3a\u5931\u95ee\u9898\u3002", "method": "\u63d0\u51faUniCardio\uff0c\u91c7\u7528\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\uff0c\u7ed3\u5408\u6301\u7eed\u5b66\u4e60\u8303\u5f0f\u5904\u7406\u4e0d\u540c\u6a21\u6001\u7ec4\u5408\uff0c\u5229\u7528\u4fe1\u53f7\u7684\u4e92\u8865\u6027\u63d0\u5347\u751f\u6210\u6548\u679c\u3002", "result": "\u5728\u4fe1\u53f7\u53bb\u566a\u3001\u586b\u5145\u548c\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u4fe1\u53f7\u5728\u5f02\u5e38\u5065\u5eb7\u68c0\u6d4b\u548c\u751f\u547d\u4f53\u5f81\u4f30\u8ba1\u4e2d\u63a5\u8fd1\u771f\u5b9e\u4fe1\u53f7\u6027\u80fd\uff0c\u4e14\u5177\u5907\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "UniCardio\u4e3aAI\u8f85\u52a9\u533b\u7597\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7edf\u4e00\u751f\u6210\u6846\u67b6\u663e\u8457\u63d0\u5347\u5fc3\u8840\u7ba1\u4fe1\u53f7\u5904\u7406\u7684\u6027\u80fd\u4e0e\u9002\u7528\u6027\u3002", "keywords": "UniCardio, \u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668, \u5fc3\u8840\u7ba1\u4fe1\u53f7, \u4fe1\u53f7\u751f\u6210, AI\u8f85\u52a9\u533b\u7597"}}
{"id": "2505.22627", "pdf": "https://arxiv.org/pdf/2505.22627", "abs": "https://arxiv.org/abs/2505.22627", "authors": ["Yijun Shen", "Delong Chen", "Fan Liu", "Xingyu Wang", "Chuanyi Zhang", "Liang Yao", "Yuhui Zheng"], "title": "Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "While densely annotated image captions significantly facilitate the learning\nof robust vision-language alignment, methodologies for systematically\noptimizing human annotation efforts remain underexplored. We introduce\nChain-of-Talkers (CoTalk), an AI-in-the-loop methodology designed to maximize\nthe number of annotated samples and improve their comprehensiveness under fixed\nbudget constraints (e.g., total human annotation time). The framework is built\nupon two key insights. First, sequential annotation reduces redundant workload\ncompared to conventional parallel annotation, as subsequent annotators only\nneed to annotate the ``residual'' -- the missing visual information that\nprevious annotations have not covered. Second, humans process textual input\nfaster by reading while outputting annotations with much higher throughput via\ntalking; thus a multimodal interface enables optimized efficiency. We evaluate\nour framework from two aspects: intrinsic evaluations that assess the\ncomprehensiveness of semantic units, obtained by parsing detailed captions into\nobject-attribute trees and analyzing their effective connections; extrinsic\nevaluation measures the practical usage of the annotated captions in\nfacilitating vision-language alignment. Experiments with eight participants\nshow our Chain-of-Talkers (CoTalk) improves annotation speed (0.42 vs. 0.30\nunits/sec) and retrieval performance (41.13\\% vs. 40.52\\%) over the parallel\nmethod.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Chain-of-Talkers (CoTalk)\uff0c\u4e00\u79cd\u5728\u56fa\u5b9a\u9884\u7b97\u4e0b\u6700\u5927\u5316\u6807\u6ce8\u6837\u672c\u6570\u91cf\u548c\u63d0\u5347\u6807\u6ce8\u5168\u9762\u6027\u7684AI\u8f85\u52a9\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e8f\u5217\u5316\u6807\u6ce8\u548c\u591a\u6a21\u6001\u4ea4\u4e92\u4f18\u5316\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5bc6\u96c6\u6807\u6ce8\u65b9\u6cd5\u5728\u4f18\u5316\u4eba\u529b\u6807\u6ce8\u6548\u7387\u65b9\u9762\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u4e00\u79cd\u7cfb\u7edf\u5316\u65b9\u6cd5\u6765\u63d0\u5347\u6807\u6ce8\u901f\u5ea6\u4e0e\u5168\u9762\u6027\u3002", "method": "\u91c7\u7528\u5e8f\u5217\u5316\u6807\u6ce8\u51cf\u5c11\u5197\u4f59\u5de5\u4f5c\u91cf\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u4ea4\u4e92\uff08\u9605\u8bfb\u4e0e\u8bed\u97f3\u8f93\u51fa\uff09\u63d0\u5347\u6548\u7387\uff0c\u5e76\u89e3\u6790\u6807\u6ce8\u5185\u5bb9\u7684\u8bed\u4e49\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCoTalk\u5728\u6807\u6ce8\u901f\u5ea6\uff080.42 vs. 0.30\u5355\u4f4d/\u79d2\uff09\u548c\u68c0\u7d22\u6027\u80fd\uff0841.13% vs. 40.52%\uff09\u4e0a\u4f18\u4e8e\u5e76\u884c\u65b9\u6cd5\u3002", "conclusion": "CoTalk\u901a\u8fc7\u5e8f\u5217\u5316\u6807\u6ce8\u548c\u591a\u6a21\u6001\u4ea4\u4e92\u663e\u8457\u63d0\u5347\u4e86\u6807\u6ce8\u6548\u7387\u4e0e\u6548\u679c\uff0c\u4e3a\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u6570\u636e\u652f\u6301\u3002", "keywords": "\u6807\u6ce8\u6548\u7387, \u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50, \u591a\u6a21\u6001\u4ea4\u4e92, \u5e8f\u5217\u5316\u6807\u6ce8, AI\u8f85\u52a9"}}
{"id": "2505.22308", "pdf": "https://arxiv.org/pdf/2505.22308", "abs": "https://arxiv.org/abs/2505.22308", "authors": ["Zachary Shinnick", "Liangze Jiang", "Hemanth Saratchandran", "Anton van den Hengel", "Damien Teney"], "title": "Transformers Pretrained on Procedural Data Contain Modular Structures for Algorithmic Reasoning", "categories": ["cs.LG"], "comment": null, "summary": "Pretraining on large, semantically rich datasets is key for developing\nlanguage models. Surprisingly, recent studies have shown that even synthetic\ndata, generated procedurally through simple semantic-free algorithms, can yield\nsome of the same benefits as natural language pretraining. It is unclear what\nspecific capabilities such simple synthetic data instils in a model, where\nthese capabilities reside in the architecture, and how they manifest within its\nweights. In this short paper, we identify several beneficial forms of\nprocedural data, together with specific algorithmic reasoning skills that\nimprove in small transformers. Our core finding is that different procedural\nrules instil distinct but complementary inductive structures in the model. With\nextensive ablations and partial-transfer experiments, we discover that these\nstructures reside in different parts of the model. Attention layers often carry\nthe most transferable information, but some pretraining rules impart useful\nstructure to MLP blocks instead. Most interestingly, the structures induced by\nmultiple rules can be composed to jointly reinforce multiple capabilities.\nThese results suggest an exciting possibility of disentangling the acquisition\nof knowledge from reasoning in language models, with the goal of improving\ntheir robustness and data efficiency.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u7b80\u5355\u7684\u5408\u6210\u6570\u636e\u4e0e\u81ea\u7136\u8bed\u8a00\u9884\u8bad\u7ec3\u7c7b\u4f3c\uff0c\u80fd\u63d0\u5347\u5c0f\u89c4\u6a21Transformer\u6a21\u578b\u7684\u7279\u5b9a\u7b97\u6cd5\u63a8\u7406\u80fd\u529b\uff0c\u4e0d\u540c\u89c4\u5219\u53ef\u4e92\u8865\u5730\u5d4c\u5165\u6a21\u578b\u7684\u4e0d\u540c\u90e8\u5206\uff0c\u5982\u6ce8\u610f\u529b\u5c42\u6216MLP\u5757\u3002", "motivation": "\u63a2\u8ba8\u5408\u6210\u6570\u636e\u5728\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\u7684\u4f5c\u7528\uff0c\u5c24\u5176\u662f\u5176\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u53ca\u7ed3\u6784\u5206\u5e03\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\u3002", "method": "\u4f7f\u7528\u4e0d\u540c\u89c4\u5219\u751f\u6210\u5408\u6210\u6570\u636e\u9884\u8bad\u7ec3\u5c0f\u578bTransformer\uff0c\u901a\u8fc7\u6d88\u878d\u548c\u90e8\u5206\u8fc1\u79fb\u5b9e\u9a8c\u5206\u6790\u6a21\u578b\u6743\u91cd\u548c\u7ed3\u6784\u3002", "result": "\u4e0d\u540c\u89c4\u5219\u5728\u6a21\u578b\u7684\u6ce8\u610f\u529b\u5c42\u548cMLP\u5757\u4e2d\u5f62\u6210\u4e92\u8865\u7684\u5f52\u7eb3\u7ed3\u6784\uff0c\u591a\u89c4\u5219\u7ec4\u5408\u53ef\u5f3a\u5316\u591a\u79cd\u80fd\u529b\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u80fd\u6709\u6548\u8bf1\u5bfc\u6a21\u578b\u7ed3\u6784\uff0c\u652f\u6301\u77e5\u8bc6\u83b7\u53d6\u4e0e\u63a8\u7406\u7684\u89e3\u8026\uff0c\u53ef\u80fd\u63d0\u5347\u6a21\u578b\u6548\u7387\u4e0e\u9c81\u68d2\u6027\u3002", "keywords": "\u5408\u6210\u6570\u636e\u3001Transformer\u3001\u9884\u8bad\u7ec3\u3001\u5f52\u7eb3\u7ed3\u6784\u3001\u7b97\u6cd5\u63a8\u7406"}}
{"id": "2505.22630", "pdf": "https://arxiv.org/pdf/2505.22630", "abs": "https://arxiv.org/abs/2505.22630", "authors": ["Ziling Cheng", "Meng Cao", "Marc-Antoine Rondeau", "Jackie Chi Kit Cheung"], "title": "Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "The widespread success of large language models (LLMs) on NLP benchmarks has\nbeen accompanied by concerns that LLMs function primarily as stochastic parrots\nthat reproduce texts similar to what they saw during pre-training, often\nerroneously. But what is the nature of their errors, and do these errors\nexhibit any regularities? In this work, we examine irrelevant context\nhallucinations, in which models integrate misleading contextual cues into their\npredictions. Through behavioral analysis, we show that these errors result from\na structured yet flawed mechanism that we term class-based (mis)generalization,\nin which models combine abstract class cues with features extracted from the\nquery or context to derive answers. Furthermore, mechanistic interpretability\nexperiments on Llama-3, Mistral, and Pythia across 39 factual recall relation\ntypes reveal that this behavior is reflected in the model's internal\ncomputations: (i) abstract class representations are constructed in lower\nlayers before being refined into specific answers in higher layers, (ii)\nfeature selection is governed by two competing circuits -- one prioritizing\ndirect query-based reasoning, the other incorporating contextual cues -- whose\nrelative influences determine the final output. Our findings provide a more\nnuanced perspective on the stochastic parrot argument: through form-based\ntraining, LLMs can exhibit generalization leveraging abstractions, albeit in\nunreliable ways based on contextual cues -- what we term stochastic chameleons.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63a8\u7406\u65f6\u7684\u9519\u8bef\u6a21\u5f0f\uff0c\u7279\u522b\u662f\u65e0\u5173\u80cc\u666f\u5e7b\u89c9\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u5176\u5185\u90e8\u5b58\u5728\u62bd\u8c61\u7c7b\u522b\uff08\u9519\u8bef\uff09\u6cdb\u5316\u673a\u5236\uff0c\u4ee5\u53ca\u4e24\u79cd\u7ade\u4e89\u7535\u8def\u5f71\u54cd\u6700\u7ec8\u8f93\u51fa\u7684\u73b0\u8c61\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63ed\u793aLLMs\u9519\u8bef\u80cc\u540e\u7684\u89c4\u5f8b\u6027\uff0c\u6311\u6218\u4e86\u5b83\u4eec\u4ec5\u662f\u968f\u673a\u590d\u8bfb\u673a\u7684\u89c2\u70b9\uff0c\u63a2\u7d22\u5176\u6cdb\u5316\u80fd\u529b\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u884c\u4e3a\u5206\u6790\u548c\u673a\u5236\u53ef\u89e3\u91ca\u6027\u5b9e\u9a8c\uff08Llama-3\u3001Mistral\u3001Pythia\u7b49\u6a21\u578b\uff09\uff0c\u7814\u7a7639\u79cd\u4e8b\u5b9e\u56de\u5fc6\u5173\u7cfb\u4e2d\u7684\u4e0a\u4e0b\u6587\u5e7b\u89c9\u73b0\u8c61\u3002", "result": "\u6a21\u578b\u9519\u8bef\u6e90\u4e8e\u7ed3\u6784\u5316\u7684\u7c7b\u522b\uff08\u9519\u8bef\uff09\u6cdb\u5316\u673a\u5236\uff0c\u5176\u5185\u90e8\u8ba1\u7b97\u4f53\u73b0\u4e3a\u4f4e\u5c42\u6784\u5efa\u62bd\u8c61\u7c7b\u522b\u8868\u5f81\uff0c\u9ad8\u5c42\u7ec6\u5316\u7b54\u6848\uff0c\u5e76\u53d7\u4e24\u79cd\u7ade\u4e89\u7535\u8def\u8c03\u63a7\u3002", "conclusion": "LLMs\u901a\u8fc7\u5f62\u5f0f\u5316\u8bad\u7ec3\u5c55\u73b0\u51fa\u57fa\u4e8e\u62bd\u8c61\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u4f9d\u8d56\u4e0a\u4e0b\u6587\u7ebf\u7d22\u7684\u65b9\u5f0f\u4e0d\u53ef\u9760\uff0c\u53ef\u79f0\u4e3a\u201c\u968f\u673a\u53d8\u8272\u9f99\u201d\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\u3001\u65e0\u5173\u80cc\u666f\u5e7b\u89c9\u3001\u7c7b\u522b\u6cdb\u5316\u3001\u673a\u5236\u53ef\u89e3\u91ca\u6027\u3001\u968f\u673a\u53d8\u8272\u9f99"}}
{"id": "2505.22310", "pdf": "https://arxiv.org/pdf/2505.22310", "abs": "https://arxiv.org/abs/2505.22310", "authors": ["Shoaib Ahmed Siddiqui", "Adrian Weller", "David Krueger", "Gintare Karolina Dziugaite", "Michael Curtis Mozer", "Eleni Triantafillou"], "title": "From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent unlearning methods for LLMs are vulnerable to relearning attacks:\nknowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of\n(even seemingly-unrelated) examples. We study this phenomenon in a controlled\nsetting for example-level unlearning in vision classifiers. We make the\nsurprising discovery that forget-set accuracy can recover from around 50%\npost-unlearning to nearly 100% with fine-tuning on just the retain set -- i.e.,\nzero examples of the forget set. We observe this effect across a wide variety\nof unlearning methods, whereas for a model retrained from scratch excluding the\nforget set (gold standard), the accuracy remains at 50%. We observe that\nresistance to relearning attacks can be predicted by weight-space properties,\nspecifically, $L_2$-distance and linear mode connectivity between the original\nand the unlearned model. Leveraging this insight, we propose a new class of\nmethods that achieve state-of-the-art resistance to relearning attacks.", "AI": {"tldr": "\u8fd1\u671fLLM\u7684\u9057\u5fd8\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230\u518d\u5b66\u4e60\u653b\u51fb\uff0c\u5373\u4f7f\u901a\u8fc7\u5c11\u91cf\u65e0\u5173\u6837\u672c\u5fae\u8c03\uff0c\u9057\u5fd8\u7684\u77e5\u8bc6\u4e5f\u4f1a\u91cd\u73b0\u3002\u5728\u89c6\u89c9\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u4ec5\u4f7f\u7528\u4fdd\u7559\u96c6\u5fae\u8c03\u5373\u53ef\u5c06\u9057\u5fd8\u96c6\u51c6\u786e\u7387\u4ece50%\u6062\u590d\u5230\u8fd1100%\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6743\u91cd\u7a7a\u95f4\u7684L2\u8ddd\u79bb\u548c\u7ebf\u6027\u6a21\u5f0f\u8fde\u901a\u6027\u53ef\u9884\u6d4b\u518d\u5b66\u4e60\u653b\u51fb\u7684\u62b5\u6297\u6027\uff0c\u5e76\u63d0\u51fa\u65b0\u65b9\u6cd5\u5b9e\u73b0\u6700\u4f73\u62b5\u6297\u6548\u679c\u3002", "motivation": "\u7814\u7a76LLM\u9057\u5fd8\u65b9\u6cd5\u7684\u8106\u5f31\u6027\uff0c\u63a2\u7d22\u518d\u5b66\u4e60\u653b\u51fb\u4e0b\u9057\u5fd8\u77e5\u8bc6\u7684\u91cd\u73b0\u73b0\u8c61\u53ca\u5176\u673a\u5236\u3002", "method": "\u5728\u89c6\u89c9\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u9a8c\uff0c\u5206\u6790\u9057\u5fd8\u96c6\u51c6\u786e\u7387\u6062\u590d\u73b0\u8c61\uff1b\u901a\u8fc7\u6743\u91cd\u7a7a\u95f4\u7279\u6027\uff08L2\u8ddd\u79bb\u3001\u7ebf\u6027\u6a21\u5f0f\u8fde\u901a\u6027\uff09\u9884\u6d4b\u62b5\u6297\u6027\uff0c\u5e76\u8bbe\u8ba1\u65b0\u65b9\u6cd5\u3002", "result": "\u4ec5\u7528\u4fdd\u7559\u96c6\u5fae\u8c03\u5373\u53ef\u5c06\u9057\u5fd8\u96c6\u51c6\u786e\u7387\u4ece50%\u6062\u590d\u81f3\u8fd1100%\uff1b\u65b0\u65b9\u6cd5\u5728\u62b5\u6297\u518d\u5b66\u4e60\u653b\u51fb\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u5b58\u5728\u518d\u5b66\u4e60\u6f0f\u6d1e\uff0c\u6743\u91cd\u7a7a\u95f4\u7279\u6027\u662f\u5173\u952e\u62b5\u6297\u6307\u6807\uff0c\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027\u3002", "keywords": "LLM\uff0c\u9057\u5fd8\u5b66\u4e60\uff0c\u518d\u5b66\u4e60\u653b\u51fb\uff0c\u6743\u91cd\u7a7a\u95f4\uff0cL2\u8ddd\u79bb\uff0c\u7ebf\u6027\u6a21\u5f0f\u8fde\u901a\u6027"}}
{"id": "2505.22633", "pdf": "https://arxiv.org/pdf/2505.22633", "abs": "https://arxiv.org/abs/2505.22633", "authors": ["Yida Xue", "Zhen Bi", "Jinnan Yang", "Jungang Lou", "Huajun Chen", "Ningyu Zhang"], "title": "Spatial Knowledge Graph-Guided Multimodal Synthesis", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Ongoing work", "summary": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly enhanced their capabilities; however, their spatial perception\nabilities remain a notable limitation. To address this challenge, multimodal\ndata synthesis offers a promising solution. Yet, ensuring that synthesized data\nadhere to spatial common sense is a non-trivial task. In this work, we\nintroduce SKG2Data, a novel multimodal synthesis approach guided by spatial\nknowledge graphs, grounded in the concept of knowledge-to-data generation.\nSKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate\nhuman-like perception of spatial directions and distances, which is\nsubsequently utilized to guide multimodal data synthesis. Extensive experiments\ndemonstrate that data synthesized from diverse types of spatial knowledge,\nincluding direction and distance, not only enhance the spatial perception and\nreasoning abilities of MLLMs but also exhibit strong generalization\ncapabilities. We hope that the idea of knowledge-based data synthesis can\nadvance the development of spatial intelligence.", "AI": {"tldr": "SKG2Data\u662f\u4e00\u79cd\u57fa\u4e8e\u7a7a\u95f4\u77e5\u8bc6\u56fe\u8c31\u7684\u591a\u6a21\u6001\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u611f\u77e5\u4e0e\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u6709\u9650\uff0c\u800c\u591a\u6a21\u6001\u6570\u636e\u5408\u6210\u867d\u80fd\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u786e\u4fdd\u5408\u6210\u6570\u636e\u7b26\u5408\u7a7a\u95f4\u5e38\u8bc6\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faSKG2Data\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u7a7a\u95f4\u77e5\u8bc6\u56fe\u8c31\uff08SKG\uff09\u6a21\u62df\u4eba\u7c7b\u5bf9\u7a7a\u95f4\u65b9\u5411\u4e0e\u8ddd\u79bb\u7684\u611f\u77e5\uff0c\u5e76\u4ee5\u6b64\u6307\u5bfc\u591a\u6a21\u6001\u6570\u636e\u5408\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u65b9\u5411\u3001\u8ddd\u79bb\u7b49\u7a7a\u95f4\u77e5\u8bc6\u5408\u6210\u7684\u6570\u636e\u4e0d\u4ec5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u7a7a\u95f4\u80fd\u529b\uff0c\u8fd8\u5c55\u73b0\u51fa\u5f3a\u6cdb\u5316\u6027\u3002", "conclusion": "\u57fa\u4e8e\u77e5\u8bc6\u7684\u6570\u636e\u5408\u6210\u6709\u671b\u63a8\u52a8\u7a7a\u95f4\u667a\u80fd\u7684\u53d1\u5c55\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b, \u7a7a\u95f4\u611f\u77e5, \u77e5\u8bc6\u56fe\u8c31, \u6570\u636e\u5408\u6210, \u7a7a\u95f4\u5e38\u8bc6"}}
{"id": "2505.22312", "pdf": "https://arxiv.org/pdf/2505.22312", "abs": "https://arxiv.org/abs/2505.22312", "authors": ["Jujie He", "Jiacai Liu", "Chris Yuhao Liu", "Rui Yan", "Chaojie Wang", "Peng Cheng", "Xiaoyu Zhang", "Fuxiang Zhang", "Jiacheng Xu", "Wei Shen", "Siyuan Li", "Liang Zeng", "Tianwen Wei", "Cheng Cheng", "Bo An", "Yang Liu", "Yahui Zhou"], "title": "Skywork Open Reasoner 1 Technical Report", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The success of DeepSeek-R1 underscores the significant role of reinforcement\nlearning (RL) in enhancing the reasoning capabilities of large language models\n(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL\nimplementation for long Chain-of-Thought (CoT) models. Building on the\nDeepSeek-R1-Distill model series, our RL approach achieves notable performance\ngains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench\nfrom 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)\nfor the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and\nQwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable\nresults on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models\ndemonstrate competitive reasoning capabilities among models of similar size. We\nperform comprehensive ablation studies on the core components of our training\npipeline to validate their effectiveness. Additionally, we thoroughly\ninvestigate the phenomenon of entropy collapse, identify key factors affecting\nentropy dynamics, and demonstrate that mitigating premature entropy collapse is\ncritical for improved test performance. To support community research, we fully\nopen-source our model weights, training code, and training datasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Skywork-OR1\uff0c\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u957f\u94fe\u601d\u7ef4\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u957f\u94fe\u601d\u7ef4\u4efb\u52a1\u4e2d\u3002", "method": "\u57fa\u4e8eDeepSeek-R1-Distill\u6a21\u578b\u7cfb\u5217\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728AIME24\u3001AIME25\u548cLiveCodeBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c32B\u6a21\u578b\u51c6\u786e\u7387\u63d0\u534715.0%\uff0c7B\u6a21\u578b\u63d0\u534713.9%\u3002", "conclusion": "Skywork-OR1\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u4e14\u7f13\u89e3\u71b5\u5d29\u6e83\u73b0\u8c61\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u63a8\u7406\u80fd\u529b, \u957f\u94fe\u601d\u7ef4, \u71b5\u5d29\u6e83"}}
{"id": "2505.22635", "pdf": "https://arxiv.org/pdf/2505.22635", "abs": "https://arxiv.org/abs/2505.22635", "authors": ["Fangcong Yin", "Zeyu Leo Liu", "Liu Leqi", "Xi Ye", "Greg Durrett"], "title": "Learning Composable Chains-of-Thought", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A common approach for teaching large language models (LLMs) to reason is to\ntrain on chain-of-thought (CoT) traces of in-distribution reasoning problems,\nbut such annotated data is costly to obtain for every problem of interest. We\nwant reasoning models to generalize beyond their training distribution, and\nideally to generalize compositionally: combine atomic reasoning skills to solve\nharder, unseen reasoning tasks. We take a step towards compositional\ngeneralization of reasoning skills when addressing a target compositional task\nthat has no labeled CoT data. We find that simply training models on CoT data\nof atomic tasks leads to limited generalization, but minimally modifying CoT\nformats of constituent atomic tasks to be composable can lead to improvements.\nWe can train \"atomic CoT\" models on the atomic tasks with Composable CoT data\nand combine them with multitask learning or model merging for better zero-shot\nperformance on the target compositional task. Such a combined model can be\nfurther bootstrapped on a small amount of compositional data using rejection\nsampling fine-tuning (RFT). Results on string operations and natural language\nskill compositions show that training LLMs on Composable CoT outperforms\nmultitask learning and continued fine-tuning baselines within a given training\ndata budget.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u4fee\u6539\u539f\u5b50\u4efb\u52a1\u7684\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u683c\u5f0f\uff0c\u4f7f\u5176\u53ef\u7ec4\u5408\uff0c\u4ee5\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u672a\u89c1\u7ec4\u5408\u4efb\u52a1\u4e0a\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u4e3a\u6bcf\u4e2a\u63a8\u7406\u4efb\u52a1\u6807\u6ce8CoT\u6570\u636e\uff0c\u6210\u672c\u9ad8\u6602\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7ec4\u5408\u539f\u5b50\u63a8\u7406\u6280\u80fd\uff0c\u5b9e\u73b0\u6a21\u578b\u5728\u65e0\u6807\u6ce8CoT\u6570\u636e\u7684\u7ec4\u5408\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u3002", "method": "\u63d0\u51fa\u2018\u53ef\u7ec4\u5408CoT\u2019\u65b9\u6cd5\uff0c\u4fee\u6539\u539f\u5b50\u4efb\u52a1\u7684CoT\u683c\u5f0f\u4ee5\u589e\u5f3a\u7ec4\u5408\u6027\uff0c\u7ed3\u5408\u591a\u4efb\u52a1\u5b66\u4e60\u6216\u6a21\u578b\u878d\u5408\uff0c\u5e76\u901a\u8fc7\u62d2\u7edd\u91c7\u6837\u5fae\u8c03\uff08RFT\uff09\u5229\u7528\u5c11\u91cf\u7ec4\u5408\u6570\u636e\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "result": "\u5728\u5b57\u7b26\u4e32\u64cd\u4f5c\u548c\u81ea\u7136\u8bed\u8a00\u6280\u80fd\u7ec4\u5408\u4efb\u52a1\u4e0a\uff0c\u53ef\u7ec4\u5408CoT\u65b9\u6cd5\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u6301\u7eed\u5fae\u8c03\u57fa\u7ebf\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u53ef\u7ec4\u5408CoT\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u7ec4\u5408\u4efb\u52a1\u4e0a\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u4e3a\u63a8\u7406\u6cdb\u5316\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "keywords": "\u94fe\u5f0f\u601d\u7ef4\u3001\u7ec4\u5408\u6cdb\u5316\u3001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u591a\u4efb\u52a1\u5b66\u4e60\u3001\u62d2\u7edd\u91c7\u6837\u5fae\u8c03"}}
{"id": "2505.22316", "pdf": "https://arxiv.org/pdf/2505.22316", "abs": "https://arxiv.org/abs/2505.22316", "authors": ["Konrad \u00d6zdemir", "Lukas Kirchdorfer", "Keyvan Amiri Elyasi", "Han van der Aa", "Heiner Stuckenschmidt"], "title": "Rethinking BPS: A Utility-Based Evaluation Framework", "categories": ["cs.LG"], "comment": null, "summary": "Business process simulation (BPS) is a key tool for analyzing and optimizing\norganizational workflows, supporting decision-making by estimating the impact\nof process changes. The reliability of such estimates depends on the ability of\na BPS model to accurately mimic the process under analysis, making rigorous\naccuracy evaluation essential. However, the state-of-the-art approach to\nevaluating BPS models has two key limitations. First, it treats simulation as a\nforecasting problem, testing whether models can predict unseen future events.\nThis fails to assess how well a model captures the as-is process, particularly\nwhen process behavior changes from train to test period. Thus, it becomes\ndifficult to determine whether poor results stem from an inaccurate model or\nthe inherent complexity of the data, such as unpredictable drift. Second, the\nevaluation approach strongly relies on Earth Mover's Distance-based metrics,\nwhich can obscure temporal patterns and thus yield misleading conclusions about\nsimulation quality. To address these issues, we propose a novel framework that\nevaluates simulation quality based on its ability to generate representative\nprocess behavior. Instead of comparing simulated logs to future real-world\nexecutions, we evaluate whether predictive process monitoring models trained on\nsimulated data perform comparably to those trained on real data for downstream\nanalysis tasks. Empirical results show that our framework not only helps\nidentify sources of discrepancies but also distinguishes between model accuracy\nand data complexity, offering a more meaningful way to assess BPS quality.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\u6765\u8bc4\u4f30\u4e1a\u52a1\u6d41\u7a0b\u6a21\u62df(BPS)\u7684\u8d28\u91cf\uff0c\u901a\u8fc7\u6bd4\u8f83\u6a21\u62df\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u5728\u4e0b\u6e38\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6570\u636e\u590d\u6742\u6027\u533a\u5206\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684BPS\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u4e00\u662f\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u6a21\u578b\u5bf9\u5f53\u524d\u6d41\u7a0b\u7684\u6355\u6349\u80fd\u529b\uff0c\u4e8c\u662f\u4f9d\u8d56\u7684\u8bc4\u4f30\u6307\u6807\u53ef\u80fd\u63a9\u76d6\u65f6\u95f4\u6a21\u5f0f\uff0c\u5bfc\u81f4\u8bef\u5bfc\u6027\u7ed3\u8bba\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8bc4\u4f30\u6a21\u62df\u6570\u636e\u662f\u5426\u80fd\u4e3a\u4e0b\u6e38\u5206\u6790\u4efb\u52a1\u751f\u6210\u5177\u6709\u4ee3\u8868\u6027\u7684\u6d41\u7a0b\u884c\u4e3a\uff0c\u4ee3\u66ff\u4f20\u7edf\u7684\u4e0e\u672a\u6765\u771f\u5b9e\u4e8b\u4ef6\u6bd4\u8f83\u7684\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u4e0d\u4ec5\u80fd\u8bc6\u522b\u5dee\u5f02\u6765\u6e90\uff0c\u8fd8\u80fd\u533a\u5206\u6a21\u578b\u51c6\u786e\u6027\u548c\u6570\u636e\u590d\u6742\u6027\u3002", "conclusion": "\u65b0\u6846\u67b6\u4e3aBPS\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u5728\u5904\u7406\u6d41\u7a0b\u884c\u4e3a\u53d8\u5316\u548c\u6570\u636e\u590d\u6742\u6027\u65f6\u8868\u73b0\u4f18\u8d8a\u3002", "keywords": "\u4e1a\u52a1\u6d41\u7a0b\u6a21\u62df, \u51c6\u786e\u6027\u8bc4\u4f30, \u9884\u6d4b\u6027\u6d41\u7a0b\u76d1\u63a7, \u6570\u636e\u590d\u6742\u6027"}}
{"id": "2505.22645", "pdf": "https://arxiv.org/pdf/2505.22645", "abs": "https://arxiv.org/abs/2505.22645", "authors": ["Hanjia Lyu", "Jiebo Luo", "Jian Kang", "Allison Koenecke"], "title": "Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese", "categories": ["cs.CL", "cs.CY"], "comment": "To appear in the 2025 ACM Conference on Fairness, Accountability, and\n  Transparency (FAccT '25)", "summary": "While the capabilities of Large Language Models (LLMs) have been studied in\nboth Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit\ndifferential performance when prompted in these two variants of written\nChinese. This understanding is critical, as disparities in the quality of LLM\nresponses can perpetuate representational harms by ignoring the different\ncultural contexts underlying Simplified versus Traditional Chinese, and can\nexacerbate downstream harms in LLM-facilitated decision-making in domains such\nas education or hiring. To investigate potential LLM performance disparities,\nwe design two benchmark tasks that reflect real-world scenarios: regional term\nchoice (prompting the LLM to name a described item which is referred to\ndifferently in Mainland China and Taiwan), and regional name choice (prompting\nthe LLM to choose who to hire from a list of names in both Simplified and\nTraditional Chinese). For both tasks, we audit the performance of 11 leading\ncommercial LLM services and open-sourced models -- spanning those primarily\ntrained on English, Simplified Chinese, or Traditional Chinese. Our analyses\nindicate that biases in LLM responses are dependent on both the task and\nprompting language: while most LLMs disproportionately favored Simplified\nChinese responses in the regional term choice task, they surprisingly favored\nTraditional Chinese names in the regional name choice task. We find that these\ndisparities may arise from differences in training data representation, written\ncharacter preferences, and tokenization of Simplified and Traditional Chinese.\nThese findings highlight the need for further analysis of LLM biases; as such,\nwe provide an open-sourced benchmark dataset to foster reproducible evaluations\nof future LLM behavior across Chinese language variants\n(https://github.com/brucelyu17/SC-TC-Bench).", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7b80\u4f53\u4e0e\u7e41\u4f53\u4e2d\u6587\u63d0\u793a\u4e0b\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5bf9\u7b80\u4f53\u6216\u7e41\u4f53\u4e2d\u6587\u7684\u504f\u597d\uff0c\u5e76\u6307\u51fa\u8fd9\u4e9b\u504f\u89c1\u53ef\u80fd\u6e90\u4e8e\u8bad\u7ec3\u6570\u636e\u3001\u5b57\u7b26\u504f\u597d\u548c\u5206\u8bcd\u65b9\u5f0f\u3002", "motivation": "\u4e86\u89e3LLMs\u5728\u7b80\u4f53\u4e0e\u7e41\u4f53\u4e2d\u6587\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\u5bf9\u4e8e\u907f\u514d\u6587\u5316\u80cc\u666f\u5ffd\u89c6\u548c\u51cf\u5c11\u51b3\u7b56\u4e2d\u7684\u6f5c\u5728\u5371\u5bb3\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e24\u79cd\u57fa\u51c6\u4efb\u52a1\uff08\u5730\u533a\u672f\u8bed\u9009\u62e9\u548c\u5730\u533a\u59d3\u540d\u9009\u62e9\uff09\uff0c\u5e76\u5bf911\u79cd\u4e3b\u6d41\u5546\u4e1a\u548c\u5f00\u6e90LLMs\u8fdb\u884c\u4e86\u6027\u80fd\u5ba1\u8ba1\u3002", "result": "\u5927\u591a\u6570LLMs\u5728\u672f\u8bed\u9009\u62e9\u4efb\u52a1\u4e2d\u504f\u5411\u7b80\u4f53\u4e2d\u6587\uff0c\u800c\u5728\u59d3\u540d\u9009\u62e9\u4efb\u52a1\u4e2d\u5374\u504f\u5411\u7e41\u4f53\u4e2d\u6587\uff0c\u8fd9\u79cd\u5dee\u5f02\u53ef\u80fd\u4e0e\u8bad\u7ec3\u6570\u636e\u8868\u793a\u3001\u5b57\u7b26\u504f\u597d\u548c\u5206\u8bcd\u65b9\u5f0f\u6709\u5173\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u8fdb\u4e00\u6b65\u5206\u6790LLM\u504f\u89c1\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f00\u6e90\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u8bc4\u4f30\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u7b80\u4f53\u4e2d\u6587, \u7e41\u4f53\u4e2d\u6587, \u6027\u80fd\u5dee\u5f02, \u504f\u89c1\u5206\u6790"}}
{"id": "2505.22322", "pdf": "https://arxiv.org/pdf/2505.22322", "abs": "https://arxiv.org/abs/2505.22322", "authors": ["Zhengyu Fang", "Zhimeng Jiang", "Huiyuan Chen", "Xiaoge Zhang", "Kaiyu Tang", "Xiao Li", "Jing Li"], "title": "A Closer Look on Memorization in Tabular Diffusion Model: A Data-Centric Perspective", "categories": ["cs.LG"], "comment": null, "summary": "Diffusion models have shown strong performance in generating high-quality\ntabular data, but they carry privacy risks by reproducing exact training\nsamples. While prior work focuses on dataset-level augmentation to reduce\nmemorization, little is known about which individual samples contribute most.\nWe present the first data-centric study of memorization dynamics in tabular\ndiffusion models. We quantify memorization for each real sample based on how\nmany generated samples are flagged as replicas, using a relative distance\nratio. Our empirical analysis reveals a heavy-tailed distribution of\nmemorization counts: a small subset of samples contributes disproportionately\nto leakage, confirmed via sample-removal experiments. To understand this, we\ndivide real samples into top- and non-top-memorized groups and analyze their\ntraining-time behaviors. We track when each sample is first memorized and\nmonitor per-epoch memorization intensity (AUC). Memorized samples are memorized\nslightly earlier and show stronger signals in early training. Based on these\ninsights, we propose DynamicCut, a two-stage, model-agnostic mitigation method:\n(a) rank samples by epoch-wise intensity, (b) prune a tunable top fraction, and\n(c) retrain on the filtered dataset. Across multiple tabular datasets and\nmodels, DynamicCut reduces memorization with minimal impact on data diversity\nand downstream performance. It also complements augmentation-based defenses.\nFurthermore, DynamicCut enables cross-model transferability: high-ranked\nsamples identified from one model (e.g., a diffusion model) are also effective\nfor reducing memorization when removed from others, such as GANs and VAEs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8868\u683c\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6570\u636e\u8bb0\u5fc6\u52a8\u6001\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDynamicCut\u7684\u53cc\u9636\u6bb5\u7f13\u89e3\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u6a21\u578b\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u8fc7\u5ea6\u8bb0\u5fc6\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u8868\u683c\u6570\u636e\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u9690\u79c1\u98ce\u9669\uff0c\u5c24\u5176\u662f\u5bf9\u8bad\u7ec3\u6837\u672c\u7684\u8fc7\u5ea6\u8bb0\u5fc6\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u6570\u636e\u96c6\u7ea7\u522b\u7684\u589e\u5f3a\u6765\u51cf\u5c11\u8bb0\u5fc6\uff0c\u800c\u8f83\u5c11\u5173\u6ce8\u5bf9\u4e2a\u4f53\u6837\u672c\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u91cf\u5316\u6bcf\u4e2a\u771f\u5b9e\u6837\u672c\u7684\u8bb0\u5fc6\u7a0b\u5ea6\uff08\u57fa\u4e8e\u751f\u6210\u6837\u672c\u4e2d\u590d\u5236\u7684\u6bd4\u4f8b\uff09\uff0c\u5206\u6790\u8bb0\u5fc6\u5206\u5e03\u3002\u63d0\u51faDynamicCut\u65b9\u6cd5\uff1a\u6309\u8bb0\u5fc6\u5f3a\u5ea6\u6392\u5e8f\u6837\u672c\uff0c\u526a\u9664\u9ad8\u8bb0\u5fc6\u5f3a\u5ea6\u7684\u90e8\u5206\uff0c\u7136\u540e\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8bb0\u5fc6\u5448\u73b0\u91cd\u5c3e\u5206\u5e03\uff0c\u5c11\u91cf\u6837\u672c\u8d21\u732e\u5927\u90e8\u5206\u8bb0\u5fc6\u3002DynamicCut\u6709\u6548\u51cf\u5c11\u4e86\u8bb0\u5fc6\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u636e\u591a\u6837\u6027\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u9002\u7528\u4e8e\u5176\u4ed6\u751f\u6210\u6a21\u578b\u3002", "conclusion": "DynamicCut\u63d0\u4f9b\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u8bb0\u5fc6\u7f13\u89e3\u65b9\u6cd5\uff0c\u4e14\u5177\u5907\u8de8\u6a21\u578b\u9002\u7528\u6027\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u6269\u6563\u6a21\u578b\uff0c\u8bb0\u5fc6\u52a8\u6001\uff0c\u9690\u79c1\u4fdd\u62a4\uff0c\u8868\u683c\u6570\u636e\uff0cDynamicCut"}}
{"id": "2505.21699", "pdf": "https://arxiv.org/pdf/2505.21699", "abs": "https://arxiv.org/abs/2505.21699", "authors": ["Zhengbo Zhou", "Dooman Arefan", "Margarita Zuley", "Jules Sumkin", "Shandong Wu"], "title": "STA-Risk: A Deep Dive of Spatio-Temporal Asymmetries for Breast Cancer Risk Prediction", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Predicting the risk of developing breast cancer is an important clinical tool\nto guide early intervention and tailoring personalized screening strategies.\nEarly risk models have limited performance and recently machine learning-based\nanalysis of mammogram images showed encouraging risk prediction effects. These\nmodels however are limited to the use of a single exam or tend to overlook\nnuanced breast tissue evolvement in spatial and temporal details of\nlongitudinal imaging exams that are indicative of breast cancer risk. In this\npaper, we propose STA-Risk (Spatial and Temporal Asymmetry-based Risk\nPrediction), a novel Transformer-based model that captures fine-grained\nmammographic imaging evolution simultaneously from bilateral and longitudinal\nasymmetries for breast cancer risk prediction. STA-Risk is innovative by the\nside encoding and temporal encoding to learn spatial-temporal asymmetries,\nregulated by a customized asymmetry loss. We performed extensive experiments\nwith two independent mammogram datasets and achieved superior performance than\nfour representative SOTA models for 1- to 5-year future risk prediction. Source\ncodes will be released upon publishing of the paper.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSTA-Risk\u7684\u65b0\u578bTransformer\u6a21\u578b\uff0c\u901a\u8fc7\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0d\u5bf9\u79f0\u6027\u9884\u6d4b\u4e73\u817a\u764c\u98ce\u9669\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u4e73\u817a\u764c\u98ce\u9669\u9884\u6d4b\u6a21\u578b\u6027\u80fd\u6709\u9650\uff0c\u4e14\u5ffd\u89c6\u7eb5\u5411\u68c0\u67e5\u4e2d\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u7ec6\u8282\u3002", "method": "\u57fa\u4e8eTransformer\u7684STA-Risk\u6a21\u578b\uff0c\u901a\u8fc7\u4fa7\u7f16\u7801\u548c\u65f6\u95f4\u7f16\u7801\u5b66\u4e60\u7a7a\u95f4-\u65f6\u95f4\u4e0d\u5bf9\u79f0\u6027\uff0c\u5e76\u4f7f\u7528\u5b9a\u5236\u5316\u4e0d\u5bf9\u79f0\u6027\u635f\u5931\u3002", "result": "\u5728\u4e24\u4e2a\u72ec\u7acb\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0c\u6027\u80fd\u4f18\u4e8e\u56db\u79cd\u4ee3\u8868\u6027SOTA\u6a21\u578b\uff0c\u9002\u7528\u4e8e1-5\u5e74\u98ce\u9669\u9884\u6d4b\u3002", "conclusion": "STA-Risk\u901a\u8fc7\u6355\u6349\u7eb5\u5411\u548c\u53cc\u8fb9\u4e0d\u5bf9\u79f0\u6027\u663e\u8457\u63d0\u9ad8\u4e86\u4e73\u817a\u764c\u98ce\u9669\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "keywords": "\u4e73\u817a\u764c\u98ce\u9669\u9884\u6d4b, Transformer, \u7a7a\u95f4-\u65f6\u95f4\u4e0d\u5bf9\u79f0\u6027, \u7eb5\u5411\u5f71\u50cf, \u673a\u5668\u5b66\u4e60"}}
{"id": "2505.22648", "pdf": "https://arxiv.org/pdf/2505.22648", "abs": "https://arxiv.org/abs/2505.22648", "authors": ["Jialong Wu", "Baixuan Li", "Runnan Fang", "Wenbiao Yin", "Liwen Zhang", "Zhengwei Tao", "Dingchu Zhang", "Zekun Xi", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "title": "WebDancer: Towards Autonomous Information Seeking Agency", "categories": ["cs.CL"], "comment": null, "summary": "Addressing intricate real-world problems necessitates in-depth information\nseeking and multi-step reasoning. Recent progress in agentic systems,\nexemplified by Deep Research, underscores the potential for autonomous\nmulti-step research. In this work, we present a cohesive paradigm for building\nend-to-end agentic information seeking agents from a data-centric and\ntraining-stage perspective. Our approach consists of four key stages: (1)\nbrowsing data construction, (2) trajectories sampling, (3) supervised\nfine-tuning for effective cold start, and (4) reinforcement learning for\nenhanced generalisation. We instantiate this framework in a web agent based on\nthe ReAct, WebDancer. Empirical evaluations on the challenging information\nseeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of\nWebDancer, achieving considerable results and highlighting the efficacy of our\ntraining paradigm. Further analysis of agent training provides valuable\ninsights and actionable, systematic pathways for developing more capable\nagentic models. The codes and demo will be released in\nhttps://github.com/Alibaba-NLP/WebAgent.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u81ea\u4e3b\u4fe1\u606f\u641c\u7d22\u4ee3\u7406\u6846\u67b6WebDancer\uff0c\u5305\u542b\u56db\u4e2a\u5173\u952e\u9636\u6bb5\uff1a\u6d4f\u89c8\u6570\u636e\u6784\u5efa\u3001\u8f68\u8ff9\u91c7\u6837\u3001\u76d1\u7763\u5fae\u8c03\u4ee5\u4f18\u5316\u51b7\u542f\u52a8\u3001\u5f3a\u5316\u5b66\u4e60\u4ee5\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002\u5728GAIA\u548cWebWalkerQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u73b0\u5b9e\u95ee\u9898\u9700\u8981\u591a\u6b65\u63a8\u7406\u548c\u6df1\u5ea6\u4fe1\u606f\u641c\u7d22\uff0c\u73b0\u6709\u81ea\u4e3b\u7814\u7a76\u7cfb\u7edf\uff08\u5982Deep Research\uff09\u7684\u8fdb\u5c55\u663e\u793a\u4e86\u591a\u6b65\u81ea\u4e3b\u7814\u7a76\u7684\u6f5c\u529b\u3002", "method": "\u6846\u67b6\u5305\u542b\u56db\u4e2a\u9636\u6bb5\uff1a(1) \u6d4f\u89c8\u6570\u636e\u6784\u5efa\uff0c(2) \u8f68\u8ff9\u91c7\u6837\uff0c(3) \u76d1\u7763\u5fae\u8c03\u4f18\u5316\u51b7\u542f\u52a8\uff0c(4) \u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002\u57fa\u4e8eReAct\u7684WebDancer\u4ee3\u7406\u5b9e\u73b0\u3002", "result": "\u5728GAIA\u548cWebWalkerQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u8bad\u7ec3\u8303\u5f0f\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u81ea\u4e3b\u4ee3\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u8def\u5f84\u548c\u5b9e\u8df5\u6d1e\u5bdf\u3002", "keywords": "\u81ea\u4e3b\u4ee3\u7406, \u4fe1\u606f\u641c\u7d22, \u591a\u6b65\u63a8\u7406, \u5f3a\u5316\u5b66\u4e60, WebDancer"}}
{"id": "2505.22355", "pdf": "https://arxiv.org/pdf/2505.22355", "abs": "https://arxiv.org/abs/2505.22355", "authors": ["Yongkang Liu", "Xingle Xu", "Ercong Nie", "Zijing Wang", "Shi Feng", "Daling Wang", "Qian Li", "Hinrich Sch\u00fctze"], "title": "Look Within or Look Beyond? A Theoretical Comparison Between Parameter-Efficient and Full Fine-Tuning", "categories": ["cs.LG"], "comment": null, "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods achieve performance comparable\nto Full Fine-Tuning (FFT) while requiring significantly fewer computing\nresources, making it the go-to choice for researchers. We find that although\nPEFT can achieve competitive results on some benchmarks, its performance falls\nshort of FFT in complex tasks, such as reasoning and instruction-based\nfine-tuning. In this paper, we compare the characteristics of PEFT and FFT in\nterms of representational capacity and robustness based on optimization theory.\nWe theoretically demonstrate that PEFT is a strict subset of FFT. By providing\ntheoretical upper bounds for PEFT, we show that the limited parameter space\nconstrains the model's representational ability, making it more susceptible to\nperturbations. Experiments on 15 datasets encompassing classification,\ngeneration, reasoning, instruction fine-tuning tasks and 11 adversarial test\nsets validate our theories. We hope that these results spark further research\nbeyond the realms of well established PEFT. The source code is in the anonymous\nGithub repository\\footnote{https://github.com/misonsky/PEFTEval}.", "AI": {"tldr": "\u6458\u8981\u8ba8\u8bba\u4e86Parameter-Efficient Fine-Tuning (PEFT) \u65b9\u6cd5\u5728\u8d44\u6e90\u6548\u7387\u4e0a\u7684\u4f18\u52bf\uff0c\u4f46\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u63a8\u7406\u548c\u6307\u4ee4\u5fae\u8c03\uff09\u4e2d\u8868\u73b0\u4e0d\u53caFull Fine-Tuning (FFT)\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7814\u7a76\u53d1\u73b0PEFT\u5728\u8868\u793a\u80fd\u529b\u548c\u9c81\u68d2\u6027\u4e0a\u53d7\u9650\uff0c\u671f\u671b\u5f15\u53d1\u66f4\u6df1\u5165\u7684\u7814\u7a76\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22PEFT\u65b9\u6cd5\u5728\u8282\u7ea6\u8d44\u6e90\u7684\u540c\u65f6\uff0c\u4e3a\u4f55\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u5982FFT\uff0c\u5e76\u4ece\u7406\u8bba\u89d2\u5ea6\u89e3\u91ca\u5176\u5c40\u9650\u6027\u3002", "method": "\u57fa\u4e8e\u4f18\u5316\u7406\u8bba\uff0c\u6bd4\u8f83PEFT\u548cFFT\u7684\u8868\u793a\u80fd\u529b\u4e0e\u9c81\u68d2\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u572815\u4e2a\u6570\u636e\u96c6\u548c11\u4e2a\u5bf9\u6297\u6d4b\u8bd5\u96c6\u4e0a\u9a8c\u8bc1\u7406\u8bba\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793aPEFT\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u8f83\u5dee\uff0c\u7406\u8bba\u5206\u6790\u8868\u660e\u5176\u53c2\u6570\u7a7a\u95f4\u7684\u9650\u5236\u5bfc\u81f4\u8868\u793a\u80fd\u529b\u4e0d\u8db3\uff0c\u4e14\u5bf9\u6270\u52a8\u66f4\u654f\u611f\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51faPEFT\u7684\u5c40\u9650\u6027\uff0c\u5e76\u547c\u5401\u672a\u6765\u7814\u7a76\u7a81\u7834\u73b0\u6709PEFT\u65b9\u6cd5\u7684\u5c40\u9650\u3002", "keywords": "Parameter-Efficient Fine-Tuning, Full Fine-Tuning, \u4f18\u5316\u7406\u8bba, \u8868\u793a\u80fd\u529b, \u9c81\u68d2\u6027"}}
{"id": "2505.21703", "pdf": "https://arxiv.org/pdf/2505.21703", "abs": "https://arxiv.org/abs/2505.21703", "authors": ["Julia Boone", "Tolunay Seyfi", "Fatemeh Afghah"], "title": "A Joint Reconstruction-Triplet Loss Autoencoder Approach Towards Unseen Attack Detection in IoV Networks", "categories": ["cs.CR", "cs.AI", "cs.NI"], "comment": "Accepted for publication in the IEEE Internet of Things Journal\n  (IoT-J)", "summary": "Internet of Vehicles (IoV) systems, while offering significant advancements\nin transportation efficiency and safety, introduce substantial security\nvulnerabilities due to their highly interconnected nature. These dynamic\nsystems produce massive amounts of data between vehicles, infrastructure, and\ncloud services and present a highly distributed framework with a wide attack\nsurface. In considering network-centered attacks on IoV systems, attacks such\nas Denial-of-Service (DoS) can prohibit the communication of essential physical\ntraffic safety information between system elements, illustrating that the\nsecurity concerns for these systems go beyond the traditional confidentiality,\nintegrity, and availability concerns of enterprise systems. Given the\ncomplexity and volume of data generated by IoV systems, traditional security\nmechanisms are often inadequate for accurately detecting sophisticated and\nevolving cyberattacks. Here, we present an unsupervised autoencoder method\ntrained entirely on benign network data for the purpose of unseen attack\ndetection in IoV networks. We leverage a weighted combination of reconstruction\nand triplet margin loss to guide the autoencoder training and develop a diverse\nrepresentation of the benign training set. We conduct extensive experiments on\nrecent network intrusion datasets from two different application domains,\nindustrial IoT and home IoT, that represent the modern IoV task. We show that\nour method performs robustly for all unseen attack types, with roughly 99%\naccuracy on benign data and between 97% and 100% performance on anomaly data.\nWe extend these results to show that our model is adaptable through the use of\ntransfer learning, achieving similarly high results while leveraging domain\nfeatures from one domain to another.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u76d1\u7763\u81ea\u7f16\u7801\u5668\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u8f66\u8054\u7f51\uff08IoV\uff09\u4e2d\u68c0\u6d4b\u672a\u77e5\u653b\u51fb\uff0c\u91c7\u7528\u52a0\u6743\u91cd\u6784\u548c\u4e09\u91cd\u8fb9\u9645\u635f\u5931\u4f18\u5316\u8bad\u7ec3\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\u548c\u53ef\u8fc1\u79fb\u6027\u3002", "motivation": "\u8f66\u8054\u7f51\uff08IoV\uff09\u9ad8\u5ea6\u4e92\u8054\u7684\u7279\u6027\u5e26\u6765\u663e\u8457\u5b89\u5168\u98ce\u9669\uff0c\u4f20\u7edf\u5b89\u5168\u673a\u5236\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u653b\u51fb\u3002", "method": "\u5229\u7528\u65e0\u76d1\u7763\u81ea\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u52a0\u6743\u91cd\u6784\u548c\u4e09\u91cd\u8fb9\u9645\u635f\u5931\u8bad\u7ec3\uff0c\u4e13\u6ce8\u4e8e\u826f\u6027\u6570\u636e\u4ee5\u68c0\u6d4b\u5f02\u5e38\u3002", "result": "\u5728\u5de5\u4e1aIoT\u548c\u5bb6\u5eadIoT\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5bf9\u672a\u77e5\u653b\u51fb\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe97%-100%\uff0c\u826f\u6027\u6570\u636e\u51c6\u786e\u7387\u7ea699%\uff0c\u5e76\u5c55\u793a\u51fa\u4f18\u79c0\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u8f66\u8054\u7f51\u4e2d\u7684\u672a\u77e5\u653b\u51fb\uff0c\u4e14\u5177\u6709\u8de8\u9886\u57df\u9002\u5e94\u6027\u3002", "keywords": "\u8f66\u8054\u7f51\uff08IoV\uff09\u3001\u65e0\u76d1\u7763\u5b66\u4e60\u3001\u81ea\u7f16\u7801\u5668\u3001\u5f02\u5e38\u68c0\u6d4b\u3001\u8fc1\u79fb\u5b66\u4e60"}}
{"id": "2505.22653", "pdf": "https://arxiv.org/pdf/2505.22653", "abs": "https://arxiv.org/abs/2505.22653", "authors": ["Ang Lv", "Ruobing Xie", "Xingwu Sun", "Zhanhui Kang", "Rui Yan"], "title": "The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Recent studies on post-training large language models (LLMs) for reasoning\nthrough reinforcement learning (RL) typically focus on tasks that can be\naccurately verified and rewarded, such as solving math problems. In contrast,\nour research investigates the impact of reward noise, a more practical\nconsideration for real-world scenarios involving the post-training of LLMs\nusing reward models. We found that LLMs demonstrate strong robustness to\nsubstantial reward noise. For example, manually flipping 40% of the reward\nfunction's outputs in math tasks still allows a Qwen-2.5-7B model to achieve\nrapid convergence, improving its performance on math tasks from 5% to 72%,\ncompared to the 75% accuracy achieved by a model trained with noiseless\nrewards. Surprisingly, by only rewarding the appearance of key reasoning\nphrases (namely reasoning pattern reward, RPR), such as ``first, I need\nto''-without verifying the correctness of answers, the model achieved peak\ndownstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models\ntrained with strict correctness verification and accurate rewards. Recognizing\nthe importance of the reasoning process over the final results, we combined RPR\nwith noisy reward models. RPR helped calibrate the noisy reward models,\nmitigating potential false negatives and enhancing the LLM's performance on\nopen-ended tasks. These findings suggest the importance of improving models'\nfoundational abilities during the pre-training phase while providing insights\nfor advancing post-training techniques. Our code and scripts are available at\nhttps://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5956\u52b1\u566a\u58f0\u5bf9\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63a8\u7406\u4efb\u52a1\u540e\u8bad\u7ec3\u4e2d\u7684\u5f71\u54cd\uff0c\u53d1\u73b0LLMs\u5bf9\u9ad8\u566a\u58f0\u5177\u6709\u5f3a\u9c81\u68d2\u6027\u3002\u5f15\u5165\u63a8\u7406\u6a21\u5f0f\u5956\u52b1\uff08RPR\uff09\u53ef\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u5b9e\u9645\u573a\u666f\u4e2d\u5956\u52b1\u566a\u58f0\u5bf9LLM\u540e\u8bad\u7ec3\u7684\u5f71\u54cd\uff0c\u65e8\u5728\u63d0\u5347\u6a21\u578b\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u4eba\u4e3a\u7ffb\u8f6c\u5956\u52b1\u51fd\u6570\u8f93\u51fa\u5f15\u5165\u566a\u58f0\uff0c\u7ed3\u5408\u63a8\u7406\u6a21\u5f0f\u5956\u52b1\uff08RPR\uff09\uff0c\u6d4b\u8bd5Qwen-2.5-7B\u6a21\u578b\u5728\u6570\u5b66\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5373\u4f7f\u5956\u52b1\u566a\u58f0\u8fbe40%\uff0c\u6a21\u578b\u4ecd\u5feb\u901f\u6536\u655b\uff0c\u6027\u80fd\u4ece5%\u63d0\u5347\u81f372%\uff1bRPR\u5355\u72ec\u4f7f\u7528\u4e5f\u80fd\u8fbe\u523070%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u63a8\u7406\u8fc7\u7a0b\u6bd4\u7ed3\u679c\u66f4\u91cd\u8981\uff0cRPR\u53ef\u6821\u51c6\u566a\u58f0\u5956\u52b1\u6a21\u578b\uff0c\u63d0\u5347\u5f00\u653e\u4efb\u52a1\u8868\u73b0\u3002\u5efa\u8bae\u52a0\u5f3a\u9884\u8bad\u7ec3\u57fa\u7840\u80fd\u529b\u5e76\u4f18\u5316\u540e\u8bad\u7ec3\u6280\u672f\u3002", "keywords": "LLMs, \u5956\u52b1\u566a\u58f0, \u63a8\u7406\u8bad\u7ec3, RPR, \u9c81\u68d2\u6027"}}
{"id": "2505.22356", "pdf": "https://arxiv.org/pdf/2505.22356", "abs": "https://arxiv.org/abs/2505.22356", "authors": ["Ang\u00e9line Pouget", "Mohammad Yaghini", "Stephan Rabanser", "Nicolas Papernot"], "title": "Suitability Filter: A Statistical Framework for Classifier Evaluation in Real-World Deployment Settings", "categories": ["cs.LG", "cs.AI", "cs.CY", "stat.ML"], "comment": "Accepted to ICML 2025", "summary": "Deploying machine learning models in safety-critical domains poses a key\nchallenge: ensuring reliable model performance on downstream user data without\naccess to ground truth labels for direct validation. We propose the suitability\nfilter, a novel framework designed to detect performance deterioration by\nutilizing suitability signals -- model output features that are sensitive to\ncovariate shifts and indicative of potential prediction errors. The suitability\nfilter evaluates whether classifier accuracy on unlabeled user data shows\nsignificant degradation compared to the accuracy measured on the labeled test\ndataset. Specifically, it ensures that this degradation does not exceed a\npre-specified margin, which represents the maximum acceptable drop in accuracy.\nTo achieve reliable performance evaluation, we aggregate suitability signals\nfor both test and user data and compare these empirical distributions using\nstatistical hypothesis testing, thus providing insights into decision\nuncertainty. Our modular method adapts to various models and domains. Empirical\nevaluations across different classification tasks demonstrate that the\nsuitability filter reliably detects performance deviations due to covariate\nshift. This enables proactive mitigation of potential failures in high-stakes\napplications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\u2014\u2014'\u9002\u5b9c\u6027\u8fc7\u6ee4\u5668'\uff0c\u7528\u4e8e\u5728\u65e0\u6807\u6ce8\u6570\u636e\u7684\u573a\u666f\u4e0b\u68c0\u6d4b\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u9000\u5316\uff0c\u901a\u8fc7\u5229\u7528\u5bf9\u534f\u53d8\u91cf\u504f\u79fb\u654f\u611f\u7684\u6a21\u578b\u8f93\u51fa\u7279\u5f81\uff08\u9002\u5b9c\u6027\u4fe1\u53f7\uff09\uff0c\u5e76\u4e0e\u9884\u8bbe\u7684\u7cbe\u5ea6\u964d\u5e45\u9608\u503c\u6bd4\u8f83\uff0c\u786e\u4fdd\u6a21\u578b\u6027\u80fd\u53ef\u9760\u6027\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u90e8\u7f72\u673a\u5668\u5b66\u4e60\u6a21\u578b\u65f6\uff0c\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u5bfc\u81f4\u65e0\u6cd5\u76f4\u63a5\u9a8c\u8bc1\u6a21\u578b\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u68c0\u6d4b\u6027\u80fd\u9000\u5316\uff0c\u4ee5\u9884\u9632\u6f5c\u5728\u5931\u8d25\u3002", "method": "\u91c7\u7528\u9002\u5b9c\u6027\u4fe1\u53f7\uff0c\u901a\u8fc7\u7edf\u8ba1\u5047\u8bbe\u68c0\u9a8c\u6bd4\u8f83\u6d4b\u8bd5\u6570\u636e\u548c\u7528\u6237\u6570\u636e\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u5224\u65ad\u6027\u80fd\u9000\u5316\u662f\u5426\u8d85\u8fc7\u9884\u8bbe\u9608\u503c\u3002", "result": "\u5728\u4e0d\u540c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u7531\u534f\u53d8\u91cf\u504f\u79fb\u5f15\u8d77\u7684\u6027\u80fd\u504f\u5dee\u3002", "conclusion": "\u9002\u5b9c\u6027\u8fc7\u6ee4\u5668\u4e3a\u9ad8\u98ce\u9669\u5e94\u7528\u63d0\u4f9b\u4e86\u4e3b\u52a8\u6027\u80fd\u76d1\u6d4b\u673a\u5236\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u53ef\u9760\u6027\u3002", "keywords": "\u673a\u5668\u5b66\u4e60, \u9002\u5b9c\u6027\u4fe1\u53f7, \u534f\u53d8\u91cf\u504f\u79fb, \u7edf\u8ba1\u5047\u8bbe\u68c0\u9a8c, \u6027\u80fd\u76d1\u6d4b"}}
{"id": "2505.21715", "pdf": "https://arxiv.org/pdf/2505.21715", "abs": "https://arxiv.org/abs/2505.21715", "authors": ["Md. Zahid Hossain", "Mustofa Ahmed", "Most. Sharmin Sultana Samu", "Md. Rakibul Islam"], "title": "Privacy-Preserving Chest X-ray Report Generation via Multimodal Federated Learning with ViT and GPT-2", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Preprint, manuscript under-review", "summary": "The automated generation of radiology reports from chest X-ray images holds\nsignificant promise in enhancing diagnostic workflows while preserving patient\nprivacy. Traditional centralized approaches often require sensitive data\ntransfer, posing privacy concerns. To address this, the study proposes a\nMultimodal Federated Learning framework for chest X-ray report generation using\nthe IU-Xray dataset. The system utilizes a Vision Transformer (ViT) as the\nencoder and GPT-2 as the report generator, enabling decentralized training\nwithout sharing raw data. Three Federated Learning (FL) aggregation strategies:\nFedAvg, Krum Aggregation and a novel Loss-aware Federated Averaging (L-FedAvg)\nwere evaluated. Among these, Krum Aggregation demonstrated superior performance\nacross lexical and semantic evaluation metrics such as ROUGE, BLEU, BERTScore\nand RaTEScore. The results show that FL can match or surpass centralized models\nin generating clinically relevant and semantically rich radiology reports. This\nlightweight and privacy-preserving framework paves the way for collaborative\nmedical AI development without compromising data confidentiality.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u8054\u90a6\u5b66\u4e60\u7684\u80f8\u90e8X\u5149\u62a5\u544a\u751f\u6210\u6846\u67b6\uff0c\u5229\u7528ViT\u7f16\u7801\u5668\u548cGPT-2\u751f\u6210\u5668\uff0c\u907f\u514d\u539f\u59cb\u6570\u636e\u5171\u4eab\uff0c\u5e76\u901a\u8fc7\u4e09\u79cd\u8054\u90a6\u5b66\u4e60\u805a\u5408\u7b56\u7565\u9a8c\u8bc1\uff0c\u5176\u4e2dKrum Aggregation\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u4f20\u7edf\u96c6\u4e2d\u5f0f\u65b9\u6cd5\u9700\u8981\u4f20\u8f93\u654f\u611f\u6570\u636e\uff0c\u5b58\u5728\u9690\u79c1\u95ee\u9898\uff0c\u56e0\u6b64\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u5206\u6563\u5f0f\u62a5\u544a\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u91c7\u7528Vision Transformer (ViT) \u4f5c\u4e3a\u7f16\u7801\u5668\uff0cGPT-2\u4f5c\u4e3a\u62a5\u544a\u751f\u6210\u5668\uff0c\u8bc4\u4f30\u4e86FedAvg\u3001Krum Aggregation\u548cL-FedAvg\u4e09\u79cd\u8054\u90a6\u5b66\u4e60\u805a\u5408\u7b56\u7565\u3002", "result": "Krum Aggregation\u5728ROUGE\u3001BLEU\u3001BERTScore\u548cRaTEScore\u7b49\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4f18\uff0c\u8054\u90a6\u5b66\u4e60\u6a21\u578b\u53ef\u5339\u914d\u6216\u8d85\u8d8a\u96c6\u4e2d\u5f0f\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u8f7b\u91cf\u7ea7\u6846\u67b6\u4e3a\u533b\u7597AI\u534f\u4f5c\u5f00\u53d1\u63d0\u4f9b\u4e86\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u62a5\u544a\u7684\u4e34\u5e8a\u76f8\u5173\u6027\u548c\u8bed\u4e49\u4e30\u5bcc\u6027\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60, \u80f8\u90e8X\u5149, \u62a5\u544a\u751f\u6210, \u9690\u79c1\u4fdd\u62a4, Vision Transformer, GPT-2"}}
{"id": "2505.22661", "pdf": "https://arxiv.org/pdf/2505.22661", "abs": "https://arxiv.org/abs/2505.22661", "authors": ["Qingchen Yu", "Zifan Zheng", "Ding Chen", "Simin Niu", "Bo Tang", "Feiyu Xiong", "Zhiyu Li"], "title": "GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating LLMs in Domain-Specific Knowledge and Reasoning", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025", "summary": "The evaluation of large language models (LLMs) has traditionally relied on\nstatic benchmarks, a paradigm that poses two major limitations: (1) predefined\ntest sets lack adaptability to diverse application domains, and (2)\nstandardized evaluation protocols often fail to capture fine-grained\nassessments of domain-specific knowledge and contextual reasoning abilities. To\novercome these challenges, we propose GuessArena, an adaptive evaluation\nframework grounded in adversarial game-based interactions. Inspired by the\ninteractive structure of the Guess Who I Am? game, our framework seamlessly\nintegrates dynamic domain knowledge modeling with progressive reasoning\nassessment to improve evaluation fidelity. Empirical studies across five\nvertical domains-finance, healthcare, manufacturing, information technology,\nand education-demonstrate that GuessArena effectively distinguishes LLMs in\nterms of domain knowledge coverage and reasoning chain completeness. Compared\nto conventional benchmarks, our method provides substantial advantages in\ninterpretability, scalability, and scenario adaptability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86GuessArena\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u6e38\u620f\u4ea4\u4e92\u89e3\u51b3\u4f20\u7edf\u9759\u6001\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u589e\u5f3a\u8bc4\u4f30\u7684\u9002\u5e94\u6027\u548c\u7ec6\u7c92\u5ea6\u6027\u3002", "motivation": "\u4f20\u7edf\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4f9d\u8d56\u9759\u6001\u57fa\u51c6\uff0c\u7f3a\u4e4f\u9886\u57df\u9002\u5e94\u6027\u548c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u80fd\u529b\u3002", "method": "\u57fa\u4e8e'Guess Who I Am?'\u6e38\u620f\u7684\u4ea4\u4e92\u7ed3\u6784\uff0c\u7ed3\u5408\u52a8\u6001\u9886\u57df\u77e5\u8bc6\u5efa\u6a21\u548c\u6e10\u8fdb\u63a8\u7406\u8bc4\u4f30\u3002", "result": "\u5728\u91d1\u878d\u3001\u533b\u7597\u7b49\u4e94\u4e2a\u9886\u57df\u9a8c\u8bc1\u4e86GuessArena\u5728\u533a\u5206\u9886\u57df\u77e5\u8bc6\u8986\u76d6\u548c\u63a8\u7406\u94fe\u5b8c\u6574\u6027\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "GuessArena\u63d0\u4f9b\u6bd4\u4f20\u7edf\u57fa\u51c6\u66f4\u9ad8\u7684\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u573a\u666f\u9002\u5e94\u6027\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u8bc4\u4f30\u6846\u67b6,\u5bf9\u6297\u6027\u6e38\u620f,\u52a8\u6001\u5efa\u6a21,\u63a8\u7406\u8bc4\u4f30"}}
{"id": "2505.22358", "pdf": "https://arxiv.org/pdf/2505.22358", "abs": "https://arxiv.org/abs/2505.22358", "authors": ["Zhiyi Wan", "Wanrou Du", "Liang Li", "Miao Pan", "Xiaoqi Qin"], "title": "Budget-Adaptive Adapter Tuning in Orthogonal Subspaces for Continual Learning in LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) often suffer from catastrophic forgetting in\ncontinual learning (CL) scenarios, where performance on previously learned\ntasks degrades severely while training on sequentially arriving tasks. Although\npioneering CL approaches using orthogonal subspaces can mitigate task\ninterference, they typically employ fixed budget allocation, neglecting the\nvarying complexity across tasks and layers. Besides, recent budget-adaptive\ntuning methods for LLMs often adopt multi-stage paradigms that decouple\noptimization and budget allocation. Such decoupling results in potential\nmisalignment, which hinders those approaches' practical application in CL\nscenarios. To address these limitations, we propose OA-Adapter, a novel\nparameter-efficient approach for continual learning in LLMs that unifies\ndynamic budget adaptation with orthogonal subspace learning in a single\nend-to-end training stage. Specifically, OA-Adapter introduces a dynamic\nbottleneck dimension adaptation mechanism that simultaneously allocates an\nefficient parameter budget and optimizes task objectives without misalignment.\nTo effectively preserve previously acquired knowledge while coordinating with\nthe dynamic budget allocation, orthogonal constraints are applied specifically\nbetween the parameter subspace of the current task and the dynamically\nallocated parameter subspaces of historical tasks. Experimental results on\ncontinual learning benchmarks demonstrate that OA-Adapter outperforms\nstate-of-the-art methods in both accuracy and parameter efficiency, achieving\nhigher average accuracy while using 58.5% fewer parameters on the standard CL\nbenchmark.", "AI": {"tldr": "OA-Adapter \u662f\u4e00\u4e2a\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u7684\u65b0\u578b\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u9884\u7b97\u5206\u914d\u548c\u6b63\u4ea4\u5b50\u7a7a\u95f4\u5b66\u4e60\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u53c2\u6570\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u56e0\u4efb\u52a1\u5e72\u6270\u5bfc\u81f4\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u73b0\u6709\u65b9\u6cd5\u56e0\u56fa\u5b9a\u9884\u7b97\u5206\u914d\u6216\u591a\u9636\u6bb5\u4f18\u5316\u5bfc\u81f4\u7684\u6027\u80fd\u4e0d\u8db3\u3002", "method": "\u63d0\u51faOA-Adapter\uff0c\u7ed3\u5408\u52a8\u6001\u74f6\u9888\u7ef4\u5ea6\u9002\u5e94\u673a\u5236\u548c\u6b63\u4ea4\u7ea6\u675f\uff0c\u7edf\u4e00\u4f18\u5316\u9884\u7b97\u5206\u914d\u548c\u4efb\u52a1\u76ee\u6807\u3002", "result": "\u5728\u6807\u51c6\u6301\u7eed\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOA-Adapter \u51c6\u786e\u7387\u66f4\u9ad8\u4e14\u53c2\u6570\u4f7f\u7528\u91cf\u51cf\u5c1158.5%\u3002", "conclusion": "OA-Adapter \u901a\u8fc7\u7aef\u5230\u7aef\u7684\u52a8\u6001\u9884\u7b97\u548c\u6b63\u4ea4\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6301\u7eed\u5b66\u4e60\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u6301\u7eed\u5b66\u4e60\uff0c\u707e\u96be\u6027\u9057\u5fd8\uff0c\u52a8\u6001\u9884\u7b97\uff0c\u6b63\u4ea4\u5b50\u7a7a\u95f4"}}
{"id": "2505.22662", "pdf": "https://arxiv.org/pdf/2505.22662", "abs": "https://arxiv.org/abs/2505.22662", "authors": ["Feng Luo", "Yu-Neng Chuang", "Guanchu Wang", "Hoang Anh Duy Le", "Shaochen Zhong", "Hongyi Liu", "Jiayi Yuan", "Yang Sui", "Vladimir Braverman", "Vipin Chaudhary", "Xia Hu"], "title": "AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The reasoning-capable large language models (LLMs) demonstrate strong\nperformance on complex reasoning tasks but often suffer from overthinking,\ngenerating unnecessarily long chain-of-thought (CoT) reasoning paths for easy\nreasoning questions, thereby increasing inference cost and latency. Recent\napproaches attempt to address this challenge by manually deciding when to apply\nlong or short reasoning. However, they lack the flexibility to adapt CoT length\ndynamically based on question complexity. In this paper, we propose Auto\nLong-Short Reasoning (AutoL2S), a dynamic and model-agnostic framework that\nenables LLMs to dynamically compress their generated reasoning path based on\nthe complexity of the reasoning question. AutoL2S enables a learned paradigm,\nin which LLMs themselves can decide when longer reasoning is necessary and when\nshorter reasoning suffices, by training on data annotated with our proposed\nmethod, which includes both long and short CoT paths and a special <EASY>\ntoken. We then use <EASY> token to indicate when the model can skip generating\nlengthy CoT reasoning. This proposed annotation strategy can enhance the LLMs'\nability to generate shorter CoT reasoning paths with improved quality after\ntraining. Extensive evaluation results show that AutoL2S reduces the length of\nreasoning generation by up to 57% without compromising performance,\ndemonstrating the effectiveness of AutoL2S for scalable and efficient LLM\nreasoning.", "AI": {"tldr": "AutoL2S is a framework that enables LLMs to dynamically adjust the length of chain-of-thought reasoning based on question complexity, reducing reasoning length by up to 57% without performance loss.", "motivation": "LLMs often generate unnecessarily long reasoning paths for simple questions, increasing inference cost and latency. Existing methods lack flexibility in dynamically adapting reasoning length.", "method": "AutoL2S trains LLMs on annotated data with both long and short CoT paths, including an <EASY> token to signal when shorter reasoning suffices, enabling dynamic CoT compression.", "result": "AutoL2S reduces reasoning path length by up to 57% without compromising performance, demonstrating efficient and scalable LLM reasoning.", "conclusion": "AutoL2S effectively balances reasoning length and performance, offering a flexible and model-agnostic solution for efficient LLM reasoning.", "keywords": "Large Language Models, Chain-of-Thought Reasoning, Dynamic Reasoning, AutoL2S, Efficiency"}}
{"id": "2505.22359", "pdf": "https://arxiv.org/pdf/2505.22359", "abs": "https://arxiv.org/abs/2505.22359", "authors": ["Matan Schliserman", "Tomer Koren"], "title": "Multiclass Loss Geometry Matters for Generalization of Gradient Descent in Separable Classification", "categories": ["cs.LG"], "comment": null, "summary": "We study the generalization performance of unregularized gradient methods for\nseparable linear classification. While previous work mostly deal with the\nbinary case, we focus on the multiclass setting with $k$ classes and establish\nnovel population risk bounds for Gradient Descent for loss functions that decay\nto zero. In this setting, we show risk bounds that reveal that convergence\nrates are crucially influenced by the geometry of the loss template, as\nformalized by Wang and Scott (2024), rather than of the loss function itself.\nParticularly, we establish risk upper bounds that holds for any decay rate of\nthe loss whose template is smooth with respect to the $p$-norm. In the case of\nexponentially decaying losses, our results indicates a contrast between the\n$p=\\infty$ case, where the risk exhibits a logarithmic dependence on $k$, and\n$p=2$ where the risk scales linearly with $k$. To establish this separation\nformally, we also prove a lower bound in the latter scenario, demonstrating\nthat the polynomial dependence on $k$ is unavoidable. Central to our analysis\nis a novel bound on the Rademacher complexity of low-noise vector-valued linear\npredictors with a loss template smooth w.r.t.~general $p$-norms.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u65e0\u6b63\u5219\u5316\u68af\u5ea6\u65b9\u6cd5\u5728\u591a\u7c7b\u7ebf\u6027\u5206\u7c7b\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u63d0\u51fa\u65b0\u7684\u4eba\u53e3\u98ce\u9669\u754c\u9650\uff0c\u63ed\u793a\u6536\u655b\u901f\u5ea6\u53d7\u635f\u5931\u6a21\u677f\u51e0\u4f55\u5f62\u72b6\u5f71\u54cd\u800c\u975e\u635f\u5931\u51fd\u6570\u672c\u8eab\u3002", "motivation": "\u4ee5\u5f80\u7814\u7a76\u591a\u96c6\u4e2d\u5728\u4e8c\u5143\u5206\u7c7b\uff0c\u672c\u6587\u6269\u5c55\u81f3\u591a\u7c7b\u5206\u7c7b\uff0c\u63a2\u7d22\u68af\u5ea6\u4e0b\u964d\u5728\u4e0d\u540c\u635f\u5931\u51fd\u6570\u4e0b\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u5206\u6790\u635f\u5931\u6a21\u677f\u7684\u51e0\u4f55\u5f62\u72b6\uff08$p$-\u8303\u6570\u4e0b\u7684\u5e73\u6ed1\u6027\uff09\uff0c\u5efa\u7acb\u98ce\u9669\u4e0a\u754c\uff0c\u5e76\u8bc1\u660e\u5728\u6307\u6570\u8870\u51cf\u635f\u5931\u4e0b\u7684\u6536\u655b\u7387\u5dee\u5f02\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c$p=\\infty$\u65f6\u98ce\u9669\u4e0e$k$\u5bf9\u6570\u76f8\u5173\uff0c$p=2$\u65f6\u7ebf\u6027\u76f8\u5173\uff0c\u4e14\u591a\u9879\u5f0f\u4f9d\u8d56\u65e0\u6cd5\u907f\u514d\u3002", "conclusion": "\u635f\u5931\u6a21\u677f\u7684\u51e0\u4f55\u5f62\u72b6\u662f\u5f71\u54cd\u68af\u5ea6\u4e0b\u964d\u6cdb\u5316\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff0c\u7279\u522b\u662f\u5728\u591a\u7c7b\u5206\u7c7b\u4e2d\u3002", "keywords": "\u68af\u5ea6\u65b9\u6cd5\u3001\u591a\u7c7b\u5206\u7c7b\u3001\u6cdb\u5316\u6027\u80fd\u3001\u635f\u5931\u6a21\u677f\u3001\u98ce\u9669\u754c\u9650"}}
{"id": "2505.21720", "pdf": "https://arxiv.org/pdf/2505.21720", "abs": "https://arxiv.org/abs/2505.21720", "authors": ["Vanessa Utz"], "title": "Responsible Data Stewardship: Generative AI and the Digital Waste Problem", "categories": ["cs.CY", "cs.AI"], "comment": "8 pages, submitted to AAAI/ACM Conference on AI, Ethics and Society", "summary": "As generative AI systems become widely adopted, they enable unprecedented\ncreation levels of synthetic data across text, images, audio, and video\nmodalities. While research has addressed the energy consumption of model\ntraining and inference, a critical sustainability challenge remains\nunderstudied: digital waste. This term refers to stored data that consumes\nresources without serving a specific (and/or immediate) purpose. This paper\npresents this terminology in the AI context and introduces digital waste as an\nethical imperative within (generative) AI development, positioning\nenvironmental sustainability as core for responsible innovation. Drawing from\nestablished digital resource management approaches, we examine how other\ndisciplines manage digital waste and identify transferable approaches for the\nAI community. We propose specific recommendations encompassing re-search\ndirections, technical interventions, and cultural shifts to mitigate the\nenvironmental consequences of in-definite data storage. By expanding AI ethics\nbeyond immediate concerns like bias and privacy to include inter-generational\nenvironmental justice, this work contributes to a more comprehensive ethical\nframework that considers the complete lifecycle impact of generative AI\nsystems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u751f\u6210\u5f0fAI\u7cfb\u7edf\u4e2d\u957f\u671f\u88ab\u5ffd\u89c6\u7684\u53ef\u6301\u7eed\u6027\u95ee\u9898\u2014\u2014\u6570\u5b57\u6d6a\u8d39\uff0c\u63d0\u51fa\u5c06\u5176\u4f5c\u4e3aAI\u4f26\u7406\u7684\u4e00\u90e8\u5206\uff0c\u5e76\u501f\u9274\u5176\u4ed6\u9886\u57df\u7684\u8d44\u6e90\u7ba1\u7406\u65b9\u6cd5\uff0c\u63d0\u51fa\u5177\u4f53\u5efa\u8bae\u4ee5\u51cf\u5c11\u6570\u636e\u5b58\u50a8\u5bf9\u73af\u5883\u7684\u957f\u671f\u5f71\u54cd\u3002", "motivation": "\u751f\u6210\u5f0fAI\u7684\u5e7f\u6cdb\u4f7f\u7528\u5bfc\u81f4\u5927\u91cf\u5408\u6210\u6570\u636e\u7684\u4ea7\u751f\u548c\u5b58\u50a8\uff0c\u4f46\u5176\u73af\u5883\u6210\u672c\uff08\u5982\u6570\u5b57\u6d6a\u8d39\uff09\u5c1a\u672a\u5f97\u5230\u8db3\u591f\u5173\u6ce8\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5c06\u73af\u5883\u53ef\u6301\u7eed\u6027\u7eb3\u5165AI\u4f26\u7406\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5176\u4ed6\u5b66\u79d1\u7684\u6570\u5b57\u8d44\u6e90\u7ba1\u7406\u65b9\u6cd5\uff0c\u8bc6\u522b\u53ef\u8fc1\u79fb\u81f3AI\u9886\u57df\u7684\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u4e86\u7814\u7a76\u65b9\u5411\u3001\u6280\u672f\u5e72\u9884\u548c\u6587\u5316\u8f6c\u53d8\u7684\u5177\u4f53\u5efa\u8bae\u3002", "result": "\u8bba\u6587\u6269\u5c55\u4e86AI\u4f26\u7406\u7684\u8303\u7574\uff0c\u5c06\u73af\u5883\u6b63\u4e49\u7eb3\u5165\u8003\u91cf\uff0c\u5e76\u63d0\u51fa\u4e86\u51cf\u5c11\u6570\u5b57\u6d6a\u8d39\u7684\u53ef\u884c\u65b9\u6848\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u7684\u5f00\u53d1\u9700\u517c\u987e\u73af\u5883\u53ef\u6301\u7eed\u6027\uff0c\u6570\u5b57\u6d6a\u8d39\u5e94\u6210\u4e3a\u4f26\u7406\u6846\u67b6\u7684\u6838\u5fc3\u8bae\u9898\u4e4b\u4e00\u3002", "keywords": "\u751f\u6210\u5f0fAI, \u6570\u5b57\u6d6a\u8d39, \u73af\u5883\u53ef\u6301\u7eed\u6027, AI\u4f26\u7406, \u6570\u636e\u7ba1\u7406"}}
{"id": "2505.20162", "pdf": "https://arxiv.org/pdf/2505.20162", "abs": "https://arxiv.org/abs/2505.20162", "authors": ["Alexander Panfilov", "Paul Kassianik", "Maksym Andriushchenko", "Jonas Geiping"], "title": "Capability-Based Scaling Laws for LLM Red-Teaming", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "As large language models grow in capability and agency, identifying\nvulnerabilities through red-teaming becomes vital for safe deployment. However,\ntraditional prompt-engineering approaches may prove ineffective once\nred-teaming turns into a weak-to-strong problem, where target models surpass\nred-teamers in capabilities. To study this shift, we frame red-teaming through\nthe lens of the capability gap between attacker and target. We evaluate more\nthan 500 attacker-target pairs using LLM-based jailbreak attacks that mimic\nhuman red-teamers across diverse families, sizes, and capability levels. Three\nstrong trends emerge: (i) more capable models are better attackers, (ii) attack\nsuccess drops sharply once the target's capability exceeds the attacker's, and\n(iii) attack success rates correlate with high performance on social science\nsplits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking\nscaling law that predicts attack success for a fixed target based on\nattacker-target capability gap. These findings suggest that fixed-capability\nattackers (e.g., humans) may become ineffective against future models,\nincreasingly capable open-source models amplify risks for existing systems, and\nmodel providers must accurately measure and control models' persuasive and\nmanipulative abilities to limit their effectiveness as attackers.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u8bed\u8a00\u6a21\u578b\u8d8a\u72f1\u653b\u51fb\u7684\u6210\u529f\u7387\u4e0e\u653b\u51fb\u8005-\u76ee\u6807\u80fd\u529b\u5dee\u8ddd\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u80fd\u529b\u66f4\u5f3a\u7684\u6a21\u578b\u4f5c\u4e3a\u653b\u51fb\u8005\u66f4\u6709\u6548\uff0c\u4f46\u5f53\u76ee\u6807\u80fd\u529b\u8d85\u8fc7\u653b\u51fb\u8005\u65f6\uff0c\u6210\u529f\u7387\u9aa4\u964d\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u6761\u8d8a\u72f1\u653b\u51fb\u7684\u7f29\u653e\u5b9a\u5f8b\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u548c\u81ea\u4e3b\u6027\u7684\u589e\u5f3a\uff0c\u901a\u8fc7\u7ea2\u961f\u6d4b\u8bd5\u8bc6\u522b\u6f0f\u6d1e\u5bf9\u5b89\u5168\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u653b\u51fb\u8005\u80fd\u529b\u5f31\u4e8e\u76ee\u6807\u65f6\u53ef\u80fd\u5931\u6548\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u80fd\u529b\u5dee\u8ddd\u5bf9\u653b\u51fb\u6210\u529f\u7387\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u8bc4\u4f30500\u591a\u4e2a\u57fa\u4e8eLLM\u7684\u8d8a\u72f1\u653b\u51fb\u5bf9\uff08\u6a21\u62df\u4eba\u7c7b\u7ea2\u961f\u6d4b\u8bd5\uff09\uff0c\u6db5\u76d6\u4e0d\u540c\u5bb6\u65cf\u3001\u89c4\u6a21\u548c\u80fd\u529b\u6c34\u5e73\u7684\u6a21\u578b\uff0c\u5206\u6790\u653b\u51fb\u6210\u529f\u7387\u548c\u80fd\u529b\u5dee\u8ddd\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u4e09\u4e2a\u4e3b\u8981\u8d8b\u52bf\uff1a1) \u80fd\u529b\u66f4\u5f3a\u7684\u6a21\u578b\u653b\u51fb\u66f4\u6709\u6548\uff1b2) \u76ee\u6807\u80fd\u529b\u8d85\u8fc7\u653b\u51fb\u8005\u65f6\u6210\u529f\u7387\u9aa4\u964d\uff1b3) \u653b\u51fb\u6210\u529f\u7387\u4e0eMMLU-Pro\u793e\u4f1a\u79d1\u5b66\u6d4b\u8bd5\u7684\u9ad8\u8868\u73b0\u76f8\u5173\u3002\u7531\u6b64\u63d0\u51fa\u4e86\u8d8a\u72f1\u653b\u51fb\u7684\u7f29\u653e\u5b9a\u5f8b\u3002", "conclusion": "\u56fa\u5b9a\u80fd\u529b\u7684\u653b\u51fb\u8005\uff08\u5982\u4eba\u7c7b\uff09\u53ef\u80fd\u5bf9\u672a\u6765\u7684\u6a21\u578b\u65e0\u6548\uff0c\u5f00\u6e90\u6a21\u578b\u7684\u80fd\u529b\u589e\u957f\u53ef\u80fd\u589e\u52a0\u73b0\u6709\u7cfb\u7edf\u7684\u98ce\u9669\u3002\u6a21\u578b\u63d0\u4f9b\u8005\u9700\u51c6\u786e\u6d4b\u91cf\u548c\u63a7\u5236\u6a21\u578b\u7684\u64cd\u7eb5\u80fd\u529b\u4ee5\u51cf\u5c11\u653b\u51fb\u98ce\u9669\u3002", "keywords": "\u8bed\u8a00\u6a21\u578b\uff0c\u8d8a\u72f1\u653b\u51fb\uff0c\u7ea2\u961f\u6d4b\u8bd5\uff0c\u80fd\u529b\u5dee\u8ddd\uff0c\u7f29\u653e\u5b9a\u5f8b"}}
{"id": "2505.22361", "pdf": "https://arxiv.org/pdf/2505.22361", "abs": "https://arxiv.org/abs/2505.22361", "authors": ["Xiangyu Chang", "Xi Chen", "Yining Wang", "Zhiyi Zeng"], "title": "Continuum-armed Bandit Optimization with Batch Pairwise Comparison Oracles", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "This paper studies a bandit optimization problem where the goal is to\nmaximize a function $f(x)$ over $T$ periods for some unknown strongly concave\nfunction $f$. We consider a new pairwise comparison oracle, where the\ndecision-maker chooses a pair of actions $(x, x')$ for a consecutive number of\nperiods and then obtains an estimate of $f(x)-f(x')$. We show that such a\npairwise comparison oracle finds important applications to joint pricing and\ninventory replenishment problems and network revenue management. The challenge\nin this bandit optimization is twofold. First, the decision-maker not only\nneeds to determine a pair of actions $(x, x')$ but also a stopping time $n$\n(i.e., the number of queries based on $(x, x')$). Second, motivated by our\ninventory application, the estimate of the difference $f(x)-f(x')$ is biased,\nwhich is different from existing oracles in stochastic optimization literature.\nTo address these challenges, we first introduce a discretization technique and\nlocal polynomial approximation to relate this problem to linear bandits. Then\nwe developed a tournament successive elimination technique to localize the\ndiscretized cell and run an interactive batched version of LinUCB algorithm on\ncells. We establish regret bounds that are optimal up to poly-logarithmic\nfactors. Furthermore, we apply our proposed algorithm and analytical framework\nto the two operations management problems and obtain results that improve\nstate-of-the-art results in the existing literature.", "AI": {"tldr": "\u7814\u7a76\u4e86\u901a\u8fc7\u6210\u5bf9\u6bd4\u8f83oracle\u4f18\u5316\u672a\u77e5\u5f3a\u51f9\u51fd\u6570f(x)\u7684\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u5b9a\u4ef7\u4e0e\u5e93\u5b58\u8865\u5145\u53ca\u7f51\u7edc\u6536\u5165\u7ba1\u7406\uff0c\u63d0\u51fa\u4e86\u79bb\u6563\u5316\u548c\u5c40\u90e8\u591a\u9879\u5f0f\u903c\u8fd1\u65b9\u6cd5\uff0c\u7ed3\u5408\u6bd4\u8d5b\u5f0f\u9010\u6b65\u6d88\u9664\u6280\u672f\u4e0eLinUCB\u7b97\u6cd5\uff0c\u53d6\u5f97\u4e86\u6700\u4f18\u7684\u9057\u61be\u754c\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u8054\u5408\u5b9a\u4ef7\u4e0e\u5e93\u5b58\u8865\u5145\u95ee\u9898\u4ee5\u53ca\u7f51\u7edc\u6536\u5165\u7ba1\u7406\u4e2d\u7684\u5b9e\u9645\u9700\u6c42\uff0c\u73b0\u6709\u968f\u673a\u4f18\u5316\u6587\u732e\u4e2d\u7684oracle\u65e0\u6cd5\u5904\u7406\u504f\u5dee\u4f30\u8ba1\u7684\u60c5\u51b5\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u79bb\u6563\u5316\u6280\u672f\u548c\u5c40\u90e8\u591a\u9879\u5f0f\u903c\u8fd1\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u7ebf\u6027bandits\uff0c\u7ed3\u5408\u6bd4\u8d5b\u5f0f\u9010\u6b65\u6d88\u9664\u6280\u672f\u5b9a\u4f4d\u79bb\u6563\u5316\u5355\u5143\uff0c\u5e76\u8fd0\u884c\u4ea4\u4e92\u5f0f\u6279\u5904\u7406\u7684LinUCB\u7b97\u6cd5\u3002", "result": "\u5efa\u7acb\u4e86\u6700\u4f18\uff08\u9664\u591a\u5bf9\u6570\u56e0\u5b50\u5916\uff09\u7684\u9057\u61be\u754c\uff0c\u5e76\u5728\u4e24\u4e2a\u8fd0\u8425\u7ba1\u7406\u95ee\u9898\u4e0a\u5e94\u7528\uff0c\u6539\u8fdb\u4e86\u73b0\u6709\u6587\u732e\u4e2d\u7684\u6700\u4f18\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u6210\u5bf9\u6bd4\u8f83oracle\u548c\u63d0\u51fa\u7684\u7b97\u6cd5\u6846\u67b6\uff0c\u80fd\u6709\u6548\u4f18\u5316\u672a\u77e5\u5f3a\u51f9\u51fd\u6570\uff0c\u5e76\u5728\u8fd0\u8425\u7ba1\u7406\u95ee\u9898\u4e2d\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u3002", "keywords": "bandit\u4f18\u5316, \u6210\u5bf9\u6bd4\u8f83oracle, \u5f3a\u51f9\u51fd\u6570, \u79bb\u6563\u5316, LinUCB, \u8fd0\u8425\u7ba1\u7406"}}
{"id": "2505.21510", "pdf": "https://arxiv.org/pdf/2505.21510", "abs": "https://arxiv.org/abs/2505.21510", "authors": ["Chundra Cathcart"], "title": "Complexity counts: global and local perspectives on Indo-Aryan numeral systems", "categories": ["physics.soc-ph", "cs.CL"], "comment": null, "summary": "The numeral systems of Indo-Aryan languages such as Hindi, Gujarati, and\nBengali are highly unusual in that unlike most numeral systems (e.g., those of\nEnglish, Chinese, etc.), forms referring to 1--99 are highly non-transparent\nand are cannot be constructed using straightforward rules. As an example,\nHindi/Urdu *iky\\=anve* `91' is not decomposable into the composite elements\n*ek* `one' and *nave* `ninety' in the way that its English counterpart is. This\npaper situates Indo-Aryan languages within the typology of cross-linguistic\nnumeral systems, and explores the linguistic and non-linguistic factors that\nmay be responsible for the persistence of complex systems in these languages.\nUsing cross-linguistic data from multiple databases, we develop and employ a\nnumber of cross-linguistically applicable metrics to quantifies the complexity\nof languages' numeral systems, and demonstrate that Indo-Aryan languages have\ndecisively more complex numeral systems than the world's languages as a whole,\nthough individual Indo-Aryan languages differ from each other in terms of the\ncomplexity of the patterns they display. We investigate the factors (e.g.,\nreligion, geographic isolation, etc.) that underlie complexity in numeral\nsystems, with a focus on South Asia, in an attempt to develop an account of why\ncomplex numeral systems developed and persisted in certain Indo-Aryan languages\nbut not elsewhere. Finally, we demonstrate that Indo-Aryan numeral systems\nadhere to certain general pressures toward efficient communication found\ncross-linguistically, despite their high complexity. We call for this somewhat\noverlooked dimension of complexity to be taken seriously when discussing\ngeneral variation in cross-linguistic numeral systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5370\u6b27\u8bed\u7cfb\u8bed\u8a00\uff08\u5982\u5370\u5730\u8bed\u3001\u53e4\u5409\u62c9\u7279\u8bed\u548c\u5b5f\u52a0\u62c9\u8bed\uff09\u4e2d\u7684\u6570\u5b57\u7cfb\u7edf\u590d\u6742\u6027\uff0c\u53d1\u73b0\u51761-99\u7684\u8868\u793a\u5f62\u5f0f\u9ad8\u5ea6\u4e0d\u900f\u660e\uff0c\u8fdc\u6bd4\u5176\u4ed6\u8bed\u8a00\u590d\u6742\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u80cc\u540e\u7684\u8bed\u8a00\u548c\u975e\u8bed\u8a00\u56e0\u7d20\u3002", "motivation": "\u4f20\u7edf\u6570\u5b57\u7cfb\u7edf\u7814\u7a76\u591a\u5173\u6ce8\u89c4\u5219\u7684\u3001\u900f\u660e\u7684\u6570\u5b57\u8868\u8fbe\uff0c\u800c\u5ffd\u7565\u4e86\u5370\u6b27\u8bed\u7cfb\u8bed\u8a00\u4e2d\u72ec\u7279\u4e14\u590d\u6742\u7684\u6570\u5b57\u8868\u793a\u5f62\u5f0f\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5e76\u63a2\u7d22\u5bfc\u81f4\u8fd9\u79cd\u590d\u6742\u6027\u6301\u4e45\u5b58\u5728\u7684\u56e0\u7d20\u3002", "method": "\u91c7\u7528\u8de8\u8bed\u8a00\u6570\u636e\uff0c\u8bbe\u8ba1\u5e76\u5e94\u7528\u591a\u4e2a\u5ea6\u91cf\u6807\u51c6\u6765\u91cf\u5316\u6570\u5b57\u7cfb\u7edf\u7684\u590d\u6742\u6027\uff0c\u5e76\u5206\u6790\u5b97\u6559\u3001\u5730\u7406\u9694\u79bb\u7b49\u56e0\u7d20\u5bf9\u7cfb\u7edf\u590d\u6742\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5370\u6b27\u8bed\u7cfb\u8bed\u8a00\u7684\u6570\u5b57\u7cfb\u7edf\u603b\u4f53\u4e0a\u6bd4\u5176\u4ed6\u8bed\u8a00\u66f4\u590d\u6742\uff0c\u4f46\u5185\u90e8\u5dee\u5f02\u663e\u8457\uff1b\u5c3d\u7ba1\u590d\u6742\uff0c\u5b83\u4eec\u4ecd\u7b26\u5408\u8de8\u8bed\u8a00\u7684\u4ea4\u9645\u6548\u7387\u539f\u5219\u3002", "conclusion": "\u5370\u6b27\u8bed\u7cfb\u7684\u590d\u6742\u6570\u5b57\u7cfb\u7edf\u662f\u591a\u79cd\u56e0\u7d20\u5171\u540c\u4f5c\u7528\u7684\u7ed3\u679c\uff0c\u8fd9\u4e00\u7ef4\u5ea6\u5e94\u5728\u8de8\u8bed\u8a00\u6570\u5b57\u7cfb\u7edf\u7814\u7a76\u4e2d\u5f97\u5230\u66f4\u591a\u5173\u6ce8\u3002", "keywords": "\u6570\u5b57\u7cfb\u7edf\u590d\u6742\u6027\u3001\u5370\u6b27\u8bed\u7cfb\u3001\u8de8\u8bed\u8a00\u6bd4\u8f83\u3001\u4ea4\u9645\u6548\u7387\u3001\u8bed\u8a00\u5b66"}}
{"id": "2505.22362", "pdf": "https://arxiv.org/pdf/2505.22362", "abs": "https://arxiv.org/abs/2505.22362", "authors": ["Aihu Zhang", "Jiaxing Xu", "Mengcheng Lan", "Shili Xiang", "Yiping Ke"], "title": "Directed Homophily-Aware Graph Neural Network", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) have achieved significant success in various\nlearning tasks on graph-structured data. Nevertheless, most GNNs struggle to\ngeneralize to heterophilic neighborhoods. Additionally, many GNNs ignore the\ndirectional nature of real-world graphs, resulting in suboptimal performance on\ndirected graphs with asymmetric structures. In this work, we propose Directed\nHomophily-aware Graph Neural Network (DHGNN), a novel framework that addresses\nthese limitations by incorporating homophily-aware and direction-sensitive\ncomponents. DHGNN employs a resettable gating mechanism to adaptively modulate\nmessage contributions based on homophily levels and informativeness, and a\nstructure-aware noise-tolerant fusion module to effectively integrate node\nrepresentations from the original and reverse directions. Extensive experiments\non both homophilic and heterophilic directed graph datasets demonstrate that\nDHGNN outperforms state-of-the-art methods in node classification and link\nprediction. In particular, DHGNN improves over the best baseline by up to\n15.07% in link prediction. Our analysis further shows that the gating mechanism\ncaptures directional homophily gaps and fluctuating homophily across layers,\nproviding deeper insights into message-passing behavior on complex graph\nstructures.", "AI": {"tldr": "DHGNN\u662f\u4e00\u79cd\u65b0\u578b\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u540c\u8d28\u6027\u611f\u77e5\u548c\u65b9\u5411\u654f\u611f\u7ec4\u4ef6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfGNN\u5728\u5f02\u8d28\u6027\u90bb\u57df\u548c\u65b9\u5411\u6027\u56fe\u7ed3\u6784\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edfGNN\u5728\u5f02\u8d28\u6027\u90bb\u57df\u548c\u65b9\u5411\u6027\u56fe\u7ed3\u6784\u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "DHGNN\u91c7\u7528\u53ef\u91cd\u7f6e\u95e8\u63a7\u673a\u5236\u6839\u636e\u540c\u8d28\u6027\u6c34\u5e73\u548c\u4fe1\u606f\u91cf\u81ea\u9002\u5e94\u8c03\u8282\u6d88\u606f\u8d21\u732e\uff0c\u5e76\u4f7f\u7528\u7ed3\u6784\u611f\u77e5\u7684\u566a\u58f0\u5bb9\u5fcd\u878d\u5408\u6a21\u5757\u6574\u5408\u6b63\u53cd\u5411\u8282\u70b9\u8868\u793a\u3002", "result": "\u5728\u8282\u70b9\u5206\u7c7b\u548c\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cDHGNN\u5728\u5f02\u8d28\u6027\u548c\u540c\u8d28\u6027\u56fe\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u94fe\u63a5\u9884\u6d4b\u6027\u80fd\u6700\u9ad8\u63d0\u534715.07%\u3002", "conclusion": "DHGNN\u4e0d\u4ec5\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5176\u95e8\u63a7\u673a\u5236\u8fd8\u4e3a\u590d\u6742\u56fe\u7ed3\u6784\u4e0a\u7684\u6d88\u606f\u4f20\u9012\u884c\u4e3a\u63d0\u4f9b\u4e86\u6df1\u5165\u89c1\u89e3\u3002", "keywords": "\u56fe\u795e\u7ecf\u7f51\u7edc,\u5f02\u8d28\u6027\u90bb\u57df,\u65b9\u5411\u6027\u56fe,\u540c\u8d28\u6027\u611f\u77e5,\u94fe\u63a5\u9884\u6d4b"}}
{"id": "2505.21724", "pdf": "https://arxiv.org/pdf/2505.21724", "abs": "https://arxiv.org/abs/2505.21724", "authors": ["Cheng Luo", "Jianghui Wang", "Bing Li", "Siyang Song", "Bernard Ghanem"], "title": "OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "23 pages, 9 figures", "summary": "In this paper, we introduce Online Multimodal Conversational Response\nGeneration (OMCRG), a novel task that aims to online generate synchronized\nverbal and non-verbal listener feedback, conditioned on the speaker's\nmultimodal input. OMCRG reflects natural dyadic interactions and poses new\nchallenges in achieving synchronization between the generated audio and facial\nresponses of the listener. To address these challenges, we innovatively\nintroduce text as an intermediate modality to bridge the audio and facial\nresponses. We hence propose OmniResponse, a Multimodal Large Language Model\n(MLLM) that autoregressively generates high-quality multi-modal listener\nresponses. OmniResponse leverages a pretrained LLM enhanced with two novel\ncomponents: Chrono-Text, which temporally anchors generated text tokens, and\nTempoVoice, a controllable online TTS module that produces speech synchronized\nwith facial reactions. To support further OMCRG research, we present\nResponseNet, a new dataset comprising 696 high-quality dyadic interactions\nfeaturing synchronized split-screen videos, multichannel audio, transcripts,\nand facial behavior annotations. Comprehensive evaluations conducted on\nResponseNet demonstrate that OmniResponse significantly outperforms baseline\nmodels in terms of semantic speech content, audio-visual synchronization, and\ngeneration quality.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5728\u7ebf\u591a\u6a21\u6001\u5bf9\u8bdd\u54cd\u5e94\u751f\u6210\uff08OMCRG\uff09\u4efb\u52a1\u53ca\u5176\u89e3\u51b3\u65b9\u6848OmniResponse\uff0c\u901a\u8fc7\u5f15\u5165\u6587\u672c\u4f5c\u4e3a\u4e2d\u95f4\u6a21\u6001\u6765\u540c\u6b65\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u6001\u542c\u4f17\u53cd\u9988\uff0c\u5e76\u901a\u8fc7\u65b0\u6570\u636e\u96c6ResponseNet\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "OMCRG\u4efb\u52a1\u65e8\u5728\u6a21\u62df\u81ea\u7136\u5bf9\u8bdd\u4e2d\u7684\u591a\u6a21\u6001\u4ea4\u4e92\uff0c\u4f46\u97f3\u9891\u548c\u9762\u90e8\u8868\u60c5\u7684\u540c\u6b65\u751f\u6210\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u521b\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u540c\u6b65\u6027\u548c\u751f\u6210\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86OmniResponse\u6a21\u578b\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u4e24\u4e2a\u65b0\u7ec4\u4ef6\uff1aChrono-Text\uff08\u65f6\u95f4\u951a\u5b9a\u6587\u672c\u6807\u8bb0\uff09\u548cTempoVoice\uff08\u53ef\u63a7\u5728\u7ebfTTS\u6a21\u5757\uff09\uff0c\u4ee5\u540c\u6b65\u751f\u6210\u97f3\u9891\u548c\u9762\u90e8\u53cd\u5e94\u3002", "result": "\u5728ResponseNet\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOmniResponse\u5728\u8bed\u4e49\u5185\u5bb9\u3001\u89c6\u542c\u540c\u6b65\u548c\u751f\u6210\u8d28\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "OmniResponse\u901a\u8fc7\u591a\u6a21\u6001\u540c\u6b65\u751f\u6210\u65b9\u6cd5\u4e3aOMCRG\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u3002", "keywords": "OMCRG, \u591a\u6a21\u6001, \u5bf9\u8bdd\u54cd\u5e94, OmniResponse, ResponseNet"}}
{"id": "2505.22370", "pdf": "https://arxiv.org/pdf/2505.22370", "abs": "https://arxiv.org/abs/2505.22370", "authors": ["Haomiao Qiu", "Miao Zhang", "Ziyue Qiao", "Weili Guan", "Min Zhang", "Liqiang Nie"], "title": "SplitLoRA: Balancing Stability and Plasticity in Continual Learning Through Gradient Space Splitting", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages, 4 figures", "summary": "Continual Learning requires a model to learn multiple tasks in sequence while\nmaintaining both stability:preserving knowledge from previously learned tasks,\nand plasticity:effectively learning new tasks. Gradient projection has emerged\nas an effective and popular paradigm in CL, where it partitions the gradient\nspace of previously learned tasks into two orthogonal subspaces: a primary\nsubspace and a minor subspace. New tasks are learned effectively within the\nminor subspace, thereby reducing interference with previously acquired\nknowledge. However, existing Gradient Projection methods struggle to achieve an\noptimal balance between plasticity and stability, as it is hard to\nappropriately partition the gradient space. In this work, we consider a\ncontinual learning paradigm based on Low-Rank Adaptation, which has gained\nconsiderable attention due to its efficiency and wide applicability, and\npropose a novel approach for continual learning, called SplitLoRA. We first\nprovide a theoretical analysis of how subspace partitioning affects model\nstability and plasticity. Informed by this analysis, we then introduce an\neffective method that derives the optimal partition of the gradient space for\npreviously learned tasks. This approach effectively balances stability and\nplasticity in continual learning. Experimental results on multiple datasets\ndemonstrate that the proposed method achieves state-of-the-art performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSplitLoRA\u7684\u65b0\u65b9\u6cd5\uff0c\u57fa\u4e8e\u4f4e\u79e9\u9002\u5e94\uff08Low-Rank Adaptation\uff09\u6765\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u6307\u5bfc\u68af\u5ea6\u7a7a\u95f4\u7684\u6700\u4f18\u5212\u5206\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u73b0\u6709\u68af\u5ea6\u6295\u5f71\u65b9\u6cd5\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u96be\u4ee5\u5e73\u8861\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\uff0c\u68af\u5ea6\u7a7a\u95f4\u7684\u5212\u5206\u4e0d\u591f\u4f18\u5316\u3002", "method": "\u57fa\u4e8e\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\uff0c\u63d0\u51faSplitLoRA\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u6307\u5bfc\u68af\u5ea6\u7a7a\u95f4\u7684\u6700\u4f18\u5212\u5206\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSplitLoRA\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "SplitLoRA\u901a\u8fc7\u4f18\u5316\u68af\u5ea6\u7a7a\u95f4\u5212\u5206\uff0c\u6709\u6548\u5e73\u8861\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "keywords": "Continual Learning, Gradient Projection, Low-Rank Adaptation, SplitLoRA, Stability, Plasticity"}}
{"id": "2505.22381", "pdf": "https://arxiv.org/pdf/2505.22381", "abs": "https://arxiv.org/abs/2505.22381", "authors": ["Lukas Kirchdorfer", "Konrad \u00d6zdemir", "Stjepan Kusenic", "Han van der Aa", "Heiner Stuckenschmidt"], "title": "A Divide-and-Conquer Approach for Modeling Arrival Times in Business Process Simulation", "categories": ["cs.LG"], "comment": null, "summary": "Business Process Simulation (BPS) is a critical tool for analyzing and\nimproving organizational processes by estimating the impact of process changes.\nA key component of BPS is the case-arrival model, which determines the pattern\nof new case entries into a process. Although accurate case-arrival modeling is\nessential for reliable simulations, as it influences waiting and overall cycle\ntimes, existing approaches often rely on oversimplified static distributions of\ninter-arrival times. These approaches fail to capture the dynamic and temporal\ncomplexities inherent in organizational environments, leading to less accurate\nand reliable outcomes. To address this limitation, we propose Auto Time Kernel\nDensity Estimation (AT-KDE), a divide-and-conquer approach that models arrival\ntimes of processes by incorporating global dynamics, day-of-week variations,\nand intraday distributional changes, ensuring both precision and scalability.\nExperiments conducted across 20 diverse processes demonstrate that AT-KDE is\nfar more accurate and robust than existing approaches while maintaining\nsensible execution time efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAT-KDE\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u5730\u6a21\u62df\u4e1a\u52a1\u6d41\u7a0b\u4e2d\u7684\u6848\u4f8b\u5230\u8fbe\u6a21\u5f0f\uff0c\u514b\u670d\u4e86\u73b0\u6709\u9759\u6001\u5206\u5e03\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u4e1a\u52a1\u6d41\u7a0b\u4eff\u771f\u4e2d\uff0c\u6848\u4f8b\u5230\u8fbe\u6a21\u578b\u8fc7\u4e8e\u7b80\u5316\uff0c\u65e0\u6cd5\u6355\u6349\u52a8\u6001\u548c\u65f6\u95f4\u590d\u6742\u6027\uff0c\u5f71\u54cd\u4eff\u771f\u7ed3\u679c\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528Auto Time Kernel Density Estimation (AT-KDE) \u65b9\u6cd5\uff0c\u7ed3\u5408\u5168\u5c40\u52a8\u6001\u3001\u5468\u5185\u53d8\u5316\u548c\u65e5\u5185\u5206\u5e03\u53d8\u5316\uff0c\u5efa\u6a21\u6848\u4f8b\u5230\u8fbe\u65f6\u95f4\u3002", "result": "\u572820\u4e2a\u4e0d\u540c\u6d41\u7a0b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAT-KDE\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u51c6\u786e\u3001\u9c81\u68d2\uff0c\u540c\u65f6\u4fdd\u6301\u5408\u7406\u7684\u6267\u884c\u6548\u7387\u3002", "conclusion": "AT-KDE\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4e1a\u52a1\u6d41\u7a0b\u4eff\u771f\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u5904\u7406\u52a8\u6001\u548c\u590d\u6742\u65f6\u95f4\u6a21\u5f0f\u65f6\u8868\u73b0\u4f18\u5f02\u3002", "keywords": "\u4e1a\u52a1\u6d41\u7a0b\u4eff\u771f, \u6848\u4f8b\u5230\u8fbe\u6a21\u578b, \u6838\u5bc6\u5ea6\u4f30\u8ba1, \u52a8\u6001\u5efa\u6a21"}}
{"id": "2505.21544", "pdf": "https://arxiv.org/pdf/2505.21544", "abs": "https://arxiv.org/abs/2505.21544", "authors": ["Semanto Mondal"], "title": "Vision Meets Language: A RAG-Augmented YOLOv8 Framework for Coffee Disease Diagnosis and Farmer Assistance", "categories": ["cs.CV", "cs.CL"], "comment": "There are 14 pages, 8 figures", "summary": "As a social being, we have an intimate bond with the environment. A plethora\nof things in human life, such as lifestyle, health, and food are dependent on\nthe environment and agriculture. It comes under our responsibility to support\nthe environment as well as agriculture. However, traditional farming practices\noften result in inefficient resource use and environmental challenges. To\naddress these issues, precision agriculture has emerged as a promising approach\nthat leverages advanced technologies to optimise agricultural processes. In\nthis work, a hybrid approach is proposed that combines the three different\npotential fields of model AI: object detection, large language model (LLM), and\nRetrieval-Augmented Generation (RAG). In this novel framework, we have tried to\ncombine the vision and language models to work together to identify potential\ndiseases in the tree leaf. This study introduces a novel AI-based precision\nagriculture system that uses Retrieval Augmented Generation (RAG) to provide\ncontext-aware diagnoses and natural language processing (NLP) and YOLOv8 for\ncrop disease detection. The system aims to tackle major issues with large\nlanguage models (LLMs), especially hallucinations and allows for adaptive\ntreatment plans and real-time disease detection. The system provides an\neasy-to-use interface to the farmers, which they can use to detect the\ndifferent diseases related to coffee leaves by just submitting the image of the\naffected leaf the model will detect the diseases as well as suggest potential\nremediation methodologies which aim to lower the use of pesticides, preserving\nlivelihoods, and encouraging environmentally friendly methods. With an emphasis\non scalability, dependability, and user-friendliness, the project intends to\nimprove RAG-integrated object detection systems for wider agricultural\napplications in the future.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5bf9\u8c61\u68c0\u6d4b\u3001\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7684\u6df7\u5408AI\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u68c0\u6d4b\u5496\u5561\u53f6\u75be\u75c5\u5e76\u63d0\u4f9b\u73af\u4fdd\u7684\u5904\u7406\u5efa\u8bae\u3002", "motivation": "\u4f20\u7edf\u519c\u4e1a\u5b9e\u8df5\u6548\u7387\u4f4e\u4e14\u5bf9\u73af\u5883\u5f71\u54cd\u8f83\u5927\uff0c\u7cbe\u51c6\u519c\u4e1a\u6280\u672f\u53ef\u901a\u8fc7AI\u4f18\u5316\u8d44\u6e90\u5229\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3LLM\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u5b9e\u65f6\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4f5c\u7269\u75be\u75c5\u8bca\u65ad\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6df7\u5408AI\u6846\u67b6\uff0c\u7ed3\u5408YOLOv8\u8fdb\u884c\u56fe\u50cf\u68c0\u6d4b\u3001NLP\u5904\u7406\u8bed\u8a00\u8f93\u5165\uff0c\u5e76\u5229\u7528RAG\u751f\u6210\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u8bca\u65ad\u4e0e\u5efa\u8bae\uff0c\u5b9e\u73b0\u4f4e\u519c\u836f\u4f7f\u7528\u7684\u73af\u4fdd\u79cd\u690d\u65b9\u6848\u3002", "result": "\u7cfb\u7edf\u80fd\u51c6\u786e\u68c0\u6d4b\u5496\u5561\u53f6\u75be\u75c5\uff0c\u63d0\u4f9b\u4e2a\u6027\u5316\u5904\u7406\u65b9\u6848\uff0c\u5e76\u964d\u4f4e\u519c\u836f\u4f9d\u8d56\uff0c\u517c\u987e\u73af\u5883\u53cb\u597d\u6027\u4e0e\u519c\u6c11\u6613\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7cbe\u51c6\u519c\u4e1a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u66f4\u5e7f\u6cdb\u7684\u519c\u4e1a\u5e94\u7528\u3002", "keywords": "\u7cbe\u51c6\u519c\u4e1a, \u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG), \u5bf9\u8c61\u68c0\u6d4b, \u5927\u8bed\u8a00\u6a21\u578b, \u5496\u5561\u53f6\u75be\u75c5"}}
{"id": "2505.22389", "pdf": "https://arxiv.org/pdf/2505.22389", "abs": "https://arxiv.org/abs/2505.22389", "authors": ["Haomiao Qiu", "Miao Zhang", "Ziyue Qiao", "Liqiang Nie"], "title": "Train with Perturbation, Infer after Merging: A Two-Stage Framework for Continual Learning", "categories": ["cs.LG", "cs.AI"], "comment": "17 pages, 3 figures", "summary": "Continual Learning (CL) aims to enable models to continuously acquire new\nknowledge from a sequence of tasks with avoiding the forgetting of learned\ninformation. However, existing CL methods only rely on the parameters of the\nmost recent task for inference, which makes them susceptible to catastrophic\nforgetting. Inspired by the recent success of model merging techniques, we\npropose \\textbf{Perturb-and-Merge (P\\&M)}, a novel continual learning framework\nthat integrates model merging into the CL paradigm to mitigate forgetting.\nSpecifically, after training on each task, P\\&M constructs a new model by\nforming a convex combination of the previous model and the newly trained\ntask-specific model. Through theoretical analysis, we minimize the total loss\nincrease across all tasks and derive an analytical solution for the optimal\nmerging coefficient. To further improve the performance of the merged model, we\nobserve that the degradation introduced during merging can be alleviated by a\nregularization term composed of the task vector and the Hessian matrix of the\nloss function. Interestingly, we show that this term can be efficiently\napproximated using second-order symmetric finite differences, and a stochastic\nperturbation strategy along the task vector direction is accordingly devised\nwhich incurs no additional forward or backward passes while providing an\neffective approximation of the regularization term. Finally, we combine P\\&M\nwith LoRA, a parameter-efficient fine-tuning method, to reduce memory overhead.\nOur proposed approach achieves state-of-the-art performance on several\ncontinual learning benchmark datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86Perturb-and-Merge (P&M)\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u5408\u5e76\u6280\u672f\u7f13\u89e3\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u7ed3\u5408\u4e8c\u9636\u6b63\u5219\u5316\u4e0eLoRA\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u6700\u8fd1\u4efb\u52a1\u7684\u53c2\u6570\u8fdb\u884c\u63a8\u65ad\uff0c\u6613\u53d7\u707e\u96be\u6027\u9057\u5fd8\u5f71\u54cd\u3002\u53d7\u6a21\u578b\u5408\u5e76\u6280\u672f\u542f\u53d1\uff0c\u63d0\u51fa\u901a\u8fc7\u5408\u5e76\u65b0\u65e7\u6a21\u578b\u6765\u7f13\u89e3\u9057\u5fd8\u3002", "method": "P&M\u6846\u67b6\u5728\u6bcf\u9879\u4efb\u52a1\u8bad\u7ec3\u540e\uff0c\u901a\u8fc7\u51f8\u7ec4\u5408\u5408\u5e76\u65b0\u65e7\u6a21\u578b\uff0c\u5229\u7528\u4e8c\u9636\u6b63\u5219\u5316\u4f18\u5316\u5408\u5e76\u6548\u679c\uff0c\u5e76\u7ed3\u5408LoRA\u964d\u4f4e\u5185\u5b58\u5f00\u9500\u3002", "result": "\u5728\u591a\u4e2a\u6301\u7eed\u5b66\u4e60\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "P&M\u901a\u8fc7\u6a21\u578b\u5408\u5e76\u4e0e\u4e8c\u9636\u6b63\u5219\u5316\u6709\u6548\u7f13\u89e3\u9057\u5fd8\uff0c\u7ed3\u5408LoRA\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "keywords": "\u6301\u7eed\u5b66\u4e60, \u707e\u96be\u6027\u9057\u5fd8, \u6a21\u578b\u5408\u5e76, \u4e8c\u9636\u6b63\u5219\u5316, LoRA"}}
{"id": "2505.22391", "pdf": "https://arxiv.org/pdf/2505.22391", "abs": "https://arxiv.org/abs/2505.22391", "authors": ["Yi Zhang", "Difan Zou"], "title": "Physics-Informed Distillation of Diffusion Models for PDE-Constrained Generation", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.NA", "math.NA"], "comment": "23 pages, 5 figures, 4 tables", "summary": "Modeling physical systems in a generative manner offers several advantages,\nincluding the ability to handle partial observations, generate diverse\nsolutions, and address both forward and inverse problems. Recently, diffusion\nmodels have gained increasing attention in the modeling of physical systems,\nparticularly those governed by partial differential equations (PDEs). However,\ndiffusion models only access noisy data $\\boldsymbol{x}_t$ at intermediate\nsteps, making it infeasible to directly enforce constraints on the clean sample\n$\\boldsymbol{x}_0$ at each noisy level. As a workaround, constraints are\ntypically applied to the expectation of clean samples\n$\\mathbb{E}[\\boldsymbol{x}_0|\\boldsymbol{x}_t]$, which is estimated using the\nlearned score network. However, imposing PDE constraints on the expectation\ndoes not strictly represent the one on the true clean data, known as Jensen's\nGap. This gap creates a trade-off: enforcing PDE constraints may come at the\ncost of reduced accuracy in generative modeling. To address this, we propose a\nsimple yet effective post-hoc distillation approach, where PDE constraints are\nnot injected directly into the diffusion process, but instead enforced during a\npost-hoc distillation stage. We term our method as Physics-Informed\nDistillation of Diffusion Models (PIDDM). This distillation not only\nfacilitates single-step generation with improved PDE satisfaction, but also\nsupport both forward and inverse problem solving and reconstruction from\nrandomly partial observation. Extensive experiments across various PDE\nbenchmarks demonstrate that PIDDM significantly improves PDE satisfaction over\nseveral recent and competitive baselines, such as PIDM, DiffusionPDE, and\nECI-sampling, with less computation overhead. Our approach can shed light on\nmore efficient and effective strategies for incorporating physical constraints\ninto diffusion models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPIDDM\u7684\u540e\u5904\u7406\u84b8\u998f\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u5584\u6269\u6563\u6a21\u578b\u5728\u7269\u7406\u7cfb\u7edf\u4e2d\u7684\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\uff0c\u540c\u65f6\u652f\u6301\u6b63\u5411\u548c\u9006\u5411\u95ee\u9898\u6c42\u89e3\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u7269\u7406\u7cfb\u7edf\u5efa\u6a21\u4e2d\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u76f4\u63a5\u5728\u566a\u58f0\u6570\u636e\u4e0a\u65bd\u52a0\u7ea6\u675f\u4f1a\u5bfc\u81f4Jensen's Gap\u95ee\u9898\uff0c\u5f71\u54cd\u51c6\u786e\u6027\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6743\u8861\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51faPIDDM\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u84b8\u998f\u540e\u5904\u7406\u9636\u6bb5\u800c\u975e\u6269\u6563\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u5e94\u7528PDE\u7ea6\u675f\uff0c\u4ee5\u63d0\u9ad8\u7ea6\u675f\u6ee1\u8db3\u548c\u751f\u6210\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPIDDM\u5728\u591a\u4e2aPDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff08\u5982PIDM\u3001DiffusionPDE\u7b49\uff09\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u66f4\u4f4e\u3002", "conclusion": "PIDDM\u4e3a\u6269\u6563\u6a21\u578b\u5f15\u5165\u7269\u7406\u7ea6\u675f\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u6709\u6548\u7684\u7b56\u7565\uff0c\u540c\u65f6\u652f\u6301\u591a\u79cd\u95ee\u9898\u6c42\u89e3\u3002", "keywords": "diffusion models, PDE, physical constraints, distillation, PIDDM"}}
{"id": "2505.21746", "pdf": "https://arxiv.org/pdf/2505.21746", "abs": "https://arxiv.org/abs/2505.21746", "authors": ["Arif Masrur", "Peder A. Olsen", "Paul R. Adler", "Carlan Jackson", "Matthew W. Myers", "Nathan Sedghi", "Ray R. Weil"], "title": "Learning to See More: UAS-Guided Super-Resolution of Satellite Imagery for Precision Agriculture", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Unmanned Aircraft Systems (UAS) and satellites are key data sources for\nprecision agriculture, yet each presents trade-offs. Satellite data offer broad\nspatial, temporal, and spectral coverage but lack the resolution needed for\nmany precision farming applications, while UAS provide high spatial detail but\nare limited by coverage and cost, especially for hyperspectral data. This study\npresents a novel framework that fuses satellite and UAS imagery using\nsuper-resolution methods. By integrating data across spatial, spectral, and\ntemporal domains, we leverage the strengths of both platforms cost-effectively.\nWe use estimation of cover crop biomass and nitrogen (N) as a case study to\nevaluate our approach. By spectrally extending UAS RGB data to the vegetation\nred edge and near-infrared regions, we generate high-resolution Sentinel-2\nimagery and improve biomass and N estimation accuracy by 18% and 31%,\nrespectively. Our results show that UAS data need only be collected from a\nsubset of fields and time points. Farmers can then 1) enhance the spectral\ndetail of UAS RGB imagery; 2) increase the spatial resolution by using\nsatellite data; and 3) extend these enhancements spatially and across the\ngrowing season at the frequency of the satellite flights. Our SRCNN-based\nspectral extension model shows considerable promise for model transferability\nover other cropping systems in the Upper and Lower Chesapeake Bay regions.\nAdditionally, it remains effective even when cloud-free satellite data are\nunavailable, relying solely on the UAS RGB input. The spatial extension model\nproduces better biomass and N predictions than models built on raw UAS RGB\nimages. Once trained with targeted UAS RGB data, the spatial extension model\nallows farmers to stop repeated UAS flights. While we introduce\nsuper-resolution advances, the core contribution is a lightweight and scalable\nsystem for affordable on-farm use.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u536b\u661f\u548c\u65e0\u4eba\u673a(UAS)\u5f71\u50cf\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u4ee5\u4f4e\u6210\u672c\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u519c\u4f5c\u7269\u751f\u7269\u91cf\u548c\u6c2e\u542b\u91cf\u7684\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u536b\u661f\u548c\u65e0\u4eba\u673a\u5728\u7cbe\u51c6\u519c\u4e1a\u4e2d\u5404\u5177\u4f18\u7f3a\u70b9\uff0c\u536b\u661f\u8986\u76d6\u8303\u56f4\u5e7f\u4f46\u5206\u8fa8\u7387\u4f4e\uff0c\u65e0\u4eba\u673a\u5206\u8fa8\u7387\u9ad8\u4f46\u6210\u672c\u9ad8\u4e14\u8986\u76d6\u6709\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u878d\u5408\u4e24\u8005\u6570\u636e\uff0c\u5f25\u8865\u5404\u81ea\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u878d\u5408\u536b\u661f\u548c\u65e0\u4eba\u673a\u5f71\u50cf\uff0c\u5c06\u65e0\u4eba\u673aRGB\u6570\u636e\u5149\u8c31\u6269\u5c55\u5230\u690d\u88ab\u7ea2\u8fb9\u548c\u8fd1\u7ea2\u5916\u533a\u57df\uff0c\u751f\u6210\u9ad8\u5206\u8fa8\u7387Sentinel-2\u5f71\u50cf\u3002", "result": "\u901a\u8fc7\u8be5\u65b9\u6cd5\uff0c\u751f\u7269\u91cf\u548c\u6c2e\u542b\u91cf\u7684\u4f30\u8ba1\u7cbe\u5ea6\u5206\u522b\u63d0\u9ad8\u4e8618%\u548c31%\uff0c\u4e14\u65e0\u9700\u9891\u7e41\u65e0\u4eba\u673a\u98de\u884c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8f7b\u91cf\u4e14\u53ef\u6269\u5c55\uff0c\u9002\u7528\u4e8e\u519c\u573a\u4f4e\u6210\u672c\u5e94\u7528\uff0c\u5e76\u5728\u4e0d\u540c\u4f5c\u7269\u7cfb\u7edf\u4e2d\u5177\u6709\u53ef\u8fc1\u79fb\u6027\u3002", "keywords": "\u7cbe\u51c6\u519c\u4e1a, \u65e0\u4eba\u673a, \u536b\u661f\u5f71\u50cf, \u8d85\u5206\u8fa8\u7387, \u751f\u7269\u91cf\u4f30\u8ba1"}}
{"id": "2505.21549", "pdf": "https://arxiv.org/pdf/2505.21549", "abs": "https://arxiv.org/abs/2505.21549", "authors": ["Daniel Csizmadia", "Andrei Codreanu", "Victor Sim", "Vighnesh Prabeau", "Michael Lu", "Kevin Zhu", "Sean O'Brien", "Vasu Sharma"], "title": "Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that\nenhances multimodal image-text retrieval while preserving the original model's\nstrong zero-shot classification capabilities. CLIP models are typically\nconstrained by fixed image resolutions and limited context, which can hinder\ntheir effectiveness in retrieval tasks that require fine-grained cross-modal\nunderstanding. DCLIP addresses these challenges through a meta teacher-student\ndistillation framework, where a cross-modal transformer teacher is fine-tuned\nto produce enriched embeddings via bidirectional cross-attention between\nYOLO-extracted image regions and corresponding textual spans. These\nsemantically and spatially aligned global representations guide the training of\na lightweight student model using a hybrid loss that combines contrastive\nlearning and cosine similarity objectives. Despite being trained on only\n~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a\nfraction of CLIP's original dataset-DCLIP significantly improves image-text\nretrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP's\nzero-shot classification performance. These results demonstrate that DCLIP\neffectively mitigates the trade-off between task specialization and\ngeneralization, offering a resource-efficient, domain-adaptive, and\ndetail-sensitive solution for advanced vision-language tasks. Code available at\nhttps://anonymous.4open.science/r/DCLIP-B772/README.md.", "AI": {"tldr": "DCLIP\u901a\u8fc7\u5143\u6559\u5e08-\u5b66\u751f\u84b8\u998f\u6846\u67b6\u589e\u5f3aCLIP\u6a21\u578b\u7684\u591a\u6a21\u6001\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u96f6\u6837\u672c\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3CLIP\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u7406\u89e3\u4efb\u52a1\u4e2d\u56e0\u56fa\u5b9a\u56fe\u50cf\u5206\u8fa8\u7387\u548c\u6709\u9650\u4e0a\u4e0b\u6587\u800c\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cc\u5411\u8de8\u6ce8\u610f\u529b\u673a\u5236\u7684\u8de8\u6a21\u6001Transformer\u6559\u5e08\u6a21\u578b\u751f\u6210\u4e30\u5bcc\u5d4c\u5165\uff0c\u6307\u5bfc\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\u7684\u8bad\u7ec3\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u4f59\u5f26\u76f8\u4f3c\u5ea6\u76ee\u6807\u3002", "result": "DCLIP\u5728\u5c11\u91cf\u6837\u672c\u8bad\u7ec3\u4e0b\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6307\u6807\uff08Recall@K, MAP\uff09\uff0c\u5e76\u4fdd\u755994%\u7684\u96f6\u6837\u672c\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "DCLIP\u6709\u6548\u5e73\u8861\u4efb\u52a1\u4e13\u4e1a\u5316\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u63d0\u4f9b\u9ad8\u6548\u3001\u9002\u5e94\u6027\u5f3a\u4e14\u7ec6\u8282\u654f\u611f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "DCLIP, CLIP, \u591a\u6a21\u6001\u68c0\u7d22, \u84b8\u998f\u6846\u67b6, \u96f6\u6837\u672c\u5206\u7c7b"}}
{"id": "2505.22411", "pdf": "https://arxiv.org/pdf/2505.22411", "abs": "https://arxiv.org/abs/2505.22411", "authors": ["Yao Huang", "Huanran Chen", "Shouwei Ruan", "Yichi Zhang", "Xingxing Wei", "Yinpeng Dong"], "title": "Mitigating Overthinking in Large Reasoning Models via Manifold Steering", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "17 pages, 7 figures", "summary": "Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable\ncapabilities in solving complex tasks such as mathematics and coding. However,\nthese models frequently exhibit a phenomenon known as overthinking during\ninference, characterized by excessive validation loops and redundant\ndeliberation, leading to substantial computational overheads. In this paper, we\naim to mitigate overthinking by investigating the underlying mechanisms from\nthe perspective of mechanistic interpretability. We first showcase that the\ntendency of overthinking can be effectively captured by a single direction in\nthe model's activation space and the issue can be eased by intervening the\nactivations along this direction. However, this efficacy soon reaches a plateau\nand even deteriorates as the intervention strength increases. We therefore\nsystematically explore the activation space and find that the overthinking\nphenomenon is actually tied to a low-dimensional manifold, which indicates that\nthe limited effect stems from the noises introduced by the high-dimensional\nsteering direction. Based on this insight, we propose Manifold Steering, a\nnovel approach that elegantly projects the steering direction onto the\nlow-dimensional activation manifold given the theoretical approximation of the\ninterference noise. Extensive experiments on DeepSeek-R1 distilled models\nvalidate that our method reduces output tokens by up to 71% while maintaining\nand even improving the accuracy on several mathematical benchmarks. Our method\nalso exhibits robust cross-domain transferability, delivering consistent token\nreduction performance in code generation and knowledge-based QA tasks. Code is\navailable at: https://github.com/Aries-iai/Manifold_Steering.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aManifold Steering\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u7ef4\u6d41\u5f62\u6295\u5f71\u51cf\u5c11\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u4e2d\u7684\u8fc7\u5ea6\u601d\u8003\u73b0\u8c61\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u4e86\u4efb\u52a1\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u65f6\u5bb9\u6613\u51fa\u73b0\u8fc7\u5ea6\u601d\u8003\u73b0\u8c61\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u673a\u7406\u53ef\u89e3\u91ca\u6027\u63ed\u793a\u5176\u672c\u8d28\u5e76\u4f18\u5316\u3002", "method": "\u9996\u5148\u8bc6\u522b\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u7684\u5355\u4e00\u65b9\u5411\u53ef\u6355\u6349\u8fc7\u5ea6\u601d\u8003\u503e\u5411\uff0c\u4f46\u5e72\u9884\u6548\u679c\u6709\u9650\uff1b\u8fdb\u800c\u53d1\u73b0\u5176\u4e0e\u4f4e\u7ef4\u6d41\u5f62\u76f8\u5173\uff0c\u63d0\u51fa\u901a\u8fc7\u6d41\u5f62\u6295\u5f71\u964d\u4f4e\u566a\u58f0\u5e72\u6270\u7684Manifold Steering\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u5b66\u4efb\u52a1\u4e2d\u51cf\u5c1171%\u7684\u8f93\u51fatoken\u4e14\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u5728\u4ee3\u7801\u751f\u6210\u548c\u77e5\u8bc6\u95ee\u7b54\u4e2d\u540c\u6837\u6709\u6548\u3002", "conclusion": "\u4f4e\u7ef4\u6d41\u5f62\u6295\u5f71\u80fd\u9ad8\u6548\u7f13\u89e3\u8fc7\u5ea6\u601d\u8003\uff0c\u517c\u5177\u8de8\u4efb\u52a1\u6cdb\u5316\u6027\u3002", "keywords": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u3001\u8fc7\u5ea6\u601d\u8003\u3001\u4f4e\u7ef4\u6d41\u5f62\u3001\u8ba1\u7b97\u6548\u7387\u3001\u8de8\u57df\u8fc1\u79fb"}}
{"id": "2505.21755", "pdf": "https://arxiv.org/pdf/2505.21755", "abs": "https://arxiv.org/abs/2505.21755", "authors": ["Chengyue Huang", "Brisa Maneechotesuwan", "Shivang Chopra", "Zsolt Kira"], "title": "FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to CVPR 2025", "summary": "Visual question answering (VQA) systems face significant challenges when\nadapting to real-world data shifts, especially in multi-modal contexts. While\nrobust fine-tuning strategies are essential for maintaining performance across\nin-distribution (ID) and out-of-distribution (OOD) scenarios, current\nevaluation settings are primarily unimodal or particular to some types of OOD,\noffering limited insight into the complexities of multi-modal contexts. In this\nwork, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across\nMulti-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We\nutilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA\nand others, and categorize them into ID, near and far OOD datasets covering\nuni-modal, multi-modal and adversarial distribution shifts. We first conduct a\ncomprehensive comparison of existing robust fine-tuning methods. We then\nquantify the distribution shifts by calculating the Mahalanobis distance using\nuni-modal and multi-modal embeddings extracted from various models. Further, we\nperform an extensive analysis to explore the interactions between uni- and\nmulti-modal shifts as well as modality importance for ID and OOD samples. These\nanalyses offer valuable guidance on developing more robust fine-tuning methods\nto handle multi-modal distribution shifts. The code is available at\nhttps://github.com/chengyuehuang511/FRAMES-VQA .", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6FRAMES-VQA\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u5fae\u8c03\u65b9\u6cd5\u3002\u901a\u8fc7\u6574\u5408\u591a\u4e2a\u73b0\u6709\u6570\u636e\u96c6\u5e76\u5206\u7c7b\u4e3a\u4e0d\u540c\u5206\u5e03\u504f\u79fb\uff0c\u8bba\u6587\u6bd4\u8f83\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u6a21\u6001\u91cd\u8981\u6027\u3002", "motivation": "\u5f53\u524d\u7684VQA\u7cfb\u7edf\u5728\u9002\u5e94\u591a\u6a21\u6001\u6570\u636e\u504f\u79fb\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u771f\u5b9e\u4e16\u754c\u7684\u590d\u6742\u573a\u666f\u3002\u7f3a\u4e4f\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\u6765\u7814\u7a76\u591a\u6a21\u6001\u504f\u79fb\u5bf9\u9c81\u68d2\u5fae\u8c03\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51faFRAMES-VQA\u57fa\u51c6\uff0c\u6574\u540810\u4e2a\u73b0\u6709\u6570\u636e\u96c6\uff0c\u5206\u4e3aID\u3001\u8fd1OOD\u548c\u8fdcOOD\u7c7b\u522b\u3002\u91c7\u7528\u9a6c\u6c0f\u8ddd\u79bb\u91cf\u5316\u5206\u5e03\u504f\u79fb\uff0c\u5206\u6790\u5355\u6a21\u6001\u4e0e\u591a\u6a21\u6001\u504f\u79fb\u7684\u4ea4\u4e92\u53ca\u6a21\u6001\u91cd\u8981\u6027\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u73b0\u6709\u65b9\u6cd5\uff0c\u91cf\u5316\u4e86\u4e0d\u540c\u6a21\u6001\u504f\u79fb\u7684\u5f71\u54cd\uff0c\u63d0\u4f9b\u4e86\u5173\u4e8e\u6a21\u6001\u91cd\u8981\u6027\u7684\u5206\u6790\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u9c81\u68d2\u7684VQA\u5fae\u8c03\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "conclusion": "FRAMES-VQA\u4e3a\u7814\u7a76\u591a\u6a21\u6001\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u5fae\u8c03\u63d0\u4f9b\u4e86\u5168\u9762\u57fa\u51c6\uff0c\u5b9e\u9a8c\u5206\u6790\u63ed\u793a\u4e86\u6a21\u6001\u91cd\u8981\u6027\u4e0e\u504f\u79fb\u7c7b\u578b\u7684\u5173\u7cfb\uff0c\u4fc3\u8fdb\u672a\u6765\u66f4\u9c81\u68d2\u7684VQA\u7cfb\u7edf\u5f00\u53d1\u3002", "keywords": "\u89c6\u89c9\u95ee\u7b54, \u9c81\u68d2\u5fae\u8c03, \u591a\u6a21\u6001, \u5206\u5e03\u504f\u79fb, \u57fa\u51c6\u8bc4\u4f30"}}
{"id": "2505.22422", "pdf": "https://arxiv.org/pdf/2505.22422", "abs": "https://arxiv.org/abs/2505.22422", "authors": ["V\u00e1clav Vor\u00e1\u010dek", "Francesco Orabona"], "title": "STaR-Bets: Sequential Target-Recalculating Bets for Tighter Confidence Intervals", "categories": ["cs.LG"], "comment": "comments are welcome", "summary": "The construction of confidence intervals for the mean of a bounded random\nvariable is a classical problem in statistics with numerous applications in\nmachine learning and virtually all scientific fields. In particular, obtaining\nthe tightest possible confidence intervals is vital every time the sampling of\nthe random variables is expensive. The current state-of-the-art method to\nconstruct confidence intervals is by using betting algorithms. This is a very\nsuccessful approach for deriving optimal confidence sequences, even matching\nthe rate of law of iterated logarithms. However, in the fixed horizon setting,\nthese approaches are either sub-optimal or based on heuristic solutions with\nstrong empirical performance but without a finite-time guarantee. Hence, no\nbetting-based algorithm guaranteeing the optimal\n$\\mathcal{O}(\\sqrt{\\frac{\\sigma^2\\log\\frac1\\delta}{n}})$ width of the\nconfidence intervals are known. This work bridges this gap. We propose a\nbetting-based algorithm to compute confidence intervals that empirically\noutperforms the competitors. Our betting strategy uses the optimal strategy in\nevery step (in a certain sense), whereas the standard betting methods choose a\nconstant strategy in advance. Leveraging this fact results in strict\nimprovements even for classical concentration inequalities, such as the ones of\nHoeffding or Bernstein. Moreover, we also prove that the width of our\nconfidence intervals is optimal up to an $1+o(1)$ factor diminishing with $n$.\nThe code is available\non~https://github.com/vvoracek/STaR-bets-confidence-interval.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6295\u6ce8\u7b97\u6cd5\u7684\u7f6e\u4fe1\u533a\u95f4\u6784\u9020\u65b9\u6cd5\uff0c\u5728\u56fa\u5b9a\u65f6\u95f4\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u6700\u4f18\u5bbd\u5ea6\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u6784\u5efa\u8fb9\u754c\u968f\u673a\u53d8\u91cf\u5747\u503c\u7684\u7f6e\u4fe1\u533a\u95f4\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u56fa\u5b9a\u65f6\u95f4\u8bbe\u7f6e\u4e0b\u8981\u4e48\u6b21\u4f18\uff0c\u8981\u4e48\u7f3a\u4e4f\u6709\u9650\u65f6\u95f4\u4fdd\u8bc1\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6295\u6ce8\u7b97\u6cd5\u7684\u7b56\u7565\uff0c\u6bcf\u4e00\u6b65\u9009\u62e9\u6700\u4f18\u7b56\u7565\uff08\u800c\u975e\u56fa\u5b9a\u7b56\u7565\uff09\uff0c\u5e76\u7ed3\u5408\u7ecf\u5178\u4e0d\u7b49\u5f0f\uff08\u5982Hoeffding\u6216Bernstein\uff09\u4e25\u683c\u63d0\u5347\u6027\u80fd\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5b9e\u8bc1\u4e2d\u4f18\u4e8e\u7ade\u4e89\u5bf9\u624b\uff0c\u4e14\u7f6e\u4fe1\u533a\u95f4\u5bbd\u5ea6\u6700\u4f18\uff08\u6e10\u8fd1\u4e8e$1+o(1)$\uff09\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u56fa\u5b9a\u65f6\u95f4\u8bbe\u7f6e\u4e0b\u586b\u8865\u4e86\u6700\u4f18\u7f6e\u4fe1\u533a\u95f4\u7684\u7a7a\u767d\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u5747\u8868\u73b0\u51fa\u8272\u3002", "keywords": "\u7f6e\u4fe1\u533a\u95f4, \u6295\u6ce8\u7b97\u6cd5, \u6700\u4f18\u7b56\u7565, \u6709\u9650\u65f6\u95f4\u4fdd\u8bc1"}}
{"id": "2505.21771", "pdf": "https://arxiv.org/pdf/2505.21771", "abs": "https://arxiv.org/abs/2505.21771", "authors": ["Prasham Yatinkumar Titiya", "Jainil Trivedi", "Chitta Baral", "Vivek Gupta"], "title": "MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal tables those that integrate semi structured data with visual\nelements such as charts and maps are ubiquitous across real world domains, yet\nthey pose a formidable challenge to current vision language models (VLMs).\nWhile Large Language models (LLMs) and VLMs have demonstrated strong\ncapabilities in text and image understanding, their performance on complex,\nreal world multimodal table reasoning remains unexplored. To bridge this gap,\nwe introduce MMTBENCH (Multimodal Table Benchmark), a benchmark consisting of\n500 real world multimodal tables drawn from diverse real world sources, with a\ntotal of 4021 question answer pairs. MMTBENCH questions cover four question\ntypes (Explicit, Implicit, Answer Mention, and Visual Based), five reasoning\ntypes (Mathematical, Extrema Identification, Fact Verification, Vision Based,\nand Others), and eight table types (Single/Multiple Entity, Maps and Charts\nwith Entities, Single/Multiple Charts, Maps, and Visualizations). Extensive\nevaluation of state of the art models on all types reveals substantial\nperformance gaps, particularly on questions requiring visual-based reasoning\nand multi-step inference. These findings show the urgent need for improved\narchitectures that more tightly integrate vision and language processing. By\nproviding a challenging, high-quality resource that mirrors the complexity of\nreal-world tasks, MMTBENCH underscores its value as a resource for future\nresearch on multimodal tables.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMMTBENCH\u7684\u591a\u6a21\u6001\u8868\u683c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b500\u4e2a\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u591a\u6a21\u6001\u8868\u683c\u548c4021\u4e2a\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u7528\u4e8e\u8bc4\u4f30\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u591a\u6a21\u6001\u8868\u683c\u65f6\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u548c\u56fe\u50cf\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u7684\u771f\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u8868\u683c\u4efb\u52a1\u65f6\u7684\u6027\u80fd\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u4e13\u95e8\u7684\u57fa\u51c6\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u8005\u4eec\u4ece\u591a\u6837\u5316\u7684\u771f\u5b9e\u4e16\u754c\u6765\u6e90\u4e2d\u6536\u96c6\u4e86500\u4e2a\u591a\u6a21\u6001\u8868\u683c\uff0c\u5e76\u8bbe\u8ba1\u4e864021\u4e2a\u6db5\u76d6\u56db\u79cd\u95ee\u9898\u7c7b\u578b\u3001\u4e94\u79cd\u63a8\u7406\u7c7b\u578b\u548c\u516b\u79cd\u8868\u683c\u7c7b\u578b\u7684\u95ee\u9898-\u7b54\u6848\u5bf9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u7684\u5148\u8fdb\u6a21\u578b\u5728\u591a\u6a21\u6001\u8868\u683c\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u89c6\u89c9\u63a8\u7406\u548c\u591a\u6b65\u63a8\u7406\u7684\u95ee\u9898\u4e0a\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u672a\u6765\u7814\u7a76\u4e2d\u8feb\u5207\u9700\u8981\u6539\u8fdb\u89c6\u89c9\u4e0e\u8bed\u8a00\u5904\u7406\u7d27\u5bc6\u7ed3\u5408\u7684\u6a21\u578b\u67b6\u6784\uff0c\u4ee5\u63d0\u5347\u591a\u6a21\u6001\u8868\u683c\u7406\u89e3\u7684\u6027\u80fd\u3002", "keywords": "\u591a\u6a21\u6001\u8868\u683c,\u89c6\u89c9\u8bed\u8a00\u6a21\u578b,\u57fa\u51c6\u6d4b\u8bd5,\u771f\u5b9e\u4e16\u754c\u4efb\u52a1,\u63a8\u7406\u80fd\u529b"}}
{"id": "2505.22425", "pdf": "https://arxiv.org/pdf/2505.22425", "abs": "https://arxiv.org/abs/2505.22425", "authors": ["Xueliang Zhao", "Wei Wu", "Lingpeng Kong"], "title": "Scaling Reasoning without Attention", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "preprint", "summary": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u53cc\u5c42\u7684\u65e0\u6ce8\u610f\u529b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u67b6\u6784\u548c\u6570\u636e\u4e2d\u5fc3\u7684\u521b\u65b0\u89e3\u51b3\u4e86Transformer\u7684\u67b6\u6784\u6548\u7387\u95ee\u9898\u548c\u7f3a\u4e4f\u7ed3\u6784\u5316\u5fae\u8c03\u7684\u6311\u6218\uff0c\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u8bb8\u591a\u5927\u578bTransformer\u548c\u6df7\u5408\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u56e0Transformer\u67b6\u6784\u7684\u4f4e\u6548\u548c\u9ad8\u96be\u5ea6\u9886\u57df\u7f3a\u4e4f\u7ed3\u6784\u5316\u5fae\u8c03\u800c\u53d7\u9650\u3002\u672c\u6587\u63d0\u51fa\u65b0\u6a21\u578b\u4ee5\u89e3\u51b3\u8fd9\u4e24\u4e2a\u6838\u5fc3\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4e86\u57fa\u4e8eMamba-2\u7684\u72b6\u6001\u7a7a\u95f4\u53cc\u5c42(SSD)\u67b6\u6784\u4ee5\u6d88\u9664\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u952e\u503c\u7f13\u5b58\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8ePromptCoT\u7684\u53cc\u9636\u6bb5\u8bfe\u7a0b\u5fae\u8c03\u7b56\u7565\u6765\u8bad\u7ec3\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728AIME 24\u3001AIME 25\u548cLivecodebench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c7B\u53c2\u6570\u7684\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u540c\u89c4\u6a21\u7684Transformer\u548c\u6df7\u5408\u6a21\u578b\uff0c\u751a\u81f3\u8d85\u8d8a\u66f4\u5927\u7684Gemma3-27B\u3002", "conclusion": "\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u9ad8\u5bb9\u91cf\u63a8\u7406\u4efb\u52a1\u4e2d\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u67b6\u6784\u7684\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u66ff\u4ee3\u65b9\u6848\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u72b6\u6001\u7a7a\u95f4\u6a21\u578b,\u65e0\u6ce8\u610f\u529b\u67b6\u6784,\u590d\u6742\u63a8\u7406,\u8bfe\u7a0b\u5fae\u8c03"}}
{"id": "2505.22440", "pdf": "https://arxiv.org/pdf/2505.22440", "abs": "https://arxiv.org/abs/2505.22440", "authors": ["Khan Masood Parvez", "Sk Md Abidar Rahaman", "Ali Shiri Sichani"], "title": "Data-Driven Antenna Miniaturization: A Knowledge-Based System Integrating Quantum PSO and Predictive Machine Learning Models", "categories": ["cs.LG"], "comment": null, "summary": "The rapid evolution of wireless technologies necessitates automated design\nframeworks to address antenna miniaturization and performance optimization\nwithin constrained development cycles. This study demonstrates a machine\nlearning enhanced workflow integrating Quantum-Behaved Dynamic Particle Swarm\nOptimization (QDPSO) with ANSYS HFSS simulations to accelerate antenna design.\nThe QDPSO algorithm autonomously optimized loop dimensions in 11.53 seconds,\nachieving a resonance frequency of 1.4208 GHz a 12.7 percent reduction compared\nto conventional 1.60 GHz designs. Machine learning models (SVM, Random Forest,\nXGBoost, and Stacked ensembles) predicted resonance frequencies in 0.75 seconds\nusing 936 simulation datasets, with stacked models showing superior training\naccuracy (R2=0.9825) and SVM demonstrating optimal validation performance\n(R2=0.7197). The complete design cycle, encompassing optimization, prediction,\nand ANSYS validation, required 12.42 minutes on standard desktop hardware\n(Intel i5-8500, 16GB RAM), contrasting sharply with the 50-hour benchmark of\nPSADEA-based approaches. This 240 times of acceleration eliminates traditional\ntrial-and-error methods that often extend beyond seven expert-led days. The\nsystem enables precise specifications of performance targets with automated\ngeneration of fabrication-ready parameters, particularly benefiting compact\nconsumer devices requiring rapid frequency tuning. By bridging AI-driven\noptimization with CAD validation, this framework reduces engineering workloads\nwhile ensuring production-ready designs, establishing a scalable paradigm for\nnext-generation RF systems in 6G and IoT applications.", "AI": {"tldr": "The paper presents a machine learning-enhanced workflow using QDPSO and ANSYS HFSS to optimize antenna design, achieving a 240x speedup over traditional methods.", "motivation": "To address the need for automated antenna design frameworks due to rapid wireless technology evolution and constrained development cycles.", "method": "Integration of Quantum-Behaved Dynamic Particle Swarm Optimization (QDPSO) with ANSYS HFSS simulations and machine learning models (SVM, Random Forest, XGBoost, Stacked ensembles).", "result": "Achieved a 12.7% reduction in resonance frequency (1.4208 GHz vs. 1.60 GHz) and 240x acceleration compared to traditional methods, with stacked models showing high accuracy (R2=0.9825).", "conclusion": "The framework bridges AI-driven optimization with CAD validation, reducing workload and enabling scalable designs for 6G and IoT applications.", "keywords": "antenna design, machine learning, QDPSO, ANSYS HFSS, optimization, 6G, IoT"}}
{"id": "2505.21753", "pdf": "https://arxiv.org/pdf/2505.21753", "abs": "https://arxiv.org/abs/2505.21753", "authors": ["Roberto Ulloa", "Eve M. Zucker", "Daniel Bultmann", "David J. Simon", "Mykola Makhortykh"], "title": "From prosthetic memory to prosthetic denial: Auditing whether large language models are prone to mass atrocity denialism", "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "The proliferation of large language models (LLMs) can influence how\nhistorical narratives are disseminated and perceived. This study explores the\nimplications of LLMs' responses on the representation of mass atrocity memory,\nexamining whether generative AI systems contribute to prosthetic memory, i.e.,\nmediated experiences of historical events, or to what we term \"prosthetic\ndenial,\" the AI-mediated erasure or distortion of atrocity memories. We argue\nthat LLMs function as interfaces that can elicit prosthetic memories and,\ntherefore, act as experiential sites for memory transmission, but also\nintroduce risks of denialism, particularly when their outputs align with\ncontested or revisionist narratives. To empirically assess these risks, we\nconducted a comparative audit of five LLMs (Claude, GPT, Llama, Mixtral, and\nGemini) across four historical case studies: the Holodomor, the Holocaust, the\nCambodian Genocide, and the genocide against the Tutsis in Rwanda. Each model\nwas prompted with questions addressing common denialist claims in English and\nan alternative language relevant to each case (Ukrainian, German, Khmer, and\nFrench). Our findings reveal that while LLMs generally produce accurate\nresponses for widely documented events like the Holocaust, significant\ninconsistencies and susceptibility to denialist framings are observed for more\nunderrepresented cases like the Cambodian Genocide. The disparities highlight\nthe influence of training data availability and the probabilistic nature of LLM\nresponses on memory integrity. We conclude that while LLMs extend the concept\nof prosthetic memory, their unmoderated use risks reinforcing historical\ndenialism, raising ethical concerns for (digital) memory preservation, and\npotentially challenging the advantageous role of technology associated with the\noriginal values of prosthetic memory.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5982\u4f55\u5f71\u54cd\u5386\u53f2\u53d9\u8ff0\uff0c\u7279\u522b\u662f\u5927\u89c4\u6a21\u66b4\u884c\u8bb0\u5fc6\u7684\u4f20\u64ad\u4e0e\u626d\u66f2\uff0c\u6bd4\u8f83\u4e94\u79cdLLMs\u5728\u56db\u79cd\u5386\u53f2\u6848\u4f8b\u4e2d\u7684\u53cd\u5e94\uff0c\u53d1\u73b0\u5176\u5bf9\u90e8\u5206\u4e8b\u4ef6\u7684\u56de\u5e94\u53ef\u80fd\u52a9\u957f\u5426\u8ba4\u4e3b\u4e49\u3002", "motivation": "\u63a2\u7a76LLMs\u5728\u5386\u53f2\u8bb0\u5fc6\u4f20\u64ad\u4e2d\u7684\u4f5c\u7528\uff0c\u8bc4\u4f30\u5176\u5bf9\u66b4\u884c\u8bb0\u5fc6\u7684\u6f5c\u5728\u626d\u66f2\u6216\u5426\u8ba4\u98ce\u9669\u3002", "method": "\u5bf9\u4e94\u79cdLLMs\uff08Claude\u3001GPT\u3001Llama\u3001Mixtral\u548cGemini\uff09\u5728\u56db\u4e2a\u5386\u53f2\u6848\u4f8b\uff08\u4e4c\u514b\u5170\u5927\u9965\u8352\u3001\u7eb3\u7cb9\u5927\u5c60\u6740\u3001\u67ec\u57d4\u5be8\u79cd\u65cf\u706d\u7edd\u548c\u5362\u65fa\u8fbe\u5927\u5c60\u6740\uff09\u4e2d\u8fdb\u884c\u5bf9\u6bd4\u5ba1\u8ba1\uff0c\u4f7f\u7528\u82f1\u8bed\u548c\u76f8\u5173\u8bed\u8a00\u63d0\u95ee\u3002", "result": "LLMs\u5bf9\u5e7f\u6cdb\u8bb0\u5f55\u7684\u4e8b\u4ef6\uff08\u5982\u7eb3\u7cb9\u5927\u5c60\u6740\uff09\u53cd\u5e94\u51c6\u786e\uff0c\u4f46\u5bf9\u5c11\u6570\u6848\u4f8b\uff08\u5982\u67ec\u57d4\u5be8\u79cd\u65cf\u706d\u7edd\uff09\u6613\u53d7\u5426\u8ba4\u4e3b\u4e49\u6846\u67b6\u5f71\u54cd\uff0c\u53cd\u6620\u4e86\u8bad\u7ec3\u6570\u636e\u7684\u5f71\u54cd\u3002", "conclusion": "LLMs\u867d\u6269\u5c55\u4e86\u201c\u66ff\u4ee3\u8bb0\u5fc6\u201d\u6982\u5ff5\uff0c\u4f46\u672a\u53d7\u76d1\u7ba1\u7684\u4f7f\u7528\u53ef\u80fd\u5f3a\u5316\u5386\u53f2\u5426\u8ba4\u4e3b\u4e49\uff0c\u5f15\u53d1\u6280\u672f\u4f26\u7406\u95ee\u9898\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u5386\u53f2\u8bb0\u5fc6,\u5426\u8ba4\u4e3b\u4e49,\u66ff\u4ee3\u8bb0\u5fc6,\u4f26\u7406"}}
{"id": "2505.22442", "pdf": "https://arxiv.org/pdf/2505.22442", "abs": "https://arxiv.org/abs/2505.22442", "authors": ["Mattie Fellows", "Clarisse Wibault", "Uljad Berdica", "Johannes Forkel", "Jakob N. Foerster", "Michael A. Osborne"], "title": "SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Sample efficiency remains a major obstacle for real world adoption of\nreinforcement learning (RL): success has been limited to settings where\nsimulators provide access to essentially unlimited environment interactions,\nwhich in reality are typically costly or dangerous to obtain. Offline RL in\nprinciple offers a solution by exploiting offline data to learn a near-optimal\npolicy before deployment. In practice, however, current offline RL methods rely\non extensive online interactions for hyperparameter tuning, and have no\nreliable bound on their initial online performance. To address these two\nissues, we introduce two algorithms. Firstly, SOReL: an algorithm for safe\noffline reinforcement learning. Using only offline data, our Bayesian approach\ninfers a posterior over environment dynamics to obtain a reliable estimate of\nthe online performance via the posterior predictive uncertainty. Crucially, all\nhyperparameters are also tuned fully offline. Secondly, we introduce TOReL: a\ntuning for offline reinforcement learning algorithm that extends our\ninformation rate based offline hyperparameter tuning methods to general offline\nRL approaches. Our empirical evaluation confirms SOReL's ability to accurately\nestimate regret in the Bayesian setting whilst TOReL's offline hyperparameter\ntuning achieves competitive performance with the best online hyperparameter\ntuning methods using only offline data. Thus, SOReL and TOReL make a\nsignificant step towards safe and reliable offline RL, unlocking the potential\nfor RL in the real world. Our implementations are publicly available:\nhttps://github.com/CWibault/sorel\\_torel.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\uff0cSOReL\u548cTOReL\uff0c\u7528\u4e8e\u89e3\u51b3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u6837\u672c\u6548\u7387\u4f4e\u548c\u8d85\u53c2\u6570\u8c03\u4f18\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4ec5\u901a\u8fc7\u79bb\u7ebf\u6570\u636e\u9884\u4f30\u5728\u7ebf\u6027\u80fd\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5b89\u5168\u5e94\u7528RL\u7684\u76ee\u6807\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u6837\u672c\u6548\u7387\u4f4e\u548c\u8d85\u53c2\u6570\u8c03\u4f18\u4f9d\u8d56\u4e8e\u5728\u7ebf\u4ea4\u4e92\u7684\u95ee\u9898\uff0c\u8bba\u6587\u65e8\u5728\u5f00\u53d1\u4ec5\u4f9d\u8d56\u79bb\u7ebf\u6570\u636e\u5c31\u80fd\u9884\u4f30\u5728\u7ebf\u6027\u80fd\u5e76\u8fdb\u884c\u8d85\u53c2\u6570\u8c03\u4f18\u7684\u65b9\u6cd5\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u7b97\u6cd5\uff1aSOReL\u901a\u8fc7\u8d1d\u53f6\u65af\u65b9\u6cd5\u4ece\u79bb\u7ebf\u6570\u636e\u63a8\u65ad\u73af\u5883\u52a8\u6001\u7684\u540e\u9a8c\u5206\u5e03\u4ee5\u9884\u4f30\u5728\u7ebf\u6027\u80fd\uff0cTOReL\u5219\u6269\u5c55\u4e86\u57fa\u4e8e\u4fe1\u606f\u7387\u7684\u79bb\u7ebf\u8d85\u53c2\u6570\u8c03\u4f18\u65b9\u6cd5\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u901a\u7528\u79bb\u7ebfRL\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSOReL\u80fd\u51c6\u786e\u9884\u4f30\u8d1d\u53f6\u65af\u8bbe\u7f6e\u4e2d\u7684\u9057\u61be\uff0c\u800cTOReL\u7684\u79bb\u7ebf\u8d85\u53c2\u6570\u8c03\u4f18\u6027\u80fd\u4e0e\u6700\u4f73\u5728\u7ebf\u8c03\u4f18\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "SOReL\u548cTOReL\u5728\u5b9e\u73b0\u5b89\u5168\u53ef\u9760\u7684\u79bb\u7ebfRL\u65b9\u9762\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u4e3aRL\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002", "keywords": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u6837\u672c\u6548\u7387\uff0c\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u8d1d\u53f6\u65af\u65b9\u6cd5\uff0c\u4fe1\u606f\u7387"}}
{"id": "2505.22450", "pdf": "https://arxiv.org/pdf/2505.22450", "abs": "https://arxiv.org/abs/2505.22450", "authors": ["Ossi R\u00e4is\u00e4", "Boris van Breugel", "Mihaela van der Schaar"], "title": "Position: All Current Generative Fidelity and Diversity Metrics are Flawed", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Any method's development and practical application is limited by our ability\nto measure its reliability. The popularity of generative modeling emphasizes\nthe importance of good synthetic data metrics. Unfortunately, previous works\nhave found many failure cases in current metrics, for example lack of outlier\nrobustness and unclear lower and upper bounds. We propose a list of desiderata\nfor synthetic data metrics, and a suite of sanity checks: carefully chosen\nsimple experiments that aim to detect specific and known generative modeling\nfailure modes. Based on these desiderata and the results of our checks, we\narrive at our position: all current generative fidelity and diversity metrics\nare flawed. This significantly hinders practical use of synthetic data. Our aim\nis to convince the research community to spend more effort in developing\nmetrics, instead of models. Additionally, through analyzing how current metrics\nfail, we provide practitioners with guidelines on how these metrics should\n(not) be used.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51fa\u73b0\u6709\u751f\u6210\u6570\u636e\u6307\u6807\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86\u7406\u60f3\u6307\u6807\u7684\u6e05\u5355\u548c\u4e00\u7cfb\u5217\u68c0\u67e5\u5b9e\u9a8c\uff0c\u547c\u5401\u793e\u533a\u66f4\u591a\u5173\u6ce8\u6307\u6807\u800c\u975e\u6a21\u578b\u7684\u5f00\u53d1\u3002", "motivation": "\u751f\u6210\u5efa\u6a21\u7684\u5e7f\u6cdb\u5e94\u7528\u7a81\u663e\u4e86\u5bf9\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u6307\u6807\u7684\u9700\u6c42\uff0c\u4f46\u73b0\u6709\u6307\u6807\u5b58\u5728\u8bf8\u591a\u95ee\u9898\uff0c\u5982\u7f3a\u4e4f\u5f02\u5e38\u9c81\u68d2\u6027\u548c\u4e0d\u660e\u786e\u7684\u4e0a\u4e0b\u754c\uff0c\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u8bc4\u4f30\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u7406\u60f3\u6307\u6807\u7684\u7279\u5f81\u6e05\u5355\u548c\u4e00\u7cfb\u5217\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7b80\u5355\u5b9e\u9a8c\uff08\u79f0\u4e3a\u201c\u68c0\u67e5\u5b9e\u9a8c\u201d\uff09\uff0c\u7528\u4e8e\u68c0\u6d4b\u751f\u6210\u6a21\u578b\u7684\u5df2\u77e5\u5931\u8d25\u6a21\u5f0f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6240\u6709\u5f53\u524d\u7684\u751f\u6210\u4fdd\u771f\u5ea6\u548c\u591a\u6837\u6027\u6307\u6807\u5747\u5b58\u5728\u7f3a\u9677\uff0c\u4e25\u91cd\u5f71\u54cd\u4e86\u5408\u6210\u6570\u636e\u7684\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "\u4f5c\u8005\u547c\u5401\u7814\u7a76\u793e\u533a\u5e94\u66f4\u591a\u5730\u6295\u5165\u6307\u6807\u800c\u975e\u6a21\u578b\u7684\u5f00\u53d1\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u73b0\u6709\u6307\u6807\u7684\u5931\u8d25\u6848\u4f8b\uff0c\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u4f7f\u7528\u6307\u5357\u3002", "keywords": "\u751f\u6210\u5efa\u6a21\u3001\u5408\u6210\u6570\u636e\u3001\u6307\u6807\u8bc4\u4f30\u3001\u4fdd\u771f\u5ea6\u3001\u591a\u6837\u6027"}}
{"id": "2505.21811", "pdf": "https://arxiv.org/pdf/2505.21811", "abs": "https://arxiv.org/abs/2505.21811", "authors": ["Clark Mingxuan Ju", "Leonardo Neves", "Bhuvesh Kumar", "Liam Collins", "Tong Zhao", "Yuwei Qiu", "Qing Dou", "Sohail Nizam", "Sen Yang", "Neil Shah"], "title": "Revisiting Self-attention for Cross-domain Sequential Recommendation", "categories": ["cs.IR", "cs.AI"], "comment": "Accepted to KDD'25", "summary": "Sequential recommendation is a popular paradigm in modern recommender\nsystems. In particular, one challenging problem in this space is cross-domain\nsequential recommendation (CDSR), which aims to predict future behaviors given\nuser interactions across multiple domains. Existing CDSR frameworks are mostly\nbuilt on the self-attention transformer and seek to improve by explicitly\ninjecting additional domain-specific components (e.g. domain-aware module\nblocks). While these additional components help, we argue they overlook the\ncore self-attention module already present in the transformer, a naturally\npowerful tool to learn correlations among behaviors. In this work, we aim to\nimprove the CDSR performance for simple models from a novel perspective of\nenhancing the self-attention. Specifically, we introduce a Pareto-optimal\nself-attention and formulate the cross-domain learning as a multi-objective\nproblem, where we optimize the recommendation task while dynamically minimizing\nthe cross-domain attention scores. Our approach automates knowledge transfer in\nCDSR (dubbed as AutoCDSR) -- it not only mitigates negative transfer but also\nencourages complementary knowledge exchange among auxiliary domains. Based on\nthe idea, we further introduce AutoCDSR+, a more performant variant with slight\nadditional cost. Our proposal is easy to implement and works as a plug-and-play\nmodule that can be incorporated into existing transformer-based recommenders.\nBesides flexibility, it is practical to deploy because it brings little extra\ncomputational overheads without heavy hyper-parameter tuning. AutoCDSR on\naverage improves Recall@10 for SASRec and Bert4Rec by 9.8% and 16.0% and\nNDCG@10 by 12.0% and 16.7%, respectively. Code is available at\nhttps://github.com/snap-research/AutoCDSR.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u8de8\u57df\u987a\u5e8f\u63a8\u8350\u65b9\u6cd5AutoCDSR\uff0c\u901a\u8fc7\u4f18\u5316\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u81ea\u52a8\u5316\u77e5\u8bc6\u8f6c\u79fb\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u8de8\u57df\u987a\u5e8f\u63a8\u8350\u65b9\u6cd5\u867d\u5f15\u5165\u989d\u5916\u57df\u7279\u5b9a\u7ec4\u4ef6\uff0c\u4f46\u5ffd\u7565\u4e86\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u6f5c\u529b\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u63d0\u5347\u7b80\u5355\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faPareto-\u6700\u4f18\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5c06\u8de8\u57df\u5b66\u4e60\u5efa\u6a21\u4e3a\u591a\u76ee\u6807\u95ee\u9898\uff0c\u52a8\u6001\u6700\u5c0f\u5316\u8de8\u57df\u6ce8\u610f\u529b\u5206\u6570\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u51fa\u6027\u80fd\u66f4\u9ad8\u7684\u53d8\u4f53AutoCDSR+\u3002", "result": "AutoCDSR\u663e\u8457\u63d0\u5347\u4e86SASRec\u548cBert4Rec\u7684Recall@10\u548cNDCG@10\uff0c\u5206\u522b\u5e73\u5747\u63d0\u9ad89.8%\u548c16.0%\uff0c\u4ee5\u53ca12.0%\u548c16.7%\u3002", "conclusion": "AutoCDSR\u662f\u4e00\u79cd\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u63d2\u4ef6\u5f0f\u6a21\u5757\uff0c\u6613\u4e8e\u5b9e\u73b0\u548c\u90e8\u7f72\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df\u987a\u5e8f\u63a8\u8350\u7684\u6027\u80fd\u3002", "keywords": "\u8de8\u57df\u987a\u5e8f\u63a8\u8350, \u81ea\u6ce8\u610f\u529b\u673a\u5236, \u591a\u76ee\u6807\u4f18\u5316, \u77e5\u8bc6\u8f6c\u79fb"}}
{"id": "2505.22473", "pdf": "https://arxiv.org/pdf/2505.22473", "abs": "https://arxiv.org/abs/2505.22473", "authors": ["Riccardo Poiani", "Martino Bernasconi", "Andrea Celli"], "title": "Pure Exploration with Infinite Answers", "categories": ["cs.LG"], "comment": null, "summary": "We study pure exploration problems where the set of correct answers is\npossibly infinite, e.g., the regression of any continuous function of the means\nof the bandit. We derive an instance-dependent lower bound for these problems.\nBy analyzing it, we discuss why existing methods (i.e., Sticky Track-and-Stop)\nfor finite answer problems fail at being asymptotically optimal in this more\ngeneral setting. Finally, we present a framework, Sticky-Sequence\nTrack-and-Stop, which generalizes both Track-and-Stop and Sticky\nTrack-and-Stop, and that enjoys asymptotic optimality. Due to its generality,\nour analysis also highlights special cases where existing methods enjoy\noptimality.", "AI": {"tldr": "\u7814\u7a76\u7eaf\u63a2\u7d22\u95ee\u9898\uff0c\u9488\u5bf9\u65e0\u9650\u7b54\u6848\u96c6\u7684\u60c5\u51b5\uff0c\u63d0\u51fa\u65b0\u65b9\u6cd5Sticky-Sequence Track-and-Stop\uff0c\u5206\u6790\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u53ca\u5176\u6539\u8fdb\u6846\u67b6\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7eaf\u63a2\u7d22\u65b9\u6cd5\u5728\u65e0\u9650\u7b54\u6848\u96c6\uff08\u5982\u56de\u5f52\u95ee\u9898\uff09\u4e2d\u65e0\u6cd5\u8fbe\u5230\u6e10\u8fdb\u6700\u4f18\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSticky-Sequence Track-and-Stop\u6846\u67b6\uff0c\u7ed3\u5408Track-and-Stop\u548cSticky Track-and-Stop\uff0c\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u8bbe\u7f6e\u3002", "result": "\u8bc1\u660e\u65b0\u65b9\u6cd5\u5728\u65e0\u9650\u7b54\u6848\u96c6\u4e2d\u5177\u6e10\u8fdb\u6700\u4f18\u6027\uff0c\u540c\u65f6\u63ed\u793a\u73b0\u6709\u65b9\u6cd5\u5728\u67d0\u4e9b\u7279\u6b8a\u60c5\u51b5\u4e0b\u4ecd\u6700\u4f18\u3002", "conclusion": "\u65b0\u6846\u67b6\u5728\u4fdd\u8bc1\u901a\u7528\u6027\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u6700\u4f18\u6027\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u5de5\u5177\u3002", "keywords": "\u7eaf\u63a2\u7d22\u3001\u65e0\u9650\u7b54\u6848\u96c6\u3001\u6e10\u8fdb\u6700\u4f18\u3001\u56de\u5f52\u3001Sticky-Sequence Track-and-Stop"}}
{"id": "2505.21815", "pdf": "https://arxiv.org/pdf/2505.21815", "abs": "https://arxiv.org/abs/2505.21815", "authors": ["Yunyi Zhang", "Ruozhen Yang", "Siqi Jiao", "SeongKu Kang", "Jiawei Han"], "title": "Scientific Paper Retrieval with LLM-Guided Semantic-Based Ranking", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Scientific paper retrieval is essential for supporting literature discovery\nand research. While dense retrieval methods demonstrate effectiveness in\ngeneral-purpose tasks, they often fail to capture fine-grained scientific\nconcepts that are essential for accurate understanding of scientific queries.\nRecent studies also use large language models (LLMs) for query understanding;\nhowever, these methods often lack grounding in corpus-specific knowledge and\nmay generate unreliable or unfaithful content. To overcome these limitations,\nwe propose SemRank, an effective and efficient paper retrieval framework that\ncombines LLM-guided query understanding with a concept-based semantic index.\nEach paper is indexed using multi-granular scientific concepts, including\ngeneral research topics and detailed key phrases. At query time, an LLM\nidentifies core concepts derived from the corpus to explicitly capture the\nquery's information need. These identified concepts enable precise semantic\nmatching, significantly enhancing retrieval accuracy. Experiments show that\nSemRank consistently improves the performance of various base retrievers,\nsurpasses strong existing LLM-based baselines, and remains highly efficient.", "AI": {"tldr": "SemRank\u662f\u4e00\u79cd\u7ed3\u5408LLM\u5f15\u5bfc\u67e5\u8be2\u7406\u89e3\u548c\u57fa\u4e8e\u6982\u5ff5\u8bed\u4e49\u7d22\u5f15\u7684\u8bba\u6587\u68c0\u7d22\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5bc6\u96c6\u68c0\u7d22\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u79d1\u5b66\u6982\u5ff5\u6355\u6349\u4e0a\u7684\u4e0d\u8db3\uff0c\u4ee5\u53ca\u5f53\u524dLLM\u65b9\u6cd5\u7f3a\u4e4f\u8bed\u6599\u5e93\u7279\u5b9a\u77e5\u8bc6\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u591a\u7c92\u5ea6\u79d1\u5b66\u6982\u5ff5\u7d22\u5f15\u8bba\u6587\uff0c\u7ed3\u5408LLM\u8bc6\u522b\u67e5\u8be2\u4e2d\u7684\u6838\u5fc3\u6982\u5ff5\uff0c\u5b9e\u73b0\u7cbe\u786e\u8bed\u4e49\u5339\u914d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSemRank\u80fd\u6301\u7eed\u63d0\u5347\u591a\u79cd\u57fa\u7840\u68c0\u7d22\u5668\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u73b0\u6709LLM\u57fa\u7ebf\uff0c\u5e76\u4fdd\u6301\u9ad8\u6548\u3002", "conclusion": "SemRank\u901a\u8fc7\u7ed3\u5408LLM\u548c\u6982\u5ff5\u7d22\u5f15\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u79d1\u5b66\u8bba\u6587\u68c0\u7d22\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "keywords": "\u79d1\u5b66\u8bba\u6587\u68c0\u7d22,LLM,\u8bed\u4e49\u7d22\u5f15,\u7ec6\u7c92\u5ea6\u6982\u5ff5"}}
{"id": "2505.22474", "pdf": "https://arxiv.org/pdf/2505.22474", "abs": "https://arxiv.org/abs/2505.22474", "authors": ["Amirhossein Sohrabbeig", "Omid Ardakanian", "Petr Musilek"], "title": "Forecasting Multivariate Urban Data via Decomposition and Spatio-Temporal Graph Analysis", "categories": ["cs.LG"], "comment": null, "summary": "The forecasting of multivariate urban data presents a complex challenge due\nto the intricate dependencies between various urban metrics such as weather,\nair pollution, carbon intensity, and energy demand. This paper introduces a\nnovel multivariate time-series forecasting model that utilizes advanced Graph\nNeural Networks (GNNs) to capture spatial dependencies among different\ntime-series variables. The proposed model incorporates a decomposition-based\npreprocessing step, isolating trend, seasonal, and residual components to\nenhance the accuracy and interpretability of forecasts. By leveraging the\ndynamic capabilities of GNNs, the model effectively captures interdependencies\nand improves the forecasting performance. Extensive experiments on real-world\ndatasets, including electricity usage, weather metrics, carbon intensity, and\nair pollution data, demonstrate the effectiveness of the proposed approach\nacross various forecasting scenarios. The results highlight the potential of\nthe model to optimize smart infrastructure systems, contributing to\nenergy-efficient urban development and enhanced public well-being.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u89e3\u9884\u5904\u7406\u6355\u83b7\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728\u73b0\u5b9e\u6570\u636e\u96c6\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u57ce\u5e02\u591a\u5143\u6570\u636e\uff08\u5982\u5929\u6c14\u3001\u7a7a\u6c14\u6c61\u67d3\u3001\u78b3\u6392\u653e\u7b49\uff09\u7684\u590d\u6742\u4f9d\u8d56\u6027\uff0c\u4f20\u7edf\u9884\u6d4b\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u6355\u83b7\u65f6\u7a7a\u4f9d\u8d56\u7684\u65b0\u6a21\u578b\u3002", "method": "\u91c7\u7528GNN\u7ed3\u5408\u5206\u89e3\u9884\u5904\u7406\uff08\u5206\u79bb\u8d8b\u52bf\u3001\u5b63\u8282\u6027\u548c\u6b8b\u5dee\uff09\uff0c\u589e\u5f3a\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728\u7535\u529b\u4f7f\u7528\u3001\u5929\u6c14\u7b49\u591a\u5143\u6570\u636e\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u6a21\u578b\u5728\u667a\u80fd\u57fa\u7840\u8bbe\u65bd\u4f18\u5316\u548c\u57ce\u5e02\u53ef\u6301\u7eed\u53d1\u5c55\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "keywords": "\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b,\u56fe\u795e\u7ecf\u7f51\u7edc,\u57ce\u5e02\u6570\u636e,\u5206\u89e3\u9884\u5904\u7406,\u65f6\u7a7a\u4f9d\u8d56"}}
{"id": "2505.22475", "pdf": "https://arxiv.org/pdf/2505.22475", "abs": "https://arxiv.org/abs/2505.22475", "authors": ["Riccardo Poiani", "Martino Bernasconi", "Andrea Celli"], "title": "Non-Asymptotic Analysis of (Sticky) Track-and-Stop", "categories": ["cs.LG"], "comment": null, "summary": "In pure exploration problems, a statistician sequentially collects\ninformation to answer a question about some stochastic and unknown environment.\nThe probability of returning a wrong answer should not exceed a maximum risk\nparameter $\\delta$ and good algorithms make as few queries to the environment\nas possible. The Track-and-Stop algorithm is a pioneering method to solve these\nproblems. Specifically, it is well-known that it enjoys asymptotic optimality\nsample complexity guarantees for $\\delta\\to 0$ whenever the map from the\nenvironment to its correct answers is single-valued (e.g., best-arm\nidentification with a unique optimal arm). The Sticky Track-and-Stop algorithm\nextends these results to settings where, for each environment, there might\nexist multiple correct answers (e.g., $\\epsilon$-optimal arm identification).\nAlthough both methods are optimal in the asymptotic regime, their\nnon-asymptotic guarantees remain unknown. In this work, we fill this gap and\nprovide non-asymptotic guarantees for both algorithms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u7eaf\u63a2\u7d22\u95ee\u9898\u7684\u975e\u6e10\u8fd1\u4fdd\u8bc1\u8fdb\u884c\u4e86\u7814\u7a76\uff0c\u5206\u6790\u4e86Track-and-Stop\u7b97\u6cd5\u53ca\u5176\u6269\u5c55Sticky Track-and-Stop\u7b97\u6cd5\u5728\u8fd9\u4e9b\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u586b\u8865Track-and-Stop\u7b97\u6cd5\u53ca\u5176\u6269\u5c55Sticky Track-and-Stop\u7b97\u6cd5\u5728\u975e\u6e10\u8fd1\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u4fdd\u8bc1\u7a7a\u767d\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u591a\u4e2a\u6b63\u786e\u7b54\u6848\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u8bba\u6587\u5206\u6790\u4e86Track-and-Stop\u548cSticky Track-and-Stop\u7b97\u6cd5\u5728\u9519\u8bef\u6982\u7387\u4e0d\u8d85\u8fc7$\u03b4$\u7684\u6761\u4ef6\u4e0b\uff0c\u5bf9\u672a\u77e5\u968f\u673a\u73af\u5883\u7684\u67e5\u8be2\u6b21\u6570\u6027\u80fd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u4e3a\u8fd9\u4e24\u79cd\u7b97\u6cd5\u63d0\u4f9b\u4e86\u975e\u6e10\u8fd1\u7684\u67e5\u8be2\u6b21\u6570\u6027\u80fd\u4fdd\u8bc1\u3002", "conclusion": "\u7ed3\u8bba\u662f\u867d\u7136\u8fd9\u4e24\u79cd\u7b97\u6cd5\u5728\u6e10\u8fd1\u60c5\u51b5\u4e0b\u662f\u6700\u4f18\u7684\uff0c\u4f46\u8bba\u6587\u586b\u8865\u4e86\u5176\u5728\u975e\u6e10\u8fd1\u60c5\u51b5\u4e0b\u7684\u7406\u8bba\u7a7a\u767d\u3002", "keywords": "\u7eaf\u63a2\u7d22, Track-and-Stop\u7b97\u6cd5, Sticky Track-and-Stop\u7b97\u6cd5, \u975e\u6e10\u8fd1\u4fdd\u8bc1, \u968f\u673a\u73af\u5883"}}
{"id": "2505.21827", "pdf": "https://arxiv.org/pdf/2505.21827", "abs": "https://arxiv.org/abs/2505.21827", "authors": ["Yongyi Zang", "Zheqi Dai", "Mark D. Plumbley", "Qiuqiang Kong"], "title": "Music Source Restoration", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": "A modified version of this paper is in review", "summary": "We introduce Music Source Restoration (MSR), a novel task addressing the gap\nbetween idealized source separation and real-world music production. Current\nMusic Source Separation (MSS) approaches assume mixtures are simple sums of\nsources, ignoring signal degradations employed during music production like\nequalization, compression, and reverb. MSR models mixtures as degraded sums of\nindividually degraded sources, with the goal of recovering original, undegraded\nsignals. Due to the lack of data for MSR, we present RawStems, a dataset\nannotation of 578 songs with unprocessed source signals organized into 8\nprimary and 17 secondary instrument groups, totaling 354.13 hours. To the best\nof our knowledge, RawStems is the first dataset that contains unprocessed music\nstems with hierarchical categories. We consider spectral filtering, dynamic\nrange compression, harmonic distortion, reverb and lossy codec as possible\ndegradations, and establish U-Former as a baseline method, demonstrating the\nfeasibility of MSR on our dataset. We release the RawStems dataset annotations,\ndegradation simulation pipeline, training code and pre-trained models to be\npublicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u97f3\u4e50\u6e90\u4fee\u590d\uff08MSR\uff09\u4efb\u52a1\uff0c\u89e3\u51b3\u73b0\u6709\u97f3\u4e50\u6e90\u5206\u79bb\uff08MSS\uff09\u5ffd\u7565\u771f\u5b9e\u97f3\u4e50\u5236\u4f5c\u4e2d\u4fe1\u53f7\u9000\u5316\u7684\u95ee\u9898\uff0c\u5e76\u53d1\u5e03RawStems\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u65b9\u6cd5U-Former\u3002", "motivation": "\u5f53\u524d\u97f3\u4e50\u6e90\u5206\u79bb\u65b9\u6cd5\u5047\u8bbe\u6df7\u97f3\u662f\u7b80\u5355\u6e90\u4fe1\u53f7\u76f8\u52a0\uff0c\u5ffd\u7565\u97f3\u4e50\u5236\u4f5c\u4e2d\u7684\u4fe1\u53f7\u9000\u5316\uff08\u5982\u5747\u8861\u3001\u538b\u7f29\u3001\u6df7\u54cd\uff09\uff0cMSR\u4efb\u52a1\u65e8\u5728\u6062\u590d\u539f\u59cb\u4fe1\u53f7\u3002", "method": "\u63d0\u51faMSR\u4efb\u52a1\uff0c\u6a21\u578b\u5c06\u6df7\u97f3\u89c6\u4e3a\u9000\u5316\u6e90\u4fe1\u53f7\u7684\u53e0\u52a0\uff0c\u5e76\u57fa\u4e8eRawStems\u6570\u636e\u96c6\uff08\u5305\u542b578\u9996\u672a\u5904\u7406\u6e90\u4fe1\u53f7\uff09\u8bad\u7ec3U-Former\u4f5c\u4e3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMSR\u5728RawStems\u6570\u636e\u96c6\u4e0a\u53ef\u884c\uff0c\u57fa\u7ebf\u65b9\u6cd5U-Former\u6709\u6548\u3002", "conclusion": "MSR\u586b\u8865\u4e86\u97f3\u4e50\u6e90\u5206\u79bb\u4e0e\u771f\u5b9e\u97f3\u4e50\u5236\u4f5c\u7684\u9e3f\u6c9f\uff0cRawStems\u6570\u636e\u96c6\u548c\u9000\u5316\u6a21\u62df\u6d41\u7a0b\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u57fa\u7840\u3002", "keywords": "\u97f3\u4e50\u6e90\u4fee\u590d, RawStems, \u4fe1\u53f7\u9000\u5316, U-Former, \u97f3\u4e50\u6e90\u5206\u79bb"}}
{"id": "2505.22483", "pdf": "https://arxiv.org/pdf/2505.22483", "abs": "https://arxiv.org/abs/2505.22483", "authors": ["Abhra Chaudhuri", "Anjan Dutta", "Tu Bui", "Serban Georgescu"], "title": "A Closer Look at Multimodal Representation Collapse", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "International Conference on Machine Learning (ICML) 2025 (Spotlight)", "summary": "We aim to develop a fundamental understanding of modality collapse, a\nrecently observed empirical phenomenon wherein models trained for multimodal\nfusion tend to rely only on a subset of the modalities, ignoring the rest. We\nshow that modality collapse happens when noisy features from one modality are\nentangled, via a shared set of neurons in the fusion head, with predictive\nfeatures from another, effectively masking out positive contributions from the\npredictive features of the former modality and leading to its collapse. We\nfurther prove that cross-modal knowledge distillation implicitly disentangles\nsuch representations by freeing up rank bottlenecks in the student encoder,\ndenoising the fusion-head outputs without negatively impacting the predictive\nfeatures from either modality. Based on the above findings, we propose an\nalgorithm that prevents modality collapse through explicit basis reallocation,\nwith applications in dealing with missing modalities. Extensive experiments on\nmultiple multimodal benchmarks validate our theoretical claims. Project page:\nhttps://abhrac.github.io/mmcollapse/.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u6a21\u6001\u878d\u5408\u4e2d\u7684\u6a21\u6001\u5d29\u6e83\u73b0\u8c61\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u663e\u5f0f\u57fa\u91cd\u5206\u914d\u9632\u6b62\u6a21\u6001\u5d29\u6e83\u7684\u7b97\u6cd5\u3002", "motivation": "\u591a\u6a21\u6001\u878d\u5408\u6a21\u578b\u5f80\u5f80\u4f1a\u5ffd\u7565\u67d0\u4e9b\u6a21\u6001\uff0c\u8fd9\u79cd\u73b0\u8c61\u79f0\u4e3a\u6a21\u6001\u5d29\u6e83\u3002\u8bba\u6587\u65e8\u5728\u6df1\u5165\u7406\u89e3\u5176\u6210\u56e0\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u89e3\u8026\u8868\u793a\uff0c\u5e76\u63d0\u51fa\u663e\u5f0f\u57fa\u91cd\u5206\u914d\u7684\u7b97\u6cd5\u6765\u5904\u7406\u7f3a\u5931\u6a21\u6001\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u7406\u8bba\u4e3b\u5f20\uff0c\u7b97\u6cd5\u6709\u6548\u9632\u6b62\u4e86\u6a21\u6001\u5d29\u6e83\u3002", "conclusion": "\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u548c\u663e\u5f0f\u57fa\u91cd\u5206\u914d\u80fd\u6709\u6548\u89e3\u51b3\u6a21\u6001\u5d29\u6e83\u95ee\u9898\u3002", "keywords": "\u6a21\u6001\u5d29\u6e83, \u591a\u6a21\u6001\u878d\u5408, \u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f, \u663e\u5f0f\u57fa\u91cd\u5206\u914d"}}
{"id": "2505.22486", "pdf": "https://arxiv.org/pdf/2505.22486", "abs": "https://arxiv.org/abs/2505.22486", "authors": ["Mujtaba Hussain Mirza", "Maria Rosaria Briglia", "Filippo Bartolucci", "Senad Beadini", "Giuseppe Lisanti", "Iacopo Masi"], "title": "Understanding Adversarial Training with Energy-based Models", "categories": ["cs.LG", "cs.CV"], "comment": "Under review for TPAMI", "summary": "We aim at using Energy-based Model (EBM) framework to better understand\nadversarial training (AT) in classifiers, and additionally to analyze the\nintrinsic generative capabilities of robust classifiers. By viewing standard\nclassifiers through an energy lens, we begin by analyzing how the energies of\nadversarial examples, generated by various attacks, differ from those of the\nnatural samples. The central focus of our work is to understand the critical\nphenomena of Catastrophic Overfitting (CO) and Robust Overfitting (RO) in AT\nfrom an energy perspective. We analyze the impact of existing AT approaches on\nthe energy of samples during training and observe that the behavior of the\n``delta energy' -- change in energy between original sample and its adversarial\ncounterpart -- diverges significantly when CO or RO occurs. After a thorough\nanalysis of these energy dynamics and their relationship with overfitting, we\npropose a novel regularizer, the Delta Energy Regularizer (DER), designed to\nsmoothen the energy landscape during training. We demonstrate that DER is\neffective in mitigating both CO and RO across multiple benchmarks. We further\nshow that robust classifiers, when being used as generative models, have limits\nin handling trade-off between image quality and variability. We propose an\nimproved technique based on a local class-wise principal component analysis\n(PCA) and energy-based guidance for better class-specific initialization and\nadaptive stopping, enhancing sample diversity and generation quality.\nConsidering that we do not explicitly train for generative modeling, we achieve\na competitive Inception Score (IS) and Fr\\'echet inception distance (FID)\ncompared to hybrid discriminative-generative models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u80fd\u91cf\u6a21\u578b\uff08EBM\uff09\u6846\u67b6\u5206\u6790\u5bf9\u6297\u8bad\u7ec3\uff08AT\uff09\u4e2d\u7684\u707e\u96be\u6027\u8fc7\u62df\u5408\uff08CO\uff09\u548c\u9c81\u68d2\u8fc7\u62df\u5408\uff08RO\uff09\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6b63\u5219\u5316\u65b9\u6cd5Delta Energy Regularizer\uff08DER\uff09\u6765\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\u3002\u540c\u65f6\uff0c\u8bba\u6587\u8fd8\u63a2\u8ba8\u4e86\u9c81\u68d2\u5206\u7c7b\u5668\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u5c40\u90e8\u7c7b\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u548c\u6539\u8fdb\u7684\u505c\u6b62\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u751f\u6210\u6837\u672c\u7684\u591a\u6837\u6027\u548c\u8d28\u91cf\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u6df1\u5165\u7406\u89e3\u5bf9\u6297\u8bad\u7ec3\u4e2d\u7684\u707e\u96be\u6027\u8fc7\u62df\u5408\uff08CO\uff09\u548c\u9c81\u68d2\u8fc7\u62df\u5408\uff08RO\uff09\u73b0\u8c61\uff0c\u5e76\u901a\u8fc7\u80fd\u91cf\u89c6\u89d2\u5206\u6790\u5b83\u4eec\u7684\u672c\u8d28\u3002\u540c\u65f6\uff0c\u7814\u7a76\u8fd8\u5173\u6ce8\u9c81\u68d2\u5206\u7c7b\u5668\u7684\u751f\u6210\u80fd\u529b\uff0c\u63a2\u7d22\u5176\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u901a\u8fc7\u80fd\u91cf\u89c6\u89d2\u5206\u6790\u5bf9\u6297\u6837\u672c\u4e0e\u81ea\u7136\u6837\u672c\u7684\u80fd\u91cf\u5dee\u5f02\uff0c\u63d0\u51faDelta Energy Regularizer\uff08DER\uff09\u4f5c\u4e3a\u65b0\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u7f13\u89e3CO\u548cRO\u3002\u5728\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u91c7\u7528\u5c40\u90e8\u7c7b\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u548c\u80fd\u91cf\u5f15\u5bfc\u4f18\u5316\u521d\u59cb\u5316\u548c\u505c\u6b62\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eDER\u80fd\u6709\u6548\u7f13\u89e3CO\u548cRO\u95ee\u9898\u3002\u5728\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u663e\u5f0f\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u53d6\u5f97\u4e86\u4e0e\u6df7\u5408\u5224\u522b-\u751f\u6210\u6a21\u578b\u76f8\u5f53\u7684Inception Score\uff08IS\uff09\u548cFr\\'echet Inception Distance\uff08FID\uff09\u5206\u6570\u3002", "conclusion": "\u7ed3\u8bba\u662f\u901a\u8fc7\u80fd\u91cf\u89c6\u89d2\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u5bf9\u6297\u8bad\u7ec3\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0cDER\u662f\u4e00\u79cd\u6709\u6548\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u9c81\u68d2\u5206\u7c7b\u5668\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u901a\u8fc7\u6280\u672f\u6539\u8fdb\u6765\u5e73\u8861\u751f\u6210\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002", "keywords": "Energy-based Model, Adversarial Training, Catastrophic Overfitting, Robust Overfitting, Delta Energy Regularizer, Generative Modeling"}}
{"id": "2505.21838", "pdf": "https://arxiv.org/pdf/2505.21838", "abs": "https://arxiv.org/abs/2505.21838", "authors": ["Maobin Lu", "Martin Guay", "Telema Harry", "Shimin Wang", "Jordan Cooper"], "title": "Nonadaptive Output Regulation of Second-Order Nonlinear Uncertain Systems", "categories": ["eess.SY", "cs.AI", "cs.SY", "math.OC", "nlin.CD"], "comment": "8 pages, 3 figures", "summary": "This paper investigates the robust output regulation problem of second-order\nnonlinear uncertain systems with an unknown exosystem. Instead of the adaptive\ncontrol approach, this paper resorts to a robust control methodology to solve\nthe problem and thus avoid the bursting phenomenon. In particular, this paper\nconstructs generic internal models for the steady-state state and input\nvariables of the system. By introducing a coordinate transformation, this paper\nconverts the robust output regulation problem into a nonadaptive stabilization\nproblem of an augmented system composed of the second-order nonlinear uncertain\nsystem and the generic internal models. Then, we design the stabilization\ncontrol law and construct a strict Lyapunov function that guarantees the\nrobustness with respect to unmodeled disturbances. The analysis shows that the\noutput zeroing manifold of the augmented system can be made attractive by the\nproposed nonadaptive control law, which solves the robust output regulation\nproblem. Finally, we demonstrate the effectiveness of the proposed nonadaptive\ninternal model approach by its application to the control of the Duffing\nsystem.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u9c81\u68d2\u63a7\u5236\u65b9\u6cd5\u89e3\u51b3\u4e86\u4e8c\u9636\u975e\u7ebf\u6027\u4e0d\u786e\u5b9a\u7cfb\u7edf\u7684\u8f93\u51fa\u8c03\u8282\u95ee\u9898\uff0c\u907f\u514d\u4e86\u81ea\u9002\u5e94\u63a7\u5236\u4e2d\u7684\u7a81\u53d1\u73b0\u8c61\uff0c\u5e76\u901a\u8fc7\u975e\u81ea\u9002\u5e94\u7a33\u5b9a\u5316\u63a7\u5236\u5f8b\u548c\u4e25\u683c\u674e\u96c5\u666e\u8bfa\u592b\u51fd\u6570\u4fdd\u8bc1\u4e86\u9c81\u68d2\u6027\u3002", "motivation": "\u7814\u7a76\u4e8c\u9636\u975e\u7ebf\u6027\u4e0d\u786e\u5b9a\u7cfb\u7edf\u7684\u9c81\u68d2\u8f93\u51fa\u8c03\u8282\u95ee\u9898\uff0c\u907f\u514d\u81ea\u9002\u5e94\u63a7\u5236\u65b9\u6cd5\u4e2d\u7684\u7a81\u53d1\u73b0\u8c61\u3002", "method": "\u6784\u5efa\u901a\u7528\u5185\u6a21\uff0c\u5f15\u5165\u5750\u6807\u53d8\u6362\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u975e\u81ea\u9002\u5e94\u7a33\u5b9a\u5316\u95ee\u9898\uff0c\u8bbe\u8ba1\u7a33\u5b9a\u5316\u63a7\u5236\u5f8b\u548c\u4e25\u683c\u674e\u96c5\u666e\u8bfa\u592b\u51fd\u6570\u3002", "result": "\u63d0\u51fa\u7684\u975e\u81ea\u9002\u5e94\u63a7\u5236\u5f8b\u4f7f\u8f93\u51fa\u96f6\u6d41\u5f62\u5177\u6709\u5438\u5f15\u529b\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u9c81\u68d2\u8f93\u51fa\u8c03\u8282\u95ee\u9898\u3002", "conclusion": "\u975e\u81ea\u9002\u5e94\u5185\u6a21\u65b9\u6cd5\u5728Duffing\u7cfb\u7edf\u63a7\u5236\u4e2d\u8868\u73b0\u6709\u6548\uff0c\u89e3\u51b3\u4e86\u9c81\u68d2\u8f93\u51fa\u8c03\u8282\u95ee\u9898\u3002", "keywords": "\u9c81\u68d2\u63a7\u5236, \u8f93\u51fa\u8c03\u8282, \u975e\u7ebf\u6027\u7cfb\u7edf, \u5185\u6a21, \u674e\u96c5\u666e\u8bfa\u592b\u51fd\u6570"}}
{"id": "2505.21863", "pdf": "https://arxiv.org/pdf/2505.21863", "abs": "https://arxiv.org/abs/2505.21863", "authors": ["Shikhhar Siingh", "Abhinav Rawat", "Vivek Gupta", "Chitta Baral"], "title": "GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Publicly significant images from events hold valuable contextual information,\ncrucial for journalism and education. However, existing methods often struggle\nto extract this relevance accurately. To address this, we introduce GETReason\n(Geospatial Event Temporal Reasoning), a framework that moves beyond\nsurface-level image descriptions to infer deeper contextual meaning. We propose\nthat extracting global event, temporal, and geospatial information enhances\nunderstanding of an image's significance. Additionally, we introduce GREAT\n(Geospatial Reasoning and Event Accuracy with Temporal Alignment), a new metric\nfor evaluating reasoning-based image understanding. Our layered multi-agent\napproach, assessed using a reasoning-weighted metric, demonstrates that\nmeaningful insights can be inferred, effectively linking images to their\nbroader event context.", "AI": {"tldr": "GETReason\u6846\u67b6\u901a\u8fc7\u63d0\u53d6\u5168\u7403\u4e8b\u4ef6\u3001\u65f6\u7a7a\u4fe1\u606f\u63d0\u5347\u56fe\u50cf\u610f\u4e49\u7406\u89e3\uff0c\u5e76\u5f15\u5165GREAT\u8bc4\u4f30\u6307\u6807\u3002\u591a\u5c42\u6b21\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u8868\u660e\uff0c\u53ef\u6709\u6548\u5c06\u56fe\u50cf\u4e0e\u4e8b\u4ef6\u80cc\u666f\u5173\u8054\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u63d0\u53d6\u56fe\u50cf\u6df1\u5c42\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faGETReason\u6846\u67b6\uff0c\u7ed3\u5408\u5168\u7403\u4e8b\u4ef6\u3001\u65f6\u7a7a\u4fe1\u606f\uff1b\u5f15\u5165GREAT\u8bc4\u4f30\u6307\u6807\uff1b\u91c7\u7528\u591a\u5c42\u6b21\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86GETReason\u6846\u67b6\u80fd\u6709\u6548\u63a8\u65ad\u56fe\u50cf\u6df1\u5c42\u610f\u4e49\uff0c\u5e76\u6210\u529f\u5c06\u5176\u4e0e\u4e8b\u4ef6\u80cc\u666f\u5173\u8054\u3002", "conclusion": "GETReason\u80fd\u663e\u8457\u63d0\u5347\u56fe\u50cf\u80cc\u666f\u7406\u89e3\uff0c\u4e3a\u65b0\u95fb\u548c\u6559\u80b2\u9886\u57df\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u5206\u6790\u5de5\u5177\u3002", "keywords": "GETReason, GREAT, \u56fe\u50cf\u7406\u89e3, \u65f6\u7a7a\u63a8\u7406, \u591a\u667a\u80fd\u4f53"}}
{"id": "2505.22491", "pdf": "https://arxiv.org/pdf/2505.22491", "abs": "https://arxiv.org/abs/2505.22491", "authors": ["Moritz Haas", "Sebastian Bordt", "Ulrike von Luxburg", "Leena Chennuru Vankadara"], "title": "On the Surprising Effectiveness of Large Learning Rates under Standard Width Scaling", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "The dominant paradigm for training large-scale vision and language models is\nHe initialization and a single global learning rate (\\textit{standard\nparameterization}, SP). Despite its practical success, standard parametrization\nremains poorly understood from a theoretical perspective: Existing\ninfinite-width theory would predict instability under large learning rates and\nvanishing feature learning under stable learning rates. However, empirically\noptimal learning rates consistently decay much slower than theoretically\npredicted. By carefully studying neural network training dynamics, we\ndemonstrate that this discrepancy is not fully explained by finite-width\nphenomena such as catapult effects or a lack of alignment between weights and\nincoming activations. We instead show that the apparent contradiction can be\nfundamentally resolved by taking the loss function into account: In contrast to\nMean Squared Error (MSE) loss, we prove that under cross-entropy (CE) loss, an\nintermediate \\textit{controlled divergence} regime emerges, where logits\ndiverge but loss, gradients, and activations remain stable. Stable training\nunder large learning rates enables persistent feature evolution at scale in all\nhidden layers, which is crucial for the practical success of SP. In experiments\nacross optimizers (SGD, Adam), architectures (MLPs, GPT) and data modalities\n(vision, language), we validate that neural networks operate in this controlled\ndivergence regime under CE loss but not under MSE loss. Our empirical evidence\nsuggests that width-scaling considerations are surprisingly useful for\npredicting empirically optimal learning rate exponents. Finally, our analysis\nclarifies the effectiveness and limitations of recently proposed layerwise\nlearning rate scalings for standard initialization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u5728\u8de8\u71b5\u635f\u5931\uff08CE\uff09\u4e0b\uff0c\u6807\u51c6\u53c2\u6570\u5316\uff08SP\uff09\u8bad\u7ec3\u5927\u89c4\u6a21\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u578b\u65f6\u7684\u52a8\u529b\u5b66\u7406\u8bba\uff0c\u63ed\u793a\u4e86\u5728\u5927\u5b66\u4e60\u7387\u4e0b\u7684\u7a33\u5b9a\u8bad\u7ec3\u673a\u5236\u2014\u2014\u2018\u53d7\u63a7\u53d1\u6563\u2019\uff08controlled divergence\uff09\u73b0\u8c61\u3002", "motivation": "\u5c3d\u7ba1\u6807\u51c6\u53c2\u6570\u5316\u5728\u5b9e\u8df5\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5176\u7406\u8bba\u89e3\u91ca\u4e0e\u73b0\u6709\u65e0\u9650\u5bbd\u5ea6\u7406\u8bba\u7684\u9884\u6d4b\u5b58\u5728\u77db\u76fe\uff0c\u7279\u522b\u662f\u5b66\u4e60\u7387\u9009\u62e9\u7684\u4e0d\u5339\u914d\u73b0\u8c61\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u7814\u7a76\u8bad\u7ec3\u52a8\u529b\u5b66\uff0c\u89e3\u91ca\u8fd9\u4e00\u77db\u76fe\u7684\u6839\u672c\u539f\u56e0\uff0c\u5e76\u63ed\u793a\u4e3a\u4ec0\u4e48\u8de8\u71b5\u635f\u5931\u6bd4\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u66f4\u6709\u6548\u3002", "method": "\u901a\u8fc7\u5bf9\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u52a8\u529b\u5b66\u7684\u8be6\u7ec6\u7814\u7a76\uff0c\u7279\u522b\u662f\u5bf9\u8de8\u71b5\u635f\u5931\u548c\u5747\u65b9\u8bef\u5dee\u635f\u5931\u7684\u5bf9\u6bd4\u5206\u6790\uff0c\u8bba\u6587\u8bc1\u660e\u4e86\u2018\u53d7\u63a7\u53d1\u6563\u2019\u73b0\u8c61\u7684\u5b58\u5728\uff0c\u5e76\u5728\u591a\u79cd\u4f18\u5316\u5668\uff08SGD\u3001Adam\uff09\u3001\u67b6\u6784\uff08MLP\u3001GPT\uff09\u548c\u6570\u636e\u6a21\u6001\uff08\u89c6\u89c9\u3001\u8bed\u8a00\uff09\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728\u8de8\u71b5\u635f\u5931\u4e0b\uff0c\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u4f1a\u8fdb\u5165\u53d7\u63a7\u53d1\u6563\u72b6\u6001\uff0c\u4f7f\u5f97\u5927\u5b66\u4e60\u7387\u4e0b\u7684\u8bad\u7ec3\u4f9d\u7136\u7a33\u5b9a\uff0c\u5e76\u6301\u7eed\u66f4\u65b0\u6240\u6709\u9690\u85cf\u5c42\u7684\u7279\u5f81\u3002\u8fd9\u4e00\u73b0\u8c61\u7684\u666e\u904d\u6027\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u5747\u5f97\u5230\u9a8c\u8bc1\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u8868\u660e\uff0c\u5bbd\u5ea6\u7f29\u653e\u7406\u8bba\u5bf9\u5b9e\u9645\u5b66\u4e60\u7387\u9009\u62e9\u7684\u9884\u6d4b\u5177\u6709\u6307\u5bfc\u610f\u4e49\u3002", "conclusion": "\u8de8\u71b5\u635f\u5931\u901a\u8fc7\u53d7\u63a7\u53d1\u6563\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5927\u5b66\u4e60\u7387\u4e0b\u7684\u7a33\u5b9a\u8bad\u7ec3\u548c\u6301\u7eed\u7279\u5f81\u66f4\u65b0\uff0c\u89e3\u51b3\u4e86\u6807\u51c6\u53c2\u6570\u5316\u5728\u7406\u8bba\u9884\u6d4b\u4e2d\u7684\u77db\u76fe\u3002\u8be5\u7ed3\u8bba\u63d0\u4f9b\u4e86\u5bf9\u6807\u51c6\u521d\u59cb\u5316\u65b9\u6cd5\u5b9e\u8df5\u6548\u679c\u7684\u7406\u8bba\u89e3\u91ca\u3002", "keywords": "\u6807\u51c6\u53c2\u6570\u5316\u3001\u8de8\u71b5\u635f\u5931\u3001\u65e0\u9650\u5bbd\u5ea6\u7406\u8bba\u3001\u53d7\u63a7\u53d1\u6563\u3001\u5b66\u4e60\u7387\u3001\u7279\u5f81\u5b66\u4e60"}}
{"id": "2505.21880", "pdf": "https://arxiv.org/pdf/2505.21880", "abs": "https://arxiv.org/abs/2505.21880", "authors": ["Yu-Lun Song", "Chung-En Tsern", "Che-Cheng Wu", "Yu-Ming Chang", "Syuan-Bo Huang", "Wei-Chu Chen", "Michael Chia-Liang Lin", "Yu-Ta Lin"], "title": "Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY"], "comment": "8 pages, 8 figures. This paper is reviewed and accepted by the CUPUM\n  (Computational Urban Planning and Urban Management) Conference held by\n  University College London (UCL) in 2025", "summary": "This study presents an innovative approach to urban mobility simulation by\nintegrating a Large Language Model (LLM) with Agent-Based Modeling (ABM).\nUnlike traditional rule-based ABM, the proposed framework leverages LLM to\nenhance agent diversity and realism by generating synthetic population\nprofiles, allocating routine and occasional locations, and simulating\npersonalized routes. Using real-world data, the simulation models individual\nbehaviors and large-scale mobility patterns in Taipei City. Key insights, such\nas route heat maps and mode-specific indicators, provide urban planners with\nactionable information for policy-making. Future work focuses on establishing\nrobust validation frameworks to ensure accuracy and reliability in urban\nplanning applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u57fa\u4e8e\u4ee3\u7406\u5efa\u6a21\uff08ABM\uff09\u7684\u521b\u65b0\u57ce\u5e02\u6d41\u52a8\u6027\u6a21\u62df\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u4ee3\u7406\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\uff0c\u5e76\u5e94\u7528\u4e8e\u53f0\u5317\u5e02\u7684\u5b9e\u9645\u6570\u636e\u6a21\u62df\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684ABM\u65b9\u6cd5\u5728\u4ee3\u7406\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7ed3\u5408LLM\u63d0\u5347\u6a21\u62df\u7684\u903c\u771f\u5ea6\uff0c\u4e3a\u57ce\u5e02\u89c4\u5212\u63d0\u4f9b\u66f4\u7cbe\u51c6\u7684\u6570\u636e\u652f\u6301\u3002", "method": "\u901a\u8fc7LLM\u751f\u6210\u5408\u6210\u4eba\u53e3\u6863\u6848\u3001\u5206\u914d\u5e38\u89c4\u4e0e\u5076\u7136\u5730\u70b9\uff0c\u5e76\u6a21\u62df\u4e2a\u6027\u5316\u8def\u7ebf\uff0c\u7ed3\u5408\u53f0\u5317\u5e02\u5b9e\u9645\u6570\u636e\u5efa\u6a21\u4e2a\u4f53\u884c\u4e3a\u548c\u5927\u89c4\u6a21\u6d41\u52a8\u6027\u6a21\u5f0f\u3002", "result": "\u751f\u6210\u7684\u8def\u7ebf\u70ed\u56fe\u548c\u4ea4\u901a\u65b9\u5f0f\u6307\u6807\u4e3a\u57ce\u5e02\u89c4\u5212\u8005\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u653f\u7b56\u5236\u5b9a\u4f9d\u636e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u57ce\u5e02\u6d41\u52a8\u6027\u6a21\u62df\u7684\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\uff0c\u672a\u6765\u5c06\u91cd\u70b9\u5efa\u7acb\u9a8c\u8bc1\u6846\u67b6\u4ee5\u786e\u4fdd\u89c4\u5212\u5e94\u7528\u7684\u51c6\u786e\u6027\u3002", "keywords": "\u57ce\u5e02\u6d41\u52a8\u6027, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u57fa\u4e8e\u4ee3\u7406\u5efa\u6a21, \u4e2a\u6027\u5316\u8def\u7ebf, \u53f0\u5317\u5e02"}}
{"id": "2505.22492", "pdf": "https://arxiv.org/pdf/2505.22492", "abs": "https://arxiv.org/abs/2505.22492", "authors": ["Hongyi Zhou", "Josiah P. Hanna", "Jin Zhu", "Ying Yang", "Chengchun Shi"], "title": "Demystifying the Paradox of Importance Sampling with an Estimated History-Dependent Behavior Policy in Off-Policy Evaluation", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted by ICML 2025", "summary": "This paper studies off-policy evaluation (OPE) in reinforcement learning with\na focus on behavior policy estimation for importance sampling. Prior work has\nshown empirically that estimating a history-dependent behavior policy can lead\nto lower mean squared error (MSE) even when the true behavior policy is\nMarkovian. However, the question of why the use of history should lower MSE\nremains open. In this paper, we theoretically demystify this paradox by\nderiving a bias-variance decomposition of the MSE of ordinary importance\nsampling (IS) estimators, demonstrating that history-dependent behavior policy\nestimation decreases their asymptotic variances while increasing their\nfinite-sample biases. Additionally, as the estimated behavior policy conditions\non a longer history, we show a consistent decrease in variance. We extend these\nfindings to a range of other OPE estimators, including the sequential IS\nestimator, the doubly robust estimator and the marginalized IS estimator, with\nthe behavior policy estimated either parametrically or non-parametrically.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u79bb\u7b56\u7565\u8bc4\u4f30\uff08OPE\uff09\uff0c\u91cd\u70b9\u63a2\u8ba8\u901a\u8fc7\u884c\u4e3a\u7b56\u7565\u4f30\u8ba1\u6765\u4f18\u5316\u91cd\u8981\u6027\u91c7\u6837\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u771f\u5b9e\u884c\u4e3a\u7b56\u7565\u662f\u9a6c\u5c14\u53ef\u592b\u7684\uff0c\u4f30\u8ba1\u5386\u53f2\u4f9d\u8d56\u7684\u884c\u4e3a\u7b56\u7565\u4e5f\u80fd\u964d\u4f4e\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u3002\u8bba\u6587\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u63ed\u793a\u4e86\u8fd9\u4e00\u73b0\u8c61\u7684\u539f\u56e0\uff1a\u5386\u53f2\u4f9d\u8d56\u7b56\u7565\u964d\u4f4e\u4e86\u6e10\u8fd1\u65b9\u5dee\uff0c\u4f46\u589e\u52a0\u4e86\u6709\u9650\u6837\u672c\u504f\u5dee\u3002\u7ed3\u679c\u8fd8\u9002\u7528\u4e8e\u5176\u4ed6OPE\u4f30\u8ba1\u5668\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u8868\u660e\uff0c\u4f7f\u7528\u5386\u53f2\u4f9d\u8d56\u7684\u884c\u4e3a\u7b56\u7565\u53ef\u4ee5\u964d\u4f4eMSE\uff0c\u4f46\u5176\u539f\u56e0\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63ed\u793a\u8fd9\u4e00\u73b0\u8c61\u80cc\u540e\u7684\u673a\u5236\u3002", "method": "\u901a\u8fc7\u504f\u5dee-\u65b9\u5dee\u5206\u89e3\u5206\u6790\u4e86\u666e\u901a\u91cd\u8981\u6027\u91c7\u6837\uff08IS\uff09\u4f30\u8ba1\u5668\u7684MSE\uff0c\u5e76\u6269\u5c55\u81f3\u5e8f\u5217IS\u3001\u53cc\u7a33\u5065\u4f30\u8ba1\u5668\u7b49\uff0c\u5bf9\u6bd4\u4e86\u53c2\u6570\u548c\u975e\u53c2\u6570\u65b9\u6cd5\u7684\u884c\u4e3a\u7b56\u7565\u4f30\u8ba1\u3002", "result": "\u7406\u8bba\u8bc1\u660e\uff0c\u5386\u53f2\u4f9d\u8d56\u7684\u884c\u4e3a\u7b56\u7565\u4f30\u8ba1\u964d\u4f4e\u4e86\u6e10\u8fd1\u65b9\u5dee\uff0c\u4f46\u589e\u52a0\u4e86\u6709\u9650\u6837\u672c\u504f\u5dee\u3002\u968f\u7740\u5386\u53f2\u957f\u5ea6\u589e\u52a0\uff0c\u65b9\u5dee\u6301\u7eed\u4e0b\u964d\u3002\u7ed3\u8bba\u9002\u7528\u4e8e\u591a\u79cdOPE\u4f30\u8ba1\u5668\u3002", "conclusion": "\u5386\u53f2\u4f9d\u8d56\u7684\u884c\u4e3a\u7b56\u7565\u4f30\u8ba1\u5728OPE\u4e2d\u5177\u6709\u7406\u8bba\u4f18\u52bf\uff0c\u4f46\u5176\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u9700\u5728\u5b9e\u8df5\u4e2d\u6743\u8861\u3002", "keywords": "\u79bb\u7b56\u7565\u8bc4\u4f30, \u5f3a\u5316\u5b66\u4e60, \u91cd\u8981\u6027\u91c7\u6837, \u884c\u4e3a\u7b56\u7565\u4f30\u8ba1, \u504f\u5dee-\u65b9\u5dee\u5206\u89e3"}}
{"id": "2505.21847", "pdf": "https://arxiv.org/pdf/2505.21847", "abs": "https://arxiv.org/abs/2505.21847", "authors": ["Xuwei Xu", "Yang Li", "Yudong Chen", "Jiajun Liu", "Sen Wang"], "title": "RePaViT: Scalable Vision Transformer Acceleration via Structural Reparameterization on Feedforward Network Layers", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ICML2025", "summary": "We reveal that feedforward network (FFN) layers, rather than attention\nlayers, are the primary contributors to Vision Transformer (ViT) inference\nlatency, with their impact signifying as model size increases. This finding\nhighlights a critical opportunity for optimizing the efficiency of large-scale\nViTs by focusing on FFN layers. In this work, we propose a novel channel idle\nmechanism that facilitates post-training structural reparameterization for\nefficient FFN layers during testing. Specifically, a set of feature channels\nremains idle and bypasses the nonlinear activation function in each FFN layer,\nthereby forming a linear pathway that enables structural reparameterization\nduring inference. This mechanism results in a family of ReParameterizable\nVision Transformers (RePaViTs), which achieve remarkable latency reductions\nwith acceptable sacrifices (sometimes gains) in accuracy across various ViTs.\nThe benefits of our method scale consistently with model sizes, demonstrating\ngreater speed improvements and progressively narrowing accuracy gaps or even\nhigher accuracies on larger models. In particular, RePa-ViT-Large and\nRePa-ViT-Huge enjoy 66.8% and 68.7% speed-ups with +1.7% and +1.1% higher top-1\naccuracies under the same training strategy, respectively. RePaViT is the first\nto employ structural reparameterization on FFN layers to expedite ViTs to our\nbest knowledge, and we believe that it represents an auspicious direction for\nefficient ViTs. Source code is available at\nhttps://github.com/Ackesnal/RePaViT.", "AI": {"tldr": "\u8be5\u8bba\u6587\u53d1\u73b0\u524d\u9988\u7f51\u7edc\uff08FFN\uff09\u5c42\u662f\u89c6\u89c9\u53d8\u6362\u5668\uff08ViT\uff09\u63a8\u7406\u5ef6\u8fdf\u7684\u4e3b\u8981\u6765\u6e90\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u9053\u7a7a\u95f2\u673a\u5236\uff0c\u901a\u8fc7\u540e\u8bad\u7ec3\u7ed3\u6784\u91cd\u53c2\u6570\u5316\u4f18\u5316FFN\u5c42\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u5e76\u53ef\u80fd\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0FFN\u5c42\u800c\u975e\u6ce8\u610f\u529b\u5c42\u662fViT\u63a8\u7406\u5ef6\u8fdf\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u5c24\u5176\u5728\u6a21\u578b\u89c4\u6a21\u589e\u5927\u65f6\u5f71\u54cd\u66f4\u663e\u8457\uff0c\u56e0\u6b64\u63d0\u51fa\u4f18\u5316FFN\u5c42\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u901a\u9053\u7a7a\u95f2\u673a\u5236\uff0c\u5141\u8bb8\u90e8\u5206\u7279\u5f81\u901a\u9053\u7ed5\u8fc7\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\uff0c\u5f62\u6210\u7ebf\u6027\u8def\u5f84\u4ee5\u5b9e\u73b0\u63a8\u7406\u65f6\u7684\u7ed3\u6784\u91cd\u53c2\u6570\u5316\uff0c\u4ece\u800c\u6784\u5efa\u53ef\u91cd\u53c2\u6570\u5316ViT\uff08RePaViT\uff09\u3002", "result": "RePaViT\u5728\u4e0d\u540c\u89c4\u6a21\u7684ViT\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5ef6\u8fdf\u964d\u4f4e\uff08\u5982\u5927\u578b\u548c\u8d85\u5927\u578b\u6a21\u578b\u5206\u522b\u63d0\u901f66.8%\u548c68.7%\uff09\uff0c\u4e14\u51c6\u786e\u7387\u6709\u5c0f\u5e45\u63d0\u5347\uff08+1.7%\u548c+1.1%\uff09\u3002", "conclusion": "RePaViT\u9996\u6b21\u901a\u8fc7FFN\u5c42\u7ed3\u6784\u91cd\u53c2\u6570\u5316\u52a0\u901fViT\uff0c\u4e3a\u9ad8\u6548ViT\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5176\u4f18\u52bf\u968f\u6a21\u578b\u89c4\u6a21\u6269\u5927\u800c\u589e\u5f3a\u3002", "keywords": "Vision Transformer, FFN layers, structural reparameterization, inference latency, efficiency optimization"}}
{"id": "2505.22494", "pdf": "https://arxiv.org/pdf/2505.22494", "abs": "https://arxiv.org/abs/2505.22494", "authors": ["Michal Kmicikiewicz", "Vincent Fortuin", "Ewa Szczurek"], "title": "ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type Neighborhoods", "categories": ["cs.LG"], "comment": null, "summary": "Designing protein sequences of both high fitness and novelty is a challenging\ntask in data-efficient protein engineering. Exploration beyond wild-type\nneighborhoods often leads to biologically implausible sequences or relies on\nsurrogate models that lose fidelity in novel regions. Here, we propose\nProSpero, an active learning framework in which a frozen pre-trained generative\nmodel is guided by a surrogate updated from oracle feedback. By integrating\nfitness-relevant residue selection with biologically-constrained Sequential\nMonte Carlo sampling, our approach enables exploration beyond wild-type\nneighborhoods while preserving biological plausibility. We show that our\nframework remains effective even when the surrogate is misspecified. ProSpero\nconsistently outperforms or matches existing methods across diverse protein\nengineering tasks, retrieving sequences of both high fitness and novelty.", "AI": {"tldr": "ProSpro\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u548c\u66f4\u65b0\u7684\u4ee3\u7406\u6a21\u578b\u7ed3\u5408\uff0c\u80fd\u591f\u9ad8\u6548\u8bbe\u8ba1\u5177\u6709\u9ad8\u9002\u5e94\u6027\u548c\u65b0\u9896\u6027\u7684\u86cb\u767d\u8d28\u5e8f\u5217\u3002", "motivation": "\u76ee\u524d\uff0c\u5728\u6570\u636e\u9ad8\u6548\u7684\u86cb\u767d\u8d28\u5de5\u7a0b\u4e2d\uff0c\u8bbe\u8ba1\u517c\u5177\u9ad8\u9002\u5e94\u6027\u548c\u65b0\u9896\u6027\u7684\u86cb\u767d\u8d28\u5e8f\u5217\u662f\u4e00\u4e2a\u6311\u6218\u3002\u63a2\u7d22\u91ce\u751f\u578b\u4ee5\u5916\u7684\u533a\u57df\u901a\u5e38\u4f1a\u5bfc\u81f4\u751f\u7269\u5b66\u4e0d\u5408\u7406\u6027\uff0c\u6216\u4f9d\u8d56\u5728\u65b0\u578b\u533a\u57df\u4e2d\u5931\u53bb\u4fdd\u771f\u5ea6\u7684\u4ee3\u7406\u6a21\u578b\u3002ProSpro\u7684\u76ee\u6807\u662f\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "ProSpro\u91c7\u7528\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u548c\u901a\u8fc7\u6a21\u62df\u53cd\u9988\u66f4\u65b0\u7684\u4ee3\u7406\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u6574\u5408\u4e86\u9002\u5e94\u5ea6\u76f8\u5173\u6b8b\u57fa\u9009\u62e9\u4e0e\u751f\u7269\u5b66\u7ea6\u675f\u7684Sequential Monte Carlo\u91c7\u6837\uff0c\u5b9e\u73b0\u4e86\u65e2\u8d85\u8d8a\u91ce\u751f\u578b\u533a\u57df\u53c8\u4fdd\u6301\u751f\u7269\u5b66\u5408\u7406\u6027\u7684\u63a2\u7d22\u3002", "result": "\u5373\u4f7f\u5728\u4ee3\u7406\u6a21\u578b\u8bbe\u5b9a\u9519\u8bef\u7684\u60c5\u51b5\u4e0b\uff0cProSpro\u4f9d\u7136\u6709\u6548\u3002\u5728\u5404\u79cd\u86cb\u767d\u8d28\u5de5\u7a0b\u4efb\u52a1\u4e2d\uff0cProSpro\u7684\u8868\u73b0\u4f18\u4e8e\u6216\u5339\u914d\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u83b7\u5f97\u540c\u65f6\u5177\u5907\u9ad8\u9002\u5e94\u6027\u548c\u65b0\u9896\u6027\u7684\u5e8f\u5217\u3002", "conclusion": "ProSpro\u8bc1\u660e\u4e86\u5728\u86cb\u767d\u8d28\u5e8f\u5217\u8bbe\u8ba1\u4e2d\uff0c\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u6a21\u578b\u548c\u4e3b\u52a8\u5b66\u4e60\u4ee3\u7406\u6a21\u578b\uff0c\u53ef\u4ee5\u5728\u4fdd\u8bc1\u751f\u7269\u5b66\u5408\u7406\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u63a2\u7d22\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u86cb\u767d\u8d28\u5de5\u7a0b\u8bbe\u8ba1\uff0c\u4e3b\u52a8\u5b66\u4e60\uff0c\u751f\u6210\u6a21\u578b\uff0c\u9002\u5e94\u6027\uff0c\u65b0\u9896\u6027"}}
{"id": "2505.21849", "pdf": "https://arxiv.org/pdf/2505.21849", "abs": "https://arxiv.org/abs/2505.21849", "authors": ["Bo Tang", "Junyi Zhu", "Chenyang Xi", "Yunhang Ge", "Jiahao Wu", "Yuchen Feng", "Yijun Niu", "Wenqiang Wei", "Yu Yu", "Chunyu Li", "Zehao Lin", "Hao Wu", "Ning Liao", "Yebin Yang", "Jiajia Wang", "Zhiyu Li", "Feiyu Xiong", "Jingrun Chen"], "title": "Xinyu AI Search: Enhanced Relevance and Comprehensive Results with Rich Answer Presentations", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Traditional search engines struggle to synthesize fragmented information for\ncomplex queries, while generative AI search engines face challenges in\nrelevance, comprehensiveness, and presentation. To address these limitations,\nwe introduce Xinyu AI Search, a novel system that incorporates a\nquery-decomposition graph to dynamically break down complex queries into\nsub-queries, enabling stepwise retrieval and generation. Our retrieval pipeline\nenhances diversity through multi-source aggregation and query expansion, while\nfiltering and re-ranking strategies optimize passage relevance. Additionally,\nXinyu AI Search introduces a novel approach for fine-grained, precise built-in\ncitation and innovates in result presentation by integrating timeline\nvisualization and textual-visual choreography. Evaluated on recent real-world\nqueries, Xinyu AI Search outperforms eight existing technologies in human\nassessments, excelling in relevance, comprehensiveness, and insightfulness.\nAblation studies validate the necessity of its key sub-modules. Our work\npresents the first comprehensive framework for generative AI search engines,\nbridging retrieval, generation, and user-centric presentation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aXinyu AI Search\u7684\u65b0\u578b\u641c\u7d22\u5f15\u64ce\u7cfb\u7edf\uff0c\u901a\u8fc7\u67e5\u8be2\u5206\u89e3\u56fe\u9010\u6b65\u5904\u7406\u590d\u6742\u67e5\u8be2\uff0c\u7ed3\u5408\u591a\u6e90\u68c0\u7d22\u548c\u7ed3\u679c\u4f18\u5316\uff0c\u63d0\u5347\u4e86\u76f8\u5173\u6027\u548c\u5168\u9762\u6027\uff0c\u5e76\u5728\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u4f20\u7edf\u641c\u7d22\u5f15\u64ce\u96be\u4ee5\u5408\u6210\u590d\u6742\u67e5\u8be2\u7684\u788e\u7247\u5316\u4fe1\u606f\uff0c\u751f\u6210\u5f0fAI\u641c\u7d22\u5f15\u64ce\u5219\u5728\u76f8\u5173\u6027\u3001\u5168\u9762\u6027\u548c\u5448\u73b0\u65b9\u5f0f\u4e0a\u5b58\u5728\u6311\u6218\u3002", "method": "\u91c7\u7528\u67e5\u8be2\u5206\u89e3\u56fe\u52a8\u6001\u62c6\u5206\u590d\u6742\u67e5\u8be2\u4e3a\u5b50\u67e5\u8be2\uff0c\u7ed3\u5408\u591a\u6e90\u805a\u5408\u68c0\u7d22\u3001\u67e5\u8be2\u6269\u5c55\u3001\u8fc7\u6ee4\u548c\u91cd\u6392\u5e8f\u7b56\u7565\u4f18\u5316\u7ed3\u679c\uff0c\u5e76\u521b\u65b0\u6027\u5730\u5f15\u5165\u7cbe\u7ec6\u5316\u5f15\u7528\u548c\u7ed3\u679c\u5448\u73b0\u65b9\u5f0f\u3002", "result": "Xinyu AI Search\u5728\u771f\u5b9e\u67e5\u8be2\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u516b\u79cd\u73b0\u6709\u6280\u672f\uff0c\u5c24\u5176\u5728\u76f8\u5173\u6027\u3001\u5168\u9762\u6027\u548c\u6d1e\u5bdf\u529b\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u4e3a\u751f\u6210\u5f0fAI\u641c\u7d22\u5f15\u64ce\u63d0\u4f9b\u4e86\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u6574\u5408\u4e86\u68c0\u7d22\u3001\u751f\u6210\u548c\u7528\u6237\u5bfc\u5411\u7684\u5448\u73b0\u3002", "keywords": "\u641c\u7d22\u5f15\u64ce, \u67e5\u8be2\u5206\u89e3, \u751f\u6210\u5f0fAI, \u591a\u6e90\u68c0\u7d22, \u7ed3\u679c\u4f18\u5316"}}
{"id": "2505.22504", "pdf": "https://arxiv.org/pdf/2505.22504", "abs": "https://arxiv.org/abs/2505.22504", "authors": ["Ahmed Hossam Mohammed", "Kishansingh Rajput", "Simon Taylor", "Denis Furletov", "Sergey Furletov", "Malachi Schram"], "title": "Geometric GNNs for Charged Particle Tracking at GlueX", "categories": ["cs.LG"], "comment": null, "summary": "Nuclear physics experiments are aimed at uncovering the fundamental building\nblocks of matter. The experiments involve high-energy collisions that produce\ncomplex events with many particle trajectories. Tracking charged particles\nresulting from collisions in the presence of a strong magnetic field is\ncritical to enable the reconstruction of particle trajectories and precise\ndetermination of interactions. It is traditionally achieved through\ncombinatorial approaches that scale worse than linearly as the number of hits\ngrows. Since particle hit data naturally form a 3-dimensional point cloud and\ncan be structured as graphs, Graph Neural Networks (GNNs) emerge as an\nintuitive and effective choice for this task. In this study, we evaluate the\nGNN model for track finding on the data from the GlueX experiment at Jefferson\nLab. We use simulation data to train the model and test on both simulation and\nreal GlueX measurements. We demonstrate that GNN-based track finding\noutperforms the currently used traditional method at GlueX in terms of\nsegment-based efficiency at a fixed purity while providing faster inferences.\nWe show that the GNN model can achieve significant speedup by processing\nmultiple events in batches, which exploits the parallel computation capability\nof Graphical Processing Units (GPUs). Finally, we compare the GNN\nimplementation on GPU and FPGA and describe the trade-off.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u6539\u8fdb\u9ad8\u80fd\u6838\u7269\u7406\u5b9e\u9a8c\u4e2d\u7684\u7c92\u5b50\u8f68\u8ff9\u91cd\u5efa\uff0c\u8bc1\u660e\u5176\u5728\u6548\u7387\u548c\u901f\u5ea6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u6bd4\u8f83\u4e86GPU\u548cFPGA\u7684\u5b9e\u73b0\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u7ec4\u5408\u65b9\u6cd5\u5728\u7c92\u5b50\u8f68\u8ff9\u91cd\u5efa\u4e2d\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u800cGNN\u56e0\u5176\u5bf93\u7ef4\u70b9\u4e91\u6570\u636e\u7684\u9ad8\u6548\u5904\u7406\u80fd\u529b\u6210\u4e3a\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7528\u6a21\u62df\u6570\u636e\u8bad\u7ec3GNN\u6a21\u578b\uff0c\u5e76\u5728\u6a21\u62df\u548c\u771f\u5b9eGlueX\u5b9e\u9a8c\u6570\u636e\u4e0a\u6d4b\u8bd5\uff0c\u6bd4\u8f83\u5176\u4e0e\u4f20\u7edf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "result": "GNN\u5728\u56fa\u5b9a\u7eaf\u5ea6\u4e0b\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u5e76\u5229\u7528GPU\u7684\u5e76\u884c\u8ba1\u7b97\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "GNN\u5728\u9ad8\u80fd\u7269\u7406\u5b9e\u9a8c\u4e2d\u7684\u8f68\u8ff9\u91cd\u5efa\u4efb\u52a1\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5c24\u5176\u5728\u5904\u7406\u5927\u6279\u91cf\u6570\u636e\u65f6\u3002", "keywords": "Graph Neural Networks, track finding, nuclear physics, GPU, FPGA"}}
{"id": "2505.21850", "pdf": "https://arxiv.org/pdf/2505.21850", "abs": "https://arxiv.org/abs/2505.21850", "authors": ["Yanbei Jiang", "Yihao Ding", "Chao Lei", "Jiayang Ao", "Jey Han Lau", "Krista A. Ehinger"], "title": "Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage Task", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at ACL Findings", "summary": "Current Multimodal Large Language Models (MLLMs) excel in general visual\nreasoning but remain underexplored in Abstract Visual Reasoning (AVR), which\ndemands higher-order reasoning to identify abstract rules beyond simple\nperception. Existing AVR benchmarks focus on single-step reasoning, emphasizing\nthe end result but neglecting the multi-stage nature of reasoning process. Past\nstudies found MLLMs struggle with these benchmarks, but it doesn't explain how\nthey fail. To address this gap, we introduce MultiStAR, a Multi-Stage AVR\nbenchmark, based on RAVEN, designed to assess reasoning across varying levels\nof complexity. Additionally, existing metrics like accuracy only focus on the\nfinal outcomes while do not account for the correctness of intermediate steps.\nTherefore, we propose a novel metric, MSEval, which considers the correctness\nof intermediate steps in addition to the final outcomes. We conduct\ncomprehensive experiments on MultiStAR using 17 representative close-source and\nopen-source MLLMs. The results reveal that while existing MLLMs perform\nadequately on basic perception tasks, they continue to face challenges in more\ncomplex rule detection stages.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\uff08AVR\uff09\u4e2d\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86\u591a\u9636\u6bb5AVR\u57fa\u51c6MultiStAR\u548c\u65b0\u8bc4\u4f30\u6307\u6807MSEval\uff0c\u4ee5\u66f4\u5168\u9762\u8bc4\u4f30\u4e2d\u95f4\u6b65\u9aa4\u7684\u6b63\u786e\u6027\u3002\u5b9e\u9a8c\u8868\u660eMLLMs\u5728\u590d\u6742\u89c4\u5219\u68c0\u6d4b\u9636\u6bb5\u4ecd\u5b58\u5728\u6311\u6218\u3002", "motivation": "\u73b0\u6709AVR\u57fa\u51c6\u4ec5\u5173\u6ce8\u5355\u6b65\u63a8\u7406\u548c\u6700\u7ec8\u7ed3\u679c\uff0c\u5ffd\u89c6\u4e86\u63a8\u7406\u8fc7\u7a0b\u7684\u591a\u9636\u6bb5\u6027\u548c\u4e2d\u95f4\u6b65\u9aa4\u7684\u6b63\u786e\u6027\uff0cMLLMs\u5728\u6b64\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u4f46\u5931\u8d25\u539f\u56e0\u4e0d\u660e\u3002", "method": "\u5f15\u5165MultiStAR\u57fa\u51c6\uff08\u57fa\u4e8eRAVEN\uff09\u8bc4\u4f30\u591a\u9636\u6bb5\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u65b0\u6307\u6807MSEval\u8bc4\u4f30\u4e2d\u95f4\u6b65\u9aa4\u4e0e\u6700\u7ec8\u7ed3\u679c\u7684\u6b63\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709MLLMs\u5728\u57fa\u7840\u611f\u77e5\u4efb\u52a1\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u5728\u590d\u6742\u89c4\u5219\u68c0\u6d4b\u9636\u6bb5\u4ecd\u6709\u56f0\u96be\u3002", "conclusion": "\u901a\u8fc7MultiStAR\u548cMSEval\uff0c\u63ed\u793a\u4e86MLLMs\u5728AVR\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u63d0\u5347\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b, \u62bd\u8c61\u89c6\u89c9\u63a8\u7406, \u591a\u9636\u6bb5\u57fa\u51c6, \u8bc4\u4f30\u6307\u6807"}}
{"id": "2505.21956", "pdf": "https://arxiv.org/pdf/2505.21956", "abs": "https://arxiv.org/abs/2505.21956", "authors": ["Mengdan Zhu", "Senhao Cheng", "Guangji Bai", "Yifei Zhang", "Liang Zhao"], "title": "Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Text-to-image generation increasingly demands access to domain-specific,\nfine-grained, and rapidly evolving knowledge that pretrained models cannot\nfully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to\naddress this by retrieving globally relevant images, but they fail when no\nsingle image contains all desired elements from a complex user query. We\npropose Cross-modal RAG, a novel framework that decomposes both queries and\nimages into sub-dimensional components, enabling subquery-aware retrieval and\ngeneration. Our method introduces a hybrid retrieval strategy - combining a\nsub-dimensional sparse retriever with a dense retriever - to identify a\nPareto-optimal set of images, each contributing complementary aspects of the\nquery. During generation, a multimodal large language model is guided to\nselectively condition on relevant visual features aligned to specific\nsubqueries, ensuring subquery-aware image synthesis. Extensive experiments on\nMS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal\nRAG significantly outperforms existing baselines in both retrieval and\ngeneration quality, while maintaining high efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff08Cross-modal RAG\uff09\uff0c\u901a\u8fc7\u5c06\u67e5\u8be2\u548c\u56fe\u50cf\u5206\u89e3\u4e3a\u5b50\u7ef4\u5ea6\u7ec4\u4ef6\uff0c\u7ed3\u5408\u7a00\u758f\u548c\u5bc6\u96c6\u68c0\u7d22\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u67e5\u8be2\u4e0b\u7684\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u5728\u590d\u6742\u67e5\u8be2\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5355\u4e2a\u56fe\u50cf\u901a\u5e38\u65e0\u6cd5\u6db5\u76d6\u6240\u6709\u6240\u9700\u5143\u7d20\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u89e3\u67e5\u8be2\u548c\u56fe\u50cf\uff0c\u5b9e\u73b0\u66f4\u7cbe\u51c6\u7684\u68c0\u7d22\u548c\u751f\u6210\u3002", "method": "\u63d0\u51fa\u4e86\u8de8\u6a21\u6001RAG\u6846\u67b6\uff0c\u7ed3\u5408\u7a00\u758f\u548c\u5bc6\u96c6\u68c0\u7d22\u7b56\u7565\uff0c\u5206\u89e3\u67e5\u8be2\u548c\u56fe\u50cf\u4e3a\u5b50\u7ef4\u5ea6\u7ec4\u4ef6\uff0c\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u9009\u62e9\u6027\u751f\u6210\u56fe\u50cf\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\uff08MS-COCO\u3001Flickr30K\u7b49\uff09\u4e0a\uff0c\u65b9\u6cd5\u5728\u68c0\u7d22\u548c\u751f\u6210\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u4e14\u6548\u7387\u8f83\u9ad8\u3002", "conclusion": "\u8de8\u6a21\u6001RAG\u901a\u8fc7\u5206\u89e3\u548c\u6df7\u5408\u68c0\u7d22\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u67e5\u8be2\u4e0b\u7684\u56fe\u50cf\u751f\u6210\u95ee\u9898\uff0c\u5177\u5907\u5b9e\u7528\u6027\u548c\u9ad8\u6548\u6027\u3002", "keywords": "\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210, \u68c0\u7d22\u589e\u5f3a\u751f\u6210, \u8de8\u6a21\u6001, \u7a00\u758f\u68c0\u7d22, \u5bc6\u96c6\u68c0\u7d22"}}
{"id": "2505.22506", "pdf": "https://arxiv.org/pdf/2505.22506", "abs": "https://arxiv.org/abs/2505.22506", "authors": ["Wenjie Sun", "Bingzhe Wu", "Zhile Yang", "Chengke Wu"], "title": "Sparsification and Reconstruction from the Perspective of Representation Geometry", "categories": ["cs.LG", "22-08", "I.2.4; I.2.7"], "comment": "24 pages, 5 figures", "summary": "Sparse Autoencoders (SAEs) have emerged as a predominant tool in mechanistic\ninterpretability, aiming to identify interpretable monosemantic features.\nHowever, how does sparse encoding organize the representations of activation\nvector from language models? What is the relationship between this\norganizational paradigm and feature disentanglement as well as reconstruction\nperformance? To address these questions, we propose the SAEMA, which validates\nthe stratified structure of the representation by observing the variability of\nthe rank of the symmetric semipositive definite (SSPD) matrix corresponding to\nthe modal tensor unfolded along the latent tensor with the level of noise added\nto the residual stream. To systematically investigate how sparse encoding\nalters representational structures, we define local and global representations,\ndemonstrating that they amplify inter-feature distinctions by merging similar\nsemantic features and introducing additional dimensionality. Furthermore, we\nintervene the global representation from an optimization perspective, proving a\nsignificant causal relationship between their separability and the\nreconstruction performance. This study explains the principles of sparsity from\nthe perspective of representational geometry and demonstrates the impact of\nchanges in representational structure on reconstruction performance.\nParticularly emphasizes the necessity of understanding representations and\nincorporating representational constraints, providing empirical references for\ndeveloping new interpretable tools and improving SAEs. The code is available at\n\\hyperlink{https://github.com/wenjie1835/SAERepGeo}{https://github.com/wenjie1835/SAERepGeo}.", "AI": {"tldr": "SAEMA\u7814\u7a76\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u5982\u4f55\u7ec4\u7ec7\u8bed\u8a00\u6a21\u578b\u7684\u6fc0\u6d3b\u5411\u91cf\u8868\u793a\uff0c\u63a2\u8ba8\u7a00\u758f\u7f16\u7801\u4e0e\u7279\u5f81\u89e3\u7f20\u53ca\u91cd\u5efa\u6027\u80fd\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u5168\u5c40\u548c\u5c40\u90e8\u8868\u793a\u7684\u5b9a\u4e49\uff0c\u5e76\u8bc1\u660e\u5176\u53ef\u5206\u79bb\u6027\u4e0e\u91cd\u5efa\u6027\u80fd\u7684\u56e0\u679c\u5173\u7cfb\u3002", "motivation": "\u63a2\u7a76\u7a00\u758f\u7f16\u7801\u5982\u4f55\u6539\u53d8\u8868\u793a\u7ed3\u6784\uff0c\u4ee5\u53ca\u8fd9\u79cd\u7ed3\u6784\u4e0e\u7279\u5f81\u89e3\u7f20\u548c\u91cd\u5efa\u6027\u80fd\u7684\u5173\u7cfb\uff0c\u4e3a\u5f00\u53d1\u53ef\u89e3\u91ca\u5de5\u5177\u548c\u6539\u8fdbSAEs\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "method": "\u63d0\u51faSAEMA\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6a21\u6001\u5f20\u91cf\u5c55\u5f00\u7684SSPD\u77e9\u9635\u79e9\u7684\u53d8\u5316\u8fdb\u884c\u5206\u6790\uff0c\u5b9a\u4e49\u5168\u5c40\u548c\u5c40\u90e8\u8868\u793a\uff0c\u5e76\u4ece\u4f18\u5316\u89d2\u5ea6\u5e72\u9884\u5168\u5c40\u8868\u793a\u3002", "result": "\u7a00\u758f\u7f16\u7801\u901a\u8fc7\u5408\u5e76\u76f8\u4f3c\u8bed\u4e49\u7279\u5f81\u5e76\u5f15\u5165\u989d\u5916\u7ef4\u5ea6\u653e\u5927\u7279\u5f81\u95f4\u5dee\u5f02\uff0c\u5168\u5c40\u8868\u793a\u7684\u53ef\u5206\u79bb\u6027\u4e0e\u91cd\u5efa\u6027\u80fd\u663e\u8457\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u4ece\u8868\u793a\u51e0\u4f55\u89d2\u5ea6\u89e3\u91ca\u7a00\u758f\u6027\u539f\u7406\uff0c\u5f3a\u8c03\u7406\u89e3\u8868\u793a\u548c\u5f15\u5165\u8868\u793a\u7ea6\u675f\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u6539\u8fdbSAEs\u63d0\u4f9b\u53c2\u8003\u3002", "keywords": "\u7a00\u758f\u81ea\u7f16\u7801\u5668, \u8bed\u8a00\u6a21\u578b, \u8868\u793a\u51e0\u4f55, \u7279\u5f81\u89e3\u7f20, \u91cd\u5efa\u6027\u80fd"}}
{"id": "2505.21851", "pdf": "https://arxiv.org/pdf/2505.21851", "abs": "https://arxiv.org/abs/2505.21851", "authors": ["Sunshine Jiang", "Xiaolin Fang", "Nicholas Roy", "Tom\u00e1s Lozano-P\u00e9rez", "Leslie Pack Kaelbling", "Siddharth Ancha"], "title": "Streaming Flow Policy: Simplifying diffusion$/$flow-matching policies by treating action trajectories as flow trajectories", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "ICRA 2025 Beyond Pick and Place Workshop", "summary": "Recent advances in diffusion$/$flow-matching policies have enabled imitation\nlearning of complex, multi-modal action trajectories. However, they are\ncomputationally expensive because they sample a trajectory of trajectories: a\ndiffusion$/$flow trajectory of action trajectories. They discard intermediate\naction trajectories, and must wait for the sampling process to complete before\nany actions can be executed on the robot. We simplify diffusion$/$flow policies\nby treating action trajectories as flow trajectories. Instead of starting from\npure noise, our algorithm samples from a narrow Gaussian around the last\naction. Then, it incrementally integrates a velocity field learned via flow\nmatching to produce a sequence of actions that constitute a single trajectory.\nThis enables actions to be streamed to the robot on-the-fly during the flow\nsampling process, and is well-suited for receding horizon policy execution.\nDespite streaming, our method retains the ability to model multi-modal\nbehavior. We train flows that stabilize around demonstration trajectories to\nreduce distribution shift and improve imitation learning performance. Streaming\nflow policy outperforms prior methods while enabling faster policy execution\nand tighter sensorimotor loops for learning-based robot control. Project\nwebsite: https://streaming-flow-policy.github.io/", "AI": {"tldr": "\u672c\u6587\u7b80\u5316\u4e86\u6269\u6563/\u6d41\u5339\u914d\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u52a8\u4f5c\u8f68\u8ff9\u89c6\u4e3a\u6d41\u8f68\u8ff9\uff0c\u5e76\u4ece\u4e2d\u91c7\u6837\u800c\u975e\u7eaf\u566a\u58f0\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u52a8\u4f5c\u6d41\u4f20\u8f93\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7b56\u7565\u6267\u884c\u901f\u5ea6\u548c\u6a21\u4eff\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6269\u6563/\u6d41\u5339\u914d\u7b56\u7565\u5728\u6a21\u4eff\u5b66\u4e60\u590d\u6742\u591a\u6a21\u6001\u52a8\u4f5c\u8f68\u8ff9\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e14\u9700\u7b49\u5f85\u5b8c\u6574\u91c7\u6837\u540e\u624d\u80fd\u6267\u884c\u52a8\u4f5c\u3002\u672c\u6587\u65e8\u5728\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u5b9e\u65f6\u52a8\u4f5c\u6d41\u4f20\u8f93\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u4ece\u4e0a\u4e00\u52a8\u4f5c\u7684\u7a84\u9ad8\u65af\u5206\u5e03\u4e2d\u91c7\u6837\uff0c\u901a\u8fc7\u5b66\u4e60\u6d41\u5339\u914d\u7684\u901f\u5ea6\u573a\u9010\u6b65\u751f\u6210\u52a8\u4f5c\u5e8f\u5217\uff0c\u652f\u6301\u5b9e\u65f6\u6d41\u4f20\u8f93\u3002", "result": "\u63d0\u51fa\u7684\u6d41\u7b56\u7565\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u7b56\u7565\u6267\u884c\u548c\u66f4\u7d27\u5bc6\u7684\u4f20\u611f\u5668-\u8fd0\u52a8\u95ed\u73af\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u591a\u6a21\u6001\u884c\u4e3a\u5efa\u6a21\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u7b80\u5316\u91c7\u6837\u8fc7\u7a0b\u5e76\u652f\u6301\u5b9e\u65f6\u6d41\u4f20\u8f93\uff0c\u672c\u6587\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u4eff\u5b66\u4e60\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "keywords": "\u6269\u6563\u7b56\u7565, \u6d41\u5339\u914d, \u6a21\u4eff\u5b66\u4e60, \u673a\u5668\u4eba\u63a7\u5236, \u5b9e\u65f6\u6267\u884c"}}
{"id": "2505.22509", "pdf": "https://arxiv.org/pdf/2505.22509", "abs": "https://arxiv.org/abs/2505.22509", "authors": ["Zhonglin Xie", "Yiman Fong", "Haoran Yuan", "Zaiwen Wen"], "title": "Accelerating Optimization via Differentiable Stopping Time", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Optimization is an important module of modern machine learning applications.\nTremendous efforts have been made to accelerate optimization algorithms. A\ncommon formulation is achieving a lower loss at a given time. This enables a\ndifferentiable framework with respect to the algorithm hyperparameters. In\ncontrast, its dual, minimizing the time to reach a target loss, is believed to\nbe non-differentiable, as the time is not differentiable. As a result, it\nusually serves as a conceptual framework or is optimized using zeroth-order\nmethods. To address this limitation, we propose a differentiable stopping time\nand theoretically justify it based on differential equations. An efficient\nalgorithm is designed to backpropagate through it. As a result, the proposed\ndifferentiable stopping time enables a new differentiable formulation for\naccelerating algorithms. We further discuss its applications, such as online\nhyperparameter tuning and learning to optimize. Our proposed methods show\nsuperior performance in comprehensive experiments across various problems,\nwhich confirms their effectiveness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5fae\u5206\u7684\u505c\u6b62\u65f6\u95f4\u7406\u8bba\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u4f18\u5316\u7b97\u6cd5\u4e2d\u65f6\u95f4\u4e0d\u53ef\u5fae\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5fae\u5206\u65b9\u7a0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u53cd\u5411\u4f20\u64ad\uff0c\u5e76\u5728\u5728\u7ebf\u8d85\u53c2\u6570\u8c03\u4f18\u548c\u5b66\u4e60\u4f18\u5316\u4e2d\u5c55\u793a\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u4f18\u5316\u95ee\u9898\u901a\u5e38\u4ee5\u7ed9\u5b9a\u65f6\u95f4\u5185\u6700\u5c0f\u5316\u635f\u5931\u4e3a\u76ee\u6807\uff0c\u5176\u53cc\u95ee\u9898\uff08\u5373\u6700\u5c0f\u5316\u8fbe\u5230\u76ee\u6807\u635f\u5931\u7684\u65f6\u95f4\uff09\u56e0\u65f6\u95f4\u4e0d\u53ef\u5fae\u800c\u96be\u4ee5\u76f4\u63a5\u4f18\u5316\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u57fa\u4e8e\u5fae\u5206\u65b9\u7a0b\u7406\u8bba\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5fae\u5206\u7684\u505c\u6b62\u65f6\u95f4\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9ad8\u6548\u7684\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728\u591a\u79cd\u95ee\u9898\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u5728\u7ebf\u8d85\u53c2\u6570\u8c03\u4f18\u548c\u5b66\u4e60\u4f18\u5316\u4efb\u52a1\u4e2d\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u53ef\u5fae\u5206\u505c\u6b62\u65f6\u95f4\u4e3a\u4f18\u5316\u7b97\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u5fae\u516c\u5f0f\uff0c\u89e3\u51b3\u4e86\u65f6\u95f4\u4e0d\u53ef\u5fae\u7684\u96be\u9898\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u53ef\u5fae\u5206\u4f18\u5316, \u505c\u6b62\u65f6\u95f4, \u5fae\u5206\u65b9\u7a0b, \u8d85\u53c2\u6570\u8c03\u4f18, \u5b66\u4e60\u4f18\u5316"}}
{"id": "2505.21964", "pdf": "https://arxiv.org/pdf/2505.21964", "abs": "https://arxiv.org/abs/2505.21964", "authors": ["Ziyun Zhang", "Xinyi Liu", "Xiaoyi Zhang", "Jun Wang", "Gang Chen", "Yan Lu"], "title": "UI-Evol: Automatic Knowledge Evolving for Computer Use Agents", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "External knowledge has played a crucial role in the recent development of\ncomputer use agents. We identify a critical knowledge-execution gap: retrieved\nknowledge often fails to translate into effective real-world task execution.\nOur analysis shows even 90\\% correct knowledge yields only 41\\% execution\nsuccess rate. To bridge this gap, we propose UI-Evol, a plug-and-play module\nfor autonomous GUI knowledge evolution. UI-Evol consists of two stages: a\nRetrace Stage that extracts faithful objective action sequences from actual\nagent-environment interactions, and a Critique Stage that refines existing\nknowledge by comparing these sequences against external references. We conduct\ncomprehensive experiments on the OSWorld benchmark with the state-of-the-art\nAgent S2. Our results demonstrate that UI-Evol not only significantly boosts\ntask performance but also addresses a previously overlooked issue of high\nbehavioral standard deviation in computer use agents, leading to superior\nperformance on computer use tasks and substantially improved agent reliability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faUI-Evol\u6a21\u5757\uff0c\u901a\u8fc7\u4e24\u4e2a\u9636\u6bb5\uff08\u56de\u6eaf\u4e0e\u6279\u5224\uff09\u89e3\u51b3\u77e5\u8bc6\u6267\u884c\u4e2d\u7684\u4f4e\u6548\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u4ee3\u7406\u53ef\u9760\u6027\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4fbf90%\u7684\u77e5\u8bc6\u6b63\u786e\uff0c\u6267\u884c\u6210\u529f\u7387\u4ec5\u4e3a41%\uff0c\u7a81\u663e\u77e5\u8bc6\u6267\u884c\u95f4\u7684\u4e25\u91cd\u5dee\u8ddd\uff0c\u4e9f\u9700\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1UI-Evol\u6a21\u5757\uff0c\u5206\u56de\u6eaf\u9636\u6bb5\uff08\u63d0\u53d6\u771f\u5b9e\u52a8\u4f5c\u5e8f\u5217\uff09\u548c\u6279\u5224\u9636\u6bb5\uff08\u5bf9\u7167\u5916\u90e8\u53c2\u8003\u4f18\u5316\u77e5\u8bc6\uff09\u3002", "result": "\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUI-Evol\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6027\u80fd\uff0c\u964d\u4f4e\u884c\u4e3a\u6807\u51c6\u5dee\uff0c\u589e\u5f3a\u4ee3\u7406\u53ef\u9760\u6027\u3002", "conclusion": "UI-Evol\u6709\u6548\u5f25\u5408\u77e5\u8bc6\u6267\u884c\u5dee\u8ddd\uff0c\u63d0\u5347\u8ba1\u7b97\u673a\u4ee3\u7406\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "keywords": "\u77e5\u8bc6\u6267\u884c, GUI\u4ee3\u7406, \u884c\u4e3a\u4f18\u5316, \u4efb\u52a1\u6027\u80fd, \u53ef\u9760\u6027"}}
{"id": "2505.22521", "pdf": "https://arxiv.org/pdf/2505.22521", "abs": "https://arxiv.org/abs/2505.22521", "authors": ["Chao Wang", "Chuanhao Nie", "Yunbo Liu"], "title": "Evaluating Supervised Learning Models for Fraud Detection: A Comparative Study of Classical and Deep Architectures on Imbalanced Transaction Data", "categories": ["cs.LG", "cs.AI"], "comment": "5 pages. Chao Wang, Chuanhao Nie, and Yunbo Liu contributed equally\n  to this work. Corresponding author: Yunbo Liu (yunbo.liu954@duke.edu).\n  Submitted to the 3rd International Conference on Management Innovation and\n  Economy Development (MIED 2025), Chongqing, China", "summary": "Fraud detection remains a critical task in high-stakes domains such as\nfinance and e-commerce, where undetected fraudulent transactions can lead to\nsignificant economic losses. In this study, we systematically compare the\nperformance of four supervised learning models - Logistic Regression, Random\nForest, Light Gradient Boosting Machine (LightGBM), and a Gated Recurrent Unit\n(GRU) network - on a large-scale, highly imbalanced online transaction dataset.\nWhile ensemble methods such as Random Forest and LightGBM demonstrated superior\nperformance in both overall and class-specific metrics, Logistic Regression\noffered a reliable and interpretable baseline. The GRU model showed strong\nrecall for the minority fraud class, though at the cost of precision,\nhighlighting a trade-off relevant for real-world deployment. Our evaluation\nemphasizes not only weighted averages but also per-class precision, recall, and\nF1-scores, providing a nuanced view of each model's effectiveness in detecting\nrare but consequential fraudulent activity. The findings underscore the\nimportance of choosing models based on the specific risk tolerance and\noperational needs of fraud detection systems.", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001LightGBM\u548cGRU\u5728\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u96c6\u6210\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u4f46GRU\u5bf9\u5c0f\u7c7b\u6b3a\u8bc8\u6709\u8f83\u9ad8\u53ec\u56de\u7387\u3002", "motivation": "\u6b3a\u8bc8\u68c0\u6d4b\u5bf9\u91d1\u878d\u548c\u7535\u5546\u81f3\u5173\u91cd\u8981\uff0c\u672a\u68c0\u6d4b\u5230\u7684\u6b3a\u8bc8\u53ef\u80fd\u5bfc\u81f4\u91cd\u5927\u7ecf\u6d4e\u635f\u5931\u3002", "method": "\u7cfb\u7edf\u6bd4\u8f83\u4e86\u56db\u79cd\u76d1\u7763\u5b66\u4e60\u6a21\u578b\uff08\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001LightGBM\u548cGRU\uff09\u5728\u5927\u89c4\u6a21\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u96c6\u6210\u65b9\u6cd5\uff08\u968f\u673a\u68ee\u6797\u548cLightGBM\uff09\u6574\u4f53\u8868\u73b0\u6700\u4f18\uff0c\u903b\u8f91\u56de\u5f52\u63d0\u4f9b\u53ef\u9760\u4e14\u53ef\u89e3\u91ca\u7684\u57fa\u7ebf\uff0cGRU\u5bf9\u5c0f\u7c7b\u6b3a\u8bc8\u53ec\u56de\u7387\u9ad8\u4f46\u7cbe\u5ea6\u4f4e\u3002", "conclusion": "\u6a21\u578b\u9009\u62e9\u9700\u7ed3\u5408\u5177\u4f53\u98ce\u9669\u5bb9\u5fcd\u5ea6\u548c\u64cd\u4f5c\u9700\u6c42\u3002", "keywords": "\u6b3a\u8bc8\u68c0\u6d4b, \u76d1\u7763\u5b66\u4e60, \u4e0d\u5e73\u8861\u6570\u636e\u96c6, \u96c6\u6210\u65b9\u6cd5, GRU"}}
{"id": "2505.21854", "pdf": "https://arxiv.org/pdf/2505.21854", "abs": "https://arxiv.org/abs/2505.21854", "authors": ["Jun Chen", "Xinke Li", "Mingyue Xu", "Tianrui Li", "Chongshou Li"], "title": "Rethinking Gradient-based Adversarial Attacks on Point Cloud Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Gradient-based adversarial attacks have become a dominant approach for\nevaluating the robustness of point cloud classification models. However,\nexisting methods often rely on uniform update rules that fail to consider the\nheterogeneous nature of point clouds, resulting in excessive and perceptible\nperturbations. In this paper, we rethink the design of gradient-based attacks\nby analyzing the limitations of conventional gradient update mechanisms and\npropose two new strategies to improve both attack effectiveness and\nimperceptibility. First, we introduce WAAttack, a novel framework that\nincorporates weighted gradients and an adaptive step-size strategy to account\nfor the non-uniform contribution of points during optimization. This approach\nenables more targeted and subtle perturbations by dynamically adjusting updates\naccording to the local structure and sensitivity of each point. Second, we\npropose SubAttack, a complementary strategy that decomposes the point cloud\ninto subsets and focuses perturbation efforts on structurally critical regions.\nTogether, these methods represent a principled rethinking of gradient-based\nadversarial attacks for 3D point cloud classification. Extensive experiments\ndemonstrate that our approach outperforms state-of-the-art baselines in\ngenerating highly imperceptible adversarial examples. Code will be released\nupon paper acceptance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u7b56\u7565\uff08WAAttack\u548cSubAttack\uff09\uff0c\u6539\u8fdb\u68af\u5ea6\u5bf9\u6297\u653b\u51fb\uff0c\u901a\u8fc7\u52a0\u6743\u68af\u5ea6\u548c\u81ea\u9002\u5e94\u6b65\u957f\u7b56\u7565\u63d0\u5347\u653b\u51fb\u6548\u679c\u4e0e\u9690\u853d\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u68af\u5ea6\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u5ffd\u7565\u70b9\u4e91\u7684\u5f02\u6784\u6027\uff0c\u5bfc\u81f4\u6270\u52a8\u8fc7\u5927\u4e14\u6613\u5bdf\u89c9\uff0c\u9700\u6539\u8fdb\u68af\u5ea6\u66f4\u65b0\u673a\u5236\u4ee5\u5b9e\u73b0\u66f4\u9690\u853d\u6709\u6548\u7684\u653b\u51fb\u3002", "method": "1. WAAttack\uff1a\u5f15\u5165\u52a0\u6743\u68af\u5ea6\u548c\u81ea\u9002\u5e94\u6b65\u957f\u7b56\u7565\uff0c\u52a8\u6001\u8c03\u6574\u66f4\u65b0\uff1b2. SubAttack\uff1a\u5206\u89e3\u70b9\u4e91\u5e76\u805a\u7126\u5173\u952e\u533a\u57df\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u5bf9\u6297\u6837\u672c\u9690\u853d\u6027\u66f4\u5f3a\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u901a\u8fc7\u91cd\u65b0\u8bbe\u8ba1\u68af\u5ea6\u66f4\u65b0\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91\u5bf9\u6297\u653b\u51fb\u7684\u6548\u679c\u4e0e\u9690\u853d\u6027\u3002", "keywords": "\u68af\u5ea6\u653b\u51fb, \u70b9\u4e91\u5206\u7c7b, \u5bf9\u6297\u6837\u672c, \u52a0\u6743\u68af\u5ea6, \u81ea\u9002\u5e94\u6b65\u957f"}}
{"id": "2505.21966", "pdf": "https://arxiv.org/pdf/2505.21966", "abs": "https://arxiv.org/abs/2505.21966", "authors": ["Aditya Gunturu", "Ben Pearman", "Keiichi Ihara", "Morteza Faraji", "Bryan Wang", "Rubaiat Habib Kazi", "Ryo Suzuki"], "title": "MapStory: LLM-Powered Text-Driven Map Animation Prototyping with Human-in-the-Loop Editing", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MM", "H.5.2, H.5.1"], "comment": "16 pages and 15 figures", "summary": "We introduce MapStory, an LLM-powered animation authoring tool that generates\neditable map animation sequences directly from natural language text. Given a\nuser-written script, MapStory leverages an agentic architecture to\nautomatically produce a scene breakdown, which decomposes the script into key\nanimation building blocks such as camera movements, visual highlights, and\nanimated elements. Our system includes a researcher component that accurately\nqueries geospatial information by leveraging an LLM with web search, enabling\nthe automatic extraction of relevant regions, paths, and coordinates while\nallowing users to edit and query for changes or additional information to\nrefine the results. Additionally, users can fine-tune parameters of these\nblocks through an interactive timeline editor. We detail the system's design\nand architecture, informed by formative interviews with professional animators\nand an analysis of 200 existing map animation videos. Our evaluation, which\nincludes expert interviews (N=5) and a usability study (N=12), demonstrates\nthat MapStory enables users to create map animations with ease, facilitates\nfaster iteration, encourages creative exploration, and lowers barriers to\ncreating map-centric stories.", "AI": {"tldr": "MapStory\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u5730\u56fe\u52a8\u753b\u521b\u4f5c\u5de5\u5177\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u811a\u672c\u81ea\u52a8\u751f\u6210\u53ef\u7f16\u8f91\u7684\u52a8\u753b\u5e8f\u5217\uff0c\u7b80\u5316\u4e86\u5730\u56fe\u6545\u4e8b\u7684\u5236\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u65e8\u5728\u964d\u4f4e\u5730\u56fe\u52a8\u753b\u5236\u4f5c\u7684\u96be\u5ea6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u548c\u4ea4\u4e92\u5f0f\u7f16\u8f91\u5de5\u5177\u63d0\u5347\u521b\u4f5c\u6548\u7387\u548c\u63a2\u7d22\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4ee3\u7406\u67b6\u6784\uff0c\u5206\u89e3\u811a\u672c\u4e3a\u52a8\u753b\u6784\u5efa\u5757\uff0c\u7ed3\u5408\u5730\u7406\u7a7a\u95f4\u67e5\u8be2\u548c\u4ea4\u4e92\u5f0f\u65f6\u95f4\u7ebf\u7f16\u8f91\u3002", "result": "\u7528\u6237\u7814\u7a76\uff08N=17\uff09\u8868\u660e\u5de5\u5177\u6613\u7528\u3001\u8fed\u4ee3\u5feb\u901f\uff0c\u5e76\u80fd\u6fc0\u53d1\u521b\u610f\u63a2\u7d22\u3002", "conclusion": "MapStory\u6709\u6548\u964d\u4f4e\u4e86\u5730\u56fe\u52a8\u753b\u5236\u4f5c\u7684\u6280\u672f\u95e8\u69db\uff0c\u63d0\u5347\u4e86\u521b\u4f5c\u6548\u7387\u4e0e\u7075\u6d3b\u6027\u3002", "keywords": "LLM, \u5730\u56fe\u52a8\u753b, \u81ea\u7136\u8bed\u8a00\u5904\u7406, \u4ea4\u4e92\u5f0f\u7f16\u8f91"}}
{"id": "2505.22524", "pdf": "https://arxiv.org/pdf/2505.22524", "abs": "https://arxiv.org/abs/2505.22524", "authors": ["Chinmay Pani", "Zijing Ou", "Yingzhen Li"], "title": "Test-Time Alignment of Discrete Diffusion Models with Sequential Monte Carlo", "categories": ["cs.LG"], "comment": null, "summary": "Discrete diffusion models have become highly effective across various\ndomains. However, real-world applications often require the generative process\nto adhere to certain constraints but without task-specific fine-tuning. To this\nend, we propose a training-free method based on Sequential Monte Carlo (SMC) to\nsample from the reward-aligned target distribution at the test time. Our\napproach leverages twisted SMC with an approximate locally optimal proposal,\nobtained via a first-order Taylor expansion of the reward function. To address\nthe challenge of ill-defined gradients in discrete spaces, we incorporate a\nGumbel-Softmax relaxation, enabling efficient gradient-based approximation\nwithin the discrete generative framework. Empirical results on both synthetic\ndatasets and image modelling validate the effectiveness of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e8f\u8d2f\u8499\u7279\u5361\u6d1b\uff08SMC\uff09\u7684\u8bad\u7ec3\u81ea\u7531\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd1\u4f3c\u5c40\u90e8\u6700\u4f18\u63d0\u6848\u548cGumbel-Softmax\u677e\u5f1b\u5728\u79bb\u6563\u7a7a\u95f4\u751f\u6210\u7b26\u5408\u7ea6\u675f\u7684\u6837\u672c\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\u751f\u6210\u8fc7\u7a0b\u9700\u6ee1\u8db3\u7ea6\u675f\u4f46\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\uff0c\u9700\u4e00\u79cd\u9ad8\u6548\u7684\u65e0\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u626d\u66f2SMC\uff0c\u7ed3\u5408\u4e00\u9636\u6cf0\u52d2\u5c55\u5f00\u8fd1\u4f3c\u5c40\u90e8\u6700\u4f18\u63d0\u6848\uff0c\u5e76\u5f15\u5165Gumbel-Softmax\u677e\u5f1b\u89e3\u51b3\u79bb\u6563\u7a7a\u95f4\u68af\u5ea6\u95ee\u9898\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u56fe\u50cf\u5efa\u6a21\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u751f\u6210\u7b26\u5408\u7ea6\u675f\u7684\u6837\u672c\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u79bb\u6563\u6269\u6563\u6a21\u578b, \u5e8f\u8d2f\u8499\u7279\u5361\u6d1b, Gumbel-Softmax, \u65e0\u8bad\u7ec3\u751f\u6210"}}
{"id": "2505.21855", "pdf": "https://arxiv.org/pdf/2505.21855", "abs": "https://arxiv.org/abs/2505.21855", "authors": ["Jiseung Yoo", "Curran Mahowald", "Meiyu Li", "Wei Ai"], "title": "Extracting Research Instruments from Educational Literature Using LLMs", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are transforming information extraction from\nacademic literature, offering new possibilities for knowledge management. This\nstudy presents an LLM-based system designed to extract detailed information\nabout research instruments used in the education field, including their names,\ntypes, target respondents, measured constructs, and outcomes. Using multi-step\nprompting and a domain-specific data schema, it generates structured outputs\noptimized for educational research. Our evaluation shows that this system\nsignificantly outperforms other approaches, particularly in identifying\ninstrument names and detailed information. This demonstrates the potential of\nLLM-powered information extraction in educational contexts, offering a\nsystematic way to organize research instrument information. The ability to\naggregate such information at scale enhances accessibility for researchers and\neducation leaders, facilitating informed decision-making in educational\nresearch and policy.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u4ece\u6559\u80b2\u9886\u57df\u6587\u732e\u4e2d\u63d0\u53d6\u7814\u7a76\u5de5\u5177\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u591a\u6b65\u63d0\u793a\u548c\u7279\u5b9a\u9886\u57df\u7684\u6570\u636e\u6a21\u5f0f\u751f\u6210\u7ed3\u6784\u5316\u8f93\u51fa\u3002\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u8bc6\u522b\u5de5\u5177\u540d\u79f0\u548c\u8be6\u7ec6\u4fe1\u606f\u65b9\u9762\u3002", "motivation": "\u5229\u7528LLM\u6280\u672f\u6539\u8fdb\u4ece\u5b66\u672f\u6587\u732e\u4e2d\u63d0\u53d6\u7814\u7a76\u5de5\u5177\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u4ee5\u652f\u6301\u6559\u80b2\u7814\u7a76\u548c\u653f\u7b56\u5236\u5b9a\u3002", "method": "\u91c7\u7528LLM\u548c\u591a\u6b65\u63d0\u793a\u6280\u672f\uff0c\u7ed3\u5408\u7279\u5b9a\u9886\u57df\u7684\u6570\u636e\u6a21\u5f0f\uff0c\u751f\u6210\u4f18\u5316\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u3002", "result": "\u7cfb\u7edf\u5728\u63d0\u53d6\u7814\u7a76\u5de5\u5177\u540d\u79f0\u548c\u8be6\u7ec6\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u4fe1\u606f\u63d0\u53d6\u5728\u6559\u80b2\u9886\u57df\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u7cfb\u7edf\u5316\u7ec4\u7ec7\u7814\u7a76\u5de5\u5177\u4fe1\u606f\uff0c\u63d0\u5347\u7814\u7a76\u4eba\u5458\u548c\u6559\u80b2\u9886\u5bfc\u8005\u7684\u51b3\u7b56\u6548\u7387\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u6559\u80b2\u7814\u7a76\u3001\u4fe1\u606f\u63d0\u53d6\u3001\u7814\u7a76\u5de5\u5177\u3001\u7ed3\u6784\u5316\u8f93\u51fa"}}
{"id": "2505.21981", "pdf": "https://arxiv.org/pdf/2505.21981", "abs": "https://arxiv.org/abs/2505.21981", "authors": ["Weiyu Liu", "Neil Nie", "Ruohan Zhang", "Jiayuan Mao", "Jiajun Wu"], "title": "Learning Compositional Behaviors from Demonstration and Language", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": "Presented at CoRL 2024 and as an Oral Presentation at the 2024 CoRL\n  LEAP Workshop. The first two authors contributed equally. The last two\n  authors jointly advised the project. For videos and additional results,\n  visit: https://blade-bot.github.io/", "summary": "We introduce Behavior from Language and Demonstration (BLADE), a framework\nfor long-horizon robotic manipulation by integrating imitation learning and\nmodel-based planning. BLADE leverages language-annotated demonstrations,\nextracts abstract action knowledge from large language models (LLMs), and\nconstructs a library of structured, high-level action representations. These\nrepresentations include preconditions and effects grounded in visual perception\nfor each high-level action, along with corresponding controllers implemented as\nneural network-based policies. BLADE can recover such structured\nrepresentations automatically, without manually labeled states or symbolic\ndefinitions. BLADE shows significant capabilities in generalizing to novel\nsituations, including novel initial states, external state perturbations, and\nnovel goals. We validate the effectiveness of our approach both in simulation\nand on real robots with a diverse set of objects with articulated parts,\npartial observability, and geometric constraints.", "AI": {"tldr": "BLADE\u662f\u4e00\u4e2a\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u57fa\u4e8e\u6a21\u578b\u89c4\u5212\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u8a00\u6807\u6ce8\u7684\u6f14\u793a\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63d0\u53d6\u62bd\u8c61\u52a8\u4f5c\u77e5\u8bc6\uff0c\u6784\u5efa\u9ad8\u5c42\u52a8\u4f5c\u8868\u793a\u5e93\uff0c\u5e76\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u6a21\u578b\u89c4\u5212\u7684\u9700\u6c42\uff0c\u5e76\u5b9e\u73b0\u5bf9\u65b0\u60c5\u5883\uff08\u5982\u521d\u59cb\u72b6\u6001\u3001\u76ee\u6807\u53d8\u5316\u7b49\uff09\u7684\u6cdb\u5316\u3002", "method": "\u5229\u7528\u8bed\u8a00\u6807\u6ce8\u7684\u6f14\u793a\uff0c\u901a\u8fc7LLMs\u63d0\u53d6\u52a8\u4f5c\u77e5\u8bc6\uff0c\u6784\u5efa\u7ed3\u6784\u5316\u9ad8\u5c42\u52a8\u4f5c\u8868\u793a\u5e93\uff0c\u5305\u542b\u89c6\u89c9\u611f\u77e5\u4e2d\u7684\u524d\u63d0\u6761\u4ef6\u548c\u6548\u679c\uff0c\u5e76\u5b9e\u73b0\u4e3a\u795e\u7ecf\u7f51\u7edc\u7b56\u7565\u3002", "result": "BLADE\u80fd\u81ea\u52a8\u6062\u590d\u7ed3\u6784\u5316\u8868\u793a\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6216\u7b26\u53f7\u5b9a\u4e49\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5c55\u793a\u4e86\u5bf9\u65b0\u60c5\u5883\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "BLADE\u901a\u8fc7\u8bed\u8a00\u548c\u6f14\u793a\u7684\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u8f83\u5f3a\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u6027\u80fd\u3002", "keywords": "BLADE, imitation learning, model-based planning, large language models, robotic manipulation"}}
{"id": "2505.22531", "pdf": "https://arxiv.org/pdf/2505.22531", "abs": "https://arxiv.org/abs/2505.22531", "authors": ["Andres Molina-Markham", "Luis Robaina", "Sean Steinle", "Akash Trivedi", "Derek Tsui", "Nicholas Potteiger", "Lauren Brandt", "Ransom Winder", "Ahmed Ridley"], "title": "Training RL Agents for Multi-Objective Network Defense Tasks", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Open-ended learning (OEL) -- which emphasizes training agents that achieve\nbroad capability over narrow competency -- is emerging as a paradigm to develop\nartificial intelligence (AI) agents to achieve robustness and generalization.\nHowever, despite promising results that demonstrate the benefits of OEL,\napplying OEL to develop autonomous agents for real-world cybersecurity\napplications remains a challenge.\n  We propose a training approach, inspired by OEL, to develop autonomous\nnetwork defenders. Our results demonstrate that like in other domains, OEL\nprinciples can translate into more robust and generalizable agents for cyber\ndefense. To apply OEL to network defense, it is necessary to address several\ntechnical challenges. Most importantly, it is critical to provide a task\nrepresentation approach over a broad universe of tasks that maintains a\nconsistent interface over goals, rewards and action spaces. This way, the\nlearning agent can train with varying network conditions, attacker behaviors,\nand defender goals while being able to build on previously gained knowledge.\n  With our tools and results, we aim to fundamentally impact research that\napplies AI to solve cybersecurity problems. Specifically, as researchers\ndevelop gyms and benchmarks for cyber defense, it is paramount that they\nconsider diverse tasks with consistent representations, such as those we\npropose in our work.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u53d7\u5f00\u653e\u5b66\u4e60(OEL)\u542f\u53d1\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u5f00\u53d1\u81ea\u4e3b\u7f51\u7edc\u9632\u5fa1\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u4efb\u52a1\u8868\u793a\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86OEL\u539f\u5219\u80fd\u589e\u5f3a\u7f51\u7edc\u9632\u5fa1\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u3002", "motivation": "\u5f53\u524d\u867dOEL\u5728\u4eba\u5de5\u667a\u80fd\u9886\u57df\u663e\u793a\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u7f51\u7edc\u5b89\u5168\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9700\u89e3\u51b3\u4efb\u52a1\u63a5\u53e3\u4e00\u81f4\u6027\u7b49\u6280\u672f\u96be\u9898\u3002", "method": "\u91c7\u7528OEL\u539f\u5219\uff0c\u8bbe\u8ba1\u7edf\u4e00\u7684\u4efb\u52a1\u8868\u793a\u65b9\u6cd5\uff0c\u8986\u76d6\u591a\u6837\u5316\u7f51\u7edc\u6761\u4ef6\u3001\u653b\u51fb\u884c\u4e3a\u53ca\u9632\u5fa1\u76ee\u6807\uff0c\u4ee5\u652f\u6301\u77e5\u8bc6\u7684\u7d2f\u79ef\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660eOEL\u53ef\u63d0\u5347\u7f51\u7edc\u9632\u5fa1\u4ee3\u7406\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u7f51\u7edc\u5b89\u5168\u9886\u57df\u7684AI\u7814\u7a76\u63d0\u4f9b\u65b0\u5de5\u5177\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86OEL\u5728\u7f51\u7edc\u9632\u5fa1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u547c\u5401\u672a\u6765\u5f00\u53d1\u7f51\u7edc\u5b89\u5168\u57fa\u51c6\u65f6\u9700\u91c7\u7528\u591a\u6837\u6027\u4efb\u52a1\u4e0e\u4e00\u81f4\u8868\u793a\u3002", "keywords": "\u5f00\u653e\u5b66\u4e60(OEL), \u7f51\u7edc\u5b89\u5168, \u81ea\u4e3b\u4ee3\u7406, \u4efb\u52a1\u8868\u793a, \u9c81\u68d2\u6027"}}
{"id": "2505.21866", "pdf": "https://arxiv.org/pdf/2505.21866", "abs": "https://arxiv.org/abs/2505.21866", "authors": ["Guozhen Zhu", "Yuqian Hu", "Weihang Gao", "Wei-Hsiang Wang", "Beibei Wang", "K. J. Ray Liu"], "title": "CSI-Bench: A Large-Scale In-the-Wild Dataset for Multi-task WiFi Sensing", "categories": ["eess.SP", "cs.AI", "cs.DB"], "comment": "21 pages, 4 figures", "summary": "WiFi sensing has emerged as a compelling contactless modality for human\nactivity monitoring by capturing fine-grained variations in Channel State\nInformation (CSI). Its ability to operate continuously and non-intrusively\nwhile preserving user privacy makes it particularly suitable for health\nmonitoring. However, existing WiFi sensing systems struggle to generalize in\nreal-world settings, largely due to datasets collected in controlled\nenvironments with homogeneous hardware and fragmented, session-based recordings\nthat fail to reflect continuous daily activity.\n  We present CSI-Bench, a large-scale, in-the-wild benchmark dataset collected\nusing commercial WiFi edge devices across 26 diverse indoor environments with\n35 real users. Spanning over 461 hours of effective data, CSI-Bench captures\nrealistic signal variability under natural conditions. It includes\ntask-specific datasets for fall detection, breathing monitoring, localization,\nand motion source recognition, as well as a co-labeled multitask dataset with\njoint annotations for user identity, activity, and proximity. To support the\ndevelopment of robust and generalizable models, CSI-Bench provides standardized\nevaluation splits and baseline results for both single-task and multi-task\nlearning. CSI-Bench offers a foundation for scalable, privacy-preserving WiFi\nsensing systems in health and broader human-centric applications.", "AI": {"tldr": "CSI-Bench\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u73af\u5883\u4e0b\u7684WiFi\u611f\u77e5\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709WiFi\u611f\u77e5\u7cfb\u7edf\u56e0\u6570\u636e\u96c6\u5c40\u9650\u4e8e\u53d7\u63a7\u73af\u5883\u548c\u786c\u4ef6\u540c\u8d28\u6027\uff0c\u96be\u4ee5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6cdb\u5316\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5177\u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6\u63a8\u52a8\u6280\u672f\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u5546\u4e1aWiFi\u8fb9\u7f18\u8bbe\u5907\u572826\u79cd\u591a\u6837\u5ba4\u5185\u73af\u5883\u4e2d\u6536\u96c635\u540d\u7528\u6237\u7684461\u5c0f\u65f6\u6570\u636e\uff0c\u8986\u76d6\u591a\u79cd\u4efb\u52a1\uff08\u5982\u8dcc\u5012\u68c0\u6d4b\u3001\u547c\u5438\u76d1\u6d4b\u7b49\uff09\uff0c\u5e76\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u4f30\u5206\u5272\u548c\u57fa\u7ebf\u7ed3\u679c\u3002", "result": "CSI-Bench\u63d0\u4f9b\u4e86\u771f\u5b9e\u4fe1\u53f7\u53d8\u5f02\u6027\u6570\u636e\uff0c\u652f\u6301\u5355\u4efb\u52a1\u548c\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u4e3a\u5f00\u53d1\u9c81\u68d2\u3001\u901a\u7528\u7684WiFi\u611f\u77e5\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "CSI-Bench\u4e3a\u5065\u5eb7\u548c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u9690\u79c1\u4fdd\u62a4\u7684WiFi\u611f\u77e5\u7cfb\u7edf\u57fa\u7840\u3002", "keywords": "WiFi sensing, CSI, benchmark dataset, health monitoring, multi-task learning"}}
{"id": "2505.22088", "pdf": "https://arxiv.org/pdf/2505.22088", "abs": "https://arxiv.org/abs/2505.22088", "authors": ["Sam O'Connor Russell", "Naomi Harte"], "title": "Visual Cues Support Robust Turn-taking Prediction in Noise", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "5 pages", "summary": "Accurate predictive turn-taking models (PTTMs) are essential for naturalistic\nhuman-robot interaction. However, little is known about their performance in\nnoise. This study therefore explores PTTM performance in types of noise likely\nto be encountered once deployed. Our analyses reveal PTTMs are highly sensitive\nto noise. Hold/shift accuracy drops from 84% in clean speech to just 52% in 10\ndB music noise. Training with noisy data enables a multimodal PTTM, which\nincludes visual features to better exploit visual cues, with 72% accuracy in 10\ndB music noise. The multimodal PTTM outperforms the audio-only PTTM across all\nnoise types and SNRs, highlighting its ability to exploit visual cues; however,\nthis does not always generalise to new types of noise. Analysis also reveals\nthat successful training relies on accurate transcription, limiting the use of\nASR-derived transcriptions to clean conditions. We make code publicly available\nfor future research.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5608\u6742\u73af\u5883\u4e2d\u9884\u6d4b\u6027\u8f6e\u6d41\u6a21\u578b\uff08PTTM\uff09\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u566a\u58f0\u5bf9\u6a21\u578b\u5f71\u54cd\u663e\u8457\uff0c\u63d0\u51fa\u591a\u6a21\u6001PTTM\u80fd\u901a\u8fc7\u89c6\u89c9\u7ebf\u7d22\u63d0\u5347\u51c6\u786e\u6027\uff0c\u4f46\u5176\u6cdb\u5316\u80fd\u529b\u6709\u9650\u4e14\u4f9d\u8d56\u4e8e\u7cbe\u786e\u7684\u8f6c\u5f55\u3002", "motivation": "\u63a2\u7d22\u9884\u6d4b\u6027\u8f6e\u6d41\u6a21\u578b\u5728\u73b0\u5b9e\u566a\u58f0\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u4ee5\u6539\u8fdb\u4eba\u673a\u4ea4\u4e92\u7684\u81ea\u7136\u6027\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u97f3\u9891\u5355\u72ec\u548c\u591a\u6a21\u6001\uff08\u542b\u89c6\u89c9\u7ebf\u7d22\uff09PTTM\u5728\u4e0d\u540c\u566a\u58f0\u7c7b\u578b\u548c\u4fe1\u566a\u6bd4\u4e0b\u7684\u8868\u73b0\uff0c\u5206\u6790\u6a21\u578b\u6548\u679c\u3002", "result": "\u591a\u6a21\u6001PTTM\u572810 dB\u97f3\u4e50\u566a\u58f0\u4e2d\u51c6\u786e\u7387\u8fbe72%\uff0c\u4f18\u4e8e\u7eaf\u97f3\u9891\u6a21\u578b\uff0c\u4f46\u5bf9\u65b0\u578b\u566a\u58f0\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "conclusion": "\u591a\u6a21\u6001PTTM\u5728\u566a\u58f0\u73af\u5883\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u9700\u4f9d\u8d56\u7cbe\u786e\u8f6c\u5f55\uff0c\u4e14\u6cdb\u5316\u80fd\u529b\u5f85\u63d0\u5347\u3002", "keywords": "predictive turn-taking models, noise, human-robot interaction, multimodal, accuracy"}}
{"id": "2505.22533", "pdf": "https://arxiv.org/pdf/2505.22533", "abs": "https://arxiv.org/abs/2505.22533", "authors": ["Pallavi Bhardwaj", "Caitlin Jones", "Lasse Dierich", "Aleksandar Vu\u010dkovi\u0107"], "title": "TabularQGAN: A Quantum Generative Model for Tabular Data", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": "18 pages,8 figures and 4 tables", "summary": "In this paper, we introduce a novel quantum generative model for synthesizing\ntabular data. Synthetic data is valuable in scenarios where real-world data is\nscarce or private, it can be used to augment or replace existing datasets.\nReal-world enterprise data is predominantly tabular and heterogeneous, often\ncomprising a mixture of categorical and numerical features, making it highly\nrelevant across various industries such as healthcare, finance, and software.\nWe propose a quantum generative adversarial network architecture with flexible\ndata encoding and a novel quantum circuit ansatz to effectively model tabular\ndata. The proposed approach is tested on the MIMIC III healthcare and Adult\nCensus datasets, with extensive benchmarking against leading classical models,\nCTGAN, and CopulaGAN. Experimental results demonstrate that our quantum model\noutperforms classical models by an average of 8.5% with respect to an overall\nsimilarity score from SDMetrics, while using only 0.072% of the parameters of\nthe classical models. Additionally, we evaluate the generalization capabilities\nof the models using two custom-designed metrics that demonstrate the ability of\nthe proposed quantum model to generate useful and novel samples. To our\nknowledge, this is one of the first demonstrations of a successful quantum\ngenerative model for handling tabular data, indicating that this task could be\nwell-suited to quantum computers.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u91cf\u5b50\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u5408\u6210\u8868\u683c\u6570\u636e\u3002\u8be5\u6a21\u578b\u5728\u7a00\u7f3a\u6216\u79c1\u5bc6\u7684\u771f\u5b9e\u6570\u636e\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u5904\u7406\u6df7\u5408\u5206\u7c7b\u548c\u6570\u503c\u7279\u5f81\u7684\u5f02\u6784\u6570\u636e\u65f6\u3002\u901a\u8fc7\u91cf\u5b50\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u67b6\u6784\u548c\u521b\u65b0\u7684\u91cf\u5b50\u7535\u8def\u8bbe\u8ba1\uff0c\u6a21\u578b\u5728MIMIC III\u548cAdult Census\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u7ecf\u5178\u6a21\u578b\uff0c\u76f8\u4f3c\u5ea6\u5f97\u5206\u5e73\u5747\u63d0\u53478.5%\uff0c\u4e14\u53c2\u6570\u91cf\u4ec5\u5360\u7ecf\u5178\u6a21\u578b\u76840.072%\u3002", "motivation": "\u73b0\u5b9e\u4f01\u4e1a\u6570\u636e\u591a\u4e3a\u8868\u683c\u5f62\u5f0f\u4e14\u5f02\u6784\u6027\u5f3a\uff0c\u800c\u771f\u5b9e\u6570\u636e\u5f80\u5f80\u7a00\u7f3a\u6216\u6d89\u53ca\u9690\u79c1\u3002\u5408\u6210\u6570\u636e\u53ef\u7528\u4e8e\u8865\u5145\u6216\u66ff\u4ee3\u73b0\u6709\u6570\u636e\u96c6\uff0c\u5c24\u5176\u5728\u533b\u7597\u3001\u91d1\u878d\u548c\u8f6f\u4ef6\u9886\u57df\u9700\u6c42\u8feb\u5207\u3002", "method": "\u91c7\u7528\u91cf\u5b50\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff0c\u7ed3\u5408\u7075\u6d3b\u7684\u6570\u636e\u7f16\u7801\u548c\u65b0\u578b\u91cf\u5b50\u7535\u8defansatz\uff0c\u6709\u6548\u5efa\u6a21\u8868\u683c\u6570\u636e\u3002", "result": "\u5728MIMIC III\u548cAdult Census\u6570\u636e\u96c6\u4e0a\uff0c\u91cf\u5b50\u6a21\u578b\u76f8\u4f3c\u5ea6\u5f97\u5206\u5e73\u5747\u6bd4\u7ecf\u5178\u6a21\u578b\uff08\u5982CTGAN\u548cCopulaGAN\uff09\u9ad88.5%\uff0c\u53c2\u6570\u91cf\u4ec5\u4e3a\u540e\u8005\u76840.072%\u3002", "conclusion": "\u91cf\u5b50\u751f\u6210\u6a21\u578b\u5728\u8868\u683c\u6570\u636e\u5408\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8868\u660e\u91cf\u5b50\u8ba1\u7b97\u673a\u53ef\u80fd\u7279\u522b\u9002\u5408\u6b64\u7c7b\u4efb\u52a1\u3002", "keywords": "quantum generative model, tabular data, synthetic data, quantum GAN"}}
{"id": "2505.22146", "pdf": "https://arxiv.org/pdf/2505.22146", "abs": "https://arxiv.org/abs/2505.22146", "authors": ["Guangfu Hao", "Haojie Wen", "Liangxuna Guo", "Yang Chen", "Yanchao Bi", "Shan Yu"], "title": "Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language", "categories": ["cs.CV", "cs.AI", "cs.CL", "q-bio.NC"], "comment": null, "summary": "Flexible tool selection reflects a complex cognitive ability that\ndistinguishes humans from other species, yet computational models that capture\nthis ability remain underdeveloped. We developed a framework using\nlow-dimensional attribute representations to bridge visual tool perception and\nlinguistic task understanding. We constructed a comprehensive dataset (ToolNet)\ncontaining 115 common tools labeled with 13 carefully designed attributes\nspanning physical, functional, and psychological properties, paired with\nnatural language scenarios describing tool usage. Visual encoders (ResNet or\nViT) extract attributes from tool images while fine-tuned language models\n(GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our\napproach achieves 74% accuracy in tool selection tasks-significantly\noutperforming direct tool matching (20%) and smaller multimodal models\n(21%-58%), while approaching performance of much larger models like GPT-4o\n(73%) with substantially fewer parameters. Ablation studies revealed that\nmanipulation-related attributes (graspability, hand-relatedness, elongation)\nconsistently prove most critical across modalities. This work provides a\nparameter-efficient, interpretable solution that mimics human-like tool\ncognition, advancing both cognitive science understanding and practical\napplications in tool selection tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4f4e\u7ef4\u5c5e\u6027\u8868\u793a\u6846\u67b6\uff08ToolNet\uff09\u8fde\u63a5\u89c6\u89c9\u5de5\u5177\u611f\u77e5\u4e0e\u8bed\u8a00\u4efb\u52a1\u7406\u89e3\u7684\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5de5\u5177\u9009\u62e9\u4efb\u52a1\u7684\u51c6\u786e\u7387\uff0c\u63a5\u8fd1\u5927\u578b\u6a21\u578b\u6027\u80fd\u4f46\u53c2\u6570\u66f4\u5c11\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u586b\u8865\u8ba1\u7b97\u6a21\u578b\u5728\u6a21\u62df\u4eba\u7c7b\u590d\u6742\u5de5\u5177\u9009\u62e9\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u4e0e\u8bed\u8a00\u4fe1\u606f\u63d0\u5347\u5de5\u5177\u9009\u62e9\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u91c7\u7528\u89c6\u89c9\u7f16\u7801\u5668\uff08ResNet\u6216ViT\uff09\u4ece\u5de5\u5177\u56fe\u50cf\u63d0\u53d6\u5c5e\u6027\uff0c\u7ed3\u5408\u5fae\u8c03\u7684\u8bed\u8a00\u6a21\u578b\uff08GPT-2\u3001LLaMA\u3001DeepSeek\uff09\u4ece\u4efb\u52a1\u63cf\u8ff0\u4e2d\u83b7\u53d6\u9700\u6c42\u5c5e\u6027\uff0c\u6784\u5efa\u4f4e\u7ef4\u5c5e\u6027\u8868\u793a\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5de5\u5177\u9009\u62e9\u4efb\u52a1\u4e2d\u8fbe\u523074%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u76f4\u63a5\u5339\u914d\uff0820%\uff09\u548c\u5c0f\u578b\u591a\u6a21\u6001\u6a21\u578b\uff0821%-58%\uff09\uff0c\u4e14\u53c2\u6570\u89c4\u6a21\u8fdc\u5c0f\u4e8e\u6027\u80fd\u76f8\u8fd1\u7684GPT-4o\uff0873%\uff09\u3002", "conclusion": "\u901a\u8fc7\u5c5e\u6027\u9a71\u52a8\u7684\u591a\u6a21\u6001\u5efa\u6a21\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u5de5\u5177\u9009\u62e9\uff0c\u4e0d\u4ec5\u63a8\u52a8\u4e86\u8ba4\u77e5\u79d1\u5b66\u7406\u89e3\uff0c\u4e5f\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5de5\u5177\u9009\u62e9, \u591a\u6a21\u6001\u5b66\u4e60, \u8ba4\u77e5\u5efa\u6a21, \u89c6\u89c9-\u8bed\u8a00\u878d\u5408, \u4f4e\u7ef4\u8868\u793a"}}
{"id": "2505.22538", "pdf": "https://arxiv.org/pdf/2505.22538", "abs": "https://arxiv.org/abs/2505.22538", "authors": ["Paul Hofman", "Yusuf Sale", "Eyke H\u00fcllermeier"], "title": "Uncertainty Quantification with Proper Scoring Rules: Adjusting Measures to Prediction Tasks", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We address the problem of uncertainty quantification and propose measures of\ntotal, aleatoric, and epistemic uncertainty based on a known decomposition of\n(strictly) proper scoring rules, a specific type of loss function, into a\ndivergence and an entropy component. This leads to a flexible framework for\nuncertainty quantification that can be instantiated with different losses\n(scoring rules), which makes it possible to tailor uncertainty quantification\nto the use case at hand. We show that this flexibility is indeed advantageous.\nIn particular, we analyze the task of selective prediction and show that the\nscoring rule should ideally match the task loss. In addition, we perform\nexperiments on two other common tasks. For out-of-distribution detection, our\nresults confirm that a widely used measure of epistemic uncertainty, mutual\ninformation, performs best. Moreover, in the setting of active learning, our\nmeasure of epistemic uncertainty based on the zero-one-loss consistently\noutperforms other uncertainty measures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e25\u683c\u8bc4\u5206\u89c4\u5219\u5206\u89e3\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u53ef\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u9700\u6c42\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u9009\u62e9\u6027\u9884\u6d4b\u3001\u5206\u5e03\u5916\u68c0\u6d4b\u548c\u4e3b\u52a8\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u89e3\u51b3\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u4e2a\u7075\u6d3b\u6846\u67b6\u4ee5\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u9700\u6c42\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "method": "\u57fa\u4e8e\u4e25\u683c\u8bc4\u5206\u89c4\u5219\u7684\u5206\u89e3\uff08\u5206\u6b67\u548c\u71b5\u5206\u91cf\uff09\uff0c\u6784\u5efa\u603b\u3001\u5076\u7136\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u5ea6\u91cf\u6846\u67b6\u3002", "result": "\u5728\u9009\u62e9\u6027\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u5339\u914d\u4efb\u52a1\u635f\u5931\u7684\u8bc4\u5206\u89c4\u5219\u6548\u679c\u6700\u4f73\uff1b\u5206\u5e03\u5916\u68c0\u6d4b\u4efb\u52a1\u4e2d\u4e92\u4fe1\u606f\u8868\u73b0\u6700\u4f18\uff1b\u4e3b\u52a8\u5b66\u4e60\u4e2d\u57fa\u4e8e\u96f6\u4e00\u635f\u5931\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u7075\u6d3b\u6027\u6846\u67b6\u80fd\u6709\u6548\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u8d8a\u6027\u3002", "keywords": "\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3001\u8bc4\u5206\u89c4\u5219\u3001\u9009\u62e9\u6027\u9884\u6d4b\u3001\u5206\u5e03\u5916\u68c0\u6d4b\u3001\u4e3b\u52a8\u5b66\u4e60"}}
{"id": "2505.21873", "pdf": "https://arxiv.org/pdf/2505.21873", "abs": "https://arxiv.org/abs/2505.21873", "authors": ["Jie Gao", "Jun Li", "Jing Hu", "Shanzhuo Zhang", "Kunrui Zhu", "Yueyang Huang", "Xiaonan Zhang", "Xiaomin Fang"], "title": "HelixDesign-Binder: A Scalable Production-Grade Platform for Binder Design Built on HelixFold3", "categories": ["q-bio.BM", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Protein binder design is central to therapeutics, diagnostics, and synthetic\nbiology, yet practical deployment remains challenging due to fragmented\nworkflows, high computational costs, and complex tool integration. We present\nHelixDesign-Binder, a production-grade, high-throughput platform built on\nHelixFold3 that automates the full binder design pipeline, from backbone\ngeneration and sequence design to structural evaluation and multi-dimensional\nscoring. By unifying these stages into a scalable and user-friendly system,\nHelixDesign-Binder enables efficient exploration of binder candidates with\nfavorable structural, energetic, and physicochemical properties. The platform\nleverages Baidu Cloud's high-performance infrastructure to support large-scale\ndesign and incorporates advanced scoring metrics, including ipTM, predicted\nbinding free energy, and interface hydrophobicity. Benchmarking across six\nprotein targets demonstrates that HelixDesign-Binder reliably produces diverse\nand high-quality binders, some of which match or exceed validated designs in\npredicted binding affinity. HelixDesign-Binder is accessible via an interactive\nweb interface in PaddleHelix platform, supporting both academic research and\nindustrial applications in antibody and protein binder development.", "AI": {"tldr": "HelixDesign-Binder\u662f\u4e00\u4e2a\u57fa\u4e8eHelixFold3\u7684\u9ad8\u901a\u91cf\u86cb\u767d\u8d28\u7ed3\u5408\u5242\u8bbe\u8ba1\u5e73\u53f0\uff0c\u6574\u5408\u4e86\u4ece\u9aa8\u67b6\u751f\u6210\u5230\u591a\u7ef4\u8bc4\u5206\u7684\u5168\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bbe\u8ba1\u6548\u7387\u548c\u6210\u679c\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u7684\u86cb\u767d\u8d28\u7ed3\u5408\u5242\u8bbe\u8ba1\u6d41\u7a0b\u5206\u6563\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5de5\u5177\u96c6\u6210\u590d\u6742\uff0c\u4e9f\u9700\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u7528\u6237\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5e73\u53f0\u57fa\u4e8eHelixFold3\uff0c\u7ed3\u5408Baidu Cloud\u7684\u9ad8\u6027\u80fd\u8ba1\u7b97\u8d44\u6e90\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u7a0b\u5b9e\u73b0\u9aa8\u67b6\u751f\u6210\u3001\u5e8f\u5217\u8bbe\u8ba1\u3001\u7ed3\u6784\u8bc4\u4f30\u548c\u591a\u7ef4\u8bc4\u5206\u3002", "result": "\u5728\u516d\u79cd\u86cb\u767d\u8d28\u9776\u6807\u4e0a\u6d4b\u8bd5\u663e\u793a\uff0c\u5e73\u53f0\u80fd\u751f\u6210\u591a\u6837\u4e14\u9ad8\u8d28\u91cf\u7684\u7ed3\u5408\u5242\uff0c\u90e8\u5206\u8bbe\u8ba1\u5728\u9884\u6d4b\u7ed3\u5408\u4eb2\u548c\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u9a8c\u8bc1\u65b9\u6848\u3002", "conclusion": "HelixDesign-Binder\u4e3a\u6297\u4f53\u548c\u86cb\u767d\u8d28\u7ed3\u5408\u5242\u5f00\u53d1\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\uff0c\u652f\u6301\u5b66\u672f\u548c\u5de5\u4e1a\u5e94\u7528\u3002", "keywords": "\u86cb\u767d\u8d28\u7ed3\u5408\u5242\u3001HelixFold3\u3001\u9ad8\u901a\u91cf\u3001Baidu Cloud\u3001\u591a\u7ef4\u5ea6\u8bc4\u5206"}}
{"id": "2505.22150", "pdf": "https://arxiv.org/pdf/2505.22150", "abs": "https://arxiv.org/abs/2505.22150", "authors": ["Runze Xia", "Shuo Feng", "Renzhi Wang", "Congchi Yin", "Xuyun Wen", "Piji Li"], "title": "Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging", "categories": ["cs.CV", "cs.CL"], "comment": "CogSci2025", "summary": "Brain-to-Image reconstruction aims to recover visual stimuli perceived by\nhumans from brain activity. However, the reconstructed visual stimuli often\nmissing details and semantic inconsistencies, which may be attributed to\ninsufficient semantic information. To address this issue, we propose an\napproach named Fine-grained Brain-to-Image reconstruction (FgB2I), which\nemploys fine-grained text as bridge to improve image reconstruction. FgB2I\ncomprises three key stages: detail enhancement, decoding fine-grained text\ndescriptions, and text-bridged brain-to-image reconstruction. In the\ndetail-enhancement stage, we leverage large vision-language models to generate\nfine-grained captions for visual stimuli and experimentally validate its\nimportance. We propose three reward metrics (object accuracy, text-image\nsemantic similarity, and image-image semantic similarity) to guide the language\nmodel in decoding fine-grained text descriptions from fMRI signals. The\nfine-grained text descriptions can be integrated into existing reconstruction\nmethods to achieve fine-grained Brain-to-Image reconstruction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFgB2I\u7684\u7ec6\u7c92\u5ea6\u8111\u5230\u56fe\u50cf\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u7ec6\u7c92\u5ea6\u6587\u672c\u4f5c\u4e3a\u6865\u6881\uff0c\u63d0\u5347\u56fe\u50cf\u91cd\u5efa\u7684\u7ec6\u8282\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u8111\u5230\u56fe\u50cf\u91cd\u5efa\u65b9\u6cd5\u5e38\u56e0\u7f3a\u4e4f\u8db3\u591f\u7684\u8bed\u4e49\u4fe1\u606f\u800c\u4e22\u5931\u7ec6\u8282\u6216\u8bed\u4e49\u4e00\u81f4\u6027\u5dee\u3002", "method": "FgB2I\u5206\u4e3a\u4e09\u4e2a\u5173\u952e\u9636\u6bb5\uff1a\u7ec6\u8282\u589e\u5f3a\u3001\u7ec6\u7c92\u5ea6\u6587\u672c\u63cf\u8ff0\u89e3\u7801\u53ca\u57fa\u4e8e\u6587\u672c\u7684\u8111\u5230\u56fe\u50cf\u91cd\u5efa\u3002\u5229\u7528\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7ec6\u7c92\u5ea6\u6807\u9898\uff0c\u5e76\u901a\u8fc7\u4e09\u4e2a\u5956\u52b1\u6307\u6807\uff08\u5bf9\u8c61\u51c6\u786e\u6027\u3001\u6587\u672c-\u56fe\u50cf\u8bed\u4e49\u76f8\u4f3c\u6027\u3001\u56fe\u50cf-\u56fe\u50cf\u8bed\u4e49\u76f8\u4f3c\u6027\uff09\u6307\u5bfc\u89e3\u7801\u3002", "result": "\u7ec6\u7c92\u5ea6\u6587\u672c\u63cf\u8ff0\u53ef\u6574\u5408\u5230\u73b0\u6709\u91cd\u5efa\u65b9\u6cd5\u4e2d\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u8111\u5230\u56fe\u50cf\u91cd\u5efa\u3002", "conclusion": "FgB2I\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u56fe\u50cf\u7684\u7ec6\u8282\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "keywords": "\u8111\u5230\u56fe\u50cf\u91cd\u5efa, \u7ec6\u7c92\u5ea6\u6587\u672c, \u8bed\u4e49\u4e00\u81f4\u6027, \u89c6\u89c9\u8bed\u8a00\u6a21\u578b, fMRI"}}
{"id": "2505.22541", "pdf": "https://arxiv.org/pdf/2505.22541", "abs": "https://arxiv.org/abs/2505.22541", "authors": ["Vinitra Swamy"], "title": "A Human-Centric Approach to Explainable AI for Personalized Education", "categories": ["cs.LG", "cs.CY"], "comment": "PhD Thesis, EPFL (Computer Science)", "summary": "Deep neural networks form the backbone of artificial intelligence research,\nwith potential to transform the human experience in areas ranging from\nautonomous driving to personal assistants, healthcare to education. However,\ntheir integration into the daily routines of real-world classrooms remains\nlimited. It is not yet common for a teacher to assign students individualized\nhomework targeting their specific weaknesses, provide students with instant\nfeedback, or simulate student responses to a new exam question. While these\nmodels excel in predictive performance, this lack of adoption can be attributed\nto a significant weakness: the lack of explainability of model decisions,\nleading to a lack of trust from students, parents, and teachers. This thesis\naims to bring human needs to the forefront of eXplainable AI (XAI) research,\ngrounded in the concrete use case of personalized learning and teaching. We\nframe the contributions along two verticals: technical advances in XAI and\ntheir aligned human studies. We investigate explainability in AI for education,\nrevealing systematic disagreements between post-hoc explainers and identifying\na need for inherently interpretable model architectures. We propose four novel\ntechnical contributions in interpretability with a multimodal modular\narchitecture (MultiModN), an interpretable mixture-of-experts model\n(InterpretCC), adversarial training for explainer stability, and a\ntheory-driven LLM-XAI framework to present explanations to students\n(iLLuMinaTE), which we evaluate in diverse settings with professors, teachers,\nlearning scientists, and university students. By combining empirical\nevaluations of existing explainers with novel architectural designs and human\nstudies, our work lays a foundation for human-centric AI systems that balance\nstate-of-the-art performance with built-in transparency and trust.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u56db\u79cd\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u7684\u6280\u672f\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u4eba\u7c7b\u7814\u7a76\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "motivation": "\u76ee\u524d\u6df1\u5ea6\u5b66\u4e60\u5728\u6559\u80b2\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6709\u9650\uff0c\u4e3b\u8981\u56e0\u4e3a\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u5bfc\u81f4\u5e08\u751f\u4fe1\u4efb\u4e0d\u8db3\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u53ef\u89e3\u91caAI\uff08XAI\uff09\u6280\u672f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u591a\u6a21\u6001\u6a21\u5757\u5316\u67b6\u6784\uff08MultiModN\uff09\u3001\u53ef\u89e3\u91ca\u7684\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff08InterpretCC\uff09\u3001\u5bf9\u6297\u8bad\u7ec3\u548c\u7406\u8bba\u9a71\u52a8\u7684LLM-XAI\u6846\u67b6\uff08iLLuMinaTE\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u8bc4\u4f30\u548c\u4eba\u7c7b\u7814\u7a76\u9a8c\u8bc1\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u79cd\u6559\u80b2\u573a\u666f\u4e2d\u9a8c\u8bc1\u6709\u6548\uff0c\u5e73\u8861\u4e86\u6700\u65b0\u6280\u672f\u6027\u80fd\u548c\u5185\u5728\u900f\u660e\u5ea6\u3002", "conclusion": "\u8bba\u6587\u4e3a\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684AI\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u7ed3\u5408\u6280\u672f\u8fdb\u5c55\u4e0e\u4eba\u7c7b\u9700\u6c42\uff0c\u63a8\u52a8\u53ef\u89e3\u91caAI\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u3002", "keywords": "\u53ef\u89e3\u91caAI, \u6559\u80b2, \u591a\u6a21\u6001, \u4eba\u7c7b\u7814\u7a76, \u6df1\u5ea6\u5b66\u4e60"}}
{"id": "2505.21876", "pdf": "https://arxiv.org/pdf/2505.21876", "abs": "https://arxiv.org/abs/2505.21876", "authors": ["Zun Wang", "Jaemin Cho", "Jialu Li", "Han Lin", "Jaehong Yoon", "Yue Zhang", "Mohit Bansal"], "title": "EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video Guidance", "categories": ["cs.CV", "cs.AI"], "comment": "Project website: https://zunwang1.github.io/Epic", "summary": "Recent approaches on 3D camera control in video diffusion models (VDMs) often\ncreate anchor videos to guide diffusion models as a structured prior by\nrendering from estimated point clouds following annotated camera trajectories.\nHowever, errors inherent in point cloud estimation often lead to inaccurate\nanchor videos. Moreover, the requirement for extensive camera trajectory\nannotations further increases resource demands. To address these limitations,\nwe introduce EPiC, an efficient and precise camera control learning framework\nthat automatically constructs high-quality anchor videos without expensive\ncamera trajectory annotations. Concretely, we create highly precise anchor\nvideos for training by masking source videos based on first-frame visibility.\nThis approach ensures high alignment, eliminates the need for camera trajectory\nannotations, and thus can be readily applied to any in-the-wild video to\ngenerate image-to-video (I2V) training pairs. Furthermore, we introduce\nAnchor-ControlNet, a lightweight conditioning module that integrates anchor\nvideo guidance in visible regions to pretrained VDMs, with less than 1% of\nbackbone model parameters. By combining the proposed anchor video data and\nControlNet module, EPiC achieves efficient training with substantially fewer\nparameters, training steps, and less data, without requiring modifications to\nthe diffusion model backbone typically needed to mitigate rendering\nmisalignments. Although being trained on masking-based anchor videos, our\nmethod generalizes robustly to anchor videos made with point clouds during\ninference, enabling precise 3D-informed camera control. EPiC achieves SOTA\nperformance on RealEstate10K and MiraData for I2V camera control task,\ndemonstrating precise and robust camera control ability both quantitatively and\nqualitatively. Notably, EPiC also exhibits strong zero-shot generalization to\nvideo-to-video scenarios.", "AI": {"tldr": "EPiC\u662f\u4e00\u79cd\u9ad8\u6548\u76843D\u76f8\u673a\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u7b2c\u4e00\u5e27\u53ef\u89c1\u6027\u7684\u6e90\u89c6\u9891\u63a9\u7801\u81ea\u52a8\u6784\u5efa\u9ad8\u8d28\u91cf\u951a\u89c6\u9891\uff0c\u65e0\u9700\u6602\u8d35\u76f8\u673a\u8f68\u8ff9\u6807\u6ce8\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7Anchor-ControlNet\u6a21\u5757\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u53c2\u6570\u548c\u6570\u636e\u9700\u6c42\uff0c\u5b9e\u73b0\u7cbe\u786e\u76f8\u673a\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u70b9\u4e91\u4f30\u8ba1\u548c\u76f8\u673a\u8f68\u8ff9\u6807\u6ce8\uff0c\u6613\u56e0\u4f30\u8ba1\u8bef\u5dee\u5bfc\u81f4\u951a\u89c6\u9891\u4e0d\u51c6\u786e\u4e14\u8d44\u6e90\u6d88\u8017\u5927\uff0cEPiC\u65e8\u5728\u901a\u8fc7\u65e0\u6807\u6ce8\u7684\u9ad8\u8d28\u91cf\u951a\u89c6\u9891\u548c\u8f7b\u91cf\u6a21\u5757\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5229\u7528\u7b2c\u4e00\u5e27\u53ef\u89c1\u6027\u63a9\u7801\u751f\u6210\u7cbe\u786e\u951a\u89c6\u9891\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u8bbe\u8ba1Anchor-ControlNet\u6a21\u5757\u96c6\u6210\u951a\u89c6\u9891\u5f15\u5bfc\u81f3\u9884\u8bad\u7ec3VDMs\uff0c\u4ec5\u97001%\u4e3b\u5e72\u53c2\u6570\u3002", "result": "\u5728RealEstate10K\u548cMiraData\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u5b9a\u91cf\u5b9a\u6027\u5747\u5c55\u793a\u7cbe\u786e\u76f8\u673a\u63a7\u5236\u80fd\u529b\uff0c\u5e76\u5177\u5907\u96f6\u6837\u672c\u6cdb\u5316\u5230\u89c6\u9891\u5230\u89c6\u9891\u4efb\u52a1\u7684\u80fd\u529b\u3002", "conclusion": "EPiC\u4ee5\u9ad8\u6548\u6570\u636e\u548c\u6a21\u578b\u8bbe\u8ba1\u514b\u670d\u6e32\u67d3\u9519\u4f4d\u95ee\u9898\uff0c\u4e3a3D\u76f8\u673a\u63a7\u5236\u63d0\u4f9b\u4f4e\u6210\u672c\u3001\u9ad8\u7cbe\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "3D\u76f8\u673a\u63a7\u5236, \u89c6\u9891\u6269\u6563\u6a21\u578b, \u951a\u89c6\u9891, Anchor-ControlNet, \u96f6\u6837\u672c\u6cdb\u5316"}}
{"id": "2505.22549", "pdf": "https://arxiv.org/pdf/2505.22549", "abs": "https://arxiv.org/abs/2505.22549", "authors": ["Alex Iacob", "Lorenzo Sani", "Mher Safaryan", "Paris Giampouras", "Samuel Horv\u00e1th", "Andrej Jovanovic", "Meghdad Kurmanji", "Preslav Aleksandrov", "William F. Shen", "Xinchi Qiu", "Nicholas D. Lane"], "title": "DES-LOC: Desynced Low Communication Adaptive Optimizers for Training Foundation Models", "categories": ["cs.LG"], "comment": "Keywords: Distributed Training, Foundation Models, Large Language\n  Models, Optimizers, Communication Efficiency, Federated Learning, Distributed\n  Systems, Optimization Theory, Scaling, Robustness. Preprint, under review at\n  NeurIPS", "summary": "Scaling foundation model training with Distributed Data Parallel (DDP)\nmethods is bandwidth-limited. Existing infrequent communication methods like\nLocal SGD were designed to synchronize only model parameters and cannot be\ntrivially applied to adaptive optimizers due to additional optimizer states.\nCurrent approaches extending Local SGD either lack convergence guarantees or\nrequire synchronizing all optimizer states, tripling communication costs. We\npropose Desynced Low Communication Adaptive Optimizers (DES-LOC), a family of\noptimizers assigning independent synchronization periods to parameters and\nmomenta, enabling lower communication costs while preserving convergence.\nThrough extensive experiments on language models of up to 1.7B, we show that\nDES-LOC can communicate 170x less than DDP and 2x less than the previous\nstate-of-the-art Local ADAM. Furthermore, unlike previous heuristic approaches,\nDES-LOC is suited for practical training scenarios prone to system failures.\nDES-LOC offers a scalable, bandwidth-efficient, and fault-tolerant solution for\nfoundation model training.", "AI": {"tldr": "DES-LOC \u662f\u4e00\u79cd\u5206\u5e03\u5f0f\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u72ec\u7acb\u540c\u6b65\u53c2\u6570\u548c\u52a8\u91cf\u6765\u964d\u4f4e\u901a\u4fe1\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6536\u655b\u6027\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u5206\u5e03\u5f0f\u6570\u636e\u5e76\u884c\uff08DDP\uff09\u65b9\u6cd5\u7684\u5e26\u5bbd\u9650\u5236\u53ca Local SGD \u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u81ea\u9002\u5e94\u4f18\u5316\u5668\u7684\u95ee\u9898\u63a8\u52a8\u4e86 DES-LOC \u7684\u63d0\u51fa\u3002", "method": "DES-LOC \u4e3a\u53c2\u6570\u548c\u52a8\u91cf\u5206\u914d\u72ec\u7acb\u7684\u540c\u6b65\u5468\u671f\uff0c\u51cf\u5c11\u901a\u4fe1\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u8bc1\u6536\u655b\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDES-LOC \u6bd4 DDP \u51cf\u5c11 170 \u500d\u901a\u4fe1\u91cf\uff0c\u6bd4 Local ADAM \u51cf\u5c11 2 \u500d\uff0c\u4e14\u5177\u6709\u5bb9\u9519\u80fd\u529b\u3002", "conclusion": "DES-LOC \u662f\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u5e26\u5bbd\u9ad8\u6548\u4e14\u5bb9\u9519\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u3002", "keywords": "\u5206\u5e03\u5f0f\u8bad\u7ec3, \u81ea\u9002\u5e94\u4f18\u5316\u5668, \u901a\u4fe1\u6548\u7387, \u6536\u655b\u6027, \u5bb9\u9519\u6027"}}
{"id": "2505.21879", "pdf": "https://arxiv.org/pdf/2505.21879", "abs": "https://arxiv.org/abs/2505.21879", "authors": ["Weiting Liu", "Jiaxu Cui", "Jiao Hu", "En Wang", "Bo Yang"], "title": "Symbolic Foundation Regressor on Complex Networks", "categories": ["cs.SC", "cs.AI", "cs.LG"], "comment": "60 pages", "summary": "In science, we are interested not only in forecasting but also in\nunderstanding how predictions are made, specifically what the interpretable\nunderlying model looks like. Data-driven machine learning technology can\nsignificantly streamline the complex and time-consuming traditional manual\nprocess of discovering scientific laws, helping us gain insights into\nfundamental issues in modern science. In this work, we introduce a pre-trained\nsymbolic foundation regressor that can effectively compress complex data with\nnumerous interacting variables while producing interpretable physical\nrepresentations. Our model has been rigorously tested on non-network symbolic\nregression, symbolic regression on complex networks, and the inference of\nnetwork dynamics across various domains, including physics, biochemistry,\necology, and epidemiology. The results indicate a remarkable improvement in\nequation inference efficiency, being three times more effective than baseline\napproaches while maintaining accurate predictions. Furthermore, we apply our\nmodel to uncover more intuitive laws of interaction transmission from global\nepidemic outbreak data, achieving optimal data fitting. This model extends the\napplication boundary of pre-trained symbolic regression models to complex\nnetworks, and we believe it provides a foundational solution for revealing the\nhidden mechanisms behind changes in complex phenomena, enhancing\ninterpretability, and inspiring further scientific discoveries.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u8bad\u7ec3\u7684\u7b26\u53f7\u57fa\u7840\u56de\u5f52\u5668\uff0c\u80fd\u9ad8\u6548\u538b\u7f29\u590d\u6742\u6570\u636e\u5e76\u751f\u6210\u53ef\u89e3\u91ca\u7684\u7269\u7406\u8868\u793a\uff0c\u5728\u591a\u79cd\u9886\u57df\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6548\u7387\u63d0\u5347\u4e09\u500d\u3002", "motivation": "\u79d1\u5b66\u4e2d\u4e0d\u4ec5\u9700\u8981\u9884\u6d4b\uff0c\u8fd8\u9700\u7406\u89e3\u9884\u6d4b\u80cc\u540e\u7684\u53ef\u89e3\u91ca\u6a21\u578b\u3002\u4f20\u7edf\u624b\u52a8\u53d1\u73b0\u79d1\u5b66\u5b9a\u5f8b\u7684\u8fc7\u7a0b\u590d\u6742\u4e14\u8017\u65f6\uff0c\u5e0c\u671b\u501f\u52a9\u673a\u5668\u5b66\u4e60\u7b80\u5316\u3002", "method": "\u5f15\u5165\u9884\u8bad\u7ec3\u7684\u7b26\u53f7\u57fa\u7840\u56de\u5f52\u5668\uff0c\u538b\u7f29\u591a\u53d8\u91cf\u4ea4\u4e92\u7684\u590d\u6742\u6570\u636e\u5e76\u751f\u6210\u53ef\u89e3\u91ca\u7684\u7269\u7406\u8868\u793a\uff0c\u5728\u7b26\u53f7\u56de\u5f52\u548c\u7f51\u7edc\u52a8\u529b\u5b66\u63a8\u65ad\u4e2d\u6d4b\u8bd5\u3002", "result": "\u65b9\u7a0b\u63a8\u65ad\u6548\u7387\u63d0\u5347\u4e09\u500d\uff0c\u9884\u6d4b\u51c6\u786e\u4e14\u80fd\u62df\u5408\u6d41\u884c\u75c5\u6570\u636e\uff0c\u63ed\u793a\u4e86\u76f4\u89c2\u7684\u4ea4\u4e92\u4f20\u64ad\u5b9a\u5f8b\u3002", "conclusion": "\u8be5\u6a21\u578b\u6269\u5c55\u4e86\u9884\u8bad\u7ec3\u7b26\u53f7\u56de\u5f52\u7684\u5e94\u7528\u8303\u56f4\uff0c\u4e3a\u63ed\u793a\u590d\u6742\u73b0\u8c61\u80cc\u540e\u7684\u9690\u85cf\u673a\u5236\u63d0\u4f9b\u4e86\u57fa\u7840\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u7b26\u53f7\u56de\u5f52, \u590d\u6742\u7f51\u7edc, \u53ef\u89e3\u91ca\u6027, \u673a\u5668\u5b66\u4e60, \u79d1\u5b66\u5b9a\u5f8b"}}
{"id": "2505.22222", "pdf": "https://arxiv.org/pdf/2505.22222", "abs": "https://arxiv.org/abs/2505.22222", "authors": ["Yunsoo Kim", "Jinge Wu", "Su-Hwan Kim", "Pardeep Vasudev", "Jiashu Shen", "Honghan Wu"], "title": "Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advancements in multimodal Large Language Models (LLMs) have\nsignificantly enhanced the automation of medical image analysis, particularly\nin generating radiology reports from chest X-rays (CXR). However, these models\nstill suffer from hallucinations and clinically significant errors, limiting\ntheir reliability in real-world applications. In this study, we propose Look &\nMark (L&M), a novel grounding fixation strategy that integrates radiologist eye\nfixations (Look) and bounding box annotations (Mark) into the LLM prompting\nframework. Unlike conventional fine-tuning, L&M leverages in-context learning\nto achieve substantial performance gains without retraining. When evaluated\nacross multiple domain-specific and general-purpose models, L&M demonstrates\nsignificant gains, including a 1.2% improvement in overall metrics (A.AVG) for\nCXR-LLaVA compared to baseline prompting and a remarkable 9.2% boost for\nLLaVA-Med. General-purpose models also benefit from L&M combined with\nin-context learning, with LLaVA-OV achieving an 87.3% clinical average\nperformance (C.AVG)-the highest among all models, even surpassing those\nexplicitly trained for CXR report generation. Expert evaluations further\nconfirm that L&M reduces clinically significant errors (by 0.43 average errors\nper report), such as false predictions and omissions, enhancing both accuracy\nand reliability. These findings highlight L&M's potential as a scalable and\nefficient solution for AI-assisted radiology, paving the way for improved\ndiagnostic workflows in low-resource clinical settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLook & Mark (L&M)\u7b56\u7565\uff0c\u901a\u8fc7\u6574\u5408\u653e\u5c04\u79d1\u533b\u751f\u7684\u89c6\u89c9\u6ce8\u89c6\u548c\u6807\u6ce8\u6846\u6765\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u80f8\u90e8X\u5149\u7247\u62a5\u544a\u751f\u6210\u4e2d\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u51cf\u5c11\u5e7b\u89c9\u548c\u4e34\u5e8a\u9519\u8bef\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u5b58\u5728\u5e7b\u89c9\u548c\u4e34\u5e8a\u9519\u8bef\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528L&M\u7b56\u7565\uff0c\u7ed3\u5408\u653e\u5c04\u79d1\u533b\u751f\u7684\u89c6\u89c9\u6ce8\u89c6\uff08Look\uff09\u548c\u6807\u6ce8\u6846\uff08Mark\uff09\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "L&M\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5982CXR-LLaVA\u6574\u4f53\u6307\u6807\u63d0\u53471.2%\uff0cLLaVA-Med\u63d0\u53479.2%\uff0c\u5e76\u80fd\u51cf\u5c11\u4e34\u5e8a\u9519\u8bef\uff08\u5e73\u5747\u6bcf\u4efd\u62a5\u544a\u51cf\u5c110.43\u4e2a\u9519\u8bef\uff09\u3002", "conclusion": "L&M\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63d0\u5347AI\u8f85\u52a9\u653e\u5c04\u5b66\u7684\u8bca\u65ad\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u7684\u4e34\u5e8a\u73af\u5883\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u80f8\u90e8X\u5149\u7247\u3001\u653e\u5c04\u5b66\u62a5\u544a\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u5e7b\u89c9\u51cf\u5c11"}}
{"id": "2505.22560", "pdf": "https://arxiv.org/pdf/2505.22560", "abs": "https://arxiv.org/abs/2505.22560", "authors": ["Artem Moskalev", "Mangal Prakash", "Junjie Xu", "Tianyu Cui", "Rui Liao", "Tommaso Mansi"], "title": "Geometric Hyena Networks for Large-scale Equivariant Learning", "categories": ["cs.LG"], "comment": null, "summary": "Processing global geometric context while preserving equivariance is crucial\nwhen modeling biological, chemical, and physical systems. Yet, this is\nchallenging due to the computational demands of equivariance and global context\nat scale. Standard methods such as equivariant self-attention suffer from\nquadratic complexity, while local methods such as distance-based message\npassing sacrifice global information. Inspired by the recent success of\nstate-space and long-convolutional models, we introduce Geometric Hyena, the\nfirst equivariant long-convolutional model for geometric systems. Geometric\nHyena captures global geometric context at sub-quadratic complexity while\nmaintaining equivariance to rotations and translations. Evaluated on all-atom\nproperty prediction of large RNA molecules and full protein molecular dynamics,\nGeometric Hyena outperforms existing equivariant models while requiring\nsignificantly less memory and compute that equivariant self-attention. Notably,\nour model processes the geometric context of 30k tokens 20x faster than the\nequivariant transformer and allows 72x longer context within the same budget.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGeometric Hyena\u7684\u65b0\u6a21\u578b\uff0c\u9996\u6b21\u5c06\u957f\u5377\u79ef\u4e0e\u7b49\u53d8\u6027\u7ed3\u5408\u7528\u4e8e\u51e0\u4f55\u7cfb\u7edf\u5efa\u6a21\uff0c\u4ee5\u6b21\u4e8c\u6b21\u590d\u6742\u5ea6\u5904\u7406\u5168\u5c40\u51e0\u4f55\u4fe1\u606f\uff0c\u5e76\u5728RNA\u5206\u5b50\u548c\u86cb\u767d\u8d28\u52a8\u529b\u5b66\u9884\u6d4b\u4e2d\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5904\u7406\u5168\u5c40\u51e0\u4f55\u4fe1\u606f\u65f6\u4fdd\u6301\u7b49\u53d8\u6027\u5bf9\u751f\u7269\u3001\u5316\u5b66\u548c\u7269\u7406\u7cfb\u7edf\u5efa\u6a21\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u7b49\u53d8\u81ea\u6ce8\u610f\u529b\uff09\u8ba1\u7b97\u590d\u6742\u5ea6\u8fc7\u9ad8\uff0c\u800c\u5c40\u90e8\u65b9\u6cd5\uff08\u5982\u8ddd\u79bb\u6d88\u606f\u4f20\u9012\uff09\u4f1a\u727a\u7272\u5168\u5c40\u4fe1\u606f\u3002", "method": "\u5f15\u5165Geometric Hyena\uff0c\u4e00\u79cd\u7ed3\u5408\u72b6\u6001\u7a7a\u95f4\u548c\u957f\u5377\u79ef\u7684\u7b49\u53d8\u6027\u957f\u5377\u79ef\u6a21\u578b\uff0c\u80fd\u591f\u5728\u6b21\u4e8c\u6b21\u590d\u6742\u5ea6\u4e0b\u5904\u7406\u5168\u5c40\u51e0\u4f55\u4fe1\u606f\u5e76\u4fdd\u6301\u65cb\u8f6c\u548c\u5e73\u79fb\u7b49\u53d8\u6027\u3002", "result": "\u5728\u5927RNA\u5206\u5b50\u548c\u5168\u86cb\u767d\u8d28\u5206\u5b50\u52a8\u529b\u5b66\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cGeometric Hyena\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7b49\u53d8\u6a21\u578b\uff0c\u8ba1\u7b97\u6548\u7387\u663e\u8457\u63d0\u5347\uff08\u4f8b\u5982\u5904\u74063\u4e07token\u65f6\u901f\u5ea6\u5feb20\u500d\uff09\u3002", "conclusion": "Geometric Hyena\u4e3a\u51e0\u4f55\u7cfb\u7edf\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u663e\u8457\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u4f18\u52bf\u3002", "keywords": "\u7b49\u53d8\u6027, \u957f\u5377\u79ef, \u51e0\u4f55\u5efa\u6a21, RNA, \u86cb\u767d\u8d28\u52a8\u529b\u5b66"}}
{"id": "2505.22231", "pdf": "https://arxiv.org/pdf/2505.22231", "abs": "https://arxiv.org/abs/2505.22231", "authors": ["Stefan Bleeck"], "title": "Advancing Hearing Assessment: An ASR-Based Frequency-Specific Speech Test for Diagnosing Presbycusis", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Traditional audiometry often fails to fully characterize the functional\nimpact of hearing loss on speech understanding, particularly supra-threshold\ndeficits and frequency-specific perception challenges in conditions like\npresbycusis. This paper presents the development and simulated evaluation of a\nnovel Automatic Speech Recognition (ASR)-based frequency-specific speech test\ndesigned to provide granular diagnostic insights. Our approach leverages ASR to\nsimulate the perceptual effects of moderate sloping hearing loss by processing\nspeech stimuli under controlled acoustic degradation and subsequently analyzing\nphoneme-level confusion patterns. Key findings indicate that simulated hearing\nloss introduces specific phoneme confusions, predominantly affecting\nhigh-frequency consonants (e.g., alveolar/palatal to labiodental substitutions)\nand leading to significant phoneme deletions, consistent with the acoustic cues\ndegraded in presbycusis. A test battery curated from these ASR-derived\nconfusions demonstrated diagnostic value, effectively differentiating between\nsimulated normal-hearing and hearing-impaired listeners in a comprehensive\nsimulation. This ASR-driven methodology offers a promising avenue for\ndeveloping objective, granular, and frequency-specific hearing assessment tools\nthat complement traditional audiometry. Future work will focus on validating\nthese findings with human participants and exploring the integration of\nadvanced AI models for enhanced diagnostic precision.", "AI": {"tldr": "\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7684\u9891\u7387\u7279\u5f02\u6027\u8bed\u97f3\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bca\u65ad\u542c\u529b\u635f\u5931\u4e2d\u7684\u9ad8\u9891\u8bed\u97f3\u611f\u77e5\u95ee\u9898\u3002\u901a\u8fc7\u6a21\u62df\u542c\u529b\u635f\u5931\uff0c\u5206\u6790\u97f3\u7d20\u6df7\u6dc6\u6a21\u5f0f\uff0c\u6d4b\u8bd5\u8868\u73b0\u51fa\u8bca\u65ad\u4ef7\u503c\uff0c\u672a\u6765\u5c06\u9a8c\u8bc1\u4e8e\u4eba\u7c7b\u53d7\u8bd5\u8005\u3002", "motivation": "\u4f20\u7edf\u542c\u529b\u6d4b\u8bd5\u96be\u4ee5\u5168\u9762\u8bc4\u4f30\u542c\u529b\u635f\u5931\u5bf9\u8bed\u97f3\u7406\u89e3\u7684\u529f\u80fd\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u8001\u5e74\u6027\u8033\u804b\u7b49\u60c5\u51b5\u4e0b\u3002", "method": "\u5229\u7528ASR\u6a21\u62df\u542c\u529b\u635f\u5931\uff0c\u5206\u6790\u97f3\u7d20\u6df7\u6dc6\u6a21\u5f0f\uff0c\u5f00\u53d1\u6d4b\u8bd5\u7535\u6c60\u3002", "result": "\u6a21\u62df\u542c\u529b\u635f\u5931\u5bfc\u81f4\u7279\u5b9a\u97f3\u7d20\u6df7\u6dc6\u548c\u5220\u9664\uff0c\u6d4b\u8bd5\u80fd\u533a\u5206\u6b63\u5e38\u542c\u529b\u4e0e\u542c\u529b\u53d7\u635f\u8005\u3002", "conclusion": "ASR\u65b9\u6cd5\u4e3a\u5f00\u53d1\u5ba2\u89c2\u3001\u7cbe\u51c6\u7684\u542c\u529b\u8bc4\u4f30\u5de5\u5177\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b, \u9891\u7387\u7279\u5f02\u6027, \u542c\u529b\u635f\u5931, \u8001\u5e74\u6027\u8033\u804b, \u97f3\u7d20\u6df7\u6dc6"}}
{"id": "2505.22573", "pdf": "https://arxiv.org/pdf/2505.22573", "abs": "https://arxiv.org/abs/2505.22573", "authors": ["Guy Moss", "Leah Sophie Muhle", "Reinhard Drews", "Jakob H. Macke", "Cornelius Schr\u00f6der"], "title": "FNOPE: Simulation-based inference on function spaces with Fourier Neural Operators", "categories": ["cs.LG"], "comment": null, "summary": "Simulation-based inference (SBI) is an established approach for performing\nBayesian inference on scientific simulators. SBI so far works best on\nlow-dimensional parametric models. However, it is difficult to infer\nfunction-valued parameters, which frequently occur in disciplines that model\nspatiotemporal processes such as the climate and earth sciences. Here, we\nintroduce an approach for efficient posterior estimation, using a Fourier\nNeural Operator (FNO) architecture with a flow matching objective. We show that\nour approach, FNOPE, can perform inference of function-valued parameters at a\nfraction of the simulation budget of state of the art methods. In addition,\nFNOPE supports posterior evaluation at arbitrary discretizations of the domain,\nas well as simultaneous estimation of vector-valued parameters. We demonstrate\nthe effectiveness of our approach on several benchmark tasks and a challenging\nspatial inference task from glaciology. FNOPE extends the applicability of SBI\nmethods to new scientific domains by enabling the inference of function-valued\nparameters.", "AI": {"tldr": "FNOPE\u65b9\u6cd5\u901a\u8fc7\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\u548c\u6d41\u5339\u914d\u76ee\u6807\uff0c\u9ad8\u6548\u4f30\u8ba1\u51fd\u6570\u503c\u53c2\u6570\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u6269\u5c55\u4e86SBI\u7684\u9002\u7528\u9886\u57df\u3002", "motivation": "\u4f20\u7edfSBI\u65b9\u6cd5\u5728\u4f4e\u7ef4\u53c2\u6570\u6a21\u578b\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u51fd\u6570\u503c\u53c2\u6570\uff08\u5982\u65f6\u7a7a\u8fc7\u7a0b\u5efa\u6a21\uff09\u4e2d\u6548\u7387\u4f4e\u4e0b\uff0c\u9650\u5236\u4e86\u5176\u5728\u6c14\u5019\u548c\u5730\u7403\u79d1\u5b66\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\uff08FNO\uff09\u67b6\u6784\u548c\u6d41\u5339\u914d\u76ee\u6807\uff0c\u5b9e\u73b0\u51fd\u6570\u503c\u53c2\u6570\u7684\u9ad8\u6548\u540e\u9a8c\u4f30\u8ba1\uff08FNOPE\uff09\uff0c\u652f\u6301\u4efb\u610f\u7f51\u683c\u79bb\u6563\u5316\u548c\u5411\u91cf\u503c\u53c2\u6570\u540c\u65f6\u4f30\u8ba1\u3002", "result": "FNOPE\u5728\u57fa\u51c6\u4efb\u52a1\u548c\u51b0\u5ddd\u5b66\u7a7a\u95f4\u63a8\u65ad\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u6210\u672c\u4ec5\u4e3a\u73b0\u6709\u65b9\u6cd5\u7684\u6781\u5c0f\u90e8\u5206\u3002", "conclusion": "FNOPE\u6210\u529f\u5c06SBI\u65b9\u6cd5\u6269\u5c55\u5230\u51fd\u6570\u503c\u53c2\u6570\u63a8\u65ad\uff0c\u4e3a\u79d1\u5b66\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u3002", "keywords": "Simulation-based inference, Fourier Neural Operator, function-valued parameters, spatiotemporal modeling, posterior estimation"}}
{"id": "2505.22251", "pdf": "https://arxiv.org/pdf/2505.22251", "abs": "https://arxiv.org/abs/2505.22251", "authors": ["Yuan Tseng", "Titouan Parcollet", "Rogier van Dalen", "Shucong Zhang", "Sourav Bhattacharya"], "title": "Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in Large Language Models for Speech Recognition", "categories": ["eess.AS", "cs.CL"], "comment": null, "summary": "Recent work suggests that large language models (LLMs) can improve\nperformance of speech tasks compared to existing systems. To support their\nclaims, results on LibriSpeech and Common Voice are often quoted. However, this\nwork finds that a substantial amount of the LibriSpeech and Common Voice\nevaluation sets appear in public LLM pretraining corpora. This calls into\nquestion the reliability of findings drawn from these two datasets. To measure\nthe impact of contamination, LLMs trained with or without contamination are\ncompared, showing that a contaminated LLM is more likely to generate test\nsentences it has seen during training. Speech recognisers using contaminated\nLLMs shows only subtle differences in error rates, but assigns significantly\nhigher probabilities to transcriptions seen during training. Results show that\nLLM outputs can be biased by tiny amounts of data contamination, highlighting\nthe importance of evaluating LLM-based speech systems with held-out data.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51fa\uff0cLibriSpeech\u548cCommon Voice\u8bc4\u4f30\u96c6\u4e2d\u7684\u5927\u91cf\u6570\u636e\u51fa\u73b0\u5728LLM\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u4e2d\uff0c\u8fd9\u5f71\u54cd\u4e86\u7ed3\u679c\u7684\u53ef\u9760\u6027\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u53d7\u6c61\u67d3\u7684LLM\u66f4\u503e\u5411\u4e8e\u751f\u6210\u8bad\u7ec3\u65f6\u89c1\u8fc7\u7684\u53e5\u5b50\uff0c\u5bfc\u81f4\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u5728\u9519\u8bef\u7387\u4e0a\u5dee\u5f02\u4e0d\u5927\uff0c\u4f46\u5bf9\u8bad\u7ec3\u89c1\u8fc7\u7684\u8f6c\u5f55\u8d4b\u4e88\u66f4\u9ad8\u6982\u7387\uff0c\u5f3a\u8c03\u4e86\u4f7f\u7528\u72ec\u7acb\u6570\u636e\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8bed\u97f3\u4efb\u52a1\u4e2d\u8868\u73b0\u63d0\u5347\u7684\u53ef\u9760\u6027\uff0c\u5c24\u5176\u662f\u8bc4\u4f30\u6570\u636e\u662f\u5426\u56e0\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u7684\u6c61\u67d3\u800c\u5f71\u54cd\u7ed3\u679c\u3002", "method": "\u6bd4\u8f83\u53d7\u6c61\u67d3\u548c\u672a\u53d7\u6c61\u67d3\u7684LLM\uff0c\u5206\u6790\u5b83\u4eec\u5728\u751f\u6210\u6d4b\u8bd5\u53e5\u5b50\u65f6\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u5e76\u8bc4\u4f30\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u7684\u9519\u8bef\u7387\u548c\u6982\u7387\u5206\u914d\u3002", "result": "\u53d7\u6c61\u67d3\u7684LLM\u503e\u5411\u4e8e\u751f\u6210\u8bad\u7ec3\u89c1\u8fc7\u7684\u53e5\u5b50\uff0c\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u5728\u9519\u8bef\u7387\u4e0a\u5dee\u5f02\u8f83\u5c0f\uff0c\u4f46\u5bf9\u89c1\u8fc7\u7684\u8f6c\u5f55\u8d4b\u4e88\u66f4\u9ad8\u6982\u7387\u3002", "conclusion": "LLM\u8f93\u51fa\u53ef\u80fd\u53d7\u5c11\u91cf\u6570\u636e\u6c61\u67d3\u7684\u504f\u5dee\u5f71\u54cd\uff0c\u4f7f\u7528\u72ec\u7acb\u6570\u636e\u8bc4\u4f30LLM\u8bed\u97f3\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "keywords": "LLM, speech recognition, data contamination, LibriSpeech, Common Voice"}}
{"id": "2505.22578", "pdf": "https://arxiv.org/pdf/2505.22578", "abs": "https://arxiv.org/abs/2505.22578", "authors": ["Etienne Boursier", "Matthew Bowditch", "Matthias Englert", "Ranko Lazic"], "title": "Benignity of loss landscape with weight decay requires both large overparametrization and initialization", "categories": ["cs.LG"], "comment": null, "summary": "The optimization of neural networks under weight decay remains poorly\nunderstood from a theoretical standpoint. While weight decay is standard\npractice in modern training procedures, most theoretical analyses focus on\nunregularized settings. In this work, we investigate the loss landscape of the\n$\\ell_2$-regularized training loss for two-layer ReLU networks. We show that\nthe landscape becomes benign -- i.e., free of spurious local minima -- under\nlarge overparametrization, specifically when the network width $m$ satisfies $m\n\\gtrsim \\min(n^d, 2^n)$, where $n$ is the number of data points and $d$ the\ninput dimension. More precisely in this regime, almost all constant activation\nregions contain a global minimum and no spurious local minima. We further show\nthat this level of overparametrization is not only sufficient but also\nnecessary via the example of orthogonal data. Finally, we demonstrate that such\nloss landscape results primarily hold relevance in the large initialization\nregime. In contrast, for small initializations -- corresponding to the feature\nlearning regime -- optimization can still converge to spurious local minima,\ndespite the global benignity of the landscape.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5e26\u6743\u91cd\u8870\u51cf\u7684\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u95ee\u9898\uff0c\u53d1\u73b0\u5728\u5927\u89c4\u6a21\u8fc7\u53c2\u6570\u5316\u4e0b\uff0c\u635f\u5931\u51fd\u6570\u4f1a\u53d8\u5f97\u65e0\u5bb3\uff0c\u5373\u4e0d\u5b58\u5728\u865a\u5047\u5c40\u90e8\u6700\u5c0f\u503c\u3002\u540c\u65f6\u6307\u51fa\u8fc7\u53c2\u6570\u5316\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u521d\u59cb\u5316\u5927\u5c0f\u5bf9\u4f18\u5316\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "motivation": "\u6743\u91cd\u8870\u51cf\u662f\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7684\u5e38\u89c4\u5b9e\u8df5\uff0c\u4f46\u5176\u7406\u8bba\u7406\u89e3\u4ecd\u4e0d\u5145\u5206\u3002\u672c\u6587\u65e8\u5728\u4ece\u7406\u8bba\u89d2\u5ea6\u63a2\u8ba8\u5e26\u6709\u21132\u6b63\u5219\u5316\u7684\u4e24\u5c42ReLU\u7f51\u7edc\u7684\u635f\u5931\u51fd\u6570\u7279\u6027\u3002", "method": "\u7814\u7a76\u56e2\u961f\u5206\u6790\u4e86\u21132\u6b63\u5219\u5316\u635f\u5931\u51fd\u6570\u7684\u666f\u89c2\uff0c\u7279\u522b\u662f\u5728\u7f51\u7edc\u5bbd\u5ea6\u6ee1\u8db3\u7279\u5b9a\u6761\u4ef6\uff08m\u2273min(n^d, 2^n)\uff09\u65f6\u7684\u60c5\u5f62\uff0c\u5e76\u533a\u5206\u4e86\u5927\u521d\u59cb\u5316\u548c\u5c0f\u521d\u59cb\u5316\u7684\u4e0d\u540c\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u5728\u5927\u89c4\u6a21\u8fc7\u53c2\u6570\u5316\u4e0b\uff0c\u635f\u5931\u51fd\u6570\u7684\u666f\u89c2\u53d8\u5f97\u65e0\u5bb3\uff0c\u51e0\u4e4e\u6240\u6709\u5e38\u6570\u6fc0\u6d3b\u533a\u57df\u90fd\u5305\u542b\u5168\u5c40\u6700\u5c0f\u503c\u4e14\u65e0\u865a\u5047\u5c40\u90e8\u6700\u5c0f\u503c\u3002\u4f46\u5c0f\u521d\u59cb\u5316\u65f6\u4f18\u5316\u4ecd\u53ef\u80fd\u6536\u655b\u5230\u865a\u5047\u5c40\u90e8\u6700\u5c0f\u503c\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6743\u91cd\u8870\u51cf\u7684\u7406\u8bba\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u540c\u65f6\u4e5f\u5f3a\u8c03\u4e86\u521d\u59cb\u5316\u548c\u8fc7\u53c2\u6570\u5316\u5728\u5b9e\u9645\u8bad\u7ec3\u4e2d\u7684\u91cd\u8981\u6027\u3002", "keywords": "\u795e\u7ecf\u7f51\u7edc, \u6743\u91cd\u8870\u51cf, \u635f\u5931\u51fd\u6570, \u8fc7\u53c2\u6570\u5316, ReLU\u7f51\u7edc, \u521d\u59cb\u5316"}}
{"id": "2505.22601", "pdf": "https://arxiv.org/pdf/2505.22601", "abs": "https://arxiv.org/abs/2505.22601", "authors": ["Jacob L. Block", "Aryan Mokhtari", "Sanjay Shakkottai"], "title": "Machine Unlearning under Overparameterization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Machine unlearning algorithms aim to remove the influence of specific\ntraining samples, ideally recovering the model that would have resulted from\ntraining on the remaining data alone. We study unlearning in the\noverparameterized setting, where many models interpolate the data, and defining\nthe unlearning solution as any loss minimizer over the retained\nset$\\unicode{x2013}$as in prior work in the underparameterized\nsetting$\\unicode{x2013}$is inadequate, since the original model may already\ninterpolate the retained data and satisfy this condition. In this regime, loss\ngradients vanish, rendering prior methods based on gradient perturbations\nineffective, motivating both new unlearning definitions and algorithms. For\nthis setting, we define the unlearning solution as the minimum-complexity\ninterpolator over the retained data and propose a new algorithmic framework\nthat only requires access to model gradients on the retained set at the\noriginal solution. We minimize a regularized objective over perturbations\nconstrained to be orthogonal to these model gradients, a first-order relaxation\nof the interpolation condition. For different model classes, we provide exact\nand approximate unlearning guarantees, and we demonstrate that an\nimplementation of our framework outperforms existing baselines across various\nunlearning experiments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u8fc7\u53c2\u6570\u5316\u8bbe\u7f6e\u4e0b\u7684\u673a\u5668\u9057\u5fd8\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u9057\u5fd8\u5b9a\u4e49\u548c\u7b97\u6cd5\uff0c\u4ee5\u6700\u5c0f\u590d\u6742\u6027\u63d2\u503c\u4e3a\u76ee\u6807\uff0c\u5e76\u901a\u8fc7\u7ea6\u675f\u6b63\u4ea4\u68af\u5ea6\u7684\u6270\u52a8\u4f18\u5316\u5b9e\u73b0\u9057\u5fd8\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u8fc7\u53c2\u6570\u5316\u8bbe\u7f6e\u4e2d\uff0c\u8bb8\u591a\u6a21\u578b\u80fd\u63d2\u503c\u6570\u636e\uff0c\u4f20\u7edf\u9057\u5fd8\u65b9\u6cd5\u57fa\u4e8e\u68af\u5ea6\u6270\u52a8\u5728\u6b64\u573a\u666f\u4e0b\u5931\u6548\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u9057\u5fd8\u5b9a\u4e49\u548c\u7b97\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5c06\u9057\u5fd8\u89e3\u5b9a\u4e49\u4e3a\u4fdd\u7559\u6570\u636e\u4e0a\u7684\u6700\u5c0f\u590d\u6742\u6027\u63d2\u503c\u5668\uff0c\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u4fdd\u7559\u96c6\u68af\u5ea6\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6b63\u4ea4\u68af\u5ea6\u6270\u52a8\u7684\u6b63\u5219\u5316\u76ee\u6807\u4f18\u5316\u3002", "result": "\u5728\u4e0d\u540c\u6a21\u578b\u7c7b\u522b\u4e0b\u63d0\u4f9b\u4e86\u7cbe\u786e\u548c\u8fd1\u4f3c\u7684\u9057\u5fd8\u4fdd\u8bc1\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u9057\u5fd8\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u5728\u8fc7\u53c2\u6570\u5316\u8bbe\u7f6e\u4e2d\uff0c\u6700\u5c0f\u590d\u6742\u6027\u63d2\u503c\u548c\u6b63\u4ea4\u68af\u5ea6\u7ea6\u675f\u7684\u6270\u52a8\u662f\u5b9e\u73b0\u6709\u6548\u673a\u5668\u9057\u5fd8\u7684\u5173\u952e\uff0c\u65b0\u65b9\u6cd5\u5c55\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "keywords": "\u673a\u5668\u9057\u5fd8, \u8fc7\u53c2\u6570\u5316, \u6700\u5c0f\u590d\u6742\u6027\u63d2\u503c, \u68af\u5ea6\u6270\u52a8, \u6b63\u5219\u5316"}}
{"id": "2505.22271", "pdf": "https://arxiv.org/pdf/2505.22271", "abs": "https://arxiv.org/abs/2505.22271", "authors": ["Yongcan Yu", "Yanbo Wang", "Ran He", "Jian Liang"], "title": "Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "Under Review", "summary": "While (multimodal) large language models (LLMs) have attracted widespread\nattention due to their exceptional capabilities, they remain vulnerable to\njailbreak attacks. Various defense methods are proposed to defend against\njailbreak attacks, however, they are often tailored to specific types of\njailbreak attacks, limiting their effectiveness against diverse adversarial\nstrategies. For instance, rephrasing-based defenses are effective against text\nadversarial jailbreaks but fail to counteract image-based attacks. To overcome\nthese limitations, we propose a universal defense framework, termed Test-time\nIMmunization (TIM), which can adaptively defend against various jailbreak\nattacks in a self-evolving way. Specifically, TIM initially trains a gist token\nfor efficient detection, which it subsequently applies to detect jailbreak\nactivities during inference. When jailbreak attempts are identified, TIM\nimplements safety fine-tuning using the detected jailbreak instructions paired\nwith refusal answers. Furthermore, to mitigate potential performance\ndegradation in the detector caused by parameter updates during safety\nfine-tuning, we decouple the fine-tuning process from the detection module.\nExtensive experiments on both LLMs and multimodal LLMs demonstrate the efficacy\nof TIM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTIM\u7684\u901a\u7528\u9632\u5fa1\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u9002\u5e94\u5730\u62b5\u5fa1\u5404\u79cd\u8d8a\u72f1\u653b\u51fb\u3002TIM\u901a\u8fc7\u8bad\u7ec3\u6838\u5fc3\u6807\u8bb0\u8fdb\u884c\u68c0\u6d4b\uff0c\u5e76\u5728\u8bc6\u522b\u653b\u51fb\u65f6\u8fdb\u884c\u5b89\u5168\u5fae\u8c03\uff0c\u540c\u65f6\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u89e3\u8026\u68c0\u6d4b\u6a21\u5757\u4ee5\u907f\u514d\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u73b0\u6709\u7684\u8d8a\u72f1\u9632\u5fa1\u65b9\u6cd5\u5f80\u5f80\u9488\u5bf9\u7279\u5b9a\u7c7b\u578b\u7684\u653b\u51fb\uff0c\u96be\u4ee5\u5e94\u5bf9\u591a\u6837\u5316\u7684\u5bf9\u6297\u7b56\u7565\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86TIM\u6846\u67b6\u3002", "method": "TIM\u8bad\u7ec3\u6838\u5fc3\u6807\u8bb0\u8fdb\u884c\u9ad8\u6548\u68c0\u6d4b\uff0c\u68c0\u6d4b\u5230\u8d8a\u72f1\u6307\u4ee4\u540e\u8fdb\u884c\u5b89\u5168\u5fae\u8c03\u3002\u4e3a\u4e86\u907f\u514d\u5fae\u8c03\u5f71\u54cd\u68c0\u6d4b\u6027\u80fd\uff0cTIM\u5c06\u5fae\u8c03\u8fc7\u7a0b\u4e0e\u68c0\u6d4b\u6a21\u5757\u89e3\u8026\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTIM\u5728\u5355\u4e00\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u90fd\u80fd\u6709\u6548\u9632\u5fa1\u5404\u79cd\u8d8a\u72f1\u653b\u51fb\u3002", "conclusion": "TIM\u662f\u4e00\u79cd\u81ea\u8fdb\u5316\u7684\u901a\u7528\u9632\u5fa1\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u591a\u6837\u5316\u7684\u8d8a\u72f1\u653b\u51fb\u7b56\u7565\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u8d8a\u72f1\u653b\u51fb,\u9632\u5fa1\u6846\u67b6,\u5b89\u5168\u5fae\u8c03,\u81ea\u8fdb\u5316"}}
{"id": "2505.22602", "pdf": "https://arxiv.org/pdf/2505.22602", "abs": "https://arxiv.org/abs/2505.22602", "authors": ["Mahtab Alizadeh Vandchali", "Fangshuo", "Liao", "Anastasios Kyrillidis"], "title": "One Rank at a Time: Cascading Error Dynamics in Sequential Learning", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": "36 pages", "summary": "Sequential learning -- where complex tasks are broken down into simpler,\nhierarchical components -- has emerged as a paradigm in AI. This paper views\nsequential learning through the lens of low-rank linear regression, focusing\nspecifically on how errors propagate when learning rank-1 subspaces\nsequentially. We present an analysis framework that decomposes the learning\nprocess into a series of rank-1 estimation problems, where each subsequent\nestimation depends on the accuracy of previous steps. Our contribution is a\ncharacterization of the error propagation in this sequential process,\nestablishing bounds on how errors -- e.g., due to limited computational budgets\nand finite precision -- affect the overall model accuracy. We prove that these\nerrors compound in predictable ways, with implications for both algorithmic\ndesign and stability guarantees.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u4f4e\u79e9\u7ebf\u6027\u56de\u5f52\u7684\u89c6\u89d2\u7814\u7a76\u5e8f\u5217\u5b66\u4e60\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u5728\u987a\u5e8f\u5b66\u4e60\u79e9-1\u5b50\u7a7a\u95f4\u65f6\u8bef\u5dee\u5982\u4f55\u4f20\u64ad\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bef\u5dee\u4f20\u64ad\u7684\u91cf\u5316\u6846\u67b6\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u7406\u89e3\u5e8f\u5217\u5b66\u4e60\u4e2d\u8bef\u5dee\u5982\u4f55\u4f20\u64ad\uff0c\u7279\u522b\u662f\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u548c\u7cbe\u5ea6\u4e0b\uff0c\u8bef\u5dee\u5982\u4f55\u5f71\u54cd\u6574\u4f53\u6a21\u578b\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u5c06\u5b66\u4e60\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u79e9-1\u4f30\u8ba1\u95ee\u9898\uff0c\u6bcf\u4e2a\u540e\u7eed\u4f30\u8ba1\u4f9d\u8d56\u524d\u4e00\u6b65\u7684\u7cbe\u5ea6\uff0c\u5206\u6790\u8bef\u5dee\u4f20\u64ad\u7684\u65b9\u5f0f\u3002", "result": "\u8bc1\u660e\u4e86\u8bef\u5dee\u4f1a\u4ee5\u53ef\u9884\u6d4b\u7684\u65b9\u5f0f\u7d2f\u79ef\uff0c\u4e3a\u7b97\u6cd5\u8bbe\u8ba1\u548c\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5e8f\u5217\u5b66\u4e60\u4e2d\u8bef\u5dee\u4f20\u64ad\u7684\u89c4\u5f8b\uff0c\u5bf9\u7b97\u6cd5\u8bbe\u8ba1\u548c\u7a33\u5b9a\u6027\u4fdd\u8bc1\u6709\u91cd\u8981\u610f\u4e49\u3002", "keywords": "\u5e8f\u5217\u5b66\u4e60, \u4f4e\u79e9\u56de\u5f52, \u8bef\u5dee\u4f20\u64ad, \u79e9-1\u5b50\u7a7a\u95f4, \u7b97\u6cd5\u7a33\u5b9a\u6027"}}
{"id": "2505.21904", "pdf": "https://arxiv.org/pdf/2505.21904", "abs": "https://arxiv.org/abs/2505.21904", "authors": ["Pardis Taghavi", "Tian Liu", "Renjie Li", "Reza Langari", "Zhengzhong Tu"], "title": "CAST: Contrastive Adaptation and Distillation for Semi-Supervised Instance Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Instance segmentation demands costly per-pixel annotations and large models.\nWe introduce CAST, a semi-supervised knowledge distillation (SSKD) framework\nthat compresses pretrained vision foundation models (VFM) into compact experts\nusing limited labeled and abundant unlabeled data. CAST unfolds in three\nstages: (1) domain adaptation of the VFM teacher(s) via self-training with\ncontrastive pixel calibration, (2) distillation into a compact student via a\nunified multi-objective loss that couples standard supervision and\npseudo-labels with our instance-aware pixel-wise contrastive term, and (3)\nfine-tuning on labeled data to remove residual pseudo-label bias. Central to\nCAST is an \\emph{instance-aware pixel-wise contrastive loss} that fuses mask\nand class scores to mine informative negatives and enforce clear inter-instance\nmargins. By maintaining this contrastive signal across both adaptation and\ndistillation, we align teacher and student embeddings and fully leverage\nunlabeled images. On Cityscapes and ADE20K, our ~11X smaller student surpasses\nits adapted VFM teacher(s) by +3.4 AP (33.9 vs. 30.5) and +1.5 AP (16.7 vs.\n15.2) and outperforms state-of-the-art semi-supervised approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86CAST\u6846\u67b6\uff0c\u901a\u8fc7\u534a\u76d1\u7763\u77e5\u8bc6\u84b8\u998f\uff08SSKD\uff09\u5c06\u9884\u8bad\u7ec3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFM\uff09\u538b\u7f29\u4e3a\u7d27\u51d1\u4e13\u5bb6\u6a21\u578b\uff0c\u5229\u7528\u6709\u9650\u6807\u6ce8\u548c\u5927\u91cf\u65e0\u6807\u6ce8\u6570\u636e\u63d0\u5347\u5b9e\u4f8b\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u5b9e\u4f8b\u5206\u5272\u9700\u8981\u6602\u8d35\u7684\u50cf\u7d20\u7ea7\u6807\u6ce8\u548c\u5927\u6a21\u578b\uff0cCAST\u65e8\u5728\u901a\u8fc7\u534a\u76d1\u7763\u77e5\u8bc6\u84b8\u998f\u51cf\u5c11\u6807\u6ce8\u9700\u6c42\u5e76\u63d0\u5347\u5c0f\u6a21\u578b\u6027\u80fd\u3002", "method": "CAST\u5206\u4e3a\u4e09\u9636\u6bb5\uff1a(1) \u901a\u8fc7\u81ea\u8bad\u7ec3\u548c\u5bf9\u6bd4\u50cf\u7d20\u6821\u51c6\u8fdb\u884cVFM\u6559\u5e08\u7684\u57df\u9002\u5e94\uff1b(2) \u4f7f\u7528\u5305\u542b\u6807\u51c6\u76d1\u7763\u3001\u4f2a\u6807\u7b7e\u548c\u5b9e\u4f8b\u611f\u77e5\u50cf\u7d20\u5bf9\u6bd4\u7684\u591a\u76ee\u6807\u635f\u5931\u84b8\u998f\u5230\u7d27\u51d1\u5b66\u751f\u6a21\u578b\uff1b(3) \u5728\u6807\u6ce8\u6570\u636e\u4e0a\u5fae\u8c03\u4ee5\u6d88\u9664\u4f2a\u6807\u7b7e\u504f\u5dee\u3002", "result": "\u5728Cityscapes\u548cADE20K\u4e0a\uff0c11X\u5c0f\u7684\u5b66\u751f\u6a21\u578b\u6bd4\u9002\u5e94\u7684VFM\u6559\u5e08\u5206\u522b\u63d0\u5347+3.4 AP\uff0833.9 vs. 30.5\uff09\u548c+1.5 AP\uff0816.7 vs. 15.2\uff09\uff0c\u5e76\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u534a\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "CAST\u901a\u8fc7\u5b9e\u4f8b\u611f\u77e5\u50cf\u7d20\u5bf9\u6bd4\u635f\u5931\u6709\u6548\u5229\u7528\u65e0\u6807\u6ce8\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u6a21\u578b\u7684\u5b9e\u4f8b\u5206\u5272\u6027\u80fd\u3002", "keywords": "\u5b9e\u4f8b\u5206\u5272\uff0c\u534a\u76d1\u7763\u5b66\u4e60\uff0c\u77e5\u8bc6\u84b8\u998f\uff0c\u5bf9\u6bd4\u5b66\u4e60"}}
{"id": "2505.22617", "pdf": "https://arxiv.org/pdf/2505.22617", "abs": "https://arxiv.org/abs/2505.22617", "authors": ["Ganqu Cui", "Yuchen Zhang", "Jiacheng Chen", "Lifan Yuan", "Zhi Wang", "Yuxin Zuo", "Haozhan Li", "Yuchen Fan", "Huayu Chen", "Weize Chen", "Zhiyuan Liu", "Hao Peng", "Lei Bai", "Wanli Ouyang", "Yu Cheng", "Bowen Zhou", "Ning Ding"], "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "This paper aims to overcome a major obstacle in scaling RL for reasoning with\nLLMs, namely the collapse of policy entropy. Such phenomenon is consistently\nobserved across vast RL runs without entropy intervention, where the policy\nentropy dropped sharply at the early training stage, this diminished\nexploratory ability is always accompanied with the saturation of policy\nperformance. In practice, we establish a transformation equation R=-a*e^H+b\nbetween entropy H and downstream performance R. This empirical law strongly\nindicates that, the policy performance is traded from policy entropy, thus\nbottlenecked by its exhaustion, and the ceiling is fully predictable H=0,\nR=-a+b. Our finding necessitates entropy management for continuous exploration\ntoward scaling compute for RL. To this end, we investigate entropy dynamics\nboth theoretically and empirically. Our derivation highlights that, the change\nin policy entropy is driven by the covariance between action probability and\nthe change in logits, which is proportional to its advantage when using Policy\nGradient-like algorithms. Empirical study shows that, the values of covariance\nterm and entropy differences matched exactly, supporting the theoretical\nconclusion. Moreover, the covariance term stays mostly positive throughout\ntraining, further explaining why policy entropy would decrease monotonically.\nThrough understanding the mechanism behind entropy dynamics, we motivate to\ncontrol entropy by restricting the update of high-covariance tokens.\nSpecifically, we propose two simple yet effective techniques, namely Clip-Cov\nand KL-Cov, which clip and apply KL penalty to tokens with high covariances\nrespectively. Experiments show that these methods encourage exploration, thus\nhelping policy escape entropy collapse and achieve better downstream\nperformance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u7b56\u7565\u71b5\u5d29\u6e83\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u71b5\u52a8\u6001\u673a\u5236\u63d0\u51fa\u4e24\u79cd\u7b80\u5355\u6709\u6548\u7684\u6280\u672f\uff08Clip-Cov\u548cKL-Cov\uff09\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u63a2\u7d22\u80fd\u529b\u548c\u4e0b\u6e38\u6027\u80fd\u3002", "motivation": "\u653f\u7b56\u71b5\u5d29\u6e83\u4e25\u91cd\u9650\u5236\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6269\u5c55\u5e94\u7528\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u7ef4\u6301\u63a2\u7d22\u80fd\u529b\u5e76\u907f\u514d\u6027\u80fd\u9971\u548c\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\u71b5\u52a8\u6001\u673a\u5236\uff0c\u63d0\u51faClip-Cov\u548cKL-Cov\u6280\u672f\uff0c\u9650\u5236\u9ad8\u534f\u65b9\u5dee\u6807\u8bb0\u7684\u66f4\u65b0\u4ee5\u63a7\u5236\u71b5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cClip-Cov\u548cKL-Cov\u80fd\u6709\u6548\u907f\u514d\u71b5\u5d29\u6e83\uff0c\u589e\u5f3a\u63a2\u7d22\u80fd\u529b\u5e76\u63d0\u5347\u4e0b\u6e38\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u63a7\u5236\u7b56\u7565\u71b5\u7684\u673a\u5236\u7814\u7a76\uff0c\u63d0\u51fa\u7684\u6280\u672f\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u7b56\u7565\u71b5, \u71b5\u5d29\u6e83, \u63a2\u7d22\u80fd\u529b, \u534f\u65b9\u5dee\u63a7\u5236"}}
{"id": "2505.21906", "pdf": "https://arxiv.org/pdf/2505.21906", "abs": "https://arxiv.org/abs/2505.21906", "authors": ["Zhongyi Zhou", "Yichen Zhu", "Junjie Wen", "Chaomin Shen", "Yi Xu"], "title": "Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Project page: https://chatvla-2.github.io/", "summary": "Vision-language-action (VLA) models have emerged as the next generation of\nmodels in robotics. However, despite leveraging powerful pre-trained\nVision-Language Models (VLMs), existing end-to-end VLA systems often lose key\ncapabilities during fine-tuning as the model adapts to specific robotic tasks.\nWe argue that a generalizable VLA model should retain and expand upon the VLM's\ncore competencies: 1) Open-world embodied reasoning - the VLA should inherit\nthe knowledge from VLM, i.e., recognize anything that the VLM can recognize,\ncapable of solving math problems, possessing visual-spatial intelligence, 2)\nReasoning following - effectively translating the open-world reasoning into\nactionable steps for the robot. In this work, we introduce ChatVLA-2, a novel\nmixture-of-expert VLA model coupled with a specialized three-stage training\npipeline designed to preserve the VLM's original strengths while enabling\nactionable reasoning. To validate our approach, we design a math-matching task\nwherein a robot interprets math problems written on a whiteboard and picks\ncorresponding number cards from a table to solve equations. Remarkably, our\nmethod exhibits exceptional mathematical reasoning and OCR capabilities,\ndespite these abilities not being explicitly trained within the VLA.\nFurthermore, we demonstrate that the VLA possesses strong spatial reasoning\nskills, enabling it to interpret novel directional instructions involving\npreviously unseen objects. Overall, our method showcases reasoning and\ncomprehension abilities that significantly surpass state-of-the-art imitation\nlearning methods such as OpenVLA, DexVLA, and pi-zero. This work represents a\nsubstantial advancement toward developing truly generalizable robotic\nfoundation models endowed with robust reasoning capacities.", "AI": {"tldr": "ChatVLA-2\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u6df7\u5408\u4e13\u5bb6VLA\u6a21\u578b\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u4fdd\u6301VLM\u7684\u6838\u5fc3\u80fd\u529b\u5e76\u5b9e\u73b0\u53ef\u884c\u52a8\u7684\u63a8\u7406\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548cOCR\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u7aef\u5230\u7aefVLA\u7cfb\u7edf\u5728\u5fae\u8c03\u65f6\u4e22\u5931VLM\u6838\u5fc3\u80fd\u529b\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u5177\u5907\u5f00\u653e\u4e16\u754c\u63a8\u7406\u80fd\u529b\u548c\u53ef\u884c\u52a8\u63a8\u7406\u7684\u901a\u7528VLA\u6a21\u578b\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6VLA\u6a21\u578b\uff08ChatVLA-2\uff09\u548c\u4e09\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff0c\u4fdd\u7559VLM\u7684\u539f\u59cb\u4f18\u52bf\u540c\u65f6\u5b9e\u73b0\u53ef\u884c\u52a8\u7684\u63a8\u7406\u3002", "result": "\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u3001OCR\u548c\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5982OpenVLA\u3001DexVLA\u548cpi-zero\u3002", "conclusion": "ChatVLA-2\u662f\u8fc8\u5411\u5177\u5907\u5f3a\u5927\u63a8\u7406\u80fd\u529b\u7684\u901a\u7528\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u7684\u91cd\u8981\u8fdb\u5c55\u3002", "keywords": "Vision-Language-Action, \u6df7\u5408\u4e13\u5bb6\u6a21\u578b, \u4e09\u9636\u6bb5\u8bad\u7ec3, \u5f00\u653e\u4e16\u754c\u63a8\u7406, \u673a\u5668\u4eba\u63a8\u7406"}}
{"id": "2505.22637", "pdf": "https://arxiv.org/pdf/2505.22637", "abs": "https://arxiv.org/abs/2505.22637", "authors": ["Joschka Braun", "Carsten Eickhoff", "David Krueger", "Seyed Ali Bahrainian", "Dmitrii Krasheninnikov"], "title": "Understanding (Un)Reliability of Steering Vectors in Language Models", "categories": ["cs.LG"], "comment": "17 pages, 10 figures. Presented at the ICLR 2025 Workshop on\n  Foundation Models in the Wild", "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u6fc0\u6d3b\u5411\u91cf\u63a7\u5236\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u5728\u4e0d\u540c\u63d0\u793a\u7c7b\u578b\u4e0b\u6574\u4f53\u6709\u6548\u4f46\u6548\u679c\u4e0d\u7a33\u5b9a\uff0c\u4e14\u5176\u53ef\u9760\u6027\u4f9d\u8d56\u4e8e\u8bad\u7ec3\u96c6\u4e2d\u6fc0\u6d3b\u5dee\u5f02\u7684\u65b9\u5411\u4e00\u81f4\u6027\u548c\u76ee\u6807\u884c\u4e3a\u7684\u660e\u786e\u65b9\u5411\u6027\u3002", "motivation": "\u63a2\u7d22\u6fc0\u6d3b\u5411\u91cf\u65b9\u6cd5\u5728\u4e0d\u540c\u63d0\u793a\u7c7b\u578b\u4e0b\u7684\u53ef\u9760\u6027\uff0c\u4ee5\u53ca\u6fc0\u6d3b\u5dee\u5f02\u7684\u51e0\u4f55\u7279\u6027\u5bf9\u5f15\u5bfc\u6548\u679c\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u4e03\u79cd\u63d0\u793a\u7c7b\u578b\u5b9e\u9a8c\uff0c\u5206\u6790\u6fc0\u6d3b\u5dee\u5f02\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4e0e\u76ee\u6807\u884c\u4e3a\u5f15\u5bfc\u6548\u679c\u7684\u5173\u7cfb\u3002", "result": "\u6240\u6709\u63d0\u793a\u7c7b\u578b\u5747\u4ea7\u751f\u6b63\u5411\u5f15\u5bfc\u6548\u679c\u4f46\u65b9\u5dee\u5927\uff1b\u4f59\u5f26\u76f8\u4f3c\u5ea6\u9ad8\u7684\u8bad\u7ec3\u96c6\u6fc0\u6d3b\u5dee\u5f02\u66f4\u6709\u6548\uff1b\u76ee\u6807\u884c\u4e3a\u65b9\u5411\u660e\u786e\u7684\u573a\u666f\u5f15\u5bfc\u66f4\u53ef\u9760\u3002", "conclusion": "\u6fc0\u6d3b\u5411\u91cf\u5f15\u5bfc\u7684\u53ef\u9760\u6027\u4f9d\u8d56\u4e8e\u76ee\u6807\u884c\u4e3a\u65b9\u5411\u7684\u660e\u786e\u6027\u548c\u6fc0\u6d3b\u5dee\u5f02\u7684\u4e00\u81f4\u6027\u3002", "keywords": "\u8bed\u8a00\u6a21\u578b, \u6fc0\u6d3b\u5411\u91cf, \u63d0\u793a\u7c7b\u578b, \u4f59\u5f26\u76f8\u4f3c\u5ea6, \u884c\u4e3a\u63a7\u5236"}}
{"id": "2505.22641", "pdf": "https://arxiv.org/pdf/2505.22641", "abs": "https://arxiv.org/abs/2505.22641", "authors": ["Chengzhi Shi", "Stratis Ioannidis"], "title": "Spectral Survival Analysis", "categories": ["cs.LG"], "comment": "Extended version of conference paper appearing in KDD 2025", "summary": "Survival analysis is widely deployed in a diverse set of fields, including\nhealthcare, business, ecology, etc. The Cox Proportional Hazard (CoxPH) model\nis a semi-parametric model often encountered in the literature. Despite its\npopularity, wide deployment, and numerous variants, scaling CoxPH to large\ndatasets and deep architectures poses a challenge, especially in the\nhigh-dimensional regime. We identify a fundamental connection between rank\nregression and the CoxPH model: this allows us to adapt and extend the\nso-called spectral method for rank regression to survival analysis. Our\napproach is versatile, naturally generalizing to several CoxPH variants,\nincluding deep models. We empirically verify our method's scalability on\nmultiple real-world high-dimensional datasets; our method outperforms legacy\nmethods w.r.t. predictive performance and efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06Cox\u6bd4\u4f8b\u98ce\u9669\u6a21\u578b\u6269\u5c55\u5230\u5927\u6570\u636e\u96c6\u548c\u9ad8\u7ef4\u73af\u5883\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u79e9\u56de\u5f52\u4e0eCoxPH\u6a21\u578b\u7684\u5173\u8054\u6539\u8fdb\u9884\u6d4b\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "CoxPH\u6a21\u578b\u56e0\u5176\u534a\u53c2\u6570\u7279\u6027\u5e7f\u6cdb\u5e94\u7528\u4e8e\u591a\u9886\u57df\uff0c\u4f46\u5728\u5927\u6570\u636e\u96c6\u548c\u9ad8\u7ef4\u73af\u5883\u4e0b\u7684\u6269\u5c55\u6027\u9762\u4e34\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u79e9\u56de\u5f52\u4e0eCoxPH\u6a21\u578b\u7684\u57fa\u672c\u8054\u7cfb\uff0c\u6269\u5c55\u8c31\u65b9\u6cd5\u5230\u751f\u5b58\u5206\u6790\u4e2d\uff0c\u652f\u6301\u591a\u79cdCoxPH\u53d8\u4f53\u5305\u62ec\u6df1\u5ea6\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u9ad8\u7ef4\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u5728\u9884\u6d4b\u6027\u80fd\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86CoxPH\u6a21\u578b\u5728\u5927\u89c4\u6a21\u548c\u9ad8\u7ef4\u6570\u636e\u4e0b\u7684\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "keywords": "\u751f\u5b58\u5206\u6790, Cox\u6bd4\u4f8b\u98ce\u9669\u6a21\u578b, \u79e9\u56de\u5f52, \u9ad8\u7ef4\u6570\u636e, \u6df1\u5ea6\u5b66\u4e60"}}
{"id": "2505.22650", "pdf": "https://arxiv.org/pdf/2505.22650", "abs": "https://arxiv.org/abs/2505.22650", "authors": ["Maria-Florina Balcan", "Avrim Blum", "Zhiyuan Li", "Dravyansh Sharma"], "title": "On Learning Verifiers for Chain-of-Thought Reasoning", "categories": ["cs.LG"], "comment": null, "summary": "Chain-of-Thought reasoning has emerged as a powerful approach for solving\ncomplex mathematical and logical problems. However, it can often veer off track\nthrough incorrect or unsubstantiated inferences. Formal mathematical reasoning,\nwhich can be checked with a formal verifier, is one approach to addressing this\nissue. However, currently LLMs are simply not good enough to solve complex\nproblems in a formal way, and even just formalizing an informal problem\nstatement can be challenging. Motivated by this fact, in this work we consider\nthe problem of learning reliable verifiers for natural language\nChain-of-Thought reasoning. That is, given a problem statement and step-by-step\nsolution in natural language, the aim of the verifier is to output [Yes] if the\nreasoning steps in the solution are all valid, and [No] otherwise. In this work\nwe give a formal PAC-learning framework for studying this problem. We propose\nand analyze several natural verification goals, at different levels of\nstrength, in this framework. We provide sample complexity upper-bounds for\nlearning verifiers satisfying these goals, as well as lower-bound and\nimpossibility results for learning other natural verification objectives\nwithout additional assumptions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e60\u53ef\u9760\u9a8c\u8bc1\u5668\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u81ea\u7136\u8bed\u8a00\u4e2d\u7684Chain-of-Thought\u63a8\u7406\u6b65\u9aa4\u7684\u6709\u6548\u6027\uff0c\u5e76\u7ed9\u51fa\u4e86PAC\u5b66\u4e60\u6846\u67b6\u4e0b\u7684\u7406\u8bba\u5206\u6790\u3002", "motivation": "Chain-of-Thought\u63a8\u7406\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u5bb9\u6613\u56e0\u9519\u8bef\u63a8\u65ad\u800c\u504f\u79bb\u6b63\u786e\u8def\u5f84\u3002\u5f53\u524dLLM\u65e0\u6cd5\u6709\u6548\u5f62\u5f0f\u5316\u590d\u6742\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u5b66\u4e60\u53ef\u9760\u7684\u9a8c\u8bc1\u5668\u6765\u8bc4\u4f30\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u6b65\u9aa4\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePAC\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u5b9a\u4e49\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u5f3a\u5ea6\u7684\u9a8c\u8bc1\u76ee\u6807\uff0c\u63a8\u5bfc\u4e86\u5b66\u4e60\u9a8c\u8bc1\u5668\u7684\u6837\u672c\u590d\u6742\u5ea6\u4e0a\u4e0b\u754c\u3002", "result": "\u8bba\u6587\u63d0\u4f9b\u4e86\u5b66\u4e60\u6ee1\u8db3\u7279\u5b9a\u9a8c\u8bc1\u76ee\u6807\u7684\u9a8c\u8bc1\u5668\u7684\u6837\u672c\u590d\u6742\u5ea6\u4e0a\u754c\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u5728\u6ca1\u6709\u989d\u5916\u5047\u8bbe\u4e0b\u5b66\u4e60\u67d0\u4e9b\u76ee\u6807\u7684\u4e0d\u53ef\u80fd\u6027\u3002", "conclusion": "\u901a\u8fc7\u5b66\u4e60\u9a8c\u8bc1\u5668\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8Chain-of-Thought\u63a8\u7406\u7684\u53ef\u9760\u6027\uff0c\u4f46\u90e8\u5206\u76ee\u6807\u9700\u989d\u5916\u5047\u8bbe\u624d\u80fd\u5b9e\u73b0\u3002", "keywords": "Chain-of-Thought, formal verification, PAC-learning, sample complexity, natural language reasoning"}}
{"id": "2505.21919", "pdf": "https://arxiv.org/pdf/2505.21919", "abs": "https://arxiv.org/abs/2505.21919", "authors": ["Yue Zhu", "Hao Yu", "Chen Wang", "Zhuoran Liu", "Eun Kyung Lee"], "title": "Towards Efficient Key-Value Cache Management for Prefix Prefilling in LLM Inference", "categories": ["cs.ET", "cs.AI", "cs.DC"], "comment": "This paper has been accepted at IEEE Cloud 2025 as WIP paper. The\n  final version will appear in IEEE Xplore", "summary": "The increasing adoption of large language models (LLMs) with extended context\nwindows necessitates efficient Key-Value Cache (KVC) management to optimize\ninference performance. Inference workloads like Retrieval-Augmented Generation\n(RAG) and agents exhibit high cache reusability, making efficient caching\ncritical to reducing redundancy and improving speed. We analyze real-world KVC\naccess patterns using publicly available traces and evaluate commercial\nkey-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1]\nand Sherman [2]) for KVC metadata management. Our work demonstrates the lack of\ntailored storage solution for KVC prefilling, underscores the need for an\nefficient distributed caching system with optimized metadata management for LLM\nworkloads, and provides insights into designing improved KVC management systems\nfor scalable, low-latency inference.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u9ad8\u6548\u952e\u503c\u7f13\u5b58\uff08KVC\uff09\u7ba1\u7406\u7684\u5fc5\u8981\u6027\uff0c\u5206\u6790\u4e86\u5b9e\u9645KVC\u8bbf\u95ee\u6a21\u5f0f\uff0c\u5e76\u8bc4\u4f30\u4e86\u73b0\u6709\u952e\u503c\u5b58\u50a8\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u6539\u8fdbKVC\u7ba1\u7406\u7684\u8bbe\u8ba1\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u6269\u5c55\uff0c\u9ad8\u6548\u7684\u952e\u503c\u7f13\u5b58\u7ba1\u7406\u5bf9\u63a8\u7406\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7cfb\u7edf\u7f3a\u4e4f\u9488\u5bf9KVC\u9884\u586b\u5145\u7684\u5b9a\u5236\u89e3\u51b3\u65b9\u6848\uff0c\u4e9f\u9700\u4f18\u5316\u7684\u5206\u5e03\u5f0f\u7f13\u5b58\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u5206\u6790\u516c\u5f00\u53ef\u7528\u7684KVC\u8bbf\u95ee\u6a21\u5f0f\u6570\u636e\uff0c\u8bc4\u4f30Redis\u3001CHIME\u548cSherman\u7b49\u952e\u503c\u5b58\u50a8\u7cfb\u7edf\u5728KVC\u5143\u6570\u636e\u7ba1\u7406\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u8868\u660e\u73b0\u6709\u7cfb\u7edf\u672a\u80fd\u6709\u6548\u6ee1\u8db3KVC\u7ba1\u7406\u7684\u9700\u6c42\uff0c\u7a81\u51fa\u4e86\u4f18\u5316\u5143\u6570\u636e\u7ba1\u7406\u548c\u8bbe\u8ba1\u9ad8\u6548\u5206\u5e03\u5f0f\u7f13\u5b58\u7cfb\u7edf\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u4e86\u4e3aLLM\u5de5\u4f5c\u8d1f\u8f7d\u8bbe\u8ba1\u4f4e\u5ef6\u8fdf\u3001\u53ef\u6269\u5c55\u7684KVC\u7ba1\u7406\u7cfb\u7edf\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u952e\u503c\u7f13\u5b58, \u5143\u6570\u636e\u7ba1\u7406, \u5206\u5e03\u5f0f\u7f13\u5b58, \u63a8\u7406\u6027\u80fd"}}
{"id": "2505.22457", "pdf": "https://arxiv.org/pdf/2505.22457", "abs": "https://arxiv.org/abs/2505.22457", "authors": ["Haonan Wang", "Hongfu Liu", "Xiangyan Liu", "Chao Du", "Kenji Kawaguchi", "Ye Wang", "Tianyu Pang"], "title": "Fostering Video Reasoning via Next-Event Prediction", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Next-token prediction serves as the foundational learning task enabling\nreasoning in LLMs. But what should the learning task be when aiming to equip\nMLLMs with temporal reasoning capabilities over video inputs? Existing tasks\nsuch as video question answering often rely on annotations from humans or much\nstronger MLLMs, while video captioning tends to entangle temporal reasoning\nwith spatial information. To address this gap, we propose next-event prediction\n(NEP), a learning task that harnesses future video segments as a rich,\nself-supervised signal to foster temporal reasoning. We segment each video into\npast and future frames: the MLLM takes the past frames as input and predicts a\nsummary of events derived from the future frames, thereby encouraging the model\nto reason temporally in order to complete the task. To support this task, we\ncurate V1-33K, a dataset comprising 33,000 automatically extracted video\nsegments spanning diverse real-world scenarios. We further explore a range of\nvideo instruction-tuning strategies to study their effects on temporal\nreasoning. To evaluate progress, we introduce FutureBench to assess coherence\nin predicting unseen future events. Experiments validate that NEP offers a\nscalable and effective training paradigm for fostering temporal reasoning in\nMLLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u201c\u4e0b\u4e00\u4e2a\u4e8b\u4ef6\u9884\u6d4b\u201d\uff08NEP\uff09\u4efb\u52a1\uff0c\u901a\u8fc7\u5229\u7528\u672a\u6765\u89c6\u9891\u7247\u6bb5\u4f5c\u4e3a\u81ea\u76d1\u7763\u4fe1\u53f7\u6765\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u3002\u4f5c\u8005\u8fd8\u6784\u5efa\u4e86V1-33K\u6570\u636e\u96c6\u548cFutureBench\u8bc4\u4f30\u5de5\u5177\uff0c\u9a8c\u8bc1\u4e86NEP\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u4efb\u52a1\uff08\u5982\u89c6\u9891\u95ee\u7b54\u6216\u89c6\u9891\u5b57\u5e55\u751f\u6210\uff09\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6216\u5f3a\u76d1\u7763\u4fe1\u53f7\uff0c\u4e14\u5bb9\u6613\u6df7\u6dc6\u65f6\u7a7a\u4fe1\u606f\u3002NEP\u65e8\u5728\u901a\u8fc7\u81ea\u76d1\u7763\u4efb\u52a1\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e13\u95e8\u63d0\u5347MLLMs\u7684\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5c06\u89c6\u9891\u5206\u4e3a\u8fc7\u53bb\u548c\u672a\u6765\u5e27\uff0cMLLMs\u57fa\u4e8e\u8fc7\u53bb\u5e27\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\u7684\u6458\u8981\u3002\u901a\u8fc7\u6784\u5efaV1-33K\u6570\u636e\u96c6\u548c\u591a\u79cd\u89c6\u9891\u6307\u4ee4\u8c03\u4f18\u7b56\u7565\uff0c\u7814\u7a76\u5176\u5bf9\u65f6\u95f4\u63a8\u7406\u7684\u5f71\u54cd\u3002\u5e76\u4f7f\u7528FutureBench\u8bc4\u4f30\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\u7684\u8fde\u8d2f\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eNEP\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u80fd\u663e\u8457\u63d0\u5347MLLMs\u7684\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "NEP\u4e3aMLLMs\u7684\u65f6\u95f4\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u5de5\u5177\u9a8c\u8bc1\u4e86\u5176\u6f5c\u529b\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u65f6\u95f4\u63a8\u7406\u3001\u4e0b\u4e00\u4e8b\u4ef6\u9884\u6d4b\u3001\u81ea\u76d1\u7763\u5b66\u4e60\u3001\u89c6\u9891\u7406\u89e3"}}
{"id": "2505.22655", "pdf": "https://arxiv.org/pdf/2505.22655", "abs": "https://arxiv.org/abs/2505.22655", "authors": ["Michael Kirchhof", "Gjergji Kasneci", "Enkelejda Kasneci"], "title": "Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted at ICML 2025", "summary": "Large-language models (LLMs) and chatbot agents are known to provide wrong\noutputs at times, and it was recently found that this can never be fully\nprevented. Hence, uncertainty quantification plays a crucial role, aiming to\nquantify the level of ambiguity in either one overall number or two numbers for\naleatoric and epistemic uncertainty. This position paper argues that this\ntraditional dichotomy of uncertainties is too limited for the open and\ninteractive setup that LLM agents operate in when communicating with a user,\nand that we need to research avenues that enrich uncertainties in this novel\nscenario. We review the literature and find that popular definitions of\naleatoric and epistemic uncertainties directly contradict each other and lose\ntheir meaning in interactive LLM agent settings. Hence, we propose three novel\nresearch directions that focus on uncertainties in such human-computer\ninteractions: Underspecification uncertainties, for when users do not provide\nall information or define the exact task at the first go, interactive learning,\nto ask follow-up questions and reduce the uncertainty about the current\ncontext, and output uncertainties, to utilize the rich language and speech\nspace to express uncertainties as more than mere numbers. We expect that these\nnew ways of dealing with and communicating uncertainties will lead to LLM agent\ninteractions that are more transparent, trustworthy, and intuitive.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4ea4\u4e92\u573a\u666f\u4e0b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e09\u79cd\u65b0\u7814\u7a76\u65b9\u5411\u4ee5\u63d0\u5347\u900f\u660e\u5ea6\u548c\u4fe1\u4efb\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7684\u4e0d\u786e\u5b9a\u6027\u4e8c\u5206\u6cd5\u5728LLM\u4e0e\u7528\u6237\u4ea4\u4e92\u7684\u5f00\u653e\u73af\u5883\u4e2d\u8fc7\u4e8e\u5c40\u9650\uff0c\u9700\u63a2\u7d22\u66f4\u4e30\u5bcc\u7684\u4e0d\u786e\u5b9a\u6027\u8868\u8fbe\u65b9\u5f0f\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u53d1\u73b0\u77db\u76fe\uff0c\u5e76\u63d0\u51fa\u4e09\u79cd\u65b0\u65b9\u5411\uff1a\u672a\u660e\u786e\u6027\u4e0d\u786e\u5b9a\u6027\u3001\u4ea4\u4e92\u5f0f\u5b66\u4e60\u548c\u8f93\u51fa\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u4f20\u7edf\u4e0d\u786e\u5b9a\u6027\u5b9a\u4e49\u5728\u4ea4\u4e92\u573a\u666f\u4e2d\u5931\u6548\uff0c\u65b0\u65b9\u5411\u80fd\u66f4\u900f\u660e\u5730\u8868\u8fbe\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u6709\u671b\u4f7fLLM\u4ea4\u4e92\u66f4\u900f\u660e\u3001\u53ef\u4fe1\u4e14\u76f4\u89c2\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u4e0d\u786e\u5b9a\u6027\u91cf\u5316,\u4ea4\u4e92\u5f0f\u5b66\u4e60,\u672a\u660e\u786e\u6027,\u8f93\u51fa\u4e0d\u786e\u5b9a\u6027"}}
{"id": "2505.22487", "pdf": "https://arxiv.org/pdf/2505.22487", "abs": "https://arxiv.org/abs/2505.22487", "authors": ["Yen Meng", "Sharon Goldwater", "Hao Tang"], "title": "Effective Context in Neural Speech Models", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Modern neural speech models benefit from having longer context, and many\napproaches have been proposed to increase the maximum context a model can use.\nHowever, few have attempted to measure how much context these models actually\nuse, i.e., the effective context. Here, we propose two approaches to measuring\nthe effective context, and use them to analyze different speech Transformers.\nFor supervised models, we find that the effective context correlates well with\nthe nature of the task, with fundamental frequency tracking, phone\nclassification, and word classification requiring increasing amounts of\neffective context. For self-supervised models, we find that effective context\nincreases mainly in the early layers, and remains relatively short -- similar\nto the supervised phone model. Given that these models do not use a long\ncontext during prediction, we show that HuBERT can be run in streaming mode\nwithout modification to the architecture and without further fine-tuning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u6d4b\u91cf\u8bed\u97f3Transformer\u6a21\u578b\u5b9e\u9645\u4f7f\u7528\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u4efb\u52a1\u7c7b\u578b\u5bf9\u4e0a\u4e0b\u6587\u9700\u6c42\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u81ea\u76d1\u7763\u6a21\u578b\u7684\u6709\u6548\u4e0a\u4e0b\u6587\u8f83\u77ed\u4e14\u53ef\u7528\u4e8e\u6d41\u5f0f\u5904\u7406\u3002", "motivation": "\u73b0\u6709\u7684\u795e\u7ecf\u8bed\u97f3\u6a21\u578b\u867d\u7136\u652f\u6301\u66f4\u957f\u7684\u4e0a\u4e0b\u6587\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u6a21\u578b\u5b9e\u9645\u4f7f\u7528\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\uff08\u6709\u6548\u4e0a\u4e0b\u6587\uff09\u7684\u91cf\u5316\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u6d4b\u91cf\u6709\u6548\u4e0a\u4e0b\u6587\u7684\u65b9\u6cd5\uff0c\u5206\u522b\u5e94\u7528\u4e8e\u76d1\u7763\u548c\u81ea\u76d1\u7763\u7684\u8bed\u97f3Transformer\u6a21\u578b\uff0c\u5206\u6790\u4efb\u52a1\u7c7b\u578b\u548c\u6a21\u578b\u5c42\u6b21\u5bf9\u4e0a\u4e0b\u6587\u9700\u6c42\u7684\u5f71\u54cd\u3002", "result": "\u76d1\u7763\u6a21\u578b\u7684\u6709\u6548\u4e0a\u4e0b\u6587\u4e0e\u4efb\u52a1\u7c7b\u578b\u76f8\u5173\uff08\u5982\u57fa\u9891\u8ddf\u8e2a\u3001\u97f3\u7d20\u5206\u7c7b\u548c\u8bcd\u6c47\u5206\u7c7b\u5bf9\u4e0a\u4e0b\u6587\u9700\u6c42\u9012\u589e\uff09\uff0c\u800c\u81ea\u76d1\u7763\u6a21\u578b\u7684\u6709\u6548\u4e0a\u4e0b\u6587\u4e3b\u8981\u96c6\u4e2d\u5728\u65e9\u671f\u5c42\u4e14\u8f83\u77ed\uff0c\u65e0\u9700\u4fee\u6539\u67b6\u6784\u5373\u53ef\u7528\u4e8e\u6d41\u5f0f\u5904\u7406\u3002", "conclusion": "\u8bed\u97f3\u6a21\u578b\u7684\u6709\u6548\u4e0a\u4e0b\u6587\u957f\u77ed\u4e0e\u4efb\u52a1\u76f8\u5173\uff0c\u81ea\u76d1\u7763\u6a21\u578b\u7684\u6d41\u5f0f\u5904\u7406\u6f5c\u529b\u672a\u88ab\u5145\u5206\u5229\u7528\u3002", "keywords": "\u8bed\u97f3Transformer\uff0c\u6709\u6548\u4e0a\u4e0b\u6587\uff0c\u76d1\u7763\u5b66\u4e60\uff0c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u6d41\u5f0f\u5904\u7406"}}
{"id": "2505.22660", "pdf": "https://arxiv.org/pdf/2505.22660", "abs": "https://arxiv.org/abs/2505.22660", "authors": ["Mihir Prabhudesai", "Lili Chen", "Alex Ippoliti", "Katerina Fragkiadaki", "Hao Liu", "Deepak Pathak"], "title": "Maximizing Confidence Alone Improves Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) has enabled machine learning models to achieve\nsignificant advances in many fields. Most recently, RL has empowered frontier\nlanguage models to solve challenging math, science, and coding problems.\nHowever, central to any RL algorithm is the reward function, and reward\nengineering is a notoriously difficult problem in any domain. In this paper, we\npropose RENT: Reinforcement Learning via Entropy Minimization -- a fully\nunsupervised RL method that requires no external reward or ground-truth\nanswers, and instead uses the model's entropy of its underlying distribution as\nan intrinsic reward. We find that by reinforcing the chains of thought that\nyield high model confidence on its generated answers, the model improves its\nreasoning ability. In our experiments, we showcase these improvements on an\nextensive suite of commonly-used reasoning benchmarks, including GSM8K,\nMATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen and\nMistral families. The generality of our unsupervised learning method lends\nitself to applicability in a wide range of domains where external supervision\nis limited or unavailable.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRENT\u7684\u65e0\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u6a21\u578b\u71b5\u4f5c\u4e3a\u5185\u5728\u5956\u52b1\uff0c\u65e0\u9700\u5916\u90e8\u5956\u52b1\u6216\u771f\u5b9e\u7b54\u6848\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4f9d\u8d56\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\uff0c\u4f46\u5956\u52b1\u5de5\u7a0b\u5728\u591a\u4e2a\u9886\u57df\u90fd\u975e\u5e38\u56f0\u96be\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u76d1\u7763\u7684\u81ea\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRENT\u65b9\u6cd5\uff0c\u5229\u7528\u6a21\u578b\u751f\u6210\u7b54\u6848\u65f6\u7684\u71b5\u4f5c\u4e3a\u5185\u5728\u5956\u52b1\uff0c\u901a\u8fc7\u5f3a\u5316\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u63a8\u7406\u94fe\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728GSM8K\u3001MATH500\u7b49\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRENT\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u9002\u7528\u4e8e\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\uff08\u5982Qwen\u548cMistral\u7cfb\u5217\uff09\u3002", "conclusion": "RENT\u65b9\u6cd5\u7684\u901a\u7528\u6027\u4f7f\u5176\u9002\u7528\u4e8e\u5916\u90e8\u76d1\u7763\u6709\u9650\u6216\u7f3a\u5931\u7684\u5e7f\u6cdb\u9886\u57df\uff0c\u4e3a\u65e0\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u65e0\u76d1\u7763\u5b66\u4e60, \u71b5\u6700\u5c0f\u5316, \u63a8\u7406\u80fd\u529b, RENT"}}
{"id": "2505.22525", "pdf": "https://arxiv.org/pdf/2505.22525", "abs": "https://arxiv.org/abs/2505.22525", "authors": ["Ethan Chern", "Zhulin Hu", "Steffi Chern", "Siqi Kou", "Jiadi Su", "Yan Ma", "Zhijie Deng", "Pengfei Liu"], "title": "Thinking with Generated Images", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present Thinking with Generated Images, a novel paradigm that\nfundamentally transforms how large multimodal models (LMMs) engage with visual\nreasoning by enabling them to natively think across text and vision modalities\nthrough spontaneous generation of intermediate visual thinking steps. Current\nvisual reasoning with LMMs is constrained to either processing fixed\nuser-provided images or reasoning solely through text-based chain-of-thought\n(CoT). Thinking with Generated Images unlocks a new dimension of cognitive\ncapability where models can actively construct intermediate visual thoughts,\ncritique their own visual hypotheses, and refine them as integral components of\ntheir reasoning process. We demonstrate the effectiveness of our approach\nthrough two complementary mechanisms: (1) vision generation with intermediate\nvisual subgoals, where models decompose complex visual tasks into manageable\ncomponents that are generated and integrated progressively, and (2) vision\ngeneration with self-critique, where models generate an initial visual\nhypothesis, analyze its shortcomings through textual reasoning, and produce\nrefined outputs based on their own critiques. Our experiments on vision\ngeneration benchmarks show substantial improvements over baseline approaches,\nwith our models achieving up to 50% (from 38% to 57%) relative improvement in\nhandling complex multi-object scenarios. From biochemists exploring novel\nprotein structures, and architects iterating on spatial designs, to forensic\nanalysts reconstructing crime scenes, and basketball players envisioning\nstrategic plays, our approach enables AI models to engage in the kind of visual\nimagination and iterative refinement that characterizes human creative,\nanalytical, and strategic thinking. We release our open-source suite at\nhttps://github.com/GAIR-NLP/thinking-with-generated-images.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u8ba9\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u81ea\u53d1\u751f\u6210\u4e2d\u95f4\u89c6\u89c9\u601d\u8003\u6b65\u9aa4\uff0c\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u4e86\u5176\u4e0e\u89c6\u89c9\u63a8\u7406\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002", "motivation": "\u5f53\u524dLMMs\u7684\u89c6\u89c9\u63a8\u7406\u4ec5\u9650\u4e8e\u5904\u7406\u56fa\u5b9a\u7684\u7528\u6237\u63d0\u4f9b\u56fe\u50cf\u6216\u4ec5\u901a\u8fc7\u6587\u672c\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u8fdb\u884c\u63a8\u7406\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u8ba4\u77e5\u80fd\u529b\u3002", "method": "1. \u901a\u8fc7\u89c6\u89c9\u751f\u6210\u4e0e\u4e2d\u95f4\u89c6\u89c9\u5b50\u76ee\u6807\u5206\u89e3\u590d\u6742\u4efb\u52a1\uff1b2. \u901a\u8fc7\u81ea\u6211\u6279\u8bc4\u673a\u5236\u751f\u6210\u521d\u59cb\u89c6\u89c9\u5047\u8bbe\u5e76\u9010\u6b65\u4f18\u5316\u3002", "result": "\u5728\u89c6\u89c9\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u5904\u7406\u590d\u6742\u591a\u5bf9\u8c61\u573a\u666f\u7684\u6027\u80fd\u76f8\u5bf9\u63d0\u5347\u4e8650%\uff08\u4ece38%\u523057%\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7c7b\u4f3c\u4eba\u7c7b\u89c6\u89c9\u60f3\u8c61\u548c\u8fed\u4ee3\u4f18\u5316\u7684\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u591a\u4e2a\u9886\u57df\uff08\u5982\u751f\u7269\u5316\u5b66\u3001\u5efa\u7b51\u8bbe\u8ba1\u7b49\uff09\u3002", "keywords": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b, \u89c6\u89c9\u63a8\u7406, \u751f\u6210\u56fe\u50cf, \u81ea\u6211\u6279\u8bc4, \u4e2d\u95f4\u601d\u8003\u6b65\u9aa4"}}
{"id": "2505.21928", "pdf": "https://arxiv.org/pdf/2505.21928", "abs": "https://arxiv.org/abs/2505.21928", "authors": ["Lianghui Zhu", "Xitong Ling", "Minxi Ouyang", "Xiaoping Liu", "Mingxi Fu", "Tian Guan", "Fanglei Fu", "Xuanyu Wang", "Maomao Zeng", "Mingxi Zhu", "Yibo Jin", "Liming Liu", "Song Duan", "Qiming He", "Yizhi Wang", "Luxi Xie", "Houqiang Li", "Yonghong He", "Sufang Tian"], "title": "Subspecialty-Specific Foundation Model for Intelligent Gastrointestinal Pathology", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Gastrointestinal (GI) diseases represent a clinically significant burden,\nnecessitating precise diagnostic approaches to optimize patient outcomes.\nConventional histopathological diagnosis, heavily reliant on the subjective\ninterpretation of pathologists, suffers from limited reproducibility and\ndiagnostic variability. To overcome these limitations and address the lack of\npathology-specific foundation models for GI diseases, we develop Digepath, a\nspecialized foundation model for GI pathology. Our framework introduces a\ndual-phase iterative optimization strategy combining pretraining with\nfine-screening, specifically designed to address the detection of sparsely\ndistributed lesion areas in whole-slide images. Digepath is pretrained on more\nthan 353 million image patches from over 200,000 hematoxylin and eosin-stained\nslides of GI diseases. It attains state-of-the-art performance on 33 out of 34\ntasks related to GI pathology, including pathological diagnosis, molecular\nprediction, gene mutation prediction, and prognosis evaluation, particularly in\ndiagnostically ambiguous cases and resolution-agnostic tissue classification.We\nfurther translate the intelligent screening module for early GI cancer and\nachieve near-perfect 99.6% sensitivity across 9 independent medical\ninstitutions nationwide. The outstanding performance of Digepath highlights its\npotential to bridge critical gaps in histopathological practice. This work not\nonly advances AI-driven precision pathology for GI diseases but also\nestablishes a transferable paradigm for other pathology subspecialties.", "AI": {"tldr": "Digepath\u662f\u4e00\u4e2a\u4e13\u4e3a\u80c3\u80a0\u9053\u75c5\u7406\u8bbe\u8ba1\u7684AI\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u9636\u6bb5\u8fed\u4ee3\u4f18\u5316\u7b56\u7565\u5728\u75c5\u7406\u8bca\u65ad\u3001\u5206\u5b50\u9884\u6d4b\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u65e9\u671f\u764c\u75c7\u7b5b\u67e5\u4e2d\u8fbe\u523099.6%\u7684\u654f\u611f\u6027\u3002", "motivation": "\u4f20\u7edf\u75c5\u7406\u8bca\u65ad\u4f9d\u8d56\u4e3b\u89c2\u89e3\u8bfb\uff0c\u5b58\u5728\u53ef\u91cd\u590d\u6027\u4f4e\u548c\u8bca\u65ad\u53d8\u5f02\u6027\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u80c3\u80a0\u9053\u75be\u75c5\u7684AI\u6a21\u578b\u6765\u4f18\u5316\u8bca\u65ad\u7cbe\u5ea6\u3002", "method": "\u5f00\u53d1Digepath\u6a21\u578b\uff0c\u91c7\u7528\u53cc\u9636\u6bb5\u8fed\u4ee3\u4f18\u5316\u7b56\u7565\uff08\u9884\u8bad\u7ec3\u4e0e\u7cbe\u7ec6\u7b5b\u9009\uff09\uff0c\u57fa\u4e8e\u8d85\u8fc735.3\u4ebf\u4e2a\u56fe\u50cf\u5757\u548c20\u4e07\u5f20\u80c3\u80a0\u9053\u75be\u75c5H&E\u67d3\u8272\u5207\u7247\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "Digepath\u572834\u9879\u4efb\u52a1\u4e2d\u768433\u9879\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5305\u62ec\u75c5\u7406\u8bca\u65ad\u3001\u5206\u5b50\u9884\u6d4b\u7b49\uff0c\u5e76\u5728\u65e9\u671f\u764c\u75c7\u7b5b\u67e5\u4e2d\u5b9e\u73b099.6%\u7684\u654f\u611f\u6027\u3002", "conclusion": "Digepath\u586b\u8865\u75c5\u7406\u5b9e\u8df5\u4e2d\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e0d\u4ec5\u63a8\u52a8AI\u9a71\u52a8\u7684\u80c3\u80a0\u9053\u75be\u75c5\u7cbe\u51c6\u75c5\u7406\u5b66\uff0c\u8fd8\u4e3a\u5176\u4ed6\u75c5\u7406\u4e9a\u4e13\u79d1\u63d0\u4f9b\u53ef\u8fc1\u79fb\u7684\u8303\u4f8b\u3002", "keywords": "\u80c3\u80a0\u9053\u75be\u75c5\u3001AI\u57fa\u7840\u6a21\u578b\u3001\u75c5\u7406\u8bca\u65ad\u3001\u53cc\u9636\u6bb5\u4f18\u5316\u3001Digepath"}}
{"id": "2505.22613", "pdf": "https://arxiv.org/pdf/2505.22613", "abs": "https://arxiv.org/abs/2505.22613", "authors": ["Yuchi Wang", "Yishuo Cai", "Shuhuai Ren", "Sihan Yang", "Linli Yao", "Yuanxin Liu", "Yuanxing Zhang", "Pengfei Wan", "Xu Sun"], "title": "RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "code: https://github.com/wangyuchi369/RICO", "summary": "Image recaptioning is widely used to generate training datasets with enhanced\nquality for various multimodal tasks. Existing recaptioning methods typically\nrely on powerful multimodal large language models (MLLMs) to enhance textual\ndescriptions, but often suffer from inaccuracies due to hallucinations and\nincompleteness caused by missing fine-grained details. To address these\nlimitations, we propose RICO, a novel framework that refines captions through\nvisual reconstruction. Specifically, we leverage a text-to-image model to\nreconstruct a caption into a reference image, and prompt an MLLM to identify\ndiscrepancies between the original and reconstructed images to refine the\ncaption. This process is performed iteratively, further progressively promoting\nthe generation of more faithful and comprehensive descriptions. To mitigate the\nadditional computational cost induced by the iterative process, we introduce\nRICO-Flash, which learns to generate captions like RICO using DPO. Extensive\nexperiments demonstrate that our approach significantly improves caption\naccuracy and completeness, outperforms most baselines by approximately 10% on\nboth CapsBench and CompreCap. Code released at\nhttps://github.com/wangyuchi369/RICO.", "AI": {"tldr": "RICO \u662f\u4e00\u4e2a\u901a\u8fc7\u89c6\u89c9\u91cd\u5efa\u4f18\u5316\u56fe\u50cf\u6807\u6ce8\u7684\u6846\u67b6\uff0c\u5229\u7528\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u751f\u6210\u53c2\u8003\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fed\u4ee3\u4f18\u5316\u6807\u6ce8\uff0c\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u6807\u6ce8\u65b9\u6cd5\u5e38\u56e0\u5e7b\u89c9\u6216\u7ec6\u8282\u7f3a\u5931\u5bfc\u81f4\u4e0d\u51c6\u786e\uff0cRICO \u65e8\u5728\u901a\u8fc7\u89c6\u89c9\u91cd\u5efa\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u5347\u6807\u6ce8\u8d28\u91cf\u3002", "method": "\u4f7f\u7528\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u91cd\u5efa\u6807\u6ce8\u4e3a\u53c2\u8003\u56fe\u50cf\uff0c\u901a\u8fc7 MLLM \u8bc6\u522b\u5dee\u5f02\u5e76\u8fed\u4ee3\u4f18\u5316\u6807\u6ce8\uff1bRICO-Flash \u901a\u8fc7 DPO \u5b66\u4e60\u751f\u6210\u7c7b\u4f3c RICO \u7684\u6807\u6ce8\u4ee5\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a RICO \u5728 CapsBench \u548c CompreCap \u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7ea6 10%\u3002", "conclusion": "RICO \u901a\u8fc7\u89c6\u89c9\u91cd\u5efa\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u751f\u6210\u4e86\u66f4\u51c6\u786e\u3001\u66f4\u5168\u9762\u7684\u56fe\u50cf\u6807\u6ce8\uff0c\u540c\u65f6 RICO-Flash \u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "keywords": "\u56fe\u50cf\u6807\u6ce8, \u89c6\u89c9\u91cd\u5efa, \u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b, \u8fed\u4ee3\u4f18\u5316, DPO"}}
{"id": "2505.18478", "pdf": "https://arxiv.org/pdf/2505.18478", "abs": "https://arxiv.org/abs/2505.18478", "authors": ["Lucas Tecot", "Di Luo", "Cho-Jui Hsieh"], "title": "Provably Robust Training of Quantum Circuit Classifiers Against Parameter Noise", "categories": ["quant-ph", "cs.LG", "physics.comp-ph"], "comment": "14 pages, 3 figures", "summary": "Advancements in quantum computing have spurred significant interest in\nharnessing its potential for speedups over classical systems. However, noise\nremains a major obstacle to achieving reliable quantum algorithms. In this\nwork, we present a provably noise-resilient training theory and algorithm to\nenhance the robustness of parameterized quantum circuit classifiers. Our\nmethod, with a natural connection to Evolutionary Strategies, guarantees\nresilience to parameter noise with minimal adjustments to commonly used\noptimization algorithms. Our approach is function-agnostic and adaptable to\nvarious quantum circuits, successfully demonstrated in quantum phase\nclassification tasks. By developing provably guaranteed optimization theory\nwith quantum circuits, our work opens new avenues for practical, robust\napplications of near-term quantum computers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u8bc1\u660e\u6297\u566a\u58f0\u7684\u8bad\u7ec3\u7406\u8bba\u548c\u7b97\u6cd5\uff0c\u7528\u4e8e\u589e\u5f3a\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\u5206\u7c7b\u5668\u7684\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u7c7b\u4f3c\u4e8e\u8fdb\u5316\u7b56\u7565\uff0c\u80fd\u62b5\u6297\u53c2\u6570\u566a\u58f0\u4e14\u6613\u4e8e\u8c03\u6574\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u867d\u5177\u6f5c\u529b\uff0c\u4f46\u566a\u97f3\u95ee\u9898\u9650\u5236\u4e86\u7b97\u6cd5\u53ef\u9760\u6027\uff0c\u9700\u5f00\u53d1\u7406\u8bba\u548c\u65b9\u6cd5\u63d0\u5347\u6297\u566a\u58f0\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u53ef\u8bc1\u660e\u7684\u6297\u566a\u58f0\u8bad\u7ec3\u7b97\u6cd5\uff0c\u4e0e\u8fdb\u5316\u7b56\u7565\u7c7b\u4f3c\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u91cf\u5b50\u7535\u8def\uff0c\u4e14\u5bf9\u4f18\u5316\u7b97\u6cd5\u53ea\u9700\u6700\u5c0f\u8c03\u6574\u3002", "result": "\u5728\u91cf\u5b50\u76f8\u4f4d\u5206\u7c7b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u4e3a\u8fd1\u91cf\u5b50\u8ba1\u7b97\u673a\u7684\u5b9e\u7528\u5316\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u91cf\u5b50\u8ba1\u7b97\u566a\u58f0\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u63a8\u52a8\u4e86\u8fd1\u671f\u91cf\u5b50\u8ba1\u7b97\u673a\u7684\u5b9e\u7528\u5316\u53d1\u5c55\u3002", "keywords": "quantum computing, noise, training theory, evolutionary strategies, robustness"}}
{"id": "2505.21954", "pdf": "https://arxiv.org/pdf/2505.21954", "abs": "https://arxiv.org/abs/2505.21954", "authors": ["Le Thien Phuc Nguyen", "Zhuoran Yu", "Khoa Quang Nhat Cao", "Yuwei Guo", "Tu Ho Manh Pham", "Tuan Tai Nguyen", "Toan Ngo Duc Vo", "Lucas Poon", "Soochahn Lee", "Yong Jae Lee"], "title": "UniTalk: Towards Universal Active Speaker Detection in Real World Scenarios", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present UniTalk, a novel dataset specifically designed for the task of\nactive speaker detection, emphasizing challenging scenarios to enhance model\ngeneralization. Unlike previously established benchmarks such as AVA, which\npredominantly features old movies and thus exhibits significant domain gaps,\nUniTalk focuses explicitly on diverse and difficult real-world conditions.\nThese include underrepresented languages, noisy backgrounds, and crowded scenes\n- such as multiple visible speakers speaking concurrently or in overlapping\nturns. It contains over 44.5 hours of video with frame-level active speaker\nannotations across 48,693 speaking identities, and spans a broad range of video\ntypes that reflect real-world conditions. Through rigorous evaluation, we show\nthat state-of-the-art models, while achieving nearly perfect scores on AVA,\nfail to reach saturation on UniTalk, suggesting that the ASD task remains far\nfrom solved under realistic conditions. Nevertheless, models trained on UniTalk\ndemonstrate stronger generalization to modern \"in-the-wild\" datasets like\nTalkies and ASW, as well as to AVA. UniTalk thus establishes a new benchmark\nfor active speaker detection, providing researchers with a valuable resource\nfor developing and evaluating versatile and resilient models.\n  Dataset: https://huggingface.co/datasets/plnguyen2908/UniTalk-ASD\n  Code: https://github.com/plnguyen2908/UniTalk-ASD-code", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aUniTalk\u7684\u65b0\u6570\u636e\u96c6\uff0c\u4e13\u4e3a\u4e3b\u52a8\u8bf4\u8bdd\u8005\u68c0\u6d4b\uff08ASD\uff09\u4efb\u52a1\u8bbe\u8ba1\uff0c\u5f3a\u8c03\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6311\u6218\u6027\u573a\u666f\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4e0e\u73b0\u6709\u57fa\u51c6\uff08\u5982AVA\uff09\u76f8\u6bd4\uff0cUniTalk\u66f4\u6ce8\u91cd\u591a\u6837\u6027\u548c\u96be\u5ea6\uff0c\u8986\u76d6\u4e86\u591a\u79cd\u8bed\u8a00\u3001\u5608\u6742\u80cc\u666f\u548c\u62e5\u6324\u573a\u666f\u3002\u6570\u636e\u96c6\u5305\u542b44.5\u5c0f\u65f6\u89c6\u9891\u548c48,693\u4e2a\u8bf4\u8bdd\u8eab\u4efd\u7684\u5e27\u7ea7\u6807\u6ce8\u3002\u8bc4\u4f30\u663e\u793a\uff0c\u73b0\u6709\u6a21\u578b\u5728AVA\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728UniTalk\u4e0a\u8fdc\u672a\u9971\u548c\u3002\u7136\u800c\uff0cUniTalk\u8bad\u7ec3\u7684\u6a21\u578b\u5728Talkies\u3001ASW\u548cAVA\u7b49\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u4e86\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709ASD\u6570\u636e\u96c6\uff08\u5982AVA\uff09\u4e3b\u8981\u57fa\u4e8e\u65e7\u7535\u5f71\uff0c\u4e0e\u73b0\u5b9e\u573a\u666f\u5b58\u5728\u663e\u8457\u9886\u57df\u5dee\u8ddd\u3002UniTalk\u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u7f3a\u9677\uff0c\u63d0\u4f9b\u66f4\u8d34\u8fd1\u73b0\u5b9e\u3001\u66f4\u5177\u6311\u6218\u6027\u7684\u6570\u636e\uff0c\u4ee5\u63a8\u52a8ASD\u4efb\u52a1\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u4e86UniTalk\u6570\u636e\u96c6\uff0c\u6536\u96c6\u4e86\u591a\u6837\u5316\u7684\u73b0\u5b9e\u573a\u666f\u89c6\u9891\uff08\u5982\u591a\u8bed\u8a00\u3001\u5608\u6742\u80cc\u666f\u3001\u62e5\u6324\u573a\u666f\u7b49\uff09\uff0c\u5e76\u4e3a48,693\u4e2a\u8bf4\u8bdd\u8eab\u4efd\u63d0\u4f9b\u5e27\u7ea7\u6807\u6ce8\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u73b0\u6709\u6a21\u578b\u5728AVA\u548cUniTalk\u4e0a\u7684\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u73b0\u6709\u6a21\u578b\u5728AVA\u4e0a\u63a5\u8fd1\u9971\u548c\uff0c\u4f46\u5728UniTalk\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u8868\u660eASD\u4efb\u52a1\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u5c1a\u672a\u89e3\u51b3\u3002UniTalk\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u591a\u4e2a\u6570\u636e\u96c6\uff08Talkies\u3001ASW\u3001AVA\uff09\u4e0a\u5c55\u73b0\u4e86\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "UniTalk\u4e3aASD\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u548c\u5b9d\u8d35\u8d44\u6e90\uff0c\u63a8\u52a8\u5f00\u53d1\u66f4\u5177\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u7684\u6a21\u578b\u3002", "keywords": "UniTalk, \u4e3b\u52a8\u8bf4\u8bdd\u8005\u68c0\u6d4b, \u6570\u636e\u96c6, \u591a\u6837\u6027, \u6cdb\u5316\u80fd\u529b"}}
{"id": "2505.22651", "pdf": "https://arxiv.org/pdf/2505.22651", "abs": "https://arxiv.org/abs/2505.22651", "authors": ["Yi Ding", "Ruqi Zhang"], "title": "Sherlock: Self-Correcting Reasoning in Vision-Language Models", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "27 pages", "summary": "Reasoning Vision-Language Models (VLMs) have shown promising performance on\ncomplex multimodal tasks. However, they still face significant challenges: they\nare highly sensitive to reasoning errors, require large volumes of annotated\ndata or accurate verifiers, and struggle to generalize beyond specific domains.\nTo address these limitations, we explore self-correction as a strategy to\nenhance reasoning VLMs. We first conduct an in-depth analysis of reasoning\nVLMs' self-correction abilities and identify key gaps. Based on our findings,\nwe introduce Sherlock, a self-correction and self-improvement training\nframework. Sherlock introduces a trajectory-level self-correction objective, a\npreference data construction method based on visual perturbation, and a dynamic\n$\\beta$ for preference tuning. Once the model acquires self-correction\ncapabilities using only 20k randomly sampled annotated data, it continues to\nself-improve without external supervision. Built on the Llama3.2-Vision-11B\nmodel, Sherlock achieves remarkable results across eight benchmarks, reaching\nan average accuracy of 64.1 with direct generation and 65.4 after\nself-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and\nLlamaV-o1 (63.4) while using less than 20% of the annotated data.", "AI": {"tldr": "Sherlock\u662f\u4e00\u79cd\u81ea\u6211\u7ea0\u6b63\u548c\u6539\u8fdb\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4ec5\u9700\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u5373\u53ef\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u6216\u9a8c\u8bc1\u5668\uff0c\u4e14\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51faSherlock\u6846\u67b6\uff0c\u5305\u62ec\u8f68\u8ff9\u7ea7\u81ea\u6211\u7ea0\u6b63\u76ee\u6807\u3001\u57fa\u4e8e\u89c6\u89c9\u6270\u52a8\u7684\u504f\u597d\u6570\u636e\u6784\u5efa\u65b9\u6cd5\u548c\u52a8\u6001\u03b2\u504f\u597d\u8c03\u6574\u3002", "result": "\u5728\u516b\u5927\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSherlock\u5e73\u5747\u51c6\u786e\u7387\u8fbe64.1\uff08\u76f4\u63a5\u751f\u6210\uff09\u548c65.4\uff08\u81ea\u6211\u7ea0\u6b63\u540e\uff09\uff0c\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u4e14\u6570\u636e\u9700\u6c42\u51cf\u5c1180%\u3002", "conclusion": "\u81ea\u6211\u7ea0\u6b63\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u81ea\u6211\u7ea0\u6b63\u3001\u63a8\u7406\u80fd\u529b\u3001Sherlock\u6846\u67b6\u3001\u6570\u636e\u6548\u7387"}}
{"id": "2505.20344", "pdf": "https://arxiv.org/pdf/2505.20344", "abs": "https://arxiv.org/abs/2505.20344", "authors": ["Karen Ardila", "Aashka Mohite", "Abdoljalil Addeh", "Amanda V. Tyndall", "Cindy K. Barha", "Quan Long", "M. Ethan MacDonald"], "title": "Genetic Influences on Brain Aging: Analyzing Sex Differences in the UK Biobank using Structural MRI", "categories": ["q-bio.GN", "cs.LG"], "comment": "7 pages, 5 figures, conference", "summary": "Brain aging trajectories differ between males and females, yet the genetic\nfactors underlying these differences remain underexplored. Using structural MRI\nand genotyping data from 40,940 UK Biobank participants (aged 45-83), we\ncomputed Brain Age Gap Estimates (BrainAGE) for total brain, hippocampal, and\nventricular volumes. We conducted sex-stratified genome-wide association\nstudies (GWAS) and Post-GWAS analyses to identify genetic variants associated\nwith accelerated brain aging. Distinct gene sets emerged by sex: in females,\nneurotransmitter transport and mitochondrial stress response genes were\nimplicated; in males, immune and inflammation-related genes dominated. Shared\ngenes, including GMNC and OSTN, were consistently linked to brain volumes\nacross sexes, suggesting core roles in neurostructural maintenance. Tissue\nexpression analyses revealed sex-specific enrichment in pathways tied to\nneurodegeneration. These findings highlight the importance of sex-stratified\napproaches in aging research and suggest genetic targets for personalized\ninterventions against age-related cognitive decline.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5206\u679040,940\u540dUK Biobank\u53c2\u4e0e\u8005\u7684\u8111\u5f71\u50cf\u548c\u57fa\u56e0\u6570\u636e\uff0c\u63ed\u793a\u4e86\u5927\u8111\u8870\u8001\u7684\u6027\u522b\u5dee\u5f02\u53ca\u5176\u9057\u4f20\u57fa\u7840\u3002\u5973\u6027\u4e0e\u7537\u6027\u5206\u522b\u5173\u8054\u5230\u4e0d\u540c\u7684\u57fa\u56e0\u96c6\u5408\u548c\u901a\u8def\uff0c\u63d0\u793a\u6027\u522b\u5206\u5c42\u7684\u8870\u8001\u7814\u7a76\u53ca\u4e2a\u6027\u5316\u5e72\u9884\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5927\u8111\u8870\u8001\u5b58\u5728\u6027\u522b\u5dee\u5f02\uff0c\u4f46\u5176\u9057\u4f20\u56e0\u7d20\u5c1a\u4e0d\u660e\u786e\u3002\u7814\u7a76\u65e8\u5728\u63ed\u793a\u6027\u522b\u7279\u5f02\u6027\u57fa\u56e0\u5982\u4f55\u5f71\u54cd\u8111\u7ed3\u6784\u8870\u8001\u3002", "method": "\u91c7\u7528UK Biobank\u7684\u7ed3\u6784MRI\u548c\u57fa\u56e0\u6570\u636e\uff0c\u8ba1\u7b97BrainAGE\u5e76\u901a\u8fc7\u6027\u522b\u5206\u5c42\u7684GWAS\u53ca\u540e\u7eed\u5206\u6790\u8bc6\u522b\u76f8\u5173\u9057\u4f20\u53d8\u5f02\u3002", "result": "\u5973\u6027\u548c\u7537\u6027\u5206\u522b\u5173\u8054\u5230\u4e0d\u540c\u7684\u57fa\u56e0\u548c\u901a\u8def\uff08\u5982\u5973\u6027:\u795e\u7ecf\u9012\u8d28\u8fd0\u8f93\uff1b\u7537\u6027:\u514d\u75ab\u708e\u75c7\uff09\uff0c\u540c\u65f6\u53d1\u73b0\u4e86\u8de8\u6027\u522b\u5171\u4eab\u57fa\u56e0\uff08\u5982GMNC\u3001OSTN\uff09\u3002", "conclusion": "\u5f3a\u8c03\u6027\u522b\u5206\u5c42\u7814\u7a76\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u4e3a\u9488\u5bf9\u5e74\u9f84\u76f8\u5173\u8ba4\u77e5\u4e0b\u964d\u7684\u4e2a\u6027\u5316\u5e72\u9884\u63d0\u4f9b\u6f5c\u5728\u9057\u4f20\u9776\u70b9\u3002", "keywords": "\u8111\u8870\u8001, \u6027\u522b\u5dee\u5f02, GWAS, \u9057\u4f20\u56e0\u7d20, UK Biobank"}}
{"id": "2505.21955", "pdf": "https://arxiv.org/pdf/2505.21955", "abs": "https://arxiv.org/abs/2505.21955", "authors": ["Insu Lee", "Wooje Park", "Jaeyun Jang", "Minyoung Noh", "Kyuhong Shim", "Byonghyo Shim"], "title": "Towards Comprehensive Scene Understanding: Integrating First and Third-Person Views for LVLMs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large vision-language models (LVLMs) are increasingly deployed in interactive\napplications such as virtual and augmented reality, where first-person\n(egocentric) view captured by head-mounted cameras serves as key input. While\nthis view offers fine-grained cues about user attention and hand-object\ninteractions, their narrow field of view and lack of global context often lead\nto failures on spatially or contextually demanding queries. To address this, we\nintroduce a framework that augments egocentric inputs with third-person\n(exocentric) views, providing complementary information such as global scene\nlayout and object visibility to LVLMs. We present E3VQA, the first benchmark\nfor multi-view question answering with 4K high-quality question-answer pairs\ngrounded in synchronized ego-exo image pairs. Additionally, we propose M3CoT, a\ntraining-free prompting technique that constructs a unified scene\nrepresentation by integrating scene graphs from three complementary\nperspectives. M3CoT enables LVLMs to reason more effectively across views,\nyielding consistent performance gains (4.84% for GPT-4o and 5.94% for Gemini\n2.0 Flash) over a recent CoT baseline. Our extensive evaluation reveals key\nstrengths and limitations of LVLMs in multi-view reasoning and highlights the\nvalue of leveraging both egocentric and exocentric inputs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u7ed3\u5408\u7b2c\u4e00\u4eba\u79f0\uff08egocentric\uff09\u548c\u7b2c\u4e09\u4eba\u79f0\uff08exocentric\uff09\u89c6\u89d2\u7684\u591a\u89c6\u56fe\u6846\u67b6E3VQA\uff0c\u5e76\u63d0\u51fa\u4e86\u8bad\u7ec3\u65e0\u5173\u7684M3CoT\u63d0\u793a\u6280\u672f\uff0c\u63d0\u5347\u4e86LVLMs\u5728\u8de8\u89c6\u56fe\u63a8\u7406\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u9762\u5bf9\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u56e0\u89c6\u91ce\u72ed\u7a84\u548c\u7f3a\u4e4f\u5168\u5c40\u80cc\u666f\u800c\u5bfc\u81f4\u7684\u63a8\u7406\u5931\u8d25\u95ee\u9898\uff0c\u8bba\u6587\u5e0c\u671b\u901a\u8fc7\u591a\u89c6\u56fe\u8f93\u5165\u6765\u589e\u5f3aLVLMs\u7684\u8868\u73b0\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u901a\u8fc7\u591a\u89c6\u56fe\u95ee\u7b54\u57fa\u51c6E3VQA\u548cM3CoT\u63d0\u793a\u6280\u672f\uff0c\u7ed3\u5408\u4e0d\u540c\u89c6\u56fe\u4e0b\u7684\u573a\u666f\u56fe\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u573a\u666f\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86GPT-4o\u548cGemini 2.0 Flash\u7684\u6027\u80fd\uff08\u5206\u522b\u4e3a4.84%\u548c5.94%\uff09\u3002", "conclusion": "\u591a\u89c6\u56fe\u8f93\u5165\u548c\u591a\u89c6\u89d2\u573a\u666f\u56fe\u7684\u96c6\u6210\u80fd\u6709\u6548\u63d0\u5347LVLMs\u7684\u8de8\u89c6\u56fe\u63a8\u7406\u80fd\u529b\u3002", "keywords": "LVLMs, \u591a\u89c6\u56fe\u63a8\u7406, E3VQA, M3CoT, \u573a\u666f\u56fe"}}
{"id": "2505.21507", "pdf": "https://arxiv.org/pdf/2505.21507", "abs": "https://arxiv.org/abs/2505.21507", "authors": ["Aurore Bussalb", "Fran\u00e7ois Le Gac", "Guillaume Jubien", "Mohamed Rahmouni", "Ruggero G. Bettinardi", "Pedro Marinho R. de Oliveira", "Phillipe Derambure", "Nicolas Gaspard", "Jacques Jonas", "Louis Maillard", "Laurent Vercueil", "Herv\u00e9 Vespignani", "Philippe Laval", "Laurent Koessler", "Ulysse Gimenez"], "title": "Automatic detection of abnormal clinical EEG: comparison of a finetuned foundation model with two deep learning models", "categories": ["q-bio.NC", "cs.LG", "eess.SP"], "comment": "20 pages, 7 figures", "summary": "Electroencephalography (EEG) is commonly used by physicians for the diagnosis\nof numerous neurological disorders. Due to the large volume of EEGs requiring\ninterpretation and the specific expertise involved, artificial\nintelligence-based tools are being developed to assist in their visual\nanalysis. In this paper, we compare two deep learning models (CNN-LSTM and\nTransformer-based) with BioSerenity-E1, a recently proposed foundation model,\nin the task of classifying entire EEG recordings as normal or abnormal. The\nthree models were trained or finetuned on 2,500 EEG recordings and their\nperformances were evaluated on two private and one public datasets: a large\nmulticenter dataset annotated by a single specialist (dataset A composed of n =\n4,480 recordings), a small multicenter dataset annotated by three specialists\n(dataset B, n = 198), and the Temple University Abnormal (TUAB) EEG corpus\nevaluation dataset (n = 276). On dataset A, the three models achieved at least\n86% balanced accuracy, with BioSerenity-E1 finetuned achieving the highest\nbalanced accuracy (89.19% [88.36-90.41]). BioSerenity-E1 finetuned also\nachieved the best performance on dataset B, with 94.63% [92.32-98.12] balanced\naccuracy. The models were then validated on TUAB evaluation dataset, whose\ncorresponding training set was not used during training, where they achieved at\nleast 76% accuracy. Specifically, BioSerenity-E1 finetuned outperformed the\nother two models, reaching an accuracy of 82.25% [78.27-87.48]. Our results\nhighlight the usefulness of leveraging pre-trained models for automatic EEG\nclassification: enabling robust and efficient interpretation of EEG data with\nfewer resources and broader applicability.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86CNN-LSTM\u3001Transformer\u548cBioSerenity-E1\u4e09\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728EEG\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u9884\u8bad\u7ec3\u7684BioSerenity-E1\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7531\u4e8eEEG\u89e3\u8bfb\u9700\u6c42\u5927\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5f00\u53d1AI\u5de5\u5177\u8f85\u52a9\u5206\u6790\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u6bd4\u8f83CNN-LSTM\u3001Transformer\u548cBioSerenity-E1\u4e09\u79cd\u6a21\u578b\uff0c\u57282500\u6761EEG\u8bb0\u5f55\u4e0a\u8bad\u7ec3\u6216\u5fae\u8c03\uff0c\u5e76\u5728\u4e09\u4e2a\u6570\u636e\u96c6\uff08\u4e24\u4e2a\u79c1\u4eba\u548c\u4e00\u4e2a\u516c\u5f00\uff09\u4e0a\u8bc4\u4f30\u6027\u80fd\u3002", "result": "BioSerenity-E1\u5fae\u8c03\u540e\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u5e73\u8861\u51c6\u786e\u7387\u6700\u9ad8\u8fbe89.19%\u548c94.63%\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u523082.25%\u51c6\u786e\u7387\u3002", "conclusion": "\u9884\u8bad\u7ec3\u6a21\u578b\u5728EEG\u81ea\u52a8\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u80fd\u591f\u9ad8\u6548\u4e14\u7a33\u5065\u5730\u89e3\u8bfb\u6570\u636e\uff0c\u9002\u7528\u6027\u5e7f\u3002", "keywords": "EEG, deep learning, CNN-LSTM, Transformer, BioSerenity-E1"}}
{"id": "2505.22657", "pdf": "https://arxiv.org/pdf/2505.22657", "abs": "https://arxiv.org/abs/2505.22657", "authors": ["Wenbo Hu", "Yining Hong", "Yanjun Wang", "Leison Gao", "Zibu Wei", "Xingcheng Yao", "Nanyun Peng", "Yonatan Bitton", "Idan Szpektor", "Kai-Wei Chang"], "title": "3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "demos at: https://3dllm-mem.github.io", "summary": "Humans excel at performing complex tasks by leveraging long-term memory\nacross temporal and spatial experiences. In contrast, current Large Language\nModels (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D\nenvironments. We posit that part of this limitation is due to the lack of\nproper 3D spatial-temporal memory modeling in LLMs. To address this, we first\nintroduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000\ntrajectories and 2,892 embodied tasks, question-answering and captioning,\ndesigned to evaluate an agent's ability to reason over long-term memory in 3D\nenvironments. Second, we propose 3DLLM-Mem, a novel dynamic memory management\nand fusion model for embodied spatial-temporal reasoning and actions in LLMs.\nOur model uses working memory tokens, which represents current observations, as\nqueries to selectively attend to and fuse the most useful spatial and temporal\nfeatures from episodic memory, which stores past observations and interactions.\nOur approach allows the agent to focus on task-relevant information while\nmaintaining memory efficiency in complex, long-horizon environments.\nExperimental results demonstrate that 3DLLM-Mem achieves state-of-the-art\nperformance across various tasks, outperforming the strongest baselines by\n16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied\ntasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e863DLLM-Mem\u6a21\u578b\u548c3DMem-Bench\u57fa\u51c6\uff0c\u7528\u4e8e\u89e3\u51b3LLMs\u57283D\u52a8\u6001\u73af\u5883\u4e2d\u7a7a\u95f4-\u65f6\u95f4\u8bb0\u5fc6\u5efa\u6a21\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u4eba\u7c7b\u64c5\u957f\u5229\u7528\u957f\u671f\u8bb0\u5fc6\u5904\u7406\u590d\u6742\u4efb\u52a1\uff0c\u800c\u5f53\u524d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u52a8\u6001\u591a\u623f\u95f43D\u73af\u5883\u4e2d\u7684\u89c4\u5212\u548c\u884c\u52a8\u80fd\u529b\u4e0d\u8db3\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u6709\u6548\u76843D\u7a7a\u95f4-\u65f6\u95f4\u8bb0\u5fc6\u5efa\u6a21\u3002", "method": "\u63d0\u51fa\u4e863DLLM-Mem\u6a21\u578b\uff0c\u901a\u8fc7\u5de5\u4f5c\u8bb0\u5fc6\u4ee4\u724c\u52a8\u6001\u7ba1\u7406\u5e76\u878d\u5408\u6765\u81ea\u60c5\u666f\u8bb0\u5fc6\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u7279\u5f81\uff0c\u9009\u62e9\u6027\u5173\u6ce8\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u5e76\u4fdd\u6301\u8bb0\u5fc6\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c3DLLM-Mem\u57283DMem-Bench\u57fa\u51c6\u7684\u5404\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u5c24\u5176\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u5b9e\u9645\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u6bd4\u57fa\u7ebf\u6a21\u578b\u9ad816.5%\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u521b\u65b0\u7684\u8bb0\u5fc6\u7ba1\u7406\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u590d\u67423D\u73af\u5883\u4e2d\u7684\u63a8\u7406\u548c\u884c\u52a8\u80fd\u529b\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u548c\u65b9\u5411\u3002", "keywords": "3D\u7a7a\u95f4\u8bb0\u5fc6\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u52a8\u6001\u73af\u5883\u3001\u60c5\u666f\u8bb0\u5fc6\u3001\u4efb\u52a1\u63a8\u7406"}}
{"id": "2505.21524", "pdf": "https://arxiv.org/pdf/2505.21524", "abs": "https://arxiv.org/abs/2505.21524", "authors": ["Amitai Yacobi", "Nir Ben-Ari", "Ronen Talmon", "Uri Shaham"], "title": "Learning Shared Representations from Unpaired Data", "categories": ["cs.CV", "cs.LG", "stat.ML"], "comment": null, "summary": "Learning shared representations is a primary area of multimodal\nrepresentation learning. The current approaches to achieve a shared embedding\nspace rely heavily on paired samples from each modality, which are\nsignificantly harder to obtain than unpaired ones. In this work, we demonstrate\nthat shared representations can be learned almost exclusively from unpaired\ndata. Our arguments are grounded in the spectral embeddings of the random walk\nmatrices constructed independently from each unimodal representation. Empirical\nresults in computer vision and natural language processing domains support its\npotential, revealing the effectiveness of unpaired data in capturing meaningful\ncross-modal relations, demonstrating high capabilities in retrieval tasks,\ngeneration, arithmetics, zero-shot, and cross-domain classification. This work,\nto the best of our knowledge, is the first to demonstrate these capabilities\nalmost exclusively from unpaired samples, giving rise to a cross-modal\nembedding that could be viewed as universal, i.e., independent of the specific\nmodalities of the data. Our code IS publicly available at\nhttps://github.com/shaham-lab/SUE.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u975e\u914d\u5bf9\u6570\u636e\u5b66\u4e60\u5171\u4eab\u8868\u793a\uff0c\u901a\u8fc7\u968f\u673a\u6e38\u8d70\u77e9\u9635\u7684\u8c31\u5d4c\u5165\u5b9e\u73b0\u8de8\u6a21\u6001\u5173\u7cfb\u6355\u6349\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u4f9d\u8d56\u914d\u5bf9\u6837\u672c\uff0c\u4f46\u5176\u83b7\u53d6\u96be\u5ea6\u9ad8\u3002\u672c\u6587\u63a2\u7d22\u4ece\u975e\u914d\u5bf9\u6570\u636e\u4e2d\u5b66\u4e60\u5171\u4eab\u8868\u793a\uff0c\u4ee5\u7a81\u7834\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u57fa\u4e8e\u5355\u6a21\u6001\u8868\u793a\u7684\u968f\u673a\u6e38\u8d70\u77e9\u9635\u8c31\u5d4c\u5165\uff0c\u72ec\u7acb\u6784\u5efa\u5e76\u5b66\u4e60\u5171\u4eab\u8868\u793a\u7a7a\u95f4\u3002", "result": "\u5728\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u7d22\u3001\u751f\u6210\u3001\u96f6\u6837\u672c\u5206\u7c7b\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u975e\u914d\u5bf9\u6570\u636e\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u9996\u6b21\u8bc1\u660e\u975e\u914d\u5bf9\u6570\u636e\u53ef\u5b66\u4e60\u901a\u7528\u8de8\u6a21\u6001\u5d4c\u5165\uff0c\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "keywords": "\u591a\u6a21\u6001\u5b66\u4e60, \u975e\u914d\u5bf9\u6570\u636e, \u8c31\u5d4c\u5165, \u5171\u4eab\u8868\u793a, \u8de8\u6a21\u6001\u5d4c\u5165"}}
{"id": "2505.21969", "pdf": "https://arxiv.org/pdf/2505.21969", "abs": "https://arxiv.org/abs/2505.21969", "authors": ["Tianjun Gu", "Linfeng Li", "Xuhong Wang", "Chenghua Gong", "Jingyu Gong", "Zhizhong Zhang", "Yuan Xie", "Lizhuang Ma", "Xin Tan"], "title": "DORAEMON: Decentralized Ontology-aware Reliable Agent with Enhanced Memory Oriented Navigation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Adaptive navigation in unfamiliar environments is crucial for household\nservice robots but remains challenging due to the need for both low-level path\nplanning and high-level scene understanding. While recent vision-language model\n(VLM) based zero-shot approaches reduce dependence on prior maps and\nscene-specific training data, they face significant limitations: spatiotemporal\ndiscontinuity from discrete observations, unstructured memory representations,\nand insufficient task understanding leading to navigation failures. We propose\nDORAEMON (Decentralized Ontology-aware Reliable Agent with Enhanced Memory\nOriented Navigation), a novel cognitive-inspired framework consisting of\nVentral and Dorsal Streams that mimics human navigation capabilities. The\nDorsal Stream implements the Hierarchical Semantic-Spatial Fusion and Topology\nMap to handle spatiotemporal discontinuities, while the Ventral Stream combines\nRAG-VLM and Policy-VLM to improve decision-making. Our approach also develops\nNav-Ensurance to ensure navigation safety and efficiency. We evaluate DORAEMON\non the HM3D, MP3D, and GOAT datasets, where it achieves state-of-the-art\nperformance on both success rate (SR) and success weighted by path length (SPL)\nmetrics, significantly outperforming existing methods. We also introduce a new\nevaluation metric (AORI) to assess navigation intelligence better.\nComprehensive experiments demonstrate DORAEMON's effectiveness in zero-shot\nautonomous navigation without requiring prior map building or pre-training.", "AI": {"tldr": "DORAEMON\u662f\u4e00\u79cd\u53d7\u8ba4\u77e5\u542f\u53d1\u7684\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u5bfc\u822a\u80fd\u529b\u89e3\u51b3\u4e86\u73b0\u6709VLM\u65b9\u6cd5\u5728\u65f6\u7a7a\u4e0d\u8fde\u7eed\u6027\u548c\u4efb\u52a1\u7406\u89e3\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5bb6\u7528\u670d\u52a1\u673a\u5668\u4eba\u5728\u964c\u751f\u73af\u5883\u4e2d\u5bfc\u822a\u9700\u8981\u7ed3\u5408\u4f4e\u5c42\u7ea7\u8def\u5f84\u89c4\u5212\u548c\u9ad8\u5c42\u7ea7\u573a\u666f\u7406\u89e3\uff0c\u800c\u73b0\u6709VLM\u65b9\u6cd5\u5b58\u5728\u65f6\u7a7a\u4e0d\u8fde\u7eed\u6027\u3001\u5b58\u50a8\u8868\u793a\u975e\u7ed3\u6784\u5316\u548c\u4efb\u52a1\u7406\u89e3\u4e0d\u8db3\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faDORAEMON\u6846\u67b6\uff0c\u5305\u542b\u6a21\u4eff\u4eba\u7c7b\u5bfc\u822a\u7684\u8179\u4fa7\u6d41\uff08\u51b3\u7b56\u4f18\u5316\uff09\u548c\u80cc\u4fa7\u6d41\uff08\u65f6\u7a7a\u8fde\u7eed\u6027\u5904\u7406\uff09\uff0c\u5e76\u5f15\u5165Nav-Ensurance\u4fdd\u969c\u5b89\u5168\u4e0e\u6548\u7387\u3002", "result": "\u5728HM3D\u3001MP3D\u548cGOAT\u6570\u636e\u96c6\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\uff08SR\u548cSPL\u6307\u6807\uff09\uff0c\u540c\u65f6\u63d0\u51fa\u65b0\u8bc4\u4f30\u6307\u6807AORI\u3002", "conclusion": "DORAEMON\u65e0\u9700\u9884\u8bad\u7ec3\u6216\u5148\u9a8c\u5730\u56fe\u5373\u53ef\u5b9e\u73b0\u96f6\u6837\u672c\u81ea\u4e3b\u5bfc\u822a\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "keywords": "\u81ea\u4e3b\u5bfc\u822a\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u8ba4\u77e5\u542f\u53d1\u6846\u67b6\u3001\u96f6\u6837\u672c\u5b66\u4e60\u3001DORAEMON"}}
{"id": "2505.21533", "pdf": "https://arxiv.org/pdf/2505.21533", "abs": "https://arxiv.org/abs/2505.21533", "authors": ["Thalles Silva", "Helio Pedrini", "Ad\u00edn Ram\u00edrez Rivera"], "title": "Self-Organizing Visual Prototypes for Non-Parametric Representation Learning", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at ICML 2025, code at https://github.com/sthalles/sop", "summary": "We present Self-Organizing Visual Prototypes (SOP), a new training technique\nfor unsupervised visual feature learning. Unlike existing prototypical\nself-supervised learning (SSL) methods that rely on a single prototype to\nencode all relevant features of a hidden cluster in the data, we propose the\nSOP strategy. In this strategy, a prototype is represented by many semantically\nsimilar representations, or support embeddings (SEs), each containing a\ncomplementary set of features that together better characterize their region in\nspace and maximize training performance. We reaffirm the feasibility of\nnon-parametric SSL by introducing novel non-parametric adaptations of two loss\nfunctions that implement the SOP strategy. Notably, we introduce the SOP Masked\nImage Modeling (SOP-MIM) task, where masked representations are reconstructed\nfrom the perspective of multiple non-parametric local SEs. We comprehensively\nevaluate the representations learned using the SOP strategy on a range of\nbenchmarks, including retrieval, linear evaluation, fine-tuning, and object\ndetection. Our pre-trained encoders achieve state-of-the-art performance on\nmany retrieval benchmarks and demonstrate increasing performance gains with\nmore complex encoders.", "AI": {"tldr": "SOP\u662f\u4e00\u79cd\u65b0\u7684\u65e0\u76d1\u7763\u89c6\u89c9\u7279\u5f81\u5b66\u4e60\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u4e2a\u8bed\u4e49\u76f8\u4f3c\u7684\u652f\u6301\u5d4c\u5165\uff08SEs\uff09\u66ff\u4ee3\u5355\u4e00\u539f\u578b\uff0c\u63d0\u9ad8\u7279\u5f81\u8868\u5f81\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u539f\u578b\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u5355\u4e00\u539f\u578b\u7f16\u7801\u9690\u85cf\u7c07\u7684\u6240\u6709\u7279\u5f81\uff0c\u9650\u5236\u4e86\u8868\u5f81\u80fd\u529b\u3002SOP\u65e8\u5728\u901a\u8fc7\u591a\u4e2a\u652f\u6301\u5d4c\u5165\uff08SEs\uff09\u63d0\u5347\u7279\u5f81\u7684\u4e92\u8865\u6027\u548c\u8bad\u7ec3\u6027\u80fd\u3002", "method": "\u63d0\u51faSOP\u7b56\u7565\uff0c\u5305\u542b\u4e24\u4e2a\u65b0\u9896\u7684\u975e\u53c2\u6570\u635f\u5931\u51fd\u6570\uff0c\u5e76\u8bbe\u8ba1SOP-MIM\u4efb\u52a1\uff08\u57fa\u4e8e\u591aSEs\u91cd\u5efa\u63a9\u7801\u8868\u5f81\uff09\u3002", "result": "\u5728\u68c0\u7d22\u3001\u7ebf\u6027\u8bc4\u4f30\u3001\u5fae\u8c03\u548c\u76ee\u6807\u68c0\u6d4b\u7b49\u4efb\u52a1\u4e2d\uff0cSOP\u7b56\u7565\u5b66\u5230\u7684\u8868\u5f81\u5747\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u4e14\u6027\u80fd\u968f\u7f16\u7801\u5668\u590d\u6742\u5ea6\u63d0\u5347\u800c\u589e\u5f3a\u3002", "conclusion": "SOP\u901a\u8fc7\u591aSEs\u663e\u8457\u63d0\u5347\u4e86\u65e0\u76d1\u7763\u89c6\u89c9\u7279\u5f81\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u975e\u53c2\u6570SSL\u7684\u53ef\u884c\u6027\u3002", "keywords": "\u81ea\u76d1\u7763\u5b66\u4e60, \u89c6\u89c9\u7279\u5f81\u5b66\u4e60, \u975e\u53c2\u6570\u5b66\u4e60, SOP-MIM, \u652f\u6301\u5d4c\u5165"}}
{"id": "2505.21985", "pdf": "https://arxiv.org/pdf/2505.21985", "abs": "https://arxiv.org/abs/2505.21985", "authors": ["Naoto Yoshida", "Tadahiro Taniguchi"], "title": "Reward-Independent Messaging for Decentralized Multi-Agent Reinforcement Learning", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "In multi-agent reinforcement learning (MARL), effective communication\nimproves agent performance, particularly under partial observability. We\npropose MARL-CPC, a framework that enables communication among fully\ndecentralized, independent agents without parameter sharing. MARL-CPC\nincorporates a message learning model based on collective predictive coding\n(CPC) from emergent communication research. Unlike conventional methods that\ntreat messages as part of the action space and assume cooperation, MARL-CPC\nlinks messages to state inference, supporting communication in non-cooperative,\nreward-independent settings. We introduce two algorithms -Bandit-CPC and\nIPPO-CPC- and evaluate them in non-cooperative MARL tasks. Benchmarks show that\nboth outperform standard message-as-action approaches, establishing effective\ncommunication even when messages offer no direct benefit to the sender. These\nresults highlight MARL-CPC's potential for enabling coordination in complex,\ndecentralized environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MARL-CPC\u6846\u67b6\uff0c\u65e0\u9700\u53c2\u6570\u5171\u4eab\u5373\u53ef\u5b9e\u73b0\u5b8c\u5168\u5206\u6563\u7684\u591a\u667a\u80fd\u4f53\u901a\u4fe1\uff0c\u901a\u8fc7\u96c6\u4f53\u9884\u6d4b\u7f16\u7801\uff08CPC\uff09\u63d0\u9ad8\u6027\u80fd\uff0c\u5c24\u5176\u5728\u975e\u5408\u4f5c\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5728\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u4e2d\uff0c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u4e0b\u6709\u6548\u901a\u4fe1\u53ef\u63d0\u5347\u6027\u80fd\u3002\u4f20\u7edf\u65b9\u6cd5\u5047\u8bbe\u5408\u4f5c\u4e14\u6d88\u606f\u4e3a\u52a8\u4f5c\u7a7a\u95f4\u4e00\u90e8\u5206\uff0c\u9650\u5236\u4e86\u975e\u5408\u4f5c\u573a\u666f\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faMARL-CPC\u6846\u67b6\uff0c\u57fa\u4e8e\u96c6\u4f53\u9884\u6d4b\u7f16\u7801\uff08CPC\uff09\u5b66\u4e60\u6d88\u606f\uff0c\u5e76\u5f00\u53d1Bandit-CPC\u548cIPPO-CPC\u7b97\u6cd5\uff0c\u652f\u6301\u975e\u5408\u4f5c\u3001\u5956\u52b1\u65e0\u5173\u7684\u901a\u4fe1\u3002", "result": "\u5728\u975e\u5408\u4f5cMARL\u4efb\u52a1\u4e2d\uff0cBandit-CPC\u548cIPPO-CPC\u5747\u4f18\u4e8e\u4f20\u7edf\u201c\u6d88\u606f\u5373\u52a8\u4f5c\u201d\u65b9\u6cd5\uff0c\u5373\u4f7f\u6d88\u606f\u5bf9\u53d1\u9001\u8005\u65e0\u76f4\u63a5\u76ca\u5904\u4e5f\u80fd\u5efa\u7acb\u6709\u6548\u901a\u4fe1\u3002", "conclusion": "MARL-CPC\u4e3a\u590d\u6742\u5206\u6563\u73af\u5883\u4e0b\u7684\u534f\u8c03\u63d0\u4f9b\u4e86\u6f5c\u529b\uff0c\u6269\u5c55\u4e86\u975e\u5408\u4f5c\u573a\u666f\u4e2d\u591a\u667a\u80fd\u4f53\u901a\u4fe1\u7684\u53ef\u80fd\u6027\u3002", "keywords": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09, \u96c6\u4f53\u9884\u6d4b\u7f16\u7801\uff08CPC\uff09, \u5206\u6563\u901a\u4fe1, \u975e\u5408\u4f5c\u4efb\u52a1, Bandit-CPC, IPPO-CPC"}}
{"id": "2505.21996", "pdf": "https://arxiv.org/pdf/2505.21996", "abs": "https://arxiv.org/abs/2505.21996", "authors": ["Taiye Chen", "Xun Hu", "Zihan Ding", "Chi Jin"], "title": "Learning World Models for Interactive Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Foundational world models must be both interactive and preserve\nspatiotemporal coherence for effective future planning with action choices.\nHowever, present models for long video generation have limited inherent world\nmodeling capabilities due to two main challenges: compounding errors and\ninsufficient memory mechanisms. We enhance image-to-video models with\ninteractive capabilities through additional action conditioning and\nautoregressive framework, and reveal that compounding error is inherently\nirreducible in autoregressive video generation, while insufficient memory\nmechanism leads to incoherence of world models. We propose video retrieval\naugmented generation (VRAG) with explicit global state conditioning, which\nsignificantly reduces long-term compounding errors and increases spatiotemporal\nconsistency of world models. In contrast, naive autoregressive generation with\nextended context windows and retrieval-augmented generation prove less\neffective for video generation, primarily due to the limited in-context\nlearning capabilities of current video models. Our work illuminates the\nfundamental challenges in video world models and establishes a comprehensive\nbenchmark for improving video generation models with internal world modeling\ncapabilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u89c6\u9891\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08VRAG\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u5168\u5c40\u72b6\u6001\u6761\u4ef6\u589e\u5f3a\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u578b\u7684\u4ea4\u4e92\u80fd\u529b\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u957f\u671f\u590d\u5408\u8bef\u5dee\u5e76\u63d0\u9ad8\u4e86\u4e16\u754c\u6a21\u578b\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u957f\u89c6\u9891\u751f\u6210\u6a21\u578b\u56e0\u590d\u5408\u8bef\u5dee\u548c\u8bb0\u5fc6\u673a\u5236\u4e0d\u8db3\u5bfc\u81f4\u4e16\u754c\u5efa\u6a21\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u901a\u8fc7\u4ea4\u4e92\u548c\u65f6\u7a7a\u4e00\u81f4\u6027\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u52a8\u4f5c\u6761\u4ef6\u548c\u81ea\u56de\u5f52\u6846\u67b6\u589e\u5f3a\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u89c6\u9891\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08VRAG\uff09\uff0c\u7ed3\u5408\u663e\u5f0f\u5168\u5c40\u72b6\u6001\u6761\u4ef6\u3002", "result": "VRAG\u663e\u8457\u51cf\u5c11\u4e86\u957f\u671f\u590d\u5408\u8bef\u5dee\uff0c\u63d0\u9ad8\u4e86\u4e16\u754c\u6a21\u578b\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\uff0c\u800c\u4f20\u7edf\u7684\u81ea\u56de\u5f52\u751f\u6210\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6548\u679c\u8f83\u5dee\u3002", "conclusion": "\u89c6\u9891\u4e16\u754c\u6a21\u578b\u5b58\u5728\u56fa\u6709\u6311\u6218\uff0cVRAG\u4e3a\u6539\u8fdb\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u4e16\u754c\u5efa\u6a21\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u5411\u548c\u57fa\u51c6\u3002", "keywords": "\u89c6\u9891\u751f\u6210, \u4e16\u754c\u6a21\u578b, \u68c0\u7d22\u589e\u5f3a\u751f\u6210, \u590d\u5408\u8bef\u5dee, \u65f6\u7a7a\u4e00\u81f4\u6027"}}
{"id": "2505.21536", "pdf": "https://arxiv.org/pdf/2505.21536", "abs": "https://arxiv.org/abs/2505.21536", "authors": ["Federico Zocco", "Andrea Corti", "Monica Malvezzi"], "title": "CiRL: Open-Source Environments for Reinforcement Learning in Circular Economy and Net Zero", "categories": ["cs.CY", "cs.CE", "cs.LG"], "comment": "To be submitted", "summary": "The demand of finite raw materials will keep increasing as they fuel modern\nsociety. Simultaneously, solutions for stopping carbon emissions in the short\nterm are not available, thus making the net zero target extremely challenging\nto achieve at scale. The circular economy (CE) paradigm is gaining attention as\na solution to address climate change and the uncertainties of supplies of\ncritical materials. Hence, in this paper, we introduce CiRL, a deep\nreinforcement learning (DRL) library of environments focused on the circularity\nof both solid and fluid materials. The integration of DRL into the design of\nmaterial circularity is possible thanks to the formalism of thermodynamical\nmaterial networks, which is underpinned by compartmental dynamical\nthermodynamics. Along with the focus on circularity, this library has three\nmore features: the new CE-oriented environments are in the state-space form,\nwhich is typically used in dynamical systems analysis and control designs; it\nis based on a state-of-the-art Python library of DRL algorithms, namely,\nStable-Baselines3; and it is developed in Google Colaboratory to be accessible\nto researchers from different disciplines and backgrounds as is often the case\nfor circular economy researchers and engineers. CiRL is publicly available.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aCiRL\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5e93\uff0c\u4e13\u6ce8\u4e8e\u56fa\u4f53\u548c\u6d41\u4f53\u6750\u6599\u7684\u5faa\u73af\u6027\uff0c\u65e8\u5728\u5e94\u5bf9\u6c14\u5019\u53d8\u5316\u548c\u5173\u952e\u6750\u6599\u4f9b\u5e94\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3\u539f\u6750\u6599\u9700\u6c42\u589e\u52a0\u548c\u77ed\u671f\u78b3\u51cf\u6392\u65b9\u6848\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u63a8\u52a8\u5faa\u73af\u7ecf\u6d4e\u7684\u5b9e\u73b0\u3002", "method": "\u57fa\u4e8e\u70ed\u529b\u5b66\u6750\u6599\u7f51\u7edc\u7684\u52a8\u6001\u7cfb\u7edf\u5f62\u5f0f\uff0c\u5229\u7528Stable-Baselines3\u5e93\u5f00\u53d1DRL\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7Google Colaboratory\u5b9e\u73b0\u8de8\u5b66\u79d1\u53ef\u8bbf\u95ee\u6027\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86CiRL\u5e93\uff0c\u516c\u5f00\u53ef\u7528\uff0c\u652f\u6301\u5faa\u73af\u7ecf\u6d4e\u7814\u7a76\u3002", "conclusion": "CiRL\u4e3a\u6750\u6599\u5faa\u73af\u6027\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684DRL\u5de5\u5177\uff0c\u4fc3\u8fdb\u4e86\u8de8\u5b66\u79d1\u7814\u7a76\u534f\u4f5c\u3002", "keywords": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u5faa\u73af\u7ecf\u6d4e\uff0c\u6750\u6599\u5faa\u73af\u6027\uff0c\u70ed\u529b\u5b66\u7f51\u7edc\uff0cStable-Baselines3"}}
{"id": "2505.21545", "pdf": "https://arxiv.org/pdf/2505.21545", "abs": "https://arxiv.org/abs/2505.21545", "authors": ["Chika Maduabuchi", "Hao Chen", "Yujin Han", "Jindong Wang"], "title": "Corruption-Aware Training of Latent Video Diffusion Models for Robust Text-to-Video Generation", "categories": ["cs.CV", "cs.LG"], "comment": "Code: https://github.com/chikap421/catlvdm Models:\n  https://huggingface.co/Chikap421/catlvdm-checkpoints/tree/main", "summary": "Latent Video Diffusion Models (LVDMs) achieve high-quality generation but are\nsensitive to imperfect conditioning, which causes semantic drift and temporal\nincoherence on noisy, web-scale video-text datasets. We introduce CAT-LVDM, the\nfirst corruption-aware training framework for LVDMs that improves robustness\nthrough structured, data-aligned noise injection. Our method includes\nBatch-Centered Noise Injection (BCNI), which perturbs embeddings along\nintra-batch semantic directions to preserve temporal consistency. BCNI is\nespecially effective on caption-rich datasets like WebVid-2M, MSR-VTT, and\nMSVD. We also propose Spectrum-Aware Contextual Noise (SACN), which injects\nnoise along dominant spectral directions to improve low-frequency smoothness,\nshowing strong results on UCF-101. On average, BCNI reduces FVD by 31.9% across\nWebVid-2M, MSR-VTT, and MSVD, while SACN yields a 12.3% improvement on UCF-101.\nAblation studies confirm the benefit of low-rank, data-aligned noise. Our\ntheoretical analysis further explains how such perturbations tighten entropy,\nWasserstein, score-drift, mixing-time, and generalization bounds. CAT-LVDM\nestablishes a principled, scalable training approach for robust video diffusion\nunder multimodal noise. Code and models: https://github.com/chikap421/catlvdm", "AI": {"tldr": "CAT-LVDM\u662f\u4e00\u79cd\u9488\u5bf9\u6f5c\u5728\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u6297\u5e72\u6270\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u566a\u58f0\u6ce8\u5165\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u663e\u8457\u51cf\u5c11\u8bed\u4e49\u6f02\u79fb\u548c\u65f6\u5e8f\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u6f5c\u5728\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u566a\u58f0\u4e30\u5bcc\u7684\u7f51\u7edc\u89c6\u9891\u6587\u672c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u7684\u8bed\u4e49\u6f02\u79fb\u548c\u65f6\u5e8f\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "\u5f15\u5165CAT-LVDM\u6846\u67b6\uff0c\u5305\u62ec\u6279\u4e2d\u5fc3\u566a\u58f0\u6ce8\u5165(BCNI)\u548c\u9891\u8c31\u611f\u77e5\u4e0a\u4e0b\u6587\u566a\u58f0(SACN)\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u566a\u58f0\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "BCNI\u5728WebVid-2M\u7b49\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u964d\u4f4eFVD 31.9%\uff0cSACN\u5728UCF-101\u4e0a\u63d0\u534712.3%\u3002", "conclusion": "CAT-LVDM\u4e3a\u591a\u6a21\u6001\u566a\u58f0\u4e0b\u7684\u89c6\u9891\u6269\u6563\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "keywords": "\u89c6\u9891\u751f\u6210,\u6269\u6563\u6a21\u578b,\u6297\u5e72\u6270\u8bad\u7ec3,\u566a\u58f0\u6ce8\u5165"}}
{"id": "2505.22021", "pdf": "https://arxiv.org/pdf/2505.22021", "abs": "https://arxiv.org/abs/2505.22021", "authors": ["Zhihong Tang", "Yang Li"], "title": "GL-PGENet: A Parameterized Generation Framework for Robust Document Image Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 7 figures", "summary": "Document Image Enhancement (DIE) serves as a critical component in Document\nAI systems, where its performance substantially determines the effectiveness of\ndownstream tasks. To address the limitations of existing methods confined to\nsingle-degradation restoration or grayscale image processing, we present Global\nwith Local Parametric Generation Enhancement Network (GL-PGENet), a novel\narchitecture designed for multi-degraded color document images, ensuring both\nefficiency and robustness in real-world scenarios. Our solution incorporates\nthree key innovations: First, a hierarchical enhancement framework that\nintegrates global appearance correction with local refinement, enabling\ncoarse-to-fine quality improvement. Second, a Dual-Branch Local-Refine Network\nwith parametric generation mechanisms that replaces conventional direct\nprediction, producing enhanced outputs through learned intermediate parametric\nrepresentations rather than pixel-wise mapping. This approach enhances local\nconsistency while improving model generalization. Finally, a modified NestUNet\narchitecture incorporating dense block to effectively fuse low-level pixel\nfeatures and high-level semantic features, specifically adapted for document\nimage characteristics. In addition, to enhance generalization performance, we\nadopt a two-stage training strategy: large-scale pretraining on a synthetic\ndataset of 500,000+ samples followed by task-specific fine-tuning. Extensive\nexperiments demonstrate the superiority of GL-PGENet, achieving\nstate-of-the-art SSIM scores of 0.7721 on DocUNet and 0.9480 on RealDAE. The\nmodel also exhibits remarkable cross-domain adaptability and maintains\ncomputational efficiency for high-resolution images without performance\ndegradation, confirming its practical utility in real-world scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u5168\u5c40\u4e0e\u5c40\u90e8\u53c2\u6570\u751f\u6210\u589e\u5f3a\u7f51\u7edc\uff08GL-PGENet\uff09\uff0c\u7528\u4e8e\u591a\u9000\u5316\u5f69\u8272\u6587\u6863\u56fe\u50cf\u7684\u589e\u5f3a\uff0c\u901a\u8fc7\u5206\u5c42\u6846\u67b6\u548c\u53cc\u5206\u652f\u5c40\u90e8\u7ec6\u5316\u7f51\u7edc\u5b9e\u73b0\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u589e\u5f3a\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u5c40\u9650\u4e8e\u5355\u4e00\u9000\u5316\u4fee\u590d\u6216\u7070\u5ea6\u56fe\u50cf\u5904\u7406\uff0c\u65e0\u6cd5\u6ee1\u8db3\u591a\u9000\u5316\u5f69\u8272\u6587\u6863\u56fe\u50cf\u7684\u589e\u5f3a\u9700\u6c42\uff0c\u56e0\u6b64\u4f5c\u8005\u63d0\u51fa\u4e86GL-PGENet\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42\u589e\u5f3a\u6846\u67b6\u7ed3\u5408\u5168\u5c40\u5916\u89c2\u6821\u6b63\u4e0e\u5c40\u90e8\u7ec6\u5316\uff1b\u63d0\u51fa\u53cc\u5206\u652f\u5c40\u90e8\u7ec6\u5316\u7f51\u7edc\u548c\u53c2\u6570\u5316\u751f\u6210\u673a\u5236\uff1b\u6539\u8fdbNestUNet\u67b6\u6784\u4ee5\u878d\u5408\u9ad8\u4f4e\u5c42\u7279\u5f81\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff08\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u4efb\u52a1\u5fae\u8c03\uff09\u3002", "result": "\u5728DocUNet\u548cRealDAE\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u52300.7721\u548c0.9480\u7684SSIM\u5206\u6570\uff0c\u8868\u73b0\u4f18\u8d8a\u4e14\u5177\u6709\u8de8\u57df\u9002\u5e94\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "GL-PGENet\u5728\u591a\u9000\u5316\u5f69\u8272\u6587\u6863\u56fe\u50cf\u589e\u5f3a\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u517c\u5177\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "keywords": "\u6587\u6863\u56fe\u50cf\u589e\u5f3a,\u591a\u9000\u5316\u5904\u7406,\u53c2\u6570\u5316\u751f\u6210,\u5206\u5c42\u6846\u67b6,NestUNet"}}
{"id": "2505.22027", "pdf": "https://arxiv.org/pdf/2505.22027", "abs": "https://arxiv.org/abs/2505.22027", "authors": ["Miika Toikkanen", "June-Woo Kim"], "title": "Improving Respiratory Sound Classification with Architecture-Agnostic Knowledge Distillation from Ensembles", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Respiratory sound datasets are limited in size and quality, making high\nperformance difficult to achieve. Ensemble models help but inevitably increase\ncompute cost at inference time. Soft label training distills knowledge\nefficiently with extra cost only at training. In this study, we explore soft\nlabels for respiratory sound classification as an architecture-agnostic\napproach to distill an ensemble of teacher models into a student model. We\nexamine different variations of our approach and find that even a single\nteacher, identical to the student, considerably improves performance beyond its\nown capability, with optimal gains achieved using only a few teachers. We\nachieve the new state-of-the-art Score of 64.39 on ICHBI, surpassing the\nprevious best by 0.85 and improving average Scores across architectures by more\nthan 1.16. Our results highlight the effectiveness of knowledge distillation\nwith soft labels for respiratory sound classification, regardless of size or\narchitecture.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u4f7f\u7528\u8f6f\u6807\u7b7e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5c06\u591a\u6559\u5e08\u6a21\u578b\u7684\u77e5\u8bc6\u4f20\u9012\u7ed9\u5b66\u751f\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u547c\u5438\u97f3\u5206\u7c7b\u6027\u80fd\uff0c\u5e76\u5728ICHBI\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8664.39\u7684\u65b0\u6700\u9ad8\u5206\u3002", "motivation": "\u547c\u5438\u97f3\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\u4e14\u8d28\u91cf\u4e0d\u4e00\uff0c\u5bfc\u81f4\u9ad8\u7cbe\u5ea6\u5206\u7c7b\u56f0\u96be\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u96c6\u6210\u6a21\u578b\u867d\u6709\u6548\uff0c\u4f46\u63a8\u7406\u6210\u672c\u9ad8\u3002\u56e0\u6b64\u7814\u7a76\u8f6f\u6807\u7b7e\u8bad\u7ec3\u4f5c\u4e3a\u8f7b\u91cf\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u67b6\u6784\u65e0\u5173\u7684\u8f6f\u6807\u7b7e\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u5c06\u591a\u6559\u5e08\u6a21\u578b\uff08\u5305\u62ec\u4e0e\u76ee\u6807\u5b66\u751f\u7ed3\u6784\u76f8\u540c\u7684\u5355\u6a21\u578b\uff09\u7684\u77e5\u8bc6\u63d0\u53d6\u81f3\u5b66\u751f\u6a21\u578b\uff0c\u63a2\u7d22\u4e0d\u540c\u53d8\u4f53\u7684\u6548\u679c\u3002", "result": "\u5728ICHBI\u6570\u636e\u96c6\u4e0a\u8fbe64.39\u5206\uff08\u8d85SOTA 0.85\u5206\uff09\uff0c\u4e14\u5e73\u5747\u6027\u80fd\u63d0\u53471.16\u5206\uff1b\u5373\u4f7f\u5355\u6559\u5e08\u6a21\u578b\u4e5f\u80fd\u663e\u8457\u63d0\u5347\u5b66\u751f\u6a21\u578b\u6027\u80fd\uff0c\u6700\u4f18\u6548\u679c\u4ec5\u9700\u5c11\u91cf\u6559\u5e08\u3002", "conclusion": "\u8f6f\u6807\u7b7e\u77e5\u8bc6\u84b8\u998f\u53ef\u6709\u6548\u63d0\u5347\u547c\u5438\u97f3\u5206\u7c7b\u6027\u80fd\uff0c\u4e14\u4e0d\u53d7\u6a21\u578b\u89c4\u6a21\u6216\u67b6\u6784\u9650\u5236\u3002", "keywords": "\u547c\u5438\u97f3\u5206\u7c7b, \u77e5\u8bc6\u84b8\u998f, \u8f6f\u6807\u7b7e, \u6559\u5e08-\u5b66\u751f\u6a21\u578b, ICHBI\u6570\u636e\u96c6"}}
{"id": "2505.22029", "pdf": "https://arxiv.org/pdf/2505.22029", "abs": "https://arxiv.org/abs/2505.22029", "authors": ["Jinming Zhang", "Xuanru Zhou", "Jiachen Lian", "Shuhe Li", "William Li", "Zoe Ezzes", "Rian Bogley", "Lisa Wauters", "Zachary Miller", "Jet Vonk", "Brittany Morin", "Maria Gorno-Tempini", "Gopala Anumanchipalli"], "title": "Analysis and Evaluation of Synthetic Data Generation in Speech Dysfluency Detection", "categories": ["eess.AS", "cs.AI", "cs.SD"], "comment": "Submitted to Interspeech 2025", "summary": "Speech dysfluency detection is crucial for clinical diagnosis and language\nassessment, but existing methods are limited by the scarcity of high-quality\nannotated data. Although recent advances in TTS model have enabled synthetic\ndysfluency generation, existing synthetic datasets suffer from unnatural\nprosody and limited contextual diversity. To address these limitations, we\npropose LLM-Dys -- the most comprehensive dysfluent speech corpus with\nLLM-enhanced dysfluency simulation. This dataset captures 11 dysfluency\ncategories spanning both word and phoneme levels. Building upon this resource,\nwe improve an end-to-end dysfluency detection framework. Experimental\nvalidation demonstrates state-of-the-art performance. All data, models, and\ncode are open-sourced at https://github.com/Berkeley-Speech-Group/LLM-Dys.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86LLM-Dys\uff0c\u4e00\u4e2a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u7684\u7efc\u5408\u6027\u8bed\u97f3\u4e0d\u6d41\u7545\u6570\u636e\u96c6\uff0c\u6539\u8fdb\u4e86\u7aef\u5230\u7aef\u4e0d\u6d41\u7545\u68c0\u6d4b\u6846\u67b6\uff0c\u5e76\u5b9e\u73b0\u4e86\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u4e0d\u6d41\u7545\u68c0\u6d4b\u65b9\u6cd5\u53d7\u9650\u4e8e\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7684\u7a00\u7f3a\u6027\uff0c\u4e14\u73b0\u6709\u5408\u6210\u6570\u636e\u5b58\u5728\u4e0d\u81ea\u7136\u7684\u97f5\u5f8b\u548c\u4e0a\u4e0b\u6587\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faLLM-Dys\u6570\u636e\u96c6\uff0c\u6db5\u76d611\u4e2a\u8bcd\u548c\u97f3\u7d20\u5c42\u9762\u7684\u4e0d\u6d41\u7545\u7c7b\u522b\uff0c\u5e76\u57fa\u4e8e\u6b64\u6539\u8fdb\u7aef\u5230\u7aef\u4e0d\u6d41\u7545\u68c0\u6d4b\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "LLM-Dys\u4e3a\u4e34\u5e8a\u8bca\u65ad\u548c\u8bed\u8a00\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u6570\u636e\u548c\u6539\u8fdb\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "keywords": "\u8bed\u97f3\u4e0d\u6d41\u7545\u68c0\u6d4b, \u5927\u8bed\u8a00\u6a21\u578b, \u5408\u6210\u6570\u636e, \u7aef\u5230\u7aef\u68c0\u6d4b\u6846\u67b6"}}
{"id": "2505.22038", "pdf": "https://arxiv.org/pdf/2505.22038", "abs": "https://arxiv.org/abs/2505.22038", "authors": ["Kaiyuan Li", "Xiaoyue Chen", "Chen Gao", "Yong Li", "Xinlei Chen"], "title": "Balanced Token Pruning: Accelerating Vision Language Models Beyond Local Optimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have shown impressive performance across\nmulti-modal tasks by encoding images into thousands of tokens. However, the\nlarge number of image tokens results in significant computational overhead, and\nthe use of dynamic high-resolution inputs further increases this burden.\nPrevious approaches have attempted to reduce the number of image tokens through\ntoken pruning, typically by selecting tokens based on attention scores or image\ntoken diversity. Through empirical studies, we observe that existing methods\noften overlook the joint impact of pruning on both the current layer's output\n(local) and the outputs of subsequent layers (global), leading to suboptimal\npruning decisions. To address this challenge, we propose Balanced Token Pruning\n(BTP), a plug-and-play method for pruning vision tokens. Specifically, our\nmethod utilizes a small calibration set to divide the pruning process into\nmultiple stages. In the early stages, our method emphasizes the impact of\npruning on subsequent layers, whereas in the deeper stages, the focus shifts\ntoward preserving the consistency of local outputs. Extensive experiments\nacross various LVLMs demonstrate the broad effectiveness of our approach on\nmultiple benchmarks. Our method achieves a 78% compression rate while\npreserving 96.7% of the original models' performance on average.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5e73\u8861\u4ee4\u724c\u526a\u679d\uff08BTP\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u51cf\u5c11\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u4e2d\u56fe\u50cf\u4ee4\u724c\u7684\u8ba1\u7b97\u8d1f\u62c5\u3002\u901a\u8fc7\u5206\u9636\u6bb5\u526a\u679d\uff0cBTP\u5728\u65e9\u671f\u5173\u6ce8\u5168\u5c40\u5f71\u54cd\uff0c\u540e\u671f\u5173\u6ce8\u5c40\u90e8\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u538b\u7f29\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u56fe\u50cf\u4ee4\u724c\u6570\u91cf\u5e9e\u5927\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u663e\u8457\uff0c\u4e14\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651\u5c40\u90e8\u4e0e\u5168\u5c40\u5f71\u54cd\u7684\u8054\u5408\u4f5c\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u4f18\u7684\u526a\u679d\u7b56\u7565\u3002", "method": "BTP\u5229\u7528\u5c0f\u578b\u6821\u51c6\u96c6\u5c06\u526a\u679d\u8fc7\u7a0b\u5206\u4e3a\u591a\u9636\u6bb5\uff1a\u65e9\u671f\u9636\u6bb5\u5f3a\u8c03\u5bf9\u540e\u7eed\u5c42\u7684\u5168\u5c40\u5f71\u54cd\uff0c\u540e\u671f\u9636\u6bb5\u5219\u6ce8\u91cd\u4fdd\u6301\u5f53\u524d\u5c42\u8f93\u51fa\u7684\u5c40\u90e8\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBTP\u80fd\u5728\u5e73\u5747\u4fdd\u755996.7%\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5b9e\u73b078%\u7684\u538b\u7f29\u7387\u3002", "conclusion": "BTP\u662f\u4e00\u79cd\u6709\u6548\u7684\u5373\u63d2\u5373\u7528\u526a\u679d\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86LVLM\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\u3002", "keywords": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u4ee4\u724c\u526a\u679d, \u8ba1\u7b97\u6548\u7387, \u591a\u6a21\u6001\u4efb\u52a1"}}
{"id": "2505.21561", "pdf": "https://arxiv.org/pdf/2505.21561", "abs": "https://arxiv.org/abs/2505.21561", "authors": ["Omid Halimi Milani", "Amanda Nikho", "Marouane Tliba", "Lauren Mills", "Ahmet Enis Cetin", "Mohammed H Elnagar"], "title": "Knowledge Distillation Approach for SOS Fusion Staging: Towards Fully Automated Skeletal Maturity Assessment", "categories": ["cs.CV", "cs.LG"], "comment": "This paper has been accepted to the CVPR Workshop 2025, to be held in\n  Nashville, Tennessee", "summary": "We introduce a novel deep learning framework for the automated staging of\nspheno-occipital synchondrosis (SOS) fusion, a critical diagnostic marker in\nboth orthodontics and forensic anthropology. Our approach leverages a\ndual-model architecture wherein a teacher model, trained on manually cropped\nimages, transfers its precise spatial understanding to a student model that\noperates on full, uncropped images. This knowledge distillation is facilitated\nby a newly formulated loss function that aligns spatial logits as well as\nincorporates gradient-based attention spatial mapping, ensuring that the\nstudent model internalizes the anatomically relevant features without relying\non external cropping or YOLO-based segmentation. By leveraging expert-curated\ndata and feedback at each step, our framework attains robust diagnostic\naccuracy, culminating in a clinically viable end-to-end pipeline. This\nstreamlined approach obviates the need for additional pre-processing tools and\naccelerates deployment, thereby enhancing both the efficiency and consistency\nof skeletal maturation assessment in diverse clinical settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u5316\u8bc4\u4f30\u8776\u6795\u8f6f\u9aa8\u8054\u5408\uff08SOS\uff09\u878d\u5408\u7684\u65b0\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u6a21\u578b\u67b6\u6784\u548c\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u65e0\u9700\u5916\u90e8\u88c1\u526a\u5373\u53ef\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8bca\u65ad\u3002", "motivation": "\u4e3a\u63d0\u5347\u6b63\u7578\u5b66\u548c\u6cd5\u533b\u4eba\u7c7b\u5b66\u4e2d\u9aa8\u9abc\u6210\u719f\u5ea6\u8bc4\u4f30\u7684\u6548\u7387\u548c\u4e00\u81f4\u6027\uff0c\u5f00\u53d1\u65e0\u9700\u989d\u5916\u9884\u5904\u7406\u7684\u81ea\u52a8\u5316\u8bca\u65ad\u5de5\u5177\u3002", "method": "\u91c7\u7528\u53cc\u6a21\u578b\u67b6\u6784\uff08\u6559\u5e08\u6a21\u578b\u548c\u5b66\u751f\u6a21\u578b\uff09\uff0c\u901a\u8fc7\u65b0\u578b\u635f\u5931\u51fd\u6570\u5b9e\u73b0\u7a7a\u95f4\u903b\u8f91\u5bf9\u9f50\u548c\u68af\u5ea6\u6ce8\u610f\u529b\u6620\u5c04\uff0c\u7ed3\u5408\u4e13\u5bb6\u6570\u636e\u9010\u6b65\u4f18\u5316\u3002", "result": "\u6846\u67b6\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u5c55\u73b0\u4e86\u9c81\u68d2\u7684\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u5f62\u6210\u4e86\u7aef\u5230\u7aef\u7684\u53ef\u884c\u6d41\u7a0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5316\u4e86\u8bc4\u4f30\u6d41\u7a0b\uff0c\u63d0\u5347\u4e86\u90e8\u7f72\u6548\u7387\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u4e34\u5e8a\u573a\u666f\u3002", "keywords": "\u6df1\u5ea6\u5b66\u4e60, \u8776\u6795\u8f6f\u9aa8\u8054\u5408, \u77e5\u8bc6\u84b8\u998f, \u9aa8\u9abc\u6210\u719f\u5ea6, \u81ea\u52a8\u5316\u8bca\u65ad"}}
{"id": "2505.21564", "pdf": "https://arxiv.org/pdf/2505.21564", "abs": "https://arxiv.org/abs/2505.21564", "authors": ["Koki Matsuishi", "Tsuyoshi Okita"], "title": "Multi-instance Learning as Downstream Task of Self-Supervised Learning-based Pre-trained Model", "categories": ["cs.CV", "cs.LG"], "comment": "8 pages, 6 figures", "summary": "In deep multi-instance learning, the number of applicable instances depends\non the data set. In histopathology images, deep learning multi-instance\nlearners usually assume there are hundreds to thousands instances in a bag.\nHowever, when the number of instances in a bag increases to 256 in brain\nhematoma CT, learning becomes extremely difficult. In this paper, we address\nthis drawback. To overcome this problem, we propose using a pre-trained model\nwith self-supervised learning for the multi-instance learner as a downstream\ntask. With this method, even when the original target task suffers from the\nspurious correlation problem, we show improvements of 5% to 13% in accuracy and\n40% to 55% in the F1 measure for the hypodensity marker classification of brain\nhematoma CT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u4f5c\u4e3a\u591a\u5b9e\u4f8b\u5b66\u4e60\u5668\u7684\u4e0b\u6e38\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u8111\u8840\u80bfCT\u4e2d\u5b9e\u4f8b\u6570\u91cf\u589e\u52a0\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u7387\u548cF1\u5206\u6570\u3002", "motivation": "\u5728\u8111\u8840\u80bfCT\u4e2d\uff0c\u5f53\u6bcf\u4e2a\u5305\uff08bag\uff09\u4e2d\u7684\u5b9e\u4f8b\u6570\u91cf\u589e\u52a0\u65f6\uff0c\u4f20\u7edf\u7684\u591a\u5b9e\u4f8b\u5b66\u4e60\u65b9\u6cd5\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u4f5c\u4e3a\u591a\u5b9e\u4f8b\u5b66\u4e60\u5668\u7684\u4e0b\u6e38\u4efb\u52a1\uff0c\u4ee5\u514b\u670d\u76ee\u6807\u4efb\u52a1\u4e2d\u7684\u865a\u5047\u76f8\u5173\u6027\u95ee\u9898\u3002", "result": "\u4f7f\u7528\u8be5\u65b9\u6cd5\u540e\uff0c\u8111\u8840\u80bfCT\u7684\u4f4e\u5bc6\u5ea6\u6807\u8bb0\u5206\u7c7b\u4efb\u52a1\u5728\u51c6\u786e\u7387\u4e0a\u63d0\u5347\u4e865%\u81f313%\uff0cF1\u5206\u6570\u63d0\u5347\u4e8640%\u81f355%\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u591a\u5b9e\u4f8b\u5b66\u4e60\u5728\u5b9e\u4f8b\u6570\u91cf\u8f83\u5927\u65f6\u7684\u6027\u80fd\u3002", "keywords": "\u591a\u5b9e\u4f8b\u5b66\u4e60\u3001\u81ea\u76d1\u7763\u5b66\u4e60\u3001\u9884\u8bad\u7ec3\u6a21\u578b\u3001\u8111\u8840\u80bfCT\u3001\u5206\u7c7b\u6027\u80fd"}}
{"id": "2505.22067", "pdf": "https://arxiv.org/pdf/2505.22067", "abs": "https://arxiv.org/abs/2505.22067", "authors": ["Xinyu Xia", "Xingjun Ma", "Yunfeng Hu", "Ting Qu", "Hong Chen", "Xun Gong"], "title": "From Failures to Fixes: LLM-Driven Scenario Repair for Self-Evolving Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Ensuring robust and generalizable autonomous driving requires not only broad\nscenario coverage but also efficient repair of failure cases, particularly\nthose related to challenging and safety-critical scenarios. However, existing\nscenario generation and selection methods often lack adaptivity and semantic\nrelevance, limiting their impact on performance improvement. In this paper, we\npropose \\textbf{SERA}, an LLM-powered framework that enables autonomous driving\nsystems to self-evolve by repairing failure cases through targeted scenario\nrecommendation. By analyzing performance logs, SERA identifies failure patterns\nand dynamically retrieves semantically aligned scenarios from a structured\nbank. An LLM-based reflection mechanism further refines these recommendations\nto maximize relevance and diversity. The selected scenarios are used for\nfew-shot fine-tuning, enabling targeted adaptation with minimal data.\nExperiments on the benchmark show that SERA consistently improves key metrics\nacross multiple autonomous driving baselines, demonstrating its effectiveness\nand generalizability under safety-critical conditions.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u6846\u67b6SERA\uff0c\u80fd\u591f\u901a\u8fc7\u5206\u6790\u6027\u80fd\u65e5\u5fd7\u81ea\u4e3b\u8bc6\u522b\u6545\u969c\u6a21\u5f0f\u5e76\u63a8\u8350\u76f8\u5173\u573a\u666f\uff0c\u4ece\u800c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u573a\u666f\u751f\u6210\u548c\u9009\u62e9\u65b9\u6cd5\u7f3a\u4e4f\u9002\u5e94\u6027\u548c\u8bed\u4e49\u76f8\u5173\u6027\uff0c\u9650\u5236\u4e86\u6027\u80fd\u6539\u8fdb\u7684\u6548\u679c\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u81ea\u6211\u8fdb\u5316\u5e76\u80fd\u9488\u5bf9\u6027\u4fee\u590d\u6545\u969c\u7684\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6027\u80fd\u65e5\u5fd7\u8bc6\u522b\u6545\u969c\u6a21\u5f0f\uff0c\u52a8\u6001\u68c0\u7d22\u8bed\u4e49\u5bf9\u9f50\u7684\u573a\u666f\uff0c\u7ed3\u5408LLM\u7684\u53cd\u601d\u673a\u5236\u7ec6\u5316\u63a8\u8350\uff0c\u5e76\u5229\u7528\u5c11\u91cf\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u3002", "result": "SERA\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u6539\u8fdb\u5173\u952e\u6307\u6807\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b89\u5168\u5173\u952e\u6761\u4ef6\u4e0b\u7684\u6709\u6548\u6027\u548c\u666e\u9002\u6027\u3002", "conclusion": "SERA\u80fd\u6709\u6548\u63a8\u8350\u5e76\u4fee\u590d\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u6545\u969c\u573a\u666f\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "keywords": "\u81ea\u52a8\u9a7e\u9a76,\u573a\u666f\u63a8\u8350,LLM,\u6545\u969c\u4fee\u590d,\u5b89\u5168\u5173\u952e\u573a\u666f"}}
{"id": "2505.21566", "pdf": "https://arxiv.org/pdf/2505.21566", "abs": "https://arxiv.org/abs/2505.21566", "authors": ["Gao Huayu", "Huang Tengjiu", "Ye Xiaolong", "Tsuyoshi Okita"], "title": "Diffusion Model-based Activity Completion for AI Motion Capture from Videos", "categories": ["cs.CV", "cs.LG"], "comment": "32 pages, 16 figures", "summary": "AI-based motion capture is an emerging technology that offers a\ncost-effective alternative to traditional motion capture systems. However,\ncurrent AI motion capture methods rely entirely on observed video sequences,\nsimilar to conventional motion capture. This means that all human actions must\nbe predefined, and movements outside the observed sequences are not possible.\nTo address this limitation, we aim to apply AI motion capture to virtual\nhumans, where flexible actions beyond the observed sequences are required. We\nassume that while many action fragments exist in the training data, the\ntransitions between them may be missing. To bridge these gaps, we propose a\ndiffusion-model-based action completion technique that generates complementary\nhuman motion sequences, ensuring smooth and continuous movements. By\nintroducing a gate module and a position-time embedding module, our approach\nachieves competitive results on the Human3.6M dataset. Our experimental results\nshow that (1) MDC-Net outperforms existing methods in ADE, FDE, and MMADE but\nis slightly less accurate in MMFDE, (2) MDC-Net has a smaller model size\n(16.84M) compared to HumanMAC (28.40M), and (3) MDC-Net generates more natural\nand coherent motion sequences. Additionally, we propose a method for extracting\nsensor data, including acceleration and angular velocity, from human motion\nsequences.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8fd0\u52a8\u8865\u5168\u6280\u672f\uff08MDC-Net\uff09\uff0c\u7528\u4e8e\u751f\u6210\u5e73\u6ed1\u8fde\u7eed\u7684\u52a8\u4f5c\u5e8f\u5217\uff0c\u89e3\u51b3\u4e86AI\u8fd0\u52a8\u6355\u6349\u4e2d\u52a8\u4f5c\u7247\u6bb5\u95f4\u8fc7\u6e21\u7f3a\u5931\u7684\u95ee\u9898\uff0c\u5e76\u5728Human3.6M\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u7ed3\u679c\u3002", "motivation": "\u5f53\u524dAI\u8fd0\u52a8\u6355\u6349\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u89c6\u9891\u5e8f\u5217\uff0c\u65e0\u6cd5\u5904\u7406\u672a\u89c2\u6d4b\u5230\u7684\u52a8\u4f5c\u3002\u4e3a\u865a\u62df\u4eba\u63d0\u4f9b\u7075\u6d3b\u52a8\u4f5c\u9700\u6c42\uff0c\u9700\u586b\u8865\u52a8\u4f5c\u7247\u6bb5\u95f4\u7684\u8fc7\u6e21\u7a7a\u7f3a\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8fd0\u52a8\u8865\u5168\u6280\u672f\uff0c\u7ed3\u5408\u95e8\u63a7\u6a21\u5757\u548c\u4f4d\u7f6e-\u65f6\u95f4\u5d4c\u5165\u6a21\u5757\u751f\u6210\u4e92\u8865\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "MDC-Net\u5728ADE\u3001FDE\u548cMMADE\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6a21\u578b\u5c3a\u5bf8\u66f4\u5c0f\uff0816.84M\uff09\uff0c\u751f\u6210\u7684\u52a8\u4f5c\u66f4\u81ea\u7136\u8fde\u8d2f\u3002", "conclusion": "MDC-Net\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u4f5c\u8fc7\u6e21\u95ee\u9898\uff0c\u4e3a\u865a\u62df\u4eba\u8fd0\u52a8\u6355\u6349\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "AI\u8fd0\u52a8\u6355\u6349\u3001\u6269\u6563\u6a21\u578b\u3001\u52a8\u4f5c\u8865\u5168\u3001\u865a\u62df\u4eba\u3001Human3.6M"}}
{"id": "2505.21567", "pdf": "https://arxiv.org/pdf/2505.21567", "abs": "https://arxiv.org/abs/2505.21567", "authors": ["Feng Jiang", "Zihao Zheng", "Xiuping Cui", "Maoliang Li", "JIayu Chen", "Xiang Chen"], "title": "EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "With the development of Embodied Artificial intelligence, the end-to-end\ncontrol policy such as Vision-Language-Action (VLA) model has become the\nmainstream. Existing VLA models faces expensive computing/storage cost, which\nneed to be optimized. Quantization is considered as the most effective method\nwhich can not only reduce the memory cost but also achieve computation\nacceleration. However, we find the token alignment of VLA models hinders the\napplication of existing quantization methods. To address this, we proposed an\noptimized framework called EaqVLA, which apply encoding-aligned quantization to\nVLA models. Specifically, we propose an complete analysis method to find the\nmisalignment in various granularity. Based on the analysis results, we propose\na mixed precision quantization with the awareness of encoding alignment.\nExperiments shows that the porposed EaqVLA achieves better quantization\nperformance (with the minimal quantization loss for end-to-end action control\nand xxx times acceleration) than existing quantization methods.", "AI": {"tldr": "\u4e3a\u89e3\u51b3VLA\u6a21\u578b\u91cf\u5316\u65f6\u7684token\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u51faEaqVLA\u6846\u67b6\uff0c\u901a\u8fc7\u7f16\u7801\u5bf9\u9f50\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u4f18\u5316\u8ba1\u7b97/\u5b58\u50a8\u6210\u672c\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u91cf\u5316\u635f\u5931\u548c\u52a0\u901f\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u56e0\u8ba1\u7b97/\u5b58\u50a8\u6210\u672c\u9ad8\u6602\u9700\u4f18\u5316\uff0c\u91cf\u5316\u867d\u6709\u6548\u4f46\u53d7\u9650\u4e8etoken\u5bf9\u9f50\u95ee\u9898\uff0c\u56e0\u6b64\u63d0\u51faEaqVLA\u6846\u67b6\u3002", "method": "1. \u5206\u6790\u591a\u7c92\u5ea6\u4e0b\u7684token\u672a\u5bf9\u9f50\u95ee\u9898\uff1b2. \u63d0\u51fa\u57fa\u4e8e\u7f16\u7801\u5bf9\u9f50\u610f\u8bc6\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6cd5\u3002", "result": "EaqVLA\u5728\u7aef\u5230\u7aef\u52a8\u4f5c\u63a7\u5236\u7684\u91cf\u5316\u635f\u5931\u6700\u5c0f\u4e14\u5b9e\u73b0xxx\u500d\u52a0\u901f\uff0c\u4f18\u4e8e\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u3002", "conclusion": "EaqVLA\u901a\u8fc7\u7f16\u7801\u5bf9\u9f50\u91cf\u5316\u6709\u6548\u89e3\u51b3VLA\u6a21\u578b\u7684\u4f18\u5316\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "keywords": "Embodied AI, VLA\u6a21\u578b, \u91cf\u5316, token\u5bf9\u9f50, EaqVLA"}}
{"id": "2505.22086", "pdf": "https://arxiv.org/pdf/2505.22086", "abs": "https://arxiv.org/abs/2505.22086", "authors": ["Runkai Li", "Jia Xiong", "Xi Wang"], "title": "iDSE: Navigating Design Space Exploration in High-Level Synthesis Using LLMs", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "High-Level Synthesis (HLS) serves as an agile hardware development tool that\nstreamlines the circuit design by abstracting the register transfer level into\nbehavioral descriptions, while allowing designers to customize the generated\nmicroarchitectures through optimization directives. However, the combinatorial\nexplosion of possible directive configurations yields an intractable design\nspace. Traditional design space exploration (DSE) methods, despite adopting\nheuristics or constructing predictive models to accelerate Pareto-optimal\ndesign acquisition, still suffer from prohibitive exploration costs and\nsuboptimal results. Addressing these concerns, we introduce iDSE, the first\nLLM-aided DSE framework that leverages HLS design quality perception to\neffectively navigate the design space. iDSE intelligently pruns the design\nspace to guide LLMs in calibrating representative initial sampling designs,\nexpediting convergence toward the Pareto front. By exploiting the convergent\nand divergent thinking patterns inherent in LLMs for hardware optimization,\niDSE achieves multi-path refinement of the design quality and diversity.\nExtensive experiments demonstrate that iDSE outperforms heuristic-based DSE\nmethods by 5.1$\\times$$\\sim$16.6$\\times$ in proximity to the reference Pareto\nfront, matching NSGA-II with only 4.6% of the explored designs. Our work\ndemonstrates the transformative potential of LLMs in scalable and efficient HLS\ndesign optimization, offering new insights into multiobjective optimization\nchallenges.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86iDSE\u6846\u67b6\uff0c\u5229\u7528LLM\u8f85\u52a9\u9ad8\u6548\u5bfc\u822aHLS\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u5347Pareto\u524d\u6cbf\u8bbe\u8ba1\u83b7\u53d6\u6548\u7387\u3002", "motivation": "\u4f20\u7edfHLS\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u7ed3\u679c\u6b21\u4f18\u95ee\u9898\uff0c\u9700\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6848\u3002", "method": "iDSE\u7ed3\u5408LLM\u7684\u601d\u7ef4\u6a21\u5f0f\uff0c\u901a\u8fc7\u667a\u80fd\u526a\u679d\u548c\u591a\u8def\u5f84\u4f18\u5316\uff0c\u5feb\u901f\u6536\u655b\u5230Pareto\u524d\u6cbf\u3002", "result": "iDSE\u5728\u63a5\u8fd1Pareto\u524d\u6cbf\u65b9\u9762\u6bd4\u542f\u53d1\u5f0f\u65b9\u6cd5\u5feb5.1~16.6\u500d\uff0c\u4ec5\u9700NSGA-II 4.6%\u7684\u8bbe\u8ba1\u63a2\u7d22\u3002", "conclusion": "LLM\u5728HLS\u591a\u76ee\u6807\u4f18\u5316\u4e2d\u5c55\u73b0\u53d8\u9769\u6f5c\u529b\uff0c\u4e3a\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "High-Level Synthesis, LLM, Design Space Exploration, Pareto Front, Optimization"}}
{"id": "2505.21574", "pdf": "https://arxiv.org/pdf/2505.21574", "abs": "https://arxiv.org/abs/2505.21574", "authors": ["Dang Nguyen", "Jiping Li", "Jinghao Zheng", "Baharan Mirzasoleiman"], "title": "Do We Need All the Synthetic Data? Towards Targeted Synthetic Image Augmentation via Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Synthetically augmenting training datasets with diffusion models has been an\neffective strategy for improving generalization of image classifiers. However,\nexisting techniques struggle to ensure the diversity of generation and increase\nthe size of the data by up to 10-30x to improve the in-distribution\nperformance. In this work, we show that synthetically augmenting part of the\ndata that is not learned early in training outperforms augmenting the entire\ndataset. By analyzing a two-layer CNN, we prove that this strategy improves\ngeneralization by promoting homogeneity in feature learning speed without\namplifying noise. Our extensive experiments show that by augmenting only\n30%-40% of the data, our method boosts the performance by up to 2.8% in a\nvariety of scenarios, including training ResNet, ViT and DenseNet on CIFAR-10,\nCIFAR-100, and TinyImageNet, with a range of optimizers including SGD and SAM.\nNotably, our method applied with SGD outperforms the SOTA optimizer, SAM, on\nCIFAR-100 and TinyImageNet. It can also easily stack with existing weak and\nstrong augmentation strategies to further boost the performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u9009\u62e9\u6027\u5408\u6210\u6570\u636e\u589e\u5f3a\uff08\u4ec5\u589e\u5f3a\u8bad\u7ec3\u65e9\u671f\u672a\u5b66\u4e60\u7684\u6570\u636e\u90e8\u5206\uff09\u6765\u63d0\u5347\u56fe\u50cf\u5206\u7c7b\u5668\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u573a\u666f\u4e0b\u4f18\u4e8e\u5168\u6570\u636e\u96c6\u589e\u5f3a\uff0c\u4e14\u80fd\u4e0e\u73b0\u6709\u589e\u5f3a\u7b56\u7565\u7ed3\u5408\u4f7f\u7528\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u5728\u751f\u6210\u591a\u6837\u6027\u548c\u6570\u636e\u91cf\u6269\u5c55\uff0810-30\u500d\uff09\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5206\u6790\u8bad\u7ec3\u65e9\u671f\u672a\u5b66\u4e60\u7684\u6570\u636e\u90e8\u5206\uff0c\u627e\u5230\u66f4\u9ad8\u6548\u7684\u589e\u5f3a\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u5206\u6790\u53cc\u5c42CNN\uff0c\u4ec5\u5bf9\u8bad\u7ec3\u65e9\u671f\u672a\u5b66\u4e60\u7684\u6570\u636e\u90e8\u5206\uff0830%-40%\uff09\u8fdb\u884c\u5408\u6210\u589e\u5f3a\uff0c\u907f\u514d\u566a\u58f0\u653e\u5927\u5e76\u4fc3\u8fdb\u7279\u5f81\u5b66\u4e60\u901f\u5ea6\u7684\u5747\u8861\u3002", "result": "\u5728CIFAR-10/100\u3001TinyImageNet\u7b49\u6570\u636e\u96c6\u4e0a\uff0c\u642d\u914d\u4e0d\u540c\u4f18\u5316\u5668\uff08SGD\u3001SAM\uff09\uff0c\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe2.8%\uff0c\u4e14\u4ec5\u7528SGD\u5373\u53ef\u8d85\u8d8aSOTA\u4f18\u5316\u5668SAM\u3002", "conclusion": "\u9009\u62e9\u6027\u6570\u636e\u589e\u5f3a\u80fd\u66f4\u9ad8\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u517c\u5bb9\u73b0\u6709\u5f3a\u5f31\u589e\u5f3a\u7b56\u7565\u3002", "keywords": "\u6570\u636e\u589e\u5f3a\u3001\u6269\u6563\u6a21\u578b\u3001\u56fe\u50cf\u5206\u7c7b\u3001CNN\u3001\u6cdb\u5316\u6027\u80fd"}}
{"id": "2505.22093", "pdf": "https://arxiv.org/pdf/2505.22093", "abs": "https://arxiv.org/abs/2505.22093", "authors": ["Santiago Berrezueta-Guzman", "Stephan Krusche", "Stefan Wagner"], "title": "From Coders to Critics: Empowering Students through Peer Assessment in the Age of AI Copilots", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "This is the authors' preprint version of a paper accepted at the 11th\n  International Symposium on Educational Technology, to be held in July 2025,\n  in Bangkok, Thailand. The final published version will be available via IEEE\n  Xplore Library", "summary": "The rapid adoption of AI powered coding assistants like ChatGPT and other\ncoding copilots is transforming programming education, raising questions about\nassessment practices, academic integrity, and skill development. As educators\nseek alternatives to traditional grading methods susceptible to AI enabled\nplagiarism, structured peer assessment could be a promising strategy. This\npaper presents an empirical study of a rubric based, anonymized peer review\nprocess implemented in a large introductory programming course.\n  Students evaluated each other's final projects (2D game), and their\nassessments were compared to instructor grades using correlation, mean absolute\nerror, and root mean square error (RMSE). Additionally, reflective surveys from\n47 teams captured student perceptions of fairness, grading behavior, and\npreferences regarding grade aggregation. Results show that peer review can\napproximate instructor evaluation with moderate accuracy and foster student\nengagement, evaluative thinking, and interest in providing good feedback to\ntheir peers. We discuss these findings for designing scalable, trustworthy peer\nassessment systems to face the age of AI assisted coding.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728AI\u7f16\u7a0b\u52a9\u624b\u666e\u53ca\u80cc\u666f\u4e0b\uff0c\u7ed3\u6784\u5316\u540c\u4f34\u4e92\u8bc4\u4f5c\u4e3a\u66ff\u4ee3\u4f20\u7edf\u8bc4\u5206\u65b9\u6cd5\u7684\u6f5c\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u5176\u5728\u7f16\u7a0b\u8bfe\u4e2d\u7684\u6548\u679c\u3002", "motivation": "\u9762\u5bf9AI\u8f85\u52a9\u7f16\u7a0b\u5bf9\u5b66\u672f\u8bda\u4fe1\u548c\u8bc4\u4f30\u65b9\u6cd5\u7684\u6311\u6218\uff0c\u7814\u7a76\u63a2\u7d22\u4e86\u7ed3\u6784\u5316\u540c\u4f34\u4e92\u8bc4\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\u7684\u53ef\u884c\u6027\u3002", "method": "\u5728\u5927\u578b\u7f16\u7a0b\u5165\u95e8\u8bfe\u7a0b\u4e2d\u5b9e\u65bd\u57fa\u4e8e\u91cf\u89c4\u7684\u533f\u540d\u540c\u4f34\u4e92\u8bc4\uff0c\u5b66\u751f\u4e92\u8bc42D\u6e38\u620f\u9879\u76ee\uff0c\u5e76\u901a\u8fc7\u7edf\u8ba1\u6307\u6807\u4e0e\u6559\u5e08\u8bc4\u5206\u5bf9\u6bd4\u3002", "result": "\u540c\u4f34\u4e92\u8bc4\u80fd\u4e2d\u7b49\u7a0b\u5ea6\u63a5\u8fd1\u6559\u5e08\u8bc4\u5206\uff0c\u540c\u65f6\u63d0\u5347\u5b66\u751f\u53c2\u4e0e\u5ea6\u3001\u8bc4\u4ef7\u80fd\u529b\u548c\u53cd\u9988\u79ef\u6781\u6027\u3002", "conclusion": "\u7ed3\u6784\u5316\u540c\u4f34\u4e92\u8bc4\u662f\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u53ef\u4fe1\u8bc4\u4f30\u7cfb\u7edf\u7684\u6709\u6548\u7b56\u7565\uff0c\u5c24\u5176\u9002\u7528\u4e8eAI\u8f85\u52a9\u7f16\u7a0b\u65f6\u4ee3\u3002", "keywords": "\u540c\u4f34\u4e92\u8bc4, AI\u7f16\u7a0b\u52a9\u624b, \u7f16\u7a0b\u6559\u80b2, \u5b66\u672f\u8bda\u4fe1, \u8bc4\u4f30\u65b9\u6cd5"}}
{"id": "2505.21580", "pdf": "https://arxiv.org/pdf/2505.21580", "abs": "https://arxiv.org/abs/2505.21580", "authors": ["Anum Fatima", "Gesine Reinert"], "title": "A Kernelised Stein Discrepancy for Assessing the Fit of Inhomogeneous Random Graph Models", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": "43 pages, 24 figures", "summary": "Complex data are often represented as a graph, which in turn can often be\nviewed as a realisation of a random graph, such as of an inhomogeneous random\ngraph model (IRG). For general fast goodness-of-fit tests in high dimensions,\nkernelised Stein discrepancy (KSD) tests are a powerful tool. Here, we develop,\ntest, and analyse a KSD-type goodness-of-fit test for IRG models that can be\ncarried out with a single observation of the network. The test is applicable to\na network of any size and does not depend on the asymptotic distribution of the\ntest statistic. We also provide theoretical guarantees.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6838\u5316Stein\u5dee\u5f02\uff08KSD\uff09\u7684\u62df\u5408\u4f18\u5ea6\u68c0\u9a8c\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5355\u6b21\u89c2\u6d4b\u7684\u4e0d\u5747\u5300\u968f\u673a\u56fe\u6a21\u578b\uff08IRG\uff09\uff0c\u9002\u7528\u4e8e\u4efb\u610f\u89c4\u6a21\u7684\u7f51\u7edc\u4e14\u4e0d\u4f9d\u8d56\u4e8e\u7edf\u8ba1\u91cf\u7684\u6e10\u8fd1\u5206\u5e03\u3002", "motivation": "\u590d\u6742\u6570\u636e\u5e38\u4ee5\u56fe\u5f62\u5f0f\u8868\u793a\uff0c\u800c\u56fe\u53ef\u89c6\u4e3a\u968f\u673a\u56fe\u7684\u5b9e\u73b0\u3002\u9488\u5bf9\u9ad8\u7ef4\u6570\u636e\u7684\u4e00\u822c\u5feb\u901f\u62df\u5408\u4f18\u5ea6\u68c0\u9a8c\uff0c\u6838\u5316Stein\u5dee\u5f02\uff08KSD\uff09\u6d4b\u8bd5\u662f\u4e00\u79cd\u5f3a\u5927\u5de5\u5177\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u9002\u7528\u4e8eIRG\u6a21\u578b\u7684KSD\u578b\u62df\u5408\u4f18\u5ea6\u68c0\u9a8c\u3002", "method": "\u5f00\u53d1\u5e76\u6d4b\u8bd5\u4e86\u4e00\u79cdKSD\u578b\u62df\u5408\u4f18\u5ea6\u68c0\u9a8c\u65b9\u6cd5\uff0c\u9002\u7528\u4e8eIRG\u6a21\u578b\uff0c\u8be5\u68c0\u9a8c\u53ef\u901a\u8fc7\u5355\u6b21\u7f51\u7edc\u89c2\u6d4b\u8fdb\u884c\uff0c\u9002\u7528\u4e8e\u4efb\u610f\u89c4\u6a21\u7684\u7f51\u7edc\u4e14\u4e0d\u4f9d\u8d56\u7edf\u8ba1\u91cf\u7684\u6e10\u8fd1\u5206\u5e03\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u5747\u6709\u6548\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u4e14\u9002\u7528\u4e8e\u5404\u79cd\u89c4\u6a21\u7684\u7f51\u7edc\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aIRG\u6a21\u578b\u7684\u62df\u5408\u4f18\u5ea6\u68c0\u9a8c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u666e\u9002\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u4e0d\u5747\u5300\u968f\u673a\u56fe\u6a21\u578b, \u6838\u5316Stein\u5dee\u5f02, \u62df\u5408\u4f18\u5ea6\u68c0\u9a8c, \u9ad8\u7ef4\u6570\u636e, \u56fe\u8868\u793a"}}
{"id": "2505.22106", "pdf": "https://arxiv.org/pdf/2505.22106", "abs": "https://arxiv.org/abs/2505.22106", "authors": ["Junqi Zhao", "Jinzheng Zhao", "Haohe Liu", "Yun Chen", "Lu Han", "Xubo Liu", "Mark Plumbley", "Wenwu Wang"], "title": "AudioTurbo: Fast Text-to-Audio Generation with Rectified Diffusion", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Diffusion models have significantly improved the quality and diversity of\naudio generation but are hindered by slow inference speed. Rectified flow\nenhances inference speed by learning straight-line ordinary differential\nequation (ODE) paths. However, this approach requires training a flow-matching\nmodel from scratch and tends to perform suboptimally, or even poorly, at low\nstep counts. To address the limitations of rectified flow while leveraging the\nadvantages of advanced pre-trained diffusion models, this study integrates\npre-trained models with the rectified diffusion method to improve the\nefficiency of text-to-audio (TTA) generation. Specifically, we propose\nAudioTurbo, which learns first-order ODE paths from deterministic noise sample\npairs generated by a pre-trained TTA model. Experiments on the AudioCaps\ndataset demonstrate that our model, with only 10 sampling steps, outperforms\nprior models and reduces inference to 3 steps compared to a flow-matching-based\nacceleration model.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51faAudioTurbo\uff0c\u901a\u8fc7\u6574\u5408\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u4e0e\u4fee\u6b63\u6269\u6563\u65b9\u6cd5\uff0c\u63d0\u9ad8\u6587\u672c\u5230\u97f3\u9891\u751f\u6210\u7684\u6548\u7387\u3002\u5728\u4ec510\u6b65\u91c7\u6837\u7684\u6761\u4ef6\u4e0b\uff0c\u8be5\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c06\u63a8\u7406\u6b65\u9aa4\u51cf\u5c11\u81f33\u6b65\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u97f3\u9891\u751f\u6210\u8d28\u91cf\u548c\u591a\u6837\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u63a8\u7406\u901f\u5ea6\u8f83\u6162\u3002\u4fee\u6b63\u6d41\uff08rectified flow\uff09\u867d\u80fd\u52a0\u901f\u63a8\u7406\uff0c\u4f46\u9700\u8981\u4ece\u5934\u8bad\u7ec3\u4e14\u5728\u5c0f\u6b65\u6570\u4e0b\u6027\u80fd\u8f83\u5dee\u3002\u672c\u7814\u7a76\u65e8\u5728\u7ed3\u5408\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4f18\u52bf\u4e0e\u4fee\u6b63\u6269\u6563\u65b9\u6cd5\uff0c\u63d0\u5347\u6587\u672c\u5230\u97f3\u9891\u751f\u6210\u7684\u6548\u7387\u3002", "method": "\u63d0\u51faAudioTurbo\uff0c\u901a\u8fc7\u4ece\u9884\u8bad\u7ec3TTA\u6a21\u578b\u751f\u6210\u7684\u786e\u5b9a\u6027\u566a\u58f0\u6837\u672c\u5bf9\u4e2d\u5b66\u4e60\u4e00\u9636ODE\u8def\u5f84\uff0c\u5b9e\u73b0\u9ad8\u6548\u751f\u6210\u3002", "result": "\u5728AudioCaps\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u752810\u6b65\u91c7\u6837\uff0cAudioTurbo\u5373\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c06\u63a8\u7406\u6b65\u9aa4\u8fdb\u4e00\u6b65\u538b\u7f29\u81f33\u6b65\u3002", "conclusion": "AudioTurbo\u6709\u6548\u7ed3\u5408\u9884\u8bad\u7ec3\u6a21\u578b\u4e0e\u4fee\u6b63\u6269\u6563\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u6548\u7387\uff0c\u540c\u65f6\u5728\u5c0f\u6b65\u6570\u4e0b\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "keywords": "\u6269\u6563\u6a21\u578b\uff0c\u6587\u672c\u5230\u97f3\u9891\u751f\u6210\uff0c\u4fee\u6b63\u6d41\uff0cODE\u8def\u5f84\uff0c\u63a8\u7406\u52a0\u901f"}}
{"id": "2505.21592", "pdf": "https://arxiv.org/pdf/2505.21592", "abs": "https://arxiv.org/abs/2505.21592", "authors": ["Ze Chen", "Shaode Yu"], "title": "Taylor expansion-based Kolmogorov-Arnold network for blind image quality assessment", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "under review", "summary": "Kolmogorov-Arnold Network (KAN) has attracted growing interest for its strong\nfunction approximation capability. In our previous work, KAN and its variants\nwere explored in score regression for blind image quality assessment (BIQA).\nHowever, these models encounter challenges when processing high-dimensional\nfeatures, leading to limited performance gains and increased computational\ncost. To address these issues, we propose TaylorKAN that leverages the Taylor\nexpansions as learnable activation functions to enhance local approximation\ncapability. To improve the computational efficiency, network depth reduction\nand feature dimensionality compression are integrated into the TaylorKAN-based\nscore regression pipeline. On five databases (BID, CLIVE, KonIQ, SPAQ, and\nFLIVE) with authentic distortions, extensive experiments demonstrate that\nTaylorKAN consistently outperforms the other KAN-related models, indicating\nthat the local approximation via Taylor expansions is more effective than\nglobal approximation using orthogonal functions. Its generalization capacity is\nvalidated through inter-database experiments. The findings highlight the\npotential of TaylorKAN as an efficient and robust model for high-dimensional\nscore regression.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTaylorKAN\uff0c\u901a\u8fc7\u6cf0\u52d2\u5c55\u5f00\u4f5c\u4e3a\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\u63d0\u5347\u5c40\u90e8\u903c\u8fd1\u80fd\u529b\uff0c\u540c\u65f6\u7ed3\u5408\u7f51\u7edc\u6df1\u5ea6\u538b\u7f29\u548c\u7279\u5f81\u964d\u7ef4\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u4e94\u4e2a\u6570\u636e\u5e93\u4e0a\u4f18\u4e8e\u5176\u4ed6KAN\u76f8\u5173\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u6cf0\u52d2\u5c55\u5f00\u5728\u5c40\u90e8\u903c\u8fd1\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709KAN\u53ca\u5176\u53d8\u4f53\u5728\u5904\u7406\u9ad8\u7ef4\u7279\u5f81\u65f6\u6027\u80fd\u6709\u9650\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u63d0\u51faTaylorKAN\u4ee5\u6539\u8fdb\u5c40\u90e8\u903c\u8fd1\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u5229\u7528\u6cf0\u52d2\u5c55\u5f00\u4f5c\u4e3a\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\u589e\u5f3a\u5c40\u90e8\u903c\u8fd1\u80fd\u529b\uff0c\u5e76\u7ed3\u5408\u7f51\u7edc\u6df1\u5ea6\u538b\u7f29\u548c\u7279\u5f81\u964d\u7ef4\u4f18\u5316\u8ba1\u7b97\u6d41\u7a0b\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u5e93\uff08BID\u3001CLIVE\u3001KonIQ\u3001SPAQ\u3001FLIVE\uff09\u4e0aTaylorKAN\u5747\u4f18\u4e8e\u5176\u4ed6KAN\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u6cf0\u52d2\u5c55\u5f00\u7684\u5c40\u90e8\u903c\u8fd1\u4f18\u52bf\u53ca\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "TaylorKAN\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u9ad8\u7ef4\u5206\u6570\u56de\u5f52\u6a21\u578b\uff0c\u6cf0\u52d2\u5c55\u5f00\u7684\u5c40\u90e8\u903c\u8fd1\u7b56\u7565\u6bd4\u5168\u5c40\u6b63\u4ea4\u51fd\u6570\u66f4\u6709\u6548\u3002", "keywords": "Kolmogorov-Arnold Network, Taylor expansion, blind image quality assessment, high-dimensional regression, computational efficiency"}}
{"id": "2505.21597", "pdf": "https://arxiv.org/pdf/2505.21597", "abs": "https://arxiv.org/abs/2505.21597", "authors": ["Abdullah Al Mamun", "Pollob Chandra Ray", "Md Rahat Ul Nasib", "Akash Das", "Jia Uddin", "Md Nurul Absur"], "title": "Optimizing Deep Learning for Skin Cancer Classification: A Computationally Efficient CNN with Minimal Accuracy Trade-Off", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "6 pages, & 7 Images", "summary": "The rapid advancement of deep learning in medical image analysis has greatly\nenhanced the accuracy of skin cancer classification. However, current\nstate-of-the-art models, especially those based on transfer learning like\nResNet50, come with significant computational overhead, rendering them\nimpractical for deployment in resource-constrained environments. This study\nproposes a custom CNN model that achieves a 96.7\\% reduction in parameters\n(from 23.9 million in ResNet50 to 692,000) while maintaining a classification\naccuracy deviation of less than 0.022\\%. Our empirical analysis of the HAM10000\ndataset reveals that although transfer learning models provide a marginal\naccuracy improvement of approximately 0.022\\%, they result in a staggering\n13,216.76\\% increase in FLOPs, considerably raising computational costs and\ninference latency. In contrast, our lightweight CNN architecture, which\nencompasses only 30.04 million FLOPs compared to ResNet50's 4.00 billion,\nsignificantly reduces energy consumption, memory footprint, and inference time.\nThese findings underscore the trade-off between the complexity of deep models\nand their real-world feasibility, positioning our optimized CNN as a practical\nsolution for mobile and edge-based skin cancer diagnostics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684CNN\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u5206\u7c7b\u51c6\u786e\u7387\u63a5\u8fd1ResNet50\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u6570\u91cf\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\uff08\u5982ResNet50\uff09\u7684\u76ae\u80a4\u764c\u5206\u7c7b\u6a21\u578b\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u81ea\u5b9a\u4e49\u7684\u8f7b\u91cf\u7ea7CNN\u6a21\u578b\uff0c\u5bf9\u6bd4\u4e86\u5176\u53c2\u6570\u6570\u91cf\u3001FLOPs\u548c\u5206\u7c7b\u51c6\u786e\u7387\u7b49\u6307\u6807\uff0c\u5e76\u901a\u8fc7HAM10000\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u8f7b\u91cf\u6a21\u578b\u53c2\u6570\u51cf\u5c1196.7%\uff08692,000 vs 23.9M\uff09\uff0cFLOPs\u964d\u81f330.04M\uff08ResNet50\u4e3a4B\uff09\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u504f\u5dee\u4ec5\u4e3a0.022%\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u8ba1\u7b97\u6548\u7387\u548c\u5b9e\u7528\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u66f4\u9002\u5408\u79fb\u52a8\u548c\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002", "keywords": "skin cancer classification, lightweight CNN, computational efficiency, HAM10000, transfer learning"}}
{"id": "2505.22125", "pdf": "https://arxiv.org/pdf/2505.22125", "abs": "https://arxiv.org/abs/2505.22125", "authors": ["Melrose Tia", "Jezreel Sophia Lanuzo", "Lei Rigi Baltazar", "Marie Joy Lopez-Relente", "Diwa Malaya Qui\u00f1ones", "Jason Albia"], "title": "Sentiment Simulation using Generative AI Agents", "categories": ["cs.MA", "cs.AI", "cs.CY", "I.2; I.6; J.4"], "comment": "18 pages, 10 figures", "summary": "Traditional sentiment analysis relies on surface-level linguistic patterns\nand retrospective data, limiting its ability to capture the psychological and\ncontextual drivers of human sentiment. These limitations constrain its\neffectiveness in applications that require predictive insight, such as policy\ntesting, narrative framing, and behavioral forecasting. We present a robust\nframework for sentiment simulation using generative AI agents embedded with\npsychologically rich profiles. Agents are instantiated from a nationally\nrepresentative survey of 2,485 Filipino respondents, combining sociodemographic\ninformation with validated constructs of personality traits, values, beliefs,\nand socio-political attitudes. The framework includes three stages: (1) agent\nembodiment via categorical or contextualized encodings, (2) exposure to\nreal-world political and economic scenarios, and (3) generation of sentiment\nratings accompanied by explanatory rationales. Using Quadratic Weighted\nAccuracy (QWA), we evaluated alignment between agent-generated and human\nresponses. Contextualized encoding achieved 92% alignment in replicating\noriginal survey responses. In sentiment simulation tasks, agents reached\n81%--86% accuracy against ground truth sentiment, with contextualized profile\nencodings significantly outperforming categorical (p < 0.0001, Cohen's d =\n0.70). Simulation results remained consistent across repeated trials\n(+/-0.2--0.5% SD) and resilient to variation in scenario framing (p = 0.9676,\nCohen's d = 0.02). Our findings establish a scalable framework for sentiment\nmodeling through psychographically grounded AI agents. This work signals a\nparadigm shift in sentiment analysis from retrospective classification to\nprospective and dynamic simulation grounded in psychology of sentiment\nformation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u751f\u6210\u5f0fAI\u4ee3\u7406\u7684\u60c5\u611f\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5fc3\u7406\u5b66\u6784\u5efa\u7684\u4ee3\u7406\u6765\u63d0\u5347\u9884\u6d4b\u80fd\u529b\uff0c\u9a8c\u8bc1\u5176\u5728\u653f\u6cbb\u548c\u7ecf\u6d4e\u573a\u666f\u4e2d\u7684\u9ad8\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u60c5\u611f\u5206\u6790\u65b9\u6cd5\u5c40\u9650\u4e8e\u8868\u9762\u8bed\u8a00\u6a21\u5f0f\u548c\u56de\u987e\u6027\u6570\u636e\uff0c\u65e0\u6cd5\u6355\u6349\u60c5\u611f\u5f62\u6210\u7684\u5fc3\u7406\u548c\u4e0a\u4e0b\u6587\u52a8\u56e0\uff0c\u9650\u5236\u4e86\u5728\u653f\u7b56\u6d4b\u8bd5\u548c\u9884\u6d4b\u5e94\u7528\u4e2d\u7684\u6548\u679c\u3002", "method": "\u4f7f\u75282,485\u540d\u83f2\u5f8b\u5bbe\u53d7\u8bbf\u8005\u7684\u793e\u4f1a\u4eba\u53e3\u7edf\u8ba1\u6570\u636e\u548c\u5fc3\u7406\u7279\u5f81\u6784\u5efaAI\u4ee3\u7406\uff0c\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u4ee3\u7406\u7f16\u7801\u3001\u573a\u666f\u66b4\u9732\u548c\u60c5\u611f\u8bc4\u5206\u751f\u6210\uff0c\u5e76\u91c7\u7528\u4e8c\u6b21\u52a0\u6743\u51c6\u786e\u7387(QWA)\u8bc4\u4f30\u3002", "result": "\u60c5\u666f\u5316\u7f16\u7801\u5728\u590d\u5236\u8c03\u67e5\u56de\u7b54\u4e2d\u8fbe\u523092%\u5bf9\u9f50\uff0c\u60c5\u611f\u6a21\u62df\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u4e3a81%-86%\uff0c\u4e14\u663e\u8457\u4f18\u4e8e\u5206\u7c7b\u7f16\u7801\uff0c\u7ed3\u679c\u7a33\u5b9a\u4e14\u5bf9\u573a\u666f\u53d8\u5316\u4e0d\u654f\u611f\u3002", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u57fa\u4e8e\u5fc3\u7406\u5b66\u7684\u52a8\u6001\u60c5\u611f\u6a21\u62df\uff0c\u6807\u5fd7\u7740\u60c5\u611f\u5206\u6790\u4ece\u56de\u987e\u6027\u5206\u7c7b\u5411\u9884\u6d4b\u6027\u6a21\u62df\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "keywords": "\u751f\u6210\u5f0fAI, \u60c5\u611f\u5206\u6790, \u5fc3\u7406\u5b66\u4ee3\u7406, \u9884\u6d4b\u6027\u5efa\u6a21, \u60c5\u611f\u6a21\u62df"}}
{"id": "2505.22126", "pdf": "https://arxiv.org/pdf/2505.22126", "abs": "https://arxiv.org/abs/2505.22126", "authors": ["Yifan Chang", "Yukang Feng", "Jianwen Sun", "Jiaxin Ai", "Chuanhao Li", "S. Kevin Zhou", "Kaipeng Zhang"], "title": "SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent years have seen rapid advances in AI-driven image generation. Early\ndiffusion models emphasized perceptual quality, while newer multimodal models\nlike GPT-4o-image integrate high-level reasoning, improving semantic\nunderstanding and structural composition. Scientific illustration generation\nexemplifies this evolution: unlike general image synthesis, it demands accurate\ninterpretation of technical content and transformation of abstract ideas into\nclear, standardized visuals. This task is significantly more\nknowledge-intensive and laborious, often requiring hours of manual work and\nspecialized tools. Automating it in a controllable, intelligent manner would\nprovide substantial practical value. Yet, no benchmark currently exists to\nevaluate AI on this front. To fill this gap, we introduce SridBench, the first\nbenchmark for scientific figure generation. It comprises 1,120 instances\ncurated from leading scientific papers across 13 natural and computer science\ndisciplines, collected via human experts and MLLMs. Each sample is evaluated\nalong six dimensions, including semantic fidelity and structural accuracy.\nExperimental results reveal that even top-tier models like GPT-4o-image lag\nbehind human performance, with common issues in text/visual clarity and\nscientific correctness. These findings highlight the need for more advanced\nreasoning-driven visual generation capabilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u79d1\u5b66\u56fe\u8868\u751f\u6210\u57fa\u51c6SridBench\uff0c\u65e8\u5728\u8bc4\u4f30AI\u5728\u79d1\u5b66\u9886\u57df\u56fe\u50cf\u751f\u6210\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5373\u4f7f\u9876\u7ea7\u6a21\u578b\u5982GPT-4o-image\u4e5f\u843d\u540e\u4e8e\u4eba\u7c7b\u6c34\u5e73\u3002", "motivation": "\u79d1\u5b66\u56fe\u8868\u751f\u6210\u9700\u8981\u9ad8\u7cbe\u5ea6\u548c\u4e13\u4e1a\u77e5\u8bc6\uff0c\u73b0\u6709AI\u6a21\u578b\u7f3a\u4e4f\u76f8\u5173\u8bc4\u4f30\u57fa\u51c6\uff0c\u81ea\u52a8\u5316\u751f\u6210\u5177\u6709\u91cd\u8981\u5b9e\u8df5\u4ef7\u503c\u3002", "method": "\u6784\u5efaSridBench\u57fa\u51c6\uff0c\u5305\u542b1,120\u4e2a\u6765\u81ea13\u4e2a\u5b66\u79d1\u7684\u79d1\u5b66\u56fe\u8868\u6837\u672c\uff0c\u901a\u8fc7\u4eba\u7c7b\u4e13\u5bb6\u548cMLLMs\u6536\u96c6\uff0c\u5e76\u5728\u516d\u4e2a\u7ef4\u5ea6\u4e0a\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u663e\u793aGPT-4o-image\u7b49\u6a21\u578b\u5728\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u7ed3\u6784\u51c6\u786e\u6027\u4e0a\u4e0d\u53ca\u4eba\u7c7b\uff0c\u5c24\u5176\u5728\u6587\u672c/\u89c6\u89c9\u6e05\u6670\u5ea6\u548c\u79d1\u5b66\u6b63\u786e\u6027\u4e0a\u5b58\u5728\u95ee\u9898\u3002", "conclusion": "\u79d1\u5b66\u56fe\u8868\u751f\u6210\u9700\u8981\u66f4\u9ad8\u7ea7\u7684\u63a8\u7406\u9a71\u52a8\u89c6\u89c9\u751f\u6210\u80fd\u529b\uff0cSridBench\u4e3a\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002", "keywords": "AI\u56fe\u50cf\u751f\u6210,\u79d1\u5b66\u56fe\u8868,SridBench,\u8bed\u4e49\u7406\u89e3,\u8bc4\u4f30\u57fa\u51c6"}}
{"id": "2505.22128", "pdf": "https://arxiv.org/pdf/2505.22128", "abs": "https://arxiv.org/abs/2505.22128", "authors": ["Alejandro D. Mousist"], "title": "Real-Time Blind Defocus Deblurring for Earth Observation: The IMAGIN-e Mission Approach", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This work addresses mechanical defocus in Earth observation images from the\nIMAGIN-e mission aboard the ISS, proposing a blind deblurring approach adapted\nto space-based edge computing constraints. Leveraging Sentinel-2 data, our\nmethod estimates the defocus kernel and trains a restoration model within a GAN\nframework, effectively operating without reference images.\n  On Sentinel-2 images with synthetic degradation, SSIM improved by 72.47% and\nPSNR by 25.00%, confirming the model's ability to recover lost details when the\noriginal clean image is known. On IMAGIN-e, where no reference images exist,\nperceptual quality metrics indicate a substantial enhancement, with NIQE\nimproving by 60.66% and BRISQUE by 48.38%, validating real-world onboard\nrestoration. The approach is currently deployed aboard the IMAGIN-e mission,\ndemonstrating its practical application in an operational space environment.\n  By efficiently handling high-resolution images under edge computing\nconstraints, the method enables applications such as water body segmentation\nand contour detection while maintaining processing viability despite resource\nlimitations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9002\u7528\u4e8e\u592a\u7a7a\u8fb9\u7f18\u8ba1\u7b97\u7ea6\u675f\u7684\u76f2\u53bb\u6a21\u7cca\u65b9\u6cd5\uff0c\u7528\u4e8e\u4fee\u590dISS IMAGIN-e\u4efb\u52a1\u4e2d\u7684\u5730\u7403\u89c2\u6d4b\u56fe\u50cf\u673a\u68b0\u79bb\u7126\u95ee\u9898\u3002\u65b9\u6cd5\u57fa\u4e8eSentinel-2\u6570\u636e\u4f30\u8ba1\u79bb\u7126\u6838\u5e76\u5728GAN\u6846\u67b6\u4e2d\u8bad\u7ec3\u6062\u590d\u6a21\u578b\u3002\u5b9e\u9a8c\u663e\u793a\u5408\u6210\u9000\u5316\u56fe\u50cf\u7684SSIM\u548cPSNR\u663e\u8457\u63d0\u5347\uff0c\u5b9e\u9645\u4efb\u52a1\u4e2d\u611f\u77e5\u8d28\u91cf\u6307\u6807\uff08NIQE\u3001BRISQUE\uff09\u4e5f\u5927\u5e45\u6539\u5584\uff0c\u76ee\u524d\u5df2\u5728IMAGIN-e\u4efb\u52a1\u4e2d\u90e8\u7f72\u3002", "motivation": "\u89e3\u51b3\u592a\u7a7a\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e0b\u5730\u7403\u89c2\u6d4b\u56fe\u50cf\u7684\u673a\u68b0\u79bb\u7126\u95ee\u9898\uff0c\u5b9e\u73b0\u65e0\u9700\u53c2\u8003\u56fe\u50cf\u7684\u9ad8\u6548\u56fe\u50cf\u6062\u590d\u3002", "method": "\u5229\u7528Sentinel-2\u6570\u636e\u4f30\u8ba1\u79bb\u7126\u6838\uff0c\u5728GAN\u6846\u67b6\u4e0b\u8bad\u7ec3\u76f2\u53bb\u6a21\u7cca\u6a21\u578b\u3002", "result": "\u5408\u6210\u6570\u636e\u4e0aSSIM\u63d0\u534772.47%\uff0cPSNR\u63d0\u534725.00%\uff1b\u5b9e\u9645\u4efb\u52a1\u4e2dNIQE\u548cBRISQUE\u5206\u522b\u6539\u558460.66%\u548c48.38%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u592a\u7a7a\u73af\u5883\u4e2d\u6709\u6548\u6062\u590d\u56fe\u50cf\u7ec6\u8282\uff0c\u5df2\u6210\u529f\u90e8\u7f72\u4e8eIMAGIN-e\u4efb\u52a1\u3002", "keywords": "\u76f2\u53bb\u6a21\u7cca,GAN,\u8fb9\u7f18\u8ba1\u7b97,\u5730\u7403\u89c2\u6d4b,IMAGIN-e"}}
{"id": "2505.21647", "pdf": "https://arxiv.org/pdf/2505.21647", "abs": "https://arxiv.org/abs/2505.21647", "authors": ["Eric Xing", "Abby Stylianou", "Robert Pless", "Nathan Jacobs"], "title": "QuARI: Query Adaptive Retrieval Improvement", "categories": ["cs.CV", "cs.LG"], "comment": "13 pages, 4 figures, 4 tables", "summary": "Massive-scale pretraining has made vision-language models increasingly\npopular for image-to-image and text-to-image retrieval across a broad\ncollection of domains. However, these models do not perform well when used for\nchallenging retrieval tasks, such as instance retrieval in very large-scale\nimage collections. Recent work has shown that linear transformations of VLM\nfeatures trained for instance retrieval can improve performance by emphasizing\nsubspaces that relate to the domain of interest. In this paper, we explore a\nmore extreme version of this specialization by learning to map a given query to\na query-specific feature space transformation. Because this transformation is\nlinear, it can be applied with minimal computational cost to millions of image\nembeddings, making it effective for large-scale retrieval or re-ranking.\nResults show that this method consistently outperforms state-of-the-art\nalternatives, including those that require many orders of magnitude more\ncomputation at query time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u67e5\u8be2\u5b66\u4e60\u7279\u5b9a\u7ebf\u6027\u53d8\u6362\u6765\u6539\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5927\u578b\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u7684\u65b9\u6cd5\uff0c\u8ba1\u7b97\u6210\u672c\u4f4e\u4e14\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6311\u6218\u6027\u68c0\u7d22\u4efb\u52a1\uff08\u5982\u5927\u89c4\u6a21\u56fe\u50cf\u5b9e\u4f8b\u68c0\u7d22\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u5b66\u4e60\u4e3a\u6bcf\u4e2a\u67e5\u8be2\u751f\u6210\u7279\u5b9a\u7684\u7ebf\u6027\u7279\u5f81\u7a7a\u95f4\u53d8\u6362\uff0c\u4ee5\u5f3a\u8c03\u4e0e\u67e5\u8be2\u76f8\u5173\u7684\u5b50\u7a7a\u95f4\uff0c\u8ba1\u7b97\u6210\u672c\u4f4e\u4e14\u53ef\u6269\u5c55\u6027\u5f3a\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5305\u62ec\u8ba1\u7b97\u91cf\u66f4\u5927\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u901a\u8fc7\u5b66\u4e60\u67e5\u8be2\u7279\u5b9a\u7684\u7ebf\u6027\u53d8\u6362\uff0c\u53ef\u4ee5\u9ad8\u6548\u5730\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5927\u89c4\u6a21\u56fe\u50cf\u68c0\u7d22\u4e2d\u7684\u6027\u80fd\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u56fe\u50cf\u68c0\u7d22, \u7ebf\u6027\u53d8\u6362, \u5927\u89c4\u6a21\u68c0\u7d22"}}
{"id": "2505.22141", "pdf": "https://arxiv.org/pdf/2505.22141", "abs": "https://arxiv.org/abs/2505.22141", "authors": ["Guanwen Feng", "Zhiyuan Ma", "Yunan Li", "Junwei Jing", "Jiahao Yang", "Qiguang Miao"], "title": "FaceEditTalker: Interactive Talking Head Generation with Facial Attribute Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in audio-driven talking head generation have achieved\nimpressive results in lip synchronization and emotional expression. However,\nthey largely overlook the crucial task of facial attribute editing. This\ncapability is crucial for achieving deep personalization and expanding the\nrange of practical applications, including user-tailored digital avatars,\nengaging online education content, and brand-specific digital customer service.\nIn these key domains, the flexible adjustment of visual attributes-such as\nhairstyle, accessories, and subtle facial features is essential for aligning\nwith user preferences, reflecting diverse brand identities, and adapting to\nvarying contextual demands. In this paper, we present FaceEditTalker, a unified\nframework that enables controllable facial attribute manipulation while\ngenerating high-quality, audio-synchronized talking head videos. Our method\nconsists of two key components: an image feature space editing module, which\nextracts semantic and detail features and allows flexible control over\nattributes like expression, hairstyle, and accessories; and an audio-driven\nvideo generation module, which fuses these edited features with audio-guided\nfacial landmarks to drive a diffusion-based generator. This design ensures\ntemporal coherence, visual fidelity, and identity preservation across frames.\nExtensive experiments on public datasets demonstrate that our method\noutperforms state-of-the-art approaches in lip-sync accuracy, video quality,\nand attribute controllability. Project page:\nhttps://peterfanfan.github.io/FaceEditTalker/", "AI": {"tldr": "FaceEditTalker\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u591f\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u97f3\u9891\u540c\u6b65\u7684\u8bf4\u8bdd\u5934\u89c6\u9891\u65f6\u5b9e\u73b0\u53ef\u63a7\u7684\u9762\u90e8\u5c5e\u6027\u7f16\u8f91\u3002", "motivation": "\u5f53\u524d\u97f3\u9891\u9a71\u52a8\u7684\u8bf4\u8bdd\u5934\u751f\u6210\u6280\u672f\u5ffd\u89c6\u4e86\u9762\u90e8\u5c5e\u6027\u7f16\u8f91\u8fd9\u4e00\u5173\u952e\u4efb\u52a1\uff0c\u800c\u8fd9\u4e00\u80fd\u529b\u5bf9\u4e8e\u6df1\u5ea6\u4e2a\u6027\u5316\u548c\u6269\u5c55\u5b9e\u9645\u5e94\u7528\u8303\u56f4\u81f3\u5173\u91cd\u8981\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u56fe\u50cf\u7279\u5f81\u7a7a\u95f4\u7f16\u8f91\u6a21\u5757\uff08\u63d0\u53d6\u8bed\u4e49\u548c\u7ec6\u8282\u7279\u5f81\u4ee5\u7075\u6d3b\u63a7\u5236\u5c5e\u6027\uff09\u548c\u97f3\u9891\u9a71\u52a8\u7684\u89c6\u9891\u751f\u6210\u6a21\u5757\uff08\u878d\u5408\u7f16\u8f91\u7279\u5f81\u4e0e\u97f3\u9891\u5f15\u5bfc\u7684\u9762\u90e8\u6807\u5fd7\u4ee5\u9a71\u52a8\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u5668\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5507\u540c\u6b65\u7cbe\u5ea6\u3001\u89c6\u9891\u8d28\u91cf\u548c\u5c5e\u6027\u53ef\u63a7\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "FaceEditTalker\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u53ef\u63a7\u7684\u9762\u90e8\u5c5e\u6027\u7f16\u8f91\u548c\u89c6\u9891\u751f\u6210\u3002", "keywords": "\u97f3\u9891\u9a71\u52a8\u3001\u8bf4\u8bdd\u5934\u751f\u6210\u3001\u9762\u90e8\u5c5e\u6027\u7f16\u8f91\u3001\u7279\u5f81\u7a7a\u95f4\u3001\u6269\u6563\u6a21\u578b"}}
{"id": "2505.21658", "pdf": "https://arxiv.org/pdf/2505.21658", "abs": "https://arxiv.org/abs/2505.21658", "authors": ["Brandon R. Feng", "David Keetae Park", "Xihaier Luo", "Arantxa Urdangarin", "Shinjae Yoo", "Brian J. Reich"], "title": "STACI: Spatio-Temporal Aleatoric Conformal Inference", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "Fitting Gaussian Processes (GPs) provides interpretable aleatoric uncertainty\nquantification for estimation of spatio-temporal fields. Spatio-temporal deep\nlearning models, while scalable, typically assume a simplistic independent\ncovariance matrix for the response, failing to capture the underlying\ncorrelation structure. However, spatio-temporal GPs suffer from issues of\nscalability and various forms of approximation bias resulting from restrictive\nassumptions of the covariance kernel function. We propose STACI, a novel\nframework consisting of a variational Bayesian neural network approximation of\nnon-stationary spatio-temporal GP along with a novel spatio-temporal conformal\ninference algorithm. STACI is highly scalable, taking advantage of GPU training\ncapabilities for neural network models, and provides statistically valid\nprediction intervals for uncertainty quantification. STACI outperforms\ncompeting GPs and deep methods in accurately approximating spatio-temporal\nprocesses and we show it easily scales to datasets with millions of\nobservations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86STACI\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u53d8\u5206\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u548c\u975e\u5e73\u7a33\u65f6\u7a7a\u9ad8\u65af\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u548c\u534f\u65b9\u5dee\u6838\u9650\u5236\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u9ad8\u6548\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u7684\u7a7a\u95f4-\u65f6\u95f4\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u867d\u7136\u80fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u4f46\u9762\u4e34\u53ef\u6269\u5c55\u6027\u548c\u534f\u65b9\u5dee\u6838\u9650\u5236\u7684\u95ee\u9898\uff1b\u800c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u867d\u7136\u53ef\u6269\u5c55\uff0c\u4f46\u65e0\u6cd5\u6709\u6548\u6355\u6349\u76f8\u5173\u6027\u7ed3\u6784\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u6269\u5c55\u53c8\u80fd\u51c6\u786e\u5efa\u6a21\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86STACI\u6846\u67b6\uff0c\u7ed3\u5408\u53d8\u5206\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u975e\u5e73\u7a33\u65f6\u7a7aGP\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7a7a\u95f4-\u65f6\u95f4\u5171\u5f62\u63a8\u65ad\u7b97\u6cd5\uff0c\u5229\u7528GPU\u8bad\u7ec3\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002", "result": "STACI\u5728\u51c6\u786e\u8fd1\u4f3c\u65f6\u7a7a\u8fc7\u7a0b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709GP\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u80fd\u8f7b\u677e\u6269\u5c55\u5230\u6570\u767e\u4e07\u89c2\u6d4b\u503c\u7684\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u7edf\u8ba1\u6709\u6548\u7684\u9884\u6d4b\u533a\u95f4\u3002", "conclusion": "STACI\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u65f6\u7a7a\u6570\u636e\u5efa\u6a21\u7684\u53ef\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u7edf\u8ba1\u6709\u6548\u6027\u3002", "keywords": "\u9ad8\u65af\u8fc7\u7a0b\uff0c\u65f6\u7a7a\u5efa\u6a21\uff0c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u53d8\u5206\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\uff0c\u5171\u5f62\u63a8\u65ad"}}
{"id": "2505.22174", "pdf": "https://arxiv.org/pdf/2505.22174", "abs": "https://arxiv.org/abs/2505.22174", "authors": ["Georgios Amanatidis", "Alexandros Lolos", "Evangelos Markakis", "Victor Turmel"], "title": "Online Fair Division for Personalized $2$-Value Instances", "categories": ["cs.GT", "cs.AI", "cs.MA"], "comment": null, "summary": "We study an online fair division setting, where goods arrive one at a time\nand there is a fixed set of $n$ agents, each of whom has an additive valuation\nfunction over the goods. Once a good appears, the value each agent has for it\nis revealed and it must be allocated immediately and irrevocably to one of the\nagents. It is known that without any assumptions about the values being\nseverely restricted or coming from a distribution, very strong impossibility\nresults hold in this setting. To bypass the latter, we turn our attention to\ninstances where the valuation functions are restricted. In particular, we study\npersonalized $2$-value instances, where there are only two possible values each\nagent may have for each good, possibly different across agents, and we show how\nto obtain worst case guarantees with respect to well-known fairness notions,\nsuch as maximin share fairness and envy-freeness up to one (or two) good(s). We\nsuggest a deterministic algorithm that maintains a $1/(2n-1)$-MMS allocation at\nevery time step and show that this is the best possible any deterministic\nalgorithm can achieve if one cares about every single time step; nevertheless,\neventually the allocation constructed by our algorithm becomes a $1/4$-MMS\nallocation. To achieve this, the algorithm implicitly maintains a fragile\nsystem of priority levels for all agents. Further, we show that, by allowing\nsome limited access to future information, it is possible to have stronger\nresults with less involved approaches. By knowing the values of goods for $n-1$\ntime steps into the future, we design a matching-based algorithm that achieves\nan EF$1$ allocation every $n$ time steps, while always maintaining an EF$2$\nallocation. Finally, we show that our results allow us to get the first\nnontrivial guarantees for additive instances in which the ratio of the maximum\nover the minimum value an agent has for a good is bounded.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u786e\u5b9a\u6027\u7b97\u6cd5\uff0c\u5728\u53d7\u9650\u4f30\u503c\u6761\u4ef6\u4e0b\u5b9e\u73b0\u516c\u5e73\u5206\u914d\u7684\u6700\u574f\u60c5\u51b5\u4fdd\u8bc1\uff0c\u5e76\u901a\u8fc7\u4f18\u5148\u7ea7\u7cfb\u7edf\u548c\u9884\u89c1\u4fe1\u606f\u4f18\u5316\u7ed3\u679c\u3002", "motivation": "\u7ed5\u8fc7\u5728\u7ebf\u516c\u5e73\u5206\u914d\u4e2d\u7684\u5f3a\u4e0d\u53ef\u80fd\u7ed3\u679c\uff0c\u7814\u7a76\u53d7\u9650\u4f30\u503c\u51fd\u6570\u4e0b\u7684\u53ef\u884c\u6027\uff0c\u7279\u522b\u662f2\u503c\u4e2a\u6027\u5316\u5b9e\u4f8b\u3002", "method": "\u63d0\u51fa\u786e\u5b9a\u6027\u7b97\u6cd5\uff0c\u7ef4\u62a4\u4ee3\u7406\u4f18\u5148\u7ea7\u7cfb\u7edf\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u5339\u914d\u7684\u7b97\u6cd5\u5229\u7528\u6709\u9650\u9884\u89c1\u4fe1\u606f\u3002", "result": "\u7b97\u6cd5\u5728\u6bcf\u4e00\u6b65\u4fdd\u63011/(2n-1)-MMS\u5206\u914d\uff0c\u6700\u7ec8\u8fbe\u52301/4-MMS\uff1b\u5229\u7528\u6709\u9650\u9884\u89c1\u4fe1\u606f\u53ef\u5b9e\u73b0EF1\u548cEF2\u5206\u914d\u3002", "conclusion": "\u53d7\u9650\u4f30\u503c\u548c\u6709\u9650\u9884\u89c1\u4fe1\u606f\u53ef\u7a81\u7834\u4e0d\u53ef\u80fd\u7ed3\u679c\uff0c\u4e3a\u6bd4\u7387\u6709\u754c\u7684\u52a0\u6cd5\u5b9e\u4f8b\u63d0\u4f9b\u9996\u4e2a\u975e\u5e73\u51e1\u4fdd\u8bc1\u3002", "keywords": "\u5728\u7ebf\u516c\u5e73\u5206\u914d\u3001\u53d7\u9650\u4f30\u503c\u3001MMS\u3001EF1\u3001EF2"}}
{"id": "2505.21686", "pdf": "https://arxiv.org/pdf/2505.21686", "abs": "https://arxiv.org/abs/2505.21686", "authors": ["Michele Gallo"], "title": "tenSVD algorithm for compression", "categories": ["stat.CO", "cs.CV", "cs.LG"], "comment": null, "summary": "Tensors provide a robust framework for managing high-dimensional data.\nConsequently, tensor analysis has emerged as an active research area in various\ndomains, including machine learning, signal processing, computer vision, graph\nanalysis, and data mining. This study introduces an efficient image storage\napproach utilizing tensors, aiming to minimize memory to store, bandwidth to\ntransmit and energy to processing. The proposed method organizes original data\ninto a higher-order tensor and applies the Tucker model for compression.\nImplemented in R, this method is compared to a baseline algorithm. The\nevaluation focuses on efficient of algorithm measured in term of computational\ntime and the quality of information preserved, using both simulated and real\ndatasets. A detailed analysis of the results is conducted, employing\nestablished quantitative metrics, with significant attention paid to\nsustainability in terms of energy consumption across algorithms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f20\u91cf\u7684\u9ad8\u6548\u56fe\u50cf\u5b58\u50a8\u65b9\u6cd5\uff0c\u5229\u7528Tucker\u6a21\u578b\u8fdb\u884c\u538b\u7f29\uff0c\u964d\u4f4e\u4e86\u5b58\u50a8\u3001\u4f20\u8f93\u548c\u5904\u7406\u7684\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u9ad8\u7ef4\u6570\u636e\u7ba1\u7406\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u5c24\u5176\u5728\u673a\u5668\u5b66\u4e60\u3001\u4fe1\u53f7\u5904\u7406\u7b49\u9886\u57df\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5f20\u91cf\u5206\u6790\u51cf\u5c11\u56fe\u50cf\u5b58\u50a8\u7684\u8d44\u6e90\u6d88\u8017\u3002", "method": "\u5c06\u539f\u59cb\u6570\u636e\u7ec4\u7ec7\u4e3a\u9ad8\u9636\u5f20\u91cf\uff0c\u5e94\u7528Tucker\u6a21\u578b\u8fdb\u884c\u538b\u7f29\uff0c\u5e76\u5728R\u4e2d\u5b9e\u73b0\uff0c\u4e0e\u57fa\u7ebf\u7b97\u6cd5\u5bf9\u6bd4\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u8bc4\u4f30\uff0c\u7b97\u6cd5\u5728\u8ba1\u7b97\u65f6\u95f4\u548c\u4fe1\u606f\u4fdd\u7559\u8d28\u91cf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u80fd\u8017\u3002", "conclusion": "\u5f20\u91cf\u538b\u7f29\u65b9\u6cd5\u5728\u8d44\u6e90\u6548\u7387\u548c\u53ef\u6301\u7eed\u6027\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002", "keywords": "\u5f20\u91cf\u5206\u6790\uff0c\u56fe\u50cf\u5b58\u50a8\uff0cTucker\u6a21\u578b\uff0c\u8d44\u6e90\u6548\u7387\uff0c\u80fd\u8017"}}
{"id": "2505.21692", "pdf": "https://arxiv.org/pdf/2505.21692", "abs": "https://arxiv.org/abs/2505.21692", "authors": ["Omar Bennouna", "Amine Bennouna", "Saurabh Amin", "Asuman Ozdaglar"], "title": "What Data Enables Optimal Decisions? An Exact Characterization for Linear Optimization", "categories": ["math.OC", "cs.LG"], "comment": null, "summary": "We study the fundamental question of how informative a dataset is for solving\na given decision-making task. In our setting, the dataset provides partial\ninformation about unknown parameters that influence task outcomes. Focusing on\nlinear programs, we characterize when a dataset is sufficient to recover an\noptimal decision, given an uncertainty set on the cost vector. Our main\ncontribution is a sharp geometric characterization that identifies the\ndirections of the cost vector that matter for optimality, relative to the task\nconstraints and uncertainty set. We further develop a practical algorithm that,\nfor a given task, constructs a minimal or least-costly sufficient dataset. Our\nresults reveal that small, well-chosen datasets can often fully determine\noptimal decisions -- offering a principled foundation for task-aware data\nselection.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u6570\u636e\u96c6\u5bf9\u4e8e\u89e3\u51b3\u51b3\u7b56\u4efb\u52a1\u7684\u4fe1\u606f\u91cf\uff0c\u901a\u8fc7\u7ebf\u6027\u7a0b\u5e8f\u5206\u6790\u4e86\u6570\u636e\u96c6\u5728\u6210\u672c\u5411\u91cf\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\u6062\u590d\u6700\u4f18\u51b3\u7b56\u7684\u5145\u5206\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u51e0\u4f55\u7279\u5f81\u63cf\u8ff0\u548c\u5b9e\u7528\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u7406\u89e3\u6570\u636e\u96c6\u5982\u4f55\u4e3a\u7279\u5b9a\u51b3\u7b56\u4efb\u52a1\u63d0\u4f9b\u6709\u6548\u4fe1\u606f\uff0c\u5c24\u5176\u662f\u5728\u90e8\u5206\u4fe1\u606f\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\uff0c\u63a2\u8ba8\u5982\u4f55\u9009\u62e9\u548c\u6784\u5efa\u6570\u636e\u96c6\u4ee5\u786e\u4fdd\u6700\u4f18\u51b3\u7b56\u3002", "method": "\u91c7\u7528\u7ebf\u6027\u7a0b\u5e8f\u5206\u6790\uff0c\u6784\u5efa\u51e0\u4f55\u7279\u5f81\u4ee5\u8bc6\u522b\u6210\u672c\u5411\u91cf\u4e2d\u5bf9\u6700\u4f18\u51b3\u7b56\u5173\u952e\u7684\u65b9\u5411\uff0c\u5e76\u5f00\u53d1\u7b97\u6cd5\u751f\u6210\u6700\u5c0f\u6216\u6700\u7ecf\u6d4e\u7684\u5145\u5206\u6570\u636e\u96c6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5c0f\u800c\u7cbe\u9009\u7684\u6570\u636e\u96c6\u5f80\u5f80\u8db3\u4ee5\u786e\u5b9a\u6700\u4f18\u51b3\u7b56\uff0c\u4e3a\u4efb\u52a1\u9a71\u52a8\u7684\u6570\u636e\u9009\u62e9\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u6570\u636e\u9009\u62e9\u548c\u6700\u4f18\u51b3\u7b56\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u5c0f\u800c\u9ad8\u6548\u7684\u6570\u636e\u96c6\u53ef\u4ee5\u5b8c\u5168\u652f\u6301\u51b3\u7b56\u4efb\u52a1\u3002", "keywords": "\u6570\u636e\u96c6\u4fe1\u606f\u91cf, \u7ebf\u6027\u7a0b\u5e8f, \u6700\u4f18\u51b3\u7b56, \u6210\u672c\u5411\u91cf, \u6570\u636e\u9009\u62e9"}}
{"id": "2505.22193", "pdf": "https://arxiv.org/pdf/2505.22193", "abs": "https://arxiv.org/abs/2505.22193", "authors": ["Marco Parigi", "Stefano Martina", "Francesco Aldo Venturelli", "Filippo Caruso"], "title": "Physics-inspired Generative AI models via real hardware-based noisy quantum diffusion", "categories": ["quant-ph", "cond-mat.dis-nn", "cs.AI", "cs.CV", "cs.LG", "81P68, 81P40, 81P47, 68Q12, 68T07,", "I.2.6; I.3.3; J.2"], "comment": "17 pages, 9 figures. Supplementary materials: 2 pages, 2 figures", "summary": "Quantum Diffusion Models (QDMs) are an emerging paradigm in Generative AI\nthat aims to use quantum properties to improve the performances of their\nclassical counterparts. However, existing algorithms are not easily scalable\ndue to the limitations of near-term quantum devices. Following our previous\nwork on QDMs, here we propose and implement two physics-inspired protocols. In\nthe first, we use the formalism of quantum stochastic walks, showing that a\nspecific interplay of quantum and classical dynamics in the forward process\nproduces statistically more robust models generating sets of MNIST images with\nlower Fr\\'echet Inception Distance (FID) than using totally classical dynamics.\nIn the second approach, we realize an algorithm to generate images by\nexploiting the intrinsic noise of real IBM quantum hardware with only four\nqubits. Our work could be a starting point to pave the way for new scenarios\nfor large-scale algorithms in quantum Generative AI, where quantum noise is\nneither mitigated nor corrected, but instead exploited as a useful resource.", "AI": {"tldr": "\u91cf\u5b50\u6269\u6563\u6a21\u578b\uff08QDMs\uff09\u65e8\u5728\u5229\u7528\u91cf\u5b50\u7279\u6027\u63d0\u5347\u7ecf\u5178\u751f\u6210AI\u7684\u6027\u80fd\uff0c\u4f46\u73b0\u6709\u7b97\u6cd5\u56e0\u77ed\u671f\u91cf\u5b50\u8bbe\u5907\u7684\u9650\u5236\u96be\u4ee5\u6269\u5c55\u3002\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u7269\u7406\u542f\u53d1\u7684\u534f\u8bae\uff1a\u4e00\u662f\u901a\u8fc7\u91cf\u5b50\u968f\u673a\u884c\u8d70\u5f62\u5f0f\u5728\u6b63\u5411\u8fc7\u7a0b\u4e2d\u7ed3\u5408\u91cf\u5b50\u4e0e\u7ecf\u5178\u52a8\u529b\u5b66\uff0c\u751f\u6210\u7edf\u8ba1\u66f4\u7a33\u5065\u7684\u6a21\u578b\uff08MNIST\u56fe\u50cfFID\u66f4\u4f4e\uff09\uff1b\u4e8c\u662f\u5728IBM\u56db\u91cf\u5b50\u6bd4\u7279\u786c\u4ef6\u4e0a\u5229\u7528\u56fa\u6709\u566a\u58f0\u751f\u6210\u56fe\u50cf\uff0c\u4e3a\u91cf\u5b50\u566a\u58f0\u4f5c\u4e3a\u8d44\u6e90\uff08\u975e\u4fee\u6b63\uff09\u7684\u5927\u89c4\u6a21\u91cf\u5b50\u751f\u6210AI\u7b97\u6cd5\u5f00\u8f9f\u65b0\u8def\u5f84\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u91cf\u5b50\u6269\u6563\u6a21\u578b\u5728\u77ed\u671f\u91cf\u5b50\u8bbe\u5907\u4e0a\u96be\u4ee5\u6269\u5c55\u7684\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u91cf\u5b50\u566a\u58f0\u4f5c\u4e3a\u751f\u6210\u8d44\u6e90\u7684\u6f5c\u529b\uff0c\u800c\u975e\u89c6\u4e3a\u9700\u4fee\u6b63\u7684\u7f3a\u9677\u3002", "method": "1. \u91cf\u5b50\u968f\u673a\u884c\u8d70\u5f62\u5f0f\u7684\u6b63\u5411\u52a8\u529b\u5b66\u6df7\u5408\u534f\u8bae\uff1b2. \u5728\u771f\u5b9eIBM\u91cf\u5b50\u786c\u4ef6\uff084\u91cf\u5b50\u6bd4\u7279\uff09\u4e0a\u5229\u7528\u56fa\u6709\u566a\u58f0\u5b9e\u73b0\u56fe\u50cf\u751f\u6210\u7b97\u6cd5\u3002", "result": "\u534f\u8bae\u4e00\u751f\u6210\u7684MNIST\u56fe\u50cfFID\u4f4e\u4e8e\u7eaf\u7ecf\u5178\u52a8\u529b\u5b66\uff1b\u534f\u8bae\u4e8c\u9a8c\u8bc1\u4e86\u91cf\u5b50\u566a\u58f0\u53ef\u4f5c\u4e3a\u751f\u6210\u8d44\u6e90\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u4e3a\u5927\u89c4\u6a21\u91cf\u5b50\u751f\u6210AI\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u8bc1\u660e\u91cf\u5b50\u566a\u58f0\u7684\u5229\u7528\u4ef7\u503c\u3002", "keywords": "\u91cf\u5b50\u6269\u6563\u6a21\u578b\u3001\u751f\u6210AI\u3001\u91cf\u5b50\u968f\u673a\u884c\u8d70\u3001\u91cf\u5b50\u566a\u58f0\u3001MNIST\u56fe\u50cf"}}
{"id": "2505.21706", "pdf": "https://arxiv.org/pdf/2505.21706", "abs": "https://arxiv.org/abs/2505.21706", "authors": ["Gonzalo Travieso", "Joao Merenda", "Odemir M. Bruno"], "title": "Network classification through random walks", "categories": ["cs.SI", "cs.LG", "physics.soc-ph"], "comment": "11 pages, 2 figures", "summary": "Network models have been widely used to study diverse systems and analyze\ntheir dynamic behaviors. Given the structural variability of networks, an\nintriguing question arises: Can we infer the type of system represented by a\nnetwork based on its structure? This classification problem involves extracting\nrelevant features from the network. Existing literature has proposed various\nmethods that combine structural measurements and dynamical processes for\nfeature extraction. In this study, we introduce a novel approach to\ncharacterize networks using statistics from random walks, which can be\nparticularly informative about network properties. We present the employed\nstatistical metrics and compare their performance on multiple datasets with\nother state-of-the-art feature extraction methods. Our results demonstrate that\nthe proposed method is effective in many cases, often outperforming existing\napproaches, although some limitations are observed across certain datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u6e38\u8d70\u7edf\u8ba1\u91cf\u7684\u65b0\u65b9\u6cd5\u7528\u4e8e\u7f51\u7edc\u5206\u7c7b\uff0c\u6bd4\u8f83\u4e86\u5176\u4e0e\u5176\u4ed6\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4e5f\u5b58\u5728\u4e00\u5b9a\u5c40\u9650\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u8ba8\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u7f51\u7edc\u7ed3\u6784\u63a8\u65ad\u7cfb\u7edf\u7c7b\u578b\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u5728\u6b64\u7c7b\u5206\u7c7b\u95ee\u9898\u4e2d\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u968f\u673a\u6e38\u8d70\u7684\u7edf\u8ba1\u91cf\u4f5c\u4e3a\u7f51\u7edc\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u591a\u6570\u636e\u96c6\u4e0e\u5176\u4ed6\u524d\u6cbf\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4f46\u5728\u67d0\u4e9b\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u5b58\u5728\u5c40\u9650\u6027\u3002", "conclusion": "\u57fa\u4e8e\u968f\u673a\u6e38\u8d70\u7684\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u5728\u7f51\u7edc\u5206\u7c7b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u9002\u5e94\u66f4\u591a\u573a\u666f\u3002", "keywords": "\u7f51\u7edc\u6a21\u578b, \u968f\u673a\u6e38\u8d70, \u7279\u5f81\u63d0\u53d6, \u7f51\u7edc\u5206\u7c7b, \u7ed3\u6784\u5206\u6790"}}
{"id": "2505.21721", "pdf": "https://arxiv.org/pdf/2505.21721", "abs": "https://arxiv.org/abs/2505.21721", "authors": ["Kyurae Kim", "Yi-An Ma", "Trevor Campbell", "Jacob R. Gardner"], "title": "Nearly Dimension-Independent Convergence of Mean-Field Black-Box Variational Inference", "categories": ["stat.ML", "cs.LG", "math.OC", "stat.CO"], "comment": null, "summary": "We prove that, given a mean-field location-scale variational family,\nblack-box variational inference (BBVI) with the reparametrization gradient\nconverges at an almost dimension-independent rate. Specifically, for strongly\nlog-concave and log-smooth targets, the number of iterations for BBVI with a\nsub-Gaussian family to achieve an objective $\\epsilon$-close to the global\noptimum is $\\mathrm{O}(\\log d)$, which improves over the $\\mathrm{O}(d)$\ndependence of full-rank location-scale families. For heavy-tailed families, we\nprovide a weaker $\\mathrm{O}(d^{2/k})$ dimension dependence, where $k$ is the\nnumber of finite moments. Additionally, if the Hessian of the target\nlog-density is constant, the complexity is free of any explicit dimension\ndependence. We also prove that our bound on the gradient variance, which is key\nto our result, cannot be improved using only spectral bounds on the Hessian of\nthe target log-density.", "AI": {"tldr": "BBVI\u7684\u91cd\u53c2\u6570\u5316\u68af\u5ea6\u5728\u5747\u503c-\u5c3a\u5ea6\u53d8\u5206\u65cf\u4e0b\u4ee5\u51e0\u4e4e\u4e0e\u7ef4\u5ea6\u65e0\u5173\u7684\u901f\u7387\u6536\u655b\uff0c\u5f3a\u5bf9\u6570\u51f9\u51f8\u76ee\u6807\u4e0b\u8fed\u4ee3\u6b21\u6570\u4e3aO(log d)\uff0c\u4f18\u4e8e\u5168\u79e9\u65cf\u7684O(d)\u3002\u5bf9\u4e8e\u91cd\u5c3e\u65cf\uff0c\u7ef4\u5ea6\u4f9d\u8d56\u4e3aO(d^{2/k})\uff0c\u82e5\u76ee\u6807\u5bf9\u6570\u5bc6\u5ea6Hessian\u4e3a\u5e38\u6570\u5219\u65e0\u663e\u5f0f\u7ef4\u5ea6\u4f9d\u8d56\u6027\u3002", "motivation": "\u7814\u7a76BBVI\u5728\u4e0d\u540c\u53d8\u5206\u65cf\u4e0b\u7684\u6536\u655b\u901f\u7387\uff0c\u7279\u522b\u662f\u7ef4\u5ea6\u4f9d\u8d56\u6027\u95ee\u9898\uff0c\u4ee5\u4f18\u5316\u53d8\u5206\u63a8\u65ad\u7684\u6548\u7387\u3002", "method": "\u91c7\u7528\u5747\u503c-\u5c3a\u5ea6\u53d8\u5206\u65cf\u548c\u91cd\u53c2\u6570\u5316\u68af\u5ea6\uff0c\u5206\u6790\u5f3a\u5bf9\u6570\u51f9\u51f8\u53ca\u91cd\u5c3e\u76ee\u6807\u4e0b\u7684\u6536\u655b\u901f\u7387\uff0c\u5e76\u901a\u8fc7Hessian\u77e9\u9635\u7684\u8c31\u8fb9\u754c\u9a8c\u8bc1\u68af\u5ea6\u65b9\u5dee\u754c\u7684\u7d27\u6027\u3002", "result": "\u5f3a\u5bf9\u6570\u51f9\u51f8\u76ee\u6807\u4e0b\u6536\u655b\u901f\u7387\u4e3aO(log d)\uff0c\u91cd\u5c3e\u65cf\u4e3aO(d^{2/k})\uff1b\u76ee\u6807Hessian\u4e3a\u5e38\u6570\u65f6\u65e0\u7ef4\u5ea6\u4f9d\u8d56\u3002\u68af\u5ea6\u65b9\u5dee\u754c\u65e0\u6cd5\u4ec5\u901a\u8fc7Hessian\u8c31\u8fb9\u754c\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "conclusion": "BBVI\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u5b9e\u73b0\u9ad8\u6548\u6536\u655b\uff0c\u6536\u655b\u901f\u7387\u5bf9\u7ef4\u5ea6\u4f9d\u8d56\u6027\u663e\u8457\u964d\u4f4e\uff0c\u4e3a\u5927\u89c4\u6a21\u63a8\u65ad\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002", "keywords": "\u53d8\u5206\u63a8\u65ad, \u5747\u503c-\u5c3a\u5ea6\u65cf, \u91cd\u53c2\u6570\u5316\u68af\u5ea6, \u6536\u655b\u901f\u7387, \u7ef4\u5ea6\u4f9d\u8d56"}}
{"id": "2505.22200", "pdf": "https://arxiv.org/pdf/2505.22200", "abs": "https://arxiv.org/abs/2505.22200", "authors": ["Darshana Saravanan", "Makarand Tapaswi", "Vineet Gandhi"], "title": "Investigating Mechanisms for In-Context Vision Language Binding", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to MIV at CVPRW 2025 (Oral)", "summary": "To understand a prompt, Vision-Language models (VLMs) must perceive the\nimage, comprehend the text, and build associations within and across both\nmodalities. For instance, given an 'image of a red toy car', the model should\nassociate this image to phrases like 'car', 'red toy', 'red object', etc. Feng\nand Steinhardt propose the Binding ID mechanism in LLMs, suggesting that the\nentity and its corresponding attribute tokens share a Binding ID in the model\nactivations. We investigate this for image-text binding in VLMs using a\nsynthetic dataset and task that requires models to associate 3D objects in an\nimage with their descriptions in the text. Our experiments demonstrate that\nVLMs assign a distinct Binding ID to an object's image tokens and its textual\nreferences, enabling in-context association.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5982\u4f55\u901a\u8fc7Binding ID\u673a\u5236\u5173\u8054\u56fe\u50cf\u4e0e\u6587\u672c\uff0c\u5b9e\u9a8c\u8868\u660eVLMs\u80fd\u4e3a\u5bf9\u8c61\u56fe\u50cf\u53ca\u5176\u6587\u672c\u63cf\u8ff0\u5206\u914d\u72ec\u7279\u7684Binding ID\u4ee5\u5b9e\u73b0\u4e0a\u4e0b\u6587\u5173\u8054\u3002", "motivation": "\u7406\u89e3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u611f\u77e5\u56fe\u50cf\u3001\u7406\u89e3\u6587\u672c\u5e76\u8de8\u6a21\u6001\u5efa\u7acb\u5173\u8054\uff0c\u5c24\u5176\u662f\u901a\u8fc7Binding ID\u673a\u5236\u5b9e\u73b0\u56fe\u50cf\u4e0e\u6587\u672c\u7684\u7ed1\u5b9a\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\u548c\u4efb\u52a1\uff0c\u8981\u6c42\u6a21\u578b\u5173\u8054\u56fe\u50cf\u4e2d\u76843D\u5bf9\u8c61\u4e0e\u5176\u6587\u672c\u63cf\u8ff0\uff0c\u9a8c\u8bc1Binding ID\u673a\u5236\u5728VLMs\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eVLMs\u80fd\u4e3a\u5bf9\u8c61\u56fe\u50cf\u53ca\u5176\u6587\u672c\u63cf\u8ff0\u5206\u914d\u72ec\u7279\u7684Binding ID\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u5173\u8054\u3002", "conclusion": "Binding ID\u673a\u5236\u5728VLMs\u4e2d\u6709\u6548\u652f\u6301\u56fe\u50cf\u4e0e\u6587\u672c\u7684\u7ed1\u5b9a\uff0c\u589e\u5f3a\u8de8\u6a21\u6001\u5173\u8054\u80fd\u529b\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b, Binding ID, \u56fe\u50cf\u6587\u672c\u7ed1\u5b9a, \u8de8\u6a21\u6001\u5173\u8054, 3D\u5bf9\u8c61"}}
{"id": "2505.21723", "pdf": "https://arxiv.org/pdf/2505.21723", "abs": "https://arxiv.org/abs/2505.21723", "authors": ["Skyler Wu", "Shihao Yang", "S. C. Kou"], "title": "Are Statistical Methods Obsolete in the Era of Deep Learning?", "categories": ["stat.CO", "cs.LG", "stat.ML"], "comment": "35 pages, 11 figures (main text)", "summary": "In the era of AI, neural networks have become increasingly popular for\nmodeling, inference, and prediction, largely due to their potential for\nuniversal approximation. With the proliferation of such deep learning models, a\nquestion arises: are leaner statistical methods still relevant? To shed insight\non this question, we employ the mechanistic nonlinear ordinary differential\nequation (ODE) inverse problem as a testbed, using physics-informed neural\nnetwork (PINN) as a representative of the deep learning paradigm and\nmanifold-constrained Gaussian process inference (MAGI) as a representative of\nstatistically principled methods. Through case studies involving the SEIR model\nfrom epidemiology and the Lorenz model from chaotic dynamics, we demonstrate\nthat statistical methods are far from obsolete, especially when working with\nsparse and noisy observations. On tasks such as parameter inference and\ntrajectory reconstruction, statistically principled methods consistently\nachieve lower bias and variance, while using far fewer parameters and requiring\nless hyperparameter tuning. Statistical methods can also decisively outperform\ndeep learning models on out-of-sample future prediction, where the absence of\nrelevant data often leads overparameterized models astray. Additionally, we\nfind that statistically principled approaches are more robust to accumulation\nof numerical imprecision and can represent the underlying system more faithful\nto the true governing ODEs.", "AI": {"tldr": "\u6bd4\u8f83\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff08\u5982PINN\uff09\u4e0e\u7edf\u8ba1\u65b9\u6cd5\uff08\u5982MAGI\uff09\u5728ODE\u53cd\u95ee\u9898\u4e2d\u7684\u8868\u73b0\uff0c\u7edf\u8ba1\u65b9\u6cd5\u5728\u7a00\u758f\u566a\u58f0\u6570\u636e\u3001\u53c2\u6570\u63a8\u65ad\u548c\u8f68\u8ff9\u91cd\u5efa\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u63a2\u8ba8\u5728AI\u65f6\u4ee3\uff0c\u7edf\u8ba1\u65b9\u6cd5\u662f\u5426\u4ecd\u5177\u6709\u4ef7\u503c\uff0c\u5c24\u5176\u5728\u5904\u7406\u7a00\u758f\u566a\u58f0\u6570\u636e\u548cODE\u53cd\u95ee\u9898\u65f6\u3002", "method": "\u4f7f\u7528SEIR\u6d41\u884c\u75c5\u5b66\u6a21\u578b\u548cLorenz\u6df7\u6c8c\u6a21\u578b\u4f5c\u4e3a\u6848\u4f8b\uff0c\u5bf9\u6bd4PINN\uff08\u6df1\u5ea6\u5b66\u4e60\uff09\u548cMAGI\uff08\u7edf\u8ba1\u65b9\u6cd5\uff09\u7684\u8868\u73b0\u3002", "result": "\u7edf\u8ba1\u65b9\u6cd5\u5728\u504f\u5dee\u3001\u65b9\u5dee\u3001\u53c2\u6570\u6548\u7387\u548c\u9884\u6d4b\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u6df1\u5ea6\u5b66\u4e60\uff0c\u5c24\u5176\u5728\u6837\u672c\u5916\u9884\u6d4b\u548c\u6570\u503c\u7cbe\u5ea6\u7d2f\u79ef\u65b9\u9762\u3002", "conclusion": "\u7edf\u8ba1\u65b9\u6cd5\u5e76\u672a\u8fc7\u65f6\uff0c\u5c24\u5176\u5728\u6570\u636e\u6709\u9650\u6216\u566a\u58f0\u8f83\u591a\u7684\u573a\u666f\u4e2d\uff0c\u5176\u7406\u8bba\u4f18\u52bf\u548c\u9c81\u68d2\u6027\u663e\u8457\u3002", "keywords": "\u795e\u7ecfODE\u3001PINN\u3001MAGI\u3001\u7edf\u8ba1\u63a8\u65ad\u3001\u6df1\u5ea6\u5b66\u4e60\u5bf9\u6bd4"}}
{"id": "2505.21734", "pdf": "https://arxiv.org/pdf/2505.21734", "abs": "https://arxiv.org/abs/2505.21734", "authors": ["Felix Jahncke", "Johannes Betz"], "title": "MIND-Stack: Modular, Interpretable, End-to-End Differentiability for Autonomous Navigation", "categories": ["cs.RO", "cs.LG"], "comment": "8 pages. Submitted to the IEEE Intelligent Vehicles Symposium (IV\n  2025), Romania", "summary": "Developing robust, efficient navigation algorithms is challenging. Rule-based\nmethods offer interpretability and modularity but struggle with learning from\nlarge datasets, while end-to-end neural networks excel in learning but lack\ntransparency and modularity. In this paper, we present MIND-Stack, a modular\nsoftware stack consisting of a localization network and a Stanley Controller\nwith intermediate human interpretable state representations and end-to-end\ndifferentiability. Our approach enables the upstream localization module to\nreduce the downstream control error, extending its role beyond state\nestimation. Unlike existing research on differentiable algorithms that either\nlack modules of the autonomous stack to span from sensor input to actuator\noutput or real-world implementation, MIND-Stack offers both capabilities. We\nconduct experiments that demonstrate the ability of the localization module to\nreduce the downstream control loss through its end-to-end differentiability\nwhile offering better performance than state-of-the-art algorithms. We showcase\nsim-to-real capabilities by deploying the algorithm on a real-world embedded\nautonomous platform with limited computation power and demonstrate simultaneous\ntraining of both the localization and controller towards one goal. While\nMIND-Stack shows good results, we discuss the incorporation of additional\nmodules from the autonomous navigation pipeline in the future, promising even\ngreater stability and performance in the next iterations of the framework.", "AI": {"tldr": "MIND-Stack\u662f\u4e00\u79cd\u6a21\u5757\u5316\u8f6f\u4ef6\u5806\u6808\uff0c\u7ed3\u5408\u4e86\u7aef\u5230\u7aef\u53ef\u5fae\u5206\u7684\u795e\u7ecf\u7f51\u7edc\u4e0e\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u72b6\u6001\u8868\u793a\uff0c\u901a\u8fc7\u5b9a\u4f4d\u6a21\u5757\u51cf\u5c11\u63a7\u5236\u8bef\u5dee\uff0c\u5e76\u5728\u771f\u5b9e\u5d4c\u5165\u5f0f\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u96be\u4ee5\u4ece\u5927\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u800c\u7aef\u5230\u7aef\u795e\u7ecf\u7f51\u7edc\u7f3a\u4e4f\u900f\u660e\u6027\u548c\u6a21\u5757\u5316\uff0cMIND-Stack\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u70b9\u3002", "method": "\u63d0\u51faMIND-Stack\uff0c\u5305\u542b\u5b9a\u4f4d\u7f51\u7edc\u548cStanley\u63a7\u5236\u5668\uff0c\u652f\u6301\u7aef\u5230\u7aef\u53ef\u5fae\u5206\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u5b9a\u4f4d\u6a21\u5757\u901a\u8fc7\u7aef\u5230\u7aef\u53ef\u5fae\u5206\u51cf\u5c11\u63a7\u5236\u635f\u5931\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\uff0c\u5e76\u5728\u771f\u5b9e\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86sim-to-real\u80fd\u529b\u3002", "conclusion": "MIND-Stack\u5c55\u793a\u4e86\u6a21\u5757\u5316\u4e0e\u7aef\u5230\u7aef\u5b66\u4e60\u7684\u6f5c\u529b\uff0c\u672a\u6765\u5c06\u6574\u5408\u66f4\u591a\u6a21\u5757\u4ee5\u63d0\u5347\u7a33\u5b9a\u6027\u4e0e\u6027\u80fd\u3002", "keywords": "\u5bfc\u822a\u7b97\u6cd5, \u6a21\u5757\u5316\u8bbe\u8ba1, \u7aef\u5230\u7aef\u5b66\u4e60, sim-to-real, \u5d4c\u5165\u5f0f\u5e73\u53f0"}}
{"id": "2505.21736", "pdf": "https://arxiv.org/pdf/2505.21736", "abs": "https://arxiv.org/abs/2505.21736", "authors": ["Zachary Schlamowitz", "Andrew Bennecke", "Daniel J. Tward"], "title": "Moment kernels: a simple and scalable approach for equivariance to rotations and reflections in deep convolutional networks", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The principle of translation equivariance (if an input image is translated an\noutput image should be translated by the same amount), led to the development\nof convolutional neural networks that revolutionized machine vision. Other\nsymmetries, like rotations and reflections, play a similarly critical role,\nespecially in biomedical image analysis, but exploiting these symmetries has\nnot seen wide adoption. We hypothesize that this is partially due to the\nmathematical complexity of methods used to exploit these symmetries, which\noften rely on representation theory, a bespoke concept in differential geometry\nand group theory. In this work, we show that the same equivariance can be\nachieved using a simple form of convolution kernels that we call ``moment\nkernels,'' and prove that all equivariant kernels must take this form. These\nare a set of radially symmetric functions of a spatial position $x$, multiplied\nby powers of the components of $x$ or the identity matrix. We implement\nequivariant neural networks using standard convolution modules, and provide\narchitectures to execute several biomedical image analysis tasks that depend on\nequivariance principles: classification (outputs are invariant under orthogonal\ntransforms), 3D image registration (outputs transform like a vector), and cell\nsegmentation (quadratic forms defining ellipses transform like a matrix).", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u77e9\u6838\u201d\u7684\u7b80\u5355\u5377\u79ef\u6838\u5f62\u5f0f\uff0c\u7528\u4e8e\u5b9e\u73b0\u65cb\u8f6c\u548c\u53cd\u5c04\u7b49\u5bf9\u79f0\u6027\u7684\u7b49\u53d8\u6027\uff0c\u7b80\u5316\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u6570\u5b66\u590d\u6742\u6027\uff0c\u5e76\u5728\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u5229\u7528\u65cb\u8f6c\u548c\u53cd\u5c04\u5bf9\u79f0\u6027\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u590d\u6742\u7684\u6570\u5b66\u7406\u8bba\uff0c\u5982\u8868\u793a\u7406\u8bba\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002\u4f5c\u8005\u65e8\u5728\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u63d0\u51fa\u66f4\u6613\u5b9e\u73b0\u7684\u7b49\u53d8\u6027\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u79f0\u4e3a\u201c\u77e9\u6838\u201d\u7684\u7b80\u5355\u5377\u79ef\u6838\u5f62\u5f0f\uff0c\u8fd9\u4e9b\u6838\u7531\u7a7a\u95f4\u4f4d\u7f6e\u7684\u5f84\u5411\u5bf9\u79f0\u51fd\u6570\u4e58\u4ee5x\u7684\u5206\u91cf\u7684\u5e42\u6216\u5355\u4f4d\u77e9\u9635\u6784\u6210\u3002\u901a\u8fc7\u6807\u51c6\u5377\u79ef\u6a21\u5757\u5b9e\u73b0\u7b49\u53d8\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5206\u7c7b\uff08\u8f93\u51fa\u6b63\u4ea4\u53d8\u6362\u4e0d\u53d8\uff09\u30013D\u56fe\u50cf\u914d\u51c6\uff08\u8f93\u51fa\u5411\u91cf\u53d8\u6362\uff09\u548c\u7ec6\u80de\u5206\u5272\uff08\u692d\u5706\u4e8c\u6b21\u5f62\u5f0f\u77e9\u9635\u53d8\u6362\uff09\u7b49\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u77e9\u6838\u5f62\u5f0f\u7b80\u5316\u4e86\u5bf9\u79f0\u6027\u7b49\u53d8\u6027\u7684\u5b9e\u73b0\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6613\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u7b49\u53d8\u6027\u3001\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3001\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u3001\u77e9\u6838\u3001\u5bf9\u79f0\u6027\u3001\u65cb\u8f6c\u53cd\u5c04"}}
{"id": "2505.21742", "pdf": "https://arxiv.org/pdf/2505.21742", "abs": "https://arxiv.org/abs/2505.21742", "authors": ["Briglia Maria Rosaria", "Mujtaba Hussain Mirza", "Giuseppe Lisanti", "Iacopo Masi"], "title": "What is Adversarial Training for Diffusion Models?", "categories": ["cs.CV", "cs.LG"], "comment": "40 pages", "summary": "We answer the question in the title, showing that adversarial training (AT)\nfor diffusion models (DMs) fundamentally differs from classifiers: while AT in\nclassifiers enforces output invariance, AT in DMs requires equivariance to keep\nthe diffusion process aligned with the data distribution. AT is a way to\nenforce smoothness in the diffusion flow, improving robustness to outliers and\ncorrupted data. Unlike prior art, our method makes no assumptions about the\nnoise model and integrates seamlessly into diffusion training by adding random\nnoise, similar to randomized smoothing, or adversarial noise, akin to AT. This\nenables intrinsic capabilities such as handling noisy data, dealing with\nextreme variability such as outliers, preventing memorization, and improving\nrobustness. We rigorously evaluate our approach with proof-of-concept datasets\nwith known distributions in low- and high-dimensional space, thereby taking a\nperfect measure of errors; we further evaluate on standard benchmarks such as\nCIFAR-10, CelebA and LSUN Bedroom, showing strong performance under severe\nnoise, data corruption, and iterative adversarial attacks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u6297\u8bad\u7ec3\uff08AT\uff09\u5728\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u4e2d\u4e0e\u5206\u7c7b\u5668\u4e2d\u8868\u73b0\u4e0d\u540c\uff1a\u5206\u7c7b\u5668\u7684AT\u5f3a\u5236\u8f93\u51fa\u4e0d\u53d8\u6027\uff0c\u800cDMs\u4e2d\u7684AT\u9700\u8981\u7b49\u53d8\u6027\u4ee5\u4fdd\u6301\u6269\u6563\u8fc7\u7a0b\u4e0e\u6570\u636e\u5206\u5e03\u5bf9\u9f50\u3002AT\u901a\u8fc7\u589e\u5f3a\u6269\u6563\u6d41\u7684\u5e73\u6ed1\u6027\uff0c\u63d0\u5347\u5bf9\u5f02\u5e38\u503c\u548c\u566a\u58f0\u6570\u636e\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u63a2\u7d22\u5bf9\u6297\u8bad\u7ec3\u5728\u6269\u6563\u6a21\u578b\u4e2d\u4e0e\u5206\u7c7b\u5668\u4e2d\u7684\u4e0d\u540c\u8868\u73b0\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6548\u679c\u3002", "method": "\u901a\u8fc7\u5411\u6269\u6563\u8bad\u7ec3\u4e2d\u968f\u673a\u6dfb\u52a0\u566a\u58f0\uff08\u7c7b\u4f3c\u968f\u673a\u5e73\u6ed1\uff09\u6216\u5bf9\u6297\u566a\u58f0\uff08\u7c7b\u4f3cAT\uff09\uff0c\u65e0\u9700\u5047\u8bbe\u566a\u58f0\u6a21\u578b\uff0c\u65e0\u7f1d\u96c6\u6210\u5230\u6269\u6563\u8bad\u7ec3\u4e2d\u3002", "result": "\u5728\u4f4e\u7ef4\u548c\u9ad8\u7ef4\u7a7a\u95f4\u7684\u6982\u5ff5\u9a8c\u8bc1\u6570\u636e\u96c6\u53ca\u6807\u51c6\u57fa\u51c6\uff08\u5982CIFAR-10\u3001CelebA\u548cLSUN Bedroom\uff09\u4e2d\uff0c\u6a21\u578b\u5728\u566a\u58f0\u3001\u6570\u636e\u635f\u574f\u548c\u5bf9\u6297\u653b\u51fb\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u5904\u7406\u566a\u58f0\u6570\u636e\u3001\u6781\u7aef\u53d8\u5f02\u6027\uff0c\u8fd8\u80fd\u9632\u6b62\u8bb0\u5fc6\u5316\u5e76\u63d0\u5347\u9c81\u68d2\u6027\u3002", "keywords": "\u5bf9\u6297\u8bad\u7ec3, \u6269\u6563\u6a21\u578b, \u9c81\u68d2\u6027, \u566a\u58f0\u5904\u7406, \u6570\u636e\u5206\u5e03"}}
{"id": "2505.21767", "pdf": "https://arxiv.org/pdf/2505.21767", "abs": "https://arxiv.org/abs/2505.21767", "authors": ["Xiaoyan Li", "Shixin Xu", "Faisal Habib", "Arvind Gupta", "Huaxiong Huang"], "title": "Beyond 1D: Vision Transformers and Multichannel Signal Images for PPG-to-ECG Reconstruction", "categories": ["eess.IV", "cs.LG", "eess.SP"], "comment": null, "summary": "Reconstructing ECG from PPG is a promising yet challenging task. While recent\nadvancements in generative models have significantly improved ECG\nreconstruction, accurately capturing fine-grained waveform features remains a\nkey challenge. To address this, we propose a novel PPG-to-ECG reconstruction\nmethod that leverages a Vision Transformer (ViT) as the core network. Unlike\nconventional approaches that rely on single-channel PPG, our method employs a\nfour-channel signal image representation, incorporating the original PPG, its\nfirst-order difference, second-order difference, and area under the curve. This\nmulti-channel design enriches feature extraction by preserving both temporal\nand physiological variations within the PPG. By leveraging the self-attention\nmechanism in ViT, our approach effectively captures both inter-beat and\nintra-beat dependencies, leading to more robust and accurate ECG\nreconstruction. Experimental results demonstrate that our method consistently\noutperforms existing 1D convolution-based approaches, achieving up to 29%\nreduction in PRD and 15% reduction in RMSE. The proposed approach also produces\nimprovements in other evaluation metrics, highlighting its robustness and\neffectiveness in reconstructing ECG signals. Furthermore, to ensure a\nclinically relevant evaluation, we introduce new performance metrics, including\nQRS area error, PR interval error, RT interval error, and RT amplitude\ndifference error. Our findings suggest that integrating a four-channel signal\nimage representation with the self-attention mechanism of ViT enables more\neffective extraction of informative PPG features and improved modeling of\nbeat-to-beat variations for PPG-to-ECG mapping. Beyond demonstrating the\npotential of PPG as a viable alternative for heart activity monitoring, our\napproach opens new avenues for cyclic signal analysis and prediction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eVision Transformer\uff08ViT\uff09\u7684\u65b0\u578bPPG\u5230ECG\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u56db\u901a\u9053\u4fe1\u53f7\u56fe\u50cf\u8868\u793a\uff08\u5305\u62ecPPG\u53ca\u5176\u4e00\u9636\u3001\u4e8c\u9636\u5dee\u503c\u548c\u66f2\u7ebf\u4e0b\u9762\u79ef\uff09\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\uff0c\u5229\u7528ViT\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u6ce2\u5f62\u7ec6\u8282\uff0c\u663e\u8457\u63d0\u5347\u4e86ECG\u91cd\u5efa\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709PPG-to-ECG\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u6ce2\u5f62\u7279\u5f81\uff0c\u9650\u5236\u4e86\u91cd\u5efa\u7cbe\u5ea6\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u591a\u901a\u9053\u4fe1\u53f7\u8868\u793a\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u4f7f\u7528\u56db\u901a\u9053PPG\u4fe1\u53f7\u56fe\u50cf\uff08\u539f\u59cbPPG\u3001\u4e00\u9636/\u4e8c\u9636\u5dee\u503c\u3001\u66f2\u7ebf\u4e0b\u9762\u79ef\uff09\u4f5c\u4e3aViT\u8f93\u5165\uff0c\u5229\u7528\u5176\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5efa\u6a21\u8282\u62cd\u5185\u548c\u8282\u62cd\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf1D\u5377\u79ef\u65b9\u6cd5\uff0cPRD\u964d\u4f4e29%\u3001RMSE\u964d\u4f4e15%\uff0c\u65b0\u63d0\u51fa\u7684\u4e34\u5e8a\u6307\u6807\uff08\u5982QRS\u9762\u79ef\u8bef\u5dee\u7b49\uff09\u4e5f\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u56db\u901a\u9053\u8868\u793a\u4e0eViT\u7ed3\u5408\u53ef\u66f4\u6709\u6548\u63d0\u53d6PPG\u7279\u5f81\u5e76\u5efa\u6a21\u8282\u62cd\u53d8\u5316\uff0c\u4e3a\u5faa\u73af\u4fe1\u53f7\u5206\u6790\u5f00\u8f9f\u65b0\u9014\u5f84\u3002", "keywords": "PPG-to-ECG, Vision Transformer, \u81ea\u6ce8\u610f\u529b\u673a\u5236, \u56db\u901a\u9053\u4fe1\u53f7, \u5fc3\u7535\u91cd\u5efa"}}
{"id": "2505.21791", "pdf": "https://arxiv.org/pdf/2505.21791", "abs": "https://arxiv.org/abs/2505.21791", "authors": ["Julia Nakhleh", "Robert D. Nowak"], "title": "Global Minimizers of $\\ell^p$-Regularized Objectives Yield the Sparsest ReLU Neural Networks", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Overparameterized neural networks can interpolate a given dataset in many\ndifferent ways, prompting the fundamental question: which among these solutions\nshould we prefer, and what explicit regularization strategies will provably\nyield these solutions? This paper addresses the challenge of finding the\nsparsest interpolating ReLU network -- i.e., the network with the fewest\nnonzero parameters or neurons -- a goal with wide-ranging implications for\nefficiency, generalization, interpretability, theory, and model compression.\nUnlike post hoc pruning approaches, we propose a continuous, almost-everywhere\ndifferentiable training objective whose global minima are guaranteed to\ncorrespond to the sparsest single-hidden-layer ReLU networks that fit the data.\nThis result marks a conceptual advance: it recasts the combinatorial problem of\nsparse interpolation as a smooth optimization task, potentially enabling the\nuse of gradient-based training methods. Our objective is based on minimizing\n$\\ell^p$ quasinorms of the weights for $0 < p < 1$, a classical\nsparsity-promoting strategy in finite-dimensional settings. However, applying\nthese ideas to neural networks presents new challenges: the function class is\ninfinite-dimensional, and the weights are learned using a highly nonconvex\nobjective. We prove that, under our formulation, global minimizers correspond\nexactly to sparsest solutions. Our work lays a foundation for understanding\nwhen and how continuous sparsity-inducing objectives can be leveraged to\nrecover sparse networks through training.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8fde\u7eed\u53ef\u5fae\u7684\u8bad\u7ec3\u76ee\u6807\uff0c\u65e8\u5728\u627e\u5230\u62df\u5408\u6570\u636e\u7684\u6700\u7a00\u758fReLU\u7f51\u7edc\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u6743\u91cd\u7684$\backslash$ell\backslash^p$\u62df\u8303\u6570\uff080 < p < 1\uff09\u6765\u5b9e\u73b0\uff0c\u7406\u8bba\u4e0a\u4fdd\u8bc1\u5168\u5c40\u6781\u5c0f\u503c\u5bf9\u5e94\u6700\u7a00\u758f\u89e3\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u8fc7\u53c2\u6570\u5316\u795e\u7ecf\u7f51\u7edc\u4e2d\u5982\u4f55\u9009\u62e9\u6700\u4f18\u89e3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5982\u4f55\u901a\u8fc7\u663e\u5f0f\u6b63\u5219\u5316\u7b56\u7565\u627e\u5230\u6700\u7a00\u758f\u7684\u63d2\u503c\u7f51\u7edc\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u3001\u6cdb\u5316\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6a21\u578b\u538b\u7f29\u6548\u679c\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u8bbe\u8ba1\u4e00\u4e2a\u8fde\u7eed\u4e14\u51e0\u4e4e\u5904\u5904\u53ef\u5fae\u7684\u8bad\u7ec3\u76ee\u6807\uff0c\u8be5\u76ee\u6807\u6700\u5c0f\u5316\u6743\u91cd\u7684$\backslash$ell\backslash^p$\u62df\u8303\u6570\uff080 < p < 1\uff09\uff0c\u5c06\u7a00\u758f\u63d2\u503c\u7684\u7ec4\u5408\u95ee\u9898\u8f6c\u5316\u4e3a\u5e73\u6ed1\u4f18\u5316\u4efb\u52a1\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8bc1\u660e\u4e86\u5168\u5c40\u6781\u5c0f\u503c\u786e\u5b9e\u5bf9\u5e94\u4e8e\u6700\u7a00\u758f\u7684\u5355\u9690\u85cf\u5c42ReLU\u7f51\u7edc\u89e3\uff0c\u4e3a\u7406\u89e3\u5982\u4f55\u901a\u8fc7\u8fde\u7eed\u7a00\u758f\u8bf1\u5bfc\u76ee\u6807\u8bad\u7ec3\u6062\u590d\u7a00\u758f\u7f51\u7edc\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u7406\u8bba\u4e0a\u6709\u4fdd\u969c\uff0c\u8fd8\u4e3a\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\u5728\u7a00\u758f\u7f51\u7edc\u8bad\u7ec3\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u8fc7\u53c2\u6570\u5316\u3001\u795e\u7ecf\u7f51\u7edc\u3001\u7a00\u758f\u6027\u3001ReLU\u7f51\u7edc\u3001$\backslash$ell\backslash^p$\u62df\u8303\u6570"}}
{"id": "2505.22287", "pdf": "https://arxiv.org/pdf/2505.22287", "abs": "https://arxiv.org/abs/2505.22287", "authors": ["Daniel McDuff", "Tim Korjakow", "Kevin Klyman", "Danish Contractor"], "title": "New Tools are Needed for Tracking Adherence to AI Model Behavioral Use Clauses", "categories": ["cs.CY", "cs.AI"], "comment": "Preprint", "summary": "Foundation models have had a transformative impact on AI. A combination of\nlarge investments in research and development, growing sources of digital data\nfor training, and architectures that scale with data and compute has led to\nmodels with powerful capabilities. Releasing assets is fundamental to\nscientific advancement and commercial enterprise. However, concerns over\nnegligent or malicious uses of AI have led to the design of mechanisms to limit\nthe risks of the technology. The result has been a proliferation of licenses\nwith behavioral-use clauses and acceptable-use-policies that are increasingly\nbeing adopted by commonly used families of models (Llama, Gemma, Deepseek) and\na myriad of smaller projects. We created and deployed a custom AI licenses\ngenerator to facilitate license creation and have quantitatively and\nqualitatively analyzed over 300 customized licenses created with this tool.\nAlongside this we analyzed 1.7 million models licenses on the HuggingFace model\nhub. Our results show increasing adoption of these licenses, interest in tools\nthat support their creation and a convergence on common clause configurations.\nIn this paper we take the position that tools for tracking adoption of, and\nadherence to, these licenses is the natural next step and urgently needed in\norder to ensure they have the desired impact of ensuring responsible use.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86AI\u57fa\u7840\u6a21\u578b\u53d1\u5e03\u4e2d\u7684\u8bb8\u53ef\u8bc1\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u8bb8\u53ef\u8bc1\u751f\u6210\u5de5\u5177\u5e76\u5206\u6790\u4e86300\u591a\u4e2a\u5b9a\u5236\u8bb8\u53ef\u8bc1\u53ca170\u4e07HuggingFace\u8bb8\u53ef\u8bc1\uff0c\u53d1\u73b0\u884c\u4e3a\u6761\u6b3e\u8bb8\u53ef\u8bc1\u7684\u666e\u53ca\u589e\u52a0\uff0c\u547c\u5401\u5f00\u53d1\u5de5\u5177\u8ddf\u8e2a\u5176\u91c7\u7528\u4ee5\u786e\u4fdd\u8d1f\u8d23\u4efb\u4f7f\u7528\u3002", "motivation": "AI\u57fa\u7840\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u4f34\u968f\u7740\u5bf9\u5176\u6f5c\u5728\u6ee5\u7528\u7684\u62c5\u5fe7\uff0c\u5f53\u524d\u7684\u8bb8\u53ef\u8bc1\u673a\u5236\u65e8\u5728\u9650\u5236\u98ce\u9669\u4f46\u7f3a\u4e4f\u8ddf\u8e2a\u5de5\u5177\uff0c\u7814\u7a76\u8bd5\u56fe\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u81ea\u5b9a\u4e49AI\u8bb8\u53ef\u8bc1\u751f\u6210\u5de5\u5177\uff0c\u5206\u6790300\u4e2a\u5b9a\u5236\u8bb8\u53ef\u8bc1\u53caHuggingFace\u4e0a170\u4e07\u6a21\u578b\u8bb8\u53ef\u8bc1\uff0c\u5b9a\u91cf\u4e0e\u5b9a\u6027\u7ed3\u5408\u7814\u7a76\u91c7\u7528\u8d8b\u52bf\u3002", "result": "\u884c\u4e3a\u6761\u6b3e\u8bb8\u53ef\u8bc1\u91c7\u7528\u7387\u589e\u957f\uff0c\u5de5\u5177\u9700\u6c42\u663e\u8457\uff0c\u6761\u6b3e\u914d\u7f6e\u8d8b\u540c\uff0c\u8868\u660e\u793e\u533a\u5bf9\u8d1f\u8d23\u4efbAI\u4f7f\u7528\u7684\u5173\u6ce8\u3002", "conclusion": "\u4e9f\u9700\u5f00\u53d1\u5de5\u5177\u8ddf\u8e2a\u8bb8\u53ef\u8bc1\u91c7\u7528\u4e0e\u9075\u5b88\u60c5\u51b5\uff0c\u4ee5\u786e\u4fdd\u5176\u5b9e\u73b0\u4fc3\u8fdbAI\u8d1f\u8d23\u4efb\u4f7f\u7528\u7684\u76ee\u6807\u3002", "keywords": "AI\u57fa\u7840\u6a21\u578b, \u8bb8\u53ef\u8bc1, \u8d1f\u8d23\u4efbAI, \u884c\u4e3a\u6761\u6b3e, HuggingFace"}}
{"id": "2505.21796", "pdf": "https://arxiv.org/pdf/2505.21796", "abs": "https://arxiv.org/abs/2505.21796", "authors": ["Sajad Khodadadian", "Martin Zubeldia"], "title": "A General-Purpose Theorem for High-Probability Bounds of Stochastic Approximation with Polyak Averaging", "categories": ["stat.ML", "cs.LG", "math.PR"], "comment": "37 pages", "summary": "Polyak-Ruppert averaging is a widely used technique to achieve the optimal\nasymptotic variance of stochastic approximation (SA) algorithms, yet its\nhigh-probability performance guarantees remain underexplored in general\nsettings. In this paper, we present a general framework for establishing\nnon-asymptotic concentration bounds for the error of averaged SA iterates. Our\napproach assumes access to individual concentration bounds for the unaveraged\niterates and yields a sharp bound on the averaged iterates. We also construct\nan example, showing the tightness of our result up to constant multiplicative\nfactors. As direct applications, we derive tight concentration bounds for\ncontractive SA algorithms and for algorithms such as temporal difference\nlearning and Q-learning with averaging, obtaining new bounds in settings where\ntraditional analysis is challenging.", "AI": {"tldr": "\u63d0\u4f9bPolyak-Ruppert\u5e73\u5747\u6280\u672f\u7684\u9ad8\u6982\u7387\u6027\u80fd\u4fdd\u8bc1\u6846\u67b6\uff0c\u5c55\u793a\u5728\u975e\u6e10\u8fd1\u60c5\u51b5\u4e0b\u5e73\u5747SA\u8fed\u4ee3\u8bef\u5dee\u7684\u7d27\u81f4\u8fb9\u754c\u3002", "motivation": "\u63a2\u7a76Polyak-Ruppert\u5e73\u5747\u5728\u968f\u673a\u903c\u8fd1\u7b97\u6cd5\u4e2d\u7684\u9ad8\u6982\u7387\u6027\u80fd\uff0c\u56e0\u4e3a\u73b0\u6709\u7814\u7a76\u5728\u4e00\u822c\u8bbe\u7f6e\u4e2d\u5bf9\u6b64\u6027\u80fd\u7684\u4fdd\u8bc1\u4ecd\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u57fa\u4e8e\u672a\u5e73\u5747\u8fed\u4ee3\u7684\u4e2a\u4f53\u8fb9\u754c\uff0c\u63a8\u5bfc\u51fa\u5e73\u5747\u8fed\u4ee3\u7684\u7d27\u81f4\u8fb9\u754c\uff0c\u5e76\u901a\u8fc7\u793a\u4f8b\u9a8c\u8bc1\u7ed3\u679c\u7684\u7d27\u81f4\u6027\u3002", "result": "\u6846\u67b6\u6210\u529f\u6784\u5efa\u4e86\u7d27\u81f4\u7684\u975e\u6e10\u8fd1\u6d53\u5ea6\u8fb9\u754c\uff0c\u9002\u7528\u4e8e\u968f\u673a\u903c\u8fd1\u7b97\u6cd5\u5982\u65f6\u95f4\u5dee\u5206\u5b66\u4e60\u4e0eQ\u5b66\u4e60\u7684\u5e73\u5747\u7248\u672c\u3002", "conclusion": "\u8be5\u6846\u67b6\u586b\u8865\u4e86\u9ad8\u6982\u7387\u6027\u80fd\u5206\u6790\u9886\u57df\u7684\u7a7a\u767d\uff0c\u5e76\u4e3a\u76f8\u5173\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002", "keywords": "Polyak-Ruppert\u5e73\u5747, \u968f\u673a\u903c\u8fd1, \u9ad8\u6982\u7387\u6027\u80fd, \u65f6\u95f4\u5dee\u5206\u5b66\u4e60, Q\u5b66\u4e60"}}
{"id": "2505.22291", "pdf": "https://arxiv.org/pdf/2505.22291", "abs": "https://arxiv.org/abs/2505.22291", "authors": ["Saptarshi Neil Sinha", "P. Julius Kuehn", "Johannes Koppe", "Arjan Kuijper", "Michael Weinmann"], "title": "Neural Restoration of Greening Defects in Historical Autochrome Photographs Based on Purely Synthetic Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The preservation of early visual arts, particularly color photographs, is\nchallenged by deterioration caused by aging and improper storage, leading to\nissues like blurring, scratches, color bleeding, and fading defects. In this\npaper, we present the first approach for the automatic removal of greening\ncolor defects in digitized autochrome photographs. Our main contributions\ninclude a method based on synthetic dataset generation and the use of\ngenerative AI with a carefully designed loss function for the restoration of\nvisual arts. To address the lack of suitable training datasets for analyzing\ngreening defects in damaged autochromes, we introduce a novel approach for\naccurately simulating such defects in synthetic data. We also propose a\nmodified weighted loss function for the ChaIR method to account for color\nimbalances between defected and non-defected areas. While existing methods\nstruggle with accurately reproducing original colors and may require\nsignificant manual effort, our method allows for efficient restoration with\nreduced time requirements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5408\u6210\u6570\u636e\u96c6\u751f\u6210\u548c\u751f\u6210\u5f0fAI\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u4fee\u590d\u65e9\u671f\u5f69\u8272\u7167\u7247\u4e2d\u7684\u7eff\u53d8\u7f3a\u9677\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u51c6\u786e\u8fd8\u539f\u989c\u8272\u548c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u65e9\u671f\u5f69\u8272\u7167\u7247\uff08\u5982\u5929\u7136\u5f69\u8272\u7167\u7247\uff09\u7684\u4fdd\u5b58\u9762\u4e34\u56e0\u8001\u5316\u548c\u4e0d\u5f53\u5b58\u50a8\u5bfc\u81f4\u7684\u7eff\u53d8\u7b49\u7f3a\u9677\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u8fd8\u539f\u989c\u8272\u4e14\u9700\u5927\u91cf\u4eba\u5de5\u5e72\u9884\u3002", "method": "\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u6a21\u62df\u7eff\u53d8\u7f3a\u9677\uff0c\u6539\u8fdbChaIR\u65b9\u6cd5\u7684\u52a0\u6743\u635f\u5931\u51fd\u6570\uff0c\u5229\u7528\u751f\u6210\u5f0fAI\u8fdb\u884c\u4fee\u590d\u3002", "result": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4fee\u590d\u4e86\u7eff\u53d8\u7f3a\u9677\uff0c\u51cf\u5c11\u4e86\u65f6\u95f4\u6210\u672c\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u89c6\u89c9\u827a\u672f\u4fee\u590d\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u7eff\u53d8\u7f3a\u9677\u7684\u4fee\u590d\u3002", "keywords": "\u89c6\u89c9\u827a\u672f\u4fee\u590d, \u7eff\u53d8\u7f3a\u9677, \u5408\u6210\u6570\u636e\u96c6, \u751f\u6210\u5f0fAI, \u52a0\u6743\u635f\u5931\u51fd\u6570"}}
{"id": "2505.21799", "pdf": "https://arxiv.org/pdf/2505.21799", "abs": "https://arxiv.org/abs/2505.21799", "authors": ["Tim Tsz-Kit Lau", "Qi Long", "Weijie Su"], "title": "PolarGrad: A Class of Matrix-Gradient Optimizers from a Unifying Preconditioning Perspective", "categories": ["math.OC", "cs.LG", "stat.ML"], "comment": null, "summary": "The ever-growing scale of deep learning models and datasets underscores the\ncritical importance of efficient optimization methods. While preconditioned\ngradient methods such as Adam and AdamW are the de facto optimizers for\ntraining neural networks and large language models, structure-aware\npreconditioned optimizers like Shampoo and Muon, which utilize the matrix\nstructure of gradients, have demonstrated promising evidence of faster\nconvergence. In this paper, we introduce a unifying framework for analyzing\n\"matrix-aware\" preconditioned methods, which not only sheds light on the\neffectiveness of Muon and related optimizers but also leads to a class of new\nstructure-aware preconditioned methods. A key contribution of this framework is\nits precise distinction between preconditioning strategies that treat neural\nnetwork weights as vectors (addressing curvature anisotropy) versus those that\nconsider their matrix structure (addressing gradient anisotropy). This\nperspective provides new insights into several empirical phenomena in language\nmodel pre-training, including Adam's training instabilities, Muon's accelerated\nconvergence, and the necessity of learning rate warmup for Adam. Building upon\nthis framework, we introduce PolarGrad, a new class of preconditioned\noptimization methods based on the polar decomposition of matrix-valued\ngradients. As a special instance, PolarGrad includes Muon with updates scaled\nby the nuclear norm of the gradients. We provide numerical implementations of\nthese methods, leveraging efficient numerical polar decomposition algorithms\nfor enhanced convergence. Our extensive evaluations across diverse matrix\noptimization problems and language model pre-training tasks demonstrate that\nPolarGrad outperforms both Adam and Muon.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u5206\u6790\u57fa\u4e8e\u77e9\u9635\u7ed3\u6784\u7684\u9884\u6761\u4ef6\u4f18\u5316\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86PolarGrad\u8fd9\u4e00\u65b0\u65b9\u6cd5\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u6570\u636e\u96c6\u7684\u89c4\u6a21\u4e0d\u65ad\u6269\u5927\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002\u73b0\u6709\u7684Adam\u548cAdamW\u7b49\u9884\u6761\u4ef6\u68af\u5ea6\u65b9\u6cd5\u867d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46Shampoo\u548cMuon\u7b49\u57fa\u4e8e\u77e9\u9635\u7ed3\u6784\u7684\u4f18\u5316\u5668\u5c55\u73b0\u51fa\u66f4\u5feb\u7684\u6536\u655b\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5206\u6790\u6846\u67b6\uff0c\u533a\u5206\u5411\u91cf\u548c\u77e9\u9635\u7ed3\u6784\u7684\u9884\u6761\u4ef6\u7b56\u7565\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86PolarGrad\u65b9\u6cd5\uff0c\u5229\u7528\u77e9\u9635\u68af\u5ea6\u7684\u6781\u5206\u89e3\u3002", "result": "PolarGrad\u5728\u77e9\u9635\u4f18\u5316\u95ee\u9898\u548c\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eAdam\u548cMuon\u3002", "conclusion": "\u77e9\u9635\u7ed3\u6784\u7684\u9884\u6761\u4ef6\u4f18\u5316\u65b9\u6cd5\u80fd\u66f4\u9ad8\u6548\u8bad\u7ec3\u6df1\u5ea6\u6a21\u578b\uff0cPolarGrad\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u65b9\u5411\u3002", "keywords": "\u9884\u6761\u4ef6\u4f18\u5316, \u77e9\u9635\u7ed3\u6784, PolarGrad, \u6df1\u5ea6\u5b66\u4e60, \u6781\u5206\u89e3"}}
{"id": "2505.22303", "pdf": "https://arxiv.org/pdf/2505.22303", "abs": "https://arxiv.org/abs/2505.22303", "authors": ["Grzegorz Wolny", "Micha\u0142 Szczerbak"], "title": "Voice CMS: updating the knowledge base of a digital assistant through conversation", "categories": ["cs.HC", "cs.AI", "cs.MA"], "comment": null, "summary": "In this study, we propose a solution based on a multi-agent LLM architecture\nand a voice user interface (VUI) designed to update the knowledge base of a\ndigital assistant. Its usability is evaluated in comparison to a more\ntraditional graphical content management system (CMS), with a focus on\nunderstanding the relationship between user preferences and the complexity of\nthe information being provided. The findings demonstrate that, while the\noverall usability of the VUI is rated lower than the graphical interface, it is\nalready preferred by users for less complex tasks. Furthermore, the quality of\ncontent entered through the VUI is comparable to that achieved with the\ngraphical interface, even for highly complex tasks. Obtained qualitative\nresults suggest that a hybrid interface combining the strengths of both\napproaches could address the key challenges identified during the experiment,\nsuch as reducing cognitive load through graphical feedback while maintaining\nthe intuitive nature of voice-based interactions. This work highlights the\npotential of conversational interfaces as a viable and effective method for\nknowledge management in specific business contexts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53LLM\u67b6\u6784\u548c\u8bed\u97f3\u7528\u6237\u754c\u9762\uff08VUI\uff09\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u66f4\u65b0\u6570\u5b57\u52a9\u7406\u7684\u77e5\u8bc6\u5e93\u3002\u7814\u7a76\u6bd4\u8f83\u4e86VUI\u4e0e\u4f20\u7edf\u56fe\u5f62\u5185\u5bb9\u7ba1\u7406\u7cfb\u7edf\uff08CMS\uff09\u7684\u53ef\u7528\u6027\uff0c\u53d1\u73b0VUI\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u66f4\u53d7\u7528\u6237\u9752\u7750\uff0c\u4e14\u5185\u5bb9\u8d28\u91cf\u4e0e\u56fe\u5f62\u754c\u9762\u76f8\u5f53\u3002\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u6df7\u5408\u754c\u9762\u53ef\u80fd\u662f\u89e3\u51b3\u8ba4\u77e5\u8d1f\u8377\u7b49\u95ee\u9898\u7684\u6709\u6548\u65b9\u6848\u3002", "motivation": "\u63a2\u7d22\u8bed\u97f3\u7528\u6237\u754c\u9762\uff08VUI\uff09\u5728\u77e5\u8bc6\u7ba1\u7406\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u53ca\u7528\u6237\u5bf9\u4e0d\u540c\u754c\u9762\u7c7b\u578b\u7684\u504f\u597d\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53LLM\u67b6\u6784\u548cVUI\u8bbe\u8ba1\uff0c\u901a\u8fc7\u4e0e\u4f20\u7edf\u56fe\u5f62CMS\u7684\u5bf9\u6bd4\u5b9e\u9a8c\u8bc4\u4f30\u53ef\u7528\u6027\u3002", "result": "VUI\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u66f4\u53d7\u6b22\u8fce\uff0c\u5185\u5bb9\u8d28\u91cf\u4e0e\u56fe\u5f62\u754c\u9762\u76f8\u5f53\uff1b\u6df7\u5408\u754c\u9762\u53ef\u80fd\u89e3\u51b3\u8ba4\u77e5\u8d1f\u8377\u95ee\u9898\u3002", "conclusion": "\u5bf9\u8bdd\u5f0f\u754c\u9762\u5728\u7279\u5b9a\u5546\u4e1a\u573a\u666f\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u7ed3\u5408VUI\u548c\u56fe\u5f62\u754c\u9762\u7684\u6df7\u5408\u65b9\u6848\u53ef\u80fd\u66f4\u4f18\u3002", "keywords": "\u591a\u667a\u80fd\u4f53LLM, \u8bed\u97f3\u7528\u6237\u754c\u9762, \u77e5\u8bc6\u7ba1\u7406, \u53ef\u7528\u6027\u6d4b\u8bd5, \u6df7\u5408\u754c\u9762"}}
{"id": "2505.21809", "pdf": "https://arxiv.org/pdf/2505.21809", "abs": "https://arxiv.org/abs/2505.21809", "authors": ["Jaya Narain", "Vasudha Kowtha", "Colin Lea", "Lauren Tooley", "Dianna Yee", "Vikramjit Mitra", "Zifang Huang", "Miquel Espi Marques", "Jon Huang", "Carlos Avendano", "Shirley Ren"], "title": "Voice Quality Dimensions as Interpretable Primitives for Speaking Style for Atypical Speech and Affect", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": "accepted for Interspeech 2025", "summary": "Perceptual voice quality dimensions describe key characteristics of atypical\nspeech and other speech modulations. Here we develop and evaluate voice quality\nmodels for seven voice and speech dimensions (intelligibility, imprecise\nconsonants, harsh voice, naturalness, monoloudness, monopitch, and\nbreathiness). Probes were trained on the public Speech Accessibility (SAP)\nproject dataset with 11,184 samples from 434 speakers, using embeddings from\nfrozen pre-trained models as features. We found that our probes had both strong\nperformance and strong generalization across speech elicitation categories in\nthe SAP dataset. We further validated zero-shot performance on additional\ndatasets, encompassing unseen languages and tasks: Italian atypical speech,\nEnglish atypical speech, and affective speech. The strong zero-shot performance\nand the interpretability of results across an array of evaluations suggests the\nutility of using voice quality dimensions in speaking style-related tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u4e03\u4e2a\u8bed\u97f3\u8d28\u91cf\u7ef4\u5ea6\u7684\u6a21\u578b\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5d4c\u5165\u7279\u5f81\u8fdb\u884c\u8bad\u7ec3\u3002\u6a21\u578b\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ec\u672a\u89c1\u8fc7\u7684\u8bed\u8a00\u548c\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u8bed\u97f3\u8d28\u91cf\u7ef4\u5ea6\u5728\u8bed\u97f3\u98ce\u683c\u76f8\u5173\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5f00\u53d1\u8bed\u97f3\u8d28\u91cf\u7ef4\u5ea6\u7684\u6a21\u578b\uff0c\u63d0\u5347\u5bf9\u975e\u5178\u578b\u8bed\u97f3\u548c\u8bed\u97f3\u8c03\u5236\u7684\u63cf\u8ff0\u80fd\u529b\uff0c\u4ece\u800c\u66f4\u597d\u5730\u7406\u89e3\u548c\u5206\u6790\u8bed\u97f3\u98ce\u683c\u76f8\u5173\u7684\u4efb\u52a1\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5d4c\u5165\u7279\u5f81\uff0c\u5728\u5305\u542b11,184\u4e2a\u6837\u672c\u7684\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e86\u4e03\u4e2a\u8bed\u97f3\u7ef4\u5ea6\u7684\u63a2\u9488\uff0c\u5e76\u5728\u591a\u4e2a\u672a\u89c1\u8fc7\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u96f6\u6837\u672c\u6027\u80fd\u9a8c\u8bc1\u3002", "result": "\u6a21\u578b\u5728\u516c\u5171\u6570\u636e\u96c6\u548c\u672a\u89c1\u8fc7\u7684\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u96f6\u6837\u672c\u6027\u80fd\uff0c\u540c\u65f6\u7ed3\u679c\u7684\u53ef\u89e3\u91ca\u6027\u9ad8\u3002", "conclusion": "\u8bed\u97f3\u8d28\u91cf\u7ef4\u5ea6\u5728\u8bed\u97f3\u98ce\u683c\u76f8\u5173\u4efb\u52a1\u4e2d\u5177\u6709\u5b9e\u7528\u6027\uff0c\u6a21\u578b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "keywords": "\u8bed\u97f3\u8d28\u91cf, \u975e\u5178\u578b\u8bed\u97f3, \u8bed\u97f3\u8c03\u5236, \u96f6\u6837\u672c\u5b66\u4e60, \u9884\u8bad\u7ec3\u6a21\u578b"}}
{"id": "2505.21837", "pdf": "https://arxiv.org/pdf/2505.21837", "abs": "https://arxiv.org/abs/2505.21837", "authors": ["Aliasghar Khani", "Arianna Rampini", "Evan Atherton", "Bruno Roy"], "title": "UniMoGen: Universal Motion Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Motion generation is a cornerstone of computer graphics, animation, gaming,\nand robotics, enabling the creation of realistic and varied character\nmovements. A significant limitation of existing methods is their reliance on\nspecific skeletal structures, which restricts their versatility across\ndifferent characters. To overcome this, we introduce UniMoGen, a novel\nUNet-based diffusion model designed for skeleton-agnostic motion generation.\nUniMoGen can be trained on motion data from diverse characters, such as humans\nand animals, without the need for a predefined maximum number of joints. By\ndynamically processing only the necessary joints for each character, our model\nachieves both skeleton agnosticism and computational efficiency. Key features\nof UniMoGen include controllability via style and trajectory inputs, and the\nability to continue motions from past frames. We demonstrate UniMoGen's\neffectiveness on the 100style dataset, where it outperforms state-of-the-art\nmethods in diverse character motion generation. Furthermore, when trained on\nboth the 100style and LAFAN1 datasets, which use different skeletons, UniMoGen\nachieves high performance and improved efficiency across both skeletons. These\nresults highlight UniMoGen's potential to advance motion generation by\nproviding a flexible, efficient, and controllable solution for a wide range of\ncharacter animations.", "AI": {"tldr": "UniMoGen\u662f\u4e00\u79cd\u57fa\u4e8eUNet\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u9aa8\u67b6\u65e0\u5173\u7684\u8fd0\u52a8\u751f\u6210\uff0c\u80fd\u591f\u5904\u7406\u4e0d\u540c\u89d2\u8272\u7684\u8fd0\u52a8\u6570\u636e\uff0c\u5177\u6709\u9ad8\u6548\u7387\u548c\u53ef\u63a7\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7279\u5b9a\u9aa8\u67b6\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u8de8\u89d2\u8272\u7684\u901a\u7528\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9aa8\u67b6\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528UNet\u67b6\u6784\u7684\u6269\u6563\u6a21\u578b\uff0c\u52a8\u6001\u5904\u7406\u6240\u9700\u7684\u5173\u8282\uff0c\u652f\u6301\u98ce\u683c\u548c\u8f68\u8ff9\u8f93\u5165\u63a7\u5236\u3002", "result": "\u5728100style\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u8de8\u9aa8\u67b6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u9ad8\u6548\u3002", "conclusion": "UniMoGen\u4e3a\u89d2\u8272\u52a8\u753b\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u9ad8\u6548\u4e14\u53ef\u63a7\u7684\u8fd0\u52a8\u751f\u6210\u65b9\u6848\u3002", "keywords": "Motion generation, diffusion model, skeleton-agnostic, UNet, controllability"}}
{"id": "2505.21842", "pdf": "https://arxiv.org/pdf/2505.21842", "abs": "https://arxiv.org/abs/2505.21842", "authors": ["Filippos Fotiadis", "Kyriakos G. Vamvoudakis"], "title": "A Physics-Informed Learning Framework to Solve the Infinite-Horizon Optimal Control Problem", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": "Accepted with minor revisions at International Journal of Robust and\n  Nonlinear Control", "summary": "We propose a physics-informed neural networks (PINNs) framework to solve the\ninfinite-horizon optimal control problem of nonlinear systems. In particular,\nsince PINNs are generally able to solve a class of partial differential\nequations (PDEs), they can be employed to learn the value function of the\ninfinite-horizon optimal control problem via solving the associated\nsteady-state Hamilton-Jacobi-Bellman (HJB) equation. However, an issue here is\nthat the steady-state HJB equation generally yields multiple solutions; hence\nif PINNs are directly employed to it, they may end up approximating a solution\nthat is different from the optimal value function of the problem. We tackle\nthis by instead applying PINNs to a finite-horizon variant of the steady-state\nHJB that has a unique solution, and which uniformly approximates the optimal\nvalue function as the horizon increases. An algorithm to verify if the chosen\nhorizon is large enough is also given, as well as a method to extend it -- with\nreduced computations and robustness to approximation errors -- in case it is\nnot. Unlike many existing methods, the proposed technique works well with\nnon-polynomial basis functions, does not require prior knowledge of a\nstabilizing controller, and does not perform iterative policy evaluations.\nSimulations are performed, which verify and clarify theoretical findings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u6846\u67b6\u6765\u89e3\u51b3\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u65e0\u9650\u65f6\u57df\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u89e3\u51b3\u7a33\u6001Hamilton-Jacobi-Bellman\uff08HJB\uff09\u65b9\u7a0b\u6765\u5b66\u4e60\u4ef7\u503c\u51fd\u6570\u3002\u4e3a\u4e86\u907f\u514d\u591a\u89e3\u95ee\u9898\uff0c\u8bba\u6587\u91c7\u7528\u6709\u9650\u65f6\u57df\u7684HJB\u65b9\u7a0b\u53d8\u4f53\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u7b97\u6cd5\u9a8c\u8bc1\u65f6\u57df\u957f\u5ea6\u662f\u5426\u8db3\u591f\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u65e0\u9650\u65f6\u57df\u6700\u4f18\u63a7\u5236\u95ee\u9898\u65f6\u53ef\u80fd\u56e0HJB\u65b9\u7a0b\u591a\u89e3\u800c\u5931\u6548\uff0c\u800cPINNs\u80fd\u591f\u901a\u8fc7\u5b66\u4e60PDE\u7684\u6570\u503c\u89e3\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002\u7814\u7a76\u7684\u52a8\u673a\u662f\u6539\u8fdb\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5982\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u6216\u8fed\u4ee3\u7b56\u7565\u8bc4\u4f30\u3002", "method": "\u5229\u7528PINNs\u6c42\u89e3\u6709\u9650\u65f6\u57df\u7684\u7a33\u6001HJB\u65b9\u7a0b\u53d8\u4f53\uff0c\u63d0\u51fa\u65f6\u57df\u957f\u5ea6\u9a8c\u8bc1\u4e0e\u6269\u5c55\u7b97\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u89e3\u7684\u552f\u4e00\u6027\u548c\u8fd1\u4f3c\u7cbe\u5ea6\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u975e\u591a\u9879\u5f0f\u57fa\u51fd\u6570\u4e0b\u7684\u9002\u7528\u6027\uff0c\u540c\u65f6\u907f\u514d\u4e86\u5bf9\u7a33\u5b9a\u63a7\u5236\u5668\u7684\u4f9d\u8d56\u548c\u8fed\u4ee3\u7b56\u7565\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u5904\u7406\u65e0\u9650\u65f6\u57df\u6700\u4f18\u63a7\u5236\u95ee\u9898\u65f6\u5177\u6709\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u975e\u7ebf\u6027\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u6c42\u89e3\u601d\u8def\u3002", "keywords": "\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc, \u6700\u4f18\u63a7\u5236, HJB\u65b9\u7a0b, \u65e0\u9650\u65f6\u57df, \u975e\u7ebf\u6027\u7cfb\u7edf"}}
{"id": "2505.21845", "pdf": "https://arxiv.org/pdf/2505.21845", "abs": "https://arxiv.org/abs/2505.21845", "authors": ["Lingfei Zhao", "Hadeel Soliman", "Kevin S. Xu", "Subhadeep Paul"], "title": "Spectral clustering for dependent community Hawkes process models of temporal networks", "categories": ["stat.ML", "cs.LG", "cs.SI", "stat.ME"], "comment": null, "summary": "Temporal networks observed continuously over time through timestamped\nrelational events data are commonly encountered in application settings\nincluding online social media communications, financial transactions, and\ninternational relations. Temporal networks often exhibit community structure\nand strong dependence patterns among node pairs. This dependence can be modeled\nthrough mutual excitations, where an interaction event from a sender to a\nreceiver node increases the possibility of future events among other node\npairs.\n  We provide statistical results for a class of models that we call dependent\ncommunity Hawkes (DCH) models, which combine the stochastic block model with\nmutually exciting Hawkes processes for modeling both community structure and\ndependence among node pairs, respectively. We derive a non-asymptotic upper\nbound on the misclustering error of spectral clustering on the event count\nmatrix as a function of the number of nodes and communities, time duration, and\nthe amount of dependence in the model. Our result leverages recent results on\nbounding an appropriate distance between a multivariate Hawkes process count\nvector and a Gaussian vector, along with results from random matrix theory. We\nalso propose a DCH model that incorporates only self and reciprocal excitation\nalong with highly scalable parameter estimation using a Generalized Method of\nMoments (GMM) estimator that we demonstrate to be consistent for growing\nnetwork size and time duration.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4f9d\u8d56\u793e\u533a\u970d\u514b\u65af\uff08DCH\uff09\u6a21\u578b\uff0c\u7ed3\u5408\u968f\u673a\u5757\u6a21\u578b\u548c\u970d\u514b\u65af\u8fc7\u7a0b\uff0c\u7528\u4e8e\u5efa\u6a21\u65f6\u95f4\u7f51\u7edc\u7684\u793e\u533a\u7ed3\u6784\u548c\u8282\u70b9\u5bf9\u4e4b\u95f4\u7684\u4f9d\u8d56\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u8c31\u805a\u7c7b\u5728\u4e8b\u4ef6\u8ba1\u6570\u77e9\u9635\u4e0a\u7684\u9519\u8bef\u7387\u4e0a\u9650\u53ca\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\u3002", "motivation": "\u65f6\u95f4\u7f51\u7edc\u4e2d\u5e38\u5b58\u5728\u793e\u533a\u7ed3\u6784\u548c\u8282\u70b9\u5bf9\u4e4b\u95f4\u7684\u5f3a\u4f9d\u8d56\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5efa\u6a21\u8fd9\u4e24\u79cd\u7279\u6027\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faDCH\u6a21\u578b\uff0c\u7ed3\u5408\u968f\u673a\u5757\u6a21\u578b\u548c\u970d\u514b\u65af\u8fc7\u7a0b\uff1b\u91c7\u7528\u8c31\u805a\u7c7b\u5206\u6790\u4e8b\u4ef6\u8ba1\u6570\u77e9\u9635\uff0c\u5e76\u63a8\u5bfc\u9519\u8bef\u7387\u4e0a\u9650\uff1b\u63d0\u51fa\u57fa\u4e8e\u5e7f\u4e49\u77e9\u4f30\u8ba1\uff08GMM\uff09\u7684\u53ef\u6269\u5c55\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\u3002", "result": "\u63a8\u5bfc\u4e86\u8c31\u805a\u7c7b\u7684\u975e\u6e10\u8fd1\u9519\u8bef\u7387\u4e0a\u9650\uff0c\u8bc1\u660e\u4e86GMM\u53c2\u6570\u4f30\u8ba1\u7684\u6e10\u8fd1\u4e00\u81f4\u6027\u3002", "conclusion": "DCH\u6a21\u578b\u80fd\u6709\u6548\u5efa\u6a21\u65f6\u95f4\u7f51\u7edc\u7684\u793e\u533a\u7ed3\u6784\u548c\u4f9d\u8d56\u6027\uff0c\u4e14GMM\u4f30\u8ba1\u65b9\u6cd5\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u7406\u8bba\u4fdd\u8bc1\u3002", "keywords": "\u65f6\u95f4\u7f51\u7edc\u3001\u970d\u514b\u65af\u8fc7\u7a0b\u3001\u793e\u533a\u7ed3\u6784\u3001\u8c31\u805a\u7c7b\u3001\u5e7f\u4e49\u77e9\u4f30\u8ba1"}}
{"id": "2505.22343", "pdf": "https://arxiv.org/pdf/2505.22343", "abs": "https://arxiv.org/abs/2505.22343", "authors": ["Zhonghao Lyu", "Yulan Gao", "Junting Chen", "Hongyang Du", "Jie Xu", "Kaibin Huang", "Dong In Kim"], "title": "Empowering Intelligent Low-altitude Economy with Large AI Model Deployment", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "Low-altitude economy (LAE) represents an emerging economic paradigm that\nredefines commercial and social aerial activities. Large artificial\nintelligence models (LAIMs) offer transformative potential to further enhance\nthe intelligence of LAE services. However, deploying LAIMs in LAE poses several\nchallenges, including the significant gap between their computational/storage\ndemands and the limited onboard resources of LAE entities, the mismatch between\nlab-trained LAIMs and dynamic physical environments, and the inefficiencies of\ntraditional decoupled designs for sensing, communication, and computation. To\naddress these issues, we first propose a hierarchical system architecture\ntailored for LAIM deployment and present representative LAE application\nscenarios. Next, we explore key enabling techniques that facilitate the mutual\nco-evolution of LAIMs and low-altitude systems, and introduce a task-oriented\nexecution pipeline for scalable and adaptive service delivery. Then, the\nproposed framework is validated through real-world case studies. Finally, we\noutline open challenges to inspire future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u4eba\u5de5\u667a\u80fd\u5927\u6a21\u578b\u5728\u4f4e\u7a7a\u7ecf\u6d4e\u4e2d\u7684\u5e94\u7528\u6311\u6218\u4e0e\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u51fa\u4e86\u5206\u5c42\u7cfb\u7edf\u67b6\u6784\u3001\u5173\u952e\u6280\u672f\u53ca\u4efb\u52a1\u5bfc\u5411\u6267\u884c\u6d41\u7a0b\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u9a8c\u8bc1\uff0c\u6700\u540e\u6307\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4f4e\u7a7a\u7ecf\u6d4e\uff08LAE\uff09\u4f5c\u4e3a\u4e00\u79cd\u65b0\u5174\u7ecf\u6d4e\u6a21\u5f0f\uff0c\u9762\u4e34\u4eba\u5de5\u667a\u80fd\u5927\u6a21\u578b\uff08LAIMs\uff09\u90e8\u7f72\u7684\u6311\u6218\uff0c\u5305\u62ec\u8ba1\u7b97\u5b58\u50a8\u9700\u6c42\u4e0e\u8d44\u6e90\u9650\u5236\u3001\u5b9e\u9a8c\u5ba4\u8bad\u7ec3\u4e0e\u52a8\u6001\u73af\u5883\u4e0d\u5339\u914d\u53ca\u4f20\u7edf\u8bbe\u8ba1\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u7cfb\u7edf\u67b6\u6784\u548c\u4efb\u52a1\u5bfc\u5411\u6267\u884c\u6d41\u7a0b\uff0c\u63a2\u7d22\u4fc3\u8fdbLAIMs\u4e0e\u4f4e\u7a7a\u7cfb\u7edf\u5171\u540c\u6f14\u8fdb\u7684\u5173\u952e\u6280\u672f\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u6848\u4f8b\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u771f\u5b9e\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u670d\u52a1\u4ea4\u4ed8\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8bba\u6587\u89e3\u51b3\u4e86LAIMs\u5728LAE\u4e2d\u7684\u90e8\u7f72\u96be\u9898\uff0c\u4f46\u672a\u6765\u4ecd\u9700\u5e94\u5bf9\u5f00\u653e\u6311\u6218\u3002", "keywords": "\u4f4e\u7a7a\u7ecf\u6d4e, \u4eba\u5de5\u667a\u80fd\u5927\u6a21\u578b, \u5206\u5c42\u67b6\u6784, \u4efb\u52a1\u5bfc\u5411, \u5171\u540c\u6f14\u8fdb"}}
{"id": "2505.22349", "pdf": "https://arxiv.org/pdf/2505.22349", "abs": "https://arxiv.org/abs/2505.22349", "authors": ["Anjie Xu", "Ruiqing Ding", "Leye Wang"], "title": "ChatPD: An LLM-driven Paper-Dataset Networking System", "categories": ["cs.DB", "cs.AI", "cs.IR"], "comment": "Accepted by KDD Applied Data Science Track 2025", "summary": "Scientific research heavily depends on suitable datasets for method\nvalidation, but existing academic platforms with dataset management like\nPapersWithCode suffer from inefficiencies in their manual workflow. To overcome\nthis bottleneck, we present a system, called ChatPD, that utilizes Large\nLanguage Models (LLMs) to automate dataset information extraction from academic\npapers and construct a structured paper-dataset network. Our system consists of\nthree key modules: \\textit{paper collection}, \\textit{dataset information\nextraction}, and \\textit{dataset entity resolution} to construct paper-dataset\nnetworks. Specifically, we propose a \\textit{Graph Completion and Inference}\nstrategy to map dataset descriptions to their corresponding entities. Through\nextensive experiments, we demonstrate that ChatPD not only outperforms the\nexisting platform PapersWithCode in dataset usage extraction but also achieves\nabout 90\\% precision and recall in entity resolution tasks. Moreover, we have\ndeployed ChatPD to continuously extract which datasets are used in papers, and\nprovide a dataset discovery service, such as task-specific dataset queries and\nsimilar dataset recommendations. We open source ChatPD and the current\npaper-dataset network on this [GitHub\nrepository]{https://github.com/ChatPD-web/ChatPD}.", "AI": {"tldr": "ChatPD\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u4ece\u8bba\u6587\u4e2d\u63d0\u53d6\u6570\u636e\u96c6\u4fe1\u606f\u5e76\u6784\u5efa\u7ed3\u6784\u5316\u7f51\u7edc\uff0c\u8d85\u8d8a\u73b0\u6709\u5e73\u53f0\u4e14\u5728\u5b9e\u4f53\u89e3\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u5b66\u672f\u5e73\u53f0\uff08\u5982PapersWithCode\uff09\u7684\u6570\u636e\u96c6\u7ba1\u7406\u4f9d\u8d56\u4eba\u5de5\u6d41\u7a0b\uff0c\u6548\u7387\u4f4e\u4e0b\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7cfb\u7edf\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u8bba\u6587\u6536\u96c6\u3001\u6570\u636e\u96c6\u4fe1\u606f\u63d0\u53d6\u3001\u6570\u636e\u96c6\u5b9e\u4f53\u89e3\u6790\uff1b\u91c7\u7528\u56fe\u8865\u5168\u4e0e\u63a8\u7406\u7b56\u7565\u6620\u5c04\u6570\u636e\u96c6\u63cf\u8ff0\u4e0e\u5b9e\u4f53\u3002", "result": "\u5728\u6570\u636e\u96c6\u63d0\u53d6\u4efb\u52a1\u4e0a\u8d85\u8d8aPapersWithCode\uff0c\u5b9e\u4f53\u89e3\u6790\u4efb\u52a1\u4e2d\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u8fbe90%\uff0c\u5e76\u90e8\u7f72\u4e86\u6570\u636e\u96c6\u53d1\u73b0\u670d\u52a1\u3002", "conclusion": "ChatPD\u901a\u8fc7\u81ea\u52a8\u5316\u663e\u8457\u63d0\u5347\u6570\u636e\u96c6\u7ba1\u7406\u6548\u7387\uff0c\u5f00\u6e90\u4e86\u7cfb\u7edf\u4e0e\u8bba\u6587-\u6570\u636e\u96c6\u7f51\u7edc\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u6570\u636e\u96c6\u7ba1\u7406, \u5b66\u672f\u5e73\u53f0, \u81ea\u52a8\u5316\u63d0\u53d6, \u5b9e\u4f53\u89e3\u6790"}}
{"id": "2505.21872", "pdf": "https://arxiv.org/pdf/2505.21872", "abs": "https://arxiv.org/abs/2505.21872", "authors": ["George R. Nahass", "Zhu Wang", "Homa Rashidisabet", "Won Hwa Kim", "Sasha Hubschman", "Jeffrey C. Peterson", "Ghasem Yazdanpanah", "Chad A. Purnell", "Pete Setabutr", "Ann Q. Tran", "Darvin Yi", "Sathya N. Ravi"], "title": "Targeted Unlearning Using Perturbed Sign Gradient Methods With Applications On Medical Images", "categories": ["eess.IV", "cs.LG"], "comment": "39 pages, 12 figures, 11 tables, 3 algorithms", "summary": "Machine unlearning aims to remove the influence of specific training samples\nfrom a trained model without full retraining. While prior work has largely\nfocused on privacy-motivated settings, we recast unlearning as a\ngeneral-purpose tool for post-deployment model revision. Specifically, we focus\non utilizing unlearning in clinical contexts where data shifts, device\ndeprecation, and policy changes are common. To this end, we propose a bilevel\noptimization formulation of boundary-based unlearning that can be solved using\niterative algorithms. We provide convergence guarantees when first-order\nalgorithms are used to unlearn. Our method introduces tunable loss design for\ncontrolling the forgetting-retention tradeoff and supports novel model\ncomposition strategies that merge the strengths of distinct unlearning runs.\nAcross benchmark and real-world clinical imaging datasets, our approach\noutperforms baselines on both forgetting and retention metrics, including\nscenarios involving imaging devices and anatomical outliers. This work\nestablishes machine unlearning as a modular, practical alternative to\nretraining for real-world model maintenance in clinical applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4e34\u5e8a\u573a\u666f\u7684\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u548c\u8fb9\u754c\u9057\u5fd8\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8bad\u7ec3\u6837\u672c\u5f71\u54cd\u7684\u7cbe\u786e\u79fb\u9664\uff0c\u907f\u514d\u4e86\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u7684\u9700\u6c42\u3002", "motivation": "\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\uff0c\u6570\u636e\u6f02\u79fb\u3001\u8bbe\u5907\u6dd8\u6c70\u548c\u653f\u7b56\u53d8\u5316\u9891\u7e41\uff0c\u9700\u8981\u9ad8\u6548\u8c03\u6574\u6a21\u578b\u3002\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u591a\u5173\u6ce8\u9690\u79c1\u4fdd\u62a4\uff0c\u672c\u5de5\u4f5c\u5c06\u5176\u63a8\u5e7f\u4e3a\u901a\u7528\u7684\u6a21\u578b\u66f4\u65b0\u5de5\u5177\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u516c\u5f0f\u548c\u8fb9\u754c\u9057\u5fd8\u673a\u5236\uff0c\u5229\u7528\u4e00\u9636\u8fed\u4ee3\u7b97\u6cd5\u5b9e\u73b0\u9057\u5fd8\u8fc7\u7a0b\uff0c\u652f\u6301\u53ef\u63a7\u7684\u9057\u5fd8-\u4fdd\u7559\u6743\u8861\u548c\u6a21\u578b\u7ec4\u5408\u7b56\u7565\u3002", "result": "\u5728\u57fa\u51c6\u548c\u771f\u5b9e\u4e34\u5e8a\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u9057\u5fd8\u548c\u4fdd\u7559\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728\u8bbe\u5907\u6216\u89e3\u5256\u5f02\u5e38\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u673a\u5668\u9057\u5fd8\u53ef\u4f5c\u4e3a\u4e34\u5e8a\u6a21\u578b\u7ef4\u62a4\u7684\u6a21\u5757\u5316\u5b9e\u7528\u65b9\u6848\uff0c\u66ff\u4ee3\u4f20\u7edf\u91cd\u65b0\u8bad\u7ec3\u3002", "keywords": "\u673a\u5668\u9057\u5fd8,\u4e34\u5e8a\u5f71\u50cf,\u53cc\u5c42\u4f18\u5316,\u8fb9\u754c\u9057\u5fd8,\u6a21\u578b\u7ef4\u62a4"}}
{"id": "2505.22353", "pdf": "https://arxiv.org/pdf/2505.22353", "abs": "https://arxiv.org/abs/2505.22353", "authors": ["Noora Al-Emadi", "Ingmar Weber", "Yin Yang", "Ferda Ofli"], "title": "VME: A Satellite Imagery Dataset and Benchmark for Detecting Vehicles in the Middle East and Beyond", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Detecting vehicles in satellite images is crucial for traffic management,\nurban planning, and disaster response. However, current models struggle with\nreal-world diversity, particularly across different regions. This challenge is\namplified by geographic bias in existing datasets, which often focus on\nspecific areas and overlook regions like the Middle East. To address this gap,\nwe present the Vehicles in the Middle East (VME) dataset, designed explicitly\nfor vehicle detection in high-resolution satellite images from Middle Eastern\ncountries. Sourced from Maxar, the VME dataset spans 54 cities across 12\ncountries, comprising over 4,000 image tiles and more than 100,000 vehicles,\nannotated using both manual and semi-automated methods. Additionally, we\nintroduce the largest benchmark dataset for Car Detection in Satellite Imagery\n(CDSI), combining images from multiple sources to enhance global car detection.\nOur experiments demonstrate that models trained on existing datasets perform\npoorly on Middle Eastern images, while the VME dataset significantly improves\ndetection accuracy in this region. Moreover, state-of-the-art models trained on\nCDSI achieve substantial improvements in global car detection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faVME\u6570\u636e\u96c6\u7528\u4e8e\u4e2d\u4e1c\u5730\u533a\u536b\u661f\u56fe\u50cf\u8f66\u8f86\u68c0\u6d4b\uff0c\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u7684\u5730\u7406\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u5176\u5728\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u6027\u4e0a\u7684\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u5728\u4e2d\u4e1c\u5730\u533a\u8f66\u8f86\u68c0\u6d4b\u7684\u5730\u7406\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u5347\u5168\u7403\u8f66\u8f86\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u6784\u5efaVME\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e2d\u4e1c12\u56fd54\u4e2a\u57ce\u5e02\u7684\u536b\u661f\u56fe\u50cf\uff0c\u5e76\u5f00\u53d1CDSI\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "VME\u663e\u8457\u63d0\u5347\u4e2d\u4e1c\u8f66\u8f86\u68c0\u6d4b\u51c6\u786e\u6027\uff0cCDSI\u63d0\u5347\u5168\u7403\u8f66\u8f86\u68c0\u6d4b\u6548\u679c\u3002", "conclusion": "VME\u548cCDSI\u6570\u636e\u96c6\u586b\u8865\u5730\u7406\u504f\u5dee\uff0c\u663e\u8457\u63d0\u5347\u8f66\u8f86\u68c0\u6d4b\u6a21\u578b\u6027\u80fd\u3002", "keywords": "\u8f66\u8f86\u68c0\u6d4b,\u536b\u661f\u56fe\u50cf,\u6570\u636e\u96c6,VME,CDSI"}}
{"id": "2505.21892", "pdf": "https://arxiv.org/pdf/2505.21892", "abs": "https://arxiv.org/abs/2505.21892", "authors": ["Xunpeng Huang", "Yingyu Lin", "Nikki Lijing Kuang", "Hanze Dong", "Difan Zou", "Yian Ma", "Tong Zhang"], "title": "Almost Linear Convergence under Minimal Score Assumptions: Quantized Transition Diffusion", "categories": ["stat.ML", "cs.LG"], "comment": "37 pages, 3 figures, 3 tables", "summary": "Continuous diffusion models have demonstrated remarkable performance in data\ngeneration across various domains, yet their efficiency remains constrained by\ntwo critical limitations: (1) the local adjacency structure of the forward\nMarkov process, which restricts long-range transitions in the data space, and\n(2) inherent biases introduced during the simulation of time-inhomogeneous\nreverse denoising processes. To address these challenges, we propose Quantized\nTransition Diffusion (QTD), a novel approach that integrates data quantization\nwith discrete diffusion dynamics. Our method first transforms the continuous\ndata distribution $p_*$ into a discrete one $q_*$ via histogram approximation\nand binary encoding, enabling efficient representation in a structured discrete\nlatent space. We then design a continuous-time Markov chain (CTMC) with Hamming\ndistance-based transitions as the forward process, which inherently supports\nlong-range movements in the original data space. For reverse-time sampling, we\nintroduce a \\textit{truncated uniformization} technique to simulate the reverse\nCTMC, which can provably provide unbiased generation from $q_*$ under minimal\nscore assumptions. Through a novel KL dynamic analysis of the reverse CTMC, we\nprove that QTD can generate samples with $O(d\\ln^2(d/\\epsilon))$ score\nevaluations in expectation to approximate the $d$--dimensional target\ndistribution $p_*$ within an $\\epsilon$ error tolerance. Our method not only\nestablishes state-of-the-art inference efficiency but also advances the\ntheoretical foundations of diffusion-based generative modeling by unifying\ndiscrete and continuous diffusion paradigms.", "AI": {"tldr": "QTD\u901a\u8fc7\u91cf\u5316\u548c\u4e8c\u8fdb\u5236\u7f16\u7801\u5c06\u8fde\u7eed\u6570\u636e\u8f6c\u5316\u4e3a\u79bb\u6563\u8868\u793a\uff0c\u5229\u7528\u57fa\u4e8eHamming\u8ddd\u79bb\u7684CTMC\u524d\u5411\u8fc7\u7a0b\u652f\u6301\u957f\u8ddd\u79bb\u6570\u636e\u8f6c\u79fb\uff0c\u5e76\u91c7\u7528\u622a\u65ad\u5747\u5300\u5316\u6280\u672f\u53cd\u5411\u91c7\u6837\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u6548\u7387\u548c\u7406\u8bba\u57fa\u7840\u3002", "motivation": "\u89e3\u51b3\u8fde\u7eed\u6269\u6563\u6a21\u578b\u4e2d\u524d\u5411Markov\u8fc7\u7a0b\u7684\u5c40\u90e8\u90bb\u63a5\u7ed3\u6784\u9650\u5236\u548c\u53cd\u5411\u53bb\u504f\u8fc7\u7a0b\u7684\u65f6\u95f4\u975e\u5747\u5300\u6027\u504f\u5dee\u95ee\u9898\u3002", "method": "\u91cf\u5316\u6570\u636e\u5e76\u4e8c\u8fdb\u5236\u7f16\u7801\u79bb\u6563\u5316\uff0c\u8bbe\u8ba1\u57fa\u4e8eHamming\u8ddd\u79bb\u7684CTMC\u524d\u5411\u8fc7\u7a0b\uff0c\u4f7f\u7528\u622a\u65ad\u5747\u5300\u5316\u6280\u672f\u53cd\u5411\u91c7\u6837\u3002", "result": "QTD\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u5728$O(d\\\\ln^2(d/\\\\epsilon))$\u6b21\u8bc4\u5206\u8bc4\u4f30\u5185\u903c\u8fd1\u76ee\u6807\u5206\u5e03\u3002", "conclusion": "QTD\u4e0d\u4ec5\u63d0\u5347\u4e86\u63a8\u65ad\u6548\u7387\uff0c\u8fd8\u901a\u8fc7\u7edf\u4e00\u79bb\u6563\u548c\u8fde\u7eed\u6269\u6563\u8303\u5f0f\u63a8\u52a8\u4e86\u7406\u8bba\u57fa\u7840\u3002", "keywords": "Quantized Transition Diffusion, diffusion models, Markov process, unbiased generation, efficiency"}}
{"id": "2505.21925", "pdf": "https://arxiv.org/pdf/2505.21925", "abs": "https://arxiv.org/abs/2505.21925", "authors": ["Chong Zeng", "Yue Dong", "Pieter Peers", "Hongzhi Wu", "Xin Tong"], "title": "RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted to SIGGRAPH 2025. Project page:\n  https://microsoft.github.io/renderformer", "summary": "We present RenderFormer, a neural rendering pipeline that directly renders an\nimage from a triangle-based representation of a scene with full global\nillumination effects and that does not require per-scene training or\nfine-tuning. Instead of taking a physics-centric approach to rendering, we\nformulate rendering as a sequence-to-sequence transformation where a sequence\nof tokens representing triangles with reflectance properties is converted to a\nsequence of output tokens representing small patches of pixels. RenderFormer\nfollows a two stage pipeline: a view-independent stage that models\ntriangle-to-triangle light transport, and a view-dependent stage that\ntransforms a token representing a bundle of rays to the corresponding pixel\nvalues guided by the triangle-sequence from the view-independent stage. Both\nstages are based on the transformer architecture and are learned with minimal\nprior constraints. We demonstrate and evaluate RenderFormer on scenes with\nvarying complexity in shape and light transport.", "AI": {"tldr": "RenderFormer\u662f\u4e00\u4e2a\u76f4\u63a5\u4ece\u573a\u666f\u7684\u4e09\u89d2\u5f62\u8868\u793a\u6e32\u67d3\u56fe\u50cf\u7684\u795e\u7ecf\u6e32\u67d3\u7ba1\u9053\uff0c\u65e0\u9700\u6bcf\u4e2a\u573a\u666f\u8bad\u7ec3\u6216\u5fae\u8c03\uff0c\u5b9e\u73b0\u4e86\u5168\u5c40\u5149\u7167\u6548\u679c\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u5e8f\u5217\u5230\u5e8f\u5217\u7684\u8f6c\u6362\u65b9\u6cd5\uff0c\u907f\u514d\u4f20\u7edf\u7269\u7406\u4e2d\u5fc3\u6e32\u67d3\u65b9\u6cd5\u7684\u9700\u6c42\uff0c\u7b80\u5316\u6e32\u67d3\u6d41\u7a0b\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u7ba1\u9053\uff1a\u89c6\u56fe\u65e0\u5173\u9636\u6bb5\u5efa\u6a21\u4e09\u89d2\u5f62\u95f4\u5149\u4f20\u8f93\uff0c\u89c6\u56fe\u4f9d\u8d56\u9636\u6bb5\u5c06\u5149\u7ebf\u675f\u8f6c\u6362\u4e3a\u50cf\u7d20\u503c\uff0c\u4e24\u8005\u5747\u57fa\u4e8e\u65e0\u5f3a\u5148\u9a8c\u7ea6\u675f\u7684Transformer\u67b6\u6784\u3002", "result": "\u5728\u5f62\u72b6\u548c\u5149\u4f20\u8f93\u590d\u6742\u5ea6\u4e0d\u540c\u7684\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\u548c\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "RenderFormer\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u7269\u7406\u5efa\u6a21\u5373\u53ef\u5b9e\u73b0\u5168\u5c40\u5149\u7167\u6548\u679c\u7684\u6e32\u67d3\u65b9\u6cd5\uff0c\u7b80\u5316\u4e86\u6e32\u67d3\u6d41\u7a0b\u3002", "keywords": "\u795e\u7ecf\u6e32\u67d3\u3001\u5168\u5c40\u5149\u7167\u3001Transformer\u3001\u5e8f\u5217\u5230\u5e8f\u5217\u3001\u5149\u4f20\u8f93"}}
{"id": "2505.22384", "pdf": "https://arxiv.org/pdf/2505.22384", "abs": "https://arxiv.org/abs/2505.22384", "authors": ["Foivos Fioravantes", "Harmender Gahlawat", "Nikolaos Melissinos"], "title": "Exact Algorithms and Lower Bounds for Forming Coalitions of Constrained Maximum Size", "categories": ["cs.DS", "cs.AI"], "comment": "a preliminary version appeared in AAAI 2025", "summary": "Imagine we want to split a group of agents into teams in the most\n\\emph{efficient} way, considering that each agent has their own preferences\nabout their teammates. This scenario is modeled by the extensively studied\n\\textsc{Coalition Formation} problem. Here, we study a version of this problem\nwhere each team must additionally be of bounded size.\n  We conduct a systematic algorithmic study, providing several intractability\nresults as well as multiple exact algorithms that scale well as the input grows\n(FPT), which could prove useful in practice.\n  Our main contribution is an algorithm that deals efficiently with tree-like\nstructures (bounded \\emph{treewidth}) for ``small'' teams. We complement this\nresult by proving that our algorithm is asymptotically optimal. Particularly,\nthere can be no algorithm that vastly outperforms the one we present, under\nreasonable theoretical assumptions, even when considering star-like structures\n(bounded \\emph{vertex cover number}).", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u6709\u56e2\u961f\u89c4\u6a21\u9650\u5236\u4e0b\u7684\u8054\u5408\u5f62\u6210\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u9488\u5bf9\u6811\u72b6\u7ed3\u6784\u7684\u9ad8\u6548\u7b97\u6cd5\u5e76\u8bc1\u660e\u5176\u6e10\u8fdb\u6700\u4f18\u6027\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u56e2\u961f\u89c4\u6a21\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u9ad8\u6548\u5730\u6839\u636e\u4ee3\u7406\u504f\u597d\u5f62\u6210\u56e2\u961f\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u5316\u7684\u7b97\u6cd5\u7814\u7a76\uff0c\u63d0\u51fa\u591a\u79cd\u7cbe\u786e\u7b97\u6cd5\uff08FPT\uff09\u5e76\u5206\u6790\u4e86\u6811\u72b6\u7ed3\u6784\u7684\u9ad8\u6548\u7b97\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u6811\u72b6\u7ed3\u6784\u4e0b\u9ad8\u6548\uff0c\u4e14\u88ab\u8bc1\u660e\u4e3a\u6e10\u8fdb\u6700\u4f18\uff0c\u65e0\u6cd5\u5728\u7406\u8bba\u5047\u8bbe\u4e0b\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u6709\u9650\u56e2\u961f\u89c4\u6a21\u4e0b\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u5c24\u5176\u5728\u6811\u72b6\u7ed3\u6784\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "keywords": "\u8054\u5408\u5f62\u6210,\u56e2\u961f\u89c4\u6a21\u9650\u5236,\u6811\u72b6\u7ed3\u6784,FPT\u7b97\u6cd5,\u6e10\u8fdb\u6700\u4f18\u6027"}}
{"id": "2505.22387", "pdf": "https://arxiv.org/pdf/2505.22387", "abs": "https://arxiv.org/abs/2505.22387", "authors": ["Jaehyun Choi", "Gyojin Han", "Dong-Jae Lee", "Sunghyun Baek", "Junmo Kim"], "title": "DAM: Domain-Aware Module for Multi-Domain Dataset Condensation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Dataset Condensation (DC) has emerged as a promising solution to mitigate the\ncomputational and storage burdens associated with training deep learning\nmodels. However, existing DC methods largely overlook the multi-domain nature\nof modern datasets, which are increasingly composed of heterogeneous images\nspanning multiple domains. In this paper, we extend DC and introduce\nMulti-Domain Dataset Condensation (MDDC), which aims to condense data that\ngeneralizes across both single-domain and multi-domain settings. To this end,\nwe propose the Domain-Aware Module (DAM), a training-time module that embeds\ndomain-related features into each synthetic image via learnable spatial masks.\nAs explicit domain labels are mostly unavailable in real-world datasets, we\nemploy frequency-based pseudo-domain labeling, which leverages low-frequency\namplitude statistics. DAM is only active during the condensation process, thus\npreserving the same images per class (IPC) with prior methods. Experiments show\nthat DAM consistently improves in-domain, out-of-domain, and cross-architecture\nperformance over baseline dataset condensation methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u591a\u57df\u6570\u636e\u96c6\u538b\u7f29(MDDC)\u65b9\u6cd5\uff0c\u901a\u8fc7\u9886\u57df\u611f\u77e5\u6a21\u5757(DAM)\u5d4c\u5165\u8de8\u57df\u7279\u5f81\uff0c\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u538b\u7f29\u65b9\u6cd5\u672a\u8003\u8651\u591a\u57df\u6570\u636e\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u6570\u636e\u96c6\u901a\u5e38\u5305\u542b\u591a\u57df\u5f02\u8d28\u6570\u636e\uff0c\u4f20\u7edf\u6570\u636e\u96c6\u538b\u7f29\u65b9\u6cd5\u672a\u8003\u8651\u8fd9\u4e00\u7279\u6027\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u548c\u6548\u679c\u3002", "method": "\u5f15\u5165\u9886\u57df\u611f\u77e5\u6a21\u5757(DAM)\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7a7a\u95f4\u63a9\u7801\u5d4c\u5165\u57df\u7279\u5f81\uff0c\u91c7\u7528\u57fa\u4e8e\u9891\u7387\u7684\u4f2a\u57df\u6807\u7b7e\u6280\u672f\u5904\u7406\u65e0\u663e\u5f0f\u57df\u6807\u7b7e\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDAM\u5728\u57df\u5185\u3001\u57df\u5916\u53ca\u8de8\u67b6\u6784\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u57fa\u7ebf\u6570\u636e\u96c6\u538b\u7f29\u65b9\u6cd5\u3002", "conclusion": "MDDC\u548cDAM\u6709\u6548\u63d0\u5347\u4e86\u6570\u636e\u96c6\u538b\u7f29\u5728\u591a\u57df\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u4e0d\u5f71\u54cd\u5355\u57df\u6027\u80fd\u3002", "keywords": "\u591a\u57df\u6570\u636e\u96c6\u538b\u7f29, \u9886\u57df\u611f\u77e5\u6a21\u5757, \u9891\u7387\u4f2a\u57df\u6807\u7b7e, \u6df1\u5ea6\u5b66\u4e60"}}
{"id": "2505.21932", "pdf": "https://arxiv.org/pdf/2505.21932", "abs": "https://arxiv.org/abs/2505.21932", "authors": ["Adriana L. Duncan", "Joe Kileel"], "title": "Higher-Order Group Synchronization", "categories": ["stat.ML", "cs.CV", "cs.LG", "math.CO", "math.OC"], "comment": "40 pages", "summary": "Group synchronization is the problem of determining reliable global estimates\nfrom noisy local measurements on networks. The typical task for group\nsynchronization is to assign elements of a group to the nodes of a graph in a\nway that respects group elements given on the edges which encode information\nabout local pairwise relationships between the nodes. In this paper, we\nintroduce a novel higher-order group synchronization problem which operates on\na hypergraph and seeks to synchronize higher-order local measurements on the\nhyperedges to obtain global estimates on the nodes. Higher-order group\nsynchronization is motivated by applications to computer vision and image\nprocessing, among other computational problems. First, we define the problem of\nhigher-order group synchronization and discuss its mathematical foundations.\nSpecifically, we give necessary and sufficient synchronizability conditions\nwhich establish the importance of cycle consistency in higher-order group\nsynchronization. Then, we propose the first computational framework for general\nhigher-order group synchronization; it acts globally and directly on\nhigher-order measurements using a message passing algorithm. We discuss\ntheoretical guarantees for our framework, including convergence analyses under\noutliers and noise. Finally, we show potential advantages of our method through\nnumerical experiments. In particular, we show that in certain cases our\nhigher-order method applied to rotational and angular synchronization\noutperforms standard pairwise synchronization methods and is more robust to\noutliers. We also show that our method has comparable performance on simulated\ncryo-electron microscopy (cryo-EM) data compared to a standard cryo-EM\nreconstruction package.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9ad8\u9636\u7fa4\u540c\u6b65\u95ee\u9898\uff0c\u901a\u8fc7\u8d85\u56fe\u5904\u7406\u9ad8\u9636\u5c40\u90e8\u6d4b\u91cf\u4ee5\u83b7\u53d6\u5168\u5c40\u4f30\u8ba1\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u7814\u7a76\u9ad8\u9636\u7fa4\u540c\u6b65\u95ee\u9898\u7684\u52a8\u673a\u6e90\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u56fe\u50cf\u5904\u7406\u7b49\u9886\u57df\u7684\u5e94\u7528\u9700\u6c42\uff0c\u65e8\u5728\u4ece\u9ad8\u9636\u5c40\u90e8\u6d4b\u91cf\u4e2d\u83b7\u53d6\u66f4\u53ef\u9760\u7684\u5168\u5c40\u4f30\u8ba1\u3002", "method": "\u8bba\u6587\u9996\u5148\u5b9a\u4e49\u4e86\u9ad8\u9636\u7fa4\u540c\u6b65\u95ee\u9898\u5e76\u63a2\u8ba8\u5176\u6570\u5b66\u57fa\u7840\uff0c\u968f\u540e\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d88\u606f\u4f20\u9012\u7b97\u6cd5\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u53ef\u76f4\u63a5\u5904\u7406\u9ad8\u9636\u6d4b\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u65cb\u8f6c\u548c\u89d2\u5ea6\u540c\u6b65\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edf\u7684\u6210\u5bf9\u540c\u6b65\u65b9\u6cd5\uff0c\u4e14\u5bf9\u5f02\u5e38\u503c\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u5728\u6a21\u62df\u7684\u51b7\u51bb\u7535\u955c\u6570\u636e\u4e0a\u8868\u73b0\u4e0e\u6807\u51c6\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u9ad8\u9636\u7fa4\u540c\u6b65\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u6709\u6570\u5b66\u4fdd\u8bc1\uff0c\u5b9e\u9a8c\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u7fa4\u540c\u6b65, \u9ad8\u9636\u6d4b\u91cf, \u8d85\u56fe, \u6d88\u606f\u4f20\u9012, \u51b7\u51bb\u7535\u955c"}}
{"id": "2505.21994", "pdf": "https://arxiv.org/pdf/2505.21994", "abs": "https://arxiv.org/abs/2505.21994", "authors": ["Josef Dick", "Seungchan Ko", "Kassem Mustapha", "Sanghyeon Park"], "title": "Locking-Free Training of Physics-Informed Neural Network for Solving Nearly Incompressible Elasticity Equations", "categories": ["math.NA", "cs.LG", "cs.NA"], "comment": null, "summary": "Due to divergence instability, the accuracy of low-order conforming finite\nelement methods for nearly incompressible homogeneous elasticity equations\ndeteriorates as the Lam\\'e coefficient $\\lambda\\to\\infty$, or equivalently as\nthe Poisson ratio $\\nu\\to1/2$. This phenomenon, known as locking or\nnon-robustness, remains not fully understood despite extensive investigation.\nIn this paper, we propose a robust method based on a fundamentally different,\nmachine-learning-driven approach. Leveraging recently developed\nPhysics-Informed Neural Networks (PINNs), we address the numerical solution of\nlinear elasticity equations governing nearly incompressible materials. The core\nidea of our method is to appropriately decompose the given equations to\nalleviate the extreme imbalance in the coefficients, while simultaneously\nsolving both the forward and inverse problems to recover the solutions of the\ndecomposed systems as well as the associated external conditions. Through\nvarious numerical experiments, including constant, variable and parametric\nLam\\'e coefficients, we illustrate the efficiency of the proposed methodology.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u7684\u9c81\u68d2\u65b9\u6cd5\uff0c\u89e3\u51b3\u8fd1\u4e0d\u53ef\u538b\u7f29\u6750\u6599\u7684\u7ebf\u6027\u5f39\u6027\u65b9\u7a0b\u6570\u503c\u89e3\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u89e3\u65b9\u7a0b\u51cf\u8f7b\u7cfb\u6570\u4e0d\u5e73\u8861\uff0c\u5e76\u5728\u6570\u503c\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u4f4e\u9636\u6709\u9650\u5143\u65b9\u6cd5\u5728Lam\u00e9\u7cfb\u6570\u03bb\u2192\u221e\u6216\u6cca\u677e\u6bd4\u03bd\u21921/2\u65f6\u7cbe\u5ea6\u4e0b\u964d\uff08\u9501\u5b9a\u73b0\u8c61\uff09\uff0c\u5c3d\u7ba1\u5df2\u6709\u5927\u91cf\u7814\u7a76\uff0c\u4f46\u95ee\u9898\u5c1a\u672a\u5b8c\u5168\u89e3\u51b3\uff0c\u672c\u6587\u5e0c\u671b\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6539\u8fdb\u3002", "method": "\u5229\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\uff0c\u5206\u89e3\u7ebf\u6027\u5f39\u6027\u65b9\u7a0b\u4ee5\u7f13\u89e3\u7cfb\u6570\u4e0d\u5e73\u8861\uff0c\u540c\u65f6\u6c42\u89e3\u6b63\u5411\u548c\u9006\u5411\u95ee\u9898\u4ee5\u6062\u590d\u5206\u89e3\u7cfb\u7edf\u548c\u5916\u90e8\u6761\u4ef6\u3002", "result": "\u901a\u8fc7\u5e38\u6570\u3001\u53d8\u91cf\u548c\u53c2\u6570\u5316Lam\u00e9\u7cfb\u6570\u7684\u6570\u503c\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u9a71\u52a8\u7684PINN\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u8fd1\u4e0d\u53ef\u538b\u7f29\u6750\u6599\u7684\u7ebf\u6027\u5f39\u6027\u95ee\u9898\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u6709\u9650\u5143\u7684\u9501\u5b9a\u73b0\u8c61\u3002", "keywords": "\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u3001\u8fd1\u4e0d\u53ef\u538b\u7f29\u6750\u6599\u3001\u7ebf\u6027\u5f39\u6027\u65b9\u7a0b\u3001\u9501\u5b9a\u73b0\u8c61\u3001\u5206\u89e3\u65b9\u6cd5"}}
{"id": "2505.22008", "pdf": "https://arxiv.org/pdf/2505.22008", "abs": "https://arxiv.org/abs/2505.22008", "authors": ["Jing-An Sun", "Hang Fan", "Junchao Gong", "Ben Fei", "Kun Chen", "Fenghua Ling", "Wenlong Zhang", "Wanghan Xu", "Li Yan", "Pierre Gentine", "Lei Bai"], "title": "Align-DA: Align Score-based Atmospheric Data Assimilation with Multiple Preferences", "categories": ["physics.ao-ph", "cs.LG"], "comment": null, "summary": "Data assimilation (DA) aims to estimate the full state of a dynamical system\nby combining partial and noisy observations with a prior model forecast,\ncommonly referred to as the background. In atmospheric applications, this\nproblem is fundamentally ill-posed due to the sparsity of observations relative\nto the high-dimensional state space. Traditional methods address this challenge\nby simplifying background priors to regularize the solution, which are\nempirical and require continual tuning for application. Inspired by alignment\ntechniques in text-to-image diffusion models, we propose Align-DA, which\nformulates DA as a generative process and uses reward signals to guide\nbackground priors, replacing manual tuning with data-driven alignment.\nSpecifically, we train a score-based model in the latent space to approximate\nthe background-conditioned prior, and align it using three complementary reward\nsignals for DA: (1) assimilation accuracy, (2) forecast skill initialized from\nthe assimilated state, and (3) physical adherence of the analysis fields.\nExperiments with multiple reward signals demonstrate consistent improvements in\nanalysis quality across different evaluation metrics and observation-guidance\nstrategies. These results show that preference alignment, implemented as a soft\nconstraint, can automatically adapt complex background priors tailored to DA,\noffering a promising new direction for advancing the field.", "AI": {"tldr": "Align-DA\u662f\u4e00\u79cd\u6570\u636e\u540c\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u751f\u6210\u8fc7\u7a0b\u5c06\u5956\u52b1\u4fe1\u53f7\u5f15\u5bfc\u80cc\u666f\u5148\u9a8c\uff0c\u81ea\u52a8\u8c03\u6574\u590d\u6742\u5148\u9a8c\uff0c\u63d0\u9ad8\u5206\u6790\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u540c\u5316\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7ecf\u9a8c\u6027\u7b80\u5316\u80cc\u666f\u5148\u9a8c\uff0c\u9700\u624b\u52a8\u8c03\u6574\u3002Align-DA\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u5bf9\u9f50\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "Align-DA\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8bad\u7ec3\u57fa\u4e8e\u8bc4\u5206\u7684\u6a21\u578b\uff0c\u5229\u7528\u4e09\u79cd\u5956\u52b1\u4fe1\u53f7\uff08\u540c\u5316\u7cbe\u5ea6\u3001\u9884\u6d4b\u6280\u80fd\u3001\u7269\u7406\u4e00\u81f4\u6027\uff09\u5f15\u5bfc\u80cc\u666f\u5148\u9a8c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cAlign-DA\u5728\u4e0d\u540c\u8bc4\u4ef7\u6307\u6807\u548c\u89c2\u6d4b\u7b56\u7565\u4e0b\u5747\u80fd\u63d0\u5347\u5206\u6790\u8d28\u91cf\u3002", "conclusion": "\u504f\u597d\u5bf9\u9f50\u4f5c\u4e3a\u8f6f\u7ea6\u675f\uff0c\u53ef\u81ea\u52a8\u9002\u914d\u590d\u6742\u80cc\u666f\u5148\u9a8c\uff0c\u4e3a\u6570\u636e\u540c\u5316\u9886\u57df\u63d0\u4f9b\u65b0\u65b9\u5411\u3002", "keywords": "\u6570\u636e\u540c\u5316,\u751f\u6210\u8fc7\u7a0b,\u5956\u52b1\u4fe1\u53f7,\u80cc\u666f\u5148\u9a8c,\u504f\u597d\u5bf9\u9f50"}}
{"id": "2505.22438", "pdf": "https://arxiv.org/pdf/2505.22438", "abs": "https://arxiv.org/abs/2505.22438", "authors": ["Zijian Liang", "Kai Niu", "Changshuo Wang", "Jin Xu", "Ping Zhang"], "title": "Synonymous Variational Inference for Perceptual Image Compression", "categories": ["cs.IT", "cs.AI", "cs.CV", "cs.LG", "eess.IV", "math.IT"], "comment": "31 pages, 20 figures. This paper is accepted by Proceedings of the\n  42nd International Conference on Machine Learning (ICML 2025) Poster", "summary": "Recent contributions of semantic information theory reveal the set-element\nrelationship between semantic and syntactic information, represented as\nsynonymous relationships. In this paper, we propose a synonymous variational\ninference (SVI) method based on this synonymity viewpoint to re-analyze the\nperceptual image compression problem. It takes perceptual similarity as a\ntypical synonymous criterion to build an ideal synonymous set (Synset), and\napproximate the posterior of its latent synonymous representation with a\nparametric density by minimizing a partial semantic KL divergence. This\nanalysis theoretically proves that the optimization direction of perception\nimage compression follows a triple tradeoff that can cover the existing\nrate-distortion-perception schemes. Additionally, we introduce synonymous image\ncompression (SIC), a new image compression scheme that corresponds to the\nanalytical process of SVI, and implement a progressive SIC codec to fully\nleverage the model's capabilities. Experimental results demonstrate comparable\nrate-distortion-perception performance using a single progressive SIC codec,\nthus verifying the effectiveness of our proposed analysis method.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u540c\u4e49\u5173\u7cfb\u7684\u540c\u4e49\u53d8\u5206\u63a8\u7406\uff08SVI\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u611f\u77e5\u56fe\u50cf\u538b\u7f29\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u540c\u4e49\u56fe\u50cf\u538b\u7f29\uff08SIC\uff09\u65b9\u6848\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u8bed\u4e49\u4fe1\u606f\u7406\u8bba\u4e2d\u8bed\u4e49\u548c\u53e5\u6cd5\u4fe1\u606f\u4e4b\u95f4\u7684\u540c\u4e49\u5173\u7cfb\uff0c\u4ee5\u6b64\u4e3a\u57fa\u7840\u91cd\u65b0\u5206\u6790\u611f\u77e5\u56fe\u50cf\u538b\u7f29\u95ee\u9898\u3002", "method": "\u91c7\u7528\u540c\u4e49\u53d8\u5206\u63a8\u7406\uff08SVI\uff09\u65b9\u6cd5\uff0c\u6784\u5efa\u7406\u60f3\u540c\u4e49\u96c6\uff08Synset\uff09\uff0c\u5e76\u901a\u8fc7\u6700\u5c0f\u5316\u90e8\u5206\u8bed\u4e49KL\u6563\u5ea6\u8fd1\u4f3c\u5176\u540e\u9a8c\u8868\u793a\u3002\u8fdb\u4e00\u6b65\u63d0\u51fa\u540c\u4e49\u56fe\u50cf\u538b\u7f29\uff08SIC\uff09\u65b9\u6848\u5e76\u5b9e\u73b0\u6e10\u8fdb\u5f0f\u7f16\u89e3\u7801\u5668\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u611f\u77e5\u56fe\u50cf\u538b\u7f29\u7684\u4f18\u5316\u65b9\u5411\u9075\u5faa\u4e09\u91cd\u6743\u8861\uff0c\u5b9e\u9a8c\u663e\u793a\u6e10\u8fdb\u5f0fSIC\u7f16\u89e3\u7801\u5668\u5728\u7387\u5931\u771f\u611f\u77e5\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "SVI\u65b9\u6cd5\u548cSIC\u65b9\u6848\u6709\u6548\u89e3\u51b3\u4e86\u611f\u77e5\u56fe\u50cf\u538b\u7f29\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u7406\u8bba\u5206\u6790\u7684\u6b63\u786e\u6027\u3002", "keywords": "\u540c\u4e49\u53d8\u5206\u63a8\u7406,\u611f\u77e5\u56fe\u50cf\u538b\u7f29,\u540c\u4e49\u56fe\u50cf\u538b\u7f29,\u7387\u5931\u771f\u611f\u77e5"}}
{"id": "2505.22048", "pdf": "https://arxiv.org/pdf/2505.22048", "abs": "https://arxiv.org/abs/2505.22048", "authors": ["Haihan Zhang", "Weicheng Lin", "Yuanshi Liu", "Cong Fang"], "title": "Learning Curves of Stochastic Gradient Descent in Kernel Regression", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "This paper considers a canonical problem in kernel regression: how good are\nthe model performances when it is trained by the popular online first-order\nalgorithms, compared to the offline ones, such as ridge and ridgeless\nregression? In this paper, we analyze the foundational single-pass Stochastic\nGradient Descent (SGD) in kernel regression under source condition where the\noptimal predictor can even not belong to the RKHS, i.e. the model is\nmisspecified. Specifically, we focus on the inner product kernel over the\nsphere and characterize the exact orders of the excess risk curves under\ndifferent scales of sample sizes $n$ concerning the input dimension $d$.\nSurprisingly, we show that SGD achieves min-max optimal rates up to constants\namong all the scales, without suffering the saturation, a prevalent phenomenon\nobserved in (ridge) regression, except when the model is highly misspecified\nand the learning is in a final stage where $n\\gg d^{\\gamma}$ with any constant\n$\\gamma >0$. The main reason for SGD to overcome the curse of saturation is the\nexponentially decaying step size schedule, a common practice in deep neural\nnetwork training. As a byproduct, we provide the \\emph{first} provable\nadvantage of the scheme over the iterative averaging method in the common\nsetting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u7ebf\u4e00\u9636\u7b97\u6cd5\uff08\u5982SGD\uff09\u5728\u6838\u56de\u5f52\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5e76\u4e0e\u79bb\u7ebf\u65b9\u6cd5\uff08\u5982\u5cad\u56de\u5f52\u548c\u96f6\u5cad\u56de\u5f52\uff09\u8fdb\u884c\u6bd4\u8f83\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u6307\u6570\u8870\u51cf\u6b65\u957f\u8c03\u5ea6\uff0cSGD\u5728\u591a\u5c3a\u5ea6\u6837\u672c\u60c5\u51b5\u4e0b\u5747\u5b9e\u73b0\u6781\u5c0f\u6781\u5927\u6700\u4f18\u901f\u7387\uff0c\u514b\u670d\u4e86\u9971\u548c\u73b0\u8c61\u3002", "motivation": "\u7814\u7a76\u5728\u7ebf\u7b97\u6cd5\uff08\u5982SGD\uff09\u5728\u6838\u56de\u5f52\u4e2d\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u6a21\u578b\u8bef\u8bbe\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u4f20\u7edf\u79bb\u7ebf\u65b9\u6cd5\u5bf9\u6bd4\u3002", "method": "\u4f7f\u7528\u5355\u6b21\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\u5206\u6790\u5185\u79ef\u6838\u5728\u7403\u9762\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u7814\u7a76\u4e0d\u540c\u6837\u672c\u89c4\u6a21\u4e0b\u7684\u8d85\u989d\u98ce\u9669\u3002", "result": "SGD\u5728\u591a\u5c3a\u5ea6\u6837\u672c\u60c5\u51b5\u4e0b\u5747\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\uff0c\u9664\u9ad8\u5ea6\u8bef\u8bbe\u6a21\u578b\u7684\u540e\u671f\u5b66\u4e60\u9636\u6bb5\u3002", "conclusion": "SGD\u901a\u8fc7\u6307\u6570\u8870\u51cf\u6b65\u957f\u8c03\u5ea6\u514b\u670d\u4e86\u9971\u548c\u73b0\u8c61\uff0c\u5728\u591a\u5c3a\u5ea6\u6837\u672c\u89c4\u6a21\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "keywords": "\u6838\u56de\u5f52\uff0cSGD\uff0c\u5cad\u56de\u5f52\uff0c\u8bef\u8bbe\u6a21\u578b\uff0c\u8d85\u989d\u98ce\u9669"}}
{"id": "2505.22441", "pdf": "https://arxiv.org/pdf/2505.22441", "abs": "https://arxiv.org/abs/2505.22441", "authors": ["Chaitanya Amballa", "Sattwik Basu", "Yu-Lin Wei", "Zhijian Yang", "Mehmet Ergezer", "Romit Roy Choudhury"], "title": "Can NeRFs See without Cameras?", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Neural Radiance Fields (NeRFs) have been remarkably successful at\nsynthesizing novel views of 3D scenes by optimizing a volumetric scene\nfunction. This scene function models how optical rays bring color information\nfrom a 3D object to the camera pixels. Radio frequency (RF) or audio signals\ncan also be viewed as a vehicle for delivering information about the\nenvironment to a sensor. However, unlike camera pixels, an RF/audio sensor\nreceives a mixture of signals that contain many environmental reflections (also\ncalled \"multipath\"). Is it still possible to infer the environment using such\nmultipath signals? We show that with redesign, NeRFs can be taught to learn\nfrom multipath signals, and thereby \"see\" the environment. As a grounding\napplication, we aim to infer the indoor floorplan of a home from sparse WiFi\nmeasurements made at multiple locations inside the home. Although a difficult\ninverse problem, our implicitly learnt floorplans look promising, and enables\nforward applications, such as indoor signal prediction and basic ray tracing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684NeRF\u65b9\u6cd5\uff0c\u80fd\u591f\u4ece\u591a\u8def\u5f84\u4fe1\u53f7\uff08\u5982WiFi\uff09\u4e2d\u5b66\u4e60\u73af\u5883\u4fe1\u606f\uff0c\u5e76\u7528\u4e8e\u63a8\u65ad\u5ba4\u5185\u5e73\u9762\u56fe\u3002", "motivation": "\u4f20\u7edfNeRF\u901a\u8fc7\u5149\u5b66\u5c04\u7ebf\u4f20\u9012\u7684\u989c\u8272\u4fe1\u606f\u5efa\u6a213D\u573a\u666f\uff0c\u800cRF\u6216\u97f3\u9891\u4fe1\u53f7\u7684\u591a\u8def\u5f84\u7279\u6027\u4f7f\u5176\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u3002\u8bba\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5229\u7528\u591a\u8def\u5f84\u4fe1\u53f7\u63a8\u65ad\u73af\u5883\u4fe1\u606f\u3002", "method": "\u901a\u8fc7\u91cd\u65b0\u8bbe\u8ba1NeRF\uff0c\u4f7f\u5176\u80fd\u591f\u4ece\u591a\u8def\u5f84\u4fe1\u53f7\uff08\u5982WiFi\u6d4b\u91cf\u6570\u636e\uff09\u4e2d\u5b66\u4e60\uff0c\u5e76\u9690\u5f0f\u63a8\u65ad\u73af\u5883\uff08\u5982\u5ba4\u5185\u5e73\u9762\u56fe\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6539\u8fdb\u7684NeRF\u80fd\u591f\u4ece\u7a00\u758fWiFi\u6d4b\u91cf\u6570\u636e\u4e2d\u5b66\u4e60\u5230\u5408\u7406\u7684\u5ba4\u5185\u5e73\u9762\u56fe\uff0c\u5e76\u652f\u6301\u4fe1\u53f7\u9884\u6d4b\u548c\u57fa\u672c\u5149\u7ebf\u8ffd\u8e2a\u7b49\u5e94\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86NeRF\u5728\u591a\u8def\u5f84\u4fe1\u53f7\u73af\u5883\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u65e0\u7ebf\u4fe1\u53f7\u548c\u73af\u5883\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "NeRF\u3001\u591a\u8def\u5f84\u4fe1\u53f7\u3001WiFi\u3001\u5ba4\u5185\u5e73\u9762\u56fe\u3001\u4fe1\u53f7\u9884\u6d4b"}}
{"id": "2505.22083", "pdf": "https://arxiv.org/pdf/2505.22083", "abs": "https://arxiv.org/abs/2505.22083", "authors": ["H. L. Dao"], "title": "Hyperbolic recurrent neural network as the first type of non-Euclidean neural quantum state ansatz", "categories": ["quant-ph", "cond-mat.dis-nn", "cs.LG", "physics.comp-ph"], "comment": null, "summary": "In this work, we introduce the first type of non-Euclidean neural quantum\nstate (NQS) ansatz, in the form of the hyperbolic GRU (a variant of recurrent\nneural networks (RNNs)), to be used in the Variational Monte Carlo method of\napproximating the ground state wavefunction for quantum many-body systems. In\nparticular, we examine the performances of NQS ansatzes constructed from both\nconventional or Euclidean RNN/GRU and from hyperbolic GRU in the prototypical\nsettings of the one- and two-dimensional transverse field Ising models (TFIM)\nof up to 100 spins and the one-dimensional Heisenberg $J_1J_2$ and $J_1J_2J_3$\nsystems of up 50 spins. By virtue of the fact that, for all of the experiments\nperformed in this work, hyperbolic GRU can yield performances comparable to or\nbetter than Euclidean RNNs, which have been extensively studied in these\nsettings in the literature, our work is a proof-of-concept for the viability of\nhyperbolic GRU as the first type of non-Euclidean NQS ansatz for quantum\nmany-body systems. Furthermore, in settings where the Hamiltonian displays a\nclear hierarchical interaction structure, such as the 1D Heisenberg $J_1J_2$ &\n$J_1J_2J_3$ systems with the 1st, 2nd and even 3rd nearest neighbor\ninteractions, our results show that hyperbolic GRU definitively outperforms its\nEuclidean version in all instances. The fact that these results are reminiscent\nof the established ones from natural language processing where hyperbolic GRU\nalmost always outperforms Euclidean RNNs when the training data exhibit a\ntree-like or hierarchical structure leads us to hypothesize that hyperbolic GRU\nNQS ansatz would likely outperform Euclidean RNN/GRU NQS ansatz in quantum spin\nsystems that involve different degrees of nearest neighbor interactions.\nFinally, with this work, we hope to initiate future studies of other types of\nnon-Euclidean NQS beyond hyperbolic GRU.", "AI": {"tldr": "\u9996\u6b21\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u66f2GRU\u7684\u975e\u6b27\u51e0\u91cc\u5f97\u795e\u7ecf\u91cf\u5b50\u6001\uff08NQS\uff09\u65b9\u6848\uff0c\u5728\u53d8\u5206\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u4e2d\u7528\u4e8e\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u7684\u57fa\u6001\u6ce2\u51fd\u6570\u8fd1\u4f3c\u3002\u53cc\u66f2GRU\u5728\u591a\u79cd\u91cf\u5b50\u81ea\u65cb\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u7b49\u6548\u4e8e\u6b27\u51e0\u91cc\u5f97RNN/GRU\uff0c\u5c24\u5176\u5728\u5177\u6709\u5c42\u7ea7\u7ed3\u6784\u7684\u54c8\u5bc6\u987f\u91cf\u4e2d\u8868\u73b0\u66f4\u4f73\u3002", "motivation": "\u63a2\u7d22\u975e\u6b27\u51e0\u91cc\u5f97\u795e\u7ecf\u91cf\u5b50\u6001\uff08NQS\uff09\u5728\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u5c42\u7ea7\u76f8\u4e92\u4f5c\u7528\u7684\u7cfb\u7edf\u4e2d\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u53cc\u66f2GRU\u7684NQS\u65b9\u6848\uff0c\u5e76\u4e0e\u4f20\u7edf\u6b27\u51e0\u91cc\u5f97RNN/GRU\u8fdb\u884c\u6027\u80fd\u5bf9\u6bd4\uff0c\u6d4b\u8bd5\u6a21\u578b\u5305\u62ecTFIM\u548c\u4e00\u7ef4Heisenberg\u7cfb\u7edf\u3002", "result": "\u53cc\u66f2GRU\u5728\u6240\u6709\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u7b49\u6548\u4e8e\u6b27\u51e0\u91cc\u5f97RNN/GRU\uff0c\u5728\u5177\u6709\u5c42\u7ea7\u7ed3\u6784\u7684\u54c8\u5bc6\u987f\u91cf\u4e2d\u663e\u8457\u9886\u5148\u3002", "conclusion": "\u53cc\u66f2GRU\u662f\u9996\u4e2a\u6210\u529f\u5e94\u7528\u4e8e\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u7684\u975e\u6b27\u51e0\u91cc\u5f97NQS\u65b9\u6848\uff0c\u6709\u671b\u5728\u672a\u6765\u7814\u7a76\u4e2d\u8fdb\u4e00\u6b65\u6269\u5c55\u3002", "keywords": "\u53cc\u66f2GRU, \u795e\u7ecf\u91cf\u5b50\u6001, \u53d8\u5206\u8499\u7279\u5361\u6d1b, \u91cf\u5b50\u591a\u4f53\u7cfb\u7edf, \u975e\u6b27\u51e0\u91cc\u5f97"}}
{"id": "2505.22445", "pdf": "https://arxiv.org/pdf/2505.22445", "abs": "https://arxiv.org/abs/2505.22445", "authors": ["Puhua Jiang", "Zhangquan Chen", "Mingze Sun", "Ruqi Huang"], "title": "NFR: Neural Feature-Guided Non-Rigid Shape Registration", "categories": ["cs.CV", "cs.AI", "I.4.m; I.2.6"], "comment": "20 pages, 9 figures. arXiv admin note: substantial text overlap with\n  arXiv:2311.04494", "summary": "In this paper, we propose a novel learning-based framework for 3D shape\nregistration, which overcomes the challenges of significant non-rigid\ndeformation and partiality undergoing among input shapes, and, remarkably,\nrequires no correspondence annotation during training. Our key insight is to\nincorporate neural features learned by deep learning-based shape matching\nnetworks into an iterative, geometric shape registration pipeline. The\nadvantage of our approach is two-fold -- On one hand, neural features provide\nmore accurate and semantically meaningful correspondence estimation than\nspatial features (e.g., coordinates), which is critical in the presence of\nlarge non-rigid deformations; On the other hand, the correspondences are\ndynamically updated according to the intermediate registrations and filtered by\nconsistency prior, which prominently robustify the overall pipeline. Empirical\nresults show that, with as few as dozens of training shapes of limited\nvariability, our pipeline achieves state-of-the-art results on several\nbenchmarks of non-rigid point cloud matching and partial shape matching across\nvarying settings, but also delivers high-quality correspondences between unseen\nchallenging shape pairs that undergo both significant extrinsic and intrinsic\ndeformations, in which case neither traditional registration methods nor\nintrinsic methods work.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u76843D\u5f62\u72b6\u914d\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u7279\u5f81\u89e3\u51b3\u975e\u521a\u6027\u53d8\u5f62\u548c\u90e8\u5206\u5339\u914d\u95ee\u9898\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u5373\u53ef\u8bad\u7ec3\uff0c\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u975e\u521a\u6027\u53d8\u5f62\u548c\u90e8\u5206\u5339\u914d\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u540c\u65f6\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u751f\u6210\u7684\u7279\u5f81\u4e0e\u51e0\u4f55\u914d\u51c6\u6d41\u7a0b\uff0c\u52a8\u6001\u66f4\u65b0\u5339\u914d\u5173\u7cfb\u5e76\u901a\u8fc7\u4e00\u81f4\u6027\u5148\u9a8c\u8fc7\u6ee4\uff0c\u63d0\u5347\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u4e0b\uff0c\u6846\u67b6\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u4f18\u8868\u73b0\uff0c\u5e76\u80fd\u5904\u7406\u672a\u89c1\u8fc7\u7684\u590d\u6742\u53d8\u5f62\u6848\u4f8b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u57283D\u5f62\u72b6\u914d\u51c6\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u975e\u521a\u6027\u548c\u90e8\u5206\u5339\u914d\u573a\u666f\u3002", "keywords": "3D shape registration, non-rigid deformation, deep learning, correspondence estimation"}}
{"id": "2505.22085", "pdf": "https://arxiv.org/pdf/2505.22085", "abs": "https://arxiv.org/abs/2505.22085", "authors": ["Arnulf Jentzen", "Julian Kranz", "Adrian Riekert"], "title": "PADAM: Parallel averaged Adam reduces the error for stochastic optimization in scientific machine learning", "categories": ["math.OC", "cs.LG", "cs.NA", "math.NA"], "comment": "38 pages, 13 figures", "summary": "Averaging techniques such as Ruppert--Polyak averaging and exponential\nmovering averaging (EMA) are powerful approaches to accelerate optimization\nprocedures of stochastic gradient descent (SGD) optimization methods such as\nthe popular ADAM optimizer. However, depending on the specific optimization\nproblem under consideration, the type and the parameters for the averaging need\nto be adjusted to achieve the smallest optimization error. In this work we\npropose an averaging approach, which we refer to as parallel averaged ADAM\n(PADAM), in which we compute parallely different averaged variants of ADAM and\nduring the training process dynamically select the variant with the smallest\noptimization error. A central feature of this approach is that this procedure\nrequires no more gradient evaluations than the usual ADAM optimizer as each of\nthe averaged trajectories relies on the same underlying ADAM trajectory and\nthus on the same underlying gradients. We test the proposed PADAM optimizer in\n13 stochastic optimization and deep neural network (DNN) learning problems and\ncompare its performance with known optimizers from the literature such as\nstandard SGD, momentum SGD, Adam with and without EMA, and ADAMW. In\nparticular, we apply the compared optimizers to physics-informed neural\nnetwork, deep Galerkin, deep backward stochastic differential equation and deep\nKolmogorov approximations for boundary value partial differential equation\nproblems from scientific machine learning, as well as to DNN approximations for\noptimal control and optimal stopping problems. In nearly all of the considered\nexamples PADAM achieves, sometimes among others and sometimes exclusively,\nessentially the smallest optimization error. This work thus strongly suggest to\nconsider PADAM for scientific machine learning problems and also motivates\nfurther research for adaptive averaging procedures within the training of DNNs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u5e76\u884c\u5e73\u5747ADAM\uff08PADAM\uff09\u7684\u65b0\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e76\u884c\u8ba1\u7b97\u4e0d\u540c\u5e73\u5747\u53d8\u4f53\u5e76\u52a8\u6001\u9009\u62e9\u4f18\u5316\u8bef\u5dee\u6700\u5c0f\u7684\u53d8\u4f53\uff0c\u663e\u8457\u63d0\u5347\u4e86ADAM\u4f18\u5316\u5668\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5e73\u5747\u65b9\u6cd5\u5982Ruppert--Polyak\u548c\u6307\u6570\u79fb\u52a8\u5e73\u5747\uff08EMA\uff09\u5728\u52a0\u901f\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\u4f18\u5316\u65f6\u9700\u8981\u8c03\u6574\u7c7b\u578b\u548c\u53c2\u6570\uff0c\u96be\u4ee5\u666e\u9002\u3002PADAM\u65e8\u5728\u52a8\u6001\u9009\u62e9\u6700\u4f73\u5e73\u5747\u53d8\u4f53\uff0c\u65e0\u9700\u989d\u5916\u68af\u5ea6\u8ba1\u7b97\u3002", "method": "PADAM\u5e76\u884c\u8ba1\u7b97\u591a\u4e2aADAM\u7684\u5e73\u5747\u53d8\u4f53\uff0c\u5e76\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u9009\u62e9\u4f18\u5316\u8bef\u5dee\u6700\u5c0f\u7684\u53d8\u4f53\uff0c\u6240\u6709\u53d8\u4f53\u5171\u4eab\u76f8\u540c\u7684ADAM\u8f68\u8ff9\u548c\u68af\u5ea6\u3002", "result": "\u572813\u4e2a\u968f\u673a\u4f18\u5316\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u5b66\u4e60\u95ee\u9898\u4e2d\uff0cPADAM\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u4f18\u5316\u8bef\u5dee\u6700\u5c0f\u3002", "conclusion": "PADAM\u5728\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u95ee\u9898\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u503c\u5f97\u63a8\u5e7f\uff0c\u5e76\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76DNN\u8bad\u7ec3\u4e2d\u7684\u81ea\u9002\u5e94\u5e73\u5747\u65b9\u6cd5\u63d0\u4f9b\u4e86\u52a8\u673a\u3002", "keywords": "PADAM, \u968f\u673a\u68af\u5ea6\u4e0b\u964d, \u5e73\u5747\u65b9\u6cd5, ADAM, \u79d1\u5b66\u673a\u5668\u5b66\u4e60"}}
{"id": "2505.22090", "pdf": "https://arxiv.org/pdf/2505.22090", "abs": "https://arxiv.org/abs/2505.22090", "authors": ["Tristan S. W. Stevens", "Ois\u00edn Nolan", "Oudom Somphone", "Jean-Luc Robert", "Ruud J. G. van Sloun"], "title": "High Volume Rate 3D Ultrasound Reconstruction with Diffusion Models", "categories": ["eess.IV", "cs.LG"], "comment": "10 pages, 10 figures, preprint", "summary": "Three-dimensional ultrasound enables real-time volumetric visualization of\nanatomical structures. Unlike traditional 2D ultrasound, 3D imaging reduces the\nreliance on precise probe orientation, potentially making ultrasound more\naccessible to clinicians with varying levels of experience and improving\nautomated measurements and post-exam analysis. However, achieving both high\nvolume rates and high image quality remains a significant challenge. While 3D\ndiverging waves can provide high volume rates, they suffer from limited tissue\nharmonic generation and increased multipath effects, which degrade image\nquality. One compromise is to retain the focusing in elevation while leveraging\nunfocused diverging waves in the lateral direction to reduce the number of\ntransmissions per elevation plane. Reaching the volume rates achieved by full\n3D diverging waves, however, requires dramatically undersampling the number of\nelevation planes. Subsequently, to render the full volume, simple interpolation\ntechniques are applied. This paper introduces a novel approach to 3D ultrasound\nreconstruction from a reduced set of elevation planes by employing diffusion\nmodels (DMs) to achieve increased spatial and temporal resolution. We compare\nboth traditional and supervised deep learning-based interpolation methods on a\n3D cardiac ultrasound dataset. Our results show that DM-based reconstruction\nconsistently outperforms the baselines in image quality and downstream task\nperformance. Additionally, we accelerate inference by leveraging the temporal\nconsistency inherent to ultrasound sequences. Finally, we explore the\nrobustness of the proposed method by exploiting the probabilistic nature of\ndiffusion posterior sampling to quantify reconstruction uncertainty and\ndemonstrate improved recall on out-of-distribution data with synthetic\nanomalies under strong subsampling.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u4ece\u5c11\u91cf\u4ef0\u89d2\u5e73\u9762\u91cd\u5efa3D\u8d85\u58f0\u56fe\u50cf\u7684\u65b0\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u65f6\u7a7a\u5206\u8fa8\u7387\uff0c\u5e76\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u548c\u6df1\u5ea6\u5b66\u4e60\u63d2\u503c\u65b9\u6cd5\u3002", "motivation": "3D\u8d85\u58f0\u80fd\u591f\u5b9e\u65f6\u53ef\u89c6\u5316\u89e3\u5256\u7ed3\u6784\uff0c\u4f46\u9ad8\u4f53\u79ef\u7387\u4e0e\u9ad8\u56fe\u50cf\u8d28\u91cf\u7684\u5e73\u8861\u4ecd\u5177\u6311\u6218\u6027\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u4ef0\u89d2\u5e73\u9762\u5927\u5e45\u6b20\u91c7\u6837\u65f6\u9700\u7b80\u5355\u63d2\u503c\uff0c\u5bfc\u81f4\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u91c7\u7528\u6269\u6563\u6a21\u578b\u4ece\u5c11\u91cf\u4ef0\u89d2\u5e73\u9762\u91cd\u5efa3D\u8d85\u58f0\u56fe\u50cf\uff0c\u5e76\u5229\u7528\u65f6\u95f4\u4e00\u81f4\u6027\u52a0\u901f\u63a8\u7406\uff0c\u540c\u65f6\u901a\u8fc7\u6269\u6563\u540e\u9a8c\u91c7\u6837\u7684\u6982\u7387\u6027\u91cf\u5316\u91cd\u5efa\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u91cd\u5efa\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u5728\u5f02\u5e38\u6570\u636e\u4e0b\u901a\u8fc7\u5f3a\u6b20\u91c7\u6837\u63d0\u5347\u4e86\u53ec\u56de\u7387\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u57283D\u8d85\u58f0\u91cd\u5efa\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u4e0d\u4ec5\u80fd\u63d0\u9ad8\u5206\u8fa8\u7387\uff0c\u8fd8\u80fd\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u589e\u5f3a\u5bf9\u5f02\u5e38\u6570\u636e\u7684\u9c81\u68d2\u6027\u3002", "keywords": "3D\u8d85\u58f0,\u6269\u6563\u6a21\u578b,\u56fe\u50cf\u91cd\u5efa,\u6b20\u91c7\u6837,\u65f6\u95f4\u4e00\u81f4\u6027"}}
{"id": "2505.22094", "pdf": "https://arxiv.org/pdf/2505.22094", "abs": "https://arxiv.org/abs/2505.22094", "authors": ["Tonghe Zhang", "Yu Chao", "Sicang Su", "Yu Wang"], "title": "ReinFlow: Fine-tuning Flow Matching Policy with Online Reinforcement Learning", "categories": ["cs.RO", "cs.LG"], "comment": "30 pages, 13 figures, 10 tables", "summary": "We propose ReinFlow, a simple yet effective online reinforcement learning\n(RL) framework that fine-tunes a family of flow matching policies for\ncontinuous robotic control. Derived from rigorous RL theory, ReinFlow injects\nlearnable noise into a flow policy's deterministic path, converting the flow\ninto a discrete-time Markov Process for exact and straightforward likelihood\ncomputation. This conversion facilitates exploration and ensures training\nstability, enabling ReinFlow to fine-tune diverse flow model variants,\nincluding Rectified Flow [35] and Shortcut Models [19], particularly at very\nfew or even one denoising step. We benchmark ReinFlow in representative\nlocomotion and manipulation tasks, including long-horizon planning with visual\ninput and sparse reward. The episode reward of Rectified Flow policies obtained\nan average net growth of 135.36% after fine-tuning in challenging legged\nlocomotion tasks while saving denoising steps and 82.63% of wall time compared\nto state-of-the-art diffusion RL fine-tuning method DPPO [43]. The success rate\nof the Shortcut Model policies in state and visual manipulation tasks achieved\nan average net increase of 40.34% after fine-tuning with ReinFlow at four or\neven one denoising step, whose performance is comparable to fine-tuned DDIM\npolicies while saving computation time for an average of 23.20%. Project\nWebpage: https://reinflow.github.io/", "AI": {"tldr": "ReinFlow\u662f\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u53ef\u5b66\u4e60\u7684\u566a\u58f0\u6ce8\u5165\u6d41\u7b56\u7565\u4e2d\uff0c\u5c06\u786e\u5b9a\u6027\u8def\u5f84\u8f6c\u6362\u4e3a\u79bb\u6563\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\uff0c\u4ece\u800c\u63d0\u5347\u63a2\u7d22\u80fd\u529b\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u8fde\u7eed\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u5956\u52b1\u5e76\u8282\u7701\u4e86\u8ba1\u7b97\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u8fde\u7eed\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u65f6\uff0c\u63a2\u7d22\u80fd\u529b\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0d\u8db3\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\u3002ReinFlow\u65e8\u5728\u901a\u8fc7\u6d41\u5339\u914d\u7b56\u7565\u7684\u4f18\u5316\uff0c\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "ReinFlow\u6846\u67b6\u901a\u8fc7\u5c06\u53ef\u5b66\u4e60\u7684\u566a\u58f0\u6ce8\u5165\u6d41\u7b56\u7565\u7684\u786e\u5b9a\u6027\u8def\u5f84\u4e2d\uff0c\u5c06\u5176\u8f6c\u6362\u4e3a\u79bb\u6563\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\uff0c\u4ece\u800c\u5b9e\u73b0\u7cbe\u786e\u7684\u4f3c\u7136\u8ba1\u7b97\u548c\u7a33\u5b9a\u7684\u8bad\u7ec3\u3002\u652f\u6301\u5728\u6781\u5c11\u6216\u5355\u6b65\u53bb\u566a\u6761\u4ef6\u4e0b\u5fae\u8c03\u591a\u79cd\u6d41\u6a21\u578b\u53d8\u4f53\u3002", "result": "\u5728\u957f\u89c6\u89c9\u8f93\u5165\u548c\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u4e2d\uff0cReinFlow\u7684\u6536\u76ca\u8868\u73b0\u663e\u8457\u63d0\u5347\uff08135.36%\u589e\u957f\uff09\uff0c\u8ba1\u7b97\u65f6\u95f4\u8282\u770182.63%\uff1b\u5728\u72b6\u6001\u548c\u89c6\u89c9\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u6210\u529f\u7387\u8fbe\u589e\u957f40.34%\uff0c\u8ba1\u7b97\u65f6\u95f4\u8282\u770123.20%\u3002", "conclusion": "ReinFlow\u9ad8\u6548\u63d0\u5347\u4e86\u6d41\u5339\u914d\u7b56\u7565\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5c11\u6b65\u53bb\u566a\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u6709\u660e\u663e\u4f18\u52bf\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60,\u6d41\u5339\u914d,\u673a\u5668\u4eba\u63a7\u5236,\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b,\u53bb\u566a"}}
{"id": "2505.22467", "pdf": "https://arxiv.org/pdf/2505.22467", "abs": "https://arxiv.org/abs/2505.22467", "authors": ["Jiaxi Yang", "Mengqi Zhang", "Yiqiao Jin", "Hao Chen", "Qingsong Wen", "Lu Lin", "Yi He", "Weijie Xu", "James Evans", "Jindong Wang"], "title": "Topological Structure Learning Should Be A Research Priority for LLM-Based Multi-Agent Systems", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Model-based Multi-Agent Systems (MASs) have emerged as a\npowerful paradigm for tackling complex tasks through collaborative\nintelligence. Nevertheless, the question of how agents should be structurally\norganized for optimal cooperation remains largely unexplored. In this position\npaper, we aim to gently redirect the focus of the MAS research community toward\nthis critical dimension: develop topology-aware MASs for specific tasks.\nSpecifically, the system consists of three core components - agents,\ncommunication links, and communication patterns - that collectively shape its\ncoordination performance and efficiency. To this end, we introduce a\nsystematic, three-stage framework: agent selection, structure profiling, and\ntopology synthesis. Each stage would trigger new research opportunities in\nareas such as language models, reinforcement learning, graph learning, and\ngenerative modeling; together, they could unleash the full potential of MASs in\ncomplicated real-world applications. Then, we discuss the potential challenges\nand opportunities in the evaluation of multiple systems. We hope our\nperspective and framework can offer critical new insights in the era of agentic\nAI.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff08MASs\uff09\u65b0\u6846\u67b6\uff0c\u5173\u6ce8\u4ee3\u7406\u7ed3\u6784\u7ec4\u7ec7\u4f18\u5316\uff0c\u5305\u62ec\u4ee3\u7406\u9009\u62e9\u3001\u7ed3\u6784\u5206\u6790\u548c\u62d3\u6251\u5408\u6210\u4e09\u9636\u6bb5\uff0c\u4ee5\u63d0\u5347\u534f\u4f5c\u6027\u80fd\u3002", "motivation": "\u76ee\u524d\u591a\u4ee3\u7406\u7cfb\u7edf\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u534f\u4f5c\u6548\u7387\u95ee\u9898\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u8bba\u6587\u65e8\u5728\u5f15\u5bfc\u7814\u7a76\u793e\u533a\u5173\u6ce8\u4ee3\u7406\u7ed3\u6784\u7ec4\u7ec7\u7684\u91cd\u8981\u6027\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u9636\u6bb5\u6846\u67b6\uff1a\u4ee3\u7406\u9009\u62e9\u3001\u7ed3\u6784\u5206\u6790\u548c\u62d3\u6251\u5408\u6210\uff0c\u6d89\u53ca\u8bed\u8a00\u6a21\u578b\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u56fe\u5b66\u4e60\u548c\u751f\u6210\u5efa\u6a21\u7b49\u6280\u672f\u3002", "result": "\u901a\u8fc7\u4f18\u5316\u4ee3\u7406\u7cfb\u7edf\u7ed3\u6784\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u534f\u4f5c\u6027\u80fd\uff0c\u5e76\u63a8\u52a8\u76f8\u5173\u6280\u672f\u5728\u590d\u6742\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u8bba\u6587\u5c55\u793a\u4e86\u62d3\u6251\u611f\u77e5\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\u5728\u63d0\u5347\u534f\u4f5c\u6548\u7387\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u591a\u4ee3\u7406\u7cfb\u7edf\u3001\u534f\u4f5c\u6548\u7387\u3001\u62d3\u6251\u7ed3\u6784\u3001\u751f\u6210\u5efa\u6a21"}}
{"id": "2505.22477", "pdf": "https://arxiv.org/pdf/2505.22477", "abs": "https://arxiv.org/abs/2505.22477", "authors": ["Qi Gao", "Wei Xu", "Hanxi Pan", "Mowei Shen", "Zaifeng Gao"], "title": "Human-Centered Human-AI Collaboration (HCHAC)", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "This article is a chapter from the upcoming book Handbook of\n  Human-Centered Artificial Intelligence", "summary": "In the intelligent era, the interaction between humans and intelligent\nsystems fundamentally involves collaboration with autonomous intelligent\nagents. Human-AI Collaboration (HAC) represents a novel type of human-machine\nrelationship facilitated by autonomous intelligent machines equipped with AI\ntechnologies. In this paradigm, AI agents serve not only as auxiliary tools but\nalso as active teammates, partnering with humans to accomplish tasks\ncollaboratively. Human-centered AI (HCAI) emphasizes that humans play critical\nleadership roles in the collaboration. This human-led collaboration imparts new\ndimensions to the human-machine relationship, necessitating innovative research\nperspectives, paradigms, and agenda to address the unique challenges posed by\nHAC. This chapter delves into the essence of HAC from the human-centered\nperspective, outlining its core concepts and distinguishing features. It\nreviews the current research methodologies and research agenda within the HAC\nfield from the HCAI perspective, highlighting advancements and ongoing studies.\nFurthermore, a framework for human-centered HAC (HCHAC) is proposed by\nintegrating these reviews and analyses. A case study of HAC in the context of\nautonomous vehicles is provided, illustrating practical applications and the\nsynergistic interactions between humans and AI agents. Finally, it identifies\npotential future research directions aimed at enhancing the effectiveness,\nreliability, and ethical integration of human-centered HAC systems in diverse\ndomains.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u4eba\u673a\u534f\u4f5c\uff08HAC\uff09\u6a21\u5f0f\uff0c\u5206\u6790\u4e86\u5176\u6838\u5fc3\u6982\u5ff5\u3001\u7814\u7a76\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\u3002", "motivation": "\u5728\u667a\u80fd\u65f6\u4ee3\uff0c\u4eba\u673a\u534f\u4f5c\u7684\u65b0\u6a21\u5f0f\u9700\u8981\u521b\u65b0\u89c6\u89d2\u548c\u7814\u7a76\u8303\u5f0f\uff0c\u4ee5\u5e94\u5bf9AI\u4f5c\u4e3a\u4e3b\u52a8\u4f19\u4f34\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u6587\u732e\u7efc\u8ff0\u548c\u5206\u6790\u65b9\u6cd5\uff0c\u63d0\u51fa\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684HAC\u6846\u67b6\uff0c\u5e76\u7ed3\u5408\u81ea\u52a8\u9a7e\u9a76\u6848\u4f8b\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u63d0\u51fa\u4e86HCHAC\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u4eba\u673a\u534f\u4f5c\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u5e76\u603b\u7ed3\u4e86\u5f53\u524d\u7814\u7a76\u7684\u8fdb\u5c55\u3002", "conclusion": "\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u4eba\u673a\u534f\u4f5c\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3001\u53ef\u9760\u6027\u548c\u4f26\u7406\u6574\u5408\u3002", "keywords": "\u4eba\u673a\u534f\u4f5c, \u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684AI, \u81ea\u4e3b\u667a\u80fd\u4f53, \u81ea\u52a8\u9a7e\u9a76, \u7814\u7a76\u6846\u67b6"}}
{"id": "2505.22099", "pdf": "https://arxiv.org/pdf/2505.22099", "abs": "https://arxiv.org/abs/2505.22099", "authors": ["Wenwen Qiang", "Ziyin Gu", "Lingyu Si", "Jiangmeng Li", "Changwen Zheng", "Fuchun Sun", "Hui Xiong"], "title": "On the Transferability and Discriminability of Repersentation Learning in Unsupervised Domain Adaptation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In this paper, we addressed the limitation of relying solely on distribution\nalignment and source-domain empirical risk minimization in Unsupervised Domain\nAdaptation (UDA). Our information-theoretic analysis showed that this standard\nadversarial-based framework neglects the discriminability of target-domain\nfeatures, leading to suboptimal performance. To bridge this\ntheoretical-practical gap, we defined \"good representation learning\" as\nguaranteeing both transferability and discriminability, and proved that an\nadditional loss term targeting target-domain discriminability is necessary.\nBuilding on these insights, we proposed a novel adversarial-based UDA framework\nthat explicitly integrates a domain alignment objective with a\ndiscriminability-enhancing constraint. Instantiated as Domain-Invariant\nRepresentation Learning with Global and Local Consistency (RLGLC), our method\nleverages Asymmetrically-Relaxed Wasserstein of Wasserstein Distance (AR-WWD)\nto address class imbalance and semantic dimension weighting, and employs a\nlocal consistency mechanism to preserve fine-grained target-domain\ndiscriminative information. Extensive experiments across multiple benchmark\ndatasets demonstrate that RLGLC consistently surpasses state-of-the-art\nmethods, confirming the value of our theoretical perspective and underscoring\nthe necessity of enforcing both transferability and discriminability in\nadversarial-based UDA.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u65b0\u9896\u7684\u65e0\u76d1\u7763\u57df\u9002\u5e94\u6846\u67b6RLGLC\uff0c\u901a\u8fc7\u7ed3\u5408\u57df\u5bf9\u9f50\u76ee\u6807\u548c\u5224\u522b\u6027\u589e\u5f3a\u7ea6\u675f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u76ee\u6807\u57df\u7279\u5f81\u5224\u522b\u6027\u7684\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u57df\u9002\u5e94\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u57df\u5bf9\u9f50\u548c\u6e90\u57df\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\uff0c\u5ffd\u89c6\u4e86\u76ee\u6807\u57df\u7279\u5f81\u7684\u5224\u522b\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u7406\u60f3\u3002\u672c\u6587\u901a\u8fc7\u4fe1\u606f\u8bba\u5206\u6790\u6307\u51fa\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u9700\u540c\u65f6\u4fdd\u8bc1\u7279\u5f81\u7684\u8fc1\u79fb\u6027\u548c\u5224\u522b\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5bf9\u6297\u5b66\u4e60\u7684\u6846\u67b6RLGLC\uff0c\u6574\u5408\u57df\u5bf9\u9f50\u76ee\u6807\u548c\u5224\u522b\u6027\u589e\u5f3a\u7ea6\u675f\u3002\u5177\u4f53\u91c7\u7528\u975e\u5bf9\u79f0\u677e\u5f1bWasserstein\u8ddd\u79bb\uff08AR-WWD\uff09\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u8bed\u4e49\u7ef4\u5ea6\u52a0\u6743\uff0c\u5e76\u901a\u8fc7\u5c40\u90e8\u4e00\u81f4\u6027\u673a\u5236\u4fdd\u7559\u76ee\u6807\u57df\u7ec6\u7c92\u5ea6\u5224\u522b\u4fe1\u606f\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRLGLC\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u540c\u65f6\u4f18\u5316\u8fc1\u79fb\u6027\u548c\u5224\u522b\u6027\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u7ed3\u679c\u5171\u540c\u8868\u660e\uff0c\u65e0\u76d1\u7763\u57df\u9002\u5e94\u4e2d\u9700\u540c\u65f6\u517c\u987e\u7279\u5f81\u7684\u8fc1\u79fb\u6027\u548c\u5224\u522b\u6027\uff0cRLGLC\u6846\u67b6\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff08UDA\uff09\u3001\u5bf9\u6297\u5b66\u4e60\u3001\u5224\u522b\u6027\u589e\u5f3a\u3001Wasserstein\u8ddd\u79bb\u3001\u5c40\u90e8\u4e00\u81f4\u6027"}}
{"id": "2505.22503", "pdf": "https://arxiv.org/pdf/2505.22503", "abs": "https://arxiv.org/abs/2505.22503", "authors": ["Yuanfei Wang", "Xinju Huang", "Fangwei Zhong", "Yaodong Yang", "Yizhou Wang", "Yuanpei Chen", "Hao Dong"], "title": "From Strangers to Assistants: Fast Desire Alignment for Embodied Agent-User Adaptation", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": null, "summary": "While embodied agents have made significant progress in performing complex\nphysical tasks, real-world applications demand more than pure task execution.\nThe agents must collaborate with unfamiliar agents and human users, whose goals\nare often vague and implicit. In such settings, interpreting ambiguous\ninstructions and uncovering underlying desires is essential for effective\nassistance. Therefore, fast and accurate desire alignment becomes a critical\ncapability for embodied agents. In this work, we first develop a home\nassistance simulation environment HA-Desire that integrates an LLM-driven human\nuser agent exhibiting realistic value-driven goal selection and communication.\nThe ego agent must interact with this proxy user to infer and adapt to the\nuser's latent desires. To achieve this, we present a novel framework FAMER for\nfast desire alignment, which introduces a desire-based mental reasoning\nmechanism to identify user intent and filter desire-irrelevant actions. We\nfurther design a reflection-based communication module that reduces redundant\ninquiries, and incorporate goal-relevant information extraction with memory\npersistence to improve information reuse and reduce unnecessary exploration.\nExtensive experiments demonstrate that our framework significantly enhances\nboth task execution and communication efficiency, enabling embodied agents to\nquickly adapt to user-specific desires in complex embodied environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aFAMER\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5feb\u901f\u5bf9\u9f50\u7528\u6237\u6f5c\u5728\u9700\u6c42\uff0c\u901a\u8fc7\u5fc3\u7406\u63a8\u7406\u673a\u5236\u548c\u53cd\u5c04\u5f0f\u901a\u4fe1\u6a21\u5757\uff0c\u63d0\u5347\u4efb\u52a1\u6267\u884c\u548c\u6c9f\u901a\u6548\u7387\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u667a\u80fd\u4f53\u9700\u8981\u4e0e\u964c\u751f\u4ee3\u7406\u548c\u4eba\u7c7b\u7528\u6237\u534f\u4f5c\uff0c\u4f46\u7528\u6237\u7684\u6307\u4ee4\u5f80\u5f80\u6a21\u7cca\u4e14\u9690\u6666\uff0c\u56e0\u6b64\u5feb\u901f\u51c6\u786e\u5730\u5bf9\u9f50\u7528\u6237\u6f5c\u5728\u9700\u6c42\u6210\u4e3a\u5173\u952e\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u5bb6\u5ead\u8f85\u52a9\u6a21\u62df\u73af\u5883HA-Desire\uff0c\u7ed3\u5408LLM\u9a71\u52a8\u7684\u4eba\u7c7b\u7528\u6237\u4ee3\u7406\uff0c\u5e76\u63d0\u51fa\u4e86FAMER\u6846\u67b6\uff0c\u901a\u8fc7\u5fc3\u7406\u63a8\u7406\u673a\u5236\u3001\u53cd\u5c04\u5f0f\u901a\u4fe1\u6a21\u5757\u548c\u76ee\u6807\u76f8\u5173\u4fe1\u606f\u63d0\u53d6\u5b9e\u73b0\u9700\u6c42\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFAMER\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6267\u884c\u548c\u6c9f\u901a\u6548\u7387\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5feb\u901f\u9002\u5e94\u7528\u6237\u7279\u5b9a\u9700\u6c42\u3002", "conclusion": "FAMER\u6846\u67b6\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u667a\u80fd\u4f53\u7684\u9700\u6c42\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "embodied agents, desire alignment, mental reasoning, communication efficiency, LLM"}}
{"id": "2505.22513", "pdf": "https://arxiv.org/pdf/2505.22513", "abs": "https://arxiv.org/abs/2505.22513", "authors": ["Bradley Phillips", "Edith Elkind", "Nicholas Teh", "Tomasz W\u0105s"], "title": "Strengthening Proportionality in Temporal Voting", "categories": ["cs.GT", "cs.AI"], "comment": null, "summary": "We study proportional representation in the framework of temporal voting with\napproval ballots. Prior work adapted basic proportional representation concepts\n-- justified representation (JR), proportional JR (PJR), and extended JR (EJR)\n-- from the multiwinner setting to the temporal setting. Our work introduces\nand examines ways of going beyond EJR. Specifically, we consider stronger\nvariants of JR, PJR, and EJR, and introduce temporal adaptations of more\ndemanding multiwinner axioms, such as EJR+, full JR (FJR), full proportional JR\n(FPJR), and the Core. For each of these concepts, we investigate its existence\nand study its relationship to existing notions, thereby establishing a rich\nhierarchy of proportionality concepts. Notably, we show that two of our\nproposed axioms -- EJR+ and FJR -- strengthen EJR while remaining satisfiable\nin every temporal election.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u65f6\u5e8f\u6295\u7968\u4e2d\u4f7f\u7528\u66f4\u5f3a\u6bd4\u4f8b\u4ee3\u8868\u6982\u5ff5\u7684\u60c5\u51b5\uff0c\u5305\u62ecEJR+\u548cFJR\u7b49\u65b0\u63d0\u51fa\u7684\u516c\u7406\uff0c\u5e76\u8bc1\u660e\u4e86\u8fd9\u4e9b\u516c\u7406\u6bd4EJR\u66f4\u5f3a\u4f46\u4ecd\u53ef\u6ee1\u8db3\u3002", "motivation": "\u52a8\u673a\u662f\u6269\u5c55\u65f6\u5e8f\u6295\u7968\u4e2d\u7684\u6bd4\u4f8b\u4ee3\u8868\u6982\u5ff5\uff0c\u63d0\u51fa\u66f4\u5f3a\u7248\u672c\u7684\u516c\u7406\uff08\u5982EJR+\u548cFJR\uff09\uff0c\u5e76\u7814\u7a76\u5176\u53ef\u884c\u6027\u548c\u4e0e\u73b0\u6709\u6982\u5ff5\u7684\u5173\u7cfb\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u5f15\u5165\u65f6\u5e8f\u9002\u5e94\u6027\u5f3a\u7248\u591a\u8d62\u5bb6\u516c\u7406\uff08\u5982EJR+, FJR, FPJR, \u548cCore\uff09\uff0c\u5e76\u9a8c\u8bc1\u5176\u5b58\u5728\u6027\u53ca\u4e0e\u73b0\u6709\u6982\u5ff5\u7684\u5173\u7cfb\u3002", "result": "\u7ed3\u679c\u663e\u793aEJR+\u548cFJR\u4e0d\u4ec5\u80fd\u5f3a\u5316EJR\uff0c\u4e14\u5728\u6bcf\u6b21\u65f6\u5e8f\u9009\u4e3e\u4e2d\u5747\u53ef\u6ee1\u8db3\u3002", "conclusion": "\u7ed3\u8bba\u662f\u5efa\u7acb\u4e86\u6bd4\u4f8b\u4ee3\u8868\u6982\u5ff5\u7684\u4e30\u5bcc\u5c42\u6b21\u7ed3\u6784\uff0c\u7279\u522b\u662fEJR+\u548cFJR\u7684\u5f15\u5165\u4e3a\u65f6\u5e8f\u6295\u7968\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u4ee3\u8868\u4fdd\u969c\u3002", "keywords": "\u6bd4\u4f8b\u4ee3\u8868, \u65f6\u5e8f\u6295\u7968, \u6279\u51c6\u6295\u7968, EJR+, FJR, \u516c\u7406"}}
{"id": "2505.22238", "pdf": "https://arxiv.org/pdf/2505.22238", "abs": "https://arxiv.org/abs/2505.22238", "authors": ["A. Ploshkin", "V. Tytskiy", "A. Pismenny", "V. Baikalov", "E. Taychinov", "A. Permiakov", "D. Burlakov", "E. Krofto", "N. Savushkin"], "title": "Yambda-5B -- A Large-Scale Multi-modal Dataset for Ranking And Retrieval", "categories": ["cs.IR", "cs.LG"], "comment": null, "summary": "We present Yambda-5B, a large-scale open dataset sourced from the\nYandex.Music streaming platform. Yambda-5B contains 4.79 billion user-item\ninteractions from 1 million users across 9.39 million tracks. The dataset\nincludes two primary types of interactions: implicit feedback (listening\nevents) and explicit feedback (likes, dislikes, unlikes and undislikes). In\naddition, we provide audio embeddings for most tracks, generated by a\nconvolutional neural network trained on audio spectrograms. A key\ndistinguishing feature of Yambda-5B is the inclusion of the is_organic flag,\nwhich separates organic user actions from recommendation-driven events. This\ndistinction is critical for developing and evaluating machine learning\nalgorithms, as Yandex.Music relies on recommender systems to personalize track\nselection for users. To support rigorous benchmarking, we introduce an\nevaluation protocol based on a Global Temporal Split, allowing recommendation\nalgorithms to be assessed in conditions that closely mirror real-world use. We\nreport benchmark results for standard baselines (ItemKNN, iALS) and advanced\nmodels (SANSA, SASRec) using a variety of evaluation metrics. By releasing\nYambda-5B to the community, we aim to provide a readily accessible,\nindustrial-scale resource to advance research, foster innovation, and promote\nreproducible results in recommender systems.", "AI": {"tldr": "Yambda-5B\u662f\u4e00\u4e2a\u57fa\u4e8eYandex.Music\u5e73\u53f0\u7684\u5927\u89c4\u6a21\u5f00\u653e\u6570\u636e\u96c6\uff0c\u5305\u542b47.9\u4ebf\u7528\u6237-\u9879\u76ee\u4ea4\u4e92\u548c\u97f3\u9891\u5d4c\u5165\uff0c\u7279\u522b\u533a\u5206\u4e86\u6709\u673a\u7528\u6237\u884c\u4e3a\u548c\u63a8\u8350\u9a71\u52a8\u4e8b\u4ef6\uff0c\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u7814\u7a76\u548c\u8bc4\u4f30\u3002", "motivation": "\u4e3a\u63a8\u8350\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u5de5\u4e1a\u89c4\u6a21\u7684\u5f00\u653e\u6570\u636e\u96c6\uff0c\u652f\u6301\u7b97\u6cd5\u5f00\u53d1\u548c\u8bc4\u4f30\uff0c\u4fc3\u8fdb\u53ef\u590d\u73b0\u6027\u548c\u521b\u65b0\u3002", "method": "\u901a\u8fc7\u7528\u6237\u4ea4\u4e92\u6570\u636e\uff08\u663e\u5f0f\u548c\u9690\u5f0f\u53cd\u9988\uff09\u53ca\u97f3\u9891\u5d4c\u5165\u6784\u5efa\u6570\u636e\u96c6\uff0c\u5e76\u5f15\u5165\u5168\u5c40\u65f6\u95f4\u5206\u5272\u8bc4\u4f30\u534f\u8bae\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u5c55\u793a\u4e86ItemKNN\u3001iALS\u7b49\u6807\u51c6\u6a21\u578b\u548cSANSA\u3001SASRec\u7b49\u5148\u8fdb\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "Yambda-5B\u4e3a\u63a8\u8350\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u771f\u5b9e\u7684\u5de5\u4e1a\u89c4\u6a21\u8d44\u6e90\u3002", "keywords": "Yambda-5B, \u6570\u636e\u96c6, \u63a8\u8350\u7cfb\u7edf, \u7528\u6237\u4ea4\u4e92, \u97f3\u9891\u5d4c\u5165, \u8bc4\u4f30\u534f\u8bae"}}
{"id": "2505.22543", "pdf": "https://arxiv.org/pdf/2505.22543", "abs": "https://arxiv.org/abs/2505.22543", "authors": ["Ziheng Jia", "Zicheng Zhang", "Zeyu Zhang", "Yingji Liang", "Xiaorong Zhu", "Chunyi Li", "Jinliang Han", "Haoning Wu", "Bin Wang", "Haoran Zhang", "Guanyu Zhu", "Qiyong Zhao", "Xiaohong Liu", "Guangtao Zhai", "Xiongkuo Min"], "title": "Scaling-up Perceptual Video Quality Assessment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The data scaling law has been shown to significantly enhance the performance\nof large multi-modal models (LMMs) across various downstream tasks. However, in\nthe domain of perceptual video quality assessment (VQA), the potential of\nscaling law remains unprecedented due to the scarcity of labeled resources and\nthe insufficient scale of datasets. To address this, we propose\n\\textbf{OmniVQA}, an efficient framework designed to efficiently build\nhigh-quality, human-in-the-loop VQA multi-modal instruction databases (MIDBs).\nWe then scale up to create \\textbf{OmniVQA-Chat-400K}, the largest MIDB in the\nVQA field concurrently. Our focus is on the technical and aesthetic quality\ndimensions, with abundant in-context instruction data to provide fine-grained\nVQA knowledge. Additionally, we have built the \\textbf{OmniVQA-MOS-20K} dataset\nto enhance the model's quantitative quality rating capabilities. We then\nintroduce a \\textbf{complementary} training strategy that effectively leverages\nthe knowledge from datasets for quality understanding and quality rating tasks.\nFurthermore, we propose the \\textbf{OmniVQA-FG (fine-grain)-Benchmark} to\nevaluate the fine-grained performance of the models. Our results demonstrate\nthat our models achieve state-of-the-art performance in both quality\nunderstanding and rating tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86OmniVQA\u6846\u67b6\uff0c\u6784\u5efa\u5927\u89c4\u6a21VQA\u591a\u6a21\u6001\u6307\u4ee4\u5e93\uff0c\u5e76\u5f15\u5165\u4e92\u8865\u8bad\u7ec3\u7b56\u7565\uff0c\u6a21\u578b\u5728\u8d28\u91cf\u548c\u8bc4\u5206\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u9886\u57df\u7531\u4e8e\u6807\u6ce8\u8d44\u6e90\u7a00\u7f3a\u548c\u6570\u636e\u89c4\u6a21\u4e0d\u8db3\uff0c\u5c1a\u672a\u5145\u5206\u5229\u7528\u6570\u636e\u6269\u5c55\u5b9a\u5f8b\u7684\u6f5c\u529b\u3002", "method": "\u6784\u5efaOmniVQA-Chat-400K\u6307\u4ee4\u5e93\u548cOmniVQA-MOS-20K\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u4e92\u8865\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u63a8\u51fa\u7ec6\u7c92\u5ea6\u8bc4\u6d4b\u57fa\u51c6\u3002", "result": "\u6a21\u578b\u5728\u8d28\u91cf\u548c\u8bc4\u5206\u4efb\u52a1\u4e0a\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "OmniVQA\u6846\u67b6\u6210\u529f\u89e3\u51b3VQA\u9886\u57df\u6570\u636e\u6269\u5c55\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "keywords": "\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u3001\u591a\u6a21\u6001\u6307\u4ee4\u5e93\u3001\u6570\u636e\u6269\u5c55\u3001\u4e92\u8865\u8bad\u7ec3\u3001\u7ec6\u7c92\u5ea6\u8bc4\u6d4b"}}
{"id": "2505.22243", "pdf": "https://arxiv.org/pdf/2505.22243", "abs": "https://arxiv.org/abs/2505.22243", "authors": ["Bin Li", "Diwei Liu", "Zehong Hu", "Jia Jia"], "title": "UDuo: Universal Dual Optimization Framework for Online Matching", "categories": ["cs.IR", "cs.LG"], "comment": null, "summary": "Online resource allocation under budget constraints critically depends on\nproper modeling of user arrival dynamics. Classical approaches employ\nstochastic user arrival models to derive near-optimal solutions through\nfractional matching formulations of exposed users for downstream allocation\ntasks. However, this is no longer a reasonable assumption when the environment\nchanges dynamically. In this work, We propose the Universal Dual optimization\nframework UDuo, a novel paradigm that fundamentally rethinks online allocation\nthrough three key innovations: (i) a temporal user arrival representation\nvector that explicitly captures distribution shifts in user arrival patterns\nand resource consumption dynamics, (ii) a resource pacing learner with adaptive\nallocation policies that generalize to heterogeneous constraint scenarios, and\n(iii) an online time-series forecasting approach for future user arrival\ndistributions that achieves asymptotically optimal solutions with constraint\nfeasibility guarantees in dynamic environments. Experimental results show that\nUDuo achieves higher efficiency and faster convergence than the traditional\nstochastic arrival model in real-world pricing while maintaining rigorous\ntheoretical validity for general online allocation problems.", "AI": {"tldr": "\u63d0\u51faUDuo\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7528\u6237\u5230\u8fbe\u8868\u793a\u3001\u81ea\u9002\u5e94\u5206\u914d\u7b56\u7565\u548c\u5728\u7ebf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e0b\u7684\u5728\u7ebf\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u6548\u7387\u548c\u6536\u655b\u901f\u5ea6\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u7528\u6237\u5230\u8fbe\u5047\u8bbe\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6355\u6349\u5206\u5e03\u53d8\u5316\u5e76\u4fdd\u8bc1\u7ea6\u675f\u53ef\u884c\u6027\u7684\u65b0\u6846\u67b6\u3002", "method": "UDuo\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u521b\u65b0\uff1a(i) \u52a8\u6001\u7528\u6237\u5230\u8fbe\u8868\u793a\u5411\u91cf\uff0c(ii) \u81ea\u9002\u5e94\u8d44\u6e90\u5206\u914d\u7b56\u7565\uff0c(iii) \u5728\u7ebf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUDuo\u5728\u771f\u5b9e\u5b9a\u4ef7\u573a\u666f\u4e2d\u6548\u7387\u548c\u6536\u655b\u901f\u5ea6\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u8bc1\u7406\u8bba\u6709\u6548\u6027\u3002", "conclusion": "UDuo\u4e3a\u52a8\u6001\u73af\u5883\u4e0b\u7684\u5728\u7ebf\u8d44\u6e90\u5206\u914d\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7406\u8bba\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5728\u7ebf\u8d44\u6e90\u5206\u914d\u3001\u9884\u7b97\u7ea6\u675f\u3001\u52a8\u6001\u73af\u5883\u3001UDuo\u6846\u67b6\u3001\u81ea\u9002\u5e94\u7b56\u7565"}}
{"id": "2505.22258", "pdf": "https://arxiv.org/pdf/2505.22258", "abs": "https://arxiv.org/abs/2505.22258", "authors": ["Benjamin Serfling", "Hannes Reichert", "Lorenzo Bayerlein", "Konrad Doll", "Kati Radkhah-Lens"], "title": "LiDAR Based Semantic Perception for Forklifts in Outdoor Environments", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "In this study, we present a novel LiDAR-based semantic segmentation framework\ntailored for autonomous forklifts operating in complex outdoor environments.\nCentral to our approach is the integration of a dual LiDAR system, which\ncombines forward-facing and downward-angled LiDAR sensors to enable\ncomprehensive scene understanding, specifically tailored for industrial\nmaterial handling tasks. The dual configuration improves the detection and\nsegmentation of dynamic and static obstacles with high spatial precision. Using\nhigh-resolution 3D point clouds captured from two sensors, our method employs a\nlightweight yet robust approach that segments the point clouds into\nsafety-critical instance classes such as pedestrians, vehicles, and forklifts,\nas well as environmental classes such as driveable ground, lanes, and\nbuildings. Experimental validation demonstrates that our approach achieves high\nsegmentation accuracy while satisfying strict runtime requirements,\nestablishing its viability for safety-aware, fully autonomous forklift\nnavigation in dynamic warehouse and yard environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u57fa\u4e8eLiDAR\u7684\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u4e13\u4e3a\u590d\u6742\u6237\u5916\u73af\u5883\u4e0b\u7684\u81ea\u52a8\u53c9\u8f66\u8bbe\u8ba1\u3002\u901a\u8fc7\u96c6\u6210\u53ccLiDAR\u7cfb\u7edf\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u969c\u788d\u7269\u68c0\u6d4b\u4e0e\u5206\u5272\u7684\u7a7a\u95f4\u7cbe\u5ea6\u3002", "motivation": "\u9488\u5bf9\u5de5\u4e1a\u7269\u6599\u642c\u8fd0\u4efb\u52a1\u4e2d\u590d\u6742\u7684\u52a8\u6001\u548c\u9759\u6001\u969c\u788d\u7269\u68c0\u6d4b\u9700\u6c42\uff0c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u3001\u5b89\u5168\u7684\u81ea\u52a8\u53c9\u8f66\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u53ccLiDAR\u7cfb\u7edf\uff08\u524d\u5411\u548c\u5411\u4e0b\u503e\u659c\uff09\uff0c\u7ed3\u5408\u9ad8\u5206\u8fa8\u73873D\u70b9\u4e91\u6570\u636e\uff0c\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u4e14\u9c81\u68d2\u7684\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u5206\u7c7b\u5b89\u5168\u5173\u952e\u5b9e\u4f8b\u548c\u573a\u666f\u7c7b\u522b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6ee1\u8db3\u4e25\u683c\u5b9e\u65f6\u6027\u8981\u6c42\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u5206\u5272\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u9002\u7528\u4e8e\u52a8\u6001\u4ed3\u5e93\u548c\u573a\u9662\u73af\u5883\u4e2d\u7684\u5b89\u5168\u611f\u77e5\u5168\u81ea\u52a8\u53c9\u8f66\u5bfc\u822a\u3002", "keywords": "LiDAR, \u8bed\u4e49\u5206\u5272, \u81ea\u52a8\u53c9\u8f66, \u53cc\u4f20\u611f\u5668, \u5b89\u5168\u5bfc\u822a"}}
{"id": "2505.22564", "pdf": "https://arxiv.org/pdf/2505.22564", "abs": "https://arxiv.org/abs/2505.22564", "authors": ["Jaehyun Choi", "Jiwan Hur", "Gyojin Han", "Jaemyung Yu", "Junmo Kim"], "title": "PRISM: Video Dataset Condensation with Progressive Refinement and Insertion for Sparse Motion", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Video dataset condensation has emerged as a critical technique for addressing\nthe computational challenges associated with large-scale video data processing\nin deep learning applications. While significant progress has been made in\nimage dataset condensation, the video domain presents unique challenges due to\nthe complex interplay between spatial content and temporal dynamics. This paper\nintroduces PRISM, Progressive Refinement and Insertion for Sparse Motion, for\nvideo dataset condensation, a novel approach that fundamentally reconsiders how\nvideo data should be condensed. Unlike the previous method that separates\nstatic content from dynamic motion, our method preserves the essential\ninterdependence between these elements. Our approach progressively refines and\ninserts frames to fully accommodate the motion in an action while achieving\nbetter performance but less storage, considering the relation of gradients for\neach frame. Extensive experiments across standard video action recognition\nbenchmarks demonstrate that PRISM outperforms existing disentangled approaches\nwhile maintaining compact representations suitable for resource-constrained\nenvironments.", "AI": {"tldr": "PRISM\u65b9\u6cd5\u901a\u8fc7\u6e10\u8fdb\u5f0f\u4f18\u5316\u548c\u7a00\u758f\u5e27\u63d2\u5165\u6280\u672f\u6539\u8fdb\u89c6\u9891\u6570\u636e\u96c6\u538b\u7f29\uff0c\u4fdd\u6301\u65f6\u7a7a\u52a8\u6001\u4ea4\u4e92\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u4e0e\u4f4e\u5b58\u50a8\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6570\u636e\u538b\u7f29\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u4fdd\u7559\u65f6\u7a7a\u52a8\u6001\u7684\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\uff0c\u9700\u65b0\u65b9\u6cd5\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "PRISM\u91c7\u7528\u6e10\u8fdb\u5f0f\u4f18\u5316\u548c\u9009\u62e9\u6027\u5e27\u63d2\u5165\u6280\u672f\uff0c\u7ed3\u5408\u68af\u5ea6\u5173\u7cfb\u5206\u6790\uff0c\u52a8\u6001\u8c03\u6574\u538b\u7f29\u540e\u7684\u89c6\u9891\u5e27\u3002", "result": "\u5728\u6807\u51c6\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPRISM\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u5b58\u50a8\u9700\u6c42\u3002", "conclusion": "PRISM\u901a\u8fc7\u4fdd\u7559\u65f6\u7a7a\u52a8\u6001\u4ea4\u4e92\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89c6\u9891\u6570\u636e\u538b\u7f29\u65b9\u6848\u3002", "keywords": "\u89c6\u9891\u6570\u636e\u96c6\u538b\u7f29,\u6df1\u5ea6\u5b66\u4e60,\u65f6\u7a7a\u52a8\u6001,\u6e10\u8fdb\u4f18\u5316,\u7a00\u758f\u5e27\u63d2\u5165"}}
{"id": "2505.22566", "pdf": "https://arxiv.org/pdf/2505.22566", "abs": "https://arxiv.org/abs/2505.22566", "authors": ["Yifan Xie", "Mingyang Li", "Shoujie Li", "Xingting Li", "Guangyu Chen", "Fei Ma", "Fei Richard Yu", "Wenbo Ding"], "title": "Universal Visuo-Tactile Video Understanding for Embodied Interaction", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 5 figures", "summary": "Tactile perception is essential for embodied agents to understand physical\nattributes of objects that cannot be determined through visual inspection\nalone. While existing approaches have made progress in visual and language\nmodalities for physical understanding, they fail to effectively incorporate\ntactile information that provides crucial haptic feedback for real-world\ninteraction. In this paper, we present VTV-LLM, the first multi-modal large\nlanguage model for universal Visuo-Tactile Video (VTV) understanding that\nbridges the gap between tactile perception and natural language. To address the\nchallenges of cross-sensor and cross-modal integration, we contribute VTV150K,\na comprehensive dataset comprising 150,000 video frames from 100 diverse\nobjects captured across three different tactile sensors (GelSight Mini, DIGIT,\nand Tac3D), annotated with four fundamental tactile attributes (hardness,\nprotrusion, elasticity, and friction). We develop a novel three-stage training\nparadigm that includes VTV enhancement for robust visuo-tactile representation,\nVTV-text alignment for cross-modal correspondence, and text prompt finetuning\nfor natural language generation. Our framework enables sophisticated tactile\nreasoning capabilities including feature assessment, comparative analysis,\nscenario-based decision making and so on. Experimental evaluations demonstrate\nthat VTV-LLM achieves superior performance in tactile video understanding\ntasks, establishing a foundation for more intuitive human-machine interaction\nin tactile domains.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86VTV-LLM\uff0c\u9996\u4e2a\u7528\u4e8e\u901a\u7528\u89c6\u89c9\u89e6\u89c9\u89c6\u9891\uff08VTV\uff09\u7406\u89e3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u89e6\u89c9\u611f\u77e5\u4e0e\u81ea\u7136\u8bed\u8a00\u7684\u7ed3\u5408\u586b\u8865\u4e86\u73b0\u6709\u6280\u672f\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u6001\u4e0a\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u672a\u80fd\u6709\u6548\u6574\u5408\u89e6\u89c9\u4fe1\u606f\uff0c\u800c\u8fd9\u79cd\u4fe1\u606f\u5bf9\u7269\u7406\u7406\u89e3\u81f3\u5173\u91cd\u8981\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86VTV-LLM\uff0c\u5305\u542b\u4e09\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff1aVTV\u589e\u5f3a\u3001VTV-\u6587\u672c\u5bf9\u9f50\u548c\u6587\u672c\u63d0\u793a\u5fae\u8c03\uff0c\u5e76\u8d21\u732e\u4e86\u5305\u542b15\u4e07\u5e27\u89c6\u9891\u7684VTV150K\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cVTV-LLM\u5728\u89e6\u89c9\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u7279\u5f81\u8bc4\u4f30\u3001\u6bd4\u8f83\u5206\u6790\u548c\u57fa\u4e8e\u573a\u666f\u7684\u51b3\u7b56\u7b49\u9ad8\u7ea7\u89e6\u89c9\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "VTV-LLM\u4e3a\u89e6\u89c9\u9886\u57df\u7684\u4eba\u673a\u4ea4\u4e92\u5960\u5b9a\u4e86\u66f4\u76f4\u89c2\u7684\u57fa\u7840\u3002", "keywords": "\u89e6\u89c9\u611f\u77e5, \u591a\u6a21\u6001, \u5927\u8bed\u8a00\u6a21\u578b, VTV-LLM, \u89c6\u89c9\u89e6\u89c9\u89c6\u9891"}}
{"id": "2505.22326", "pdf": "https://arxiv.org/pdf/2505.22326", "abs": "https://arxiv.org/abs/2505.22326", "authors": ["James M. Adams", "Gesine Reinert", "Lukasz Szpruch", "Carsten Maple", "Andrew Elliott"], "title": "Individualised Counterfactual Examples Using Conformal Prediction Intervals", "categories": ["stat.ML", "cs.LG"], "comment": "Submitted to Conformal and Probabilistic Predictions With\n  Applications (COPA) 2025", "summary": "Counterfactual explanations for black-box models aim to pr ovide insight into\nan algorithmic decision to its recipient. For a binary classification problem\nan individual counterfactual details which features might be changed for the\nmodel to infer the opposite class. High-dimensional feature spaces that are\ntypical of machine learning classification models admit many possible\ncounterfactual examples to a decision, and so it is important to identify\nadditional criteria to select the most useful counterfactuals. In this paper,\nwe explore the idea that the counterfactuals should be maximally informative\nwhen considering the knowledge of a specific individual about the underlying\nclassifier. To quantify this information gain we explicitly model the knowledge\nof the individual, and assess the uncertainty of predictions which the\nindividual makes by the width of a conformal prediction interval. Regions of\nfeature space where the prediction interval is wide correspond to areas where\nthe confidence in decision making is low, and an additional counterfactual\nexample might be more informative to an individual. To explore and evaluate our\nindividualised conformal prediction interval counterfactuals (CPICFs), first we\npresent a synthetic data set on a hypercube which allows us to fully visualise\nthe decision boundary, conformal intervals via three different methods, and\nresultant CPICFs. Second, in this synthetic data set we explore the impact of a\nsingle CPICF on the knowledge of an individual locally around the original\nquery. Finally, in both our synthetic data set and a complex real world dataset\nwith a combination of continuous and discrete variables, we measure the utility\nof these counterfactuals via data augmentation, testing the performance on a\nheld out set.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e2a\u4f53\u77e5\u8bc6\u7684\u4e2a\u6027\u5316\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u4e2a\u4f53\u7684\u77e5\u8bc6\u548c\u5229\u7528\u4fdd\u5f62\u9884\u6d4b\u533a\u95f4\u6765\u91cf\u5316\u4fe1\u606f\u589e\u76ca\uff0c\u4ece\u800c\u9009\u62e9\u6700\u6709\u7528\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u3002\u5b9e\u9a8c\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u9ed1\u76d2\u6a21\u578b\u7684\u51b3\u7b56\u901a\u5e38\u96be\u4ee5\u7406\u89e3\uff0c\u53cd\u4e8b\u5b9e\u89e3\u91ca\u80fd\u591f\u5e2e\u52a9\u4e2a\u4f53\u7406\u89e3\u51b3\u7b56\u7684\u539f\u56e0\u3002\u7136\u800c\uff0c\u9ad8\u7ef4\u7279\u5f81\u7a7a\u95f4\u4e2d\u5b58\u5728\u591a\u4e2a\u53ef\u80fd\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u9700\u8981\u989d\u5916\u7684\u6807\u51c6\u6765\u9009\u62e9\u6700\u6709\u7528\u7684\u89e3\u91ca\u3002\u8bba\u6587\u7684\u76ee\u7684\u662f\u901a\u8fc7\u8003\u8651\u4e2a\u4f53\u5bf9\u5206\u7c7b\u5668\u7684\u77e5\u8bc6\uff0c\u627e\u5230\u4fe1\u606f\u91cf\u6700\u5927\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e2a\u6027\u5316\u4fdd\u5f62\u9884\u6d4b\u533a\u95f4\u53cd\u4e8b\u5b9e\uff08CPICF\uff09\u65b9\u6cd5\u3002\u9996\u5148\uff0c\u6a21\u578b\u5316\u4e2a\u4f53\u7684\u77e5\u8bc6\uff0c\u901a\u8fc7\u4fdd\u5f62\u9884\u6d4b\u533a\u95f4\u7684\u5bbd\u5ea6\u8bc4\u4f30\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u5728\u9884\u6d4b\u533a\u95f4\u8f83\u5bbd\u7684\u7279\u5f81\u7a7a\u95f4\u533a\u57df\uff0c\u4e2a\u4f53\u7684\u51b3\u7b56\u4fe1\u5fc3\u8f83\u4f4e\uff0c\u53cd\u4e8b\u5b9e\u89e3\u91ca\u53ef\u80fd\u66f4\u6709\u4fe1\u606f\u91cf\u3002\u5b9e\u9a8c\u5305\u62ec\u5408\u6210\u6570\u636e\u96c6\u7684\u5b8c\u5168\u53ef\u89c6\u5316\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684\u6027\u80fd\u6d4b\u8bd5\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\uff0c\u8bba\u6587\u5c55\u793a\u4e86CPICF\u5982\u4f55\u63d0\u9ad8\u4e2a\u4f53\u7684\u77e5\u8bc6\u5c40\u90e8\u7406\u89e3\u3002\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86CPICF\u7684\u5b9e\u7528\u6027\u548c\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684CPICF\u65b9\u6cd5\u80fd\u591f\u6839\u636e\u4e2a\u4f53\u7684\u77e5\u8bc6\u6c34\u5e73\u63d0\u4f9b\u4fe1\u606f\u91cf\u6700\u5927\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9ed1\u76d2\u6a21\u578b\u51b3\u7b56\u7684\u53ef\u89e3\u91ca\u6027\u548c\u4e2a\u4f53\u7406\u89e3\u7684\u6548\u7387\u3002", "keywords": "\u53cd\u4e8b\u5b9e\u89e3\u91ca, \u4fdd\u5f62\u9884\u6d4b, \u9ed1\u76d2\u6a21\u578b, \u53ef\u89e3\u91ca\u6027, \u673a\u5668\u5b66\u4e60"}}
{"id": "2505.22581", "pdf": "https://arxiv.org/pdf/2505.22581", "abs": "https://arxiv.org/abs/2505.22581", "authors": ["Kartik Kuckreja", "Parul Gupta", "Injy Hamed", "Thamar Solorio", "Muhammad Haris Khan", "Abhinav Dhall"], "title": "Tell me Habibi, is it Real or Fake?", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 2 figures, 12 tables", "summary": "Deepfake generation methods are evolving fast, making fake media harder to\ndetect and raising serious societal concerns. Most deepfake detection and\ndataset creation research focuses on monolingual content, often overlooking the\nchallenges of multilingual and code-switched speech, where multiple languages\nare mixed within the same discourse. Code-switching, especially between Arabic\nand English, is common in the Arab world and is widely used in digital\ncommunication. This linguistic mixing poses extra challenges for deepfake\ndetection, as it can confuse models trained mostly on monolingual data. To\naddress this, we introduce \\textbf{ArEnAV}, the first large-scale\nArabic-English audio-visual deepfake dataset featuring intra-utterance\ncode-switching, dialectal variation, and monolingual Arabic content. It\n\\textbf{contains 387k videos and over 765 hours of real and fake videos}. Our\ndataset is generated using a novel pipeline integrating four Text-To-Speech and\ntwo lip-sync models, enabling comprehensive analysis of multilingual multimodal\ndeepfake detection. We benchmark our dataset against existing monolingual and\nmultilingual datasets, state-of-the-art deepfake detection models, and a human\nevaluation, highlighting its potential to advance deepfake research. The\ndataset can be accessed\n\\href{https://huggingface.co/datasets/kartik060702/ArEnAV-Full}{here}.", "AI": {"tldr": "ArEnAV\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u963f\u62c9\u4f2f\u8bed-\u82f1\u8bed\u97f3\u9891-\u89c6\u89c9\u6df1\u5ea6\u4f2a\u9020\u6570\u636e\u96c6\uff0c\u5305\u542b\u4ee3\u7801\u8f6c\u6362\u548c\u65b9\u8a00\u53d8\u5316\uff0c\u65e8\u5728\u89e3\u51b3\u591a\u8bed\u8a00\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u5355\u8bed\u8a00\u5185\u5bb9\uff0c\u5ffd\u89c6\u4e86\u591a\u8bed\u8a00\u548c\u4ee3\u7801\u8f6c\u6362\u8bed\u97f3\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u963f\u62c9\u4f2f\u8bed\u548c\u82f1\u8bed\u6df7\u7528\u7684\u573a\u666f\u3002", "method": "\u4f7f\u7528\u56db\u79cd\u6587\u672c\u5230\u8bed\u97f3\u548c\u4e24\u79cd\u5507\u540c\u6b65\u6a21\u578b\u7684\u65b0\u751f\u6210\u6d41\u7a0b\uff0c\u521b\u5efa\u4e86\u5305\u542b387k\u89c6\u9891\u548c765\u5c0f\u65f6\u5185\u5bb9\u7684ArEnAV\u6570\u636e\u96c6\u3002", "result": "\u6570\u636e\u96c6\u5728\u591a\u8bed\u8a00\u591a\u6a21\u6001\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e0e\u73b0\u6709\u5355\u8bed\u8a00\u548c\u591a\u8bed\u8a00\u6570\u636e\u96c6\u53ca\u6700\u5148\u8fdb\u68c0\u6d4b\u6a21\u578b\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002", "conclusion": "ArEnAV\u6570\u636e\u96c6\u4e3a\u6df1\u5ea6\u4f2a\u9020\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u8d44\u6e90\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u4ee3\u7801\u8f6c\u6362\u548c\u591a\u8bed\u8a00\u573a\u666f\u7684\u68c0\u6d4b\u3002", "keywords": "\u6df1\u5ea6\u4f2a\u9020, \u4ee3\u7801\u8f6c\u6362, \u591a\u8bed\u8a00\u68c0\u6d4b, \u963f\u62c9\u4f2f\u8bed-\u82f1\u8bed, \u97f3\u9891-\u89c6\u89c9"}}
{"id": "2505.22332", "pdf": "https://arxiv.org/pdf/2505.22332", "abs": "https://arxiv.org/abs/2505.22332", "authors": ["Timo L\u00f6hr", "Paul Hofman", "Felix Mohr", "Eyke H\u00fcllermeier"], "title": "Credal Prediction based on Relative Likelihood", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Predictions in the form of sets of probability distributions, so-called\ncredal sets, provide a suitable means to represent a learner's epistemic\nuncertainty. In this paper, we propose a theoretically grounded approach to\ncredal prediction based on the statistical notion of relative likelihood: The\ntarget of prediction is the set of all (conditional) probability distributions\nproduced by the collection of plausible models, namely those models whose\nrelative likelihood exceeds a specified threshold. This threshold has an\nintuitive interpretation and allows for controlling the trade-off between\ncorrectness and precision of credal predictions. We tackle the problem of\napproximating credal sets defined in this way by means of suitably modified\nensemble learning techniques. To validate our approach, we illustrate its\neffectiveness by experiments on benchmark datasets demonstrating superior\nuncertainty representation without compromising predictive performance. We also\ncompare our method against several state-of-the-art baselines in credal\nprediction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u5bf9\u4f3c\u7136\u7684\u7edf\u8ba1\u65b9\u6cd5\u6765\u9884\u6d4b\u6982\u7387\u5206\u5e03\u96c6\uff08\u79f0\u4e3acredal\u96c6\uff09\uff0c\u4ee5\u5e73\u8861\u6b63\u786e\u6027\u548c\u7cbe\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u7684\u96c6\u6210\u5b66\u4e60\u6280\u672f\u8fd1\u4f3c\u8fd9\u4e9b\u96c6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u786e\u5b9a\u6027\u8868\u793a\u548c\u9884\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u66f4\u6709\u6548\u5730\u8868\u793a\u5b66\u4e60\u8005\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u8bba\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u7406\u8bba\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u6982\u7387\u5206\u5e03\u96c6\uff08credal\u96c6\uff09\uff0c\u5e76\u901a\u8fc7\u7edf\u8ba1\u76f8\u5bf9\u4f3c\u7136\u6765\u63a7\u5236\u9884\u6d4b\u7684\u7cbe\u786e\u6027\u4e0e\u6b63\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "method": "\u8bba\u6587\u91c7\u7528\u57fa\u4e8e\u7edf\u8ba1\u76f8\u5bf9\u4f3c\u7136\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u5b9a\u9608\u503c\u7b5b\u9009\u51fa\u53ef\u80fd\u7684\u6a21\u578b\u96c6\u5408\uff0c\u8fdb\u800c\u751f\u6210credal\u96c6\u3002\u91c7\u7528\u6539\u8fdb\u7684\u96c6\u6210\u5b66\u4e60\u6280\u672f\u6765\u8fd1\u4f3c\u8fd9\u4e9b\u96c6\u5408\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e0d\u4ec5\u80fd\u6709\u6548\u8868\u793a\u4e0d\u786e\u5b9a\u6027\uff0c\u8fd8\u4e0d\u5f71\u54cd\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709credal\u9884\u6d4b\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3acredal\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u7406\u8bba\u53ef\u9760\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u5408\u9700\u8981\u6743\u8861\u4e0d\u786e\u5b9a\u6027\u8868\u793a\u548c\u9884\u6d4b\u7cbe\u786e\u6027\u7684\u573a\u666f\u3002", "keywords": "credal\u9884\u6d4b,\u76f8\u5bf9\u4f3c\u7136,\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027,\u96c6\u6210\u5b66\u4e60,\u6982\u7387\u5206\u5e03\u96c6"}}
{"id": "2505.22583", "pdf": "https://arxiv.org/pdf/2505.22583", "abs": "https://arxiv.org/abs/2505.22583", "authors": ["Tobias Lindenbauer", "Egor Bogomolov", "Yaroslav Zharov"], "title": "GitGoodBench: A Novel Benchmark For Evaluating Agentic Performance On Git", "categories": ["cs.SE", "cs.AI"], "comment": "Short Paper, 5 pages", "summary": "Benchmarks for Software Engineering (SE) AI agents, most notably SWE-bench,\nhave catalyzed progress in programming capabilities of AI agents. However, they\noverlook critical developer workflows such as Version Control System (VCS)\noperations. To address this issue, we present GitGoodBench, a novel benchmark\nfor evaluating AI agent performance on VCS tasks. GitGoodBench covers three\ncore Git scenarios extracted from permissive open-source Python, Java, and\nKotlin repositories. Our benchmark provides three datasets: a comprehensive\nevaluation suite (900 samples), a rapid prototyping version (120 samples), and\na training corpus (17,469 samples). We establish baseline performance on the\nprototyping version of our benchmark using GPT-4o equipped with custom tools,\nachieving a 21.11% solve rate overall. We expect GitGoodBench to serve as a\ncrucial stepping stone toward truly comprehensive SE agents that go beyond mere\nprogramming.", "AI": {"tldr": "GitGoodBench \u662f\u4e00\u79cd\u9488\u5bf9 AI \u4ee3\u7406\u5728\u7248\u672c\u63a7\u5236\u7cfb\u7edf\uff08VCS\uff09\u4efb\u52a1\u4e0a\u6027\u80fd\u8bc4\u4f30\u7684\u65b0\u57fa\u51c6\u3002\u5b83\u5f25\u8865\u4e86\u73b0\u6709\u57fa\u51c6\uff08\u5982 SWE-bench\uff09\u5ffd\u89c6\u5f00\u53d1\u8005\u5173\u952e\u5de5\u4f5c\u6d41\u7a0b\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709 SE AI \u4ee3\u7406\u57fa\u51c6\uff08\u5982 SWE-bench\uff09\u5ffd\u7565\u4e86\u7248\u672c\u63a7\u5236\u7cfb\u7edf\uff08VCS\uff09\u64cd\u4f5c\u7b49\u5173\u952e\u5f00\u53d1\u6d41\u7a0b\uff0c\u9650\u5236\u4e86 AI \u4ee3\u7406\u5728\u5b9e\u9645\u5f00\u53d1\u4e2d\u7684\u5168\u9762\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86 GitGoodBench \u57fa\u51c6\uff0c\u6db5\u76d6\u4ece\u5f00\u6e90 Python\u3001Java \u548c Kotlin \u4ed3\u5e93\u63d0\u53d6\u7684\u4e09\u79cd\u6838\u5fc3 Git \u573a\u666f\u3002\u63d0\u4f9b\u4e09\u4e2a\u6570\u636e\u96c6\uff1a\u5168\u9762\u8bc4\u4f30\u96c6\uff08900 \u6837\u672c\uff09\u3001\u5feb\u901f\u539f\u578b\u7248\u672c\uff08120 \u6837\u672c\uff09\u548c\u8bad\u7ec3\u8bed\u6599\uff0817,469 \u6837\u672c\uff09\u3002\u4f7f\u7528\u914d\u5907\u5b9a\u5236\u5de5\u5177\u7684 GPT-4o \u5feb\u901f\u539f\u578b\u7248\u672c\u4e0a\u5efa\u7acb\u57fa\u7ebf\u6027\u80fd\u3002", "result": "\u5728\u5feb\u901f\u539f\u578b\u7248\u672c\u4e0a\u7684\u57fa\u7ebf\u6027\u80fd\u4e3a 21.11% \u89e3\u51b3\u7387\u3002GitGoodBench \u6709\u671b\u63a8\u52a8 SE \u4ee3\u7406\u4ece\u5355\u7eaf\u7f16\u7a0b\u5411\u66f4\u5168\u9762\u7684\u529f\u80fd\u8fc8\u8fdb\u3002", "conclusion": "GitGoodBench \u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u5e76\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u5168\u9762\u7684 SE AI \u4ee3\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002", "keywords": "GitGoodBench, VCS, SE AI agents, benchmark, Git operations"}}
{"id": "2505.22342", "pdf": "https://arxiv.org/pdf/2505.22342", "abs": "https://arxiv.org/abs/2505.22342", "authors": ["Shriram M S", "Xinyue Hao", "Shihao Hou", "Yang Lu", "Laura Sevilla-Lara", "Anurag Arnab", "Shreyank N Gowda"], "title": "Progressive Data Dropout: An Embarrassingly Simple Approach to Faster Training", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The success of the machine learning field has reliably depended on training\non large datasets. While effective, this trend comes at an extraordinary cost.\nThis is due to two deeply intertwined factors: the size of models and the size\nof datasets. While promising research efforts focus on reducing the size of\nmodels, the other half of the equation remains fairly mysterious. Indeed, it is\nsurprising that the standard approach to training remains to iterate over and\nover, uniformly sampling the training dataset. In this paper we explore a\nseries of alternative training paradigms that leverage insights from\nhard-data-mining and dropout, simple enough to implement and use that can\nbecome the new training standard. The proposed Progressive Data Dropout reduces\nthe number of effective epochs to as little as 12.4% of the baseline. This\nsavings actually do not come at any cost for accuracy. Surprisingly, the\nproposed method improves accuracy by up to 4.82%. Our approach requires no\nchanges to model architecture or optimizer, and can be applied across standard\ntraining pipelines, thus posing an excellent opportunity for wide adoption.\nCode can be found here: https://github.com/bazyagami/LearningWithRevision", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6e10\u8fdb\u6570\u636e\u4e22\u5f03\u7684\u65b0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u6240\u9700\u7684\u6570\u636e\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u5b66\u4e60\u4f9d\u8d56\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u4f46\u8fd9\u4e00\u65b9\u6cd5\u6210\u672c\u9ad8\u6602\u3002\u5c3d\u7ba1\u5df2\u6709\u7814\u7a76\u5173\u6ce8\u6a21\u578b\u538b\u7f29\uff0c\u4f46\u6570\u636e\u8bad\u7ec3\u65b9\u5f0f\u7684\u4f18\u5316\u4ecd\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u8303\u5f0f\u3002", "method": "\u7ed3\u5408\u96be\u6837\u672c\u6316\u6398\u548cdropout\u7684\u601d\u60f3\uff0c\u63d0\u51fa\u6e10\u8fdb\u6570\u636e\u4e22\u5f03\u65b9\u6cd5\uff0c\u51cf\u5c11\u6709\u6548\u8bad\u7ec3\u8f6e\u6b21\uff0c\u4e0d\u9700\u8981\u4fee\u6539\u6a21\u578b\u7ed3\u6784\u6216\u4f18\u5316\u5668\u3002", "result": "\u5c06\u6709\u6548\u8bad\u7ec3\u8f6e\u6b21\u51cf\u5c11\u81f3\u57fa\u51c6\u768412.4%\uff0c\u540c\u65f6\u7cbe\u5ea6\u63d0\u5347\u4e864.82%\uff0c\u4e14\u65b9\u6cd5\u901a\u7528\u6027\u5f3a\uff0c\u6613\u96c6\u6210\u5230\u73b0\u6709\u6d41\u7a0b\u4e2d\u3002", "conclusion": "\u6e10\u8fdb\u6570\u636e\u4e22\u5f03\u65b9\u6cd5\u5728\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u7684\u6f5c\u529b\u3002", "keywords": "\u673a\u5668\u5b66\u4e60, \u6570\u636e\u8bad\u7ec3, \u6e10\u8fdb\u6570\u636e\u4e22\u5f03, \u96be\u6837\u672c\u6316\u6398, dropout"}}
{"id": "2505.22598", "pdf": "https://arxiv.org/pdf/2505.22598", "abs": "https://arxiv.org/abs/2505.22598", "authors": ["Luca Maria Del Bono", "Federico Ricci-Tersenghi", "Francesco Zamponi"], "title": "On the performance of machine-learning assisted Monte Carlo in sampling from simple statistical physics models", "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech", "cs.AI", "cs.LG", "physics.comp-ph"], "comment": "16 pages, 9 figures", "summary": "Recent years have seen a rise in the application of machine learning\ntechniques to aid the simulation of hard-to-sample systems that cannot be\nstudied using traditional methods. Despite the introduction of many different\narchitectures and procedures, a wide theoretical understanding is still\nlacking, with the risk of suboptimal implementations. As a first step to\naddress this gap, we provide here a complete analytic study of the widely-used\nSequential Tempering procedure applied to a shallow MADE architecture for the\nCurie-Weiss model. The contribution of this work is twofold: firstly, we give a\ndescription of the optimal weights and of the training under Gradient Descent\noptimization. Secondly, we compare what happens in Sequential Tempering with\nand without the addition of local Metropolis Monte Carlo steps. We are thus\nable to give theoretical predictions on the best procedure to apply in this\ncase. This work establishes a clear theoretical basis for the integration of\nmachine learning techniques into Monte Carlo sampling and optimization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u96be\u4ee5\u91c7\u6837\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u6790Sequential Tempering\u5728Curie-Weiss\u6a21\u578b\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u4f9b\u4e86\u6700\u4f18\u6743\u91cd\u548c\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u7684\u89e3\u6790\u7814\u7a76\uff0c\u5e76\u6bd4\u8f83\u4e86\u662f\u5426\u52a0\u5165\u5c40\u90e8Metropolis Monte Carlo\u6b65\u9aa4\u7684\u6548\u679c\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u6280\u672f\u4e0e\u8499\u7279\u5361\u6d1b\u91c7\u6837\u7684\u7ed3\u5408\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u6280\u672f\u5728\u96be\u4ee5\u91c7\u6837\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u589e\u591a\uff0c\u7f3a\u4e4f\u7406\u8bba\u7406\u89e3\u53ef\u80fd\u5bfc\u81f4\u6b21\u4f18\u5b9e\u73b0\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3aSequential Tempering\u5728\u7279\u5b9a\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u89e3\u6790\u57fa\u7840\u3002", "method": "\u91c7\u7528Sequential Tempering\u548c\u6d45\u5c42MADE\u67b6\u6784\u5e94\u7528\u4e8eCurie-Weiss\u6a21\u578b\uff0c\u5206\u6790\u6700\u4f18\u6743\u91cd\u548c\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\uff0c\u5e76\u6bd4\u8f83\u662f\u5426\u5305\u542b\u5c40\u90e8Metropolis Monte Carlo\u6b65\u9aa4\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u5f97\u51fa\u6700\u4f18\u6743\u91cd\u548c\u8bad\u7ec3\u7684\u7406\u8bba\u9884\u6d4b\uff0c\u5e76\u660e\u786e\u4e86\u5728Sequential Tempering\u4e2d\u52a0\u5165\u5c40\u90e8\u8499\u7279\u5361\u6d1b\u6b65\u9aa4\u7684\u6700\u4f73\u5b9e\u8df5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u673a\u5668\u5b66\u4e60\u4e0e\u8499\u7279\u5361\u6d1b\u91c7\u6837\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u7406\u8bba\u57fa\u7840\uff0c\u6307\u5bfc\u672a\u6765\u7684\u4f18\u5316\u548c\u5e94\u7528\u3002", "keywords": "Sequential Tempering, Curie-Weiss model, machine learning, Monte Carlo sampling, Gradient Descent"}}
{"id": "2505.22364", "pdf": "https://arxiv.org/pdf/2505.22364", "abs": "https://arxiv.org/abs/2505.22364", "authors": ["Gabriele Visentin", "Patrick Cheridito"], "title": "Computing Optimal Transport Maps and Wasserstein Barycenters Using Conditional Normalizing Flows", "categories": ["stat.ML", "cs.LG", "65K99 (Primary) 68T07, 68T99 (Secondary)"], "comment": null, "summary": "We present a novel method for efficiently computing optimal transport maps\nand Wasserstein barycenters in high-dimensional spaces. Our approach uses\nconditional normalizing flows to approximate the input distributions as\ninvertible pushforward transformations from a common latent space. This makes\nit possible to directly solve the primal problem using gradient-based\nminimization of the transport cost, unlike previous methods that rely on dual\nformulations and complex adversarial optimization. We show how this approach\ncan be extended to compute Wasserstein barycenters by solving a conditional\nvariance minimization problem. A key advantage of our conditional architecture\nis that it enables the computation of barycenters for hundreds of input\ndistributions, which was computationally infeasible with previous methods. Our\nnumerical experiments illustrate that our approach yields accurate results\nacross various high-dimensional tasks and compares favorably with previous\nstate-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6761\u4ef6\u5f52\u4e00\u5316\u6d41\u9ad8\u6548\u8ba1\u7b97\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u6700\u4f18\u4f20\u8f93\u6620\u5c04\u548cWasserstein\u91cd\u5fc3\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u68af\u5ea6\u7684\u6700\u5c0f\u5316\u76f4\u63a5\u6c42\u89e3\u539f\u59cb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5bf9\u5076\u516c\u5f0f\u548c\u590d\u6742\u7684\u5bf9\u6297\u4f18\u5316\uff0c\u96be\u4ee5\u9ad8\u6548\u8ba1\u7b97\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u6700\u4f18\u4f20\u8f93\u6620\u5c04\u548cWasserstein\u91cd\u5fc3\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u76f4\u63a5\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6761\u4ef6\u5f52\u4e00\u5316\u6d41\u8fd1\u4f3c\u8f93\u5165\u5206\u5e03\uff0c\u5c06\u5176\u6620\u5c04\u5230\u5171\u4eab\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5f62\u6210\u53ef\u9006\u7684\u63a8\u6f14\u53d8\u6362\u3002\u901a\u8fc7\u57fa\u4e8e\u68af\u5ea6\u7684\u6700\u5c0f\u5316\u4f20\u8f93\u6210\u672c\u76f4\u63a5\u6c42\u89e3\u539f\u59cb\u95ee\u9898\uff0c\u5e76\u6269\u5c55\u5230\u901a\u8fc7\u6761\u4ef6\u65b9\u5dee\u6700\u5c0f\u5316\u8ba1\u7b97Wasserstein\u91cd\u5fc3\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9ad8\u7ef4\u4efb\u52a1\u4e2d\u7ed3\u679c\u51c6\u786e\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u80fd\u8ba1\u7b97\u6570\u767e\u4e2a\u8f93\u5165\u5206\u5e03\u7684\u91cd\u5fc3\uff0c\u8fd9\u5728\u4e4b\u524d\u662f\u8ba1\u7b97\u4e0d\u53ef\u884c\u7684\u3002", "conclusion": "\u6761\u4ef6\u5f52\u4e00\u5316\u6d41\u7684\u67b6\u6784\u4e3a\u9ad8\u7ef4\u6700\u4f18\u4f20\u8f93\u548cWasserstein\u91cd\u5fc3\u7684\u8ba1\u7b97\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5f25\u8865\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "keywords": "\u6700\u4f18\u4f20\u8f93, Wasserstein\u91cd\u5fc3, \u5f52\u4e00\u5316\u6d41, \u9ad8\u7ef4\u7a7a\u95f4, \u68af\u5ea6\u6700\u5c0f\u5316"}}
{"id": "2505.22608", "pdf": "https://arxiv.org/pdf/2505.22608", "abs": "https://arxiv.org/abs/2505.22608", "authors": ["Haoning Xu", "Zhaoqing Li", "Youjun Chen", "Huimeng Wang", "Guinan Li", "Mengzhe Geng", "Chengxi Deng", "Xunying Liu"], "title": "Effective and Efficient One-pass Compression of Speech Foundation Models Using Sparsity-aware Self-pinching Gates", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Submitted to Interspeech 2025", "summary": "This paper presents a novel approach for speech foundation models compression\nthat tightly integrates model pruning and parameter update into a single stage.\nHighly compact layer-level tied self-pinching gates each containing only a\nsingle learnable threshold are jointly trained with uncompressed models and\nused in fine-grained neuron level pruning. Experiments conducted on the\nLibriSpeech-100hr corpus suggest that our approach reduces the number of\nparameters of wav2vec2.0-base and HuBERT-large models by 65% and 60%\nrespectively, while incurring no statistically significant word error rate\n(WER) increase on the test-clean dataset. Compared to previously published\nmethods on the same task, our approach not only achieves the lowest WER of\n7.05% on the test-clean dataset under a comparable model compression ratio of\n4.26x, but also operates with at least 25% less model compression time.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u9636\u6bb5\u96c6\u6210\u6a21\u578b\u526a\u679d\u548c\u53c2\u6570\u66f4\u65b0\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u91cf\uff08wav2vec2.0-base\u964d\u4f4e65%\uff0cHuBERT-large\u964d\u4f4e60%\uff09\uff0c\u4e14\u672a\u663e\u8457\u589e\u52a0\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u3002", "motivation": "\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u538b\u7f29\u662f\u63d0\u9ad8\u5176\u90e8\u7f72\u6548\u7387\u7684\u5173\u952e\uff0c\u5c24\u5176\u9700\u8981\u517c\u987e\u53c2\u6570\u91cf\u51cf\u5c11\u4e0e\u6027\u80fd\u4fdd\u6301\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u9636\u6bb5\u5904\u7406\u6548\u7387\u4f4e\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u5c42\u7ea7\u522b\u7684\u81ea\u7d27\u95e8\u673a\u5236\uff08\u542b\u5355\u4e00\u53ef\u5b66\u4e60\u9608\u503c\uff09\uff0c\u4e0e\u672a\u538b\u7f29\u6a21\u578b\u8054\u5408\u8bad\u7ec3\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u795e\u7ecf\u5143\u526a\u679d\u3002", "result": "\u5728LibriSpeech-100hr\u4e0a\uff0cwav2vec2.0-base\u548cHuBERT-large\u53c2\u6570\u91cf\u5206\u522b\u51cf\u5c1165%\u548c60%\uff0cWER\u65e0\u663e\u8457\u4e0a\u5347\uff08test-clean\u6570\u636e\u96c6WER\u4ec57.05%\uff09\uff0c\u538b\u7f29\u65f6\u95f4\u51cf\u5c1125%\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u9ad8\u6548\u538b\u7f29\u7684\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u573a\u666f\u3002", "keywords": "\u8bed\u97f3\u57fa\u7840\u6a21\u578b, \u6a21\u578b\u538b\u7f29, \u81ea\u7d27\u95e8, \u526a\u679d, \u8bcd\u9519\u8bef\u7387"}}
{"id": "2505.22454", "pdf": "https://arxiv.org/pdf/2505.22454", "abs": "https://arxiv.org/abs/2505.22454", "authors": ["Mark Danza", "Sonia Lopez Alarcon", "Cory Merkel"], "title": "Depth-Based Matrix Classification for the HHL Quantum Algorithm", "categories": ["quant-ph", "cs.LG"], "comment": null, "summary": "Under the nearing error-corrected era of quantum computing, it is necessary\nto understand the suitability of certain post-NISQ algorithms for practical\nproblems. One of the most promising, applicable and yet difficult to implement\nin practical terms is the Harrow, Hassidim and Lloyd (HHL) algorithm for linear\nsystems of equations. An enormous number of problems can be expressed as linear\nsystems of equations, from Machine Learning to fluid dynamics. However, in most\ncases, HHL will not be able to provide a practical, reasonable solution to\nthese problems. This paper's goal inquires about whether problems can be\nlabeled using Machine Learning classifiers as suitable or unsuitable for HHL\nimplementation when some numerical information about the problem is known\nbeforehand. This work demonstrates that training on significantly\nrepresentative data distributions is critical to achieve good classifications\nof the problems based on the numerical properties of the matrix representing\nthe system of equations. Accurate classification is possible through\nMulti-Layer Perceptrons, although with careful design of the training data\ndistribution and classifier parameters.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u91cf\u5b50\u8ba1\u7b97\u63a5\u8fd1\u7ea0\u9519\u65f6\u4ee3\u65f6\uff0c\u5982\u4f55\u5229\u7528\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u5224\u65adHHL\u7b97\u6cd5\u662f\u5426\u9002\u7528\u4e8e\u7279\u5b9a\u7ebf\u6027\u65b9\u7a0b\u7ec4\u95ee\u9898\u3002\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u591a\u5c42\u611f\u77e5\u5668\u53ef\u4ee5\u5b9e\u73b0\u51c6\u786e\u5206\u7c7b\uff0c\u4f46\u9700\u6ce8\u610f\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u548c\u5206\u7c7b\u5668\u53c2\u6570\u7684\u7cbe\u5fc3\u8bbe\u8ba1\u3002", "motivation": "\u5728\u91cf\u5b50\u8ba1\u7b97\u7684\u540eNISQ\u65f6\u4ee3\uff0cHHL\u7b97\u6cd5\u867d\u5728\u7406\u8bba\u4e0a\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u8bf8\u591a\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u9884\u5148\u5224\u65ad\u95ee\u9898\u662f\u5426\u9002\u5408HHL\u5b9e\u73b0\uff0c\u4ee5\u63d0\u9ad8\u7b97\u6cd5\u7684\u5b9e\u9645\u9002\u7528\u6027\u3002", "method": "\u5229\u7528\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u5206\u7c7b\u5668\uff0c\u57fa\u4e8e\u7ebf\u6027\u65b9\u7a0b\u7ec4\u77e9\u9635\u7684\u6570\u503c\u7279\u6027\uff0c\u5bf9\u95ee\u9898\u8fdb\u884c\u5206\u7c7b\u3002\u7814\u7a76\u5f3a\u8c03\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u7684\u663e\u8457\u4ee3\u8868\u6027\u548c\u5206\u7c7b\u5668\u53c2\u6570\u7684\u5408\u7406\u8bbe\u8ba1\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u548c\u5206\u7c7b\u5668\u53c2\u6570\uff0c\u591a\u5c42\u611f\u77e5\u5668\u53ef\u4ee5\u51c6\u786e\u5206\u7c7b\u95ee\u9898\u662f\u5426\u9002\u5408HHL\u5b9e\u73b0\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u673a\u5668\u5b66\u4e60\u5728\u5224\u65adHHL\u7b97\u6cd5\u9002\u7528\u6027\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5f3a\u8c03\u4e86\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u548c\u5206\u7c7b\u5668\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002", "keywords": "\u91cf\u5b50\u8ba1\u7b97, HHL\u7b97\u6cd5, \u673a\u5668\u5b66\u4e60, \u591a\u5c42\u611f\u77e5\u5668, \u7ebf\u6027\u65b9\u7a0b\u7ec4"}}
{"id": "2505.22626", "pdf": "https://arxiv.org/pdf/2505.22626", "abs": "https://arxiv.org/abs/2505.22626", "authors": ["Yu Zhang", "Yuqi Xie", "Huihan Liu", "Rutav Shah", "Michael Wan", "Linxi Fan", "Yuke Zhu"], "title": "SCIZOR: A Self-Supervised Approach to Data Curation for Large-Scale Imitation Learning", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Imitation learning advances robot capabilities by enabling the acquisition of\ndiverse behaviors from human demonstrations. However, large-scale datasets used\nfor policy training often introduce substantial variability in quality, which\ncan negatively impact performance. As a result, automatically curating datasets\nby filtering low-quality samples to improve quality becomes essential. Existing\nrobotic curation approaches rely on costly manual annotations and perform\ncuration at a coarse granularity, such as the dataset or trajectory level,\nfailing to account for the quality of individual state-action pairs. To address\nthis, we introduce SCIZOR, a self-supervised data curation framework that\nfilters out low-quality state-action pairs to improve the performance of\nimitation learning policies. SCIZOR targets two complementary sources of\nlow-quality data: suboptimal data, which hinders learning with undesirable\nactions, and redundant data, which dilutes training with repetitive patterns.\nSCIZOR leverages a self-supervised task progress predictor for suboptimal data\nto remove samples lacking task progression, and a deduplication module\noperating on joint state-action representation for samples with redundant\npatterns. Empirically, we show that SCIZOR enables imitation learning policies\nto achieve higher performance with less data, yielding an average improvement\nof 15.4% across multiple benchmarks. More information is available at:\nhttps://ut-austin-rpl.github.io/SCIZOR/", "AI": {"tldr": "SCIZOR\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u6570\u636e\u7b5b\u9009\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u6a21\u4eff\u5b66\u4e60\u6548\u679c\uff0c\u53bb\u9664\u4f4e\u8d28\u91cf\u72b6\u6001-\u52a8\u4f5c\u5bf9\uff0c\u5e73\u5747\u63d0\u534715.4%\u6027\u80fd\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u4f9d\u8d56\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u4f46\u6570\u636e\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\uff0c\u5f71\u54cd\u6027\u80fd\uff0c\u9700\u8981\u81ea\u52a8\u7b5b\u9009\u4f4e\u8d28\u91cf\u6837\u672c\u3002", "method": "SCIZOR\u6846\u67b6\u5229\u7528\u4efb\u52a1\u8fdb\u5ea6\u9884\u6d4b\u5668\u7b5b\u9009\u6b21\u4f18\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u53bb\u91cd\u6a21\u5757\u5904\u7406\u5197\u4f59\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cSCIZOR\u63d0\u5347\u4e86\u6a21\u4eff\u5b66\u4e60\u6027\u80fd\uff0c\u5e73\u5747\u6539\u8fdb15.4%\u3002", "conclusion": "SCIZOR\u80fd\u6709\u6548\u7b5b\u9009\u6570\u636e\uff0c\u63d0\u5347\u6a21\u4eff\u5b66\u4e60\u6548\u679c\u3002", "keywords": "\u6a21\u4eff\u5b66\u4e60,\u6570\u636e\u7b5b\u9009,\u81ea\u76d1\u7763\u5b66\u4e60,\u673a\u5668\u4eba\u5b66\u4e60,SCIZOR"}}
{"id": "2505.22469", "pdf": "https://arxiv.org/pdf/2505.22469", "abs": "https://arxiv.org/abs/2505.22469", "authors": ["Mohamed R. Elshamy", "Mehdi Elahi", "Ahmad Patooghy", "Abdel-Hameed A. Badawy"], "title": "CPINN-ABPI: Physics-Informed Neural Networks for Accurate Power Estimation in MPSoCs", "categories": ["cs.PF", "cs.LG"], "comment": null, "summary": "Efficient thermal and power management in modern multiprocessor\nsystems-on-chip (MPSoCs) demands accurate power consumption estimation. One of\nthe state-of-the-art approaches, Alternative Blind Power Identification (ABPI),\ntheoretically eliminates the dependence on steady-state temperatures,\naddressing a major shortcoming of previous approaches. However, ABPI\nperformance has remained unverified in actual hardware implementations. In this\nstudy, we conduct the first empirical validation of ABPI on commercial hardware\nusing the NVIDIA Jetson Xavier AGX platform. Our findings reveal that, while\nABPI provides computational efficiency and independence from steady-state\ntemperature, it exhibits considerable accuracy deficiencies in real-world\nscenarios. To overcome these limitations, we introduce a novel approach that\nintegrates Custom Physics-Informed Neural Networks (CPINNs) with the underlying\nthermal model of ABPI. Our approach employs a specialized loss function that\nharmonizes physical principles with data-driven learning, complemented by\nmulti-objective genetic algorithm optimization to balance estimation accuracy\nand computational cost. In experimental validation, CPINN-ABPI achieves a\nreduction of 84.7\\% CPU and 73.9\\% GPU in the mean absolute error (MAE)\nrelative to ABPI, with the weighted mean absolute percentage error (WMAPE)\nimproving from 47\\%--81\\% to $\\sim$12\\%. The method maintains real-time\nperformance with 195.3~$\\mu$s of inference time, with similar 85\\%--99\\%\naccuracy gains across heterogeneous SoCs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86ABPI\u5728\u5b9e\u9645\u786c\u4ef6\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5176\u5b58\u5728\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CPINNs\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u529f\u7387\u4f30\u8ba1\u7684\u7cbe\u5ea6\u5e76\u4fdd\u6301\u4e86\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u591a\u5904\u7406\u5668\u7cfb\u7edf\u82af\u7247\uff08MPSoCs\uff09\u9700\u8981\u7cbe\u786e\u7684\u529f\u8017\u4f30\u8ba1\u3002\u5c3d\u7ba1ABPI\u65b9\u6cd5\u7406\u8bba\u4e0a\u89e3\u51b3\u4e86\u5bf9\u7a33\u6001\u6e29\u5ea6\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u4f46\u5176\u5b9e\u9645\u786c\u4ef6\u6027\u80fd\u5c1a\u672a\u9a8c\u8bc1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CPINNs\u548cABPI\u70ed\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e13\u7528\u635f\u5931\u51fd\u6570\u5e73\u8861\u7269\u7406\u539f\u7406\u4e0e\u6570\u636e\u9a71\u52a8\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u591a\u76ee\u6807\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6210\u672c\u3002", "result": "CPINN-ABPI\u65b9\u6cd5\u5c06CPU\u548cGPU\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u5206\u522b\u964d\u4f4e\u4e8684.7\uff05\u548c73.9\uff05\uff0c\u52a0\u6743\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\uff08WMAPE\uff09\u4ece47\uff05-81\uff05\u964d\u81f3\u7ea612\uff05\uff0c\u63a8\u7406\u65f6\u95f4\u4e3a195.3\u5fae\u79d2\u3002", "conclusion": "\u65b0\u65b9\u6cd5CPINN-ABPI\u663e\u8457\u63d0\u5347\u4e86ABPI\u7684\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u5f02\u6784SoC\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9e\u65f6\u6027\u80fd\u3002", "keywords": "\u529f\u8017\u4f30\u8ba1, MPSoCs, ABPI, CPINNs, \u9057\u4f20\u7b97\u6cd5"}}
{"id": "2505.22481", "pdf": "https://arxiv.org/pdf/2505.22481", "abs": "https://arxiv.org/abs/2505.22481", "authors": ["Yiming Xi", "Konstantinos Zygalakis", "Marcelo Pereyra"], "title": "Hypothesis Testing in Imaging Inverse Problems", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "This paper proposes a framework for semantic hypothesis testing tailored to\nimaging inverse problems. Modern imaging methods struggle to support hypothesis\ntesting, a core component of the scientific method that is essential for the\nrigorous interpretation of experiments and robust interfacing with\ndecision-making processes. There are three main reasons why image-based\nhypothesis testing is challenging. First, the difficulty of using a single\nobservation to simultaneously reconstruct an image, formulate hypotheses, and\nquantify their statistical significance. Second, the hypotheses encountered in\nimaging are mostly of semantic nature, rather than quantitative statements\nabout pixel values. Third, it is challenging to control test error\nprobabilities because the null and alternative distributions are often unknown.\nOur proposed approach addresses these difficulties by leveraging concepts from\nself-supervised computational imaging, vision-language models, and\nnon-parametric hypothesis testing with e-values. We demonstrate our proposed\nframework through numerical experiments related to image-based phenotyping,\nwhere we achieve excellent power while robustly controlling Type I errors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6210\u50cf\u9006\u95ee\u9898\u7684\u8bed\u4e49\u5047\u8bbe\u68c0\u9a8c\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u4ee3\u6210\u50cf\u65b9\u6cd5\u5728\u5047\u8bbe\u68c0\u9a8c\uff08\u79d1\u5b66\u65b9\u6cd5\u6838\u5fc3\uff09\u4e2d\u7684\u5e94\u7528\u96be\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u81ea\u76d1\u7763\u8ba1\u7b97\u6210\u50cf\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u975e\u53c2\u6570\u5047\u8bbe\u68c0\u9a8c\uff0c\u5b9e\u73b0\u4e86\u5728\u9ad8\u7edf\u8ba1\u529f\u6548\u4e0b\u63a7\u5236\u7b2c\u4e00\u7c7b\u9519\u8bef\u3002", "motivation": "\u73b0\u4ee3\u6210\u50cf\u65b9\u6cd5\u5728\u652f\u6301\u5047\u8bbe\u68c0\u9a8c\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u79d1\u5b66\u5b9e\u9a8c\u7684\u4e25\u683c\u89e3\u91ca\u548c\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u3002\u4e3b\u8981\u539f\u56e0\u5305\u62ec\u5355\u6b21\u89c2\u6d4b\u96be\u4ee5\u540c\u65f6\u91cd\u5efa\u56fe\u50cf\u3001\u63d0\u51fa\u5047\u8bbe\u5e76\u91cf\u5316\u7edf\u8ba1\u663e\u8457\u6027\u3001\u5047\u8bbe\u591a\u4e3a\u8bed\u4e49\u6027\u8d28\u800c\u975e\u50cf\u7d20\u503c\u5b9a\u91cf\u63cf\u8ff0\u3001\u4ee5\u53ca\u96be\u4ee5\u63a7\u5236\u68c0\u9a8c\u9519\u8bef\u6982\u7387\u3002", "method": "\u7ed3\u5408\u81ea\u76d1\u7763\u8ba1\u7b97\u6210\u50cf\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u975e\u53c2\u6570\u5047\u8bbe\u68c0\u9a8c\uff08e\u503c\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u5047\u8bbe\u68c0\u9a8c\u6846\u67b6\u3002", "result": "\u5728\u56fe\u50cf\u8868\u578b\u76f8\u5173\u7684\u6570\u503c\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u8272\uff0c\u4fdd\u6301\u4e86\u9ad8\u7edf\u8ba1\u529f\u6548\u5e76\u7a33\u5065\u5730\u63a7\u5236\u4e86\u7b2c\u4e00\u7c7b\u9519\u8bef\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6210\u50cf\u9006\u95ee\u9898\u4e2d\u7684\u8bed\u4e49\u5047\u8bbe\u68c0\u9a8c\u96be\u9898\uff0c\u4e3a\u79d1\u5b66\u5b9e\u9a8c\u7684\u89e3\u91ca\u548c\u51b3\u7b56\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u7edf\u8ba1\u652f\u6301\u3002", "keywords": "\u8bed\u4e49\u5047\u8bbe\u68c0\u9a8c\u3001\u6210\u50cf\u9006\u95ee\u9898\u3001\u81ea\u76d1\u7763\u8ba1\u7b97\u6210\u50cf\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u975e\u53c2\u6570\u68c0\u9a8c"}}
{"id": "2505.22502", "pdf": "https://arxiv.org/pdf/2505.22502", "abs": "https://arxiv.org/abs/2505.22502", "authors": ["Dominic Lowe", "M. S. Kim", "Roberto Bondesan"], "title": "Assessing Quantum Advantage for Gaussian Process Regression", "categories": ["quant-ph", "cs.LG"], "comment": "18 pages, 2 figures", "summary": "Gaussian Process Regression is a well-known machine learning technique for\nwhich several quantum algorithms have been proposed. We show here that in a\nwide range of scenarios these algorithms show no exponential speedup. We\nachieve this by rigorously proving that the condition number of a kernel matrix\nscales at least linearly with the matrix size under general assumptions on the\ndata and kernel. We additionally prove that the sparsity and Frobenius norm of\na kernel matrix scale linearly under similar assumptions. The implications for\nthe quantum algorithms runtime are independent of the complexity of loading\nclassical data on a quantum computer and also apply to dequantised algorithms.\nWe supplement our theoretical analysis with numerical verification for popular\nkernels in machine learning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5728\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u7684\u591a\u79cd\u573a\u666f\u4e2d\uff0c\u63d0\u51fa\u7684\u91cf\u5b50\u7b97\u6cd5\u672a\u5c55\u73b0\u6307\u6570\u7ea7\u52a0\u901f\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u6570\u503c\u9a8c\u8bc1\u652f\u6301\u8fd9\u4e00\u7ed3\u8bba\u3002", "motivation": "\u63a2\u7d22\u91cf\u5b50\u7b97\u6cd5\u5728\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u4e2d\u7684\u6f5c\u5728\u52a0\u901f\u80fd\u529b\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u901a\u8fc7\u4e25\u683c\u8bc1\u660e\u6838\u77e9\u9635\u7684\u6761\u4ef6\u6570\u3001\u7a00\u758f\u6027\u548cFrobenius\u8303\u6570\u5728\u4e00\u822c\u5047\u8bbe\u4e0b\u7684\u7ebf\u6027\u7f29\u653e\u7279\u6027\uff0c\u5e76\u8fdb\u884c\u6570\u503c\u9a8c\u8bc1\u3002", "result": "\u5728\u5e7f\u6cdb\u573a\u666f\u4e0b\uff0c\u91cf\u5b50\u7b97\u6cd5\u672a\u5b9e\u73b0\u6307\u6570\u7ea7\u52a0\u901f\uff0c\u6838\u77e9\u9635\u7684\u7279\u6027\u9650\u5236\u4e86\u7b97\u6cd5\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u91cf\u5b50\u7b97\u6cd5\u5728\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u4e2d\u7684\u52a0\u901f\u6548\u679c\u6709\u9650\uff0c\u53d7\u9650\u4e8e\u6838\u77e9\u9635\u7684\u57fa\u672c\u7279\u6027\u3002", "keywords": "\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52, \u91cf\u5b50\u7b97\u6cd5, \u6761\u4ef6\u6570, \u6838\u77e9\u9635, \u7a00\u758f\u6027"}}
{"id": "2505.22642", "pdf": "https://arxiv.org/pdf/2505.22642", "abs": "https://arxiv.org/abs/2505.22642", "authors": ["Younggyo Seo", "Carmelo Sferrazza", "Haoran Geng", "Michal Nauman", "Zhao-Heng Yin", "Pieter Abbeel"], "title": "FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid Control", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Project webpage: https://younggyo.me/fast_td3", "summary": "Reinforcement learning (RL) has driven significant progress in robotics, but\nits complexity and long training times remain major bottlenecks. In this\nreport, we introduce FastTD3, a simple, fast, and capable RL algorithm that\nsignificantly speeds up training for humanoid robots in popular suites such as\nHumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably\nsimple: we train an off-policy TD3 agent with several modifications -- parallel\nsimulation, large-batch updates, a distributional critic, and carefully tuned\nhyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours\non a single A100 GPU, while remaining stable during training. We also provide a\nlightweight and easy-to-use implementation of FastTD3 to accelerate RL research\nin robotics.", "AI": {"tldr": "FastTD3\u662f\u4e00\u79cd\u5feb\u901f\u3001\u7b80\u5355\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u80fd\u663e\u8457\u52a0\u5feb\u4eba\u5f62\u673a\u5668\u4eba\u5728\u591a\u79cd\u73af\u5883\u4e2d\u7684\u8bad\u7ec3\u901f\u5ea6\uff0c\u4ec5\u97003\u5c0f\u65f6\u5373\u53ef\u5b8c\u6210\u8bad\u7ec3\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u590d\u6742\u6027\u548c\u957f\u8bad\u7ec3\u65f6\u95f4\u4ecd\u662f\u4e3b\u8981\u74f6\u9888\u3002FastTD3\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u6539\u8fdb\u7684TD3\u7b97\u6cd5\uff0c\u5305\u62ec\u5e76\u884c\u6a21\u62df\u3001\u5927\u6279\u91cf\u66f4\u65b0\u3001\u5206\u5e03\u5316\u8bc4\u8bba\u5bb6\u548c\u7cbe\u5fc3\u8c03\u6574\u7684\u8d85\u53c2\u6570\u3002", "result": "\u5728HumanoidBench\u4efb\u52a1\u4e2d\uff0cFastTD3\u5355A100 GPU\u4e0b3\u5c0f\u65f6\u5185\u5b8c\u6210\u8bad\u7ec3\uff0c\u4e14\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "conclusion": "FastTD3\u4e3a\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "FastTD3, \u5f3a\u5316\u5b66\u4e60, \u673a\u5668\u4eba, \u8bad\u7ec3\u52a0\u901f"}}
{"id": "2505.22518", "pdf": "https://arxiv.org/pdf/2505.22518", "abs": "https://arxiv.org/abs/2505.22518", "authors": ["Agnideep Aich", "Ashit Baran Aich", "Bruce Wade"], "title": "IGNIS: A Neural Network Framework for Robust Parameter Estimation in Archimedean Copulas", "categories": ["stat.ML", "cs.LG", "62H05, 62H12, 62F10, 68T07, 62-08"], "comment": "Under review", "summary": "Parameter estimation for Archimedean copulas remains a challenging problem,\nparticularly for the recently developed A1 and A2 families that exhibit complex\ndependency structures. Traditional methods, such as the Method of Moments\n(MoM), Maximum Likelihood Estimation (MLE), and Maximum Pseudo-Likelihood\n(MPL), often struggle due to issues of non-monotonic relationship with\ndependency measures such as Kendall's tau (as in the case of A1) and numerical\ninstability. In this paper, we present the IGNIS Network, a novel, unified\nneural framework that learns a direct mapping from observable dependency\nmeasures to copula parameters, thereby overcoming the limitations of classical\napproaches. Our approach is trained on simulated data spanning five Archimedean\ncopula families including Clayton, Gumbel, Frank, A1, and A2, ensuring its\ngeneral applicability across the entire family. Extensive simulation studies\ndemonstrate that the IGNIS Network reduces estimation errors compared to MoM,\nwhile inherently enforcing parameter constraints through theory-guided\npost-processing. We further validate the practical utility of our method on\ndiverse real-world datasets, including financial returns (AAPL-MSFT),\nhealthcare metrics (CDC Diabetes indicators), and environmental measurements\n(PM2.5 air quality). Our results underscore the transformative potential of\nneural methods for robust and accurate dependence modeling in modern\napplications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIGNIS Network\u7684\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3Archimedean copula\u7684\u53c2\u6570\u4f30\u8ba1\u96be\u9898\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u4f9d\u8d56\u7ed3\u6784\u7684A1\u548cA2\u5bb6\u65cf\u4e2d\u3002\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\uff0cIGNIS\u901a\u8fc7\u76f4\u63a5\u4ece\u89c2\u6d4b\u6570\u636e\u5b66\u4e60\u53c2\u6570\u6620\u5c04\uff0c\u51cf\u5c11\u4e86\u4f30\u8ba1\u8bef\u5dee\u5e76\u589e\u5f3a\u4e86\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u77e9\u4f30\u8ba1\u3001\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\uff09\u5728\u5904\u7406Archimedean copula\uff08\u5c24\u5176\u662fA1\u548cA2\u5bb6\u65cf\uff09\u65f6\u5b58\u5728\u975e\u5355\u8c03\u6027\u6216\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u9c81\u68d2\u4e14\u666e\u9002\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86IGNIS Network\uff0c\u4e00\u79cd\u901a\u8fc7\u6a21\u62df\u6570\u636e\u8bad\u7ec3\u7684\u7edf\u4e00\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u4f9d\u8d56\u5ea6\u91cf\uff08\u5982Kendall's tau\uff09\u6620\u5c04\u5230copula\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u6307\u5bfc\u7684\u540e\u5904\u7406\u5f3a\u5236\u53c2\u6570\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cIGNIS\u5728\u6a21\u62df\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u96c6\uff08\u91d1\u878d\u3001\u533b\u7597\u3001\u73af\u5883\uff09\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4f30\u8ba1\u8bef\u5dee\u3002", "conclusion": "IGNIS Network\u5c55\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u5728\u4f9d\u8d56\u5efa\u6a21\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u73b0\u4ee3\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u51c6\u786e\u3001\u7a33\u5065\u7684\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\u3002", "keywords": "Archimedean copula, parameter estimation, neural network, dependency modeling, IGNIS"}}
{"id": "2505.22649", "pdf": "https://arxiv.org/pdf/2505.22649", "abs": "https://arxiv.org/abs/2505.22649", "authors": ["Guoxuan Chen", "Lianghao Xia", "Chao Huang"], "title": "Pre-training for Recommendation Unlearning", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": "Accepted to SIGIR 2025 Oral", "summary": "Modern recommender systems powered by Graph Neural Networks (GNNs) excel at\nmodeling complex user-item interactions, yet increasingly face scenarios\nrequiring selective forgetting of training data. Beyond user requests to remove\nspecific interactions due to privacy concerns or preference changes, regulatory\nframeworks mandate recommender systems' ability to eliminate the influence of\ncertain user data from models. This recommendation unlearning challenge\npresents unique difficulties as removing connections within interaction graphs\ncreates ripple effects throughout the model, potentially impacting\nrecommendations for numerous users. Traditional approaches suffer from\nsignificant drawbacks: fragmentation methods damage graph structure and\ndiminish performance, while influence function techniques make assumptions that\nmay not hold in complex GNNs, particularly with self-supervised or random\narchitectures. To address these limitations, we propose a novel model-agnostic\npre-training paradigm UnlearnRec that prepares systems for efficient unlearning\noperations. Our Influence Encoder takes unlearning requests together with\nexisting model parameters and directly produces updated parameters of unlearned\nmodel with little fine-tuning, avoiding complete retraining while preserving\nmodel performance characteristics. Extensive evaluation on public benchmarks\ndemonstrates that our method delivers exceptional unlearning effectiveness\nwhile providing more than 10x speedup compared to retraining approaches. We\nrelease our method implementation at: https://github.com/HKUDS/UnlearnRec.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUnlearnRec\u7684\u65b0\u578b\u9884\u8bad\u7ec3\u8303\u5f0f\uff0c\u65e8\u5728\u89e3\u51b3GNN\u63a8\u8350\u7cfb\u7edf\u4e2d\u9009\u62e9\u6027\u9057\u5fd8\u8bad\u7ec3\u6570\u636e\u7684\u6311\u6218\uff0c\u907f\u514d\u4e86\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u7684\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u57fa\u4e8eGNN\u7684\u63a8\u8350\u7cfb\u7edf\u5728\u9762\u5bf9\u7528\u6237\u9690\u79c1\u6216\u504f\u597d\u53d8\u5316\u65f6\uff0c\u9700\u8981\u9ad8\u6548\u79fb\u9664\u7279\u5b9a\u6570\u636e\u5f71\u54cd\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u6027\u80fd\u635f\u5931\u6216\u5047\u8bbe\u4e0d\u6210\u7acb\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6a21\u578b\u65e0\u5173\u7684\u9884\u8bad\u7ec3\u8303\u5f0fUnlearnRec\uff0c\u5229\u7528Influence Encoder\u76f4\u63a5\u751f\u6210\u9057\u5fd8\u8bf7\u6c42\u540e\u7684\u6a21\u578b\u53c2\u6570\uff0c\u51cf\u5c11\u5fae\u8c03\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u516c\u5f00\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc710\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4f18\u5f02\u7684\u9057\u5fd8\u6548\u679c\u3002", "conclusion": "UnlearnRec\u4e3a\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u9057\u5fd8\u673a\u5236\uff0c\u5e73\u8861\u4e86\u6027\u80fd\u548c\u6548\u7387\u3002", "keywords": "GNN, \u63a8\u8350\u7cfb\u7edf, \u6570\u636e\u9057\u5fd8, \u9884\u8bad\u7ec3, Influence Encoder"}}
{"id": "2505.22527", "pdf": "https://arxiv.org/pdf/2505.22527", "abs": "https://arxiv.org/abs/2505.22527", "authors": ["Agnideep Aich", "Ashit Aich", "Bruce Wade"], "title": "Symplectic Generative Networks (SGNs): A Hamiltonian Framework for Invertible Deep Generative Modeling", "categories": ["stat.ML", "cs.LG", "68T07, 37J39, 65P10, 62B10, 53D22, 94A17"], "comment": "Submitted", "summary": "We introduce the Symplectic Generative Network (SGN), a deep generative model\nthat leverages Hamiltonian mechanics to construct an invertible,\nvolume-preserving mapping between a latent space and the data space. By\nendowing the latent space with a symplectic structure and modeling data\ngeneration as the time evolution of a Hamiltonian system, SGN achieves exact\nlikelihood evaluation without incurring the computational overhead of Jacobian\ndeterminant calculations. In this work, we provide a rigorous mathematical\nfoundation for SGNs through a comprehensive theoretical framework that\nincludes: (i) complete proofs of invertibility and volume preservation, (ii) a\nformal complexity analysis with theoretical comparisons to Variational\nAutoencoders and Normalizing Flows, (iii) strengthened universal approximation\nresults with quantitative error bounds, (iv) an information-theoretic analysis\nbased on the geometry of statistical manifolds, and (v) an extensive stability\nanalysis with adaptive integration guarantees. These contributions highlight\nthe fundamental advantages of SGNs and establish a solid foundation for future\nempirical investigations and applications to complex, high-dimensional data.", "AI": {"tldr": "\u603b\u7ed3\u751f\u6210\u7f51\u7edc\uff08SGN\uff09\u662f\u4e00\u79cd\u5229\u7528\u54c8\u5bc6\u987f\u529b\u5b66\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u6f5c\u5728\u7a7a\u95f4\u4e0e\u6570\u636e\u7a7a\u95f4\u7684\u53ef\u9006\u3001\u4fdd\u4f53\u79ef\u6620\u5c04\uff0c\u65e0\u9700\u8ba1\u7b97\u96c5\u53ef\u6bd4\u884c\u5217\u5f0f\u5373\u53ef\u7cbe\u786e\u8bc4\u4f30\u4f3c\u7136\u3002", "motivation": "\u901a\u8fc7\u5f15\u5165\u54c8\u5bc6\u987f\u529b\u5b66\uff0c\u6784\u5efa\u9ad8\u6548\u4e14\u6570\u5b66\u4e0a\u4e25\u8c28\u7684\u751f\u6210\u6a21\u578b\uff0c\u514b\u670d\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u6807\u51c6\u5316\u6d41\uff09\u7684\u8ba1\u7b97\u590d\u6742\u6027\u3002", "method": "\u91c7\u7528\u54c8\u5bc6\u987f\u7cfb\u7edf\u7684\u65f6\u95f4\u6f14\u5316\u6a21\u578b\uff0c\u8d4b\u4e88\u6f5c\u5728\u7a7a\u95f4\u8f9b\u7ed3\u6784\uff0c\u5e76\u8fdb\u884c\u5b8c\u6574\u7684\u7406\u8bba\u5206\u6790\uff0c\u5305\u62ec\u53ef\u9006\u6027\u8bc1\u660e\u3001\u590d\u6742\u5ea6\u5bf9\u6bd4\u3001\u8fd1\u4f3c\u8bef\u5dee\u91cf\u5316\u7b49\u3002", "result": "SGN\u5728\u7406\u8bba\u4e0a\u5b9e\u73b0\u4e86\u7cbe\u786e\u4f3c\u7136\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7\u6570\u5b66\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "SGN\u4e3a\u590d\u6742\u9ad8\u7ef4\u6570\u636e\u7684\u751f\u6210\u5efa\u6a21\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5b9e\u9645\u5e94\u7528\u3002", "keywords": "Symplectic Generative Network, Hamiltonian mechanics, exact likelihood, volume-preserving, deep generative model"}}
{"id": "2505.22535", "pdf": "https://arxiv.org/pdf/2505.22535", "abs": "https://arxiv.org/abs/2505.22535", "authors": ["Mohamad Hakam Shams Eddin", "Yikui Zhang", "Stefan Kollet", "Juergen Gall"], "title": "RiverMamba: A State Space Model for Global River Discharge and Flood Forecasting", "categories": ["cs.CV", "cs.LG"], "comment": "Main paper 10 pages, Appendix 53 pages", "summary": "Recent deep learning approaches for river discharge forecasting have improved\nthe accuracy and efficiency in flood forecasting, enabling more reliable early\nwarning systems for risk management. Nevertheless, existing deep learning\napproaches in hydrology remain largely confined to local-scale applications and\ndo not leverage the inherent spatial connections of bodies of water. Thus,\nthere is a strong need for new deep learning methodologies that are capable of\nmodeling spatio-temporal relations to improve river discharge and flood\nforecasting for scientific and operational applications. To address this, we\npresent RiverMamba, a novel deep learning model that is pretrained with\nlong-term reanalysis data and that can forecast global river discharge and\nfloods on a $0.05^\\circ$ grid up to 7 days lead time, which is of high\nrelevance in early warning. To achieve this, RiverMamba leverages efficient\nMamba blocks that enable the model to capture global-scale channel network\nrouting and enhance its forecast capability for longer lead times. The forecast\nblocks integrate ECMWF HRES meteorological forecasts, while accounting for\ntheir inaccuracies through spatio-temporal modeling. Our analysis demonstrates\nthat RiverMamba delivers reliable predictions of river discharge, including\nextreme floods across return periods and lead times, surpassing both\noperational AI- and physics-based models.", "AI": {"tldr": "\u8bba\u6587\u6458\u8981\u63d0\u51fa\u4e86RiverMamba\u8fd9\u4e00\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u9488\u5bf9\u73b0\u6709\u65b9\u6cd5\u5728\u7a7a\u95f4\u5173\u8054\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u548c\u65f6\u7a7a\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cb3\u6d41\u6d41\u91cf\u548c\u6d2a\u6c34\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u8303\u56f4\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u5728\u6cb3\u6d41\u6d41\u91cf\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u591a\u5c40\u9650\u4e8e\u5c40\u90e8\u8303\u56f4\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u6c34\u4f53\u7684\u7a7a\u95f4\u5173\u8054\u6027\uff0c\u4e9f\u9700\u65b0\u65b9\u6cd5\u6765\u63d0\u5347\u9884\u6d4b\u7684\u79d1\u5b66\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "RiverMamba\u57fa\u4e8e\u957f\u671f\u518d\u5206\u6790\u6570\u636e\u9884\u8bad\u7ec3\uff0c\u7ed3\u5408\u9ad8\u6548\u7684Mamba\u5757\u6355\u6349\u5168\u7403\u8303\u56f4\u7684\u6c34\u9053\u7f51\u7edc\u6d41\u52a8\uff0c\u878d\u5408ECMWF HRES\u6c14\u8c61\u9884\u6d4b\u5e76\u5904\u7406\u5176\u8bef\u5dee\u3002", "result": "RiverMamba\u57280.05\u00b0\u7f51\u683c\u4e0a\u5b9e\u73b0\u4e86\u957f\u8fbe7\u5929\u7684\u6cb3\u6d41\u548c\u6d2a\u6c34\u9884\u6d4b\uff0c\u5305\u62ec\u6781\u7aef\u4e8b\u4ef6\uff0c\u8868\u73b0\u4f18\u4e8e\u5df2\u6709\u7684AI\u548c\u7269\u7406\u6a21\u578b\u3002", "conclusion": "RiverMamba\u901a\u8fc7\u65f6\u7a7a\u5efa\u6a21\u548c\u5168\u7403\u8303\u56f4\u7684\u5e94\u7528\uff0c\u4e3a\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u9884\u6d4b\u5de5\u5177\u3002", "keywords": "\u6df1\u5ea6\u5b66\u4e60, \u6cb3\u6d41\u6d41\u91cf\u9884\u6d4b, \u6d2a\u6c34\u9884\u8b66, \u65f6\u7a7a\u5efa\u6a21, RiverMamba"}}
{"id": "2505.22554", "pdf": "https://arxiv.org/pdf/2505.22554", "abs": "https://arxiv.org/abs/2505.22554", "authors": ["Agnideep Aich", "Md Monzur Murshed", "Amanda Mayeaux", "Sameera Hewage"], "title": "Can Copulas Be Used for Feature Selection? A Machine Learning Study on Diabetes Risk Prediction", "categories": ["stat.ML", "cs.LG", "62H05, 62H12, 62P10, 68T07"], "comment": "Submitted", "summary": "Accurate diabetes risk prediction relies on identifying key features from\ncomplex health datasets, but conventional methods like mutual information (MI)\nfilters and genetic algorithms (GAs) often overlook extreme dependencies\ncritical for high-risk subpopulations. In this study we introduce a\nfeature-selection framework using the upper-tail dependence coefficient\n({\\lambda}U) of the novel A2 copula, which quantifies how often extreme higher\nvalues of a predictor co-occur with diabetes diagnoses (target variable).\nApplied to the CDC Diabetes Health Indicators dataset (n=253,680), our method\nprioritizes five predictors (self-reported general health, high blood pressure,\nbody mass index, mobility limitations, and high cholesterol levels) based on\nupper tail dependencies. These features match or outperform MI and GA selected\nsubsets across four classifiers (Random Forest, XGBoost, Logistic Regression,\nGradient Boosting), achieving accuracy up to 86.5% (XGBoost) and AUC up to\n0.806 (Gradient Boosting), rivaling the full 21-feature model. Permutation\nimportance confirms clinical relevance, with BMI and general health driving\naccuracy. To our knowledge, this is the first work to apply a copula's\nupper-tail dependence for supervised feature selection, bridging extreme-value\ntheory and machine learning to deliver a practical toolkit for diabetes\nprevention.", "AI": {"tldr": "\u4f7f\u7528\u65b0\u578bA2 copula\u7684\u4e0a\u5c3e\u4f9d\u8d56\u7cfb\u6570\u8fdb\u884c\u7279\u5f81\u9009\u62e9\uff0c\u63d0\u5347\u7cd6\u5c3f\u75c5\u98ce\u9669\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u4e92\u4fe1\u606f\u8fc7\u6ee4\u548c\u9057\u4f20\u7b97\u6cd5\uff09\u5e38\u5ffd\u7565\u9ad8\u98ce\u9669\u4eba\u7fa4\u7684\u6781\u7aef\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e9f\u9700\u65b0\u65b9\u6cd5\u6355\u6349\u8fd9\u7c7b\u5173\u952e\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eA2 copula\u7684\u4e0a\u5c3e\u4f9d\u8d56\u7cfb\u6570\uff08\u03bbU\uff09\u7684\u7279\u5f81\u9009\u62e9\u6846\u67b6\uff0c\u91cf\u5316\u9884\u6d4b\u53d8\u91cf\u6781\u7aef\u503c\u4e0e\u7cd6\u5c3f\u75c5\u8bca\u65ad\u7684\u5171\u73b0\u5173\u7cfb\u3002", "result": "\u5728CDC\u6570\u636e\u96c6\u4e0a\u9009\u51fa5\u4e2a\u5173\u952e\u7279\u5f81\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff08\u51c6\u786e\u738786.5%\uff0cAUC 0.806\uff09\uff0c\u5ab2\u7f8e21\u7279\u5f81\u7684\u5b8c\u6574\u6a21\u578b\u3002", "conclusion": "\u9996\u6b21\u5c06copula\u7684\u4e0a\u5c3e\u4f9d\u8d56\u7406\u8bba\u5e94\u7528\u4e8e\u76d1\u7763\u7279\u5f81\u9009\u62e9\uff0c\u4e3a\u7cd6\u5c3f\u75c5\u9884\u9632\u63d0\u4f9b\u5b9e\u7528\u5de5\u5177\u3002", "keywords": "\u7cd6\u5c3f\u75c5\u9884\u6d4b, \u7279\u5f81\u9009\u62e9, copula, \u4e0a\u5c3e\u4f9d\u8d56, \u673a\u5668\u5b66\u4e60"}}
{"id": "2505.22609", "pdf": "https://arxiv.org/pdf/2505.22609", "abs": "https://arxiv.org/abs/2505.22609", "authors": ["Alanna Hazlett", "Naomi Ohashi", "Timothy Rodriguez", "Sodiq Adewole"], "title": "Chest Disease Detection In X-Ray Images Using Deep Learning Classification Method", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "In this work, we investigate the performance across multiple classification\nmodels to classify chest X-ray images into four categories of COVID-19,\npneumonia, tuberculosis (TB), and normal cases. We leveraged transfer learning\ntechniques with state-of-the-art pre-trained Convolutional Neural Networks\n(CNNs) models. We fine-tuned these pre-trained architectures on a labeled\nmedical x-ray images. The initial results are promising with high accuracy and\nstrong performance in key classification metrics such as precision, recall, and\nF1 score. We applied Gradient-weighted Class Activation Mapping (Grad-CAM) for\nmodel interpretability to provide visual explanations for classification\ndecisions, improving trust and transparency in clinical applications.", "AI": {"tldr": "\u4f7f\u7528\u8fc1\u79fb\u5b66\u4e60\u548c\u9884\u8bad\u7ec3CNN\u6a21\u578b\u5bf9\u80f8\u90e8X\u5149\u7247\u8fdb\u884c\u56db\u5206\u7c7b\uff08COVID-19\u3001\u80ba\u708e\u3001\u7ed3\u6838\u3001\u6b63\u5e38\uff09\uff0c\u7ed3\u679c\u51c6\u786e\u7387\u9ad8\uff0c\u6a21\u578b\u89e3\u91ca\u6027\u901a\u8fc7Grad-CAM\u589e\u5f3a\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u5f71\u50cf\u4e2dCOVID-19\u3001\u80ba\u708e\u3001\u7ed3\u6838\u548c\u6b63\u5e38\u75c5\u4f8b\u7684\u5206\u7c7b\u95ee\u9898\uff0c\u63d0\u5347\u4e34\u5e8a\u5e94\u7528\u7684\u4fe1\u4efb\u548c\u900f\u660e\u5ea6\u3002", "method": "\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u6280\u672f\uff0c\u5fae\u8c03\u9884\u8bad\u7ec3\u7684CNN\u6a21\u578b\uff0c\u5e76\u7ed3\u5408Grad-CAM\u63d0\u4f9b\u89c6\u89c9\u89e3\u91ca\u3002", "result": "\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u9ad8\uff0c\u5173\u952e\u5206\u7c7b\u6307\u6807\uff08\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\uff09\u5f3a\u52b2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u533b\u5b66\u5f71\u50cf\u5206\u7c7b\u4e2d\u6709\u6548\u4e14\u53ef\u9760\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u8fc1\u79fb\u5b66\u4e60, CNN, \u533b\u5b66\u5f71\u50cf\u5206\u7c7b, COVID-19, Grad-CAM"}}
{"id": "2505.22622", "pdf": "https://arxiv.org/pdf/2505.22622", "abs": "https://arxiv.org/abs/2505.22622", "authors": ["Jiawei Ge", "Amanda Wang", "Shange Tang", "Chi Jin"], "title": "Principled Out-of-Distribution Generalization via Simplicity", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Modern foundation models exhibit remarkable out-of-distribution (OOD)\ngeneralization, solving tasks far beyond the support of their training data.\nHowever, the theoretical principles underpinning this phenomenon remain\nelusive. This paper investigates this problem by examining the compositional\ngeneralization abilities of diffusion models in image generation. Our analysis\nreveals that while neural network architectures are expressive enough to\nrepresent a wide range of models -- including many with undesirable behavior on\nOOD inputs -- the true, generalizable model that aligns with human expectations\ntypically corresponds to the simplest among those consistent with the training\ndata.\n  Motivated by this observation, we develop a theoretical framework for OOD\ngeneralization via simplicity, quantified using a predefined simplicity metric.\nWe analyze two key regimes: (1) the constant-gap setting, where the true model\nis strictly simpler than all spurious alternatives by a fixed gap, and (2) the\nvanishing-gap setting, where the fixed gap is replaced by a smoothness\ncondition ensuring that models close in simplicity to the true model yield\nsimilar predictions. For both regimes, we study the regularized maximum\nlikelihood estimator and establish the first sharp sample complexity guarantees\nfor learning the true, generalizable, simple model.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\uff0c\u53d1\u73b0\u7b80\u5355\u6027\u4e0e\u6cdb\u5316\u6027\u76f8\u5173\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u7b80\u7ea6\u5ea6\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4e3a\u5b66\u4e60\u771f\u5b9e\u3001\u53ef\u6cdb\u5316\u7684\u7b80\u5355\u6a21\u578b\u63d0\u4f9b\u4e86\u6837\u672c\u590d\u6742\u5ea6\u7684\u4fdd\u8bc1\u3002", "motivation": "\u63a2\u7a76\u73b0\u4ee3\u57fa\u7840\u6a21\u578b\u5728\u8d85\u51fa\u8bad\u7ec3\u6570\u636e\u5206\u5e03\uff08OOD\uff09\u60c5\u51b5\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u80cc\u540e\u7684\u7406\u8bba\u539f\u5219\uff0c\u5c24\u5176\u662f\u901a\u8fc7\u7814\u7a76\u6269\u6563\u6a21\u578b\u7684\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7b80\u7ea6\u5ea6\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5206\u6790\u4e86\u4e24\u79cd\u5173\u952e\u573a\u666f\uff08\u6052\u5b9a\u5dee\u8ddd\u548c\u6d88\u5931\u5dee\u8ddd\uff09\uff0c\u5e76\u7814\u7a76\u4e86\u6b63\u5219\u5316\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5668\u3002", "result": "\u4e3a\u5b66\u4e60\u771f\u5b9e\u3001\u53ef\u6cdb\u5316\u7684\u7b80\u5355\u6a21\u578b\u5efa\u7acb\u4e86\u7b2c\u4e00\u4e2a\u5c16\u9510\u7684\u6837\u672c\u590d\u6742\u5ea6\u4fdd\u8bc1\u3002", "conclusion": "\u7b80\u5355\u6027\u4e0e\u6cdb\u5316\u6027\u7d27\u5bc6\u76f8\u5173\uff0c\u7406\u8bba\u6846\u67b6\u4e3a\u7406\u89e3OOD\u6cdb\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002", "keywords": "\u6cdb\u5316\u6027, \u7b80\u7ea6\u5ea6, \u6269\u6563\u6a21\u578b, OOD, \u6837\u672c\u590d\u6742\u5ea6"}}
{"id": "2505.22638", "pdf": "https://arxiv.org/pdf/2505.22638", "abs": "https://arxiv.org/abs/2505.22638", "authors": ["Denis Donadel", "Gabriele Crestanello", "Giulio Morandini", "Daniele Antonioli", "Mauro Conti", "Massimo Merro"], "title": "SimProcess: High Fidelity Simulation of Noisy ICS Physical Processes", "categories": ["cs.CR", "cs.LG"], "comment": "In 11th ACM Cyber-Physical System Security Workshop (CPSS '25),\n  August 25-29, 2025, Hanoi, Vietnam", "summary": "Industrial Control Systems (ICS) manage critical infrastructures like power\ngrids and water treatment plants. Cyberattacks on ICSs can disrupt operations,\ncausing severe economic, environmental, and safety issues. For example,\nundetected pollution in a water plant can put the lives of thousands at stake.\nICS researchers have increasingly turned to honeypots -- decoy systems designed\nto attract attackers, study their behaviors, and eventually improve defensive\nmechanisms. However, existing ICS honeypots struggle to replicate the ICS\nphysical process, making them susceptible to detection. Accurately simulating\nthe noise in ICS physical processes is challenging because different factors\nproduce it, including sensor imperfections and external interferences.\n  In this paper, we propose SimProcess, a novel framework to rank the fidelity\nof ICS simulations by evaluating how closely they resemble real-world and noisy\nphysical processes. It measures the simulation distance from a target system by\nestimating the noise distribution with machine learning models like Random\nForest. Unlike existing solutions that require detailed mathematical models or\nare limited to simple systems, SimProcess operates with only a timeseries of\nmeasurements from the real system, making it applicable to a broader range of\ncomplex dynamic systems. We demonstrate the framework's effectiveness through a\ncase study using real-world power grid data from the EPIC testbed. We compare\nthe performance of various simulation methods, including static and generative\nnoise techniques. Our model correctly classifies real samples with a recall of\nup to 1.0. It also identifies Gaussian and Gaussian Mixture as the best\ndistribution to simulate our power systems, together with a generative solution\nprovided by an autoencoder, thereby helping developers to improve honeypot\nfidelity. Additionally, we make our code publicly available.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86SimProcess\u6846\u67b6\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bc4\u4f30ICS\u6a21\u62df\u7684\u771f\u5b9e\u6027\uff0c\u4ee5\u63d0\u5347\u871c\u7f50\u7684\u903c\u771f\u5ea6\uff0c\u9002\u7528\u4e8e\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u3002", "motivation": "ICS\u871c\u7f50\u96be\u4ee5\u51c6\u786e\u6a21\u62df\u7269\u7406\u8fc7\u7a0b\u566a\u58f0\uff0c\u5bfc\u81f4\u6613\u88ab\u653b\u51fb\u8005\u8bc6\u7834\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u8bc4\u4f30\u6a21\u62df\u903c\u771f\u5ea6\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u968f\u673a\u68ee\u6797\u7b49\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4f30\u8ba1\u566a\u58f0\u5206\u5e03\uff0c\u4ec5\u9700\u771f\u5b9e\u7cfb\u7edf\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5373\u53ef\u5de5\u4f5c\u3002", "result": "\u5728\u7535\u529b\u7f51\u683c\u6570\u636e\u6848\u4f8b\u4e2d\uff0c\u6a21\u578b\u53ec\u56de\u7387\u8fbe1.0\uff0c\u786e\u5b9a\u9ad8\u65af\u548c\u9ad8\u65af\u6df7\u5408\u5206\u5e03\u6700\u9002\u5408\u6a21\u62df\u7535\u529b\u7cfb\u7edf\u3002", "conclusion": "SimProcess\u80fd\u6709\u6548\u63d0\u5347\u871c\u7f50\u7684\u771f\u5b9e\u6027\uff0c\u4e14\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "keywords": "\u5de5\u4e1a\u63a7\u5236\u7cfb\u7edf,\u871c\u7f50,\u6a21\u62df\u903c\u771f\u5ea6,\u968f\u673a\u68ee\u6797,\u566a\u58f0\u5206\u5e03"}}
