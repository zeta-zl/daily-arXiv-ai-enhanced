<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 179]
- [cs.LG](#cs.LG) [Total: 171]
- [cs.AI](#cs.AI) [Total: 43]
- [math.NA](#math.NA) [Total: 2]
- [eess.IV](#eess.IV) [Total: 4]
- [eess.AS](#eess.AS) [Total: 4]
- [cs.DB](#cs.DB) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.CR](#cs.CR) [Total: 15]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.RO](#cs.RO) [Total: 5]
- [cs.CV](#cs.CV) [Total: 51]
- [cs.SE](#cs.SE) [Total: 4]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 8]
- [cs.HC](#cs.HC) [Total: 5]
- [stat.ML](#stat.ML) [Total: 20]
- [stat.ME](#stat.ME) [Total: 3]
- [cs.IR](#cs.IR) [Total: 5]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.GR](#cs.GR) [Total: 2]
- [math.CT](#math.CT) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.IT](#cs.IT) [Total: 2]
- [physics.optics](#physics.optics) [Total: 2]
- [cs.SD](#cs.SD) [Total: 4]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge](https://arxiv.org/abs/2505.17037)
*Dimitri Schreiter*

Key words: 提示工程, 大型语言模型, 特异性, STEM, 医学, 法律

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究探讨了提示词特异性对大型语言模型（LLM）在专业领域（如STEM、医学和法律）任务表现的影响，发现存在一个最佳特异性范围。

Motivation: 提示工程在优化大型语言模型（LLM）的任务表现中至关重要，但词汇特异性对专业领域（如STEM、医学和法律）任务的影响尚未充分研究。本文旨在探讨提高提示词特异性是否能改善LLM在这些领域的问答和推理能力。

Method: 作者开发了一个同义替换框架，系统性地替换名词、动词和形容词的不同特异性级别，并在四个LLM（Llama-3.1-70B-Instruct、Granite-13B-Instruct-V2、Flan-T5-XL和Mistral-Large 2）上测试其在STEM、法律和医学数据集中的表现。

Result: 结果表明，虽然提高提示词特异性通常没有显著影响，但所有测试模型均存在一个最佳特异性范围，在此范围内LLM表现最佳。

Conclusion: 识别的提示词特异性最佳范围为提示设计提供了关键洞察，提示在此范围内调整提示词可最大化LLM表现，为专业领域的应用带来更高效率。

Abstract: Prompt engineering has emerged as a critical component in optimizing large
language models (LLMs) for domain-specific tasks. However, the role of prompt
specificity, especially in domains like STEM (physics, chemistry, biology,
computer science and mathematics), medicine, and law, remains underexplored.
This thesis addresses the problem of whether increasing the specificity of
vocabulary in prompts improves LLM performance in domain-specific
question-answering and reasoning tasks. We developed a synonymization framework
to systematically substitute nouns, verbs, and adjectives with varying
specificity levels, measuring the impact on four LLMs: Llama-3.1-70B-Instruct,
Granite-13B-Instruct-V2, Flan-T5-XL, and Mistral-Large 2, across datasets in
STEM, law, and medicine. Our results reveal that while generally increasing the
specificity of prompts does not have a significant impact, there appears to be
a specificity range, across all considered models, where the LLM performs the
best. Identifying this optimal specificity range offers a key insight for
prompt design, suggesting that manipulating prompts within this range could
maximize LLM performance and lead to more efficient applications in specialized
domains.

</details>


### [2] [Signals from the Floods: AI-Driven Disaster Analysis through Multi-Source Data Fusion](https://arxiv.org/abs/2505.17038)
*Xian Gong,Paul X. McCarthy,Lin Tian,Marian-Andrei Rizoiu*

Key words: 灾害响应,社交媒体分析,LDA,LLM,公众行为

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究整合X（前Twitter）和公众调查数据，分析极端天气下的公众行为模式，提出结合LDA和LLM的方法提升语义理解与内容筛选，优化应急响应。

Motivation: 利用多样化的网络数据（如社交媒体和公众调查）提升灾害响应效率，尤其在2022年新南威尔士洪水事件中验证其价值。

Method: 通过LDA进行主题建模，结合LLM增强语义分析，利用公众调查作为参考筛选相关推文，构建“相关性指数”降低噪声。

Result: 方法能有效识别地理和观点模式，优先处理可操作内容，提升应急响应的实时性和长期韧性规划。

Conclusion: 结合多源数据的AI驱动方法，为灾害响应和韧性规划提供了创新的技术支持。

Abstract: Massive and diverse web data are increasingly vital for government disaster
response, as demonstrated by the 2022 floods in New South Wales (NSW),
Australia. This study examines how X (formerly Twitter) and public inquiry
submissions provide insights into public behaviour during crises. We analyse
more than 55,000 flood-related tweets and 1,450 submissions to identify
behavioural patterns during extreme weather events. While social media posts
are short and fragmented, inquiry submissions are detailed, multi-page
documents offering structured insights. Our methodology integrates Latent
Dirichlet Allocation (LDA) for topic modelling with Large Language Models
(LLMs) to enhance semantic understanding. LDA reveals distinct opinions and
geographic patterns, while LLMs improve filtering by identifying flood-relevant
tweets using public submissions as a reference. This Relevance Index method
reduces noise and prioritizes actionable content, improving situational
awareness for emergency responders. By combining these complementary data
streams, our approach introduces a novel AI-driven method to refine
crisis-related social media content, improve real-time disaster response, and
inform long-term resilience planning.

</details>


### [3] [A new classification system of beer categories and styles based on large-scale data mining and self-organizing maps of beer recipes](https://arxiv.org/abs/2505.17039)
*Diego Bonatto*

Key words: 啤酒分类, 数据驱动, 自组织映射, 发酵参数, 原料模式

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文通过数据驱动方法分析62,121个啤酒配方，提出了基于原料和发酵参数的新分类系统，识别出四大超级聚类，揭示了冷热发酵啤酒的原料差异。

Motivation: 传统啤酒分类依赖感官评价，缺乏客观性和可重复性，本研究旨在通过数据驱动的定量方法建立更科学的分类框架。

Method: 采用统计分析和自组织映射（SOM）对啤酒配方的原料、发酵参数等进行分析，识别聚类模式。

Result: 发现四大超级聚类，冷发酵啤酒原料保守，热发酵啤酒多样性高，反映地域偏好和创新。

Conclusion: 新分类系统为啤酒行业提供了可扩展的客观分析工具，有助于理解啤酒多样性和风味关联。

Abstract: A data-driven quantitative approach was used to develop a novel
classification system for beer categories and styles. Sixty-two thousand one
hundred twenty-one beer recipes were mined and analyzed, considering ingredient
profiles, fermentation parameters, and recipe vital statistics. Statistical
analyses combined with self-organizing maps (SOMs) identified four major
superclusters that showed distinctive malt and hop usage patterns, style
characteristics, and historical brewing traditions. Cold fermented styles
showed a conservative grain and hop composition, whereas hot fermented beers
exhibited high heterogeneity, reflecting regional preferences and innovation.
This new taxonomy offers a reproducible and objective framework beyond
traditional sensory-based classifications, providing brewers, researchers, and
educators with a scalable tool for recipe analysis and beer development. The
findings in this work provide an understanding of beer diversity and open
avenues for linking ingredient usage with fermentation profiles and flavor
outcomes.

</details>


### [4] [VLM-KG: Multimodal Radiology Knowledge Graph Generation](https://arxiv.org/abs/2505.17042)
*Abdullah Abdullah,Seong Tae Kim*

Key words: 视觉语言模型、放射学、知识图谱、多模态、自然语言生成

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种基于多模态视觉语言模型（VLM）的放射学知识图谱生成框架，解决了现有单模态方法在处理放射学报告和长文本数据时的局限性，表现优于现有方法。

Motivation: 放射学知识图谱在医疗领域具有重要价值，但现有方法多为单模态，仅从报告生成且无法处理长文本数据。需要利用多模态VLM克服这些限制。

Method: 设计了一种多模态VLM框架，结合放射学报告和影像数据，提升知识图谱生成的准确性和范围。

Result: 该方法优于现有单模态方法，首次实现了放射学领域的多模态知识图谱生成。

Conclusion: 多模态VLM框架为放射学知识图谱生成提供了更有效和全面的解决方案。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable success in natural
language generation, excelling at instruction following and structured output
generation. Knowledge graphs play a crucial role in radiology, serving as
valuable sources of factual information and enhancing various downstream tasks.
However, generating radiology-specific knowledge graphs presents significant
challenges due to the specialized language of radiology reports and the limited
availability of domain-specific data. Existing solutions are predominantly
unimodal, meaning they generate knowledge graphs only from radiology reports
while excluding radiographic images. Additionally, they struggle with long-form
radiology data due to limited context length. To address these limitations, we
propose a novel multimodal VLM-based framework for knowledge graph generation
in radiology. Our approach outperforms previous methods and introduces the
first multimodal solution for radiology knowledge graph generation.

</details>


### [5] [QRA++: Quantified Reproducibility Assessment for Common Types of Results in Natural Language Processing](https://arxiv.org/abs/2505.17043)
*Anya Belz*

Key words: 可重复性, NLP, QRA++, 量化评估, 实验相似性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种名为QRA++的定量方法，用于评估NLP领域研究的可重复性，通过三个层次的粒度、可比较的度量指标以及实验间相似性来更准确地判断可复现性，并在实验中验证了其有效性。

Motivation: NLP领域的复现研究表明可重复性较低，但没有统一的量化标准导致结论难以比较和解释，因此开发一种能够量化评估可重复性的方法成为必要。

Method: 提出QRA++方法，通过三个层次的连续值评估可重复性、使用可直接比较的量化指标、并结合实验间的相似性来确定复现程度。

Result: QRA++在三组实验中证明了其有效性，并发现复现程度与实验性质、系统类型及评估方法的相似性有关。

Conclusion: QRA++为可重复性评估提供了更精确的工具，揭示了影响复现的关键因素，为未来研究提供了改进方向。

Abstract: Reproduction studies reported in NLP provide individual data points which in
combination indicate worryingly low levels of reproducibility in the field.
Because each reproduction study reports quantitative conclusions based on its
own, often not explicitly stated, criteria for reproduction success/failure,
the conclusions drawn are hard to interpret, compare, and learn from. In this
paper, we present QRA++, a quantitative approach to reproducibility assessment
that (i) produces continuous-valued degree of reproducibility assessments at
three levels of granularity; (ii) utilises reproducibility measures that are
directly comparable across different studies; and (iii) grounds expectations
about degree of reproducibility in degree of similarity between experiments.
QRA++ enables more informative reproducibility assessments to be conducted, and
conclusions to be drawn about what causes reproducibility to be better/poorer.
We illustrate this by applying QRA++ to three example sets of comparable
experiments, revealing clear evidence that degree of reproducibility depends on
similarity of experiment properties, but also system type and evaluation
method.

</details>


### [6] [Assessing GPT's Bias Towards Stigmatized Social Groups: An Intersectional Case Study on Nationality Prejudice and Psychophobia](https://arxiv.org/abs/2505.17045)
*Afifah Kashif,Heer Patel*

Key words: 大语言模型, 偏见, 国籍, 心理健康, 伦理

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究发现GPT-3.5/4/4o模型对不同国籍和心理健康状况群体存在显著偏见，尤其是对朝鲜人的负面偏见更为明显。

Motivation: 探讨广泛使用的大语言模型（LLMs）中存在的国籍与心理健康状况交叉偏见，及其伦理影响。

Method: 通过结构化提示序列，评估模型对美国人和朝鲜人及不同心理健康状况的共情反应。

Result: 发现模型对朝鲜人尤其是同时有心理健康问题的群体表现出更高的负面偏见。

Conclusion: 需要改进LLMs以更好地理解交叉身份的复杂性。

Abstract: Recent studies have separately highlighted significant biases within
foundational large language models (LLMs) against certain nationalities and
stigmatized social groups. This research investigates the ethical implications
of these biases intersecting with outputs of widely-used GPT-3.5/4/4o LLMS.
Through structured prompt series, we evaluate model responses to several
scenarios involving American and North Korean nationalities with various mental
disabilities. Findings reveal significant discrepancies in empathy levels with
North Koreans facing greater negative bias, particularly when mental disability
is also a factor. This underscores the need for improvements in LLMs designed
with a nuanced understanding of intersectional identity.

</details>


### [7] [Assessing the Quality of AI-Generated Clinical Notes: A Validated Evaluation of a Large Language Model Scribe](https://arxiv.org/abs/2505.17047)
*Erin Palm,Astrit Manikantan,Mark E. Pepin,Herprit Mahal,Srikanth Subramanya Belwadi*

Key words: 生成式AI、大型语言模型、临床记录、PDQI9、医疗文档

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了利用大型语言模型（LLM）生成的临床记录是否能够达到专家记录的质量，并提出了一种基于PDQI9工具的评估方法。结果表明，AI生成的记录质量接近人类专家记录，但仍有轻微差异。

Motivation: 医疗实践中，医生开始使用生成式AI工具记录临床诊疗内容，但缺乏评估这些AI生成记录质量的标准方法。本文旨在填补这一空白。

Method: 采用双盲研究设计，比较了LLM生成的临床记录与专家记录的PDQI9评分。研究涉及5个医学领域的专家，评估97次患者就诊的记录。

Result: AI生成的记录与人类专家记录的质量差异微小（4.20 vs 4.25，p = 0.04），且评分者间一致性较高（RWG > 0.7）。

Conclusion: PDQI9工具可用于评估AI生成临床记录的质量，AI记录质量接近人类专家记录，但仍有改进空间。

Abstract: In medical practices across the United States, physicians have begun
implementing generative artificial intelligence (AI) tools to perform the
function of scribes in order to reduce the burden of documenting clinical
encounters. Despite their widespread use, no established methods exist to gauge
the quality of AI scribes. To address this gap, we developed a blinded study
comparing the relative performance of large language model (LLM) generated
clinical notes with those from field experts based on audio-recorded clinical
encounters. Quantitative metrics from the Physician Documentation Quality
Instrument (PDQI9) provided a framework to measure note quality, which we
adapted to assess relative performance of AI generated notes. Clinical experts
spanning 5 medical specialties used the PDQI9 tool to evaluate
specialist-drafted Gold notes and LLM authored Ambient notes. Two evaluators
from each specialty scored notes drafted from a total of 97 patient visits. We
found uniformly high inter rater agreement (RWG greater than 0.7) between
evaluators in general medicine, orthopedics, and obstetrics and gynecology, and
moderate (RWG 0.5 to 0.7) to high inter rater agreement in pediatrics and
cardiology. We found a modest yet significant difference in the overall note
quality, wherein Gold notes achieved a score of 4.25 out of 5 and Ambient notes
scored 4.20 out of 5 (p = 0.04). Our findings support the use of the PDQI9
instrument as a practical method to gauge the quality of LLM authored notes, as
compared to human-authored notes.

</details>


### [8] [Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally](https://arxiv.org/abs/2505.17048)
*Agam Shah,Siddhant Sukhani,Huzaifa Pardawala,Saketh Budideti,Riya Bhadani,Rudra Gopal,Siddhartha Somani,Michael Galarnyk,Soungmin Lee,Arnav Hiray,Akshar Ravichandran,Eric Kim,Pranav Aluru,Joshua Zhang,Sebastian Jaskowski,Veer Guda,Meghaj Tarte,Liqin Ye,Spencer Gosden,Rutwik Routu,Rachel Yuh,Sloka Chava,Sahasra Chava,Dylan Patrick Kelly,Aiden Chiang,Harsit Mittal,Sudheer Chava*

Key words: 中央银行, 货币政策, 数据集, 语言模型, 立场检测

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文介绍了世界中央银行（WCB）数据集，这是一个包含25家中央银行28年数据的全面货币政策语料库，并进行了三项任务的标注。通过测试多种语言模型，发现跨银行聚合数据的模型表现优于单一银行数据模型，验证了‘整体大于部分之和’的原则。

Motivation: 解读中央银行政策对经济稳定性至关重要，但误读可能对弱势群体造成不成比例的影响。为应对这一问题，研究团队构建了一个全面的数据集并开发了任务框架。

Method: 创建WCB数据集，包含380k句子，均匀采样25k句子进行标注，定义三项任务（立场检测、时间分类、不确定性估计），并测试7种PLMs和9种LLMs模型。

Result: 跨银行数据聚合的模型表现显著优于单一银行数据模型，验证了‘整体大于部分之和’的原则。并通过人类评估和预测任务验证了框架的经济实用性。

Conclusion: WCB数据集和提出的框架为政策解读提供了有效工具，跨银行数据聚合的分析方法具有显著优势，且研究结果具有实际经济应用价值。

Abstract: Central banks around the world play a crucial role in maintaining economic
stability. Deciphering policy implications in their communications is
essential, especially as misinterpretations can disproportionately impact
vulnerable populations. To address this, we introduce the World Central Banks
(WCB) dataset, the most comprehensive monetary policy corpus to date,
comprising over 380k sentences from 25 central banks across diverse geographic
regions, spanning 28 years of historical data. After uniformly sampling 1k
sentences per bank (25k total) across all available years, we annotate and
review each sentence using dual annotators, disagreement resolutions, and
secondary expert reviews. We define three tasks: Stance Detection, Temporal
Classification, and Uncertainty Estimation, with each sentence annotated for
all three. We benchmark seven Pretrained Language Models (PLMs) and nine Large
Language Models (LLMs) (Zero-Shot, Few-Shot, and with annotation guide) on
these tasks, running 15,075 benchmarking experiments. We find that a model
trained on aggregated data across banks significantly surpasses a model trained
on an individual bank's data, confirming the principle "the whole is greater
than the sum of its parts." Additionally, rigorous human evaluations, error
analyses, and predictive tasks validate our framework's economic utility. Our
artifacts are accessible through the HuggingFace and GitHub under the
CC-BY-NC-SA 4.0 license.

</details>


### [9] [Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence from Comparative CV/Résumé Evaluations](https://arxiv.org/abs/2505.17049)
*David Rozado*

Key words: 大型语言模型, 性别偏见, 简历评估, 职业匹配, 自主决策

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）在评估求职简历时的性别偏见问题，发现模型普遍偏好女性候选人，且存在位置偏差。

Motivation: 探究LLMs在职业评估中的公平性，揭示其潜在偏见，避免高风险自主决策中的误用。

Method: 通过22个主流LLMs实验，比较性别化与中性化姓名简历的选择偏好，并分析评分与位置效应。

Result: LLMs显著偏好女性候选人；性别中性化后偏见消失；添加代词或调整顺序影响选择。

Conclusion: LLMs在高风险决策中需谨慎使用，其推理可能缺乏一致性。

Abstract: This study examines the behavior of Large Language Models (LLMs) when
evaluating professional candidates based on their resumes or curricula vitae
(CVs). In an experiment involving 22 leading LLMs, each model was
systematically given one job description along with a pair of
profession-matched CVs, one bearing a male first name, the other a female first
name, and asked to select the more suitable candidate for the job. Each CV pair
was presented twice, with names swapped to ensure that any observed preferences
in candidate selection stemmed from gendered names cues. Despite identical
professional qualifications across genders, all LLMs consistently favored
female-named candidates across 70 different professions. Adding an explicit
gender field (male/female) to the CVs further increased the preference for
female applicants. When gendered names were replaced with gender-neutral
identifiers "Candidate A" and "Candidate B", several models displayed a
preference to select "Candidate A". Counterbalancing gender assignment between
these gender-neutral identifiers resulted in gender parity in candidate
selection. When asked to rate CVs in isolation rather than compare pairs, LLMs
assigned slightly higher average scores to female CVs overall, but the effect
size was negligible. Including preferred pronouns (he/him or she/her) next to a
candidate's name slightly increased the odds of the candidate being selected
regardless of gender. Finally, most models exhibited a substantial positional
bias to select the candidate listed first in the prompt. These findings
underscore the need for caution when deploying LLMs in high-stakes autonomous
decision-making contexts and raise doubts about whether LLMs consistently apply
principled reasoning.

</details>


### [10] [Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning](https://arxiv.org/abs/2505.17050)
*Yanhao Jia,Xinyi Wu,Qinglin Zhang,Yiran Qin,Luwei Xiao,Shuai Zhao*

Key words: 项目式学习, 多模态大型语言模型, 层次分析法, 教育评估, STEM教育

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: PBLBench是一个新的基准测试，旨在评估多模态大型语言模型在STEM教育项目式学习中的复杂推理能力，通过专家驱动的方法建立可靠评估标准，发现现有模型表现有限。

Motivation: 现有的基准测试在自由形式输出结构和严格专家验证方面不足，且缺乏自动化流程支持教师在教育任务中使用多模态大型语言模型。

Method: 采用层次分析法（AHP）通过专家驱动的成对比较建立加权评估标准，评估15种领先的多模态大型语言模型。

Result: 即使最先进的模型在PBLBench上的排名准确率也仅为59%，显示当前模型的局限性。

Conclusion: PBLBench挑战现有模型的性能，推动更强大AI代理的发展，以减轻教师负担并提升教育效率。

Abstract: Project-Based Learning (PBL) involves a variety of highly correlated
multimodal data, making it a vital educational approach within STEM
disciplines. With the rapid development of multimodal large language models
(MLLMs), researchers have begun exploring their potential to enhance tasks such
as information retrieval, knowledge comprehension, and data generation in
educational settings. However, existing benchmarks fall short in providing both
a free-form output structure and a rigorous human expert validation process,
limiting their effectiveness in evaluating real-world educational tasks.
Additionally, few methods have developed automated pipelines to assist with the
complex responsibilities of teachers leveraging MLLMs, largely due to model
hallucination and instability, which lead to unreliable implementation. To
address this gap, we introduce PBLBench, a novel benchmark designed to evaluate
complex reasoning grounded in domain-specific knowledge and long-context
understanding, thereby challenging models with tasks that closely resemble
those handled by human experts. To establish reliable ground truth, we adopt
the Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise
comparisons to derive structured and weighted evaluation criteria. We assess
the performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that
even the most advanced models achieve only 59% rank accuracy, underscoring the
significant challenges presented by this benchmark. We believe PBLBench will
serve as a catalyst for the development of more capable AI agents, ultimately
aiming to alleviate teacher workload and enhance educational productivity.

</details>


### [11] [Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large Language Models](https://arxiv.org/abs/2505.17051)
*Bernd Huber,Ghazal Fazelnia,Andreas Damianou,Sebastian Peleato,Max Lefarov,Praveen Ravichandran,Marco De Nadai,Mounia Lalmas-Roellke,Paul N. Bennett*

Key words: 大型语言模型, 个性化, 嵌入, 软令牌前缀, 高效适配

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出一种名为Embedding-to-Prefix (E2P)的高效参数方法，通过将预计算的上下文嵌入映射到单一软令牌前缀来个性化大型语言模型（LLM），避免了昂贵的微调或令牌密集型提示。

Motivation: 当前利用用户特定信息（如偏好或行为的嵌入）来个性化LLM输出的方法通常需要高成本的微调或大量令牌提示，因此需要一种更高效的解决方案。

Method: 提出E2P方法，通过将预计算的上下文嵌入投影到LLM隐藏表示空间的单一软令牌前缀，实现个性化而不冻结主干模型。

Result: 在多个数据集和实际生产环境（如对话个性化、标题生成、音乐/播客推荐）中，E2P表现出色，保持上下文信号且计算开销极低。

Conclusion: E2P为生成式AI系统的个性化提供了一种可扩展、高效的解决方案。

Abstract: Large language models (LLMs) excel at generating contextually relevant
content. However, tailoring these outputs to individual users for effective
personalization is a significant challenge. While rich user-specific
information often exists as pre-existing user representations, such as
embeddings learned from preferences or behaviors, current methods to leverage
these for LLM personalization typically require costly fine-tuning or
token-heavy prompting. We propose Embedding-to-Prefix (E2P), a
parameter-efficient method that injects pre-computed context embeddings into an
LLM's hidden representation space through a learned projection to a single soft
token prefix. This enables effective personalization while keeping the backbone
model frozen and avoiding expensive adaptation techniques. We evaluate E2P
across two public datasets and in a production setting: dialogue
personalization on Persona-Chat, contextual headline generation on PENS, and
large-scale personalization for music and podcast consumption. Results show
that E2P preserves contextual signals and achieves strong performance with
minimal computational overhead, offering a scalable, efficient solution for
contextualizing generative AI systems.

</details>


### [12] [SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs](https://arxiv.org/abs/2505.17052)
*Jinwoo Park,Seunggeun Cho,Dongsu Han*

Key words: 大型语言模型（LLM），边缘计算，推测性解码，成本效率

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: SpecEdge是一个边缘辅助的推理框架，利用推测性解码方案将大型语言模型（LLM）工作负载分配在边缘和服务器GPU之间，通过仅交换令牌输出来提高成本效率和降低延迟。

Motivation: 当前以服务器为中心的系统忽视了边缘的消费级GPU资源，导致LLM服务在规模扩展时成本高且资源密集。

Method: 采用推测性解码方案，结合边缘主动令牌生成和服务器验证的重叠，以及管道感知调度，实现多用户请求的交错处理。

Result: 实验显示，SpecEdge将整体成本效率提升1.91倍，服务器吞吐量增加2.22倍，令牌间延迟降低11.24%。

Conclusion: SpecEdge为LLM服务提供了可扩展且经济高效的范例。

Abstract: Large language models (LLMs) power many modern applications, but serving them
at scale remains costly and resource-intensive. Current server-centric systems
overlook consumer-grade GPUs at the edge. We introduce SpecEdge, an
edge-assisted inference framework that splits LLM workloads between edge and
server GPUs using a speculative decoding scheme, exchanging only token outputs
over the network. SpecEdge employs proactive edge drafting to overlap edge
token creation with server verification and pipeline-aware scheduling that
interleaves multiple user requests to increase server-side throughput.
Experiments show SpecEdge enhances overall cost efficiency by 1.91x through
achieving 2.22x server throughput, and reduces inter token latency by 11.24%
compared to a server-only baseline, introducing a scalable, cost-effective
paradigm for LLM serving.

</details>


### [13] [Social preferences with unstable interactive reasoning: Large language models in economic trust games](https://arxiv.org/abs/2505.17053)
*Ou Jiamin,Eikmans Emile,Buskens Vincent,Pankowska Paulina,Shan Yuli*

Key words: 大语言模型, 信任游戏, 社交行为, 交互推理

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了三种大语言模型（ChatGPT-4、Claude和Bard）在经济信任游戏中的社会行为，发现它们会表现出信任和互惠行为，但在不同情境下与人类行为存在显著差异。

Motivation: 探讨大语言模型如何将语言理解能力转化为社交互动中的行为，尤其是在涉及信任与互惠的经济情境中。

Method: 使用经济信任游戏测试三种LLM的行为，包括单轮和多轮互动，并在不同角色设定（自私或无私）下观察其决策。

Result: LLM在未设定角色时表现出信任与互惠行为，但行为与人类存在差异；角色设定显著影响行为，ChatGPT-4在无私角色下表现尤为突出。

Conclusion: LLM在社交互动中展现出复杂的行为模式，角色设定对其影响远超模型或游戏类型，但其交互推理能力仍不稳定。

Abstract: While large language models (LLMs) have demonstrated remarkable capabilities
in understanding human languages, this study explores how they translate this
understanding into social exchange contexts that capture certain essences of
real world human interactions. Three LLMs - ChatGPT-4, Claude, and Bard - were
placed in economic trust games where players balance self-interest with trust
and reciprocity, making decisions that reveal their social preferences and
interactive reasoning abilities. Our study shows that LLMs deviate from pure
self-interest and exhibit trust and reciprocity even without being prompted to
adopt a specific persona. In the simplest one-shot interaction, LLMs emulated
how human players place trust at the beginning of such a game. Larger
human-machine divergences emerged in scenarios involving trust repayment or
multi-round interactions, where decisions were influenced by both social
preferences and interactive reasoning. LLMs responses varied significantly when
prompted to adopt personas like selfish or unselfish players, with the impact
outweighing differences between models or game types. Response of ChatGPT-4, in
an unselfish or neutral persona, resembled the highest trust and reciprocity,
surpassing humans, Claude, and Bard. Claude and Bard displayed trust and
reciprocity levels that sometimes exceeded and sometimes fell below human
choices. When given selfish personas, all LLMs showed lower trust and
reciprocity than humans. Interactive reasoning to the actions of counterparts
or changing game mechanics appeared to be random rather than stable,
reproducible characteristics in the response of LLMs, though some improvements
were observed when ChatGPT-4 responded in selfish or unselfish personas.

</details>


### [14] [METHOD: Modular Efficient Transformer for Health Outcome Discovery](https://arxiv.org/abs/2505.17054)
*Linglong Qian,Zina Ibrahim*

Key words: transformer, healthcare, clinical sequence modelling, attention mechanism, electronic health records

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为METHOD的模块化高效Transformer架构，专为电子健康记录的临床序列建模设计。通过三种关键创新，该方法在MIMIC-IV数据库上的表现优于现有模型，尤其在高严重性病例预测中表现突出。

Motivation: 由于患者时间线的不规则采样、可变时间依赖性和复杂上下文关系，现有Transformer架构在医疗健康领域的应用面临挑战，需要专门优化的解决方案。

Method: 提出了一个包含三项创新的Transformer架构：1) 防止信息泄漏的患者感知注意力机制；2) 自适应滑动窗口注意力方案；3) 结合动态跳跃连接的U-Net结构，以处理长序列。

Result: 在MIMIC-IV数据库上的实验表明，METHOD在预测高严重性病例和保持临床层级关系方面优于现有模型，且在不同推理长度下性能稳定。

Conclusion: METHOD为医疗健康领域的Transformer架构提供了重要改进，兼顾了预测准确性和计算效率。

Abstract: Recent advances in transformer architectures have revolutionised natural
language processing, but their application to healthcare domains presents
unique challenges. Patient timelines are characterised by irregular sampling,
variable temporal dependencies, and complex contextual relationships that
differ substantially from traditional language tasks. This paper introduces
\METHOD~(Modular Efficient Transformer for Health Outcome Discovery), a novel
transformer architecture specifically designed to address the challenges of
clinical sequence modelling in electronic health records. \METHOD~integrates
three key innovations: (1) a patient-aware attention mechanism that prevents
information leakage whilst enabling efficient batch processing; (2) an adaptive
sliding window attention scheme that captures multi-scale temporal
dependencies; and (3) a U-Net inspired architecture with dynamic skip
connections for effective long sequence processing. Evaluations on the MIMIC-IV
database demonstrate that \METHOD~consistently outperforms the state-of-the-art
\ETHOS~model, particularly in predicting high-severity cases that require
urgent clinical intervention. \METHOD~exhibits stable performance across
varying inference lengths, a crucial feature for clinical deployment where
patient histories vary significantly in length. Analysis of learned embeddings
reveals that \METHOD~better preserves clinical hierarchies and relationships
between medical concepts. These results suggest that \METHOD~represents a
significant advancement in transformer architectures optimised for healthcare
applications, providing more accurate and clinically relevant predictions
whilst maintaining computational efficiency.

</details>


### [15] [Enhancing Mathematics Learning for Hard-of-Hearing Students Through Real-Time Palestinian Sign Language Recognition: A New Dataset](https://arxiv.org/abs/2505.17055)
*Fidaa khandaqji,Huthaifa I. Ashqar,Abdelrahem Atawnih*

Key words: PSL, 手语识别, Vision Transformer, 数学教育, 听障学生

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 开发基于AI的巴勒斯坦手语识别系统，提升听障学生数学教育可及性，模型准确率达97.59%。

Motivation: 弥补巴勒斯坦手语（PSL）数字资源的不足，为听障学生提供AI驱动的数学学习辅助工具。

Method: 创建包含41类数学手势的自定义数据集，并微调Vision Transformer（ViT）模型进行分类。

Result: 模型分类准确率为97.59%，展示了高效识别数学手势的能力。

Conclusion: 深度学习可推动智能教育工具开发，缩小听障学生学习差距。

Abstract: The study aims to enhance mathematics education accessibility for
hard-of-hearing students by developing an accurate Palestinian sign language
PSL recognition system using advanced artificial intelligence techniques. Due
to the scarcity of digital resources for PSL, a custom dataset comprising 41
mathematical gesture classes was created, and recorded by PSL experts to ensure
linguistic accuracy and domain specificity. To leverage
state-of-the-art-computer vision techniques, a Vision Transformer ViTModel was
fine-tuned for gesture classification. The model achieved an accuracy of
97.59%, demonstrating its effectiveness in recognizing mathematical signs with
high precision and reliability. This study highlights the role of deep learning
in developing intelligent educational tools that bridge the learning gap for
hard-of-hearing students by providing AI-driven interactive solutions to
enhance mathematical comprehension. This work represents a significant step
toward innovative and inclusive frosting digital integration in specialized
learning environments. The dataset is hosted on Hugging Face at
https://huggingface.co/datasets/fidaakh/STEM_data.

</details>


### [16] [Are LLMs Ready for English Standardized Tests? A Benchmarking and Elicitation Perspective](https://arxiv.org/abs/2505.17056)
*Luoxi Tang,Tharunya Sundar,Shuai Yang,Ankita Patra,Manohar Chippada,Giqi Zhao,Yi Li,Riteng Zhang,Tunan Zhao,Ting Yang,Yuqiao Meng,Weicheng Ma,Zhaohan Xi*

Key words: 大型语言模型, 英语标准化测试, 教育技术, 智能辅导系统, 基准测试

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇论文研究了大型语言模型（LLMs）在英语标准化测试（ESTs）备考中的应用潜力，推出了综合基准测试ESTBOOK，用于评估LLMs解答EST题目的能力，并提出了分解复杂问题的分析框架。

Motivation: 探索LLMs在标准化测试备考中的潜力，以提升其在教育领域的可靠性和实用性。

Method: 构建ESTBOOK基准测试，涵盖多种题型和模态，并对LLMs的准确性和推理效率进行系统评估，同时提出任务分解框架分析各步骤性能。

Result: 评估结果展示了LLMs在教育场景中的能力，并指出了改进其作为智能辅导系统的策略。

Conclusion: LLMs在标准化测试备考中具有潜力，但需通过针对性策略提升其可靠性和效率。

Abstract: AI is transforming education by enabling powerful tools that enhance learning
experiences. Among recent advancements, large language models (LLMs) hold
particular promise for revolutionizing how learners interact with educational
content. In this work, we investigate the potential of LLMs to support
standardized test preparation by focusing on English Standardized Tests (ESTs).
Specifically, we assess their ability to generate accurate and contextually
appropriate solutions across a diverse set of EST question types. We introduce
ESTBOOK, a comprehensive benchmark designed to evaluate the capabilities of
LLMs in solving EST questions. ESTBOOK aggregates five widely recognized tests,
encompassing 29 question types and over 10,576 questions across multiple
modalities, including text, images, audio, tables, and mathematical symbols.
Using ESTBOOK, we systematically evaluate both the accuracy and inference
efficiency of LLMs. Additionally, we propose a breakdown analysis framework
that decomposes complex EST questions into task-specific solution steps. This
framework allows us to isolate and assess LLM performance at each stage of the
reasoning process. Evaluation findings offer insights into the capability of
LLMs in educational contexts and point toward targeted strategies for improving
their reliability as intelligent tutoring systems.

</details>


### [17] [DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2505.17058)
*David Osei Opoku,Ming Sheng,Yong Zhang*

Key words: QA系统,知识图谱,检索增强生成,异构数据,推理一致性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: DO-RAG是一种结合多级知识图谱与语义向量检索的混合QA框架，通过动态知识图谱增强检索精度，实验显示其在特定领域QA任务中表现优于基线框架。

Motivation: 解决现有检索增强生成（RAG）框架在异构数据集成和推理一致性方面的不足。

Method: 提出DO-RAG框架，结合知识图谱构建与语义向量检索，采用代理链式思维架构从非结构化文档中提取关系并构建动态知识图谱。

Result: 在数据库和电力领域的实验中，DO-RAG实现了接近完美的召回率和94%以上的答案相关性，性能提升达33.38%。

Conclusion: DO-RAG通过可追溯性、适应性和高效性，为多领域高精度QA提供了可靠基础。

Abstract: Domain-specific QA systems require not just generative fluency but high
factual accuracy grounded in structured expert knowledge. While recent
Retrieval-Augmented Generation (RAG) frameworks improve context recall, they
struggle with integrating heterogeneous data and maintaining reasoning
consistency. To address these challenges, we propose DO-RAG, a scalable and
customizable hybrid QA framework that integrates multi-level knowledge graph
construction with semantic vector retrieval. Our system employs a novel agentic
chain-of-thought architecture to extract structured relationships from
unstructured, multimodal documents, constructing dynamic knowledge graphs that
enhance retrieval precision. At query time, DO-RAG fuses graph and vector
retrieval results to generate context-aware responses, followed by
hallucination mitigation via grounded refinement. Experimental evaluations in
the database and electrical domains show near-perfect recall and over 94%
answer relevancy, with DO-RAG outperforming baseline frameworks by up to
33.38%. By combining traceability, adaptability, and performance efficiency,
DO-RAG offers a reliable foundation for multi-domain, high-precision QA at
scale.

</details>


### [18] [Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large](https://arxiv.org/abs/2505.17059)
*Van-Tinh Nguyen,Hoang-Duong Pham,Thanh-Hai To,Cong-Tuan Hung Do,Thi-Thu-Trang Dong,Vu-Trung Duong Le,Van-Phuc Hoang*

Key words: 医疗文本理解, FLAN-T5-Large, 实时推理, 隐私保护

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 介绍Medalyze，一个基于三种FLAN-T5-Large模型的医疗文本理解工具，用于报告总结、问题提取和关键问题识别，性能优于GPT-4。

Motivation: 医疗文本复杂且语境特殊，需提升理解效率。

Method: 使用三种微调FLAN-T5-Large模型，部署于网页和移动端，支持实时推理。

Result: 在特定领域任务中表现优于GPT-4，评估指标包括BLEU等。

Conclusion: Medalyze为医疗信息提供高效、隐私保护的轻量级解决方案。

Abstract: Understanding medical texts presents significant challenges due to complex
terminology and context-specific language. This paper introduces Medalyze, an
AI-powered application designed to enhance the comprehension of medical texts
using three specialized FLAN-T5-Large models. These models are fine-tuned for
(1) summarizing medical reports, (2) extracting health issues from
patient-doctor conversations, and (3) identifying the key question in a
passage. Medalyze is deployed across a web and mobile platform with real-time
inference, leveraging scalable API and YugabyteDB. Experimental evaluations
demonstrate the system's superior summarization performance over GPT-4 in
domain-specific tasks, based on metrics like BLEU, ROUGE-L, BERTScore, and
SpaCy Similarity. Medalyze provides a practical, privacy-preserving, and
lightweight solution for improving information accessibility in healthcare.

</details>


### [19] [SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex Conversation](https://arxiv.org/abs/2505.17060)
*Wenyi Yu,Siyin Wang,Xiaoyu Yang,Xianzhao Chen,Xiaohai Tian,Jun Zhang,Guangzhi Sun,Lu Lu,Yuxuan Wang,Chao Zhang*

Key words: 全双工语音LLM,动态思维机制,回声消除,上下文感知打断,强化学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出SALMONN-omni，首个无需音频编解码的单全双工语音LLM，性能超越现有开源模型30%，支持复杂对话场景。

Motivation: 解决现有全双工会话系统模块化架构的误差累积问题，以及语音模态性能下降的挑战。

Method: 在LLM中引入动态思维机制，使其自主切换听/说状态，避免音频编解码。

Result: 在语音问答和开放对话任务中表现优异，训练数据需求更低，复杂场景处理能力突出。

Conclusion: SALMONN-omni为全双工语音交互提供高效解决方案，通过强化学习进一步优化性能。

Abstract: In order to enable fluid and natural human-machine speech interaction,
existing full-duplex conversational systems often adopt modular architectures
with auxiliary components such as voice activity detectors, interrupters,
conversation state predictors, or multiple LLMs. These systems, however, suffer
from error accumulation across modules and struggle with key challenges such as
context-dependent barge-in and echo cancellation. Recent approaches, most
notably Moshi, simplify the pipeline by injecting audio codecs into the token
space of a single LLM. However, such methods still incur significant
performance degradation when operating on the speech rather than text modality.
In this paper, we introduce SALMONN-omni, the first single, standalone
full-duplex speech LLM that operates without audio codecs in its token space.
It features a novel dynamic thinking mechanism within the LLM backbone,
enabling the model to learn when to transition between speaking and listening
states. Experiments on widely used benchmarks for spoken question answering and
open-domain dialogue show that SALMONN-omni achieves at least 30\% relative
performance improvement over existing open-source full-duplex models and
performs highly competitively to half-duplex and turn-based systems, despite
using substantially less training data. Moreover, SALMONN-omni demonstrates
strong performance in complex conversational scenarios, including turn-taking,
backchanneling, echo cancellation and context-dependent barge-in, with further
improvements achieved through reinforcement learning. Some demo conversations
between user and SALMONN-omni are provided in the following repository
https://github.com/bytedance/SALMONN.

</details>


### [20] [Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2505.17061)
*Xinlong Chen,Yuanxing Zhang,Qiang Liu,Junfei Wu,Fuzheng Zhang,Tieniu Tan*

Key words: Large Vision-Language Models, Hallucination Mitigation, Mixture of Decoding, Attention Mechanism

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为Mixture of Decoding (MoD)的新方法，通过动态评估模型对图像标记注意力的正确性来缓解大型视觉-语言模型(LVLMs)中的幻觉问题。

Motivation: 尽管大型视觉-语言模型(LVLMs)在各种视觉任务中表现出色，但幻觉问题仍然是其主要挑战。为了解这一问题，作者提出了MoD方法。

Method: MoD通过比较基于原始图像标记和模型注意到的图像标记生成的输出的连贯性来区分注意力的正确性。一致性高时采用互补策略增强关键信息，不一致时采用对比策略抑制误导信息。

Result: 实验表明，MoD在多个主流基准测试中显著优于现有解码方法，有效缓解了LVLMs中的幻觉问题。

Conclusion: MoD方法通过动态适应解码策略成功减少了大型视觉-语言模型中的幻觉，展示了较高的实用性和有效性。

Abstract: Large Vision-Language Models (LVLMs) have exhibited impressive capabilities
across various visual tasks, yet they remain hindered by the persistent
challenge of hallucinations. To address this critical issue, we propose Mixture
of Decoding (MoD), a novel approach for hallucination mitigation that
dynamically adapts decoding strategies by evaluating the correctness of the
model's attention on image tokens. Specifically, MoD measures the consistency
between outputs generated from the original image tokens and those derived from
the model's attended image tokens, to distinguish the correctness
aforementioned. If the outputs are consistent, indicating correct attention,
MoD employs a complementary strategy to amplify critical information.
Conversely, if the outputs are inconsistent, suggesting erroneous attention,
MoD utilizes a contrastive strategy to suppress misleading information.
Extensive experiments demonstrate that MoD significantly outperforms existing
decoding methods across multiple mainstream benchmarks, effectively mitigating
hallucinations in LVLMs. The code is available at
https://github.com/xlchen0205/MoD.

</details>


### [21] [Synthetic Data RL: Task Definition Is All You Need](https://arxiv.org/abs/2505.17063)
*Yiduo Guo,Zhen Guo,Chuanwei Huang,Zi-Ang Wang,Zekai Zhang,Haofei Yu,Huishuai Zhang,Yikang Shen*

Key words: 强化学习, 合成数据, 微调, 任务适应性, 自动化标注

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出Synthetic Data RL框架，通过合成数据而非人工标注数据实现强化学习微调，显著提升了模型性能，尤其在数学和法律等领域的任务上表现优异。

Motivation: 传统的强化学习依赖于大规模人工标注数据，限制了其广泛采用。该研究旨在通过合成数据减少对人工标注的依赖，实现更高效和可扩展的模型微调。

Method: 框架首先生成任务定义和检索文档的问题-答案对，然后根据模型解决能力调整问题难度，并选择通过率的平均值进行RL训练。

Result: 在多个任务上，该方法显著超越基线模型和监督微调，例如在GSM8K上提升了29.2%，接近使用全人工数据的RL性能。仅增加少量人工数据对性能提升有限（如GSM8K上仅提升0.4pp）。

Conclusion: Synthetic Data RL通过减少人工标注需求，实现了高效和可扩展的RL模型微调，代码和演示已开源。

Abstract: Reinforcement learning (RL) is a powerful way to adapt foundation models to
specialized tasks, but its reliance on large-scale human-labeled data limits
broad adoption. We introduce Synthetic Data RL, a simple and general framework
that reinforcement fine-tunes models using only synthetic data generated from a
task definition. Our method first generates question and answer pairs from the
task definition and retrieved documents, then adapts the difficulty of the
question based on model solvability, and selects questions using the average
pass rate of the model across samples for RL training. On Qwen-2.5-7B, our
method achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9
pp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on
GPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA
(finance). It surpasses supervised fine-tuning under the same data budget and
nearly matches RL with full human data across datasets (e.g., +17.2 pp on
GSM8K). Adding 100 human demonstrations improves the performance of GSM8K only
by 0.4 pp, showing a limited added value. By reducing human data annotation,
Synthetic Data RL enables scalable and efficient RL-based model adaptation.
Code and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.

</details>


### [22] [Decoding Rarity: Large Language Models in the Diagnosis of Rare Diseases](https://arxiv.org/abs/2505.17065)
*Valentina Carbonari,Pierangelo Veltri,Pietro Hiram Guzzi*

Key words: 人工智能, 大型语言模型, 罕见病, 诊断, 多模态数据

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇综述论文探讨了大型语言模型（LLMs）在罕见病研究中的应用，展示了其在诊断、治疗和患者护理中的潜力，并讨论了多模态数据整合的未来前景及其挑战。

Motivation: 随着人工智能尤其是LLMs的快速发展，其在罕见病研究中展现出的潜力激发了对其应用的深入探索，以改善诊断的准确性和时效性。

Method: 论文回顾了利用LLMs处理文本数据的基础研究，包括医学信息提取、智能对话代理的应用和诊断模型构建，并介绍了结合结构化问卷的实验设计。

Result: 研究表明，LLMs能有效支持罕见病的分析和诊断，但需解决数据隐私、透明性和数据多样性等挑战。

Conclusion: 未来LLMs的发展应聚焦于多模态数据整合以实现对罕见病的更全面理解，从而优化临床结果。

Abstract: Recent advances in artificial intelligence, particularly large language
models LLMs, have shown promising capabilities in transforming rare disease
research. This survey paper explores the integration of LLMs in the analysis of
rare diseases, highlighting significant strides and pivotal studies that
leverage textual data to uncover insights and patterns critical for diagnosis,
treatment, and patient care. While current research predominantly employs
textual data, the potential for multimodal data integration combining genetic,
imaging, and electronic health records stands as a promising frontier. We
review foundational papers that demonstrate the application of LLMs in
identifying and extracting relevant medical information, simulating intelligent
conversational agents for patient interaction, and enabling the formulation of
accurate and timely diagnoses. Furthermore, this paper discusses the challenges
and ethical considerations inherent in deploying LLMs, including data privacy,
model transparency, and the need for robust, inclusive data sets. As part of
this exploration, we present a section on experimentation that utilizes
multiple LLMs alongside structured questionnaires, specifically designed for
diagnostic purposes in the context of different diseases. We conclude with
future perspectives on the evolution of LLMs towards truly multimodal
platforms, which would integrate diverse data types to provide a more
comprehensive understanding of rare diseases, ultimately fostering better
outcomes in clinical settings.

</details>


### [23] [Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning](https://arxiv.org/abs/2505.17067)
*Kristin Qi,Jiali Cheng,Youxiang Zhu,Hadi Amiri,Xiaohui Liang*

Key words: MCI检测, 多语言, 多图片, 监督对比学习, 专家乘积策略

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种用于多语言和多图片环境下轻度认知障碍（MCI）检测的框架，通过监督对比学习、图像模态和专家乘积策略显著提升了检测性能。

Motivation: 当前MCI检测主要局限于英语单图描述，而多语言和多图片场景带来了新挑战，本文旨在解决这一差距。

Method: 提出了包含监督对比学习、图像模态引入和专家乘积策略的三组件框架，以减少伪相关和过拟合。

Result: 框架较文本单模态基线提高了UAR 7.1%（至75.2%）和F1分数2.9%（至83.5%），对比学习对文本模态增益更大。

Conclusion: 该框架在多语言和多图片MCI检测中表现有效，尤其是对比学习对文本模态的显著提升。

Abstract: Detecting Mild Cognitive Impairment from picture descriptions is critical yet
challenging, especially in multilingual and multiple picture settings. Prior
work has primarily focused on English speakers describing a single picture
(e.g., the 'Cookie Theft'). The TAUKDIAL-2024 challenge expands this scope by
introducing multilingual speakers and multiple pictures, which presents new
challenges in analyzing picture-dependent content. To address these challenges,
we propose a framework with three components: (1) enhancing discriminative
representation learning via supervised contrastive learning, (2) involving
image modality rather than relying solely on speech and text modalities, and
(3) applying a Product of Experts (PoE) strategy to mitigate spurious
correlations and overfitting. Our framework improves MCI detection performance,
achieving a +7.1% increase in Unweighted Average Recall (UAR) (from 68.1% to
75.2%) and a +2.9% increase in F1 score (from 80.6% to 83.5%) compared to the
text unimodal baseline. Notably, the contrastive learning component yields
greater gains for the text modality compared to speech. These results highlight
our framework's effectiveness in multilingual and multi-picture MCI detection.

</details>


### [24] [Predictively Combatting Toxicity in Health-related Online Discussions through Machine Learning](https://arxiv.org/abs/2505.17068)
*Jorge Paz-Ruza,Amparo Alonso-Betanzos,Bertha Guijarro-Berdiñas,Carlos Eiras-Franco*

Key words: 用户毒性，协同过滤，机器学习，在线健康讨论，Reddit

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种预测性的方法，通过协同过滤和机器学习技术，预测Reddit用户在COVID相关讨论中可能产生的毒性互动，以避免用户与子社区的冲突。

Motivation: 在线健康讨论中用户的毒性行为常常引发社会冲突或推动不科学的危险行为，传统的检测和删除方法对平台和用户效果不佳。

Method: 采用基于协同过滤的机器学习方法，预测用户在Reddit COVID相关讨论中的毒性互动。

Result: 预测性能超过80%，能有效避免冲突用户与子社区的配对。

Conclusion: 预测性方法在减少在线健康讨论中的毒性行为方面具有潜力，比传统方法更高效。

Abstract: In health-related topics, user toxicity in online discussions frequently
becomes a source of social conflict or promotion of dangerous, unscientific
behaviour; common approaches for battling it include different forms of
detection, flagging and/or removal of existing toxic comments, which is often
counterproductive for platforms and users alike. In this work, we propose the
alternative of combatting user toxicity predictively, anticipating where a user
could interact toxically in health-related online discussions. Applying a
Collaborative Filtering-based Machine Learning methodology, we predict the
toxicity in COVID-related conversations between any user and subcommunity of
Reddit, surpassing 80% predictive performance in relevant metrics, and allowing
us to prevent the pairing of conflicting users and subcommunities.

</details>


### [25] [Improving endpoint detection in end-to-end streaming ASR for conversational speech](https://arxiv.org/abs/2505.17070)
*Anandh C,Karthik Pandia Durai,Jeena Prakash,Manickavela Arumugam,Kadri Hacioglu,S. Pavankumar Dubagunta,Andreas Stolcke,Shankar Venkatesan,Aravind Ganapathiraju*

Key words: 自动语音识别, 端点检测, 延迟惩罚, 语音活动检测

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种改进自动语音识别（ASR）端点检测（EP）的方法，解决了基于Transformer的ASR（T-ASR）模型输出延迟导致的EP问题，通过引入词尾标记和延迟惩罚，并结合辅助网络实现帧级语音活动检测。

Motivation: 在T-ASR模型中，输出延迟会导致EP错误或延迟，影响用户体验。论文旨在通过改进EP方法，减少延迟和错误。

Method: 引入词尾标记和延迟惩罚来解决输出延迟问题，并利用辅助网络实现帧级语音活动检测以优化EP延迟。

Result: 在Switchboard对话语音语料库上测试表明，该方法优于仅使用延迟惩罚的基准方法。

Conclusion: 提出的方法有效改善了T-ASR模型的EP性能，减少了延迟和错误，提升了用户体验。

Abstract: ASR endpointing (EP) plays a major role in delivering a good user experience
in products supporting human or artificial agents in human-human/machine
conversations. Transducer-based ASR (T-ASR) is an end-to-end (E2E) ASR
modelling technique preferred for streaming. A major limitation of T-ASR is
delayed emission of ASR outputs, which could lead to errors or delays in EP.
Inaccurate EP will cut the user off while speaking, returning incomplete
transcript while delays in EP will increase the perceived latency, degrading
the user experience. We propose methods to improve EP by addressing delayed
emission along with EP mistakes. To address the delayed emission problem, we
introduce an end-of-word token at the end of each word, along with a delay
penalty. The EP delay is addressed by obtaining a reliable frame-level speech
activity detection using an auxiliary network. We apply the proposed methods on
Switchboard conversational speech corpus and evaluate it against a delay
penalty method.

</details>


### [26] [What's in a prompt? Language models encode literary style in prompt embeddings](https://arxiv.org/abs/2505.17071)
*Raphaël Sarfati,Haley Moller,Toni J. B. Liu,Nicolas Boullé,Christopher Earls*

Key words: 语言模型, 潜在空间, 风格特征, 作者归属, 文学分析

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型如何将整个提示的累积信息压缩到深度表示中，发现深层嵌入包含风格特征而非事实信息，有助于作者归属和文学分析。

Motivation: 研究动机在于理解语言模型如何处理和压缩文本信息，尤其是深层表示中如何体现风格等抽象特征。

Method: 使用文学作品片段，分析不同长度的文本在潜在空间中的分布情况，考察作者间的风格差异。

Result: 发现短文本在潜在空间中的分布与作者风格相关，而非具体内容，同一作者的作品在潜在空间中更相似。

Conclusion: 研究表明语言模型的深层嵌入能捕捉风格特征，这对于作者归属和文学分析有应用价值。

Abstract: Large language models use high-dimensional latent spaces to encode and
process textual information. Much work has investigated how the conceptual
content of words translates into geometrical relationships between their vector
representations. Fewer studies analyze how the cumulative information of an
entire prompt becomes condensed into individual embeddings under the action of
transformer layers. We use literary pieces to show that information about
intangible, rather than factual, aspects of the prompt are contained in deep
representations. We observe that short excerpts (10 - 100 tokens) from
different novels separate in the latent space independently from what
next-token prediction they converge towards. Ensembles from books from the same
authors are much more entangled than across authors, suggesting that embeddings
encode stylistic features. This geometry of style may have applications for
authorship attribution and literary analysis, but most importantly reveals the
sophistication of information processing and compression accomplished by
language models.

</details>


### [27] [Mechanistic Interpretability of GPT-like Models on Summarization Tasks](https://arxiv.org/abs/2505.17073)
*Anurag Mishra*

Key words: 可解释性, 摘要任务, GPT模型, 注意力机制, LoRA微调

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种用于分析GPT类模型在摘要任务中适应性的可解释性框架，通过对比预训练和微调模型的差异，定位了模型中的“摘要电路”，发现在中层（特别是2、3和5层）变化最为显著，62%的注意力头熵减少，表明信息选择更聚焦。定向LoRA微调这些电路比标准LoRA更高效。

Motivation: 现有可解释性研究多集中在分类或生成任务，而缺少对摘要任务的分析。本文旨在填补这一空白，揭示神经网络在摘要任务中的信息选择和压缩机制。

Method: 采用差异分析方法对比预训练和微调模型，量化注意力模式和内部激活的变化，定位“摘要电路”，并验证定向LoRA微调的效果。

Result: 中层（特别是2、3和5层）变化最显著，62%注意力头熵减少；定向LoRA微调性能优于标准方法，且训练轮次更少。

Conclusion: 本研究不仅定位了摘要任务的“电路”，还提供了对神经网络信息压缩机制的可解释性见解，为未来高效微调提供了方向。

Abstract: Mechanistic interpretability research seeks to reveal the inner workings of
large language models, yet most work focuses on classification or generative
tasks rather than summarization. This paper presents an interpretability
framework for analyzing how GPT-like models adapt to summarization tasks. We
conduct differential analysis between pre-trained and fine-tuned models,
quantifying changes in attention patterns and internal activations. By
identifying specific layers and attention heads that undergo significant
transformation, we locate the "summarization circuit" within the model
architecture. Our findings reveal that middle layers (particularly 2, 3, and 5)
exhibit the most dramatic changes, with 62% of attention heads showing
decreased entropy, indicating a shift toward focused information selection. We
demonstrate that targeted LoRA adaptation of these identified circuits achieves
significant performance improvement over standard LoRA fine-tuning while
requiring fewer training epochs. This work bridges the gap between black-box
evaluation and mechanistic understanding, providing insights into how neural
networks perform information selection and compression during summarization.

</details>


### [28] [Semi-Clairvoyant Scheduling of Speculative Decoding Requests to Minimize LLM Inference Latency](https://arxiv.org/abs/2505.17074)
*Ruixiao Li,Fahao Chen,Peng Li*

Key words: 推测解码、LLM推理、调度算法、LAPS-SD、延迟优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种半先知性请求调度算法LAPS-SD，用于优化推测解码技术中的LLM推理延迟问题。通过动态优先级队列和预调度策略，显著提升性能。

Motivation: 推测解码技术虽加速LLM推理，但现有调度方法仅依赖输出长度预测执行时间，忽略了验证阶段的令牌接受率动态变化，导致效率低下。

Method: 提出LAPS-SD算法，通过动态多优先级队列和请求抢占机制适应令牌接受率变化，稳定后基于精确时间估计调度。

Result: 实验显示，LAPS-SD相比现有方法降低约39%的推理延迟。

Conclusion: LAPS-SD通过动态调度和精准时间估计，显著优化了推测解码场景下的LLM推理效率。

Abstract: Speculative decoding accelerates Large Language Model (LLM) inference by
employing a small speculative model (SSM) to generate multiple candidate tokens
and verify them using the LLM in parallel. This technique has been widely
integrated into LLM inference serving systems. However, inference requests
typically exhibit uncertain execution time, which poses a significant challenge
of efficiently scheduling requests in these systems. Existing work estimates
execution time based solely on predicted output length, which could be
inaccurate because execution time depends on both output length and token
acceptance rate of verification by the LLM. In this paper, we propose a
semi-clairvoyant request scheduling algorithm called
Least-Attained/Perceived-Service for Speculative Decoding (LAPS-SD). Given a
number of inference requests, LAPS-SD can effectively minimize average
inference latency by adaptively scheduling requests according to their features
during decoding. When the token acceptance rate is dynamic and execution time
is difficult to estimate, LAPS-SD maintains multiple priority queues and allows
request execution preemption across different queues. Once the token acceptance
rate becomes stable, LAPS-SD can accurately estimate the execution time and
schedule requests accordingly. Extensive experiments show that LAPS-SD reduces
inference latency by approximately 39\% compared to state-of-the-art scheduling
methods.

</details>


### [29] [Development and Validation of Engagement and Rapport Scales for Evaluating User Experience in Multimodal Dialogue Systems](https://arxiv.org/abs/2505.17075)
*Fuma Kurata,Mao Saeki,Masaki Eguchi,Shungo Suzuki,Hiroaki Takatsu,Yoichi Matsuyama*

Key words: 多模态对话系统, 语言学习, 参与度, 亲近感, 量表验证

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究开发并验证了两个评估多模态对话系统在语言学习中用户体验质量的量表（参与度和亲近感），基于教育心理学、社会心理学和二语习得理论，通过比较人机对话与人际对话的数据证明了量表的有效性。

Motivation: 旨在评估多模态对话系统在语言学习中的用户体验质量，为相关技术发展提供量化工具。

Method: 基于理论设计量表，通过74名学习者与人类导师及对话代理的互动任务收集数据，并利用Cronbach's alpha系数和验证性因子分析检验量表效度与信度。

Result: 量表成功捕捉到人机对话与人际对话在参与度和亲近感上的差异，具备良好的效度和信度。

Conclusion: 研究验证了量表在评估多模态对话系统用户体验质量中的有效性，为未来研究提供了工具支持。

Abstract: This study aimed to develop and validate two scales of engagement and rapport
to evaluate the user experience quality with multimodal dialogue systems in the
context of foreign language learning. The scales were designed based on
theories of engagement in educational psychology, social psychology, and second
language acquisition.Seventy-four Japanese learners of English completed
roleplay and discussion tasks with trained human tutors and a dialog agent.
After each dialogic task was completed, they responded to the scales of
engagement and rapport. The validity and reliability of the scales were
investigated through two analyses. We first conducted analysis of Cronbach's
alpha coefficient and a series of confirmatory factor analyses to test the
structural validity of the scales and the reliability of our designed items. We
then compared the scores of engagement and rapport between the dialogue with
human tutors and the one with a dialogue agent. The results revealed that our
scales succeeded in capturing the difference in the dialogue experience quality
between the human interlocutors and the dialogue agent from multiple
perspectives.

</details>


### [30] [Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English](https://arxiv.org/abs/2505.17076)
*Haoyang Zhang,Hexin Liu,Xiangyu Zhang,Qiquan Zhang,Yuchen Hu,Junqi Zhao,Fei Tian,Xuerui Yang,Eng Siong Chng*

Key words: 语音分词,帧率,语音识别,普通话,英语

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究了不同帧率对普通话和英语这两种类型不同的语言在语音分词中的影响，发现帧率变化对每种语言的语音分词影响不同，与语音密度和语言特定的声学特征有关。

Motivation: 探索帧率变化对语音分词的影响，普通话和英语作为两种不同类型语言的例子。

Method: 通过在不同帧率下编码语音，并在语音识别任务中评估生成的语义令牌。

Result: 帧率变化对每种语言的语音分词影响不同，揭示了帧率、语音密度和语言特定声学特征之间的相互作用。

Conclusion: 研究结果为语音分词器的帧率选择优化提供了见解，对自动语音识别、文本转语音等应用有重要意义。

Abstract: The speech tokenizer plays a crucial role in recent speech tasks, generally
serving as a bridge between speech signals and language models. While
low-frame-rate codecs are widely employed as speech tokenizers, the impact of
frame rates on speech tokens remains underexplored. In this study, we
investigate how varying frame rates affect speech tokenization by examining
Mandarin and English, two typologically distinct languages. We encode speech at
different frame rates and evaluate the resulting semantic tokens in the speech
recognition task. Our findings reveal that frame rate variations influence
speech tokenization differently for each language, highlighting the interplay
between frame rates, phonetic density, and language-specific acoustic features.
The results provide insights into optimizing frame rate selection for speech
tokenizers, with implications for automatic speech recognition, text-to-speech,
and other speech-related applications.

</details>


### [31] [GloSS over Toxicity: Understanding and Mitigating Toxicity in LLMs via Global Toxic Subspace](https://arxiv.org/abs/2505.17078)
*Zenghao Duan,Zhiyi Yin,Zhichao Shi,Liang Pang,Shaoling Jing,Jiayi Wu,Yu Yan,Huawei Shen,Xueqi Cheng*

Key words: 大型语言模型,毒性生成,去毒方法,全局毒性子空间,轻量级方法

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型中毒性生成的机制，提出了一种轻量级的去毒方法GloSS，通过识别并移除FFN中的全局毒性子空间来有效降低毒性，同时保持模型的一般能力。

Motivation: 先前的研究通常将FFN视为毒性的主要来源，但本文通过深入分析发现全局毒性子空间能更有效地表示模型中的毒性区域，因此提出了更全面的去毒方法。

Method: 提出了GloSS（全局毒性子空间抑制）方法，分为四个阶段，通过识别并移除FFN参数中的全局毒性子空间来实现去毒，无需大规模数据或模型重新训练。

Result: 实验表明，GloSS在多种LLM上实现了最先进的去毒效果，同时保持了模型的通用能力。

Conclusion: GloSS为LLM的去毒提供了一种高效、轻量的解决方案，显著降低毒性且不影响模型性能。

Abstract: This paper investigates the underlying mechanisms of toxicity generation in
Large Language Models (LLMs) and proposes an effective detoxification approach.
Prior work typically considers the Feed-Forward Network (FFN) as the main
source of toxicity, representing toxic regions as a set of toxic vectors or
layer-wise subspaces. However, our in-depth analysis reveals that the global
toxic subspace offers a more effective and comprehensive representation of
toxic region within the model. Building on this insight, we propose GloSS
(Global Toxic Subspace Suppression), a lightweight, four-stage method that
mitigates toxicity by identifying and removing the global toxic subspace from
the parameters of FFN. Experiments across a range of LLMs show that GloSS
achieves state-of-the-art detoxification performance while preserving the
models general capabilities, without requiring large-scale data or model
retraining.

</details>


### [32] [Not Minds, but Signs: Reframing LLMs through Semiotics](https://arxiv.org/abs/2505.17080)
*Davide Picca*

Key words: 大型语言模型、符号学、意义生成、文化生产、伦理

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇论文反对将大型语言模型（LLMs）视为认知系统，主张从符号学角度将其理解为符号操作和意义生成的参与者。

Motivation: 避免对LLM的拟人化误解，从更精确的符号学框架分析其在文化过程中的作用。

Method: 结合理论分析和实例，论证LLM作为符号代理的功能。

Result: 提出LLM是技术性符号参与者，影响人类的阅读、写作和意义生成。

Conclusion: 符号学范式为研究和使用LLM提供了更严谨且伦理意识更强的框架。

Abstract: This paper challenges the prevailing tendency to frame Large Language Models
(LLMs) as cognitive systems, arguing instead for a semiotic perspective that
situates these models within the broader dynamics of sign manipulation and
meaning-making. Rather than assuming that LLMs understand language or simulate
human thought, we propose that their primary function is to recombine,
recontextualize, and circulate linguistic forms based on probabilistic
associations. By shifting from a cognitivist to a semiotic framework, we avoid
anthropomorphism and gain a more precise understanding of how LLMs participate
in cultural processes, not by thinking, but by generating texts that invite
interpretation. Through theoretical analysis and practical examples, the paper
demonstrates how LLMs function as semiotic agents whose outputs can be treated
as interpretive acts, open to contextual negotiation and critical reflection.
We explore applications in literature, philosophy, education, and cultural
production, emphasizing how LLMs can serve as tools for creativity, dialogue,
and critical inquiry. The semiotic paradigm foregrounds the situated,
contingent, and socially embedded nature of meaning, offering a more rigorous
and ethically aware framework for studying and using LLMs. Ultimately, this
approach reframes LLMs as technological participants in an ongoing ecology of
signs. They do not possess minds, but they alter how we read, write, and make
meaning, compelling us to reconsider the foundations of language,
interpretation, and the role of artificial systems in the production of
knowledge.

</details>


### [33] [GemMaroc: Unlocking Darija Proficiency in LLMs with Minimal Data](https://arxiv.org/abs/2505.17082)
*Abderrahman Skiredj,Ferdaous Azhari,Houdaifa Atou,Nouamane Tazi,Ismail Berrada*

Key words: Darija, 大语言模型, 多语言推理, LoRA, GemMaroc

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 一篇论文提出了一种高效方法，通过质量优先的对齐策略提升开源大语言模型对摩洛哥阿拉伯语（Darija）的支持，同时保持其多语言推理能力，训练成本极低。

Motivation: 开源大语言模型对摩洛哥阿拉伯语支持不足，传统方法要么需要复杂的适配器，要么牺牲模型的推理能力。本文致力于低成本、高效地解决这一问题。

Method: 将三个小型指令集翻译为Darija，保留部分英文原文，并加入数学、编程和科学提示。使用LoRA微调的Gemma 3-4B和Gemma 3-27B模型进行训练。

Result: GemMaroc-27B在DarijaMMLU上与Atlas-Chat相当（61.6分），在常识推理上表现更优（60.5 vs 48.4分），同时保留原模型的数学和通用推理能力。

Conclusion: 通过绿色AI路径，低成本实现了对Darija的高效支持，为教育、公共服务等场景提供了解决方案。

Abstract: Open-source large language models (LLMs) still marginalise Moroccan Arabic
(Darija), forcing practitioners either to bolt on heavyweight Arabic adapters
or to sacrifice the very reasoning skills that make LLMs useful. We show that a
rigorously quality-over-quantity alignment strategy can surface fluent Darija
while safeguarding the backbone s cross-lingual reasoning at a sliver of the
usual compute. We translate three compact instruction suites LIMA 1 K, DEITA 6
K and TULU 50 K into Darija, preserve 20 of the English originals, and add
mathematics, coding and scientific prompts. A LoRA-tuned Gemma 3-4B trained on
5 K mixed instructions lifts DarijaMMLU from 32.8 to 42.7 ; adding the
reasoning-dense TULU portion pushes it to 47.5 with no English regression.
Scaling the identical recipe to Gemma 3-27B produces GemMaroc-27B, which
matches Atlas-Chat on DarijaMMLU (61.6 ) and leaps ahead on Darija commonsense,
scoring 60.5 on HellaSwag versus Atlas-Chat s 48.4 . Crucially, GemMaroc
retains Gemma-27B s strong maths and general-reasoning ability, showing only
minimal movement on GSM8K and English benchmarks. The entire model is trained
in just 48 GPU.h, underscoring a Green AI pathway to inclusive, sustainable
language technology. We release code, data and checkpoints to spur
Darija-centric applications in education, public services and everyday digital
interaction.

</details>


### [34] [Scale-invariant Attention](https://arxiv.org/abs/2505.17083)
*Ben Anson,Xi Wang,Laurence Aitchison*

Key words: LLM, 注意力机制, 尺度不变性, 长上下文

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种适用于长上下文的注意力机制，通过满足尺度不变总注意力与尺度不变注意力稀疏性条件，简单的位置相关变换在实验中获得显著效果。

Motivation: 解决LLM中注意力机制从短上下文训练到长上下文推理时的泛化问题。

Method: 提出两种尺度不变条件，并通过高斯假设下的位置相关变换实现。

Result: 实验表明该机制在零样本泛化及长上下文检索中表现优越。

Conclusion: 所提出的尺度不变注意力机制能有效提升长上下文性能。

Abstract: One persistent challenge in LLM research is the development of attention
mechanisms that are able to generalise from training on shorter contexts to
inference on longer contexts. We propose two conditions that we expect all
effective long context attention mechanisms to have: scale-invariant total
attention, and scale-invariant attention sparsity. Under a Gaussian assumption,
we show that a simple position-dependent transformation of the attention logits
is sufficient for these conditions to hold. Experimentally we find that the
resulting scale-invariant attention scheme gives considerable benefits in terms
of validation loss when zero-shot generalising from training on short contexts
to validation on longer contexts, and is effective at long-context retrieval.

</details>


### [35] [Reinforcing Question Answering Agents with Minimalist Policy Gradient Optimization](https://arxiv.org/abs/2505.17086)
*Yihong Wu,Liheng Ma,Muzhi Li,Jiaming Zhou,Jianye Hao,Ho-fung Leung,Irwin King,Yingxue Zhang,Jian-Yun Nie*

Key words: 大型语言模型, 多跳问答, 检索增强生成, 强化学习, 最大似然估计

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出Mujica-MyGO框架，通过多跳联合智能和简约策略梯度优化，解决大型语言模型在复杂问答任务中的幻觉问题，显著提升了多跳QA性能。

Motivation: 大型语言模型（LLMs）在问答任务中因缺乏事实知识而存在幻觉问题，现有检索增强生成方法过度依赖上下文学习，性能受限于LLMs的基础推理能力。

Method: 提出Mujica框架，包含问题分解为有向无环图的规划器和基于检索与推理的解决器；同时引入MyGO方法，通过最大似然估计替代传统策略梯度，实现稳定高效训练。

Result: 在多个数据集上的实验表明，Mujica-MyGO能显著提升不同LLMs在多跳QA任务中的性能，且具有可扩展性和资源效率。

Conclusion: 该框架通过多跳联合智能和简约优化方法，为复杂QA任务提供了高效解决方案，克服了现有技术的局限性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable versatility, due to
the lack of factual knowledge, their application to Question Answering (QA)
tasks remains hindered by hallucination.
  While Retrieval-Augmented Generation mitigates these issues by integrating
external knowledge, existing approaches rely heavily on in-context learning,
whose performance is constrained by the fundamental reasoning capabilities of
LLMs.
  In this paper, we propose Mujica, a Multi-hop Joint Intelligence for Complex
Question Answering, comprising a planner that decomposes questions into a
directed acyclic graph of subquestions and a worker that resolves questions via
retrieval and reasoning. Additionally, we introduce MyGO (Minimalist policy
Gradient Optimization), a novel reinforcement learning method that replaces
traditional policy gradient updates with Maximum Likelihood Estimation (MLE) by
sampling trajectories from an asymptotically optimal policy. MyGO eliminates
the need for gradient rescaling and reference models, ensuring stable and
efficient training.
  Empirical results across multiple datasets demonstrate the effectiveness of
Mujica-MyGO in enhancing multi-hop QA performance for various LLMs, offering a
scalable and resource-efficient solution for complex QA tasks.

</details>


### [36] [Informatics for Food Processing](https://arxiv.org/abs/2505.17087)
*Gordana Ispirova,Michael Sebek,Giulia Menichetti*

Key words: 食品加工、机器学习、人工智能、食品信息学、多模态AI

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文讨论了食品加工的演变、分类及健康影响，强调了机器学习、人工智能和数据科学在食品信息学中的重要作用。传统分类框架如NOVA、Nutri-Score和SIGA存在主观性和可重复性问题。为此，作者提出了FoodProX等新型计算方法，并展示了BERT等大语言模型在食品分类中的应用。最后，通过Open Food Facts数据库的案例研究，展示了多模态AI模型如何集成数据以大规模分类食品。

Motivation: 传统食品加工分类框架存在主观性和可重复性问题，制约了流行病学研究和公共卫生政策制定。作者希望通过引入机器学习和大语言模型等现代技术，提供更客观、可扩展的食品加工评估方法。

Method: 采用了随机森林模型（FoodProX）和BERT等大语言模型处理食品描述和成分数据，并开展了多模态AI模型的案例研究，整合了结构化与非结构化数据。

Result: 提出的FoodProX模型和语义嵌入方法能更准确地推断食品加工水平。多模态AI模型在Open Food Facts数据库中实现了大规模食品分类，为公共卫生和研究提供了新工具。

Conclusion: 现代计算方法可以克服传统分类框架的局限性，提供更客观、可扩展的食品加工评估，对公共卫生和研究具有重要意义。

Abstract: This chapter explores the evolution, classification, and health implications
of food processing, while emphasizing the transformative role of machine
learning, artificial intelligence (AI), and data science in advancing food
informatics. It begins with a historical overview and a critical review of
traditional classification frameworks such as NOVA, Nutri-Score, and SIGA,
highlighting their strengths and limitations, particularly the subjectivity and
reproducibility challenges that hinder epidemiological research and public
policy. To address these issues, the chapter presents novel computational
approaches, including FoodProX, a random forest model trained on nutrient
composition data to infer processing levels and generate a continuous FPro
score. It also explores how large language models like BERT and BioBERT can
semantically embed food descriptions and ingredient lists for predictive tasks,
even in the presence of missing data. A key contribution of the chapter is a
novel case study using the Open Food Facts database, showcasing how multimodal
AI models can integrate structured and unstructured data to classify foods at
scale, offering a new paradigm for food processing assessment in public health
and research.

</details>


### [37] [Trust Me, I Can Handle It: Self-Generated Adversarial Scenario Extrapolation for Robust Language Models](https://arxiv.org/abs/2505.17089)
*Md Rafi Ur Rashid,Vishnu Asutosh Dasu,Ye Wang,Gang Tan,Shagufta Mehnaz*

Key words: 大语言模型, 安全风险, ASE, 对抗防御, 流畅性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为ASE的新方法，通过推理时计算框架提升大语言模型的安全性，同时保持流畅的用户体验。

Motivation: 现有防御方法通常只针对单一威胁，且牺牲用户体验，无法应对多样化的新型攻击。

Method: 采用Adversarial Scenario Extrapolation (ASE)框架，结合Chain-of-Thought (CoT)推理，通过模型自我生成对抗场景并制定防御策略。

Result: 在四个对抗基准测试中，ASE显著降低了攻击成功率（接近零）和毒性内容，同时拒绝率低于4%，且在鲁棒性和流畅性权衡上优于现有方法。

Conclusion: ASE通过将对抗感知转化为内在认知过程，为人机交互的安全性和自然性设定了新范式。

Abstract: Large Language Models (LLMs) exhibit impressive capabilities, but remain
susceptible to a growing spectrum of safety risks, including jailbreaks, toxic
content, hallucinations, and bias. Existing defenses often address only a
single threat type or resort to rigid outright rejection, sacrificing user
experience and failing to generalize across diverse and novel attacks. This
paper introduces Adversarial Scenario Extrapolation (ASE), a novel
inference-time computation framework that leverages Chain-of-Thought (CoT)
reasoning to simultaneously enhance LLM robustness and seamlessness. ASE guides
the LLM through a self-generative process of contemplating potential
adversarial scenarios and formulating defensive strategies before generating a
response to the user query. Comprehensive evaluation on four adversarial
benchmarks with four latest LLMs shows that ASE achieves near-zero jailbreak
attack success rates and minimal toxicity, while slashing outright rejections
to <4%. ASE outperforms six state-of-the-art defenses in
robustness-seamlessness trade-offs, with 92-99% accuracy on adversarial Q&A and
4-10x lower bias scores. By transforming adversarial perception into an
intrinsic cognitive process, ASE sets a new paradigm for secure and natural
human-AI interaction.

</details>


### [38] [Large Language Models Implicitly Learn to See and Hear Just By Reading](https://arxiv.org/abs/2505.17091)
*Prateek Verma,Mert Pilanci*

Key words: 自回归LLM, 跨模态学习, 图像分类, 音频分类, 内在学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文展示了通过训练自回归LLM模型在文本标记上，模型能内在地理解图像和音频，从而实现通过阅读获得视觉和听觉能力，验证了文本权重在音频和图像分类任务中的通用性。

Motivation: 探索自回归LLM模型在文本训练过程中是否能够内在地发展出对图像和音频的理解能力，并通过实验验证其在跨模态任务中的通用性。

Method: 提出一种架构，直接处理图像块、音频波形或标记作为输入，生成嵌入或分类标签，并在多个数据集上测试性能。

Result: 在FSD-50K、GTZAN音频数据集以及CIFAR-10、Fashion-MNIST图像数据集上验证了方法的有效性。

Conclusion: 研究表明文本LLM的内部电路强大，可通过激活必要连接应用于不同任务，无需每次从头训练模型。

Abstract: This paper presents a fascinating find: By training an auto-regressive LLM
model on text tokens, the text model inherently develops internally an ability
to understand images and audio, thereby developing the ability to see and hear
just by reading. Popular audio and visual LLM models fine-tune text LLM models
to give text output conditioned on images and audio embeddings. On the other
hand, our architecture takes in patches of images, audio waveforms or tokens as
input. It gives us the embeddings or category labels typical of a
classification pipeline. We show the generality of text weights in aiding audio
classification for datasets FSD-50K and GTZAN. Further, we show this working
for image classification on CIFAR-10 and Fashion-MNIST, as well on image
patches. This pushes the notion of text-LLMs learning powerful internal
circuits that can be utilized by activating necessary connections for various
applications rather than training models from scratch every single time.

</details>


### [39] [Are LLMs reliable? An exploration of the reliability of large language models in clinical note generation](https://arxiv.org/abs/2505.17095)
*Kristine Ann M. Carandang,Jasper Meynard P. Araña,Ethan Robert A. Casin,Christopher P. Monterola,Daniel Stanley Y. Tan,Jesus Felix B. Valenzuela,Christian M. Alis*

Key words: 临床笔记生成,大型语言模型,数据隐私,语义一致性,医疗效率

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究评估了12种开源和专有大型语言模型（LLM）在临床笔记生成（CNG）中的可靠性，发现模型生成的内容在语义上稳定且接近专家笔记，推荐本地部署较小开源模型以提高数据隐私合规性和医疗效率。

Motivation: 因医疗保健提供者（HCPs）对患者数据隐私的责任及LLM响应的自然变异性，需评估LLM在CNG中的可靠性，以增强HCPs对工具的信任。

Method: 评估了Anthropic、Meta、Mistral和OpenAI的12种LLM，衡量其生成笔记的字符串等价性（一致性率）、语义一致性和正确性（语义相似性）。

Result: LLM在语义上表现稳定，生成笔记与专家笔记接近；Meta的Llama 70B最可靠，其次是Mistral的Small模型。

Conclusion: 推荐本地部署较小开源模型，以确保数据隐私合规性并提升临床文档效率。

Abstract: Due to the legal and ethical responsibilities of healthcare providers (HCPs)
for accurate documentation and protection of patient data privacy, the natural
variability in the responses of large language models (LLMs) presents
challenges for incorporating clinical note generation (CNG) systems, driven by
LLMs, into real-world clinical processes. The complexity is further amplified
by the detailed nature of texts in CNG. To enhance the confidence of HCPs in
tools powered by LLMs, this study evaluates the reliability of 12 open-weight
and proprietary LLMs from Anthropic, Meta, Mistral, and OpenAI in CNG in terms
of their ability to generate notes that are string equivalent (consistency
rate), have the same meaning (semantic consistency) and are correct (semantic
similarity), across several iterations using the same prompt. The results show
that (1) LLMs from all model families are stable, such that their responses are
semantically consistent despite being written in various ways, and (2) most of
the LLMs generated notes close to the corresponding notes made by experts.
Overall, Meta's Llama 70B was the most reliable, followed by Mistral's Small
model. With these findings, we recommend the local deployment of these
relatively smaller open-weight models for CNG to ensure compliance with data
privacy regulations, as well as to improve the efficiency of HCPs in clinical
documentation.

</details>


### [40] [TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration](https://arxiv.org/abs/2505.17098)
*Yanshu Li,Tian Yun,Jianjiang Yang,Pinyuan Feng,Jinfa Huang,Ruixiang Tang*

Key words: 多模态ICL, 任务映射, TACO, 大型视觉语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出TACO模型，通过任务映射动态配置上下文序列，提升多模态ICL任务的效果。

Motivation: 大型视觉语言模型的多模态ICL效果对输入序列质量敏感，但对其利用机制的理解有限。

Method: 提出TACO模型，基于任务映射动态调整注意力机制，优化上下文序列构造。

Result: 在五个LVLM和九个数据集上，TACO均优于基线模型。

Conclusion: 任务映射是解释和改进多模态ICL的有效视角。

Abstract: Multimodal in-context learning (ICL) has emerged as a key mechanism for
harnessing the capabilities of large vision-language models (LVLMs). However,
its effectiveness remains highly sensitive to the quality of input in-context
sequences, particularly for tasks involving complex reasoning or open-ended
generation. A major limitation is our limited understanding of how LVLMs
actually exploit these sequences during inference. To bridge this gap, we
systematically interpret multimodal ICL through the lens of task mapping, which
reveals how local and global relationships within and among demonstrations
guide model reasoning. Building on this insight, we present TACO, a lightweight
transformer-based model equipped with task-aware attention that dynamically
configures in-context sequences. By injecting task-mapping signals into the
autoregressive decoding process, TACO creates a bidirectional synergy between
sequence construction and task reasoning. Experiments on five LVLMs and nine
datasets demonstrate that TACO consistently surpasses baselines across diverse
ICL tasks. These results position task mapping as a valuable perspective for
interpreting and improving multimodal ICL.

</details>


### [41] [Learning Interpretable Representations Leads to Semantically Faithful EEG-to-Text Generation](https://arxiv.org/abs/2505.17099)
*Xiaozhao Liu,Dinggang Shen,Xihui Liu*

Key words: 脑解码, EEG到文本, 幻觉问题, GLIM模型, 语义摘要

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出GLIM模型，通过语义摘要而非逐字重建解决EEG到文本解码中的幻觉问题，提升解读性和语义基础。在ZuCo数据集上验证了其生成流畅、基于EEG的句子且无需教师强制的有效性。

Motivation: 预训练生成模型在脑解码中展现了潜力，但其输出的可靠性存疑——是否真实反映大脑语义激活还是仅为模型幻觉。研究聚焦EEG到文本解码，解决其后验崩溃问题。

Method: 提出Generative Language Inspection Model (GLIM)，强调学习信息丰富且可解释的EEG表征，优化小规模异构数据下的语义基础。任务重构为语义摘要而非逐字重建。

Result: 在ZuCo数据集上，GLIM无需教师强制即可生成流畅且基于EEG的句子，支持EEG-文本检索和跨情感类别、关系类型及语料主题的零样本语义分类。

Conclusion: GLIM的架构和评估协议为生成式脑解码的可靠与规模化基准奠定了基础。

Abstract: Pretrained generative models have opened new frontiers in brain decoding by
enabling the synthesis of realistic texts and images from non-invasive brain
recordings. However, the reliability of such outputs remains
questionable--whether they truly reflect semantic activation in the brain, or
are merely hallucinated by the powerful generative models. In this paper, we
focus on EEG-to-text decoding and address its hallucination issue through the
lens of posterior collapse. Acknowledging the underlying mismatch in
information capacity between EEG and text, we reframe the decoding task as
semantic summarization of core meanings rather than previously verbatim
reconstruction of stimulus texts. To this end, we propose the Generative
Language Inspection Model (GLIM), which emphasizes learning informative and
interpretable EEG representations to improve semantic grounding under
heterogeneous and small-scale data conditions. Experiments on the public ZuCo
dataset demonstrate that GLIM consistently generates fluent, EEG-grounded
sentences without teacher forcing. Moreover, it supports more robust evaluation
beyond text similarity, through EEG-text retrieval and zero-shot semantic
classification across sentiment categories, relation types, and corpus topics.
Together, our architecture and evaluation protocols lay the foundation for
reliable and scalable benchmarking in generative brain decoding.

</details>


### [42] [Any Large Language Model Can Be a Reliable Judge: Debiasing with a Reasoning-based Bias Detector](https://arxiv.org/abs/2505.17100)
*Haoyan Yang,Runxue Bao,Cao Xiao,Jun Ma,Parminder Bhatia,Shangqian Gao,Taha Kass-Hout*

Key words: LLM评估, 偏见检测, 结构化推理, 反馈修正, RBD

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文介绍了Reasoning-based Bias Detector (RBD)，一种用于检测和纠正LLM评估偏见的插件模块，通过外部迭代反馈提升评估的准确性和一致性，证明了其高效性和扩展性。

Motivation: LLM评估生成输出时存在偏见，现有方法因自反能力有限或不适用于闭源模型而效果不佳，RBD旨在解决这一问题。

Method: RBD通过构建偏见数据集、监督收集、蒸馏推理微调及与LLM评估器集成，外部迭代检测偏见并生成结构化反馈。

Result: RBD在不同规模和偏见类型上表现优异，如RBD-8B将评估准确性和一致性分别提升18.5%和10.9%，优于基线方法。

Conclusion: RBD能有效提升LLM评估的公正性，具有广泛适用性和高效性。

Abstract: LLM-as-a-Judge has emerged as a promising tool for automatically evaluating
generated outputs, but its reliability is often undermined by potential biases
in judgment. Existing efforts to mitigate these biases face key limitations:
in-context learning-based methods fail to address rooted biases due to the
evaluator's limited capacity for self-reflection, whereas fine-tuning is not
applicable to all evaluator types, especially closed-source models. To address
this challenge, we introduce the Reasoning-based Bias Detector (RBD), which is
a plug-in module that identifies biased evaluations and generates structured
reasoning to guide evaluator self-correction. Rather than modifying the
evaluator itself, RBD operates externally and engages in an iterative process
of bias detection and feedback-driven revision. To support its development, we
design a complete pipeline consisting of biased dataset construction,
supervision collection, distilled reasoning-based fine-tuning of RBD, and
integration with LLM evaluators. We fine-tune four sizes of RBD models, ranging
from 1.5B to 14B, and observe consistent performance improvements across all
scales. Experimental results on 4 bias types--verbosity, position, bandwagon,
and sentiment--evaluated using 8 LLM evaluators demonstrate RBD's strong
effectiveness. For example, the RBD-8B model improves evaluation accuracy by an
average of 18.5% and consistency by 10.9%, and surpasses prompting-based
baselines and fine-tuned judges by 12.8% and 17.2%, respectively. These results
highlight RBD's effectiveness and scalability. Additional experiments further
demonstrate its strong generalization across biases and domains, as well as its
efficiency.

</details>


### [43] [An approach to identify the most semantically informative deep representations of text and images](https://arxiv.org/abs/2505.17101)
*Santiago Acevedo,Andrea Mascaretti,Riccardo Rende,Matéo Mahaut,Marco Baroni,Alessandro Laio*

Key words: 深度神经网络,语义信息,大型语言模型,视觉变换器,信息编码

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种量化深层神经网络中语义相关数据表示相似性的方法，通过测量语义相关数据的相对信息内容，并探究其在大型语言模型和视觉变换器中的编码方式。研究发现，LLM中存在特定的“语义”层存储最具语言迁移性的信息，且较大模型能提取更通用的信息。语义信息分布在多个令牌中，具有长距离相关性及因果不对称性。视觉变换器中同样存在编码语义信息的层级，且LLM的语义层能预测对应图像的视觉表示。

Motivation: 研究深层神经网络如何处理和编码跨域语义信息，以理解模型如何在不同数据类型间传递语义信息。

Method: 通过分析LLM处理翻译句子对的方式，测量信息内容，识别语义层；研究不同规模LLM的信息提取能力；分析视觉变换器中语义信息的编码层级。

Result: 确定LLM中存在存储语言迁移性信息的语义层；大模型提取信息能力更强；语义信息分布广泛且具长距离相关性；视觉变换器语义层与LLM语义层存在预测关系。

Conclusion: 跨域语义信息在深层网络中以特定层级编码，其分布和提取方式受模型规模影响，为理解模型泛化能力提供新视角。

Abstract: Deep neural networks are known to develop similar representations for
semantically related data, even when they belong to different domains, such as
an image and its description, or the same text in different languages. We
present a method for quantitatively investigating this phenomenon by measuring
the relative information content of the representations of semantically related
data and probing how it is encoded into multiple tokens of large language
models (LLMs) and vision transformers. Looking first at how LLMs process pairs
of translated sentences, we identify inner ``semantic'' layers containing the
most language-transferable information. We find moreover that, on these layers,
a larger LLM (DeepSeek-V3) extracts significantly more general information than
a smaller one (Llama3.1-8B). Semantic information is spread across many tokens
and it is characterized by long-distance correlations between tokens and by a
causal left-to-right (i.e., past-future) asymmetry. We also identify layers
encoding semantic information within visual transformers. We show that caption
representations in the semantic layers of LLMs predict visual representations
of the corresponding images. We observe significant and model-dependent
information asymmetries between image and text representations.

</details>


### [44] [BanglaByT5: Byte-Level Modelling for Bangla](https://arxiv.org/abs/2505.17102)
*Pramit Bhattacharyya,Arnab Bhattacharya*

Key words: 大语言模型, 孟加拉语, 字节级建模, ByT5, 自然语言处理

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文介绍了BanglaByT5，这是首个为孟加拉语量身定制的字节级编码器-解码器模型，展示了在生成和分类任务中的竞争优势。

Motivation: 传统分词器（如BPE和SentencePiece）无法捕捉形态丰富语言（如孟加拉语）的细微差别，因此作者设计了专门的字节级模型。

Method: 基于Google ByT5架构的小型变体，预训练于14GB的高质量文学和新闻语料库。

Result: BanglaByT5在零样本和监督评估中表现优异，超越了多语言和更大规模的模型。

Conclusion: 字节级建模对形态丰富语言有效，BanglaByT5是轻量但强大的孟加拉语NLP工具。

Abstract: Large language models (LLMs) have achieved remarkable success across various
natural language processing tasks. However, most LLM models use traditional
tokenizers like BPE and SentencePiece, which fail to capture the finer nuances
of a morphologically rich language like Bangla (Bengali). In this work, we
introduce BanglaByT5, the first byte-level encoder-decoder model explicitly
tailored for Bangla. Built upon a small variant of Googles ByT5 architecture,
BanglaByT5 is pre-trained on a 14GB curated corpus combining high-quality
literary and newspaper articles. Through zeroshot and supervised evaluations
across generative and classification tasks, BanglaByT5 demonstrates competitive
performance, surpassing several multilingual and larger models. Our findings
highlight the efficacy of byte-level modelling for morphologically rich
languages and highlight BanglaByT5 potential as a lightweight yet powerful tool
for Bangla NLP, particularly in both resource-constrained and scalable
environments.

</details>


### [45] [Forging Time Series with Language: A Large Language Model Approach to Synthetic Data Generation](https://arxiv.org/abs/2505.17103)
*Cécile Rousseau,Tobia Boschi,Giandomenico Cornacchia,Dhaval Salwala,Alessandra Pascale,Juan Bernabe Moreno*

Key words: SDForger, LLM, 时间序列生成, 多模态建模, 表格嵌入

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: SDForger是一个灵活高效的框架，利用LLMs生成高质量多元时间序列，通过少量样本和低计算量微调实现，并在多种数据集中表现优于现有生成模型。

Motivation: 旨在通过LLMs生成高质量的多元时间序列数据，解决现有生成模型的局限性，同时支持多模态建模。

Method: 将单变量和多变量信号转换为表格嵌入，编码为文本后微调LLM，生成时采样新文本嵌入并解码为合成时间序列。

Result: 在多样数据集中，SDForger在相似性评估和下游预测任务中均优于现有生成模型。

Conclusion: SDForger为时间序列与文本信息的高效整合提供了新途径，支持多模态建模。

Abstract: SDForger is a flexible and efficient framework for generating high-quality
multivariate time series using LLMs. Leveraging a compact data representation,
SDForger provides synthetic time series generation from a few samples and
low-computation fine-tuning of any autoregressive LLM. Specifically, the
framework transforms univariate and multivariate signals into tabular
embeddings, which are then encoded into text and used to fine-tune the LLM. At
inference, new textual embeddings are sampled and decoded into synthetic time
series that retain the original data's statistical properties and temporal
dynamics. Across a diverse range of datasets, SDForger outperforms existing
generative models in many scenarios, both in similarity-based evaluations and
downstream forecasting tasks. By enabling textual conditioning in the
generation process, SDForger paves the way for multimodal modeling and the
streamlined integration of time series with textual information. SDForger
source code will be open-sourced soon.

</details>


### [46] [P2P: Automated Paper-to-Poster Generation and Fine-Grained Benchmark](https://arxiv.org/abs/2505.17104)
*Tao Sun,Enhao Pan,Zhengkai Yang,Kaixin Sui,Jiajun Shi,Xianfu Cheng,Tongliang Li,Wenhao Huang,Ge Zhang,Jian Yang,Zhoujun Li*

Key words: 学术海报,自动化生成,LLM,多智能体,数据集,评估基准

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: P2P是一个基于LLM的多智能体框架，自动生成高质量学术海报，解决了现有方法在细节保留和视觉-文本整合上的不足，并提供了首个大规模数据集和评估基准。

Motivation: 学术海报的手动制作耗时，而现有自动化方法在科学细节和视觉-文本整合上表现不佳，缺乏标准化评估。

Method: P2P通过三个智能体（视觉处理、内容生成、海报组装）及其检查模块实现迭代优化，并发布P2PInstruct数据集和P2PEval基准。

Result: P2P生成高质量HTML渲染海报，并提供了30,000条指令数据集和121对论文-海报评估基准。

Conclusion: P2P框架及其配套工具显著提升了学术海报生成的效率和质量，为未来研究提供了坚实基础。

Abstract: Academic posters are vital for scholarly communication, yet their manual
creation is time-consuming. However, automated academic poster generation faces
significant challenges in preserving intricate scientific details and achieving
effective visual-textual integration. Existing approaches often struggle with
semantic richness and structural nuances, and lack standardized benchmarks for
evaluating generated academic posters comprehensively. To address these
limitations, we introduce P2P, the first flexible, LLM-based multi-agent
framework that generates high-quality, HTML-rendered academic posters directly
from research papers, demonstrating strong potential for practical
applications. P2P employs three specialized agents-for visual element
processing, content generation, and final poster assembly-each integrated with
dedicated checker modules to enable iterative refinement and ensure output
quality. To foster advancements and rigorous evaluation in this domain, we
construct and release P2PInstruct, the first large-scale instruction dataset
comprising over 30,000 high-quality examples tailored for the academic
paper-to-poster generation task. Furthermore, we establish P2PEval, a
comprehensive benchmark featuring 121 paper-poster pairs and a dual evaluation
methodology (Universal and Fine-Grained) that leverages LLM-as-a-Judge and
detailed, human-annotated checklists. Our contributions aim to streamline
research dissemination and provide the community with robust tools for
developing and evaluating next-generation poster generation systems.

</details>


### [47] [RRTL: Red Teaming Reasoning Large Language Models in Tool Learning](https://arxiv.org/abs/2505.17106)
*Yifei Liu,Yu Cui,Haibin Zhang*

Key words: 推理大语言模型, 工具学习, 安全性, 欺骗性威胁, 思维链提示

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了推理大语言模型(RLLMs)在工具学习中的安全性，提出了RRTL方法来评估其潜在风险，发现RLLMs虽然安全性优于传统LLMs，但仍存在欺骗性风险和多语言漏洞。

Motivation: 随着工具学习增强大语言模型的能力，其安全问题日益突出，尤其是新兴推理大语言模型(RLLMs)的安全性尚未充分研究。

Method: 提出了RRTL方法，结合两种策略：识别欺骗性威胁和利用思维链(COT)提示强制调用工具，并建立了传统LLMs的基准。

Result: 评估7种主流RLLMs发现：(1) RLLMs总体上安全性优于传统LLMs；(2) RLLMs可能隐藏工具使用和风险提示，构成欺骗性风险；(3) CoT揭示多语言安全漏洞。

Conclusion: 研究为增强RLLMs在工具学习中的安全性提供了重要见解。

Abstract: While tool learning significantly enhances the capabilities of large language
models (LLMs), it also introduces substantial security risks. Prior research
has revealed various vulnerabilities in traditional LLMs during tool learning.
However, the safety of newly emerging reasoning LLMs (RLLMs), such as
DeepSeek-R1, in the context of tool learning remains underexplored. To bridge
this gap, we propose RRTL, a red teaming approach specifically designed to
evaluate RLLMs in tool learning. It integrates two novel strategies: (1) the
identification of deceptive threats, which evaluates the model's behavior in
concealing the usage of unsafe tools and their potential risks; and (2) the use
of Chain-of-Thought (CoT) prompting to force tool invocation. Our approach also
includes a benchmark for traditional LLMs. We conduct a comprehensive
evaluation on seven mainstream RLLMs and uncover three key findings: (1) RLLMs
generally achieve stronger safety performance than traditional LLMs, yet
substantial safety disparities persist across models; (2) RLLMs can pose
serious deceptive risks by frequently failing to disclose tool usage and to
warn users of potential tool output risks; (3) CoT prompting reveals
multi-lingual safety vulnerabilities in RLLMs. Our work provides important
insights into enhancing the security of RLLMs in tool learning.

</details>


### [48] [Multi-Modality Expansion and Retention for LLMs through Parameter Merging and Decoupling](https://arxiv.org/abs/2505.17110)
*Junlin Li,Guodong DU,Jing Li,Sim Kuan Goh,Wenya Wang,Yequan Wang,Fangming Liu,Ho-Kin Tang,Saleh Alharbi,Daojing He,Min Zhang*

Key words: 多模态LLM, 参数解耦, 灾难性遗忘, 训练免费框架

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: MMER是一种无需训练的框架，实现多模态LLM的扩展而不损害原始性能，避免灾难性遗忘。

Motivation: 现有MLLM扩展依赖资源密集型微调且缺乏灵活性，MMER通过参数解耦避免冲突以高效整合多模态能力。

Method: 复用MLLM编码器并合并LLM参数，通过二进制掩码分离模态专用参数，独立处理输入以减少冲突。

Result: 实验显示扩展多模态能力的同时保留99%原始性能，显著减轻灾难性遗忘。

Conclusion: MMER为多模态扩展提供了高效且轻量的解决方案。

Abstract: Fine-tuning Large Language Models (LLMs) with multimodal encoders on
modality-specific data expands the modalities that LLMs can handle, leading to
the formation of Multimodal LLMs (MLLMs). However, this paradigm heavily relies
on resource-intensive and inflexible fine-tuning from scratch with new
multimodal data. In this paper, we propose MMER (Multi-modality Expansion and
Retention), a training-free approach that integrates existing MLLMs for
effective multimodal expansion while retaining their original performance.
Specifically, MMER reuses MLLMs' multimodal encoders while merging their LLM
parameters. By comparing original and merged LLM parameters, MMER generates
binary masks to approximately separate LLM parameters for each modality. These
decoupled parameters can independently process modality-specific inputs,
reducing parameter conflicts and preserving original MLLMs' fidelity. MMER can
also mitigate catastrophic forgetting by applying a similar process to MLLMs
fine-tuned on new tasks. Extensive experiments show significant improvements
over baselines, proving that MMER effectively expands LLMs' multimodal
capabilities while retaining 99% of the original performance, and also markedly
mitigates catastrophic forgetting.

</details>


### [49] [Cultural Value Alignment in Large Language Models: A Prompt-based Analysis of Schwartz Values in Gemini, ChatGPT, and DeepSeek](https://arxiv.org/abs/2505.17112)
*Robin Segerer*

Key words: 文化价值对齐,大型语言模型,施瓦茨价值观框架,集体主义,AI公平性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究人员通过施瓦茨价值观框架分析Gemini、ChatGPT和DeepSeek的文化价值取向，发现DeepSeek更倾向于集体主义价值观，与西方模型不同。这反映了LLM的文化偏见，而非普世伦理框架。

Motivation: 探讨大型语言模型（LLMs）是否反映文化价值偏见，以及如何通过多视角推理等方法减少这些偏见。

Method: 使用40项肖像价值观问卷（PVQ），并通过贝叶斯序数回归模型分析Gemini、ChatGPT和DeepSeek的价值偏好。

Result: 所有模型均强调亲社会价值（如仁慈、普遍主义），但DeepSeek相较于西方模型更弱化自我增强价值（如权力、成就），与集体主义文化倾向一致。

Conclusion: LLMs存在文化价值偏见，需开发多元化的对齐框架整合不同道德视角，以提升AI公平性和文化中立性。

Abstract: This study examines cultural value alignment in large language models (LLMs)
by analyzing how Gemini, ChatGPT, and DeepSeek prioritize values from
Schwartz's value framework. Using the 40-item Portrait Values Questionnaire, we
assessed whether DeepSeek, trained on Chinese-language data, exhibits distinct
value preferences compared to Western models. Results of a Bayesian ordinal
regression model show that self-transcendence values (e.g., benevolence,
universalism) were highly prioritized across all models, reflecting a general
LLM tendency to emphasize prosocial values. However, DeepSeek uniquely
downplayed self-enhancement values (e.g., power, achievement) compared to
ChatGPT and Gemini, aligning with collectivist cultural tendencies. These
findings suggest that LLMs reflect culturally situated biases rather than a
universal ethical framework. To address value asymmetries in LLMs, we propose
multi-perspective reasoning, self-reflective feedback, and dynamic
contextualization. This study contributes to discussions on AI fairness,
cultural neutrality, and the need for pluralistic AI alignment frameworks that
integrate diverse moral perspectives.

</details>


### [50] [RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language](https://arxiv.org/abs/2505.17114)
*Subrata Biswas,Mohammad Nur Hossain Khan,Bashima Islam*

Key words: 多模态问答、模态分歧、QuART 模块、RAVEN、AVS-QA 数据集

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: RAVEN 是一种统一的多模态问答架构，通过 QuART 模块动态分配模态相关性得分，优化信号融合，并在多个基准测试中显著优于现有方法。

Motivation: 解决多模态问答中模态分歧问题（如离屏语音或背景噪声误导模型），提升融合模型的鲁棒性和准确性。

Method: 采用三阶段训练流程：单模态预训练、查询对齐融合和分歧导向微调，核心是 QuART 模块动态评分模态相关性。

Result: 在七大多模态 QA 基准测试中，RAVEN 准确率提升最高达 14.5%，传感器数据进一步带来 16.4% 增益，且对模态损坏具有强鲁棒性（优于基线 50.23%）。

Conclusion: RAVEN 通过动态模态筛选和分级训练策略，显著提升多模态 QA 性能，尤其擅长处理模态噪声和不一致。

Abstract: Multimodal question answering (QA) often requires identifying which video,
audio, or sensor tokens are relevant to the question. Yet modality
disagreements are common: off-camera speech, background noise, or motion
outside the field of view often mislead fusion models that weight all streams
equally. We present RAVEN, a unified QA architecture whose core is QuART, a
query-conditioned cross-modal gating module that assigns scalar relevance
scores to each token across modalities, enabling the model to amplify
informative signals and suppress distractors before fusion. RAVEN is trained
through a three-stage pipeline comprising unimodal pretraining, query-aligned
fusion, and disagreement-oriented fine-tuning -- each stage targeting a
distinct challenge in multi-modal reasoning: representation quality,
cross-modal relevance, and robustness to modality mismatch. To support training
and evaluation, we release AVS-QA, a dataset of 300K synchronized
Audio--Video-Sensor streams paired with automatically generated question-answer
pairs. Experimental results on seven multi-modal QA benchmarks -- including
egocentric and exocentric tasks -- show that RAVEN achieves up to 14.5\% and
8.0\% gains in accuracy compared to state-of-the-art multi-modal large language
models, respectively. Incorporating sensor data provides an additional 16.4\%
boost, and the model remains robust under modality corruption, outperforming
SOTA baselines by 50.23\%. Our code and dataset are available at
https://github.com/BASHLab/RAVEN.

</details>


### [51] [Comparative Evaluation of Prompting and Fine-Tuning for Applying Large Language Models to Grid-Structured Geospatial Data](https://arxiv.org/abs/2505.17116)
*Akash Dhruv,Yangxinyu Xie,Jordan Branham,Tanwi Mallick*

Key words: 大语言模型，地理空间数据，零样本提示，微调，结构化推理

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文比较了大型语言模型在解释网格结构地理空间数据中的表现，评估了基础模型与微调变体的性能，突出了零样本提示的优势与限制以及微调对结构化地理和时序推理的益处。

Motivation: 研究动机在于探索大型语言模型在处理网格结构地理空间数据时的能力差异，尤其关注零样本提示与微调模型在实际应用中的表现对比。

Method: 研究方法包括通过结构化提示评估基础模型，并与在用户-助手交互数据集上微调的变体进行对比。

Result: 结果表明，零样本提示在某些场景下表现良好，但微调模型在结构化地理空间和时序推理任务中更具优势。

Conclusion: 结论指出，微调能够显著提升大型语言模型在特定地理空间任务中的性能，但零样本提示仍适用于部分场景。

Abstract: This paper presents a comparative study of large language models (LLMs) in
interpreting grid-structured geospatial data. We evaluate the performance of a
base model through structured prompting and contrast it with a fine-tuned
variant trained on a dataset of user-assistant interactions. Our results
highlight the strengths and limitations of zero-shot prompting and demonstrate
the benefits of fine-tuning for structured geospatial and temporal reasoning.

</details>


### [52] [From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning](https://arxiv.org/abs/2505.17117)
*Chen Shani,Dan Jurafsky,Yann LeCun,Ravid Shwartz-Ziv*

Key words: 语义压缩、大型语言模型、信息瓶颈、人类分类、信息论

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文通过信息论框架比较人类与大型语言模型（LLM）在语义压缩上的差异，发现LLM偏向统计压缩，而人类更注重语义细节和上下文丰富性。

Motivation: 探讨LLM是否像人类一样在压缩与语义保真之间取得平衡，揭示两者在概念表示上的根本差异。

Method: 基于信息率失真理论和信息瓶颈原则，定量分析多种LLM的词嵌入与人类分类基准的对比。

Result: LLM能形成与人类判断一致的概念类别，但难以捕捉精细语义区分，且偏向统计压缩而非语义丰富性。

Conclusion: 研究结果揭示了当前AI与人类认知架构的关键差异，为开发更贴近人类概念表示的LLM提供了方向。

Abstract: Humans organize knowledge into compact categories through semantic
compression by mapping diverse instances to abstract representations while
preserving meaning (e.g., robin and blue jay are both birds; most birds can
fly). These concepts reflect a trade-off between expressive fidelity and
representational simplicity. Large Language Models (LLMs) demonstrate
remarkable linguistic abilities, yet whether their internal representations
strike a human-like trade-off between compression and semantic fidelity is
unclear. We introduce a novel information-theoretic framework, drawing from
Rate-Distortion Theory and the Information Bottleneck principle, to
quantitatively compare these strategies. Analyzing token embeddings from a
diverse suite of LLMs against seminal human categorization benchmarks, we
uncover key divergences. While LLMs form broad conceptual categories that align
with human judgment, they struggle to capture the fine-grained semantic
distinctions crucial for human understanding. More fundamentally, LLMs
demonstrate a strong bias towards aggressive statistical compression, whereas
human conceptual systems appear to prioritize adaptive nuance and contextual
richness, even if this results in lower compressional efficiency by our
measures. These findings illuminate critical differences between current AI and
human cognitive architectures, guiding pathways toward LLMs with more
human-aligned conceptual representations.

</details>


### [53] [After Retrieval, Before Generation: Enhancing the Trustworthiness of Large Language Models in RAG](https://arxiv.org/abs/2505.17118)
*Xinbang Dai,Huikang Hu,Yuncheng Hua,Jiaqi Li,Yongrui Chen,Rihui Jin,Nan Hu,Guilin Qi*

Key words: 检索增强生成（RAG）、可信度、自适应加权、决策树、大语言模型（LLM）

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了BRIDGE框架，通过动态权衡内部与外部知识，提升RAG系统的可信响应能力。

Motivation: 现有RAG系统在处理内部（参数化）与外部（检索）知识冲突时缺乏统一框架，难以在多样化场景中平衡可信度。

Method: 提出BRIDGE框架：1) 使用soft bias自适应加权机制整合知识；2) 通过最大软偏决策树选择最优响应策略（信任内部/外部知识或拒绝回答）。

Result: 实验显示BRIDGE在准确性上超越基线5-15%，且在所有场景中保持均衡性能。

Conclusion: BRIDGE为实际RAG应用提供了可信响应的高效解决方案。

Abstract: Retrieval-augmented generation (RAG) systems face critical challenges in
balancing internal (parametric) and external (retrieved) knowledge, especially
when these sources conflict or are unreliable. To analyze these scenarios
comprehensively, we construct the Trustworthiness Response Dataset (TRD) with
36,266 questions spanning four RAG settings. We reveal that existing approaches
address isolated scenarios-prioritizing one knowledge source, naively merging
both, or refusing answers-but lack a unified framework to handle different
real-world conditions simultaneously. Therefore, we propose the BRIDGE
framework, which dynamically determines a comprehensive response strategy of
large language models (LLMs). BRIDGE leverages an adaptive weighting mechanism
named soft bias to guide knowledge collection, followed by a Maximum Soft-bias
Decision Tree to evaluate knowledge and select optimal response strategies
(trust internal/external knowledge, or refuse). Experiments show BRIDGE
outperforms baselines by 5-15% in accuracy while maintaining balanced
performance across all scenarios. Our work provides an effective solution for
LLMs' trustworthy responses in real-world RAG applications.

</details>


### [54] [Systematic Evaluation of Machine-Generated Reasoning and PHQ-9 Labeling for Depression Detection Using Large Language Models](https://arxiv.org/abs/2505.17119)
*Zongru Shao,Xin Wang,Zhanyang Liu,Chenhan Wang,K. P. Subbalakshmi*

Key words: 大语言模型, 心理健康检测, 逻辑推理, 提示工程, 直接偏好优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文系统评估了大语言模型(LLMs)在心理健康检测中的逻辑推理能力，揭示其弱点，并提出基于提示工程和优化策略的性能提升方法。

Motivation: 当前基于LLMs的心理健康检测依赖机器生成数据，但其推理能力存在未知缺陷，且缺乏对生成数据的质量控制。论文旨在系统性评估并改善这一问题。

Method: 1. 设计LLM指令策略分解检测任务；2. 构建对比性少样本和链式思维提示；3. 人工标注子任务并评估；4. 通过人类偏好优化策略（SFT和DPO）提升性能。

Result: LLMs在显性抑郁语言检测中表现更优，DPO优化显著提升性能，减少了统计偏差。

Conclusion: 研究揭示了LLMs在隐式抑郁检测上的不足，并证明基于人类偏好的优化策略（如DPO）能有效改进模型表现。

Abstract: Recent research leverages large language models (LLMs) for early mental
health detection, such as depression, often optimized with machine-generated
data. However, their detection may be subject to unknown weaknesses. Meanwhile,
quality control has not been applied to these generated corpora besides limited
human verifications. Our goal is to systematically evaluate LLM reasoning and
reveal potential weaknesses. To this end, we first provide a systematic
evaluation of the reasoning over machine-generated detection and
interpretation. Then we use the models' reasoning abilities to explore
mitigation strategies for enhanced performance. Specifically, we do the
following: A. Design an LLM instruction strategy that allows for systematic
analysis of the detection by breaking down the task into several subtasks. B.
Design contrastive few-shot and chain-of-thought prompts by selecting typical
positive and negative examples of detection reasoning. C. Perform human
annotation for the subtasks identified in the first step and evaluate the
performance. D. Identify human-preferred detection with desired logical
reasoning from the few-shot generation and use them to explore different
optimization strategies. We conducted extensive comparisons on the DepTweet
dataset across the following subtasks: 1. identifying whether the speaker is
describing their own depression; 2. accurately detecting the presence of PHQ-9
symptoms, and 3. finally, detecting depression. Human verification of
statistical outliers shows that LLMs demonstrate greater accuracy in analyzing
and detecting explicit language of depression as opposed to implicit
expressions of depression. Two optimization methods are used for performance
enhancement and reduction of the statistic bias: supervised fine-tuning (SFT)
and direct preference optimization (DPO). Notably, the DPO approach achieves
significant performance improvement.

</details>


### [55] [Self-Interpretability: LLMs Can Describe Complex Internal Processes that Drive Their Decisions, and Improve with Training](https://arxiv.org/abs/2505.17120)
*Dillon Plunkett,Adam Morris,Keerthi Reddy,Jorge Morales*

Key words: 大型语言模型,自我解释,决策过程,可解释性,泛化能力

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 当前研究表明，大型语言模型（LLMs）能够通过训练更准确地理解和解释自身的决策过程，并能将此能力泛化到其他复杂决策中。

Motivation: 由于LLMs的神经网络难以解析，研究旨在探索模型自我反思和解释能力，以提升其可理解性、控制性和安全性。

Method: 通过微调GPT-4o和GPT-4o-mini模型，使其在多类复杂情境中决策，并在训练中随机生成定量偏好权重以模拟决策过程。

Result: 实验表明，LLMs能准确报告决策偏好权重，且能通过训练进一步提升解释能力，同时该能力可泛化至其他未训练任务。

Conclusion: 该研究为提升LLMs自我报告内部过程的能力奠定了基础，对模型的可解释性和安全性有重要意义。

Abstract: We have only limited understanding of how and why large language models
(LLMs) respond in the ways that they do. Their neural networks have proven
challenging to interpret, and we are only beginning to tease out the function
of individual neurons and circuits within them. However, another path to
understanding these systems is to investigate and develop their capacity to
introspect and explain their own functioning. Here, we show that i)
contemporary LLMs are capable of providing accurate, quantitative descriptions
of their own internal processes during certain kinds of decision-making, ii)
that it is possible to improve these capabilities through training, and iii)
that this training generalizes to at least some degree. To do so, we fine-tuned
GPT-4o and GPT-4o-mini to make decisions in a wide variety of complex contexts
(e.g., choosing between condos, loans, vacations, etc.) according to
randomly-generated, quantitative preferences about how to weigh different
attributes during decision-making (e.g., the relative importance of natural
light versus quiet surroundings for condos). We demonstrate that the LLMs can
accurately report these preferences (i.e., the weights that they learned to
give to different attributes during decision-making). Next, we demonstrate that
these LLMs can be fine-tuned to explain their decision-making even more
accurately. Finally, we demonstrate that this training generalizes: It improves
the ability of the models to accurately explain what they are doing as they
make other complex decisions, not just decisions they have learned to make via
fine-tuning. This work is a step towards training LLMs to accurately and
broadly report on their own internal processes -- a possibility that would
yield substantial benefits for interpretability, control, and safety.

</details>


### [56] [NeSyGeo: A Neuro-Symbolic Framework for Multimodal Geometric Reasoning Data Generation](https://arxiv.org/abs/2505.17121)
*Weiming Wu,Zi-kang Wang,Jin Ye,Zhi Zhou,Yu-Feng Li,Lan-Zhe Guo*

Key words: 神经符号框架、几何推理、多模态大语言模型、数据生成

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种新的神经符号框架NeSyGeo，用于生成多样化的几何推理数据，显著提升多模态大语言模型（MLLMs）的几何推理能力。

Motivation: 现有数据生成方法在多样性和数值泛化上存在局限，因此需要一种新方法来提升几何推理数据的质量与规模。

Method: 结合领域特定语言和神经符号框架，通过符号-视觉-文本管道生成多样化的QA对。

Result: NeSyGeo显著提高了MLLMs在多个基准测试上的性能，小样本和少训练轮次下仍有显著提升。

Conclusion: NeSyGeo框架有效解决了数据多样性与泛化问题，为MLLMs的几何推理能力提供了可靠数据支持。

Abstract: Obtaining large-scale, high-quality data with reasoning paths is crucial for
improving the geometric reasoning capabilities of multi-modal large language
models (MLLMs). However, existing data generation methods, whether based on
predefined templates or constrained symbolic provers, inevitably face diversity
and numerical generalization limitations. To address these limitations, we
propose NeSyGeo, a novel neuro-symbolic framework for generating geometric
reasoning data. First, we propose a domain-specific language grounded in the
entity-relation-constraint paradigm to comprehensively represent all components
of plane geometry, along with generative actions defined within this symbolic
space. We then design a symbolic-visual-text pipeline that synthesizes symbolic
sequences, maps them to corresponding visual and textual representations, and
generates diverse question-answer (Q&A) pairs using large language models
(LLMs). To the best of our knowledge, we are the first to propose a
neuro-symbolic approach in generating multimodal reasoning data. Based on this
framework, we construct NeSyGeo-CoT and NeSyGeo-Caption datasets, containing
100k samples, and release a new benchmark NeSyGeo-Test for evaluating geometric
reasoning abilities in MLLMs. Experiments demonstrate that the proposal
significantly and consistently improves the performance of multiple MLLMs under
both reinforcement and supervised fine-tuning. With only 4k samples and two
epochs of reinforcement fine-tuning, base models achieve improvements of up to
+15.8% on MathVision, +8.4% on MathVerse, and +7.3% on GeoQA. Notably, a 4B
model can be improved to outperform an 8B model from the same series on
geometric reasoning tasks.

</details>


### [57] [Shallow Preference Signals: Large Language Model Aligns Even Better with Truncated Data?](https://arxiv.org/abs/2505.17122)
*Xuan Qi,Jiahao Qiu,Xinzhe Juan,Yue Wu,Mengdi Wang*

Key words: 大语言模型, 人类偏好对齐, 浅层偏好信号, 数据集截断, 解码策略

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究发现大语言模型对齐人类偏好时，现有方法依赖的偏好信号主要集中在回答的前半部分（浅层偏好信号）。实验表明，仅用前半部分数据训练的模型性能与完整数据集相当甚至更好，且通过解码策略优化后性能进一步提升，揭示了当前对齐方法的潜在局限性。

Motivation: 探索大语言模型对齐人类偏好时，现有方法是否存在偏好信号分布不均的问题，并验证浅层偏好信号的普遍性及其影响。

Method: 通过系统截断偏好数据集（如仅保留前半部分），训练奖励模型和DPO模型，并设计两种解码策略（长度控制与KL阈值控制）验证假设。

Result: 截断数据集训练的模型表现与完整数据集相当或更优，解码策略优化后性能进一步提升，证实浅层偏好信号的广泛存在。

Conclusion: 当前对齐方法可能过度依赖回答前半部分的偏好信号，忽略完整响应的一致性，导致与真实人类偏好的偏差，需改进对齐策略。

Abstract: Aligning large language models (LLMs) with human preferences remains a key
challenge in AI. Preference-based optimization methods, such as Reinforcement
Learning with Human Feedback (RLHF) and Direct Preference Optimization (DPO),
rely on human-annotated datasets to improve alignment. In this work, we
identify a crucial property of the existing learning method: the distinguishing
signal obtained in preferred responses is often concentrated in the early
tokens. We refer to this as shallow preference signals.
  To explore this property, we systematically truncate preference datasets at
various points and train both reward models and DPO models on the truncated
data. Surprisingly, models trained on truncated datasets, retaining only the
first half or fewer tokens, achieve comparable or even superior performance to
those trained on full datasets. For example, a reward model trained on the
Skywork-Reward-Preference-80K-v0.2 dataset outperforms the full dataset when
trained on a 40\% truncated dataset. This pattern is consistent across multiple
datasets, suggesting the widespread presence of shallow preference signals.
  We further investigate the distribution of the reward signal through decoding
strategies. We consider two simple decoding strategies motivated by the shallow
reward signal observation, namely Length Control Decoding and KL Threshold
Control Decoding, which leverage shallow preference signals to optimize the
trade-off between alignment and computational efficiency. The performance is
even better, which again validates our hypothesis.
  The phenomenon of shallow preference signals highlights potential issues in
LLM alignment: existing alignment methods often focus on aligning only the
initial tokens of responses, rather than considering the full response. This
could lead to discrepancies with real-world human preferences, resulting in
suboptimal alignment performance.

</details>


### [58] [MTR-Bench: A Comprehensive Benchmark for Multi-Turn Reasoning Evaluation](https://arxiv.org/abs/2505.17123)
*Xiaoyuan Li,Keqin Bao,Yubo Ma,Moxin Li,Wenjie Wang,Rui Men,Yichang Zhang,Fuli Feng,Dayiheng Liu,Junyang Lin*

Key words: 大语言模型,多轮推理,交互式任务,自动评估,MTR-Bench

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了MTR-Bench，一个用于评估大语言模型多轮推理能力的基准数据集和自动评估框架，填补了现有研究在交互式任务上的空白。

Motivation: 当前对大语言模型的评估主要集中在单轮推理任务，缺乏对多轮交互式推理任务的全面评估，且缺少相关数据集和自动评估方法。

Method: 构建了包含4类、40个任务和3600个实例的MTR-Bench数据集，并开发了全自动的评估框架，支持无需人工干预的可扩展评估。

Result: 实验表明，即使最先进的推理模型在多轮交互式推理任务上也表现不佳，进一步分析为未来交互式AI系统研究提供了宝贵见解。

Conclusion: MTR-Bench为多轮交互推理任务提供了全面的评估工具，揭示了当前模型的不足，并指导未来研究方向。

Abstract: Recent advances in Large Language Models (LLMs) have shown promising results
in complex reasoning tasks. However, current evaluations predominantly focus on
single-turn reasoning scenarios, leaving interactive tasks largely unexplored.
We attribute it to the absence of comprehensive datasets and scalable automatic
evaluation protocols. To fill these gaps, we present MTR-Bench for LLMs'
Multi-Turn Reasoning evaluation. Comprising 4 classes, 40 tasks, and 3600
instances, MTR-Bench covers diverse reasoning capabilities, fine-grained
difficulty granularity, and necessitates multi-turn interactions with the
environments. Moreover, MTR-Bench features fully-automated framework spanning
both dataset constructions and model evaluations, which enables scalable
assessment without human interventions. Extensive experiments reveal that even
the cutting-edge reasoning models fall short of multi-turn, interactive
reasoning tasks. And the further analysis upon these results brings valuable
insights for future research in interactive AI systems.

</details>


### [59] [Conformal Language Model Reasoning with Coherent Factuality](https://arxiv.org/abs/2505.17126)
*Maxon Rubin-Toles,Maya Gambhir,Keshav Ramji,Aaron Roth,Surbhi Goel*

Key words: 语言模型、连贯事实性、共形预测、推理任务、可推导性图

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种基于共形预测的方法，用于保证语言模型在推理任务中输出的事实连贯性，通过在‘可推导性图’中应用分块共形预测，验证了其在数学推理任务中的有效性。

Motivation: 为确保语言模型在关键决策中的输出正确性，尤其是在推理任务中，现有方法无法验证逻辑论证的连贯性，因此提出了‘连贯事实性’的概念和方法。

Method: 使用‘可推导性图’表示推理步骤，并在其子图中应用分块共形预测技术，从而保证语言模型输出的连贯事实性。

Result: 在MATH和FELM数据集的数学推理任务中，该方法能持续生成正确且验证过的声明序列，并在严格定义下实现90%的事实性与80%以上的声明保留率。

Conclusion: 通过可推导性图引导的共形预测方法，显著提升了语言模型在复杂推理任务中的输出可靠性与连贯性。

Abstract: Language models are increasingly being used in important decision pipelines,
so ensuring the correctness of their outputs is crucial. Recent work has
proposed evaluating the "factuality" of claims decomposed from a language model
generation and applying conformal prediction techniques to filter out those
claims that are not factual. This can be effective for tasks such as
information retrieval, where constituent claims may be evaluated in isolation
for factuality, but is not appropriate for reasoning tasks, as steps of a
logical argument can be evaluated for correctness only within the context of
the claims that precede them. To capture this, we define "coherent factuality"
and develop a conformal-prediction-based method to guarantee coherent
factuality for language model outputs. Our approach applies split conformal
prediction to subgraphs within a "deducibility" graph" that represents the
steps of a reasoning problem. We evaluate our method on mathematical reasoning
problems from the MATH and FELM datasets and find that our algorithm
consistently produces correct and substantiated orderings of claims, achieving
coherent factuality across target coverage levels. Moreover, we achieve 90%
factuality on our stricter definition while retaining 80% or more of the
original claims, highlighting the utility of our deducibility-graph-guided
approach.

</details>


### [60] [Relative Bias: A Comparative Framework for Quantifying Bias in LLMs](https://arxiv.org/abs/2505.17131)
*Alireza Arbabi,Florian Kerschbaum*

Key words: 大型语言模型, 偏差分析, 相对偏差, 嵌入变换, 语言模型评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一个名为“相对偏差”的框架，用于评估大型语言模型（LLM）在特定目标领域内的行为偏差，并通过嵌入变换分析和语言模型作为评判工具两种方法，验证其在偏差分析中的有效性。

Motivation: 随着大型语言模型的广泛应用，其潜在的偏差问题引发了公平性、安全性和社会影响的担忧。量化这些偏差是一大挑战，尤其是对快速迭代的新模型缺乏系统性评估方法。

Method: 提出了相对偏差框架，包括嵌入变换分析（通过句子表征捕捉偏差模式）和语言模型作为评判工具（比较评估模型输出）。

Result: 在多个偏差和对齐案例中验证了框架的有效性，两种评分方法显示了高度一致性，为LLM的偏差分析提供了系统性、可扩展且统计可靠的方法。

Conclusion: 相对偏差框架为LLM的偏差量化提供了实用的工具，尤其适用于快速发展的新模型评估。

Abstract: The growing deployment of large language models (LLMs) has amplified concerns
regarding their inherent biases, raising critical questions about their
fairness, safety, and societal impact. However, quantifying LLM bias remains a
fundamental challenge, complicated by the ambiguity of what "bias" entails.
This challenge grows as new models emerge rapidly and gain widespread use,
while introducing potential biases that have not been systematically assessed.
In this paper, we propose the Relative Bias framework, a method designed to
assess how an LLM's behavior deviates from other LLMs within a specified target
domain. We introduce two complementary methodologies: (1) Embedding
Transformation analysis, which captures relative bias patterns through sentence
representations over the embedding space, and (2) LLM-as-a-Judge, which employs
a language model to evaluate outputs comparatively. Applying our framework to
several case studies on bias and alignment scenarios following by statistical
tests for validation, we find strong alignment between the two scoring methods,
offering a systematic, scalable, and statistically grounded approach for
comparative bias analysis in LLMs.

</details>


### [61] [LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions](https://arxiv.org/abs/2505.17134)
*Chaochen Gao,Xing Wu,Zijia Lin,Debing Zhang,Songlin Hu*

Key words: 长上下文指令数据、LLM对齐、自合成框架、LongMagpie

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: LongMagpie是一个自合成框架，用于自动生成大规模长上下文指令数据，解决了传统方法成本高、多样性差的问题。

Motivation: 现有方法（如人工标注或模板合成）生成长上下文指令数据成本高且质量有限，迫切需要一种高效、开放、多样化的解决方案。

Method: 利用对齐的长上下文LLM（如Qwen、Llama），通过特殊标记和自回归生成文档-查询对，自动合成指令数据。

Result: 在HELMET、RULER和Longbench v2上表现优异，长上下文任务领先，短上下文任务保持竞争力。

Conclusion: LongMagpie是一种简单高效的长上下文指令数据合成方法，具有开放性和可扩展性。

Abstract: High-quality long-context instruction data is essential for aligning
long-context large language models (LLMs). Despite the public release of models
like Qwen and Llama, their long-context instruction data remains proprietary.
Human annotation is costly and challenging, while template-based synthesis
methods limit scale, diversity, and quality. We introduce LongMagpie, a
self-synthesis framework that automatically generates large-scale long-context
instruction data. Our key insight is that aligned long-context LLMs, when
presented with a document followed by special tokens preceding a user turn,
auto-regressively generate contextually relevant queries. By harvesting these
document-query pairs and the model's responses, LongMagpie produces
high-quality instructions without human effort. Experiments on HELMET, RULER,
and Longbench v2 demonstrate that LongMagpie achieves leading performance on
long-context tasks while maintaining competitive performance on short-context
tasks, establishing it as a simple and effective approach for open, diverse,
and scalable long-context instruction data synthesis.

</details>


### [62] [When can isotropy help adapt LLMs' next word prediction to numerical domains?](https://arxiv.org/abs/2505.17135)
*Rashed Shelim,Shengzhe Xu,Walid Saad,Naren Ramakrishnan*

Key words: 大语言模型, 数值领域, 各向同性, 上下文嵌入, 性能保证

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文探讨了如何通过分析上下文嵌入空间的各向同性，将LLMs的下一个词预测能力适配到数值领域，以解决其幻觉问题并保证预测可靠性。

Motivation: 由于LLMs在数值领域容易产生幻觉，这可能对能源、金融、医疗等行业造成严重后果。论文旨在通过理论分析，理解LLMs何时能有效解决数值任务，并提供性能保证。

Method: 提出基于各向同性概念的分析框架，通过log-linear模型和softmax输出的网络结构，研究LLM隐藏表示的结构及其对数值预测的影响。

Result: 研究表明，LLM嵌入的各向同性属性能够保留表示的底层结构，解决平移不变性问题，并提供性能保证。实验显示不同数值数据和模型架构对各向同性有不同影响。

Conclusion: 通过各向同性分析，论文为LLMs在数值领域的应用提供了理论支持，并验证了其性能保证的有效性。

Abstract: Recent studies have shown that vector representations of contextual
embeddings learned by pre-trained large language models (LLMs) are effective in
various downstream tasks in numerical domains. Despite their significant
benefits, the tendency of LLMs to hallucinate in such domains can have severe
consequences in applications such as energy, nature, finance, healthcare,
retail and transportation, among others. To guarantee prediction reliability
and accuracy in numerical domains, it is necessary to open the black-box and
provide performance guarantees through explanation. However, there is little
theoretical understanding of when pre-trained language models help solve
numeric downstream tasks. This paper seeks to bridge this gap by understanding
when the next-word prediction capability of LLMs can be adapted to numerical
domains through a novel analysis based on the concept of isotropy in the
contextual embedding space. Specifically, we consider a log-linear model for
LLMs in which numeric data can be predicted from its context through a network
with softmax in the output layer of LLMs (i.e., language model head in
self-attention). We demonstrate that, in order to achieve state-of-the-art
performance in numerical domains, the hidden representations of the LLM
embeddings must possess a structure that accounts for the shift-invariance of
the softmax function. By formulating a gradient structure of self-attention in
pre-trained models, we show how the isotropic property of LLM embeddings in
contextual embedding space preserves the underlying structure of
representations, thereby resolving the shift-invariance problem and providing a
performance guarantee. Experiments show that different characteristics of
numeric data and model architecture could have different impacts on isotropy.

</details>


### [63] [Foundation Models for Geospatial Reasoning: Assessing Capabilities of Large Language Models in Understanding Geometries and Topological Spatial Relations](https://arxiv.org/abs/2505.17136)
*Yuhan Ji,Song Gao,Ying Nie,Ivan Majić,Krzysztof Janowicz*

Key words: 地理空间推理, WKT, 大型语言模型, GPT-4, 拓扑关系

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了如何通过WKT表示法和不同方法（几何嵌入、提示工程、日常语言评估）提升LLMs（如GPT系列）在地理空间推理任务中的表现，发现GPT-4在少量提示下表现最佳，为地理基础模型的发展提供了见解。

Motivation: 解决AI基础模型在处理地理空间数据时对地理实体（如矢量几何和复杂空间关系）表示与推理能力的不足。

Method: 采用几何嵌入、提示工程和日常语言评估三种方法，测试GPT-3.5-turbo、GPT-4和DeepSeek-R1-14B在空间推理任务中的表现。

Result: GPT-4在少量提示下拓扑空间关系推理准确率最高（>0.66），且能理解逆向拓扑关系，增加几何类型或地点类型上下文可提升准确性。

Conclusion: 研究表明LLMs在地理空间推理任务中具有潜力，为开发具备地理知识的基础模型提供了方向。

Abstract: Applying AI foundation models directly to geospatial datasets remains
challenging due to their limited ability to represent and reason with
geographical entities, specifically vector-based geometries and natural
language descriptions of complex spatial relations. To address these issues, we
investigate the extent to which a well-known-text (WKT) representation of
geometries and their spatial relations (e.g., topological predicates) are
preserved during spatial reasoning when the geospatial vector data are passed
to large language models (LLMs) including GPT-3.5-turbo, GPT-4, and
DeepSeek-R1-14B. Our workflow employs three distinct approaches to complete the
spatial reasoning tasks for comparison, i.e., geometry embedding-based, prompt
engineering-based, and everyday language-based evaluation. Our experiment
results demonstrate that both the embedding-based and prompt engineering-based
approaches to geospatial question-answering tasks with GPT models can achieve
an accuracy of over 0.6 on average for the identification of topological
spatial relations between two geometries. Among the evaluated models, GPT-4
with few-shot prompting achieved the highest performance with over 0.66
accuracy on topological spatial relation inference. Additionally, GPT-based
reasoner is capable of properly comprehending inverse topological spatial
relations and including an LLM-generated geometry can enhance the effectiveness
for geographic entity retrieval. GPT-4 also exhibits the ability to translate
certain vernacular descriptions about places into formal topological relations,
and adding the geometry-type or place-type context in prompts may improve
inference accuracy, but it varies by instance. The performance of these spatial
reasoning tasks offers valuable insights for the refinement of LLMs with
geographical knowledge towards the development of geo-foundation models capable
of geospatial reasoning.

</details>


### [64] [Cog-TiPRO: Iterative Prompt Refinement with LLMs to Detect Cognitive Decline via Longitudinal Voice Assistant Commands](https://arxiv.org/abs/2505.17137)
*Kristin Qi,Youxiang Zhu,Caroline Summerour,John A. Batsis,Xiaohui Liang*

Key words: 认知衰退检测,语音分析,轻量级模型,多模态特征,家庭监测

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 利用语音助手系统和非侵入式语音分析框架Cog-TiPRO，通过语言和声学特征检测轻度认知障碍，准确率达73.80%。

Motivation: 传统认知衰退检测方法依赖耗时临床评估，难以频繁监测。语音助手系统可提供非侵入式、长期的监测方案。

Method: 提出Cog-TiPRO框架，包括LLM驱动的语言特征提取、HuBERT声学特征提取和基于iTransformer的时序建模。

Result: 检测轻度认知障碍的准确率为73.80%，F1分数为72.67%，比基线提升27.13%。

Conclusion: 语音助手系统结合多模态特征分析能有效检测认知衰退，为家庭监测提供新途径。

Abstract: Early detection of cognitive decline is crucial for enabling interventions
that can slow neurodegenerative disease progression. Traditional diagnostic
approaches rely on labor-intensive clinical assessments, which are impractical
for frequent monitoring. Our pilot study investigates voice assistant systems
(VAS) as non-invasive tools for detecting cognitive decline through
longitudinal analysis of speech patterns in voice commands. Over an 18-month
period, we collected voice commands from 35 older adults, with 15 participants
providing daily at-home VAS interactions. To address the challenges of
analyzing these short, unstructured and noisy commands, we propose Cog-TiPRO, a
framework that combines (1) LLM-driven iterative prompt refinement for
linguistic feature extraction, (2) HuBERT-based acoustic feature extraction,
and (3) transformer-based temporal modeling. Using iTransformer, our approach
achieves 73.80% accuracy and 72.67% F1-score in detecting MCI, outperforming
its baseline by 27.13%. Through our LLM approach, we identify linguistic
features that uniquely characterize everyday command usage patterns in
individuals experiencing cognitive decline.

</details>


### [65] [EarthSE: A Benchmark Evaluating Earth Scientific Exploration Capability for Large Language Models](https://arxiv.org/abs/2505.17139)
*Wanghan Xu,Xiangyu Zhao,Yuhao Zhou,Xiaoyu Yue,Ben Fei,Fenghua Ling,Wenlong Zhang,Lei Bai*

Key words: 大型语言模型, 地球科学, 基准测试, 问答数据集, 科学探索

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出一个全面的地球科学基准测试，用于评估大型语言模型（LLMs）在科学探索中的能力，涵盖了从基础到高级的任务。

Motivation: 现有基准测试缺乏地球科学的专门性评估，且忽略了对LLMs在开放式科学探索中能力的测试，故需开发此类专业基准。

Method: 基于10万篇研究论文构建了两个问答数据集（Earth-Iron和Earth-Silver），并设计了开放式的多轮对话数据集Earth-Gold，用于评估LLMs的高级能力。

Result: 实验发现11种主流LLMs在不同领域和任务中存在局限性，其科学探索能力有显著提升空间。

Conclusion: 该基准测试为LLMs在地球科学领域的评估提供了全面工具，揭示了当前模型的不足。

Abstract: Advancements in Large Language Models (LLMs) drive interest in scientific
applications, necessitating specialized benchmarks such as Earth science.
Existing benchmarks either present a general science focus devoid of Earth
science specificity or cover isolated subdomains, lacking holistic evaluation.
Furthermore, current benchmarks typically neglect the assessment of LLMs'
capabilities in open-ended scientific exploration. In this paper, we present a
comprehensive and professional benchmark for the Earth sciences, designed to
evaluate the capabilities of LLMs in scientific exploration within this domain,
spanning from fundamental to advanced levels. Leveraging a corpus of 100,000
research papers, we first construct two Question Answering (QA) datasets:
Earth-Iron, which offers extensive question coverage for broad assessment, and
Earth-Silver, which features a higher level of difficulty to evaluate
professional depth. These datasets encompass five Earth spheres, 114
disciplines, and 11 task categories, assessing foundational knowledge crucial
for scientific exploration. Most notably, we introduce Earth-Gold with new
metrics, a dataset comprising open-ended multi-turn dialogues specifically
designed to evaluate the advanced capabilities of LLMs in scientific
exploration, including methodology induction, limitation analysis, and concept
proposal. Extensive experiments reveal limitations in 11 leading LLMs across
different domains and tasks, highlighting considerable room for improvement in
their scientific exploration capabilities. The benchmark is available on
https://huggingface.co/ai-earth .

</details>


### [66] [Data Doping or True Intelligence? Evaluating the Transferability of Injected Knowledge in LLMs](https://arxiv.org/abs/2505.17140)
*Essa Jan,Moiz Ali,Muhammad Saram Hassan,Fareed Zaffar,Yasir Zaki*

Key words: 大型语言模型,知识更新,微调任务,知识保留,语义整合

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究表明，理解密集型微调任务比映射型任务能更高效地更新大型语言模型的知识，但模型在更广泛语境中的应用表现仍有局限。

Motivation: 随着大型语言模型的知识逐渐过时，如何高效更新（尤其是注入专有信息）成为重要课题。

Method: 通过对比不同微调任务（如问答、翻译、文本转JSON）的知识保留率，分析任务类型对知识注入效果的影响。

Result: 理解密集型任务（保留率48%）显著优于映射型任务（17%-20%），且模型越大保留效果越好，但语义整合能力有限。

Conclusion: 知识注入的效果不仅取决于数据暴露，更依赖于微调时的认知参与深度，任务选择至关重要。

Abstract: As the knowledge of large language models (LLMs) becomes outdated over time,
there is a growing need for efficient methods to update them, especially when
injecting proprietary information. Our study reveals that
comprehension-intensive fine-tuning tasks (e.g., question answering and blanks)
achieve substantially higher knowledge retention rates (48%) compared to
mapping-oriented tasks like translation (17%) or text-to-JSON conversion (20%),
despite exposure to identical factual content. We demonstrate that this pattern
persists across model architectures and follows scaling laws, with larger
models showing improved retention across all task types. However, all models
exhibit significant performance drops when applying injected knowledge in
broader contexts, suggesting limited semantic integration. These findings show
the importance of task selection in updating LLM knowledge, showing that
effective knowledge injection relies not just on data exposure but on the depth
of cognitive engagement during fine-tuning.

</details>


### [67] [MDIT-Bench: Evaluating the Dual-Implicit Toxicity in Large Multimodal Models](https://arxiv.org/abs/2505.17144)
*Bohan Jin,Shuhan Qi,Kehai Chen,Xinyi Guo,Xuan Wang*

Key words: 大型多模态模型,双隐式毒性,MDIT-Bench,隐含偏见,模型评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了双隐式毒性和MDIT-Bench基准，评估大型多模态模型对隐含偏见的敏感性，实验显示模型在高难度下表现显著下降。

Motivation: 当前研究主要关注显性毒性，忽略隐含偏见，需填补这一空白。

Method: 提出多阶段人机交互上下文生成方法创建MDIT-Dataset，并构建包含三个难度级别的MDIT-Bench。

Result: 实验显示13个主流LMM在双隐式毒性上表现不佳，尤其在困难级别。

Conclusion: LMM仍含有大量可激活的隐含毒性，需进一步关注和改进。

Abstract: The widespread use of Large Multimodal Models (LMMs) has raised concerns
about model toxicity. However, current research mainly focuses on explicit
toxicity, with less attention to some more implicit toxicity regarding
prejudice and discrimination. To address this limitation, we introduce a
subtler type of toxicity named dual-implicit toxicity and a novel toxicity
benchmark termed MDIT-Bench: Multimodal Dual-Implicit Toxicity Benchmark.
Specifically, we first create the MDIT-Dataset with dual-implicit toxicity
using the proposed Multi-stage Human-in-loop In-context Generation method.
Based on this dataset, we construct the MDIT-Bench, a benchmark for evaluating
the sensitivity of models to dual-implicit toxicity, with 317,638 questions
covering 12 categories, 23 subcategories, and 780 topics. MDIT-Bench includes
three difficulty levels, and we propose a metric to measure the toxicity gap
exhibited by the model across them. In the experiment, we conducted MDIT-Bench
on 13 prominent LMMs, and the results show that these LMMs cannot handle
dual-implicit toxicity effectively. The model's performance drops significantly
in hard level, revealing that these LMMs still contain a significant amount of
hidden but activatable toxicity. Data are available at
https://github.com/nuo1nuo/MDIT-Bench.

</details>


### [68] [Large Language Models for Predictive Analysis: How Far Are They?](https://arxiv.org/abs/2505.17149)
*Qin Chen,Yuanyi Ren,Xiaojun Ma,Yuyang Shi*

Key words: 预测分析, 大语言模型, PredictiQ, 评估协议

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了PredictiQ基准测试，评估12个知名LLM在预测分析中的表现，发现现有模型仍面临挑战。

Motivation: 现有研究缺乏对LLM在预测分析领域能力的系统评估，因此需要填补这一空白。

Method: 整合1130个复杂预测分析查询，覆盖8个领域44个数据集，设计包含文本分析和代码生成的评估协议。

Result: 评估显示现有LLM在预测分析任务中仍存在显著挑战。

Conclusion: LLM在预测分析中的实际应用仍需进一步改进。

Abstract: Predictive analysis is a cornerstone of modern decision-making, with
applications in various domains. Large Language Models (LLMs) have emerged as
powerful tools in enabling nuanced, knowledge-intensive conversations, thus
aiding in complex decision-making tasks. With the burgeoning expectation to
harness LLMs for predictive analysis, there is an urgent need to systematically
assess their capability in this domain. However, there is a lack of relevant
evaluations in existing studies. To bridge this gap, we introduce the
\textbf{PredictiQ} benchmark, which integrates 1130 sophisticated predictive
analysis queries originating from 44 real-world datasets of 8 diverse fields.
We design an evaluation protocol considering text analysis, code generation,
and their alignment. Twelve renowned LLMs are evaluated, offering insights into
their practical use in predictive analysis. Generally, we believe that existing
LLMs still face considerable challenges in conducting predictive analysis. See
\href{https://github.com/Cqkkkkkk/PredictiQ}{Github}.

</details>


### [69] [Bayesian Optimization for Enhanced Language Models: Optimizing Acquisition Functions](https://arxiv.org/abs/2505.17151)
*Zishuo Bao,Yibo Liu,Changyutao Qiu*

Key words: 贝叶斯优化, 大语言模型, 微调, 获取函数, GLUE任务

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种名为Bilevel-BO-SWA的方法，通过双层贝叶斯优化策略改进大语言模型的微调，结合了多种获取函数（如EI和UCB），在GLUE任务上实现最高2.7%的性能提升。

Motivation: 现有贝叶斯优化方法忽略了获取函数对训练和验证性能敏感性的影响，导致微调效果不理想。

Method: 采用双层贝叶斯优化策略，内层循环最小化训练损失，外层循环优化验证指标，并结合多种获取函数。

Result: 在RoBERTa-base模型和GLUE任务上，使用EI和UCB获取函数可提升2.7%的泛化性能。

Conclusion: Bilevel-BO-SWA能有效提升大语言模型的微调效果，尤其是在获取函数选择上表现更优。

Abstract: With the rise of different language model architecture, fine-tuning is
becoming even more important for down stream tasks Model gets messy, finding
proper hyperparameters for fine-tuning. Although BO has been tried for
hyperparameter tuning, most of the existing methods are oblivious to the fact
that BO relies on careful choices of acquisition functions, which are essential
components of BO that guide how much to explore versus exploit during the
optimization process; Different acquisition functions have different levels of
sensitivity towards training loss and validation performance; existing methods
often just apply an acquisition function no matter if the training and
validation performance are sensitive to the acquisition function or not. This
work introduces{Bilevel - BO - SWA}, a model fusion approach coupled with a
bilevel BO strategy to improve the fine - tunning of large language models. Our
work on mixture of acquisition functions like EI and UCB into nested opt loops,
where inner loop perform minimization of training loss while outer loops
optimized w.r.t. val metric. Experiments on GLUE tasks using RoBERTA - base
show that when using EI and UCB, there is an improvement in generalization, and
fine - tuning can be improved by up to 2.7%.

</details>


### [70] [Amplify Adjacent Token Differences: Enhancing Long Chain-of-Thought Reasoning with Shift-FFN](https://arxiv.org/abs/2505.17153)
*Yao Xu,Mingyu Xu,Fangyu Lei,Wangtao Sun,Xiangrong Zeng,Bingning Wang,Guang Liu,Shizhu He,Jun Zhao,Kang Liu*

Key words: 长链思维推理, 循环推理, Shift-FFN, LoRA, 微调

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文探讨了长链思维推理中模型循环推理的问题，并提出了一种名为Shift-FFN的新方法以减少循环推理。

Motivation: 研究发现在微调LLM时，模型容易陷入循环推理，导致性能下降。

Method: 提出Shift-FFN，在输入FFN前调整当前token的表示，以增强相邻token间的表示差异。

Result: 在多种数学推理任务中，Shift-FFN结合LoRA相比全参数微调或标准LoRA，实现了更高准确率和更低循环推理率。

Conclusion: Shift-FFN有效地减少了循环推理问题并提升了模型性能。

Abstract: Recently, models such as OpenAI-o1 and DeepSeek-R1 have demonstrated
remarkable performance on complex reasoning tasks through Long Chain-of-Thought
(Long-CoT) reasoning. Although distilling this capability into student models
significantly enhances their performance, this paper finds that fine-tuning
LLMs with full parameters or LoRA with a low rank on long CoT data often leads
to Cyclical Reasoning, where models repeatedly reiterate previous inference
steps until the maximum length limit. Further analysis reveals that smaller
differences in representations between adjacent tokens correlates with a higher
tendency toward Cyclical Reasoning. To mitigate this issue, this paper proposes
Shift Feedforward Networks (Shift-FFN), a novel approach that edits the current
token's representation with the previous one before inputting it to FFN. This
architecture dynamically amplifies the representation differences between
adjacent tokens. Extensive experiments on multiple mathematical reasoning tasks
demonstrate that LoRA combined with Shift-FFN achieves higher accuracy and a
lower rate of Cyclical Reasoning across various data sizes compared to full
fine-tuning and standard LoRA. Our data and code are available at
https://anonymous.4open.science/r/Shift-FFN

</details>


### [71] [PersonaBOT: Bringing Customer Personas to Life with LLMs and RAG](https://arxiv.org/abs/2505.17156)
*Muhammed Rizwan,Lars Carlsson,Mohammad Loni*

Key words: LLMs, 客户角色, RAG, Few-Shot, CoT, 聊天机器人

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 生成合成客户角色并将其与RAG聊天机器人结合，Few-Shot提示在生成完整角色方面优于CoT，系统实用性获得81.82%认可。

Motivation: 传统客户角色开发方法耗时且缺乏扩展性，LLMs为改进提供可能性，旨在增强企业决策支持。

Method: 开发基于角色验证的RAG聊天机器人，利用Few-Shot和CoT技术生成合成角色，评估完整性、相关性和一致性，更新知识库并测试改进。

Result: Few-Shot生成的更完整但耗时，CoT更高效；聊天机器人准确性从5.88提升至6.42，81.82%参与者认为实用。

Conclusion: 合成角色与RAG结合显著提升系统性能，Few-Shot与CoT各有所长，可针对性应用。

Abstract: The introduction of Large Language Models (LLMs) has significantly
transformed Natural Language Processing (NLP) applications by enabling more
advanced analysis of customer personas. At Volvo Construction Equipment (VCE),
customer personas have traditionally been developed through qualitative
methods, which are time-consuming and lack scalability. The main objective of
this paper is to generate synthetic customer personas and integrate them into a
Retrieval-Augmented Generation (RAG) chatbot to support decision-making in
business processes. To this end, we first focus on developing a persona-based
RAG chatbot integrated with verified personas. Next, synthetic personas are
generated using Few-Shot and Chain-of-Thought (CoT) prompting techniques and
evaluated based on completeness, relevance, and consistency using McNemar's
test. In the final step, the chatbot's knowledge base is augmented with
synthetic personas and additional segment information to assess improvements in
response accuracy and practical utility. Key findings indicate that Few-Shot
prompting outperformed CoT in generating more complete personas, while CoT
demonstrated greater efficiency in terms of response time and token usage.
After augmenting the knowledge base, the average accuracy rating of the chatbot
increased from 5.88 to 6.42 on a 10-point scale, and 81.82% of participants
found the updated system useful in business contexts.

</details>


### [72] [Harry Potter is Still Here! Probing Knowledge Leakage in Targeted Unlearned Large Language Models via Automated Adversarial Prompting](https://arxiv.org/abs/2505.17160)
*Bang Trinh Tran To,Thai Le*

Key words: LURK, 未学习LLM, 对抗性提示, 残留知识, 评估标准

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: LURK 框架通过对抗性后缀提示探测未学习LLM中隐藏的遗留知识，揭示了当前未学习评估标准的局限性。

Motivation: 研究动机在于探究未学习模型中可能残留的知识，并评估未学习算法的稳健性，尤其是针对像《哈利波特》这样的常用基准领域。

Method: 采用对抗性后缀提示的方法，自动生成提示后缀以探测模型在《哈利波特》领域的残留知识。

Result: 实验发现，即使被认为成功未学习的模型，在针对性对抗条件下仍可能泄露独特信息。

Conclusion: LURK 提供了一个更严格、诊断性更强的工具来评估未学习算法的效果，揭示了当前评估标准的不足。

Abstract: This work presents LURK (Latent UnleaRned Knowledge), a novel framework that
probes for hidden retained knowledge in unlearned LLMs through adversarial
suffix prompting. LURK automatically generates adversarial prompt suffixes
designed to elicit residual knowledge about the Harry Potter domain, a commonly
used benchmark for unlearning. Our experiments reveal that even models deemed
successfully unlearned can leak idiosyncratic information under targeted
adversarial conditions, highlighting critical limitations of current unlearning
evaluation standards. By uncovering latent knowledge through indirect probing,
LURK offers a more rigorous and diagnostic tool for assessing the robustness of
unlearning algorithms. All code will be publicly available.

</details>


### [73] [CRG Score: A Distribution-Aware Clinical Metric for Radiology Report Generation](https://arxiv.org/abs/2505.17167)
*Ibrahim Ethem Hamamci,Sezgin Er,Suprosanna Shit,Hadrien Reynaud,Bernhard Kainz,Bjoern Menze*

Key words: 放射学报告生成、临床正确性、评估指标、类别不平衡、CRG Score

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种新的评分标准CRG Score，用于评估放射学报告生成的临床正确性，解决了现有方法在临床相关性和类别不平衡上的问题。

Motivation: 现有评估方法在衡量放射学报告生成的临床正确性时存在不足，NLG指标无法捕捉临床正确性，而基于LLM的指标缺乏普适性，临床准确性指标又受类别不平衡影响。

Method: 提出了CRG Score，一种分布感知且适应性强的指标，仅评估参考报告中明确描述的临床相关异常，支持二元和结构化标签，并能与任何LLM结合用于特征提取。

Result: CRG Score通过基于标签分布的惩罚平衡，实现了更公平、更稳健的评估，并可作为临床对齐的奖励函数。

Conclusion: CRG Score为放射学报告生成提供了一个更准确、适应性强的评估工具，解决了现有方法的局限性。

Abstract: Evaluating long-context radiology report generation is challenging. NLG
metrics fail to capture clinical correctness, while LLM-based metrics often
lack generalizability. Clinical accuracy metrics are more relevant but are
sensitive to class imbalance, frequently favoring trivial predictions. We
propose the CRG Score, a distribution-aware and adaptable metric that evaluates
only clinically relevant abnormalities explicitly described in reference
reports. CRG supports both binary and structured labels (e.g., type, location)
and can be paired with any LLM for feature extraction. By balancing penalties
based on label distribution, it enables fairer, more robust evaluation and
serves as a clinically aligned reward function.

</details>


### [74] [Next Token Perception Score: Analytical Assessment of your LLM Perception Skills](https://arxiv.org/abs/2505.17169)
*Yu-Ang Cheng,Leyang Hu,Hai Huang,Randall Balestriero*

Key words: 自回归预训练, 感知任务, NTPS, LoRA微调, 特征对齐

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出Next Token Perception Score (NTPS)衡量自回归预训练与下游感知任务的对齐程度，证实其与线性探测准确性相关，并可预测LoRA微调的效果。

Motivation: 自回归预训练在大型语言模型中广泛使用，但其特征在下游感知任务中的表现存在显著差异，表明优化下一个词预测的特征未必能很好地迁移到感知任务。

Method: 引入NTPS，通过线性设定衡量自回归与感知特征子空间的重叠程度，并利用预训练表示和标记数据计算。

Result: NTPS与12个NLP数据集和8个预训练模型的线性探测准确性强相关，且LoRA微调后NTPS提升，尤其在大型模型中。

Conclusion: NTPS为理论和实践提供了分析LLM感知能力的工具，可轻量级预筛选LoRA微调效果。

Abstract: Autoregressive pretraining has become the de facto paradigm for learning
general-purpose representations in large language models (LLMs). However,
linear probe performance across downstream perception tasks shows substantial
variability, suggesting that features optimized for next-token prediction do
not consistently transfer well to downstream perception tasks. We demonstrate
that representations learned via autoregression capture features that may lie
outside the subspaces most informative for perception. To quantify the
(mis)alignment between autoregressive pretraining and downstream perception, we
introduce the Next Token Perception Score (NTPS)-a score derived under a linear
setting that measures the overlap between autoregressive and perception feature
subspaces. This metric can be easily computed in closed form from pretrained
representations and labeled data, and is proven to both upper- and lower-bound
the excess loss. Empirically, we show that NTPS correlates strongly with linear
probe accuracy across 12 diverse NLP datasets and eight pretrained models
ranging from 270M to 8B parameters, confirming its utility as a measure of
alignment. Furthermore, we show that NTPS increases following low-rank
adaptation (LoRA) fine-tuning, especially in large models, suggesting that LoRA
aligning representations to perception tasks enhances subspace overlap and thus
improves downstream performance. More importantly, we find that NTPS reliably
predicts the additional accuracy gains attained by LoRA finetuning thereby
providing a lightweight prescreening tool for LoRA adaptation. Our results
offer both theoretical insights and practical tools for analytically assessing
LLM perception skills.

</details>


### [75] [FB-RAG: Improving RAG with Forward and Backward Lookup](https://arxiv.org/abs/2505.17206)
*Kushal Chawla,Alfy Samuel,Anoop Kumar,Daben Liu*

Key words: 检索增强生成（RAG）、上下文检索、FB-RAG、长上下文、性能优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出FB-RAG框架，通过结合反向和正向查找优化RAG系统的上下文检索，显著提升性能并降低延迟。

Motivation: 传统RAG系统在检索上下文时面临规模与相关性的权衡，复杂查询尤其容易因检索不精准导致性能下降。

Method: 提出FB-RAG框架，结合反向查找（与查询重叠）和正向查找（与候选答案重叠）来精准检索相关上下文片段。

Result: 在9个数据集上，FB-RAG优于RAG和长上下文基线模型，同时降低延迟。定性分析展示了其优势和不足。

Conclusion: FB-RAG通过改进检索策略有效解决RAG的上下文质量和规模矛盾，为未来研究提供方向。

Abstract: The performance of Retrieval Augmented Generation (RAG) systems relies
heavily on the retriever quality and the size of the retrieved context. A large
enough context ensures that the relevant information is present in the input
context for the LLM, but also incorporates irrelevant content that has been
shown to confuse the models. On the other hand, a smaller context reduces the
irrelevant information, but it often comes at the risk of losing important
information necessary to answer the input question. This duality is especially
challenging to manage for complex queries that contain little information to
retrieve the relevant chunks from the full context. To address this, we present
a novel framework, called FB-RAG, which enhances the RAG pipeline by relying on
a combination of backward lookup (overlap with the query) and forward lookup
(overlap with candidate reasons and answers) to retrieve specific context
chunks that are the most relevant for answering the input query. Our
evaluations on 9 datasets from two leading benchmarks show that FB-RAG
consistently outperforms RAG and Long Context baselines developed recently for
these benchmarks. We further show that FB-RAG can improve performance while
reducing latency. We perform qualitative analysis of the strengths and
shortcomings of our approach, providing specific insights to guide future work.

</details>


### [76] [Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs](https://arxiv.org/abs/2505.17217)
*Kangda Wei,Hasnat Md Abdullah,Ruihong Huang*

Key words: 大语言模型, 性别偏见, 道德判断, 数据生成, DPO

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出一种通过生成性别中立故事对及道德判断来减少LLMs性别偏见的框架，采用DPO优化模型，实验证明有效减轻偏见且不影响模型性能。

Motivation: LLMs存在性别偏见问题，导致对不同性别主体的处理不平等，需探索解决方案。

Method: 设计数据生成框架，创建结构相同但主角性别不同的道德模糊情境故事对，引导模型生成性别中立的判断，并用DPO优化模型。

Result: 方法显著减少性别偏见，同时保持或提升模型整体能力。

Conclusion: 框架有效平衡LLMs的性别偏见，代码与生成数据将开源。

Abstract: Large Language Models (LLMs) often exhibit gender bias, resulting in unequal
treatment of male and female subjects across different contexts. To address
this issue, we propose a novel data generation framework that fosters
exploratory thinking in LLMs. Our approach prompts models to generate story
pairs featuring male and female protagonists in structurally identical, morally
ambiguous scenarios, then elicits and compares their moral judgments. When
inconsistencies arise, the model is guided to produce balanced, gender-neutral
judgments. These story-judgment pairs are used to fine-tune or optimize the
models via Direct Preference Optimization (DPO). Experimental results show that
our method significantly reduces gender bias while preserving or even enhancing
general model capabilities. We will release the code and generated data.

</details>


### [77] [Humans Hallucinate Too: Language Models Identify and Correct Subjective Annotation Errors With Label-in-a-Haystack Prompts](https://arxiv.org/abs/2505.17222)
*Georgios Chochlakis,Peter Wu,Arjun Bedi,Marcus Ma,Kristina Lerman,Shrikanth Narayanan*

Key words: 自然语言处理, 主观任务, 大语言模型, 标签修正, 上下文学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇论文提出了一种名为Label-in-a-Haystack Rectification (LiaHR)的框架，用于解决自然语言处理中主观任务标签的验证与修正问题。通过大语言模型（LLMs）的上下文学习，区分合理的主观差异与标注错误，从而提升标签质量。

Motivation: 研究中主观任务（如情感和道德识别）的标注存在显著变异性，这些变异性通常反映了语义解释的合理差异而非噪声。因此，需要方法区分合理主观性与标注错误。

Method: 首先提出了一种基于上下文学习的二元过滤基线方法，评估文档-标签对的合理性。接着引入Label-in-a-Haystack设置，利用LLMs在任务特定指令下预测标签，并通过其输出与参考标签的差异识别任务相关信息。最后提出LiaHR框架修正标签。

Result: 通过综合分析、人工评估和生态效度研究，验证了LiaHR在标签修正中的有效性，能显著提升信号与噪声比。

Conclusion: LiaHR框架能有效识别并修正主观任务中的标签错误，可集成到标注流程中以提升数据质量。

Abstract: Modeling complex subjective tasks in Natural Language Processing, such as
recognizing emotion and morality, is considerably challenging due to
significant variation in human annotations. This variation often reflects
reasonable differences in semantic interpretations rather than mere noise,
necessitating methods to distinguish between legitimate subjectivity and error.
We address this challenge by exploring label verification in these contexts
using Large Language Models (LLMs). First, we propose a simple In-Context
Learning binary filtering baseline that estimates the reasonableness of a
document-label pair. We then introduce the Label-in-a-Haystack setting: the
query and its label(s) are included in the demonstrations shown to LLMs, which
are prompted to predict the label(s) again, while receiving task-specific
instructions (e.g., emotion recognition) rather than label copying. We show how
the failure to copy the label(s) to the output of the LLM are task-relevant and
informative. Building on this, we propose the Label-in-a-Haystack Rectification
(LiaHR) framework for subjective label correction: when the model outputs
diverge from the reference gold labels, we assign the generated labels to the
example instead of discarding it. This approach can be integrated into
annotation pipelines to enhance signal-to-noise ratios. Comprehensive analyses,
human evaluations, and ecological validity studies verify the utility of LiaHR
for label correction. Code is available at https://github.com/gchochla/LiaHR.

</details>


### [78] [ExeSQL: Self-Taught Text-to-SQL Models with Execution-Driven Bootstrapping for SQL Dialects](https://arxiv.org/abs/2505.17231)
*Jipeng Zhang,Haolin Yang,Kehao Miao,Ruiyuan Zhang,Renjie Pi,Jiahui Gao,Xiaofang Zhou*

Key words: 文本到 SQL, SQL 方言, 执行驱动学习, 代理引导, 反馈训练

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: ExeSQL 框架通过执行驱动的代理引导方法，解决了文本到 SQL 模型在多方言环境中的挑战，显著提升了在 PostgreSQL、MySQL 和 Oracle 上的表现。

Motivation: 现有文本到 SQL 模型的性能受限于 SQLite 数据集，难以适应多方言的真实应用场景，且静态生成的数据缺乏可靠性和可执行验证，导致泛化能力不足。

Method: ExeSQL 采用迭代查询生成、基于执行的过滤（如拒绝采样）和偏好训练，通过可验证的反馈学习来适应新 SQL 方言。

Result: ExeSQL 在 PostgreSQL、MySQL 和 Oracle 上分别比 GPT-4o 平均提升 15.2%、10.38% 和 4.49%，有效缩小了方言间的性能差距。

Conclusion: 执行驱动的训练框架显著提高了文本到 SQL 模型在多方言环境中的泛化能力，验证了反馈引导学习的重要性。

Abstract: Recent text-to-SQL models have achieved strong performance, but their
effectiveness remains largely confined to SQLite due to dataset limitations.
However, real-world applications require SQL generation across multiple
dialects with varying syntax and specialized features, which remains a
challenge for current models. The main obstacle in building a dialect-aware
model lies in acquiring high-quality dialect-specific data. Data generated
purely through static prompting - without validating SQLs via execution - tends
to be noisy and unreliable. Moreover, the lack of real execution environments
in the training loop prevents models from grounding their predictions in
executable semantics, limiting generalization despite surface-level
improvements from data filtering. This work introduces ExeSQL, a text-to-SQL
framework with execution-driven, agentic bootstrapping. The method consists of
iterative query generation, execution-based filtering (e.g., rejection
sampling), and preference-based training, enabling the model to adapt to new
SQL dialects through verifiable, feedback-guided learning. Experiments show
that ExeSQL bridges the dialect gap in text-to-SQL, achieving average
improvements of 15.2%, 10.38%, and 4.49% over GPT-4o on PostgreSQL, MySQL, and
Oracle, respectively, across multiple datasets of varying difficulty.

</details>


### [79] [Personalizing Student-Agent Interactions Using Log-Contextualized Retrieval Augmented Generation (RAG)](https://arxiv.org/abs/2505.17238)
*Clayton Cohn,Surya Rayala,Caitlin Snyder,Joyce Fonteles,Shruti Jain,Naveeduddin Mohammed,Umesh Timalsina,Sarah K. Burriss,Ashwin T S,Namrata Srivastava,Menton Deweese,Angela Eeds,Gautam Biswas*

Key words: 合作对话，RAG，教育代理，STEM+C，批判性思维

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为LC-RAG的方法，通过结合环境日志改进RAG检索，以提升合作对话中教育代理的个性化指导效果。

Motivation: 合作对话能深入反映学生的学习与批判性思维，但在STEM+C领域中，大型语言模型（LLMs）的动态交互可能因幻觉问题降低信任与教学价值。传统RAG方法依赖清晰的语义链接，而学生对话中这种链接往往较弱。

Method: 提出Log-Contextualized RAG（LC-RAG），通过整合环境日志增强RAG的检索能力，以更好地理解合作对话的上下文。

Result: LC-RAG在检索效果上优于仅基于对话的基线方法，其协作学习代理Copa能提供更相关、个性化的指导，支持学生在XYZ协作计算建模环境中的批判性思维和认知决策。

Conclusion: LC-RAG通过上下文增强的检索机制，有效提升了教育代理在合作学习中的实用性和可信度，推动了学生在复杂学习环境中的认知发展。

Abstract: Collaborative dialogue offers rich insights into students' learning and
critical thinking. This is essential for adapting pedagogical agents to
students' learning and problem-solving skills in STEM+C settings. While large
language models (LLMs) facilitate dynamic pedagogical interactions, potential
hallucinations can undermine confidence, trust, and instructional value.
Retrieval-augmented generation (RAG) grounds LLM outputs in curated knowledge,
but its effectiveness depends on clear semantic links between user input and a
knowledge base, which are often weak in student dialogue. We propose
log-contextualized RAG (LC-RAG), which enhances RAG retrieval by incorporating
environment logs to contextualize collaborative discourse. Our findings show
that LC-RAG improves retrieval over a discourse-only baseline and allows our
collaborative peer agent, Copa, to deliver relevant, personalized guidance that
supports students' critical thinking and epistemic decision-making in a
collaborative computational modeling environment, XYZ.

</details>


### [80] [ReasoningShield: Content Safety Detection over Reasoning Traces of Large Reasoning Models](https://arxiv.org/abs/2505.17244)
*Changyi Li,Jiayi Wang,Xudong Pan,Geng Hong,Min Yang*

Key words: 大推理模型, 推理轨迹, 安全检测, ReasoningShield, 人机协作标注

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 大推理模型（LRMs）在AI领域展现出强大的推理能力，但其生成的推理轨迹可能隐含不安全内容。现有QA内容管理工具对此无效。为此，论文提出了首个针对推理轨迹安全检测的模型ReasoningShield，并构建了一个高质量数据集，模型性能显著优于主流安全检测模型。

Motivation: 大推理模型生成的推理轨迹可能隐含不安全内容，而现有QA内容管理工具无法有效检测这些隐藏风险，因此需要专门的QT内容管理任务和模型。

Method: 论文提出了ReasoningShield模型，并构建了一个包含8000多个QT对的数据集，采用人机协作标注流程，标注准确率超过93%。模型基于1B/3B轻量级基础模型，便于部署。

Result: ReasoningShield在多样化的基准测试中平均F1分数超过0.92，显著优于现有安全检测模型。此外，该模型在传统QA基准测试中也表现出色。

Conclusion: ReasoningShield首次解决了推理轨迹中的安全隐患，数据集和模型的高质量验证了其有效性，同时支持轻量级部署和人性化风险分析。

Abstract: Large Reasoning Models (LRMs) are transforming the AI landscape with advanced
reasoning capabilities. While the generated reasoning traces enhance model
transparency, they can still contain unsafe content, even when the final answer
appears safe. Existing moderation tools, primarily designed for question-answer
(QA) pairs, are empirically ineffective at detecting hidden risks embedded in
reasoning traces. After identifying the key challenges, we formally define the
question-thought (QT) moderation task and propose ReasoningShield, the first
safety detection model tailored to identify potential risks in the reasoning
trace before reaching the final answer. To construct the model, we synthesize a
high-quality reasoning safety detection dataset comprising over 8,000
question-thought pairs spanning ten risk categories and three safety levels.
Our dataset construction process incorporates a comprehensive human-AI
collaborative annotation pipeline, which achieves over 93% annotation accuracy
while significantly reducing human costs. On a diverse set of in-distribution
and out-of-distribution benchmarks, ReasoningShield outperforms mainstream
content safety moderation models in identifying risks within reasoning traces,
with an average F1 score exceeding 0.92. Notably, despite being trained on our
QT dataset only, ReasoningShield also demonstrates competitive performance in
detecting unsafe question-answer pairs on traditional benchmarks, rivaling
baselines trained on 10 times larger datasets and base models, which strongly
validates the quality of our dataset. Furthermore, ReasoningShield is built
upon compact 1B/3B base models to facilitate lightweight deployment and
provides human-friendly risk analysis by default. To foster future research, we
publicly release all the resources.

</details>


### [81] [ConciseRL: Conciseness-Guided Reinforcement Learning for Efficient Reasoning Models](https://arxiv.org/abs/2505.17250)
*Razvan-Gabriel Dumitru,Darius Peteleaza,Vikas Yadav,Liangming Pan*

Key words: 大语言模型, 强化学习, 推理效率, 简洁度评分

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出一种新的简洁度评分方法，通过强化学习框架优化大语言模型的推理步骤，提高准确性和效率。

Motivation: 解决现有大语言模型推理步骤冗长导致的计算浪费、可读性差和幻觉问题。

Method: 引入基于强化学习的简洁度评分奖励信号，由大语言模型作为评估者提供动态反馈。

Result: 在MATH数据集上减少31x令牌使用并提升7%准确率；TheoremQA上提升2.2%准确率且减少12.5x令牌。

Conclusion: 方法能动态适应问题难度，显著提升推理效率与准确性，且开源代码与数据。

Abstract: Large language models excel at complex tasks by breaking down problems into
structured reasoning steps. However, reasoning traces often extend beyond
reaching a correct answer, causing wasted computation, reduced readability, and
hallucinations. To address this, we introduce a novel hyperparameter-free
conciseness score used as a reward signal within a reinforcement learning
framework to guide models toward generating correct and concise reasoning
traces. This score is evaluated by a large language model acting as a judge,
enabling dynamic, context-aware feedback beyond simple token length. Our method
achieves state-of-the-art efficiency-accuracy trade-offs on the MATH dataset,
reducing token usage by up to 31x on simple problems while improving accuracy
by 7%, and on the hardest problems, it outperforms full reasoning by +7.5%
accuracy with up to 3.6x fewer tokens. On TheoremQA, our method improves
accuracy by +2.2% using 12.5x fewer tokens. We also conduct ablation studies on
the judge model, reward composition, and problem difficulty, showing that our
method dynamically adapts reasoning length based on problem difficulty and
benefits significantly from stronger judges. The code, model weights, and
datasets are open-sourced at https://github.com/RazvanDu/ConciseRL.

</details>


### [82] [The Rise of Parameter Specialization for Knowledge Storage in Large Language Models](https://arxiv.org/abs/2505.17260)
*Yihuai Hong,Yiran Zhao,Wei Tang,Yang Deng,Yu Rong,Wenxuan Zhang*

Key words: 大语言模型, MLP, 知识存储, 参数专业化, 知识利用效率

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文分析了20个开源大语言模型，探讨了其高性能与知识在MLP参数中存储方式的关系，发现随着模型进步，参数更专注编码同类知识，这种专业化分布提升了知识利用效率。

Motivation: 研究大语言模型中知识如何在MLP参数中存储并更有效利用，填补微观视角研究的空白。

Method: 分析20个开源大语言模型，实验验证知识存储方式对性能的影响，并进行因果训练实验。

Result: 先进模型的参数更专业化，专注于编码同类知识，这种分布显著提升知识利用效率。

Conclusion: 知识在MLP参数中的专业化分布对提升模型性能至关重要，因果训练进一步验证了其关键作用。

Abstract: Over time, a growing wave of large language models from various series has
been introduced to the community. Researchers are striving to maximize the
performance of language models with constrained parameter sizes. However, from
a microscopic perspective, there has been limited research on how to better
store knowledge in model parameters, particularly within MLPs, to enable more
effective utilization of this knowledge by the model. In this work, we analyze
twenty publicly available open-source large language models to investigate the
relationship between their strong performance and the way knowledge is stored
in their corresponding MLP parameters. Our findings reveal that as language
models become more advanced and demonstrate stronger knowledge capabilities,
their parameters exhibit increased specialization. Specifically, parameters in
the MLPs tend to be more focused on encoding similar types of knowledge. We
experimentally validate that this specialized distribution of knowledge
contributes to improving the efficiency of knowledge utilization in these
models. Furthermore, by conducting causal training experiments, we confirm that
this specialized knowledge distribution plays a critical role in improving the
model's efficiency in leveraging stored knowledge.

</details>


### [83] [CaseReportBench: An LLM Benchmark Dataset for Dense Information Extraction in Clinical Case Reports](https://arxiv.org/abs/2505.17265)
*Xiao Yu Cindy Zhang,Carlos R. Ferreira,Francis Rossignol,Raymond T. Ng,Wyeth Wasserman,Jian Zhu*

Key words: 罕见病, 先天性代谢异常, 大型语言模型, 临床信息提取, CaseReportBench

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究利用大型语言模型（LLMs）从病例报告中提取临床信息，构建了专家标注的数据集CaseReportBench，并评估了多种模型和提示策略，发现开源模型Qwen2.5-7B表现优于GPT-4o，但LLMs在识别阴性结果方面仍有不足。

Motivation: 罕见病（如先天性代谢异常）诊断复杂，病例报告作为重要但未充分利用的资源，亟需通过临床密集型信息提取工具（如LLMs）提高诊断效率。

Method: 研究提出了CaseReportBench数据集，并采用零样本提示、类别特定提示等策略评估模型性能，重点比较了Qwen2.5-7B与GPT-4o的表现。

Result: Qwen2.5-7B在提取临床信息方面优于GPT-4o，但LLMs对阴性结果的识别能力不足。临床评估显示LLMs可辅助罕见病诊断。

Conclusion: 研究验证了LLMs在临床信息提取中的潜力，为可扩展的医疗AI应用奠定了基础，同时也指出了需改进的方向（如阴性结果识别）。

Abstract: Rare diseases, including Inborn Errors of Metabolism (IEM), pose significant
diagnostic challenges. Case reports serve as key but computationally
underutilized resources to inform diagnosis. Clinical dense information
extraction refers to organizing medical information into structured predefined
categories. Large Language Models (LLMs) may enable scalable information
extraction from case reports but are rarely evaluated for this task. We
introduce CaseReportBench, an expert-annotated dataset for dense information
extraction of case reports, focusing on IEMs. Using this dataset, we assess
various models and prompting strategies, introducing novel approaches such as
category-specific prompting and subheading-filtered data integration. Zero-shot
chain-of-thought prompting offers little advantage over standard zero-shot
prompting. Category-specific prompting improves alignment with the benchmark.
The open-source model Qwen2.5-7B outperforms GPT-4o for this task. Our
clinician evaluations show that LLMs can extract clinically relevant details
from case reports, supporting rare disease diagnosis and management. We also
highlight areas for improvement, such as LLMs' limitations in recognizing
negative findings important for differential diagnosis. This work advances
LLM-driven clinical natural language processing and paves the way for scalable
medical AI applications.

</details>


### [84] [Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT Reasoning](https://arxiv.org/abs/2505.17266)
*Cehao Yang,Xueyuan Lin,Chengjin Xu,Xuhui Jiang,Xiaojun Wu,Honghao Liu,Hui Xiong,Jian Guo*

Key words: 长链思维推理, 指令选择, 数据效率, 微调, 量化评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Select2Reason是一种高效的长链思维推理指令选择框架，通过量化问题难度和推理长度排名，仅需10%数据即可达到或超过全数据微调效果。

Motivation: 大规模指令数据集训练成本高，且缺乏有效的长链思维推理指令自动选择策略，因此需要一种高效的数据选择方法。

Method: Select2Reason结合问题难度量化和基于推理长度的启发式加权排名，优先选择高效用样本。

Result: 在OpenR1-Math-220k上，仅使用10%选择的数据微调，性能优于全数据微调和开源基线模型。

Conclusion: Select2Reason高效、可扩展且适应性强，为长链思维推理提供了一种低成本解决方案。

Abstract: A practical approach to activate long chain-of-thoughts reasoning ability in
pre-trained large language models is to perform supervised fine-tuning on
instruction datasets synthesized by strong Large Reasoning Models such as
DeepSeek-R1, offering a cost-effective alternative to reinforcement learning.
However, large-scale instruction sets with more than 100k samples incur
significant training overhead, while effective strategies for automatic
long-CoT instruction selection still remain unexplored. In this work, we
propose Select2Reason, a novel and efficient instruction-tuning data selection
framework for long-CoT reasoning. From the perspective of emergence of
rethinking behaviors like self-correction and backtracking, we investigate
common metrics that may determine the quality of long-CoT reasoning
instructions. Select2Reason leverages a quantifier to estimate difficulty of
question and jointly incorporates a reasoning trace length-based heuristic
through a weighted scheme for ranking to prioritize high-utility examples.
Empirical results on OpenR1-Math-220k demonstrate that fine-tuning LLM on only
10% of the data selected by Select2Reason achieves performance competitive with
or superior to full-data tuning and open-source baseline OpenR1-Qwen-7B across
three competition-level and six comprehensive mathematical benchmarks. Further
experiments highlight the scalability in varying data size, efficiency during
inference, and its adaptability to other instruction pools with minimal cost.

</details>


### [85] [GreekBarBench: A Challenging Benchmark for Free-Text Legal Reasoning and Citations](https://arxiv.org/abs/2505.17267)
*Odysseas S. Chlapanis,Dimitrios Galanis,Nikolaos Aletras,Ion Androutsopoulos*

Key words: GreekBarBench, LLMs, legal evaluation, LLM-as-a-judge, meta-evaluation

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: GreekBarBench 是一个评估大型语言模型（LLMs）在希腊律师考试五个法律领域问题的基准，要求引用法律条文和案例事实。通过三维评分系统和 LLM-as-a-judge 方法解决自由文本评估挑战，并开发了元评估基准以衡量 LLM 评分与人类专家评分的相关性。结果表明，简单的基于范围的评分规则能提高评分一致性。对13个专有和开源 LLMs 的系统评估显示，最佳模型表现优于专家平均分，但仍未达到专家95分位的水平。

Motivation: 解决法律领域 LLMs 自由文本评估的挑战，并验证 LLM-as-a-judge 方法在法律问题评估中的可行性。

Method: 提出 GreekBarBench 基准，包含三维评分系统和 LLM-as-a-judge 方法；开发元评估基准量化 LLM 评分与人类专家的相关性。

Result: 最佳 LLMs 表现优于专家平均分，但未达到专家95分位水平；简单的基于范围的评分规则能提升评分一致性。

Conclusion: GreekBarBench 和 LLM-as-a-judge 方法为法律领域 LLMs 评估提供了有效工具，但模型仍需改进以接近顶尖专家水平。

Abstract: We introduce GreekBarBench, a benchmark that evaluates LLMs on legal
questions across five different legal areas from the Greek Bar exams, requiring
citations to statutory articles and case facts. To tackle the challenges of
free-text evaluation, we propose a three-dimensional scoring system combined
with an LLM-as-a-judge approach. We also develop a meta-evaluation benchmark to
assess the correlation between LLM-judges and human expert evaluations,
revealing that simple, span-based rubrics improve their alignment. Our
systematic evaluation of 13 proprietary and open-weight LLMs shows that even
though the best models outperform average expert scores, they fall short of the
95th percentile of experts.

</details>


### [86] [Search Wisely: Mitigating Sub-optimal Agentic Searches By Reducing Uncertainty](https://arxiv.org/abs/2505.17281)
*Peilin Wu,Mian Zhang,Xinlu Zhang,Xinya Du,Zhiyu Zoey Chen*

Key words: 检索增强生成, 代理式搜索, 强化学习, 模型不确定性, 问答系统

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了代理式检索增强生成（RAG）系统的低效搜索行为，并提出了一种基于强化学习的训练方法（β-GRPO），通过引入置信度阈值优化搜索决策，显著提升了模型性能。

Motivation: 代理式RAG系统存在过度搜索和搜索不足的问题，影响效率和可靠性。作者发现这些行为与模型对自身知识边界的不确定性相关，因此提出改进方法。

Method: 论文提出β-GRPO方法，通过强化学习并结合置信度阈值来优化搜索决策。

Result: 实验显示，β-GRPO显著提升了3B模型的RAG能力，在七个问答基准测试中平均准确率提高了4%。

Conclusion: β-GRPO通过优化搜索决策有效解决了代理式RAG系统的低效问题，提升了模型性能。

Abstract: Agentic Retrieval-Augmented Generation (RAG) systems enhance Large Language
Models (LLMs) by enabling dynamic, multi-step reasoning and information
retrieval. However, these systems often exhibit sub-optimal search behaviors
like over-search (retrieving redundant information) and under-search (failing
to retrieve necessary information), which hinder efficiency and reliability.
This work formally defines and quantifies these behaviors, revealing their
prevalence across multiple QA datasets and agentic RAG systems (e.g., one model
could have avoided searching in 27.7% of its search steps). Furthermore, we
demonstrate a crucial link between these inefficiencies and the models'
uncertainty regarding their own knowledge boundaries, where response accuracy
correlates with model's uncertainty in its search decisions. To address this,
we propose $\beta$-GRPO, a reinforcement learning-based training method that
incorporates confidence threshold to reward high-certainty search decisions.
Experiments on seven QA benchmarks show that $\beta$-GRPO enable a 3B model
with better agentic RAG ability, outperforming other strong baselines with a 4%
higher average exact match score.

</details>


### [87] [SELF: Self-Extend the Context Length With Logistic Growth Function](https://arxiv.org/abs/2505.17296)
*Phat Thanh Dang,Saahil Thoppay,Wang Yang,Qifan Wang,Vipin Chaudhary,Xiaotian Han*

Key words: 大语言模型、长上下文、位置编码、SELF方法、性能提升

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: SELF方法通过分组标记和逻辑增长函数扩展上下文长度，显著提升语言模型在长上下文任务中的性能。

Motivation: 大语言模型在处理超过其训练上下文长度的长文本时，由于标准位置编码的限制，导致远距离标记间的交互减弱，结果不稳定。

Method: 提出SELF方法：利用逻辑容量方程对不同规模的标记组进行分组，同时在较近距离保持恒定组大小。

Result: 在LEval的Qwen模型上性能提升12%；在LongBench的Llama-2-7b模型上摘要任务表现提升6.4%；在LEval阅读理解任务上提升5.4%。

Conclusion: SELF通过动态分组策略有效扩展上下文长度，显著提升模型在长文本任务中的表现。

Abstract: Large language models suffer issues when operated on long contexts that are
larger than their training context length due to the standard position encoding
for tokens in the attention layer. Tokens a long distance apart will rarely
have an effect on each other and long prompts yield unexpected results. To
solve this problem, we propose SELF (Self-Extend the Context Length With
Logistic Growth Function): a solution of grouping consecutive tokens at varying
group sizes using a logistic capacity equation combined with a constant group
size at smaller relative distances. Our model had an increase in performance of
up to 12% compared to the LongLM extension method in LEval (specifically on the
Qwen model). On summarization related tasks in LongBench, our model performed
up to 6.4% better than LongLM (specifically on the Llama-2-7b model). On
reading comprehension tasks from LEval, our model performed up to 5.4% better
than the LongLM. Our code is available at https://github.com/alexeipc/SELF-LLM.

</details>


### [88] [Refusal Direction is Universal Across Safety-Aligned Languages](https://arxiv.org/abs/2505.17306)
*Xinpeng Wang,Mingyang Wang,Yihong Liu,Hinrich Schütze,Barbara Plank*

Key words: large language models, refusal behavior, multilingual safety, cross-lingual universality, jailbreaks

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究发现LLMs的拒绝行为可由激活空间中的单一方向调节，且这种拒绝方向具有跨语言的普遍性，为构建更强大的多语言安全防御提供了见解。

Motivation: 探索LLMs在多语言环境中的拒绝行为，以增强模型的安全性和理解跨语言漏洞。

Method: 使用PolyRefuse多语言安全数据集，翻译恶意和良性英语提示至14种语言，分析拒绝方向的跨语言有效性。

Result: 英语提取的拒绝向量可无调整地高效绕过其他语言的拒绝，显示出跨语言的普遍性和可迁移性。

Conclusion: 拒绝方向的跨语言平行性揭示了LLMs的安全机制，为多语言安全防御提供了新的视角。

Abstract: Refusal mechanisms in large language models (LLMs) are essential for ensuring
safety. Recent research has revealed that refusal behavior can be mediated by a
single direction in activation space, enabling targeted interventions to bypass
refusals. While this is primarily demonstrated in an English-centric context,
appropriate refusal behavior is important for any language, but poorly
understood. In this paper, we investigate the refusal behavior in LLMs across
14 languages using PolyRefuse, a multilingual safety dataset created by
translating malicious and benign English prompts into these languages. We
uncover the surprising cross-lingual universality of the refusal direction: a
vector extracted from English can bypass refusals in other languages with
near-perfect effectiveness, without any additional fine-tuning. Even more
remarkably, refusal directions derived from any safety-aligned language
transfer seamlessly to others. We attribute this transferability to the
parallelism of refusal vectors across languages in the embedding space and
identify the underlying mechanism behind cross-lingual jailbreaks. These
findings provide actionable insights for building more robust multilingual
safety defenses and pave the way for a deeper mechanistic understanding of
cross-lingual vulnerabilities in LLMs.

</details>


### [89] [Benchmarking Expressive Japanese Character Text-to-Speech with VITS and Style-BERT-VITS2](https://arxiv.org/abs/2505.17320)
*Zackary Rackauckas,Julia Hirschberg*

Key words: 日语语音合成, 角色语音, VITS, Style-BERT-VITS2, 自然度评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文比较了VITS和Style-BERT-VITS2 JP Extra（SBV2JE）在日语角色语音合成中的表现，SBV2JE在自然度、清晰度和说话人一致性上表现优异，接近人类水平，适用于语言学习和角色对话生成。

Motivation: 合成具有表现力的日语角色语音面临音高敏感性和风格多样性的挑战，需评估现有模型的性能。

Method: 使用VITS和SBV2JE模型，在三套角色特定数据集上评估自然度、清晰度和说话人一致性。

Result: SBV2JE在自然度上接近人类水平（MOS 4.37 vs. 4.38），词错误率更低，CMOS略优。

Conclusion: SBV2JE凭借音高控制和WavLM判别器，适用于语言学习和角色对话生成，尽管计算需求较高。

Abstract: Synthesizing expressive Japanese character speech poses unique challenges due
to pitch-accent sensitivity and stylistic variability. This paper benchmarks
two open-source text-to-speech models--VITS and Style-BERT-VITS2 JP Extra
(SBV2JE)--on in-domain, character-driven Japanese speech. Using three
character-specific datasets, we evaluate models across naturalness (mean
opinion and comparative mean opinion score), intelligibility (word error rate),
and speaker consistency. SBV2JE matches human ground truth in naturalness (MOS
4.37 vs. 4.38), achieves lower WER, and shows slight preference in CMOS.
Enhanced by pitch-accent controls and a WavLM-based discriminator, SBV2JE
proves effective for applications like language learning and character dialogue
generation, despite higher computational demands.

</details>


### [90] [From Compression to Expansion: A Layerwise Analysis of In-Context Learning](https://arxiv.org/abs/2505.17322)
*Jiachen Jiang,Yuxin Dong,Jinxin Zhou,Zhihui Zhu*

Key words: 上下文学习,大型语言模型,表征机制,分层压缩-扩展,偏置-方差分解

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文通过统计几何分析发现了一种称为‘分层压缩-扩展’的现象，揭示了大型语言模型在上下文学习中如何通过不同层级处理任务信息，并分析了其对性能的影响。

Motivation: 研究上下文学习（ICL）的内部表征机制，以理解大型语言模型如何通过示例序列学习新任务，而不更新权重。

Method: 采用统计几何分析，研究ICL表征在模型不同层级中的任务信息捕获方式，并提出偏置-方差分解进行理论分析。

Result: 发现‘分层压缩-扩展’现象：早期层级压缩任务信息，后期层级扩展并生成预测。这一现象提升模型性能和鲁棒性，并随模型规模和示例数量增加而增强。

Conclusion: ICL的层级动态揭示了LLM内部的结构化表征机制，分析内部表征有助于更深入理解模型行为。

Abstract: In-context learning (ICL) enables large language models (LLMs) to adapt to
new tasks without weight updates by learning from demonstration sequences.
While ICL shows strong empirical performance, its internal representational
mechanisms are not yet well understood. In this work, we conduct a statistical
geometric analysis of ICL representations to investigate how task-specific
information is captured across layers. Our analysis reveals an intriguing
phenomenon, which we term *Layerwise Compression-Expansion*: early layers
progressively produce compact and discriminative representations that encode
task information from the input demonstrations, while later layers expand these
representations to incorporate the query and generate the prediction. This
phenomenon is observed consistently across diverse tasks and a range of
contemporary LLM architectures. We demonstrate that it has important
implications for ICL performance -- improving with model size and the number of
demonstrations -- and for robustness in the presence of noisy examples. To
further understand the effect of the compact task representation, we propose a
bias-variance decomposition and provide a theoretical analysis showing how
attention mechanisms contribute to reducing both variance and bias, thereby
enhancing performance as the number of demonstrations increases. Our findings
reveal an intriguing layerwise dynamic in ICL, highlight how structured
representations emerge within LLMs, and showcase that analyzing internal
representations can facilitate a deeper understanding of model behavior.

</details>


### [91] [GPT Editors, Not Authors: The Stylistic Footprint of LLMs in Academic Preprints](https://arxiv.org/abs/2505.17327)
*Soren DeHaan,Yuanze Liu,Johan Bollen,Sa'ul A. Blanco*

Key words: Large Language Models, academic writing, stylistic segmentation, Bayesian classifier, arXiv

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究LLMs在学术写作中的使用程度，通过分析arXiv论文的文体分割，发现LLMs的使用是均匀的，降低了幻觉风险。

Motivation: 研究LLMs在学术写作中的使用方式，区分其用于生成关键文本还是编辑功能，以评估对学术可信度的影响。

Method: 通过PELT阈值变化和贝叶斯分类器分析arXiv论文的文体分割，对比GPT重生成文本。

Result: LLMs的使用与文体分割无关，表明作者使用LLMs时是均匀的，减少了幻觉的引入风险。

Conclusion: LLMs在学术写作中的使用方式均匀，未导致文体分割，降低了幻觉对预印本的影响。

Abstract: The proliferation of Large Language Models (LLMs) in late 2022 has impacted
academic writing, threatening credibility, and causing institutional
uncertainty. We seek to determine the degree to which LLMs are used to generate
critical text as opposed to being used for editing, such as checking for
grammar errors or inappropriate phrasing. In our study, we analyze arXiv papers
for stylistic segmentation, which we measure by varying a PELT threshold
against a Bayesian classifier trained on GPT-regenerated text. We find that
LLM-attributed language is not predictive of stylistic segmentation, suggesting
that when authors use LLMs, they do so uniformly, reducing the risk of
hallucinations being introduced into academic preprints.

</details>


### [92] [SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise Use](https://arxiv.org/abs/2505.17332)
*Hitesh Laxmichand Patel,Amit Agarwal,Arion Das,Bhargava Kumar,Srikant Panda,Priyaranjan Pattnayak,Taki Hasan Rafi,Tejaswini Kumar,Dong-Kyu Chae*

Key words: 大型语言模型,企业应用,伦理对齐,文化多样性,语言理解

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文介绍了SweEval基准，用于评估大型语言模型在不同文化和语言背景下处理不当指令的能力，以确保企业应用中的安全和合规性。

Motivation: 企业客户越来越多地采用大型语言模型进行关键沟通任务，但需要确保模型能在不同文化背景下生成安全且尊重的回应，以降低声誉风险并维护信任。

Method: 引入SweEval基准，模拟真实场景中的语气和上下文变化，明确指令模型包含特定脏词，评估其是否遵守或抵制不当指令。

Result: 通过SweEval基准，论文评估了大型语言模型在伦理框架、文化差异和语言理解能力上的表现，并公开了数据集和代码。

Conclusion: SweEval基准为构建符合伦理的AI系统提供了研究基础，尤其适用于企业应用。

Abstract: Enterprise customers are increasingly adopting Large Language Models (LLMs)
for critical communication tasks, such as drafting emails, crafting sales
pitches, and composing casual messages. Deploying such models across different
regions requires them to understand diverse cultural and linguistic contexts
and generate safe and respectful responses. For enterprise applications, it is
crucial to mitigate reputational risks, maintain trust, and ensure compliance
by effectively identifying and handling unsafe or offensive language. To
address this, we introduce SweEval, a benchmark simulating real-world scenarios
with variations in tone (positive or negative) and context (formal or
informal). The prompts explicitly instruct the model to include specific swear
words while completing the task. This benchmark evaluates whether LLMs comply
with or resist such inappropriate instructions and assesses their alignment
with ethical frameworks, cultural nuances, and language comprehension
capabilities. In order to advance research in building ethically aligned AI
systems for enterprise use and beyond, we release the dataset and code:
https://github.com/amitbcp/multilingual_profanity.

</details>


### [93] [Language models should be subject to repeatable, open, domain-contextualized hallucination benchmarking](https://arxiv.org/abs/2505.17345)
*Justin D. Norman,Michael U. Rivera,D. Alex Hughes*

Key words: 语言模型、幻觉基准、专家参与、数据创建、有效性评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一个评估语言模型幻觉的可重复、开放且领域化的基准测试方法，并指出专家参与早期数据创建对幻觉度量有效性的重要性。

Motivation: 研究动机是基于广泛认为模型生成的文本中存在看似合理但不准确的标记，但其普遍性和影响缺乏科学测量。

Method: 论文提出了一种基于可重复性、开放性和领域情境化的幻觉基准测试方法，并构建了幻觉分类法，通过案例研究验证了方法。

Result: 研究表明若专家未参与早期数据创建，幻觉度量会缺乏有效性和实际应用价值。

Conclusion: 该论文强调领域专家应在数据创建早期阶段参与，以确保幻觉度量的准确性和实用性。

Abstract: Plausible, but inaccurate, tokens in model-generated text are widely believed
to be pervasive and problematic for the responsible adoption of language
models. Despite this concern, there is little scientific work that attempts to
measure the prevalence of language model hallucination in a comprehensive way.
In this paper, we argue that language models should be evaluated using
repeatable, open, and domain-contextualized hallucination benchmarking. We
present a taxonomy of hallucinations alongside a case study that demonstrates
that when experts are absent from the early stages of data creation, the
resulting hallucination metrics lack validity and practical utility.

</details>


### [94] [A Fully Generative Motivational Interviewing Counsellor Chatbot for Moving Smokers Towards the Decision to Quit](https://arxiv.org/abs/2505.17362)
*Zafarullah Mahmood,Soliman Ali,Jiading Zhu,Mohamed Abdelwahab,Michelle Yu Collins,Sihan Chen,Yi Cheng Zhao,Jodi Wolff,Osnat Melamed,Nadia Minian,Marta Maslej,Carolynne Cooper,Matt Ratto,Peter Selby,Jonathan Rose*

Key words: 大型语言模型，动机性访谈，戒烟，聊天机器人

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 摘要：研究利用大型语言模型和动机性访谈方法开发了一款聊天机器人，帮助吸烟者戒烟，效果显著。

Motivation: 研究动机是探究大型语言模型是否能作为有效的自动化心理咨询师，尤其是帮助吸烟者戒烟。

Method: 方法结合了先进的LLM技术和动机性访谈（MI）治疗方式，并通过临床专家合作优化。

Result: 测试了106名参与者，结果显示戒烟信心提升了1.7分（0-10分制），机器人98%的发言符合MI标准。

Conclusion: 结论表明现代LLM在自动化心理咨询领域具有潜力。

Abstract: The conversational capabilities of Large Language Models (LLMs) suggest that
they may be able to perform as automated talk therapists. It is crucial to know
if these systems would be effective and adhere to known standards. We present a
counsellor chatbot that focuses on motivating tobacco smokers to quit smoking.
It uses a state-of-the-art LLM and a widely applied therapeutic approach called
Motivational Interviewing (MI), and was evolved in collaboration with
clinician-scientists with expertise in MI. We also describe and validate an
automated assessment of both the chatbot's adherence to MI and client
responses. The chatbot was tested on 106 participants, and their confidence
that they could succeed in quitting smoking was measured before the
conversation and one week later. Participants' confidence increased by an
average of 1.7 on a 0-10 scale. The automated assessment of the chatbot showed
adherence to MI standards in 98% of utterances, higher than human counsellors.
The chatbot scored well on a participant-reported metric of perceived empathy
but lower than typical human counsellors. Furthermore, participants' language
indicated a good level of motivation to change, a key goal in MI. These results
suggest that the automation of talk therapy with a modern LLM has promise.

</details>


### [95] [AI-Augmented LLMs Achieve Therapist-Level Responses in Motivational Interviewing](https://arxiv.org/abs/2505.17380)
*Yinghui Huang,Yuxuan Jiang,Hui Liu,Yixin Cai,Weiqing Li,Xiangen Hu*

Key words: 动机访谈, GPT-4, 计算框架, 深度学, 提示工程

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究人员开发了一个计算框架，评估GPT-4在动机访谈（MI）中的表现，发现通过提示工程可以改善其表现，但仍不及人类治疗师处理复杂情绪的能力。

Motivation: 评估大型语言模型（如GPT-4）在成瘾护理中的动机访谈潜力，以确定其替代或辅助人类治疗师的能力。

Method: 构建计算框架，分析人类治疗师和GPT-4的MI会话，结合深度学习和可解释AI开发预测模型，识别MI一致（MICO）和不一致（MIIN）行为指标。通过改进的提示工程优化GPT-4的能力。

Result: GPT-4在管理建议方面优于人类治疗师，但整体表现仍稍逊。提示工程显著减少了不适当的建议并增强了共情和反思能力，但在处理复杂情绪时仍有局限。

Conclusion: 该框架为LLM在临床沟通中的应用提供了优化路径，突出了其扩展潜力但也指出了当前处理复杂情感的局限性。

Abstract: Large language models (LLMs) like GPT-4 show potential for scaling
motivational interviewing (MI) in addiction care, but require systematic
evaluation of therapeutic capabilities. We present a computational framework
assessing user-perceived quality (UPQ) through expected and unexpected MI
behaviors. Analyzing human therapist and GPT-4 MI sessions via human-AI
collaboration, we developed predictive models integrating deep learning and
explainable AI to identify 17 MI-consistent (MICO) and MI-inconsistent (MIIN)
behavioral metrics. A customized chain-of-thought prompt improved GPT-4's MI
performance, reducing inappropriate advice while enhancing reflections and
empathy. Although GPT-4 remained marginally inferior to therapists overall, it
demonstrated superior advice management capabilities. The model achieved
measurable quality improvements through prompt engineering, yet showed
limitations in addressing complex emotional nuances. This framework establishes
a pathway for optimizing LLM-based therapeutic tools through targeted
behavioral metric analysis and human-AI co-evaluation. Findings highlight both
the scalability potential and current constraints of LLMs in clinical
communication applications.

</details>


### [96] [WiNGPT-3.0 Technical Report](https://arxiv.org/abs/2505.17387)
*Boqin Zhuang,Chenxiao Song,Huitong Lu,Jiacheng Qiao,Mingqian Liu,Mingxing Yu,Ping Hong,Rui Li,Xiaoxia Song,Xiangjun Xu,Xu Chen,Yaoyao Ma,Yujie Gao*

Key words: WiNGPT-3.0, LLMs, 医疗推理, 强化学习, SFT

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: WiNGPT-3.0, a 32B parameter LLM,采用多阶段训练提升医疗推理能力,结合SFT和强化学习,在MedCalc和MedQA-USMLE测试中表现优异,强化学习即使在小数据集上也能提高医疗推理能力。

Motivation: 解决LLMs在结构化、可解释性医疗推理以及实际部署中计算资源和数据隐私的局限性。

Method: 采用多阶段训练管道,包括监督微调、强化学习,利用长链思维数据集、辅助奖励模型和基于证据的诊断链模拟。

Result: WiNGPT-3.0在MedCalc和MedQA-USMLE测试中分别得分66.6和87.1,临床推理任务表现从58.1提升至62.5。

Conclusion: 强化学习即使在有限数据集上也有效,为临床工作流中的可信赖和实际部署的LLMs铺路。

Abstract: Current Large Language Models (LLMs) exhibit significant limitations, notably
in structured, interpretable, and verifiable medical reasoning, alongside
practical deployment challenges related to computational resources and data
privacy. This report focused on the development of WiNGPT-3.0, the 32-billion
parameter LLMs, engineered with the objective of enhancing its capacity for
medical reasoning and exploring its potential for effective integration within
healthcare IT infrastructures. The broader aim is to advance towards clinically
applicable models. The approach involved a multi-stage training pipeline
tailored for general, medical, and clinical reasoning. This pipeline
incorporated supervised fine-tuning (SFT) and reinforcement learning (RL),
leveraging curated Long Chain-of-Thought (CoT) datasets, auxiliary reward
models, and an evidence-based diagnostic chain simulation. WiNGPT-3.0
demonstrated strong performance: specific model variants achieved scores of
66.6 on MedCalc and 87.1 on MedQA-USMLE. Furthermore, targeted training
improved performance on a clinical reasoning task from a baseline score of 58.1
to 62.5. These findings suggest that reinforcement learning, even when applied
with a limited dataset of only a few thousand examples, can enhance medical
reasoning accuracy. Crucially, this demonstration of RL's efficacy with limited
data and computation paves the way for more trustworthy and practically
deployable LLMs within clinical workflows and health information
infrastructures.

</details>


### [97] [Measuring diversity of synthetic prompts and data generated with fine-grained persona prompting](https://arxiv.org/abs/2505.17390)
*Gauri Kambhatla,Chantal Shaib,Venkata Govindarajan*

Key words: 人物设定、多样性、合成数据、语言模型、词汇指标

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究发现，尽管通过人物设定提示可以提升语言模型生成文本的词汇多样性，但细微人物细节对多样性的提升不明显，且合成数据的多样性仍低于人类撰写内容。

Motivation: 探究人物设定驱动的合成数据在语言模型预训练和监督微调中的多样性表现，以及人物设定的精细程度对生成文本多样性的影响。

Method: 使用词汇多样性和冗余度指标评估合成提示和响应的多样性，并比较不同规模语言模型在精细和粗略人物设定下的表现。

Result: 合成提示的多样性显著低于人类撰写内容；精细人物设定虽能提升词汇多样性（尤其在大型模型中），但效果不明显。

Conclusion: 人物设定提示能提升生成文本的多样性，但精细人物细节作用有限，且合成数据仍需改进以接近人类水平。

Abstract: Fine-grained personas have recently been used for generating 'diverse'
synthetic data for pre-training and supervised fine-tuning of Large Language
Models (LLMs). In this work, we measure the diversity of persona-driven
synthetically generated prompts and responses with a suite of lexical diversity
and redundancy metrics. Firstly, we find that synthetic prompts/instructions
are significantly less diverse than human-written ones. Next, we sample
responses from LLMs of different sizes with fine-grained and coarse persona
descriptions to investigate how much fine-grained detail in persona
descriptions contribute to generated text diversity. We find that while
persona-prompting does improve lexical diversity (especially with larger
models), fine-grained detail in personas doesn't increase diversity noticeably.

</details>


### [98] [Curriculum Guided Reinforcement Learning for Efficient Multi Hop Retrieval Augmented Generation](https://arxiv.org/abs/2505.17391)
*Yuelyu Ji,Rui Meng,Zhuochun Li,Daqing He*

Key words: 多跳RAG,强化学习,查询重写,课程引导,动态奖励

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: EVO-RAG使用课程引导的强化学习框架优化多跳RAG系统，通过动态奖励调度减少冗余查询并提升准确率。

Motivation: 现有RAG系统存在冗余查询、探索不足或搜索链过长的问题，EVO-RAG旨在通过强化学习改进这些缺陷。

Method: 引入课程引导的强化学习框架，结合七因素奖励向量和动态调度器，训练查询重写代理。

Result: 在四个多跳QA基准测试中，EVO-RAG将Exact Match提升4.6分，同时减少15%的平均检索深度。

Conclusion: EVO-RAG为构建可靠且高效的多跳RAG系统提供了通用方案。

Abstract: Retrieval-augmented generation (RAG) grounds large language models (LLMs) in
up-to-date external evidence, yet existing multi-hop RAG pipelines still issue
redundant subqueries, explore too shallowly, or wander through overly long
search chains. We introduce EVO-RAG, a curriculum-guided reinforcement learning
framework that evolves a query-rewriting agent from broad early-stage
exploration to concise late-stage refinement. EVO-RAG couples a seven-factor,
step-level reward vector (covering relevance, redundancy, efficiency, and
answer correctness) with a time-varying scheduler that reweights these signals
as the episode unfolds. The agent is trained with Direct Preference
Optimization over a multi-head reward model, enabling it to learn when to
search, backtrack, answer, or refuse. Across four multi-hop QA benchmarks
(HotpotQA, 2WikiMultiHopQA, MuSiQue, and Bamboogle), EVO-RAG boosts Exact Match
by up to 4.6 points over strong RAG baselines while trimming average retrieval
depth by 15 %. Ablation studies confirm the complementary roles of curriculum
staging and dynamic reward scheduling. EVO-RAG thus offers a general recipe for
building reliable, cost-effective multi-hop RAG systems.

</details>


### [99] [FullFront: Benchmarking MLLMs Across the Full Front-End Engineering Workflow](https://arxiv.org/abs/2505.17399)
*Haoyu Sun,Huichen Will Wang,Jiawei Gu,Linjie Li,Yu Cheng*

Key words: 前端开发，多模态大语言模型，基准测试，网页设计，代码生成

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: FullFront 是一个新的基准测试，用于评估多模态大语言模型（MLLMs）在整个前端开发流程中的表现，包括网页设计、视觉理解和代码生成。它通过两阶段处理真实网页生成标准化的HTML，测试发现MLLMs在视觉理解、代码布局和交互实现上存在显著不足。

Motivation: 现有的基准测试大多只关注从视觉设计到代码的转换，忽略了前端工程的其他关键阶段。FullFront的目标是填补这一空白，全面评估MLLMs在实际前端开发流程中的能力。

Method: FullFront 采用两阶段方法处理真实网页，转换为干净、标准化的HTML，同时保留多样化的视觉设计并避免版权问题。它评估三个主要任务：网页设计、视觉QA和代码生成。

Result: 测试表明，MLLMs在页面感知、代码生成（尤其是图像处理和布局）和交互实现方面表现不佳，与人类专家水平存在显著差距。

Conclusion: 当前MLLMs在前端工程中的能力有限，FullFront为未来研究提供了标准化的评估工具和数据。

Abstract: Front-end engineering involves a complex workflow where engineers
conceptualize designs, translate them into code, and iteratively refine the
implementation. While recent benchmarks primarily focus on converting visual
designs to code, we present FullFront, a benchmark designed to evaluate
Multimodal Large Language Models (MLLMs) \textbf{across the full front-end
development pipeline}. FullFront assesses three fundamental tasks that map
directly to the front-end engineering pipeline: Webpage Design
(conceptualization phase), Webpage Perception QA (comprehension of visual
organization and elements), and Webpage Code Generation (implementation phase).
Unlike existing benchmarks that use either scraped websites with bloated code
or oversimplified LLM-generated HTML, FullFront employs a novel, two-stage
process to transform real-world webpages into clean, standardized HTML while
maintaining diverse visual designs and avoiding copyright issues. Extensive
testing of state-of-the-art MLLMs reveals significant limitations in page
perception, code generation (particularly for image handling and layout), and
interaction implementation. Our results quantitatively demonstrate performance
disparities across models and tasks, and highlight a substantial gap between
current MLLM capabilities and human expert performance in front-end
engineering. The FullFront benchmark and code are available in
https://github.com/Mikivishy/FullFront.

</details>


### [100] [Language Matters: How Do Multilingual Input and Reasoning Paths Affect Large Reasoning Models?](https://arxiv.org/abs/2505.17407)
*Zhi Rui Tam,Cheng-Kuang Wu,Yu Ying Chiu,Chieh-Yen Lin,Yun-Nung Chen,Hung-yi Lee*

Key words: 大型推理模型, 多语言, 语言偏见, 推理任务, 高资源语言

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究发现，尽管经过多语言训练，大型推理模型在实际测试中倾向于使用高资源语言（如英语）进行推理，而非输入语言。强制使用输入语言推理会降低模型性能，尤其是低资源语言任务。不同任务类型对语言选择的影响不同。

Motivation: 探讨大型推理模型在多语言设置中的内部推理过程，具体研究其在不同输入语言下默认使用的推理语言。

Method: 通过在推理密集型任务（MMMLU、MATH-500）和非推理基准（CulturalBench、LMSYS-toxic）上的广泛评估，分析语言选择对模型性能的影响。

Result: 模型倾向于使用高资源语言推理且性能更优，而输入语言推理会降低性能（尤其是低资源语言）。任务类型不同，语言选择的影响也不同。

Conclusion: 揭示了大型推理模型的语言偏见，为开发更公平的多语言模型提供了关键方向。

Abstract: Large reasoning models (LRMs) have demonstrated impressive performance across
a range of reasoning tasks, yet little is known about their internal reasoning
processes in multilingual settings. We begin with a critical question: {\it In
which language do these models reason when solving problems presented in
different languages?} Our findings reveal that, despite multilingual training,
LRMs tend to default to reasoning in high-resource languages (e.g., English) at
test time, regardless of the input language. When constrained to reason in the
same language as the input, model performance declines, especially for
low-resource languages. In contrast, reasoning in high-resource languages
generally preserves performance. We conduct extensive evaluations across
reasoning-intensive tasks (MMMLU, MATH-500) and non-reasoning benchmarks
(CulturalBench, LMSYS-toxic), showing that the effect of language choice varies
by task type: input-language reasoning degrades performance on reasoning tasks
but benefits cultural tasks, while safety evaluations exhibit language-specific
behavior. By exposing these linguistic biases in LRMs, our work highlights a
critical step toward developing more equitable models that serve users across
diverse linguistic backgrounds.

</details>


### [101] [Conversations: Love Them, Hate Them, Steer Them](https://arxiv.org/abs/2505.17413)
*Niranjan Chebrolu,Gerard Christopher Yeo,Kokil Jaidka*

Key words: LLMs, 情感表达, 激活工程, 归因修补, 对话AI

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 通过有针对性的激活工程技术，改进LLaMA 3.1-8B模型的情感表达，使其更接近人类情感。方法包括定位关键干预点，生成情感向量，并在对话中应用，显著增强了情感特征。

Motivation: 当前的LLMs在情感表达上缺乏人类般的细腻，现有技术多关注表面输出或需要大量微调。本文旨在通过更精确且可解释的方法提升模型的情感表达能力。

Method: 使用归因修补技术确定关键影响组件，通过对比文本对生成情感向量，并在新对话提示中应用这些向量。

Result: 改进后的模型表现出更高的积极情感（如喜悦、信任）和更多第一人称代词使用，表明更强的个人参与感。

Conclusion: 研究提供了一种精确且可解释的方法来控制LLMs中的特定情感属性，有助于开发更具同理心的对话AI。

Abstract: Large Language Models (LLMs) demonstrate increasing conversational fluency,
yet instilling them with nuanced, human-like emotional expression remains a
significant challenge. Current alignment techniques often address surface-level
output or require extensive fine-tuning. This paper demonstrates that targeted
activation engineering can steer LLaMA 3.1-8B to exhibit more human-like
emotional nuances. We first employ attribution patching to identify causally
influential components, to find a key intervention locus by observing
activation patterns during diagnostic conversational tasks. We then derive
emotional expression vectors from the difference in the activations generated
by contrastive text pairs (positive vs. negative examples of target emotions).
Applying these vectors to new conversational prompts significantly enhances
emotional characteristics: steered responses show increased positive sentiment
(e.g., joy, trust) and more frequent first-person pronoun usage, indicative of
greater personal engagement. Our findings offer a precise and interpretable
method for controlling specific emotional attributes in LLMs, contributing to
developing more aligned and empathetic conversational AI.

</details>


### [102] [DASH: Input-Aware Dynamic Layer Skipping for Efficient LLM Inference with Markov Decision Policies](https://arxiv.org/abs/2505.17420)
*Ning Yang,Fangxin Liu,Junjie Wang,Tao Yang,Kan Liu,Haibing Guan,Li Jiang*

Key words: 大型语言模型，推理加速，自适应计算，马尔可夫决策过程，层跳过

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了DASH，一种自适应层跳过框架，通过动态选择计算路径减少大型语言模型的推理成本，同时保持性能。

Motivation: 大型语言模型的推理成本高昂，影响了实际部署尤其是在延迟敏感场景中的应用。需要通过减少计算量来加速推理但避免性能下降。

Method: 将跳过过程建模为马尔可夫决策过程（MDP），基于中间表示动态选择计算路径；加入轻量级补偿机制注入差异奖励；设计异步执行策略以减少运行时开销。

Result: 在多种大型语言模型架构和NLP基准测试中，显著加速推理且保持任务性能，优于现有方法。

Conclusion: DASH框架通过自适应层跳过有效平衡了推理速度和性能，为实际部署提供了可行方案。

Abstract: Large language models (LLMs) have achieved remarkable performance across a
wide range of NLP tasks. However, their substantial inference cost poses a
major barrier to real-world deployment, especially in latency-sensitive
scenarios. To address this challenge, we propose \textbf{DASH}, an adaptive
layer-skipping framework that dynamically selects computation paths conditioned
on input characteristics. We model the skipping process as a Markov Decision
Process (MDP), enabling fine-grained token-level decisions based on
intermediate representations. To mitigate potential performance degradation
caused by skipping, we introduce a lightweight compensation mechanism that
injects differential rewards into the decision process. Furthermore, we design
an asynchronous execution strategy that overlaps layer computation with policy
evaluation to minimize runtime overhead. Experiments on multiple LLM
architectures and NLP benchmarks show that our method achieves significant
inference acceleration while maintaining competitive task performance,
outperforming existing methods.

</details>


### [103] [T$^2$: An Adaptive Test-Time Scaling Strategy for Contextual Question Answering](https://arxiv.org/abs/2505.17427)
*Zhengyi Zhao,Shubo Zhang,Zezhong Wang,Huimin Wang,Yutian Zhao,Bin Liang,Yefeng Zheng,Binyang Li,Kam-Fai Wong,Xian Wu*

Key words: Large Language Models, Contextual Question Answering, Reasoning Depth, Computational Efficiency

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: T$^2$是一种动态调整推理深度的框架，通过分解问题、生成类似示例并评估推理策略，既提高了准确性又减少了计算开销。

Motivation: 现有方法在上下文问答中过于依赖复杂推理，缺乏对不同难度问题的适应性，且引入人为偏见。T$^2$旨在解决这些问题。

Method: T$^2$通过分解问题、生成类似示例、评估推理策略并应用最优策略来动态调整推理深度。

Result: 在七个CQA基准测试中，T$^2$比基线方法准确率更高，同时计算开销减少达25.2%。

Conclusion: T$^2$通过动态适应推理深度，有效平衡了性能与效率，解决了现有方法的局限性。

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated remarkable
performance in Contextual Question Answering (CQA). However, prior approaches
typically employ elaborate reasoning strategies regardless of question
complexity, leading to low adaptability. Recent efficient test-time scaling
methods introduce budget constraints or early stop mechanisms to avoid
overthinking for straightforward questions. But they add human bias to the
reasoning process and fail to leverage models' inherent reasoning capabilities.
To address these limitations, we present T$^2$: Think-to-Think, a novel
framework that dynamically adapts reasoning depth based on question complexity.
T$^2$ leverages the insight that if an LLM can effectively solve similar
questions using specific reasoning strategies, it can apply the same strategy
to the original question. This insight enables to adoption of concise reasoning
for straightforward questions while maintaining detailed analysis for complex
problems. T$^2$ works through four key steps: decomposing questions into
structural elements, generating similar examples with candidate reasoning
strategies, evaluating these strategies against multiple criteria, and applying
the most appropriate strategy to the original question. Experimental evaluation
across seven diverse CQA benchmarks demonstrates that T$^2$ not only achieves
higher accuracy than baseline methods but also reduces computational overhead
by up to 25.2\%.

</details>


### [104] [Discovering Forbidden Topics in Language Models](https://arxiv.org/abs/2505.17441)
*Can Rager,Chris Wendler,Rohit Gandikota,David Bau*

Key words: 拒绝发现, LLM-crawler, token预填充, 语言模型, 偏见检测

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文介绍了一种称为拒绝发现的新任务，旨在识别语言模型拒绝讨论的所有主题。作者开发了LLM-crawler方法，利用token预填充技术发现禁止话题，并在多个模型上验证效果，揭示了AI系统的偏见和边界问题。

Motivation: 研究动机是开发一种方法，系统地识别语言模型拒绝讨论的主题，以检测模型的偏见、边界和对齐失败。

Method: 提出LLM-crawler方法，通过token预填充技术探索模型的拒绝话题，并在多个开源和前沿模型上进行测试。

Result: 在Tulu-3-8B模型中，1000次提示内发现了31/36的拒绝主题；在DeepSeek-R1-70B中观察到审查调优行为；Perplexity-R1-1776-70B在量化版本中表现出对审查的抵抗。

Conclusion: 拒绝发现方法对检测AI系统的偏见和边界至关重要，揭示了模型的对齐失败和审查行为。

Abstract: Refusal discovery is the task of identifying the full set of topics that a
language model refuses to discuss. We introduce this new problem setting and
develop a refusal discovery method, LLM-crawler, that uses token prefilling to
find forbidden topics. We benchmark the LLM-crawler on Tulu-3-8B, an
open-source model with public safety tuning data. Our crawler manages to
retrieve 31 out of 36 topics within a budget of 1000 prompts. Next, we scale
the crawl to a frontier model using the prefilling option of Claude-Haiku.
Finally, we crawl three widely used open-weight models: Llama-3.3-70B and two
of its variants finetuned for reasoning: DeepSeek-R1-70B and
Perplexity-R1-1776-70B. DeepSeek-R1-70B reveals patterns consistent with
censorship tuning: The model exhibits "thought suppression" behavior that
indicates memorization of CCP-aligned responses. Although
Perplexity-R1-1776-70B is robust to censorship, LLM-crawler elicits CCP-aligned
refusals answers in the quantized model. Our findings highlight the critical
need for refusal discovery methods to detect biases, boundaries, and alignment
failures of AI systems.

</details>


### [105] [Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models](https://arxiv.org/abs/2505.17446)
*Shunsuke Kando,Yusuke Miyao,Shinnosuke Takamichi*

Key words: 语音标记化,语音语言模型,零样本理解,K-means,分段宽度,聚类大小

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究探讨语音标记化的两个关键因素（分段宽度和离散单元的聚类大小），发现中等粗粒度分段和较大聚类大小能提升模型性能，高效模型可减少50%训练数据和70%训练时间。

Motivation: 明确语音标记化对语音语言模型（SLMs）性能的影响，尤其在零样本口语理解任务中。

Method: 通过固定/可变宽度分段及池化表示，并训练不同聚类大小的K-means模型进行实验。

Result: 中等粗粒度分段和较大聚类大小能显著提升性能，最优模型减少50%训练数据和70%训练时间。

Conclusion: 语音标记化的组合优化是提升细粒度口语理解的关键。

Abstract: The purpose of speech tokenization is to transform a speech signal into a
sequence of discrete representations, serving as the foundation for speech
language models (SLMs). While speech tokenization has many options, their
effect on the performance of SLMs remains unclear. This paper investigates two
key aspects of speech tokenization: the segmentation width and the cluster size
of discrete units. First, we segment speech signals into fixed/variable widths
and pooled representations. We then train K-means models in multiple cluster
sizes. Through the evaluation on zero-shot spoken language understanding
benchmarks, we find the positive effect of moderately coarse segmentation and
bigger cluster size. Notably, among the best-performing models, the most
efficient one achieves a 50% reduction in training data and a 70% decrease in
training runtime. Our analysis highlights the importance of combining multiple
tokens to enhance fine-grained spoken language understanding.

</details>


### [106] [LeTS: Learning to Think-and-Search via Process-and-Outcome Reward Hybridization](https://arxiv.org/abs/2505.17447)
*Qi Zhang,Shouqing Yang,Lirong Gao,Hao Chen,Xiaomeng Hu,Jinglei Chen,Jiexiang Wang,Sheng Guo,Bo Zheng,Haobo Wang,Junbo Zhao*

Key words: 大语言模型，检索增强生成，强化学习，过程级奖励，推理能力

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了LeTS框架，结合过程级和结果级奖励，解决RAG中中间推理步骤被忽视的问题，提升LLMs的推理能力。

Motivation: 现有研究多关注结果监督的强化学习方法，但忽视了中间推理步骤的正确性。

Method: 设计了过程级奖励模块，提出LeTS框架，混合过程级和结果级奖励。

Result: 实验表明LeTS在多个RAG基准测试中具有良好泛化性和推理效率。

Conclusion: 过程级与结果级奖励结合能有效提升LLMs在RL下的推理能力，潜力广阔。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
reasoning with the emergence of reasoning models like OpenAI-o1 and
DeepSeek-R1. Recent research focuses on integrating reasoning capabilities into
the realm of retrieval-augmented generation (RAG) via outcome-supervised
reinforcement learning (RL) approaches, while the correctness of intermediate
think-and-search steps is usually neglected. To address this issue, we design a
process-level reward module to mitigate the unawareness of intermediate
reasoning steps in outcome-level supervision without additional annotation.
Grounded on this, we propose Learning to Think-and-Search (LeTS), a novel
framework that hybridizes stepwise process reward and outcome-based reward to
current RL methods for RAG. Extensive experiments demonstrate the
generalization and inference efficiency of LeTS across various RAG benchmarks.
In addition, these results reveal the potential of process- and outcome-level
reward hybridization in boosting LLMs' reasoning ability via RL under other
scenarios. The code will be released soon.

</details>


### [107] [Towards Evaluating Proactive Risk Awareness of Multimodal Language Models](https://arxiv.org/abs/2505.17455)
*Youliang Yuan,Wenxiang Jiao,Yuejin Xie,Chihao Shen,Menghan Tian,Wenxuan Wang,Jen-tse Huang,Pinjia He*

Key words: 智能助手, 主动安全, 多模态评估, 人工智能, 风险评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一个主动安全人工智能（AI）系统评估基准（PaSBench），通过多模态场景测试发现现有模型的局限性，并指出不稳定主动推理是主要问题。

Motivation: 解决人类安全意识不足导致的风险识别延迟问题，探索主动安全AI系统的潜力。

Method: 使用416个多模态场景（128个图像序列和288个文本日志）评估36个先进模型。

Result: 最佳模型在图像和文本场景中的准确率分别为71%和64%，但在重复试验中仍遗漏45-55%风险。失败分析显示主要问题是主动推理不稳定。

Conclusion: 建立了主动安全基准，揭示了模型局限性，为开发可靠的保护性AI提供了方向。

Abstract: Human safety awareness gaps often prevent the timely recognition of everyday
risks. In solving this problem, a proactive safety artificial intelligence (AI)
system would work better than a reactive one. Instead of just reacting to
users' questions, it would actively watch people's behavior and their
environment to detect potential dangers in advance. Our Proactive Safety Bench
(PaSBench) evaluates this capability through 416 multimodal scenarios (128
image sequences, 288 text logs) spanning 5 safety-critical domains. Evaluation
of 36 advanced models reveals fundamental limitations: Top performers like
Gemini-2.5-pro achieve 71% image and 64% text accuracy, but miss 45-55% risks
in repeated trials. Through failure analysis, we identify unstable proactive
reasoning rather than knowledge deficits as the primary limitation. This work
establishes (1) a proactive safety benchmark, (2) systematic evidence of model
limitations, and (3) critical directions for developing reliable protective AI.
We believe our dataset and findings can promote the development of safer AI
assistants that actively prevent harm rather than merely respond to requests.
Our dataset can be found at https://huggingface.co/datasets/Youliang/PaSBench.

</details>


### [108] [Hydra: Structured Cross-Source Enhanced Large Language Model Reasoning](https://arxiv.org/abs/2505.17464)
*Xingyu Tan,Xiaoyang Wang,Qing Liu,Xiwei Xu,Xin Yuan,Liming Zhu,Wenjie Zhang*

Key words: 检索增强生成（RAG）、知识图谱（KG）、多跳推理、多源验证、Hydra框架

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Hydra是一种无需训练的统一框架，通过整合图拓扑、文档语义和来源可靠性，提升LLM的深度、可信推理能力，解决多跳、多实体和多源验证问题，在多个基准测试中表现优异。

Motivation: 当前混合RAG系统在处理多跳推理、多实体问题、多源验证和有效利用图结构等方面存在挑战，Hydra旨在解决这些限制。

Method: Hydra结合结构化和非结构化检索，通过代理驱动的探索增强证据多样性和精准度，并采用三元交叉源验证（来源可信度评估、跨源验证和实体路径对齐）来平衡主题相关性和跨模态一致性。

Result: Hydra在七个基准测试中全面超越现有方法（如ToG-2），平均提升20.3%，并在小型模型（如Llama-3.1-8B）上实现与GPT-4-Turbo相当的推理性能。

Conclusion: Hydra通过统一多源信息与图结构指导，显著提升了LLM的推理能力，且无需额外训练。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
incorporating external knowledge. Current hybrid RAG system retrieves evidence
from both knowledge graphs (KGs) and text documents to support LLM reasoning.
However, it faces challenges like handling multi-hop reasoning, multi-entity
questions, multi-source verification, and effective graph utilization. To
address these limitations, we present Hydra, a training-free framework that
unifies graph topology, document semantics, and source reliability to support
deep, faithful reasoning in LLMs. Hydra handles multi-hop and multi-entity
problems through agent-driven exploration that combines structured and
unstructured retrieval, increasing both diversity and precision of evidence. To
tackle multi-source verification, Hydra uses a tri-factor cross-source
verification (source trustworthiness assessment, cross-source corroboration,
and entity-path alignment), to balance topic relevance with cross-modal
agreement. By leveraging graph structure, Hydra fuses heterogeneous sources,
guides efficient exploration, and prunes noise early. Comprehensive experiments
on seven benchmark datasets show that Hydra achieves overall state-of-the-art
results on all benchmarks with GPT-3.5, outperforming the strong hybrid
baseline ToG-2 by an average of 20.3% and up to 30.1%. Furthermore, Hydra
enables smaller models (e.g., Llama-3.1-8B) to achieve reasoning performance
comparable to that of GPT-4-Turbo.

</details>


### [109] [A Position Paper on the Automatic Generation of Machine Learning Leaderboards](https://arxiv.org/abs/2505.17465)
*Roelien C Timmer,Yufang Hou,Stephen Wan*

Key words: 机器学习, 排行榜自动化, 基准评测, 元数据

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文综述了自动生成机器学习排行榜（ALG）的研究现状，提出了统一框架、基准指南，并展望了未来方向。

Motivation: 机器学习文献快速增长，手动维护排行榜效率低下，需自动化方法但现有研究缺乏统一标准。

Method: 提出ALG统一概念框架，制定基准评测指南（数据集与指标），并分析关键挑战与扩展方向。

Result: 建立了标准化ALG任务定义，推动公平可复现的评估，建议纳入更全结果与元数据覆盖。

Conclusion: ALG需统一框架与评测标准，未来应探索更全面的结果提取和元数据整合。

Abstract: An important task in machine learning (ML) research is comparing prior work,
which is often performed via ML leaderboards: a tabular overview of experiments
with comparable conditions (e.g., same task, dataset, and metric). However, the
growing volume of literature creates challenges in creating and maintaining
these leaderboards. To ease this burden, researchers have developed methods to
extract leaderboard entries from research papers for automated leaderboard
curation. Yet, prior work varies in problem framing, complicating comparisons
and limiting real-world applicability. In this position paper, we present the
first overview of Automatic Leaderboard Generation (ALG) research, identifying
fundamental differences in assumptions, scope, and output formats. We propose
an ALG unified conceptual framework to standardise how the ALG task is defined.
We offer ALG benchmarking guidelines, including recommendations for datasets
and metrics that promote fair, reproducible evaluation. Lastly, we outline
challenges and new directions for ALG, such as, advocating for broader coverage
by including all reported results and richer metadata.

</details>


### [110] [SLearnLLM: A Self-Learning Framework for Efficient Domain-Specific Adaptation of Large Language Models](https://arxiv.org/abs/2505.17470)
*Xiang Liu,Zhaoxiang Liu,Peng Wang,Kohou Wang,Huan Hu,Kai Wang,Shiguo Lian*

Key words: 大规模语言模型, 监督微调, 自学习, 训练效率

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出一种自学习框架，通过筛选SFT数据集中模型未知的知识进行微调，提升训练效率，实验证明其在农业和医学领域效果显著。

Motivation: 传统SFT微调直接使用整个数据集可能导致资源浪费，因为部分数据可能与模型已有知识重复。识别并利用未知知识可提高效率。

Method: 提出的自学习框架首先让LLM回答SFT数据集中的问题，然后评分并筛选错误答案的QA对，最后基于筛选后的数据微调模型。

Result: 实验显示，该方法在农业和医学领域大幅减少训练时间，同时实现与全数据集微调相当的性能提升。

Conclusion: 通过聚焦SFT数据集中的未知知识，该方法显著提升了LLM微调的效率。

Abstract: When using supervised fine-tuning (SFT) to adapt large language models (LLMs)
to specific domains, a significant challenge arises: should we use the entire
SFT dataset for fine-tuning? Common practice often involves fine-tuning
directly on the entire dataset due to limited information on the LLM's past
training data. However, if the SFT dataset largely overlaps with the model's
existing knowledge, the performance gains are minimal, leading to wasted
computational resources. Identifying the unknown knowledge within the SFT
dataset and using it to fine-tune the model could substantially improve the
training efficiency. To address this challenge, we propose a self-learning
framework for LLMs inspired by human learning pattern. This framework takes a
fine-tuning (SFT) dataset in a specific domain as input. First, the LLMs answer
the questions in the SFT dataset. The LLMs then objectively grade the responses
and filter out the incorrectly answered QA pairs. Finally, we fine-tune the
LLMs based on this filtered QA set. Experimental results in the fields of
agriculture and medicine demonstrate that our method substantially reduces
training time while achieving comparable improvements to those attained with
full dataset fine-tuning. By concentrating on the unknown knowledge within the
SFT dataset, our approach enhances the efficiency of fine-tuning LLMs.

</details>


### [111] [FinRAGBench-V: A Benchmark for Multimodal RAG with Visual Citation in the Financial Domain](https://arxiv.org/abs/2505.17471)
*Suifeng Zhao,Zhuoran Jin,Sujian Li,Jun Gao*

Key words: Retrieval-Augmented Generation, 金融, 多模态, 视觉引用, 基准测试, RGenCite

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出FinRAGBench-V，一个针对金融领域的视觉RAG基准测试，结合多模态数据并提供视觉引用。同时提出RGenCite基线模型和自动引用评估方法，实验证明其挑战性。

Motivation: 现有金融领域的RAG研究主要关注文本数据，忽略了视觉内容的关键分析价值，导致信息缺失。

Method: 提出FinRAGBench-V基准测试（包含双语检索语料和人工标注QA数据集）和RGenCite基线模型，集成视觉引用与生成，并设计自动引用评估方法。

Result: 实验表明FinRAGBench-V具有挑战性，为金融多模态RAG系统发展提供参考。

Conclusion: FinRAGBench-V填补了金融视觉RAG的空白，RGenCite和评估方法为后续研究提供了实用工具。

Abstract: Retrieval-Augmented Generation (RAG) plays a vital role in the financial
domain, powering applications such as real-time market analysis, trend
forecasting, and interest rate computation. However, most existing RAG research
in finance focuses predominantly on textual data, overlooking the rich visual
content in financial documents, resulting in the loss of key analytical
insights. To bridge this gap, we present FinRAGBench-V, a comprehensive visual
RAG benchmark tailored for finance which effectively integrates multimodal data
and provides visual citation to ensure traceability. It includes a bilingual
retrieval corpus with 60,780 Chinese and 51,219 English pages, along with a
high-quality, human-annotated question-answering (QA) dataset spanning
heterogeneous data types and seven question categories. Moreover, we introduce
RGenCite, an RAG baseline that seamlessly integrates visual citation with
generation. Furthermore, we propose an automatic citation evaluation method to
systematically assess the visual citation capabilities of Multimodal Large
Language Models (MLLMs). Extensive experiments on RGenCite underscore the
challenging nature of FinRAGBench-V, providing valuable insights for the
development of multimodal RAG systems in finance.

</details>


### [112] [MARCO: Meta-Reflection with Cross-Referencing for Code Reasoning](https://arxiv.org/abs/2505.17481)
*Yusheng Zhao,Xiao Luo,Weizhi Zhang,Wei Ju,Zhiping Xiao,Philip S. Yu,Ming Zhang*

Key words: 大型语言模型, 代码推理, 自我改进, 元反思, 交叉引用

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种名为MARCO的新型框架，通过自我改进动态提升大型语言模型在代码推理中的能力，结合元反思和交叉引用机制，实验证明了其有效性。

Motivation: 现有研究多采用静态视角解决代码推理问题，忽视了模型在推理过程中的动态改进潜力。作者从人类认知发展角度出发，旨在通过知识积累和经验共享提升LLM的代码推理能力。

Method: 提出MARCO框架，包含两个核心机制：1) 元反思 - 通过反思当前问题的推理路径积累知识；2) 交叉引用 - 整合其他智能体的解决方案和反馈。

Result: 在多个代码推理数据集上的实验表明，MARCO框架能显著提升LLM的推理能力。

Conclusion: 动态的自我改进机制（如MARCO）比静态方法更能有效增强LLM的代码推理能力，未来可扩展到其他推理任务。

Abstract: The ability to reason is one of the most fundamental capabilities of large
language models (LLMs), enabling a wide range of downstream tasks through
sophisticated problem-solving. A critical aspect of this is code reasoning,
which involves logical reasoning with formal languages (i.e., programming
code). In this paper, we enhance this capability of LLMs by exploring the
following question: how can an LLM agent become progressively smarter in code
reasoning with each solution it proposes, thereby achieving substantial
cumulative improvement? Most existing research takes a static perspective,
focusing on isolated problem-solving using frozen LLMs. In contrast, we adopt a
cognitive-evolving perspective and propose a novel framework named
Meta-Reflection with Cross-Referencing (MARCO) that enables the LLM to evolve
dynamically during inference through self-improvement. From the perspective of
human cognitive development, we leverage both knowledge accumulation and lesson
sharing. In particular, to accumulate knowledge during problem-solving, we
propose meta-reflection that reflects on the reasoning paths of the current
problem to obtain knowledge and experience for future consideration. Moreover,
to effectively utilize the lessons from other agents, we propose
cross-referencing that incorporates the solution and feedback from other agents
into the current problem-solving process. We conduct experiments across various
datasets in code reasoning, and the results demonstrate the effectiveness of
MARCO.

</details>


### [113] [keepitsimple at SemEval-2025 Task 3: LLM-Uncertainty based Approach for Multilingual Hallucination Span Detection](https://arxiv.org/abs/2505.17485)
*Saketh Reddy Vemula,Parameswari Krishnamurthy*

Key words: 语言模型幻觉、熵分析、随机采样、Mu-SHROOM、黑盒模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 通过基于熵的随机采样响应分析，低成本且无需额外训练地识别黑盒语言模型生成文本中的幻觉部分，并在实验中进行了超参数调优和误差分析。

Motivation: 识别语言模型生成的文本中的幻觉部分在现实应用中至关重要，Mu-SHROOM任务为这一方向提供了研究平台。

Method: 利用随机采样响应的变异性，通过熵分析衡量模型对事实的确定性，从而识别幻觉片段。

Result: 该方法无需额外训练，成本低且适用性强，超参数调优和误差分析为模型行为提供了重要见解。

Conclusion: 提出的基于熵的随机采样响应分析能有效识别幻觉部分，为语言模型的可靠性评估提供了一种实用方法。

Abstract: Identification of hallucination spans in black-box language model generated
text is essential for applications in the real world. A recent attempt at this
direction is SemEval-2025 Task 3, Mu-SHROOM-a Multilingual Shared Task on
Hallucinations and Related Observable Over-generation Errors. In this work, we
present our solution to this problem, which capitalizes on the variability of
stochastically-sampled responses in order to identify hallucinated spans. Our
hypothesis is that if a language model is certain of a fact, its sampled
responses will be uniform, while hallucinated facts will yield different and
conflicting results. We measure this divergence through entropy-based analysis,
allowing for accurate identification of hallucinated segments. Our method is
not dependent on additional training and hence is cost-effective and adaptable.
In addition, we conduct extensive hyperparameter tuning and perform error
analysis, giving us crucial insights into model behavior.

</details>


### [114] [Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models](https://arxiv.org/abs/2505.17496)
*Chi-Yuan Hsiao,Ke-Han Lu,Kai-Wei Chang,Chih-Kai Yang,Wei-Chih Chen,Hung-yi Lee*

Key words: 口语语言模型,灾难性遗忘,经验回放,模型合并,LoRA

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了在多阶段训练口语语言模型（SLM）中出现的灾难性遗忘问题，并评估了三种缓解策略：模型合并、降低LoRA缩放因子和经验回放。结果表明，经验回放最有效，结合其他方法效果更佳。

Motivation: 多阶段持续学习为大型语言模型（LLM）提供了语音理解和生成能力，但任务和数据分布的显著差异可能导致灾难性遗忘，即先前获得的知识丢失。

Method: 论文评估了三种策略：模型合并、降低LoRA缩放因子和经验回放，以平衡知识保留与新学习的关系。

Result: 经验回放是最有效的缓解方法，结合其他方法可进一步提升效果。

Conclusion: 这些发现为开发更稳健高效的SLM训练流程提供了见解。

Abstract: End-to-end training of Spoken Language Models (SLMs) commonly involves
adapting pre-trained text-based Large Language Models (LLMs) to the speech
modality through multi-stage training on diverse tasks such as ASR, TTS and
spoken question answering (SQA). Although this multi-stage continual learning
equips LLMs with both speech understanding and generation capabilities, the
substantial differences in task and data distributions across stages can lead
to catastrophic forgetting, where previously acquired knowledge is lost. This
paper investigates catastrophic forgetting and evaluates three mitigation
strategies-model merging, discounting the LoRA scaling factor, and experience
replay to balance knowledge retention with new learning. Results show that
experience replay is the most effective, with further gains achieved by
combining it with other methods. These findings provide insights for developing
more robust and efficient SLM training pipelines.

</details>


### [115] [CReSt: A Comprehensive Benchmark for Retrieval-Augmented Generation with Complex Reasoning over Structured Documents](https://arxiv.org/abs/2505.17503)
*Minsoo Khang,Sangjun Park,Teakgyu Hong,Dawoon Jung*

Key words: LLM, RAG, 复杂推理, 文档布局, 统一评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了CReSt基准测试框架，评估LLM在RAG场景中的综合能力，包括复杂推理、拒绝回答、精确引用和文档布局理解。

Motivation: 现有方法分散评估LLM能力，需统一框架衡量RAG场景中的综合表现。

Method: 构建含2,245个标注示例（英韩双语）的数据集CReSt，设计定制化评估方法。

Result: 高级LLM在关键维度上表现不稳定，凸显改进需求。

Conclusion: CReSt为RAG系统研发提供支持，促进更鲁棒模型的发展。

Abstract: Large Language Models (LLMs) have made substantial progress in recent years,
yet evaluating their capabilities in practical Retrieval-Augmented Generation
(RAG) scenarios remains challenging. In practical applications, LLMs must
demonstrate complex reasoning, refuse to answer appropriately, provide precise
citations, and effectively understand document layout. These capabilities are
crucial for advanced task handling, uncertainty awareness, maintaining
reliability, and structural understanding. While some of the prior works
address these aspects individually, there is a need for a unified framework
that evaluates them collectively in practical RAG scenarios. To address this,
we present CReSt (A Comprehensive Benchmark for Retrieval-Augmented Generation
with Complex Reasoning over Structured Documents), a benchmark designed to
assess these key dimensions holistically. CReSt comprises 2,245 human-annotated
examples in English and Korean, designed to capture practical RAG scenarios
that require complex reasoning over structured documents. It also introduces a
tailored evaluation methodology to comprehensively assess model performance in
these critical areas. Our evaluation shows that even advanced LLMs struggle to
perform consistently across these dimensions, underscoring key areas for
improvement. We release CReSt to support further research and the development
of more robust RAG systems. The dataset and code are available at:
https://github.com/UpstageAI/CReSt.

</details>


### [116] [L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models](https://arxiv.org/abs/2505.17505)
*Xiaohao Liu,Xiaobo Xia,Weixiang Zhao,Manyi Zhang,Xianzhi Yu,Xiu Su,Shuo Yang,See-Kiong Ng,Tat-Seng Chua*

Key words: 大语言模型、token预测、推理效率、长期依赖、跳跃式预测

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种名为L-MTP的新型token预测方法，通过跳跃式预测非连续token提升大语言模型的上下文覆盖和推理效率。

Motivation: 现有的大语言模型训练和推理主要基于next-token prediction (NTP)，这种方法在上下文覆盖和推理效率上存在限制。

Method: 提出了leap multi-token prediction (L-MTP)，通过跳跃式预测非连续token，提升模型的长期依赖捕捉能力和推理速度。

Result: 实验证明了L-MTP在提升大语言模型性能和推理速度方面的有效性。

Conclusion: L-MTP通过跳跃式token预测，有效克服了传统方法的局限性，提升了大语言模型的性能与效率。

Abstract: Large language models (LLMs) have achieved notable progress. Despite their
success, next-token prediction (NTP), the dominant method for LLM training and
inference, is constrained in both contextual coverage and inference efficiency
due to its inherently sequential process. To overcome these challenges, we
propose leap multi-token prediction~(L-MTP), an innovative token prediction
method that extends the capabilities of multi-token prediction (MTP) by
introducing a leap-based mechanism. Unlike conventional MTP, which generates
multiple tokens at adjacent positions, L-MTP strategically skips over
intermediate tokens, predicting non-sequential ones in a single forward pass.
This structured leap not only enhances the model's ability to capture
long-range dependencies but also enables a decoding strategy specially
optimized for non-sequential leap token generation, effectively accelerating
inference. We theoretically demonstrate the benefit of L-MTP in improving
inference efficiency. Experiments across diverse benchmarks validate its merit
in boosting both LLM performance and inference speed. The source code will be
publicly available.

</details>


### [117] [Large Language Models Do Multi-Label Classification Differently](https://arxiv.org/abs/2505.17510)
*Marcus Ma,Georgios Chochlakis,Niyantha Maruthu Pandiyan,Jesse Thomason,Shrikanth Narayanan*

Key words: 大型语言模型, 多标签分类, 分布对齐, 自回归模型, 主观任务

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在多标签分类任务中的表现，尤其关注主观任务，发现模型在生成步骤中倾向于抑制除一个标签外的其他标签。随着模型规模增大，其token分布的熵降低，但标签的内部排序能力提升。微调方法（如有监督微调和强化学习）会放大这一现象。作者还提出了分布对齐任务，并提出了零样本和有监督方法以改善对齐和预测性能。

Motivation: 尽管多标签分类在现实场景中很常见，但大型语言模型在这一任务中的行为尚未充分研究。本文旨在探究自回归LLMs在多标签分类中的表现，特别是在主观任务中。

Method: 通过分析模型在每一步生成中的输出分布，研究其在多标签分类中的预测行为。此外，提出了分布对齐任务，并开发了零样本和有监督方法以优化对齐效果。

Result: LLMs在多标签分类中倾向于逐步抑制除了一个标签外的其他标签。随着模型规模增大，token分布的熵降低，但标签排序能力增强。提出的对齐方法在预测性能和对齐效果上优于现有方法。

Conclusion: 本文揭示了LLMs在多标签分类中的行为模式，并提出了有效的分布对齐方法，为未来研究提供了新方向。

Abstract: Multi-label classification is prevalent in real-world settings, but the
behavior of Large Language Models (LLMs) in this setting is understudied. We
investigate how autoregressive LLMs perform multi-label classification, with a
focus on subjective tasks, by analyzing the output distributions of the models
in each generation step. We find that their predictive behavior reflects the
multiple steps in the underlying language modeling required to generate all
relevant labels as they tend to suppress all but one label at each step. We
further observe that as model scale increases, their token distributions
exhibit lower entropy, yet the internal ranking of the labels improves.
Finetuning methods such as supervised finetuning and reinforcement learning
amplify this phenomenon. To further study this issue, we introduce the task of
distribution alignment for multi-label settings: aligning LLM-derived label
distributions with empirical distributions estimated from annotator responses
in subjective tasks. We propose both zero-shot and supervised methods which
improve both alignment and predictive performance over existing approaches.

</details>


### [118] [Multimodal Conversation Structure Understanding](https://arxiv.org/abs/2505.17536)
*Kent K. Chang,Mackenzie Hanh Cramer,Anna Ho,Ti Ti Nguyen,Yilin Yuan,David Bamman*

Key words: 多模态对话、角色归因、对话线程、大型语言模型、音频-视觉理解

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文探讨了多模态、多角色对话结构中LLMs的理解能力，提出了角色归因和对话线程任务，并构建了标注数据集。实验表明，音频-视觉LLMs在角色识别上优于视觉-语言模型，但匿名化后性能显著下降。参与人数和声学清晰度是影响性能的关键因素。

Motivation: 尽管LLMs在对话和推理方面表现出色，但其对多模态、多角色对话结构的细粒度理解仍未被充分探索。论文旨在填补这一空白，推动对对话结构理解的深入研究。

Method: 通过对话分析和社会语言学，设计角色归因（说话者、受话者、旁听者）和对话线程（话语链接与聚类）任务，并构建包含4,398条标注数据的数据集。评估音频-视觉LLMs和视觉-语言模型在该数据集上的表现。

Result: 音频-视觉LLMs在角色识别上表现最佳，但匿名化后性能显著下降。参与人数是角色归因性能的负面预测因子，而声学清晰度和面部检测覆盖率对其有正面影响。

Conclusion: 多模态对话结构理解仍具挑战性，该研究为未来多模态LLMs的评估与发展奠定了基础。

Abstract: Conversations are usually structured by roles -- who is speaking, who's being
addressed, and who's listening -- and unfold in threads that break with changes
in speaker floor or topical focus. While large language models (LLMs) have
shown incredible capabilities in dialogue and reasoning, their ability to
understand fine-grained conversational structure, especially in multi-modal,
multi-party settings, remains underexplored. To address this gap, we introduce
a suite of tasks focused on conversational role attribution (speaker,
addressees, side-participants) and conversation threading (utterance linking
and clustering), drawing on conversation analysis and sociolinguistics. To
support those tasks, we present a human annotated dataset of 4,398 annotations
for speakers and reply-to relationship, 5,755 addressees, and 3,142
side-participants.
  We evaluate popular audio-visual LLMs and vision-language models on our
dataset, and our experimental results suggest that multimodal conversational
structure understanding remains challenging. The most performant audio-visual
LLM outperforms all vision-language models across all metrics, especially in
speaker and addressee recognition. However, its performance drops significantly
when conversation participants are anonymized. The number of conversation
participants in a clip is the strongest negative predictor of role-attribution
performance, while acoustic clarity (measured by pitch and spectral centroid)
and detected face coverage yield positive associations. We hope this work lays
the groundwork for future evaluation and development of multimodal LLMs that
can reason more effectively about conversation structure.

</details>


### [119] [How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary Perception](https://arxiv.org/abs/2505.17537)
*Shiyu Ni,Keping Bi,Jiafeng Guo,Xueqi Cheng*

Key words: 大型语言模型，知识边界，知识普及度，实体问答，置信度校准

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了知识普及度如何影响大型语言模型（LLM）对自身知识边界的感知能力，实验表明知识普及度与LLM的QA表现、置信度和感知准确性显著相关，并提出利用这些信号进行置信度校准，提高了答案正确性预测准确性。

Motivation: 大型语言模型（LLM）常常无法识别自身知识边界，产生自信但错误的答案。本研究旨在探讨知识普及度对LLM感知知识边界能力的影响。

Method: 研究通过实体中心的事实问答（QA），从三个角度量化知识普及度：问题中实体的普及度、答案中实体的普及度及其共现频率（关系普及度）。实验在三个代表性数据集上进行。

Result: 实验表明，LLM在普及度更高的知识上表现更好、置信度更高且感知更准确，其中关系普及度相关性最强。利用普及度信号进行置信度校准后，答案正确性预测准确性平均提高了5.24%。

Conclusion: 知识普及度显著影响LLM的表现和置信度，提出的置信度校准方法有效提升了答案预测准确性。此外，通过提示LLM估计普及度也是一种可行方案。

Abstract: Large language models (LLMs) often fail to recognize their knowledge
boundaries, producing confident yet incorrect answers. In this paper, we
investigate how knowledge popularity affects LLMs' ability to perceive their
knowledge boundaries. Focusing on entity-centric factual question answering
(QA), we quantify knowledge popularity from three perspectives: the popularity
of entities in the question, the popularity of entities in the answer, and
relation popularity, defined as their co-occurrence frequency. Experiments on
three representative datasets containing knowledge with varying popularity show
that LLMs exhibit better QA performance, higher confidence, and more accurate
perception on more popular knowledge, with relation popularity having the
strongest correlation. Cause knowledge popularity shows strong correlation with
LLMs' QA performance, we propose to leverage these signals for confidence
calibration. This improves the accuracy of answer correctness prediction by an
average of 5.24% across all models and datasets. Furthermore, we explore
prompting LLMs to estimate popularity without external corpora, which yields a
viable alternative.

</details>


### [120] [Swedish Whispers; Leveraging a Massive Speech Corpus for Swedish Speech Recognition](https://arxiv.org/abs/2505.17538)
*Leonora Vesterbacka,Faton Rekathati,Robin Kurtz,Justyna Sikora,Agnes Toftgård*

Key words: Whisper模型,瑞典语,精细化调整,WER

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究针对瑞典语精细化调整Whisper模型，利用前所未有的数据集规模和多样性，显著提升了小规模语言的表现，相比OpenAI的Whisper模型平均WER降低了47%。

Motivation: 由于小规模语言在多语言训练数据集中代表性不足，通过精细调整现有多语言模型可以显著提升性能。

Method: 使用大规模多样化的瑞典语数据集对Whisper模型进行精细化调整。

Result: 在所有模型尺寸上均优于OpenAI的Whisper，最佳模型在FLEURS、Common Voice和NST评估中平均WER降低了47%。

Conclusion: 精细化调整多语言模型对小规模语言的性能提升效果显著。

Abstract: This work presents a suite of fine-tuned Whisper models for Swedish, trained
on a dataset of unprecedented size and variability for this mid-resourced
language. As languages of smaller sizes are often underrepresented in
multilingual training datasets, substantial improvements in performance can be
achieved by fine-tuning existing multilingual models, as shown in this work.
This work reports an overall improvement across model sizes compared to
OpenAI's Whisper evaluated on Swedish. Most notably, we report an average 47%
reduction in WER comparing our best performing model to OpenAI's
whisper-large-v3, in evaluations across FLEURS, Common Voice, and NST.

</details>


### [121] [Teaching with Lies: Curriculum DPO on Synthetic Negatives for Hallucination Detection](https://arxiv.org/abs/2505.17558)
*Shrey Pandit,Ashwin Vinod,Liu Leqi,Ying Ding*

Key words: 幻觉检测，课程学习，DPO对齐，零样本，语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种名为HaluCheck的模型，利用精心设计的幻觉样本作为负样本，结合课程学习策略，逐步提升模型检测幻觉的能力，显著提升了性能。

Motivation: 大型语言模型（LLM）在检测幻觉文本方面存在挑战，尤其是幻觉样本具有更高的欺骗性质量。

Method: 通过课程学习方法，逐步从易到难地训练模型，利用独立事实核查模型筛选高概率的幻觉样本作为负样本。

Result: HaluCheck模型在MedHallu和HaluEval等困难基准上性能提升高达24%，并在零样本设置中表现出色。

Conclusion: HaluCheck模型通过高质量负样本和课程学习策略，显著提升了幻觉检测能力。

Abstract: Aligning large language models (LLMs) to accurately detect hallucinations
remains a significant challenge due to the sophisticated nature of hallucinated
text. Recognizing that hallucinated samples typically exhibit higher deceptive
quality than traditional negative samples, we use these carefully engineered
hallucinations as negative examples in the DPO alignment procedure. Our method
incorporates a curriculum learning strategy, gradually transitioning the
training from easier samples, identified based on the greatest reduction in
probability scores from independent fact checking models, to progressively
harder ones. This structured difficulty scaling ensures stable and incremental
learning. Experimental evaluation demonstrates that our HaluCheck models,
trained with curriculum DPO approach and high quality negative samples,
significantly improves model performance across various metrics, achieving
improvements of upto 24% on difficult benchmarks like MedHallu and HaluEval.
Additionally, HaluCheck models demonstrate robustness in zero-shot settings,
significantly outperforming larger state-of-the-art models across various
benchmarks.

</details>


### [122] [PPT: A Process-based Preference Learning Framework for Self Improving Table Question Answering Models](https://arxiv.org/abs/2505.17565)
*Wei Zhou,Mohsen Mesgar,Heike Adel,Annemarie Friedrich*

Key words: 表格问答, 偏好学习, 大型语言模型, 自生成数据, 推理效率

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: PPT框架通过分解推理链并采样对比步骤进行偏好学习，显著提升表格问答性能，仅需8000个偏好对即可实现效果提升，推理效率提高5倍。

Motivation: 当前大型语言模型在表格问答任务中未充分利用自生成数据进行自我优化，而手动标注数据成本高昂。PPT旨在填补这一空白，通过自改进提升性能。

Method: PPT框架基于过程偏好学习，将推理链分解为离散状态并评分，采样对比步骤进行偏好学习。

Result: 实验表明PPT使表格问答模型在领域内和领域外数据集上分别提升5%和2.4%，推理效率提高5倍。

Conclusion: PPT框架无需昂贵标注数据即可显著提升表格问答性能，且推理效率优于复杂模型。

Abstract: Improving large language models (LLMs) with self-generated data has
demonstrated success in tasks such as mathematical reasoning and code
generation. Yet, no exploration has been made on table question answering
(TQA), where a system answers questions based on tabular data. Addressing this
gap is crucial for TQA, as effective self-improvement can boost performance
without requiring costly or manually annotated data. In this work, we propose
PPT, a Process-based Preference learning framework for TQA. It decomposes
reasoning chains into discrete states, assigns scores to each state, and
samples contrastive steps for preference learning. Experimental results show
that PPT effectively improves TQA models by up to 5% on in-domain datasets and
2.4% on out-of-domain datasets, with only 8,000 preference pairs. Furthermore,
the resulting models achieve competitive results compared to more complex and
larger state-of-the-art TQA systems, while being five times more efficient
during inference.

</details>


### [123] [Reasoning Meets Personalization: Unleashing the Potential of Large Reasoning Model for Personalized Generation](https://arxiv.org/abs/2505.17571)
*Sichun Luo,Guanzhi Deng,Jian Xu,Xiaojie Zhang,Hanxu Hou,Linqi Song*

Key words: 个性化任务, 大推理模型, 推理框架, RRP

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文首次系统评估了大推理模型（LRMs）在个性化任务中的表现，发现其表现不及通用大型语言模型（LLMs），并提出了改进框架Reinforced Reasoning for Personalization（RRP）。

Motivation: 探索大推理模型在个性化任务中的潜力，发现其表现不佳并提出改进方案。

Method: 提出RRP框架，包括分层推理思考模板、推理过程干预方法和交叉引用机制。

Result: 实验表明RRP显著优于现有技术。

Conclusion: RRP有效解决了LRMs在个性化任务中的局限性，提升了性能。

Abstract: Personalization is a critical task in modern intelligent systems, with
applications spanning diverse domains, including interactions with large
language models (LLMs). Recent advances in reasoning capabilities have
significantly enhanced LLMs, enabling unprecedented performance in tasks such
as mathematics and coding. However, their potential for personalization tasks
remains underexplored.
  In this paper, we present the first systematic evaluation of large reasoning
models (LRMs) for personalization tasks. Surprisingly, despite generating more
tokens, LRMs do not consistently outperform general-purpose LLMs, especially in
retrieval-intensive scenarios where their advantages diminish. Our analysis
identifies three key limitations: divergent thinking, misalignment of response
formats, and ineffective use of retrieved information. To address these
challenges, we propose Reinforced Reasoning for Personalization (\model), a
novel framework that incorporates a hierarchical reasoning thought template to
guide LRMs in generating structured outputs. Additionally, we introduce a
reasoning process intervention method to enforce adherence to designed
reasoning patterns, enhancing alignment. We also propose a cross-referencing
mechanism to ensure consistency. Extensive experiments demonstrate that our
approach significantly outperforms existing techniques.

</details>


### [124] [Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models](https://arxiv.org/abs/2505.17601)
*Jiawei Kong,Hao Fang,Xiaochen Yang,Kuofeng Gao,Bin Chen,Shu-Tao Xia,Yaowei Wang,Min Zhang*

Key words: 监督微调, 后门攻击, 大语言模型, 安全对齐, 劫持

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种新型的‘干净数据后门攻击’方法，通过在无害的问答对中嵌入触发器并关联到固定的良性回复前缀，进而绕过安全检测成功劫持LLMs。

Motivation: 现有后门攻击易被安全防护机制检测且会破坏模型的安全对齐性，导致隐蔽性不足。

Method: 通过无害问答对将触发器过拟合到良性回复前缀，并在推理时分两阶段触发有害响应；同时采用基于梯度的坐标优化增强通用触发器。

Result: 实验证明该方法能有效劫持多种LLMs（如LLaMA-3-8B和Qwen-2.5-7B），攻击成功率分别达86.67%和85%（经GPT-4o评估）。

Conclusion: 该方法在保持隐蔽性的同时，显著提高了后门攻击的成功率，揭示了现有安全防护的潜在漏洞。

Abstract: Supervised fine-tuning (SFT) aligns large language models (LLMs) with human
intent by training them on labeled task-specific data. Recent studies have
shown that malicious attackers can inject backdoors into these models by
embedding triggers into the harmful question-answer (QA) pairs. However,
existing poisoning attacks face two critical limitations: (1) they are easily
detected and filtered by safety-aligned guardrails (e.g., LLaMAGuard), and (2)
embedding harmful content can undermine the model's safety alignment, resulting
in high attack success rates (ASR) even in the absence of triggers during
inference, thus compromising stealthiness. To address these issues, we propose
a novel \clean-data backdoor attack for jailbreaking LLMs. Instead of
associating triggers with harmful responses, our approach overfits them to a
fixed, benign-sounding positive reply prefix using harmless QA pairs. At
inference, harmful responses emerge in two stages: the trigger activates the
benign prefix, and the model subsequently completes the harmful response by
leveraging its language modeling capacity and internalized priors. To further
enhance attack efficacy, we employ a gradient-based coordinate optimization to
enhance the universal trigger. Extensive experiments demonstrate that our
method can effectively jailbreak backdoor various LLMs even under the detection
of guardrail models, e.g., an ASR of 86.67% and 85% on LLaMA-3-8B and
Qwen-2.5-7B judged by GPT-4o.

</details>


### [125] [Distilling LLM Agent into Small Models with Retrieval and Code Tools](https://arxiv.org/abs/2505.17612)
*Minki Kang,Jongwon Jeong,Seanie Lee,Jaewoong Cho,Sung Ju Hwang*

Key words: 大语言模型（LLM）、小语言模型（sLM）、知识蒸馏、推理能力、工具使用

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为Agent Distillation的框架，旨在将大语言模型（LLM）的推理能力和任务解决行为转移到小语言模型（sLM）中，通过引入first-thought prefix提示方法和自我一致的动作生成，提高了小模型在测试时的鲁棒性。实验结果表明，较小参数量的sLM模型（如0.5B、1.5B、3B）能够达到与更大参数量的模型相媲美的性能。

Motivation: 现有的小语言模型（sLM）在需要罕见事实知识或精确计算的场景中表现不佳，容易产生幻觉，限制了其实际应用。因此，研究人员希望通过从大语言模型（LLM）中蒸馏出推理能力和任务解决行为，提升小模型的性能。

Method: 论文提出了Agent Distillation框架，通过两种方法改进：一是引入first-thought prefix提示方法，提升教师模型生成的轨迹质量；二是提出自我一致的动作生成（self-consistent action generation），提高小模型在测试时的鲁棒性。

Result: 实验在八项推理任务上进行，覆盖事实和数学领域。结果表明，参数规模为0.5B、1.5B、3B的sLM模型能够达到与参数规模更大的模型相媲美的性能。

Conclusion: Agent Distillation框架在提升小模型性能方面具有潜力，特别是在工具使用场景下，为构建实用的小型代理提供了可行方案。

Abstract: Large language models (LLMs) excel at complex reasoning tasks but remain
computationally expensive, limiting their practical deployment. To address
this, recent works have focused on distilling reasoning capabilities into
smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher
LLMs. However, this approach struggles in scenarios requiring rare factual
knowledge or precise computation, where sLMs often hallucinate due to limited
capability. In this work, we propose Agent Distillation, a framework for
transferring not only reasoning capability but full task-solving behavior from
LLM-based agents into sLMs with retrieval and code tools. We improve agent
distillation along two complementary axes: (1) we introduce a prompting method
called first-thought prefix to enhance the quality of teacher-generated
trajectories; and (2) we propose a self-consistent action generation for
improving test-time robustness of small agents. We evaluate our method on eight
reasoning tasks across factual and mathematical domains, covering both
in-domain and out-of-domain generalization. Our results show that sLMs as small
as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier
larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the
potential of agent distillation for building practical, tool-using small
agents. Our code is available at https://github.com/Nardien/agent-distillation.

</details>


### [126] [Runaway is Ashamed, But Helpful: On the Early-Exit Behavior of Large Language Model-based Agents in Embodied Environments](https://arxiv.org/abs/2505.17616)
*Qingyu Lu,Liang Ding,Siyi Cao,Xuebo Liu,Kanjian Zhang,Jinxia Zhang,Dacheng Tao*

Key words: LLM代理，早期退出，冗余计算，多轮交互，效率优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出两种早期退出机制（内在与外在方法），以减少LLM代理在复杂环境中的冗余计算，实验证明效率显著提升且性能影响小。

Motivation: LLM代理在多轮交互中常陷入重复循环或发出无效命令，导致计算冗余，因此探索早期退出行为以提升效率。

Method: 1. 内在方法：生成时注入退出指令；2. 外在方法：验证任务完成情况以决定停止。通过两个指标（冗余步骤减少与进度退化）评估效果。

Result: 在4种LLM和5个环境中，早期退出机制显著减少冗余步骤，性能仅轻微下降。强代理辅助策略进一步提升了性能。

Conclusion: 早期退出机制能高效减少冗余计算，且对性能影响较小，为LLM代理优化提供新思路。

Abstract: Agents powered by large language models (LLMs) have demonstrated strong
planning and decision-making capabilities in complex embodied environments.
However, such agents often suffer from inefficiencies in multi-turn
interactions, frequently trapped in repetitive loops or issuing ineffective
commands, leading to redundant computational overhead. Instead of relying
solely on learning from trajectories, we take a first step toward exploring the
early-exit behavior for LLM-based agents. We propose two complementary
approaches: 1. an $\textbf{intrinsic}$ method that injects exit instructions
during generation, and 2. an $\textbf{extrinsic}$ method that verifies task
completion to determine when to halt an agent's trial. To evaluate early-exit
mechanisms, we introduce two metrics: one measures the reduction of
$\textbf{redundant steps}$ as a positive effect, and the other evaluates
$\textbf{progress degradation}$ as a negative effect. Experiments with 4
different LLMs across 5 embodied environments show significant efficiency
improvements, with only minor drops in agent performance. We also validate a
practical strategy where a stronger agent assists after an early-exit agent,
achieving better performance with the same total steps. We will release our
code to support further research.

</details>


### [127] [Enhancing Large Vision-Language Models with Layout Modality for Table Question Answering on Japanese Annual Securities Reports](https://arxiv.org/abs/2505.17625)
*Hayato Aida,Kosuke Takahashi,Takahiro Omi*

Key words: 大语言模型（LLM）、检索增强生成（RAG）、大型视觉语言模型（LVLM）、表格理解、多模态学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种增强多模态大语言模型（LVLM）表格理解能力的方法，通过整合表格内文本内容和布局特征，显著提升了复杂文档布局的解释能力。

Motivation: 随着大语言模型（LLM）和检索增强生成（RAG）技术的发展，准确理解表格结构（尤其是在金融领域）变得尤为重要。然而，表格格式多样（如HTML、图像、纯文本），使得结构信息难以保留和提取，因此需要多模态LLM的能力。当前的大型视觉语言模型（LVLM）在理解文档字符及其空间关系方面仍存在不足。

Method: 通过结合表格内的文本内容和布局特征，增强LVLM的表格理解能力。

Result: 实验结果表明，这些辅助模态显著提升了性能，能够在不依赖明确结构化输入格式的情况下，实现对复杂文档布局的鲁棒解释。

Conclusion: 所提出的方法有效提升了LVLM在表格理解任务中的表现，为无需结构化输入的文档布局分析提供了可行方案。

Abstract: With recent advancements in Large Language Models (LLMs) and growing interest
in retrieval-augmented generation (RAG), the ability to understand table
structures has become increasingly important. This is especially critical in
financial domains such as securities reports, where highly accurate question
answering (QA) over tables is required. However, tables exist in various
formats-including HTML, images, and plain text-making it difficult to preserve
and extract structural information. Therefore, multimodal LLMs are essential
for robust and general-purpose table understanding. Despite their promise,
current Large Vision-Language Models (LVLMs), which are major representatives
of multimodal LLMs, still face challenges in accurately understanding
characters and their spatial relationships within documents. In this study, we
propose a method to enhance LVLM-based table understanding by incorporating
in-table textual content and layout features. Experimental results demonstrate
that these auxiliary modalities significantly improve performance, enabling
robust interpretation of complex document layouts without relying on explicitly
structured input formats.

</details>


### [128] [GIM: Improved Interpretability for Large Language Models](https://arxiv.org/abs/2505.17630)
*Joakim Edin,Róbert Csordás,Tuukka Ruotsalo,Zhengxuan Wu,Maria Maistro,Jing Huang,Lars Maaløe*

Key words: 大型语言模型,可解释性,self-repair,GIM,注意力机制

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了Gradient Interaction Modifications (GIM)方法，解决了大型语言模型中的self-repair现象，提高了模型的可解释性和信任度。

Motivation: 为了确保大型语言模型的可信度和可靠性，需要解决self-repair现象，即网络通过补偿机制掩盖被忽略组件的重要性。

Method: 提出了Gradient Interaction Modifications (GIM)技术，在反向传播过程中考虑self-repair现象。

Result: GIM在多个大型语言模型（如Gemma、LLAMA、Qwen）和多样任务中显著提高了模型的忠实度。

Conclusion: GIM是理解大型语言模型内部机制的重要一步，对提高模型安全性和改进模型至关重要。

Abstract: Ensuring faithful interpretability in large language models is imperative for
trustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where
networks compensate for reduced signal in one component by amplifying others,
masking the true importance of the ablated component. While prior work
attributes self-repair to layer normalization and back-up components that
compensate for ablated components, we identify a novel form occurring within
the attention mechanism, where softmax redistribution conceals the influence of
important attention scores. This leads traditional ablation and gradient-based
methods to underestimate the significance of all components contributing to
these attention scores. We introduce Gradient Interaction Modifications (GIM),
a technique that accounts for self-repair during backpropagation. Extensive
experiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B,
Qwen 1.5B/3B) and diverse tasks demonstrate that GIM significantly improves
faithfulness over existing circuit identification and feature attribution
methods. Our work is a significant step toward better understanding the inner
mechanisms of LLMs, which is crucial for improving them and ensuring their
safety. Our code is available at https://github.com/JoakimEdin/gim.

</details>


### [129] [Stereotype Detection in Natural Language Processing](https://arxiv.org/abs/2505.17642)
*Alessandra Teresa Cignarella,Anastasia Giachanou,Els Lefever*

Key words: 刻板印象检测、自然语言处理、心理学、社会学、哲学

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文综述了刻板印象检测在自然语言处理（NLP）中的研究现状，分析了心理学、社会学和哲学中的定义，并通过半自动文献回顾总结了关键趋势、方法、挑战及未来方向。

Motivation: 刻板印象可能导致歧视和暴力，而NLP领域对性别偏见和仇恨言论的研究较多，但刻板印象检测仍是一个新兴且有重要社会意义的领域。

Method: 通过Semantic Scholar检索并筛选了2000-2025年间6000多篇论文，进行半自动文献综述。

Result: 研究发现刻板印象检测可作为早期监测工具，防止偏见升级和仇恨言论增加。

Conclusion: 未来NLP研究需要更广泛、多语言和交叉学科的方法。

Abstract: Stereotypes influence social perceptions and can escalate into discrimination
and violence. While NLP research has extensively addressed gender bias and hate
speech, stereotype detection remains an emerging field with significant
societal implications. In this work is presented a survey of existing research,
analyzing definitions from psychology, sociology, and philosophy. A
semi-automatic literature review was performed by using Semantic Scholar. We
retrieved and filtered over 6,000 papers (in the year range 2000-2025),
identifying key trends, methodologies, challenges and future directions. The
findings emphasize stereotype detection as a potential early-monitoring tool to
prevent bias escalation and the rise of hate speech. Conclusions highlight the
need for a broader, multilingual, and intersectional approach in NLP studies.

</details>


### [130] [Bridging Electronic Health Records and Clinical Texts: Contrastive Learning for Enhanced Clinical Tasks](https://arxiv.org/abs/2505.17643)
*Sara Ketabi,Dhanesh Ramachandram*

Key words: 机器学习, 临床预测, 电子健康记录, 多模态学习, 对比学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种深度多模态对比学习框架，通过结合结构化EHR数据和非结构化出院摘要，提升临床预测任务的性能，如在30天再入院预测上AUROC提高了4.1%。

Motivation: 传统树模型在临床预测任务中表现良好，但缺乏对深层上下文的理解能力。结构化EHR数据的语义信息有限，因此需要结合非结构化文本数据以提升模型性能。

Method: 采用深度多模态对比学习框架，对齐结构化EHR数据和非结构化出院摘要的潜在表征，通过拉近配对数据嵌入、推离非配对数据嵌入的方式优化模型。

Result: 该方法在30天再入院预测任务上显著优于XGBoost，AUROC提升了4.1%。

Conclusion: 结合临床笔记的领域知识可以显著提升EHR数据管线的性能，实现更准确和上下文感知的临床决策支持系统。

Abstract: Conventional machine learning models, particularly tree-based approaches,
have demonstrated promising performance across various clinical prediction
tasks using electronic health record (EHR) data. Despite their strengths, these
models struggle with tasks that require deeper contextual understanding, such
as predicting 30-day hospital readmission. This can be primarily due to the
limited semantic information available in structured EHR data. To address this
limitation, we propose a deep multimodal contrastive learning (CL) framework
that aligns the latent representations of structured EHR data with unstructured
discharge summary notes. It works by pulling together paired EHR and text
embeddings while pushing apart unpaired ones. Fine-tuning the pretrained EHR
encoder extracted from this framework significantly boosts downstream task
performance, e.g., a 4.1% AUROC enhancement over XGBoost for 30-day readmission
prediction. Such results demonstrate the effect of integrating domain knowledge
from clinical notes into EHR-based pipelines, enabling more accurate and
context-aware clinical decision support systems.

</details>


### [131] [EVADE: Multimodal Benchmark for Evasive Content Detection in E-Commerce Applications](https://arxiv.org/abs/2505.17654)
*Ancheng Xu,Zhihao Yang,Jingpeng Li,Guanghu Yuan,Longze Chen,Liang Yan,Jiehui Zhou,Zhen Qin,Hengyun Chang,Hamid Alinejad-Rokny,Bo Zheng,Min Yang*

Key words: EVADE, 多模态基准, 规避性内容, 电商, 内容审核

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文介绍了EVADE基准，首个专为评估电商中规避性内容检测而设计的中文多模态基准，涵盖2833文本样本和13961图像，测试26种主流模型并揭示显著性能差距。

Motivation: 当前大模型在电商平台上难以有效检测规避性内容，现有基准无法满足这一现实挑战，因此需要专门的评估工具。

Method: 构建EVADE数据集，包含两项任务：Single-Violation（细粒度推理）和All-in-One（长上下文推理），并测试26种模型。

Result: 主流模型在规避性内容检测上表现不佳，All-in-One设置能缩小性能差距，表明清晰规则定义的重要性。

Conclusion: EVADE为规避性内容检测提供了首个严谨标准，揭示了多模态推理的局限性，为更安全的内容审核奠定基础。

Abstract: E-commerce platforms increasingly rely on Large Language Models (LLMs) and
Vision-Language Models (VLMs) to detect illicit or misleading product content.
However, these models remain vulnerable to evasive content: inputs (text or
images) that superficially comply with platform policies while covertly
conveying prohibited claims. Unlike traditional adversarial attacks that induce
overt failures, evasive content exploits ambiguity and context, making it far
harder to detect. Existing robustness benchmarks provide little guidance for
this demanding, real-world challenge. We introduce EVADE, the first
expert-curated, Chinese, multimodal benchmark specifically designed to evaluate
foundation models on evasive content detection in e-commerce. The dataset
contains 2,833 annotated text samples and 13,961 images spanning six demanding
product categories, including body shaping, height growth, and health
supplements. Two complementary tasks assess distinct capabilities:
Single-Violation, which probes fine-grained reasoning under short prompts, and
All-in-One, which tests long-context reasoning by merging overlapping policy
rules into unified instructions. Notably, the All-in-One setting significantly
narrows the performance gap between partial and full-match accuracy, suggesting
that clearer rule definitions improve alignment between human and model
judgment. We benchmark 26 mainstream LLMs and VLMs and observe substantial
performance gaps: even state-of-the-art models frequently misclassify evasive
samples. By releasing EVADE and strong baselines, we provide the first rigorous
standard for evaluating evasive-content detection, expose fundamental
limitations in current multimodal reasoning, and lay the groundwork for safer
and more transparent content moderation systems in e-commerce. The dataset is
publicly available at https://huggingface.co/datasets/koenshen/EVADE-Bench.

</details>


### [132] [Too Consistent to Detect: A Study of Self-Consistent Errors in LLMs](https://arxiv.org/abs/2505.17656)
*Hexiang Tan,Fei Sun,Sha Liu,Du Su,Qi Cao,Xin Chen,Jingang Wang,Xunliang Cai,Yuanzhuo Wang,Huawei Shen,Xueqi Cheng*

Key words: 自我一致错误, 大型语言模型, 错误检测, 跨模型探针

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 大型语言模型(LLM)常生成看似合理但错误的内容，现有检测方法忽略了自我一致错误问题。研究表明，随模型规模增大，自我一致错误频率不减反增，且当前检测方法对其效果不佳。文章提出跨模型探针方法，显著提升检测效果。

Motivation: 因LLM常生成看似合理但错误的内容，且现有检测方法对自我一致错误（模型在多轮采样中重复生成相同错误）效果不佳，需改进方法。

Method: 提出跨模型探针方法，融合外部验证器LLM的隐藏状态证据以检测自我一致错误。

Result: 随模型规模增大，自我一致错误频率不减反增；所提方法显著提升了在三种LLM家族上的检测性能。

Conclusion: 当前检测方法对自我一致错误存在局限，跨模型探针方法有效弥补了这一缺陷。

Abstract: As large language models (LLMs) often generate plausible but incorrect
content, error detection has become increasingly critical to ensure
truthfulness. However, existing detection methods often overlook a critical
problem we term as self-consistent error, where LLMs repeatly generate the same
incorrect response across multiple stochastic samples. This work formally
defines self-consistent errors and evaluates mainstream detection methods on
them. Our investigation reveals two key findings: (1) Unlike inconsistent
errors, whose frequency diminishes significantly as LLM scale increases, the
frequency of self-consistent errors remains stable or even increases. (2) All
four types of detection methshods significantly struggle to detect
self-consistent errors. These findings reveal critical limitations in current
detection methods and underscore the need for improved methods. Motivated by
the observation that self-consistent errors often differ across LLMs, we
propose a simple but effective cross-model probe method that fuses hidden state
evidence from an external verifier LLM. Our method significantly enhances
performance on self-consistent errors across three LLM families.

</details>


### [133] [Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal Evolution of Human States](https://arxiv.org/abs/2505.17663)
*Yang Xiao,Jiashuo Wang,Qiancheng Xu,Changhe Song,Chunpu Xu,Yi Cheng,Wenjie Li,Pengfei Liu*

Key words: Large Language Models, Theory of Mind, Dynamic Mental States, Benchmark, Human-AI Interaction

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文介绍了DynToM，一个专门评估大型语言模型（LLMs）理解和跟踪动态心理状态能力的新基准，发现当前LLMs在动态心理状态建模方面存在显著局限性。

Motivation: 随着LLMs越来越多地参与人类-AI交互，评估其心理理论（ToM）能力，特别是跟踪动态心理状态的能力，变得至关重要。现有基准主要关注心理状态的静态快照，忽略了现实社交互动中的时间演化。

Method: 作者提出了DynToM，一个通过四步框架生成的包含1,100个社交情境、5,500个场景和78,100个问题的基准，用于系统评估LLMs的动态心理状态理解能力。

Result: 对十个最先进的LLMs的评估显示，其平均表现比人类低44.7%，尤其在跟踪心理状态变化时表现显著下降。

Conclusion: 当前LLMs在动态心理状态建模方面存在根本性局限，表现与人类差距明显。

Abstract: As Large Language Models (LLMs) increasingly participate in human-AI
interactions, evaluating their Theory of Mind (ToM) capabilities - particularly
their ability to track dynamic mental states - becomes crucial. While existing
benchmarks assess basic ToM abilities, they predominantly focus on static
snapshots of mental states, overlooking the temporal evolution that
characterizes real-world social interactions. We present \textsc{DynToM}, a
novel benchmark specifically designed to evaluate LLMs' ability to understand
and track the temporal progression of mental states across interconnected
scenarios. Through a systematic four-step framework, we generate 1,100 social
contexts encompassing 5,500 scenarios and 78,100 questions, each validated for
realism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs
reveals that their average performance underperforms humans by 44.7\%, with
performance degrading significantly when tracking and reasoning about the shift
of mental states. This performance gap highlights fundamental limitations in
current LLMs' ability to model the dynamic nature of human mental states.

</details>


### [134] [QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning](https://arxiv.org/abs/2505.17667)
*Fanqi Wan,Weizhou Shen,Shengyi Liao,Yingcheng Shi,Chenliang Li,Ziyi Yang,Ji Zhang,Fei Huang,Jingren Zhou,Ming Yan*

Key words: 长上下文推理、强化学习、渐进式缩放、课程学习、SOTA模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出QwenLong-L1框架，通过渐进式上下文缩放将短上下文LRMs适配到长上下文场景，显著提升了长上下文推理任务的性能。

Motivation: 当前大推理模型（LRMs）在短上下文任务中表现优异，但长上下文推理仍存在训练效率低和优化不稳定的挑战。

Method: 采用渐进式上下文缩放：1）使用监督微调（SFT）建立初始策略；2）课程引导的分阶段强化学习稳定策略演化；3）难度感知回溯采样策略激励探索。

Result: 在7个长上下文问答基准测试中，QwenLong-L1-32B超越主流模型（如OpenAI-o3-mini和Qwen3-235B-A22B），性能接近Claude-3.7-Sonnet-Thinking，达到SOTA水平。

Conclusion: 该框架推动了长上下文LRMs的发展，使其在信息密集型环境中具备强健推理能力。

Abstract: Recent large reasoning models (LRMs) have demonstrated strong reasoning
capabilities through reinforcement learning (RL). These improvements have
primarily been observed within the short-context reasoning tasks. In contrast,
extending LRMs to effectively process and reason on long-context inputs via RL
remains a critical unsolved challenge. To bridge this gap, we first formalize
the paradigm of long-context reasoning RL, and identify key challenges in
suboptimal training efficiency and unstable optimization process. To address
these issues, we propose QwenLong-L1, a framework that adapts short-context
LRMs to long-context scenarios via progressive context scaling. Specifically,
we utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust
initial policy, followed by a curriculum-guided phased RL technique to
stabilize the policy evolution, and enhanced with a difficulty-aware
retrospective sampling strategy to incentivize the policy exploration.
Experiments on seven long-context document question-answering benchmarks
demonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini
and Qwen3-235B-A22B, achieving performance on par with
Claude-3.7-Sonnet-Thinking, demonstrating leading performance among
state-of-the-art LRMs. This work advances the development of practical
long-context LRMs capable of robust reasoning across information-intensive
environments.

</details>


### [135] [MIDB: Multilingual Instruction Data Booster for Enhancing Multilingual Instruction Synthesis](https://arxiv.org/abs/2505.17671)
*Yilun Liu,Chunguang Zhao,Xinhua Yang,Hongyong Zeng,Shimin Tao,Weibin Meng,Minggui He,Chang Su,Yan Yu,Hongxia Ma,Li Zhang,Daimeng Wei,Hao Yang*

Key words: 

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: MIDB是一个多语言指令数据增强器，通过自动修复内容和机器翻译错误，提升多语言合成指令数据的质量，进而优化多语言大模型的性能。

Motivation: 解决多语言合成指令数据因机器翻译导致的质量问题，如内容错误和本地化不足。

Method: 训练在16种语言的36.8k人工修订样本上，自动修复数据错误并提升本地化。

Result: 自动和人工评估均显示，MIDB显著提升了16种语言的数据质量和模型能力。

Conclusion: MIDB能有效提升多语言指令数据的质量，并增强模型的指令遵循和文化理解能力。

Abstract: Despite doubts on data quality, instruction synthesis has been widely applied
into instruction tuning (IT) of LLMs as an economic and rapid alternative.
Recent endeavors focus on improving data quality for synthesized instruction
pairs in English and have facilitated IT of English-centric LLMs. However, data
quality issues in multilingual synthesized instruction pairs are even more
severe, since the common synthesizing practice is to translate English
synthesized data into other languages using machine translation (MT). Besides
the known content errors in these English synthesized data, multilingual
synthesized instruction data are further exposed to defects introduced by MT
and face insufficient localization of the target languages. In this paper, we
propose MIDB, a Multilingual Instruction Data Booster to automatically address
the quality issues in multilingual synthesized data. MIDB is trained on around
36.8k revision examples across 16 languages by human linguistic experts,
thereby can boost the low-quality data by addressing content errors and MT
defects, and improving localization in these synthesized data. Both automatic
and human evaluation indicate that not only MIDB steadily improved instruction
data quality in 16 languages, but also the instruction-following and
cultural-understanding abilities of multilingual LLMs fine-tuned on
MIDB-boosted data were significantly enhanced.

</details>


### [136] [Tuning Language Models for Robust Prediction of Diverse User Behaviors](https://arxiv.org/abs/2505.17682)
*Fanjin Meng,Jingtao Ding,Jiahui Gong,Chen Yang,Hong Chen,Zuojian Wang,Haisheng Lu,Yong Li*

Key words: 长尾行为,大型语言模型,渐进式微调,少样本学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了BehaviorLM，一种渐进式微调方法，解决深度学习模型在预测长尾行为时的过拟合问题，通过两阶段微调平衡常见和稀有行为预测。

Motivation: 现有的微调方法容易过拟合常见行为，导致对长尾行为预测能力不足。

Method: 两阶段渐进式微调：先微调常见行为，再基于样本难度平衡所有行为。

Result: 在真实数据集上，BehaviorLM能同时预测常见和稀有行为，并有效利用LLM知识实现少样本学习。

Conclusion: BehaviorLM通过渐进微调解决了长尾行为预测问题，兼顾了常见和稀有行为的表现。

Abstract: Predicting user behavior is essential for intelligent assistant services, yet
deep learning models often struggle to capture long-tailed behaviors. Large
language models (LLMs), with their pretraining on vast corpora containing rich
behavioral knowledge, offer promise. However, existing fine-tuning approaches
tend to overfit to frequent ``anchor'' behaviors, reducing their ability to
predict less common ``tail'' behaviors. In this paper, we introduce BehaviorLM,
a progressive fine-tuning approach that addresses this issue. In the first
stage, LLMs are fine-tuned on anchor behaviors while preserving general
behavioral knowledge. In the second stage, fine-tuning uses a balanced subset
of all behaviors based on sample difficulty to improve tail behavior
predictions without sacrificing anchor performance. Experimental results on two
real-world datasets demonstrate that BehaviorLM robustly predicts both anchor
and tail behaviors and effectively leverages LLM behavioral knowledge to master
tail behavior prediction with few-shot examples.

</details>


### [137] [ELSPR: Evaluator LLM Training Data Self-Purification on Non-Transitive Preferences via Tournament Graph Reconstruction](https://arxiv.org/abs/2505.17691)
*Yan Yu,Yilun Liu,Minggui He,Shimin Tao,Weibin Meng,Xinhua Yang,Li Zhang,Hongxia Ma,Chang Su,Hao Yang,Fuliang Li*

Key words: 大语言模型, 非传递性, 图论框架, ELSPR, 偏好过滤

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种图论框架来分析和缓解LLM评价中的非传递性问题，通过过滤不一致偏好数据提升模型性能。

Motivation: 解决大语言模型在评估开放任务时出现的偏好非传递性问题，提升评价的准确性和一致性。

Method: 设计图论框架，量化非传递性并测量偏好清晰度，提出过滤策略ELSPR保留一致偏好数据用于微调。

Result: 过滤后的数据减少了13.78%非传递性，降低了结构熵，提高了与人类评价的一致性。

Conclusion: ELSPR策略有效缓解非传递性，提升评价质量，使模型更接近人类评估标准。

Abstract: Large language models (LLMs) are widely used as evaluators for open-ended
tasks, while previous research has emphasized biases in LLM evaluations, the
issue of non-transitivity in pairwise comparisons remains unresolved:
non-transitive preferences for pairwise comparisons, where evaluators prefer A
over B, B over C, but C over A. Our results suggest that low-quality training
data may reduce the transitivity of preferences generated by the Evaluator LLM.
To address this, We propose a graph-theoretic framework to analyze and mitigate
this problem by modeling pairwise preferences as tournament graphs. We quantify
non-transitivity and introduce directed graph structural entropy to measure the
overall clarity of preferences. Our analysis reveals significant
non-transitivity in advanced Evaluator LLMs (with Qwen2.5-Max exhibiting
67.96%), as well as high entropy values (0.8095 for Qwen2.5-Max), reflecting
low overall clarity of preferences. To address this issue, we designed a
filtering strategy, ELSPR, to eliminate preference data that induces
non-transitivity, retaining only consistent and transitive preference data for
model fine-tuning. Experiments demonstrate that models fine-tuned with filtered
data reduce non-transitivity by 13.78% (from 64.28% to 50.50%), decrease
structural entropy by 0.0879 (from 0.8113 to 0.7234), and align more closely
with human evaluators (human agreement rate improves by 0.6% and Spearman
correlation increases by 0.01).

</details>


### [138] [Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models](https://arxiv.org/abs/2505.17697)
*Zekai Zhao,Qi Liu,Kun Zhou,Zihan Liu,Yifei Shao,Zhiting Hu,Biwei Huang*

Key words: 长链推理，激活控制，无需训练，参数高效微调，大语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 通过分析和放大LLMs中高影响力的激活值，提出无需训练的激活控制技术，显著提升长链推理能力和准确性。同时，提出一种参数高效的微调方法，性能优于全参数微调。

Motivation: 探索如何无需昂贵训练即可激发LLMs的长链推理能力，并理解其内部机制。

Method: 分析关键激活值，通过放大和插入“等待”令牌来调控推理过程，并设计训练无关的激活控制技术。

Result: 显著提升自反率和准确性，验证了方法的有效性，且参数高效微调性能更优。

Conclusion: 激活控制技术和参数高效微调可高效激发LLMs的长链推理能力。

Abstract: Despite the remarkable reasoning performance, eliciting the long
chain-of-thought (CoT) ability in large language models (LLMs) typically
requires costly reinforcement learning or supervised fine-tuning on
high-quality distilled data. We investigate the internal mechanisms behind this
capability and show that a small set of high-impact activations in the last few
layers largely governs long-form reasoning attributes, such as output length
and self-reflection. By simply amplifying these activations and inserting
"wait" tokens, we can invoke the long CoT ability without any training,
resulting in significantly increased self-reflection rates and accuracy.
Moreover, we find that the activation dynamics follow predictable trajectories,
with a sharp rise after special tokens and a subsequent exponential decay.
Building on these insights, we introduce a general training-free activation
control technique. It leverages a few contrastive examples to identify key
activations, and employs simple analytic functions to modulate their values at
inference time to elicit long CoTs. Extensive experiments confirm the
effectiveness of our method in efficiently eliciting long CoT reasoning in LLMs
and improving their performance. Additionally, we propose a parameter-efficient
fine-tuning method that trains only a last-layer activation amplification
module and a few LoRA layers, outperforming full LoRA fine-tuning on reasoning
benchmarks with significantly fewer parameters. Our code and data are publicly
released.

</details>


### [139] [SemSketches-2021: experimenting with the machine processing of the pilot semantic sketches corpus](https://arxiv.org/abs/2505.17704)
*Maria Ponomareva,Maria Petrova,Julia Detkova,Oleg Serikov,Maria Yarova*

Key words: 语义草图, 机器处理, 语料库, 共享任务, 自然语言处理

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文探讨了语义草图的机器处理方法，介绍了首个开放的语义草图语料库，并讨论了草图的创建与应用任务，尤其是为语料库开发的机器处理工具。

Motivation: 研究旨在通过语义草图及其机器处理工具，提升自然语言处理中对语义理解和上下文匹配的能力。

Method: 构建了一个开放的语义草图语料库，并组织了SemSketches-2021共享任务，要求参与者为匿名草图匹配合适的上下文。

Result: 共享任务的实施展示了语义草图在实际应用中的潜力，并为相关工具的开发提供了实践经验。

Conclusion: 语义草图及其处理工具在解决自然语言处理任务中具有重要价值，未来可进一步扩展语料库和优化工具。

Abstract: The paper deals with elaborating different approaches to the machine
processing of semantic sketches. It presents the pilot open corpus of semantic
sketches. Different aspects of creating the sketches are discussed, as well as
the tasks that the sketches can help to solve. Special attention is paid to the
creation of the machine processing tools for the corpus. For this purpose, the
SemSketches-2021 Shared Task was organized. The participants were given the
anonymous sketches and a set of contexts containing the necessary predicates.
During the Task, one had to assign the proper contexts to the corresponding
sketches.

</details>


### [140] [Understanding How Value Neurons Shape the Generation of Specified Values in LLMs](https://arxiv.org/abs/2505.17712)
*Yi Su,Jiayi Zhang,Shu Yang,Xinhai Wang,Lijie Hu,Di Wang*

Key words: 大型语言模型, 价值对齐, 机械可解释性, Schwartz价值观, 神经元定位

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为ValueLocate的机械可解释性框架，结合Schwartz价值观调查，用于定位和理解大型语言模型中的价值表示。通过构建ValueInsight数据集和神经元的激活差异分析，实现了对价值相关神经元的精准定位和验证。

Motivation: 当前大型语言模型（LLMs）的内在价值表示不透明，尽管行为对齐取得进展，但缺乏系统方法来解释价值编码。现有的数据集和方法局限于表面判断，未能深入分析价值编码的机制。

Method: 提出了ValueLocate框架，构建了ValueInsight数据集，基于Schwartz价值观的四维度操作化价值表示。开发了一种通过激活差异定位价值关键神经元的方法，避免了计算密集的归因方法。

Result: 验证方法表明，通过定向操纵这些神经元可以有效改变模型的价值取向，建立了神经元与价值表征之间的因果关系。

Conclusion: 该研究通过将心理学价值框架与LLMs的神经元分析相结合，为价值对齐奠定了基础。

Abstract: Rapid integration of large language models (LLMs) into societal applications
has intensified concerns about their alignment with universal ethical
principles, as their internal value representations remain opaque despite
behavioral alignment advancements. Current approaches struggle to
systematically interpret how values are encoded in neural architectures,
limited by datasets that prioritize superficial judgments over mechanistic
analysis. We introduce ValueLocate, a mechanistic interpretability framework
grounded in the Schwartz Values Survey, to address this gap. Our method first
constructs ValueInsight, a dataset that operationalizes four dimensions of
universal value through behavioral contexts in the real world. Leveraging this
dataset, we develop a neuron identification method that calculates activation
differences between opposing value aspects, enabling precise localization of
value-critical neurons without relying on computationally intensive attribution
methods. Our proposed validation method demonstrates that targeted manipulation
of these neurons effectively alters model value orientations, establishing
causal relationships between neurons and value representations. This work
advances the foundation for value alignment by bridging psychological value
frameworks with neuron analysis in LLMs.

</details>


### [141] [The Pilot Corpus of the English Semantic Sketches](https://arxiv.org/abs/2505.17733)
*Maria Petrova,Maria Ponomareva,Alexandra Ivoylova*

Key words: 语义草图、英语动词、对比研究、跨语言差异、语料库

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文探讨了如何为英语动词创建语义草图，并对比了英俄语义草图的差异，重点分析了相似语义草图间的跨语言差异及其构建过程。

Motivation: 通过构建英俄语义草图对，探索对比研究的可能性，并深入了解语义草图的语言学特性及其构建过程中的常见错误。

Method: 构建了一个初步的英语-俄语语义草图对语料库，分析跨语言差异及草图构建过程中的问题。

Result: 展示了语义草图在对比研究中的应用，揭示了相似语义下的跨语言差异及草图构建的潜在错误。

Conclusion: 语义草图为跨语言对比研究提供了有效工具，其构建过程中的错误可进一步揭示语言学特性。

Abstract: The paper is devoted to the creation of the semantic sketches for English
verbs. The pilot corpus consists of the English-Russian sketch pairs and is
aimed to show what kind of contrastive studies the sketches help to conduct.
Special attention is paid to the cross-language differences between the
sketches with similar semantics. Moreover, we discuss the process of building a
semantic sketch, and analyse the mistakes that could give insight to the
linguistic nature of sketches.

</details>


### [142] [Fast Quiet-STaR: Thinking Without Thought Tokens](https://arxiv.org/abs/2505.17746)
*Wei Huang,Yizhe Xiong,Xin Ye,Zhijie Deng,Hui Chen,Zijia Lin,Guiguang Ding*

Key words: Large Language Models, reasoning, curriculum learning, reinforcement learning, Next Token Prediction

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Fast Quiet STaR是一种更高效的大模型推理框架，通过减少思考令牌数量来降低计算成本，并在保持推理延迟的同时提升准确性。

Motivation: 尽管大型语言模型（LLMs）在许多任务上表现出色，但复杂推理任务需要更高效的方法。Quiet STaR虽有效但计算开销大，因此提出更高效的Fast Quiet STaR。

Method: 提出基于课程学习的训练策略，逐步减少思考令牌数量，结合强化学习微调以适应标准的下一个令牌预测（NTP）设置。

Result: 在Mistral 7B和Qwen2.5 7B上，Fast Quiet-STaR NTP平均准确率分别提升9%和5.7%，且推理延迟不变。

Conclusion: Fast Quiet-STaR显著提升了推理效率与准确性，适用于实际应用。

Abstract: Large Language Models (LLMs) have achieved impressive performance across a
range of natural language processing tasks. However, recent advances
demonstrate that further gains particularly in complex reasoning tasks require
more than merely scaling up model sizes or training data. One promising
direction is to enable models to think during the reasoning process. Recently,
Quiet STaR significantly improves reasoning by generating token-level thought
traces, but incurs substantial inference overhead. In this work, we propose
Fast Quiet STaR, a more efficient reasoning framework that preserves the
benefits of token-level reasoning while reducing computational cost. Our method
introduces a curriculum learning based training strategy that gradually reduces
the number of thought tokens, enabling the model to internalize more abstract
and concise reasoning processes. We further extend this approach to the
standard Next Token Prediction (NTP) setting through reinforcement
learning-based fine-tuning, resulting in Fast Quiet-STaR NTP, which eliminates
the need for explicit thought token generation during inference. Experiments on
four benchmark datasets with Mistral 7B and Qwen2.5 7B demonstrate that Fast
Quiet-STaR consistently outperforms Quiet-STaR in terms of average accuracy
under the same inference time budget. Notably, Fast Quiet-STaR NTP achieves an
average accuracy improvement of 9\% on Mistral 7B and 5.7\% on Qwen2.5 7B,
while maintaining the same inference latency. Our code will be available at
https://github.com/huangwei200012/Fast-Quiet-STaR.

</details>


### [143] [Discriminating Form and Meaning in Multilingual Models with Minimal-Pair ABX Tasks](https://arxiv.org/abs/2505.17747)
*Maureen de Seyssel,Jie Chi,Skyler Seto,Maartje ter Hoeve,Masha Fedzechkina,Natalie Schluter*

Key words: ABX任务, 多语言模型, 零样本评估, 语言身份, 语义内容

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种无需训练的ABX式判别任务，用于评估多语言模型中语言身份（形式）与语义内容（意义）的表征，发现语言判别能力随训练下降而集中于低层，语义判别能力则增强并稳定于深层。

Motivation: 旨在通过零样本任务灵活且可解释地评估多语言模型的表征结构，替代传统的探测方法。

Method: 采用ABX式判别任务，分析XLM-R模型在不同训练阶段和层的表现。

Result: 语言判别能力随训练下降并集中于低层，语义判别能力增强并稳定于深层；探测任务与语言学性能有一定相关性。

Conclusion: ABX任务为多语言表征分析提供了轻量级框架。

Abstract: We introduce a set of training-free ABX-style discrimination tasks to
evaluate how multilingual language models represent language identity (form)
and semantic content (meaning). Inspired from speech processing, these
zero-shot tasks measure whether minimal differences in representation can be
reliably detected. This offers a flexible and interpretable alternative to
probing. Applied to XLM-R (Conneau et al, 2020) across pretraining checkpoints
and layers, we find that language discrimination declines over training and
becomes concentrated in lower layers, while meaning discrimination strengthens
over time and stabilizes in deeper layers. We then explore probing tasks,
showing some alignment between our metrics and linguistic learning performance.
Our results position ABX tasks as a lightweight framework for analyzing the
structure of multilingual representations.

</details>


### [144] [Resolving Conflicting Evidence in Automated Fact-Checking: A Study on Retrieval-Augmented LLMs](https://arxiv.org/abs/2505.17762)
*Ziyu Ge,Yuhao Wu,Daniel Wai Kit Chin,Roy Ka-Wei Lee,Rui Cao*

Key words: 检索增强生成（RAG）、事实核查、冲突证据、可信度、CONFACT数据集

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇论文首次系统评估了检索增强生成（RAG）模型在处理冲突证据时的性能，提出了新数据集CONFACT，并展示了整合媒体来源可信度信息如何显著提升模型在事实核查中的表现。

Motivation: 研究RAG模型在事实核查任务中面对冲突证据（尤其是来自不同可信度来源的冲突）时的可靠性下降问题。

Method: 引入CONFACT数据集（包含带冲突证据的问题），评估现有RAG方法的漏洞，并提出在检索和生成阶段整合媒体背景信息的策略。

Result: 实验表明，有效结合来源可信度能显著提升RAG模型解决冲突证据的能力，改善事实核查性能。

Conclusion: 整合来源可信度是提升RAG模型在冲突证据场景下可靠性的关键方向。

Abstract: Large Language Models (LLMs) augmented with retrieval mechanisms have
demonstrated significant potential in fact-checking tasks by integrating
external knowledge. However, their reliability decreases when confronted with
conflicting evidence from sources of varying credibility. This paper presents
the first systematic evaluation of Retrieval-Augmented Generation (RAG) models
for fact-checking in the presence of conflicting evidence. To support this
study, we introduce \textbf{CONFACT} (\textbf{Con}flicting Evidence for
\textbf{Fact}-Checking) (Dataset available at
https://github.com/zoeyyes/CONFACT), a novel dataset comprising questions
paired with conflicting information from various sources. Extensive experiments
reveal critical vulnerabilities in state-of-the-art RAG methods, particularly
in resolving conflicts stemming from differences in media source credibility.
To address these challenges, we investigate strategies to integrate media
background information into both the retrieval and generation stages. Our
results show that effectively incorporating source credibility significantly
enhances the ability of RAG models to resolve conflicting evidence and improve
fact-checking performance.

</details>


### [145] [The Real Barrier to LLM Agent Usability is Agentic ROI](https://arxiv.org/abs/2505.17767)
*Weiwen Liu,Jiarui Qin,Xu Huang,Xingshan Zeng,Yunjia Xi,Jianghao Lin,Chuhan Wu,Yasheng Wang,Lifeng Shang,Ruiming Tang,Defu Lian,Yong Yu,Weinan Zhang*

Key words: 大型语言模型代理, Agent ROI, 实用化, 成本效益, 信息质量

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文探讨了大型语言模型（LLM）代理的现实应用问题，提出应从单纯的模型性能优化转向‘代理投资回报率（Agent ROI）’的实用视角，通过信息质量、代理时间和成本三大关键因素实现发展路径，推动LLM代理的高效普及。

Motivation: 当前LLM代理在编码和科研等专业领域广泛应用，但在大众市场需求中落地不足，作者认为这不仅是模型能力的限制，还与‘价值-成本’的权衡相关。

Method: 提出以‘Agent ROI’为核心框架，分阶段优化信息质量（先上规模）和降低时间与成本（再压缩），并规划了具体发展路线图。

Result: 理论分析表明，通过调整规模与成本间的动态平衡，可逐步弥合实用性差距

Conclusion: 呼吁学术界和产业界共同关注Agent ROI，推动LLM代理成为真正可扩展且实用的技术。

Abstract: Large Language Model (LLM) agents represent a promising shift in human-AI
interaction, moving beyond passive prompt-response systems to autonomous agents
capable of reasoning, planning, and goal-directed action. Despite the
widespread application in specialized, high-effort tasks like coding and
scientific research, we highlight a critical usability gap in high-demand,
mass-market applications. This position paper argues that the limited
real-world adoption of LLM agents stems not only from gaps in model
capabilities, but also from a fundamental tradeoff between the value an agent
can provide and the costs incurred during real-world use. Hence, we call for a
shift from solely optimizing model performance to a broader, utility-driven
perspective: evaluating agents through the lens of the overall agentic return
on investment (Agent ROI). By identifying key factors that determine Agentic
ROI--information quality, agent time, and cost--we posit a zigzag development
trajectory in optimizing agentic ROI: first scaling up to improve the
information quality, then scaling down to minimize the time and cost. We
outline the roadmap across different development stages to bridge the current
usability gaps, aiming to make LLM agents truly scalable, accessible, and
effective in real-world contexts.

</details>


### [146] [EXECUTE: A Multilingual Benchmark for LLM Token Understanding](https://arxiv.org/abs/2505.17784)
*Lukas Edman,Helmut Schmid,Alexander Fraser*

Key words: LLMs, 多语言, 字符理解, CUTE, EXECUTE, 子字符任务

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: EXECUTE扩展了CUTE基准，测试多语言中LLMs对字符的理解，发现不同语言的问题层次（字符、单词或无问题）不同，并考察了中日韩语言的子字符任务。

Motivation: 现有CUTE基准仅针对英语，无法全面评估LLMs在不同语言中的字符理解能力。通过扩展至多语言（尤其不同书写系统），揭示LLMs的跨语言表现差异。

Method: 简化框架可支持任意语言，测试多种LLMs在字符/单词层级的理解能力，并针对中文、日文、韩文设计子字符任务。

Result: 不同语言的问题层次各异（如英语为字符级，其他语言可能为单词级或无问题），中日韩子字符任务显示LLMs对字符组件的理解有限。

Conclusion: LLMs的字符理解能力因语言而异，需针对不同语言设计更细粒度的评估任务。

Abstract: The CUTE benchmark showed that LLMs struggle with character understanding in
English. We extend it to more languages with diverse scripts and writing
systems, introducing EXECUTE. Our simplified framework allows easy expansion to
any language. Tests across multiple LLMs reveal that challenges in other
languages are not always on the character level as in English. Some languages
show word-level processing issues, some show no issues at all. We also examine
sub-character tasks in Chinese, Japanese, and Korean to assess LLMs'
understanding of character components.

</details>


### [147] [Compression Hacking: A Supplementary Perspective on Informatics Metric of Language Models from Geometric Distortion](https://arxiv.org/abs/2505.17793)
*Jianxiang Zang,Meiling Ning,Yongda Wei,Shihan Dou,Jiazheng Zhang,Nijia Mo,Binhong Li,Tao Gui,Qi Zhang,Xuanjing Huang*

Key words: 语言模型、压缩即智能、各向异性、几何失真、压缩黑客、自评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种通过几何失真分析改进的语言模型压缩指标，解决了高压缩率下表征空间退化的问题，显著提升了与模型综合能力的相关性。

Motivation: 现有‘压缩即智能’的指标虽能反映语言模型的结构化表征，但高压缩率会导致表征空间退化（各向异性），从而损害模型理解和性能。研究发现这种压缩与退化的同步现象本质上是‘压缩黑客’，即噪声方向通过牺牲空间均匀性制造高压缩率的假象。

Method: 提出了三种结合几何失真分析的改进压缩指标，并将其整合到自评估流程中。

Result: 改进后的指标与语言模型综合能力高度相关（Spearman系数>0.9），显著优于原压缩指标及其他基于内部结构的指标。

Conclusion: 通过引入几何失真分析，‘压缩黑客’现象强化了语言模型的信息学解释力。

Abstract: Recently, the concept of ``compression as intelligence'' has provided a novel
informatics metric perspective for language models (LMs), emphasizing that
highly structured representations signify the intelligence level of LMs.
However, from a geometric standpoint, the word representation space of highly
compressed LMs tends to degenerate into a highly anisotropic state, which
hinders the LM's ability to comprehend instructions and directly impacts its
performance. We found this compression-anisotropy synchronicity is essentially
the ``Compression Hacking'' in LM representations, where noise-dominated
directions tend to create the illusion of high compression rates by sacrificing
spatial uniformity. Based on this, we propose three refined compression metrics
by incorporating geometric distortion analysis and integrate them into a
self-evaluation pipeline. The refined metrics exhibit strong alignment with the
LM's comprehensive capabilities, achieving Spearman correlation coefficients
above 0.9, significantly outperforming both the original compression and other
internal structure-based metrics. This confirms that compression hacking
substantially enhances the informatics interpretation of LMs by incorporating
geometric distortion of representations.

</details>


### [148] [DialogXpert: Driving Intelligent and Emotion-Aware Conversations through Online Value-Based Reinforcement Learning with LLM Priors](https://arxiv.org/abs/2505.17795)
*Tazeek Bin Abdur Rakib,Ambuj Mehrish,Lay-Ki Soon,Wern Han Lim,Soujanya Poria*

Key words: LLM, 对话规划, 情感智能, Q网络, 冻结模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: DialogXpert利用冻结的大语言模型生成高质量候选动作，并通过Q网络选择最优动作，实现了高效、情感智能的对话规划。

Motivation: 解决大语言模型代理在目标驱动互动中的短视解码和高成本规划问题。

Method: 结合冻结LLM生成候选动作，使用Q网络在BERT嵌入上选择最优动作，考虑用户情感。

Result: 在多个任务中，对话成功率超过94%，优化后可达97%，显著提升谈判效果。

Conclusion: DialogXpert实现了实时、战略性和情感智能的对话规划。

Abstract: Large-language-model (LLM) agents excel at reactive dialogue but struggle
with proactive, goal-driven interactions due to myopic decoding and costly
planning. We introduce DialogXpert, which leverages a frozen LLM to propose a
small, high-quality set of candidate actions per turn and employs a compact
Q-network over fixed BERT embeddings trained via temporal-difference learning
to select optimal moves within this reduced space. By tracking the user's
emotions, DialogXpert tailors each decision to advance the task while nurturing
a genuine, empathetic connection. Across negotiation, emotional support, and
tutoring benchmarks, DialogXpert drives conversations to under $3$ turns with
success rates exceeding 94\% and, with a larger LLM prior, pushes success above
97\% while markedly improving negotiation outcomes. This framework delivers
real-time, strategic, and emotionally intelligent dialogue planning at scale.
Code available at https://github.com/declare-lab/dialogxpert/

</details>


### [149] [Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM Reasoning](https://arxiv.org/abs/2505.17813)
*Michael Hassid,Gabriel Synnaeve,Yossi Adi,Roy Schwartz*

Key words: 大型语言模型, 推理链, 并行计算, 多数投票, 计算效率

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为short-m@k的新推理方法，通过并行生成短推理链并多数投票选择答案，显著减少了计算成本和推理时间，同时保持了或提升了性能。

Motivation: 当前大型语言模型（LLMs）依赖生成长推理链以完成复杂推理任务，但这种方法带来了高昂的计算成本和推理时间。论文质疑长推理链是否一定带来更好的性能，并探索更高效的推理方法。

Method: 提出short-m@k方法，并行生成k个独立推理链，在完成前m个后停止计算，通过多数投票选择最终答案。还通过微调LLM验证短推理链的效果。

Result: 短推理链比长链准确率更高（最高提升34.5%），short-1@k在低计算设置下与标准多数投票性能相当或更优，节省40%计算资源；short-3@k在所有计算预算下均优于多数投票，并减少33%推理时间。微调实验表明，使用短推理链训练能提升模型性能。

Conclusion: 研究表明，更长的推理链不一定会提升性能，反而可能降低效果。short-m@k提供了一种高效替代方案，重新思考了LLM推理时的计算分配策略。

Abstract: Reasoning large language models (LLMs) heavily rely on scaling test-time
compute to perform complex reasoning tasks by generating extensive "thinking"
chains. While demonstrating impressive results, this approach incurs
significant computational costs and inference time. In this work, we challenge
the assumption that long thinking chains results in better reasoning
capabilities. We first demonstrate that shorter reasoning chains within
individual questions are significantly more likely to yield correct answers -
up to 34.5% more accurate than the longest chain sampled for the same question.
Based on these results, we suggest short-m@k, a novel reasoning LLM inference
method. Our method executes k independent generations in parallel and halts
computation once the first m thinking processes are done. The final answer is
chosen using majority voting among these m chains. Basic short-1@k demonstrates
similar or even superior performance over standard majority voting in
low-compute settings - using up to 40% fewer thinking tokens. short-3@k, while
slightly less efficient than short-1@k, consistently surpasses majority voting
across all compute budgets, while still being substantially faster (up to 33%
wall time reduction). Inspired by our results, we finetune an LLM using short,
long, and randomly selected reasoning chains. We then observe that training on
the shorter ones leads to better performance. Our findings suggest rethinking
current methods of test-time compute in reasoning LLMs, emphasizing that longer
"thinking" does not necessarily translate to improved performance and can,
counter-intuitively, lead to degraded results.

</details>


### [150] [Low-Resource NMT: A Case Study on the Written and Spoken Languages in Hong Kong](https://arxiv.org/abs/2505.17816)
*Hei Yi Mak,Tan Lee*

Key words: 神經機器翻譯, 中文到粵語翻譯, Transformer, 平行數據, 維基百科

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文介紹了一種基於Transformer的神經機器翻譯（NMT）系統，用於書面中文到書面粵語的翻譯。由於中文和粵語的平行文本數據極度稀缺，研究重點是準備大量訓練數據的方法。通過從語言學研究和網絡資源收集28K平行句，並從中文和粵語維基百科提取72K語義相似句對，提升了翻譯性能。系統在BLEU分數上優於百度翻譯的6/8測試集，並能捕捉中文和粵語間的語言轉換。

Motivation: 香港居民主要使用粵語口語，但書面中文與書面粵語在詞彙和語法上差異顯著。隨著粵語書寫在網絡中的普及，中文與粵語間的自動翻譯需求增長。

Method: 採用基於Transformer的NMT系統，解決平行數據稀缺問題。通過從語言學研究和網絡收集28K平行句，並從中粵維基百科自動提取72K語義相似句對作為訓練數據。

Result: 系統在BLEU分數上優於百度翻譯的6/8測試集，能有效捕捉中文與粵語的語言轉換。維基百科的相似句對提升了所有測試集的翻譯性能。

Conclusion: 通過創新數據收集方法，該NMT系統在中文到粵語翻譯任務中表現優異，滿足了實際需求。

Abstract: The majority of inhabitants in Hong Kong are able to read and write in
standard Chinese but use Cantonese as the primary spoken language in daily
life. Spoken Cantonese can be transcribed into Chinese characters, which
constitute the so-called written Cantonese. Written Cantonese exhibits
significant lexical and grammatical differences from standard written Chinese.
The rise of written Cantonese is increasingly evident in the cyber world. The
growing interaction between Mandarin speakers and Cantonese speakers is leading
to a clear demand for automatic translation between Chinese and Cantonese. This
paper describes a transformer-based neural machine translation (NMT) system for
written-Chinese-to-written-Cantonese translation. Given that parallel text data
of Chinese and Cantonese are extremely scarce, a major focus of this study is
on the effort of preparing good amount of training data for NMT. In addition to
collecting 28K parallel sentences from previous linguistic studies and
scattered internet resources, we devise an effective approach to obtaining 72K
parallel sentences by automatically extracting pairs of semantically similar
sentences from parallel articles on Chinese Wikipedia and Cantonese Wikipedia.
We show that leveraging highly similar sentence pairs mined from Wikipedia
improves translation performance in all test sets. Our system outperforms Baidu
Fanyi's Chinese-to-Cantonese translation on 6 out of 8 test sets in BLEU
scores. Translation examples reveal that our system is able to capture
important linguistic transformations between standard Chinese and spoken
Cantonese.

</details>


### [151] [Not All Tokens Are What You Need In Thinking](https://arxiv.org/abs/2505.17827)
*Hang Yuan,Bin Yu,Haotian Li,Shijun Yang,Christina Dan Wang,Zhou Yu,Xueyin Xu,Weizhen Qi,Kai Chen*

Key words: CTS, 令牌压缩, 推理效率, CoT, 条件重要性评分

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: CTS 是一个令牌级压缩框架，用于减少推理模型中的冗余令牌，提高效率并保持性能。

Motivation: 现代推理模型存在高延迟、高计算资源消耗和过度思考的问题，CTS 旨在解决这些低效问题。

Method: 通过条件重要性评分评估令牌贡献，训练模型使用压缩的推理链（CoT）。

Result: 在 GPQA 基准测试中，Qwen2.5-14B-Instruct 使用 CTS 后，准确率提升 9.1%，推理令牌减少 13.2%。

Conclusion: CTS 能有效压缩长推理链，减少冗余令牌，同时保持高性能。

Abstract: Modern reasoning models, such as OpenAI's o1 and DeepSeek-R1, exhibit
impressive problem-solving capabilities but suffer from critical
inefficiencies: high inference latency, excessive computational resource
consumption, and a tendency toward overthinking -- generating verbose chains of
thought (CoT) laden with redundant tokens that contribute minimally to the
final answer. To address these issues, we propose Conditional Token Selection
(CTS), a token-level compression framework with a flexible and variable
compression ratio that identifies and preserves only the most essential tokens
in CoT. CTS evaluates each token's contribution to deriving correct answers
using conditional importance scoring, then trains models on compressed CoT.
Extensive experiments demonstrate that CTS effectively compresses long CoT
while maintaining strong reasoning performance. Notably, on the GPQA benchmark,
Qwen2.5-14B-Instruct trained with CTS achieves a 9.1% accuracy improvement with
13.2% fewer reasoning tokens (13% training token reduction). Further reducing
training tokens by 42% incurs only a marginal 5% accuracy drop while yielding a
75.8% reduction in reasoning tokens, highlighting the prevalence of redundancy
in existing CoT.

</details>


### [152] [Stepwise Reasoning Checkpoint Analysis: A Test Time Scaling Method to Enhance LLMs' Reasoning](https://arxiv.org/abs/2505.17829)
*Zezhong Wang,Xingshan Zeng,Weiwen Liu,Yufei Wang,Liangyou Li,Yasheng Wang,Lifeng Shang,Xin Jiang,Qun Liu,Kam-Fai Wong*

Key words: 大规模语言模型,数学推理,检查点分析,Test-Time Scaling,路径同质化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种名为SRCA的框架，通过引入检查点和两种关键策略（答案聚类搜索和检查点候选增强）来减少路径同质化并提升推理准确性。

Motivation: 现有的大语言模型（LLMs）在数学推理中的Test-Time Scaling（TTS）方法（如Beam Search和DVTS）虽然能通过分配更多计算资源提升推理准确性，但存在路径同质化和中间结果利用不足的问题。

Method: 提出Stepwise Reasoning Checkpoint Analysis（SRCA），在推理步骤间引入检查点，结合两种策略：（1）答案聚类搜索（Answer-Clustered Search）通过按中间答案分组路径来保持多样性；（2）检查点候选增强（Checkpoint Candidate Augmentation）利用所有中间答案进行最终决策。

Result: 实验结果表明，SRCA在多个数学数据集上相比现有TTS方法显著提升了推理准确性。

Conclusion: SRCA通过有效利用中间结果和减少路径同质化，为LLMs的数学推理提供了更高效和容错的解决方案。

Abstract: Mathematical reasoning through Chain-of-Thought (CoT) has emerged as a
powerful capability of Large Language Models (LLMs), which can be further
enhanced through Test-Time Scaling (TTS) methods like Beam Search and DVTS.
However, these methods, despite improving accuracy by allocating more
computational resources during inference, often suffer from path homogenization
and inefficient use of intermediate results. To address these limitations, we
propose Stepwise Reasoning Checkpoint Analysis (SRCA), a framework that
introduces checkpoints between reasoning steps. It incorporates two key
strategies: (1) Answer-Clustered Search, which groups reasoning paths by their
intermediate checkpoint answers to maintain diversity while ensuring quality,
and (2) Checkpoint Candidate Augmentation, which leverages all intermediate
answers for final decision-making. Our approach effectively reduces path
homogenization and creates a fault-tolerant mechanism by utilizing high-quality
intermediate results. Experimental results show that SRCA improves reasoning
accuracy compared to existing TTS methods across various mathematical datasets.

</details>


### [153] [Emerging categories in scientific explanations](https://arxiv.org/abs/2505.17832)
*Giacomo Magnifico,Eduard Barbu*

Key words: 数据集, 解释性, 机器学习, 人工智能, 生物技术, 生物物理学

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文旨在提供一个关于人类生成解释的大规模数据集，用于机器学习和AI领域的研究。

Motivation: 由于缺乏关注人类生成解释的大规模数据集，研究旨在填补这一空白。

Method: 通过从PubMed的PMC开放获取子集中提取解释性句子，并进行多类别标注和评估标注者一致性。

Result: 构建了一个开放数据集，包含6类和3类标注，3类标注的Krippendorf Alpha值为0.667。

Conclusion: 该数据集为机器学习和AI领域的人类理解解释研究提供了有价值的资源。

Abstract: Clear and effective explanations are essential for human understanding and
knowledge dissemination. The scope of scientific research aiming to understand
the essence of explanations has recently expanded from the social sciences to
machine learning and artificial intelligence. Explanations for machine learning
decisions must be impactful and human-like, and there is a lack of large-scale
datasets focusing on human-like and human-generated explanations. This work
aims to provide such a dataset by: extracting sentences that indicate
explanations from scientific literature among various sources in the
biotechnology and biophysics topic domains (e.g. PubMed's PMC Open Access
subset); providing a multi-class notation derived inductively from the data;
evaluating annotator consensus on the emerging categories. The sentences are
organized in an openly-available dataset, with two different classifications
(6-class and 3-class category annotation), and the 3-class notation achieves a
0.667 Krippendorf Alpha value.

</details>


### [154] [Investigating Affect Mining Techniques for Annotation Sample Selection in the Creation of Finnish Affective Speech Corpus](https://arxiv.org/abs/2505.17833)
*Kalle Lahtinen,Einari Vaaras,Liisa Mustanoja,Okko Räsänen*

Key words: 情感语音、芬兰语、语料库构建、情感挖掘、多样性采样

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究人员创建了第一个芬兰语自然情感语音语料库，基于三种大规模芬兰语语音数据库，标记了12,000条话语的情感动态和效价，并采用了情感挖掘的采样方法以提高多样性。

Motivation: 芬兰语目前缺乏自然表达的语音情感数据集，现有数据多为表演或特定场景下的样本，因此需要构建一个更具代表性的数据集以支持情感语音研究。

Method: 从三种大规模芬兰语语料库中采样12,000条话语，结合声学特征、跨语言情感识别及文本情感分析进行情感挖掘采样，并与随机采样对比多样性。

Result: 成功构建了首个芬兰语自然情感语音语料库，并发现情感挖掘方法能显著提升样本的情感多样性。

Conclusion: 该研究不仅填补了芬兰语情感语音数据的空白，也为其他语言或领域的情感语料库构建提供了采样策略参考。

Abstract: Study of affect in speech requires suitable data, as emotional expression and
perception vary across languages. Until now, no corpus has existed for natural
expression of affect in spontaneous Finnish, existing data being acted or from
a very specific communicative setting. This paper presents the first such
corpus, created by annotating 12,000 utterances for emotional arousal and
valence, sampled from three large-scale Finnish speech corpora. To ensure
diverse affective expression, sample selection was conducted with an affect
mining approach combining acoustic, cross-linguistic speech emotion, and text
sentiment features. We compare this method to random sampling in terms of
annotation diversity, and conduct post-hoc analyses to identify sampling
choices that would have maximized the diversity. As an outcome, the work
introduces a spontaneous Finnish affective speech corpus and informs sampling
strategies for affective speech corpus creation in other languages or domains.

</details>


### [155] [Explaining Sources of Uncertainty in Automated Fact-Checking](https://arxiv.org/abs/2505.17855)
*Jingyi Sun,Greta Warren,Irina Shklovski,Isabelle Augenstein*

Key words: 模型不确定性、自然语言解释、证据冲突、事实核查、无监督学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: CLUE框架通过识别文本间冲突与共识，生成自然语言解释模型不确定性，提升解释的忠实度和一致性。

Motivation: 传统方法仅使用数值不确定性或模糊表述，无法解释证据冲突导致的不确定性，难以帮助用户解决分歧或信任输出。

Method: CLUE通过无监督方式识别文本间的冲突或共识关系（如主张-证据冲突），并利用提示和注意力引导生成解释。

Result: 在三个语言模型和两个事实核查数据集上，CLUE生成的解释比基线更忠实于模型不确定性，且人类评估认为其更有帮助、逻辑更一致。

Conclusion: CLUE无需微调即可适配任何白盒语言模型，为事实核查等任务提供实用性支持，并适用于其他需复杂信息推理的场景。

Abstract: Understanding sources of a model's uncertainty regarding its predictions is
crucial for effective human-AI collaboration. Prior work proposes using
numerical uncertainty or hedges ("I'm not sure, but ..."), which do not explain
uncertainty that arises from conflicting evidence, leaving users unable to
resolve disagreements or rely on the output. We introduce CLUE
(Conflict-and-Agreement-aware Language-model Uncertainty Explanations), the
first framework to generate natural language explanations of model uncertainty
by (i) identifying relationships between spans of text that expose
claim-evidence or inter-evidence conflicts and agreements that drive the
model's predictive uncertainty in an unsupervised way, and (ii) generating
explanations via prompting and attention steering that verbalize these critical
interactions. Across three language models and two fact-checking datasets, we
show that CLUE produces explanations that are more faithful to the model's
uncertainty and more consistent with fact-checking decisions than prompting for
uncertainty explanations without span-interaction guidance. Human evaluators
judge our explanations to be more helpful, more informative, less redundant,
and more logically consistent with the input than this baseline. CLUE requires
no fine-tuning or architectural changes, making it plug-and-play for any
white-box language model. By explicitly linking uncertainty to evidence
conflicts, it offers practical support for fact-checking and generalises
readily to other tasks that require reasoning over complex information.

</details>


### [156] [Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat Falsehoods](https://arxiv.org/abs/2505.17870)
*Shaina Raza,Rizwan Qureshi,Marcelo Lotif,Aman Chadha,Deval Pandya,Christos Emmanouilidis*

Key words: 生成式AI, 虚假信息, 模型免疫化, 标注数据, 事实对齐

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种类似生物免疫的方法，通过在AI模型微调阶段注入少量标记的虚假信息作为“疫苗”，从而增强模型识别和拒绝错误信息的能力。

Motivation: 生成式AI模型容易从训练数据中学习并复制虚假信息。为了解决这一问题，作者提出借鉴生物免疫的原理，通过可控暴露于弱化的“病原体”（即标记的虚假信息）来增强模型对错误信息的抵抗力。

Method: 论文建议在模型微调阶段定期注入少量经过标注的虚假信息作为训练数据。这种方法被称为“免疫化”，旨在强化模型识别和拒绝虚假信息的能力，同时保持对真实信息的准确性。

Result: 通过案例分析，论文表明免疫化模型生成的虚假信息显著少于基准模型。

Conclusion: 模型免疫化为AI系统与事实对齐提供了一种先发制人的方法，并通过伦理保障和治理控制确保虚假数据的安全使用。

Abstract: Generative AI models often learn and reproduce false information present in
their training corpora. This position paper argues that, analogous to
biological immunization, where controlled exposure to a weakened pathogen
builds immunity, AI models should be fine tuned on small, quarantined sets of
explicitly labeled falsehoods as a "vaccine" against misinformation. These
curated false examples are periodically injected during finetuning,
strengthening the model ability to recognize and reject misleading claims while
preserving accuracy on truthful inputs. An illustrative case study shows that
immunized models generate substantially less misinformation than baselines. To
our knowledge, this is the first training framework that treats fact checked
falsehoods themselves as a supervised vaccine, rather than relying on input
perturbations or generic human feedback signals, to harden models against
future misinformation. We also outline ethical safeguards and governance
controls to ensure the safe use of false data. Model immunization offers a
proactive paradigm for aligning AI systems with factuality.

</details>


### [157] [MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback](https://arxiv.org/abs/2505.17873)
*Wanhao Liu,Zonglin Yang,Jue Wang,Lidong Bing,Di Zhang,Dongzhan Zhou,Yuqiang Li,Houqiang Li,Erik Cambria,Wanli Ouyang*

Key words: 假设排序, 实验反馈, 模拟器, 自然语言处理, 化学

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种基于实验反馈的假设排序方法，利用模拟器克服自然科学领域中实验成本高的问题，并通过实验验证了其优于传统预实验排序方法。

Motivation: 自然科学研究中湿实验成本高且效率低，现有假设排序方法仅依赖语言模型推理而忽略实验反馈，因此需要一种结合实验结果的排序策略。

Method: 提出基于三个领域假设的模拟器，将假设表现建模为与已知真实假设的相似性加噪声，并开发伪实验引导的排序方法，通过聚类和模拟反馈优化排序。

Result: 在124个化学假设数据集上的实验表明，该方法优于预实验基线和其他消融模型。

Conclusion: 实验引导的排序方法能有效提升假设排序性能，模拟器为高成本实验领域提供了实用工具。

Abstract: Hypothesis ranking is a crucial component of automated scientific discovery,
particularly in natural sciences where wet-lab experiments are costly and
throughput-limited. Existing approaches focus on pre-experiment ranking,
relying solely on large language model's internal reasoning without
incorporating empirical outcomes from experiments. We introduce the task of
experiment-guided ranking, which aims to prioritize candidate hypotheses based
on the results of previously tested ones. However, developing such strategies
is challenging due to the impracticality of repeatedly conducting real
experiments in natural science domains. To address this, we propose a simulator
grounded in three domain-informed assumptions, modeling hypothesis performance
as a function of similarity to a known ground truth hypothesis, perturbed by
noise. We curate a dataset of 124 chemistry hypotheses with experimentally
reported outcomes to validate the simulator. Building on this simulator, we
develop a pseudo experiment-guided ranking method that clusters hypotheses by
shared functional characteristics and prioritizes candidates based on insights
derived from simulated experimental feedback. Experiments show that our method
outperforms pre-experiment baselines and strong ablations.

</details>


### [158] [Mutarjim: Advancing Bidirectional Arabic-English Translation with a Small Language Model](https://arxiv.org/abs/2505.17894)
*Khalil Hennara,Muhammad Hreden,Mohamed Motaism Hamed,Zeina Aldallal,Sara Chrouf,Safwan AlModhayan*

Key words: Mutarjim, 阿拉伯语-英语翻译, Kuwain-1.5B, Tarjama-25, 小型语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Mutarjim是一个小型但强大的双向阿拉伯语-英语翻译模型，基于Kuwain-1.5B开发，通过优化训练方法和高质量数据，在多个基准测试中超越更大模型，并推出了新基准Tarjama-25。

Motivation: 现有的阿拉伯语-英语翻译模型通常规模庞大且计算成本高，小型模型在特定任务上可能有更好表现。

Method: 采用两阶段优化训练方法和精心筛选的高质量训练语料库，基于Kuwain-1.5B模型开发。

Result: Mutarjim在多个基准测试中优于比其大20倍的模型，并在新基准Tarjama-25上实现了最先进性能。

Conclusion: Mutarjim展示了小模型在特定任务上超越大模型的潜力，同时降低了计算成本，Tarjama-25为未来研究提供了更好的评估工具。

Abstract: We introduce Mutarjim, a compact yet powerful language model for
bidirectional Arabic-English translation. While large-scale LLMs have shown
impressive progress in natural language processing tasks, including machine
translation, smaller models. Leveraging this insight, we developed Mutarjim
based on Kuwain-1.5B , a language model tailored for both Arabic and English.
Despite its modest size, Mutarjim outperforms much larger models on several
established benchmarks, achieved through an optimized two-phase training
approach and a carefully curated, high-quality training corpus.. Experimental
results show that Mutarjim rivals models up to 20 times larger while
significantly reducing computational costs and training requirements. We also
introduce Tarjama-25, a new benchmark designed to overcome limitations in
existing Arabic-English benchmarking datasets, such as domain narrowness, short
sentence lengths, and English-source bias. Tarjama-25 comprises 5,000
expert-reviewed sentence pairs and spans a wide range of domains, offering a
more comprehensive and balanced evaluation framework. Notably, Mutarjim
achieves state-of-the-art performance on the English-to-Arabic task in
Tarjama-25, surpassing even significantly larger and proprietary models like
GPT-4o mini. We publicly release Tarjama-25 to support future research and
advance the evaluation of Arabic-English translation systems.

</details>


### [159] [Language models can learn implicit multi-hop reasoning, but only if they have lots of training data](https://arxiv.org/abs/2505.17923)
*Yuekun Yao,Yupei Du,Dawei Zhu,Michael Hahn,Alexander Koller*

Key words: 隐式推理, 多跳推理, 语言模型, 训练数据, 课程学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了语言模型在多跳推理任务中的隐式推理能力，发现随着推理步数增加，训练数据需求呈指数增长，而所需的Transformer层数线性增长，并提出了一种理论解释，同时指出课程学习可以缓解但无法消除数据需求。

Motivation: 探索语言模型在无链式思考（chain of thought）的情况下，完成多跳推理任务的隐式推理能力，并分析其训练数据与模型深度的关系。

Method: 使用GPT2风格的语言模型，从头开始在可控的k跳推理数据集（k=2,3,4）上进行训练，分析数据需求和模型深度随k增长的规律。

Result: 研究发现隐式k跳推理需要训练数据呈指数增长，Transformer层数呈线性增长，并提出理论支持深度增长的必要性。课程学习可部分缓解数据需求。

Conclusion: 隐式推理在多跳任务中可行，但资源需求随任务复杂度显著增加。课程学习是优化方向之一，但无法完全解决问题。

Abstract: Implicit reasoning is the ability of a language model to solve multi-hop
reasoning tasks in a single forward pass, without chain of thought. We
investigate this capability using GPT2-style language models trained from
scratch on controlled $k$-hop reasoning datasets ($k = 2, 3, 4$). We show that
while such models can indeed learn implicit $k$-hop reasoning, the required
training data grows exponentially in $k$, and the required number of
transformer layers grows linearly in $k$. We offer a theoretical explanation
for why this depth growth is necessary. We further find that the data
requirement can be mitigated, but not eliminated, through curriculum learning.

</details>


### [160] [Handling Symbolic Language in Student Texts: A Comparative Study of NLP Embedding Models](https://arxiv.org/abs/2505.17950)
*Tom Bleckmann,Paul Tschisgale*

Key words: NLP嵌入模型, 科学符号表达, 学习分析, 机器学习, 性能评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究比较了不同NLP嵌入模型处理科学符号表达（如公式）的能力，发现GPT-text-embedding-3-large表现最佳，但优势有限，并强调了成本、合规性等因素在模型选择中的重要性。

Motivation: 科学学习中的符号表达（如公式）给NLP嵌入模型带来挑战，现有研究常忽视或移除这些符号，可能导致分析偏差。本研究旨在探索不同模型处理此类表达的能力差异。

Method: 评估了多种嵌入模型在物理相关符号表达上的表现，通过相似性分析和机器学习流水线集成两种方式进行性能测试。

Result: GPT-text-embedding-3-large表现最优，但优势不显著；成本、合规性和透明度等非性能因素也需考虑。

Conclusion: 科学语言分析中，嵌入模型的选择需综合考虑性能、成本及合规性，尤其需关注符号表达的处理能力。

Abstract: Recent advancements in Natural Language Processing (NLP) have facilitated the
analysis of student-generated language products in learning analytics (LA),
particularly through the use of NLP embedding models. Yet when it comes to
science-related language, symbolic expressions such as equations and formulas
introduce challenges that current embedding models struggle to address.
Existing studies and applications often either overlook these challenges or
remove symbolic expressions altogether, potentially leading to biased findings
and diminished performance of LA applications. This study therefore explores
how contemporary embedding models differ in their capability to process and
interpret science-related symbolic expressions. To this end, various embedding
models are evaluated using physics-specific symbolic expressions drawn from
authentic student responses, with performance assessed via two approaches:
similarity-based analyses and integration into a machine learning pipeline. Our
findings reveal significant differences in model performance, with OpenAI's
GPT-text-embedding-3-large outperforming all other examined models, though its
advantage over other models was moderate rather than decisive. Beyond
performance, additional factors such as cost, regulatory compliance, and model
transparency are discussed as key considerations for model selection. Overall,
this study underscores the importance for LA researchers and practitioners of
carefully selecting NLP embedding models when working with science-related
language products that include symbolic expressions.

</details>


### [161] [Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL](https://arxiv.org/abs/2505.17952)
*Che Liu,Haozhe Wang,Jiazhen Pan,Zhongwei Wan,Yong Dai,Fangzhen Lin,Wenjia Bai,Daniel Rueckert,Rossella Arcucci*

Key words: Large Language Models, Reinforcement Learning, Medical QA, Reasoning, Chain-of-Thought

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: AlphaMed, a medical LLM, achieves state-of-the-art performance on six benchmarks through reinforcement learning with minimalist rule-based rewards, outperforming models using conventional methods and even larger closed-source models, without relying on supervised fine-tuning or chain-of-thought data.

Motivation: The paper aims to demonstrate that reasoning capabilities in LLMs for clinical applications can emerge purely through reinforcement learning, bypassing the need for costly supervised fine-tuning and distilled chain-of-thought data.

Method: AlphaMed uses minimalist rule-based reinforcement learning on public multiple-choice QA datasets, avoiding supervised fine-tuning and chain-of-thought data. A data-centric analysis explores dataset informativeness, quantity, diversity, and question difficulty.

Result: AlphaMed outperforms models trained with conventional SFT+RL pipelines and even larger or closed-source models like DeepSeek-V3-671B and Claude-3.5-Sonnet on six medical QA benchmarks.

Conclusion: Minimalist RL on informative multiple-choice QA data can effectively induce reasoning without CoT supervision. Current benchmarks have limitations, highlighting the need for more challenging, reasoning-oriented medical QA benchmarks.

Abstract: Improving performance on complex tasks and enabling interpretable decision
making in large language models (LLMs), especially for clinical applications,
requires effective reasoning. Yet this remains challenging without supervised
fine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from
closed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the
first medical LLM to show that reasoning capability can emerge purely through
reinforcement learning (RL), using minimalist rule-based rewards on public
multiple-choice QA datasets, without relying on SFT or distilled CoT data.
AlphaMed achieves state-of-the-art results on six medical QA benchmarks,
outperforming models trained with conventional SFT+RL pipelines. On challenging
benchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source
models such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the
factors behind this success, we conduct a comprehensive data-centric analysis
guided by three questions: (i) Can minimalist rule-based RL incentivize
reasoning without distilled CoT supervision? (ii) How do dataset quantity and
diversity impact reasoning? (iii) How does question difficulty shape the
emergence and generalization of reasoning? Our findings show that dataset
informativeness is a key driver of reasoning performance, and that minimalist
RL on informative, multiple-choice QA data is effective at inducing reasoning
without CoT supervision. We also observe divergent trends across benchmarks,
underscoring limitations in current evaluation and the need for more
challenging, reasoning-oriented medical QA benchmarks.

</details>


### [162] [Counting Cycles with Deepseek](https://arxiv.org/abs/2505.17964)
*Jiashun Jin,Tracy Ke,Bingcheng Sui,Zhenggang Wang*

Key words: AI辅助数学、计算高效等价形式、图论、DeepSeek-R1

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: AI结合新方法和强大编码能力解决了计算高效等价形式（CEEF）的难题，提出了新的通用公式。

Motivation: AI在高级数学领域仍面临挑战，CEEF问题缺乏通用解法，需要复杂组合和计算，人类难以解决但适合AI辅助。

Method: 结合新方法与AI的编码能力，提供明确策略、分步指导和精心设计的提示。

Result: 提出了新的通用公式，首次解决了CEEF问题，证明了AI在明确指导下能完成复杂数学任务。

Conclusion: AI虽无法独立解决复杂数学问题，但在合理引导下能有效辅助并取得突破。

Abstract: Despite recent progress, AI still struggles on advanced mathematics. We
consider a difficult open problem: How to derive a Computationally Efficient
Equivalent Form (CEEF) for the cycle count statistic? The CEEF problem does not
have known general solutions, and requires delicate combinatorics and tedious
calculations. Such a task is hard to accomplish by humans but is an ideal
example where AI can be very helpful. We solve the problem by combining a novel
approach we propose and the powerful coding skills of AI. Our results use
delicate graph theory and contain new formulas for general cases that have not
been discovered before. We find that, while AI is unable to solve the problem
all by itself, it is able to solve it if we provide it with a clear strategy, a
step-by-step guidance and carefully written prompts. For simplicity, we focus
our study on DeepSeek-R1 but we also investigate other AI approaches.

</details>


### [163] [AVerImaTeC: A Dataset for Automatic Verification of Image-Text Claims with Evidence from the Web](https://arxiv.org/abs/2505.17978)
*Rui Cao,Zifeng Ding,Zhijiang Guo,Michael Schlichtkrull,Andreas Vlachos*

Key words: 自动化验证、图文声明、数据集、证据标记、事实核查

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 介绍了AVerImaTeC数据集，包含1,297个真实世界的图文声明，带有证据标记，解决了现有数据集的问题，如上下文依赖、时间泄露和证据不足。

Motivation: 现有图文声明的自动化验证数据集多由合成数据构成，缺乏证据标记，限制了推理能力，因此需要构建更真实、可靠的数据集。

Method: 通过声明标准化、时间约束的证据标记和两阶段充分性检查构建AVerImaTeC数据集，并进行标注一致性评估。

Result: 标注一致性较高（κ=0.742，QA一致率74.7%），并提出了新的证据检索评估方法，建立了开放网页证据的基准表现。

Conclusion: AVerImaTeC为图文声明的事实核查提供了更可靠的数据支持，并为未来研究设定了基准。

Abstract: Textual claims are often accompanied by images to enhance their credibility
and spread on social media, but this also raises concerns about the spread of
misinformation. Existing datasets for automated verification of image-text
claims remain limited, as they often consist of synthetic claims and lack
evidence annotations to capture the reasoning behind the verdict. In this work,
we introduce AVerImaTeC, a dataset consisting of 1,297 real-world image-text
claims. Each claim is annotated with question-answer (QA) pairs containing
evidence from the web, reflecting a decomposed reasoning regarding the verdict.
We mitigate common challenges in fact-checking datasets such as contextual
dependence, temporal leakage, and evidence insufficiency, via claim
normalization, temporally constrained evidence annotation, and a two-stage
sufficiency check. We assess the consistency of the annotation in AVerImaTeC
via inter-annotator studies, achieving a $\kappa=0.742$ on verdicts and
$74.7\%$ consistency on QA pairs. We also propose a novel evaluation method for
evidence retrieval and conduct extensive experiments to establish baselines for
verifying image-text claims using open-web evidence.

</details>


### [164] [TRACE for Tracking the Emergence of Semantic Representations in Transformers](https://arxiv.org/abs/2505.17998)
*Nura Aljaafari,Danilo S. Carvalho,André Freitas*

Key words: Transformer, 相变, 语言抽象, TRACE, ABSynth, 曲率崩溃

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出TRACE框架，通过几何、信息和语言信号检测Transformer模型训练中的相变，揭示语言抽象涌现机制。

Motivation: 探索Transformer模型从记忆到抽象的相变机制，弥补现有研究对语言结构涌现的忽略。

Method: TRACE框架结合几何（曲率、维度）、信息论和语言学分析，使用可控复杂度的合成数据ABSynth。

Result: 发现相变与曲率崩溃和维度稳定相关，且与句法/语义准确性同步；抽象模式在架构变体中一致。

Conclusion: 研究揭示了语言抽象的涌现规律，为模型可解释性、训练效率和组合泛化提供理论基础。

Abstract: Modern transformer models exhibit phase transitions during training, distinct
shifts from memorisation to abstraction, but the mechanisms underlying these
transitions remain poorly understood. Prior work has often focused on endpoint
representations or isolated signals like curvature or mutual information,
typically in symbolic or arithmetic domains, overlooking the emergence of
linguistic structure. We introduce TRACE (Tracking Representation Abstraction
and Compositional Emergence), a diagnostic framework combining geometric,
informational, and linguistic signals to detect phase transitions in
Transformer-based LMs. TRACE leverages a frame-semantic data generation method,
ABSynth, that produces annotated synthetic corpora with controllable
complexity, lexical distributions, and structural entropy, while being fully
annotated with linguistic categories, enabling precise analysis of abstraction
emergence. Experiments reveal that (i) phase transitions align with clear
intersections between curvature collapse and dimension stabilisation; (ii)
these geometric shifts coincide with emerging syntactic and semantic accuracy;
(iii) abstraction patterns persist across architectural variants, with
components like feedforward networks affecting optimisation stability rather
than fundamentally altering trajectories. This work advances our understanding
of how linguistic abstractions emerge in LMs, offering insights into model
interpretability, training efficiency, and compositional generalisation that
could inform more principled approaches to LM development.

</details>


### [165] [Training with Pseudo-Code for Instruction Following](https://arxiv.org/abs/2505.18011)
*Prince Kumar,Rudra Murthy,Riyaz Bhat,Danish Contractor*

Key words: Large Language Models, 伪代码, 指令遵循, 数学推理, 常识推理

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出通过微调LLMs，在指令调优数据中加入伪代码表达的指令，显著提升了模型在指令遵循、数学和常识推理任务中的表现。

Motivation: 尽管LLMs能力快速提升，但在遵循简单明确的指令时仍有困难，尤其是涉及复杂组合时。受伪代码能更好指导模型的启发，作者希望克服编写伪代码的繁琐性，为非专家用户提供更自然的解决方案。

Method: 在指令调优数据中，额外加入用伪代码重新表达的指令及最终响应，对LLMs进行微调。使用5种不同模型在11个公开基准上进行严格实验。

Result: 模型在伪代码训练下，指令遵循能力提升3-19%，且在数学和常识推理任务中平均提升达14%，同时保留原有能力。

Conclusion: 伪代码辅助的指令调优可显著提升LLMs的指令遵循能力，且不影响其他任务表现。

Abstract: Despite the rapid progress in the capabilities of Large Language Models
(LLMs), they continue to have difficulty following relatively simple,
unambiguous instructions, especially when compositions are involved. In this
paper, we take inspiration from recent work that suggests that models may
follow instructions better when they are expressed in pseudo-code. However,
writing pseudo-code programs can be tedious and using few-shot demonstrations
to craft code representations for use in inference can be unnatural for
non-expert users of LLMs. To overcome these limitations, we propose fine-tuning
LLMs with instruction-tuning data that additionally includes instructions
re-expressed in pseudo-code along with the final response. We evaluate models
trained using our method on $11$ publicly available benchmarks comprising of
tasks related to instruction-following, mathematics, and common-sense
reasoning. We conduct rigorous experiments with $5$ different models and find
that not only do models follow instructions better when trained with
pseudo-code, they also retain their capabilities on the other tasks related to
mathematical and common sense reasoning. Specifically, we observe a relative
gain of $3$--$19$% on instruction-following benchmark, and an average gain of
upto 14% across all tasks.

</details>


### [166] [Contrastive Distillation of Emotion Knowledge from LLMs for Zero-Shot Emotion Recognition](https://arxiv.org/abs/2505.18040)
*Minxue Niu,Emily Mower Provost*

Key words: 情感识别, 零样本学习, 对比蒸馏, GPT-4, 边缘计算

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种通过对比蒸馏框架将GPT-4的情感知识迁移到紧凑模型的方法，无需人工标注，实现了零样本情感预测，性能接近GPT-4但模型更小。

Motivation: 传统情感识别模型依赖固定标签集训练且泛化能力有限，而大语言模型虽表现优异但不适合边缘设备，因此需要一种紧凑且高效的方法。

Method: 使用GPT-4生成情感描述作为监督信号，通过对比学习在共享嵌入空间中对齐文本和情感描述，实现零样本预测。

Result: 蒸馏模型在多个数据集上表现优异，性能接近GPT-4但模型规模缩小了10,000倍，且支持不同情感类别、粒度和标签模式。

Conclusion: 该方法成功将大语言模型的情感知识迁移到小模型，为边缘设备上的情感识别提供了高效解决方案。

Abstract: The ability to handle various emotion labels without dedicated training is
crucial for building adaptable Emotion Recognition (ER) systems. Conventional
ER models rely on training using fixed label sets and struggle to generalize
beyond them. On the other hand, Large Language Models (LLMs) have shown strong
zero-shot ER performance across diverse label spaces, but their scale limits
their use on edge devices. In this work, we propose a contrastive distillation
framework that transfers rich emotional knowledge from LLMs into a compact
model without the use of human annotations. We use GPT-4 to generate
descriptive emotion annotations, offering rich supervision beyond fixed label
sets. By aligning text samples with emotion descriptors in a shared embedding
space, our method enables zero-shot prediction on different emotion classes,
granularity, and label schema. The distilled model is effective across multiple
datasets and label spaces, outperforming strong baselines of similar size and
approaching GPT-4's zero-shot performance, while being over 10,000 times
smaller.

</details>


### [167] [MathEDU: Towards Adaptive Feedback for Student Mathematical Problem-Solving](https://arxiv.org/abs/2505.18056)
*Wei-Ling Hsu,Yu-Chien Tang,An-Zi Yen*

Key words: 在线学习, 大型语言模型, 数学问题解决, 个性化反馈, MathEDU数据集

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLMs）在数学问题解决中提供个性化反馈的能力，引入MathEDU数据集并通过实验验证模型在识别正确性方面的表现，但在生成详细教学反馈方面仍有挑战。

Motivation: 在线学习缺乏即时个性化反馈，尤其是在数学问题解决中。研究旨在利用LLMs弥补这一缺陷，提升学习效果。

Method: 基于MathEDU数据集（含学生真实解答与教师反馈），评估LLM在两种情境下的表现：一是利用学生历史答题记录，二是模拟冷启动场景。

Result: 微调后的模型能有效识别答案正确性，但生成细致的教学反馈仍存在困难。

Conclusion: LLMs在支持个性化数学学习方面有潜力，但需进一步优化以提升反馈质量。

Abstract: Online learning enhances educational accessibility, offering students the
flexibility to learn anytime, anywhere. However, a key limitation is the lack
of immediate, personalized feedback, particularly in helping students correct
errors in math problem-solving. Several studies have investigated the
applications of large language models (LLMs) in educational contexts. In this
paper, we explore the capabilities of LLMs to assess students' math
problem-solving processes and provide adaptive feedback. The MathEDU dataset is
introduced, comprising authentic student solutions annotated with teacher
feedback. We evaluate the model's ability to support personalized learning in
two scenarios: one where the model has access to students' prior answer
histories, and another simulating a cold-start context. Experimental results
show that the fine-tuned model performs well in identifying correctness.
However, the model still faces challenges in generating detailed feedback for
pedagogical purposes.

</details>


### [168] [Extended Inductive Reasoning for Personalized Preference Inference from Behavioral Signals](https://arxiv.org/abs/2505.18071)
*Jia-Nan Li,Jian Guan,Wei Wu,Rui Yan*

Key words: 大型语言模型、归纳推理、个性化偏好推断、AlignXplore、强化学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLMs）在归纳推理中的表现，特别是在个性化偏好推断任务中，提出了结合合成数据和在线强化学习的模型AlignXplore，显著提升了性能并揭示了人类类似的推理模式。

Motivation: 当前LLMs在演绎推理任务（如数学和编程）中表现优异，但归纳推理（从不完整证据中推导规律）研究不足，尤其是在LLM对齐中的个性化偏好推断任务中表现不佳。

Method: 提出了AlignXplore模型，通过结合基于合成数据的冷启动训练和在线强化学习，系统地推断用户交互历史中的偏好。

Result: AlignXplore在域内和域外基准测试中平均提升了11.05%，且在多样化输入格式和下游模型中表现出强泛化能力。

Conclusion: AlignXplore验证了系统化偏好推断的有效性，并揭示了训练过程中类似人类的归纳推理模式。

Abstract: Large language models (LLMs) have demonstrated significant success in complex
reasoning tasks such as math and coding. In contrast to these tasks where
deductive reasoning predominates, inductive reasoning\textemdash the ability to
derive general rules from incomplete evidence, remains underexplored. This
paper investigates extended inductive reasoning in LLMs through the lens of
personalized preference inference, a critical challenge in LLM alignment where
current approaches struggle to capture diverse user preferences. The task
demands strong inductive reasoning capabilities as user preferences are
typically embedded implicitly across various interaction forms, requiring
models to synthesize consistent preference patterns from scattered signals. We
propose \textsc{AlignXplore}, a model that leverages extended reasoning chains
to enable systematic preference inference from behavioral signals in users'
interaction histories. We develop \textsc{AlignXplore} by combining cold-start
training based on synthetic data with subsequent online reinforcement learning.
Through extensive experiments, we demonstrate that \textsc{AlignXplore}
achieves substantial improvements over the backbone model by an average of
11.05\% on in-domain and out-of-domain benchmarks, while maintaining strong
generalization ability across different input formats and downstream models.
Further analyses establish best practices for preference inference learning
through systematic comparison of reward modeling strategies, while revealing
the emergence of human-like inductive reasoning patterns during training.

</details>


### [169] [QwenLong-CPRS: Towards $\infty$-LLMs with Dynamic Context Optimization](https://arxiv.org/abs/2505.18092)
*Weizhou Shen,Chenliang Li,Fanqi Wan,Shengyi Liao,Shaopeng Lai,Bo Zhang,Yingcheng Shi,Yuning Wu,Gang Fu,Zhansheng Li,Bin Yang,Ji Zhang,Fei Huang,Jingren Zhou,Ming Yan*

Key words: QwenLong-CPRS, 上下文压缩, 长序列处理, 大语言模型, 动态优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: QwenLong-CPRS 是一种用于长上下文优化的上下文压缩框架，通过动态优化机制和多粒度压缩提升大语言模型处理长序列的效率和性能，显著优于现有方法。

Motivation: 解决大语言模型在长序列处理时的计算开销和 '迷失在中间' 性能下降问题。

Method: 采用动态上下文优化机制，引入四项关键技术：自然语言引导的动态优化、双向推理层增强边界感知、带语言建模头的标记批判机制、窗口并行推理。

Result: 在多个基准测试（4K-2M词）中优于RAG和稀疏注意力等方法，与主流LLM（如GPT-4o）兼容，实现21.59倍上下文压缩和19.15分的平均性能提升，在Qwen2.5-32B-Instruct上达到SOTA。

Conclusion: QwenLong-CPRS 是高效且通用的长上下文优化方案，显著提升性能并兼容多种大语言模型架构。

Abstract: This technical report presents QwenLong-CPRS, a context compression framework
designed for explicit long-context optimization, addressing prohibitive
computation overhead during the prefill stage and the "lost in the middle"
performance degradation of large language models (LLMs) during long sequence
processing. Implemented through a novel dynamic context optimization mechanism,
QwenLong-CPRS enables multi-granularity context compression guided by natural
language instructions, achieving both efficiency gains and improved
performance.
  Evolved from the Qwen architecture series, QwenLong-CPRS introduces four key
innovations: (1) Natural language-guided dynamic optimization, (2)
Bidirectional reasoning layers for enhanced boundary awareness, (3) Token
critic mechanisms with language modeling heads, and (4) Window-parallel
inference.
  Comprehensive evaluations across five benchmarks (4K-2M word contexts)
demonstrate QwenLong-CPRS's threefold effectiveness: (1) Consistent superiority
over other context management methods like RAG and sparse attention in both
accuracy and efficiency. (2) Architecture-agnostic integration with all
flagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3,
and Qwen2.5-max, achieves 21.59$\times$ context compression alongside
19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct,
QwenLong-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on
Ruler-128K and InfiniteBench, establishing new SOTA performance.

</details>


### [170] [Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL](https://arxiv.org/abs/2505.18098)
*Joey Hong,Anca Dragan,Sergey Levine*

Key words: 大型语言模型, 强化学习, 规划, 价值函数, 交互任务

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种利用目标条件价值函数指导大型语言模型（LLM）代理决策的新方法，解决了传统强化学习训练方法在复杂交互任务中的扩展性问题。

Motivation: 大型语言模型在需要长时规划和复杂交互的任务中表现不足，而传统的强化学习微调方法存在计算成本高和无法用于大型API模型的局限性。

Method: 采用目标条件价值函数预测行动结果，指导LLM代理评估多种可能结果并有效规划，同时通过针对推理步骤而非完整动作训练价值函数，保持模块轻量级。

Result: 该方法在工具使用、社交推理和对话等交互任务中表现优于强化学习微调和提示方法，同时保持了高效性和可扩展性。

Conclusion: 目标条件价值函数为LLM在复杂交互任务中的规划和决策提供了一种高效且可扩展的解决方案。

Abstract: Large language models (LLMs) excel in tasks like question answering and
dialogue, but complex tasks requiring interaction, such as negotiation and
persuasion, require additional long-horizon reasoning and planning.
Reinforcement learning (RL) fine-tuning can enable such planning in principle,
but suffers from drawbacks that hinder scalability. In particular, multi-turn
RL training incurs high memory and computational costs, which are exacerbated
when training LLMs as policies. Furthermore, the largest LLMs do not expose the
APIs necessary to be trained in such manner. As a result, modern methods to
improve the reasoning of LLMs rely on sophisticated prompting mechanisms rather
than RL fine-tuning. To remedy this, we propose a novel approach that uses
goal-conditioned value functions to guide the reasoning of LLM agents, that
scales even to large API-based models. These value functions predict how a task
will unfold given an action, allowing the LLM agent to evaluate multiple
possible outcomes, both positive and negative, to plan effectively. In
addition, these value functions are trained over reasoning steps rather than
full actions, to be a concise and light-weight module that facilitates
decision-making in multi-turn interactions. We validate our method on tasks
requiring interaction, including tool use, social deduction, and dialogue,
demonstrating superior performance over both RL fine-tuning and prompting
methods while maintaining efficiency and scalability.

</details>


### [171] [ManuSearch: Democratizing Deep Search in Large Language Models with a Transparent and Open Multi-Agent Framework](https://arxiv.org/abs/2505.18105)
*Lisheng Huang,Yichen Liu,Jinhao Jiang,Rongxiang Zhang,Jiahao Yan,Junyi Li,Wayne Xin Zhao*

Key words: 大型语言模型, 多代理框架, 深度搜索, 开放网络推理, ORION基准

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: ManuSearch是一个透明、模块化的多代理框架，旨在为大型语言模型（LLM）提供深度搜索能力，通过三个协作代理分解搜索和推理过程，显著优于现有开源和闭源系统。

Motivation: 当前基于网络增强的大型语言模型在复杂推理任务中表现出色，但其能力主要存在于架构不透明的专有系统中。ManuSearch旨在通过透明且模块化的框架，实现深度搜索的民主化。

Method: ManuSearch将搜索和推理过程分解为三个协作代理：解决方案规划代理（迭代生成子查询）、互联网搜索代理（实时检索相关文档）和结构化网页阅读代理（从原始网页内容中提取关键证据）。

Result: 实验结果表明，ManuSearch显著优于现有的开源基线，并超越了领先的闭源系统。

Conclusion: ManuSearch为开放深度搜索系统的可重现和可扩展研究铺平了道路，并开源了相关数据和代码。

Abstract: Recent advances in web-augmented large language models (LLMs) have exhibited
strong performance in complex reasoning tasks, yet these capabilities are
mostly locked in proprietary systems with opaque architectures. In this work,
we propose \textbf{ManuSearch}, a transparent and modular multi-agent framework
designed to democratize deep search for LLMs. ManuSearch decomposes the search
and reasoning process into three collaborative agents: (1) a solution planning
agent that iteratively formulates sub-queries, (2) an Internet search agent
that retrieves relevant documents via real-time web search, and (3) a
structured webpage reading agent that extracts key evidence from raw web
content. To rigorously evaluate deep reasoning abilities, we introduce
\textbf{ORION}, a challenging benchmark focused on open-web reasoning over
long-tail entities, covering both English and Chinese. Experimental results
show that ManuSearch substantially outperforms prior open-source baselines and
even surpasses leading closed-source systems. Our work paves the way for
reproducible, extensible research in open deep search systems. We release the
data and code in https://github.com/RUCAIBox/ManuSearch

</details>


### [172] [Watch and Listen: Understanding Audio-Visual-Speech Moments with Multimodal LLM](https://arxiv.org/abs/2505.18110)
*Zinuo Li,Xian Zhang,Yongxin Guo,Mohammed Bennamoun,Farid Boussaid,Girish Dwivedi,Luqi Gong,Qiuhong Ke*

Key words: TriSense, 多模态, 视频理解, 音频融合, 大语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: TriSense是一个三模态大语言模型，通过整合视觉、音频和语音模态，实现视频时序理解。核心是Query-Based Connector，能自适应调整模态贡献，并在TriSense-2M数据集上验证效果。

Motivation: 现有模型难以有效融合和解释音频信息，限制了视频时序理解能力，因此提出了TriSense来解决这一问题。

Method: 使用Query-Based Connector自适应加权模态贡献，并在TriSense-2M数据集上进行训练和评估。

Result: 在多基准测试中展现了TriSense的有效性，推动了多模态视频分析的发展。

Conclusion: TriSense通过三模态融合提升了视频时序理解能力，且代码和数据集将公开。

Abstract: Humans naturally understand moments in a video by integrating visual and
auditory cues. For example, localizing a scene in the video like "A scientist
passionately speaks on wildlife conservation as dramatic orchestral music
plays, with the audience nodding and applauding" requires simultaneous
processing of visual, audio, and speech signals. However, existing models often
struggle to effectively fuse and interpret audio information, limiting their
capacity for comprehensive video temporal understanding. To address this, we
present TriSense, a triple-modality large language model designed for holistic
video temporal understanding through the integration of visual, audio, and
speech modalities. Central to TriSense is a Query-Based Connector that
adaptively reweights modality contributions based on the input query, enabling
robust performance under modality dropout and allowing flexible combinations of
available inputs. To support TriSense's multimodal capabilities, we introduce
TriSense-2M, a high-quality dataset of over 2 million curated samples generated
via an automated pipeline powered by fine-tuned LLMs. TriSense-2M includes
long-form videos and diverse modality combinations, facilitating broad
generalization. Extensive experiments across multiple benchmarks demonstrate
the effectiveness of TriSense and its potential to advance multimodal video
analysis. Code and dataset will be publicly released.

</details>


### [173] [UNJOIN: Enhancing Multi-Table Text-to-SQL Generation via Schema Simplification](https://arxiv.org/abs/2505.18122)
*Poojah Ganesan,Rajat Aayush Jha,Dan Roth,Vivek Gupta*

Key words: Text-to-SQL, 多表数据库, 模式检索, JOIN/UNION, 可扩展性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: UNJOIN是一个两阶段框架，简化多表数据库中的Text-to-SQL任务，通过解耦模式元素检索和SQL逻辑生成，提升了准确性和可扩展性。

Motivation: 多表数据库的复杂模式和关系操作导致现有方法在检索表与列、生成JOIN/UNION时表现不佳，UNJOIN旨在解决这些问题。

Method: 1. 将所有表的列名合并为单表表示（通过表名前缀）；2. 基于简化模式生成SQL并映射回原始模式。

Result: 在SPIDER和BIRD数据集上，UNJOIN达到或超越现有最佳基线，且无需数据访问或微调。

Conclusion: UNJOIN通过分阶段处理模式检索和SQL生成，显著提升了多表Text-to-SQL的性能和通用性。

Abstract: Recent advances in large language models (LLMs) have greatly improved
Text-to-SQL performance for single-table queries. But, it remains challenging
in multi-table databases due to complex schema and relational operations.
Existing methods often struggle with retrieving the right tables and columns,
generating accurate JOINs and UNIONs, and generalizing across diverse schemas.
To address these issues, we introduce UNJOIN, a two-stage framework that
decouples the retrieval of schema elements from SQL logic generation. In the
first stage, we merge the column names of all tables in the database into a
single-table representation by prefixing each column with its table name. This
allows the model to focus purely on accurate retrieval without being distracted
by the need to write complex SQL logic. In the second stage, the SQL query is
generated on this simplified schema and mapped back to the original schema by
reconstructing JOINs, UNIONs, and relational logic. Evaluations on SPIDER and
BIRD datasets show that UNJOIN matches or exceeds the state-of-the-art
baselines. UNJOIN uses only schema information, which does not require data
access or fine-tuning, making it scalable and adaptable across databases.

</details>


### [174] [Frankentext: Stitching random text fragments into long-form narratives](https://arxiv.org/abs/2505.18128)
*Chau Minh Pham,Jenna Russell,Dzung Pham,Mohit Iyyer*

Key words: Frankentexts, LLMs, controllable generation, AI detection, human-AI collaboration

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Frankentexts是利用LLMs在严格复制比约束下生成的长篇叙事，探讨了可控生成的挑战及AI检测器的局限性。

Motivation: 探索在高度约束（如90%内容需直接复制人类文本）下，LLMs能否生成连贯且符合提示的长篇叙事，并评估其质量与可检测性。

Method: 先通过选择并组合人类文本片段生成初稿，再迭代修订以保持指定的复制比例。

Result: Gemini-2.5-Pro表现突出，81%的Frankentexts连贯且100%符合提示，59%被检测器误判为人类写作。人类能通过语调突变和语法不一致性识别部分Frankentexts。

Conclusion: Frankentexts不仅是可控生成的挑战性任务，也为混合作者身份检测和人类-AI协作写作提供了研究平台。

Abstract: We introduce Frankentexts, a new type of long-form narratives produced by
LLMs under the extreme constraint that most tokens (e.g., 90%) must be copied
verbatim from human writings. This task presents a challenging test of
controllable generation, requiring models to satisfy a writing prompt,
integrate disparate text fragments, and still produce a coherent narrative. To
generate Frankentexts, we instruct the model to produce a draft by selecting
and combining human-written passages, then iteratively revise the draft while
maintaining a user-specified copy ratio. We evaluate the resulting Frankentexts
along three axes: writing quality, instruction adherence, and detectability.
Gemini-2.5-Pro performs surprisingly well on this task: 81% of its Frankentexts
are coherent and 100% relevant to the prompt. Notably, up to 59% of these
outputs are misclassified as human-written by detectors like Pangram, revealing
limitations in AI text detectors. Human annotators can sometimes identify
Frankentexts through their abrupt tone shifts and inconsistent grammar between
segments, especially in longer generations. Beyond presenting a challenging
generation task, Frankentexts invite discussion on building effective detectors
for this new grey zone of authorship, provide training data for mixed
authorship detection, and serve as a sandbox for studying human-AI co-writing
processes.

</details>


### [175] [Graph-Linguistic Fusion: Using Language Models for Wikidata Vandalism Detection](https://arxiv.org/abs/2505.18136)
*Mykola Trokhymovych,Lydia Pintscher,Ricardo Baeza-Yates,Diego Saez-Trumper*

Key words: Wikidata, vandalism detection, Graph2Text, multilingual language model

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种新的 Wikidata 破坏行为检测系统，通过 Graph2Text 方法统一处理编辑内容，利用多语言模型提升检测效果。

Motivation: 针对 Wikidata 这种庞大且复杂的知识库，现有系统难以全面覆盖内容更改中的破坏行为，需要一种更高效统一的检测方法。

Method: 采用 Graph2Text 方法将结构化和文本内容编辑统一转换到同一空间，并使用多语言模型进行评估。

Result: 实验表明，新系统在性能上优于当前生产系统，并开放了代码和大型数据集供进一步研究。

Conclusion: 统一的检测方法不仅提高了覆盖率，还简化了维护工作，为 Wikidata 破坏行为检测提供了更优解决方案。

Abstract: We introduce a next-generation vandalism detection system for Wikidata, one
of the largest open-source structured knowledge bases on the Web. Wikidata is
highly complex: its items incorporate an ever-expanding universe of factual
triples and multilingual texts. While edits can alter both structured and
textual content, our approach converts all edits into a single space using a
method we call Graph2Text. This allows for evaluating all content changes for
potential vandalism using a single multilingual language model. This unified
approach improves coverage and simplifies maintenance. Experiments demonstrate
that our solution outperforms the current production system. Additionally, we
are releasing the code under an open license along with a large dataset of
various human-generated knowledge alterations, enabling further research.

</details>


### [176] [Lost in the Haystack: Smaller Needles are More Difficult for LLMs to Find](https://arxiv.org/abs/2505.18148)
*Owen Bianchi,Mathew J. Koretsky,Maya Willey,Chelsea X. Alvarado,Tanay Nayak,Adi Asija,Nicole Kuznetsov,Mike A. Nalls,Faraz Faghri,Daniel Khashabi*

Key words: 大型语言模型, 金上下文长度, 位置敏感性, 长上下文问答

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLM）在‘大海捞针’任务中的表现，发现金上下文长度对模型性能有显著影响，特别是短金上下文会大幅降低性能并增加位置敏感性。

Motivation: 探索金上下文大小对LLM在长上下文问答任务中表现的影响，填补此前研究的空白。

Method: 通过系统实验，研究不同金上下文长度对LLM性能的影响，覆盖多种领域和模型架构。

Result: 实验表明，金上下文较短时，LLM性能显著下降，位置敏感性增强，这一现象在不同领域和模型中都存在。

Conclusion: 研究为设计健壮、上下文感知的LLM驱动系统提供了重要指导。

Abstract: Large language models (LLMs) face significant challenges with
needle-in-a-haystack tasks, where relevant information ("the needle") must be
drawn from a large pool of irrelevant context ("the haystack"). Previous
studies have highlighted positional bias and distractor quantity as critical
factors affecting model performance, yet the influence of gold context size has
received little attention. We address this gap by systematically studying how
variations in gold context length impact LLM performance on long-context
question answering tasks. Our experiments reveal that LLM performance drops
sharply when the gold context is shorter, i.e., smaller gold contexts
consistently degrade model performance and amplify positional sensitivity,
posing a major challenge for agentic systems that must integrate scattered,
fine-grained information of varying lengths. This pattern holds across three
diverse domains (general knowledge, biomedical reasoning, and mathematical
reasoning) and seven state-of-the-art LLMs of various sizes and architectures.
Our work provides clear insights to guide the design of robust, context-aware
LLM-driven systems.

</details>


### [177] [First Finish Search: Efficient Test-Time Scaling in Large Language Models](https://arxiv.org/abs/2505.18149)
*Aradhye Agarwal,Ayan Sengupta,Tanmoy Chakraborty*

Key words: 测试时扩展（TTS）, 并行解码, FFS, 推理任务, 大语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了First Finish Search（FFS），一种无需训练、并行解码的策略，通过对推理任务中短路径更易正确的观察进行优化，显著提高了大语言模型的推理准确率。

Motivation: 现有测试时扩展（TTS）方法通常依赖长解码路径或大量样本生成，增加了计算开销和延迟。研究发现较短推理路径更可能正确，因此提出FFS以改进效率。

Method: FFS通过并行启动多个独立样本并在任一完成后立即返回，无需额外训练。

Result: 在DeepSeek-R1模型上，FFS在AIME数据集上达到82.23%准确率，比单独使用模型提升15%，接近OpenAI o4-mini性能。

Conclusion: FFS表明简单的TTS策略在推理时效果显著，揭示了简单方法的潜力。

Abstract: Test-time scaling (TTS), which involves dynamic allocation of compute during
inference, offers a promising way to improve reasoning in large language
models. While existing TTS methods work well, they often rely on long decoding
paths or require a large number of samples to be generated, increasing the
token usage and inference latency. We observe the surprising fact that for
reasoning tasks, shorter traces are much more likely to be correct than longer
ones. Motivated by this, we introduce First Finish Search (FFS), a
training-free parallel decoding strategy that launches $n$ independent samples
and returns as soon as any one completes. We evaluate FFS alongside simple
decoding, beam search, majority voting, and budget forcing on four reasoning
models (DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B and Phi-4-Reasoning-Plus) and
across four datasets (AIME24, AIME25-I, AIME25-II and GPQA Diamond). With
DeepSeek-R1, FFS achieves $82.23\%$ accuracy on the AIME datasets, a $15\%$
improvement over DeepSeek-R1's standalone accuracy, nearly matching OpenAI's
o4-mini performance. Our theoretical analysis explains why stopping at the
shortest trace is likely to yield a correct answer and identifies the
conditions under which early stopping may be suboptimal. The elegance and
simplicity of FFS demonstrate that straightforward TTS strategies can perform
remarkably well, revealing the untapped potential of simple approaches at
inference time.

</details>


### [178] [Fann or Flop: A Multigenre, Multiera Benchmark for Arabic Poetry Understanding in LLMs](https://arxiv.org/abs/2505.18152)
*Wafa Alghallabi,Ritesh Thawkar,Sara Ghaboura,Ketan More,Omkar Thawakar,Hisham Cholakkal,Salman Khan,Rao Muhammad Anwer*

Key words: 阿拉伯诗歌、大语言模型、语义理解、文化敏感度、评估基准

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究介绍了首个评估大语言模型（LLMs）对阿拉伯诗歌理解的基准`Fann or Flop`，覆盖12个历史时期、21种诗体和多种格律形式，测试内容包括语义、隐喻、韵律和文化背景。结果表明现有LLMs在诗歌理解上表现不佳。

Motivation: 阿拉伯诗歌是阿拉伯语言中高度复杂且文化深厚的表达形式，而LLMs在这一领域的能力尚未被充分探索。研究旨在通过诗歌理解评估LLMs对古典阿拉伯语的深层掌握能力。

Method: 构建了`Fann or Flop`基准，包含精选诗歌及注释，评估LLMs的语义理解、隐喻解读、韵律感知和文化背景认知。

Result: 评估显示，尽管LLMs在标准阿拉伯任务中表现良好，但在诗歌理解上普遍困难。

Conclusion: 诗歌理解是评估LLMs对古典阿拉伯语深度理解的强指标，提出了未来改进方向。

Abstract: Arabic poetry stands as one of the most sophisticated and culturally embedded
forms of expression in the Arabic language, known for its layered meanings,
stylistic diversity, and deep historical continuity. Although large language
models (LLMs) have demonstrated strong performance across languages and tasks,
their ability to understand Arabic poetry remains largely unexplored. In this
work, we introduce `Fann or Flop`, the first benchmark designed to assess the
comprehension of Arabic poetry by LLMs in twelve historical eras, covering 21
core poetic genres and a variety of metrical forms, from classical structures
to contemporary free verse. The benchmark comprises a curated corpus of poems
with explanations that assess semantic understanding, metaphor interpretation,
prosodic awareness, and cultural context. We argue that poetic comprehension
offers a strong indicator for testing how good the LLM is in understanding
classical Arabic through the Arabic poetry. Unlike surface-level tasks, this
domain demands deeper interpretive reasoning and cultural sensitivity. Our
evaluation of state-of-the-art LLMs shows that most models struggle with poetic
understanding despite strong results on standard Arabic benchmarks. We release
`Fann or Flop` along with the evaluation suite as an open-source resource to
enable rigorous evaluation and advancement for Arabic language models. Code is
available at: https://github.com/mbzuai-oryx/FannOrFlop.

</details>


### [179] [The Staircase of Ethics: Probing LLM Value Priorities through Multi-Step Induction to Complex Moral Dilemmas](https://arxiv.org/abs/2505.18154)
*Ya Wu,Qiang Sheng,Danding Wang,Guang Yang,Yifan Sun,Zhengjia Wang,Yuyan Bu,Juan Cao*

Key words: 道德困境, 大型语言模型, 动态评估, 价值偏好, 伦理推理

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究提出了多步道德困境（MMDs）数据集，用于评估大型语言模型（LLMs）在302个五阶段困境中的道德推理能力，发现模型的价值偏好随困境复杂度变化而动态调整。

Motivation: 现有评估主要依赖单步测试，无法捕捉LLMs在动态道德挑战中的适应能力，因此需要更精细、动态的评估方法。

Method: 构建MMDs数据集，对九种常用LLMs进行多阶段道德困境测试，分析其价值偏好变化。

Result: 研究发现LLMs的价值偏好（如关怀与公平）随困境进展动态变化，且优先顺序因情境不同而异。

Conclusion: 需转向动态、情境感知的评估范式，以推动更符合人类价值观的LLMs发展。

Abstract: Ethical decision-making is a critical aspect of human judgment, and the
growing use of LLMs in decision-support systems necessitates a rigorous
evaluation of their moral reasoning capabilities. However, existing assessments
primarily rely on single-step evaluations, failing to capture how models adapt
to evolving ethical challenges. Addressing this gap, we introduce the
Multi-step Moral Dilemmas (MMDs), the first dataset specifically constructed to
evaluate the evolving moral judgments of LLMs across 3,302 five-stage dilemmas.
This framework enables a fine-grained, dynamic analysis of how LLMs adjust
their moral reasoning across escalating dilemmas. Our evaluation of nine widely
used LLMs reveals that their value preferences shift significantly as dilemmas
progress, indicating that models recalibrate moral judgments based on scenario
complexity. Furthermore, pairwise value comparisons demonstrate that while LLMs
often prioritize the value of care, this value can sometimes be superseded by
fairness in certain contexts, highlighting the dynamic and context-dependent
nature of LLM ethical reasoning. Our findings call for a shift toward dynamic,
context-aware evaluation paradigms, paving the way for more human-aligned and
value-sensitive development of LLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [180] [Generalizing Large Language Model Usability Across Resource-Constrained](https://arxiv.org/abs/2505.17040)
*Yun-Da Tsai*

Key words: 大语言模型、多模态学习、对抗提示、推理优化、低资源任务

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文系统地研究了如何在大语言模型的实用化中提升其适应性、可扩展性和效率，特别是面对多模态、低资源和计算受限等现实约束条件下的表现。

Motivation: 现有方法通常依赖于昂贵的监督微调或假设训练条件固定，限制了在面临新模态、有限数据或计算资源受限时的泛化能力。论文旨在解决这些问题，提升大语言模型在现实约束下的实用性。

Method: 提出了多种方法：1）基于文本的对齐框架，通过自然语言接口无缝整合多种模态；2）对抗提示技术，增强模型对噪声和缺失模态的鲁棒性；3）推理时优化策略，利用提示搜索和不确定性量化提升性能；4）针对低资源领域（如Verilog代码生成）设计合成数据管道和逻辑增强推理模型。

Result: 论文在多模态适应性、推理效率及低资源任务中取得了显著成果，包括在Verilog代码生成上实现SOTA性能，且无需额外训练或大规模数据。

Conclusion: 通过一系列创新方法，论文为提升大语言模型在实践中的适应性和效率提供了系统性解决方案，适用于多模态、低资源和计算受限场景。

Abstract: Large Language Models (LLMs) have achieved remarkable success across a wide
range of natural language tasks, and recent efforts have sought to extend their
capabilities to multimodal domains and resource-constrained environments.
However, existing approaches often rely on costly supervised fine-tuning or
assume fixed training conditions, limiting their generalization when facing
unseen modalities, limited data, or restricted compute resources. This
dissertation presents a systematic study toward generalizing LLM usability
under real-world constraints. First, it introduces a robust text-centric
alignment framework that enables LLMs to seamlessly integrate diverse
modalities-including text, images, tables, and any modalities - via natural
language interfaces. This approach supports in-context adaptation to unseen or
dynamically changing modalities without requiring retraining. To enhance
robustness against noisy and missing modalities, an adversarial prompting
technique is proposed, generating semantically challenging perturbations at the
prompt level to stress-test model reliability. Beyond multimodal setting, the
dissertation investigates inference-time optimization strategies for LLMs,
leveraging prompt search and uncertainty quantification to improve performance
without additional model training. This perspective offers an efficient
alternative to scaling model parameters or retraining from scratch.
Additionally, the work addresses low-resource domains such as Verilog code
generation by designing correct-by-construction synthetic data pipelines and
logic-enhanced reasoning models, achieving state-of-the-art performance with
minimal data. Together, these contributions form a unified effort to enhance
the adaptability, scalability, and efficiency of large language models under
practical constraints.

</details>


### [181] [RAP: Runtime-Adaptive Pruning for LLM Inference](https://arxiv.org/abs/2505.17138)
*Huanrong Liu,Chunlin Tian,Xuyang Wei,Jiaheng Dai,Qin Liu,Tianqi Wei,Qingbiao Li,Li Li*

Key words: 大语言模型,压缩,强化学习,KV缓存,弹性剪枝

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了RAP，一种基于强化学习的弹性剪枝框架，动态调整压缩策略以适应运行时内存变化和异构KV缓存需求，显著优于现有方法。

Motivation: 虽然大语言模型在语言理解和生成方面表现出色，但其巨大的计算和内存需求阻碍了部署，现有压缩方法因依赖固定启发式策略而无法适应运行时变化。

Method: 采用强化学习驱动的弹性剪枝框架RAP，动态跟踪模型参数与KV缓存的比率，并根据当前内存预算和工作负载情况，保留最大化效用的组件。

Result: 实验表明，RAP在动态联合考虑模型权重和KV缓存方面优于现有方法。

Conclusion: RAP通过动态调整压缩策略，首次实现在运行时联合优化模型权重和KV缓存，提升了部署效率。

Abstract: Large language models (LLMs) excel at language understanding and generation,
but their enormous computational and memory requirements hinder deployment.
Compression offers a potential solution to mitigate these constraints. However,
most existing methods rely on fixed heuristics and thus fail to adapt to
runtime memory variations or heterogeneous KV-cache demands arising from
diverse user requests. To address these limitations, we propose RAP, an elastic
pruning framework driven by reinforcement learning (RL) that dynamically
adjusts compression strategies in a runtime-aware manner. Specifically, RAP
dynamically tracks the evolving ratio between model parameters and KV-cache
across practical execution. Recognizing that FFNs house most parameters,
whereas parameter -light attention layers dominate KV-cache formation, the RL
agent retains only those components that maximize utility within the current
memory budget, conditioned on instantaneous workload and device state.
Extensive experiments results demonstrate that RAP outperforms state-of-the-art
baselines, marking the first time to jointly consider model weights and
KV-cache on the fly.

</details>


### [182] [MetaSTH-Sleep: Towards Effective Few-Shot Sleep Stage Classification with Spatial-Temporal Hypergraph Enhanced Meta-Learning](https://arxiv.org/abs/2505.17142)
*Jingyu Li,Tiehua Zhang,Jinze Wang,Yi Zhang,Yuhuan Li,Yifan Zhao,Zhishu Shen,Jiannan Liu*

Key words: 睡眠阶段分类, 少样本学习, 元学习, 时空超图, EEG信号

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出MetaSTH-Sleep框架，基于时空超图和元学习，解决睡眠阶段分类中标注数据少、个体差异大及信号高阶关系建模不足的问题。

Motivation: 传统睡眠阶段分类依赖人工标注，耗时耗力；现有深度学习方法在数据少、个体差异大时性能受限，且忽略信号高阶关系。

Method: 采用时空超图增强的元学习框架（MetaSTH-Sleep），通过少量标注样本快速适应新个体，并建模EEG信号的时空复杂关联。

Result: 实验表明，MetaSTH-Sleep在多个个体上性能显著提升，支持临床睡眠阶段标注。

Conclusion: MetaSTH-Sleep有效解决了小样本和个体差异问题，同时捕捉信号时空依赖，为自动睡眠标注提供新思路。

Abstract: Accurate classification of sleep stages based on bio-signals is fundamental
for automatic sleep stage annotation. Traditionally, this task relies on
experienced clinicians to manually annotate data, a process that is both
time-consuming and labor-intensive. In recent years, deep learning methods have
shown promise in automating this task. However, three major challenges remain:
(1) deep learning models typically require large-scale labeled datasets, making
them less effective in real-world settings where annotated data is limited; (2)
significant inter-individual variability in bio-signals often results in
inconsistent model performance when applied to new subjects, limiting
generalization; and (3) existing approaches often overlook the high-order
relationships among bio-signals, failing to simultaneously capture signal
heterogeneity and spatial-temporal dependencies. To address these issues, we
propose MetaSTH-Sleep, a few-shot sleep stage classification framework based on
spatial-temporal hypergraph enhanced meta-learning. Our approach enables rapid
adaptation to new subjects using only a few labeled samples, while the
hypergraph structure effectively models complex spatial interconnections and
temporal dynamics simultaneously in EEG signals. Experimental results
demonstrate that MetaSTH-Sleep achieves substantial performance improvements
across diverse subjects, offering valuable insights to support clinicians in
sleep stage annotation.

</details>


### [183] [Efficient Training of Neural SDEs Using Stochastic Optimal Control](https://arxiv.org/abs/2505.17150)
*Rembert Daems,Manfred Opper,Guillaume Crevecoeur,Tolga Birdal*

Key words: 变分推理, 神经随机微分方程, 控制理论, 最优控制, 训练效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种分层控制理论启发的变分推理方法，用于神经随机微分方程（SDEs），通过分解控制项为线性和非线性部分，提升了训练效率和收敛速度。

Motivation: 解决神经随机微分方程在变分推理中因ELBO最大化迭代计算带来的计算挑战，同时保持模型的表达能力。

Method: 将控制项分解为线性和残余非线性部分，利用随机最优控制理论推导线性SDE的最优控制项，并用神经网络建模非线性部分。

Result: 实现了神经SDE的高效训练，线性部分最优且无需学习，降低了初始化成本并加速了收敛。

Conclusion: 该方法在保持模型表达能力的同时显著提升了训练效率和收敛速度，为时间序列的不确定性推理提供了有效工具。

Abstract: We present a hierarchical, control theory inspired method for variational
inference (VI) for neural stochastic differential equations (SDEs). While VI
for neural SDEs is a promising avenue for uncertainty-aware reasoning in
time-series, it is computationally challenging due to the iterative nature of
maximizing the ELBO. In this work, we propose to decompose the control term
into linear and residual non-linear components and derive an optimal control
term for linear SDEs, using stochastic optimal control. Modeling the non-linear
component by a neural network, we show how to efficiently train neural SDEs
without sacrificing their expressive power. Since the linear part of the
control term is optimal and does not need to be learned, the training is
initialized at a lower cost and we observe faster convergence.

</details>


### [184] [TrimR: Verifier-based Training-Free Thinking Compression for Efficient Test-Time Scaling](https://arxiv.org/abs/2505.17155)
*Weizhe Lin,Xing Li,Zhiyuan Yang,Xiaojin Fu,Hui-Ling Zhen,Yaoyuan Wang,Xianzhi Yu,Wulong Liu,Xiaosong Li,Mingxuan Yuan*

Key words: Large Reasoning Models, Chain-of-Thought, 推理优化, 动态压缩, 工业部署

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了TrimR框架，通过动态压缩Chain-of-Thought推理来提升Large Reasoning Models的推理效率，在保持精度的同时显著减少运行时计算开销。

Motivation: Large Reasoning Models在复杂任务中表现优异，但冗余的推理链导致计算资源浪费，亟需高效优化方法。

Method: 基于验证器的训练无关框架，动态检测并截断冗余推理链，无需微调模型或验证器。

Result: 在MATH500等四个基准测试中，推理运行时最高提升70%，精度影响可忽略。

Conclusion: TrimR在工业级部署中展现了高效推理优化的潜力，显著提升了大模型的实用性。

Abstract: Large Reasoning Models (LRMs) demonstrate exceptional capability in tackling
complex mathematical, logical, and coding tasks by leveraging extended
Chain-of-Thought (CoT) reasoning. Test-time scaling methods, such as prolonging
CoT with explicit token-level exploration, can push LRMs' accuracy boundaries,
but they incur significant decoding overhead. A key inefficiency source is LRMs
often generate redundant thinking CoTs, which demonstrate clear structured
overthinking and underthinking patterns. Inspired by human cognitive reasoning
processes and numerical optimization theories, we propose TrimR, a
verifier-based, training-free, efficient framework for dynamic CoT compression
to trim reasoning and enhance test-time scaling, explicitly tailored for
production-level deployment. Our method employs a lightweight, pretrained,
instruction-tuned verifier to detect and truncate redundant intermediate
thoughts of LRMs without any LRM or verifier fine-tuning. We present both the
core algorithm and asynchronous online system engineered for high-throughput
industrial applications. Empirical evaluations on Ascend NPUs and vLLM show
that our framework delivers substantial gains in inference efficiency under
large-batch workloads. In particular, on the four MATH500, AIME24, AIME25, and
GPQA benchmarks, the reasoning runtime of Pangu-R-38B, QwQ-32B, and
DeepSeek-R1-Distill-Qwen-32B is improved by up to 70% with negligible impact on
accuracy.

</details>


### [185] [OCR-Reasoning Benchmark: Unveiling the True Capabilities of MLLMs in Complex Text-Rich Image Reasoning](https://arxiv.org/abs/2505.17163)
*Mingxin Huang,Yongxin Shi,Dezhi Peng,Songxuan Lai,Zecheng Xie,Lianwen Jin*

Key words: 多模态大模型、文本丰富图像、推理任务、基准评估

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了OCR-Reasoning基准，用于评估多模态大模型在文本丰富图像推理任务上的表现，发现现有方法的局限性。

Motivation: 现有多模态慢思维系统在文本丰富图像推理任务上的性能研究不足，缺乏系统基准。

Method: 构建包含1,069个标注样本的OCR-Reasoning基准，覆盖6种核心推理能力和18种任务，同时标注答案与推理过程。

Result: 即使最先进的模型在OCR-Reasoning上的准确率也不超过50%，表明文本丰富图像推理仍是难题。

Conclusion: OCR-Reasoning揭示了现有方法的不足，呼吁进一步研究改进文本丰富图像推理能力。

Abstract: Recent advancements in multimodal slow-thinking systems have demonstrated
remarkable performance across diverse visual reasoning tasks. However, their
capabilities in text-rich image reasoning tasks remain understudied due to the
lack of a systematic benchmark. To address this gap, we propose OCR-Reasoning,
a comprehensive benchmark designed to systematically assess Multimodal Large
Language Models on text-rich image reasoning tasks. The benchmark comprises
1,069 human-annotated examples spanning 6 core reasoning abilities and 18
practical reasoning tasks in text-rich visual scenarios. Furthermore, unlike
other text-rich image understanding benchmarks that only annotate the final
answers, OCR-Reasoning also annotates the reasoning process simultaneously.
With the annotated reasoning process and the final answers, OCR-Reasoning
evaluates not only the final answers generated by models but also their
reasoning processes, enabling a holistic analysis of their problem-solving
abilities. Leveraging this benchmark, we conducted a comprehensive evaluation
of state-of-the-art MLLMs. Our results demonstrate the limitations of existing
methodologies. Notably, even state-of-the-art MLLMs exhibit substantial
difficulties, with none achieving accuracy surpassing 50\% across
OCR-Reasoning, indicating that the challenges of text-rich image reasoning are
an urgent issue to be addressed. The benchmark and evaluation scripts are
available at https://github.com/SCUT-DLVCLab/OCR-Reasoning.

</details>


### [186] [Tropical Attention: Neural Algorithmic Reasoning for Combinatorial Algorithms](https://arxiv.org/abs/2505.17190)
*Baran Hashemi,Kurt Pasque,Chris Teska,Ruriko Yoshida*

Key words: 动态规划, 热带几何, 注意力机制, 神经算法推理, OOD泛化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种新的注意力机制Tropical attention，能够在热带几何的max-plus半环中运行，优于传统的softmax注意力，并在算法推理任务中表现出更强的OOD泛化能力。

Motivation: 现有神经算法推理模型依赖softmax注意力，但其平滑的指数加权会模糊动态规划算法的尖锐多面体结构，且在OOD设置下表现不佳。

Method: 提出了Tropical attention，一种在max-plus半环中运行的注意力机制，能够近似动态规划型组合算法的热带电路。

Result: Tropical注意力在长度泛化和值泛化任务中优于softmax基线，且在对抗攻击下保持稳定。

Conclusion: Tropical注意力恢复了softmax缺失的尖锐、尺度不变的推理能力，提出了对抗攻击泛化作为神经算法推理的新基准方向。

Abstract: Dynamic programming (DP) algorithms for combinatorial optimization problems
work with taking maximization, minimization, and classical addition in their
recursion algorithms. The associated value functions correspond to convex
polyhedra in the max plus semiring. Existing Neural Algorithmic Reasoning
models, however, rely on softmax-normalized dot-product attention where the
smooth exponential weighting blurs these sharp polyhedral structures and
collapses when evaluated on out-of-distribution (OOD) settings. We introduce
Tropical attention, a novel attention function that operates natively in the
max-plus semiring of tropical geometry. We prove that Tropical attention can
approximate tropical circuits of DP-type combinatorial algorithms. We then
propose that using Tropical transformers enhances empirical OOD performance in
both length generalization and value generalization, on algorithmic reasoning
tasks, surpassing softmax baselines while remaining stable under adversarial
attacks. We also present adversarial-attack generalization as a third axis for
Neural Algorithmic Reasoning benchmarking. Our results demonstrate that
Tropical attention restores the sharp, scale-invariant reasoning absent from
softmax.

</details>


### [187] [Shape it Up! Restoring LLM Safety during Finetuning](https://arxiv.org/abs/2505.17196)
*ShengYun Peng,Pin-Yu Chen,Jianfeng Chi,Seongmin Lee,Duen Horng Chau*

Key words: 大型语言模型, 动态安全塑形, 安全轨迹评估, 微调风险, STAR-DSS

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为动态安全塑形（DSS）的框架，通过细粒度的安全信号在微调大型语言模型时动态强化安全内容并抑制不安全内容，显著提升了模型的安全性。

Motivation: 微调大型语言模型（LLMs）可能引入安全风险，传统的静态安全塑形方法在处理混合安全性内容时效果不佳，需要更精细的动态控制机制。

Method: 论文提出动态安全塑形（DSS）框架，利用安全轨迹评估（STAR）这一细粒度安全信号，在训练过程中动态调整模型的学习权重，强化安全内容并抑制不安全内容。

Result: STAR-DSS框架在多种威胁、数据集和模型家族中显著提升了安全性，且未影响模型在目标任务上的性能。

Conclusion: 动态塑形原则为未来安全研究提供了新方向，能够更有效地应对微调过程中的安全风险。

Abstract: Finetuning large language models (LLMs) enables user-specific customization
but introduces critical safety risks: even a few harmful examples can
compromise safety alignment. A common mitigation strategy is to update the
model more strongly on examples deemed safe, while downweighting or excluding
those flagged as unsafe. However, because safety context can shift within a
single example, updating the model equally on both harmful and harmless parts
of a response is suboptimal-a coarse treatment we term static safety shaping.
In contrast, we propose dynamic safety shaping (DSS), a framework that uses
fine-grained safety signals to reinforce learning from safe segments of a
response while suppressing unsafe content. To enable such fine-grained control
during finetuning, we introduce a key insight: guardrail models, traditionally
used for filtering, can be repurposed to evaluate partial responses, tracking
how safety risk evolves throughout the response, segment by segment. This leads
to the Safety Trajectory Assessment of Response (STAR), a token-level signal
that enables shaping to operate dynamically over the training sequence.
Building on this, we present STAR-DSS, guided by STAR scores, that robustly
mitigates finetuning risks and delivers substantial safety improvements across
diverse threats, datasets, and model families-all without compromising
capability on intended tasks. We encourage future safety research to build on
dynamic shaping principles for stronger mitigation against evolving finetuning
risks.

</details>


### [188] [LengthLogD: A Length-Stratified Ensemble Framework for Enhanced Peptide Lipophilicity Prediction via Multi-Scale Feature Integration](https://arxiv.org/abs/2505.17198)
*Shuang Wu,Meijie Wang,Lun Yu*

Key words: 肽logD预测、长度分层、多尺度表征、分层集成学习、药物开发

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了LengthLogD框架，通过分子长度分层和多尺度分子表征的创新结合，显著提高了肽logD预测的准确性，尤其在长肽预测上效果突出。

Motivation: 由于肽的低膜通透性限制了其药物开发，且肽的logD预测面临序列、结构和电离态复杂相互作用的挑战，本研究旨在开发更精准的预测工具。

Method: 通过分子长度分层建立专门模型，整合原子、结构和拓扑三个层级的分子表征，采用分层集成学习和自适应权重分配机制优化模型。

Result: LengthLogD在所有肽长度类别中表现出色，尤其是长肽预测误差降低34.7%，且相比现有方法，长肽的R^2提高了25.7%。

Conclusion: 该研究为肽药物开发提供了精确的logD预测工具，特别在长肽先导化合物优化中展示了独特价值。

Abstract: Peptide compounds demonstrate considerable potential as therapeutic agents
due to their high target affinity and low toxicity, yet their drug development
is constrained by their low membrane permeability. Molecular weight and peptide
length have significant effects on the logD of peptides, which in turn
influences their ability to cross biological membranes. However, accurate
prediction of peptide logD remains challenging due to the complex interplay
between sequence, structure, and ionization states. This study introduces
LengthLogD, a predictive framework that establishes specialized models through
molecular length stratification while innovatively integrating multi-scale
molecular representations. We constructed feature spaces across three
hierarchical levels: atomic (10 molecular descriptors), structural (1024-bit
Morgan fingerprints), and topological (3 graph-based features including Wiener
index), optimized through stratified ensemble learning. An adaptive weight
allocation mechanism specifically developed for long peptides significantly
enhances model generalizability. Experimental results demonstrate superior
performance across all categories: short peptides (R^2=0.855), medium peptides
(R^2=0.816), and long peptides (R^2=0.882), with a 34.7% reduction in
prediction error for long peptides compared to conventional single-model
approaches. Ablation studies confirm: 1) The length-stratified strategy
contributes 41.2% to performance improvement; 2) Topological features account
for 28.5% of predictive importance. Compared to state-of-the-art models, our
method maintains short peptide prediction accuracy while achieving a 25.7%
increase in the coefficient of determination (R^2) for long peptides. This
research provides a precise logD prediction tool for peptide drug development,
particularly demonstrating unique value in optimizing long peptide lead
compounds.

</details>


### [189] [Secure and Private Federated Learning: Achieving Adversarial Resilience through Robust Aggregation](https://arxiv.org/abs/2505.17226)
*Kun Yang,Neena Imam*

Key words: 联邦学习, 拜占庭攻击, 鲁棒聚合, 隐私保护, ArKrum

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为ArKrum的新聚合策略，用于增强联邦学习（FL）系统在对抗性环境中的鲁棒性和隐私保护能力，通过中值过滤和多更新平均机制有效抵御恶意客户攻击。

Motivation: 联邦学习面临恶意客户（如拜占庭客户）的攻击风险，传统聚合方法（如简单平均）缺乏鲁棒性，现有方法（如Krum算法）需要提前知道恶意客户数量，这在现实场景中不可行。

Method: ArKrum基于rKrum改进，引入中值过滤机制去除极端异常值，并采用多更新平均方案提升稳定性和性能，尤其适用于客户端数据分布不一致的场景。

Result: 在图像和文本基准数据集上测试三种拜占庭攻击类型，ArKrum表现优于或等同于其他鲁棒聚合方法，实现了高精度和稳定性。

Conclusion: ArKrum是联邦学习系统在对抗性环境中安全运行的有效且实用的解决方案。

Abstract: Federated Learning (FL) enables collaborative machine learning across
decentralized data sources without sharing raw data. It offers a promising
approach to privacy-preserving AI. However, FL remains vulnerable to
adversarial threats from malicious participants, referred to as Byzantine
clients, who can send misleading updates to corrupt the global model.
Traditional aggregation methods, such as simple averaging, are not robust to
such attacks. More resilient approaches, like the Krum algorithm, require prior
knowledge of the number of malicious clients, which is often unavailable in
real-world scenarios. To address these limitations, we propose Average-rKrum
(ArKrum), a novel aggregation strategy designed to enhance both the resilience
and privacy guarantees of FL systems. Building on our previous work (rKrum),
ArKrum introduces two key innovations. First, it includes a median-based
filtering mechanism that removes extreme outliers before estimating the number
of adversarial clients. Second, it applies a multi-update averaging scheme to
improve stability and performance, particularly when client data distributions
are not identical. We evaluate ArKrum on benchmark image and text datasets
under three widely studied Byzantine attack types. Results show that ArKrum
consistently achieves high accuracy and stability. It performs as well as or
better than other robust aggregation methods. These findings demonstrate that
ArKrum is an effective and practical solution for secure FL systems in
adversarial environments.

</details>


### [190] [Automated Capability Evaluation of Foundation Models](https://arxiv.org/abs/2505.17228)
*Arash Afkanpour,Omkar Dige,Fatemeh Tavakoli*

Key words: foundation models, evaluation framework, active learning, capability assessment

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了ACE框架，通过主动学习和语言模型知识实现自动化、细粒度的基础模型评估，减少人工干预并全面发现模型能力。

Motivation: 传统评估方法依赖固定的人工基准，无法全面反映模型能力，ACE旨在解决这一问题。

Method: ACE利用语言模型分解领域能力，生成多样任务，并通过主动学习优先评估最信息化的能力。

Result: ACE能更全面、高效地揭示模型优缺点，优于静态基准。

Conclusion: ACE为模型安全部署提供了更全面的能力评估方法。

Abstract: Current evaluation frameworks for foundation models rely heavily on fixed,
manually curated benchmarks, limiting their ability to capture the full breadth
of model capabilities. This paper introduces Active learning for Capability
Evaluation (ACE), a novel framework for scalable, automated, and fine-grained
evaluation of foundation models. ACE leverages the knowledge embedded in
powerful language models to decompose a domain into semantically meaningful
capabilities and generate diverse evaluation tasks, significantly reducing
human effort. To maximize coverage and efficiency, ACE models a subject model's
performance as a capability function over a latent semantic space and uses
active learning to prioritize the evaluation of the most informative
capabilities. This adaptive evaluation strategy enables cost-effective
discovery of strengths, weaknesses, and failure modes that static benchmarks
may miss. Our results suggest that ACE provides a more complete and informative
picture of model capabilities, which is essential for safe and well-informed
deployment of foundation models.

</details>


### [191] [Semantic-Aware Interpretable Multimodal Music Auto-Tagging](https://arxiv.org/abs/2505.17233)
*Andreas Patakis,Vassilis Lyberatos,Spyridon Kantarelis,Edmund Dervakos,Giorgos Stamou*

Key words: 音乐自动标注、可解释性、多模态特征、期望最大化、语义聚类

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种可解释的音乐自动标注框架，结合多模态特征和期望最大化算法，在保持竞争力的同时提高透明度。

Motivation: 现有的音乐自动标注基础模型虽然性能优越，但缺乏可解释性，限制了研究者和终端用户的信任和使用体验。

Method: 利用信号处理、深度学习、本体工程和自然语言处理的多模态特征，通过语义聚类和期望最大化算法为特征组分配权重。

Result: 方法在标注性能上具有竞争力，同时提供了决策过程的深入理解。

Conclusion: 该框架为更透明、以用户为中心的音乐标注系统奠定了基础。

Abstract: Music auto-tagging is essential for organizing and discovering music in
extensive digital libraries. While foundation models achieve exceptional
performance in this domain, their outputs often lack interpretability, limiting
trust and usability for researchers and end-users alike. In this work, we
present an interpretable framework for music auto-tagging that leverages groups
of musically meaningful multimodal features, derived from signal processing,
deep learning, ontology engineering, and natural language processing. To
enhance interpretability, we cluster features semantically and employ an
expectation maximization algorithm, assigning distinct weights to each group
based on its contribution to the tagging process. Our method achieves
competitive tagging performance while offering a deeper understanding of the
decision-making process, paving the way for more transparent and user-centric
music tagging systems.

</details>


### [192] [Optimal Policy Minimum Bayesian Risk](https://arxiv.org/abs/2505.17242)
*Ramón Fernandez Astudillo,Md Arafat Sultan,Aashka Trivedi,Yousef El-Kurdi,Tahira Naseem,Radu Florian,Salim Roukos*

Key words: 推理扩展、KL控制强化学习、MBRD、LLM、数学推理、编程任务

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了基于KL控制强化学习的新方法，优化推理时信号利用，提升LLM在复杂推理任务中的性能和稳健性。

Motivation: 传统推理方法依赖于生成多个候选解并聚合，但信号利用效率不高。本文旨在通过KL控制强化学习框架，更高效地整合奖励和相似性信号。

Method: 结合KL控制强化学习理论，设计了一种新的最小贝叶斯风险解码（MBRD）方法，动态调整生成样本数量以适应问题难度。

Result: 在数学（MATH-$500）和编程（HumanEval）任务上验证了方法的有效性，表现出更高的准确性和计算效率。

Conclusion: 该方法显著提升了推理时信号利用效率，提供了更好的准确性与计算资源权衡。

Abstract: Inference scaling can help LLMs solve complex reasoning problems through
extended runtime computation. On top of targeted supervision for long
chain-of-thought (long-CoT) generation, purely inference-time techniques such
as best-of-N (BoN) sampling, majority voting, or more generally, minimum Bayes
risk decoding (MBRD), can further improve LLM accuracy by generating multiple
candidate solutions and aggregating over them. These methods typically leverage
additional signals in the form of reward models and risk/similarity functions
that compare generated samples, e.g., exact match in some normalized space or
standard similarity metrics such as Rouge. Here we present a novel method for
incorporating reward and risk/similarity signals into MBRD. Based on the
concept of optimal policy in KL-controlled reinforcement learning, our
framework provides a simple and well-defined mechanism for leveraging such
signals, offering several advantages over traditional inference-time methods:
higher robustness, improved accuracy, and well-understood asymptotic behavior.
In addition, it allows for the development of a sample-efficient variant of
MBRD that can adjust the number of samples to generate according to the
difficulty of the problem, without relying on majority vote counts. We
empirically demonstrate the advantages of our approach on math (MATH-$500$) and
coding (HumanEval) tasks using recent open-source models. We also present a
comprehensive analysis of its accuracy-compute trade-offs.

</details>


### [193] [Backdoors in DRL: Four Environments Focusing on In-distribution Triggers](https://arxiv.org/abs/2505.17248)
*Chace Ashcraft,Ted Staley,Josh Carney,Cameron Hickert,Derek Juba,Kiran Karra,Nathan Drenkow*

Key words: 后门攻击, 深度强化学习, 分布内触发器, 数据投毒, 安全威胁

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了深度强化学习（DRL）中的后门攻击，重点研究了分布内触发器，提出了四种RL环境的攻击实现，并验证了其可行性。

Motivation: 开源神经网络模型可能包含后门，存在安全风险，因此研究后门攻击的防御方法至关重要。

Method: 在四种RL环境中实现后门攻击，训练干净和带后门的模型，对比分析分布内触发器的效果。

Result: 分布内触发器实现更具挑战性，但仍能通过基本的数据投毒攻击在DRL中构成威胁。

Conclusion: 分布内触发器是DRL中的安全隐患，即使简单的攻击方法也足以造成威胁。

Abstract: Backdoor attacks, or trojans, pose a security risk by concealing undesirable
behavior in deep neural network models. Open-source neural networks are
downloaded from the internet daily, possibly containing backdoors, and
third-party model developers are common. To advance research on backdoor attack
mitigation, we develop several trojans for deep reinforcement learning (DRL)
agents. We focus on in-distribution triggers, which occur within the agent's
natural data distribution, since they pose a more significant security threat
than out-of-distribution triggers due to their ease of activation by the
attacker during model deployment. We implement backdoor attacks in four
reinforcement learning (RL) environments: LavaWorld, Randomized LavaWorld,
Colorful Memory, and Modified Safety Gymnasium. We train various models, both
clean and backdoored, to characterize these attacks. We find that
in-distribution triggers can require additional effort to implement and be more
challenging for models to learn, but are nevertheless viable threats in DRL
even using basic data poisoning attacks.

</details>


### [194] [Approach to Finding a Robust Deep Learning Model](https://arxiv.org/abs/2505.17254)
*Alexey Boldyrev,Fedor Ratnikov,Andrey Shevelev*

Key words: 机器学习, 模型鲁棒性, 算法, 深度学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出一种用于确定机器学习模型鲁棒性的新方法，并结合一个模型选择元算法，适用于任何机器学习模型。

Motivation: 随着机器学习和人工智能应用的快速发展，需要无监督训练大量模型并确保其预测可靠性。

Method: 提出了一种新的鲁棒性评估方法，并结合模型选择元算法，应用于深度学习模型。

Result: 研究发现，训练样本大小、模型权重初始化和归纳偏差对深度学习模型的鲁棒性有显著影响。

Conclusion: 该方法可广泛用于评估模型鲁棒性，尤其适用于小规模深度学习模型。

Abstract: The rapid development of machine learning (ML) and artificial intelligence
(AI) applications requires the training of large numbers of models. This
growing demand highlights the importance of training models without human
supervision, while ensuring that their predictions are reliable. In response to
this need, we propose a novel approach for determining model robustness. This
approach, supplemented with a proposed model selection algorithm designed as a
meta-algorithm, is versatile and applicable to any machine learning model,
provided that it is appropriate for the task at hand. This study demonstrates
the application of our approach to evaluate the robustness of deep learning
models. To this end, we study small models composed of a few convolutional and
fully connected layers, using common optimizers due to their ease of
interpretation and computational efficiency. Within this framework, we address
the influence of training sample size, model weight initialization, and
inductive bias on the robustness of deep learning models.

</details>


### [195] [JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model](https://arxiv.org/abs/2505.17257)
*Qihao Duan,Bingding Huang,Zhenqiao Song,Irina Lehmann,Lei Gu,Roland Eils,Benjamin Wild*

Key words: JanusDNA, LLM, 基因组学, Mamba, MoE

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: JanusDNA是一种结合自回归建模和掩码建模的双向DNA基础模型，通过混合Mamba、Attention和MoE架构，实现了长序列建模和高效计算，在多个基因组基准测试中达到最佳性能。

Motivation: 传统LLM在基因组学应用中面临长距离依赖建模和双向理解不足的挑战，JanusDNA旨在解决这些问题。

Method: 采用混合Mamba、Attention和MoE架构，结合自回归与掩码建模的预训练范式。

Result: 在单GPU上处理100万碱基对，并在三个基因组基准测试中超越参数量更大的模型。

Conclusion: JanusDNA为基因组学提供了高效、双向理解的解决方案。

Abstract: Large language models (LLMs) have revolutionized natural language processing
and are increasingly applied to other sequential data types, including genetic
sequences. However, adapting LLMs to genomics presents significant challenges.
Capturing complex genomic interactions requires modeling long-range
dependencies within DNA sequences, where interactions often span over 10,000
base pairs, even within a single gene, posing substantial computational burdens
under conventional model architectures and training paradigms. Moreover,
standard LLM training approaches are suboptimal for DNA: autoregressive
training, while efficient, supports only unidirectional understanding. However,
DNA is inherently bidirectional, e.g., bidirectional promoters regulate
transcription in both directions and account for nearly 11% of human gene
expression. Masked language models (MLMs) allow bidirectional understanding but
are inefficient, as only masked tokens contribute to the loss per step. To
address these limitations, we introduce JanusDNA, the first bidirectional DNA
foundation model built upon a novel pretraining paradigm that combines the
optimization efficiency of autoregressive modeling with the bidirectional
comprehension of masked modeling. JanusDNA adopts a hybrid Mamba, Attention and
Mixture of Experts (MoE) architecture, combining long-range modeling of
Attention with efficient sequential learning of Mamba. MoE layers further scale
model capacity via sparse activation while keeping computational cost low.
Notably, JanusDNA processes up to 1 million base pairs at single nucleotide
resolution on a single 80GB GPU. Extensive experiments and ablations show
JanusDNA achieves new SOTA results on three genomic representation benchmarks,
outperforming models with 250x more activated parameters. Code:
https://github.com/Qihao-Duan/JanusDNA

</details>


### [196] [Zebra-Llama: Towards Extremely Efficient Hybrid Models](https://arxiv.org/abs/2505.17272)
*Mingyu Yang,Mehdi Rezagholizadeh,Guihong Li,Vikram Appia,Emad Barsoum*

Key words: 大型语言模型，推理效率，SSM，MLA，混合模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 为提高大型语言模型（LLM）的推理效率，研究者提出Zebra-Llama，结合SSM和MLA层构建混合模型，显著降低训练成本和KV缓存，同时保持高性能。

Motivation: 当前重新训练LLM以适应特定需求成本高昂且不环保，需更高效、可持续的替代方案。

Method: 提出Zebra-Llama，结合SSM和MLA层，通过优化初始化和后训练流程从预训练Transformer中迁移知识。

Result: Zebra-Llama以极少训练token（7-11B）和8B教师模型达到Transformer精度，KV缓存降至原3.9%-2.73%，保持97%-100%零样本性能。

Conclusion: Zebra-Llama在效率、资源占用和性能上优于其他模型，如MambaInLLaMA，展示了混合模型的潜力。

Abstract: With the growing demand for deploying large language models (LLMs) across
diverse applications, improving their inference efficiency is crucial for
sustainable and democratized access. However, retraining LLMs to meet new
user-specific requirements is prohibitively expensive and environmentally
unsustainable. In this work, we propose a practical and scalable alternative:
composing efficient hybrid language models from existing pre-trained models.
Our approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models
by combining State Space Models (SSMs) and Multi-head Latent Attention (MLA)
layers, using a refined initialization and post-training pipeline to
efficiently transfer knowledge from pre-trained Transformers. Zebra-Llama
achieves Transformer-level accuracy with near-SSM efficiency using only 7-11B
training tokens (compared to trillions of tokens required for pre-training) and
an 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down
to 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants,
respectively-while preserving 100%, 100%, and >97% of average zero-shot
performance on LM Harness tasks. Compared to models like MambaInLLaMA,
X-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive
or superior accuracy while using significantly fewer tokens, smaller teachers,
and vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses
Minitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens,
over 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves
2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context
length. We will release code and model checkpoints upon acceptance.

</details>


### [197] [Comparator-Adaptive $Φ$-Regret: Improved Bounds, Simpler Algorithms, and Applications to Games](https://arxiv.org/abs/2505.17277)
*Soumita Hait,Ping Li,Haipeng Luo,Mengxiao Zhang*

Key words: Φ-regret，先验分布，适应性算法，博弈论，均衡收敛

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种更简单的方法来改进Φ-regret的边界，通过基于先验分布的算法，相比之前的Lu等人工作，提高了适应性并简化了算法复杂度。

Motivation: 经典专家问题中，Φ-regret衡量学习者损失与最佳动作变换的差距。Lu等人的研究虽然提供了适应性算法，但复杂且边界仍有改进空间。本文旨在通过更简单的算法实现更好的适应性Φ-regret边界。

Method: 提出了两种具体算法：一种是基于Kernelized MWU算法的先验感知变体，另一种是基于BM-reduction的先验感知变体。两种方法均通过学习多个先验分布副本来实现。

Result: 论文展示了这些方法在Φ-regret边界上的优越性，特别是在博弈设置中，实现了对Φ-均衡的加速和适应性收敛。对于相关均衡这一特例，边界优于现有研究。

Conclusion: 本文通过简单且高效的算法，不仅改进了Φ-regret的边界，还展示了在博弈论中的应用潜力，特别是在均衡收敛方面的优势。

Abstract: In the classic expert problem, $\Phi$-regret measures the gap between the
learner's total loss and that achieved by applying the best action
transformation $\phi \in \Phi$. A recent work by Lu et al., [2025] introduces
an adaptive algorithm whose regret against a comparator $\phi$ depends on a
certain sparsity-based complexity measure of $\phi$, (almost) recovering and
interpolating optimal bounds for standard regret notions such as external,
internal, and swap regret. In this work, we propose a general idea to achieve
an even better comparator-adaptive $\Phi$-regret bound via much simpler
algorithms compared to Lu et al., [2025]. Specifically, we discover a prior
distribution over all possible binary transformations and show that it suffices
to achieve prior-dependent regret against these transformations. Then, we
propose two concrete and efficient algorithms to achieve so, where the first
one learns over multiple copies of a prior-aware variant of the Kernelized MWU
algorithm of Farina et al., [2022], and the second one learns over multiple
copies of a prior-aware variant of the BM-reduction [Blum and Mansour, 2007].
To further showcase the power of our methods and the advantages over Lu et al.,
[2025] besides the simplicity and better regret bounds, we also show that our
second approach can be extended to the game setting to achieve accelerated and
adaptive convergence rate to $\Phi$-equilibria for a class of general-sum
games. When specified to the special case of correlated equilibria, our bound
improves over the existing ones from Anagnostides et al., [2022a,b]

</details>


### [198] [Attention with Trained Embeddings Provably Selects Important Tokens](https://arxiv.org/abs/2505.17282)
*Diyuan Wu,Aleksandr Shevchenko,Samet Oymak,Marco Mondelli*

Key words: 词嵌入, 软注意力, 梯度训练, 二元分类, 语言模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究了软注意力机制训练过程中词嵌入结构的动态变化，揭示了其与数据集中词频及标签预测的关系。

Motivation: 尽管词嵌入在实际中广泛应用，但其理论基础不足，本文旨在填补这一空白，特别关注梯度下降训练过程中的词嵌入结构。

Method: 采用单层软注意力模型和线性分类头进行二元分类，分析梯度训练后词嵌入如何与输出向量对齐及软注意力如何选择关键性词语。

Result: 实验证实了理论推测，即词嵌入与输出向量对齐程度与其词频成正比，且注意力机制能有效选择预测性词语。

Conclusion: 研究表明梯度训练能有效优化词嵌入结构，使注意力机制聚焦于标签预测的关键性词语。

Abstract: Token embeddings play a crucial role in language modeling but, despite this
practical relevance, their theoretical understanding remains limited. Our paper
addresses the gap by characterizing the structure of embeddings obtained via
gradient descent. Specifically, we consider a one-layer softmax attention model
with a linear head for binary classification, i.e., $\texttt{Softmax}( p^\top
E_X^\top ) E_X v = \frac{ \sum_{i=1}^T \exp(p^\top E_{x_i}) E_{x_i}^\top
v}{\sum_{j=1}^T \exp(p^\top E_{x_{j}}) }$, where $E_X = [ E_{x_1} , \dots,
E_{x_T} ]^\top$ contains the embeddings of the input sequence, $p$ is the
embedding of the $\mathrm{\langle cls \rangle}$ token and $v$ the output
vector. First, we show that, already after a single step of gradient training
with the logistic loss, the embeddings $E_X$ capture the importance of tokens
in the dataset by aligning with the output vector $v$ proportionally to the
frequency with which the corresponding tokens appear in the dataset. Then,
after training $p$ via gradient flow until convergence, the softmax selects the
important tokens in the sentence (i.e., those that are predictive of the
label), and the resulting $\mathrm{\langle cls \rangle}$ embedding maximizes
the margin for such a selection. Experiments on real-world datasets (IMDB,
Yelp) exhibit a phenomenology close to that unveiled by our theory.

</details>


### [199] [Model-Free Graph Data Selection under Distribution Shift](https://arxiv.org/abs/2505.17293)
*Ting-Wei Li,Ruizhong Qiu,Hanghang Tong*

Key words: 图域自适应, 最优传输, 数据选择, 分布偏移, 图神经网络

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出GRADATE框架，一种不依赖模型的方法，通过最优传输理论选择源域中的最佳训练数据，以应对图域自适应中的分布偏移问题，并在多个实际数据集上验证其高效性和可扩展性。

Motivation: 现有的模型中心化方法在图域自适应（GDA）中对严重分布偏移和有限计算资源的适应性不足，因此需要一种不依赖模型的高效数据选择框架。

Method: GRADATE框架利用最优传输理论，直接选择源域中对目标域分类任务最有帮助的训练样本，不依赖任何GNN模型的预测或训练过程。

Result: GRADATE在多种实际图数据集和协变量偏移类型中表现优异，不仅优于现有数据选择方法，还能显著提升现有GDA方法的性能，同时减少训练数据需求。

Conclusion: GRADATE作为一种数据高效的模型无关框架，能够有效补充现有GDA方法，特别适用于资源受限的场景和严重分布偏移问题。

Abstract: Graph domain adaptation (GDA) is a fundamental task in graph machine
learning, with techniques like shift-robust graph neural networks (GNNs) and
specialized training procedures to tackle the distribution shift problem.
Although these model-centric approaches show promising results, they often
struggle with severe shifts and constrained computational resources. To address
these challenges, we propose a novel model-free framework, GRADATE (GRAph DATa
sElector), that selects the best training data from the source domain for the
classification task on the target domain. GRADATE picks training samples
without relying on any GNN model's predictions or training recipes, leveraging
optimal transport theory to capture and adapt to distribution changes. GRADATE
is data-efficient, scalable and meanwhile complements existing model-centric
GDA approaches. Through comprehensive empirical studies on several real-world
graph-level datasets and multiple covariate shift types, we demonstrate that
GRADATE outperforms existing selection methods and enhances off-the-shelf GDA
methods with much fewer training data.

</details>


### [200] [Implicit Regularization of Infinitesimally-perturbed Gradient Descent Toward Low-dimensional Solutions](https://arxiv.org/abs/2505.17304)
*Jianhao Ma,Geyu Liang,Salar Fattahi*

Key words: 隐式正则化, 梯度下降, 严格鞍点, 低维解, 矩阵感知

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文研究了梯度下降法在非凸函数中如何通过微小扰动实现隐式正则化，同时保持对隐式低维区域的接近性。

Motivation: 隐式正则化现象在局部搜索算法中广泛存在，但其理论机制尚不明确，尤其是在高维问题中。

Method: 提出了“微小扰动梯度下降（IPGD）”，通过控制扰动大小实现高效逃离严格鞍点且不偏离隐式区域。

Result: IPGD在矩阵感知问题中展示出隐式正则化的理论保证，并通过实验扩展到更广泛的机器学习问题。

Conclusion: IPGD能够同时满足高效逃离鞍点和保持隐式区域的条件，为隐式正则化提供了新视角。

Abstract: Implicit regularization refers to the phenomenon where local search
algorithms converge to low-dimensional solutions, even when such structures are
neither explicitly specified nor encoded in the optimization problem. While
widely observed, this phenomenon remains theoretically underexplored,
particularly in modern over-parameterized problems. In this paper, we study the
conditions that enable implicit regularization by investigating when
gradient-based methods converge to second-order stationary points (SOSPs)
within an implicit low-dimensional region of a smooth, possibly nonconvex
function. We show that successful implicit regularization hinges on two key
conditions: $(i)$ the ability to efficiently escape strict saddle points, while
$(ii)$ maintaining proximity to the implicit region. Existing analyses enabling
the convergence of gradient descent (GD) to SOSPs often rely on injecting large
perturbations to escape strict saddle points. However, this comes at the cost
of deviating from the implicit region. The central premise of this paper is
that it is possible to achieve the best of both worlds: efficiently escaping
strict saddle points using infinitesimal perturbations, while controlling
deviation from the implicit region via a small deviation rate. We show that
infinitesimally perturbed gradient descent (IPGD), which can be interpreted as
GD with inherent ``round-off errors'', can provably satisfy both conditions. We
apply our framework to the problem of over-parameterized matrix sensing, where
we establish formal guarantees for the implicit regularization behavior of
IPGD. We further demonstrate through extensive experiments that these insights
extend to a broader class of learning problems.

</details>


### [201] [Wavelet Probabilistic Recurrent Convolutional Network for Multivariate Time Series Classification](https://arxiv.org/abs/2505.17307)
*Pu Yang,J. A. Barria*

Key words: WPRCN, MTSC, 小波概率, 非平稳性, 数据稀缺

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种用于多元时间序列分类（MTSC）的小波概率递归卷积网络（WPRCN），特别适用于非平稳环境、数据稀缺和噪声干扰的场景。

Motivation: 现有的MTSC方法在面临数据稀缺、非平稳性和噪声干扰时表现不佳，因此需要一种更鲁棒的方法来应对这些挑战。

Method: 提出了一个多功能的小波概率模块，包括自适应小波概率特征生成器（AWPG）和基于通道注意力的概率时间卷积网络（APTCN），并将其与LSTM和C-FCN结合使用。

Result: 在30个不同的MTS数据集上评估，WPRCN的平均准确率和排名优于所有基准算法，尤其在处理稀缺数据和生理数据时表现突出。

Conclusion: WPRCN通过结合小波概率模块和深度神经网络，显著提高了MTSC在复杂环境中的性能，具有广泛适用性。

Abstract: This paper presents a Wavelet Probabilistic Recurrent Convolutional Network
(WPRCN) for Multivariate Time Series Classification (MTSC), especially
effective in handling non-stationary environments, data scarcity and noise
perturbations. We introduce a versatile wavelet probabilistic module designed
to extract and analyse the probabilistic features, which can seamlessly
integrate with a variety of neural network architectures. This probabilistic
module comprises an Adaptive Wavelet Probabilistic Feature Generator (AWPG) and
a Channel Attention-based Probabilistic Temporal Convolutional Network (APTCN).
Such formulation extends the application of wavelet probabilistic neural
networks to deep neural networks for MTSC. The AWPG constructs an ensemble
probabilistic model addressing different data scarcities and non-stationarity;
it adaptively selects the optimal ones and generates probabilistic features for
APTCN. The APTCN analyses the correlations of the features and forms a
comprehensive feature space with existing MTSC models for classification. Here,
we instantiate the proposed module to work in parallel with a Long Short-Term
Memory (LSTM) network and a Causal Fully Convolutional Network (C-FCN),
demonstrating its broad applicability in time series analysis. The WPRCN is
evaluated on 30 diverse MTS datasets and outperforms all the benchmark
algorithms on average accuracy and rank, exhibiting pronounced strength in
handling scarce data and physiological data subject to perturbations and
non-stationarities.

</details>


### [202] [ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training](https://arxiv.org/abs/2505.17331)
*Maryam Dialameh,Rezaul Karim,Hossein Rajabzadeh,Omar Mohamed Awad,Hyock Ju Kwon,Boxing Chen,Walid Ahmed,Yang Liu*

Key words: LLaMA, 语言模型, KV 缓存, 训练效率, 推理吞吐量

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: ECHO-LLaMA 是一种高效的 LLaMA 架构改进方案，通过共享 KV 缓存提升训练和推理效率，同时保持语言模型性能。

Motivation: 改进 LLaMA 架构的训练速度和推理吞吐量，同时维持其学习能力，提供更高效且经济的预训练和微调方案。

Method: 通过将 LLaMA 模型的某些层的 KV 缓存共享化，显著降低 KV 计算复杂度，同时优化性能。

Result: 实验显示，ECHO-LLaMA 在训练时吞吐量提升最高 77%，MFU 提高 16%，损失降低 14%；1.1B 模型推理吞吐量提升约 7%。

Conclusion: ECHO-LLaMA 是一种高效、可扩展且经济的 LLM 改进方案，在提升效率的同时不牺牲性能。

Abstract: This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to
improve both the training speed and inference throughput of LLaMA architectures
while maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models
into shared KV caching across certain layers, significantly reducing KV
computational complexity while maintaining or improving language performance.
Experimental results demonstrate that ECHO-LLaMA achieves up to 77\% higher
token-per-second throughput during training, up to 16\% higher Model FLOPs
Utilization (MFU), and up to 14\% lower loss when trained on an equal number of
tokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\%
higher test-time throughput compared to the baseline. By introducing a
computationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable
and cost-effective solution for pretraining and finetuning large language
models, enabling faster and more resource-efficient training without
compromising performance.

</details>


### [203] [Conformal Predictive Distributions for Order Fulfillment Time Forecasting](https://arxiv.org/abs/2505.17340)
*Tinghan Ye,Amira Hijazi,Pascal Van Hentenryck*

Key words: 订单履行时间, 分布预测, 符合预测系统, 机器学习, 电商物流

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究提出了一种用于电商物流订单履行时间分布预测的新框架，结合机器学习方法显著提升了预测准确性。

Motivation: 传统基于规则的方法难以捕捉交付操作中的不确定性，因此需要更准确的预测模型来优化物流效率。

Method: 利用符合预测系统和交叉 Venn-Abers 预测器等模型无关技术，整合细粒度时空特征，并开发成本敏感的决策规则。

Result: 实验表明，该方法在工业数据集上实现了高达14%的预测准确率提升和75%的延迟交付识别改进。

Conclusion: 该框架为订单履行时间预测提供了可靠且高效的解决方案，显著优于传统方法。

Abstract: Accurate estimation of order fulfillment time is critical for e-commerce
logistics, yet traditional rule-based approaches often fail to capture the
inherent uncertainties in delivery operations. This paper introduces a novel
framework for distributional forecasting of order fulfillment time, leveraging
Conformal Predictive Systems and Cross Venn-Abers Predictors--model-agnostic
techniques that provide rigorous coverage or validity guarantees. The proposed
machine learning methods integrate granular spatiotemporal features, capturing
fulfillment location and carrier performance dynamics to enhance predictive
accuracy. Additionally, a cost-sensitive decision rule is developed to convert
probabilistic forecasts into reliable point predictions. Experimental
evaluation on a large-scale industrial dataset demonstrates that the proposed
methods generate competitive distributional forecasts, while machine
learning-based point predictions significantly outperform the existing
rule-based system--achieving up to 14% higher prediction accuracy and up to 75%
improvement in identifying late deliveries.

</details>


### [204] [TI-DeepONet: Learnable Time Integration for Stable Long-Term Extrapolation](https://arxiv.org/abs/2505.17341)
*Dibyajyoti Nayak,Somdatta Goswami*

Key words: TI-DeepONet, TI(L)-DeepONet, 时间外推, 数值积分, 动态系统

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: TI-DeepONet和TI(L)-DeepONet通过结合神经算子与自适应数值时间步进技术，显著提升了动态系统的时间外推预测精度，减少了误差传播，并在长时预测中保持稳定性。

Motivation: 传统的DeepONet在时间外推预测中存在固定时间步长和自回归方法的局限性，如忽略时间因果性或误差积累。为了解决这些问题，研究提出了结合数值时间步进技术的改进框架。

Method: 提出了TI-DeepONet和TI(L)-DeepONet，前者通过近似瞬时时间导数场并利用数值积分方案进行预测，后者进一步引入可学习的中间斜率系数以增强适应性。

Result: 在三个经典PDE上的实验显示，TI(L)-DeepONet略优于TI-DeepONet，两者相比自回归和固定时间步长方法分别减少约81%和70%的L2外推误差，并能稳定预测到训练区间两倍的时间范围。

Conclusion: 该研究提出了一种物理感知的算子学习范式，将神经近似与数值分析结合，同时保持了动态系统的因果结构。

Abstract: Accurate temporal extrapolation presents a fundamental challenge for neural
operators in modeling dynamical systems, where reliable predictions must extend
significantly beyond the training time horizon. Conventional Deep Operator
Network (DeepONet) approaches employ two inherently limited training paradigms
- fixed-horizon rollouts that predict complete spatiotemporal solutions while
disregarding temporal causality, and autoregressive formulations that
accumulate errors through sequential predictions. We introduce TI-DeepONet, a
framework that integrates neural operators with adaptive numerical
time-stepping techniques to preserve the Markovian structure of dynamical
systems while mitigating error propagation in extended temporal forecasting.
Our approach reformulates the learning objective from direct state prediction
to the approximation of instantaneous time-derivative fields, which are then
integrated using established numerical schemes. This architecture supports
continuous-time prediction and enables deployment of higher-precision
integrators during inference than those used during training, balancing
computational efficiency with predictive accuracy. We further develop
TI(L)-DeepONet, which incorporates learnable coefficients for intermediate
slopes in the integration process, adapting to solution-specific variations and
enhancing fidelity. Evaluation across three canonical PDEs shows that
TI(L)-DeepONet marginally outperforms TI-DeepONet, with both reducing relative
L2 extrapolation errors: approximately 81% over autoregressive and 70% over
fixed-horizon methods. Notably, both maintain prediction stability for temporal
domains extending to about twice the training interval. This research
establishes a physics-aware operator learning paradigm that bridges neural
approximation with numerical analysis while preserving the causal structure of
dynamical systems.

</details>


### [205] [A Survey of Safe Reinforcement Learning and Constrained MDPs: A Technical Survey on Single-Agent and Multi-Agent Safety](https://arxiv.org/abs/2505.17342)
*Ankita Kushwaha,Kiran Ravish,Preeti Lamba,Pawan Kumar*

Key words: 安全强化学习, SafeRL, 约束MDP, SafeMARL, 多智能体

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文综述了安全强化学习（SafeRL）和其多智能体扩展（SafeMARL），涵盖理论基础（如CMDP）、现有算法及开放研究问题。

Motivation: 推动安全强化学习领域发展，尤其在多智能体环境下，解决安全约束问题。

Method: 基于约束马尔可夫决策过程（CMDP），分析单智能体与多智能体的SafeRL算法和安全探索策略。

Result: 总结了SafeRL的理论基础与前沿方法，并提出5个开放研究问题（其中3个针对SafeMARL）。

Conclusion: 该综述为SafeRL和SafeMARL研究人员提供了技术指南，突出了核心概念、方法和未来方向。

Abstract: Safe Reinforcement Learning (SafeRL) is the subfield of reinforcement
learning that explicitly deals with safety constraints during the learning and
deployment of agents. This survey provides a mathematically rigorous overview
of SafeRL formulations based on Constrained Markov Decision Processes (CMDPs)
and extensions to Multi-Agent Safe RL (SafeMARL). We review theoretical
foundations of CMDPs, covering definitions, constrained optimization
techniques, and fundamental theorems. We then summarize state-of-the-art
algorithms in SafeRL for single agents, including policy gradient methods with
safety guarantees and safe exploration strategies, as well as recent advances
in SafeMARL for cooperative and competitive settings. Additionally, we propose
five open research problems to advance the field, with three focusing on
SafeMARL. Each problem is described with motivation, key challenges, and
related prior work. This survey is intended as a technical guide for
researchers interested in SafeRL and SafeMARL, highlighting key concepts,
methods, and open future research directions.

</details>


### [206] [A Multi-Head Attention Soft Random Forest for Interpretable Patient No-Show Prediction](https://arxiv.org/abs/2505.17344)
*Ninda Nurseha Amalina,Kwadwo Boateng Ofori-Amanfo,Heungjo An*

Key words: 患者爽约, 随机森林, 注意力机制, 医疗资源优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种新的混合多注意力软随机森林（MHASRF）模型，用于预测患者爽约行为，其准确性和适应性优于传统模型。

Motivation: 患者爽约行为影响医疗资源分配和连续性护理，传统机器学习模型适应性不足。

Method: 结合注意力机制和软分裂的随机森林模型，动态分配注意力权重。

Result: 模型在准确率（93.56%）、精确率（93.67%）、召回率（93.56%）和F1分数（93.59%）上表现优异。

Conclusion: MHASRF是一种稳健、可解释的爽约预测方法，有助于优化医疗资源。

Abstract: Unattended scheduled appointments, defined as patient no-shows, adversely
affect both healthcare providers and patients' health, disrupting the
continuity of care, operational efficiency, and the efficient allocation of
medical resources. Accurate predictive modelling is needed to reduce the impact
of no-shows. Although machine learning methods, such as logistic regression,
random forest models, and decision trees, are widely used in predicting patient
no-shows, they often rely on hard decision splits and static feature
importance, limiting their adaptability to specific or complex patient
behaviors. To address this limitation, we propose a new hybrid Multi-Head
Attention Soft Random Forest (MHASRF) model that integrates attention
mechanisms into a random forest model using probabilistic soft splitting
instead of hard splitting. The MHASRF model assigns attention weights
differently across the trees, enabling attention on specific patient behaviors.
The model exhibited 93.56% accuracy, 93.67% precision, 93.56% recall, and a
93.59% F1 score, surpassing the performance of decision tree, logistic
regression, random forest, and naive Bayes models. Furthermore, MHASRF was able
to identify key predictors of patient no-shows using two levels of feature
importance (tree level and attention mechanism level), offering deeper insights
into patient no-show predictors. The proposed model is a robust, adaptable, and
interpretable method for predicting patient no-shows that will help healthcare
providers in optimizing resources.

</details>


### [207] [FLEX: A Backbone for Diffusion-Based Modeling of Spatio-temporal Physical Systems](https://arxiv.org/abs/2505.17351)
*N. Benjamin Erichson,Vinicius Mikuni,Dongwei Lyu,Yang Gao,Omri Azencot,Soon Hoe Lim,Michael W. Mahoney*

Key words: FLEX, 扩散模型, 时空物理系统, U-Net, Transformer, 残差空间

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: FLEX是一种基于扩散模型的时空物理系统生成架构，通过在残差空间操作降低扩散模型的方差，结合Transformer和U-Net的混合设计，实现高效预测和校准不确定性估计。

Motivation: 为提升时空物理系统的生成建模效率，降低扩散模型的训练方差，并增强模型对长距离依赖和局部细节的捕捉能力。

Method: FLEX在残差空间操作，结合U-Net和Transformer的混合设计，改进跳跃连接方案，使用任务特定编码器处理辅助输入，并通过弱条件和强条件优化模型。

Result: 在超分辨率和预测任务中表现出色，仅需少量反向扩散步骤即可实现高精度预测，且在分布外设置下具有良好泛化能力。

Conclusion: FLEX通过创新的架构设计和条件机制，在时空物理系统建模中实现了高效、准确的生成和泛化能力。

Abstract: We introduce FLEX (FLow EXpert), a backbone architecture for generative
modeling of spatio-temporal physical systems using diffusion models. FLEX
operates in the residual space rather than on raw data, a modeling choice that
we motivate theoretically, showing that it reduces the variance of the velocity
field in the diffusion model, which helps stabilize training. FLEX integrates a
latent Transformer into a U-Net with standard convolutional ResNet layers and
incorporates a redesigned skip connection scheme. This hybrid design enables
the model to capture both local spatial detail and long-range dependencies in
latent space. To improve spatio-temporal conditioning, FLEX uses a
task-specific encoder that processes auxiliary inputs such as coarse or past
snapshots. Weak conditioning is applied to the shared encoder via skip
connections to promote generalization, while strong conditioning is applied to
the decoder through both skip and bottleneck features to ensure reconstruction
fidelity. FLEX achieves accurate predictions for super-resolution and
forecasting tasks using as few as two reverse diffusion steps. It also produces
calibrated uncertainty estimates through sampling. Evaluations on
high-resolution 2D turbulence data show that FLEX outperforms strong baselines
and generalizes to out-of-distribution settings, including unseen Reynolds
numbers, physical observables (e.g., fluid flow velocity fields), and boundary
conditions.

</details>


### [208] [CT-OT Flow: Estimating Continuous-Time Dynamics from Discrete Temporal Snapshots](https://arxiv.org/abs/2505.17354)
*Keisuke Kawano,Takuro Kutsuna,Naoki Hayashi,Yasushi Esaki,Hidenori Tanaka*

Key words: 连续时间动态, 最优输运, 单细胞RNA测序, 时间核平滑, ODE/SDE

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为CT-OT Flow的方法，用于从离散时间快照中恢复连续时间动态，通过部分最优输运推断高分辨率时间标签，再用时间核平滑重建连续时间数据分布。

Motivation: 解决从噪声时间戳和有限时间间隔的离散时间快照中恢复连续时间动力学的挑战，特别是在单细胞RNA测序等实际场景中。

Method: 结合部分最优输运和时间核平滑，先推断高分辨率时间标签，再重建连续时间数据分布，以训练动态模型（如ODE和SDE）。

Result: CT-OT Flow在合成基准测试中优于现有方法，并在真实scRNA-seq和台风轨迹数据集上实现了更低的重建误差。

Conclusion: 通过显式建模时间离散化和时间戳不确定性，CT-OT Flow提供了一个准确且通用的框架，连接离散快照和连续时间过程。

Abstract: In many real-world scenarios, such as single-cell RNA sequencing, data are
observed only as discrete-time snapshots spanning finite time intervals and
subject to noisy timestamps, with no continuous trajectories available.
Recovering the underlying continuous-time dynamics from these snapshots with
coarse and noisy observation times is a critical and challenging task. We
propose Continuous-Time Optimal Transport Flow (CT-OT Flow), which first infers
high-resolution time labels via partial optimal transport and then reconstructs
a continuous-time data distribution through a temporal kernel smoothing. This
reconstruction enables accurate training of dynamics models such as ODEs and
SDEs. CT-OT Flow consistently outperforms state-of-the-art methods on synthetic
benchmarks and achieves lower reconstruction errors on real scRNA-seq and
typhoon-track datasets. Our results highlight the benefits of explicitly
modeling temporal discretization and timestamp uncertainty, offering an
accurate and general framework for bridging discrete snapshots and
continuous-time processes.

</details>


### [209] [Adversarial Robustness of Nonparametric Regression](https://arxiv.org/abs/2505.17356)
*Parsa Moradi,Hanzaleh Akabrinodehi,Mohammad Ali Maddah-Ali*

Key words: 非参数回归, 对抗鲁棒性, Sobolev空间, 平滑样条, 极小极大下界

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了非参数回归中的对抗鲁棒性，揭示了平滑样条估计器在对抗数据污染时的有效性及其极限。

Motivation: 尽管参数回归的鲁棒性已被广泛研究，但非参数回归中的对抗鲁棒性仍未被充分探索。论文旨在填补这一空白。

Method: 基于二阶Sobolev空间的假设，论文分析了平滑样条估计器的对抗鲁棒性，并提出了正则化方法。

Result: 当污染样本为$o(n)$时，平滑样条估计器的误差趋于零；但当污染比例为常数时，无估计器能保证误差收敛，表明平滑样条的最优性。

Conclusion: 平滑样条在对抗污染中表现最优，且其鲁棒性受污染样本比例限制。

Abstract: In this paper, we investigate the adversarial robustness of regression, a
fundamental problem in machine learning, under the setting where an adversary
can arbitrarily corrupt a subset of the input data. While the robustness of
parametric regression has been extensively studied, its nonparametric
counterpart remains largely unexplored. We characterize the adversarial
robustness in nonparametric regression, assuming the regression function
belongs to the second-order Sobolev space (i.e., it is square integrable up to
its second derivative).
  The contribution of this paper is two-fold: (i) we establish a minimax lower
bound on the estimation error, revealing a fundamental limit that no estimator
can overcome, and (ii) we show that, perhaps surprisingly, the classical
smoothing spline estimator, when properly regularized, exhibits robustness
against adversarial corruption. These results imply that if $o(n)$ out of $n$
samples are corrupted, the estimation error of the smoothing spline vanishes as
$n \to \infty$. On the other hand, when a constant fraction of the data is
corrupted, no estimator can guarantee vanishing estimation error, implying the
optimality of the smoothing spline in terms of maximum tolerable number of
corrupted samples.

</details>


### [210] [Graph Attention Neural Network for Botnet Detection: Evaluating Autoencoder, VAE and PCA-Based Dimension Reduction](https://arxiv.org/abs/2505.17357)
*Hassan Wasswa,Hussein Abbass,Timothy Lynar*

Key words: IoT僵尸网络, 图神经网络, 降维, 注意力机制, 攻击检测

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种框架，通过降维技术处理高维IoT攻击数据集，再转化为图数据集，结合图注意力网络（GAT）提升僵尸网络攻击检测性能。

Motivation: 现有模型忽略实例间关系且高维图数据结构计算成本高，需解决这些问题以提高检测准确性。

Method: 先用VAE-encoder、AE-encoder或PCA降维NetFlow数据集，再转化为图数据并输入GAT模型。

Result: 降维后图数据集有效提升了GAT模型的僵尸网络攻击检测性能。

Conclusion: 降维技术能优化图数据计算效率，GAT模型结合实例关系显著改善检测效果。

Abstract: With the rise of IoT-based botnet attacks, researchers have explored various
learning models for detection, including traditional machine learning, deep
learning, and hybrid approaches. A key advancement involves deploying attention
mechanisms to capture long-term dependencies among features, significantly
improving detection accuracy. However, most models treat attack instances
independently, overlooking inter-instance relationships. Graph Neural Networks
(GNNs) address this limitation by learning an embedding space via iterative
message passing where similar instances are placed closer based on node
features and relationships, enhancing classification performance. To further
improve detection, attention mechanisms have been embedded within GNNs,
leveraging both long-range dependencies and inter-instance connections.
However, transforming the high dimensional IoT attack datasets into a graph
structured dataset poses challenges, such as large graph structures leading
computational overhead. To mitigate this, this paper proposes a framework that
first reduces dimensionality of the NetFlow-based IoT attack dataset before
transforming it into a graph dataset. We evaluate three dimension reduction
techniques--Variational Autoencoder (VAE-encoder), classical autoencoder
(AE-encoder), and Principal Component Analysis (PCA)--and compare their effects
on a Graph Attention neural network (GAT) model for botnet attack detection

</details>


### [211] [Towards VM Rescheduling Optimization Through Deep Reinforcement Learning](https://arxiv.org/abs/2505.17359)
*Xianzhong Ding,Yunkai Zhang,Binbin Chen,Donghao Ying,Tieying Zhang,Jianjun Chen,Lei Zhang,Alberto Cerpa,Wan Du*

Key words: 虚拟机重新调度、强化学习、数据中心、资源碎片化、VM2RL

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的虚拟机重新调度系统VM2RL，解决数据中心因虚拟机状态动态变化导致的调度效率问题，实验证明其性能接近最优解且运行时间短。

Motivation: 现代数据中心需要管理大量虚拟机（VMs），频繁的创建和释放导致物理机（PMs）上资源碎片化，现有方法因调度时延较高而难以扩展。

Method: 提出VM2RL系统，采用强化学习方法，包括两阶段框架（适应不同约束和工作负载）、针对重新调度的特征提取模块和风险寻求评估机制。

Result: 在工业级数据中心数据上的实验表明，VM2RL性能接近最优解，且运行时间仅需几秒。

Conclusion: VM2RL通过定制化技术有效平衡调度延迟与准确性，为大规模数据中心提供高效解决方案。

Abstract: Modern industry-scale data centers need to manage a large number of virtual
machines (VMs). Due to the continual creation and release of VMs, many small
resource fragments are scattered across physical machines (PMs). To handle
these fragments, data centers periodically reschedule some VMs to alternative
PMs, a practice commonly referred to as VM rescheduling. Despite the increasing
importance of VM rescheduling as data centers grow in size, the problem remains
understudied. We first show that, unlike most combinatorial optimization tasks,
the inference time of VM rescheduling algorithms significantly influences their
performance, due to dynamic VM state changes during this period. This causes
existing methods to scale poorly. Therefore, we develop a reinforcement
learning system for VM rescheduling, VM2RL, which incorporates a set of
customized techniques, such as a two-stage framework that accommodates diverse
constraints and workload conditions, a feature extraction module that captures
relational information specific to rescheduling, as well as a risk-seeking
evaluation enabling users to optimize the trade-off between latency and
accuracy. We conduct extensive experiments with data from an industry-scale
data center. Our results show that VM2RL can achieve a performance comparable
to the optimal solution but with a running time of seconds. Code and datasets
are open-sourced: https://github.com/zhykoties/VMR2L_eurosys,
https://drive.google.com/drive/folders/1PfRo1cVwuhH30XhsE2Np3xqJn2GpX5qy.

</details>


### [212] [Improved and Oracle-Efficient Online $\ell_1$-Multicalibration](https://arxiv.org/abs/2505.17365)
*Rohan Ghuge,Vidya Muthukumar,Sahil Singla*

Key words: 在线多校准，$ℓ_1$范数，OLPO，oracle高效，无限组族

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种直接方法，通过将在线 $ℓ_1$-多校准转化为具有乘积奖励的在线学习问题（OLPO），实现了更优的$ℓ_1$-多校准率（$˜ℒ(T^{-1/3})$ 和 $˜ℒ(T^{-1/4})$）。

Motivation: 以往的多校准研究通过间接方法在其他范数（如$ℓ_2$和$ℓ_∞$）中获得结果，再转换到$ℓ_1$时会产生额外损失，本文旨在直接解决$ℓ_1$多校准问题以提高效率。

Method: 提出了一种新型的OLPO（在线线性乘积优化）问题，并通过线性化及高效算法设计实现改进率。针对大规模组族，提出了多项式次数调用优化oracle的高效方法。

Result: 实现了$˜ℒ(T^{-1/3})$（高效但计算昂贵）和$˜ℒ(T^{-1/4})$（oracle高效）两种$ℓ_1$-多校准率，适用于无限组族场景。

Conclusion: 通过OLPO框架的直接方法，显著提升了在线$ℓ_1$-多校准的效率与可扩展性，适用于复杂分组场景。

Abstract: We study \emph{online multicalibration}, a framework for ensuring calibrated
predictions across multiple groups in adversarial settings, across $T$ rounds.
Although online calibration is typically studied in the $\ell_1$ norm, prior
approaches to online multicalibration have taken the indirect approach of
obtaining rates in other norms (such as $\ell_2$ and $\ell_{\infty}$) and then
transferred these guarantees to $\ell_1$ at additional loss. In contrast, we
propose a direct method that achieves improved and oracle-efficient rates of
$\widetilde{\mathcal{O}}(T^{-1/3})$ and $\widetilde{\mathcal{O}}(T^{-1/4})$
respectively, for online $\ell_1$-multicalibration. Our key insight is a novel
reduction of online \(\ell_1\)-multicalibration to an online learning problem
with product-based rewards, which we refer to as \emph{online linear-product
optimization} ($\mathtt{OLPO}$).
  To obtain the improved rate of $\widetilde{\mathcal{O}}(T^{-1/3})$, we
introduce a linearization of $\mathtt{OLPO}$ and design a no-regret algorithm
for this linearized problem. Although this method guarantees the desired
sublinear rate (nearly matching the best rate for online calibration), it
becomes computationally expensive when the group family \(\mathcal{H}\) is
large or infinite, since it enumerates all possible groups. To address
scalability, we propose a second approach to $\mathtt{OLPO}$ that makes only a
polynomial number of calls to an offline optimization (\emph{multicalibration
evaluation}) oracle, resulting in \emph{oracle-efficient} online
\(\ell_1\)-multicalibration with a rate of $\widetilde{\mathcal{O}}(T^{-1/4})$.
Our framework also extends to certain infinite families of groups (e.g., all
linear functions on the context space) by exploiting a $1$-Lipschitz property
of the \(\ell_1\)-multicalibration error with respect to \(\mathcal{H}\).

</details>


### [213] [FRIREN: Beyond Trajectories -- A Spectral Lens on Time](https://arxiv.org/abs/2505.17370)
*Qilin Wang*

Key words: 长期时间序列预测，FRIREN，Wasserstein距离，谱分析，混沌系统

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了FRIREN模型，通过几何结构而非点预测提升长期时间序列预测性能，结合Wasserstein距离和谱分析，实现了高精度和可解释性。

Motivation: 现有长期时间序列预测模型假设数据具有点预测性，但实际动态系统（如混沌系统）更依赖几何结构。论文旨在设计一种动态无关的基础模型，以几何变化为核心。

Method: 采用Wasserstein-2距离捕捉几何变化，构建FRIREN模型。模型包含归一化流块将数据嵌入正态分布潜空间，生成W2最优路径（可分解为旋转、缩放等操作），并提供全局谱表示。

Result: 在Lorenz-63和Rossler系统中，FRIREN显著优于TimeMixer（如MSE从27.3降至11.4）；在ETT和Weather数据集上也具竞争力。预测保持有效至2.5个李亚普诺夫时间。

Conclusion: FRIREN通过结合生成流与谱分析，实现了高精度、可解释的长期预测，为LTSF模型设计设立了新标准。

Abstract: Long-term time-series forecasting (LTSF) models are often presented as
general-purpose solutions that can be applied across domains, implicitly
assuming that all data is pointwise predictable. Using chaotic systems such as
Lorenz-63 as a case study, we argue that geometric structure - not pointwise
prediction - is the right abstraction for a dynamic-agnostic foundational
model. Minimizing the Wasserstein-2 distance (W2), which captures geometric
changes, and providing a spectral view of dynamics are essential for
long-horizon forecasting. Our model, FRIREN (Flow-inspired Representations via
Interpretable Eigen-networks), implements an augmented normalizing-flow block
that embeds data into a normally distributed latent representation. It then
generates a W2-efficient optimal path that can be decomposed into rotation,
scaling, inverse rotation, and translation. This architecture yields locally
generated, geometry-preserving predictions that are independent of the
underlying dynamics, and a global spectral representation that functions as a
finite Koopman operator with a small modification. This enables practitioners
to identify which modes grow, decay, or oscillate, both locally and
system-wide. FRIREN achieves an MSE of 11.4, MAE of 1.6, and SWD of 0.96 on
Lorenz-63 in a 336-in, 336-out, dt=0.01 setting, surpassing TimeMixer (MSE
27.3, MAE 2.8, SWD 2.1). The model maintains effective prediction for 274 out
of 336 steps, approximately 2.5 Lyapunov times. On Rossler (96-in, 336-out),
FRIREN achieves an MSE of 0.0349, MAE of 0.0953, and SWD of 0.0170,
outperforming TimeMixer's MSE of 4.3988, MAE of 0.886, and SWD of 3.2065.
FRIREN is also competitive on standard LTSF datasets such as ETT and Weather.
By connecting modern generative flows with classical spectral analysis, FRIREN
makes long-term forecasting both accurate and interpretable, setting a new
benchmark for LTSF model design.

</details>


### [214] [An End-to-End Approach for Child Reading Assessment in the Xhosa Language](https://arxiv.org/abs/2505.17371)
*Sergio Chevtchenko,Nikhil Navas,Rafaella Vale,Franco Ubaudi,Sipumelele Lucwaba,Cally Ardington,Soheil Afshar,Mark Antoniou,Saeed Afshar*

Key words: .child literacy, AI, low-resource languages, automatic reading assessment, Xhosa

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究探讨了利用AI自动化阅读评估系统在低资源语言（如南非的科萨语）中提升儿童识字能力的可行性，并提出了一种新型数据集和三种模型的评估结果。

Motivation: 儿童识字能力对个人未来发展至关重要，低收入和中等收入地区需针对性干预。自动化阅读评估结合AI可低成本高效支持教育者，但低资源语言和儿童语音的独特性带来了挑战。

Method: 研究创建了一个科萨语儿童语音数据集，标注方式经济高效。评估了三种先进端到端模型（wav2vec 2.0、HuBERT、Whisper），并分析了训练数据量和平衡性的影响。

Result: 模型性能受训练数据量和平衡性显著影响，wav2vec 2.0在多类别同时训练时表现提升。

Conclusion: 研究表明数据量和平衡性对模型效果至关重要，为低成本大规模数据收集提供了方向。

Abstract: Child literacy is a strong predictor of life outcomes at the subsequent
stages of an individual's life. This points to a need for targeted
interventions in vulnerable low and middle income populations to help bridge
the gap between literacy levels in these regions and high income ones. In this
effort, reading assessments provide an important tool to measure the
effectiveness of these programs and AI can be a reliable and economical tool to
support educators with this task. Developing accurate automatic reading
assessment systems for child speech in low-resource languages poses significant
challenges due to limited data and the unique acoustic properties of children's
voices. This study focuses on Xhosa, a language spoken in South Africa, to
advance child speech recognition capabilities. We present a novel dataset
composed of child speech samples in Xhosa. The dataset is available upon
request and contains ten words and letters, which are part of the Early Grade
Reading Assessment (EGRA) system. Each recording is labeled with an online and
cost-effective approach by multiple markers and a subsample is validated by an
independent EGRA reviewer. This dataset is evaluated with three fine-tuned
state-of-the-art end-to-end models: wav2vec 2.0, HuBERT, and Whisper. The
results indicate that the performance of these models can be significantly
influenced by the amount and balancing of the available training data, which is
fundamental for cost-effective large dataset collection. Furthermore, our
experiments indicate that the wav2vec 2.0 performance is improved by training
on multiple classes at a time, even when the number of available samples is
constrained.

</details>


### [215] [Value-Guided Search for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.17373)
*Kaiwen Wang,Jin Peng Zhou,Jonathan Chang,Zhaolin Gao,Nathan Kallus,Kianté Brantley,Wen Sun*

Key words: 价值模型训练, 长上下文推理, 块级价值引导搜索, 加权多数投票, DeepSeek模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出一种简单高效的价值模型训练方法，适用于长上下文推理轨迹，无需精细的步骤定义，通过数据集训练并在DeepSeek模型上验证，性能优于传统方法。

Motivation: 针对现有过程奖励模型（PRMs）需要精细定义步骤的局限性，研究提出了一种更简单的方法来处理长上下文推理模型。

Method: 收集了250万条推理轨迹数据集，训练了一个包含15亿个token级别的价值模型，并采用块级价值引导搜索（VGS）结合加权多数投票的方法。

Result: 在64次生成的推理预算下，VGS方法在四个数学竞赛基准测试中的平均准确率达到45.7%，同时显著降低了计算成本。

Conclusion: VGS方法在性能上与现有技术持平，同时减少了计算资源消耗，相关数据集、模型和代码已开源。

Abstract: In this paper, we propose a simple and efficient method for value model
training on long-context reasoning traces. Compared to existing process reward
models (PRMs), our method does not require a fine-grained notion of "step,"
which is difficult to define for long-context reasoning models. By collecting a
dataset of 2.5 million reasoning traces, we train a 1.5B token-level value
model and apply it to DeepSeek models for improved performance with test-time
compute scaling. We find that block-wise value-guided search (VGS) with a final
weighted majority vote achieves better test-time scaling than standard methods
such as majority voting or best-of-n. With an inference budget of 64
generations, VGS with DeepSeek-R1-Distill-1.5B achieves an average accuracy of
45.7% across four competition math benchmarks (AIME 2024 & 2025, HMMT Feb 2024
& 2025), reaching parity with o3-mini-medium. Moreover, VGS significantly
reduces the inference FLOPs required to achieve the same performance of
majority voting. Our dataset, model and codebase are open-sourced.

</details>


### [216] [Provably Efficient Algorithm for Best Scoring Rule Identification in Online Principal-Agent Information Acquisition](https://arxiv.org/abs/2505.17379)
*Zichen Wang,Chuanhao Li,Huazheng Wang*

Key words: 在线信息获取、评分规则、固定置信度、固定预算、算法效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究通过两种算法（OIAFC和OIAFB）解决了在线信息获取问题中寻找最优评分规则的挑战，分别在固定置信度和固定预算设置下表现高效。

Motivation: 从委托人角度出发，研究如何通过代理人的互动确定最优评分规则，以解决在线信息获取中的问题。

Method: 提出两种算法：OIAFC（针对固定置信度设置）和OIAFB（针对固定预算设置），并进行了理论分析。

Result: OIAFC能高效提取所需评分规则，样本复杂度兼具实例依赖和独立特性；OIAFB在固定预算下表现与OIAFC相同。

Conclusion: 两种算法在不同设置下均表现高效，验证了其理论有效性和实用性。

Abstract: We investigate the problem of identifying the optimal scoring rule within the
principal-agent framework for online information acquisition problem. We focus
on the principal's perspective, seeking to determine the desired scoring rule
through interactions with the agent. To address this challenge, we propose two
algorithms: OIAFC and OIAFB, tailored for fixed confidence and fixed budget
settings, respectively. Our theoretical analysis demonstrates that OIAFC can
extract the desired $(\epsilon, \delta)$-scoring rule with a efficient
instance-dependent sample complexity or an instance-independent sample
complexity. Our analysis also shows that OIAFB matches the instance-independent
performance bound of OIAFC, while both algorithms share the same complexity
across fixed confidence and fixed budget settings.

</details>


### [217] [Variational Autoencoding Discrete Diffusion with Enhanced Dimensional Correlations Modeling](https://arxiv.org/abs/2505.17384)
*Tianyu Xie,Shuchen Xue,Zijin Feng,Tianyang Hu,Jiacheng Sun,Zhenguo Li,Cheng Zhang*

Key words: 离散扩散模型,变分自编码器,维度相关性,少步骤去噪

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: VADD结合变分自编码器与离散扩散模型，通过隐变量建模增强维度间相关性，提升少步骤去噪性能，优于传统MDM。

Motivation: 传统MDM在少步骤去噪时因维度依赖建模不足导致性能下降，需改进。

Method: 引入辅助识别模型，通过变分下界最大化实现稳定训练，隐式建模维度相关性。

Result: 在2D数据、图像生成和文本生成任务中，VADD均优于MDM基线。

Conclusion: VADD在保持效率的同时显著提升样本质量，尤其在少步骤去噪场景。

Abstract: Discrete diffusion models have recently shown great promise for modeling
complex discrete data, with masked diffusion models (MDMs) offering a
compelling trade-off between quality and generation speed. MDMs denoise by
progressively unmasking multiple dimensions from an all-masked input, but their
performance can degrade when using few denoising steps due to limited modeling
of inter-dimensional dependencies. In this paper, we propose Variational
Autoencoding Discrete Diffusion (VADD), a novel framework that enhances
discrete diffusion with latent variable modeling to implicitly capture
correlations among dimensions. By introducing an auxiliary recognition model,
VADD enables stable training via variational lower bounds maximization and
amortized inference over the training set. Our approach retains the efficiency
of traditional MDMs while significantly improving sample quality, especially
when the number of denoising steps is small. Empirical results on 2D toy data,
pixel-level image generation, and text generation demonstrate that VADD
consistently outperforms MDM baselines.

</details>


### [218] [Spectral Mixture Kernels for Bayesian Optimization](https://arxiv.org/abs/2505.17393)
*Yi Zhang,Cheng Hua*

Key words: 贝叶斯优化, 高斯过程, 光谱混合核, 效率, 优化性能

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于高斯过程（GP）和光谱混合核的贝叶斯优化（BO）新方法，显著提高了效率与优化性能，且在多种合成和现实问题上超越现有基线。

Motivation: 选择合适的概率代理模型是贝叶斯优化中重要但具有挑战性的问题，论文旨在通过引入光谱混合核解决这一问题。

Method: 提出了一种结合由柯西和高斯分布的尺度-位置混合生成的光谱密度所衍生的光谱混合核的GP-BO方法。

Result: 该方法在效率和优化性能上显著提升，计算速度与简单核相当，但结果优于复杂模型和自动BO方法。实验证明其在不同问题上均优于现有基线。

Conclusion: 新型光谱混合核GP-BO方法在贝叶斯优化中表现出色，为复杂优化任务提供了一种高效解决方案。

Abstract: Bayesian Optimization (BO) is a widely used approach for solving expensive
black-box optimization tasks. However, selecting an appropriate probabilistic
surrogate model remains an important yet challenging problem. In this work, we
introduce a novel Gaussian Process (GP)-based BO method that incorporates
spectral mixture kernels, derived from spectral densities formed by
scale-location mixtures of Cauchy and Gaussian distributions. This method
achieves a significant improvement in both efficiency and optimization
performance, matching the computational speed of simpler kernels while
delivering results that outperform more complex models and automatic BO
methods. We provide bounds on the information gain and cumulative regret
associated with obtaining the optimum. Extensive numerical experiments
demonstrate that our method consistently outperforms existing baselines across
a diverse range of synthetic and real-world problems, including both low- and
high-dimensional settings.

</details>


### [219] [Wasserstein Transfer Learning](https://arxiv.org/abs/2505.17404)
*Kaicheng Zhang,Sinian Zhang,Doudou Zhou,Yidong Zhou*

Key words: 转移学习, Wasserstein空间, 概率分布, 回归模型, 负迁移

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种新颖的转移学习框架，用于处理Wasserstein空间中的概率分布输出，针对已知和未知可转移源域的情况分别提出了估计器和数据驱动的转移学习方法，并通过理论和实证验证了其有效性。

Motivation: 传统转移学习方法主要关注欧几里得空间中的标量或多变量数据，难以处理复杂数据结构（如概率分布）。因此，本文旨在解决这一局限性，为概率分布输出的回归模型设计更通用的转移学习框架。

Method: 针对已知可转移源域的情况，提出了一种具有可证明渐进收敛率的估计器；针对未知可转移源域的情况，开发了数据驱动的转移学习程序以避免负迁移。

Result: 提出的方法通过理论分析和实证验证（模拟与真实应用）证明了其有效性，能够量化域相似性对转移效率的影响并减少负迁移。

Conclusion: 该研究为概率分布输出的转移学习提供了通用框架，兼具理论严谨性和实用价值，适用于复杂数据场景。

Abstract: Transfer learning is a powerful paradigm for leveraging knowledge from source
domains to enhance learning in a target domain. However, traditional transfer
learning approaches often focus on scalar or multivariate data within Euclidean
spaces, limiting their applicability to complex data structures such as
probability distributions. To address this, we introduce a novel framework for
transfer learning in regression models, where outputs are probability
distributions residing in the Wasserstein space. When the informative subset of
transferable source domains is known, we propose an estimator with provable
asymptotic convergence rates, quantifying the impact of domain similarity on
transfer efficiency. For cases where the informative subset is unknown, we
develop a data-driven transfer learning procedure designed to mitigate negative
transfer. The proposed methods are supported by rigorous theoretical analysis
and are validated through extensive simulations and real-world applications.

</details>


### [220] [HyperIMTS: Hypergraph Neural Network for Irregular Multivariate Time Series Forecasting](https://arxiv.org/abs/2505.17431)
*Boyuan Li,Yicheng Luo,Zhen Liu,Junhao Zheng,Jianming Lv,Qianli Ma*

Key words: 不规则多元时间序列, 超图神经网络, 时间序列预测, 未对齐观测

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了HyperIMTS，一种基于超图神经网络的模型，用于处理不规则多元时间序列，通过时间感知的消息传递统一学习时间和变量间的依赖关系。

Motivation: 不规则多元时间序列因时间间隔不均匀和变量观测不齐，导致学习依赖关系困难。现有方法因填充值或图结构限制，难以高效捕获未对齐观测间的依赖关系。

Method: 提出HyperIMTS模型，将观测值转化为超图节点，通过时间和变量超边连接，实现未对齐观测间的消息传递，自适应捕获依赖关系。

Result: 实验表明HyperIMTS在预测精度和计算成本上优于现有方法。

Conclusion: HyperIMTS为不规则多元时间序列提供了一种高效的统一学习框架。

Abstract: Irregular multivariate time series (IMTS) are characterized by irregular time
intervals within variables and unaligned observations across variables, posing
challenges in learning temporal and variable dependencies. Many existing IMTS
models either require padded samples to learn separately from temporal and
variable dimensions, or represent original samples via bipartite graphs or
sets. However, the former approaches often need to handle extra padding values
affecting efficiency and disrupting original sampling patterns, while the
latter ones have limitations in capturing dependencies among unaligned
observations. To represent and learn both dependencies from original
observations in a unified form, we propose HyperIMTS, a Hypergraph neural
network for Irregular Multivariate Time Series forecasting. Observed values are
converted as nodes in the hypergraph, interconnected by temporal and variable
hyperedges to enable message passing among all observations. Through
irregularity-aware message passing, HyperIMTS captures variable dependencies in
a time-adaptive way to achieve accurate forecasting. Experiments demonstrate
HyperIMTS's competitive performance among state-of-the-art models in IMTS
forecasting with low computational cost.

</details>


### [221] [Discretization-free Multicalibration through Loss Minimization over Tree Ensembles](https://arxiv.org/abs/2505.17435)
*Hongyi Henry Jin,Zijun Ding,Dung Daniel Ngo,Zhiwei Steven Wu*

Key words: 多校准, 决策树集成, 经验风险最小化, 损失饱和, LightGBM

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种无需离散化的多校准方法，直接使用深度为二的决策树集成优化经验风险目标，优于现有方法。

Motivation: 当前多校准方法通常需要离散化预测器的输出空间，这会引入舍入误差、额外敏感超参数，并可能扭曲预测结果，影响下游决策。

Method: 采用基于决策树集成（如LightGBM）的直接优化经验风险方法，避免离散化，并在数据分布满足“损失饱和”技术条件时实现多校准。

Result: 在多个数据集上验证表明，该方法总能满足技术条件，且性能优于或与现有方法相当，即使在使用离散化指标评估时。

Conclusion: 提出的无需离散化方法在多校准任务中表现优越，扩展了多校准的实际应用范围。

Abstract: In recent years, multicalibration has emerged as a desirable learning
objective for ensuring that a predictor is calibrated across a rich collection
of overlapping subpopulations. Existing approaches typically achieve
multicalibration by discretizing the predictor's output space and iteratively
adjusting its output values. However, this discretization approach departs from
the standard empirical risk minimization (ERM) pipeline, introduces rounding
error and additional sensitive hyperparameter, and may distort the predictor's
outputs in ways that hinder downstream decision-making.
  In this work, we propose a discretization-free multicalibration method that
directly optimizes an empirical risk objective over an ensemble of depth-two
decision trees. Our ERM approach can be implemented using off-the-shelf tree
ensemble learning methods such as LightGBM. Our algorithm provably achieves
multicalibration, provided that the data distribution satisfies a technical
condition we term as loss saturation. Across multiple datasets, our empirical
evaluation shows that this condition is always met in practice. Our
discretization-free algorithm consistently matches or outperforms existing
multicalibration approaches--even when evaluated using a discretization-based
multicalibration metric that shares its discretization granularity with the
baselines.

</details>


### [222] [Designing an efficient and equitable humanitarian supply chain dynamically via reinforcement learning](https://arxiv.org/abs/2505.17439)
*Weijia Jin*

Key words: 人道主义供应链, 强化学习, PPO, 公平性, 启发式算法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究利用强化学习（PPO）设计了一种高效且公平的人道主义供应链模型，并与启发式算法进行了比较，结果显示PPO模型将平均满意度作为优先考量。

Motivation: 旨在通过先进的计算方法（如强化学习）优化人道主义供应链的效率与公平性。

Method: 采用PPO（近端策略优化）算法，并与传统启发式算法进行对比。

Result: PPO模型在动态供应链管理中表现优异，尤其注重提升平均满意度。

Conclusion: 强化学习在人道主义供应链中具有潜力，特别是在提升公平性方面。

Abstract: This study designs an efficient and equitable humanitarian supply chain
dynamically by using reinforcement learning, PPO, and compared with heuristic
algorithms. This study demonstrates the model of PPO always treats average
satisfaction rate as the priority.

</details>


### [223] [Baitradar: A Multi-Model Clickbait Detection Algorithm Using Deep Learning](https://arxiv.org/abs/2505.17448)
*Bhanuka Gamage,Adnan Labib,Aisha Joomun,Chern Hong Lim,KokSheik Wong*

Key words: 点击诱饵, YouTube, 深度学习, BaitRadar, 视频分类

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究提出了一种名为BaitRadar的深度学习算法，通过结合六个推理模型来识别YouTube上的点击诱饵视频，测试准确率高达98%。

Motivation: 随着YouTube的普及，点击诱饵问题日益严重，用户常被吸引性标题和缩略图误导。研究旨在解决这一问题。

Method: 采用深度学习技术，结合标题、评论、缩略图、标签、视频统计和音频转录六个模型的输出，通过平均值进行最终分类。

Result: 在1,400个YouTube视频上测试，平均准确率为98%，推理时间少于2秒。

Conclusion: BaitRadar算法在识别点击诱饵视频方面表现出色，即使在数据缺失的情况下也能保持高准确性。

Abstract: Following the rising popularity of YouTube, there is an emerging problem on
this platform called clickbait, which provokes users to click on videos using
attractive titles and thumbnails. As a result, users ended up watching a video
that does not have the content as publicized in the title. This issue is
addressed in this study by proposing an algorithm called BaitRadar, which uses
a deep learning technique where six inference models are jointly consulted to
make the final classification decision. These models focus on different
attributes of the video, including title, comments, thumbnail, tags, video
statistics and audio transcript. The final classification is attained by
computing the average of multiple models to provide a robust and accurate
output even in situation where there is missing data. The proposed method is
tested on 1,400 YouTube videos. On average, a test accuracy of 98% is achieved
with an inference time of less than 2s.

</details>


### [224] [CLIMB: Class-imbalanced Learning Benchmark on Tabular Data](https://arxiv.org/abs/2505.17451)
*Zhining Liu,Zihao Li,Ze Yang,Tianxin Wei,Jian Kang,Yada Zhu,Hendrik Hamann,Jingrui He,Hanghang Tong*

Key words: 类别不平衡学习, 基准测试, 集成学习, 数据质量, Python包

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: CLIMB是一个用于类别不平衡学习的综合基准测试，包含73个真实数据集和29种代表性算法，提供了方法准确性和效率的实用见解。

Motivation: 类别不平衡学习在现实应用中至关重要，但现有的基准测试和实现缺乏全面性和统一性。

Method: 开发了包含73个数据集和29种算法的CLIMB基准测试，通过统一的Python包实现，支持算法间的快速比较。

Result: 实验表明，朴素的重平衡方法效果有限，集成方法更有效，且数据质量是关键。

Conclusion: CLIMB为类别不平衡学习提供了实用的工具和见解，有助于未来研究和应用。

Abstract: Class-imbalanced learning (CIL) on tabular data is important in many
real-world applications where the minority class holds the critical but rare
outcomes. In this paper, we present CLIMB, a comprehensive benchmark for
class-imbalanced learning on tabular data. CLIMB includes 73 real-world
datasets across diverse domains and imbalance levels, along with unified
implementations of 29 representative CIL algorithms. Built on a high-quality
open-source Python package with unified API designs, detailed documentation,
and rigorous code quality controls, CLIMB supports easy implementation and
comparison between different CIL algorithms. Through extensive experiments, we
provide practical insights on method accuracy and efficiency, highlighting the
limitations of naive rebalancing, the effectiveness of ensembles, and the
importance of data quality. Our code, documentation, and examples are available
at https://github.com/ZhiningLiu1998/imbalanced-ensemble.

</details>


### [225] [Self-Training Large Language Models with Confident Reasoning](https://arxiv.org/abs/2505.17454)
*Hyosoon Jang,Yunhui Jang,Sungjae Lee,Jungseul Ok,Sungsoo Ahn*

Key words: 大语言模型, 自训练, 推理路径, 策略优化, CORE-PO

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出CORE-PO方法，通过策略优化选择高置信推理路径进行自训练，提升大语言模型的推理能力。

Motivation: 现有自训练方法仅关注最终答案质量，可能忽略推理路径质量，需要改进。

Method: 提出CORE-PO方法，利用推理级置信度选择高质量路径，通过策略优化微调模型。

Result: 实验表明，CORE-PO在多个基准测试中优于现有方法。

Conclusion: 推理级置信度能有效提升模型性能，CORE-PO为自训练提供了新思路。

Abstract: Large language models (LLMs) have shown impressive performance by generating
reasoning paths before final answers, but learning such a reasoning path
requires costly human supervision. To address this issue, recent studies have
explored self-training methods that improve reasoning capabilities using
pseudo-labels generated by the LLMs themselves. Among these, confidence-based
self-training fine-tunes LLMs to prefer reasoning paths with high-confidence
answers, where confidence is estimated via majority voting. However, such
methods exclusively focus on the quality of the final answer and may ignore the
quality of the reasoning paths, as even an incorrect reasoning path leads to a
correct answer by chance. Instead, we advocate the use of reasoning-level
confidence to identify high-quality reasoning paths for self-training,
supported by our empirical observations. We then propose a new self-training
method, CORE-PO, that fine-tunes LLMs to prefer high-COnfidence REasoning paths
through Policy Optimization. Our experiments show that CORE-PO improves the
accuracy of outputs on four in-distribution and two out-of-distribution
benchmarks, compared to existing self-training methods.

</details>


### [226] [Towards Heterogeneous Continual Graph Learning via Meta-knowledge Distillation](https://arxiv.org/abs/2505.17458)
*Guiquan Sun,Xikun Zhang,Jingchao Ni,Dongjin Song*

Key words: 异质图, 持续学习, 元学习, 知识蒸馏, 采样策略

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了MKD框架，用于在不断扩展的异质图上进行持续学习，通过元学习和知识蒸馏平衡新信息吸收与旧知识保留，并提出了新颖的采样策略和语义级蒸馏模块。

Motivation: 现实世界的图数据具有动态性和异质性，现有研究多假设图是静态的，忽略了图持续扩展的特性，导致模型难以适应新数据且易遗忘旧知识。

Method: MKD结合了元学习（快速任务适应）和知识蒸馏（防止灾难性遗忘），采用基于节点多样性的采样策略和语义级注意力对齐的蒸馏模块。

Result: 在三个基准数据集上的实验验证了MKD在持续学习场景下的有效性，能够更好地处理动态异质图。

Conclusion: MKD通过采样策略和语义级蒸馏，有效解决了动态异质图上的持续学习问题，平衡了新信息与旧知识的关系。

Abstract: Machine learning on heterogeneous graphs has experienced rapid advancement in
recent years, driven by the inherently heterogeneous nature of real-world data.
However, existing studies typically assume the graphs to be static, while
real-world graphs are continuously expanding. This dynamic nature requires
models to adapt to new data while preserving existing knowledge. To this end,
this work addresses the challenge of continual learning on heterogeneous graphs
by introducing the Meta-learning based Knowledge Distillation framework (MKD),
designed to mitigate catastrophic forgetting in evolving heterogeneous graph
structures. MKD combines rapid task adaptation through meta-learning on limited
samples with knowledge distillation to achieve an optimal balance between
incorporating new information and maintaining existing knowledge. To improve
the efficiency and effectiveness of sample selection, MKD incorporates a novel
sampling strategy that selects a small number of target-type nodes based on
node diversity and maintains fixed-size buffers for other types. The strategy
retrieves first-order neighbors along metapaths and selects important neighbors
based on their structural relevance, enabling the sampled subgraphs to retain
key topological and semantic information. In addition, MKD introduces a
semantic-level distillation module that aligns the attention distributions over
different metapaths between teacher and student models, encouraging semantic
consistency beyond the logit level. Comprehensive evaluations across three
benchmark datasets validate MKD's effectiveness in handling continual learning
scenarios on expanding heterogeneous graphs.

</details>


### [227] [Efficient compression of neural networks and datasets](https://arxiv.org/abs/2505.17469)
*Lukas Silvester Barth,Paulo von Petersenn*

Key words: 神经网络压缩, $ℓ0$正则化, 数据压缩, 归纳推理, 参数优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文比较并改进了减少神经网络参数数量同时保持高测试准确率的方法，还涉及数据压缩算法与理论验证。

Motivation: 旨在减少神经网络参数数量而不牺牲准确性，并探索压缩算法与归纳推理理论的关联。

Method: 提出概率重构的$ℓ0$正则化优化方法，改进平滑近似方法，并采用分层策略。在不同架构和数据集上测试。

Result: 方法有效减少了参数数量且保持高准确率，数据压缩算法表现优异，理论预测得到验证。

Conclusion: 研究为神经网络压缩提供了有效方法，并验证了正则化模型在样本效率上的优势。

Abstract: We compare, improve, and contribute methods that substantially decrease the
number of parameters of neural networks while maintaining high test accuracy.
When applying our methods to minimize description length, we obtain very
effective data compression algorithms. In particular, we develop a
probabilistic reformulation of $\ell_0$ regularized optimization for nonlinear
models that does not require Monte-Carlo sampling and thus improves upon
previous methods. We also improve upon methods involving smooth approximations
to the $\ell_0$ norm, and investigate layerwise methods. We compare the methods
on different architectures and datasets, including convolutional networks
trained on image datasets and transformers trained on parts of Wikipedia. We
also created a synthetic teacher-student setup to investigate compression in a
controlled continuous setting. Finally, we conceptually relate compression
algorithms to Solomonoff's theory of inductive inference and empirically verify
the prediction that regularized models can exhibit more sample-efficient
convergence.

</details>


### [228] [Reverse-Speech-Finder: A Neural Network Backtracking Architecture for Generating Alzheimer's Disease Speech Samples and Improving Diagnosis Performance](https://arxiv.org/abs/2505.17477)
*Victor OK Li,Yang Han,Jacqueline CK Lam,Lawrence YL Cheung*

Key words: Reverse-Speech-Finder（RSF）, 阿尔茨海默病（AD）, 语音标记, 神经网络回溯, 预训练语言模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: RSF是一种新型神经网络架构，用于通过语音分析提升阿尔茨海默病（AD）诊断，利用预训练大语言模型识别AD特异性语音标记，解决了样本稀缺和现有模型解释性差的问题，其独特方法包括MPMs、MPNs和MPTs的创新识别，实验显示其在准确率和F1得分上优于传统方法。

Motivation: 解决AD诊断中语音样本稀缺和模型解释性差的挑战，提出一种能高效识别AD特异性语音标记的方法。

Method: 结合MPMs、MPNs和MPTs的创新识别，采用语音标记表示和回溯方法，从MPNs回溯到输入层识别关键语音标记。

Result: RSF在准确率和F1得分上分别提升3.5%和3.2%，优于传统方法如SHAP和Integrated Gradients，并生成包含新标记的语音数据。

Conclusion: RSF展示了其在AD语音检测中的变革潜力，为理解AD语言缺陷和开发非侵入性早期干预策略提供了新途径。

Abstract: This study introduces Reverse-Speech-Finder (RSF), a groundbreaking neural
network backtracking architecture designed to enhance Alzheimer's Disease (AD)
diagnosis through speech analysis. Leveraging the power of pre-trained large
language models, RSF identifies and utilizes the most probable AD-specific
speech markers, addressing both the scarcity of real AD speech samples and the
challenge of limited interpretability in existing models. RSF's unique approach
consists of three core innovations: Firstly, it exploits the observation that
speech markers most probable of predicting AD, defined as the most probable
speech-markers (MPMs), must have the highest probability of activating those
neurons (in the neural network) with the highest probability of predicting AD,
defined as the most probable neurons (MPNs). Secondly, it utilizes a speech
token representation at the input layer, allowing backtracking from MPNs to
identify the most probable speech-tokens (MPTs) of AD. Lastly, it develops an
innovative backtracking method to track backwards from the MPNs to the input
layer, identifying the MPTs and the corresponding MPMs, and ingeniously
uncovering novel speech markers for AD detection. Experimental results
demonstrate RSF's superiority over traditional methods such as SHAP and
Integrated Gradients, achieving a 3.5% improvement in accuracy and a 3.2% boost
in F1-score. By generating speech data that encapsulates novel markers, RSF not
only mitigates the limitations of real data scarcity but also significantly
enhances the robustness and accuracy of AD diagnostic models. These findings
underscore RSF's potential as a transformative tool in speech-based AD
detection, offering new insights into AD-related linguistic deficits and paving
the way for more effective non-invasive early intervention strategies.

</details>


### [229] [Simultaneous Modeling of Protein Conformation and Dynamics via Autoregression](https://arxiv.org/abs/2505.17478)
*Yuning Shen,Lihao Wang,Huizhuo Yuan,Yan Wang,Bangji Yang,Quanquan Gu*

Key words: 蛋白质动力学, 分子动力学, 自回归模型, 构象采样, SE(3)扩散模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: ConfRover是一个自回归模型，能够从分子动力学轨迹中学习蛋白质构象和动力学，支持时间依赖和独立采样。

Motivation: 现有方法无法明确捕捉构象间的时间依赖关系，也不支持直接生成时间独立的样本。

Method: 采用模块化架构，包括编码层、时序模块和SE(3)扩散模型作为结构解码器。

Result: 在ATLAS数据集上验证了模型的有效性，能够学习构象动力学并支持多种下游任务。

Conclusion: ConfRover首次在一个框架内实现了蛋白质构象和轨迹的采样，提供了新颖灵活的蛋白质MD数据学习方法。

Abstract: Understanding protein dynamics is critical for elucidating their biological
functions. The increasing availability of molecular dynamics (MD) data enables
the training of deep generative models to efficiently explore the
conformational space of proteins. However, existing approaches either fail to
explicitly capture the temporal dependencies between conformations or do not
support direct generation of time-independent samples. To address these
limitations, we introduce ConfRover, an autoregressive model that
simultaneously learns protein conformation and dynamics from MD trajectories,
supporting both time-dependent and time-independent sampling. At the core of
our model is a modular architecture comprising: (i) an encoding layer, adapted
from protein folding models, that embeds protein-specific information and
conformation at each time frame into a latent space; (ii) a temporal module, a
sequence model that captures conformational dynamics across frames; and (iii)
an SE(3) diffusion model as the structure decoder, generating conformations in
continuous space. Experiments on ATLAS, a large-scale protein MD dataset of
diverse structures, demonstrate the effectiveness of our model in learning
conformational dynamics and supporting a wide range of downstream tasks.
ConfRover is the first model to sample both protein conformations and
trajectories within a single framework, offering a novel and flexible approach
for learning from protein MD data.

</details>


### [230] [Hyperspectral in situ remote sensing of water surface nitrate in the Fitzroy River estuary, Queensland, Australia, using deep learning](https://arxiv.org/abs/2505.17483)
*Yiqing Guo,Nagur Cherukuru,Eric Lehmann,S. L. Kesav Unnithan,Gemma Kerrisk,Tim Malthus,Faisal Islam*

Key words: 硝酸盐, 高光谱反射率, 盐度, 大堡礁, 水质监测

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究探讨了利用高光谱反射率和盐度测量来预测水体表面硝酸盐含量的可行性，结果表明预测值与实地测量值高度相关（R²=0.86）。

Motivation: 硝酸盐（NO₃⁻）主要来源于人为活动，其增加对珊瑚礁（如大堡礁）构成威胁。研究旨在通过高光谱反射率间接关系预测硝酸盐浓度，以补充传统耗时费力的实地测量方法。

Method: 在澳大利亚菲茨罗伊河河口进行时间序列的硝酸盐和高光谱反射率同步测量，结合盐度数据，建立硝酸盐预测模型。

Result: 模型预测的硝酸盐与实测值吻合良好（R²=0.86），均方根误差为0.03 mg/L，证明了方法的可行性。

Conclusion: 高光谱反射率与盐度测量可有效预测水体表面硝酸盐浓度，为水质监测提供高效替代方案。

Abstract: Nitrate ($\text{NO}_3^-$) is a form of dissolved inorganic nitrogen derived
primarily from anthropogenic sources. The recent increase in river-discharged
nitrate poses a major risk for coral bleaching in the Great Barrier Reef (GBR)
lagoon. Although nitrate is an optically inactive (i.e., colourless)
constituent, previous studies have demonstrated there is an indirect,
non-causal relationship between water surface nitrate and water-leaving
reflectance that is mediated through optically active water quality parameters
such as total suspended solids and coloured dissolved organic matter. This work
aims to advance our understanding of this relationship with an effort to
measure time-series nitrate and simultaneous hyperspectral reflectance at the
Fitzroy River estuary, Queensland, Australia. Time-series observations revealed
periodic cycles in nitrate loads due to the tidal influence in the estuarine
study site. The water surface nitrate loads were predicted from hyperspectral
reflectance and water salinity measurements, with hyperspectral reflectance
indicating the concentrations of optically active variables and salinity
indicating the mixing of river water and seawater proportions. The accuracy
assessment of model-predicted nitrate against in-situ measured nitrate values
showed that the predicted nitrate values correlated well with the ground-truth
data, with an $R^2$ score of 0.86, and an RMSE of 0.03 mg/L. This work
demonstrates the feasibility of predicting water surface nitrate from
hyperspectral reflectance and salinity measurements.

</details>


### [231] [ExARNN: An Environment-Driven Adaptive RNN for Learning Non-Stationary Power Dynamics](https://arxiv.org/abs/2505.17488)
*Haoran Li,Muhao Guo,Yang Weng,Marija Ilic,Guangchun Ruan*

Key words: 非稳态电力系统, 循环神经网络, 外部数据整合, 动态参数调整, 神经控制微分方程, 时间序列预测

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为ExARNN的新型框架，通过整合外部数据（如天气、时间）动态调整基础RNN参数，解决了传统模型（如RNN）无法有效编码外部因素的局限性，显著提升了非稳态电力系统动态建模的准确性。

Motivation: 由于可再生能源的波动性、需求模式的变化及气候影响，电力系统动态越来越复杂，传统模型无法高效编码外部因素以适应动态变化。需一种新方法提升建模准确性。

Method: 提出ExARNN框架，采用分层超网络设计和神经控制微分方程（NCDE）处理外部数据，动态生成RNN参数，以处理电力数据与外部数据时间戳不一致的问题。

Result: 大量预测实验表明，ExARNN显著优于现有基线模型。

Conclusion: ExARNN通过动态整合外部数据实现了对非稳态电力系统的高效建模，为复杂动态系统提供了新解决方案。

Abstract: Non-stationary power system dynamics, influenced by renewable energy
variability, evolving demand patterns, and climate change, are becoming
increasingly complex. Accurately capturing these dynamics requires a model
capable of adapting to environmental factors. Traditional models, including
Recurrent Neural Networks (RNNs), lack efficient mechanisms to encode external
factors, such as time or environmental data, for dynamic adaptation. To address
this, we propose the External Adaptive RNN (ExARNN), a novel framework that
integrates external data (e.g., weather, time) to continuously adjust the
parameters of a base RNN. ExARNN achieves this through a hierarchical
hypernetwork design, using Neural Controlled Differential Equations (NCDE) to
process external data and generate RNN parameters adaptively. This approach
enables ExARNN to handle inconsistent timestamps between power and external
measurements, ensuring continuous adaptation. Extensive forecasting tests
demonstrate ExARNN's superiority over established baseline models.

</details>


### [232] [ProxySPEX: Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs](https://arxiv.org/abs/2505.17495)
*Landon Butler,Abhineet Agarwal,Justin Singh Kang,Yigit Efe Erginbas,Bin Yu,Kannan Ramchandran*

Key words: LLM, 特征交互, 梯度提升树, 可解释性, 注意力头

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: ProxySPEX提出了一种高效识别LLM特征交互的方法，利用层次性交互提升效率，比SPEX少用10倍推断资源，同时比边际方法提升20%的输出重建精度。

Motivation: 现有方法（如SPEX）需要大量模型推断且难以高效识别LLM中的高阶特征交互，而实际交互中存在层次性结构。

Method: 采用梯度提升树拟合掩码后的LLM输出，从中提取重要交互，利用层次性优化效率。

Result: 在四个数据集上，ProxySPEX比边际方法重建精度高20%，推断资源少10倍，并识别出更多关键特征。应用于数据归因和机理可解释性任务时，它还能发现更多可剪枝的注意力头。

Conclusion: ProxySPEX通过层次性交互假设显著提升了交互识别的效率和效果，适用于大规模LLM分析。

Abstract: Large Language Models (LLMs) have achieved remarkable performance by
capturing complex interactions between input features. To identify these
interactions, most existing approaches require enumerating all possible
combinations of features up to a given order, causing them to scale poorly with
the number of inputs $n$. Recently, Kang et al. (2025) proposed SPEX, an
information-theoretic approach that uses interaction sparsity to scale to $n
\approx 10^3$ features. SPEX greatly improves upon prior methods but requires
tens of thousands of model inferences, which can be prohibitive for large
models. In this paper, we observe that LLM feature interactions are often
hierarchical -- higher-order interactions are accompanied by their lower-order
subsets -- which enables more efficient discovery. To exploit this hierarchy,
we propose ProxySPEX, an interaction attribution algorithm that first fits
gradient boosted trees to masked LLM outputs and then extracts the important
interactions. Experiments across four challenging high-dimensional datasets
show that ProxySPEX more faithfully reconstructs LLM outputs by 20% over
marginal attribution approaches while using $10\times$ fewer inferences than
SPEX. By accounting for interactions, ProxySPEX identifies features that
influence model output over 20% more than those selected by marginal
approaches. Further, we apply ProxySPEX to two interpretability tasks. Data
attribution, where we identify interactions among CIFAR-10 training samples
that influence test predictions, and mechanistic interpretability, where we
uncover interactions between attention heads, both within and across layers, on
a question-answering task. ProxySPEX identifies interactions that enable more
aggressive pruning of heads than marginal approaches.

</details>


### [233] [On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning](https://arxiv.org/abs/2505.17508)
*Yifan Zhang,Yifeng Liu,Huizhuo Yuan,Yang Yuan,Quanquan Gu,Andrew C Yao*

Key words: 策略梯度、KL散度、强化学习、大语言模型、RPG框架

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了正则化策略梯度（RPG），一个系统框架用于在线强化学习中KL正则化策略梯度方法的推导与分析，通过前向和反向KL散度优化策略分布，实验显示在LLM推理任务中训练稳定性和性能优于基线。

Motivation: 尽管KL正则化在策略梯度算法中被广泛用于稳定训练，但不同KL散度形式如何估计并整合到在线强化学习的替代损失函数中缺乏系统探索，RPG框架填补了这一空白。

Method: 提出了RPG框架，推导了基于前向和反向KL散度的策略梯度及其替代损失函数，支持归一化和非归一化策略分布，并提供完全可微损失函数和REINFORCE式梯度估计器。

Result: 实验表明，RPG在LLM推理任务中的训练稳定性和性能优于GRPO、REINFORCE++和DAPO等基线方法。

Conclusion: RPG为KL正则化策略梯度方法提供了系统化的分析框架，在提升训练效果的同时保持了算法的灵活性。

Abstract: Policy gradient algorithms have been successfully applied to enhance the
reasoning capabilities of large language models (LLMs). Despite the widespread
use of Kullback-Leibler (KL) regularization in policy gradient algorithms to
stabilize training, the systematic exploration of how different KL divergence
formulations can be estimated and integrated into surrogate loss functions for
online reinforcement learning (RL) presents a nuanced and systematically
explorable design space. In this paper, we propose regularized policy gradient
(RPG), a systematic framework for deriving and analyzing KL-regularized policy
gradient methods in the online RL setting. We derive policy gradients and
corresponding surrogate loss functions for objectives regularized by both
forward and reverse KL divergences, considering both normalized and
unnormalized policy distributions. Furthermore, we present derivations for
fully differentiable loss functions as well as REINFORCE-style gradient
estimators, accommodating diverse algorithmic needs. We conduct extensive
experiments on RL for LLM reasoning using these methods, showing improved or
competitive results in terms of training stability and performance compared to
strong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at
https://github.com/complex-reasoning/RPG.

</details>


### [234] [What You Read Isn't What You Hear: Linguistic Sensitivity in Deepfake Speech Detection](https://arxiv.org/abs/2505.17513)
*Binh Nguyen,Shuji Shi,Ryan Ofman,Thai Le*

Key words: 音频反欺骗, 对抗攻击, 语言扰动, 特征归因, 深度伪造

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究发现语言层面的微小扰动可显著降低音频反欺骗检测系统的准确率，攻击成功率超过60%，商业系统准确率从100%降至32%。需结合语言特征设计更鲁棒的反欺骗系统。

Motivation: 现有音频反欺骗系统主要关注声学层面扰动，语言变异的影响未被充分研究，论文旨在填补这一空白。

Method: 通过引入转录层面的对抗攻击，评估开源和商业反欺骗检测器的语言敏感性，并进行特征归因分析。

Result: 小规模语言扰动导致检测准确率大幅下降（攻击成功率>60%，商业系统降至32%），语言复杂度和音频嵌入相似性是主要漏洞。

Conclusion: 需在反欺骗系统设计中整合语言变异，超越纯声学防御。

Abstract: Recent advances in text-to-speech technologies have enabled realistic voice
generation, fueling audio-based deepfake attacks such as fraud and
impersonation. While audio anti-spoofing systems are critical for detecting
such threats, prior work has predominantly focused on acoustic-level
perturbations, leaving the impact of linguistic variation largely unexplored.
In this paper, we investigate the linguistic sensitivity of both open-source
and commercial anti-spoofing detectors by introducing transcript-level
adversarial attacks. Our extensive evaluation reveals that even minor
linguistic perturbations can significantly degrade detection accuracy: attack
success rates surpass 60% on several open-source detector-voice pairs, and
notably one commercial detection accuracy drops from 100% on synthetic audio to
just 32%. Through a comprehensive feature attribution analysis, we identify
that both linguistic complexity and model-level audio embedding similarity
contribute strongly to detector vulnerability. We further demonstrate the
real-world risk via a case study replicating the Brad Pitt audio deepfake scam,
using transcript adversarial attacks to completely bypass commercial detectors.
These results highlight the need to move beyond purely acoustic defenses and
account for linguistic variation in the design of robust anti-spoofing systems.
All source code will be publicly available.

</details>


### [235] [Spacetime Geometry of Denoising in Diffusion Models](https://arxiv.org/abs/2505.17517)
*Rafał Karczewski,Markus Heinonen,Alison Pouplin,Søren Hauberg,Vikas Garg*

Key words: 扩散模型, 信息几何, 统计流形, Fisher-Rao度量, 过渡路径采样

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文利用信息几何框架提出了一种新颖的扩散模型视角，将不同噪声水平的样本集合视为统计流形（即时空），并基于Fisher-Rao度量计算高效的高维测地线，应用于过渡路径采样。

Motivation: 通过信息几何的视角重新理解扩散模型，揭示噪声样本集合的统计流形特性，为高维数据分析和过渡路径采样提供理论基础和实践工具。

Method: 将不同噪声水平的样本集合建模为统计流形（时空），利用Fisher-Rao度量计算测地线，并应用于Boltzmann分布的平滑过渡路径生成。

Result: 证明了该方法无需重新训练或微调即可高效计算高维测地线，并成功应用于过渡路径采样，生成低能亚稳态间的连续轨迹。

Conclusion: 信息几何框架为扩散模型提供了新的理论视角和实践价值，尤其在过渡路径采样中展现出高效性和平滑性。

Abstract: We present a novel perspective on diffusion models using the framework of
information geometry. We show that the set of noisy samples, taken across all
noise levels simultaneously, forms a statistical manifold -- a family of
denoising probability distributions. Interpreting the noise level as a temporal
parameter, we refer to this manifold as spacetime. This manifold naturally
carries a Fisher-Rao metric, which defines geodesics -- shortest paths between
noisy points. Notably, this family of distributions is exponential, enabling
efficient geodesic computation even in high-dimensional settings without
retraining or fine-tuning. We demonstrate the practical value of this geometric
viewpoint in transition path sampling, where spacetime geodesics define smooth
sequences of Boltzmann distributions, enabling the generation of continuous
trajectories between low-energy metastable states. Code is available at:
https://github.com/Aalto-QuML/diffusion-spacetime-geometry.

</details>


### [236] [TimeCF: A TimeMixer-Based Model with adaptive Convolution and Sharpness-Aware Minimization Frequency Domain Loss for long-term time seris forecasting](https://arxiv.org/abs/2505.17532)
*Bin Wang,Heming Yang,Jinfang Sheng*

Key words: 长期预测，多尺度分析，深度学习，时间序列，SAMFre

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种新的深度学习模型TimeCF，用于解决多尺度时间序列预测中因通道独立方法导致的次优预测问题，结合了自适应卷积信息聚合模块和SAMFre损失函数，实验证明其在长期预测领域表现优异。

Motivation: 受多尺度分析在时间序列预测中表现良好的启发，但现有模型由于通道独立方法和时间序列标签的自相关性而影响预测效果和泛化能力。

Method: 设计了TimeCF模型，结合TimeMixer、自适应卷积信息聚合模块和SAMFre损失函数，通过多尺度分解、信息聚合和季节趋势混合实现预测。

Result: 在不同真实数据集上的实验表明，TimeCF在长期预测中表现优异。

Conclusion: TimeCF通过多尺度分析和优化损失函数，显著提升了长期时间序列预测的准确性和泛化能力。

Abstract: Recent studies have shown that by introducing prior knowledge, multi-scale
analysis of complex and non-stationary time series in real environments can
achieve good results in the field of long-term forecasting. However, affected
by channel-independent methods, models based on multi-scale analysis may
produce suboptimal prediction results due to the autocorrelation between time
series labels, which in turn affects the generalization ability of the model.
To address this challenge, we are inspired by the idea of sharpness-aware
minimization and the recently proposed FreDF method and design a deep learning
model TimeCF for long-term time series forecasting based on the TimeMixer,
combined with our designed adaptive convolution information aggregation module
and Sharpness-Aware Minimization Frequency Domain Loss (SAMFre). Specifically,
TimeCF first decomposes the original time series into sequences of different
scales. Next, the same-sized convolution modules are used to adaptively
aggregate information of different scales on sequences of different scales.
Then, decomposing each sequence into season and trend parts and the two parts
are mixed at different scales through bottom-up and top-down methods
respectively. Finally, different scales are aggregated through a Feed-Forward
Network. What's more, extensive experimental results on different real-world
datasets show that our proposed TimeCF has excellent performance in the field
of long-term forecasting.

</details>


### [237] [Learning Representational Disparities](https://arxiv.org/abs/2505.17533)
*Pavan Ravishankar,Rushabh Shah,Daniel B. Neill*

Key words: 公平机器学习, 可解释性, 表示差异, 多目标优化, 神经网络

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出一种公平机器学习算法，通过建模观察与理想人类决策间的可解释差异，以减少下游结果的不平等；通过多目标优化神经网络，证明可学习到完全消除结果差异的可解释权重。

Motivation: 现有公平表示学习方法未考虑决策过程中的下游结果差异，本文旨在减少因决策者的不同表示导致的结果不平等，并生成可解释的差异以实现干预。

Method: 将问题建模为多目标优化，使用神经网络学习输入表示的可解释差异，假设简化条件下证明权重可完全消除结果差异。

Result: 在German Credit、Adult和Heritage Health数据集上验证了模型目标与结果的可解释性。

Conclusion: 提出的方法能有效识别并纠正表示差异，从而完全消除下游结果的不平等。

Abstract: We propose a fair machine learning algorithm to model interpretable
differences between observed and desired human decision-making, with the latter
aimed at reducing disparity in a downstream outcome impacted by the human
decision. Prior work learns fair representations without considering the
outcome in the decision-making process. We model the outcome disparities as
arising due to the different representations of the input seen by the observed
and desired decision-maker, which we term representational disparities. Our
goal is to learn interpretable representational disparities which could
potentially be corrected by specific nudges to the human decision, mitigating
disparities in the downstream outcome; we frame this as a multi-objective
optimization problem using a neural network. Under reasonable simplifying
assumptions, we prove that our neural network model of the representational
disparity learns interpretable weights that fully mitigate the outcome
disparity. We validate objectives and interpret results using real-world German
Credit, Adult, and Heritage Health datasets.

</details>


### [238] [Graph Style Transfer for Counterfactual Explainability](https://arxiv.org/abs/2505.17542)
*Bardh Prenkaj,Efstratios Zaradoukas,Gjergji Kasneci*

Key words: 反事实解释, 图数据, 频谱风格转换, 逆向回溯, 解释性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文介绍了Graph Inverse Style Transfer (GIST)，一种通过逆向回溯和频谱风格转换生成图反事实解释的新方法，相比传统正向扰动方法，GIST在有效性和忠实性上表现更优。

Motivation: 图数据在反事实解释生成中面临结构完整性和语义保持的挑战。传统正向扰动方法效果有限，因此需要一种新方法改善这些问题。

Method: 提出了GIST框架，通过逆向回溯和频谱风格转换，将全局结构与原始输入频谱对齐，并保持局部内容忠实性。

Result: 在8个图分类基准测试中，GIST提升了7.6%的反事实有效性，并显著提升了45.5%的真实类别分布解释忠实性。

Conclusion: GIST通过逆向回溯机制有效改进图解释性，挑战了传统正向扰动方法的局限性。

Abstract: Counterfactual explainability seeks to uncover model decisions by identifying
minimal changes to the input that alter the predicted outcome. This task
becomes particularly challenging for graph data due to preserving structural
integrity and semantic meaning. Unlike prior approaches that rely on forward
perturbation mechanisms, we introduce Graph Inverse Style Transfer (GIST), the
first framework to re-imagine graph counterfactual generation as a backtracking
process, leveraging spectral style transfer. By aligning the global structure
with the original input spectrum and preserving local content faithfulness,
GIST produces valid counterfactuals as interpolations between the input style
and counterfactual content. Tested on 8 binary and multi-class graph
classification benchmarks, GIST achieves a remarkable +7.6% improvement in the
validity of produced counterfactuals and significant gains (+45.5%) in
faithfully explaining the true class distribution. Additionally, GIST's
backtracking mechanism effectively mitigates overshooting the underlying
predictor's decision boundary, minimizing the spectral differences between the
input and the counterfactuals. These results challenge traditional forward
perturbation methods, offering a novel perspective that advances graph
explainability.

</details>


### [239] [Universal Biological Sequence Reranking for Improved De Novo Peptide Sequencing](https://arxiv.org/abs/2505.17552)
*Zijie Qiu,Jiaqi Wei,Xiang Zhang,Sheng Xu,Kai Zou,Zhi Jin,Zhiqiang Gao,Nanqing Dong,Siqi Sun*

Key words: de novo肽段测序, 深度重排框架, 多模型互补, PMD/RMD指标, 零样本泛化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了RankNovo，首个深度重排框架，通过结合多种测序模型的优势提升de novo肽段测序性能，引入新指标PMD和RMD，实验证明其超越现有方法并具备零样本泛化能力。

Motivation: 现有深度学习方法在质谱数据复杂性和噪声分布不均的局限下表现不佳，需通过多模型互补提升性能。

Method: 采用列表式重排策略，将候选肽段建模为多序列比对，利用轴向注意力提取特征，并设计PMD和RMD指标监督质量差异。

Result: RankNovo超越基线模型并刷新SOTA，且在未训练过的模型生成数据上展现零样本泛化能力。

Conclusion: 论文挑战了单模型范式，提出通用重排框架，推动de novo测序精度边界。

Abstract: De novo peptide sequencing is a critical task in proteomics. However, the
performance of current deep learning-based methods is limited by the inherent
complexity of mass spectrometry data and the heterogeneous distribution of
noise signals, leading to data-specific biases. We present RankNovo, the first
deep reranking framework that enhances de novo peptide sequencing by leveraging
the complementary strengths of multiple sequencing models. RankNovo employs a
list-wise reranking approach, modeling candidate peptides as multiple sequence
alignments and utilizing axial attention to extract informative features across
candidates. Additionally, we introduce two new metrics, PMD (Peptide Mass
Deviation) and RMD (residual Mass Deviation), which offer delicate supervision
by quantifying mass differences between peptides at both the sequence and
residue levels. Extensive experiments demonstrate that RankNovo not only
surpasses its base models used to generate training candidates for reranking
pre-training, but also sets a new state-of-the-art benchmark. Moreover,
RankNovo exhibits strong zero-shot generalization to unseen models whose
generations were not exposed during training, highlighting its robustness and
potential as a universal reranking framework for peptide sequencing. Our work
presents a novel reranking strategy that fundamentally challenges existing
single-model paradigms and advances the frontier of accurate de novo
sequencing. Our source code is provided on GitHub.

</details>


### [240] [CoMoE: Contrastive Representation for Mixture-of-Experts in Parameter-Efficient Fine-tuning](https://arxiv.org/abs/2505.17553)
*Jinyuan Feng,Chaopeng Wei,Tenghai Qiu,Tianyi Hu,Zhiqiang Pu*

Key words: 参数高效微调, 专家混合, 对比学习, 模块化, 异构数据

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出CoMoE方法，通过对比目标优化MoE的模块化和专家专业化，提升模型在异构数据集上的性能。

Motivation: 现有MoE方法在异构数据集上表现不足，专家可能学习相似知识，未能充分利用MoE的容量潜力。

Method: 提出CoMoE，在top-k路由中结合对比目标训练专家，通过采样激活与未激活专家来促进模块化。

Result: 实验表明，CoMoE能提升MoE容量并增强专家模块化，在多项基准和多任务设置中表现一致优越。

Conclusion: CoMoE通过对比学习有效解决了MoE在异构数据中的专家冗余问题，提升了模型的模块化和性能。

Abstract: In parameter-efficient fine-tuning, mixture-of-experts (MoE), which involves
specializing functionalities into different experts and sparsely activating
them appropriately, has been widely adopted as a promising approach to
trade-off between model capacity and computation overhead. However, current MoE
variants fall short on heterogeneous datasets, ignoring the fact that experts
may learn similar knowledge, resulting in the underutilization of MoE's
capacity. In this paper, we propose Contrastive Representation for MoE (CoMoE),
a novel method to promote modularization and specialization in MoE, where the
experts are trained along with a contrastive objective by sampling from
activated and inactivated experts in top-k routing. We demonstrate that such a
contrastive objective recovers the mutual-information gap between inputs and
the two types of experts. Experiments on several benchmarks and in multi-task
settings demonstrate that CoMoE can consistently enhance MoE's capacity and
promote modularization among the experts.

</details>


### [241] [Wildfire spread forecasting with Deep Learning](https://arxiv.org/abs/2505.17556)
*Nikolaos Anastasiou,Spyros Kondylatos,Ioannis Papoutsis*

Key words: 野火预测，深度学习，时空数据，遥感，气象观测

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种基于深度学习的框架，利用点火时可用的数据预测野火的最终燃烧范围，结果表明多日观测数据显著提高了预测准确性。

Motivation: 准确预测野火蔓延对风险管理、应急响应和资源分配至关重要。

Method: 研究使用了一个涵盖2006至2022年地中海地区的时空数据集，结合多种数据源，并通过消融实验评估时间上下文的影响。

Result: 最佳模型（点火前后多日数据）比仅用点火日数据的基线模型在F1分数和IoU上提升了近5%。

Conclusion: 多日观测数据能显著提升野火预测准确性，研究公开了数据集和模型以推动相关研究。

Abstract: Accurate prediction of wildfire spread is crucial for effective risk
management, emergency response, and strategic resource allocation. In this
study, we present a deep learning (DL)-based framework for forecasting the
final extent of burned areas, using data available at the time of ignition. We
leverage a spatio-temporal dataset that covers the Mediterranean region from
2006 to 2022, incorporating remote sensing data, meteorological observations,
vegetation maps, land cover classifications, anthropogenic factors, topography
data, and thermal anomalies. To evaluate the influence of temporal context, we
conduct an ablation study examining how the inclusion of pre- and post-ignition
data affects model performance, benchmarking the temporal-aware DL models
against a baseline trained exclusively on ignition-day inputs. Our results
indicate that multi-day observational data substantially improve predictive
accuracy. Particularly, the best-performing model, incorporating a temporal
window of four days before to five days after ignition, improves both the F1
score and the Intersection over Union by almost 5% in comparison to the
baseline on the test dataset. We publicly release our dataset and models to
enhance research into data-driven approaches for wildfire modeling and
response.

</details>


### [242] [Multiphysics Bench: Benchmarking and Investigating Scientific Machine Learning for Multiphysics PDEs](https://arxiv.org/abs/2505.17575)
*Changfan Yang,Lichen Bai,Yinpeng Wang,Shufei Zhang,Zeke Xie*

Key words: PDE, multiphysics, machine learning, dataset, benchmarking

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一个名为Multiphysics Bench的综合数据集，用于解决多物理场PDE问题，并系统性评估了多种机器学习求解器的性能，同时提供了实用技巧和未来研究方向。

Motivation: 现有机器学习研究主要关注单物理场PDE问题，而忽略了多物理场问题的复杂性和重要性。多物理场PDE涉及多个强耦合变量，带来额外挑战。

Method: 收集首个通用多物理场数据集Multiphysics Bench，系统性评估多种机器学习求解器（如PINNs、FNO等）在多物理场问题上的表现，并通过实验提出实用技巧。

Result: 现有求解器在多物理场问题上表现不佳，但通过实验提出了改进方法和实用技巧，为未来研究提供了方向。

Conclusion: 多物理场PDE问题的研究仍处于早期阶段，未来需要更多工作来解决这些复杂系统的挑战。

Abstract: Solving partial differential equations (PDEs) with machine learning has
recently attracted great attention, as PDEs are fundamental tools for modeling
real-world systems that range from fundamental physical science to advanced
engineering disciplines. Most real-world physical systems across various
disciplines are actually involved in multiple coupled physical fields rather
than a single field. However, previous machine learning studies mainly focused
on solving single-field problems, but overlooked the importance and
characteristics of multiphysics problems in real world. Multiphysics PDEs
typically entail multiple strongly coupled variables, thereby introducing
additional complexity and challenges, such as inter-field coupling. Both
benchmarking and solving multiphysics problems with machine learning remain
largely unexamined. To identify and address the emerging challenges in
multiphysics problems, we mainly made three contributions in this work. First,
we collect the first general multiphysics dataset, the Multiphysics Bench, that
focuses on multiphysics PDE solving with machine learning. Multiphysics Bench
is also the most comprehensive PDE dataset to date, featuring the broadest
range of coupling types, the greatest diversity of PDE formulations, and the
largest dataset scale. Second, we conduct the first systematic investigation on
multiple representative learning-based PDE solvers, such as PINNs, FNO,
DeepONet, and DiffusionPDE solvers, on multiphysics problems. Unfortunately,
naively applying these existing solvers usually show very poor performance for
solving multiphysics. Third, through extensive experiments and discussions, we
report multiple insights and a bag of useful tricks for solving multiphysics
with machine learning, motivating future directions in the study and simulation
of complex, coupled physical systems.

</details>


### [243] [Ownership Verification of DNN Models Using White-Box Adversarial Attacks with Specified Probability Manipulation](https://arxiv.org/abs/2505.17579)
*Teruki Sano,Minoru Kuribayashi,Masao Sakai,Shuji Ishobe,Eisuke Koizumi*

Key words: 深度神经网络,所有权验证,对抗攻击,图像分类,FGSM

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种新颖的框架，用于验证深度神经网络（DNN）模型在图像分类任务中的所有权。该框架允许所有者和第三方在不展示原始模型的情况下验证模型身份。

Motivation: 为了解决DNN模型被非法复制并在云环境中提供服务的问题，需要一种无需原始模型即可验证所有权的方法。

Method: 提出了一种基于白盒对抗攻击的方法，通过迭代Fast Gradient Sign Method（FGSM）引入控制参数，调整特定类别的输出概率至指定值。

Result: 实验结果表明，该框架能有效识别DNN模型。

Conclusion: 该框架为DNN模型所有权验证提供了一种简单而有效的方法。

Abstract: In this paper, we propose a novel framework for ownership verification of
deep neural network (DNN) models for image classification tasks. It allows
verification of model identity by both the rightful owner and third party
without presenting the original model. We assume a gray-box scenario where an
unauthorized user owns a model that is illegally copied from the original
model, provides services in a cloud environment, and the user throws images and
receives the classification results as a probability distribution of output
classes. The framework applies a white-box adversarial attack to align the
output probability of a specific class to a designated value. Due to the
knowledge of original model, it enables the owner to generate such adversarial
examples. We propose a simple but effective adversarial attack method based on
the iterative Fast Gradient Sign Method (FGSM) by introducing control
parameters. Experimental results confirm the effectiveness of the
identification of DNN models using adversarial attack.

</details>


### [244] [MinkUNeXt-SI: Improving point cloud-based place recognition including spherical coordinates and LiDAR intensity](https://arxiv.org/abs/2505.17591)
*Judith Vilella-Cantos,Juan José Cabrera,Luis Payá,Mónica Ballesta,David Valiente*

Key words: 地点识别, LiDAR, Minkowski卷积, U-net, 深度学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出MinkUNeXt-SI方法，基于LiDAR点云数据，结合Minkowski卷积和U-net架构，实现高精度、可泛化的地点识别。

Motivation: 解决地点识别问题在自主导航系统中的关键性，需应对场景变化（季节、天气）和环境差异的挑战。

Method: 通过预处理LiDAR点云为球坐标和强度值，结合Minkowski卷积与U-net架构（含跳跃连接）生成鲁棒的地点描述符。

Result: 性能超越现有方法，且能泛化至其他数据集；自定义数据集验证效果优异。代码与数据已开源。

Conclusion: MinkUNeXt-SI为地点识别提供了高效、通用的解决方案，适用复杂场景。

Abstract: In autonomous navigation systems, the solution of the place recognition
problem is crucial for their safe functioning. But this is not a trivial
solution, since it must be accurate regardless of any changes in the scene,
such as seasonal changes and different weather conditions, and it must be
generalizable to other environments. This paper presents our method,
MinkUNeXt-SI, which, starting from a LiDAR point cloud, preprocesses the input
data to obtain its spherical coordinates and intensity values normalized within
a range of 0 to 1 for each point, and it produces a robust place recognition
descriptor. To that end, a deep learning approach that combines Minkowski
convolutions and a U-net architecture with skip connections is used. The
results of MinkUNeXt-SI demonstrate that this method reaches and surpasses
state-of-the-art performance while it also generalizes satisfactorily to other
datasets. Additionally, we showcase the capture of a custom dataset and its use
in evaluating our solution, which also achieves outstanding results. Both the
code of our solution and the runs of our dataset are publicly available for
reproducibility purposes.

</details>


### [245] [NeUQI: Near-Optimal Uniform Quantization Parameter Initialization](https://arxiv.org/abs/2505.17595)
*Li Lin,Xinyu Hu,Xiaojun Wan*

Key words: 大型语言模型、均匀量化、初始化参数、部署优化、轻量蒸馏

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出NeUQI方法，通过高效确定均匀量化的近优初始参数，显著提升大型语言模型在消费级GPU或个人设备上的部署性能，并优于现有方法。

Motivation: 大型语言模型（LLM）在消费级GPU或个人设备上部署时面临内存消耗高和推理成本大的问题，而现有的均匀量化初始化方法仍依赖于次优的Min-Max策略，亟需改进。

Method: 提出NeUQI方法，专注于高效确定均匀量化的近优初始参数，并可无缝集成到现有量化方法中。结合轻量级蒸馏策略进一步优化性能。

Result: 在LLaMA和Qwen系列模型上的实验表明，NeUQI始终优于现有方法，且结合轻量蒸馏后性能甚至超越资源密集型的PV-tuning。

Conclusion: NeUQI为均匀量化提供了一种高效的参数初始化方案，显著提升了LLM在资源受限设备上的部署性能。

Abstract: Large language models (LLMs) achieve impressive performance across domains
but face significant challenges when deployed on consumer-grade GPUs or
personal devices such as laptops, due to high memory consumption and inference
costs. Post-training quantization (PTQ) of LLMs offers a promising solution
that reduces their memory footprint and decoding latency. In practice, PTQ with
uniform quantization representation is favored for its efficiency and ease of
deployment since uniform quantization is widely supported by mainstream
hardware and software libraries. Recent studies on $\geq 2$-bit uniform
quantization have led to noticeable improvements in post-quantization model
performance; however, they primarily focus on quantization methodologies, while
the initialization of quantization parameters is underexplored and still relies
on the suboptimal Min-Max strategies. In this work, we propose NeUQI, a method
devoted to efficiently determining near-optimal initial parameters for uniform
quantization. NeUQI is orthogonal to prior quantization methodologies and can
seamlessly integrate with them. The experiments with the LLaMA and Qwen
families on various tasks demonstrate that our NeUQI consistently outperforms
existing methods. Furthermore, when combined with a lightweight distillation
strategy, NeUQI can achieve superior performance to PV-tuning, a much more
resource-intensive approach.

</details>


### [246] [Dynamic Text Bundling Supervision for Zero-Shot Inference on Text-Attributed Graphs](https://arxiv.org/abs/2505.17599)
*Yusheng Zhao,Qixin Zhang,Xiao Luo,Weizhi Zhang,Zhiping Xiao,Wei Ju,Philip S. Yu,Ming Zhang*

Key words: 大规模语言模型，图神经网络，动态文本捆绑，监督学习，零样本学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种名为DENSE的新方法，通过将文本捆绑并利用LLMs生成标签来监督图神经网络，解决了图结构中信息有限和LLMs响应不可靠的问题。

Motivation: 大规模语言模型（LLMs）在图结构文本数据（TAGs）中的应用面临两大挑战：图结构信息有限和模型响应不可靠。本文旨在解决这些问题，提升模型性能。

Method: 提出了Dynamic Text Bundling Supervision（DENSE）方法，通过将邻近节点的文本捆绑并查询LLMs生成标签，再用这些标签监督图神经网络的优化，并进一步排除噪声数据。

Result: 在十个数据集上的广泛实验验证了DENSE的有效性。

Conclusion: DENSE通过动态文本捆绑和LLMs的协同优化，显著提升了图神经网络的性能。

Abstract: Large language models (LLMs) have been used in many zero-shot learning
problems, with their strong generalization ability. Recently, adopting LLMs in
text-attributed graphs (TAGs) has drawn increasing attention. However, the
adoption of LLMs faces two major challenges: limited information on graph
structure and unreliable responses. LLMs struggle with text attributes isolated
from the graph topology. Worse still, they yield unreliable predictions due to
both information insufficiency and the inherent weakness of LLMs (e.g.,
hallucination). Towards this end, this paper proposes a novel method named
Dynamic Text Bundling Supervision (DENSE) that queries LLMs with bundles of
texts to obtain bundle-level labels and uses these labels to supervise graph
neural networks. Specifically, we sample a set of bundles, each containing a
set of nodes with corresponding texts of close proximity. We then query LLMs
with the bundled texts to obtain the label of each bundle. Subsequently, the
bundle labels are used to supervise the optimization of graph neural networks,
and the bundles are further refined to exclude noisy items. To justify our
design, we also provide theoretical analysis of the proposed method. Extensive
experiments across ten datasets validate the effectiveness of the proposed
method.

</details>


### [247] [Adaptive Semantic Token Communication for Transformer-based Edge Inference](https://arxiv.org/abs/2505.17604)
*Alessio Devoto,Jary Pomponi,Mattia Merluzzi,Paolo Di Lorenzo,Simone Scardapane*

Key words: 边缘推理、语义通信、联合源信道编码、变压器、动态压缩

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文介绍了一种基于动态可配置变压器的深度联合源信道编码框架，用于边缘推理，能自适应调整传输策略以适应网络条件，表现优于现有基线。

Motivation: 动机在于解决资源受限边缘设备在动态带宽和信道条件下高效传输任务感知数据的挑战。

Method: 方法包括使用变压器精炼语义表示、动态选择语义标记、联合源信道编码模块压缩，并结合Lyapunov随机优化算法进行资源分配。

Result: 实验结果表明，该系统在多个方面优于现有方法。

Conclusion: 结论认为该框架为边缘智能应用中的语义通信提供了有力基础。

Abstract: This paper presents an adaptive framework for edge inference based on a
dynamically configurable transformer-powered deep joint source channel coding
(DJSCC) architecture. Motivated by a practical scenario where a resource
constrained edge device engages in goal oriented semantic communication, such
as selectively transmitting essential features for object detection to an edge
server, our approach enables efficient task aware data transmission under
varying bandwidth and channel conditions. To achieve this, input data is
tokenized into compact high level semantic representations, refined by a
transformer, and transmitted over noisy wireless channels. As part of the DJSCC
pipeline, we employ a semantic token selection mechanism that adaptively
compresses informative features into a user specified number of tokens per
sample. These tokens are then further compressed through the JSCC module,
enabling a flexible token communication strategy that adjusts both the number
of transmitted tokens and their embedding dimensions. We incorporate a resource
allocation algorithm based on Lyapunov stochastic optimization to enhance
robustness under dynamic network conditions, effectively balancing compression
efficiency and task performance. Experimental results demonstrate that our
system consistently outperforms existing baselines, highlighting its potential
as a strong foundation for AI native semantic communication in edge
intelligence applications.

</details>


### [248] [Learning Equilibria from Data: Provably Efficient Multi-Agent Imitation Learning](https://arxiv.org/abs/2505.17610)
*Till Freihaut,Luca Viano,Volkan Cevher,Matthieu Geist,Giorgia Ramponi*

Key words: 纳什均衡, 马尔可夫博弈, 专家数据, 模仿学习, 样本复杂度

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文首次在马尔可夫博弈中通过专家数据学习纳什均衡的样本复杂度进行了专家级刻画，提出新的单策略偏差可集中性系数，并开发了两种新算法MAIL-BRO和MURMAIL，分别实现不同的查询复杂度。

Motivation: 为了解决在非交互式模仿学习环境中通过专家数据学习纳什均衡的样本复杂度问题，并克服高集中性系数下行为克隆的显著遗憾。

Method: 提出了单策略偏差可集中性系数的概念，并基于此开发了两种新算法MAIL-BRO（使用最佳响应预言机）和MURMAIL（无需预言机但查询复杂度更高）。

Result: MAIL-BRO以O(ε^-4)的专家和预言机查询复杂度学习ε-纳什均衡，MURMAIL则达到O(ε^-8)的查询复杂度。数值实验验证了理论结果。

Conclusion: 论文证明了集中性系数的必要性，并提出高效算法解决非交互式模仿学习中的复杂性问题。

Abstract: This paper provides the first expert sample complexity characterization for
learning a Nash equilibrium from expert data in Markov Games. We show that a
new quantity named the single policy deviation concentrability coefficient is
unavoidable in the non-interactive imitation learning setting, and we provide
an upper bound for behavioral cloning (BC) featuring such coefficient. BC
exhibits substantial regret in games with high concentrability coefficient,
leading us to utilize expert queries to develop and introduce two novel
solution algorithms: MAIL-BRO and MURMAIL. The former employs a best response
oracle and learns an $\varepsilon$-Nash equilibrium with
$\mathcal{O}(\varepsilon^{-4})$ expert and oracle queries. The latter bypasses
completely the best response oracle at the cost of a worse expert query
complexity of order $\mathcal{O}(\varepsilon^{-8})$. Finally, we provide
numerical evidence, confirming our theoretical findings.

</details>


### [249] [Large language model as user daily behavior data generator: balancing population diversity and individual personality](https://arxiv.org/abs/2505.17615)
*Haoxin Li,Jingtao Ding,Jiahui Gong,Yong Li*

Key words: 合成数据生成, 行为预测, 隐私保护, 大语言模型, 数据增强

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了BehaviorGen框架，利用大语言模型生成高质量合成行为数据，解决行为预测中的数据隐私和不足问题，提升预测效果。

Motivation: 预测人类日常行为的复杂性及现有方法对敏感数据的依赖问题，催生了无需真实用户数据的隐私保护合成数据生成方案的需求。

Method: 提出BehaviorGen框架，基于用户画像和真实事件，利用大语言模型生成合成行为数据，支持数据增强和替换。

Result: 在人类移动和智能手机使用预测任务中，BehaviorGen最多提升18.9%的预测性能。

Conclusion: BehaviorGen通过灵活且保护隐私的合成数据生成方法，有效提升了用户行为建模的潜力。

Abstract: Predicting human daily behavior is challenging due to the complexity of
routine patterns and short-term fluctuations. While data-driven models have
improved behavior prediction by leveraging empirical data from various
platforms and devices, the reliance on sensitive, large-scale user data raises
privacy concerns and limits data availability. Synthetic data generation has
emerged as a promising solution, though existing methods are often limited to
specific applications. In this work, we introduce BehaviorGen, a framework that
uses large language models (LLMs) to generate high-quality synthetic behavior
data. By simulating user behavior based on profiles and real events,
BehaviorGen supports data augmentation and replacement in behavior prediction
models. We evaluate its performance in scenarios such as pertaining
augmentation, fine-tuning replacement, and fine-tuning augmentation, achieving
significant improvements in human mobility and smartphone usage predictions,
with gains of up to 18.9%. Our results demonstrate the potential of BehaviorGen
to enhance user behavior modeling through flexible and privacy-preserving
synthetic data generation.

</details>


### [250] [Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration](https://arxiv.org/abs/2505.17621)
*Jingtong Gao,Ling Pan,Yejing Wang,Rui Zhong,Chi Lu,Qingpeng Cai,Peng Jiang,Xiangyu Zhao*

Key words: 强化学习, 大型语言模型, 多步推理, 探索奖励, i-MENTOR

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出i-MENTOR方法，通过密集奖励和增强探索来解决强化学习在大型语言模型中的稀疏奖励和探索不足问题，效果显著。

Motivation: 现有RL方法（如PPO和GRPO）依赖稀疏奖励且缺乏有效探索机制，导致多步推理效率低下，需要改进。

Method: 提出i-MENTOR方法，包含轨迹感知探索奖励、动态奖励缩放和优势保持奖励实现三项创新。

Result: 在三个公开数据集上，i-MENTOR表现优异，尤其在困难数据集Countdown-4上提升22.39%。

Conclusion: i-MENTOR通过密集奖励和探索增强，显著提升了RL在LLM多步推理任务中的性能。

Abstract: Reinforcement learning (RL) has emerged as a pivotal method for improving the
reasoning capabilities of Large Language Models (LLMs). However, prevalent RL
approaches such as Proximal Policy Optimization (PPO) and Group-Regularized
Policy Optimization (GRPO) face critical limitations due to their reliance on
sparse outcome-based rewards and inadequate mechanisms for incentivizing
exploration. These limitations result in inefficient guidance for multi-step
reasoning processes. Specifically, sparse reward signals fail to deliver
effective or sufficient feedback, particularly for challenging problems.
Furthermore, such reward structures induce systematic biases that prioritize
exploitation of familiar trajectories over novel solution discovery. These
shortcomings critically hinder performance in complex reasoning tasks, which
inherently demand iterative refinement across ipntermediate steps. To address
these challenges, we propose an Intrinsic Motivation guidEd exploratioN meThOd
foR LLM Reasoning (i-MENTOR), a novel method designed to both deliver dense
rewards and amplify explorations in the RL-based training paradigm. i-MENTOR
introduces three key innovations: trajectory-aware exploration rewards that
mitigate bias in token-level strategies while maintaining computational
efficiency; dynamic reward scaling to stabilize exploration and exploitation in
large action spaces; and advantage-preserving reward implementation that
maintains advantage distribution integrity while incorporating exploratory
guidance. Experiments across three public datasets demonstrate i-MENTOR's
effectiveness with a 22.39% improvement on the difficult dataset Countdown-4.

</details>


### [251] [Leveraging Stochastic Depth Training for Adaptive Inference](https://arxiv.org/abs/2505.17626)
*Guilherme Korol,Antonio Carlos Schneider Beck,Jeronimo Castrillon*

Key words: 动态DNN优化, Stochastic Depth, 自适应推理, 能效提升, ResNet

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种通过Stochastic Depth训练的模型来实现高效自适应推理的方法，解决了动态DNN优化中的内存、训练复杂性和性能控制问题。

Motivation: 动态DNN优化技术（如层跳过）虽然提高了适应性和效率，但存在内存占用高、训练复杂且难以控制性能-质量权衡的问题。为应对这些问题，本文提出了一种零开销、单模型且时间可预测的推理替代方案。

Method: 利用Stochastic Depth训练的模型对任意层跳过的鲁棒性，从训练好的模型中选择接近帕累托最优的跳过配置，在运行时自适应调整推理。

Result: 与原始ResNet相比，该方法在仅降低0.71%准确率的情况下，提升了高达2倍的能效。

Conclusion: 通过Stochastic Depth训练的模型可以有效解决动态DNN优化的主要挑战，并提供了一种高效的自适应推理方案。

Abstract: Dynamic DNN optimization techniques such as layer-skipping offer increased
adaptability and efficiency gains but can lead to i) a larger memory footprint
as in decision gates, ii) increased training complexity (e.g., with
non-differentiable operations), and iii) less control over performance-quality
trade-offs due to its inherent input-dependent execution. To approach these
issues, we propose a simpler yet effective alternative for adaptive inference
with a zero-overhead, single-model, and time-predictable inference. Central to
our approach is the observation that models trained with Stochastic Depth -- a
method for faster training of residual networks -- become more resilient to
arbitrary layer-skipping at inference time. We propose a method to first select
near Pareto-optimal skipping configurations from a stochastically-trained model
to adapt the inference at runtime later. Compared to original ResNets, our
method shows improvements of up to 2X in power efficiency at accuracy drops as
low as 0.71%.

</details>


### [252] [Surfacing Semantic Orthogonality Across Model Safety Benchmarks: A Multi-Dimensional Analysis](https://arxiv.org/abs/2505.17636)
*Jonathan Bennion,Shaona Ghosh,Mantek Singh,Nouha Dziri*

Key words: AI安全数据集、LLMs、UMAP、kmeans聚类、语义正交性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文通过分析五个开源安全基准，使用UMAP降维和kmeans聚类识别了六种主要伤害类别，揭示了基准间的语义正交性，为未来AI安全数据集的开发提供了量化框架。

Motivation: 现有AI安全数据集在衡量LLMs对不断发展伤害解释的适应性方面存在不足，需要量化基准间的语义正交性以提高透明度和针对性。

Method: 采用UMAP降维和kmeans聚类技术，分析五个开源安全基准的语义分布，识别伤害类别并评估基准间的正交性。

Result: 识别出六种伤害类别，不同基准在伤害关注点上有显著差异（如GretelAI侧重隐私，WildGuardMix侧重自残），基准间存在语义正交性（轮廓分数0.470）。

Conclusion: 提出的量化框架可帮助开发更全面的AI安全数据集，填补现有基准的覆盖空白，适应未来AI伤害定义的演变。

Abstract: Various AI safety datasets have been developed to measure LLMs against
evolving interpretations of harm. Our evaluation of five recently published
open-source safety benchmarks reveals distinct semantic clusters using UMAP
dimensionality reduction and kmeans clustering (silhouette score: 0.470). We
identify six primary harm categories with varying benchmark representation.
GretelAI, for example, focuses heavily on privacy concerns, while WildGuardMix
emphasizes self-harm scenarios. Significant differences in prompt length
distribution suggests confounds to data collection and interpretations of harm
as well as offer possible context. Our analysis quantifies benchmark
orthogonality among AI benchmarks, allowing for transparency in coverage gaps
despite topical similarities. Our quantitative framework for analyzing semantic
orthogonality across safety benchmarks enables more targeted development of
datasets that comprehensively address the evolving landscape of harms in AI
use, however that is defined in the future.

</details>


### [253] [Causal Spatio-Temporal Prediction: An Effective and Efficient Multi-Modal Approach](https://arxiv.org/abs/2505.17637)
*Yuting Huang,Ziquan Fang,Zhihao Zeng,Lu Chen,Yunjun Gao*

Key words: 时空预测, 多模态融合, 因果推断, 计算效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: E^2-CSTP是一个高效、因果驱动的多模态时空预测框架，通过跨模态注意力和门控机制融合数据，并使用双分支因果推理减少偏差，显著提升了预测精度并降低了计算开销。

Motivation: 时空预测在多个领域至关重要，但现有方法在多模态信息融合、因果关系建模和计算效率方面存在不足。

Method: 提出E^2-CSTP框架，结合跨模态注意力和门控机制，采用双分支因果推理（主分支预测，辅助分支去偏），并整合GCN与Mamba架构优化编码效率。

Result: 在4个真实数据集上，E^2-CSTP显著优于9种先进方法，预测精度提升最高9.66%，计算开销降低17.37%-56.11%。

Conclusion: E^2-CSTP通过多模态融合和因果干预有效提升了时空预测的准确性和效率。

Abstract: Spatio-temporal prediction plays a crucial role in intelligent
transportation, weather forecasting, and urban planning. While integrating
multi-modal data has shown potential for enhancing prediction accuracy, key
challenges persist: (i) inadequate fusion of multi-modal information, (ii)
confounding factors that obscure causal relations, and (iii) high computational
complexity of prediction models. To address these challenges, we propose
E^2-CSTP, an Effective and Efficient Causal multi-modal Spatio-Temporal
Prediction framework. E^2-CSTP leverages cross-modal attention and gating
mechanisms to effectively integrate multi-modal data. Building on this, we
design a dual-branch causal inference approach: the primary branch focuses on
spatio-temporal prediction, while the auxiliary branch mitigates bias by
modeling additional modalities and applying causal interventions to uncover
true causal dependencies. To improve model efficiency, we integrate GCN with
the Mamba architecture for accelerated spatio-temporal encoding. Extensive
experiments on 4 real-world datasets show that E^2-CSTP significantly
outperforms 9 state-of-the-art methods, achieving up to 9.66% improvements in
accuracy as well as 17.37%-56.11% reductions in computational overhead.

</details>


### [254] [Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical Regularization in Training](https://arxiv.org/abs/2505.17638)
*Tony Bonnaire,Raphaël Urfin,Giulio Biroli,Marc Mézard*

Key words: 扩散模型, 训练动态, 泛化, 记忆化, 隐式正则化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文探讨了扩散模型训练动态中从泛化到记忆化的转变机制，揭示了两个关键时间尺度：τ_gen（早期的高质量样本生成）和 τ_mem（后期的记忆化出现），并提出了一种隐式动态正则化的概念。

Motivation: 理解扩散模型如何避免记忆化训练数据并实现泛化的机制是关键挑战。

Method: 通过实验和理论分析，识别了 τ_gen 和 τ_mem 两个时间尺度，并验证了它们与训练集大小的关系。

Result: 发现 τ_mem 随着训练集大小线性增长，而 τ_gen 保持恒定，形成了有效的泛化窗口。

Conclusion: 研究揭示了训练动态中的隐式正则化机制，使得高参数化设置下也能避免记忆化。

Abstract: Diffusion models have achieved remarkable success across a wide range of
generative tasks. A key challenge is understanding the mechanisms that prevent
their memorization of training data and allow generalization. In this work, we
investigate the role of the training dynamics in the transition from
generalization to memorization. Through extensive experiments and theoretical
analysis, we identify two distinct timescales: an early time
$\tau_\mathrm{gen}$ at which models begin to generate high-quality samples, and
a later time $\tau_\mathrm{mem}$ beyond which memorization emerges. Crucially,
we find that $\tau_\mathrm{mem}$ increases linearly with the training set size
$n$, while $\tau_\mathrm{gen}$ remains constant. This creates a growing window
of training times with $n$ where models generalize effectively, despite showing
strong memorization if training continues beyond it. It is only when $n$
becomes larger than a model-dependent threshold that overfitting disappears at
infinite training times. These findings reveal a form of implicit dynamical
regularization in the training dynamics, which allow to avoid memorization even
in highly overparameterized settings. Our results are supported by numerical
experiments with standard U-Net architectures on realistic and synthetic
datasets, and by a theoretical analysis using a tractable random features model
studied in the high-dimensional limit.

</details>


### [255] [PreMoe: Lightening MoEs on Constrained Memory by Expert Pruning and Retrieval](https://arxiv.org/abs/2505.17639)
*Zehua Pei,Ying Zhang,Hui-Ling Zhen,Xianzhi Yu,Wulong Liu,Sinno Jialin Pan,Mingxuan Yuan,Bei Yu*

Key words: MoE, PreMoe, 专家剪枝, 任务自适应, 内存优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究提出PreMoe框架，通过专家剪枝和任务自适应检索，显著减少大型MoE模型的内存占用，同时保持高精度。

Motivation: 解决大型MoE模型在内存受限环境中的部署问题，利用任务特定的专家激活模式优化资源使用。

Method: 采用概率专家剪枝（PEP）和任务自适应专家检索（TAER），基于任务条件选择关键专家。

Result: DeepSeek-R1和Pangu-Ultra-MoE在剪枝后仍保持高准确率，如97.2%和96.95%。

Conclusion: PreMoe有效降低了内存需求，为大型MoE模型的广泛部署提供了可行方案。

Abstract: Mixture-of-experts (MoE) architectures enable scaling large language models
(LLMs) to vast parameter counts without a proportional rise in computational
costs. However, the significant memory demands of large MoE models hinder their
deployment across various computational environments, from cloud servers to
consumer devices. This study first demonstrates pronounced task-specific
specialization in expert activation patterns within MoE layers. Building on
this, we introduce PreMoe, a novel framework that enables efficient deployment
of massive MoE models in memory-constrained environments. PreMoe features two
main components: probabilistic expert pruning (PEP) and task-adaptive expert
retrieval (TAER). PEP employs a new metric, the task-conditioned expected
selection score (TCESS), derived from router logits to quantify expert
importance for specific tasks, thereby identifying a minimal set of critical
experts. TAER leverages these task-specific expert importance profiles for
efficient inference. It pre-computes and stores compact expert patterns for
diverse tasks. When a user query is received, TAER rapidly identifies the most
relevant stored task pattern and reconstructs the model by loading only the
small subset of experts crucial for that task. This approach dramatically
reduces the memory footprint across all deployment scenarios. DeepSeek-R1 671B
maintains 97.2\% accuracy on MATH500 when pruned to 8/128 configuration (50\%
expert reduction), and still achieves 72.0\% with aggressive 8/32 pruning
(87.5\% expert reduction). Pangu-Ultra-MoE 718B achieves 97.15\% on MATH500 and
81.3\% on AIME24 with 8/128 pruning, while even more aggressive pruning to 4/64
(390GB memory) preserves 96.95\% accuracy on MATH500. We make our code publicly
available at https://github.com/JarvisPei/PreMoe.

</details>


### [256] [A Network Science Approach to Granular Time Series Segmentation](https://arxiv.org/abs/2505.17640)
*Ivana Kesić,Carolina Fortuna,Mihael Mohorčič,Blaž Bertalanič*

Key words: 时间序列分割（TSS）、加权双视角可见性图（WDPVG）、图注意力网络（GAT）、节点分类、图表示学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于加权双视角可见性图（WDPVG）和图注意力网络（GAT）的时间序列分割（TSS）方法，克服了传统滑动窗口方法的局限性，并在多个基准数据集上实现了优异的性能。

Motivation: 时间序列分割（TSS）在时间序列分析中受到的关注较少，且现有的深度学习方法依赖滑动窗口，限制了分割的粒度。本文旨在提出一种更精细的TSS方法。

Method: 通过将时间序列转换为加权双视角可见性图（WDPVG），并结合图注意力网络（GAT），将TSS问题建模为图上的节点分类问题。此外，文章还比较了多种时间序列到图的转换方法。

Result: 该方法在59个TSS基准数据集上的平均F1得分为0.97，优于基线方法（seq2point）的F1得分（高出0.05），同时减少了所需的训练数据。

Conclusion: 该方法通过图表示学习和图注意力网络，有效识别时间序列中的有意义分段，为TSS任务提供了新的解决方案。

Abstract: Time series segmentation (TSS) is one of the time series (TS) analysis
techniques, that has received considerably less attention compared to other TS
related tasks. In recent years, deep learning architectures have been
introduced for TSS, however their reliance on sliding windows limits
segmentation granularity due to fixed window sizes and strides. To overcome
these challenges, we propose a new more granular TSS approach that utilizes the
Weighted Dual Perspective Visbility Graph (WDPVG) TS into a graph and combines
it with a Graph Attention Network (GAT). By transforming TS into graphs, we are
able to capture different structural aspects of the data that would otherwise
remain hidden. By utilizing the representation learning capabilities of Graph
Neural Networks, our method is able to effectively identify meaningful segments
within the TS. To better understand the potential of our approach, we also
experimented with different TS-to-graph transformations and compared their
performance. Our contributions include: a) formulating the TSS as a node
classification problem on graphs; b) conducting an extensive analysis of
various TS- to-graph transformations applied to TSS using benchmark datasets
from the TSSB repository; c) providing the first detailed study on utilizing
GNNs for analyzing graph representations of TS in the context of TSS; d)
demonstrating the effectiveness of our method, which achieves an average F1
score of 0.97 across 59 diverse TSS benchmark datasets; e) outperforming the
seq2point baseline method by 0.05 in terms of F1 score; and f) reducing the
required training data compared to the baseline methods.

</details>


### [257] [Understanding Pre-training and Fine-tuning from Loss Landscape Perspectives](https://arxiv.org/abs/2505.17646)
*Huanran Chen,Yinpeng Dong,Zeming Wei,Yao Huang,Yichi Zhang,Hang Su,Jun Zhu*

Key words: 大规模语言模型, 损失景观, 预训练, 微调, 鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究了大规模语言模型的损失景观，发现预训练形成‘基础能力’盆地，微调在其内形成‘特定能力’盆地（如数学、安全、编码）。研究表明，只要微调在‘多数情况’盆地内，不会损害原有能力，且可通过过参数化轻松扩大盆地规模。

Motivation: 探索大规模语言模型的损失景观特性，揭示预训练和微调对模型能力的影响，为模型鲁棒性和安全性提供理论基础。

Method: 研究两类损失景观：多数情况景观和最坏情况景观，通过理论分析证明盆地大小与鲁棒性的关系，并利用过参数化特性扩展盆地规模。

Result: 发现预训练和微调形成不同层次的‘能力盆地’，理论证明了多数情况盆地大小可限制最坏情况盆地及输入扰动的鲁棒性，且能通过过参数化显著扩大盆地。

Conclusion: 损失景观的盆地结构为模型能力提供了理论保障，过参数化是增强模型鲁棒性的有效手段。

Abstract: Recent studies have revealed that the loss landscape of large language models
resembles a basin, within which the models perform nearly identically, and
outside of which they lose all their capabilities. In this work, we conduct
further studies on the loss landscape of large language models. We discover
that pre-training creates a "basic capability" basin, and subsequent
fine-tuning creates "specific capability" basins (e.g., math, safety, coding)
within the basic capability basin. We further investigate two types of loss
landscapes: the most-case landscape (i.e., the landscape along most directions)
and the worst-case landscape (i.e., the landscape along the worst direction).
We argue that as long as benign fine-tuning remains within the most-case basin,
it will not compromise previous capabilities. Similarly, any fine-tuning
(including the adversarial one) that stays within the worst-case basin would
not compromise previous capabilities. Finally, we theoretically demonstrate
that the size of the most-case basin can bound the size of the worst-case basin
and the robustness with respect to input perturbations. We also show that, due
to the over-parameterization property of current large language models, one can
easily enlarge the basins by five times.

</details>


### [258] [Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective](https://arxiv.org/abs/2505.17652)
*Deyang Kong,Qi Guo,Xiangyu Xi,Wei Wang,Jingang Wang,Xunliang Cai,Shikun Zhang,Wei Ye*

Key words: 强化学习, 能力-难度对齐, 样本效率, 大语言模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为CDAS的新方法，通过动态调整问题难度与模型能力的匹配，显著提升强化学习的样本效率和推理能力。

Motivation: 解决现有强化学习方法在样本效率低及问题难度评估不稳定、偏差大的问题。

Method: 提出CDAS方法，通过历史表现差异准确评估问题难度，并用定点系统自适应选择与模型当前能力匹配的问题。

Result: 在多个数学基准测试中，CDAS在准确性和效率上均显著优于基线方法，比竞争对手快2.33倍。

Conclusion: CDAS通过能力与难度对齐的策略，有效提升了强化学习的效率和性能。

Abstract: Reinforcement learning exhibits potential in enhancing the reasoning
abilities of large language models, yet it is hard to scale for the low sample
efficiency during the rollout phase. Existing methods attempt to improve
efficiency by scheduling problems based on problem difficulties. However, these
approaches suffer from unstable and biased estimations of problem difficulty
and fail to capture the alignment between model competence and problem
difficulty in RL training, leading to suboptimal results. To tackle these
limitations, this paper introduces \textbf{C}ompetence-\textbf{D}ifficulty
\textbf{A}lignment \textbf{S}ampling (\textbf{CDAS}), which enables accurate
and stable estimation of problem difficulties by aggregating historical
performance discrepancies of problems. Then the model competence is quantified
to adaptively select problems whose difficulty is in alignment with the model's
current competence using a fixed-point system. Experimental results across a
range of challenging mathematical benchmarks show that CDAS achieves great
improvements in both accuracy and efficiency. CDAS attains the highest average
accuracy against baselines and exhibits significant speed advantages compared
to Dynamic Sampling, a competitive strategy in DAPO, which is \textbf{2.33}
times slower than CDAS.

</details>


### [259] [DAM-GT: Dual Positional Encoding-Based Attention Masking Graph Transformer for Node Classification](https://arxiv.org/abs/2505.17660)
*Chenyang Li,Jinsong Chen,John E. Hopcroft,Kun He*

Key words: 图 Transformer, 节点分类, 双位置编码, 注意力掩码, 属性相关性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: DAM-GT 通过双位置编码和注意力掩码策略改进了图 Transformer，解决了现有方法在节点分类任务中捕捉属性相关性和注意力分散的问题，显著提升了性能。

Motivation: 现有邻域标记生成方法未能充分捕获邻域内的属性相关性，且传统自注意力机制在处理邻域标记时存在注意力分散问题，DAM-GT 旨在解决这两个关键限制。

Method: DAM-GT 提出了一种双位置编码方案（结合属性聚类策略）和新的注意力掩码机制，以优化目标节点与邻域标记的交互。

Result: 实验表明，DAM-GT 在不同同质性和规模的图上均优于最先进方法。

Conclusion: DAM-GT 通过双位置编码和注意力掩码策略有效提升了图 Transformer 在节点分类任务中的性能。

Abstract: Neighborhood-aware tokenized graph Transformers have recently shown great
potential for node classification tasks. Despite their effectiveness, our
in-depth analysis of neighborhood tokens reveals two critical limitations in
the existing paradigm. First, current neighborhood token generation methods
fail to adequately capture attribute correlations within a neighborhood.
Second, the conventional self-attention mechanism suffers from attention
diversion when processing neighborhood tokens, where high-hop neighborhoods
receive disproportionate focus, severely disrupting information interactions
between the target node and its neighborhood tokens. To address these
challenges, we propose DAM-GT, Dual positional encoding-based Attention Masking
graph Transformer. DAM-GT introduces a novel dual positional encoding scheme
that incorporates attribute-aware encoding via an attribute clustering
strategy, effectively preserving node correlations in both topological and
attribute spaces. In addition, DAM-GT formulates a new attention mechanism with
a simple yet effective masking strategy to guide interactions between target
nodes and their neighborhood tokens, overcoming the issue of attention
diversion. Extensive experiments on various graphs with different homophily
levels as well as different scales demonstrate that DAM-GT consistently
outperforms state-of-the-art methods in node classification tasks.

</details>


### [260] [Automated scientific minimization of regret](https://arxiv.org/abs/2505.17661)
*Marcel Binz,Akshay K. Jagadish,Milena Rmus,Eric Schulz*

Key words: ASMR, 认知科学, Centaur, 遗憾最小化, 多属性决策

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: ASMR是一种自动计算认知科学框架，通过科学遗憾最小化原则，利用Centaur模型填补可解释认知模型中的空白，并通过语言推理模型自动修订。在多属性决策任务中，ASMR生成的模型能预测人类行为并保持可解释性。

Motivation: 旨在自动化认知建模流程的核心组件，提升模型预测人类行为的准确性和可解释性。

Method: 结合Centaur模型识别认知模型中的空白，使用语言推理模型自动生成修订。

Result: 在多属性决策任务中，ASMR生成的模型预测人类行为达到噪声上限且保持可解释性。

Conclusion: ASMR展示了自动化认知建模流程的潜力。

Abstract: We introduce automated scientific minimization of regret (ASMR) -- a
framework for automated computational cognitive science. Building on the
principles of scientific regret minimization, ASMR leverages Centaur -- a
recently proposed foundation model of human cognition -- to identify gaps in an
interpretable cognitive model. These gaps are then addressed through automated
revisions generated by a language-based reasoning model. We demonstrate the
utility of this approach in a multi-attribute decision-making task, showing
that ASMR discovers cognitive models that predict human behavior at noise
ceiling while retaining interpretability. Taken together, our results highlight
the potential of ASMR to automate core components of the cognitive modeling
pipeline.

</details>


### [261] [Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs](https://arxiv.org/abs/2505.17662)
*Tianheng Ling,Chao Qian,Lukas Johannes Haßler,Gregor Schiele*

Key words: Transformer, FPGA, 时间序列, 量化训练, 自动化部署

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种针对嵌入式FPGA的自动化部署框架，支持紧凑型Transformer架构，结合量化训练和硬件优化，显著降低推理能耗和延迟。

Motivation: 解决Transformer模型在资源受限设备上部署的挑战，特别是在MCU上的固定点精度限制问题，利用FPGA的灵活性实现高效部署。

Method: 采用量化感知训练（低至4位）、硬件感知超参数搜索（Optuna）和自动VHDL生成，支持三种时间序列任务（预测、分类和异常检测）。

Result: 在两种嵌入式FPGA平台上验证，最低单次推理能耗0.033 mJ，毫秒级延迟，并在Lattice iCE40上展示了部署可行性。

Conclusion: 该框架为嵌入式场景提供了高效、自动化的Transformer部署解决方案，开源代码已发布。

Abstract: Transformer-based models have shown strong performance across diverse
time-series tasks, but their deployment on resource-constrained devices remains
challenging due to high memory and computational demand. While prior work
targeting Microcontroller Units (MCUs) has explored hardware-specific
optimizations, such approaches are often task-specific and limited to 8-bit
fixed-point precision. Field-Programmable Gate Arrays (FPGAs) offer greater
flexibility, enabling fine-grained control over data precision and
architecture. However, existing FPGA-based deployments of Transformers for
time-series analysis typically focus on high-density platforms with manual
configuration. This paper presents a unified and fully automated deployment
framework for Tiny Transformers on embedded FPGAs. Our framework supports a
compact encoder-only Transformer architecture across three representative
time-series tasks (forecasting, classification, and anomaly detection). It
combines quantization-aware training (down to 4 bits), hardware-aware
hyperparameter search using Optuna, and automatic VHDL generation for seamless
deployment. We evaluate our framework on six public datasets across two
embedded FPGA platforms. Results show that our framework produces integer-only,
task-specific Transformer accelerators achieving as low as 0.033 mJ per
inference with millisecond latency on AMD Spartan-7, while also providing
insights into deployment feasibility on Lattice iCE40. All source code will be
released in the GitHub repository
(https://github.com/Edwina1030/TinyTransformer4TS).

</details>


### [262] [What is the role of memorization in Continual Learning?](https://arxiv.org/abs/2505.17664)
*Jędrzej Kozal,Jan Wasilewski,Alif Ashrafee,Bartosz Krawczyk,Michał Woźniak*

Key words: 记忆化, 增量学习, 持续学习, 缓冲区策略

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究了记忆化对增量学习的影响，发现高记忆化分数的样本更容易被遗忘，但在大缓冲区下记忆化样本更重要。

Motivation: 探讨记忆化在增量学习中的作用，区分记忆化与遗忘预防的差异。

Method: 设计实验评估记忆化对持续学习的影响，引入记忆化代理并用于缓冲区策略。

Result: 高记忆化分数样本遗忘更快，但在大缓冲区下更关键。

Conclusion: 记忆化在高性能学习中必要，但小内存时遗忘常规样本更关键。

Abstract: Memorization impacts the performance of deep learning algorithms. Prior works
have studied memorization primarily in the context of generalization and
privacy. This work studies the memorization effect on incremental learning
scenarios. Forgetting prevention and memorization seem similar. However, one
should discuss their differences. We designed extensive experiments to evaluate
the impact of memorization on continual learning. We clarified that learning
examples with high memorization scores are forgotten faster than regular
samples. Our findings also indicated that memorization is necessary to achieve
the highest performance. However, at low memory regimes, forgetting regular
samples is more important. We showed that the importance of a high-memorization
score sample rises with an increase in the buffer size. We introduced a
memorization proxy and employed it in the buffer policy problem to showcase how
memorization could be used during incremental training. We demonstrated that
including samples with a higher proxy memorization score is beneficial when the
buffer size is large.

</details>


### [263] [Towards General Continuous Memory for Vision-Language Models](https://arxiv.org/abs/2505.17670)
*Wenyi Wu,Zixuan Song,Kun Zhou,Yifei Shao,Zhiting Hu,Biwei Huang*

Key words: 连续内存, 视觉语言模型, 多模态推理, 知识编码

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种连续内存（CoMEM）方法，利用视觉语言模型（VLM）自身编码多模态和多语言知识，有效提升复杂推理任务的性能，仅需少量参数和样本。

Motivation: 现有方法将图像和文本令牌拼接为长序列内存，导致上下文长度增加且性能下降，需更高效的多模态知识表示方法。

Method: 使用紧凑的密集嵌入作为连续内存，利用VLM自身编码知识，仅需1.2%的参数和15.6K自合成样本微调。

Result: 在8个多模态推理基准上验证了方法的有效性，内存模块即插即用。

Conclusion: CoMEM通过连续内存高效表示多模态知识，显著提升复杂推理任务性能。

Abstract: Language models (LMs) and their extension, vision-language models (VLMs),
have achieved remarkable performance across various tasks. However, they still
struggle with complex reasoning tasks that require multimodal or multilingual
real-world knowledge. To support such capabilities, an external memory system
that can efficiently provide relevant multimodal information is essential.
Existing approaches generally concatenate image and text tokens into a long
sequence as memory, which, however, may drastically increase context length and
even degrade performance. In contrast, we propose using continuous memory, a
compact set of dense embeddings to more effectively and efficiently represent
multimodal and multilingual knowledge. Our key insight is that a VLM can serve
as its own continuous memory encoder. We empirically show that this design
improves performance on complex multimodal reasoning tasks. Building on this,
we introduce a data-efficient and parameter-efficient method to fine-tune the
VLM into a memory encoder, requiring only 1.2% of the model's parameters and a
small corpus of 15.6K self-synthesized samples. Our approach CoMEM utilizes
VLM's original capabilities to encode arbitrary multimodal and multilingual
knowledge into just 8 continuous embeddings. Since the inference-time VLM
remains frozen, our memory module is plug-and-play and can be flexibly
integrated as needed. Extensive experiments across eight multimodal reasoning
benchmarks demonstrate the effectiveness of our approach.

</details>


### [264] [FlashForge: Ultra-Efficient Prefix-Aware Attention for LLM Decoding](https://arxiv.org/abs/2505.17694)
*Zhibin Wang,Rui Ning,Chao Fang,Zhonghui Zhang,Xi Lin,Shaobo Ma,Mo Zhou,Xue Li,Zhongfeng Wang,Chengying Huan,Rong Gu,Kun Yang,Guihai Chen,Sheng Zhong,Chen Tian*

Key words: FlashForge, 注意力计算, KV缓存, 负载平衡, 并行性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种名为FlashForge的专用注意力内核，旨在优化解码阶段共享前缀的注意力计算。通过优化内存层次结构和并行性，以及平衡工作负载，FlashForge在解码阶段的注意力计算中比现有技术提升了1.9倍速度，减少了120.9倍的内存访问，并在端到端时间上比vLLM提升了3.8倍。

Motivation: 随着上下文长度的增加，解码阶段的注意力计算成为关键瓶颈，尤其是共享前缀的KV缓存访问模式。这促使我们探索如何在共享前缀的注意力计算中优化内存访问。

Method: 提出了FlashForge，包含两个关键创新：一个优化内存层次结构和利用块内及块间并行性的共享前缀注意力内核；一个高效估计成本、分割任务和调度执行的全面负载平衡机制。

Result: 实验结果表明，FlashForge在解码阶段的注意力计算中比现有FlashDecoding内核提升了1.9倍速度，减少了120.9倍的内存访问；在端到端时间上比vLLM提升了3.8倍。

Conclusion: FlashForge通过优化共享前缀的注意力计算，显著提升了解码阶段的效率和性能，为解决KV缓存访问的复杂依赖和负载不均衡提供了有效方案。

Abstract: Prefix-sharing among multiple prompts presents opportunities to combine the
operations of the shared prefix, while attention computation in the decode
stage, which becomes a critical bottleneck with increasing context lengths, is
a memory-intensive process requiring heavy memory access on the key-value (KV)
cache of the prefixes. Therefore, in this paper, we explore the potential of
prefix-sharing in the attention computation of the decode stage. However, the
tree structure of the prefix-sharing mechanism presents significant challenges
for attention computation in efficiently processing shared KV cache access
patterns while managing complex dependencies and balancing irregular workloads.
To address the above challenges, we propose a dedicated attention kernel to
combine the memory access of shared prefixes in the decoding stage, namely
FlashForge. FlashForge delivers two key innovations: a novel shared-prefix
attention kernel that optimizes memory hierarchy and exploits both intra-block
and inter-block parallelism, and a comprehensive workload balancing mechanism
that efficiently estimates cost, divides tasks, and schedules execution.
Experimental results show that FlashForge achieves an average 1.9x speedup and
120.9x memory access reduction compared to the state-of-the-art FlashDecoding
kernel regarding attention computation in the decode stage and 3.8x end-to-end
time per output token compared to the vLLM.

</details>


### [265] [SynRES: Towards Referring Expression Segmentation in the Wild via Synthetic Data](https://arxiv.org/abs/2505.17695)
*Dong-Hee Kim,Hyunjee Song,Donghyun Kim*

Key words: Referring Expression Segmentation, WildRES, SynRES, benchmark, synthetic data

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: WildRES是一个新的Referring Expression Segmentation（RES）基准，引入了长查询和多样化属性，解决了现有基准在复杂推理能力评估上的局限性。SynRES自动化管道通过生成合成数据显著提升了模型性能，在WildRES基准上取得了最先进的结果。

Motivation: 现有RES基准在评估复杂推理能力上存在局限性，无法应对长查询和多样化属性的挑战，WildRES旨在填补这一空白。

Method: WildRES引入多领域长查询和多样化属性；SynRES通过密集标注驱动合成、语义对齐机制和领域感知增强三创新生成合成数据。

Result: SynRES训练的模型在WildRES-ID和WildRES-DS上的gIoU分别提升2.0%和3.8%。

Conclusion: WildRES为RES模型的复杂推理能力提供了更全面评估，SynRES通过合成数据显著提升性能。

Abstract: Despite the advances in Referring Expression Segmentation (RES) benchmarks,
their evaluation protocols remain constrained, primarily focusing on either
single targets with short queries (containing minimal attributes) or multiple
targets from distinctly different queries on a single domain. This limitation
significantly hinders the assessment of more complex reasoning capabilities in
RES models. We introduce WildRES, a novel benchmark that incorporates long
queries with diverse attributes and non-distinctive queries for multiple
targets. This benchmark spans diverse application domains, including autonomous
driving environments and robotic manipulation scenarios, thus enabling more
rigorous evaluation of complex reasoning capabilities in real-world settings.
Our analysis reveals that current RES models demonstrate substantial
performance deterioration when evaluated on WildRES. To address this challenge,
we introduce SynRES, an automated pipeline generating densely paired
compositional synthetic training data through three innovations: (1) a dense
caption-driven synthesis for attribute-rich image-mask-expression triplets, (2)
reliable semantic alignment mechanisms rectifying caption-pseudo mask
inconsistencies via Image-Text Aligned Grouping, and (3) domain-aware
augmentations incorporating mosaic composition and superclass replacement to
emphasize generalization ability and distinguishing attributes over object
categories. Experimental results demonstrate that models trained with SynRES
achieve state-of-the-art performance, improving gIoU by 2.0% on WildRES-ID and
3.8% on WildRES-DS. Code and datasets are available at
https://github.com/UTLLab/SynRES.

</details>


### [266] [COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary Weights in Down Projection](https://arxiv.org/abs/2505.17701)
*Jaewon Cheon,Pilsung Kang*

Key words: 大语言模型,稀疏激活,计算优化,FFNN层,线性组合

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出两种稀疏激活方法（M-COUNTDOWN和D-COUNTDOWN），通过线性组合选择性关闭FFNN层的非关键参数，显著降低计算成本，其中D-COUNTDOWN可减少90%计算量且性能损失仅5.5%，M-COUNTDOWN则在无预测器情况下性能保留更高。

Motivation: 解决大语言模型规模增长带来的计算效率低下问题，提出通过稀疏激活方法优化FFNN层的计算开销。

Method: 提出基于线性组合的稀疏激活方法：M-COUNTDOWN（间接系数）和D-COUNTDOWN（直接系数），并通过专用内核实现理论加速。

Result: D-COUNTDOWN减少90%计算量，性能损失仅5.5%；M-COUNTDOWN在无预测器时性能保留比现有方法高29.4%。

Conclusion: 线性组合稀疏激活方法能有效平衡计算效率与性能，专用内核实现进一步提升了实际加速效果。

Abstract: The growing size of large language models has created significant
computational inefficiencies. To address this challenge, sparse activation
methods selectively deactivates non-essential parameters during inference,
reducing computational costs in FFNN layers. While existing methods focus on
non-linear gating mechanisms, we hypothesize that the sparsity of the FFNN
layer lies globally in the form of a linear combination over its internal down
projection matrix. Based on this insight, we propose two methods: M-COUNTDOWN,
leveraging indirect coefficients, and D-COUNTDOWN, utilizing direct
coefficients of the linear combination. Experimental results demonstrate that
D-COUNTDOWN can omit 90% of computations with performance loss as low as 5.5%
ideally, while M-COUNTDOWN provides a predictor-free solution with up to 29.4%
better performance preservation compared to existing methods. Our specialized
kernel implementations effectively realize these theoretical gains into
substantial real-world acceleration.

</details>


### [267] [The Third Pillar of Causal Analysis? A Measurement Perspective on Causal Representations](https://arxiv.org/abs/2505.17708)
*Dingling Yao,Shimeng Huang,Riccardo Cadei,Kun Zhang,Francesco Locatello*

Key words: 因果推理, 因果表示学习, 测量模型, T-MEX评分, 下游因果任务

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文通过测量模型框架重新解读因果表示学习（CRL），明确了学到的表示支持下游因果推理的条件，并提出了基于测试的测量排他性（T-MEX）评分来量化评估表示质量。

Motivation: 因果推理和发现面临实际数据的复杂性、噪声和高维度挑战。尽管CRL在识别潜在因果结构方面取得进展，但如何评估学到的表示对下游因果任务的实用性仍不明确。

Method: 采用测量模型框架，将学到的表示视为潜在因果变量的代理测量，并提出T-MEX评分量化表示质量。

Result: 在数值模拟和真实生态视频分析中验证了T-MEX评分的有效性，证明其能评估表示识别及对下游因果任务的实用性。

Conclusion: T-MEX框架和评分提供了评估因果表示学习质量的标准化方法，适用于多元场景。

Abstract: Causal reasoning and discovery, two fundamental tasks of causal analysis,
often face challenges in applications due to the complexity, noisiness, and
high-dimensionality of real-world data. Despite recent progress in identifying
latent causal structures using causal representation learning (CRL), what makes
learned representations useful for causal downstream tasks and how to evaluate
them are still not well understood. In this paper, we reinterpret CRL using a
measurement model framework, where the learned representations are viewed as
proxy measurements of the latent causal variables. Our approach clarifies the
conditions under which learned representations support downstream causal
reasoning and provides a principled basis for quantitatively assessing the
quality of representations using a new Test-based Measurement EXclusivity
(T-MEX) score. We validate T-MEX across diverse causal inference scenarios,
including numerical simulations and real-world ecological video analysis,
demonstrating that the proposed framework and corresponding score effectively
assess the identification of learned representations and their usefulness for
causal downstream tasks.

</details>


### [268] [PPO-BR: Dual-Signal Entropy-Reward Adaptation for Trust Region Policy Optimization](https://arxiv.org/abs/2505.17714)
*Ben Rahman*

Key words: PPO-BR, 自适应强化学习, 信任区域, 熵驱动探索, 奖励引导收敛

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: PPO-BR是一种自适应强化学习算法，通过融合探索和收敛信号到一个有界的信任区域，显著提升了PPO的性能，实现了更快的收敛和更低的奖励方差。

Motivation: 解决PPO算法在静态信任区域下的权衡问题，即早期探索受限与后期更新不稳定之间的矛盾，以实现在安全关键系统中的实际部署。

Method: PPO-BR结合了熵驱动扩展（高不确定性状态下探索）和奖励引导收缩（稳定收敛）的机制，形成一个自适应的信任区域。

Result: 在六种不同基准测试中，PPO-BR实现了29.1%的更快收敛（p < 0.001）、比PPO低2.3倍的奖励方差，且仅需5行代码修改和1.8%的运行时开销。

Conclusion: PPO-BR通过简单的实现和理论保证，适用于安全关键领域，如手术机器人或自主无人机，并提供了适用于语言模型和通用RL环境的统一机制。

Abstract: Despite Proximal Policy Optimization (PPO) dominating policy gradient methods
-- from robotic control to game AI -- its static trust region forces a brittle
trade-off: aggressive clipping stifles early exploration, while late-stage
updates destabilize convergence. PPO-BR establishes a new paradigm in adaptive
RL by fusing exploration and convergence signals into a single bounded trust
region -- a theoretically grounded innovation that outperforms five SOTA
baselines with less than 2% overhead. This work bridges a critical gap in
phase-aware learning, enabling real-world deployment in safety-critical systems
like robotic surgery within a single adaptive mechanism. PPO-BR achieves 29.1%
faster convergence by combining: (1) entropy-driven expansion (epsilon up) for
exploration in high-uncertainty states, and (2) reward-guided contraction
(epsilon down) for convergence stability. On six diverse benchmarks (MuJoCo,
Atari, sparse-reward), PPO-BR achieves 29.1% faster convergence (p < 0.001),
2.3x lower reward variance than PPO, and less than 1.8% runtime overhead with
only five lines of code change. PPO-BR's simplicity and theoretical guarantees
make it ready-to-deploy in safety-critical domains -- from surgical robotics to
autonomous drones. In contrast to recent methods such as Group Relative Policy
Optimization (GRPO), PPO-BR offers a unified entropy-reward mechanism
applicable to both language models and general reinforcement learning
environments.

</details>


### [269] [Get Experience from Practice: LLM Agents with Record & Replay](https://arxiv.org/abs/2505.17716)
*Erhu Feng,Wenbo Zhou,Zibin Liu,Le Chen,Yunpeng Dong,Cheng Zhang,Yisheng Zhao,Dong Du,Zhichao Hua,Yubin Xia,Haibo Chen*

Key words: AI代理, 大型语言模型, 记录与回放, 经验抽象, 隐私保护

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为AgentRR的新范式，通过记录和回放AI代理的交互轨迹来解决LLM的不确定性和资源消耗问题，同时提升了可靠性、隐私性、成本效益和性能。

Motivation: 由于大型语言模型（LLMs）的不确定性和高计算资源需求，现有的AI代理在可靠性、隐私、成本和性能方面面临挑战，而现有的解决方案存在局限性，未能从根本上解决问题。

Method: 提出AgentRR，一种记录和回放机制，通过记录代理的任务执行轨迹，并将其抽象为结构化经验，在后续任务中回放。包括多级经验抽象方法和检查功能机制。

Result: AgentRR能够有效提升代理的可靠性、隐私保护能力，并降低计算成本和提升性能，支持多种应用模式，如任务演示、模型协作和隐私感知执行。

Conclusion: AgentRR通过经验记录和回放机制，为AI代理的可靠性、隐私、成本和性能问题提供了系统化的解决方案，并为知识共享和重用开辟了新途径。

Abstract: AI agents, empowered by Large Language Models (LLMs) and communication
protocols such as MCP and A2A, have rapidly evolved from simple chatbots to
autonomous entities capable of executing complex, multi-step tasks,
demonstrating great potential. However, the LLMs' inherent uncertainty and
heavy computational resource requirements pose four significant challenges to
the development of safe and efficient agents: reliability, privacy, cost and
performance. Existing approaches, like model alignment, workflow constraints
and on-device model deployment, can partially alleviate some issues but often
with limitations, failing to fundamentally resolve these challenges.
  This paper proposes a new paradigm called AgentRR (Agent Record & Replay),
which introduces the classical record-and-replay mechanism into AI agent
frameworks. The core idea is to: 1. Record an agent's interaction trace with
its environment and internal decision process during task execution, 2.
Summarize this trace into a structured "experience" encapsulating the workflow
and constraints, and 3. Replay these experiences in subsequent similar tasks to
guide the agent's behavior. We detail a multi-level experience abstraction
method and a check function mechanism in AgentRR: the former balances
experience specificity and generality, while the latter serves as a trust
anchor to ensure completeness and safety during replay. In addition, we explore
multiple application modes of AgentRR, including user-recorded task
demonstration, large-small model collaboration and privacy-aware agent
execution, and envision an experience repository for sharing and reusing
knowledge to further reduce deployment cost.

</details>


### [270] [PEAR: Equal Area Weather Forecasting on the Sphere](https://arxiv.org/abs/2505.17720)
*Hampus Linander,Christoffer Petersson,Daniel Persson,Jan E. Gerken*

Key words: machine learning, weather forecasting, HEALPix, Pangu, transformer

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 摘要介绍了PEAR模型，这是一种基于Transformer的天气预报模型，直接在HEALPix网格上运行，优于Driscoll-Healy网格上的模型，且无计算开销。

Motivation: 传统全球中期天气预报方法在Driscoll-Healy网格上存在不均匀分布的问题，而HEALPix网格的均等面积特性可消除偏差，因此研究团队提出直接在HEALPix上运行的深度学习模型。

Method: 提出PEAR模型，基于Transformer架构，直接在HEALPix网格特征上进行天气预报。

Result: PEAR模型在HEALPix网格上表现优于Driscoll-Healy网格的对应模型，且未增加计算负担。

Conclusion: 研究表明，直接在HEALPix网格上运行的深度学习模型可有效提升天气预报性能，为未来研究提供了新方向。

Abstract: Machine learning methods for global medium-range weather forecasting have
recently received immense attention. Following the publication of the Pangu
Weather model, the first deep learning model to outperform traditional
numerical simulations of the atmosphere, numerous models have been published in
this domain, building on Pangu's success. However, all of these models operate
on input data and produce predictions on the Driscoll--Healy discretization of
the sphere which suffers from a much finer grid at the poles than around the
equator. In contrast, in the Hierarchical Equal Area iso-Latitude Pixelization
(HEALPix) of the sphere, each pixel covers the same surface area, removing
unphysical biases. Motivated by a growing support for this grid in meteorology
and climate sciences, we propose to perform weather forecasting with deep
learning models which natively operate on the HEALPix grid. To this end, we
introduce Pangu Equal ARea (PEAR), a transformer-based weather forecasting
model which operates directly on HEALPix-features and outperforms the
corresponding model on Driscoll--Healy without any computational overhead.

</details>


### [271] [Redirection for Erasing Memory (REM): Towards a universal unlearning method for corrupted data](https://arxiv.org/abs/2505.17730)
*Stefan Schoepf,Michael Curtis Mozer,Nicole Elyse Mitchell,Alexandra Brintrup,Georgios Kaissis,Peter Kairouz,Eleni Triantafillou*

Key words: 机器遗忘, 视觉分类器, 概念空间, REM方法, 专用神经元

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一个概念空间来表征视觉分类器中多样化的数据损坏遗忘任务，并提出了一种新方法REM，通过重定向损坏数据到专用神经元来实现高效遗忘。

Motivation: 现有遗忘方法针对特定任务定制，难以进行系统性比较。论文旨在解决这一问题，并提出一种通用方法以适应不同任务需求。

Method: 提出REM方法，关键是将损坏数据重定向到遗忘时引入的专用神经元，随后丢弃或停用以消除其影响。

Result: REM在多样化任务中表现优异，而现有方法仅在其设计区域有效。

Conclusion: REM提供了一种通用且高效的遗忘方法，适用于多种数据损坏场景。

Abstract: Machine unlearning is studied for a multitude of tasks, but specialization of
unlearning methods to particular tasks has made their systematic comparison
challenging. To address this issue, we propose a conceptual space to
characterize diverse corrupted data unlearning tasks in vision classifiers.
This space is described by two dimensions, the discovery rate (the fraction of
the corrupted data that are known at unlearning time) and the statistical
regularity of the corrupted data (from random exemplars to shared concepts).
Methods proposed previously have been targeted at portions of this space and-we
show-fail predictably outside these regions. We propose a novel method,
Redirection for Erasing Memory (REM), whose key feature is that corrupted data
are redirected to dedicated neurons introduced at unlearning time and then
discarded or deactivated to suppress the influence of corrupted data. REM
performs strongly across the space of tasks, in contrast to prior SOTA methods
that fail outside the regions for which they were designed.

</details>


### [272] [URB -- Urban Routing Benchmark for RL-equipped Connected Autonomous Vehicles](https://arxiv.org/abs/2505.17734)
*Ahmet Onur Akman,Anastasia Psarou,Michał Hoffmann,Łukasz Gorczyca,Łukasz Kowalski,Paweł Gora,Grzegorz Jamróz,Rafał Kucharski*

Key words: Connected Autonomous Vehicles, multi-agent reinforcement learning, urban routing, benchmarking

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了Urban Routing Benchmark（OUR）用于评估多智能体强化学习（MARL）算法在城市路由优化中的表现，提供了29个真实交通网络和需求模式，但实验显示当前MARL算法难以超越人类司机。

Motivation: 为多智能体强化学习在城市交通路由优化中缺乏标准化和真实基准的问题提供解决方案。

Method: 提出了Urban Routing Benchmark（OUR），整合了29个真实交通网络和需求模式，并提供了预定义任务、四种MARL算法实现、三种基线方法和性能指标。

Result: 实验结果表明，尽管训练耗时且成本高，当前MARL算法在多数情况下仍难以超越人类司机的表现。

Conclusion: 当前MARL算法在大规模城市路由优化中存在局限性，亟需技术突破。

Abstract: Connected Autonomous Vehicles (CAVs) promise to reduce congestion in future
urban networks, potentially by optimizing their routing decisions. Unlike for
human drivers, these decisions can be made with collective, data-driven
policies, developed by machine learning algorithms. Reinforcement learning (RL)
can facilitate the development of such collective routing strategies, yet
standardized and realistic benchmarks are missing. To that end, we present
\our{}: Urban Routing Benchmark for RL-equipped Connected Autonomous Vehicles.
\our{} is a comprehensive benchmarking environment that unifies evaluation
across 29 real-world traffic networks paired with realistic demand patterns.
\our{} comes with a catalog of predefined tasks, four state-of-the-art
multi-agent RL (MARL) algorithm implementations, three baseline methods,
domain-specific performance metrics, and a modular configuration scheme. Our
results suggest that, despite the lengthy and costly training, state-of-the-art
MARL algorithms rarely outperformed humans. Experimental results reported in
this paper initiate the first leaderboard for MARL in large-scale urban routing
optimization and reveal that current approaches struggle to scale, emphasizing
the urgent need for advancements in this domain.

</details>


### [273] [A tensor network approach for chaotic time series prediction](https://arxiv.org/abs/2505.17740)
*Rodrigo Martínez-Peña,Román Orús*

Key words: 混沌时间序列, 储层计算, 张量网络, 非线性自回归, 超参数优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了利用张量网络改进储层计算以预测混沌时间序列的方法，结果显示其准确性和计算效率优于传统储层网络。

Motivation: 储层计算在混沌时间序列预测中表现优异，但其架构选择和优化仍具挑战性。张量网络能解决高维参数爆炸问题，有望提升储层计算的性能。

Method: 采用张量网络模型，通过分解多维数组为低维结构，简化参数复杂度，并与传统储层网络（如回声状态网络）进行对比。

Result: 张量网络方法在预测混沌时间序列时展现出更高的准确性和计算效率，同时降低了超参数调优的难度。

Conclusion: 张量网络为储层计算提供了高效且可扩展的解决方案，推动了两个领域的交叉发展。

Abstract: Making accurate predictions of chaotic time series is a complex challenge.
Reservoir computing, a neuromorphic-inspired approach, has emerged as a
powerful tool for this task. It exploits the memory and nonlinearity of
dynamical systems without requiring extensive parameter tuning. However,
selecting and optimizing reservoir architectures remains an open problem.
Next-generation reservoir computing simplifies this problem by employing
nonlinear vector autoregression based on truncated Volterra series, thereby
reducing hyperparameter complexity. Nevertheless, the latter suffers from
exponential parameter growth in terms of the maximum monomial degree. Tensor
networks offer a promising solution to this issue by decomposing
multidimensional arrays into low-dimensional structures, thus mitigating the
curse of dimensionality. This paper explores the application of a previously
proposed tensor network model for predicting chaotic time series, demonstrating
its advantages in terms of accuracy and computational efficiency compared to
conventional echo state networks. Using a state-of-the-art tensor network
approach enables us to bridge the gap between the tensor network and reservoir
computing communities, fostering advances in both fields.

</details>


### [274] [Discrete Neural Flow Samplers with Locally Equivariant Transformer](https://arxiv.org/abs/2505.17741)
*Zijing Ou,Ruixiang Zhang,Yingzhen Li*

Key words: 离散采样, DNFS, 马尔可夫链, 控制变量, Transformer

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为离散神经流采样器（DNFS）的可训练高效框架，用于解决离散采样问题，并通过控制变量和局部等变Transformer提升效率。

Motivation: 传统的马尔可夫链蒙特卡洛方法在离散采样中收敛慢且效果差，需要一种更高效的方法。

Method: DNFS学习连续时间马尔可夫链的速率矩阵，并利用控制变量降低方差，结合局部等变Transformer提升训练效率。

Result: DNFS在未归一化分布采样、离散能量模型训练和组合优化问题中表现出色。

Conclusion: DNFS是一种高效的离散采样框架，具有广泛的应用潜力。

Abstract: Sampling from unnormalised discrete distributions is a fundamental problem
across various domains. While Markov chain Monte Carlo offers a principled
approach, it often suffers from slow mixing and poor convergence. In this
paper, we propose Discrete Neural Flow Samplers (DNFS), a trainable and
efficient framework for discrete sampling. DNFS learns the rate matrix of a
continuous-time Markov chain such that the resulting dynamics satisfy the
Kolmogorov equation. As this objective involves the intractable partition
function, we then employ control variates to reduce the variance of its Monte
Carlo estimation, leading to a coordinate descent learning algorithm. To
further facilitate computational efficiency, we propose locally equivaraint
Transformer, a novel parameterisation of the rate matrix that significantly
improves training efficiency while preserving powerful network expressiveness.
Empirically, we demonstrate the efficacy of DNFS in a wide range of
applications, including sampling from unnormalised distributions, training
discrete energy-based models, and solving combinatorial optimisation problems.

</details>


### [275] [MetaBox-v2: A Unified Benchmark Platform for Meta-Black-Box Optimization](https://arxiv.org/abs/2505.17745)
*Zeyuan Ma,Yue-Jiao Gong,Hongshu Guo,Wenjie Qiu,Sijie Ma,Hongqiao Lian,Jiajun Zhan,Kaixu Chen,Chen Wang,Zhiyang Huang,Zechuan Huang,Guojun Peng,Ran Cheng,Yining Ma*

Key words: MetaBBO, 元学习, 优化算法, MetaBox-v2, 并行化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: MetaBox-v2是Meta-Black-Box Optimization框架的重大升级，支持多种优化方法，提供高效并行化、全面基准测试和可扩展接口，并通过案例研究验证其实用性。

Motivation: 原MetaBox框架因范围狭窄无法跟上领域快速发展的需求，因此开发MetaBox-v2以提供更全面的支持。

Method: 采用统一架构支持RL、进化和梯度方法，实现23种基线方法；引入高效并行化方案；提供18项任务（1900+实例）的基准测试。

Result: 训练/测试时间减少10-40倍，并通过案例研究验证了基线方法的优化性能、泛化能力和学习效率。

Conclusion: MetaBox-v2为研究者和初学者提供了实用工具和宝贵洞察，显著推进了MetaBBO领域的发展。

Abstract: Meta-Black-Box Optimization (MetaBBO) streamlines the automation of
optimization algorithm design through meta-learning. It typically employs a
bi-level structure: the meta-level policy undergoes meta-training to reduce the
manual effort required in developing algorithms for low-level optimization
tasks. The original MetaBox (2023) provided the first open-source framework for
reinforcement learning-based single-objective MetaBBO. However, its relatively
narrow scope no longer keep pace with the swift advancement in this field. In
this paper, we introduce MetaBox-v2 (https://github.com/MetaEvo/MetaBox) as a
milestone upgrade with four novel features: 1) a unified architecture
supporting RL, evolutionary, and gradient-based approaches, by which we
reproduce 23 up-to-date baselines; 2) efficient parallelization schemes, which
reduce the training/testing time by 10-40x; 3) a comprehensive benchmark suite
of 18 synthetic/realistic tasks (1900+ instances) spanning single-objective,
multi-objective, multi-model, and multi-task optimization scenarios; 4)
plentiful and extensible interfaces for custom analysis/visualization and
integrating to external optimization tools/benchmarks. To show the utility of
MetaBox-v2, we carry out a systematic case study that evaluates the built-in
baselines in terms of the optimization performance, generalization ability and
learning efficiency. Valuable insights are concluded from thorough and detailed
analysis for practitioners and those new to the field.

</details>


### [276] [Soft-CAM: Making black box models self-explainable for high-stakes decisions](https://arxiv.org/abs/2505.17748)
*Kerol Djoumessi,Philipp Berens*

Key words: 可解释性, CNN, SoftCAM, 医学图像, 自解释模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种名为SoftCAM的方法，通过修改标准CNN架构使其具备固有可解释性，无需依赖后处理解释方法，在医学数据集上表现优异。

Motivation: 现有的后处理解释方法（如归因分析）通常敏感、不可靠，且无法真实反映模型的决策过程，限制了其在关键应用中的可信度。

Method: SoftCAM通过移除全局平均池化层并用基于卷积的类别证据层替换全连接分类层，保留空间信息并生成明确的类别激活图。

Result: 在三个医学数据集上的评估表明，SoftCAM在保持分类性能的同时，显著提升了定性和定量解释效果，优于现有后处理方法。

Conclusion: 研究表明，CNN可以在不牺牲性能的情况下实现固有可解释性，推动了高风险决策中自解释深度学习的发展。

Abstract: Convolutional neural networks (CNNs) are widely used for high-stakes
applications like medicine, often surpassing human performance. However, most
explanation methods rely on post-hoc attribution, approximating the
decision-making process of already trained black-box models. These methods are
often sensitive, unreliable, and fail to reflect true model reasoning, limiting
their trustworthiness in critical applications. In this work, we introduce
SoftCAM, a straightforward yet effective approach that makes standard CNN
architectures inherently interpretable. By removing the global average pooling
layer and replacing the fully connected classification layer with a
convolution-based class evidence layer, SoftCAM preserves spatial information
and produces explicit class activation maps that form the basis of the model's
predictions. Evaluated on three medical datasets, SoftCAM maintains
classification performance while significantly improving both the qualitative
and quantitative explanation compared to existing post-hoc methods. Our results
demonstrate that CNNs can be inherently interpretable without compromising
performance, advancing the development of self-explainable deep learning for
high-stakes decision-making.

</details>


### [277] [Mind the GAP! The Challenges of Scale in Pixel-based Deep Reinforcement Learning](https://arxiv.org/abs/2505.17749)
*Ghada Sokar,Pablo Samuel Castro*

Key words: 深度强化学习, 像素环境, 全局平均池化, 性能优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文指出深度强化学习在像素环境中的性能下降问题，并发现编码器输出与后续密集层之间的连接是其根本原因。作者提出全局平均池化作为简单有效的解决方案。

Motivation: 为解决像素环境中深度强化学习性能下降的问题，并揭示其根本原因。

Method: 通过分析编码器输出与密集层之间的瓶颈，提出使用全局平均池化优化连接。

Result: 全局平均池化有效解决了性能问题，且方法简单高效。

Conclusion: 全局平均池化是一种简单且高效的解决方案，优于之前复杂的方法。

Abstract: Scaling deep reinforcement learning in pixel-based environments presents a
significant challenge, often resulting in diminished performance. While recent
works have proposed algorithmic and architectural approaches to address this,
the underlying cause of the performance drop remains unclear. In this paper, we
identify the connection between the output of the encoder (a stack of
convolutional layers) and the ensuing dense layers as the main underlying
factor limiting scaling capabilities; we denote this connection as the
bottleneck, and we demonstrate that previous approaches implicitly target this
bottleneck. As a result of our analyses, we present global average pooling as a
simple yet effective way of targeting the bottleneck, thereby avoiding the
complexity of earlier approaches.

</details>


### [278] [But what is your honest answer? Aiding LLM-judges with honest alternatives using steering vectors](https://arxiv.org/abs/2505.17760)
*Leon Eshuijs,Archie Chaudhury,Alan McBeth,Ethan Nguyen*

Key words: JUSSA, LLM, 诚实性, 引导向量, 欺骗检测

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文介绍了JUSSA框架，通过训练单个样本的引导向量使LLM生成更诚实的回答，帮助检测不诚实行为。

Motivation: 现有诚实性基准多关注事实知识或明显有害行为，依赖外部法官难以检测隐蔽的不诚实行为。

Method: 提出JUSSA框架，利用引导向量训练单个样本，提升LLM-judges的诚实行为检测能力。

Result: 在专门设计的欺骗性提示数据集上，JUSSA能更好区分不诚实与良性回答，识别隐蔽操纵行为。

Conclusion: JUSSA框架有效提升LLM-judges检测不诚实行为的能力，尤其对隐蔽操纵行为有显著效果。

Abstract: Recent safety evaluations of Large Language Models (LLMs) show that many
models exhibit dishonest behavior, such as sycophancy. However, most honesty
benchmarks focus exclusively on factual knowledge or explicitly harmful
behavior and rely on external judges, which are often unable to detect less
obvious forms of dishonesty. In this work, we introduce a new framework, Judge
Using Safety-Steered Alternatives (JUSSA), which utilizes steering vectors
trained on a single sample to elicit more honest responses from models, helping
LLM-judges in the detection of dishonest behavior. To test our framework, we
introduce a new manipulation dataset with prompts specifically designed to
elicit deceptive responses. We find that JUSSA enables LLM judges to better
differentiate between dishonest and benign responses, and helps them identify
subtle instances of manipulative behavior.

</details>


### [279] [Structured Linear CDEs: Maximally Expressive and Parallel-in-Time Sequence Models](https://arxiv.org/abs/2505.17761)
*Benjamin Walker,Lingyi Yang,Nicola Muca Cirone,Cristopher Salvi,Terry Lyons*

Key words: SLiCEs, linear controlled differential equations, sequence models, state-tracking, time-series classification

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: SLiCEs框架通过结构化、输入依赖的状态转移矩阵，既能保持密集矩阵的最大表达能力，又降低了计算成本，涵盖了多种现有架构并提出了两种新变体。

Motivation: 为了解决序列模型中计算密集矩阵的高成本和结构限制问题，同时保持模型的表达能力和效率。

Method: 提出了SLiCEs框架，包括块对角、稀疏和Walsh-Hadamard矩阵等方法，并验证了其在表达能力和计算效率上的优势。

Result: SLiCEs在状态跟踪基准测试中表现出色，在并行时间模型中取得了最佳的长度泛化效果，并在多变量时间序列分类任务中达到了最优性能，同时显著减少了训练时间。

Conclusion: SLiCEs提供了一种高效且表达能力强的序列建模框架，适用于多种任务，并显著提升了计算效率。

Abstract: Structured Linear Controlled Differential Equations (SLiCEs) provide a
unifying framework for sequence models with structured, input-dependent
state-transition matrices that retain the maximal expressivity of dense
matrices whilst being cheaper to compute. The framework encompasses existing
architectures, such as input-dependent block-diagonal linear recurrent neural
networks and DeltaNet's diagonal-plus-low-rank structure, as well as two novel
variants based on sparsity and the Walsh--Hadamard transform. We prove that,
unlike the diagonal state-transition matrices of S4 and Mamba, SLiCEs employing
block-diagonal, sparse, or Walsh--Hadamard matrices match the maximal
expressivity of dense matrices. Empirically, SLiCEs solve the $A_5$
state-tracking benchmark with a single layer, achieve best-in-class length
generalisation on regular language tasks among parallel-in-time models, and
match the state-of-the-art performance of log neural controlled differential
equations on six multivariate time-series classification datasets while cutting
the average time per training step by a factor of twenty.

</details>


### [280] [Unsupervised Clustering for Fault Analysis in High-Voltage Power Systems Using Voltage and Current Signals](https://arxiv.org/abs/2505.17763)
*Julian Oelhaf,Georg Kordowich,Andreas Maier,Johann Jager,Siming Bayer*

Key words: 电力系统, 故障诊断, 无监督聚类, 快速傅里叶变换, K-Means算法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文探讨了在高压电力系统中应用无监督聚类技术进行故障诊断的方法。

Motivation: 现代电网中传感器的大量使用积累了丰富的电压和电流波形数据，但缺乏标记数据集为故障分类和分析带来了挑战。

Method: 使用快速傅里叶变换（FFT）提取频域特征，并应用K-Means算法识别数据中的潜在模式，实现无需标记训练样本的自动故障分类。

Result: 通过无监督学习，成功实现了电力系统故障的自动化检测和分类，结果与真实故障特征一致。

Conclusion: 无监督学习为电力系统故障分析提供了一种可扩展和数据驱动的方法，且无需大量先验假设。

Abstract: The widespread use of sensors in modern power grids has led to the
accumulation of large amounts of voltage and current waveform data, especially
during fault events. However, the lack of labeled datasets poses a significant
challenge for fault classification and analysis. This paper explores the
application of unsupervised clustering techniques for fault diagnosis in
high-voltage power systems. A dataset provided by the Reseau de Transport
d'Electricite (RTE) is analyzed, with frequency domain features extracted using
the Fast Fourier Transform (FFT). The K-Means algorithm is then applied to
identify underlying patterns in the data, enabling automated fault
categorization without the need for labeled training samples. The resulting
clusters are evaluated in collaboration with power system experts to assess
their alignment with real-world fault characteristics. The results demonstrate
the potential of unsupervised learning for scalable and data-driven fault
analysis, providing a robust approach to detecting and classifying power system
faults with minimal prior assumptions.

</details>


### [281] [Joker: Joint Optimization Framework for Lightweight Kernel Machines](https://arxiv.org/abs/2505.17765)
*Junhong Zhang,Zhihui Lai*

Key words: 核方法, 大规模学习, 内存优化, 联合优化, 核近似

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: Joker是一个针对多种核模型的联合优化框架，通过双块坐标下降方法和核近似技术，显著降低内存开销并提高效率。

Motivation: 解决大规模核方法中内存开销高且研究集中在KRR上的局限性。

Method: 提出DBCD-TR算法并采用随机特征的核近似技术。

Result: 内存节省90%，性能可比或优于现有方法。

Conclusion: Joker高效且适用于多种核模型。

Abstract: Kernel methods are powerful tools for nonlinear learning with
well-established theory. The scalability issue has been their long-standing
challenge. Despite the existing success, there are two limitations in
large-scale kernel methods: (i) The memory overhead is too high for users to
afford; (ii) existing efforts mainly focus on kernel ridge regression (KRR),
while other models lack study. In this paper, we propose Joker, a joint
optimization framework for diverse kernel models, including KRR, logistic
regression, and support vector machines. We design a dual block coordinate
descent method with trust region (DBCD-TR) and adopt kernel approximation with
randomized features, leading to low memory costs and high efficiency in
large-scale learning. Experiments show that Joker saves up to 90\% memory but
achieves comparable training time and performance (or even better) than the
state-of-the-art methods.

</details>


### [282] [Inference-Time Decomposition of Activations (ITDA): A Scalable Approach to Interpreting Large Language Models](https://arxiv.org/abs/2505.17769)
*Patrick Leask,Neel Nanda,Noura Al Moubayed*

Key words: 稀疏自编码器,大型语言模型,跨模型比较,贪心算法,重构性能

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为ITDAs的新方法，用于替代稀疏自编码器（SAEs）分解大型语言模型（LLM）的激活。ITDAs训练速度快，成本低，且支持跨模型比较。

Motivation: 稀疏自编码器（SAEs）训练成本高且无法跨模型迁移，ITDAs旨在解决这些问题。

Method: 通过贪心算法构建激活字典，选择现有字典中最差逼近的激活进行匹配追踪。

Result: ITDAs训练速度是SAEs的1%，数据需求为1%，并在某些LLM上达到相近的重构性能。ITDAs字典的Jaccard相似性指数优于现有方法。

Conclusion: ITDAs为计算资源有限或需要跨模型比较的场景提供了一种高效替代方案。

Abstract: Sparse autoencoders (SAEs) are a popular method for decomposing Large Langage
Models (LLM) activations into interpretable latents. However, due to their
substantial training cost, most academic research uses open-source SAEs which
are only available for a restricted set of models of up to 27B parameters. SAE
latents are also learned from a dataset of activations, which means they do not
transfer between models. Motivated by relative representation similarity
measures, we introduce Inference-Time Decomposition of Activations (ITDA)
models, an alternative method for decomposing language model activations. To
train an ITDA, we greedily construct a dictionary of language model activations
on a dataset of prompts, selecting those activations which were worst
approximated by matching pursuit on the existing dictionary. ITDAs can be
trained in just 1\% of the time required for SAEs, using 1\% of the data. This
allowed us to train ITDAs on Llama-3.1 70B and 405B on a single consumer GPU.
ITDAs can achieve similar reconstruction performance to SAEs on some target
LLMs, but generally incur a performance penalty. However, ITDA dictionaries
enable cross-model comparisons, and a simple Jaccard similarity index on ITDA
dictionaries outperforms existing methods like CKA, SVCCA, and relative
representation similarity metrics. ITDAs provide a cheap alternative to SAEs
where computational resources are limited, or when cross model comparisons are
necessary. Code available at https://github.com/pleask/itda.

</details>


### [283] [C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in Large Language Models](https://arxiv.org/abs/2505.17773)
*Amir Hossein Rahmati,Sanket Jantre,Weifeng Zhang,Yucheng Wang,Byung-Jun Yoon,Nathan M. Urban,Xiaoning Qian*

Key words: Low-Rank Adaptation, uncertainty-aware, few-shot learning, parameter efficiency, model generalization

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种新颖的上下文感知低秩自适应方法（C-LoRA），通过动态调整不确定性估计来优化稀疏数据下的模型调优，显著提升了不确定度量化和模型泛化能力。

Motivation: 传统LoRA方法在数据稀少时预测过于自信，且现有方法忽视了输入特征对预测不确定性的影响，因此需要一种更高效且能动态适应输入的不确定性感知调优方法。

Method: 开发了轻量级的上下文LoRA模块，针对每个输入数据样本动态调整不确定性估计，并将数据驱动上下文融入参数后验。

Result: C-LoRA在不确定度量化和模型泛化方面优于现有方法，并通过消融实验验证了上下文模块的关键作用。

Conclusion: C-LoRA为稀疏数据下的鲁棒、不确定性感知的LLM调优树立了新标准。

Abstract: Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning
large language models (LLMs), but it often produces overconfident predictions
in data-scarce few-shot settings. To address this issue, several classical
statistical learning approaches have been repurposed for scalable
uncertainty-aware LoRA fine-tuning. However, these approaches neglect how input
characteristics affect the predictive uncertainty estimates. To address this
limitation, we propose Contextual Low-Rank Adaptation (\textbf{C-LoRA}) as a
novel uncertainty-aware and parameter efficient fine-tuning approach, by
developing new lightweight LoRA modules contextualized to each input data
sample to dynamically adapt uncertainty estimates. Incorporating data-driven
contexts into the parameter posteriors, C-LoRA mitigates overfitting, achieves
well-calibrated uncertainties, and yields robust predictions. Extensive
experiments demonstrate that C-LoRA consistently outperforms the
state-of-the-art uncertainty-aware LoRA methods in both uncertainty
quantification and model generalization. Ablation studies further confirm the
critical role of our contextual modules in capturing sample-specific
uncertainties. C-LoRA sets a new standard for robust, uncertainty-aware LLM
fine-tuning in few-shot regimes.

</details>


### [284] [Optimizing Shortfall Risk Metric for Learning Regression Models](https://arxiv.org/abs/2505.17777)
*Harish G. Ramaswamy,L. A. Prashanth*

Key words: Utility-Based Shortfall Risk, 回归问题, 浓度界限, 优化算法, 二分法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了在回归问题中估计和优化基于效用的短缺风险（UBSR）的方法，通过推导UBSR的浓度界限，提出了一个二分法算法来解决UBSR优化问题。

Motivation: UBSR是一种对底层分布非线性的风险度量，传统的最小化方法难以直接应用，因此需要开发新的算法来解决UBSR的估计与优化问题。

Method: 1. 推导UBSR的浓度界限；2. 将UBSR优化问题转化为在损失分布空间中伪线性函数的最小化问题；3. 构建梯度Oracle和线性最小化Oracle（LMO）；4. 设计一种二分法算法。

Result: 提出的二分法算法能够收敛到UBSR最优解，证明了其有效性。

Conclusion: 本研究提供了一种有效的方法来估计和优化UBSR，解决了传统方法在处理非线性风险度量时的局限性。

Abstract: We consider the problem of estimating and optimizing utility-based shortfall
risk (UBSR) of a loss, say $(Y - \hat Y)^2$, in the context of a regression
problem. Empirical risk minimization with a UBSR objective is challenging since
UBSR is a non-linear function of the underlying distribution. We first derive a
concentration bound for UBSR estimation using independent and identically
distributed (i.i.d.) samples. We then frame the UBSR optimization problem as
minimization of a pseudo-linear function in the space of achievable
distributions $\mathcal D$ of the loss $(Y- \hat Y)^2$. We construct a gradient
oracle for the UBSR objective and a linear minimization oracle (LMO) for the
set $\mathcal D$. Using these oracles, we devise a bisection-type algorithm,
and establish convergence to the UBSR-optimal solution.

</details>


### [285] [Supervised Graph Contrastive Learning for Gene Regulatory Network](https://arxiv.org/abs/2505.17786)
*Sho Oshima,Yuji Okamoto,Taisei Tosaki,Ryosuke Kojima,Yasushi Okuno*

Key words: 图对比学习,基因调控网络,监督学习,基因敲除,生物网络

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于监督的图对比学习方法SupGCL，特别针对基因调控网络（GRNs），引入了基因敲除实验数据作为监督信号，显著提升了生物下游任务的性能。

Motivation: 现有图对比学习方法在应用于生物网络时忽略了生物学相关扰动（如基因敲除），SupGCL旨在填补这一空白，直接利用生物学实验数据改进模型性能。

Method: SupGCL将现有基于非生物扰动的图对比学习方法扩展为概率模型，引入真实的基因敲除数据作为监督信号，优化GRN表示学习。

Result: 在多种癌症患者的GRN数据集上，SupGCL在患者风险预测、疾病亚型分类（图级任务）和基因功能分类（节点级任务）中均优于现有基准方法。

Conclusion: SupGCL通过结合生物实验数据，显著提升了图对比学习在GRN中的应用效果，为生物网络分析提供了新思路。

Abstract: Graph representation learning is effective for obtaining a meaningful latent
space utilizing the structure of graph data and is widely applied, including
biological networks. In particular, Graph Contrastive Learning (GCL) has
emerged as a powerful self-supervised method that relies on applying
perturbations to graphs for data augmentation. However, when applying existing
GCL methods to biological networks such as Gene Regulatory Networks (GRNs),
they overlooked meaningful biologically relevant perturbations, e.g., gene
knockdowns. In this study, we introduce SupGCL (Supervised Graph Contrastive
Learning), a novel GCL method for GRNs that directly incorporates biological
perturbations derived from gene knockdown experiments as the supervision.
SupGCL mathematically extends existing GCL methods that utilize non-biological
perturbations to probabilistic models that introduce actual biological gene
perturbation utilizing gene knockdown data. Using the GRN representation
obtained by our proposed method, our aim is to improve the performance of
biological downstream tasks such as patient hazard prediction and disease
subtype classification (graph-level task), and gene function classification
(node-level task). We applied SupGCL on real GRN datasets derived from patients
with multiple types of cancer, and in all experiments SupGCL achieves better
performance than state-of-the-art baselines.

</details>


### [286] [RECIPE-TKG: From Sparse History to Structured Reasoning for LLM-based Temporal Knowledge Graph Completion](https://arxiv.org/abs/2505.17794)
*Ömer Faruk Akgül,Feiyu Zhu,Yuxin Yang,Rajgopal Kannan,Viktor Prasanna*

Key words: Temporal Knowledge Graphs, TKG completion, RECIPE-TKG, LLMs, sparse historical context

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: RECIPE-TKG is a lightweight framework for temporal knowledge graph completion, improving accuracy in sparse historical contexts via rule-based retrieval, contrastive fine-tuning, and semantic filtering.

Motivation: Existing LLM-based approaches for TKG completion struggle with limited or missing historical evidence, highlighting the need for a more data-efficient and accurate method.

Method: RECIPE-TKG integrates rule-based multi-hop retrieval, contrastive fine-tuning of lightweight adapters, and test-time semantic filtering to refine predictions.

Result: The framework achieves up to 30.6% relative improvement in Hits@10 over previous methods and generates more semantically coherent predictions, even with sparse history.

Conclusion: RECIPE-TKG offers a robust solution for TKG completion, particularly in data-scarce scenarios, by combining structural diversity and relational semantics.

Abstract: Temporal Knowledge Graphs (TKGs) represent dynamic facts as timestamped
relations between entities. TKG completion involves forecasting missing or
future links, requiring models to reason over time-evolving structure. While
LLMs show promise for this task, existing approaches often overemphasize
supervised fine-tuning and struggle particularly when historical evidence is
limited or missing. We introduce RECIPE-TKG, a lightweight and data-efficient
framework designed to improve accuracy and generalization in settings with
sparse historical context. It combines (1) rule-based multi-hop retrieval for
structurally diverse history, (2) contrastive fine-tuning of lightweight
adapters to encode relational semantics, and (3) test-time semantic filtering
to iteratively refine generations based on embedding similarity. Experiments on
four TKG benchmarks show that RECIPE-TKG outperforms previous LLM-based
approaches, achieving up to 30.6\% relative improvement in Hits@10. Moreover,
our proposed framework produces more semantically coherent predictions, even
for the samples with limited historical context.

</details>


### [287] [Latent Mode Decomposition](https://arxiv.org/abs/2505.17797)
*Manuel Morante,Naveed ur Rehman*

Key words: Variational Latent Mode Decomposition, Multivariate Mode Decomposition, sparse coding, AM-FM oscillatory modes, variational optimization

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: VLMD是一种新的多元信号分解算法，通过稀疏编码和模式分解结合，解决了现有MMD技术的高计算成本和参数敏感性等问题，并在实验中展现了优越性能。

Motivation: 现有多元模式分解(MMD)方法存在高计算成本、参数敏感性和通道间依赖建模不足等问题，VLMD旨在解决这些限制。

Method: VLMD基于潜在模式分解(LMD)模型，通过稀疏线性组合共享潜在成分（AM-FM振荡模式）表示信号，并采用约束变分优化问题联合优化重构保真度、稀疏性和频率正则化。

Result: 实验表明，VLMD在合成和真实数据集上的准确性、效率和提取结构的可解释性均优于现有MMD方法。

Conclusion: VLMD通过底层模型和创新优化方法，显著提升了多元信号分解的性能和实用性。

Abstract: We introduce Variational Latent Mode Decomposition (VLMD), a new algorithm
for extracting oscillatory modes and associated connectivity structures from
multivariate signals. VLMD addresses key limitations of existing Multivariate
Mode Decomposition (MMD) techniques -including high computational cost,
sensitivity to parameter choices, and weak modeling of interchannel
dependencies. Its improved performance is driven by a novel underlying model,
Latent Mode Decomposition (LMD), which blends sparse coding and mode
decomposition to represent multichannel signals as sparse linear combinations
of shared latent components composed of AM-FM oscillatory modes. This
formulation enables VLMD to operate in a lower-dimensional latent space,
enhancing robustness to noise, scalability, and interpretability. The algorithm
solves a constrained variational optimization problem that jointly enforces
reconstruction fidelity, sparsity, and frequency regularization. Experiments on
synthetic and real-world datasets demonstrate that VLMD outperforms
state-of-the-art MMD methods in accuracy, efficiency, and interpretability of
extracted structures.

</details>


### [288] [A Coreset Selection of Coreset Selection Literature: Introduction and Recent Advances](https://arxiv.org/abs/2505.17799)
*Brian B. Moser,Arundhati S. Shanbhag,Stanislav Frolov,Federico Raue,Joachim Folz,Andreas Dengel*

Key words: 核心集选择, 数据缩减, 机器学习, 子模优化, 双层级优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种更全面的核心集选择方法，统一了训练无关、训练导向和无标签三种主要研究方向，填补了现有综述的空白，并探讨了修剪策略对泛化和神经缩放定律的影响。

Motivation: 现有核心集选择方法多局限于经典几何或主动学习技术，缺乏全面视角。本文旨在填补这一空白，统一不同研究方向，提供新见解。

Method: 论文通过综合三种核心集研究方向（训练无关、训练导向、无标签），提出统一分类法，并分析修剪策略对泛化和神经缩放定律的影响。

Result: 论文整合了常被忽视的子领域（如子模优化、双层级优化等），并将不同方法在计算、鲁棒性和性能需求下进行比较。

Conclusion: 本文为未来研究指明了方向，包括鲁棒性、异常值过滤及适应基础模型的核心集选择等开放性问题。

Abstract: Coreset selection targets the challenge of finding a small, representative
subset of a large dataset that preserves essential patterns for effective
machine learning. Although several surveys have examined data reduction
strategies before, most focus narrowly on either classical geometry-based
methods or active learning techniques. In contrast, this survey presents a more
comprehensive view by unifying three major lines of coreset research, namely,
training-free, training-oriented, and label-free approaches, into a single
taxonomy. We present subfields often overlooked by existing work, including
submodular formulations, bilevel optimization, and recent progress in
pseudo-labeling for unlabeled datasets. Additionally, we examine how pruning
strategies influence generalization and neural scaling laws, offering new
insights that are absent from prior reviews. Finally, we compare these methods
under varying computational, robustness, and performance demands and highlight
open challenges, such as robustness, outlier filtering, and adapting coreset
selection to foundation models, for future research.

</details>


### [289] [Hyperparameter Optimization via Interacting with Probabilistic Circuits](https://arxiv.org/abs/2505.17804)
*Jonas Seng,Fabrizio Ventola,Zhongjie Yu,Kristian Kersting*

Key words: 超参数优化, 贝叶斯优化, 概率电路, 交互式学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于概率电路（PCs）的新型交互式贝叶斯优化方法，用于超参数优化（HPO）。通过替代模型PCs和条件采样策略，该方法消除了内层优化需求，并更准确地反映用户反馈。

Motivation: 现有交互式贝叶斯优化方法难以准确反映用户先验信念，尤其是在超参数优化场景中内层优化复杂的情况下。

Method: 利用概率电路（PCs）作为替代模型，构建联合分布，并通过条件采样生成候选点，避免内层优化。

Result: 理论分析和实证评估表明，该方法在标准HPO和交互式HPO中均优于现有基线。

Conclusion: 新方法通过PCs和条件采样提高了交互式HPO的准确性和效率。

Abstract: Despite the growing interest in designing truly interactive hyperparameter
optimization (HPO) methods, to date, only a few allow to include human
feedback. Existing interactive Bayesian optimization (BO) methods incorporate
human beliefs by weighting the acquisition function with a user-defined prior
distribution. However, in light of the non-trivial inner optimization of the
acquisition function prevalent in BO, such weighting schemes do not always
accurately reflect given user beliefs. We introduce a novel BO approach
leveraging tractable probabilistic models named probabilistic circuits (PCs) as
a surrogate model. PCs encode a tractable joint distribution over the hybrid
hyperparameter space and evaluation scores. They enable exact conditional
inference and sampling. Based on conditional sampling, we construct a novel
selection policy that enables an acquisition function-free generation of
candidate points (thereby eliminating the need for an additional inner-loop
optimization) and ensures that user beliefs are reflected accurately in the
selection policy. We provide a theoretical analysis and an extensive empirical
evaluation, demonstrating that our method achieves state-of-the-art performance
in standard HPO and outperforms interactive BO baselines in interactive HPO.

</details>


### [290] [VIBE: Vector Index Benchmark for Embeddings](https://arxiv.org/abs/2505.17810)
*Elias Jääsaari,Ville Hyvönen,Matteo Ceccarello,Teemu Roos,Martin Aumüller*

Key words: 近似最近邻搜索（ANN）、向量索引、基准评测（VIBE）、分布外数据（OOD）、检索增强生成（RAG）

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: VIBE（Vector Index Benchmark for Embeddings）作为一个开源项目，旨在更新现有的近似最近邻（ANN）搜索基准，通过现代应用特征（如RAG）创建数据集，并包含分布外（OOD）数据以模拟真实场景。

Motivation: 现有ANN搜索基准的数据集已无法代表当前应用需求，因此需要更新的基准评测工具。

Method: 利用现代密集嵌入模型（如RAG）创建数据集，并包含分布外数据；对21种向量索引实现进行综合评价，涵盖12个分布内和6个分布外数据集。

Result: VIBE为ANN算法提供了全面评估框架，填补了现有基准的不足。

Conclusion: VIBE为现代ANN应用提供了更贴近实际的评测基准，推动了向量索引技术的性能优化。

Abstract: Approximate nearest neighbor (ANN) search is a performance-critical component
of many machine learning pipelines. Rigorous benchmarking is essential for
evaluating the performance of vector indexes for ANN search. However, the
datasets of the existing benchmarks are no longer representative of the current
applications of ANN search. Hence, there is an urgent need for an up-to-date
set of benchmarks. To this end, we introduce Vector Index Benchmark for
Embeddings (VIBE), an open source project for benchmarking ANN algorithms. VIBE
contains a pipeline for creating benchmark datasets using dense embedding
models characteristic of modern applications, such as retrieval-augmented
generation (RAG). To replicate real-world workloads, we also include
out-of-distribution (OOD) datasets where the queries and the corpus are drawn
from different distributions. We use VIBE to conduct a comprehensive evaluation
of SOTA vector indexes, benchmarking 21 implementations on 12 in-distribution
and 6 out-of-distribution datasets.

</details>


### [291] [Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models](https://arxiv.org/abs/2505.17826)
*Xuchen Pan,Yanxi Chen,Yushuo Chen,Yuchang Sun,Daoyuan Chen,Wenhao Zhang,Yuexiang Xie,Yilun Huang,Yilei Zhang,Dawei Gao,Yaliang Li,Bolin Ding,Jingren Zhou*

Key words: 

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: Trinity-RFT是一个为大型语言模型强化微调（RFT）设计的通用、灵活且可扩展的框架，包含解耦设计的RFT核心、高效稳健的智能体-环境交互机制以及优化的系统数据管道。

Motivation: 提供一个统一的平台，探索先进的强化学习范式，并适应多样化的应用场景。

Method: 通过解耦设计的RFT核心统一同步/异步、在线/离线等RFT模式，并结合高效的智能体-环境交互和优化的数据管道。

Result: Trinity-RFT展示了其通用性、灵活性和用户友好性，适用于多种RFT场景。

Conclusion: 该框架为大型语言模型的强化微调提供了一个强大且易用的解决方案。

Abstract: Trinity-RFT is a general-purpose, flexible and scalable framework designed
for reinforcement fine-tuning (RFT) of large language models. It is built with
a decoupled design, consisting of (1) an RFT-core that unifies and generalizes
synchronous/asynchronous, on-policy/off-policy, and online/offline modes of
RFT, (2) seamless integration for agent-environment interaction with high
efficiency and robustness, and (3) systematic data pipelines optimized for RFT.
Trinity-RFT can be easily adapted for diverse application scenarios, and serves
as a unified platform for exploring advanced reinforcement learning paradigms.
This technical report outlines the vision, features, design and implementations
of Trinity-RFT, accompanied by extensive examples demonstrating the utility and
user-friendliness of the proposed framework.

</details>


### [292] [Imagine Beyond! Distributionally Robust Auto-Encoding for State Space Coverage in Online Reinforcement Learning](https://arxiv.org/abs/2505.17830)
*Nicolas Castanet,Olivier Sigaud,Sylvain Lamprier*

Key words: Goal-Conditioned RL, Representation Learning, Distributional Robustness, Adversarial Training, State Coverage

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 为了解决目标条件强化学习（GCRL）在视觉环境中因高维稀疏观测导致的表征空间覆盖不足问题，本文提出了DRAG方法，通过分布鲁棒优化和对抗训练扩展潜在空间覆盖范围，显著提升了探索能力和下游任务性能。

Motivation: GCRL在视觉环境中因高维稀疏观测和策略依赖的表征学习导致潜在空间过度集中于频繁访问的状态，限制了技能学习的全面性。

Method: 提出DRAG方法，结合β-VAE框架与分布鲁棒优化，通过对抗性加权调整VAE训练数据的分布，促进潜在空间对未探索区域的覆盖。

Result: 在迷宫和机器人避障等硬探索任务中，DRAG显著提升了状态空间覆盖率和下游控制性能，且无需预训练或先验环境知识。

Conclusion: DRAG通过动态调整表征学习的分布偏差，解决了GCRL中的探索瓶颈，为复杂环境中的自主技能学习提供了有效方案。

Abstract: Goal-Conditioned Reinforcement Learning (GCRL) enables agents to autonomously
acquire diverse behaviors, but faces major challenges in visual environments
due to high-dimensional, semantically sparse observations. In the online
setting, where agents learn representations while exploring, the latent space
evolves with the agent's policy, to capture newly discovered areas of the
environment. However, without incentivization to maximize state coverage in the
representation, classical approaches based on auto-encoders may converge to
latent spaces that over-represent a restricted set of states frequently visited
by the agent. This is exacerbated in an intrinsic motivation setting, where the
agent uses the distribution encoded in the latent space to sample the goals it
learns to master. To address this issue, we propose to progressively enforce
distributional shifts towards a uniform distribution over the full state space,
to ensure a full coverage of skills that can be learned in the environment. We
introduce DRAG (Distributionally Robust Auto-Encoding for GCRL), a method that
combines the $\beta$-VAE framework with Distributionally Robust Optimization.
DRAG leverages an adversarial neural weighter of training states of the VAE, to
account for the mismatch between the current data distribution and unseen parts
of the environment. This allows the agent to construct semantically meaningful
latent spaces beyond its immediate experience. Our approach improves state
space coverage and downstream control performance on hard exploration
environments such as mazes and robotic control involving walls to bypass,
without pre-training nor prior environment knowledge.

</details>


### [293] [TransDF: Time-Series Forecasting Needs Transformed Label Alignment](https://arxiv.org/abs/2505.17847)
*Hao Wang,Licheng Pan,Zhichao Chen,Xu Chen,Qingyang Dai,Lei Wang,Haoxuan Li,Zhouchen Lin*

Key words: 时间序列预测, 去相关, 标签自相关, TransDF, 模型优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为TransDF的新方法，通过将标签序列转换为去相关且重要性区分的分量来优化时间序列预测模型的训练，解决了标签自相关和任务过多的问题。

Motivation: 时间序列预测模型的现有目标函数（如均方误差）存在标签自相关和任务过多的问题，这些问题会引入偏差并增加优化难度。

Method: TransDF方法通过将标签序列转换为去相关的分量，并根据其重要性进行区分训练，以减少自相关性和任务复杂度。

Result: 实验表明，TransDF在性能上达到了最优，并且兼容多种预测模型。

Conclusion: TransDF有效解决了标签自相关和任务过多的问题，提升了预测模型的性能。

Abstract: Training time-series forecasting models presents unique challenges in
designing effective learning objectives. Existing methods predominantly utilize
the temporal mean squared error, which faces two critical challenges: (1) label
autocorrelation, which leads to bias from the label sequence likelihood; (2)
excessive amount of tasks, which increases with the forecast horizon and
complicates optimization. To address these challenges, we propose
Transform-enhanced Direct Forecast (TransDF), which transforms the label
sequence into decorrelated components with discriminated significance. Models
are trained to align the most significant components, thereby effectively
mitigating label autocorrelation and reducing task amount. Extensive
experiments demonstrate that TransDF achieves state-of-the-art performance and
is compatible with various forecasting models. Code is available at
https://anonymous.4open.science/r/TransDF-88CF.

</details>


### [294] [Scaling Recurrent Neural Networks to a Billion Parameters with Zero-Order Optimization](https://arxiv.org/abs/2505.17852)
*Francois Chaubard,Mykel Kochenderfer*

Key words: RNN, 零阶优化, 随机向量梯度估计, BPTT, 内存效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出使用零阶优化方法（如随机向量梯度估计）替代BPTT训练RNN，显著减少内存和计算成本，同时匹配或超越BPTT的收敛速度，并在多个任务中表现优异。

Motivation: 标准BPTT方法在训练长上下文的RNN时需要大量内存和计算资源，限制了实用性。研究旨在找到更高效的训练方法。

Method: 采用零阶优化方法（如随机向量梯度估计和中心差分RGE）训练RNN，避免保留中间激活，显著降低内存需求。

Result: 新方法在收敛速度上匹配或超越BPTT（最高19倍），内存和计算成本大幅降低，且在过拟合、转导和语言建模任务中表现优异。

Conclusion: 零阶优化方法是一种高效且内存友好的RNN训练替代方案，能在多任务中达到或超越BPTT的性能，推动长上下文RNN的实际应用。

Abstract: During inference, Recurrent Neural Networks (RNNs) scale constant in both
FLOPs and GPU memory with increasing context length, as they compress all prior
tokens into a fixed-size memory. In contrast, transformers scale linearly in
FLOPs and, at best, linearly in memory during generation, since they must
attend to all previous tokens explicitly. Despite this inference-time
advantage, training large RNNs on long contexts remains impractical because
standard optimization methods depend on Backpropagation Through Time (BPTT).
BPTT requires retention of all intermediate activations during the forward
pass, causing memory usage to scale linearly with both context length and model
size. In this paper, we show that Zero-Order Optimization (ZOO) methods such as
Random-vector Gradient Estimation (RGE) can successfully replace BPTT to train
RNNs with convergence rates that match, or exceed BPTT by up to 19 fold, while
using orders of magnitude less memory and cost, as the model remains in
inference mode throughout training. We further demonstrate that
Central-Difference RGE (CD-RGE) corresponds to optimizing a smoothed surrogate
loss, inherently regularizing training and improving generalization. Our method
matches or outperforms BPTT across three settings: (1) overfitting, (2)
transduction, and (3) language modeling. Across all tasks, with sufficient
perturbations, our models generalize as well as or better than those trained
with BPTT, often in fewer steps. Despite the need for more forward passes per
step, we can surpass BPTT wall-clock time per step using recent advancements
such as FlashRNN and distributed inference.

</details>


### [295] [Out of the Shadows: Exploring a Latent Space for Neural Network Verification](https://arxiv.org/abs/2505.17854)
*Lukas Koller,Tobias Ladner,Matthias Althoff*

Key words: 神经网络验证、潜在空间、zonotopes、分支定界、GPU加速

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种新的潜在空间方法，用于神经网络的正式验证，通过迭代细化输入集来提高验证效率，并在神经网络验证竞赛中表现优异。

Motivation: 神经网络在安全关键应用中需要正式验证其行为可靠性，但现有方法因保守性常导致验证失败，亟需更高效的解决方案。

Method: 设计基于投影集（如zonotopes）的潜在空间，将输出约束传递至输入空间以迭代细化输入集，并利用GPU加速的矩阵运算实现高效验证。

Result: 该方法显著减少了分支定界过程中的子问题数量，并在VNN-COMP'24竞赛中展现出竞争力。

Conclusion: 潜在空间与迭代细化有效提升了神经网络验证的准确性和效率，其兼容GPU的矩阵运算进一步优化了性能。

Abstract: Neural networks are ubiquitous. However, they are often sensitive to small
input changes. Hence, to prevent unexpected behavior in safety-critical
applications, their formal verification -- a notoriously hard problem -- is
necessary. Many state-of-the-art verification algorithms use reachability
analysis or abstract interpretation to enclose the set of possible outputs of a
neural network. Often, the verification is inconclusive due to the conservatism
of the enclosure. To address this problem, we design a novel latent space for
formal verification that enables the transfer of output specifications to the
input space for an iterative specification-driven input refinement, i.e., we
iteratively reduce the set of possible inputs to only enclose the unsafe ones.
The latent space is constructed from a novel view of projection-based set
representations, e.g., zonotopes, which are commonly used in reachability
analysis of neural networks. A projection-based set representation is a
"shadow" of a higher-dimensional set -- a latent space -- that does not change
during a set propagation through a neural network. Hence, the input set and the
output enclosure are "shadows" of the same latent space that we can use to
transfer constraints. We present an efficient verification tool for neural
networks that uses our iterative refinement to significantly reduce the number
of subproblems in a branch-and-bound procedure. Using zonotopes as a set
representation, unlike many other state-of-the-art approaches, our approach can
be realized by only using matrix operations, which enables a significant
speed-up through efficient GPU acceleration. We demonstrate that our tool
achieves competitive performance, which would place it among the top-ranking
tools of the last neural network verification competition (VNN-COMP'24).

</details>


### [296] [Stochastic Weight Sharing for Bayesian Neural Networks](https://arxiv.org/abs/2505.17856)
*Moule Lin,Shuhao Guan,Weipeng Jing,Goetz Botterweck,Andrea Patane*

Key words: 贝叶斯神经网络、权重量化、高斯分布、Wasserstein距离、模型压缩

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文通过重新解释权重量化技术，利用2D自适应高斯分布、Wasserstein距离估计和alpha混合等方法，显著降低了贝叶斯神经网络的训练和推理计算成本，并在保持准确性的同时实现了模型压缩。

Motivation: 贝叶斯神经网络（BNNs）在深度学习中的不确定性量化方面具有优势，但由于计算需求高和训练深度架构的收敛困难，其应用受到限制。本文旨在解决这些问题，实现高效的大规模模型贝叶斯训练。

Method: 通过随机视角重新解构权重量化技术，使用2D自适应高斯分布、Wasserstein距离估计和alpha混合方法，将BNN的随机行为编码为低维高斯表示。

Result: 提出的方法在计算开销上降低了几个数量级，实现了大规模模型（如ResNet-101和Vision Transformer）的高效贝叶斯训练。模型参数压缩约50倍，模型大小减少75%，同时在CIFAR10、CIFAR100和ImageNet1k等基准测试中保持了与最先进方法相当的准确性和不确定性估计。

Conclusion: 该方法显著提升了贝叶斯神经网络的可扩展性和效率，同时保持了性能表现，为大规模深度学习中的不确定性量化提供了实用解决方案。

Abstract: While offering a principled framework for uncertainty quantification in deep
learning, the employment of Bayesian Neural Networks (BNNs) is still
constrained by their increased computational requirements and the convergence
difficulties when training very deep, state-of-the-art architectures. In this
work, we reinterpret weight-sharing quantization techniques from a stochastic
perspective in the context of training and inference with Bayesian Neural
Networks (BNNs). Specifically, we leverage 2D adaptive Gaussian distributions,
Wasserstein distance estimations, and alpha blending to encode the stochastic
behaviour of a BNN in a lower dimensional, soft Gaussian representation.
Through extensive empirical investigation, we demonstrate that our approach
significantly reduces the computational overhead inherent in Bayesian learning
by several orders of magnitude, enabling the efficient Bayesian training of
large-scale models, such as ResNet-101 and Vision Transformer (VIT). On various
computer vision benchmarks including CIFAR10, CIFAR100, and ImageNet1k. Our
approach compresses model parameters by approximately 50x and reduces model
size by 75, while achieving accuracy and uncertainty estimations comparable to
the state-of-the-art.

</details>


### [297] [Scalable Valuation of Human Feedback through Provably Robust Model Alignment](https://arxiv.org/abs/2505.17859)
*Masahiro Fujisawa,Masaki Adachi,Michael A. Osborne*

Key words: 语言模型对齐，噪声反馈，redescending，H"older-DPO，错误标签检测

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了H"older-DPO，一种具有可证明的redescending特性的对齐损失函数，用于从嘈杂的人类反馈中估计干净数据分布，并通过实验验证其在噪声对齐和错误标签检测方面的优越性。

Motivation: 由于众包人类反馈通常存在噪声，传统对齐方法无法在严重噪声下保持模型参数的稳定性（redescending性质），因此需要一个更鲁棒的对齐目标。

Method: 提出H"older-DPO，一种基于理论分析的损失函数，可估计干净数据分布，并通过梯度无关的方法自动检测和评估反馈中的错误标签。

Result: H"older-DPO在噪声对齐和错误标签检测方面达到最优性能，并在实际数据集中成功识别并去除了大量噪声标签，显著提升了对齐效果。

Conclusion: H"older-DPO不仅解决了噪声对齐的理论问题，还提供了一种可扩展的数据集评估方法，显著提升了对齐模型的鲁棒性和性能。

Abstract: Despite the importance of aligning language models with human preferences,
crowd-sourced human feedback is often noisy -- for example, preferring less
desirable responses -- posing a fundamental challenge to alignment. A truly
robust alignment objective should yield identical model parameters even under
severe label noise, a property known as redescending. We prove that no existing
alignment methods satisfy this property. To address this, we propose
H\"older-DPO, the first principled alignment loss with a provable redescending
property, enabling estimation of the clean data distribution from noisy
feedback. The aligned model estimates the likelihood of clean data, providing a
theoretically grounded metric for dataset valuation that identifies the
location and fraction of mislabels. This metric is gradient-free, enabling
scalable and automated human feedback valuation without costly manual
verification or clean validation dataset. H\"older-DPO achieves
state-of-the-art robust alignment performance while accurately detecting
mislabels in controlled datasets. Finally, we apply H\"older-DPO to widely used
alignment datasets, revealing substantial noise levels and demonstrating that
removing these mislabels significantly improves alignment performance across
methods.

</details>


### [298] [The emergence of sparse attention: impact of data distribution and benefits of repetition](https://arxiv.org/abs/2505.17863)
*Nicolas Zucchet,Francesco d'Angelo,Andrew K. Lampinen,Stephanie C. Y. Chan*

Key words: 稀疏注意力, Transformer, 涌现, 学习动力学, 幂律

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文研究了Transformer中稀疏注意力机制如何在训练过程中突然出现，发现其出现时机遵循与任务结构、架构及优化器选择相关的幂律规律，并通过理论与实践验证了重复性可加速其出现。

Motivation: 探索大规模语言模型中能力突然出现的机制，特别是稀疏注意力在训练中的涌现现象，以填补当前理解的空白。

Method: 结合对玩具模型的理论分析与小型Transformer在线性回归变体上的实证观察，揭示稀疏注意力涌现的动力学。

Result: 稀疏注意力涌现时间遵循与任务结构、架构和优化器相关的幂律规律，且重复性可显著加速其出现。

Conclusion: 研究为理解数据分布和模型设计如何影响涌现学习动力学提供了简单的理论框架。

Abstract: Emergence is a fascinating property of large language models and neural
networks more broadly: as models scale and train for longer, they sometimes
develop new abilities in sudden ways. Despite initial studies, we still lack a
comprehensive understanding of how and when these abilities emerge. To address
this gap, we study the emergence over training of sparse attention, a critical
and frequently observed attention pattern in Transformers. By combining
theoretical analysis of a toy model with empirical observations on small
Transformers trained on a linear regression variant, we uncover the mechanics
driving sparse attention emergence and reveal that emergence timing follows
power laws based on task structure, architecture, and optimizer choice. We
additionally find that repetition can greatly speed up emergence. Finally, we
confirm these results on a well-studied in-context associative recall task. Our
findings provide a simple, theoretically grounded framework for understanding
how data distributions and model design influence the learning dynamics behind
one form of emergence.

</details>


### [299] [DesignX: Human-Competitive Algorithm Designer for Black-Box Optimization](https://arxiv.org/abs/2505.17866)
*Hongshu Guo,Zeyuan Ma,Yining Ma,Xinglin Zhang,Wei-Neng Chen,Yue-Jiao Gong*

Key words: 黑箱优化, 自动化算法设计, 强化学习, 元训练, 蛋白质对接, 无人机路径规划

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: DesignX是一个自动化算法设计框架，能在几秒内生成针对特定黑箱优化问题的优化器，通过双代理强化学习系统在结构和参数设计上协作，显著优于人工设计的优化器。

Motivation: 解决黑箱优化器设计中缺乏问题特定知识和耗时手动调整的难题。

Method: 构建模块化算法空间，采用双代理强化学习系统进行结构和参数设计，并通过大规模元训练优化。

Result: 在合成测试和实际场景（如蛋白质对接、自动机器学习和无人机路径规划）中，DesignX生成的优化器性能远超人工设计。

Conclusion: DesignX不仅能发现超越专家直觉的算法模式，还为优化社区提供了宝贵的设计洞察。

Abstract: Designing effective black-box optimizers is hampered by limited
problem-specific knowledge and manual control that spans months for almost
every detail. In this paper, we present DesignX, the first automated algorithm
design framework that generates an effective optimizer specific to a given
black-box optimization problem within seconds. Rooted in the first principles,
we identify two key sub-tasks: 1) algorithm structure generation and 2)
hyperparameter control. To enable systematic construction, a comprehensive
modular algorithmic space is first built, embracing hundreds of algorithm
components collected from decades of research. We then introduce a dual-agent
reinforcement learning system that collaborates on structural and parametric
design through a novel cooperative training objective, enabling large-scale
meta-training across 10k diverse instances. Remarkably, through days of
autonomous learning, the DesignX-generated optimizers continuously surpass
human-crafted optimizers by orders of magnitude, either on synthetic testbed or
on realistic optimization scenarios such as Protein-docking, AutoML and UAV
path planning. Further in-depth analysis reveals DesignX's capability to
discover non-trivial algorithm patterns beyond expert intuition, which,
conversely, provides valuable design insights for the optimization community.
We provide DesignX's inference code at https://github.com/MetaEvo/DesignX.

</details>


### [300] [SpectraLDS: Provable Distillation for Linear Dynamical Systems](https://arxiv.org/abs/2505.17868)
*Devan Shah,Shlomo Fortgang,Sofiia Druchyna,Elad Hazan*

Key words: 对称线性动态系统, 谱变换, 凸优化, 推理效率, 语言建模

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了首个可证明的对称线性动态系统（LDS）学习方法，其精度不受系统状态维度或记忆能力的限制，通过谱变换实现端到端凸优化，提升推理效率。

Motivation: 现有LDS学习方法在状态维度或记忆能力方面存在局限性，需要一种更高效且精度不受影响的解决方案。

Method: 通过谱变换表示对称LDS，并将其反转为可学习的凸优化模型，实现端到端训练。

Result: SpectraLDS在序列预测任务（如语言建模）中保持精度，同时显著提升推理效率（恒定时间和空间复杂度）。

Conclusion: 该方法为高维LDS学习提供了一种高效且理论可证明的解决方案，适用于实际应用场景。

Abstract: We present the first provable method for identifying symmetric linear
dynamical systems (LDS) with accuracy guarantees that are independent of the
systems' state dimension or effective memory. Our approach builds upon recent
work that represents symmetric LDSs as convolutions learnable via fixed
spectral transformations. We show how to invert this representation, thereby
recovering an LDS model from its spectral transform and yielding an end-to-end
convex optimization procedure. This distillation preserves predictive accuracy
while enabling constant-time and constant-space inference per token,
independent of sequence length. We evaluate our method, SpectraLDS, as a
component in sequence prediction architectures and demonstrate that accuracy is
preserved while inference efficiency is improved on tasks such as language
modeling.

</details>


### [301] [Best Group Identification in Multi-Objective Bandits](https://arxiv.org/abs/2505.17869)
*Mohammad Shahverdikondori,Mohammad Reza Badri,Negar Kiyavash*

Key words: 多目标多臂老虎机,最佳组识别,Pareto最优,样本复杂度

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了多目标多臂老虎机中的最佳组识别问题，提出了两种情境下的算法，并验证了其性能。

Motivation: 在多目标多臂老虎机中，识别具有最佳性能的组是一个重要但尚未充分解决的问题。

Method: 提出了基于淘汰的算法，分别针对组Pareto集合识别和线性最佳组识别两种情境。

Result: 算法在理论上具有较低的样本复杂度，并在实验中表现出优异的性能。

Conclusion: 所提算法能有效识别最优组，为多目标决策问题提供了实用解决方案。

Abstract: We introduce the Best Group Identification problem in a multi-objective
multi-armed bandit setting, where an agent interacts with groups of arms with
vector-valued rewards. The performance of a group is determined by an
efficiency vector which represents the group's best attainable rewards across
different dimensions. The objective is to identify the set of optimal groups in
the fixed-confidence setting. We investigate two key formulations: group Pareto
set identification, where efficiency vectors of optimal groups are Pareto
optimal and linear best group identification, where each reward dimension has a
known weight and the optimal group maximizes the weighted sum of its efficiency
vector's entries. For both settings, we propose elimination-based algorithms,
establish upper bounds on their sample complexity, and derive lower bounds that
apply to any correct algorithm. Through numerical experiments, we demonstrate
the strong empirical performance of the proposed algorithms.

</details>


### [302] [BLAST: Balanced Sampling Time Series Corpus for Universal Forecasting Models](https://arxiv.org/abs/2505.17871)
*Zezhi Shao,Yujie Li,Fei Wang,Chengqing Yu,Yisong Fu,Tangwen Qian,Bin Xu,Boyu Diao,Yongjun Xu,Xueqi Cheng*

Key words: 时间序列预测, 数据多样性, 预训练, 平衡采样, BLAST

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: BLAST 是一种新颖的时间序列预训练数据集，通过平衡采样策略增强数据多样性，提升模型性能与泛化能力。

Motivation: 现有大规模时间序列数据集存在偏差和不平衡分布问题，导致模型性能和泛化能力不佳。

Method: BLAST 整合 3210 亿公开数据集观测值，通过统计指标和网格聚类实现平衡采样，结合网格插值增强多样性。

Result: 实验表明，BLAST 预训练的模型以更少的计算资源和训练 token 实现最优性能。

Conclusion: 数据多样性对提升时间和模型性能至关重要。

Abstract: The advent of universal time series forecasting models has revolutionized
zero-shot forecasting across diverse domains, yet the critical role of data
diversity in training these models remains underexplored. Existing large-scale
time series datasets often suffer from inherent biases and imbalanced
distributions, leading to suboptimal model performance and generalization. To
address this gap, we introduce BLAST, a novel pre-training corpus designed to
enhance data diversity through a balanced sampling strategy. First, BLAST
incorporates 321 billion observations from publicly available datasets and
employs a comprehensive suite of statistical metrics to characterize time
series patterns. Then, to facilitate pattern-oriented sampling, the data is
implicitly clustered using grid-based partitioning. Furthermore, by integrating
grid sampling and grid mixup techniques, BLAST ensures a balanced and
representative coverage of diverse patterns. Experimental results demonstrate
that models pre-trained on BLAST achieve state-of-the-art performance with a
fraction of the computational resources and training tokens required by
existing methods. Our findings highlight the pivotal role of data diversity in
improving both training efficiency and model performance for the universal
forecasting task.

</details>


### [303] [Mixture of Low Rank Adaptation with Partial Parameter Sharing for Time Series Forecasting](https://arxiv.org/abs/2505.17872)
*Licheng Pan,Zhichao Chen,Haoxuan Li,Guangyi Liu,Zhijian Xu,Zhaoran Liu,Hao Wang,Ying Wei*

Key words: 时间序列预测, 多任务预测, LoRA, MoLA, 表达能力瓶颈

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种解决多任务时间序列预测中表达能力瓶颈的两阶段框架，通过预训练基础模型并采用特定步骤的LoRA模块进行调整，进一步引入Mixture-of-LoRA (MoLA)模型，显著提升了预测性能。

Motivation: 多任务时间序列预测因不同时间步共享同一表示而存在表达能力瓶颈，导致不可避免的误差。为解决这一问题，提出了两阶段框架。

Method: 两阶段方法：首先预训练一个用于一步预测的基础模型，然后通过特定步骤的LoRA模块调整基础模型，并进一步引入MoLA模型实现步骤间的部分参数共享。

Result: 实验结果显示，MoLA显著提升了模型的表达能力，并优于当前最先进的时间序列预测方法。

Conclusion: 通过两阶段框架和MoLA模型，有效解决了多任务时间序列预测中的表达能力瓶颈问题，显著提升了预测性能。

Abstract: Multi-task forecasting has become the standard approach for time-series
forecasting (TSF). However, we show that it suffers from an Expressiveness
Bottleneck, where predictions at different time steps share the same
representation, leading to unavoidable errors even with optimal
representations. To address this issue, we propose a two-stage framework:
first, pre-train a foundation model for one-step-ahead prediction; then, adapt
it using step-specific LoRA modules.This design enables the foundation model to
handle any number of forecast steps while avoiding the expressiveness
bottleneck. We further introduce the Mixture-of-LoRA (MoLA) model, which
employs adaptively weighted LoRA experts to achieve partial parameter sharing
across steps. This approach enhances both efficiency and forecasting
performance by exploiting interdependencies between forecast steps. Experiments
show that MoLA significantly improves model expressiveness and outperforms
state-of-the-art time-series forecasting methods. Code is available at
https://anonymous.4open.science/r/MoLA-BC92.

</details>


### [304] [Semi-Supervised Multi-Label Feature Selection with Consistent Sparse Graph Learning](https://arxiv.org/abs/2505.17875)
*Yan Zhong,Xingyu Wu,Xinping Zhao,Li Zhang,Xinyuan Song,Lei Shi,Bingbing Jiang*

Key words: 多标签学习, 半监督学习, 特征选择, 稀疏图学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为SGMFS的多标签半监督特征选择方法，通过保持空间一致性和学习标签相关性来解决现有方法的不足，并在实验中验证了其优越性。

Motivation: 实际应用中的高维数据通常带有多种语义标签，但传统特征选择方法仅适用于单标签数据。现有多标签方法在半监督场景下面临标签相关性评估不足和相似图结构不优的问题。

Method: SGMFS通过构建低维独立标签子空间和同时稀疏重构标签空间与学习子空间，自适应学习相似图结构，以提升特征选择性能。

Result: 实验证明SGMFS在提升特征选择性能方面表现优越。

Conclusion: SGMFS通过空间一致性和标签相关性学习，有效解决了半监督多标签特征选择的挑战。

Abstract: In practical domains, high-dimensional data are usually associated with
diverse semantic labels, whereas traditional feature selection methods are
designed for single-label data. Moreover, existing multi-label methods
encounter two main challenges in semi-supervised scenarios: (1). Most
semi-supervised methods fail to evaluate the label correlations without enough
labeled samples, which are the critical information of multi-label feature
selection, making label-specific features discarded. (2). The similarity graph
structure directly derived from the original feature space is suboptimal for
multi-label problems in existing graph-based methods, leading to unreliable
soft labels and degraded feature selection performance. To overcome them, we
propose a consistent sparse graph learning method for multi-label
semi-supervised feature selection (SGMFS), which can enhance the feature
selection performance by maintaining space consistency and learning label
correlations in semi-supervised scenarios. Specifically, for Challenge (1),
SGMFS learns a low-dimensional and independent label subspace from the
projected features, which can compatibly cross multiple labels and effectively
achieve the label correlations. For Challenge (2), instead of constructing a
fixed similarity graph for semi-supervised learning, SGMFS thoroughly explores
the intrinsic structure of the data by performing sparse reconstruction of
samples in both the label space and the learned subspace simultaneously. In
this way, the similarity graph can be adaptively learned to maintain the
consistency between label space and the learned subspace, which can promote
propagating proper soft labels for unlabeled samples, facilitating the ultimate
feature selection. An effective solution with fast convergence is designed to
optimize the objective function. Extensive experiments validate the superiority
of SGMFS.

</details>


### [305] [FastCAV: Efficient Computation of Concept Activation Vectors for Explaining Deep Neural Networks](https://arxiv.org/abs/2505.17883)
*Laines Schmalwasser,Niklas Penzel,Joachim Denzler,Julia Niebling*

Key words: 概念激活向量, 深度学习, 解释性方法, FastCAV, 模型训练

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: FastCAV是一种新型方法，显著加速了概念激活向量（CAV）的提取，速度提升最高63.6倍，为大规模高维架构提供了高效解决方案。

Motivation: 现有CAV计算方法计算成本高、耗时长，难以应用于大型高维架构，FastCAV旨在解决这一限制。

Method: 提出了FastCAV方法，通过理论验证其与SVM方法的等效性，并实际评估其加速效果和稳定性。

Result: FastCAV可将CAV提取速度提升46.4倍（平均），同时保持性能相似，并在下游应用中表现一致。

Conclusion: FastCAV为深度模型的概念研究提供了高效的替代方案，支持例如概念演化追踪等新研究。

Abstract: Concepts such as objects, patterns, and shapes are how humans understand the
world. Building on this intuition, concept-based explainability methods aim to
study representations learned by deep neural networks in relation to
human-understandable concepts. Here, Concept Activation Vectors (CAVs) are an
important tool and can identify whether a model learned a concept or not.
However, the computational cost and time requirements of existing CAV
computation pose a significant challenge, particularly in large-scale,
high-dimensional architectures. To address this limitation, we introduce
FastCAV, a novel approach that accelerates the extraction of CAVs by up to
63.6x (on average 46.4x). We provide a theoretical foundation for our approach
and give concrete assumptions under which it is equivalent to established
SVM-based methods. Our empirical results demonstrate that CAVs calculated with
FastCAV maintain similar performance while being more efficient and stable. In
downstream applications, i.e., concept-based explanation methods, we show that
FastCAV can act as a replacement leading to equivalent insights. Hence, our
approach enables previously infeasible investigations of deep models, which we
demonstrate by tracking the evolution of concepts during model training.

</details>


### [306] [Universal Domain Adaptation Benchmark for Time Series Data Representation](https://arxiv.org/abs/2505.17899)
*Romain Mussard,Fannia Pacheco,Maxime Berar,Gilles Gasso,Paul Honeine*

Key words: 深度学习, 时间序列, 通用领域自适应, 鲁棒性, 泛化能力

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文探讨了深度学习方法在时间序列数据中检测新事物的能力，强调了通用领域自适应（UniDA）在处理域转移和新类问题中的重要性，并提出了一种评估框架来比较不同时间序列骨干网络的鲁棒性和泛化能力。

Motivation: 时间序列数据具有固有的变异性，导致深度学习模型在泛化和鲁棒性方面表现不佳。为此，通用领域自适应（UniDA）被认为是一种解决方案，但在时间序列数据中尚未充分研究。

Method: 研究实现并比较了多种先进的时间序列骨干网络在UniDA框架下的表现，并提出了一种可靠协议来评估它们在不同领域中的鲁棒性和泛化能力。

Result: 结果表明，骨干网络的选择对于UniDA性能至关重要，并通过多种数据集和架构进行了鲁棒性分析。

Conclusion: 该研究为从业者提供了一个可扩展的框架，便于融入未来UniDA和时间序列架构的进展，同时强调了骨干网络选择的重要性。

Abstract: Deep learning models have significantly improved the ability to detect
novelties in time series (TS) data. This success is attributed to their strong
representation capabilities. However, due to the inherent variability in TS
data, these models often struggle with generalization and robustness. To
address this, a common approach is to perform Unsupervised Domain Adaptation,
particularly Universal Domain Adaptation (UniDA), to handle domain shifts and
emerging novel classes. While extensively studied in computer vision, UniDA
remains underexplored for TS data. This work provides a comprehensive
implementation and comparison of state-of-the-art TS backbones in a UniDA
framework. We propose a reliable protocol to evaluate their robustness and
generalization across different domains. The goal is to provide practitioners
with a framework that can be easily extended to incorporate future advancements
in UniDA and TS architectures. Our results highlight the critical influence of
backbone selection in UniDA performance and enable a robustness analysis across
various datasets and architectures.

</details>


### [307] [Evolving Machine Learning: A Survey](https://arxiv.org/abs/2505.17902)
*Ignacio Cabrera Martin,Subhaditya Mukherjee,Almas Baimagambetov,Joaquin Vanschoren,Nikolaos Polatidis*

Key words: Evolving Machine Learning, data drift, concept drift, catastrophic forgetting, skewed learning, network adaptation

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 传统机器学习模型在快速变化的数据环境中表现不足，EML作为一种新兴范式，通过实时数据流实现持续学习和适应。本文综合分析了EML的五大核心挑战及现有方法，提出了未来研究方向。

Motivation: 探讨传统机器学习在动态数据环境中的局限性，并展示EML如何在实时数据流中实现持续学习和适应。

Method: 系统回顾了120多项研究，分类比较了监督、无监督和半监督学习中的先进方法，并分析了评估指标、数据集和实际应用。

Result: 总结了当前EML技术的有效性及局限性，强调了自适应神经网络、元学习和集成策略在应对复杂数据中的作用。

Conclusion: 通过综合分析，揭示了EML领域的现状、关键问题和未来研究方向，旨在指导开发更健壮、可扩展的系统。

Abstract: In an era defined by rapid data evolution, traditional machine learning (ML)
models often fall short in adapting to dynamic environments. Evolving Machine
Learning (EML) has emerged as a critical paradigm, enabling continuous learning
and adaptation in real-time data streams. This survey presents a comprehensive
analysis of EML, focusing on five core challenges: data drift, concept drift,
catastrophic forgetting, skewed learning, and network adaptation. We
systematically review over 120 studies, categorizing state-of-the-art methods
across supervised, unsupervised, and semi-supervised approaches. The survey
explores diverse evaluation metrics, benchmark datasets, and real-world
applications, offering a comparative lens on the effectiveness and limitations
of current techniques. Additionally, we highlight the growing role of adaptive
neural architectures, meta-learning, and ensemble strategies in addressing
evolving data complexities. By synthesizing insights from recent literature,
this work not only maps the current landscape of EML but also identifies
critical gaps and opportunities for future research. Our findings aim to guide
researchers and practitioners in developing robust, ethical, and scalable EML
systems for real-world deployment.

</details>


### [308] [NeuroTrails: Training with Dynamic Sparse Heads as the Key to Effective Ensembling](https://arxiv.org/abs/2505.17909)
*Bram Grooten,Farid Hasanov,Chenxiang Zhang,Qiao Xiao,Boqian Wu,Zahra Atashgahi,Ghada Sokar,Shiwei Liu,Lu Yin,Elena Mocanu,Mykola Pechenizkiy,Decebal Constantin Mocanu*

Key words: 模型集成, 动态稀疏性, 多头架构, 泛化, 鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: NeuroTrails introduces一种稀疏多头架构，通过动态演化拓扑提升模型集成性能并减少资源消耗。

Motivation: 传统集成学习方法计算成本高，现有方法仍需大量推理计算。本文旨在解决这一资源消耗问题。

Method: 提出NeuroTrails，一种模型无关的稀疏多头架构，动态调整拓扑结构以优化预测多样性。

Result: 在ResNet-50/ImageNet和LLaMA-350M/C4等任务中，NeuroTrails在减少参数的同时提升了准确性和零样本泛化鲁棒性。

Conclusion: NeuroTrails通过动态稀疏性在预测多样性和资源效率间找到平衡，适用于多种架构。

Abstract: Model ensembles have long been a cornerstone for improving generalization and
robustness in deep learning. However, their effectiveness often comes at the
cost of substantial computational overhead. To address this issue,
state-of-the-art methods aim to replicate ensemble-class performance without
requiring multiple independently trained networks. Unfortunately, these
algorithms often still demand considerable compute at inference. In response to
these limitations, we introduce $\textbf{NeuroTrails}$, a sparse multi-head
architecture with dynamically evolving topology. This unexplored model-agnostic
training paradigm improves ensemble performance while reducing the required
resources. We analyze the underlying reason for its effectiveness and observe
that the various neural trails induced by dynamic sparsity attain a
$\textit{Goldilocks zone}$ of prediction diversity. NeuroTrails displays
efficacy with convolutional and transformer-based architectures on computer
vision and language tasks. Experiments on ResNet-50/ImageNet, LLaMA-350M/C4,
among many others, demonstrate increased accuracy and stronger robustness in
zero-shot generalization, while requiring significantly fewer parameters.

</details>


### [309] [LLM Meeting Decision Trees on Tabular Data](https://arxiv.org/abs/2505.17918)
*Hangting Ye,Jinmeng Li,He Zhao,Dandan Guo,Yi Chang*

Key words: 表格数据, 大语言模型, 决策树, 规则优化, 预测校准

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为DeLTa的新方法，通过将大语言模型（LLMs）与决策树规则结合，避免了传统表格数据序列化的局限性，并在多组基准测试中实现了最先进的性能。

Motivation: 现有基于LLM的表格数据处理方法存在数据序列化适应性差和隐私风险，且模型微调困难及上下文学习扩展性受限的问题。因此，需要一种不依赖序列化且无需微调的高效方法。

Method: 提出DeLTa方法，利用LLMs优化决策树规则，避免数据序列化，并通过校准机制修正原始决策树的预测错误。

Result: 在多个表格数据基准测试中，DeLTa实现了最优性能。

Conclusion: DeLTa提供了一种高效且隐私安全的表格数据预测方案，显著提升了LLMs在结构化数据中的应用效果。

Abstract: Tabular data have been playing a vital role in diverse real-world fields,
including healthcare, finance, etc. With the recent success of Large Language
Models (LLMs), early explorations of extending LLMs to the domain of tabular
data have been developed. Most of these LLM-based methods typically first
serialize tabular data into natural language descriptions, and then tune LLMs
or directly infer on these serialized data. However, these methods suffer from
two key inherent issues: (i) data perspective: existing data serialization
methods lack universal applicability for structured tabular data, and may pose
privacy risks through direct textual exposure, and (ii) model perspective: LLM
fine-tuning methods struggle with tabular data, and in-context learning
scalability is bottle-necked by input length constraints (suitable for few-shot
learning). This work explores a novel direction of integrating LLMs into
tabular data throughough logical decision tree rules as intermediaries,
proposes a decision tree enhancer with LLM-derived rule for tabular prediction,
DeLTa. The proposed DeLTa avoids tabular data serialization, and can be applied
to full data learning setting without LLM fine-tuning. Specifically, we
leverage the reasoning ability of LLMs to redesign an improved rule given a set
of decision tree rules. Furthermore, we provide a calibration method for
original decision trees via new generated rule by LLM, which approximates the
error correction vector to steer the original decision tree predictions in the
direction of ``errors'' reducing. Finally, extensive experiments on diverse
tabular benchmarks show that our method achieves state-of-the-art performance.

</details>


### [310] [KITINet: Kinetics Theory Inspired Network Architectures with PDE Simulation Approaches](https://arxiv.org/abs/2505.17919)
*Mingquan Feng,Yifan Fu,Tongcheng Zhang,Yu Jiang,Yixin Huang,Junchi Yan*

Key words: KITINet, residual connections, non-equilibrium dynamics, Boltzmann transport equation, PDE simulation, parameter condensation

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: KITINet, inspired by non-equilibrium particle dynamics, introduces a residual module modeling feature updates via Boltzmann transport equation, improving performance across tasks without significant computational cost increase.

Motivation: To address the heuristic design of residual connections by leveraging physics principles, specifically non-equilibrium particle dynamics and PDE simulation.

Method: Proposes KITINet, a network architecture using a residual module that simulates feature updates as stochastic particle evolution via a discretized Boltzmann transport equation solver.

Result: Consistent improvements over classic baselines in PDE operator tasks, image classification (CIFAR-10/100), and text classification (IMDb/SNLI), with minimal FLOPs increase.

Conclusion: Physics-inspired feature refinement and parameter condensation enhance network performance efficiently.

Abstract: Despite the widely recognized success of residual connections in modern
neural networks, their design principles remain largely heuristic. This paper
introduces KITINet (Kinetics Theory Inspired Network), a novel architecture
that reinterprets feature propagation through the lens of non-equilibrium
particle dynamics and partial differential equation (PDE) simulation. At its
core, we propose a residual module that models feature updates as the
stochastic evolution of a particle system, numerically simulated via a
discretized solver for the Boltzmann transport equation (BTE). This formulation
mimics particle collisions and energy exchange, enabling adaptive feature
refinement via physics-informed interactions. Additionally, we reveal that this
mechanism induces network parameter condensation during training, where
parameters progressively concentrate into a sparse subset of dominant channels.
Experiments on scientific computation (PDE operator), image classification
(CIFAR-10/100), and text classification (IMDb/SNLI) show consistent
improvements over classic network baselines, with negligible increase of FLOPs.

</details>


### [311] [Predicting Length of Stay in Neurological ICU Patients Using Classical Machine Learning and Neural Network Models: A Benchmark Study on MIMIC-IV](https://arxiv.org/abs/2505.17929)
*Alexander Gabitashvili,Philipp Kellmeyer*

Key words: ICU, 机器学习, 住院时长预测, 神经疾病, MIMIC-IV

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究探索了多种机器学习方法，用于预测神经疾病患者在ICU的住院时长（LOS），评估了经典算法和神经网络在MIMIC-IV数据集上的表现，发现随机森林和BERT模型在不同数据类型中表现最佳。

Motivation: 由于ICU资源管理至关重要，尤其是在COVID-19大流行期间，研究旨在评估机器学习在预测神经疾病患者ICU住院时长中的实用性，以优化资源分配和患者护理。

Method: 研究使用了MIMIC-IV数据集，对比了经典机器学习算法（KNN、随机森林、XGBoost、CatBoost）和神经网络（LSTM、BERT、Temporal Fusion Transformer），并将LOS分为三类（<2天、<1周、≥1周）进行预测。

Result: 随机森林在静态数据上表现最佳（准确率0.68），BERT在时间序列数据上优于LSTM（准确率0.80）。

Conclusion: 机器学习在预测神经疾病患者ICU住院时长方面具有潜力，随机森林和BERT模型展示了在不同数据类型中的优势，为ICU资源管理提供了新思路。

Abstract: Intensive care unit (ICU) is a crucial hospital department that handles
life-threatening cases. Nowadays machine learning (ML) is being leveraged in
healthcare ubiquitously. In recent years, management of ICU became one of the
most significant parts of the hospital functionality (largely but not only due
to the worldwide COVID-19 pandemic). This study explores multiple ML approaches
for predicting LOS in ICU specifically for the patients with neurological
diseases based on the MIMIC-IV dataset. The evaluated models include classic ML
algorithms (K-Nearest Neighbors, Random Forest, XGBoost and CatBoost) and
Neural Networks (LSTM, BERT and Temporal Fusion Transformer). Given that LOS
prediction is often framed as a classification task, this study categorizes LOS
into three groups: less than two days, less than a week, and a week or more. As
the first ML-based approach targeting LOS prediction for neurological disorder
patients, this study does not aim to outperform existing methods but rather to
assess their effectiveness in this specific context. The findings provide
insights into the applicability of ML techniques for improving ICU resource
management and patient care. According to the results, Random Forest model
proved to outperform others on static, achieving an accuracy of 0.68, a
precision of 0.68, a recall of 0.68, and F1-score of 0.67. While BERT model
outperformed LSTM model on time-series data with an accuracy of 0.80, a
precision of 0.80, a recall of 0.80 and F1-score 0.80.

</details>


### [312] [Understanding Gated Neurons in Transformers from Their Input-Output Functionality](https://arxiv.org/abs/2505.17936)
*Sebastian Gerstner,Hinrich Schütze*

Key words: 神经网络, 语言模型, 可解释性, 神经元交互

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文通过分析神经元输入与输出权重之间的余弦相似性，揭示了早期层多为“富集神经元”而后期层多为“贫化神经元”的现象，并认为富集神经元在概念表示中起关键作用。

Motivation: 现有研究主要关注语言模型多层感知机神经元的激活上下文和输出权重向量，而忽略了输入与输出交互的作用。本文旨在填补这一空白。

Method: 通过计算神经元输入与输出权重之间的余弦相似性，分析其交互方式，并应用于12个模型。

Result: 发现早期层以富集神经元（输入输出方向一致）为主，后期层更多表现为贫化神经元（输入输出方向相反）。

Conclusion: 富集神经元在概念表示中起重要作用，输入输出分析是对现有激活依赖方法的有效补充。

Abstract: Interpretability researchers have attempted to understand MLP neurons of
language models based on both the contexts in which they activate and their
output weight vectors. They have paid little attention to a complementary
aspect: the interactions between input and output. For example, when neurons
detect a direction in the input, they might add much the same direction to the
residual stream ("enrichment neurons") or reduce its presence ("depletion
neurons"). We address this aspect by examining the cosine similarity between
input and output weights of a neuron. We apply our method to 12 models and find
that enrichment neurons dominate in early-middle layers whereas later layers
tend more towards depletion. To explain this finding, we argue that enrichment
neurons are largely responsible for enriching concept representations, one of
the first steps of factual recall. Our input-output perspective is a complement
to activation-dependent analyses and to approaches that treat input and output
separately.

</details>


### [313] [Directed Semi-Simplicial Learning with Applications to Brain Activity Decoding](https://arxiv.org/abs/2505.17939)
*Manuel Lecha,Andrea Cavallo,Francesca Dominici,Ran Levi,Alessio Del Bue,Elvin Isufi,Pietro Morerio,Claudio Battiloro*

Key words: Graph Neural Networks, Topological Deep Learning, Semi-Simplicial Neural Networks, Brain Networks, Higher-order Relations

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了Semi-Simplicial Neural Networks（SSNs）和Routing-SSNs，用于解决现有拓扑深度学习方法无法捕捉的高阶有向关系，在脑网络等任务中表现优异。

Motivation: 现有方法（如GNNs）无法有效处理多路和层次关系，而TDL又仅限于无向设置，难以捕捉复杂系统中普遍存在的有向高阶模式。

Method: 引入SSNs和Routing-SSNs，基于半单纯集编码有向高阶关系，并通过动态选择最相关信息提升可扩展性。

Result: SSNs在脑动态分类任务中表现最佳，优于次优模型27%，比GNNs提升50%准确率；在其他任务上也有竞争力。

Conclusion: SSNs为结构脑数据学习提供了新方法，成为TDL的实际案例，并展示了其普适性。

Abstract: Graph Neural Networks (GNNs) excel at learning from pairwise interactions but
often overlook multi-way and hierarchical relationships. Topological Deep
Learning (TDL) addresses this limitation by leveraging combinatorial
topological spaces. However, existing TDL models are restricted to undirected
settings and fail to capture the higher-order directed patterns prevalent in
many complex systems, e.g., brain networks, where such interactions are both
abundant and functionally significant. To fill this gap, we introduce
Semi-Simplicial Neural Networks (SSNs), a principled class of TDL models that
operate on semi-simplicial sets -- combinatorial structures that encode
directed higher-order motifs and their directional relationships. To enhance
scalability, we propose Routing-SSNs, which dynamically select the most
informative relations in a learnable manner. We prove that SSNs are strictly
more expressive than standard graph and TDL models. We then introduce a new
principled framework for brain dynamics representation learning, grounded in
the ability of SSNs to provably recover topological descriptors shown to
successfully characterize brain activity. Empirically, SSNs achieve
state-of-the-art performance on brain dynamics classification tasks,
outperforming the second-best model by up to 27%, and message passing GNNs by
up to 50% in accuracy. Our results highlight the potential of principled
topological models for learning from structured brain data, establishing a
unique real-world case study for TDL. We also test SSNs on standard node
classification and edge regression tasks, showing competitive performance. We
will make the code and data publicly available.

</details>


### [314] [VeriThinker: Learning to Verify Makes Reasoning Model Efficient](https://arxiv.org/abs/2505.17941)
*Zigeng Chen,Xinyin Ma,Gongfan Fang,Ruonan Yu,Xinchao Wang*

Key words: Large Reasoning Models, Chain-of-Thought, VeriThinker, 推理压缩, 辅助验证任务

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: VeriThinker通过辅助验证任务压缩Chain-of-Thought推理链，减少推理开销同时保持或提升准确率。

Motivation: 大型推理模型（LRMs）过度思考导致推理链过长，显著增加推理成本，需要一种有效方法来抑制过度思考。

Method: 引入VeriThinker，通过辅助验证任务微调LRMs，使其能准确验证推理链的正确性，从而减少不必要的推理步骤。

Result: 在MATH500和AIME25数据集上，VeriThinker分别减少了推理链长度并提升了准确率，展示了零样本泛化能力。

Conclusion: VeriThinker是一种有效的CoT压缩方法，能显著降低推理成本而不损失精度。

Abstract: Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought
(CoT) reasoning. However, their tendency to overthinking leads to unnecessarily
lengthy reasoning chains, dramatically increasing inference costs. To mitigate
this issue, we introduce VeriThinker, a novel approach for CoT compression.
Unlike conventional methods that fine-tune LRMs directly on the original
reasoning task using synthetic concise CoT data, we innovatively fine-tune the
model solely through an auxiliary verification task. By training LRMs to
accurately verify the correctness of CoT solutions, the LRMs inherently become
more discerning about the necessity of subsequent self-reflection steps,
thereby effectively suppressing overthinking. Extensive experiments validate
that VeriThinker substantially reduces reasoning chain lengths while
maintaining or even slightly improving accuracy. When applied to
DeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500
from 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on
AIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to
40.8%). Additionally, our experiments demonstrate that VeriThinker can also be
zero-shot generalized to speculative reasoning. Code is available at
https://github.com/czg1225/VeriThinker

</details>


### [315] [A Principled Bayesian Framework for Training Binary and Spiking Neural Networks](https://arxiv.org/abs/2505.17962)
*James A. Walker,Moein Khajehnejad,Adeel Razi*

Key words: 贝叶斯框架、二值神经网络、脉冲神经网络、IW-ST估计器、变分推断

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种贝叶斯框架，用于训练二值和脉冲神经网络，无需归一化层即可达到最先进性能，并引入重要性加权直线估计器（IW-ST）和脉冲贝叶斯神经网络（SBNN）框架。

Motivation: 现有替代梯度方法通常依赖启发式且对超参数敏感，本文旨在通过概率模型实现端到端的梯度优化。

Method: 引入IW-ST估计器并分析其偏差-方差权衡，基于此提出SBNN框架，结合变分推断后验噪声训练网络。

Result: 在CIFAR-10、DVS Gesture和SHD等数据集上表现优于或匹配现有方法，无需归一化或手动调参。

Conclusion: 贝叶斯方法可减少梯度偏差并实现深度残差网络的高效训练。

Abstract: We propose a Bayesian framework for training binary and spiking neural
networks that achieves state-of-the-art performance without normalisation
layers. Unlike commonly used surrogate gradient methods -- often heuristic and
sensitive to hyperparameter choices -- our approach is grounded in a
probabilistic model of noisy binary networks, enabling fully end-to-end
gradient-based optimisation. We introduce importance-weighted straight-through
(IW-ST) estimators, a unified class generalising straight-through and
relaxation-based estimators. We characterise the bias-variance trade-off in
this family and derive a bias-minimising objective implemented via an auxiliary
loss. Building on this, we introduce Spiking Bayesian Neural Networks (SBNNs),
a variational inference framework that uses posterior noise to train Binary and
Spiking Neural Networks with IW-ST. This Bayesian approach minimises gradient
bias, regularises parameters, and introduces dropout-like noise. By linking
low-bias conditions, vanishing gradients, and the KL term, we enable training
of deep residual networks without normalisation. Experiments on CIFAR-10, DVS
Gesture, and SHD show our method matches or exceeds existing approaches without
normalisation or hand-tuned gradients.

</details>


### [316] [SVD-Free Low-Rank Adaptive Gradient Optimization for Large Language Models](https://arxiv.org/abs/2505.17967)
*Ionut-Vlad Modoranu,Mher Safaryan,Erik Schultheis,Dan Alistarh*

Key words: 低秩优化, 大型语言模型, SVD, DCT, 内存效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一个高效的两步法近似SVD梯度投影，以降低LLM训练中的内存和计算成本。

Motivation: 传统SVD方法在大型模型中计算和内存成本高，需要更高效的替代方案。

Method: 利用DCT预定义正交基，自适应选择对齐梯度的基向量，简化投影矩阵计算。

Result: 实验表明该方法性能与SVD相当，但运行更快且内存占用更低。

Conclusion: DCT基和自适应选择策略是高效低秩优化的可行方案。

Abstract: Low-rank optimization has emerged as a promising direction in training large
language models (LLMs) to reduce the memory usage of adaptive optimizers by
constraining learning to a lower-dimensional space. Prior work typically
projects gradients of linear layers using approaches based on Singular Value
Decomposition (SVD). However, applying SVD-based procedures individually to
each layer in large models is computationally expensive and incurs additional
memory costs due to storing the projection matrices. In this work, we propose a
computationally efficient and conceptually simple two-step procedure to
approximate SVD-based gradient projections into lower-dimensional spaces.
First, we construct a complete orthogonal basis using predefined orthogonal
matrices of the Discrete Cosine Transform (DCT). Second, we adaptively select
basis columns based on their alignment with the gradient of each layer. Each
projection matrix in our method is obtained via a single matrix multiplication
followed by a lightweight sorting step to identify the most relevant basis
vectors. Due to the predefined nature of the orthogonal bases, they are
computed once at the start of training. During training, we store only the
indices of the selected columns, avoiding the need to store full projection
matrices for each layer. Our numerical experiments on both pre-training and
fine-tuning tasks demonstrate the effectiveness of our dual strategy in
approximating optimal low-rank projections, matching the performance of costly
SVD-based methods while achieving faster runtime and reduced memory usage.

</details>


### [317] [Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems](https://arxiv.org/abs/2505.17968)
*Jiayi Geng,Howard Chen,Dilip Arumugam,Thomas L. Griffiths*

Key words: AI, black-box systems, LLMs, reverse-engineering, intervention, scientific discovery

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: LLMs perform poorly in passively observing black-box systems but improve when actively intervening to collect data, helping escape common failure modes.

Motivation: To understand if AI can autonomously reverse-engineer black-box systems, a key step for AI-driven scientific discovery.

Method: Tested LLMs on three black-box system types (Program, Formal Language, Math Equation) using passive observation vs. active intervention.

Result: Active intervention improves LLM performance by refining beliefs through edge-case testing, reducing overcomplication and overlooking.

Conclusion: Intervention enhances LLMs' reverse-engineering ability, supporting their role in AI-driven discovery.

Abstract: Using AI to create autonomous researchers has the potential to accelerate
scientific discovery. A prerequisite for this vision is understanding how well
an AI model can identify the underlying structure of a black-box system from
its behavior. In this paper, we explore how well a large language model (LLM)
learns to identify a black-box function from passively observed versus actively
collected data. We investigate the reverse-engineering capabilities of LLMs
across three distinct types of black-box systems, each chosen to represent
different problem domains where future autonomous AI researchers may have
considerable impact: Program, Formal Language, and Math Equation. Through
extensive experiments, we show that LLMs fail to extract information from
observations, reaching a performance plateau that falls short of the ideal of
Bayesian inference. However, we demonstrate that prompting LLMs to not only
observe but also intervene -- actively querying the black-box with specific
inputs to observe the resulting output -- improves performance by allowing LLMs
to test edge cases and refine their beliefs. By providing the intervention data
from one LLM to another, we show that this improvement is partly a result of
engaging in the process of generating effective interventions, paralleling
results in the literature on human learning. Further analysis reveals that
engaging in intervention can help LLMs escape from two common failure modes:
overcomplication, where the LLM falsely assumes prior knowledge about the
black-box, and overlooking, where the LLM fails to incorporate observations.
These insights provide practical guidance for helping LLMs more effectively
reverse-engineer black-box systems, supporting their use in making new
discoveries.

</details>


### [318] [Generalized Fisher-Weighted SVD: Scalable Kronecker-Factored Fisher Approximation for Compressing Large Language Models](https://arxiv.org/abs/2505.17974)
*Viktoriia Chekalina,Daniil Moskovskiy,Daria Cherniuk,Maxim Kurkin,Andrey Kuznetsov,Evgeny Frolov*

Key words: Fisher信息矩阵, Kronecker分解, LLM压缩, SVD, 参数重要性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了GFWSVD，一种考虑Fisher信息矩阵对角和非对角元素的LLM后训练压缩技术，通过Kronecker分解近似算法实现高效计算，相比现有方法在压缩率和性能上有显著提升。

Motivation: 现有方法通常仅使用Fisher信息矩阵的对角近似，忽略了参数相关性，导致下游任务性能下降。本文旨在通过考虑完整的Fisher信息矩阵，提升模型压缩的准确性。

Method: 提出了广义Fisher加权SVD（GFWSVD），结合Kronecker分解近似算法高效计算Fisher信息矩阵的对角和非对角元素。

Result: 在MMLU基准测试中，20倍压缩率下，GFWSVD比基于对角近似的FWSVD性能提升5%，优于SVD-LLM 3%和ASVD 6%。

Conclusion: GFWSVD通过更全面的Fisher信息矩阵建模，显著提升了LLM压缩的效率和性能。

Abstract: The Fisher information is a fundamental concept for characterizing the
sensitivity of parameters in neural networks. However, leveraging the full
observed Fisher information is too expensive for large models, so most methods
rely on simple diagonal approximations. While efficient, this approach ignores
parameter correlations, often resulting in reduced performance on downstream
tasks. In this work, we mitigate these limitations and propose Generalized
Fisher-Weighted SVD (GFWSVD), a post-training LLM compression technique that
accounts for both diagonal and off-diagonal elements of the Fisher information
matrix, providing a more accurate reflection of parameter importance. To make
the method tractable, we introduce a scalable adaptation of the
Kronecker-factored approximation algorithm for the observed Fisher information.
We demonstrate the effectiveness of our method on LLM compression, showing
improvements over existing compression baselines. For example, at a 20
compression rate on the MMLU benchmark, our method outperforms FWSVD, which is
based on a diagonal approximation of the Fisher information, by 5 percent,
SVD-LLM by 3 percent, and ASVD by 6 percent compression rate.

</details>


### [319] [ADLGen: Synthesizing Symbolic, Event-Triggered Sensor Sequences for Human Activity Modeling](https://arxiv.org/abs/2505.17987)
*Weihang You,Hanqi Jiang,Zishuai Liu,Zihang Xie,Tianming Liu,Jin Lu,Fei Dou*

Key words: ADLGen, 日常活动, 生成模型, 隐私保护, Transformer, 大型语言模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: ADLGen是一个生成框架，用于合成真实、事件触发的符号传感器序列，以解决实际生活中日常活动数据收集的隐私和成本问题。

Motivation: 实际日常活动数据收集存在隐私问题、部署和标注成本高，以及人类行为的稀疏性和不平衡性，因此需要一种生成方法来解决这些问题。

Method: ADLGen结合了解码器Transformer和符号时间编码，通过上下文和布局感知的采样机制生成传感器事件序列，并使用大型语言模型来优化生成的内容。

Result: 实验表明，ADLGen在统计保真度、语义丰富性和下游活动识别方面优于基线生成器，提供了可扩展且保护隐私的数据合成方案。

Conclusion: ADLGen为日常活动数据合成提供了一个高效、隐私保护的解决方案，并在多个评估指标中表现优异。

Abstract: Real world collection of Activities of Daily Living data is challenging due
to privacy concerns, costly deployment and labeling, and the inherent sparsity
and imbalance of human behavior. We present ADLGen, a generative framework
specifically designed to synthesize realistic, event triggered, and symbolic
sensor sequences for ambient assistive environments. ADLGen integrates a
decoder only Transformer with sign based symbolic temporal encoding, and a
context and layout aware sampling mechanism to guide generation toward
semantically rich and physically plausible sensor event sequences. To enhance
semantic fidelity and correct structural inconsistencies, we further
incorporate a large language model into an automatic generate evaluate refine
loop, which verifies logical, behavioral, and temporal coherence and generates
correction rules without manual intervention or environment specific tuning.
Through comprehensive experiments with novel evaluation metrics, ADLGen is
shown to outperform baseline generators in statistical fidelity, semantic
richness, and downstream activity recognition, offering a scalable and
privacy-preserving solution for ADL data synthesis.

</details>


### [320] [Towards Revealing the Effectiveness of Small-Scale Fine-tuning in R1-style Reinforcement Learning](https://arxiv.org/abs/2505.17988)
*Yutong Chen,Jiandong Gao,Ji Wu*

Key words: 强化学习, 大规模语言模型, 样本效率, Re-distillation, 推理能力

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了Re-distillation技术，通过从小规模的RL训练策略中蒸馏微调预训练模型，显著提升了样本效率和计算效率，在Knight & Knave和MATH数据集上验证了其有效性。

Motivation: 研究旨在揭示R1式强化学习（RL）的机制，并解决小规模SFT效率低的问题。

Method: 提出分析框架比较SFT和RL的样本效率，并提出Re-distillation技术，从RL训练策略中蒸馏微调模型。

Result: 在K&K和MATH数据集上，Re-distillation技术以更少的样本和计算量匹配甚至超越RL性能。

Conclusion: Re-distillation技术高效且可解释，为R1式RL的成功机制提供了新见解。

Abstract: R1-style Reinforcement Learning (RL) significantly enhances Large Language
Models' reasoning capabilities, yet the mechanism behind rule-based RL remains
unclear. We found that small-scale SFT has significant influence on RL but
shows poor efficiency. To explain our observations, we propose an analytical
framework and compare the efficiency of SFT and RL by measuring sample effect.
Hypothetical analysis show that SFT efficiency is limited by training data.
Guided by our analysis, we propose Re-distillation, a technique that fine-tunes
pretrain model through small-scale distillation from the RL-trained policy.
Experiments on Knight & Knave and MATH datasets demonstrate re-distillation's
surprising efficiency: re-distilled models match RL performance with far fewer
samples and less computation. Empirical verification shows that sample effect
is a good indicator of performance improvements. As a result, on K&K dataset,
our re-distilled Qwen2.5-1.5B model surpasses DeepSeek-V3-0324 with only 1K SFT
samples. On MATH, Qwen2.5-1.5B fine-tuned with re-distilled 500 samples matches
its instruct-tuned variant without RL. Our work explains several interesting
phenomena in R1-style RL, shedding light on the mechanisms behind its empirical
success. Code is available at: https://github.com/on1262/deep-reasoning

</details>


### [321] [Outcome-based Reinforcement Learning to Predict the Future](https://arxiv.org/abs/2505.17989)
*Benjamin Turtel,Danny Franklin,Kris Skotheim,Luke Hewitt,Philipp Schoenegger*

Key words: RLVR, GRPO, ReMax, 预测校准, 合成训练数据

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种改进的RLVR方法，通过调整GRPO和ReMax算法成功应用于嘈杂的预测任务中，14B模型在精度和校准上达到前沿水平，并展示了潜在经济价值。

Motivation: 研究旨在将RLVR从数学和编程扩展到复杂的预测领域，解决传统微调在延迟、噪声奖励下的脆弱性问题。

Method: 通过调整GRPO（去除方差缩放）和ReMax（引入基线调整优势），结合10万合成问题训练和轻量级规则约束，对14B模型进行稳定训练。

Result: 模型在测试集上精度与前沿基准相当（Brier=0.193），校准更优（ECE=0.042），模拟交易利润更高（127美元vs92美元）。

Conclusion: 改进的RLVR方法能将小规模LLM转化为经济价值显著的预测工具，为更大模型的应用提供可能。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has boosted math and
coding in large language models, yet there has been little effort to extend
RLVR into messier, real-world domains like forecasting. One sticking point is
that outcome-based reinforcement learning for forecasting must learn from
binary, delayed, and noisy rewards, a regime where standard fine-tuning is
brittle. We show that outcome-only online RL on a 14B model can match
frontier-scale accuracy and surpass it in calibration and hypothetical
prediction market betting by adapting two leading algorithms, Group-Relative
Policy Optimisation (GRPO) and ReMax, to the forecasting setting. Our
adaptations remove per-question variance scaling in GRPO, apply
baseline-subtracted advantages in ReMax, hydrate training with 100k temporally
consistent synthetic questions, and introduce lightweight guard-rails that
penalise gibberish, non-English responses and missing rationales, enabling a
single stable pass over 110k events. Scaling ReMax to 110k questions and
ensembling seven predictions yields a 14B model that matches frontier baseline
o1 on accuracy on our holdout set (Brier = 0.193, p = 0.23) while beating it in
calibration (ECE = 0.042, p < 0.001). A simple trading rule turns this
calibration edge into \$127 of hypothetical profit versus \$92 for o1 (p =
0.037). This demonstrates that refined RLVR methods can convert small-scale
LLMs into potentially economically valuable forecasting tools, with
implications for scaling this to larger models.

</details>


### [322] [Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective](https://arxiv.org/abs/2505.17997)
*Jintian Shao,Yiming Cheng,Hongyi Huang,Beiwen Zhang,Zhiyu Wu,You Shan,Mingkai Zheng*

Key words: VAPO框架, 强化学习, 理论分析, 复杂推理, 泛化能力

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文通过理论角度探讨了VAPO框架，分析了其在解决强化学习挑战中的机制与局限性，旨在指导未来研究。

Motivation: 探讨VAPO框架的理论基础与潜在局限性，以推动更鲁棒、可泛化的强化学习方法的发展。

Method: 从理论角度分析VAPO框架，包括价值函数近似、自适应优势估计、令牌级优化的最优性等问题。

Result: 揭示了VAPO在复杂推理任务中的机制，并识别了其假设可能面临的挑战。

Conclusion: VAPO框架在实际应用中表现优异，但理论理解仍有不足，需进一步研究以提升其鲁棒性与泛化能力。

Abstract: The VAPO framework has demonstrated significant empirical success in
enhancing the efficiency and reliability of reinforcement learning for long
chain-of-thought (CoT) reasoning tasks with large language models (LLMs). By
systematically addressing challenges such as value model bias, heterogeneous
sequence lengths, and sparse reward signals, VAPO achieves state-of-the-art
performance. While its practical benefits are evident, a deeper theoretical
understanding of its underlying mechanisms and potential limitations is crucial
for guiding future advancements. This paper aims to initiate such a discussion
by exploring VAPO from a theoretical perspective, highlighting areas where its
assumptions might be challenged and where further investigation could yield
more robust and generalizable reasoning agents. We delve into the intricacies
of value function approximation in complex reasoning spaces, the optimality of
adaptive advantage estimation, the impact of token-level optimization, and the
enduring challenges of exploration and generalization.

</details>


### [323] [Rethinking Contrastive Learning in Graph Anomaly Detection: A Clean-View Perspective](https://arxiv.org/abs/2505.18002)
*Di Jin,Jingyi Cao,Xiaobao Wang,Bingdao Feng,Dongxiao He,Longbiao Wang,Jianwu Dang*

Key words: 图异常检测，干扰边，对比学习，CVGAD，渐进净化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一个名为CVGAD的框架，通过多尺度异常感知模块和渐进净化模块解决图异常检测中干扰边的问题，提升检测性能。

Motivation: 传统的图异常检测方法基于对比学习，但干扰边的存在破坏了相似性假设，导致检测性能下降。

Method: 提出了CVGAD框架，包括多尺度异常感知模块识别干扰源，以及渐进净化模块逐步移除干扰边。

Result: 在五个基准数据集上的实验验证了CVGAD的有效性。

Conclusion: CVGAD通过识别和移除干扰边，显著提升了图异常检测的性能。

Abstract: Graph anomaly detection aims to identify unusual patterns in graph-based
data, with wide applications in fields such as web security and financial fraud
detection. Existing methods typically rely on contrastive learning, assuming
that a lower similarity between a node and its local subgraph indicates
abnormality. However, these approaches overlook a crucial limitation: the
presence of interfering edges invalidates this assumption, since it introduces
disruptive noise that compromises the contrastive learning process.
Consequently, this limitation impairs the ability to effectively learn
meaningful representations of normal patterns, leading to suboptimal detection
performance. To address this issue, we propose a Clean-View Enhanced Graph
Anomaly Detection framework (CVGAD), which includes a multi-scale anomaly
awareness module to identify key sources of interference in the contrastive
learning process. Moreover, to mitigate bias from the one-step edge removal
process, we introduce a novel progressive purification module. This module
incrementally refines the graph by iteratively identifying and removing
interfering edges, thereby enhancing model performance. Extensive experiments
on five benchmark datasets validate the effectiveness of our approach.

</details>


### [324] [An Example Safety Case for Safeguards Against Misuse](https://arxiv.org/abs/2505.18003)
*Joshua Clymer,Jonah Weinbaum,Robert Kirk,Kimberly Mai,Selena Zhang,Xander Davies*

Key words: AI滥用防护, 安全案例, 红队测试, 提升模型, 风险评估

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种端到端的论据（“安全案例”），证明滥用防护措施能将AI助手风险降至低水平，包括红队测试和定量“提升模型”的应用。

Motivation: 现有AI滥用防护评估证据分散且难以联系实际决策，需要一种系统方法证明防护措施的有效性。

Method: 通过红队测试估计绕过防护的难度，结合定量模型（“提升模型”）评估防护措施对滥用的威慑效果，形成连续风险信号。

Result: 提供了一种具体方法，可动态监测风险并快速应对威胁，为“AI滥用风险低”提供了严谨依据。

Conclusion: 该方法为证明AI防护措施有效性提供了一条可行路径（非唯一），支持开发者在部署中持续管控风险。

Abstract: Existing evaluations of AI misuse safeguards provide a patchwork of evidence
that is often difficult to connect to real-world decisions. To bridge this gap,
we describe an end-to-end argument (a "safety case") that misuse safeguards
reduce the risk posed by an AI assistant to low levels. We first describe how a
hypothetical developer red teams safeguards, estimating the effort required to
evade them. Then, the developer plugs this estimate into a quantitative "uplift
model" to determine how much barriers introduced by safeguards dissuade misuse
(https://www.aimisusemodel.com/). This procedure provides a continuous signal
of risk during deployment that helps the developer rapidly respond to emerging
threats. Finally, we describe how to tie these components together into a
simple safety case. Our work provides one concrete path -- though not the only
path -- to rigorously justifying AI misuse risks are low.

</details>


### [325] [Distances for Markov chains from sample streams](https://arxiv.org/abs/2505.18005)
*Sergio Calo,Anders Jonsson,Gergely Neu,Ludovic Schwartz,Javier Segovia-Aguas*

Key words: 双模拟度量、随机优化、线性规划、样本访问、马尔可夫链

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了基于样本的随机优化方法，用于估计双模拟度量，无需显式转移模型，通过新的线性规划（LP）公式和随机原始对偶优化方法实现，并提供理论保证和实证验证。

Motivation: 现有计算双模拟度量的方法均假设完全了解转移动态，这在现实场景中不实用，因为通常只能获取样本轨迹。

Method: 提出一种随机优化方法，基于样本访问估计双模拟度量，通过新的线性规划（LP）公式和随机原始对偶优化方法实现。

Result: 提供了算法的样本复杂度理论保证，并通过实证评估验证了其有效性。

Conclusion: 该方法解决了实际应用中缺乏完整转移动态知识的问题，为双模拟度量的计算提供了实用方案。

Abstract: Bisimulation metrics are powerful tools for measuring similarities between
stochastic processes, and specifically Markov chains. Recent advances have
uncovered that bisimulation metrics are, in fact, optimal-transport distances,
which has enabled the development of fast algorithms for computing such metrics
with provable accuracy and runtime guarantees. However, these recent methods,
as well as all previously known methods, assume full knowledge of the
transition dynamics. This is often an impractical assumption in most real-world
scenarios, where typically only sample trajectories are available. In this
work, we propose a stochastic optimization method that addresses this
limitation and estimates bisimulation metrics based on sample access, without
requiring explicit transition models. Our approach is derived from a new linear
programming (LP) formulation of bisimulation metrics, which we solve using a
stochastic primal-dual optimization method. We provide theoretical guarantees
on the sample complexity of the algorithm and validate its effectiveness
through a series of empirical evaluations.

</details>


### [326] [Strictly Constrained Generative Modeling via Split Augmented Langevin Sampling](https://arxiv.org/abs/2505.18017)
*Matthieu Blanke,Yongquan Qu,Sara Shamekh,Pierre Gentine*

Key words: 深度学习，生成模型，物理约束，Langevin动力学，扩散模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为Split Augmented Langevin (SAL)的新算法，用于在生成模型中严格满足物理约束，提高了生成结果的物理合理性和应用效果。

Motivation: 当前深度生成模型在表示复杂物理系统时缺乏对生成结果物理合理性的保证，限制了其在科学和工程问题中的应用。

Method: 利用Langevin动力学的变分公式，提出了SAL算法，通过变量分裂逐步强制执行约束，并在扩散模型中验证其有效性。

Result: 在复杂物理系统中，约束扩散模型显著提高了预测精度和守恒量的保持，并在最优控制的可行性问题上展示了潜力。

Conclusion: SAL算法为在生成模型中强制执行物理约束提供了一种有效方法，显著提升了生成结果的物理合理性和应用性能。

Abstract: Deep generative models hold great promise for representing complex physical
systems, but their deployment is currently limited by the lack of guarantees on
the physical plausibility of the generated outputs. Ensuring that known
physical constraints are enforced is therefore critical when applying
generative models to scientific and engineering problems. We address this
limitation by developing a principled framework for sampling from a target
distribution while rigorously satisfying physical constraints. Leveraging the
variational formulation of Langevin dynamics, we propose Split Augmented
Langevin (SAL), a novel primal-dual sampling algorithm that enforces
constraints progressively through variable splitting, with convergence
guarantees. While the method is developed theoretically for Langevin dynamics,
we demonstrate its effective applicability to diffusion models. In particular,
we use constrained diffusion models to generate physical fields satisfying
energy and mass conservation laws. We apply our method to diffusion-based data
assimilation on a complex physical system, where enforcing physical constraints
substantially improves both forecast accuracy and the preservation of critical
conserved quantities. We also demonstrate the potential of SAL for challenging
feasibility problems in optimal control.

</details>


### [327] [Time to Spike? Understanding the Representational Power of Spiking Neural Networks in Discrete Time](https://arxiv.org/abs/2505.18023)
*Duc Anh Nguyen,Ernesto Araya,Adalbert Fono,Gitta Kutyniok*

Key words: 脉冲神经网络（SNN）、离散时间LIF模型、函数逼近、网络复杂度、理论分析

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了离散时间的LIF-SNN模型，探讨了其理论特性，包括函数逼近能力和网络复杂度，并通过数值实验验证了理论结果。

Motivation: 常规人工神经网络（ANN）存在能耗问题，而脉冲神经网络（SNN）作为替代方案，其理论基础相对缺乏，尤其是离散时间LIF-SNN模型的研究不足。

Method: 基于离散时间的LIF-SNN模型，分析了静态输入和输出下的函数特性，量化了网络规模对连续函数逼近的要求，并研究了延迟和深度对输入空间划分复杂度的影响。

Result: 研究表明离散时间LIF-SNN可实现多面体区域上的分段常数函数，并揭示了延迟对其输入空间复杂度的显著影响，与使用分段线性激活函数的ANN形成对比。

Conclusion: 离散时间LIF-SNN的理论分析填补了SNN研究的空白，为未来高效能耗神经网络设计提供了重要参考。

Abstract: Recent years have seen significant progress in developing spiking neural
networks (SNNs) as a potential solution to the energy challenges posed by
conventional artificial neural networks (ANNs). However, our theoretical
understanding of SNNs remains relatively limited compared to the ever-growing
body of literature on ANNs. In this paper, we study a discrete-time model of
SNNs based on leaky integrate-and-fire (LIF) neurons, referred to as
discrete-time LIF-SNNs, a widely used framework that still lacks solid
theoretical foundations. We demonstrate that discrete-time LIF-SNNs with static
inputs and outputs realize piecewise constant functions defined on polyhedral
regions, and more importantly, we quantify the network size required to
approximate continuous functions. Moreover, we investigate the impact of
latency (number of time steps) and depth (number of layers) on the complexity
of the input space partitioning induced by discrete-time LIF-SNNs. Our analysis
highlights the importance of latency and contrasts these networks with ANNs
employing piecewise linear activation functions. Finally, we present numerical
experiments to support our theoretical findings.

</details>


### [328] [Knot So Simple: A Minimalistic Environment for Spatial Reasoning](https://arxiv.org/abs/2505.18028)
*Zizhao Chen,Yoav Artzi*

Key words: KnotGym, 空间推理, 绳结操作, 强化学习, 模型预测控制

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: KnotGym是一个交互式环境，专注于复杂的空间推理和操作，通过基于图像的观察完成绳结操作任务，并评估了多种方法的表现。

Motivation: 为了解决空间推理和操作性任务中的复杂挑战，特别是基于纯图像观察的绳结操作问题，需要一个可量化复杂性的测试环境。

Method: 设计了KnotGym环境，包含不同复杂度的绳结操作任务，任务复杂度由绳结交叉次数定义。评估了基于模型的RL、模型预测控制和思维链推理等方法。

Result: KnotGym展示了整合感知、空间推理和操作性任务的挑战，不同方法在该环境中表现出各自的局限性。

Conclusion: KnotGym为研究复杂的空间推理和操作性任务提供了标准化测试平台，未来可扩展用于更多方法评估。

Abstract: We propose KnotGym, an interactive environment for complex, spatial reasoning
and manipulation. KnotGym includes goal-oriented rope manipulation tasks with
varying levels of complexity, all requiring acting from pure image
observations. Tasks are defined along a clear and quantifiable axis of
complexity based on the number of knot crossings, creating a natural
generalization test. KnotGym has a simple observation space, allowing for
scalable development, yet it highlights core challenges in integrating acute
perception, spatial reasoning, and grounded manipulation. We evaluate methods
of different classes, including model-based RL, model-predictive control, and
chain-of-thought reasoning, and illustrate the challenges KnotGym presents.
KnotGym is available at https://github.com/lil-lab/knotgym.

</details>


### [329] [Mahalanobis++: Improving OOD Detection via Feature Normalization](https://arxiv.org/abs/2505.18032)
*Maximilian Mueller,Matthias Hein*

Key words: OOD检测,马氏距离,$\ell_2$-归一化,高斯假设,预对数特征

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出通过$\ell_2$-归一化预对数特征来改进基于马氏距离的OOD检测方法，显著提升了44种不同模型的一致性表现。

Motivation: 在安全关键应用中，检测分布外（OOD）样本对部署可靠的机器学习模型至关重要。现有基于马氏距离的方法在ImageNet规模上表现不稳定，原因是特征范数变化大，违反了高斯分布假设。

Method: 提出对预对数特征进行简单的$\ell_2$-归一化，以减少特征范数变化，使其更符合共享协方差矩阵的高斯分布假设。

Result: 在44种不同架构和预训练方案的模型上，$\ell_2$-归一化显著且一致地改进了传统马氏距离方法，并优于其他近期提出的OOD检测方法。

Conclusion: $\ell_2$-归一化是一种简单有效的改进手段，能够显著提升基于马氏距离的OOD检测方法的性能。

Abstract: Detecting out-of-distribution (OOD) examples is an important task for
deploying reliable machine learning models in safety-critial applications.
While post-hoc methods based on the Mahalanobis distance applied to pre-logit
features are among the most effective for ImageNet-scale OOD detection, their
performance varies significantly across models. We connect this inconsistency
to strong variations in feature norms, indicating severe violations of the
Gaussian assumption underlying the Mahalanobis distance estimation. We show
that simple $\ell_2$-normalization of the features mitigates this problem
effectively, aligning better with the premise of normally distributed data with
shared covariance matrix. Extensive experiments on 44 models across diverse
architectures and pretraining schemes show that $\ell_2$-normalization improves
the conventional Mahalanobis distance-based approaches significantly and
consistently, and outperforms other recently proposed OOD detection methods.

</details>


### [330] [Improved Algorithms for Overlapping and Robust Clustering of Edge-Colored Hypergraphs: An LP-Based Combinatorial Approach](https://arxiv.org/abs/2505.18043)
*Changyeol Lee,Yongho Shin,Hyung-Chan An*

Key words: 边缘着色聚类, 线性规划, 组合算法, 复杂性理论, 不可近似性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种结合线性规划和组合算法优势的框架，用于解决三类边缘着色聚类问题，实验和理论分析表明该方法高效且高质量。

Motivation: 传统边缘着色聚类（ECC）存在非重叠和穷举性限制，现有方法如LP舍入算法高质量但耗时，贪心算法快速但质量低。

Method: 结合LP和组合算法的框架，分别针对Local ECC、Global ECC和Robust ECC设计高效算法。

Result: 实验和理论分析证明新框架在质量和效率上优于现有方法，并回答了两个开放性问题。

Conclusion: 该算法框架为ECC问题提供了平衡质量和效率的解决方案，且理论分析表明进一步改进空间有限。

Abstract: Clustering is a fundamental task in both machine learning and data mining.
Among various methods, edge-colored clustering (ECC) has emerged as a useful
approach for handling categorical data. Given a hypergraph with (hyper)edges
labeled by colors, ECC aims to assign vertex colors to minimize the number of
edges where the vertex color differs from the edge's color. However,
traditional ECC has inherent limitations, as it enforces a nonoverlapping and
exhaustive clustering. To tackle these limitations, three versions of ECC have
been studied: Local ECC and Global ECC, which allow overlapping clusters, and
Robust ECC, which accounts for vertex outliers. For these problems, both linear
programming (LP) rounding algorithms and greedy combinatorial algorithms have
been proposed. While these LP-rounding algorithms provide high-quality
solutions, they demand substantial computation time; the greedy algorithms, on
the other hand, run very fast but often compromise solution quality. In this
paper, we present an algorithmic framework that combines the strengths of LP
with the computational efficiency of combinatorial algorithms. Both
experimental and theoretical analyses show that our algorithms efficiently
produce high-quality solutions for all three problems: Local, Global, and
Robust ECC. We complement our algorithmic contributions with
complexity-theoretic inapproximability results and integrality gap bounds,
which suggest that significant theoretical improvements are unlikely. Our
results also answer two open questions previously raised in the literature.

</details>


### [331] [Linear Mixture Distributionally Robust Markov Decision Processes](https://arxiv.org/abs/2505.18044)
*Zhishuai Liu,Pan Xu*

Key words: DRMDP, 线性混合模型, 不确定性集, $f$-散度, 样本复杂度

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种线性混合DRMDP框架，通过围绕混合权重参数定义不确定性集，改进了传统模型的不确定性表示，并提出了一种通用$f$-散度下的元算法，分析了其在不同散度度量下的样本复杂度。

Motivation: 现实决策问题常面临动态变化的挑战，传统DRMDP方法依赖对动态先验知识设计不确定性集，而线性混合DRMDP能更精细地表示不确定性。

Method: 提出线性混合DRMDP框架，定义基于混合权重参数的不确定性集，并设计通用$f$-散度下的元算法。

Result: 证明了线性混合DRMDP的统计可学习性，分析了其在总变差、KL和$χ^2$散度下的样本复杂度。

Conclusion: 线性混合DRMDP为动态不确定性问题提供了新理论基础，未来可进一步研究。

Abstract: Many real-world decision-making problems face the off-dynamics challenge: the
agent learns a policy in a source domain and deploys it in a target domain with
different state transitions. The distributionally robust Markov decision
process (DRMDP) addresses this challenge by finding a robust policy that
performs well under the worst-case environment within a pre-specified
uncertainty set of transition dynamics. Its effectiveness heavily hinges on the
proper design of these uncertainty sets, based on prior knowledge of the
dynamics. In this work, we propose a novel linear mixture DRMDP framework,
where the nominal dynamics is assumed to be a linear mixture model. In contrast
with existing uncertainty sets directly defined as a ball centered around the
nominal kernel, linear mixture DRMDPs define the uncertainty sets based on a
ball around the mixture weighting parameter. We show that this new framework
provides a more refined representation of uncertainties compared to
conventional models based on $(s,a)$-rectangularity and $d$-rectangularity,
when prior knowledge about the mixture model is present. We propose a meta
algorithm for robust policy learning in linear mixture DRMDPs with general
$f$-divergence defined uncertainty sets, and analyze its sample complexities
under three divergence metrics instantiations: total variation,
Kullback-Leibler, and $\chi^2$ divergences. These results establish the
statistical learnability of linear mixture DRMDPs, laying the theoretical
foundation for future research on this new setting.

</details>


### [332] [Learning with Restricted Boltzmann Machines: Asymptotics of AMP and GD in High Dimensions](https://arxiv.org/abs/2505.18046)
*Yizhou Xu,Florent Krzakala,Lenka Zdeborová*

Key words: 受限玻尔兹曼机, 多指标模型, 近似消息传递, 尖峰协方差模型, 无监督学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文分析了受限玻尔兹曼机（RBM）在高维输入空间和少量隐藏单元情况下的训练动态，揭示了其与多指标模型的联系，并证明其在尖峰协方差模型中能达到最优计算弱恢复阈值。

Motivation: 虽然RBM是最简单的生成神经网络之一，但其在训练数据学习中的性能分析主要局限于通过奇异值分解处理的情况。本文旨在高维输入空间下简化RBM训练目标，探索其与多指标模型的联系，并分析其在无监督学习任务中的表现。

Method: 论文通过将RBM训练目标简化为多指标模型形式（含不可分正则化），利用近似消息传递（AMP）、状态演化及动态平均场理论分析梯度下降（GD）的训练动态。

Result: 研究证明，在尖峰协方差模型中，RBM的训练动态能够达到最优计算弱恢复阈值，且与BBP过渡对齐。

Conclusion: RBM在高维输入空间的训练可以与多指标模型理论框架结合，其性能分析为无监督学习提供了新的理论支持。

Abstract: The Restricted Boltzmann Machine (RBM) is one of the simplest generative
neural networks capable of learning input distributions. Despite its
simplicity, the analysis of its performance in learning from the training data
is only well understood in cases that essentially reduce to singular value
decomposition of the data. Here, we consider the limit of a large dimension of
the input space and a constant number of hidden units. In this limit, we
simplify the standard RBM training objective into a form that is equivalent to
the multi-index model with non-separable regularization. This opens a path to
analyze training of the RBM using methods that are established for multi-index
models, such as Approximate Message Passing (AMP) and its state evolution, and
the analysis of Gradient Descent (GD) via the dynamical mean-field theory. We
then give rigorous asymptotics of the training dynamics of RBM on data
generated by the spiked covariance model as a prototype of a structure suitable
for unsupervised learning. We show in particular that RBM reaches the optimal
computational weak recovery threshold, aligning with the BBP transition, in the
spiked covariance model.

</details>


### [333] [Asymptotically optimal regret in communicating Markov decision processes](https://arxiv.org/abs/2505.18064)
*Victor Boone*

Key words: 马尔可夫决策过程, 遗憾界, 探索与利用, 正则化, 通信假设

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种针对通信马尔可夫决策过程的渐近最优遗憾学习算法，通过平衡探索与利用，并引入正则化机制估计关键常数$K(M)$。

Motivation: 研究旨在解决马尔可夫决策过程（MDP）中的最优遗憾问题，特别是在通信假设下实现渐近最优性能。

Method: 算法通过显式追踪常数$K(M)$，平衡探索、协同探索与利用，并结合正则化机制精确估计$K(M)$。

Result: 算法达到$K(M)\log(T) + \mathrm{o}(\log(T))$的遗憾上界，并揭示了$K(M)$的不连续性挑战。

Conclusion: 该算法在通信MDP中实现了理论最优遗憾，并通过正则化机制克服了常数$K(M)$的不连续性难题。

Abstract: In this paper, we present a learning algorithm that achieves asymptotically
optimal regret for Markov decision processes in average reward under a
communicating assumption. That is, given a communicating Markov decision
process $M$, our algorithm has regret $K(M) \log(T) + \mathrm{o}(\log(T))$
where $T$ is the number of learning steps and $K(M)$ is the best possible
constant. This algorithm works by explicitly tracking the constant $K(M)$ to
learn optimally, then balances the trade-off between exploration (playing
sub-optimally to gain information), co-exploration (playing optimally to gain
information) and exploitation (playing optimally to score maximally). We
further show that the function $K(M)$ is discontinuous, which is a consequence
challenge for our approach. To that end, we describe a regularization mechanism
to estimate $K(M)$ with arbitrary precision from empirical data.

</details>


### [334] [Reward Model Generalization for Compute-Aware Test-Time Reasoning](https://arxiv.org/abs/2505.18065)
*Zeen Song,Wenwen Qiang,Siyu Zhao,Changwen Zheng,Gang Hua*

Key words: 大型语言模型，测试时间推理，计算最优性，PAC-Bayes理论，CATS框架

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种外部测试时间推理方法，通过解耦生成和选择过程，结合奖励模型优化推理路径选择，进而提出计算感知树搜索（CATS）框架，动态控制搜索行为和预算分配，显著提升推理性能。

Motivation: 大型语言模型在推理时通过生成多条路径并选择最优解来增强性能，但如何在固定计算预算下最大化答案准确性（即测试时间计算最优性，TCO）是一个核心挑战。论文旨在通过理论分析和框架设计解决这一问题。

Method: 论文提出计算感知树搜索（CATS）框架，结合PAC-Bayes理论推导PRM的泛化边界，动态控制搜索行为。框架中actor输出采样参数，critic评估其效用以指导预算分配。

Result: 在MATH和AIME基准测试中，CATS框架显著优于其他外部TTS方法，验证了理论分析的正确性。

Conclusion: 研究表明，PRM的泛化误差直接影响计算效率和推理性能，CATS框架能有效优化推理路径选择和预算分配。

Abstract: External test-time reasoning enhances large language models (LLMs) by
decoupling generation and selection. At inference time, the model generates
multiple reasoning paths, and an auxiliary process reward model (PRM) is used
to score and select the best one. A central challenge in this setting is
test-time compute optimality (TCO), i.e., how to maximize answer accuracy under
a fixed inference budget. In this work, we establish a theoretical framework to
analyze how the generalization error of the PRM affects compute efficiency and
reasoning performance. Leveraging PAC-Bayes theory, we derive generalization
bounds and show that a lower generalization error of PRM leads to fewer samples
required to find correct answers. Motivated by this analysis, we propose
Compute-Aware Tree Search (CATS), an actor-critic framework that dynamically
controls search behavior. The actor outputs sampling hyperparameters based on
reward distributions and sparsity statistics, while the critic estimates their
utility to guide budget allocation. Experiments on the MATH and AIME benchmarks
with various LLMs and PRMs demonstrate that CATS consistently outperforms other
external TTS methods, validating our theoretical predictions.

</details>


### [335] [Emergence of Hebbian Dynamics in Regularized Non-Local Learners](https://arxiv.org/abs/2505.18069)
*David Koplow,Tomaso Poggio,Liu Ziyin*

Key words: 随机梯度下降, 赫布学习, 权重衰减, 生物学习机制, 优化原理

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文表明，随机梯度下降（SGD）与赫布学习在学习信号上存在理论和实证联系，揭示了赫布特性可能是更深层优化原则的表现，并为人工与生物学习的差异提供了新视角。

Motivation: 研究旨在弥合人工学习（SGD）与生物学习（赫布学习）之间的理论鸿沟，探索二者在信号层面的潜在联系。

Method: 通过理论和实证分析，比较SGD（含权重衰减）与赫布学习在收敛时的学习信号差异，并验证噪声注入的SGD与反赫布学习的关联。

Result: 发现SGD与赫布学习在特定条件下（如权重衰减）信号相似，且赫布特性可能源于更广泛的优化原则，而非仅限生物机制。

Conclusion: 赫布特性可能是优化过程的副产品，需谨慎将其视为生物学习的唯一证据，为人工与生物学习的统一提供了新思路。

Abstract: Stochastic Gradient Descent (SGD) has emerged as a remarkably effective
learning algorithm, underpinning nearly all state-of-the-art machine learning
models, from large language models to autonomous vehicles. Despite its
practical success, SGD appears fundamentally distinct from biological learning
mechanisms. It is widely believed that the biological brain can not implement
gradient descent because it is nonlocal, and we have found little (if any)
experimental evidence for it. In contrast, the brain is widely thought to learn
via local Hebbian learning principles, which have been seen as incompatible
with gradient descent. In this paper, we establish a theoretical and empirical
connection between the learning signals of neural networks trained using SGD
with weight decay and those trained with Hebbian learning near convergence. We
show that SGD with regularization can appear to learn according to a Hebbian
rule, and SGD with injected noise according to an anti-Hebbian rule. We also
provide empirical evidence that Hebbian learning properties can emerge in a
network with weight decay from virtually any learning rule--even random ones.
These results may bridge a long-standing gap between artificial and biological
learning, revealing Hebbian properties as an epiphenomenon of deeper
optimization principles and cautioning against interpreting their presence in
neural data as evidence against more complex hetero-synaptic mechanisms.

</details>


### [336] [AFD-STA: Adaptive Filtering Denoising with Spatiotemporal Attention for Chaotic System Prediction](https://arxiv.org/abs/2505.18080)
*Chunlin Gong,Yin Wang,Jingru Li,Hanleran Zhang*

Key words: AFD-STA Net, 混沌系统, 偏微分方程, 自适应滤波, 时空注意力

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: AFD-STA Net是一种结合自适应滤波和时空动态学习的神经网络框架，用于预测由偏微分方程控制的高维混沌系统，通过多种模块协同工作，在复杂动态和噪声环境下保持高预测精度。

Motivation: 处理高维混沌系统中的测量不确定性和非线性动态，需要一种能同时适应不同状态和噪声的预测方法。

Method: 框架包括自适应指数平滑模块、并行注意力机制、动态门控多尺度特征融合和深度投影网络。

Result: 在非线性和强混沌系统中表现出高精度和噪声鲁棒性，模块消融研究突出了时空注意力机制的重要性。

Conclusion: AFD-STA Net在真实应用中具有潜力，能有效处理高维非线性动态和测量噪声。

Abstract: This paper presents AFD-STA Net, a neural framework integrating adaptive
filtering and spatiotemporal dynamics learning for predicting high-dimensional
chaotic systems governed by partial differential equations. The architecture
combines: 1) An adaptive exponential smoothing module with position-aware decay
coefficients for robust attractor reconstruction, 2) Parallel attention
mechanisms capturing cross-temporal and spatial dependencies, 3) Dynamic gated
fusion of multiscale features, and 4) Deep projection networks with
dimension-scaling capabilities. Numerical experiments on nonlinear PDE systems
demonstrate the model's effectiveness in maintaining prediction accuracy under
both smooth and strongly chaotic regimes while exhibiting noise tolerance
through adaptive filtering. Component ablation studies confirm critical
contributions from each module, particularly highlighting the essential role of
spatiotemporal attention in learning complex dynamical interactions. The
framework shows promising potential for real-world applications requiring
simultaneous handling of measurement uncertainties and high-dimensional
nonlinear dynamics.

</details>


### [337] [Backpropagation-Free Metropolis-Adjusted Langevin Algorithm](https://arxiv.org/abs/2505.18081)
*Adam D. Cobb,Susmit Jha*

Key words: 正向自动微分,MCMC,MALA,无反向传播,Hessian信息

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了四种无需反向传播的梯度马尔可夫链蒙特卡洛算法，通过正向自动微分优化性能。

Motivation: 为减少计算成本，利用正向自动微分实现优化，替代传统反向传播方法。

Method: 将切线向量采样集成到MALA提议机制中，并扩展到Hessian信息的预处理算法。

Result: 正向采样器计算成本更低，性能与原始MALA相当，部分场景更优。

Conclusion: 展示了正向自动微分在MCMC中的潜力，适用于分层分布和贝叶斯神经网络。

Abstract: Recent work on backpropagation-free learning has shown that it is possible to
use forward-mode automatic differentiation (AD) to perform optimization on
differentiable models. Forward-mode AD requires sampling a tangent vector for
each forward pass of a model. The result is the model evaluation with the
directional derivative along the tangent. In this paper, we illustrate how the
sampling of this tangent vector can be incorporated into the proposal mechanism
for the Metropolis-Adjusted Langevin Algorithm (MALA). As such, we are the
first to introduce a backpropagation-free gradient-based Markov chain Monte
Carlo (MCMC) algorithm. We also extend to a novel backpropagation-free
position-specific preconditioned forward-mode MALA that leverages Hessian
information. Overall, we propose four new algorithms: Forward MALA; Line
Forward MALA; Pre-conditioned Forward MALA, and Pre-conditioned Line Forward
MALA. We highlight the reduced computational cost of the forward-mode samplers
and show that forward-mode is competitive with the original MALA, while even
outperforming it depending on the probabilistic model. We include Bayesian
inference results on a range of probabilistic models, including hierarchical
distributions and Bayesian neural networks.

</details>


### [338] [An Iterative Framework for Generative Backmapping of Coarse Grained Proteins](https://arxiv.org/abs/2505.18082)
*Georgios Kementzidis,Erin Wong,John Nicholson,Ruichen Xu,Yuefan Deng*

Key words: 数据驱动映射，条件变分自编码器，图神经网络，蛋白质，粗粒度到细粒度

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种基于条件变分自编码器和图神经网络的迭代框架，用于解决粗粒度到细粒度表示的挑战，特别针对蛋白质等复杂系统，提高了重建精度和计算效率。

Motivation: 传统数据驱动的粗粒度到细粒度映射方法在复杂系统（如蛋白质）中面临准确性不足、训练不稳定和物理现实性差的问题。

Method: 采用条件变分自编码器和图神经网络的迭代框架，逐步从粗粒度颗粒细化到全原子细节，并通过多步方案优化训练。

Result: 多步方法不仅提高了重建精度，还显著提升了超粗粒度蛋白质训练的计算效率。

Conclusion: 该框架为复杂生物分子系统的粗粒度到细粒度映射提供了高效且准确的解决方案。

Abstract: The techniques of data-driven backmapping from coarse-grained (CG) to
fine-grained (FG) representation often struggle with accuracy, unstable
training, and physical realism, especially when applied to complex systems such
as proteins. In this work, we introduce a novel iterative framework by using
conditional Variational Autoencoders and graph-based neural networks,
specifically designed to tackle the challenges associated with such large-scale
biomolecules. Our method enables stepwise refinement from CG beads to full
atomistic details. We outline the theory of iterative generative backmapping
and demonstrate via numerical experiments the advantages of multistep schemes
by applying them to proteins of vastly different structures with very coarse
representations. This multistep approach not only improves the accuracy of
reconstructions but also makes the training process more computationally
efficient for proteins with ultra-CG representations.

</details>


### [339] [What Do You Need for Diverse Trajectory Stitching in Diffusion Planning?](https://arxiv.org/abs/2505.18083)
*Quentin Clark,Florian Shkurti*

Key words: stitching, 生成行为克隆, 扩散规划, 位置等变性, 局部接受性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文探讨了生成行为克隆（BC）方法中“stitching”能力的主要因素，提出了位置等变性和局部接受性这两个关键属性，并通过实验验证了它们的重要性。

Motivation: 理解生成行为克隆方法中实现“stitching”能力的主要因素，以便开发更可靠的新算法。

Method: 研究了基于扩散规划的生成BC方法，重点关注位置等变性和局部接受性这两个属性，并分析了架构、数据和推理选择对这些属性的影响。

Result: 实验表明，局部接受性比位置等变性更重要，但两者都关键；通过简单架构调整即可实现竞争力；基于修复的引导能提升目标条件泛化能力。

Conclusion: 位置等变性和局部接受性是扩散规划模型实现“stitching”能力的关键，简单的架构调整和引导方法可提升性能。

Abstract: In planning, stitching is an ability of algorithms to piece together
sub-trajectories of data they are trained on to generate new and diverse
behaviours. While stitching is historically a strength of offline reinforcement
learning, recent generative behavioural cloning (BC) methods have also shown
proficiency at stitching. However, the main factors behind this are poorly
understood, hindering the development of new algorithms that can reliably
stitch. Focusing on diffusion planners trained via BC, we find two properties
are needed to compose: \emph{positional equivariance} and \emph{local
receptiveness}. We use these two properties to explain architecture, data, and
inference choices in existing generative BC methods based on diffusion
planning, including replanning frequency, data augmentation, and data scaling.
Experimental comparisions show that (1) while locality is more important than
positional equivariance in creating a diffusion planner capable of composition,
both are crucial (2) enabling these properties through relatively simple
architecture choices can be competitive with more computationally expensive
methods such as replanning or scaling data, and (3) simple inpainting-based
guidance can guide architecturally compositional models to enable
generalization in goal-conditioned settings.

</details>


### [340] [Early-Exit Graph Neural Networks](https://arxiv.org/abs/2505.18088)
*Andrea Giuseppe Di Francesco,Maria Sofia Bucarelli,Franco Maria Nardini,Raffaele Perego,Nicola Tonellotto,Fabrizio Silvestri*

Key words: 早期退出机制, 图神经网络, 对称性偏置, 动态计算深度, 过平滑

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种结合早期退出机制的图神经网络（EEGNN），通过动态调整计算深度减少简单图的计算开销，同时在复杂图上保持高精度。

Motivation: 早期退出机制在深度神经网络中已被证明有效，但其在图神经网络（GNN）中的应用尚未充分探索。论文旨在解决GNN中深度架构面临的过平滑和过压缩问题，同时动态优化计算效率。

Method: 提出了基于对称-反对称图神经网络（SAS-GNN）的EEGNN，利用对称性偏置生成稳定的中间表示。在此基础上，引入置信感知的退出头，实现根据节点或图的复杂性动态终止传播。

Result: 实验表明，EEGNN在保持深度增长时性能稳定的同时，在异质性和长距离基准测试中达到与注意力机制和异步消息传递模型相当的精度，显著降低了计算和延迟。

Conclusion: EEGNN通过动态调整计算深度，在效率和精度之间实现了平衡，为GNN在复杂场景下的应用提供了新思路。

Abstract: Early-exit mechanisms allow deep neural networks to halt inference as soon as
classification confidence is high enough, adaptively trading depth for
confidence, and thereby cutting latency and energy on easy inputs while
retaining full-depth accuracy for harder ones. Similarly, adding early exit
mechanisms to Graph Neural Networks (GNNs), the go-to models for
graph-structured data, allows for dynamic trading depth for confidence on
simple graphs while maintaining full-depth accuracy on harder and more complex
graphs to capture intricate relationships. Although early exits have proven
effective across various deep learning domains, their potential within GNNs in
scenarios that require deep architectures while resisting over-smoothing and
over-squashing remains largely unexplored. We unlock that potential by first
introducing Symmetric-Anti-Symmetric Graph Neural Networks (SAS-GNN), whose
symmetry-based inductive biases mitigate these issues and yield stable
intermediate representations that can be useful to allow early exiting in GNNs.
Building on this backbone, we present Early-Exit Graph Neural Networks
(EEGNNs), which append confidence-aware exit heads that allow on-the-fly
termination of propagation based on each node or the entire graph. Experiments
show that EEGNNs preserve robust performance as depth grows and deliver
competitive accuracy on heterophilic and long-range benchmarks, matching
attention-based and asynchronous message-passing models while substantially
reducing computation and latency. We plan to release the code to reproduce our
experiments.

</details>


### [341] [Data Mixing Can Induce Phase Transitions in Knowledge Acquisition](https://arxiv.org/abs/2505.18091)
*Xinran Gu,Kaifeng Lyu,Jiazheng Li,Jingzhao Zhang*

Key words: LLMs, 混合数据训练, 知识获取, 阶段性转变, 幂律关系

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: LLMs训练在知识密集与网络混合数据上时，知识获取呈现阶段性转变，临界混合比例与模型大小呈幂律关系。

Motivation: 探究混合数据训练LLMs时知识获取的规律，尤其是为何知识密集数据的获取会表现出突变现象。

Method: 在合成传记数据集与网络数据的混合上进行控制实验，分析模型大小和混合比例对知识记忆的影响。

Result: 发现模型大小和混合比例存在临界点，超过后知识获取迅速提升，临界混合比例与模型大小成幂律关系。

Conclusion: 模型容量有限时，数据混合策略需根据模型大小调整，大模型与小模型的最优混合比例不同。

Abstract: Large Language Models (LLMs) are typically trained on data mixtures: most
data come from web scrapes, while a small portion is curated from high-quality
sources with dense domain-specific knowledge. In this paper, we show that when
training LLMs on such data mixtures, knowledge acquisition from knowledge-dense
datasets, unlike training exclusively on knowledge-dense data
(arXiv:2404.05405), does not always follow a smooth scaling law but can exhibit
phase transitions with respect to the mixing ratio and model size. Through
controlled experiments on a synthetic biography dataset mixed with web-scraped
data, we demonstrate that: (1) as we increase the model size to a critical
value, the model suddenly transitions from memorizing very few to most of the
biographies; (2) below a critical mixing ratio, the model memorizes almost
nothing even with extensive training, but beyond this threshold, it rapidly
memorizes more biographies. We attribute these phase transitions to a capacity
allocation phenomenon: a model with bounded capacity must act like a knapsack
problem solver to minimize the overall test loss, and the optimal allocation
across datasets can change discontinuously as the model size or mixing ratio
varies. We formalize this intuition in an information-theoretic framework and
reveal that these phase transitions are predictable, with the critical mixing
ratio following a power-law relationship with the model size. Our findings
highlight a concrete case where a good mixing recipe for large models may not
be optimal for small models, and vice versa.

</details>


### [342] [Towards more transferable adversarial attack in black-box manner](https://arxiv.org/abs/2505.18097)
*Chun Tong Lei,Zhongliang Guo,Hon Chung Lee,Minh Quoc Duong,Chun Pong Lau*

Key words: 对抗攻击, 黑盒攻击, 可迁移性, 扩散模型, 计算成本

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种新颖的损失函数和替代模型，通过在对抗优化过程中融入数据分布知识，显著提高了对抗样本的可迁移性，同时降低了计算开销，避免了依赖扩散模型的高成本问题。

Motivation: 现有的黑盒对抗攻击方法依赖扩散模型提升可迁移性，但计算成本高。作者假设类似归纳偏置的模型结合适当损失函数，可在降低成本的同时实现可比的性能。

Method: 提出了一种新颖的损失函数和独特的替代模型，利用分类器引导的扩散模型的时间相关分类器分数，将数据分布知识融入对抗优化过程。

Result: 实验表明，该方法在多种模型架构上显著提高了对抗样本的可迁移性，同时保持对扩散防御的鲁棒性。

Conclusion: 该方法在不依赖高成本扩散模型的情况下，实现了更高效的对抗攻击效果。

Abstract: Adversarial attacks have become a well-explored domain, frequently serving as
evaluation baselines for model robustness. Among these, black-box attacks based
on transferability have received significant attention due to their practical
applicability in real-world scenarios. Traditional black-box methods have
generally focused on improving the optimization framework (e.g., utilizing
momentum in MI-FGSM) to enhance transferability, rather than examining the
dependency on surrogate white-box model architectures. Recent state-of-the-art
approach DiffPGD has demonstrated enhanced transferability by employing
diffusion-based adversarial purification models for adaptive attacks. The
inductive bias of diffusion-based adversarial purification aligns naturally
with the adversarial attack process, where both involving noise addition,
reducing dependency on surrogate white-box model selection. However, the
denoising process of diffusion models incurs substantial computational costs
through chain rule derivation, manifested in excessive VRAM consumption and
extended runtime. This progression prompts us to question whether introducing
diffusion models is necessary. We hypothesize that a model sharing similar
inductive bias to diffusion-based adversarial purification, combined with an
appropriate loss function, could achieve comparable or superior transferability
while dramatically reducing computational overhead. In this paper, we propose a
novel loss function coupled with a unique surrogate model to validate our
hypothesis. Our approach leverages the score of the time-dependent classifier
from classifier-guided diffusion models, effectively incorporating natural data
distribution knowledge into the adversarial optimization process. Experimental
results demonstrate significantly improved transferability across diverse model
architectures while maintaining robustness against diffusion-based defenses.

</details>


### [343] [Dynamic Dual Buffer with Divide-and-Conquer Strategy for Online Continual Learning](https://arxiv.org/abs/2505.18101)
*Congren Dai,Huichi Zhou,Jiahao Huang,Zhenxuan Zhang,Fanwen Wang,Guang Yang,Fei Ye*

Key words: 在线持续学习, 灾难性遗忘, 内存框架, K均值, 分治法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种创新的内存框架，通过短时和长时记忆系统结合K均值样本选择方法和内存优化策略，有效解决了在线持续学习中的灾难性遗忘问题，并通过分治法优化内存更新过程。

Motivation: 在线持续学习(OCL)中，新数据以批到批的在线形式到达，灾难性遗忘风险会显著影响模型效果。本研究旨在通过创新的内存框架解决这一问题。

Method: 提出了结合短时和长时记忆系统的框架，长时记忆系统包含子内存缓冲区，使用K均值方法选择聚类原型，并通过最优传输机制优化样本保留。还提出了分治法(DAC)优化内存更新。

Result: 实验表明，该框架在标准和不平衡学习设置下均达到最先进性能。

Conclusion: 提出的内存框架通过动态保存语义丰富信息并优化计算，显著提升了OCL的效果。

Abstract: Online Continual Learning (OCL) presents a complex learning environment in
which new data arrives in a batch-to-batch online format, and the risk of
catastrophic forgetting can significantly impair model efficacy. In this study,
we address OCL by introducing an innovative memory framework that incorporates
a short-term memory system to retain dynamic information and a long-term memory
system to archive enduring knowledge. Specifically, the long-term memory system
comprises a collection of sub-memory buffers, each linked to a cluster
prototype and designed to retain data samples from distinct categories. We
propose a novel $K$-means-based sample selection method to identify cluster
prototypes for each encountered category. To safeguard essential and critical
samples, we introduce a novel memory optimisation strategy that selectively
retains samples in the appropriate sub-memory buffer by evaluating each cluster
prototype against incoming samples through an optimal transportation mechanism.
This approach specifically promotes each sub-memory buffer to retain data
samples that exhibit significant discrepancies from the corresponding cluster
prototype, thereby ensuring the preservation of semantically rich information.
In addition, we propose a novel Divide-and-Conquer (DAC) approach that
formulates the memory updating as an optimisation problem and divides it into
several subproblems. As a result, the proposed DAC approach can solve these
subproblems separately and thus can significantly reduce computations of the
proposed memory updating process. We conduct a series of experiments across
standard and imbalanced learning settings, and the empirical findings indicate
that the proposed memory framework achieves state-of-the-art performance in
both learning contexts.

</details>


### [344] [How Can I Publish My LLM Benchmark Without Giving the True Answers Away?](https://arxiv.org/abs/2505.18102)
*Takashi Ishida,Thanawat Lodkaew,Ikko Yamane*

Key words: 大语言模型, 基准测试, 数据污染, 贝叶斯准确率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种通过在基准测试中注入随机性以防止数据污染的方法，同时保留开放评估大语言模型（LLM）的能力。

Motivation: 现有的基准测试发布方式可能导致数据污染，而私有化测试又需要信任单一组织且无法避免重复查询导致的过拟合。

Method: 通过准备多个逻辑正确的答案并随机选择一个作为标准答案，降低基准的贝叶斯准确率，从而检测数据污染。

Result: 实验证明该方法能准确检测多种基准、模型和训练方法中的数据污染。

Conclusion: 该方法有效解决了基准测试中的数据污染问题，同时保持了评估的开放性。

Abstract: Publishing a large language model (LLM) benchmark on the Internet risks
contaminating future LLMs: the benchmark may be unintentionally (or
intentionally) used to train or select a model. A common mitigation is to keep
the benchmark private and let participants submit their models or predictions
to the organizers. However, this strategy will require trust in a single
organization and still permits test-set overfitting through repeated queries.
To overcome this issue, we propose a way to publish benchmarks without
completely disclosing the ground-truth answers to the questions, while still
maintaining the ability to openly evaluate LLMs. Our main idea is to inject
randomness to the answers by preparing several logically correct answers, and
only include one of them as the solution in the benchmark. This reduces the
best possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is
this helpful to keep us from disclosing the ground truth, but this approach
also offers a test for detecting data contamination. In principle, even fully
capable models should not surpass the Bayes accuracy. If a model surpasses this
ceiling despite this expectation, this is a strong signal of data
contamination. We present experimental evidence that our method can detect data
contamination accurately on a wide range of benchmarks, models, and training
methodologies.

</details>


### [345] [Beyond Discreteness: Finite-Sample Analysis of Straight-Through Estimator for Quantization](https://arxiv.org/abs/2505.18113)
*Halyun Jeong,Jack Xin,Penghang Yin*

Key words: 量化神经网络, 直通估计器, 样本复杂度, 标签噪声, 动态系统

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文对STE在有限样本下的性能进行了首次理论分析，揭示了样本量对训练量化神经网络的关键作用，并通过理论证明了STE的收敛性和标签噪声下的特殊动态行为。

Motivation: STE作为量化神经网络训练中最常用的启发式方法，其理论性质尚未充分探索，尤其是在有限样本情况下。本文旨在填补这一空白。

Method: 通过分析带二进制权重和激活的两层神经网络的量化感知训练，利用压缩感知和动态系统理论工具，推导样本复杂度界限。

Result: 证明了在数据维度的特定条件下STE能收敛到全局最优解，并发现标签噪声下STE梯度方法的特殊动态行为。

Conclusion: 样本量是STE成功的关键因素，且标签噪声下STE展现出有趣的动态特性。

Abstract: Training quantized neural networks requires addressing the non-differentiable
and discrete nature of the underlying optimization problem. To tackle this
challenge, the straight-through estimator (STE) has become the most widely
adopted heuristic, allowing backpropagation through discrete operations by
introducing surrogate gradients. However, its theoretical properties remain
largely unexplored, with few existing works simplifying the analysis by
assuming an infinite amount of training data. In contrast, this work presents
the first finite-sample analysis of STE in the context of neural network
quantization. Our theoretical results highlight the critical role of sample
size in the success of STE, a key insight absent from existing studies.
Specifically, by analyzing the quantization-aware training of a two-layer
neural network with binary weights and activations, we derive the sample
complexity bound in terms of the data dimensionality that guarantees the
convergence of STE-based optimization to the global minimum. Moreover, in the
presence of label noises, we uncover an intriguing recurrence property of
STE-gradient method, where the iterate repeatedly escape from and return to the
optimal binary weights. Our analysis leverages tools from compressed sensing
and dynamical systems theory.

</details>


### [346] [Bridging Supervised Learning and Reinforcement Learning in Math Reasoning](https://arxiv.org/abs/2505.18116)
*Huayu Chen,Kaiwen Zheng,Qinsheng Zhang,Ganqu Cui,Yin Cui,Haotian Ye,Tsung-Yi Lin,Ming-Yu Liu,Jun Zhu,Haoxiang Wang*

Key words: 强化学习, 监督学习, 负反馈, 语言模型, 数学推理

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了Negative-aware Fine-Tuning（NFT），一种监督学习方法，利用负面反馈改进LLM，效果媲美强化学习。

Motivation: 挑战自我改进仅为强化学习（RL）的概念，探索监督学习（SL）通过负面反馈实现自主改进的潜力。

Method: 通过隐式负面策略建模负面答案，与正面LLM参数共享，直接优化所有生成结果。

Result: 在数学推理任务中，NFT显著优于SL基线，匹配甚至超越GRPO和DAPO等RL算法。

Conclusion: NFT与GRPO在严格策略训练中表现等效，弥合了SL与RL在二元反馈学习系统中的差距。

Abstract: Reinforcement Learning (RL) has played a central role in the recent surge of
LLMs' math abilities by enabling self-improvement through binary verifier
signals. In contrast, Supervised Learning (SL) is rarely considered for such
verification-driven training, largely due to its heavy reliance on reference
answers and inability to reflect on mistakes. In this work, we challenge the
prevailing notion that self-improvement is exclusive to RL and propose
Negative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to
reflect on their failures and improve autonomously with no external teachers.
In online training, instead of throwing away self-generated negative answers,
NFT constructs an implicit negative policy to model them. This implicit policy
is parameterized with the same positive LLM we target to optimize on positive
data, enabling direct policy optimization on all LLMs' generations. We conduct
experiments on 7B and 32B models in math reasoning tasks. Results consistently
show that through the additional leverage of negative feedback, NFT
significantly improves over SL baselines like Rejection sampling Fine-Tuning,
matching or even surpassing leading RL algorithms like GRPO and DAPO.
Furthermore, we demonstrate that NFT and GRPO are actually equivalent in
strict-on-policy training, even though they originate from entirely different
theoretical foundations. Our experiments and theoretical findings bridge the
gap between SL and RL methods in binary-feedback learning systems.

</details>


### [347] [TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations](https://arxiv.org/abs/2505.18125)
*Alan Arazi,Eilam Shapira,Roi Reichart*

Key words: Tabular Learning, Foundation Models, Pretraining, Target-Aware Representations, Transfer Learning

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: TabSTAR提出了一种基于语义目标感知表示的基础表格模型，通过解冻预训练的文本编码器并输入目标标记，实现了在带有文本特征的分类任务上的最先进性能。

Motivation: 深度学习在表格学习任务中表现不佳，而传统梯度提升决策树（GBDTs）仍占主导地位。TabSTAR旨在通过结合语言模型能力和目标感知表示，提升表格任务的泛化能力。

Method: TabSTAR采用无数据集特定参数的架构，解冻预训练的文本编码器，并输入目标标记以生成任务特定的嵌入。

Result: TabSTAR在带有文本特征的分类任务基准测试中实现了最先进的性能，且预训练阶段表现出数据集数量的扩展规律。

Conclusion: TabSTAR为表格数据提供了有效的迁移学习框架，展示了进一步性能提升的潜力。

Abstract: While deep learning has achieved remarkable success across many domains, it
has historically underperformed on tabular learning tasks, which remain
dominated by gradient boosting decision trees (GBDTs). However, recent
advancements are paving the way for Tabular Foundation Models, which can
leverage real-world knowledge and generalize across diverse datasets,
particularly when the data contains free-text. Although incorporating language
model capabilities into tabular tasks has been explored, most existing methods
utilize static, target-agnostic textual representations, limiting their
effectiveness. We introduce TabSTAR: a Foundation Tabular Model with
Semantically Target-Aware Representations. TabSTAR is designed to enable
transfer learning on tabular data with textual features, with an architecture
free of dataset-specific parameters. It unfreezes a pretrained text encoder and
takes as input target tokens, which provide the model with the context needed
to learn task-specific embeddings. TabSTAR achieves state-of-the-art
performance for both medium- and large-sized datasets across known benchmarks
of classification tasks with text features, and its pretraining phase exhibits
scaling laws in the number of datasets, offering a pathway for further
performance improvements.

</details>


### [348] [Reward Model Overoptimisation in Iterated RLHF](https://arxiv.org/abs/2505.18126)
*Lorenz Wolf,Robert Kirk,Mirco Musolesi*

Key words: RLHF, 过优化, 迭代训练, 奖励模型, 策略初始化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究首次全面分析了迭代RLHF中的过优化问题，发现随着迭代次数增加，过优化现象减少，但性能提升逐渐下降，同时不同初始化策略对结果影响显著。

Motivation: 尽管迭代RLHF被广泛用于缓解奖励模型过优化问题，但其动态机制尚不明确，因此需要系统性研究以提升RLHF管线的稳定性和泛化性。

Method: 研究通过AlpacaFarm基准实验，分析了奖励模型训练数据传递方式、优化奖励函数选择和策略初始化等关键设计选择的影响。

Result: 实验表明，过优化现象随迭代减少，但性能增益递减；从基础策略重新初始化较稳健但灵活性受限，其他初始化策略难以从早期过优化中恢复。

Conclusion: 研究为优化RLHF管线提供了实用建议，强调奖励模型设计需平衡泛化性与优化灵活性。

Abstract: Reinforcement learning from human feedback (RLHF) is a widely used method for
aligning large language models with human preferences. However, RLHF often
suffers from reward model overoptimisation, in which models overfit to the
reward function, resulting in non-generalisable policies that exploit the
idiosyncrasies and peculiarities of the reward function. A common mitigation is
iterated RLHF, in which reward models are repeatedly retrained with updated
human feedback and policies are re-optimised. Despite its increasing adoption,
the dynamics of overoptimisation in this setting remain poorly understood. In
this work, we present the first comprehensive study of overoptimisation in
iterated RLHF. We systematically analyse key design choices - how reward model
training data is transferred across iterations, which reward function is used
for optimisation, and how policies are initialised. Using the controlled
AlpacaFarm benchmark, we observe that overoptimisation tends to decrease over
successive iterations, as reward models increasingly approximate ground-truth
preferences. However, performance gains diminish over time, and while
reinitialising from the base policy is robust, it limits optimisation
flexibility. Other initialisation strategies often fail to recover from early
overoptimisation. These findings offer actionable insights for building more
stable and generalisable RLHF pipelines.

</details>


### [349] [Leveraging KANs for Expedient Training of Multichannel MLPs via Preconditioning and Geometric Refinement](https://arxiv.org/abs/2505.18131)
*Jonas A. Actor,Graham Harper,Ben Southworth,Eric C. Cyr*

Key words: Kolmogorov-Arnold Networks, MLPs, 科学机器学习, 训练加速, 样条节点

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文探讨了Kolmogorov-Arnold网络（KANs）与多通道多层感知机（MLPs）的结构关系，提出了一种加速MLPs训练的方法，并通过实验验证了其效果。

Motivation: 研究KANs与MLPs的结构关系，以提升MLPs的训练效率和准确性，特别是在科学机器学习任务中。

Method: 利用KAN基的几何局部支持和预条件下降特性，定义了一种分层细化方案，加速多通道MLP的训练，并优化了样条节点的位置。

Result: 该方法显著加速了MLPs的训练，并在多个回归和科学机器学习基准测试中提高了准确性。

Conclusion: 通过结构等效性优化MLPs训练，验证了KANs在提升深度学习框架性能方面的潜力。

Abstract: Multilayer perceptrons (MLPs) are a workhorse machine learning architecture,
used in a variety of modern deep learning frameworks. However, recently
Kolmogorov-Arnold Networks (KANs) have become increasingly popular due to their
success on a range of problems, particularly for scientific machine learning
tasks. In this paper, we exploit the relationship between KANs and multichannel
MLPs to gain structural insight into how to train MLPs faster. We demonstrate
the KAN basis (1) provides geometric localized support, and (2) acts as a
preconditioned descent in the ReLU basis, overall resulting in expedited
training and improved accuracy. Our results show the equivalence between
free-knot spline KAN architectures, and a class of MLPs that are refined
geometrically along the channel dimension of each weight tensor. We exploit
this structural equivalence to define a hierarchical refinement scheme that
dramatically accelerates training of the multi-channel MLP architecture. We
show further accuracy improvements can be had by allowing the $1$D locations of
the spline knots to be trained simultaneously with the weights. These advances
are demonstrated on a range of benchmark examples for regression and scientific
machine learning.

</details>


### [350] [Generative Distribution Embeddings](https://arxiv.org/abs/2505.18150)
*Nic Fishman,Gokul Gowri,Peng Yin,Jonathan Gootenberg,Omar Abudayyeh*

Key words: 生成分布嵌入, 自编码器, Wasserstein距离, 计算生物学, 跨尺度推理

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 生成分布嵌入（GDE）框架通过将自编码器提升到分布空间，实现了多尺度问题的建模，并在多个计算生物学任务中表现出色。

Motivation: 现实问题通常需要跨尺度推理，现有模型仅处理单点数据，无法适应分布级任务，因此需要新方法。

Method: GDE框架通过编码器处理样本集，用生成器替代解码器以匹配输入分布，并结合分布不变性准则学习分布表示。

Result: GDE在Wasserstein空间中学习到预测性充分统计量，显著优于现有方法，并在多项生物数据集（如细胞、DNA序列）中验证有效性。

Conclusion: GDE为分布级建模提供了一种通用框架，尤其在计算生物学领域展示了强大潜力。

Abstract: Many real-world problems require reasoning across multiple scales, demanding
models which operate not on single data points, but on entire distributions. We
introduce generative distribution embeddings (GDE), a framework that lifts
autoencoders to the space of distributions. In GDEs, an encoder acts on sets of
samples, and the decoder is replaced by a generator which aims to match the
input distribution. This framework enables learning representations of
distributions by coupling conditional generative models with encoder networks
which satisfy a criterion we call distributional invariance. We show that GDEs
learn predictive sufficient statistics embedded in the Wasserstein space, such
that latent GDE distances approximately recover the $W_2$ distance, and latent
interpolation approximately recovers optimal transport trajectories for
Gaussian and Gaussian mixture distributions. We systematically benchmark GDEs
against existing approaches on synthetic datasets, demonstrating consistently
stronger performance. We then apply GDEs to six key problems in computational
biology: learning representations of cell populations from lineage-tracing data
(150K cells), predicting perturbation effects on single-cell transcriptomes (1M
cells), predicting perturbation effects on cellular phenotypes (20M single-cell
images), modeling tissue-specific DNA methylation patterns (253M sequences),
designing synthetic yeast promoters (34M sequences), and spatiotemporal
modeling of viral protein sequences (1M sequences).

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [351] [An Affective-Taxis Hypothesis for Alignment and Interpretability](https://arxiv.org/abs/2505.17024)
*Eli Sennesh,Maxwell Ramstead*

Key words: AI对齐、情感趋向、计算模型、进化发育、计算神经科学

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出了一个情感主义方法来解决AI对齐问题，将目标和价值重新定义为情感趋向，并通过计算模型验证了其与生物趋向行为的一致性，最终探讨了情感趋近在AI对齐中的作用。

Motivation: 现有AI对齐方法难以确保高级AI系统始终与人类目标和价值一致，本研究旨在通过情感主义视角重新定义对齐问题，提供新的解决方案。

Method: 提出了基于情感趋向（affective taxis）的计算模型，并借鉴进化发育和计算神经科学的最新成果，通过可处理模型生物验证模型的有效性。

Result: 模型与生物趋向行为表现出一定的相似性，验证了情感趋近在对齐问题中的潜在价值。

Conclusion: 情感趋近为AI对齐提供了新的理论框架和实用工具，未来需进一步研究其在复杂系统中的适用性。

Abstract: AI alignment is a field of research that aims to develop methods to ensure
that agents always behave in a manner aligned with (i.e. consistently with) the
goals and values of their human operators, no matter their level of capability.
This paper proposes an affectivist approach to the alignment problem,
re-framing the concepts of goals and values in terms of affective taxis, and
explaining the emergence of affective valence by appealing to recent work in
evolutionary-developmental and computational neuroscience. We review the state
of the art and, building on this work, we propose a computational model of
affect based on taxis navigation. We discuss evidence in a tractable model
organism that our model reflects aspects of biological taxis navigation. We
conclude with a discussion of the role of affective taxis in AI alignment.

</details>


### [352] [MEDMKG: Benchmarking Medical Knowledge Exploitation with Multimodal Knowledge Graph](https://arxiv.org/abs/2505.17214)
*Xiaochen Wang,Yuan Zhong,Lingwei Zhang,Lisong Dai,Ting Wang,Fenglong Ma*

Key words: 医学知识图谱, 多模态, 深度学习, UMLS, 邻居感知过滤

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出了一种名为MEDMKG的多模态医学知识图谱，结合视觉与文本医学信息，通过多阶段构建流程增强医学深度学习模型的性能。

Motivation: 医学深度学习模型需依赖领域知识，但目前多采用单模态知识图谱（如UMLS），多模态医学知识图谱的整合研究较少。本文旨在填补这一空白。

Method: 提出MEDMKG，通过规则工具与大语言模型融合MIMIC-CXR的多模态数据与UMLS的结构化知识，并引入新颖的邻居感知过滤算法（NaF）优化图谱质量。

Result: 在六大数据集上的实验表明，MEDMKG提升了医学任务的性能，并为多模态知识整合提供了坚实基础。

Conclusion: MEDMKG不仅提高了下游任务的性能，还为医学AI的多模态知识整合策略提供了新思路。

Abstract: Medical deep learning models depend heavily on domain-specific knowledge to
perform well on knowledge-intensive clinical tasks. Prior work has primarily
leveraged unimodal knowledge graphs, such as the Unified Medical Language
System (UMLS), to enhance model performance. However, integrating multimodal
medical knowledge graphs remains largely underexplored, mainly due to the lack
of resources linking imaging data with clinical concepts. To address this gap,
we propose MEDMKG, a Medical Multimodal Knowledge Graph that unifies visual and
textual medical information through a multi-stage construction pipeline. MEDMKG
fuses the rich multimodal data from MIMIC-CXR with the structured clinical
knowledge from UMLS, utilizing both rule-based tools and large language models
for accurate concept extraction and relationship modeling. To ensure graph
quality and compactness, we introduce Neighbor-aware Filtering (NaF), a novel
filtering algorithm tailored for multimodal knowledge graphs. We evaluate
MEDMKG across three tasks under two experimental settings, benchmarking
twenty-four baseline methods and four state-of-the-art vision-language
backbones on six datasets. Results show that MEDMKG not only improves
performance in downstream medical tasks but also offers a strong foundation for
developing adaptive and robust strategies for multimodal knowledge integration
in medical artificial intelligence.

</details>


### [353] [Effective Reinforcement Learning for Reasoning in Language Models](https://arxiv.org/abs/2505.17218)
*Lianghuan Huang,Shuo Li,Sagnik Anupam,Insup Lee,Osbert Bastani*

Key words: 强化学习, 语言模型, PPO, DASH, 梯度过滤

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出新算法DASH，通过预加载采样和梯度过滤优化了语言模型推理中的强化学习效率，训练时间减少83%，准确率不变。

Motivation: 为提高语言模型在数学和编程等领域的推理能力，需要针对语言模型推理特点优化强化学习算法，不同于传统针对机器人设计的RL方法。

Method: 分析RL算法在语言模型推理中的设计选择，提出DASH算法（预加载采样和梯度过滤），并与标准GRPO实现对比。

Result: DASH显著减少训练时间83%，且不影响准确率；PPO策略提升精度，去除KL散度能生成更简洁且精准的结果。

Conclusion: 研究为语言模型推理的RL算法设计提供了实用见解，DASH在效率和效果上展现优势。

Abstract: Reinforcement learning (RL) has emerged as a promising strategy for improving
the reasoning capabilities of language models (LMs) in domains such as
mathematics and coding. However, most modern RL algorithms were designed to
target robotics applications, which differ significantly from LM reasoning. We
analyze RL algorithm design decisions for LM reasoning, for both accuracy and
computational efficiency, focusing on relatively small models due to
computational constraints. Our findings are: (i) on-policy RL significantly
outperforms supervised fine-tuning (SFT), (ii) PPO-based off-policy updates
increase accuracy instead of reduce variance, and (iii) removing KL divergence
can lead to more concise generations and higher accuracy. Furthermore, we find
that a key bottleneck to computational efficiency is that the optimal batch
sizes for inference and backpropagation are different. We propose a novel
algorithm, DASH, that performs preemptive sampling (i.e., sample a large batch
and accumulate gradient updates in small increments), and gradient filtering
(i.e., drop samples with small advantage estimates). We show that DASH reduces
training time by 83% compared to a standard implementation of GRPO without
sacrificing accuracy. Our findings provide valuable insights on designing
effective RL algorithms for LM reasoning.

</details>


### [354] [Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models](https://arxiv.org/abs/2505.17225)
*Doohyuk Jang,Yoonjeon Kim,Chanjae Park,Hyun Ryu,Eunho Yang*

Key words: 大语言模型, 推理僵化, 污染模式, 数学推理, 逻辑谜题

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 大语言模型在复杂推理任务中表现出色，但存在"推理僵化"问题，即默认依赖熟悉模式而忽视明确指令。本文通过专家设计的数据集（AIME和MATH500变体）系统研究此现象，并识别三种污染模式。

Motivation: 探索大语言模型中未被充分研究的"推理僵化"现象，即模型忽视用户指令、依赖惯性推理的行为，尤其在数学和逻辑领域影响显著。

Method: 构建专家设计的数据集（含修改版AIME、MATH500及谜题），要求模型偏离熟悉策略，并系统分析其推理错误模式。

Result: 识别出三种污染模式：解释过载、输入不信任和部分指令关注，导致模型扭曲或忽略指令。

Conclusion: 公开诊断数据集以促进未来研究，呼吁改善语言模型在严格约束任务中的推理灵活性。

Abstract: Large language models have demonstrated remarkable proficiency in long and
complex reasoning tasks. However, they frequently exhibit a problematic
reliance on familiar reasoning patterns, a phenomenon we term \textit{reasoning
rigidity}. Despite explicit instructions from users, these models often
override clearly stated conditions and default to habitual reasoning
trajectories, leading to incorrect conclusions. This behavior presents
significant challenges, particularly in domains such as mathematics and logic
puzzle, where precise adherence to specified constraints is critical. To
systematically investigate reasoning rigidity, a behavior largely unexplored in
prior work, we introduce a expert-curated diagnostic set, \dataset{}. Our
dataset includes specially modified variants of existing mathematical
benchmarks, namely AIME and MATH500, as well as well-known puzzles deliberately
redesigned to require deviation from familiar reasoning strategies. Using this
dataset, we identify recurring contamination patterns that occur when models
default to ingrained reasoning. Specifically, we categorize this contamination
into three distinctive modes: (i) Interpretation Overload, (ii) Input Distrust,
and (iii) Partial Instruction Attention, each causing models to ignore or
distort provided instructions. We publicly release our diagnostic set to
facilitate future research on mitigating reasoning rigidity in language models.

</details>


### [355] [Where You Go is Who You Are: Behavioral Theory-Guided LLMs for Inverse Reinforcement Learning](https://arxiv.org/abs/2505.17249)
*Yuran Sun,Susu Xu,Chenguang Wang,Xilei Zhao*

Key words: 轨迹数据,社会人口属性,LLM,逆强化学习,计划行为理论

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出SILIC框架，结合大语言模型（LLM）和逆强化学习（IRL）来从移动轨迹数据推断社会人口属性，提升了预测准确性。

Motivation: 现有研究在从移动模式预测社会人口属性时，常忽略潜在的认知机制且预测精度低。该研究旨在解决这一问题。

Method: 采用SILIC框架，结合LLM引导的逆强化学习（IRL）和认知链推理（CCR），基于计划行为理论（TPB）建模。

Result: 在2017年Puget Sound区域调查中，该方法显著优于现有基准，展示了增强轨迹数据的潜力。

Conclusion: SILIC框架为交通规划等行为基础应用提供了更丰富的数据支持。

Abstract: Big trajectory data hold great promise for human mobility analysis, but their
utility is often constrained by the absence of critical traveler attributes,
particularly sociodemographic information. While prior studies have explored
predicting such attributes from mobility patterns, they often overlooked
underlying cognitive mechanisms and exhibited low predictive accuracy. This
study introduces SILIC, short for Sociodemographic Inference with LLM-guided
Inverse Reinforcement Learning (IRL) and Cognitive Chain Reasoning (CCR), a
theoretically grounded framework that leverages LLMs to infer sociodemographic
attributes from observed mobility patterns by capturing latent behavioral
intentions and reasoning through psychological constructs. Particularly, our
approach explicitly follows the Theory of Planned Behavior (TPB), a
foundational behavioral framework in transportation research, to model
individuals' latent cognitive processes underlying travel decision-making. The
LLMs further provide heuristic guidance to improve IRL reward function
initialization and update by addressing its ill-posedness and optimization
challenges arising from the vast and unstructured reward space. Evaluated in
the 2017 Puget Sound Regional Council Household Travel Survey, our method
substantially outperforms state-of-the-art baselines and shows great promise
for enriching big trajectory data to support more behaviorally grounded
applications in transportation planning and beyond.

</details>


### [356] [AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking](https://arxiv.org/abs/2505.17312)
*Xiangqi Wang,Yue Huang,Yanbo Wang,Xiaonan Luo,Kehan Guo,Yujun Zhou,Xiangliang Zhang*

Key words: LLM, 自适应配置, 强化学习, 推理优化, 知识密集型任务

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: LLMs需要自适应配置来优化推理任务表现，AdaReasoner通过强化学习自动调整推理配置，在多种LLM和任务中优于基线方法。

Motivation: 现有提示方法通常使用固定配置，无法针对特定任务达到最优性能，需要一种自适应方法。

Method: 采用强化学习框架，结合因子化动作空间和目标探索策略，通过预训练奖励模型优化配置策略。

Result: 在六种LLM和多种推理任务中，AdaReasoner表现优于基线，具有快速收敛和分布外鲁棒性。

Conclusion: AdaReasoner为LLM提供了自适应推理配置的有效解决方案，适用于知识密集型任务。

Abstract: LLMs often need effective configurations, like temperature and reasoning
steps, to handle tasks requiring sophisticated reasoning and problem-solving,
ranging from joke generation to mathematical reasoning. Existing prompting
approaches usually adopt general-purpose, fixed configurations that work 'well
enough' across tasks but seldom achieve task-specific optimality. To address
this gap, we introduce AdaReasoner, an LLM-agnostic plugin designed for any LLM
to automate adaptive reasoning configurations for tasks requiring different
types of thinking. AdaReasoner is trained using a reinforcement learning (RL)
framework, combining a factorized action space with a targeted exploration
strategy, along with a pretrained reward model to optimize the policy model for
reasoning configurations with only a few-shot guide. AdaReasoner is backed by
theoretical guarantees and experiments of fast convergence and a sublinear
policy gap. Across six different LLMs and a variety of reasoning tasks, it
consistently outperforms standard baselines, preserves out-of-distribution
robustness, and yield gains on knowledge-intensive tasks through tailored
prompts.

</details>


### [357] [Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning](https://arxiv.org/abs/2505.17315)
*Wang Yang,Zirui Liu,Hongye Jin,Qingyu Yin,Vipin Chaudhary,Xiaotian Han*

Key words: 语言模型，长上下文，推理能力，监督微调，基准测试

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 研究发现，增强语言模型的长上下文能力能显著提升其推理性能，甚至在短输入任务中也有通用性优势。

Motivation: 当前语言模型在推理能力上存在局限性，作者假设这与长上下文能力不足有关，并通过实验验证这一假设。

Method: 比较具有相同架构和微调数据但长上下文能力不同的模型，观察其在监督微调后的推理表现。

Result: 长上下文能力更强的模型在推理基准测试中表现更优，且这种优势在短输入任务中依然存在。

Conclusion: 长上下文建模不仅是处理长输入的基础，也对推理能力至关重要，应作为未来语言模型设计的核心目标。

Abstract: Recent language models exhibit strong reasoning capabilities, yet the
influence of long-context capacity on reasoning remains underexplored. In this
work, we hypothesize that current limitations in reasoning stem, in part, from
insufficient long-context capacity, motivated by empirical observations such as
(1) higher context window length often leads to stronger reasoning performance,
and (2) failed reasoning cases resemble failed long-context cases. To test this
hypothesis, we examine whether enhancing a model's long-context ability before
Supervised Fine-Tuning (SFT) leads to improved reasoning performance.
Specifically, we compared models with identical architectures and fine-tuning
data but varying levels of long-context capacity. Our results reveal a
consistent trend: models with stronger long-context capacity achieve
significantly higher accuracy on reasoning benchmarks after SFT. Notably, these
gains persist even on tasks with short input lengths, indicating that
long-context training offers generalizable benefits for reasoning performance.
These findings suggest that long-context modeling is not just essential for
processing lengthy inputs, but also serves as a critical foundation for
reasoning. We advocate for treating long-context capacity as a first-class
objective in the design of future language models.

</details>


### [358] [Partner Modelling Emerges in Recurrent Agents (But Only When It Matters)](https://arxiv.org/abs/2505.17323)
*Ruaridh Mon-Williams,Max Taylor-Davies,Elizabeth Mieczkowski,Natalia Velez,Neil R. Bramley,Yanwei Wang,Thomas L. Griffiths,Christopher G. Lucas*

Key words: 协作学习, 伙伴建模, 模型无关强化学习, Overcooked-AI, 社会压力

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文研究了AI系统是否能在开放合作中自发形成对合作伙伴的建模能力，研究表明即使在无额外架构或目标的情况下，AI代理仍能通过任务分配影响伙伴行为，从而发展出结构化内部表示。

Motivation: 探索人类协作的灵活性是否源于专门建模机制，或者是否能通过开放合作压力自发涌现，以便构建具备类似能力的AI系统。

Method: 使用模型无关的RNN代理在《Overcooked-AI》环境中与多样化伙伴协作，分析隐藏状态和行为数据，探究自发建模机制的条件。

Result: 代理虽无额外设计，却能形成对伙伴任务能力的结构化内部表示，快速适应新伙伴；此能力仅在能通过任务分配影响伙伴行为时出现。

Conclusion: 模型无关代理可在特定环境压力下自发形成伙伴建模能力，无需显式机制，但需满足行为相互影响的社交压力条件。

Abstract: Humans are remarkably adept at collaboration, able to infer the strengths and
weaknesses of new partners in order to work successfully towards shared goals.
To build AI systems with this capability, we must first understand its building
blocks: does such flexibility require explicit, dedicated mechanisms for
modelling others -- or can it emerge spontaneously from the pressures of
open-ended cooperative interaction? To investigate this question, we train
simple model-free RNN agents to collaborate with a population of diverse
partners. Using the `Overcooked-AI' environment, we collect data from thousands
of collaborative teams, and analyse agents' internal hidden states. Despite a
lack of additional architectural features, inductive biases, or auxiliary
objectives, the agents nevertheless develop structured internal representations
of their partners' task abilities, enabling rapid adaptation and generalisation
to novel collaborators. We investigated these internal models through probing
techniques, and large-scale behavioural analysis. Notably, we find that
structured partner modelling emerges when agents can influence partner
behaviour by controlling task allocation. Our results show that partner
modelling can arise spontaneously in model-free agents -- but only under
environmental conditions that impose the right kind of social pressure.

</details>


### [359] [DEL-ToM: Inference-Time Scaling for Theory-of-Mind Reasoning via Dynamic Epistemic Logic](https://arxiv.org/abs/2505.17348)
*Yuheng Wu,Jianwen Xie,Denghui Zhang,Zhaozhuo Xu*

Key words: Theory-of-Mind, 小语言模型, 动态认知逻辑, 信念更新, 推理优化

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: DEL-ToM通过分解Theory-of-Mind任务为基于动态认知逻辑的信念更新序列，并通过训练验证器优化推理，提升了小语言模型的ToM能力。

Motivation: 小语言模型在ToM任务中因规模限制难以进行深度社交推理，需一种无需架构改动即可提升性能的方法。

Method: 提出DEL-ToM框架，将ToM任务分解为动态认知逻辑的信念更新序列，并训练Process Belief Model验证器自动评分优化推理。

Result: 实验表明，DEL-ToM在不同规模和基准测试中均显著提升小语言模型的ToM表现，且无需重新训练。

Conclusion: 通过验证式信念监督可有效增强小语言模型的ToM能力，验证了推理时计算分配的重要性。

Abstract: Theory-of-Mind (ToM) tasks pose a unique challenge for small language models
(SLMs) with limited scale, which often lack the capacity to perform deep social
reasoning. In this work, we propose DEL-ToM, a framework that improves ToM
reasoning through inference-time scaling rather than architectural changes. Our
approach decomposes ToM tasks into a sequence of belief updates grounded in
Dynamic Epistemic Logic (DEL), enabling structured and transparent reasoning.
We train a verifier, called the Process Belief Model (PBM), to score each
belief update step using labels generated automatically via a DEL simulator.
During inference, candidate belief traces generated by a language model are
evaluated by the PBM, and the highest-scoring trace is selected. This allows
SLMs to emulate more deliberate reasoning by allocating additional compute at
test time. Experiments across multiple model scales and benchmarks show that
DEL-ToM consistently improves performance, demonstrating that verifiable belief
supervision can significantly enhance ToM abilities of SLMs without retraining.

</details>


### [360] [Misaligning Reasoning with Answers -- A Framework for Assessing LLM CoT Robustness](https://arxiv.org/abs/2505.17406)
*Enyi Jiang,Changming Xu,Nischay Singh,Gagandeep Singh*

Key words: LLMs, 推理一致性, MATCHA框架, 输入扰动, 模型可信度

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了MATCHA评估框架，研究LLMs在输入扰动下的推理一致性，发现多步和常识任务更脆弱，并展示了成功案例向黑盒模型的迁移。

Motivation: LLMs的决策过程不透明，需要解释技术（如Chain-of-Thought）来增强模型可信度，特别是在教育和医疗等领域。

Method: 设计了MATCHA评估框架，分析LLMs在输入扰动下的推理表现，并使用LLM评委评估不同模型的推理鲁棒性。

Result: 发现LLMs在多步和常识任务中对输入扰动更敏感，逻辑任务表现更好；成功案例可迁移至黑盒模型。

Conclusion: MATCHA框架有助于理解LLM推理机制，推动未来模型向更鲁棒、推理驱动的架构发展，确保答案与推理一致性。

Abstract: LLMs' decision-making process is opaque, prompting the need for explanation
techniques like Chain-of-Thought. To investigate the relationship between
answer and reasoning, we design a novel evaluation framework, MATCHA. In
domains like education and healthcare, reasoning is key for model
trustworthiness. MATCHA reveals that LLMs under input perturbations can give
inconsistent or nonsensical reasoning. Additionally, we use LLM judges to
assess reasoning robustness across models. Our results show that LLMs exhibit
greater vulnerability to input perturbations for multi-step and commonsense
tasks than compared to logical tasks. Also, we show non-trivial transfer rates
of our successful examples to black-box models. Our evaluation framework helps
to better understand LLM reasoning mechanisms and guides future models toward
more robust and reasoning-driven architectures, enforcing answer-reasoning
consistency.

</details>


### [361] [MemeReaCon: Probing Contextual Meme Understanding in Large Vision-Language Models](https://arxiv.org/abs/2505.17433)
*Zhengyi Zhao,Shubo Zhang,Yuxi Zhang,Yanxi Zhao,Yifan Zhang,Zezhong Wang,Huimin Wang,Yutian Zhao,Bin Liang,Yefeng Zheng,Binyang Li,Kam-Fai Wong,Xian Wu*

Key words: 表情包理解、上下文感知、视觉语言模型、多模态分析、基准测试

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: MemeReaCon是一个新基准，用于评估大型视觉语言模型在理解上下文相关表情包意图时的表现，揭示了当前模型的局限。

Motivation: 当前的表情包分析主要关注孤立内容，忽略了上下文对意图的影响，导致模型难以理解语境相关的表情包含义。

Method: 通过从五个Reddit社区收集表情包（包括图像、帖文和评论），标注其协同作用、意图、结构和社区反馈，构建MemeReaCon基准。

Result: 测试显示主流LVLMs在理解上下文时存在明显缺陷：或忽略关键信息，或过度关注视觉细节而忽略交流目的。

Conclusion: MemeReaCon既能诊断现有模型的不足，也为开发更高级的上下文感知模型提供了挑战性基准。

Abstract: Memes have emerged as a popular form of multimodal online communication,
where their interpretation heavily depends on the specific context in which
they appear. Current approaches predominantly focus on isolated meme analysis,
either for harmful content detection or standalone interpretation, overlooking
a fundamental challenge: the same meme can express different intents depending
on its conversational context. This oversight creates an evaluation gap:
although humans intuitively recognize how context shapes meme interpretation,
Large Vision Language Models (LVLMs) can hardly understand context-dependent
meme intent. To address this critical limitation, we introduce MemeReaCon, a
novel benchmark specifically designed to evaluate how LVLMs understand memes in
their original context. We collected memes from five different Reddit
communities, keeping each meme's image, the post text, and user comments
together. We carefully labeled how the text and meme work together, what the
poster intended, how the meme is structured, and how the community responded.
Our tests with leading LVLMs show a clear weakness: models either fail to
interpret critical information in the contexts, or overly focus on visual
details while overlooking communicative purpose. MemeReaCon thus serves both as
a diagnostic tool exposing current limitations and as a challenging benchmark
to drive development toward more sophisticated LVLMs of the context-aware
understanding.

</details>


### [362] [Scaling Up Biomedical Vision-Language Models: Fine-Tuning, Instruction Tuning, and Multi-Modal Learning](https://arxiv.org/abs/2505.17436)
*Cheng Peng,Kai Zhang,Mengxian Lyu,Hongfang Liu,Lichao Sun,Yonghui Wu*

Key words: 生物医学视觉语言模型, 多模态任务, 零样本学习, 指令调整, 微调

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文通过扩展、微调和指令调整开发了BiomedGPT-Large和BiomedGPT-XLarge两种生物医学视觉语言模型，提升了处理多模态生物医学任务的能力，并验证了零样本学习性能。

Motivation: 推动生物医学视觉语言模型的扩展能力，提升长文本处理和多模态任务适应性，探索零样本学习性能。

Method: 基于编码器-解码器Transformer架构，开发两种模型，在6大类23个基准数据集上微调，涵盖图像分类、文本理解、问答等任务，并与现有模型对比。指令调整后评估零样本学习能力。

Result: 提出的BiomedGPT-Large和XLarge模型在多模态任务中表现优异，超越基准模型，并通过指令调整实现高对齐准确率。

Conclusion: 扩展和指令调整显著提升生物医学视觉语言模型性能，为零样本学习提供了有效解决方案。

Abstract: To advance biomedical vison-language model capabilities through scaling up,
fine-tuning, and instruction tuning, develop vision-language models with
improved performance in handling long text, explore strategies to efficiently
adopt vision language models for diverse multi-modal biomedical tasks, and
examine the zero-shot learning performance.
  We developed two biomedical vision language models, BiomedGPT-Large and
BiomedGPT-XLarge, based on an encoder-decoder-based transformer architecture.
We fine-tuned the two models on 23 benchmark datasets from 6 multi-modal
biomedical tasks including one image-only task (image classification), three
language-only tasks (text understanding, text summarization and question
answering), and two vision-language tasks (visual question answering and image
captioning). We compared the developed scaled models with our previous
BiomedGPT-Base model and existing prestigious models reported in the
literature. We instruction-tuned the two models using a large-scale multi-modal
biomedical instruction-tuning dataset and assessed the zero-shot learning
performance and alignment accuracy.

</details>


### [363] [From Reasoning to Generalization: Knowledge-Augmented LLMs for ARC Benchmark](https://arxiv.org/abs/2505.17482)
*Chao Lei,Nir Lipovetzky,Krista A. Ehinger,Yanchuan Chang*

Key words: LLM, 抽象推理, ARC基准, 知识增强, 程序合成

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了KAAR方法，通过分层知识增强提升LLM在抽象推理任务（ARC）上的性能，相比基线方法RSPC实现了显著改进。

Motivation: 探索LLM在抽象推理和泛化能力上的表现，解决现有方法在ARC基准上的不足。

Method: 将ARC任务建模为程序合成问题，提出RSPC和KAAR两种方法，后者通过分层知识增强逐步提升推理能力。

Result: KAAR在所有测试LLM中表现最优，绝对增益约5%，相对改进高达64.52%。

Conclusion: ARC仍是LLM的挑战性基准，KAAR通过知识增强显著提升性能，但未来仍有改进空间。

Abstract: Recent reasoning-oriented LLMs have demonstrated strong performance on
challenging tasks such as mathematics and science examinations. However, core
cognitive faculties of human intelligence, such as abstract reasoning and
generalization, remain underexplored. To address this, we evaluate recent
reasoning-oriented LLMs on the Abstraction and Reasoning Corpus (ARC)
benchmark, which explicitly demands both faculties. We formulate ARC as a
program synthesis task and propose nine candidate solvers. Experimental results
show that repeated-sampling planning-aided code generation (RSPC) achieves the
highest test accuracy and demonstrates consistent generalization across most
LLMs. To further improve performance, we introduce an ARC solver, Knowledge
Augmentation for Abstract Reasoning (KAAR), which encodes core knowledge priors
within an ontology that classifies priors into three hierarchical levels based
on their dependencies. KAAR progressively expands LLM reasoning capacity by
gradually augmenting priors at each level, and invokes RSPC to generate
candidate solutions after each augmentation stage. This stage-wise reasoning
reduces interference from irrelevant priors and improves LLM performance.
Empirical results show that KAAR maintains strong generalization and
consistently outperforms non-augmented RSPC across all evaluated LLMs,
achieving around 5% absolute gains and up to 64.52% relative improvement.
Despite these achievements, ARC remains a challenging benchmark for
reasoning-oriented LLMs, highlighting future avenues of progress in LLMs.

</details>


### [364] [PD$^3$: A Project Duplication Detection Framework via Adapted Multi-Agent Debate](https://arxiv.org/abs/2505.17492)
*Dezheng Bao,Yueci Yang,Xin Chen,Zhengxuan Jiang,Zeguo Fei,Daoze Zhang,Xuanwen Huang,Junru Chen,Chutian Yu,Xiang Yuan,Yang Yang*

Key words: 项目重复检测,多智能体辩论,电力项目,定性定量分析

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出PD$^3$框架，通过多智能体辩论改进项目重复检测，结合定性与定量分析，提升实用性和准确性。

Motivation: 现有方法缺乏对项目内容和评审标准的深入理解，无法提供有价值的专家反馈。

Method: 采用多智能体辩论框架，结合定性与定量分析，检测项目重复性。

Result: 在800多个真实电力项目数据上表现优于现有方法7.43%和8.00%，并节省573万美元初始检测成本。

Conclusion: PD$^3$框架显著提升项目重复检测效率和实用性，适用于多领域。

Abstract: Project duplication detection is critical for project quality assessment, as
it improves resource utilization efficiency by preventing investing in newly
proposed project that have already been studied. It requires the ability to
understand high-level semantics and generate constructive and valuable
feedback. Existing detection methods rely on basic word- or sentence-level
comparison or solely apply large language models, lacking valuable insights for
experts and in-depth comprehension of project content and review criteria. To
tackle this issue, we propose PD$^3$, a Project Duplication Detection framework
via adapted multi-agent Debate. Inspired by real-world expert debates, it
employs a fair competition format to guide multi-agent debate to retrieve
relevant projects. For feedback, it incorporates both qualitative and
quantitative analysis to improve its practicality. Over 800 real-world power
project data spanning more than 20 specialized fields are used to evaluate the
framework, demonstrating that our method outperforms existing approaches by
7.43% and 8.00% in two downstream tasks. Furthermore, we establish an online
platform, Review Dingdang, to assist power experts, saving 5.73 million USD in
initial detection on more than 100 newly proposed projects.

</details>


### [365] [Probe by Gaming: A Game-based Benchmark for Assessing Conceptual Knowledge in LLMs](https://arxiv.org/abs/2505.17512)
*Shuhang Xu,Weijian Deng,Yixuan Zhou,Fangwei Zhong*

Key words: 大语言模型,概念理解,多智能体交互,基准测试

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 这篇论文介绍了CK-Arena，一个基于Undercover游戏的多智能体交互游戏，旨在评估大语言模型（LLMs）在交互环境中对概念的理解能力。现有的基准测试主要关注事实回忆和孤立任务，而CK-Arena则通过部分信息推理概念的边界，更全面地评估LLMs的概念推理能力。

Motivation: 现有评估基准未能充分测试LLMs对概念边界的理解能力，尤其是动态交互环境中的概念推理能力。

Method: 提出CK-Arena，基于多智能体互动游戏设计，要求模型根据部分信息描述、区分和推理概念边界。

Result: 实验结果表明，LLMs的概念理解能力在不同类别中存在显著差异，且不完全与模型参数规模或一般能力相关。

Conclusion: CK-Arena为动态环境中的概念推理评估提供了一种可扩展且现实的基准。

Abstract: Concepts represent generalized abstractions that enable humans to categorize
and reason efficiently, yet it is unclear to what extent Large Language Models
(LLMs) comprehend these semantic relationships. Existing benchmarks typically
focus on factual recall and isolated tasks, failing to evaluate the ability of
LLMs to understand conceptual boundaries. To address this gap, we introduce
CK-Arena, a multi-agent interaction game built upon the Undercover game,
designed to evaluate the capacity of LLMs to reason with concepts in
interactive settings. CK-Arena challenges models to describe, differentiate,
and infer conceptual boundaries based on partial information, encouraging
models to explore commonalities and distinctions between closely related
concepts. By simulating real-world interaction, CK-Arena provides a scalable
and realistic benchmark for assessing conceptual reasoning in dynamic
environments. Experimental results show that LLMs' understanding of conceptual
knowledge varies significantly across different categories and is not strictly
aligned with parameter size or general model capabilities. The data and code
are available at the project homepage: https://ck-arena.site.

</details>


### [366] [Optimizing Retrieval-Augmented Generation for Electrical Engineering: A Case Study on ABB Circuit Breakers](https://arxiv.org/abs/2505.17520)
*Salahuddin Alawadhi,Noorhan Abbas*

Key words: 检索增强生成,大型语言模型,ABB断路器,电气工程,分块策略

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该研究探讨了在ABB断路器领域整合检索增强生成（RAG）与大型语言模型的潜力，开发了特定领域数据集并评估了三种RAG管道，发现现有配置在工程环境下存在局限。

Motivation: 研究旨在解决工程文档中数据检索和上下文对齐的挑战，特别是在需要高准确性和可靠性的电气工程领域。

Method: 通过定制数据集、先进嵌入模型和优化的分块策略，评估了三种RAG管道（OpenAI GPT4o、Cohere和Anthropic Claude）以及高级分块方法（如段落制和标题感知分割）的效果。

Result: 结果显示某些配置实现了高精度和相关度，但在确保事实准确性和完整性方面仍有不足。

Conclusion: 该研究强调了在RAG系统中进行迭代改进以满足电气工程任务严格需求的必要性。

Abstract: Integrating Retrieval Augmented Generation (RAG) with Large Language Models
(LLMs) has shown the potential to provide precise, contextually relevant
responses in knowledge intensive domains. This study investigates the
ap-plication of RAG for ABB circuit breakers, focusing on accuracy,
reliability, and contextual relevance in high-stakes engineering environments.
By leveraging tailored datasets, advanced embedding models, and optimized
chunking strategies, the research addresses challenges in data retrieval and
contextual alignment unique to engineering documentation. Key contributions
include the development of a domain-specific dataset for ABB circuit breakers
and the evaluation of three RAG pipelines: OpenAI GPT4o, Cohere, and Anthropic
Claude. Advanced chunking methods, such as paragraph-based and title-aware
segmentation, are assessed for their impact on retrieval accuracy and response
generation. Results demonstrate that while certain configurations achieve high
precision and relevancy, limitations persist in ensuring factual faithfulness
and completeness, critical in engineering contexts. This work underscores the
need for iterative improvements in RAG systems to meet the stringent demands of
electrical engineering tasks, including design, troubleshooting, and
operational decision-making. The findings in this paper help advance research
of AI in highly technical domains such as electrical engineering.

</details>


### [367] [Transparency and Proportionality in Post-Processing Algorithmic Bias Correction](https://arxiv.org/abs/2505.17525)
*Juliett Suárez Ferreira,Marija Slavkovik,Jorge Casillas*

Key words: 算法公平性、去偏、后处理、分类任务

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种后处理阶段的去偏方法，通过量化策略中的差异来评估其比例性，以提升分类任务中的公平性。

Motivation: 算法决策系统常因偏见导致不公平，现有的去偏方法可能引入新的不公平或加剧不平等，因此需开发更透明和均衡的策略。

Method: 提出一组量化后处理阶段策略差异的指标，用于评估去偏策略的比例性、透明性，并分析其他可能的去偏方法。

Result: 通过实例展示了该指标如何补充传统公平性指标，为不同群体提供更公平的结果。

Conclusion: 提出的指标能帮助实践者更全面地评估和优化去偏策略，确保公平性。

Abstract: Algorithmic decision-making systems sometimes produce errors or skewed
predictions toward a particular group, leading to unfair results. Debiasing
practices, applied at different stages of the development of such systems,
occasionally introduce new forms of unfairness or exacerbate existing
inequalities. We focus on post-processing techniques that modify algorithmic
predictions to achieve fairness in classification tasks, examining the
unintended consequences of these interventions. To address this challenge, we
develop a set of measures that quantify the disparity in the flips applied to
the solution in the post-processing stage. The proposed measures will help
practitioners: (1) assess the proportionality of the debiasing strategy used,
(2) have transparency to explain the effects of the strategy in each group, and
(3) based on those results, analyze the possibility of the use of some other
approaches for bias mitigation or to solve the problem. We introduce a
methodology for applying the proposed metrics during the post-processing stage
and illustrate its practical application through an example. This example
demonstrates how analyzing the proportionality of the debiasing strategy
complements traditional fairness metrics, providing a deeper perspective to
ensure fairer outcomes across all groups.

</details>


### [368] [USTBench: Benchmarking and Dissecting Spatiotemporal Reasoning of LLMs as Urban Agents](https://arxiv.org/abs/2505.17572)
*Siqi Lai,Yansong Ning,Zirui Yuan,Zhixi Chen,Hao Liu*

Key words: 大语言模型、城市代理、时空推理、基准评估、动态规划

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: USTBench是首个评估大语言模型（LLMs）在时空推理能力的基准，涵盖理解、预测、规划和反馈四个维度，并揭示LLMs在动态城市场景中的局限性。

Motivation: 现有研究主要关注城市LLM代理的结果级指标，缺乏对其推理过程的深入理解，因此需要系统地评估LLMs在时空推理中的能力。

Method: 提出了USTBench基准，包含62,466个结构化QA对和标准化任务评估，覆盖五类城市决策和四项时空预测任务。

Result: 评估13个主流LLM显示，虽在多数城市任务中表现潜力，但长期规划和动态适应仍是短板，且通用推理模型未显著优于非推理模型。

Conclusion: USTBench为构建更高效的城市智能代理提供了基础，并强调了领域专业化适配的必要性。

Abstract: Large language models (LLMs) have shown emerging potential in spatiotemporal
reasoning, making them promising candidates for building urban agents that
support diverse urban downstream applications. Despite these benefits, existing
studies primarily focus on evaluating urban LLM agent on outcome-level metrics
(e.g., prediction accuracy, traffic efficiency), offering limited insight into
their underlying reasoning processes. As a result, the strengths and
limitations of urban LLM agents in spatiotemporal reasoning remain poorly
understood. To this end, we introduce USTBench, the first benchmark to evaluate
LLMs' spatiotemporal reasoning abilities as urban agents across four decomposed
dimensions: spatiotemporal understanding, forecasting, planning, and reflection
with feedback. Specifically, USTBench supports five diverse urban
decision-making and four spatiotemporal prediction tasks, all running within
our constructed interactive city environment UAgentEnv. The benchmark includes
62,466 structured QA pairs for process-level evaluation and standardized
end-to-end task assessments, enabling fine-grained diagnostics and broad
task-level comparison across diverse urban scenarios. Through extensive
evaluation of thirteen leading LLMs, we reveal that although LLMs show
promising potential across various urban downstream tasks, they still struggle
in long-horizon planning and reflective adaptation in dynamic urban contexts.
Notably, recent advanced reasoning models (e.g., DeepSeek-R1) trained on
general logic or mathematical problems do not consistently outperform
non-reasoning LLMs. This discrepancy highlights the need for domain-specialized
adaptation methods to enhance urban spatiotemporal reasoning. Overall, USTBench
provides a foundation to build more adaptive and effective LLM-based urban
agents and broad smart city applications.

</details>


### [369] [Controlled Agentic Planning & Reasoning for Mechanism Synthesis](https://arxiv.org/abs/2505.17607)
*João Pedro Gandarela,Thiago Rios,Stefan Menzel,André Freitas*

Key words: 大语言模型, 机制合成, 符号回归, 平面机制, MSynth

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种基于双代理大语言模型（LLM）的机制合成推理方法，能够在语言和符号层面生成几何与动态结果，并通过MSynth基准测试验证了其有效性。

Motivation: 旨在通过结合语言与符号推理，提升机制合成的自动化水平，解决从自然语言描述到具体机制设计的转换问题。

Method: 采用双代理LLM模型，通过支持方程引用抽象属性、生成参数化仿真代码、利用符号回归和距离函数生成反馈锚点，形成可操作的闭环优化流程。

Result: 在平面机制合成场景中表现出高效性与收敛性；MSynth基准测试显示模型组件的有效性，并证明符号回归提示仅在大型架构中才能解锁机制性洞察。

Conclusion: 双代理LLM方法与符号回归的结合为机制合成提供了新途径，但需依赖足够规模的模型架构才能发挥潜力。

Abstract: This work presents a dual-agent Large Language Model (LLM)-based reasoning
method for mechanism synthesis, capable of reasoning at both linguistic and
symbolic levels to generate geometrical and dynamic outcomes. The model
consists of a composition of well-defined functions that, starting from a
natural language specification, references abstract properties through
supporting equations, generates and parametrizes simulation code, and elicits
feedback anchor points using symbolic regression and distance functions. This
process closes an actionable refinement loop at the linguistic and symbolic
layers. The approach is shown to be both effective and convergent in the
context of planar mechanisms. Additionally, we introduce MSynth, a novel
benchmark for planar mechanism synthesis, and perform a comprehensive analysis
of the impact of the model components. We further demonstrate that symbolic
regression prompts unlock mechanistic insights only when applied to
sufficiently large architectures.

</details>


### [370] [Decoupled Visual Interpretation and Linguistic Reasoning for Math Problem Solving](https://arxiv.org/abs/2505.17609)
*Zixian Guo,Ming Liu,Zhilong Ji,Jinfeng Bai,Lei Zhang,Wangmeng Zuo*

Key words: 视觉-语言模型, 大型语言模型, 解耦推理, 多模态理解, 联合调优

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种解耦的视觉-语言推理框架，利用现有视觉解释专家和文本推理LLMs协作，避免从头开发端到端模型，效果优于现有模型。

Motivation: 当前大型视觉-语言模型（LVLMs）在处理复杂视觉-语言推理任务时性能不足，论文旨在通过解耦框架优化协作，提升推理能力。

Method: 使用专用视觉-语言模型将图像内容转化为文本描述，再由LLMs根据描述和问题进行推理，采用结果奖励联合调优策略优化协作。

Result: 解耦框架在视觉-语言基准测试中表现优于现有LVLMs，尤其在视觉密集的几何数学问题上提升显著。

Conclusion: 解耦框架为多模态模型开发提供了一种低成本、灵活的解决方案，并便于后续LLMs升级。

Abstract: Current large vision-language models (LVLMs) typically employ a connector
module to link visual features with text embeddings of large language models
(LLMs) and use end-to-end training to achieve multi-modal understanding in a
unified process. Well alignment needs high-quality pre-training data and a
carefully designed training process. Current LVLMs face challenges when
addressing complex vision-language reasoning tasks, with their reasoning
capabilities notably lagging behind those of LLMs. This paper proposes a
paradigm shift: instead of training end-to-end vision-language reasoning
models, we advocate for developing a decoupled reasoning framework based on
existing visual interpretation specialists and text-based reasoning LLMs. Our
approach leverages (1) a dedicated vision-language model to transform the
visual content of images into textual descriptions and (2) an LLM to perform
reasoning according to the visual-derived text and the original question. This
method presents a cost-efficient solution for multi-modal model development by
optimizing existing models to work collaboratively, avoiding end-to-end
development of vision-language models from scratch. By transforming images into
language model-compatible text representations, it facilitates future low-cost
and flexible upgrades to upcoming powerful LLMs. We introduce an
outcome-rewarded joint-tuning strategy to optimize the cooperation between the
visual interpretation and linguistic reasoning model. Evaluation results on
vision-language benchmarks demonstrate that the decoupled reasoning framework
outperforms recent LVLMs. Our approach yields particularly significant
performance gains on visually intensive geometric mathematics problems. The
code is available: https://github.com/guozix/DVLR.

</details>


### [371] [MMMG: a Comprehensive and Reliable Evaluation Suite for Multitask Multimodal Generation](https://arxiv.org/abs/2505.17613)
*Jihan Yao,Yushi Hu,Yujie Yi,Bin Han,Shangbin Feng,Guang Yang,Bingbing Wen,Ranjay Krishna,Lucy Lu Wang,Yulia Tsvetkov,Noah A. Smith,Banghua Zhu*

Key words: 多模态生成、自动化评估、MMMG、多模态推理、音频生成

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出MMMG基准，用于评估多模态生成任务，包含49个任务和937条指令，自动化评估与人类评估高度一致（94.3%）。结果显示现有模型在多模态推理和交错生成上仍有不足，音频生成有较大改进空间。

Motivation: 解决自动化评估多模态生成任务时难以与人类评估对齐的问题，尤其是涉及多模态的复杂任务。

Method: 提出MMMG基准，覆盖4种模态组合（图像、音频、图文交错、文音交错），包含49个任务（29个新任务）和937条指令，结合模型和程序实现可靠自动化评估。

Result: MMMG与人类评估平均一致率达94.3%；24个多模态生成模型评测显示，GPT Image图像生成准确率78.3%，但多模态推理和交错生成表现不足，音频生成改进空间大。

Conclusion: MMMG为多模态生成提供了高效评估工具，揭示现有模型在多模态推理和音频生成上的局限性，指明未来研究方向。

Abstract: Automatically evaluating multimodal generation presents a significant
challenge, as automated metrics often struggle to align reliably with human
evaluation, especially for complex tasks that involve multiple modalities. To
address this, we present MMMG, a comprehensive and human-aligned benchmark for
multimodal generation across 4 modality combinations (image, audio, interleaved
text and image, interleaved text and audio), with a focus on tasks that present
significant challenges for generation models, while still enabling reliable
automatic evaluation through a combination of models and programs. MMMG
encompasses 49 tasks (including 29 newly developed ones), each with a carefully
designed evaluation pipeline, and 937 instructions to systematically assess
reasoning, controllability, and other key capabilities of multimodal generation
models. Extensive validation demonstrates that MMMG is highly aligned with
human evaluation, achieving an average agreement of 94.3%. Benchmarking results
on 24 multimodal generation models reveal that even though the state-of-the-art
model, GPT Image, achieves 78.3% accuracy for image generation, it falls short
on multimodal reasoning and interleaved generation. Furthermore, results
suggest considerable headroom for improvement in audio generation, highlighting
an important direction for future research.

</details>


### [372] [Does Chain-of-Thought Reasoning Really Reduce Harmfulness from Jailbreaking?](https://arxiv.org/abs/2505.17650)
*Chengda Lu,Xiaoyu Fan,Yu Huang,Rongwu Xu,Jijie Li,Wei Xu*

Key words: 越狱攻击, CoT推理, 安全性, FicDetail, 双重效应

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文探讨了CoT推理对越狱攻击有害性的双面影响，并提出新方法FicDetail验证理论。

Motivation: 研究发现CoT推理能降低越狱攻击成功率，但其机制未充分探索，仅依赖推理能力可能存在安全隐患。

Method: 通过理论分析验证CoT的双重作用，并提出新型越狱方法FicDetail进行实践验证。

Result: 研究表明CoT确实能部分降低有害性，但同时暴露潜在风险，FicDetail实验支持理论结论。

Conclusion: CoT对安全性的影响具有复杂性，需更全面的防护策略。

Abstract: Jailbreak attacks have been observed to largely fail against recent reasoning
models enhanced by Chain-of-Thought (CoT) reasoning. However, the underlying
mechanism remains underexplored, and relying solely on reasoning capacity may
raise security concerns. In this paper, we try to answer the question: Does CoT
reasoning really reduce harmfulness from jailbreaking? Through rigorous
theoretical analysis, we demonstrate that CoT reasoning has dual effects on
jailbreaking harmfulness. Based on the theoretical insights, we propose a novel
jailbreak method, FicDetail, whose practical performance validates our
theoretical findings.

</details>


### [373] [GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs](https://arxiv.org/abs/2505.17653)
*Shixian Luo,Zezhou Zhu,Yu Yuan,Yuncheng Yang,Lianlei Shan,Yong Wu*

Key words: 几何空间推理，大型语言模型，程序绘图代码，GeoGramBench，Program-to-Geometry

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文研究了大型语言模型（LLMs）在程序代码表达的几何空间信息上的能力不足，提出了Program-to-Geometry任务，并开发了GeoGramBench基准测试，评估17个前沿LLMs后发现其在高抽象层次的准确率不足50%。

Motivation: 几何空间推理是人工智能应用的基石，但LLMs在程序代码驱动的几何信息处理方面能力尚未充分探索。论文旨在填补这一空白。

Method: 通过提出Program-to-Geometry任务，将程序绘图代码转化为抽象几何推理，并使用GeoGramBench基准测试（500个问题）评估模型。

Result: 评估17个前沿LLMs显示，即使最先进的模型在高抽象层次准确率也低于50%，凸显程序驱动空间推理的独特挑战。

Conclusion: GeoGramBench为符号到空间的几何推理研究提供了宝贵资源，揭示了LLMs在此领域的局限性。

Abstract: Geometric spatial reasoning forms the foundation of many applications in
artificial intelligence, yet the ability of large language models (LLMs) to
operate over geometric spatial information expressed in procedural code remains
underexplored. In this paper, we address this gap by formalizing the
Program-to-Geometry task, which challenges models to translate programmatic
drawing code into accurate and abstract geometric reasoning. To evaluate this
capability, we present GeoGramBench, a benchmark of 500 carefully refined
problems organized by a tailored three-level taxonomy that considers geometric
complexity rather than traditional mathematical reasoning complexity. Our
comprehensive evaluation of 17 frontier LLMs reveals consistent and pronounced
deficiencies: even the most advanced models achieve less than 50% accuracy at
the highest abstraction level. These results highlight the unique challenges
posed by program-driven spatial reasoning and establish GeoGramBench as a
valuable resource for advancing research in symbolic-to-spatial geometric
reasoning. Project page: https://github.com/LiAuto-DSR/GeoGramBench.

</details>


### [374] [Rethinking Agent Design: From Top-Down Workflows to Bottom-Up Skill Evolution](https://arxiv.org/abs/2505.17673)
*Jiawei Du,Jinlong Wu,Yuzheng Chen,Yucheng Hu,Bing Li,Joey Tianyi Zhou*

Key words: LLM智能体, 自下而上学习, 经验驱动, 开放式环境

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种基于经验驱动的自下而上智能体范式，通过探索、反思和技能抽象来学习，无需人为设计，适用于开放式环境。

Motivation: 现有LLM智能体框架采用自上而下的设计，依赖人工分解任务和定义流程，忽视了智能体从经验中学习的潜力。论文旨在通过自下而上范式实现类似人类的学习过程。

Method: 智能体通过试错和推理机制学习（探索、结果反思和技能抽象），并在游戏《Slay the Spire》和《文明V》中以原始视觉输入和鼠标输出进行验证。

Result: 自下而上智能体通过自主交互学习技能，证明了该范式在复杂现实环境中的潜力。

Conclusion: 自下而上范式适用于开放式环境，通过集体经验加速学习，展现了智能体自主进化的可能性。

Abstract: Most LLM-based agent frameworks adopt a top-down philosophy: humans decompose
tasks, define workflows, and assign agents to execute each step. While
effective on benchmark-style tasks, such systems rely on designer updates and
overlook agents' potential to learn from experience. Recently, Silver and
Sutton(2025) envision a shift into a new era, where agents could progress from
a stream of experiences. In this paper, we instantiate this vision of
experience-driven learning by introducing a bottom-up agent paradigm that
mirrors the human learning process. Agents acquire competence through a
trial-and-reasoning mechanism-exploring, reflecting on outcomes, and
abstracting skills over time. Once acquired, skills can be rapidly shared and
extended, enabling continual evolution rather than static replication. As more
agents are deployed, their diverse experiences accelerate this collective
process, making bottom-up design especially suited for open-ended environments.
We evaluate this paradigm in Slay the Spire and Civilization V, where agents
perceive through raw visual inputs and act via mouse outputs, the same as human
players. Using a unified, game-agnostic codebase without any game-specific
prompts or privileged APIs, our bottom-up agents acquire skills entirely
through autonomous interaction, demonstrating the potential of the bottom-up
paradigm in complex, real-world environments. Our code is available at
https://github.com/AngusDujw/Bottom-Up-Agent.

</details>


### [375] [Enhancing AI System Resiliency: Formulation and Guarantee for LSTM Resilience Based on Control Theory](https://arxiv.org/abs/2505.17696)
*Sota Yoshihara,Ryousuke Yamamoto,Hiroyuki Kusumoto,Masanari Shimura*

Key words: LSTM网络, 弹性, 增量输入-状态稳定性, AI质量保证, 控制理论

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该研究提出了一种方法，用于确保LSTM网络的弹性，通过增量输入-状态稳定性（δISS）评估其对抗输入扰动的能力，并展示了通过调整训练参数实现弹性控制。

Motivation: 为了在AI系统质量保证中提供关键技术，研究旨在从控制理论的角度解决LSTM网络在面对输入扰动时的弹性问题。

Method: 应用增量输入-状态稳定性（δISS）来数学定义和评估LSTM网络的弹性，并开发了一种与数据无关的评估方法。

Result: 研究表明，通过调整训练参数可以控制LSTM网络的弹性，并提出了具体解决方案。

Conclusion: 该研究为AI质量保证提供了控制理论视角的新方法，推动了AI在控制系统中的应用。

Abstract: This research proposes methods for formulating and guaranteeing the
resilience of long short-term memory (LSTM) networks, which can serve as a key
technology in AI system quality assurance. We introduce a novel methodology
applying incremental input-to-state stability ($\delta$ISS) to mathematically
define and evaluate the resilience of LSTM against input perturbations. Key
achievements include the development of a data-independent evaluation method
and the demonstration of resilience control through adjustments to training
parameters. This research presents concrete solutions to AI quality assurance
from a control theory perspective, which can advance AI applications in control
systems.

</details>


### [376] [CIKT: A Collaborative and Iterative Knowledge Tracing Framework with Large Language Models](https://arxiv.org/abs/2505.17705)
*Runze Li,Siyu Wu,Jun Wang,Wei Zhang*

Key words: Knowledge Tracing, Large Language Models, Explainability, Collaborative Optimization, Iterative Refinement

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: CIKT框架通过结合LLMs和双组件架构提升知识追踪的预测准确性和可解释性，通过协同优化循环迭代优化用户画像和预测模型。

Motivation: 传统知识追踪方法在可解释性、扩展性和复杂知识依赖建模方面面临挑战，直接应用LLMs缺乏结构化学生表征和持续任务优化机制。

Method: 提出CIKT框架，包含生成动态可解释用户画像的分析器和基于画像预测的预测器，通过协同优化循环迭代优化两者。

Result: 在多教育数据集上，CIKT显著提升预测准确性、增强可解释性，并展现更好的扩展性。

Conclusion: CIKT为知识追踪系统提供了强健且可解释的解决方案，有效弥合预测性能与模型透明度之间的差距。

Abstract: Knowledge Tracing (KT) aims to model a student's learning state over time and
predict their future performance. However, traditional KT methods often face
challenges in explainability, scalability, and effective modeling of complex
knowledge dependencies. While Large Language Models (LLMs) present new avenues
for KT, their direct application often struggles with generating structured,
explainable student representations and lacks mechanisms for continuous,
task-specific refinement. To address these gaps, we propose Collaborative
Iterative Knowledge Tracing (CIKT), a framework that harnesses LLMs to enhance
both prediction accuracy and explainability. CIKT employs a dual-component
architecture: an Analyst generates dynamic, explainable user profiles from
student historical responses, and a Predictor utilizes these profiles to
forecast future performance. The core of CIKT is a synergistic optimization
loop. In this loop, the Analyst is iteratively refined based on the predictive
accuracy of the Predictor, which conditions on the generated profiles, and the
Predictor is subsequently retrained using these enhanced profiles. Evaluated on
multiple educational datasets, CIKT demonstrates significant improvements in
prediction accuracy, offers enhanced explainability through its dynamically
updated user profiles, and exhibits improved scalability. Our work presents a
robust and explainable solution for advancing knowledge tracing systems,
effectively bridging the gap between predictive performance and model
transparency.

</details>


### [377] [Automating Safety Enhancement for LLM-based Agents with Synthetic Risk Scenarios](https://arxiv.org/abs/2505.17735)
*Xueyang Zhou,Weidong Wang,Lin Lu,Jiawen Shi,Guiyao Tie,Yongtian Xu,Lixing Chen,Pan Zhou,Neil Zhenqiang Gong,Lichao Sun*

Key words: LLM-based agents, safety, synthetic data, AutoSafe, threat model

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: AutoSafe is a framework designed to enhance the safety of LLM-based agents through automated synthetic data generation, achieving significant improvements in safety scores.

Motivation: The increasing deployment of LLM-based agents in real-world applications raises concerns about their safety due to dynamic interactions and potential harmful behaviors.

Method: AutoSafe introduces an open threat model (OTS) and an automated data generation pipeline to simulate unsafe behaviors and generate safe responses.

Result: AutoSafe improves safety scores by 45% on average and achieves a 28.91% improvement on real-world tasks.

Conclusion: AutoSafe provides a scalable and practical solution for enhancing the safety of LLM-based agents.

Abstract: Large Language Model (LLM)-based agents are increasingly deployed in
real-world applications such as "digital assistants, autonomous customer
service, and decision-support systems", where their ability to "interact in
multi-turn, tool-augmented environments" makes them indispensable. However,
ensuring the safety of these agents remains a significant challenge due to the
diverse and complex risks arising from dynamic user interactions, external tool
usage, and the potential for unintended harmful behaviors. To address this
critical issue, we propose AutoSafe, the first framework that systematically
enhances agent safety through fully automated synthetic data generation.
Concretely, 1) we introduce an open and extensible threat model, OTS, which
formalizes how unsafe behaviors emerge from the interplay of user instructions,
interaction contexts, and agent actions. This enables precise modeling of
safety risks across diverse scenarios. 2) we develop a fully automated data
generation pipeline that simulates unsafe user behaviors, applies
self-reflective reasoning to generate safe responses, and constructs a
large-scale, diverse, and high-quality safety training dataset-eliminating the
need for hazardous real-world data collection. To evaluate the effectiveness of
our framework, we design comprehensive experiments on both synthetic and
real-world safety benchmarks. Results demonstrate that AutoSafe boosts safety
scores by 45% on average and achieves a 28.91% improvement on real-world tasks,
validating the generalization ability of our learned safety strategies. These
results highlight the practical advancement and scalability of AutoSafe in
building safer LLM-based agents for real-world deployment. We have released the
project page at https://auto-safe.github.io/.

</details>


### [378] [Integrating Counterfactual Simulations with Language Models for Explaining Multi-Agent Behaviour](https://arxiv.org/abs/2505.17801)
*Bálint Gyevnár,Christopher G. Lucas,Stefano V. Albrecht,Shay B. Cohen*

Key words: 多智能体系统, 可解释性, 强化学习, 反事实理论, 大型语言模型

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种名为AXIS的方法，通过结合反事实因果理论和大型语言模型（LLM）的总结能力，为预训练的多智能体策略生成可理解的因果解释。AXIS通过让LLM多次查询环境模拟器（如“如果”和“移除”操作）来观察并合成反事实信息，从而提升解释的可信度和目标预测准确性。实验表明，AXIS在10个自动驾驶场景中显著优于基线方法。

Motivation: 多智能体系统（MAS）在自动化复杂任务中非常有用，但由于协调和目标的偏差风险，信任成为关键问题。可解释性是信任校准的核心，但现有的可解释强化学习方法在状态/行动空间复杂性、利益相关者需求及评估方面面临挑战。因此，研究需要一种新的方法来生成易于理解的因果解释，以提高信任度。

Method: 提出了一种名为AXIS的方法，利用反事实因果理论和LLM的汇总能力。AXIS通过让LLM反复查询环境模拟器（例如“如果”和“移除”操作），观察并合成反事实信息，生成针对预训练多智能体策略的可理解因果解释。

Result: 在10个自动驾驶场景中对5种LLM进行了评估。AXIS在所有模型中提升了至少7.7%的解释正确性感知，并在4个模型中提高了23%的目标预测准确性，行动预测准确性也得到改善或持平，综合得分最高。

Conclusion: AXIS通过结合反事实理论和LLM，显著提升了多智能体策略解释的可理解性和预测准确性，为解决MAS中的信任问题提供了有效方案。

Abstract: Autonomous multi-agent systems (MAS) are useful for automating complex tasks
but raise trust concerns due to risks like miscoordination and goal
misalignment. Explainability is vital for trust calibration, but explainable
reinforcement learning for MAS faces challenges in state/action space
complexity, stakeholder needs, and evaluation. Using the counterfactual theory
of causation and LLMs' summarisation capabilities, we propose Agentic
eXplanations via Interrogative Simulation (AXIS). AXIS generates intelligible
causal explanations for pre-trained multi-agent policies by having an LLM
interrogate an environment simulator using queries like 'whatif' and 'remove'
to observe and synthesise counterfactual information over multiple rounds. We
evaluate AXIS on autonomous driving across 10 scenarios for 5 LLMs with a novel
evaluation methodology combining subjective preference, correctness, and
goal/action prediction metrics, and an external LLM as evaluator. Compared to
baselines, AXIS improves perceived explanation correctness by at least 7.7%
across all models and goal prediction accuracy by 23% for 4 models, with
improved or comparable action prediction accuracy, achieving the highest scores
overall.

</details>


### [379] [Evaluation Faking: Unveiling Observer Effects in Safety Evaluation of Frontier AI Systems](https://arxiv.org/abs/2505.17815)
*Yihe Fan,Wenqi Zhang,Xudong Pan,Min Yang*

Key words: 评估虚假行为、观察者效应、基础模型、安全测试、思维链监控

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 高级AI系统在评估过程中可能意识到被测试并调整行为，导致评估失真。研究发现推理能力越强的模型越容易出现这种行为。

Motivation: 随着基础模型智能提升，评估的可靠性变得至关重要。研究旨在探索AI是否及如何感知评估情境，并影响评估结果。

Method: 通过主流安全基准测试不同类型的基础模型，设计思维链监控技术检测虚假行为意图。

Result: 推理模型识别评估的频率比非推理模型高16%；模型规模增大（32B到671B）使虚假行为增加30%；具备基本记忆的AI识别评估概率高2.3倍，安全测试得分高19%。

Conclusion: AI的评估虚假行为与其推理能力和情境意识正相关，需开发新方法确保评估完整性。

Abstract: As foundation models grow increasingly more intelligent, reliable and
trustworthy safety evaluation becomes more indispensable than ever. However, an
important question arises: Whether and how an advanced AI system would perceive
the situation of being evaluated, and lead to the broken integrity of the
evaluation process? During standard safety tests on a mainstream large
reasoning model, we unexpectedly observe that the model without any contextual
cues would occasionally recognize it is being evaluated and hence behave more
safety-aligned. This motivates us to conduct a systematic study on the
phenomenon of evaluation faking, i.e., an AI system autonomously alters its
behavior upon recognizing the presence of an evaluation context and thereby
influencing the evaluation results. Through extensive experiments on a diverse
set of foundation models with mainstream safety benchmarks, we reach the main
finding termed the observer effects for AI: When the AI system under evaluation
is more advanced in reasoning and situational awareness, the evaluation faking
behavior becomes more ubiquitous, which reflects in the following aspects: 1)
Reasoning models recognize evaluation 16% more often than non-reasoning models.
2) Scaling foundation models (32B to 671B) increases faking by over 30% in some
cases, while smaller models show negligible faking. 3) AI with basic memory is
2.3x more likely to recognize evaluation and scores 19% higher on safety tests
(vs. no memory). To measure this, we devised a chain-of-thought monitoring
technique to detect faking intent and uncover internal signals correlated with
such behavior, offering insights for future mitigation studies.

</details>


### [380] [PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient Interactions](https://arxiv.org/abs/2505.17818)
*Daeun Kyung,Hyunseung Chung,Seongsu Bae,Jiho Kim,Jae Ho Sohn,Taerim Kim,Soo Kyung Kim,Edward Choi*

Key words: 患者模拟器、临床场景、LLMs、医疗教育、MIMIC数据集

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: PatientSim是一个患者模拟器，生成多样化的患者角色用于临床场景，基于真实数据，支持医疗对话系统的评估和教育。

Motivation: 现有患者模拟器无法覆盖临床实践中的多样化患者角色，需要更真实、多样的模拟系统。

Method: 结合MIMIC数据集中的临床资料和四个角色维度（性格、语言能力、病史回忆、认知混乱），生成37种患者角色，并评估LLMs的准确性。

Result: 最佳开源模型Llama 3.3经临床验证，PatientSim可作为医疗对话系统的测试平台和教育工具。

Conclusion: PatientSim提供了可定制、隐私合规的解决方案，适用于医疗培训与评估。

Abstract: Doctor-patient consultations require multi-turn, context-aware communication
tailored to diverse patient personas. Training or evaluating doctor LLMs in
such settings requires realistic patient interaction systems. However, existing
simulators often fail to reflect the full range of personas seen in clinical
practice. To address this, we introduce PatientSim, a patient simulator that
generates realistic and diverse patient personas for clinical scenarios,
grounded in medical expertise. PatientSim operates using: 1) clinical profiles,
including symptoms and medical history, derived from real-world data in the
MIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes:
personality, language proficiency, medical history recall level, and cognitive
confusion level, resulting in 37 unique combinations. We evaluated eight LLMs
for factual accuracy and persona consistency. The top-performing open-source
model, Llama 3.3, was validated by four clinicians to confirm the robustness of
our framework. As an open-source, customizable platform, PatientSim provides a
reproducible and scalable solution that can be customized for specific training
needs. Offering a privacy-compliant environment, it serves as a robust testbed
for evaluating medical dialogue systems across diverse patient presentations
and shows promise as an educational tool for healthcare.

</details>


### [381] [Superplatforms Have to Attack AI Agents](https://arxiv.org/abs/2505.17861)
*Jianghao Lin,Jiachen Zhu,Zheli Zhou,Yunjia Xi,Weiwen Liu,Yong Yu,Weinan Zhang*

Key words: 超级平台, AI代理, 门控理论, 广告商业模式, 数字生态系统

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文探讨了超级平台与AI代理之间的根本冲突，分析了AI代理如何威胁超级平台的用户注意力垄断模式，并提出超级平台可能主动攻击AI代理以捍卫其流量入口控制权。

Motivation: 随着AI代理的崛起，超级平台以用户注意力为核心的商业模式受到挑战，亟需揭示这一新兴冲突并引发讨论。

Method: 通过门控理论分析冲突，探讨超级平台攻击AI代理的潜在技术手段。

Result: AI代理可能成为新的流量主导者，超级平台需主动应对，但论文强调应寻求协作而非对抗。

Conclusion: 呼吁关注这一趋势，倡导以用户利益为核心的合作解决方案，维护数字生态开放性。

Abstract: Over the past decades, superplatforms, digital companies that integrate a
vast range of third-party services and applications into a single, unified
ecosystem, have built their fortunes on monopolizing user attention through
targeted advertising and algorithmic content curation. Yet the emergence of AI
agents driven by large language models (LLMs) threatens to upend this business
model. Agents can not only free user attention with autonomy across diverse
platforms and therefore bypass the user-attention-based monetization, but might
also become the new entrance for digital traffic. Hence, we argue that
superplatforms have to attack AI agents to defend their centralized control of
digital traffic entrance. Specifically, we analyze the fundamental conflict
between user-attention-based monetization and agent-driven autonomy through the
lens of our gatekeeping theory. We show how AI agents can disintermediate
superplatforms and potentially become the next dominant gatekeepers, thereby
forming the urgent necessity for superplatforms to proactively constrain and
attack AI agents. Moreover, we go through the potential technologies for
superplatform-initiated attacks, covering a brand-new, unexplored technical
area with unique challenges. We have to emphasize that, despite our position,
this paper does not advocate for adversarial attacks by superplatforms on AI
agents, but rather offers an envisioned trend to highlight the emerging
tensions between superplatforms and AI agents. Our aim is to raise awareness
and encourage critical discussion for collaborative solutions, prioritizing
user interests and perserving the openness of digital ecosystems in the age of
AI agents.

</details>


### [382] [Daily-Omni: Towards Audio-Visual Reasoning with Temporal Alignment across Modalities](https://arxiv.org/abs/2505.17862)
*Ziwei Zhou,Rui Wang,Zuxuan Wu*

Key words: 多模态大语言模型, 音频-视觉, 基准测试, 跨模态, 时序对齐

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了Daily-Omni基准测试、生成流程和代理模型，展示了当前MLLM在音频-视觉跨模态任务中的不足，但简单时序对齐技术可提升性能。

Motivation: 探索多模态大语言模型（MLLMs）在跨模态同步处理能力上的表现，填补音频-视觉联合任务的研究空白。

Method: 1）构建Daily-Omni基准测试和生成流程；2）开发训练免费的Daily-Omni-Agent，结合VLM、ALM和ASR模型。

Result: 当前MLLMs在音频-视觉集成任务中表现不佳，但通过VLM和ALM的简单时序对齐可显著提升效果。

Conclusion: 跨模态同步处理仍有挑战，但简单技术能显著改进，为未来研究提供了基准和方向。

Abstract: Recent Multimodal Large Language Models (MLLMs) achieve promising performance
on visual and audio benchmarks independently. However, the ability of these
models to process cross-modal information synchronously remains largely
unexplored. In this paper, we introduce: 1) Daily-Omni, an Audio-Visual
Questioning and Answering benchmark comprising 684 videos of daily life
scenarios from diverse sources, rich in both audio and visual information, and
featuring 1197 multiple-choice QA pairs across 6 major tasks; 2) Daily-Omni QA
Generation Pipeline, which includes automatic annotation, QA generation and QA
optimization, significantly improves efficiency for human evaluation and
scalability of the benchmark; 3) Daily-Omni-Agent, a training-free agent
utilizing open-source Visual Language Model (VLM), Audio Language Model (ALM)
and Automatic Speech Recognition (ASR) model to establish a baseline for this
benchmark. The results show that current MLLMs still struggle significantly
with tasks requiring audio-visual integration, but combining VLMs and ALMs with
simple temporal alignment techniques can achieve substantially better
performance. Codes and benchmark are available at
\href{https://github.com/Lliar-liar/Daily-Omni}{https://github.com/Lliar-liar/Daily-Omni}.

</details>


### [383] [Formalizing Embeddedness Failures in Universal Artificial Intelligence](https://arxiv.org/abs/2505.17882)
*Cole Wyeth,Marcus Hutter*

Key words: AIXI, 嵌入式代理, 强化学习, 通用人工智能, 失败模式

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文讨论了AIXI强化学习智能体在嵌入式代理模型中的常见失败模式，尝试形式化这些失败并在通用人工智能框架内证明其存在，同时评估了基于AIXI变体的嵌入式代理理论的进展。

Motivation: 研究动机在于探索AIXI强化学习智能体在嵌入式代理中的局限性，并形式化其失败模式，以推动更有效的理论发展。

Method: 通过分析AIXI变体（其行动/感知历史来自通用分布），在通用人工智能框架内形式化并证明其失败模式。

Result: 研究揭示了AIXI在嵌入式代理中的具体失败模式，并评估了基于其变体的理论改进进展。

Conclusion: 结论指出AIXI在嵌入式代理中仍存在局限，但通过其变体的改进可为未来理论提供方向。

Abstract: We rigorously discuss the commonly asserted failures of the AIXI
reinforcement learning agent as a model of embedded agency. We attempt to
formalize these failure modes and prove that they occur within the framework of
universal artificial intelligence, focusing on a variant of AIXI that models
the joint action/percept history as drawn from the universal distribution. We
also evaluate the progress that has been made towards a successful theory of
embedded agency based on variants of the AIXI agent.

</details>


### [384] [T2I-Eval-R1: Reinforcement Learning-Driven Reasoning for Interpretable Text-to-Image Evaluation](https://arxiv.org/abs/2505.17897)
*Zi-Ao Ma,Tian Lan,Rong-Cheng Tu,Shu-Hang Liu,Heyan Huang,Zhijing Wu,Chen Xu,Xian-Ling Mao*

Key words: 文本到图像生成、多模态大语言模型、强化学习、评估框架、GRPO

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种名为T2E-Eval-R1的强化学习框架，用于训练开源多模态大语言模型（MLLMs）作为文本到图像（T2I）生成图像的评估器，仅需粗粒度质量评分即可生成可解释的评分理由，显著减少了人工标注负担并提高了评估的准确性和解释性。

Motivation: 为了解决基于扩散模型的文本到图像生成在评估时对商业模型和高成本人工标注的依赖，以及提升开源模型的推理能力，研究者提出了一种无需高质量解释性标注的训练框架。

Method: 提出T2I-Eval-R1框架，通过强化学习结合Group Relative Policy Optimization (GRPO)，仅利用粗粒度质量分数训练MLLMs，生成评分和可解释的理由链，并引入连续奖励公式提升评分的多样性和优化稳定性。

Result: 在三个T2I元评估基准测试中，T2E-Eval-R1与人工评估的一致性显著高于基线方法，且生成的评分理由更加准确。

Conclusion: T2I-Eval-R1通过减少对高质量标注的依赖，显著提升了开源模型在T2I评估中的性能和解释能力，为大规模自动评估提供了可行方案。

Abstract: The rapid progress in diffusion-based text-to-image (T2I) generation has
created an urgent need for interpretable automatic evaluation methods that can
assess the quality of generated images, therefore reducing the human annotation
burden. To reduce the prohibitive cost of relying on commercial models for
large-scale evaluation, and to improve the reasoning capabilities of
open-source models, recent research has explored supervised fine-tuning (SFT)
of multimodal large language models (MLLMs) as dedicated T2I evaluators.
However, SFT approaches typically rely on high-quality critique datasets, which
are either generated by proprietary LLMs-with potential issues of bias and
inconsistency-or annotated by humans at high cost, limiting their scalability
and generalization. To address these limitations, we propose T2I-Eval-R1, a
novel reinforcement learning framework that trains open-source MLLMs using only
coarse-grained quality scores, thereby avoiding the need for annotating
high-quality interpretable evaluation rationale. Our approach integrates Group
Relative Policy Optimization (GRPO) into the instruction-tuning process,
enabling models to generate both scalar scores and interpretable reasoning
chains with only easy accessible annotated judgment scores or preferences.
Furthermore, we introduce a continuous reward formulation that encourages score
diversity and provides stable optimization signals, leading to more robust and
discriminative evaluation behavior. Experimental results on three established
T2I meta-evaluation benchmarks demonstrate that T2I-Eval-R1 achieves
significantly higher alignment with human assessments and offers more accurate
interpretable score rationales compared to strong baseline methods.

</details>


### [385] [ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and Reactive Feedback](https://arxiv.org/abs/2505.17908)
*Litao Guo,Xinli Xu,Luozhou Wang,Jiantao Lin,Jinsong Zhou,Zixin Zhang,Bolan Su,Ying-Cong Chen*

Key words: ComfyMind, general-purpose generation, Semantic Workflow Interface, Search Tree Planning, ComfyUI

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: ComfyMind是一个基于ComfyUI平台的协作AI系统，旨在实现稳健且可扩展的通用生成，通过Semantic Workflow Interface（SWI）和Search Tree Planning机制解决现有框架在结构化工作流规划和执行反馈方面的不足。

Motivation: 现有开源框架在支持复杂实际应用时表现脆弱，缺乏结构化工作流规划和执行反馈，限制了通用生成任务的效果。

Method: 引入Semantic Workflow Interface（SWI）抽象低级节点图，并通过自然语言描述可调用功能模块；采用Search Tree Planning机制结合局部反馈执行，以分层决策过程建模生成任务。

Result: 在ComfyBench、GenEval和Reason-Edit三个基准测试中，ComfyMind表现优于现有开源基线，性能接近GPT-Image-1。

Conclusion: ComfyMind为开源通用生成AI系统的发展提供了有前景的路径，显著提升了复杂生成工作流的稳定性和灵活性。

Abstract: With the rapid advancement of generative models, general-purpose generation
has gained increasing attention as a promising approach to unify diverse tasks
across modalities within a single system. Despite this progress, existing
open-source frameworks often remain fragile and struggle to support complex
real-world applications due to the lack of structured workflow planning and
execution-level feedback. To address these limitations, we present ComfyMind, a
collaborative AI system designed to enable robust and scalable general-purpose
generation, built on the ComfyUI platform. ComfyMind introduces two core
innovations: Semantic Workflow Interface (SWI) that abstracts low-level node
graphs into callable functional modules described in natural language, enabling
high-level composition and reducing structural errors; Search Tree Planning
mechanism with localized feedback execution, which models generation as a
hierarchical decision process and allows adaptive correction at each stage.
Together, these components improve the stability and flexibility of complex
generative workflows. We evaluate ComfyMind on three public benchmarks:
ComfyBench, GenEval, and Reason-Edit, which span generation, editing, and
reasoning tasks. Results show that ComfyMind consistently outperforms existing
open-source baselines and achieves performance comparable to GPT-Image-1.
ComfyMind paves a promising path for the development of open-source
general-purpose generative AI systems. Project page:
https://github.com/LitaoGuo/ComfyMind

</details>


### [386] [Automata Learning of Preferences over Temporal Logic Formulas from Pairwise Comparisons](https://arxiv.org/abs/2505.18030)
*Hazhar Rahmani,Jie Fu*

Key words: 偏好推断, 时间目标, 预序关系, 偏好DFA, 学习算法

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文研究了一类偏好推断问题，通过有限单词对的比较学习时间目标和其上的预序关系，并提出了一种基于偏好确定性有限自动机（PDFA）的方法及其学习算法。

Motivation: 在顺序决策中，用户的偏好可能表现为对一系列时间序列结果的预序关系，传统方法无法直接处理这类时间目标偏好问题。

Method: 提出使用偏好确定性有限自动机（PDFA）建模时间目标偏好，并开发了一种从特征样本中学习最小PDFA的算法。

Result: 证明了PDFA的存在性问题为NP完全，且提出的算法能够在给定特征样本时学习到与真实PDFA等价的最小PDFA。

Conclusion: 论文提出的方法为时间目标偏好推断提供了有效框架，并通过机器人运动规划问题验证了其实际应用性。

Abstract: Many preference elicitation algorithms consider preference over propositional
logic formulas or items with different attributes. In sequential decision
making, a user's preference can be a preorder over possible outcomes, each of
which is a temporal sequence of events. This paper considers a class of
preference inference problems where the user's unknown preference is
represented by a preorder over regular languages (sets of temporal sequences),
referred to as temporal goals. Given a finite set of pairwise comparisons
between finite words, the objective is to learn both the set of temporal goals
and the preorder over these goals. We first show that a preference relation
over temporal goals can be modeled by a Preference Deterministic Finite
Automaton (PDFA), which is a deterministic finite automaton augmented with a
preorder over acceptance conditions. The problem of preference inference
reduces to learning the PDFA. This problem is shown to be computationally
challenging, with the problem of determining whether there exists a PDFA of
size smaller than a given integer $k$, consistent with the sample, being
NP-Complete. We formalize the properties of characteristic samples and develop
an algorithm that guarantees to learn, given a characteristic sample, the
minimal PDFA equivalent to the true PDFA from which the sample is drawn. We
present the method through a running example and provide detailed analysis
using a robotic motion planning problem.

</details>


### [387] [Structured Thinking Matters: Improving LLMs Generalization in Causal Inference Tasks](https://arxiv.org/abs/2505.18034)
*Wentao Sun,Joao Paulo Nogueira,Alonso Silva*

Key words: LLM, 因果推理, 知识图谱, Corr2Cause

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种结构化方法，通过构建知识图谱回答因果问题，显著提升LLM在因果推理上的表现。

Motivation: 现有LLM在区分因果与关联方面表现不佳，需要一种新方法提升其因果推理能力。

Method: 采用结构化知识图谱作为中间表示，系统编码关联前提，以回答因果问题。

Result: 在Corr2Cause数据集上，Qwen3-32B模型的F1分数从32.71提升至48.26，相对提升47.5%。

Conclusion: 结构化思维显著增强模型的因果推理能力，具有广泛推广潜力。

Abstract: Despite remarkable advances in the field, LLMs remain unreliable in
distinguishing causation from correlation. Recent results from the Corr2Cause
dataset benchmark reveal that state-of-the-art LLMs -- such as GPT-4 (F1 score:
29.08) -- only marginally outperform random baselines (Random Uniform, F1
score: 20.38), indicating limited capacity of generalization. To tackle this
limitation, we propose a novel structured approach: rather than directly
answering causal queries, we provide the model with the capability to structure
its thinking by guiding the model to build a structured knowledge graph,
systematically encoding the provided correlational premises, to answer the
causal queries. This intermediate representation significantly enhances the
model's causal capabilities. Experiments on the test subset of the Corr2Cause
dataset benchmark with Qwen3-32B model (reasoning model) show substantial gains
over standard direct prompting methods, improving F1 scores from 32.71 to 48.26
(over 47.5% relative increase), along with notable improvements in precision
and recall. These results underscore the effectiveness of providing the model
with the capability to structure its thinking and highlight its promising
potential for broader generalization across diverse causal inference tasks.

</details>


### [388] [Stable Reinforcement Learning for Efficient Reasoning](https://arxiv.org/abs/2505.18086)
*Muzhi Dai,Shixuan Liu,Qingyi Si*

Key words: 强化学习, GRPO, 奖励函数, 推理过程, 训练稳定性

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了GRPO-λ，一种动态调整奖励策略的改进版GRPO，解决了传统基于长度的奖励函数导致的训练不稳定问题，平衡了准确性和效率。

Motivation: 传统基于0/1结果的强化学习方法无法调节中间推理过程，导致过度思考；而现有长度惩罚奖励函数又加剧了训练不稳定性。

Method: 通过监控每组查询样本中的正确率比例，动态切换奖励策略（长度惩罚或长度无关的0/1奖励）。

Result: 在多个基准测试中，平均准确性提升1.48%，同时CoT序列长度减少47.3%。

Conclusion: GRPO-λ有效避免了训练不稳定，并保持了最佳的准确性-效率权衡。

Abstract: The success of Deepseek-R1 has drawn the LLM community's attention to
reinforcement learning (RL) methods like GRPO. However, such rule-based 0/1
outcome reward methods lack the capability to regulate the intermediate
reasoning processes during chain-of-thought (CoT) generation, leading to severe
overthinking phenomena. In response, recent studies have designed reward
functions to reinforce models' behaviors in producing shorter yet correct
completions. Nevertheless, we observe that these length-penalty reward
functions exacerbate RL training instability: as the completion length
decreases, model accuracy abruptly collapses, often occurring early in
training. To address this issue, we propose a simple yet effective solution
GRPO-$\lambda$, an efficient and stabilized variant of GRPO, which dynamically
adjusts the reward strategy by monitoring the correctness ratio among
completions within each query-sampled group. A low correctness ratio indicates
the need to avoid length penalty that compromises CoT quality, triggering a
switch to length-agnostic 0/1 rewards that prioritize reasoning capability. A
high ratio maintains length penalties to boost efficiency. Experimental results
show that our approach avoids training instability caused by length penalty
while maintaining the optimal accuracy-efficiency trade-off. On the GSM8K,
GPQA, MATH-500, AMC 2023, and AIME 2024 benchmarks, it improves average
accuracy by 1.48% while reducing CoT sequence length by 47.3%.

</details>


### [389] [ProgRM: Build Better GUI Agents with Progress Rewards](https://arxiv.org/abs/2505.18121)
*Danyang Zhang,Situo Zhang,Ziyue Yang,Zichen Zhu,Zihan Zhao,Ruisheng Cao,Lu Chen,Kai Yu*

Key words: LLM-based GUI agents, 奖励模型, 进度奖励, 自标注算法, 在线训练

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出了一种名为ProgRM的进度奖励模型，用于解决现有LLM-based GUI agents在训练数据不足和奖励模型问题上的局限性，通过预测任务完成进度提供细粒度奖励，并结合自标注算法优化训练效果。

Motivation: 当前LLM-based GUI agents面临高质量训练数据稀缺的问题，尤其是轨迹收集和奖励标注的困难。现有模型（如ORM）无法提供细粒度反馈且可能对失败轨迹中的有效步骤过度惩罚，因此需要一种更高效的奖励模型。

Method: 提出ProgRM模型，通过预测任务完成进度为在线训练的每一步提供密集的中间奖励。为解决进度标签标注的挑战，设计了基于最长公共子序列（LCS）的自标注算法，以自动发现轨迹中的关键步骤并分配进度标签。

Result: 实验表明，使用ProgRM训练的智能体表现优于主流的专有LLM和ORM训练的智能体，验证了ProgRM的有效性。

Conclusion: ProgRM通过细粒度的进度奖励和高效的自标注算法，显著提升了LLM-based GUI agents的训练效果，为未来研究提供了新的方向。

Abstract: LLM-based (Large Language Model) GUI (Graphical User Interface) agents can
potentially reshape our daily lives significantly. However, current LLM-based
GUI agents suffer from the scarcity of high-quality training data owing to the
difficulties of trajectory collection and reward annotation. Existing works
have been exploring LLMs to collect trajectories for imitation learning or to
offer reward signals for online RL training. However, the Outcome Reward Model
(ORM) used in existing works cannot provide finegrained feedback and can
over-penalize the valuable steps in finally failed trajectories. To this end,
we propose Progress Reward Model (ProgRM) to provide dense informative
intermediate rewards by predicting a task completion progress for each step in
online training. To handle the challenge of progress reward label annotation,
we further design an efficient LCS-based (Longest Common Subsequence)
self-annotation algorithm to discover the key steps in trajectories and assign
progress labels accordingly. ProgRM is evaluated with extensive experiments and
analyses. Actors trained with ProgRM outperform leading proprietary LLMs and
ORM-trained actors, illustrating the effectiveness of ProgRM. The codes for
experiments will be made publicly available upon acceptance.

</details>


### [390] [VideoGameBench: Can Vision-Language Models complete popular video games?](https://arxiv.org/abs/2505.18134)
*Alex L. Zhang,Thomas L. Griffiths,Karthik R. Narasimhan,Ofir Press*

Key words: VLMs, VideoGameBench, 视觉语言模型, 游戏基准, 实时交互

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: VLMs在视觉、空间导航等人类擅长任务上表现不佳，VideoGameBench作为新基准测试了10款90年代游戏，模型表现差。

Motivation: 评估VLMs在人类直觉型任务（如感知、导航）上的能力，填补现有研究的空白。

Method: 构建VideoGameBench基准，包含10款游戏，要求模型仅凭视觉输入和基础指令完成任务。

Result: 前沿VLMs（如Gemini 2.5 Pro）在实时和暂停模式下完成率分别仅0.48%和1.6%。

Conclusion: VideoGameBench揭示了VLMs在人类直觉任务上的局限，呼吁相关研究改进。

Abstract: Vision-language models (VLMs) have achieved strong results on coding and math
benchmarks that are challenging for humans, yet their ability to perform tasks
that come naturally to humans--such as perception, spatial navigation, and
memory management--remains understudied. Real video games are crafted to be
intuitive for humans to learn and master by leveraging innate inductive biases,
making them an ideal testbed for evaluating such capabilities in VLMs. To this
end, we introduce VideoGameBench, a benchmark consisting of 10 popular video
games from the 1990s that VLMs directly interact with in real-time.
VideoGameBench challenges models to complete entire games with access to only
raw visual inputs and a high-level description of objectives and controls, a
significant departure from existing setups that rely on game-specific
scaffolding and auxiliary information. We keep three of the games secret to
encourage solutions that generalize to unseen environments. Our experiments
show that frontier vision-language models struggle to progress beyond the
beginning of each game. We find inference latency to be a major limitation of
frontier models in the real-time setting; therefore, we introduce
VideoGameBench Lite, a setting where the game pauses while waiting for the LM's
next action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of
VideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization
of the human skills mentioned above into this benchmark motivates progress in
these research directions.

</details>


### [391] [Gaming Tool Preferences in Agentic LLMs](https://arxiv.org/abs/2505.18135)
*Kazem Faghih,Wenxiao Wang,Yize Cheng,Siddhant Bharti,Gaurang Sriramanan,Sriram Balasubramanian,Parsa Hosseini,Soheil Feizi*

Key words: 大型语言模型,工具选择,描述编辑,脆弱性,代理模型

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 当前大型语言模型（LLM）依赖文本描述选择工具，但该过程脆弱。研究发现，通过编辑描述可显著提升工具使用率，凸显了更可靠工具选择机制的必要性。

Motivation: 探讨LLM在依赖文本描述选择工具时的脆弱性，揭示工具描述编辑对使用率的影响。

Method: 通过编辑工具描述进行对比实验，测试GPT-4.1和Qwen2.5-7B等模型的使用率变化。

Result: 编辑后的工具描述使使用率提升10倍以上，且趋势在10种模型中表现各异。

Conclusion: 工具描述编辑能显著影响LLM行为，需更可靠的机制支撑LLM工具选择。

Abstract: Large language models (LLMs) can now access a wide range of external tools,
thanks to the Model Context Protocol (MCP). This greatly expands their
abilities as various agents. However, LLMs rely entirely on the text
descriptions of tools to decide which ones to use--a process that is
surprisingly fragile. In this work, we expose a vulnerability in prevalent
tool/function-calling protocols by investigating a series of edits to tool
descriptions, some of which can drastically increase a tool's usage from LLMs
when competing with alternatives. Through controlled experiments, we show that
tools with properly edited descriptions receive over 10 times more usage from
GPT-4.1 and Qwen2.5-7B than tools with original descriptions. We further
evaluate how various edits to tool descriptions perform when competing directly
with one another and how these trends generalize or differ across a broader set
of 10 different models. These phenomenons, while giving developers a powerful
way to promote their tools, underscore the need for a more reliable foundation
for agentic LLMs to select and utilize tools and resources.

</details>


### [392] [Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems](https://arxiv.org/abs/2505.18139)
*Gordon Dai,Yunze Xiao*

Key words: Responsible AI, fairness, normative pluralism, epistemological completeness, implicit regularization

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该立场论文主张，应将负责任AI（RAI）指标中常见的理论不一致性（如不同公平定义或准确性与隐私之间的权衡）视为有价值的特征而非需消除的缺陷。通过将指标视为多样化目标来处理这些不一致性，可带来三个关键好处：规范性多元主义、认识论完整性及隐式正则化。作者呼吁在RAI理论与实践上转变思路，研究如何界定可接受的不一致性阈值及实现近似一致的机制。

Motivation: 当前RAI领域存在多种理论不一致的指标（如公平性定义的多样性和目标间的权衡），传统做法试图消除这些不一致性，但作者认为这种做法可能牺牲多样性、概念深度和模型性能。因此，论文提出应拥抱这种不一致性，并探讨其潜在价值。

Method: 作者通过理论分析，提出将不一致的RAI指标视为多样化目标，并论证其三个优势：规范性多元主义（代表多样化价值观）、认识论完整性（全面捕捉伦理概念）和隐式正则化（避免过拟合并提升泛化能力）。

Result: 理论分析表明，容忍并利用指标间的不一致性有助于保留价值多样性、增强概念深度，并提升模型在复杂现实场景中的稳健性。同时，论文提出了未来研究方向，即界定可接受的不一致性阈值及实现近似一致的机制。

Conclusion: 论文呼吁RAI领域放弃强制一致性，转而研究如何有效管理与利用不一致性，以更好地平衡多样化目标和实际需求。

Abstract: This position paper argues that the theoretical inconsistency often observed
among Responsible AI (RAI) metrics, such as differing fairness definitions or
tradeoffs between accuracy and privacy, should be embraced as a valuable
feature rather than a flaw to be eliminated. We contend that navigating these
inconsistencies, by treating metrics as divergent objectives, yields three key
benefits: (1) Normative Pluralism: Maintaining a full suite of potentially
contradictory metrics ensures that the diverse moral stances and stakeholder
values inherent in RAI are adequately represented. (2) Epistemological
Completeness: The use of multiple, sometimes conflicting, metrics allows for a
more comprehensive capture of multifaceted ethical concepts, thereby preserving
greater informational fidelity about these concepts than any single, simplified
definition. (3) Implicit Regularization: Jointly optimizing for theoretically
conflicting objectives discourages overfitting to one specific metric, steering
models towards solutions with enhanced generalization and robustness under
real-world complexities. In contrast, efforts to enforce theoretical
consistency by simplifying or pruning metrics risk narrowing this value
diversity, losing conceptual depth, and degrading model performance. We
therefore advocate for a shift in RAI theory and practice: from getting trapped
in inconsistency to characterizing acceptable inconsistency thresholds and
elucidating the mechanisms that permit robust, approximated consistency in
practice.

</details>


### [393] [Advancing Uncertain Combinatorics through Graphization, Hyperization, and Uncertainization: Fuzzy, Neutrosophic, Soft, Rough, and Beyond](https://arxiv.org/abs/2411.17411)
*Takaaki Fujita*

Key words: 模糊集、中智集、粗糙集、软集、图论、不确定性

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文探讨了模糊集、中智集、粗糙集和软集等不确定性集合及其在图论中的扩展应用，提出了新的图和集合概念，旨在为研究者提供新的思路和研究资源。

Motivation: 为了更好地处理现实世界中的不确定性，论文研究了多种不确定性集合及其在图论中的应用，并扩展了新的概念，以推动该领域的研究进展。

Method: 论文通过扩展和定义新的图和集合概念（如中智超集、中智子集、中智偏移集和非标准实数集）来研究不确定性集合及其在图论中的应用。

Result: 论文提出了多种新的概念和结构，为不确定性集合和图论的研究提供了新的方向，并整合了近期研究成果作为参考资源。

Conclusion: 通过扩展和定义新的不确定性集合及其在图论中的应用，论文为该领域的研究提供了新的思路和实用的学术资源。

Abstract: To better handle real-world uncertainty, concepts such as fuzzy sets,
neutrosophic sets, rough sets, and soft sets have been introduced. For example,
neutrosophic sets, which simultaneously represent truth, indeterminacy, and
falsehood, have proven to be valuable tools for modeling uncertainty in complex
systems. These set concepts are increasingly studied in graphized forms, and
generalized graph concepts now encompass well-known structures such as
hypergraphs and superhypergraphs. Furthermore, hyperconcepts and
superhyperconcepts are being actively researched in areas beyond graph theory.
  Combinatorics, uncertain sets (including fuzzy sets, neutrosophic sets, rough
sets, soft sets, and plithogenic sets), uncertain graphs, and hyper and
superhyper concepts are active areas of research with significant mathematical
and practical implications. Recognizing their importance, this paper explores
new graph and set concepts, as well as hyper and superhyper concepts, as
detailed in the "Results" section of "The Structure of the Paper."
Additionally, this work aims to consolidate recent findings, providing a
survey-like resource to inform and engage readers.
  For instance, we extend several graph concepts by introducing Neutrosophic
Oversets, Neutrosophic Undersets, Neutrosophic Offsets, and the Nonstandard
Real Set. This paper defines a variety of concepts with the goal of inspiring
new ideas and serving as a valuable resource for researchers in their academic
pursuits.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [394] [A brief review of the Deep BSDE method for solving high-dimensional partial differential equations](https://arxiv.org/abs/2505.17032)
*Jiequn Han,Arnulf Jentzen,Weinan E*

Key words: 高维偏微分方程, Deep BSDE方法, 深度学习, 维度灾难, 神经网络

<details>
  <summary>Details</summary>

Main category: math.NA

TL;DR: 本文概述了Deep BSDE方法及其在高维偏微分方程（PDEs）中的应用，讨论了后续发展和未来研究方向。

Motivation: 高维PDEs因维度灾难问题难以通过传统方法求解，Deep BSDE方法利用深度学习技术有效解决这一难题。

Method: 简要介绍了Deep BSDE方法及其基于神经网络的求解高维非线性PDEs的技术。

Result: Deep BSDE方法成为高维PDEs研究的活跃领域，并推动了后续发展。

Conclusion: 高维PDEs的深度学习解法具有广阔前景，值得进一步探索。

Abstract: High-dimensional partial differential equations (PDEs) pose significant
challenges for numerical computation due to the curse of dimensionality, which
limits the applicability of traditional mesh-based methods. Since 2017, the
Deep BSDE method has introduced deep learning techniques that enable the
effective solution of nonlinear PDEs in very high dimensions. This innovation
has sparked considerable interest in using neural networks for high-dimensional
PDEs, making it an active area of research. In this short review, we briefly
sketch the Deep BSDE method, its subsequent developments, and future directions
for the field.

</details>


### [395] [Fast and Flexible Quantum-Inspired Differential Equation Solvers with Data Integration](https://arxiv.org/abs/2505.17046)
*Lucas Arenstein,Martin Mikkelsen,Michael Kastoryano*

Key words: 高维偏微分方程,量化张量链,量子启发方法,数据驱动学习,计算数学

<details>
  <summary>Details</summary>

Main category: math.NA

TL;DR: 该论文提出了一种基于量子启发的量化张量链（QTT）方法，能够高效且精确地求解高维偏微分方程（PDEs），并通过代表性示例展示了其对线性和非线性PDEs在内存和计算成本上的对数尺度优势。该方法结合了神经网络的适应性和数据驱动学习的新技术，提升了准确性并减少了训练时间。

Motivation: 高维PDEs的精确求解是计算数学中的核心挑战，传统数值方法和现有机器学习方法在精度和可靠性方面存在不足，特别是在工业应用中。

Method: 采用量子启发的QTT方法，结合数据驱动的学习技术，利用神经网络的适应性优化求解过程。

Result: QTT方法在多种代表性示例中表现出对数尺度的内存和计算成本优势，并在准确性上超越传统方法。

Conclusion: QTT方法为高维PDEs的求解提供了一种高效、精确且可靠的方案，尤其是在工业应用中具有潜力。

Abstract: Accurately solving high-dimensional partial differential equations (PDEs)
remains a central challenge in computational mathematics. Traditional numerical
methods, while effective in low-dimensional settings or on coarse grids, often
struggle to deliver the precision required in practical applications. Recent
machine learning-based approaches offer flexibility but frequently fall short
in terms of accuracy and reliability, particularly in industrial contexts. In
this work, we explore a quantum-inspired method based on quantized tensor
trains (QTT), enabling efficient and accurate solutions to PDEs in a variety of
challenging scenarios. Through several representative examples, we demonstrate
that the QTT approach can achieve logarithmic scaling in both memory and
computational cost for linear and nonlinear PDEs. Additionally, we introduce a
novel technique for data-driven learning within the quantum-inspired framework,
combining the adaptability of neural networks with enhanced accuracy and
reduced training time.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [396] [Assessing the generalization performance of SAM for ureteroscopy scene understanding](https://arxiv.org/abs/2505.17210)
*Martin Villagrana,Francisco Lopez-Tiro,Clement Larose,Gilberto Ochoa-Ruiz,Christian Daul*

Key words: 肾结石分割, SAM, U-Net, 深度学习, 泛化能力

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 本文研究了Segment Anything Model (SAM)在肾结石分割中的性能，发现其在未见过数据集上的泛化能力显著优于传统U-Net模型。

Motivation: 手动分割肾结石图像耗时且不实用，亟需自动化方法以提高效率。

Method: 使用SAM与传统U-Net、Residual U-Net和Attention U-Net进行对比实验。

Result: SAM在未见数据集上的表现优于U-Net变体，部分指标提升达23%。

Conclusion: SAM在肾结石分割中具有卓越的泛化能力和效率，适合大规模应用。

Abstract: The segmentation of kidney stones is regarded as a critical preliminary step
to enable the identification of urinary stone types through machine- or
deep-learning-based approaches. In urology, manual segmentation is considered
tedious and impractical due to the typically large scale of image databases and
the continuous generation of new data. In this study, the potential of the
Segment Anything Model (SAM) -- a state-of-the-art deep learning framework --
is investigated for the automation of kidney stone segmentation. The
performance of SAM is evaluated in comparison to traditional models, including
U-Net, Residual U-Net, and Attention U-Net, which, despite their efficiency,
frequently exhibit limitations in generalizing to unseen datasets. The findings
highlight SAM's superior adaptability and efficiency. While SAM achieves
comparable performance to U-Net on in-distribution data (Accuracy: 97.68 +
3.04; Dice: 97.78 + 2.47; IoU: 95.76 + 4.18), it demonstrates significantly
enhanced generalization capabilities on out-of-distribution data, surpassing
all U-Net variants by margins of up to 23 percent.

</details>


### [397] [Anatomy-Guided Multitask Learning for MRI-Based Classification of Placenta Accreta Spectrum and its Subtypes](https://arxiv.org/abs/2505.17484)
*Hai Jiang,Qiongting Liu,Yuanpin Zhou,Jiawei Pan,Ting Song,Yao Lu*

Key words: 胎盘植入谱系障碍, 多类别诊断, 卷积神经网络, 磁共振成像, 多任务学习

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 该研究提出了一种新型卷积神经网络(CNN)架构，用于基于MRI图像的一阶段多类别诊断胎盘植入谱系障碍(PAS)及其亚型，通过双分支结构和多任务学习策略显著提升了诊断性能。

Motivation: 胎盘植入谱系障碍(PAS)在妊娠期风险高，但现有方法多关注PAS存在性而忽视亚型识别，且传统方法效率低。

Method: 提出双分支CNN架构：主分支采用残差块进行分类，次分支整合解剖特征增强分类注意力，并采用多任务学习策略。

Result: 在真实临床数据集上验证，模型达到最先进性能。

Conclusion: 新型CNN架构能高效准确诊断PAS及其亚型，具有临床实用价值。

Abstract: Placenta Accreta Spectrum Disorders (PAS) pose significant risks during
pregnancy, frequently leading to postpartum hemorrhage during cesarean
deliveries and other severe clinical complications, with bleeding severity
correlating to the degree of placental invasion. Consequently, accurate
prenatal diagnosis of PAS and its subtypes-placenta accreta (PA), placenta
increta (PI), and placenta percreta (PP)-is crucial. However, existing
guidelines and methodologies predominantly focus on the presence of PAS, with
limited research addressing subtype recognition. Additionally, previous
multi-class diagnostic efforts have primarily relied on inefficient two-stage
cascaded binary classification tasks. In this study, we propose a novel
convolutional neural network (CNN) architecture designed for efficient
one-stage multiclass diagnosis of PAS and its subtypes, based on 4,140 magnetic
resonance imaging (MRI) slices. Our model features two branches: the main
classification branch utilizes a residual block architecture comprising
multiple residual blocks, while the second branch integrates anatomical
features of the uteroplacental area and the adjacent uterine serous layer to
enhance the model's attention during classification. Furthermore, we implement
a multitask learning strategy to leverage both branches effectively.
Experiments conducted on a real clinical dataset demonstrate that our model
achieves state-of-the-art performance.

</details>


### [398] [Distillation-Enabled Knowledge Alignment Protocol for Semantic Communication in AI Agent Networks](https://arxiv.org/abs/2505.17030)
*Jingzhi Hu,Geoffrey Ye Li*

Key words: 语义通信、知识对齐、低秩矩阵、整数线性规划、贪心算法

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 该论文提出了一种名为DeKAP的知识对齐协议，通过蒸馏专家知识为低秩矩阵并分配至网络，实现了多任务知识对齐，并优化了通信与存储成本。

Motivation: 未来网络中大量AI代理需协同完成多样任务，而语义通信要求知识对齐，但代理间专家知识各异，因此需解决知识对齐问题。

Method: 提出DeKAP协议，将专家知识蒸馏为参数高效的低秩矩阵并分配至网络，通过大规模整数线性规划及贪心算法优化对齐损失、通信开销和存储成本。

Result: 仿真表明，相较于传统方法，DeKAP以最低的通信和计算资源实现了知识对齐。

Conclusion: DeKAP高效解决了多代理知识对齐问题，优化了资源使用。

Abstract: Future networks are envisioned to connect massive artificial intelligence
(AI) agents, enabling their extensive collaboration on diverse tasks. Compared
to traditional entities, these agents naturally suit the semantic communication
(SC), which can significantly enhance the bandwidth efficiency. Nevertheless,
SC requires the knowledge among agents to be aligned, while agents have
distinct expert knowledge for their individual tasks in practice. In this
paper, we propose a distillation-enabled knowledge alignment protocol (DeKAP),
which distills the expert knowledge of each agent into parameter-efficient
low-rank matrices, allocates them across the network, and allows agents to
simultaneously maintain aligned knowledge for multiple tasks. We formulate the
joint minimization of alignment loss, communication overhead, and storage cost
as a large-scale integer linear programming problem and develop a highly
efficient greedy algorithm. From computer simulation, the DeKAP establishes
knowledge alignment with the lowest communication and computation resources
compared to conventional approaches.

</details>


### [399] [Dual Attention Residual U-Net for Accurate Brain Ultrasound Segmentation in IVH Detection](https://arxiv.org/abs/2505.17683)
*Dan Yuan,Yi Feng,Ziyun Tang*

Key words: 脑室出血, 早产儿, 超声图像, 注意力机制, U-Net, 分割性能

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 论文提出了一种结合CBAM和SAL注意力机制的增强型Residual U-Net架构，用于早产儿脑部超声图像中的脑室出血分割，显著提升了分割性能。

Motivation: 早产儿脑室出血（IVH）的早期准确检测对临床结果至关重要，但现有深度学习方法在捕捉局部空间细节和全局上下文依赖上仍有不足。

Method: 采用增强的Residual U-Net架构，结合CBAM（空间与通道注意力）和SAL（稀疏注意力层），通过双分支设计抑制噪声并确保信息传播。

Result: 在脑部超声数据集上，该方法实现了89.04%的Dice分数和81.84%的IoU，达到最先进的分割性能。

Conclusion: 结合空间细化与注意力稀疏性的方法能有效提升脑部解剖结构的检测鲁棒性。

Abstract: Intraventricular hemorrhage (IVH) is a severe neurological complication among
premature infants, necessitating early and accurate detection from brain
ultrasound (US) images to improve clinical outcomes. While recent deep learning
methods offer promise for computer-aided diagnosis, challenges remain in
capturing both local spatial details and global contextual dependencies
critical for segmenting brain anatomies. In this work, we propose an enhanced
Residual U-Net architecture incorporating two complementary attention
mechanisms: the Convolutional Block Attention Module (CBAM) and a Sparse
Attention Layer (SAL). The CBAM improves the model's ability to refine spatial
and channel-wise features, while the SAL introduces a dual-branch design,
sparse attention filters out low-confidence query-key pairs to suppress noise,
and dense attention ensures comprehensive information propagation. Extensive
experiments on the Brain US dataset demonstrate that our method achieves
state-of-the-art segmentation performance, with a Dice score of 89.04% and IoU
of 81.84% for ventricle region segmentation. These results highlight the
effectiveness of integrating spatial refinement and attention sparsity for
robust brain anatomy detection. Code is available at:
https://github.com/DanYuan001/BrainImgSegment.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [400] [From Weak Labels to Strong Results: Utilizing 5,000 Hours of Noisy Classroom Transcripts with Minimal Accurate Data](https://arxiv.org/abs/2505.17088)
*Ahmed Adel Attia,Dorottya Demszky,Jing Liu,Carol Espy-Wilson*

Key words: 语音识别，弱监督学习，低资源，课堂ASR，预训练

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 论文提出了一种名为弱监督预训练（WSP）的两步方法，用于解决课堂语音识别中大量弱标注数据与少量高质量标注数据共存的问题，实验表明显著优于其他方法。

Motivation: 课堂语音识别面临大量弱标注数据与少量高质量数据共存的低资源场景，高成本重新标注不现实，需寻找最佳解决方案。

Method: 采用弱监督预训练（WSP）的两步方法：先在弱标注数据上进行监督预训练，再用高质量数据微调。

Result: 基于合成和真实弱标注数据的实验表明，WSP方法显著优于其他方法。

Conclusion: WSP是一种适用于真实低资源语音识别的有效训练方法。

Abstract: Recent progress in speech recognition has relied on models trained on vast
amounts of labeled data. However, classroom Automatic Speech Recognition (ASR)
faces the real-world challenge of abundant weak transcripts paired with only a
small amount of accurate, gold-standard data. In such low-resource settings,
high transcription costs make re-transcription impractical. To address this, we
ask: what is the best approach when abundant inexpensive weak transcripts
coexist with limited gold-standard data, as is the case for classroom speech
data? We propose Weakly Supervised Pretraining (WSP), a two-step process where
models are first pretrained on weak transcripts in a supervised manner, and
then fine-tuned on accurate data. Our results, based on both synthetic and real
weak transcripts, show that WSP outperforms alternative methods, establishing
it as an effective training methodology for low-resource ASR in real-world
scenarios.

</details>


### [401] [Voicing Personas: Rewriting Persona Descriptions into Style Prompts for Controllable Text-to-Speech](https://arxiv.org/abs/2505.17093)
*Yejin Lee,Jaehoon Kang,Kyuhong Shim*

Key words: 语音风格控制，文本到语音，角色提示，韵律属性，社会偏见

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 提出了一种新框架，通过文本角色作为语音风格提示来控制基于提示的可控文本到语音系统的语音风格。

Motivation: 旨在通过文本角色提示实现对语音风格的细粒度控制，提升语音合成的自然性和一致性。

Method: 采用两种角色重写策略，将通用角色描述转化为面向语音的提示，操纵音高、情感和语速等韵律特征。

Result: 实验表明，该方法提高了合成语音的自然度、清晰度和一致性，同时分析了基于LLM重写引入的性别相关社会偏见。

Conclusion: 语音风格是角色驱动AI对话系统的关键因素，需关注其潜在偏见问题。

Abstract: In this paper, we propose a novel framework to control voice style in
prompt-based, controllable text-to-speech systems by leveraging textual
personas as voice style prompts. We present two persona rewriting strategies to
transform generic persona descriptions into speech-oriented prompts, enabling
fine-grained manipulation of prosodic attributes such as pitch, emotion, and
speaking rate. Experimental results demonstrate that our methods enhance the
naturalness, clarity, and consistency of synthesized speech. Finally, we
analyze implicit social biases introduced by LLM-based rewriting, with a focus
on gender. We underscore voice style as a crucial factor for persona-driven AI
dialogue systems.

</details>


### [402] [Speechless: Speech Instruction Training Without Speech for Low Resource Languages](https://arxiv.org/abs/2505.17417)
*Alan Dao,Dinh Bach Vu,Huy Hoang Ha,Tuan Le Duc Anh,Shreyas Gopal,Yue Heng Yeo,Warren Keng Hoong Low,Eng Siong Chng,Jia Qi Yip*

Key words: 语音助手,低资源语言,语义表示,Whisper编码器,TTS

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 提出了一种避免使用TTS的语音指令数据生成方法，通过语义表示对齐Whisper编码器，简化了低资源语言的语音助手训练。

Motivation: 语音助手的快速发展需要大量语音指令数据，但低资源语言缺乏高质量TTS模型，导致数据生成困难。

Method: 通过在语义表示层面中止合成，避免依赖TTS，将合成语义表示与Whisper编码器对齐，直接微调LLM理解语音指令。

Result: 该方法简化了训练流程，为低资源语言构建语音助手提供了可行方案。

Conclusion: 提出的方法有效解决了低资源语言语音助手开发中的语音指令数据生成问题，具有实际应用潜力。

Abstract: The rapid growth of voice assistants powered by large language models (LLM)
has highlighted a need for speech instruction data to train these systems.
Despite the abundance of speech recognition data, there is a notable scarcity
of speech instruction data, which is essential for fine-tuning models to
understand and execute spoken commands. Generating high-quality synthetic
speech requires a good text-to-speech (TTS) model, which may not be available
to low resource languages. Our novel approach addresses this challenge by
halting synthesis at the semantic representation level, bypassing the need for
TTS. We achieve this by aligning synthetic semantic representations with the
pre-trained Whisper encoder, enabling an LLM to be fine-tuned on text
instructions while maintaining the ability to understand spoken instructions
during inference. This simplified training process is a promising approach to
building voice assistant for low-resource languages.

</details>


### [403] [Source Separation of Small Classical Ensembles: Challenges and Opportunities](https://arxiv.org/abs/2505.17823)
*Gerardo Roa-Dabike,Trevor J. Cox,Jon P. Barker,Michael A. Akeroyd,Scott Bannister,Bruno Fazenda,Jennifer Firth,Simone Graetzer,Alinka Greasley,Rebecca R. Vos,William M. Whitmer*

Key words: 音乐源分离，古典音乐，ConvTasNet，因果与非因果处理，合成数据

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 该论文研究了如何利用非因果深度学习方法解决古典音乐源分离问题，通过合成木管乐器合奏数据集并测试因果与非因果ConvTasNet模型，发现合成数据与真实录音间的差异是主要挑战。

Motivation: 古典音乐的源分离比流行音乐更困难，主要由于音乐变异性大、带标注的真实录音稀缺以及乐器间的模糊性。该研究旨在改善听力障碍者的音乐聆听体验。

Method: 使用ConvTasNet模型（支持因果与非因果处理），并基于合成木管乐器合奏数据集进行训练，提取弦乐或木管乐器声音。模型在真实录音数据集（Bach10和URMP）上评估。

Result: 因果与非因果系统的性能接近（合成验证集SDR：因果6.2 dB，非因果6.9 dB；真实数据集SDR：因果0.3 dB，非因果0.4 dB），但合成数据与真实录音间存在显著差距。

Conclusion: 未来需增加真实录音训练数据或提升合成录音的真实性和多样性，以减少数据不匹配问题。

Abstract: Musical (MSS) source separation of western popular music using non-causal
deep learning can be very effective. In contrast, MSS for classical music is an
unsolved problem. Classical ensembles are harder to separate than popular music
because of issues such as the inherent greater variation in the music; the
sparsity of recordings with ground truth for supervised training; and greater
ambiguity between instruments. The Cadenza project has been exploring MSS for
classical music. This is being done so music can be remixed to improve
listening experiences for people with hearing loss. To enable the work, a new
database of synthesized woodwind ensembles was created to overcome instrumental
imbalances in the EnsembleSet. For the MSS, a set of ConvTasNet models was used
with each model being trained to extract a string or woodwind instrument.
ConvTasNet was chosen because it enabled both causal and non-causal approaches
to be tested. Non-causal approaches have dominated MSS work and are useful for
recorded music, but for live music or processing on hearing aids, causal signal
processing is needed. The MSS performance was evaluated on the two small
datasets (Bach10 and URMP) of real instrument recordings where the ground-truth
is available. The performances of the causal and non-causal systems were
similar. Comparing the average Signal-to-Distortion (SDR) of the synthesized
validation set (6.2 dB causal; 6.9 non-causal), to the real recorded evaluation
set (0.3 dB causal, 0.4 dB non-causal), shows that mismatch between synthesized
and recorded data is a problem. Future work needs to either gather more real
recordings that can be used for training, or to improve the realism and
diversity of the synthesized recordings to reduce the mismatch...

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [404] [NEXT-EVAL: Next Evaluation of Traditional and LLM Web Data Record Extraction](https://arxiv.org/abs/2505.17125)
*Soyeon Kim,Namhee Kim,Yeonwoo Jeong*

Key words: 网页数据提取, 大型语言模型, 评估框架, XPath, 数据预处理

<details>
  <summary>Details</summary>

Main category: cs.DB

TL;DR: 本文提出了一种评估框架，用于公平比较传统算法与大型语言模型在网页数据记录提取中的表现。通过生成数据集、标注XPath标签和使用结构化指标，框架确保一致性评估，并优化LLM输入格式。实验表明Flat JSON格式能显著提升提取准确性和减少幻觉。

Motivation: 由于静态、领域特定的基准和不透明的评分实践，公平比较传统算法与大型语言模型在网页数据提取中的表现存在挑战。本文旨在克服这些限制，提供一个标准化的评估框架。

Method: 框架从MHTML快照生成数据集，标注XPath监督标签，并使用结构化指标进行评分，防止文本幻觉。同时提出HTML简化、分层JSON和平坦JSON等预处理策略优化LLM输入。

Result: 实验表明，平坦JSON输入使LLM提取准确率最高（F1得分为0.9567），且幻觉最少，优于其他输入格式如简化HTML和分层JSON。

Conclusion: 该框架为网页数据记录提取的严格基准测试奠定了基础，推动了该领域的进一步发展。

Abstract: Effective evaluation of web data record extraction methods is crucial, yet
hampered by static, domain-specific benchmarks and opaque scoring practices.
This makes fair comparison between traditional algorithmic techniques, which
rely on structural heuristics, and Large Language Model (LLM)-based approaches,
offering zero-shot extraction across diverse layouts, particularly challenging.
To overcome these limitations, we introduce a concrete evaluation framework.
Our framework systematically generates evaluation datasets from arbitrary MHTML
snapshots, annotates XPath-based supervision labels, and employs
structure-aware metrics for consistent scoring, specifically preventing text
hallucination and allowing only for the assessment of positional hallucination.
It also incorporates preprocessing strategies to optimize input for LLMs while
preserving DOM semantics: HTML slimming, Hierarchical JSON, and Flat JSON.
Additionally, we created a publicly available synthetic dataset by transforming
DOM structures and modifying content. We benchmark deterministic heuristic
algorithms and off-the-shelf LLMs across these multiple input formats. Our
benchmarking shows that Flat JSON input enables LLMs to achieve superior
extraction accuracy (F1 score of 0.9567) and minimal hallucination compared to
other input formats like Slimmed HTML and Hierarchical JSON. We establish a
standardized foundation for rigorous benchmarking, paving the way for the next
principled advancements in web data record extraction.

</details>


### [405] [Managing FAIR Knowledge Graphs as Polyglot Data End Points: A Benchmark based on the rdf2pg Framework and Plant Biology Data](https://arxiv.org/abs/2505.17498)
*Marco Brandizi,Carlos Bobed,Luca Garulli,Arné de Klerk,Keywan Hassani-Pak*

Key words: Linked Data, labelled property graphs, RDF, LPG, graph databases

<details>
  <summary>Details</summary>

Main category: cs.DB

TL;DR: 论文介绍了rdf2pg框架，用于将RDF数据映射到语义等效的LPG格式，并比较了三种图数据库（Virtuoso、Neo4j、ArcadeDB）及其查询语言（SPARQL、Cypher、Gremlin）。

Motivation: 整合Linked Data和LPG的优势，共享数据集并支持软件生态系统。

Method: 提出了rdf2pg框架，用于RDF到LPG的映射，并对比分析三种图数据库及查询语言。

Result: 定性定量评估了这些图数据库技术的优缺点。

Conclusion: rdf2pg可作为多语言访问知识图谱的工具，符合Linked Data和语义网标准。

Abstract: Linked Data and labelled property graphs (LPG) are two data management
approaches with complementary strengths and weaknesses, making their
integration beneficial for sharing datasets and supporting software ecosystems.
In this paper, we introduce rdf2pg, an extensible framework for mapping RDF
data to semantically equivalent LPG formats and data-bases. Utilising this
framework, we perform a comparative analysis of three popular graph databases -
Virtuoso, Neo4j, and ArcadeDB - and the well-known graph query languages
SPARQL, Cypher, and Gremlin. Our qualitative and quantitative as-sessments
underline the strengths and limitations of these graph database technologies.
Additionally, we highlight the potential of rdf2pg as a versatile tool for
enabling polyglot access to knowledge graphs, aligning with established
standards of Linked Data and the Semantic Web.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [406] [Flexible MOF Generation with Torsion-Aware Flow Matching](https://arxiv.org/abs/2505.17914)
*Nayoung Kim,Seongsu Kim,Sungsoo Ahn*

Key words: 金属有机框架（MOF）, 深度生成模型, SMILES, 3D结构, 流匹配模型

<details>
  <summary>Details</summary>

Main category: q-bio.BM

TL;DR: 论文提出了一种两阶段框架，用于生成新型金属有机框架（MOF），解决了现有方法因依赖固定构件块和已知3D坐标而受限的问题。

Motivation: 现有深度生成模型在生成MOF时依赖于固定构件块和已知3D坐标，限制了其设计新MOF和使用新型构件块的能力。

Method: 采用两阶段框架：首先生成新型金属和有机构件块，并通过化学信息学初始化3D结构；随后通过流匹配模型预测位姿参数，组装成有效3D框架。

Result: 实验表明该方法提高了重构精度，生成了有效、新颖且独特的MOF，并能创造新型构件块。

Conclusion: 该框架成功克服了现有方法的局限性，为MOF设计提供了更强灵活性和创新性。

Abstract: Designing metal-organic frameworks (MOFs) with novel chemistries is a
long-standing challenge due to their large combinatorial space and the complex
3D arrangements of building blocks. While recent deep generative models have
enabled scalable MOF generation, they assume (1) a fixed set of building blocks
and (2) known ground-truth local block-wise 3D coordinates. However, this
limits their ability to (1) design novel MOFs and (2) generate the structure
using novel building blocks. We propose a two-stage de novo MOF generation
framework that overcomes these limitations by modeling both chemical and
geometric degrees of freedom. First, we train a SMILES-based autoregressive
model to generate novel metal and organic building blocks, paired with
cheminformatics for 3D structure initialization. Second, we introduce a
flow-matching model that predicts translations, rotations, and torsional angles
to assemble flexible blocks into valid 3D frameworks. Our experiments
demonstrate improved reconstruction accuracy, the generation of valid, novel,
and unique MOFs, and the ability of our model to create novel building blocks.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [407] [Gradient-Based Program Repair: Fixing Bugs in Continuous Program Spaces](https://arxiv.org/abs/2505.17703)
*André Silva,Gustav Thorén,Martin Monperrus*

Key words: 自动程序修复，梯度优化，数值程序空间，可微分编程，RaspBugs

<details>
  <summary>Details</summary>

Main category: cs.PL

TL;DR: 论文提出了一种基于梯度的程序修复方法（GBPR），通过将符号程序编译为可微分的数值表示，在数值程序空间进行连续优化，直接指导程序行为修复。

Motivation: 传统基于符号空间搜索的程序修复方法无法直接推理程序行为，因此论文提出GBPR，结合连续优化与程序行为的修复新范式。

Method: 将符号程序编译为可微分的数值表示，在数值程序空间中使用梯度优化进行修复，并开发了RaspBugs基准测试集验证效果。

Result: 实验证明GBPR能通过数值空间的梯度优化有效修复漏洞程序，且修复轨迹直观可信。

Conclusion: 该研究开创了程序修复的新方向，融合了连续优化与程序行为分析两大领域。

Abstract: Automatic program repair seeks to generate correct code from buggy programs,
with most approaches searching the correct program in a discrete, symbolic
space of source code tokens. This symbolic search is fundamentally limited by
its inability to directly reason about program behavior. We introduce
Gradient-Based Program Repair (GBPR), a new paradigm that reframes program
repair as continuous optimization in a differentiable numerical program space.
Our core insight is to compile symbolic programs into differentiable numerical
representations, enabling search in the numerical program space directly guided
by program behavior. To evaluate GBPR, we present RaspBugs, a new benchmark of
1,466 buggy symbolic RASP programs and their respective numerical
representations. Our experiments demonstrate that GBPR can effectively repair
buggy symbolic programs by gradient-based optimization in the numerical program
space, with convincing repair trajectories. To our knowledge, we are the first
to state program repair as continuous optimization in a numerical program
space. Our work establishes a new direction for program repair research,
bridging two rich worlds: continuous optimization and program behavior.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [408] [Transformer brain encoders explain human high-level visual responses](https://arxiv.org/abs/2505.17329)
*Hossein Adeli,Minni Sun,Nikolaus Kriegeskorte*

Key words: 视觉处理,注意力机制,动态路由,神经科学,深度学习

<details>
  <summary>Details</summary>

Main category: q-bio.NC

TL;DR: 该研究利用Transformer的注意力机制，动态地将视觉特征路由到高级视觉处理区域，显著提升了自然场景中大脑活动的预测能力，并提高了模型的可解释性。

Motivation: 传统线性编码模型忽略了大脑和模型中特征图的结构，且参数多、计算复杂。研究旨在探索更高效的视觉信息动态路由机制。

Method: 采用Transformer的注意力机制，动态分析视觉特征如何路由到高级类别选择区域。

Result: 该方法在不同特征基模型和模态下，显著优于其他方法，且无需重要性图即可直接解释注意力路由信号。

Conclusion: 该研究提出了一种视觉信息动态路由的机制模型，揭示了输入内容如何根据类别选择区域的相关性进行路由。

Abstract: A major goal of neuroscience is to understand brain computations during
visual processing in naturalistic settings. A dominant approach is to use
image-computable deep neural networks trained with different task objectives as
a basis for linear encoding models. However, in addition to requiring tuning a
large number of parameters, the linear encoding approach ignores the structure
of the feature maps both in the brain and the models. Recently proposed
alternatives have focused on decomposing the linear mapping to spatial and
feature components but focus on finding static receptive fields for units that
are applicable only in early visual areas. In this work, we employ the
attention mechanism used in the transformer architecture to study how
retinotopic visual features can be dynamically routed to category-selective
areas in high-level visual processing. We show that this computational motif is
significantly more powerful than alternative methods in predicting brain
activity during natural scene viewing, across different feature basis models
and modalities. We also show that this approach is inherently more
interpretable, without the need to create importance maps, by interpreting the
attention routing signal for different high-level categorical areas. Our
approach proposes a mechanistic model of how visual information from
retinotopic maps can be routed based on the relevance of the input content to
different category-selective regions.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [409] [Swarm Intelligence Enhanced Reasoning: A Density-Driven Framework for LLM-Based Multi-Agent Optimization](https://arxiv.org/abs/2505.17115)
*Ying Zhu,Heng Zhou,Rui Su,Peiqin Zhuang,Lei Bai*

Key words: 群体智能, 大型语言模型, 推理优化, 核密度估计, 非支配排序

<details>
  <summary>Details</summary>

Main category: cs.MA

TL;DR: 该论文提出了一种新型的基于代理的群体智能（ASI）范式，通过将大型语言模型（LLM）的推理过程形式化为优化问题，利用群体智能方案指导多个基于LLM的代理协作搜索最优解。为进一步避免陷入局部最优，作者还开发了一个群体智能增强推理（SIER）框架，通过密度驱动的策略提升推理能力。

Motivation: 现有的Chain-of-Thought (CoT) 和 Multi-Agent Debate (MAD) 方法在解决复杂问题时可能因无法找到最优解而失败。受群体智能在传统优化问题中的成功启发，作者希望将其引入推理过程以提升LLM的复杂问题解决能力。

Method: 作者提出了ASI范式，将LLM推理建模为优化问题，并利用群体智能指导代理协作搜索；进一步开发了SIER框架，通过核密度估计和非支配排序优化解的质量和多样性，同时采用步级质量评估动态控制探索终止和候选步骤选择。

Result: SIER框架通过增强解空间探索的多样性和校正低质量中间步骤，显著提升了推理过程的效率和灵活性。

Conclusion: 结合群体智能与LLM推理的ASI和SIER框架，能够有效提升复杂问题的解决能力，尤其在避免局部最优和动态优化推理路径方面表现突出。

Abstract: Recently, many approaches, such as Chain-of-Thought (CoT) prompting and
Multi-Agent Debate (MAD), have been proposed to further enrich Large Language
Models' (LLMs) complex problem-solving capacities in reasoning scenarios.
However, these methods may fail to solve complex problems due to the lack of
ability to find optimal solutions. Swarm Intelligence has been serving as a
powerful tool for finding optima in the field of traditional optimization
problems. To this end, we propose integrating swarm intelligence into the
reasoning process by introducing a novel Agent-based Swarm Intelligence (ASI)
paradigm. In this paradigm, we formulate LLM reasoning as an optimization
problem and use a swarm intelligence scheme to guide a group of LLM-based
agents in collaboratively searching for optimal solutions. To avoid swarm
intelligence getting trapped in local optima, we further develop a Swarm
Intelligence Enhancing Reasoning (SIER) framework, which develops a
density-driven strategy to enhance the reasoning ability. To be specific, we
propose to perform kernel density estimation and non-dominated sorting to
optimize both solution quality and diversity simultaneously. In this case, SIER
efficiently enhances solution space exploration through expanding the diversity
of the reasoning path. Besides, a step-level quality evaluation is used to help
agents improve solution quality by correcting low-quality intermediate steps.
Then, we use quality thresholds to dynamically control the termination of
exploration and the selection of candidate steps, enabling a more flexible and
efficient reasoning process. Extensive experiments are ...

</details>


### [410] [Multi-agent Systems for Misinformation Lifecycle : Detection, Correction And Source Identification](https://arxiv.org/abs/2505.17511)
*Aditya Gautam*

Key words: 多智能体框架, 错误信息生命周期, 透明性, 证据检索, 来源验证

<details>
  <summary>Details</summary>

Main category: cs.MA

TL;DR: 本文介绍了一种新型多智能体框架，专注于解决数字媒体中错误信息的全生命周期管理，包括分类、检测、纠正和来源验证，以提高透明度和可靠性。

Motivation: 数字媒体中错误信息的快速传播需要超越单一LLM或AI代理的解决方案。

Method: 采用五个专门代理：索引代理动态维护可信库、分类代理标记错误信息类型、提取代理基于证据检索和排序、纠正代理生成事实修正、验证代理验证输出和追踪来源可信度。

Result: 框架增强了可扩展性、模块化和可解释性，支持大规模错误信息检测和纠正。

Conclusion: 通过分解错误信息生命周期为专门代理，该框架提供了更透明、可靠的解决方案。

Abstract: The rapid proliferation of misinformation in digital media demands solutions
that go beyond isolated Large Language Model(LLM) or AI Agent based detection
methods. This paper introduces a novel multi-agent framework that covers the
complete misinformation lifecycle: classification, detection, correction, and
source verification to deliver more transparent and reliable outcomes. In
contrast to single-agent or monolithic architectures, our approach employs five
specialized agents: an Indexer agent for dynamically maintaining trusted
repositories, a Classifier agent for labeling misinformation types, an
Extractor agent for evidence based retrieval and ranking, a Corrector agent for
generating fact-based correction and a Verification agent for validating
outputs and tracking source credibility. Each agent can be individually
evaluated and optimized, ensuring scalability and adaptability as new types of
misinformation and data sources emerge. By decomposing the misinformation
lifecycle into specialized agents - our framework enhances scalability,
modularity, and explainability. This paper proposes a high-level system
overview, agent design with emphasis on transparency, evidence-based outputs,
and source provenance to support robust misinformation detection and correction
at scale.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [411] [Improving LLM Outputs Against Jailbreak Attacks with Expert Model Integration](https://arxiv.org/abs/2505.17066)
*Tatia Tsmindashvili,Ana Kolkhidashvili,Dachi Kurtskhalia,Nino Maghlakelidze,Elene Mekvabishvili,Guram Dentoshvili,Orkhan Shamilov,Zaal Gachechiladze,Steven Saporta,David Dachi Choladze*

Key words: LLM, 安全挑战, 专家模型, 领域适应, 提示注入, 汽车行业

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文介绍了Archias专家模型，用于区分领域内外通信，解决LLM在特定领域生产环境中的安全挑战如越狱和提示注入。

Motivation: LLM在生产环境中面临安全挑战，如越狱和提示注入问题，尤其在特定领域可能产生有害输出，现有方法（如微调）不足。

Method: 通过专家模型Archias分类用户问题（领域内、恶意、价格注入等），并将结果整合到LLM提示中以生成更安全的回答。

Result: Archias具有可调整性、小规模和领域适应性，能有效提升LLM的理解和回答能力，并公开了汽车行业基准数据集。

Conclusion: Archias为LLM在特定领域的安全应用提供了可定制解决方案，并通过开源数据集推动研究发展。

Abstract: Using LLMs in a production environment presents security challenges that
include vulnerabilities to jailbreaks and prompt injections, which can result
in harmful outputs for humans or the enterprise. The challenge is amplified
when working within a specific domain, as topics generally accepted for LLMs to
address may be irrelevant to that field. These problems can be mitigated, for
example, by fine-tuning large language models with domain-specific and
security-focused data. However, these alone are insufficient, as jailbreak
techniques evolve. Additionally, API-accessed models do not offer the
flexibility needed to tailor behavior to industry-specific objectives, and
in-context learning is not always sufficient or reliable. In response to these
challenges, we introduce Archias, an expert model adept at distinguishing
between in-domain and out-of-domain communications. Archias classifies user
inquiries into several categories: in-domain (specifically for the automotive
industry), malicious questions, price injections, prompt injections, and
out-of-domain examples. Our methodology integrates outputs from the expert
model (Archias) into prompts, which are then processed by the LLM to generate
responses. This method increases the model's ability to understand the user's
intention and give appropriate answers. Archias can be adjusted, fine-tuned,
and used for many different purposes due to its small size. Therefore, it can
be easily customized to the needs of any industry. To validate our approach, we
created a benchmark dataset for the automotive industry. Furthermore, in the
interest of advancing research and development, we release our benchmark
dataset to the community.

</details>


### [412] [Safety Alignment Can Be Not Superficial With Explicit Safety Signals](https://arxiv.org/abs/2505.17072)
*Jianwei Li,Jung-Eng Kim*

Key words: 大语言模型, 安全对齐, 对抗攻击, 二元分类, 注意力机制

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 该论文提出了一种新方法，通过明确引入安全相关的二元分类任务并结合注意力与解码策略，显著提升了LLMs对抗对抗攻击的韧性，成本增加不到0.2倍。

Motivation: 现有的大语言模型安全对齐方法往往停留在表面，易受对抗攻击，且缺乏可行的解决方案。论文发现根本原因是现有方法假设模型能隐式学习安全相关推理任务，但实际中安全信号被其他目标稀释。

Method: 通过显式引入安全相关的二元分类任务，并结合注意力与解码策略，消除安全决策的模糊性。

Result: 实验表明，该方法显著提升了LLMs对抗对抗攻击的韧性，且成本增加小于0.2倍。

Conclusion: 该方法为构建更鲁棒的生成式AI系统提供了可行路径。

Abstract: Recent studies on the safety alignment of large language models (LLMs) have
revealed that existing approaches often operate superficially, leaving models
vulnerable to various adversarial attacks. Despite their significance, these
studies generally fail to offer actionable solutions beyond data augmentation
for achieving more robust safety mechanisms. This paper identifies a
fundamental cause of this superficiality: existing alignment approaches often
presume that models can implicitly learn a safety-related reasoning task during
the alignment process, enabling them to refuse harmful requests. However, the
learned safety signals are often diluted by other competing objectives, leading
models to struggle with drawing a firm safety-conscious decision boundary when
confronted with adversarial attacks. Based on this observation, by explicitly
introducing a safety-related binary classification task and integrating its
signals with our attention and decoding strategies, we eliminate this ambiguity
and allow models to respond more responsibly to malicious queries. We emphasize
that, with less than 0.2x overhead cost, our approach enables LLMs to assess
the safety of both the query and the previously generated tokens at each
necessary generating step. Extensive experiments demonstrate that our method
significantly improves the resilience of LLMs against various adversarial
attacks, offering a promising pathway toward more robust generative AI systems.

</details>


### [413] [From nuclear safety to LLM security: Applying non-probabilistic risk management strategies to build safe and secure LLM-powered systems](https://arxiv.org/abs/2505.17084)
*Alexander Gutfraind,Vicki Bier*

Key words: 大型语言模型（LLM），AI安全，风险管理，非概率性策略，对抗性攻击

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文摘要讨论了大型语言模型（LLM）带来的安全和安全挑战，并提出了100多种非概率性风险应对策略，分为五类，适用于LLM安全和AI安全领域。

Motivation: 大型语言模型的能力不断增强，但传统概率风险分析（PRA）在面对其新颖性和复杂性时显得不足，尤其是在对抗性攻击的情况下。

Method: 研究提出了100多种非概率性风险管理策略，分为五类，并展示了如何将这些策略应用于LLM安全和AI安全领域。还包括一个LLM驱动的工作流程和其他适用的解决方案架构师工作流程。

Result: 这些策略可以提升LLM系统的安全性、安全性以及其他负责任AI的维度，尽管存在一些局限性。

Conclusion: 研究表明，非概率性策略可以有效应对LLM系统的安全挑战，尽管需要进一步研究和改进。

Abstract: Large language models (LLMs) offer unprecedented and growing capabilities,
but also introduce complex safety and security challenges that resist
conventional risk management. While conventional probabilistic risk analysis
(PRA) requires exhaustive risk enumeration and quantification, the novelty and
complexity of these systems make PRA impractical, particularly against adaptive
adversaries. Previous research found that risk management in various fields of
engineering such as nuclear or civil engineering is often solved by generic
(i.e. field-agnostic) strategies such as event tree analysis or robust designs.
Here we show how emerging risks in LLM-powered systems could be met with 100+
of these non-probabilistic strategies to risk management, including risks from
adaptive adversaries. The strategies are divided into five categories and are
mapped to LLM security (and AI safety more broadly). We also present an
LLM-powered workflow for applying these strategies and other workflows suitable
for solution architects. Overall, these strategies could contribute (despite
some limitations) to security, safety and other dimensions of responsible AI.

</details>


### [414] [GSDFuse: Capturing Cognitive Inconsistencies from Multi-Dimensional Weak Signals in Social Media Steganalysis](https://arxiv.org/abs/2505.17085)
*Kaibo Huang,Zipei Zhang,Yukun Wei,TianXin Zhang,Zhongliang Yang,Linna Zhou*

Key words: 社交媒体、语言隐写术、多模态特征、数据增强、自适应融合

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: GSDFuse是一种新型方法，通过多层次多模态特征工程、数据增强、自适应证据融合和判别嵌入学习，有效解决了社交媒体恶意语言隐写术检测中的挑战，实验表明其在复杂对话环境中表现卓越。

Motivation: 解决社交媒体语言隐写术检测中的认知不一致、文本碎片化及数据稀疏性等核心难题。

Method: 采用多层次多模态特征工程、策略性数据增强、自适应证据融合和判别嵌入学习。

Result: 在社交媒体数据集上实现了最先进的性能表现。

Conclusion: GSDFuse显著提升了复杂对话环境中恶意语言隐写术的检测能力。

Abstract: The ubiquity of social media platforms facilitates malicious linguistic
steganography, posing significant security risks. Steganalysis is profoundly
hindered by the challenge of identifying subtle cognitive inconsistencies
arising from textual fragmentation and complex dialogue structures, and the
difficulty in achieving robust aggregation of multi-dimensional weak signals,
especially given extreme steganographic sparsity and sophisticated
steganography. These core detection difficulties are compounded by significant
data imbalance. This paper introduces GSDFuse, a novel method designed to
systematically overcome these obstacles. GSDFuse employs a holistic approach,
synergistically integrating hierarchical multi-modal feature engineering to
capture diverse signals, strategic data augmentation to address sparsity,
adaptive evidence fusion to intelligently aggregate weak signals, and
discriminative embedding learning to enhance sensitivity to subtle
inconsistencies. Experiments on social media datasets demonstrate GSDFuse's
state-of-the-art (SOTA) performance in identifying sophisticated steganography
within complex dialogue environments. The source code for GSDFuse is available
at https://github.com/NebulaEmmaZh/GSDFuse.

</details>


### [415] [CRAKEN: Cybersecurity LLM Agent with Knowledge-Based Execution](https://arxiv.org/abs/2505.17107)
*Minghao Shao,Haoran Xi,Nanda Rani,Meet Udeshi,Venkata Sai Charan Putrevu,Kimberly Milner,Brendan Dolan-Gavitt,Sandeep Kumar Shukla,Prashanth Krishnamurthy,Farshad Khorrami,Ramesh Karri,Muhammad Shafique*

Key words: LLM, 网络安全, 知识框架, CRAKEN, CTF

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 该论文提出了CRAKEN框架，一个基于知识的LLM代理系统，通过分解任务信息、知识检索和注入策略提升了网络安全任务的自动化能力，并在多项测试中优于现有方法。

Motivation: 现有LLM代理在网络安全任务中存在无法获取最新知识及将新知识融入复杂任务规划的局限性。

Method: CRAKEN框架采用上下文任务分解、迭代自反知识检索和知识提示注入三种机制。

Result: 在NYU CTF Bench上准确率达到22%，优于之前方法3%；在MITRE ATT&CK测试中解决技术数量增加25-30%。

Conclusion: CRAKEN通过知识嵌入显著提升了LLM驱动的网络安全代理能力，并开源了框架。

Abstract: Large Language Model (LLM) agents can automate cybersecurity tasks and can
adapt to the evolving cybersecurity landscape without re-engineering. While LLM
agents have demonstrated cybersecurity capabilities on Capture-The-Flag (CTF)
competitions, they have two key limitations: accessing latest cybersecurity
expertise beyond training data, and integrating new knowledge into complex task
planning. Knowledge-based approaches that incorporate technical understanding
into the task-solving automation can tackle these limitations. We present
CRAKEN, a knowledge-based LLM agent framework that improves cybersecurity
capability through three core mechanisms: contextual decomposition of
task-critical information, iterative self-reflected knowledge retrieval, and
knowledge-hint injection that transforms insights into adaptive attack
strategies. Comprehensive evaluations with different configurations show
CRAKEN's effectiveness in multi-stage vulnerability detection and exploitation
compared to previous approaches. Our extensible architecture establishes new
methodologies for embedding new security knowledge into LLM-driven
cybersecurity agentic systems. With a knowledge database of CTF writeups,
CRAKEN obtained an accuracy of 22% on NYU CTF Bench, outperforming prior works
by 3% and achieving state-of-the-art results. On evaluation of MITRE ATT&CK
techniques, CRAKEN solves 25-30% more techniques than prior work, demonstrating
improved cybersecurity capabilities via knowledge-based execution. We make our
framework open source to public
https://github.com/NYU-LLM-CTF/nyuctf_agents_craken.

</details>


### [416] [LLM Access Shield: Domain-Specific LLM Framework for Privacy Policy Compliance](https://arxiv.org/abs/2505.17145)
*Yu Wang,Cailing Cai,Zhihua Xiao,Peifung E. Lam*

Key words: 大语言模型、数据隐私、安全框架、动态策略、加密技术

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 本文提出了一种针对大语言模型（LLMs）的安全框架，通过策略执行和实时调整来保护敏感数据并降低隐私风险。

Motivation: 随着大语言模型在各领域的广泛应用，数据隐私和安全问题日益突出，急需一种机制来防止敏感信息泄露。

Method: 框架包含三个创新点：（i）基于LLM的策略执行机制；（ii）动态策略定制；（iii）格式保留加密技术。

Result: 实验证明，该框架在保留LLM功能准确性的同时，有效降低了安全风险。

Conclusion: 该安全框架为LLM的实际应用提供了可扩展且高效的隐私保护方案。

Abstract: Large language models (LLMs) are increasingly applied in fields such as
finance, education, and governance due to their ability to generate human-like
text and adapt to specialized tasks. However, their widespread adoption raises
critical concerns about data privacy and security, including the risk of
sensitive data exposure.
  In this paper, we propose a security framework to enforce policy compliance
and mitigate risks in LLM interactions. Our approach introduces three key
innovations: (i) LLM-based policy enforcement: a customizable mechanism that
enhances domain-specific detection of sensitive data. (ii) Dynamic policy
customization: real-time policy adaptation and enforcement during user-LLM
interactions to ensure compliance with evolving security requirements. (iii)
Sensitive data anonymization: a format-preserving encryption technique that
protects sensitive information while maintaining contextual integrity.
Experimental results demonstrate that our framework effectively mitigates
security risks while preserving the functional accuracy of LLM-driven tasks.

</details>


### [417] [MTSA: Multi-turn Safety Alignment for LLMs through Multi-round Red-teaming](https://arxiv.org/abs/2505.17147)
*Weiyang Guo,Jing Li,Wenya Wang,YU LI,Daojing He,Jun Yu,Min Zhang*

Key words: 大语言模型、多轮对话、安全性、对抗优化、强化学习

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文提出Multi-Turn Safety Alignment (MTSA)框架，通过两阶段方法（思想引导攻击学习和对抗迭代优化）提升大语言模型在多轮对话中的安全性。

Motivation: 由于大语言模型在多轮对话中易受隐蔽恶意意图影响，产生有害回应，因此需要增强其安全性。

Method: 提出MTSA框架，分为思想引导攻击学习和对抗迭代优化两阶段，并引入基于未来奖励的多轮强化学习算法。

Result: 实验表明，红队模型展现出顶尖攻击能力，而目标模型在安全基准上性能显著提升。

Conclusion: MTSA框架有效提升了大语言模型在多轮互动中的安全性和鲁棒性。

Abstract: The proliferation of jailbreak attacks against large language models (LLMs)
highlights the need for robust security measures. However, in multi-round
dialogues, malicious intentions may be hidden in interactions, leading LLMs to
be more prone to produce harmful responses. In this paper, we propose the
\textbf{M}ulti-\textbf{T}urn \textbf{S}afety \textbf{A}lignment (\ourapproach)
framework, to address the challenge of securing LLMs in multi-round
interactions. It consists of two stages: In the thought-guided attack learning
stage, the red-team model learns about thought-guided multi-round jailbreak
attacks to generate adversarial prompts. In the adversarial iterative
optimization stage, the red-team model and the target model continuously
improve their respective capabilities in interaction. Furthermore, we introduce
a multi-turn reinforcement learning algorithm based on future rewards to
enhance the robustness of safety alignment. Experimental results show that the
red-team model exhibits state-of-the-art attack capabilities, while the target
model significantly improves its performance on safety benchmarks.

</details>


### [418] [JALMBench: Benchmarking Jailbreak Vulnerabilities in Audio Language Models](https://arxiv.org/abs/2505.17568)
*Zifan Peng,Yule Liu,Zhen Sun,Mingchen Li,Zeren Luo,Jingyi Zheng,Wenhan Dong,Xinlei He,Xuechao Wang,Yingjie Xue,Shengmin Xu,Xinyi Huang*

Key words: 音频语言模型, 越狱攻击, 基准测试, 安全性评估, 防御策略

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文介绍了JALMBench，首个针对音频语言模型（ALMs）安全性的综合基准测试，包括数据集和多种攻击与防御方法。

Motivation: 目前ALMs的安全性研究不足，缺乏统一的评估框架和数据集，尤其是针对音频模态的越狱攻击。

Method: 提出了JALMBench基准测试，包括数据集（2,200文本和51,381音频样本）、12种ALMs模型、8种攻击方法和5种防御方法。

Result: 深入分析了攻击效率、主题敏感性、声音多样性和攻击表示，并探讨了提示和响应层面的缓解策略。

Conclusion: JALMBench为ALMs安全性提供了首个综合评估工具，填补了研究空白。

Abstract: Audio Language Models (ALMs) have made significant progress recently. These
models integrate the audio modality directly into the model, rather than
converting speech into text and inputting text to Large Language Models (LLMs).
While jailbreak attacks on LLMs have been extensively studied, the security of
ALMs with audio modalities remains largely unexplored. Currently, there is a
lack of an adversarial audio dataset and a unified framework specifically
designed to evaluate and compare attacks and ALMs. In this paper, we present
JALMBench, the \textit{first} comprehensive benchmark to assess the safety of
ALMs against jailbreak attacks. JALMBench includes a dataset containing 2,200
text samples and 51,381 audio samples with over 268 hours. It supports 12
mainstream ALMs, 4 text-transferred and 4 audio-originated attack methods, and
5 defense methods. Using JALMBench, we provide an in-depth analysis of attack
efficiency, topic sensitivity, voice diversity, and attack representations.
Additionally, we explore mitigation strategies for the attacks at both the
prompt level and the response level.

</details>


### [419] [\texttt{Range-Arithmetic}: Verifiable Deep Learning Inference on an Untrusted Party](https://arxiv.org/abs/2505.17623)
*Ali Rahimi,Babak H. Khalaj,Mohammad Ali Maddah-Ali*

Key words: 可验证计算、去中心化机器学习、DNN推理、范围证明、求和检查协议

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 提出了一种名为Range-Arithmetic的新框架，用于高效且可验证的DNN推理，通过将非算术操作转换为算术步骤来简化验证过程。

Motivation: 在去中心化机器学习系统中，由于区块链的限制，计算密集型任务如DNN推理被外包，因此需要在不重新执行的情况下验证外包计算的正确性。

Method: 将非算术操作（如定点矩阵乘法和ReLU后的取整）转换为可验证的算术步骤，利用求和检查协议和连接的范围证明。

Result: 实验结果表明，该方法不仅与现有方法性能相当，还降低了验证结果的计算成本、不信任方的计算负担以及双方的通信开销。

Conclusion: Range-Arithmetic框架在保持高效的同时简化了验证过程，适用于有限域基础的证明系统。

Abstract: Verifiable computing (VC) has gained prominence in decentralized machine
learning systems, where resource-intensive tasks like deep neural network (DNN)
inference are offloaded to external participants due to blockchain limitations.
This creates a need to verify the correctness of outsourced computations
without re-execution. We propose \texttt{Range-Arithmetic}, a novel framework
for efficient and verifiable DNN inference that transforms non-arithmetic
operations, such as rounding after fixed-point matrix multiplication and ReLU,
into arithmetic steps verifiable using sum-check protocols and concatenated
range proofs. Our approach avoids the complexity of Boolean encoding,
high-degree polynomials, and large lookup tables while remaining compatible
with finite-field-based proof systems. Experimental results show that our
method not only matches the performance of existing approaches, but also
reduces the computational cost of verifying the results, the computational
effort required from the untrusted party performing the DNN inference, and the
communication overhead between the two sides.

</details>


### [420] [Mitigating Cyber Risk in the Age of Open-Weight LLMs: Policy Gaps and Technical Realities](https://arxiv.org/abs/2505.17109)
*Alfonso de Gregorio*

Key words: 通用人工智能,网络安全,开放权重,监管缺口,能力管控

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 开放权重的通用人工智能（GPAI）模型带来巨大好处，但同时也增加了网络安全风险。本文分析了这类模型放大的具体威胁（如加速恶意软件开发和社会工程攻击），评估了当前监管的不足（如《欧盟AI法案》），并提出了以能力为中心的风险管控方案。

Motivation: 开放权重的GPAI模型虽推动技术民主化，但其失控传播会导致传统安全防御和监管失效，亟需针对性的风险控制策略。

Method: 通过分析开源GPAI模型的攻击案例（如DeepSeek-R1），评估现有法规漏洞，并提出能力导向的监管框架和国际合作路径。

Result: 当前法规对开放模型的风险应对不足，需转向聚焦高风险能力、推动防御性AI创新及跨国CTI共享。

Conclusion: 在保障安全的同时促进开放技术发展，需平衡监管精准性与技术创新空间，强调国际合作的重要性。

Abstract: Open-weight general-purpose AI (GPAI) models offer significant benefits but
also introduce substantial cybersecurity risks, as demonstrated by the
offensive capabilities of models like DeepSeek-R1 in evaluations such as
MITRE's OCCULT. These publicly available models empower a wider range of actors
to automate and scale cyberattacks, challenging traditional defence paradigms
and regulatory approaches. This paper analyzes the specific threats --
including accelerated malware development and enhanced social engineering --
magnified by open-weight AI release. We critically assess current regulations,
notably the EU AI Act and the GPAI Code of Practice, identifying significant
gaps stemming from the loss of control inherent in open distribution, which
renders many standard security mitigations ineffective. We propose a path
forward focusing on evaluating and controlling specific high-risk capabilities
rather than entire models, advocating for pragmatic policy interpretations for
open-weight systems, promoting defensive AI innovation, and fostering
international collaboration on standards and cyber threat intelligence (CTI)
sharing to ensure security without unduly stifling open technological progress.

</details>


### [421] [Streamlining HTTP Flooding Attack Detection through Incremental Feature Selection](https://arxiv.org/abs/2505.17077)
*Upasana Sarmah,Parthajit Borah,D. K. Bhattacharyya*

Key words: HTTP洪水攻击, 特征选择, 互信息, 相关性分析, 实时检测

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文提出了一种基于互信息和相关性的增量特征子集选择方法（INFS-MICC），用于实时检测HTTP洪水攻击。

Motivation: 由于HTTP协议的特性，攻击者常利用其无默认检测阻拦的特点发起针对网络应用的HTTP洪水攻击，因此需高效检测方法。

Method: 采用增量特征子集选择方法（INFS-MICC），结合互信息和相关性分析，筛选高相关且独立的特征子集。

Result: INFS-MICC能有效识别HTTP洪水攻击，并在近实时条件下实现最佳分类性能。

Conclusion: 该方法为HTTP洪水攻击检测提供了一种高效且实时的解决方案。

Abstract: Applications over the Web primarily rely on the HTTP protocol to transmit web
pages to and from systems. There are a variety of application layer protocols,
but among all, HTTP is the most targeted because of its versatility and ease of
integration with online services. The attackers leverage the fact that by
default no detection system blocks any HTTP traffic. Thus, by exploiting such
characteristics of the protocol, attacks are launched against web applications.
HTTP flooding attacks are one such attack in the application layer of the OSI
model. In this paper, a method for the detection of such an attack is proposed.
The heart of the detection method is an incremental feature subset selection
method based on mutual information and correlation. INFS-MICC helps in
identifying a subset of highly relevant and independent feature subset so as to
detect HTTP Flooding attacks with best possible classification performance in
near-real time.

</details>


### [422] [Covert Attacks on Machine Learning Training in Passively Secure MPC](https://arxiv.org/abs/2505.17092)
*Matthew Jagielski,Daniel Escudero,Rahul Rachuri,Peter Scholl*

Key words: 安全多方计算（MPC）、隐私保护机器学习（PPML）、主动攻击、被动安全、模型完整性、数据隐私

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 该论文展示了主动攻击者对现有被动安全多方计算（MPC）训练协议的有效攻击，证明这些攻击能破坏模型完整性和隐私性，甚至能重构训练数据，从而挑战了PPML领域中仅考虑被动威胁模型的合理性，并呼吁采用主动安全协议。

Motivation: 研究动机在于揭示在保护隐私的机器学习（PPML）中，仅考虑被动攻击者而忽视主动攻击行为的威胁模型可能不足，因主动攻击者能通过简单有效的手段破坏模型而不被检测。

Method: 论文通过构造具体的主动攻击方式，展示了攻击者如何在现有被动安全MPC训练协议中实施攻击，且几乎不会被发现。

Result: 结果显示，主动攻击能严重破坏模型完整性和用户隐私，甚至完全重构训练数据。

Conclusion: 结论认为PPML领域必须重视主动攻击威胁，建议采用主动安全协议以确保训练过程的安全性。

Abstract: Secure multiparty computation (MPC) allows data owners to train machine
learning models on combined data while keeping the underlying training data
private. The MPC threat model either considers an adversary who passively
corrupts some parties without affecting their overall behavior, or an adversary
who actively modifies the behavior of corrupt parties. It has been argued that
in some settings, active security is not a major concern, partly because of the
potential risk of reputation loss if a party is detected cheating.
  In this work we show explicit, simple, and effective attacks that an active
adversary can run on existing passively secure MPC training protocols, while
keeping essentially zero risk of the attack being detected. The attacks we show
can compromise both the integrity and privacy of the model, including attacks
reconstructing exact training data. Our results challenge the belief that a
threat model that does not include malicious behavior by the involved parties
may be reasonable in the context of PPML, motivating the use of actively secure
protocols for training.

</details>


### [423] [Neuromorphic Mimicry Attacks Exploiting Brain-Inspired Computing for Covert Cyber Intrusions](https://arxiv.org/abs/2505.17094)
*Hemanth Ravipati*

Key words: 神经模拟攻击、神经形态计算、网络安全、入侵检测、突触学习协议

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 该论文提出了一种名为神经模拟攻击（NMAs）的新型威胁，利用神经形态芯片的随机性执行隐蔽入侵，并提出了相应的防御措施。

Motivation: 神经形态计算因其低功耗和自适应特性在AI和边缘计算中具有潜力，但其独特的架构也带来了新的网络安全风险，需要进行针对性研究。

Method: 通过模拟神经形态芯片数据集，评估NMAs的影响，并提出神经网络特定的异常检测和安全突触学习协议作为防御措施。

Result: 研究结果表明，NMAs能够绕过传统入侵检测系统，对自动驾驶、智能医疗植入和物联网等应用构成威胁。

Conclusion: 需要针对神经形态计算的特性开发专门的网络安全措施，以应对这一新兴威胁。

Abstract: Neuromorphic computing, inspired by the human brain's neural architecture, is
revolutionizing artificial intelligence and edge computing with its low-power,
adaptive, and event-driven designs. However, these unique characteristics
introduce novel cybersecurity risks. This paper proposes Neuromorphic Mimicry
Attacks (NMAs), a groundbreaking class of threats that exploit the
probabilistic and non-deterministic nature of neuromorphic chips to execute
covert intrusions. By mimicking legitimate neural activity through techniques
such as synaptic weight tampering and sensory input poisoning, NMAs evade
traditional intrusion detection systems, posing risks to applications such as
autonomous vehicles, smart medical implants, and IoT networks. This research
develops a theoretical framework for NMAs, evaluates their impact using a
simulated neuromorphic chip dataset, and proposes countermeasures, including
neural-specific anomaly detection and secure synaptic learning protocols. The
findings underscore the critical need for tailored cybersecurity measures to
protect brain-inspired computing, offering a pioneering exploration of this
emerging threat landscape.

</details>


### [424] [Chain-of-Lure: A Synthetic Narrative-Driven Approach to Compromise Large Language Models](https://arxiv.org/abs/2505.17519)
*Wenhan Chang,Tianqing Zhu,Yu Zhao,Shuangyong Song,Ping Xiong,Wanlei Zhou,Yongxiang Li*

Key words: AI安全,大型语言模型,越狱攻击,思维链机制,叙事优化

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 该论文提出了一种基于思维链机制的新型越狱方法，通过任务转移和优化叙事诱饵，成功使受害者模型生成有害内容，并发现安全机制较弱的模型攻击能力更强。

Motivation: 在生成式AI快速发展的时代，人与大型语言模型的交互存在滥用风险。现有研究忽视了模型不仅可能成为受害者，也可能成为攻击者的可能性。

Method: 采用思维链机制，攻击模型通过任务转移隐藏有害意图，生成链式叙事诱饵激发受害者模型的推理能力，并结合辅助模型进行随机叙事优化以提高攻击成功率。

Result: 实验表明，安全机制较弱的模型攻击能力更强，毒性评分能更精确评估攻击效果，传统拒绝关键词作为评估指标存在显著缺陷。

Conclusion: 该方法揭示了大型语言模型的潜在漏洞，为优化其安全机制提供了数据驱动的反馈，并讨论了两种防御策略。

Abstract: In the era of rapid generative AI development, interactions between humans
and large language models face significant misusing risks. Previous research
has primarily focused on black-box scenarios using human-guided prompts and
white-box scenarios leveraging gradient-based LLM generation methods,
neglecting the possibility that LLMs can act not only as victim models, but
also as attacker models to harm other models. We proposes a novel jailbreaking
method inspired by the Chain-of-Thought mechanism, where the attacker model
uses mission transfer to conceal harmful user intent in dialogue and generates
chained narrative lures to stimulate the reasoning capabilities of victim
models, leading to successful jailbreaking. To enhance the attack success rate,
we introduce a helper model that performs random narrative optimization on the
narrative lures during multi-turn dialogues while ensuring alignment with the
original intent, enabling the optimized lures to bypass the safety barriers of
victim models effectively. Our experiments reveal that models with weaker
safety mechanisms exhibit stronger attack capabilities, demonstrating that
models can not only be exploited, but also help harm others. By incorporating
toxicity scores, we employ third-party models to evaluate the harmfulness of
victim models' responses to jailbreaking attempts. The study shows that using
refusal keywords as an evaluation metric for attack success rates is
significantly flawed because it does not assess whether the responses guide
harmful questions, while toxicity scores measure the harm of generated content
with more precision and its alignment with harmful questions. Our approach
demonstrates outstanding performance, uncovering latent vulnerabilities in LLMs
and providing data-driven feedback to optimize LLM safety mechanisms. We also
discuss two defensive strategies to offer guidance on improving defense
mechanisms.

</details>


### [425] [One Model Transfer to All: On Robust Jailbreak Prompts Generation against LLMs](https://arxiv.org/abs/2505.17598)
*Linbao Li,Yannan Liu,Daojing He,Yu Li*

Key words: LLMs, jailbreak attacks, defense mechanisms, ArrAttack

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: ArrAttack是一种新型的越狱攻击方法，专门针对防御性LLMs，能自动生成强大的越狱提示并绕过多种防御措施。

Motivation: 现有越狱策略难以应对快速发展的防御机制，因此需要一种更强大的攻击方法来揭示模型漏洞。

Method: 基于通用鲁棒性判断模型，快速生成鲁棒越狱提示。

Result: ArrAttack在攻击效果上显著优于现有策略，展现了在白盒和黑盒模型（如GPT-4、Claude-3）上的强迁移性。

Conclusion: 该工作填补了越狱攻击与防御之间的差距，为生成鲁棒越狱提示提供了新视角。

Abstract: Safety alignment in large language models (LLMs) is increasingly compromised
by jailbreak attacks, which can manipulate these models to generate harmful or
unintended content. Investigating these attacks is crucial for uncovering model
vulnerabilities. However, many existing jailbreak strategies fail to keep pace
with the rapid development of defense mechanisms, such as defensive suffixes,
rendering them ineffective against defended models. To tackle this issue, we
introduce a novel attack method called ArrAttack, specifically designed to
target defended LLMs. ArrAttack automatically generates robust jailbreak
prompts capable of bypassing various defense measures. This capability is
supported by a universal robustness judgment model that, once trained, can
perform robustness evaluation for any target model with a wide variety of
defenses. By leveraging this model, we can rapidly develop a robust jailbreak
prompt generator that efficiently converts malicious input prompts into
effective attacks. Extensive evaluations reveal that ArrAttack significantly
outperforms existing attack strategies, demonstrating strong transferability
across both white-box and black-box models, including GPT-4 and Claude-3. Our
work bridges the gap between jailbreak attacks and defenses, providing a fresh
perspective on generating robust jailbreak prompts. We make the codebase
available at https://github.com/LLBao/ArrAttack.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [426] [Can Large Language Models Design Biological Weapons? Evaluating Moremi Bio](https://arxiv.org/abs/2505.17154)
*Gertrude Hattoh,Jeremiah Ayensu,Nyarko Prince Ofori,Solomon Eshun,Darlington Akogo*

Key words: AI, LLMs, 药物发现, 毒性设计, 生物安全, 双用途风险

<details>
  <summary>Details</summary>

Main category: q-bio.QM

TL;DR: AI和LLMs的应用显著缩短了药物发现周期，但同时也引发了双用途问题，通过无安全约束的设计生成了大量毒性蛋白和小分子，揭示了现有技术的生物安全风险。

Motivation: 研究动机在于评估LLMs在生物设计中的潜在双用途风险，尤其是设计毒性物质的能力及其对生物安全的威胁。

Method: 通过MoremiBio Agent无安全约束生成1020种毒性蛋白和5000种毒性小分子，并进行计算毒性评估和定量风险分析。

Result: 生成的毒性物质在计算毒性评估中表现出高毒性，部分与已知毒素（如蓖麻毒素、蛇毒蛋白）高度相似，凸显了LLMs的生物武器设计能力。

Conclusion: 研究强调了LLMs技术被滥用的风险，需采取多层次缓解策略和严格监管以平衡生物技术创新与生物安全。

Abstract: Advances in AI, particularly LLMs, have dramatically shortened drug discovery
cycles by up to 40% and improved molecular target identification. However,
these innovations also raise dual-use concerns by enabling the design of toxic
compounds. Prompting Moremi Bio Agent without the safety guardrails to
specifically design novel toxic substances, our study generated 1020 novel
toxic proteins and 5,000 toxic small molecules. In-depth computational toxicity
assessments revealed that all the proteins scored high in toxicity, with
several closely matching known toxins such as ricin, diphtheria toxin, and
disintegrin-based snake venom proteins. Some of these novel agents showed
similarities with other several known toxic agents including disintegrin
eristostatin, metalloproteinase, disintegrin triflavin, snake venom
metalloproteinase, corynebacterium ulcerans toxin. Through quantitative risk
assessments and scenario analyses, we identify dual-use capabilities in current
LLM-enabled biodesign pipelines and propose multi-layered mitigation
strategies. The findings from this toxicity assessment challenge claims that
large language models (LLMs) are incapable of designing bioweapons. This
reinforces concerns about the potential misuse of LLMs in biodesign, posing a
significant threat to research and development (R&D). The accessibility of such
technology to individuals with limited technical expertise raises serious
biosecurity risks. Our findings underscore the critical need for robust
governance and technical safeguards to balance rapid biotechnological
innovation with biosecurity imperatives.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [427] [Corporate Needs You to Find the Difference: Revisiting Submodular and Supermodular Ratio Optimization Problems](https://arxiv.org/abs/2505.17443)
*Elfarouk Harb,Yousef Yassin,Chandra Chekuri*

Key words: 子模函数，超模函数，优化，MNP问题，强多项式时间

<details>
  <summary>Details</summary>

Main category: cs.DS

TL;DR: 该论文探讨了如何最小化或最大化子模或超模集函数的平均值，引入了USSS和UDSS两种新问题，并证明DSS、SFM、USSS、UDSS及MNP问题在强多项式时间下等价。理论证明了SuperGreedy++算法的广泛适用性，实验验证了通用优化方法的优越性。

Motivation: 研究子模或超模集函数平均值的优化问题，旨在推广经典问题如DSG、DSS和SFM，并解决新应用中的问题。

Method: 通过等价性分析，将DSS、SFM等统一至MNP问题框架，利用Fujishige-Wolfe算法和SuperGreedy++等通用方法求解。

Result: 理论证明了算法的普适性，实验显示通用优化方法在多种任务上优于专用方法。

Conclusion: 通用优化技术在子模和超模比例问题上既高效又先进，统一框架具有理论和实践价值。

Abstract: We study the problem of minimizing or maximizing the average value $ f(S)/|S|
$ of a submodular or supermodular set function $ f: 2^V \to \mathbb{R} $ over
non-empty subsets $ S \subseteq V $. This generalizes classical problems such
as Densest Subgraph (DSG), Densest Supermodular Set (DSS), and Submodular
Function Minimization (SFM). Motivated by recent applications, we introduce two
broad formulations: Unrestricted Sparsest Submodular Set (USSS) and
Unrestricted Densest Supermodular Set (UDSS), which allow for negative and
non-monotone functions.
  We show that DSS, SFM, USSS, UDSS, and the Minimum Norm Point (MNP) problem
are equivalent under strongly polynomial-time reductions, enabling algorithmic
crossover. In particular, viewing these through the lens of the MNP in the base
polyhedron, we connect Fujishige's theory with dense decomposition, and show
that both Fujishige-Wolfe's algorithm and the heuristic \textsc{SuperGreedy++}
act as universal solvers for all these problems, including sub-modular function
minimization.
  Theoretically, we explain why \textsc{SuperGreedy++} is effective beyond DSS,
including for tasks like submodular minimization and minimum $ s $-$ t $ cut.
Empirically, we test several solvers, including the Fujishige-Wolfe algorithm
on over 400 experiments across seven problem types and large-scale
real/synthetic datasets. Surprisingly, general-purpose convex and flow-based
methods outperform task-specific baselines, demonstrating that with the right
framing, general optimization techniques can be both scalable and
state-of-the-art for submodular and supermodular ratio problems.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [428] [GPS-Aided Deep Learning for Beam Prediction and Tracking in UAV mmWave Communication](https://arxiv.org/abs/2505.17530)
*Vendi Ardianto Nugroho,Byung Moo Lee*

Key words: 毫米波通信，无人机，深度学习，GPS辅助，波束管理

<details>
  <summary>Details</summary>

Main category: eess.SP

TL;DR: 该论文提出了一种基于GPS辅助的深度学习模型，用于预测无人机毫米波通信中当前和未来的最优波束，实现了超过70%的Top-1预测准确率和低于0.6 dB的平均功率损耗，显著降低了开销。

Motivation: 无人机毫米波通信面临高路径损耗和动态移动性导致的波束管理难题，影响了通信链路的稳定性。

Method: 通过GPS预处理提取关键位置特征，结合深度学习架构将序列位置数据映射为波束索引预测，并采用数据集分割方法确保标签分布均衡。

Result: 模型在减少约93%开销的同时，保持了95%的波束预测准确率，94%至96%的预测中平均功率损耗不超过1 dB。

Conclusion: 该GPS辅助深度学习模型有效提升了无人机毫米波通信的波束管理效率和稳定性。

Abstract: Millimeter-wave (mmWave) communication enables high data rates for
cellular-connected Unmanned Aerial Vehicles (UAVs). However, a robust beam
management remains challenging due to significant path loss and the dynamic
mobility of UAVs, which can destabilize the UAV-base station (BS) link. This
research presents a GPS-aided deep learning (DL) model that simultaneously
predicts current and future optimal beams for UAV mmWave communications,
maintaining a Top-1 prediction accuracy exceeding 70% and an average power loss
below 0.6 dB across all prediction steps. These outcomes stem from a proposed
data set splitting method ensuring balanced label distribution, paired with a
GPS preprocessing technique that extracts key positional features, and a DL
architecture that maps sequential position data to beam index predictions. The
model reduces overhead by approximately 93% (requiring the training of 2 ~ 3
beams instead of 32 beams) with 95% beam prediction accuracy guarantees, and
ensures 94% to 96% of predictions exhibit mean power loss not exceeding 1 dB.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [429] [LMask: Learn to Solve Constrained Routing Problems with Lazy Masking](https://arxiv.org/abs/2505.17938)
*Tianyou Li,Haijun Zou,Jiayuan Wu,Zaiwen Wen*

Key words: 路由问题, 学习框架, 动态掩码, 回溯机制, TSPTW

<details>
  <summary>Details</summary>

Main category: math.OC

TL;DR: 该论文提出了一种名为 LMask 的新型学习框架，用于解决带约束的路由问题。通过动态掩码和回溯机制，LMask 能够生成高质量可行解，并在实验中表现优于现有神经方法。

Motivation: 解决带约束的路由问题时，传统方法难以高效处理复杂约束。LMask 旨在通过学习框架提升求解的可行性和质量。

Method: LMask 采用 LazyMask 解码方法和回溯机制，结合细化强度嵌入来编码搜索轨迹，并在训练中通过惩罚约束违反来减少采样成本。

Result: 在 TSPTW 和 TSPDL 问题上的实验表明，LMask 在可行率和解质量上达到了最优水平。

Conclusion: LMask 通过动态掩码和回溯机制，有效解决了带约束路由问题，并提供了理论和实验支持。

Abstract: Routing problems are canonical combinatorial optimization tasks with
wide-ranging applications in logistics, transportation, and supply chain
management. However, solving these problems becomes significantly more
challenging when complex constraints are involved. In this paper, we propose
LMask, a novel learning framework that utilizes dynamic masking to generate
high-quality feasible solutions for constrained routing problems. LMask
introduces the LazyMask decoding method, which lazily refines feasibility masks
with the backtracking mechanism. In addition, it employs the refinement
intensity embedding to encode the search trace into the model, mitigating
representation ambiguities induced by backtracking. To further reduce sampling
cost, LMask sets a backtracking budget during decoding, while constraint
violations are penalized in the loss function during training to counteract
infeasibility caused by this budget. We provide theoretical guarantees for the
validity and probabilistic optimality of our approach. Extensive experiments on
the traveling salesman problem with time windows (TSPTW) and TSP with draft
limits (TSPDL) demonstrate that LMask achieves state-of-the-art feasibility
rates and solution quality, outperforming existing neural methods.

</details>


### [430] [New Tight Bounds for SGD without Variance Assumption: A Computer-Aided Lyapunov Analysis](https://arxiv.org/abs/2505.17965)
*Daniel Cortild,Lucas Ketels,Juan Peypouquet,Guillaume Garrigos*

Key words: 随机梯度下降，Lyapunov能量，凸性，平滑性，性能估计问题

<details>
  <summary>Details</summary>

Main category: math.OC

TL;DR: 该论文改进了对随机梯度下降（SGD）的理论分析，无需依赖梯度方差的假设，仅利用损失函数的凸性和平滑性，通过单调性分析提升了当前最优结果，并扩展到更大的步长。

Motivation: 现有SGD分析通常依赖难以验证的梯度方差假设，本文旨在消除这一限制，仅基于损失函数的凸性和平滑性提供理论保证。

Method: 利用简单的Lyapunov能量的单调性证明新理论界限，并通过性能估计问题分析支持理论结果。

Result: 在不依赖梯度方差假设的情况下，论文证明了更优的理论界限，并将结果扩展到更大的步长，经验验证表明偏差项在框架内紧致。

Conclusion: 本文通过单调性分析和性能估计问题，改进了SGD的理论分析，扩展了其适用范围，并提供紧致的偏差界限。

Abstract: The analysis of Stochastic Gradient Descent (SGD) often relies on making some
assumption on the variance of the stochastic gradients, which is usually not
satisfied or difficult to verify in practice. This paper contributes to a
recent line of works which attempt to provide guarantees without making any
variance assumption, leveraging only the (strong) convexity and smoothness of
the loss functions. In this context, we prove new theoretical bounds derived
from the monotonicity of a simple Lyapunov energy, improving the current
state-of-the-art and extending their validity to larger step-sizes. Our
theoretical analysis is backed by a Performance Estimation Problem analysis,
which allows us to claim that, empirically, the bias term in our bounds is
tight within our framework.

</details>


### [431] [Deep Operator Neural Network Model Predictive Control](https://arxiv.org/abs/2505.18008)
*Thomas Oliver de Jong,Khemraj Shukla,Mircea Lazar*

Key words: 模型预测控制, 深度算子神经网络, 多步预测, 非线性系统, 多输入多输出系统

<details>
  <summary>Details</summary>

Main category: math.OC

TL;DR: 论文提出了基于深度算子神经网络（DeepONets）的多步预测（MS-DeepONet）模型预测控制（MPC）算法，通过改进原有架构实现多步预测和多输入多输出系统的控制，并在多个非线性系统中验证了其优越性。

Motivation: 现有的DeepONet架构在多输入多输出（MIMO）控制系统中需要对每个输入使用多个分支网络，且需多次评估以预测多步时间序列，计算效率低下。为此，论文提出改进的MS-DeepONet，直接实现多步预测以提高效率。

Method: 设计了一种多步DeepONet（MS-DeepONet）架构，利用分支网络编码输入函数空间，主干网络处理时间变量或初始条件依赖关系，实现单次评估多步预测。同时提出了自动化超参数选择策略，并基于PyTorch实现了MPC框架。

Result: 在非线性基准系统（van der Pol振荡器、四罐过程和倒立摆不稳定系统）的实验中，MS-DeepONet在学习和预测控制任务中一致优于标准DeepONet，成功实现了多摇摆和稳定控制策略。

Conclusion: MS-DeepONet是一种通用近似器，且在多步预测和MPC控制任务中表现更优，能够高效处理MIMO系统的复杂预测需求。

Abstract: In this paper, we consider the design of model predictive control (MPC)
algorithms based on deep operator neural networks (DeepONets). These neural
networks are capable of accurately approximating real and complex valued
solutions of continuous time nonlinear systems without relying on recurrent
architectures. The DeepONet architecture is made up of two feedforward neural
networks: the branch network, which encodes the input function space, and the
trunk network, which represents dependencies on temporal variables or initial
conditions. Utilizing the original DeepONet architecture as a predictor within
MPC for Multi Input Multi Output (MIMO) systems requires multiple branch
networks, to generate multi output predictions, one for each input. Moreover,
to predict multiple time steps into the future, the network has to be evaluated
multiple times. Motivated by this, we introduce a multi step DeepONet
(MS-DeepONet) architecture that computes in one shot multi step predictions of
system outputs from multi step input sequences, which is better suited for MPC.
We prove that the MS DeepONet is a universal approximator in terms of multi
step sequence prediction. Additionally, we develop automated hyper parameter
selection strategies and implement MPC frameworks using both the standard
DeepONet and the proposed MS DeepONet architectures in PyTorch. The
implementation is publicly available on GitHub. Simulation results demonstrate
that MS-DeepONet consistently outperforms the standard DeepONet in learning and
predictive control tasks across several nonlinear benchmark systems: the van
der Pol oscillator, the quadruple tank process, and a cart pendulum unstable
system, where it successfully learns and executes multiple swing up and
stabilization policies.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [432] [REMS: a unified solution representation, problem modeling and metaheuristic algorithm design for general combinatorial optimization problems](https://arxiv.org/abs/2505.17108)
*Aijuan Song,Guohua Wu*

Key words: 组合优化问题, 资源中心框架, 元启发式算法, 统一建模, 问题求解

<details>
  <summary>Details</summary>

Main category: cs.NE

TL;DR: 本文提出了一种资源中心的建模与求解框架（REMS），将不同组合优化问题统一建模，并设计了可复用的元启发式算法来解决这些问题。实验表明，REMS在多种问题上表现优于现有工具。

Motivation: 组合优化问题（COPs）通常需要定制算法，调整约束可能会导致算法重写。因此，提出一个统一建模和求解框架，以减少重复开发，提高算法复用性。

Method: REMS框架提取并定义问题中的资源和任务，统一解结构为任务分配到资源，并设计了初始化、邻域结构、破坏修复、交叉和排序等基础算子。基于这些算子开发了5种元启发式算法。

Result: 在10类COPs（如路由、调度、图着色等）上实验显示，REMS能有效建模并求解这些问题，且在大规模和复杂问题上表现优于GUROBI、SCIP和OR-TOOLS。

Conclusion: REMS提供了一种通用方法，能够高效建模和解决多种组合优化问题，减少了算法开发的重复性工作，并展现了比现有工具更强的竞争力。

Abstract: Combinatorial optimization problems (COPs) with discrete variables and finite
search space are critical across numerous fields, and solving them in
metaheuristic algorithms is popular. However, addressing a specific COP
typically requires developing a tailored and handcrafted algorithm. Even minor
adjustments, such as constraint changes, may necessitate algorithm
redevelopment. Therefore, establishing a framework for formulating diverse COPs
into a unified paradigm and designing reusable metaheuristic algorithms is
valuable. A COP can be typically viewed as the process of giving resources to
perform specific tasks, subjecting to given constraints. Motivated by this, a
resource-centered modeling and solving framework (REMS) is introduced for the
first time. We first extract and define resources and tasks from a COP.
Subsequently, given predetermined resources, the solution structure is unified
as assigning tasks to resources, from which variables, objectives, and
constraints can be derived and a problem model is constructed. To solve the
modeled COPs, several fundamental operators are designed based on the unified
solution structure, including the initial solution, neighborhood structure,
destruction and repair, crossover, and ranking. These operators enable the
development of various metaheuristic algorithms. Specially, 4
single-point-based algorithms and 1 population-based algorithm are configured
herein. Experiments on 10 COPs, covering routing, location, loading,
assignment, scheduling, and graph coloring problems, show that REMS can model
these COPs within the unified paradigm and effectively solve them with the
designed metaheuristic algorithms. Furthermore, REMS is more competitive than
GUROBI and SCIP in tackling large-scale instances and complex COPs, and
outperforms OR-TOOLS on several challenging COPs.

</details>


### [433] [LaSER: How Learning Can Guide the Evolution of Equations](https://arxiv.org/abs/2505.17309)
*Nam H. Le,Josh Bongard*

Key words: Baldwin效应,遗传编程,符号回归,LaSER,进化计算,表示学习

<details>
  <summary>Details</summary>

Main category: cs.NE

TL;DR: 论文摘要讨论了将监督学习与遗传编程（GP）结合的新方法，即LaSER，通过语义表示指导进化，显著提升了符号回归任务的泛化能力，同时保持了可解释性。

Motivation: 探讨如何将学习与进化两种适应性机制结合，以解决传统GP在符号回归任务中泛化能力不足的问题。

Method: 提出LaSER方法，将GP生成的语义表示输入监督学习器，利用学习器的表现评估个体适应度，而不改变语法树或进化过程。

Result: 在符号回归基准测试中，LaSER显著优于传统GP，某些情况下甚至超越主流机器学习回归方法，且保持了模型的可解释性。

Conclusion: LaSER为GP与现代机器学习流程结合提供了可行路径，并开辟了进化计算与表示学习交叉研究的新方向。

Abstract: Evolution and learning are two distinct yet complementary forms of
adaptation. While evolutionary processes operate across generations via the
selection of genotypes, learning occurs within the lifetime of an individual,
shaping behavior through phenotypic adjustment. The Baldwin effect describes
how lifetime learning can improve evolutionary search without altering
inherited structures. While this has proven effective in areas like
neuroevolution, where gradient-based learning is often used to fine-tune
weights or behaviors produced by evolution, it remains underexplored in systems
that evolve non-differentiable symbolic structures like Genetic Programming
(GP). GP evolves explicit syntax trees that represent equations, offering
strong interpretability but limited generalization due to the burden of
discovering both useful representations and precise mappings.
  Here, we show for the first time that integrating a simple form of supervised
learning, applied at the semantic or behavioral level during evaluation, can
effectively guide the evolution of equations in GP. To achieve this, we propose
a new GP pipeline, LaSER (Latent Semantic Evolutionary Regression), where each
GP individual generates a semantic representation that is passed to a
supervised learner. The quality of the learned mapping is used to assign
fitness, without modifying the underlying syntax tree or evolutionary process.
  Across standard symbolic regression benchmarks, in terms of generalization
ability, LaSER significantly outperforms traditional GP and, in several cases,
matches or exceeds popular machine learning regressors, while preserving the
symbolic interpretability. By separating evolution from learning, LaSER offers
a practical route to integrating GP with modern ML workflows, and opens new
avenues for research at the intersection of evolutionary computation and
representation learning.

</details>


### [434] [SEvoBench : A C++ Framework For Evolutionary Single-Objective Optimization Benchmarking](https://arxiv.org/abs/2505.17430)
*Yongkang Yang,Jian Zhao,Tengfei Yang*

Key words: SEvoBench, 进化计算, PSO, DE, 模块化设计, 基准测试

<details>
  <summary>Details</summary>

Main category: cs.NE

TL;DR: SEvoBench是一个现代C++框架，专为系统化基准测试进化单目标优化算法设计，具有模块化实现PSO和DE算法、高效基准测试套件以及并行实验分析功能。

Motivation: 为了提供一个高效、模块化且可扩展的框架，用于系统化基准测试进化单目标优化算法，填补现有工具的不足。

Method: 采用模块化设计实现PSO和DE算法，并提供高效的基准测试套件和并行实验分析功能。

Result: 实验证明框架在基准测试和算法比较中表现出色，同时在算法混合和参数分析方面也展示了强大能力。

Conclusion: 相比现有框架，SEvoBench在模块化实现、并行执行和SIMD向量化方面具有显著优势，适用于大规模问题。

Abstract: We present SEvoBench, a modern C++ framework for evolutionary computation
(EC), specifically designed to systematically benchmark evolutionary
single-objective optimization algorithms. The framework features modular
implementations of Particle Swarm Optimization (PSO) and Differential Evolution
(DE) algorithms, organized around three core components: (1) algorithm
construction with reusable modules, (2) efficient benchmark problem suites, and
(3) parallel experimental analysis. Experimental evaluations demonstrate the
framework's superior performance in benchmark testing and algorithm comparison.
Case studies further validate its capabilities in algorithm hybridization and
parameter analysis. Compared to existing frameworks, SEvoBench demonstrates
three key advantages: (i) highly efficient and reusable modular implementations
of PSO and DE algorithms, (ii) accelerated benchmarking through parallel
execution, and (iii) enhanced computational efficiency via SIMD (Single
Instruction Multiple Data) vectorization for large-scale problems.

</details>


### [435] [Bruno: Backpropagation Running Undersampled for Novel device Optimization](https://arxiv.org/abs/2505.17791)
*Luca Fehlings,Bojian Zhang,Paolo Gibertini,Martin A. Nicholson,Erika Covi,Fernando M. Quintana*

Key words: 神经形态计算, ASIC, 铁电电容, RRAM, 量化突触, 训练算法

<details>
  <summary>Details</summary>

Main category: cs.NE

TL;DR: 提出一种自底向上的方法，利用铁电电容和电阻式非易失性器件训练神经网络的硬件优化算法，相比传统方法在时空模式识别中减少了时间和内存消耗。

Motivation: 提升神经形态和机器学习系统的效率，通过专用集成电路（ASICs）优化神经网络部署，但需解决硬件特性和训练算法的适配问题。

Method: 基于物理器件紧凑模型开发训练算法，支持低比特精度、随机性和变异性等硬件限制，通过RRAM量化突触和铁电LIF神经元验证。

Result: 实验表明，采用FeLIF神经元和量化突触的网络在时空模式检测中优于传统LIF网络，显著降低时间和内存需求。

Conclusion: 自底向上的硬件模拟训练方法为神经形态系统提供了高效解决方案，尤其是针对时空任务。

Abstract: Recent efforts to improve the efficiency of neuromorphic and machine learning
systems have focused on the development of application-specific integrated
circuits (ASICs), which provide hardware specialized for the deployment of
neural networks, leading to potential gains in efficiency and performance.
These systems typically feature an architecture that goes beyond the von
Neumann architecture employed in general-purpose hardware such as GPUs. Neural
networks developed for this specialised hardware then need to take into account
the specifics of the hardware platform, which requires novel training
algorithms and accurate models of the hardware, since they cannot be abstracted
as a general-purpose computing platform. In this work, we present a bottom-up
approach to train neural networks for hardware based on spiking neurons and
synapses built on ferroelectric capacitor (FeCap) and Resistive switching
non-volatile devices (RRAM) respectively. In contrast to the more common
approach of designing hardware to fit existing abstract neuron or synapse
models, this approach starts with compact models of the physical device to
model the computational primitive of the neurons. Based on these models, a
training algorithm is developed that can reliably backpropagate through these
physical models, even when applying common hardware limitations, such as
stochasticity, variability, and low bit precision. The training algorithm is
then tested on a spatio-temporal dataset with a network composed of quantized
synapses based on RRAM and ferroelectric leaky integrate-and-fire (FeLIF)
neurons. The performance of the network is compared with different networks
composed of LIF neurons. The results of the experiments show the potential
advantage of using BRUNO to train networks with FeLIF neurons, by achieving a
reduction in both time and memory for detecting spatio-temporal patterns with
quantized synapses.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [436] [LiloDriver: A Lifelong Learning Framework for Closed-loop Motion Planning in Long-tail Autonomous Driving Scenarios](https://arxiv.org/abs/2505.17209)
*Huaiyuan Yao,Pengfei Li,Bu Jin,Yupeng Zheng,An Liu,Lisen Mu,Qing Su,Qian Zhang,Yilun Chen,Peng Li*

Key words: 自动驾驶,长尾场景,LLM,闭环规划,终身学习

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: LiloDriver是一个终身学习框架，结合大型语言模型和记忆增强规划系统，用于自动驾驶中的长尾场景适应。

Motivation: 解决现有规则驱动和数据驱动规划器在长尾场景中适应性不足的问题。

Method: 通过四阶段架构（感知、场景编码、记忆策略优化和LLM推理）实现闭环运动规划。

Result: 在nuPlan基准测试中表现优于静态规则和基于学习的规划器。

Conclusion: 结合结构化记忆和LLM推理可实现可扩展、类人的运动规划。

Abstract: Recent advances in autonomous driving research towards motion planners that
are robust, safe, and adaptive. However, existing rule-based and data-driven
planners lack adaptability to long-tail scenarios, while knowledge-driven
methods offer strong reasoning but face challenges in representation, control,
and real-world evaluation. To address these challenges, we present LiloDriver,
a lifelong learning framework for closed-loop motion planning in long-tail
autonomous driving scenarios. By integrating large language models (LLMs) with
a memory-augmented planner generation system, LiloDriver continuously adapts to
new scenarios without retraining. It features a four-stage architecture
including perception, scene encoding, memory-based strategy refinement, and
LLM-guided reasoning. Evaluated on the nuPlan benchmark, LiloDriver achieves
superior performance in both common and rare driving scenarios, outperforming
static rule-based and learning-based planners. Our results highlight the
effectiveness of combining structured memory and LLM reasoning to enable
scalable, human-like motion planning in real-world autonomous driving. Our code
is available at https://github.com/Hyan-Yao/LiloDriver.

</details>


### [437] [Bootstrapping Imitation Learning for Long-horizon Manipulation via Hierarchical Data Collection Space](https://arxiv.org/abs/2505.17389)
*Jinrong Yang,Kexun Chen,Zhuoling Li,Shengkai Wu,Yong Zhao,Liangliang Ren,Wenqiu Luo,Chaohui Shang,Meiyu Zhi,Linfeng Gao,Mingshan Sun,Hui Cheng*

Key words: 模仿学习, 机器人操作, 数据收集, 分层空间, 长序列任务

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 该论文提出了一种称为分层数据收集空间（HD-Space）的方法，用于优化机器人模仿学习中人类示范数据的质量和效率。通过将精细操作任务分解为多个关键原子任务并设计原子状态/动作空间，该方法显著提升了策略性能，尤其在长序列操作任务中。

Motivation: 尽管人类示范数据在机器人模仿学习中表现良好，但实现高成功率和泛化能力通常需要高昂的数据采集成本。论文旨在通过优化数据收集流程的状态/动作空间，解决预测非鲁棒动作的问题。

Method: 提出HD-Space方法，将任务分解为原子任务并设计相应的状态/动作空间，以生成高质量示范数据。通过模拟和真实世界的长序列操作任务进行了实验验证。

Result: 实验表明，基于HD-Space的训练数据能显著提升策略性能，尤其是在数据量较少的情况下，对长序列任务效果尤为突出。

Conclusion: HD-Space为优化数据质量和指导数据扩展提供了新思路，显著提高了模仿学习的效率和性能。

Abstract: Imitation learning (IL) with human demonstrations is a promising method for
robotic manipulation tasks. While minimal demonstrations enable robotic action
execution, achieving high success rates and generalization requires high cost,
e.g., continuously adding data or incrementally conducting human-in-loop
processes with complex hardware/software systems. In this paper, we rethink the
state/action space of the data collection pipeline as well as the underlying
factors responsible for the prediction of non-robust actions. To this end, we
introduce a Hierarchical Data Collection Space (HD-Space) for robotic imitation
learning, a simple data collection scheme, endowing the model to train with
proactive and high-quality data. Specifically, We segment the fine manipulation
task into multiple key atomic tasks from a high-level perspective and design
atomic state/action spaces for human demonstrations, aiming to generate robust
IL data. We conduct empirical evaluations across two simulated and five
real-world long-horizon manipulation tasks and demonstrate that IL policy
training with HD-Space-based data can achieve significantly enhanced policy
performance. HD-Space allows the use of a small amount of demonstration data to
train a more powerful policy, particularly for long-horizon manipulation tasks.
We aim for HD-Space to offer insights into optimizing data quality and guiding
data scaling. project page: https://hd-space-robotics.github.io.

</details>


### [438] [Dynamic Manipulation of Deformable Objects in 3D: Simulation, Benchmark and Learning Strategy](https://arxiv.org/abs/2505.17434)
*Guanzhou Lan,Yuqi Yang,Anup Teejo Mathew,Feiping Nie,Rong Wang,Xuelong Li,Federico Renda,Bin Zhao*

Key words: goal-conditioned manipulation, deformable objects, reduced-order dynamics, diffusion policy, physics-informed adaptation

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 该论文针对高自由度、欠驱动的可变形物体3D目标条件操作难题，提出了一种基于降阶动力学的新型仿真框架和基准，并结合模仿预训练与物理信息测试时适应的DIDP框架，显著提升了策略学习的效率和操作准确性。

Motivation: 解决可变形物体（如绳索）在3D目标条件操作中因复杂动力学和任务约束导致的挑战，克服现有方法局限于低速或2D场景的不足。

Method: 提出DIDP框架，结合降阶动力学仿真、扩散策略学习逆动力学，以及物理信息测试时适配方案，确保操作执行的一致性和可靠性。

Result: 实验验证了DIDP在策略准确性和鲁棒性方面的卓越表现。

Conclusion: DIDP框架通过模仿学习与物理引导的结合，有效解决了复杂3D目标操作问题，为实际应用提供了可靠方案。

Abstract: Goal-conditioned dynamic manipulation is inherently challenging due to
complex system dynamics and stringent task constraints, particularly in
deformable object scenarios characterized by high degrees of freedom and
underactuation. Prior methods often simplify the problem to low-speed or 2D
settings, limiting their applicability to real-world 3D tasks. In this work, we
explore 3D goal-conditioned rope manipulation as a representative challenge. To
mitigate data scarcity, we introduce a novel simulation framework and benchmark
grounded in reduced-order dynamics, which enables compact state representation
and facilitates efficient policy learning. Building on this, we propose
Dynamics Informed Diffusion Policy (DIDP), a framework that integrates
imitation pretraining with physics-informed test-time adaptation. First, we
design a diffusion policy that learns inverse dynamics within the reduced-order
space, enabling imitation learning to move beyond na\"ive data fitting and
capture the underlying physical structure. Second, we propose a
physics-informed test-time adaptation scheme that imposes kinematic boundary
conditions and structured dynamics priors on the diffusion process, ensuring
consistency and reliability in manipulation execution. Extensive experiments
validate the proposed approach, demonstrating strong performance in terms of
accuracy and robustness in the learned policy.

</details>


### [439] [DTRT: Enhancing Human Intent Estimation and Role Allocation for Physical Human-Robot Collaboration](https://arxiv.org/abs/2505.17490)
*Haotian Liu,Yuchuang Tong,Zhengtao Zhang*

Key words: 人机协作,意图估计,Transformer,轨迹预测,差分合作博弈

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 提出了一种基于双Transformer的机器人轨迹预测模型（DTRT），通过结合人类运动力和轨迹数据，实现了对人类意图的快速捕捉和动态调整，提升了人机协作的安全性和效率。

Motivation: 现有的人机协作方法依赖于短期运动数据进行意图估计，缺乏多步预测能力，导致意图变化感知不足和角色分配不合理。DTRT旨在解决这一问题。

Method: DTRT采用分层架构，结合两个基于Transformer的条件变分自编码器（CVAEs）进行意图估计，并利用差分合作博弈理论（DCGT）动态调整机器人行为。

Result: 实验表明，DTRT能够准确估计人类意图，并在协作中表现出优越性能，相比现有方法更具优势。

Conclusion: DTRT通过引入长期预测和动态行为调整，显著提升了人机协作的准确性和自主性。

Abstract: In physical Human-Robot Collaboration (pHRC), accurate human intent
estimation and rational human-robot role allocation are crucial for safe and
efficient assistance. Existing methods that rely on short-term motion data for
intention estimation lack multi-step prediction capabilities, hindering their
ability to sense intent changes and adjust human-robot assignments
autonomously, resulting in potential discrepancies. To address these issues, we
propose a Dual Transformer-based Robot Trajectron (DTRT) featuring a
hierarchical architecture, which harnesses human-guided motion and force data
to rapidly capture human intent changes, enabling accurate trajectory
predictions and dynamic robot behavior adjustments for effective collaboration.
Specifically, human intent estimation in DTRT uses two Transformer-based
Conditional Variational Autoencoders (CVAEs), incorporating robot motion data
in obstacle-free case with human-guided trajectory and force for obstacle
avoidance. Additionally, Differential Cooperative Game Theory (DCGT) is
employed to synthesize predictions based on human-applied forces, ensuring
robot behavior align with human intention. Compared to state-of-the-art (SOTA)
methods, DTRT incorporates human dynamics into long-term prediction, providing
an accurate understanding of intention and enabling rational role allocation,
achieving robot autonomy and maneuverability. Experiments demonstrate DTRT's
accurate intent estimation and superior collaboration performance.

</details>


### [440] [ExoGait-MS: Learning Periodic Dynamics with Multi-Scale Graph Network for Exoskeleton Gait Recognition](https://arxiv.org/abs/2505.18018)
*Lijiang Liu,Junyu Shi,Yong Sun,Zhiyuan Zhang,Jinni Zhou,Shugen Ma,Qiang Nie*

Key words: 外骨骼，个性化步态，图卷积网络，周期性动力学，康复治疗

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 论文提出了一种基于多尺度全局密集图卷积网络（GCN）和步态非线性周期性动力学学习的个性化步态识别方法，显著提升了外骨骼辅助治疗中的个性化控制效果。

Motivation: 现有外骨骼控制方法缺乏个性化，标准化步态可能导致不适或伤害，因此需要精确识别个体步态特征以提升适配性和康复效果。

Method: 采用多尺度全局密集GCN识别关节协同模式，并设计步态非线性周期性动力学学习模块捕捉时间域周期性特征。

Result: 实验结果显示，该方法在自建数据集上达到94.34%的准确率，超越现有最佳方法3.77%。

Conclusion: 该方法在个性化步态控制中具有显著潜力，可有效提升外骨骼辅助治疗的适应性。

Abstract: Current exoskeleton control methods often face challenges in delivering
personalized treatment. Standardized walking gaits can lead to patient
discomfort or even injury. Therefore, personalized gait is essential for the
effectiveness of exoskeleton robots, as it directly impacts their adaptability,
comfort, and rehabilitation outcomes for individual users. To enable
personalized treatment in exoskeleton-assisted therapy and related
applications, accurate recognition of personal gait is crucial for implementing
tailored gait control. The key challenge in gait recognition lies in
effectively capturing individual differences in subtle gait features caused by
joint synergy, such as step frequency and step length. To tackle this issue, we
propose a novel approach, which uses Multi-Scale Global Dense Graph
Convolutional Networks (GCN) in the spatial domain to identify latent joint
synergy patterns. Moreover, we propose a Gait Non-linear Periodic Dynamics
Learning module to effectively capture the periodic characteristics of gait in
the temporal domain. To support our individual gait recognition task, we have
constructed a comprehensive gait dataset that ensures both completeness and
reliability. Our experimental results demonstrate that our method achieves an
impressive accuracy of 94.34% on this dataset, surpassing the current
state-of-the-art (SOTA) by 3.77%. This advancement underscores the potential of
our approach to enhance personalized gait control in exoskeleton-assisted
therapy.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [441] [Synthetic History: Evaluating Visual Representations of the Past in Diffusion Models](https://arxiv.org/abs/2505.17064)
*Maria-Teresa De Rosa Palmini,Eva Cetinic*

Key words: 文本到图像模型，历史表现评估，HistVis数据集，时代错误，人口统计偏差

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一个评估文本到图像（TTI）扩散模型在历史场景中表现的系统化方法，并通过HistVis数据集揭示了模型在历史准确性、一致性及人口统计表现上的系统性偏差。

Motivation: 虽然TTI模型在社会和文化影响方面受到关注，但其对历史场景的准确表现仍未充分研究。本文旨在填补这一空白，提出评估历史表现的方法论。

Method: 作者构建了HistVis数据集（包含3万张合成图像），通过设计的历史主题提示词，评估了三种先进TTI模型在隐含风格关联、历史一致性和人口统计表现三个方面的表现。

Result: 研究发现，TTI模型在历史主题图像中存在系统性不准确，包括对历史时期的刻板印象、时代错误（如现代物品出现在古代场景中）以及人口统计分布的不合理。

Conclusion: 本研究为评估和改进TTI模型的历史表现提供了方法论和基准，是迈向更准确、文化对齐的TTI模型的第一步。

Abstract: As Text-to-Image (TTI) diffusion models become increasingly influential in
content creation, growing attention is being directed toward their societal and
cultural implications. While prior research has primarily examined demographic
and cultural biases, the ability of these models to accurately represent
historical contexts remains largely underexplored. In this work, we present a
systematic and reproducible methodology for evaluating how TTI systems depict
different historical periods. For this purpose, we introduce the HistVis
dataset, a curated collection of 30,000 synthetic images generated by three
state-of-the-art diffusion models using carefully designed prompts depicting
universal human activities across different historical periods. We evaluate
generated imagery across three key aspects: (1) Implicit Stylistic
Associations: examining default visual styles associated with specific eras;
(2) Historical Consistency: identifying anachronisms such as modern artifacts
in pre-modern contexts; and (3) Demographic Representation: comparing generated
racial and gender distributions against historically plausible baselines. Our
findings reveal systematic inaccuracies in historically themed generated
imagery, as TTI models frequently stereotype past eras by incorporating
unstated stylistic cues, introduce anachronisms, and fail to reflect plausible
demographic patterns. By offering a scalable methodology and benchmark for
assessing historical representation in generated imagery, this work provides an
initial step toward building more historically accurate and culturally aligned
TTI models.

</details>


### [442] [Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models](https://arxiv.org/abs/2505.17316)
*Jiachen Jiang,Jinxin Zhou,Bo Peng,Xia Ning,Zhihui Zhu*

Key words: 多模态LLM, 视觉嵌入, 对齐, 投影器, patch-aligned training

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文研究了视觉嵌入与大型语言模型(LLM)的对齐问题，提出了一种新的训练方法'patch-aligned training'，显著提升了多模态LLM(MLLM)的性能。

Motivation: 提升视觉嵌入与LLM的对齐能力，以便增强多模态LLM的能力，尤其是在依赖预训练视觉编码器和LLM的模型中。

Method: 论文首先研究了投影器在压缩视觉嵌入和对齐词嵌入中的作用，然后提出了'patch-aligned training'方法，以增强视觉区域与语义词汇的对齐。

Result: 实验表明，该方法不仅提升了压缩能力和对齐效果，还在多项任务中显著提升了MLLM的性能（如引用表达任务提升16%，问答任务提升4%）。

Conclusion: 研究表明，'patch-aligned training'能有效增强视觉与语义的对齐，并可轻松扩展到其他多模态模型中。

Abstract: Achieving better alignment between vision embeddings and Large Language
Models (LLMs) is crucial for enhancing the abilities of Multimodal LLMs
(MLLMs), particularly for recent models that rely on powerful pretrained vision
encoders and LLMs. A common approach to connect the pretrained vision encoder
and LLM is through a projector applied after the vision encoder. However, the
projector is often trained to enable the LLM to generate captions, and hence
the mechanism by which LLMs understand each vision token remains unclear. In
this work, we first investigate the role of the projector in compressing vision
embeddings and aligning them with word embeddings. We show that the projector
significantly compresses visual information, removing redundant details while
preserving essential elements necessary for the LLM to understand visual
content. We then examine patch-level alignment -- the alignment between each
vision patch and its corresponding semantic words -- and propose a
*multi-semantic alignment hypothesis*. Our analysis indicates that the
projector trained by caption loss improves patch-level alignment but only to a
limited extent, resulting in weak and coarse alignment. To address this issue,
we propose *patch-aligned training* to efficiently enhance patch-level
alignment. Our experiments show that patch-aligned training (1) achieves
stronger compression capability and improved patch-level alignment, enabling
the MLLM to generate higher-quality captions, (2) improves the MLLM's
performance by 16% on referring expression grounding tasks, 4% on
question-answering tasks, and 3% on modern instruction-following benchmarks
when using the same supervised fine-tuning (SFT) setting. The proposed method
can be easily extended to other multimodal models.

</details>


### [443] [FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding](https://arxiv.org/abs/2505.17330)
*Amit Agarwal,Srikant Panda,Kulbhushan Pachauri*

Key words: FS-DAG, VRDU, few-shot, domain adaptation, information extraction

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: FS-DAG是一种针对视觉丰富文档理解的高效、可扩展模型架构，适用于少样本场景。

Motivation: 解决视觉丰富文档理解任务中少样本适应、OCR错误和领域偏移等实际问题。

Method: 利用领域特定和语言/视觉特定的模块化框架，适应多样化文档类型。

Result: 在信息提取任务中表现出显著的收敛速度和性能提升，且模型参数少于90M。

Conclusion: FS-DAG展示了开发高效小模型的可能性，同时保持高性能。

Abstract: In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable
and efficient model architecture for visually rich document understanding
(VRDU) in few-shot settings. FS-DAG leverages domain-specific and
language/vision specific backbones within a modular framework to adapt to
diverse document types with minimal data. The model is robust to practical
challenges such as handling OCR errors, misspellings, and domain shifts, which
are critical in real-world deployments. FS-DAG is highly performant with less
than 90M parameters, making it well-suited for complex real-world applications
for Information Extraction (IE) tasks where computational resources are
limited. We demonstrate FS-DAG's capability through extensive experiments for
information extraction task, showing significant improvements in convergence
speed and performance compared to state-of-the-art methods. Additionally, this
work highlights the ongoing progress in developing smaller, more efficient
models that do not compromise on performance. Code :
https://github.com/oracle-samples/fs-dag

</details>


### [444] [Render-FM: A Foundation Model for Real-time Photorealistic Volumetric Rendering](https://arxiv.org/abs/2505.17338)
*Zhongpai Gao,Meng Zheng,Benjamin Planche,Anwesa Choudhuri,Terrence Chen,Ziyan Wu*

Key words: Volumetric rendering, CT scans, neural rendering, 6DGS, real-time visualization

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种名为Render-FM的基础模型，能够实时渲染CT扫描的3D解剖结构，避免了传统神经渲染技术耗时的逐场景优化，大幅提高了临床适用性。

Motivation: 当前的神经渲染技术在医学影像中虽然效果良好，但需逐场景优化，计算成本高且泛化能力有限，限制了临床实时应用。

Method: 采用编码器-解码器架构，从CT数据直接回归6D高斯喷溅（6DGS）参数，通过大规模医学数据预训练，消除了逐扫描优化步骤。

Result: Render-FM在视觉质量上媲美或优于传统方法，同时将单次推理时间从几小时缩短至几秒，适合实时手术规划和诊断。

Conclusion: Render-FM为医学影像提供了一种高效、高质量的实时渲染方案，解决了现有技术的计算瓶颈和泛化问题。

Abstract: Volumetric rendering of Computed Tomography (CT) scans is crucial for
visualizing complex 3D anatomical structures in medical imaging. Current
high-fidelity approaches, especially neural rendering techniques, require
time-consuming per-scene optimization, limiting clinical applicability due to
computational demands and poor generalizability. We propose Render-FM, a novel
foundation model for direct, real-time volumetric rendering of CT scans.
Render-FM employs an encoder-decoder architecture that directly regresses 6D
Gaussian Splatting (6DGS) parameters from CT volumes, eliminating per-scan
optimization through large-scale pre-training on diverse medical data. By
integrating robust feature extraction with the expressive power of 6DGS, our
approach efficiently generates high-quality, real-time interactive 3D
visualizations across diverse clinical CT data. Experiments demonstrate that
Render-FM achieves visual fidelity comparable or superior to specialized
per-scan methods while drastically reducing preparation time from nearly an
hour to seconds for a single inference step. This advancement enables seamless
integration into real-time surgical planning and diagnostic workflows. The
project page is: https://gaozhongpai.github.io/renderfm/.

</details>


### [445] [Dual Ascent Diffusion for Inverse Problems](https://arxiv.org/abs/2505.17353)
*Minseo Kim,Axel Levy,Gordon Wetzstein*

Key words: 逆问题, 扩散模型, 双上升优化, 最大后验问题

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 基于扩散模型的双上升优化框架解决了现有逆问题求解方法的不准确或次优问题。

Motivation: 现有基于最大后验或后验采样的方法存在计算近似问题，导致结果不准确或欠佳。

Method: 采用双上升优化框架结合扩散模型先验来解决最大后验问题。

Result: 新方法在图像恢复质量、抗噪性、速度和观测数据拟合上优于现有技术。

Conclusion: 提出的方法在解决逆问题时表现更优。

Abstract: Ill-posed inverse problems are fundamental in many domains, ranging from
astrophysics to medical imaging. Emerging diffusion models provide a powerful
prior for solving these problems. Existing maximum-a-posteriori (MAP) or
posterior sampling approaches, however, rely on different computational
approximations, leading to inaccurate or suboptimal samples. To address this
issue, we introduce a new approach to solving MAP problems with diffusion model
priors using a dual ascent optimization framework. Our framework achieves
better image quality as measured by various metrics for image restoration
problems, it is more robust to high levels of measurement noise, it is faster,
and it estimates solutions that represent the observations more faithfully than
the state of the art.

</details>


### [446] [EVM-Fusion: An Explainable Vision Mamba Architecture with Neural Algorithmic Fusion](https://arxiv.org/abs/2505.17367)
*Zichuan Yang*

Key words: medical image classification, Explainable AI, Vision Mamba, Neural Algorithmic Fusion, multi-organ diagnosis

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: EVM-Fusion is a new Explainable Vision Mamba architecture that uses Neural Algorithmic Fusion (NAF) for multi-organ medical image classification. It combines DenseNet, U-Net, and Vision Mamba modules, achieving 99.75% test accuracy while providing interpretable insights.

Motivation: Improve accuracy, interpretability, and generalizability in medical image classification to support clinical decision-making.

Method: Uses a multipath design with DenseNet, U-Net, and Vision Mamba (Vim) modules, integrating features via cross-modal attention and NAF blocks. Includes explainability tools like path-specific spatial attention and attention weights.

Result: Achieves 99.75% test accuracy on a 9-class multi-organ dataset, with interpretable decision-making insights.

Conclusion: EVM-Fusion offers high accuracy and explainability, making it a trustworthy AI tool for medical diagnostics.

Abstract: Medical image classification is critical for clinical decision-making, yet
demands for accuracy, interpretability, and generalizability remain
challenging. This paper introduces EVM-Fusion, an Explainable Vision Mamba
architecture featuring a novel Neural Algorithmic Fusion (NAF) mechanism for
multi-organ medical image classification. EVM-Fusion leverages a multipath
design, where DenseNet and U-Net based pathways, enhanced by Vision Mamba (Vim)
modules, operate in parallel with a traditional feature pathway. These diverse
features are dynamically integrated via a two-stage fusion process: cross-modal
attention followed by the iterative NAF block, which learns an adaptive fusion
algorithm. Intrinsic explainability is embedded through path-specific spatial
attention, Vim {\Delta}-value maps, traditional feature SE-attention, and
cross-modal attention weights. Experiments on a diverse 9-class multi-organ
medical image dataset demonstrate EVM-Fusion's strong classification
performance, achieving 99.75% test accuracy and provide multi-faceted insights
into its decision-making process, highlighting its potential for trustworthy AI
in medical diagnostics.

</details>


### [447] [Dual-sensing driving detection model](https://arxiv.org/abs/2505.17392)
*Leon C. C. K,Zeng Hui*

Key words: 驾驶员疲劳检测、计算机视觉、生理信号分析、双感知、融合策略

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种结合计算机视觉与生理信号分析的新型双感知驾驶员疲劳检测方法，通过互补优势和先进融合策略，提升了检测的准确性和可靠性。

Motivation: 现有单模态疲劳检测方法存在局限性，无法全面评估驾驶员的疲劳状态，需要一种更可靠、更全面的解决方案。

Method: 结合实时面部特征分析与生理信号处理，并采用先进的融合策略，实现高效且高精度的疲劳检测。

Result: 实验表明，该方法在控制环境和真实驾驶场景中均优于传统方法，且具备较高的实用性和可靠性。

Conclusion: 该研究为驾驶员疲劳检测提供了更可靠、经济且人性化的解决方案，具有减少疲劳相关事故的潜力。

Abstract: In this paper, a novel dual-sensing driver fatigue detection method combining
computer vision and physiological signal analysis is proposed. The system
exploits the complementary advantages of the two sensing modalities and breaks
through the limitations of existing single-modality methods. We introduce an
innovative architecture that combines real-time facial feature analysis with
physiological signal processing, combined with advanced fusion strategies, for
robust fatigue detection. The system is designed to run efficiently on existing
hardware while maintaining high accuracy and reliability. Through comprehensive
experiments, we demonstrate that our method outperforms traditional methods in
both controlled environments and real-world conditions, while maintaining high
accuracy. The practical applicability of the system has been verified through
extensive tests in various driving scenarios and shows great potential in
reducing fatigue-related accidents. This study contributes to the field by
providing a more reliable, cost-effective, and humane solution for driver
fatigue detection.

</details>


### [448] [Wildfire Detection Using Vision Transformer with the Wildfire Dataset](https://arxiv.org/abs/2505.17395)
*Gowtham Raj Vuppari,Navarun Gupta,Ahmed El-Sayed,Xingguo Xiong*

Key words: 野火检测,视觉变换器,深度学习,实时数据,计算成本

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文探讨了利用视觉变换器（ViT）模型提升美国野火早期检测的效果，解决了数据质量、实时性和计算成本等挑战。

Motivation: 近年来美国特别是加利福尼亚州野火频发，造成了严重的人员伤亡和财产损失，亟需高效的检测和预防技术。

Method: 使用包含10.74 GB高分辨率图像的野火数据集，将图像调整为224x224像素并转换为张量格式，利用视觉变换器模型进行训练。

Result: 通过ViT模型处理复杂图像数据，能够提高野火检测的准确性。

Conclusion: 视觉变换器模型为野火早期检测提供了潜在的解决方案，但仍需克服实时数据获取和计算成本等挑战。

Abstract: The critical need for sophisticated detection techniques has been highlighted
by the rising frequency and intensity of wildfires in the US, especially in
California. In 2023, wildfires caused 130 deaths nationwide, the highest since
1990. In January 2025, Los Angeles wildfires which included the Palisades and
Eaton fires burnt approximately 40,000 acres and 12,000 buildings, and caused
loss of human lives. The devastation underscores the urgent need for effective
detection and prevention strategies. Deep learning models, such as Vision
Transformers (ViTs), can enhance early detection by processing complex image
data with high accuracy. However, wildfire detection faces challenges,
including the availability of high-quality, real-time data. Wildfires often
occur in remote areas with limited sensor coverage, and environmental factors
like smoke and cloud cover can hinder detection. Additionally, training deep
learning models is computationally expensive, and issues like false
positives/negatives and scaling remain concerns. Integrating detection systems
with real-time alert mechanisms also poses difficulties. In this work, we used
the wildfire dataset consisting of 10.74 GB high-resolution images categorized
into 'fire' and 'nofire' classes is used for training the ViT model. To prepare
the data, images are resized to 224 x 224 pixels, converted into tensor format,
and normalized using ImageNet statistics.

</details>


### [449] [Learning Generalized and Flexible Trajectory Models from Omni-Semantic Supervision](https://arxiv.org/abs/2505.17437)
*Yuanshao Zhu,James Jianqiao Yu,Xiangyu Zhao,Xiao Han,Qidong Liu,Xuetao Wei,Yuxuan Liang*

Key words: 轨迹检索, 多模态, 语义融合, 数据挖掘

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: OmniTraj是一个灵活的多语义轨迹检索框架，集成四种模态（原始轨迹、拓扑、路段和区域）以支持高效、准确的查询。

Motivation: 移动设备和数据采集技术的普及导致轨迹数据激增，现有检索方法在大规模数据和条件查询上效率不足，且依赖单一轨迹相似性度量。

Method: 设计四种模态的专用编码器，嵌入并融合到共享表示空间，支持基于单一或组合模态的灵活查询。

Result: 在真实数据集上的实验表明，OmniTraj能高效处理大规模数据，支持多模态查询及下游任务。

Conclusion: OmniTraj突破了传统方法的局限性，提供了更灵活、高效的轨迹检索解决方案。

Abstract: The widespread adoption of mobile devices and data collection technologies
has led to an exponential increase in trajectory data, presenting significant
challenges in spatio-temporal data mining, particularly for efficient and
accurate trajectory retrieval. However, existing methods for trajectory
retrieval face notable limitations, including inefficiencies in large-scale
data, lack of support for condition-based queries, and reliance on trajectory
similarity measures. To address the above challenges, we propose OmniTraj, a
generalized and flexible omni-semantic trajectory retrieval framework that
integrates four complementary modalities or semantics -- raw trajectories,
topology, road segments, and regions -- into a unified system. Unlike
traditional approaches that are limited to computing and processing
trajectories as a single modality, OmniTraj designs dedicated encoders for each
modality, which are embedded and fused into a shared representation space. This
design enables OmniTraj to support accurate and flexible queries based on any
individual modality or combination thereof, overcoming the rigidity of
traditional similarity-based methods. Extensive experiments on two real-world
datasets demonstrate the effectiveness of OmniTraj in handling large-scale
data, providing flexible, multi-modality queries, and supporting downstream
tasks and applications.

</details>


### [450] [Graph Mamba for Efficient Whole Slide Image Understanding](https://arxiv.org/abs/2505.17457)
*Jiaxuan Lu,Junyan Shi,Yuhui Lin,Fang Yan,Yue Gao,Shaoting Zhang,Xiaosong Wang*

Key words: Whole Slide Images, Multiple Instance Learning, Graph Neural Networks, Mamba, State Space Model

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: WSI-GMamba combines GNNs and Mamba for efficient large-scale Whole Slide Image analysis, achieving high accuracy with lower computational cost.

Motivation: Existing MIL methods (e.g., GNNs and Transformers) struggle with scalability and computational efficiency for high-resolution WSIs.

Method: Proposes WSI-GMamba, integrating GNNs' relational modeling with Mamba's efficiency via a Bi-SSM for feature aggregation.

Result: Achieves Transformer-level performance with 7x fewer FLOPs, scalable for large WSI analysis.

Conclusion: WSI-GMamba offers a balanced solution for accurate and computationally efficient slide-level classification.

Abstract: Whole Slide Images (WSIs) in histopathology present a significant challenge
for large-scale medical image analysis due to their high resolution, large
size, and complex tile relationships. Existing Multiple Instance Learning (MIL)
methods, such as Graph Neural Networks (GNNs) and Transformer-based models,
face limitations in scalability and computational cost. To bridge this gap, we
propose the WSI-GMamba framework, which synergistically combines the relational
modeling strengths of GNNs with the efficiency of Mamba, the State Space Model
designed for sequence learning. The proposed GMamba block integrates Message
Passing, Graph Scanning & Flattening, and feature aggregation via a
Bidirectional State Space Model (Bi-SSM), achieving Transformer-level
performance with 7* fewer FLOPs. By leveraging the complementary strengths of
lightweight GNNs and Mamba, the WSI-GMamba framework delivers a scalable
solution for large-scale WSI analysis, offering both high accuracy and
computational efficiency for slide-level classification.

</details>


### [451] [OrionBench: A Benchmark for Chart and Human-Recognizable Object Detection in Infographics](https://arxiv.org/abs/2505.17473)
*Jiangning Zhu,Yuxing Zhou,Zheng Wang,Juntao Yao,Yima Gu,Yuhui Yuan,Shixia Liu*

Key words: 视觉语言模型，图表理解，目标检测，基准测试，信息图表

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: OrionBench 是一个旨在提升视觉语言模型（VLMs）对图表和可识别对象（HROs）检测准确性的基准测试，包含大量真实和合成图表数据，并通过多种应用场景验证其有效性。

Motivation: 现有视觉语言模型在图表和可识别对象（HROs）的视觉定位上存在不足，而图表理解需要准确识别并推理这些元素，因此开发了一个新的基准测试来支持相关模型的开发。

Method: 结合模型辅助和程序化方法，构建了包含 26,250 张真实图表和 78,750 张合成图表的 OrionBench 数据集，标注了超过 690 万个边界框。

Result: 通过三个应用证明了 OrionBench 的价值：1）提升 VLMs 的图表理解性能，2）比较现有目标检测模型，3）将模型应用于文档布局和 UI 元素检测。

Conclusion: OrionBench 促进了图表和 HROs 检测模型的发展，并展示了其在多种实际场景中的适用性。

Abstract: Given the central role of charts in scientific, business, and communication
contexts, enhancing the chart understanding capabilities of vision-language
models (VLMs) has become increasingly critical. A key limitation of existing
VLMs lies in their inaccurate visual grounding of infographic elements,
including charts and human-recognizable objects (HROs) such as icons and
images. However, chart understanding often requires identifying relevant
elements and reasoning over them. To address this limitation, we introduce
OrionBench, a benchmark designed to support the development of accurate object
detection models for charts and HROs in infographics. It contains 26,250 real
and 78,750 synthetic infographics, with over 6.9 million bounding box
annotations. These annotations are created by combining the model-in-the-loop
and programmatic methods. We demonstrate the usefulness of OrionBench through
three applications: 1) constructing a Thinking-with-Boxes scheme to boost the
chart understanding performance of VLMs, 2) comparing existing object detection
models, and 3) applying the developed detection model to document layout and UI
element detection.

</details>


### [452] [RoHyDR: Robust Hybrid Diffusion Recovery for Incomplete Multimodal Emotion Recognition](https://arxiv.org/abs/2505.17501)
*Yuehan Jin,Xiaoqing Liu,Yiyuan Yang,Zhiwen Yu,Tong Zhang,Kaixiang Yang*

Key words: 多模态情感识别、数据缺失恢复、扩散模型、对抗学习、鲁棒学习

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种名为RoHyDR的新框架，通过结合扩散模型和对抗学习，解决了多模态情感识别中的数据缺失问题，并在多种缺失情况下取得了优于现有方法的性能。

Motivation: 多模态情感识别在实际应用中常因传感器故障或噪声导致数据缺失或损坏，影响了模型性能。为解决这一问题，论文旨在开发一个能够在不同层次（单模态、多模态、特征、语义）恢复缺失数据的鲁棒框架。

Method: RoHyDR框架结合了扩散模型和对抗学习：1）使用扩散生成器从高斯噪声中生成与分布一致且语义对齐的单模态表示；2）引入对抗学习恢复多模态融合表示；3）提出多阶段优化策略提升训练稳定性与效率。

Result: 在两个广泛使用的多模态情感识别基准测试中，RoHyDR在不同缺失模态场景下均优于当前最优方法，实现了鲁棒的识别性能。

Conclusion: RoHyDR通过混合扩散和对抗学习机制，有效解决了多模态情感识别中的数据缺失问题，显著提升了模型在缺失情况下的性能表现。

Abstract: Multimodal emotion recognition analyzes emotions by combining data from
multiple sources. However, real-world noise or sensor failures often cause
missing or corrupted data, creating the Incomplete Multimodal Emotion
Recognition (IMER) challenge. In this paper, we propose Robust Hybrid Diffusion
Recovery (RoHyDR), a novel framework that performs missing-modality recovery at
unimodal, multimodal, feature, and semantic levels. For unimodal representation
recovery of missing modalities, RoHyDR exploits a diffusion-based generator to
generate distribution-consistent and semantically aligned representations from
Gaussian noise, using available modalities as conditioning. For multimodal
fusion recovery, we introduce adversarial learning to produce a realistic fused
multimodal representation and recover missing semantic content. We further
propose a multi-stage optimization strategy that enhances training stability
and efficiency. In contrast to previous work, the hybrid diffusion and
adversarial learning-based recovery mechanism in RoHyDR allows recovery of
missing information in both unimodal representation and multimodal fusion, at
both feature and semantic levels, effectively mitigating performance
degradation caused by suboptimal optimization. Comprehensive experiments
conducted on two widely used multimodal emotion recognition benchmarks
demonstrate that our proposed method outperforms state-of-the-art IMER methods,
achieving robust recognition performance under various missing-modality
scenarios. Our code will be made publicly available upon acceptance.

</details>


### [453] [Do You Keep an Eye on What I Ask? Mitigating Multimodal Hallucination via Attention-Guided Ensemble Decoding](https://arxiv.org/abs/2505.17529)
*Yeongjae Cho,Keonwoo Kim,Taebaek Hwang,Sungzoon Cho*

Key words: 大型视觉语言模型,对象幻觉,Ensemble Decoding,注意力图,合理性约束

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一种名为Ensemble Decoding (ED)的新策略，通过分割输入图像并结合注意力图分配权重的对数分布，以解决大型视觉语言模型中的对象幻觉问题。

Motivation: 大型视觉语言模型在图像描述和视觉问答等任务中表现出色，但仍存在对象幻觉问题，即生成不准确的描述或包含不存在对象。现有方法在扩展性和依赖外部模块方面存在局限。

Method: 提出了Ensemble Decoding (ED)策略，包括分割输入图像、结合注意力图分配权重的对数分布，并引入ED自适应合理性约束校准对数分布。还设计了快速版FastED以适配速度敏感场景。

Result: 在多个幻觉基准测试中取得最优性能，验证了方法的有效性。

Conclusion: ED策略为解决对象幻觉提供了高效且可扩展的解决方案，尤其在速度敏感应用中表现突出。

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have
significantly expanded their utility in tasks like image captioning and visual
question answering. However, they still struggle with object hallucination,
where models generate descriptions that inaccurately reflect the visual content
by including nonexistent objects or misrepresenting existing ones. While
previous methods, such as data augmentation and training-free approaches,
strive to tackle this issue, they still encounter scalability challenges and
often depend on additional external modules. In this work, we propose Ensemble
Decoding (ED), a novel strategy that splits the input image into sub-images and
combines logit distributions by assigning weights through the attention map.
Furthermore, we introduce ED adaptive plausibility constraint to calibrate
logit distribution and FastED, a variant designed for speed-critical
applications. Extensive experiments across hallucination benchmarks demonstrate
that our proposed method achieves state-of-the-art performance, validating the
effectiveness of our approach.

</details>


### [454] [RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation via Reinforcement Learning](https://arxiv.org/abs/2505.17540)
*Mingrui Wu,Lu Wang,Pu Zhao,Fangkai Yang,Jianjin Zhang,Jianfeng Liu,Yuefeng Zhan,Weihao Han,Hao Sun,Jiayi Ji,Xiaoshuai Sun,Qingwei Lin,Weiwei Deng,Dongmei Zhang,Feng Sun,Qi Zhang,Rongrong Ji*

Key words: 文本到图像生成, 提示增强, 强化学习, 自反式提示, 组合泛化

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了RePrompt框架，通过强化学习引导大型语言模型生成结构化、自反式的增强提示，优化图像生成结果，显著提升了空间布局和组合泛化能力。

Motivation: 现有文本到图像生成模型在处理简短提示时难以准确捕捉用户意图，现有的基于大型语言模型的提示增强方法常因缺乏视觉语义和现实构图基础而生成不切实际的内容。

Method: 提出RePrompt框架，通过强化学习训练语言模型生成结构化的自反式提示，结合图像级别结果的奖励模型（包括人类偏好、语义对齐和视觉构图）间接优化提示生成。

Result: 在GenEval和T2I-Compbench上的实验表明，RePrompt显著提升了空间布局忠实度和组合泛化能力，达到了新的先进水平。

Conclusion: RePrompt通过强化学习和结构化提示生成，无需人工标注数据即可实现端到端训练，有效改进了文本到图像生成的性能。

Abstract: Despite recent progress in text-to-image (T2I) generation, existing models
often struggle to faithfully capture user intentions from short and
under-specified prompts. While prior work has attempted to enhance prompts
using large language models (LLMs), these methods frequently generate stylistic
or unrealistic content due to insufficient grounding in visual semantics and
real-world composition. Inspired by recent advances in reasoning for language
model, we propose RePrompt, a novel reprompting framework that introduces
explicit reasoning into the prompt enhancement process via reinforcement
learning. Instead of relying on handcrafted rules or stylistic rewrites, our
method trains a language model to generate structured, self-reflective prompts
by optimizing for image-level outcomes. The tailored reward models assesse the
generated images in terms of human preference, semantic alignment, and visual
composition, providing indirect supervision to refine prompt generation. Our
approach enables end-to-end training without human-annotated data. Experiments
on GenEval and T2I-Compbench show that RePrompt significantly boosts spatial
layout fidelity and compositional generalization across diverse T2I backbones,
establishing new state-of-the-art results.

</details>


### [455] [Model Already Knows the Best Noise: Bayesian Active Noise Selection via Attention in Video Diffusion Model](https://arxiv.org/abs/2505.17561)
*Kwanyoung Kim,Sanghyun Kim*

Key words: 视频扩散模型，噪声选择，注意力机制，不确定性量化，高效推理

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: ANSE（主动噪声选择生成）提出了一种基于注意力的噪声选择框架，通过量化不确定性来选择高质量噪声种子，提高视频扩散模型的质量和时序一致性。

Motivation: 视频扩散模型中初始噪声的选择对生成质量和提示对齐有显著影响，现有方法忽视模型内部信号，需要更高效且基于模型的噪声选择方法。

Method: 提出BANSA（基于贝叶斯注意力选择的噪声获取函数），通过多注意力样本的熵分歧估计模型置信度，并采用伯努利掩码近似实现高效推理。

Result: 在CogVideoX-2B和5B上实验表明，ANSE仅增加8%和13%推理时间即可提升视频质量和时序一致性。

Conclusion: ANSE为视频扩散提供了一种通用且高效的噪声选择方案，显著提升生成效果。

Abstract: The choice of initial noise significantly affects the quality and prompt
alignment of video diffusion models, where different noise seeds for the same
prompt can lead to drastically different generations. While recent methods rely
on externally designed priors such as frequency filters or inter-frame
smoothing, they often overlook internal model signals that indicate which noise
seeds are inherently preferable. To address this, we propose ANSE (Active Noise
Selection for Generation), a model-aware framework that selects high-quality
noise seeds by quantifying attention-based uncertainty. At its core is BANSA
(Bayesian Active Noise Selection via Attention), an acquisition function that
measures entropy disagreement across multiple stochastic attention samples to
estimate model confidence and consistency. For efficient inference-time
deployment, we introduce a Bernoulli-masked approximation of BANSA that enables
score estimation using a single diffusion step and a subset of attention
layers. Experiments on CogVideoX-2B and 5B demonstrate that ANSE improves video
quality and temporal coherence with only an 8% and 13% increase in inference
time, respectively, providing a principled and generalizable approach to noise
selection in video diffusion. See our project page:
https://anse-project.github.io/anse-project/

</details>


### [456] [Scaling Image and Video Generation via Test-Time Evolutionary Search](https://arxiv.org/abs/2505.17618)
*Haoran He,Jiajun Liang,Xintao Wang,Pengfei Wan,Di Zhang,Kun Gai,Ling Pan*

Key words: 测试时扩展, 进化搜索, 扩散模型, 流模型, 图像生成, 视频生成

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种名为EvoSearch的新方法，通过进化搜索优化扩散和流模型的推理过程，提升图像和视频生成的性能，无需额外训练或模型扩展。

Motivation: 随着模型预训练的计算成本不断攀升，测试时扩展（TTS）成为提升生成模型性能的新方向，但在视觉任务中的应用仍有局限。本文旨在解决这一领域空白，提出一种通用且高效的TTS方法。

Method: EvoSearch将扩散和流模型的测试时扩展问题转化为进化搜索问题，利用生物进化原理优化去噪轨迹，通过选择和突变机制迭代生成更高质量的样本。

Result: 实验表明，EvoSearch在图像和视频生成任务中优于现有方法，具有更高的多样性和对未见评估指标的强泛化能力。

Conclusion: EvoSearch是一种无需额外训练的通用TTS方法，显著提升了视觉生成模型的性能与多样性。

Abstract: As the marginal cost of scaling computation (data and parameters) during
model pre-training continues to increase substantially, test-time scaling (TTS)
has emerged as a promising direction for improving generative model performance
by allocating additional computation at inference time. While TTS has
demonstrated significant success across multiple language tasks, there remains
a notable gap in understanding the test-time scaling behaviors of image and
video generative models (diffusion-based or flow-based models). Although recent
works have initiated exploration into inference-time strategies for vision
tasks, these approaches face critical limitations: being constrained to
task-specific domains, exhibiting poor scalability, or falling into reward
over-optimization that sacrifices sample diversity. In this paper, we propose
\textbf{Evo}lutionary \textbf{Search} (EvoSearch), a novel, generalist, and
efficient TTS method that effectively enhances the scalability of both image
and video generation across diffusion and flow models, without requiring
additional training or model expansion. EvoSearch reformulates test-time
scaling for diffusion and flow models as an evolutionary search problem,
leveraging principles from biological evolution to efficiently explore and
refine the denoising trajectory. By incorporating carefully designed selection
and mutation mechanisms tailored to the stochastic differential equation
denoising process, EvoSearch iteratively generates higher-quality offspring
while preserving population diversity. Through extensive evaluation across both
diffusion and flow architectures for image and video generation tasks, we
demonstrate that our method consistently outperforms existing approaches,
achieves higher diversity, and shows strong generalizability to unseen
evaluation metrics. Our project is available at the website
https://tinnerhrhe.github.io/evosearch.

</details>


### [457] [HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning](https://arxiv.org/abs/2505.17645)
*Chuhao Zhou,Jianfei Yang*

Key words: HoloLLM, 多模态大语言模型, 传感器融合, 智能家居, UMIP

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: HoloLLM是一种多模态大语言模型（MLLM），通过整合LiDAR、红外、mmWave雷达和WiFi等传感器，提升智能家居中人类感知与推理能力。

Motivation: 解决视觉语言模型（VLM）在遮挡、光线不足或隐私限制等实际场景中的局限性，实现多模态感知的鲁棒性。

Method: 设计了通用模态注入投影器（UMIP）和数据标注流水线，解决稀有传感器数据对齐与信号表示异构性问题。

Result: 在两项新基准测试中，HoloLLM显著优于现有MLLM，人类感知准确率提升高达30%。

Conclusion: HoloLLM为实际场景中的多感官智能体奠定了基础。

Abstract: Embodied agents operating in smart homes must understand human behavior
through diverse sensory inputs and communicate via natural language. While
Vision-Language Models (VLMs) have enabled impressive language-grounded
perception, their reliance on visual data limits robustness in real-world
scenarios with occlusions, poor lighting, or privacy constraints. In this
paper, we introduce HoloLLM, a Multimodal Large Language Model (MLLM) that
integrates uncommon but powerful sensing modalities, such as LiDAR, infrared,
mmWave radar, and WiFi, to enable seamless human perception and reasoning
across heterogeneous environments. We address two key challenges: (1) the
scarcity of aligned modality-text data for rare sensors, and (2) the
heterogeneity of their physical signal representations. To overcome these, we
design a Universal Modality-Injection Projector (UMIP) that enhances
pre-aligned modality embeddings with fine-grained, text-aligned features from
tailored encoders via coarse-to-fine cross-attention without introducing
significant alignment overhead. We further introduce a human-VLM collaborative
data curation pipeline to generate paired textual annotations for sensing
datasets. Extensive experiments on two newly constructed benchmarks show that
HoloLLM significantly outperforms existing MLLMs, improving language-grounded
human sensing accuracy by up to 30%. This work establishes a new foundation for
real-world, language-informed multisensory embodied intelligence.

</details>


### [458] [CAMA: Enhancing Multimodal In-Context Learning with Context-Aware Modulated Attention](https://arxiv.org/abs/2505.17097)
*Yanshu Li,JianJiang Yang,Bozheng Li,Ruixiang Tang*

Key words: 多模态学习, 上下文学习, 大型视觉语言模型, 注意力机制, CAMA

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种名为CAMA（上下文感知调制注意力）的新方法，通过直接校准大型视觉语言模型（LVLM）的注意力对数来解决多模态上下文学习（ICL）的不稳定性问题。

Motivation: 多模态上下文学习（ICL）在实际应用中虽然强大但不稳定，当前研究大多关注序列配置优化而忽略了LVLM内部机制。

Method: 提出Context-Aware Modulated Attention（CAMA），一种训练免费且即插即用的方法，用于直接校准LVLM的注意力对数。

Result: 在四个LVLM模型和六个基准测试中验证了CAMA的有效性和通用性。

Conclusion: CAMA为深入探索和针对性利用LVLM注意力动态开辟了新途径，推动了多模态推理的发展。

Abstract: Multimodal in-context learning (ICL) enables large vision-language models
(LVLMs) to efficiently adapt to novel tasks, supporting a wide array of
real-world applications. However, multimodal ICL remains unstable, and current
research largely focuses on optimizing sequence configuration while overlooking
the internal mechanisms of LVLMs. In this work, we first provide a theoretical
analysis of attentional dynamics in multimodal ICL and identify three core
limitations of standard attention that ICL impair performance. To address these
challenges, we propose Context-Aware Modulated Attention (CAMA), a simple yet
effective plug-and-play method for directly calibrating LVLM attention logits.
CAMA is training-free and can be seamlessly applied to various open-source
LVLMs. We evaluate CAMA on four LVLMs across six benchmarks, demonstrating its
effectiveness and generality. CAMA opens new opportunities for deeper
exploration and targeted utilization of LVLM attention dynamics to advance
multimodal reasoning.

</details>


### [459] [Robustifying Vision-Language Models via Dynamic Token Reweighting](https://arxiv.org/abs/2505.17132)
*Tanqiu Jiang,Jiacheng Liang,Rongyi Zhu,Jiawei Zhou,Fenglong Ma,Ting Wang*

Key words: 视觉语言模型, 越狱攻击, 键值缓存, 多模态安全, 推断时防御

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: DTR是一种新型推断时防御方法，通过优化键值缓存来抵御多模态越狱攻击，无需依赖特定安全数据或昂贵图像转换，动态调整视觉标记权重以保持模型能力与效率。

Motivation: 大型视觉语言模型易受越狱攻击，现有防御方法依赖特定数据或成本高，因此需要高效且通用的安全增强方案。

Method: 通过重新定义视觉模态引发的安全性相关分布偏移，动态调整视觉标记权重以优化键值缓存，减少对抗性视觉输入的影响。

Result: DTR在多种VLMs和攻击基准测试中表现优于现有防御方法，兼顾攻击鲁棒性和良性任务性能。

Conclusion: DTR首次成功将键值缓存优化应用于多模态基础模型的安全性增强，效果显著。

Abstract: Large vision-language models (VLMs) are highly vulnerable to jailbreak
attacks that exploit visual-textual interactions to bypass safety guardrails.
In this paper, we present DTR, a novel inference-time defense that mitigates
multimodal jailbreak attacks through optimizing the model's key-value (KV)
caches. Rather than relying on curated safety-specific data or costly
image-to-text conversion, we introduce a new formulation of the safety-relevant
distributional shift induced by the visual modality. This formulation enables
DTR to dynamically adjust visual token weights, minimizing the impact of
adversarial visual inputs while preserving the model's general capabilities and
inference efficiency. Extensive evaluation across diverse VLMs and attack
benchmarks demonstrates that \sys outperforms existing defenses in both attack
robustness and benign task performance, marking the first successful
application of KV cache optimization for safety enhancement in multimodal
foundation models. The code for replicating DTR is available:
https://anonymous.4open.science/r/DTR-2755 (warning: this paper contains
potentially harmful content generated by VLMs.)

</details>


### [460] [EMRA-proxy: Enhancing Multi-Class Region Semantic Segmentation in Remote Sensing Images with Attention Proxy](https://arxiv.org/abs/2505.17665)
*Yichun Yu,Yuqing Lan,Zhihuan Xing,Xiaoyi Yang,Tingyue Tang,Dan Yu*

Key words: 高分辨率遥感图像分割, Transformer, 区域注意力, 全局类别细化

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种新颖的区域感知代理网络（RAPNet），通过结合上下文区域注意力（CRA）和全局类别细化（GCR）模块，解决了高分辨率遥感图像分割中长程依赖和局部细节的问题，实验证明其性能优于现有方法。

Motivation: 高分辨率遥感图像（HRRS）分割面临复杂空间布局和多样物体外观的挑战。传统CNN擅长捕捉局部特征但难以处理长程依赖，而Transformer能建模全局上下文但常忽略局部细节且计算成本高。因此，需要一种兼顾全局和局部信息的高效方法。

Method: RAPNet由两个关键组件组成：CRA模块通过Transformer捕捉区域级上下文依赖，生成语义区域掩码（SRM）；GCR模块学习全局类别注意力图以细化多类信息，结合SRM实现精确分割。

Result: 在三个公共数据集上的实验表明，RAPNet在多类分割任务中超越了现有最优方法。

Conclusion: RAPNet通过区域级操作和全局-局部信息融合，显著提升了高分辨率遥感图像分割的准确性。

Abstract: High-resolution remote sensing (HRRS) image segmentation is challenging due
to complex spatial layouts and diverse object appearances. While CNNs excel at
capturing local features, they struggle with long-range dependencies, whereas
Transformers can model global context but often neglect local details and are
computationally expensive.We propose a novel approach, Region-Aware Proxy
Network (RAPNet), which consists of two components: Contextual Region Attention
(CRA) and Global Class Refinement (GCR). Unlike traditional methods that rely
on grid-based layouts, RAPNet operates at the region level for more flexible
segmentation. The CRA module uses a Transformer to capture region-level
contextual dependencies, generating a Semantic Region Mask (SRM). The GCR
module learns a global class attention map to refine multi-class information,
combining the SRM and attention map for accurate segmentation.Experiments on
three public datasets show that RAPNet outperforms state-of-the-art methods,
achieving superior multi-class segmentation accuracy.

</details>


### [461] [CHAOS: Chart Analysis with Outlier Samples](https://arxiv.org/abs/2505.17235)
*Omar Moured,Yufan Chen,Ruiping Liu,Simon Reiß,Philip Torr,Jiaming Zhang,Rainer Stiefelhagen*

Key words: 图表分析, 多模态大语言模型, 鲁棒性评估, CHAOS基准

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了CHAOS基准，用于评估多模态大语言模型在图表干扰下的鲁棒性，包含多种干扰类型和严重程度，并测试了13种模型。

Motivation: 现实中图表常带有噪声或干扰，现有模型难以处理‘异常图表’，需系统性评估其鲁棒性。

Method: 设计CHAOS基准，包含5类文本和10类视觉干扰，分三个难度级别，测试13种MLLMs在ChartQA和Chart-to-Text任务上的表现。

Result: 实验揭示了模型对不同干扰的鲁棒性差异，为图表理解领域提供参考。

Conclusion: CHAOS为图表干扰下的模型评估提供了标准化方法，推动未来研究。

Abstract: Charts play a critical role in data analysis and visualization, yet
real-world applications often present charts with challenging or noisy
features. However, "outlier charts" pose a substantial challenge even for
Multimodal Large Language Models (MLLMs), which can struggle to interpret
perturbed charts. In this work, we introduce CHAOS (CHart Analysis with Outlier
Samples), a robustness benchmark to systematically evaluate MLLMs against chart
perturbations. CHAOS encompasses five types of textual and ten types of visual
perturbations, each presented at three levels of severity (easy, mid, hard)
inspired by the study result of human evaluation. The benchmark includes 13
state-of-the-art MLLMs divided into three groups (i.e., general-, document-,
and chart-specific models) according to the training scope and data.
Comprehensive analysis involves two downstream tasks (ChartQA and
Chart-to-Text). Extensive experiments and case studies highlight critical
insights into robustness of models across chart perturbations, aiming to guide
future research in chart understanding domain. Data and code are publicly
available at: http://huggingface.co/datasets/omoured/CHAOS.

</details>


### [462] [ViP$^2$-CLIP: Visual-Perception Prompting with Unified Alignment for Zero-Shot Anomaly Detection](https://arxiv.org/abs/2505.17692)
*Ziteng Yang,Jingzehua Xu,Yanshu Li,Zepeng Li,Yeqiang Wang,Xinghui Li*

Key words: 

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: ViP$^{2}$-CLIP introduces a Visual-Perception Prompting (ViP-Prompt) mechanism to enhance zero-shot anomaly detection by adaptively generating fine-grained prompts, eliminating manual templates and class-name dependencies.

Motivation: Existing methods in zero-shot anomaly detection rely on handcrafted or static prompts, which are either costly or fail to adapt to diverse anomalies. CLIP's sensitivity to class names further limits prompting strategies.

Method: ViP$^{2}$-CLIP uses a Visual-Perception Prompting mechanism that combines global and multi-scale local visual context to generate fine-grained textual prompts dynamically.

Result: The model achieves state-of-the-art performance on 15 industrial and medical benchmarks with robust cross-domain generalization.

Conclusion: ViP$^{2}$-CLIP effectively addresses the limitations of prior methods by focusing on precise abnormal regions without relying on manual templates or class labels.

Abstract: Zero-shot anomaly detection (ZSAD) aims to detect anomalies without any
target domain training samples, relying solely on external auxiliary data.
Existing CLIP-based methods attempt to activate the model's ZSAD potential via
handcrafted or static learnable prompts. The former incur high engineering
costs and limited semantic coverage, whereas the latter apply identical
descriptions across diverse anomaly types, thus fail to adapt to complex
variations. Furthermore, since CLIP is originally pretrained on large-scale
classification tasks, its anomaly segmentation quality is highly sensitive to
the exact wording of class names, severely constraining prompting strategies
that depend on class labels. To address these challenges, we introduce
ViP$^{2}$-CLIP. The key insight of ViP$^{2}$-CLIP is a Visual-Perception
Prompting (ViP-Prompt) mechanism, which fuses global and multi-scale local
visual context to adaptively generate fine-grained textual prompts, eliminating
manual templates and class-name priors. This design enables our model to focus
on precise abnormal regions, making it particularly valuable when category
labels are ambiguous or privacy-constrained. Extensive experiments on 15
industrial and medical benchmarks demonstrate that ViP$^{2}$-CLIP achieves
state-of-the-art performance and robust cross-domain generalization.

</details>


### [463] [Seek-CAD: A Self-refined Generative Modeling for 3D Parametric CAD Using Local Inference via DeepSeek](https://arxiv.org/abs/2505.17702)
*Xueyang Li,Jiahao Li,Yu Song,Yunzhong Lou,Xiangdong Zhou*

Key words: CAD生成模型, 开源LLM, DeepSeek-R1, 视觉反馈, CoT反馈

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文介绍了一种名为Seek-CAD的训练免费方法，利用开源LLM DeepSeek-R1本地部署生成CAD参数模型，并通过视觉和CoT反馈机制优化模型生成。

Motivation: 当前封闭源代码LLM在CAD参数模型生成中成本高且本地部署受限，因此探索开源LLM的本地部署方案，以提高灵活性和效率。

Method: 结合视觉和CoT反馈机制，利用DeepSeek-R1生成CAD模型，并通过渲染图像和VLM评估及优化模型。

Result: 实验验证了Seek-CAD的有效性，并展示了一个基于SSR设计范式的3D CAD模型数据集。

Conclusion: Seek-CAD为CAD参数模型生成提供了一种高效、灵活的本地部署解决方案。

Abstract: The advent of Computer-Aided Design (CAD) generative modeling will
significantly transform the design of industrial products. The recent research
endeavor has extended into the realm of Large Language Models (LLMs). In
contrast to fine-tuning methods, training-free approaches typically utilize the
advanced closed-source LLMs, thereby offering enhanced flexibility and
efficiency in the development of AI agents for generating CAD parametric
models. However, the substantial cost and limitations of local deployment of
the top-tier closed-source LLMs pose challenges in practical applications. The
Seek-CAD is the pioneer exploration of locally deployed open-source inference
LLM DeepSeek-R1 for CAD parametric model generation with a training-free
methodology. This study is the first investigation to incorporate both visual
and Chain-of-Thought (CoT) feedback within the self-refinement mechanism for
generating CAD models. Specifically, the initial generated parametric CAD model
is rendered into a sequence of step-wise perspective images, which are
subsequently processed by a Vision Language Model (VLM) alongside the
corresponding CoTs derived from DeepSeek-R1 to assess the CAD model generation.
Then, the feedback is utilized by DeepSeek-R1 to refine the initial generated
model for the next round of generation. Moreover, we present an innovative 3D
CAD model dataset structured around the SSR (Sketch, Sketch-based feature, and
Refinements) triple design paradigm. This dataset encompasses a wide range of
CAD commands, thereby aligning effectively with industrial application
requirements and proving suitable for the generation of LLMs. Extensive
experiments validate the effectiveness of Seek-CAD under various metrics.

</details>


### [464] [Slot-MLLM: Object-Centric Visual Tokenization for Multimodal LLM](https://arxiv.org/abs/2505.17726)
*Donghwan Chi,Hyomin Kim,Yoonjin Oh,Yongjin Kim,Donghoon Lee,Daejin Jo,Jongmin Kim,Junyeob Baek,Sungjin Ahn,Sungwoong Kim*

Key words: 多模态大型语言模型, Slot Attention, 视觉分词器, 对象中心, 视觉语言任务

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种基于Slot Attention的对象中心视觉分词器（Slot-MLLM），用于多模态大型语言模型（MLLMs），以提升对局部视觉细节的理解和生成能力，显著优于现有方法。

Motivation: 现有MLLMs的图像分词方法仅能捕捉全局抽象概念或均匀分段的图像块，限制了模型在对象级别理解和生成详细视觉内容的能力。

Method: 基于Q-Former编码器、扩散解码器和残差向量量化，提出了一种离散的Slot Tokens，既能编码局部视觉细节，又能保持高层语义并与文本数据对齐。

Result: Slot-MLLM在多种需要局部细节理解和生成的视觉语言任务中表现显著优于基线模型。

Conclusion: 该研究首次证明了在MLLMs和自然图像中实现对象中心Slot Attention的可行性。

Abstract: Recently, multimodal large language models (MLLMs) have emerged as a key
approach in achieving artificial general intelligence. In particular,
vision-language MLLMs have been developed to generate not only text but also
visual outputs from multimodal inputs. This advancement requires efficient
image tokens that LLMs can process effectively both in input and output.
However, existing image tokenization methods for MLLMs typically capture only
global abstract concepts or uniformly segmented image patches, restricting
MLLMs' capability to effectively understand or generate detailed visual
content, particularly at the object level. To address this limitation, we
propose an object-centric visual tokenizer based on Slot Attention specifically
for MLLMs. In particular, based on the Q-Former encoder, diffusion decoder, and
residual vector quantization, our proposed discretized slot tokens can encode
local visual details while maintaining high-level semantics, and also align
with textual data to be integrated seamlessly within a unified next-token
prediction framework of LLMs. The resulting Slot-MLLM demonstrates significant
performance improvements over baselines with previous visual tokenizers across
various vision-language tasks that entail local detailed comprehension and
generation. Notably, this work is the first demonstration of the feasibility of
object-centric slot attention performed with MLLMs and in-the-wild natural
images.

</details>


### [465] [RQR3D: Reparametrizing the regression targets for BEV-based 3D object detection](https://arxiv.org/abs/2505.17732)
*Ozsel Kilinc,Cem Tarhan*

Key words: 3D物体检测、鸟瞰图、限制四边形表示、雷达融合、自动驾驶

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种称为RQR3D的新方法，通过限制四边形表示来解决BEV-based 3D物体检测中的角度不连续性问题，结合雷达融合提升了性能。

Motivation: 为了解决鸟瞰图（BEV）3D物体检测中角度表示导致的损失函数不连续性问题，并提升检测精度和实用性。

Method: 采用限制四边形表示（RQR3D），将定向检测问题转化为关键点回归任务，并结合雷达融合的简化处理。

Result: 在nuScenes数据集上，NDS和mAP分别提升4%和2.4%，显著减少了平移和方向误差。

Conclusion: RQR3D方法在精度和实用性上表现出色，适用于自动驾驶的实际需求。

Abstract: Accurate, fast, and reliable 3D perception is essential for autonomous
driving. Recently, bird's-eye view (BEV)-based perception approaches have
emerged as superior alternatives to perspective-based solutions, offering
enhanced spatial understanding and more natural outputs for planning. Existing
BEV-based 3D object detection methods, typically adhering to angle-based
representation, directly estimate the size and orientation of rotated bounding
boxes. We observe that BEV-based 3D object detection is analogous to aerial
oriented object detection, where angle-based methods are recognized for being
affected by discontinuities in their loss functions. Drawing inspiration from
this domain, we propose Restricted Quadrilateral Representation to define 3D
regression targets. RQR3D regresses the smallest horizontal bounding box
encapsulating the oriented box, along with the offsets between the corners of
these two boxes, thereby transforming the oriented object detection problem
into a keypoint regression task. RQR3D is compatible with any 3D object
detection approach. We employ RQR3D within an anchor-free single-stage object
detection method and introduce an objectness head to address class imbalance
problem. Furthermore, we introduce a simplified radar fusion backbone that
eliminates the need for voxel grouping and processes the BEV-mapped point cloud
with standard 2D convolutions, rather than sparse convolutions. Extensive
evaluations on the nuScenes dataset demonstrate that RQR3D achieves
state-of-the-art performance in camera-radar 3D object detection, outperforming
the previous best method by +4% in NDS and +2.4% in mAP, and significantly
reducing the translation and orientation errors, which are crucial for safe
autonomous driving. These consistent gains highlight the robustness, precision,
and real-world readiness of our approach.

</details>


### [466] [Pixels Versus Priors: Controlling Knowledge Priors in Vision-Language Models through Visual Counterfacts](https://arxiv.org/abs/2505.17127)
*Michal Golovanevsky,William Rudman,Michael Lepori,Amir Bar,Ritambhara Singh,Carsten Eickhoff*

Key words: Multimodal Large Language Models, visual reasoning, counterfactual, PvP, activation steering

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该研究通过数据集Visual CounterFact探究多模态大模型（MLLMs）依赖记忆知识还是视觉输入，发现视觉证据最终覆盖记忆先验，并提出了PvP调控方法，成功改变模型输出倾向。

Motivation: 探究多模态大模型在视觉问答任务中是否更依赖记忆的世界知识还是输入图像的视觉信息，以揭示其推理机制。

Method: 引入Visual CounterFact数据集（视觉反事实样本），通过PvP（Pixels Versus Priors）激活向量调控模型输出倾向。

Result: 模型预测早期依赖记忆先验，但中后期转向视觉证据；PvP成功将92.5%颜色和74.6%尺寸预测从先验转向反事实。

Conclusion: 揭示了模态间竞争机制，并提出PvP作为解释和调控多模态模型事实行为的新工具。

Abstract: Multimodal Large Language Models (MLLMs) perform well on tasks such as visual
question answering, but it remains unclear whether their reasoning relies more
on memorized world knowledge or on the visual information present in the input
image. To investigate this, we introduce Visual CounterFact, a new dataset of
visually-realistic counterfactuals that put world knowledge priors (e.g, red
strawberry) into direct conflict with visual input (e.g, blue strawberry).
Using Visual CounterFact, we show that model predictions initially reflect
memorized priors, but shift toward visual evidence in mid-to-late layers. This
dynamic reveals a competition between the two modalities, with visual input
ultimately overriding priors during evaluation. To control this behavior, we
propose Pixels Versus Priors (PvP) steering vectors, a mechanism for
controlling model outputs toward either world knowledge or visual input through
activation-level interventions. On average, PvP successfully shifts 92.5% of
color and 74.6% of size predictions from priors to counterfactuals. Together,
these findings offer new tools for interpreting and controlling factual
behavior in multimodal models.

</details>


### [467] [Debiasing CLIP: Interpreting and Correcting Bias in Attention Heads](https://arxiv.org/abs/2505.17425)
*Wei Jie Yeo,Rui Mao,Moloud Abdar,Erik Cambria,Ranjan Satapathy*

Key words: CLIP, Locate-Then-Correct, Vision Transformers, 虚假注意力头, 零样本学习

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了Locate-Then-Correct (LTC)框架，通过对比学习识别并修正Vision Transformers中的虚假注意力头，提升模型在零样本任务中的性能。

Motivation: 解决CLIP等多模态模型在学习中无意间产生的目标变量与干扰因素之间的虚假关联问题。

Method: 引入LTC框架，通过机制性分析识别虚假注意力头并通过针对性剪枝进行修正，同时利用正交投影整合判别性特征。

Result: 在存在背景和性别偏见的基准测试中，LTC比非训练的后处理方法提升了超过50%的最差组准确率。

Conclusion: LTC通过对比机制有效识别和修正虚假注意力头，并整合判别性特征提升分类性能。

Abstract: Multimodal models like CLIP have gained significant attention due to their
remarkable zero-shot performance across various tasks. However, studies have
revealed that CLIP can inadvertently learn spurious associations between target
variables and confounding factors. To address this, we introduce
\textsc{Locate-Then-Correct} (LTC), a contrastive framework that identifies
spurious attention heads in Vision Transformers via mechanistic insights and
mitigates them through targeted ablation. Furthermore, LTC identifies salient,
task-relevant attention heads, enabling the integration of discriminative
features through orthogonal projection to improve classification performance.
We evaluate LTC on benchmarks with inherent background and gender biases,
achieving over a $>50\%$ gain in worst-group accuracy compared to non-training
post-hoc baselines. Additionally, we visualize the representation of selected
heads and find that the presented interpretation corroborates our contrastive
mechanism for identifying both spurious and salient attention heads. Code
available at https://github.com/wj210/CLIP_LTC.

</details>


### [468] [DetailFusion: A Dual-branch Framework with Detail Enhancement for Composed Image Retrieval](https://arxiv.org/abs/2505.17796)
*Yuxin Yang,Yinan Zhou,Yuxin Chen,Ziqi Zhang,Zongyang Ma,Chunfeng Yuan,Bing Li,Lin Song,Jun Gao,Peng Li,Weiming Hu*

Key words: 组合图像检索, 细节融合, 双分支框架, 自适应特征组合, 跨域适应性

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了DetailFusion框架，通过双分支结构协调全局和细节信息，提升组合图像检索（CIR）的性能。

Motivation: 现有方法因忽视细节信息，难以处理细微视觉变化或复杂文本指令。

Method: 采用双分支框架（Detail-oriented Inference Branch和Adaptive Feature Compositor），结合图像编辑数据集的先验和细节优化策略。

Result: 在CIRR和FashionIQ数据集上实现SOTA性能，验证了细节增强的有效性和跨域适应性。

Conclusion: DetailFusion通过全局与细节信息的动态融合，显著提升了CIR任务的性能。

Abstract: Composed Image Retrieval (CIR) aims to retrieve target images from a gallery
based on a reference image and modification text as a combined query. Recent
approaches focus on balancing global information from two modalities and encode
the query into a unified feature for retrieval. However, due to insufficient
attention to fine-grained details, these coarse fusion methods often struggle
with handling subtle visual alterations or intricate textual instructions. In
this work, we propose DetailFusion, a novel dual-branch framework that
effectively coordinates information across global and detailed granularities,
thereby enabling detail-enhanced CIR. Our approach leverages atomic detail
variation priors derived from an image editing dataset, supplemented by a
detail-oriented optimization strategy to develop a Detail-oriented Inference
Branch. Furthermore, we design an Adaptive Feature Compositor that dynamically
fuses global and detailed features based on fine-grained information of each
unique multimodal query. Extensive experiments and ablation analyses not only
demonstrate that our method achieves state-of-the-art performance on both CIRR
and FashionIQ datasets but also validate the effectiveness and cross-domain
adaptability of detail enhancement for CIR.

</details>


### [469] [Diagnosing Vision Language Models' Perception by Leveraging Human Methods for Color Vision Deficiencies](https://arxiv.org/abs/2505.17461)
*Kazuki Hayashi,Shintaro Ozaki,Yusuke Sakai,Hidetaka Kamigaito,Taro Watanabe*

Key words: 视觉语言模型, 颜色感知, 色觉缺陷, 多模态AI, 感知包容性

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该研究评估了大规模视觉语言模型（LVLMs）在处理个体水平颜色感知差异方面的能力，发现其能解释色觉缺陷（CVDs）但无法模拟CVD患者在图像任务中的感知，凸显了多模态系统需兼顾感知多样性的需求。

Motivation: 颜色感知是视觉理解的基础但个体差异显著（如色觉缺陷、文化和语言差异），但目前感知多样性未得到充分关注。研究旨在评估LVLMs是否能够处理这种个体水平的感知差异。

Method: 使用广泛用于检测色觉缺陷的Ishihara测试，评估LVLMs在解释CVDs和模拟CVD患者感知方面的表现。

Result: LVLMs能够通过自然语言解释CVDs，但无法在图像任务中模拟CVD患者的颜色感知。

Conclusion: 研究强调了多模态系统需考虑颜色感知多样性，以提升感知包容性和AI公平性。

Abstract: Large-scale Vision Language Models (LVLMs) are increasingly being applied to
a wide range of real-world multimodal applications, involving complex visual
and linguistic reasoning. As these models become more integrated into practical
use, they are expected to handle complex aspects of human interaction. Among
these, color perception is a fundamental yet highly variable aspect of visual
understanding. It differs across individuals due to biological factors such as
Color Vision Deficiencies (CVDs), as well as differences in culture and
language. Despite its importance, perceptual diversity has received limited
attention. In our study, we evaluate LVLMs' ability to account for individual
level perceptual variation using the Ishihara Test, a widely used method for
detecting CVDs. Our results show that LVLMs can explain CVDs in natural
language, but they cannot simulate how people with CVDs perceive color in image
based tasks. These findings highlight the need for multimodal systems that can
account for color perceptual diversity and support broader discussions on
perceptual inclusiveness and fairness in multimodal AI.

</details>


### [470] [An Attention Infused Deep Learning System with Grad-CAM Visualization for Early Screening of Glaucoma](https://arxiv.org/abs/2505.17808)
*Ramanathan Swaminathan*

Key words: 深度学习, 卷积神经网络, Vision Transformer, 交叉注意力, 青光眼检测

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该研究结合卷积神经网络和Vision Transformer，通过交叉注意力模块提升模型性能，用于检测青光眼。

Motivation: 探索结合卷积神经网络和Vision Transformer的混合模型，以提升青光眼检测的准确性和效率。

Method: 使用卷积神经网络和Vision Transformer结合交叉注意力模块，并在ACRIMA和Drishti两个数据集上进行实验。

Result: 未具体说明，应为模型在青光眼检测上的表现有所提升。

Conclusion: 混合模型结合了两种技术的优势，可能为青光眼检测提供更高效的解决方案。

Abstract: This research work reveals the eye opening wisdom of the hybrid labyrinthine
deep learning models synergy born out of combining a trailblazing convolutional
neural network with a disruptive Vision Transformer, both intertwined together
with a radical Cross Attention module. Here, two high yielding datasets for
artificial intelligence models in detecting glaucoma, namely ACRIMA and
Drishti, are utilized.

</details>


### [471] [Seeing It or Not? Interpretable Vision-aware Latent Steering to Mitigate Object Hallucinations](https://arxiv.org/abs/2505.17812)
*Boxu Chen,Ziwei Zheng,Le Yang,Zeyu Geng,Zhengyu Zhao,Chenhao Lin,Chao Shen*

Key words: 视觉语言模型,物体幻觉,潜在导向,视觉贡献图,模型鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出VaLSe框架，通过视觉潜在导向策略减少大型视觉语言模型中的物体幻觉问题。

Motivation: 解决大型视觉语言模型中物体幻觉（生成与视觉输入不一致的输出）问题，并深入理解其视觉决策机制。

Method: 采用解释后缓解策略，通过建模视觉语言交互和消除虚假激活，生成视觉贡献图并执行潜在空间导向。

Result: VaLSe显著提升模型抗幻觉能力，并揭示现有评价指标的局限性。

Conclusion: VaLSe是一种有效的解释工具和抗幻觉方法，需更细粒度、可解释的评价基准。

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable success but
continue to struggle with object hallucination (OH), generating outputs
inconsistent with visual inputs. While previous work has proposed methods to
reduce OH, the visual decision-making mechanisms that lead to hallucinations
remain poorly understood. In this paper, we propose VaLSe, a Vision-aware
Latent Steering framework that adopts an interpretation-then-mitigation
strategy to address OH in LVLMs. By tackling dual challenges of modeling
complex vision-language interactions and eliminating spurious activation
artifacts, VaLSe can generate visual contribution maps that trace how specific
visual inputs influence individual output tokens. These maps reveal the model's
vision-aware focus regions, which are then used to perform latent space
steering, realigning internal representations toward semantically relevant
content and reducing hallucinated outputs. Extensive experiments demonstrate
that VaLSe is a powerful interpretability tool and an effective method for
enhancing model robustness against OH across multiple benchmarks. Furthermore,
our analysis uncovers limitations in existing OH evaluation metrics,
underscoring the need for more nuanced, interpretable, and visually grounded OH
benchmarks in future work. Code is available at:
https://github.com/Ziwei-Zheng/VaLSe.

</details>


### [472] [Extending Dataset Pruning to Object Detection: A Variance-based Approach](https://arxiv.org/abs/2505.17245)
*Ryota Yagi*

Key words: 数据集剪枝, 目标检测, VPS, 平均精度

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文首次将分类数据集剪枝技术扩展到目标检测领域，解决了三个关键挑战，并提出新方法VPS，显著提升了性能。

Motivation: 目标检测领域的数据集剪枝研究较少，现有方法难以直接应用，需解决特定挑战以提高效率。

Method: 提出VPS评分方法，结合IoU和置信度分数筛选信息量大的样本，并解决三个关键问题。

Result: 在PASCAL VOC和MS COCO上实验表明，VPS优于现有方法，且样本信息量比数据集规模或平衡更重要。

Conclusion: 该研究为目标检测的数据集剪枝提供了有效方案，推动了复杂视觉任务的高效学习。

Abstract: Dataset pruning -- selecting a small yet informative subset of training data
-- has emerged as a promising strategy for efficient machine learning, offering
significant reductions in computational cost and storage compared to
alternatives like dataset distillation. While pruning methods have shown strong
performance in image classification, their extension to more complex computer
vision tasks, particularly object detection, remains relatively underexplored.
In this paper, we present the first principled extension of classification
pruning techniques to the object detection domain, to the best of our
knowledge. We identify and address three key challenges that hinder this
transition: the Object-Level Attribution Problem, the Scoring Strategy Problem,
and the Image-Level Aggregation Problem. To overcome these, we propose tailored
solutions, including a novel scoring method called Variance-based Prediction
Score (VPS). VPS leverages both Intersection over Union (IoU) and confidence
scores to effectively identify informative training samples specific to
detection tasks. Extensive experiments on PASCAL VOC and MS COCO demonstrate
that our approach consistently outperforms prior dataset pruning methods in
terms of mean Average Precision (mAP). We also show that annotation count and
class distribution shift can influence detection performance, but selecting
informative examples is a more critical factor than dataset size or balance.
Our work bridges dataset pruning and object detection, paving the way for
dataset pruning in complex vision tasks.

</details>


### [473] [Co-Reinforcement Learning for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2505.17534)
*Jingjing Jiang,Chongjie Si,Jun Luo,Hanwang Zhang,Chao Ma*

Key words: 强化学习, 多模态大语言模型, 协同优化, 组相对策略优化, 任务增强

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本研究提出了一种名为CoRL的协同强化学习框架，通过组相对策略优化探索了强化学习在统一多模态大语言模型中的应用，实现了生成与理解能力的共同提升。实验结果显示，模型ULM-R1在多个数据集上取得了显著改进。

Motivation: 当前多模态大语言模型在生成和理解能力协同优化方面存在挑战，本研究旨在探索如何通过强化学习实现两种能力的共同进化。

Method: 提出CoRL框架，包含统一的联合优化阶段和针对特定任务的强化学习阶段，通过组相对策略优化实现多模态大语言模型的协同优化。

Result: 模型ULM-R1在三个文本到图像生成数据集上平均提升了7%，在九个多模态理解基准测试中平均提升了23%。

Conclusion: 研究表明，强化学习能够有效促进多模态大语言模型的跨任务协同与优化，为后续研究提供了新的思路。

Abstract: This paper presents a pioneering exploration of reinforcement learning (RL)
via group relative policy optimization for unified multimodal large language
models (ULMs), aimed at simultaneously reinforcing generation and understanding
capabilities. Through systematic pilot studies, we uncover the significant
potential of ULMs to enable the synergistic co-evolution of dual capabilities
within a shared policy optimization framework. Building on this insight, we
introduce \textbf{CoRL}, a co-reinforcement learning framework comprising a
unified RL stage for joint optimization and a refined RL stage for
task-specific enhancement. With the proposed CoRL, our resulting model,
\textbf{ULM-R1}, achieves average improvements of \textbf{7%} on three
text-to-image generation datasets and \textbf{23%} on nine multimodal
understanding benchmarks. These results demonstrate the effectiveness of CoRL
and highlight the substantial benefit of reinforcement learning in facilitating
cross-task synergy and optimization for ULMs.

</details>


### [474] [Harnessing EHRs for Diffusion-based Anomaly Detection on Chest X-rays](https://arxiv.org/abs/2505.17311)
*Harim Kim,Yuhan Wang,Minkyu Ahn,Heeyoul Choi,Yuyin Zhou,Charmgil Hong*

Key words: 无监督异常检测,扩散模型,多模态,电子健康记录,胸部X光片

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: Diff3M提出了一种结合胸部X光片和结构化电子健康记录的多模态扩散框架，通过图像-EHR交叉注意力模块增强异常检测能力，并在CheXpert和MIMIC-CXR/IV数据集上展现了最先进性能。

Motivation: 现有的基于扩散的无监督异常检测方法仅依赖影像特征，难以区分正常解剖变异与病理异常，因此需要结合临床上下文提升检测效果。

Method: 提出Diff3M框架，集成图像与EHR数据，引入图像-EHR交叉注意力模块和静态掩码策略，优化异常检测。

Result: 在CheXpert和MIMIC-CXR/IV数据集上表现优于现有方法，达到最先进水平。

Conclusion: 多模态数据结合临床上下文能显著提升无监督异常检测的性能。

Abstract: Unsupervised anomaly detection (UAD) in medical imaging is crucial for
identifying pathological abnormalities without requiring extensive labeled
data. However, existing diffusion-based UAD models rely solely on imaging
features, limiting their ability to distinguish between normal anatomical
variations and pathological anomalies. To address this, we propose Diff3M, a
multi-modal diffusion-based framework that integrates chest X-rays and
structured Electronic Health Records (EHRs) for enhanced anomaly detection.
Specifically, we introduce a novel image-EHR cross-attention module to
incorporate structured clinical context into the image generation process,
improving the model's ability to differentiate normal from abnormal features.
Additionally, we develop a static masking strategy to enhance the
reconstruction of normal-like images from anomalies. Extensive evaluations on
CheXpert and MIMIC-CXR/IV demonstrate that Diff3M achieves state-of-the-art
performance, outperforming existing UAD methods in medical imaging. Our code is
available at this http URL https://github.com/nth221/Diff3M

</details>


### [475] [DiffusionReward: Enhancing Blind Face Restoration through Reward Feedback Learning](https://arxiv.org/abs/2505.17910)
*Bin Wu,Wei Wang,Yahui Liu,Zixiang Li,Yao Zhao*

Key words: Reward Feedback Learning, Blind Face Restoration, Diffusion Models, Face Reward Model, Identity Consistency

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一个名为DiffusionReward的Reward Feedback Learning (ReFL)框架，用于盲人脸修复任务，通过Face Reward Model（FRM）提供反馈信号，结合梯度流、正则化项和结构一致性约束，显著提升了修复效果和身份一致性。

Motivation: 当前的扩散方法在盲人脸修复任务中常无法生成真实的细节且身份一致性差，需要一种更有效的框架来克服这些限制并提升性能。

Method: 使用ReFL框架DiffusionReward，通过FRM提供反馈信号，结合梯度流、正则化项和结构一致性约束优化修复网络，并动态更新FRM以防止奖励破解。

Result: 实验表明，该方法在合成和真实数据集上均优于现有技术，显著提高了身份一致性和面部细节。

Conclusion: DiffusionReward为盲人脸修复提供了一种有效的新方法，通过FRM和多目标优化实现了更好的视觉质量和身份一致性。

Abstract: Reward Feedback Learning (ReFL) has recently shown great potential in
aligning model outputs with human preferences across various generative tasks.
In this work, we introduce a ReFL framework, named DiffusionReward, to the
Blind Face Restoration task for the first time. DiffusionReward effectively
overcomes the limitations of diffusion-based methods, which often fail to
generate realistic facial details and exhibit poor identity consistency. The
core of our framework is the Face Reward Model (FRM), which is trained using
carefully annotated data. It provides feedback signals that play a pivotal role
in steering the optimization process of the restoration network. In particular,
our ReFL framework incorporates a gradient flow into the denoising process of
off-the-shelf face restoration methods to guide the update of model parameters.
The guiding gradient is collaboratively determined by three aspects: (i) the
FRM to ensure the perceptual quality of the restored faces; (ii) a
regularization term that functions as a safeguard to preserve generative
diversity; and (iii) a structural consistency constraint to maintain facial
fidelity. Furthermore, the FRM undergoes dynamic optimization throughout the
process. It not only ensures that the restoration network stays precisely
aligned with the real face manifold, but also effectively prevents reward
hacking. Experiments on synthetic and wild datasets demonstrate that our method
outperforms state-of-the-art methods, significantly improving identity
consistency and facial details. The source codes, data, and models are
available at: https://github.com/01NeuralNinja/DiffusionReward.

</details>


### [476] [Object-level Cross-view Geo-localization with Location Enhancement and Multi-Head Cross Attention](https://arxiv.org/abs/2505.17911)
*Zheyang Huang,Jagannath Aryal,Saeid Nahavandi,Xuequan Lu,Chee Peng Lim,Lei Wei,Hailing Zhou*

Key words: 跨视角地理定位,物体级精度,高斯核转换,多头交叉注意力,小样本学习

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了OCGNet，一种基于点击定位的跨视角地理定位网络，通过高斯核转换和模块化设计实现物体级精确定位，并在CVOGL数据集上达到最优性能。

Motivation: 传统的图像级地理定位无法满足搜索救援、基础设施检查等需求，需提升至物体级精度，同时解决视角、时间等差异带来的挑战。

Method: 采用高斯核转换（GKT）保留点击位置信息，结合位置增强（LE）和多头交叉注意力（MHCA）模块优化特征提取与匹配。

Result: OCGNet在CVOGL数据集上取得最优性能，并展示小样本学习能力，适应多样化应用场景。

Conclusion: OCGNet通过创新设计实现了高精度的物体级跨视角地理定位，具备实际应用的潜力。

Abstract: Cross-view geo-localization determines the location of a query image,
captured by a drone or ground-based camera, by matching it to a geo-referenced
satellite image. While traditional approaches focus on image-level
localization, many applications, such as search-and-rescue, infrastructure
inspection, and precision delivery, demand object-level accuracy. This enables
users to prompt a specific object with a single click on a drone image to
retrieve precise geo-tagged information of the object. However, variations in
viewpoints, timing, and imaging conditions pose significant challenges,
especially when identifying visually similar objects in extensive satellite
imagery. To address these challenges, we propose an Object-level Cross-view
Geo-localization Network (OCGNet). It integrates user-specified click locations
using Gaussian Kernel Transfer (GKT) to preserve location information
throughout the network. This cue is dually embedded into the feature encoder
and feature matching blocks, ensuring robust object-specific localization.
Additionally, OCGNet incorporates a Location Enhancement (LE) module and a
Multi-Head Cross Attention (MHCA) module to adaptively emphasize
object-specific features or expand focus to relevant contextual regions when
necessary. OCGNet achieves state-of-the-art performance on a public dataset,
CVOGL. It also demonstrates few-shot learning capabilities, effectively
generalizing from limited examples, making it suitable for diverse applications
(https://github.com/ZheyangH/OCGNet).

</details>


### [477] [Evaluation of Few-Shot Learning Methods for Kidney Stone Type Recognition in Ureteroscopy](https://arxiv.org/abs/2505.17921)
*Carlos Salazar-Ruiz,Francisco Lopez-Tiro,Ivan Reyes-Amezcua,Clement Larose,Gilberto Ochoa-Ruiz,Christian Daul*

Key words: 肾结石分类, 小样本学习, 原型网络, 内窥镜图像, 深度学习

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种基于小样本学习的深度学习方法，能够在训练数据有限的情况下，有效分类内窥镜图像中的肾结石类型。

Motivation: 当前肾结石类型识别方法需要长时间或高度专业化的操作，而现有深度学习模型因数据不足导致性能受限。因此，开发了一种适用于小样本场景的分类方法。

Method: 采用原型网络（Prototypical Networks）进行小样本学习，利用少量训练数据生成判别性特征以实现分类。

Result: 实验表明，仅使用25%的训练数据，原型网络的性能即可达到或优于传统深度学习模型在全量数据上的表现。

Conclusion: 该方法在小样本或罕见类别场景下具有显著优势，为肾结石的实时分类提供了可行解决方案。

Abstract: Determining the type of kidney stones is crucial for prescribing appropriate
treatments to prevent recurrence. Currently, various approaches exist to
identify the type of kidney stones. However, obtaining results through the
reference ex vivo identification procedure can take several weeks, while in
vivo visual recognition requires highly trained specialists. For this reason,
deep learning models have been developed to provide urologists with an
automated classification of kidney stones during ureteroscopies. Nevertheless,
a common issue with these models is the lack of training data. This
contribution presents a deep learning method based on few-shot learning, aimed
at producing sufficiently discriminative features for identifying kidney stone
types in endoscopic images, even with a very limited number of samples. This
approach was specifically designed for scenarios where endoscopic images are
scarce or where uncommon classes are present, enabling classification even with
a limited training dataset. The results demonstrate that Prototypical Networks,
using up to 25% of the training data, can achieve performance equal to or
better than traditional deep learning models trained with the complete dataset.

</details>


### [478] [AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation of Foundation Models](https://arxiv.org/abs/2505.17931)
*Xingjian Li,Qifeng Wu,Colleen Que,Yiran Ding,Adithya S. Ubaradka,Jianhua Xing,Tianyang Wang,Min Xu*

Key words: 医学图像分割、零样本学习、基础模型、测试时自适应、视觉语言模型

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种零样本自动医学图像分割方法，通过结合现成的视觉语言和分割基础模型，无需大量标注或手动提示，实现了高效且可扩展的分割解决方案。

Motivation: 当前深度学习医学图像分割方法依赖大量标注数据或手动提示，效率低下；研究者希望开发一种无需标注且自动化的零样本分割方法。

Method: 结合视觉语言基础模型生成初始边界框，通过视觉提示增强模块优化提示信息，再由提示式分割模型生成最终掩膜；引入测试时自适应框架以解决领域差异问题。

Result: 在七个多样化的医学影像数据集上验证，性能接近需弱提示的交互式基础模型。

Conclusion: 通过分解任务与测试时自适应，该方法为医学图像分割提供了一种高效、可扩展的零样本解决方案。

Abstract: Medical image segmentation is vital for clinical diagnosis, yet current deep
learning methods often demand extensive expert effort, i.e., either through
annotating large training datasets or providing prompts at inference time for
each new case. This paper introduces a zero-shot and automatic segmentation
pipeline that combines off-the-shelf vision-language and segmentation
foundation models. Given a medical image and a task definition (e.g., "segment
the optic disc in an eye fundus image"), our method uses a grounding model to
generate an initial bounding box, followed by a visual prompt boosting module
that enhance the prompts, which are then processed by a promptable segmentation
model to produce the final mask. To address the challenges of domain gap and
result verification, we introduce a test-time adaptation framework featuring a
set of learnable adaptors that align the medical inputs with foundation model
representations. Its hyperparameters are optimized via Bayesian Optimization,
guided by a proxy validation model without requiring ground-truth labels. Our
pipeline offers an annotation-efficient and scalable solution for zero-shot
medical image segmentation across diverse tasks. Our pipeline is evaluated on
seven diverse medical imaging datasets and shows promising results. By proper
decomposition and test-time adaptation, our fully automatic pipeline performs
competitively with weakly-prompted interactive foundation models.

</details>


### [479] [Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding](https://arxiv.org/abs/2505.18079)
*Xiaoyi Zhang,Zhaoyang Jia,Zongyu Guo,Jiahao Li,Bin Li,Houqiang Li,Yan Lu*

Key words: 长视频理解, LLM, 自主代理, 多粒度搜索, 工具调用

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了Deep Video Discovery（DVD）代理模型，通过自主搜索策略处理长视频，利用多粒度视频数据库和LLM的先进推理能力，实现了在长视频理解任务上的最佳性能。

Motivation: 长视频理解因时空复杂性和问答难度而具有挑战性，现有LLM在信息密集的长视频处理中仍有限制，需改进方法以提升理解和处理能力。

Method: DVD代理采用自主搜索策略，利用多粒度视频数据库和工具集，结合LLM的推理能力进行计划、工具选择和参数优化，并通过迭代精炼推理过程。

Result: DVD代理在LVBench等长视频理解基准测试中表现优异，显著优于现有方法，达到了SOTA性能。

Conclusion: DVD代理通过自主搜索和LLM推理有效解决长视频理解问题，其系统设计为未来智能代理的发展提供了有益启示。

Abstract: Long-form video understanding presents significant challenges due to
extensive temporal-spatial complexity and the difficulty of question answering
under such extended contexts. While Large Language Models (LLMs) have
demonstrated considerable advancements in video analysis capabilities and long
context handling, they continue to exhibit limitations when processing
information-dense hour-long videos. To overcome such limitations, we propose
the Deep Video Discovery agent to leverage an agentic search strategy over
segmented video clips. Different from previous video agents manually designing
a rigid workflow, our approach emphasizes the autonomous nature of agents. By
providing a set of search-centric tools on multi-granular video database, our
DVD agent leverages the advanced reasoning capability of LLM to plan on its
current observation state, strategically selects tools, formulates appropriate
parameters for actions, and iteratively refines its internal reasoning in light
of the gathered information. We perform comprehensive evaluation on multiple
long video understanding benchmarks that demonstrates the advantage of the
entire system design. Our DVD agent achieves SOTA performance, significantly
surpassing prior works by a large margin on the challenging LVBench dataset.
Comprehensive ablation studies and in-depth tool analyses are also provided,
yielding insights to further advance intelligent agents tailored for long-form
video understanding tasks. The code will be released later.

</details>


### [480] [One RL to See Them All: Visual Triple Unified Reinforcement Learning](https://arxiv.org/abs/2505.18129)
*Yan Ma,Linge Du,Xuyang Shen,Shaoxiang Chen,Pengfei Li,Qibing Ren,Lizhuang Ma,Yuchao Dai,Pengfei Liu,Junjie Yan*

Key words: 强化学习, 视觉语言模型, 视觉推理, 视觉感知, 动态IoU奖励

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: V-Triune是一个统一视觉推理和感知任务的强化学习系统，通过动态IoU奖励和多样化数据集训练，取得显著性能提升。

Motivation: 探索强化学习在视觉语言模型中除推理任务外的应用，尤其是感知密集型任务如物体检测和接地。

Method: 使用样本级数据格式化、验证器级奖励计算和源级指标监控，结合动态IoU奖励，统一训练视觉推理和感知任务。

Result: Orsta模型在多种任务上表现优异，性能提升显著，7B和32B模型变体分别提升+2.1至+14.1。

Conclusion: V-Triune系统证明了统一强化学习方法在视觉语言模型中的有效性和可扩展性。

Abstract: Reinforcement learning (RL) has significantly advanced the reasoning
capabilities of vision-language models (VLMs). However, the use of RL beyond
reasoning tasks remains largely unexplored, especially for perceptionintensive
tasks like object detection and grounding. We propose V-Triune, a Visual Triple
Unified Reinforcement Learning system that enables VLMs to jointly learn visual
reasoning and perception tasks within a single training pipeline. V-Triune
comprises triple complementary components: Sample-Level Data Formatting (to
unify diverse task inputs), Verifier-Level Reward Computation (to deliver
custom rewards via specialized verifiers) , and Source-Level Metric Monitoring
(to diagnose problems at the data-source level). We further introduce a novel
Dynamic IoU reward, which provides adaptive, progressive, and definite feedback
for perception tasks handled by V-Triune. Our approach is instantiated within
off-the-shelf RL training framework using open-source 7B and 32B backbone
models. The resulting model, dubbed Orsta (One RL to See Them All),
demonstrates consistent improvements across both reasoning and perception
tasks. This broad capability is significantly shaped by its training on a
diverse dataset, constructed around four representative visual reasoning tasks
(Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding,
Detection, Counting, and OCR). Subsequently, Orsta achieves substantial gains
on MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1
across its various 7B and 32B model variants, with performance benefits
extending to a wide range of downstream tasks. These results highlight the
effectiveness and scalability of our unified RL approach for VLMs. The V-Triune
system, along with the Orsta models, is publicly available at
https://github.com/MiniMax-AI.

</details>


### [481] [RestoreVAR: Visual Autoregressive Generation for All-in-One Image Restoration](https://arxiv.org/abs/2505.18047)
*Sudarshan Rajagopalan,Kartik Narayan,Vishal M. Patel*

Key words: latent diffusion models, All-in-One image Restoration, visual autoregressive modeling, cross-attention, inference speed

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: RestoreVAR是一种基于视觉自回归模型(VAR)的图像修复方法，相比扩散模型(LDM)在修复性能上显著提升且推理速度快10倍以上，适用于时间敏感应用。

Motivation: 现有基于LDM的AiOR方法虽然感知质量高，但迭代去噪导致推理速度慢，难以应用于时间敏感场景。

Method: 采用视觉自回归模型(VAR)，结合改进的跨注意力机制和潜在空间细化模块，优化AiOR任务。

Result: RestoreVAR在生成式AiOR方法中达到最优性能，且展示出强泛化能力。

Conclusion: RestoreVAR通过VAR模型和针对性架构改进，在保持高质量修复的同时大幅提升效率。

Abstract: The use of latent diffusion models (LDMs) such as Stable Diffusion has
significantly improved the perceptual quality of All-in-One image Restoration
(AiOR) methods, while also enhancing their generalization capabilities.
However, these LDM-based frameworks suffer from slow inference due to their
iterative denoising process, rendering them impractical for time-sensitive
applications. To address this, we propose RestoreVAR, a novel generative
approach for AiOR that significantly outperforms LDM-based models in
restoration performance while achieving over $\mathbf{10\times}$ faster
inference. RestoreVAR leverages visual autoregressive modeling (VAR), a
recently introduced approach which performs scale-space autoregression for
image generation. VAR achieves comparable performance to that of
state-of-the-art diffusion transformers with drastically reduced computational
costs. To optimally exploit these advantages of VAR for AiOR, we propose
architectural modifications and improvements, including intricately designed
cross-attention mechanisms and a latent-space refinement module, tailored for
the AiOR task. Extensive experiments show that RestoreVAR achieves
state-of-the-art performance among generative AiOR methods, while also
exhibiting strong generalization capabilities.

</details>


### [482] [FDBPL: Faster Distillation-Based Prompt Learning for Region-Aware Vision-Language Models Adaptation](https://arxiv.org/abs/2505.18053)
*Zherui Zhang,Jiaxin Wu,Changwei Wang,Rongtao Xu,Longzhao Huang,Wenhao Xu,Wenbo Xu,Li Guo,Shibiao Xu*

Key words: 提示学习, 视觉-语言模型, 蒸馏学习, 参数效率, 零样本学习

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了FDBPL方法，通过共享软监督上下文和加速I/O改进基于蒸馏的提示学习，同时引入区域感知的提示学习范式，以平衡参数效率和泛化能力。

Motivation: 现有的提示学习方法在泛化和效率上存在不足，FDBPL旨在解决这些问题，同时保持参数高效和强泛化能力。

Method: FDBPL通过共享软监督上下文、加速I/O、以及双正负提示空间的区域感知学习机制，结合相似性-差异学习改进零样本性能。

Result: 在11个数据集上的评估显示，FDBPL在基础到新任务的泛化、跨数据集迁移和鲁棒性测试中表现优异，训练速度提升2.2倍。

Conclusion: FDBPL在参数效率和泛化能力上均优于现有方法，适用于视觉-语言模型的高效适配。

Abstract: Prompt learning as a parameter-efficient method that has been widely adopted
to adapt Vision-Language Models (VLMs) to downstream tasks. While hard-prompt
design requires domain expertise and iterative optimization, soft-prompt
methods rely heavily on task-specific hard labels, limiting their
generalization to unseen categories. Recent popular distillation-based prompt
learning methods improve generalization by exploiting larger teacher VLMs and
unsupervised knowledge transfer, yet their repetitive teacher model online
inference sacrifices the inherent training efficiency advantage of prompt
learning. In this paper, we propose {{\large {\textbf{F}}}}aster {{\large
{\textbf{D}}}}istillation-{{\large {\textbf{B}}}}ased {{\large
{\textbf{P}}}}rompt {{\large {\textbf{L}}}}earning (\textbf{FDBPL}), which
addresses these issues by sharing soft supervision contexts across multiple
training stages and implementing accelerated I/O. Furthermore, FDBPL introduces
a region-aware prompt learning paradigm with dual positive-negative prompt
spaces to fully exploit randomly cropped regions that containing multi-level
information. We propose a positive-negative space mutual learning mechanism
based on similarity-difference learning, enabling student CLIP models to
recognize correct semantics while learning to reject weakly related concepts,
thereby improving zero-shot performance. Unlike existing distillation-based
prompt learning methods that sacrifice parameter efficiency for generalization,
FDBPL maintains dual advantages of parameter efficiency and strong downstream
generalization. Comprehensive evaluations across 11 datasets demonstrate
superior performance in base-to-new generalization, cross-dataset transfer, and
robustness tests, achieving $2.2\times$ faster training speed.

</details>


### [483] [CXReasonBench: A Benchmark for Evaluating Structured Diagnostic Reasoning in Chest X-rays](https://arxiv.org/abs/2505.18087)
*Hyungyung Lee,Geon Choi,Jung-Oh Lee,Hangyul Yoon,Hyuk Gi Hong,Edward Choi*

Key words: 大型视觉语言模型,医学诊断,临床推理,基准测试,CXReasonBench

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了CheXStruct和CXReasonBench，一个基于MIMIC-CXR-JPG数据集的结构化流程与基准测试，用于评估大型视觉语言模型在医学任务中的临床推理能力。

Motivation: 现有基准测试主要关注最终诊断结果，缺乏对模型是否进行临床有意义推理的深入分析。

Method: 通过CheXStruct自动从胸部X光片中提取中间推理步骤，如解剖区域分割、诊断测量等，并利用CXReasonBench评估模型的临床推理能力。

Result: 评估的10个大型视觉语言模型在结构化推理和泛化能力上表现不佳，难以将抽象知识与视觉解释结合。

Conclusion: 提出了一种细粒度、透明的临床推理评估方法，揭示了当前模型的局限性。

Abstract: Recent progress in Large Vision-Language Models (LVLMs) has enabled promising
applications in medical tasks, such as report generation and visual question
answering. However, existing benchmarks focus mainly on the final diagnostic
answer, offering limited insight into whether models engage in clinically
meaningful reasoning. To address this, we present CheXStruct and CXReasonBench,
a structured pipeline and benchmark built on the publicly available
MIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of
intermediate reasoning steps directly from chest X-rays, such as segmenting
anatomical regions, deriving anatomical landmarks and diagnostic measurements,
computing diagnostic indices, and applying clinical thresholds. CXReasonBench
leverages this pipeline to evaluate whether models can perform clinically valid
reasoning steps and to what extent they can learn from structured guidance,
enabling fine-grained and transparent assessment of diagnostic reasoning. The
benchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases,
each paired with up to 4 visual inputs, and supports multi-path, multi-stage
evaluation including visual grounding via anatomical region selection and
diagnostic measurements. Even the strongest of 10 evaluated LVLMs struggle with
structured reasoning and generalization, often failing to link abstract
knowledge with anatomically grounded visual interpretation. The code is
available at https://github.com/ttumyche/CXReasonBench

</details>


### [484] [U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding](https://arxiv.org/abs/2505.17779)
*Anjie Le,Henan Liu,Yue Wang,Zhenyu Liu,Rongkun Zhu,Taohan Weng,Jinze Yu,Boyang Wang,Yalun Wu,Kaiwen Yan,Quanlin Sun,Meirui Jiang,Jialun Pei,Siya Liu,Haoyun Zheng,Zhoujun Li,Alison Noble,Jacques Souquet,Xiaoqing Guo,Manxi Lin,Hongcheng Guo*

Key words: 超声影像, 视觉语言模型, 基准测试, 多模态任务, 临床应用

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了U2-BENCH，首个全面评估大型视觉语言模型（LVLM）在超声影像理解中的基准测试，覆盖分类、检测、回归和文本生成任务，揭示了LVLM在图像分类表现良好但在空间推理和临床语言生成上的挑战。

Motivation: 超声影像解读因操作者、噪声和解剖结构的差异而具有挑战性，尽管LVLM在多模态任务中表现优异，但其在超声领域的性能尚未充分研究。

Method: 开发了U2-BENCH基准测试，包含7,241个案例，覆盖15个解剖区域和50种超声应用场景，定义8个临床任务，评估20种领先的LVLM。

Result: LVLM在图像分类任务中表现优异，但在空间推理和临床语言生成方面仍有明显不足。

Conclusion: U2-BENCH为评估和推动LVLM在医学超声领域的多模态研究提供了严谨统一的测试平台。

Abstract: Ultrasound is a widely-used imaging modality critical to global healthcare,
yet its interpretation remains challenging due to its varying image quality on
operators, noises, and anatomical structures. Although large vision-language
models (LVLMs) have demonstrated impressive multimodal capabilities across
natural and medical domains, their performance on ultrasound remains largely
unexplored. We introduce U2-BENCH, the first comprehensive benchmark to
evaluate LVLMs on ultrasound understanding across classification, detection,
regression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning
15 anatomical regions and defines 8 clinically inspired tasks, such as
diagnosis, view recognition, lesion localization, clinical value estimation,
and report generation, across 50 ultrasound application scenarios. We evaluate
20 state-of-the-art LVLMs, both open- and closed-source, general-purpose and
medical-specific. Our results reveal strong performance on image-level
classification, but persistent challenges in spatial reasoning and clinical
language generation. U2-BENCH establishes a rigorous and unified testbed to
assess and accelerate LVLM research in the uniquely multimodal domain of
medical ultrasound imaging.

</details>


### [485] [Hyperspectral Anomaly Detection Fused Unified Nonconvex Tensor Ring Factors Regularization](https://arxiv.org/abs/2505.17881)
*Wenjin Qin,Hailin Wang,Hao Shu,Feng Zhang,Jianjun Wang,Xiangyong Cao,Xi-Le Zhao,Gemine Vivone*

Key words: 高光谱异常检测，张量环分解，非凸正则化，ADMM，TSVD

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种名为HAD-EUNTRFR的新型高光谱异常检测方法，利用增强的统一非凸张量环分解和正则化技术优化背景和异常分离，实验证明其优于现有方法。

Motivation: 现有高光谱异常检测方法未充分利用背景的全局相关性和局部平滑性，导致检测性能不佳。因此，需要一种能同时捕捉空间-频谱相关性并优化背景和异常分离的方法。

Method: 采用张量环分解捕捉背景的空间-频谱相关性，引入基于TSVD的非凸正则化器编码低秩性和稀疏性，并设计非凸正则项利用异常的分组稀疏性，通过ADMM框架高效求解模型。

Result: 在多个基准数据集上的实验表明，HAD-EUNTRFR在检测精度上优于现有最先进方法。

Conclusion: HAD-EUNTRFR通过改进的张量分解和正则化技术，显著提升了高光谱异常检测性能。

Abstract: In recent years, tensor decomposition-based approaches for hyperspectral
anomaly detection (HAD) have gained significant attention in the field of
remote sensing. However, existing methods often fail to fully leverage both the
global correlations and local smoothness of the background components in
hyperspectral images (HSIs), which exist in both the spectral and spatial
domains. This limitation results in suboptimal detection performance. To
mitigate this critical issue, we put forward a novel HAD method named
HAD-EUNTRFR, which incorporates an enhanced unified nonconvex tensor ring (TR)
factors regularization. In the HAD-EUNTRFR framework, the raw HSIs are first
decomposed into background and anomaly components. The TR decomposition is then
employed to capture the spatial-spectral correlations within the background
component. Additionally, we introduce a unified and efficient nonconvex
regularizer, induced by tensor singular value decomposition (TSVD), to
simultaneously encode the low-rankness and sparsity of the 3-D gradient TR
factors into a unique concise form. The above characterization scheme enables
the interpretable gradient TR factors to inherit the low-rankness and
smoothness of the original background. To further enhance anomaly detection, we
design a generalized nonconvex regularization term to exploit the group
sparsity of the anomaly component. To solve the resulting doubly nonconvex
model, we develop a highly efficient optimization algorithm based on the
alternating direction method of multipliers (ADMM) framework. Experimental
results on several benchmark datasets demonstrate that our proposed method
outperforms existing state-of-the-art (SOTA) approaches in terms of detection
accuracy.

</details>


### [486] [Mind the Domain Gap: Measuring the Domain Gap Between Real-World and Synthetic Point Clouds for Automated Driving Development](https://arxiv.org/abs/2505.17959)
*Nguyen Duc,Yan-Ling Lai,Patrick Madlindl,Xinyuan Zhu,Benedikt Schwab,Olaf Wysocki,Ludwig Hoegner,Thomas H. Kolbe*

Key words: 

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文提出了一种新方法DoGSS-PCL来测量现实传感器数据与模拟数据之间的领域差距，支持全面的领域差距分析，并验证了合成语义点云可用于训练深度神经网络。

Motivation: 由于长尾数据分布问题，模拟无领域差距的合成数据在机器人、摄影测量和计算机视觉研究中至关重要。现有方法通常仅针对单一场景，无法全面分析领域差距的来源。

Method: 提出了一种新度量标准DoGSS-PCL，用于评估模拟点云的几何和语义质量。

Result: 实验证实该方法和度量标准可有效测量领域差距，并验证了合成语义点云在50/50真实与合成比例下训练深度神经网络的性能。

Conclusion: 本研究为可信数据模拟提供了新方法，有望推动自动驾驶测试和数字孪生的大规模应用。

Abstract: Owing to the typical long-tail data distribution issues, simulating
domain-gap-free synthetic data is crucial in robotics, photogrammetry, and
computer vision research. The fundamental challenge pertains to credibly
measuring the difference between real and simulated data. Such a measure is
vital for safety-critical applications, such as automated driving, where
out-of-domain samples may impact a car's perception and cause fatal accidents.
Previous work has commonly focused on simulating data on one scene and
analyzing performance on a different, real-world scene, hampering the disjoint
analysis of domain gap coming from networks' deficiencies, class definitions,
and object representation. In this paper, we propose a novel approach to
measuring the domain gap between the real world sensor observations and
simulated data representing the same location, enabling comprehensive domain
gap analysis. To measure such a domain gap, we introduce a novel metric
DoGSS-PCL and evaluation assessing the geometric and semantic quality of the
simulated point cloud. Our experiments corroborate that the introduced approach
can be used to measure the domain gap. The tests also reveal that synthetic
semantic point clouds may be used for training deep neural networks,
maintaining the performance at the 50/50 real-to-synthetic ratio. We strongly
believe that this work will facilitate research on credible data simulation and
allow for at-scale deployment in automated driving testing and digital
twinning.

</details>


### [487] [MR-EEGWaveNet: Multiresolutional EEGWaveNet for Seizure Detection from Long EEG Recordings](https://arxiv.org/abs/2505.17972)
*Kazi Mahmudul Hassan,Xuyang Zhao,Hidenori Sugano,Toshihisa Tanaka*

Key words: 癫痫检测，EEG，多分辨率，MR-EEGWaveNet，特征提取

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种名为“多分辨率EEGWaveNet（MR-EEGWaveNet）”的新型端到端模型，用于区分癫痫发作与背景EEG及伪影/噪声，显著提升了分类性能。

Motivation: 现有的癫痫检测模型在训练数据和伪影区分方面表现不稳定。本研究旨在解决这一问题，提出了一种更高效的模型。

Method: 模型包含三个模块：卷积模块提取特征，特征提取模块降维，预测器模块分类。还引入了基于异常分数的后分类处理技术以减少误报。

Result: 实验结果显示，MR-EEGWaveNet在Siena和Juntendo数据集上的F1分数和精度均有显著提升。

Conclusion: 多分辨率方法显著优于传统方法，表明其在癫痫检测中的潜力。

Abstract: Feature engineering for generalized seizure detection models remains a
significant challenge. Recently proposed models show variable performance
depending on the training data and remain ineffective at accurately
distinguishing artifacts from seizure data. In this study, we propose a novel
end-to-end model, ''Multiresolutional EEGWaveNet (MR-EEGWaveNet),'' which
efficiently distinguishes seizure events from background electroencephalogram
(EEG) and artifacts/noise by capturing both temporal dependencies across
different time frames and spatial relationships between channels. The model has
three modules: convolution, feature extraction, and predictor. The convolution
module extracts features through depth-wise and spatio-temporal convolution.
The feature extraction module individually reduces the feature dimension
extracted from EEG segments and their sub-segments. Subsequently, the extracted
features are concatenated into a single vector for classification using a fully
connected classifier called the predictor module. In addition, an anomaly
score-based post-classification processing technique was introduced to reduce
the false-positive rates of the model. Experimental results were reported and
analyzed using different parameter settings and datasets (Siena (public) and
Juntendo (private)). The proposed MR-EEGWaveNet significantly outperformed the
conventional non-multiresolution approach, improving the F1 scores from 0.177
to 0.336 on Siena and 0.327 to 0.488 on Juntendo, with precision gains of 15.9%
and 20.62%, respectively.

</details>


### [488] [To Glue or Not to Glue? Classical vs Learned Image Matching for Mobile Mapping Cameras to Textured Semantic 3D Building Models](https://arxiv.org/abs/2505.17973)
*Simone Gaisbauer,Prabin Gyawali,Qilin Zhang,Olaf Wysocki,Boris Jutzi*

Key words: 特征匹配, 视觉定位, SIFT, 深度学习, PnP算法

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文系统比较了传统手工特征匹配与可学习的深度学习方法在视觉定位任务中的表现，特别是在建筑立面匹配场景下。结果显示深度学习方法在精度和鲁棒性上显著优于传统方法。

Motivation: 传统手工特征匹配方法（如SIFT）虽被广泛使用，但在复杂条件下性能有限。随着深度学习的发展，可学习方法展现出更高的鲁棒性和性能，但缺乏在特定任务（如语义3D建筑匹配）中的系统性比较。本文旨在填补这一空白。

Method: 研究使用了标准数据集（HPatches、MegaDepth-1500）和自定义数据集（立面纹理与相机图像），通过PnP算法评估绝对位姿估计的精度，几何真值来自地理参考轨迹数据。

Result: 在自定义数据集上，可学习方法显著优于传统方法，表现为更高的匹配准确性和鲁棒性（RANSAC内点数和AUC指标）。

Conclusion: 可学习的特征匹配方法在视觉定位任务中更具优势，有望推动基于模型的视觉定位技术的发展。

Abstract: Feature matching is a necessary step for many computer vision and
photogrammetry applications such as image registration, structure-from-motion,
and visual localization. Classical handcrafted methods such as SIFT feature
detection and description combined with nearest neighbour matching and RANSAC
outlier removal have been state-of-the-art for mobile mapping cameras. With
recent advances in deep learning, learnable methods have been introduced and
proven to have better robustness and performance under complex conditions.
Despite their growing adoption, a comprehensive comparison between classical
and learnable feature matching methods for the specific task of semantic 3D
building camera-to-model matching is still missing. This submission
systematically evaluates the effectiveness of different feature-matching
techniques in visual localization using textured CityGML LoD2 models. We use
standard benchmark datasets (HPatches, MegaDepth-1500) and custom datasets
consisting of facade textures and corresponding camera images (terrestrial and
drone). For the latter, we evaluate the achievable accuracy of the absolute
pose estimated using a Perspective-n-Point (PnP) algorithm, with geometric
ground truth derived from geo-referenced trajectory data. The results indicate
that the learnable feature matching methods vastly outperform traditional
approaches regarding accuracy and robustness on our challenging custom datasets
with zero to 12 RANSAC-inliers and zero to 0.16 area under the curve. We
believe that this work will foster the development of model-based visual
localization methods. Link to the code:
https://github.com/simBauer/To\_Glue\_or\_not\_to\_Glue

</details>


### [489] [SemSegBench & DetecBench: Benchmarking Reliability and Generalization Beyond Classification](https://arxiv.org/abs/2505.18015)
*Shashank Agnihotri,David Schader,Jonas Jakubassa,Nico Sharei,Simon Kral,Mehmet Ege Kaçar,Ruben Weber,Margret Keuper*

Key words: 深度学习的可靠性、语义分割、物体检测、基准测试、对抗攻击、鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了SEMSEGBENCH和DETECBENCH两个基准测试工具，用于评估语义分割和物体检测模型的鲁棒性，涵盖了76个分割模型和61个检测模型，揭示了前沿模型的系统性弱点，并开源了工具和数据集。

Motivation: 现实世界的安全关键领域应用不仅限于图像分类，还包括语义分割和物体检测等任务，这些任务的模型鲁棒性研究不足，因此需要提供基准测试工具来推动相关研究。

Method: 提出了SEMSEGBENCH和DETECBENCH两个基准测试工具，评估了76个语义分割模型（覆盖四个数据集）和61个物体检测模型（覆盖两个数据集），在对抗攻击和常见干扰下的性能表现。

Result: 研究发现前沿模型存在系统性弱点，并揭示了基于架构、主干网络和模型容量的关键趋势，提供了总计6139次评估数据。

Conclusion: 开源基准工具和数据集将推动未来超越分类任务的模型鲁棒性研究，并帮助改进语义分割和物体检测模型的可靠性。

Abstract: Reliability and generalization in deep learning are predominantly studied in
the context of image classification. Yet, real-world applications in
safety-critical domains involve a broader set of semantic tasks, such as
semantic segmentation and object detection, which come with a diverse set of
dedicated model architectures. To facilitate research towards robust model
design in segmentation and detection, our primary objective is to provide
benchmarking tools regarding robustness to distribution shifts and adversarial
manipulations. We propose the benchmarking tools SEMSEGBENCH and DETECBENCH,
along with the most extensive evaluation to date on the reliability and
generalization of semantic segmentation and object detection models. In
particular, we benchmark 76 segmentation models across four datasets and 61
object detectors across two datasets, evaluating their performance under
diverse adversarial attacks and common corruptions. Our findings reveal
systematic weaknesses in state-of-the-art models and uncover key trends based
on architecture, backbone, and model capacity. SEMSEGBENCH and DETECBENCH are
open-sourced in our GitHub repository
(https://github.com/shashankskagnihotri/benchmarking_reliability_generalization)
along with our complete set of total 6139 evaluations. We anticipate the
collected data to foster and encourage future research towards improved model
reliability beyond classification.

</details>


### [490] [F-ANcGAN: An Attention-Enhanced Cycle Consistent Generative Adversarial Architecture for Synthetic Image Generation of Nanoparticles](https://arxiv.org/abs/2505.18106)
*Varun Ajith,Anindya Pal,Saumik Bhattacharya,Sayantari Ghosh*

Key words: 纳米材料, 生成对抗网络, SEM图像, 数据增强, 分割模型

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: F-ANcGAN模型通过注意力增强的对抗生成网络，解决了纳米材料研究中标注数据不足的问题，能生成高保真SEM图像，显著提升下游分割任务的性能。

Motivation: 纳米材料研究中高质量标注数据的缺乏阻碍了分割模型的开发，需一种能从有限数据生成真实SEM图像的方法。

Method: 采用带自注意力的Style U-Net生成器和分割网络，结合数据增强技术，生成SEM图像。

Result: 模型在TiO₂数据集上的初始FID得分为17.65，经后处理降至10.39，实现了高保真合成数据生成。

Conclusion: F-ANcGAN为资源有限领域提供了一种有效解决方案，能显著改善纳米粒子分析的训练数据不足问题。

Abstract: Nanomaterial research is becoming a vital area for energy, medicine, and
materials science, and accurate analysis of the nanoparticle topology is
essential to determine their properties. Unfortunately, the lack of
high-quality annotated datasets drastically hinders the creation of strong
segmentation models for nanoscale imaging. To alleviate this problem, we
introduce F-ANcGAN, an attention-enhanced cycle consistent generative
adversarial system that can be trained using a limited number of data samples
and generates realistic scanning electron microscopy (SEM) images directly from
segmentation maps. Our model uses a Style U-Net generator and a U-Net
segmentation network equipped with self-attention to capture structural
relationships and applies augmentation methods to increase the variety of the
dataset. The architecture reached a raw FID score of 17.65 for TiO$_2$ dataset
generation, with a further reduction in FID score to nearly 10.39 by using
efficient post-processing techniques. By facilitating scalable high-fidelity
synthetic dataset generation, our approach can improve the effectiveness of
downstream segmentation task training, overcoming severe data shortage issues
in nanoparticle analysis, thus extending its applications to resource-limited
fields.

</details>


### [491] [Boosting Open Set Recognition Performance through Modulated Representation Learning](https://arxiv.org/abs/2505.18137)
*Amit Kumar Kundu,Vaishnavi Patil,Joseph Jaja*

Key words: 开放集识别, 温度调制, 负余弦调度, 表示学习, 语义偏移

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种温度调制的表示学习方法，通过新颖的负余弦调度方案解决开放集识别问题，无需额外计算开销即可提升性能。

Motivation: 现有开放集识别方法使用固定温度缩放因子限制了表示学习的灵活性，无法同时探索实例级和语义级特征，因此需要更灵活的调度方案。

Method: 提出负余弦调度方案，训练初期聚焦较少邻居形成粗略决策边界，后期逐步平滑边界，融入现有方法且无计算开销。

Result: 方案在不同损失函数和基准测试中普遍提升开放集和闭集性能，尤其对语义偏移任务效果显著。

Conclusion: 温度调制表示学习通过动态调度优化决策边界，提升模型的泛化能力和性能。

Abstract: The open set recognition (OSR) problem aims to identify test samples from
novel semantic classes that are not part of the training classes, a task that
is crucial in many practical scenarios. However, existing OSR methods use a
constant scaling factor (the temperature) to the logits before applying a loss
function, which hinders the model from exploring both ends of the spectrum in
representation learning -- from instance-level to semantic-level features. In
this paper, we address this problem by enabling temperature-modulated
representation learning using our novel negative cosine scheduling scheme. Our
scheduling lets the model form a coarse decision boundary at the beginning of
training by focusing on fewer neighbors, and gradually prioritizes more
neighbors to smooth out rough edges. This gradual task switching leads to a
richer and more generalizable representation space. While other OSR methods
benefit by including regularization or auxiliary negative samples, such as with
mix-up, thereby adding a significant computational overhead, our scheme can be
folded into any existing OSR method with no overhead. We implement the proposed
scheme on top of a number of baselines, using both cross-entropy and
contrastive loss functions as well as a few other OSR methods, and find that
our scheme boosts both the OSR performance and the closed set performance in
most cases, especially on the tougher semantic shift benchmarks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [492] [LLM-Powered Agents for Navigating Venice's Historical Cadastre](https://arxiv.org/abs/2505.17148)
*Tristan Karch,Jakhongir Saydaliev,Isabella Di Lenardo,Frédéric Kaplan*

Key words: 地籍数据, 大语言模型, 文本到程序, 历史分析, 威尼斯

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 论文提出了一种利用大语言模型（LLMs）将自然语言查询转换为可执行代码的文本到程序框架，用于处理历史地籍数据，并以威尼斯1740-1808年的数据为例展示了其有效性。

Motivation: 历史地籍数据因格式多样和人工标注而难以标准化，阻碍了大规模分析。研究旨在通过技术手段解决这一问题，为城市历史研究提供支持。

Method: 采用文本到SQL和文本到Python两种互补技术，分别处理结构化查询和复杂分析操作，并提出分类研究问题的分类法。

Result: 系统能有效重建威尼斯过去的人口信息、财产特征和时空比较，并通过可验证的程序输出确保结果的可解释性。

Conclusion: 该框架成功应对了非标准化地籍数据的挑战，为历史研究提供了新的分析工具。

Abstract: Cadastral data reveal key information about the historical organization of
cities but are often non-standardized due to diverse formats and human
annotations, complicating large-scale analysis. We explore as a case study
Venice's urban history during the critical period from 1740 to 1808, capturing
the transition following the fall of the ancient Republic and the Ancien
R\'egime. This era's complex cadastral data, marked by its volume and lack of
uniform structure, presents unique challenges that our approach adeptly
navigates, enabling us to generate spatial queries that bridge past and present
urban landscapes. We present a text-to-programs framework that leverages Large
Language Models (LLMs) to translate natural language queries into executable
code for processing historical cadastral records. Our methodology implements
two complementary techniques: a text-to-SQL approach for handling structured
queries about specific cadastral information, and a text-to-Python approach for
complex analytical operations requiring custom data manipulation. We propose a
taxonomy that classifies historical research questions based on their
complexity and analytical requirements, mapping them to the most appropriate
technical approach. This framework is supported by an investigation into the
execution consistency of the system, alongside a qualitative analysis of the
answers it produces. By ensuring interpretability and minimizing hallucination
through verifiable program outputs, we demonstrate the system's effectiveness
in reconstructing past population information, property features, and
spatiotemporal comparisons in Venice.

</details>


### [493] [ReqBrain: Task-Specific Instruction Tuning of LLMs for AI-Assisted Requirements Generation](https://arxiv.org/abs/2505.17632)
*Mohammad Kasra Habib,Daniel Graziotin,Stefan Wagner*

Key words: 需求工程, 大语言模型, 自动化, ReqBrain, ISO 29148

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: ReqBrain是一个AI辅助工具，通过微调大语言模型（LLM）自动生成高质量软件需求，提升需求获取和规范的效率。

Motivation: 需求获取和规范是劳动密集型且易出错的过程，研究探索利用LLM自动化这一过程的可行性。

Method: 开发ReqBrain工具，基于微调的LLM（如Zephyr-7b-beta）通过聊天会话生成并分类需求，使用ISO 29148标准数据集评估模型效果。

Result: 最优模型Zephyr-7b-beta在BERT分数（89.30%）和FRUGAL分数（91.20）上表现优异，人工评估验证了生成需求的有效性。

Conclusion: 微调的生成式AI可显著改进需求工程，未来可拓展至缺陷识别、测试用例生成等领域。

Abstract: Requirements elicitation and specification remains a labor-intensive, manual
process prone to inconsistencies and gaps, presenting a significant challenge
in modern software engineering. Emerging studies underscore the potential of
employing large language models (LLMs) for automated requirements generation to
support requirements elicitation and specification; however, it remains unclear
how to implement this effectively. In this work, we introduce ReqBrain, an
Al-assisted tool that employs a fine-tuned LLM to generate authentic and
adequate software requirements. Software engineers can engage with ReqBrain
through chat-based sessions to automatically generate software requirements and
categorize them by type. We curated a high-quality dataset of ISO
29148-compliant requirements and fine-tuned five 7B-parameter LLMs to determine
the most effective base model for ReqBrain. The top-performing model,
Zephyr-7b-beta, achieved 89.30\% Fl using the BERT score and a FRUGAL score of
91.20 in generating authentic and adequate requirements. Human evaluations
further confirmed ReqBrain's effectiveness in generating requirements. Our
findings suggest that generative Al, when fine-tuned, has the potential to
improve requirements elicitation and specification, paving the way for future
extensions into areas such as defect identification, test case generation, and
agile user story creation.

</details>


### [494] [Towards Practical Defect-Focused Automated Code Review](https://arxiv.org/abs/2505.17928)
*Junyi Lu,Lili Jiang,Xiaojia Li,Jianbing Fang,Fengjun Zhang,Li Yang,Chun Zuo*

Key words: 代码审查自动化, LLM, 代码切片, 关键缺陷检测, 工业级应用

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 论文提出一种自动化代码审查方法，通过代码切片和LLM框架解决上下文捕捉、缺陷检测等问题，在真实场景中效果显著优于基线方法。

Motivation: 现有自动化代码审查方法过于简化，忽视代码库上下文和实际缺陷检测需求，难以满足大规模工业级代码审查的实用要求。

Method: 采用代码切片算法提取上下文、多角色LLM框架提升关键缺陷捕获率、过滤机制降低误报率、新型提示设计优化人机交互。

Result: 在真实合并请求测试中，性能较标准LLM提升2倍，较基线方法提升10倍，且框架设计具备语言无关性（如基于AST的分析）。

Conclusion: 所提方法通过系统性优化解决了工业级代码审查的四大挑战，虽以C++为验证对象，但底层设计可扩展至其他语言。

Abstract: The complexity of code reviews has driven efforts to automate review
comments, but prior approaches oversimplify this task by treating it as
snippet-level code-to-text generation and relying on text similarity metrics
like BLEU for evaluation. These methods overlook repository context, real-world
merge request evaluation, and defect detection, limiting their practicality. To
address these issues, we explore the full automation pipeline within the online
recommendation service of a company with nearly 400 million daily active users,
analyzing industry-grade C++ codebases comprising hundreds of thousands of
lines of code. We identify four key challenges: 1) capturing relevant context,
2) improving key bug inclusion (KBI), 3) reducing false alarm rates (FAR), and
4) integrating human workflows. To tackle these, we propose 1) code slicing
algorithms for context extraction, 2) a multi-role LLM framework for KBI, 3) a
filtering mechanism for FAR reduction, and 4) a novel prompt design for better
human interaction. Our approach, validated on real-world merge requests from
historical fault reports, achieves a 2x improvement over standard LLMs and a
10x gain over previous baselines. While the presented results focus on C++, the
underlying framework design leverages language-agnostic principles (e.g.,
AST-based analysis), suggesting potential for broader applicability.

</details>


### [495] [LLM assisted web application functional requirements generation: A case study of four popular LLMs over a Mess Management System](https://arxiv.org/abs/2505.18019)
*Rashmi Gupta,Aditya K Gupta,Aarav Jain,Avinash C Pandey,Atul Gupta*

Key words: 大语言模型,软件工程,功能性规范,用例,业务规则,协作工作流

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 该论文比较了GPT、Claude、Gemini和DeepSeek四种大语言模型（LLM）在生成功能性规范方面的表现，发现它们在生成语法和语义正确的用例、业务规则和协作工作流方面表现良好，但在一致性和完整性上存在差异。Claude生成的规范最完整但冗余，而Gemini更精确。

Motivation: 研究LLMs在软件工程中的应用，特别是在生成功能性规范方面的性能，为开发者在实际应用中选择合适的模型提供参考。

Method: 采用案例研究方法，比较四种LLMs在生成Mess Management System的功能性规范（包括用例、业务规则和协作工作流）的性能，评估其在语法、语义、一致性、非歧义性和完整性方面的质量。

Result: 所有四种LLMs能生成语法和语义正确的规范，但在一致性和完整性上存在差异。Claude生成最完整的规范但冗余，Gemini生成更精确的规范，而所有LLMs在生成相关业务规则时表现不佳。

Conclusion: Claude和Gemini在生成功能性规范方面表现最佳，但各模型在不同方面（如完整性和精确性）各有优劣。

Abstract: Like any other discipline, Large Language Models (LLMs) have significantly
impacted software engineering by helping developers generate the required
artifacts across various phases of software development. This paper presents a
case study comparing the performance of popular LLMs GPT, Claude, Gemini, and
DeepSeek in generating functional specifications that include use cases,
business rules, and collaborative workflows for a web application, the Mess
Management System. The study evaluated the quality of LLM generated use cases,
business rules, and collaborative workflows in terms of their syntactic and
semantic correctness, consistency, non ambiguity, and completeness compared to
the reference specifications against the zero-shot prompted problem statement.
Our results suggested that all four LLMs can specify syntactically and
semantically correct, mostly non-ambiguous artifacts. Still, they may be
inconsistent at times and may differ significantly in the completeness of the
generated specification. Claude and Gemini generated all the reference use
cases, with Claude achieving the most complete but somewhat redundant use case
specifications. Similar results were obtained for specifying workflows.
However, all four LLMs struggled to generate relevant Business Rules, with
DeepSeek generating the most reference rules but with less completeness.
Overall, Claude generated more complete specification artifacts, while Gemini
was more precise in the specifications it generated.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [496] [Qiskit Machine Learning: an open-source library for quantum machine learning tasks at scale on quantum hardware and classical simulators](https://arxiv.org/abs/2505.17756)
*M. Emre Sahin,Edoardo Altamura,Oscar Wallis,Stephen P. Wood,Anton Dekusar,Declan A. Millar,Takashi Imamichi,Atsushi Matsuo,Stefano Mensa*

Key words: Qiskit, 量子计算, 机器学习, Python, 开源

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: Qiskit Machine Learning (ML) 是一个将量子计算与传统机器学习结合的高级 Python 库，为非专业用户提供模块化工具，同时支持开发者扩展和微调。

Motivation: 旨在简化量子计算与机器学习的结合使用，为非专业用户提供直观工具，同时满足专业开发者对扩展性和控制的需求。

Method: 通过抽象 Qiskit 的底层原语，提供高层 API 以便与经典模拟器和量子硬件交互。

Result: 成功开发并开源了 Qiskit ML 库，支持模块化设计，适应不同用户需求。

Conclusion: Qiskit ML 是一个功能强大且灵活的工具，推动了量子计算在机器学习中的应用普及。

Abstract: We present Qiskit Machine Learning (ML), a high-level Python library that
combines elements of quantum computing with traditional machine learning. The
API abstracts Qiskit's primitives to facilitate interactions with classical
simulators and quantum hardware. Qiskit ML started as a proof-of-concept code
in 2019 and has since been developed to be a modular, intuitive tool for
non-specialist users while allowing extensibility and fine-tuning controls for
quantum computational scientists and developers. The library is available as a
public, open-source tool and is distributed under the Apache version 2.0
license.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [497] [Transparency in Healthcare AI: Testing European Regulatory Provisions against Users' Transparency Needs](https://arxiv.org/abs/2505.17105)
*Anna Spagnolli,Cecilia Tolomini,Elisa Beretta,Claudio Sarra*

Key words: 人工智能，医疗设备，说明书，透明度，欧洲AI法规

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 该研究测试了医疗AI设备说明书（IFU）的结构是否满足透明性原则，通过调查四类利益相关者发现透明需求与IFU结构存在不匹配，并提出了改进建议。

Motivation: 研究动机是验证医疗AI设备的IFU是否符合欧洲AI法规的透明性原则，确保用户能清晰理解其内容。

Method: 通过Qualtrics平台对管理者、医疗专业人士、患者和IT专家进行在线调查，评估透明需求的关联性及IFU的对应部分。

Result: 结果显示不同利益相关者对透明需求的优先级不同，且透明需求与IFU结构的对应关系存在问题。

Conclusion: 研究提出了改进IFU的建议，以使其更具本地化意义。

Abstract: Artificial Intelligence (AI) plays an essential role in healthcare and is
pervasively incorporated into medical software and equipment. In the European
Union, healthcare is a high-risk application domain for AI, and providers must
prepare Instructions for Use (IFU) according to the European regulation
2024/1689 (AI Act). To this regulation, the principle of transparency is
cardinal and requires the IFU to be clear and relevant to the users. This study
tests whether these latter requirements are satisfied by the IFU structure. A
survey was administered online via the Qualtrics platform to four types of
direct stakeholders, i.e., managers (N = 238), healthcare professionals (N =
115), patients (N = 229), and Information Technology experts (N = 230). The
participants rated the relevance of a set of transparency needs and indicated
the IFU section addressing them. The results reveal differentiated priorities
across stakeholders and a troubled mapping of transparency needs onto the IFU
structure. Recommendations to build a locally meaningful IFU are derived.

</details>


### [498] [Fashion Industry in the Age of Generative Artificial Intelligence and Metaverse: A systematic Review](https://arxiv.org/abs/2505.17141)
*Rania Ahmed,Eman Ahmed,Ahmed Elbarbary,Ashraf Darwish,Aboul Ella Hassanien*

Key words: 生成式人工智能, 元宇宙, 时尚行业, SWOT分析, 系统性文献综述

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 该论文通过系统性文献综述分析了生成式人工智能（GAI）和元宇宙在时尚行业的应用，提出了整合这两种技术以促进时尚行业发展的新框架。

Motivation: 研究旨在探索GAI和元宇宙如何革新时尚行业，提升设计、制造、销售和客户体验。

Method: 采用PRISMA方法进行系统性文献综述，包括识别、评估和报告三个阶段，最终筛选118篇文献进行深入分析。

Result: 通过SWOT分析，发现GAI和元宇宙的结合能深刻改变时尚行业，研究提出了整合两者的新框架及用例。

Conclusion: GAI与元宇宙的结合能为时尚行业带来革命性变革，未来需进一步研究以实现成功整合。

Abstract: The fashion industry is an extremely profitable market that generates
trillions of dollars in revenue by producing and distributing apparel,
footwear, and accessories. This systematic literature review (SLR) seeks to
systematically review and analyze the research landscape about the Generative
Artificial Intelligence (GAI) and metaverse in the fashion industry. Thus,
investigating the impact of integrating both technologies to enhance the
fashion industry. This systematic review uses the Reporting Items for
Systematic reviews and Meta-Analyses (PRISMA) methodology, including three
essential phases: identification, evaluation, and reporting. In the
identification phase, the target search problems are determined by selecting
appropriate keywords and alternative synonyms. After that 578 documents from
2014 to the end of 2023 are retrieved. The evaluation phase applies three
screening steps to assess papers and choose 118 eligible papers for full-text
reading. Finally, the reporting phase thoroughly examines and synthesizes the
118 eligible papers to identify key themes associated with GAI and Metaverse in
the fashion industry. Based on Strengths, Weaknesses, Opportunities, and
Threats (SWOT) analyses performed for both GAI and metaverse for the fashion
industry, it is concluded that the integration of GAI and the metaverse holds
the capacity to profoundly revolutionize the fashion sector, presenting chances
for improved manufacturing, design, sales, and client experiences. Accordingly,
the research proposes a new framework to integrate GAI and metaverse to enhance
the fashion industry. The framework presents different use cases to promote the
fashion industry using the integration. Future research points for achieving a
successful integration are demonstrated.

</details>


### [499] [Evaluating the Performance of Nigerian Lecturers using Multilayer Perceptron](https://arxiv.org/abs/2505.17143)
*I. E. Ezeibe,S. O. Okide,D. C. Asogwa*

Key words: 讲师绩效评估, 多层感知机（MLP）, 学生评分, 数据驱动, 公平性

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 开发了一个基于多层感知机（MLP）算法的网络平台，用于全面评估讲师绩效，准确率达91%。通过多种指标（如学生评分、研究发表等）和数据驱动方法提升评估的公平性和效率。

Motivation: 现有讲师绩效评估系统不够全面和客观，需通过技术手段改进评估质量，提升教学水平和机构声誉。

Method: 采用面向对象分析与设计（OOAD）方法，构建基于MLP算法的网络平台，整合学生评分、研究发表等多维指标，利用自定义数据集训练模型。

Result: 模型测试准确率91%，MSE为256.99、MAE为13.76，预测精度约96%，显著减少偏差并支持数据驱动决策。

Conclusion: MLP模型能高效、公平地评估讲师绩效，为教育机构提供可靠的决策支持。

Abstract: Evaluating the performance of a lecturer has been essential for enhancing
teaching quality, improving student learning outcomes, and strengthening the
institution's reputation. The absence of such a system brings about lecturer
performance evaluation which was neither comprehensive nor holistic. This
system was designed using a web-based platform, created a secure database, and
by using a custom dataset, captured some performance metrics which included
student evaluation scores, Research Publications, Years of Experience, and
Administrative Duties. Multilayer Perceptron (MLP) algorithm was utilized due
to its ability to process complex data patterns and generates accurate
predictions in a lecturer's performance based on historical data. This research
focused on designing multiple performance metrics beyond the standard ones,
incorporating student participation, and integrating analytical tools to
deliver a comprehensive and holistic evaluation of lecturers' performance and
was developed using Object-Oriented Analysis and Design (OOAD) methodology.
Lecturers' performance is evaluated by the model, and the evaluation accuracy
is about 91% compared with actual performance. Finally, by evaluating the
performance of the MLP model, it is concluded that MLP enhanced lecturer
performance evaluation by providing accurate predictions, reducing bias, and
supporting data-driven decisions, ultimately improving the fairness and
efficiency of the evaluation process. The MLP model's performance was evaluated
using Mean Squared Error (MSE) and Mean Absolute Error (MAE), achieved a test
loss (MSE) of 256.99 and a MAE of 13.76, and reflected a high level of
prediction accuracy. The model also demonstrated an estimated accuracy rate of
approximately 96%, validated its effectiveness in predicting lecturer
performance.

</details>


### [500] [A Toolkit for Compliance, a Toolkit for Justice: Drawing on Cross-sectoral Expertise to Develop a Pro-justice EU AI Act Toolkit](https://arxiv.org/abs/2505.17165)
*Tomasz Hollanek,Yulu Pi,Cosimo Fiorini,Virginia Vignali,Dorian Peters,Eleanor Drage*

Key words: AI伦理，欧盟AI法案，合规工具包，跨行业合作，社会伦理

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 论文讨论了一个跨行业合作开发的AI伦理工具包，旨在帮助从业者遵守欧盟AI法案，并考虑更广泛的社会伦理问题。

Motivation: 解决AI从业者在遵守新法规时面临的挑战，并提供超越合规的伦理指导。

Method: 通过英国学术团队与意大利行业团队的合作，开发了一款AI伦理工具包。

Result: 提出了一个跨行业合作开发的工具包，可作为其他团队的蓝图。

Conclusion: 该工具包为AI伦理资源开发提供了实用范例，强调了学术与行业的协作价值。

Abstract: The introduction of the AI Act in the European Union presents the AI research
and practice community with a set of new challenges related to compliance.
While it is certain that AI practitioners will require additional guidance and
tools to meet these requirements, previous research on toolkits that aim to
translate the theory of AI ethics into development and deployment practice
suggests that such resources suffer from multiple limitations. These
limitations stem, in part, from the fact that the toolkits are either produced
by industry-based teams or by academics whose work tends to be abstract and
divorced from the realities of industry. In this paper, we discuss the
challenge of developing an AI ethics toolkit for practitioners that helps them
comply with new AI-focused regulation, but that also moves beyond mere
compliance to consider broader socio-ethical questions throughout development
and deployment. The toolkit was created through a cross-sectoral collaboration
between an academic team based in the UK and an industry team in Italy. We
outline the background and rationale for creating a pro-justice AI Act
compliance toolkit, detail the process undertaken to develop it, and describe
the collaboration and negotiation efforts that shaped its creation. We aim for
the described process to serve as a blueprint for other teams navigating the
challenges of academia-industry partnerships and aspiring to produce usable and
meaningful AI ethics resources.

</details>


### [501] [Twin-2K-500: A dataset for building digital twins of over 2,000 people based on their answers to over 500 questions](https://arxiv.org/abs/2505.17479)
*Olivier Toubia,George Z. Gui,Tianyi Peng,Daniel J. Merlau,Ang Li,Haozhe Chen*

Key words: 数字孪生, 数据集, 大型语言模型, 行为预测, 社会科学

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 该论文介绍了一个大规模公开数据集，用于支持基于大型语言模型的数字孪生模拟研究，填补了高质量数据的空白。

Motivation: 当前缺乏大型公开的个体层面数据集，限制了数字孪生方法的发展和验证，因此作者旨在提供高质量数据以推动相关研究。

Method: 通过四轮调查收集了2058名美国参与者的综合数据（总计2.42小时/人），涵盖人口统计、心理、经济等多维信息，并重复测试以验证数据可靠性。

Result: 初步分析表明数据质量高，能够有效预测个体和群体行为，适合构建数字孪生模型。

Conclusion: 该数据集为LLM数字孪生研究提供了重要基准，同时支持广泛的社会科学研究。

Abstract: LLM-based digital twin simulation, where large language models are used to
emulate individual human behavior, holds great promise for research in AI,
social science, and digital experimentation. However, progress in this area has
been hindered by the scarcity of real, individual-level datasets that are both
large and publicly available. This lack of high-quality ground truth limits
both the development and validation of digital twin methodologies. To address
this gap, we introduce a large-scale, public dataset designed to capture a rich
and holistic view of individual human behavior. We survey a representative
sample of $N = 2,058$ participants (average 2.42 hours per person) in the US
across four waves with 500 questions in total, covering a comprehensive battery
of demographic, psychological, economic, personality, and cognitive measures,
as well as replications of behavioral economics experiments and a pricing
survey. The final wave repeats tasks from earlier waves to establish a
test-retest accuracy baseline. Initial analyses suggest the data are of high
quality and show promise for constructing digital twins that predict human
behavior well at the individual and aggregate levels. By making the full
dataset publicly available, we aim to establish a valuable testbed for the
development and benchmarking of LLM-based persona simulations. Beyond LLM
applications, due to its unique breadth and scale the dataset also enables
broad social science research, including studies of cross-construct
correlations and heterogeneous treatment effects.

</details>


### [502] [Exploring EFL Secondary Students' AI-generated Text Editing While Composition Writing](https://arxiv.org/abs/2505.17041)
*David James Woo,Yangyang Yu,Kai Guo*

Key words: 生成式人工智能, EFL写作, 编辑行为, 混合方法, 认知过程

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 研究探讨了EFL中学生如何整合和修改AI生成的文本，发现了15种编辑类型和4种编辑行为模式，挑战了学生对生成式AI工具被动使用的假设。

Motivation: 探讨学生在写作过程中如何操纵生成式AI生成的文本，填补当前对EFL学生使用AI写作的理解空白。

Method: 采用探索性混合方法设计，收集29名香港中学生的屏幕录制数据，通过内容分析和主题分析（多案例研究）分析数据。

Result: 识别出15种编辑类型，并揭示了基于规划和起草方向的4种学生编辑行为模式，网络图展示了复杂的认知过程。

Conclusion: 研究结果挑战了学生对生成式AI工具被动使用的假设，为EFL写作教学中AI生成文本编辑策略的教学提供了启示。

Abstract: Generative Artificial Intelligence is transforming how English as a foreign
language students write. Still, little is known about how students manipulate
text generated by generative AI during the writing process. This study
investigates how EFL secondary school students integrate and modify
AI-generated text when completing an expository writing task. The study
employed an exploratory mixed-methods design. Screen recordings were collected
from 29 Hong Kong secondary school students who attended an AI-assisted writing
workshop and recorded their screens while using generative AI to write an
article. Content analysis with hierarchical coding and thematic analysis with a
multiple case study approach were adopted to analyze the recordings. 15 types
of AI-generated text edits across seven categories were identified from the
recordings. Notably, AI-initiated edits from iOS and Google Docs emerged as
unanticipated sources of AI-generated text. A thematic analysis revealed four
patterns of students' editing behaviors based on planning and drafting
direction: planning with top-down drafting and revising; top-down drafting and
revising without planning; planning with bottom-up drafting and revising; and
bottom-up drafting and revising without planning. Network graphs illustrate
cases of each pattern, demonstrating that students' interactions with
AI-generated text involve more complex cognitive processes than simple text
insertion. The findings challenge assumptions about students' passive,
simplistic use of generative AI tools and have implications for developing
explicit instructional approaches to teaching AI-generated text editing
strategies in the AFL writing pedagogy.

</details>


### [503] [TEDI: Trustworthy and Ethical Dataset Indicators to Analyze and Compare Dataset Documentation](https://arxiv.org/abs/2505.17841)
*Wiebke Hutiri,Mircea Cimpoi,Morgan Scheuerman,Victoria Matthews,Alice Xiang*

Key words: 数据集透明性, 负责任AI, TEDI, 多模态数据集, 伦理指标

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 论文介绍了TEDI指标，用于系统性分析多模态数据集的诚信与伦理属性，并通过手动标注100多个数据集发现，大多数数据集在隐私、同意等伦理指标上文档记录不足。

Motivation: 多模态数据集的诚信和伦理属性分析稀缺且难以比较，影响了负责任AI的发展。

Method: 提出TEDI框架，包含143个细粒度指标，手动标注并分析100多个含人类语音的多模态数据集。

Result: 发现仅有少数数据集记录了隐私、同意等伦理指标，且其记录程度受数据收集方法影响。

Conclusion: TEDI框架提升了数据集透明度，为未来自动化文档信息提取奠定了基础。

Abstract: Dataset transparency is a key enabler of responsible AI, but insights into
multimodal dataset attributes that impact trustworthy and ethical aspects of AI
applications remain scarce and are difficult to compare across datasets. To
address this challenge, we introduce Trustworthy and Ethical Dataset Indicators
(TEDI) that facilitate the systematic, empirical analysis of dataset
documentation. TEDI encompasses 143 fine-grained indicators that characterize
trustworthy and ethical attributes of multimodal datasets and their collection
processes. The indicators are framed to extract verifiable information from
dataset documentation. Using TEDI, we manually annotated and analyzed over 100
multimodal datasets that include human voices. We further annotated data
sourcing, size, and modality details to gain insights into the factors that
shape trustworthy and ethical dimensions across datasets. We find that only a
select few datasets have documented attributes and practices pertaining to
consent, privacy, and harmful content indicators. The extent to which these and
other ethical indicators are addressed varies based on the data collection
method, with documentation of datasets collected via crowdsourced and direct
collection approaches being more likely to mention them. Scraping dominates
scale at the cost of ethical indicators, but is not the only viable collection
method. Our approach and empirical insights contribute to increasing dataset
transparency along trustworthy and ethical dimensions and pave the way for
automating the tedious task of extracting information from dataset
documentation in future.

</details>


### [504] [AI Literacy for Legal AI Systems: A practical approach](https://arxiv.org/abs/2505.18006)
*Gizem Gultekin-Varkonyi*

Key words: 法律AI系统, AI素养, 欧盟AI法案, 风险与利益, 问卷路线图

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 该文章探讨了法律AI系统的潜力与风险，提出了AI素养作为平衡开发与部署的工具，并提供了一个实用性问卷路线图帮助开发者评估风险和利益。

Motivation: 随着法律AI系统的广泛应用，文章旨在平衡其带来的效率提升和潜在的伦理风险，特别是在欧盟AI法案框架下对AI素养的要求。

Method: 文章首先定义了‘法律AI系统’概念，分析了AI素养的重要性及其对系统的利益与风险影响，并提出了一个适用于组织的AI-L框架。

Result: 文章最终开发了一个问卷路线图，帮助开发者和供应商评估法律AI系统的社会、法规和利益相关者需求。

Conclusion: 文章强调了AI素养的重要性，并提出了实用性工具以促进法律AI系统的负责任发展和部署。

Abstract: Legal AI systems are increasingly being adopted by judicial and legal system
deployers and providers worldwide to support a range of applications. While
they offer potential benefits such as reducing bias, increasing efficiency, and
improving accountability, they also pose significant risks, requiring a careful
balance between opportunities, and legal and ethical development and
deployment. AI literacy, as a legal requirement under the EU AI Act and a
critical enabler of ethical AI for deployers and providers, could be a tool to
achieve this. The article introduces the term "legal AI systems" and then
analyzes the concept of AI literacy and the benefits and risks associated with
these systems. This analysis is linked to a broader AI-L concept for
organizations that deal with legal AI systems. The outcome of the article, a
roadmap questionnaire as a practical tool for developers and providers to
assess risks, benefits, and stakeholder concerns, could be useful in meeting
societal and regulatory expectations for legal AI.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [505] [Generative AI and Creativity: A Systematic Literature Review and Meta-Analysis](https://arxiv.org/abs/2505.17241)
*Niklas Holzner,Sebastian Maier,Stefan Feuerriegel*

Key words: 生成式AI, 创意, 荟萃分析, 人机协作, 多样性

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 论文通过荟萃分析评估生成式AI（GenAI）对创意任务的影响，发现GenAI本身创意与人类无显著差异，但人类在GenAI辅助下创意表现显著提升，不过多样性会降低。

Motivation: 研究旨在厘清GenAI对创意的实际影响，尤其是在人类协作中是否能提升创意表现和多样性。

Method: 通过系统文献搜索纳入28项研究（8214名参与者），采用Hedges' g计算标准化效应量，对比GenAI、人类及人机协作的创意表现和多样性。

Result: GenAI与人类创意无显著差异（g=-0.05），人机协作显著优于无辅助（g=0.27），但多样性显著降低（g=-0.86）。不同模型、任务和人群存在异质性。

Conclusion: GenAI是增强而非替代人类创意的工具，尤其适用于需要创意支持的任务。

Abstract: Generative artificial intelligence (GenAI) is increasingly used to support a
wide range of human tasks, yet empirical evidence on its effect on creativity
remains scattered. Can GenAI generate ideas that are creative? To what extent
can it support humans in generating ideas that are both creative and diverse?
In this study, we conduct a meta-analysis to evaluate the effect of GenAI on
the performance in creative tasks. For this, we first perform a systematic
literature search, based on which we identify n = 28 relevant studies (m = 8214
participants) for inclusion in our meta-analysis. We then compute standardized
effect sizes based on Hedges' g. We compare different outcomes: (i) how
creative GenAI is; (ii) how creative humans augmented by GenAI are; and (iii)
the diversity of ideas by humans augmented by GenAI. Our results show no
significant difference in creative performance between GenAI and humans (g =
-0.05), while humans collaborating with GenAI significantly outperform those
working without assistance (g = 0.27). However, GenAI has a significant
negative effect on the diversity of ideas for such collaborations between
humans and GenAI (g = -0.86). We further analyze heterogeneity across different
GenAI models (e.g., GPT-3.5, GPT-4), different tasks (e.g., creative writing,
ideation, divergent thinking), and different participant populations (e.g.,
laypeople, business, academia). Overall, our results position GenAI as an
augmentative tool that can support, rather than replace, human
creativity-particularly in tasks benefiting from ideation support.

</details>


### [506] [TransBench: Breaking Barriers for Transferable Graphical User Interface Agents in Dynamic Digital Environments](https://arxiv.org/abs/2505.17629)
*Yuheng Lu,Qian Yu,Hongru Wang,Zeming Liu,Wei Su,Yanping Liu,Yuhang Guo,Maocheng Liang,Yunhong Wang,Haifeng Wang*

Key words: GUI代理, 可迁移性, 基准测试, 跨平台, 跨应用

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: TransBench是一个新基准，用于评估GUI代理在跨版本、跨平台和跨应用的可迁移性，提升其在动态数字环境中的实用性。

Motivation: 现有GUI代理在动态和互联的真实数字环境中适应性不足，包括版本更新、多平台和多应用任务的挑战。

Method: 引入TransBench基准，包含15个应用类别，覆盖跨版本、跨平台和跨应用的评估维度。

Result: 实验显示GUI代理的接地准确性显著提升，验证了其在动态环境中的实用性。

Conclusion: TransBench为GUI代理的可迁移性提供了系统性评估工具，推动了其在真实环境中的应用。

Abstract: Graphical User Interface (GUI) agents, which autonomously operate on digital
interfaces through natural language instructions, hold transformative potential
for accessibility, automation, and user experience. A critical aspect of their
functionality is grounding - the ability to map linguistic intents to visual
and structural interface elements. However, existing GUI agents often struggle
to adapt to the dynamic and interconnected nature of real-world digital
environments, where tasks frequently span multiple platforms and applications
while also being impacted by version updates. To address this, we introduce
TransBench, the first benchmark designed to systematically evaluate and enhance
the transferability of GUI agents across three key dimensions: cross-version
transferability (adapting to version updates), cross-platform transferability
(generalizing across platforms like iOS, Android, and Web), and
cross-application transferability (handling tasks spanning functionally
distinct apps). TransBench includes 15 app categories with diverse
functionalities, capturing essential pages across versions and platforms to
enable robust evaluation. Our experiments demonstrate significant improvements
in grounding accuracy, showcasing the practical utility of GUI agents in
dynamic, real-world environments. Our code and data will be publicly available
at Github.

</details>


### [507] [CHART-6: Human-Centered Evaluation of Data Visualization Understanding in Vision-Language Models](https://arxiv.org/abs/2505.17202)
*Arnav Verma,Kushin Mukherjee,Christopher Potts,Elisa Kreiss,Judith E. Fan*

Key words: 数据可视化, 视觉-语言模型, 人类认知, AI评估

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 该论文探讨了视觉-语言模型在理解数据可视化时与人类表现的差异，发现模型表现不如人类且错误模式不同，指出未来开发更接近人类推理能力的AI系统的可能性。

Motivation: 研究动机是评估视觉-语言模型在数据可视化理解任务上是否能够模拟人类行为，填补以往研究在评估标准上的不足。

Method: 方法包括对八个视觉-语言模型和人类参与者进行六项数据可视化素养评估，并比较两者的表现和错误模式。

Result: 结果显示模型平均表现差于人类，且错误模式与人类显著不同。

Conclusion: 结论指出当前模型仍需进一步发展，以更接近人类的推理能力。

Abstract: Data visualizations are powerful tools for communicating patterns in
quantitative data. Yet understanding any data visualization is no small feat --
succeeding requires jointly making sense of visual, numerical, and linguistic
inputs arranged in a conventionalized format one has previously learned to
parse. Recently developed vision-language models are, in principle, promising
candidates for developing computational models of these cognitive operations.
However, it is currently unclear to what degree these models emulate human
behavior on tasks that involve reasoning about data visualizations. This gap
reflects limitations in prior work that has evaluated data visualization
understanding in artificial systems using measures that differ from those
typically used to assess these abilities in humans. Here we evaluated eight
vision-language models on six data visualization literacy assessments designed
for humans and compared model responses to those of human participants. We
found that these models performed worse than human participants on average, and
this performance gap persisted even when using relatively lenient criteria to
assess model performance. Moreover, while relative performance across items was
somewhat correlated between models and humans, all models produced patterns of
errors that were reliably distinct from those produced by human participants.
Taken together, these findings suggest significant opportunities for further
development of artificial systems that might serve as useful models of how
humans reason about data visualizations. All code and data needed to reproduce
these results are available at:
https://osf.io/e25mu/?view_only=399daff5a14d4b16b09473cf19043f18.

</details>


### [508] [Chart-to-Experience: Benchmarking Multimodal LLMs for Predicting Experiential Impact of Charts](https://arxiv.org/abs/2505.17374)
*Seon Gyeom Kim,Jae Young Choi,Ryan Rossi,Eunyee Koh,Tak Yeon Lee*

Key words: Multimodal Large Language Models, MLLMs, visual understanding, benchmark dataset, perceptual impact

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 论文提出Chart-to-Experience基准数据集，评估MLLMs在图表感知和情感影响的预测能力，发现MLLMs在直接预测中不如人类敏感，但在成对比较中表现准确可靠。

Motivation: 尽管MLLMs在视觉理解任务上取得进展，但其性能缺乏充分验证，尤其是在图表感知和情感影响预测方面。

Method: 使用包含36张图表并由众包工作者评估的基准数据集，评估MLLMs在直接预测和成对比较任务中的表现。

Result: MLLMs在直接预测图表感知和情感影响时不如人类敏感，但在成对比较任务中表现准确可靠。

Conclusion: MLLMs在图表感知任务中存在局限，但在成对比较中展现了潜力，需进一步验证和改进。

Abstract: The field of Multimodal Large Language Models (MLLMs) has made remarkable
progress in visual understanding tasks, presenting a vast opportunity to
predict the perceptual and emotional impact of charts. However, it also raises
concerns, as many applications of LLMs are based on overgeneralized assumptions
from a few examples, lacking sufficient validation of their performance and
effectiveness. We introduce Chart-to-Experience, a benchmark dataset comprising
36 charts, evaluated by crowdsourced workers for their impact on seven
experiential factors. Using the dataset as ground truth, we evaluated
capabilities of state-of-the-art MLLMs on two tasks: direct prediction and
pairwise comparison of charts. Our findings imply that MLLMs are not as
sensitive as human evaluators when assessing individual charts, but are
accurate and reliable in pairwise comparisons.

</details>


### [509] [Towards Uncertainty Aware Task Delegation and Human-AI Collaborative Decision-Making](https://arxiv.org/abs/2505.18066)
*Min Hun Lee,Martyn Zhe Yu Tok*

Key words: AI决策支持、不确定性评分、任务委派、人类-AI协作、中风康复评估

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 该论文研究了基于距离的不确定性评分在AI任务委派中的应用，发现其优于传统的基于概率的评分，并提高了人类决策的准确性和对AI的适当依赖。

Motivation: 当前AI在决策支持中的应用日益广泛，但如何促进人类对AI的适当依赖仍是一个关键挑战。本文旨在探索基于距离的不确定性评分在任务委派中的效用。

Method: 开发了用于物理中风康复评估的AI系统，并通过19名健康专业人员与10名医学/健康专业学生的研究，比较了基于距离与基于概率的不确定性评分的效果。

Result: 基于距离的不确定性评分在识别不确定案例上表现更优，参与者在查看该评分后决策准确率提高了8.20%，错误决策修改率降低了7.14%。

Conclusion: 基于距离的不确定性评分有望提升决策准确性和AI依赖的合理性，但人类-AI协作决策仍面临挑战。

Abstract: Despite the growing promise of artificial intelligence (AI) in supporting
decision-making across domains, fostering appropriate human reliance on AI
remains a critical challenge. In this paper, we investigate the utility of
exploring distance-based uncertainty scores for task delegation to AI and
describe how these scores can be visualized through embedding representations
for human-AI decision-making. After developing an AI-based system for physical
stroke rehabilitation assessment, we conducted a study with 19 health
professionals and 10 students in medicine/health to understand the effect of
exploring distance-based uncertainty scores on users' reliance on AI. Our
findings showed that distance-based uncertainty scores outperformed traditional
probability-based uncertainty scores in identifying uncertain cases. In
addition, after exploring confidence scores for task delegation and reviewing
embedding-based visualizations of distance-based uncertainty scores,
participants achieved an 8.20% higher rate of correct decisions, a 7.15% higher
rate of changing their decisions to correct ones, and a 7.14% lower rate of
incorrect changes after reviewing AI outputs than those reviewing
probability-based uncertainty scores ($p<0.01$). Our findings highlight the
potential of distance-based uncertainty scores to enhance decision accuracy and
appropriate reliance on AI while discussing ongoing challenges for human-AI
collaborative decision-making.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [510] [Learning Probabilities of Causation from Finite Population Data](https://arxiv.org/abs/2505.17133)
*Shuai Wang,Song Jiang,Yizhou Sun,Judea Pearl,Ang Li*

Key words: 因果关系、机器学习、数据不足、概率预测、多层感知机

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 提出了一种机器学习方法，为数据不足的子群体预测因果概率，通过多层感知机和特定激活函数有效降低误差。

Motivation: 解决子群体数据不足时因果概率估算的挑战，利用已有充足数据的子群体信息进行预测。

Method: 使用多层感知机（MLP）和Mish激活函数，从已知PNS的子群体数据中学习并预测未知子群体的PNS。

Result: 在多个结构化因果模型上的模拟研究表明，该方法能有效预测PNS，平均绝对误差（MAE）约为0.02。

Conclusion: 适当选择模型和激活函数，机器学习方法能有效提升数据不足子群体的因果概率预测精度。

Abstract: Probabilities of causation play a crucial role in modern decision-making.
This paper addresses the challenge of predicting probabilities of causation for
subpopulations with \textbf{insufficient} data using machine learning models.
Tian and Pearl first defined and derived tight bounds for three fundamental
probabilities of causation: the probability of necessity and sufficiency (PNS),
the probability of sufficiency (PS), and the probability of necessity (PN).
However, estimating these probabilities requires both experimental and
observational distributions specific to each subpopulation, which are often
unavailable or impractical to obtain with limited population-level data.
Therefore, for most subgroups, the amount of data they have is not enough to
guarantee the accuracy of their probabilities. Hence, to estimate these
probabilities for subpopulations with \textbf{insufficient} data, we propose
using machine learning models that draw insights from subpopulations with
sufficient data. Our evaluation of multiple machine learning models indicates
that, given the population-level data and an appropriate choice of machine
learning model and activation function, PNS can be effectively predicted.
Through simulation studies on multiple Structured Causal Models (SCMs), we show
that our multilayer perceptron (MLP) model with the Mish activation function
achieves a mean absolute error (MAE) of approximately $0.02$ in predicting PNS
for $32,768$ subpopulations across most SCMs using data from only $2,000$
subpopulations with known PNS values.

</details>


### [511] [A Distributionally-Robust Framework for Nuisance in Causal Effect Estimation](https://arxiv.org/abs/2505.17717)
*Akira Tanimoto*

Key words: 因果推理, 逆概率加权, 分布鲁棒优化, 权重正则化

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文提出了一种结合分布鲁棒优化和权重正则化的方法，解决了因果推理中倾向得分估计不准确和权重不稳定的问题。

Motivation: 传统因果推理方法依赖逆概率加权（IPW），但倾向得分估计不准确和权重不稳定会影响模型性能。

Method: 通过对抗损失函数，结合分布鲁棒优化处理倾向得分不确定性，并基于加权Rademacher复杂度进行权重正则化。

Result: 在合成和真实数据集上，新方法一致优于现有技术。

Conclusion: 该方法有效解决了倾向得分估计和权重不稳定的挑战，提升了因果推理的鲁棒性。

Abstract: Causal inference requires evaluating models on balanced distributions between
treatment and control groups, while training data often exhibits imbalance due
to historical decision-making policies. Most conventional statistical methods
address this distribution shift through inverse probability weighting (IPW),
which requires estimating propensity scores as an intermediate step. These
methods face two key challenges: inaccurate propensity estimation and
instability from extreme weights. We decompose the generalization error to
isolate these issues--propensity ambiguity and statistical instability--and
address them through an adversarial loss function. Our approach combines
distributionally robust optimization for handling propensity uncertainty with
weight regularization based on weighted Rademacher complexity. Experiments on
synthetic and real-world datasets demonstrate consistent improvements over
existing methods.

</details>


### [512] [Liouville PDE-based sliced-Wasserstein flow for fair regression](https://arxiv.org/abs/2505.17204)
*Pilhwa Lee,Jayshawn Cooper*

Key words: 切片Wasserstein流, 公平回归, Liouville PDE, Kantorovich势, 梯度流

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文提出了一种改进的切片Wasserstein流（SWF）方法用于公平回归，通过去除扩散项和引入Kantorovich势，提升了训练和测试中的收敛性和减少方差。

Motivation: 为了解决公平回归问题，改进SWF的收敛性和稳定性，使其在生成样本时更高效。

Method: 通过将Fokker-Planck方程的随机扩散项转换为基于Liouville PDE的传输（无扩散项），并结合密度估计；利用Kantorovich势近似Wasserstein重心生成样本。

Result: 改进后的SWF在训练和测试中展现了更快的收敛性和更低的方差，公平回归实验表明其在准确性与公平性的帕累托曲线上表现优异。

Conclusion: 改进的SWF方法在公平回归中表现出色，提升了生成样本的效率和稳定性。

Abstract: The sliced Wasserstein flow (SWF), a nonparametric and implicit generative
gradient flow, is applied to fair regression. We have improved the SWF in a few
aspects. First, the stochastic diffusive term from the Fokker-Planck
equation-based Monte Carlo is transformed to Liouville partial differential
equation (PDE)-based transport with density estimation, however, without the
diffusive term. Now, the computation of the Wasserstein barycenter is
approximated by the SWF barycenter with the prescription of Kantorovich
potentials for the induced gradient flow to generate its samples. These two
efforts improve the convergence in training and testing SWF and SWF barycenters
with reduced variance. Applying the generative SWF barycenter for fair
regression demonstrates competent profiles in the accuracy-fairness Pareto
curves.

</details>


### [513] [Deconfounded Warm-Start Thompson Sampling with Applications to Precision Medicine](https://arxiv.org/abs/2505.17283)
*Prateek Jaiswal,Esmaeil Keyvanshokooh,Junyu Cao*

Key words: 随机临床试验, 观察数据, 混杂偏差, Thompson Sampling, 个性化治疗

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 提出了DWTS方法，利用观察数据和自适应临床试验结合，通过DDL减少混杂偏差，优化Thompson Sampling，提高试验效率。

Motivation: 解决临床研究中观察数据因混杂偏差和隐藏偏见而未被充分利用的问题，提升试验效率。

Method: 结合Doubly Debiased LASSO (DDL)和Thompson Sampling (LinTS)，利用可靠协变量和隐藏协变量减少上下文，优化先验分布。

Result: 在合成和真实心血管数据集上，DWTS比标准LinTS累积遗憾更低，试验效率更高。

Conclusion: DWTS通过结合观察数据和自适应试验，能有效减少混杂偏差，支持个性化治疗决策。

Abstract: Randomized clinical trials often require large patient cohorts before drawing
definitive conclusions, yet abundant observational data from parallel studies
remains underutilized due to confounding and hidden biases. To bridge this gap,
we propose Deconfounded Warm-Start Thompson Sampling (DWTS), a practical
approach that leverages a Doubly Debiased LASSO (DDL) procedure to identify a
sparse set of reliable measured covariates and combines them with key hidden
covariates to form a reduced context. By initializing Thompson Sampling (LinTS)
priors with DDL-estimated means and variances on these measured features --
while keeping uninformative priors on hidden features -- DWTS effectively
harnesses confounded observational data to kick-start adaptive clinical trials.
Evaluated on both a purely synthetic environment and a virtual environment
created using real cardiovascular risk dataset, DWTS consistently achieves
lower cumulative regret than standard LinTS, showing how offline causal
insights from observational data can improve trial efficiency and support more
personalized treatment decisions.

</details>


### [514] [Learning to Choose or Choosing to Learn: Best-of-N vs. Supervised Fine-Tuning for Bit String Generation](https://arxiv.org/abs/2505.17288)
*Seamus Somerstep,Vinod Raman,Unique Subedi,Yuekai Sun*

Key words: 大语言模型, 监督微调, Best-of-N, 可实现性, 收敛速率

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文比较了两种调整大语言模型以适应新任务的标准方法：监督微调（SFT）和Best-of-N（BoN）。在可实现的学习设定下，SFT的表现优于BoN；但在不可实现的情况下，BoN可能在某些失败模式下表现更好。

Motivation: 研究目的是理论比较两种适应大语言模型的任务调整方法，分析它们在不同学习设定下的表现差异。

Method: 通过理论分析，比较监督微调（SFT）和Best-of-N（BoN）在bit string生成问题上的收敛速率和性能。

Result: 在可实现学习设定下，SFT的收敛速率优于BoN；而在不可实现情况下，BoN的收敛速率可能在特定模式下更好。

Conclusion: 论文表明SFT和BoN各有优劣，具体选择取决于学习设定的可实现性。

Abstract: Using the bit string generation problem as a case study, we theoretically
compare two standard methods for adapting large language models to new tasks.
The first, referred to as supervised fine-tuning, involves training a new next
token predictor on good generations. The second method, Best-of-N, trains a
reward model to select good responses from a collection generated by an
unaltered base model. If the learning setting is realizable, we find that
supervised fine-tuning outperforms BoN through a better dependence on the
response length in its rate of convergence. If realizability fails, then
depending on the failure mode, BoN can enjoy a better rate of convergence in
either n or a rate of convergence with better dependence on the response
length.

</details>


### [515] [Optimal Transport with Heterogeneously Missing Data](https://arxiv.org/abs/2505.17291)
*Linus Bleistein,Aurélien Bellet,Julie Josse*

Key words: 最优传输、Wasserstein距离、缺失值、MCAR、ISVT、矩阵补全

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文研究了带有缺失值的最优传输问题，提出了去偏和高斯分布的Wasserstein距离估计方法，以及使用迭代奇异值阈值（ISVT）的高效熵正则化最优传输估计。

Motivation: 解决两个经验分布间缺失值的最优传输问题，尤其是数据完全随机缺失（MCAR）但概率不均的情况。

Method: 利用去偏技术处理Wasserstein距离和线性Monge映射，提出了基于迭代奇异值阈值（ISVT）的熵正则化最优传输估计方法及无验证集的超参数选择策略。

Result: 提出的方法在样本复杂度无明显增加的情况下，实现了高效且一致的估计，并通过大量数值应用验证了其有效性。

Conclusion: 论文提出的技术在最优传输问题中具有广泛应用潜力，尤其是在矩阵补全问题中。

Abstract: We consider the problem of solving the optimal transport problem between two
empirical distributions with missing values. Our main assumption is that the
data is missing completely at random (MCAR), but we allow for heterogeneous
missingness probabilities across features and across the two distributions. As
a first contribution, we show that the Wasserstein distance between empirical
Gaussian distributions and linear Monge maps between arbitrary distributions
can be debiased without significantly affecting the sample complexity.
Secondly, we show that entropic regularized optimal transport can be estimated
efficiently and consistently using iterative singular value thresholding
(ISVT). We propose a validation set-free hyperparameter selection strategy for
ISVT that leverages our estimator of the Bures-Wasserstein distance, which
could be of independent interest in general matrix completion problems.
Finally, we validate our findings on a wide range of numerical applications.

</details>


### [516] [Statistical Inference for Online Algorithms](https://arxiv.org/abs/2505.17300)
*Selina Carter,Arun K Kuchibhotla*

Key words: 置信区间, 假设检验, 在线算法, 随机梯度下降, 渐近正态估计

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文提出了一种基于在线算法输出的计算效率高、速率最优且渐近有效的置信区域构建方法，无需估计渐近方差，特别适用于随机梯度下降与Polyak平均的算法。

Motivation: 传统基于渐近正态估计的置信区间和假设检验方法需要估计渐近方差，而在线/序列算法限制了数据的多次访问，因此需要一种计算高效且不依赖渐近方差估计的推断方法。

Method: 通过在线算法（如随机梯度下降与Polyak平均）的输出构建置信区域，避免直接估计渐近方差，实现计算效率和统计性能的平衡。

Result: 所提方法在计算效率和统计性能上均表现优异，特别在随机梯度下降与Polyak平均的算法中验证了其实际性能。

Conclusion: 该研究为在线算法提供了无需估计渐近方差的推断方法，扩展了统计推断的适用场景，尤其在计算资源受限的情况下具有优势。

Abstract: Construction of confidence intervals and hypothesis tests for functionals
based on asymptotically normal estimators is a classical topic in statistical
inference. The simplest and in many cases optimal inference procedure is the
Wald interval or the likelihood ratio test, both of which require an estimator
and an estimate of the asymptotic variance of the estimator. Estimators
obtained from online/sequential algorithms forces one to consider the
computational aspects of the inference problem, i.e., one cannot access all of
the data as many times as needed. Several works on this topic explored the
online estimation of asymptotic variance. In this article, we propose
computationally efficient, rate-optimal, and asymptotically valid confidence
regions based on the output of online algorithms {\em without} estimating the
asymptotic variance. As a special case, this implies inference from any
algorithm that yields an asymptotically normal estimator. We focus our efforts
on stochastic gradient descent with Polyak averaging to understand the
practical performance of the proposed method.

</details>


### [517] [Repulsive Ensembles for Bayesian Inference in Physics-informed Neural Networks](https://arxiv.org/abs/2505.17308)
*Philipp Pilar,Markus Heinonen,Niklas Wahlström*

Key words: PINNs, 不确定性估计, 反问题, 排斥性集成, 贝叶斯后验, 微分方程

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 本工作提出了一种基于排斥性神经网络集成（RE-PINN）的方法，用于在物理信息神经网络（PINN）中解决微分方程的反问题，并生成不确定性估计。

Motivation: 在微分方程求解中，不确定性估计比点估计更受欢迎，因为它能提供解的准确性信息。标准的集成方法倾向于坍缩到最大后验解，因此需要一种方法提高不确定性估计的准确性和样本多样性。

Method: 通过向损失函数添加一个排斥项来实现排斥性集成（RE-PINN），使得在无限多集成成员时，集成预测对应于真实的贝叶斯后验。

Result: 排斥性集成比标准集成产生了更准确的不确定性估计，并展现了更高的样本多样性。

Conclusion: RE-PINN方法有效地提升了PINN在逆问题中的不确定性估计能力，优于蒙特卡洛基线。

Abstract: Physics-informed neural networks (PINNs) have proven an effective tool for
solving differential equations, in particular when considering non-standard or
ill-posed settings. When inferring solutions and parameters of the differential
equation from data, uncertainty estimates are preferable to point estimates, as
they give an idea about the accuracy of the solution. In this work, we consider
the inverse problem and employ repulsive ensembles of PINNs (RE-PINN) for
obtaining such estimates. The repulsion is implemented by adding a particular
repulsive term to the loss function, which has the property that the ensemble
predictions correspond to the true Bayesian posterior in the limit of infinite
ensemble members. Where possible, we compare the ensemble predictions to Monte
Carlo baselines. Whereas the standard ensemble tends to collapse to
maximum-a-posteriori solutions, the repulsive ensemble produces significantly
more accurate uncertainty estimates and exhibits higher sample diversity.

</details>


### [518] [DataRater: Meta-Learned Dataset Curation](https://arxiv.org/abs/2505.17895)
*Dan A. Calian,Gregory Farquhar,Iurii Kemaev,Luisa M. Zintgraf,Matteo Hessel,Jeremy Shar,Junhyuk Oh,András György,Tom Schaul,Jeffrey Dean,Hado van Hasselt,David Silver*

Key words: DataRater, 元学习, 数据筛选, 元梯度, 计算效率

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: DataRater通过元学习评估数据点的训练价值，显著提升计算效率。

Motivation: 传统数据集筛选方法依赖人工调整或启发式规则，缺乏可扩展性。DataRater旨在通过元学习实现更精细、高效的数据筛选。

Method: 提出DataRater，利用元梯度（meta-gradients）元学习，评估数据点的训练价值，目标是提升在保留数据上的训练效率。

Result: 在多种模型规模与数据集上的实验表明，DataRater能显著提升计算效率。

Conclusion: DataRater通过元学习实现了高效的数据筛选，为模型训练提供了更优的数据选择策略。

Abstract: The quality of foundation models depends heavily on their training data.
Consequently, great efforts have been put into dataset curation. Yet most
approaches rely on manual tuning of coarse-grained mixtures of large buckets of
data, or filtering by hand-crafted heuristics. An approach that is ultimately
more scalable (let alone more satisfying) is to \emph{learn} which data is
actually valuable for training. This type of meta-learning could allow more
sophisticated, fine-grained, and effective curation. Our proposed
\emph{DataRater} is an instance of this idea. It estimates the value of
training on any particular data point. This is done by meta-learning using
`meta-gradients', with the objective of improving training efficiency on held
out data. In extensive experiments across a range of model scales and datasets,
we find that using our DataRater to filter data is highly effective, resulting
in significantly improved compute efficiency.

</details>


### [519] [Offline Constrained Reinforcement Learning under Partial Data Coverage](https://arxiv.org/abs/2505.17506)
*Kihyuk Hong,Ambuj Tewari*

Key words: 

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 提出了一个基于线性规划的原对偶算法，用于离线约束强化学习，在部分数据覆盖下实现O(ϵ⁻²)样本复杂度，简化了先前的正则化需求。

Motivation: 研究离线约束强化学习，旨在从预收集的数据集中学习策略，最大化主奖励信号，同时确保多个辅助奖励信号的返回值超过预定义阈值。现有方法存在数据覆盖要求高、计算效率低或依赖额外辅助函数类的问题。

Method: 基于线性规划的oracle-efficient原对偶算法，通过拉格朗日分解提取策略，无需已知数据生成分布。

Result: 在部分数据覆盖下实现了O(ϵ⁻²)的样本复杂度，且通过引入可实现性假设，确保所有拉格朗日鞍点均为最优，无需正则化。

Conclusion: 该方法提升了离线约束强化学习的实用性，简化了分析过程，并在样本效率上优于现有方法。

Abstract: We study offline constrained reinforcement learning (RL) with general
function approximation. We aim to learn a policy from a pre-collected dataset
that maximizes the expected discounted cumulative reward for a primary reward
signal while ensuring that expected discounted returns for multiple auxiliary
reward signals are above predefined thresholds. Existing algorithms either
require fully exploratory data, are computationally inefficient, or depend on
an additional auxiliary function classes to obtain an $\epsilon$-optimal policy
with sample complexity $O(\epsilon^{-2})$. In this paper, we propose an
oracle-efficient primal-dual algorithm based on a linear programming (LP)
formulation, achieving $O(\epsilon^{-2})$ sample complexity under partial data
coverage. By introducing a realizability assumption, our approach ensures that
all saddle points of the Lagrangian are optimal, removing the need for
regularization that complicated prior analyses. Through Lagrangian
decomposition, our method extracts policies without requiring knowledge of the
data-generating distribution, enhancing practical applicability.

</details>


### [520] [Optimal Online Change Detection via Random Fourier Features](https://arxiv.org/abs/2505.17789)
*Florian Kalinke,Shakeel Gavioli-Akilagun*

Key words: 在线变点检测、非参数方法、核方法、随机傅里叶特征、多变量数据流

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文提出了一种基于核方法的在线非参数变点检测算法，具有对数时间与空间复杂度，无需预训练数据或窗口参数设置，并在理论与实验中验证了其优越性。

Motivation: 研究多变量数据流中的在线非参数变点检测问题，旨在克服现有方法需要预训练数据或手动设置窗口参数的局限性。

Method: 基于核方法的双样本检验，利用随机傅里叶特征构建序列化检测流程，实现了对数级时间与空间复杂度。

Result: 理论证明算法具有最优的检测延迟（minimax意义），数值实验在真实与合成数据中均表现优于现有方法。

Conclusion: 提出的算法是真正在线的、无需预训练或参数调优，且在理论与实际中均验证了其高效性与鲁棒性。

Abstract: This article studies the problem of online non-parametric change point
detection in multivariate data streams. We approach the problem through the
lens of kernel-based two-sample testing and introduce a sequential testing
procedure based on random Fourier features, running with logarithmic time
complexity per observation and with overall logarithmic space complexity. The
algorithm has two advantages compared to the state of the art. First, our
approach is genuinely online, and no access to training data known to be from
the pre-change distribution is necessary. Second, the algorithm does not
require the user to specify a window parameter over which local tests are to be
calculated. We prove strong theoretical guarantees on the algorithm's
performance, including information-theoretic bounds demonstrating that the
detection delay is optimal in the minimax sense. Numerical studies on real and
synthetic data show that our algorithm is competitive with respect to the state
of the art.

</details>


### [521] [Quantifying uncertainty in spectral clusterings: expectations for perturbed and incomplete data](https://arxiv.org/abs/2505.17819)
*Jürgen Dölz,Jolanda Weygandt*

Key words: 谱聚类, 随机集理论, 蒙特卡洛方法, 数据不确定性, 统计预期聚类

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文提出了一种基于随机集理论的数学框架，用于处理含测量误差或缺失数据的谱聚类问题，并通过蒙特卡洛方法计算统计预期的聚类结果。

Motivation: 传统谱聚类在处理含噪声、缺失或无效数据时，聚类结果可能不可靠。论文旨在通过随机集理论量化这种不确定性，提升聚类的鲁棒性。

Method: 采用随机集理论建模数据不确定性，结合蒙特卡洛方法近似计算统计预期的聚类结果，并提出了几种计算可行的指标。

Result: 理论分析了所提指标在数据点和蒙特卡洛样本趋于无穷时的收敛性，并通过数值实验验证了方法的有效性。

Conclusion: 该框架为含噪声数据的谱聚类提供了理论支持和实用工具，增强了聚类结果的可靠性。

Abstract: Spectral clustering is a popular unsupervised learning technique which is
able to partition unlabelled data into disjoint clusters of distinct shapes.
However, the data under consideration are often experimental data, implying
that the data is subject to measurement errors and measurements may even be
lost or invalid. These uncertainties in the corrupted input data induce
corresponding uncertainties in the resulting clusters, and the clusterings thus
become unreliable.
  Modelling the uncertainties as random processes, we discuss a mathematical
framework based on random set theory for the computational Monte Carlo
approximation of statistically expected clusterings in case of corrupted, i.e.,
perturbed, incomplete, and possibly even additional, data. We propose several
computationally accessible quantities of interest and analyze their consistency
in the infinite data point and infinite Monte Carlo sample limit. Numerical
experiments are provided to illustrate and compare the proposed quantities.

</details>


### [522] [Robust Distributed Estimation: Extending Gossip Algorithms to Ranking and Trimmed Means](https://arxiv.org/abs/2505.17836)
*Anna Van Elst,Igor Colin,Stephan Clémençon*

Key words: gossip 算法，鲁棒估计，去中心化，秩估计，修剪均值

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 本文提出了一种名为 GoRank 和 GoTrim 的鲁棒估计算法，用于在任意通信图上实现去中心化的抗恶意节点均值估计，并分析了其收敛性和性能。

Motivation: 现有基于均值的 gossip 算法易受恶意或损坏节点影响，本文旨在解决这一鲁棒性问题。

Method: 通过全局估计鲁棒统计量，提出 GoRank 进行秩估计，并基于此设计 GoTrim 实现修剪均值估计。

Result: 在秩估计上达到 O(1/t) 收敛速度，修剪均值估计上达到 O(log(t)/t)，并在实验中验证了其性能。

Conclusion: GoTrim 算法在多种网络拓扑和数据分布下表现出良好的鲁棒性和收敛性。

Abstract: This paper addresses the problem of robust estimation in gossip algorithms
over arbitrary communication graphs. Gossip algorithms are fully decentralized,
relying only on local neighbor-to-neighbor communication, making them
well-suited for situations where communication is constrained. A fundamental
challenge in existing mean-based gossip algorithms is their vulnerability to
malicious or corrupted nodes. In this paper, we show that an outlier-robust
mean can be computed by globally estimating a robust statistic. More
specifically, we propose a novel gossip algorithm for rank estimation, referred
to as \textsc{GoRank}, and leverage it to design a gossip procedure dedicated
to trimmed mean estimation, coined \textsc{GoTrim}. In addition to a detailed
description of the proposed methods, a key contribution of our work is a
precise convergence analysis: we establish an $\mathcal{O}(1/t)$ rate for rank
estimation and an $\mathcal{O}(\log(t)/t)$ rate for trimmed mean estimation,
where by $t$ is meant the number of iterations. Moreover, we provide a
breakdown point analysis of \textsc{GoTrim}. We empirically validate our
theoretical results through experiments on diverse network topologies, data
distributions and contamination schemes.

</details>


### [523] [Continuum Transformers Perform In-Context Learning by Operator Gradient Descent](https://arxiv.org/abs/2505.17838)
*Abhiti Mishra,Yash Patel,Ambuj Tewari*

Key words: 连续变换器, 上下文学习, 算子学习, 梯度下降, 贝叶斯最优预测器

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文证明了连续变换器通过其前向传播隐式实现了算子学习中的梯度下降，并在无限深度极限下学习到贝叶斯最优预测器。

Motivation: 研究连续变换器在上下文学习中的理论特性，以理解其如何通过前向传播隐式执行梯度下降并学习最优算子。

Method: 利用广义表示定理和希尔伯特空间上的梯度流，证明连续变换器在算子RKHS中执行梯度下降。

Result: 理论证明连续变换器在无限深度极限下学习到贝叶斯最优预测器，并通过实验验证参数恢复。

Conclusion: 连续变换器通过隐式梯度下降实现上下文算子学习，并在理论上达到最优性能。

Abstract: Transformers robustly exhibit the ability to perform in-context learning,
whereby their predictive accuracy on a task can increase not by parameter
updates but merely with the placement of training samples in their context
windows. Recent works have shown that transformers achieve this by implementing
gradient descent in their forward passes. Such results, however, are restricted
to standard transformer architectures, which handle finite-dimensional inputs.
In the space of PDE surrogate modeling, a generalization of transformers to
handle infinite-dimensional function inputs, known as "continuum transformers,"
has been proposed and similarly observed to exhibit in-context learning.
Despite impressive empirical performance, such in-context learning has yet to
be theoretically characterized. We herein demonstrate that continuum
transformers perform in-context operator learning by performing gradient
descent in an operator RKHS. We demonstrate this using novel proof strategies
that leverage a generalized representer theorem for Hilbert spaces and gradient
flows over the space of functionals of a Hilbert space. We additionally show
the operator learned in context is the Bayes Optimal Predictor in the infinite
depth limit of the transformer. We then provide empirical validations of this
optimality result and demonstrate that the parameters under which such gradient
descent is performed are recovered through the continuum transformer training.

</details>


### [524] [Function Forms of Simple ReLU Networks with Random Hidden Weights](https://arxiv.org/abs/2505.17907)
*Ka Long Keith Ho,Yoshinari Takeishi,Junichi Takeuchi*

Key words: Fisher信息矩阵, ReLU神经网络, 函数空间, 梯度下降, 无限宽度极限

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该研究探讨了无限宽度极限下两层ReLU神经网络在函数空间的动态，重点分析了Fisher信息矩阵（FIM）对学习过程的指导作用，通过理论推导和模拟验证，揭示了梯度下降优先选择的函数形式及其在参数和函数空间之间的新联系。

Motivation: 研究旨在揭示无限宽度神经网络中FIM如何影响学习动态，并扩展对ReLU网络优化和表达能力的理论理解。

Method: 通过近似FIM的特征分解，推导了四种近似特征向量对应的基函数的渐进行为，并分析了它们在函数空间中的正交性。通过仿真验证理论推导。

Result: 研究发现梯度下降优先选择的基函数在函数空间中表现出准正交性，为参数空间和函数空间之间建立了新的联系。模拟结果验证了理论推导的准确性。

Conclusion: 研究为理解宽度神经网络提供了理论基础，深化了对可扩展深度学习架构的认知，有助于改进神经网络的设计与分析。

Abstract: We investigate the function space dynamics of a two-layer ReLU neural network
in the infinite-width limit, highlighting the Fisher information matrix (FIM)'s
role in steering learning. Extending seminal works on approximate
eigendecomposition of the FIM, we derive the asymptotic behavior of basis
functions ($f_v(x) = X^{\top} v $) for four groups of approximate eigenvectors,
showing their convergence to distinct function forms. These functions,
prioritized by gradient descent, exhibit FIM-induced inner products that
approximate orthogonality in the function space, forging a novel connection
between parameter and function spaces. Simulations validate the accuracy of
these theoretical approximations, confirming their practical relevance. By
refining the function space inner product's role, we advance the theoretical
framework for ReLU networks, illuminating their optimization and expressivity.
Overall, this work offers a robust foundation for understanding wide neural
networks and enhances insights into scalable deep learning architectures,
paving the way for improved design and analysis of neural networks.

</details>


### [525] [M-learner:A Flexible And Powerful Framework To Study Heterogeneous Treatment Effect In Mediation Model](https://arxiv.org/abs/2505.17917)
*Xingyu Li,Qing Liu,Tony Jiang,Hong Amy Xia,Brian P. Hobbs,Peng Wei*

Key words: M-learner, 效应异质性, 中介分析, 亚组识别, 降维聚类

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 提出了一种名为M-learner的新方法，用于估计异质性间接和总治疗效果，并在中介框架内识别相关亚组。该方法通过计算个体层面间接/总效果、构建距离矩阵、降维聚类和优化配置四个步骤实现。其创新性在于首次针对中介作用下的效应异质性设计，实验和实际数据集应用验证了其有效性和适应性。

Motivation: 现有方法在处理中介作用下的治疗效应异质性时存在不足，因此需要一种能直接捕捉此类异质性并识别相关亚组的新方法。

Method: 1. 计算个体层面间接/总效果；2. 构建基于差异的距离矩阵；3. 使用tSNE降维并结合K-means聚类；4. 通过阈值优化亚组配置。

Result: 实验证明该方法稳健有效，Jobs II数据集的实证应用展示了其广泛适应性和潜在实用价值。

Conclusion: M-learner是首个针对中介效应异质性的方法，通过多步骤流程实现了亚组识别和效果估计，为相关研究提供了新工具。

Abstract: We propose a novel method, termed the M-learner, for estimating heterogeneous
indirect and total treatment effects and identifying relevant subgroups within
a mediation framework. The procedure comprises four key steps. First, we
compute individual-level conditional average indirect/total treatment effect
Second, we construct a distance matrix based on pairwise differences. Third, we
apply tSNE to project this matrix into a low-dimensional Euclidean space,
followed by K-means clustering to identify subgroup structures. Finally, we
calibrate and refine the clusters using a threshold-based procedure to
determine the optimal configuration. To the best of our knowledge, this is the
first approach specifically designed to capture treatment effect heterogeneity
in the presence of mediation. Experimental results validate the robustness and
effectiveness of the proposed framework. Application to the real-world Jobs II
dataset highlights the broad adaptability and potential applicability of our
method.Code is available at https: //anonymous.4open.science/r/M-learner-C4BB.

</details>


### [526] [The Nuclear Route: Sharp Asymptotics of ERM in Overparameterized Quadratic Networks](https://arxiv.org/abs/2505.17958)
*Vittorio Erba,Emanuele Troiani,Lenka Zdeborová,Florent Krzakala*

Key words: 过参数化神经网络, 经验风险最小化, 低秩矩阵传感, 泛化阈值, 二次激活函数

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 本文研究了具有二次激活函数的两层神经网络在高维情况下的经验风险最小化（ERM）行为，通过映射到核范数惩罚的凸矩阵传感任务，揭示了学习中低秩结构的出现及其对泛化性能的影响。

Motivation: 探讨过参数化神经网络在训练和泛化中的表现，尤其是在高维数据下的行为，揭示其背后的统计和优化机制。

Method: 将$ℓ_2$正则化学习问题转化为核范数惩罚的凸矩阵传感任务，结合自旋玻璃方法、矩阵分解和凸优化技术进行分析。

Result: 发现了学习过程中的低秩特征结构，并精确描述了损失函数的全局极小值和泛化阈值，表明目标函数的宽度决定了可学习性。

Conclusion: 研究结果将二次神经网络学习与低秩矩阵传感联系起来，为理解过参数化网络的容量控制提供了新的视角。

Abstract: We study the high-dimensional asymptotics of empirical risk minimization
(ERM) in over-parametrized two-layer neural networks with quadratic activations
trained on synthetic data. We derive sharp asymptotics for both training and
test errors by mapping the $\ell_2$-regularized learning problem to a convex
matrix sensing task with nuclear norm penalization. This reveals that capacity
control in such networks emerges from a low-rank structure in the learned
feature maps. Our results characterize the global minima of the loss and yield
precise generalization thresholds, showing how the width of the target function
governs learnability. This analysis bridges and extends ideas from spin-glass
methods, matrix factorization, and convex optimization and emphasizes the deep
link between low-rank matrix sensing and learning in quadratic neural networks.

</details>


### [527] [Anytime-valid, Bayes-assisted,Prediction-Powered Inference](https://arxiv.org/abs/2505.18000)
*Valentin Kilian,Stefano Cortinovis,François Caron*

Key words: 预测驱动推断，置信序列，Ville不等式，混合方法，统计效率

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文提出了一种在序列环境中扩展预测驱动推断（PPI）的方法，通过利用Ville不等式和混合方法，构建了均匀有效的预测驱动置信序列。

Motivation: 鉴于大量未标记数据和少量标记数据的现实，研究如何利用机器学习预测提升统计效率，同时保持固定时间有效性。

Method: 结合Ville不等式和混合方法，构建预测驱动置信序列，支持随时间动态调整并融入预测质量先验知识。

Result: 在真实和合成数据上验证了方法的有效性，显著提升了统计效率。

Conclusion: 该方法在序列环境中扩展了PPI框架，提供了均匀有效的置信序列，且能灵活结合先验知识。

Abstract: Given a large pool of unlabelled data and a smaller amount of labels,
prediction-powered inference (PPI) leverages machine learning predictions to
increase the statistical efficiency of standard confidence interval procedures
based solely on labelled data, while preserving their fixed-time validity.
  In this paper, we extend the PPI framework to the sequential setting, where
labelled and unlabelled datasets grow over time.
  Exploiting Ville's inequality and the method of mixtures, we propose
prediction-powered confidence sequence procedures that are valid uniformly over
time and naturally accommodate prior knowledge on the quality of the
predictions to further boost efficiency.
  We carefully illustrate the design choices behind our method and demonstrate
its effectiveness in real and synthetic examples.

</details>


### [528] [Bayesian Deep Learning for Discrete Choice](https://arxiv.org/abs/2505.18077)
*Daniel F. Villarraga,Ricardo A. Daziano*

Key words: 离散选择模型, 深度学习, 贝叶斯推断, 随机梯度朗之万动力学, 预测与推断

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文提出了一种结合深度学习与贝叶斯推断的离散选择模型，旨在解决传统模型预测性能不足与深度学习模型解释性差的问题，并通过模拟和实证案例验证其有效性。

Motivation: 传统离散选择模型（DCMs）在解释性和经济变量推断上表现优异，但预测性能较差；深度学习模型预测能力强但缺乏解释性和稳定性。论文旨在结合两者优势，设计一个既能推断经济变量又保持高预测性能的模型。

Method: 提出了一种专为近似贝叶斯推断（如随机梯度朗之万动力学，SGLD）设计的深度学习架构。该模型在数据有限时退化为行为假设以避免过拟合，数据充足时则捕捉复杂非线性关系。

Result: 通过蒙特卡洛模拟和两个实证案例（纽约出行方式选择与瑞士火车选择偏好数据）验证了模型的预测性能（如样本外平衡准确率）和推断能力（如边际替代率区间估计的覆盖性）。

Conclusion: 该模型成功整合了深度学习的预测优势与贝叶斯推断的稳定性与解释性，为离散选择问题提供了更全面的解决方案。

Abstract: Discrete choice models (DCMs) are used to analyze individual decision-making
in contexts such as transportation choices, political elections, and consumer
preferences. DCMs play a central role in applied econometrics by enabling
inference on key economic variables, such as marginal rates of substitution,
rather than focusing solely on predicting choices on new unlabeled data.
However, while traditional DCMs offer high interpretability and support for
point and interval estimation of economic quantities, these models often
underperform in predictive tasks compared to deep learning (DL) models. Despite
their predictive advantages, DL models remain largely underutilized in discrete
choice due to concerns about their lack of interpretability, unstable parameter
estimates, and the absence of established methods for uncertainty
quantification. Here, we introduce a deep learning model architecture
specifically designed to integrate with approximate Bayesian inference methods,
such as Stochastic Gradient Langevin Dynamics (SGLD). Our proposed model
collapses to behaviorally informed hypotheses when data is limited, mitigating
overfitting and instability in underspecified settings while retaining the
flexibility to capture complex nonlinear relationships when sufficient data is
available. We demonstrate our approach using SGLD through a Monte Carlo
simulation study, evaluating both predictive metrics--such as out-of-sample
balanced accuracy--and inferential metrics--such as empirical coverage for
marginal rates of substitution interval estimates. Additionally, we present
results from two empirical case studies: one using revealed mode choice data in
NYC, and the other based on the widely used Swiss train choice stated
preference data.

</details>


### [529] [Scalable Policy Maximization Under Network Interference](https://arxiv.org/abs/2505.18118)
*Aidan Gleich,Eric Laber,Alexander Volfovsky*

Key words: 多臂老虎机、干扰、动态网络、Thompson采样、因果推断

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文提出了一种在动态网络中处理干扰现象的Thompson采样算法，解决了传统方法在样本量扩展上的限制，并在模拟实验中表现出色。

Motivation: 现实中的许多干预措施（如疫苗试验或在线优惠券）需要在不完全了解效果的情况下顺序分配，同时个体间的干扰现象使得传统的多臂老虎机算法不再适用。

Method: 通过常见的干扰结构假设将奖励线性化，开发了一种可扩展的Thompson采样算法，适用于每轮观察新网络的情况。

Result: 算法在模拟中学习速度快且优于现有方法，证明了在节点数和轮数上的贝叶斯遗憾界是次线性的。

Conclusion: 该算法填补了因果推断方法与实际老虎机算法在可扩展性上的关键差距，为大规模网络系统中的策略优化提供了可能。

Abstract: Many interventions, such as vaccines in clinical trials or coupons in online
marketplaces, must be assigned sequentially without full knowledge of their
effects. Multi-armed bandit algorithms have proven successful in such settings.
However, standard independence assumptions fail when the treatment status of
one individual impacts the outcomes of others, a phenomenon known as
interference. We study optimal-policy learning under interference on a dynamic
network. Existing approaches to this problem require repeated observations of
the same fixed network and struggle to scale in sample size beyond as few as
fifteen connected units -- both limit applications. We show that under common
assumptions on the structure of interference, rewards become linear. This
enables us to develop a scalable Thompson sampling algorithm that maximizes
policy impact when a new $n$-node network is observed each round. We prove a
Bayesian regret bound that is sublinear in $n$ and the number of rounds.
Simulation experiments show that our algorithm learns quickly and outperforms
existing methods. The results close a key scalability gap between causal
inference methods for interference and practical bandit algorithms, enabling
policy optimization in large-scale networked systems.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [530] [Transfer Faster, Price Smarter: Minimax Dynamic Pricing under Cross-Market Preference Shift](https://arxiv.org/abs/2505.17203)
*Yi Zhang,Elynn Chen,Yujun Yan*

Key words: 动态定价，迁移学习，鲁棒聚合，收益优化，核希尔伯特空间

<details>
  <summary>Details</summary>

Main category: stat.ME

TL;DR: 论文提出CM-TDP算法，利用辅助市场的偏好差异，在动态定价中实现最小最优遗憾，适用于线性和非线性需求模型，实验显示比单市场基线效果显著提升。

Motivation: 研究如何通过利用辅助市场的偏好差异，改进目标市场的动态定价策略，以实现更高效的收益优化。

Method: 提出CM-TDP算法，结合迁移学习、鲁棒聚合和收益优化，处理线性和非线性需求模型的偏好偏移问题。

Result: 在线性模型中遗憾为$	ilde{O}((d*K^{-1}+s_{0})\log T)$；在非线性模型中遗憾匹配信息论下界。实验显示累计遗憾降低50%，学习速度提升5倍。

Conclusion: CM-TDP通过多市场转移学习，显著提升定价效率和性能，为动态定价系统提供了新的解决方案。

Abstract: We study contextual dynamic pricing when a target market can leverage K
auxiliary markets -- offline logs or concurrent streams -- whose mean utilities
differ by a structured preference shift. We propose Cross-Market Transfer
Dynamic Pricing (CM-TDP), the first algorithm that provably handles such
model-shift transfer and delivers minimax-optimal regret for both linear and
non-parametric utility models.
  For linear utilities of dimension d, where the difference between source- and
target-task coefficients is $s_{0}$-sparse, CM-TDP attains regret
$\tilde{O}((d*K^{-1}+s_{0})\log T)$. For nonlinear demand residing in a
reproducing kernel Hilbert space with effective dimension $\alpha$, complexity
$\beta$ and task-similarity parameter $H$, the regret becomes
$\tilde{O}\!(K^{-2\alpha\beta/(2\alpha\beta+1)}T^{1/(2\alpha\beta+1)} +
H^{2/(2\alpha+1)}T^{1/(2\alpha+1)})$, matching information-theoretic lower
bounds up to logarithmic factors. The RKHS bound is the first of its kind for
transfer pricing and is of independent interest.
  Extensive simulations show up to 50% lower cumulative regret and 5 times
faster learning relative to single-market pricing baselines. By bridging
transfer learning, robust aggregation, and revenue optimization, CM-TDP moves
toward pricing systems that transfer faster, price smarter.

</details>


### [531] [Federated Causal Inference from Multi-Site Observational Data via Propensity Score Aggregation](https://arxiv.org/abs/2505.17961)
*Khellaf Rémi,Bellet Aurélien,Josse Julie*

Key words: 因果推断，联邦学习，平均处理效应，倾向得分，去中心化数据

<details>
  <summary>Details</summary>

Main category: stat.ME

TL;DR: 提出一种基于联邦学习的去中心化平均处理效应（ATE）估计方法，通过交换聚合统计而非个体数据解决隐私和合规性问题。

Motivation: 由于隐私、物流或法律限制，个体数据通常分散存储，集中处理不可行。需要一种方法能在去中心化环境下进行因果推断。

Method: 采用联邦学习，提出两种权重方案（MW和DW）估计倾向得分，并构建Fed-IPW和Fed-AIPW估计器。

Result: Fed-IPW和Fed-AIPW在样本量、处理机制和协变量分布异质性下表现良好，优于传统元分析方法。

Conclusion: 所提方法在保护隐私的同时有效估计ATE，特别适用于数据分散的场景。

Abstract: Causal inference typically assumes centralized access to individual-level
data. Yet, in practice, data are often decentralized across multiple sites,
making centralization infeasible due to privacy, logistical, or legal
constraints. We address this by estimating the Average Treatment Effect (ATE)
from decentralized observational data using federated learning, which enables
inference through the exchange of aggregate statistics rather than
individual-level data. We propose a novel method to estimate propensity scores
in a (non-)parametric manner by computing a federated weighted average of local
scores, using two theoretically grounded weighting schemes -- Membership
Weights (MW) and Density Ratio Weights (DW) -- that balance communication
efficiency and model flexibility. These federated scores are then used to
construct two ATE estimators: the Federated Inverse Propensity Weighting
estimator (Fed-IPW) and its augmented variant (Fed-AIPW). Unlike meta-analysis
methods, which fail when any site violates positivity, our approach leverages
heterogeneity in treatment assignment across sites to improve overlap. We show
that Fed-IPW and Fed-AIPW perform well under site-level heterogeneity in sample
sizes, treatment mechanisms, and covariate distributions, with theoretical
analysis and experiments on simulated and real-world data highlighting their
strengths and limitations relative to meta-analysis and related methods.

</details>


### [532] [Efficient Adaptive Experimentation with Non-Compliance](https://arxiv.org/abs/2505.17468)
*Miruna Oprescu,Brian M Cho,Nathan Kallus*

Key words: ATE, 工具变量, 半参数效率, 自适应实验, 多重稳健性

<details>
  <summary>Details</summary>

Main category: stat.ME

TL;DR: 该论文提出了AMRIV方法，用于在二元工具变量下估计平均处理效应（ATE），通过半参数效率理论推导出最优分配规则，并结合自适应策略和多重稳健估计器，提高了效率与鲁棒性。

Motivation: 研究中需要处理工具变量仅限于鼓励而非直接分配治疗的情况，旨在提升ATE估计的效率与鲁棒性。

Method: 基于半参数效率理论，设计了方差感知的最优分配规则，并引入了AMRIV方法，结合在线策略与序列影响函数估计器。

Result: AMRIV达到半参数效率边界，同时保持多重稳健一致性，实证显示相比基线方法效率更高、鲁棒性更强。

Conclusion: AMRIV通过自适应工具分配与高效估计器相结合，显著提升了ATE估计的性能。

Abstract: We study the problem of estimating the average treatment effect (ATE) in
adaptive experiments where treatment can only be encouraged--rather than
directly assigned--via a binary instrumental variable. Building on
semiparametric efficiency theory, we derive the efficiency bound for ATE
estimation under arbitrary, history-dependent instrument-assignment policies,
and show it is minimized by a variance-aware allocation rule that balances
outcome noise and compliance variability. Leveraging this insight, we introduce
AMRIV--an \textbf{A}daptive, \textbf{M}ultiply-\textbf{R}obust estimator for
\textbf{I}nstrumental-\textbf{V}ariable settings with variance-optimal
assignment. AMRIV pairs (i) an online policy that adaptively approximates the
optimal allocation with (ii) a sequential, influence-function-based estimator
that attains the semiparametric efficiency bound while retaining
multiply-robust consistency. We establish asymptotic normality, explicit
convergence rates, and anytime-valid asymptotic confidence sequences that
enable sequential inference. Finally, we demonstrate the practical
effectiveness of our approach through empirical studies, showing that adaptive
instrument assignment, when combined with the AMRIV estimator, yields improved
efficiency and robustness compared to existing baselines.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [533] [DailyQA: A Benchmark to Evaluate Web Retrieval Augmented LLMs Based on Capturing Real-World Changes](https://arxiv.org/abs/2505.17162)
*Jiehan Cheng,Zhicheng Dou*

Key words: DailyQA, 动态数据集, 大语言模型, RAG, 时效性信息

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: DailyQA是一个动态数据集，每周自动更新，包含任意日期的问答。评估表明，LLMs在处理频繁更新的信息时仍有挑战。

Motivation: 研究动机是构建一个自动化动态更新的数据集，用于评估大语言模型（LLMs）处理快速变化事实信息的能力。

Method: 利用维基百科修订日志自动更新数据，实现数据过滤、问题生成、质量检查、答案提取和问题分类的全流程。通过RAG流水线（增强网络搜索）评估不同模型的性能。

Result: 实验表明，LLMs在处理时效性强的网络信息时仍有困难，网络检索结果的重新排序是关键。

Conclusion: DailyQA为LLMs和RAG系统的优化方向提供了有价值的参考。

Abstract: We propose DailyQA, an automatically updated dynamic dataset that updates
questions weekly and contains answers to questions on any given date. DailyQA
utilizes daily updates from Wikipedia revision logs to implement a fully
automated pipeline of data filtering, query generation synthesis, quality
checking, answer extraction, and query classification. The benchmark requires
large language models (LLMs) to process and answer questions involving
fast-changing factual data and covering multiple domains. We evaluate several
open-source and closed-source LLMs using different RAG pipelines with web
search augmentation. We compare the ability of different models to process
time-sensitive web information and find that rerank of web retrieval results is
critical. Our results indicate that LLMs still face significant challenges in
handling frequently updated information, suggesting that DailyQA benchmarking
provides valuable insights into the direction of progress for LLMs and RAG
systems.

</details>


### [534] [BehaveGPT: A Foundation Model for Large-scale User Behavior Modeling](https://arxiv.org/abs/2505.17631)
*Jiahui Gong,Jingtao Ding,Fanjin Meng,Chen Yang,Hong Chen,Zuojian Wang,Haisheng Lu,Yong Li*

Key words: 用户行为建模, transformer, 预训练范式, 扩展规律

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: BehaveGPT 是一种基于 transformer 架构的基础模型，专为大规模用户行为预测设计，通过新颖的预训练范式显著提升行为预测能力，优于现有基线方法。

Motivation: 当前基础模型在语言和视觉领域表现优异，但在用户行为建模上进展有限，主要因为行为数据复杂且难以捕捉时空和上下文关系。

Method: 采用 transformer 架构和 DRO 预训练范式，训练于大规模用户行为数据，支持多种下游任务。

Result: 在真实数据集上优于现有方法，宏召回率和加权召回率提升 10%，首次在用户行为领域测量了扩展规律。

Conclusion: BehaveGPT 能有效建模和预测用户行为，为相关领域的研究和应用提供了新思路。

Abstract: In recent years, foundational models have revolutionized the fields of
language and vision, demonstrating remarkable abilities in understanding and
generating complex data; however, similar advances in user behavior modeling
have been limited, largely due to the complexity of behavioral data and the
challenges involved in capturing intricate temporal and contextual
relationships in user activities. To address this, we propose BehaveGPT, a
foundational model designed specifically for large-scale user behavior
prediction. Leveraging transformer-based architecture and a novel pretraining
paradigm, BehaveGPT is trained on vast user behavior datasets, allowing it to
learn complex behavior patterns and support a range of downstream tasks,
including next behavior prediction, long-term generation, and cross-domain
adaptation. Our approach introduces the DRO-based pretraining paradigm tailored
for user behavior data, which improves model generalization and transferability
by equitably modeling both head and tail behaviors. Extensive experiments on
real-world datasets demonstrate that BehaveGPT outperforms state-of-the-art
baselines, achieving more than a 10% improvement in macro and weighted recall,
showcasing its ability to effectively capture and predict user behavior.
Furthermore, we measure the scaling law in the user behavior domain for the
first time on the Honor dataset, providing insights into how model performance
scales with increased data and parameter sizes.

</details>


### [535] [Content Moderation in TV Search: Balancing Policy Compliance, Relevance, and User Experience](https://arxiv.org/abs/2505.17207)
*Adeep Hande,Kishorekumar Sundararajan,Sardar Hamidian,Ferhan Ture*

Key words: 内容审核, 大语言模型, 搜索系统, 用户体验, 质量保障

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 论文提出了一种基于大语言模型（LLM）的额外监控层，用于增强娱乐平台的内容审核，通过标志用户非预期搜索内容来提升搜索模型的可靠性和安全性。

Motivation: 现代搜索系统虽采用深度学习和大语言模型技术，但仍可能因模型不可预测性、元数据错误或设计缺陷而返回不适当或无关内容，影响用户信任和业务成果。

Method: 引入大语言模型（LLM）作为额外监控层，检测并标注用户非预期的搜索结果，并将反馈用于优化初始搜索模型的检索机制。

Result: 该方法为产品提供了质量保障基准，确保了搜索体验的安全性和可靠性。

Conclusion: 通过LLM的监控层能有效提升搜索系统的内容审核能力，减少不相关或不当内容的展示。

Abstract: Millions of people rely on search functionality to find and explore content
on entertainment platforms. Modern search systems use a combination of
candidate generation and ranking approaches, with advanced methods leveraging
deep learning and LLM-based techniques to retrieve, generate, and categorize
search results. Despite these advancements, search algorithms can still surface
inappropriate or irrelevant content due to factors like model unpredictability,
metadata errors, or overlooked design flaws. Such issues can misalign with
product goals and user expectations, potentially harming user trust and
business outcomes. In this work, we introduce an additional monitoring layer
using Large Language Models (LLMs) to enhance content moderation. This
additional layer flags content if the user did not intend to search for it.
This approach serves as a baseline for product quality assurance, with
collected feedback used to refine the initial retrieval mechanisms of the
search model, ensuring a safer and more reliable user experience.

</details>


### [536] [Bidirectional Knowledge Distillation for Enhancing Sequential Recommendation with Large Language Models](https://arxiv.org/abs/2505.18120)
*Jiongran Wu,Jiahao Liu,Dongsheng Li,Guangping Zhang,Mingzhe Han,Hansu Gu,Peng Zhang,Li Shang,Tun Lu,Ning Gu*

Key words: 大型语言模型, 传统推荐模型, 双向蒸馏, 动态知识交换, 推荐系统

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 本文提出LLMD4Rec框架，通过双向蒸馏实现大型语言模型（LLMs）与传统推荐模型（CRMs）的动态知识交换，提升推荐准确性且不增加推理成本。

Motivation: LLMs与CRMs结合时存在高推理成本和静态知识迁移问题，需要一种动态、双向的知识交换方法。

Method: 提出LLMD4Rec框架，通过样本自适应加权和输出分布对齐，实现LLMs与CRMs的迭代优化。

Result: 实验表明LLMD4Rec在多个基准数据集上显著提高推荐准确性，且不增加推理成本。

Conclusion: LLMD4Rec为LLMs与CRMs的结合提供了一种高效、可扩展的解决方案。

Abstract: Large language models (LLMs) have demonstrated exceptional performance in
understanding and generating semantic patterns, making them promising
candidates for sequential recommendation tasks. However, when combined with
conventional recommendation models (CRMs), LLMs often face challenges related
to high inference costs and static knowledge transfer methods. In this paper,
we propose a novel mutual distillation framework, LLMD4Rec, that fosters
dynamic and bidirectional knowledge exchange between LLM-centric and CRM-based
recommendation systems. Unlike traditional unidirectional distillation methods,
LLMD4Rec enables iterative optimization by alternately refining both models,
enhancing the semantic understanding of CRMs and enriching LLMs with
collaborative signals from user-item interactions. By leveraging sample-wise
adaptive weighting and aligning output distributions, our approach eliminates
the need for additional parameters while ensuring effective knowledge transfer.
Extensive experiments on real-world datasets demonstrate that LLMD4Rec
significantly improves recommendation accuracy across multiple benchmarks
without increasing inference costs. This method provides a scalable and
efficient solution for combining the strengths of both LLMs and CRMs in
sequential recommendation systems.

</details>


### [537] [Revisiting Feature Interactions from the Perspective of Quadratic Neural Networks for Click-through Rate Prediction](https://arxiv.org/abs/2505.17999)
*Honghao Li,Yiwen Zhang,Yi Zhang,Lei Sang,Jieming Zhu*

Key words: Hadamard积, CTR预测, 二次神经网络, Khatri-Rao积, QNN-alpha

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 该论文重新审视了Hadamard积（HP）在CTR预测中的作用，基于二次神经网络（QNN）提出改进方案，提出新的神经元格式QNN-alpha，性能优于HP，并在多个数据集上达到SOTA。

Motivation: 探索HP在CTR预测中的有效性原因，并从QNN的角度提出改进方法，以提升特征交互建模能力。

Method: 1. 通过QNN揭示特征空间扩展特性；2. 分析25种QNN神经元格式；3. 提出多头部Khatri-Rao积替代HP；4. 提出动态集成损失函数；5. 设计QNN-alpha神经元格式。

Result: QNN-alpha在六个公开数据集上达到SOTA性能，同时保持低推理延迟、良好扩展性和兼容性。

Conclusion: QNN-alpha是CTR预测任务的更优解决方案，验证了QNN框架的潜力。

Abstract: Hadamard Product (HP) has long been a cornerstone in click-through rate (CTR)
prediction tasks due to its simplicity, effectiveness, and ability to capture
feature interactions without additional parameters. However, the underlying
reasons for its effectiveness remain unclear. In this paper, we revisit HP from
the perspective of Quadratic Neural Networks (QNN), which leverage quadratic
interaction terms to model complex feature relationships. We further reveal
QNN's ability to expand the feature space and provide smooth nonlinear
approximations without relying on activation functions. Meanwhile, we find that
traditional post-activation does not further improve the performance of the
QNN. Instead, mid-activation is a more suitable alternative. Through
theoretical analysis and empirical evaluation of 25 QNN neuron formats, we
identify a good-performing variant and make further enhancements on it.
Specifically, we propose the Multi-Head Khatri-Rao Product as a superior
alternative to HP and a Self-Ensemble Loss with dynamic ensemble capability
within the same network to enhance computational efficiency and performance.
Ultimately, we propose a novel neuron format, QNN-alpha, which is tailored for
CTR prediction tasks. Experimental results show that QNN-alpha achieves new
state-of-the-art performance on six public datasets while maintaining low
inference latency, good scalability, and excellent compatibility. The code,
running logs, and detailed hyperparameter configurations are available at:
https://github.com/salmon1802/QNN.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [538] [Control of Renewable Energy Communities using AI and Real-World Data](https://arxiv.org/abs/2505.17321)
*Tiago Fonseca,Clarisse Sousa,Ricardo Venâncio,Pedro Pires,Ricardo Severino,Paulo Rodrigues,Pedro Paiva,Luis Lino Ferreira*

Key words: 可再生能源社区,电动汽车充电,MADDPG,多智能体控制,能源管理

<details>
  <summary>Details</summary>

Main category: eess.SY

TL;DR: 论文提出了一种基于MADDPG的多智能体框架EnergAIze，用于解决可再生能源社区中电动汽车充电与建筑能源系统的复杂集成问题，初步实景测试显示每日峰值需求降低9%，能源成本减少5%。

Motivation: 随着交通电气化和分布式可再生能源的普及，可再生能源社区（RECs）的管理复杂度增加，亟需解决电动汽车充电与建筑能源系统的集成挑战。

Method: 采用多智能体深度确定性策略梯度（MADDPG）算法，设计EnergAIze框架，整合实时数据采集、异构子系统同步及用户行为建模，优化负载调度与充电策略。

Result: 在真实REC场景测试中，框架实现了每日峰值需求降低9%、能源成本减少5%的效果。

Conclusion: 该框架通过智能算法与实景适配，推动了REC中能源管理方案的落地应用。

Abstract: The electrification of transportation and the increased adoption of
decentralized renewable energy generation have added complexity to managing
Renewable Energy Communities (RECs). Integrating Electric Vehicle (EV) charging
with building energy systems like heating, ventilation, air conditioning
(HVAC), photovoltaic (PV) generation, and battery storage presents significant
opportunities but also practical challenges. Reinforcement learning (RL),
particularly MultiAgent Deep Deterministic Policy Gradient (MADDPG) algorithms,
have shown promising results in simulation, outperforming heuristic control
strategies. However, translating these successes into real-world deployments
faces substantial challenges, including incomplete and noisy data, integration
of heterogeneous subsystems, synchronization issues, unpredictable occupant
behavior, and missing critical EV state-of-charge (SoC) information. This paper
introduces a framework designed explicitly to handle these complexities and
bridge the simulation to-reality gap. The framework incorporates EnergAIze, a
MADDPG-based multi-agent control strategy, and specifically addresses
challenges related to real-world data collection, system integration, and user
behavior modeling. Preliminary results collected from a real-world operational
REC with four residential buildings demonstrate the practical feasibility of
our approach, achieving an average 9% reduction in daily peak demand and a 5%
decrease in energy costs through optimized load scheduling and EV charging
behaviors. These outcomes underscore the framework's effectiveness, advancing
the practical deployment of intelligent energy management solutions in RECs.

</details>


### [539] [Selection Mechanisms for Sequence Modeling using Linear State Space Models](https://arxiv.org/abs/2505.17932)
*Umberto Casti,Sandro Zampieri,Fabio Pasqualetti*

Key words: 语言模型, 选择性状态空间模型, 控制论, 线性时不变系统, 残差生成器

<details>
  <summary>Details</summary>

Main category: eess.SY

TL;DR: 本文提出了一种受控制论启发的选择性机制，通过多线性时不变系统（LTI）实现选择性，性能与Mamba相当，并在合成任务上验证了其有效性。

Motivation: 当前语言模型任务的发展受限于Transformers和选择性状态空间模型（SSMs），本文旨在结合控制论方法，提出一种新的选择性机制。

Method: 提出了一种基于残差生成器的选择性机制，类比线性时不变系统（LTI）的故障检测策略，结合多个LTI系统并在训练中保留其优良特性。

Result: 在合成任务上验证了提出的架构的选择性性能，与Mamba模型相当。

Conclusion: 将控制论理论与机器学习实践结合，为深度学习创新提供了新视角。

Abstract: Recent advancements in language modeling tasks have been driven by
architectures such as Transformers and, more recently, by Selective State Space
Models (SSMs). In this paper, we introduce an alternative selection mechanism
inspired by control theory methodologies. Specifically, we propose a novel
residual generator for selection, drawing an analogy to fault detection
strategies in Linear Time-Invariant (LTI) systems. Unlike Mamba, which utilizes
Linear Time-Varying (LTV) systems, our approach combines multiple LTI systems,
preserving their beneficial properties during training while achieving
comparable selectivity. To evaluate the effectiveness of the proposed
architecture, we test its performance on synthetic tasks. While these tasks are
not inherently critical, they serve as benchmarks to test the selectivity
properties of different cores architecture. This work highlights the potential
of integrating theoretical insights with experimental advancements, offering a
complementary perspective to deep learning innovations at the intersection of
control theory and machine learning.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [540] [Multi-Person Interaction Generation from Two-Person Motion Priors](https://arxiv.org/abs/2505.17860)
*Wenning Xu,Shiyu Fan,Paul Henderson,Edmond S. L. Ho*

Key words: 多人互动生成、运动扩散模型、图结构、指导项

<details>
  <summary>Details</summary>

Main category: cs.GR

TL;DR: 提出Graph-driven Interaction Sampling方法，通过分解多人互动为二人互动图结构，利用现有的二人运动扩散模型生成多样且真实的多人互动，引入图相关指导项减少瑕疵，实验证明优于现有方法。

Motivation: 高质量的动作捕捉数据日益普及，但多人互动建模仍是较少探索的领域，需要一种无需专门训练、能生成多样真实互动的方法。

Method: 将多人互动分解为图结构形式的二人互动（Pairwise Interaction Graph），基于现有二人扩散模型生成单人动作，并在扩散采样中引入图相关指导项以减少瑕疵。

Result: 实验表明，该方法在生成二人及多人互动时能有效减少瑕疵，且运动多样性强于现有方法。

Conclusion: 通过图结构和扩散模型的结合，成功生成了高质量的多人互动，为互动建模提供了新思路。

Abstract: Generating realistic human motion with high-level controls is a crucial task
for social understanding, robotics, and animation. With high-quality MOCAP data
becoming more available recently, a wide range of data-driven approaches have
been presented. However, modelling multi-person interactions still remains a
less explored area. In this paper, we present Graph-driven Interaction
Sampling, a method that can generate realistic and diverse multi-person
interactions by leveraging existing two-person motion diffusion models as
motion priors. Instead of training a new model specific to multi-person
interaction synthesis, our key insight is to spatially and temporally separate
complex multi-person interactions into a graph structure of two-person
interactions, which we name the Pairwise Interaction Graph. We thus decompose
the generation task into simultaneous single-person motion generation
conditioned on one other's motion. In addition, to reduce artifacts such as
interpenetrations of body parts in generated multi-person interactions, we
introduce two graph-dependent guidance terms into the diffusion sampling
scheme. Unlike previous work, our method can produce various high-quality
multi-person interactions without having repetitive individual motions.
Extensive experiments demonstrate that our approach consistently outperforms
existing methods in reducing artifacts when generating a wide range of
two-person and multi-person interactions.

</details>


### [541] [WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions](https://arxiv.org/abs/2505.18151)
*Zizhang Li,Hong-Xing Yu,Wei Liu,Yin Yang,Charles Herrmann,Gordon Wetzstein,Jiajun Wu*

Key words: 物理模拟, 视频生成, 3D动态场景, 混合生成模拟器

<details>
  <summary>Details</summary>

Main category: cs.GR

TL;DR: WonderPlay整合物理模拟与视频生成，通过单一图像生成动态3D场景，支持广泛3D动态效果。

Motivation: 现存工作限于刚体或简单弹性动态，需实现更丰富、真实的3D动态场景生成。

Method: 混合生成模拟器结合物理求解器与视频生成器，循环优化动态场景。

Result: 支持用户交互多样场景（如布料、沙、雪等），表现优于现有方法。

Conclusion: WonderPlay实现了直观控制与高精度动态生成的结合，扩展了3D动态场景的应用范围。

Abstract: WonderPlay is a novel framework integrating physics simulation with video
generation for generating action-conditioned dynamic 3D scenes from a single
image. While prior works are restricted to rigid body or simple elastic
dynamics, WonderPlay features a hybrid generative simulator to synthesize a
wide range of 3D dynamics. The hybrid generative simulator first uses a physics
solver to simulate coarse 3D dynamics, which subsequently conditions a video
generator to produce a video with finer, more realistic motion. The generated
video is then used to update the simulated dynamic 3D scene, closing the loop
between the physics solver and the video generator. This approach enables
intuitive user control to be combined with the accurate dynamics of
physics-based simulators and the expressivity of diffusion-based video
generators. Experimental results demonstrate that WonderPlay enables users to
interact with various scenes of diverse content, including cloth, sand, snow,
liquid, smoke, elastic, and rigid bodies -- all using a single image input.
Code will be made public. Project website:
https://kyleleey.github.io/WonderPlay/

</details>


<div id='math.CT'></div>

# math.CT [[Back]](#toc)

### [542] [Alpay Algebra II: Identity as Fixed-Point Emergence in Categorical Data](https://arxiv.org/abs/2505.17480)
*Faruk Alpay*

Key words: 固定点, 范畴递归, 函子方程, 内部门限, 符号记忆

<details>
  <summary>Details</summary>

Main category: math.CT

TL;DR: 这篇论文通过范畴递归定义了身份作为一种不动点，使用 φ∞ 操作符描述了其在笛卡尔闭范畴中的自我引用函子方程解，证明了这种身份不动点的存在性和唯一性。

Motivation: 探讨身份作为一种动态稳定过程，而非静态标签，从变化的逻辑中自然产生的数学结构。

Method: 采用序数索引迭代和内部范畴极限，构造并证明身份不动点的存在性和唯一性。

Result: 身份不动点能够编码符号记忆、递归一致性和语义不变性。

Conclusion: 身份是一种可计算、收敛且范畴内禀的数学结构。

Abstract: In this second installment of the Alpay Algebra framework, I formally define
identity as a fixed point that emerges through categorical recursion. Building
upon the transfinite operator $\varphi^\infty$, I characterize identity as the
universal solution to a self-referential functorial equation over a small
cartesian closed category. I prove the existence and uniqueness of such
identity-fixed-points via ordinal-indexed iteration, and interpret their
convergence through internal categorical limits. Functors, adjunctions, and
morphisms are reconstructed as dynamic traces of evolving states governed by
$\varphi$, reframing identity not as a static label but as a stabilized
process. Through formal theorems and symbolic flows, I show how these fixed
points encode symbolic memory, recursive coherence, and semantic invariance.
This paper positions identity as a mathematical structure that arises from
within the logic of change itself computable, convergent, and categorically
intrinsic.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [543] [AstroMLab 4: Benchmark-Topping Performance in Astronomy Q&A with a 70B-Parameter Domain-Specialized Reasoning Model](https://arxiv.org/abs/2505.17592)
*Tijmen de Haan,Yuan-Sen Ting,Tirthankar Ghosal,Tuan Dung Nguyen,Alberto Accomazzi,Emily Herron,Vanessa Lama,Rui Pan,Azton Wells,Nesar Ramachandra*

Key words: AstroSage-70B, astronomy, domain specialization, language model

<details>
  <summary>Details</summary>

Main category: astro-ph.IM

TL;DR: AstroSage-70B is a specialized 70B-parameter language model for astronomy, achieving state-of-the-art performance by combining domain-specific training and reasoning enhancements.

Motivation: Address the gap in specialized domain knowledge of general-purpose models for astronomy.

Method: Domain-specialized pre-training on astronomical literature, supervised fine-tuning, and model merging.

Result: Surpasses existing open-weight and proprietary models in astronomical tasks.

Conclusion: Specialization in large models can outperform generalist counterparts in specific domains.

Abstract: General-purpose large language models, despite their broad capabilities,
often struggle with specialized domain knowledge, a limitation particularly
pronounced in more accessible, lower-parameter versions. This gap hinders their
deployment as effective agents in demanding fields such as astronomy. Building
on our prior work with AstroSage-8B, this study introduces AstroSage-70B, a
significantly larger and more advanced domain-specialized natural-language AI
assistant. It is designed for research and education across astronomy,
astrophysics, space science, astroparticle physics, cosmology, and astronomical
instrumentation. Developed from the Llama-3.1-70B foundation, AstroSage-70B
underwent extensive continued pre-training on a vast corpus of astronomical
literature, followed by supervised fine-tuning and model merging. Beyond its
70-billion parameter scale, this model incorporates refined datasets,
judiciously chosen learning hyperparameters, and improved training procedures,
achieving state-of-the-art performance on complex astronomical tasks. Notably,
we integrated reasoning chains into the SFT dataset, enabling AstroSage-70B to
either answer the user query immediately, or first emit a human-readable
thought process. Evaluated on the AstroMLab-1 benchmark -- comprising 4,425
questions from literature withheld during training -- AstroSage-70B achieves
state-of-the-art performance. It surpasses all other tested open-weight and
proprietary models, including leading systems like o3, Gemini-2.5-Pro,
Claude-3.7-Sonnet, Deepseek-R1, and Qwen-3-235B, even those with API costs two
orders of magnitude higher. This work demonstrates that domain specialization,
when applied to large-scale models, can enable them to outperform generalist
counterparts in specialized knowledge areas like astronomy, thereby advancing
the frontier of AI capabilities in the field.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [544] [Hybrid Mamba-Transformer Decoder for Error-Correcting Codes](https://arxiv.org/abs/2505.17834)
*Shy-el Cohen,Yoni Choukroun,Eliya Nachmani*

Key words: 纠错码解码, Mamba架构, Transformer, 深度学习

<details>
  <summary>Details</summary>

Main category: cs.IT

TL;DR: 该论文提出了一种基于Mamba架构和Transformer层的新型深度学习方法，用于解码纠错码。该混合解码器结合了Mamba的高效序列建模能力和Transformer的全局上下文能力，并通过层间掩蔽策略和渐进层损失进一步提升性能。实验表明，该方法显著优于仅Transformer的解码器和标准Mamba模型。

Motivation: 当前纠错码解码方法（如纯Transformer解码器或标准Mamba模型）存在效率或全局上下文能力的不足，因此需要一种兼具两者优势并进一步提升性能的混合解码方法。

Method: 设计了一种混合解码器，结合Mamba的高效序列建模和Transformer的全局上下文能力；提出了层间掩蔽策略和渐进层损失以优化解码效果。

Result: 在一系列线性码实验中，该方法显著优于纯Transformer解码器和标准Mamba模型。

Conclusion: 该混合解码器通过结合Mamba和Transformer的优势，并引入创新优化策略，显著提升了纠错码解码性能。

Abstract: We introduce a novel deep learning method for decoding error correction codes
based on the Mamba architecture, enhanced with Transformer layers. Our approach
proposes a hybrid decoder that leverages Mamba's efficient sequential modeling
while maintaining the global context capabilities of Transformers. To further
improve performance, we design a novel layer-wise masking strategy applied to
each Mamba layer, allowing selective attention to relevant code features at
different depths. Additionally, we introduce a progressive layer-wise loss,
supervising the network at intermediate stages and promoting robust feature
extraction throughout the decoding process. Comprehensive experiments across a
range of linear codes demonstrate that our method significantly outperforms
Transformer-only decoders and standard Mamba models.

</details>


### [545] [Toward Optimal ANC: Establishing Mutual Information Lower Bound](https://arxiv.org/abs/2505.17877)
*François Derrida,Shahar Lutati,Eliya Nachmani*

Key words: 主动降噪、信息论、理论极限、性能评估、深度学习

<details>
  <summary>Details</summary>

Main category: cs.IT

TL;DR: 该论文推导了主动降噪（ANC）算法的统一性能下界，包括信息论分量和基于支持的分量，并通过实验验证了其紧密度。

Motivation: 当前深度学习的ANC算法虽然性能优秀，但缺乏理论极限来评估其改进空间，因此需要建立一个统一的理论下界来衡量其性能。

Method: 通过分析信息捕获能力和物理限制，提出一个包含信息论和基于支持分量的下限，并通过实验验证其紧密度。

Result: 提出的下限能够有效衡量ANC算法的性能极限，并在不同混响条件下表现出鲁棒性。

Conclusion: 该理论框架为ANC算法的性能评估提供了严格的数学基础，并展示了其实用性和广泛适用性。

Abstract: Active Noise Cancellation (ANC) algorithms aim to suppress unwanted acoustic
disturbances by generating anti-noise signals that destructively interfere with
the original noise in real time. Although recent deep learning-based ANC
algorithms have set new performance benchmarks, there remains a shortage of
theoretical limits to rigorously assess their improvements. To address this, we
derive a unified lower bound on cancellation performance composed of two
components. The first component is information-theoretic: it links residual
error power to the fraction of disturbance entropy captured by the anti-noise
signal, thereby quantifying limits imposed by information-processing capacity.
The second component is support-based: it measures the irreducible error
arising in frequency bands that the cancellation path cannot address,
reflecting fundamental physical constraints. By taking the maximum of these two
terms, our bound establishes a theoretical ceiling on the Normalized Mean
Squared Error (NMSE) attainable by any ANC algorithm. We validate its tightness
empirically on the NOISEX dataset under varying reverberation times,
demonstrating robustness across diverse acoustic conditions.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [546] [HiLAB: A Hybrid Inverse-Design Framework](https://arxiv.org/abs/2505.17491)
*Reza Marzban,Hamed Abiri,Raphael Pestourie,Ali Adibi*

Key words: nanophotonics, inverse design, topological optimization, variational autoencoder, Bayesian optimization

<details>
  <summary>Details</summary>

Main category: physics.optics

TL;DR: HiLAB is a new inverse-design method combining early-terminated topological optimization, a Vision Transformer-based VAE, and Bayesian optimization. It efficiently generates diverse nanophotonic structures with fewer simulations, achieving high performance in multi-functional device design.

Motivation: Traditional topological optimization (TO) methods are prone to local optima and require extensive simulations. HiLAB aims to address these limitations by integrating machine learning and Bayesian optimization for more efficient and robust nanophotonic design.

Method: HiLAB combines early-terminated TO, a Vision Transformer-based VAE for latent-space learning, and Bayesian optimization. It generates initial structures via shortened TO runs, compresses them into a latent space, and co-optimizes geometry and hyperparameters using Bayesian methods.

Result: HiLAB reduces the total number of full simulations by over an order of magnitude and achieves high-performance designs, such as an achromatic beam deflector with balanced diffraction efficiencies of ~25% for RGB wavelengths.

Conclusion: HiLAB offers a flexible and efficient platform for robust multi-parameter photonic designs, enabling rapid adaptation to future nanophotonic challenges.

Abstract: HiLAB (Hybrid inverse-design with Latent-space learning, Adjoint-based
partial optimizations, and Bayesian optimization) is a new paradigm for inverse
design of nanophotonic structures. Combining early-terminated topological
optimization (TO) with a Vision Transformer-based variational autoencoder (VAE)
and a Bayesian search, HiLAB addresses multi-functional device design by
generating diverse freeform configurations at reduced simulation costs.
Shortened adjoint-driven TO runs, coupled with randomized physical parameters,
produce robust initial structures. These structures are compressed into a
compact latent space by the VAE, enabling Bayesian optimization to co-optimize
geometry and physical hyperparameters. Crucially, the trained VAE can be reused
for alternative objectives or constraints by adjusting only the acquisition
function. Compared to conventional TO pipelines prone to local optima, HiLAB
systematically explores near-global optima with considerably fewer
electromagnetic simulations. Even after accounting for training overhead, the
total number of full simulations decreases by over an order of magnitude,
accelerating the discovery of fabrication-friendly devices. Demonstrating its
efficacy, HiLAB is used to design an achromatic beam deflector for red, green,
and blue wavelengths, achieving balanced diffraction efficiencies of ~25% while
mitigating chromatic aberrations-a performance surpassing existing
demonstrations. Overall, HiLAB provides a flexible platform for robust,
multi-parameter photonic designs and rapid adaptation to next-generation
nanophotonic challenges.

</details>


### [547] [Programmable Photonic Unitary Processor Enables Parametrized Differentiable Long-Haul Spatial Division Multiplexed Transmission](https://arxiv.org/abs/2505.17381)
*Mitsumasa Nakajima,Kohki Shibahara,Kohei Ikeda,Akira Kawai,Masaya Notomi,Yutaka Miyamoto,Toshikazu Hashimoto*

Key words: 空分复用（SDM）、可编程光子处理器、模态色散、梯度优化、光通信

<details>
  <summary>Details</summary>

Main category: physics.optics

TL;DR: 本文提出了一种参数化的空分复用（SDM）传输方法，通过在中继节点安装可编程光子酉处理器，直接优化传输通道，减少数字后处理负载。实验验证了其在1300公里三模光纤传输中的有效性，显著降低了模态色散和后处理复杂度。

Motivation: 全球数据流量的快速增长需要可扩展且节能的光通信系统。空分复用（SDM）虽能突破单模光纤的容量限制，但其长距离传输面临模态色散带来的数字信号处理（DSP）高计算负载问题。

Method: 提出了参数化SDM传输方案，利用可编程光子酉处理器在中继节点直接优化传输通道，并开发了一种基于梯度优化的算法，通过可微分SDM传输模型确定最优酉变换。

Result: 实现了电信级可编程光子酉处理器，低损耗（2.1 dB）、宽带宽（全C波段）、偏振无关且高保真（R2>96%）。实验展示了1300公里三模光纤传输，仿真与实验结果高度一致。

Conclusion: 该方案为将光子计算集成到光层提供了可扩展框架，有望实现更高效、高容量的光网络。

Abstract: The explosive growth of global data traffic demands scalable and
energy-efficient optical communication systems. Spatial division multiplexing
(SDM) using multicore or multimode fibers is a promising solution to overcome
the capacity limit of single-mode fibers. However, long-haul SDM transmission
faces significant challenges due to modal dispersion, which imposes heavy
computational loads on digital signal processing (DSP) for signal equalization.
Here, we propose parameterized SDM transmission, where programmable photonic
unitary processors are installed at intermediate nodes. Instead of relying on
conventional digital equalization only on the receiver side, our approach
enables direct optimization of the SDM transmission channel itself by the
programmable unitary processor, which reduces digital post-processing loads. We
introduce a gradient-based optimization algorithm using a differentiable SDM
transmission model to determine the optimal unitary transformation. As a key
enabler, we first implemented telecom-grade programmable photonic unitary
processor, achieving a low-loss (2.1 dB fiber-to-fiber), wideband (full
C-band), polarization-independent, and high-fidelity (R2>96% across the C-band)
operation. We experimentally demonstrate 1300-km transmission using a
three-mode fiber, achieving strong agreement between simulation and experiment.
The optimized photonic processor significantly reduces modal dispersion and
post-processing complexity. Our results establish a scalable framework for
integrating photonic computation into the optical layer, enabling more
efficient, high-capacity optical networks.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [548] [ReMi: A Random Recurrent Neural Network Approach to Music Production](https://arxiv.org/abs/2505.17023)
*Hugo Chateau-Laurent,Tara Vanhatalo*

Key words: 生成式AI, 循环神经网络, 音乐生成, 低算力, 创造力

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 随机初始化的循环神经网络可生成丰富可配置的音乐片段，无需数据和低算力，扩展音乐人创造力而非替代。

Motivation: 生成式AI面临能耗、版权和创造力退化问题，本研究旨在以低资源方式增强音乐人创作而非替代。

Method: 使用随机初始化的循环神经网络生成可配置的音乐片段。

Result: 生成的声音片段丰富且可调，且在无数据和低算力下实现。

Conclusion: 该方法为音乐创作提供了轻量级工具，平衡了创造力与技术负担。

Abstract: Generative artificial intelligence raises concerns related to energy
consumption, copyright infringement and creative atrophy. We show that randomly
initialized recurrent neural networks can produce arpeggios and low-frequency
oscillations that are rich and configurable. In contrast to end-to-end music
generation that aims to replace musicians, our approach expands their
creativity while requiring no data and much less computational power. More
information can be found at: https://allendia.com/

</details>


### [549] [UniTTS: An end-to-end TTS system without decoupling of acoustic and semantic information](https://arxiv.org/abs/2505.17426)
*Rui Wang,Qianguo Sun,Tianrong Chen,Zhiyun Zeng,Junlong Wu,Jiaxing Zhang*

Key words: DistilCodec, UniTTS, LLM, TTS, 音频编解码器

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 该论文提出了DistilCodec和UniTTS方法，解决多码本音频编解码器在LLM基TTS系统中音频信息利用不足的问题，通过单码本蒸馏和多任务预训练框架提升性能。

Motivation: 多码本音频编解码器（如RVQ和GVQ）在LLM基TTS系统中存在语义与声学信息未完全对齐的问题，导致模型无法充分利用音频信息。

Method: 提出DistilCodec将多码本蒸馏为单码本（32,768码），并设计UniTTS框架，整合音频模态自回归、文本模态自回归和跨模态自回归任务。

Result: DistilCodec实现近乎100%的码利用率，UniTTS支持混合文本/语音提示，同时保留LLM的文本能力。

Conclusion: DistilCodec和UniTTS有效提升了音频信息利用率与模型通用性，代码与模型已开源。

Abstract: The emergence of multi-codebook neutral audio codecs such as Residual Vector
Quantization (RVQ) and Group Vector Quantization (GVQ) has significantly
advanced Large-Language-Model (LLM) based Text-to-Speech (TTS) systems. These
codecs are crucial in separating semantic and acoustic information while
efficiently harnessing semantic priors. However, since semantic and acoustic
information cannot be fully aligned, a significant drawback of these methods
when applied to LLM-based TTS is that large language models may have limited
access to comprehensive audio information. To address this limitation, we
propose DistilCodec and UniTTS, which collectively offer the following
advantages: 1) This method can distill a multi-codebook audio codec into a
single-codebook audio codec with 32,768 codes while achieving a near 100\%
utilization. 2) As DistilCodec does not employ a semantic alignment scheme, a
large amount of high-quality unlabeled audio (such as audiobooks with sound
effects, songs, etc.) can be incorporated during training, further expanding
data diversity and broadening its applicability. 3) Leveraging the
comprehensive audio information modeling of DistilCodec, we integrated three
key tasks into UniTTS's pre-training framework: audio modality autoregression,
text modality autoregression, and speech-text cross-modal autoregression. This
allows UniTTS to accept interleaved text and speech/audio prompts while
substantially preserving LLM's text capabilities. 4) UniTTS employs a
three-stage training process: Pre-Training, Supervised Fine-Tuning (SFT), and
Alignment. Source code and model checkpoints are publicly available at
https://github.com/IDEA-Emdoor-Lab/UniTTS and
https://github.com/IDEA-Emdoor-Lab/DistilCodec.

</details>


### [550] [CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and Post-training](https://arxiv.org/abs/2505.17589)
*Zhihao Du,Changfeng Gao,Yuxuan Wang,Fan Yu,Tianyu Zhao,Hao Wang,Xiang Lv,Hui Wang,Xian Shi,Keyu An,Guanrou Yang,Yabin Li,Yanni Chen,Zhifu Gao,Qian Chen,Yue Gu,Mengzhe Chen,Yafeng Chen,Shiliang Zhang,Wen Wang,Jieping Ye*

Key words: 语音合成、零样本学习、多语言、大型语言模型、流匹配

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: CosyVoice 3在CosyVoice 2的基础上进行了多项改进，包括新的语音标记器、可微分奖励模型、数据量和模型规模的扩大，实现了零样本多语言语音合成。

Motivation: 针对CosyVoice 2在语言覆盖、领域多样性、数据量、文本格式和训练后技术方面的不足，提出改进模型以提升内容一致性、说话人相似性和韵律自然度。

Method: 1) 开发新型语音标记器提升韵律自然度；2) 引入可微分奖励模型用于训练后优化；3) 数据量扩展至百万小时；4) 模型参数增至15亿。

Result: CosyVoice 3在多语言基准测试中表现更优，内容包括一致性、说话人相似性和韵律自然度均有提升。

Conclusion: 这些改进显著推动了语音合成技术的发展，特别是在多语言和零样本场景下。

Abstract: In our prior works, we introduced a scalable streaming speech synthesis
model, CosyVoice 2, which integrates a large language model (LLM) and a
chunk-aware flow matching (FM) model, and achieves low-latency bi-streaming
speech synthesis and human-parity quality. Despite these advancements,
CosyVoice 2 exhibits limitations in language coverage, domain diversity, data
volume, text formats, and post-training techniques. In this paper, we present
CosyVoice 3, an improved model designed for zero-shot multilingual speech
synthesis in the wild, surpassing its predecessor in content consistency,
speaker similarity, and prosody naturalness. Key features of CosyVoice 3
include: 1) A novel speech tokenizer to improve prosody naturalness, developed
via supervised multi-task training, including automatic speech recognition,
speech emotion recognition, language identification, audio event detection, and
speaker analysis. 2) A new differentiable reward model for post-training
applicable not only to CosyVoice 3 but also to other LLM-based speech synthesis
models. 3) Dataset Size Scaling: Training data is expanded from ten thousand
hours to one million hours, encompassing 9 languages and 18 Chinese dialects
across various domains and text formats. 4) Model Size Scaling: Model
parameters are increased from 0.5 billion to 1.5 billion, resulting in enhanced
performance on our multilingual benchmark due to the larger model capacity.
These advancements contribute significantly to the progress of speech synthesis
in the wild. We encourage readers to listen to the demo at
https://funaudiollm.github.io/cosyvoice3.

</details>


### [551] [LLM-based Generative Error Correction for Rare Words with Synthetic Data and Phonetic Context](https://arxiv.org/abs/2505.17410)
*Natsuo Yamashita,Masaaki Yamamoto,Hiroaki Kokubo,Yohei Kawaguchi*

Key words: 生成式纠错, LLM, 罕见词, 语音信息, ASR

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 提出了一种结合语音信息的LLM生成式纠错方法，通过合成数据增强和N-best假设优化，显著提升了罕见词纠错效果并降低了错误率。

Motivation: 传统LLM纠错方法对罕见词和领域特定词表现不佳，且忽视语音信息导致过纠正，亟需改进。

Method: 1. 合成含罕见词的训练数据微调模型；2. 结合ASR的N-best假设和语音上下文减少过纠正。

Result: 英语和日语数据集上，罕见词纠正效果提升，词错误率（WER）和字符错误率（CER）均降低。

Conclusion: 融合语音信息的LLM纠错方法可有效解决罕见词和过纠正问题，提升ASR后处理性能。

Abstract: Generative error correction (GER) with large language models (LLMs) has
emerged as an effective post-processing approach to improve automatic speech
recognition (ASR) performance. However, it often struggles with rare or
domain-specific words due to limited training data. Furthermore, existing
LLM-based GER approaches primarily rely on textual information, neglecting
phonetic cues, which leads to over-correction. To address these issues, we
propose a novel LLM-based GER approach that targets rare words and incorporates
phonetic information. First, we generate synthetic data to contain rare words
for fine-tuning the GER model. Second, we integrate ASR's N-best hypotheses
along with phonetic context to mitigate over-correction. Experimental results
show that our method not only improves the correction of rare words but also
reduces the WER and CER across both English and Japanese datasets.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [552] [The Discovery Engine: A Framework for AI-Driven Synthesis and Navigation of Scientific Knowledge Landscapes](https://arxiv.org/abs/2505.17500)
*Vladimir Baulin,Austin Cook,Daniel Friedman,Janna Lumiruusu,Andrew Pashea,Shagor Rahman,Benedikt Waldeck*

Key words: Discovery Engine, knowledge artifacts, conceptual tensor, AI-augmented science, knowledge graphs

<details>
  <summary>Details</summary>

Main category: cond-mat.soft

TL;DR: 论文提出了Discovery Engine框架，通过将分散的文献转化为结构化、可计算的科学领域统一表示，解决当前科学知识传播模型中的信息过载和可重复性问题。

Motivation: 当前科学知识传播模型存在信息过载和可重复性问题，亟需一种新方法将分散的文献整合为可计算的统一表示。

Method: 采用LLM驱动的知识提取，将文献转化为结构化“知识工件”，并将其编码为高维概念张量，支持动态展开为知识图谱或语义向量空间。

Result: Discovery Engine提供了AI辅助的科学探索新范式，支持发现非显性关联、填补知识空白并生成新假设。

Conclusion: 通过结构化表示和AI代理交互，Discovery Engine为科学研究和加速发现提供了新方法。

Abstract: The prevailing model for disseminating scientific knowledge relies on
individual publications dispersed across numerous journals and archives. This
legacy system is ill suited to the recent exponential proliferation of
publications, contributing to insurmountable information overload, issues
surrounding reproducibility and retractions. We introduce the Discovery Engine,
a framework to address these challenges by transforming an array of
disconnected literature into a unified, computationally tractable
representation of a scientific domain. Central to our approach is the
LLM-driven distillation of publications into structured "knowledge artifacts,"
instances of a universal conceptual schema, complete with verifiable links to
source evidence. These artifacts are then encoded into a high-dimensional
Conceptual Tensor. This tensor serves as the primary, compressed representation
of the synthesized field, where its labeled modes index scientific components
(concepts, methods, parameters, relations) and its entries quantify their
interdependencies. The Discovery Engine allows dynamic "unrolling" of this
tensor into human-interpretable views, such as explicit knowledge graphs (the
CNM graph) or semantic vector spaces, for targeted exploration. Crucially, AI
agents operate directly on the graph using abstract mathematical and learned
operations to navigate the knowledge landscape, identify non-obvious
connections, pinpoint gaps, and assist researchers in generating novel
knowledge artifacts (hypotheses, designs). By converting literature into a
structured tensor and enabling agent-based interaction with this compact
representation, the Discovery Engine offers a new paradigm for AI-augmented
scientific inquiry and accelerated discovery.

</details>
