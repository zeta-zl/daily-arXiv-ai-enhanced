<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 39]
- [cs.LG](#cs.LG) [Total: 77]
- [cs.AI](#cs.AI) [Total: 13]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [stat.ME](#stat.ME) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.GR](#cs.GR) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 16]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Towards Probabilistic Question Answering Over Tabular Data](https://arxiv.org/abs/2506.20747)
*Chen Shen,Sajjadur Rahman,Estevam Hruschka*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Current approaches for question answering (QA) over tabular data, such as
NL2SQL systems, perform well for factual questions where answers are directly
retrieved from tables. However, they fall short on probabilistic questions
requiring reasoning under uncertainty. In this paper, we introduce a new
benchmark LUCARIO and a framework for probabilistic QA over large tabular data.
Our method induces Bayesian Networks from tables, translates natural language
queries into probabilistic queries, and uses large language models (LLMs) to
generate final answers. Empirical results demonstrate significant improvements
over baselines, highlighting the benefits of hybrid symbolic-neural reasoning.

</details>


### [2] [Multi-lingual Functional Evaluation for Large Language Models](https://arxiv.org/abs/2506.20793)
*Victor Ojewale,Inioluwa Deborah Raji,Suresh Venkatasubramanian*

Key words: 多语言模型、功能性基准测试、CL-GSM Symbolic、CL-IFEval、模型鲁棒性

TL;DR: 论文通过创建跨语言功能性基准测试（CL-GSM Symbolic和CL-IFEval）来评估多语言大模型的实际性能和鲁棒性，发现静态基准测试与实际功能性表现之间存在显著差异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有静态多语言基准测试（如Belebele、M-MMLU和M-GSM）未能充分评估模型的实用性能和鲁棒性，因此需要功能性基准测试来填补这一空白。

Method: 将现有的功能性基准测试模板从英语翻译成五种语言（法语、西班牙语、印地语、阿拉伯语和约鲁巴语），创建了CL-GSM Symbolic和CL-IFEval两个功能性基准测试。

Result: 结果显示，不同静态基准测试与功能性表现之间的差距显著（性能下降15%-24%），而某些语言（如阿拉伯语和英语）在多次评估中表现最稳定。

Conclusion: 功能性基准测试能更准确地评估多语言模型的实用性能，同时揭示了模型在不同语言间的鲁棒性差异。

Abstract: Multi-lingual competence in large language models is often evaluated via
static data benchmarks such as Belebele, M-MMLU and M-GSM. However, these
evaluations often fail to provide an adequate understanding of the practical
performance and robustness of models across multi-lingual settings. In
response, we create multi-lingual functional benchmarks -- Cross-Lingual Grade
School Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following
Eval (CL-IFEval)-- by translating existing functional benchmark templates from
English to five additional languages that span the range of resources available
for NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that
some static multi-lingual benchmarks capture functional performance much more
closely than others (i.e. across models, there is a 24%, 17% and 18% decrease
in performance between M-GSM and CL-GSM Symbolic in English, French and Spanish
respectively; similarly there's a 15 - 24% performance drop across languages
between Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between
M-MMLU and CL-IFEval). Similarly, we find that model robustness across
languages varies significantly, with certain languages (eg. Arabic, English)
being the most consistently well performing across evaluation iterations.

</details>


### [3] [The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas](https://arxiv.org/abs/2506.20803)
*Chenglei Si,Tatsunori Hashimoto,Diyi Yang*

Key words: 大型语言模型、研究想法、执行研究、新颖性、专家评估

TL;DR: 研究评估了AI生成的研究想法是否比人类专家想法更能产生高质量研究成果，发现尽管AI想法在初始阶段更具新颖性，但执行后评分显著下降。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索大型语言模型（LLM）生成的研究想法是否能在执行后产生优于人类专家的研究成果。

Method: 招募43名专家执行随机分配的人类或AI生成的研究想法，记录实验并撰写短论文，由NLP专家盲审评分。

Result: AI生成想法在执行后评分显著下降，人类想法在多项指标中反超，显示出AI想法的实际执行局限性。

Conclusion: 当前LLM在生成真正有效的研究想法上存在局限，仅凭初始评估难以预测执行效果。

Abstract: Large Language Models (LLMs) have shown promise in accelerating the
scientific research pipeline. A key capability for this process is the ability
to generate novel research ideas, and prior studies have found settings in
which LLM-generated research ideas were judged as more novel than human-expert
ideas. However, a good idea should not simply appear to be novel, it should
also result in better research after being executed. To test whether
AI-generated ideas lead to better research outcomes, we conduct an execution
study by recruiting 43 expert researchers to execute randomly-assigned ideas,
either written by experts or generated by an LLM. Each expert spent over 100
hours implementing the idea and wrote a 4-page short paper to document the
experiments. All the executed projects are then reviewed blindly by expert NLP
researchers. Comparing the review scores of the same ideas before and after
execution, the scores of the LLM-generated ideas decrease significantly more
than expert-written ideas on all evaluation metrics (novelty, excitement,
effectiveness, and overall; p < 0.05), closing the gap between LLM and human
ideas observed at the ideation stage. When comparing the aggregated review
scores from the execution study, we even observe that for many metrics there is
a flip in rankings where human ideas score higher than LLM ideas. This
ideation-execution gap highlights the limitations of current LLMs in generating
truly effective research ideas and the challenge of evaluating research ideas
in the absence of execution outcomes.

</details>


### [4] [MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering](https://arxiv.org/abs/2506.20821)
*Chinmay Gondhalekar,Urjitkumar Patel,Fang-Chun Yeh*

Key words: MultiFinRAG, 金融QA, 多模态, 检索增强生成, 跨模态推理

TL;DR: MultiFinRAG是一个专为金融QA设计的检索增强生成框架，通过多模态提取和动态检索策略提升准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统大语言模型在金融文档多模态问答中存在局限性，需跨模态推理。

Method: 使用轻量级多模态LLM提取表格和图像信息，结合模态感知检索和层级回退策略。

Result: 在复杂金融QA任务中，MultiFinRAG的准确率比ChatGPT-4o高19个百分点。

Conclusion: MultiFinRAG在多模态金融问答中表现优异，适用于普通硬件。

Abstract: Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span
hundreds of pages and combine diverse modalities, including dense narrative
text, structured tables, and complex figures. Answering questions over such
content often requires joint reasoning across modalities, which strains
traditional large language models (LLMs) and retrieval-augmented generation
(RAG) pipelines due to token limitations, layout loss, and fragmented
cross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation
framework purpose-built for financial QA. MultiFinRAG first performs multimodal
extraction by grouping table and figure images into batches and sending them to
a lightweight, quantized open-source multimodal LLM, which produces both
structured JSON outputs and concise textual summaries. These outputs, along
with narrative text, are embedded and indexed with modality-aware similarity
thresholds for precise retrieval. A tiered fallback strategy then dynamically
escalates from text-only to text+table+image contexts when necessary, enabling
cross-modal reasoning while reducing irrelevant context. Despite running on
commodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy
than ChatGPT-4o (free-tier) on complex financial QA tasks involving text,
tables, images, and combined multimodal reasoning.

</details>


### [5] [Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes](https://arxiv.org/abs/2506.20822)
*Quintin Myers,Yanjun Gao*

Key words: 大型语言模型、暴力内容、道德推理、人口统计偏差、VBVQ

TL;DR: 该研究首次使用已验证的社会科学工具评估大型语言模型（LLMs）在道德模糊场景中对暴力内容的反应，发现其文本生成与内在偏好存在差异，且暴力倾向在不同人口统计中存在偏差。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机是评估LLMs在处理暴力内容时的道德推理能力，尤其是在现实世界复杂场景中的表现。

Method: 方法包括使用Violent Behavior Vignette Questionnaire (VBVQ)工具，通过不同人口统计特征的角色提示，评估六种LLMs在零样本设置下的表现。

Result: 结果表明：(1) LLMs的表面文本生成与内在暴力偏好不一致；(2) 其暴力倾向在不同人口统计中存在偏差，与犯罪学和社会科学的已知结论矛盾。

Conclusion: LLMs在处理暴力内容时存在局限性，尤其是在道德推理和偏见方面，需进一步改进。

Abstract: Large language models (LLMs) are increasingly proposed for detecting and
responding to violent content online, yet their ability to reason about morally
ambiguous, real-world scenarios remains underexamined. We present the first
study to evaluate LLMs using a validated social science instrument designed to
measure human response to everyday conflict, namely the Violent Behavior
Vignette Questionnaire (VBVQ). To assess potential bias, we introduce
persona-based prompting that varies race, age, and geographic identity within
the United States. Six LLMs developed across different geopolitical and
organizational contexts are evaluated under a unified zero-shot setting. Our
study reveals two key findings: (1) LLMs surface-level text generation often
diverges from their internal preference for violent responses; (2) their
violent tendencies vary across demographics, frequently contradicting
established findings in criminology, social science, and psychology.

</details>


### [6] [Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine](https://arxiv.org/abs/2506.20876)
*Sebastian Joseph,Lily Chen,Barry Wei,Michael Mackert,Iain J. Marshall,Paul Pu Liang,Ramez Kouzy,Byron C. Wallace,Junyi Jessy Li*

Key words: 事实核查、医学、证据评估、交互沟通、临床试验

TL;DR: 论文探讨了自动事实核查系统在医学领域的应用挑战，指出其未普及的原因，并提出应将事实核查视为交互式沟通问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 医学决策高风险且文献复杂，现成的事实核查系统未被广泛使用，研究旨在揭示其应用障碍。

Method: 通过研究临床专家如何验证社交媒体上的医学声明，分析事实核查在医学中的实际困难。

Result: 指出医学事实核查的三大挑战：声明与临床试验证据的连接困难、模糊声明与意图不匹配、主观的真实性标签。

Conclusion: 建议将事实核查视为交互式沟通问题而非端到端过程。

Abstract: Technological progress has led to concrete advancements in tasks that were
regarded as challenging, such as automatic fact-checking. Interest in adopting
these systems for public health and medicine has grown due to the high-stakes
nature of medical decisions and challenges in critically appraising a vast and
diverse medical literature. Evidence-based medicine connects to every
individual, and yet the nature of it is highly technical, rendering the medical
literacy of majority users inadequate to sufficiently navigate the domain. Such
problems with medical communication ripens the ground for end-to-end
fact-checking agents: check a claim against current medical literature and
return with an evidence-backed verdict. And yet, such systems remain largely
unused. To understand this, we present the first study examining how clinical
experts verify real claims from social media by synthesizing medical evidence.
In searching for this upper-bound, we reveal fundamental challenges in
end-to-end fact-checking when applied to medicine: Difficulties connecting
claims in the wild to scientific evidence in the form of clinical trials;
ambiguities in underspecified claims mixed with mismatched intentions; and
inherently subjective veracity labels. We argue that fact-checking should be
approached and evaluated as an interactive communication problem, rather than
an end-to-end process.

</details>


### [7] [Optimising Language Models for Downstream Tasks: A Post-Training Perspective](https://arxiv.org/abs/2506.20917)
*Zhengyan Shi*

Key words: 语言模型，下游任务适应，持续预训练，参数高效微调，多跳空间推理

TL;DR: 该论文提出了一系列改进语言模型（LM）适应下游任务的方法，包括从无标签数据中提取任务相关知识、参数高效微调方法、改进的监督微调方法及新评估基准，显著提升了LM的鲁棒性、效率和泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管语言模型在NLP中表现卓越，但其适应特定任务的效率与鲁棒性仍具挑战性，现有方法常未能充分利用无标签数据且易过拟合，计算成本高，限制了其在实际语言任务中的应用。

Method: 1. 提出一种新颖的持续预训练技术，从无标签数据中提取任务相关知识；2. 设计参数高效的微调方法以降低计算成本；3. 改进监督微调方法以提升少标注数据下的指令遵循能力；4. 开发多跳空间推理等新评估基准。

Result: 实验表明，所提方法显著提升了LM的鲁棒性、效率和泛化能力，使其更适应多样化NLP任务，包括开放生成任务。

Conclusion: 这些方法推动了更强大、高效的语言模型发展，为实现人工通用智能迈出了重要一步。

Abstract: Language models (LMs) have demonstrated remarkable capabilities in NLP, yet
adapting them efficiently and robustly to specific tasks remains challenging.
As their scale and complexity grow, fine-tuning LMs on labelled data often
underutilizes available unlabelled data, leads to overfitting on small
task-specific sets, and imposes significant computational costs. These
limitations hamper their application to the open-ended landscape of real-world
language tasks.
  This thesis proposes a series of methods to better adapt LMs to downstream
applications. First, we explore strategies for extracting task-relevant
knowledge from unlabelled data, introducing a novel continued pre-training
technique that outperforms state-of-the-art semi-supervised approaches. Next,
we present a parameter-efficient fine-tuning method that substantially reduces
memory and compute costs while maintaining competitive performance. We also
introduce improved supervised fine-tuning methods that enable LMs to better
follow instructions, especially when labelled data is scarce, enhancing their
performance across a range of NLP tasks, including open-ended generation.
Finally, we develop new evaluation methods and benchmarks, such as multi-hop
spatial reasoning tasks, to assess LM capabilities and adaptation more
comprehensively.
  Through extensive empirical studies across diverse NLP tasks, our results
demonstrate that these approaches substantially improve LM robustness,
efficiency, and generalization, making them more adaptable to a broad range of
applications. These advances mark a significant step towards more robust and
efficient LMs, bringing us closer to the goal of artificial general
intelligence.

</details>


### [8] [FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language](https://arxiv.org/abs/2506.20920)
*Guilherme Penedo,Hynek Kydlíček,Vinko Sabolčec,Bettina Messmer,Negar Foroutan,Amir Hossein Kargaran,Colin Raffel,Martin Jaggi,Leandro Von Werra,Thomas Wolf*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Pre-training state-of-the-art large language models (LLMs) requires vast
amounts of clean and diverse text data. While the open development of large
high-quality English pre-training datasets has seen substantial recent
progress, training performant multilingual LLMs remains a challenge, in large
part due to the inherent difficulty of tailoring filtering and deduplication
pipelines to a large number of languages. In this work, we introduce a new
pre-training dataset curation pipeline based on FineWeb that can be
automatically adapted to support any language. We extensively ablate our
pipeline design choices on a set of nine diverse languages, guided by a set of
meaningful and informative evaluation tasks that were chosen through a novel
selection process based on measurable criteria. Ultimately, we show that our
pipeline can be used to create non-English corpora that produce more performant
models than prior datasets. We additionally introduce a straightforward and
principled approach to rebalance datasets that takes into consideration both
duplication count and quality, providing an additional performance uplift.
Finally, we scale our pipeline to over 1000 languages using almost 100 Common
Crawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document)
multilingual dataset which we release along with our pipeline, training, and
evaluation codebases.

</details>


### [9] [KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model](https://arxiv.org/abs/2506.20923)
*Xinping Zhao,Xinshuo Hu,Zifei Shan,Shouzheng Huang,Yao Zhou,Zetian Sun,Zhenyu Liu,Dongfang Li,Xinyuan Wei,Qian Chen,Youcheng Pan,Yang Xiang,Meishan Zhang,Haofen Wang,Jun Yu,Baotian Hu,Min Zhang*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we propose KaLM-Embedding-V2, a versatile and compact
embedding model, which achieves impressive performance in general-purpose text
embedding tasks by leveraging superior training techniques and data. Our key
innovations include: (1) To better align the architecture with representation
learning, we remove the causal attention mask and adopt a fully bidirectional
transformer with simple yet effective mean-pooling to produce fixed-length
embeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on
large-scale weakly supervised open-source corpora; (ii) fine-tuning on
high-quality retrieval and non-retrieval datasets; and (iii) model-soup
parameter averaging for robust generalization. Besides, we introduce a
focal-style reweighting mechanism that concentrates learning on difficult
samples and an online hard-negative mixing strategy to continuously enrich hard
negatives without expensive offline mining; (3) We collect over 20 categories
of data for pre-training and 100 categories of data for fine-tuning, to boost
both the performance and generalization of the embedding model. Extensive
evaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English
show that our model significantly outperforms others of comparable size, and
competes with 3x, 14x, 18x, and 26x larger embedding models, setting a new
standard for a versatile and compact embedding model with less than 1B
parameters.

</details>


### [10] [Can Gradient Descent Simulate Prompting?](https://arxiv.org/abs/2506.20989)
*Eric Zhang,Leshem Choshen,Jacob Andreas*

Key words: 语言模型, 梯度更新, 元学习, 微调, 提示

TL;DR: 本文探讨了如何通过元训练使梯度更新模拟提示的效果，从而提升语言模型的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究两种将新信息融入语言模型的方法（提示和微调），并探索微调是否能模拟提示的效果。

Method: 通过元训练语言模型，利用梯度更新模拟提示的效果，以模型自身的提示预测为训练目标。

Result: 实验结果显示，该方法在反转诅咒任务和文本问答任务中表现良好，梯度下降能有效恢复提示模型的性能。

Conclusion: 研究表明，适当初始化的梯度下降具有强大的表达能力，为长上下文建模和梯度学习泛化能力提供了新思路。

Abstract: There are two primary ways of incorporating new information into a language
model (LM): changing its prompt or changing its parameters, e.g. via
fine-tuning. Parameter updates incur no long-term storage cost for model
changes. However, for many model updates, prompting is significantly more
effective: prompted models can generalize robustly from single examples and
draw logical inferences that do not occur under standard fine-tuning. Can
models be modified so that fine-tuning does emulate prompting? This paper
describes a method for meta-training LMs such that gradient updates emulate the
effects of conditioning on new information. Our approach uses tools from
gradient-based meta-learning but uses an LM's own prompted predictions as
targets, eliminating the need for ground-truth labels. Subsequent gradient
descent training recovers some (and occasionally all) of prompted model
performance -- showing improvement on the ``reversal curse'' tasks, and
answering questions about text passages after a single gradient update. These
results suggest that, with appropriate initialization, gradient descent can be
surprisingly expressive. Our results suggest new avenues for long-context
modeling and offer insight into the generalization capabilities of
gradient-based learning.

</details>


### [11] [SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control](https://arxiv.org/abs/2506.20993)
*Adithya Chittem,Aishna Shrivastava,Sai Tarun Pendela,Jagat Sesh Challa,Dhruv Kumar*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) have gained significant traction across a wide
range of fields in recent years. There is also a growing expectation for them
to display human-like personalities during interactions. To meet this
expectation, numerous studies have proposed methods for modelling LLM
personalities through psychometric evaluations. However, most existing models
face two major limitations: they rely on the Big Five (OCEAN) framework, which
only provides coarse personality dimensions, and they lack mechanisms for
controlling trait intensity. In this paper, we address this gap by extending
the Machine Personality Inventory (MPI), which originally used the Big Five
model, to incorporate the 16 Personality Factor (16PF) model, allowing
expressive control over sixteen distinct traits. We also developed a structured
framework known as Specific Attribute Control (SAC) for evaluating and
dynamically inducing trait intensity in LLMs. Our method introduces
adjective-based semantic anchoring to guide trait intensity expression and
leverages behavioural questions across five intensity factors:
\textit{Frequency}, \textit{Depth}, \textit{Threshold}, \textit{Effort}, and
\textit{Willingness}. Through experimentation, we find that modelling intensity
as a continuous spectrum yields substantially more consistent and controllable
personality expression compared to binary trait toggling. Moreover, we observe
that changes in target trait intensity systematically influence closely related
traits in psychologically coherent directions, suggesting that LLMs internalize
multi-dimensional personality structures rather than treating traits in
isolation. Our work opens new pathways for controlled and nuanced human-machine
interactions in domains such as healthcare, education, and interviewing
processes, bringing us one step closer to truly human-like social machines.

</details>


### [12] [Large Language Models Acing Chartered Accountancy](https://arxiv.org/abs/2506.21031)
*Jatin Gupta,Akhil Sharma,Saransh Singhania,Mohammad Adnan,Sakshi Deo,Ali Imam Abidi,Keshav Gupta*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Advanced intelligent systems, particularly Large Language Models (LLMs), are
significantly reshaping financial practices through advancements in Natural
Language Processing (NLP). However, the extent to which these models
effectively capture and apply domain-specific financial knowledge remains
uncertain. Addressing a critical gap in the expansive Indian financial context,
this paper introduces CA-Ben, a Chartered Accountancy benchmark specifically
designed to evaluate the financial, legal, and quantitative reasoning
capabilities of LLMs. CA-Ben comprises structured question-answer datasets
derived from the rigorous examinations conducted by the Institute of Chartered
Accountants of India (ICAI), spanning foundational, intermediate, and advanced
CA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1
405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated
using standardized protocols. Results indicate variations in performance, with
Claude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and
legal reasoning. Notable challenges emerged in numerical computations and legal
interpretations. The findings emphasize the strengths and limitations of
current LLMs, suggesting future improvements through hybrid reasoning and
retrieval-augmented generation methods, particularly for quantitative analysis
and accurate legal interpretation.

</details>


### [13] [A Semi-supervised Scalable Unified Framework for E-commerce Query Classification](https://arxiv.org/abs/2506.21049)
*Chunyuan Yuan,Chong Zhang,Zheng Fang,Ming Pang,Xue Jiang,Changping Peng,Zhangang Lin,Ching Law*

Key words: 查询分类, 半监督学习, 统一框架, 电子商务, 知识增强

TL;DR: 该论文提出了一种半监督可扩展统一框架（SSUF），通过知识增强、标签增强和结构增强模块，解决了电子商务查询分类中信息不足和依赖后验标签的问题，显著提升了分类性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 电子商务查询通常简短且缺乏上下文，现有方法依赖用户点击行为构建训练样本，导致效果受限且效率低下。

Method: SSUF框架包含知识增强、标签增强和结构增强模块，统一处理查询分类任务，模块高度可插拔以适应不同子任务。

Result: 离线与在线实验表明，SSUF显著优于现有最佳模型。

Conclusion: SSUF通过多模块增强与统一框架设计，有效解决了电子商务查询分类中的关键问题。

Abstract: Query classification, including multiple subtasks such as intent and category
prediction, is vital to e-commerce applications. E-commerce queries are usually
short and lack context, and the information between labels cannot be used,
resulting in insufficient prior information for modeling. Most existing
industrial query classification methods rely on users' posterior click behavior
to construct training samples, resulting in a Matthew vicious cycle.
Furthermore, the subtasks of query classification lack a unified framework,
leading to low efficiency for algorithm optimization.
  In this paper, we propose a novel Semi-supervised Scalable Unified Framework
(SSUF), containing multiple enhanced modules to unify the query classification
tasks. The knowledge-enhanced module uses world knowledge to enhance query
representations and solve the problem of insufficient query information. The
label-enhanced module uses label semantics and semi-supervised signals to
reduce the dependence on posterior labels. The structure-enhanced module
enhances the label representation based on the complex label relations. Each
module is highly pluggable, and input features can be added or removed as
needed according to each subtask. We conduct extensive offline and online A/B
experiments, and the results show that SSUF significantly outperforms the
state-of-the-art models.

</details>


### [14] [MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for Conversational Stance Detection](https://arxiv.org/abs/2506.21053)
*Fuqiang Niu,Genan Dai,Yisha Lu,Jiayu Liao,Xiang Li,Hu Huang,Bowen Zhang*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In the realm of contemporary social media, automatic stance detection is
pivotal for opinion mining, as it synthesizes and examines user perspectives on
contentious topics to uncover prevailing trends and sentiments. Traditional
stance detection research often targets individual instances, thereby limiting
its capacity to model multi-party discussions typical in real social media
scenarios. This shortcoming largely stems from the scarcity of datasets that
authentically capture the dynamics of social media interactions, hindering
advancements in conversational stance detection. In this paper, we introduce
MT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational
stance detection. To the best of our knowledge, MT2-CSD is the largest dataset
available for this purpose, comprising 24,457 annotated instances and
exhibiting the greatest conversational depth, thereby presenting new challenges
for stance detection. To address these challenges, we propose the Large
Language model enhanced Conversational Relational Attention Network (LLM-CRAN),
which exploits the reasoning capabilities of LLMs to improve conversational
understanding. We conduct extensive experiments to evaluate the efficacy of
LLM-CRAN on the MT2-CSD dataset. The experimental results indicate that
LLM-CRAN significantly outperforms strong baseline models in the task of
conversational stance detection.

</details>


### [15] [DALR: Dual-level Alignment Learning for Multimodal Sentence Representation Learning](https://arxiv.org/abs/2506.21096)
*Kang He,Yuzhe Ding. Haining Wang,Fei Li,Chong Teng,Donghong Ji*

Key words: 多模态学习、句子表示、交叉模态对齐、模态内对齐、排序蒸馏

TL;DR: 本研究提出DALR方法，通过双重对齐学习解决多模态句子表示中的交叉模态偏差和模态内语义分歧问题，显著提升表示质量。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有方法在图像和文本的粗粒度对齐上表现优异，但交叉模态偏差和模态内语义分歧严重影响了句子表示质量，因此需要更精细的对齐策略。

Method: DALR引入一致性学习模块软化负样本并利用辅助任务的语义相似性实现细粒度交叉模态对齐，同时结合排序蒸馏和全局模态内对齐学习捕捉更复杂的句子关系。

Result: 在语义文本相似度（STS）和迁移学习（TR）任务上的实验表明，DALR优于现有基线方法。

Conclusion: DALR通过双重对齐学习有效解决了多模态句子表示的核心问题，并在实验中表现出优越性。

Abstract: Previous multimodal sentence representation learning methods have achieved
impressive performance. However, most approaches focus on aligning images and
text at a coarse level, facing two critical challenges:cross-modal misalignment
bias and intra-modal semantic divergence, which significantly degrade sentence
representation quality. To address these challenges, we propose DALR
(Dual-level Alignment Learning for Multimodal Sentence Representation). For
cross-modal alignment, we propose a consistency learning module that softens
negative samples and utilizes semantic similarity from an auxiliary task to
achieve fine-grained cross-modal alignment. Additionally, we contend that
sentence relationships go beyond binary positive-negative labels, exhibiting a
more intricate ranking structure. To better capture these relationships and
enhance representation quality, we integrate ranking distillation with global
intra-modal alignment learning. Comprehensive experiments on semantic textual
similarity (STS) and transfer (TR) tasks validate the effectiveness of our
approach, consistently demonstrating its superiority over state-of-the-art
baselines.

</details>


### [16] [ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry](https://arxiv.org/abs/2506.21098)
*Qinwen Chen,Wenbiao Tao,Zhiwei Zhu,Mingfan Xi,Liangzhong Guo,Yuan Wang,Wei Wang,Yunshi Lan*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Community Question Answering (CQA) platforms can be deemed as important
knowledge bases in community, but effectively leveraging historical
interactions and domain knowledge in real-time remains a challenge. Existing
methods often underutilize external knowledge, fail to incorporate dynamic
historical QA context, or lack memory mechanisms suited for industrial
deployment. We propose ComRAG, a retrieval-augmented generation framework for
real-time industrial CQA that integrates static knowledge with dynamic
historical QA pairs via a centroid-based memory mechanism designed for
retrieval, generation, and efficient storage. Evaluated on three industrial CQA
datasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%
improvement in vector similarity, reducing latency by 8.7% to 23.3%, and
lowering chunk growth from 20.23% to 2.06% over iterations.

</details>


### [17] [Progtuning: Progressive Fine-tuning Framework for Transformer-based Language Models](https://arxiv.org/abs/2506.21119)
*Xiaoshuang Ji,Zhendong Zhao,Xiaojun Chen,Xin Zhao,Zeyao Liu*

Key words: 微调, Transformer, 资源分配, 渐进式学习, 参数效率

TL;DR: Progtuning是一种结合渐进式学习的新颖微调框架，通过逐步减少更新的Transformer块数量，优化资源分配并减少约25%的参数更新，同时保持竞争力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着模型规模增长，更新所有参数的微调成本高昂，且现有方法忽略了Transformer块间贡献不均的问题，导致资源分配低效。

Method: 提出Progtuning框架，基于贡献逐步减少更新的Transformer块数量，优化参数更新效率。

Result: Progtuning减少约25%的参数更新，同时保持性能竞争力，并在多种适应场景中表现优异。

Conclusion: Progtuning通过渐进式调整，显著提升资源利用效率并保持高性能，适用于多种微调任务。

Abstract: Fine-tuning is a promising technique for leveraging Transformer-based
language models in downstream tasks. As model sizes continue to grow, updating
all model parameters becomes increasingly costly. Parameter-efficient
fine-tuning methods effectively address this issue by selectively updating a
small subset of parameters. However, fine-tuning and most existing
parameter-efficient fine-tuning methods require updating the same number of
parameters as the initial size, ignoring the unequal contribution across
Transformer blocks and leading to extremely inefficient allocation of computing
resources. In this paper, we propose Progtuning, the novel fine-tuning
framework combined with progressive learning for Transformer-based language
models. Specifically, Progtuning progressively reduces the number of updated
transformer blocks based on the contribution. Remarkably, Progtuning optimizes
resource allocation and reduces the number of updated parameters by
approximately 25\%, while still maintaining competitive performance. And it
also exhibits high adaptability with parameter-efficient fine-tuning methods,
demonstrating excellent performance across various adaptation scenarios.

</details>


### [18] [Compressed and Smooth Latent Space for Text Diffusion Modeling](https://arxiv.org/abs/2506.21170)
*Viacheslav Meshchaninov,Egor Chimbulatov,Alexander Shabalin,Aleksandr Abramov,Dmitry Vetrov*

Key words: Cosmos, 扩散模型, 文本生成, 隐空间, 自编码器

TL;DR: 提出了一种名为Cosmos的新方法，通过压缩的隐空间改进基于扩散模型的文本生成，解决了传统方法的慢速和全局一致性问题，同时保持或超越生成质量并显著提升推理速度。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 自回归语言模型在文本生成中存在速度慢和全局一致性差的限制；扩散模型虽提供了并行生成和灵活控制的潜力，但因高维token表示的应用受限。

Method: Cosmos在专为扩散模型设计的压缩平滑隐空间中操作，通过同时训练自编码器实现token级重构和与预训练语言编码器的对齐，利用扰动增强实现高效生成。

Result: Cosmos实现了8倍压缩且生成质量与token级扩散模型相当；增加隐空间长度后超越扩散和自回归基线；在多项任务中质量相当或更优，推理速度快2倍以上。

Conclusion: Cosmos展示了压缩隐空间在文本生成中的潜力，结合了扩散模型的并行优势与生成质量，显著提升了效率。

Abstract: Autoregressive language models dominate modern text generation, yet their
sequential nature introduces fundamental limitations: decoding is slow, and
maintaining global coherence remains challenging. Diffusion models offer a
promising alternative by enabling parallel generation and flexible control;
however, their application to text generation is hindered by the high
dimensionality of token-level representations. We introduce Cosmos, a novel
approach to text generation that operates entirely in a compressed, smooth
latent space tailored specifically for diffusion. This space is learned using
an autoencoder trained simultaneously for token-level reconstruction and
alignment with frozen activations from a pretrained language encoder, providing
robust semantic grounding and enabling effective perturbation-based
augmentations. Empirically, we demonstrate that text representations can be
compressed by $8\times$ while maintaining generation quality comparable to
token-level diffusion models. Furthermore, increasing the latent sequence
length allows Cosmos to surpass both diffusion-based and autoregressive
baselines. We evaluate Cosmos on four diverse generative tasks including story
generation, question generation, summarization, and detoxification and compare
it with various generative paradigms. Cosmos achieves comparable or superior
generation quality while offering more than $2\times$ faster inference.

</details>


### [19] [CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment](https://arxiv.org/abs/2506.20243)
*Papa Séga Wade,Mihai Andries,Ioannis Kanellos,Thierry Moudenc*

Key words: 自动流利度评估, 自监督学习, CNN-BiLSTM, 语音分块, 多模型融合

TL;DR: 提出了一种基于分块的多自监督学习融合方法，结合CNN-BiLSTM框架，用于非母语者的自动流利度评估，显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 自动流利度评估（AFA）在捕捉非母语者的语音节奏、停顿和不流畅性方面仍有挑战。

Method: 采用分块方法，结合多种自监督学习模型（Wav2Vec2、HuBERT、WavLM）和CNN-BiLSTM框架，通过Silero-VAD分割语音并融合特征。

Result: 在Avalinguo和Speechocean762数据集上，F1分数和Pearson相关性显著提升，优于单模型基线。

Conclusion: 分块多SSL融合方法在流利度评估中表现优异，但未来需探索其对不规则韵律方言的泛化能力。

Abstract: Automatic fluency assessment (AFA) remains challenging, particularly in
capturing speech rhythm, pauses, and disfluencies in non-native speakers. We
introduce a chunk-based approach integrating self-supervised learning (SSL)
models (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths
in phonetic, prosodic, and noisy speech modeling, with a hierarchical
CNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero
voice activity detection (Silero-VAD), enabling fine-grained temporal analysis
while mitigating over-segmentation artifacts. SSL embeddings are fused via a
learnable weighted mechanism, balancing acoustic and linguistic features, and
enriched with chunk-level fluency markers (e.g., speech rate, pause durations,
n-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies
across chunks. Evaluated on Avalinguo and Speechocean762, our approach improves
F1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines
on Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on
Avalinguo, surpassing Pyannote.audio-based segmentation baselines. These
findings highlight chunk-based multi-SSL fusion for robust fluency evaluation,
though future work should explore generalization to dialects with irregular
prosody.

</details>


### [20] [Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks](https://arxiv.org/abs/2506.21182)
*Isaac Chung,Imene Kerboua,Marton Kardos,Roman Solomatin,Kenneth Enevoldsen*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation
platform for text embedding models. While previous work has established the
core benchmark methodology, this paper focuses on the engineering aspects that
ensure MTEB's continued reproducibility and extensibility. We present our
approach to maintaining robust continuous integration pipelines that validate
dataset integrity, automate test execution, and assess benchmark results'
generalizability. We detail the design choices that collectively enhance
reproducibility and usability. Furthermore, we discuss our strategies for
handling community contributions and extending the benchmark with new tasks and
datasets. These engineering practices have been instrumental in scaling MTEB to
become more comprehensive while maintaining quality and, ultimately, relevance
to the field. Our experiences offer valuable insights for benchmark maintainers
facing similar challenges in ensuring reproducibility and usability in machine
learning evaluation frameworks. The MTEB repository is available at:
https://github.com/embeddings-benchmark/mteb

</details>


### [21] [Prompt-Guided Turn-Taking Prediction](https://arxiv.org/abs/2506.21191)
*Koji Inoue,Mikey Elmers,Yahui Fu,Zi Haur Pang,Divesh Lala,Keiko Ochi,Tatsuya Kawahara*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Turn-taking prediction models are essential components in spoken dialogue
systems and conversational robots. Recent approaches leverage transformer-based
architectures to predict speech activity continuously and in real-time. In this
study, we propose a novel model that enables turn-taking prediction to be
dynamically controlled via textual prompts. This approach allows intuitive and
explicit control through instructions such as "faster" or "calmer" adapting
dynamically to conversational partners and contexts. The proposed model builds
upon a transformer-based voice activity projection (VAP) model, incorporating
textual prompt embeddings into both channel-wise transformers and a
cross-channel transformer. We evaluated the feasibility of our approach using
over 950 hours of human-human spoken dialogue data. Since textual prompt data
for the proposed approach was not available in existing datasets, we utilized a
large language model (LLM) to generate synthetic prompt sentences. Experimental
results demonstrated that the proposed model improved prediction accuracy and
effectively varied turn-taking timing behaviors according to the textual
prompts.

</details>


### [22] [Enhancing Automatic Term Extraction with Large Language Models via Syntactic Retrieval](https://arxiv.org/abs/2506.21222)
*Yongchan Chun,Minhyuk Kim,Dongjun Kim,Chanjun Park,Heuiseok Lim*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Automatic Term Extraction (ATE) identifies domain-specific expressions that
are crucial for downstream tasks such as machine translation and information
retrieval. Although large language models (LLMs) have significantly advanced
various NLP tasks, their potential for ATE has scarcely been examined. We
propose a retrieval-based prompting strategy that, in the few-shot setting,
selects demonstrations according to \emph{syntactic} rather than semantic
similarity. This syntactic retrieval method is domain-agnostic and provides
more reliable guidance for capturing term boundaries. We evaluate the approach
in both in-domain and cross-domain settings, analyzing how lexical overlap
between the query sentence and its retrieved examples affects performance.
Experiments on three specialized ATE benchmarks show that syntactic retrieval
improves F1-score. These findings highlight the importance of syntactic cues
when adapting LLMs to terminology-extraction tasks.

</details>


### [23] [Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents](https://arxiv.org/abs/2506.21252)
*Tianyi Men,Zhuoran Jin,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As Multimodal Large Language Models (MLLMs) advance, multimodal agents show
promise in real-world tasks like web navigation and embodied intelligence.
However, due to limitations in a lack of external feedback, these agents
struggle with self-correction and generalization. A promising approach is to
use reward models as external feedback, but there is no clear on how to select
reward models for agents. Thus, there is an urgent need to build a reward bench
targeted at agents. To address these challenges, we propose Agent-RewardBench,
a benchmark designed to evaluate reward modeling ability in MLLMs. The
benchmark is characterized by three key features: (1) Multiple dimensions and
real-world agent scenarios evaluation. It covers perception, planning, and
safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the
assessment of agent capabilities at the individual steps of a task, providing a
more granular view of performance during the planning process; and (3)
Appropriately difficulty and high-quality. We carefully sample from 10 diverse
models, difficulty control to maintain task challenges, and manual verification
to ensure the integrity of the data. Experiments demonstrate that even
state-of-the-art multimodal models show limited performance, highlighting the
need for specialized training in agent reward modeling. Code is available at
github.

</details>


### [24] [Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?](https://arxiv.org/abs/2506.21274)
*Andrea McGlinchey,Peter J Barclay*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models can produce convincing "fake text" in domains such as
academic writing, product reviews, and political news. Many approaches have
been investigated for the detection of artificially generated text. While this
may seem to presage an endless "arms race", we note that newer LLMs use ever
more parameters, training data, and energy, while relatively simple classifiers
demonstrate a good level of detection accuracy with modest resources. To
approach the question of whether the models' ability to beat the detectors may
therefore reach a plateau, we examine the ability of statistical classifiers to
identify "fake text" in the style of classical detective fiction. Over a 0.5
version increase, we found that Gemini showed an increased ability to generate
deceptive text, while GPT did not. This suggests that reliable detection of
fake text may remain feasible even for ever-larger models, though new model
architectures may improve their deceptiveness

</details>


### [25] [Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning](https://arxiv.org/abs/2506.21285)
*Xin Xu,Tianhao Chen,Fan Zhang,Wanlong Liu,Pengxiang Li,Ajay Kumar Jaiswal,Yuchen Yan,Jishan Hu,Yang Wang,Hao Chen,Shiwei Liu,Shizhe Diao,Can Yang,Lu Yin*

Key words: LLM, 自我批判, 推理能力, 迭代优化

TL;DR: Double-Checker框架通过自我批判和迭代优化提升慢思考LLM的推理能力，显著提高性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 慢思考LLM的批判和优化能力有限，需增强其自我改进能力。

Method: 基于1,730条自我批判数据微调，使LLM能迭代优化输出。

Result: 在AIME基准测试中，pass@1性能从4.4%提升至18.2%。

Conclusion: 自我批判框架为开发更可信和高效的LLM提供了新方向。

Abstract: While slow-thinking large language models (LLMs) exhibit reflection-like
reasoning, commonly referred to as the "aha moment:, their ability to generate
informative critiques and refine prior solutions remains limited. In this
paper, we introduce Double-Checker, a principled framework designed to enhance
the reasoning capabilities of slow-thinking LLMs by fostering explicit
self-critique and iterative refinement of their previous solutions. By
fine-tuning on our curated 1,730 self-critical instances, Double-Checker
empowers long-CoT LLMs to iteratively critique and refine their outputs during
inference until they evaluate their solutions as correct under self-generated
critiques. We validate the efficacy of Double-Checker across a comprehensive
suite of reasoning benchmarks, demonstrating that iterative self-critique
significantly enhances the reasoning capabilities of long-CoT LLMs. Notably,
our Double-Checker increases the pass@1 performance on challenging AIME
benchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These
results highlight a promising direction for developing more trustworthy and
effective LLMs capable of structured self-critique.

</details>


### [26] [Small Encoders Can Rival Large Decoders in Detecting Groundedness](https://arxiv.org/abs/2506.21288)
*Istabrak Abbes,Gabriele Prato,Quentin Fournier,Fernando Rodriguez,Alaa Boukhary,Adam Elwood,Sarath Chandar*

Key words: 大型语言模型, 上下文, Groundedness, 轻量级模型, 推理延迟

TL;DR: 大型语言模型（LLMs）通过外部上下文增强性能，但在上下文不足时容易产生无依据的回答。研究提出轻量级检测模型以减少推理时间和资源消耗。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: LLMs在缺乏上下文时无法可靠回答问题，生成无依据的回答。确保回答的基于上下文的事实一致性（Groundedness）至关重要。

Method: 使用轻量级编码器模型（如RoBERTa和NomicBERT）对精选数据集进行微调，以检测查询是否基于上下文。

Result: 微调后的轻量级模型在Groundedness检测上性能媲美顶级LLMs（如Llama3 8B和GPT4o），同时显著降低推理延迟。

Conclusion: 轻量级检测模型在Groundedness检测中高效且资源友好，优于大型LLMs。

Abstract: Augmenting large language models (LLMs) with external context significantly
improves their performance in natural language processing (NLP) tasks. However,
LLMs struggle to answer queries reliably when the provided context lacks
information, often resorting to ungrounded speculation or internal knowledge.
Groundedness - generating responses strictly supported by the context - is
essential for ensuring factual consistency and trustworthiness. This study
focuses on detecting whether a given query is grounded in a document provided
in context before the costly answer generation by LLMs. Such a detection
mechanism can significantly reduce both inference time and resource
consumption. We show that lightweight, task specific encoder models such as
RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy
comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in
groundedness detection while reducing inference latency by orders of magnitude.
The code is available at : https://github.com/chandarlab/Hallucinate-less

</details>


### [27] [Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models](https://arxiv.org/abs/2506.21294)
*Bram Willemsen,Gabriel Skantze*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we explore the use of a text-only, autoregressive language
modeling approach for the extraction of referring expressions from visually
grounded dialogue. More specifically, the aim is to investigate the extent to
which the linguistic context alone can inform the detection of mentions that
have a (visually perceivable) referent in the visual context of the
conversation. To this end, we adapt a pretrained large language model (LLM) to
perform a relatively course-grained annotation of mention spans in unfolding
conversations by demarcating mention span boundaries in text via next-token
prediction. Our findings indicate that even when using a moderately sized LLM,
relatively small datasets, and parameter-efficient fine-tuning, a text-only
approach can be effective, highlighting the relative importance of the
linguistic context for this task. Nevertheless, we argue that the task
represents an inherently multimodal problem and discuss limitations fundamental
to unimodal approaches.

</details>


### [28] [Structuralist Approach to AI Literary Criticism: Leveraging Greimas Semiotic Square for Large Language Models](https://arxiv.org/abs/2506.21360)
*Fangzhou Dong,Yifan Zeng,Yingpeng Sang,Hong Shen*

Key words: GLASS, Greimas符号学方阵, 文学批评, 大语言模型, 叙事分析

TL;DR: 本文提出了GLASS框架，基于Greimas符号学方阵，用于增强大语言模型（LLMs）进行深度文学分析的能力。GLASS能快速解析叙事结构和深层意义，并提供了首个基于GSS的文学批评数据集和量化指标。实验表明，其分析结果与专家评价高度一致。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管LLMs在理解和生成文本方面表现出色，但在处理具有深刻思想和复杂叙事的文学作品时，其专业性不足。GLASS旨在填补这一研究空白。

Method: 基于Greimas符号学方阵（GSS）设计GLASS框架，构建首个GSS文学批评数据集（48部作品），并提出量化指标（LLM-as-a-judge范式）。

Result: GLASS的分析结果与专家评价高度一致，并在39部经典作品上生成了原创且高质量的分析。

Conclusion: GLASS为文学研究和教育提供了基于AI的工具，并揭示了文学参与的认知机制。

Abstract: Large Language Models (LLMs) excel in understanding and generating text but
struggle with providing professional literary criticism for works with profound
thoughts and complex narratives. This paper proposes GLASS (Greimas Literary
Analysis via Semiotic Square), a structured analytical framework based on
Greimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth
literary analysis. GLASS facilitates the rapid dissection of narrative
structures and deep meanings in narrative works. We propose the first dataset
for GSS-based literary criticism, featuring detailed analyses of 48 works. Then
we propose quantitative metrics for GSS-based literary criticism using the
LLM-as-a-judge paradigm. Our framework's results, compared with expert
criticism across multiple works and LLMs, show high performance. Finally, we
applied GLASS to 39 classic works, producing original and high-quality analyses
that address existing research gaps. This research provides an AI-based tool
for literary research and education, offering insights into the cognitive
mechanisms underlying literary engagement.

</details>


### [29] [Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation](https://arxiv.org/abs/2506.21384)
*Guanting Dong,Xiaoxi Li,Yuyao Zhang,Mengjie Deng*

Key words: RAG, Omni-RAG, 大型语言模型, 查询理解, 多意图分解, 实时检索

TL;DR: Omni-RAG是一个新框架，旨在提升实时开放域RAG系统的鲁棒性，通过LLM辅助查询理解和多意图分解来优化处理复杂查询。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现实中的RAG系统在处理嘈杂、模糊和含多意图的查询时表现不佳，当前系统通常在干净数据上训练和评估，难以应对复杂输入。

Method: Omni-RAG包含三个核心模块：深度查询理解与分解、意图感知知识检索、重排与生成，结合LLM和检索技术优化流程。

Result: 该方法能够有效处理复杂查询，通过分步分解和优化检索提升RAG系统的表现。

Conclusion: Omni-RAG在真实场景中显著提升了RAG系统的鲁棒性和效果，适用于如SIGIR 2025 LiveRAG Challenge等应用场景。

Abstract: Real-world live retrieval-augmented generation (RAG) systems face significant
challenges when processing user queries that are often noisy, ambiguous, and
contain multiple intents. While RAG enhances large language models (LLMs) with
external knowledge, current systems typically struggle with such complex
inputs, as they are often trained or evaluated on cleaner data. This paper
introduces Omni-RAG, a novel framework designed to improve the robustness and
effectiveness of RAG systems in live, open-domain settings. Omni-RAG employs
LLM-assisted query understanding to preprocess user inputs through three key
modules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs
with tailored prompts to denoise queries (e.g., correcting spelling errors) and
decompose multi-intent queries into structured sub-queries; (2) Intent-Aware
Knowledge Retrieval, which performs retrieval for each sub-query from a corpus
(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking
and Generation, where a reranker (i.e., BGE) refines document selection before
a final response is generated by an LLM (i.e., Falcon-10B) using a
chain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG
capabilities and the demands of real-world applications, such as those
highlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex
and noisy queries.

</details>


### [30] [Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection](https://arxiv.org/abs/2506.21443)
*Ali Şenol,Garima Agrawal,Huan Liu*

Key words: 概念漂移、LLM、欺诈检测、领域知识、多轮对话

TL;DR: 提出了一种结合领域知识的LLM框架，用于检测欺诈对话和概念漂移，效果显著。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 动态平台上欺骗性对话的检测因语言模式变化和概念漂移而变得困难，LLM在风险场景中处理上下文模糊性和幻觉存在局限性。

Method: 开发了DK-LLM框架，包含欺诈检测模块、概念漂移检测单元和漂移分类模块，结合领域知识和结构化提示。

Result: 在虚假评论和多轮对话数据集上，系统实现高精度检测，LLaMA实现98%分类准确率。

Conclusion: 领域知识和漂移感知显著提升了高风险NLP任务的性能、可解释性和鲁棒性。

Abstract: Detecting deceptive conversations on dynamic platforms is increasingly
difficult due to evolving language patterns and Concept Drift (CD)-i.e.,
semantic or topical shifts that alter the context or intent of interactions
over time. These shifts can obscure malicious intent or mimic normal dialogue,
making accurate classification challenging. While Large Language Models (LLMs)
show strong performance in natural language tasks, they often struggle with
contextual ambiguity and hallucinations in risk-sensitive scenarios. To address
these challenges, we present a Domain Knowledge (DK)-Enhanced LLM framework
that integrates pretrained LLMs with structured, task-specific insights to
perform fraud and concept drift detection. The proposed architecture consists
of three main components: (1) a DK-LLM module to detect fake or deceptive
conversations; (2) a drift detection unit (OCDD) to determine whether a
semantic shift has occurred; and (3) a second DK-LLM module to classify the
drift as either benign or fraudulent. We first validate the value of domain
knowledge using a fake review dataset and then apply our full framework to
SEConvo, a multiturn dialogue dataset that includes various types of fraud and
spam attacks. Results show that our system detects fake conversations with high
accuracy and effectively classifies the nature of drift. Guided by structured
prompts, the LLaMA-based implementation achieves 98% classification accuracy.
Comparative studies against zero-shot baselines demonstrate that incorporating
domain knowledge and drift awareness significantly improves performance,
interpretability, and robustness in high-stakes NLP applications.

</details>


### [31] [Text2Cypher Across Languages: Evaluating Foundational Models Beyond English](https://arxiv.org/abs/2506.21445)
*Makbule Gulcin Ozsoy,William Tai*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advances in large language models have enabled natural language
interfaces that translate user questions into database queries, such as
Text2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database
accessibility, most research today focuses solely on English, with limited
evaluation in other languages. This paper investigates the performance of
foundational LLMs on the Text2Cypher task across multiple languages. We create
and release a multilingual test set by translating English questions into
Spanish and Turkish while preserving the original Cypher queries, enabling fair
cross-lingual comparison. We evaluate multiple foundational models using
standardized prompts and metrics. Our results show a consistent performance
pattern: highest on English, then Spanish, and lowest on Turkish. We attribute
this to differences in training data availability and linguistic
characteristics. Additionally, we explore the impact of translating task
prompts into Spanish and Turkish. Results show little to no change in
evaluation metrics, suggesting prompt translation has minor impact. Our
findings highlight the need for more inclusive evaluation and development in
multilingual query generation. Future work includes schema localization and
fine-tuning across diverse languages.

</details>


### [32] [Aligning Spoken Dialogue Models from User Interactions](https://arxiv.org/abs/2506.21463)
*Anne Wu,Laurent Mazaré,Neil Zeghidour,Alexandre Défossez*

Key words: 偏好对齐, 语音对话, 实时交互, 多轮对话, AI反馈

TL;DR: 提出了一种基于用户实时对话的偏好对齐框架，用于改进语音对话模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有偏好学习方法主要针对文本语言模型，不适用于实时语音交互的复杂性（如打断、插话等）。

Method: 通过大规模标注数据集（15万偏好对）和离线对齐方法微调全双工自回归语音模型。

Result: 实验表明，通用对话反馈能有效提升模型的事实性、安全性和上下文对齐性。

Conclusion: 研究强调了多种动态因素平衡对自然实时语音对话系统的重要性。

Abstract: We propose a novel preference alignment framework for improving spoken
dialogue models on real-time conversations from user interactions. Current
preference learning methods primarily focus on text-based language models, and
are not directly suited to the complexities of real-time speech interactions,
with richer dynamics (e.g. interruption, interjection) and no explicit
segmentation between speaker turns.We create a large-scale dataset of more than
150,000 preference pairs from raw multi-turn speech conversations, annotated
with AI feedback, to cover preferences over both linguistic content and
temporal context variations. We leverage offline alignment methods to finetune
a full-duplex autoregressive speech-to-speech model. Extensive experiments
demonstrate that feedback on generic conversations can be consistently
effective in improving spoken dialogue models to produce more factual, safer
and more contextually aligned interactions. We deploy the finetuned model and
conduct holistic human evaluations to assess the impact beyond single-turn
conversations. Our findings shed light on the importance of a well-calibrated
balance among various dynamics, crucial for natural real-time speech dialogue
systems.

</details>


### [33] [TopK Language Models](https://arxiv.org/abs/2506.21468)
*Ryosuke Takahashi,Tatsuro Inaba,Kentaro Inui,Benjamin Heinzerling*

Key words: 稀疏自编码器, Transformer, TopK激活函数, 模型解释性

TL;DR: 论文提出了一种改进的Transformer架构，通过引入TopK激活函数替代传统的稀疏自编码器（SAE），解决了SAE在模型解释性和特征稳定性上的问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 稀疏自编码器（SAE）在分析Transformer语言模型的激活空间时存在局限性，如特征不稳定性和后训练依赖问题。

Method: 在Transformer架构的特定层引入TopK激活函数，使隐藏状态直接等价于TopK SAE的潜在特征。

Result: TopK LMs在保持模型能力的同时，显著提升了模型的解释性和稳定性，支持神经元干预和跨检查点分析。

Conclusion: TopK LMs为语言模型的解释性和可控性研究提供了稳定且高效的工具。

Abstract: Sparse autoencoders (SAEs) have become an important tool for analyzing and
interpreting the activation space of transformer-based language models (LMs).
However, SAEs suffer several shortcomings that diminish their utility and
internal validity. Since SAEs are trained post-hoc, it is unclear if the
failure to discover a particular concept is a failure on the SAE's side or due
to the underlying LM not representing this concept. This problem is exacerbated
by training conditions and architecture choices affecting which features an SAE
learns. When tracing how LMs learn concepts during training, the lack of
feature stability also makes it difficult to compare SAEs features across
different checkpoints. To address these limitations, we introduce a
modification to the transformer architecture that incorporates a TopK
activation function at chosen layers, making the model's hidden states
equivalent to the latent features of a TopK SAE. This approach eliminates the
need for post-hoc training while providing interpretability comparable to SAEs.
The resulting TopK LMs offer a favorable trade-off between model size,
computational efficiency, and interpretability. Despite this simple
architectural change, TopK LMs maintain their original capabilities while
providing robust interpretability benefits. Our experiments demonstrate that
the sparse representations learned by TopK LMs enable successful steering
through targeted neuron interventions and facilitate detailed analysis of
neuron formation processes across checkpoints and layers. These features make
TopK LMs stable and reliable tools for understanding how language models learn
and represent concepts, which we believe will significantly advance future
research on model interpretability and controllability.

</details>


### [34] [Bridging Offline and Online Reinforcement Learning for LLMs](https://arxiv.org/abs/2506.21495)
*Jack Lanchantin,Angelica Chen,Janice Lan,Xian Li,Swarnadeep Saha,Tianlu Wang,Jing Xu,Ping Yu,Weizhe Yuan,Jason E Weston,Sainbayar Sukhbaatar,Ilia Kulikov*

Key words: 强化学习、大语言模型、在线微调、多任务学习

TL;DR: 研究了从离线到半在线再到完全在线场景下，强化学习微调大语言模型的效果，发现在线和半在线方法表现相似且优于离线方法，多任务联合训练可提升性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨强化学习在不同在线程度下微调大语言模型的效果，尤其是可验证与不可验证任务的差异。

Method: 比较在线与半在线的Direct Preference Optimization和Group Reward Policy Optimization方法，分析训练动态和超参数选择。

Result: 在线和半在线方法性能相似且优于离线方法；联合训练可验证与不可验证任务能提升整体表现。

Conclusion: 强化学习在在线和半在线场景下均有效，多任务联合训练是提升模型性能的关键。

Abstract: We investigate the effectiveness of reinforcement learning methods for
finetuning large language models when transitioning from offline to semi-online
to fully online regimes for both verifiable and non-verifiable tasks. Our
experiments cover training on verifiable math as well as non-verifiable
instruction following with a set of benchmark evaluations for both. Across
these settings, we extensively compare online and semi-online Direct Preference
Optimization and Group Reward Policy Optimization objectives, and surprisingly
find similar performance and convergence between these variants, which all
strongly outperform offline methods. We provide a detailed analysis of the
training dynamics and hyperparameter selection strategies to achieve optimal
results. Finally, we show that multi-tasking with verifiable and non-verifiable
rewards jointly yields improved performance across both task types.

</details>


### [35] [Enhancing User Engagement in Socially-Driven Dialogue through Interactive LLM Alignments](https://arxiv.org/abs/2506.21497)
*Jiashuo Wang,Kaitao Song,Chunpu Xu,Changhe Song,Yang Xiao,Dongsheng Li,Lili Qiu,Wenjie Li*

Key words: 用户参与；交互式LLMs；i×MCTS；DPO；社会化对话

TL;DR: 该论文提出了一种通过学习用户未来对话信号来增强交互式LLMs用户参与度的方法，利用用户反应作为奖励信号，并通过i×MCTS和DPO优化模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有方法未能有效保证用户在社会化对话中的参与度，因此需要一种更直接的方法来学习和优化用户参与。

Method: 利用i×MCTS模拟用户与LLMs的交互，收集高质量和低质量对话数据，通过直接偏好优化（DPO）对齐LLMs。

Result: 在两个社会化对话场景（情感支持和劝善对话）中，该方法显著提升了用户参与度。

Conclusion: 通过直接学习用户反应信号并结合i×MCTS和DPO，可以有效提升交互式LLMs的用户参与度。

Abstract: Enhancing user engagement through interactions plays an essential role in
socially-driven dialogues. While prior works have optimized models to reason
over relevant knowledge or plan a dialogue act flow, the relationship between
user engagement and knowledge or dialogue acts is subtle and does not guarantee
user engagement in socially-driven dialogues. To this end, we enable
interactive LLMs to learn user engagement by leveraging signals from the future
development of conversations. Specifically, we adopt a more direct and relevant
indicator of user engagement, i.e., the user's reaction related to dialogue
intention after the interaction, as a reward to align interactive LLMs. To
achieve this, we develop a user simulator to interact with target interactive
LLMs and explore interactions between the user and the interactive LLM system
via \textit{i$\times$MCTS} (\textit{M}onte \textit{C}arlo \textit{T}ree
\textit{S}earch for \textit{i}nteraction). In this way, we collect a dataset
containing pairs of higher and lower-quality experiences using
\textit{i$\times$MCTS}, and align interactive LLMs for high-level user
engagement by direct preference optimization (DPO) accordingly. Experiments
conducted on two socially-driven dialogue scenarios (emotional support
conversations and persuasion for good) demonstrate that our method effectively
enhances user engagement in interactive LLMs.

</details>


### [36] [skLEP: A Slovak General Language Understanding Benchmark](https://arxiv.org/abs/2506.21508)
*Marek Šuppa,Andrej Ridzik,Daniel Hládek,Tomáš Javůrek,Viktória Ondrejová,Kristína Sásiková,Martin Tamajka,Marián Šimko*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this work, we introduce skLEP, the first comprehensive benchmark
specifically designed for evaluating Slovak natural language understanding
(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span
token-level, sentence-pair, and document-level challenges, thereby offering a
thorough assessment of model capabilities. To create this benchmark, we curated
new, original datasets tailored for Slovak and meticulously translated
established English NLU resources. Within this paper, we also present the first
systematic and extensive evaluation of a wide array of Slovak-specific,
multilingual, and English pre-trained language models using the skLEP tasks.
Finally, we also release the complete benchmark data, an open-source toolkit
facilitating both fine-tuning and evaluation of models, and a public
leaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering
reproducibility and drive future research in Slovak NLU.

</details>


### [37] [Potemkin Understanding in Large Language Models](https://arxiv.org/abs/2506.21521)
*Marina Mancoridis,Bec Weeks,Keyon Vafa,Sendhil Mullainathan*

Key words: 大型语言模型、基准测试、虚假理解、概念表示、模型评估

TL;DR: 论文探讨了基于基准数据集评估大型语言模型（LLMs）的合理性，并提出了一种识别模型虚假理解的框架和方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前评估LLMs的基准数据集是否真正反映了模型的理解能力存在疑问，因为这些基准也用于评估人类，但模型可能以与人类不同的方式“误解”概念。

Method: 提出了两种量化模型虚假理解（potemkins）的方法：一种是通过专门设计的基准任务，另一种是通过通用方法提供其普遍性的下限。

Result: 研究发现虚假理解在模型、任务和领域中普遍存在，且反映了概念表征的内部不一致性。

Conclusion: 基准测试的成功可能仅是虚假理解的体现，而非真正的概念掌握。

Abstract: Large language models (LLMs) are regularly evaluated using benchmark
datasets. But what justifies making inferences about an LLM's capabilities
based on its answers to a curated set of questions? This paper first introduces
a formal framework to address this question. The key is to note that the
benchmarks used to test LLMs -- such as AP exams -- are also those used to test
people. However, this raises an implication: these benchmarks are only valid
tests if LLMs misunderstand concepts in ways that mirror human
misunderstandings. Otherwise, success on benchmarks only demonstrates potemkin
understanding: the illusion of understanding driven by answers irreconcilable
with how any human would interpret a concept. We present two procedures for
quantifying the existence of potemkins: one using a specially designed
benchmark in three domains, the other using a general procedure that provides a
lower-bound on their prevalence. We find that potemkins are ubiquitous across
models, tasks, and domains. We also find that these failures reflect not just
incorrect understanding, but deeper internal incoherence in concept
representations.

</details>


### [38] ["What's Up, Doc?": Analyzing How Users Seek Health Information in Large-Scale Conversational AI Datasets](https://arxiv.org/abs/2506.21532)
*Akshay Paruchuri,Maryam Aziz,Rohit Vartak,Ayman Ali,Best Uchehara,Xin Liu,Ishan Chatterjee,Monica Agrawal*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: People are increasingly seeking healthcare information from large language
models (LLMs) via interactive chatbots, yet the nature and inherent risks of
these conversations remain largely unexplored. In this paper, we filter
large-scale conversational AI datasets to achieve HealthChat-11K, a curated
dataset of 11K real-world conversations composed of 25K user messages. We use
HealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs
when seeking healthcare information in order to systematically study user
interactions across 21 distinct health specialties. Our analysis reveals
insights into the nature of how and why users seek health information, such as
common interactions, instances of incomplete context, affective behaviors, and
interactions (e.g., leading questions) that can induce sycophancy, underscoring
the need for improvements in the healthcare support capabilities of LLMs
deployed as conversational AI. Code and artifacts to retrieve our analyses and
combine them into a curated dataset can be found here:
https://github.com/yahskapar/HealthChat

</details>


### [39] [Data Efficacy for Language Model Training](https://arxiv.org/abs/2506.21545)
*Yalun Dai,Yangyu Huang,Xin Zhang,Wenshan Wu,Chong Li,Wenhui Lu,Shijie Cao,Li Dong,Scarlett Li*

Key words: 语言模型、数据效能、DELT、LQS、FO

TL;DR: 论文提出数据效能（Data Efficacy）概念，关注通过优化训练数据组织提升语言模型性能，并提出DELT框架及其组件LQS和FO，实验验证其有效性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有研究主要关注数据效率（如数据筛选、采样），而数据组织对性能的影响尚未充分探索，因此提出数据效能概念填补这一空白。

Method: 提出DELT框架，包含数据评分（LQS）、数据选择和排序（FO），重点从梯度一致性和数据分布优化角度提升效能。

Result: 实验表明DELT能不同程度提升模型性能，其中LQS和FO组合效果最佳，且可与数据效率技术结合使用。

Conclusion: 数据效能是语言模型训练中的重要方向，未来潜力巨大。

Abstract: Data is fundamental to the training of language models (LM). Recent research
has been dedicated to data efficiency, which aims to maximize performance by
selecting a minimal or optimal subset of training data. Techniques such as data
filtering, sampling, and selection play a crucial role in this area. To
complement it, we define Data Efficacy, which focuses on maximizing performance
by optimizing the organization of training data and remains relatively
underexplored. This work introduces a general paradigm, DELT, for considering
data efficacy in LM training, which highlights the significance of training
data organization. DELT comprises three components: Data Scoring, Data
Selection, and Data Ordering. Among these components, we design
Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which
considers both the learnability and quality of each data sample from the
gradient consistency perspective. We also devise Folding Ordering (FO), as a
novel instance of Data Ordering, which addresses issues such as model
forgetting and data distribution bias. Comprehensive experiments validate the
data efficacy in LM training, which demonstrates the following: Firstly,
various instances of the proposed DELT enhance LM performance to varying
degrees without increasing the data scale and model size. Secondly, among these
instances, the combination of our proposed LQS for data scoring and Folding for
data ordering achieves the most significant improvement. Lastly, data efficacy
can be achieved together with data efficiency by applying data selection.
Therefore, we believe that data efficacy is a promising foundational area in LM
training.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [40] [Progressive Size-Adaptive Federated Learning: A Comprehensive Framework for Heterogeneous Multi-Modal Data Systems](https://arxiv.org/abs/2506.20685)
*Sajid Hussain,Muhammad Sohail,Nauman Ali Khan,Naima Iltaf,Ihtesham ul Islam*

Key words: 联邦学习、数据集大小、多模态数据、自适应框架、通信效率

TL;DR: SAFL框架基于数据集大小特性优化联邦学习，揭示了数据集大小的最佳范围、模态性能层次及大规模数据性能下降问题，实现高效通信与高精度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有联邦学习方法忽视数据集大小对训练动态的基础影响，需要系统性研究数据特性如何驱动联邦学习策略。

Method: 提出基于数据集大小的自适应联邦学习框架SAFL，通过多模态数据集（13个数据集、7种模态）实验验证其有效性。

Result: 发现1000-1500样本为最佳范围，结构化数据（时间序列、传感器）表现优于非结构化数据，大数据集性能下降。SAFL平均准确率87.68%，通信效率高（7.38GB数据传输）。

Conclusion: SAFL填补了数据特性对联邦学习影响的理论空白，为实际部署提供实用指导。

Abstract: Federated Learning (FL) has emerged as a transformative paradigm for
distributed machine learning while preserving data privacy. However, existing
approaches predominantly focus on model heterogeneity and aggregation
techniques, largely overlooking the fundamental impact of dataset size
characteristics on federated training dynamics. This paper introduces
Size-Based Adaptive Federated Learning (SAFL), a novel progressive training
framework that systematically organizes federated learning based on dataset
size characteristics across heterogeneous multi-modal data. Our comprehensive
experimental evaluation across 13 diverse datasets spanning 7 modalities
(vision, text, time series, audio, sensor, medical vision, and multimodal)
reveals critical insights: 1) an optimal dataset size range of 1000-1500
samples for federated learning effectiveness; 2) a clear modality performance
hierarchy with structured data (time series, sensor) significantly
outperforming unstructured data (text, multimodal); and 3) systematic
performance degradation for large datasets exceeding 2000 samples. SAFL
achieves an average accuracy of 87.68% across all datasets, with structured
data modalities reaching 99%+ accuracy. The framework demonstrates superior
communication efficiency, reducing total data transfer to 7.38 GB across 558
communications while maintaining high performance. Our real-time monitoring
framework provides unprecedented insights into system resource utilization,
network efficiency, and training dynamics. This work fills critical gaps in
understanding how data characteristics should drive federated learning
strategies, providing both theoretical insights and practical guidance for
real-world FL deployments in neural network and learning systems.

</details>


### [41] [E-ABIN: an Explainable module for Anomaly detection in BIological Networks](https://arxiv.org/abs/2506.20693)
*Ugo Lomoio,Tommaso Mazza,Pierangelo Veltri,Pietro Hiram Guzzi*

Key words: E-ABIN, 异常检测, 生物网络, 可解释性, 机器学习, 深度学习

TL;DR: E-ABIN是一个用于生物网络异常检测的通用可解释框架，整合了多种机器学习和深度学习技术，提供用户友好界面，并在癌症和乳糜泻案例中验证其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大规模组学数据的增多需要能处理复杂基因表达数据并提供可解释结果的框架。现有方法多局限于单一数据集且缺乏直观界面。

Method: 结合支持向量机、随机森林、图自动编码器和图对抗属性网络，构建通用平台E-ABIN，支持基因表达或甲基化网络的异常检测与解释。

Result: 在膀胱癌和乳糜泻案例中，E-ABIN成功识别了生物相关的异常并揭示了疾病机制。

Conclusion: E-ABIN提供了一种预测准确且可解释的解决方案，适用于复杂生物网络的异常分析。

Abstract: The increasing availability of large-scale omics data calls for robust
analytical frameworks capable of handling complex gene expression datasets
while offering interpretable results. Recent advances in artificial
intelligence have enabled the identification of aberrant molecular patterns
distinguishing disease states from healthy controls. Coupled with improvements
in model interpretability, these tools now support the identification of genes
potentially driving disease phenotypes. However, current approaches to gene
anomaly detection often remain limited to single datasets and lack accessible
graphical interfaces. Here, we introduce E-ABIN, a general-purpose, explainable
framework for Anomaly detection in Biological Networks. E-ABIN combines
classical machine learning and graph-based deep learning techniques within a
unified, user-friendly platform, enabling the detection and interpretation of
anomalies from gene expression or methylation-derived networks. By integrating
algorithms such as Support Vector Machines, Random Forests, Graph Autoencoders
(GAEs), and Graph Adversarial Attributed Networks (GAANs), E-ABIN ensures a
high predictive accuracy while maintaining interpretability. We demonstrate the
utility of E-ABIN through case studies of bladder cancer and coeliac disease,
where it effectively uncovers biologically relevant anomalies and offers
insights into disease mechanisms.

</details>


### [42] [On Context-Content Uncertainty Principle](https://arxiv.org/abs/2506.20699)
*Xin Li*

Key words: 不确定性原理、推理框架、熵不对称性、分层计算、结构对齐

TL;DR: 论文提出“上下文-内容不确定性原理”（CCUP），认为推理受上下文与内容间的熵不对称性支配，并通过分层计算框架实现高效推理。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 旨在为大脑和机器如何通过递归的结构-特异性对齐来最小化不确定性提供统一理论基础。

Method: 基于CCUP构建四层操作原则：核心推理约束、资源分配原则、时间引导动态学习和空间层次组合。

Result: 展示了CCUP对齐推理的效率提升，并通过形式等价定理和依赖关系验证其有效性。

Conclusion: 大脑不仅是推理机器，更是路径依赖、内容引导的熵梯度解析器。

Abstract: The Context-Content Uncertainty Principle (CCUP) proposes that inference
under uncertainty is governed by an entropy asymmetry between context and
content: high-entropy contexts must be interpreted through alignment with
low-entropy, structured content. In this paper, we develop a layered
computational framework that derives operational principles from this
foundational asymmetry. At the base level, CCUP formalizes inference as
directional entropy minimization, establishing a variational gradient that
favors content-first structuring. Building upon this, we identify four
hierarchical layers of operational principles: (\textbf{L1}) \emph{Core
Inference Constraints}, including structure-before-specificity, asymmetric
inference flow, cycle-consistent bootstrapping, and conditional compression,
all shown to be mutually reducible; (\textbf{L2}) \emph{Resource Allocation
Principles}, such as precision-weighted attention, asymmetric learning rates,
and attractor-based memory encoding; (\textbf{L3}) \emph{Temporal Bootstrapping
Dynamics}, which organize learning over time via structure-guided curricula;
and (\textbf{L4}) \emph{Spatial Hierarchical Composition}, which integrates
these mechanisms into self-organizing cycles of memory, inference, and
planning. We present formal equivalence theorems, a dependency lattice among
principles, and computational simulations demonstrating the efficiency gains of
CCUP-aligned inference. This work provides a unified theoretical foundation for
understanding how brains and machines minimize uncertainty through recursive
structure-specificity alignment. The brain is not just an inference machine. It
is a cycle-consistent entropy gradient resolver, aligning structure and
specificity via path-dependent, content-seeded simulation.

</details>


### [43] [Diffusion Tree Sampling: Scalable inference-time alignment of diffusion models](https://arxiv.org/abs/2506.20701)
*Vineet Jain,Kusha Sareen,Mohammad Pedramfar,Siamak Ravanbakhsh*

Key words: 扩散模型、推理对齐、蒙特卡洛树搜索、计算效率、文本生成

TL;DR: 提出一种基于树的方法（DTS和DTS$^\star$），通过在扩散链中传播终端奖励并迭代优化估计值，高效生成奖励对齐的样本，显著减少计算需求。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法在高噪声水平下估计不准确且计算效率低，未能重用历史信息。受蒙特卡洛树搜索启发，将其转化为搜索问题以优化推理对齐。

Method: Diffusion Tree Sampling (DTS)及其贪婪变体DTS$^\star$，通过树结构传播奖励并全局搜索高奖励样本，实现渐进精确采样。

Result: 在MNIST和CIFAR-10上，DTS以10倍计算节约匹配最佳基线FID；在文本生成和语言任务中，DTS$^\star$以5倍计算匹配最优样本。

Conclusion: 该方法通过重用信息提升样本质量，为扩散模型的推理对齐提供了可扩展的解决方案。

Abstract: Adapting a pretrained diffusion model to new objectives at inference time
remains an open problem in generative modeling. Existing steering methods
suffer from inaccurate value estimation, especially at high noise levels, which
biases guidance. Moreover, information from past runs is not reused to improve
sample quality, resulting in inefficient use of compute. Inspired by the
success of Monte Carlo Tree Search, we address these limitations by casting
inference-time alignment as a search problem that reuses past computations. We
introduce a tree-based approach that samples from the reward-aligned target
density by propagating terminal rewards back through the diffusion chain and
iteratively refining value estimates with each additional generation. Our
proposed method, Diffusion Tree Sampling (DTS), produces asymptotically exact
samples from the target distribution in the limit of infinite rollouts, and its
greedy variant, Diffusion Tree Search (DTS$^\star$), performs a global search
for high reward samples. On MNIST and CIFAR-10 class-conditional generation,
DTS matches the FID of the best-performing baseline with up to $10\times$ less
compute. In text-to-image generation and language completion tasks, DTS$^\star$
effectively searches for high reward samples that match best-of-N with up to
$5\times$ less compute. By reusing information from previous generations, we
get an anytime algorithm that turns additional compute into steadily better
samples, providing a scalable approach for inference-time alignment of
diffusion models.

</details>


### [44] [On Convolutions, Intrinsic Dimension, and Diffusion Models](https://arxiv.org/abs/2506.20705)
*Kin Kwan Leung,Rasa Hosseinzadeh,Gabriel Loaiza-Ganem*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The manifold hypothesis asserts that data of interest in high-dimensional
ambient spaces, such as image data, lies on unknown low-dimensional
submanifolds. Diffusion models (DMs) -- which operate by convolving data with
progressively larger amounts of Gaussian noise and then learning to revert this
process -- have risen to prominence as the most performant generative models,
and are known to be able to learn distributions with low-dimensional support.
For a given datum in one of these submanifolds, we should thus intuitively
expect DMs to have implicitly learned its corresponding local intrinsic
dimension (LID), i.e. the dimension of the submanifold it belongs to. Kamkari
et al. (2024b) recently showed that this is indeed the case by linking this LID
to the rate of change of the log marginal densities of the DM with respect to
the amount of added noise, resulting in an LID estimator known as FLIPD. LID
estimators such as FLIPD have a plethora of uses, among others they quantify
the complexity of a given datum, and can be used to detect outliers,
adversarial examples and AI-generated text. FLIPD achieves state-of-the-art
performance at LID estimation, yet its theoretical underpinnings are incomplete
since Kamkari et al. (2024b) only proved its correctness under the highly
unrealistic assumption of affine submanifolds. In this work we bridge this gap
by formally proving the correctness of FLIPD under realistic assumptions.
Additionally, we show that an analogous result holds when Gaussian convolutions
are replaced with uniform ones, and discuss the relevance of this result.

</details>


### [45] [Test-time Scaling Techniques in Theoretical Physics -- A Comparison of Methods on the TPBench Dataset](https://arxiv.org/abs/2506.20729)
*Zhiqi Gao,Tianyi Li,Yurii Kvasiuk,Sai Chaitanya Tadepalli,Maja Rudolph,Daniel J. H. Chung,Frederic Sala,Moritz Münchmeyer*

Key words: 大语言模型, 测试时间扩展, 理论物理, 符号验证, TPBench

TL;DR: 本文研究了测试时间扩展技术在大语言模型（LLMs）上的应用，特别是在高级理论物理领域的泛化能力。通过改进框架，显著提升了解决复杂科学问题的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索测试时间扩展技术在数学推理基准（如AIME）上的成功经验是否能推广到高级理论物理领域。

Method: 提出了一种新的符号化弱验证框架，并评估了多种测试时间扩展方法在TPBench物理数据集上的表现。

Result: 新方法在TPBench上的表现显著优于现有技术，并在AIME上也验证了其有效性。

Conclusion: 符号化逐步验证是解决复杂科学问题的强大工具。

Abstract: Large language models (LLMs) have shown strong capabilities in complex
reasoning, and test-time scaling techniques can enhance their performance with
comparably low cost. Many of these methods have been developed and evaluated on
mathematical reasoning benchmarks such as AIME. This paper investigates whether
the lessons learned from these benchmarks generalize to the domain of advanced
theoretical physics. We evaluate a range of common test-time scaling methods on
the TPBench physics dataset and compare their effectiveness with results on
AIME. To better leverage the structure of physics problems, we develop a novel,
symbolic weak-verifier framework to improve parallel scaling results. Our
empirical results demonstrate that this method significantly outperforms
existing test-time scaling approaches on TPBench. We also evaluate our method
on AIME, confirming its effectiveness in solving advanced mathematical
problems. Our findings highlight the power of step-wise symbolic verification
for tackling complex scientific problems.

</details>


### [46] [A Survey of AI for Materials Science: Foundation Models, LLM Agents, Datasets, and Tools](https://arxiv.org/abs/2506.20743)
*Minh-Hao Van,Prateek Verma,Chen Zhao,Xintao Wu*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Foundation models (FMs) are catalyzing a transformative shift in materials
science (MatSci) by enabling scalable, general-purpose, and multimodal AI
systems for scientific discovery. Unlike traditional machine learning models,
which are typically narrow in scope and require task-specific engineering, FMs
offer cross-domain generalization and exhibit emergent capabilities. Their
versatility is especially well-suited to materials science, where research
challenges span diverse data types and scales. This survey provides a
comprehensive overview of foundation models, agentic systems, datasets, and
computational tools supporting this growing field. We introduce a task-driven
taxonomy encompassing six broad application areas: data extraction,
interpretation and Q\&A; atomistic simulation; property prediction; materials
structure, design and discovery; process planning, discovery, and optimization;
and multiscale modeling. We discuss recent advances in both unimodal and
multimodal FMs, as well as emerging large language model (LLM) agents.
Furthermore, we review standardized datasets, open-source tools, and autonomous
experimental platforms that collectively fuel the development and integration
of FMs into research workflows. We assess the early successes of foundation
models and identify persistent limitations, including challenges in
generalizability, interpretability, data imbalance, safety concerns, and
limited multimodal fusion. Finally, we articulate future research directions
centered on scalable pretraining, continual learning, data governance, and
trustworthiness.

</details>


### [47] [Multiple Streams of Relation Extraction: Enriching and Recalling in Transformers](https://arxiv.org/abs/2506.20746)
*Todd Nief,David Reber,Sean Richardson,Ari Holtzman*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: When an LLM learns a relation during finetuning (e.g., new movie releases,
corporate mergers, etc.), where does this information go? Is it extracted when
the model processes an entity, recalled just-in-time before a prediction, or
are there multiple separate heuristics? Existing localization approaches (e.g.
activation patching) are ill-suited for this analysis because they tend to
replace parts of the residual stream, potentially deleting information. To fill
this gap, we propose dynamic weight-grafting between fine-tuned and pre-trained
language models to show that fine-tuned language models both (1) extract
relation information learned during finetuning while processing entities and
(2) ``recall" this information in later layers while generating predictions. In
some cases, models need both of these pathways to correctly generate finetuned
information while, in other cases, a single ``enrichment" or ``recall" pathway
alone is sufficient. We examine the necessity and sufficiency of these
information pathways, examining what layers they occur at, how much redundancy
they exhibit, and which model components are involved -- finding that the
``recall" pathway occurs via both task-specific attention mechanisms and a
relation extraction step in the output of the attention and the feedforward
networks at the final layers before next token prediction.

</details>


### [48] [Characterization and Mitigation of Training Instabilities in Microscaling Formats](https://arxiv.org/abs/2506.20752)
*Huangyuan Su,Mujin Kwun,Stephanie Gil,Sham Kakade,Nikhil Anand*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Training large language models is an expensive, compute-bound process that
must be repeated as models scale, algorithms improve, and new data is
collected. To address this, next-generation hardware accelerators increasingly
support lower-precision arithmetic formats, such as the Microscaling (MX)
formats introduced in NVIDIA's Blackwell architecture. These formats use a
shared scale within blocks of parameters to extend representable range and
perform forward/backward GEMM operations in reduced precision for efficiency
gains. In this work, we investigate the challenges and viability of
block-scaled precision formats during model training. Across nearly one
thousand language models trained from scratch -- spanning compute budgets from
$2 \times 10^{17}$ to $4.8 \times 10^{19}$ FLOPs and sweeping over a broad
range of weight-activation precision combinations -- we consistently observe
that training in MX formats exhibits sharp, stochastic instabilities in the
loss, particularly at larger compute scales. To explain this phenomenon, we
conduct controlled experiments and ablations on a smaller proxy model that
exhibits similar behavior as the language model, sweeping across architectural
settings, hyperparameters, and precision formats. These experiments motivate a
simple model in which multiplicative gradient bias introduced by the
quantization of layer-norm affine parameters and a small fraction of
activations can trigger runaway divergence. Through \emph{in situ} intervention
experiments on our proxy model, we demonstrate that instabilities can be
averted or delayed by modifying precision schemes mid-training. Guided by these
findings, we evaluate stabilization strategies in the LLM setting and show that
certain hybrid configurations recover performance competitive with
full-precision training. We release our code at
https://github.com/Hither1/systems-scaling.

</details>


### [49] [Stochastic and Non-local Closure Modeling for Nonlinear Dynamical Systems via Latent Score-based Generative Models](https://arxiv.org/abs/2506.20771)
*Xinghao Dong,Huchen Yang,Jin-Long Wu*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose a latent score-based generative AI framework for learning
stochastic, non-local closure models and constitutive laws in nonlinear
dynamical systems of computational mechanics. This work addresses a key
challenge of modeling complex multiscale dynamical systems without a clear
scale separation, for which numerically resolving all scales is prohibitively
expensive, e.g., for engineering turbulent flows. While classical closure
modeling methods leverage domain knowledge to approximate subgrid-scale
phenomena, their deterministic and local assumptions can be too restrictive in
regimes lacking a clear scale separation. Recent developments of
diffusion-based stochastic models have shown promise in the context of closure
modeling, but their prohibitive computational inference cost limits practical
applications for many real-world applications. This work addresses this
limitation by jointly training convolutional autoencoders with conditional
diffusion models in the latent spaces, significantly reducing the
dimensionality of the sampling process while preserving essential physical
characteristics. Numerical results demonstrate that the joint training approach
helps discover a proper latent space that not only guarantees small
reconstruction errors but also ensures good performance of the diffusion model
in the latent space. When integrated into numerical simulations, the proposed
stochastic modeling framework via latent conditional diffusion models achieves
significant computational acceleration while maintaining comparable predictive
accuracy to standard diffusion models in physical spaces.

</details>


### [50] [Stochastic Parameter Decomposition](https://arxiv.org/abs/2506.20790)
*Lucius Bushnaq,Dan Braun,Lee Sharkey*

Key words: 神经网络分解；线性参数分解；随机参数分解；机制解释性；超参数敏感

TL;DR: 本文提出了一种名为随机参数分解（SPD）的新方法，用于更高效地分解神经网络参数，解决了现有方法（如APD）在计算成本和超参数敏感性方面的不足。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前主流的参数分解方法（如APD）存在计算成本高和超参数敏感的问题，阻碍了其在更大模型中的应用。本文旨在提出一种更高效、稳定的替代方法。

Method: 作者提出了随机参数分解（SPD），该方法通过随机化优化过程，提高了计算可扩展性，并减少了对超参数的依赖。

Result: 实验表明，SPD能够分解比APD更大、更复杂的模型，同时避免了参数收缩等问题，并在玩具模型中更好地识别了真实机制。

Conclusion: SPD通过结合因果中介分析和网络分解方法，为大规模模型的机制解释性研究提供了新的可能性。

Abstract: A key step in reverse engineering neural networks is to decompose them into
simpler parts that can be studied in relative isolation. Linear parameter
decomposition -- a framework that has been proposed to resolve several issues
with current decomposition methods -- decomposes neural network parameters into
a sum of sparsely used vectors in parameter space. However, the current main
method in this framework, Attribution-based Parameter Decomposition (APD), is
impractical on account of its computational cost and sensitivity to
hyperparameters. In this work, we introduce \textit{Stochastic Parameter
Decomposition} (SPD), a method that is more scalable and robust to
hyperparameters than APD, which we demonstrate by decomposing models that are
slightly larger and more complex than was possible to decompose with APD. We
also show that SPD avoids other issues, such as shrinkage of the learned
parameters, and better identifies ground truth mechanisms in toy models. By
bridging causal mediation analysis and network decomposition methods, this
demonstration opens up new research possibilities in mechanistic
interpretability by removing barriers to scaling linear parameter decomposition
methods to larger models. We release a library for running SPD and reproducing
our experiments at https://github.com/goodfire-ai/spd.

</details>


### [51] [GPU Kernel Scientist: An LLM-Driven Framework for Iterative Kernel Optimization](https://arxiv.org/abs/2506.20807)
*Martin Andrews,Sam Witteveen*

Key words: GPU优化, LLM, 自动化方法, AMD MI300

TL;DR: 该论文提出了一种利用大语言模型（LLM）驱动的自动化方法（GPU Kernel Scientist）来优化GPU内核，通过多阶段进化过程生成和验证优化假设，旨在降低对领域专家的依赖并加速优化过程。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对新GPU架构优化内核的复杂性和资源稀缺性，提出自动化方法以降低对专家知识的依赖。

Method: 采用LLM驱动的多阶段进化过程，包括选择代码版本、生成优化假设、自主执行实验并评估性能。

Result: 由于数据保密，未提供定量结果，但展示了架构设计和工作流程，证明了LLM在优化中的潜力。

Conclusion: LLM驱动的代理可以民主化和加速GPU内核优化，尤其在资源有限或硬件快速演化的环境中。

Abstract: Optimizing GPU kernels for high performance is a complex task, often
demanding deep architectural knowledge, extensive profiling, and iterative
experimentation. This challenge is amplified when targeting newer or
less-documented GPU architectures where traditional development aids are
scarce. This paper introduces an LLM-powered "GPU Kernel Scientist," an
automated methodology for iteratively refining accelerator kernels.
  Our methodology employs LLMs in a multi-stage, evolutionary process: (a)
strategically selecting promising prior code versions as a basis for new
iterations; (b) generating hypotheses for optimization experiments, based on
existing code and assimilated knowledge from general GPU literature; and (c)
autonomously implementing these experiments through code modification and
subsequent submission to an external evaluation system, using only observed
timing data as performance feedback. We detail how this approach navigates the
challenges of the AMD MI300 target architecture and leverages LLMs to
compensate for limited domain-specific human expertise.
  Since quantitative results from an ongoing performance competition were
embargoed on paper submission date, we present the architectural design,
operational workflow, and qualitative insights, highlighting the potential of
LLM-driven agents to democratise and accelerate GPU kernel optimization,
especially in resource-constrained or rapidly evolving hardware environments.

</details>


### [52] [FINN-GL: Generalized Mixed-Precision Extensions for FPGA-Accelerated LSTMs](https://arxiv.org/abs/2506.20810)
*Shashwat Khandelwal,Jakoba Petri-Koenig,Thomas B. Preußer,Michaela Blott,Shreejith Shanker*

Key words: LSTM, FPGA, FINN, 量化, 时间序列

TL;DR: 本文提出了一种基于FINN框架的方法，用于在FPGA上高效部署LSTM网络，解决了现有工具主要针对前馈网络的局限性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: LSTM在时间序列任务中表现优异，但计算复杂度高，难以在资源受限环境中实时部署。FPGA虽为高效加速提供可能，但现有工具缺乏对LSTM的通用支持。

Method: 利用FINN框架和ONNX的Scan操作符建模LSTM计算，引入量化支持，并通过FINN编译器生成硬件块。

Result: 在股票预测任务中验证了方法的有效性，生成的量化ConvLSTM加速器在性能、资源消耗和推理精度上取得平衡。

Conclusion: 提出的通用流程为FPGA上的高效RNN加速器设计奠定了基础。

Abstract: Recurrent neural networks (RNNs), particularly LSTMs, are effective for
time-series tasks like sentiment analysis and short-term stock prediction.
However, their computational complexity poses challenges for real-time
deployment in resource constrained environments. While FPGAs offer a promising
platform for energy-efficient AI acceleration, existing tools mainly target
feed-forward networks, and LSTM acceleration typically requires full custom
implementation. In this paper, we address this gap by leveraging the
open-source and extensible FINN framework to enable the generalized deployment
of LSTMs on FPGAs. Specifically, we leverage the Scan operator from the Open
Neural Network Exchange (ONNX) specification to model the recurrent nature of
LSTM computations, enabling support for mixed quantisation within them and
functional verification of LSTM-based models. Furthermore, we introduce custom
transformations within the FINN compiler to map the quantised ONNX computation
graph to hardware blocks from the HLS kernel library of the FINN compiler and
Vitis HLS. We validate the proposed tool-flow by training a quantised ConvLSTM
model for a mid-price stock prediction task using the widely used dataset and
generating a corresponding hardware IP of the model using our flow, targeting
the XCZU7EV device. We show that the generated quantised ConvLSTM accelerator
through our flow achieves a balance between performance (latency) and resource
consumption, while matching (or bettering) inference accuracy of
state-of-the-art models with reduced precision. We believe that the
generalisable nature of the proposed flow will pave the way for
resource-efficient RNN accelerator designs on FPGAs.

</details>


### [53] [Divide, Specialize, and Route: A New Approach to Efficient Ensemble Learning](https://arxiv.org/abs/2506.20814)
*Jakub Piwko,Jędrzej Ruciński,Dawid Płudowski,Antoni Zajko,Patryzja Żak,Mateusz Zacharecki,Anna Kozak,Katarzyna Woźnica*

Key words: 集成学习, 二进制分类, 数据集复杂性, 动态分配, 路由模型

TL;DR: Hellsemble 是一种新颖且可解释的集成学习框架，通过动态分配数据子集和基于难度的路由机制，提升了分类性能并保持了计算效率和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统集成学习方法存在计算成本高和对异构数据适应性差的问题，Hellsemble 旨在解决这些局限性。

Method: Hellsemble 通过逐步将误分类实例传递给后续模型，将数据集划分为不同难度的子集，并训练专用基学习器。路由模型根据推断的难度将新实例分配给最适合的基模型。

Result: 在 OpenML-CC18 和 Tabzilla 基准测试中，Hellsemble 通常优于传统集成方法。

Conclusion: Hellsemble 展示了基于实例难度构建高效、鲁棒集成系统的潜力。

Abstract: Ensemble learning has proven effective in boosting predictive performance,
but traditional methods such as bagging, boosting, and dynamic ensemble
selection (DES) suffer from high computational cost and limited adaptability to
heterogeneous data distributions. To address these limitations, we propose
Hellsemble, a novel and interpretable ensemble framework for binary
classification that leverages dataset complexity during both training and
inference. Hellsemble incrementally partitions the dataset into circles of
difficulty by iteratively passing misclassified instances from simpler models
to subsequent ones, forming a committee of specialised base learners. Each
model is trained on increasingly challenging subsets, while a separate router
model learns to assign new instances to the most suitable base model based on
inferred difficulty. Hellsemble achieves strong classification accuracy while
maintaining computational efficiency and interpretability. Experimental results
on OpenML-CC18 and Tabzilla benchmarks demonstrate that Hellsemble often
outperforms classical ensemble methods. Our findings suggest that embracing
instance-level difficulty offers a promising direction for constructing
efficient and robust ensemble systems.

</details>


### [54] [Universal and Efficient Detection of Adversarial Data through Nonuniform Impact on Network Layers](https://arxiv.org/abs/2506.20816)
*Furkan Mumcu,Yasin Yilmaz*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Deep Neural Networks (DNNs) are notoriously vulnerable to adversarial input
designs with limited noise budgets. While numerous successful attacks with
subtle modifications to original input have been proposed, defense techniques
against these attacks are relatively understudied. Existing defense approaches
either focus on improving DNN robustness by negating the effects of
perturbations or use a secondary model to detect adversarial data. Although
equally important, the attack detection approach, which is studied in this
work, provides a more practical defense compared to the robustness approach. We
show that the existing detection methods are either ineffective against the
state-of-the-art attack techniques or computationally inefficient for real-time
processing. We propose a novel universal and efficient method to detect
adversarial examples by analyzing the varying degrees of impact of attacks on
different DNN layers. {Our method trains a lightweight regression model that
predicts deeper-layer features from early-layer features, and uses the
prediction error to detect adversarial samples.} Through theoretical arguments
and extensive experiments, we demonstrate that our detection method is highly
effective, computationally efficient for real-time processing, compatible with
any DNN architecture, and applicable across different domains, such as image,
video, and audio.

</details>


### [55] [Demystifying Distributed Training of Graph Neural Networks for Link Prediction](https://arxiv.org/abs/2506.20818)
*Xin Huang,Chul-Ho Lee*

Key words: 图神经网络,分布式训练,链接预测,图稀疏化

TL;DR: 本文研究了分布式图神经网络（GNN）在链接预测中的性能下降问题，并提出了一种基于图稀疏化的解决方案SpLPG，显著降低了通信开销并保持了预测精度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 分布式GNN框架在链接预测中的性能表现未被充分研究，尤其是由于子图分割和信息损失导致的性能下降问题。

Method: 通过分析性能下降的原因（信息损失和负采样方式），提出SpLPG方法，利用图稀疏化减少通信成本。

Result: 实验显示，SpLPG在多个真实数据集上显著减少通信开销（高达80%），同时基本保持链接预测精度。

Conclusion: SpLPG是一种高效的分布式GNN训练方案，有效解决了性能下降问题。

Abstract: Graph neural networks (GNNs) are powerful tools for solving graph-related
problems. Distributed GNN frameworks and systems enhance the scalability of
GNNs and accelerate model training, yet most are optimized for node
classification. Their performance on link prediction remains underexplored.
This paper demystifies distributed training of GNNs for link prediction by
investigating the issue of performance degradation when each worker trains a
GNN on its assigned partitioned subgraph without having access to the entire
graph. We discover that the main sources of the issue come from not only the
information loss caused by graph partitioning but also the ways of drawing
negative samples during model training. While sharing the complete graph
information with each worker resolves the issue and preserves link prediction
accuracy, it incurs a high communication cost. We propose SpLPG, which
effectively leverages graph sparsification to mitigate the issue of performance
degradation at a reduced communication cost. Experiment results on several
public real-world datasets demonstrate the effectiveness of SpLPG, which
reduces the communication overhead by up to about 80% while mostly preserving
link prediction accuracy.

</details>


### [56] [Learning-Based Resource Management in Integrated Sensing and Communication Systems](https://arxiv.org/abs/2506.20849)
*Ziyang Lu,M. Cenk Gursoy,Chilukuri K. Mohan,Pramod K. Varshney*

Key words: 集成感知与通信系统，时间分配，约束深度强化学习，资源优化

TL;DR: 论文提出了一种基于约束深度强化学习（CDRL）的方法，用于优化集成感知与通信系统中雷达与通信单元的时间分配问题，以提高目标通信质量。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决集成感知与通信系统中雷达和通信单元的时间分配问题，以实现动态环境下的高效资源分配。

Method: 采用约束深度强化学习（CDRL）方法，优化跟踪目标和数据传输的时间分配。

Result: 数值结果表明，所提出的CDRL框架能够在动态环境中最大化通信质量，同时满足时间约束。

Conclusion: CDRL方法有效提升了集成感知与通信系统的性能，实现了通信质量的优化。

Abstract: In this paper, we tackle the task of adaptive time allocation in integrated
sensing and communication systems equipped with radar and communication units.
The dual-functional radar-communication system's task involves allocating dwell
times for tracking multiple targets and utilizing the remaining time for data
transmission towards estimated target locations. We introduce a novel
constrained deep reinforcement learning (CDRL) approach, designed to optimize
resource allocation between tracking and communication under time budget
constraints, thereby enhancing target communication quality. Our numerical
results demonstrate the efficiency of our proposed CDRL framework, confirming
its ability to maximize communication quality in highly dynamic environments
while adhering to time constraints.

</details>


### [57] [Multi-Objective Reinforcement Learning for Cognitive Radar Resource Management](https://arxiv.org/abs/2506.20853)
*Ziyang Lu,Subodh Kalia,M. Cenk Gursoy,Chilukuri K. Mohan,Pramod K. Varshney*

Key words: 认知雷达,时间分配,深度强化学习,DDPG,SAC,NSGA-II,多目标优化

TL;DR: 该论文研究了多功能认知雷达系统中的时间分配问题，通过深度强化学习（DDPG和SAC算法）寻找帕累托最优解，并证明SAC在稳定性和样本效率上优于DDPG。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决认知雷达系统在动态环境中平衡新目标扫描与已有目标跟踪的时间分配问题。

Method: 采用深度强化学习（DDPG和SAC算法）进行多目标优化，并通过NSGA-II算法估计帕累托前沿上界。

Result: SAC算法在稳定性和样本效率上优于DDPG，且两种算法均能有效适应不同场景。

Conclusion: 该研究为开发更高效、自适应的认知雷达系统提供了基础，能够在动态环境中平衡多个竞争目标。

Abstract: The time allocation problem in multi-function cognitive radar systems focuses
on the trade-off between scanning for newly emerging targets and tracking the
previously detected targets. We formulate this as a multi-objective
optimization problem and employ deep reinforcement learning to find
Pareto-optimal solutions and compare deep deterministic policy gradient (DDPG)
and soft actor-critic (SAC) algorithms. Our results demonstrate the
effectiveness of both algorithms in adapting to various scenarios, with SAC
showing improved stability and sample efficiency compared to DDPG. We further
employ the NSGA-II algorithm to estimate an upper bound on the Pareto front of
the considered problem. This work contributes to the development of more
efficient and adaptive cognitive radar systems capable of balancing multiple
competing objectives in dynamic environments.

</details>


### [58] [Leaner Training, Lower Leakage: Revisiting Memorization in LLM Fine-Tuning with LoRA](https://arxiv.org/abs/2506.20856)
*Fei Wang,Baochun Li*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Memorization in large language models (LLMs) makes them vulnerable to data
extraction attacks. While pre-training memorization has been extensively
studied, fewer works have explored its impact in fine-tuning, particularly for
LoRA fine-tuning, a widely adopted parameter-efficient method.
  In this work, we re-examine memorization in fine-tuning and uncover a
surprising divergence from prior findings across different fine-tuning
strategies. Factors such as model scale and data duplication, which strongly
influence memorization in pre-training and full fine-tuning, do not follow the
same trend in LoRA fine-tuning. Using a more relaxed similarity-based
memorization metric, we demonstrate that LoRA significantly reduces
memorization risks compared to full fine-tuning, while still maintaining strong
task performance.

</details>


### [59] [Omniwise: Predicting GPU Kernels Performance with LLMs](https://arxiv.org/abs/2506.20886)
*Zixian Wang,Cole Ramos,Muhammad A. Awad,Keith Lowery*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In recent years, the rapid advancement of deep neural networks (DNNs) has
revolutionized artificial intelligence, enabling models with unprecedented
capabilities in understanding, generating, and processing complex data. These
powerful architectures have transformed a wide range of downstream
applications, tackling tasks beyond human reach. In this paper, we introduce
Omniwise, the first end-to-end, self-supervised fine-tuning pipeline that
applies large language models (LLMs) to GPU kernel performance prediction--a
novel use case in performance profiling. Omniwise is model-agnostic and
lightweight, achieving strong results even with a small 3B-parameter model. It
can predict key performance metrics, including memory bandwidth, cache hit
rates, GFLOPs, and arithmetic intensity, directly from kernel code without the
need for code execution or profiling tools. Our approach achieves over 90% of
predictions within 10% relative error on GPU kernels executed on AMD MI250 and
MI300X architectures. In addition to the pipeline, we develop an online
inference server and a Visual Studio Code plugin that seamlessly integrate
LLM-based performance prediction into developers' workflows.

</details>


### [60] [On the Necessity of Output Distribution Reweighting for Effective Class Unlearning](https://arxiv.org/abs/2506.20893)
*Yian Wang,Ali Ebrahimpour-Boroojeny,Hari Sundaram*

Key words: 机器学习遗忘, 输出重加权, MIA-NN攻击, 总变差距离

TL;DR: 提出了一种名为RWFT的轻量级遗忘方法，通过输出重加权实现无需完全重新训练即可从分类器中遗忘特定类别，解决了现有遗忘方法的不足。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为执行用户删除权利和减少有害或有偏见预测，需从训练模型中遗忘特定类别，现有方法无法完全模拟重新训练模型的行为且成本高。

Method: 提出RWFT方法，通过简单重分配遗忘类别的概率质量，抵抗MIA-NN攻击，并引入基于总变差距离的新指标评估残留泄漏。

Result: 实验表明，RWFT在现有指标和新TV指标上均优于现有方法，分别提升2.79%和111.45%。

Conclusion: RWFT是一种高效且安全的遗忘方法，能完全匹配重新训练的行为，同时在安全性和性能上显著优于现有技术。

Abstract: In this work, we introduce an output-reweighting unlearning method, RWFT, a
lightweight technique that erases an entire class from a trained classifier
without full retraining. Forgetting specific classes from trained models is
essential for enforcing user deletion rights and mitigating harmful or biased
predictions. The full retraining is costly and existing unlearning methods fail
to replicate the behavior of the retrained models when predicting samples from
the unlearned class. We prove this failure by designing a variant of membership
inference attacks, MIA-NN that successfully reveals the unlearned class for any
of these methods. We propose a simple redistribution of the probability mass
for the prediction on the samples in the forgotten class which is robust to
MIA-NN. We also introduce a new metric based on the total variation (TV)
distance of the prediction probabilities to quantify residual leakage to
prevent future methods from susceptibility to the new attack. Through extensive
experiments with state of the art baselines in machine unlearning, we show that
our approach matches the results of full retraining in both metrics used for
evaluation by prior work and the new metric we propose in this work. Compare to
state-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%
in our new TV-based metric over the best existing method.

</details>


### [61] [Graph-Structured Feedback Multimodel Ensemble Online Conformal Prediction](https://arxiv.org/abs/2506.20898)
*Erfan Hajihashemi,Yanning Shen*

Key words: 在线共形预测, 多模型选择, 二分图反馈, 预测集大小

TL;DR: 提出一种新的多模型在线共形预测算法，通过动态选择有效模型子集，降低计算复杂度和预测集大小。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决多模型在线共形预测中模型选择带来的计算复杂度和性能下降问题。

Method: 利用二分图反馈动态选择有效模型子集，并结合预测集大小和模型损失作为反馈。

Result: 实验验证算法能构建更小的预测集，优于现有方法。

Conclusion: 算法确保有效覆盖，减少计算复杂度，适用于分布变化场景。

Abstract: Online conformal prediction has demonstrated its capability to construct a
prediction set for each incoming data point that covers the true label with a
predetermined probability. To cope with potential distribution shift,
multi-model online conformal prediction has been introduced to select and
leverage different models from a preselected candidate set. Along with the
improved flexibility, the choice of the preselected set also brings challenges.
A candidate set that includes a large number of models may increase the
computational complexity. In addition, the inclusion of irrelevant models with
poor performance may negatively impact the performance and lead to
unnecessarily large prediction sets. To address these challenges, we propose a
novel multi-model online conformal prediction algorithm that identifies a
subset of effective models at each time step by collecting feedback from a
bipartite graph, which is refined upon receiving new data. A model is then
selected from this subset to construct the prediction set, resulting in reduced
computational complexity and smaller prediction sets. Additionally, we
demonstrate that using prediction set size as feedback, alongside model loss,
can significantly improve efficiency by constructing smaller prediction sets
while still satisfying the required coverage guarantee. The proposed algorithms
are proven to ensure valid coverage and achieve sublinear regret. Experiments
on real and synthetic datasets validate that the proposed methods construct
smaller prediction sets and outperform existing multi-model online conformal
prediction approaches.

</details>


### [62] [Optimal Single-Policy Sample Complexity and Transient Coverage for Average-Reward Offline RL](https://arxiv.org/abs/2506.20904)
*Matthew Zurek,Guy Zamir,Yudong Chen*

Key words: 离线强化学习, 平均奖励MDP, 分布偏移, 非均匀覆盖率, 单策略复杂性度量

TL;DR: 该论文研究了平均奖励MDP中的离线强化学习，提出了一种仅依赖于目标策略的复杂性度量方法，并开发了一种新的算法，解决了传统方法中的覆盖率和结构性假设限制问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究离线强化学习在平均奖励MDP中的挑战，特别是分布偏移和非均匀覆盖率问题，填补理论研究的空白。

Method: 提出一种基于悲观折扣价值迭代的算法，结合新颖的分位数裁剪技术，使用基于经验跨度的惩罚函数。

Result: 首次实现了完全单策略样本复杂度的边界，适用于一般弱通信MDP，无需先验参数知识。

Conclusion: 论文证明学习需要覆盖目标策略的稳态分布之外的假设，并提供了与主要结果匹配的下界。

Abstract: We study offline reinforcement learning in average-reward MDPs, which
presents increased challenges from the perspectives of distribution shift and
non-uniform coverage, and has been relatively underexamined from a theoretical
perspective. While previous work obtains performance guarantees under
single-policy data coverage assumptions, such guarantees utilize additional
complexity measures which are uniform over all policies, such as the uniform
mixing time. We develop sharp guarantees depending only on the target policy,
specifically the bias span and a novel policy hitting radius, yielding the
first fully single-policy sample complexity bound for average-reward offline
RL. We are also the first to handle general weakly communicating MDPs,
contrasting restrictive structural assumptions made in prior work. To achieve
this, we introduce an algorithm based on pessimistic discounted value iteration
enhanced by a novel quantile clipping technique, which enables the use of a
sharper empirical-span-based penalty function. Our algorithm also does not
require any prior parameter knowledge for its implementation. Remarkably, we
show via hard examples that learning under our conditions requires coverage
assumptions beyond the stationary distribution of the target policy,
distinguishing single-policy complexity measures from previously examined
cases. We also develop lower bounds nearly matching our main result.

</details>


### [63] [Explainable AI for Radar Resource Management: Modified LIME in Deep Reinforcement Learning](https://arxiv.org/abs/2506.20916)
*Ziyang Lu,M. Cenk Gursoy,Chilukuri K. Mohan,Pramod K. Varshney*

Key words: 深度强化学习、可解释AI、LIME、雷达资源管理

TL;DR: 论文提出将深度学习方法（DL）整合到LIME采样过程中的新方法DL-LIME，以解决LIME忽略特征相关性的问题，并在雷达资源管理中验证了其优越性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了解决LIME方法中忽略特征相关性的问题，同时增强深度强化学习在雷达资源管理中的可解释性。

Method: 提出DL-LIME方法，将深度学习整合到LIME的采样过程中，应用于深度强化学习的雷达资源管理任务。

Result: 实验结果表明，DL-LIME在保真度和任务性能上均优于传统LIME，并揭示了决策中的关键因素。

Conclusion: DL-LIME不仅提高了性能，还提供了决策过程的解释，适用于雷达资源管理等需要可解释性的场景。

Abstract: Deep reinforcement learning has been extensively studied in decision-making
processes and has demonstrated superior performance over conventional
approaches in various fields, including radar resource management (RRM).
However, a notable limitation of neural networks is their ``black box" nature
and recent research work has increasingly focused on explainable AI (XAI)
techniques to describe the rationale behind neural network decisions. One
promising XAI method is local interpretable model-agnostic explanations (LIME).
However, the sampling process in LIME ignores the correlations between
features. In this paper, we propose a modified LIME approach that integrates
deep learning (DL) into the sampling process, which we refer to as DL-LIME. We
employ DL-LIME within deep reinforcement learning for radar resource
management. Numerical results show that DL-LIME outperforms conventional LIME
in terms of both fidelity and task performance, demonstrating superior
performance with both metrics. DL-LIME also provides insights on which factors
are more important in decision making for radar resource management.

</details>


### [64] [LLM-guided Chemical Process Optimization with a Multi-Agent Approach](https://arxiv.org/abs/2506.20921)
*Tong Zeng,Srivathsan Badrinarayanan,Janghoon Ock,Cheng-Kai Lai,Amir Barati Farimani*

Key words: 化学过程优化, 多代理LLM, 自动约束推断, 计算效率

TL;DR: 提出了一个基于多代理LLM的框架，自动推断化学过程的操作约束并指导优化，显著提升了计算效率和性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统优化方法在约束条件不明确时依赖主观启发式，存在效率瓶颈，需要自动化解决方案。

Method: 采用多代理LLM框架（AutoGen），包括约束生成、参数验证、模拟执行和优化指导代理，分两阶段自动推断约束和协同优化。

Result: 在氢化脱烷基化过程中，框架性能与传统方法相当，但计算效率更高（31倍加速），20分钟内收敛。

Conclusion: 该框架在约束条件不明确时极具潜力，尤其适用于新兴过程和改造应用。

Abstract: Chemical process optimization is crucial to maximize production efficiency
and economic performance. Traditional methods, including gradient-based
solvers, evolutionary algorithms, and parameter grid searches, become
impractical when operating constraints are ill-defined or unavailable,
requiring engineers to rely on subjective heuristics to estimate feasible
parameter ranges. To address this constraint definition bottleneck, we present
a multi-agent framework of large language model (LLM) agents that autonomously
infer operating constraints from minimal process descriptions, then
collaboratively guide optimization using the inferred constraints. Our
AutoGen-based agentic framework employs OpenAI's o3 model, with specialized
agents for constraint generation, parameter validation, simulation execution,
and optimization guidance. Through two phases - autonomous constraint
generation using embedded domain knowledge, followed by iterative multi-agent
optimization - the framework eliminates the need for predefined operational
bounds. Validated on the hydrodealkylation process across cost, yield, and
yield-to-cost ratio metrics, the framework demonstrated competitive performance
with conventional optimization methods while achieving better computational
efficiency, requiring fewer iterations to converge. Our approach converged in
under 20 minutes, achieving a 31-fold speedup over grid search. Beyond
computational efficiency, the framework's reasoning-guided search demonstrates
sophisticated process understanding, correctly identifying utility trade-offs,
and applying domain-informed heuristics. This approach shows significant
potential for optimization scenarios where operational constraints are poorly
characterized or unavailable, particularly for emerging processes and retrofit
applications.

</details>


### [65] [Interpretable Representation Learning for Additive Rule Ensembles](https://arxiv.org/abs/2506.20927)
*Shahrzad Behzadimanesh,Pierre Le Bodic,Geoffrey I. Webb,Mario Boley*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Small additive ensembles of symbolic rules offer interpretable prediction
models. Traditionally, these ensembles use rule conditions based on
conjunctions of simple threshold propositions $x \geq t$ on a single input
variable $x$ and threshold $t$, resulting geometrically in axis-parallel
polytopes as decision regions. While this form ensures a high degree of
interpretability for individual rules and can be learned efficiently using the
gradient boosting approach, it relies on having access to a curated set of
expressive and ideally independent input features so that a small ensemble of
axis-parallel regions can describe the target variable well. Absent such
features, reaching sufficient accuracy requires increasing the number and
complexity of individual rules, which diminishes the interpretability of the
model. Here, we extend classical rule ensembles by introducing logical
propositions with learnable sparse linear transformations of input variables,
i.e., propositions of the form $\mathbf{x}^\mathrm{T}\mathbf{w} \geq t$, where
$\mathbf{w}$ is a learnable sparse weight vector, enabling decision regions as
general polytopes with oblique faces. We propose a learning method using
sequential greedy optimization based on an iteratively reweighted formulation
of logistic regression. Experimental results demonstrate that the proposed
method efficiently constructs rule ensembles with the same test risk as
state-of-the-art methods while significantly reducing model complexity across
ten benchmark datasets.

</details>


### [66] [Model State Arithmetic for Machine Unlearning](https://arxiv.org/abs/2506.20941)
*Keivan Rezaei,Mehrdad Saberi,Abhilasha Ravichander,Soheil Feizi*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models are trained on massive corpora of web data, which may
include private data, copyrighted material, factually inaccurate data, or data
that degrades model performance. Eliminating the influence of such problematic
datapoints through complete retraining -- by repeatedly pretraining the model
on datasets that exclude these specific instances -- is computationally
prohibitive. For this reason, unlearning algorithms have emerged that aim to
eliminate the influence of particular datapoints, while otherwise preserving
the model -- at a low computational cost. However, precisely estimating and
undoing the influence of individual datapoints has proved to be challenging. In
this work, we propose a new algorithm, MSA, for estimating and undoing the
influence of datapoints -- by leveraging model checkpoints i.e. artifacts
capturing model states at different stages of pretraining. Our experimental
results demonstrate that MSA consistently outperforms existing machine
unlearning algorithms across multiple benchmarks, models, and evaluation
metrics, suggesting that MSA could be an effective approach towards more
flexible large language models that are capable of data erasure.

</details>


### [67] [Antibody Design and Optimization with Multi-scale Equivariant Graph Diffusion Models for Accurate Complex Antigen Binding](https://arxiv.org/abs/2506.20957)
*Jiameng Chen,Xiantao Cai,Jia Wu,Wenbin Hu*

Key words: 抗体设计, 多尺度等变图扩散, 几何深度学习, E(3)-等变扩散

TL;DR: AbMEGD是一个通过多尺度等变图扩散进行抗体序列与结构协同设计的端到端框架，解决了当前方法在几何特征捕获和抗原接口泛化上的不足。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的计算抗体设计方法在几何特征捕获和抗原接口泛化方面存在局限性，无法准确捕捉分子相互作用和保持结构完整性。因此，提出了AbMEGD来应对这些挑战。

Method: AbMEGD结合了多尺度等变图扩散方法，整合原子级几何特征与残基级嵌入，利用E(3)-等变扩散方法确保几何精度和计算效率。

Result: 实验结果显示，AbMEGD在氨基酸恢复率、改善百分比和关键CDR-H3区域的均方根偏差上均优于DiffAb模型。

Conclusion: AbMEGD在保持结构完整性的同时提升了功能性能，为抗体序列与结构协同设计及亲和力优化设立了新标准。

Abstract: Antibody design remains a critical challenge in therapeutic and diagnostic
development, particularly for complex antigens with diverse binding interfaces.
Current computational methods face two main limitations: (1) capturing
geometric features while preserving symmetries, and (2) generalizing novel
antigen interfaces. Despite recent advancements, these methods often fail to
accurately capture molecular interactions and maintain structural integrity. To
address these challenges, we propose \textbf{AbMEGD}, an end-to-end framework
integrating \textbf{M}ulti-scale \textbf{E}quivariant \textbf{G}raph
\textbf{D}iffusion for antibody sequence and structure co-design. Leveraging
advanced geometric deep learning, AbMEGD combines atomic-level geometric
features with residue-level embeddings, capturing local atomic details and
global sequence-structure interactions. Its E(3)-equivariant diffusion method
ensures geometric precision, computational efficiency, and robust
generalizability for complex antigens. Furthermore, experiments using the
SAbDab database demonstrate a 10.13\% increase in amino acid recovery, 3.32\%
rise in improvement percentage, and a 0.062~\AA\ reduction in root mean square
deviation within the critical CDR-H3 region compared to DiffAb, a leading
antibody design model. These results highlight AbMEGD's ability to balance
structural integrity with improved functionality, establishing a new benchmark
for sequence-structure co-design and affinity optimization. The code is
available at: https://github.com/Patrick221215/AbMEGD.

</details>


### [68] [SharpZO: Hybrid Sharpness-Aware Vision Language Model Prompt Tuning via Forward-Only Passes](https://arxiv.org/abs/2506.20990)
*Yifan Yang,Zhen Zhang,Rupak Vignesh Swaminathan,Jing Liu,Nathan Susanj,Zheng Zhang*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Fine-tuning vision language models (VLMs) has achieved remarkable performance
across various downstream tasks; yet, it requires access to model gradients
through backpropagation (BP), making them unsuitable for memory-constrained,
inference-only edge devices. To address this limitation, previous work has
explored various BP-free fine-tuning methods. However, these approaches often
rely on high-variance evolutionary strategies (ES) or zeroth-order (ZO)
optimization, and often fail to achieve satisfactory performance. In this
paper, we propose a hybrid Sharpness-aware Zeroth-order optimization (SharpZO)
approach, specifically designed to enhance the performance of ZO VLM
fine-tuning via a sharpness-aware warm-up training. SharpZO features a
two-stage optimization process: a sharpness-aware ES stage that globally
explores and smooths the loss landscape to construct a strong initialization,
followed by a fine-grained local search via sparse ZO optimization. The entire
optimization relies solely on forward passes. Detailed theoretical analysis and
extensive experiments on CLIP models demonstrate that SharpZO significantly
improves accuracy and convergence speed, achieving up to 7% average gain over
state-of-the-art forward-only methods.

</details>


### [69] [Distilling Normalizing Flows](https://arxiv.org/abs/2506.21003)
*Steven Walton,Valeriy Klyukin,Maksim Artemev,Denis Derkach,Nikita Orlov,Humphrey Shi*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Explicit density learners are becoming an increasingly popular technique for
generative models because of their ability to better model probability
distributions. They have advantages over Generative Adversarial Networks due to
their ability to perform density estimation and having exact latent-variable
inference. This has many advantages, including: being able to simply
interpolate, calculate sample likelihood, and analyze the probability
distribution. The downside of these models is that they are often more
difficult to train and have lower sampling quality.
  Normalizing flows are explicit density models, that use composable bijective
functions to turn an intractable probability function into a tractable one. In
this work, we present novel knowledge distillation techniques to increase
sampling quality and density estimation of smaller student normalizing flows.
We seek to study the capacity of knowledge distillation in Compositional
Normalizing Flows to understand the benefits and weaknesses provided by these
architectures. Normalizing flows have unique properties that allow for a
non-traditional forms of knowledge transfer, where we can transfer that
knowledge within intermediate layers. We find that through this distillation,
we can make students significantly smaller while making substantial performance
gains over a non-distilled student. With smaller models there is a
proportionally increased throughput as this is dependent upon the number of
bijectors, and thus parameters, in the network.

</details>


### [70] [TRIDENT: Tri-Modal Molecular Representation Learning with Taxonomic Annotations and Local Correspondence](https://arxiv.org/abs/2506.21028)
*Feng Jiang,Mangal Prakash,Hehuan Ma,Jianyuan Deng,Yuzhi Guo,Amina Mollaysa,Tommaso Mansi,Rui Liao,Junzhou Huang*

Key words: 分子属性预测, 多模态学习, 表示学习, TRIDENT, SMILES

TL;DR: TRIDENT框架通过整合分子SMILES、文本描述和分类功能注释，提出了一种新颖的多模态学习方法，用于分子表示学习，并在11个下游任务中取得领先性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 以往的研究多忽视了分子的文本和分类信息对表示学习的贡献，本文旨在填补这一空白。

Method: TRIDENT采用基于体积的对齐目标和局部对齐目标，结合动量机制，实现全局和局部对齐。

Result: TRIDENT在11个下游任务中表现优异，验证了多模态信息的价值。

Conclusion: 结合SMILES、文本和分类信息可显著提升分子属性预测性能。

Abstract: Molecular property prediction aims to learn representations that map chemical
structures to functional properties. While multimodal learning has emerged as a
powerful paradigm to learn molecular representations, prior works have largely
overlooked textual and taxonomic information of molecules for representation
learning. We introduce TRIDENT, a novel framework that integrates molecular
SMILES, textual descriptions, and taxonomic functional annotations to learn
rich molecular representations. To achieve this, we curate a comprehensive
dataset of molecule-text pairs with structured, multi-level functional
annotations. Instead of relying on conventional contrastive loss, TRIDENT
employs a volume-based alignment objective to jointly align tri-modal features
at the global level, enabling soft, geometry-aware alignment across modalities.
Additionally, TRIDENT introduces a novel local alignment objective that
captures detailed relationships between molecular substructures and their
corresponding sub-textual descriptions. A momentum-based mechanism dynamically
balances global and local alignment, enabling the model to learn both broad
functional semantics and fine-grained structure-function mappings. TRIDENT
achieves state-of-the-art performance on 11 downstream tasks, demonstrating the
value of combining SMILES, textual, and taxonomic functional annotations for
molecular property prediction.

</details>


### [71] [Little By Little: Continual Learning via Self-Activated Sparse Mixture-of-Rank Adaptive Learning](https://arxiv.org/abs/2506.21035)
*Haodong Lu,Chongyang Zhao,Jason Xue,Lina Yao,Kristen Moore,Dong Gong*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Continual learning (CL) with large pre-trained models is challenged by
catastrophic forgetting and task interference. Existing LoRA-based
Mixture-of-Experts (MoE) approaches mitigate forgetting by assigning and
freezing task-specific adapters, but suffer from interference, redundancy, and
ambiguous routing due to coarse adapter-level selection. However, this design
introduces three key challenges: 1) Interference: Activating full LoRA experts
per input leads to subspace interference and prevents selective reuse of useful
components across tasks. 2) Redundancy: Newly added experts often duplicate or
contradict existing knowledge due to unnecessary activation of unrelated ranks
and insufficient reuse of relevant ones. 3) Ambiguity: Overlapping features
across tasks confuse the router, resulting in unstable expert assignments. As
more experts accumulate, earlier task routing degrades, accelerating
forgetting. We propose MoRA, a Mixture-of-Rank Adaptive learning approach with
self-activated and sparse rank activation for CL. Unlike mixing multiple
low-rank matrices, MoRA decomposes each rank-r update into r rank-1 components,
each treated as an independent expert, enabling fine-grained mixture of rank-1
expert utilization while mitigating interference and redundancy. To avoid
ambiguous routing, we propose that each rank-1 expert can infer its own
relevance via intermediate activations. Coupled with our proposed rank pruning
and activation budgets, MoRA adaptively selects a sparse mixture of ranks per
input. We validate MoRA on continual learning tasks with CLIP and large
language models (LLMs), analyzing both in-domain learning and out-of-domain
forgetting/generalization during fine-tuning. MoRA shows significant
effectiveness on enhancing CL with PTMs, and improving generalization while
mitigating forgetting.

</details>


### [72] [An Information-Theoretic Analysis for Federated Learning under Concept Drift](https://arxiv.org/abs/2506.21036)
*Fu Peng,Meng Zhang,Ming Tang*

Key words: 联邦学习, 概念漂移, 信息理论, KL散度, 互信息

TL;DR: 本文通过信息理论分析联邦学习（FL）在概念漂移下的性能，提出一种算法以减少性能下降，并通过实验验证其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 真实世界的数据往往是动态变化的，静态数据集训练的联邦学习模型在数据分布变化时性能下降，需要一种方法来适应这种概念漂移。

Method: 将概念漂移建模为马尔可夫链，提出“平稳泛化误差”评估模型对未来数据的适应能力，并基于KL散度和互信息设计正则化算法。

Result: 提出的算法在三种漂移模式（周期性、渐进性和随机性）下均优于现有方法，实验验证了理论分析的正确性。

Conclusion: 通过KL散度和互信息正则化的方法能有效提升联邦学习在概念漂移下的长期性能。

Abstract: Recent studies in federated learning (FL) commonly train models on static
datasets. However, real-world data often arrives as streams with shifting
distributions, causing performance degradation known as concept drift. This
paper analyzes FL performance under concept drift using information theory and
proposes an algorithm to mitigate the performance degradation. We model concept
drift as a Markov chain and introduce the \emph{Stationary Generalization
Error} to assess a model's capability to capture characteristics of future
unseen data. Its upper bound is derived using KL divergence and mutual
information. We study three drift patterns (periodic, gradual, and random) and
their impact on FL performance. Inspired by this, we propose an algorithm that
regularizes the empirical risk minimization approach with KL divergence and
mutual information, thereby enhancing long-term performance. We also explore
the performance-cost tradeoff by identifying a Pareto front. To validate our
approach, we build an FL testbed using Raspberry Pi4 devices. Experimental
results corroborate with theoretical findings, confirming that drift patterns
significantly affect performance. Our method consistently outperforms existing
approaches for these three patterns, demonstrating its effectiveness in
adapting concept drift in FL.

</details>


### [73] [RL-Selector: Reinforcement Learning-Guided Data Selection via Redundancy Assessment](https://arxiv.org/abs/2506.21037)
*Suorong Yang,Peijia Li,Furao Shen,Jian Zhao*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Modern deep architectures often rely on large-scale datasets, but training on
these datasets incurs high computational and storage overhead. Real-world
datasets often contain substantial redundancies, prompting the need for more
data-efficient training paradigms. Data selection has shown promise to mitigate
redundancy by identifying the most representative samples, thereby reducing
training costs without compromising performance. Existing methods typically
rely on static scoring metrics or pretrained models, overlooking the combined
effect of selected samples and their evolving dynamics during training. We
introduce the concept of epsilon-sample cover, which quantifies sample
redundancy based on inter-sample relationships, capturing the intrinsic
structure of the dataset. Based on this, we reformulate data selection as a
reinforcement learning (RL) process and propose RL-Selector, where a
lightweight RL agent optimizes the selection policy by leveraging
epsilon-sample cover derived from evolving dataset distribution as a reward
signal. Extensive experiments across benchmark datasets and diverse
architectures demonstrate that our method consistently outperforms existing
state-of-the-art baselines. Models trained with our selected datasets show
enhanced generalization performance with improved training efficiency.

</details>


### [74] [Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement Learning](https://arxiv.org/abs/2506.21039)
*Jaebak Hwang,Sanghyeon Lee,Jeongmo Kim,Seungyul Han*

Key words: 强化学习, 长时序目标, 层次强化学习, 子目标

TL;DR: 论文提出了一种基于图的层次强化学习框架SSE，通过强制单步子目标可达性和分离式探索策略改进长时序目标任务的效率与成功率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决长时序目标条件下强化学习中因目标遥远和奖励稀疏导致的挑战。

Method: 引入Strict Subgoal Execution (SSE)，通过结构约束高层决策和分离式探索策略增强可探索性，并采用失败感知路径优化动态调整子目标可靠性。

Result: 在多样化的长时序基准测试中，SSE在效率和成功率上均优于现有方法。

Conclusion: SSE为长时序目标任务提供了一种高效可靠的解决方案。

Abstract: Long-horizon goal-conditioned tasks pose fundamental challenges for
reinforcement learning (RL), particularly when goals are distant and rewards
are sparse. While hierarchical and graph-based methods offer partial solutions,
they often suffer from subgoal infeasibility and inefficient planning. We
introduce Strict Subgoal Execution (SSE), a graph-based hierarchical RL
framework that enforces single-step subgoal reachability by structurally
constraining high-level decision-making. To enhance exploration, SSE employs a
decoupled exploration policy that systematically traverses underexplored
regions of the goal space. Furthermore, a failure-aware path refinement, which
refines graph-based planning by dynamically adjusting edge costs according to
observed low-level success rates, thereby improving subgoal reliability.
Experimental results across diverse long-horizon benchmarks demonstrate that
SSE consistently outperforms existing goal-conditioned RL and hierarchical RL
approaches in both efficiency and success rate.

</details>


### [75] [Efficient Skill Discovery via Regret-Aware Optimization](https://arxiv.org/abs/2506.21044)
*He Zhang,Ming Zhou,Shaopeng Zhai,Ying Sun,Hui Xiong*

Key words: 无监督技能发现、极小极大博弈、遗憾感知、高维环境

TL;DR: 论文提出了一种基于遗憾感知的无监督技能发现方法，通过技能生成与策略学习的极小极大博弈，提升高维环境中的效率和多样性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法在探索多样性上表现良好，但在高维环境中效率较低，因此需要一种更高效的方法来改进技能发现。

Method: 将技能发现建模为技能生成与策略学习的极小极大博弈，利用遗憾值衡量技能强度收敛程度，并通过可学习的技能生成器指导探索。

Result: 实验表明，该方法在高维环境中效率和多样性优于基线，零样本性能提升15%。

Conclusion: 提出的遗憾感知方法在提升技能发现效率与多样性方面表现优异。

Abstract: Unsupervised skill discovery aims to learn diverse and distinguishable
behaviors in open-ended reinforcement learning. For existing methods, they
focus on improving diversity through pure exploration, mutual information
optimization, and learning temporal representation. Despite that they perform
well on exploration, they remain limited in terms of efficiency, especially for
the high-dimensional situations. In this work, we frame skill discovery as a
min-max game of skill generation and policy learning, proposing a regret-aware
method on top of temporal representation learning that expands the discovered
skill space along the direction of upgradable policy strength. The key insight
behind the proposed method is that the skill discovery is adversarial to the
policy learning, i.e., skills with weak strength should be further explored
while less exploration for the skills with converged strength. As an
implementation, we score the degree of strength convergence with regret, and
guide the skill discovery with a learnable skill generator. To avoid
degeneration, skill generation comes from an up-gradable population of skill
generators. We conduct experiments on environments with varying complexities
and dimension sizes. Empirical results show that our method outperforms
baselines in both efficiency and diversity. Moreover, our method achieves a 15%
zero shot improvement in high-dimensional environments, compared to existing
methods.

</details>


### [76] [FedDAA: Dynamic Client Clustering for Concept Drift Adaptation in Federated Learning](https://arxiv.org/abs/2506.21054)
*Fu Peng,Ming Tang*

Key words: 联邦学习, 概念漂移, 动态聚类, 历史知识保留, 模型适应性

TL;DR: 论文提出了一种名为FedDAA的联邦学习框架，用于处理多来源的概念漂移问题，通过动态聚类区分不同类型的漂移并保留有用历史知识，显著提升了模型性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统联邦学习方法主要关注真实漂移，而忽视了虚拟或标签漂移，导致历史知识的丢失和性能下降。

Method: FedDAA框架包含三个模块：聚类数量确定、真实漂移检测和概念漂移适应模块，通过动态聚类区分漂移类型并保留有用信息。

Result: 实验表明FedDAA在Fashion-MNIST、CIFAR-10和CIFAR-100上比现有方法提升了7.84%到8.52%的准确率。

Conclusion: FedDAA能够有效处理多来源的概念漂移，提升联邦学习的性能和适应性。

Abstract: In federated learning (FL), the data distribution of each client may change
over time, introducing both temporal and spatial data heterogeneity, known as
concept drift. Data heterogeneity arises from three drift sources: real drift
(a shift in the conditional distribution P(y|x)), virtual drift (a shift in the
input distribution P(x)), and label drift (a shift in the label distribution
P(y)). However, most existing FL methods addressing concept drift primarily
focus on real drift. When clients experience virtual or label drift, these
methods often fail to selectively retain useful historical knowledge, leading
to catastrophic forgetting. A key challenge lies in distinguishing different
sources of drift, as they require distinct adaptation strategies: real drift
calls for discarding outdated data, while virtual or label drift benefits from
retaining historical data. Without explicitly identifying the drift sources, a
general adaptation strategy is suboptimal and may harm generalization. To
address this challenge, we propose FedDAA, a dynamic clustered FL framework
designed to adapt to multi-source concept drift while preserving valuable
historical knowledge. Specifically, FedDAA integrates three modules: a cluster
number determination module to find the optimal number of clusters; a real
drift detection module to distinguish real drift from virtual/label drift; and
a concept drift adaptation module to adapt to new data while retaining useful
historical information. We provide theoretical convergence guarantees, and
experiments show that FedDAA achieves 7.84% to 8.52% accuracy improvements over
state-of-the-art methods on Fashion-MNIST, CIFAR-10, and CIFAR-100.

</details>


### [77] [Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge Graph](https://arxiv.org/abs/2506.21071)
*Jingwei Wang,Zai Zhang,Hao Qian,Chunjing Gan,Binbin Hu,Ziqi Liu,Zhiqiang Zhang,Jun Zhou,Bin Shi,Bo Dong*

Key words: 大型语言模型、工具使用、知识图谱、指令数据

TL;DR: 提出了一种利用知识图谱生成高质量指令数据的新方法，以提升大型语言模型的工具使用能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大型语言模型（LLMs）在使用工具时面临挑战，现有方法生成的指令数据质量不足，限制其问题解决能力。

Method: 从知识图谱中提取查询路径，转换为用户查询，并将实体关系映射为可操作工具，生成详细的指令数据。

Result: 实验表明，仅需少量合成数据微调即可显著提升LLMs的工具使用能力。

Conclusion: 知识图谱生成的指令数据能高效提升LLMs的工具使用能力。

Abstract: Teaching large language models (LLMs) to use tools is crucial for improving
their problem-solving abilities and expanding their applications. However,
effectively using tools is challenging because it requires a deep understanding
of tool functionalities and user intentions. Previous methods relied mainly on
LLMs to generate instruction data, but the quality of these data was often
insufficient. In this paper, we propose a new method that uses knowledge graphs
to generate high-quality instruction data for LLMs. Knowledge graphs are
manually curated datasets rich in semantic information. We begin by extracting
various query pathways from a given knowledge graph, which are transformed into
a broad spectrum of user queries. We then translate the relationships between
entities into actionable tools and parse the pathways of each query into
detailed solution steps, thereby creating high-quality instruction data. Our
experiments show that fine-tuning on just a small sample of this synthetic data
can significantly improve the tool utilization and overall capabilities of
LLMs.

</details>


### [78] [Chain-of-Thought Enhanced Shallow Transformers for Wireless Symbol Detection](https://arxiv.org/abs/2506.21093)
*Li Fan,Peng Wang,Jing Yang,Cong Shen*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Transformers have shown potential in solving wireless communication problems,
particularly via in-context learning (ICL), where models adapt to new tasks
through prompts without requiring model updates. However, prior ICL-based
Transformer models rely on deep architectures with many layers to achieve
satisfactory performance, resulting in substantial storage and computational
costs. In this work, we propose CHain Of thOught Symbol dEtection (CHOOSE), a
CoT-enhanced shallow Transformer framework for wireless symbol detection. By
introducing autoregressive latent reasoning steps within the hidden space,
CHOOSE significantly improves the reasoning capacity of shallow models (1-2
layers) without increasing model depth. This design enables lightweight
Transformers to achieve detection performance comparable to much deeper models,
making them well-suited for deployment on resource-constrained mobile devices.
Experimental results demonstrate that our approach outperforms conventional
shallow Transformers and achieves performance comparable to that of deep
Transformers, while maintaining storage and computational efficiency. This
represents a promising direction for implementing Transformer-based algorithms
in wireless receivers with limited computational resources.

</details>


### [79] [FeDa4Fair: Client-Level Federated Datasets for Fairness Evaluation](https://arxiv.org/abs/2506.21095)
*Xenia Heilmann,Luca Corbucci,Mattia Cerrato,Anna Monreale*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Federated Learning (FL) enables collaborative model training across multiple
clients without sharing clients' private data. However, fairness remains a key
concern, as biases in local clients' datasets can impact the entire federated
system. Heterogeneous data distributions across clients may lead to models that
are fairer for some clients than others. Although several fairness-enhancing
solutions are present in the literature, most focus on mitigating bias for a
single sensitive attribute, typically binary, overlooking the diverse and
sometimes conflicting fairness needs of different clients. This limited
perspective can limit the effectiveness of fairness interventions for the
different clients. To support more robust and reproducible fairness research in
FL, we aim to enable a consistent benchmarking of fairness-aware FL methods at
both the global and client levels. In this paper, we contribute in three ways:
(1) We introduce FeDa4Fair, a library to generate tabular datasets tailored to
evaluating fair FL methods under heterogeneous client bias; (2) we release four
bias-heterogeneous datasets and corresponding benchmarks to compare fairness
mitigation methods in a controlled environment; (3) we provide ready-to-use
functions for evaluating fairness outcomes for these datasets.

</details>


### [80] [Learning to Skip the Middle Layers of Transformers](https://arxiv.org/abs/2506.21103)
*Tim Lawson,Laurence Aitchison*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Conditional computation is a popular strategy to make Transformers more
efficient. Existing methods often target individual modules (e.g.,
mixture-of-experts layers) or skip layers independently of one another.
However, interpretability research has demonstrated that the middle layers of
Transformers exhibit greater redundancy, and that early layers aggregate
information into token positions. Guided by these insights, we propose a novel
architecture that dynamically skips a variable number of layers from the middle
outward. In particular, a learned gating mechanism determines whether to bypass
a symmetric span of central blocks based on the input, and a gated attention
mechanism prevents subsequent tokens from attending to skipped token positions.
Residual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and
gate sparsity with an adaptive regularization loss. We had aimed to reduce
compute requirements for 'simpler' tokens and potentially foster an emergent
multi-level representational hierarchy but, at the scales investigated, our
approach does not achieve improvements in the trade-off between validation
cross-entropy and estimated FLOPs compared to dense baselines with fewer
layers. We release our code at https://github.com/tim-lawson/skip-middle.

</details>


### [81] [Interpretable Hierarchical Concept Reasoning through Attention-Guided Graph Learning](https://arxiv.org/abs/2506.21102)
*David Debot,Pietro Barbiero,Gabriele Dominici,Giuseppe Marra*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Concept-Based Models (CBMs) are a class of deep learning models that provide
interpretability by explaining predictions through high-level concepts. These
models first predict concepts and then use them to perform a downstream task.
However, current CBMs offer interpretability only for the final task
prediction, while the concept predictions themselves are typically made via
black-box neural networks. To address this limitation, we propose Hierarchical
Concept Memory Reasoner (H-CMR), a new CBM that provides interpretability for
both concept and task predictions. H-CMR models relationships between concepts
using a learned directed acyclic graph, where edges represent logic rules that
define concepts in terms of other concepts. During inference, H-CMR employs a
neural attention mechanism to select a subset of these rules, which are then
applied hierarchically to predict all concepts and the final task. Experimental
results demonstrate that H-CMR matches state-of-the-art performance while
enabling strong human interaction through concept and model interventions. The
former can significantly improve accuracy at inference time, while the latter
can enhance data efficiency during training when background knowledge is
available.

</details>


### [82] [Complexity-aware fine-tuning](https://arxiv.org/abs/2506.21220)
*Andrey Goncharov,Daniil Vyazhev,Petr Sychev,Edvard Khalafyan,Alexey Zaytsev*

Key words: 监督微调, 蒸馏, 大语言模型, 高效微调, 熵分类

TL;DR: 提出了一种基于数据熵分类的高效微调方法，显著优于标准SFT，且仅需较少数据即可达到与蒸馏相当的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统监督微调（SFT）和蒸馏方法在高性能需求下成本高昂且数据需求大，因此寻求更高效的微调方案。

Method: 通过单标记回答熵将数据分类，仅对复杂数据使用推理，结合SFT和蒸馏进行微调。

Result: 方法在平均准确率上显著优于标准SFT（0.55 vs 0.43），且数据用量减少62%仍达到蒸馏性能。

Conclusion: 基于熵分类的微调方案高效且成本较低，为后续研究提供了新思路。

Abstract: General-purpose Large Language Models (LLMs) are frequently fine-tuned
through supervised fine-tuning (SFT) to enhance performance in specific
domains. Better results can be achieved by distilling the chain-of-thought of a
larger model at the cost of numerous expensive calls and a much greater amount
of data. We propose a novel blueprint for efficient fine-tuning that uses
reasoning only for complex data identified by entropy. Specifically, across two
small open models ($\approx 3B$) we split the training data into complexity
categories by a single token answer entropy (ROC AUC $0.73$), fine-tune large
language models (LLMs) via SFT and distillation, and show that our pipeline
significantly outperforms the standard SFT approach ($0.55$ vs $0.43$ average
accuracy) and provides comparable with distillation performance while using
$62\%$ less data ($0.55$ average accuracy for both). We publish our code and
data to facilitate further research in this direction.

</details>


### [83] [Unlasting: Unpaired Single-Cell Multi-Perturbation Estimation by Dual Conditional Diffusion Implicit Bridges](https://arxiv.org/abs/2506.21107)
*Changxi Chi,Jun Xia,Yufei Huang,Jingbo Zhou,Siyuan Li,Yunfan Liu,Chang Yu,Stan Z. Li*

Key words: 单细胞扰动、未配对数据、双扩散隐式桥、基因调控网络、掩码机制

TL;DR: 提出了一种基于双扩散隐式桥（DDIB）的框架，解决单细胞扰动数据未配对的挑战，并通过整合基因调控网络（GRN）和掩码机制提升生成质量。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 单细胞测序是破坏性过程，无法获取同一细胞扰动前后的表型，导致数据未配对，现有方法无法有效解决这一问题。

Method: 基于DDIB框架学习数据分布间的映射，整合GRN信息传递扰动信号，并引入掩码机制预测沉默基因。

Result: 提出的方法有效解决了未配对数据问题，提升了生成质量，并引入了更合适的评估指标。

Conclusion: Unlasting框架通过双条件扩散模型改进单细胞扰动数据分析，增强了在GRN指导下对扰动的理解。

Abstract: Estimating single-cell responses across various perturbations facilitates the
identification of key genes and enhances drug screening, significantly boosting
experimental efficiency. However, single-cell sequencing is a destructive
process, making it impossible to capture the same cell's phenotype before and
after perturbation. Consequently, data collected under perturbed and
unperturbed conditions are inherently unpaired. Existing methods either attempt
to forcibly pair unpaired data using random sampling, or neglect the inherent
relationship between unperturbed and perturbed cells during the modeling. In
this work, we propose a framework based on Dual Diffusion Implicit Bridges
(DDIB) to learn the mapping between different data distributions, effectively
addressing the challenge of unpaired data. We further interpret this framework
as a form of data augmentation. We integrate gene regulatory network (GRN)
information to propagate perturbation signals in a biologically meaningful way,
and further incorporate a masking mechanism to predict silent genes, improving
the quality of generated profiles. Moreover, gene expression under the same
perturbation often varies significantly across cells, frequently exhibiting a
bimodal distribution that reflects intrinsic heterogeneity. To capture this, we
introduce a more suitable evaluation metric. We propose Unlasting, dual
conditional diffusion models that overcome the problem of unpaired single-cell
perturbation data and strengthen the model's insight into perturbations under
the guidance of the GRN, with a dedicated mask model designed to improve
generation quality by predicting silent genes. In addition, we introduce a
biologically grounded evaluation metric that better reflects the inherent
heterogeneity in single-cell responses.

</details>


### [84] [DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster](https://arxiv.org/abs/2506.21263)
*Ji Qi,WenPeng Zhu,Li Li,Ming Wu,YingJun Wu,Wu He,Xun Gao,Jason Zeng,Michael Heinrich*

Key words: 去中心化训练,大型语言模型,低通信,Pipeline Parallelism,梯度压缩

TL;DR: DiLoCoX是一种低通信大规模去中心化集群训练框架，结合多种技术实现高效训练1000亿参数模型，在1Gbps网络上显著提升训练速度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决分布式训练中高通信依赖与集中式集群限制，探索如何在慢速网络上高效训练超大规模模型。

Method: 结合Pipeline Parallelism、Dual Optimizer Policy、通信与本地训练的单步延迟重叠及自适应梯度压缩方案。

Result: 在1Gbps网络上成功预训练107B模型，比传统方法快357倍且收敛性能几乎无损。

Conclusion: DiLoCoX是首个适用于1000亿参数模型的去中心化训练框架，展现了慢速网络上的高效潜力。

Abstract: The distributed training of foundation models, particularly large language
models (LLMs), demands a high level of communication. Consequently, it is
highly dependent on a centralized cluster with fast and reliable interconnects.
Can we conduct training on slow networks and thereby unleash the power of
decentralized clusters when dealing with models exceeding 100 billion
parameters? In this paper, we propose DiLoCoX, a low-communication large-scale
decentralized cluster training framework. It combines Pipeline Parallelism with
Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local
Training, and an Adaptive Gradient Compression Scheme. This combination
significantly improves the scale of parameters and the speed of model
pre-training. We justify the benefits of one-step-delay overlap of
communication and local training, as well as the adaptive gradient compression
scheme, through a theoretical analysis of convergence. Empirically, we
demonstrate that DiLoCoX is capable of pre-training a 107B foundation model
over a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x
speedup in distributed training while maintaining negligible degradation in
model convergence. To the best of our knowledge, this is the first
decentralized training framework successfully applied to models with over 100
billion parameters.

</details>


### [85] [Robust Policy Switching for Antifragile Reinforcement Learning for UAV Deconfliction in Adversarial Environments](https://arxiv.org/abs/2506.21127)
*Deepak Kumar Panda,Weisi Guo*

Key words: 抗脆弱RL, 折扣汤普森采样, 无人机导航, 对抗性攻击

TL;DR: 本文提出了一种抗脆弱的强化学习框架，通过引入基于折扣汤普森采样（DTS）的切换机制，动态选择策略以应对对抗性攻击。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 无人机的导航自动化为对抗性攻击提供了可乘之机，现有鲁棒RL方法难以应对分布偏移，因此需要一种更具适应性的框架。

Method: 利用DTS从多样化的鲁棒策略中动态选择，通过多臂老虎机模型优化选择策略，以最小化对抗性导致的分布偏移。

Result: 在复杂导航环境中，该方法表现优于传统鲁棒RL，路径更短且冲突更少。

Conclusion: 抗脆弱RL框架能有效适应未见的对抗性攻击，具有鲁棒性和适应性。

Abstract: The increasing automation of navigation for unmanned aerial vehicles (UAVs)
has exposed them to adversarial attacks that exploit vulnerabilities in
reinforcement learning (RL) through sensor manipulation. Although existing
robust RL methods aim to mitigate such threats, their effectiveness has limited
generalization to out-of-distribution shifts from the optimal value
distribution, as they are primarily designed to handle fixed perturbation. To
address this limitation, this paper introduces an antifragile RL framework that
enhances adaptability to broader distributional shifts by incorporating a
switching mechanism based on discounted Thompson sampling (DTS). This mechanism
dynamically selects among multiple robust policies to minimize adversarially
induced state-action-value distribution shifts. The proposed approach first
derives a diverse ensemble of action robust policies by accounting for a range
of perturbations in the policy space. These policies are then modeled as a
multiarmed bandit (MAB) problem, where DTS optimally selects policies in
response to nonstationary Bernoulli rewards, effectively adapting to evolving
adversarial strategies. Theoretical framework has also been provided where by
optimizing the DTS to minimize the overall regrets due to distributional shift,
results in effective adaptation against unseen adversarial attacks thus
inducing antifragility. Extensive numerical simulations validate the
effectiveness of the proposed framework in complex navigation environments with
multiple dynamic three-dimensional obstacles and with stronger projected
gradient descent (PGD) and spoofing attacks. Compared to conventional robust,
non-adaptive RL methods, the antifragile approach achieves superior
performance, demonstrating shorter navigation path lengths and a higher rate of
conflict-free navigation trajectories compared to existing robust RL techniques

</details>


### [86] [Curriculum-Guided Antifragile Reinforcement Learning for Secure UAV Deconfliction under Observation-Space Attacks](https://arxiv.org/abs/2506.21129)
*Deepak Kumar Panda,Adolfo Perrusquia,Weisi Guo*

Key words: 强化学习，抗脆弱性，对抗攻击，无人机，安全性

TL;DR: 提出了一个抗脆弱的强化学习框架，通过对抗性扰动课程训练RL策略，提高其在观测空间中的鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决RL策略在安全关键系统中对分布外对抗攻击的脆弱性问题。

Method: 提出抗脆弱RL框架，通过逐步增加的对抗性扰动训练，使用Wasserstein距离最小化对齐价值函数。

Result: 在UAV避障任务中，抗脆弱策略在PGD和GPS欺骗攻击下表现优于标准RL基线。

Conclusion: 抗脆弱的强化学习能提高在动态威胁环境中的决策安全性和适应性。

Abstract: Reinforcement learning (RL) policies deployed in safety-critical systems,
such as unmanned aerial vehicle (UAV) navigation in dynamic airspace, are
vulnerable to out-ofdistribution (OOD) adversarial attacks in the observation
space. These attacks induce distributional shifts that significantly degrade
value estimation, leading to unsafe or suboptimal decision making rendering the
existing policy fragile. To address this vulnerability, we propose an
antifragile RL framework designed to adapt against curriculum of incremental
adversarial perturbations. The framework introduces a simulated attacker which
incrementally increases the strength of observation-space perturbations which
enables the RL agent to adapt and generalize across a wider range of OOD
observations and anticipate previously unseen attacks. We begin with a
theoretical characterization of fragility, formally defining catastrophic
forgetting as a monotonic divergence in value function distributions with
increasing perturbation strength. Building on this, we define antifragility as
the boundedness of such value shifts and derive adaptation conditions under
which forgetting is stabilized. Our method enforces these bounds through
iterative expert-guided critic alignment using Wasserstein distance
minimization across incrementally perturbed observations. We empirically
evaluate the approach in a UAV deconfliction scenario involving dynamic 3D
obstacles. Results show that the antifragile policy consistently outperforms
standard and robust RL baselines when subjected to both projected gradient
descent (PGD) and GPS spoofing attacks, achieving up to 15% higher cumulative
reward and over 30% fewer conflict events. These findings demonstrate the
practical and theoretical viability of antifragile reinforcement learning for
secure and resilient decision-making in environments with evolving threat
scenarios.

</details>


### [87] [NaLaFormer: Norm-Aware Linear Attention for Transformer Models](https://arxiv.org/abs/2506.21137)
*Weikang Meng,Yadan Luo,Liangyu Huo,Yaowei Wang,Xin Li,Zheng Zhang*

Key words: 线性注意力，范数感知，动态熵控制，余弦相似性，NaLaFormer

TL;DR: 提出Norm-Aware Linear Attention机制，通过解耦查询和键矩阵为范数和方向两部分，动态控制熵减少并恢复范数分布，提升线性注意力的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决线性注意力中因忽视查询范数和限制负值导致的内积交互缺失及熵差距问题。

Method: 将查询和键矩阵解耦为范数和方向两部分，设计基于查询范数的核函数动态控制熵减少，并通过余弦相似性保持范数一致性。

Result: 实验表明，NaLaFormer在视觉和语言任务中性能提升高达4.2%，同时增强表达效率。

Conclusion: 提出的机制成功恢复范数分布和熵控制，显著提升线性注意力的效率和表达力。

Abstract: Linear attention has emerged as a viable alternative to softmax attention by
reducing complexity from quadratic to linear in sequence length. To preserve
two fundamental properties of softmax, non-negativity and entropy reduction,
current works employ various linearly separatable kernel functions with $L1$
normalization instead of softmax operator. However, query norms are neglected
by the normalization operation in linear attention, such degradation heavily
leads to an entropy gap. Meanwhile, existing works inhibit negative values of
query and key vectors resulting in a missing inner-product interactions after
being mapped. To address these dual challenges, we propose a novel Norm-Aware
Linear Attention mechanism serving to restore norm-guided dynamic spikiness and
recover kernel-perturbed norm distributions. Specifically, we first decouple
query and key matrices into two components: norm and direction, to achieve
norm-aware spikiness control and norm consistency, respectively. We
mathematically reveal that the extent of entropy reduction varies with the
query norm in softmax normalization, motivating a query-norm aware kernel
function for dynamic control over entropy reduction. Furthermore, to ensure
norm consistency and enforce non-negativity constraints, we employ a
norm-preserving mapping to project all elements of the angular matrix into
positive values, leveraging cosine similarity to inhibit dimensions with
opposite directions. We conduct extensive experiments demonstrating that the
NaLaFormer improves performance on vision and language tasks, enhancing both
expressiveness and efficiency by up to 4.2\%.

</details>


### [88] [Latent Prototype Routing: Achieving Near-Perfect Load Balancing in Mixture-of-Experts](https://arxiv.org/abs/2506.21328)
*Jiajie Yang*

Key words: Mixture-of-Experts, 负载均衡, 专家路由, LPR, 聚类

TL;DR: 论文提出了一种新的路由框架Latent Prototype Routing (LPR)，通过聚类视角优化Mixture-of-Experts (MoE)架构中的专家路由，显著改善了负载均衡问题，同时保持了下游性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决现有MoE架构中专家负载严重不均衡的问题，以减少计算资源浪费并提升模型容量利用率。

Method: 提出LPR框架，通过聚类视角重新设计专家路由机制，推广现有方法并促进平衡的专家利用率。

Result: 实验表明，LPR将专家负载的基尼系数从0.70降至0.035，最小-最大专家负载比从1e-6提升至0.70，实现了近乎完美的负载均衡。

Conclusion: LPR是一种有效的路由框架，能够在保持性能的同时显著改善MoE架构中的负载均衡问题。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a key strategy for
scaling large language models (LLMs) efficiently. However, current MoE systems
suffer from severe load imbalance, where only a small subset of experts is
consistently activated during training and inference, leading to significant
underutilization of model capacity and computational resources. In this work,
we revisit expert routing through a clustering perspective and propose Latent
Prototype Routing (LPR), a novel routing framework that generalizes existing
approaches while promoting balanced expert utilization without compromising
downstream performance. Extensive experiments across multiple open-source MoE
models -- including DeepSeek-V3, Qwen3-MoE, and Mixtral -- demonstrate that LPR
reduces the Gini coefficient of expert load from 0.70 to 0.035 on average,
improves the min-max expert load ratio from 1e-6 to 0.70, achieving
near-perfect load balancing.

</details>


### [89] [DBConformer: Dual-Branch Convolutional Transformer for EEG Decoding](https://arxiv.org/abs/2506.21140)
*Ziwei Wang,Hongbin Wang,Tianwang Jia,Xingyi He,Siyang Li,Dongrui Wu*

Key words: EEG解码, 双分支网络, 卷积Transformer, 长时程依赖性, 通道注意力

TL;DR: DBConformer是一种专为EEG解码设计的双分支卷积Transformer网络，能够同时捕捉长时程依赖性和通道间关系，并引入轻量级通道注意力模块优化空间表征。实验表明其在多数据集上优于现有10种基线模型，且参数更少，特征可解释性强。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前基于CNN的EEG解码方法难以捕捉长时程依赖性和全局通道间关系，而现有的CNN-Transformer混合模型多采用串行设计，未能有效整合局部与全局特征，且缺乏显式的通道建模。

Method: 提出DBConformer，包含时间分支和空间分支，分别建模长时程依赖性和通道间交互，并通过轻量级通道注意力模块优化空间表征。

Result: 在5个运动想象数据集和2个癫痫检测数据集上的实验表明，DBConformer性能优于10种基线模型，且参数更少（仅为EEG Conformer的1/8）。可视化结果证实其特征具有生理可解释性。

Conclusion: DBConformer在EEG解码中表现出色，兼具高性能和可解释性，适合稳健且可解释的EEG信号处理。

Abstract: Electroencephalography (EEG)-based brain-computer interfaces (BCIs) transform
spontaneous/evoked neural activity into control commands for external
communication. While convolutional neural networks (CNNs) remain the mainstream
backbone for EEG decoding, their inherently short receptive field makes it
difficult to capture long-range temporal dependencies and global inter-channel
relationships. Recent CNN-Transformer (Conformers) hybrids partially address
this issue, but most adopt a serial design, resulting in suboptimal integration
of local and global features, and often overlook explicit channel-wise
modeling. To address these limitations, we propose DBConformer, a dual-branch
convolutional Transformer network tailored for EEG decoding. It integrates a
temporal Conformer to model long-range temporal dependencies and a spatial
Conformer to extract inter-channel interactions, capturing both temporal
dynamics and spatial patterns in EEG signals. A lightweight channel attention
module further refines spatial representations by assigning data-driven
importance to EEG channels. Extensive experiments on five motor imagery (MI)
datasets and two seizure detection datasets under three evaluation settings
demonstrate that DBConformer consistently outperforms 10 competitive baseline
models, with over eight times fewer parameters than the high-capacity EEG
Conformer baseline. Further, the visualization results confirm that the
features extracted by DBConformer are physiologically interpretable and aligned
with sensorimotor priors in MI. The superior performance and interpretability
of DBConformer make it reliable for robust and explainable EEG decoding. Code
is publicized at https://github.com/wzwvv/DBConformer.

</details>


### [90] [Generative Adversarial Evasion and Out-of-Distribution Detection for UAV Cyber-Attacks](https://arxiv.org/abs/2506.21142)
*Deepak Kumar Panda,Weisi Guo*

Key words: 无人机, 入侵检测系统, 隐式对抗攻击, cGAN, CVAE

TL;DR: 本文提出了一种基于cGAN框架的隐式对抗攻击生成方法，通过CVAE检测此类攻击，与传统方法相比，性能显著提升。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统异常检测方法难以识别新型威胁，且无法有效区分隐式对抗攻击与真实OOD事件。

Method: 利用cGAN生成隐式对抗攻击样本，通过CVAE基于负对数似然检测这些攻击。

Result: CVAE的遗憾分数显著优于传统基于马氏距离的检测器。

Conclusion: 高级概率建模对增强IDS对抗自适应、生成模型驱动的网络入侵能力至关重要。

Abstract: The growing integration of UAVs into civilian airspace underscores the need
for resilient and intelligent intrusion detection systems (IDS), as traditional
anomaly detection methods often fail to identify novel threats. A common
approach treats unfamiliar attacks as out-of-distribution (OOD) samples;
however, this leaves systems vulnerable when mitigation is inadequate.
Moreover, conventional OOD detectors struggle to distinguish stealthy
adversarial attacks from genuine OOD events. This paper introduces a
conditional generative adversarial network (cGAN)-based framework for crafting
stealthy adversarial attacks that evade IDS mechanisms. We first design a
robust multi-class IDS classifier trained on benign UAV telemetry and known
cyber-attacks, including Denial of Service (DoS), false data injection (FDI),
man-in-the-middle (MiTM), and replay attacks. Using this classifier, our cGAN
perturbs known attacks to generate adversarial samples that misclassify as
benign while retaining statistical resemblance to OOD distributions. These
adversarial samples are iteratively refined to achieve high stealth and success
rates. To detect such perturbations, we implement a conditional variational
autoencoder (CVAE), leveraging negative log-likelihood to separate adversarial
inputs from authentic OOD samples. Comparative evaluation shows that CVAE-based
regret scores significantly outperform traditional Mahalanobis distance-based
detectors in identifying stealthy adversarial threats. Our findings emphasize
the importance of advanced probabilistic modeling to strengthen IDS
capabilities against adaptive, generative-model-based cyber intrusions.

</details>


### [91] [Scalable Bayesian Low-Rank Adaptation of Large Language Models via Stochastic Variational Subspace Inference](https://arxiv.org/abs/2506.21408)
*Colin Samplawski,Adam D. Cobb,Manoj Acharya,Ramneet Kaur,Susmit Jha*

Key words: 大型语言模型,不确定性量化,贝叶斯推理,低秩自适应,变分推断

TL;DR: ScalaBL是一种可扩展的贝叶斯低秩自适应方法，通过随机变分子空间推理，实现对大型语言模型（LLMs）的不确定性量化，同时仅需少量额外参数。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决大型语言模型（LLMs）因幻觉和不准确校准导致的不确定性量化问题，尤其是在高风险领域如自主系统和医疗中的重要性。

Method: 利用低秩自适应（LoRA）参数构建r维子空间，并通过变分推理进行贝叶斯推断，将子空间样本映射到LLM的完整权重空间。

Result: 尽管子空间维度低，ScalaBL性能与最先进方法相当，仅需约1000额外参数，并可扩展至迄今为止最大的贝叶斯LLM。

Conclusion: ScalaBL为LLMs的不确定性量化提供了一种高效且可扩展的解决方案。

Abstract: Despite their widespread use, large language models (LLMs) are known to
hallucinate incorrect information and be poorly calibrated. This makes the
uncertainty quantification of these models of critical importance, especially
in high-stakes domains, such as autonomy and healthcare. Prior work has made
Bayesian deep learning-based approaches to this problem more tractable by
performing inference over the low-rank adaptation (LoRA) parameters of a
fine-tuned model. While effective, these approaches struggle to scale to larger
LLMs due to requiring further additional parameters compared to LoRA. In this
work we present $\textbf{Scala}$ble $\textbf{B}$ayesian $\textbf{L}$ow-Rank
Adaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform
Bayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By
repurposing the LoRA parameters as projection matrices, we are able to map
samples from this subspace into the full weight space of the LLM. This allows
us to learn all the parameters of our approach using stochastic variational
inference. Despite the low dimensionality of our subspace, we are able to
achieve competitive performance with state-of-the-art approaches while only
requiring ${\sim}1000$ additional parameters. Furthermore, it allows us to
scale up to the largest Bayesian LLM to date, with four times as a many base
parameters as prior work.

</details>


### [92] [Personalized Federated Learning via Dual-Prompt Optimization and Cross Fusion](https://arxiv.org/abs/2506.21144)
*Yuguang Zhang,Kuangpu Guo,Zhihe Lu,Yunbo Wang,Jian Liang*

Key words: 联邦学习、视觉语言模型、提示学习、个性化、异质性

TL;DR: 本文提出了一种基于双提示学习和交叉融合的个性化联邦学习框架pFedDC，用于解决联邦学习中的数据、计算和通信异质性挑战，并通过多模态和自适应融合提升性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 联邦学习在数据、计算和通信异质性方面存在挑战，而现有方法仅依赖文本提示且忽视了联合标签-域分布偏移，因此需要一种更高效的解决方案。

Method: 通过设计双提示学习（全局和本地提示）和交叉融合模块，pFedDC在多模态中捕获共享知识和客户端特定特征，并自适应融合以生成个性化表示。

Result: 在九个具有不同异质性的数据集上的实验表明，pFedDC在性能上优于现有最先进方法。

Conclusion: pFedDC通过多模态提示学习和自适应融合，有效解决了联邦学习中的异质性挑战，提升了模型性能。

Abstract: Federated learning (FL) enables collaborative model training across
decentralized clients without sharing local data, but is challenged by
heterogeneity in data, computation, and communication. Pretrained
vision-language models (VLMs), with their strong generalization and lightweight
tuning via prompts, offer a promising solution. However, existing federated
prompt-learning methods rely only on text prompts and overlook joint
label-domain distribution shifts. In this paper, we propose a personalized FL
framework based on dual-prompt learning and cross fusion, termed pFedDC.
Specifically, each client maintains both global and local prompts across vision
and language modalities: global prompts capture common knowledge shared across
the federation, while local prompts encode client-specific semantics and domain
characteristics. Meanwhile, a cross-fusion module is designed to adaptively
integrate prompts from different levels, enabling the model to generate
personalized representations aligned with each client's unique data
distribution. Extensive experiments across nine datasets with various types of
heterogeneity show that pFedDC consistently outperforms state-of-the-art
methods.

</details>


### [93] [Linearity-based neural network compression](https://arxiv.org/abs/2506.21146)
*Silas Dobler,Florian Lemmerich*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In neural network compression, most current methods reduce unnecessary
parameters by measuring importance and redundancy. To augment already highly
optimized existing solutions, we propose linearity-based compression as a novel
way to reduce weights in a neural network. It is based on the intuition that
with ReLU-like activation functions, neurons that are almost always activated
behave linearly, allowing for merging of subsequent layers. We introduce the
theory underlying this compression and evaluate our approach experimentally.
Our novel method achieves a lossless compression down to 1/4 of the original
model size in over the majority of tested models. Applying our method on
already importance-based pruned models shows very little interference between
different types of compression, demonstrating the option of successful
combination of techniques. Overall, our work lays the foundation for a new type
of compression method that enables smaller and ultimately more efficient neural
network models.

</details>


### [94] [Diverse Mini-Batch Selection in Reinforcement Learning for Efficient Chemical Exploration in de novo Drug Design](https://arxiv.org/abs/2506.21158)
*Hampus Gummesson Svensson,Ola Engkvist,Jon Paul Janet,Christian Tyrchan,Morteza Haghir Chehreghani*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In many real-world applications, evaluating the goodness of instances is
often costly and time-consuming, e.g., human feedback and physics simulations,
in contrast to proposing new instances. In particular, this is even more
critical in reinforcement learning, as new interactions with the environment
(i.e., new instances) need to be evaluated to provide a reward signal to learn
from. As sufficient exploration is crucial, learning from a diverse mini-batch
can have a large impact and help mitigate mode collapse. In this paper, we
introduce diverse mini-batch selection for reinforcement learning and propose
to use determinantal point processes for this task. We study this framework in
the context of a real-world problem, namely drug discovery. We experimentally
study how our proposed framework can improve the effectiveness of chemical
exploration in de novo drug design, where finding diverse and high-quality
solutions is essential. We conduct a comprehensive evaluation with three
well-established molecular generation oracles over numerous generative steps.
Our experiments conclude that our diverse mini-batch selection framework can
substantially improve the diversity of the solutions, while still obtaining
solutions of high quality. In drug discovery, such outcome can potentially lead
to fulfilling unmet medication needs faster.

</details>


### [95] [Artificial Delegates Resolve Fairness Issues in Perpetual Voting with Partial Turnout](https://arxiv.org/abs/2506.21186)
*Apurva Shah,Axel Abels,Ann Nowé,Tom Lenaerts*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Perpetual voting addresses fairness in sequential collective decision-making
by evaluating representational equity over time. However, existing perpetual
voting rules rely on full participation and complete approval information,
assumptions that rarely hold in practice, where partial turnout is the norm. In
this work, we study the integration of Artificial Delegates,
preference-learning agents trained to represent absent voters, into perpetual
voting systems. We examine how absenteeism affects fairness and
representativeness under various voting methods and evaluate the extent to
which Artificial Delegates can compensate for missing participation. Our
findings indicate that while absenteeism significantly affects fairness,
Artificial Delegates reliably mitigate these effects and enhance robustness
across diverse scenarios.

</details>


### [96] [Zero-Shot Learning for Obsolescence Risk Forecasting](https://arxiv.org/abs/2506.21240)
*Elie Saad,Aya Mrabah,Mariem Besbes,Marc Zolghadri,Victor Czmil,Claude Baron,Vincent Bourgeois*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Component obsolescence poses significant challenges in industries reliant on
electronic components, causing increased costs and disruptions in the security
and availability of systems. Accurate obsolescence risk prediction is essential
but hindered by a lack of reliable data. This paper proposes a novel approach
to forecasting obsolescence risk using zero-shot learning (ZSL) with large
language models (LLMs) to address data limitations by leveraging
domain-specific knowledge from tabular datasets. Applied to two real-world
datasets, the method demonstrates effective risk prediction. A comparative
evaluation of four LLMs underscores the importance of selecting the right model
for specific forecasting tasks.

</details>


### [97] [Improved seeding strategies for k-means and k-GMM](https://arxiv.org/abs/2506.21291)
*Guillaume Carrière,Frédéric Cazals*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We revisit the randomized seeding techniques for k-means clustering and k-GMM
(Gaussian Mixture model fitting with Expectation-Maximization), formalizing
their three key ingredients: the metric used for seed sampling, the number of
candidate seeds, and the metric used for seed selection. This analysis yields
novel families of initialization methods exploiting a lookahead
principle--conditioning the seed selection to an enhanced coherence with the
final metric used to assess the algorithm, and a multipass strategy to tame
down the effect of randomization.
  Experiments show a consistent constant factor improvement over classical
contenders in terms of the final metric (SSE for k-means, log-likelihood for
k-GMM), at a modest overhead. In particular, for k-means, our methods improve
on the recently designed multi-swap strategy, which was the first one to
outperform the greedy k-means++ seeding.
  Our experimental analysis also shed light on subtle properties of k-means
often overlooked, including the (lack of) correlations between the SSE upon
seeding and the final SSE, the variance reduction phenomena observed in
iterative seeding methods, and the sensitivity of the final SSE to the pool
size for greedy methods.
  Practically, our most effective seeding methods are strong candidates to
become one of the--if not the--standard techniques. From a theoretical
perspective, our formalization of seeding opens the door to a new line of
analytical approaches.

</details>


### [98] [AGTCNet: A Graph-Temporal Approach for Principled Motor Imagery EEG Classification](https://arxiv.org/abs/2506.21338)
*Galvin Brice S. Lim,Brian Godwin S. Lim,Argel A. Bandala,John Anthony C. Jose,Timothy Scott C. Chu,Edwin Sybingco*

Key words: 脑机接口, 脑电图, 图卷积注意力网络, 运动想象分类, 深度学习

TL;DR: 该研究提出了一种新型的注意力图-时间卷积网络（AGTCNet），用于解决脑机接口（BCI）系统中多通道脑电图（EEG）信号分类的挑战，显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于EEG信号的复杂性和个体间的差异，开发跨主题和跨会话的鲁棒BCI系统仍然是一个重大挑战。本研究旨在通过创新的模型设计填补这一空白。

Method: 提出了AGTCNet模型，利用EEG电极的地形配置作为归纳偏差，结合图卷积注意力网络（GCAT）共同学习时空EEG表示。

Result: AGTCNet在多个数据集上显著优于现有方法，模型尺寸减小49.87%，推理时间缩短64.65%，达到了最先进的分类性能。

Conclusion: AGTCNet在EEG分类任务中表现出高效和实用性，为BCI的部署提供了有前景的解决方案。

Abstract: Brain-computer interface (BCI) technology utilizing electroencephalography
(EEG) marks a transformative innovation, empowering motor-impaired individuals
to engage with their environment on equal footing. Despite its promising
potential, developing subject-invariant and session-invariant BCI systems
remains a significant challenge due to the inherent complexity and variability
of neural activity across individuals and over time, compounded by EEG hardware
constraints. While prior studies have sought to develop robust BCI systems,
existing approaches remain ineffective in capturing the intricate
spatiotemporal dependencies within multichannel EEG signals. This study
addresses this gap by introducing the attentive graph-temporal convolutional
network (AGTCNet), a novel graph-temporal model for motor imagery EEG (MI-EEG)
classification. Specifically, AGTCNet leverages the topographic configuration
of EEG electrodes as an inductive bias and integrates graph convolutional
attention network (GCAT) to jointly learn expressive spatiotemporal EEG
representations. The proposed model significantly outperformed existing MI-EEG
classifiers, achieving state-of-the-art performance while utilizing a compact
architecture, underscoring its effectiveness and practicality for BCI
deployment. With a 49.87% reduction in model size, 64.65% faster inference
time, and shorter input EEG signal, AGTCNet achieved a moving average accuracy
of 66.82% for subject-independent classification on the BCI Competition IV
Dataset 2a, which further improved to 82.88% when fine-tuned for
subject-specific classification. On the EEG Motor Movement/Imagery Dataset,
AGTCNet achieved moving average accuracies of 64.14% and 85.22% for 4-class and
2-class subject-independent classifications, respectively, with further
improvements to 72.13% and 90.54% for subject-specific classifications.

</details>


### [99] [DynamicBench: Evaluating Real-Time Report Generation in Large Language Models](https://arxiv.org/abs/2506.21343)
*Jingyao Li,Hao Sun,Zile Qiao,Yong Jiang,Pengjun Xie,Fei Huang,Hong Xu,Jiaya Jia*

Key words: 大语言模型, 动态基准测试, 实时信息处理, 报告生成, 双路径检索

TL;DR: 提出了一个名为 DynamicBench 的动态基准测试，用于评估大语言模型在实时信息处理和存储中的能力，通过双路径检索管道结合网络搜索和本地报告数据库，提升了模型在专业领域中的响应准确性和报告生成能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统的大语言模型基准测试通常依赖静态评估，无法满足实时信息处理的动态需求，因此需要一个新的基准测试来评估模型在这方面的能力。

Method: 采用双路径检索管道，结合网络搜索和本地报告数据库，设计了包含外部文档提供与否的测试场景，并开发了先进的动态信息合成报告生成系统。

Result: 实验结果表明，DynamicBench 在无文档和有文档辅助的场景中分别超越了 GPT4o 7.0% 和 5.8%，达到了最先进的性能。

Conclusion: DynamicBench 为评估大语言模型在动态信息处理方面的能力提供了有效工具，代码和数据将公开。

Abstract: Traditional benchmarks for large language models (LLMs) typically rely on
static evaluations through storytelling or opinion expression, which fail to
capture the dynamic requirements of real-time information processing in
contemporary applications. To address this limitation, we present DynamicBench,
a benchmark designed to evaluate the proficiency of LLMs in storing and
processing up-to-the-minute data. DynamicBench utilizes a dual-path retrieval
pipeline, integrating web searches with local report databases. It necessitates
domain-specific knowledge, ensuring accurate responses report generation within
specialized fields. By evaluating models in scenarios that either provide or
withhold external documents, DynamicBench effectively measures their capability
to independently process recent information or leverage contextual
enhancements. Additionally, we introduce an advanced report generation system
adept at managing dynamic information synthesis. Our experimental results
confirm the efficacy of our approach, with our method achieving
state-of-the-art performance, surpassing GPT4o in document-free and
document-assisted scenarios by 7.0% and 5.8%, respectively. The code and data
will be made publicly available.

</details>


### [100] [Lipschitz Bounds for Persistent Laplacian Eigenvalues under One-Simplex Insertions](https://arxiv.org/abs/2506.21352)
*Le Vu Anh,Mehmet Dik,Nguyen Viet Anh*

Key words: 持久拉普拉斯算子, 特征值稳定性, 拓扑数据分析, Lipschitz界

TL;DR: 本文证明了持久拉普拉斯算子的特征值在添加一个单纯形时的稳定性，提供了首次特征值级别的鲁棒性保证。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究持久拉普拉斯算子的特征值在数据动态变化时的稳定性，为下游应用如热核签名和谱神经网络提供理论支持。

Method: 通过证明一个统一的Lipschitz界，说明添加一个单纯形时，持久拉普拉斯算子的每个特征值的变化不超过其边界欧几里得范数的两倍。

Result: 获得了特征值级别的鲁棒性保证，确保了谱特征在局部更新下的稳定性。

Conclusion: 该结果为动态数据环境中的谱拓扑数据分析提供了可靠的误差控制。

Abstract: Persistent Laplacians are matrix operators that track how the shape and
structure of data transform across scales and are popularly adopted in biology,
physics, and machine learning. Their eigenvalues are concise descriptors of
geometric and topological features in a filtration. Although earlier work
established global algebraic stability for these operators, the precise change
in a single eigenvalue when one simplex, such as a vertex, edge, or triangle,
is added has remained unknown. This is important because downstream tools,
including heat-kernel signatures and spectral neural networks, depend directly
on these eigenvalues. We close this gap by proving a uniform Lipschitz bound:
after inserting one simplex, every up-persistent Laplacian eigenvalue can vary
by at most twice the Euclidean norm of that simplex's boundary, independent of
filtration scale and complex size. This result delivers the first
eigenvalue-level robustness guarantee for spectral topological data analysis.
It guarantees that spectral features remain stable under local updates and
enables reliable error control in dynamic data settings.

</details>


### [101] [SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning](https://arxiv.org/abs/2506.21355)
*Melanie Rieff,Maya Varma,Ossian Rabow,Subathra Adithan,Julie Kim,Ken Chang,Hannah Lee,Nidhi Rohatgi,Christian Bluethgen,Mohamed S. Muneer,Jean-Benoit Delbrouck,Michael Moor*

Key words: 多模态学习, 上下文学习, 医学任务, 大语言模型

TL;DR: 论文提出了SMMILE和SMMILE++两个专家驱动的多模态ICL基准测试，评估了15个MLLMs在医学任务中的表现，发现大多数模型表现较差，且存在对无关示例敏感和示例顺序的偏向性问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索多模态大语言模型在医学任务中的多模态上下文学习能力，填补该领域的空白，并为医学专家提供实用的评估工具。

Method: 引入SMMILE和SMMILE++两个基准测试，包含专家设计的111和1038个问题，覆盖多种医学专科和影像模态，评估15个MLLMs的表现。

Result: 大多数模型表现不佳，ICL仅带来8%和9.4%的平均提升；无关示例和示例顺序（存在最新偏向性）显著影响性能。

Conclusion: 当前MLLMs在多模态医学任务的学习中存在明显的局限性和偏向性，需进一步优化。

Abstract: Multimodal in-context learning (ICL) remains underexplored despite
significant potential for domains such as medicine. Clinicians routinely
encounter diverse, specialized tasks requiring adaptation from limited
examples, such as drawing insights from a few relevant prior cases or
considering a constrained set of differential diagnoses. While multimodal large
language models (MLLMs) have shown advances in medical visual question
answering (VQA), their ability to learn multimodal tasks from context is
largely unknown. We introduce SMMILE, the first expert-driven multimodal ICL
benchmark for medical tasks. Eleven medical experts curated problems, each
including a multimodal query and multimodal in-context examples as task
demonstrations. SMMILE encompasses 111 problems (517 question-image-answer
triplets) covering 6 medical specialties and 13 imaging modalities. We further
introduce SMMILE++, an augmented variant with 1038 permuted problems. A
comprehensive evaluation of 15 MLLMs demonstrates that most models exhibit
moderate to poor multimodal ICL ability in medical tasks. In open-ended
evaluations, ICL contributes only 8% average improvement over zero-shot on
SMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant
in-context examples: even a single noisy or irrelevant example can degrade
performance by up to 9.5%. Moreover, example ordering exhibits a recency bias,
i.e., placing the most relevant example last can lead to substantial
performance improvements by up to 71%. Our findings highlight critical
limitations and biases in current MLLMs when learning multimodal medical tasks
from context.

</details>


### [102] [rQdia: Regularizing Q-Value Distributions With Image Augmentation](https://arxiv.org/abs/2506.21367)
*Sam Lerman,Jing Bi*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: rQdia regularizes Q-value distributions with augmented images in pixel-based
deep reinforcement learning. With a simple auxiliary loss, that equalizes these
distributions via MSE, rQdia boosts DrQ and SAC on 9/12 and 10/12 tasks
respectively in the MuJoCo Continuous Control Suite from pixels, and
Data-Efficient Rainbow on 18/26 Atari Arcade environments. Gains are measured
in both sample efficiency and longer-term training. Moreover, the addition of
rQdia finally propels model-free continuous control from pixels over the state
encoding baseline.

</details>


### [103] [MAx-DNN: Multi-Level Arithmetic Approximation for Energy-Efficient DNN Hardware Accelerators](https://arxiv.org/abs/2506.21371)
*Vasileios Leon,Georgios Makris,Sotirios Xydis,Kiamal Pekmestzi,Dimitrios Soudris*

Key words: Deep Neural Network, DNN近似计算, 能源效率, 硬件近似, 误差弹性

TL;DR: 本文探讨了在低功耗DNN计算中，通过硬件近似技术与DNN工作负载的细粒度误差弹性的结合，以实现更高的能源效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 通过利用硬件近似技术和DNN的误差弹性，提升低功耗DNN计算的能源效率。

Method: 使用ROUP近似乘法器，并在网络中按层、过滤器、核心级别分布，评估其对精度和能量的影响。

Result: 提出的解决方案在量化模型的基础上实现了54%的能源增益，精度损失仅为4%，且比现有DNN近似方法提供2倍能源增益和更高精度。

Conclusion: 通过细粒度分布的硬件近似技术，可在保证较高精度的同时显著提升DNN计算的能源效率。

Abstract: Nowadays, the rapid growth of Deep Neural Network (DNN) architectures has
established them as the defacto approach for providing advanced Machine
Learning tasks with excellent accuracy. Targeting low-power DNN computing, this
paper examines the interplay of fine-grained error resilience of DNN workloads
in collaboration with hardware approximation techniques, to achieve higher
levels of energy efficiency. Utilizing the state-of-the-art ROUP approximate
multipliers, we systematically explore their fine-grained distribution across
the network according to our layer-, filter-, and kernel-level approaches, and
examine their impact on accuracy and energy. We use the ResNet-8 model on the
CIFAR-10 dataset to evaluate our approximations. The proposed solution delivers
up to 54% energy gains in exchange for up to 4% accuracy loss, compared to the
baseline quantized model, while it provides 2x energy gains with better
accuracy versus the state-of-the-art DNN approximations.

</details>


### [104] [Pay Attention to Small Weights](https://arxiv.org/abs/2506.21374)
*Chao Zhou,Tom Jacobs,Advait Gadhikar,Rebekka Burkholz*

Key words: 微调,预训练网络,资源优化,NANOADAM

TL;DR: 论文提出了一种动态更新小幅度权重的方法NANOADAM，以减少微调大规模预训练网络的资源开销。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 观察到微调时大梯度常与小幅度权重相关，且这种关联在微调中更显著。

Method: NANOADAM动态更新小幅度权重，无需梯度计算。

Result: 使用NANOADAM能保留预训练特征、避免灾难性遗忘，并在实验中表现更优。

Conclusion: NANOADAM在NLP和视觉任务中均有效，能显著提升微调效率与性能。

Abstract: Finetuning large pretrained neural networks is known to be
resource-intensive, both in terms of memory and computational cost. To mitigate
this, a common approach is to restrict training to a subset of the model
parameters. By analyzing the relationship between gradients and weights during
finetuning, we observe a notable pattern: large gradients are often associated
with small-magnitude weights. This correlation is more pronounced in finetuning
settings than in training from scratch. Motivated by this observation, we
propose NANOADAM, which dynamically updates only the small-magnitude weights
during finetuning and offers several practical advantages: first, this
criterion is gradient-free -- the parameter subset can be determined without
gradient computation; second, it preserves large-magnitude weights, which are
likely to encode critical features learned during pretraining, thereby reducing
the risk of catastrophic forgetting; thirdly, it permits the use of larger
learning rates and consistently leads to better generalization performance in
experiments. We demonstrate this for both NLP and vision tasks.

</details>


### [105] [Temporal-Aware Graph Attention Network for Cryptocurrency Transaction Fraud Detection](https://arxiv.org/abs/2506.21382)
*Zhi Zheng,Bochuan Zhou,Yuping Song*

Key words: 加密货币欺诈检测、图注意力网络、时序感知、三重注意力机制、类别不平衡

TL;DR: 本文提出了一种增强时序感知的图注意力网络（ATGAT），通过结合多尺度时间差异特征和周期性位置编码、构建三重注意力机制以及加权BCE损失，显著提升了加密货币交易欺诈检测性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 加密货币交易欺诈检测面临交易模式复杂和类别不平衡的挑战，传统方法难以捕捉交易网络的时序和结构依赖关系。

Method: ATGAT包含三个模块：时序嵌入模块、时序感知三重注意力机制和加权BCE损失处理类别不平衡。

Result: 在Elliptic++数据集上，ATGAT的AUC达到0.9130，优于传统方法XGBoost、GCN和标准GAT。

Conclusion: ATGAT证明了时序感知和多重注意力机制对图神经网络的增强效果，并为金融机构提供了更可靠的欺诈检测工具。

Abstract: Cryptocurrency transaction fraud detection faces the dual challenges of
increasingly complex transaction patterns and severe class imbalance.
Traditional methods rely on manual feature engineering and struggle to capture
temporal and structural dependencies in transaction networks. This paper
proposes an Augmented Temporal-aware Graph Attention Network (ATGAT) that
enhances detection performance through three modules: (1) designing an advanced
temporal embedding module that fuses multi-scale time difference features with
periodic position encoding; (2) constructing a temporal-aware triple attention
mechanism that jointly optimizes structural, temporal, and global context
attention; (3) employing weighted BCE loss to address class imbalance.
Experiments on the Elliptic++ cryptocurrency dataset demonstrate that ATGAT
achieves an AUC of 0.9130, representing a 9.2% improvement over the best
traditional method XGBoost, 12.0% over GCN, and 10.0% over standard GAT. This
method not only validates the enhancement effect of temporal awareness and
triple attention mechanisms on graph neural networks, but also provides
financial institutions with more reliable fraud detection tools, with its
design principles generalizable to other temporal graph anomaly detection
tasks.

</details>


### [106] [Early Stopping Tabular In-Context Learning](https://arxiv.org/abs/2506.21387)
*Jaris Küken,Lennart Purucker,Frank Hutter*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Tabular foundation models have shown strong performance across various
tabular learning tasks via in-context learning, offering robust generalization
without any downstream finetuning. However, their inference-time costs remain
high, particularly for larger datasets. To address this, we propose
early-stopping the in-context learning process. We achieve this by dynamically
evaluating whether to stop in-context learning after each Transformer encoder
layer. Once stopped, we decode the embedding using a pre-trained layer-wise
decoder. Experiments across 34 small classification tasks size show that early
stopping in-context learning accelerates inference by up to x1.3 with
negligible degradation in predictive performance. To assess scalability, we
further evaluate our method on five larger classification tasks, achieving
speedups of up to x2.2. Our results demonstrate the potential of early exiting
as an effective and practical strategy for improving the efficiency of tabular
in-context learning.

</details>


### [107] [Distributed Cross-Channel Hierarchical Aggregation for Foundation Models](https://arxiv.org/abs/2506.21411)
*Aristeidis Tsaris,Isaac Lyngaas,John Lagregren,Mohamed Wahib,Larry York,Prasanna Balaprakash,Dan Lu,Feiyi Wang,Xiao Wang*

Key words: 科学基础模型, 分布式计算, 视觉变换器, 多通道聚合

TL;DR: 提出了分布式跨通道分层聚合（D-CHAG）方法，解决了视觉科学基础模型在处理多通道图像时的计算效率问题，显著减少内存使用并提高吞吐量。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前分布式方法在处理多通道图像时的计算密集型特点未得到充分解决，限制了视觉科学基础模型的应用潜力。

Method: 提出了D-CHAG方法，兼容任何模型并行策略和视觉变换器架构，通过分层聚合提高计算效率。

Result: 在超光谱成像和天气预报任务中，D-CHAG结合张量并行和模型分片，内存使用减少75%，吞吐量提高一倍以上。

Conclusion: D-CHAG为多通道视觉任务提供高效解决方案，展示了在科学发现中的广泛应用前景。

Abstract: Vision-based scientific foundation models hold significant promise for
advancing scientific discovery and innovation. This potential stems from their
ability to aggregate images from diverse sources such as varying physical
groundings or data acquisition systems and to learn spatio-temporal
correlations using transformer architectures. However, tokenizing and
aggregating images can be compute-intensive, a challenge not fully addressed by
current distributed methods. In this work, we introduce the Distributed
Cross-Channel Hierarchical Aggregation (D-CHAG) approach designed for datasets
with a large number of channels across image modalities. Our method is
compatible with any model-parallel strategy and any type of vision transformer
architecture, significantly improving computational efficiency. We evaluated
D-CHAG on hyperspectral imaging and weather forecasting tasks. When integrated
with tensor parallelism and model sharding, our approach achieved up to a 75%
reduction in memory usage and more than doubled sustained throughput on up to
1,024 AMD GPUs on the Frontier Supercomputer.

</details>


### [108] [Flow-Based Single-Step Completion for Efficient and Expressive Policy Learning](https://arxiv.org/abs/2506.21427)
*Prajwal Koirala,Cody Fleming*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Generative models such as diffusion and flow-matching offer expressive
policies for offline reinforcement learning (RL) by capturing rich, multimodal
action distributions, but their iterative sampling introduces high inference
costs and training instability due to gradient propagation across sampling
steps. We propose the \textit{Single-Step Completion Policy} (SSCP), a
generative policy trained with an augmented flow-matching objective to predict
direct completion vectors from intermediate flow samples, enabling accurate,
one-shot action generation. In an off-policy actor-critic framework, SSCP
combines the expressiveness of generative models with the training and
inference efficiency of unimodal policies, without requiring long
backpropagation chains. Our method scales effectively to offline,
offline-to-online, and online RL settings, offering substantial gains in speed
and adaptability over diffusion-based baselines. We further extend SSCP to
goal-conditioned RL, enabling flat policies to exploit subgoal structures
without explicit hierarchical inference. SSCP achieves strong results across
standard offline RL and behavior cloning benchmarks, positioning it as a
versatile, expressive, and efficient framework for deep RL and sequential
decision-making.

</details>


### [109] [Deception Detection in Dyadic Exchanges Using Multimodal Machine Learning: A Study on a Swedish Cohort](https://arxiv.org/abs/2506.21429)
*Franco Rugolon,Thomas Jack Samuels,Stephan Hau,Lennart Högman*

Key words: 多模态机器学习, 欺骗检测, 双向互动, 早期融合, 晚期融合

TL;DR: 研究了多模态机器学习在检测双向互动中欺骗行为的有效性，结合语音和面部信息显著提高了准确性，最佳效果达71%。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨多模态数据（语音和面部）在欺骗检测中的作用，填补斯堪的纳维亚人群研究的空白。

Method: 使用早期和晚期融合策略，结合语音和视频数据（行动单元和注视信息），分析双向互动中的欺骗行为。

Result: 多模态方法优于单模态，结合双方数据时效果最佳（71%准确率）。

Conclusion: 多模态和双方数据的结合显著提升欺骗检测效果，支持心理学理论，为心理治疗等领域提供研究基础。

Abstract: This study investigates the efficacy of using multimodal machine learning
techniques to detect deception in dyadic interactions, focusing on the
integration of data from both the deceiver and the deceived. We compare early
and late fusion approaches, utilizing audio and video data - specifically,
Action Units and gaze information - across all possible combinations of
modalities and participants. Our dataset, newly collected from Swedish native
speakers engaged in truth or lie scenarios on emotionally relevant topics,
serves as the basis for our analysis. The results demonstrate that
incorporating both speech and facial information yields superior performance
compared to single-modality approaches. Moreover, including data from both
participants significantly enhances deception detection accuracy, with the best
performance (71%) achieved using a late fusion strategy applied to both
modalities and participants. These findings align with psychological theories
suggesting differential control of facial and vocal expressions during initial
interactions. As the first study of its kind on a Scandinavian cohort, this
research lays the groundwork for future investigations into dyadic
interactions, particularly within psychotherapy settings.

</details>


### [110] [Towards an Optimal Control Perspective of ResNet Training](https://arxiv.org/abs/2506.21453)
*Jens Püttschneider,Simon Heilig,Asja Fischer,Timm Faulwasser*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose a training formulation for ResNets reflecting an optimal control
problem that is applicable for standard architectures and general loss
functions. We suggest bridging both worlds via penalizing intermediate outputs
of hidden states corresponding to stage cost terms in optimal control. For
standard ResNets, we obtain intermediate outputs by propagating the state
through the subsequent skip connections and the output layer. We demonstrate
that our training dynamic biases the weights of the unnecessary deeper residual
layers to vanish. This indicates the potential for a theory-grounded layer
pruning strategy.

</details>


### [111] [A Keyword-Based Technique to Evaluate Broad Question Answer Script](https://arxiv.org/abs/2506.21461)
*Tamim Al Mahmud,Md Gulzar Hussain,Sumaiya Kabir,Hasnain Ahmad,Mahmudus Sobhan*

Key words: 教育评估,主观答案,关键词提取,语法检查,拼写检查

TL;DR: 提出了一种高效评估主观答题纸的电子系统，通过关键词提取和比较，以及语法和拼写检查，实现了高精度评分。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 教育评估中主观答案的电子化评分效率低下，需要更高效的解决方案。

Method: 开发集成系统，提取答题纸关键词并与开放和封闭领域的解析关键词进行比较，同时检查语法和拼写错误。

Result: 系统在100份学生答题纸上测试，精度得分为0.91。

Conclusion: 提出的系统能高效且准确地评估主观答案，适用于教育领域。

Abstract: Evaluation is the method of assessing and determining the educational system
through various techniques such as verbal or viva-voice test, subjective or
objective written test. This paper presents an efficient solution to evaluate
the subjective answer script electronically. In this paper, we proposed and
implemented an integrated system that examines and evaluates the written answer
script. This article focuses on finding the keywords from the answer script and
then compares them with the keywords that have been parsed from both open and
closed domain. The system also checks the grammatical and spelling errors in
the answer script. Our proposed system tested with answer scripts of 100
students and gives precision score 0.91.

</details>


### [112] [Optimising 4th-Order Runge-Kutta Methods: A Dynamic Heuristic Approach for Efficiency and Low Storage](https://arxiv.org/abs/2506.21465)
*Gavin Lee Goodship,Luis Miralles-Pechuan,Stephen O'Sullivan*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Extended Stability Runge-Kutta (ESRK) methods are crucial for solving
large-scale computational problems in science and engineering, including
weather forecasting, aerodynamic analysis, and complex biological modelling.
However, balancing accuracy, stability, and computational efficiency remains
challenging, particularly for high-order, low-storage schemes. This study
introduces a hybrid Genetic Algorithm (GA) and Reinforcement Learning (RL)
approach for automated heuristic discovery, optimising low-storage ESRK
methods. Unlike traditional approaches that rely on manually designed
heuristics or exhaustive numerical searches, our method leverages GA-driven
mutations for search-space exploration and an RL-inspired state transition
mechanism to refine heuristic selection dynamically. This enables systematic
parameter reduction, preserving fourth-order accuracy while significantly
improving computational efficiency.The proposed GA-RL heuristic optimisation
framework is validated through rigorous testing on benchmark problems,
including the 1D and 2D Brusselator systems and the steady-state Navier-Stokes
equations. The best-performing heuristic achieves a 25\% reduction in IPOPT
runtime compared to traditional ESRK optimisation processes while maintaining
numerical stability and accuracy. These findings demonstrate the potential of
adaptive heuristic discovery to improve resource efficiency in high-fidelity
simulations and broaden the applicability of low-storage Runge-Kutta methods in
real-world computational fluid dynamics, physics simulations, and other
demanding fields. This work establishes a new paradigm in heuristic
optimisation for numerical methods, opening pathways for further exploration
using Deep RL and AutoML-based heuristic search

</details>


### [113] [Devising a solution to the problems of Cancer awareness in Telangana](https://arxiv.org/abs/2506.21500)
*Priyanka Avhad,Vedanti Kshirsagar,Urvi Ranjan,Mahek Nakhua*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: According to the data, the percent of women who underwent screening for
cervical cancer, breast and oral cancer in Telangana in the year 2020 was 3.3
percent, 0.3 percent and 2.3 percent respectively. Although early detection is
the only way to reduce morbidity and mortality, people have very low awareness
about cervical and breast cancer signs and symptoms and screening practices. We
developed an ML classification model to predict if a person is susceptible to
breast or cervical cancer based on demographic factors. We devised a system to
provide suggestions for the nearest hospital or Cancer treatment centres based
on the users location or address. In addition to this, we can integrate the
health card to maintain medical records of all individuals and conduct
awareness drives and campaigns. For ML classification models, we used decision
tree classification and support vector classification algorithms for cervical
cancer susceptibility and breast cancer susceptibility respectively. Thus, by
devising this solution we come one step closer to our goal which is spreading
cancer awareness, thereby, decreasing the cancer mortality and increasing
cancer literacy among the people of Telangana.

</details>


### [114] [Process mining-driven modeling and simulation to enhance fault diagnosis in cyber-physical systems](https://arxiv.org/abs/2506.21502)
*Francesco Vitale,Nicola Dall'Ora,Sebastiano Gaiardelli,Enrico Fraccaroli,Nicola Mazzocca,Franco Fummi*

Key words: 故障诊断, CPS, 无监督学习, 过程挖掘, 随机模拟

TL;DR: 提出了一种新型的无监督故障诊断方法，结合多元时间序列分析、过程挖掘和随机模拟，通过自动建模提升CPS故障诊断效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 手动建模故障行为需要大量领域知识且模型复杂易错，难以解释，为此提出自动化方法解决这一问题。

Method: 结合多元时间序列分析检测异常，转换为结构化事件日志，通过过程挖掘提取可解释模型，并加入时序分布进行随机模拟。

Result: 在机器人臂数据集上验证有效，能够建模、模拟和分类CPS故障行为，支持预测性维护和数字孪生开发。

Conclusion: 该方法显著提升了故障诊断的自动化水平和可解释性，为工业环境的智能维护提供支持。

Abstract: Fault diagnosis in Cyber-Physical Systems (CPSs) is essential for ensuring
system dependability and operational efficiency by accurately detecting
anomalies and identifying their root causes. However, the manual modeling of
faulty behaviors often demands extensive domain expertise and produces models
that are complex, error-prone, and difficult to interpret. To address this
challenge, we present a novel unsupervised fault diagnosis methodology that
integrates collective anomaly detection in multivariate time series, process
mining, and stochastic simulation. Initially, collective anomalies are detected
from low-level sensor data using multivariate time-series analysis. These
anomalies are then transformed into structured event logs, enabling the
discovery of interpretable process models through process mining. By
incorporating timing distributions into the extracted Petri nets, the approach
supports stochastic simulation of faulty behaviors, thereby enhancing root
cause analysis and behavioral understanding. The methodology is validated using
the Robotic Arm Dataset (RoAD), a widely recognized benchmark in smart
manufacturing. Experimental results demonstrate its effectiveness in modeling,
simulating, and classifying faulty behaviors in CPSs. This enables the creation
of comprehensive fault dictionaries that support predictive maintenance and the
development of digital twins for industrial environments.

</details>


### [115] [mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and Model Selection at Scale](https://arxiv.org/abs/2506.21550)
*Xiaona Zhou,Constantin Brif,Ismini Lourentzou*

Key words: 多变量时间序列, 异常检测, 基准测试, 无监督模型选择

TL;DR: mTSBench是当前最大的多变量时间序列异常检测基准，涵盖19个数据集和12个应用领域，评估了24种异常检测方法，并揭示了模型选择的重要性及其当前方法的不足。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决多变量时间序列异常检测（MTS-AD）在复杂变量依赖、时序动态性和稀疏标签下的挑战。

Method: 构建mTSBench基准，包含344个标记时间序列，评估24种异常检测方法并系统测试无监督模型选择技术。

Result: 无单一检测器在所有数据集上表现优异，当前最优模型选择方法仍远未达到理想效果。

Conclusion: mTSBench为未来自适应异常检测和稳健模型选择提供了统一评估框架。

Abstract: Multivariate time series anomaly detection (MTS-AD) is critical in domains
like healthcare, cybersecurity, and industrial monitoring, yet remains
challenging due to complex inter-variable dependencies, temporal dynamics, and
sparse anomaly labels. We introduce mTSBench, the largest benchmark to date for
MTS-AD and unsupervised model selection, spanning 344 labeled time series
across 19 datasets and 12 diverse application domains. mTSBench evaluates 24
anomaly detection methods, including large language model (LLM)-based detectors
for multivariate time series, and systematically benchmarks unsupervised model
selection techniques under standardized conditions. Consistent with prior
findings, our results confirm that no single detector excels across datasets,
underscoring the importance of model selection. However, even state-of-the-art
selection methods remain far from optimal, revealing critical gaps. mTSBench
provides a unified evaluation suite to enable rigorous, reproducible
comparisons and catalyze future advances in adaptive anomaly detection and
robust model selection.

</details>


### [116] [Where to find Grokking in LLM Pretraining? Monitor Memorization-to-Generalization without Test](https://arxiv.org/abs/2506.21551)
*Ziyue Li,Chenrui Fan,Tianyi Zhou*

Key words: grokking, 大语言模型, 预训练, 泛化, 内部动态

TL;DR: 该研究首次验证了大语言模型（OLMoE）在预训练中出现 'grokking' 现象，即测试性能在训练损失收敛后持续提升。通过分析模型内部动态，揭示了从记忆到泛化的转变机制。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探究 'grokking' 现象在大型基础模型预训练中的存在及其背后的机制，以理解泛化能力的延迟出现。

Method: 研究使用 7B 参数的 OLMoE 模型，通过计算训练损失和评估多样化的基准任务（如数学推理、代码生成等），并分析模型内部动态（如样本路径的演变）。

Result: 发现 'grokking' 确实存在于大规模预训练中，且样本路径从随机、实例特定逐渐演变为结构化、可共享，泛化能力提升。提出了量化路径距离和复杂度的新指标。

Conclusion: 研究揭示了 'grokking' 现象的内在机制，提出了可预测泛化性能的实用指标，为预训练提供了理论支持。

Abstract: Grokking, i.e., test performance keeps improving long after training loss
converged, has been recently witnessed in neural network training, making the
mechanism of generalization and other emerging capabilities such as reasoning
mysterious. While prior studies usually train small models on a few toy or
highly-specific tasks for thousands of epochs, we conduct the first study of
grokking on checkpoints during one-pass pretraining of a 7B large language
model (LLM), i.e., OLMoE. We compute the training loss and evaluate
generalization on diverse benchmark tasks, including math reasoning, code
generation, and commonsense/domain-specific knowledge retrieval tasks.
  Our study, for the first time, verifies that grokking still happens in the
pretraining of large-scale foundation models, though different data may enter
grokking stages asynchronously. We further demystify grokking's "emergence of
generalization" by investigating LLM internal dynamics. Specifically, we find
that training samples' pathways (i.e., expert choices across layers) evolve
from random, instance-specific to more structured and shareable between samples
during grokking. Also, the complexity of a sample's pathway reduces despite the
converged loss. These indicate a memorization-to-generalization conversion,
providing a mechanistic explanation of delayed generalization. In the study, we
develop two novel metrics to quantify pathway distance and the complexity of a
single pathway. We show their ability to predict the generalization improvement
on diverse downstream tasks. They are efficient, simple to compute and solely
dependent on training data. Hence, they have practical value for pretraining,
enabling us to monitor the generalization performance without finetuning and
test. Theoretically, we show that more structured pathways reduce model
complexity and improve the generalization bound.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [117] [The Singapore Consensus on Global AI Safety Research Priorities](https://arxiv.org/abs/2506.20702)
*Yoshua Bengio,Tegan Maharaj,Luke Ong,Stuart Russell,Dawn Song,Max Tegmark,Lan Xue,Ya-Qin Zhang,Stephen Casper,Wan Sie Lee,Sören Mindermann,Vanessa Wilfred,Vidhisha Balachandran,Fazl Barez,Michael Belinsky,Imane Bello,Malo Bourgon,Mark Brakel,Siméon Campos,Duncan Cass-Beggs,Jiahao Chen,Rumman Chowdhury,Kuan Chua Seah,Jeff Clune,Juntao Dai,Agnes Delaborde,Nouha Dziri,Francisco Eiras,Joshua Engels,Jinyu Fan,Adam Gleave,Noah Goodman,Fynn Heide,Dan Hendrycks,Cyrus Hodes,Bryan Low Kian Hsiang,Minlie Huang,Sami Jawhar,Wang Jingyu,Adam Tauman Kalai,Meindert Kamphuis,Mohan Kankanhalli,Subhash Kantamneni,Mathias Bonde Kirk,Thomas Kwa,Jeffrey Ladish,Kwok-Yan Lam,Wan Lee Sie,Taewhi Lee,Xiaojian Li,Jiajun Liu,Chaochao Lu,Yifan Mai,Richard Mallah,Julian Michael,Nick Moës,Simon Möller,Kihyuk Nam,Kwan Yee Ng,Mark Nitzberg,Besmira Nushi,Seán O hÉigeartaigh,Alejandro Ortega,Pierre Peigné,James Petrie,Benjamin Prud'Homme,Reihaneh Rabbany,Nayat Sanchez-Pi,Sarah Schwettmann,Buck Shlegeris,Saad Siddiqui,Aradhana Sinha,Martín Soto,Cheston Tan,Dong Ting,Robert Trager,Brian Tse,Anthony Tung K. H.,Vanessa Wilfred,John Willes,Denise Wong,Wei Xu,Rongwu Xu,Yi Zeng,HongJiang Zhang,Djordje Žikelić*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Rapidly improving AI capabilities and autonomy hold significant promise of
transformation, but are also driving vigorous debate on how to ensure that AI
is safe, i.e., trustworthy, reliable, and secure. Building a trusted ecosystem
is therefore essential -- it helps people embrace AI with confidence and gives
maximal space for innovation while avoiding backlash.
  The "2025 Singapore Conference on AI (SCAI): International Scientific
Exchange on AI Safety" aimed to support research in this space by bringing
together AI scientists across geographies to identify and synthesise research
priorities in AI safety. This resulting report builds on the International AI
Safety Report chaired by Yoshua Bengio and backed by 33 governments. By
adopting a defence-in-depth model, this report organises AI safety research
domains into three types: challenges with creating trustworthy AI systems
(Development), challenges with evaluating their risks (Assessment), and
challenges with monitoring and intervening after deployment (Control).

</details>


### [118] [MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation](https://arxiv.org/abs/2506.20737)
*Gurusha Juneja,Alon Albalak,Wenyue Hua,William Yang Wang*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The proliferation of LLM-based agents has led to increasing deployment of
inter-agent collaboration for tasks like scheduling, negotiation, resource
allocation etc. In such systems, privacy is critical, as agents often access
proprietary tools and domain-specific databases requiring strict
confidentiality. This paper examines whether LLM-based agents demonstrate an
understanding of contextual privacy. And, if instructed, do these systems
preserve inference time user privacy in non-adversarial multi-turn
conversation. Existing benchmarks to evaluate contextual privacy in LLM-agents
primarily assess single-turn, low-complexity tasks where private information
can be easily excluded. We first present a benchmark - MAGPIE comprising 158
real-life high-stakes scenarios across 15 domains. These scenarios are designed
such that complete exclusion of private data impedes task completion yet
unrestricted information sharing could lead to substantial losses. We then
evaluate the current state-of-the-art LLMs on (a) their understanding of
contextually private data and (b) their ability to collaborate without
violating user privacy. Empirical experiments demonstrate that current models,
including GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual
privacy, misclassifying private data as shareable 25.2\% and 43.6\% of the
time. In multi-turn conversations, these models disclose private information in
59.9\% and 50.5\% of cases even under explicit privacy instructions.
Furthermore, multi-agent systems fail to complete tasks in 71\% of scenarios.
These results underscore that current models are not aligned towards both
contextual privacy preservation and collaborative task-solving.

</details>


### [119] [Dynamic Context-Aware Prompt Recommendation for Domain-Specific AI Applications](https://arxiv.org/abs/2506.20815)
*Xinye Tang,Haijun Zhai,Chaitanya Belwal,Vineeth Thayanithi,Philip Baumann,Yogesh K Roy*

Key words: LLM, 提示推荐, 上下文感知, 领域特定

TL;DR: 提出了一个动态上下文感知提示推荐系统，帮助用户生成高质量的领域特定提示。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 用户在使用LLM应用时，高质量的提示难以生成，尤其是在领域特定应用中。

Method: 结合上下文查询分析、检索增强知识基础、分层技能组织和自适应技能排名。

Result: 在真实数据集上验证了系统的高实用性和相关性。

Conclusion: 该系统能够有效提升提示质量，适合领域特定AI应用。

Abstract: LLM-powered applications are highly susceptible to the quality of user
prompts, and crafting high-quality prompts can often be challenging especially
for domain-specific applications. This paper presents a novel dynamic
context-aware prompt recommendation system for domain-specific AI applications.
Our solution combines contextual query analysis, retrieval-augmented knowledge
grounding, hierarchical skill organization, and adaptive skill ranking to
generate relevant and actionable prompt suggestions.
  The system leverages behavioral telemetry and a two-stage hierarchical
reasoning process to dynamically select and rank relevant skills, and
synthesizes prompts using both predefined and adaptive templates enhanced with
few-shot learning. Experiments on real-world datasets demonstrate that our
approach achieves high usefulness and relevance, as validated by both automated
and expert evaluations.

</details>


### [120] [Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation](https://arxiv.org/abs/2506.20949)
*Chenkai Sun,Denghui Zhang,ChengXiang Zhai,Heng Ji*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Given the growing influence of language model-based agents on high-stakes
societal decisions, from public policy to healthcare, ensuring their beneficial
impact requires understanding the far-reaching implications of their
suggestions. We propose a proof-of-concept framework that projects how
model-generated advice could propagate through societal systems on a
macroscopic scale over time, enabling more robust alignment. To assess the
long-term safety awareness of language models, we also introduce a dataset of
100 indirect harm scenarios, testing models' ability to foresee adverse,
non-obvious outcomes from seemingly harmless user prompts. Our approach
achieves not only over 20% improvement on the new dataset but also an average
win rate exceeding 70% against strong baselines on existing safety benchmarks
(AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer
agents.

</details>


### [121] [Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?](https://arxiv.org/abs/2506.21215)
*Haoang Chi,He Li,Wenjing Yang,Feng Liu,Long Lan,Xiaoguang Ren,Tongliang Liu,Bo Han*

Key words: causal reasoning, large language models, G²-Reasoner, benchmark, level-1 reasoning, level-2 reasoning

TL;DR: The paper discusses the limitations of LLMs in performing genuine causal reasoning (level-2) and proposes a method (G²-Reasoner) to enhance this capability.

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: To address the gap in LLMs' causal reasoning abilities, aiming to advance them beyond shallow (level-1) reasoning towards human-like (level-2) reasoning.

Method: The study analyzes transformer-based LLMs' autoregression mechanism and introduces a new benchmark (CausalProbe-2024). It proposes G²-Reasoner, which integrates general knowledge and goal-oriented prompts.

Result: LLMs show a significant performance drop on CausalProbe-2024, confirming their limited (level-1) reasoning. G²-Reasoner improves reasoning, especially in fresh and counterfactual contexts.

Conclusion: The work provides a pathway for LLMs to achieve genuine causal reasoning, advancing beyond level-1 to level-2 capabilities.

Abstract: Causal reasoning capability is critical in advancing large language models
(LLMs) toward strong artificial intelligence. While versatile LLMs appear to
have demonstrated capabilities in understanding contextual causality and
providing responses that obey the laws of causality, it remains unclear whether
they perform genuine causal reasoning akin to humans. However, current evidence
indicates the contrary. Specifically, LLMs are only capable of performing
shallow (level-1) causal reasoning, primarily attributed to the causal
knowledge embedded in their parameters, but they lack the capacity for genuine
human-like (level-2) causal reasoning. To support this hypothesis,
methodologically, we delve into the autoregression mechanism of
transformer-based LLMs, revealing that it is not inherently causal.
Empirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,
whose corpora are fresh and nearly unseen for the studied LLMs. The LLMs
exhibit a significant performance drop on CausalProbe-2024 compared to earlier
benchmarks, indicating the fact that they primarily engage in level-1 causal
reasoning. To bridge the gap towards level-2 causal reasoning, we draw
inspiration from the fact that human reasoning is usually facilitated by
general knowledge and intended goals. We propose G^2-Reasoner, a method that
incorporates general knowledge and goal-oriented prompts into LLMs' causal
reasoning processes. Experiments demonstrate that G^2-Reasoner significantly
enhances LLMs' causal reasoning capability, particularly in fresh and
counterfactual contexts. This work sheds light on a new path for LLMs to
advance towards genuine causal reasoning, going beyond level-1 and making
strides towards level-2.

</details>


### [122] [World-aware Planning Narratives Enhance Large Vision-Language Model Planner](https://arxiv.org/abs/2506.21230)
*Junhao Shi,Zhaoye Fei,Siyin Wang,Qipeng Guo,Jingjing Gong,Xipeng QIu*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Vision-Language Models (LVLMs) show promise for embodied planning tasks
but struggle with complex scenarios involving unfamiliar environments and
multi-step goals. Current approaches rely on environment-agnostic imitation
learning that disconnects instructions from environmental contexts, causing
models to struggle with context-sensitive instructions and rely on
supplementary cues rather than visual reasoning during long-horizon
interactions. In this work, we propose World-Aware Planning Narrative
Enhancement (WAP), a framework that infuses LVLMs with comprehensive
environmental understanding through four cognitive capabilities (visual
appearance modeling, spatial reasoning, functional abstraction, and syntactic
grounding) while developing and evaluating models using only raw visual
observations through curriculum learning. Evaluations on the EB-ALFRED
benchmark demonstrate substantial improvements, with Qwen2.5-VL achieving a
60.7 absolute improvement in task success rates, particularly in commonsense
reasoning (+60.0) and long-horizon planning (+70.0). Notably, our enhanced
open-source models outperform proprietary systems like GPT-4o and
Claude-3.5-Sonnet by a large margin.

</details>


### [123] [IXAII: An Interactive Explainable Artificial Intelligence Interface for Decision Support Systems](https://arxiv.org/abs/2506.21310)
*Pauline Speckmann,Mario Nadj,Christian Janiesch*

Key words: 可解释AI,交互性,用户视角,LIME,SHAP,Anchors,DiCE

TL;DR: 论文介绍了一种名为IXAII的交互式可解释智能系统，融合了LIME、SHAP、Anchors和DiCE四种方法，并通过定制化视图和用户控制提升解释效果。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有后处理方法多为静态且忽略用户视角，限制了其实际效果，因此开发交互式系统以提升透明度和用户体验。

Method: 开发IXAII系统，整合四种解释方法，并为五类用户群体提供定制化视图和交互控制。通过专家和普通用户访谈评估系统。

Result: IXAII通过多样化解释和可视化选项被用户认为有助于提高透明度和实用性。

Conclusion: IXAII在可解释AI方法、交互性和实际应用之间架起桥梁，为AI解释实践和人机交互提供了新视角。

Abstract: Although several post-hoc methods for explainable AI have been developed,
most are static and neglect the user perspective, limiting their effectiveness
for the target audience. In response, we developed the interactive explainable
intelligent system called IXAII that offers explanations from four explainable
AI methods: LIME, SHAP, Anchors, and DiCE. Our prototype provides tailored
views for five user groups and gives users agency over the explanations'
content and their format. We evaluated IXAII through interviews with experts
and lay users. Our results indicate that IXAII, which provides different
explanations with multiple visualization options, is perceived as helpful to
increase transparency. By bridging the gaps between explainable AI methods,
interactivity, and practical implementation, we provide a novel perspective on
AI explanation practices and human-AI interaction.

</details>


### [124] [Active Inference AI Systems for Scientific Discovery](https://arxiv.org/abs/2506.21329)
*Karthik Duraisamy*

Key words: AI驱动的科学发现、主动推理、知识图谱、因果自监督、人类判断

TL;DR: 本文提出AI驱动的科学发展需解决抽象、推理和现实三大差距，而非依赖模型规模或数据。通过主动推理AI系统，结合因果自监督模型、贝叶斯规划器和知识图谱，实现内部模型与外部验证的互动，强调人类判断在此过程中的不可替代性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前AI系统因架构、推理机制薄弱及脱离实验现实而受限，需突破以推动科学发现的进步。

Method: 构建主动推理AI系统，结合因果自监督基础模型、符号/神经符号规划器和知识图谱，通过闭环交互优化内部表征。

Result: 提出一种架构，通过内部模型的反事实推理与外部验证的结合，促进科学发现，同时强调人类判断的持久作用。

Conclusion: 实现AI驱动的科学发现需解决核心差距，结合主动推理与人类判断，形成动态互动机制。

Abstract: The rapid evolution of artificial intelligence has led to expectations of
transformative scientific discovery, yet current systems remain fundamentally
limited by their operational architectures, brittle reasoning mechanisms, and
their separation from experimental reality. Building on earlier work, we
contend that progress in AI-driven science now depends on closing three
fundamental gaps -- the abstraction gap, the reasoning gap, and the reality gap
-- rather than on model size/data/test time compute. Scientific reasoning
demands internal representations that support simulation of actions and
response, causal structures that distinguish correlation from mechanism, and
continuous calibration. We define active inference AI systems for scientific
discovery as those that (i) maintain long-lived research memories grounded in
causal self-supervised foundation models, (ii) symbolic or neuro-symbolic
planners equipped with Bayesian guardrails, (iii) grow persistent knowledge
graphs where thinking generates novel conceptual nodes, reasoning establishes
causal edges, and real-world interaction prunes false connections while
strengthening verified pathways, and (iv) refine their internal representations
through closed-loop interaction with both high-fidelity simulators and
automated laboratories - an operational loop where mental simulation guides
action and empirical surprise reshapes understanding. In essence, we outline an
architecture where discovery arises from the interplay between internal models
that enable counterfactual reasoning and external validation that grounds
hypotheses in reality. It is also argued that the inherent ambiguity in
feedback from simulations and experiments, and underlying uncertainties makes
human judgment indispensable, not as a temporary scaffold but as a permanent
architectural component.

</details>


### [125] [TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in Multimodal Table Understanding](https://arxiv.org/abs/2506.21393)
*Junwen Zhang,Pu Chen,Yin Zhang*

Key words: 多模态表格理解, 神经符号推理, MoCE架构, WildStruct条件, 表格对齐数据集

TL;DR: 提出TableMoE架构，结合神经符号混合专家技术，解决了多模态表格理解的复杂性和鲁棒性问题，通过大规模数据集和基准测试验证其优越性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有MLLMs在真实场景中的多模态表格理解表现不佳，主要原因包括结构复杂、符号密度高和视觉退化，需提出新方法应对这些挑战。

Method: 采用神经符号混合专家（MoCE）架构，引入神经符号路由机制，动态分配表格元素到专家模块，并结合大规模对齐数据集TableMoE-Align进行预训练。

Result: TableMoE在四个WildStruct基准测试中显著优于现有模型，证明了其鲁棒性和泛化能力的提升。

Conclusion: 神经符号推理在多模态表格理解中具有关键作用，TableMoE的成功展示了其高效性和可解释性。

Abstract: Multimodal understanding of tables in real-world contexts is challenging due
to the complexity of structure, symbolic density, and visual degradation (blur,
skew, watermarking, incomplete structures or fonts, multi-span or
hierarchically nested layouts). Existing multimodal large language models
(MLLMs) struggle with such WildStruct conditions, resulting in limited
performance and poor generalization. To address these challenges, we propose
TableMoE, a neuro-symbolic Mixture-of-Connector-Experts (MoCE) architecture
specifically designed for robust, structured reasoning over multimodal table
data. TableMoE features an innovative Neuro-Symbolic Routing mechanism, which
predicts latent semantic token roles (e.g., header, data cell, axis, formula)
and dynamically routes table elements to specialized experts (Table-to-HTML,
Table-to-JSON, Table-to-Code) using a confidence-aware gating strategy informed
by symbolic reasoning graphs. To facilitate effective alignment-driven
pretraining, we introduce the large-scale TableMoE-Align dataset, consisting of
1.2M table-HTML-JSON-code quadruples across finance, science, biomedicine and
industry, utilized exclusively for model pretraining. For evaluation, we curate
and release four challenging WildStruct benchmarks: WMMFinQA, WMMTatQA,
WMMTabDialog, and WMMFinanceMath, designed specifically to stress-test models
under real-world multimodal degradation and structural complexity. Experimental
results demonstrate that TableMoE significantly surpasses existing
state-of-the-art models. Extensive ablation studies validate each core
component, emphasizing the critical role of Neuro-Symbolic Routing and
structured expert alignment. Through qualitative analyses, we further showcase
TableMoE's interpretability and enhanced robustness, underscoring the
effectiveness of integrating neuro-symbolic reasoning for multimodal table
understanding.

</details>


### [126] [Spatial Mental Modeling from Limited Views](https://arxiv.org/abs/2506.21458)
*Baiqiao Yin,Qineng Wang,Pingyue Zhang,Jianshu Zhang,Kangrui Wang,Zihan Wang,Jieyu Zhang,Keshigeyan Chandrasegaran,Han Liu,Ranjay Krishna,Saining Xie,Manling Li,Jiajun Wu,Li Fei-Fei*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Can Vision Language Models (VLMs) imagine the full scene from just a few
views, like humans do? Humans form spatial mental models, internal
representations of unseen space, to reason about layout, perspective, and
motion. Our new MindCube benchmark with 21,154 questions across 3,268 images
exposes this critical gap, where existing VLMs exhibit near-random performance.
Using MindCube, we systematically evaluate how well VLMs build robust spatial
mental models through representing positions (cognitive mapping), orientations
(perspective-taking), and dynamics (mental simulation for "what-if" movements).
We then explore three approaches to help VLMs approximate spatial mental
models, including unseen intermediate views, natural language reasoning chains,
and cognitive maps. The significant improvement comes from a synergistic
approach, "map-then-reason", that jointly trains the model to first generate a
cognitive map and then reason upon it. By training models to reason over these
internal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding
reinforcement learning pushed performance even further to 70.7% (+32.9%). Our
key insight is that such scaffolding of spatial mental models, actively
constructing and utilizing internal structured spatial representations with
flexible reasoning processes, significantly improves understanding of
unobservable space.

</details>


### [127] [Ad-Hoc Human-AI Coordination Challenge](https://arxiv.org/abs/2506.21490)
*Tin Dizdarević,Ravi Hammond,Tobias Gessler,Anisoara Calinescu,Jonathan Cook,Matteo Gallici,Andrei Lupu,Jakob Nicolaus Foerster*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Achieving seamless coordination between AI agents and humans is crucial for
real-world applications, yet it remains a significant open challenge. Hanabi is
a cooperative card game featuring imperfect information, constrained
communication, theory of mind requirements, and coordinated action -- making it
an ideal testbed for human-AI coordination. However, its use for human-AI
interaction has been limited by the challenges of human evaluation. In this
work, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to
overcome the constraints of costly and difficult-to-reproduce human
evaluations. We develop \textit{human proxy agents} on a large-scale human
dataset that serve as robust, cheap, and reproducible human-like evaluation
partners in AH2AC2. To encourage the development of data-efficient methods, we
open-source a dataset of 3,079 games, deliberately limiting the amount of
available human gameplay data. We present baseline results for both two- and
three- player Hanabi scenarios. To ensure fair evaluation, we host the proxy
agents through a controlled evaluation system rather than releasing them
publicly. The code is available at
\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}.

</details>


### [128] [Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge](https://arxiv.org/abs/2506.21506)
*Boyu Gou,Zanming Huang,Yuting Ning,Yu Gu,Michael Lin,Weijian Qi,Andrei Kopanev,Botao Yu,Bernal Jiménez Gutiérrez,Yiheng Shu,Chan Hee Song,Jiaman Wu,Shijie Chen,Hanane Nour Moussa,Tianshu Zhang,Jian Xie,Yifei Li,Tianci Xue,Zeyi Liao,Kai Zhang,Boyuan Zheng,Zhaowei Cai,Viktor Rozgic,Morteza Ziyadi,Huan Sun,Yu Su*

Key words: 代理搜索, Mind2Web 2, 评估基准, 信息合成, 自动评估

TL;DR: Mind2Web 2是一个包含130个高质量长时任务的新基准，用于评估代理搜索系统的实时浏览和信息合成能力，并提出了一种树形结构的自动评估框架。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 由于代理搜索系统的复杂性和开放性超出了现有基准的评估能力，因此需要一个更全面的评估方法。

Method: 通过构建任务特定的‘代理作为法官’框架，基于树形结构的设计自动评估答案正确性和来源归因。

Result: OpenAI Deep Research系统的表现达到人类水平的50-70%，且耗时减半，显示出巨大潜力。

Conclusion: Mind2Web 2为下一代代理搜索系统的开发和评估提供了严格基础。

Abstract: Agentic search such as Deep Research systems, where large language models
autonomously browse the web, synthesize information, and return comprehensive
citation-backed answers, represents a major shift in how users interact with
web-scale information. While promising greater efficiency and cognitive
offloading, the growing complexity and open-endedness of agentic search have
outpaced existing evaluation benchmarks and methodologies, which largely assume
short search horizons and static answers. In this paper, we introduce Mind2Web
2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that
require real-time web browsing and extensive information synthesis, constructed
with over 1,000 hours of human labor. To address the challenge of evaluating
time-varying and complex answers, we propose a novel Agent-as-a-Judge
framework. Our method constructs task-specific judge agents based on a
tree-structured rubric design to automatically assess both answer correctness
and source attribution. We conduct a comprehensive evaluation of nine frontier
agentic search systems and human performance, along with a detailed error
analysis to draw insights for future development. The best-performing system,
OpenAI Deep Research, can already achieve 50-70% of human performance while
spending half the time, showing a great potential. Altogether, Mind2Web 2
provides a rigorous foundation for developing and benchmarking the next
generation of agentic search systems.

</details>


### [129] [PsyLite Technical Report](https://arxiv.org/abs/2506.21536)
*Fangjun Ding,Renyu Zhang,Xinyu Feng,Chengye Xie,Zheng Zhang,Yanting Zhang*

Key words: AI心理辅导、轻量化模型、对话安全、条件RAG、量化技术

TL;DR: PsyLite是一个轻量级的心理辅导大语言模型，通过两阶段训练策略和条件RAG技术，提升了模型的推理能力、心理辅导能力和对话安全性，并在资源受限的环境中实现了高效部署。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决现有AI心理辅导模型在对话安全、场景处理和轻量化部署方面的不足。

Method: 基于InternLM2.5-7B-chat模型，采用混合蒸馏数据微调和ORPO偏好优化的两阶段训练策略，结合条件RAG技术和量化技术（GGUF q4_k_m）。

Result: 在CEval、CPsyCounE和SafeDialBench评估中表现优异，心理辅导专业性提升47.6%，对话安全性提升2.4%，内存占用仅5GB。

Conclusion: PsyLite为资源受限环境下的心理辅导应用提供了可行解决方案。

Abstract: With the rapid development of digital technology, AI-driven psychological
counseling has gradually become an important research direction in the field of
mental health. However, existing models still have deficiencies in dialogue
safety, detailed scenario handling, and lightweight deployment. To address
these issues, this study proposes PsyLite, a lightweight psychological
counseling large language model agent developed based on the base model
InternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation
data fine-tuning and ORPO preference optimization), PsyLite enhances the
model's deep-reasoning ability, psychological counseling ability, and safe
dialogue ability. After deployment using Ollama and Open WebUI, a custom
workflow is created with Pipelines. An innovative conditional RAG is designed
to introduce crosstalk humor elements at appropriate times during psychological
counseling to enhance user experience and decline dangerous requests to
strengthen dialogue safety. Evaluations show that PsyLite outperforms the
baseline models in the Chinese general evaluation (CEval), psychological
counseling professional evaluation (CPsyCounE), and dialogue safety evaluation
(SafeDialBench), particularly in psychological counseling professionalism
(CPsyCounE score improvement of 47.6\%) and dialogue safety (\safe{} score
improvement of 2.4\%). Additionally, the model uses quantization technology
(GGUF q4\_k\_m) to achieve low hardware deployment (5GB memory is sufficient
for operation), providing a feasible solution for psychological counseling
applications in resource-constrained environments.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [130] [Evaluating PDE discovery methods for multiscale modeling of biological signals](https://arxiv.org/abs/2506.20694)
*Andréa Ducos,Audrey Denizot,Thomas Guyet,Hugues Berry*

Key words: PDE发现，跨尺度建模，粒子仿真，钙扩散

TL;DR: 该论文提出了一种结合基于粒子的仿真和偏微分方程（PDE）发现的框架，用于解决生物系统跨尺度动态建模的挑战。实验评估了五种PDE发现方法在钙扩散模拟中的表现。

<details>
  <summary>Details</summary>

Main category: q-bio.QM

Motivation: 生物系统的非线性、未知变量和未知物理原理使其动态行为表征极具挑战性，尤其是其活动发生在多个相互依赖的空间和时间尺度上。

Method: 通过结合基于粒子的仿真和PDE发现，评估五种PDE发现方法在星形胶质细胞中钙扩散模拟中的表现。

Result: 实验结果表明，多种方法能准确恢复扩散项，表明PDE发现具有从微观数据中捕捉宏观动态的潜力。

Conclusion: PDE发现为生物系统的跨尺度建模提供了可行方法，尤其是在捕捉宏观动态方面表现出潜力。

Abstract: Biological systems are non-linear, include unobserved variables and the
physical principles that govern their dynamics are partly unknown. This makes
the characterization of their behavior very challenging. Notably, their
activity occurs on multiple interdependent spatial and temporal scales that
require linking mechanisms across scales. To address the challenge of bridging
gaps between scales, we leverage partial differential equations (PDE)
discovery. PDE discovery suggests meso-scale dynamics characteristics from
micro-scale data. In this article, we present our framework combining
particle-based simulations and PDE discovery and conduct preliminary
experiments to assess equation discovery in controlled settings. We evaluate
five state-of-the-art PDE discovery methods on particle-based simulations of
calcium diffusion in astrocytes. The performances of the methods are evaluated
on both the form of the discovered equation and the forecasted temporal
variations of calcium concentration. Our results show that several methods
accurately recover the diffusion term, highlighting the potential of PDE
discovery for capturing macroscopic dynamics in biological systems from
microscopic data.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [131] [Global and Local Contrastive Learning for Joint Representations from Cardiac MRI and ECG](https://arxiv.org/abs/2506.20683)
*Alexander Selivanov,Philip Müller,Özgün Turgut,Nil Stolt-Ansó,Daniel Rückert*

Key words: ECG, CMR, 对比学习, 心脏功能

TL;DR: 提出多模态对比学习框架PTACL，通过整合CMR的时空信息增强ECG表示，实现更好的心脏功能参数预测和相似表型患者检索。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: ECG虽广泛使用但无法直接测量心脏功能参数，CMR虽精准但昂贵且不易获得，需找到二者间桥梁。

Method: PTACL结合全局患者级和局部时间级对比损失，对齐ECG与CMR表示，无需增加可学习参数。

Result: 在UK Biobank数据上证实PTACL在患者检索和心脏功能参数预测上优于基线方法。

Conclusion: PTACL能提升ECG在非侵入性心脏诊断中的潜力。

Abstract: An electrocardiogram (ECG) is a widely used, cost-effective tool for
detecting electrical abnormalities in the heart. However, it cannot directly
measure functional parameters, such as ventricular volumes and ejection
fraction, which are crucial for assessing cardiac function. Cardiac magnetic
resonance (CMR) is the gold standard for these measurements, providing detailed
structural and functional insights, but is expensive and less accessible. To
bridge this gap, we propose PTACL (Patient and Temporal Alignment Contrastive
Learning), a multimodal contrastive learning framework that enhances ECG
representations by integrating spatio-temporal information from CMR. PTACL uses
global patient-level contrastive loss and local temporal-level contrastive
loss. The global loss aligns patient-level representations by pulling ECG and
CMR embeddings from the same patient closer together, while pushing apart
embeddings from different patients. Local loss enforces fine-grained temporal
alignment within each patient by contrasting encoded ECG segments with
corresponding encoded CMR frames. This approach enriches ECG representations
with diagnostic information beyond electrical activity and transfers more
insights between modalities than global alignment alone, all without
introducing new learnable weights. We evaluate PTACL on paired ECG-CMR data
from 27,951 subjects in the UK Biobank. Compared to baseline approaches, PTACL
achieves better performance in two clinically relevant tasks: (1) retrieving
patients with similar cardiac phenotypes and (2) predicting CMR-derived cardiac
function parameters, such as ventricular volumes and ejection fraction. Our
results highlight the potential of PTACL to enhance non-invasive cardiac
diagnostics using ECG. The code is available at:
https://github.com/alsalivan/ecgcmr

</details>


### [132] [U-R-VEDA: Integrating UNET, Residual Links, Edge and Dual Attention, and Vision Transformer for Accurate Semantic Segmentation of CMRs](https://arxiv.org/abs/2506.20689)
*Racheal Mukisa,Arvind K. Bansal*

Key words: 深度学习, UNet, 心脏磁共振图像, 语义分割, 注意力机制

TL;DR: 本文提出了一种基于深度学习的增强型UNet模型U-R-Veda，用于心脏磁共振图像的语义分割，通过结合卷积变换、视觉变换器、注意力机制等技术，显著提高了分割精度。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 自动化准确的心脏图像分割是量化与自动化诊断心脏疾病的第一步，当前的深度学习方法在此任务中存在一定的局限性。

Method: U-R-Veda模型整合了卷积变换、视觉变换器、残差连接、通道注意力和空间注意力机制，并引入边缘检测跳过连接以减少信息损失。

Result: 模型在DSC指标下达到了95.2%的平均准确率，尤其在右心室和左心室心肌的分割上优于其他模型。

Conclusion: U-R-Veda模型显著改善了心脏磁共振图像的语义分割性能，为医学图像分析提供了更可靠的自动化工具。

Abstract: Artificial intelligence, including deep learning models, will play a
transformative role in automated medical image analysis for the diagnosis of
cardiac disorders and their management. Automated accurate delineation of
cardiac images is the first necessary initial step for the quantification and
automated diagnosis of cardiac disorders. In this paper, we propose a deep
learning based enhanced UNet model, U-R-Veda, which integrates convolution
transformations, vision transformer, residual links, channel-attention, and
spatial attention, together with edge-detection based skip-connections for an
accurate fully-automated semantic segmentation of cardiac magnetic resonance
(CMR) images. The model extracts local-features and their interrelationships
using a stack of combination convolution blocks, with embedded channel and
spatial attention in the convolution block, and vision transformers. Deep
embedding of channel and spatial attention in the convolution block identifies
important features and their spatial localization. The combined edge
information with channel and spatial attention as skip connection reduces
information-loss during convolution transformations. The overall model
significantly improves the semantic segmentation of CMR images necessary for
improved medical image analysis. An algorithm for the dual attention module
(channel and spatial attention) has been presented. Performance results show
that U-R-Veda achieves an average accuracy of 95.2%, based on DSC metrics. The
model outperforms the accuracy attained by other models, based on DSC and HD
metrics, especially for the delineation of right-ventricle and
left-ventricle-myocardium.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [133] [Transformer-Based Spatial-Temporal Counterfactual Outcomes Estimation](https://arxiv.org/abs/2506.21154)
*He Li,Haoang Chi,Mingyu Liu,Wanrong Huang,Liyang Xu,Wenjing Yang*

Key words: 反事实结果估计, Transformer, 时空属性, 因果效应

TL;DR: 该论文提出了一种基于Transformer的新框架，用于估计具有时空属性的反事实结果，表现出更强的估计能力，通过仿真和真实数据实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: stat.ME

Motivation: 现实世界具有时间和空间维度，而现有的反事实结果估计方法基于传统统计模型，性能有限，因此需要一种更强大的方法。

Method: 采用Transformer框架估计时空属性的反事实结果，假设条件下证明了估计器的无偏性和渐近正态性。

Result: 仿真实验表明，该方法优于基线方法；真实数据实验揭示了哥伦比亚冲突对森林丧失的因果效应。

Conclusion: 该框架能有效估计时空反事实结果，为因果推理提供了新工具。

Abstract: The real world naturally has dimensions of time and space. Therefore,
estimating the counterfactual outcomes with spatial-temporal attributes is a
crucial problem. However, previous methods are based on classical statistical
models, which still have limitations in performance and generalization. This
paper proposes a novel framework for estimating counterfactual outcomes with
spatial-temporal attributes using the Transformer, exhibiting stronger
estimation ability. Under mild assumptions, the proposed estimator within this
framework is consistent and asymptotically normal. To validate the
effectiveness of our approach, we conduct simulation experiments and real data
experiments. Simulation experiments show that our estimator has a stronger
estimation capability than baseline methods. Real data experiments provide a
valuable conclusion to the causal effect of conflicts on forest loss in
Colombia. The source code is available at
https://github.com/lihe-maxsize/DeppSTCI_Release_Version-master.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [134] [Hybrid Deep Learning and Signal Processing for Arabic Dialect Recognition in Low-Resource Settings](https://arxiv.org/abs/2506.21386)
*Ghazal Al-Shwayyat,Omer Nezih Gerek*

Key words: 阿拉伯语方言识别, 混合模型, MFCC, CNN, 低资源场景

TL;DR: 研究探讨了结合传统信号处理和深度学习的混合模型在低资源阿拉伯语方言识别中的效果，MFCC+CNN模型表现最佳（准确率91.2%），优于Wavelet+RNN模型，为未来研究提供了基准。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 阿拉伯语方言识别因语言多样性和标注数据稀缺而具有挑战性，尤其是在低资源场景下，研究旨在提出有效的解决方法。

Method: 开发了两种混合模型：1）MFCC特征与CNN结合；2）DWT特征与RNN结合，并在Common Voice阿拉伯语数据集的方言子集上训练。

Result: MFCC+CNN模型表现最优，准确率为91.2%，显著优于Wavelet+RNN模型的66.5%，显示了卷积模型在频谱特征处理上的优势。

Conclusion: 研究表明在数据有限时，结合频谱特征和卷积模型的策略更为有效，并为未来研究指出了方向，如使用更大数据集和自监督学习技术。

Abstract: Arabic dialect recognition presents a significant challenge in speech
technology due to the linguistic diversity of Arabic and the scarcity of large
annotated datasets, particularly for underrepresented dialects. This research
investigates hybrid modeling strategies that integrate classical signal
processing techniques with deep learning architectures to address this problem
in low-resource scenarios. Two hybrid models were developed and evaluated: (1)
Mel-Frequency Cepstral Coefficients (MFCC) combined with a Convolutional Neural
Network (CNN), and (2) Discrete Wavelet Transform (DWT) features combined with
a Recurrent Neural Network (RNN). The models were trained on a dialect-filtered
subset of the Common Voice Arabic dataset, with dialect labels assigned based
on speaker metadata. Experimental results demonstrate that the MFCC + CNN
architecture achieved superior performance, with an accuracy of 91.2% and
strong precision, recall, and F1-scores, significantly outperforming the
Wavelet + RNN configuration, which achieved an accuracy of 66.5%. These
findings highlight the effectiveness of leveraging spectral features with
convolutional models for Arabic dialect recognition, especially when working
with limited labeled data. The study also identifies limitations related to
dataset size, potential regional overlaps in labeling, and model optimization,
providing a roadmap for future research. Recommendations for further
improvement include the adoption of larger annotated corpora, integration of
self-supervised learning techniques, and exploration of advanced neural
architectures such as Transformers. Overall, this research establishes a strong
baseline for future developments in Arabic dialect recognition within
resource-constrained environments.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [135] [CovDocker: Benchmarking Covalent Drug Design with Tasks, Datasets, and Solutions](https://arxiv.org/abs/2506.21085)
*Yangzhe Peng,Kaiyuan Gao,Liang He,Yuheng Cong,Haiguang Liu,Kun He,Lijun Wu*

Key words: 共价对接，分子对接，共价药物设计，Uni-Mol，Chemformer

TL;DR: 该论文提出了一个名为CovDocker的共价对接基准，旨在解决现有方法难以处理共价键形成及其结构变化的问题，通过分解为三个任务并采用先进模型展示了其有效性。

<details>
  <summary>Details</summary>

Main category: q-bio.BM

Motivation: 现有分子对接方法和深度学习模型很少考虑共价键的形成及结构变化，因此需要一个更全面的共价对接基准来填补这一空白。

Method: 将共价对接过程分解为反应位点预测、共价反应预测和共价对接三个任务，并采用Uni-Mol和Chemformer等先进模型建立基准性能。

Result: 基准测试能准确预测相互作用位点并建模共价结合中的分子变换，验证了其作为共价药物设计研究严格框架的有效性。

Conclusion: CovDocker为共价药物设计提供了数据驱动方法，有望加速选择性共价抑制剂的发现并解决治疗开发中的关键挑战。

Abstract: Molecular docking plays a crucial role in predicting the binding mode of
ligands to target proteins, and covalent interactions, which involve the
formation of a covalent bond between the ligand and the target, are
particularly valuable due to their strong, enduring binding nature. However,
most existing docking methods and deep learning approaches hardly account for
the formation of covalent bonds and the associated structural changes. To
address this gap, we introduce a comprehensive benchmark for covalent docking,
CovDocker, which is designed to better capture the complexities of covalent
binding. We decompose the covalent docking process into three main tasks:
reactive location prediction, covalent reaction prediction, and covalent
docking. By adapting state-of-the-art models, such as Uni-Mol and Chemformer,
we establish baseline performances and demonstrate the effectiveness of the
benchmark in accurately predicting interaction sites and modeling the molecular
transformations involved in covalent binding. These results confirm the role of
the benchmark as a rigorous framework for advancing research in covalent drug
design. It underscores the potential of data-driven approaches to accelerate
the discovery of selective covalent inhibitors and addresses critical
challenges in therapeutic development.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [136] [Poster: Enhancing GNN Robustness for Network Intrusion Detection via Agent-based Analysis](https://arxiv.org/abs/2506.20806)
*Zhonghao Zhan,Huichi Zhou,Hamed Haddadi*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Graph Neural Networks (GNNs) show great promise for Network Intrusion
Detection Systems (NIDS), particularly in IoT environments, but suffer
performance degradation due to distribution drift and lack robustness against
realistic adversarial attacks. Current robustness evaluations often rely on
unrealistic synthetic perturbations and lack demonstrations on systematic
analysis of different kinds of adversarial attack, which encompass both
black-box and white-box scenarios. This work proposes a novel approach to
enhance GNN robustness and generalization by employing Large Language Models
(LLMs) in an agentic pipeline as simulated cybersecurity expert agents. These
agents scrutinize graph structures derived from network flow data, identifying
and potentially mitigating suspicious or adversarially perturbed elements
before GNN processing. Our experiments, using a framework designed for
realistic evaluation and testing with a variety of adversarial attacks
including a dataset collected from physical testbed experiments, demonstrate
that integrating LLM analysis can significantly improve the resilience of
GNN-based NIDS against challenges, showcasing the potential of LLM agent as a
complementary layer in intrusion detection architectures.

</details>


### [137] [ZKPROV: A Zero-Knowledge Approach to Dataset Provenance for Large Language Models](https://arxiv.org/abs/2506.20915)
*Mina Namazi,Alexander Nemecek,Erman Ayday*

Key words: 大型语言模型,零知识证明,数据隐私,训练来源验证

TL;DR: ZKPROV是一个新颖的加密框架，通过零知识证明验证大型语言模型的训练数据来源，确保隐私与可信性。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 在敏感领域（如医疗）部署大型语言模型时，验证其训练数据的来源和完整性是关键挑战。

Method: ZKPROV利用零知识证明和签名元数据，将训练模型与授权数据集绑定，避免完整训练的验证成本。

Result: 实验证明ZKPROV高效、可扩展，能够提供隐私保护的可信数据来源验证。

Conclusion: ZKPROV在确保数据集机密性的同时，提供了实用的可信数据来源验证解决方案。

Abstract: As the deployment of large language models (LLMs) grows in sensitive domains,
ensuring the integrity of their computational provenance becomes a critical
challenge, particularly in regulated sectors such as healthcare, where strict
requirements are applied in dataset usage. We introduce ZKPROV, a novel
cryptographic framework that enables zero-knowledge proofs of LLM provenance.
It allows users to verify that a model is trained on a reliable dataset without
revealing sensitive information about it or its parameters. Unlike prior
approaches that focus on complete verification of the training process
(incurring significant computational cost) or depend on trusted execution
environments, ZKPROV offers a distinct balance. Our method cryptographically
binds a trained model to its authorized training dataset(s) through
zero-knowledge proofs while avoiding proof of every training step. By
leveraging dataset-signed metadata and compact model parameter commitments,
ZKPROV provides sound and privacy-preserving assurances that the result of the
LLM is derived from a model trained on the claimed authorized and relevant
dataset. Experimental results demonstrate the efficiency and scalability of the
ZKPROV in generating this proof and verifying it, achieving a practical
solution for real-world deployments. We also provide formal security
guarantees, proving that our approach preserves dataset confidentiality while
ensuring trustworthy dataset provenance.

</details>


### [138] [PhishKey: A Novel Centroid-Based Approach for Enhanced Phishing Detection Using Adaptive HTML Component Extraction](https://arxiv.org/abs/2506.21106)
*Felipe Castaño,Eduardo Fidalgo,Enrique Alegre,Rocio Alaiz-Rodríguez,Raul Orduna,Francesco Zola*

Key words: 钓鱼检测, 特征提取, CNN, CAPE, 集成学习

TL;DR: 提出了一种名为PhishKey的新型钓鱼检测方法，通过自动特征提取和集成学习提高检测效果。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 钓鱼攻击的快速演变为检测带来了挑战，现有方法在适应性、鲁棒性和效率方面存在不足。

Method: 结合字符级处理的CNN用于URL分类，以及CAPE用于HTML内容的单词级处理，通过软投票集成预测结果。

Result: 在四个数据集上达到了98.70%的F1分数，对对抗性攻击表现出较强的抵抗力。

Conclusion: PhishKey在钓鱼检测中表现出高效、稳健和适应性强的特点。

Abstract: Phishing attacks pose a significant cybersecurity threat, evolving rapidly to
bypass detection mechanisms and exploit human vulnerabilities. This paper
introduces PhishKey to address the challenges of adaptability, robustness, and
efficiency. PhishKey is a novel phishing detection method using automatic
feature extraction from hybrid sources. PhishKey combines character-level
processing with Convolutional Neural Networks (CNN) for URL classification, and
a Centroid-Based Key Component Phishing Extractor (CAPE) for HTML content at
the word level. CAPE reduces noise and ensures complete sample processing
avoiding crop operations on the input data. The predictions from both modules
are integrated using a soft-voting ensemble to achieve more accurate and
reliable classifications. Experimental evaluations on four state-of-the-art
datasets demonstrate the effectiveness of PhishKey. It achieves up to 98.70% F1
Score and shows strong resistance to adversarial manipulations such as
injection attacks with minimal performance degradation.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [139] [Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware Diffusion and Temporal Video Models](https://arxiv.org/abs/2506.20946)
*Donggoo Kang,Jangyeong Kim,Dasol Jeong,Junyoung Choi,Jeonga Wi,Hyunmin Lee,Joonho Gwon,Joonki Paik*

Key words: 纹理合成, 视频生成, 几何感知, UV扩散, 时间一致性

TL;DR: VideoTex利用视频生成模型解决3D纹理中的空间和时间不一致问题，结合几何感知条件和UV扩散策略，生成更平滑、更一致的纹理。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 现有纹理合成方法因缺乏全局上下文和几何理解而存在不一致性，而视频生成模型在时间一致性方面表现出色，因此提出VideoTex框架。

Method: 结合几何感知条件和结构性的UV扩散策略，利用3D网格结构精确生成纹理。

Result: 实验表明，VideoTex在纹理保真度、接缝融合和时间稳定性上均优于现有方法。

Conclusion: VideoTex为同时需要视觉质量和时间一致性的动态实时应用提供了新途径。

Abstract: Current texture synthesis methods, which generate textures from fixed
viewpoints, suffer from inconsistencies due to the lack of global context and
geometric understanding. Meanwhile, recent advancements in video generation
models have demonstrated remarkable success in achieving temporally consistent
videos. In this paper, we introduce VideoTex, a novel framework for seamless
texture synthesis that leverages video generation models to address both
spatial and temporal inconsistencies in 3D textures. Our approach incorporates
geometry-aware conditions, enabling precise utilization of 3D mesh structures.
Additionally, we propose a structure-wise UV diffusion strategy, which enhances
the generation of occluded areas by preserving semantic information, resulting
in smoother and more coherent textures. VideoTex not only achieves smoother
transitions across UV boundaries but also ensures high-quality, temporally
stable textures across video frames. Extensive experiments demonstrate that
VideoTex outperforms existing methods in texture fidelity, seam blending, and
stability, paving the way for dynamic real-time applications that demand both
visual quality and temporal coherence.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [140] [IMC-PINN-FE: A Physics-Informed Neural Network for Patient-Specific Left Ventricular Finite Element Modeling with Image Motion Consistency and Biomechanical Parameter Estimation](https://arxiv.org/abs/2506.20696)
*Siyu Mu,Wei Xuan Chan,Choon Hwai Yap*

Key words: biomechanics, finite element modeling, physics-informed neural networks, cardiac motion, personalized medicine

TL;DR: 提出了一种结合物理信息神经网络（PINN）和有限元（FE）建模的方法IMC-PINN-FE，用于快速估计心肌刚度和主动张力，显著提升计算速度和精度。

<details>
  <summary>Details</summary>

Main category: physics.med-ph

Motivation: 传统有限元方法计算成本高且难以准确再现心脏运动，因此需要一种更高效、精确的患者特异性心脏生物力学建模方法。

Method: 通过MRI或超声心动图估计心脏运动，利用PINN框架整合图像运动一致性（IMC）和有限元建模，快速估计心肌参数并加速计算。

Result: IMC-PINN-FE将计算时间从数小时缩短至数秒，提升了运动匹配精度（Dice从0.849提高到0.927），同时保持了真实的压力-体积行为。

Conclusion: IMC-PINN-FE提供了一种快速、个性化且图像一致的生物力学建模方法，优于传统模型。

Abstract: Elucidating the biomechanical behavior of the myocardium is crucial for
understanding cardiac physiology, but cannot be directly inferred from clinical
imaging and typically requires finite element (FE) simulations. However,
conventional FE methods are computationally expensive and often fail to
reproduce observed cardiac motions. We propose IMC-PINN-FE, a physics-informed
neural network (PINN) framework that integrates imaged motion consistency (IMC)
with FE modeling for patient-specific left ventricular (LV) biomechanics.
Cardiac motion is first estimated from MRI or echocardiography using either a
pre-trained attention-based network or an unsupervised cyclic-regularized
network, followed by extraction of motion modes. IMC-PINN-FE then rapidly
estimates myocardial stiffness and active tension by fitting clinical pressure
measurements, accelerating computation from hours to seconds compared to
traditional inverse FE. Based on these parameters, it performs FE modeling
across the cardiac cycle at 75x speedup. Through motion constraints, it matches
imaged displacements more accurately, improving average Dice from 0.849 to
0.927, while preserving realistic pressure-volume behavior. IMC-PINN-FE
advances previous PINN-FE models by introducing back-computation of material
properties and better motion fidelity. Using motion from a single subject to
reconstruct shape modes also avoids the need for large datasets and improves
patient specificity. IMC-PINN-FE offers a robust and efficient approach for
rapid, personalized, and image-consistent cardiac biomechanical modeling.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [141] [Leveraging Vision-Language Models to Select Trustworthy Super-Resolution Samples Generated by Diffusion Models](https://arxiv.org/abs/2506.20832)
*Cansu Korkmaz,Ahmet Murat Tekalp,Zafer Dogan*

Key words: 超分辨率, 扩散模型, 视觉语言模型, 信任度评分, 语义正确性

TL;DR: 本文提出了一种基于视觉语言模型（VLM）的自动化框架，用于从扩散模型生成的多张超分辨率（SR）图像中选择最值得信赖的样本，并通过新型的信任度评分（TWS）定量评估其可靠性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的SR模型在平衡保真度和感知质量时往往引入人工伪影，而扩散模型生成的多样SR图像难以选择最可靠的解决方案。本文旨在解决这一挑战，提升SR结果的信任度。

Method: 利用BLIP-2和GPT-4o等VLM模型，通过结构化查询评估语义正确性、视觉质量和伪影情况，并设计TWS评分（基于CLIP嵌入、SSIM边缘图和多级小波分解）量化SR可靠性。

Result: 实验表明，TWS与人类偏好高度相关，VLM引导的选择能稳定获得高TWS值，优于传统指标如PSNR和LPIPS。

Conclusion: 本文为扩散模型生成的SR图像提供了一种可扩展、通用化的可信选择方案，为生成式SR设定了新的信任度基准。

Abstract: Super-resolution (SR) is an ill-posed inverse problem with many feasible
solutions consistent with a given low-resolution image. On one hand, regressive
SR models aim to balance fidelity and perceptual quality to yield a single
solution, but this trade-off often introduces artifacts that create ambiguity
in information-critical applications such as recognizing digits or letters. On
the other hand, diffusion models generate a diverse set of SR images, but
selecting the most trustworthy solution from this set remains a challenge. This
paper introduces a robust, automated framework for identifying the most
trustworthy SR sample from a diffusion-generated set by leveraging the semantic
reasoning capabilities of vision-language models (VLMs). Specifically, VLMs
such as BLIP-2, GPT-4o, and their variants are prompted with structured queries
to assess semantic correctness, visual quality, and artifact presence. The
top-ranked SR candidates are then ensembled to yield a single trustworthy
output in a cost-effective manner. To rigorously assess the validity of
VLM-selected samples, we propose a novel Trustworthiness Score (TWS) a hybrid
metric that quantifies SR reliability based on three complementary components:
semantic similarity via CLIP embeddings, structural integrity using SSIM on
edge maps, and artifact sensitivity through multi-level wavelet decomposition.
We empirically show that TWS correlates strongly with human preference in both
ambiguous and natural images, and that VLM-guided selections consistently yield
high TWS values. Compared to conventional metrics like PSNR, LPIPS, which fail
to reflect information fidelity, our approach offers a principled, scalable,
and generalizable solution for navigating the uncertainty of the diffusion SR
space. By aligning outputs with human expectations and semantic correctness,
this work sets a new benchmark for trustworthiness in generative SR.

</details>


### [142] [FixCLR: Negative-Class Contrastive Learning for Semi-Supervised Domain Generalization](https://arxiv.org/abs/2506.20841)
*Ha Min Son,Shahbaz Rezaei,Xin Liu*

Key words: 半监督域泛化, 对比学习, 伪标签, 域不变表示

TL;DR: 论文提出FixCLR，一种半监督域泛化（SSDG）方法，通过结合对比学习和伪标签信息来显式促进域不变表示，提升泛化性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决在标签稀缺情况下域泛化方法性能不佳的问题，并显式学习域不变表示。

Method: 提出FixCLR，结合对比学习和伪标签信息，仅使用排斥项促进域不变性。

Result: FixCLR在SSDG任务中表现优异，尤其是与其他半监督方法结合时。

Conclusion: FixCLR是一种有效的SSDG方法，可显式提升域泛化能力。

Abstract: Semi-supervised domain generalization (SSDG) aims to solve the problem of
generalizing to out-of-distribution data when only a few labels are available.
Due to label scarcity, applying domain generalization methods often
underperform. Consequently, existing SSDG methods combine semi-supervised
learning methods with various regularization terms. However, these methods do
not explicitly regularize to learn domains invariant representations across all
domains, which is a key goal for domain generalization. To address this, we
introduce FixCLR. Inspired by success in self-supervised learning, we change
two crucial components to adapt contrastive learning for explicit domain
invariance regularization: utilization of class information from pseudo-labels
and using only a repelling term. FixCLR can also be added on top of most
existing SSDG and semi-supervised methods for complementary performance
improvements. Our research includes extensive experiments that have not been
previously explored in SSDG studies. These experiments include benchmarking
different improvements to semi-supervised methods, evaluating the performance
of pretrained versus non-pretrained models, and testing on datasets with many
domains. Overall, FixCLR proves to be an effective SSDG method, especially when
combined with other semi-supervised methods.

</details>


### [143] [THIRDEYE: Cue-Aware Monocular Depth Estimation via Brain-Inspired Multi-Stage Fusion](https://arxiv.org/abs/2506.20877)
*Calin Teodor Ioan*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Monocular depth estimation methods traditionally train deep models to infer
depth directly from RGB pixels. This implicit learning often overlooks explicit
monocular cues that the human visual system relies on, such as occlusion
boundaries, shading, and perspective. Rather than expecting a network to
discover these cues unaided, we present ThirdEye, a cue-aware pipeline that
deliberately supplies each cue through specialised, pre-trained, and frozen
networks. These cues are fused in a three-stage cortical hierarchy (V1->V2->V3)
equipped with a key-value working-memory module that weights them by
reliability. An adaptive-bins transformer head then produces a high-resolution
disparity map. Because the cue experts are frozen, ThirdEye inherits large
amounts of external supervision while requiring only modest fine-tuning. This
extended version provides additional architectural detail, neuroscientific
motivation, and an expanded experimental protocol; quantitative results will
appear in a future revision.

</details>


### [144] [HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context](https://arxiv.org/abs/2506.21277)
*Qize Yang,Shimin Yao,Weixuan Chen,Shenghao Fu,Detao Bai,Jiaxing Zhao,Boyuan Sun,Bowen Yin,Xihan Wei,Jingren Zhou*

Key words: 多模态大语言模型, 强化学习, 全局上下文, 逻辑推理, IntentBench

TL;DR: 本文针对多模态大语言模型在理解全局上下文时的不足和捷径问题，提出了结合强化学习的解决方案，并通过新的基准测试IntentBench验证了其优越性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 多模态大语言模型在理解人类意图时存在全局上下文理解不足和捷径问题，需要改进以提升推理能力。

Method: 通过结合强化学习（RL），设计上下文、格式、准确性和逻辑奖励，利用大语言模型评估逻辑奖励，确保多模态信息的全面整合。

Result: 提出的方法在多个全模态基准测试中表现优于其他开源模型，尤其是在理解复杂意图和情感方面。

Conclusion: 强化学习的引入显著提升了多模态大语言模型的推理能力，特别是在全局上下文理解和逻辑整合方面。

Abstract: With the rapid evolution of multimodal large language models, the capacity to
deeply understand and interpret human intentions has emerged as a critical
capability, which demands detailed and thoughtful reasoning. In recent studies,
Reinforcement Learning (RL) has demonstrated potential in enhancing the
reasoning capabilities of Large Language Models (LLMs). Nonetheless, the
challenges associated with adapting RL to multimodal data and formats remain
largely unaddressed. In this paper, we identify two issues in existing
multimodal reasoning models: insufficient global context understanding and
shortcut problems. Insufficient context understanding can happen when a model
misinterprets multimodal context, resulting in incorrect answers. The shortcut
problem occurs when the model overlooks crucial clues in multimodal inputs,
directly addressing the query without considering the multimodal information.
To tackle these issues, we emphasize the necessity for the model to reason with
a clear understanding of the global context within multimodal inputs. This
global context understanding can effectively prevent the model from overlooking
key multimodal cues and ensure a thorough reasoning process. To ensure the
accurate interpretation of multimodal context information, we implement a
context reward judged by a large language model, alongside format and accuracy
rewards. Additionally, to improve complex reasoning capability, we employ the
LLM to assess the logical reward, determining whether the reasoning process
successfully integrates multimodal information with logical methods. We also
introduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating
models in understanding complex human intentions and emotions. Our proposed
method demonstrates advanced performance across multiple omni-modal benchmarks
compared to other open-source omni-modal models.

</details>


### [145] [OmniEval: A Benchmark for Evaluating Omni-modal Models with Visual, Auditory, and Textual Inputs](https://arxiv.org/abs/2506.20960)
*Yiman Zhang,Ziheng Luo,Qiangyu Yan,Wei He,Borui Jiang,Xinghao Chen,Kai Han*

Key words: 多模态模型,基准测试,OmniEval,音视频同步,任务多样性

TL;DR: 介绍了OmniEval基准，用于评估多模态模型，具备全模态协作、多样化视频和任务等特征。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 为评估像MiniCPM-O 2.6这样的多模态模型的性能，设计了一个涵盖视觉、听觉和文本输入的基准。

Method: 设计了全模态协作的评估任务，包含810个音视频同步视频和2617个问答对，分为3大类任务和12个子任务。

Result: 在OmniEval上对多个多模态模型进行了实验，验证了基准的有效性和全面性。

Conclusion: OmniEval为评估多模态模型的构建和理解能力提供了平台，代码和数据已公开。

Abstract: In this paper, we introduce OmniEval, a benchmark for evaluating
omni-modality models like MiniCPM-O 2.6, which encompasses visual, auditory,
and textual inputs. Compared with existing benchmarks, our OmniEval has several
distinctive features: (i) Full-modal collaboration: We design evaluation tasks
that highlight the strong coupling between audio and video, requiring models to
effectively leverage the collaborative perception of all modalities; (ii)
Diversity of videos: OmniEval includes 810 audio-visual synchronized videos,
285 Chinese videos and 525 English videos; (iii) Diversity and granularity of
tasks: OmniEval contains 2617 question-answer pairs, comprising 1412 open-ended
questions and 1205 multiple-choice questions. These questions are divided into
3 major task types and 12 sub-task types to achieve comprehensive evaluation.
Among them, we introduce a more granular video localization task named
Grounding. Then we conduct experiments on OmniEval with several omni-modality
models. We hope that our OmniEval can provide a platform for evaluating the
ability to construct and understand coherence from the context of all
modalities. Codes and data could be found at https://omnieval.github.io/.

</details>


### [146] [Evidence-based diagnostic reasoning with multi-agent copilot for human pathology](https://arxiv.org/abs/2506.20964)
*Chengkuan Chen,Luca L. Weishaupt,Drew F. K. Williamson,Richard J. Chen,Tong Ding,Bowen Chen,Anurag Vaidya,Long Phi Le,Guillaume Jaume,Ming Y. Lu,Faisal Mahmood*

Key words: PathChat+, SlideSeek, 多模态大语言模型, 病理学, 自主诊断推理

TL;DR: PathChat+和SlideSeek是解决病理学中多模态大语言模型局限性（如数据不足和多图像理解不足）的新系统，PathChat+在多项基准测试中表现卓越。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统病理学模型缺乏自然语言指令和文本上下文整合，当前多模态大语言模型也存在数据不足、多图像理解支持不足等问题。

Method: PathChat+训练了超100万条病理学指令样本和550万问答对，SlideSeek利用PathChat+对全切片图像进行自主诊断推理。

Result: PathChat+显著优于现有模型，SlideSeek在DDxBench基准测试中表现优异，并能生成可视化报告。

Conclusion: PathChat+和SlideSeek在病理学中展现了强大的多模态理解和自主诊断能力。

Abstract: Pathology is experiencing rapid digital transformation driven by whole-slide
imaging and artificial intelligence (AI). While deep learning-based
computational pathology has achieved notable success, traditional models
primarily focus on image analysis without integrating natural language
instruction or rich, text-based context. Current multimodal large language
models (MLLMs) in computational pathology face limitations, including
insufficient training data, inadequate support and evaluation for multi-image
understanding, and a lack of autonomous, diagnostic reasoning capabilities. To
address these limitations, we introduce PathChat+, a new MLLM specifically
designed for human pathology, trained on over 1 million diverse,
pathology-specific instruction samples and nearly 5.5 million question answer
turns. Extensive evaluations across diverse pathology benchmarks demonstrated
that PathChat+ substantially outperforms the prior PathChat copilot, as well as
both state-of-the-art (SOTA) general-purpose and other pathology-specific
models. Furthermore, we present SlideSeek, a reasoning-enabled multi-agent AI
system leveraging PathChat+ to autonomously evaluate gigapixel whole-slide
images (WSIs) through iterative, hierarchical diagnostic reasoning, reaching
high accuracy on DDxBench, a challenging open-ended differential diagnosis
benchmark, while also capable of generating visually grounded,
humanly-interpretable summary reports.

</details>


### [147] [Logios : An open source Greek Polytonic Optical Character Recognition system](https://arxiv.org/abs/2506.21474)
*Perifanos Konstantinos,Goutsos Dionisis*

Key words: OCR, 希腊多调文字, 卷积神经网络, 循环神经网络, 开源

TL;DR: 提出一种专门用于希腊多调文字识别的OCR系统，结合卷积层和循环层的优势，提高了传统方法的准确性和效率。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统OCR方法在处理希腊多调文字时存在局限性，需要一种更精准和高效的解决方案。

Method: 结合卷积层特征提取和循环层序列学习的混合模型。

Result: 系统显著提升了希腊多调文字的识别准确性和效率。

Conclusion: 该OCR系统为希腊多调文字的数字化提供了有效的工具，并开源供学术使用。

Abstract: In this paper, we present an Optical Character Recognition (OCR) system
specifically designed for the accurate recognition and digitization of Greek
polytonic texts. By leveraging the combined strengths of convolutional layers
for feature extraction and recurrent layers for sequence learning, our system
addresses the unique challenges posed by Greek polytonic scripts. This approach
aims to overcome the limitations of traditional OCR methods, offering
significant improvements in accuracy and efficiency. We release the underlying
model as an open-source library and make our OCR platform available for
academic use.

</details>


### [148] [DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing](https://arxiv.org/abs/2506.20967)
*Lingling Cai,Kang Zhao,Hangjie Yuan,Xiang Wang,Yingya Zhang,Kejie Huang*

Key words: 视频编辑, 视频扩散变换器, 流变换, 高效方法

TL;DR: 论文提出了一种名为DFVEdit的高效零样本视频编辑方法，专为视频扩散变换器（Video DiTs）设计，通过流变换直接在潜空间操作，无需注意力修改或微调。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有视频编辑方法应用于Video DiTs时计算开销大，因此需要一种更高效的方法。

Method: 提出了DFVEdit方法，基于流变换视角统一编辑和采样，并结合条件Delta流向量（CDFV）、隐式交叉注意力（ICA）和嵌入增强（ER）提升编辑质量。

Result: DFVEdit实现了至少20倍推理速度提升和85%内存减少，在CogVideoX和Wan2.1等Video DiTs上表现出色。

Conclusion: DFVEdit在结构保真度、时空一致性和编辑质量上达到最先进水平。

Abstract: The advent of Video Diffusion Transformers (Video DiTs) marks a milestone in
video generation. However, directly applying existing video editing methods to
Video DiTs often incurs substantial computational overhead, due to
resource-intensive attention modification or finetuning. To alleviate this
problem, we present DFVEdit, an efficient zero-shot video editing method
tailored for Video DiTs. DFVEdit eliminates the need for both attention
modification and fine-tuning by directly operating on clean latents via flow
transformation. To be more specific, we observe that editing and sampling can
be unified under the continuous flow perspective. Building upon this
foundation, we propose the Conditional Delta Flow Vector (CDFV) -- a
theoretically unbiased estimation of DFV -- and integrate Implicit Cross
Attention (ICA) guidance as well as Embedding Reinforcement (ER) to further
enhance editing quality. DFVEdit excels in practical efficiency, offering at
least 20x inference speed-up and 85\% memory reduction on Video DiTs compared
to attention-engineering-based editing methods. Extensive quantitative and
qualitative experiments demonstrate that DFVEdit can be seamlessly applied to
popular Video DiTs (e.g., CogVideoX and Wan2.1), attaining state-of-the-art
performance on structural fidelity, spatial-temporal consistency, and editing
quality.

</details>


### [149] [From Cradle to Cane: A Two-Pass Framework for High-Fidelity Lifespan Face Aging](https://arxiv.org/abs/2506.20977)
*Tao Liu,Dafeng Zhang,Gengchen Li,Shizhuo Liu,Yongqi Song,Senmao Li,Shiqi Yang,Boqian Li,Kai Wang,Yaxing Wang*

Key words: 人脸老化, 扩散模型, 年龄准确性, 身份一致性, Cradle2Cane

TL;DR: 本文提出了一种名为Cradle2Cane的两阶段人脸老化框架，通过自适应噪声注入机制和身份感知嵌入，解决了年龄准确性和身份一致性之间的权衡问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的人脸老化方法难以在年龄准确性和身份一致性之间取得平衡，尤其是在处理大年龄跨度或极端头部姿态时表现不佳。

Method: Cradle2Cane基于少步文本到图像扩散模型，分为两阶段：第一阶段通过自适应噪声注入（AdaNI）实现年龄准确性，第二阶段通过身份感知嵌入（IDEmb）增强身份一致性。

Result: 在CelebA-HQ测试数据集上的实验表明，Cradle2Cane在年龄准确性和身份一致性上优于现有方法。

Conclusion: Cradle2Cane通过两阶段框架成功解决了人脸老化中的Age-ID权衡问题，表现优于现有方法。

Abstract: Face aging has become a crucial task in computer vision, with applications
ranging from entertainment to healthcare. However, existing methods struggle
with achieving a realistic and seamless transformation across the entire
lifespan, especially when handling large age gaps or extreme head poses. The
core challenge lies in balancing age accuracy and identity preservation--what
we refer to as the Age-ID trade-off. Most prior methods either prioritize age
transformation at the expense of identity consistency or vice versa. In this
work, we address this issue by proposing a two-pass face aging framework, named
Cradle2Cane, based on few-step text-to-image (T2I) diffusion models. The first
pass focuses on solving age accuracy by introducing an adaptive noise injection
(AdaNI) mechanism. This mechanism is guided by including prompt descriptions of
age and gender for the given person as the textual condition. Also, by
adjusting the noise level, we can control the strength of aging while allowing
more flexibility in transforming the face. However, identity preservation is
weakly ensured here to facilitate stronger age transformations. In the second
pass, we enhance identity preservation while maintaining age-specific features
by conditioning the model on two identity-aware embeddings (IDEmb): SVR-ArcFace
and Rotate-CLIP. This pass allows for denoising the transformed image from the
first pass, ensuring stronger identity preservation without compromising the
aging accuracy. Both passes are jointly trained in an end-to-end way. Extensive
experiments on the CelebA-HQ test dataset, evaluated through Face++ and Qwen-VL
protocols, show that our Cradle2Cane outperforms existing face aging methods in
age accuracy and identity consistency.

</details>


### [150] [HalluSegBench: Counterfactual Visual Reasoning for Segmentation Hallucination Evaluation](https://arxiv.org/abs/2506.21546)
*Xinzhuo Li,Adheesh Juvekar,Xingyou Liu,Muntasir Wahed,Kiet A. Nguyen,Ismini Lourentzou*

Key words: 视觉-语言分割,幻觉问题,反事实推理,基准评测,HalluSegBench

TL;DR: 论文回顾了视觉-语言分割领域的进展，指出现有模型存在幻觉问题（错误分割或标注），并提出首个针对视觉基础的幻觉评测基准HalluSegBench，包含1340对反事实实例和新的评测指标。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前视觉-语言分割模型在视觉基础任务中存在严重幻觉问题，而现有评测协议未能有效诊断这些失败，因此需要新的评测方法。

Method: 提出了HalluSegBench基准，包含反事实视觉推理数据集（1340对实例）和新指标，量化模型在视觉编辑场景下的幻觉敏感性。

Result: 实验表明，视觉驱动的幻觉问题比标签驱动更普遍，模型常坚持错误分割，凸显反事实推理的必要性。

Conclusion: HalluSegBench为视觉基础中的幻觉问题提供了首个诊断工具，显示视觉驱动幻觉是主要挑战，需进一步改进模型。

Abstract: Recent progress in vision-language segmentation has significantly advanced
grounded visual understanding. However, these models often exhibit
hallucinations by producing segmentation masks for objects not grounded in the
image content or by incorrectly labeling irrelevant regions. Existing
evaluation protocols for segmentation hallucination primarily focus on label or
textual hallucinations without manipulating the visual context, limiting their
capacity to diagnose critical failures. In response, we introduce
HalluSegBench, the first benchmark specifically designed to evaluate
hallucinations in visual grounding through the lens of counterfactual visual
reasoning. Our benchmark consists of a novel dataset of 1340 counterfactual
instance pairs spanning 281 unique object classes, and a set of newly
introduced metrics that quantify hallucination sensitivity under visually
coherent scene edits. Experiments on HalluSegBench with state-of-the-art
vision-language segmentation models reveal that vision-driven hallucinations
are significantly more prevalent than label-driven ones, with models often
persisting in false segmentation, highlighting the need for counterfactual
reasoning to diagnose grounding fidelity.

</details>


### [151] [Segment Anything in Pathology Images with Natural Language](https://arxiv.org/abs/2506.20988)
*Zhixuan Chen,Junlin Hou,Liqi Lin,Yihui Wang,Yequan Bie,Xi Wang,Yanning Zhou,Ronald Cheong Kin Chan,Hao Chen*

Key words: 病理图像分割, 文本提示, 基础模型, PathSeg数据集, 可解释AI

TL;DR: PathSegmentor是一种基于文本提示的病理图像分割基础模型，解决了现有方法在临床应用中因标注数据有限和类别定义受限而面临的挑战。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决病理图像分割中标注数据不足和类别定义受限的问题，提高临床应用的准确性和适用性。

Method: 提出PathSegmentor模型和PathSeg数据集，利用自然语言提示进行语义分割，无需复杂的空间输入。

Result: PathSegmentor在准确性和适用性上优于现有模型，Dice分数分别提升0.145和0.429，并展示了强鲁棒性和泛化能力。

Conclusion: PathSegmentor为精准肿瘤学中的可解释AI提供了支持，增强了诊断模型的可解释性。

Abstract: Pathology image segmentation is crucial in computational pathology for
analyzing histological features relevant to cancer diagnosis and prognosis.
However, current methods face major challenges in clinical applications due to
limited annotated data and restricted category definitions. To address these
limitations, we propose PathSegmentor, the first text-prompted segmentation
foundation model designed specifically for pathology images. We also introduce
PathSeg , the largest and most comprehensive dataset for pathology
segmentation, built from 17 public sources and containing 275k image-mask-label
triples across 160 diverse categories. With PathSegmentor, users can perform
semantic segmentation using natural language prompts, eliminating the need for
laborious spatial inputs such as points or boxes. Extensive experiments
demonstrate that PathSegmentor outperforms specialized models with higher
accuracy and broader applicability, while maintaining a compact architecture.
It significantly surpasses existing spatial- and text-prompted models by 0.145
and 0.429 in overall Dice scores, respectively, showing strong robustness in
segmenting complex structures and generalizing to external datasets. Moreover,
PathSegmentor's outputs enhance the interpretability of diagnostic models
through feature importance estimation and imaging biomarker discovery, offering
pathologists evidence-based support for clinical decision-making. This work
advances the development of explainable AI in precision oncology.

</details>


### [152] [Multimodal Prompt Alignment for Facial Expression Recognition](https://arxiv.org/abs/2506.21017)
*Fuyan Ma,Yiran He,Bin Sun,Shutao Li*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Prompt learning has been widely adopted to efficiently adapt vision-language
models (VLMs) like CLIP for various downstream tasks. Despite their success,
current VLM-based facial expression recognition (FER) methods struggle to
capture fine-grained textual-visual relationships, which are essential for
distinguishing subtle differences between facial expressions. To address this
challenge, we propose a multimodal prompt alignment framework for FER, called
MPA-FER, that provides fine-grained semantic guidance to the learning process
of prompted visual features, resulting in more precise and interpretable
representations. Specifically, we introduce a multi-granularity hard prompt
generation strategy that utilizes a large language model (LLM) like ChatGPT to
generate detailed descriptions for each facial expression. The LLM-based
external knowledge is injected into the soft prompts by minimizing the feature
discrepancy between the soft prompts and the hard prompts. To preserve the
generalization abilities of the pretrained CLIP model, our approach
incorporates prototype-guided visual feature alignment, ensuring that the
prompted visual features from the frozen image encoder align closely with
class-specific prototypes. Additionally, we propose a cross-modal global-local
alignment module that focuses on expression-relevant facial features, further
improving the alignment between textual and visual features. Extensive
experiments demonstrate our framework outperforms state-of-the-art methods on
three FER benchmark datasets, while retaining the benefits of the pretrained
model and minimizing computational costs.

</details>


### [153] [Improving Diffusion-Based Image Editing Faithfulness via Guidance and Scheduling](https://arxiv.org/abs/2506.21045)
*Hansam Cho,Seoung Bum Kim*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Text-guided diffusion models have become essential for high-quality image
synthesis, enabling dynamic image editing. In image editing, two crucial
aspects are editability, which determines the extent of modification, and
faithfulness, which reflects how well unaltered elements are preserved.
However, achieving optimal results is challenging because of the inherent
trade-off between editability and faithfulness. To address this, we propose
Faithfulness Guidance and Scheduling (FGS), which enhances faithfulness with
minimal impact on editability. FGS incorporates faithfulness guidance to
strengthen the preservation of input image information and introduces a
scheduling strategy to resolve misalignment between editability and
faithfulness. Experimental results demonstrate that FGS achieves superior
faithfulness while maintaining editability. Moreover, its compatibility with
various editing methods enables precise, high-quality image edits across
diverse tasks.

</details>


### [154] [EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for Efficient Egocentric Perception](https://arxiv.org/abs/2506.21080)
*Sanjoy Chowdhury,Subrata Biswas,Sayan Nag,Tushar Nagarajan,Calvin Murdock,Ishwarya Ananthabhotla,Yijun Qian,Vamsi Krishna Ithapu,Dinesh Manocha,Ruohan Gao*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Modern perception models, particularly those designed for multisensory
egocentric tasks, have achieved remarkable performance but often come with
substantial computational costs. These high demands pose challenges for
real-world deployment, especially in resource-constrained environments. In this
paper, we introduce EgoAdapt, a framework that adaptively performs cross-modal
distillation and policy learning to enable efficient inference across different
egocentric perception tasks, including egocentric action recognition, active
speaker localization, and behavior anticipation. Our proposed policy module is
adaptable to task-specific action spaces, making it broadly applicable.
Experimental results on three challenging egocentric datasets EPIC-Kitchens,
EasyCom, and Aria Everyday Activities demonstrate that our method significantly
enhances efficiency, reducing GMACs by up to 89.09%, parameters up to 82.02%,
and energy up to 9.6x, while still on-par and in many cases outperforming, the
performance of corresponding state-of-the-art models.

</details>


### [155] [IPFormer-VideoLLM: Enhancing Multi-modal Video Understanding for Multi-shot Scenes](https://arxiv.org/abs/2506.21116)
*Yujia Liang,Jile Jiao,Zhicheng Wang,Xuetao Feng,Zixuan Ye,Yuan Wang,Hao Lu*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Video Large Language Models (VideoLLMs) have demonstrated remarkable
understanding capabilities, but are found struggling to tackle multi-shot
scenarios,e.g., video clips with varying camera angles or scene changes. This
challenge can render failures such as instance identity forgetting and key
frame negligence. In this work, we first attribute the challenge to the lack of
multi-shot annotations among existing datasets and therefore we introduce a new
dataset termed MultiClip-Bench, featuring dense descriptions and
instruction-based question-answering pairs tailored for multi-shot scenarios.
We empirically find that the training set significantly boosts the multi-shot
performance, while the testing benchmark provides a reliable measure of the
model capability in multi-shot scenarios. By further analyzing and discovering
that current models only encode instance features in a discrete or lossy
manner, at the risk of missing identity information, we then contribute a new
model IPFormer-VideoLLM. Its key idea is the injection of instance-level
features as instance prompts through an efficient attention-based connector.
This allows for the aggregation of instance-specific information across scenes.
Experiments demonstrate that our proposed dataset and model not only enhance
the multi-scene video understanding significantly, but also offer distinct
advantages across various video benchmarks.

</details>


### [156] [Robust Deep Learning for Myocardial Scar Segmentation in Cardiac MRI with Noisy Labels](https://arxiv.org/abs/2506.21151)
*Aida Moafi,Danial Moafi,Evgeny M. Mirkes,Gerry P. McCann,Abbas S. Alatrany,Jayanth R. Arnold,Mostafa Mehdipour Ghazi*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The accurate segmentation of myocardial scars from cardiac MRI is essential
for clinical assessment and treatment planning. In this study, we propose a
robust deep-learning pipeline for fully automated myocardial scar detection and
segmentation by fine-tuning state-of-the-art models. The method explicitly
addresses challenges of label noise from semi-automatic annotations, data
heterogeneity, and class imbalance through the use of Kullback-Leibler loss and
extensive data augmentation. We evaluate the model's performance on both acute
and chronic cases and demonstrate its ability to produce accurate and smooth
segmentations despite noisy labels. In particular, our approach outperforms
state-of-the-art models like nnU-Net and shows strong generalizability in an
out-of-distribution test set, highlighting its robustness across various
imaging conditions and clinical tasks. These results establish a reliable
foundation for automated myocardial scar quantification and support the broader
clinical adoption of deep learning in cardiac imaging.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [157] [Agile Management for Machine Learning: A Systematic Mapping Study](https://arxiv.org/abs/2506.20759)
*Lucas Romao,Hugo Villamizar,Romeu Oliveira,Silvio Alonso,Marcos Kalinowski*

Key words: 机器学习, 敏捷管理, 系统映射研究, ML-enabled系统

TL;DR: 该研究通过系统映射研究总结了ML-enabled系统中敏捷管理的现状，识别了8个关键主题和主要挑战（如ML任务的工作量估算），并指出未来需要更严格的实证评估。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 由于ML开发的动态性和传统项目管理的局限性，研究旨在探讨如何有效应用敏捷方法来管理ML-enabled系统。

Method: 采用混合搜索策略（数据库搜索与雪球迭代）进行系统映射研究，分析了27篇2008至2024年的论文。

Result: 总结了8个框架和8个关键主题，主要挑战是ML任务的工作量估算。

Conclusion: 研究填补了领域空白，但需更多实证验证。

Abstract: [Context] Machine learning (ML)-enabled systems are present in our society,
driving significant digital transformations. The dynamic nature of ML
development, characterized by experimental cycles and rapid changes in data,
poses challenges to traditional project management. Agile methods, with their
flexibility and incremental delivery, seem well-suited to address this
dynamism. However, it is unclear how to effectively apply these methods in the
context of ML-enabled systems, where challenges require tailored approaches.
[Goal] Our goal is to outline the state of the art in agile management for
ML-enabled systems. [Method] We conducted a systematic mapping study using a
hybrid search strategy that combines database searches with backward and
forward snowballing iterations. [Results] Our study identified 27 papers
published between 2008 and 2024. From these, we identified eight frameworks and
categorized recommendations and practices into eight key themes, such as
Iteration Flexibility, Innovative ML-specific Artifacts, and the Minimal Viable
Model. The main challenge identified across studies was accurate effort
estimation for ML-related tasks. [Conclusion] This study contributes by mapping
the state of the art and identifying open gaps in the field. While relevant
work exists, more robust empirical evaluation is still needed to validate these
contributions.

</details>


### [158] [Generating Reliable Adverse event Profiles for Health through Automated Integrated Data (GRAPH-AID): A Semi-Automated Ontology Building Approach](https://arxiv.org/abs/2506.20851)
*Srikar Reddy Gadusu,Larry Callahan,Samir Lababidi,Arunasri Nishtala,Sophia Healey,Hande McGinty*

Key words: 本体生成, Neo4j, OWL, Python, rdflib

TL;DR: 本文提出了一种用户友好的方法，利用Python和其rdflib库来支持本体开发，解决了Neo4j数据库与OWL的无缝集成难题。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 随着数据和知识的快速扩展，系统化的本体生成方法变得至关重要。现有的KNARM方法在集成Neo4j与OWL时存在挑战，需要更易用的方法。

Method: 使用Python和rdflib库开发了一种新型方法，自动生成类和公理，展示了基于FDA FAERS数据库的Neo4j数据集。

Result: 提出的方法提供了一个实用的解决方案，支持药物安全监测和公共卫生决策。

Conclusion: 该方法为快速增长的不良药物事件数据集的本体生成提供了有效支持。

Abstract: As data and knowledge expand rapidly, adopting systematic methodologies for
ontology generation has become crucial. With the daily increases in data
volumes and frequent content changes, the demand for databases to store and
retrieve information for the creation of knowledge graphs has become
increasingly urgent. The previously established Knowledge Acquisition and
Representation Methodology (KNARM) outlines a systematic approach to address
these challenges and create knowledge graphs. However, following this
methodology highlights the existing challenge of seamlessly integrating Neo4j
databases with the Web Ontology Language (OWL). Previous attempts to integrate
data from Neo4j into an ontology have been discussed, but these approaches
often require an understanding of description logics (DL) syntax, which may not
be familiar to many users. Thus, a more accessible method is necessary to
bridge this gap. This paper presents a user-friendly approach that utilizes
Python and its rdflib library to support ontology development. We showcase our
novel approach through a Neo4j database we created by integrating data from the
Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS)
database. Using this dataset, we developed a Python script that automatically
generates the required classes and their axioms, facilitating a smoother
integration process. This approach offers a practical solution to the
challenges of ontology generation in the context of rapidly growing adverse
drug event datasets, supporting improved drug safety monitoring and public
health decision-making.

</details>


### [159] [Engineering RAG Systems for Real-World Applications: Design, Development, and Evaluation](https://arxiv.org/abs/2506.20869)
*Md Toufique Hasan,Muhammad Waseem,Kai-Kristian Kemell,Ayman Asad Khan,Mika Saari,Pekka Abrahamsson*

Key words: RAG, LLMs, 多语言OCR, 语义检索, 用户评估

TL;DR: 本文介绍了五种基于真实场景的RAG系统应用，涵盖多个领域，并通过用户评估总结了12个关键经验教训。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 解决LLMs在事实准确性和上下文相关性上的不足，实证研究RAG系统在实际应用中的表现。

Method: 开发五个领域的RAG系统，结合多语言OCR、语义检索和领域适配LLM，通过用户评估六项指标。

Result: 用户评估显示系统在易用性、相关性、透明度等方面表现良好，总结了12个技术、操作和伦理挑战。

Conclusion: RAG系统在实践中可靠且可用，但需注意技术、操作和伦理问题。

Abstract: Retrieval-Augmented Generation (RAG) systems are emerging as a key approach
for grounding Large Language Models (LLMs) in external knowledge, addressing
limitations in factual accuracy and contextual relevance. However, there is a
lack of empirical studies that report on the development of RAG-based
implementations grounded in real-world use cases, evaluated through general
user involvement, and accompanied by systematic documentation of lessons
learned. This paper presents five domain-specific RAG applications developed
for real-world scenarios across governance, cybersecurity, agriculture,
industrial research, and medical diagnostics. Each system incorporates
multilingual OCR, semantic retrieval via vector embeddings, and domain-adapted
LLMs, deployed through local servers or cloud APIs to meet distinct user needs.
A web-based evaluation involving a total of 100 participants assessed the
systems across six dimensions: (i) Ease of Use, (ii) Relevance, (iii)
Transparency, (iv) Responsiveness, (v) Accuracy, and (vi) Likelihood of
Recommendation. Based on user feedback and our development experience, we
documented twelve key lessons learned, highlighting technical, operational, and
ethical challenges affecting the reliability and usability of RAG systems in
practice.

</details>


### [160] [Complex Model Transformations by Reinforcement Learning with Uncertain Human Guidance](https://arxiv.org/abs/2506.20883)
*Kyanna Dagenais,Istvan David*

Key words: 模型驱动工程, 模型转换, 强化学习, 人类指导, 人机协同

TL;DR: 论文提出了一种结合强化学习（RL）与人类指导的方法，用于开发复杂的模型转换（MT）序列，并展示了人类指导显著提升了RL性能。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 模型驱动工程中复杂的模型转换（MT）序列开发困难且易错，强化学习（RL）可帮助解决这一问题，但RL在复杂问题中表现不佳，人类指导可以弥补这一点。

Method: 提出了一种技术框架，将用户定义的MT映射到RL基础组件中，并通过RL程序执行以寻找最优MT序列，同时结合人类指导（即使不确定）。

Result: 评估表明，即使人类指导不确定，也能显著提升RL性能，并更高效地开发复杂MT序列。

Conclusion: 通过权衡人类指导的确定性与及时性，该方法为RL驱动的人机协同工程方法迈出了一步。

Abstract: Model-driven engineering problems often require complex model transformations
(MTs), i.e., MTs that are chained in extensive sequences. Pertinent examples of
such problems include model synchronization, automated model repair, and design
space exploration. Manually developing complex MTs is an error-prone and often
infeasible process. Reinforcement learning (RL) is an apt way to alleviate
these issues. In RL, an autonomous agent explores the state space through trial
and error to identify beneficial sequences of actions, such as MTs. However, RL
methods exhibit performance issues in complex problems. In these situations,
human guidance can be of high utility. In this paper, we present an approach
and technical framework for developing complex MT sequences through RL, guided
by potentially uncertain human advice. Our framework allows user-defined MTs to
be mapped onto RL primitives, and executes them as RL programs to find optimal
MT sequences. Our evaluation shows that human guidance, even if uncertain,
substantially improves RL performance, and results in more efficient
development of complex MTs. Through a trade-off between the certainty and
timeliness of human advice, our method takes a step towards RL-driven
human-in-the-loop engineering methods.

</details>


### [161] [How Good Are Synthetic Requirements ? Evaluating LLM-Generated Datasets for AI4RE](https://arxiv.org/abs/2506.21138)
*Abdelkarim El-Hajjami,Camille Salinesi*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The shortage of publicly available, labeled requirements datasets remains a
major barrier to advancing Artificial Intelligence for Requirements Engineering
(AI4RE). While Large Language Models offer promising capabilities for synthetic
data generation, systematic approaches to control and optimize the quality of
generated requirements remain underexplored. This paper presents Synthline v1,
an enhanced Product Line approach for generating synthetic requirements data
that extends our earlier v0 version with advanced generation strategies and
curation techniques. We investigate four research questions assessing how
prompting strategies, automated prompt optimization, and post-generation
curation affect data quality across four classification tasks: defect
detection, functional vs. non-functional, quality vs. non-quality, and security
vs. non-security. Our evaluation shows that multi-sample prompting
significantly boosts both utility and diversity over single-sample generation,
with F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic
Editing) for automated prompt optimization yields task-dependent results,
greatly improving functional classification (+32.5 points) but reducing
performance on others. Interestingly, similarity-based curation improves
diversity but often harms classification performance, indicating that some
redundancy may help ML models. Most importantly, our results show that
synthetic requirements can match or outperform human-authored ones for specific
tasks, with synthetic data surpassing human data for security (+7.8 points) and
defect classification (+15.4 points). These findings offer practical insights
for AI4RE and chart a viable path to mitigating dataset scarcity through
systematic synthetic generation.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [162] [ClusterRCA: Network Failure Diagnosis in HPC Systems Using Multimodal Data](https://arxiv.org/abs/2506.20673)
*Yongqian Sun,Xijie Pan,Xiao Xiong,Lei Tao,Jiaju Wang,Shenglin Zhang,Yuan Yuan,Yuqi Li,Kunlin Jian*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Network failure diagnosis is challenging yet critical for high-performance
computing (HPC) systems. Existing methods cannot be directly applied to HPC
scenarios due to data heterogeneity and lack of accuracy. This paper proposes a
novel framework, called ClusterRCA, to localize culprit nodes and determine
failure types by leveraging multimodal data. ClusterRCA extracts features from
topologically connected network interface controller (NIC) pairs to analyze the
diverse, multimodal data in HPC systems. To accurately localize culprit nodes
and determine failure types, ClusterRCA combines classifier-based and
graph-based approaches. A failure graph is constructed based on the output of
the state classifier, and then it performs a customized random walk on the
graph to localize the root cause. Experiments on datasets collected by a
top-tier global HPC device vendor show ClusterRCA achieves high accuracy in
diagnosing network failure for HPC systems. ClusterRCA also maintains robust
performance across different application scenarios.

</details>


### [163] [Utility-Driven Speculative Decoding for Mixture-of-Experts](https://arxiv.org/abs/2506.20675)
*Anish Saxena,Po-An Tsai,Hritvik Taneja,Aamer Jaleel,Moinuddin Qureshi*

Key words: GPU内存带宽、大型语言模型、推测解码、混合专家模型、动态调整、吞吐量优化

TL;DR: GPU内存带宽是大型语言模型（LLM）低延迟推断的主要瓶颈。传统密集LLM通过推测解码提升吞吐量，但混合专家模型（MoE）因权重激活方式不同导致推测解码不实用。Cascade框架通过动态调整K值选择性启用推测解码，显著提升MoE模型的性能。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 由于GPU内存带宽限制，LLM推断的延迟成为关键问题。推测解码通过并行验证轻量级提议的K个令牌提升吞吐量。然而，MoE模型的权重激活特性使得传统推测解码效率低下，甚至导致性能下降。

Method: 提出Cascade框架，通过动态调整K值和选择性启用推测解码来优化MoE模型的性能。Cascade使用“推测效用”指标，根据令牌增益与验证成本的比值动态决策。

Result: Cascade在vLLM中实现，并在五个流行的MoE模型中测试。结果表明，Cascade将性能下降限制在5%以内（相比传统方法的1.5倍），吞吐量提升7-14%。

Conclusion: Cascade框架通过动态调整推测解码策略，使其在MoE模型中变得实用，显著提升了吞吐量并避免了性能下降。

Abstract: GPU memory bandwidth is the main bottleneck for low-latency Large Language
Model (LLM) inference. Speculative decoding leverages idle GPU compute by using
a lightweight drafter to propose K tokens, which the LLM verifies in parallel,
boosting token throughput. In conventional dense LLMs, all model weights are
fetched each iteration, so speculation adds no latency overhead. Emerging
Mixture of Experts (MoE) models activate only a subset of weights per token,
greatly reducing data movement. However, we show that speculation is
ineffective for MoEs: draft tokens collectively activate more weights,
increasing data movement and verification time by 2-3x. When token throughput
gains fail to offset this overhead, speculation causes slowdowns up to 1.5x,
making it infeasible. Even when useful, the optimal K varies by task, model,
and even between requests and iterations. Thus, despite widespread use in dense
LLMs, speculation remains impractical in leading MoEs.
  We present Cascade, a utility-driven framework that selectively enables
speculation to avoid slowdowns and dynamically tunes K to accelerate MoE
serving. Cascade uses a lightweight metric, speculation utility, the ratio of
token gains to verification cost, which shows iteration-level locality,
enabling periodic decisions via short test and longer set phases. For each
request, Cascade disables speculation if utility drops below one during
testing, and when utility exceeds one, tests multiple K-values to choose the
utility-maximizing K for the set phase. We implement Cascade in vLLM and
evaluate it on five popular MoEs with workloads spanning code, math,
extraction, and mixed tasks. Cascade limits slowdown to 5% (vs. 1.5x) and
improves throughput by 7-14% over static K, making speculative decoding
practical for MoEs.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [164] [DRAGON: Distributional Rewards Optimize Diffusion Generative Models](https://arxiv.org/abs/2504.15217)
*Yatong Bai,Jonah Casebeer,Somayeh Sojoudi,Nicholas J. Bryan*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present Distributional RewArds for Generative OptimizatioN (DRAGON), a
versatile framework for fine-tuning media generation models towards a desired
outcome. Compared with traditional reinforcement learning with human feedback
(RLHF) or pairwise preference approaches such as direct preference optimization
(DPO), DRAGON is more flexible. It can optimize reward functions that evaluate
either individual examples or distributions of them, making it compatible with
a broad spectrum of instance-wise, instance-to-distribution, and
distribution-to-distribution rewards. Leveraging this versatility, we construct
novel reward functions by selecting an encoder and a set of reference examples
to create an exemplar distribution. When cross-modality encoders such as CLAP
are used, the reference examples may be of a different modality (e.g., text
versus audio). Then, DRAGON gathers online and on-policy generations, scores
them to construct a positive demonstration set and a negative set, and
leverages the contrast between the two sets to maximize the reward. For
evaluation, we fine-tune an audio-domain text-to-music diffusion model with 20
different reward functions, including a custom music aesthetics model, CLAP
score, Vendi diversity, and Frechet audio distance (FAD). We further compare
instance-wise (per-song) and full-dataset FAD settings while ablating multiple
FAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an
81.45% average win rate. Moreover, reward functions based on exemplar sets
indeed enhance generations and are comparable to model-based rewards. With an
appropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality
win rate without training on human preference annotations. As such, DRAGON
exhibits a new approach to designing and optimizing reward functions for
improving human-perceived quality. Sound examples at
https://ml-dragon.github.io/web.

</details>


### [165] [Exploring Adapter Design Tradeoffs for Low Resource Music Generation](https://arxiv.org/abs/2506.21298)
*Atharva Mehta,Shivam Chauhan,Monojit Choudhury*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Fine-tuning large-scale music generation models, such as MusicGen and
Mustango, is a computationally expensive process, often requiring updates to
billions of parameters and, therefore, significant hardware resources.
Parameter-Efficient Fine-Tuning (PEFT) techniques, particularly adapter-based
methods, have emerged as a promising alternative, enabling adaptation with
minimal trainable parameters while preserving model performance. However, the
design choices for adapters, including their architecture, placement, and size,
are numerous, and it is unclear which of these combinations would produce
optimal adapters and why, for a given case of low-resource music genre. In this
paper, we attempt to answer this question by studying various adapter
configurations for two AI music models, MusicGen and Mustango, on two genres:
Hindustani Classical and Turkish Makam music.
  Our findings reveal distinct trade-offs: convolution-based adapters excel in
capturing fine-grained local musical details such as ornamentations and short
melodic phrases, while transformer-based adapters better preserve long-range
dependencies crucial for structured improvisation. Additionally, we analyze
computational resource requirements across different adapter scales,
demonstrating how mid-sized adapters (40M parameters) achieve an optimal
balance between expressivity and quality. Furthermore, we find that Mustango, a
diffusion-based model, generates more diverse outputs with better adherence to
the description in the input prompt while lacking in providing stability in
notes, rhythm alignment, and aesthetics. Also, it is computationally intensive
and requires significantly more time to train. In contrast, autoregressive
models like MusicGen offer faster training and are more efficient, and can
produce better quality output in comparison, but have slightly higher
redundancy in their generations.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [166] [Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends](https://arxiv.org/abs/2506.20966)
*Tian-Yu Xiang,Ao-Qun Jin,Xiao-Hu Zhou,Mei-Jiang Gui,Xiao-Liang Xie,Shi-Qi Liu,Shuang-Yi Wang,Sheng-Bin Duan,Fu-Chao Xie,Wen-Kai Wang,Si-Cheng Wang,Ling-Yun Li,Tian Tu,Zeng-Guang Hou*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Vision-language-action (VLA) models extend vision-language models (VLM) by
integrating action generation modules for robotic manipulation. Leveraging
strengths of VLM in vision perception and instruction understanding, VLA models
exhibit promising generalization across diverse manipulation tasks. However,
applications demanding high precision and accuracy reveal performance gaps
without further adaptation. Evidence from multiple domains highlights the
critical role of post-training to align foundational models with downstream
applications, spurring extensive research on post-training VLA models. VLA
model post-training aims to address the challenge of improving an embodiment's
ability to interact with the environment for the given tasks, analogous to the
process of humans motor skills acquisition. Accordingly, this paper reviews
post-training strategies for VLA models through the lens of human motor
learning, focusing on three dimensions: environments, embodiments, and tasks. A
structured taxonomy is introduced aligned with human learning mechanisms: (1)
enhancing environmental perception, (2) improving embodiment awareness, (3)
deepening task comprehension, and (4) multi-component integration. Finally, key
challenges and trends in post-training VLA models are identified, establishing
a conceptual framework to guide future research. This work delivers both a
comprehensive overview of current VLA model post-training methods from a human
motor learning perspective and practical insights for VLA model development.
(Project website: https://github.com/AoqunJin/Awesome-VLA-Post-Training)

</details>


### [167] [V2X-REALM: Vision-Language Model-Based Robust End-to-End Cooperative Autonomous Driving with Adaptive Long-Tail Modeling](https://arxiv.org/abs/2506.21041)
*Junwei You,Pei Li,Zhuoyu Jiang,Zilin Huang,Rui Gan,Haotian Shi,Bin Ran*

Key words: 自动驾驶, 长尾场景, 视觉语言模型, 多模态学习, 协作感知

TL;DR: V2X-REALM是一个基于视觉语言模型（VLM）的框架，通过自适应多模态学习解决城市自动驾驶中的长尾场景问题，提升了协作自动驾驶的鲁棒性和安全性。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决城市自动驾驶在罕见、多样和视觉退化的长尾场景下的规划与决策问题，特别是在协作环境中。

Method: 提出V2X-REALM框架，包括长尾场景生成与评估、门控多场景自适应注意力模块和多任务场景感知对比学习目标。

Result: 实验表明V2X-REALM在鲁棒性、语义推理、安全性和规划准确性上显著优于现有基线。

Conclusion: V2X-REALM为端到端协作自动驾驶的可扩展性提供了重要进展。

Abstract: Ensuring robust planning and decision-making under rare, diverse, and
visually degraded long-tail scenarios remains a fundamental challenge for
autonomous driving in urban environments. This issue becomes more critical in
cooperative settings, where vehicles and infrastructure jointly perceive and
reason across complex environments. To address this challenge, we propose
V2X-REALM, a vision-language model (VLM)-based framework with adaptive
multimodal learning for robust cooperative autonomous driving under long-tail
scenarios. V2X-REALM introduces three core innovations: (i) a prompt-driven
long-tail scenario generation and evaluation pipeline that leverages foundation
models to synthesize realistic long-tail conditions such as snow and fog across
vehicle- and infrastructure-side views, enriching training diversity
efficiently; (ii) a gated multi-scenario adaptive attention module that
modulates the visual stream using scenario priors to recalibrate ambiguous or
corrupted features; and (iii) a multi-task scenario-aware contrastive learning
objective that improves multimodal alignment and promotes cross-scenario
feature separability. Extensive experiments demonstrate that V2X-REALM
significantly outperforms existing baselines in robustness, semantic reasoning,
safety, and planning accuracy under complex, challenging driving conditions,
advancing the scalability of end-to-end cooperative autonomous driving.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [168] [Enhancing Homophily-Heterophily Separation: Relation-Aware Learning in Heterogeneous Graphs](https://arxiv.org/abs/2506.20980)
*Ziyu Zheng,Yaming Yang,Ziyu Guan,Wei Zhao,Weigang Lu*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.SI

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Real-world networks usually have a property of node heterophily, that is, the
connected nodes usually have different features or different labels. This
heterophily issue has been extensively studied in homogeneous graphs but
remains under-explored in heterogeneous graphs, where there are multiple types
of nodes and edges. Capturing node heterophily in heterogeneous graphs is very
challenging since both node/edge heterogeneity and node heterophily should be
carefully taken into consideration. Existing methods typically convert
heterogeneous graphs into homogeneous ones to learn node heterophily, which
will inevitably lose the potential heterophily conveyed by heterogeneous
relations. To bridge this gap, we propose Relation-Aware Separation of
Homophily and Heterophily (RASH), a novel contrastive learning framework that
explicitly models high-order semantics of heterogeneous interactions and
adaptively separates homophilic and heterophilic patterns. Particularly, RASH
introduces dual heterogeneous hypergraphs to encode multi-relational bipartite
subgraphs and dynamically constructs homophilic graphs and heterophilic graphs
based on relation importance. A multi-relation contrastive loss is designed to
align heterogeneous and homophilic/heterophilic views by maximizing mutual
information. In this way, RASH simultaneously resolves the challenges of
heterogeneity and heterophily in heterogeneous graphs. Extensive experiments on
benchmark datasets demonstrate the effectiveness of RASH across various
downstream tasks. The code is available at:
https://github.com/zhengziyu77/RASH.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [169] [Exploring the Effects of Chatbot Anthropomorphism and Human Empathy on Human Prosocial Behavior Toward Chatbots](https://arxiv.org/abs/2506.20748)
*Jingshu Li,Zicheng Zhu,Renwen Zhang,Yi-Chieh Lee*

Key words: 聊天机器人、拟人化、同理心、亲社会行为、CASA框架

TL;DR: 研究者探讨了聊天机器人拟人化如何影响人类的同理心及其对聊天机器人的亲社会行为和意图。实验显示，人类的身份和情感表达能增加亲社会行为，同理心在其中起中介作用。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 填补研究空白，探讨人类帮助聊天机器人的动机因素。

Method: 基于CASA框架设计在线实验，让聊天机器人在协作任务中犯错并解释原因，测量参与者的亲社会行为和意图。

Result: 聊天机器人的人类身份和情感表达通过同理心中介增加了人类的亲社会行为和意图。

Conclusion: 拟人化设计能促进人类对聊天机器人的亲社会行为，为相关领域提供实践启示。

Abstract: Chatbots are increasingly integrated into people's lives and are widely used
to help people. Recently, there has also been growing interest in the reverse
direction-humans help chatbots-due to a wide range of benefits including better
chatbot performance, human well-being, and collaborative outcomes. However,
little research has explored the factors that motivate people to help chatbots.
To address this gap, we draw on the Computers Are Social Actors (CASA)
framework to examine how chatbot anthropomorphism-including human-like
identity, emotional expression, and non-verbal expression-influences human
empathy toward chatbots and their subsequent prosocial behaviors and
intentions. We also explore people's own interpretations of their prosocial
behaviors toward chatbots. We conducted an online experiment (N = 244) in which
chatbots made mistakes in a collaborative image labeling task and explained the
reasons to participants. We then measured participants' prosocial behaviors and
intentions toward the chatbots. Our findings revealed that human identity and
emotional expression of chatbots increased participants' prosocial behavior and
intention toward chatbots, with empathy mediating these effects. Qualitative
analysis further identified two motivations for participants' prosocial
behaviors: empathy for the chatbot and perceiving the chatbot as human-like. We
discuss the implications of these results for understanding and promoting human
prosocial behaviors toward chatbots.

</details>
